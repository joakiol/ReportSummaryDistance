Disambiguation ofSuper Pint, of Speech (or Supertags)Ahnost ParsingAravind i(.
Joshi and B. Sriniw~sDepa.rtment of Computer  and Information ScienceUniversity o\[ Pen nsylvanh~I>hiladelphia, PA 19104, USA{joshi, stint} ~linc.cis.upeiln.eduAbst rac t :  In a lexicalized grammar Ibrnlal-isni such as LexicMized Tree-Adjoining (h'~unmar( I3'AG), each lexicM item is associated with atleast one elementary structure (supertag) thatlocalizes syntactic a.nd semantic dependencies.Thus a parser for a lexicalized grammar mustsearch a large set of supertags to choose the rightones to combine for the parse of the sentence.
Wepresent techniques I'or dlsambiguating supertagsusing local inlorlnlttion s~Lch as lexicM preferenceand local lexicN dependencies.
Tim similaritybetween LTAG and l)ependency grammars i ex-ploited in the dependency niodel of snpertag dis-a.mbiguation.
The performance results for vari-otis models of supert;tg disambigu~ttk)n such asunigram; trigram and dependency-based modelsare presented.1 In t roduct ionl>art-of-spee<:h disanll>iguation techni<lues (tag-gers) are often used to eliminat<, (or sul>sl;an-tlally reduce) the lm.rt-of-spee,<;h anil>iguity priorto parsing.
The ta.ggel's are all local hi the sensethat they use inform~tion front a limited contextin deciding which tag(s) to choose for each word.As is well known, these taggers are quil;e, success-ful.In a lexicalized grammar such its the I,exicM-ized "Dee-Adjoining Grammar (13~AG), each lex-ical item is associated with at least one elemen-tary structure (tree).
The elementary structuresof I'PAG localize dependencies, including longdistance dependencies, by requiring that M1 andonly the dependent elements be present withinthe saute structnre.
As a result of this localiza-tion, a IoxicM item may be (a.nd, in general, a,1-most alwa,ys is) associa,ted with more.
than oneelementary structure.
~Ve will cMl these ele.-mentary structures upertags ,  in order to dis-tinguish them l'rom the standard part-of-speechtags.
Note that even when a word has a uniquestandard part-of-speech, say a verb (V), therewill usually lie more than one superta.g associatedwith this word.
Since when the parse is complete,there is only one supertag for each word (assum-ing there is no global ambiguity), an L'\['AC, parser(SchMms, 1988) nee.ds to search a large space o1"supertags to select the right one lbr each wordbefore combining them for the parse of a. sen-tence.
It is this 1)roblem of supertag dis;unbigua-tion that we address in I, his paper.Since l,'l.
'A(',s are lexlcalized, we are.
presentedwith a novel opportunil;y to elimill;tte or substan-tially reduce the supertag assignnmnt ambigu-ity by using local information such a.s local lex-ical dependencies, prior to parsing.
As in stan-dard lmrt-ofslieech disambiguatioii, we can uselocal statistical iufortnatiot~ in the term o\[n-gt'anlmodels based Oil the distri l)ution of stiperl;ags hia I,'I'A(I liarsed corpus.
Moreover, since the sli-l)erta.gs elicode depemde, ncy hfl'ornlal;k)n~ we canalso use informa.tion about the distribution of dis-tances between a, given superi;ag and its depen-dent su perl;ags.Note that its ill sta,ndard part-of-speech disaun-biguation, superl;ag disambiguation could havebeen done by a parser, lloweveG carrying outpart-of-speech disaml)igua.tion prior to pamsinglnMces the job o1' the.
parser much easier andtherefore, speeds it np.
Stlpertag disalnl)igua-tion a.s proposed in this paper reduces the workof the parser even further.
After snpertag dis-ainbiguation, we have effectively completed the1,54parse *rod the p~u'ser need 'only'  (:omhine the indi-vi(hlM structures; hence the term--ahnost  parsing.This method can a.lso be used go i)~v'se senten(:efragments in cases where the snpertag sequenceafter the disambiguat ion may not combine into :Lsingle structure.The ma.in tom of this paper is to presentte.chniques for dis~unbiguating Sul)erta.gs , and to(wahu~te their pe\]'formm~ce a.nd their impa.ct onI;I'AG parsing.
Although presented with resl)ectto Ill'A(',, these techniques are a l)plica.bD to lex-icalized gl'aitltlla.rs ill generM.
Section 2 I)rovi(h,~~m introduct ion to l,exi('.~dized '\['ree AdjoiningGr~mlmaa's.
The objective of supertag (lisa.m-1)iguation is i l lustrated through an example inSection 3.
Section 4 l)rielly deseril)es the sys-tem used to collect the data, needed for Sul)ert~tgdisambiguation.
Various methods and their i)er-formance results for superta.g (tisambigua.tion arediscussed in (let~dl in Section 5.Lexica l ized 'lS'ee Ad jo in ingGrammarsl ,exicalized '.lYee Adjoining (\]r~mmla~r (I:t'AC) is~ lexicMized tree rewriting grammar lbrm~dism.The primary structures of ILFAG ~u'e I'~LI,;MI,;N-TALLY 'PII.FI,IS.
l'~wh elementary tree has a lexi-ea.l item (a,nchor) on its fi'ontier and l)rovides a,nextended (lomain of /ocMity over which the au-('hor specifies syntactic a.nd semantic (pre(lica.teargument)  constra.ints, l'~lementary trees a.reof two ldnds: INITIAl, TRI,H,:S a~rtd AUXILIARYTI/I,~,:s. Examples of initial trees (~ts) ~ul(I a.uxil-ia,ry trees (fls) are shown in I,'igure I.
Nodes (mthe frontier ofinit iM trees are, ma, rke(I a,s sul)stil, u-tion sites 1)y a ' J ' ,  while exa.ctly one node on thefl'ontier of an a.uxili~ry tree.
whose la, h(q m:~tchesthe hal}el of the root of the tree, is ula.rked as ;~foot node 1)y ~L ' , ' .
The other nodes on the fron-tier of an mlxiliary tree ~u'e marked as sul)stit, u-tion sites, lfl!A(l \['actors ecursion \['rom the sta, te-ment of the syntactic dependencies, l!
',lementarytrees (initiM and ~mxiliary) are the domain forspecifying dependencies.
Recursion in speciliedvia the auxili~u'y trees.
\];'Jementa.ry trees ~Lre com-l)ined by the Subst i tu t ion  and Ad junet ion  op-eri~tions.
Subst i tut ion inserts element;u'y tre.es a,tthe subst i tut ion odes of other elementa.ry trees.Adjunct ion inserts :mxili~ry trees into elemen-tary trees at the node whose la.bel in the same asthe rout lM)el of tile auxilia,ry tree.
An ~Ln exam-pie, the (:Oml)onent trees ( ~s, me, n,.% n.4, fls, (~s,n's), shown in Figure l c~m be combined to formthe sentence John saw a man with lhe telescope las follows:\[.2.3./t.n's substitutes at the NP0 node.
in (~.n,:~ sul)stitutes a.t the \])etl ) node in e~4, theres~llt of which is sul)stituted :~1: the NPlnode in r~2.~:~ substitutes :LI, the I)etl ) node in ~(;, theresult of which is sul)sl, ituted a,L the NI ) nodehl 138.The result of step (3) a.bove a.djoins to theVP node of the result of step (2).
Tim re-surfing pa,rse tree.
is shown in Figure 2(:t).The process of coral)thing tim elementm'y treesresulting in the I)a.rse of the seutel~ce is rel)re.-sented by the der iw l t ion  t ree ,  shown in li'ig-ure 2(I)).
The nodes of the deriwttion tree arethe tree names that a.re anchored by the ~Lppro-pria.te lexical item.
The c.omposition opera.lionis indica.ted by the nature of the a.rcs - (h~shedllne.
for sul)stitutiou :uLd bold line.
for a.
(ljunction,while the ~ul(h'ess of the operation is h~dica.ted aspart of the node label.
The deriwLtion tree ea, na.lso I)e iuterpre.ted ,~s :~ dependency gra.l)h withunhd)eled a.rcs I)etweell words o\[" the.
sentence asshown in Figure 2((').We will ca.ll the elementa.ry structures asso-ei:tted with ea.ch lexi(:a.I item a.s super l)a.rts-of -speech (super I 'OS) or super tngs .3 Example of SupertaggingAn a. result o\[' locafization ht I:I'A(I, a. lexica.I itemmay I)e assoch~.l.ed with more tha.n one SUl)ert~g.The eXaml)le hi I,'igure 3 ilhlstr;Ltes the iniLia.l seto\[" su pertags assigned to each word of the sentence,\]olz,* saw a mmz wilh lhe lclescope.
The ordc'rof the superta,gs for e;Lch lexica.l item in the ex-aml)leis not signili(:ant.
Figure 3 Msoshows the\[i na, l SUl)e.rtag se(llnmce a.ssigned by the su pe.rt~Lg-ger, which picl(s the best supertag sequence llSil'lgsta.tistica.l inforlna,tion (descril)e.d in Section el)~dmug hMividual superta.gs aim theh' dependen-cies on other supertags.
The chosen SUl)ert~Lgsnre (:ombltled to derive a.
\[)axse., as exl)h).ined ill~The parse with tit(: PP ~tl.Lached to the NP has not\])(!ell .
'~}lOWll.155NPl )e tP$  NIJ ohns,/NNP~ VPv NFAIs~wI)etP NP  nPNI}" I'I'I )  l )etP l N nx /XI' NI~ I I~I Ill;HI ~l lhOiL Oi2 (~3 (~',I f~lAnl~l ve n /Nv nlsn~John  \[ i0:' 8 0 '  9Rib vl'n ~* n~ A/ \V NI'ILIJoh l l  uwDetPI)lhe(/' 5DeI P N P vlt 1)et P,&;& I1 llt~ N I) ll~l,P NI~ 1 i ~ III IIIIIII with theOi l0  (~'11 f~8 (X'I2DeIP r 1~ s,I'P N"I )  I )e lP\]  + I%1 / kI\[l Ill\[Ill xlllhI )e lP  rI) De lPpItheNPI )e IP$  NItelescope?.V(;NI'NItelescope/Xn N;*telescope#2 ,-~r ,qa #,, #~ #,~ firNAPigm'e I: \ ] , ' l ementary  t rees  of  LTAGN Vl' pl,I ~ l.l,.
v nl' P ~eI /~  Imw \[hip N +llh lkll' NI I 1 I II m~ II td?~.p+I I .
.
(~)P:u'se Tree,, 2lsa~q~l ~l\[.hdmJ (11 ILSIwIIM (21 a 4lnuml (2,21~' ~l e~r- lW\] (221 n Sial(I).
5li e l i  )(11)l)eriw~.tion Treel?igure 2: S t ructm'es  of  LTAGsaw.lldm MIh humI Itelesclqk' aIthe(,.
)l)eI>en dell cy (,~ a,p hSentence: ,lohn s~txv a man with.
1;1lo telescope.initia,l Supertag set: (~1(I' 8 (~.
*~ Oim (vii fls OiJ2 (~r~Oir ~a:, fl,, fls fl,; firF'inM Assignment: Oi8 Oi'2 rv:~ (v,i f18 Ois o:(;Pigure 3: Super tag  Ass ignment  for , loire saw ama' .
,  willz Iltc /eles(:Ol~e156Section 2.Without  the superta.gger, the \[)au'ser wouhl ha,veto process combinatioi ,  s o\[ tlte entire set o\[' tree+s(28) ;  w i th  it the  parser  must  on ly  i ) rocesses con>b inat ions  of  7 trees.4 Data  Co l lec t ionThe (t+~t:+ re(luired for disantbigtta.tine; superta.gs(discussed in Section 5) luwe I)een colle<:l:ed I)yl)a,rsing the Wa.ll Street ,Iottrna+.l '2, l l~IVl-tnautudand ATIS {:ori)ot':t using the wide+coveraxe I%-g l ish gr:-tmma,r being (l(weloped a.s part of theX'FA(I systeni (1)or;m eL.
a\]., 1994).
The pa.rsesgene.ra,ted by the system for these sentett(:es Iromthe corpora, ;tl'(.'
llot subjecte<l to :tny 1,:iu<l or lil-tering or selection.
All the deriw~tion structuresare used in the collection o1' the sta+tistics.4.1.
About  XTAGX'IJAC,' is a, large ongoing proje(:t to develop a,wide-c.overage gra.mma.r f,.
)r l",nglish, l)ased ()n tit,:;\];\[ 'AG form+dism.
It a.lso serves +Ls a.tl \]'\['A(',grammlar develolmteut system and consists of a.predictive left-to-r ight i)a.rser, au X-wht(low interface, ~ rnorphologicad a+na.lyzer a,nd a. part-of-speec\]l tagger.
The wide-(:overa.,e.
;e English grammar of the XTA(\] system contains :tl7,000 in-tleeted iLetns in the morl)hology (2 13,0(1(\] of these~Lre nouns ~tnd 46,500 are verbs) a.nd 37,000 (mtries in tit(', syntact ic  lexicon.
The syntactic lex+i(:on a, ssocia, tes words with the trees i;ha.i; theyanchor.
'Fhere ;u'e 385 trees itI a.ll, in a, gra.tnln+Lrwhich is compose.d of 40 (li\[l'erent sul)c~d, egoriza+tion I'rantes.
lC, au:h word iu the syt,tactic lexi('+m,on the +w(,,rage, depending on the st~utda.t'd lm.rts -of-speech of the word, is an a.nchor l'or a.boutto d0 element;try trees.Mode ls ,  Exper iments  andResu l t sThe SUl:)ert~Lg statist ics which h+a.ve been tts,:'din the prellndna.ry exper iments descrihed I)elowh~we been toilet:ted from the XTAG parsed cor-pora.
The deriwU:ion structur('s resulting rromi)a.rsed corpor~ (W~dl Street .JournaL1, for the +~xperiments descril)ed here) serve as tr~h~ing da.ta.for these experiments.
'2Sentences of hmgth < 15 words5.
:1 Unigram mode lOne met.hod o\[' disanil,igua, ting i.h,:' sup(n'l,a.e;s as-signed to e;tch word iX to or(ler the Stll)el't:-i,gs bythe lexic~d l)rel'erence tluLt the word ha.s (+or thelll.The l'r(Xluency with which a. certaiu supertag isassociated with a+ word is a, (lire(:t inca.sure of ii:~lexica.i lU'(d'eren(:e R+r tha.1, SUl)(wta.g.
Associatingfre(luench>s with the superta.gs a.nd ttsin,e; thelnto ;ISSO<'i;Lt(+ ;t. l)a.rli(:ula.r Sttl)erta.g with a. wordis clearly the simplest inca.its ',)1' (lisa.utl)i+vguatin KSll \])('t't:-t.gs.
Thus,S,~I,ot.L;~.~,;(,,,~) = ta :) a.,'p;n,a?,,: t,ip;t';,.m(t\]... 1 ""J.5.1.\] Ex l )e r iments  and  Resu l tsOwing to sl)a.rs+'lmss o\[' (la.ta., we ha.w+ I)a.
(:ke(l-c~ffI'rom word /super tag  pa, it's to i)a.rt-o\[ -sl)eech/st,l)erta.g pa.irs, i.e., collected the unigramI're(ll,eucies or superl.a./~;s as;so('.i~ted with the pa.rt-or-speech :~+ssigned to words instea.d or the wordsthemselves.
Ta.ble l illusl.r;ttes the na+ture o\[ 't lmstatistics used, with n. rew sa.ml)le entries.\[\]Tiu'(~)iCTt~e(~-I~- (SUlwrtag , u,ligram i)rol)al)ilitv)I NL - v .
.
( .> ,I I) ((~.~, 0,9(13)T:d)le 1 : ,qa.nl l)h, en t, ries of u uigra.m d a.ta.I)a+se'l'al)h' 2:Model'l'op n SUl)erta.gs % Success" - .
, i  =-:~ .
.
.
.
2~-cX7- +.
;TY-~+ - - F : , ~II('+~;t,ll+~ \['r<mt the \[luigt'a.nl +qUl)(,rt:t.piTim w()r<ls u.re first a,,~;,~dgu(,(I stauda, rd i>arL>of-speech usint, ~ ~ couventioual  ta,gger (Churd l ,l!)gg).
Then the set o\[' Sul)ertags a.sso(qated withca.oh word is retrieved rroln XTAC,'s synta.ctic(lata.bn.se.
'l 'hese sul)erta.gs a.re ordered ha.sed .
:)ntheir u ni,<,;ra.m rr<~(lUeUCy , a.n(I the top n Sul)erta.gsa.re a.ssocia.ted with th(, word.
'r~Lble 2 suntm;>rizes the success l){,rcenti~g~e on a, held out testset or 100 Wall Street ,lottrna.l SelltelH'A~8~ .
:IS 11 iSvaried, lr a, sentence p;u'ses using the n sllperta.gssele(:ted for mL(:h wor(I then the a.ssigument is cou-si(lered a, success.The unigt'a.tn superta.gger tha+t selects Ix) l) threeSul)ertags has l)een interl'aced wiLh X' \ ] 'A( : .
This157(I'.O.S,Supertag)(D,<~)(N,~s)(N,,~,)(V,o,2)Direction ofDependentSupertag(-)( - ,  +)( - ,  +)\])ependentSupertag(Y3('ggTable 3: I)ependeney DataOrdinalposition Prob- 1 C)..()99- I C).3001 (L374speeds the runtime of the parser by 87% on theaverage, whenever the snpertagger succeeds.5.2 n -gram mode lIn a unigram model a word is always associatedwith the supertag that is most preferred by theword, irrespective of the context in which theword appears.
An Mternate method that is sen-sitive to context is the n-gram model.
The n-gram model takes into account he eontextuaJ de-pendency probabilities between supertags withina window of n words in associating supertagswith words.
Thus the most prob~tble supertagsequence for a N word sentence is given byY' = argmaxr Pr(T~,5'~,...,TN) *Pr(I'VI,I'V2,...,WN IT~/&,.
.
.
,7~)To compute this using only local information,we approximate, taking the I)robM)ility of a wordto depend only on its supertagPr(W1,W2,.. .
,WN IT, ,T2,.
.
.
,7~)l-I Y_-, Pr(l+~,' I ~1~)and also use an n-gram (trigram, in this case)approximationP"OL-'&,.
.
.
,TN) ~ F\[~, P"('/~ I "L-~, '/t~-I)5.2.1 Exper iments  and Resu l tsA trigram model has been used to model thecontextual dei)endencies in supertag sequences.Again, due to sparseness of (hint, the particu-lar words have been ignored and the training ofthe tr igram model has been done on the part-of-speech/supertag pair.
The model has been testedon the same set of held out sentences as in theunigram experiment.
The percentage success is68%, i.e., 68% of the words of the test corpuswere assigned the correct sui)ertag.5.3 Dependency  mode lhi the n-gram model lot (lis~unbiguating su-pertags, dependencies t)etween supertags thatappear beyond the n word window ea, nnot be in-corporated into the mode.1.
This l imitation canbe overcome, if no a priori bound is set on the sizeo\[" the window but instead a prol)ability distril)u-tion of the distanee.s o\[' the <lel)endent supertagsfor each supertag is ma.intained.
A supertag isdependent on another supert~g i\[' the former sul)-stitutes or adjoins into tit(.'
latter "~.5.3.1 Exper iments  and l / ,esultsTable 3 shows the data required for the depen-dency model of supertag disambigua.tion.
Ide-ally each entry would be in(lexed by a (word, su-i)ertag) pair I)ut, due to si)arseness o\[' (lata, wehave backed-off to a.
(I)()S, supertag) pa.ir, l'3a(:hentry contains the following information.?
POS and Supertag p~dr.IJst ol' + aml - ,  representing the (lirectioll ofthe (h, peIM(mt superta,gs with resl)e(:t to theindexed supe.rtag.
(Size of this list iiMicatesthe total number of dependeltt SUl)e,'ta.gs re-quired.)?
l)ependent supertag.Signed numl)er representhig the directiona.nd the ordinal position of the l)a.rticul;u'dependent SUl)e.rtag mentioned in the entryfrom the position (ff the indexed su\[)ertag.aWe are computing dependencies between words withrespect o supertags associated with the words, althoughthe complete structure of the supcrtags i not used.
It is ofinterest o COml)~U:e our work with some other dependency-based appro~ches as described by, for example, Sle~tor(Sleator and Teml)erley, 1990), l\[indle (llindle, \] 993), Mil-ward (M ilward, 1!)!)2).158?
A probal)ility of occnrrence of such :t (lepen-dency.
The sum probabil ity over all the dependent  super tags  at all ord inal  pos i t ions  inthe same d i rect ion  is one.For example, the fourth entry in the T:d)le ;Ireads that the tree (~2, a.nehored 1)y a verl) (V ) ,has a left and a right dependent ( - ,  +) and thetirst word to the left ( -1 ) ,  with the 1;ree.
(~s, isdependent on the current word.
The strength ofthis association is rel)resented by the i)robal)ility0.300.The dependency model of (lisaunl)iguationworks as fol lows.
Stil)l)ose (~'2 iS a, llleiillie.r of tile'set of super(ass associa.te(l with :t word a.t posities n in the senten(:e. The :d<e;orithul proceedsto slttisfy the depende.ncy req'<lh'e.ment of <t,2 I)ypieldng up the dependency entries for e:t<:h (>\[ thedirections.
It picks a, del)en<lency dai, at entry (thefourth entry, say) from the (hmd):tse that is in-dexed by a2 all(I proceeds to sol; i1 l) at pa.tll withtile first word to tile left that has the (lepe.ndentsupertag ((~8) a.s a ineml)er (!\[' its set o\[" sul)erLa.gs.If the first word to  the left th~tt ha,s (h~ as ac lneu>ber of its set of super(ass is a.t l)ositiou m, t,111!1i a.IIarc is set up 1)etwee.n c~,2 and (~s.
Also, the arc isverified not to kite-string-tangle/i with auly otheri~l'(:s in the path up to e~2.
The i);ttll prol)M)ilityup to a2 is incremented by log 0.300 to reflect thesuccess of the ma, tch.
The l)atth probad)ility uI) to(Is incorporates the nnigra!n probability of (vs.On the other hand, if  no word is found 1,\[llti; \]la.sa8 as ;~ member of its set of supertags then theentry is ignored.
The a\]gorit\]inl mltkes a greedyelloice t)y selecting the path wit\]/ the ill;i.xil/lllIllpath probabilii, y to extend to the reimdniug di-rections i l l  t i le  ( \ ]e lml l ( le l lcy  l ist.
A SllCl'l,Ss\[ul Sllper (as  seqllen(;e is one which ;~l,SSit~llS it Sllp(!l't;I.gto (.
'itch l)osition such that eau:h supertag \]His all()fits dependents an(1 ma?hnizes the accunlula.i.edpath l)rob~d)ility.
It is to lie noted tllatl, tile algo-rithm when pairing l, he head itll(l its del)endentis not really parsing since it does so evell withoutlooking at tim strllctllre o~" the striilg~ l)etween thehead and the del)endent.The implementation and testing of this Ill()(l(,Iof sl ipertag (lis~mbiguation is underway.
Ta.1)le dshows preliminary results on the same held outtest set of 100 Wall Street Jollrlla\] seiitelices thai:was used in the unigram and triRrain models.The table shows two nieastlres of eva.hlal, ioil.
Ill4'l'wo arcs (a,c) and (b,d) kite-string-tangle, if .. < b <c<dorb<a<d<c,the first, the dependency l ink measnre, the testseilteRces were indel)endently ha.n(l tagged withdependency links an(l tlien were used tO nla.tch1,he the lhlks output I)y the del)endency nlodel.The c:ohuni+s how tit(; total nunllJer el' clel)en-(lency liuks hi the lilmd tagged set, the nuiriberof nm.tched links output by this model and thei)el 'cellta.~e (-OlTeetlless.
The  second lllOaSlll'e~ Sll-f)erta.gs, shows the tot:.1 null)her of cori'ect su-l)ertag, s assiDled to the words hi the COl'l)US t)ythis model.C,'it(.,.io,, I ,,U_@,'~ \[_ (,o,',',.c.t _1 <.o,.,.,~<?_1SUlierl'lgs \] 915_ ~__ 707 77 26% __' "2 '_ ~' ' _  .
.
.
.
.
.
.
.
.
~__~' " {'~'l'id)le .
'l: Results el" l)epeudency nlo(le{6 Conc lus ionLexica\[ized grammars :i,ssociate with each wordricher sgructllre~; (trees ill case ()\[' l' l 'A(',s and c~t-egories hi case o1' (Joml)hl~tI, ory Ca, l, egoriaJ (',I'\[LI\]t--I l l ; l l ' S  ((\](~(~S.))
OVeI' which tile wor(I specilles yn-t:t(:gic : l id  S(qll;i.lltiC collstrathlts.
I lence everyword is asso('ia.ted with ~t uluch la.rger set oflllOl'e COlll\[)\]ex stl'll('tlll'es \[,hail ill the ca,se wherethe words :.re associated with sta,nda.rd i)a.rts -olZsl)eech, llowever, these more complex de-scriptions alk)w more comple-~ coustraints to beimposed a.nd w,'ified locally on the coutexts inwhich these words a?pea.r.
This fea.ture of lexi-calized grammars can be taken a,dvantage of, tofurther reduce the (lisalnl)iguatioii task of theI)arser, as slmwll in SUlmri.ag disa.ml)igua.i.ion.Ileu(:e sui)el'Da,g ~ (lisai, nll)igua(,ioli (;a,l/ Im use(I :t~';a. g;enera.I i)re-i)a.rsing (:olnl)oneut o\[' lexicalized~rl'all) Illal' pa i'sels.The d(,gree of distiuct, ion l)etwe(m SUlml'(.a.g dis-aml)igua.tion a.n(I i/arsing va.ries, depen(ling onthe.
lexicalized g;ranima.r be(us (:onsi(M'ed.
l,'orboth I/I'A(', an(I C'CG, supertag disaml)igui~tionserves as a, preq)arser filter i;tutt effectively we.edsOil( iila, l)l)rol)ria, te eIelIl(':llta, ry stl ' i l( 'tures (tre.es orcategories) givenl the c(mtext of the sentence.
Italso in(liea.tes the dopenden('ies alnoi~g the ele-mentary stru('tlu'es but not tim spe('ific el)era.tiesto lie used l,o coral)(he the strllctul/es or tim it(I-dress a.t which the el)era.ties is to be l)erformed"a.ll ahliost parse", l if c'ases where 1,1l(; SUl)ertagsequelice \[Tir the ~iW.~li hil)ut stri l ig c:l, l l i lot lie159combined to form a complete structure, the "at-most parse" may mdee(i be the best one can do.In case of LTAG, even though no exl)licitsubstitutions or adjunctions are shown, the de-pendencies among LTAG trees uniquely iden-tify the combining operation between the treesand the node at which the operation (:an beperformed is almost always unique s. Thus su-pertag disambiguation is almost parsing lbr UI'-AGs.
In contrast, the dependencies among theCCG categories do not result in directly identi-fying the combining operations between the cate-gories since two categories can often be corn I)inedin more than one way.
Hence for CCG fiu'therprocessing needs to be performed to obtain thecomplete parse of the sentence, although withoutany supertag ambiguities.The supertag disaml)iguation, dependencymodel in particular, is even closer to p~wsing independency grammar formalism, l)ependencyparsers establish relationships among words, un-like the phrase-structure parsers which constructa phrase-structure tree spanning the words ofthe input.
Since LTAGs are lexicalized andeach elementary tree is associated will, a.t leastone lexical item, the supertag disaml)iguationfor EPAG can therefore be viewed as establish-ing the relationship a among words as depen-dency parsers do.
Then the elementary stru(>tures that the related words anchor are combinedto reconstruct he phrase-structure tree similarto the result of phrase-structure parsers.
Th,sthe interplay of both dependency ,~nd phrase-structure grammars can be seen in U\['AGs.
Ram-bow and Joshi (R, ambow and Joshi, 1993) dis-cuss in greater detail the use of LTAC, in reh~tingdei)endency analyses to phrase-structure analy-ses and I)rOl)OSe a dei)endency-I)ased l)arser for a,phrase-structure based grammar.In summary, we have presented a new tech-nique that performs the disambiguation of su-pertags using local intbrmation such as lexi('alpreference and local lexical dependencies.
Thistechnique, like part-of-speech disambigua.tlon, ro.-duces the disambiguation task that needs to beSin some cases, the dependency information betweenan auxiliary and an elementary tree may be insufficient ouniquely identify the address of adjunction, if the auxiliarytree can adjoin to more than one node in the elementarytree, since the specific attachments are not shown.6The relational abels between two words it, L'I'AG isassociated with the address of the operation between thetrees that the words anchor.done 1)y the parser.
After the disa.nd)iguation,we have effectively comi)leted the parse of thesentence ~md the parser needs %nly' to coml)letethe ~djunction and substitutions.
This methodcan also serve to parse Selltetlce \['ra~lfleuts illcases where the supertag sequence after the dis-ambiguation may not contbine to form a singlestructure.
We have implemented this techniqueof disambiguation using the n-gram models usingthe prol)ability data collected from LTAG I)arsedcorpus.
The similarity between l i lAC  and l)e-pendency grammars is exploited in the (lepen-dency mo(M of supertag disambigm~tion.
Theper\['ormance r sults of these models have beenpresented.ReferencesChurch, K. (1988).
A Stochastic Pari;s I)rogram andNoun Phrase Parser for Unrestri('i;ed TexL In 2~ldApplied Natural Language Processing ConJ'cre~tcc1988.Doran, C., l'\]gedi, D., l\[ockey, B.A.
and Srinivas, B.(1994).
XTAG 7'ec&/cal Report.
I)ep~rtrnent o\['Computer a.ml hdbrmation Sciences, University orlhmnsylwmia, l)hihuh!lldli~t, PA.
In In'ogressIlindle, D. (1993).
I'rediction of Lexic~dized TreeFragments in 'I'ex~; ARPA Workshop on \[\[um~ml,anguage Technology, March 11993.Milward, D. (1992).
Dynamics, Dependency Gram-mars and Incremental \[nterpreta.tion.
In Pro-ceedings of Ihe 14 th International Confe'ccncc onComlrutalional Linguistics (COLINC'92), Nantes,France, August.1Lambow, O. and Joshi, A.K.
(1993).
1)epen-(h'ncy Parsing for I)hrase-Structure (\]rammars.Man'usc'cipt, U,liversil,y of I'(mnsylv:mia..qh,ator, I).
a.n(1 'l'elnp,wh'y, I).
(1991).
ParsingI';nglish wil.h a laid{ (h'muinar.
7}~chnical 'reportCMU-C',q'-91-196, Deparl.ment of Compul,er Sci-ence, Carnegie Mellon Uldversity, 1991.Schabes, Y., AIMII6 A. and Joshi, A.K.
(t988).
Pro's-ing strat;egies with 'lexicalized' grammars: Appli-c;d, ion to tree adjoining rammars.
In P'.,'oceedinysof the 12 th Inlernalional CoT@:rence on Comp'ula-lional Linguislics (COLINC'88), BudN)est , Ilun-gary, August.Schal)es, Y.
(\]9q()).
Malhcmalical and Comp.ula-lional Aspects of Lczicalizcd Grammars.
Ph.I).
I, he-sis, University of Pe,msylva.nia, l'hiladelphia, PA,August.
Ava, ilal)le as technicM rel)ort (MS-CIS-90-48, LINC LA\]~,I79) from the l)elmrl,menl; of (\]om-lmter and htI'ormation S(:icnce.160
