Automatic Fine-GrainedSemantic Classification forDomain AdaptationMaria LiakataUniversity of Wales, Aberystwyth (UK)email: mal@aber.ac.ukStephen PulmanUniversity of Oxford (UK)email: sgp@clg.ox.ac.ukAbstractAssigning arguments of verbs to different semantic classes (?semantictyping?
), or alternatively, checking the ?selectional restrictions?
of predi-cates, is a fundamental component of many natural language processingtasks.
However, a common experience has been that general purpose se-mantic classes, such as those encoded in resources like WordNet, or hand-crafted subject-specific ontologies, are seldom quite right when it comesto analysing texts from a particular domain.
In this paper we describe amethod of automatically deriving fine-grained, domain-specific semanticclasses of arguments while simultaneously clustering verbs into semanti-cally meaningful groups: the first step in verb sense induction.
We showthat in a small pilot study on new examples from the same domain we areable to achieve almost perfect recall and reasonably high precision in thesemantic typing of verb arguments in these texts.139140 Liakata and Pulman1 IntroductionSince the earliest days of computational linguistics the semantic properties of verbalarguments have played an important role in processing.
Many classic types of am-biguity, and hence their resolution, depend on this: ?flying planes can be dangerous?is ambiguous because ?flying planes?
can describe an activity or a plural entity, ei-ther of which can be a semantically appropriate subject of ?be dangerous?, whereas?swallowing apples can be dangerous?
does not display this ambiguity.
Both ?fly?
and?swallow?
can be transitive or intransitive, but whereas ?planes?
is both a semanticallyappropriate subject for intransitive ?fly?
and an appropriate object for transitive ?fly?,?apples?
is not a semantically appropiate subject for intransitive ?swallow?.
Seman-tic (mis)typing rules out this syntactically valid combination.
Similarly, an importantcomponent of reference resolution is the knowledge of what semantic category an en-tity falls under.
For example, in ?The crop can be used to produce ethanol.
This canbe used to power trucks or cars?, knowledge that ethanol is the kind of thing that canbe subject of ?power?, whereas ?crop?
is not, is required to successfully resolve thereference of ?this?.When considering division into semantic categories one?s immediate thought wouldbe to take advantage of existing semantic resources (such as WordNet (Miller, 1995))or FrameNet (Baker et al, 1998).
For example, Clark and Weir (2002) calculate theprobability of a noun sense appearing as a particular argument by using WordNet togeneralise over the noun sense.
However, even though WordNet has been extremelyuseful in numerous applications, many researchers have found that the fact that it islargely developed via the intuitions of lexicographers, rather than being empiricallybased, means that the semantic information often is poorly matched with word usagein a particular domain.
Pantel and Lin (2002) and Phillips and Riloff (2002) havepointed out that WordNet often includes many rare senses while missing out domain-specific senses and terminology.
Some authors, Kilgariff (1997) and Hanks and Puste-jovsky (2004), among others, reject the basic idea shared by WordNet and FrameNet(as well as traditional dictionaries) that there is a fixed list of senses for many verbs,arguing that individual senses will often be domain specific and should be discoveredempirically by examining the syntactic and semantic contexts they occur in.
We arehighly sympathetic to this view and in this work we assume, as Hanks and Pustejovskydo, that rather than relying on the intuitions of a lexicographer, it is better to try to in-duce verb senses and semantic types automatically from data drawn from the domainof interest.In this paper we report on some experiments in learning semantic classes.
We carryout prior syntactic and semantic analysis of a relevant corpus so that verb+argumentpairs can be identified.
Since we are interested in domain specific semantic classifi-cation we make the ?one sense per corpus?
hypothesis and ignore word sense disam-biguation.
For a given verb, we find the head nouns occurring in the subject, object andindirect object noun phrases (where they exist) occurring frequently within the corpus.Now that we have information about nouns co-occurring in different argument slotsof verbs we cluster the verbs according to shared argument slots: verbs which have anargument slot (not necessarily the same one) occupied by members of the same clusterare in turn clustered together.
The effect of this is to derive noun clusters characteris-ing the semantic types of the argument slots for individual verbs (learning selectionalAutomatic Fine-Grained Semantic Classification for Domain Adaptation 141restrictions) while simultaneously clustering verbs which have similar argument slots.In the case where the same argument slot is involved across verbs, the effect of thisis to induce a fine-grained semantic classification of verbs (the first step in learningverb senses).
Where different argument slots are involved the effect is to suggest morecomplex causal or inferential relations between groups of verbs.To give a simple illustration, if admit, deny, suspect all take the word ?wrongdoing?as their object, then admit_arg2, deny_arg2, suspect_arg21 are clustered togetherinto one group A.
If we also find that words like ?oversight?
also appear frequently inthe same argument position with roughly the same set of verbs, then ?oversight?
willbe clustered with the other fillers of group A.A side-effect of the process is a classification of the verbs as well: if admit_arg1,deny_arg1 and admit_arg2, deny_arg2 respectively take the same values, ?deny?
and?admit?
are clustered together.
We may also note that the same classes occur in differ-ent argument slots of different verbs: in Liakata and Pulman (2004) we showed howthis could lead to the discovery of causal relations specific to a domain: for example(in company succession events), that A succeeds B if B resigns from position C andA is appointed to C.In the remainder of the paper we describe this clustering process in more detail.We also describe a simple pilot evaluation, by taking two unseen texts from the samedomain, and observing to what extent the semantic groupings arrived at can be usedto assign semantic types to arguments of verbs.
We were pleasantly surprised to findalmost perfect recall, and respectable precision figures.2 MethodThe method of clustering together verb argument slots for obtaining domain specificverb senses (either in terms of verb classes or through the assignment of semantictypes to the verb arguments) is applied as a proof of concept to the domain of finan-cial news.
We chose this domain since the WSJ section of the Penn Treebank II isalready available in the form of predicate-argument structures, obtained according tothe method described in Liakata and Pulman (2002).
However, the same approach canapply to predicate-arg structures from non-treebank data such as the QLFs derivedfrom LFG structures in Cahill et al (2003) or semantic representations such as in Boset al (2004).The WSJ corpus consists of 2,454 articles with a total of 2,798 distinct verb pred-icates, 62 prepositional predicates and 221 copular predicates containing the verb to?be?.
Here we are only dealing with the non-prepositional predicates.
The latter fol-low an uneven distribution of occurrences; there is a minority of very frequent verbswhereas the majority are rather sparse.
The problem with infrequent predicates isthat the number of instances is often too small to allow for meaningful clusteringof the verb-argument slots.
To circumvent this, we pre-process predicates with lowfrequencies (freq < 5) by looking them up in WordNet to find the conceptual group(synset) to which they belong and assigning to them the frequency of the member ofthe synset with the greatest count of occurrences in the corpus.
Thus, words featuringas arguments of the most frequent synset member are counted as arguments of the1arg1 is subject; arg2 is direct object; arg3 is indirect object, roughly142 Liakata and Pulmanless frequent semantically related predicate, so that the latter receives a count boost.For example, the words that appear as subjects of the verb ?hit?
are also consideredsubjects of the verb ?clobber?, which belongs to the same synset as ?hit?
but is under-represented in the corpus.
This is making use of the knowledge that semanticallysimilar verbs are similar in terms of subcategorisation (Korhonen and Preiss, 2003)and is in agreement with the approach in Briscoe and Carroll (1997) where the subcat-egorisation frames (SCFs) of representative verbs are merged together to form SCFsof the rest of the verbs belonging to the same semantic class.
We understand that theabove process may be indirectly adding false positives to the verb senses.
It wouldbe interesting in the future to examine the trade-off between boosting the counts ofinfrequent verbs and the addition of false positives.A second pre-processing stage was applied to the arguments of the 2,798 verb pred-icates.
The idea underlying this process was to create a version of the predicates whereobvious semantic groupingwould have already taken place.
This involvedmerging to-gether the instances of ?named entity?
classes: person names, companies, locations,propositions, money expressions, and percentage and numeric expressions.
Companynames and suffixes, locations and people?s first names are contained in a gazetteer listcollected from internet resources.Since the similarity of argument slots of predicates is to be determined by howmany common fillers they share, it is natural to use the Vector Space Model (VSM)originally from Information Retrieval to define similarities.
In IR documents contain-ing the same words are considered to be similar.
Argument slots of predicates can becharacterised by their filler words in the same way that a document is characterised bythe words it contains.
This means a predicate argument slot can be modelled in termsof a vector of filler-word frequencies.In order to apply clustering methods to the predicate-arguments, we combined theminto a matrix, where each row corresponds to a verb-argument slot (verb-subj/arg1,verb-obj/arg2 or verb-iobj/arg3) and the columns correspond to words-fillers of theverb-argument slots.
Each cell ?wi j?
in the matrix represents the frequency of word ?
j?as a filler of predicate argument slot ?i?.
However, even after the first step of group-ing together named entities of the same type (as described above) there were 32,990distinct possible fillers of the three argument positions of the 2,798 verb predicates.By including all possible word fillers as columns, we would end up with a very sparsematrix of 2,798 ?
3= 8,394 rows and 32,990 columns.To reduce the size of the matrix it was essential to select a small number of words asrepresentatives of the argument fillers.
Even though the literature for feature selectionis vast when it comes to supervised machine learning methods, there is very little onfeature selection for clustering.
Principal Component Analysis (PCA) would be onealternative here as it reduces dimensionality while preserving as much of the variancein the high dimensionality space as possible.
However, since PCA does not considerclass separability information there is no guarantee that the direction of maximumvariance will contain good features for discrimination.
In this preliminary experimentwe decided simply to use the 100 most frequent words as features.
Thus the newmatrix is of the order 8,394 ?
100.Automatic Fine-Grained Semantic Classification for Domain Adaptation 1432.1 Clustering methodTo perform the clustering we chose a probabilistic clustering method which allowsinstances to belong to more than one class with different probabilities, as this givesa better indication of the quality of each class and agrees more with the intuitionthat there is more than one possibility for defining the groups.
In addition to this,we do not know what the expected number of classes is so ideally we would likethe clustering algorithm to predict the optimal number of classes.
For the previousreasons we decided to use Autoclass (Cheeseman and Stutz, 1995) which is a systemfor unsupervised classification, consisting of a classical mixture model enhanced by aBayesian method for determining the optimal classes.Autoclass is an extension of the mixture model as each instance can be charac-terised by multiple attributes instead of just one, so that the dataset is represented asa matrix of attribute values.
One need not specify the exact number of clusters sincethe system first performs a random classification which it then improves through localchanges.
Autoclass adopts a fully Bayesian approach by assuming a prior probabilitydistribution for each parameter.
In the current experiment the instances in each class(verb argument slots) were assumed to follow a log normal distribution.2.2 The verb argument clustersAutoclass performed over 500 trials to converge to various solutions with differingnumbers of clusters.
The most probable clustering, i.e.
the one with the highest logposterior, corresponds to 32 classes and was obtained after 200 trials.
The high numberof classes2returned may be due to overfitting of the model or may be a sign that thereis not a clear structure in the data itself.A class obtained from Autoclass is characterized by its class weight, the numberof verb-argument slots that constitute its members, as well as its class strength andclass cross entropy.
There are very heavy, populous classes (e.g.
class 0 with 5,571members) and lightweight, scantily populated classes (e.g.
class 31 with 34 mem-bers).
Class strengths are defined by the mean probability that any instance belongingto a class would have been generated by its probabilistic model.
The higher the classstrength the more meaningful the class.
The strongest class is class 0 followed byclass 2, the third most populous class.
Admittedly, the class strength for the rest ofthe classes is very small casting some doubt over the model?s predictive power withrespect to the data set.
Almost every instance is assigned to a class with probabil-ity 1, which means that the classes are clearly separated.
Class cross entropy, howstrongly the model helps differentiate each class from the whole dataset, ranges fromzero, for identical distributions, to infinite for distributions that make a complete sep-aration between differing values of the same attribute.
A class is more meaningful ifits distribution is distinct from the global distribution.
In this case class cross entropyhas a value of over 118 for every class, suggesting that classes are distinct from thedistribution of the data set.
Attributes with the most overall influence in classificationare the ones corresponding to precise concepts associated with specific contexts suchas ?spokeswoman?, ?reporter?, ?source?.
The least useful features for the classificationare the ones with the most scattered frequencies across predicates such as ?person?.Thus, one should employ frequent features with counts concentrated in a subset of2From here onwards we shall be using the terms ?class?
and ?cluster?
interchangeably144 Liakata and Pulmanpredicates and penalise the significance of words with counts distributed evenly in allpredicates.
This suggests that it would be more reliable to use freq*idf3 as a criterionfor feature selection.2.3 Interpretation of the clustersIn order to be able to interpret classes, class membership was inspected by processingthe class report showing which cases belong to a particular class.
Class 0 containsnearly all of the third arguments (indirect objects) of all verbs, which are usuallypropositions functioning as verb complements.
The interpretation of the resultingclasses is not straightforward, though about half have some intuitive basis once outliersare ignored.
For example, class 9 (below) seems to hold first arguments (subjects) ofverbs denoting sudden movement and numeric change such as:class(9,[?add_arg1?,?add_up_arg1?,?back_arg1?,?balloon_arg1?,?base_arg2?,?base_arg1?,?bear_arg1?,?begin_arg1?,?bestow_arg1?,?block_arg1?,?blossom_arg1?,?blow_arg1?,?blow_up_arg1?,?come_up_arg1?,?boost_arg2?,?bother_arg1?,?break_arg1?,?break_arg2?,?breathe_arg1?,?bring_in_arg1?,?bud_arg1?,?build_up_arg1?,?bump_up_arg1?,?clean_up_arg1?,?clear_arg1?,?climb_arg1?,?come_along_arg1?,?come_back_arg1?,?cut_arg2?,?come_down_arg2?,?come_on_arg1?,?boom_arg1?,?continue_arg1?,?contract_arg1?,?deal_arg1?,?contribute_arg1?,?count_arg1?,?count_arg2?,?crack_arg1?,?crash_arg1?,?come_down_arg1?
?cut_arg1?,?cut_down_arg1?,?contrast_arg2?,?decline_arg1?,?deduct_arg1?,?defend_arg2?,?deflate_arg1?, ?deflate_arg2?,?settle_arg1?,?set_off_arg1?,?shape_up_arg1?,?shine_arg1?,?shoot_arg1?,?shorten_arg1?,?sink_arg1?,?sit_arg1?,?sit_down_arg1?,?slip_arg1?,?slip_in_arg1?,?slump_arg1?,?soar_arg1?
]).A closer look at the filler words of the above verbs show that most of them are?financial indicators?
of some sort such as the following:CLIMB_arg1: share, asset, imports, exports, fund, price, rate, percentage, stock, wages,dollar, dividend, income, volume, market, capital, interest, trading, demand, maker, cost, index,new_bank_index.SINK_arg1: percentage, yield, stake, wages, stock, index, share, dollar, georgia_gulf_stock,money, company, bank, income, investment, dividend, payout, payroll.SOAR_arg1: earnings, asset, yield, location, exports, imports, purchase, fund, price, rate,number, wages, share, stock, rating, bid, dollar, interest, dividend, profit, income, volume, risk,holder.DROP_arg1: borrowing, imports, increase, market, share, investor, surge, capital, money,company, auction, firm, price, bank, limit, scale, holder, profit, dollar, performance, asset, stoc,rate, index, bid, earnings, volatility.3idf here is defined as id fi = |All pred-arg slots||pred-arg slots filled by i|Automatic Fine-Grained Semantic Classification for Domain Adaptation 145Some other classes containing verb arguments with a clear semantic relationship toeach other can be found in the following:Class 1 consists among others of the first arguments of:think, rethink, believe, know, consider, reconsider, understand, remember, respect, underesti-mate, value, view, visualize, respectClass 4 contains objects of verbs related to financial transactions and consumptionsuch as: buy, sell, calculate, acquire, afford, auction, buy_up, buy_out, cut_down, exchange,lose, begin, continue, feed, keep, maintain, market, obtain, regain, retain, trade, use.Class 4 also contains subjects of verbs such as:diminish, decrease, descend, crush, double, eat, eat_up, end, extend, fail, discharge, dismiss,dispatch, dissolve, distort, exhaust, launch, multiply, pay, plunge, profit, quadruple, shrink,spend, yield, triple.2.4 Semantic typing of verb argumentsClustering verb argument slots as described above leads both to the semantic groupingof verbs as well as the indirect semantic typing of the words that feature as argumentsto the verbs.
For the latter, details regarding membership of the 32 classes of verbargument positions were combined with information about which words appear inwhich slots, so that each term was assigned to the corresponding classes.
This in-evitably resulted in a word belonging to more than one class.
For example, the term?spokeswoman?
is a filler of verb argument slots in classes 6, 9, 7 and therefore be-longs to the homonymous classes.
However, its frequency in each class will differso that combined with the ipf4, the respective tf-ipfs give a better idea of how mean-ingful class membership is.
For example, the tf-ipfs for the term?spokeswoman?
are0.0075, 0.00275 and 0.0005 for classes 6, 9 and 7 respectively, making class 6 themost representative class for this word.By looking at the 15 highest ranking terms in each class, where rank is determinedby descending tf-ipf, we attempted to give labels to the 32 classes of verb arguments.The labels originated from the 3?4 terms with the highest frequency among the top 15words and are shown below:class label0 proposition1 company_organisation2 unspecified_someone3 proposition_truth_profit_patient_impact4 percentage_money_income_revenue_stock_share_asset5 percentage_mony_numXpression6 spokesman_company_person_analyst7 income_revenue_net_rate_cost_stock8 place_step_effect_loss_action9 proposition_company_spokesman_revenue_analyst10 proposition_stake_rate_percentage11 proposition_percentage_sure_decision_bid12 year_percentage_quarter_index13 reporter_dividend_money_percentage_analyst14 percentage_proposition_numXpression4ipf (inverse predicate frequency) is defined in the same way as idf146 Liakata and Pulman15 proposition16 percentage_stake_demand_money_rate_cash_capital17 proposition_projection_rate18 proposition_trading_pressure19 proposition_table_corner_board_tide20 proposition_percentage_public_private_high_low_numXpression21 government_civilian_unspecified22 proposition_unspecified_game_role_cash_company_agreement23 percentage_proposition_numXpression24 percentage_proposition_date_profit25 director_court_partner_company26 proposition_contract_profit_demand_requirement_proposal27 demand_problem_leak28 year_month_time29 proposition_money_percentage_share_stock30 year_time31 fund_proposal_investorAs can be seen from the above, obtaining a clean-cut label reflecting the meaningof the contents in each argument class is a non-trivial process; there seems to be asignificant amount of sense variance within a class and overlap between classes.2.5 Adding hierarchy to the semantic typingIn order to obtain a sense of the extent of overlap and similarity between classes, wecomputed a similarity matrix consisting of the pairwise similarities between each ofthe 32 classes where similarity between classes was defined in terms of the overlapcoefficient:sim(A,B) = |A?B|min(|A|,|B|)Where |A?B| is the number of words in both A,B and |A|,|B| are the number of wordsin classes A,B respectively.The overlap coefficient considers classes to be similar when one subsumes or nearlysubsumes the other.
Hierarchical clustering was performed on the the basis of theoverlap similarity matrix, using euclidean distance as the distance metric.
The resultis the cluster dendrogram below, which illustrates the relation between classes a lotmore clearly than a set of flat labels can and allows for a generalisation hierarchy ofthe senses reflected by the classes?
semantic types.Even though it is difficult to designate human-friendly labels to the classes thatrepresent their meaning in a straightforward manner, we will show that these classescan be used reliably to automatically assign semantic types to the arguments of verbs.First, we combined the information about class membership of verb argument slotsto create patterns of the form:ARG1 VERB1 (ARG2) (ARG3)As verb-argument slots were assigned to each class with probability 1 (see Section 2.2)and we made the ?one sense per corpus?
assumption, there is just one pattern for eachverb.
Thus, for example, the pattern for the verb ?report?
is the following:Automatic Fine-Grained Semantic Classification for Domain Adaptation 1473148911713611230292518141503510192172423162021222826270 1 2 3 4 5Cluster Dendrogramhclust (*, "complete")dist(matrixO)HeightFigure 1: Class dendrogram[1 report 4 (10)], which takes the following form when replacing class IDs with ten-tative semantic labels:[company_organisation] report[percentage_money_income_revenue_stock_share_asset][proposition_stake_rate_percentage]However, this does not mean that a person cannot be the 1st argument of report; thereis overlap between classes 1 and 6 (the major person class) and Figure 1 shows theyare closely linked.
Such proximity of classes is considered during pattern evaluation(Section 3).148 Liakata and PulmanThe patterns were stored in a MySQL database.
They are partly modelled on the?Corpus Pattern Analysis?
model described in Pustejovsky et al (2004).
These aresyntagmatic patterns representing a selection context for the predicate they include,which determines the sense of the latter although CPA Patterns as defined by Puste-jovsky et al (2004) and Rumshisky and Pustejovsky (2006) are in fact rather moredetailed than our patterns.3 Results and EvaluationTo evaluate the semantic types assigned by the automatically derived classes as well asthe transferability of the derived CPA-like patterns to unseen instances, we performeda pilot study where we applied the patterns to two randomly selected articles fromthe on-line versions of the WSJ and the FT from March 2008.
We believe this to bea useful test for the validity of the patterns since the new articles are guaranteed tobe distinct from the training WSJ data of the 90s, while still belonging to the samedomain.
We parsed the article using the CCG parser (Clark and Curran, 2007) andconcentrated on its RASP option (Briscoe et al, 1997) output, consisting of depen-dency relations.
Since our patterns concern the semantic typing of verb arguments,we focussed on the relations ncsubj (non-clausal subject), dobj (direct obj) and iobj(indirect obj) between a verb and the respective argument position.
We ignored er-roneous parses5 as well as copular predicates with the verb to ?be?, since the CCGparser?s dependency relations did not maintain the connection between ?be?
and theadjective or participle, making it clumsy to automatically link arguments in the waywe need to.We then followed the evaluation procedure below, where for each verb-argumentpair token in the evaluation set:1.
We looked for a pattern in the database matching the verb-argument relation andaugmented the count for recall if a match was found for the right verb.2.
We obtained the type (that is, the class ID) that the pattern assigns to the argu-ment filler word.
We then checked the latter in the database, to see which classesit belongs to as well as its freq, tf-idf for each class.
Determining which shouldbe the correct, gold standard class of a word given the 32 classes is very diffi-cult considering the class overlap.
Therefore, the three highest ranking classeswere taken as describing the correct semantic type for the word.
Here rank isdefined by looking at the 10 first classes where the term has the highest tf-Idfand returning the 3 of these with the highest frequency.3.
If the type assigned to the argument filler matches any of the 3 classes-semantictypes, we assumed the type assignment is correct.4.
If the type returned was not among the 3 correct semantic types, we lookedat the cluster dendrogram from the previous section and counted the distancebetween the correct and returned types.
If the correct type and the returned typeare in the same cluster at the same level, we count the distance as 1.
If we need5This can be justified by the fact that we are evaluating the patterns, not the system for producing thedependency relations for evaluationAutomatic Fine-Grained Semantic Classification for Domain Adaptation 149to go up a level from the returned type for them to be in the same cluster thedistance is 2, if two levels, the distance is 3.5.
Proceeded to the next verb-argument pair.To illustrate the assignment of semantic types through the application of the patternsand the ensuing evaluation procedure, we consider two example verb argument rela-tions from the WSJ text, namely ?dobj shows declines?
and ?ncsubj dropped indexes?.In the first case, we looked in the database for a pattern of the verb ?show?.
The match-ing pattern is ?
6 show 4 14?, which assigns semantic type 4 to the object of the verb,?declines?.
When looking up the noun ?decline?
in the database, the 3 types constitut-ing its correct semantic type are 9, 8, 4.
Since 4, the type allocated by the pattern isamong them, we consider this to have been the correct assignment of semantic type.For the second example, the pattern available in the database of the verb ?drop?
is ?9drop 8 28?, which means that the pattern assigns type 9 to the word ?index?.
However,when we look up the word ?index?
in the database, the correct semantic types for it are7,12,4.
We check in the cluster dendrogram to calculate the closest distance betweentype 9 and types 7,12,4 which is 2 steps, between classes 9 and 4.
The semantic typeassignment is therefore considered once more correct.There were 46 distinct verbs and 78 distinct verb-argument relations that met thecriteria for evaluation (out of 119 extracted predicate argument relations) in the WSJarticle.
For the FT article the corresponding numbers were 25 and 53 respectively (thelatter out of 129 predicate-argument relations).
The difference in these figures can bedue to the size of the articles (6,002 words for the WSJ as opposed to only 2,702 forthe FT one) as well as the preference for nominal predicates and nominalisations inthe FT article.A verb pattern existed for each of the verb-argument relations, which gave a per-fect recall, 78/78, 53/53 (100%).
This is gratifying since the patterns seem to coveradequately the financial domain, given that the test data come from two different news-papers.
When allowing a distance of up to 3 between the assigned and correct classesprecision was 60/78 (76.9%) for the WSJ and 33/53 (62.2%) for the FT article.
Forexample, in the predicate ?oversees Mac?, ?Mac?, which is a company, was allocatedto class 13 by the patterns whereas the correct class should have been one of 6, 9, 1.The distance between classes 13 and 6 is 3 steps, whereas ?company?
features in bothclasses with tf 0.0008 and 0.011 respectively.
The precision was reduced to 55/68(70.5%) and 30/53 (56.6%) if we only allowed up to 2 steps (e.g.
in ?index fell?
?in-dex?
was assigned to class 9 where its tf is 0.0014, as opposed to 4 where its tf is0.0016).
Precision fell further to 41/68 (53%) and 26/33 (49%) respectively for up to1 steps (e.g.
where in ?reported at 75?
the iobj ?75?
was classified as being in class 10(tf 0.0004) as opposed to 5 (tf 0.004).
For strictly exact matches, precision was 33/78(43%) for the WSJ and 21/53 (39.6%) for the FT (e.g.
?director?
in ?director said?being assigned to class 6 where the correct type is defined by classes 25,12,6).The results between the two articles are definitely comparable.
However, it is dif-ficult to tell whether the observed difference at the upper end is indeed statisticallysignificant and to what extend the difference between British English and US Englishplays a role here.
Nevertheless, even though the evaluation was only performed ona small scale, we consider the results to be at the very least, encouraging, since the150 Liakata and Pulmantexts we tested the patterns on were picked at random from the domain of financialnews.
The perfect recall would suggest that the verb patterns provide reasonably fullcoverage of the domain, while we can assign informative fine-grained semantic typesto arguments with a reasonable degree of precision.
Of course, a larger evaluationwould be desirable, as would some task-related measure of how much this semantictyping helps in accurate processing.
We hope to do this in future work.4 Related WorkThe literature on acquiring semantic classes of words is very extensive.
It is mostlymotivated by WSD and WSI where the aim is to discover or be able to differentiatebetween different senses of a target word.
Pereira et al (1993) describes a methodfor clustering words according to their distributions in particular syntactic contexts.Nouns for instance are classified according to their distribution as direct objects ofverbs, where it is assumed that the classification of verbs and nouns co-varies.
Inour approach we also make this assumption and nouns are clustered indirectly byfirst grouping together the verb argument slots they fill.
Clustering in both cases isprobabilistic with the assumptions that members of the same cluster follow similardistributions or in our case a joint distribution.Phillips and Riloff (2002) and Pantel and Lin (2002) also describe work on cluster-ing nouns to derive semantic classes.
Work more directly comparable to ours includesSchulte im Walde (2003, 2006) who presents a method for clustering German verbsby linguistically motivated feature selection.
Evaluation against a manually annotatedgold standard showed that syntactic subcategorisation features were most informativewhereas selectional preferences added noise to the clustering.
However, the authorconcludes that there is no perfect choice of verb features and that some verbs can bedistinguished on a coarse feature level while others require fine-grained information.Korhonen et al (2006) also use syntactically motivated features to cluster togetherverbs from the biomedical domain and in more recent work (Sun et al, 2008) showedthat rich syntactic information about both arguments and adjuncts of verbs constitutethe best performing feature set for verb clustering.Gamallo et al (2007) follow a similar approach to Pantel and Lin (2002) where aninitial set of specific clusters, containing manually chosen terms representative of thedomain as well as their lexicosyntactic contexts, are aggregated to form intermediateclusters to which hierarchical clustering is applied for further generalisation.
A veryinteresting aspect of this work is that concept-clusters have a dual nature, consistingboth of words-terms (extension) and their lexico-syntactic contexts (intension).
As isthe case in our approach, cluster formation is twofold, by grouping together wordsaccording to the contexts they appear in but also by clustering contexts based on thewords they share though this is mentioned as future work in Gamallo et al (2007).However, in earlier work Gamallo et al (2005) cluster together similar syntactic po-sitions in Portuguese derived automatically and each cluster represents a semanticcondition.
Words-fillers of the common position are used to extensionally define theparticular condition.
Clusters are formed in two stages, where first the similarity be-tween any two positions is calculated in terms of their common word fillers, the 20most similar ones for each position are aggreggated and the intersection of commonwords kept as features.
Next, basic clusters are agglomerated according to the amountAutomatic Fine-Grained Semantic Classification for Domain Adaptation 151of shared features.
The result is a lexicon of words with syntactico-semantic require-ments applied successfully to PP-attachment.The current work has a different agenda in that it aims to obtain semantic classes ofnouns that feature as verb arguments.
This information is combined to form selectioncontexts for verbs, similar to CPA patterns (Pustejovsky et al, 2004), which are thenevaluated on the assignment of semantic types.
However, whereas our patterns areobtained in a fully automated way, CPA patterns are acquired semi-automatically afterthe initial manual construction of core verb subcategorisation frames.5 Summary and Future WorkWe have presented a method for automatically acquiring domain-specific selectionalrestrictions for verbs in terms of semantic typing of their arguments.
This was achievedby clustering together verb argument slots sharing the same filler words after obtain-ing all predicate-argument relations in the corpus.
This also resulted in the semanticgrouping of nouns, which instantiate the verb arguments.
The clustering method usedwas Autoclass, an extension of the mixture model.
We combined the information fromthe clusters of nouns and verb-argument slots to create contextual verb patterns.
Thelatter were evaluated on a text chosen at random from the same domain and achievedperfect recall and reasonably high precision.As this pilot study showed that fine-grained domain-specific semantic patterns forverbs can be obtained automatically, we would like to port the approach to a do-main where fine-grained typing is of paramount importance.
This is the case with thebiomedical domain, where for instance verbs of biological interaction, such as inhibitor activate are semantically underspecified (Rumshisky et al, 2006; Korhonen et al,2006).
However, the specific biological interactions come only through the detailsof the actual arguments participating in the interaction (Rumshisky et al, 2006).
Wewould also like to experiment with different clustering methods and use more sophis-ticated linguistically motivated filters for feature selection.AcknowledgementsWe would like to thank Rachele de Felice for her assistance and Stephen Clark & RadaMihalcea for their useful comments.
This work was partially funded by the Compan-ions project (http://www.companions-project.org) sponsored by the EuropeanCommission as part of the Information Society Technologies (IST) programme underEC grant number IST-FP6-034434 and the ART Project (http://www.aber.ac.uk/compsci/Research/bio/art/) funded by the Joint Information Systems Committee(JISC).ReferencesBaker, C. F., C. J. Fillmore, and J.
B. Lowe (1998).
The Berkeley Framenet project.In Proceedings of the COLING-ACL, Montreal, Canada.Bos, J., S. Clark, M. Steedman, J. Curran, and J. Hockenmaier (2004).
Wide-Coverage Semantic Representations from a CCG parser.
In Proceedings of152 Liakata and Pulmanthe 20th International Conference on Computational Linguistics (COLING-04),Geneva,Switzerland, pp.
1240?1246.Briscoe, E. and J. Carroll (1997).
Automatic extraction of subcategorisation fromcorpora.
In Proceedings of ACL ANLP 97, pp.
356?363.Briscoe, E., J. Carroll, and R. Watson (1997).
The Second Release of the RASP Sys-tem.
In Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions,Sydney, Australia.Cahill, A., M. McCarthy, J. van Genabith, and A.
Way (2003).
Quasi-Logical Formsfor the Penn Treebank.
In I. v. d. S. Harry Bunt and R. Morante (Eds.
), Proceed-ings of the Fifth International Workshop on Computational Semantics, IWCS-05,Tilburg, The Netherlands, pp.
55?71.Cheeseman, P. and J. Stutz (1995).
Bayesian classification (Autoclass): Theory andresults.
In P. S. U. Fayyad, G. Piatesky-Shapiro and R. Uthurusamy (Eds.
), Ad-vances in Knowledge Discovery and Data Mining, pp.
153?180.
Menlo Park, CA:AAAI Press.Clark, S. and J. Curran (2007).
Wide-Coverage Efficient Statistical Parsing with CCGand Log-Linear Models.
Computational Linguistics 33(4), 493?552.Clark, S. and D. Weir (2002).
Class-Based Probability Estimation Using a SemanticHierarchy.
Computational Linguistics 28(2), 145?186.Gamallo, P., A. Agustini, and G. Lopes (2005).
Clustering Syntactic Positions withSimilar Semantic Requirements.
Computational Linguistics 31(1), 107?146.Gamallo, P., G. Lopes, and A. Agustini (2007).
Inducing Classes of Terms from Text.In Proceedings of TSD 2007, pp.
31?38.Hanks, P. and J. Pustejovsky (2004).
Common Sense About Word Meaning: Sense inContext.
In TSD 2004, pp.
15?18.Kilgariff, A.
(1997).
I don?t believe in word senses.
Computers and the Humani-ties 31, 91?113.Korhonen, A., Y. Krymolowski, and N. Collier (2006).
Automatic Classification ofVerbs in Biomedical Texts.
In Proceedings of AC-COLING 2006, Sydney, Aus-tralia.Korhonen, A. and J. Preiss (2003).
Improving Subcategorization Acquisition usingWord Sense Disambiguation.
In Proceedings of ACL 2003, Sapporo, Japan, pp.48?55.Liakata, M. and S. Pulman (2002).
From Trees to Predicate-Argument Structures.In Proceedings of the 19th International Conference on Computational Linguistics(COLING 2002), Taipei, Taiwan, pp.
563?569.Automatic Fine-Grained Semantic Classification for Domain Adaptation 153Liakata, M. and S. Pulman (2004).
Learning Theories from Text.
In Proceedings ofthe 20th International Conference on Computational Linguistics (COLING 2004),Geneva, Switzerland.Miller, G. A.
(1995).
?Wordnet: a lexical database for English.?.
In Communicationsof the ACM, Volume 38 (11), pp.
39 ?41.Pantel, P. and D. Lin (2002).
Concept Discovery from Text.
In In Proceedings ofCOLING 2002, Taipei, Taiwan.Pereira, F., N. Tishby, and L. Lee (1993).
?Distributional clustering of Englishwords.?.
In Proceedings of the 31th Annual Meeting of the Association of Com-putational Linguistics(ACL 93?
), Columbus, Ohio.Phillips, W. and E. Riloff (2002).
Exploiting Strong Syntactic Heuristics and Co-Training to Learn Semantic Lexicons.
In Proceedings of EMNLP 2002.Pustejovsky, J., P. Hanks, and A. Rumshisky (2004).
Automated Induction of Sensein Context.
In COLING 2004 5th International Workshop on Linguistically Inter-preted Corpora, Geneva, Switzerland, pp.
55?58.Rumshisky, A., P. Hanks, C. Havasi, and J. Pustejovsky (2006).
Constructing aCorpus-based Ontology using Model Bias.
In FLAIRS 2006, Melbourne Beach,Florida.Rumshisky, A. and J. Pustejovsky (2006).
Inducing Sense-Discriminating ContextPatterns from Sense-Tagged Corpora.
In LREC 2006, Genoa, Italy.Schulte im Walde, S. (2003).
Experiments on the Choice of Features for LearningVerb Classes.
In Proceedings of EACL 2003, Budapest, Ungarn.Schulte im Walde, S. (2006).
Experiments on the Automatic Induction of GermanSemantic Verb Classes.
Computational Linguistics 32(2), 159?194.Sun, L., A. Korhonen, and Y. Krymolowski (2008).
Verb Class Discovery from RichSyntactic Data.
In Proceedings of the 9th International Conference on IntelligentText Processing and Computational Linguistics, Haifa, Israel.
