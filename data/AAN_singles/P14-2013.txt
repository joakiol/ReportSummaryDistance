Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 75?80,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsGraph Ranking for Collective Named Entity DisambiguationAyman Alhelbawy1,2and Robert Gaizauskas11The University of Sheffield, Regent Court, 211 Portobello Street, Sheffield, S1 4DP, U.K2Faculty of Computers and Information, Fayoum University, Fayoum, Egyptayman,R.Gaizauskas@dcs.shef.ac.ukAbstractNamed Entity Disambiguation (NED)refers to the task of mapping differentnamed entity mentions in running text totheir correct interpretations in a specificknowledge base (KB).
This paper presentsa collective disambiguation approach us-ing a graph model.
All possible NE candi-dates are represented as nodes in the graphand associations between different candi-dates are represented by edges between thenodes.
Each node has an initial confidencescore, e.g.
entity popularity.
Page-Rankis used to rank nodes and the final rankis combined with the initial confidencefor candidate selection.
Experiments on27,819 NE textual mentions show the ef-fectiveness of using Page-Rank in con-junction with initial confidence: 87% ac-curacy is achieved, outperforming bothbaseline and state-of-the-art approaches.1 IntroductionNamed entities (NEs) have received much atten-tion over the last two decades (Nadeau and Sekine,2007), mostly focused on recognizing the bound-aries of textual NE mentions and classifying themas, e.g., Person, Organization or Location.
How-ever, references to entities in the real world are of-ten ambiguous: there is a many-to-many relationbetween NE mentions and the entities they denotein the real world.
For example, Norfolk may referto a person, ?Peter Norfolk, a wheelchair tennisplayer?, a place in the UK, ?Norfolk County?, orin the US, ?Norfolk, Massachusetts?
; conversely,one entity may be known by many names, such as?Cat Stevens?, ?Yusuf Islam?
and ?Steven Geor-giou?.
The NED task is to establish a correct map-ping between each NE mention in a document andthe real world entity it denotes.
Following most re-searchers in this area, we treat entries in a largeFigure 1: Example of solution graphknowledge base (KB) as surrogates for real worldentities when carrying out NED and, in particu-lar, use Wikipedia as the reference KB for dis-ambiguating NE mentions.
NED is important fortasks like KB population, where we want to ex-tract new information from text about an entity andadd this to a pre-existing entry in a KB; or for in-formation retrieval, where we may want to clusteror filter results for different entities with the sametextual mentions.The main hypothesis in this work is that differ-ent NEs in a document help to disambiguate eachother.
The problem is that other textual mentionsin the document are also ambiguous.
So, what isneeded is a collective disambiguation approachthat jointly disambiguates all NE textual mentions.In our approach we model each possible can-didate for every NE mention in a document as adistinct node in a graph and model candidate co-herence by links between the nodes.
We call suchgraphs solution graphs.
Figure 1 shows an exam-ple of the solution graph for three mentions ?A?,?B?, and ?C?
found in a document, where the can-didate entities for each mention are referred to us-ing the lower case form of the mention?s letter to-gether with a distinguishing subscript.
The goal ofdisambiguation is to find a set of nodes where onlyone candidate is selected from the set of entitiesassociated with each mention, e.g.
a3, b2, c2.Our approach first ranks all nodes in the solu-tion graph using the Page-Rank algorithm, then re-75ranks all nodes by combining the initial confidenceand graph ranking scores.
We consider several dif-ferent measures for computing the initial confi-dence assigned to each node and several measuresfor determining and weighting the graph edges.Node linking relies on the fact that the textual por-tion of KB entries typically contains mentions ofother NEs.
When these mentions are hyper-linkedto KB entries, we can infer that there is some rela-tion between the real world entities correspondingto the KB entries, i.e.
that they should be linkedin our solution graph.
These links also allow us tobuild up statistical co-occurrence counts betweenentities that occur in the same context which maybe used to weight links in our graph.We evaluate our approach on the AIDA dataset(Hoffart et al, 2011).
Comparison with thebaseline approach and some state-of-the-art ap-proaches shows our approach offers substantialimprovements in disambiguation accuracy.2 Related WorkIn 2009, NIST proposed the shared task challengeof Entity Linking (EL) (McNamee and Dang,2009).
EL is a similar but broader task than NEDbecause NED is concerned with disambiguatinga textual NE mention where the correct entity isknown to be one of the KB entries, while EL alsorequires systems to deal with the case where thereis no entry for the NE in the reference KB.
Ji etal.
(2011) group and summarise the different ap-proaches to EL taken by participating systems.In general, there are two main lines of approachto the NED problem.
Single entity disambigua-tion approaches (SNED), disambiguate one entityat a time without considering the effect of otherNEs.
These approaches use local context textualfeatures of the mention and compare them to thetextual features of NE candidate documents in theKB, and link to the most similar.
The first ap-proach in this line was Bunescu and Pasca (2006),who measure similarity between the textual con-text of the NE mention and the Wikipedia cate-gories of the candidate.
More similarity featureswere added by Cucerzan (2007) who realized thattopical coherence between a candidate entity andother entities in the context will improve NED ac-curacy and by Milne and Witten (2008) who builton Cucerzan?s work.
Han and Sun (2011) combinedifferent forms of disambiguation knowledge us-ing evidence from mention-entity associations andentity popularity in the KB, and context similarity.The second line of approach is collective namedentity disambiguation (CNED), where all men-tions of entities in the document are disambiguatedjointly.
These approaches try to model the interde-pendence between the different candidate entitiesfor different NE mentions in the query document,and reformulate the problem of NED as a globaloptimization problem whose aim is to find the bestset of entities.
As this new formulation is NP-hard, many approximations have been proposed.Alhelbawy and Gaizauskas (2013) proposed a se-quence dependency model using HMMs to modelNE interdependency.
Another approximation usesa mixture of local and global features to train thecoefficients of a linear ranking SVM to rank dif-ferent NE candidates (Ratinov et al, 2011).
Shi-rakawa et al (2011) cluster related textual men-tions and assign a concept to each cluster usinga probabilistic taxonomy.
The concept associatedwith a mention is used in selecting the correct en-tity from the Freebase KB.Graph models are widely used in collective ap-proaches1.
All these approaches model NE in-terdependencies, while different methods may beused for disambiguation.
Han (2011) uses localdependency between NE mention and the can-didate entity, and semantic relatedness betweencandidate entities to construct a referent graph,proposing a collective inference algorithm to in-fer the correct reference node in the graph.
Hoffert(2011) poses the problem as one of finding a densesub-graph, which is infeasible in a huge graph.
So,an algorithm originally used to find strongly inter-connected, size-limited groups in social media isadopted to prune the graph, and then a greedy al-gorithm is used to find the densest graph.Our proposed model uses the Page-Rank (PR)algorithm (Page et al, 1999), which to our knowl-edge has not previously been applied to NED.Xing and Ghorbani (2004) adopted PR to considerthe weights of links and the nodes?
importance.
PRand Personalized PR algorithms have been usedsuccessfully in WSD (Sinha and Mihalcea, 2007;Agirre and Soroa, 2009).3 Solution GraphIn this section we discuss the construction ofa graph representation that we call the solution1Graph models are also widely used in Word Sense Dis-ambiguation (WSD), which has lots of similarities to NED(Guti?errez et al, 2011; Guti?errez et al, 2012).76graph.
The input is a document containing pre-tagged NE textual mentions.
The solution graphis an undirected graph G = (V,D) where V is thenode set of all possible NE candidates for differ-ent textual mentions in the input document and Dis the set of edges between nodes.
Edges are notdrawn between different nodes for the same men-tion.
They are drawn between two entities whenthere is a relation between them, as described be-low.
Each candidate has associated with it an ini-tial confidence score, also detailed below.Assume the input document D has a set ofmentions M = {m1,m2,m3, ...,mk}.
For eachmi?
M , we rank each candidate entity, wherethe list of candidates for a mention miis Ei={ei,1, ei,2, ..., ei,j}.
The graph nodes are formu-lated as a set V = {(mi, ei,j) | ?ei,j?
Ei, ?mi?M}.
Nodes are represented as ordered pairs oftextual mentions and candidate entities, since thesame entity may be found multiple times as a can-didate for different textual mentions and each oc-currence must be evaluated independently.3.1 NE Candidate GenerationThe first step in constructing a solution graph is tofind all possible candidates for each NE mentionin the query document.
For each such mention theKB entry titles are searched to find all entries towhich the mention could refer.
This includes en-tries with titles that fully or partially contain thequery mention and those that could be an acronymof the query mention.
These candidate entries arepaired with their textual mentions in the documentto become nodes in the solution graph.3.2 Initial ConfidenceInitial confidence IConf(ei,j) is an independentfeature of the NE candidate regardless of othercandidates in the document.
This confidence maybe calculated locally using the local mention con-text, or globally using, e.g., the Freebase popular-ity score for the KB entry (Bollacker et al, 2008).Local NE Candidate Confidence: The localconfidence is computed by a similarity measurebetween the NE mention in the query documentand the KB entry of the candidate entity.
We pro-pose four different measures to be used in the dis-ambiguation phase.cos: The cosine similarity between the named en-tity textual mention and the KB entry title.jwSim: While the cosine similarity between a tex-tual mention in the document and the candidateNE title in the KB is widely used in NED, thissimilarity is a misleading feature.
For example,the textual mention ?Essex?
may refer to eitherof the following candidates ?Essex County CricketClub?
or ?Danbury, Essex?, both of which are re-turned by the candidate generation process.
Thecosine similarity between ?Essex?
and ?Danbury,Essex?
is higher than that between ?Essex?
and?Essex County Cricket Club?, which is not helpfulin the NED setting.
We adopted a new mention-candidate similarity function, jwSim, which usesJaro-Winkler similarity as a first estimate of theinitial confidence value for each candidate.
Thisfunction considers all terms found in the candidateentity KB entry title, but not in the textual mentionas disambiguation terms.
The percentage of dis-ambiguation terms found in the query document isused to boost in the initial jwSim value, in addi-tion to an acronym check (whether the NE textualmention could be an acronym for a specific can-didate entity title).
Experiments show that jwSimperforms much better than cos.ctxt: The cosine similarity between the sentencecontaining the NE mention in the query documentand the textual description of the candidate NE inthe KB (we use the first section of the Wikipediaarticle as the candidate entity description).Global NE Candidate Confidence: Globalconfidence is a measure of the global importanceof the candidate entity.
Entity popularity has beenused successfully as a discriminative feature forNED (Nebhi, 2013).
Freebase provides an APIto get an entity?s popularity score (FB), which iscomputed during Freebase indexing.
This score isa function of the entity?s inbound and outboundlink counts in Freebase and Wikipedia2.
The initialconfidence is not normalized across all NEs be-cause each score is calculated independently.
Ini-tial confidence scores of all candidates for a singleNE mention are normalized to sum to 1.3.3 Entity CoherenceEntity coherence refers to the real world related-ness of different entities which are candidate inter-pretations of different textual mentions in the doc-ument.
It is not based on context, so it is alwaysthe same regardless of the query document.
Co-herence is represented as an edge between nodesin the solution graph.
We used two measures forcoherence, described as follows:2https://developers.google.com/freebase/v1/search77Ref: Uses the Wikipedia documents for both en-tity candidates to check if either document has alink to the other.
This relation is directed, but weassume an inverse relation also exists; so this rela-tion is represented as undirected.Ref(ei, ej) ={1, if eior ejrefers to the other0, otherwise(1)JProb: An estimate of the probability of bothentities appearing in the same sentence.
Wikipediadocuments are used to estimate this probability, asshown in (2), where S(e) is the set of all sentencesthat contain the entity e and S the set of sentencescontaining any entity references.JProb(ei, ej) =|S(ei)?S(ej)||S|(2)4 DisambiguationThe solution graph contains all possible candi-dates for each NE mention in the document.
Eachcandidate has an initial confidence, with someconnected by association relations.
The disam-biguation phase ranks all nodes in the solutiongraph and selects the best from the candidate listfor each NE textual mention.
The process of dis-ambiguation consists of three steps.
The first stepis initial graph ranking, where all nodes are rankedaccording to the link structure.
The second step isto re-rank the nodes by combining the graph rankwith the initial confidence.
The highest rank is notalways correct, so in the third step a selection al-gorithm is used to choose the best candidate.Graph Ranking: The links between differentcandidates in the solution graph represent realworld relations.
These relations may be used to re-liably boost relevant candidates.
All nodes in thegraph are ranked according to these relations usingPR.
Initial confidence is used as an initial rank forthe graph nodes, while entities?
coherence mea-sures are used as link weights which play a role indistributing a node?s rank over its outgoing nodes.Candidate Re-ranking: A problem with Page-Rank for our purposes is the dissipation of initialnode weight (confidence) over all outgoing nodes.The final rank of a node is based solely on the im-portance of incoming nodes and the initial confi-dence play no further role.
In our case this is notappropriate, so the final rank for each mention isdetermined after graph ranking, by combining thegraph rank with the initial confidence.Let us refer to the graph rank of a candidate asPR(ei).
Two combination schemes are used:Rs(ei,j) = IConf(ei,j) + PR(ei,j) (3)Rm(ei,j) = IConf(ei,j)?
PR(ei,j) (4)Named Entity Selection: The simplest ap-proach is to select the highest ranked entity in thelist for each mention miaccording to equation5, where R could refer to Rmor Rs.
However,we found that a dynamic choice between the re-ranking schemes, based on the difference betweenthe top two candidates, as described in algorithm1 and indicated by eg,works best.
The underlyingintuition of this algorithm is that a greater differ-ence between the top ranks reflects more confidentdiscrimination between candidates.
So, the twocombination schemes assign different ranks to thecandidates and the algorithm selects the schemewhich appears more discriminative.e?i= argmaxei,jR(ei,j) (5)Data: Two lists, R1 and R2, of candidates Ei, where R1is ranked using Rs, and R2 is ranked using RmResult: One NE egiSort R1 and R2 in descending order;R1diff = R1[0]-R1[1];R2diff = R2[0]-R2[1];if R1diff > R2diff thenreturn highest rank scored entity of R1elsereturn highest rank scored entity of R2endAlgorithm 1: Selection Algorithm5 Experiments and ResultsWe used AIDA dataset3, which is based on theCoNLL 2003 data for NER tagging.
All mentionsare manually disambiguated against Wikipedia(Hoffart et al, 2011).
This dataset contains 1393documents and 34,965 annotated mentions.
Weonly consider NE mentions with an entry in theWikipedia KB, ignoring the 20% of query men-tions (7136) without a link to the KB, as Hoffartdid.
Micro-averaged and macro-averaged accu-racy are used for evaluation.
In this context micro-averaged accuracy corresponds to the propor-tion of textual mentions correctly disambiguatedwhile macro-averaged accuracy corresponds to theproportion of textual mentions correctly disam-biguated per entity, averaged over all entities.5.1 ResultsInitially, we evaluated the performance of twobaselines.
One is a setup where a ranking basedsolely on different initial confidence scores is used3http://www.mpi-inf.mpg.de/yago-naga/aida/78IConf PRCPRIPRICCucerzan Kulkarni Hoffart Shirakawa AlhelbawyAmacro78.09 80.98 84.19 82.80 43.74 76.74 81.91 83.02 74.18Amicro80.55 83.59 87.59 86.10 51.03 72.87 81.82 82.29 78.49Table 1: Results comparison between Proposed Approach and State-of-the-artPR egIConf AmicroAmacroAmicroAmacrocos 70.6 60.83 78.41 72.35jwSim 70.61 60.94 83.16 78.28ctxt 70.61 60.83 75.45 65.22freebase 71.78 81.07 87.59 84.19Table 2: Results using initial confidence (PRI)PR egEdge Weight AmicroAmacroAmicroAmacroJprob 66.52 55.83 83.31 80.38Ref 67.48 59.76 81.80 78.53prob+ refs 72.69 65.71 83.46 80.69Table 3: Results using weighted edges (PRC)for candidate selection, i.e.
without using PR.
Inthis setup a ranking based on Freebase popularitydoes best, with micro- and macro-averaged accu-racy scores of 80.55% and 78.09% respectively.This is a high baseline, close to the state-of-the-art.
Our second baseline is the basic PR algorithm,where weights of nodes and edges are uniform (i.e.initial node and edge weights set to 1, edges be-ing created wherever REF or JProb are not zero).Micro and macro accuracy scores of 70.60% and60.91% were obtained with this baseline.To study graph ranking using PR, and the con-tributions of the initial confidence and entity co-herence, experiments were carried out using PR indifferent modes and with different selection tech-niques.
In the first experiment, referred to as PRI,initial confidence is used as an initial node rank forPR and edge weights are uniform, edges, as in thePR baseline, being created wherever REF or JProbare not zero.
Table 2 shows the results both beforere-ranking, i.e.
using only the PR score for rank-ing, and after re-ranking using the dynamic selec-tion scheme eg.
When comparing these results tothe PR baseline we notice a slight positive effectwhen using the initial confidence as an initial rankinstead of uniform ranking.
The major improve-ment comes from re-ranking nodes by combininginitial confidence with PR score.In our second experiment, PRC, entity coher-ence features are tested by setting the edge weightsto the coherence score and using uniform ini-tial node weights.
We compared JProb and Refeg(jwSim) eg(freebase)Edge Weight AmicroAmacroAmicroAmacroJprob 82.56 76.16 86.29 82.77Ref 78.61 71.12 83.16 80.01Jprob+Ref 81.97 75.63 86.10 82.80Table 4: Results using IConf and weighted edges PRICedge weighting approaches, where for each ap-proach edges were created only where the coher-ence score according to the approach was non-zero.
We also investigated a variant, called JProb +Ref, in which the Ref edge weights are normalizedto sum to 1 over the whole graph and then addedto the JProb edge weights (here edges result wher-ever JProb or Ref scores are non-zero).
Results inTable 3 show the JProb feature seems to be morediscriminative than the Ref feature but the com-bined Jprob + Ref feature performs better thaneach separately, just outperforming the baseline.We used the best initial confidence score (Free-base) for re-ranking.
Again, combining the initialconfidence with the PR score improves the results.Finally, Table 4 shows the accuracy when usingdifferent combinations of initial confidence andentity coherence scores just in the case when re-ranking is applied.
Here the jprob + refs com-bination does not add any value over jprob alone.Interestingly using initial confidence with differ-entially weighted edges does not show any ben-efit over using initial confidence and uniformlyweighted edges (Table 2).To compare our results with the state-of-the-art,we report Hoffart et al?s (2011) results as they re-implemented two other systems and also ran themover the AIDA dataset.
We also compare with Al-helbawy and Gaizauskas (2013) and Shirakawa etal.
(2011) who carried out their experiments usingthe same dataset.
Table 1 presents a comparisonbetween our approach and the state-of-the-art andshows our approach exceeds the state-of-the-art.Futhermore our approach is very simple and directto apply, unlike Hoffart et al?s and Shirakawa etal.
?s which are considerably more complex.
Also,our approach does not need any kind of training,as does the Alhelbawy approach.6 ConclusionOur results show that Page-Rank in conjunctionwith re-ranking by initial confidence score can beused as an effective approach to collectively dis-ambiguate named entity textual mentions in a doc-ument.
Our proposed features are very simple andeasy to extract, and work well when employed inPR.
In future work we plan to explore enrichingthe edges between nodes by incorporating seman-tic relations extracted from an ontology.79ReferencesEneko Agirre and Aitor Soroa.
2009.
Personaliz-ing pagerank for word sense disambiguation.
InProceedings of the 12th Conference of the Euro-pean Chapter of the Association for ComputationalLinguistics, pages 33?41.
Association for Computa-tional Linguistics.Ayman Alhelbawy and Robert Gaizauskas.
2013.Named entity disambiguation using hmms.
InWeb Intelligence (WI) and Intelligent Agent Tech-nologies (IAT), 2013 IEEE/WIC/ACM InternationalJoint Conferences on, volume 3, pages 159?162.Kurt Bollacker, Colin Evans, Praveen Paritosh, TimSturge, and Jamie Taylor.
2008.
Freebase: A col-laboratively created graph database for structuringhuman knowledge.
In Proceedings of the 2008ACM SIGMOD International Conference on Man-agement of Data, SIGMOD ?08, pages 1247?1250,New York, NY, USA.
ACM.Razvan C. Bunescu and Marius Pasca.
2006.
Us-ing encyclopedic knowledge for named entity dis-ambiguation.
In EACL.
The Association for Com-puter Linguistics.Silviu Cucerzan.
2007.
Large-scale named entity dis-ambiguation based on wikipedia data.
In Proceed-ings of EMNLP-CoNLL, volume 6, pages 708?716.Yoan Guti?errez, Sonia V?azquez, and Andr?es Montoyo.2011.
Word sense disambiguation: a graph-basedapproach using n-cliques partitioning technique.
InNatural Language Processing and Information Sys-tems, pages 112?124.
Springer.Yoan Guti?errez, Sonia V?azquez, and Andr?es Montoyo.2012.
A graph-based approach to wsd using rele-vant semantic trees and n-cliques model.
In Compu-tational Linguistics and Intelligent Text Processing,pages 225?237.
Springer.Xianpei Han and Le Sun.
2011.
A generative entity-mention model for linking entities with knowledgebase.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies-Volume 1, pages 945?954.
Association for Computational Linguistics.Xianpei Han, Le Sun, and Jun Zhao.
2011.
Collectiveentity linking in web text: a graph-based method.
InProceedings of the 34th international ACM SIGIRconference on Research and development in Infor-mation Retrieval, pages 765?774.
ACM.Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bor-dino, Hagen F?urstenau, Manfred Pinkal, Marc Span-iol, Bilyana Taneva, Stefan Thater, and GerhardWeikum.
2011.
Robust disambiguation of namedentities in text.
In Proceedings of the Conference onEmpirical Methods in Natural Language Process-ing, pages 782?792.
Association for ComputationalLinguistics.Heng Ji and Ralph Grishman.
2011.
Knowledgebase population: successful approaches and chal-lenges.
In Proceedings of the 49th Annual Meetingof the Association for Computational Linguistics:Human Language Technologies-Volume 1, pages1148?1158.
Association for Computational Linguis-tics.Paul McNamee and Hoa Trang Dang.
2009.
Overviewof the tac 2009 knowledge base population track.
InText Analysis Conference (TAC), volume 17, pages111?113.David Milne and Ian H Witten.
2008.
Learning to linkwith wikipedia.
In Proceeding of the 17th ACM con-ference on Information and knowledge management,pages 509?518.
ACM.David Nadeau and Satoshi Sekine.
2007.
A sur-vey of named entity recognition and classification.Lingvisticae Investigationes, 30(1):3?26.Kamel Nebhi.
2013.
Named entity disambiguationusing freebase and syntactic parsing.
In CEUR-WS.org, editor, Proceedings of the First Interna-tional Workshop on Linked Data for Information Ex-traction (LD4IE 2013) co-located with the 12th In-ternational Semantic Web Conference (ISWC 2013).Lawrence Page, Sergey Brin, Rajeev Motwani, andTerry Winograd.
1999.
The pagerank citation rank-ing: Bringing order to the web.
Technical Report1999-66, Stanford InfoLab, November.
Previousnumber = SIDL-WP-1999-0120.Lev Ratinov, Dan Roth, Doug Downey, and MikeAnderson.
2011.
Local and global algorithmsfor disambiguation to wikipedia.
In Proceedingsof the 49th Annual Meeting of the Associationfor Computational Linguistics: Human LanguageTechnologies-Volume 1, pages 1375?1384.
Associ-ation for Computational Linguistics.Masumi Shirakawa, Haixun Wang, Yangqiu Song,Zhongyuan Wang, Kotaro Nakayama, TakahiroHara, and Shojiro Nishio.
2011.
Entity disam-biguation based on a.
Technical report, Technicalreport, Technical Report MSR-TR-2011-125, Mi-crosoft Research.Ravi Sinha and Rada Mihalcea.
2007.
Unsupervisedgraph-basedword sense disambiguation using mea-sures of word semantic similarity.
In Semantic Com-puting, 2007.
ICSC 2007. International Conferenceon, pages 363?369.
IEEE.Wenpu Xing and Ali Ghorbani.
2004.
Weighted pager-ank algorithm.
In Communication Networks andServices Research, 2004.
Proceedings.
Second An-nual Conference on, pages 305?314.
IEEE.80
