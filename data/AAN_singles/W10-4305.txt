Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 28?36,The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational LinguisticsEvaluation Metrics For End-to-End Coreference Resolution SystemsJie Cai and Michael StrubeHeidelberg Institute for Theoretical Studies gGmbHSchlo?-Wolfsbrunnenweg 3569118 Heidelberg, Germany(jie.cai|michael.strube)@h-its.orgAbstractCommonly used coreference resolutionevaluation metrics can only be applied tokey mentions, i.e.
already annotated men-tions.
We here propose two variants of theB3 and CEAF coreference resolution eval-uation algorithms which can be appliedto coreference resolution systems dealingwith system mentions, i.e.
automaticallydetermined mentions.
Our experimentsshow that our variants lead to intuitive andreliable results.1 IntroductionThe coreference resolution problem can be di-vided into two steps: (1) determining mentions,i.e., whether an expression is referential andcan take part in a coreferential relationship, and(2) deciding whether mentions are coreferent ornot.
Most recent research on coreference res-olution simplifies the resolution task by provid-ing the system with key mentions, i.e.
already an-notated mentions (Luo et al (2004), Denis &Baldridge (2007), Culotta et al (2007), Haghighi& Klein (2007), inter alia; see also the task de-scription of the recent SemEval task on coref-erence resolution at http://stel.ub.edu/semeval2010-coref), or ignores an impor-tant part of the problem by evaluating on key men-tions only (Ponzetto & Strube, 2006; Bengtson &Roth, 2008, inter alia).
We follow here Stoyanovet al (2009, p.657) in arguing that such evalua-tions are ?an unrealistic surrogate for the originalproblem?
and ask researchers to evaluate end-to-end coreference resolution systems.However, the evaluation of end-to-end coref-erence resolution systems has been inconsistentmaking it impossible to compare the results.
Nico-lae & Nicolae (2006) evaluate using the MUCscore (Vilain et al, 1995) and the CEAF algorithm(Luo, 2005) without modifications.
Yang et al(2008) use only the MUC score.
Bengtson & Roth(2008) and Stoyanov et al (2009) derive variantsfrom the B3 algorithm (Bagga & Baldwin, 1998).Rahman & Ng (2009) propose their own variantsof B3 and CEAF.
Unfortunately, some of the met-rics?
descriptions are so concise that they leave toomuch room for interpretation.
Also, some of themetrics proposed are too lenient or are more sen-sitive to mention detection than to coreference res-olution.
Hence, though standard corpora are used,the results are not comparable.This paper attempts to fill that desideratum byanalysing several variants of the B3 and CEAF al-gorithms.
We propose two new variants, namelyB3sys and CEAFsys, and provide algorithmic de-tails in Section 2.
We describe two experiments inSection 3 showing that B3sys and CEAFsys lead tointuitive and reliable results.
Implementations ofB3sys and CEAFsys are available open source alongwith extended examples1.2 Coreference Evaluation MetricsWe discuss the problems which arise when apply-ing the most prevalent coreference resolution eval-uation metrics to end-to-end systems and proposeour variants which overcome those problems.
Weprovide detailed analyses of illustrative examples.2.1 MUCThe MUC score (Vilain et al, 1995) countsthe minimum number of links between mentionsto be inserted or deleted when mapping a sys-tem response to a gold standard key set.
Al-though pairwise links capture the informationin a set, they cannot represent singleton en-tities, i.e.
entities, which are mentioned onlyonce.
Therefore, the MUC score is not suitablefor the ACE data (http://www.itl.nist.1http://www.h-its.org/nlp/download28gov/iad/mig/tests/ace/), which includessingleton entities in the keys.
Moreover, the MUCscore does not give credit for separating singletonentities from other chains.
This becomes problem-atic in a realistic system setup, when mentions areextracted automatically.2.2 B3The B3 algorithm (Bagga & Baldwin, 1998) over-comes the shortcomings of the MUC score.
In-stead of looking at the links, B3 computes preci-sion and recall for all mentions in the document,which are then combined to produce the final pre-cision and recall numbers for the entire output.For each mention, the B3 algorithm computes aprecision and recall score using equations 1 and 2:Precision(mi) =|Rmi ?
Kmi ||Rmi |(1)Recall(mi) =|Rmi ?
Kmi ||Kmi |(2)where Rmi is the response chain (i.e.
the systemoutput) which includes the mention mi, and Kmiis the key chain (manually annotated gold stan-dard) with mi.
The overall precision and recall arecomputed by averaging them over all mentions.Since B3?s calculations are based on mentions,singletons are taken into account.
However, aproblematic issue arises when system mentionshave to be dealt with: B3 assumes the mentions inthe key and in the response to be identical.
Hence,B3 has to be extended to deal with system men-tions which are not in the key and key mentionsnot extracted by the system, so called twinlessmentions (Stoyanov et al, 2009).2.2.1 Existing B3 variantsA few variants of the B3 algorithm for dealing withsystem mentions have been introduced recently.Stoyanov et al (2009) suggest two variants of theB3 algorithm to deal with system mentions, B30 andB3all2.
For example, a key and a response are pro-vided as below:Key : {a b c}Response: {a b d}B30 discards all twinless system mentions (i.e.mention d) and penalizes recall by settingrecallmi = 0 for all twinless key mentions (i.e.mention c).
The B30 precision, recall and F-score2Our discussion of B30 and B3all is based on the analysisof the source code available at http://www.cs.utah.edu/nlp/reconcile/.Set 1System 1 key {a b c}response {a b d}P R FB30 1.0 0.444 0.615B3all 0.556 0.556 0.556B3r&n 0.556 0.556 0.556B3sys 0.667 0.556 0.606CEAFsys 0.5 0.667 0.572System 2 key {a b c}response {a b d e}P R FB30 1.0 0.444 0.615B3all 0.375 0.556 0.448B3r&n 0.375 0.556 0.448B3sys 0.5 0.556 0.527CEAFsys 0.4 0.667 0.500Table 1: Problems of B30(i.e.
F = 2 ?
Precision?RecallPrecision+Recall ) for the example arecalculated as:PrB30 =12 ( 22 + 22 ) = 1.0RecB30 =13 ( 23 + 23 + 0).= 0.444FB30 = 2 ?1.0?0.4441.0+0.444.= 0.615B3all retains twinless system mentions.
It assigns1/|Rmi | to a twinless system mention as its preci-sion and similarly 1/|Kmi | to a twinless key men-tion as its recall.
For the same example above, theB3all precision, recall and F-score are given by:PrB3all= 13 ( 23 + 23 + 13 ).= 0.556RecB3all= 13 ( 23 + 23 + 13 ).= 0.556FB3all= 2 ?
0.556?0.5560.556+0.444.= 0.556Tables 1, 2 and 3 illustrate the problems with B30and B3all.
The rows labeled System give the origi-nal keys and system responses while the rows la-beled B30, B3all and B3sys show the performance gen-erated by Stoyanov et al?s variants and the onewe introduce in this paper, B3sys (the row labeledCEAFsys is discussed in Subsection 2.3).In Table 1, there are two system outputs (i.e.System 1 and System 2).
Mentions d and e arethe twinless system mentions erroneously resolvedand c a twinless key mention.
System 1 is sup-posed to be slightly better with respect to preci-sion, because System 2 produces one more spu-rious resolution (i.e.
for mention e ).
However,B30 computes exactly the same numbers for bothsystems.
Hence, there is no penalty for erroneouscoreference relations in B30, if the mentions do notappear in the key, e.g.
putting mentions d or e inSet 1 does not count as precision errors.
?
B30is too lenient by only evaluating the correctly ex-tacted mentions.29Set 1 SingletonsSystem 1 key {a b c}response {a b d}P R FB3all 0.556 0.556 0.556B3r&n 0.556 0.556 0.556B3sys 0.667 0.556 0.606CEAFsys 0.5 0.667 0.572System 2 key {a b c}response {a b d} {c}P R FB3all 0.667 0.556 0.606B3r&n 0.667 0.556 0.606B3sys 0.667 0.556 0.606CEAFsys 0.5 0.667 0.572Table 2: Problems of B3all (1)Set 1 SingletonsSystem 1 key {a b}response {a b d}P R FB3all 0.556 1.0 0.715B3r&n 0.556 1.0 0.715B3sys 0.556 1.0 0.715CEAFsys 0.667 1.0 0.800System 2 key {a b}response {a b d} {i} {j} {k}P R FB3all 0.778 1.0 0.875B3r&n 0.556 1.0 0.715B3sys 0.556 1.0 0.715CEAFsys 0.667 1.0 0.800Table 3: Problems of B3all (2)B3all deals well with the problem illustrated inTable 1, the figures reported correspond to in-tuition.
However, B3all can output different re-sults for identical coreference resolutions whenexposed to different mention taggers as shown inTables 2 and 3.
B3all manages to penalize erro-neous resolutions for twinless system mentions,however, it ignores twinless key mentions whenmeasuring precision.
In Table 2, System 1 and Sys-tem 2 generate the same outputs, except that themention tagger in System 2 also extracts mentionc.
Intuitively, the same numbers are expected forboth systems.
However, B3all gives a higher preci-sion to System 2, which results in a higher F-score.B3all retains all twinless system mentions, as canbe seen in Table 3.
System 2?s mention tagger tagsmore mentions (i.e.
the mentions i, j and k), whileboth System 1 and System 2 have identical coref-erence resolution performance.
Still, B3all outputsquite different results for precision and thus for F-score.
This is due to the credit B3all takes from un-resolved singleton twinless system mentions (i.e.mention i, j, k in System 2).
Since the metric is ex-pected to evaluate the end-to-end coreference sys-tem performance rather than the mention taggingquality, it is not satisfying to observe that B3all?snumbers actually fluctuate when the system is ex-posed to different mention taggers.Rahman & Ng (2009) apply another variant, de-noted here as B3r&n.
They remove only those twin-less system mentions that are singletons before ap-plying the B3 algorithm.
So, a system would notbe rewarded by the the spurious mentions whichare correctly identified as singletons during reso-lution (as has been the case with B3all?s higher pre-cision for System 2 as can be seen in Table 3).We assume that Rahman & Ng apply a strategysimilar to B3all after the removing step (this is notclear in Rahman & Ng (2009)).
While it avoids theproblem with singleton twinless system mentions,B3r&n still suffers from the problem dealing withtwinless key mentions, as illustrated in Table 2.2.2.2 B3sysWe here propose a coreference resolution evalua-tion metric, B3sys, which deals with system men-tions more adequately (see the rows labeled B3sysin Tables 1, 2, 3, 4 and 5).
We put all twinless keymentions into the response as singletons which en-ables B3sys to penalize non-resolved coreferent keymentions without penalizing non-resolved single-ton key mentions, and also avoids the problem B3alland B3r&n have as shown in Table 2.
All twinlesssystem mentions which were deemed not coref-erent (hence being singletons) are discarded.
Tocalculate B3sys precision, all twinless system men-tions which were mistakenly resolved are put intothe key since they are spurious resolutions (equiv-alent to the assignment operations in B3all), whichshould be penalized by precision.
Unlike B3all,B3sys does not benefit from unresolved twinlesssystem mentions (i.e.
the twinless singleton sys-tem mentions).
For recall, the algorithm only goesthrough the original key sets, similar to B3all andB3r&n.
Details are given in Algorithm 1.For example, a coreference resolution systemhas the following key and response:Key : {a b c}Response: {a b d} {i j}To calculate the precision of B3sys, the key and re-sponse are altered to:Keyp : {a b c} {d} {i} {j}Responsep: {a b d} {i j} {c}30Algorithm 1 B3sysInput: key sets key, response sets responseOutput: precision P , recall R and F-score F1: Discard all the singleton twinless system mentions inresponse;2: Put all the twinless annotated mentions into response;3: if calculating precision then4: Merge all the remaining twinless system mentionswith key to form keyp;5: Use response to form responsep6: Through keyp and responsep;7: Calculate B3 precision P .8: end if9: if calculating recall then10: Discard all the remaining twinless system mentions inresponse to from responser;11: Use key to form keyr12: Through keyr and responser;13: Calculate B3 recall R14: end if15: Calculate F-score FSo, the precision of B3sys is given by:PrB3sys =16 ( 23 + 23 + 13 + 12 + 12 + 1).= 0.611The modified key and response for recall are:Keyr : {a b c}Responser: {a b} {c}The resulting recall of B3sys is:RecB3sys =13 ( 23 + 23 + 13 ).= 0.556Thus the F-score number is calculated as:FB3sys = 2 ?0.611?0.5560.611+0.556.= 0.582B3sys indicates more adequately the performance ofend-to-end coreference resolution systems.
It isnot easily tricked by different mention taggers3.2.3 CEAFLuo (2005) criticizes the B3 algorithm for usingentities more than one time, because B3 computesprecision and recall of mentions by comparing en-tities containing that mention.
Hence Luo pro-poses the CEAF algorithm which aligns entities inkey and response.
CEAF applies a similarity met-ric (which could be either mention based or entitybased) for each pair of entities (i.e.
a set of men-tions) to measure the goodness of each possiblealignment.
The best mapping is used for calculat-ing CEAF precision, recall and F-measure.Luo proposes two entity based similarity met-rics (Equation 3 and 4) for an entity pair (Ki, Rj)originating from key, Ki, and response, Rj .
?3(Ki, Rj) = |Ki ?
Rj | (3)?4(Ki, Rj) =2|Ki ?
Rj ||Ki| + |Rj |(4)3Further example analyses can be found in Appendix A.The CEAF precision and recall are derived fromthe alignment which has the best total similarity(denoted as ?(g?
)), shown in Equations 5 and 6.Precision = ?(g?
)?i ?
(Ri, Ri)(5)Recall = ?(g?
)?i ?
(Ki,Ki)(6)If not specified otherwise, we apply Luo?s ?3(?, ?
)in the example illustrations.
We denote the origi-nal CEAF algorithm as CEAForig.Detailed calculations are illustrated below:Key : {a b c}Response: {a b d}The CEAForig ?3(?, ?)
are given by:?3(K1, R1) = 2 (K1 : {abc};R1 : {abd})?3(K1,K1) = 3?3(R1, R1) = 3So the CEAForig evaluation numbers are:PrCEAForig = 23 = 0.667RecCEAForig = 23 = 0.667FCEAForig = 2 ?
0.667?0.6670.667+0.667 = 0.6672.3.1 Problems of CEAForigCEAForig was intended to deal with key mentions.Its adaptation to system mentions has not been ad-dressed explicitly.
Although CEAForig theoreti-cally does not require to have the same number ofmentions in key and response, it still cannot be di-rectly applied to end-to-end systems, because theentity alignments are based on mention mappings.As can be seen from Table 4, CEAForig failsto produce intuitive results for system mentions.System 2 outputs one more spurious entity (con-taining mention i and j) than System 1 does, how-ever, achieves a same CEAForig precision.
Sincetwinless system mentions do not have mappings inkey, they contribute nothing to the mapping simi-larity.
So, resolution mistakes for system mentionsare not calculated, and moreover, the precision iseasily skewed by the number of output entities.CEAForig reports very low precision for systemmentions (see also Stoyanov et al (2009)).2.3.2 Existing CEAF variantsRahman & Ng (2009) briefly introduce theirCEAF variant, which is denoted as CEAFr&nhere.
They use ?3(?, ?
), which results in equalCEAFr&n precision and recall figures when usingtrue mentions.
Since Rahman & Ng?s experimentsusing system mentions produce unequal precisionand recall figures, we assume that, after removing31Set 1 Set 2 SingletonsSystem 1 key {a b c}response {a b} {c} {i} {j}P R FCEAForig 0.4 0.667 0.500B3sys 1.0 0.556 0.715CEAFsys 0.667 0.667 0.667System 2 key {a b c}response {a b} {i j} {c}P R FCEAForig 0.4 0.667 0.500B3sys 0.8 0.556 0.656CEAFsys 0.6 0.667 0.632Table 4: Problems of CEAForigSet 1 Set 2 Set 3 SingletonsSystem 1 key {a b c}response {a b} {i j} {k l} {c}P R FCEAFr&n 0.286 0.667 0.400B3sys 0.714 0.556 0.625CEAFsys 0.571 0.667 0.615System 2 key {a b c}response {a b} {i j k l} {c}P R FCEAFr&n 0.286 0.667 0.400B3sys 0.571 0.556 0.563CEAFsys 0.429 0.667 0.522Table 5: Problems of CEAFr&ntwinless singleton system mentions, they do notput any twinless mentions into the other set.
In theexample in Table 5, CEAFr&n does not penalizeadequately the incorrectly resolved entities con-sisting of twinless sytem mentions.
So CEAFr&ndoes not tell the difference between System 1 andSystem 2.
It can be concluded from the examplesthat the same number of mentions in key and re-sponse is needed for computing the CEAF score.2.3.3 CEAFsysWe propose to adjust CEAF in the same way aswe did for B3sys, resulting in CEAFsys.
We putall twinless key mentions into the response as sin-gletons.
All singleton twinless system mentionsare discarded.
For calculating CEAFsys precision,all twinless system mentions which were mistak-enly resolved are put into the key.
For computingCEAFsys recall, only the original key sets are con-sidered.
That way CEAFsys deals adequately withsystem mentions (see Algorithm 2 for details).Algorithm 2 CEAFsysInput: key sets key, response sets responseOutput: precision P , recall R and F-score F1: Discard all the singleton twinless system mentions inresponse;2: Put all the twinless annotated mentions into response;3: if calculating precision then4: Merge all the remaining twinless system mentionswith key to form keyp;5: Use response to form responsep6: Form Map g?
between keyp and responsep7: Calculate CEAF precision P using ?3(?, ?
)8: end if9: if calculating recall then10: Discard all the remaining twinless system mentions inresponse to form responser;11: Use key to form keyr12: Form Map g?
between keyr and responser13: Calculate CEAF recall R using ?3(?, ?
)14: end if15: Calculate F-score FTaking System 2 in Table 4 as an example, key andresponse are altered for precision:Keyp : {a b c} {i} {j}Responsep: {a b d} {i j} {c}So the ?3(?, ?)
are as below, only listing the bestmappings:?3(K1, R1) = 2 (K1 : {abc};R1 : {abd})?3(K2, R2) = 1 (K2 : {i};R2 : {ij})?3(?, R3) = 0 (R3 : {c})?3(R1, R1) = 3?3(R2, R2) = 2?3(R3, R3) = 1The precision is thus give by:PrCEAFsys = 2+1+03+2+1 = 0.6The key and response for recall are:Keyr : {a b c}Responser: {a b} {c}The resulting ?3(?, ?)
are:?3(K1, R1) = 2(K1 : {abc};R1 : {ab})?3(?, R2) = 0(R2 : {c})?3(K1,K1) = 3?3(R1, R1) = 2?3(R2, R2) = 1The recall and F-score are thus calculated as:RecCEAFsys = 23 = 0.667FCEAFsys = 2 ?
0.6?0.6670.6+0.667 = 0.632However, one additional complication ariseswith regard to the similarity metrics used byCEAF.
It turns out that only ?3(?, ?)
is suitablefor dealing with system mentions while ?4(?, ?
)produces uninituitive results (see Table 6).
?4(?, ?)
computes a normalized similarity foreach entity pair using the summed number of men-tions in the key and the response.
CEAF precisionthen distributes that similarity evenly over the re-sponse set.
Spurious system entities, such as theone with mention i and j in Table 6, are not pe-nalized.
?3(?, ?)
calculates unnormalized similar-ities.
It compares the two systems in Table 6 ade-quately.
Hence we use only ?3(?, ?)
in CEAFsys.32Set 1 SingletonsSystem 1 key {a b c}response {a b} {c} {i} {j}P R F?4(?, ?)
0.4 0.8 0.533?3(?, ?)
0.667 0.667 0.667System 2 key {a b c}response {a b} {i j} {c}P R F?4(?, ?)
0.489 0.8 0.607?3(?, ?)
0.6 0.667 0.632Table 6: Problems of ?4(?, ?
)When normalizing the similarities by the num-ber of entities or mentions in the key (for recall)and the response (for precision), the CEAF al-gorithm considers all entities or mentions to beequally important.
Hence CEAF tends to computequite low precision for system mentions whichdoes not represent the system performance ade-quately.
Here, we do not address this issue.2.4 BLANCRecently, a new coreference resolution evalua-tion algorithm, BLANC, has been introduced (Re-casens & Hovy, 2010).
This measure implementsthe Rand index (Rand, 1971) which has been orig-inally developed to evaluate clustering methods.The BLANC algorithm deals correctly with sin-gleton entities and rewards correct entities accord-ing to the number of mentions.
However, a ba-sic assumption behind BLANC is, that the sum ofall coreferential and non-coreferential links is con-stant for a given set of mentions.
This implies thatBLANC assumes identical mentions in key and re-sponse.
It is not clear how to adapt BLANC to sys-tem mentions.
We do not address this issue here.3 ExperimentsWhile Section 2 used toy examples to motivate ourmetrics B3sys and CEAFsys, we here report resultson two larger experiments using ACE2004 data.3.1 Data and Mention TaggersWe use the ACE2004 (Mitchell et al, 2004) En-glish training data which we split into three setsfollowing Bengtson & Roth (2008): Train (268docs), Dev (76), and Test (107).
We use two in-house mention taggers.
The first (SM1) imple-ments a heuristic aiming at high recall.
The second(SM2) uses the J48 decision tree classifier (Wit-ten & Frank, 2005).
The number of detected men-tions, head coverage, and accuracy on testing dataSM1 SM2training mentions 31,370 16,081twin mentions 13,072 14,179development mentions 8,045 ?twin mentions 3,371 ?test mentions 8,387 4,956twin mentions 4,242 4,212head coverage 79.3% 73.3%accuracy 57.3% 81.2%Table 7: Mention Taggers on ACE2004 Dataare shown in Table 7.3.2 Artificial SettingFor the artificial setting we report results on thedevelopment data using the SM1 tagger.
To illus-trate the stability of the evaluation metrics withrespect to different mention taggers, we reducethe number of twinless system mentions in inter-vals of 10%, while correct (non-twinless) ones arekept untouched.
The coreference resolution sys-tem used is the BART (Versley et al, 2008) reim-plementation of Soon et al (2001).
The results areplotted in Figures 1 and 2.0.550.60.650.70.750.80.850 0.2 0.4 0.6 0.8 1F-scorefor ACE04Development DataProportion of twinless system mentions used in the experimentMUCBCubedsysBCubed0BCubedallBCubedngFigure 1: Artificial Setting B3 Variants0.40.450.50.550.60.650.70.750.80 0.2 0.4 0.6 0.8 1F-scorefor ACE04Development DataProportion of twinless system mentions used in the experimentMUCCEAFsysCEAForigCEAFngFigure 2: Artificial Setting CEAF Variants33MUCR Pr FSoon (SM1) 51.7 53.1 52.4Soon (SM2) 49.1 69.9 57.7Table 8: Realistic Setting MUCOmitting twinless system mentions from thetraining data while keeping the number of cor-rect mentions constant should improve the corefer-ence resolution performance, because a more pre-cise coreference resolution model is obtained.
Ascan be seen from Figures 1 and 2, the MUC-score,B3sys and CEAFsys follow this intuition.B30 is almost constant.
It does not take twinlessmentions into account.
B3all?s curve, also, has alower slope in comparison to B3sys and MUC (i.e.B3all computes similar numbers for worse models).This shows that the B3all score can be tricked byusing a high recall mention tagger, e.g.
in caseswith the worse models (i.e.
ones on the left sideof the figures) which have much more twinlesssystem mentions.
The original CEAF algorithm,CEAForig, is too sensitive to the input systemmentions making it less reliable.
CEAFsys is par-allel to B3sys.
Thus both of our metrics exhibit thesame intuition.3.3 Realistic Setting3.3.1 Experiment 1For the realistic setting we compare SM1 and SM2as preprocessing components for the BART (Ver-sley et al, 2008) reimplementation of Soon et al(2001).
The coreference resolution system withthe SM2 tagger performs better, because a bettercoreference model is achieved from system men-tions with higher accuracy.The MUC, B3sys and CEAFsys metrics have thesame tendency when applied to systems with dif-ferent mention taggers (Table 8, 9 and 10 and thebold numbers are higher with a p-value of 0.05,by a paired-t test).
Since the MUC scorer doesnot evaluate singleton entities, it produces too lownumbers which are not informative any more.As shown in Table 9, B3all reports counter-intuitive results when a system is fed with systemmentions generated by different mention taggers.B3all cannot be used to evaluate two different end-to-end coreference resolution systems, because themention tagger is likely to have bigger impact thanthe coreference resolution system.
B30 fails to gen-erate the right comparison too, because it is tooB3sys B30R Pr F R Pr FSoon (SM2) 64.1 87.3 73.9 54.7 91.3 68.4Bengtson 66.1 81.9 73.1 69.5 74.7 72.0Table 11: Realistic Settinglenient by ignoring all twinless mentions.The CEAForig numbers in Table 10 illustrate thebig influence the system mentions have on preci-sion (e.g.
the very low precision number for Soon(SM1)).
The big improvement for Soon (SM2) islargely due to the system mentions it uses, ratherthan to different coreference models.Both B3r&n and CEAFr&n show no serious prob-lems in the experimental results.
However, as dis-cussed before, they fail to penalize the spuriousentities with twinless system mentions adequately.3.3.2 Experiment 2We compare results of Bengtson & Roth?s (2008)system with our Soon (SM2) system.
Bengtson &Roth?s embedded mention tagger aims at high pre-cision, generating half of the mentions SM1 gen-erates (explicit statistics are not available to us).Bengtson & Roth report a B3 F-score for sys-tem mentions, which is very close to the one fortrue mentions.
Their B3-variant does not imputeerrors of twinless mentions and is assumed to bequite similar to the B30 strategy.We integrate both the B30 and B3sys variants intotheir system and show results in Table 11 (we can-not report significance, because we do not have ac-cess to results for single documents in Bengtson &Roth?s system).
It can be seen that, when differentvariants of evaluation metrics are applied, the per-formance of the systems vary wildly.4 ConclusionsIn this paper, we address problems of commonlyused evaluation metrics for coreference resolutionand suggest two variants for B3 and CEAF, calledB3sys and CEAFsys.
In contrast to the variantsproposed by Stoyanov et al (2009), B3sys andCEAFsys are able to deal with end-to-end systemswhich do not use any gold information.
The num-bers produced by B3sys and CEAFsys are able toindicate the resolution performance of a systemmore adequately, without being tricked easily bytwisting preprocessing components.
We believethat the explicit description of evaluation metrics,as given in this paper, is a precondition for the re-34B3sys B30 B3all B3r&nR Pr F R Pr F R Pr F R Pr FSoon (SM1) 65.7 76.8 70.8 57.0 91.1 70.1 65.1 85.8 74.0 65.1 78.7 71.2Soon (SM2) 64.1 87.3 73.9 54.7 91.3 68.4 64.3 87.1 73.9 64.3 84.9 73.2Table 9: Realistic Setting B3 VariantsCEAFsys CEAForig CEAFr&nR Pr F R Pr F R Pr FSoon (SM1) 66.4 61.2 63.7 62.0 39.9 48.5 62.1 59.8 60.9Soon (SM2) 67.4 65.2 66.3 60.0 56.6 58.2 60.0 66.2 62.9Table 10: Realistic Setting CEAF Variantsliabe comparison of end-to-end coreference reso-lution systems.Acknowledgements.
This work has beenfunded by the Klaus Tschira Foundation, Hei-delberg, Germany.
The first author has beensupported by a HITS PhD.
scholarship.
Wewould like to thank ?Eva Mu?jdricza-Maydt forimplementing the mention taggers.ReferencesBagga, Amit & Breck Baldwin (1998).
Algorithms for scor-ing coreference chains.
In Proceedings of the 1st Inter-national Conference on Language Resources and Evalua-tion, Granada, Spain, 28?30 May 1998, pp.
563?566.Bengtson, Eric & Dan Roth (2008).
Understanding the valueof features for coreference resolution.
In Proceedings ofthe 2008 Conference on Empirical Methods in NaturalLanguage Processing, Waikiki, Honolulu, Hawaii, 25-27October 2008, pp.
294?303.Culotta, Aron, Michael Wick & Andrew McCallum (2007).First-order probabilistic models for coreference resolu-tion.
In Proceedings of Human Language Technologies2007: The Conference of the North American Chapter ofthe Association for Computational Linguistics, Rochester,N.Y., 22?27 April 2007, pp.
81?88.Denis, Pascal & Jason Baldridge (2007).
Joint determinationof anaphoricity and coreference resolution using integerprogramming.
In Proceedings of Human Language Tech-nologies 2007: The Conference of the North AmericanChapter of the Association for Computational Linguistics,Rochester, N.Y., 22?27 April 2007, pp.
236?243.Haghighi, Aria & Dan Klein (2007).
Unsupervised coref-erence resolution in a nonparametric Bayesian model.
InProceedings of the 45th Annual Meeting of the Associationfor Computational Linguistics, Prague, Czech Republic,23?30 June 2007, pp.
848?855.Luo, Xiaoqiang (2005).
On coreference resolution perfor-mance metrics.
In Proceedings of the Human LanguageTechnology Conference and the 2005 Conference on Em-pirical Methods in Natural Language Processing, Vancou-ver, B.C., Canada, 6?8 October 2005, pp.
25?32.Luo, Xiaoqiang, Abe Ittycheriah, Hongyan Jing, NandaKambhatla & Salim Roukos (2004).
A mention-synchronous coreference resolution algorithm based onthe Bell Tree.
In Proceedings of the 42nd Annual Meet-ing of the Association for Computational Linguistics,Barcelona, Spain, 21?26 July 2004, pp.
136?143.Mitchell, Alexis, Stephanie Strassel, Shudong Huang &Ramez Zakhary (2004).
ACE 2004 Multilingual TrainingCorpus.
LDC2005T09, Philadelphia, Penn.
: LinguisticData Consortium.Nicolae, Cristina & Gabriel Nicolae (2006).
BestCut: Agraph algorithm for coreference resolution.
In Proceed-ings of the 2006 Conference on Empirical Methods in Nat-ural Language Processing, Sydney, Australia, 22?23 July2006, pp.
275?283.Ponzetto, Simone Paolo & Michael Strube (2006).
Exploitingsemantic role labeling, WordNet and Wikipedia for coref-erence resolution.
In Proceedings of the Human LanguageTechnology Conference of the North American Chapter ofthe Association for Computational Linguistics, New York,N.Y., 4?9 June 2006, pp.
192?199.Rahman, Altaf & Vincent Ng (2009).
Supervised models forcoreference resolution.
In Proceedings of the 2009 Con-ference on Empirical Methods in Natural Language Pro-cessing, Singapore, 6-7 August 2009, pp.
968?977.Rand, William R. (1971).
Objective criteria for the evaluationof clustering methods.
Journal of the American StatisticalAssociation, 66(336):846?850.Recasens, Marta & Eduard Hovy (2010).
BLANC: Imple-menting the Rand index for coreference evaluation.
Sub-mitted.Soon, Wee Meng, Hwee Tou Ng & Daniel Chung YongLim (2001).
A machine learning approach to coreferenceresolution of noun phrases.
Computational Linguistics,27(4):521?544.Stoyanov, Veselin, Nathan Gilbert, Claire Cardie & EllenRiloff (2009).
Conundrums in noun phrase coreferenceresolution: Making sense of the state-of-the-art.
In Pro-ceedings of the Joint Conference of the 47th Annual Meet-ing of the Association for Computational Linguistics andthe 4th International Joint Conference on Natural Lan-guage Processing, Singapore, 2?7 August 2009, pp.
656?664.Versley, Yannick, Simone Paolo Ponzetto, Massimo Poesio,Vladimir Eidelman, Alan Jern, Jason Smith, XiaofengYang & Alessandro Moschitti (2008).
BART: A modulartoolkit for coreference resolution.
In Companion Volumeto the Proceedings of the 46th Annual Meeting of the As-sociation for Computational Linguistics, Columbus, Ohio,15?20 June 2008, pp.
9?12.Vilain, Marc, John Burger, John Aberdeen, Dennis Connolly& Lynette Hirschman (1995).
A model-theoretic corefer-ence scoring scheme.
In Proceedings of the 6th MessageUnderstanding Conference (MUC-6), pp.
45?52.
San Ma-teo, Cal.
: Morgan Kaufmann.Witten, Ian H. & Eibe Frank (2005).
Data Mining: PracticalMachine Learning Tools and Techniques (2nd ed.).
SanFrancisco, Cal.
: Morgan Kaufmann.Yang, Xiaofeng, Jian Su & Chew Lim Tan (2008).
A twin-candidate model for learning-based anaphora resolution.Computational Linguistics, 34(3):327?356.35A B3sys Example OutputHere, we provide additional examples for analyzing the behavior of B3sys where we systematically varysystem outputs.
Since we proposed B3sys for dealing with end-to-end systems, we consider only examplesalso containing twinless mentions.
The systems in Table 12 and 14 generate different twinless keymentions while keeping the twinless system mentions untouched.
In Table 13 and 15, the number oftwinless system mentions changes through different responses and the number of twinless key mentionsis fixed.In Table 12, B3sys recall goes up when more key mentions are resolved into the correct set.
And theprecision stays the same, because there is no change in the number of the erroneous resolutoins (i.e.
thespurious cluster with mentions i and j).
For the examples in Tables 13 and 15, B3sys gives worse precisionto the outputs with more spurious resolutions, and the same recall if the systems resolve key mentions inthe same way.
Since the set of key mentions intersects with the set of twinless system mentions in Table14, we do not have an intuitive explanation for the decrease in precision from response1 to response4.However, both the F-score and the recall still show the right tendency.Set 1 Set 2 B3syskey {a b c d e} P R Fresponse1 {a b} {i j} 0.857 0.280 0.422response2 {a b c} {i j} 0.857 0.440 0.581response3 {a b c d} {i j} 0.857 0.68 0.784response4 {a b c d e} {i j} 0.857 1.0 0.923Table 12: Analysis of B3sys 1Set 1 Set 2 B3syskey {a b c d e} P R Fresponse1 {a b c} {i j} 0.857 0.440 0.581response2 {a b c} {i j k} 0.75 0.440 0.555response3 {a b c} {i j k l} 0.667 0.440 0.530response4 {a b c} {i j k l m} 0.6 0.440 0.508Table 13: Analysis of B3sys 2Set 1 B3syskey {a b c d e} P R Fresponse1 {a b i j} 0.643 0.280 0.390response2 {a b c i j} 0.6 0.440 0.508response3 {a b c d i j} 0.571 0.68 0.621response4 {a b c d e i j} 0.551 1.0 0.711Table 14: Analysis of B3sys 3Set 1 B3syskey {a b c d e} P R Fresponse1 {a b c i j} 0.6 0.440 0.508response2 {a b c i j k} 0.5 0.440 0.468response3 {a b c i j k l} 0.429 0.440 0.434response4 {a b c i j k l m} 0.375 0.440 0.405Table 15: Analysis of B3sys 436
