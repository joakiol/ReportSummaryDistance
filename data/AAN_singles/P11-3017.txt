Proceedings of the ACL-HLT 2011 Student Session, pages 94?98,Portland, OR, USA 19-24 June 2011. c?2011 Association for Computational LinguisticsTurn-Taking Cues in a Human Tutoring CorpusHeather FriedbergDepartment of Computer ScienceUniversity of PittsburghPittsburgh, PA, 15260, USAfriedberg@cs.pitt.eduAbstractMost spoken dialogue systems are stilllacking in their ability to accurately modelthe complex process that is human turn-taking.
This research analyzes a human-human tutoring corpus in order to identifyprosodic turn-taking cues, with the hopesthat they can be used by intelligent tutoringsystems to predict student turn boundaries.Results show that while there was variationbetween subjects, three features were sig-nificant turn-yielding cues overall.
In addi-tion, a positive relationship between thenumber of cues present and the probabilityof a turn yield was demonstrated.1 IntroductionHuman conversation is a seemingly simple, every-day phenomenon that requires a complex mentalprocess of turn-taking, in which participants man-age to yield and hold the floor with little pause in-between speaking turns.
Most linguists subscribeto the idea that this process is governed by a sub-conscious internal mechanism, that is, a set of cuesor rules that steers humans toward proper turn-taking (Duncan, 1972).
These cues may includelexical features such as the words used to end theturn, or prosodic features such as speaking rate,pitch, and intensity (Cutler and Pearson, 1986).While successful turn-taking is fairly easy forhumans to accomplish, it is still difficult for mod-els to be implemented in spoken dialogue sys-tems.
Many systems use a set time-out to decidewhen a user is finished speaking, often resulting inunnaturally long pauses or awkward overlaps(Ward, et.
al., 2005).
Others detect when a userinterrupts the system, known as ?barge-in?, thoughthis is characteristic of failed turn-taking ratherthan successful conversation (Glass, 1999).Improper turn-taking can often be a source of us-er discomfort and dissatisfaction with a spokendialogue system.
Little work has been done tostudy turn-taking in tutoring, so we hope to inves-tigate it further while using a human-human (HH)tutoring corpus and language technologies to ex-tract useful information about turn-taking cues.This analysis is particularly interesting in a tutor-ing domain because of the speculated unequal sta-tuses of participants.
The goal is to eventuallydevelop a model for turn-taking based on this anal-ysis which can be implemented in an existent tutor-ing system, ITSPOKE, an intelligent tutor forcollege-level Newtonian physics (Litman and Sil-liman, 2004).
ITSPOKE currently uses a time-outto determine the end of a student turn and does notrecognize student barge-in.
We hypothesize thatimproving upon the turn-taking model this systemuses will help engage students and hopefully leadto increased student learning, a standard perfor-mance measure of intelligent tutoring systems(Litman et.
al., 2006).2 Related WorkTurn-taking has been a recent focus in spoken di-alogue system work, with research producingmany different models and approaches.
Raux andEskenazi (2009) proposed a finite-state turn-taking94model, which is used to predict end-of-turn andperformed significantly better than a fixed-threshold baseline in reducing endpointing latencyin a spoken dialogue system.
Selfridge and Hee-man (2010) took a different approach and pre-sented a bidding model for turn-taking, in whichdialogue participants compete for the turn based onthe importance of what they will say next.Of considerable inspiration to the research in thispaper was Gravano and Hirschberg?s (2009) analy-sis of their games corpus, which showed that it waspossible for turn-yielding cues to be identified inan HH corpus.
A similar method was used in thisanalysis, though it was adapted based on the toolsand data that were readily available for our corpus.Since these differences may prevent direct compar-ison between corpora, future work will focus onmaking our method more analogous.Since our work is similar to that done by Grava-no and Hirschberg (2009), we hypothesize thatturn-yielding cues can also be identified in our HHtutoring corpus.
However, it is possible that thecues identified will be very different, due to factorsspecific to a tutoring environment.
These include,but are not limited to, status differences betweenthe student and tutor, engagement of the student,and the different goals of the student and tutor.Our hypothesis is that for certain prosodic fea-tures, there will be a significant difference betweenplaces where students yield their turn (allow thetutor to speak) and places where they hold it (con-tinue talking).
This would designate these featuresas turn-taking cues, and would allow them to beused as features in a turn-taking model for a spo-ken dialogue system in the future.3 MethodThe data for this analysis is from an HH tutoringcorpus recorded during the 2002-2003 schoolyear.
This is an audio corpus of 17 university stu-dents, all native Standard English speakers, work-ing with a tutor (the same for all subjects) onphysics problems (Litman et.
al., 2006).
Both thestudent and the tutor were sitting in front of sepa-rate work stations, so they could communicate onlythrough microphones or, in the case of a student-written essay, through the shared computer envi-ronment.
Any potential turn-taking cues that thetutor received from the student were very compa-rable to what a spoken dialogue system would haveto analyze during a user interaction.For each participant, student speech was iso-lated and segmented into breath groups.
A breathgroup is defined as any segment of speech by onedialogue participant bounded by 200 ms of silenceor more based on a certain threshold of intensity(Liscombe et.
al., 2005).
This break-down allowedfor feature measurement and comparison at placesthat were and were not turn boundaries.
AlthoughGravano and Hirschberg (2009) segmented theircorpus by 50 ms of silence, we used 200 ms to di-vide the breath groups, as this data had alreadybeen calculated for another experiment done withthe HH corpus (Liscombe et.
al., 2005).
1Figure 1.
Conversation Segmented into Breath GroupsEach breath group was automatically labeled asone of the following: HOLD, when a breath groupwas immediately followed by a second breathgroup from the same person, YIELD, when abreath group was immediately followed by speechfrom the other participant, or OVERLAP, whenspeech from another participant started before thecurrent one ended.
Figure 1 is a diagram of a hy-pothetical conversation between two participants,with examples of HOLD?s and YIELD?s labeled.These groups were determined strictly by time andnot by the actually speech being spoken.
Speechacts such as backchannels, then, would be includedin the YIELD group if they were spoken duringclear gaps in the tutor?s speech, but would beplaced in the OVERLAP group if they occurredduring or overlapping with tutor speech.
Therewere 9,169 total HOLD's in the corpus and 4,773YIELD?s; these were used for comparison, whilethe OVERLAP?s were set aside for future work.Four prosodic features were calculated for eachbreath group: duration, pitch, RMS, and percentsilence.
Duration is the length of the breath groupin seconds.
Pitch is the mean fundamental fre-quency (f0) of the speech.
RMS (the root mean1 Many thanks to the researchers at Columbia University forproviding the breath group data for this corpus.95squared amplitude) is the energy or loudness.
Per-cent silence was the amount of internal silencewithin the breath group.
For pitch and RMS, themean was taken over the length of the breathgroup.
These features were used because they aresimilar to those used by Gravano and Hirschberg(2009), and are already used in the spoken dialo-gue system we will be using (Forbes-Riley andLitman, 2011).
While only a small set of featuresis examined here, future work will include expand-ing the feature set.Mean values for each feature for HOLD?s andYIELD?s were calculated and compared using thestudent T-test in SPSS Statistics software.
Twoseparate tests were done, one to compare themeans for each student individually, and one tocompare the means across all students.
p ?
.05 isconsidered significant for all statistical tests.
Thep-values given are the probability of obtaining thedifference between groups by chance.4 Results4.1 Individual CuesFirst, means for each feature for HOLD?s andYIELD?s were compared for each subject indivi-dually.
These individual results indicated thatwhile turn-taking cues could be identified, therewas much variation between students.
Table 1displays the results of the analysis for one subject,student 111.
For this student, all four prosodic fea-tures are turn-taking cues, as there is a significantdifferent between the HOLD and YIELD groupsfor all of them.
However, for all other students,this was not the case.
As shown in Table 3, mul-tiple significant cues could be identified for moststudents, and there was only one which appeared tohave no significant turn-yielding cues.Because there was so much individual variation,a paired T-test was used to compare the meansacross subjects.
In this analysis, duration, pitch,and RMS were all found to be significant cues.Percent silence, however, was not.
The results ofthis test are summarized in Table 2.
A more de-tailed look at each of the three significant cues isdone below.Number ofSignificant CuesNumber ofStudents0 11 02 63 94 1Table 3.
Number of Students withSignificant CuesDuration: The mean duration for HOLD?s islonger than the mean duration for YIELD?s.
Thissuggests that students speak for a longer uninter-rupted time when they are trying to hold their turn,and yield their turns with shorter utterances.
This isthe opposite of Gravano and Hirschberg?s (2009)results, which found that YIELD?s were longer.Pitch: The mean pitch for YIELD?s is higherthan the mean pitch for HOLD?s.
Gravano andHirschberg (2009), on the other hand, found thatYIELD?s were lower pitched than HOLD?s.
Thisdifference may be accounted for by the differencein tasks.
During tutoring, students are possiblyN duration percent silence pitch RMSHOLD Group Mean 993 1.07 0.34 102.24 165.27YIELD Group Mean 480 0.78 0.39 114.87 138.89Significance  * p = 0.018 * p < 0.001 * p < 0.001 * p < 0.001Table 1.
Individual Results for Subject 111* denotes a significant p valueN duration percent silence pitch RMSHOLD Group Mean 17 1.49 0.300 140.44 418.00YIELD Group Mean 17 0.82 0.310 147.58 354.65Significance  * p = 0.022 p = 0.590 * p = 0.009 * p < 0.001Table 2.
Results from Paired T-Test96more uncertain, which may raise the mean pitch ofthe YIELD breath groups.RMS: The mean RMS, or energy, for HOLD?s ishigher than the mean energy for YIELD?s.
This isconsistent with student?s speaking more softly, i.e.,trailing off, at the end of their turn, a usual pheno-menon in human speech.
This is consistent withthe results from the Columbia games corpus (Gra-vano and Hirshberg, 2009).4.2 Combining CuesGravano and Hirschberg (2009) were able to showusing their cues and corpus that there is a positiverelationship between the number of turn-yieldingcues present and the probability of a turn actuallybeing taken.
This suggests that in order to makesure that the other participant is aware whether theturn is going to continue or end, the speaker maysubconsciously give them more informationthrough multiple cues.To see whether this relationship existed in ourdata, each breath group was marked with a binaryvalue for each significant cue, representing wheth-er the cue was present or not present within thatbreath group.
A cue was considered present if thevalue for that breath group was strictly closer tothe student?s mean for YIELD?s thanHOLD?s.
The number of cues present for eachbreath group was totaled.
Only the three cuesfound to be significant cues were used for thesecalculations.
For each number of cues possible x(0 to 3, inclusively), the probability of the turn be-ing taken was calculated by p(x) = Y / T, where Y isthe number of YIELD?s with x cues present, and Tis the total number of breath groups with x cuespresent.Figure 2.
Cues Present v. Probability of YIELDAccording to these results, a positive relationshipseems to exist for these cues and this corpus.
Fig-ure 2 shows the results plotted with a fitted regres-sion.
The number of cues present and probabilityof a turn yield is strongly correlated (r = .923,p=.038).
A regression analysis done using SPSSshowed that the adjusted r2 = .779 (p = .077).When no turn-yielding cues are present, there isstill a majority chance that the student will yieldtheir turn; however, this is understandable due tothe small number of cues being analyzed.
Regard-less, this gives a very preliminary support for theidea that it is possible to predict when a turn willbe taken based on the number of cues present.5 ConclusionsThis paper presented preliminary work in using anHH tutoring corpus to construct a turn-taking mod-el that can later be implemented in a spoken dialo-gue system.
A small set of prosodic features wasused to try and identify turn-taking cues by com-paring their values at places where students yieldedtheir turn to the tutor and places where they heldit.
Results show that turn-taking cues such as thoseinvestigated can be identified for the corpus, andmay hold predictive ability for turn boundaries.5.1 Future WorkWhen building on this work, there are two differ-ent directions in which we can go.
While thiswork uncovers some interesting results in the tutor-ing domain, there are some shortcomings in themethod that may make it difficult to effectivelyevaluate the results.
As the breath group is differ-ent from the segment used in Gravano and Hir-schberg?s (2009) experiment, and the set ofprosodic features is smaller, direct comparison be-comes quite difficult.
The differences between thetwo methods provide enough doubt for the resultsto truly be interpreted as contradictory.
Thus thefirst line of future inquiry is to redo this methodusing a smaller silence boundary (50 ms) and dif-ferent set of prosodic features so that it is trulycomparable to Gravano and Hirschberg?s (2009)work with the game corpus.
This could yield in-teresting discoveries in the differences between thetwo corpora, shedding light on phenomena that areparticular to tutoring scenarios.On the other hand, other researchers have useddifferent segments; for example, Clemens and Di-ekhaus (2009) divide their corpus by ?topic units?that are grammatically and semantically complete.In addition, Litman et.
al.
(2009) were able to use60.00%70.00%80.00%90.00%100.00%0 1 2 397word-level units to calculate prosody and classifyturn-level uncertainty.
Perhaps direct comparisonis not entirely necessary, and instead this workshould be considered an isolated look at an HHcorpus that provides insight into turn-taking, spe-cifically in tutoring and other domains with un-equal power levels.
Future work in this directionwould include growing the set of features by add-ing more prosodic ones and introducing lexicalones such as bi-grams and uni-grams.
Already,work has been done to investigate the features usedin the INTERSPEECH 2009 Emotion Challengeusing openSMILE (Eyben et.
al., 2009).
When alarge feature bank has been developed, significantcues will be used in conjunction with machinelearning techniques to build a model for turn-taking which can be implemented in a spoken di-alogue tutoring system.
The goal would be to learnmore about human turn-taking while seeing if bet-ter turn-taking by a computer tutor ultimately leadsto increased student learning in an intelligent tutor-ing system.AcknowledgmentsThis work was supported by the NSF (#0631930).I would like to thank Diane Litman, my advisor,Scott Silliman, for software assistance, JoannaDrummond, for many helpful comments on thispaper, and the ITSPOKE research group for theirfeedback on my work.ReferencesCaroline Clemens and Christoph Diekhaus.
2009.
Pro-sodic turn-yielding Cues with and without opticalFeedback.
In Proceedings of the SIGDIAL 2009 Con-ference: The 10th Annual Meeting of the Special In-terest Group on Discourse and Dialogue.Association for Computational Linguistics.Anne Cutler and Mark Pearson.
1986.
On the analysisof prosodic turn-taking cues.
In C. Johns-Lewis, Ed.,Intonation in Discourse, pp.
139-156.
College-Hill.Starkey Duncan.
1972.
Some signals and rules for tak-ing speaking turns in conversations.
Journal of Per-sonality and Social Psychology, 24(2):283-292.Florian Eyben, Martin W?llmer, Bj?rn Schuller.
2010.openSMILE - The Munich Versatile and Fast Open-Source Audio Feature Extractor.
Proc.
ACM Multi-media (MM), ACM, Florence, Italy.
pp.
1459-1462.James R. Glass.
1999.
Challenges for spoken dialoguesystems.
In Proceedings of the 1999 IEEE ASRUWorkshop.Agust?n Gravano and Julia Hirschberg.
2009.
Turn-yielding cues in task-oriented dialogue.
In Proceed-ings of the SIGDIAL 2009 Conference: The 10th An-nual Meeting of the Special Interest Group onDiscourse and Dialogue, 253--261.
Association forComputational Linguistics.Jackson Liscombe, Julia Hirschberg, and Jennifer J.Venditti.
2005.
Detecting certainness in spoken tu-torial dialogues.
In Interspeech.Diane J. Litman, Carolyn P. Rose, Kate Forbes-Riley,Kurt VanLehn, Dumisizwe Bhembe, and Scott Silli-man.
2006.
Spoken Versus Typed Human and Com-puter Dialogue Tutoring.
In International Journal ofArtificial Intelligence in Education, 26: 145-170.Diane Litman, Mihai Rotaru, and Greg Nicholas.2009.
Classifying Turn-Level Uncertainty UsingWord-Level Prosody.
Proceedings Interspeech,Brighton, UK, September.Kate Forbes-Riley and Diane Litman.
2011.
Benefitsand Challenges of Real-Time Uncertainty Detectionand Adaptation in a Spoken Dialogue Computer Tu-tor.
Speech Communication, in press.Diane J. Litman and Scott Silliman.
2004.
Itspoke: Anintelligent tutoring spoken dialogue system.
InHLT/NAACL.Antoine Raux and Maxine Eskenazi.
2009.
A finite-stateturn-taking model for spoken dialog systems.
InProc.
NAACL/HLT 2009, Boulder, CO, USA.Ethan O. Selfridge and Peter A. Heeman.
2010.
Impor-tance-Driven Turn-Bidding for spoken dialogue sys-tems.
In Proceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics (ACL'10).
Association for Computational Linguistics,Stroudsburg, PA, USA, 177-185.Nigel Ward, Anais Rivera, Karen Ward, and David G.Novick.
2005.
Root causes of lost time and userstress in a simple dialog system.
In Proceedings ofInterspeech.98
