A Stat i s t i ca l  Approach  to Sense D isambiguat ion  inMach ine  Trans la t ionPeter F. Brown, Stepheu A. Della.
Pietra, Vince,,t d. Della Pietra, Robert L. Mercer?
-' S IBM Resea,rch Division, qlhorns,: J. Wa, tson Resea,rch CenterYorktown Heights, NY 10598ABSTRACTWe describe ~ statisticM technique for assigning sensesto words.
An instance of ~ word is assigned ;~ sense byasking a question about the context in which the word~tppears.
The qttestlou is constructed to ha, re high mutua,1i~fformation with the word's translations.INTRODUCTIONAn a,lluring a,spect of the staMstica,1 a,pproa,ch toins,chine tra,nsla,tion rejuvena.ted by Brown, et al, \[_1\]is the systems.tic framework it provides for a.tta.ck-ing the problem of lexicM dis~tmbigua.tion.
For exam-ple, the system they describe tra,ns\]a.tes th.e Frenchsentence Je vais prendre la ddeision a,s \[ will makethe decision, thereby correctly interpreting prendre a.smake, The staMstica.l tra.nslation model, which sup-plies English.
tra,nsla,tions of French words, prefers themore common tra.nslation take, but the trigram la.n-gu.age mode\] recognizes tha.t the three-word sequencemake the decision is much more proba\])le tha.n takethe decision.The system is not a.lwa,ys o successful.
It incor-rectly renders Je vats prendre ma propre ddcision a.s 1will take my own decision.
Here, the la.nguage modeldoes not realize tha, t take my own decision is improb-able beca,use take a,nd decision no longer fall within a.single trigram.Errors such a.s this a,re common because otlr sta,-tistical models o. ly capture loca,l phenomena,; if l, hecontext necessa,ry to determine ~ transla, tion fa,lls out-side the scope of our models, the word is likely to betra,nsla,ted incorrectly.
However, if the re\]evant co.-text is encoded locally, the word should be tra, nsla, tedcorrectly.
We ca,n a,chieve this within the traditionMp,~radigm of a.na,lysis - tra,nsfer - synthesis by incorpo-ra,ting into the ana,lysis pha,se a, sense--disa, mbigu~tioncompo,ent hat assigns sense la, bels to French words.\]if prendre is labeled with one sense in the context ofddcisiou but wil.h a, different sense in other contexts,then the tra,nsla,tion model will learn from trainingdata tha,t the first sense usua,lly tra.nslates to make,where.a,s the other sense usua,lly tra.nslates to take.In this paper, we describe a. sta, tistica,1 procedurefor constructing a. sense-disambiguation eomponentthat label words so as to elucida.te their translations.STAT IST ICAL  TRANSLAT IONAs described by Brown, et al \[\]\], in the sta.tistica.1a.l)proa.ch to transla, tion, one chooses for tile tra,nsla,-tion of a. French sentence .F, tha.t English sentence Ewhich ha.s the greatest l)robability, P r (E IF ) ,  a.ccord-i ,g to a, model of th.e tra, ns\]ation process.
By Ba.yes'r,,le, Pr(EI ~') = Pr(E) Pr(FIE ) /Pr( .F).
Since the(lenomina.tor does not del)end on E, the sentence forwhich Pr (E IF  ) is grea, test is also the sentence forwhich the product P r (E )  P r (F IE  ) is grea~test.
Thefirst term in this product is a~ sta, tisticM cha.ra.cteriza-tion of the, English \]a.nguage a,nd the second term isa, statistical cha.ra.cteriza,timt of the process by whichEnglish sentences are tra.nslated into French.
We cancompute neither of these probabilities precisely.
Rather,in statistical tra.nslat, iou, we employ a. language modelP,,od~l(E) which 1)rovide, s a,n estima.te of Pr (E) and a,lrav, slatiov, model which provides a,n estimate oft'r ( Vl/~:).146The performance of the system depends on theextent to which these statistical models approximatethe actual probabilities.
A useful gauge of this is tilecross entropy 1H(EIF)-= - ~ Pr (E ,F )  log PmoZ~,(EI F) (1)E,Fwhich measures the average uncertainty that the modelhas about the English translation E of a French sen-tence F. A better model has less uncertainty and thusa lower cross entropy.A shortcoming of the architecture described aboveis that it requires the statistical models to deal di-rectly with English and French sentences.
Clearly theprobability distributions P r (E )  and Pr (F IE  ) oversentences are immensely complicated.
On the otherhand, in practice the statistical models must be rela-tively simple in order that their parameters can be re-liably estimated from a manageable amount of train-ing data.
This usually means that they are restrictedto the modeling of local linguistic phenonrena.
As a.result, the estimates Pmodcz(E) and Pmodd(F I E) willbe inaccurate.This difficulty can be addressed by integrating sta-tistical models into the traditional machine transla-tion architecture of analysis-transfer-synthesis.
Theresulting system employs1.
An analysis component which encodes a Frenchsentence F into an intermediate structure F<2.
A statistical transfer component which trans-lates F t a corresponding intermediate Englishstructure E'.
This component incorporates alanguage model, a translation model, and a de-coder as before, but here these components dealwith the intermediate structures rather than thesentences directly.3.
A synthesis component which reconstructs anEnglish sentence E from E t.For statistical modeling we require that the synthe-sis transformation E ~ ~ E be invertible.
Typically,analysis and synthesis will involve a sequence of suc-cessive transformations in which F p is incrementallytin this equation and in the remainder of the paper, we usebold face letters (e.g.
E) for random variables and roman letters(e.g.
E) for the values of random variables.constructed from F, or E is incrementally recoveredfrom E I.
'File purpose of analysis and synthesis is to facili-tate the task of statistical transfer.
This will be thecase if the probability distribution Pr (E ~, F ~) is eas-ier to model then the original distribution Pr (E, F).In practice this nleans that E'  and F'  should encodeglobal linguistic facts about E and F in a local form.The utility of tile analysis and synthesis transfor-matious can be measured in terms of cross-entropy.Thus transfotma.tions F -+ F' and t~/ ---+ E are use-ful if we Call construct models ' P ,~od~t( F I E') andP',,,oa+,(E') such that H(E '  I r ' )  < H(E IF  ).SENSE D ISAMBIGUATIONIn this paper we present a statistical method forautomatically constructing analysis and synthesis trans-formations which perform cross-lingual word-sense labeling.The goal of such transformations i  to label the wordsof a French sentence so as to ehlcidate their English.trauslations, and, conversely, to label the words of anEnglish sentence so as to elucidate their French trans-lations.
For exa.mple, in some contexts the Frenchverb prendre translates as to take, but in other con-texts it translates as to make.
A sense disambiguationtransformation, by examining the contexts, might la-bel occurrences of prendre that likely mean to takewith one lal)el, and other occurrences of prendre withanother label.
Then the uncertainty in the transla-tion of prendre given the label would be less than theuncertainty in the translation of prendre without thelabel.
All, hough tile label does not provide any infofmation that is not already present in the context, itencodes this information locally.
Thus a local statisti-cal model for the transfer of labeled sentences houldbe more accurate than one for the transfer of unla-l)eled ones.While the translation o:f a word depends on manywoMs in its context, we can often obtain informationby looking at only a single word.
For example, in thesentence .Ic vats prendre ma propre ddeision (I will'make my own decisiou), tile verb prendre should betranslated as make because its object is ddcision.
Ifwe replace ddcision by voiture then prendre should betranslated as take: Je vais prendre ma propre voiture(l will take my own car).
Thus we can reduce theuncertainity in the translation of prendre by asking aquestion about its object, which is often the first noun147to its right, and we might assign a sense to prendrebased upon the answer to this question.In It doute que Ins ndtres gagnent (He doubts thatwe will win), the word il should be translated as he.On the other hand, if we replace doute by faut thenil should be translated as it: It faut que les nStresgagnent (It is necessary that we win).
Here, we mightassign a sense label to il by asking a,bout the identityof the first verb to its right.These examples motivate a. sense-labeling schemein which the la.bel of a word is determined by a ques-tion aJ)out an informant word in its context.
In thefirst example, the informant of prendre is the firstnoun to the right; in.
the second example, the infofmant of ilis the first verb to the right.
If we wantto assign n senses to a word then we can consider aquestion with n answers.We can fit this scheme into the fl:amework of theprevious section a.s follows:The Intermediate Structures.
The intermediate struc-tures E'  and F r consist of sequences of wordslabeled by their senses.
Thus F'  is a sentenceover the expanded vocabulary whose 'words' f 'are pairs ( f , l )  where f is a word in the origi-nal French vocabulary and 1 is its sense label.Similarly, E ?
is a sentence over the expandedvocabulary whose words e t are pairs (e, l) wheree is a.n English word and l is its sense label.The analysis and synthesis transformations.
For eachFrench word and each English word we choosean informant site, such as first noun to the left,and an n-ary question about the va,lue of the in-formant at that site.
The analysis transforma-tion F ~ U and the inverse synthesis transfofmarion E ~ E ~ map a sentence to the interme-diate structure in which each word is labeled bya sense determined by the question a\])out its in-formant.
The synthesis transformation E ~ ~ Emaps a labeled sentence to a sentence in whichthe labels have been removed.The probability models.
We use the translation modelthat was discussed in \[l\] for bothe ;~oaet (F ' lE ' )  and for P,nodd(FIE).
We usea trigram language model.
\[1\] for P,,~oa~a(E) andIn order to construct hese tra.nsformations we needto choose for each English and French word a.n infor-mant and a question.
As suggested in the previoussection, a criterion for doing this is that of minimiz-ing the (:ross entropy H(E '  I F').
In the remainder ofthe l)aper we present an algorithm for doing this.THE TRANSLAT ION MODELWe begin by reviewing our statistical model for thetranslation of a sentence from one language to another\[1\].
In statistical French to English translation system.we need to model transformations from English sen-tences E to French sentences F, or from intermediateEnglish structures E' to intermediate French struc-tures F t. ltowever, it is clarifying to consider trans-formations from an arbitrary source language to anarbitrary target language.Rev iew of  the Mode lThe l)urpose of a translation model is to computethe prol)al)i\]ity P,,odet(T \[ S) of transforming a sourcesentence S into a. target sentence T. For our simplemode\], we assume that each word of S independent\]yI)rodnces zero or mote words from the target vocabu-lary and that these words are then ordered to produceT.
We use the term alignment to refer to an associa-tion between words in T and words in S. The proba-bility P,,oda(T I S) is the sum of the probabilities ofall possible alignnmnts A between S and TI S) = e,,oa t(T, A i s ) .A(2)The joint probal)ility P,,odft(7', A I S) of T and a pat-ticula.r a.\]ignmeut is given by1',,,od?,(7', A IS) = (a)H P(tl"~A(t)) I I  P(iZA(s) ls)-Pdi.
'toTtio'(T, A I S).t6T s6.5'llere .iA(t) is tile word of ,5' aligned with t in the align-men t A, a.nd fi.A (s) is the number of words of T alignedwith s ill A. Tile distortion model Pdistortlon describestile ordering of tile words of T. We will not give itexplicitly.
The parameters in (3) areI.
The l)robabilities p(n \] s) that a word s in thesource language generates n target words;2.
"File prol)abilities p(t I s) that s generates theword t;3.
The pa.ra,meters of the distortion model.148We determine values for these parameters using max-imv.m likelihood training.
Thus we collect a largebilingual corpus consisting of pairs of sentences (S, T)which are translations of one another, and we seekparameter va.lues that maximize the likelihood of thistraining data as computed by the model.
This isequivalent o minimizing the cross entropyI f (T  IS) = - ~ Pt~,i,,(S,T) log P,,,od,t(TI S) (4)S,Twhere Ptr~.i,~(S,T) is the empirical distribution ob-tained by counting the number of times that the pair(S, T) occurs in the training corpus.The Vi terb i  Approx imat ionThe sum over alignments in (2) is too expensiveto compute directly since the number of alignmentsincreases exponentially with sentence length.
It isuseful to approximate this sum by the single termcorresponding to the alignment, A(S,T),  with great-est probability.
We refer to this approximation as theViterbi approzimation and to A(S,T)  as the Viterbialignment.Let c(s,t) be the expected number of times thats is aligned with t in the Viterbi alignnmnt of a pairof sentences drawn at random from the training data..Let c(s, n) be the expected number of times that s isaligned with n words.
Thenc(s,t) = ~ P,~o,~(S,T)c(s,t l J (S,T)  )S,Te(s,n) = ~ Pt,~i,(S,T)c(s, n I A(S,T) ) (5)S,Twhere c(.s,t I A) is the number of times that s isaligned with t in the alignment A, and c(s, n I A) isthe number of times that s generates n target wordsin A.
It can be shown \[2\] that these counts are alsoaverages with respect o the modelc(s, t) = ~ P,,,oda(S, T) c(s, t I A(,5', T) )S,T~(s,~) = ~ P.,o~,(S,T)e(s,,~ I A(S,T)).
(6)S,TBy normalizing the counts c(s,t) and c(s,n) weobtain probability distributions p(s, t) and p(s, n) 21 1 p(~, t) _ I t (S ,  t) p(~, ,,) _ I ~(s, ,3.
(7)7ZOT77~ ~OT77Z~In these equations and in the remainder of the paper, weThe conditional distributions p(t I s) and p(n Is) arethe Viterbi approximation estimates \[or the parame-ters of the model.
The marginals satisfy~p( .%, , )  = ,,,(,,) ~p( , , t )  : ~,(t)~qt~.p(s , t ) -  _2_~(~),,,(~) (8)twhere u(s) and u(t) are the unigram distributions of sand t and Fz(s) = ~ p(n I s )n  is the average numberof target words aligned with s. These formulae reflectthe fact that in any alignment each target word isaligned with exactly one source word.CROSS ENTROPY\]n this section we express the cross entropiesH ( S I T ) and \]\[(S ~I Tt) in terms of the in-formation between source and target words.In the Viterbi approximation the cross entropyH(T IS) is given byH(T I s) : Lr { H(t I s) + H(n t ~) } (9)where LT is the average length of the target sentencesin the training data, and lt(t I s) and It(n I s) are theconditional entropies for the probability distributions1,(s, t) and p(.., ~):H(t Is) = -~p(s,t) log p(tls),%t,"(,, I~) : - ~p( , , , ,~)  log v(., l~).
(10).$ t~.We wa.nt a similar exl)ression for the cross entropyI\[(S IT) .
Sincel ,,,oa~,(~, T) P,,,o~.dT I S) P,,~o~z(S),this cross entropy depends on both the translationmodel, \]',,,oact(T I S), and the language model, P,,.oact(S).We now show that with a suitable additional approx-itn ationH(S I T) : Lr { H(n I+) - ~(+,t) } + H(S) (~1)use the generic symbol ~ to denote ~ normalizing fa.ctor that norgncom, er!s counts to probabilities.
We let the actua.1 value of .o lI,e implicit from the context.
Thus, for example, in the left ha.ndequation of (7), the normalizing factor is norm = ~, , ,  c(s, t)which equals tile a,verage length of target sentences.
In theright hand equation of (7), the normalizing fa.ctor is the average\]engt.h of source sentences.149where H(S) is the cross entropy of P,+od+t(S) andI(s, t) is tire mutual information between t and s forthe probability distribution p(s, t).The additional approximation that we require isH iT )  ,~ LTHit) =- --LT ~p(t ) log  pi t)t(12)where p(t) is the marginal of p(s,t).
This amountsto approximating Pmod?l(T) by the unigram distribu-tion that is closest to it in cross entropy.
Grantingthis, formula (11) is a consequence of (9) and of theidentitiesHCS IT) = Hi T I S) - HCT) + I/iS),HCt,) = HCt I +) + I(+, t).
(13)Target Quest ionsFor sensing target sentences, a question about aninformant is a f, nction ~ from the target vocabularyinto the set of possible senses.
If the informant oft is z, then t is assigned the sense 5(z).
We wantto choose the function fi(z) to minimize the cross en-tropy It(S IT ' ) .
Front formula (34), we see that thisis equivale:,t o maximizing the conditional mutuali , formation I(s, t' I t) between s and t'p(s,~(z) I t) (15) ICs, t' I t ) = ~_,pC.s,x \[ t) log pCs 1 t)P(+(.+) t 0where p(s, t, x) is the probability distribution obtainedby counting the number of times in the Viterbi align-ments that s is aligned with t and the value of theinforma, t of t is x,Next consider H(S'  I T').
Let S ~ S' and TT' be sense labeling transformations of the type dis-cussed in Section 2.
Assume that these transforma-tions preserve Viterbi alignments; that is, if the wordss and t are aligned in the Viterbi alignment for ($, T),then their sensed versions s ~ and t' are aligned inthe Viterbi alignment for (SI,T').
It follows thatthe word translation probabilities obtained from theViterbi align ntents satisfy p(s,t) = Zt'etP(S,t') =~, 'oP( 'S ' , t )  where the sums range over tire sensedversions t' of t and the sensed versions s' of ~.By applying (11) to the cross entropies HCS I T),It(S I T'), and H(S ' I  T), it is not hard.
to verify thatHCSIT') = HCSI T ) -  LT~PCO/Cs, t'I?
)tHCS'IT) = HiS IT)- (:14)L~ ~ ~(,){:(t, +' I s )+ .rCn, ~', Is)}.$Here I(s, t' I t) is the conditional mutual informationgiven a target word t between its translations  and itssensed versions t'; I ( t ,  s' \[ s) is the conditional mutualinformation given a source word s between its trans-lations t a.nd its sensed versions s'; and I (n , s '  I s) isthe conditional mutual information given .s betweenn and its sensed versions s'.pC.+, ~, ~) -p Cs, +, +) -1 ~ P,ro+..CS, T) e(s, +, + I ACS, T))71"()7+71~ S,T~ v0, t,:~).
(16)?~Or77% x:e(z)=cAn exhaustive search for the best ~ requires a com-putation that is exponential in the number of values ofx and is not practical.
In previous work \[3\] we found agood ~ usi,g the flip-flop algorithm \[4\], which is onlyal)l)licable if the number of senses is restricted to two.Since then, we have developed a different Mgorithmthat can be used to find 5 for any number of senses.The algorithm uses the technique of alternating min-imization, and is similar to the k-means algorithm fordetermining pattern clusters and to the generalizedLloyd algorithm for designing vector quantitizers.
Adiscussion of alternating minimization, together withrefcrences, can be found in Chou \[5\].The algorithm is ba,sed on tile fact that, up toa constant independent of 5, the mutual informationl ( s , t  t I t) can be expressed as an infimum over condi-tional probal)ility distributions q(s I c),l(s, t' If) = (17)i .
f  ~ pix)D(pis I x,t)  ; q(s I 5(x)) + constantq :rSELECT ING QUEST IONSWe now present an algorithm for finding good in-formants and questions for sensing.whereDi~(+) ; q(+)) =- ~V(s) log p(') (18) + q(s)150The best value of the information is thus a.n infimiimover both the choice for 2. and the choice for the q .This suggests the following iterative procedure for ob-taining a good 2:1.
For given q, find the best E:E(x) = argmin,D(p(s ( x , t )  ; g(s ( c)).2.
For this E l  find the best 3:3 .
Iterate steps (1) a.nd (2) ilntil no fnrther increasein I ( s ,  t' I t )  results.Source QuestionsFor sensing source sentences, a, question a.bont aninformant is a Iunction 2: from the source voca1)iila.ryint'o the set of possible senses.
We want to chose 2.to minimize the entropy H ( S 1  I T).
From ( 14) this isequivalent to ~na.ximizing the sumI ( t , s t  I s)  + T (  n , s' I s ).
In analogy to (18),and we can again find a good 2 by alternating mini-miza.tion.CONCLUSIONIn this paper we presented a general frameworkfor integrating analysis and synthesis with statisti-cal translation, and within this framework we invcs-tigated cross-lingnal sense labeling.
We gave an algo-rithm for antoinatically constructing a simple labelingtransformation that assigns a sense to a word by ask-ing a question about a single word of the context.In a companion paper [3] we present results of trans-lation experiments using a sense-labeling cvnlponentthat employs a similar algorithn~.
We are currentlystudying the auton~atic onstruction of more complextransformations which utilize more detailed contex-tual informa tion.
((A stat,istic:xl a.pproa.ch to madline transla.tion,))Compufnlio1rrc.1 Ling.zl.istics, vol.
16, pp.
79-85,;June 1990.
[2] P. Brown, S. Dellal'ietra, V.
Uella.Pietra., andI t .
Mercer, "Initial estimates of word tra.nsla.tionprol)a.Bilities."
In prepa,ra.tion.
[3] P. Brown, S.
Della.Pietra., V.  DellaPietra, andR.
Mercer, "Word sense disainbigua.tion wingstatistica.1 metl~ods," in  proceeding.^ 29 th  Annualh4eeting of the ~'ssociat iol t jor  Comp~itationnl Lin-g~rislics, (Berkeley, CA), June 1991.
[4] A. Na.das, D. Na.hamoo, M. Picheny, a.nd J .
Pow-ell, "An iterat,ive "flip-flop"') a.pproximation of themost inIorma.tive split in the construction of de-cision trees," in Proceedings of the IEEE Inlernn-lionir.1 Con,jerence on Acoustics, Speech and SignalProcessing, (Toronto, Cana.da.
), May 1991.
[5] 1'.
Chon, Applicntions of I ~ ?
, j o r m a t i o ~  Theory toPnttcrn Recognition and the Design of Decision'I?ree.s and Trellises.
PhD t,hesis, Sta.nford Univer-sit,y, .Inne 1988.REFERENCES[I] P. Brown, J .
Cocke, S. DellaPietra, V.  DellaPietra,F.
Jelinek, .J.
Lafferty, R. Mercer, and P. Roossin,
