Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1537?1546,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsImproving Text Simplification Language ModelingUsing Unsimplified Text DataDavid KauchakMiddlebury CollegeMiddlebury, VT 05753dkauchak@middlebury.eduAbstractIn this paper we examine language mod-eling for text simplification.
Unlike sometext-to-text translation tasks, text simplifi-cation is a monolingual translation task al-lowing for text in both the input and out-put domain to be used for training the lan-guage model.
We explore the relation-ship between normal English and simpli-fied English and compare language mod-els trained on varying amounts of textfrom each.
We evaluate the models intrin-sically with perplexity and extrinsicallyon the lexical simplification task from Se-mEval 2012.
We find that a combinedmodel using both simplified and normalEnglish data achieves a 23% improvementin perplexity and a 24% improvement onthe lexical simplification task over a modeltrained only on simple data.
Post-hoc anal-ysis shows that the additional unsimplifieddata provides better coverage for unseenand rare n-grams.1 IntroductionAn important component of many text-to-texttranslation systems is the language model whichpredicts the likelihood of a text sequence beingproduced in the output language.
In some problemdomains, such as machine translation, the trans-lation is between two distinct languages and thelanguage model can only be trained on data inthe output language.
However, some problem do-mains (e.g.
text compression, text simplificationand summarization) can be viewed as monolingualtranslation tasks, translating between text varia-tions within a single language.
In these monolin-gual problems, text could be used from both theinput and output domain to train a language model.In this paper, we investigate this possibility for textsimplification where both simplified English textand normal English text are available for traininga simple English language model.Table 1 shows the n-gram overlap proportionsin a sentence aligned data set of 137K sentencepairs from aligning Simple English Wikipedia andEnglish Wikipedia articles (Coster and Kauchak,2011a).1 The data highlights two conflictingviews: does the benefit of additional data out-weigh the problem of the source of the data?Throughout the rest of this paper we refer tosentences/articles/text from English Wikipedia asnormal and sentences/articles/text from SimpleEnglish Wikipedia as simple.On the one hand, there is a strong correspon-dence between the simple and normal data.
At theword level 96% of the simple words are found inthe normal corpus and even for n-grams as large as5, more than half of the n-grams can be found inthe normal text.
In addition, the normal text doesrepresent English text and contains many n-gramsnot seen in the simple corpus.
This extra informa-tion may help with data sparsity, providing betterestimates for rare and unseen n-grams.On the other hand, there is still only modestoverlap between the sentences for longer n-grams,particularly given that the corpus is sentence-aligned and that 27% of the sentence pairs inthis aligned data set are identical.
If the worddistributions were very similar between simpleand normal text, then the overlap proportions be-tween the two languages would be similar re-gardless of which direction the comparison ismade.
Instead, we see that the normal text hasmore varied language and contains more n-grams.Previous research has also shown other differ-ences between simple and normal data sourcesthat could impact language model performanceincluding average number of syllables, reading1http://www.cs.middlebury.edu/?dkauchak/simplification1537n-gram size: 1 2 3 4 5simple in normal 0.96 0.80 0.68 0.61 0.55normal in simple 0.87 0.68 0.58 0.51 0.46Table 1: The proportion of n-grams that overlapin a corpus of 137K sentence-aligned pairs fromSimple English Wikipedia and English Wikipedia.complexity, and grammatical complexity (Napolesand Dredze, 2010; Zhu et al, 2010; Coster andKauchak, 2011b).
In addition, for some monolin-gual translation domains, it has been argued that itis not appropriate to train a language model usingdata from the input domain (Turner and Charniak,2005).Although this question arises in other monolin-gual translation domains, text simplification rep-resents an ideal problem area for analysis.
First,simplified text data is available in reasonablequantities.
Simple English Wikipedia containsmore than 60K articles written in simplified En-glish.
This is not the case for all monolingualtranslation tasks (Knight and Marcu, 2002; Cohnand Lapata, 2009).
Second, the quantity of sim-ple text data available is still limited.
After pre-processing, the 60K articles represents less thanhalf a million sentences which is orders of mag-nitude smaller than the amount of normal Englishdata available (for example the English Gigawordcorpus (David Graff, 2003)).
Finally, many recenttext simplification systems have utilized languagemodels trained only on simplified data (Zhu et al,2010; Woodsend and Lapata, 2011; Coster andKauchak, 2011a; Wubben et al, 2012); improve-ments in simple language modeling could translateinto improvements for these systems.2 Related WorkIf we view the normal data as out-of-domain data,then the problem of combining simple and nor-mal data is similar to the language model do-main adaption problem (Suzuki and Gao, 2005),in particular cross-domain adaptation (Bellegarda,2004) where a domain-specific model is improvedby incorporating additional general data.
Adapta-tion techniques have been shown to improve lan-guage modeling performance based on perplexity(Rosenfeld, 1996) and in application areas suchas speech transcription (Bacchiani and Roark,2003) and machine translation (Zhao et al, 2004),though no previous research has examined the lan-guage model domain adaptation problem for textsimplification.
Pan and Yang (2010) provide a sur-vey on the related problem of domain adaptationfor machine learning (also referred to as ?transferlearning?
), which utilizes similar techniques.
Inthis paper, we explore some basic adaptation tech-niques, however this paper is not a comparison ofdomain adaptation techniques for language mod-eling.
Our goal is more general: to examine therelationship between simple and normal data anddetermine whether normal data is helpful.
Previ-ous domain adaptation research is complementaryto our experiments and could be explored in thefuture for additional performance improvements.Simple language models play a role in a va-riety of text simplification applications.
Manyrecent statistical simplification techniques buildupon models from machine translation and uti-lize a simple language model during simplifica-tion/decoding both in English (Zhu et al, 2010;Woodsend and Lapata, 2011; Coster and Kauchak,2011a; Wubben et al, 2012) and in other lan-guages (Specia, 2010).
Simple English languagemodels have also been used as predictive featuresin other simplification sub-problems such as lexi-cal simplification (Specia et al, 2012) and predict-ing text simplicity (Eickhoff et al, 2010).Due to data scarcity, little research has beendone on language modeling in other monolin-gual translation domains.
For text compression,most systems are trained on uncompressed datasince the largest text compression data sets con-tain only a few thousand sentences (Knight andMarcu, 2002; Galley and McKeown, 2007; Cohnand Lapata, 2009; Nomoto, 2009).
Similarly forsummarization, systems that have employed lan-guage models trained only on unsummarized text(Banko et al, 2000; Daume and Marcu, 2002).3 CorpusWe collected a data set from English Wikipediaand Simple English Wikipedia with the formerrepresenting normal English and the latter sim-ple English.
Simple English Wikipedia has beenpreviously used for many text simplification ap-proaches (Zhu et al, 2010; Yatskar et al, 2010;Biran et al, 2011; Coster and Kauchak, 2011a;Woodsend and Lapata, 2011; Wubben et al, 2012)and has been shown to be simpler than normal En-glish Wikipedia by both automatic measures andhuman perception (Coster and Kauchak, 2011b;1538simple normalsentences 385K 2540Kwords 7.15M 64.7Mvocab size 78K 307KTable 2: Summary counts for the simple-normalarticle aligned data set consisting of 60K articlepairs.Woodsend and Lapata, 2011).
We downloaded allarticles from Simple English Wikipedia then re-moved stubs, navigation pages and any article thatconsisted of a single sentence, resulting in 60Ksimple articles.To partially normalize for content and sourcedifferences we generated a document aligned cor-pus for our experiments.
We extracted the cor-responding 60K normal articles from EnglishWikipedia based on the article title to represent thenormal data.
We held out 2K article pairs for useas a testing set in our experiments.
The extracteddata set is available for download online.2Table 2 shows count statistics for the collecteddata set.
Although the simple and normal datacontain the same number of articles, because nor-mal articles tend to be longer and contain morecontent, the normal side is an order of magnitudelarger.4 Language Model Evaluation:PerplexityTo analyze the impact of data source on simpleEnglish language modeling, we trained languagemodels on varying amounts of simple data, nor-mal data, and a combination of the two.
For ourfirst task, we evaluated these language models us-ing perplexity based on how well they modeled thesimple side of the held-out data.4.1 Experimental SetupWe used trigram language models with interpo-lated Kneser-Kney discounting trained using theSRI language modeling toolkit (Stolcke, 2002).
Toensure comparability, all models were closed vo-cabulary with the same vocabulary set based onthe words that occurred in the simple side of thetraining corpus, though similar results were seenfor other vocabulary choices.
We generated differ-ent models by varying the size and type of training2http://www.cs.middlebury.edu/?dkauchak/simplification1001502002503003500.5M 1M 1.5M 2M 2.5M 3Mperplexitytotal number of sentencessimple-onlynormal-onlysimple-ALL+normalFigure 1: Language model perplexities on theheld-out test data for models trained on increasingamounts of data.data:- simple-only: simple sentences only- normal-only: normal sentences only- simple-X+normal: X simple sentences com-bined with a varying number of normal sen-tencesTo evaluate the language models we calculatedthe model perplexity (Chen et al, 1998) on thesimple side of the held-out data.
The test set con-sisted of 2K simple English articles with 7,799simple sentences and 179K words.
Perplexitymeasures how likely a model finds a test set, withlower scores indicating better performance.4.2 Perplexity ResultsFigure 1 shows the language model perplexi-ties for the three types of models for increasingamounts of training data.
As expected, whentrained on the same amount of data, the languagemodels trained on simple data perform signifi-cantly better than language models trained on nor-mal data.
In addition, as we increase the amount ofdata, the simple-only model improves more thanthe normal-only model.However, the results also show that the normaldata does have some benefit.
The perplexity forthe simple-ALL+normal model, which starts withall available simple data, continues to improve asnormal data is added resulting in a 23% improve-ment over the model trained with only simple data(from a perplexity of 129 down to 100).
Evenby itself the normal data does have value.
Thenormal-only model achieves a slightly better per-plexity than the simple-only model, though onlyby utilizing an order of magnitude more data.15391001502002503000.5M 1M 1.5M 2M 2.5Mperplexitynumber of additional normal sentencessimple-50k+normalsimple-100k+normalsimple-150k+normalsimple-200k+normalsimple-250k+normalsimple-300k+normalsimple-350k+normalFigure 2: Language model perplexities for com-bined simple-normal models.
Each line representsa model trained on a different amount of simpledata as normal data is added.To better understand how the amount of sim-ple and normal data impacts perplexity, Figure 2shows perplexity scores for models trained onvarying amounts of simple data as we add increas-ing amounts of normal data.
We again see thatnormal data is beneficial; regardless of the amountof simple data, adding normal data improves per-plexity.
This improvement is most beneficial whensimple data is limited.
Models trained on lesssimple data achieved larger performance increasesthan those models trained on more simple data.Figure 2 also shows again that simple datais more valuable than normal data.
For ex-ample, the simple-only model trained on 250Ksentences achieves a perplexity of approximately150.
To achieve this same perplexity level start-ing with 200K simple sentences requires an ad-ditional 300K normal sentences, or starting with100K simple sentences an additional 850K normalsentences.4.3 Language Model AdaptationIn the experiments above, we generated the lan-guage models by treating the simple and normaldata as one combined corpus.
This approach hasthe benefit of simplicity, however, better perfor-mance for combining related corpora has beenseen by domain adaptation techniques which com-bine the data in more structured ways (Bacchianiand Roark, 2003).
Our goal for this paper is notto explore domain adaptation techniques, but todetermine if normal data is useful for the simplelanguage modeling task.
However, to provide an-other dimension for comparison and to understand95100105110115120125130simple-only 0.2 0.4 0.6 0.8 normal-onlyperplexitylambdaFigure 3: Perplexity scores for a linearly interpo-lated model between the simple-only model andthe normal-only model for varying lambda values.if domain adaptation techniques may be useful, wealso investigated a linearly interpolated languagemodel.A linearly interpolated language model com-bines the probabilities of two or more languagemodels as a weighted sum.
In our case, the in-terpolated model combines the simple model esti-mate, ps(wi|wi?2, wi?1), and the normal model esti-mate, pn(wi|wi?2, wi?1), linearly (Jelinek and Mer-cer, 1980; Hsu, 2007):pinterpolated(wi|wi?2, wi?1) =?
pn(wi|wi?2, wi?1) + (1?
?)
ps(wi|wi?2, wi?1)where 0 ?
?
?
1.Figure 3 shows perplexity scores for vary-ing lambda values ranging from the simple-onlymodel on the left with ?
= 0 to the normal-onlymodel on the right with ?
= 1.
As with the pre-vious experiments, adding normal data improvesimproves perplexity.
In fact, with a lambda of0.5 (equal weight between the models) the per-formance is slightly better than the aggregate ap-proaches above with a perplexity of 98.
The re-sults also highlight the balance between simpleand normal data; normal data is not as good assimple data and adding too much of it can causethe results to degrade.5 Language Model Evaluation:Lexical SimplificationCurrently, no automated methods exist for eval-uating sentence-level or document-level text sim-plification systems and manual evaluation is time-consuming, expensive and has not been vali-dated.
Because of these evaluation challenges, wechose to evaluate the language models extrinsi-1540Word: tightContext: With the physical market as tight as it has been in memory, silver could fly at any time.Candidates: constricted, pressurised, low, high-strung, tightHuman ranking: tight, low, constricted, pressurised, high-strungFigure 4: A lexical substitution example from the SemEval 2012 data set.cally based on the lexical simplification task fromSemEval 2012 (Specia et al, 2012).Lexical simplification is a sub-problem of thegeneral text simplification problem (Chandrasekarand Srinivas, 1997); a sentence is simplified bysubstituting words or phrases in the sentence with?simpler?
variations.
Lexical simplification ap-proaches have been shown to improve the read-ability of texts (Urano, 2000; Leroy et al, 2012),are useful in domains such as medical texts wheremajor content changes are restricted, and they maybe useful as a pre- or post-processing step for gen-eral simplification systems.5.1 Experimental SetupExamples from the lexical simplification data setfrom SemEval 2012 consist of three parts: w, theword to be simplified; s1, ..., si?1, w, si+1, ..., sn,a sentence containing the word; and, r1, r2, ..., rm,a list of candidate simplifications for w. The goalof the task is to rank the candidate simplificationsaccording to their simplicity in the context of thesentence.
Figure 4 shows an example from thedata set.
The data set contains a development setof 300 examples and a test set of 1710 examples.3For our experiments, we evaluated the models onthe test set.Given a language model p(?)
and a lexical sim-plification example, we ranked the list of candi-dates based on the probability the language modelassigns to the sentence with the candidate simplifi-cation inserted in context.
Specifically, we scoredeach candidate simplification rj byp(s1... si?1 rj si+1... sn)and then ranked them based on this score.
For ex-ample, to calculate the ranking for the example inFigure 4 we calculate the probability of each of:With the physical market as constricted as it has been ...With the physical market as pressurised as it has been ...With the physical market as low as it has been ...With the physical market as high-strung as it has been ...With the physical market as tight as it has been ...with the language model and then rank them bytheir probability.
We do not suggest this as a com-3http://www.cs.york.ac.uk/semeval-2012/task1/0.240.260.280.30.320.340.360.5M 1M 1.5M 2M 2.5M 3Mkapparankscoretotal number of sentencessimple-onlynormal-onlysimple-ALL+normalFigure 5: Kappa rank scores for the models trainedon increasing amounts of data.plete lexical substitution system, but it was a com-mon feature for many of the submitted systems, itperforms well relative to the other systems, and itallows for a concrete comparison between the lan-guage models on a simplification task.To evaluate the rankings, we use the metric fromthe SemEval 2012 task, the Cohen?s kappa coeffi-cient (Landis and Koch, 1977) between the systemranking and the human ranking, which we denotethe ?kappa rank score?.
See Specia et al (2012)for the full details of how the evaluation metric iscalculated.We use the same setup for training the languagemodels as in the perplexity experiments exceptthe models are open vocabulary instead of closed.Open vocabulary models allow for the languagemodels to better utilize the varying amounts ofdata and since the lexical simplification problemonly requires a comparison of probabilities withina given model to produce the final ranking, we donot need the closed vocabulary requirement.5.2 Lexical Simplification ResultsFigure 5 shows the kappa rank scores for thesimple-only, normal-only and combined models.As with the perplexity results, for similar amountsof data the simple-only model performs better thanthe normal-only model.
We also again see that theperformance difference between the two modelsgrows as the amount of data increases.
However,15410.240.260.280.30.320.340.360.5M 1M 1.5M 2M 2.5Mkapparankscorenumber of additional normal sentencesnormal-onlysimple-100k+normalsimple-150k+normalsimple-200k+normalsimple-250k+normalsimple-300k+normalsimple-350k+normalsimple-ALL+normalFigure 6: Kappa rank scores for models trainedwith varying amounts of simple data combinedwith increasing amounts of normal data.unlike the perplexity results, simply appending ad-ditional normal data to the entire simple data setdoes not improve the performance of the lexicalsimplifier.To determine if additional normal data im-proves the performance for models trained onsmaller amounts of simple data, Figure 6 showsthe kappa rank scores for models trained on differ-ent amounts of simple data as additional normaldata is added.
For smaller amounts of simple dataadding normal data does improve the kappa rankscore.
For example, a language model trained with100K simple sentences achieves a score of 0.246and is improved by almost 40% to 0.344 by addingall of the additional normal data.
Even the perfor-mance of a model trained with 300K simple sen-tences is increased by 3% (0.01 improvement inkappa rank score) by adding normal data.5.3 Language Model AdaptationThe results in the previous section show thatadding normal data to a simple data set can im-prove the lexical simplifier if the amount of simpledata is limited.
To investigate this benefit further,we again generated linearly interpolated languagemodels between the simple-only model and thenormal-only model.
Figure 7 shows results for thesame experimental design as Figure 6 with vary-ing amounts of simple and normal data, however,rather than appending the normal data we trainedthe models separately and created a linearly inter-polated model as described in Section 4.3.
Thebest lambda was chosen based on a linear searchoptimized on the SemEval 2012 development set.For all starting amounts of simple data, interpo-0.240.270.30.330.360.390.420.5M 1M 1.5M 2M 2.5Mkapparankscorenumber of additional normal sentencessimple-100ksimple-150ksimple-200ksimple-250ksimple-300ksimple-350ksimple-ALLFigure 7: Kappa rank scores for linearly inter-polated models between simple-only and normal-only models trained with varying amounts of sim-ple and normal data.lating the simple model with the normal model re-sults in a large increase in the kappa rank score.Combining the model trained on all the simpledata with the model trained on all the normal dataachieves a score of 0.419, an improvement of 23%over the model trained on only simple data.
Al-though our goal was not to create the best lexicalsimplification system, this approach would haveranked 6th out of 11 submitted systems in theSemEval 2012 competition (Specia et al, 2012).Interestingly, although the performance of thesimple-only models varied based on the amount ofsimple data, when these models are interpolatedwith a model trained on normal data, the perfor-mance tended to converge.
This behavior is alsoseen in Figure 6, though to a lesser extent.
Thismay indicate that for some tasks like lexical sim-plification, only a modest amount of simple data isrequired when combining with additional normaldata to achieve reasonable performance.6 Why Does Unsimplified Data Help?For both the perplexity experiments and the lexi-cal simplification experiments, utilizing additionalnormal data resulted in large performance im-provements; using all of the simple data available,performance is still significantly improved whencombined with normal data.
In this section, weinvestigate why the additional normal data is ben-eficial for simple language modeling.6.1 More n-gramsIntuitively, adding normal data provides additionalEnglish data to train on.
Most language models are1542Perplexity test data Lexical simplificationsimple normal % inc. simple normal % inc.1-grams 0.85 0.93 9.4% 0.74 0.78 6.2%2-grams 0.66 0.82 24% 0.34 0.54 56%3-grams 0.39 0.57 46% 0.088 0.19 117%Table 3: Proportion of n-grams in the test sets thatoccur in the simple and normal training data sets.trained using a smoothed version of the maximumlikelihood estimate for an n-gram.
For trigrams,this is:p(a|bc) = count(abc)count(bc)where count(?)
is the number of times the n-gramoccurs in the training corpus.
For interpolatedand backoff n-gram models, these counts aresmoothed based on the probabilities of lower or-der n-gram models, which are in-turn calculatedbased on counts from the corpus.We hypothesize that the key benefit of addi-tional normal data is access to more n-gram countsand therefore better probability estimation, partic-ularly for n-grams in the simple corpus that areunseen or have low frequency.
For n-grams thathave never been seen before, the normal data pro-vides some estimate from English text.
This isparticularly important for unigrams (i.e.
words)since there is no lower order model to gain infor-mation from and most language models assume auniform prior on unseen words, treating them allequally.
For n-grams that have been seen but arerare, the additional normal data can help providebetter probability estimates.
Because frequenciestend to follow a Zipfian distribution, these raren-grams make up a large portion of n-grams inreal data (Ha et al, 2003).To partially validate this hypothesis, we exam-ined the n-gram overlap between the n-grams inthe training data and the n-grams in the test setsfrom the two tasks.
Table 3 shows the percentageof unigrams, bigrams and trigrams from the twotest sets that are found in the simple and normaltraining data.For all n-gram sizes the normal data containedmore test set n-grams than the simple data.
Evenat the unigram level, the normal data containedsignificantly more of the test set unigrams than thesimple data.
On the perplexity data set, the 9.4%increase in word occurrence between the simpleand normal data set represents an over 50% reduc-tion in the number of out of vocabulary words.
ForPerplexity test data Lexical simplificationsimple + % inc. over simple + % inc. overnormal normal normal normal1-grams 0.93 0.2% 0.78 0.0%2-grams 0.83 0.8% 0.54 1.1%3-grams 0.58 2.5% 0.20 2.6%Table 4: Proportion of n-grams in the test sets thatoccur in the combination of both the simple andnormal data.larger n-grams, the difference between the simpleand normal data sets are even more pronounced.On the lexical simplification data the normal datacontained more than twice as many test trigramsas the simple data.
These additional n-grams al-low for better probability estimates on the test dataand therefore better performance on the two tasks.6.2 The Role of Normal DataEstimation of rare events is one component of lan-guage model performance, but other factors alsoimpact performance.
Table 4 shows the test setn-gram overlap on the combined data set of simpleand normal data.
Because the simple and normaldata come from the same content areas, the simpledata provides little additional coverage if the nor-mal data is already used.
For example, adding thesimple data to the normal data only increases thenumber of seen unigrams by 0.2%, representingonly about 600 new words.
However, the exper-iments above showed the combined models per-formed much better than models trained only onnormal data.This discrepancy highlights the key problemwith normal data: it is out-of-domain data.
Whileit shares some characteristics with the simple data,it represents a different distribution over the lan-guage.
To make this discrepancy more explicit,we created a sentence aligned data set by align-ing the simple and normal articles using the ap-proach from Coster and Kauchak (2011b).
Thisapproach has been previously used for aligningEnglish Wikipedia and Simple English Wikipediawith reasonable accuracy.
The resulting data setcontains 150K aligned simple-normal sentencepairs.Figure 8 shows the perplexity scores for lan-guage models trained on this data set.
Becausethe data is aligned and therefore similar, we seethe perplexity curves run parallel to each other asmore data is added.
However, even though these154310015020025030025K 50K 75K 100K 125K 150Kperplexitynumber of sentencessimple-only-alignednormal-only-alignedFigure 8: Language model perplexities for mod-els trained on increasing data sizes for a simple-normal sentence aligned data set.sentences represent the same content, the languageuse is different between simple and normal and thenormal data performs consistently worse.6.3 A Balance Between Simple and NormalExamining the optimal lambda values for the lin-early interpolated models also helps understandthe role of the normal data.
On the perplexity task,the best perplexity results were obtained with alambda of 0.5, or an equal weighting between thesimple and normal models.
Even though the nor-mal data contained six times as many sentencesand nine times as many words, the best model-ing performance balanced the quality of the simplemodel with the coverage of the normal model.For the simplification task, the optimal lambdavalue determined on the development set was 0.98,with a very strong bias towards the simple model.Only when the simple model did not provide dif-ferentiation between lexical choices will the nor-mal model play a role in selecting the candidates.For the lexical simplification task, the role of thenormal model is even more clear: to handle rareoccurrences not covered by the simple model andto smooth the simple model estimates.7 Conclusions and Future WorkIn the experiments above we have shown that ontwo different tasks utilizing additional normal dataimproves the performance of simple English lan-guage models.
On the perplexity task, the com-bined model achieved a performance improvementof 23% over the simple-only model and on thelexical simplification task, the combined modelachieved a 24% improvement.
These improve-ments are achieved over a simple-only model thatuses all simple English data currently available inthis domain.For both tasks, the best improvements wereseen when using language model adaptation tech-niques, however, the adaptation results also indi-cated that the role of normal data is partially taskdependent.
On the perplexity task, the best resultswere achieved with an equal weighting betweenthe simple-only and normal-only model.
How-ever, on the lexical simplification task, the bestresults were achieved with a very strong bias to-wards the simple-only model.
For other simplifi-cation tasks, the optimal parameters will need tobe investigated.For many of the experiments, combining asmaller amount of simple data (50K-100K sen-tences) with normal data achieved results that weresimilar to larger simple data set sizes.
For ex-ample, on the lexical simplification task, whenusing a linearly interpolated model, the modelcombining 100K simple sentences with all thenormal data achieved comparable results to themodel combining all the simple sentences with allthe normal data.
This is encouraging for othermonolingual domains such as text compressionor text simplification in non-English languageswhere less data is available.There are still a number of open research ques-tions related to simple language modeling.
First,further experiments with larger normal data setsare required to understand the limits of addingout-of-domain data.
Second, we have only uti-lized data from Wikipedia for normal text.
Manyother text sources are available and the impact ofnot only size, but also of domain should be in-vestigated.
Third, it still needs to be determinedhow language model performance will impactsentence-level and document-level simplificationapproaches.
In machine translation, improvedlanguage models have resulted in significant im-provements in translation performance (Brants etal., 2007).
Finally, in this paper we only in-vestigated linearly interpolated language models.Many other domain adaptations techniques existand may produce language models with better per-formance.1544ReferencesMichiel Bacchiani and Brian Roark.
2003.
Unsuper-vised language model adaptation.
In Proceedings ofICASSP.Michele Banko, Vibhu Mittal, and Michael Witbrock.2000.
Headline generation based on statistical trans-lation.
In Proceedings of ACL.Jerome R. Bellegarda.
2004.
Statistical languagemodel adaptation: Review and perspectives.
SpeechCommunication.Or Biran, Samuel Brody, and Noem?ie Elhadad.
2011.Putting it simply: A context-aware approach to lexi-cal simplification.
In Proceedings of ACL.Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.Och, and Jeffrey Dean.
2007.
Large languagemodels in machine translation.
In Proceedings ofEMNLP.Raman Chandrasekar and Bangalore Srinivas.
1997.Automatic induction of rules for text simplification.Knowledge Based Systems.Stanley Chen, Douglas Beeferman, and Ronald Rosen-feld.
1998.
Evaluation metrics for language mod-els.
In DARPA Broadcast News Transcription andUnderstanding Workshop.Trevor Cohn and Mirella Lapata.
2009.
Sentence com-pression as tree transduction.
Journal of ArtificialIntelligence Research.William Coster and David Kauchak.
2011a.
Learningto simplify sentences using Wikipedia.
In Proceed-ings of Text-To-Text Generation.William Coster and David Kauchak.
2011b.
SimpleEnglish Wikipedia: A new text simplification task.In Proceedings of ACL.Hal Daume and Daniel Marcu.
2002.
A noisy-channelmodel for document compression.
In Proceedingsof ACL.Christopher Cieri David Graff.
2003.
En-glish gigaword.
http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2003T05.Carsten Eickhoff, Pavel Serdyukov, and Arjen P.de Vries.
2010.
Web page classification on childsuitability.
In Proceedings of CIKM.Michel Galley and Kathleen McKeown.
2007.
Lex-icalized Markov grammars for sentence compres-sion.
In Proceedings of HLT-NAACL.Le Quan Ha, E. I. Sicilia-Garcia, Ji Ming, and F. J.Smith.
2003.
Extension of Zipf?s law to word andcharacter n-grams for English and Chinese.
Compu-tational Linguistics and Chinese Language Process-ing.Bo-June Hsu.
2007.
Generalized linear interpolationof language models.
In IEEE Workshop on ASRU.Frederick Jelinek and Robert Mercer.
1980.
Interpo-lated estimation of markov source parameters fromsparse data.
In Proceedings of the Workshop on Pat-ter Recognition in Practice.Kevin Knight and Daniel Marcu.
2002.
Summariza-tion beyond sentence extraction: a probabilistic ap-proach to sentence compression.
Artificial Intelli-gence.J.
Richard Landis and Gary G. Koch.
1977.
The mea-surement of observer agreement for categorical data.Biometrics.Gondy Leroy, James E. Endicott, Obay Mouradi, DavidKauchak, and Melissa Just.
2012.
Improving per-ceived and actual text difficulty for health informa-tion consumers using semi-automated methods.
InAmerican Medical Informatics Association (AMIA)Fall Symposium.Courtney Napoles and Mark Dredze.
2010.
Learn-ing simple Wikipedia: A cogitation in ascertain-ing abecedarian language.
In Proceedings ofHLT/NAACL Workshop on Computation Linguisticsand Writing.Tadashi Nomoto.
2009.
A comparison of model freeversus model intensive approaches to sentence com-pression.
In Proceedings of EMNLP.Sinno Jialin Pan and Qiang Yang.
2010.
A survey ontransfer learning.
IEEE Transactions on Knowledgeand Data Engineering.Ronald Rosenfeld.
1996.
A maximum entropy ap-proach to adaptive statistical language modeling.Computer, Speech and Language.Lucia Specia, Sujay Kumar Jauhar, and Rada Mihal-cea.
2012.
Semeval-2012 task 1: English lexicalsimplification.
In Joint Conference on Lexical andComputerational Semantics (*SEM).Lucia Specia.
2010.
Translating from complex to sim-plified sentences.
In Proceedings of ComputationalProcessing of the Portuguese Language.Andreas Stolcke.
2002.
SRILM - An extensible lan-guage modeling toolkit.
In Proceedings of ICSLP.Hisami Suzuki and Jianfeng Gao.
2005.
A compara-tive study on language model adaptation techniques.In Proceedings of EMNLP.Jenine Turner and Eugene Charniak.
2005.
Super-vised and unsupervised learning for sentence com-pression.
In Proceedings of ACL.Ken Urano.
2000.
Lexical simplification and elabo-ration: Sentence comprehension and incidental vo-cabulary acquisition.
Master?s thesis, University ofHawaii.1545Kristian Woodsend and Mirella Lapata.
2011.
Learn-ing to simplify sentences with quasi-synchronousgrammar and integer programming.
In Proceedingsof EMNLP.Sander Wubben, Antal van den Bosch, and EmielKrahmer.
2012.
Sentence simplification by mono-lingual machine translation.
In Proceedings of ACL.Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-Mizil, and Lillian Lee.
2010.
For the sake of sim-plicity: Unsupervised extraction of lexical simplifi-cations from Wikipedia.
In Proceedings of NAACL.Bing Zhao, Matthias Eck, and Stephan Vogel.
2004.Language model adaptation for statistical machinetranslation with structured query models.
In Pro-ceedings of COLING.Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.2010.
A monolingual tree-based translation modelfor sentence simplification.
In Proceedings of ICCL.1546
