Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 705?713,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsUptraining for Accurate Deterministic Question ParsingSlav Petrov, Pi-Chuan Chang, Michael Ringgaard, Hiyan AlshawiGoogle Research{slav,pichuan,ringgaard,hiyan}@google.comAbstractIt is well known that parsing accuracies dropsignificantly on out-of-domain data.
What isless known is that some parsers suffer morefrom domain shifts than others.
We showthat dependency parsers have more difficultyparsing questions than constituency parsers.In particular, deterministic shift-reduce depen-dency parsers, which are of highest interestfor practical applications because of their lin-ear running time, drop to 60% labeled accu-racy on a question test set.
We propose anuptraining procedure in which a deterministicparser is trained on the output of a more ac-curate, but slower, latent variable constituencyparser (converted to dependencies).
Uptrain-ing with 100K unlabeled questions achievesresults comparable to having 2K labeled ques-tions for training.
With 100K unlabeled and2K labeled questions, uptraining is able toimprove parsing accuracy to 84%, closingthe gap between in-domain and out-of-domainperformance.1 IntroductionParsing accuracies on the popular Section 23 of theWall Street Journal (WSJ) portion of the Penn Tree-bank have been steadily improving over the pastdecade.
At this point, we have many different pars-ing models that reach and even surpass 90% depen-dency or constituency accuracy on this test set (Mc-Donald et al, 2006; Nivre et al, 2007; Charniak andJohnson, 2005; Petrov et al, 2006; Carreras et al,2008; Koo and Collins, 2010).
Quite impressively,models based on deterministic shift-reduce parsingalgorithms are able to rival the other computation-ally more expensive models (see Nivre (2008) andreferences therein for more details).
Their linearrunning time makes them ideal candidates for largescale text processing, and our model of choice forthis paper.Unfortunately, the parsing accuracies of all mod-els have been reported to drop significantly on out-of-domain test sets, due to shifts in vocabulary andgrammar usage (Gildea, 2001; McClosky et al,2006b; Foster, 2010).
In this paper, we focus ourattention on the task of parsing questions.
Questionspose interesting challenges for WSJ-trained parsersbecause they are heavily underrepresented in thetraining data (there are only 334 questions amongthe 39,832 training sentences).
At the same time,questions are of particular interest for user facingapplications like question answering or web search,which necessitate parsers that can process questionsin a fast and accurate manner.We start our investigation in Section 3 by train-ing several state-of-the-art (dependency and con-stituency) parsers on the standard WSJ training set.When evaluated on a question corpus, we observedramatic accuracy drops exceeding 20% for the de-terministic shift-reduce parsers.
In general, depen-dency parsers (McDonald et al, 2006; Nivre et al,2007), seem to suffer more from this domain changethan constituency parsers (Charniak and Johnson,2005; Petrov et al, 2006).
Overall, the latent vari-able approach of Petrov et al (2006) appears to gen-eralize best to this new domain, losing only about5%.
Unfortunately, the parsers that generalize betterto this new domain have time complexities that arecubic in the sentence length (or even higher), render-ing them impractical for web-scale text processing.705SBARQWHNPWPWhatSQVBZdoesNPDTtheNNPPeugeotNNcompanyVPVBmanufacture.?
(a)What does the Peugeot company manufacture ?ROOTdobj aux det nn p   nsubjroot(b)Figure 1: Example constituency tree from the QuestionBank (a) converted to labeled Stanford dependencies (b).We therefore propose an uptraining method, inwhich a deterministic shift-reduce parser is trainedon the output of a more accurate, but slower parser(Section 4).
This type of domain adaptation is rem-iniscent of self-training (McClosky et al, 2006a;Huang and Harper, 2009) and co-training (Blum andMitchell, 1998; Sagae and Lavie, 2006), except thatthe goal here is not to further improve the perfor-mance of the very best model.
Instead, our aim isto train a computationally cheaper model (a lineartime dependency parser) to match the performanceof the best model (a cubic time constituency parser),resulting in a computationally efficient, yet highlyaccurate model.In practice, we parse a large amount of unlabeleddata from the target domain with the constituencyparser of Petrov et al (2006) and then train a deter-ministic dependency parser on this noisy, automat-ically parsed data.
The accuracy of the linear timeparser on a question test set goes up from 60.06%(LAS) to 76.94% after uptraining, which is compa-rable to adding 2,000 labeled questions to the train-ing data.
Combining uptraining with 2,000 labeledquestions further improves the accuracy to 84.14%,fully recovering the drop between in-domain andout-of-domain accuracy.We also present a detailed error analysis in Sec-tion 5, showing that the errors of the WSJ-trainedmodel are primarily caused by sharp changes in syn-tactic configurations and only secondarily due tolexical shifts.
Uptraining leads to large improve-ments across all error metrics and especially on im-portant dependencies like subjects (nsubj).2 Experimental SetupWe used the following experimental protocolthroughout the paper.2.1 DataOur main training set consists of Sections 02-21 ofthe Wall Street Journal portion of the Penn Treebank(Marcus et al, 1993), with Section 22 serving as de-velopment set for source domain comparisons.
Forour target domain experiments, we evaluate on theQuestionBank (Judge et al, 2006), which includesa set of manually annotated questions from a TRECquestion answering task.
The questions in the Ques-tionBank are very different from our training data interms of grammatical constructions and vocabularyusage, making this a rather extreme case of domain-adaptation.
We split the 4,000 questions containedin this corpus in three parts: the first 2,000 ques-tions are reserved as a small target-domain trainingset; the remaining 2,000 questions are split in twoequal parts, the first serving as development set andthe second as our final test set.
We report accuracieson the developments sets throughout this paper, andtest only at the very end on the final test set.We convert the trees in both treebanks from con-stituencies to labeled dependencies (see Figure 1)using the Stanford converter, which produces 46types of labeled dependencies1 (de Marneffe et al,2006).
We evaluate on both unlabeled (UAS) andlabeled dependency accuracy (LAS).2Additionally, we use a set of 2 million ques-tions collected from Internet search queries as unla-beled target domain data.
All user information wasanonymized and only the search query string was re-tained.
The question sample is selected at randomafter passing two filters that select queries that are1We use the Stanford Lexicalized Parser v1.6.2.2Because the QuestionBank does not contain function tags,we decided to strip off the function tags from the WSJ be-fore conversion.
The Stanford conversion only uses the -ADVand -TMP tags, and removing all function tags from the WSJchanged less than 0.2% of the labels (primarily tmod labels).706Training on Evaluating on WSJ Section 22 Evaluating on QuestionBankWSJ Sections 02-21 F1 UAS LAS POS F1 UAS LAS POSNivre et al (2007) ?
88.42 84.89 95.00 ?
74.14 62.81 88.48McDonald et al (2006) ?
89.47 86.43 95.00 ?
80.01 67.00 88.48Charniak (2000) 90.27 92.33 89.86 96.71 83.01 85.61 73.59 90.49Charniak and Johnson (2005) 91.92 93.56 91.24 96.69 84.47 87.13 75.94 90.59Petrov et al (2006) 90.70 92.91 90.48 96.27 85.52 88.17 79.10 90.57Petrov (2010) 92.10 93.85 91.60 96.44 86.62 88.77 79.92 91.08Our shift-reduce parser ?
88.24 84.69 95.00 ?
72.23 60.06 88.48Our shift-reduce parser (gold POS) ?
90.51 88.53 100.00 ?
78.30 68.92 100.00Table 1: Parsing accuracies for parsers trained on newswire data and evaluated on newswire and question test sets.similar in style to the questions in the QuestionBank:(i) the queries must start with an English functionword that can be used to start a question (what, whowhen, how, why, can, does, etc.
), and (ii) the querieshave a maximum length of 160 characters.2.2 ParsersWe use multiple publicly available parsers, as wellas our own implementation of a deterministic shift-reduce parser in our experiments.
The depen-dency parsers that we compare are the determinis-tic shift-reduce MaltParser (Nivre et al, 2007) andthe second-order minimum spanning tree algorithmbased MstParser (McDonald et al, 2006).
Our shift-reduce parser is a re-implementation of the Malt-Parser, using a standard set of features and a lin-ear kernel SVM for classification.
We also train andevaluate the generative lexicalized parser of Char-niak (2000) on its own, as well as in combinationwith the discriminative reranker of Charniak andJohnson (2005).
Finally, we run the latent variableparser (a.k.a.
BerkeleyParser) of Petrov et al (2006),as well as the recent product of latent variable gram-mars version (Petrov, 2010).
To facilitate compar-isons between constituency and dependency parsers,we convert the output of the constituency parsers tolabeled dependencies using the same procedure thatis applied to the treebanks.
We also report their F1scores for completeness.While the constituency parsers used in our experi-ments view part-of-speech (POS) tagging as an inte-gral part of parsing, the dependency parsers requirethe input to be tagged with a separate POS tagger.We use the TnT tagger (Brants, 2000) in our experi-ments, because of its efficiency and ease of use.
Tag-ger and parser are always trained on the same data.3 Parsing QuestionsWe consider two domain adaptation scenarios in thispaper.
In the first scenario (sometimes abbreviatedas WSJ), we assume that we do not have any labeledtraining data from the target domain.
In practice, thiswill always be the case when the target domain isunknown or very diverse.
The second scenario (ab-breviated as WSJ+QB) assumes a small amount oflabeled training data from the target domain.
Whilethis might be expensive to obtain, it is certainly fea-sible for narrow domains (e.g.
questions), or when ahigh parsing accuracy is really important.3.1 No Labeled Target Domain DataWe first trained all parsers on the WSJ training setand evaluated their performance on the two domainspecific evaluation sets (newswire and questions).As can be seen in the left columns of Table 1, allparsers perform very well on the WSJ developmentset.
While there are differences in the accuracies,all scores fall within a close range.
The table alsoconfirms the commonly known fact (Yamada andMatsumoto, 2003; McDonald et al, 2005) that con-stituency parsers are more accurate at producing de-pendencies than dependency parsers (at least whenthe dependencies were produced by a deterministictransformation of a constituency treebank, as is thecase here).This picture changes drastically when the per-formance is measured on the QuestionBank devel-opment set (right columns in Table 1).
As one707Evaluating on Training on WSJ + QB Training on QuestionBankQuestionBank F1 UAS LAS POS F1 UAS LAS POSNivre et al (2007) ?
83.54 78.85 91.32 ?
79.72 73.44 88.80McDonald et al (2006) ?
84.95 80.17 91.32 ?
82.52 77.20 88.80Charniak (2000) 89.40 90.30 85.01 94.17 79.70 76.69 69.69 87.84Petrov et al (2006) 90.96 90.98 86.90 94.01 86.62 84.09 78.92 87.56Petrov (2010) 92.81 92.23 88.84 94.48 87.72 85.07 80.08 87.79Our shift-reduce parser ?
83.70 78.27 91.32 ?
80.44 74.29 88.80Our shift-reduce parser (gold POS) ?
89.39 86.60 100.00 ?
87.31 84.15 100.00Table 2: Parsing accuracies for parsers trained on newswire and question data and evaluated on a question test set.might have expected, the accuracies are significantlylower, however, the drop for some of the parsersis shocking.
Most notably, the deterministic shift-reduce parsers lose almost 25% (absolute) on la-beled accuracies, while the latent variable parserslose around 12%.3 Note also that even with goldPOS tags, LAS is below 70% for our determinis-tic shift-reduce parser, suggesting that the drop inaccuracy is primarily due to a syntactic shift ratherthan a lexical shift.
These low accuracies are espe-cially disturbing when one considers that the aver-age question in the evaluation set is only nine wordslong and therefore potentially much less ambiguousthan WSJ sentences.
We will examine the main errortypes more carefully in Section 5.Overall, the dependency parsers seem to suf-fer more from the domain change than the con-stituency parsers.
One possible explanation is thatthey lack the global constraints that are enforced bythe (context-free) grammars.
Even though the Mst-Parser finds the globally best spanning tree, all con-straints are local.
This means for example, that itis not possible to require the final parse to containa verb (something that can be easily expressed bya top-level production of the form S ?
NP VP in acontext free grammar).
This is not a limitation of de-pendency parsers in general.
For example, it wouldbe easy to enforce such constraints in the Eisner(1996) algorithm or using Integer Linear Program-ming approaches (Riedel and Clarke, 2006; Martinset al, 2009).
However, such richer modeling capac-ity comes with a much higher computational cost.Looking at the constituency parsers, we observe3The difference between our shift-reduce parser and theMaltParser are due to small differences in the feature sets.that the lexicalized (reranking) parser of Charniakand Johnson (2005) loses more than the latent vari-able approach of Petrov et al (2006).
This differ-ence doesn?t seem to be a difference of generativevs.
discriminative estimation.
We suspect that thelatent variable approach is better able to utilize thelittle evidence in the training data.
Intuitively speak-ing, some of the latent variables seem to get alo-cated for modeling the few questions present in thetraining data, while the lexicalization contexts arenot able to distinguish between declarative sentencesand questions.To verify this hypothesis, we conducted two addi-tional experiments.
In the first experiment, we col-lapsed the question specific phrasal categories SQand SBARQ to their declarative sentence equivalentsS and SBAR.
When the training and test data areprocessed this way, the lexicalized parser loses 1.5%F1, while the latent variable parser loses only 0.7%.It is difficult to examine the grammars, but one canspeculate that some of the latent variables were usedto model the question specific constructions and themodel was able to re-learn the distinctions that wepurposefully collapsed.
In the second experiment,we removed all questions from the WSJ training setand retrained both parsers.
This did not make asignificant difference when evaluating on the WSJdevelopment set, but of course resulted in a largeperformance drop when evaluating on the Question-Bank.
The lexicalized parser came out ahead in thisexperiment,4 confirming our hypothesis that the la-tent variable model is better able to pick up the smallamount of relevant evidence that is present in theWSJ training data (rather than being systematically4The F1 scores were 52.40% vs. 56.39% respectively.708better suited for modeling questions).3.2 Some Labeled Target Domain DataIn the above experiments, we considered a situationwhere we have no labeled training data from the tar-get domain, as will typically be the case.
We nowconsider a situation where a small amount of labeleddata (2,000 manually parsed sentences) from the do-main of interest is available for training.We experimented with two different ways of uti-lizing this additional training data.
In a first experi-ment, we trained models on the concatenation of theWSJ and QuestionBank training sets (we did not at-tempt to weight the different corpora).
As Table 2shows (left columns), even a modest amount of la-beled data from the target domain can significantlyboost parsing performance, giving double-digit im-provements in some cases.
While not shown in thetable, the parsing accuracies on the WSJ develop-ment set where largely unaffected by the additionaltraining data.Alternatively, one can also train models exclu-sively on the QuestionBank data, resulting in ques-tion specific models.
The parsing accuracies ofthese domain-specific models are shown in the rightcolumns of Table 2, and are significantly lower thanthose of models trained on the concatenated trainingsets.
They are often times even lower than the resultsof parsers trained exclusively on the WSJ, indicatingthat 2,000 sentences are not sufficient to train accu-rate parsers, even for quite narrow domains.4 Uptraining for Domain-AdaptationThe results in the previous section suggest thatparsers without global constraints have difficul-ties dealing with the syntactic differences betweendeclarative sentences and questions.
A possible ex-planation is that similar word configurations can ap-pear in both types of sentences, but with very differ-ent syntactic interpretation.
Local models withoutglobal constraints are therefore mislead into dead-end interpretations from which they cannot recover(McDonald and Nivre, 2007).
Our approach willtherefore be to use a large amount of unlabeled datato bias the model towards the appropriate distribu-tion for the target domain.
Rather than lookingfor feature correspondences between the domains70758085901M100K10K1K100100UASWSJ+QBWSJ6065707580851M100K10K1K100100LASNumber of unlabeled questionsWSJ+QBWSJFigure 2: Uptraining with large amounts of unlabeleddata gives significant improvements over two differentsupervised baselines.
(Blitzer et al, 2006), we propose to use automati-cally labeled target domain data to learn the targetdomain distribution directly.4.1 Uptraining vs. Self-trainingThe idea of training parsers on their own output hasbeen around for as long as there have been statis-tical parsers, but typically does not work well atall (Charniak, 1997).
Steedman et al (2003) andClark et al (2003) present co-training proceduresfor parsers and taggers respectively, which are ef-fective when only very little labeled data is avail-able.
McClosky et al (2006a) were the first to im-prove a state-of-the-art constituency parsing systemby utilizing unlabeled data for self-training.
In sub-sequent work, they show that the same idea can beused for domain adaptation if the unlabeled data ischosen accordingly (McClosky et al, 2006b).
Sagaeand Tsujii (2007) co-train two dependency parsersby adding automatically parsed sentences for whichthe parsers agree to the training data.
Finally, Suzukiet al (2009) present a very effective semi-supervisedapproach in which features from multiple generativemodels estimated on unlabeled data are combined ina discriminative system for structured prediction.All of these approaches have in common that theirultimate goal is to improve the final performance.Our work differs in that instead of improving the709Uptraining with Using only WSJ data Using WSJ + QB datadifferent base parsers UAS LAS POS UAS LAS POSBaseline 72.23 60.06 88.48 83.70 78.27 91.32Self-training 73.62 61.63 89.60 84.26 79.15 92.09Uptraining on Petrov et al (2006) 86.02 76.94 90.75 88.38 84.02 93.63Uptraining on Petrov (2010) 85.21 76.19 90.74 88.63 84.14 93.53Table 3: Uptraining substantially improves parsing accuracies, while self-training gives only minor improvements.performance of the best parser, we want to builda more efficient parser that comes close to the ac-curacy of the best parser.
To do this, we parsethe unlabeled data with our most accurate parserand generate noisy, but fairly accurate labels (parsetrees) for the unlabeled data.
We refer to the parserused for producing the automatic labels as the baseparser (unless otherwise noted, we used the latentvariable parser of Petrov et al (2006) as our baseparser).
Because the most accurate base parsers areconstituency parsers, we need to convert the parsetrees to dependencies using the Stanford converter(see Section 2).
The automatically parsed sentencesare appended to the labeled training data, and theshift-reduce parser (and the part-of-speech tagger)are trained on this new training set.
We did notincrease the weight of the WSJ training data, butweighted the QuestionBank training data by a fac-tor of ten in the WSJ+QB experiments.4.2 Varying amounts of unlabeled dataFigure 2 shows the efficacy of uptraining as a func-tion of the size of the unlabeled data.
Both la-beled (LAS) and unlabeled accuracies (UAS) im-prove sharply when automatically parsed sentencesfrom the target domain are added to the training data,and level off after 100,000 sentences.
Comparingthe end-points of the dashed lines (models havingaccess only to labeled data from the WSJ) and thestarting points of the solid lines (models that haveaccess to both WSJ and QuestionBank), one can seethat roughly the same improvements (from 72% to86% UAS and from 60% to 77% LAS) can be ob-tained by having access to 2,000 labeled sentencesfrom the target domain or uptraining with a largeamount of unlabeled data from the target domain.The benefits seem to be complementary and can becombined to give the best results.
The final accu-racy of 88.63 / 84.14 (UAS / LAS) on the questionevaluation set is comparable to the in-domain per-formance on newswire data (88.24 / 84.69).4.3 Varying the base parserTable 3 then compares uptraining on the output ofdifferent base parsers to pure self-training.
In theseexperiments, the same set of 500,000 questions wasparsed by different base parsers.
The automaticparses were then added to the labeled training dataand the parser was retrained.
As the results show,self-training provides only modest improvements ofless than 2%, while uptraining gives double-digitimprovements in some cases.
Interestingly, thereseems to be no substantial difference between up-training on the output of a single latent variableparser (Petrov et al, 2006) and a product of latentvariable grammars (Petrov, 2010).
It appears thatthe roughly 1% accuracy difference between the twobase parsers is not important for uptraining.4.4 POS-less parsingOur uptraining procedure improves parse quality onout-of-domain data to the level of in-domain ac-curacy.
However, looking closer at Table 3, onecan see that the POS accuracy is still relatively low(93.53%), potentially limiting the final accuracy.To remove this limitation (and also the depen-dence on a separate POS tagger), we experimentedwith word cluster features.
As shown in Koo et al(2008), word cluster features can be used in con-junction with POS tags to improve parsing accuracy.Here, we use them instead of POS tags in order tofurther reduce the domain-dependence of our model.Similar to Koo et al (2008), we use the Brown clus-tering algorithm (Brown et al, 1992) to produce adeterministic hierarchical clustering of our input vo-cabulary.
We then extract features based on vary-710UAS LAS POSPart-of-Speech Tags 88.35 84.05 93.53Word Cluster Features 87.92 83.73 ?Table 4: Parsing accuracies of uptrained parsers with andwithout part-of-speech tags and word cluster features.ing cluster granularities (6 and 10 bits in our experi-ments).
Table 4 shows that roughly the same level ofaccuracy can be achieved with cluster based featuresinstead of POS tag features.
This change makes ourparser completely deterministic and enables us toprocess sentences in a single left-to-right pass.5 Error AnalysisTo provide a better understanding of the challengesinvolved in parsing questions, we analyzed the er-rors made by our WSJ-trained shift-reduce parserand also compared them to the errors that are leftafter uptraining.5.1 POS errorsMany parsing errors can be traced back to POS tag-ging errors, which are much more frequent on out-of-domain data than on in-domain data (88.8% onthe question data compared to above 95.0% on WSJdata).
Part of the reason for the lower POS taggingaccuracy is the higher unknown word ratio (7.3% onthe question evaluation set, compared to 3.4% on theWSJ evaluation set).
Another reason is a change inthe lexical distribution.For example, wh-determiners (WDT) are quiterare in the WSJ training data (relative frequency0.45%), but five times more common in the Ques-tionBank training data (2.49%).
In addition to thisfrequency difference, 52.43% of the WDTs in theWSJ are the word ?which?
and 46.97% are?that?.
Inthe QuestionBank on the other hand, ?what?
is byfar the most common WDT word (81.40%), while?which?
and ?that?
account only for 13.65% and4.94% respectively.
Not surprisingly the most com-mon POS error involves wh-determiners (typicallythe word ?what?)
being incorrectly labeled as Wh-pronouns (WP), resulting in head and label errorslike the one shown in Figure 3(a).To separate out POS tagging errors from parsingerrors, we also ran experiments with correct (gold)Dep.
Label Frequency WSJ Uptrainednsubj 934 41.02 88.64amod 556 78.21 86.00dobj 555 70.10 83.12attr 471 8.64 93.49aux 467 77.31 82.56Table 5: F1 scores for the most frequent labels in theQuestionBank development set.
Uptraining leads to hugeimprovements compared to training only on the WSJ.POS tags.
The parsing accuracies of our shift-reduceparser using gold POS tags are listed in the last rowsof Tables 1 and 2.
Even with gold POS tags, the de-terministic shift-reduce parser falls short of the ac-curacies of the constituency parsers (with automatictags), presumably because the shift-reduce model ismaking only local decisions and is lacking the globalconstraints provided by the context-free grammar.5.2 Dependency errorsTo find the main error types, we looked at the mostfrequent labels in the QuestionBank developmentset, and analyzed the ones that benefited the mostfrom uptraining.
Table 5 has the frequency and F-scores of the dependency types that we are going todiscuss in the following.
We also provide exampleswhich are illustrated in Figure 3.nsubj: The WSJ-trained model is often producingparses that are missing a subject (nsubj).
Questionslike ?What is the oldest profession??
and ?Whenwas Ozzy Osbourne born??
should have ?profes-sion?
and ?Osbourne?
as nsubjs, but in both casesthe WSJ-trained parser did not label any subj (seeFigures 3(b) and 3(c)).
Another common error is tomislabel nsubj.
For example, the nsubj of ?What areliver enzymes??
should be enzymes, but the WSJ-trained parser labels ?What?
as the nsubj, whichmakes sense in a statement but not in a question.amod: The model is overpredicting ?amod?, re-sulting in low precision figures for this label.
Anexample is ?How many points make up a perfectfivepin bowling score??.
The Stanford dependencyuses ?How?
as the head of ?many?
in noun phraseslike ?How many points?, and the relation is a generic?dep?.
But in the WSJ model prediction, ?many?s?head is ?points,?
and the relation mislabeled asamod.
Since it?s an adjective preceding the noun,711What is the oldest profession ?ROOTdet     amod proot attr nsubjWP VBZ DT JJS NN .ROOTdet     amod proot   attrdepWP VBZ DT JJS NN .What is the oldest profession ?When was Ozzy Osbourne born ?ROOT WRB VBZ  NNP NNP VBN .root padvmod aux nn nsubjWhen was Ozzy Osbourne   born ?ROOT WRB VBZ  NNP NNP   NNP .root    nnnn pcompl nsubjWhat films featured the character ?ROOT WDT NNS  VBD DT NN  NNP NNP .Popeye Doylensubj dep det    nn nn dobjWhat films featured the character ?ROOT WP NNS  VBD DT NN  NNP NNP .Popeye Doylensubj    compl det    nn nn   root    ccomp(a)(b)(c)(d)How many people did Randy ?ROOT WRB JJ  NNS VBD NNP  NNP VB .Craft killdobj dep aux    nn nsubj p?.depHow many people did RandyROOT WRB JJ  NNS VBD NNP  NNP VBCraft killcompl amod ccomp   nn nsubj pnsubjrootFigure 3: Example questions from the QuestionBank development set and their correct parses (left), as well as thepredictions of a model trained on the WSJ (right).the WSJ model often makes this mistake and there-fore the precision is much lower when it doesn?t seemore questions in the training data.dobj: The WSJ model doesn?t predict object ex-traction well.
For example, in ?How many peopledid Randy Craft kill??
(Figure 3(d)), the direct ob-ject of kill should be ?How many people.?
In theStanford dependencies, the correct labels for thisnoun phrase are ?dobj dep dep,?
but the WSJ modelpredicts ?compl amod nsubj.?
This is a commonerror caused by the different word order in ques-tions.
The uptrained model is much better at han-dling these type of constructions.attr: An attr (attributive) is a wh-noun phrase(WHNP) complement of a copular verb.
In the WSJtraining data, only 4,641 out of 950,028 dependen-cies are attr (0.5%); in the QuestionBank trainingdata, 1,023 out of 17,069 (6.0%) are attr.
As a con-sequence, the WSJ model cannot predict this labelin questions very well.aux: ?What does the abbreviation AIDS standfor??
should have ?stand?
as the main head of thesentence, and ?does?
as its aux.
However, the WSJmodel labeled ?does?
as the main head.
Similarpatterns occur in many questions, and therefore theWSJ has a very low recall rate.In contrast, mostly local labels (that are not re-lated to question/statement structure differences)have a consistently high accuracy.
For example: dethas an accuracy of 98.86% with the WSJ-trainedmodel, and 99.24% with the uptrained model.6 ConclusionsWe presented a method for domain adaptation of de-terministic shift-reduce parsers.
We evaluated mul-tiple state-of-the-art parsers on a question corpusand showed that parsing accuracies degrade substan-tially on this out-of-domain task.
Most notably, de-terministic shift-reduce parsers have difficulty deal-ing with the modified word order and lose morethan 20% in accuracy.
We then proposed a simple,yet very effective uptraining method for domain-adaptation.
In a nutshell, we trained a deterministicshift-reduce parser on the output of a more accurate,but slower parser.
Uptraining with large amounts ofunlabeled data gives similar improvements as hav-ing access to 2,000 labeled sentences from the targetdomain.
With 2,000 labeled questions and a largeamount of unlabeled questions, uptraining is able toclose the gap between in-domain and out-of-domainaccuracy.712AcknowledgementsWe would like to thank Ryan McDonald for run-ning the MstParser experiments and for many fruit-ful discussions on this topic.
We would also like tothank Joakim Nivre for help with the MatlParser andMarie-Catherine de Marneffe for help with the Stan-ford Dependency Converter.ReferencesJ.
Blitzer, R. McDonald, and F. Pereira.
2006.
Domainadaptation with structural correspondence learning.
InEMNLP ?06.A.
Blum and T. Mitchell.
1998.
Combining labeled andunlabeled data with co-training.
In COLT ?98.T.
Brants.
2000.
TnT ?
a statistical part-of-speech tagger.In ANLP ?00.P.
Brown, V. Della Pietra, P. deSouza, J. Lai, and R. Mer-cer.
1992.
Class-based n-gram models of natural lan-guage.
Computational Linguistics.X.
Carreras, M. Collins, and T. Koo.
2008.
TAG, dy-namic programming, and the perceptron for efficient,feature-rich parsing.
In CoNLL ?08.E.
Charniak and M. Johnson.
2005.
Coarse-to-Fine N-Best Parsing and MaxEnt Discriminative Reranking.In ACL?05.E.
Charniak.
1997.
Statistical parsing with a context-freegrammar and word statistics.
In AI ?97.E.
Charniak.
2000.
A maximum?entropy?inspiredparser.
In NAACL ?00.S.
Clark, J. Curran, and M. Osborne.
2003.
Bootstrap-ping pos-taggers using unlabelled data.
In CoNLL ?03.M.-C. de Marneffe, B. MacCartney, and C. Manning.2006.
Generating typed dependency parses fromphrase structure parses.
In LREC ?06.J.
Eisner.
1996.
Three new probabilistic models for de-pendency parsing: An exploration.
In COLING ?96.J.
Foster.
2010.
?cba to check the spelling?
: Investigat-ing parser performance on discussion forum posts.
InNAACL ?10.D.
Gildea.
2001.
Corpus variation and parser perfor-mance.
In EMNLP ?01.Z.
Huang and M. Harper.
2009.
Self-training PCFGgrammars with latent annotations across languages.
InEMNLP ?09.J.
Judge, A. Cahill, and J. v. Genabith.
2006.
Question-bank: creating a corpus of parse-annotated questions.In ACL ?06.T.
Koo and M. Collins.
2010.
Efficient third-order de-pendency parsers.
In ACL ?10.T.
Koo, X. Carreras, and M. Collins.
2008.
Simple semi-supervised dependency parsing.
In ACL ?08.M.
Marcus, B. Santorini, and M. Marcinkiewicz.
1993.Building a large annotated corpus of English: ThePenn Treebank.
In Computational Linguistics.A.F.T.
Martins, N.A.
Smith, and E.P.
Xing.
2009.
Con-cise integer linear programming formulations for de-pendency parsing.
In ACL ?09.D.
McClosky, E. Charniak, and M. Johnson.
2006a.
Ef-fective self-training for parsing.
In NAACL ?06.D.
McClosky, E. Charniak, and M. Johnson.
2006b.Reranking and self-training for parser adaptation.
InACL ?06.R.
McDonald and J. Nivre.
2007.
Characterizing theerrors of data-driven dependency parsing models.
InEMNLP ?07.R.
McDonald, K. Crammer, and F. Pereira.
2005.
Onlinelarge-margin training of dependency parsers.
In ACL?05.R.
McDonald, K. Lerman, and F. Pereira.
2006.
Multi-lingual dependency analysis with a two-stage discrim-inative parser.
In CoNLL ?06.J.
Nivre, J.
Hall, J. Nilsson, A. Chanev, G. Eryigit,S.
Kbler, S. Marinov, and E. Marsi.
2007.
Maltparser:A language-independent system for data-driven de-pendency parsing.
Natural Language Engineering,13(2).J.
Nivre.
2008.
Algorithms for deterministic incremen-tal dependency parsing.
Computational Linguistics,34(4).S.
Petrov, L. Barrett, R. Thibaux, and D. Klein.
2006.Learning accurate, compact, and interpretable tree an-notation.
In ACL ?06.S.
Petrov.
2010.
Products of random latent variablegrammars.
In NAACL ?10.S.
Riedel and J. Clarke.
2006.
Incremental integer linearprogramming for non-projective dependency parsing.In EMNLP ?06.K.
Sagae and A. Lavie.
2006.
Parser combination byreparsing.
In NAACL ?06.K.
Sagae and J. Tsujii.
2007.
Dependency parsing anddomain adaptation with lr models and parser ensem-bles.
In CoNLL ?07.M.
Steedman, M. Osborne, A. Sarkar, S. Clark, R. Hwa,J.
Hockenmaier, P. Ruhlen, S. Baker, and J. Crim.2003.
Bootstrapping statistical parsers from smalldatasets.
In EACL ?03.J.
Suzuki, H. Isozaki, X. Carreras, and M. Collins.
2009.An empirical study of semi-supervised structured con-ditional models for dependency parsing.
In EMNLP?09.H.
Yamada and Y. Matsumoto.
2003.
Statistical depen-dency analysis with support vector machines.
In IWPT?03.713
