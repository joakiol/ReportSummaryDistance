Discovering Phonotactic Finite-State Automata by Genetic SearchAnja BelzSchool of Cognitive and Comput ing  SciencesUniversity of SussexBrighton BN1 9QH, UKemail: anj  ab?cogs,  susx .
ac.
ukAbst rac tThis paper presents a genetic algorithm basedapproach to the automatic discovery of finite-state automata (FSAs) from positive data.FSAs are commonly used in computationalphonology, but - given the limited learnabil-ity of FSAs from arbitrary language subsets- are usually constructed manually.
The ap-proach presented here offers a practical auto-matic method that helps reduce the cost of man-ual FSA construction.1 BackgroundFinite-state techniques have always been cen-tral to computational phonology.
DefiniteFSAs are commonly used to encode phonotac-tic constraints in a variety of NLP contexts.Such FSAs are usually constructed in a time-consuming manual process.Following notational convention, an FSA is aquintuple (S,I, ~, so, F), in which S is a finitenonempty set of states, I is a finite nonemptyalphabet, $ is the state transition function, sois the initial state, and F is a nonempty setof final states in S. The language L acceptedby the finite automaton A, denoted L(A), is?
F}.Generally, the problem considered here is thatof identifying a language L from a fixed finitesample D = (D+,D- ) ,  where D + C L andD-  AL  = 0 (D-  may be empty).
If D -  isempty, and D + is structurally complete with re-gard to L, the problem is not complex, and thereexist a number of algorithms based on first con-structing a canonical automaton from the dataset which is then reduced or merged in variousways.
If D + is an arbitrary strict subset of L,the problem is less clearly defined.
Since any fi-nite sample is consistent with an infinite numberof languages, L cannot be identified uniquelyfrom D +.
"...the best we can hope to do is toinfer a grammar that will describe the stringsin D + and predict other strings that in somesense are of the same nature as those containedin D+. "
(Fu and Booth, 1986, p. 345)To constrain the set of possible languages L,the inferred grammar is typically required tobe as small as possible.
However, the prob-lem of finding a minimal grammar consistentwith a given sample D was shown to be NP-hard by Gold (1978).
Nonapproximability re-sults of varying strength ave been added since.In the special case where D contains all stringsof symbols over a finite alphabet I of lengthshorter than k, a polynomial-time algorithm canbe found (Trakhtenbrot and Barzdin, 1973), butif even a small fraction of examples is missing,the problem is again NP-hard (Angluin, 1978).2 Task Descr ip t ionGiven a known finite alphabet of symbols I ,  atarget finite-state language L, and a data sam-ple D + _C L _C I ' ,  the task is to find an FSA A,such that L( A ) is consistent with D +, L( A ) isa superset of D + encoding generalisation overthe structural regularities of D +, and the sizeof S is as small as possible.
Where the targetlanguage is known in advance, the degree of lan-guage and size approximation can be measured,and its adequacy relative to training set size andrepresentativeness canbe described.
In the caseof inference of automata that encode (part of)a phonological grammar, language approxima-tion and its degree of adequacy can be describedrelative to a set of theoretical linguistic assump-tions that describes a target grammar.3 Search  MethodBy direct analogy with natural evolution, ge-netic algorithms (GAs) work with a population1472of individuals each of which represents a candi-date solution to the given problem.
These indi-viduals are assigned afitness score and on its ba-sis are selected to 'mate', and produce the nextgeneration.
This process is typically iterateduntil the population has converged, i.e.
whenindividuals have reached a degree of similaritybeyond which further improvement becomes im-possible.
Characteristics that form part of goodsolutions are passed on through the generationsand begin to combine in the offspring to ap-proach global optima, an effect that has beenexplained in terms of the building block hypothe-sis (Goldberg, 1989).
Unlike other search meth-ods, GAs sample different areas of the searchspace simultaneously, and are therefore able toescape local optima and to avoid areas of lowfitness.The main issues in GA design are encodingthe candidate solutions (individuals) as datastructures for the GA to work on, defining afitness .function that accurately expresses thegoodness of candidate solutions, and designinggenetic operators that combine and alter thegenetic material of good solutions (parents) toproduce new, better solutions (offspring).In the present GA 1, the state-transition ma-trices of FSAs are directly converted into geno-types.
Mutation randomly adds or deletes onetransition in each FSA and a variant of uniformcrossover tends to preserve the general struc-ture of the fitter parent, while adding some sub-structures from the weaker parent (offspring canbe larger or smaller than both parents).
Fit-ness is evaluated according to three fitness cri-teria.
The first two follow directly from thetask description: (1) size of S (smallness), and(2) ability to parse strings in D + (consistency),where ability to partially parse strings is alsorewarded.
Used on their own, however, thesecriteria lead search in the direction of universalautomata that produce all strings x E I= Up tothe length of the longest string in D +.
To avoidthis, (3) an overgeneration criterion is addedthat requires automata to achieve a given degreeof generalisation, such that the size of L(A) isequal to the size of the target language (wherethe target language is not known, its size is es-1Developed jointly with Berkan Eskikaya, COGS,University of Sussex, and described in more detail in(Belz and Eskikaya, 1998)S "t" SuEt.size (Avg)8 016 3(1.4)32 6(3.7)48 10(5.2)64 8(5.4)80 9(6.0)96 9(6.2)112 9(6.1)128 9(6.4)Targetfound(Avg)33(49)40(67)46(83)45(68)37(73)35(75)46(73)36(77)Conver-genceat gen.201211172171191142114158135Accurate(Target)automata025(21)58(49)99(92)79(73)79(79)s0(76)s0(s0)89(89)Table 1: Toy data set results.timated).
Transitions that are not required toparse any string in D + are eliminated.
Fitnesscriteria 1-3 are weighted (reflecting their rela-tive importance to fitness evaluation).
Theseweights can be manipulated to directly affectthe structural and functional properties of au-tomata.4 Resu l tsToy Data  Set The data used in the first setof tests was generated with the grammar S --,cA, A --* C lV lB  , A ~ c2v2B,  B "-'* c. Here, cabbreviates a set of 4 consonants, cl 2 sharpedconsonants, c2 2 non-sharped consonants, v 1 2front vowels, and v2 2 non-front vowels.
Thegrammar (generating a total of 128 strings) isa simple version of a phonotactic onstraint inRussian, where non-sharped consonants cannotbe followed by front vowels, (e.g.
Halle (1971)).Tests were carried out for different-size ran-domly selected language subsets.
Different com-binations of weights for the three fitness crite-ria were tested.
Table 1 gives results for thebest weight combination averaged over 10 runsdiffering only in random population initialisa-tion (and results averaged over all other weightcombinations in brackets).
The first column in-dicates how many successful runs there were outof 10 for the best weight combination (and theaverage for all weight combinations in brack-ets).
A run was deemed successful if the targetautomaton was found before convergence.
Thelast column shows how many accurate automatawere in final populations, and in brackets howmany of these also matched the target FSA insize.1473Train.
Test.Best 100% 61%Avg 94% 49%General.
States100 7100.3 7.1Table 2: Results for Russian noun data set.The general effects of reducing the size of D +are that successful runs become less likely, andthat the weight assigned to the degree of over-generation becomes more important and tendsto have to be increased.
The larger the data set,the more similar the results that can be achievedwith different weights.Russ ian  Noun Data  Set The data used inthe second series of tests were bisyllabic femi-nine Russian nouns ending in -a.
The alphabetconsisted of 36 phonemic symbols 2.
The train-ing set contained 100 strings, and a related setof 100 strings was used as a test set.Results are shown in Table 2.
The targetdegree of overgeneration was set to 100 timesthe size of the data set.
Tests were carried outfor different weights assigned to the overgener-ation criterion.
Results are given for the bestautomaton found in 10 runs for the best weightsettings, and for the corresponding averages forall 10 runs.Figure 1 shows the fittest automaton fromTable 1.
Phonemes are grouped together (aslabel sets on arcs) in several inguistically use-ful ways.
Vowels and consonants are separatedcompletely.
Vowels are separated into the setof those that can be preceded by sharped con-sonants (capitalised symbols) and those thatcannot.
Correspondingly, sharped consonantstend to be separated from nonsharped equiva-lents.
The phonemes k, r ,  1: are singled out(arc 4 ~ 5) because they combine only withnonsharped consonants to form stem-final clus-ters.
The groupings S (0 ---* 6) and L,M,P,R(6 ---* 1) reflect stem-initial consonant clusters.These groupings are typical of the automatadiscovered in all 10 runs.
Occasionally ka (thefeminine diminutive nding) was singled out asa separate nding, and the stem vowels werefrequently grouped together more efficiently.Different degrees of generalisaton were2The set of phonemic symbols used here is based onHaJJe (Halle, 1971).
Capital symbols represent sharpedversions of non-capitalised counterparts.Figure 1: Best automaton for Russian noun set.achieved with different weight settings.
The au-tomaton shown here corresponds most closely to(Halle, 1971).5 Conc lus ion  and  Fur ther  ResearchThis paper presented a GA for FSA inductionand preliminary results for a toy data set and asimple set of Russian nouns.
These results in-dicate that genetic search can successfully beapplied to the automatic discovery of finite-state automata that encode (parts of) phono-logical grammars from arbitrary subsets of pos-itive data.
A more detailed description of theGA and a report on subsequent results can befound in Belz and Eskikaya (1998).
The methodis currently being extended to allow for sets ofnegative xamples.Re ferencesD.
Angluin.
1978.
On the complexity of min-imum inference of regular sets.
Informationand Control, 39:337-350.A.
Belz and B. Eskikaya.
1998.
A genetic al-gorithm for finite-state automaton i duction.Cognitive Science Research Paper 487, Schoolof Cognitive and Computing Sciences, Uni-versity of Sussex.K.
S. Fu and T. L. Booth.
1986.
Grammati-cal inference: introduction and survey.
IEEETransactions on Pattern Analysis and Ma-chine Intelligence, PAMI-8:343-375.E.
M. Gold.
1978.
Complexity of automatonidentification from given data.
Informationand Control, 37:302-320.D.
E. Goldberg.
1989.
Genetic Algorithms insearch, optimization and machine learning.Addison-Wesley.M.
Halle.
1971.
The Sound Pattern of Russian.Mouton, The Hague.B.
B. Trakhtenbrot and Ya.
Barzdin.
1973.
Fi-nite Automata.
North Holland, Amsterdam.1474
