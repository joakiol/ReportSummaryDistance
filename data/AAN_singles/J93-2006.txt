Coping with Ambiguity and UnknownWords through Probabilistic ModelsRalph Weischedel*BBN Systems and TechnologiesRichard Schwartz*BBN Systems and TechnologiesJeff Palmucci*BBN Systems and TechnologiesMarie Meteer tRensselaer Polytechnic InstituteLance Ramshaw*Bowdoin CollegeFrom spring 1990 through fall 1991, we performed a battery of small experiments o test theeffectiveness of supplementing knowledge-based techniques with probabilistic models.
This paperreports our experiments in predicting parts of speech of highly ambiguous words, predicting theintended interpretation of an utterance when more than one interpretation satisfies all knownsyntactic and semantic onstraints, and learning case frame information for verbs from exampleuses.From these experiments, we are convinced that probabilistic models based on annotatedcorpora can effectively reduce the ambiguity in processing text and can be used to acquire lexicalinformation from a corpus, by supplementing knowledge-based techniques.Based on the results of those experiments, we have constructed a new natural languagesystem (PLUM)for extracting data from text, e.g., newswire text.1.
IntroductionNatural language processing, and AI in general, have focused mainly on buildingrule-based systems with carefully handcrafted rules and domain knowledge.
Our ownnatural language database query systems, JANUS (Weischedel t al.
1989), ParlanceTM, 1and Delphi (Stallard 1989), have used these techniques quite successfully.
However, aswe move from the application of understanding database queries in limited domainsto applications of processing open-ended text, we found challenges that questionedour previous assumptions and suggested probabilistic models instead.1.
We could no longer assume a limited vocabulary.
Rather in the domainof terrorist incidents of the Third Message Understanding conference(MUC-3) (Sundheim 1991), roughly 20,000 vocabulary items appear in acorpus 430,000 words long.
Additional text from that domain wouldundoubtedly contain ew words.
Probabilistic models offer a mathematicallygrounded, empirically based means of predicting the most likely interpretation.
* BBN Systems and Technologies, 70 Fawcett Street, Cambridge MA 02138.t Sage Lab, Rensselaer Polytechnic Institute, Troy NY 12180.:~ Computer Science Department, Bowdoin College, Brunswick ME 04011.1 Parlance isa trademark of BBN Systems and Technologies.
(~) 1993 Association for Computational LinguisticsComputational Linguistics Volume 19, Number 22.
Having semantics for all (or even most) of the words of the vocabularywould violate the limited domain assumption, since roughly 50% of themessage stream mentions no terrorist incident, and even those that domay be primarily about a different opic or topics.
Therefore, the powerof semantic onstraints in limited domains would be diluted.
Probabilitymodels could be employed where less knowledge was available.3.
Given the vocabulary size, we could not expect o give full syntactic orsemantic features.
The labor for handcrafted definitions would not bewarranted.
Statistical language models have a learning component that mightsupplement handcrafted knowledge.4.
Purely rule-based techniques seemed too brittle for dealing with thevariety of constructions, the long sentences (averaging 29 words persentence), and the degree of unexpected input.
Statistical models based onlocal information (e.g., DeRose 1988; Church 1988) might operate ffectively inspite of sentence l ngth and unexpected input.To see whether our four hypotheses (in italics above) effectively addressed thefour concerns above, we chose to test the hypotheses on two well-known problems:ambiguity (both at the structural level and at the part-of-speech level) and inferringsyntactic and semantic information about unknown words.Guided by the past success of probabilistic models in speech processing, we haveintegrated probabilistic models into our language processing systems.
Early speechresearch used purely knowledge-based approaches, analogous to knowledge-basedapproaches in NLP systems today.
These required much detailed, handcrafted knowl-edge from several sources (e.g., acoustic and phonetic).
However, when it became clearthat these techniques were too brittle and not scalable, speech researchers turned toprobabilistic models.
These provided a flexible control structure for combining mul-tiple sources of knowledge (providing improved accuracy and ability to deal withmore complex domains) and algorithms for training the system on large bodies ofdata (providing reduced cost in moving the technology to a new application domain).Since probability theory offers a general mathematical modeling tool for estimatinghow likely an event is, probability theory may be applied at all levels in naturallanguage processing, because some set of events can be associated with each algorithm.For example, in morphological processing in English (Section 2), the events are the useof a word with a particular part of speech in a string of words.
At the level of syntax(Section 3), an event is the use of a particular structure; the model predicts what themost likely rule is given a particular situation.
One can similarly use probabilities forassigning semantic structure (Section 4).We report in Section 2 on our experiments on the assignment of part of speech towords in text.
The effectiveness of such models is well known (DeRose 1988; Church1988; Kupiec 1989; Jelinek 1985), and they are currently in use in parsers (e.g.
de Mar-cken 1990).
Our work is an incremental improvement on these models in three ways:(1) Much less training data than theoretically required proved adequate; (2) we inte-grated a probabilistic model of word features to handle unknown words uniformlywithin the probabilistic model and measured its contribution; and (3) we have appliedthe forward-backward algorithm to accurately compute the most likely tag set.In Section 3, we demonstrate that probability models can improve the performanceof knowledge-based syntactic and semantic processing in dealing with structural am-biguity and with unknown words.
Though the probability model employed isnot new,our empirical findings are novel.
When a choice among alternative interpretations pro-360Ralph Weischedel t al.
Coping with Ambiguity and Unknown Wordsduced by a unification-based parser and semantic interpreter must be made, a simplecontext-free probability model reduced the error rate by a factor of two compared withusing no model.
It is well known that a unification parser can process an unknownword by collecting the assumptions it makes while trying to find an interpretationfor a sentence.
As a second result, we found that adding a context-free probabilitymodel improved the unification predictions of syntactic and semantic properties of anunknown word, reducing the error rate by a factor of two compared with no model.In Section 4, we report an experiment in learning case frame information of un-known verbs from examples.
The probabilistic algorithm is critical to selecting theappropriate generalizations to make from a set of examples.
The effectiveness of thesemantic ase frames inferred is measured by testing how well those case frames pre-dict the correct attachment point for prepositional phrases.
In this case, a significantnew model synthesizing both semantic and syntactic knowledge is employed.2.
POST: Using Probabilities to Tag Part of SpeechIdentifying the part of speech of a word illustrates both the problem of ambiguity andthe problem of unknown words.
Many words are ambiguous in several ways, as inthe following:a round table: adjectivea round of cheese: nounto round out your interests: verbto work the year round: adverbEven in context, part of speech can be ambiguous, as in the famous example: "Timeflies like an arrow," where the first three words are ambiguous in two ways, result-ing in four grammatical interpretations of the sentence.
In processing text such asnewswire, ambiguity at the word level is high.
In an analysis of texts from the WallStreet Journal (wsJ), we found that the average number of parts of speech per wordwas approximately two.Determining the part of speech of an unknown word can help the system to knowhow the word functions in the sentence; for instance, that it is a verb stating an actionor state of affairs, that it is a common oun stating a class of persons, places, or things,that it is a proper noun naming a particular person, place, or thing, etc.
If it can dothat well, then more precise classification and understanding is feasible.The most critical feature to us is to have local criteria for ranking the alternativeparts of speech, rather than relying solely on a globally correct parse.
The probabilitymodel we selected offers these features.The name of our component for part of speech is POST (part-of-speech tagger).2.1 The n-Gram ModelIn our work, we have used well-known probability models known as Hidden MarkovModels; therefore, none of the background in Section 2.1 is novel.
If we want todetermine the most likely syntactic part of speech or tag for each word in a sentence,we can formulate a probabilistic tagging model.
Let us assume that we want to knowthe most likely tag sequence, T = {h, t2,..., tn}, given a particular word sequence,361Computational Linguistics Volume 19, Number 2W = {wl, w2,... Wn}.
Using Bayes' rule we know thatp(T \[ W) - p(T)p(W \] T)p(W)where p(T) is the a priori probability of tag sequence T, p(W \] T) is the conditionalprobability of word sequence W occurring given that a sequence of tags T occurred,and p(W) is the unconditioned probability of word sequence W. Then, in principle,we can consider all possible tag sequences, evaluate p(T \] W) of each, and choose thetag sequence T that is most likely, i.e., the sequence that maximizes p(T I W).
Since Wis the same for all hypothesized tag sequences, we can disregard p(W).We can rewrite the probability of each sequence as a product of the conditionalprobabilities of each word or tag given all of the previous tags.p(T \] W)p(W)-- hp( t i  \] ti_l,ti_2~...~tl)p(wi \] t i .
.
.
t l~wi-1.
.
.wl)i=1Typically, one makes two simplifying assumptions to cut down on the numberof probabilities to be estimated.
Rather than assuming wi depends on all previouswords and all previous tags, one assumes wi depends only on ti.
This independenceassumption, of course, is not correct.
Yet, it so reduces the number of probabilities thatmust be estimated and therefore so reduces the amount of data needed to estimateprobabilities, that it is a worthwhile simplifying assumption.
It is an empirical issuewhether alternative assumptions would yield significantly better performance.Second, rather than assuming the tag ti depends on the full sequence of previoustags, we can assume that local context is sufficient.
Typically, individuals have assumedtag ti depends only on ti-1 and ti_ 2 (a tri-tag model) or only on ti-1 (a bi-tag model).This assumed locality is termed a Markov independence assumption.Using a tri-tag model, we then have the following:np(T \[ W)p(W) = p(tl)p(t2 \] tl) I-I p(ti \] ti_l,ti_2) p(wi \[ ti)i=3If we have sufficient raining data, we can estimate the tag n-gram sequence ofprobabilities and the probability of each word given a tag (lexical probabilities).
Usinga tagged corpus to train the model is called "supervised training," since a human hasprepared the correct raining data.
We conducted supervised training to derive both abi-tag and a tri-tag model based on a corpus from the University of Pennsylvania, whichwas created as part of the TREEBANK project (Santorini 1990) consisting of Wall StreetJournal (WSJ) articles, texts from the Library of America, transcribed radio broadcasts,and transcribed ialogues.
The full TREEBANK consists of approximately 4 millionwords of text.
Of the 47 parts of speech, 36 are word tags, and 11 are punctuation tags.Of the word tags, 22 are tags for open class words and 14 for closed class words.
Eachword or punctuation mark has been tagged, as shown in the following example, whereNNS is plural noun; VBD is past tense verb; RB is adverbial; VBN is past participleverb.Terms / NNS were / VBD not / RB disclosed / VBN .
/ .A bi-tag model predicts the relative likelihood of a particular tag given the pre-ceding tag, e.g., how likely is the tag VBD on the second word in the above example,given that the previous word was tagged NNS.
A tri-tag model predicts the relative362Ralph Weischedel t al.
Coping with Ambiguity and Unknown Wordslikelihood of a particular tag given the two preceding tags, e.g., how likely is the tagRB on the third word in the above example, given that the two previous words weretagged NNS and VBD.
While the bi-tag model is faster at processing time, the tri-tagmodel has a lower error rate.The algorithm for supervised training is straightforward.
One counts for eachpossible pair of tags, the number of times that the pair was followed by each possiblethird tag.
The number of times a given third tag t' occurs after tags tl and t2 dividedby the number of times tl and t2 are followed by any third tag is an estimate of theprobability of p(t' I t2~ tl).One also estimates from the training data the conditional probability of each par-ticular word given a known tag (e.g., how likely is the word "terms" if the tag isNNS); this is called the "word emit" probability.
This is simply the number of timesa particular word appears as part of speech t, divided by the number of times part ofspeech t appears in the corpus.No matter how large the training corpus, one may not see all pairs or triples oftags, nor all words used in each part of speech possible in the language, nor all words.It seems unwise to assume that the probability of an unseen event is zero.
To dealwith the previously unseen, one employs one of several estimation techniques called"padding."
Thus far, we have employed the simplest of these techniques for estimatingp(t3 \[ t2tl) if tlt2t3 was not present in the training corpus.
Suppose triples beginningwith ht2 appear m times in the corpus.
Suppose further that for j distinct ags t, tlt2t'was not present in the corpus.
Then, we estimate p(t I t2tl) = 1/m (as if it actuallyhad been seen once).
So that the probability of tags given tit2 sum to one, we subtract1/jm from the probability of each triple that actually was observed in the corpus, i.e.,if ht2t ~ was observed k times in the corpus, then we estimate p(t ~ I ht2) = k/m - 1/jm.Given these probabilities, one can then find the most likely tag sequence for agiven word sequence.
Using the Viterbi algorithm, we selected the path whose overallprobability was highest, and then took the tag predictions from that path.
We replicatedthe earlier results that this process is able to predict the parts of speech with only a3-4% error rate when the possible parts of speech of each of the words in the corpusare known.
This is in fact about the rate of discrepancies among human taggers onthe TREEBANK project (Marcus, Santorini, and Magerman 1990).2.2 How Much Training Data Is Required?While supervised training is shown here to be very effective, it requires a correctlytagged corpus.
How much manually annotated ata is required?In our experiments, we demonstrated that the training set can, in fact, be muchsmaller than might have been expected.
One rule of thumb suggests that the trainingset needs to be large enough to contain on average ten instances of each type of tagsequence that occurs.
This would imply that a tri-tag model using 47 possible partsof speech would need a bit more than 1 million words of training, if all possible tagsequences occur.
However, we found that much less training data is necessary, sincemany possible sequences do not occur.It can be shown that if the average number of tokens of each tri-gram that hasbeen observed is ten, then the lower bound on the probability of new tri-grams is1/10.
Thus the likelihood of a new tri-gram is fairly low.While theoretically the set of possible events is all permutations of the tags, inpractice only a relatively small number of tri-tag sequences actually occur.
Out ofabout 97,000 possible triples, we found only 6,170 unique triples when we trainedon 64,000 words, and about 10,000 when we trained on 1,000,000 words.
Thus, even363Computational Linguistics Volume 19, Number 2~3vI I I I I64K 250K 500K 750K 1MSIZI~.
OF TRAINING SETFigure 1Size of tri-tag training sets.though an additional 4,000 sequences are observed in the full training set, they are sorare (0.4%) that they do not significantly affect he overall accuracy.In our initial experiments, which were limited to known words, the error rate fora supervised tri-tag model increased only from 3.30% to 3.87% when the size of thetraining set was reduced from 1 million words to 64,000 words (see Figure 1).
All thatis really necessary, recalling the rule of thumb, is enough training to allow for ten ofeach of the tag sequences that do occur.This result is applicable to new tag sets, subdomains, or languages.
We simplycontinue to increase the amount of training data until the number of training tokens isat least ten times the number of different sequences observed so far.
Alternatively, wecan stop when the singleton events account for a small enough percentage (say 5%)of the total data.
Thus, in applications such as tagging, where a significant numberof the theoretically possible events do not occur in practice, we can use supervisedtraining of probabilistic models without needing prohibitively large corpora.Of course, performance of POST is also affected by the estimates of p(wi I ti) forknown words and unknown words.
How to estimate p(wi \] ti) for unknown wordsis covered in the next section.
For an observed word, a small training set of 64,000words may still be adequate for estimates of p(wi I ti).
We found that by treating wordsobserved only once as if they had not been observed at all (and are thus handledby the probabilistic models for unknown words) that performance actually increasedslightly.
This suggests that adequate performance can be obtained from a relativelysmall training set.We are not aware of any other published studies documenting empirically theimpact of training set size on performance.2.3 Unknown WordsSources of open-ended text, such as a newswire, present natural language processingtechnology with a major challenge: what to do with words the system has never seenbefore.
Current echnology depends on handcrafted linguistic and domain knowledge.For instance, the system that performed most successfully in the evaluation of softwareto extract data from text at the Second Message Understanding Conference held at theNaval Ocean Systems Center, June 1989, would simply halt processing a sentencewhen a new word was encountered.364Ralph Weischedel t al.
Coping with Ambiguity and Unknown WordsUsing the UPenn set of parts of speech, unknown words can be in any of 22categories.
A tri-tag model can be used to estimate the most probable one.
Randomchoice among the 22 open classes would be expected to show an error rate for newwords of 95%.
The best previously reported error rate based on probabilistic modelswas 75% (Kuhn and DeMori 1990).In our first tests using the tri-tag model we showed an error rate of only 51.6%.However, this model only took into account the context of the word, and no informa-tion about the word itself.
In many languages, including English, word endings givestrong indicators of the part of speech.
Furthermore, capitalization i formation, whenavailable, can help to indicate whether a word is a proper noun.We have developed a novel probabilistic model that takes into account features ofthe word in determining the likelihood of the word given a part of speech.
This wasused instead of the "word emit" probabilities p(wi I ti) for known words.
To estimatep(wi I ti) for an unknown word, we first determined the features we thought woulddistinguish parts of speech.
There are four independent categories of features: inflec-tional endings, derivational endings, hyphenation, and capitalization; these are notnecessarily independent, though we are treating them as such for our tests.
Our initialtest had 3 inflectional endings (-ed, -s, -ing), and 32 derivational endings (including-ion, -al, -ive, -ly).
Capitalization has four values, in our system (+ initial + capitalized,- initial + capitalized, etc.)
in order to take into account the first word of a sentence.We can incorporate these features of the word into the probability that this particularword will occur given a particular tag using the following:p(wi I ti) = p(unknown-word I ti) ?
p(Capital - feature I ti) ?
p(endings/hyph I ti)We estimate the probability of each ending for each tag directly from supervisedtraining data.
While these probabilities are not strictly independent, the approximationis good enough to make a marked difference in classification of unknown words.
Asthe results in Figure 2 show, the use of orthographic endings of words reduces theerror rate on the unknown words by a factor of three.We tested capitalization separately, since some data, such as that in the Third Mes-sage Understanding Conference (Sundheim 1991) is uppercase only.
Titles and bibli-ographies will cause similar distortions in a system trained on mixed case and usingcapitalization as a feature.
Furthermore, some languages, uch as Japanese, have noexplicit marking of proper nouns.
Interestingly, the capitalization feature contributedvery little to the reduction in error rates, whereas using the word features contributeda great deal.
However, it does undeniably reduce confusion with respect to the propernoun category.Some well-known previous efforts (Church 1988; de Marcken 1990) have dealt withunknown words using various heuristics.
For instance, Church's program PARTS hasa prepass prior to applying the tri-tag probability model that predicts proper nounsbased on capitalization.
The new aspects of our work are (1) incorporating the treat-ment of unknown words uniformly within the probability model, (2) approximatingthe component probabilities for unknowns directly from the training data, and (3) mea-suring the contribution of the tri-tag model, of the ending, and of capitalization.In sum, adding a probability model of typical endings of words to the tri-tag modelhas yielded an accuracy of 82% for unknown words.
Adding a model of capitalizationto the other two models further increased the accuracy to 85%.
The total effect of BBN'smodel has been a reduction of a factor of five in the error rate of the best previouslyreported performance.365Computational Linguistics Volume 19, Number 2302C10NoFeatures40Only Endings and AllCapitalization hyphenation FeaturesOverall Error RateError rate for Known wordsError rate for Unknown wordsFigure 2Decreasing error rate with use of word features.2.4 K-best Tag SetsAn alternative mode of running POST is to return the set of most likely tags for eachword, rather than a single tag for each.In our first test, the system returned the sequence of most likely tags for thesentence.
This has the advantage of eliminating ambiguity; however, even with arather low error rate of 3.7%, there are cases in which the system returns the wrongtag, which can be fatal for a parsing system trying to deal with sentences averagingmore than 20 words in length.De Marcken (1990) developed an approximate method for finding multiple tagsfor each word given the preceding words and one following word.
We addressed thisproblem by adding the ability of the tagger to return for each word an ordered listof tags, marked by their probability using the Forward Backward algorithm (Baumand Eagon 1967).
That yields a more precise method of determining the probability ofeach possible tag since it sums over all possible tag sequences, taking into account heentire sentence and not just the preceding tags.
The Forward Backward algorithm isnormally used in unsupervised training to estimate the model that finds the maximumlikelihood of the parameters of that model.
The exact probability of a particular taggiven a particular word is computed irectly by the product of the "forward" and"backward" probabilities to that tag, divided by the probability of the word sequencegiven this model.Figure 3 shows k-best agging output, with the correct ag for each word markedin bold.
Note that the probabilities are in natural og base e. Thus for each differenceof 1, there is a factor of 2.718 in the probability.In two of the words ("Controls" and "computerized"), the first tag is not the366Ralph Weischedel t al.
Coping with Ambiguity and Unknown WordsBailey Controls, based in Wickliffe Ohio, makes computerized industrial controls ystems.Bailey (NP.
-1.17) (RB.
-1.35) (FW.
-2.32) (N'N.
-2.93) (NI~.
-2.95) (JJS.
-3.06) (JJ.
-3.31) (LS.
-3.41) (JJR?
-3.70) (NNS.
-3.73) (VBG, -3.91)...Controls (VBZ.
-0.19) (NNS.
-1.93) (NIPS.
-3.75) (N-P. -4.97)based (VBN.
-0.0001)in (IN.
-.001) (RBV.
-7.07) (N-P?
-9.002)Wickliffe (NIP.
-0.23) (NPS.
-1.54)Ohio (NIP.
-0.0001)makes (VBZ.-0.0001)computerized (VBN.
-0.23) (J J,&- 1.56)industrial (J$.
-0.19) (NP.
-1.73)controls (NNS.
-0.18) (VBZ.
-1.77)systems (NNS.
-0.43) (NPS.
-1.56) (NP.
-1.95)Figure 3K-best ags and probabilities.correct one.
However, in all instances the correct ag is included in the set.
Note thefirst word, "Bailey," is unknown to the system, therefore, all of the open class tags arepossible.In order to reduce the ambiguity further, we tested various ways to limit howmany tags were returned based on their probabilities.
Often one tag is very likelyand the others, while possible, are given a low probability, as in the word "in" above.Therefore, we tried removing all tags whose probability was less than some arbitrarythreshold (similar to de Marcken's "factor"), for example removing all tags whoselikelihood is more than e 2 less likely than the most likely tag.
So only tags within thethreshold 2.0 of the most likely would be included (i.e., if the most likely tag had alog probability of -0.19, only tags with a log probability greater than -2.19 would beincluded)?
This reduced the ambiguity for known words from 1.93 tags per word to1.23, and for unknown words, from 15.2 to 2.0.However, the negative side of using cutoffs is that the correct ag may be excluded.Note that a threshold of 2.0 would exclude the correct ag for the word "Controls"above.
By changing the threshold to 4.0, we are sure to include all the correct ags inthis example, but the ambiguity for known words increases from 1.23 to 1.24, and forunknown words from 2.0 to 3.7, for an ambiguity rating of 1.57 overall.We are continuing experiments o determine the most effective way of limitingthe number of tags returned, and hence decreasing ambiguity, while ensuring that thecorrect ag is likely to be in the set.
Balancing the tradeoff between ambiguity andaccuracy is very dependent on the use the tagging will be put to.
It is dependentboth on the component that the tagged text directly feeds into, such as a parser thatcan efficiently follow many parses, but cannot recover easily from errors versus one367Computational Linguistics Volume 19, Number 2capable of returning a partial parse, and on the application, such as an applicationrequiring high accuracy (database query) versus one requiring high speed (processingnewswire text as it comes in).2.5 Moving to a New DomainIn all of the tests discussed so far, we both trained and tested on sets of articles in thesame domain, the Wall Street Journal subset of the Penn TREEBANK Project.
However,an important measure of the usefulness of the system is how well it performs in otherdomains.
While we would not expect high performance in radically different kinds oftext, such as transcriptions of conversations or technical manuals, we would hope forsimilar performance on newspaper articles from different sources and on other topics.We tested this hypothesis using data from the Third Message Understanding Con-ference (MUC-3).
The goal of MUC-3 was to extract data from texts on terrorism inLatin American countries.
The texts are a mixture of news, interviews, and speeches.The University of Pennsylvania TREEBANK project agged 400 MUC messages (ap-proximately 100,000 words), which we divided into 90% training and 10% testing.For our first test, we used the original probability tables trained from the WallStreet Journal articles, but tested on MUC messages.
We then retrained the probabilitieson the MUC messages and ran a second test on MUC messages, with an averageimprovement of three percentage points in both bi- and tri- tags.
The full results areshown in Figure 4; 8.5% of the words in the test were unknown:While the results using the new tables are an improvement in these first-best tests,we saw the best results using K-best mode, which obtained a .7% error rate.
We ranseveral tests using our K-best algorithm with various thresholds.
As described in Sec-tion 2.4, the threshold limits how many tags are returned based on their probabilities.While this reduces the ambiguity compared to considering all possibilities, it also in-creases the error rate.
Figure 5 shows this tradeoff rom effectively no threshold, onthe right-hand side of the graph (shown in the figure as a threshold of 12), whichhas a .7% error rate and an ambiguity of 3, through a cutoff of 2, which has an errorrate of 2.9, but an ambiguity of nearly zero--i.e., one tag per word.
(Note that the farleft of the graph is the error rate for a cutoff of 0, that is, only considering the firstof the k-best tags, which is approximately the same as the bi-tag error rate shown inFigure 4.
)2.6 Using DictionariesIn all of the results reported here, we are using word/part-of-speech tables derivedfrom training, rather than on-line dictionaries to determine the possible tags for agiven word.
The advantage of the tables is that the training provides the probabilityof a word given a tag, whereas the dictionary makes no distinctions between commonand uncommon uses of a word.
The disadvantage of this is that uses of a word thatdid not occur in the training set will be unknown to the system.
For example, in thetraining portion of the WSJ corpus, the word "put" only occurred as a verb.
However,in our test set, it occurred as a noun in the compound "put option."
Since for efficiencyreasons, we only consider those tags known to be possible for a word, this will causean error.We have since integrated on-line dictionaries into the system, so that alternativeword senses will be considered, while still not opening the set of tags considered fora known word to all open class tags.
This will not completely eliminate the problem,since words are often used in novel ways, as in this example from a public radio pleafor funds: "You can Mastercard your pledge.
"368Ralph Weischedel t al.
Coping with Ambiguity and Unknown WordsBI-TAGS: TEST 1 TEST 2Overall error rate: 8.5 5.6Number of correct tags: 10340 10667Number of incorrect tags: 966 639Error rate for known words: 6.3 4.6Error rate for unknown words: 25 16TRI-TAGS:Overall error rate: 8.3 5.7Number of correct tags: 10358 10651Number of incorrect tags: 948 655Error rate for known words: 5.9 4.6Error rate for unknown words: 26 18Figure 4Comparison of original and trained probabilities.3.
Pars ing w i th  a Context-Free BackboneThe performance of today's natural anguage understanding systems is hindered bythe following three complementary problems:1.
Frequently more than one interpretation remains even after all linguisticand domain knowledge has been used in processing an input.2.
Partial interpretation, when no complete interpretation can be found, isminimal.3.
Finding any interpretation if the input includes an unknown word.Our results on problems (1) and (3) above are presented in this section.
The problem ofpartial interpretation when no complete interpretation can be found is touched uponin Section 4.Probabilities can quantify the likelihood of alternative complete interpretations ofa sentence.
In these experiments, we used the grammar of the Delphi component fromBBN's HARC system (Stallard 1989), which combines yntax and semantics in a uni-fication formalism.
We employed a context-free model, which estimates the probability369Computational Linguistics Volume 19, Number 220 j- Overa l l  Error Rate.
~t~-.
Known Words.................... ~:~ ..................
Unknown Wordst_O z_t -10!i::::iii!iii~ ~:!iiiii~:______ .
.
.= 2 4 6 12Average # 1Tags1.1 1 .2  1.3 3ThresholdFigure 5Comparison of thresholds for K-best.of each rule in the grammar independently (in contrast o a context-sensitive model,such as the tri-tag model described above, which bases the probability of a tag onwhat other tags are in the adjacent context).In the context-free model, we associate a probability with each rule of the grammar.For each distinct major category (left-hand side) of the grammar, there is a set ofcontext-free rulesLHS ,-- RHS1LHS *-- RHS2LHS ~ RHSn.For each rule, one estimates the probability of the right-hand side given the left-hand side, p(RHSj I LHS).
With supervised training, where a set of correct parse treesis provided as training, one estimates p(RHSj I LHS) by the number of times ruleLHS *--- RHSj appears in the training set divided by the number of times LHS appearsin the trees.The probability of a syntactic structure S, given the input string W, is then modeledby the product of the probabilities of the rules used in S. Chitrao and Grishman (1990)used a similar context-free model.
Using this model, we explored the following issues:What method of training the rule probabilities hould be employed?
Ourresults were much more effective with supervised training, whichexplains why the model performed better in our experiments hanChitrao and Grishman found with unsupervised training.370Ralph Weischedel t at.
Coping with Ambiguity and Unknown WordsSTARTIS/ / / \/ / / \ vPNP / / ORD N-BAR / ~  ~ V P  .LPRO V DET ORDDIGIT N DET N V V DET Nit is the third bid the company has gotten \[ \] this yearRules UsedSTART --) SS--)NP VPVP---) V NPNP--~ DET ORD N-BAR NPADJUNCTp(TREE\[W) = !-~ PiFigure 6Probability of a parse tree given words.?
How much (little) training data is required for reliable estimates?
Aslittle as 80 sentences of supervised training proved adequate for agrammar of about 1,000 rules.?
How is system performance influenced?
Contrasted with no model, thecontext-free probability model reduced the error rate by a factor of two.?
Do the results suggest refinements in the probability model?
Extensionsto account for lowest attachment are suggested by an analysis of errorsoccurring in the test sets.3.1 Selecting among InterpretationsOur intention is to use the TREEBANK corpus being developed at the Universityof Pennsylvania s a source of correct structures for training.
However, in our firstexperiments, we used small training sets taken from an existing question-answeringcorpus of sentences about a personnel database.
To our surprise, we found that aslittle as 80 sentences of supervised training (in which a person, using graphical tools,identifies the correct parse) are sufficient o improve the ranking of the interpretationsfound.
In our tests, the NLP system produces all interpretations satisfying all syntacticand semantic onstraints.
From that set, the intended interpretation must be chosen.The context-free probability model reduced the error rate on an independent test setComputational Linguistics Volume 19, Number 230~ 20"~10"UNSUPERVISED SUPERVISEDTEST 1 TEST 2 TEST 3I \] Best PossibleChauc~TestFigure 7Predictions of probabilistic language model.by a factor of two to four, compared with no model, i.e., random selection from theinterpretations satisfying all knowledge-based constraints.We tested the predictive power of rule probabilities using this model both in un-supervised and in supervised mode.
In the former case, the input is all parse trees(whether correct or not) for the sentences in the training set.
In the latter case, the train-ing data included a specification of the correct parse as hand picked by the grammar'sauthor from among the parse trees produced by the system.The detailed results from using a training set of 81 sentences appear in the his-togram in Figure 7.
The fact that so little data was adequate deserves further scrutiny.The grammar had approximately 1,050 rules, one third of which are lexical, e.g., a cate-gory goes to a word.
Estimating the lexical evel is best handled via the part-of-speechtechniques covered in the previous ection.
Therefore, there were 700 nonlexical rules.The training corpus consisted of 81 sentences whose parses averaged approximately35 rules per sentence.
Therefore, the corpus of trees included approximately 2,850 ruleoccurrences, or about 4 per rule on average over all rules.
However, as few as half ofthe rules were actually employed, leading to an average of roughly 8 rule occurrencesper rule observed.
Therefore, there was close to the amount of data one would predictas desirable.One further note about counting rule occurrences in the unification grammar.Rather than counting different unification bindings as different rules, we counted therule with unbound variables, representing an equivalence class of rules with boundvariables.The "best possible" error rates for each test indicates the percentage of cases forwhich none of the interpretations produced by the system was judged correct, sothat no selection scheme could achieve a lower error rate than that.
The "chance"score gives the error rate that would be expected with random selection from allinterpretations produced.
The "test" column shows the error rate with the supervisedor unsupervised probability model in question.
The first supervised test had an 81.4%improvement, the second a 50.8% improvement, and the third a 56% improvement.372Ralph Weischedel t al.
Coping with Ambiguity and Unknown WordsThese results tate how much better than chance the given model did as a percentageof the maximum possible improvement.We expect o improve the model's performance by recording probabilities for otherfeatures in addition to just the set of rules involved in producing them.
For example,in the grammar used for this test, two different attachments for a prepositional phraseproduced trees with the same set of rules, but differing in shape.
Thus the simple,context-free model based on the product of rule probabilities could not capture pref-erences concerning such attachment.
By adding to the model probabilities for suchadditional features, we expect hat the power of the probabilistic model to automati-cally select he correct parse can be substantially increased.Second, a much more reliable estimate of p(word I category) can be estimated asdescribed in Section 2.
In fact, one should be able to improve the estimate of a tree'slikelihood via p(S I W) = p(S \] T) ?
p(T I W).3.2 Learning Lexical Syntax and Lexical SemanticsOne purpose for probabilistic models is to contribute to handling new words or par-tially understood sentences.
We have done preliminary experiments hat show thatthere is promise in learning lexical, syntactic, and semantic features from context whenprobabilistic tools are used to help control the ambiguity.In our experiments, we used a corpus of sentences each with one word that thesystem did not know.
To create the corpus, we began with a corpus of sentences knownto parse from a personnel question-answering domain.
We then replaced one word ineach sentence with an undefined word.For example, in the following sentence, the word "contact" is undefined in thesystem: Who in Division Four is the contact for MIT?
That word has both a noun and averb part of speech; however, the pattern of parts of speech of the words surrounding"contact" causes the tri-tag model to return a high probability that the word is a noun.Using unification variables for all possible features of a noun, the parser producesmultiple parses.
Applying the context-free rule probabilities to select he most probableof the resulting parses allows the system to conclude both syntactic and semantic factsabout "contact."
Syntactically, the system discovers that it is a count noun, with thirdperson singular agreement.
Semantically, the system learns (from the use of "who")that "contact" is in the semantic lass PERSONS.Furthermore, the partially specified semantic representation for the sentence as awhole also shows the semantic relation to SCHOOLS, which is expressed here by thefor phrase.
Thus, even a single use of an unknown word in context can supply usefuldata about its syntactic and semantic features.Probabilistic modeling plays a key role in this process.
While context-sensitivetechniques for inferring lexical features can contribute a great deal, they can still leavesubstantial mbiguity.
As a simple example, suppose the word "list" is undefined inthe sentence List the employees.
The tri-tag model predicts both a noun and a verb part ofspeech in that position.
Using an underspecified noun sense combined with the usualdefinitions for the rest of the words yields no parses.
However, an underspecifiedverb sense yields three parses, differing in the subcategorizafion frame of the verb"list."
For more complex sentences, even with this very limited protocol, the numberof parses for the appropriate word sense can reach into the hundreds.Using the rule probabilities acquired through supervised training (described inthe previous ection), the likelihood of the ambiguous interpretations resulting froma sentence with an unknown word was computed.
Then we tested whether the treeranked most highly matched the tree previously selected by a person as the correct one.This tree equivalence t st was based on the tree's tructure and on the rule applied at373Computational Linguistics Volume 19, Number 2each node; while an underspecified tree might have some less-specified feature valuesthan the chosen fully specified tree, it would still be equivalent in the sense above.Of 160 inputs with an unknown word, in 130 cases the most likely tree matchedthe correct one, for an error rate of 18.75%, while picking at random would haveresulted in an error rate of 37%, for an improvement by a factor of 2.
This suggeststhat probabilistic modeling can be a powerful tool for controlling the high degree ofambiguity in efforts to automatically acquire lexical data.We have also begun to explore heuristics for combining lexical data for a singleword acquired from a number of partial parses.
There are some cases in which the bestapproach is to unify the two learned sets of lexical features, o that the derived sensebecomes the sum of the information learned from the two examples.
For instance, theverb subcategorization nformation learned from one example could be thus combinedwith agreement information learned from another.
On the other hand, there are manycases, including alternative subcategorization frames, where each of the encounteredoptions needs to be included as a separate alternative.4.
Partial Parsing and Learning Case Frame InformationTraditionally, natural anguage processing (NLP) has focused on obtaining completesyntactic analyses of all input and on semantic analysis based on handcrafted knowl-edge.
However, grammars are incomplete, text often contains new words, and thereare errors in text.
Furthermore, as research activities tackle broader domains, if theresearch results are to scale up to realistic applications, handcrafting knowledge mustgive way to automatic knowledge base construction.An alternative to traditional parsers is represented in FIDDITCH (Hindle 1983),MITFP (de Marcken 1990), and CASS (Abney 1990).
Instead of requiring completeparses, a forest is frequently produced, each tree in the forest representing a nonover-lapping fragment of the input.
However, algorithms for finding the semantics of thewhole from the disjoint fragments have not previously been developed or evaluated.We have been comparing several differing algorithms from various sites to evalu-ate both the effectiveness of such a strategy in correctly predicting fragments.
This isreported first (Section 5.1).The central experiment in this section tests the feasibility of learning case frameinformation for verbs from examples.
In the method tested, we assume that a body offully parsed sentences, uch as those from TREEBANK, are available, we furthermoreassume that every head noun and head verb has a lexical ink to a unary predicate ina taxonomic domain model; that unary predicate is the most specific semantic lass ofentities denoted by the headword.
From the parsed examples and the lexical inks tothe domain model, an algorithm identifies case frame relations for the verbs.4.1 Finding Core Noun PhrasesIf an algorithm is to learn case frame relations (or selection restrictions) from text, abasic concern is to reliably identify noun phrases and their semantic ategory, even ifneither full syntactic nor full semantic analysis is possible.
First, we discuss reliablyfinding them based on local syntactic information.
In the next section, we describefinding their semantic ategory.Two of our experiments have focused on the identification of core noun phrases,a primary way of expressing entities in text.
A core NP is defined syntactically asthe maximal simple noun phrase, i.e., the largest one containing no post-modifiers.374Ralph Weischedel t al.
Coping with Ambiguity and Unknown WordsHere are some examples of core NPs (marked by italics) within their full noun phrases:a joint venture with the Chinese government tobuild an automobile-partsassembly planta $50.9 million loss from discontinued operations in the third quarterbecause of the proposed saleSuch complex, full NPs require too many linguistic decisions to be directly pro-cessed without detailed syntactic and semantic knowledge about each word, an as-sumption that need not be true for open-ended text.We tested two differing algorithms on text from the Wall Street Journal (WSJ).
UsingBBN's part-of-speech tagger (POST), tagged text was parsed using the full unificationgrammar of Delphi to find only core NPs, 695 in 100 sentences.
Hand-scoring ofthe results indicated that 85% of the core NPs were identified correctly.
Subsequentanalysis uggested that half the errors could be removed with only a little additionalwork, suggesting that over 90% performance is achievable.In a related test, we explored the bracketings produced by Church's PARTS pro-gram (Church 1988).
We extracted 200 sentences of WSJ text by taking every tenthsentence from a collection of manually corrected parse trees (data from the TREE-BANK Project at the University of Pennsylvania).
We evaluated the NP bracketings inthese 200 sentences by hand and tried to classify the errors.
Of 1226 phrases in the 200sentences, 131 were errors, for a 10.7% error rate.
The errors were classified by handas follows:?
Two consecutive but unrelated phrases grouped as one: 10?
Phrase consisted of a single word, which was not an NP: 70?
Missed phrases (those that should have been bracketed but were not): 12?
Ellided head (e.g.
part of a conjoined premodifier to an NP): 4?
Missed premodifiers: 4?
Head of phrase was verb form that was missed: 4?
Other: 27.The 90% success rate in both tests suggests that identification of core NPs canbe achieved using only local information and with minimal knowledge of the words.Next we consider the issue of what semantics should be assigned and how reliablythat can be accomplished.4.2 Semantics of Core Noun PhrasesIn trying to extract pre-specified data from open-ended text such as a newswire, it isclear that full semantic interpretation of such texts is not on the horizon.
However,our hypothesis that it need not be for automatic data base update.
The type of infor-mation to be extracted permits ome partial understanding.
For semantic processing,minimally, for each noun phrase (NP), one would like to identify the class in thedomain model that is the smallest pre-defined class containing the NP's denotation.Since we have assumed that the lexicon has a pointer to the most specific class in the375Computational Linguistics Volume 19, Number 2domain model, the issue reduces to whether we can algorithmically predict he word,if any, in a noun phrase that denotes the NP's semantic lass.
For each clause, onewould like to identify the corresponding event class or state of affairs denoted.Our pilot experiment focused on the reliability of identifying the minimal classfor each noun phrase.Assigning a semantic lass to a core noun phrase can be handled via some struc-tural rules.
Usually the semantic lass of the headword is correct for the semanticclass not only of the core noun phrase but also of the complete noun phrase it is partof.
Additional rules cover exceptions, uch as "set of ... ".
These heuristics correctlypredicted the semantic lass of the whole noun phrase 99% of the time in the sampleof over 1000 noun phrases from the WSJ that were correctly predicted by Church'sPARTS program.Furthermore, ven some of the NPs whose left boundary was not predicted cor-rectly by PARTS nevertheless were assigned the correct semantic lass.
One conse-quence of this is that the correct semantic lass of a complex noun phrase can bepredicted even if some of the words in the noun phrase are unknown and even ifits full structure is unknown.
Thus, fully correct identification of core noun phraseboundaries and of noun phrase boundaries may not be necessary to accurately pro-duce database updates.This result is crucial to our method of inferring case frames of verbs from examples.Simple rules can predict which word designates the semantic lause of a noun phrasevery reliably.
We can use these simple rules plus lexical lookup to identify the basicsemantic lass of a noun phrase.4.3 Learning Semantic InformationSemantic knowledge called selection restrictions or case frames governs what phrasesmake sense with a particular verb or noun (what arguments go with a particular verbor noun).
Traditionally such semantic knowledge ishandcrafted, though some softwareaids exist to enable greater productivity (Ayuso, Shaked, and Weischedel 1987; Bates1989; Grishman, Hirschman, and Nhan 1986; Weischedel t al.
1989).Instead of handcrafting this semantic knowledge, our goal is to learn that knowl-edge from examples, using a three-step rocess:1.
Simple manual semantic annotation2.
Supervised training based on parsed sentences3.
Estimation of probabilities.4.3.1 Simple Manual Semantic Annotation.
Given a sample of text, we annotate achnoun, verb, and proper noun in the sample with the semantic lass corresponding to itin the domain model.
For instance, dawn would be annotated <time>, explode wouldbe <explosion event>, and Yunguyo would be <city>.
For our experiment, 560 nounsand 170 verbs were defined in this way.
We estimate that this semantic annotationproceeded at about 90 words per hour.4.3.2 Supervised Training.
From the TREEBANK project at the University of Penn-sylvania, we used 20,000 words of MUC-3 texts that had been bracketed according tomajor syntactic ategory.
The bracketed constituents for the sentence below appearsin Figure 8.376Ralph Weischedel t al.
Coping with Ambiguity and Unknown WordsA bomb exploded today at dawn in the Peruvian town of Yunguyo, near the lake, very near wherethe Presidential summit was to take place.
((S(NP a bomb)(VP explodedtoday(PP at(NP dawn) )(PP in(NP the Peruvian town(PP of(NP yunguyo) ) ) )(PP near(NP the lake) )(SBAR ( WHPP verynear(WHADVP where) )(S (NP the presidential snmmit)(VP was(S (NP*)to(VP take(NP place) ) ) ) ) ) ) ).
)Figure 8Example of TREEBANK analysis.From the example one can clearly infer that bombs can explode, or more properly,that bomb can be the logical subject of explode, that at dawn can modify explode, etc.Naturally good generalizations based on the instances are more valuable than theinstances themselves.Since we have a hierarchical domain model, and since the manual semantic anno-tation states the relationship between lexical items and concepts in the domain model,we can use the domain model hierarchy as a given set of categories for generalization.However, the critical issue is selecting the right level of generalization given the setof examples in the supervised training set.We have chosen a known statistical procedure (Katz 1987) that selects the min-imum level of generalization such that there is sufficient data in the training set tosupport discrimination of cases of attaching phrases (arguments) to their head.
Thisleads us to the next topic, estimation of probabilities from the supervised training set.4.3.3 Estimation of Probabilities.
The case relation, or selection restriction, to belearned is of the form X P O, where X is a headword or its semantic lass; P is acase, e.g., logical subject, logical object, preposition, etc.
; and O is a head word or itssemantic lass.One factor in the probability that O attaches to X with case P is p'(X I P, O), anestimate of the likelihood of attaching PO to X given P and O.
We chose to modela second multiplicative factor p(d), the probability of an attachment where d words377Computational Linguistics Volume 19, Number 2separate the headword X from the phrase to be attached (intuitively, the notion ofattachment distance).
For instance, in the example previously discussed, in the town isattached to the verb explode, at a distance of four words; of Yunguyo is attached to thenoun town at a distance of one word back, etc.
Thus we estimate the probability ofattachment as p'(X I P, O) * p(d).Since a 20,000-word corpus does not constitute nough data to estimate the prob-ability of all triples, we used an extension and generalization of an algorithm (Katz1987) to automatically move up the hierarchical domain model from X to its parent,and from O to its parent.
The "backing-off" that was originally proposed for the es-timation of probabilities of n-gram sequences of words starts with the most detailedmodel.
In this case we start with the explicit probability of the phrase PO attaching tothe word X.
If we have no examples of X P O in the training set we consider with somepenalty a class of X or O.
Thus the event becomes less specific but more likely to havebeen observed.
We back off on the detail until we can estimate the probability fromthe training set.
The Katz algorithm gives a way to estimate the back-off penalty asthe probability that we would not have observed the more detailed triple even thoughit was possible.4.3.4 The Experiment.
By examining the table of triples X P O that were learned, itwas clear that meaningful information was induced from the examples.
For instance,(<attack> against <building>) and (<attack> against <residence>) were learned,which correspond to two cases of importance in the MUC domain.
As a consequence,useful semantic information was learned by the training algorithm.However, we ran a far more meaningful evaluation of what was learned by mea-suring how effective the learned information would be at predicting 166 prepositionalphrase attachments hat were not made by our partial parser.
For example, in thefollowing sentence, in the Peruvian town can be attached syntactically at three places:modifying dawn, modifying today, or modifying explode.A bomb exploded today at dawn in the Peruvian town of Yunguyo, near thelake, very near where the Presidential summit was to take place.Closest attachment, a purely syntactic onstraint, worked quite effectively, havinga 25% error rate.
Using the semantic probabilities alone p~(X I P, O) had poorer per-formance, a 34% error rate.
However, the richer probability model p'(X I P, O) ?
p(d)outperformed both the purely semantic model and the purely syntactic model (closestattachment), yielding an 18% error rate.However, the degree of reduction of error rate should not be taken as the finalword, for the following reasons:20,000 words of training data is much less than one would want.Since many of the headwords in the 20,000 word corpus are not ofimport in the MUC-3 domain, their semantic type is vague, i.e.,<unknown event>, <unknown entity>, etc.4.4 Related WorkIn addition to the work discussed earlier on tools to increase the portability of naturallanguage systems, another ecent paper (Hindle and Rooth 1990) is directly related toour goal of inferring case frame information from examples.378Ralph Weischedel t al.
Coping with Ambiguity and Unknown WordsHindle and Rooth focused only on prepositional phrase attachment using a prob-abilistic model, whereas our work applies to all case relations.
Their work used anunsupervised training corpus of 13 million words to judge the strength of preposi-tional affinity to verbs, e.g., how likely it is for to to attach to the word go, for fromto attach to the word leave, or for to to attach to the word flight.
This lexical affin-ity is measured independently of the object of the preposition.
By contrast, we areexploring induction of semantic relations from supervised training, where very littletraining may be available.
Furthermore, we are looking at triples of headword (orsemantic lass), syntactic ase, and headword (or semantic lass).In Hindle and Rooth's test, they evaluated their probability model in the limitedcase of verb-noun phrase-prepositional phrase.
Therefore, no model at all would be atleast 50% accurate.
In our test, many of the test cases involved three or more possibleattachment points for the prepositional phrase, which provided a more realistic test.An interesting next step would be to combine these two probabilistic models (per-haps via linear weights) in order to get the benefit of domain-specific knowledge, aswe have explored, and the benefits of domain-independent k owledge, as Hindle andRooth have explored.4.5 Future Work: Finding Relations/Combining FragmentsThe experiments on the effectiveness of finding core NPs using only local informationwere run by midsummer 1990.
In fall 1990, another alternative, the Fast Partial Parser(FPP), which is a derivative of earlier work (de Marcken 1990), became available to us.It finds fragments using a stochastic part of speech algorithm and a nearly determin-istic parser.
It produces fragments averaging three to four words in length.
Figure 9shows an example output for the sentence.A BOMB EXPLODED TODAY AT DAWN IN THE PERUVIAN TOWNOF YUNGUYO, NEAR THE LAKE, VERY NEAR WHERE THE PRESI-DENTIAL SUMMIT WAS TO TAKE PLACE.Certain sequences of fragments appear frequently, as illustrated in Tables 1 and 2.One frequently occurring pair is an S followed by a PP (prepositional phrase).
Sincethere is more than one way the parser could attach the PP, and syntactic grounds alonefor attaching the PP would yield poor performance, semantic preferences applied bya post-process that combines fragments are called for.In our approach, we propose using local syntactic and semantic information ratherthan assuming a global syntactic and semantic form will be found.
The first step isto compute a semantic interpretation for each fragment found without assuming thatthe meaning of each word is known.
For instance, as described above, the semanticclass for any noun phrase can be computed provided the head noun has semantics inthe domain.Based on the data above, a reasonable approach is an algorithm that moves left-to-right through the set of fragments produced by FPP, deciding to attach fragments(or not) based on semantic riteria.
To avoid requiring a complete, global analysis, awindow two constituents wide is used to find patterns of possible relations amongphrases.
For example, an S followed by a PP invokes an action of finding all pointsalong the "right edge" of the S tree where a PP could attach, applying the fragmentcombining patterns at each such spot, and ranking the alternatives.As is evident in Table 2, FPP frequently does not attach punctuation.
This is to beexpected, since punctuation is used in many ways, and there is no deterministic basisfor attaching the constituent following the punctuation to the constituent preceding it.379Computational Linguistics Volume 19, Number 2(S (NP (DETERMINER "A") (N "BOMB"))(VP (AUX (NP (MONTH "TODAY"))(PP (PREP "AT")(NP (N "DAWN"))))(VP (V "EXPLODED"))))(PP(PP (PREP "IN")(NP (NP (DETERMINER "THE")(N "PERUVIAN")(N "TOWN"))(PP (PREP "OF")(NP (N "YUNGUY0")))))(PUNCT ","))(PP (PP (PREP "NEAR")(NP (DETERMINER "THE")(N "LAKE")))(PUNCT ","))(ADJP (DEGREESPEC "VERY")(ADJP (ADJ "NEAR")))(ADV "WHERE")(NP (DETERMINER "THE")(ADJP (ADJ "PRESIDENTIAL"))(N "SUMMIT"))(VP (AUX) (VP (V "WAS")))(VP (AUX "T0")(VP (V "TAKE")(NP (N "PLACE"))))(PUNCT ".
")Figure 9Example output.Table 1Most frequently occurring pairs (in 2500pairs).Pair OccurrencesS PP 104NP VP 89VP VP 72S VP 65PP PP 62PP NP 58NP PP 56VP PP 54PP VP 48NP NP 34Therefore, if the pair being examined by the combining algorithms ends in punctua-tion, the algorithm looks at the constituent following it, trying to combine it with theconstituent to the left of the punctuation.A similar case is when the pair ends in a conjunction.
Here the algorithm triesto combine the constituent to the right of the conjunction with that on the left of theconjunction.380Ralph Weischedel t al.
Coping with Ambiguity and Unknown WordsTable 2Frequently occurring fragment pairs surroundingpunctuation.Triple OccurrencesNP PUNCT NP 53VP PUNCT S 20S PUNCT S 19NP PUNCT S 19S PUNCT NP 17VP PUNCT N 12NP PUNCT PP 10NP PUNCT VP 95.
Conc lus ionsOur pilot experiments indicate that a hybrid approach to text processing includingcorpus-based probabilistic models to supplement knowledge-based techniques i  bothfeasible and promising.In part-of-speech labeling, we have evaluated POST in the laboratory, evaluating itsresults against the work of people doing the same task.
However, the real test of such asystem is how well it functions as a component in a larger system.
Can it make a parserwork faster and more accurately?
Can it help to extract certain kinds of phrases fromunrestricted text?
We are currently running these experiments by making POST a partof existing systems.
It was run as a preprocessor toGrishman's Proteus ystem for theMUC-3 competition (Grishman and Sterling 1989).
Preliminary results howed it spedup Proteus by a factor of two in one-best mode and by a factor of 33% with a thresholdof T=2.
It is integrated into a new message processing system (PLUM) at BBN.For reducing interpretation ambiguity, our context-free probability model, trainedin supervised mode on only 81 sentences, was able to reduce the error rate for selectingthe correct parse on independent test sets by a factor of 2-4.
For the problem of process-ing new words in text, the probabilistic model reduced the error rate for picking thecorrect part of speech for such words from 91.5% to 15%.
And once the possible partsof speech for a word are known (or hypothesized using the tri-tag model), the proba-bilistic language model proved useful in indicating which parses hould be looked atfor learning more complex lexical information about an unknown word.
However, themost innovative aspect of our approach is the automatic nduction of semantic knowl-edge from annotated examples.
The use of probabilistic models offers the inductionprocedure a decision criterion for making generalizations from the corpus of examples.AcknowledgmentsThe work reported here was supported bythe Defense Advanced Research ProjectsAgency and was monitored by RomeLaboratory under ContractNo.
F30602-87-D-0093 and ContractNo.
F30602-91-C-0051.
The viewsand conclusions contained in thisdocument are those of the authors andshould not be interpreted as necessarilyrepresenting the official policies, whetherexpressed or implied, of the DefenseAdvanced Research Projects Agency orthe United States Government.381Computational Linguistics Volume 19, Number 2ReferencesAbney, S. (1990).
"Rapid incrementalparsing with repair."
In Proceedings, SixthAnnual Conference ofthe UW Centre for theNew Oxford English Dictionary and TextResearch.
1-9.Ayuso, D. M.; Shaked, V.; and Weischedel,R.
M. (1987).
"An environment foracquiring semantic information."
InProceedings, 25th Annual Meeting of theAssociation for Computational Linguistics.32--40.Bates, M. (1989).
"Rapid porting of theParlance TM natural anguage interface."
InProceedings, Speech and Natural LanguageWorkshop.
Morgan Kaufmann Publishers.83-88.Baum, L. E., and Eagon, J.
A.
(1967).
"Aninequality with applications to statisticalestimation for probabilistic functions ofMarkov processes and to a model ofecology."
Amer.
Math Soc.
Bulletin, 73,360-362.Chitrao, M. V., and Grishman, R.
(1990).
"Statistical parsing of messages."
InProceedings, Speech and Natural LanguageWorkshop.
Morgan Kaufmann Publishers.263-266.Church, K. (1988).
"A stochastic partsprogram and noun phrase parser forunrestricted text."
In Proceedings, SecondConference on Applied Natural LanguageProcessing.
136-143.de Marcken, C. G. (1990).
"Parsing the LOBcorpus."
In Proceedings, 28th AnnualMeeting of the Association for ComputationalLinguistics.
243-251.DeRose, S. J.
(1988).
"Grammatical categorydisambiguation by statisticaloptimization."
Computational Linguistics,14, 31-39.Grishman, R., and Sterling, J.
(1989).
"Preference semantics for messageunderstanding."
In Proceedings, Speech andNatural Language Workshop.
MorganKaufmann Publishers.
71-74.Grishman, R.; Hirschman, L.; and Nhan,N.
T. (1986).
"Discovery procedures forsublanguage s lectional patterns: Initialexperiments."
Computational Linguistics,12, 205-215.Hindle, D. (1983).
"User manual forFidditch."
Naval Research LaboratoryTechnical Memorandum #7590-142.Hindle, D., and Rooth, M.
(1990).
"Structural ambiguity and lexicalrelations."
In Proceedings, Speech andNatural Language Workshop.
MorganKaufman Publishers.
257-262.Jelinek, F. (1985).
"Self organizing languagemodeling for speech recognition.
"Unpublished Technical Report, IBMT.
J. Watson Research Center, YorktownHeights, NY.Katz, S. M. (1987).
"Estimation ofprobabilities from sparse data for thelanguage model component of a speechrecognizer."
In IEEE Transactions onAcoustics, Speech, and Signal Processing,Vol.
ASSP-35 No.
3.Kuhn, R., and De Mori, R. (1990).
"Acache-based natural anguage model forspeech recognition."
In IEEE Transactionson Pattern Analysis and Machine Intelligence,12, 570-583.Kupiec, J.
(1989).
"Augmenting a hiddenMarkov model for phrase-dependentword tagging."
In Proceedings, Speech andNatural Language Workshop.
MorganKaufmann Publishers.
92-98.Marcus, M.; Santorini, B.; and Magerman(1990).
"First steps towards an annotateddatabase of American English."
InReadings for Tagging Linguistic Informationin a Text Corpus, edited by Langendoenand Marcus, Tutorial for the 28th AnnualMeeting of the Association forComputational Linguistics.Santorini, B.
(1990).
"Annotation manual forthe Penn TREEBANK project."
TechnicalReport, CIS Department, University ofPennsylvania.Stallard, D. (1989).
"Unification-basedsemantic interpretation in the BBNspoken language system."
In Proceedings,Speech and Natural Language Workshop.Morgan Kaufmann Publishers.
39-46.Sundheim, B. M. (1991).
"Overview of theThird Message Understanding Evaluationand Conference."
In Proceedings, ThirdMessage Understanding Conference (MUC-3).Morgan Kaufmann Publishers, 3-16.Weischedel, R. M.; Bobrow, R.; Ayuso,D.
M.; and Ramshaw, L.
(1989).
"Portability in the Janus natural anguageinterface."
In Speech and Natural Language.Morgan Kaufmann Publishers.
112-117.382
