Mistake-Driven Learning in Text CategorizationI do  Dagan*Dept.
of Math.
& CSBar Ilan UniversityRamat Gan 52900, Israeldagan@cs .biu.
ac.
ilYae l  KarovDept.
of Appl.
Math.
& CSWeizmann Institute of ScienceRehovot 76100, Israelyaelk@wisdom, we ?zmann.
ac.
ilDan Roth  tDept.
of Appl.
Math.
& CSWeizmann Institute of ScienceRehovot 76100, IsraeldanrQwisdom, weizmalm, ac.
ilAbst ractLearning problems in the text processingdomain often map the text to a spacewhose dimensions are the measured fea-tures of the text, e.g., its words.
Threecharacteristic properties of this domain are(a) very high dimensionality, (b) both thelearned concepts and the instances residevery sparsely in the feature space, and (c)a high variation in the number of activefeatures in an instance.
In this work westudy three mistake-driven learning algo-rithms for a typical task of this nature -text categorization.We argue that these a lgor i thms-  whichcategorize documents bY learning a linearseparator in the feature space - have a fewproperties that make them ideal for this do-main.
We then show that a quantum leapin performance is achieved when we fur-ther modify the algorithms to better ad-dress some of the specific characteristics ofthe domain.
In particular, we demonstrate(1) how variation in document length canbe tolerated by either normalizing featureweights or by using negative weights, (2)the positive effect of applying a thresholdrange in training, (3) alternatives in consid-ering feature frequency, and (4) the bene-fits of discarding features while training.Overall, we present an algorithm, a vari-ation of Littlestone's Winnow, which per-forms significantly better than any otheralgorithm tested on this task using a simi-lar feature set.
*Partly supported by a grant no.
8560195 from theIsraeh Ministry of Science.tPartly supported by a grant from the Israeli Ministryof Science.
Part of this work was done while visiting atHarvard University, supported by ONR grant N00014-96-1-0550.1 In t roduct ionLearning problems in the natural anguage and textprocessing domains are often studied by mappingthe text to a space whose dimensions are the mea-sured features of the text, e.g., the words appearingin a document.
Three characteristic propertie s ofthis domain are (a) very high dimensionality, (b)both the learned concepts and the instances residevery sparsely in the feature space and, consequently,(c) there is a high variation in the number of activefeatures in an instance.Multiplicative weight-updating algorithms uch asWinnow (Littlestone, 1988) have been studied exten-sively in the theoretical learning literature.
Theoret-ical analysis has shown that they have exceptionallygood behavior in domains with these characteristics,and in particular in the presence of irrelevant at-tributes, noise, and even a target function chang-ing in time (Littlestone, 1988; Littlestone and War-muth, 1994; Herbster and Warmuth, 1995), but onlyrecently have people started to use them in applica-tions (Golding and Roth, 1996; Lewis et al, 1996;Cohen and Singer, 1996).
We address these claimsempirically in an important application domain formachine learning - text categorization.
In partic-ular, we study mistake-driven learning algorithmsthat are based on the Winnow family/, and investi-gate ways to apply them in domains with the abovecharacteristics.The learning algorithms tudied here offer a largespace of choices to be made and, correspondingly,may vary widely in performance when applied in spe-cific domains.
We concentrate here on the text pro-cessing domain, with the characteristics mentionedabove, and explore this space of choices in it.In particular, we investigate three variations ofon-line prediction algorithms and evaluate them ex-perimentally on large text categorization problems.The algorithms we study are all learning algorithmsfor linear functions.
They are used to categorizedocuments by learning, for each category, a linearseparator in the feature space.
The algorithms dif-fer by whether they allow the use of negative or only55positive weights and by the way they update theirweights during the training phase.We find that while a vanilla version of these algo-rithms performs rather well, a quantum leap in per-formance is achieved when we modify the algorithmsto better address ome of the specific characteristicswe identify in textual domains.
In particular, we ad-dress problems uch as wide variations in documentsizes, word repetitions and the need to rank docu-ments rather than just decide whether they belongto a category or not.
In some cases we adopt so-lutions that are well known in the IR literature tothe class of algorithms we use; in others we modifyknown algorithms to better suit the characteristicsof the domain.
We motivate the modifications tothe basic algorithms and justify them experimentallyby exhibiting their contribution to improvement inperformance.
Overall, the best variation we investi-gate, performs ignificantly better than any knownalgorithm tested on this task, using a similar set offeatures.The rest of the paper is organized as follows: Thenext section describes the task of text categoriza-tion, how we model it as a classification task, andsome related work.
The family of algorithms we useis introduced in Section 3 and the extensions to thebasic algorithms, along with their experimental eval-uations, is presented in Section 4.
In Section 5 wepresent our final experimental results and comparethem to previous works in the literature.2 Text  Categor i za t ionIn text categorization, given a text documentand a collection of potential classes, the algo-rithm decides which classes it belongs to, orhow strongly it belongs to each class.
Forexample, possible classes (categories) may be{bond}, {loan}, {interest}, {acquisition}.
Docu-ments that have been categorized by humans areusually used as training data for a text categoriza-tion system; later on, the trained system is usedto categorize new documents.
Algorithms used totrain text categorization systems in information re-trieval (IR) are often ad-hoc and poorly understood.In particular, very little is known about their gen-eralization performance, that is, their behavior ondocuments outside the training data.
Only recently,some machine learning techniques for training lin-ear classifiers have been used and shown to be effec-tive in this domain (Lewis et al, 1996; Cohen andSinger, 1996).
These techniques have the advantagethat they are better understood from a theoreticalstandpoint, leading to performance guarantees andguidance in parameter settings.
Continuing this lineof research we present different algorithms and fo-cus on adjusting them to the unique characteristicsof the domain, yielding good performance on thecategorization task.2.1 T ra in ing  Text  Classif iersText classifiers represent a document as a set of fea-tures d = {fl , f2,.
.
.
fm}, where m is the numberof active features in the document, that is, featuresthat occur in the document.
A feature fi may typ-ically represent a word w, a set wl,.
.
.
Wk of words(Cohen and Singer, 1996) or a phrasal structure(Lewis, 1992; Tzeras and Hartmann, 1993).
Thestrength of the feature f in the document d is de-noted by s(f, d).
The strength is usually a functionof the number of times f appears in d (denoted byn(f, d)).
The strength may be used only to indicatethe presence or absence of f in the document, inwhich case it takes on only the values 0 or 1, it maybe equal to n(f, d), or it can take other values toreflect also the size of the document.In order to rank documents, for each category,a text categorization system keeps a function Fcwhich, when evaluated on d, produces a score Fc(d).A decision is then made by assigning to the categoryc only those documents that exceed some threshold,or just by placing at the top of the ranking docu-ments with the highest such score.A linear text classifier epresents a category as aweight vector wc = (w(fl ,  c), w(f2, c),.., w(fn, c))(wl, w2, .
.
.
Wn), where n is the total number of fea-tures in the domain and w(f, c) is the weight of thefeature f for this category.
It evaluates the score ofthe document by computing the dot product:F (a) = siS, w(S, e).$edThe problem is modeled as a supervised learn-ing problem.
The algorithms use the training data,where each document is labeled by zero or more cate-gories, to learn a classifier which classifies new texts.A document is considered as a positive example forall categories with which it is labeled, and as a neg-ative example to all others.The task of a training algorithm for a linear textclassifier is to find a weight vector which best classi-fies new text documents.
While a linear text classi-fier is a linear separator in the space defined by thefeatures, it may not be linear with respect to thedocument, if one chooses to use complex featuressuch as conjunctions of simple features.
In addition,a training algorithm may give also advice on the is-sue of feature selection, by reducing the weight ofnon-important features and thus effectively discard-ing them.2.2 Re la ted  WorkMany of the techniques previously used in text cat-egorization make use of linear classifiers, mainlyfor reasons of efficiency.
The classical vector spacemodel, which ranks documents using a nonlinearsimilarity measure (the "cosine correlation") (Saltonand Buckley, 1983) can also be recast as a linear clas-sification by incorporating length normalization i to56the weight vector and the document vector featuresvalues.
State of the art IR systems determine thestrength of a term based on three values: (1) thefrequency of the feature in the document (t\]), (2)an inverse measure of the frequency of the featurethroughout the data set (id\]), and (3) a normaliza-tion factor that takes into account he length of thedocument.
In Sections 4.1 and 4.3 we discuss howwe incorporate those ideas in our setting.Most relevant to our work are non-parametricmethods, which seem to yield better results thanparametric techniques.
Rocchio's algorithm (Roc-chio, 1971), one of the most commonly used tech-niques, is a batch method that works in a relevancefeedback context.
Typically, classifiers produced bythe Rocchio algorithm are restricted to having non-negative weights.
An important distinction betweenmost of the classical non-parametric methods andthe learning techniques we study here is that in theformer case, there was no theoretical work that ad-dressed the generalization ability of the learned clas-sifter, that is, how it behaves on new data.The methods that are most similar to our tech-niques are the on-line algorithms used in (Lewis etal., 1996) and (Cohen and Singer, 1996).
In the first,two algorithms, a multiplicative update and additiveupdate algorithms uggested in (Kivinen and War-muth, 1995a) are evaluated in the text categoriza-tion domain, and are shown to perform somewhatbetter than Rocchio's algorithm.
While both theseworks make use of multiplicative update algorithms,as we do, there are two major differences betweenthose studies and the current one.
First, there aresome important technical differences between the al-gorithms used.
Second, the algorithms we study hereare mistake-driven; they update the weight vectoronly when a mistake is made, and not after everyexample seen.
The Experts algorithm studied in(Cohen and Singer, 1996) is very similar to a basicversion of the BalancedWinnow algorithm which westudy here.
The way we treat the negative weights isdifferent, though, and significantly more efficient, es-pecially in sparse domains (see Section 3.1).
Cohenand Singer experiment also, using the same algo-rithm, with more complex features (sparse n-grams)and show that, as expected, it yields better results.Our additive update algorithm, Perceptron, issomewhat similar to what is used in (Wiener, Peder-sen, and Weigend, 1995).
They use a more complexrepresentation, a multi-layer network, but this ad-ditional expressiveness seems to make training morecomplicated, without contributing to better results.2.3 Methodo logyWe evaluate our algorithms on the the Reuters-22173 text collection (Lewis, 1992), one of the mostcommonly used benchmarks in the literature.For the experiments reported In Sections 3.2 weexplore and compare different variations of the al-gorithms; we evaluate those on two disjoint pairs ofa training set and a test set, both subsets of theReuters collection.
Each pair consists of 2000 train-ing documents and 1000 test documents, and wasused to train and test the classifier on a sample of10 topical categories.
The figures reported are theaverage results on the two test sets.In addition, we have tested our final version ofthe classifier on two common partitions of the com-plete Reuters collection, and compare the resultswith those of other works.
The two partitions usedare those of Lewis (Lewis, 1992) (14704 documentsfor training, 6746 for testing) and Apte (Apte, Dam-erau, and Weiss, 1994) (10645 training, 3672 testing,omitting documents with no topical category).To evaluate performance, the usual measures ofrecall and precision were used.
Specifically, we mea-sured the effectiveness of the classification by keep-ing track of the following four numbers:?
Pl = number of correctly classified class mem-bers?
P2 = number of mis-classified class members?
nl = number of correctly classified non-classmembers?
n2 = number of mis-classified ion-class mem-bersIn those terms, the recall measure is defines asPl/Pl+P2, and the precision is defined as pl/pl?n2.Performance was further summarized by a break-even point - a hypothetical point, obtained by in-terpolation, in which precision equals recall.3 On-L ine  learn ing  a lgor i thmsIn this section we present he basic versions of thelearning algorithms we use.
The algorithms are usedto learn a classifier Fc for each category c. Thesealgorithms use the training data, where each docu-ment is labeled by zero or more categories, to learna weight vector which is used later on, in the testphase, to classify new text documents.
A documentis considered as a positive example for all categorieswith which it is labeled, and as a negative xam-ple to all others?
The algorithms are on-line andmistake-driven.
In the on-line learning model, learn-ing takes place in a sequence of trials.
On each trial,the learner first makes a prediction and then receivesfeedback which may be used to update the currenthypothesis (the vector of weights).
A mistake-drivenalgorithm updates its hypothesis only when a mis-take is made.
In the training phase, given a collec-tion of examples, we may repeat this process a fewtimes, by iterating on the data.
In the testing phase,the same process is repeated on the test collection,only that the hypothesis i  not updated.Let n be the number of features of the currentcategory.
For the remainder of this section we de-note a training document with rn active features57by d = (s i l ,s i~,.
.
.s i , , ) ,  where sij stands for thestrength of the ij feature.
The label of the documentis denoted by y; y takes the value 1 if the documentis relevant o the category and 0 otherwise.
Notice,that we care only about the active features in the do-main, following (Blum, 1992).
The algorithms havethree parameters: a threshold/9, and two update pa-rameters, a promotion parameter o~ and a demotionparameter ft.Pos i t ive  Winnow (L i t t les tone ,  1988):The algorithm keeps an n-dimensional weight vec-tor w = (wl ,w2, .
.
.Wn),  wi being the weight of theith feature, which it updates whenever a mistake ismade.
Initially, the weight vector is typically set toassign equal positive weight to all features.
(We usethe value/9/d, where d is the average number of ac-tive features in a document; in this way initial scoresare close to/9.)
The promotion parameter is a > 1and the demotion is 0 < ~ < 1.For a given instance (Sil,Sia... ,8ira) the algo-rithm predicts 1 iffm~ Wijaij ~ O,j= lwhere wit is the weight corresponding to the activefeature indexed by ij.
The algorithm updates itshypothesis only when a mistake is made, as follows:(1) If the algorithm predicts 0 and the label is 1(positive xample) then the weights of all the activefeatures are promoted - -  the weight wit is multipliedby o~.
(2) If the algorithm predicts 1 and the receivedlabel is 0 (negative xample) then the weights of allthe active features are demoted - -  the weight wit ismultiplied by ft.
In both cases, weights of inactivefeatures maintain the same value.Percept ron  (Rosenb la t t ,  1958)As in PositiveWinnow, in Perceptron we also keepan n-dimensional weight vector w = (wl, w2,.
.
,  wn)whose entries correspond to the set of potential fea-tures, which is updated whenever a mistake is made.As above, the initial weight vector is typically set toassign equal weight to all features.
The only dif-ference between the algorithms is that in this casethe weights are updated in an additive fashion.
Asingle update parameter c~ > 0 is used, and a weightis promoted by adding c~ to its previous value, andis demoted by subtracting o~ from it.
In both cases,all other weights maintain the same value.Ba lanced  Winnow (L i t t les tone ,  1988):In this case, the algorithm keeps two weights,w +, w- ,  for each feature.
The overall weight of afeature is the difference between these two weights,thus allowing for negative weights.
For a given in-stance (si~, sis .
.
.
,  si~) the algorithm predicts 1 iffm- > /9 ,  (1)j= lwhere w~, wi- ~ correspond to the active feature in-dexed by ij.
In our implementation, the weights w +are initialized to 20/d and the weights w- are set to0/d, where d is the average number of active featuresin a document in the collection.The algorithm updates the weights of active fea-tures only when a mistake is made, as follows: (1) Inthe promotion step, following a mistake on a positiveexample, the positive part of the weight is promoted,w~ ~ a ?
w~ while the negative part of the weightis demoted, wi~ ~-- ft. wij.
Overall, the coefficient ofsij in Eq.
1 increases after a promotion.
(2) In thedemotion step, following a mistake on a negative x-ample, the coefficient ofsij in Eq.
1 is decreased: thepositive part of the weight is demoted, w~ ~ j3.
w~while the negative part of the weight is promoted,m wij *- a .
w~.
In both cases, all other weights main-tain the same value.In this algorithm (see in Eq.
1) the coefficient ofthe ith feature can take negative values, unlike therepresentation used in PositiveWinnow.
There areother versions of the Winnow algorithm that allowthe use of negative features: (1) Littlestone, whenintroducing the Balanced version, introduced also asimpler version - a version of PositiveWinnow witha duplication of the number of features.
(2) A ver-sion of the Winnow algorithm with negative featuresis used in (Cohen and Singer, 1996).
In both cases,however, whenever there is a need to update theweights, all the weights are being updated (actually,n out of the 2n).
In the version we use, only weightsof active features are being updated; this gives a sig-nificant computational dvantage when working ina sparse high dimensional space.3.1 P roper t ies  of  the  A lgor i thmsWinnow and its variations were introduced in Little-stone's seminal paper (Littlestone, 1988); the the-oretical behavior of multiplicative weight-updatingalgorithms for learning linear functions has beenstudied since then extensively.
In particular, Win-now has been shown to learn efficiently any linearthreshold function (Littlestone, 1988).
These arefunctions F : {0, 1} n ---~ {0, 1} for which there ex-ist real weights w l , .
.
.
,wn  and a real threshold /9such that F(s l , .
.
.
, sn )  = 1 iff ~i"=1 wisi > /9.
Inparticular, these functions include Boolean disjunc-tions and conjunctions on k _< n variables and r-of-kthreshold functions (1 < r < k _< n).
While Win-now is guaranteed to find a perfect separator if oneexists, it also appears to be fairly successful whenthere is no perfect separator.
The algorithm makesno independence or ~tny other assumptions on thefeatures, in contrast o other parametric estimationtechniques (typically, Bayesian predictors) which arecommonly used in statistical NLP.Theoretical analysis has shown that the algorithmhas exceptionally good behavior in the presence of58irrelevant features, noise, and even a target func-tion changing in time (Littlestone, 1988; Littlestone,1991; Littlestone and Warmuth, 1994; Herbster andWarmuth, 1995), and there is already some empiri-cal support for these claims (Littlestone, 1995; Gold-ing and Roth, 1996; Blum, 1995).
The key featureof Winnow is that its mistake bound grows linearlywith the number of relevant features and only log-arithmically with the total number of features.
Asecond important property is being mistake driven.Intuitively, this makes the algorithm more sensitiveto the relationships among the features - -  relation-ships that may go unnoticed by an algorithm thatis based on counts accumulated separately for eachattribute.
This is crucial in the analysis of the algo-rithm as well as empirically (Littlestone, 1995; Gold-ing and Roth, 1996).The discussion above holds for both versions ofWinnow studied here, PositiveWinnow and Bal-ancedWinnow.
The theoretical results differ onlyslightly in the mistake bounds, but have the sameflavor.
However, the major difference between thetwo algorithms, one using only positive weights andthe other allowing also negative weights, plays a sig-nificant role when applied in the current domain, asdiscussed in Section 4.Winnow is closely related, and has servedas the motivation for a collection of recentworks on combining the "advice" of different"experts"(Littlestone and Warmuth, 1994; Cesa-Bianchi et al, 1995; Cesa-Bianchi et al, 1994).
Thefeatures used are the "experts" and the learning al-gorithm can be viewed as an algorithm that learnshow to combine the classifications of the differentexperts in an optimal way.The additive-update algorithm that we evaluatehere, the Perceptron, goes back to (Rosenblatt,1958).
While this algorithm is also known to learnthe target linear function when it exists, the boundsgiven by the Perceptron convergence theorem (Dudaand Hart, 1973) may be exponential in the opti-mal mistake bound, even for fairly simple functions(Kivinen and Warmuth, 1995b).
We refer to (Kivi-nen and Warmuth, 1995a) for a thorough analysisof multiplicative update algorithms versus additiveupdate algorithms.
In particular, it is shown thatthe number of mistakes the additive and multiplica-tive update algorithms make, depend differently onthe domain characteristics.
Informally speaking, itis shown that the multiplicative update algorithmshave advantages in high dimensional problems (i.e.,when the number of features is large) and when thetarget weight vector is sparse (i.e., contain manyweights that are close to 0).
This explains the re-cent success in using these methods on high dimen-sional problems (Golding and Roth, 1996) and sug-gests that multiplicative-update algorithms mightdo well on IR applications, provided that a goodset of features is selected.
On the other hand, it isshown that additive-update algorithms have advan-tages when the examples are sparse in the featurespace, another typical characteristics of the IR do-main, which motivates us to study experimentallyan additive-update algorithm as well.3.2 Evaluating the Basic VersionsWe started by evaluating the basic versions of thethree algorithms.
The features we use throughoutthe experiments are single words, at the lemma level,for nouns and verbs only, with minimal frequency of3 occurrences in the corpus.
In the basic versionsthe strength of the feature is taken to indicate onlythe presence or absence of f in the document, thatis, it is either 1 or 0.
The training algorithm was runiteratively on the training set, until no mistakes weremade on the training collection or until some upperbound (50) on the number of iterations was reached.The results for the basic versions are shown in thefirst column of Table 1.4 Extens ions  to  the  Bas ic  a lgor i thms4.1 Length Variation and Negat ive featuresText documents vary widely in their length and atext classifier needs to tolerate this variation.
Thisissue is a potential problem for a linear classifierwhich scores a document by summing the weightsof all its active features: a long document may havea better chance of exceeding the threshold merely byits length.This problem has been identified earlier on andattracted a lot of work in the classical work on IR(Salton and Buckley, 1983), as we have indicatedin Section 2.2.
The treatment described there ad-dresses at the same time at least two different con-cerns: length variation of documents and featurerepetition.
In this section we consider the first ofthose, and discuss how it applies to the algorithmswe investigate.
The second concern is discussed inSection 4.3.Algorithms that allow the use of negative features,such as BalancedWinnow and Perceptron, toleratevariation in the documents length naturally, andthus have a significant advantage in this respect.In these cases, it can be expected that the cumu-lative contribution of the weights and, in particular,those that are not indicative to the current cate-gory, does not count towards exceeding the thresh-old, but rather averages out to 0.
Indeed, as wefound out, no special normalization is required whenusing these algorithms.
Their significant advantageover the unnormalized version of PositiveWinnow isreadily seen in Table 1.In addition, using negative weights gives the textclassifier more flexibility in capturing "truly nega-tive" features, where the presence of a feature is in-dicative for the irrelevance of the document o thecategory.
However, we found that this phenomenon59Algorithm VersionBasic Norm 0-range Linear Freq.BalancedWinnow 64.87 NA 69.66 72.11PositiveWinnow 55.56 63.56 65.80 67.20Perceptron 65.91 NA 63.05 66.72Sqrt.
Freq Discard71.56 73.269.67 70.068.29 70.8Table 1: Recall/precision break-even point (in percentages) for different versions of the algorithm.
Eachfigure is an average result for two pairs of training and testing sets, each containing 2000 training documentsand 1000 test documents.only rarely occurs in text categorization and thusthe main use of the negative features is to toleratethe length variation of the documents.When using PositiveWinnow, which uses only pos-itive weights, we no longer have this advantage andwe seek a modification that tolerates the variation inlength.
As in the standard IR solution, we suggestto modify s(f, d), the strength of the feature f in d,by using a quantity that is normalized with respectto the document size.Formally, we replace the strength s(f,d) (whichmay be determined in several ways according to fea-ture frequency, as explained below) by a normalizedstrenglh,s(f, d)sn(f, d) = E fEd s(f, d)"In this case (which applies, as discussed above,only for PositiveWinnow), we also change the initialweight vector and initialize all the weights to 0.Using normalization gives an effect that is similarto the use of negative weights, but to a lesser degree.The reason is that it is used uniformly; in long doc-uments, the number of indicative features does notincrease significantly, but their strength, neverthe-less, is reduced proportionally to the total numberof features in the document.
In the long version ofthe paper we present a more thorough analysis ofthis issue.The results presented in Table 1 (second column)show the significant improvements achieved in Pos-itiveWinnow performance, when normalization isused.
In all the results presented from this point on,positive winnow is normalized.4.2 Us ing Thresho ld  rangeTraining a linear text classifier is a search for aweight vector in the feature space.
The search is for alinear separator that best separates documents thatare relevant o the category from those that are not.In general, there is no guarantee that a weight vec-tor of this sort exists, even in the training data, buta good selection of features make this more likely.While the basic versions of our algorithms earchfor linear separators, we have modified those so thatour search for a linear classifier is biased to look for"thick" classifiers.
To understand this, consider, forthe moment, the case in which all the data is per-fectly linearly separable.
Then there will generallybe many linear classifiers that separate the trainingdata we actually see.
Among these, it seems plau-sible that we have a better chance of doing well onthe unseen test data if we choose a linear separatorthat separates the positive and negative training ex-amples as "widely" as possible.
The idea of havinga wide separation is less clear when there is no per-fect separator, but we can still appeal to the basicintuition.Using a "thick" separator is even more impor-tant when documents are ranked rather than sim-ply classified; that is, when the actual score pro-duced by the classifier is used in the decision process.The reason is that if Fc(d) is the score produced bythe classifier Fc when evaluated on the document dthen, under some assumptions on the dependenciesamong the features, the probability that the doc-ument d is relevant to the category c is given byProb(d E c) _ l+e=~;r~7 This function, known asthe sigmoid function, "flattens" the decision regionin a way that only scores that are far apart from thethreshold value indicate that the decision is madewith significant probability.Formally, among those weight vectors we wouldlike to choose the hyper-plane with the largest "sep-arating parameter", where the separating parameterr is defined as the largest value for which there existsa classifier F?
(defined by a weight vector w) suchthat for all positive examples d, F?
(d) > 0 + r/2 andfor all negative d, Fc(d) < 0 - r/2.In this implementation we do not try to find theoptimal r (as is done in (Cortes and Vapnik, 1995),but rather determine it heuristically.
In order tofind a "thick" separator, we modify, in all three al-gorithms, the update rule used during the trainingphase as follows: Rather than using a single thresh-old we use two separate thresholds, 0+ and 0-, suchthat 0 + - 0- = 7-.
During training, we say that thealgorithm predicts 0 (and makes a mistake, if the ex-ample is labeled positive) when the score it assignsan example is below 0- .
Similarly, we say that thealgorithm predicts 1 when the score exceeds 0+.
Allexamples with scores in the range \[0-, 0 +\] are con-sidered mistakes.
'Parameters used: 0-=0.9, 0 + =1.1, 0 = 1).60The results presented in the third column of Ta-ble 1 show the improvements obtained when thethreshold range is used.
In all the results presentedfrom this point on, all the algorithms use the thresh-old range modification.4.3 Feature  Repet i t ionDue to the bursty nature of term occurrence in doc-uments, as well as the variation in document length,a feature may occur in a document more than once.It is therefore important o consider the frequencyof a feature when determining its strength.
On onehand, there are cases where a feature is more indica-tive to the relevance of the document to a categorywhen it appears several times in a document.
Onthe other hand, in any long document, there maybe some random feature that is not significantly in-dicative to the current category although it repeatsmany times.
While the weight of f in the weightvector of the category, w(f, c), may be fairly small,its cumulative contribution might be too large if weincrease its strength, s(f, d), in proportion to its fre-quency in the document.As mentioned in Section 2.2, the classical IR liter-ature has addressed this problem using the if and idffactors.
We note that the standard treatment in IRsuggests a solution to this problem that suits batchalgorithms - algorithms that determine the weightof a feature after seeing all the examples.
We, onthe other hand, seek a solution that can be used inan on-line algorithm.
Thus, the frequency of a fea-ture throughout the data set, for example, cannot betaken into account and we take into account only theif term.
We have experimented with three alterna-tive ways of adjusting the value of s(f, d) accordingto the frequency of the feature in the document: (1)Our default is to let the strength indicate only theactivity of the feature.
That is, s(f, d) = 1, if the fea-ture is present in the document (active feature) ands(f, d) = 0 otherwise.
(2) s(f,d) = n(f,d), wheren(f, d) is the number of occurrences of f in d; and(3) s(f, d) = ~ d) (as in (Wiener, Pedersen, andWeigend, 1995)).
These three alternatives examinethe tradeoff between the positive and negative im-pacts of assigning a strength in proportion to featurefrequency.
In most of our experiments, on differentdata sets, the choice of using ~/n(f, d) performedbest.
The results of the comparative evaluation ap-pear in columns 3, 4, and 5 of Table 1, correspondingto the three alternatives above.4.4 D iscard ing  featuresMultiplicative update algorithm are known to tol-erate a very large number of features.
However, itseems plausible that most categories depend only onfairly small subsets of indicative features and not onall the features that occur in documents that belongto this class.
Efficiency reasons, as well as the occa-sional need to generate comprehensible explanationsto the classifications, uggest hat discarding irrele-vant features is a desirable goal in IR applications.If done correctly, discarding irrelevant features mayalso improve the accuracy of the classifier, since irrel-evant features contribute noise to the classificationscore.An important property of the algorithms investi-gated here is that they do not require a feature se-lection pre-processing stage.
Instead, they can runin the presence of a large number of features, andallow for discarding features "on the fly", based ontheir contribution to an accurate classification.
Thisproperty is especially important if one is consideringenriching the set of features, as is done in (Goldingand Roth, 1996; Cohen and Singer, 1996); in thesecases it is important o allow the algorithm to de-cide for itself which of the features contribute to theaccuracy of the classification.We filter features that are irrelevant for the cate-gory based on the weights they were assigned in thefirst few training rounds.The algorithm is given as input a range of weightvalue which we call the filtering range.
First, thetraining algorithm is run for several iterations, untilthe number of mistakes on the training data dropsbelow a certain threshold.
After this initial training,we filter out all the features whose weight lie in thisfiltering range.
Training then continues as usual.There are various ways to determine the filteringrange.
The obvious one may be to filter out all fea-tures whose weight is very close to 0, but there area few subtle issues involved due to the normaliza-tion done in the PositiveWinnow algorithm.
In theresults presented here we have used, instead, a dif-ferent filtering range: Our filtering range is centeredaround the initial value assigned to the weights (asspecified earlier for each algorithm), and is boundedabove and below by the values obtained after onepromotion or demotion step relative to the initialvalue.
Thus, with high likelihood, we discard fea-tures which have not contributed to many mistakes- those that were promoted or demoted at most once(possibly, with additional promotions and demotionswhich canceled each other, though).The results of classification with feature filteringappear in the last column of Table 1.
We hypothe-size that the improved results are due to reductionin the noise introduced by irrelevant features.
Fur-ther investigation of this issue will be presented inthe long version of this paper.
Typically, about twothirds of the features were filtered for each category,significantly reducing the output representation size.5 Summary  o f  Exper imenta l  Resu l tsThe study described in Section 3.2 was used todetermined the version that performs best, outof those we have experimented with.
Eventually,we have selected the version of the BalancedWin-61AlgorithmBa lancedWinnow +Experts unigram (Cohen and Singer, 1996)Neural Network (Wiener, Pedersen, and Weigend, 1995)Rocchio (Rocchio, 1971)Ripper (Cohen and Singer, 1996)Decision trees (Lewis and Ringuette, 1994)Bayes (Lewis and Ringuette, 1994)SWAP (Apte, Damerau, and Weiss, 1994)Apte's split83.364.7Lewis's split74.765.677.5 NA74.5 66.079.6 71.9NA 67.0NA78.965.0NATable 2: Break-even points comparison.
The data is split into training set and test set based on Lewis'ssplit - (Lewis, 1992), 14704 documents for training, 6746 for testing, and Apte's split - (Apte, Damerau,and Weiss, 1994), 10645 training, 3672 testing, omitting documents with no topical category.now algorithm, which incorporates the 0-range mod-ification, a square-root of occurrences as the fea-ture strength and the discard features modification(BalancedWinnow + in Table 2).We have compared this version with a few otheralgorithms which have appeared in the literatureon the complete Reuters corpus.
Table 2 presentsbreak-even points for BalancedWinnow + and theother algorithms, as defined in Section 2.3.The results are reported for two splits of the com-plete Reuters corpus as explained in Section 2.3.
Thealgorithm was run with iterations, threshold range,feature filtering, and frequency-square-root featurestrength.The first two rows in Table 2 compare the per-formance of BalancedWinnow + with the two algo-rithms that most resemble our approach, the Ex-perts algorithm from (Cohen and Singer, 1996) and aneural network approach presented in (Wiener, Ped-ersen, and Weigend, 1995).
(see Section 2.2).Rocchio's algorithm is one of the classical algo-rithms for this tasks, and it still performs verygood compared to newly developed techniques (e.g,(Lewis et al, 1996)).
We also compared with theRipper algorithm presented in(Cohen and Singer,1996) (we present he best results for this task, withnegative tests), a simple decision tree learning sys-tem and a Bayesian classifier.
The last two figure aretaken from (Lewis and Ringuette, 1994) where theywere evaluated only on Lewis's split.
The last com-parison is with the learning system used by (Apte,Damerau, and Weiss, 1994), SWAP, which was eval-uated only on Apte's split.Our results ignificantly outperform (by at least 2-4%) all results which appear in that table and use thesame set of features (based on single words).
Of theresults we know of in the literature, only a version ofthe Experts algorithm of (Cohen and Singer, 1996)which uses a richer feature set - sparse word trigrams- outperforms our result on the Lewis split, witha break-even point of 75.3%, compared with 74.6%for the unigram-based BalancedWinnow + .
However,this version achieves only 75.9% on the Apte split(compared with 83.3% of BalancedWinnow+).
Inthe long version of this paper we plan to present heresults of our algorithm on a richer feature set aswell.6 Conc lus ionsTheoretical analyses of the Winnow family of algo-rithms have predicted an exceptional ability to dealwith large numbers of features and to adapt to newtrends not seen during training.
Until recently, theseproperties have remained largely undemonstrated.We have shown that while these algorithms havemany advantages there is still a lot of room to ex-plore when applying them to a real-world problem.In particular, we have demonstrated (1) how vari-ation in document length can be tolerated througheither normalization ornegative weights, (2) the pos-itive effect of applying a threshold range in training,(3) alternatives in considering feature frequency, and(4) the benefits of discarding irrelevant features aspart of the training algorithm.
The main contri-bution of this work, however, is that we have pre-sented an algorithm, BalancedWinnow +, which per-forms significantly better than any other algorithmtested on these tasks using unigram features.We have exhibited that, as expected,multiplicative-update lgorithms have exceptionallygood behavior in high dimensional feature spaces,even in the presence of irrelevant features.
One ad-vantage this important property has is that is allowsone to decompose the learning problem from the fea-ture selection problem.
Using this family of algo-rithms frees the designer from the need to choose theappropriate set of features ahead of time: A large setof features can be used and the algorithm will even-tually discard those that do not contribute to theaccuracy of the classifier.
While we have chosen inthis study to use a fairly simple set of features, it isstraight forward to plug in instead a richer set of fea-tures.
We expect that this will further improve theresults of the algorithm, although further research is62needed on policies of d iscarding features and avoid-ance of over-fitting.
In conclusion, we suggest hatthe demonstrated advantages of the Winnow-fami lyof a lgor i thms make it an appeal ing candidate for fur-ther use in this domain.AcknowledgmentsThank to Michal Landau for her help in running theexperiments.ReferencesApte, C., F. Damerau, and S. Weiss.
1994.
Towards lan-guage independent automated learning of text catego-rization models.
In Proceedings of ACM-SIGIR Con-ference on Information Retrieval.Blum, A.
1992.
Learning boolean functions in an infi-nite attribute space.
Machine Learning, 9(4):373-386,October.Blum, A.
1995.
Empirical support for Winnow andweighted-majority based algorithms: results on a cal-endar scheduling domain.
In Proc.
12th InternationalConference on Machine Learning, pages 64-72.
Mor-gan Kaufmann.Cesa-Bianchi, N., Y. Freund, D. P. Helmbold, D. Haus-sler, and R. E. Schapire and  M. K. Warmuth.
1995.How to use expert advice, pages 382-391.Cesa-Bianchi, N., Y. Freund, D. P. Helmbold, andM.
Warmuth.
1994.
On-line prediction and conver-sion strategies.
In Computational Learning Theory:Eurocolt '93, volume New Series Number 53 of TheInstitute of Mathematics arid its Applications Confer-ence Series, pages 205-216, Oxford.
Oxford UniversityPress.Cohen, W. W. and Y.
Singer.
1996.
Context-sensitivelearning methods for text categorization.
In Proc.
ofthe 19th Annual Int.
ACM Conference on Researchand Development in Information Retrieval.Cortes, Corinna and Vladimir Vapnik.
1995.
Support-vector networks.
Machine Learning, 20(3):273-297.Duda, R. O. and P. E. Hart.
1973.
Pattern Classificationand Scene Analysis.
Wiley.Golding, A. R. and D. Roth.
1996.
Applying winnow tocontext-sensitive spelling correction.
In Proc.
of theInternational Conference on Machine Learning.Herbster, M. and M. Warmuth.
1995.
Tracking thebest expert.
In Proc.
12th International Conferenceon Machine Learning, pages 286-294.
Morgan Kanf-mann.Kivinen, J. and M. K. Warmuth.
1995a.
Exponentiatedgradient versus gradient descent for linear predictors.In Proc.
of STOC.
Tech Report UCSC-CRL-94-16.Kivinen, J. and M. K. Warmuth.
1995b.
The perceptronalgorithm vs. Winnow: linear vs. logarithmic mistakebounds when few input variables are relevant.
In Proc.8th Annu.
Conf.
on Comput.
Learning Theory, pages289-296.
ACM Press, New York, NY.Lewis, D. 1992.
An evaluation of phrasal and clusteredrepresentations on a text categorization problem.
InProc.
of the 15th Int.
ACM-SIGIR Conference on In-formation Retrieval.Lewis, D. and M. Ringuette.
1994.
A comparison of twolearning algorithms for text categorization.
In Proc.of Symposium on Document Analysis and InformationRetrieval.Lewis, D., R. E. Schapire, J. P. Callan, and R. Papka.1996.
Training algorithms for linear text classifiers.In SIGIR '96: Proc.
of the 19th Int.
Conference onResearch and Development in Information Retrieval,1996.Littlestone, N. 1988.
Learning quickly when irrelevantattributes abound: A new finear-threshold algorithm.Machine Learning, 2:285-318.Littlestone, N. 1991.
Redundant noisy attributes, at-tribute errors, and linear threshold learning usingWinnow.
In Proc.
$th Annu.
Workshop on Corn-put.
Learning Theory, pages 147-156, San Mateo, CA.Morgan Kanfmann.Littlestone, N. 1995.
Comparing severallinear-thresholdlearning algorithms on tasks involving superfluousattributes.
In Proc.
12th International Conferenceon Machine Learning, pages 353-361.
Morgan Kauf-mann.Littlestone, N. and M. K. Warmuth.
1994.
The weightedmajority algorithm.
Information and Computation,108(2):212-261.Rocchio, 3.
1971.
Relevance feedback information re-trieval.
In G. Salton, editor, The SMART retrievalsystem - experiments in automatic document process-ing.
Prentice-Hall, pages 313-323.Rosenblatt, F. 1958.
The perceptron: A probabilisticmodel for information storage and organization i  thebrain.
Psychological Review, 65:386-407.
(Reprintedin Neurocomputing (MIT Press, 1988).
).Salton, G. and C. Buckley.
1983.
Introduction to ModernInformation Retrieval.
McGraw-Hill.Tzeras, K. and S. Hartmann.
1993.
Automatic index-ing based on bayesian inference networks.
In Proc.of 16th Int.
ACM SIGIR Conference on Research andDevelopment in Information Retrieval.Wiener, E., J. Pedersen, and A. Weigend.
1995.
A neu-ral network approach to topic spotting.
In Symposiumon Document Analysis and Information Retrieval.63
