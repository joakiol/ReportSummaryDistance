Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 242?250,Atlanta, Georgia, June 13 2013. c?2013 Association for Computational LinguisticsNative Language Identification: A Key N-gram Category Approach   Kristopher Kyle, Scott Crossley Jianmin Dai, Danielle S. McNamara Georgia State University Arizona State University 34 Peachtree Ave, Ste 1200 PO Box 872111 Atlanta, GA 30303 Tempe, AZ 85287 kkyle3@student.gsu.edu, scrossley@gsu.edu Jianmin.Dai@asu.edu,  dsmcnamara1@gmail.com        AbstractThis study explores the efficacy of an ap-proach to native language identification that utilizes grammatical, rhetorical, semantic, syntactic, and cohesive function categories comprised of key n-grams.
The study found that a model based on these categories of key n-grams was able to successfully predict the L1 of essays written in English by L2 learners from 11 different L1 backgrounds with an ac-curacy of 59%.
Preliminary findings concern-ing instances of crosslinguistic influence are discussed, along with evidence of language similarities based on patterns of language misclassification.1.
Introduction Native language identification (NLI) is generally an automated task that can be used in authorship profiling (Wong & Dras, 2009) and in assisting automatic writing evaluation systems provide fo-cused feedback (e.g., Rozovskaya & Roth, 2011).
NLI is achieved by identifying patterns of lan-guage use that are common to a group of users of a particular second language (L2; e.g., English) that share a native language (L1).
Useful to the discus-sion of these patterns is the concept of crosslin-guistic influence (CLI), which references ?the consequences - both direct and indirect - that being a speaker of a particular native language (L1) has on the person?s use of a later learned language (Jarvis, 2012, p.1).
Beyond its theoretical applica-tions, CLI can also be used to inform L2 classroom pedagogy (Granger, 2009; Laufer & Girsai, 2008).
NLI studies, then, are informed by and can inform CLI, and have diverse applications.
The current study seeks to add to the discus-sions of NLI and CLI by testing the efficacy of a new approach ?
the use of grammatical, rhetorical, semantic, syntactic, and cohesive function catego-ries of key n-grams.
2.
Background In this section we outline two approaches to CLI, provide a selected review of relevant literature, and address gaps in the current body of NLI research.
2.1 Approaches to CLI  Jarvis (2000, 2010, 2012) has outlined two ap-proaches to the investigation of CLI: a compari-son-based and a detection-based approach.
The comparison-based approach is generally con-structed based on specific observed difference be-tween language systems (e.g., article usage in English as compared to article usage in Korean).
Whether or not these L1 differences affect L2 pro-duction is then analyzed by examining example texts (e.g., inappropriate use of articles by native speakers of Korean writing in English as an L2).
The detection-based argument, on the other hand, is built with the opposite trajectory.
Instead of be-ginning with hypotheses based on differences in language systems, researchers begin by identifying patterns of language use (e.g., inappropriate article use) that occur regularly by members of an L1 that242use a particular L2 (intragroup homogeneity) but do not occur regularly by other L1 users of the same L2 (intergroup heterogeneity).
These patterns of use are then verified through statistical and ma-chine learning techniques that use these patterns to predict the L1 group membership of L2 texts (i.e., NLI).
Recent advances in corpus development and natural language processing allow for larger num-bers of texts to be searched using a greater number of linguistic features.
These features can then be used to create an NLI predictor model.
A success-ful model not only fulfills the NLI task, but pro-vides further evidence that the observed patterns of language use can be attributable to CLI.
While Type I errors are certainly a potential issue in this argument, Jarvis (2012) explains that false posi-tives can be mitigated by balancing or controlling for potentially confounding variables (e.g., profi-ciency levels and essay prompts) during the con-struction of the target corpus.
2.2 Selected literature review A limited but growing number of studies have in-vestigated CLI using the detection-based approach, many of which are included in a volume edited by Jarvis and Crossley (2012).
Researchers have ex-plored the topic of CLI in the areas of lexical style (Jarvis et al 2012a), lexical n-grams (Jarvis & Paquot, 2012), character n-grams (Tsur & Rappo-prot, 2007), using variables related to cohesion, lexical sophistication, syntactic complexity and conceptual knowledge (Crossley & McNamara, 2012), error patterns (Bestgen, et al 2012; Wong & Dras, 2009), and a combination of these ap-proaches (Jarvis et al 2012b; Koppel et al 2005; Mayfield Tomokiyo & Jones, 2001, Wong & Dras, 2009).
Such studies have demonstrated relatively strong success rates for classifying an L2 writing sample based on the L1 of the writer.
For instance, Jarvis and Paquot (2012), using 1-4-grams as pre-dictor variables on a subset of argumentative es-says included in the International Corpus of Learner English (ICLE) (Granger et al 2009) achieved a 53.6% classification accuracy for 12 groups of L1s.
Crossley and McNamara (2012) used features related to cohesion, lexical sophisti-cation, syntactic complexity, and conceptual knowledge taken from the computational tool Coh-Metrix (Graesser et al 2004) to classify essays written in English by Czech, Finnish, German, and Spanish participants and achieved an L1 classifica-tion accuracy of 65-67.6%.
Using error types, Bestgen et al(2012), on 223 ICLE essays written by French, German, and Spanish L1 participants, achieved a classification accuracy of 65%.
In a follow-up study, Jarvis et al(2012b) explored the relative efficacy of these three CLI methods (n-grams, Coh-Metrix indices, and error types) using the corpus found in Bestgen et al(2012).
When all three approaches were used in the classification task, the accuracy increased to 79%.
2.3 Weakness of extant research in CLI Although the studies discussed so far have pro-duced statistical models that can predict the L1 group of a text written in L2 English with accura-cies well above chance, the degree to which these studies have demonstrated instances of CLI may be questionable as they draw on the ICLE corpus, which is arguably imbalanced (Jarvis et al 2012a, and Mayfield Tomokiyo, & Jones, 2001 being the exceptions).
While ICLE was designed with an attempt to control for a number of variables, the proficiency levels vary across language groups (as suggested by Koppel et al 2005, and empirically confirmed by Bestgen et al 2012) and though the argumentative texts are limited to a particular set of prompts within the corpus, these prompts are not equally distributed across language groups, raising the question of the degree to which the ob-served differences in texts were due to CLI, profi-ciency level, or essay prompt.
In addition, many of the linguistic features previously investigated did not lend themselves to providing strong links between observed differ-ences and CLI (e.g., the word concreteness and word frequency variables investigated in Crossley & McNamara, 2012).
A potentially promising method that has not been applied to detection-based CLI studies that may address these limita-tions is the use of rhetorical, syntactic, grammati-cal and cohesive categories comprised of key n-grams.
Such features have recently been investi-gated by Crossley, Defore, Kyle, Dai, and McNa-mara (submitted for publication), in which they explored their usefulness for assessing the efficacy of an automatic writing evaluation (AWE) system.
In this study, Crossley et alseparated a corpus of243essays into introduction, body, and conclusion paragraphs, and then further separated these into high and low proficiency categories based on over-all essay score.
They then identified n-grams that occurred significantly more often (positive keyness values) in paragraphs of a certain type (e.g., intro-duction) from high scoring essays than the same type of paragraphs from low-scoring essays.
Addi-tionally, they identified n-grams that occurred sig-nificantly less often (negative keyness values) in high-scoring paragraphs of a certain type than low-scoring paragraphs of the same type.
Positively and negatively key n-grams for each paragraph type were then separated into categories based on their rhetorical, syntactic, grammatical, and cohesive features.
These categories were then successfully used as variables in a multiple regression to create a model that accounted for between 24%-33% of the variance in essay scores.
This study demon-strates the efficacy of using grammatical, rhetori-cal, syntactic, and cohesive function categories of key n-grams to identify instances of linguistic variation that successfully predict essay quality.
These findings hold promise for the use of similar methods to contribute to the study of CLI by iden-tifying linguistic variation across different L1 groups writing in the same L2.
2.4 Goals of the current study The current study, while drawing on previous re-search (notably Jarvis & Paquot, 2012 and Crossley et al submitted for publication), contrib-utes to the detection-based CLI discussion by: a) examining a prompt and proficiency-controlled corpus and, b) using n-gram indices related to grammatical, rhetorical, semantic, syntactic, and cohesive functions to assess difference in L2 es-says based on the L1 of the writers.
This study is guided by the following research questions:  1.
Can a model consisting of functional categorical n-grams predict the native language of an L2 writer of English?
2.
Does the resulting model inform theories of CLI?3.
Method In this section, we describe the corpus used for our training and test set, the methods used for key n-gram identification, and the grouping of these n-grams into grammatical, rhetorical, semantic, syn-tactic, and cohesive categories.
3.1 Corpus For this project we used an 11,000 essay subset of the 12,100 essay TOEFL11 corpus (Blanchard, Tetreault, Higgins, Cahill, & Chodorow, 2013).
The TOEFL11 corpus is comprised of independent task essays written during administrations of the Test of English as a Foreign Language (TOEFL) between 2006-2007 (Blanchard et al 2013).
The corpus is balanced across 11 native language (L1) groups, includes responses to eight different inde-pendent-task prompts, and includes essays written by low, medium, and high proficiency writers.
The languages represented include Arabic, Chinese, French, German, Hindi, Italian, Japanese, Korean, Spanish, Telugu, and Turkish.
Following the pro-cedures of the NLI shared task (Tetreault, Blanch-ard, & Cahill, 2013), 1,100 of the original 11,000 essays were set aside as the test set, leaving a train-ing corpus of 9,900 essays.
3.2 Identifying key n-grams In this study, we considered n-grams from 1-10 words in length.
N-grams were considered to be key if they occur in a corpus significantly more or less frequently than in a reference corpus.
We identified key n-grams using the KeyWords func-tion of Wordsmith Tools 6 (Scott, 2013) and the default log likelihood method of identifying key n-grams (McEnery & Hardie, 2012).
To ensure that the keyness of a particular n-gram was representa-tive of use across a particular L1 group and not due to prolific use by a small number of individuals, we set the minimum threshold for inclusion at a range of 10 percent (n-grams had to occur in at least 10 percent of the texts written by a particular L1 group).
Using these parameters, we conducted keyness tests for each language group.
To create the key n-gram list for the Arabic group, for exam-ple, we compared the frequency of n-grams in the Arabic group to the frequency of n-grams in all of the other language groups combined.
This process244was completed for each language group until a key n-gram list existed for each.
Because one of the goals of our study was to generalize instances of CLI to essays written on prompts other than those included in the TOEFL11 Corpus, it was important to remove all prompt-based words from our key n-gram lists.
Removing all words occurring in the prompts from the n-grams list would remove a number of high fre-quency words that may not be prompt-based (e.g., the, to), so prompt-based words were operationally defined as content words and their lemmas in-cluded in the prompt that had a Kucera and Francis (1967) written frequency value of 715 or less.
N-grams were removed from potential predictor sets if they contained any of these prompt-based words.
The remaining key n-grams for each language group were then sorted by absolute keyness in each group and filtered for redundancy.
For example, prior to this stage, the Chinese key n-gram list in-cluded both more and have more.
Because more had a higher absolute keyness value than have more, have more was removed from the Chinese key n-gram list.
Table 1 provides a summary of the length of key n-grams identified in each stage of the selec-tion process.
Although n-grams from 1-10 words in length were initially considered, no n-grams longer than 5-grams were identified as being key.
Addi-tionally, all 5-grams, such as the key Chinese n-gram ?group led by a tour?, and the Telugu n-gram ?agree with the statement that?
contained prompt-based words and were removed from further con-sideration.
After the final n-gram refining step, the longest n-gram was a single 4-gram, the Turkish n-gram ?on the other hand?.3.3 Grouping of key n-grams into indices The last stage in our variable selection process was to group the key n-grams in each language group into categories.
First, two indices for each lan-guage group were created.
The first included all n-grams with positive keyness values that remained after the filtering process described above.
The second included all of the n-grams with negative keyness values after filtering.
Next, positive and negative n-grams were sorted into grammatical, rhetorical, semantic, syntactic, and cohesive func-tion categories by two trained linguists with expe-rience in the area of second language writing.
The purpose of sorting n-grams in this manner was to identify patterns of relative over/underuse by each language group.
See Table 2 for a list of all of the indices created during this process.
3.4 Evaluation of model In CLI studies and other studies that attempt to predict the group membership of a text, discrimi-nant function analysis (DFA) is often used (Jarvis & Paquot, 2012; Crossley & McNamara, 2012).
Although other methods can be used, such as sup-port vector machine decision trees (e.g., Koppel et al 2005) or Na?ve Bayes (e.g., Mayfield To-mokiyo & Jones, 2001), DFA has the advantage of being the most transparent of these with regard to interpreting results (Jarvis, 2012).
DFA was there-fore chosen as the method of analysis for this study, using L1 as the dependent variable and n-gram indices as independent variables.
The first step in the analysis was to check the independent variables for multicollinearity using a Pearson correlation matrix.
Any two variables above a threshold of p>.899 were flagged for fur-ther analysis.
A MANOVA was then conducted using the languages from one proficiency group as independent variables and the predictor indices/n-grams as dependent variables.
The effect sizes pro-duced by the MANOVA were used to select which variables flagged in the correlation matrix would be retained, and which would be eliminated.
Within each highly correlated pair, the variable with the largest effect size was kept.
Finally, a DFA was conducted on the training set.
The pre-dictor model sets identified in the DFA were thenN-gram Length Original No Prompt Words After Final Sort 5 5 0 0 4 19 3 1 3 110 54 8 2 699 512 147 1 1100 877 770 Total 1933 1446 926     Table 1: Length of key n-grams.245L1 Index Coverage  Variable Category - + Total Examples ALL  11 11 22 see below Adjectives Syntactic 0 1 1 little, kind, real Adverbs Syntactic 0 2 4 always, easily just, still Articles Cohesion 8 8 16 a, an, the Auxilliary Verbs Syntactic 2 0 2 has, have, will Certainty Semantic 0 1 1 necessary, sure, true Cognition Semantic 0 1 1 experience, thought Comparatives Rhetorical 0 1 1 easier, much more Conjunctions Cohesion 6 5 11 and, because, or Connectives Cohesion 1 2 3 and to, and that, also Determiners Cohesion 1 0 1 that, this Evaluation Semantic 0 1 1 good, fun, like to Examples Semantic 0 1 1 particular, etc Explanation Semantic 0 4 4 explain, in order to, that is Go Semantic 0 1 1 are going, go, going to Irrealis Grammatical 0 1 1 what, will Modality Rhetorical 9 9 18 we can, could, can be Negation Syntactic 3 8 11 but not, no Nouns Syntactic 3 7 10 country, person, places Options Rhetorical 0 1 1 consider, different, instead People Semantic 1 4 5 people, society, friends Place Semantic 0 1 1 city, place, places Possession Semantic 1 1 2 his, having, your Possibility Rhetorical 0 3 3 probably, maybe, possible Pre-infinitive Syntactic 0 1 1 how to, time to, way to Prepositions Grammatical 10 9 19 from, about, with a Problems Semantic 1 1 2 problem, problems Pronouns Cohesion 10 11 21 he, his, your Quantity Semantic 11 11 22 every, more than, some Questions Syntactic 7 6 13 where, who, why, question Science/ Tech-nology Semantic 0 2 2 computer, internet Signifying Rhetorical 0 1 1 see, mean Specificity Rhetorical 0 3 3 certain, especially, special Stance Rhetorical 2 6 8 feel that, in my, opinion Temporality Semantic 6 7 13 during, more and more, often  To Be Syntactic 6 8 14 are, been, it is Transitions Cohesion 4 9 13 but, however, therefore Vagueness Semantic 0 1 1 general, someone, something Verbs Syntactic 5 8 13 choose, make, play Work/Study Semantic 2 7 9 money, study, parents Total  110 167 277        Table 2: Negative and positive key n-gram variables.246used on the essays in the test set to determine whether the model sets could generalize to a new population.4.
Results The training set DFA predicted L1 group member-ship of TOEFL independent essays with an accu-racy of 60% using 184 indices (df= 100, n= 9900, ?2= 32997.259, p< .001), which is significantly higher than the baseline chance of 9%.
The re-ported Kappa = .560, indicates a moderate rela-tionship between actual and predicted L1.
The predictive accuracy of the model was veri-fied on the test set, in which L1 group membership was predicted with an accuracy of 59% (df= 100, n= 1100, ?2= 3550.791, p< .001).
The reported Kappa = .549, indicates a moderate agreement be-tween the actual and predicted L1.
Table 3 in-cludes the test set confusion matrix.
5.
Discussion  The results of this study suggest the usefulness of key n-grams grouped into categories based on their grammatical, rhetorical, semantic, syntactic, and cohesive features for NLI.
The results demonstrate that such indices can correctly classify 59% of es-says written in English as belonging to 1 of 11 L1 populations.
In addition, with regard to n-gram length, we found that although n-grams 1-10 words in length were initially considered, no n-grams longer than5-grams were identified as key, and the longest n-gram that remained after removing prompt-based and redundancy was a single 4-gram.
This suggests that 4-grams (or possibly even 3-grams) may be a useful threshold for future investigations.
5.1 Preliminary CLI findings As Jarvis (2012) notes, CLI studies that use the detection-based argument to CLI are exploratory in nature, while studies that use the comparison-based argument are confirmatory in nature.
The present study is, thus, exploratory in nature, and without substantial further investigation, we cannot defini-tively posit whether observed differences and simi-larities in English use can be attributed to the influence of the L1 itself or to cultural or educa-tional norms.
Nonetheless, a few preliminary observations are worthy of discussion.
First, we identified a number of patterns of language use that may be attributable to CLI.
Although a full discussion of these is beyond the scope of this paper, Table 4 includes examples of potential CLI features in ref-erence to the German writers represented in the corpus.
The table demonstrates the particular n-grams that German writers are likely to use more or less often than writers of the other 10 languages.
German writers, for example, are more likely to use the phrasal modals able to, have to, has to, and singular modals might and would more often than writers of the other language groups, but are less likely to use the modals can and may.
These find-ings are preliminary, and further research that linksARACHI FRE GER HIN ITA JPN KOR SPA TEL TUR Precision Recall F-measure ARA 66 0 5 3 1 3 2 4 8 1 7 53.2% 66.0% 58.9% CHI 3 63 5 3 2 0 6 9 0 3 6 57.8% 63.0% 60.3% FRE 3 4 64 7 3 6 2 1 6 0 4 64.6% 64.0% 64.3% GER 2 5 5 64 3 5 2 4 6 0 4 62.7% 64.0% 63.4% HIN 4 5 0 7 54 1 0 1 6 17 5 56.8% 54.0% 55.4% ITA 4 1 9 10 1 64 2 1 6 0 2 68.8% 64.0% 66.3% JPN 6 7 1 1 0 1 64 9 2 1 8 61.5% 64.0% 62.7% KOR 5 9 2 1 2 0 19 56 2 0 4 57.7% 56.0% 56.9% SPA 14 6 6 3 4 9 2 3 43 2 8 47.3% 43.0% 45.0% TEL 5 3 0 1 22 1 1 1 4 60 2 70.6% 60.0% 64.9% TUR 12 6 2 2 3 3 4 8 8 1 51 50.5% 51.0% 50.7%                Table 3: Test set confusion matrix.247these English n-grams with patterns of use in Ger-man is needed.
Additionally, our findings provide some evi-dence for close relationships between languages.
For example, when checking for multicollinearity,we found that the All Negative Japanese and All Negative Korean categories were very strongly correlated (r =.946, p< .001).
Upon further exami-nation, 8 of the 19 n-grams (42%) in the All Nega-tive Japanese category occurred in the corresponding Korean category.
The overlapping n-grams were the n-grams all, any, but, different, or, person, this, and your, which may indicatesimilarities between these language systems in that speakers from both language avoid the use of these words.
Patterns of essay categorization also provide preliminary insights into language similarities.
Based on the test set confusion matrix (see Table 3), a few conflicting patterns emerged.
Among the Indo-European languages represented, the Ro-mance (French, Italian, and Spanish) and Germanic (German) languages were regularly miscategorized as one another.
Italian essays, for example, were predicted to be French, German, and Spanish 9%, 10%, and 6% of the time, respectively, but were predicted to be other languages only 0%-4% of the time.
This seems to confirm generally accepted language taxonomies, though Spanish was pre-dicted to be Arabic (14%) and Turkish (8%) more often than Italian (6%) or French (6%) (as com-pared to 3% for German, and no more than 4% for other languages).
While similarities between language families seem to support extant language taxonomies (see Blanchard et al 2013) and lend credence to claims of CLI, other observations may cast doubt on these.
Hindi (an Indo-Iranian member of the Indo-European family) essays were predicted to be Telugu (Dravidian) essays 17% of the time, and Telugu essays were predicted to be Hindi essays 22% of the time.
This may indicate instances of cultural proximity or educational similarities as opposed to linguistic transfer (and/or borrowing) because these languages are both spoken within India.
Further investigations of these issues are clearly needed.
5.2 Limitations While we have confidence in our findings, there are limitations to the analysis that need to be dis-cussed.
The TOEFL11 corpus was designed to be comparable across languages.
While it largely ac-complishes this goal, it is not well balanced across proficiency levels (which may reflect the relative proficiency levels of TOEFL test-takers).
Although medium and high proficiency levels are well (though not equally) represented, the low profi-ciency group represents only 11% of the number of texts and an estimated 7.2% of total words (based on mean lengths of essays from each proficiency level given in Blanchard et al 2013).
The medium proficiency group represented 54.4% of the textsVariable Positive Negative Adverbs just, only, there, nec-essary  Compara-tives easier, much more  Conjunc-tions or, but, as well  Modals able to, have to, has to, might, would can, may Nouns development, job, topic, something person, place Preposi-tions at, on about, byPronouns everybody, this, you, yourshe, its, I his, us, he, we, they, our Quantity (and ex-ample) another, amount of, both, less, lot, whole any, many, some, such Specific certain, especially, special  Stance in my, of course, opinion, pointTempo-rality often, stillday, now, second, then, time to, second To Be be able, it is, to be was Transi-tions furthermore, one hand, other hand  Verbs look, to get, work go, going, study    Table 4: German predictor variables.248and an estimated 52.8% of words in the corpus, and the high proficiency group comprised the re-maining 34.7% of the texts and an estimated 40% of the words.
This indicates that caution should be used when generalizing any CLI findings from this study to low proficiency language users.
Further-more, any CLI findings will be biased towards me-dium proficiency language users.
Another limitation that may have affected the accuracy of the model was the way in which poten-tial predictor variables were refined.
For each lan-guage, the absolute keyness values were used when refining the lists of potential n-gram predictors (as discussed in Section 3.2).
After the data had been processed, we discovered that this process re-moved some n-grams that should have remained.
In a very few instances redundant n-grams (e.g., have; have more) had a positive keyness value for one n-gram (have) and a negative keyness value for the other (have more).
Because all n-grams were later grouped into categories based on posi-tive and negative keyness values, both have and have more should have been retained (as they would not have occurred in any of the same cate-gories).
In future studies, positive and negative n-grams will be kept separate during the elimination of redundant n-grams.
Another limitation that was discovered after the data analysis was that a data input error caused All Negative Chinese n-gram category to be com-bined with two n-grams included in the Positive Chinese School and Home category.
A similar er-ror retained two positive German adverb categories (with one overlapping n-gram, just).
The models described in this study retained these variables, as they were not highly correlated with each other or any other variable (based on the r > .899 thresh-old), so any CLI findings based solely on these variables should be considered with caution.
5.3 Future research Although it is clear that categorical n-grams can be used as successful NLI predictor variables, it is unclear whether this approach is more or less ef-fective than the use of raw counts of frequent words or n-grams (e.g., Jarvis et al 2012a; Jarvis & Paquot, 2012).
Future research should explore the relative effectiveness of these methods using the TOEFL11 corpus to determine whether thetime involved to create key n-gram lists and then sort those lists into categories is warranted.
Finally, another remaining question is whether the key n-grams identified in this study are due to linguistic factors or, alternatively, other influences such as culture and educational materials.
Acknowledgements We thank ETS for compiling and providing the TOEFL11 corpus, and we also thank the organizers of the NLI Shared Task 2013.
References  Bestgen, Y., Granger, S., & Thewissen, J.
(2012).
Error patterns and automatic l1 identification.
In S. Jarvis and S. A. Crossley (Eds.
), Approaching Language Transfer through Text Classification: Explorations in the Detection-Based Approach.
(pp.
127-153).
Bris-tol, UK: Multilingual Matters.
Blanchard, D., Tetreault, J., Higgins, D., Cahill, A., & Chodorow, M. (2013).
TOEFL11: A Corpus of Non-Native English.
Princeton, NJ: Educational Testing Service.
Crossley, S. A., & McNamara, D. S. (2012).
Detecting the first language of second language writers using automated indices of cohesion, lexical sophistication, syntactic complexity, and conceptual knowledge.
In S. Jarvis and S. A. Crossley (Eds.
), Approaching Language Transfer through Text Classification: Ex-plorations in the Detection-Based Approach.
(pp.
106-126).
Bristol, UK: Multilingual Matters.
Crossley, S. A., Defore, C., Kyle, K., Dai, J., & McNa-mara, D. S. (under review).
Paragraph specific n-gram approaches to automatically assessing essay quality.
Sixth International Conference on Educa-tional Data Mining, Memphis, TN.
Graesser, A. C., McNamara, D. S., Louwerse, M. M., & Cai, Z.
(2004).
Coh-metrix: Analysis of text on cohe-sion and language.
Behavior Research Methods, In-struments, & Computers, 36(2), 193-202.
Granger, S., Dagneaux, E.., Meunier, F., Paquot, M.
(Eds.)
(2009).
International corpus of learner eng-lish.
version 2.
Belgium: Presses universitaires de Louvain.
Granger, S. (2009) The contribution of learner corpora to second language acquisition and foreign language teaching: A critical evaluation.
In: Aijmer, K., Cor-pora and Language Teaching, Benjamins: Amster-dam and Philadelphia, 2009, p. 13-32.
Jarvis, S. (2000).
Methodological rigor in the study of transfer: Identifying L1 influence in the interlan-guage lexicon.
Language Learning, 50, 245-309.
Jarvis, S. (2010).
Comparison-based and detection-based approaches to transfer research.
In L. Roberts,249M.
Howard, M. ?
Laoire, & D. Singleton (Eds.
), EUROSLA Yearbook 10 (pp.
169-192).
Amsterdam: Benjamins.
Jarvis, S. (2012).
The detection-based approach: An overview.
In S. Jarvis & S.A. Crossley (Eds.
), Ap-proaching language transfer through text classifica-tion: Explorations in the detection-based approach (pp.
1-33).
Bristol, UK: Multilingual Matters.
Jarvis, S., & Crossley, S. A.
(2012).
Approaching lan-guage transfer through text classification: Explora-tions in the detection-based approach.
Bristol, UK: Multilingual Matters.
Jarvis, S., Bestgen, Y., Crossley, S. A., Granger, S., Paquot, M., Thewissen, J., & McNamara, D. S. (2012).
The comparative and combined contributions of n-grams, Coh-Metrix indices, and error types in the L1 classification of learner texts.
In S. Jarvis & S.A. Crossley (Eds.
), Approaching language transfer through text classification: Explorations in the detec-tion-based approach (pp.
154-177).
Bristol, UK: Multilingual Matters.
Jarvis, S., & Paquot, M. (2012).
Exploring the role of n-grams in L1 identification.
In S. Jarvis & S.A. Crossley (Eds.
), Approaching language transfer through text classification: Explorations in the detec-tion-based approach (pp.
71-105).
Bristol, UK: Mul-tilingual Matters.
Jarvis, S., Casta?eda-Jim?nez, G., & Nielsen, R. (2012).
Detecting L2 writers?
L1s on the basis of their lexical styles.
In S. Jarvis & S.A. Crossley (Eds.
), Approach-ing language transfer through text classification: Ex-plorations in the detection-based approach (pp.
34-70).
Bristol, UK: Multilingual Matters.
Koppel, M., Schler, J.
& Zigdon, K. (2005).
Determin-ing an author?s native language by mining for text er-rors.
In Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining (pp.
624-628).
Chicago: Association for Computing Machinery.
Kucera, H. and Francis, W. N. (1967).
Computational Analysis of Present-Day American English Provi-dence, RI: Brown University Press.
Laufer, B., & Girsai, N. (2008).
Form-focused instruc-tion in second language vocabulary learning: A case for contrastive analysis and translation.
Applied Lin-guistics, 29(4), 694-716.
Mayfield Tomokiyo, L. & Jones, R. (2001).
You?re not from ?round here, are you?
Na?ve Bayes detection of non-native utterance text.
In Proceedings of the Sec-ond Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL ?01), unpaginated electronic document.
Cambridge, MA: The Association for Computational Linguistics.
McEnery, T., & Hardie, A.
(2012).
Corpus linguistics: method, theory and practice.
Cambridge, New York: Cambridge University Press, 2012.Rozovskaya, A.
& Roth, D. (2011).
Algorithm selection and model adaptation for ESL correction tasks.
In Proceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL ?11).
Scott, M., (2013).
WordSmith Tools.
Liverpool: Lexical Analysis Software.
Tetreault, J., Blanchard, D., & Cahill, A.
(2013).
Sum-mary report on the first shared task on native lan-guage identification.
In Proceedings of the Eighth Workshop on Building Educational Applications Us-ing NLP, unpaginated electronic document.
Atlanta, GA: Association for Computational Linguistics.
Tsur, O.
& Rappoport, A.
(2007).
Using classifier fea-tures for studying the effect of native language on the choice of written second language words.
In Pro-ceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition.
(pp.
9-16).
Cambridge, MA: The Association for Computational Linguistics.
Wong, S.-M.J. & Dras, M. (2009).
Contrastive analysis and native language identification.
In Proceedings of the Australasion Language Technology Association (pp.
53-61).
Cambridge, MA: The Association for Computational Linguistics.250
