Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 886?894,Beijing, August 2010Using Web-scale N-grams to Improve Base NP Parsing PerformanceEmily PitlerComputer and Information ScienceUniversity of Pennsylvaniaepitler@seas.upenn.eduShane BergsmaDepartment of Computing ScienceUniversity of Albertasbergsma@ualberta.caDekang LinGoogle, Inc.lindek@google.comKenneth ChurchHuman Language Technology Center of ExcellenceJohns Hopkins Universitykenneth.church@jhu.eduAbstractWe use web-scale N-grams in a base NPparser that correctly analyzes 95.4% of thebase NPs in natural text.
Web-scale dataimproves performance.
That is, there is nodata like more data.
Performance scaleslog-linearly with the number of parame-ters in the model (the number of uniqueN-grams).
The web-scale N-grams areparticularly helpful in harder cases, suchas NPs that contain conjunctions.1 IntroductionNoun phrases (NPs) provide an index to theworld?s information.
About 70% of web queriesare NPs (Barr et al, 2008).
A robust NP parsercould help search engines improve retrieval per-formance on multi-word NP queries (Zhai, 1997).For example, by knowing the correct parse of?washed (baby carrots),?
a search engine couldensure that returned pages (and advertisements)concern clean carrots rather than clean babies.
NPstructure is also helpful for query expansion andsubstitution (Jones et al, 2006).This paper is concerned with base NP pars-ing.
We are given a base NP string as input,and the task is to produce a parse tree as output.Base NPs are NPs that do not contain embeddednoun phrases.
These are sometimes called NPchunks, or core/non-recursive NPs (Church, 1988;Ramshaw and Marcus, 1995).
Correctly parsing(or, equivalently, bracketing) base NPs is chal-lenging because the same part-of-speech (POS)sequence can be parsed differently depending onthe specific words involved.
For example, ?retired(science teacher)?
and ?
(social science) teacher?have different structures even though they haveidentical POS sequences.Lexical statistics are therefore needed in orderto parse the above examples, and they must becomputed over a lot of text to avoid sparsity.
Allof our lexical statistics are derived from a newand improved web-scale N-gram corpus (Lin etal., 2010), which we call Google V2.Despite the importance of base NPs, mostsentence parsers do not parse base NPs, sincethe main training corpus for parsers, the PennTreebank (PTB) (Marcus et al, 1994), leaves aflat structure for base NPs.
Recent annotationsby Vadas and Curran (2007a) added NP structureto the PTB.
We use these annotations (describedin Section 3) for our experiments.NP parsers usually focus on bracketing three-word noun compounds.
Parsing three-word nouncompounds is a fairly artificial task; we show thatsequences of three nouns make up less than 1%of the three-word-or-longer base NPs in naturaltext.
As the NP length increases, the number ofpossible binary trees (parses) increases with theCatalan numbers (Church and Patil, 1982).
NPs oflength three have just two possible parses (chanceis 50%), while NPs of length six already haveforty-two possible parses (chance is 2%).
LongNPs therefore provide much more opportunity toimprove performance over the baseline.
In Table1 (Section 7), we show the distribution of base NPlength in the PTB.
While most NPs are of lengththree, NP length has a long tail.886The three-word noun compound assumptionalso restricts research to the case in which allwords are nouns, while base NPs also contain de-terminers, possessives, adjectives, and conjunc-tions.
Conjunctions and their scopes are particu-larly challenging.
For example, in the NP, ?Frenchtelevision and movie producers,?
a parser shouldconjoin ?
(television) and (movie),?
as opposed to?
(French television) and (movie),?
?
(French tele-vision) and (movie producers)?
or ?
(television)and (movie producers).
?To resolve these issues, we train a classifierwhich uses contextual information from the entireNP and lexical statistics derived from the web-scale N-gram corpus to predict if a given spanis a constituent.
Our parser then uses this clas-sifier to produce a score for every possible NP-internal bracketing and creates a chart of bracket-ing scores.
This chart can be used as features in afull sentence parser or parsed directly with a chartparser.
Our parses are highly accurate, creating astrong new standard for this task.Finally, we present experiments that investigatethe effects of N-gram frequency cutoffs and vari-ous sources of N-gram data.
We show an interest-ing relationship between accuracy and the numberof unique N-gram types in the data.2 Related Work2.1 Three-Word Noun CompoundsThe most commonly used data for NP parsing isfrom Lauer (1995), who extracted 244 three-wordnoun compounds from the Grolier encyclopedia.When there are only three words, this task reducesto a binary decision:?
Left Branching: * [retired science] teacher?
Right Branching: retired [science teacher]In Lauer (1995)?s set of noun compounds, two-thirds are left branching.The main approach to these three-word nouncompounds has been to compute associationstatistics between pairs of words and then choosethe bracketing that corresponds to the more highlyassociated pair.
The two main models are theadjacency model (Marcus, 1980; Liberman andSproat, 1992; Pustejovsky et al, 1993; Resnik,1993) and the dependency model (Lauer, 1995).Under the adjacency model, the bracketing deci-sion is made by comparing the associations be-tween words one and two versus words two andthree (i.e.
comparing retired science versus sci-ence teacher).
In contrast, the dependency modelcompares the associations between one and twoversus one and three (retired science versus retiredteacher).
Lauer (1995) compares the two modelsand finds the dependency model to be more accu-rate.Nakov and Hearst (2005) compute the associ-ation scores using frequencies, conditional proba-bilities, ?2, and mutual information, for both pairsof words and for linguistically-motivated para-phrases.
Lapata and Keller (2005) found that us-ing web-scale data for associations is better thanusing the (smaller) 100M-word British NationalCorpus.2.2 Longer NPsFocusing on only the three word case misses alarge opportunity for base NP parsing.
NPs longerthan three words commonly occur, making up29% of our test set.
In addition, a chance baselinedoes exponentially worse as the length of the NPincreases.
These longer NPs are therefore a majoropportunity to improve overall base NP parsing.Since in the general case, NP parsing can nolonger be thought of as a single binary classifica-tion problem, different strategies are required.Barker (1998) reduces the task of parsinglonger NPs to making sequential three-word de-cisions, moving a sliding window along the NP.The window is first moved from right-to-left, in-serting right bracketings, and then again from left-to-right, finalizing left bracketings.
While Barker(1998) assumes that these three-word decisionscan be made in isolation, this is not always valid.1Vadas and Curran (2007b) employ Barker?s algo-rithm, but use a supervised classifier to make thesequential bracketing decisions.
Because theseapproaches rely on a sequence of binary decisions,1E.g., although the right-most three words are identicalin 1) ?soap opera stars and television producers,?
and 2)?movie and television producers,?
the initial right-bracketingdecision for ?and television producers?
should be differentin each.887early mistakes can cascade and lead to a chain ofincorrect bracketings.Our approach differs from previous work in NPparsing; rather than greedily inserting brackets asin Barker?s algorithm, we use dynamic program-ming to find the global maximum-scoring parse.In addition, unlike previous approaches that haveused local features to make local decisions, we usethe full NP to score each potential bracketing.A related line of research aims to segmentlonger phrases that are queried on Internet searchengines (Bergsma and Wang, 2007; Guo et al,2008; Tan and Peng, 2008).
Bergsma and Wang(2007) focus on NP queries of length four orgreater.
They use supervised learning to makesegmentation decisions, with features derivedfrom the noun compound bracketing literature.Evaluating the benefits of parsing NP queries,rather than simply segmenting them, is a naturalapplication of our system.3 Annotated DataOur training and testing data are derived from re-cent annotations by Vadas and Curran (2007a).The original PTB left a flat structure for base nounphrases.
For example, ?retired science teacher,?would be represented as:(NP (JJ retired) (NN science) (NN teacher))Vadas and Curran (2007a) annotated NP-internalstructure by adding annotations whenever there isa left-bracketing.
If no annotations were added,right-branching is assumed.
The inter-annotatoragreement for exactly matching the brackets on anNP was 98.5%.This data provides a valuable new resource forparsing research, but little work has so far madeuse of it.
Vadas and Curran (2007b) performsome preliminary experiments on NP bracketing,but use gold standard part-of-speech and named-entity annotations as features in their classifier.Our work establishes a strong and realistic stan-dard on this data; our results will serve as a basisfor further research on this topic.4 Unlabeled N-gram DataAll of our N-gram features described in Sec-tion 6.1 rely on probabilities derived from unla-beled data.
To use the largest amount of datapossible, we exploit web-scale N-gram corpora.N-gram counts are an efficient way to compresslarge amounts of data (such as all the text on theweb) into a manageable size.
An N-gram corpusrecords how often each unique sequence of wordsoccurs.
Co-occurrence probabilities can be calcu-lated directly from the N-gram counts.
To keepthe size manageable, N-grams that occur with afrequency below a particular threshold can be fil-tered.The corpus we use is Google V2 (Lin et al,2010): a new N-gram corpus with N-grams oflength 1-5 that we created from the same 1 tril-lion word snapshot of the web as Google N-gramsVersion 1 (Brants and Franz, 2006), but with sev-eral enhancements.
Duplicate sentences are re-moved, as well as ?sentences?
which are probablynoise (indicated by having a large proportion ofnon-alphanumeric characters, being very long, orbeing very short).
Removing duplicate sentencesis especially important because automatically-generated websites, boilerplate text, and legal dis-claimers skew the source web data, with sentencesthat may have only been authored once occurringmillions of times.
We use the suffix array toolsdescribed in Lin et al (2010) to quickly extractN-gram counts.5 Base NP Parsing ApproachOur goal is to take a base NP string as input andproduce a parse tree as output.
In practice, itwould be most useful if the NP parse could beintegrated into a sentence parser.
Previous NPparsers are difficult to apply in practice.2 Workin prepositional phrase attachment that assumesgold-standard knowledge of the competing attach-ment sites has been criticized as unrealistic (At-terer and Schu?tze, 2007).Our system can easily be integrated into fullparsers.
Its input can be identified quickly andreliably and its output is compatible with down-stream parsers.2For example, Vadas and Curran (2007b) report results onNP parsing, but these results include NPs containing preposi-tional or adverbial phrases (confirmed by personal communi-cation).
Practical application of their system would thereforerequire resolving prepositional phrase attachment as a pre-processing step.888Our parser?s input is base NPs, which can beidentified with very high accuracy.
Kudo and Mat-sumoto (2001) report 95.8% NP chunking accu-racy on PTB data.Once provided with an NP, our system uses asupervised classifier to predict the probability ofa particular contiguous subsequence (span) of theNP being a constituent, given the entire NP as con-text.
This probability can be inserted into the chartthat a standard chart parser would use.For example, the base NP ?French televisionand movie producers?
would be decomposed intonine different classification problems, scoring thefollowing potential bracketings:(French television) and movie producersFrench (television and) movie producers(French television and) movie producers ...French television and (movie producers)In Section 6, we detail the set of statistical andstructural features used by the classifier.The output of our classifier can be easily usedas a feature in a full-sentence structured predictionparser, as in Taskar et al (2004).
Alternatively,our work could be integrated into a full-sentenceparser by using our feature representations di-rectly in a discriminative CFG parser (Finkel etal., 2008), or in a parse re-ranker (Ratnaparkhi etal., 1994; Collins and Koo, 2005; Charniak andJohnson, 2005).While our main objective is to use web-scalelexical statistics to create an accurate classifier forbase NP-internal constituents, we do produce aparse tree for evaluation purposes.
The probabil-ity of a parse tree is defined as the product of theprobabilities of all the spans (constituents) in thetree.
The most probable tree is computed with theCYK algorithm.6 FeaturesOver the course of development experiments, wediscovered that the more position-specific our fea-tures were, the more effectively we could parseNPs.
We define a word?s position as its distancefrom the right of the full NP, as the semantic headof NPs is most often the right-most word.
Ulti-mately, we decided to conjoin each feature withthe position of the proposed bracketing.
Sincethe features for differing proposed bracketings arenow disjoint, this is equivalent to scoring bracket-ings with different classifiers, with each classifierchosen according to the bracketing position.
Wenow outline the feature types that are common,but weighted differently, in each proposed brack-eting?s feature set.6.1 N-gram FeaturesAll of the features described in this section requireestimates of the probability of specific words orsequences of words.
All probabilities are com-puted using Google V2 (Section 4).6.1.1 PMIRecall that the adjacency model for the three-word task uses the associations of the two pairs ofadjacent words, while the dependency model usesthe associations of the two pairs of attachmentsites for the initial noun.
We generalize the ad-jacency and dependency models by including thepointwise mutual information (Church and Hanks,1990) between all pairs of words in the NP:PMI(x, y) = log p(?x y?)p(?x?)p(?y?)
(1)For NPs of length n, for each proposed bracket-ing, we include separate features for the PMI be-tween all(n2)pairs of words in the NP.
For NPs in-cluding conjunctions, we include additional PMIfeatures (Section 6.1.2).Since these features are also tied to the pro-posed bracketing positions (as explained above),this allows us to learn relationships between var-ious associations within the NP and each poten-tial bracketing.
For example, consider a proposedbracketing from word 4 to word 5.
We learn thata high association of words inside a bracketing(here, a high association between word 4 and word5) indicates a bracketing is likely, while a highassociation between words that cross a proposedbracketing (e.g., a high association between word3 and word 4) indicates the bracketing is unlikely.The value of these features is the PMI, if it isdefined.
If the PMI is undefined, we include oneof two binary features:p(?x y?)
= 0 or p(?x?)
?
p(?y?)
= 0.889We illustrate the PMI features with an example.In deciding whether (movie producers) is a rea-sonable bracketing within ?French television andmovie producers,?
the classifier weighs featuresfor all of:PMI(French, television)PMI(French, and).
.
.PMI(television, producers)PMI(and, producers)PMI(movie, producers)6.1.2 ConjunctionsProperly handling NPs containing conjunc-tions (NP+conj) requires special statistical fea-tures.
For example, television and movie arecommonly conjoined, but the relevant statisticsthat suggest placing brackets around the phrase?television and movie?
are not provided by theabove PMI features (i.e., this is not clear fromPMI(television, and), PMI(television, movie), norPMI(and, movie)).
Rather, we want to know if thefull phrase ?television and movie?
is common.We thus have additional NP+conj features thatconsider the PMI association across the word and:PMIand(x, y) = logp(?x and y?
)p(?x and?
)p(?and y?)
(2)When PMIand between a pair of words is high,they are likely to be the constituents of a conjunc-tion.Let NP=(w1 .
.
.
wi?1, ?and?, wi+1 .
.
.
wn) bean NP+conj.
We include the PMIand features be-tween wi?1 and all w ?
wi+1 .
.
.
wn.
In the exam-ple ?French television and movie producers,?
wewould include features PMIand(television, movie)and PMIand(television, producers).In essence, we are assuming wi?1 is the headof one of the items being conjoined, and we scorethe likelihood of each of the words to the rightof the and being the head for the other item.
Inour running example, the conjunction has narrowscope, and PMIand(television, movie) is greaterthan PMIand(television, producers), indicating toour classifier that (television and movie) is a goodbracketing.
In other examples the conjunction willjoin heads that are further apart, as in ((French TV)and (British radio)) stars, where both of the fol-lowing hold:PMIand(TV, radio) > PMIand(TV, British)PMIand(TV, radio) > PMIand(TV, stars)6.2 LexicalWe include a binary feature to indicate the pres-ence of a particular word at each position in theNP.
We learn that, for instance, the word Inc. innames tends to occur outside of brackets.6.3 ShapePrevious work on NP bracketing has used gold-standard named entity tags (Vadas and Curran,2007b) as features.
We did not want to use anygold-standard features in our experiments, how-ever NER information is helpful in separating pre-modifiers from names, i.e.
(news reporter) (Wal-ter Cronkite).As an expedient way to get both NER informa-tion and useful information from hyphenated ad-jectives, abbreviations, and other punctuation, wenormalize each string using the following regularexpressions:[A-Z]+ ?
A [a-z]+ ?
aWe use this normalized string as an indicatorfeature.
E.g.
the word ?Saudi-born?
will fire thebinary feature ?Aa-a.
?6.4 PositionWe also include the position of the proposedbracketing as a feature.
This represents the priorof a particular bracketing, regardless of the actualwords.7 Experiments7.1 Experimental DetailsWe use Vadas and Curran (2007a)?s annotations(Section 3) to create training, development andtesting data for base NPs, using standard splits ofthe Penn Treebank (Table 1).
We consider all non-trivial base NPs, i.e., those longer than two words.For training, we expand each NP in our train-ing set into independent examples correspondingto all the possible internal NP-bracketings, andrepresent these examples as feature vectors (Sec-tion 5).
Each example is positively labeled if it is890Data Set Train Dev Test ChancePTB Section 2-22 24 23Length=3 41353 1428 2498 50%Length=4 12067 445 673 20%Length=5 3930 148 236 7%Length=6 1272 34 81 2%Length>6 616 29 34 < 1%Total NPs 59238 2084 3522Table 1: Breakdown of the PTB base NPs used inour experiments.
Chance = 1/Catalan(length).Features All NPs NP+conj NP-conjAll features 95.4 89.7 95.7-N-grams 94.0 84.0 94.5-lexical 92.2 87.4 92.5-shape 94.9 89.7 95.2-position 95.3 89.7 95.6Right 72.6 58.3 73.5bracketingTable 2: Accuracy (%) of base NPs parsing; abla-tion of different feature classes.consistent with the gold-standard bracketing, oth-erwise it is a negative example.We train using LIBLINEAR, an efficient linearSupport Vector Machine (SVM).3 We use an L2-loss function, and optimize the regularization pa-rameter on the development set (reaching an opti-mum at C=1).
We converted the SVM output toprobabilities.4 Perhaps surprisingly, since SVMsare not probabilistic, performance on the devel-opment set with these SVM-derived probabilitieswas higher than using probabilities from the LIB-LINEAR logistic regression solver.At test time, we again expand the NPs and cal-culate the probability of each constituent, insert-ing the score into a chart.
We run the CYK algo-rithm to find the most probable parse of the entireNP according to the chart.
Our evaluation metricis Accuracy: the proportion of times our proposedparse of the NP exactly matches the gold standard.8 Results8.1 Base NPsOur method improves substantially over the base-line of assuming a completely right-branchingstructure, 95.4% versus 72.6% (Table 2).
The ac-curacy of the constituency classifier itself (beforethe CYK parser is used) is 96.1%.The lexical features are most important, but allfeature classes are somewhat helpful.
In particu-lar, including N-gram PMI features significantlyimproves the accuracy, from 94.0% to 95.4%.5Correctly parsing more than 19 base NPs out of 20is an exceptional level of accuracy, and provides astrong new standard on this task.
The most com-parable result is by Vadas and Curran (2007b),who achieved 93.0% accuracy on a different set ofPTB noun phrases (see footnote 2), but their clas-sifier used features based on gold-standard part-of-speech and named-entity information.Exact match is a tough metric for parsing, andthe difficulty increases as the length of the NPincreases (because there are more decisions tomake correctly).
At three word NPs, our accu-racy is 98.5%; by six word NPs, our accuracydrops to 79.0% (Figure 1).
Our method?s accu-racy decreases as the length of the NP increases,but much less rapidly than a right-bracketing orchance baseline.8.2 Base NPs with ConjunctionsN-gram PMI features help more on NP+conj thanon those that do not contain conjunctions (NP-conj) (Table 2).
N-gram PMI features are the mostimportant features for NP+conj, increasing accu-racy from 84.0% to 89.7%, a 36% relative reduc-tion in error.8.3 Effect of Thresholding N-gram dataWe now address two important related questions:1) how does our parser perform as the amountof unlabeled auxiliary data varies, and 2) whatis the effect of thresholding an N-gram corpus?The second question is of widespread relevance as3www.csie.ntu.edu.tw/?cjlin/liblinear/4Following instructions in http://www.csie.ntu.edu.tw/?cjlin/liblinear/FAQ.html5McNemar?s test, p < 0.058911101006543Accuracy(%)Length of Noun Compound (words)ProposedRight-bracketingChanceFigure 1: Accuracy (log scale) over different NPlengths, of our method, the right-bracketing base-line, and chance (1/Catalan(length)).thresholded N-gram corpora are now widely usedin NLP.
Without thresholds, web-scale N-gramdata can be unmanageable.While we cannot lower the threshold after cre-ating the N-gram corpus, we can raise it, filteringmore N-grams, and then measure the relationshipbetween threshold and performance.Threshold Unique N-grams Accuracy10 4,145,972,000 95.4%100 391,344,991 95.3%1,000 39,368,488 95.2%10,000 3,924,478 94.8%100,000 386,639 94.8%1,000,000 37,567 94.4%10,000,000 3,317 94.0%Table 3: There is no data like more data.
Accuracyimproves with the number of parameters (uniqueN-grams).We repeat the parsing experiments while in-cluding in our PMI features only N-grams witha count ?10 (the whole data set), ?100, ?1000,.
.
., ?107.
All other features (lexical, shape, posi-tion) remain unchanged.
The N-gram data almostperfectly exhibits Zipf?s power law: raising thethreshold by a factor of ten decreases the numberof unique N-grams by a factor of ten (Table 3).The improvement in accuracy scales log-linearlywith the number of unique N-grams.
From a prac-tical standpoint, we see a trade-off between stor-Corpus # of tokens ?
# of typesNEWS 3.2 B 1 3.7 BGoogle V1 1,024.9 B 40 3.4 BGoogle V2 207.4 B 10 4.1 BTable 4: N-gram data, with total number of words(tokens) in the original corpus (in billions, B), fre-quency threshold used to filter the data, ?
, and to-tal number of unique N-grams (types) remainingin the data after thresholding.age and accuracy.
There are consistent improve-ments in accuracy from lowering the thresholdand increasing the amount of auxiliary data.
If forsome application it is necessary to reduce storageby several orders of magnitude, then one can eas-ily estimate the resulting impact on performance.We repeat the thresholding experiments usingtwo other N-gram sources:NEWS: N-gram data created from a large setof news articles including the Reuters and Giga-word (Graff, 2003) corpora, not thresholded.Google V1: The original web-scale N-gramcorpus (Section 4).Details of these sources are given in Table 4.For a given number of unique N-grams, usingany of the three sources does about the same (Fig-ure 2).
It does not matter that the source corpusfor Google V1 is about five times larger than thesource corpus for Google V2, which in turn issixty-five times larger than NEWS (Table 4).
Ac-curacies increase linearly with the log of the num-ber of types in the auxiliary data set.Google V1 is the one data source for whichthe relationship between accuracy and number ofN-grams is not monotonic.
After about 100 mil-lion unique N-grams, performance starts decreas-ing.
This drop shows the need for Google V2.Since Google V1 contains duplicated web pagesand sentences, mistakes that should be rare canappear to be quite frequent.
Google V2, whichcomes from the same snapshot of the web asGoogle V1, but has only unique sentences, doesnot show this drop.We regard the results in Figure 2 as a compan-ion to Banko and Brill (2001)?s work on expo-nentially increasing the amount of labeled train-ing data.
Here we see that varying the amount of8929494.59595.5961e91e81e71e61e51e4Accuracy(%)Number of Unique N-gramsGoogle V1Google V2NEWSFigure 2: There is no data like more data.
Ac-curacy improves with the number of parameters(unique N-grams).
This trend holds across threedifferent sources of N-grams.unlabeled data can cause an equally predictableimprovement in classification performance, with-out the cost of labeling data.Suzuki and Isozaki (2008) also found a log-linear relationship between unlabeled data (up toa billion words) and performance on three NLPtasks.
We have shown that this trend continueswell beyond Gigaword-sized corpora.
Brants etal.
(2007) also found that more unlabeled data (inthe form of input to a language model) leads toimprovements in BLEU scores for machine trans-lation.Adding noun phrase parsing to the list of prob-lems for which there is a ?bigger is better?
rela-tionship between performance and unlabeled datashows the wide applicability of this principle.
Asboth the amount of text on the web and the powerof computer architecture continue to grow expo-nentially, collecting and exploiting web-scale aux-iliary data in the form of N-gram corpora shouldallow us to achieve gains in performance linear intime, without any human annotation, research, orengineering effort.9 ConclusionWe used web-scale N-grams to produce a newstandard in performance of base NP parsing:95.4%.
The web-scale N-grams substantially im-prove performance, particularly in long NPs thatinclude conjunctions.
There is no data like moredata.
Performance improves log-linearly with thenumber of parameters (unique N-grams).
One canincrease performance with larger models, e.g., in-creasing the size of the unlabeled corpora, or bydecreasing the frequency threshold.
Alternatively,one can decrease storage costs with smaller mod-els, e.g., decreasing the size of the unlabeled cor-pora, or by increasing the frequency threshold.
Ei-ther way, the log-linear relationship between accu-racy and model size makes it easy to estimate thetrade-off between performance and storage costs.AcknowledgmentsWe gratefully acknowledge the Center for Lan-guage and Speech Processing at Johns HopkinsUniversity for hosting the workshop at which thisresearch was conducted.ReferencesAtterer, M. and H. Schu?tze.
2007.
Prepositionalphrase attachment without oracles.
ComputationalLinguistics, 33(4):469?476.Banko, M. and E. Brill.
2001.
Scaling to very verylarge corpora for natural language disambiguation.In ACL.Barker, K. 1998.
A trainable bracketer for noun mod-ifiers.
In Twelfth Canadian Conference on ArtificialIntelligence (LNAI 1418).Barr, C., R. Jones, and M. Regelson.
2008.
The lin-guistic structure of English web-search queries.
InEMNLP.Bergsma, S. and Q.I.
Wang.
2007.
Learning nounphrase query segmentation.
In EMNLP-CoNLL.Brants, T. and A. Franz.
2006.
The Google Web 1T5-gram Corpus Version 1.1.
LDC2006T13.Brants, T., A.C. Popat, P. Xu, F.J. Och, and J. Dean.2007.
Large language models in machine transla-tion.
In EMNLP.Charniak, E. and M. Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminative reranking.In ACL.Church, K.W.
and P. Hanks.
1990.
Word associa-tion norms, mutual information, and lexicography.Computational Linguistics, 16(1):22?29.Church, K. and R. Patil.
1982.
Coping with syntacticambiguity or how to put the block in the box on thetable.
Computational Linguistics, 8(3-4):139?149.893Church, K.W.
1988.
A stochastic parts program andnoun phrase parser for unrestricted text.
In ANLP.Collins, M. and T. Koo.
2005.
Discriminative rerank-ing for natural language parsing.
ComputationalLinguistics, 31(1):25?70.Finkel, J.R., A. Kleeman, and C.D.
Manning.
2008.Efficient, feature-based, conditional random fieldparsing.
In ACL.Graff, D. 2003.
English Gigaword.
LDC2003T05.Guo, J., G. Xu, H. Li, and X. Cheng.
2008.
A unifiedand discriminative model for query refinement.
InSIGIR.Jones, R., B. Rey, O. Madani, and W. Greiner.
2006.Generating query substitutions.
In WWW.Kudo, T. and Y. Matsumoto.
2001.
Chunking withsupport vector machines.
In NAACL.Lapata, M. and F. Keller.
2005.
Web-based models fornatural language processing.
ACM Transactions onSpeech and Language Processing, 2(1):1?31.Lauer, M. 1995.
Corpus statistics meet the noun com-pound: some empirical results.
In ACL.Liberman, M. and R. Sproat.
1992.
The stress andstructure of modified noun phrases in English.
Lex-ical matters, pages 131?181.Lin, D., K. Church, H. Ji, S. Sekine, D. Yarowsky,S.
Bergsma, K. Patil, E. Pitler, R. Lathbury, V. Rao,K.
Dalwani, and S. Narsale.
2010.
New tools forweb-scale n-grams.
In LREC.Marcus, M.P., B. Santorini, and M.A.
Marcinkiewicz.1994.
Building a Large Annotated Corpus of En-glish: The Penn Treebank.
Computational Linguis-tics, 19(2):313?330.Marcus, M.P.
1980.
Theory of Syntactic Recogni-tion for Natural Languages.
MIT Press, Cambridge,MA, USA.Nakov, P. and M. Hearst.
2005.
Search engine statis-tics beyond the n-gram: Application to noun com-pound bracketing.
In CoNLL.Pustejovsky, J., P. Anick, and S. Bergler.
1993.
Lex-ical semantic techniques for corpus analysis.
Com-putational Linguistics, 19(2):331?358.Ramshaw, L.A. and M.P.
Marcus.
1995.
Text chunk-ing using transformation-based learning.
In 3rdACL Workshop on Very Large Corpora.Ratnaparkhi, A., S. Roukos, and R.T. Ward.
1994.A maximum entropy model for parsing.
In ThirdInternational Conference on Spoken Language Pro-cessing.Resnik, P. 1993.
Selection and information: a class-based approach to lexical relationships.
Ph.D. the-sis, University of Pennsylvania.Suzuki, J. and H. Isozaki.
2008.
Semi-supervised se-quential labeling and segmentation using giga-wordscale unlabeled data.
In ACL.Tan, B. and F. Peng.
2008.
Unsupervised querysegmentation using generative language models andWikipedia.
In WWW.Taskar, B., D. Klein, M. Collins, D. Koller, andC.
Manning.
2004.
Max-margin parsing.
InEMNLP.Vadas, D. and J.R. Curran.
2007a.
Adding nounphrase structure to the Penn Treebank.
In ACL.Vadas, D. and J.R. Curran.
2007b.
Large-scale su-pervised models for noun phrase bracketing.
In PA-CLING.Zhai, C. 1997.
Fast statistical parsing of noun phrasesfor document indexing.
In ANLP.894
