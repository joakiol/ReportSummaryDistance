OPTIMIZING THE COMPUTATIONAL LEXICALIZATION OFLARGE GRAMMARSChristian JACQUEMINInstitut de Recherche en Informatique de Nantes (IR/N)IUT de Nantes - 3, rue du MarEchal JoffreF-441M1 NANTES Cedex 01 - FRANCEa--mail : jaequemin@ irin.iut-nantas.univ-nantas.frAbstractThe computational lexicalization of agrammar is the optimization of the linksbetween lexicalized rules and lexical items inorder to improve the quality of the bottom-upfiltering during parsing.
This problem isN P-complete and untractable on largegrammars.
An approximation algorithm ispresented.
The quality of the suboptimalsolution is evaluated on real-world grammars aswell as on randomly generated ones.IntroductionLexicalized grammar formalisms and morespecifically Lexicalized Tree AdjoiningGrammars (LTAGs) give a lexical account ofphenomena which cannot be considered aspurely syntactic (Schabes et al 1990).
Aformalism is said to be lexicalized if it iscomposed of structures or rules associated witheach lexical item and operations to derive newstructures from these elementary ones.
Thechoice of the lexical anchor of a rule issupposed to be determined on purely linguisticgrounds.
This is the linguistic side oflexicalization which links to each lexical head aset of minimal and complete structures.
Butlexicalization also has a computational spectbecause parsing algorithms for lexicalizedgrammars can take advantage of lexical linksthrough a two-step strategy (Schabes and Joshi,1990).
The first step is the selection of the setof rules or elementary structures associatedwith the lexical items in the input sentence ~.
Inthe second step, the parser uses the rulesfiltered by the first step.The two kinds of anchors corresponding tothese two aspects of lexicalization can beconsidered separately :?
The linguistic anchors are used to access thegrammar, update the data, gather togetheritems with similar structures, organize thegrammar into a hierarchy...?
The computational anchors are used toselect he relevant rules during the first stepof parsing and to improve computationaland conceptual tractability of the parsingalgorithm.Unlike linguistic lexicalization, computationalanchoring concerns any of the lexical itemsfound in a rule and is only motivated by thequality of the induced filtering.
For example,the systematic linguistic anchoring of the rulesdescribing "Nmetal alloy" to their head noun"alloy" should be avoided and replaced by amore distributed lexicalization.
Then, only afew rules "Nmetal lloy" will be activated whenencountering the word "alloy" in the input.In this paper, we investigate the problem ofthe opt imizat ion of computat iona llexicalization.
We study how to choose thecomputational anchors of a lexicalizedgrammar so that the distribution of the rules onto the lexical items is the most uniform possibleThe computational anchor of a rule should not beoptional (viz included in a disjunction) to make surethat it will be encountered in any string derived fromthis rule.196with respect to rule weights.
Althoughintroduced with reference to LTAGs, thisoptimization concerns any portion of agrammar where rules include one or morepotential exical anchors such as Head DrivenPhrase Structure Grammar (Pollard and Sag,1987) or Lexicalized Context-Free Grammar(Schabes and Waters, 1993).This algorithm is currently used to goodeffect in FASTR a unification-based parser forterminology extraction from large corpora(Jacquemin, 1994).
In this framework, termsare represented by rules in a lexicalizedconstraint-based formalism.
Due to the largesize of  the grammar, the quality of  thelexicalization is a determining factor for thecomputational tractability of the application.FASTR is applied to automatic indexing onindustrial data and lays a strong emphasis onthe handling of term variations (Jacquemin andRoyaut6, 1994).The remainder of this paper is organized asfollows.
In the following part, we prove that theproblem of the Lexicalization of a Grammar isNP-complete and hence that there is no betteralgorithm known to solve it than anexponential exhaustive search.
As this solutionis untractable on large data, an approximationalgor i thm is presented which has acomputational-time complexity proportional tothe cubic size of the grammar.
In the last part,an evaluation of this algorithm on real-worldgrammars of 6,622 and 71,623 rules as well ason randomly generated ones confirms itscomputational tractability and the quality ofthe lexicalization.The Problem of theLexieal izat ion of a GrammarGiven a lexicalized grammar, this part describesthe problem of  the optimization of  thecomputational lexicalization.
The solution tothis problem is a lexicalization function(henceforth a lexicalization) which associates toeach grammar ule one of the lexical items itincludes (its lexical anchor).
A lexicalization isoptimized to our sense if it induces an optimalpreprocessing of the grammar.
Preprocessing isintended to activate the rules whose lexicalanchors are in the input and make all thepossible filtering of these rules before theproper  pars ing  a lgor i thm.
Ma in ly ,preprocessing discards the rules selectedthrough lexicalization including at least onelexical item which is not found in the input.The first step of the optimization of thelexicalization is to assign a weight o each rule.The weight is assumed to represent the cost ofthe cor respond ing  rule dur ing thepreprocessing.
For a given lexicalization, theweight of a lexical item is the sum of theweights of the rules linked to it.
The weightsare chosen so that a uniform distribution of therules on to the lexical items ensures an optimalpreprocessing.
Thus, the problem is to find ananchoring which achieves such a uniformdistribution.The weights depend on the physicalconstraints of the system.
For example, theweight is the number of nodes if the memorysize is the critical point.
In this case, a uniformdistribution ensures that the rules linked to anitem will not require more than a givenmemory space.
The weight is the number ofterminal or non-terminal  nodes if  thecomputational cost has to be minimized.Experimental measures can be performed on atest set of rules in order to determine the mostaccurate weight assignment.Two simplifying assumptions are made :?
The weight of a rule does not depend on thelexical item to which it is anchored.?
The weight of a rule does not depend on theother rules simultaneously activated.The second assumption is essential for settlinga tractable problem.
The first assumption canbe avoided at the cost of  a more complexrepresentation.
In this case, instead of having aunique weight, a rule must have as manyweights as potential lexical anchors.
Apart fromthis modification, the algorithm that will bepresented in the next part remains much thesame than in the case of a single weight.
If thefirst assumption is removed, data about thefrequency of  the items in corpora can beaccounted for.
Assigning smaller weights torules when they are anchored to rare items will197make the algorithm favor the anchoring tothese items.
Thus, due to their rareness, thecorresponding rules will be rarely selected.I l lustration Terms, compounds and moregenerally idioms require a lexicalized syntacticrepresentation such as LTAGs to account forthe syntax of these lexical entries (Abeill6 andSchabes, I989).
The grammars chosen toillustrate the problem of the optimization of thelexicalization and to evaluate the algorithmconsist of idiom rules such as 9 :9 = {from time to time, high time,high grade, high grade steel}Each rule is represented by a pair (w i, Ai) wherew i is the weight and A i the set of potentialanchors.
I f  we choose the total number ofwords in an idiom as its weight and its non-empty words as its potential anchors, 9 isrepresented by the following grammar :G 1 = {a = (4, {time}), b = (2, {high, time}),c = (2, {grade, high}),d = (3, {grade, high,steel}) }We call vocabulary, the union V of all the setsof potential anchors A i.
Here, V = {grade, high,steel, time}.
A lexicalization is a function ~.associating a lexical anchor to each rule.Given a th resho ld  O, the membershipproblem called the Lexical izat ion of  aGrammar (LG) is to find a lexicalization so thatthe weight of any lexical item in V is less thanor equal to 0.
If 0 >4 in the precedingexample, LG has a solution g :g(a) = time, ~.
(b) = ~(c) = high,;t(d) = steelIf 0 < 3, LG has no solution.Definition of the LG ProblemG = {(w i, Ai) } (wie Q+, A i finite sets)V= {Vi} =k.
)A i ;Oe 1~+(1) LG-  { (V, G, O, ~.)
l where :t : G ---> V is atotal function anchoring the rules so that(V(w, A)e G) 2((w, A))eAand (We V) ~ w < 0 }Z((w, A)) = vThe associated optimization problem is todetermine the lowest value Oop t of the threshold0 so that there exists a solution (V, G, Oop t,/q.)
toLG.
The solution of  the optimization problemfor the preceding example is 0op t= 4.Lemma LG is in NP.It is evident that checking whether a givenlexicalization is indeed a solution to LG can bedone in polynomial time.
The relation Rdefined by (2) is polynomially decidable :(2) R(V, G, O, 2.)
"-- \[if ~.
: V-~G and (We V)w < 0 then true else false\]2((w, a)) = vThe weights of the items can be computedthrough matrix products : a matrix for thegrammar and a matrix for the lexicalization.The size of any lexicalization ~ is linear in thesize of the grammar.
As (V, G, O, &)e LG if andonly if \[R(V, G, 0, ~.
)\] is true, LG is in NP.
?Theorem LG is NP-complete.Bin Packing (BP) which is NP-complete ispolynomial-t ime Karp reducible to LG.
BP(Baase, 1986) is the problem defined by (3) :(3) BP "-- { (R, {R I .
.
.
.
.
Rk}) I whereR = { r 1 .
.
.
.
.
r n } is a set of n posit iverational numbers less than or equal to 1and {R 1 .
.
.
.
.
Rk} is a partition of R (k binsin which the rjs are packed) such that(Vi~{1 .
.
.
.
.
k}) ,~  r< 1.re RiFirst, any instance of BP can be represented asan instance of LG.
Let (R, {R 1 .
.
.
.
.
Rk}) be aninstance of  BP it is transformed into theinstance (V, G, 0, &) of LG as follows :(4) V= {v I .
.
.
.
.
vk} a set of k symbols, O= 1,G = {(r v V) .
.
.
.
.
(rn, V)}and (Vie {1 .
.
.
.
.
k}) (Vje {1 .
.
.
.
.
n})~t((rj, v)) = V i ?~ rje R iFor all i~{I ..... k} and js{1  ..... n}, weconsider the assignment of rj to the bin R i ofBP as the anchoring of the rule (rj, V) to theitem v i of LG.
I f(R, {R 1 .
.
.
.
.
Rk})eBP then :198(5) (VIE{1 .
.
.
.
.
k}) 2_, r< 1rE  Ri?~ (Vie { I ..... k}) ~_~ r _ IA((r, v)) = viThus (V, G, 1,/q.)~LG.
Conversely, given asolution (V, G, 1, Z) of LG, let R i "- {rye R IZ((ry, V)) = vi} for all ie { 1 .
.
.
.
.
k}.
Clearly{R 1 .
.
.
.
.
Rk} is a partition of R because thelexicalization is a total function and thepreceding formula ensures that each bin iscorrectly loaded.
Thus (R, {R I .
.
.
.
.
Rk})EBP.
Itis also simple to verify that the transformationfrom B P to L G can be performed inpolynomial time.
\[\]The optimization of an NP-completeproblem is NP-complete (Sommerhalder andvan Westrhenen, 1988), then the optimizationversion of LG is NP-complete.An Approximation Algorithmfor L GThis part presents and evaluates an n3-timeapproximation algorithm for the LG problemwhich yields a suboptimal solution close to theoptimal one.
The first step is the 'easy'anchoring of rules including at least one rarelexical item to one of these items.
The secondstep handles the 'hard' lexicalization of theremaining rules including only common itemsfound in several other rules and for which thedecision is not straightforward.
Thediscrimination between these two kinds of itemsis made on the basis of their global weight G W(6) which is the sum of the weights of the ruleswhich are not yet anchored and which have thislemma as potential anchor.
Vx and Gx aresubsets of V and G which denote the items andthe rules not yet anchored.
The ws and 0 areassumed to be integers by multiplying them bytheir lowest common denominator if necessary.
(6) (Vw V Z) GW(v) = ~_~ w(w ,A)  e Gx ,vE  AStep 1 : 'Easy' Lexiealization of Rare ItemsThis first step of the optimization algorithm isalso the first step of the exhaustive search.
Thevalue of the minimal threshold Omi n given by(7) is computed by dividing the sum of the ruleweights by the number of lemmas (\['xl standsfor the smallest integer greater than or equal tox and \[ V;tl stands for the size of the set Vx)"(7) 0,m.
n = (w, A) E G~t W where I V~.I ~ 0lEviAll the rules which include a lemma with aglobal weight less than or equal to Orain areanchored to this lemma.
When this linking isachieved in a non-deterministic manner, Omi .
isrecomputed.
The algorithm loops on thislexicalization, starting it from scratch everytime, until Omi .
remains unchanged or until allthe rules are anchored.
The output value of 0,,i,is the minimal threshold such that LG has asolution and therefore is less than or equal to0o_ r After Step 1, either each rule is anchored /Jor all the remaining items in Va. have a globalweight strictly greater than Omin.
The algorithmis shown in Figure 1.Step 2 : 'Hard' Lexicalization of CommonItems During this step, the algorithmrepeatedly removes an item from the remainingvocabulary and yields the anchoring of thisitem.
The item with the lowest global weight ishandled first because it has the smallestcombination of anchorings and hence theprobability of making a wrong choice for thelexicalization is low.
Given an item, thecandidate rules with this item as potentialanchor are ranked according to :1 The highest priority is given to the ruleswhose set of potential anchors only includesthe current item as non-anchored item.2 The remaining candidate rules taken firstare the ones whose potential anchors havethe highest global weights (items found inseveral other non-anchored rules).The algorithm is shown in Figure 2.
Theoutput of Step 2 is the suboptimalcomputational lexicalization Z of the wholegrammar and the associated threshold 0s,,boprBoth steps can be optimized.
Uselesscomputation is avoided by watching the capital199of weight C defined by (8) with 0 - 0m/~ duringStep 1 and 0 - Osubopt during Step 2 :(8) c=o.lvxl- w(w, A) ~ GxC corresponds to the weight which can be lostby giving a weight W(m) which is strictly lessthan the current threshold 0.
Every time ananchoring to a unit m is completed, C isreduced from 0- W(t~).
If C becomes negativein either of both steps, the algorithm will fail tomake the lexicalization of the grammar andmust be started again from Step 1 with a highervalue for 0.InputOutputSteplV,G0m/,,, V;t, G;t, 2 : (G - Ga) ---> (V -V  a)I -\[  -'Gw Omi,, ~- (w,A)~ IVlrepeatG;t~G ; Vx<--- V;for each ve V such as GW(v)<Omi,, dofor each (w, A)~ G such as wAand ~((w, A)) not yet defined do~((w, A)) ~ v ;Gx~-Gx-{(w,A)}  ;update GW(v) ;endv~ ~ v~-  {v} ;endif( ( O'mi n<-- 0,,~and ( (Vve Va) GW(v)  > Omin ) )or G~ = 0 )then exit repeat ;Omi n~-- O'mi n ;until( false ) ;Figure 1: Step 1 of the approximation algorithm.InputOutputStep2O~, V, G, V,~, G~,~.
: (G-GO ~ (V-V~O~.~p t, A. : G ---> VO,.~pt ~ Omi,, ;repeat;; anchoring the rules with only m as;; free potential anchor (t~ e V x with;; the lowest global weight)~J~--vi;GaI ~- { (w,A)~G~tlAnV~= {t~} };if ( ~ w < 0~bo~, )(w, A) ~ Go, 1then 0m/n ~ Omin + 1 ; goto Stepl ;for each (w, A)~ G~, 1 doX((w, A)) ~- ~ ;G;t~--G~t-{ (w,A) } ;endGt~,2 ~-- {(w, A)eG;~ ;AnV z D {t~} ;W(~) ~ ~ w ;:t((w, A)) = ~Y;; ranking 2 G~, 2 and anchoringfo r ( i  ~ 1; i_< \[GruEl; i~- i+ 1 )do(w, A) <--- r -l(i) ;; t lh ranked by rif( W( t~) + W > Omin )then exit for ;w(~)  ~ w(~)  + w ;~((w, A )) ~ ~ ;G~ ~ G~t-{(w, A)} ;endv~-  v~-  {~} ;until ( G~t = 0 ) ;Figure 2: Step 2 of the approximation algorithm.2 The ranking function r: Gt~ 2 --> { 1 .... \[ G~2 \[ } issuch that r((w, A)) > r((w', A3?
m in  ~W(v')  v ~ A ~n~v~- t~ W(v) > v' E A' ,~ V~-200Example 3 The algorithm has been applied toa test grammar G2 obtained from 41 terms with11 potential anchors.
The algorithm fails inmaking the lexicalization of G 2 with theminimal threshold Omin = 12, but achieves itwith Os,,bopt = 13.
This value of Os,,bop t Can becompared with the optimal one by running theexhaustive search.
There are 232 (= 4 109)possible lexicalizations among which 35,336are optimal ones with a threshold of 13.
Thisresult shows that the approximation algorithmbrings forth one of the optimal solutions whichonly represent a proportion of 8 10 -6 of thepossible lexicalizations.
In this case the optimaland the suboptimal threshold coincide.Time-Complexity of the ApproximationAlgorithm A grammar G on a vocabulary Vcan be represented by a \ ]G lx  \]V I-matrix ofBoolean values for the set of potential anchorsand a lx I G l-matrix for the weights.
In orderto evaluate the complexity of the algorithms asa function of the size of the grammar, weassume that I V I and I GI are of the same orderof magnitude n. Step 1 of the algorithmcorresponds to products and sums on thepreceding matrixes and takes O(n 3) time.
Theworst-case time-complexity for Step 2 of thealgorithm is also O(n 3) when using a naiveO(n 2) algorithm to sort the items and the rulesby decreasing priority.
In all, the time requiredby the approximation algorithm is proportionalto the cubic size of the grammar.This order of magnitude nsures that thealgorithm can be applied to large real-worldgrammars uch as terminological grammars.On a Spare 2, the lexicalization of aterminological grammar composed of 6,622rules and 3,256 words requires 3 seconds (realtime) and the lexicalization of a very largeterminological grammar of 71,623 rules and38,536 single words takes 196 seconds.
Thetwo grammars used for these experiment weregenerated from two lists of terms provided bythe documentation center INIST/CNRS.3 The exhausitve grammar nd more details about hisexample and the computations of the followingsection are in (Jacquemin, 1991).Evaluation of theApproximation AlgorithmBench Marks on Artificial Grammars Inorder to check the quality of the lexicalizationon different kinds of grammars, the algorithmhas been tested on eight randomly generatedgrammars of 4,000 rules having from 2 to 10potential anchors (Table 1).
The lexicon of thefirst four grammars i 40 times smaller than thegrammar while the lexicon of the last four onesis 4 times smaller than the grammar (thisproportion is close to the one of the real-worldgrammar studied in the next subsection).
Theeight grammars differ in their distribution ofthe items on to the rules.
The uniformdistribution corresponds to a uniform randomchoice of the items which build the set ofpotential anchors while the Gaussian onecorresponds to a choice taking more frequentlysome items.
The higher the parameter s, theflatter the Gaussian distribution.The last two columns of Table 1 give theminimal threshold Omi n after Step 1 and thesuboptimal threshold Osul, op , found by theapproximation algorithm.
As mentioned whenpresenting Step 1, the optimal threshold Ooe t isnecessarily greater than or equal to Omin afterStep 1.
Table 1 reports that the suboptimalthreshold Os,,t, opt is not over 2 units greater thanOmin after Step 1.
The suboptimal thresholdyielded by the approximation algorithm onthese examples has a high quality because it isat worst 2 units greater than the optimal one.A Comparison with Linguistic Lexicalizationon a Real-World Grammar This evaluationconsists in applying the algorithm to a naturallanguage grammar composed of 6,622 rules(terms from the domain of metallurgyprovided by INIST/CNRS) and a lexicon of3,256 items.
Figure 3 depicts the distribution ofthe weights with the natural linguisticlexicalization.
The frequent head words such asal loy are heavily loaded because of thenumerous terms in N-a l loy  with N being aname of metal.
Conversely, in Figure 4 thedistribution of the weights from theapproximation algorithm is much more201uniform.
The maximal weight of an item is 241with the linguistic lexicalization while it is only34 with the optimized lexicalization.
Thethreshold after Step 1 being 34, the suboptimalthreshold yielded by the approximationalgorithm is equal to the optimal one.Lexicon size Distribution of the On~ n On~n Osuboptitems on the rules before Step 1 after Step I suboptimal threshold100 uniform 143 143 143100 Gaussian (s = 30) 141 143 144100 Gaussian (s = 20) 141 260 261100 Gaussian (s = 10) 141 466 4681,000 uniform 15 15 161,000 Gaussian (s = 30) 14 117 1181,000 Gaussian (s = 20) 15 237 2381,000 Gaussian (s = 10) 14 466 467Table 1: Bench marks of the approximation algorithm on eight randomly generated grammars.Number ofitems(log scale)300010001001015 30Weight45 60 75 90 105 120 135 150 165 180 195 210 225 240Figure 3: Distribution of the weights of the lexical items with the lexicalization on head words.Number ofitems(log scale)100010010,,,, .... ,,,,,,,,,,1111 234 5678 910 12 14 16 18 20 22 24 26 28 30 32 34 36WeightFigure 4: Distribution of the weights of the lexical items with the optimized lexicalization.202ConclusionAs mentioned in the introduction, theimprovement of the lexicalization through anoptimization algorithm is currently used inFASTR a parser for terminological extractionthrough NLP techniques where terms arerepresented by lexicalized rules.
In thisframework as in top-down parsing with LTAGs(Schabes and Joshi, 1990), the first phase ofparsing is a filtering of the rules with theiranchors in the input sentence.
An unbalanceddistribution of the rules on to the lexical itemshas the major computational drawback ofselecting an excessive number of rules whenthe input sentence includes a common headword such as "'alloy" (127 rules have "alloy"as head).
The use of the optimizedlexicalization allows us to filter 57% of therules selected by the linguistic lexicalization.This reduction is comparable to the filteringinduced by linguistic lexicalization which isaround 85% (Schabes and Joshi, 1990).Correlatively the parsing speed is multiplied by2.6 confirming the computational saving of theoptimization reported in this study.There are many directions in which thiswork could be refined and extended.
Inparticular, an optimization of this optimizationcould be achieved by testing different weightassignments in correlation with the parsinga lgor i thm.
Thus,  the computat iona llexical izat ion would fasten both thepreprocessing and the parsing algorithm.AcknowledgmentsI would like to thank Alain Colmerauer for hisvaluable comments and a long discussion on adraft version of my PhD dissertation.
I alsogratefully acknowledge Chantal Enguehardand two anonymous reviewers for their remarkson earlier drafts.
The experiments on industrialdata were done with term lists from thedocumentation center INIST/CNRS.REFERENCESAbeill6, Anne, and Yves Schabes.
1989.
ParsingIdioms in Tree Adjoining Grammars.
InProceedings, 4 th Conference of theEuropean Chapter of the Association forComputational Linguistics (EACL'89),Manchester, UK.Baase, Sara.
1978.
Computer Algorithms.Addison Wesley, Reading, MA.Jacquemin, Christian.
1991.
Transformationsdes noms composds.
PhD Thesis inComputer Science, Universit6 of Paris 7.Unpublished.Jacquemin, Christian.
1994.
FASTR : Aunification grammar and a parser forterminology extraction from large corpora.In Proceedings, IA-94, Paris, EC2, June1994.Jacquemin, Christian and Jean Royaut6.
1994.Retrieving terms and their variants in alexicalized unification-based framework.
InProceedings, 17 th Annual InternationalACM SIGIR Conference (SIGIR'94), Dublin,July 1994.Pollard, Carl and Ivan Sag.
1987.
Information-Based Syntax and Semantics.
Vol 1:Fundamentals.
CSLI, Stanford, CA.Schabes, Yves, Anne Abeill6, and Aravind K.Joshi.
1988.
Parsing strategies with'lexicalized' grammars: Application to treeadjoining grammar.
In Proceedings, 12 thInternational Conference on ComputationalLinguistics (COLING'88), Budapest,Hungary.Schabes, Yves and Aravind K. Joshi.
1990.Parsing strategies with ' lexical ized'grammars: Application to tree adjoininggrammar.
In Masaru Tomita, editor, CurrentIssues in Parsing Technologies.
KluwerAcademic Publishers, Dordrecht.Schabes, Yves and Richard C. Waters.
1993.Lexicalized Context-Free Grammars.
InProceedings, 31 st Meeting of theAssociation for Computational Linguistics(ACL'93), Columbus, Ohio.Sommerhalder, Rudolph and S. Christian vanWestrhenen.
1988.
The Theory ofComputability: Programs, Machines,Effectiveness and Feasibility.
Addison-Wesley, Reading, MA.203
