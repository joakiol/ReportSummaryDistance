Detecting Shifts in News Stories for Paragraph ExtractionFumiyo Fukumoto Yoshimi SuzukiDepartment of Computer Science and Media Engineering,Yamanashi University4-3-11, Takeda, Kofu, 400-8511, Japan{fukumoto@skye.esb, ysuzuki@alps1.esi}.yamanashi.ac.jpAbstractFor multi-document summarization where docu-ments are collected over an extended period of time,the subject in a document changes over time.
Thispaper focuses on subject shift and presents a methodfor extracting key paragraphs from documents thatdiscuss the same event.
Our extraction method usesthe results of event tracking which starts from a fewsample documents and finds all subsequent docu-ments that discuss the same event.
The method wastested on the TDT1 corpus, and the result shows theeffectiveness of the method.1 IntroductionMulti-document summarization of news stories dif-fers from single document in that it is importantto identify differences and similarities across doc-uments.
This can be interpreted as the questionof how to identify an event and a subject in doc-uments.
According to the TDT project, an event issomething that occurs at a specific place and time as-sociated with some specific actions, and it becomesthe background among documents.
A subject, onthe other hand, refers to a theme of the documentitself.
Another important factor, which is typical ina stream of news, is recognizing and handling sub-ject shift.
The extracted paragraphs based on anevent and a subject may include the main pointsof each document and the background among docu-ments.
However, when they are strung together, theresulting summary still contains much overlappinginformation.This paper focuses on subject shift and presentsa method for extracting key paragraphs from docu-ments that discuss the same event.
We use the re-sults of our tracking technique which automaticallydetects subject shift, and produces the optimal win-dow size in the training data so as to include only thedata which are sufficiently related to the current sub-ject.
The idea behind this is that, of two documentsfrom the target event which are close in chronologicalorder, the latter discusses (i) the same subject as anearlier one, or (ii) a new subject related to the tar-get event.
This is particularly well illustrated by theKobe Japan quake event in the TDT1 data.
The firstdocument says that a severe earthquake shook thecity of Kobe.
It continues until the 5th document.The 6th through 17th documents report damage, lo-cation and nature of quake.
The 18th document, onthe other hand, states that the Osaka area sufferedmuch less damage than Kobe.
The subject of thedocument is different from the earlier ones, while allof these are related to the Kobe Japan quake event.We use the leave-one-out estimator of Support Vec-tor Machines(SVMs)(Vapnik, 1995) to make a cleardistinction between (i) and (ii) and thus estimate theoptimal window size in the training data.
For the re-sults of tracking where documents are divided intoseveral sets, each of which covers a different subjectrelated to the same event, we apply SVMs again andinduce classifiers.
Using these classifiers, we extractkey paragraphs.The next section explains why we need to detectsubject shift by providing notions of an event, asubject class and a subject which are properties thatidentify key paragraphs.
After describing SVMs, wepresent our system.
Finally, we report some exper-iments using the TDT1 and end with a very briefsummary of existing techniques.2 An Event, A Subject Class and ASubjectOur hypothesis about key paragraphs in multipledocuments related to the target event is that theyinclude words related to the subject of a document,a subject class among documents, and the targetevent.
We call these words subject, subject classand event words.
The notion of a subject wordrefers to the theme of the document itself, i.e., some-thing a writer wishes to express, and it appearsacross paragraphs, but does not appear in other doc-uments(Luhn, 1958).
A subject class word differen-tiates it from a specific subject, i.e.
it is a broaderclass of subjects, but narrower than an event.
It ap-pears across documents, and these documents dis-cuss related subjects.
An event word, on the otherhand, is something that occurs at a specific placeand time associated with some specific actions, andit appears across documents about the target event.Let us take a look at the following three documentsconcerning the Kobe Japan quake from the TDT1.1.
Emergency work continues after earthquake in Japan1-1.
Casualties are mounting in [Japan], where a strong[earthquake] eight hours ago struck [Kobe].
Up to400 {people} related {deaths} are confirmed, thou-sands of {injuries}, and rescue crews are searching?
?
?
?
?
?
?
?
?
?
?
?
?
?
?2.
Quake Collapses Buildings in Central Japan2-1.
At least two {people} died and dozens {injuries}when a powerful [earthquake] rolled through central[Japan] Tuesday morning, collapsing buildings andsetting off fires in the cities of [Kobe] and Osaka.2-2.
The [Japan] Meteorological Agency said the[earthquake], which measured 7.2 on the open-endedRichter scale, rumbled across Honshu Island fromthe Pacific Ocean to the [Japan] Sea.2-3.
The worst hit areas were the port city of [Kobe]and the nearby island of Awajishima where inboth places dozens of fires broke out and up to 50buildings, including several apartment blocks,?
?
?
?
?
?
?
?
?
?
?
?
?
?
?3.
US forces to fly blankets to Japan quake survivors3-1.
United States forces based in [Kobe] [Japan] will takeblankets to help [earthquake] survivors Thursday, inthe U.S. military?s first disaster relief operation in[Japan] since it set up bases here.3-2.
A military transporter was scheduled to take off inthe afternoon from Yokota air base on the outskirtsof Tokyo and fly to Osaka with 37,000 blankets.3-3.
Following the [earthquake] Tuesday, President Clin-ton offered the assistance of U.S. military forces in[Japan], and Washington provided the Japanese?
?
?
?
?
?
?
?
?
?
?
?
?
?
?Figure 1: Documents from the TDT1The underlined words in Figure 1 denote a subjectword in each document.
Words marked with ?{}?
and?[]?
refer to a subject class word and an event word,respectively.
Words such as ?Kobe?
and ?Japan?
areassociated with an event, since all of these docu-ments concern the Kobe Japan quake.
The firstdocument says that emergency work continues af-ter the earthquake in Japan.
Underlined words suchas ?rescue?
and ?crews?
denote the subject of the doc-ument.
The second document states that the quakecollapsed buildings in central Japan.
These two doc-uments mention the same thing: A powerful earth-quake rolled through central Japan, and many peo-ple were injured.
Therefore, words such as ?people?and ?injuries?
which appear in both documents aresubject class words, and these documents are classi-fied into the same set.
If we can determine that thesedocuments discuss related subjects, we can eliminateredundancy between them.
The third document, onthe other hand, states that the US military will flyblankets to Japan quake survivors.
The subject ofthe document is different from the earlier ones, i.e.,the subject has shifted.Though it is hard to make a clear distinction be-tween a subject and a subject class, it is easier tofind properties to determine whether the later docu-ment discusses the same subject as an earlier one ornot.
Our method exploits this feature of documents.3 SVMsWe use a supervised learning technique,SVMs(Vapnik, 1995), in the tracking and paragraphextraction task.
SVMs are defined over a vectorspace where the problem is to find a decision surfacethat ?best?
separates a set of positive examplesfrom a set of negative examples by introducingthe maximum ?margin?
between two sets.
Figure2 illustrates a simple problem that is linearlyseparable.MarginwPositive examplesNegative examplesFigure 2: The decision surface of a linear SVMSolid line denotes a decision surface, and two dashedlines refer to the boundaries.
The extra circlesare called support vectors, and their removal wouldchange the decision surface.
Precisely, the decisionsurface for linearly separable space is a hyperplanewhich can be written as w?x+b = 0, where x is an ar-bitrary data point(x?Rn) and w and b are learnedfrom a training set.
In the linearly separable casemaximizing the margin can be expressed as an opti-mization problem:Minimize : ?
?li=1?i + 12?li,j=1?i?jyiyjxi ?
xj (1)s.t :?li=1?iyi = 0 ?i : ?i ?
0w =?li=1?iyixi (2)where xi = (xi1,?
?
?,xin) is the i-th training exam-ple and yi is a label corresponding the i-th trainingexample.
In formula (2), each element of w, wk (1?
k ?
n) corresponds to each word in the trainingexamples, and the larger value of wk =?li?iyixikis, the more the word wk features positive examples.We use an upper bound value, E?loo of the leave-one-out error of SVMs to estimate the optimal win-dow size in the training data.
E ?loo can estimatethe performance of a classifier.
It is based on theidea of leave-one-out technique: The first example isremoved from l training examples.
The resulting ex-ample is used for training, and a classifier is induced.The classifier is tested on the held out example.
Theprocess is repeated for all training examples.
Thenumber of errors divided by l, Eloo, is the leave-one-out estimate of the generalization error.
E?loouses an upper bound on Eloo instead of calculatingthem, which is computationally very expensive.
Re-call that the removal of support vectors change thedecision surface.
Thus the worst happens when ev-ery support vector will become an error.
Let l bethe number of training examples of a set S, and mbe the number of support vectors.
E?loo(S) is definedas follows:Eloo(S) ?
E?loo(S) =ml(3)4 System Design4.1 Tracking by Window AdjustmentLike much previous research, our hypotheses regard-ing event tracking is that exploiting time will lead toimproved data adjustment because documents closertogether in the stream are more likely to discuss re-lated subject than documents further apart.
Let x1,?
?
?, xp be positive training documents, i.e., being thetarget event, which are in chronological order.
Letalso y1, ?
?
?, yq be negative training documents.
Thealgorithm can be summarized as follows:1.
Scoring negative training documentsIn the TDT tracking task, the number of labelledpositive training documents is small (at most 16documents) compared to the negative training doc-uments.
Therefore, the choice of good training datais an important issue to produce optimal results.
Wefirst represent each document as a vector in an n di-mensional space, where n is the number of words inthe collection.
The cosine of the angle between twovectors, xi and yj is shown in (4).cos(xi, yj) =?nk=1xik ?
yjk??nk=1x2ik??
?nk=1y2jk(4)where xik and yjk are the term frequency of word kin the document xi and yj , respectively.
We com-pute a relevance score for each negative training doc-ument by the cosine of the angle between a vector ofthe center of gravity on positive training documentsand a vector of the negative training document, i.e.,cos(g, yj) (1 ?
j ?
q), where yj is the j-th negativetraining document, and g is defined as follows:g = (g1, ?
?
?
, gn) = (1pp?i=1xi1, ?
?
?
,1pp?i=1xin) (5)xij (1 ?
j ?
n) is the term frequency of word jin the positive document xi.
The negative trainingdocuments are sorted in the descending order of theirrelevance scores: y1, ?
?
?, yq?1 and yq .2.
Adjusting window sizeWe estimate that the most recent positive trainingdocument, xp discusses either (i) the same subjectas the previous positive one, or (ii) a new subject.To do this, we use the value of E?loo.
Let y1, ?
?
?, yrbe negative training documents whose cosine simi-larity values are the top r among q negative trainingdocuments.
Let alo Set1 be a set consisting of x1,xp, y1, ?
?
?, yr , and Set2 be a set which consists ofxp?1, xp, y1, ?
?
?, yr.
We compute E?loo on sets Set1and Set2.
If the value of E ?loo on Set2 is smallerthan that of Set1, this means that xp has the samesubject as the previous document xp?1, since a clas-sifier which is induced by training Set2 is estimatedto generate a smaller error rate than that of Set1.In this case, we need to find the optimal window sizeso as to include only the positive documents whichare sufficiently related to the subject.
The flow ofthe algorithm is shown in Figure 3.beginfor k = 1 to p-3num = ?,Let Seta = {x1, xp, ?
?
?
, xp?k, y1, ?
?
?
, yr?1, yr}.Setb = {x1, x(p?1), ?
?
?
, x(p?1)?k, y1, ?
?
?
, yr?1, yr}if E?loo(Seta) < E?loo(Setb)then num = k + 2 exit loopend ifend forif num = ?then num = pend ifendFigure 3: Flow of window adjustmentOn the other hand, if the value of E?loo of Set2 islarger than that of Set1 , xp is regarded to discussa new subject.
We use all previously seen positivedocuments for training as a default strategy.3.
TrackingLet num be the number of adjusted positive train-ing documents.
The top num negative documentsare extracted from q negative documents and mergedinto num positive documents.
The new set is trainedby SVMs, and a classifier is induced.
Recall thatE?loo is computationally less expensive.
However,they are sometimes too tight for the small size oftraining data.
This causes a high F/A rate whichis signaled by the ratio of the documents that werejudged as negative but were evaluated as positive.We then use a simple measure for the test documentwhich is determined to be positive by a classifier.For each training document, we compute the cosinebetween the test and the training document vectors.If the cosine between the test and the negative doc-uments is largest, the test document is judged to benegative.
Otherwise, it is truly positive and trackingis terminated.
The procedure 1, 2 and 3 is repeateduntil the last test document is judged.4.2 Paragraph ExtractionOur window adjustment algorithm is applied eachtime the document discusses the target event.Therefore, some documents are assigned to morethan one set of documents.
We thus eliminate somesets which completely overlap each other, and applyparagraph extraction to the result.
Our hypothesisabout key paragraphs is that they include subject,subject class, and event words.
Let xp be a para-graph in the document x and x\1 be the resultingdocument with xp removed.
Let alo l be the totalnumber of documents in a set where each documentdiscusses subjects related to x.
If xp includes subjectwords, xp is related to x\1 rather than the other l-1documents, since subject words appear across para-graphs in x\1 rather than the other l?1 documents.We apply SVMs to the training data, which consistsof l documents, and induce a classifier sbj(xp), whichidentifies whether xp is related to x\1 or not.sbj(xp) ={1 if xp is assigned to x\10 elseWe note that SVMs are basically introduced for solv-ing binary classification, while our paragraph ex-traction is a multi-class classification problem, i.e.,l classes.
We use the pairwise technique for usingSVMs with multi-class data(Weston and C.Watkins,1998), and assign xp to one of the l documents.
In asimilar way, we apply SVMs to the other two train-ing data and induce classifiers: sbj class(xp) andevent(xp).sbj class(xp) ={1 if xp is assigned to sbj classx\10 elseevent(xp) ={1 if xp is assigned to eventx\10 elsesbj class(xp) refers to a classifier which identifieswhether or not xp is assigned to the set sbj classx\1including x\1.
It is induced by training datawhich consists of m different sets including the setsbj classx\1 , each of which covers a different subjectrelated to the target event.
The classifier event(xp)is induced by training data which consists of two dif-ferent sets: one is a set of all documents includingx\1, and concerning the target event.
The other isa set of documents which are not the target event.We extract paragraphs for which (6) holds.sbj(xp) = 1 & sbj class(xp) = 1 & event(xp) = 1 (6)5 ExperimentsWe used the TDT1 corpus which comprises a setof different sources, Reuters(7,965 documents) andCNN(7,898 documents)(Allan et al, 1998a).
A setof 25 target events were defined.
Each document islabeled according to whether or not the documentdiscusses the target event.
All 15,863 documentswere tagged by a part-of-speech tagger(Brill, 1992)and stemmed using WordNet information(Fellbaum,1998).
We extracted all nouns in the documents.5.1 Tracking TaskTable 1 summarizes the results which were obtainedusing the standard TDT evaluation measure1.Table 1: Tracking resultsNt Miss F/A Prec F11 31% 0.16% 70% 0.682 27% 0.16% 79% 0.784 24% 0.09% 87% 0.788 23% 0.09% 87% 0.7916 22% 0.09% 86% 0.79?Nt?
denotes the number of initial positive trainingdocuments where Nt takes on values 1, 2, 4, 8 and 16.When Nt takes on value 1, we use the document dand one negative training document y1 for training.Here, y1 is a vector whose cosine value of d and y1is the largest among the other negative documents.The test set is always the collection minus the Nt =16 documents.
?Miss?
denotes Miss rate, which is theratio of the documents that were judged as Yes butwere not evaluated as Yes.
?F/A?
shows false alarmrate, which is the ratio of the documents judged asNo but were evaluated as Yes.
?Prec?
stands for pre-cision, which is the ratio of correct assignments bythe system divided by the total number of the sys-tem?s assignments.
?F1?
is a measure that balancesrecall and precision, where recall denotes the ratioof correct assignments by the system divided by thetotal number of correct assignments.
Table 1 showsthat there is no significant difference among Nt val-ues except for 1, since F1 ranges from 0.78 to 0.79.This shows that the method works well even for asmall number of initial positive training documents.Furthermore, the results are comparable to the ex-isting event tracking techniques, since the F1, Missand F/A score by CMU were 0.66, 29 and 0.40, andthose of UMass were 0.62, 39 and 0.27, respectively,when Nt is 4(Allan et al, 1998b).The contribution of the adaptive window algo-rithm is best explained by looking at the windowsizes it estimates.
Table 2 illustrates the sample re-sult of tracking for ?Kobe Japan Quake?
event on theNt = 16.
This event has many documents, each ofthese discusses a new subject related to the targetevent.
The result shows the first 10 documents inchronological order which are evaluated as positive.Columns 1-3 in Table 2 denote id number, dates,and title of the document, respectively.
?id=1?, forexample, denotes the first document which is eval-uated as positive.
Columns 4 and 5 stand for theresult of our method, and the majority of three hu-man judges, respectively.
They take on three values:?Yes?
denotes that the document discusses the samesubject as an earlier one, ?New?
indicates that thedocument discusses a new subject, and ?No?, thatthe document is not a positive document.
We can1http://www.nist.gov/speech/tests/tdt/index.htmTable 2: The adaptive window size in Event 15, ?Kobe Japan Quake?id date title shifts adjusted window sizesystem actual recall precision F11 01/17/95 Kobe Residents Unable to Commence Rescue Operations New New 100% 100% 1.002 01/17/95 Emergency Efforts Continue After Quake in Japan Yes Yes 100% 100% 1.003 01/17/95 Japan Helpline Worker Discusses Emergency Efforts Yes New 100% 5% 0.104 01/17/95 U.S.
Businessman Describes Japan Earthquake Yes Yes 100% 80% 0.895 01/17/95 Osaka, Japan, Withstands Earthquake Better Than Others Yes New 100% 5% 0.096 01/17/95 President Clinton Drums Up Support in Humanitarian Trip No New 100% 5% 0.097 01/17/95 Engineer Examines Causes of Damage in Japan Quake Yes New 100% 50% 0.678 01/18/95 Mike Chinoy Updates Japan?s Earthquake Recovery Efforts Yes Yes 100% 100% 1.009 01/18/95 Smoke Hangs in a Pall Over Quake-, Fire-Ravaged Kobe New New 100% 4% 0.0810 01/18/95 Japanese Wonder If Their Cities Are Really ?Quakeproof?
New New 100% 4% 0.07see that the method correctly recognizes a test doc-ument as discussing an earlier subject or a new one,since the result of our method(?system?)
and humanjudges(?actual?)
coincide except for ?id=5, 6 and 7?.Columns 6-8 stand for the accuracy of the ad-justed window size.
Recall denotes the numberof documents selected by both the system and hu-man judges divided by the total number of doc-uments selected by human judges, and precisionshows the number of documents selected by boththe system and human judges divided by the to-tal number of documents selected by the system.When the method correctly recognizes a test doc-ument as discussing an earlier subject(?system = ac-tual = Yes?
), our algorithm selects documents whichare sufficiently related to the current subject, sincethe total average of F1 was 0.82.
We note that theratio of precision in ?system = New?
is low.
Thisis because we use a default strategy, i.e., we useall previously seen positive documents for trainingwhen the most recent training document is judgedto discuss a new subject.5.2 Paragraph ExtractionWe used 15 out of 25 events which have more than 16positive documents in the experiment.
Table 3 de-notes the number of documents and paragraphs ineach event.
?Avg.?
in ?doc?
shows the average num-ber of documents per event, and ?Avg.?
in ?para?denotes the average number of paragraphs per doc-ument.
The maximum number of paragraphs perdocument was 100.Table 4 shows the result of paragraph extraction.?CNN?
refers to the results using the CNN corpusas both training and test data.
?Reuters?
denotesthe results using the Reuters corpus.
?Total?
standsfor the results using both corpora.
?Tracking result?refers to the F1 score obtained by using tracking re-sults.
?Perfect analysis?
stands for the F1 achievedusing the perfect (post-edited) output of the track-ing method, i.e., the errors by both tracking anddetecting shifts were corrected.
Precisely, the docu-ments judged as Yes but were not evaluated as YesTable 3: DataEvent CNN Reutersdoc para doc para3(Carter in Bosnia) 26 314 8 375(Clinic Murders (Salvi)) 36 416 5 346(Comet into Jupiter) 41 539 4 238(Death of Kim Jong Il) 28 337 39 3539(DNA in OJ trial) 108 1,407 6 7511(Hall?s copter (N. Korea)) 77 875 22 17012(Humble, TX, flooding) 22 243 0 015(Kobe Japan quake) 72 782 12 6416(Lost in Iraq) 34 395 10 7817(NYC Subway bombing) 22 374 2 218(OK-City bombing) 214 3,209 59 43921(Serbians down F-16) 50 572 15 13522(Serbs violate Bihac) 56 669 35 34924(USAir 427 crash) 32 435 7 9825(WTC Bombing trial) 18 132 4 54Avg.
55.4 12.7 15.2 9.7were eliminated, and the documents judged as Nobut were evaluated as Yes were added.
Further,the documents were divided by a human into sev-eral sets, each of which covers a different subjectrelated to the same event.
The evaluation is madeby three humans.
The classification is determinedto be correct if the majority of three human judgesagrees.
Table 4 shows that the average F1 of ?Track-ing results?
(0.68) in ?Total?
was 0.06 lower than thatof ?Perfect analysis?(0.74).
Overall, the result us-ing ?CNN?
was better than that of ?Reuters?.
Onereason behind this lies in the difference between thetwo corpora: CNN consists of a larger number ofwords per paragraph than Reuters.
This causes ahigh recall rate, since a paragraph which consists ofa large number of words is more likely to includeevent, subject-class, and subject words than a para-graph containing a small number of words.Recall that in SVMs each value of word wk iscalculated using formula (2), and the larger valueof wk is, the more the word wk features positiveexamples.
Table 5 illustrates sample words whichTable 4: Performance of paragraph extractionNtTracking results Perfect analysisCNN Reuters Total CNN Reuters Total1 0.70 0.56 0.622 0.75 0.60 0.674 0.76 0.61 0.70 0.78 0.62 0.748 0.76 0.62 0.7016 0.77 0.62 0.72Avg.
0.85 0.60 0.68have the highest weighted value calculated using for-mula (2).
Each classifier, sbj(xp), sbj class(xp),and event(xp) is the result using both corpora.
Theevent is the Kobe Japan quake, and the documentwhich includes xp states that the death toll hasrisen to over 800 in the Kobe-Osaka earthquake,and officials are concentrating on getting peopleout.
?Words?
denote words which have the highestweighted value in each classifier and they are used todetermine whether xp is a key paragraph or not.
Weassume these words are subject, subject class andevent words, while some words such as ?earthquake?and ?activity?
appear in more than one classifier.Table 5: Sample words in the Kobe Japan quakeclassifier wordssbj(xp) earthquake activity Japan seismologistnews conference living prime minister Mu-rayama crew Bill Dormansbj class(xp) city something floor quake Tokyo after-shock activity street injury fire seismolo-gist police people building cryevent(xp) Kobe magnitude survivor earthquake col-lapse death fire damage aftershock Kyototoll quake magnitude emergency Osaka-Kobe Japan OsakaFigure 4: F1 v.s.
the number of documentsFigure 4 illustrates how the number of documentsinfluences extraction accuracy.
The event is the US-Air 427 crash, and F1 is 0.68, which is lower thanthe average F1 of all events(0.79).
The result is whenNt is 16.
?P ana of tracking?
refers to the result us-ing the post-edited output of the tracking, i.e., onlythe errors of tracking were corrected, while ?Perfectanalysis?
refers to the result using the output: theerrors by both tracking and detecting shifts werecorrected.
Figure 4 shows that our method doesnot depend on the number of documents, since theperformance does not monotonically decrease whenthe number of documents increases.
Figure 4 alsoshows that there is no significant difference between?P ana of tracking?
and ?Perfect analysis?
comparedto the difference between ?Tracking results?
and ?Per-fect analysis?.
This indicates that (i) subject shiftsare correctly detected, and (ii) the performance ofour paragraph extraction explicitly depends on thetracking results.We note the contribution of detecting shifts forparagraph extraction.
Figures 5 and 6 illustrate therecall and precision with two methods: with andwithout detecting shift.
In the method without de-tecting shift, we use the ?full memory?
approach fortracking, i.e., SVMs generate its classification modelfrom all previously seen documents.
For the result oftracking, we extract paragraphs for which sbj(xp) =1 and sbj class(xp) = 1 hold.
We can see from bothFigure 5 and Figure 6 that the method that detectsshifts outperformed the method without detectingshifts in all Nt values.
More surprisingly, Figure 6shows that the precision scores in all Nt values usingthe tracking results with detecting shift were higherthan that of ?P ana?
without detecting shift.
Fur-ther, the difference in precision between two meth-ods is larger than that of recall.
This demonstratesthat it is necessary to detect subject shifts and thusto identify subject class words for paragraph extrac-tion, since the system without detecting shift ex-tracts many documents, which yields redundancy.Figure 5: Recall with and without detecting shiftFigure 6: Precision with and without detecting shift6 Related WorkMost of the work on summarization task by para-graph or sentence extraction has applied statisticaltechniques based on word distribution to the targetdocument(Kupiec et al, 1995).
More recently, otherapproaches have investigated the use of machinelearning to find patterns in documents(Strzalkowskiet al, 1998) and the utility of parameterized mod-ules so as to deal with different genres or cor-pora(Goldstein et al, 2000).
Some of these ap-proaches to single document summarization havebeen extended to deal with multi-document sum-marization(Mani and E.Bloedorn, 1997), (Barzilayet al, 1999), (McKeown et al, 1999).Our work differs from the earlier work in severalimportant respects.
First, our method focuses onsubject shift of the documents from the target eventrather than the sets of documents from differentevents(Radev et al, 2000).
Detecting subject shiftfrom the documents in the target event, however,presents special difficulties, since these documentsare collected from a very restricted domain.
We thuspresent a window adjustment algorithm which auto-matically adjusts the optimal window in the trainingdocuments, so as to include only the data which aresufficiently related to the current subject.
Second,our approach works in a living way, while many ap-proaches are stable ones, i.e., they use documentswhich are prepared in advance and apply a varietyof techniques to create summaries.
We are interestedin a substantially smaller number of initial trainingdocuments, which are then utilized to extract para-graphs from documents relevant to the initial doc-uments.
Because the small number of documentswhich are used for initial training is easy to col-lect, and costly human intervention can be avoided.To do this, we use a tracking technique.
The smallsize of the training corpus, however, requires sophis-ticated parameters tuning for learning techniques,since we can not make one or more validation setsof documents from the initial training documentswhich are required for optimal results.
Instead weuse E?loo of SVMs to cope with this problem.
Fur-ther, our method does not use specific features fortraining such as ?Presence and type of agent?
and?Presence of citation?, which makes it possible to beextendable to other domains(Teufel, 2001).7 ConclusionThis paper studied the effectiveness of detecting sub-ject shifts in paragraph extraction.
Future workincludes (i) incorporating Named Entity extractioninto the method, (ii) applying the method to theTDT2 and TDT3 corpora for quantitative evalua-tion, and (iii) extending the method to on-line para-graph extraction for real-world applications, whichwill extract key paragraphs each time the documentdiscusses the target event.AcknowledgmentsWe would like to thank Prof. Virginia Teller ofHunter College CUNY for her valuable commentsand the anonymous reviewers for their helpful sug-gestions.ReferencesJ.
Allan, J.Carbonell, G.Doddington, J.Yamron,and Y.Yang.
1998a.
Topic Detection and Track-ing pilot study final report.
In Proc.
of DARPAWorkshop.J.
Allan, R.Papka, and V.Lavrenko.
1998b.
On-line new event detection and tracking.
In Proc.of ACM SIGIR?98, pages 37?45.R.
Barzilay, K. R. McKeown, and M. Elhadad.1999.
Information fusion in the context of multi-document summarization.
In Proc.
of ACL?99,pages 550?557.E.
Brill.
1992.
A simple rule-based part of speechtagger.
In Proc.
of ANLP?92, pages 152?155.C.
Fellbaum, editor.
1998.
Nouns in WordNet, AnElectronic Lexical Database.
MIT.J.
Goldstein, V.Mittal, J.Carbonell, andM.Kantrowitz.
2000.
Multi-document sum-marization by sentence extraction.
In Proc.
of theANLP/NAACL-2000 Workshop on AutomaticSummarization, pages 40?48.J.
Kupiec, J.Pedersen, and F.Chen.
1995.
A train-able document summarizer.
In Proc.
of ACM SI-GIR?95, pages 68?73.H.
P. Luhn.
1958.
The automatic creation of litera-ture abstracts.
IBM journal, 2(1):159?165.I.
Mani and E.Bloedorn.
1997.
Multi-documentsummarization by graph search and merging.
InProc.
of AAAI-97, pages 622?628.K.
McKeown, J.Klavans, V.Hatzivassiloglou,R.Barzilay, and E.Eskin.
1999.
Towards mul-tidocument summarization by reformulation:Progress and prospects.
In Proc.
of the 16thNational Conference on AI, pages 18?22.D.
Radev, H.Jing, and M.Budzikowska.
2000.Centroid-based summarization of multiple doc-uments: Sentence extraction, utility-based eval-uation, and user studies.
In Proc.
of theANLP/NAACL-2000 Workshop on AutomaticSummarization, pages 21?30.T.
Strzalkowski, J.Wang, and B.Wise.
1998.
Arobust practical text summarization system.
InProc.
of AAAI Intelligent Text summarizationWorkshop, pages 26?30.S.
Teufel.
2001.
Task-based evaluation of summaryquality: Describing relationships between scien-tific papers.
In Proc.
of NAACL 2001 Workshopon Automatic Summarization, pages 12?21.V.
Vapnik.
1995.
The Nature of Statistical LearningTheory.
Springer.J.
Weston and C.Watkins.
1998.
Multi-class Sup-port Vector Machines.
In Technical Report CSD-TR-98-04.
