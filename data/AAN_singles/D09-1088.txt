Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 842?851,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPAn Alternative to Head-Driven Approaches forParsing a (Relatively) Free Word-Order LanguageReut Tsarfaty Khalil Sima?an Remko SchaInstitute for Logic Language and ComputationUniversity of Amsterdam{r.tsarfaty,k.simaan,r.scha}@uva.nlAbstractApplying statistical parsers developed forEnglish to languages with freer word-order has turned out to be harder thanexpected.
This paper investigates theadequacy of different statistical parsingmodels for dealing with a (relatively)free word-order language.
We showthat the recently proposed Relational-Realizational (RR) model consistentlyoutperforms state-of-the-art Head-Driven(HD) models on the Hebrew Treebank.Our analysis reveals a weakness of HDmodels: their intrinsic focus on configu-rational information.
We conclude that theform-function separation ingrained in RRmodels makes them better suited for pars-ing nonconfigurational phenomena.1 IntroductionParsing technology has come a long way sinceCharniak (1996) demonstrated that a simple tree-bank PCFG performs better than any other parser(with F175 accuracy) on parsing the WSJ Penntreebank (Marcus et al, 1993).
Treebank Gram-mars (Scha, 1990; Charniak, 1996) trained onlarge corpora nowadays present the best availablemeans to parse natural language text.The performance curve for parsing the WSJ wasa steep one at first, as the incorporation of no-tions such as head, distance, subcategorization(Charniak, 1997; Collins, 1999) brought abouta dramatic increase in parsing accuracy to thelevel of F188.
Discriminative approaches, Data-Oriented Parsing (?all-subtrees?)
approaches, andself-training techniques brought further improve-ments, and recent results are starting to level off ataround F192.1 (McClosky et al, 2008).As the interest of the NLP community growsto encompass more languages, we observe effortstowards adapting an English parser for parsingother languages (e.g., (Collins et al, 1999)), ortowards designing a language-independent frame-work based on principles underlying the mod-els for parsing English (Bikel, 2002).
The per-formance curve for parsing other languages withthese models looks rather different.
A case in pointis Modern Standard Arabic.
Since the initial ef-fort of (Bikel, 2002) to parse the Arabic treebank(Maamouri et al, 2004), which yielded F175 ac-curacy, four years and successive revisions haveled to no more than F179 (Maamouri et al, 2008).This pattern from Arabic is not peculiar.
Thelevel of state-of-the-art results for other languagesstill lags behind those for English, even afterputting considerable effort into the adaptation.1Given that these languages are inherently differ-ent from English and from one another, it appearsthat we cannot avoid a question concerning the ad-equacy of the models used to parse them.
That is,given the properties of a language, which model-ing strategy would be appropriate for parsing it?Until recently, there has been practicallyno computationally affordable alternative to theHead-Driven (HD) approach in the developmentof phrase-structure based statistical parsing mod-els.
Recently, we proposed the Relational-Realizational (RR) approach that rests upon differ-ent premises (Tsarfaty and Sima?an, 2008).
Thequestion of how the RR model fares against theHD models that have so far been predominantlyused has never been tackled.
Yet, it is preciselysuch a comparison that can shed new light on thequestion of adequacy we posed above.Empirically quantifying the effects of differ-ent modeling choices has been addressed for En-glish by, e.g., (Johnson, 1998; Klein and Manning,2003), and for German by, e.g., (Dubey, 2004;1Consider, e.g., ?The PaGe shared task on parsing Ger-man?
(Kubler, 2008), reporting F175, F179, F183 for theparticipating parsers.842Rafferty and Manning, 2008).
This paper providesan empirical systematic comparison of conceptu-ally different modeling strategies with respect toparsing Hebrew.
This comparison is intended toprovide a first answer to the question of parser ad-equacy in the face of word-order freedom.Our two empirical results are unequivocal.Firstly, RR models significantly outperform HDmodels (about 2 points absolute improvement inF1) in parsing the Modern Hebrew treebank.
Inparticular, RR models show better performancein identifying the constituents for which syntacticpositions are relatively free.
Secondly, we showa novel variation of the HD model, incorporatingthe Relational notions of the RR model, on the hy-pothesis that this might bridge the gap.
The RRmodel remains superior.Our post-experimental analysis shows that HDmodeling is inherently problematic for parsing alanguage with freer word-order because of thehard-wiring of notions such as left, right and dis-tance from the head.
RR models, taking a prin-cipled approach towards capturing variable form-function correspondence patterns, are better suitedfor parsing nonconfigurational phenomena.2 The DataThis section describes some properties of ModernHebrew (henceforth, Hebrew) that make it signifi-cantly different from English.
These properties af-fect the syntactic representations found in the He-brew Treebank and the kind of syntactic phenom-ena a parser for Hebrew has to cope with.Modern Hebrew is a Semitic language with acanonical SVO word-order pattern,2 yet it allowsconsiderable freedom in the placement of syntac-tic constituents in a clause.
For example, linguisticelements of any kind may be fronted, triggeringan inversion familiar from Germanic languagesas in (1b) (Triggered Inversion (TI) in (Shlonsky,1997)).
Under some information structuring con-ditions, Verb Initial (VI) constructions are also al-lowed, as in (1c) (Melnik, 2002).
All sentencesin (1) thus mean ?Dani gave the present to Dina?,despite their different word-ordering.
(1) a. dani natan et hamatana ledinaDani gave ACC the-present to-Dinab.
et hamatana natan dani ledinaACC the-present gave Dani to-Dina2SVO is an abbreviation for the Subject-Verb-Object typein the basic word-order typology of (Greenberg, 1963).Word Order Frequency Relative FrequencySV 1612 41%VS 1144 29%No S 624 16%No V 550 14%Table 1: Modern Hebrew Predicative Clause-Types in 3930 Predicative Matrix Clauses in theTraining Set of the Modern Hebrew Treebank.c.
natan dani et hamatana ledinagave Dani ACC the-present to-DinaA corpus study we conducted on a fragment ofthe Modern Hebrew treebank reveals that althoughthere is a significant number of subjects precedingverbs in simple (matrix) clauses (41%), there arealso a fair number of sentences for which this or-der is reversed (29%), and there is evidence forother configurations, such as empty realization ofsubjects (16%) and non-verbal realization of pred-icates (14%).In the face of such lack of consistency in itsconfigurational position, the grammatical functionObject in Hebrew is indicated by Differential Ob-ject Marking (DOM) (Aissen, 2003).
NP objectsin Hebrew are marked for accusativity (using themarker et) if they are also marked for definiteness(indicated by the prefix ha).
So, in contrast with(2a)-(2b), the indefinite object renders (2c) un-grammatical, and the missing accusativity renders(2d) awkward.
The fact that marking NP objectsinvolves the joint contribution of multiple surfaceelements (et, ha) contributing features to the NPconstituent is referred to as extended exponence(Matthews, 1993, p.
182).
(2) a. dani natan matana ledinaDani gave present to-Dina?Dani gave a present to Dina?b.
dani natan et hamatana ledinaDani gave ACC the-present to-Dina?Dani gave the present to Dina?c.
*dani natan et matana ledinaDani gave ACC present to-Dinad.
?
?dani natan hamatana ledinaDani gave the-present to-DinaThese data pose a challenge to generative pars-ing models, as they would be required to gener-ate alternative word-order patterns while maintain-ing a coherent pattern of object marking, encom-843passing the contribution of multiple surface expo-nents.
The question this paper addresses is there-fore what kind of modeling approach would be ad-equate for modeling the interplay between syntaxand morphology in marking grammatical relationsin Hebrew, as reflected by the sentence-pair (3).They both mean, roughly, ?Dani gave the presentto Dina yesterday; their word-order vary, but thepattern of object marking is retained.
(3) a. dani natan etmol et hamatana ledinaDani gave yesterday ACC the-presentto-Dinab.
et hamatana natan etmol dani ledinaACC the-present gave yesterday danito-dina3 The ModelsThe different models we experiment with are alltrained on syntactic structures annotated in theModern Hebrew Treebank (Sima?an et al, 2001).The native representation of clause-level cate-gories in the Treebank employs flat structures.This choice was made due to the lack of empiricalevidence in Hebrew for grouping freely positionedsyntactic elements to form a constituent.3 In orderto compensate for the ambiguity in the interpreta-tion of flat structures, additional information suchas morphological marking and grammatical func-tion labels is added to the phrase-structure trees.3.1 The State-Splits ApproachThe simplest way to encode grammatical func-tions information on top of the phrase-structurerepresentation in the treebank is by decoratingnon-terminal nodes with morphological or func-tional features, similarly to the rich representationformat of syntactic categories in GPSG.
This isthe approach taken by the annotators of the He-brew treebank in which information about mor-phological marking appears at multiple levels ofconstituency (Guthmann et al, 2009), and func-tional features (such as subject, object, etc.)
deco-rate phrase-level constituent labels (Sima?an et al,2001).
The S-level representation of our examplesentences (3a)?
(3b) then would be as we depictin figure 1, which can be read off as feature-rich3Such clauses are defined formally as exocentric in for-mal theories of syntax, and are used to describe syntacticstructures in, e.g., Tagalog, Hungarian and Warlpiri (Bres-nan, 2001, page 110).
This flat representation format is char-acteristic of treebanks for other languages with relatively-freeword-order as well, such as German (cf.
(Kubler, 2008)).PCFG productions.
We refer to this approach asthe State-Splits (SP) approach, which serves as thebaseline for the rest of our investigation.3.2 The Head-Driven ApproachFollowing the linguistic wisdom that the inter-nal organization of syntactic constituents revolvesaround their heads, Head-Driven (HD) modelshave been proposed by (Magerman, 1995; Char-niak, 1997; Collins, 1999).
In a generative HDmodel, the head daughter is generated first, con-ditioned on properties of the mother node.
Then,sisters of the head daughter are generated condi-tioned on the head, typically by left and right gen-eration processes.
Overall, HD processes have themodeling advantage that they capture structurally-marked positions that characterize the argumentstructure of the sentence.
The simplest possibleprocess uses unigram probabilities, but (Klein andManning, 2003) show that using vertical and hori-zontal Markovization improves parsing accuracy.4An unlexicalized generative HD model willgenerate our two example sentences as we illus-trate in figure 2.
The generation of the context-free events in figure 1 is then broken down toseven different context-free parameters each, en-coding head-parent and head-sister structural rela-tionships ?
the latter mediated with a structurally-marked delta function (?i).
The rich morpho-logical representation of phrase-level NP objects(+def/acc), for instance, is conditioned on thehead sister, its direction, and the distance from thehead (check, e.g., nodes ?L1,?R2).3.3 The Relational-Realizational ApproachThe Relational-Realizational (RR) parsing modelof (Tsarfaty and Sima?an, 2008) similarly decom-poses the generation of the context-free events infigure 1 into multiple independent parameters, butdoes so in a conceptually different way.
Instead ofdecomposing a context-free event to head and sis-ters, the RR model is best viewed as a generativegrammar that decomposes it to form and function.The RR grammar first generates a set of gram-matical functions depicting the Relational Net-work (RN) (Perlmutter, 1982) of the clause.
This4The success of Head-Driven models (Charniak, 1997;Collins, 2003) was initially attributed to the fact that theywere fully lexicalized, but (Klein and Manning, 2003) showthat an unlexicalized model combining Head-Driven Marko-vian processes with linguistically motivated state-splits canapproach the performance of fully lexicalized models.844(3a) SNP-SBJDaniVP-PRDnatangaveADVPetmolyesterdayNP+D+ACC-OBJet-hamatanathe-presentPP-COMle-dinato-Dina(3b) SNP+D+ACC-OBJet-ha-matanathe-presentVP-PRDnatangaveADVPetmolyesterdayNP-SBJDaniDaniPP-COMle-dinato-DinaFigure 1: The State-Splits Approach for Ex.
(3)(3a) SV P@SL,?L1, V P@SNPDaniDaniHEAD,V P@SVPnatangaveR,?R1, V P@SADVPetmolyesterdayR,?R2, V P@SNP+D+ACCet-ha-matanathe-PresentR,?R3, V P@SPPle-dinato-Dina(3b) SV P@SL,?L1, V P@SNP+D+ACCet-ha-matanathe-presentHEAD,V P@SVPnatangaveR,?R1, V P@SADVPetmolyesterdayR,?R2, V P@SNPDaniDaniR,?R3, V P@SPPle-dinato-DinaFigure 2: The Head-Driven Approach for Ex.
(3)(3a) S{SBJ,PRD,OBJ,COM}@SSBJ@SNPDaniDaniPRD@SVPnatangavePRD : OBJ@SADVPetmolyesterdayOBJ@SNP+D+ACCet-hamatanathe-presentCOM@SPPle-dinato-Dina(3b) S{SBJ,PRD,OBJ,COM}@SOBJ@SNP+D+ACCet-ha-matanathe-PresentPRD@SVPnatangavePRD : SBJ@SADVP@SetmolyesterdaySBJ@SNPDaniDaniCOM@SPPle-dinato-DinaFigure 3: The Relational-Realizational ApproachRN provides an abstract set-theoretic representa-tion of the argument structure of the clause.5 Thisis called the projection phase.
Then an orderingof the grammatical relations is generated, includ-ing reserved contextual slots for adjunction and/orpunctuation marks.
This is called the configura-tion phase.
Finally, each of the grammatical func-tion labels and adjunction slots gets realized as amorphosyntactic representation (a category labelplus dominated morphological features) of the re-spective daughter constituent.
This is called therealization phase.6Figure 3 shows the generation of sentences(3a)?
(3b) following the projection, configurationand realization phases corresponding to the top-down context-free layers of the tree.
In bothcases, the same relational network is generated,capturing the fact that they have the same argu-ment structure.
Then the different orderings ofthe grammatical elements are generated, reservingan adjunction slot for sentential modification (la-beled by short context).
Interestingly, the HD/RRmodels for our sentences are of comparable size(seven parameters) but the parameter types en-code radically different notions.
Illustrative of thedifference is the realization of a morphologicallymarked NP object.
In the RR model this is con-ditioned on a grammatical relation (check, for in-stance, node OBJ@S) and in the HD model it isconditioned on linear ordering or configurationalnotions such as left, right and distance.4 ExperimentsGoal We set out to compare the performanceof the different modeling approaches for pars-ing Modern Hebrew.
Considerable effort was de-voted to making the models strictly comparable,in terms of preparing the data, defining statisticalevents, and unifying the rules determining cross-cutting linguistic notions (e.g., heads and predi-cates, grammatical functions and subcat sets).
Wespell out some of the setup considerations below.Data We use the Modern Hebrew treebank(MHTB) (Sima?an et al, 2001) consisting of 6501sentences from news-wire texts, morphologicallyanalyzed and syntactically annotated as phrase-5Unlike in HD models or dependency grammars, the headpredicative element has no distinguished status here.6Realization of adjunction slots (but not of function la-bels) may generate multiple sisters adjoining at a singleposition.845GF Description Applicable to.
.
.PRD Predicative Elements VP, PREDPSBJ Grammatical Subjects NP, SBAROBJ Direct Objects NPCOM Indirect Objects NP, PPFInite Complements SBARIC Infinitival Complements VPCNJ A Conjunct withina Conjunction Structure AllTable 2: Grammatical Functions in the MHTBSP-PCFG Expansion P(Cln, .
.
.
, Ch, .
.
.
, Crn|P )HD-PCFG Head P(Ch|P )Left Branch?
P(L:?l1, H:?h|Ch, P )Right Branch?
P(Ch, R:?r1|?h, Ch, P )Left Arg/Mod P(Cli,?li+1| L ,?li, Ch, P )Right Arg/Mod P(Cri,?ri+1| R ,?ri, Ch, P )Left Final?
P(C1| L ,?ln?1, Ch, P )Right Final?
P(Cn| R ,?rn?1, Ch, P )RR-PCFG Projection P({gr1, .
.
.
, grm}|P )Configuration P(?gr1, .
.
.
, grm?|{gr1, .
.
.
, grm}P )Realization P(Cj|grj, P )Adjunction P(Cj1, .
.
.
, Cjn|grj: grj+1, P )Table 3: PCFG Parameter Classes for All Modelsstructure trees.
In our version of the MHTB, def-initeness and accusativity features are percolatedfrom the PoS-tags level to phrase-level categories,extending the procedure of (Guthmann et al,2009).
For all models, we applied non-terminalstate-splits distinguishing finite from non-finiteverb forms and possessive from non-possessivenoun phrases.
We head-annotated the treebank,and based on the ?subject?, ?object?, ?complement?and ?conjunction?
labels in the MHTB we devisedan automatic procedure to annotate all the gram-matical functions indicated in table 2.7Procedure For all models, we learn a PCFG byreading off the parameters described in table 3,in accordance with the trees depicted in figures1?3.8 For all models, we use relative frequencyestimates.
For lexical parameters, we use a sim-ple smoothing procedure assigning probability tounknown words using the per-tag distribution ofrare words (?rare?
threshold set to < 2).
The in-put to our parser consists of morphologically seg-mented surface forms, and the parser has to as-7The enhanced corpus will be available at www.science.uva.nl/?rtsarfat/resources.htm.8Our training procedure is strictly equivalent to thetransform-detransform methodology of (Johnson, 1998), butwe implement a tree-traverse procedure as in (Bikel, 2002)collecting all parameters per event at once.sign the syntactic as well as morphological anal-ysis to the surface segments.9 We use the stan-dard development/training/test split as in (Tsarfatyand Sima?an, 2008).
Since our goal is a detailedcomparison and fine-grained analysis of the resultswe concentrate on the development set.
We usea general-purpose CKY parser (Schmid, 2004) toexhaustively parse the sentences, and we strip offall model-specific information prior to evaluation.Evaluation We use standard Parseval measurescalculated for the original, flat, canonical repre-sentation of the parse trees.10 We report Pre-cision/Recall for the coarse-grained non-terminalcategories.
In addition to overall Parseval scoreswe report the accuracy results Per Syntactic Cate-gory.
We further report model size in terms of thenumber of parameters.
As is well known in Ma-chine Learning, models with more parameters re-quire more data to learn, and are more vulnerableto sparseness.
In our evaluation we thus follow therule of thumb that (all else being equal) for mod-els of equal size the better performing model ispreferred, and for models with equal performance,the smaller one is preferred.5 Results and Analysis5.1 Overall ResultsTable 4 shows the parsing results for the State-Split (SP) PCFG, the Head-Driven (HD) PCFGand the Relational-Realizational (RR) PCFGmodels on parsing the Modern Hebrew Treebank,with definiteness and accusativity marked on PoS-tags as well as phrase-level categories.
For allmodels, we experiment with grandparent encod-ing.
For non-HD models, we also examine theutility of a head-category split.119This setup is more difficult than, e.g., the Arabic parsingsetup of (Bikel, 2002), as they assume gold-standard pos-tagsas input.
Yet it is easier than the setup of (Tsarfaty, 2006;Goldberg and Tsarfaty, 2008) which uses unsegmented sur-face forms as input.
The decision to use segmented and un-tagged forms was made to retain a realistic scenario.
Mor-phological analysis is known to be ambiguous, and we donot assume that morphological features are known up front.Morphological segmentation is also ambiguous, but for ourpurposes it is unavoidable.
When comparing different mod-els on an individual sentence they may propose segmenta-tion to sequences of different lengths, for which accuracy re-sults cannot be faithfully compared.
See (Tsarfaty, 2006) fordiscussion.10The flat canonical representation also allows for a faircomparison that is not biased by the differing branching fac-tors of the different models.11In HD models, a head-tag is already assumed in the con-ditioning context for sister nodes (Klein and Manning, 2003).846SP-PCFGGrand-Parent ?
?
+ +Head-Tag ?
+ ?
+Prec/Rec 70.05/72.40 71.14/72.03 74.66/74.35 71.99/72.17(#Params) (4995) (8366) (7385) (11633)HD-PCFGGrand-Parent ?
?
+ +Markov 0 1 0 1Prec/Rec 66.87/71.64 70.40/74.35 73.04/71.94 73.52/74.84(#Params) (6678) (10015) (19066) (21399)RR-PCFGGrand-Parent ?
?
+ +Head Tag ?
+ ?
+Prec/Rec 69.90/73.96 72.96/75.73 74.19/75.03 76.32/76.51(#Params) (3791) (7546) (7611) (13618)Table 4: The Performance of Different Modelsin Parsing Hebrew: Parsing Results Prec/Recallfor Sentences of Length ?
40.For all models, grandparent encoding is help-ful.
For HD models, a higher Markovian order im-proves performance.
This shows that even in He-brew there are linear-precedence tendencies thathelp steer the disambiguation in the right direc-tion, which is in line with our observation thatword-order patterns in Modern Hebrew are notcompletely free (cf.
table 1).The best SP model performs equally or betterthan all HD models.
This might be due to thesmaller size of SP grammars, resulting in more ro-bust estimates.
But it is remarkable that, given thefeature-rich representation, such a simple treebankgrammar provides better disambiguation capacitythan linguistically articulated HD models.
We at-tribute this to the fact that parent-daughter rela-tions have a stronger association with grammati-cal functions than relations between neighbouringnodes.
For Hebrew, such adjacency relations maybe arbitrary due to word-order variability.Overall, RR models show the best performancefor the set of all models with parent encoding, andfor the set of all models without.
Our best RRmodel shows 6.6%/8.4% Prec/Rec error reductionfrom the best SP model.
The Recall improvementshows that the RR model is much better in gener-alizing, recovering successfully more of the con-stituents found in the gold representation.
Thebest RR model also outperforms HD models with8.7%/6.7% Prec/Rec error reduction from the bestIn our SP or RR models, head-information is used as yet an-other feature-value pair rather than an object with a distin-guished status during generation.Model / SP-PCFG HD-PCFG RR-PCFGCategoryNP 77.39 / 74.32 77.94 / 73.75 78.96 / 76.11PP 71.78 / 71.14 71.83 / 69.24 74.4 / 72.02SBAR 55.73 / 59.71 53.79 / 57.49 57.97 / 61.67ADVP 71.37 / 77.01 72.52 / 73.56 73.57 / 77.59ADJP 79.37 / 78.96 78.47 / 77.14 78.69 / 78.18S 73.25 / 79.07 71.07 / 76.49 72.37 / 78.33SQ 36.00 / 32.14 30.77 / 14.29 55.56 / 17.86PREDP 36.31 / 39.63 44.74 / 39.63 44.51 / 46.95VP 76.34 / 80.81 77.33 / 82.51 78.59 / 81.18Table 5: Per-Category Evaluation of ParsingPerformance for Different Models: Prec/RecPer Category Calculated for All Sentences.HD model.
The resulting precision improvementof the RR relative to HD is larger than the im-provement relative to SP, and the Recall improve-ment pattern is reversed.
So it seems that the HDmodel generalizes better than the SP model, butalso gets generalizations wrong more often thanthe SP model.The RR model combines the generalizationadvantage of breaking down context-free eventswhile it maintains the coherence advantage oflearning flat trees (cf.
(Johnson, 1998)).
The bestRR model obtains the best performance amongall models: F176.41.
To put this result in con-text, for the setting in which the Arabic parser of(Maamouri et al, 2008) obtains F178.1, ?
i.e.,with gold standard feature-rich tags ?
the bestRR model obtains F183.3 accuracy which is thebest parsing result reported for a Semitic languageso far.
RR models also have the advantage of re-sulting in more compact grammars, which makeslearning and parsing with them much more com-putationally efficient.5.2 Per-Category Break-Down AnalysisTo understand better the merits of the differentmodels we conducted a break-down analysis ofperformance-per-category for the best performingmodels of each kind.
The break-down results areshown in table 5.
We divided the table into threesets of categories: those for which the RR modelgave the best performance, those for which the SPmodel gave the best performance, and those forwhich there is no clear trend.The most striking outcome is that the RR modelidentifies at higher accuracy precisely those syn-tactic elements that are freely positioned with re-847spect to the head: NPs, PPs, ADVPs and SBARs.Adjectives, in contrast, have clear ordering con-straints ?
they always appear after the noun.
Slevel elements, when embedded, always appearimmediately after a conjunction or a relativizer.In particular, NPs and PPs realize arguments andadjuncts that may occupy different positions rela-tive to the head.
The RR model is better than theother models in identifying those elements partlybecause morphological information helps to dis-ambiguate syntactically relevant chunks and makecorrect attachment decisions about them.Remarkably, predicative (verb-less) phrases(PREDP), which are characteristic of Semitic lan-guages, are hard to parse, but here too the RR doesslightly better than the other two, as it allows forvariability in the means to realize a (verbal or verb-less) predicate.
Both RR and HD models outper-form SP for VPs, which is due to the specific na-ture of VPs in the MHTB ?
they exist only forcomplement phrases with strict linear ordering.6 Distances, Functions andSubcategorization FramesMarkovian processes to the left and to the right ofthe head provide a first approximation of the pred-icate?s argument structure, as they capture trendsin the co-occurrences of constituents reflected intheir pattern of positioning and adjacency.
Butas our results so far show, such an approxima-tion is empirically less rewarding for a languagein which grammatical relations are not tightly cor-related with structural notions.12Collins (2003) attempted a more abstract for-mulation of argument-structure by articulating leftand right subcat-sets.
Each set represents thosearguments that are expected to occur at each sideof the head.
Argument sisters (?complements?
)are generated if and only if they are required, andtheir generation ?cancels?
the requirement in theset.
Adjuncts (?modifiers?)
may be freely gener-ated at any position.At first glance, such a dissociation of configura-tional positions and subcategorization sets seemsto be more adequate for parsing Hebrew, becauseit allows for some variability in the order of gen-eration.
But here too, since the model uses sets of12Conditioning based on adjacency and distance is alsocommon inside dependency parsing models, and we conjec-ture that this is one of the reasons for their difficulty in copingwith freer word-order languages, a difficulty pointed out in(Nivre et al, 2007).
(3a) SV P@SL,{SBJ}, V P@SNPDaniDaniH,V P@SVPnatangaveR,{OBJ,COM}, V P@SADVPetmolyesterdayR,{OBJ,COM}, V P@SNP+D+ACCet-ha-matanathe-PresentR,{COM}, V P@SPPle-dinato-Dina(3b) SV P@SL,{OBJ}, V P@SNP+D+ACCet-ha-matanathe-presentH,V P@SVPnatangaveR,{SBJ,COM}, V P@SADVPetmolyesterdayR,{SBJ,COM}, V P@SNPDaniDaniR,{COM}, V P@SPPle-dinato-DinaFigure 4: The Relational Head-Driven Approachconstituent labels, it disambiguates the grammati-cal functions of an NP solely based on the direc-tion of the head, which is adequate for English butnot for Hebrew.
In order to relax this associationfurther, we propose to replace constituent labelsin the subcat-sets with grammatical relations iden-tical to the functional elements in the relationalnetwork of the RR.
This provides means to medi-ate the cancellation of constituents in the sets withtheir functions and correlate it with morphology.To get an idea of the implications of such amodeling strategy, let us consider our examplesentences in such a Relational-HD model as de-picted in figure 4.
Both representations sharethe event of generating the verbal head.
Sistersare generated conditioned on the head and thefunctional elements remaining to be ?cancelled?.Each of the two trees consists of an event real-izing an ?object?, one for an NP to the right ofthe head, and the other for an NP to its left.
Inboth cases, an object constituent will be generatedjointly with the morphological features associatedwith it.
Evidently, when using sets of grammaticalrelations instead of constituent-labels, correlationof morphology and grammatical functions is morestraight-forward to maintain.848Model SP-PCFG HD-PCFG HD-PCFG HD-PCFG HD-PCFG RR-PCFGType of Distance ?
Phrase-Level Intervening Left and Right Left and Right Left and Right Subcat Setsor Subcategorization State-Splits Verb/Punc #Constituents Constituent Labels Function Labels ConfigurationPrecision/Recall 70.95/70.32 72.39 / 71.97 72.70 / 74.46 72.42 / 74.29 72.84/74.62 76.32/76.51(#Params) (13884) (11650) (18058) (16334) (16460) (13618)Table 6: Incorporating Distance and Grammatical Functions into Head-Driven Parsing ModelsReporting Precison/Recall (#Parameters) for Sentences Length < 40.6.1 Results and AnalysisTable 6 reports the results of experimenting withHD models with different instantiations of a dis-tance function, starting from the standard notionof (Collins, 2003) and ending with our proposed,relational, function sets.
For all HD models, weretain the head, left and right generation cycle andonly change the conditioning context (?i) for sis-ter generation.As a baseline, we show the results of addinggrammatical function information as state-splitson top of an SP-PCFG.13 This SP model presentsmuch lower performance than the RR model al-though they are almost of the same size and theystart off with the same information.
This resultshows that sophisticated modeling can blunt theclaws of the sparseness problem.
One may ob-tain the same number of parameters for two dif-ferent models, but correlate them with more pro-found linguistic notions in one model than in theother.
In our case, there is more statistical evi-dence in the data for, e.g., case marking patterns,than for association of grammatical relations withstructurally-marked positions.For all HD variations, the RR model contin-ues to outperform HD models.
The function-setvariation performs slightly (but not significantly)better than the category-set.
What seems to bestill standing in the way of getting useful dis-ambiguation cues for HD models is the fact thatthe left and right direction of realization is hard-wired in their representation.
This breaks down acoherent distribution over morphosyntactic repre-sentations realizing grammatical relations to arbi-trary position-dependent fragments, which resultsin larger grammars and inferior performance.1413The startegy of adding grammatical functions as state-splits is used in, e.g., German (Rafferty and Manning, 2008).14Due to the difference in the size of the grammars, onecould argue that smoothing will bridge the gap betweenthe HD and RR modeling strategies.
However, the bettersize/accuracy trade-off shown here for RR models suggeststhat they provide a good bias/variance balancing point, es-pecially for feature-rich models characterizing morphologi-7 A Typological DetourHebrew, Arabic and other Semitic Languages areknown to be substantially different from Englishin that English is strongly configurational.
Inconfigurational languages word-order is fixed, andinformation about the grammatical functions ofconstituents (e.g., subject or object) is often cor-related with structurally-marked positions insidehighly-nested constituency structures.
Nonconfig-urational languages (Hale, 1983), in contrast, al-low for freedom in their word-ordering and infor-mation about grammatical relations between con-stituents is often marked by means of morphology.Configurationality is hardly a clear-cut notion.The difference in the configurationality level ofdifferent languages is often conceived as depictedin figure 7.
In linguistic typology, the branchof linguistics that studies the differences betweenlanguages (Song, 2001), the division of labor be-tween linear ordering and morphological markingin the realization of grammatical relations is of-ten viewed as a continuum.
Common wisdom hasit that the lower a language is on the configura-tionality scale, the more morphological markingwe expect to be used (Bresnan, 2001, page 6).For a statistical parser to cope with nonconfig-urational phenomena as observed in, for instance,Hebrew or German, it should allow for flexibil-ity in the form of realization of the grammati-cal functions within the phrase-structure represen-tation of trees.
Recent morphological theoriesemploy Form-Function separation as a widely-accepted practice for enhancing the adequacy ofmodels describing variability in the realization ofgrammatical properties.
Our results suggest thatthe adequacy of syntactic processing models is re-lated to such typological insights as well, and isenhanced by adopting a similar form-function sep-aration for expressing grammatical relations.cally rich languages.
A promising strategy then would be tosmooth or split-and-merge (Petrov et al, 2006)) RR-basedmodels rather than to add an elaborate smoothing componentto configurationally-based HD models.849configurational ??????
nonconfigurationalChinese>English>{German,Hebrew}>WarlpiriFigure 5: The Configurationality ScaleThe HD assumptions take the function of a con-stituent to be transparently related to its formalposition, which entails word-order rigidity.
Suchtransparent relations between configurational po-sitions and grammatical functions are assumed byother kinds of parsing frameworks such as the ?all-subtrees?
approach of Data-Oriented Parsing, andthe distinction between left and right applicationin CCG-based parsers.The RR modeling strategy stipulates a strictseparation between form ?
parametrizing explic-itly basic word-order (Greenberg, 1963) and mor-phological realization (Greenberg, 1954) ?
andfunction ?
parametrizing relational networks bor-rowed from (Perlmutter, 1982) ?
which makesit possible to statistically learn complex form-function mapping reflected in the data.
This isan adequate means to capture, e.g., morphosyn-tactic interactions, which characterize the less-configurational languages on the scale.8 ConclusionIn our comparison of the HD and RR modelingapproaches, the RR approach is shown to be em-pirically superior and typologically more adequatefor parsing a language exhibiting word-order vari-ation interleaved with extended morphology.
HDmodels are less accurate and more vulnerable tosparseness as they assume transparent mappingsbetween form and function, based on left and rightdecompositions hard-wired in the HD representa-tion.
RR models, in contrast, employ form andfunction separation which allows the statisticalmodel to learn complex correspondance patternsreflected in the data.
In the future we plan to in-vestigate how the different models fare against oneanother in parsing different languages.
In particu-lar we wish to examine whether parsing differentlanguages should be pursued by different models,or whether the RR strategy can effectively copewith different languages types.
Finally, we wishto explore the implications of RR modeling forapplications that consider the form of expressionin multiple languages, for instance Statistical Ma-chine Translation (SMT).9 AcknowledgementsWe thank Jelle Zuidema, Inbal Tsarfati, DavidMcCloskey and Yoav Golderg for excellent com-ments on earlier versions.
We also thank MilesOsborne and Tikitu de Jager for comments on thecamera-ready draft.
All errors are our own.
Thework of the first author is funded by the Dutch Sci-ence Foundation (NWO) grant 017.001.271.ReferencesJ.
Aissen.
2003.
Differential Object Marking: Iconic-ity vs. Economy.
Natural Language and LinguisticTheory, 21.D.
M. Bikel.
2002.
Design of a Multi-lingual, Parallel-processing Statistical Parsing Engine.
In Proceed-ings of HLT.J.
Bresnan.
2001.
Lexical-Functional Syntax.
Black-well Textbooks in Linguistics.
Blackwell.E.
Charniak.
1996.
Tree-Bank Grammars.
InAAAI/IAAI, Vol.
2.E.
Charniak.
1997.
Statistical Parsing with a Context-Free Grammar and Word Statistics.
In AAAI/IAAI.M.
Collins, J.
Hajic?, E. Brill, L. Ramshaw, and C. Till-mann.
1999.
A Statistical Parser of Czech.
In Pro-ceedings ACL.M.
Collins.
1999.
Head-Driven Statistical Models forNatural Language Parsing.
Ph.D. thesis, Universityof Pennsylvania.M.
Collins.
2003.
Head-Driven Statistical Models forNatural Language Parsing.
Computational Linguis-tics.A.
Dubey.
2004.
Statistical Parsing for German:Modeling syntactic properties and annotation differ-ences.
Ph.D. thesis, Saarland University, Germany.Y.
Goldberg and R. Tsarfaty.
2008.
A Single Frame-work for Joint Morphological Segmentation andSyntactic Parsing.
In Proceedings of ACL.J.H.
Greenberg.
1954.
A Quantitative Approach tothe Morphological Typology of Language.
In R. F.Spencer, editor, Method and Perspective in Anthro-pology.
University of Minessota Press.J.
H. Greenberg.
1963.
Some Universals of Grammarwith Particular Reference to the Order of Meaning-ful Elements.
In Joseph H. Greenberg, editor, Uni-versals of Language.
MIT Press.N.
Guthmann, Y. Krymolowski, A. Milea, and Y. Win-ter.
2009.
Automatic Annotation of Morpho-Syntactic Dependencies in a Modern Hebrew Tree-bank.
In Proceedings of TLT.850K.
L. Hale.
1983.
Warlpiri and the Grammar of Non-Configurational Languages.
Natural Language andLinguistic Theory, 1(1).M.
Johnson.
1998.
PCFG Models of Linguistic TreeRepresentations.
Computational Linguistics, 24(4).D.
Klein and C. Manning.
2003.
Accurate Unlexical-ized Parsing.
In Proceedings of ACL.S.
Kubler.
2008.
The PaGe Shared task on ParsingGerman.
In ACL Workshop on Parsing German.M.
Maamouri, A. Bies, T. Buckwalter, and W. Mekki.2004.
The Penn Arabic Treebank: Building a Large-Scale Annotated Arabic Corpus.
In Proceedings ofNEMLAR.M.
Maamouri, A. Bies, and S. Kulick.
2008.
EnhancedAnnotation and Parsing of the Arabic treebank.
InProceedings of INFOS.D.
M. Magerman.
1995.
Statistical Decision-TreeModels for Parsing.
In Proceedings of ACL.M.
Marcus, B. Santorini, and M. A. Marcinkiewicz.1993.
Building a Large Annotated Corpus of En-glish: The Penn Treebank.
Computational Linguis-tics.P.
H. Matthews.
1993.
Morphology.
Cambridge.D.
McClosky, E. Charniak, and M. Johnson.
2008.When is self-training effective for parsing?
In Pro-ceedings of CoLing.N.
Melnik.
2002.
Verb-Initial Constructions in Mod-ern Hebrew.
Ph.D. thesis, Berkeley, California.Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-Donald, Jens Nilsson, Sebastian Riedel, and DenizYuret.
2007.
The Shared Task on Dependency Pars-ing.
In Proceedings of the CoNLL Shared Task.D.
M. Perlmutter.
1982.
Syntactic Representation,Syntactic Levels, and the Notion of a Subject.
InPauline Jacobson and Geoffrey Pullum, editors, TheNature of Syntactic Representation.
Springer.S.
Petrov, L. Barrett, R. Thibaux, and D. Klein.
2006.Learning Accurate, Compact, and Interpretable TreeAnnotation.
In Proceedings of ACL.A.
Rafferty and C. D. Manning.
2008.
Parsing ThreeGerman Treebanks: Lexicalized and UnlexicalizedBaselines.
In ACL WorkShop on Parsing German.R.
Scha.
1990.
Language Theory and Language Tech-nology; Competence and Performance.
In Q.
A. M.de Kort and G. L. J. Leerdam, editors, Computer-toepassingen in de Neerlandistiek.
Almere: LVVN.H.
Schmid.
2004.
Efficient Parsing of Highly Am-biguous Context-Free Grammars with Bit vectors.In Proceedings of COLING.U.
Shlonsky.
1997.
Clause Structure and Word Orderin Hebrew and Arabic.
Oxford University Press.K.
Sima?an, A. Itai, Y.
Winter, A. Altman, and N. Na-tiv.
2001.
Building a Tree-Bank for Modern He-brew Text.
In Traitement Automatique des Langues.J.
J.
Song.
2001.
Linguistic Typology: Morphologyand Syntax.
Pearson Education Limited, Edinbrugh.R.
Tsarfaty and K. Sima?an.
2008.
Relational-Realizational Parsing.
In Proceedings of CoLing.R.
Tsarfaty.
2006.
Integrated Morphological and Syn-tactic Disambiguation for Modern Hebrew.
In Pro-ceeding of ACL-SRW.851
