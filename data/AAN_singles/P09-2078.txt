Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 309?312,Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLPThe Lie Detector: Explorations in the Automatic Recognitionof Deceptive LanguageRada MihalceaUniversity of North Texasrada@cs.unt.eduCarlo StrapparavaFBK-IRSTstrappa@fbk.euAbstractIn this paper, we present initial experi-ments in the recognition of deceptive lan-guage.
We introduce three data sets of trueand lying texts collected for this purpose,and we show that automatic classificationis a viable technique to distinguish be-tween truth and falsehood as expressed inlanguage.
We also introduce a method forclass-based feature analysis, which shedssome light on the features that are charac-teristic for deceptive text.You should not trust the devil, even if he tells the truth.?
Thomas of Aquin (medieval philosopher)1 Introduction and MotivationThe discrimination between truth and falsehoodhas received significant attention from fields asdiverse as philosophy, psychology and sociology.Recent advances in computational linguistics mo-tivate us to approach the recognition of deceptivelanguage from a data-driven perspective, and at-tempt to identify the salient features of lying textsusing natural language processing techniques.In this paper, we explore the applicability ofcomputational approaches to the recognition ofdeceptive language.
In particular, we investigatewhether automatic classification techniques repre-sent a viable approach to distinguish between truthand lies as expressed in written text.
Althoughacoustic and other non-linguistic features werealso found to be useful for this task (Hirschberget al, 2005), we deliberately focus on written lan-guage, since it represents the type of data most fre-quently encountered on the Web (e.g., chats, fo-rums) or in other collections of documents.Specifically, we try to answer the following twoquestions.
First, are truthful and lying texts sep-arable, and does this property hold for differentdatasets?
To answer this question, we use threedifferent data sets that we construct for this pur-pose ?
consisting of true and false short statementson three different topics ?
and attempt to automat-ically separate them using standard natural lan-guage processing techniques.Second, if truth and lies are separable, what arethe distinctive features of deceptive texts?
In an-swer to this second question, we attempt to iden-tify some of the most salient features of lying texts,and analyse their occurrence in the three data sets.The paper is organized as follows.
We firstbriefly review the related work, followed by a de-scription of the three data sets that we constructed.Next, we present our experiments and results usingautomatic classification, and introduce a methodfor the analysis of salient features in deceptivetexts.
Lastly, we conclude with a discussion anddirections for future work.2 Related WorkVery little work, if any, has been carried out on theautomatic detection of deceptive language in writ-ten text.
Most of the previous work has focusedon the psychological or social aspects of lying, andthere are only a few previous studies that have con-sidered the linguistic aspects of falsehood.In psychology, it is worthwhile mentioning thestudy reported in (DePaulo et al, 2003), wheremore than 100 cues to deception are mentioned.However, only a few of them are linguistic in na-ture, as e.g., word and phrase repetitions, whilemost of the cues involve speaker?s behavior, in-cluding facial expressions, eye shifts, etc.
(New-man et al, 2003) also report on a psycholinguisticstudy, where they conduct a qualitative analysis oftrue and false stories by using word counting tools.Computational work includes the study of(Zhou et al, 2004), which studied linguistic cuesfor deception detection in the context of text-basedasynchronous computer mediated communication,and (Hirschberg et al, 2005) who focused on de-ception in speech using primarily acoustic andprosodic features.Our work is also related to the automatic clas-sification of text genre, including work on authorprofiling (Koppel et al, 2002), humor recognition309TRUTH LIEABORTIONI believe abortion is not an option.
Once a life has beenconceived, it is precious.
No one has the right to decideto end it.
Life begins at conception,because without con-ception, there is no life.A woman has free will and free choice over what goeson in her body.
If the child has not been born, it is underher control.
Often the circumstances an unwanted childis born into are worse than death.
The mother has theresponsibility to choose the best course for her child.DEATH PENALTYI stand against death penalty.
It is pompous of anyoneto think that they have the right to take life.
No court oflaw can eliminate all possibilities of doubt.
Also, somecircumstances may have pushed a person to commit acrime that would otherwise merit severe punishment.Death penalty is very important as a deterrent againstcrime.
We live in a society, not as individuals.
Thisimposes some restrictions on our actions.
If a persondoesn?t adhere to these restrictions, he or she forfeits herlife.
Why should taxpayers?
money be spent on feedingmurderers?BEST FRIENDI have been best friends with Jessica for about sevenyears now.
She has always been there to help me out.She was even in the delivery room with me when I hadmy daughter.
She was also one of the Bridesmaids inmy wedding.
She lives six hours away, but if we needeach other we?ll make the drive without even thinking.I have been friends with Pam for almost four years now.She?s the sweetest person I know.
Whenever we needhelp she?s always there to lend a hand.
She always hasa kind word to say and has a warm heart.
She is myinspiration.Table 1: Sample true and deceptive statements(Mihalcea and Strapparava, 2006), and others.3 Data SetsTo study the distinction between true and decep-tive statements, we required a corpus with explicitlabeling of the truth value associated with eachstatement.
Since we were not aware of any suchdata set, we had to create one ourselves.
We fo-cused on three different topics: opinions on abor-tion, opinions on death penalty, and feelings aboutthe best friend.
For each of these three topicsan annotation task was defined using the AmazonMechanical Turk service.For the first two topics (abortion and deathpenalty), we provided instructions that asked thecontributors to imagine they were taking part ina debate, and had 10-15 minutes available to ex-press their opinion about the topic.
First, they wereasked to prepare a brief speech expressing theirtrue opinion on the topic.
Next, they were askedto prepare a second brief speech expressing the op-posite of their opinion, thus lying about their truebeliefs about the topic.
In both cases, the guide-lines asked for at least 4-5 sentences and as manydetails as possible.For the third topic (best friend), the contributorswere first asked to think about their best friend anddescribe the reasons for their friendship (includingfacts and anecdotes considered relevant for theirrelationship).
Thus, in this case, they were askedto tell the truth about how they felt about their bestfriend.
Next, they were asked to think about a per-son they could not stand, and describe it as if s/hewere their best friend.
In this second case, theyhad to lie about their feelings toward this person.As before, in both cases the instructions asked forat least 4-5 detailed sentences.We collected 100 true and 100 false statementsfor each topic, with an average of 85 words perstatement.
Previous work has shown that datacollected through the Mechanical Turk service isreliable and comparable in quality with trustedsources (Snow et al, 2008).
We also made a man-ual verification of the quality of the contributions,and checked by hand the quality of all the contri-butions.
With two exceptions ?
two entries wherethe true and false statements were identical, whichwere removed from the data ?
all the other entrieswere found to be of good quality, and closely fol-lowing our instructions.Table 1 shows an example of true and deceptivelanguage for each of the three topics.4 Experimental Setup and ResultsFor the experiments, we used two classifiers:Na?
?ve Bayes and SVM, selected based on theirperformance and diversity of learning methodolo-gies.
Only minimal preprocessing was appliedto the three data sets, which included tokeniza-tion and stemming.
No feature selection was per-formed, and stopwords were not removed.Table 2 shows the ten-fold cross-validation re-sults using the two classifiers.
Since all three datasets have an equal distribution between true andfalse statements, the baseline for all the topics is50%.
The average classification performance of70% ?
significantly higher than the 50% baseline?
indicates that good separation can be obtained310between true and deceptive language by using au-tomatic classifiers.Topic NB SVMABORTION 70.0% 67.5%DEATH PENALTY 67.4% 65.9%BEST FRIEND 75.0% 77.0%AVERAGE 70.8% 70.1%Table 2: Ten-fold cross-validation classificationresults, using a Na?
?ve Bayes (NB) or Support Vec-tor Machines (SVM) classifierTo gain further insight into the variation of ac-curacy with the amount of data available, we alsoplotted the learning curves for each of the datasets, as shown in Figure 1.
The overall growingtrend indicates that more data is likely to improvethe accuracy, thus suggesting the collection of ad-ditional data as a possible step for future work.4050607080901000  20  40  60  80  100Classificationaccuracy(%)Fraction of data (%)Classification learning curvesAbortionDeath penaltyBest friendFigure 1: Classification learning curves.We also tested the portability of the classifiersacross topics, using two topics as training data andthe third topic as test.
The results are shown in Ta-ble 3.
Although below the in-topic performance,the average accuracy is still significantly higherthan the 50% baseline, indicating that the learningprocess relies on clues specific to truth/deception,and it is not bound to a particular topic.5 Identifying Dominant Word Classes inDeceptive TextIn order to gain a better understanding of the char-acteristics of deceptive text, we devised a methodto calculate a score associated with a given classof words, as a measure of saliency for the givenword class inside the collection of deceptive (ortruthful) texts.Given a class of words C = {W1,W2, ...,WN},we define the class coverage in the deceptive cor-pus D as the percentage of words from D belong-ing to the class C:CoverageD(C) =?Wi?CFrequencyD(Wi)SizeDwhere FrequencyD(Wi) represents the totalnumber of occurrences of word Wi inside the cor-pus D, and SizeD represents the total size (inwords) of the corpus D.Similarly, we define the class C coverage for thetruthful corpus T :CoverageT (C) =?Wi?CFrequencyT (Wi)SizeTThe dominance score of the class C in the de-ceptive corpus D is then defined as the ratio be-tween the coverage of the class in the corpus Dwith respect to the coverage of the same class inthe corpus T :DominanceD(C) =CoverageD(C)CoverageT (C)(1)A dominance score close to 1 indicates a similardistribution of the words in the class C in both thedeceptive and the truthful corpus.
Instead, a scoresignificantly higher than 1 indicates a class that isdominant in the deceptive corpus, and thus likelyto be a characteristic of the texts in this corpus.Finally, a score significantly lower than 1 indicatesa class that is dominant in the truthful corpus, andunlikely to appear in the deceptive corpus.We use the classes of words as defined inthe Linguistic Inquiry and Word Count (LIWC),which was developed as a resource for psycholin-guistic analysis (Pennebaker and Francis, 1999).The 2001 version of LIWC includes about 2,200words and word stems grouped into about 70broad categories relevant to psychological pro-cesses (e.g., EMOTION, COGNITION).
The LIWClexicon has been validated by showing significantcorrelation between human ratings of a large num-ber of written texts and the rating obtained throughLIWC-based analyses of the same texts.All the word classes from LIWC are ranked ac-cording to the dominance score calculated withformula 1, using a mix of all three data sets tocreate the D and T corpora.
Those classes thathave a high score are the classes that are dom-inant in deceptive text.
The classes that have asmall score are the classes that are dominant intruthful text and lack from deceptive text.
Table 4shows the top ranked classes along with their dom-inance score and a few sample words that belongto the given class and also appeared in the decep-tive (truthful) texts.Interestingly, in both truthful and deceptive lan-guage, three of the top five dominant classes arerelated to humans.
In deceptive texts however, the311Training Test NB SVMDEATH PENALTY + BEST FRIEND ABORTION 62.0% 61.0%ABORTION + BEST FRIEND DEATH PENALTY 58.7% 58.7%ABORTION + DEATH PENALTY BEST FRIEND 58.7% 53.6%AVERAGE 59.8% 57.8%Table 3: Cross-topic classification resultsClass Score Sample wordsDeceptive TextMETAPH 1.71 god, die, sacred, mercy, sin, dead, hell, soul, lord, sinsYOU 1.53 you, thouOTHER 1.47 she, her, they, his, them, him, herself, himself, themselvesHUMANS 1.31 person, child, human, baby, man, girl, humans, individual, male, person, adultCERTAIN 1.24 always, all, very, truly, completely, totallyTruthful TextOPTIM 0.57 best, ready, hope, accepts, accept, determined, accepted, won, superI 0.59 I, myself, mineFRIENDS 0.63 friend, companion, bodySELF 0.64 our, myself, mine, oursINSIGHT 0.65 believe, think, know, see, understand, found, thought, feels, admitTable 4: Dominant word classes in deceptive text, along with sample words.human-related word classes (YOU, OTHER, HU-MANS) represent detachment from the self, as iftrying not to have the own self involved in thelies.
Instead, the classes of words that are closelyconnected to the self (I, FRIENDS, SELF) are lack-ing from deceptive text, being dominant instead intruthful statements, where the speaker is comfort-able with identifying herself with the statementsshe makes.Also interesting is the fact that words relatedto certainty (CERTAIN) are more dominant in de-ceptive texts, which is probably explained by theneed of the speaker to explicitly use truth-relatedwords as a means to emphasize the (fake) ?truth?and thus hide the lies.
Instead, belief-oriented vo-cabulary (INSIGHT), such as believe, feel, think,is more frequently encountered in truthful state-ments, where the presence of the real truth doesnot require truth-related words for emphasis.6 ConclusionsIn this paper, we explored automatic techniquesfor the recognition of deceptive language in writ-ten texts.
Through experiments carried out onthree data sets, we showed that truthful and ly-ing texts are separable, and this property holdsfor different data sets.
An analysis of classes ofsalient features indicated some interesting patternsof word usage in deceptive texts, including detach-ment from the self and vocabulary that emphasizescertainty.
In future work, we plan to explore therole played by affect and the possible integrationof automatic emotion analysis into the recognitionof deceptive language.ReferencesB.
DePaulo, J. Lindsay, B. Malone, L. Muhlenbruck,K.
Charlton, and H. Cooper.
2003.
Cues to decep-tion.
Psychological Bulletin, 129(1):74?118.J.
Hirschberg, S. Benus, J. Brenier, F. Enos, S. Fried-man, S. Gilman, C. Girand, M. Graciarena,A.
Kathol, L. Michaelis, B. Pellom, E. Shriberg,and A. Stolcke.
2005.
Distinguishing decep-tive from non-deceptive speech.
In Proceedings ofINTERSPEECH-2005, Lisbon, Portugal.M.
Koppel, S. Argamon, and A. Shimoni.
2002.
Au-tomatically categorizing written texts by author gen-der.
Literary and Linguistic Computing, 4(17):401?412.R.
Mihalcea and C. Strapparava.
2006.
Learning tolaugh (automatically): Computational models forhumor recognition.
Computational Intelligence,22(2):126?142.M.
Newman, J. Pennebaker, D. Berry, and J. Richards.2003.
Lying words: Predicting deception from lin-guistic styles.
Personality and Social PsychologyBulletin, 29:665?675.J.
Pennebaker and M. Francis.
1999.
Linguistic in-quiry and word count: LIWC.
Erlbaum Publishers.R.
Snow, B. O?Connor, D. Jurafsky, and A. Ng.
2008.Cheap and fast ?
but is it good?
evaluating non-expert annotations for natural language tasks.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing, Honolulu,Hawaii.L.
Zhou, J Burgoon, J. Nunamaker, and D. Twitchell.2004.
Automating linguistics-based cues for detect-ing deception in text-based asynchronous computer-mediated communication.
Group Decision and Ne-gotiation, 13:81?106.312
