A Lightweight Semantic Chunking Model Based On TaggingKadri HaciogluCenter for Spoken Language Research,University of Colorado, Boulderhacioglu@cslr.colorado.eduAbstractIn this paper, a framework for the develop-ment of a fast, accurate, and highly portablesemantic chunker is introduced.
The frame-work is based on a non-overlapping, shallowtree-structured language.
The derivation of thetree is considered as a sequence of tagging ac-tions in a predefined linguistic context, and anovel semantic chunker is accordingly devel-oped.
It groups the phrase chunks into the ar-guments of a given predicate in a bottom-upfashion.
This is quite different from currentapproaches to semantic parsing or chunkingthat depend on full statistical syntactic parsersthat require tree bank style annotation.
Wecompare it with a recently proposed word-by-word semantic chunker and present resultsthat show that the phrase-by-phrase approachperforms better than its word-by-word coun-terpart.1 IntroductionSemantic representation, and, obviously, its extractionfrom an input text, are very important for several naturallanguage processing tasks; namely, information extrac-tion, question answering, summarization, machine trans-lation and dialog management.
For example, in questionanswering systems, semantic representations can beused to understand the user?s question, expand thequery, find relevant documents and present a summaryof multiple documents as the answer.Semantic representations are often defined as acollection of frames with a number of slots for eachframe to represent the task structure and domain objects.This frame-based semantic representation has beensuccessfully used in many limited-domain tasks.
ForThis work is supported by the ARDA AQUAINT pro-gram via contract OCG4423B and by the NSF via grant ISS-9978025fully used in many limited-domain tasks.
For example,in a spoken dialog system designed for travel planningone might have an Air frame with slots Origin, Destina-tion, Depart_date, Airline etc.
The drawback of thisdomain specific representation is the high cost toachieve adequate coverage in a new domain.
A new setof frames and slots are needed when the task is extendedor changed.
Authoring the patterns that instantiate thoseframes is time consuming and expensive.Domain independent semantic representations canovercome the poor portability of domain specific repre-sentations.
A natural candidate for this representation isthe predicate-argument structure of a sentence that ex-ists in most languages.
In this structure, a word is speci-fied as a predicate and a number of word groups areconsidered as arguments accompanying the predicate.Those arguments are assigned different semantic cate-gories depending on the roles that they play with respectto the predicate.
Researchers have used several differentsets of argument labels.
One possibility are the non-mnemonic labels used in the PropBank corpus (Kings-bury and Palmer, 2002): ARG0, ARG1, ?, ARGM-LOC, etc.
An alternative set are thematic roles similarto those proposed in (Gildea and Jurafsky, 2002):AGENT, ACTOR, BENEFICIARY, CAUSE, etc.Shallow semantic parsing with the goal of creating adomain independent meaning representation based onpredicate/argument structure was first explored in detailby (Gildea and Jurafsky, 2002).
Since then several vari-ants of the basic approach have been introduced usingdifferent features and different classifiers based on vari-ous machine-learning methods (Gildea and Palmer,2002;.Gildea and Hockenmaier, 2003; Surdeanu et.
al.,2003;  Chen and Rambow, 2003; Fleischman and Hovy,2003; Hacioglu and Ward, 2003; Thompson et.
al., 2003; Pradhan et.
al., 2003).
Large semantically annotateddatabases, like FrameNet (Baker et.al, 1998) and Prop-Bank (Kingsbury and Palmer, 2002) have been used totrain and test the classifiers.
Most of these approachescan be divided into two broad classes: Constituent-by-Constituent  (C-by-C) or Word-by-Word (W-by-W)classifiers.
In C-by-C classification, the syntactic treeFigure 1.
Proposed non-overlapping, shallow lexicalized syntactic/semantic tree structurerepresentation of a sentence is linearized into a sequenceof its syntactic constituents (non-terminals).
Then eachconstituent is classified into one of several arguments orsemantic roles using a number of features derived fromits respective context.
In the W-by-W method (Haciogluand Ward, 2003) the problem is formulated as a chunk-ing task and the features are derived for each word (as-suming part of speech tags and syntactic phrase chunksare available), and the word is classified into one of thesemantic labels using an IOB2 representation.
Amongthose methods, only the W-by-W method consideredsemantic classification with features created in a bot-tom-up manner.
The motivations for bottom-up analysisare?
Full syntactic parsing is computationally expen-sive?
Taggers and chunkers are fast?
Not all languages have full syntactic parsers?
The annotation effort required for a full syntacticparser is larger than that required for taggers andchunkers.In this paper, we propose a non-overlapping shallowtree structure, at lexical, syntactic and semantic levels torepresent the language.
The goal is to improve the port-ability of semantic processing to other applications,domains and languages.
The new structure is complexenough to capture crucial (non-exclusive) semanticknowledge for intended applications and simple enoughto allow flat, easier and fast annotation.
The human ef-fort required for flat labeling is significantly less thanthat required for creating tree bank style labels.
We pre-sent a particular derivation of the structure yielding alightweight machine learned semantic chunker.2 Representation of LanguageWe assume a flat, non-overlapping (or chunked) repre-sentation of language at the lexical, syntactic and se-mantic levels.
In this representation a sentence is asequence of base phrases at a syntactic level.
A basephrase is a phrase that does not dominate anotherphrase.
At a semantic level, the chosen predicate has anumber of arguments attached to it.
The arguments arefilled by a sequence of base phrases that span sequencesof words tagged with their part of speech.
We proposeto organize this flat structure in a lexicalized tree as il-lustrated in Fig 1.
The root is the standard non-terminalS lexicalized with the predicate.
One level below, argu-ments attached to the predicate are organized in a flatstructure and lexicalized with headwords.
The next levelis organized in terms of the syntactic chunks spanned byeach argument.
The lower levels consist of the part ofspeech tags and the words.
The lower level can also beextended to include flat morphological representationsof words to deal with morphologically rich languageslike Arabic, Korean and Turkish.
One can introduce arelatively deeper structure using a small set of rules atthe phrasal level under each semantic non-terminal.
Forexample, the application of simple rules in order onTHEME?s chunks, such as (1) combine flat PP NP intoa right branching PP and then  (2) combine flat NP withPP into a recursive NP, will result in a relatively deepertree.
Although the main focus of the paper is on thestructure presented in Figure 1, we note that a deeperstructure obtained by using a small number of simplehand-crafted rules on syntactic chunks (applied in abottom-up manner) is worthy of further research.3 Model for Tree DecompositionThe tree structure introduced in the preceding sectioncan be generated as a unique sequence of derivationactions in many different ways.
We propose a modelthat decomposes the tree into a sequence of tagging ac-tions at the word, phrase and argument levels.
In thismodel the procedure is a bottom up derivation of thetree that is accomplished in several steps.
Each stepconsists of a number of actions.
The first step is a se-quence of actions to tag the words with their Part-Of-Speech (POS).
Then the words are tagged as inside aphrase (I), outside a phrase (O) or beginning of a phrase(B) (Ramhsaw and Marcus, 1995).
For example, in Fig-ure 1, the word For is tagged as B-PP, fiscal is tagged asB-NP, 1989 is tagged as I-NP, etc.
This step is followedby a sequence of join actions.
A sequence that startswith a B-tag and continues with zero or more I-tags ofthe same type is joined into a single tag that representsthe type of the phrase (e.g.
NP, PP etc.).
The next steptags phrases as inside an argument, outside an argumentor beginning of an argument.
Finally, we join IOB ar-gument tags as we did for base phrases.4 Parsing StrategyThe parse strategy based on the tagging actions consistsof tinpumensemponinpu ,andthatfineworworapp(SVto lstagtextfeattagsusesimthegroshoalonbasthephrusindectheiwithincreasing the context window, adding new sentencelevel and predicate dependent features, and introducingalternate organizations of the input.
An alternative toour approach is the W-by-W approach proposed in (Ha-cioglu and Ward, 2003).
We show it below:Here the labeling is carried out in a word-by-wordbasis.
We note that the Phrase-by-Phrase (P-by-P) tag-ging classifies larger units, ignores some of the wordsFor       IN     B-PP B-TEMPORALfiscal     JJ      B-NP    I-TEMPORAL1989               CD    I-NP               ??Mr.
NNP   B-NPMcGovern     NNP   I-NPreceived      VBD    B-VPa           DT       B-NPsalary           ` NN  I-NPof                    PP     B-PP877,663          CD         B-NPcontextcurrentdecision hree components that are sequentially applied to thet text for a chosen predicate to determine its argu-ts.
These components are POS, base phrase andantic taggers/chunkers.
In the following, each com-ent will be described along the dimensions of its (i)t, (ii) decision context, (ii) features, (iv) classifier(v) output.In the first stage, the input is the sequence of wordsare processed from left-to-right.
The context is de-d to be a fixed-size window centered around thed in focus.
The features are derived from a set ofd specific features and previous tag decisions thatear in the context.
A Support Vector MachineM) (Vapnik, 1995) as a multi-class classifier is usedabel words with their POS tags1.
In the seconde, the input is the sequence of word/tag pairs.
Con-is defined in the same way as in the first stage.
Theures are the word/tag pairs and previous phrase IOBthat appear in the context.
An SVM classifier isd to classify the base phrase IOB label.
This is veryilar to the set up in (Kudo and Matsumato, 2000).
Inlast stage (the major contribution of the paper) weup the input, context, features and decisions aswn below.The input is the base-phrase labels and headwordsg with their part of speech tags and positions in thee phrase.
The context is ?2/+2 window centered atbase phrase in question.
An SVM classifies the basease into semantic role tags in an IOB representationg a context including the two previous semantic tagisions.
It is possible to enrich the set of features by1 Although not limited to, SVMs are selected because ofr ability to manage a large number of overlapping featuresa good generalization performance.
(modifiers), uses effectively a wider linguistic contextfor a given window size and performs tagging in asmaller number of steps.5 ExperimentsAll experiments were carried out using sections 15-18of the PropBank data holding out Section-00 and Sec-tion-23 for development and test, respectively.
We usedchunklink 2  to flatten syntactic trees.
Then using thepredicate argument annotation we obtained a new cor-pus of  the tree structure introduced in Section 2.All SVM classifiers, for POS tagging, syntacticphrase chunking and semantic argument labeling, wererealized using the TinySVM3 with the polynomial ker-nel of degree 2 and the general purpose SVM basedchunker YamCha4 .
The results were evaluated usingprecision and recall numbers along with the F metric.Table 1 compares W-by-W and P-by-P approaches.The base features described in Section 4 along with twoadditional predicate specific features were used; thelemma of the predicate and a binary feature that indi-cates the word is before or after the predicate.Table 1.
Performance comparisonsMethod Precision Recall F1W-by-W 58% (60%) 49% (52%) 53% (56%)P-by-P 63% (66%) 56% (59%) 59% (62%)In these experiments the accuracy of the POS taggerwas 95.5% and the F-metric of the phrase chunker was94.5%.
The figures in parantheses are for gold standard2 http://ilk.uvt.nl/~sabine/chunklink3 http://cl.aist-nara.ac.jp/~taku-ku/software/TinySVM4 http://cl.aist-nara.ac.jp/~taku-ku/software/yamchacurrentdecisionPP    For  IN          B-PP B-TEMPORALNP   1989  CD      I-NP I-TEMPORALNP   McGovern NNP I-NP                ?
?VP   received VBD B-VPNP   salary NN         I-NPPP    of  IN B-PPNP    877,663 CD B-NPcontext(i.e.
POS and phrase features are derived from hand-annotated trees).
The others show the performance ofthe sequential bottom-up tagging scheme that we havedescribed in section 4.
We experimented with a reducedset of  PropBank arguments.
The set contains the mostfrequent 19 arguments in the corpus.It is interesting to note that there is a huge drop inperformance for ?chunked?
semantic analysis as com-pared with the performances at mid 90s for the syntacticand lexical analyses.
This clearly shows that the extrac-tion of even ?chunked?
semantics of a text is a verydifficult task and still a lot remains to be done to bridgethe gap.
This is partly due to the difficulty of havingconsistent semantic annotations, partly due to the miss-ing information/features for word senses and usages,partly due to the absence of world knowledge and partlydue to the relatively small size of the training set.
Ourother experiments clearly show that with more trainingdata and additional features it is possible to improve theperformance by 10-15% absolute (Hacioglu et.
al.,2004).
The feature engineering for semantic chunking isopen-ended and the discussion of it is beyond the scopeof the short paper.
Here, we have illustrated that the P-by-P approach is a promising alternative to the recentlyproposed W-by-W approach (Hacioglu and Ward,2003).6 ConclusionsWe have developed a novel phrase-by-phrase semanticchunker based on a non-overlapping (or chunked) shal-low language structure at lexical, syntactic and semanticlevels.
We have implemented a baseline system andcompared it to a recently proposed word-by-word sys-tem.
We have shown better performance with thephrase-by-phrase approach.
It has been also pointed outthat the new method has several advantages; it classifieslarger units, uses wider context, runs faster.
Prior workhas not considered this bottom-up strategy for semanticchunking, which we claim yields a lightweight, fast, androbust chunker at moderately high performance.
Al-though we have flattened the trees in the PropBank cor-pus for our experiments, the proposed languagestructure supports flat annotation from scratch, whichwe believe is useful for porting the method to other do-mains and languages.
While our initial results have beenencouraging, this work must be extended and enhancedto produce the quality of semantic parse produced bysystems using a full syntactic parse.ReferencesCollin F. Baker, Charles J. Fillmore, and John B. Lowe1998.
The Berkley FrameNet Project.
Proceedings ofColing-ACL, pp.
86-90.John Chen and Owen Rambow.
2003.
Use of Deep Lin-guistic Features for the Recognition and Labeling ofSemantic Arguments.
In Proceedings of EMNLP-2003, Sapporo, Japan.Daniel Gildea and Daniel Jurafsky.
2002.
AutomaticLabeling of Semantic Roles.
Computational Linguis-tics, 28:3, pages 245-288.Daniel Gildea  and Martha Palmer.
2002.
The necessityof syntactic parsing for predicate argument recogni-tion.
In Proceedings of ACL?02.Daniel Gildea and Julia Hockenmaier.
2003.
IdentifyingSemantic Roles Using Combinatory CategoricalGrammar.
In Proceedings of EMNL?03, Japan.Micheal Fleischman and Eduard Hovy.
2003.
A Maxi-mum Entropy Approach to FrameNet Tagging.
Pro-ceedings of  HLT/NAACL-03.Kadri Hacioglu and Wayne Ward.
2003.
Target wordDetection and semantic role chunking using supportvector machines.
Proceedings of  HLT/NAACL-03.Kadri Hacioglu, Sameer Pradhan, Wayne Ward, JamesH.
Martin and Daniel Jurafsky.
2004.
Semantic RoleLabeling by Tagging Syntactic Chunks.
CONLL-2004 Shared Task.Paul Kingsbury, Martha Palmer, 2002.
From TreeBankto PropBank.
Conference on Language Resourcesand Evaluation LREC-2002.Taku Kudo, Yuji Matsumato.
2000.
Use of support vec-tor learning for chunk identification.
Proc.
of the 4thConference on Very Large Corpora, pp.
142-144.Sameer Pradhan, Kadri Hacioglu, Wayne Ward, JamesH.
Martin, Dan Jurafsky.2003.
Semantic Role Pars-ing: Adding Semantic Structure to Unstructured Text.In Proceedings of ICDM 2003, Melbourne, Florida.Lance E. Ramhsaw and Mitchell P. Marcus.
1995.
TextChunking Using Transformation Based Learning.Proceedings of  the 3rd ACL  Workshop on  VeryLarge Corpora, pages 82-94.Mihai Surdeanu, Sanda Harabagiu, John Williams, andPaul Aarseth.
2003.
Using Predicate-ArgumentStructure for Information Extraction.
Proceedings ofthe 41th Annual Conference on the Association forComputational Linguistics (ACL-03).Cynthia A. Thompson, Roger Levy, and Christopher D.Manning.
2003.
A Generative Model for SemanticRole Labeling.
Proc.
of the European Conference onMachine Learning (ECML-03).Vladamir Vapnik 1995.
The Nature of Statistical Learn-ing Theory.
Springer Verlag, New York, USA.
