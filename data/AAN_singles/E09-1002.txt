Proceedings of the 12th Conference of the European Chapter of the ACL, pages 10?15,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsNLP and the humanities: the revival of an old liaisonFranciska de JongUniversity of TwenteEnschede, The Netherlandsfdejong@ewi.utwente.nlAbstractThis paper present an overview of someemerging trends in the application of NLPin the domain of the so-called Digital Hu-manities and discusses the role and natureof metadata, the annotation layer that is socharacteristic of documents that play a rolein the scholarly practises of the humani-ties.
It is explained how metadata are thekey to the added value of techniques suchas text and link mining, and an outline isgiven of what measures could be taken toincrease the chances for a bright future forthe old ties between NLP and the humani-ties.
There is no data like metadata!1 IntroductionThe humanities and the field of natural languageprocessing (NLP) have always had common play-grounds.
The liaison was never constrained to lin-guistics; also philosophical, philological and lit-erary studies have had their impact on NLP , andthere have always been dedicated conferences andjournals for the humanities and the NLP com-munity of which the journal Computers and theHumanities (1966-2004) is probably known best.Among the early ideas on how to use machines todo things with text that had been done manuallyfor ages is the plan to build a concordance for an-cient literature, such as the works of St ThomasAquinas (Schreibman et al, 2004).
which was ex-pressed already in the late 1940s.
Later on hu-manities researchers started thinking about noveltasks for machines, things that were not feasiblewithout the power of computers, such as author-ship discovery.
For NLP the units of process-ing gradually became more complex and shiftedfrom the character level to units for which stringprocessing is an insufficient basis.
At some stagesyntactic parsers and generators were seen as amethod to prove the correctness of linguistic the-ories.
Nowadays semantic layers can be analysedat much more complex levels of granularity.
Notjust phrases and sentences are processed, but alsoentire documents or even document collections in-cluding those involving multimodal features.
Andin addition to NLP for information carriers, alsolanguage-based interaction has grown into a ma-tured field, and applications in other domains thanthe humanities now seem more dominant.
Theimpact of the wide range of functionalities thatinvolve NLP in all kinds of information process-ing tasks is beyond what could be imagined 60years ago and has given rise to the outreach ofNLP in many domains, but during a long periodthe humanities were one of the few valuable play-grounds.Even though the humanities have been ableto conduct NLP-empowered research that wouldhave been impossible without the the early toolsand resources already for many decades, the morerecent introduction of statistical methods in lan-gauge is affecting research practises in the human-ities at yet another scale.
An important explana-tion for this development is of course the widescale digitisation that is taken up in the humani-ties.
All kinds of initiatives for converting ana-logue resources into data sets that can be storedin digital repositories have been initiated.
It iswidely known that ?There is no data like moredata?
(Mercer, 1985), and indeed the volumes ofdigital humanities resources have reached the levelrequired for adequate performance of all kinds oftasks that require the training of statistical mod-els.
In addition, ICT-enabled methodologies andtypes of collaboration are being developed andhave given rise to new epistemic cultures.
DigitalHumanities (sometimes also referred to as Com-putational Humanities) are a trend, and digitalscholarship seems a prerequisite for a successfulresearch career.
But in itself the growth of digi-10tal resources is not the main factor that makes thehumanities again a good testbed for NLP.
A keyaspect is the nature and role of metadata in the hu-manities.
In the next section the role of metadatain the humanities and the the ways in which theycan facilitate and enhance the application of textand data mining tools will be described in moredetail.
The paper takes the position that for the hu-manities a variant of Mercer?s saying is even moretrue.
There is no data like metadata!The relation between NLP and the humanitiesis worth reviewing, as a closer look into the wayin which techniques such as text and link miningcan demonstrate that the potential for mutual im-pact has gained in strength and diversity, and thatimportant lessons can be learned for other appli-cation areas than the humanities.
This renewedliaison with the now digital humanities can helpNLP to set up an innovative research agenda whichcovers a wide range of topics including semanticanalysis, integration of multimodal information,language-based interaction, performance evalua-tion, service models, and usability studies.
Thefurther and combined exploration of these topicswill help to develop an infrastructure that will alsoallow content and data-driven research domains inthe humanities to renew their field and to exploitthe additional potential coming from the ongoingand future digitisation efforts, as well as the rich-ness in terms of available metadata.
To name afew fields of scholarly research: art history, mediastudies, oral history, archeology, archiving stud-ies, they all have needs that can be served in novelways by the mature branches that NLP offers to-day.
After a sketch in section 2 of the role ofmetadata, so crucial for the interaction betweenthe humanities and NLP, a rough overview of rel-evant initiatives will be given.
Inspired by sometelling examples, it will be outlined what could bedone to increase the chances for a bright future forthe old ties, and how other domains can benefit aswell from the reinvention of the old common play-ground between NLP and the humanities.2 Metadata in the HumanitiesDigital text, but also multimedia content, can bemined for the occurrence of patterns at all kindsof layers, and based on techniques for informationextraction and classification, documents can be an-notated automatically with a variety of labels, in-cluding indications of topic, event types, author-ship, stylistics, etc.
Automatically generated an-notations can be exploited to support to what isoften called the semantic access to content, whichis typically seen as more powerful than plain fulltext search, but in principle also includes concep-tual search and navigation.The data used in research in the domain ofthe humanities comes from a variety of sources:archives, musea (or in general cultural heritagecollections), libraries, etc.
As a testbed for NLPthese collections are particularly challenging be-cause of the combination of complexity increas-ing features, such as language and spelling changeover time, diversity in orthography, noisy content(due to errors introduced during data conversion,e.g., OCR or transcription of spoken word ma-terial), wider than average stylistic variation andcross-lingual and cross-media links.
They arealso particularly attractive because of the avail-able metadata or annotation records, which are thereflection of analytical and comparative scholarlyprocesses.
In addition, there is a wide diversityof annotation types to be found in the domain (cf.the annotation dimensions distinguished by (Mar-shall, 1998)), and the field has developed mod-elling procedures to exploit this diversity (Mc-Carty, 2005) and visualisation tools (Unsworth,2005).2.1 Metadata for TextFor many types of textual data automatically gen-erated annotations are the sole basis for seman-tic search, navigation and mining.
For human-ities and cultural heritage collections, automati-cally generated annotation is often an addition tothe catalogue information traditionally producedby experts in the field.
The latter kind of manu-ally produced metadataa is often specified in ac-cordance to controlled key word lists and meta-data schemata agreed for the domain.
NLP tag-ging is then an add on to a semantic layer that initself can already be very rich and of high qual-ity.
More recently initiatives and support tools forso-called social tagging have been proposed thatcan in principle circumvent the costly annotationby experts, and that could be either based on freetext annotation or on the application of so-calledfolksonomies as a replacement for the traditionaltaxonomies.
Digital librarians have initiated thedevelopment of platforms aiming at the integrationof the various annotation processes and at sharing11tools that can help to realise an infrastructure fordistributed annotation.
But whatever the genesis isof annotations capturing the semantics of an entiredocument, they are a very valuable source for thetraining of automatic classifiers.
And traditionally,textual resources in the humanities have lots of it,partly because the mere art of annotating texts hasbeen invented in this domain.2.2 Metadata for MultimediaPart of the resources used as basis for scholarlyresearch is non-textual.
Apart from numeric dataresources, which are typically strongly structuredin database-like environments, there is a growingamount of audiovisual material that is of interestto humanities researchers.
Various kinds of multi-media collections can be a primary source of infor-mation for humanities researchers, in particular ifthere is a substantial amount of spoken word con-tent, e.g., broadcast news archives, and even moreprominently: oral history collections.It is commonly agreed that accessibility of het-erogeneous audiovisual archives can be boostedby indexing not just via the classical metadata,but by enhancing indexing mechanisms throughthe exploitation of the spoken audio.
For sev-eral types of audiovisual data, transcription of thespeech segments can be a good basis for a time-coded index.
Research has shown that the qualityof the automatically generated speech transcrip-tions, and as a consequence also the index quality,can increase if the language models applied havebeen optimised to both the available metadata (inparticular on the named entities in the annotations)and the collateral sources available (Huijbregts etal., 2007).
?Collateral data is the term used forsecondary information objects that relate to theprimary documents, e.g., reviews, program guidesummaries, biographies, all kinds of textual pub-lications, etc.
This requires that primary sourceshave been annotated with links to these secondarymaterials.
These links can be pointers to sourcelocations within the collection, but also links to re-lated documents from external sources.
In labora-tory settings the amount of collateral data is typi-cally scarce, but in real life spoken word archives,experts are available to identify and collect related(textual) content that can help to turn generic lan-guage models into domain specific models withhigher accuracy.2.3 Metadata for Surprise DataThe quality of automatically generated content an-notations in real life settings is lagging behind incomparison to experimental settings.
This is ofcourse an obstacle for the uptake of technology,but a number of pilot projects with collectionsfrom the humanities domain show us what can bedone to overcome the obstacles.
This can be illus-trated again with the situation in the field of spo-ken document retrieval.For many A/V collections with a spoken au-dio track, metadata is not or only sparsely avail-able, which is why this type of collection is oftenonly searchable by linear exploration.
Althoughthere is common agreement that speech-based, au-tomatically generated annotation of audiovisualarchives may boost the semantic access to frag-ments of spoken word archives enormously (Gold-man et al, 2005; Garofolo et al, 2000; Smeatonet al, 2006), success stories for real life archivesare scarce.
(Exceptions can be found in researchprojects in the broadcast news and cultural her-itage domains, such as MALACH (Byrne et al,2004), and systems such as SpeechFind (Hansenet al, 2005).)
In lab conditions the focus is usu-ally on data that (i) have well-known characteris-tics (e.g, news content), often learned along withannual benchmark evaluations,1 (ii) form a rela-tively homogeneous collection, (iii) are based ontasks that hardly match the needs of real users, and(iv) are annotated in large quantities for trainingpurposes.
In real life however, the exact character-istics of archival data are often unknown, and arefar more heterogeneous in nature than those foundin laboratory settings.
Language models for real-istic audio sets, sometimes referred to as surprisedata (Huijbregts, 2008), can benefit from a cleveruse of this contextual information.Surprise data sets are increasingly being takeninto account in research agendas in the field focus-ing on multimedia indexing and search (de Jonget al, 2008).
In addition to the fact that they areless homogenous, and may come with links to re-lated documents, real user needs may be availablefrom query logs, and as a consequence they arean interesting challenge for cross-media indexingstrategies targeting aggregated collections.
Sur-1E.g., evaluation activities such as those organised byNIST, the National Institute of Standards, e.g., TREC forsearch tasks involving text, TRECVID for video search, RichTranscription for the analysis of speech data, etc.
http://www.nist.gov/12prise data are therefore an ideal source for the de-velopment of best practises for the application oftools for exploiting collateral content and meta-data.
The exploitation of available contextual in-formation for surprise content and the organisationof this dual annotation process can be improved,but in principle joining forces between NLP tech-nologies and the capacity of human annotators isattractive.
On the one hand for the improved ac-cess to the content, on the other hand for an inno-vation of the NLP research agenda.3 Ingredients for a NovelKnowledge-driven WorkflowA crucial condition for the revival of the com-mon playground for NLP and the humanities isthe availability of representatives of communitiesthat could use the outcome, either in the devel-opment of services to their users or as end users.These representatives may be as diverse and in-clude e.g., archivists, scholars with a research in-terest in a collection, collection keepers in librariesand musea, developers of educational materials,but in spite of the divergence that can be attributedto such groups, they have a few important charac-teristics in common: they have a deep understand-ing of the structure, semantic layers and contentof collections, and in developing new road mapsand novel ways of working, the pressure they en-counter to be cost-effective is modest.
They arethe first to understand that the technical solutionsand business models of the popular web search en-gines are not directly applicable to their domainin which the workflow is typically knowledge-driven and labour-intensive.
Though with the in-troduction of new technologies the traditional roleof documentalists as the primary source of highquality annotations may change, the availability oftheir expertise is likely to remain one of the majorsuccess factors in the realisation of a digital in-frastructure that is as rich source as the reposito-ries from the analogue era used to be.All kinds of coordination bodies and actionplans exist to further the field of Digital Hu-manities, among which The Alliance of Dig-ital Humanities Organizations http://www.digitalhumanities.org/ and HASTAC(https://www.hastac.org/) and DigitalArts an Humanities www.arts-humanities.net, and dedicated journals and events haveemerged, such as the LaTeCH workshop series.
Inpart they can build on results of initiatives for col-laboration and harmonisation that were started ear-lier, e.g., as Digital Libraries support actions or ascoordinated actions for the international commu-nity of cultural heritage institutions.
But in orderto reinforce the liaison between NLP and the hu-manities continued attention, support and fundingis needed for the following:Coordination of coherent platforms (both lo-cal and international) for the interaction be-tween the communities involved that stim-ulate the exchange of expertise, tools, ex-perience and guidelines.
Good exampleshereof exist already in several domains,e.g., the field of broadcast archiving (ISTproject PrestoSpace; www.prestospace.org/), the research area of Oral History, allkinds of communities and platforms targetingthe accessibility of cultural heritage collec-tions (e.g., CATCH; http://www.nwo.nl/catch), but the long-term sustainabilityof accessible interoperable institutional net-works remains a concern.Infrastructural facilities for the support of re-searchers and developers of NLP tools; suchfacilities should support them in finetuningthe instruments they develop to the needsof scholarly research.
CLARIN (http://www.clarin.eu/) is a promising initia-tive in the EU context that is aiming to coverexactly this (and more) for the social sciencesand the humanities.Open access, source and standards to increasethe chances for inter-institutional collabora-tion and exchange of content and tools inaccordance with the policies of the de factoleading bodies, such as TEI (http://www.tei-c.org/) and OAI (http://www.openarchives.org/).Metadata schemata that can accommodateNLP-specific features:?
automatically generated labels and sum-maries?
reliability scores?
indications of the suitability of items fortraining purposesExchange mechanisms for best practices e.g.,of building and updating training data, the13use of annotation tools and the analysis ofquery logs.Protocols and tools for the mark-up of content,the specification of links between collections,the handling of IPR and privacy issues, etc.Service centers that can offer heavy processingfacilities (e.g.
named entity extraction orspeech transcription) for collections kept intechnically modestly equipped environmentshereof.User Interfaces that can flexibly meet the needsof scholarly users for expressing their infor-mation needs, and for visualising relation-ships between interactive information ele-ments (e.g., timelines and maps).Pilot projects in which researchers from vari-ous backgrounds collaborate in analysinga specific digital resource as a centralobject in order to learn to understandhow the interfaces between their fieldscan be opened up.
An interesting ex-ample is the the project Veteran Tapes(http://www.surffoundation.nl/smartsite.dws?id=14040).
Thisinitiative is linked to the interview collectionwhich is emerging as a result for the DutchVeterans Interview-project, which aims atcollecting 1000 interviews with a represen-tative group of veterans of all conflicts andpeace-missions in which The Netherlandswere involved.
The research results will beintegrated in a web-based fashion to formwhat is called an enriched publication.Evaluation frameworks that will trigger contri-butions to the enhancement en tuning of whatNLP has to offer to the needs of the hu-manities.
These frameworks should includebenchmarks addressing tasks and user needsthat are more realistic than most of the ex-isting performance evaluation frameworks.This will require close collaboration betweenNLP developers and scholars.4 ConclusionThe assumption behind presenting these issues aspriorities is that NLP-empowered use of digitalcontent by humanities scholars will be beneficialto both communities.
NLP can use the testbedof the Digital Humanities for the further shapingof that part of the research agenda that covers therole of NLP in information handling, and in par-ticular those avenues that fall under the concept ofmining.
By focussing on the integration of meta-data in the models underlying the mining tools andsearching for ways to increase the involvement ofmetadata generators, both experts and ?amateurs?,important insights are likely to emerge that couldhelp to shape agendas for the role of NLP in otherdisciplines.
Examples are the role of NLP in thestudy of recorded meeting content, in the field ofsocial studies, or the organisation and support oftagging communities in the biomedical domain,both areas where manual annotation by expertsused to be common practise, and both areas wheremining could be done with aggregated collections.Equally important are the benefits for the hu-manities.
The added value of metadata-based min-ing technology for enhanced indexing is not somuch in the cost-reduction as in the wider usabil-ity of the materials, and in the impulse this maybring for sharing collections that otherwise wouldtoo easily be considered as of no general impor-tance.
Furthermore the evolution of digital textsfrom ?book surrogates?
towards the rich semanticlayers and networks generated by text and/or me-dia mining tools that take all available metadatainto account should help the fields involved in notjust answering their research questions more effi-ciently, but also in opening up grey literature forresearch purposes and in scheduling entirely newquestions for which the availability of such net-works are a conditio sine qua non.AcknowledgmentsPart of what is presented in this paper has beeninspired by collaborative work with colleagues.
Inparticular I would like to thank Willemijn Heeren,Roeland Ordelman and Stef Scagliola for their rolein the genesis of ideas and insights.ReferencesW.
Byrne, D.Doermann, M. Franz, S. Gustman, J. Ha-jic, D. Oard, M. Picheny, J. Psutka, B. Ramabhad-ran, D. Soergel, T.Ward, andW-J.
Zhu.
2004.
Auto-matic recognition of spontaneous speech for accessto multilingual oral history archives.
IEEE Transac-tions on Speech and Audio Processing, 12(4).F.
M. G. de Jong, D. W. Oard, W. F. L. Heeren, andR.
J. F. Ordelman.
2008.
Access to recorded inter-14views: A research agenda.
ACM Journal on Com-puting and Cultural Heritage (JOCCH), 1(1):3:1?3:27, June.J.S.
Garofolo, C.G.P.
Auzanne, and E.M Voorhees.2000.
The TREC SDR Track: A Success Story.In 8th Text Retrieval Conference, pages 107?129,Washington.J.
Goldman, S. Renals, S. Bird, F. M. G. de Jong,M.
Federico, C. Fleischhauer, M. Kornbluh,L.
Lamel, D. W. Oard, C. Stewart, and R. Wright.2005.
Accessing the spoken word.
InternationalJournal on Digital Libraries, 5(4):287?298.J.H.L.
Hansen, R. Huang, B. Zhou, M. Deadle, J.R.Deller, A. R. Gurijala, M. Kurimo, and P. Angk-ititrakul.
2005.
Speechfind: Advances in spokendocument retrieval for a national gallery of the spo-ken word.
IEEE Transactions on Speech and AudioProcessing, 13(5):712?730.M.A.H.
Huijbregts, R.J.F.
Ordelman, and F.M.G.de Jong.
2007.
Annotation of heterogeneous multi-media content using automatic speech recognition.In Proceedings of SAMT 2007, volume 4816 ofLecture Notes in Computer Science, pages 78?90,Berlin.
Springer Verlag.M.A.H.
Huijbregts.
2008.
Segmentation, Diarizationand Speech Transcription: Surprise Data Unrav-eled.
Phd thesis, University of Twente.C.
Marshall.
1998.
Toward an ecology of hypertextannotation.
In Proceedings of the ninth ACM con-ference on Hypertext and hypermedia : links, ob-jects, time and space?structure in hypermedia sys-tems (HYPERTEXT ?98), pages 40?49, Pittsburgh,Pennsylvania.W.
McCarty.
2005.
Humanities Computing.
Bas-ingstoke, Palgrave Macmillan.S.
Schreibman, R. Siemens, and J. Unsworth (eds.).2004.
A Companion to Digital Humanities.
Black-well.A.F.
Smeaton, P. Over, and W. Kraaij.
2006.
Evalu-ation campaigns and trecvid.
In 8th ACM SIGMMInternational Workshop on Multimedia InformationRetrieval (MIR2006).J.
Unsworth.
2005.
New Methods for Humanities Re-search.
The 2005 Lyman Award Lecture.
NationalHumanities Center, NC.15
