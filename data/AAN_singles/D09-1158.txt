Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1523?1532,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPDomain adaptive bootstrapping for named entity recognitionDan Wu1, Wee Sun Lee2, Nan Ye21Singapore MIT Alliance2Department of Computer ScienceNational University of Singapore{dwu@,leews@comp,g0701171@}nus.edu.sgHai Leong ChieuDSO National Laboratorieschaileon@dso.org.sgAbstractBootstrapping is the process of improvingthe performance of a trained classifier byiteratively adding data that is labeled bythe classifier itself to the training set, andretraining the classifier.
It is often usedin situations where labeled training data isscarce but unlabeled data is abundant.
Inthis paper, we consider the problem of do-main adaptation: the situation where train-ing data may not be scarce, but belongs toa different domain from the target appli-cation domain.
As the distribution of un-labeled data is different from the trainingdata, standard bootstrapping often has dif-ficulty selecting informative data to add tothe training set.
We propose an effectivedomain adaptive bootstrapping algorithmthat selects unlabeled target domain datathat are informative about the target do-main and easy to automatically label cor-rectly.
We call these instances bridges, asthey are used to bridge the source domainto the target domain.
We show that themethod outperforms supervised, transduc-tive and bootstrapping algorithms on thenamed entity recognition task.1 IntroductionMost recent researches on natural language pro-cessing (NLP) problems are based on machinelearning algorithms.
High performance can oftenbe achieved if the system is trained and tested ondata from the same domain.
However, the perfor-mance of NLP systems often degrades badly whenthe test data is drawn from a source that is differ-ent from the labeled data used to train the system.For named entity recognition (NER), for example,Ciaramita and Altun (2005) reported that a systemtrained on a labeled Reuters corpus achieved anF-measure of 91% on a Reuters test set, but only64% on a Wall Street Journal test set.The task of adapting a system trained on one do-main (called the source domain) to a new domain(called the target domain) is called domain adap-tation.
In domain adaptation, it is generally as-sumed that we have labeled data in the source do-main while labeled data may or may not be avail-able in the target domain.
Previous work in do-main adaptation can be classified into two cate-gories: [S+T+], where a small, labeled target do-main data is available, e.g.
(Blitzer et al, 2006;Jiang and Zhai, 2007; Daum?e III, 2007; Finkel andManning, 2009), or [S+T-], where no labeled tar-get domain data is available, e.g.
(Blitzer et al,2006; Jiang and Zhai, 2007).
In both cases, and es-pecially for [S+T-], domain adaptation can lever-age on large amounts of unlabeled data in the tar-get domain.
In practice, it is often unreasonableto expect labeled data for every new domain thatwe come across, such as blogs, emails, a differentnewspaper agency, or simply articles from a differ-ent topic or period in time.
Thus although [S+T+]is easier to handle, [S+T-] is of higher practicalimportance.In this paper, we propose a domain adaptivebootstrapping (DAB) approach to tackle the do-main adaptation problem under the setting [S+T-].Bootstrapping is an iterative process that uses atrained classifier to label and select unlabeled in-stances to add to the training set for retrainingthe classifier.
It is often used when labeled train-ing data is scarce but unlabeled data is abundant.In contrast, for domain adaptation problems, wemay have a lot of training data but the target ap-plication domain has a different data distribution.Standard bootstrapping usually selects instancesthat are most confidently labeled from the unla-beled data.
In domain adaptation situations, usu-ally the most confidently labeled instances are theones that are most similar to the source domain in-1523stances - these instances tend to contain very littleinformation about the target domain.
For domainadaptive bootstrapping, we propose a selection cri-terion that selects instances that are informativeand easy to automatically label correctly.
In addi-tion, we propose a criterion for stopping the pro-cess of bootstrapping before it adds uninformativeand incorrectly labeled instances that can reduceperformance.Our approach leverages on instances in the tar-get domain called bridges.
These instances con-tain domain-independent features, as well as fea-tures specific to the target domain.
As they containdomain-independent features, they can be classi-fied correctly by classifiers trained on the sourcedomain labeled data.
We argue that these instancesact as a bridge between the source and the targetdomain.
We show that, on the NER task, DABoutperforms supervised, transductive and standardbootstrapping algorithms, as well as a bootstrap-ping variant, called balanced bootstrapping (Jiangand Zhai, 2007), that has recently been proposedfor domain adaptation.2 Related workOne general class of approaches to domain adap-tation is to consider that the instances from thesource and the target domain are drawn from dif-ferent distributions.
Bickel et al (Bickel et al,2007) discriminatively learns a scaling factor forsource domain training data, so as to adapt thesource domain data distribution to resemble thetarget domain data distribution, under the [S+T-]setting.
Daume III and Marcu (Daum?e III andMarcu, 2006) considers that the data distribution isa mixture distribution over general, source domainand target domain data.
They learn the underlyingmixture distribution using the conditional expec-tation maximization algorithm, under the [S+T+]setting.
Jiang and Zhai (2007) proposed an in-stance re-weighting framework that handles boththe [S+T+] and [S+T-] settings.
For [S+T-], theresulting algorithm is a balanced bootstrapping al-gorithm, which was shown to outperform the stan-dard bootstrapping algorithm.
In this paper, weassume the [S+T-] settings, and we show that theapproach proposed in this paper, domain adaptivebootstrapping (DAB), outperforms the balancedbootstrapping algorithm on NER.Another class of approaches to domain adap-tation is feature-based.
Daume III (Daum?e III,2007) divided features into three classes: domain-independent features, source-domain features andtarget-domain features.
He assumed the existenceof training data in the target-domain (under thesetting [S+T+]), so that the three classes of fea-tures can be jointly trained using source and targetdomain labeled data.
This cannot be done in thesetting [S+T-], where no training data is availablein the target domain.
Using a different approach,Blitzer et al (2006) induces correspondences be-tween feature spaces in different domains, by de-tecting pivot features.
Pivot features are featuresthat occur frequently and behave similarly in dif-ferent domains.
Pivot features are used to putdomain-specific features in correspondence.
Inthis paper, instead of pivot features, we attemptto leverage on pivot instances that we call bridges,which are instances that bridge the source and tar-get domain.
This will be illustrated in Section 3.It is generally recognized that adding informa-tive and correctly labeled instances is more usefulfor learning.
Active learning queries the user forlabels of most informative or relevant instances.Active learning, which has been applied to theproblem of NER in (Shen et al, 2004), is used insituations where a large amount of unlabeled dataexists and data labeling is expensive.
It has alsobeen applied to the problem of domain adaptationfor word sense disambiguation in (Chan and Ng,2007).
However, active learning requires humanintervention.
Here, we want to achieve the samegoal without human intervention.3 Bootstrapping for domain adaptationWe first define the notations used for domain adap-tation in the [S+T-] setting.
A set of training dataDS= {xi, yi}1?i?|DS|is given in the source do-main, where the notation |X| denotes the size of aset X .
Each instance xiin DShas been manuallyannotated with a label, yi, from a given set of la-bels Y .
The objective of domain adaptation is tolabel a set of unlabeled data, DT= {xi}1?i?|DT|with labels from Y .
A machine learning algorithmwill take a labeled data set (for e.g.
DS) and out-puts a classifier, which can then be used to classifyunlabeled data, i.e.
assign labels to unlabeled in-stances.A special class of machine learning algorithms,called transductive learning algorithms, is able totake the unlabeled data DTinto account duringthe learning process (see e.g.
(Joachims, 1999)).1524However, such algorithms do not take into accountthe shift in domain of the test data.
Jiang and Zhai(2007) recently proposed an instance re-weightingframework to take domain shift into account.
For[S+T-], the resulting algorithm is a balanced boot-strapping algorithm, which we describe below.3.1 Standard and balanced bootstrappingWe define a general bootstrapping algorithm in Al-gorithm 1.
The algorithm can be applied to anymachine learning algorithm that allows training in-stances to be weighted, and that gives confidencescores for the labels when used to classify testdata.
The bootstrapping procedure iteratively im-proves the performance of a classifier SCtover anumber of iterations.
In Algorithm 1, we have lefta number of parameters unspecified.
These param-eters are (1) the selection-criterion for instances tobe added to the training data, (2) the termination-criterion for the bootstrapping process, and (3) theweights (wS, wT) given to the labeled and boot-strapped training sets.Standard bootstrapping: (Jiang and Zhai,2007) the selection-criterion is based on selectingthe top k most-confidently labeled instances in Rt.The weight wStis equal to wTt.
The value of k is aparameter for the bootstrapping algorithm.Balanced bootstrapping: (Jiang and Zhai,2007) the selection-criterion is still based on se-lecting the top k most-confidently labeled in-stances in Rt.
Balanced bootstrapping was for-mulated for domain adaptation, and hence they setthe weights to satisfy the ratiowStwTt=|Tt||DS|.
Thisallows the small amount of target data added, Tt,to have an equal weight to the large source domaintraining set DS.In this paper, we formulate a selection-criterionand a termination-criterion which are better thanthose used in standard and balanced bootstrap-ping.
Regarding the selection-criterion, standardand balanced bootstrapping both select instanceswhich are confidently labeled by SCtto be usedfor training SCt+1, in the hope of avoiding us-ing wrongly labeled data in bootstrapping.
How-ever, instances that are already confidently labeledby SCtmay not contain sufficient informationwhich is not in DS, and using them to train SCt+1may result in SCt+1performing similarly to SCt.This motivates us to select samples which are bothinformative and easy to automatically label cor-rectly.
Regarding the termination-criterion, whichAlgorithm 1 Bootstrapping algorithmInput: labeled data DS, test data DTand a ma-chine learning algorithm.Output: the predicted labels of the set DT.Set T0= ?, R0= DT, and t = 0Repeat1.
learn a classifier SCtwith (DS, Tt) withweights (wSt, wTt)2. label the set Rtwith SCt3.
select St?
Rtbased on selection-criterion4.
Tt+1= Tt?
St, and Rt+1= Rt\ St.Until termination-criterionOutput the predicted labels of DTby SCt.is not mentioned in the paper (Jiang and Zhai,2007), we assume that bootstrapping is simply runfor either a single iteration, or a small and fixednumber of iterations.
However, it is known thatsuch simple criterion may result in stopping tooearly or too late, leading to sub-optimal perfor-mance.
We propose a more effective termination-criterion here.3.2 Domain adaptive bootstrapping (DAB)Our selection-criterion relies on the observationthat in domain adaptation, instances (from thesource or the target domain) can be divided intothree types according to their information content:generalists are instances that contain only domain-independent information and are present in all do-mains; specialists are instances containing onlydomain-specific information and are present onlyin their respective domains; bridges are instancescontaining both domain-independent and domain-specific information, also present only in their re-spective domains but are useful as a ?bridge?
be-tween the source and the target domains.The implication of the above observation isthat when choosing unlabeled target domain datafor bootstrapping, we should exploit the bridges,because the generalists are not likely to containmuch information not in DSdue to their domain-independence, and the specialists are difficult to belabeled correctly due to their domain-specificity.In contrast, the bridges are informative and eas-ier to label correctly.
Choosing confidently clas-sified instances for bootstrapping, as in standardbootstrapping and balanced bootstrapping, is sim-ple, but results in choosing mostly generalists, andis too conservative.
We design a scoring function1525on instances, which has high value when the in-stance is informative and sufficiently likely to becorrectly labeled in order to identify correctly la-beled bridges.Intuitively, informativeness of an instance canbe measured by the prediction results of the idealclassifier IS for the source domain and the idealclassifier IT for the target domain.
If IS and ITare both probabilistic classifiers, IS should returna noninformative distribution while IT should re-turn an informative one.
The ideal classifier for thesource domain is approximated with a source clas-sifier SC trained on DS, while the ideal classifierfor the target domain is approximated by training aclassifier, TC, on target domain instances labeledby the source classifier.We also try to ensure that instances that are se-lected are correctly classified.
As the label usedis provided by the target classifier, we estimatethe precision of the target classification.
The finalranking function is constructed by combining thisestimate with the informativeness of the instance.We show the algorithm for the instance selec-tion in Algorithm 2.
The notations used followthose used in Algorithm 1.
For simplicity, we as-sume that wSt= wTt= 1 for all t. We expectTC to be a reasonable classifier on DTdue to thepresence of generalists and bridges.
Note that thetarget classifier is constructed by randomly split-ting DTinto two partitions, training a classifieron each partition and using the prediction of thetrained classifier on the partition it is not trainedon.
This is because classifiers tend to fit the datathat they have been trained on too well making theprobability estimates on their training data unreli-able.
Also, a random partition is used to ensurethat the data in each partition is representative ofDu.3.3 The scoring function: score(p(s), p(t))The scoring function score(p(s), p(t)) in Algo-rithm 2 is simply implemented as the product oftwo components: a measure of the informative-ness and the probability that SC?s label is correct.We show how the intuitive ideas (described above)behind these two components are formalized.Informativeness of a distribution p on a set ofdiscrete labels Y is measured by its entropy h(p)defined byh(p) = ?
?y?Yp(y) log p(y).Algorithm 2 Algorithm for selecting instances forbootstrapping at iteration tInput: Labeled source domain data DS, target do-main training data Tt, remaining data Rt, the clas-sifier SCttrained on DS?
Tt, and a scoring func-tion score(p(s), p(t))Output: k instances for bootstrapping.1.
Label Rtwith SCt, and to each instance xi?Rt, SCtoutputs a distribution p(s)i(yi) overits labels.2.
Randomly split Rtinto two partitions, R0tand R1twith their labels assigned by SCt.3.
Train each target classifier, TCxtwith the dataRxt, for x = {0, 1}.4.
Label R(1?x)twith the classifier TCxt, whichto each instance xi?
Rt, outputs a distribu-tion p(t)i(yi) over its labels.5.
Score each instance from xi?
Rtwith thefunction score(p(s)i, p(t)i).6.
Select top k instances from Rtwith the high-est scores.h(p) is nonnegative; h(p) = 0 if and only if phas probability 1 on one of the labels; h(p) attainsits maximum value when the distribution p is uni-form over all labels.
Hence, an instance is clas-sified with high confidence when the distributionover its labels has low entropy.We measure the informativeness of an instanceusing h(p(s))?
h(p(t)), where p(s)and p(t)are asin Algorithm 2.
We argue that a larger value of thisexpression implies that the instance is more likelyto be a bridge instance.
This expression has a highvalue when the source classifier is uncertain, andthe target classifier is certain.
Uncertain classifi-cation by the source classifier indicates that the in-stance is unlikely to be a generalist.
Moreover, ifthe target classifier is certain on xi, it means thatinstances similar to the instance xiare consistentlylabeled with the same label by the source classifierSCt, indicating that it is likely to be a bridge in-stance.The probability that TC?s label is correct can-not be estimated directly because we do not havelabeled target domain data.
Instead, we use thesource domain to give an estimate.
We do this witha simple pre-processing step: we split the data DSinto two partitions of equal size, train a classifieron each partition, and test each classifier on the1526other partition.
We then measure the resulting ac-curacy given each label:?
(y) =# correctly labeled instances of label y# total instances of label y.Summarizing the above discussion, the scoringfunction is as shown below.score(p(s), p(t)) = ?(y?
)[h(p(s))?h(p(t))],where y?= argmaxy?Yp(s)(y)The scoring function has a high value when theinformation content of the example is high and thelabel has high precision.3.4 The termination criterionIntuitively, our algorithm terminates when thereare not enough informative instances.
Formally,we define the termination criterion as follows: weterminate the bootstrapping process when, thereexists an instance xiin the top k instances satis-fying the following condition:1. h(p(s)i) < h(p(t)i), or2.
maxy?Yp(s)i(y) > maxy?Yp(t)i(y)The second case is used to check for instanceswhere the classifier SCtis more confident thanthe target classifiers TCxt, on their respective pre-dicted labels.
This shows that the instance xiismore of a generalist than a bridge.4 NER task and implementationThe algorithm described in Section 3 is not spe-cific to any particular application.
In this paper,we apply it to the problem of named entity recog-nition (NER).
In this section, we describe the NERclassifier and the features used in our experiments.4.1 NER featuresWe used the features generated by the CRF pack-age (Finkel et al, 2005).
These features includethe word string feature, the case feature for the cur-rent word, the context words for the current wordand their cases, the presence in dictionaries for thecurrent word, the position of the current word inthe sentence, prefix and suffix of the current wordas well as the case information of the multiple oc-currences of the current word.
We use the sameset of features for all classifiers used in the boot-strapping process, and for all baselines used in theexperimental section.4.2 Machine learning algorithmsA base machine learning algorithm is required inbootstrapping approaches.
We describe the twomachine learning algorithms used in this paper.We chose these algorithms for their good perfor-mance on the NER task.Maximum entropy classification (MaxEnt):The MaxEnt approach, or logistic regression, isone of the most competitive methods for namedentity recognition (Tjong and Meulder, 2003).MaxEnt is a discriminative method that learns adistribution, p(yi|xi), over the labels, yi, giventhe vector of features, xi.
We used the imple-mentation of MaxEnt classifier described in (Man-ning and Klein, 2003).
For NER, each instancerepresents a single word token within a sentence,with the feature vector xiderived from the sen-tence as described in the previous section.
Max-Ent is not designed for sequence classification.
Todeal with sequences, each name-class (e.g.
PER-SON) is divided into sub-classes: first token (e.g.PERSON-begin), unique token (e.g.
PERSON-unique), or subsequent tokens (e.g.
PERSON-continue) in the name-class.
To ensure that theresults returned by MaxEnt is coherent, we de-fine deterministic transition probabilities that dis-allow transitions such as one from PERSON-beginto LOCATION-continue.
A Viterbi parse is usedto find the valid sequence of name-classes with thehighest probability.Support vector machines (SVM): The basicidea behind SVM for binary classification prob-lems is to consider the data points in their fea-ture space, and to separate the two classes with ahyper-plane, by maximizing the shortest distancebetween the data points and the hyper-plane.
Ifthere exists no hyperplane that can split the two la-bels, the soft margin version of SVM will choosea hyperplane that splits the examples as cleanly aspossible, while still maximizing the distance to thenearest cleanly split examples (Joachims, 2002).We used the SVMlightpackage for our experi-ments (Joachims, 2002).
For the multi-label NERclassification with N classes, we learn N SVMclassifiers, and use a softmax function to obtainthe distribution.
Formally, denoting by s(y) theconfidence returned by the classifier for each labely ?
Y , the probability of the label yiis given byp(yi|xi) =exp(s(yi))?y?Yexp(s(y))1527Similarly to MaxEnt, we subdivide name-classesinto begin, continue, and unique sub-classes, anduse a Viterbi parse for the sequence of highestprobability.
The SVMlightpackage also imple-ments a transductive version of the SVM algo-rithm.
We also compare our approach with thetransductive SVM (Joachims, 1999) in our experi-mental results.5 Experimental resultsIn this paper, we use the annotated data providedby the Automatic Content Extraction (ACE) pro-gram.
The ACE data set is annotated for an EntityDetection task, and the annotation consists of thelabeling of entity names (e.g.
Powell) and men-tions for each entity (e.g.
pronouns such as he).In this paper, we are interested in the problem ofrecognition of the proper names (the named entityrecognition task), and hence use only entities la-beled with the type NAM (LDC, 2005).
Entitiesare classified into seven types: Person entities arehumans mentioned in a document; Organizationentities are limited to established associations ofpeople; Geo-political entities are geographical ar-eas defined by political and/or social groups; Lo-cation entities are geographical items like land-masses and bodies of water; Facility entities re-fer to buildings and real estate improvements; Ve-hicle entities are devices used for transportation;and Weapon entities are devices used for harmingor destruction.We compare performances of a few algorithms:MaxEnt classifier (MaxEnt); MaxEnt classifierwith standard bootstrapping (MaxEnt-SB); bal-anced bootstrapping based on MaxEnt classi-fier (MaxEnt-BB); MaxEnt with DAB (MaxEnt-DAB); SVM classifier (SVM); transductive SVMclassifier (SVM-Trans); and DAB based on SVMclassifier (SVM-DAB).
No regularization is usedfor MaxEnt classifiers.
SVM classifiers use avalue of 10 for parameter C (trade-off betweentraining error and margin).
Bootstrapping basedalgorithms are run for 30 iterations and 100 in-stances are selected in every iteration.The evaluation measure used is the F-measure.F-measure is the harmonic mean of precision andrecall, and is commonly used to evaluate NERsystems.
We use the scorer for CONLL 2003shared task (Tjong and Meulder, 2003) where theF-measure is computed by averaging F-measuresfor name-classes, weighted by the number of oc-Code Source Num docsNW Newswire 81BC Broadcast conversation 52WL Weblog 114CTS Conversational Telephone Speech 34Table 1: The sources, and the number of docu-ments in each source, in the ACE 2005 data set.currences.5.1 Cross-source transferThe ACE 2005 data set consists of articles drawnfrom a variety of sources.
We use the four cate-gories shown in Table 1.
Each category is consid-ered to be a domain, and we consider each pair ofcategories as the source and the target domain inturn.Figure 1 compares the performance of MaxEnt-SB, MaxEnt-BB and MaxEnt-DAB over multipleiterations.
Figure 2 compares the performanceof SVM, SVM-Trans and SVM-DAB.
Each linein the figures represents the average F-measureacross all the domains over many iterations.
Whenthe termination condition is met for one domain,its F-measure remains at the value of the final iter-ation.Despite a large number of iterations, both stan-dard and balanced bootstrapping fail to improveperformance.
Supervised learning performance oneach domain is shown in Table 3 (by 2-fold cross-validation with random ordering) as a reference.In Table 5, we compare the F-measures obtainedby different algorithms at the last iteration theywere run.
We will discuss more on this in Sec-tion 5.3.5.2 Cross-topic transferThis data set is constructed from 175 articles fromthe ACE 2005 corpus.
The data set is used to eval-uate transfer across topics.
We manually classifythe articles into 4 categories: military operations(MO), political relationship or politicians (POL),terrorism-related (TER), and those which are notin the above categories (OTH).
A detailed break-down of the number of documents in the eachtopic is given in Table 2.Supervised learning performance on each do-main is shown in Table 4 (by 2-fold cross-validation with random ordering) as a reference.Experimental results on cross-topic evaluation areshown in Table 6.
Figure 3 compares the perfor-mance of MaxEnt-SB, MaxEnt-BB and MaxEnt-152857.558.058.559.059.560.060.561.061.562.00  5  10  15  20  25  30  35F-measureNumber of iterationsMaxEnt-DABMaxEnt-SBMaxEnt-BBFigure 1: Average performance on the cross-source transfer using MaxEnt classifier.35.040.045.050.055.060.065.070.01  2  3  4  5  6F-measureNumber of iterationsSVM-DABSVMSVM-TransFigure 2: Average performance on the cross-source transfer using SVM classifier.66.266.466.666.867.067.267.467.667.868.068.20  5  10  15  20  25  30  35F-measureNumber of iterationsMaxEnt-DABMaxEnt-SBMaxEnt-BBFigure 3: Average performance on the cross-topictransfer using MaxEnt classifier.56.058.060.062.064.066.068.070.072.01  2  3  4F-measureNumber of iterationsSVM-DABSVMSVM-TransFigure 4: Average performance on the cross-topictransfer using SVM classifier.Topic Topic description # docsMO Military operations 92POL Political relationships 40TER Terrorist-related 28OTH None of the above 15Table 2: The topics, their descriptions, and thenumber of training and test documents in eachtopic.Domain MaxEnt SVMNW 82.47 82.32BC 78.21 77.91WL 71.41 71.84CTS 93.90 94.01Table 3: F-measure of supervised learning on thecross-source target domains.DAB over multiple iterations.
Figure 4 comparesthe performance of SVM, SVM-Trans and SVM-DAB.
Similar to cross-source transfer, standardand balanced bootstrapping perform badly.
Thiswill be discussed in Section 5.3.Domain MaxEnt SVMMO 80.52 80.6POL 77.99 79.05TER 81.74 82.12OTH 71.33 72.08Table 4: F-measure of supervised learning on thecross-topic target domains.5.3 DiscussionWe show in our experiments that DAB outper-forms standard and balanced bootstrapping, aswell as the transductive SVM.
We have also shownDAB to be robust across two state-of-the-art clas-sifiers, MaxEnt and SVM.
Balanced bootstrappinghas been shown to be more effective for domainadaptation than standard bootstrapping (Jiang andZhai, 2007) for named entity classification on asubset of the dataset used here.
In contrast, wefound that both methods perform poorly on do-main adaptation for NER.
In named entity clas-sification, the names have already been segmentedout and only need to be classified with the appro-priate class.
However, for NER, the names also1529Train Test MaxEnt MaxEnt-SB MaxEnt-BB MaxEnt-DAB SVM SVM-Trans SVM-DABBC CTS 74.26 74.19 74.16 81.03 72.47 43.27 75.43BC NW 64.81 64.76 64.80 66.20 64.08 43.01 64.39BC WL 47.81 47.80 47.76 49.52 47.98 36.58 47.93CTS BC 46.19 46.12 46.40 54.62 46.02 40.44 49.64CTS NW 54.25 54.15 54.26 53.07 55.63 23.61 58.99CTS WL 40.42 40.43 40.72 41.27 39.96 29.05 42.04NW BC 59.90 59.83 59.80 60.55 59.89 45.71 58.42NW CTS 66.64 66.48 66.59 66.73 68.28 28.80 73.47NW WL 52.52 52.53 52.47 53.44 52.19 36.39 52.30WL BC 58.58 58.79 58.65 56.00 58.43 52.64 58.64WL CTS 64.63 63.89 64.50 80.45 65.96 45.04 81.04WL NW 67.79 67.72 67.92 68.46 68.38 43.40 69.33Average 58.15 58.06 58.17 60.95 58.27 39.00 60.97Table 5: F-measure of the cross-source transfer.Train Test MaxEnt MaxEnt-SB MaxEnt-BB MaxEnt-DAB SVM SVM-Trans SVM-DABMO OTH 81.70 81.48 81.57 81.95 81.78 75.68 81.94MO POL 73.21 73.11 73.28 74.97 72.56 58.13 72.66MO TER 68.13 68.07 68.24 69.89 69.40 65.02 69.38OTH MO 63.30 63.80 63.94 63.91 64.18 61.03 65.45OTH POL 67.96 68.05 67.86 69.13 68.29 56.50 70.67OTH TER 45.34 44.82 45.30 51.06 45.71 48.77 52.87POL MO 62.14 62.12 61.95 61.94 61.98 51.67 62.32POL OTH 77.91 77.72 77.79 76.58 78.11 65.71 78.13POL TER 66.55 66.38 66.08 66.38 66.44 51.29 67.24TER MO 58.35 58.62 58.02 57.29 58.30 49.80 58.14TER OTH 66.83 67.61 66.83 68.97 66.28 58.25 68.12TER POL 67.34 66.94 67.16 72.00 67.54 50.55 70.65Average 66.56 66.56 66.50 67.84 66.71 57.70 68.13Table 6: F-measure of the cross-topic transfer.need to be separated from not-a-name instances.We find that the addition of not-a-name instanceschanges the problem - the not-a-names form mostof the instances classified with high confidence.As a result, we find that both standard and bal-anced bootstrapping fail to improve performance:the selection of the most confident instances nolonger provide sufficient new information to im-prove performance.We also find that transductive SVM performspoorly on this task.
This is because it assumesthat the unlabeled data comes from the same dis-tribution as the labeled data.
In general, apply-ing semi-supervised learning methods directly to[S+T-] type domain adaptation problems do notwork and appropriate modifications need to bemade to the methods.The ACE 2005 data set alo contains a set ofariticles from the broadcast news (BN) sourcewhich is written entirely in lower case.
This makesNER much more difficult.
However, when BN isthe source domain, the capitalization informationcan be discovered by DAB.
Figures 5 and 6 showthe average performance when BN is used as thesource domain and all other domains in Table 1 asthe target domains.The source domain classifier tends to have highprecision and low recall, DAB results in an in-crease in recall, with a small decrease in precision.Testing the significance of the F-measure is nottrivial because the named entities wrongly labeledby two classifiers are not directly comparable.
Wetested the labeling disagreements instead, using aMcNemar paired test.
The significance test is per-formed on the improvement of MaxEnt-DAB overMaxEnt and SVM-DAB over SVM.
In most ofthe domains for the cross-source transfer, the im-provements are significant at a significance levelof 0.05, using MaxEnt classifier.
The exceptionaltrain-test pairs are NW-WL and WL-BC.
In thecase of WL-BC, this means the slight decrement inperformance is not statistically significant.
Similarresult is achieved for the cross-source transfer us-ing SVM classifier.
In the cross-topic transfer, thesource domain and the target domain are not verydifferent.
When we have a large amount of train-ing data and little testing data, the gain of DABcan be not statistically significant, as in the casewhen we train with MO and POL domains.153020.025.030.035.040.045.050.01  2  3  4  5  6  7  8  9F-measureNumber of iterationsMaxEnt-DABMaxEnt-SBMaxEnt-BBFigure 5: Performance on recovering capitaliza-tion using MaxEnt classifier.28.030.032.034.036.038.040.042.044.046.01  2  3  4F-measureNumber of iterationsSVM-DABSVMSVM-TransFigure 6: Performance on recovering capitaliza-tion using SVM classifier.6 ConclusionWe proposed a bootstrapping approach for domainadaptation, and we applied it to the named entityrecognition task.
Our approach leverages on in-stances that serve as bridges between the sourceand target domain.
Empirically, our method out-performs baseline approaches including super-vised, transductive and standard bootstrapping ap-proaches.
It also outperforms balanced bootstrap-ping, an approach designed for domain adaptation(Jiang and Zhai, 2007).ReferencesSteffen Bickel, Michael Br?uckner, and Tobias Scheffer.2007.
Discriminative learning for differing trainingand test distributions.
In ICML ?07: Proceedings ofthe 24th international conference on Machine learn-ing, pages 81?88, New York, NY, USA.
ACM Press.John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In Conference on Empirical Meth-ods in Natural Language Processing, Sydney, Aus-tralia.Yee Seng Chan and Hwee Tou Ng.
2007.
Do-main adaptation with active learning for word sensedisambiguation.
In Proceedings of the 45th An-nual Meeting of the Association of ComputationalLinguistics, pages 49?56, Prague, Czech Republic,June.
Association for Computational Linguistics.Massimiliano Ciaramita and Yasemin Altun.
2005.Named-entity recognition in novel domains with ex-ternal lexical knowledge.
In Advances in StructuredLearning for Text and Speech Processing Workshop.Hal Daum?e III and Daniel Marcu.
2006.
Domain adap-tation for statistical classifiers.
Journal of ArtificialIntelligence Research, 26:101?126.Hal Daum?e III.
2007.
Frustratingly easy domain adap-tation.
In Conference of the Association for Compu-tational Linguistics (ACL), Prague, Czech Republic.Jenny Rose Finkel and Christopher D. Manning.
2009.Hierarchical bayesian domain adaptation.
In Pro-ceedings of the Human Language Technology Con-ference of the NAACL, Main Conference, New YorkCity, USA.
Association for Computational Linguis-tics.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by gibbssampling.
In ACL ?05: Proceedings of the 43rdAnnual Meeting on Association for ComputationalLinguistics, pages 363?370, Morristown, NJ, USA.Association for Computational Linguistics.Jing Jiang and ChengXiang Zhai.
2007.
Instanceweighting for domain adaptation in nlp.
In Pro-ceedings of the 45th Annual Meeting of the Associ-ation of Computational Linguistics, pages 264?271,Prague, Czech Republic, June.
Association for Com-putational Linguistics.Thorsten Joachims.
1999.
Transductive inference fortext classification using support vector machines.
InICML ?99: Proceedings of the Sixteenth Interna-tional Conference on Machine Learning, pages 200?209, San Francisco, CA, USA.
Morgan KaufmannPublishers Inc.T.
Joachims.
2002.
Learning to Classify Text UsingSupport Vector Machines ?
Methods, Theory, andAlgorithms.
Kluwer/Springer.Linguistic Data Consortium LDC.
2005.
ACE(Automatic Content Extraction) English AnnotationGuidelines for Entities.Christopher Manning and Dan Klein.
2003.
Opti-mization, maxent models, and conditional estima-tion without magic.
In NAACL ?03: Proceedings ofthe 2003 Conference of the North American Chapterof the Association for Computational Linguistics on1531Human Language Technology, pages 8?8, Morris-town, NJ, USA.
Association for Computational Lin-guistics.Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, andChew-Lim Tan.
2004.
Multi-criteria-based ac-tive learning for named entity recognition.
In Pro-ceedings of the 42nd Meeting of the Association forComputational Linguistics (ACL?04), Main Volume,pages 589?596, Barcelona, Spain, July.Erik Tjong and Fien De Meulder.
2003.
Intro-duction to the conll-2003 shared task: Language-independent named entity recognition.
In Proceed-ings of Conference on Computational Natural Lan-guage Learning.1532
