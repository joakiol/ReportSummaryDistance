Proceedings of the NAACL HLT 2010 Student Research Workshop, pages 46?51,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsIdentifying Opinion Holders and Targets with Dependency Parser inChinese News TextsBin LuDepartment of Chinese, Translation and Linguistics &Language Information Sciences Research CentreCity University of Hong KongKowloon, Hong Konglubin2010@gmail.comAbstractIn this paper, we propose to identifyopinion holders and targets withdependency parser in Chinese news texts,i.e.
to identify opinion holders by means ofreporting verbs and to identify opiniontargets by considering both opinion holdersand opinion-bearing words.
Theexperiments with NTCIR-7 MOAT?sChinese test data show that our approachprovides better performance than thebaselines and most systems reported atNTCIR-7.1 IntroductionIn recent years, sentiment analysis, which minesopinions from information sources such as news,blogs and product reviews, has drawn muchattention in the NLP field (Hatzivassiloglou andMcKeown, 1997; Pang et al, 2002; Turney, 2002;Hu and Liu, 2004).An opinion expressed in a text involves differentcomponents, including opinion expression, opinionholder and target (Wilson and Wiebe, 2003).Opinion holder is usually an entity that holds anopinion, and opinion target is what the opinion isabout (Kim and Hovy, 2006).
Although there havebeen research on identifying opinion holders andtargets in  English product reviews and news texts,little work has been reported on similar tasksinvolving Chinese news texts.In this study, we investigate how dependencyparsing can be used to help the task on opinionholder/target identification in Chinese news texts.Three possible contributions from this study are: 1)we propose that the existence of reporting verbs isa very important feature for identifying opinionholders in news texts, which has not been clearlyindicated; 2) we argue that the identification ofopinion targets should not be done alone withoutconsidering opinion holders, because opinionholders are much easier to be identified in newstexts and the identified holders are quite useful forthe identification of the associated targets.
Ourapproach shows encouraging performance onopinion holder/target identification, and the resultsare much better than the baseline results and mostresults reported in NTCIR-7 (Seki et al, 2008).The paper is organized as follows.
Sec.
2introduces related work.
Sec.
3 gives the linguisticanalysis of opinion holder/target.
The proposedapproach is described in Sec.
4, followed by theexperiments in Sec.
5.
Lastly we conclude in Sec.
6.2 Related WorkAlthough document-level sentiment analysis(Turney, 2002; Pang et al, 2002) can provide theoverall polarity of the whole text, it fails to detectthe holders and targets of the sentiment in texts.2.1 Opinion Holders/ Target IdentificationFor opinion mining of product reviews, opinionholder identification is usually omitted under theassumption that opinion holder is the review writer;and opinion targets are limited to the productdiscussed and its features (Hu and Liu, 2004).
Butin news texts, opinion holders/targets are morediverse: all named entities and noun phrases can beopinion holders; while opinion targets could benoun phrases, verb phrases or even clauses (Kimand Hovy, 2006; Ruppenhofer et al 2008).Bethard et al (2004) identify opinionpropositions and their holders by semantic parsingtechniques.
Choi et al (2005) and Kim and Hovy(2005) identify only opinion holders on the MPQAcorpus (Wilson and Wiebe, 2003).
Kim and Hovy(2006) proposed to map the semantic frames ofFrameNet into opinion holder and target for onlyadjectives and verbs.
Kim et al (2008) proposed to46use syntactic structures for target identificationwithout considering opinion holders.
Stoyanov andCardie (2008) define opinion topic and target andtreat the task as a co-reference resolution problem.For the identification of opinion holders/targetsin Chinese, there were several reports at NTCIR-7(Seki et al, 2008).
Xu et al (2008) proposed to usesome heuristic rules for opinion holder/targetidentification.
Ku et al (2008) treated opinionholder identification as a binary classificationproblem of determining if a word was a part of anopinion holder.2.2 Chinese Dependency ParsingDependency structures represent all sentencerelationships uniformly as typed dependencyrelations between pairs of words.
Some majordependency relations for Chinese (Ma et al, 2004)include ??
(Subject-Verb, SBV), ??
(Verb-Object,VOB), ??
(Attributive-Noun, ATT), ??
(Quantifier,QUN) and ???
?
(Independent structure, IS).Consider the following Chinese sentence:a) ??
??
????
????
??
???
?
??
??
?
??
?
?Russian Foreign Minister Ivanov said thatNATO's eastward expansion was "Towards thewrong direction.
"Its dependency tree is shown in Figure1.
Its headis the verb?
(said), whose subject and object arerespectively ????????
(Russian ForeignMinister Ivanov) and the embedded clause ????????????????
(NATO's eastwardexpansion was "towards the wrong direction.
").3 Linguistic Analysis of OpinionsThe opinions in news text may be explicitlymentioned or be expressed indirectly by the typesof words and the style of language (Wilson andWiebe, 2003).
Two kinds of lexical clues areexploited here for opinion holder/targetidentification:Reporting verbs: verbs indicating speechevents;Opinion-bearing Words: words or phrasescontaining polarity (i.e.
positive, negative orneutral).In sentence a) above, the reporting verb ?
(said)indicates a speech event expressing an opiniongiven by the holder ??
??
????
(RussianForeign Minister Ivanov).
Meanwhile, the opinion-bearing word ?
?
(wrong) shows negativeattitude towards the target ?
?
?
?
?
?
(NATO's eastward expansion).Therefore, we assume that a large proportion ofholders are governed by such reporting verbs,while targets are usually governed by opinion-bearing words/phrases.Opinion holders are usually named entities,including, but not limited to, person names (e.g.
??????
/economist Ol), organization names(e.g.
????
/UK government), and personaltitles (e.g.
????
/the economist).
Opinionholders can also be common noun phrases, such as??
(companies), ?????
(two thousandstudents).
Pronouns1 can also be opinion holders,e.g.
?
(he), ??
(they), ?(I).
Opinion targets aremore abstract and diverse, and could be agents,concrete objects, actions, events or even abstractideas.
In addition to noun phrases, opinion targetscould also be verb phrases or embedded clauses.4 Identifying Opinion Holders/TargetsIn this section, we introduce our approach ofidentifying opinion holders/targets.
We use thedependency parser in the HIT LTP package(http://ir.hit.edu.cn/) to get the dependencyrelations of the simplified Chinese sentencesconverted from the traditional Chinese ones.4.1 Lexical ResourcesThe reporting verbs were firstly collected from theChinese sample data of NTCIR-6 OAPT (Seki etal., 2007) in which the OPINION_OPR tag wasused to mark them.
We then use HowNet,WordNet and Tongyici Cilin to extend thereporting verbs from 68 to 308 words throughmanual synonym search.
Some frequently usedreporting verbs include ?
(say), ??
(express), ??
(think), etc.
Some of the reporting verbs couldalso convey opinions, such as ??
(criticize), ??
(condemn), ??
(praise), etc.For opinion-bearing words/phrases, we use TheLexicon of Chinese Positive Words (Shi and Zhu,2006) and The Lexicon of Chinese Negative Words(Yang and Zhu, 2006), which consist of 5046positive items and 3499 negative ones, respectively.1 The resolution of the anaphor or co-reference has not beendealt with yet, i.e.
the identified holders of the sentence areassumed to be in the same form as it appears in the sentence.47Figure 1.
Dependency Tree for Sentence a)4.2 Chinese Sentence Preprocessing (SP)To enhance the robustness of the dependencyparser, named entities are first recognized with atraditional Chinese word segmentation tool withaccess to the very large LIVAC dictionary(http://www.livac.org) collected from Chinesenews published in Hong Kong and Taiwan.
Theidentified named entities, as well as the collectedreporting verbs and opinion-bearing words areadded to the user dictionary of the HIT LTPpackage to help parsing.Before parsing, the parentheses enclosing onlyEnglish words or numbers are removed insentences, because the parser cannot properlyprocess the parentheses which may greatlyinfluence the parsing result.4.3 Identifying Opinion Holders withReporting Verbs4.3.1 Holder Candidate GenerationTwo hypotheses are used to identify opinionholders in opinionated sentences: 1) the subject ofreporting verbs will be the opinion holders; 2) if noreporting verb is found, the author could be theopinion holder.
In addition to the two hypothesesabove, the following heuristic rules (HR) are used:1) Other words having relations with reportingverbsIf the subject of reporting verbs is not found inthe sentence, we will find the word havingrelationship of ATT, VOB or IS with the reportingverbs, because sometimes the parser may wronglymarked the subject as other relations.2) Colon processing in HeadlinesIf no reporting verbs are found in newsheadlines, we just pick up the noun before thecolon as the target candidate in the headlinesbecause the author usually replaces the reportingverb with a colon due to length limitation.
E.g.
inthe headline ?????????
(Morgan:Economic growth has been shut down), the noun??
(Morgan) before colon is chosen as theopinion holder.3) Holder in the previous sentenceIf no opinion holder is found in the currentclause and one holder candidate is found in theprevious clause, we just choose the opinion holderof the previous clause as the holder candidate,because an opinion holder may express severalideas through consecutive sentences or clauses.4.3.2 Holder Candidate Expansion (EP)Through the procedure of candidate generation, wemay find a holder candidate containing only onesingle word.
But the holder may be a wordsequence instead of a single word.
Thus we furtherexpand the holder candidates from the core headword by the following rules:1) Attributive modifier (ATT)E.g.
in sentence a) mentioned in Sec.
2.2, thesubject of the reporting verb?
(said) is ????
(Ivanov), which has the attributive noun ??
(Foreign Minister) modified further by anattributive noun ??(Russia).
Therefore, the finalextended opinion holder would be ??????
(Russian Foreign Minister Ivanov).2) Quantifier modifier and ?/?
(and/or)E.g.
the quantifier modifier ??
(some) in thenoun phrase ??????
(some Asian countries)should be part of the opinion holder.
Sometime, weneed to extend the holder across ?/?
(and/or), e.g.????????????
(Suharto and twoother army generals).Furthermore, time nouns, numbers and wordsonly containing one Chinese character (except forpronouns) are removed from the candidates, asthey are unlikely to be opinion holders.4.4 Identifying Opinion Targets withOpinion-bearing Words48Here we propose to use automatically identifiedreporting verbs and opinion holders to help opiniontarget identification.
The heuristic rules (HR) areas follows.1) If a candidate of opinion holder isautomatically identified with a reporting verb in anopinionated sentence, we will try to find thesubject in the embedded clause as the targetcandidate by the following two steps: a) Find thesubject of the object verb of the reporting verb.
E.g.in sentence a) in Sec.
2.2, the opinion target ??????
(NATO's eastward expansion) is thesubject of the verb ?
(was) in the embeddedclause which is in turn the object of the reportingverb?
(said); b) If no target candidate is found instep a, we try to find after the reporting verb thesubject whose parent is an opinion-bearing word asthe target candidate.2) If no target candidate is found in step 1, andno opinion holder is found in the sentence, we findthe subject of the sentence as the target candidate,because the author may be the opinion holder andthe target could be the subject of the sentence.3) If still no target candidate is found in step 2,we find the object in the sentence as the targetbecause the object could be the opinion target incase there is no subject and no opinion holder.Target candidate expansion (EP) is similar toholder candidate expansion described in Sec.
4.3.2.If an opinion target is in the opinion holdercandidates (we call it holder conflict, HC), weremove it from the target candidates, and then tryto find another using the above procedure.5 ExperimentsWe use the traditional Chinese test data in NTCIR-7 MOAT (Seki et al, 2008) for our experiments.Out of 4465 sentences, 2174 are annotated asopinionated by the lenient standard, and theopinion holders of some opinionated sentences aremarked as POST_AUTHOR denoting the author ofthe news article.
We use the final list given by theorganizers as the gold standard.Baselines for opinion holder identification:Baseline 1: We just use the subject of reportingverbs as the opinion holder, without sentencepreprocessing described in Sec.
4.2 and anyheuristic rules introduced in Sec.
4.3.1.Baseline 2: We also implement the CRF modelfor detecting opinion holders (Choi et al, 2006) byusing CRF++.
The training data is the NTCIR-6Chinese test data.
The labels used by CRFcomprise Holder, Parent of Holder, None (notholder or parent) and the features for each word inour implementation include: basic features (i.e.word, POS-tag, whether the word itself is areporting verb or not), dependency features (i.e.parent word, POS-tag of its parent, dependencyrelation with its parent, whether its parent is areporting verb) and semantic features (i.e.
WSDentry in Tongyici Cilin, WSD entry of its parent).Baseline for opinion target identification:Baseline 1: we try to find the subject or object ofopinion-bearing words as the targets.
If both asubject and an object are found, we just simplychoose the subject as the target.We evaluate performance using 3 measures:exact match (EM), head match (HM), and partialmatch (PM), similar to Choi et al (2006).
We usethree evaluation metrics: recall (Rec), precision(Pre), and F1.
For opinion holder identification, weconsider two cases: 1) all opinionated sentences; 2)only the opinionated sentences whose opinionholders do not contain POST_AUTHOR.
Themetric ALL_Pre reported below is the precision incase 1 which is the same with recall and F1.5.1 Results for Opinion Holder IdentificationThe results for holder identification are shown inTable 1, from which we can observe that ourproposed approach significantly outperforms thetwo baseline methods, including the unsupervisedbaseline 1 and the supervised baseline 2.ALL_Pre Pre Rec F1EM 52.4 46.8 31.6 37.8HM 67.1 80.2 54.2 64.7 Baseline1  PM 72.1 89.3 60.4 72.0EM 45.5 34.7 18.1 23.8HM 55.2 63.6 33.1 43.6 Baseline2 (CRF) PM 55.6 64.9 33.8 44.4EM 69.8 74.4 63.6 68.5HM 72.5 79.2 67.7 73.0 OurApproach PM 75.7 85.1 72.7 78.4Table 1.
Results for Opinion HoldersUnexpectedly, even the unsupervised baseline 1achieves better performance than baseline 2 (theCRF-based method).
The possible reasons are: 1)the training data is not large enough to cover thecases in the test data, resulting in low recall of theCRF model; 2) the features used by the CRF modelcould be refined to improve the performance.49Here we also evaluate the influences of thefollowing three factors on the performance:sentences preprocessing (SP) in Sec.
4.2, holderexpansion (EP) in Sec.
4.3.2 and the heuristic rules(HR) in Sec.
4.3.1.
The results are shown in Figure2 for different combinations, in which BL refers tobaseline 1.Influence of Factors3540455055606570758085BL BL+SP BL+EP BL+SP+EP Our Approach(BL+SP+EP+HR)ApproachesF1EMHMPMFigure 2.
Influences of Factors on Opinion HoldersFrom Figure 2, we can observe that: 1) All threefactors have positive effects on performancecompared to baseline 1, and our approach byintegrating all factors achieves the bestperformance; 2) SP improve the performance interms of all three metrics, showing that SPincluding named entity recognition and parenthesisremoving are useful for holder identification; 3)The major improvement of EP lies in EM, showingthat the main contribution of EP is to get the exactopinion holders by expanding the core head noun;4) SP+EP+HR improves the performance in termsof all three metrics compared with SP+HR,showing the heuristic rules are useful to improvethe performance.5.2 Results for Opinion Target IdentificationThe results for opinion target identification areshown in Table 2, from which we can observe thatour proposed approach significantly outperformsthe baseline method.Pre Rec F1EM 11.1 9.2 10.1HM 24.0 19.9 21.8 Baseline 1  PM 39.4 32.7 35.8EM 29.3 28.5 28.9HM 38.4 38.0 38.2OurApproachPM 59.3 58.7 59.0Table 2.
Results for Opinion TargetsWe also investigate the influences of thefollowing four factors on the performance:sentence preprocessing (SP) in Sec.
4.2, targetexpansion (EP) in Sec.
4.4, holder conflict (HC),the heuristic rules (HR) proposed in Sec.
4.4.
TheF1s for EM, HM and PM are shown in Figure 3, inwhich BL refers to baseline 1.Influence of Factors8182838485868BL BL+SP BL+EP BL+HC BL+SP+EP+HCOur ApproachApproachesF1EMHMPMFigure 3.
Influences of Factors on Opinion TargetsFrom Figure 3, we can observe that: 1) All fourfactors have positive effects on performancecompared to the baseline, and our approachintegrating all the factors achieves the bestperformance; 2)  EP significantly improves F1 ofEM without much improvement on F1 of HM orPM, showing that EP?s major contribution lies inexact match; 3) The major contribution of HC isthe improvement of F1s of HM and PM, showingthe automatically identified opinion holders arequite helpful for finding opinion targets; 4)SP+EP+HC improves the performance in terms ofall three metrics; and our approach furtherimproves the performance by adding HR.5.3 DiscussionHere we compare our results with those reported atNTCIR-7 MOAT traditional Chinese test (Seki etal., 2008).
Without considering the errors in theprevious step, the highest F1s for opinion holderanalysis reported by the four participants wererespectively 82.5%, 59.9%, 50.3% and 59.5%, andthe highest F1s for target reported by the threeparticipants were respectively 60.6%, 2.1% and3.6%.
Compared to the results at NTCIR-7, ourperformances on both opinion holder identificationin Table 1 and that on target identification in Table2 seem quite encouraging even by the EM metrics.Consider the evaluation for opinionholders/targets was semi-automatic at NTCIR-7.We should note that although the generatedstandard had been supplemented by theparticipants?
submissions, some correct answersmay still be missing, especially for targets sinceonly three teams participated in the target50identification task and the recalls were not high.Thus the performance reported in Table 1 and 2may be underestimated.Here we also give an estimate on thepercentages of opinionated sentences containingboth opinion holders and at least one reportingverb in NTCIR-6 and NTCIR-7?s traditionalChinese test data, which are respectively 94.5%and 83.9%.
The high percentages show thatreporting verbs are very common in news report.6 Conclusion and Future WorkIn this paper, we investigate the problem ofidentifying opinion holders/targets in opinionatedsentences of Chinese news texts based on Chinesedependency parser, reporting verbs and opinion-bearing words.
Our proposed approach showsencouraging performance on opinion holder/targetidentification with the NTCIR-7?s traditionalChinese test data, and outperforms most systemsreported at NTCIR-7 and the baseline methodsincluding the CRF-based model.The proposed approach is highly dependent ondependency parser, and we would like to furtherinvestigate machine learning approaches (includingthe CRF model) by treating dependency structuresas one of the linguistic features, which could bemore robust to parsing errors.
Opinion targets aremore difficult to be identified than opinion holders,and deserve more attention in the NLP field, andwe also would extend the targets to verb phrasesand embedded clauses in addition to noun phrases.To explore the effectiveness of our approach withEnglish data such as MPQA is another direction.AcknowledgementsWe acknowledge the help of our colleagues(Professor Benjamin K. Tsou and Mr. Jiang Tao).ReferenceSteven Bethard, Hong Yu, Ashley Thornton, VasileiosHatzivassiloglou, and Dan Jurafsky.
2004.
AutomaticExtraction of Opinion Propositions and their Holders,AAAI Spring Symposium on Exploring Attitude andAffect in Text: Theories and Applications.Yejin Choi, Claire Cardie, Ellen Riloff, and SiddharthPatwardhan.
2005.
Identifying sources of opinionswith conditional random fields and extractionpatterns.
In Proc.
of HLT/EMNLP-05.Vasileios Hatzivassiloglou and Kathleen McKeown.1997.
Predicting the Semantic Orientation ofAdjectives.
In Proc.
of ACL-97.
174-181.Minqing Hu and Bing Liu.
2004.
Mining OpinionFeatures in Customer Reviews.
In Proc.
of AAAI-04.Soo-Min Kim and Eduard Hovy.
2005.
IdentifyingOpinion Holders for Question Answering in OpinionTexts, In Proc.
of AAAI-05 Workshop on QuestionAnswering in Restricted Domains.
Pittsburgh, PA.Soo-Min Kim and Eduard Hovy.
2006.
Extractingopinions, opinion holders, and topics expressed inonline news media text, In Proc.
of ACL Workshopon Sentiment and Subjectivity in Text.Youngho Kim, Seongchan Kim, and Sung-HyonMyaeng.
2008.
Extracting Topic-related Opinionsand their Targets in NTCIR-7, Proc.
of NTCIR-7Workshop, Tokyo, Japan.Lun-Wei Ku, I-Chien Liu, Chia-Ying Lee, Kuan-huaChen and Hsin-Hsi Chen.
2008.
Sentence-LevelOpinion Analysis by CopeOpi in NTCIR-7.
In Proc.of NTCIR-7 Workshop.
Tokyo, Japan.Jinshan Ma, Yu Zhang, Ting Liu and Sheng Li.
2004.
Astatistical dependency parser of Chinese under smalltraining data.
IJCNLP-04 Workshop: Beyond shallowanalyses - Formalisms and statistical modeling fordeep analyses.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
Sentiment classification usingmachine learning techniques.
In Proc.
of EMNLP-02.Josef Ruppenhofer, Swapna Somasundaran, JanyceWiebe.
2008.
Finding the Sources and Targets ofSubjective Expressions.
In Proc.
of LREC 2008.Yohei Seki, David Kirk Evans, Lun-Wei Ku, and et al2007.
Overview of Opinion Analysis Pilot Task atNTCIR-6.
Proc.
of the NTCIR-6 Workshop.Yohei Seki, David Kirk Evans, Lun-Wei Ku, and et al2008.
Overview of Multilingual Opinion AnalysisTask at NTCIR-7.
Proc.
of the NTCIR-7 Workshop.Japan.
2008.
12.Jilin Shi and Yinggui Zhu.
2006.
The Lexicon ofChinese Positive Words (?????
).
SichuanLexicon Press.Veselin Stoyanov and Claire Cardie.
2008.
TopicIdentification for Fine-Grained Opinion Analysis.
InProc.
of COLING-08.Peter D. Turney.
2002.
Thumbs up or thumbs down?Semantic orientation applied to unsupervisedclassification of reviews, In Proc.
of ACL-02.Theresa Wilson and Janyce Wiebe.
2003.
Annotatingopinions in the world press.
Proc.
of the 4th ACLSIGdial Workshop on Discourse and Dialogue(SIGdial-03).Ruifeng Xu, Kam-Fai Wong and Yunqing Xia.
2008.Coarse-Fine Opinion Mining - WIA in NTCIR-7MOAT Task.
Proc.
of the 7th NTCIR Workshop.Ling Yang and Yinggui Zhu.
2006.
The Lexicon ofChinese Negative Words (?????
).
SichuanLexicon Press.51
