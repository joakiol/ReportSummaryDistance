Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 620?628,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsUnsupervised Approaches for Automatic Keyword Extraction UsingMeeting TranscriptsFeifan Liu, Deana Pennell, Fei Liu and Yang LiuComputer Science DepartmentThe University of Texas at DallasRichardson, TX 75080, USA{ffliu,deana,feiliu,yangl}@hlt.utdallas.eduAbstractThis paper explores several unsupervised ap-proaches to automatic keyword extractionusing meeting transcripts.
In the TFIDF(term frequency, inverse document frequency)weighting framework, we incorporated part-of-speech (POS) information, word clustering,and sentence salience score.
We also evalu-ated a graph-based approach that measures theimportance of a word based on its connectionwith other sentences or words.
The systemperformance is evaluated in different ways, in-cluding comparison to human annotated key-words using F-measure and a weighted scorerelative to the oracle system performance, aswell as a novel alternative human evaluation.Our results have shown that the simple un-supervised TFIDF approach performs reason-ably well, and the additional information fromPOS and sentence score helps keyword ex-traction.
However, the graph method is lesseffective for this domain.
Experiments werealso performed using speech recognition out-put and we observed degradation and differentpatterns compared to human transcripts.1 IntroductionKeywords in a document provide important infor-mation about the content of the document.
Theycan help users search through information more effi-ciently or decide whether to read a document.
Theycan also be used for a variety of language process-ing tasks such as text categorization and informa-tion retrieval.
However, most documents do notprovide keywords.
This is especially true for spo-ken documents.
Current speech recognition systemperformance has improved significantly, but thereis no rich structural information such as topics andkeywords in the transcriptions.
Therefore, there isa need to automatically generate keywords for thelarge amount of written or spoken documents avail-able now.There have been many efforts toward keyword ex-traction for text domain.
In contrast, there is lesswork on speech transcripts.
In this paper we fo-cus on one speech genre ?
the multiparty meetingdomain.
Meeting speech is significantly differentfrom written text and most other speech data.
Forexample, there are typically multiple participantsin a meeting, the discussion is not well organized,and the speech is spontaneous and contains disflu-encies and ill-formed sentences.
It is thus ques-tionable whether we can adopt approaches that havebeen shown before to perform well in written textfor automatic keyword extraction in meeting tran-scripts.
In this paper, we evaluate several differ-ent keyword extraction algorithms using the tran-scripts of the ICSI meeting corpus.
Starting fromthe simple TFIDF baseline, we introduce knowl-edge sources based on POS filtering, word cluster-ing, and sentence salience score.
In addition, wealso investigate a graph-based algorithm in order toleverage more global information and reinforcementfrom summary sentences.
We used different per-formance measurements: comparing to human an-notated keywords using individual F-measures anda weighted score relative to the oracle system per-formance, and conducting novel human evaluation.Experiments were conducted using both the humantranscripts and the speech recognition (ASR) out-620put.
Overall the TFIDF based framework seems towork well for this domain, and the additional knowl-edge sources help improve system performance.
Thegraph-based approach yielded worse results, espe-cially for the ASR condition, suggesting further in-vestigation for this task.2 Related WorkTFIDF weighting has been widely used for keywordor key phrase extraction.
The idea is to identifywords that appear frequently in a document, but donot occur frequently in the entire document collec-tion.
Much work has shown that TFIDF is very ef-fective in extracting keywords for scientific journals,e.g., (Frank et al, 1999; Hulth, 2003; Kerner et al,2005).
However, we may not have a big backgroundcollection that matches the test domain for a reli-able IDF estimate.
(Matsuo and Ishizuka, 2004) pro-posed a co-occurrence distribution based method us-ing a clustering strategy for extracting keywords fora single document without relying on a large corpus,and reported promising results.Web information has also been used as an ad-ditional knowledge source for keyword extraction.
(Turney, 2002) selected a set of keywords first andthen determined whether to add another keyword hy-pothesis based on its PMI (point-wise mutual infor-mation) score to the current selected keywords.
Thepreselected keywords can be generated using basicextraction algorithms such as TFIDF.
It is impor-tant to ensure the quality of the first selection for thesubsequent addition of keywords.
Other researchersalso used PMI scores between each pair of candidatekeywords to select the top k% of words that havethe highest average PMI scores as the final keywords(Inkpen and Desilets, 2004).Keyword extraction has also been treated as aclassification task and solved using supervised ma-chine learning approaches (Frank et al, 1999; Tur-ney, 2000; Kerner et al, 2005; Turney, 2002; Tur-ney, 2003).
In these approaches, the learning al-gorithm needs to learn to classify candidate wordsin the documents into positive or negative examplesusing a set of features.
Useful features for this ap-proach include TFIDF and its variations, position ofa phrase, POS information, and relative length of aphrase (Turney, 2000).
Some of these features maynot work well for meeting transcripts.
For exam-ple, the position of a phrase (measured by the num-ber of words before its first appearance divided bythe document length) is very useful for news articletext, since keywords often appear early in the doc-ument (e.g., in the first paragraph).
However, forthe less well structured meeting domain (lack of ti-tle and paragraph), these kinds of features may notbe indicative.
A supervised approach to keyword ex-traction was used in (Liu et al, 2008).
Even thoughthe data set in that study is not very big, it seems thata supervised learning approach can achieve reason-able performance for this task.Another line of research for keyword extrac-tion has adopted graph-based methods similar toGoogle?s PageRank algorithm (Brin and Page,1998).
In particular, (Wan et al, 2007) attemptedto use a reinforcement approach to do keyword ex-traction and summarization simultaneously, on theassumption that important sentences usually containkeywords and keywords are usually seen in impor-tant sentences.
We also find that this assumption alsoholds using statistics obtained from the meeting cor-pus used in this study.
Graph-based methods havenot been used in a genre like the meeting domain;therefore, it remains to be seen whether these ap-proaches can be applied to meetings.Not many studies have been performed on speechtranscripts for keyword extraction.
The most rel-evant work to our study is (Plas et al, 2004),where the task is keyword extraction in the mul-tiparty meeting corpus.
They showed that lever-aging semantic resources can yield significant per-formance improvement compared to the approachbased on the relative frequency ratio (similar toIDF).
There is also some work using keywords forother speech processing tasks, e.g., (Munteanu etal., 2007; Bulyko et al, 2007; Wu et al, 2007; De-silets et al, 2002; Rogina, 2002).
(Wu et al, 2007)showed that keyword extraction combined with se-mantic verification can be used to improve speechretrieval performance on broadcast news data.
In(Rogina, 2002), keywords were extracted from lec-ture slides, and then used as queries to retrieve rel-evant web documents, resulting in an improved lan-guage model and better speech recognition perfor-mance of lectures.
There are many differences be-tween written text and speech ?
meetings in par-ticular.
Thus our goal in this paper is to investi-621gate whether we can successfully apply some exist-ing techniques, as well as propose new approachesto extract keywords for the meeting domain.
Theaim of this study is to set up some starting points forresearch in this area.3 DataWe used the meetings from the ICSI meeting data(Janin et al, 2003), which are recordings of naturallyoccurring meetings.
All the meetings have beentranscribed and annotated with dialog acts (DA)(Shriberg et al, 2004), topics, and extractive sum-maries (Murray et al, 2005).
The ASR output forthis corpus is obtained from a state-of-the-art SRIconversational telephone speech system (Zhu et al,2005), with a word error rate of about 38.2% onthe entire corpus.
We align the human transcriptsand ASR output, then map the human annotated DAboundaries and topic boundaries to the ASR words,such that we have human annotation of these infor-mation for the ASR output.We recruited three Computer Science undergradu-ate students to annotate keywords for each topic seg-ment, using 27 selected ICSI meetings.1 Up to fiveindicative key words or phrases were annotated foreach topic.
In total, we have 208 topics annotatedwith keywords.
The average length of the topics(measured using the number of dialog acts) amongall the meetings is 172.5, with a high standard devi-ation of 236.8.
We used six meetings as our devel-opment set (the same six meetings as the test set in(Murray et al, 2005)) to optimize our keyword ex-traction methods, and the remaining 21 meetings forfinal testing in Section 5.One example of the annotated keywords for atopic segment is:?
Annotator I: analysis, constraints, templatematcher;?
Annotator II: syntactic analysis, parser, patternmatcher, finite-state transducers;?
Annotator III: lexicon, set processing, chunkparser.Note that these meetings are research discussions,and that the annotators may not be very familiar with1We selected these 27 meetings because they have been usedin previous work for topic segmentation and summarization(Galley et al, 2003; Murray et al, 2005).the topics discussed and often had trouble decidingthe important sentences or keywords.
In addition,limiting the number of keywords that an annotatorcan select for a topic also created some difficulty.Sometimes there are more possible keywords andthe annotators felt it is hard to decide which five arethe most topic indicative.
Among the three annota-tors, we notice that in general the quality of anno-tator I is the poorest.
This is based on the authors?judgment, and is also confirmed later by an indepen-dent human evaluation (in Section 6).For a better understanding of the gold standardused in this study and the task itself, we thoroughlyanalyzed the human annotation consistency.
We re-moved the topics labeled with ?chitchat?
by at leastone annotator, and also the digit recording part inthe ICSI data, and used the remaining 140 topic seg-ments.
We calculated the percentage of keywordsagreed upon by different annotators for each topic,as well as the average for all the meetings.
All of theconsistency analysis is performed based on words.Figure 1 illustrates the annotation consistency overdifferent meetings and topics.
The average consis-tency rate across topics is 22.76% and 5.97% amongany two and all three annotators respectively.
Thissuggests that people do not have a high agreementon keywords for a given document.
We also noticethat the two person agreement is up to 40% for sev-eral meetings and 80% for several individual top-ics, and the agreement among all three annotatorsreaches 20% and 40% for some meetings or topics.This implies that the consistency depends on topics(e.g., the difficulty or ambiguity of a topic itself, theannotators?
knowledge of that topic).
Further studiesare needed for the possible factors affecting humanagreement.
We are currently creating more annota-tions for this data set for better agreement measureand also high quality annotation.4 MethodsOur task is to extract keywords for each of the topicsegments in each meeting transcript.
Therefore, by?document?, we mean a topic segment in the re-mainder of this paper.
Note that our task is differentfrom keyword spotting, where a keyword is providedand the task is to spot it in the audio (along with itstranscript).The core part of keyword extraction is for the sys-62200.20.40.60.810 30 60 90 1203 agree2 agree00.10.20.30.40.51 3 5 7 9 11 13 15 17 19 21 23 25 273 agree2 agreeFigure 1: Human annotation consistency across differ-ent topics (upper graph) and meetings (lower graph).
Y-axis is the percent of the keywords agreed upon by two orthree annotators.tem to assign an importance score to a word, andthen pick the top ranked words as keywords.
Wecompare different methods for weight calculation inthis study, broadly divided into the following twocategories: the TFIDF framework and the graph-based model.
Both are unsupervised learning meth-ods.2 In all of the following approaches, when se-lecting the final keywords, we filter out any wordsappearing on the stopword list.
These stopwords aregenerated based on the IDF values of the words us-ing all the meeting data by treating each topic seg-ment as a document.
The top 250 words from thislist (with the lowest IDF values) were used as stop-words.
We generated two different stopword lists forhuman transcripts and ASR output respectively.
Inaddition, in this paper we focus on performing key-word extraction at the single word level, thereforeno key phrases are generated.2Note that by unsupervised methods, we mean that no dataannotated with keywords is needed.
These methods do requirethe use of some data to generate information such as IDF, orpossibly a development set to optimize some parameters orheuristic rules.4.1 TFIDF Framework(A) Basic TFIDF weightingThe term frequency (TF) for a word wi in a doc-ument is the number of times the word occurs in thedocument.
The IDF value is:IDFi = log(N/Ni)whereNi denotes the number of the documents con-taining word wi, and N is the total number of thedocuments in the collection.
We also performed L2normalization for the IDF values when combiningthem with other scores.
(B) Part of Speech (POS) filteringIn addition to using a stopword list to removewords from consideration, we also leverage POS in-formation to filter unlikely keywords.
Our hypothe-sis is that verb, noun and adjective words are morelikely to be keywords, so we restrict our selection towords with these POS tags only.
We used the TnTPOS tagger (Brants, 2000) trained from the Switch-board data to tag the meeting transcripts.
(C) Integrating word clusteringOne weakness of the baseline TFIDF is that itcounts the frequency for a particular word, withoutconsidering any words that are similar to it in termsof semantic meaning.
In addition, when the docu-ment is short, the TF may not be a reliable indicatorof the importance of the word.
Our idea is thereforeto account for the frequency of other similar wordswhen calculating the TF of a word in the document.For this, we group all the words into clusters in anunsupervised fashion.
If the total term frequencyof all the words in one cluster is high, it is likelythat this cluster contributes more to the current topicfrom a thematic point of view.
Thus we want to as-sign higher weights to the words in this cluster.We used the SRILM toolkit (Stolcke, 2002) forautomatic word clustering over the entire docu-ment collection.
It minimizes the perplexity of theinduced class-based n-gram language model com-pared to the original word-based model.
Using theclusters, we then adjust the TF weighting by inte-grating with the cluster term frequency (CTF):TF CTF (wi) = TF (wi)??
(Pwl?Ci,wl 6=wi freq(wl))where the last summation component means the to-tal term frequency of all the other words in this docu-ment that belong to the same clusterCi as the current623word wi.
We set parameter ?
to be slightly largerthan 1.
We did not include stopwords when addingthe term frequencies for the words in a cluster.
(D) Combining with sentence salience scoreIntuitively, the words in an important sentenceshould be assigned a high weight for keyword ex-traction.
In order to leverage the sentence infor-mation, we adjust a word?s weight by the saliencescores of the sentences containing that word.
Thesentence score is calculated based on its cosine sim-ilarity to the entire meeting.
This score is often usedin extractive summarization to select summary sen-tences (Radev et al, 2001).
The cosine similaritybetween two vectors, D1 and D2, is defined as:sim(D1, D2) =?i t1it2i?
?i t21i ??
?i t22iwhere ti is the term weight for a word wi, for whichwe use the TFIDF value.4.2 Graph-based MethodsFor the graph-based approach, we adopt the itera-tive reinforcement approach from (Wan et al, 2007)in the hope of leveraging sentence information forkeyword extraction.
This algorithm is based on theassumption that important sentences/words are con-nected to other important sentences/words.Four graphs are created: one graph in which sen-tences are connected to other sentences (S-S graph),one in which words are connected to other words(W-W graph), and two graphs connecting words tosentences with uni-directional edges (W-S and S-Wgraphs).
Stopwords are removed before the creationof the graphs so they will be ineligible to be key-words.The final weight for a word node depends on itsconnection to other words (W-W graph) and othersentences (W-S graph); similarly, the weight fora sentence node is dependent on its connection toother sentences (S-S graph) and other words (S-Wgraph).
That is,u = ?UTu+ ?W?
T vv = ?V T v + ?W Tuwhere u and v are the weight vectors for sentenceand word nodes respectively, U, V,W, W?
representthe S-S, W-W, S-W, and W-S connections.
?
and ?specify the contributions from the homogeneous andthe heterogeneous nodes.
The initial weight is a uni-form one for the word and sentence vector.
Thenthe iterative reinforcement algorithm is used untilthe node weight values converge (the difference be-tween scores at two iterations is below 0.0001 for allnodes) or 5,000 iterations are reached.We have explored various ways to assign weightsto the edges in the graphs.
Based on the results onthe development set, we use the following setup inthis paper:?
W-W Graph: We used a diagonal matrix forthe graph connection, i.e., there is no connec-tion among words.
The self-loop values arethe TFIDF values of the words.
This is alsoequivalent to using an identity matrix for theword-word connection and TFIDF as the initialweight for each vertex in the graph.
We investi-gated other strategies to assign a weight for theedge between two word nodes; however, so farthe best result we obtained is using this diago-nal matrix.?
S-W and W-S Graphs: The weight for anedge between a sentence and a word is the TFof the word in the sentence multiplied by theword?s IDF value.
These weights are initiallyadded only to the S-W graph, as in (Wan et al,2007); then that graph is normalized and trans-posed to create the W-S graph.?
S-S Graph: The sentence node uses a vectorspace model and is composed of the weights ofthose words connected to this sentence in theS-W graph.
We then use cosine similarity be-tween two sentence vectors.Similar to the above TFIDF framework, we alsouse POS filtering for the graph-based approach.
Af-ter the weights for all the words are determined, weselect the top ranked words with the POS restriction.5 Experimental Results: AutomaticEvaluationUsing the approaches described above, we com-puted weights for the words and then picked the topfive words as the keywords for a topic.
We chose fivekeywords since this is the number of keywords that624human annotators used as a guideline, and it alsoyielded good performance in the development set.To evaluate system performance, in this section weuse human annotated keywords as references, andcompare the system output to them.
The first metricwe use is F-measure, which has been widely usedfor this task and other detection tasks.
We comparethe system output with respect to each human anno-tation, and calculate the maximum and the averageF-scores.
Note that our keyword evaluation is word-based.
When human annotators choose key phrases(containing more than one word), we split them intowords and measure the matching words.
Therefore,when the system only generates five keywords, theupper bound of the recall rate may not be 100%.
In(Liu et al, 2008), a lenient metric is used which ac-counts for some inflection of words.
Since that ishighly correlated with the results using exact wordmatch, we report results based on strict matching inthe following experiments.The second metric we use is similar to Pyramid(Nenkova and Passonneau, 2004), which has beenused for summarization evaluation.
Instead of com-paring the system output with each individual hu-man annotation, the method creates a ?pyramid?using all the human annotated keywords, and thencompares system output to this pyramid.
The pyra-mid consists of all the annotated keywords at dif-ferent levels.
Each keyword has a score based onhow many annotators have selected this one.
Thehigher the score, the higher up the keyword will be inthe pyramid.
Then we calculate an oracle score thata system can obtain when generating k keywords.This is done by selecting keywords in the decreas-ing order in terms of the pyramid levels until weobtain k keywords.
Finally for the system hypoth-esized k keywords, we compute its score by addingthe scores of the keywords that match those in thepyramid.
The system?s performance is measured us-ing the relative performance of the system?s pyramidscores divided by the oracle score.Table 1 shows the results using human transcriptsfor different methods on the 21 test meetings (139topic segments in total).
For comparison, we alsoshow results using the supervised approach as in(Liu et al, 2008), which is the average of the 21-fold cross validation.
We only show the maximumF-measure with respect to individual annotations,since the average scores show similar trend.
In ad-dition, the weighted relative scores already accountsfor the different annotation and human agreement.Methods F-measure weighted relative scoreTFIDF 0.267 0.368+ POS 0.275 0.370+ Clustering 0.277 0.367+ Sent weight 0.290 0.404Graph 0.258 0.364Graph+POS 0.277 0.380Supervised 0.312 0.401Table 1: Keyword extraction results using human tran-scripts compared to human annotations.We notice that for the TFIDF framework, addingPOS information slightly helps the basic TFIDFmethod.
In all the meetings, our statistics show thatadding POS filtering removed 2.3% of human anno-tated keywords from the word candidates; therefore,this does not have a significant negative impact onthe upper bound recall rate, but helps eliminate un-likely keyword candidates.
Using word clusteringdoes not yield a performance gain, most likely be-cause of the clustering technique we used ?
it doesclustering simply based on word co-occurrence anddoes not capture semantic similarity properly.Combining the term weight with the sentencesalience score improves performance, supporting thehypothesis that summary sentences and keywordscan reinforce each other.
In fact we performed ananalysis of keywords and summaries using the fol-lowing two statistics:(1) k = Psummary(wi)Ptopic(wi)where Psummary(wi) and Ptopic(wi) represent thethe normalized frequency of a keyword wi in thesummary and the entire topic respectively; and(2) s = PSsummaryPStopicwhere PSsummary represents the percentage of thesentences containing at least one keyword among allthe sentences in the summary, and similarly PStopicis measured using the entire topic segment.
Wefound that the average k and s are around 3.42 and6.33 respectively.
This means that keywords are625more likely to occur in the summary compared to therest of the topic, and the chance for a summary sen-tence to contain at least one keyword is much higherthan for the other sentences in the topic.For the graph-based methods, we notice thatadding POS filtering also improves performance,similar to the TFIDF framework.
However, thegraph method does not perform as well as the TFIDFapproach.
Comparing with using TFIDF alone, thegraph method (without using POS) yielded worse re-sults.
In addition to using the TFIDF for the wordnodes, information from the sentences is used in thegraph method since a word is linked to sentencescontaining this word.
The global information in theS-S graph (connecting a sentence to other sentencesin the document) is propagated to the word nodes.Unlike the study in (Wan et al, 2007), this infor-mation does not yield any gain.
We did find that thegraph approach performed better in the developmentset, but it seems that it does not generalize to this testset.Compared to the supervised results, the TFIDFapproach is worse in terms of the individual maxi-mum F-measure, but achieves similar performancewhen using the weighted relative score.
However,the unsupervised TFIDF approach is much simplerand does not require any annotated data for train-ing.
Therefore it may be easily applied to a newdomain.
Again note that these results used word-based selection.
(Liu et al, 2008) investigatedadding bigram key phrases, which we expect tobe independent of these unigram-based approachesand adding bigram phrases will yield further per-formance gain for the unsupervised approach.
Fi-nally, we analyzed if the system?s keyword ex-traction performance is correlated with human an-notation disagreement using the unsupervised ap-proach (TFIDF+POS+Sent weight).
The correla-tion (Spearman?s ?
value) between the system?sF-measure and the three-annotator consistency onthe 27 meetings is 0.5049 (p=0.0072).
This indi-cates that for the meetings with a high disagreementamong human annotators, it is also challenging forthe automatic systems.Table 2 shows the results using ASR output forvarious approaches.
The performance measure isthe same as used in Table 1.
We find that in gen-eral, there is a performance degradation comparedto using human transcripts, which is as expected.We found that only 59.74% of the human annotatedkeywords appear in ASR output, that is, the upperbound of recall is very low.
The TFIDF approachstill outperforms the graph method.
Unlike on hu-man transcripts, the addition of information sourcesin the TFIDF approach did not yield significant per-formance gain.
A big difference from the humantranscript condition is the use of sentence weight-ing ?
adding it degrades performance in ASR, incontrast to the improvement in human transcripts.This is possibly because the weighting of the sen-tences is poor when there are many recognition er-rors from content words.
In addition, compared tothe supervised results, the TFIDF method has sim-ilar maximum F-measure, but is slightly worse us-ing the weighted score.
Further research is neededfor the ASR condition to investigate better modelingapproaches.Methods F-measure weighted relative scoreTFIDF 0.191 0.257+ POS 0.196 0.259+ Clustering 0.196 0.259+ Sent weigh 0.178 0.241Graph 0.173 0.223Graph+POS 0.183 0.233Supervised 0.197 0.269Table 2: Keyword extraction results using ASR output.6 Experimental Results: HumanEvaluationGiven the disagreement among human annotators,one question we need to answer is whether F-measure or even the weighted relative scores com-pared with human annotations are appropriate met-rics to evaluate system-generated keywords.
Forexample, precision measures among the system-generated keywords how many are correct.
How-ever, this does not measure if the unmatched system-generated keywords are bad or acceptable.
Wetherefore performed a small scale human evaluation.We selected four topic segments from four differ-ent meetings, and gave output from different sys-tems to five human subjects.
The subjects rangedin age from 22 to 63, and all but one had only basicknowledge of computers.
We first asked the eval-626uators to read the entire topic transcript, and thenpresented them with the system-generated keywords(randomly ordered by different systems).
For com-parison, the keywords annotated by our three hu-man annotators were also included without reveal-ing which sets of keywords were generated by ahuman and which by a computer.
Because therewas such disagreement between annotators regard-ing what made good keywords, we instead asked ourevaluators to mark any words that were definitelynot keywords.
Systems that produced more of theserejected words (such as ?basically?
or ?mmm-hm?
)are assumed to be worse than those containing fewerrejected words.
We then measured the percentage ofrejected keywords for each system/annotator.
Theresults are shown in Table 3.
Not surprisingly, thehuman annotations rank at the top.
Overall, we findhuman evaluation results to be consistent with theautomatic evaluation metrics in terms of the rankingof different systems.Systems Rejection rateAnnotator 2 8%Annotator 3 19%Annotator 1 25%TFIDF + POS 28%TFIDF 30%Table 3: Human evaluation results: percentage of the re-jected keywords by human evaluators for different sys-tems/annotators.Note this rejection rate is highly related to the re-call/precision measure in the sense that it measureshow many keywords are acceptable (or rejected)among the system generated ones.
However, insteadof comparing to a fixed set of human annotated key-words (e.g., five) and using that as a gold standardto compute recall/precision, in this evaluation, thehuman evaluator may have a larger set of accept-able keywords in their mind.
We also measured thehuman evaluator agreement regarding the acceptedor bad keywords.
We found that the agreement ona bad keyword among five, four, and three humanevaluator is 10.1%, 14.8%, and 10.1% respectively.This suggests that humans are more likely to agreeon a bad keyword selection compared to agreementon the selected keywords, as discussed in Section 3(even though the data sets in these two analysis arenot the same).
Another observation from the humanevaluation is that sometimes a person rejects a key-word from one system output, but accepts that onthe list from another system.
We are not sure yetwhether this is the inconsistency from human evalu-ators or whether the judgment is based on a word?soccurrence with other provided keywords and thussome kind of semantic coherence.
Further investi-gation on human evaluation is still needed.7 Conclusions and Future WorkIn this paper, we evaluated unsupervised keywordextraction performance for the meeting domain, agenre that is significantly different from most pre-vious work.
We compared several different ap-proaches using the transcripts of the ICSI meetingcorpus.
Our results on the human transcripts showthat the simple TFIDF based method is very compet-itive.
Adding additional knowledge such as POS andsentence salience score helps improve performance.The graph-based approach performs less well in thistask, possibly because of the lack of structure inthis domain.
We use different performance measure-ments, including F-measure with respect to individ-ual human annotations and a weighted metric rela-tive to the oracle system performance.
We also per-formed a new human evaluation for this task and ourresults show consistency with the automatic mea-surement.
In addition, experiments on the ASR out-put show performance degradation, but more impor-tantly, different patterns in terms of the contributionsof information sources compared to using humantranscripts.
Overall the unsupervised approaches aresimple but effective; however, system performancecompared to the human performance is still low,suggesting more work is needed for this domain.For the future work, we plan to investigate dif-ferent weighting algorithms for the graph-based ap-proach.
We also need a better way to decide thenumber of keywords to generate instead of using afixed number.
Furthermore, since there are multiplespeakers in the meeting domain, we plan to incor-porate speaker information in various approaches.More importantly, we will perform a more rigoroushuman evaluation, and also use extrinsic evaluationto see whether automatically generated keywords fa-cilitate tasks such as information retrieval or meetingbrowsing.627AcknowledgmentsThis work is supported by NSF award IIS-0714132.Any opinions expressed in this work are those of theauthors and do not necessarily reflect the views ofNSF.ReferencesT.
Brants.
2000.
TnT ?
a statistical part-of-speech tagger.In Proceedings of the 6th Applied NLP Conference.S.
Brin and L. Page.
1998.
The anatomy of a large-scalehypertextual web search engine.
Computer Networksand ISDN Systems, 30.I.
Bulyko, M. Ostendorf, M. Siu, T. Ng, A. Stolcke, andO.
Cetin.
2007.
Web resources for language modelingin conversational speech recognition.
ACM Transac-tions on Speech and Language Processing, 5:1?25.A.
Desilets, B.D.
Bruijn, and J. Martin.
2002.
Extractingkeyphrases from spoken audio documents.
In Infor-mation Retrieval Techniques for Speech Applications,pages 339?342.E.
Frank, G.W.
Paynter, I.H.
Witten, C. Gutwin, and C.G.Nevill-Manning.
1999.
Domain-specific keyphraseextraction.
In Proceedings of IJCAI, pages 688?673.M.
Galley, K. McKeown, E. Fosler-Lussier, and H. Jing.2003.
Discourse segmentation of multi-party conver-sation.
In Proceedings of ACL.A.
Hulth.
2003.
Improved automatic keyword extractiongiven more linguistic knowledge.
In Proceedings ofEMNLP, pages 216?223.D.
Inkpen and A. Desilets.
2004.
Extractingsemantically-coherent keyphrases from speech.
Cana-dian Acoustics Association, 32:130?131.A.
Janin, D. Baron, J. Edwards, D. Ellis, G .
Gelbart,N.
Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke,and C. Wooters.
2003.
The ICSI meeting corpus.
InProceedings of ICASSP.Y.H.
Kerner, Z.
Gross, and A. Masa.
2005.
Automaticextraction and learning of keyphrases from scientificarticles.
In Computational Linguistics and IntelligentText Processing, pages 657?669.F.
Liu, F. Liu, and Y. Liu.
2008.
Automatic keywordextraction for the meeting corpus using supervised ap-proach and bigram expansion.
In Proceedings of IEEESLT.Y.
Matsuo and M. Ishizuka.
2004.
Keyword extractionfrom a single document using word co-occurrence sta-tistical information.
International Journal on Artifi-cial Intelligence, 13(1):157?169.C.
Munteanu, G. Penn, and R. Baecker.
2007.
Web-based language modeling for automatic lecture tran-scription.
In Proceedings of Interspeech.G.
Murray, S. Renals, J. Carletta, and J. Moore.
2005.Evaluating automatic summaries of meeting record-ings.
In Proceedings of ACL 2005 MTSE Workshop,pages 33?40.A.
Nenkova and R. Passonneau.
2004.
Evaluating con-tent selection in summarization: the pyramid method.In Proceedings of HLT/NAACL.L.
Plas, V. Pallotta, M. Rajman, and H. Ghorbel.
2004.Automatic keyword extraction from spoken text.
acomparison of two lexical resources: the EDR andWordNet.
In Proceedings of the LREC.D.
Radev, S. Blair-Goldensohn, and Z. Zhang.
2001.
Ex-periments in single and multi-document summariza-tion using MEAD.
In Proceedings of The First Docu-ment Understanding Conference.I.
Rogina.
2002.
Lecture and presentation tracking in anintelligent meeting room.
In Proceedings of ICMI.E.
Shriberg, R. Dhillon, S. Bhagat, J. Ang, and H. Carvey.2004.
The ICSI meeting recorder dialog act (MRDA)corpus.
In Proceedings of SIGDial Workshop, pages97?100.A.
Stolcke.
2002.
SRILM ?
An extensible languagemodeling toolkit.
In Proceedings of ICSLP, pages901?904.P.D.
Turney.
2000.
Learning algorithms for keyphraseextraction.
Information Retrieval, 2:303?336.P.D.
Turney.
2002.
Mining the web for lexical knowl-edge to improve keyphrase extraction: Learning fromlabeled and unlabeled data.
In National ResearchCouncil, Institute for Information Technology, Techni-cal Report ERB-1096.P.D.
Turney.
2003.
Coherent keyphrase extraction viaweb mining.
In Proceedings of IJCAI, pages 434?439.X.
Wan, J. Yang, and J. Xiao.
2007.
Towards an iter-ative reinforcement approach for simultaneous docu-ment summarization and keyword extraction.
In Pro-ceedings of ACL, pages 552?559.C.H.
Wu, C.L.
Huang, C.S.
Hsu, and K.M.
Lee.
2007.Speech retrieval using spoken keyword extraction andsemantic verification.
In Proceedings of IEEE Region10 Conference, pages 1?4.Q.
Zhu, A. Stolcke, B. Chen, and N. Morgan.
2005.Using MLP features in SRI?s conversational speechrecognition system.
In Proceedings of Interspeech.628
