Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1961?1966,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsImproving LSTM-based Video Descriptionwith Linguistic Knowledge Mined from TextSubhashini VenugopalanUT Austinvsub@cs.utexas.eduLisa Anne HendricksUC Berkeleylisa anne@berkeley.eduRaymond MooneyUT Austinmooney@cs.utexas.eduKate SaenkoBoston Universitysaenko@bu.eduAbstractThis paper investigates how linguistic knowl-edge mined from large text corpora can aid thegeneration of natural language descriptions ofvideos.
Specifically, we integrate both a neu-ral language model and distributional seman-tics trained on large text corpora into a recentLSTM-based architecture for video descrip-tion.
We evaluate our approach on a collectionof Youtube videos as well as two large moviedescription datasets showing significant im-provements in grammaticality while modestlyimproving descriptive quality.1 IntroductionThe ability to automatically describe videos in nat-ural language (NL) enables many important appli-cations including content-based video retrieval andvideo description for the visually impaired.
Themost effective recent methods (Venugopalan et al,2015a; Yao et al, 2015) use recurrent neural net-works (RNN) and treat the problem as machinetranslation (MT) from video to natural language.Deep learning methods such as RNNs need largetraining corpora; however, there is a lack of high-quality paired video-sentence data.
In contrast, rawtext corpora are widely available and exhibit richlinguistic structure that can aid video description.Most work in statistical MT utilizes both a languagemodel trained on a large corpus of monolingual tar-get language data as well as a translation modeltrained on more limited parallel bilingual data.
Thispaper explores methods to incorporate knowledgefrom language corpora to capture general linguisticregularities to aid video description.This paper integrates linguistic information intoa video-captioning model based on Long ShortTerm Memory (LSTM) (Hochreiter and Schmidhu-ber, 1997) RNNs which have shown state-of-the-artperformance on the task.
Further, LSTMs are alsoeffective as language models (LMs) (Sundermeyeret al, 2010).
Our first approach (early fusion) isto pre-train the network on plain text before train-ing on parallel video-text corpora.
Our next two ap-proaches, inspired by recent MT work (Gulcehre etal., 2015), integrate an LSTM LM with the existingvideo-to-text model.
Furthermore, we also explorereplacing the standard one-hot word encoding withdistributional vectors trained on external corpora.We present detailed comparisons between the ap-proaches, evaluating them on a standard Youtubecorpus and two recent large movie descriptiondatasets.
The results demonstrate significant im-provements in grammaticality of the descriptions(as determined by crowdsourced human evaluations)and more modest improvements in descriptive qual-ity (as determined by both crowdsourced humanjudgements and standard automated comparison tohuman-generated descriptions).
Our main contribu-tions are 1) multiple ways to incorporate knowledgefrom external text into an existing captioning model,2) extensive experiments comparing the methods onthree large video-caption datasets, and 3) humanjudgements to show that external linguistic knowl-edge has a significant impact on grammar.2 LSTM-based Video DescriptionWe use the successful S2VT video descriptionframework from Venugopalan et al (2015a) as our1961Knowledge fromText CorporacatdogcarFigure 1: The S2VT architecture encodes a sequence of framesand decodes them to a sentence.
We propose to add knowledgefrom text corpora to enhance the quality of video description.underlying model and describe it briefly here.
S2VTuses a sequence to sequence approach (Sutskeveret al, 2014; Cho et al, 2014) that maps an input~x = (x1, ... , xT ) video frame feature sequence to afixed dimensional vector and then decodes this intoa sequence of output words ~y = (y1, ... , yN ).As shown in Fig.
1, it employs a stack of twoLSTM layers.
The input ~x to the first LSTM layeris a sequence of frame features obtained from thepenultimate layer (fc7) of a Convolutional NeuralNetwork (CNN) after the ReLu operation.
ThisLSTM layer encodes the video sequence.
At eachtime step, the hidden control state ht is provided asinput to a second LSTM layer.
After viewing all theframes, the second LSTM layer learns to decode thisstate into a sequence of words.
This can be viewedas using one LSTM layer to model the visual fea-tures, and a second LSTM layer to model languageconditioned on the visual representation.
We modifythis architecture to incorporate linguistic knowledgeat different stages of the training and generation pro-cess.
Although our methods use S2VT, they aresufficiently general and could be incorporated intoother CNN-RNN based captioning models.3 ApproachExisting visual captioning models (Vinyals et al,2015; Donahue et al, 2015) are trained solely on textfrom the caption datasets and tend to exhibit somelinguistic irregularities associated with a restrictedlanguage model and a small vocabulary.
Here, weinvestigate several techniques to integrate prior lin-guistic knowledge into a CNN/LSTM-based net-work for video to text (S2VT) and evaluate their ef-fectiveness at improving the overall description.Early Fusion.
Our first approach (early fusion), isto pre-train portions of the network modeling lan-guage on large corpora of raw NL text and thencontinue ?fine-tuning?
the parameters on the pairedvideo-text corpus.
An LSTM model learns to esti-mate the probability of an output sequence given aninput sequence.
To learn a language model, we trainthe LSTM layer to predict the next word given theprevious words.
Following the S2VT architecture,we embed one-hot encoded words in lower dimen-sional vectors.
The network is trained on web-scaletext corpora and the parameters are learned throughbackpropagation using stochastic gradient descent.1The weights from this network are then used to ini-tialize the embedding and weights of the LSTM lay-ers of S2VT, which is then trained on video-textdata.
This trained LM is also used as the LSTM LMin the late and deep fusion models.Late Fusion.
Our late fusion approach is similarto how neural machine translation models incorpo-rate a trained language model during decoding.
Ateach step of sentence generation, the video captionmodel proposes a distribution over the vocabulary.We then use the language model to re-score the fi-nal output by considering the weighted average ofthe sum of scores proposed by the LM as well as theS2VT video-description model (VM).
More specif-ically, if yt denotes the output at time step t, and ifpVM and pLM denote the proposal distributions ofthe video captioning model, and the language mod-els respectively, then for all words y?
?
V in thevocabulary we can recompute the score of each newword, p(yt = y?)
as:?
?
pVM (yt = y?)
+ (1?
?)
?
pLM (yt = y?)
(1)Hyper-parameter ?
is tuned on the validation set.Deep Fusion.
In the deep fusion approach (Fig.
2),we integrate the LM a step deeper in the genera-tion process by concatenating the hidden state of thelanguage model LSTM (hLMt ) with the hidden stateof the S2VT video description model (hVMt ) anduse the combined latent vector to predict the out-put word.
This is similar to the technique proposedby Gulcehre et al (2015) for incorporating languagemodels trained on monolingual corpora for machinetranslation.
However, our approach differs in two1The LM was trained to achieve a perplexity of 12019629Ld/6T06oft0ax/0 Re-Scoreyt9Ld/6T0/0yt-19Ld/6T0/0(a) Late Fusion(b) Deep FusionytFigure 2: Illustration of our late and deep fusion ap-proaches to integrate an independently trained LM to aidvideo captioning.
The deep fusion model learns jointlyfrom the hidden representations of the LM and S2VTvideo-to-text model (Vid-LSTM), whereas the late fusionre-scores the softmax output of the video-to-text model.key ways: (1) we only concatenate the hidden statesof the S2VT LSTM and language LSTM and do notuse any additional context information, (2) we fixthe weights of the LSTM language model but trainthe full video captioning network.
In this case, theprobability of the predicted word at time step t is:p(yt|~y<t, ~x) ?
exp(Wf(hVMt , hLMt ) + b) (2)where ~x is the visual feature input, W is the weightmatrix, and b the biases.
We avoid tuning the LSTMLM to prevent overwriting already learned weightsof a strong language model.
But we train the fullvideo caption model to incorporate the LM outputswhile training on the caption domain.Distributional Word Representations.
TheS2VT network, like most image and video cap-tioning models, represents words using a 1-of-N(one hot) encoding.
During training, the modellearns to embed ?one-hot?
words into a lower500d space by applying a linear transformation.However, the embedding is learned only fromthe limited and possibly noisy text in the captiondata.
There are many approaches (Mikolov etal., 2013; Pennington et al, 2014) that use largetext corpora to learn vector-space representationsof words that capture fine-grained semantic andsyntactic regularities.
We propose to take advantageof these to aid video description.
Specifically, wereplace the embedding matrix from one-hot vectorsand instead use 300-dimensional GloVe vectors(Pennington et al, 2014) pre-trained on 6B tokensfrom Gigaword and Wikipedia 2014.
In additionto using the distributional vectors for the input, wealso explore variations where the model predictsboth the one-hot word (trained on the softmax loss),as well as predicting the distributional vector fromthe LSTM hidden state using Euclidean loss as theobjective.
Here the output vector (yt) is computedas yt = (Wght + bg), and the loss is given by:L(yt, wglove) = ?
(Wght + bg)?
wglove?2 (3)where ht is the LSTM output, wglove is the word?sGloVe embedding and W , b are weights and biases.The network then essentially becomes a multi-taskmodel with two loss functions.
However, we usethis loss only to influence the weights learned by thenetwork, the predicted word embedding is not used.Ensembling.
The overall loss function of thevideo-caption network is non-convex, and difficultto optimize.
In practice, using an ensemble of net-works trained slightly differently can improve per-formance (Hansen and Salamon, 1990).
In our workwe also present results of an ensemble by averagingthe predictions of the best performing models.4 ExperimentsDatasets.
Our language model was trained onsentences from Gigaword, BNC, UkWaC, andWikipedia.
The vocabulary consisted of 72,700most frequent tokens also containing GloVe embed-dings.
Following the evaluation in Venugopalan etal.
(2015a), we compare our models on the Youtubedataset (Chen and Dolan, 2011), as well as two largemovie description corpora: MPII-MD (Rohrbach etal., 2015) and M-VAD (Torabi et al, 2015).Evaluation Metrics.
We evaluate performanceusing machine translation (MT) metrics ME-TEOR (Denkowski and Lavie, 2014) and BLEU(Papineni et al, 2002) to compare the machine-generated descriptions to human ones.
For themovie corpora which have just a single descriptionwe use only METEOR which is more robust.Human Evaluation.
We also obtain human judge-ments using Amazon Turk on a random subset of200 video clips for each dataset.
Each sentence wasrated by 3 workers on a Likert scale of 1 to 5 (higheris better) for relevance and grammar.
No video wasprovided during grammar evaluation.
For movies,due to copyright, we only evaluate on grammar.1963Model METEOR B-4 Relevance GrammarS2VT 29.2 37.0 2.06 3.76Early Fusion 29.6 37.6 - -Late Fusion 29.4 37.2 - -Deep Fusion 29.6 39.3 - -Glove 30.0 37.0 - -Glove+Deep- Web Corpus 30.3 38.1 2.12 4.05*- In-Domain 30.3 38.8 2.21* 4.17*Ensemble 31.4 42.1 2.24* 4.20*Table 1: Youtube dataset: METEOR and BLEU@4 in %,and human ratings (1-5) on relevance and grammar.
Bestresults in bold, * indicates significant over S2VT.4.1 Youtube Video Dataset ResultsComparison of the proposed techniques in Table 1shows that Deep Fusion performs well on both ME-TEOR and BLEU; incorporating Glove embeddingssubstantially increases METEOR, and combiningthem both does best.
Our final model is an ensem-ble (weighted average) of the Glove, and the twoGlove+Deep Fusion models trained on the externaland in-domain COCO (Lin et al, 2014) sentences.We note here that the state-of-the-art on this datasetis achieved by HRNE (Pan et al, 2015) (METEOR33.1) which proposes a superior visual processingpipeline using attention to encode the video.Human ratings also correlate well with the ME-TEOR scores, confirming that our methods give amodest improvement in descriptive quality.
How-ever, incorporating linguistic knowledge signifi-cantly2 improves the grammaticality of the results,making them more comprehensible to human users.Embedding Influence.
We experimented multipleways to incorporate word embeddings: (1) GloVe in-put: Replacing one-hot vectors with GloVe on theLSTM input performed best.
(2) Fine-tuning: Ini-tializing with GloVe and subsequently fine-tuningthe embedding matrix reduced validation results by0.4 METEOR.
(3) Input and Predict.
Training theLSTM to accept and predict GloVe vectors, as de-scribed in Section 3, performed similar to (1).
Allscores reported in Tables 1 and 2 correspond to thesetting in (1) with GloVe embeddings only as input.2Using the Wilcoxon Signed-Rank test, results were signifi-cant with p < 0.02 on relevance and p < 0.001 on grammar.Model MPII-MD M-VADMETEOR Grammar METEOR GrammarS2VT?
6.5 2.6 6.6 2.2Early Fusion 6.7 - 6.8 -Late Fusion 6.5 - 6.7 -Deep Fusion 6.8 - 6.8 -Glove 6.7 3.9* 6.7 3.1*Glove+Deep 6.8 4.1* 6.7 3.3*Table 2: Movie Corpora: METEOR (%) and humangrammar ratings (1-5, higher is better).
Best results inbold, * indicates significant over S2VT.Figure 3: Two frames from a clip.
Models generate visu-ally relevant sentences but differ from groundtruth (GT).4.2 Movie Description ResultsResults on the movie corpora are presented in Ta-ble 2.
Both MPII-MD and M-VAD have only a sin-gle ground truth description for each video, whichmakes both learning and evaluation very challeng-ing (E.g.
Fig.3).
METEOR scores are fairly lowon both datasets since generated sentences are com-pared to a single reference translation.
S2VT?
is are-implementation of the base S2VT model with thenew vocabulary and architecture (embedding dimen-sion).
We observe that the ability of external lin-guistic knowledge to improve METEOR scores onthese challenging datasets is small but consistent.Again, human evaluations show significant (withp < 0.0001) improvement in grammatical quality.5 Related WorkFollowing the success of LSTM-based models onMachine Translation (Sutskever et al, 2014; Bah-danau et al, 2015), and image captioning (Vinyalset al, 2015; Donahue et al, 2015), recent video de-scription works (Venugopalan et al, 2015b; Venu-gopalan et al, 2015a; Yao et al, 2015) proposeCNN-RNN based models that generate a vector rep-resentation for the video and ?decode?
it using anLSTM sequence model to generate a description.Venugopalan et al (2015b) also incorporate exter-nal data such as images with captions to improve1964video description, however in this work, our focusis on integrating external linguistic knowledge forvideo captioning.
We specifically investigate the useof distributional semantic embeddings and LSTM-based language models trained on external text cor-pora to aid existing CNN-RNN based video descrip-tion models.LSTMs have proven to be very effective languagemodels (Sundermeyer et al, 2010).
Gulcehre etal.
(2015) developed an LSTM model for machinetranslation that incorporates a monolingual languagemodel for the target language showing improved re-sults.
We utilize similar approaches (late fusion,deep fusion) to train an LSTM for translating videoto text that exploits large monolingual-English cor-pora (Wikipedia, BNC, UkWac) to improve RNNbased video description networks.
However, unlikeGulcehre et al (2015) where the monolingual LM isused only to tune specific parameters of the transla-tion network, the key advantage of our approach isthat the output of the monolingual language model isused (as an input) when training the full underlyingvideo description network.Contemporaneous to us, Yu et al (2015), Pan etal.
(2015) and Ballas et al (2016) propose video de-scription models focusing primarily on improvingthe video representation itself using a hierarchicalvisual pipeline, and attention.
Without the attentionmechanism their models achieve METEOR scoresof 31.1, 32.1 and 31.6 respectively on the Youtubedataset.
The interesting aspect, as demonstrated inour experiments (Table 1), is that the contribution oflanguage alone is considerable and only slightly lessthan the visual contribution on this dataset.
Hence,it is important to focus on both aspects to generatebetter descriptions.6 ConclusionThis paper investigates multiple techniques to in-corporate linguistic knowledge from text corpora toaid video captioning.
We empirically evaluate ourapproaches on Youtube clips as well as two moviedescription corpora.
Our results show significantimprovements on human evaluations of grammarwhile modestly improving the overall descriptivequality of sentences on all datasets.
While the pro-posed techniques are evaluated on a specific video-caption network, they are generic and can be ap-Figure 4: Representative frames from clips in the movie de-scription corpora.
S2VT is the baseline model, Glove indicatesthe model trained with input Glove vectors, and Glove+Deepuses input Glove vectors with the Deep Fusion approach.
GTindicates groundtruth sentence.plied to many captioning models.
The code andmodels are shared on http://vsubhashini.github.io/language_fusion.html.AcknowledgementsThis work was supported by NSF awards IIS-1427425 and IIS-1212798, and ONR ATL GrantN00014-11-1-010, and DARPA under AFRL grantFA8750-13-2-0026.
Raymond Mooney and KateSaenko also acknowledge support from a Googlegrant.
Lisa Anne Hendricks is supported by the Na-tional Defense Science and Engineering Graduate(NDSEG) Fellowship.1965References[Bahdanau et al2015] Dzmitry Bahdanau, KyunghyunCho, and Yoshua Bengio.
2015.
Neural machinetranslation by jointly learning to align and translate.ICLR.
[Ballas et al2016] Nicolas Ballas, Li Yao, Chris Pal, andAaron C. Courville.
2016.
Delving deeper into con-volutional networks for learning video representations.ICLR.
[Chen and Dolan2011] David Chen and William Dolan.2011.
Collecting highly parallel data for paraphraseevaluation.
In ACL.
[Cho et al2014] Kyunghyun Cho, Bart van Merrie?nboer,Dzmitry Bahdanau, and Yoshua Bengio.
2014.
Onthe properties of neural machine translation: Encoder?decoder approaches.
Syntax, Semantics and Structurein Statistical Translation, page 103.
[Denkowski and Lavie2014] Michael Denkowski andAlon Lavie.
2014.
Meteor universal: Languagespecific translation evaluation for any target language.In EACL.
[Donahue et al2015] Jeff Donahue, Lisa Anne Hen-dricks, Sergio Guadarrama, Marcus Rohrbach, Sub-hashini Venugopalan, Kate Saenko, and Trevor Dar-rell.
2015.
Long-term recurrent convolutional net-works for visual recognition and description.
InCVPR.
[Gulcehre et al2015] C. Gulcehre, O. Firat, K. Xu,K.
Cho, L. Barrault, H.C. Lin, F. Bougares,H.
Schwenk, and Y. Bengio.
2015.
On using mono-lingual corpora in neural machine translation.
arXivpreprint arXiv:1503.03535.
[Hansen and Salamon1990] L. K. Hansen and P. Sala-mon.
1990.
Neural network ensembles.
IEEE TPAMI,12(10):993?1001, Oct.[Hochreiter and Schmidhuber1997] Sepp Hochreiter andJu?rgen Schmidhuber.
1997.
Long short-term memory.Neural computation, 9(8).
[Lin et al2014] Tsung-Yi Lin, Michael Maire, Serge Be-longie, James Hays, Pietro Perona, Deva Ramanan, Pi-otr Dolla?r, and C Lawrence Zitnick.
2014.
Microsoftcoco: Common objects in context.
In ECCV.
[Mikolov et al2013] Tomas Mikolov, Kai Chen, GregCorrado, and Jeffrey Dean.
2013.
Efficient estimationof word representations in vector space.
NIPS.
[Pan et al2015] Pingbo Pan, Zhongwen Xu, Yi Yang, FeiWu, and Yueting Zhuang.
2015.
Hierarchical recur-rent neural encoder for video representation with ap-plication to captioning.
CVPR.
[Papineni et al2002] Kishore Papineni, Salim Roukos,Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: amethod for automatic evaluation of machine transla-tion.
In ACL.
[Pennington et al2014] Jeffrey Pennington, RichardSocher, and Christopher D Manning.
2014.
Glove:Global vectors for word representation.
Proceedingsof the Empiricial Methods in Natural LanguageProcessing (EMNLP 2014), 12:1532?1543.
[Rohrbach et al2015] Anna Rohrbach, MarcusRohrbach, Niket Tandon, and Bernt Schiele.
2015.
Adataset for movie description.
In CVPR.
[Sundermeyer et al2010] M. Sundermeyer, R. Schluter,and H. Ney.
2010.
Lstm neural networks for languagemodeling.
In INTERSPEECH.
[Sutskever et al2014] Ilya Sutskever, Oriol Vinyals, andQuoc V. Le.
2014.
Sequence to sequence learningwith neural networks.
In NIPS.
[Torabi et al2015] Atousa Torabi, Christopher Pal, HugoLarochelle, and Aaron Courville.
2015.
Using de-scriptive video services to create a large data sourcefor video annotation research.
arXiv:1503.01070v1.
[Venugopalan et al2015a] S. Venugopalan, M. Rohrbach,J.
Donahue, R. Mooney, T. Darrell, and K. Saenko.2015a.
Sequence to sequence - video to text.
ICCV.
[Venugopalan et al2015b] Subhashini Venugopalan, Hui-juan Xu, Jeff Donahue, Marcus Rohrbach, RaymondMooney, and Kate Saenko.
2015b.
Translating videosto natural language using deep recurrent neural net-works.
In NAACL.
[Vinyals et al2015] Oriol Vinyals, Alexander Toshev,Samy Bengio, and Dumitru Erhan.
2015.
Show andtell: A neural image caption generator.
CVPR.
[Yao et al2015] Li Yao, Atousa Torabi, Kyunghyun Cho,Nicolas Ballas, Christopher Pal, Hugo Larochelle, andAaron Courville.
2015.
Describing videos by exploit-ing temporal structure.
ICCV.
[Yu et al2015] Haonan Yu, Jiang Wang, Zhiheng Huang,Yi Yang, and Wei Xu.
2015.
Video paragraph cap-tioning using hierarchical recurrent neural networks.CVPR.1966
