Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 90?98,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsSemi-Supervised Chinese Word Segmentation Using Partial-LabelLearning With Conditional Random FieldsFan YangNuance Communications Inc.fan.yang@nuance.comPaul VozilaNuance Communications Inc.paul.vozila@nuance.comAbstractThere is rich knowledge encoded in on-line web data.
For example, punctua-tion and entity tags in Wikipedia datadefine some word boundaries in a sen-tence.
In this paper we adopt partial-labellearning with conditional random fields tomake use of this valuable knowledge forsemi-supervised Chinese word segmenta-tion.
The basic idea of partial-label learn-ing is to optimize a cost function thatmarginalizes the probability mass in theconstrained space that encodes this knowl-edge.
By integrating some domain adap-tation techniques, such as EasyAdapt, ourresult reaches an F-measure of 95.98% onthe CTB-6 corpus, a significant improve-ment from both the supervised baselineand a previous proposed approach, namelyconstrained decode.1 IntroductionA general approach for supervised Chinese wordsegmentation is to formulate it as a character se-quence labeling problem, to label each charac-ter with its location in a word.
For example,Xue (2003) proposes a four-label scheme based onsome linguistic intuitions: ?B?
for the beginningcharacter of a word, ?I?
for the internal characters,?E?
for the ending character, and ?S?
for single-character word.
Thus the word sequence ????????
can be turned into a character sequencewith labels as?\B?\I?\E?\S?\B?\E.A machine learning algorithm for sequence label-ing, such as conditional random fields (CRF) (Laf-ferty et al., 2001), can be applied to the labelledtraining data to learn a model.Labelled data for supervised learning of Chi-nese word segmentation, however, is usually ex-pensive and tends to be of a limited amount.
Re-searchers are thus interested in semi-supervisedlearning, which is to make use of unlabelled datato further improve the performance of supervisedlearning.
There is a large amount of unlabelleddata available, for example, the Gigaword corpusin the LDC catalog or the Chinese Wikipedia onthe web.Faced with the large amount of unlabelled data,an intuitive idea is to use self-training or EM, byfirst training a baseline model (from the superviseddata) and then iteratively decoding the unlabelleddata and updating the baseline model.
Jiao et al.
(2006) and Mann and McCallum (2007) furtherpropose to minimize the entropy of the predictedlabel distribution on unlabeled data and use it asa regularization term in CRF (i.e.
entropy reg-ularization).
Beyond these ideas, Liang (2005)and Sun and Xu (2011) experiment with deriv-ing a large set of statistical features such as mu-tual information and accessor variety from un-labelled data, and add them to supervised dis-criminative training.
Zeng et al.
(2013b) experi-ment with graph propagation to extract informa-tion from unlabelled data to regularize the CRFtraining.
Yang and Vozila (2013), Zhang et al.
(2013), and Zeng et al.
(2013a) experiment withco-training for semi-supervised Chinese word seg-mentation.
All these approaches only leveragethe distribution of the unlabelled data, yet do notmake use of the knowledge that the unlabelled datamight have integrated in.There could be valuable information encodedwithin the unlabelled data that researchers can takeadvantage of.
For example, punctuation createsnatural word boundaries (Li and Sun, 2009): thecharacter before a comma can only be labelledas either ?S?
or ?E?, while the character after acomma can only be labelled as ?S?
or ?B?.
Fur-thermore, entity tags (HTML tags or Wikipediatags) on the web, such as emphasis and cross refer-ence, also provide rich information for word seg-mentation: they might define a word or at least90Figure 1: Sausage constraint (partial labels) from natural annotations and punctuationgive word boundary information similar to punc-tuation.
Jiang et al.
(2013) refer to such structuralinformation on the web as natural annotations, andpropose that they encode knowledge for NLP.
ForChinese word segmentation, natural annotationsand punctuation create a sausage1constraint forthe possible labels, as illustrated in Figure 1.
Inthe sentence ???????????????????
?, the first character ?
can only be la-belled with ?S?
or ?B?
; and the characters?
beforethe comma and ?
before the Chinese period canonly be labelled as ?S?
or ?E?.
??????
and ??????
are two Wikipedia entities, and so theydefine the word boundaries before the first char-acter and after the last character of the entities aswell.
The single character ?
between these twoentities has only one label ?S?.
This sausage con-straint thus encodes rich information for word seg-mentation.To make use of the knowledge encoded in thesausage constraint, Jiang et al.
(2013) adopt a con-strained decode approach.
They first train a base-line model with labelled data, and then run con-strained decode on the unlabelled data by bindingthe search space with the sausage; and so the de-coded labels are consistent with the sausage con-straint.
The unlabelled data, together with thelabels from constrained decode, are then selec-tively added to the labelled data for training thefinal model.
This approach, using constrained de-code as a middle step, provides an indirect wayof leaning the knowledge.
However, the middlestep, constrained decode, has the risk of reinforc-ing the errors in the baseline model: the decodedlabels added to the training data for building thefinal model might contain errors introduced fromthe baseline model.
The knowledge encoded in1Also referred to as confusion network.the data carrying the information from punctuationand natural annotations is thus polluted by the er-rorful re-decoded labels.A sentence where each character has exactlyone label is fully-labelled; and a sentence whereeach character receives all possible labels is zero-labelled.
A sentence with sausage-constrained la-bels can be viewed as partially-labelled.
Thesepartial labels carry valuable information that re-searchers would like to learn in a model, yet thenormal CRF training typically uses fully-labelledsentences.
Recently, T?ackstr?om et al.
(2013) pro-pose an approach to train a CRF model directlyfrom partial labels.
The basic idea is to marginal-ize the probability mass of the constrained sausagein the cost function.
The normal CRF training us-ing fully-labelled sentences is a special case wherethe sausage constraint is a linear line; while onthe other hand a zero-labelled sentence, where thesausage constraint is the full lattice, makes no con-tribution in the learning since the sum of proba-bilities is deemed to be one.
This new approach,without the need of using constrained re-decodingas a middle step, provides a direct means to learnthe knowledge in the partial labels.In this research we explore using the partial-label learning for semi-supervised Chinese wordsegmentation.
We use the CTB-6 corpus as thelabelled training, development and test data, anduse the Chinese Wikipedia as the unlabelled data.We first train a baseline model with labelled dataonly, and then selectively add Wikipedia data withpartial labels to build a second model.
Becausethe Wikipedia data is out of domain and has dis-tribution bias, we also experiment with two do-main adaptation techniques: model interpolationand EasyAdapt (Daum?e III, 2007).
Our resultreaches an F-measure of 95.98%, an absolute im-provement of 0.72% over the very strong base-91line (corresponding to 15.19% relative error re-duction), and 0.33% over the constrained decodeapproach (corresponding to 7.59% relative errorreduction).
We conduct a detailed error analy-sis, illustrating how partial-label learning excelsconstrained decode in learning the knowledge en-coded in the Wikipedia data.
As a note, our resultalso out-performs (Wang et al., 2011) and (Sunand Xu, 2011).2 Partial-Label Learning with CRFIn this section, we review in more detail thepartial-label learning algorithm with CRF pro-posed by (T?ackstr?om et al., 2013).
CRF is anexponential model that expresses the conditionalprobability of the labels given a sequence, asEquation 1, where y denotes the labels, x denotesthe sequence, ?
(x, y) denotes the feature func-tions, and ?
is the parameter vector.
Z(x) =?yexp(?T?
(x, y)) is the normalization term.p?
(y|x) =exp(?T?
(x, y))Z(x)(1)In full-label training, where each item in the se-quence is labelled with exactly one tag, maximumlikelihood is typically used as the optimization tar-get.
We simply sum up the log-likelihood of the nlabelled sequences in the training set, as shown inEquation 2.L(?)
=n?i=1log p?(y|x)=n?i=1(?T?
(xi, yi)?
log Z(xi))(2)The gradient is calculated as Equation 3, inwhich the first term1n?ni=1?jis the empiricalexpectation of feature function ?j, and the secondterm E[?j] is the model expectation.
Typically aforward-backward process is adopted for calculat-ing the latter.???jL(?)
=1nn?i=1?j?
E[?j] (3)In partial-label training, each item in the se-quence receives multiple labels, and so for eachsequence we have a sausage constraint, denoted as?Y (x, y?).
The marginal probability of the sausageis defined as Equation 4.p?
(?Y (x, y?
)|x) =?y?
?Y (x,y?)p?
(y|x) (4)The optimization target thus is to maximize theprobability mass of the sausage, as shown in Equa-tion 5.L(?)
=n?i=1logp?
(?Y (xi, y?i)|xi) (5)A gradient-based approach such as L-BFGS(Liu and Nocedal, 1989) can be employed to op-timize Equation 5.
The gradient is calculated asEquation 6, where E?Y (x,y?
)[?j] is the empirical ex-pectation of feature function ?jconstrained by thesausage, and E[?j] is the same model expectationas in standard CRF.
E?Y (x,y?
)[?j] can be calculatedvia a forward-backward process in the constrainedsausage.???jL(?)
= E?Y (x,y?)[?j]?
E[?j] (6)For fully-labelled sentences, E?Y (x,y?
)[?j] =1n?ni=1?j, and so the standard CRF is actuallya special case of the partial-label learning.3 Experiment setupIn this section we describe the basic setup forour experiments of semi-supervised Chinese wordsegmentation.3.1 DataWe use the CTB-6 corpus as the labelled data.
Wefollow the official CTB-6 guideline in splitting thecorpus into a training set, a development set, and atest set.
The training set has 23420 sentences; thedevelopment set has 2079 sentences; and the testset has 2796 sentences.
These are fully-labelleddata.For unlabelled data we use the ChineseWikipedia.
The Wikipedia data is quite noisyand asks for a lot of cleaning.
We first filter outreferences and lists etc., and sentences with ob-viously bad segmentations, for example, whereevery character is separated by a space.
Wealso remove sentences that contain mostly En-glish words.
We then convert all characters intofull-width.
We also convert traditional Chinesecharacters into simplified characters using the tool92mediawiki-zhconverter2.
We then randomly select7737 sentences and reserve them as the test set.To create the partial labels in the Wikipediadata, we use the information from cross-reference,emphasis, and punctuation.
In our pilot study wefound that it?s beneficial to force a cross-referenceor emphasis entity as a word when the item has2 or 3 characters.
That is, if an entity in theWikipedia has three characters it receives the la-bels of ?BIE?
; and if it has two characters it is la-belled as ?BE?.33.2 Supervised baseline modelWe create the baseline supervised model by usingan order-1 linear CRF with L2 regularization, tolabel a character sequence with the four candidatelabels ?BIES?.
We use the tool wapiti (Lavergneet al., 2010).Following Sun et al.
(2009), Sun (2010), andLow et al.
(2005), we extract two types of fea-tures: character-level features and word-level fea-tures.
Given a character c0in the character se-quence ...c?2c?1c0c1c2...:Character-level features :?
Character unigrams: c?2, c?1, c0, c1, c2?
Character bigrams: c?2c?1, c?1c?0,c0c1, c1c2?
Consecutive character equivalence:?c?2= c?1, ?c?1= c?0, ?c0= c1,?c1= c2?
Separated character equivalence:?c?3= c?1, ?c?2= c0, ?c?1= c1,?c0= c2, ?c1= c3?
Whether the current character is a punc-tuation: ?Punct(c0)?
Character sequence pattern:T (C?2)T (C?1)T (C0)T (C1)T (C2).We classify all characters into fourtypes.
Type one has three characters???
(year) ???
(month) ???
(date).Type two includes number characters.Type three includes English characters.All others are Type four characters.Thus ?????S?
would generate thecharacter sequence pattern ?41213?.2https://github.com/tszming/mediawiki-zhconverter3Another possibility is to label it as ?SS?
but we find thatit?s very rare the case.Word-level features :?
The identity of the string c[s : i] (i?6 <s < i), if it matches a word from thelist of word unigrams; multiple featurescould be generated.?
The identity of the string c[i : e] (i <e < i+6), if it matches a word; multiplefeatures could be generated.?
The identity of the bi-gram c[s : i ?1]c[i : e] (i ?
6 < s, e < i + 6), ifit matches a word bigram; multiple fea-tures could be generated.?
The identity of the bi-gram c[s : i]c[i +1 : e] (i?6 < s, e < i+6), if it matchesa word bigram; multiple features couldbe generated.?
Idiom.
We use the idiom list from (Sunand Xu, 2011).
If the current characterc0and its surrounding context composean idiom, we generate a feature for c0ofits position in the idiom.
For example, ifc?1c0c1c2is an idiom, we generate fea-ture ?Idiom-2?
for c0.The above features together with label bigramsare fed to wapiti for training.
The supervised base-line model is created with the CTB-6 corpus with-out the use of Wikipedia data.3.3 Partial-label learningThe overall process of applying partial-label learn-ing to Wikipedia data is shown in Algorithm 1.Following (Jiang et al., 2013), we first train thesupervised baseline model, and use it to estimatethe potential contribution for each sentence in theWikipedia training data.
We label the sentencewith the baseline model, and then compare thelabels with the constrained sausage.
For eachcharacter, a consistent label is defined as an ele-ment in the constrained labels.
For example, ifthe constrained labels for a character are ?SB?,the label ?S?
or ?B?
is consistent but ?I?
or ?E?
isnot.
The number of inconsistent labels for eachsentence is then used as its potential contributionto the partial-label learning: higher number indi-cates that the partial-labels for the sentence con-tain more knowledge that the baseline system doesnot integrate, and so have higher potential contri-bution.
The Wikipedia training sentences are thenranked by their potential contribution, and the top93Figure 2: Encoded knowledge: inconsistency ratioand label reductionK sentences together with their partial labels arethen added to the CTB-6 training data to build anew model, using partial-label learning.4In ourexperiments, we try six data points with K =100k, 200k, 300k, 400k, 500k, 600k.
Figure 2gives a rough idea of the knowledge encoded inWikipedia for these data points with inconsistencyratio and label reduction.
Inconsistency ratio is thepercentage of characters that have inconsistent la-bels; and label reduction is the percentage of thelabels reduced in the full lattice.We modify wapiti to implement the partial-labellearning as described in Section 2.
Same as base-line, L2 regularization is adopted.Algorithm 1 Partial-label learning1.
Train supervised baseline model M02.
For each sentence x in Wiki-Train:3. y?
Decode(x, M0)4. diff?
Inconsistent(y,?Y (x, ?y))5. if diff > 0:6.
C?
C ?
(?Y (x, y?
), diff)7.
Sort(C, diff, reverse)8.
Train model Mplwith CTB-6 and top K sen-tences in C using partial-label learning3.4 Constrained decodeJiang et al.
(2013) implement the constrained de-code algorithm with perceptron.
However, CRFis generally believed to out-perform perceptron,yet the comparison of CRF vs perceptron is out4Knowledge is sparsely distributed in the Wikipedia data.Using the Wikipedia data without the CTB-6 data for partial-label learning does not necessarily guarantee convergence.Also the CTB-6 training data helps to learn that certain la-bel transitions, such as ?B B?
or ?E E?, are not legal.of the scope of this paper.
Thus for fair compar-ison, we re-implement the constrained decode al-gorithm with CRF.Algorithm 2 shows the constrained decode im-plementation.
We first train the baseline modelwith the CTB-6 data.
We then use this baselinemodel to run normal decode and constrained de-code for each sentence in the Wikipedia trainingset.
If the normal decode and constrained decodehave different labels, we add the constrained de-code together with the number of different labelsto the filtered Wikipedia training corpus.
The fil-tered Wikipedia training corpus is then sorted us-ing the number of different labels, and the top Ksentences with constrained decoded labels are thenadded to the CTB-6 training data for building anew model using normal CRF.Algorithm 2 Constrained decode1.
Train supervised baseline model M02.
For each sentence x in Wiki-Train:3. y?
Decode(x, M0)4.
y??
ConstrainedDecode(x, M0)5. diff?
Difference(y, y?)6.
if diff > 0:7.
C?
C ?
(y?, diff)8.
Sort(C, diff, reverse)9.
Train model Mcdwith CTB-6 and top K sen-tences in C using normal CRF4 Evaluation on Wikipedia test setIn order to determine how well the models learnthe encoded knowledge (i.e.
partial labels) fromthe Wikipedia data, we first evaluate the mod-els against the Wikipedia test set.
The Wikipediatest set, however, is only partially-labelled.
Thusthe metric we use here is consistent label accu-racy, similar to how we rank the sentences in Sec-tion 3.3, defined as whether a predicted label fora character is an element in the constrained la-bels.
Because partial labels are only sparsely dis-tributed in the test data, a lot of characters receiveall four labels in the constrained sausage.
Eval-uating against characters with all four labels donot really represent the models?
difference as it isdeemed to be consistent.
Thus beyond evaluatingagainst all characters in the Wikipedia test set (re-ferred to as Full measurement), we also evaluateagainst characters that are only constrained withless than four labels (referred to as Label mea-surement).
The Label measurement focuses on en-94coded knowledge in the test set and so can betterrepresent the model?s capability of learning fromthe partial labels.Results are shown in Figure 3 with the Fullmeasurement and in Figure 4 with the Label mea-surement.
The x axes are the size of Wikipediatraining data, as explained in Section 3.3.
Ascan be seen, both constrained decode and partial-label learning perform much better than the base-line supervised model that is trained from CTB-6data only, indicating that both of them are learningthe encoded knowledge from the Wikipedia train-ing data.
Also we see the trend that the perfor-mance improves with more data in training, alsosuggesting the learning of encoded knowledge.Most importantly, we see that partial-label learn-ing consistently out-performs constrained decodein all data points.
With the Label measurement,partial-label learning gives 1.7% or higher abso-lute improvement over constrained decode acrossall data points.
At the data point of 600k, con-strained decode gives an accuracy of 97.14%,while partial-label learning gives 98.93% (base-line model gives 87.08%).
The relative gain (fromlearning the knowledge) of partial-label learningover constrained decode is thus 18% ((98.93 ?97.14)/(97.14 ?
87.08)).
These results suggestthat partial-label learning is more effective inlearning the encoded knowledge in the Wikipediadata than constrained decode.5 CTB evaluation5.1 Model adaptationOur ultimate goal, however, is to determinewhether we can leverage the encoded knowledgein the Wikipedia data to improve the word seg-mentation in CTB-6.
We run our models againstthe CTB-6 test set, with results shown in Fig-ure 5.
Because we have fully-labelled sentencesin the CTB-6 data, we adopt the F-measure asour evaluation metric here.
The baseline modelachieves 95.26% in F-measure, providing a state-of-the-art supervised performance.
Constraineddecode is able to improve on this already verystrong baseline performance, and we see the nicetrend of higher performance with more unlabeleddata for training, indicating that constrained de-code is making use of the encoded knowledge inthe Wikipedia data to help CTB-6 segmentation.When we look at the partial-label model, how-ever, the results tell a totally different story.Figure 3: Wiki label evaluation results: FullFigure 4: Wiki label evaluation results: LabelFigure 5: CTB evaluation results95First, it actually performs worse than the base-line model, and the more data added to train-ing, the worse the performance is.
In the previ-ous section we show that partial-label learning ismore effective in learning the encoded knowledgein Wikipedia data than constrained decode.
So,what goes wrong?
We hypothesize that there isan out-of-domain distribution bias in the partial la-bels, and so the more data we add, the worse thein-domain performance is.
Constrained decodeactually helps to smooth out the out-of-domaindistribution bias by using the re-decoded labelswith the in-domain supervised baseline model.For example, both the baseline model and con-strained decode correctly give the segmentation???/?/??/?/???/?/?
?, while partial-label learning gives incorrect segmentation ???/?/??/?/?/??/?/??.
Looking at theWikipedia training data, ??
is tagged as an en-tity 13 times; and ??
?, although occurs 13times in the data, is never tagged as an entity.Partial-label learning, which focuses on the taggedentities, thus overrules the segmentation of ???.
Constrained decode, on the other hand, by us-ing the correctly re-decoded labels from the base-line model, observes enough evidence to correctlysegment???
as a word.To smooth out the out-of-domain distributionbias, we experiment with two approaches: modelinterpolation and EasyAdapt (Daum?e III, 2007).5.1.1 Model interpolationWe linearly interpolate the model of partial-labellearningMplwith the baseline modelM0to createthe final model Mpl+, as shown in Equation 7.
Theinterpolation weight is optimized via a grid searchbetween 0.0 and 1.0 with a step of 0.1, tuned onthe CTB-6 development set.
Again we modifywapiti so that it takes two models and an interpo-lation weight as input.
For each model it createsa search lattice with posteriors, and then linearlycombines the two lattices using the interpolationweight to create the final search space for decod-ing.
As shown in Figure 5, model Mpl+consis-tently out-performs constrained decode in all datapoints.
We also see the trend of better performancewith more training data.Mpl+= ?
?M0+ (1?
?)
?Mpl(7)5.1.2 EasyAdaptEasyAdapt is a straightforward technique but hasbeen shown effective in many domain adaptationtasks (Daum?e III, 2007).
We train the modelMpleawith feature augmentation.
For each out-of-domain training instance < xo, yo>, where xois the input features and yois the (partial) labels,we copy the features and file them as an additionalfeature set, and so the training instance becomes<xo, xo, yo>.
The in-domain training data remainsthe same.
Consistent with (Daum?e III, 2007),EasyAdapt gives us the best performance, as showin Figure 5.
Furthermore, unlike in (Jiang et al.,2013) where they find a plateau, our results showno harm adding more training data for partial-labellearning when integrated with domain adaptation,although the performance seems to saturate after400k sentences.Finally, we search for the parameter setting ofbest performance on the CTB-6 development set,which is to use EasyAdapt with K = 600k sen-tences of Wikipedia data.
With this setting, theperformance on the CTB-6 test set is 95.98%in F-measure.
This is 0.72% absolute improve-ment over supervised baseline (corresponding to15.19% relative error reduction), and 0.33% ab-solute improvement over constrained decode (cor-responding to 7.59% relative error reduction); thedifferences are both statistically significant (p <0.001).5As a note, this result out-performs (Sunand Xu, 2011) (95.44%) and (Wang et al., 2011)(95.79%), and the differences are also statisticallysignificant (p < 0.001).5.2 Analysis with examplesTo better understand why partial-label learning ismore effective in learning the encoded knowledge,we look at cases where M0and Mcdhave the in-correct segmentation while Mpl(and its domainadaptation variance Mpl+and Mplea) have the cor-rect segmentation.
We find that the majority isdue to the error in re-decoded labels outside of en-coded knowledge.
For example, M0and Mcdgivethe segmentation ???/?/?/?/6.9/?
?, yet thecorrect segmentation given by partial-label learn-ing is ???/?/?
?/6.9/ ??.
Looking at theWikipedia training data, there are 38 tagged enti-ties of?
?, but there are another 190 mentions of5Statistical significance is evaluated with z-test using thestandard deviation of?F ?
(1 ?
F )/N , where F is the F-measure and N is the number of words.96??
that are not tagged as an entity.
Thus for con-strained decode it sees 38 cases of ?
?\B ?\E?and 190 cases of ?
?\S ?\S?
in the Wikipediatraining data.
The former comes from the encodedknowledge while the latter comes from re-decodedlabels by the baseline model.
The much biggernumber of incorrect labels from the baseline re-decoding badly pollute the encoded knowledge.This example illustrates that constrained decodereinforces the errors from the baseline.
On theother hand, the training materials for partial-labellearning are purely the encoded knowledge, whichis not impacted by the baseline model error.
In thisexample, partial-label learning focuses only on the38 cases of ?
?\B ?\E?
and so is able to learnthat??
is a word.As a final remark, we want to make a point that,although the re-decoded labels serve to smooth outthe distribution bias, the Wikipedia data is indeednot the ideal data set for such a purpose, becauseit itself is out of domain.
The performance tendsto degrade when we apply the baseline model tore-decode the out-of-domain Wikipedia data.
Theerrorful re-decoded labels, when being used totrain the model Mcd, could lead to further er-rors.
For example, the baseline model M0is ableto give the correct segmentation ???/???
?in the CTB-6 test set.
However, when it is ap-plied to the Wikipedia data for constrained de-code, for the seven occurrences of??
?, three ofwhich are correctly labelled as ?
?\B?\I?\E?,but the other four have incorrect labels.
The fi-nal model Mcdtrained from these labels thengives incorrect segmentation ??/?/??/?/??/?/??/??/??/??/???
in the CTB-6 test set.
On the other hand, model interpolationor EasyAdapt with partial-label learning, focusingonly on the encoded knowledge and not being im-pacted by the errorful re-decoded labels, performscorrectly in this case.
For a more fair comparisonbetween partial-label learning and constrained de-code, we have also plotted the results of model in-terpolation and EasyAdapt for constrained decodein Figure 5.
As can be seen, they improve on con-strained decode a bit but still fall behind the cor-respondent domain adaptation approach of partial-label learning.6 Conclusion and future workThere is rich information encoded in online webdata.
For example, punctuation and entity tags de-fine some word boundaries.
In this paper we showthe effectiveness of partial-label learning in digest-ing the encoded knowledge from Wikipedia datafor the task of Chinese word segmentation.
Unlikeapproaches such as constrained decode that usethe errorful re-decoded labels, partial-label learn-ing provides a direct means to learn the encodedknowledge.
By integrating some domain adap-tation techniques such as EasyAdapt, we achievean F-measure of 95.98% in the CTB-6 corpus, asignificant improvement from both the supervisedbaseline and constrained decode.
Our results alsobeat (Wang et al., 2011) and (Sun and Xu, 2011).In this research we employ a sausage constraintto encode the knowledge for Chinese word seg-mentation.
However, a sausage constraint doesnot reflect the legal label sequence.
For exam-ple, in Figure 1 the links between label ?B?
andlabel ?S?, between ?S?
and ?E?, and between ?E?and ?I?
are illegal, and can confuse the machinelearning.
In our current work we solve this issueby adding some fully-labelled data into training.Instead we can easily extend our work to use a lat-tice constraint by removing the illegal transitionsfrom the sausage.
The partial-label learning standsthe same, by executing the forward-backward pro-cess in the constrained lattice.
In future work wewill examine partial-label learning with this moreenforced lattice constraint in depth.AcknowledgmentsThe authors would like to thank Wenbin Jiang, Xi-aodong Zeng, and Weiwei Sun for helpful discus-sions, and the anonymous reviewers for insightfulcomments.ReferencesHal Daum?e III.
2007.
Frustratingly easy domain adap-tation.
In Annual meetingassociation for computa-tional linguistics, pages 256?263.
Association forComputational Linguistics.Wenbin Jiang, Meng Sun, Yajuan Lv, Yating Yang,and Qun Liu.
2013.
Discriminative learning withnatural annotations: Word segmentation as a casestudy.
In Proceedings of The 51st Annual Meet-ing of the Association for Computational Linguis-tics, pages 761?769.Feng Jiao, Shaojun Wang, Chi-Hoon Lee, RussellGreiner, and Dale Schuurmans.
2006.
Semi-supervised conditional random fields for improvedsequence segmentation and labeling.
In Proceed-ings of the 21st International Conference on Com-97putational Linguistics and the 44th Annual Meet-ing of the Association for Computational Linguis-tics, ACL-44, pages 209?216.John D. Lafferty, Andrew McCallum, and FernandoC.
N. Pereira.
2001.
Conditional random fields:Probabilistic models for segmenting and labeling se-quence data.
In Proceedings of the Eighteenth In-ternational Conference on Machine Learning, pages282?289, San Francisco, CA, USA.Thomas Lavergne, Olivier Capp?e, and Franc?ois Yvon.2010.
Practical very large scale CRFs.
In Proceed-ings the 48th Annual Meeting of the Association forComputational Linguistics (ACL), pages 504?513.Association for Computational Linguistics, July.Zhongguo Li and Maosong Sun.
2009.
Punctuation asimplicit annotations for Chinese word segmentation.Computational Linguistics, 35:505?512.Percy Liang.
2005.
Semi-supervised learning for natu-ral language.
Master?s thesis, MASSACHUSETTSINSTITUTE OF TECHNOLOGY, May.D.
C. Liu and J. Nocedal.
1989.
On the limited mem-ory bfgs method for large scale optimization.
Math-ematical Programming, 45(3):503?528, December.Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo.
2005.A maximum entropy approach to chinese word seg-mentation.
In Proceedings of the Fourth SIGHANWorkshop on Chinese Language Processing, pages161?164, San Francisco, CA, USA.Gideon S. Mann and Andrew McCallum.
2007.
Ef-ficient computation of entropy gradient for semi-supervised conditional random fields.
In HumanLanguage Technologies 2007: The Conference ofthe North American Chapter of the Associationfor Computational Linguistics; Companion Volume,Short Papers, NAACL-Short ?07, pages 109?112.Weiwei Sun and Jia Xu.
2011.
Enhancing chineseword segmentation using unlabeled data.
In Pro-ceedings of the 2011 Conference on Empirical Meth-ods in Natural Language Processing, pages 970?979, Edinburgh, Scotland, UK., July.Xu Sun, Yaozhong Zhang, Takuya Matsuzaki, Yoshi-masa Tsuruoka, and Jun?ichi Tsujii.
2009.
A dis-criminative latent variable Chinese segmenter withhybrid word/character information.
In Proceedingsof Human Language Technologies: The 2009 An-nual Conference of the North American Chapterof the Association for Computational Linguistics,pages 56?64.Weiwei Sun.
2010.
Word-based and character-basedword segmentation models: comparison and com-bination.
In Proceedings of the 23rd InternationalConference on Computational Linguistics: Posters,pages 1211?1219.Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, RyanMcDonald, and Joakim Nivre.
2013.
Token andtype constraints for cross-lingual part-of-speech tag-ging.
Transactions of the Association for Computa-tional Linguistics, 1:1?12.Yiou Wang, Jun?ichi Kazama, Yoshimasa Tsuruoka,Wenliang Chen, Yujie Zhang, and Kentaro Tori-sawa.
2011.
Improving chinese word segmentationand POS tagging with semi-supervised methods us-ing large auto-analyzed data.
In Proceedings of the5th International Joint Conference on Natural Lan-guage Processing, pages 309?317.Nianwen Xue.
2003.
Chinese word segmentation ascharacter tagging.
Computational Linguistics andChinese Language Processing, pages 29?48.Fan Yang and Paul Vozila.
2013.
An empirical studyof semi-supervised Chinese word segmentation us-ing co-training.
In Proceedings of the 2013 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1191?1200, Seattle, Washington,USA, October.
Association for Computational Lin-guistics.Xiaodong Zeng, Derek F. Wong, Lidia S. Chao, andIsabel Trancoso.
2013a.
Co-regularizing character-based and word-based models for semi-supervisedchinese word segmentation.
In Proceedings of The51st Annual Meeting of the Association for Compu-tational Linguistics, pages 171?176.Xiaodong Zeng, Derek F. Wong, Lidia S. Chao,and Isabel Trancoso.
2013b.
Graph-based semi-supervised model for joint chinese word segmen-tation and part-of-speech tagging.
In Proceedingsof The 51st Annual Meeting of the Association forComputational Linguistics, pages 770?779.Longkai Zhang, Houfeng Wang, Xu Sun, and MairgupMansur.
2013.
Exploring representations from un-labeled data with co-training for Chinese word seg-mentation.
In Proceedings of the 2013 Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 311?321, Seattle, Washington, USA,October.
Association for Computational Linguistics.98
