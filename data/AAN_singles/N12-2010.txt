Proceedings of the NAACL HLT 2012 Student Research Workshop, pages 54?59,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsAutomatic Metrics for Genre-specific Text QualityAnnie LouisUniversity of PennsylvaniaPhiladelphia, PA 19103, USAlannie@seas.upenn.eduAbstractTo date, researchers have proposed differentways to compute the readability and coher-ence of a text using a variety of lexical, syn-tax, entity and discourse properties.
But thesemetrics have not been defined with special rel-evance to any particular genre but rather pro-posed as general indicators of writing qual-ity.
In this thesis, we propose and evalu-ate novel text quality metrics that utilize theunique properties of different genres.
We fo-cus on three genres: academic publications,news articles about science, and machine gen-erated text, in particular the output from auto-matic text summarization systems.1 IntroductionAutomatic methods to measure the writing quality ofa text can be quite useful for several applications, forexample search and recommendation systems, andwriting support and grading tools.
There are twomain categories of prior work on this topic.
The firstis studies on ?readability?
which have proposed met-rics to select texts appropriate (easy to read) for anaudience of given age and education level (Flesch,1948; Collins-Thompson and Callan, 2004).
Thesemetrics typically classify texts as suitable for adultor child, or into a more fine-grained set of 12 ed-ucational grade levels.
The second line of workare recent computational metrics to predict coher-ence.
These methods identify regularities in words(Barzilay and Lee, 2004), entity coreference (Barzi-lay and Lapata, 2008) and discourse relations (Pitlerand Nenkova, 2008) from a large collection of ar-ticles and use these patterns to predict the coher-ence.
They assume a particular competency level(adult educated readers) and also fix the text (typi-cally news articles, which are appropriate for adultreaders).
By removing the focus on age/educationlevel, these methods compute textual differences be-tween good and poorly written texts as perceived bya single audience level.In my thesis, I propose a new definition ?
textquality: the overall well-written characteristic of anarticle.
It differs from prior work in three respects:1.
We consider a single fixed audience level andthe texts that audience is typically exposed to.For example, a college educated reader of anewspaper might find some articles better writ-ten than others, even though he understands andcan read nearly all of them with ease.2.
It is a holistic property of texts.
At a mini-mum, at least four factors influence quality: thecontent/topic that is discussed, sentence level-grammaticality, discourse coherence and writ-ing style.
Here writing style refers to extraproperties introduced into the text by the au-thor but do not necessarily interfere with co-herence if not provided.
For example, the useof metaphors, examples and humour can haveconnections with quality.
Previous work on co-herence metrics do not consider these aspects.3.
Such a property would also have genre-specificdimensions: an academic article should aboveall be clear and a thriller-story should be fast-paced and interesting.
Further even if the same54quality aspect is relevant for multiple genres, ithas higher weight in one versus another.
Priorreadability and coherence studies were not pro-posed with relvance to any particular genre.These aspects make the investigation of text qual-ity linguistically interesting because by definitionthe focus is on a wide range of properties of the textitself rather than appropriateness for a reader.In this thesis, we propose computable measuresto capture genre-specific text quality.
Our hypoth-esis is that writing quality is a combination somegeneric aspects that matter for most texts, such asgrammatical sentences, and other unique ones whichhave high impact in a particular genre.Specifically, we consider three genres whichhave high relevance for writing quality research?academic writing, science journalism and output ofautomatic summarization systems.Both academic writing and science news articlesdescribe science, but their audience is quite differ-ent.
Academic writing aims to clearly explain thedetails of the research to other experts, while sci-ence news conveys interesting research findings tolay readers.
This fact creates distinctive content andwriting style in the two genres.
There is also a hugeopportunity in these genres for developing applica-tions involving text quality, for example, authoringtools for academic writing and information retrivaland recommendation for news articles.
We also in-clude a third genre?automatically generated sum-maries.
Here, when systems produce multi-sentencetext, they must ensure that the text is readable andcoherent.
Automatic evaluation of content and lin-guistic quality is therefore necessary for system de-velopment in this genre.2 Thesis Summary and ContributionsFor this thesis, we only consider the discourse andstyle components of text quality, aspects that havereceived less focus in prior work.
Sentence-levelproblems have been widely explored and recently,even specifically for academic writing (Dale andKilgarriff, 2010).
We also do not consider contentin our work, for example, academic writing qualityalso depends on the ideas and arguments presentedbut these aspects are outside the scope of this thesis.As defined previously, we focus on a fixed audiencelevel.
We assume a reader at the top level of thecompetency spectrum: an adult educated reader forscience news and automatic summaries, and for aca-demic articles, an expert on the topic.
This definitionhas minimal focus on reader abilities and allows usto analyze textual differences exclusively.The specific contributions of this thesis are:1.
Defining text quality in terms of linguistic as-pects rather than readability: Our work is the firstto propose a quality definition where well-writtennature is the central focus and including genre-dependent aspects and writing style.2.
Investigating genre-specific metrics: This studyis also the first to design and evaluate genre-specificfeatures for text quality prediction.
For each genre:academic writing, science journalism and automaticsummaries, we develop metrics unique to the genreand evaluate their ability to predict text quality bothindividually and in combination with generic fea-tures put forth in prior work.3.
Proposing new discourse-level features: Inprior work, there are discourse-based features basedon coreference, discourse relations and word co-occurrence between adjacent sentences.
We intro-duce new features which capture aspects such asorganization of communicative goals and general-specific nature of sentences.Specifically, we introduce the following metrics:a) Patterns in communicative goals (Section 5):Every text has a purpose and the author uses asequence of communicative goals realized as sen-tences to convey that purpose.
We introduce a met-ric that predicts coherence based on the size and se-quence of communicative goals for a genre.
This as-pect is most relevant for research writing: academicand science journalism because there is a clear goaland well-defined purpose for these articles.b) General-specific nature of sentences (Section6): Some sentences in a text convey only generalcontent, others provide details and a well-writtentext would have a certain balance between the two.Particularly, while creating summaries, there is alength contraint, so it cannot include all specific con-tent but some information must be made more gen-eral.
We introduce a method to predict the specificityfor a sentence and examine how specificity and se-quence of general-specific sentences is related to the55quality of automatic summaries.c) Information cohesiveness (Section 7): Thisidea is also proposed for automatic summaries, theymust have a focus and present a small set of ideaswith easy to understand links between them.
Weshow that cohesiveness properties (computed auto-matically) of the source text to be summarized canbe linked to the expected content quality of sum-maries that can be generated for that text.
This workwill be extended to analyze the relationship of cohe-siveness with ratings of focus for the summaries.d) Aspects of style (Section 8): Here we inves-tigate metrics beyond coherence and related to ex-tra features included in the article.
We consider thegenre of science journalism and investigate whethersurprise-invoking sentence construction, visual de-scriptions and emotional content of the articles arealso correlated with perceived quality.We will evaluate our approaches in two ways:1.
We investigate the extent to which genre-specific metrics are indicative of text quality andwhether they complement generic features.2.
We also examine how unique these metrics arefor a given genre, for example: are surprising arti-cles always considered well-written even if they arenot science-news?
For this analysis, we will con-sider a set of randomly selected news texts (no genredivision) with text quality ratings.
On this set, wewill test the performance of generic and each setof genre-specific metrics.
We expect that on thisdata, the generic features would be best with littleimprovement from the genre-specific metrics.So far, we have designed some of the metrics thatwe described above and have found them to be pre-dictive of writing quality.
We will carry out exten-sive evaluation of these measures in future work.3 Related workEarly readability metrics used sentence length, num-ber of syllables in words and number of ?easy?words to distinguish texts from different grade lev-els (Flesch, 1948; Gunning, 1952; Dale and Chall,1948).
Other measures are based on word familiarity(Collins-Thompson and Callan, 2004; Si and Callan,2001), difficulty of concepts (Zhao and Kan, 2010)and features of sentence syntax (Schwarm and Os-tendorf, 2005).
There are also readability studies foraudience distinctions other than grade levels.
Fenget al (2009) consider adult readers with intellectualdisability and therefore introduce features such asthe number of entities a person should keep in work-ing memory for that text and how far entity linksstretch.
Heilman et al (2007) show that grammati-cal features make a bigger impact while predictingreadability for second language learners in contrastto native speakers.Newer coherence measures do not focus on readerabilities.
They are typically run on news articlesand assume an adult audience.
They show thatword co-occurrence (Soricut and Marcu, 2006), sub-topic structure (Barzilay and Lee, 2004), discourserelations (Pitler and Nenkova, 2008; Lin et al,2011) and coreference patterns (Barzilay and Lap-ata, 2008) learn from large corpora can be used topredict coherence.But prior metrics are not proposed as unique toany genre.
Some metrics using word patterns (Si andCallan, 2001; Barzilay and Lee, 2004) are domain-dependent in that they require documents from thetarget domain for training.
But they can be trainedfor any domain in this manner.However recent work show that genre-specificindicators could be quite useful for applications.McIntyre and Lapata (2009) automatically generateshort children?s stories using patterns of event andentity co-occurrences.
They find that people judgetheir stories as better when the text is optimized notonly for coherence and but also its interesting nature.They use a supervised approach to predict the inter-est value for a story during the generation process.Burstein et al (2010) find that for predicting the co-herence of student essays, better accuracies can beobtained by augmenting generic coherence metricswith features related to student writing such as wordvariety and spelling errors.In my own work on automatic evaluation of sum-maries (Pitler et al, 2010), I have observed the im-pact of genre.
We consider a corpus of summarieswritten by people and those produced by automaticsystems.
Psycholinguistic metrics previously pro-posed for analyzing coherence of human texts worksuccessfully on human summaries but are less ac-curate for system summaries.
Similarly, metricswhich predict the fluency of machine translationsaccurately, work barely above baseline for judgingthe grammaticality of sentences from human sum-56maries.
But they give high accuracies on machinesummary sentences.
So for machine and human gen-erated text, clearly different features matter.4 Corpora for text qualityFor the automatic summarization genre, severalyears of evaluation workshops organized by NIST1have created large-scale datasets of automatic sum-maries rated manually by people for content and lin-guistic quality.
We utilize this data for our experi-ments but such corpora do not exist for other genres.For academic writing, we plan to use a collectionof biology journal articles marked with the impactfactor of the journal.
The intuition is that the pop-ular journals are more competitive and so the writ-ing is on average better than less impactful venues.It is however not a direct measure of text quality.For some of our experiments done so far, we havetaken an approach that is common with prior studieson coherence (Barzilay and Lee, 2004; Barzilay andLapata, 2008; Lin et al, 2011).
We take an originalarticle and create a random permutation of its sen-tences, the latter we consider as an incoherent articleand the original version as coherent.For science news, we expect that Amazon Me-chanical Turk will be a suitable platform for obtain-ing ratings of popular and interesting articles fromthe target audience.
We also plan to use proxies suchas lists of most emailed/viewed articles from newswebsites.
Here the negative examples would beother articles published during the same day/periodbut not appearing in the popular article list.5 Patterns in communicative goalsConsider the related work section of a conferencepaper.
One might suppose that a good structure forthis section would contain a description of an at-tribute of the current work, followed by previouswork on the topic and then reporting how the currentwork is different and addresses shortcomings if anyof prior work.
In fact, this intuition of seeing textsas a sequence of semantic zones is well-understoodfor the academic writing genre.
Prior research hasidentified that a small set of argumentative zones ex-ist in academic articles such as motivation, results,prior work, speculations and descriptions.
They also1http://www.nist.gov/tac/found that sentences could be manually annotatedinto zones with high agreement and automaticallypredicting the zone for a sentence can also be donewith high accuracy (Teufel and Moens, 2000; Li-akata et al, 2010).
We hypothesize that these zoneswould also have a certain distribution and sequencein well-written articles versus others and propose ametric based on this aspect for the academic writingand science journalism genres.Rather than using a predefined set of communica-tive goals, we develop an unsupervised techniqueto identify analogs to semantic zones and use thepatterns in zones to predict coherence (Louis andNenkova, 2012a).
Our key idea is that the syntaxof a sentence can be a useful proxy for its commu-nicative goal.
For example, questions and definitionsentences have unique syntax.
We extend this ideato a large scale analysis.
Our model represents a sen-tence either using productions from its constituencyparse tree or as a sequence of phrasal nodes.
Thenwe employ two methods that learn patterns in theserepresentations from a collection of articles.
Thefirst local method detects patterns in the syntax ofadjacent sentences.
The second approach is global,where sentences are first grouped into clusters basedon syntactic similarity and a Hidden Markov Modelis used to record patterns.
Each hidden state is as-sumed to generate the syntax of sentences from aparticular zone.We have evaluated our method on conferencepublications from the ACL anthology.
Our resultsindicate that we can distinguish an original introduc-tion, abstract or related work section from a corre-sponding perturbed version (where the sentences arerandomly permuted and is therefore incoherent text)with accuracies of 64 to 74% over a 50% baseline.6 General-specific nature of sentencesIn any article, some sentences convey the topic ata high level with other sentences providing detailssuch as justification and examples.
The idea is par-ticularly relevant for summaries.
Since summariesare much shorter than their source documents, theycannot include all the details from the source.
Somedetails have to be omitted and others made moregeneral.
So we explore the preferred degree ofgeneral-specific content and its relationship to textquality for summaries.57We developed a classifier to distinguish betweengeneral and specific sentences from news articles(Louis and Nenkova, 2011a; Louis and Nenkova,2012b).
The classifier uses features such as the wordspecificity, presence of named entities, word polar-ity, counts of different phrase types, sentence length,likelihood under language models and the identitiesof the words themselves.
For example, sentenceswith named entities tended to be specific whereassentences with shorter verb phrases and more polar-ity words were general.
This classifier was trainedon sentences multiply annotated by people as gen-eral or specific and produces an accuracy of about79%.
Further the classifier confidence was found tobe indicative of the annotator agreement on the sen-tences; when there was high agreement that a sen-tence was either general or specific, the classifieralso made a very confident prediction for the correctclass.
So our system also provides a graded scorefor specificity rather than binary predictions.Using the classifier we analyzed a large corpus ofnews summaries created by people and by automaticsystems (Louis and Nenkova, 2011b).
We foundthat summaries written by people have more generalcontent than automatic summaries.
Similarly, whenpeople were asked to rate automatic summaries forcontent quality, they gave higher scores to generalsummaries than specific.
On the linguistic qualityside an opposite trend was found.
Summaries thatwere more specific had higher scores.
Our examina-tions revealed that general sentences, since they aretopic oriented and high level, need to be followedby proper substantiation and details.
But automaticsystems are rather poor at achieving such ordering.So even though more general content is preferred insummaries, proper ordering of general-specific sen-tences is needed to create the right effect.7 Information cohesivenessIf an article has too many ideas it would be difficultto read.
Also if the ideas were not closely relatedin the article that would create additional difficulty.This aspect is important for machine generated text:an automatic summary should focus on a few mainaspects rather than present a bag of many unrelatedfacts.
In fact, in large scale evaluation workshops,automatic summaries are also manually graded for a?focus?
aspect.
For this purpose, we want to identifymetrics which can indicate cohesiveness and focusof an article.
In our studies so far, we have havedeveloped cohesiveness metrics for clusters of arti-cles (Nenkova and Louis, 2008; Louis and Nenkova,2009).
In future work, we will explore how thesemetrics work for individual articles.Information quality also arises in the context ofsource documents given for automatic summariza-tion.
Particularly for systems which summarize on-line news, the input is created by clustering togethernews on the same topic from different sources.
Forexample, a cluster may be created for the Japaneseearthquake and aftermath.
When the period coveredis too large or when the documents discuss manydifferent opinions and ideas it becomes hard for asystem to point out the most relevant facts.
So oneproxy for cohesiveness of the input cluster is the av-erage quality of a number of automatic summariesproduced for it by different methods.
If most ofthese methods fail to produce a good summary, thenthat input can be deemed as difficult and incohesive.We used a large collection of inputs, their au-tomatic summaries and summary scores from theDUC workshops.
We computed the average contentquality score given by people to each summary andcomputed the average performance on summariescreated for the same input.
This value represents theexpected system performance for that input and wedevelop features to predict the same.
We simplifythe task as binary prediction, average system perfor-mance above mean value ?
low difficulty, and highdifficulty otherwise.One indicative feature was the entropy of the dis-tribution of words in the input.
When the entropywas low, the difficulty was less since there are fewmain ideas to summarize.
Another useful featurewas the divergence computed between the word dis-tribution in an input and that of a random collectionof documents not on any topic.
If the input distri-bution was closer to random documents it indicatesthe lack of a coherent topic for the source clusterand such inputs were under the hard category.
Weenvision that similar features might help to predictjudgements of focus for automatic summaries.8 Current and future workFor future work, we want to focus on metrics relatedto style of writing.
We will do this analysis for sci-58ence news articles since journalists employ creativeways to convey technical research content to non-experts readers.
For example, authors use analogiesand visual language and incorporate a story line.
Wealso noticed that some of the most emailed articlesare entertaining and even contain humor.
Two exam-ple snippets from such articles are provided below todemonstrate some of our intuitions about text qualityin this genre.
Our aim is to obtain lexical and syn-tactic correlates that capture some of these uniquefactors for this domain.[1]...
caused by defects in the cilia?solitary sliversthat poke out of almost every cell in the body.
They arenot the wisps that wave Rockette-like in our airways.
[2] News flash: we?re boring.
New research that makescreative use of sensitive location-tracking data from cell-phones in Europe suggests that most people can be foundin one of just a few locations at any time.Future work will also include extensive evaluationof our proposed models.ReferencesR.
Barzilay and M. Lapata.
2008.
Modeling local coher-ence: An entity-based approach.
Computational Lin-guistics, 34(1):1?34.R.
Barzilay and L. Lee.
2004.
Catching the drift: Proba-bilistic content models, with applications to generationand summarization.
In Proceedings of NAACL-HLT,pages 113?120.J.
Burstein, J. Tetreault, and S. Andreyev.
2010.
Usingentity-based features to model coherence in student es-says.
In Proceedings of HLT-NAACL, pages 681?684.K.
Collins-Thompson and J. Callan.
2004.
A languagemodeling approach to predicting reading difficulty.
InProceedings of HLT-NAACL, pages 193?200.E.
Dale and J. S. Chall.
1948.
A formula for predictingreadability.
Edu.
Research Bulletin, 27(1):11?28.R.
Dale and A. Kilgarriff.
2010.
Helping our own:text massaging for computational linguistics as a newshared task.
In Proceedings of INLG, pages 263?267.L.
Feng, N. Elhadad, and M. Huenerfauth.
2009.
Cog-nitively motivated features for readability assessment.In Proceedings of EACL, pages 229?237.R.
Flesch.
1948.
A new readability yardstick.
Journal ofApplied Psychology, 32:221 ?
233.R.
Gunning.
1952.
The technique of clear writing.McGraw-Hill; Fouth Printing edition.M.
Heilman, K. Collins-Thompson, J. Callan, and M. Es-kenazi.
2007.
Combining lexical and grammati-cal features to improve readability measures for firstand second language texts.
In Proceedings of HLT-NAACL, pages 460?467.M.
Liakata, S. Teufel, A. Siddharthan, and C. Batchelor.2010.
Corpora for the conceptualisation and zoning ofscientific papers.
In Proceedings of LREC.Z.
Lin, H. Ng, and M. Kan. 2011.
Automatically evalu-ating text coherence using discourse relations.
In Pro-ceedings of ACL-HLT, pages 997?1006.A.
Louis and A. Nenkova.
2009.
Performance con-fidence estimation for automatic summarization.
InProceedings of EACL, pages 541?548.A.
Louis and A. Nenkova.
2011a.
Automatic identi-fication of general and specific sentences by leverag-ing discourse annotations.
In Proceedings of IJCNLP,pages 605?613.A.
Louis and A. Nenkova.
2011b.
Text specificity andimpact on quality of news summaries.
In Proceedingsof the Workshop on Monolingual Text-To-Text Genera-tion, pages 34?42.A.
Louis and A. Nenkova.
2012a.
A coherence modelbased on syntactic patterns.
Technical Report, Univer-sity of Pennsylvania.A.
Louis and A. Nenkova.
2012b.
A corpus of generaland specific sentences from news.
In Proceedings ofLREC.N.
McIntyre and M. Lapata.
2009.
Learning to tell tales:A data-driven approach to story generation.
In Pro-ceedings of ACL-IJCNLP, pages 217?225.A.
Nenkova and A. Louis.
2008.
Can you summa-rize this?
identifying correlates of input difficulty formulti-document summarization.
In Proceedings ofACL-HLT, pages 825?833.E.
Pitler and A. Nenkova.
2008.
Revisiting readabil-ity: A unified framework for predicting text quality.
InProceedings of EMNLP, pages 186?195.E.
Pitler, A. Louis, and A. Nenkova.
2010.
Automaticevaluation of linguistic quality in multi-documentsummarization.
In Proceedings of ACL.S.
Schwarm and M. Ostendorf.
2005.
Reading level as-sessment using support vector machines and statisticallanguage models.
In Proceedings of ACL, pages 523?530.L.
Si and J. Callan.
2001.
A statistical model for scien-tific readability.
In Proceedings of CIKM, pages 574?576.R.
Soricut and D. Marcu.
2006.
Discourse generation us-ing utility-trained coherence models.
In Proceedingsof COLING-ACL, pages 803?810.S.
Teufel and M. Moens.
2000.
What?s yours and what?smine: determining intellectual attribution in scientifictext.
In Proceedings of EMNLP, pages 9?17.J.
Zhao and M. Kan. 2010.
Domain-specific iterativereadability computation.
In Proceedings of JDCL,pages 205?214.59
