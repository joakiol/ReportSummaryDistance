Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 17?24,Athens, Greece, 31 March 2009. c?2009 Association for Computational LinguisticsUnsupervised Classification with Dependency Based Word SpacesKlaus Rothenh?usler and Hinrich Sch?tzeInstitute for Natural Language ProcessingUniversity of StuttgartStuttgart, Germany{Klaus.Rothenhaeusler, Hinrich.Schuetze}@ims.uni-stuttgart.deAbstractWe present the results of clustering exper-iments with a number of different evalu-ation sets using dependency based wordspaces.
Contrary to previous results wefound a clear advantage using a parsedcorpus over word spaces constructed withthe help of simple patterns.
We achieveconsiderable gains in performance overthese spaces ranging between 9 and 13%in absolute terms of cluster purity.1 IntroductionWord space models have become a mainstay in theautomatic acquisition of lexical semantic knowl-edge.
The computation of semantic relatedness oftwo words in such models is based on their distri-butional similarity.
The most crucial way in whichsuch models differ is the definition of distribu-tional similarity: In a regular word space modelthe observed distribution concerns the immediateneighbours of a word within a predefined win-dow to the left and right (Sch?tze, 1992; Sahlgren,2006).
Early on in the development as an alter-native models were proposed that relied on thesimilarity of the distribution of syntactic relations(Hindle, 1990; Pad?
and Lapata, 2007).
Morerecently the distribution of the occurrence withinsimple patterns defined in the form of regular ex-pressions that are supposed to capture explicit se-mantic relations was explored as the basis of distri-butional similarity (Almuhareb and Poesio, 2004).Whereas dependency based semantic spaceshave been shown to surpass other word space mod-els for a number of problems (Pad?
and Lapata,2007; Lin, 1998), for the task of categorisationsimple pattern based spaces have been shown toperform equally good if not better (Poesio and Al-muhareb, 2005b; Almuhareb and Poesio, 2005b).We want to show that dependency based spacesalso fare better in these tasks if the dependency re-lations used are selected reasonably.
At the sametime we want to show that such a system can bebuilt with freely available components and with-out the need to rely on the index of a proprietarysearch engine vendor.We propose to use the web acquired data of theukWaC (Ferraresi et al, 2008), which is huge butstill manageable and comes in a pre-cleaned ver-sion with HTML markup removed.
It can easilybe fed into a parser like MiniPar which allows forthe subsequent extraction of dependency relationsof different types and complexity.
In particular wework with dependency paths that can reach beyonddirect dependencies as opposed to Lin (1998) butin the line of Pado and Lapata (2007).
In contrastto the latter, however, different paths that end inthe same word are not generally mapped to thesame dimension in our model.
A path in a depen-dency graph can pass through several nodes andencompass different relations.We experimented with two sets of nouns pre-viously used in the literature for word clustering.The nouns in both sets are taken from a numberof different WordNet categories.
Hence, the taskconsists in clustering together the words from thesame category.
By keeping the clustering algo-rithm constant, differences in performance can beattributed to the differences of the word represen-tations.The next section provides a formal descriptionof our word space model.
Section 3 reports on ourclustering experiments with two sets of conceptsused previously to evaluate the categorisation abil-ities of word spaces.
Section 4 discusses these re-17sults and draws some conclusions.2 Word Space ConstructionWe follow the formalisation and terminology de-veloped in Pado and Lapata (2007) according towhich a dependency based space is determined bythe sets of its basis elements B and targets T thatform a matrix M = B ?
T , a similarity functionS that assigns a real-valued similarity measure topairs of elements from T , the association measureA that captures the strength of the relation betweena target and a basis element, the context selectionfunction cont, the basis mapping function ?
andthe path value function v. Our set of targets is al-ways a subset of the lemmas output by MiniPar.The remaining elements are defined in this section.We use pi to denote a path in a dependency graphwhich is conceived of as an undirected graph forthis purpose.
So, in general a dependency path hasan upward and downward part where one can havelength zero.
All the paths used to define the con-texts for target words are anchored there, i.e.
theystart from the target.In choosing the context definitions that deter-mine what dependency paths are used in the con-struction of the word vectors, we oriented our-selves at the sets proposed in Pado and Lap-ata (2007).
As Pado and Lapata (2007) achievedtheir best results with it we started from theirmedium sized set of context definitions, fromwhich we extracted the appropriate ones for ourexperiments and added some that seemed to makesense for our purposes: As our evaluation sets con-sist entirely of nouns, we used only context defi-nitions that start at a noun.
Thereby we can en-sure that only nominal uses are recorded in a wordvector if a target word can have different parts ofspeech.
The complete set of dependency relationsour context selection function cont comprises isgiven in Figure 1 along with an example for each.We only chose paths that end in an open wordclass assuming that they are more informativeabout the meaning of a target word.
Paths end-ing in a preposition for instance, as used byPado and Lapata (2007), were not considered.
Forthe same reason we implemented a simple stopword filter that discards paths ending in a pronoun,which are assigned the tag N by MiniPar just likeany other noun.On the other hand we added the relation be-tween a prepositional complement and the noun itmodifies (appearing as relation IX in Figure 1) asa close approximation of the pattern used by (Al-muhareb and Poesio, 2004) to identify attributesof a concept as detailed in the next section.
Pathspecifications X and XI are also additions wemade that are thought to gather additional attributevalues to the ones already covered by III.As a basis mapping function ?
we used a gen-eralisation of the one used by Grefenstette (1994)and Lin (1998).
They map a dependency betweentwo words to a pair consisting of the relation la-bel l and the end word of the dependency end(pi).As we use paths that span more than a single re-lation, this approach is not directly applicable toour setup.
Instead we use a mapping function thatmaps a path to the sequence of edge labels throughwhich it passes combined with the end word:?
(pi) = (l(pi),end(pi))where l(?)
is a labelling function that returnsthe sequence of edge labels for a given path.With this basis mapping function the nodes orwords respectively through which a path passesare all neglected except for the node where thepath ends.
So, for the noun human the se-quence human and mouse genome as well asthe sequence human and chimpanzee genomeincrease the count for the same basis element:N:conj:N:*:N:nn:N:genome.
Here weuse a path notation of the general form:(: POS : rel : POS : {word,?
})nwhere POS is a part of speech, rel a relation andword a node label, i.e.
a lemma, all as producedby MiniPar.
The length of a path is determined byn and the asterisk (*) indicates that a node label isignored by the basis mapping function.As an alternative we experimented with a lexi-cal basis mapping function that maps a path to itsend word:?
(pi) = end(pi)This reduces the number of dimensions consider-ably and yields semantic spaces that are similarto window based word spaces.
As this mappingfunction consistently delivered worse results, wedropped it from our evaluation.Considering that (Pad?
and Lapata, 2007) onlyreported very small differences for different pathvaluation functions, we only used a constant valu-ation of paths:vconst(pi) = 118(I) the subject of a verbAll humans die.PreDetNVpresubj(II) an object of a verbGods from another world created humansVNsubj obj(III) modified by an adjectiveYoung dogs are like young humansVBEPrepANs predpcomp-nmod(IV) linked to another noun via a genitive relationThe human?s eyes glimmered with comprehensionDetNNVdetsubj modgen(V) part of a nominal complexThe human body presents a problem.Det NNVsubjdetobjnn(VI) part of a conjunctionHumans and animals are equally fair game.NU NVBEspuncpredconj(VII) the subject of a predicate nounHumans are the only specie that has sex for pleasure.NVBENCsdet, modpredrelsubj(VIII) the subject of a predicate adjectiveHumans are fallible.NVBEAs predsubj(IX) the prepositional complement modifying a nounYou must get into the mind of humans.N AuxVDetNPrepNs auxdetobjmodpcomp-n(X) the prepositional complement modifying anoun that is the subject of a predicate adjectiveThe nature of humans is corrupt.NPrepNVBEAsmodpcomp-npred(XI) the prepositional complement modifying a noun that is the subject of a predicate nounChief diseases of humans are infections.NPrepNVBENsmodpcomp-npred(XII) relations I-IV and VI-XI above but now with the target as part of a complex noun phrase as shown fora conjunction relation (VI) in the exampleThey interrogated him about the human body and reproduction.PrepDet NNU Nmoddetpcomp-npuncnn conjFigure 1: Context definitions used in the construction of our word spaces.
All examples show contextsfor the target human.
Greyed out parts are just for illustrative purposes and have no impact on the wordvectors.
The examples are slightly simplified versions of sentences found in ukWaC.19Thus, an occurrence of any path, irrespective oflength or grammatical relations that are involved,increases the count of the respective basis elementby one.We implemented three different associationfunctions, A, to transform the raw frequencycounts and weight the influence of the different co-occurrences.
We worked with an implementationof the log likelihood ratio (g-Score) as proposedby Dunning (1993) and two variants of the t-score,one considering all values (t-score) and one whereonly positive values (t-score+) are kept followingthe results of Curran and Moens (2002).
We alsoexperimented with different frequency cutoffs re-moving dimensions that occur very frequently orvery rarely.3 EvaluationFor all our experiments we used the ukWaC cor-pus1 to construct the word spaces, which wasparsed using MiniPar.
The latter provides lemmainformation, which we used as possible target andcontext words.
The word vectors we built fromthis data were represented as pseudo documents inan inverted index.
To our knowledge the experi-ments described in this paper are the first to workwith a completely parsed version of the ukWaC.For the evaluation the word vectors for thetest sets were clustered into a predefined numberof clusters corresponding to the number of con-cept classes from which the words were drawn.All experiments were conducted with the CLUTOtoolkit (Karypis, 2003) using the repeated bisec-tions clustering algorithm with global optimisa-tion and the cosine as a distance measure to main-tain comparability with related work, e.g.
Ba-roni et al (2008).As the main evaluation measure we used pu-rity for the whole set as supplied by CLUTO.
Fora clustering solution ?
of n clusters and a set ofclasses C, purity can be defined as:purity(?,C) = 1n?kmaxj|?k ?
c j|where ?k denotes the set of terms in a cluster andc j the set of terms in a class.
This aggregate mea-sure of purity corresponds to the weighted sum ofpurities for the individual clusters, which is de-fined as the ratio of items in a cluster that belongto the majority class.
The results for the two test1http://wacky.sslmit.unibo.itsets we used are described in the following twosubsections.3.1 Results for 214 nouns fromAlmuhareb and Poesio (2004)The first set we worked with was introduced byAlmuhareb and Poesio (2004) and consists of 214nouns from 13 different categories in WordNet.
Inthe original paper the best results were achievedwith vector representations built from concept at-tributes and their values as identified by simplepatterns.
For the identification of attribute valuesof a concept C the following pattern was used?
[a|an|the] * C [is|was]?It will find instances such as an adult human isidentifying adult as a value for an attribute (age)of [HUMAN] (we use small capitals enclosed insquare brackets to denote a concept).
Attributesthemselves are searched with the pattern?the * of the C [is|was]?A match for the concept [HUMAN] would be thedignity of the human is, which yields dignity asan attribute.
These patterns were translated intoqueries and submitted to the Google2 search en-gine.We compare our dependency based spaces withthe results achieved with the pattern based ap-proach in Table 1.associationmeasureg-score t-score t-score+dependencybased space77.1% 85.5% 96.7%window basedspace84.1% 82.7% 89.3%pattern basedspace- - 85.5%Table 1: Categorisation results for the 214concepts and 13 classes proposed in Al-muhareb and Poesio (2004), which is alsothe source of the result for the pattern based space.They only used t-score+.
The numbers given arethe best accuracies achieved under the differentsettings.For the window based space we used the bestperforming in a free association task with a win-dow size of six words to each side and all the2http://www.google.com20context accuracy # dimensions(I) 82.2% 7359(II) 92.5% 6680(III) 88.3% 45322(IV) ?
37231(V) 82.2% 240157(VI) 95.3% 93917(VII) 86.9% 45527(VIII) 77.1% 5245(IX) 91.6% 87765(X) ?
2186(XI) ?
6967(XII) 93.0% 188763Table 2: Clustering results using only one kind ofpath specification.
For (IV), (X) and (XI) purityvalues are missing because vectors for some of thewords could not be built.words that appeared at least two times as dimen-sions ignoring stop words.
The effective dimen-sionality of the so built word vectors is 417 837.The results for the dependency based spaceswere built by selecting all paths without anyfrequency thresholds which resulted in a set of767 119 dimensions.As can be seen, both window and dependencybased spaces exceed the pattern based space forcertain association measures.
But the dependencyspace also has a clear advantage over the windowbased space.
In particular the t-score+ measureyields very good results.
In contrast the g-scoreoffers the worst results with the t-score retainingnegative values somewhere in between.
For ourfurther experiments we hence used the t-score+ as-sociation measure.3.1.1 Further AnalysisWe ran a number of experiments to quantify theimpact the different kinds of paths have on theclustering result.
We first built spaces using onlya single kind of path to find out how good eachperforms on its own.
The result can be found inTable 2.
For some of the words in the evaluationset no contexts could be found when only one ofthe two most complex context specifications (X),(XI) was used or when the context was reduced tothe genitive relation (IV).
Apart from that the re-sults suggest that even a single type of relation onits own can prove highly effective.
Especially theconjunctive relation (VI) performs very well witha purity value of 95.3%.removed context accuracy(I) 97.2%(II) 97.7%(III) 97.2%(IV) 97.2%(V) 98.1%(VI) 96.3%(VII) 97.2%(VIII) 97.2%(IX) 96.7%(X) 97.2%(XI) 97.2%(XII) 96.7%Table 3: Clustering results for spaces with onecontext specification removed.To further clarify the role of the different kindsof contexts, we ran the experiment with wordspaces where we removed each one of the twelvecontext specifications in turn.
The results as givenin Table 3 are a bit astonishing at first sight: Onlythe removal of the conjunctive relation actuallyleads to a decrease in performance.
All the othercontexts seem to be either redundant ?
with per-formance staying the same when they are removed?
or even harmful ?
with performance increasingonce they are removed.
Having observed this, wetried to remove further context specifications andsurprisingly found that the best performance of98.1% can be reached by only including the con-junction (VI) and the object (II) relations.
The di-mensionality of these vectors is only a fraction ofthe original ones with 100 597.The result for the best performing dependencybased space listed in the table is almost perfect.Having a closer look at the results reveals that infact only four words are put into a wrong cluster.These words are: lounge, pain, mouse, oyster.The first is classified as [BUILDING] instead of[FURNITURE].
In the case of lounge the misclas-sification seems to be attributable to the ambiguityof the word which can either denote a piece of fur-niture or a waiting room.
The latter is apparentlythe more prominent sense in the data.
In this usagethe word often appears in conjunctions with roomor hotel just like restaurant, inn or clubhouse.Pain is misclassified as an [ILLNESS] insteadof a [FEELING] which is at least a close miss.The misclassification of mouse as a [BODY PART]seems rather odd on the other hand.
The reason for21it becomes apparent when looking at the most de-scriptive and discriminating features of the [BODYPART] cluster: In both lists the highest in the rank-ing is the dimension :N:mod:A:left, i.e.
leftas an adjectival modifier of the word in question.The prominence of this particular modification isof course due to the fact that a lot of body partscome in pairs and that the members of these pairsare commonly identified by assigning them to theleft or right half of the body.
Certainly, the wordmouse enters this cluster not through its sense ofmouse1 as an animal but rather through its sense ofmouse2 as a piece of computer equipment that hastwo buttons, which are also referred to as the leftand right one.
Unfortunately, MiniPar frequentlyresolves left in a wrong way as a modifier of mouseinstead of button.Finally for oyster which is put into the [EDIBLEFRUIT] instead of the [ANIMAL] cluster it is con-spicuous that oyster is the only sea animal in theevaluation set and consequently it rarely occursin conjunctions with the other animals.
Conjunc-tions, however, seem to be the most important fea-tures for defining all the clusters.
Additionallyoyster scores low on a lot of dimensions that aretypical for a big number of the members of the an-imal cluster, e.g.
:N:obj:V:kill.3.2 Results for 402 words fromAlmuhareb and Poesio (2005a)In Poesio and Almuhareb (2005a) a larger evalu-ation set is introduced that comprises 402 nounssampled from the hierarchies under the 21 uniquebeginners in WordNet.
The words were also cho-sen so that candidates from different frequencybands and different levels of ambiguity were rep-resented.
Further results using this set are reportedin Almuhareb and Poesio (2005b).
The best resultwas obtained with the attribute pattern alone andfiltering to include only nouns.
We tried to assem-ble word vectors with the same patterns based onthe ukWaC corpus.
But even if we included bothpatterns, we were only able to construct vectorsfor 363 of the 402 words.
For 118 of them thenumber of occurrences, on which they were based,was less than ten.
This gives an impression of thesize of the index that is necessary for such an ap-proach.
To date such an immense amount of datais only available through proprietary search engineproviders.
This makes a system dependant uponthe availability of an API of such a vendor.
In factthe version of the Google API on which the orig-inal experiments relied has since been axed.
Ourapproach circumvents such problems.We ran analogous experiments to the ones de-scribed in the previous section on this evaluationset, now producing 21 clusters.
The results givenin Table 4 are for a dependency space without anyfrequency thresholds and the complete set of con-text specifications as defined above.
The settingsfor the window based space were also the same(6 words to each side).
Again the results achievedwith the t-score+ association were clearly superiorto the others and were used in all the followingexperiments.
Unsurprisingly, for this more diffi-cult task the performance is not as good as for thesmaller set but nevertheless the superiority of thedependency based space is clearly visible with anabsolute increase in cluster purity of 8.2% com-pared with the pattern based space.associationmeasureg-score t-score t-score+dependencybased space67.9% 67.2% 79.1%window basedspace65.7% 60.7% 67.9%pattern basedspace- - 70.9%Table 4: Categorisation results for the 402concepts and 21 classes proposed in Al-muhareb and Poesio (2005a) which is alsothe source of the result for the pattern basedspace.
The numbers given are the best accuraciesachieved under the different settings.3.2.1 Further AnalysisAgain we ran further experiments to determine theimpact of the different kinds of relations.
The re-moval of any single context specification leads toa performance drop with this evaluation set.
Thesmallest decrease is observed when removing con-text specification XII.
However, as we had seen inthe previous experiment with the smaller set thatonly two context specifications suffice to reachpeak performance, we conducted another exper-iment where we started from the best perform-ing space constructed from a single context spec-ification (the conjunction relation, VI) and suc-cessively added the specification that led to thebiggest performance gain.
The crucial results are22majority class conceptssolid tetrahedron, salient, ring, ovoid, octahedron, knob, icosahedron, fluting, dome, dodecahedron,cylinder, cuboid, cube, crinkle, concavity, samba, coco, nonce, divan, ball, stitch, floater, trove,hoard, mousetime yesteryear, yesterday, tonight, tomorrow, today, quaternary, period, moment, hereafter, gesta-tion, future, epoch, day, date, aeon, stretch, snap, throb, straddle, napmotivation wanderlust, urge, superego, obsession, morality, mania, life, impulse, ethics, dynamic, con-science, compulsion, plasticity, opinion, acceptance, sensitivity, desire, interestassets wager, taxation, quota, profit, payoff, mortgage, investment, income, gain, fund, credit, cap-ital, allotment, allocation, possession, inducement, incentive, disincentive, deterrence, share,sequestrian, cheque, check, bond, tailordistrict village, town, sultanate, suburb, state, shire, seafront, riverside, prefecture, parish, metropolis,land, kingdom, county, country, city, canton, borough, borderland, anchorage, tribe, nation,house, fen, cordoba, farolegal document treaty, statute, rescript, obligation, licence, law, draft, decree, convention, constitution, bill,assignment, commencement, extension, incitement, caliphate, clemency, venture, dispensationphysical property weight, visibility, temperature, radius, poundage, momentum, mass, length, diameter, deflec-tion, taper, indentation, droop, corner, concavitysocial unit troop , team, platoon, office, legion, league, household, family, department, confederacy, com-pany, committee, club, bureau, brigade, branch, agencyatmosphericphenomenonwind, typhoon, tornado, thunderstorm, snowfall, shower, sandstorm, rainstorm, lightning, hur-ricane, fog, drizzle, cyclone, crosswind, cloudburst, cloud, blast, aurora, airstream, glowsocial occasion wedding, rededication, prom, pageantry, inaugural, graduation, funeral, fundraiser, fiesta, fete,feast, enthronement, dance, coronation, commemoration, ceremony, celebration, occasion, raf-fle, beanomonetary unit zloty, yuan, shilling, rupee, rouble, pound, peso, penny, lira, guilder, franc, escudo, drachma,dollar, dirham, dinar, centtree sycamore, sapling, rowan, pine, palm, oak, mangrove, jacaranda, hornbeam, conifer, cinchona,casuarina, acacia, rielchemical element zinc, titanium, silver, potassium, platinum, oxygen, nitrogen, neon, magnesium, lithium, iron,hydrogen, helium, germanium, copper, charcoal, carbon, calcium, cadmium, bismuth, alu-minium, goldillness smallpox, plague, meningitis, malnutrition, leukemia, hepatitis, glaucoma, flu, eczema, dia-betes, cirrhosis, cholera, cancer, asthma, arthritis, anthrax, acne, menopausefeeling wonder, shame, sadness, pleasure, passion, love, joy, happiness, fear, anger, heaviness, cool-ness, torment, tenderness, suffering, stingingvehicle van, truck, ship, rocket, pickup, motorcycle, helicopter, cruiser, car, boat, bicycle, automobile,airplane, aircraft, jagcreator producer, photographer, painter, originator, musician, manufacturer, maker, inventor, farmer,developer, designer, craftsman, constructor, builder, artist, architect, motivatorpain toothache, soreness, sting, soreness, sciatica, neuralgia, migraine, lumbago, headache, earache,burn, bellyache, backache, ache, rheumatism, painanimal zebra, turtle, tiger, sheep, rat, puppy, monkey, lion, kitten, horse, elephant, dog, deer, cow, cat,camel, bull, beargame whist, volleyball, tennis, softball, soccer, rugby, lotto, keno, handball, golf, football, curling,chess, bowling, basketball, baccarat, twisteredible fruit watermelon, strawberry, pineapple, pear, peach, orange, olive, melon, mango, lemon, kiwi,grape, cherry, berry, banana, apple, oyster, walnut, pistachio, mandarin, lime, fig, chestnutFigure 2: Optimal clustering for large evaluation set.contexts used purity(VI) 73.4%(VI), (II) 76.6%(VI), (II), (III) 80.1%Table 5: Clustering the larger evaluation set withan increasing number of context specifications.given in Table 5.
As can be seen the object re-lation is added first again.
This time though theinclusion of adjectival modification brings anotherperformance increase which is even one per centabove the result for the space built from all possi-ble relations.
The addition of any further contextsconsistently degrades performance.
The clusteringsolution thus produced is given in Figure 2.
Fromthe 1 872 698 dimension used in the original spaceonly 341 214 are retained.4 Discussion and ConclusionOur results are counterintuitive at first sight as itcould be expected that a larger number of differ-ent contexts would increase performance.
Insteadwe see the best performance with only a very lim-23ited set of possible contexts.
We suspect that thisbehaviour is due to a large amount of correlationbetween the different kinds of contexts.
The ad-dition of further contexts beyond a certain pointtherefore has no positive effect.
As an indicationfor this it might be noticed that the three contextspecifications that yield the best result for the 402word set comprise relations with the three mainopen word classes.
It is to be expected that theycontribute orthogonal information that covers cen-tral dimensions of meaning.
The slight decreasein performance that can be observed when furthercontexts are added is probably due to chance fluc-tuations and almost certainly not significant; withsignificance being hard to determine for any of theresults.However, it is obviously necessary to cover abasic variety of features.
Patterns which are usedto explicitly track semantic relations on the tex-tual surface seem to be too restrictive.
Informa-tion accessible from co-occurring verbs for exam-ple is completely lost.
In a regular window basedword space such information is retained and itsperformance is competitive with a pattern basedapproach.
This method is obviously too liberal,though, if compared to the dependency spaces.In general we were able to show that seman-tic spaces are obviously able to capture categori-cal knowledge about concepts best when they arebuilt from a syntactically annotated source.
Thisis true even if the context specification used is notthe most parsimonious.
The problem of determin-ing the right set of contexts is therefore rather anoptimisation issue than a question of using depen-dency based spaces or not.
It is a considerable one,though, as computations are much cheaper withvectors of reduced dimensionality, of course.For the categorisation task the inclusion of morecomplex relations reaching over several dependen-cies does not seem to be helpful considering theycan all be dropped without a decrease in perfor-mance.
As Pado and Lapata (2007) reached betterresults in their experiments with a broader set ofcontext specifications we conclude that the selec-tion of the kinds of context to include when con-structing a word space depends largely on the taskat hand.ReferencesA.
Almuhareb and M. Poesio.
2004.
Attribute-based and value-based clustering: An evaluation.In Dekang Lin and Dekai Wu, editors, Proceedingsof EMNLP 2004, pages 158?165, Barcelona, Spain,July.
Association for Computational Linguistics.M.
Poesio and A. Almuhareb.
2005a.
Concept learn-ing and categorization from the web.
In Proceedingsof CogSci2005 - XXVII Annual Conference of theCognitive Science Society, pages 103?108, Stresa,Italy.A.
Almuhareb and M. Poesio.
2005b.
Finding at-tributes in the web using a parser.
In Proceedingsof Corpus Linguistics, Birmingham.Marco Baroni, Stefan Evert, and Alessandro Lenci, ed-itors.
2008.
ESSLLI Workshop on DistributionalLexical Semantics, Hamburg, August.J.
R. Curran and M. Moens.
2002.
Improvements inautomatic thesaurus extraction.
In Proceedings ofthe ACL-02 workshop on Unsupervised lexical ac-quisition, pages 59?66, Morristown, NJ, USA.
As-sociation for Computational Linguistics.T.
Dunning.
1993.
Accurate methods for the statisticsof surprise and coincidence.
Computational Lin-guistics, 19(1):61?74.A.
Ferraresi, E. Zanchetta, M. Baroni, and S. Bernar-dini.
2008.
Introducing and evaluating ukwac, avery large web-derived corpus of english.
In Pro-ceedings of the WAC4 Workshop at LREC 2008.G.
Grefenstette.
1994.
Explorations in Automatic The-saurus Discovery.
Kluwer Academic Publishers,Dordrecht.D.
Hindle.
1990.
Noun classification from predicate-argument structures.
In Meeting of the Associationfor Computational Linguistics, pages 268?275.G.
Karypis.
2003.
Cluto: A clustering toolkit.
tech-nical report 02-017.
Technical report, University ofMinnesota, November.D.
Lin.
1998.
Automatic retrieval and clustering ofsimilar words.
In COLING-ACL, pages 768?774.S.
Pad?
and M. Lapata.
2007.
Dependency-based con-struction of semantic space models.
Comput.
Lin-guist., 33(2):161?199.M.
Poesio and A. Almuhareb.
2005b.
Identifying con-cept attributes using a classifier.
In Proceedings ofthe ACL-SIGLEX Workshop on Deep Lexical Acqui-sition, pages 18?27, Ann Arbor, Michigan, June.
As-sociation for Computational Linguistics.M.
Sahlgren.
2006.
The Word Space Model.
Ph.D.thesis, Department of Linguistics, Stockholm Uni-versity.H.
Sch?tze.
1992.
Dimensions of meaning.
In Super-computing ?92: Proceedings of the 1992 ACM/IEEEconference on Supercomputing, pages 787?796, LosAlamitos, CA, USA.
IEEE Computer Society Press.24
