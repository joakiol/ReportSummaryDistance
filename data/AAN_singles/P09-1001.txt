Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1?9,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPHeterogeneous Transfer Learning for Image Clustering via the Social WebQiang YangHong Kong University of Science and Technology, Clearway Bay, Kowloon, Hong Kongqyang@cs.ust.hkYuqiang Chen Gui-Rong Xue Wenyuan Dai Yong YuShanghai Jiao Tong University, 800 Dongchuan Road, Shanghai 200240, China{yuqiangchen,grxue,dwyak,yyu}@apex.sjtu.edu.cnAbstractIn this paper, we present a new learningscenario, heterogeneous transfer learn-ing, which improves learning performancewhen the data can be in different featurespaces and where no correspondence be-tween data instances in these spaces is pro-vided.
In the past, we have classified Chi-nese text documents using English train-ing data under the heterogeneous trans-fer learning framework.
In this paper,we present image clustering as an exam-ple to illustrate how unsupervised learningcan be improved by transferring knowl-edge from auxiliary heterogeneous dataobtained from the social Web.
Imageclustering is useful for image sense dis-ambiguation in query-based image search,but its quality is often low due to image-data sparsity problem.
We extend PLSAto help transfer the knowledge from socialWeb data, which have mixed feature repre-sentations.
Experiments on image-objectclustering and scene clustering tasks showthat our approach in heterogeneous trans-fer learning based on the auxiliary data isindeed effective and promising.1 IntroductionTraditional machine learning relies on the avail-ability of a large amount of data to train a model,which is then applied to test data in the samefeature space.
However, labeled data are oftenscarce and expensive to obtain.
Various machinelearning strategies have been proposed to addressthis problem, including semi-supervised learning(Zhu, 2007), domain adaptation (Wu and Diet-terich, 2004; Blitzer et al, 2006; Blitzer et al,2007; Arnold et al, 2007; Chan and Ng, 2007;Daume, 2007; Jiang and Zhai, 2007; Reichartand Rappoport, 2007; Andreevskaia and Bergler,2008), multi-task learning (Caruana, 1997; Re-ichart et al, 2008; Arnold et al, 2008), self-taughtlearning (Raina et al, 2007), etc.
A commonalityamong these methods is that they all require thetraining data and test data to be in the same fea-ture space.
In addition, most of them are designedfor supervised learning.
However, in practice, weoften face the problem where the labeled data arescarce in their own feature space, whereas theremay be a large amount of labeled heterogeneousdata in another feature space.
In such situations, itwould be desirable to transfer the knowledge fromheterogeneous data to domains where we have rel-atively little training data available.To learn from heterogeneous data, researchershave previously proposed multi-view learning(Blum and Mitchell, 1998; Nigam and Ghani,2000) in which each instance has multiple views indifferent feature spaces.
Different from previousworks, we focus on the problem of heterogeneoustransfer learning, which is designed for situationwhen the training data are in one feature space(such as text), and the test data are in another (suchas images), and there may be no correspondencebetween instances in these spaces.
The type ofheterogeneous data can be very different, as in thecase of text and image.
To consider how hetero-geneous transfer learning relates to other types oflearning, Figure 1 presents an intuitive illustrationof four learning strategies, including traditionalmachine learning, transfer learning across differ-ent distributions, multi-view learning and hetero-geneous transfer learning.
As we can see, animportant distinguishing feature of heterogeneoustransfer learning, as compared to other types oflearning, is that more constraints on the problemare relaxed, such that data instances do not need tocorrespond anymore.
This allows, for example, acollection of Chinese text documents to be classi-fied using another collection of English text as the1training data (c.f.
(Ling et al, 2008) and Section2.1).In this paper, we will give an illustrative exam-ple of heterogeneous transfer learning to demon-strate how the task of image clustering can ben-efit from learning from the heterogeneous socialWeb data.
A major motivation of our work isWeb-based image search, where users submit tex-tual queries and browse through the returned resultpages.
One problem is that the user queries are of-ten ambiguous.
An ambiguous keyword such as?Apple?
might retrieve images of Apple comput-ers and mobile phones, or images of fruits.
Im-age clustering is an effective method for improv-ing the accessibility of image search result.
Loeffet al (2006) addressed the image clustering prob-lem with a focus on image sense discrimination.In their approach, images associated with textualfeatures are used for clustering, so that the textand images are clustered at the same time.
Specif-ically, spectral clustering is applied to the distancematrix built from a multimodal feature set associ-ated with the images to get a better feature repre-sentation.
This new representation contains bothimage and text information, with which the per-formance of image clustering is shown to be im-proved.
A problem with this approach is that whenimages contained in the Web search results arevery scarce and when the textual data associatedwith the images are very few, clustering on the im-ages and their associated text may not be very ef-fective.Different from these previous works, in this pa-per, we address the image clustering problem asa heterogeneous transfer learning problem.
Weaim to leverage heterogeneous auxiliary data, so-cial annotations, etc.
to enhance image cluster-ing performance.
We observe that the World WideWeb has many annotated images in Web sites suchas Flickr (http://www.flickr.com), whichcan be used as auxiliary information source forour clustering task.
In this work, our objectiveis to cluster a small collection of images that weare interested in, where these images are not suf-ficient for traditional clustering algorithms to per-form well due to data sparsity and the low level ofimage features.
We investigate how to utilize thereadily available socially annotated image data onthe Web to improve image clustering.
Althoughthese auxiliary data may be irrelevant to the im-ages to be clustered and cannot be directly usedto solve the data sparsity problem, we show thatthey can still be used to estimate a good latent fea-ture representation, which can be used to improveimage clustering.2 Related Works2.1 Heterogeneous Transfer LearningBetween LanguagesIn this section, we summarize our previous workon cross-language classification as an example ofheterogeneous transfer learning.
This exampleis related to our image clustering problem be-cause they both rely on data from different featurespaces.As the World Wide Web in China grows rapidly,it has become an increasingly important prob-lem to be able to accurately classify Chinese Webpages.
However, because the labeled Chinese Webpages are still not sufficient, we often find it diffi-cult to achieve high accuracy by applying tradi-tional machine learning algorithms to the ChineseWeb pages directly.
Would it be possible to makethe best use of the relatively abundant labeled En-glish Web pages for classifying the Chinese Webpages?To answer this question, in (Ling et al, 2008),we developed a novel approach for classifying theWeb pages in Chinese using the training docu-ments in English.
In this subsection, we give abrief summary of this work.
The problem to besolved is: we are given a collection of labeledEnglish documents and a large number of unla-beled Chinese documents.
The English and Chi-nese texts are not aligned.
Our objective is to clas-sify the Chinese documents into the same labelspace as the English data.Our key observation is that even though the datause different text features, they may still sharemany of the same semantic information.
What weneed to do is to uncover this latent semantic in-formation by finding out what is common amongthem.
We did this in (Ling et al, 2008) by us-ing the information bottleneck theory (Tishby etal., 1999).
In our work, we first translated theChinese document into English automatically us-ing some available translation software, such asGoogle translate.
Then, we encoded the trainingtext as well as the translated target text together,in terms of the information theory.
We allowed allthe information to be put through a ?bottleneck?and be represented by a limited number of code-2Figure 1: An intuitive illustration of different kinds learning strategies using classification/clustering ofimage apple and banana as the example.words (i.e.
labels in the classification problem).Finally, information bottleneck was used to main-tain most of the common information between thetwo data sources, and discard the remaining irrel-evant information.
In this way, we can approxi-mate the ideal situation where similar training andtranslated test pages shared in the common part areencoded into the same codewords, and are thus as-signed the correct labels.
In (Ling et al, 2008), weexperimentally showed that heterogeneous trans-fer learning can indeed improve the performanceof cross-language text classification as comparedto directly training learning models (e.g., NaiveBayes or SVM) and testing on the translated texts.2.2 Other Works in Transfer LearningIn the past, several other works made use of trans-fer learning for cross-feature-space learning.
Wuand Oard (2008) proposed to handle the cross-language learning problem by translating the datainto a same language and applying kNN on thelatent topic space for classification.
Most learningalgorithms for dealing with cross-language hetero-geneous data require a translator to convert thedata to the same feature space.
For those data thatare in different feature spaces where no transla-tor is available, Davis and Domingos (2008) pro-posed a Markov-logic-based transfer learning al-gorithm, which is called deep transfer, for trans-ferring knowledge between biological domainsand Web domains.
Dai et al (2008a) proposeda novel learning paradigm, known as translatedlearning, to deal with the problem of learning het-erogeneous data that belong to quite different fea-ture spaces by using a risk minimization frame-work.2.3 Relation to PLSAOur work makes use of PLSA.
Probabilistic la-tent semantic analysis (PLSA) is a widely usedprobabilistic model (Hofmann, 1999), and couldbe considered as a probabilistic implementation oflatent semantic analysis (LSA) (Deerwester et al,1990).
An extension to PLSA was proposed in(Cohn and Hofmann, 2000), which incorporatedthe hyperlink connectivity in the PLSA model byusing a joint probabilistic model for connectivityand content.
Moreover, PLSA has shown a lotof applications ranging from text clustering (Hof-mann, 2001) to image analysis (Sivic et al, 2005).2.4 Relation to ClusteringCompared to many previous works on image clus-tering, we note that traditional image cluster-ing is generally based on techniques such as K-means (MacQueen, 1967) and hierarchical clus-tering (Kaufman and Rousseeuw, 1990).
How-ever, when the data are sparse, traditional clus-tering algorithms may have difficulties in obtain-ing high-quality image clusters.
Recently, severalresearchers have investigated how to leverage theauxiliary information to improve target clustering3performance, such as supervised clustering (Fin-ley and Joachims, 2005), semi-supervised cluster-ing (Basu et al, 2004), self-taught clustering (Daiet al, 2008b), etc.3 Image Clustering with AnnotatedAuxiliary DataIn this section, we present our annotation-basedprobabilistic latent semantic analysis algorithm(aPLSA), which extends the traditional PLSAmodel by incorporating annotated auxiliary im-age data.
Intuitively, our algorithm aPLSA per-forms PLSA analysis on the target images, whichare converted to an image instance-to-feature co-occurrence matrix.
At the same time, PLSA isalso applied to the annotated image data from so-cial Web, which is converted into a text-to-image-feature co-occurrence matrix.
In order to unifythose two separate PLSA models, these two stepsare done simultaneously with common latent vari-ables used as a bridge linking them.
Throughthese common latent variables, which are nowconstrained by both target image data and auxil-iary annotation data, a better clustering result isexpected for the target data.3.1 Probabilistic Latent Semantic AnalysisLet F = {fi}|F|i=1 be an image feature space, andV = {vi}|V|i=1 be the image data set.
Each imagevi ?
V is represented by a bag-of-features {f |f ?vi ?
f ?
F}.Based on the image data set V , we can esti-mate an image instance-to-feature co-occurrencematrix A|V|?|F| ?
R|V|?|F|, where each elementAij (1 ?
i ?
|V| and 1 ?
j ?
|F|) in the matrixA is the frequency of the feature fj appearing inthe instance vi.LetW = {wi}|W|i=1 be a text feature space.
Theannotated image data allow us to obtain the co-occurrence information between images v and textfeatures w ?
W .
An example of annotated im-age data is the Flickr (http://www.flickr.com), which is a social Web site containing a largenumber of annotated images.By extracting image features from the annotatedimages v, we can estimate a text-to-image fea-ture co-occurrence matrix B|W|?|F| ?
R|W|?|F|,where each element Bij (1 ?
i ?
|W| and1 ?
j ?
|F|) in the matrix B is the frequencyof the text feature wi and the image feature fj oc-curring together in the annotated image data set.V Z FP (z|v) P (f |z)Figure 2: Graphical model representation of PLSAmodel.LetZ = {zi}|Z|i=1 be the latent variable set in ouraPLSA model.
In clustering, each latent variablezi ?
Z corresponds to a certain cluster.Our objective is to estimate a clustering func-tion g : V 7?
Z with the help of the two co-occurrence matrices A and B as defined above.To formally introduce the aPLSA model, westart from the probabilistic latent semantic anal-ysis (PLSA) (Hofmann, 1999) model.
PLSA isa probabilistic implementation of latent seman-tic analysis (LSA) (Deerwester et al, 1990).
Inour image clustering task, PLSA decomposes theinstance-feature co-occurrence matrix A under theassumption of conditional independence of imageinstances V and image features F , given the latentvariables Z .P (f |v) =?z?ZP (f |z)P (z|v).
(1)The graphical model representation of PLSA isshown in Figure 2.Based on the PLSA model, the log-likelihood canbe defined as:L =?i?jAij?j?
Aij?logP (fj |vi) (2)where A|V|?|F| ?
R|V|?|F| is the image instance-feature co-occurrence matrix.
The term AijPj?
Aij?in Equation (2) is a normalization term ensuringeach image is giving the same weight in the log-likelihood.Using EM algorithm (Dempster et al, 1977),which locally maximizes the log-likelihood ofthe PLSA model (Equation (2)), the probabilitiesP (f |z) and P (z|v) can be estimated.
Then, theclustering function is derived asg(v) = argmaxz?ZP (z|v).
(3)Due to space limitation, we omit the details for thePLSA model, which can be found in (Hofmann,1999).3.2 aPLSA: Annotation-based PLSAIn this section, we consider how to incorporatea large number of socially annotated images in a4VWZ FP (z|v)P (z|w)P (f |z)Figure 3: Graphical model representation ofaPLSA model.unified PLSA model for the purpose of utilizingthe correlation between text features and imagefeatures.
In the auxiliary data, each image has cer-tain textual tags that are attached by users.
Thecorrelation between text features and image fea-tures can be formulated as follows.P (f |w) =?z?ZP (f |z)P (z|w).
(4)It is clear that Equations (1) and (4) share a sameterm P (f |z).
So we design a new PLSA model byjoining the probabilistic model in Equation (1) andthe probabilistic model in Equation (4) into a uni-fied model, as shown in Figure 3.
In Figure 3, thelatent variables Z depend not only on the corre-lation between image instances V and image fea-tures F , but also the correlation between text fea-turesW and image featuresF .
Therefore, the aux-iliary socially-annotated image data can be usedto help the target image clustering performance byestimating good set of latent variables Z .Based on the graphical model representation inFigure 3, we derive the log-likelihood objectivefunction, in a similar way as in (Cohn and Hof-mann, 2000), as followsL =?j[??iAij?j?
Aij?logP (fj |vi)+(1?
?)?lBlj?j?
Blj?logP (fj |wl)],(5)where A|V|?|F| ?
R|V|?|F| is the image instance-feature co-occurrence matrix, and B|W|?|F| ?R|W|?|F| is the text-to-image feature-level co-occurrence matrix.
Similar to Equation (2),AijPj?
Aij?and BljPj?
Blj?in Equation (5) are the nor-malization terms to prevent imbalanced cases.Furthermore, ?
acts as a trade-off parameter be-tween the co-occurrence matrices A and B. Inthe extreme case when ?
= 1, the log-likelihoodobjective function ignores all the biases from thetext-to-image occurrence matrix B.
In this case,the aPLSA model degenerates to the traditionalPLSA model.
Therefore, aPLSA is an extensionto the PLSA model.Now, the objective is to maximize the log-likelihood L of the aPLSA model in Equation (5).Then we apply the EM algorithm (Dempster etal., 1977) to estimate the conditional probabilitiesP (f |z), P (z|w) and P (z|v) with respect to eachdependence in Figure 3 as follows.?
E-Step: calculate the posterior probability ofeach latent variable z given the observationof image features f , image instances v andtext features w based on the old estimate ofP (f |z), P (z|w) and P (z|v):P (zk|vi, fj) =P (fj |zk)P (zk|vi)?k?
P (fj |zk?
)P (zk?
|vi)(6)P (zk|wl, fj) =P (fj |zk)P (zk|wl)?k?
P (fj |zk?
)P (zk?
|wl)(7)?
M-Step: re-estimates conditional probabili-ties P (zk|vi) and P (zk|wl):P (zk|vi) =?jAij?j?
Aij?P (zk|vi, fj) (8)P (zk|wl) =?jBlj?j?
Blj?P (zk|wl, fj) (9)and conditional probability P (fj |zk), whichis a mixture portion of posterior probabilityof latent variablesP (fj |zk) ?
??iAij?j?
Aij?P (zk|vi, fj)+ (1?
?)?lBlj?j?
Blj?P (zk|wl, fj)(10)Finally, the clustering function for a certain im-age v isg(v) = argmaxz?ZP (z|v).
(11)From the above equations, we can deriveour annotation-based probabilistic latent semanticanalysis (aPLSA) algorithm.
As shown in Algo-rithm 1, aPLSA iteratively performs the E-Stepand the M-Step in order to seek local optimalpoints based on the objective function L in Equa-tion (5).5Algorithm 1 Annotation-based PLSA Algorithm(aPLSA)Input: The V-F co-occurrence matrix A andW-F co-occurrence matrix B.Output: A clustering (partition) function g : V 7?Z , which maps an image instance v ?
V to a latentvariable z ?
Z .1: Initial Z so that |Z| equals the number clus-ters desired.2: Initialize P (z|v), P (z|w), P (f |z) randomly.3: while the change of L in Eq.
(5) between twosequential iterations is greater than a prede-fined threshold do4: E-Step: Update P (z|v, f) and P (z|w, f)based on Eq.
(6) and (7) respectively.5: M-Step: Update P (z|v), P (z|w) andP (f |z) based on Eq.
(8), (9) and (10) re-spectively.6: end while7: for all v in V do8: g(v)?
argmaxzP (z|v).9: end for10: Return g.4 ExperimentsIn this section, we empirically evaluate the aPLSAalgorithm together with some state-of-art base-line methods on two widely used image corpora,to demonstrate the effectiveness of our algorithmaPLSA.4.1 Data SetsIn order to evaluate the effectiveness of our algo-rithm aPLSA, we conducted experiments on sev-eral data sets generated from two image corpora,Caltech-256 (Griffin et al, 2007) and the fifteen-scene (Lazebnik et al, 2006).
The Caltech-256data set has 256 image objective categories, rang-ing from animals to buildings, from plants to au-tomobiles, etc.
The fifteen-scene data set con-tains 15 scenes such as store and forest.From these two corpora, we randomly generatedeleven image clustering tasks, including seven 2-way clustering tasks, two 4-way clustering task,one 5-way clustering task and one 8-way cluster-ing task.
The detailed descriptions for these clus-tering tasks are given in Table 1.
In these tasks,bi7 and oct1 were generated from fifteen-scenedata set, and the rest were from Caltech-256 dataset.DATA SET INVOLVED CLASSES DATA SIZEbi1 skateboard, airplanes 102, 800bi2 billiards, mars 278, 155bi3 cd, greyhound 102, 94bi4 electric-guitar, snake 122, 112bi5 calculator, dolphin 100, 106bi6 mushroom, teddy-bear 202, 99bi7 MIThighway, livingroom 260, 289quad1 calculator, diamond-ring, dolphin,microscope 100, 118, 106, 116quad2 bonsai, comet, frog, saddle 122, 120, 115, 110quint1 frog, kayak, bear, jesus-christ, watch 115, 102, 101, 87,201oct1MIThighway, MITmountain,kitchen, MITcoast, PARoffice, MIT-tallbuilding, livingroom, bedroom260, 374, 210, 360,215, 356, 289, 216tune1 coin, horse 123, 270tune2 socks, spider 111, 106tune3 galaxy, snowmobile 80, 112tune4 dice, fern 98, 110tune5 backpack, lightning, mandolin, swan 151, 136, 93, 114Table 1: The descriptions of all the image clus-tering tasks used in our experiment.
Amongthese data sets, bi7 and oct1 were generatedfrom fifteen-scene data set, and the rest were fromCaltech-256 data set.To empirically investigate the parameter ?
andthe convergence of our algorithm aPLSA, we gen-erated five more date sets as the development sets.The detailed description of these five developmentsets, namely tune1 to tune5 is listed in Table 1as well.The auxiliary data were crawled from the Flickr(http://www.flickr.com/) web site dur-ing August 2007.
Flickr is an internet communitywhere people share photos online and express theiropinions as social tags (annotations) attached toeach image.
From Flicker, we collected 19, 959images and 91, 719 related annotations, amongwhich 2, 600 words are distinct.
Based on themethod described in Section 3, we estimated theco-occurrence matrix B between text features andimage features.
This co-occurrence matrix B wasused by all the clustering tasks in our experiments.For data preprocessing, we adopted the bag-of-features representation of images (Li and Perona,2005) in our experiments.
Interesting points werefound in the images and described via the SIFTdescriptors (Lowe, 2004).
Then, the interestingpoints were clustered to generate a codebook toform an image feature space.
The size of code-book was set to 2, 000 in our experiments.
Basedon the codebook, which serves as the image fea-ture space, each image can be represented as a cor-responding feature vector to be used in the nextstep.To set our evaluation criterion, we used the6Data Set KMeans PLSA STC aPLSAseparate combined separate combinedbi1 0.645?0.064 0.548?0.031 0.544?0.074 0.537?0.033 0.586?0.139 0.482?0.062bi2 0.687?0.003 0.662?0.014 0.464?0.074 0.692?0.001 0.577?0.016 0.455?0.096bi3 1.294?0.060 1.300?0.015 1.085?0.073 1.126?0.036 1.103?0.108 1.029?0.074bi4 1.227?0.080 1.164?0.053 0.976?0.051 1.038?0.068 1.024?0.089 0.919?0.065bi5 1.450?0.058 1.417?0.045 1.426?0.025 1.405?0.040 1.411?0.043 1.377?0.040bi6 1.969?0.078 1.852?0.051 1.514?0.039 1.709?0.028 1.589?0.121 1.503?0.030bi7 0.686?0.006 0.683?0.004 0.643?0.058 0.632?0.037 0.651?0.012 0.624?0.066quad1 0.591?0.094 0.675?0.017 0.488?0.071 0.662?0.013 0.580?0.115 0.432?0.085quad2 0.648?0.036 0.646?0.045 0.614?0.062 0.626?0.026 0.591?0.087 0.515?0.098quint1 0.557?0.021 0.508?0.104 0.547?0.060 0.539?0.051 0.538?0.100 0.502?0.067oct1 0.659?0.031 0.680?0.012 0.340?0.147 0.691?0.002 0.411?0.089 0.306?0.101average 0.947?0.029 0.922?0.017 0.786?0.009 0.878?0.006 0.824?0.036 0.741?0.018Table 2: Experimental result in term of entropy for all data sets and evaluation methods.entropy to measure the quality of our clusteringresults.
In information theory, entropy (Shan-non, 1948) is a measure of the uncertainty as-sociated with a random variable.
In our prob-lem, entropy serves as a measure of randomnessof clustering result.
The entropy of g on a sin-gle latent variable z is defined to be H(g, z) ,?
?c?C P (c|z) log2 P (c|z), where C is the classlabel set of V and P (c|z) = |{v|g(v)=z?t(v)=c}||{v|g(v)=z}| ,in which t(v) is the true class label of image v.Lower entropy H(g,Z) indicates less randomnessand thus better clustering result.4.2 Empirical AnalysisWe now empirically analyze the effectiveness ofour aPLSA algorithm.
Because, to our best ofknowledge, few existing methods addressed theproblem of image clustering with the help of so-cial annotation image data, we can only compareour aPLSA with several state-of-the-art cluster-ing algorithms that are not directly designed forour problem.
The first baseline is the well-knownKMeans algorithm (MacQueen, 1967).
Since ouralgorithm is designed based on PLSA (Hofmann,1999), we also included PLSA for clustering as abaseline method in our experiments.For each of the above two baselines, we havetwo strategies: (1) separated: the baselinemethod was applied on the target image data only;(2) combined: the baseline method was appliedto cluster the combined data consisting of bothtarget image data and the annotated image data.Clustering results on target image data were usedfor evaluation.
Note that, in the combined data, allthe annotations were thrown away since baselinemethods evaluated in this paper do not leverageannotation information.In addition, we compared our algorithm aPLSAto a state-of-the-art transfer clustering strategy,known as self-taught clustering (STC) (Dai et al,2008b).
STC makes use of auxiliary data to esti-mate a better feature representation to benefit thetarget clustering.
In these experiments, the anno-tated image data were used as auxiliary data inSTC, which does not use the annotation text.In our experiments, the performance is in theform of the average entropy and variance of fiverepeats by randomly selecting 50 images fromeach of the categories.
We selected only 50 im-ages per category, since this paper is focused onclustering sparse data.
Table 2 shows the perfor-mance with respect to all comparison methods oneach of the image clustering tasks measured bythe entropy criterion.
From the tables, we can seethat our algorithm aPLSA outperforms the base-line methods in all the data sets.
We believe that isbecause aPLSA can effectively utilize the knowl-edge from the socially annotated image data.
Onaverage, aPLSA gives rise to 21.8% of entropy re-duction and as compared to KMeans, 5.7% of en-tropy reduction as compared to PLSA, and 10.1%of entropy reduction as compared to STC.4.2.1 Varying Data SizeWe now show how the data size affects aPLSA,with two baseline methods KMeans and PLSA asreference.
The experiments were conducted ondifferent amounts of target image data, varyingfrom 10 to 80.
The corresponding experimentalresults in average entropy over all the 11 clusteringtasks are shown in Figure 4(a).
From this figure,we observe that aPLSA always yields a significantreduction in entropy as compared with two base-line methods KMeans and PLSA, regardless of thesize of target image data that we used.710 20 30 40 50 60 70 800.70.750.80.850.90.951Data size per categoryEntropyKMeansPLSAaPLSA(a)0 0.2 0.4 0.6 0.8 10.40.450.50.550.60.650.70.75?Entropyaverage over 5 development sets(b)0 50 100 150 200 250 3000.50.550.60.650.70.75Number of IterationEntropyaverage over 5 development sets(c)Figure 4: (a) The entropy curve as a function of different amounts of data per category.
(b) The entropycurve as a function of different number of iterations.
(c) The entropy curve as a function of differenttrade-off parameter ?.4.2.2 Parameter SensitivityIn aPLSA, there is a trade-off parameter ?
that af-fects how the algorithm relies on auxiliary data.When ?
= 0, the aPLSA relies only on annotatedimage data B.
When ?
= 1, aPLSA relies onlyon target image data A, in which case aPLSA de-generates to PLSA.
Smaller ?
indicates heavier re-liance on the annotated image data.
We have donesome experiments on the development sets to in-vestigate how different ?
affect the performanceof aPLSA.
We set the number of images per cate-gory to 50, and tested the performance of aPLSA.The result in average entropy over all developmentsets is shown in Figure 4(b).
In the experimentsdescribed in this paper, we set ?
to 0.2, which isthe best point in Figure 4(b).4.2.3 ConvergenceIn our experiments, we tested the convergenceproperty of our algorithm aPLSA as well.
Fig-ure 4(c) shows the average entropy curve givenby aPLSA over all development sets.
From thisfigure, we see that the entropy decreases very fastduring the first 100 iterations and becomes stableafter 150 iterations.
We believe that 200 iterationsis sufficient for aPLSA to converge.5 ConclusionsIn this paper, we proposed a new learning scenariocalled heterogeneous transfer learning and illus-trated its application to image clustering.
Imageclustering, a vital component in organizing searchresults for query-based image search, was shownto be improved by transferring knowledge fromunrelated images with annotations in a social Web.This is done by first learning the high-quality la-tent variables in the auxiliary data, and then trans-ferring this knowledge to help improve the cluster-ing of the target image data.
We conducted experi-ments on two image data sets, using the Flickr dataas the annotated auxiliary image data, and showedthat our aPLSA algorithm can greatly outperformseveral state-of-the-art clustering algorithms.In natural language processing, there are manyfuture opportunities to apply heterogeneous trans-fer learning.
In (Ling et al, 2008) we have shownhow to classify the Chinese text using English textas the training data.
We may also consider cluster-ing, topic modeling, question answering, etc., tobe done using data in different feature spaces.
Wecan consider data in different modalities, such asvideo, image and audio, as the training data.
Fi-nally, we will explore the theoretical foundationsand limitations of heterogeneous transfer learningas well.Acknowledgement Qiang Yang thanks HongKong CERG grant 621307 for supporting the re-search.ReferencesAlina Andreevskaia and Sabine Bergler.
2008.
When spe-cialists and generalists work together: Overcoming do-main dependence in sentiment tagging.
In ACL-08: HLT,pages 290?298, Columbus, Ohio, June.Andrew Arnold, Ramesh Nallapati, and William W. Cohen.2007.
A comparative study of methods for transductivetransfer learning.
In ICDM 2007 Workshop on Miningand Management of Biological Data, pages 77-82.Andrew Arnold, Ramesh Nallapati, and William W. Cohen.2008.
Exploiting feature hierarchy for transfer learning innamed entity recognition.
In ACL-08: HLT.Sugato Basu, Mikhail Bilenko, and Raymond J. Mooney.2004.
A probabilistic framework for semi-supervisedclustering.
In ACM SIGKDD 2004, pages 59?68.John Blitzer, Ryan Mcdonald, and Fernando Pereira.
2006.Domain adaptation with structural correspondence learn-ing.
In EMNLP 2006, pages 120?128, Sydney, Australia.8John Blitzer, Mark Dredze, and Fernando Pereira.
2007.Biographies, bollywood, boom-boxes and blenders: Do-main adaptation for sentiment classification.
In ACL 2007,pages 440?447, Prague, Czech Republic.Avrim Blum and Tom Mitchell.
1998.
Combining labeledand unlabeled data with co-training.
In COLT 1998, pages92?100, New York, NY, USA.
ACM.Rich Caruana.
1997.
Multitask learning.
Machine Learning,28(1):41?75.Yee Seng Chan and Hwee Tou Ng.
2007.
Domain adaptationwith active learning for word sense disambiguation.
InACL 2007, Prague, Czech Republic.David A. Cohn and Thomas Hofmann.
2000.
The missinglink - a probabilistic model of document content and hy-pertext connectivity.
In NIPS 2000, pages 430?436.Wenyuan Dai, Yuqiang Chen, Gui-Rong Xue, Qiang Yang,and Yong Yu.
2008a.
Translated learning: Transfer learn-ing across different feature spaces.
In NIPS 2008, pages353?360.Wenyuan Dai, Qiang Yang, Gui-Rong Xue, and Yong Yu.2008b.
Self-taught clustering.
In ICML 2008, pages 200?207.
Omnipress.Hal Daume, III.
2007.
Frustratingly easy domain adaptation.In ACL 2007, pages 256?263, Prague, Czech Republic.Jesse Davis and Pedro Domingos.
2008.
Deep transfer viasecond-order markov logic.
In AAAI 2008 Workshop onTransfer Learning, Chicago, USA.Scott Deerwester, Susan T. Dumais, George W. Furnas,Thomas K. L, and Richard Harshman.
1990.
Indexing bylatent semantic analysis.
Journal of the American Societyfor Information Science, pages 391?407.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.
Max-imum likelihood from incomplete data via the em algo-rithm.
J. of the Royal Statistical Society, 39:1?38.Thomas Finley and Thorsten Joachims.
2005.
Supervisedclustering with support vector machines.
In ICML 2005,pages 217?224, New York, NY, USA.
ACM.G.
Griffin, A. Holub, and P. Perona.
2007.
Caltech-256 ob-ject category dataset.
Technical Report 7694, CaliforniaInstitute of Technology.Thomas Hofmann.
1999 Probabilistic latent semantic anal-ysis.
In Proc.
of Uncertainty in Artificial Intelligence,UAI99.
Pages 289?296Thomas Hofmann.
2001.
Unsupervised learning by proba-bilistic latent semantic analysis.
Machine Learning.
vol-ume 42, number 1-2, pages 177?196.
Kluwer AcademicPublishers.Jing Jiang and Chengxiang Zhai.
2007.
Instance weightingfor domain adaptation in NLP.
In ACL 2007, pages 264?271, Prague, Czech Republic, June.Leonard Kaufman and Peter J. Rousseeuw.
1990.
Findinggroups in data: an introduction to cluster analysis.
JohnWiley and Sons, New York.Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce.
2006.Beyond bags of features: Spatial pyramid matching forrecognizing natural scene categories.
In CVPR 2006,pages 2169?2178, Washington, DC, USA.Fei-Fei Li and Pietro Perona.
2005.
A bayesian hierarchi-cal model for learning natural scene categories.
In CVPR2005, pages 524?531, Washington, DC, USA.Xiao Ling, Gui-Rong Xue, Wenyuan Dai, Yun Jiang, QiangYang, and Yong Yu.
2008.
Can chinese web pages beclassified with english data source?
In WWW 2008, pages969?978, New York, NY, USA.
ACM.Nicolas Loeff, Cecilia Ovesdotter Alm, and David A.Forsyth.
2006.
Discriminating image senses by clusteringwith multimodal features.
In COLING/ACL 2006 Mainconference poster sessions, pages 547?554.David G. Lowe.
2004.
Distinctive image features from scale-invariant keypoints.
International Journal of ComputerVision (IJCV) 2004, volume 60, number 2, pages 91?110.J.
B. MacQueen.
1967.
Some methods for classification andanalysis of multivariate observations.
In Proceedings ofFifth Berkeley Symposium on Mathematical Statistics andProbability, pages 1:281?297, Berkeley, CA, USA.Kamal Nigam and Rayid Ghani.
2000.
Analyzing the effec-tiveness and applicability of co-training.
In Proceedingsof the Ninth International Conference on Information andKnowledge Management, pages 86?93, New York, USA.Rajat Raina, Alexis Battle, Honglak Lee, Benjamin Packer,and Andrew Y. Ng.
2007.
Self-taught learning: transferlearning from unlabeled data.
In ICML 2007, pages 759?766, New York, NY, USA.
ACM.Roi Reichart and Ari Rappoport.
2007.
Self-training forenhancement and domain adaptation of statistical parserstrained on small datasets.
In ACL 2007.Roi Reichart, Katrin Tomanek, Udo Hahn, and Ari Rap-poport.
2008.
Multi-task active learning for linguisticannotations.
In ACL-08: HLT, pages 861?869.C.
E. Shannon.
1948.
A mathematical theory of communi-cation.
Bell system technical journal, 27.J.
Sivic, B. C. Russell, A.
A. Efros, A. Zisserman, and W. T.Freeman.
2005.
Discovering object categories in imagecollections.
In ICCV 2005.Naftali Tishby, Fernando C. Pereira, and William Bialek.
Theinformation bottleneck method.
1999.
In Proc.
of the 37-th Annual Allerton Conference on Communication, Con-trol and Computing, pages 368?377.Pengcheng Wu and Thomas G. Dietterich.
2004.
Improvingsvm accuracy by training on auxiliary data sources.
InICML 2004, pages 110?117, New York, NY, USA.Yejun Wu and Douglas W. Oard.
2008.
Bilingual topic as-pect classification with a few training examples.
In ACMSIGIR 2008, pages 203?210, New York, NY, USA.Xiaojin Zhu.
2007.
Semi-supervised learning literature sur-vey.
Technical Report 1530, Computer Sciences, Univer-sity of Wisconsin-Madison.9
