A New Semantics:Merging Propositional and Distributional InformationEduard HovyInformation Sciences InstituteUniversity of Southern Californiahovy@isi.eduDespite hundreds of years of study on semantics, theories and representations of semanticcontent?the actual meaning of the symbols used in semantic propositions?remainimpoverished.
The traditional extensional and intensional models of semantics are difficultto actually flesh out in practice, and no large-scale models of this kind exist.
Recently,researchers in Natural Language Processing (NLP) have increasingly treated topic signatureword distributions (also called ?context vectors?, ?topic models?, ?language models?, etc.)
as ade facto placeholder for semantics at various levels of granularity.
This talk argues for a newkind of semantics that combines traditional symbolic logic-based proposition-style semantics(of the kind used in older NLP) with (computation-based) statistical word distributioninformation (what is being called Distributional Semantics in modern NLP).
The coreresource is a single lexico-semantic ?lexicon?
that can be used for a variety of tasks.
I showhow to define such a lexicon, how to build and format it, and how to use it for various tasks.Combining the two views of semantics opens many fascinating questions that beg study,including the operation of logical operators such as negation and modalities over word(sense)distributions, the nature of ontological facets required to define concepts, and the action ofcompositionality over statistical concepts.14
