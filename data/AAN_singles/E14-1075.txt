Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 712?721,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsImproving the Estimation of Word Importance for News Multi-DocumentSummarizationKai HongUniversity of PennsylvaniaPhiladelphia, PA, 19104hongkai1@seas.upenn.eduAni NenkovaUniversity of PennsylvaniaPhiladelphia, PA, 19104nenkova@seas.upenn.eduAbstractWe introduce a supervised modelfor predicting word importance thatincorporates a rich set of features.
Ourmodel is superior to prior approachesfor identifying words used in humansummaries.
Moreover we showthat an extractive summarizer usingthese estimates of word importance iscomparable in automatic evaluation withthe state-of-the-art.1 IntroductionIn automatic extractive summarization, sentenceimportance is calculated by taking into account,among possibly other features, the importanceof words that appear in the sentence.
In thispaper, we describe experiments on identifyingwords from the input that are also included inhuman summaries; we call such words summarykeywords.
We review several unsupervisedapproaches for summary keyword identificationand further combine these, along with featuresincluding position, part-of-speech, subjectivity,topic categories, context and intrinsic importance,in a superior supervised model for predicting wordimportance.One of the novel features we develop aimsto determine the intrinsic importance of words.To this end, we analyze abstract-article pairs inthe New York Times corpus (Sandhaus, 2008)to identify words that tend to be preserved inthe abstracts.
We demonstrate that judging wordimportance just based on this criterion leads tosignificantly higher performance than selectingsentences at random.
Identifying intrinsicallyimportant words allows us to generate summarieswithout doing any feature computation on theinput, equivalent in quality to the standard baselineof extracting the first 100 words from the latestarticle in the input.
Finally, we integrate theschemes for assignment of word importance intoa summarizer which greedily optimizes for thepresence of important words.
We show that ourbetter estimation of word importance leads tobetter extractive summaries.2 Prior workThe idea of identifying words that are descriptiveof the input can be dated back to Luhn?s earliestwork in automatic summarization (Luhn, 1958).There keywords were identified based on thenumber of times they appeared in the input,and words that appeared most and least oftenwere excluded.
Then the sentences in whichkeywords appeared near each other, presumablybetter conveying the relationship between thekeywords, were selected to form a summary.Many successful recent systems also estimateword importance.
The simplest but competitiveway to do this task is to estimate the wordprobability from the input (Nenkova andVanderwende, 2005).
Another powerful methodis log-likelihood ratio test (Lin and Hovy, 2000),which identifies the set of words that appear inthe input more often than in a background corpus(Conroy et al., 2006; Harabagiu and Lacatusu,2005).In contrast to selecting a set of keywords,weights are assigned to all words in the inputin the majority of summarization methods.Approaches based on (approximately) optimizingthe coverage of these words have become widelypopular.
Earliest such work relied on TF*IDFweights (Filatova and Hatzivassiloglou, 2004),later approaches included heuristics to identifysummary-worthy bigrams (Riedhammer et al.,2010).
Most optimization approaches, however,use TF*IDF or word probability in the input asword weights (McDonald, 2007; Shen and Li,2010; Berg-Kirkpatrick et al., 2011).712Word weights have also been estimated bysupervised approaches, with word probability andlocation of occurrence as typical features (Yih etal., 2007; Takamura and Okumura, 2009; Sipos etal., 2012).A handful of investigations have productivelyexplored the mutually reinforcing relationshipbetween word and sentence importance, iterativelyre-estimating each in either supervised orunsupervised framework (Zha, 2002; Wan etal., 2007; Wei et al., 2008; Liu et al., 2011).Most existing work directly focuses on predictingsentence importance, with emphasis on theformalization of the problem (Kupiec et al., 1995;Celikyilmaz and Hakkani-Tur, 2010; Litvak et al.,2010).
There has been little work directly focusedon predicting keywords from the input that willappear in human summaries.
Also there has beenonly a few investigations of suitable featuresfor estimating word importance and identifyingkeywords in summaries; we address this issue byexploring a range of possible indicators of wordimportance in our model.3 Data and Planned ExperimentsWe carry out our experiments on two datasets fromthe Document Understanding Conference (DUC)(Over et al., 2007).
DUC 2003 is used for trainingand development, DUC 2004 is used for testing.These are the last two years in which genericsummarization was evaluated at DUC workshops.There are 30 multi-document clusters in DUC2003 and 50 in DUC 2004, each with about 10news articles on a related topic.
The task isto produce a 100-word generic summary.
Fourhuman abstractive summaries are available foreach cluster.We compare different keyword extractionmethods by the F-measure1they achieve againstthe gold-standard summary keywords.
We do notuse stemming when calculating these scores.In our work, keywords for an input are definedas those words that appear in at least i of thehuman abstracts, yielding four gold-standard setsof keywords, denoted by Gi.
|Gi| is thus thecardinality of the set for the input.
We onlyconsider the words in the summary that alsoappear in the original input2, with stopwords12*precision*recall/(precision+recall)2On average 26.3% (15.0% with stemming) of the wordsin the four abstracts never appear in the input.excluded3.
Table 1 shows the average number ofunique content words for the respective keywordgold-standard.i 1 2 3 4Mean |Gi| 102 32 15 6Table 1: Average number of words in GiFor the summarization task, we compare resultsusing ROUGE (Lin, 2004).
We report ROUGE-1,-2, -4 recall, with stemming and without removingstopwords.
We consider ROUGE-2 recall asthe main metric for this comparison due to itseffectiveness in comparing machine summaries(Owczarzak et al., 2012).
All of the summarieswere truncated to the first 100 words by ROUGE4.We use Wilcoxon signed-rank test to examinethe statistical significance as advocated by Rankelet al.
(2011) for both tasks, and considerdifferences to be significant if the p-value is lessthan 0.05.4 Unsupervised Word WeightingIn this section we describe three unsupervisedapproaches of assigning importance weights towords.
The first two are probability andlog-likelihood ratio, which have been extensivelyused in prior work.
We also apply a markovrandom walk model for keyword ranking, similarto Mihalcea and Tarau (2004).
In the nextsection we describe a summarizer that uses theseweights to form a summary and then describeour regression approach to combine these andother predictors in order to achieve more accuratepredictions for the word importance in Section 7.The task is to assign a score to each word in theinput.
The keywords extracted are thus the contentwords with highest scores.4.1 Word Probability (Prob)The frequency with which a word occurs in theinput is often considered as an indicator of itsimportance.
The weight for a word is computedas p(w) =c(w)N, where c(w) is the number oftimes word w appears in the input and N is thetotal number of word tokens in the input.3We use the stopword list from the SMART system(Salton, 1971), augmented with punctuation and symbols.4ROUGE version 1.5.5 with parameters: -c 95 -r 1000 -n4 -m -a -l 100 -x7134.2 Log-likelihood Ratio (LLR)The log-likelihood ratio test (Lin and Hovy, 2000)compares the distribution of a word in the inputwith that in a large background corpus to identifytopic words.
We use the Gigaword corpus (Graff etal., 2007) for background counts.
The test statistichas a ?2distribution, so a desired confidence levelcan be chosen to find a small set of topic words.4.3 Markov Random Walk Model (MRW)Graph methods have been successfully applied toweighting sentences for generic (Wan and Yang,2008; Mihalcea and Tarau, 2004; Erkan andRadev, 2004) and query-focused summarization(Otterbacher et al., 2009).Here instead of constructing a graph withsentences as nodes and edges weighted bysentence similarity, we treat the words as vertices,similar to Mihalcea and Tarau (2004).
Thedifference in our approach is that the edgesbetween the words are defined by syntacticdependencies rather than depending on theco-occurrence of words within a window of k. Weuse the Stanford dependency parser (Marneffe etal., 2006).
In our approach, we consider a wordw more likely to be included in a human summarywhen it is syntactically related to other (important)words, even if w itself is not mentioned often.The edge weight between two vertices is equal tothe number of syntactic dependencies of any typebetween two words within the same sentence inthe input.
The weights are then normalized bysumming up the weights of edges linked to onenode.We apply the Pagerank algorithm (Lawrenceet al., 1998) on the resulting graph.
We set theprobability of performing random jump betweennodes ?=0.15.
The algorithm terminates whenthe change of node weight between iterations issmaller than 10?4for all nodes.
Word importanceis equal to the final weight of its correspondingnode in the graph.5 Summary Generation ProcessIn this section, we outline how summariesare generated by a greedy optimization systemwhich selects the sentence with highest weightiteratively.
This is the main process we use in allour summarization systems.
For comparison wealso use a summarization algorithm based on KLdivergence.5.1 Greedy Optimization ApproachOur algorithm extracts sentences by weightingthem based on word importance.
The approach issimilar to the standard word probability baseline(Nenkova et al., 2006) but we explore a rangeof possibilities for assigning weights to individualwords.
For each sentence, we calculate thesentence weight by summing up the weights ofall words, normalized by the number of words inthe sentence.
We sort the sentences in descendingorder of their scores into a queue.
To create asummary, we iteratively dequeue one sentence,check if the sentence is more than 8 words (asin Erkan and Radev (2004)), then append it tothe current summary if it is non-redundant.
Asentence is considered non-redundant if it is notsimilar to any sentences already in the summary,measured by cosine similarity on binary vectorrepresentations with stopwords excluded.
We usethe cut-off of 0.5 for cosine similarity.
This valuewas tuned on the DUC 2003 dataset, by testing theimpact of the cut-off value on the ROUGE scoresfor the final summary.
Possible values rangedfrom 0.1 to 0.9 with step of 0.1.5.2 KL Divergence SummarizerThe KLSUM summarizer (Haghighi andVanderwende, 2009) aims at minimizing the KLdivergence between the probability distributionover words estimated from the summary andthe input respectively.
This summarizer is acomponent of the popular topic model approaches(Daum?e and Marcu, 2006; Celikyilmaz andHakkani-T?ur, 2011; Mason and Charniak, 2011)and achieves competitive performance withminimal differences compared to a full-blowntopic model system.6 Global Indicators from NYTSome words evoke topics that are of intrinsicinterest to people.
Here we search for globalindicators of word importance regardless ofparticular input.6.1 Global Indicators of Word ImportanceWe analyze a large corpus of original documentsand corresponding summaries in order to identifywords that consistently get included in or excludedfrom the summary.
In the 2004-2007 NYT corpus,many news articles have abstracts along with theoriginal article, which makes it an appropriate714Metric Top-30 wordsKL(A ?
G)(w) photo(s), pres, article, column, reviews, letter, York, Sen, NY, discusses, drawing, op-ed, holds, Bushcorrection, editorial, dept, city, NJ, map, corp, graph, contends, Iraq, John, dies, sec, state, commentsKL(G ?
A)(w) Mr, Ms, p.m., lot, Tuesday, CA, Wednesday, Friday, told, Monday, time, a.m., added, thing, Sundaythings, asked, good, night, Saturday, nyt, back, senator, wanted, kind, Jr., Mrs, bit, looked, wrotePrA(w) photo, photos, article, York, column, letter, Bush, state, reviews, million, Americanpres, percent, Iraq, year, people, government, John, years, company, correctionnational, federal, officials, city, drawing, billion, public, world, administrationTable 2: Top 30 words by three metrics from NYT corpusresource to do such analysis.
We identified160, 001 abstract-original pairs in the corpus.From these, we generate two language models,one estimated from the text of all abstracts (LMA),the other estimated from the corpus of originalarticles (LMG).
We use SRILM (Stolcke, 2002)with Ney smoothing.We denote the probability of wordw in LMAasPrA(w), the probability in LMGas PrG(w), andcalculate the difference PrA(w)?PrG(w) and theratio PrA(w)/PrG(w) to capture the change ofprobability.
In addition, we calculate KL-likeweighted scores for words which reflect both thechange of probabilities between the two samplesand the overall frequency of the word.
Herewe calculate both KL(A ?
G) and KL(G ?A).
Words with high values for the former scoreare favored in the summaries because they havehigher probability in the abstracts than in theoriginals and have relatively high probability inthe abstracts.
The later score is high for words thatare often not included in summaries.KL(A ?
G)(w) = PrA(w) ?
lnPrA(w)PrG(w)KL(G ?
A)(w) = PrG(w) ?
lnPrG(w)PrA(w)Table 2 shows examples of the globalinformation captured from the three typesof scores?KL(A ?
G), KL(G ?
A) andPrA(w)?listing the 30 content words withhighest scores for each type.
Words that tend tobe used in the summaries, characterized by highKL(A ?
G) scores, include locations (York, NJ,Iraq), people?s names and titles (Bush, Sen, John),some abbreviations (pres, corp, dept) and verbs ofconflict (contends, dies).
On the other hand, fromKL(G ?
A), we can see that it is unlikely forwriters to include courtesy titles (Mr, Ms, Jr.) andrelative time reference in summaries.
The wordswith high PrA(w) scores overlaps with thoseranked highly by KL(A ?
G) to some extent,but also includes a number of generally frequentwords which appeared often both in the abstractsand original texts, such as million and percent.6.2 Blind Sentence ExtractionIn later sections we include the measures ofglobal word importance as a feature of ourregression model for predicting word weights forsummarization.
Before turning to that, however,we report the results of an experiment aimed toconfirm the usefulness of these features.
Wepresent a system, BLIND, which uses only weightsassigned to words by KL(A ?
G) from NYT,without doing any analysis of the original input.We rank all non-stopword words from the inputaccording to this score.
The top k words are givenweight 1, while the others are given weight 0.The summaries are produced following the greedyprocedure described in Section 5.1.Systems R-1 R-2 R-4RANDOM 30.32 4.42 0.36BLIND (80 keywords) 30.77 5.18 0.53BLIND (300 keywords) 32.91 5.94 0.61LASTESTLEAD 31.39 6.11 0.63FIRST-SENTENCE 34.26 7.22 1.21Table 3: Blind sentence extraction system,compared with three baseline systems (%)Table 3 shows that the BLIND system has R-2recall of 0.0594 using the top 300 keywords,significantly better than picking sentences fromthe input randomly.
It also achieves comparableperformance with the baseline in DUC 2004,formed by selecting the first 100 words fromthe latest article in the input (LASTESTLEAD).However it is significantly worse than anotherbaseline of selecting the first sentences from theinput.
Table 4 gives sample summaries generatedby these three approaches.
These results confirmthat the information gleaned from the analysis715Random SummaryIt was sunny and about 14 degrees C(57 degrees F) in Tashkent on Sunday.
The president is a strong person, and he has beenthrough far more difficult political situations, Mityukov said, according to Interfax.
But Yeltsin?s aides say his first term,from 1991 to 1996, does not count because it began six months before the Soviet Union collapsed and before the currentconstitution took effect.
He must stay in bed like any other person, Yakushkin said.
The issue was controversial earlier thisyear when Yeltsin refused to spell out his intentions and his aides insisted he had the legal right to seek re-election.NYT Summary from global keyword selection, KL(A ?
G), k = 300Russia?s constitutional court opened hearings Thursday on whether Boris Yeltsin can seek a third term.
Yeltsin?s growinghealth problems would also seem to rule out another election campaign.
The Russian constitution has a two-term limit forpresidents.
Russian president Boris Yeltsin cut short a trip to Central Asia on Monday due to a respiratory infection thatrevived questions about his overall health and ability to lead Russia through a sustained economic crisis.
The upper house ofparliament was busy voting on a motion saying he should resign.
The start of the meeting was shown on Russian television.First Sentence Generated SummaryPresident Boris Yeltsin has suffered minor burns on his right hand, his press office said Thursday.
President Boris Yeltsin?sdoctors have pronounced his health more or less normal, his wife Naina said in an interview published Wednesday.
PresidentBoris Yeltsin, on his first trip out of Russia since this spring, canceled a welcoming ceremony in Uzbekistan on Sundaybecause he wasn?t feeling well, his spokesman said.
Doctors ordered Russian President Boris Yeltsin to cut short his CentralAsian trip because of a respiratory infection and he agreed to return home Monday, a day earlier than planned, officials said.Table 4: Summary comparison by Random, Blind Extraction and First Sentence systemsof NYT abstract-original pairs encodes highlyrelevant information about important contentindependent of the actual text of the input.7 Regression-Based Keyword ExtractionHere we introduce a logistic regression modelfor assigning importance weights to words in theinput.
Crucially, this model combines evidencefrom multiple indicators of importance.
We haveat our disposal abundant data for learning becauseeach content word in the input can be treated asa labeled instance.
There are in total 32, 052samples from the 30 inputs of DUC 2003 fortraining, 54, 591 samples from the 50 inputs ofDUC 2004 for testing.
For a word in the input,we assign label 1 if the word appears in at leastone of the four human summaries for this input.Otherwise we assign label 0.In the rest of this section, we describe the richvariety of features included in our system.
We alsoanalyze and discuss the predictive power of thosefeatures by performing Wilcoxon signed-rank teston the DUC 2003 dataset.
There are in total 9, 261features used, among them 1, 625 are significant(p-value < 0.05).
We rank these features inincreasing p-values derived from Wilcoxon test.Apart from the widely used features of wordfrequency and positions, some other less exploredfeatures are highly significant.7.1 Frequency FeaturesWe use the Probability, LLR chi-square statisticvalue and MRW scores as features.
Since priorwork has demonstrated that for LLR weights inparticular, it is useful to identify a small set ofimportant words and ignore all other words insummary selection (Gupta et al., 2007), we usea number of keyword indicators as features.
Forthese indicators, the value of feature is 1 if theword is ranked within top ki, 0 otherwise.
Here kiare preset cutoffs5.
These cutoffs capture differentpossibilities for defining the keywords in the input.We also add the number of input documents thatcontain the word as a feature.
There are a total of100 features in this group, all of which are highlysignificant, ranked among the top 200.7.2 Standard featuresWe now describe some standard features whichhave been applied in prior work on summarization.Word Locations: Especially in news articles,sentences that occur at the beginning are often themost important ones.
In line with this observation,we calculate several features related to the positionin which a word appears.
We first computethe relative positions for word tokens, wherethe tokens are numbered sequentially in order ofappearance in each document in the input.
Therelative position for one word token is thereforeits corresponding number, divided by total numberof tokens minus one in the document, e.g., 0for the first token, 1 for the last token.
Foreach word, we calculate its earliest first location,latest last location, average location and averagefirst location for tokens of this word across alldocuments in the input.
In addition we have abinary feature indicating if the word appears in the510, 15, 20, 30, 40, ?
?
?
, 190, 200, 220, 240, 260, 280,300, 350, 400, 450, 500, 600, 700 (in total 33 values)716first sentence and the number of times it appearsin a first sentence among documents in one input.There are 6 features in this group.
All of them arevery significant, ranked within the top 100.Word type: These features include Part ofSpeech (POS) tags, Name Entity (NE) labels andcapitalization information.
We use the StanfordPOS-Tagger (Toutanova et al., 2003) and NameEntity Recognizer (Finkel et al., 2005).
We haveone feature corresponding to each possible POSand NE tag.
The value of this feature is theproportion of occurrences of the word with thistag; in most cases only one feature gets a non-zerovalue.
We have two features which indicate ifone word has been capitalized and the ratio of itscapitalized occurrences.Most of the NE features (6 out of 8) aresignificant: there are more Organizations andLocations but fewer Time and Date words in thehuman summaries.
Of the POS tags, 11 out of 41are significant: there are more nouns (NN, NNS,NNPS); fewer verbs (VBG, VBP, VB) and fewercardinal numbers in the abstracts compared to theinput.
Capitalized words also tend to be includedin human summaries.KL: Prior work has shown that having estimatesof sentence importance can also help in estimatingword importance (Wan et al., 2007; Liu et al.,2011; Wei et al., 2008).
The summarizer basedon KL-divergence assigns importance to sentencesdirectly, in a complex function according to theword distribution in the sentence.
Therefore,we use these summaries as potential indicatorsof word importance.
We include two featureshere, the first one indicates if the word appearsin a KLSUM summary of the input, as well asa feature corresponding to the number of timesthe word appeared in that summary.
Both of thefeatures are highly significant, ranked within thetop 200.7.3 NYT-weights as FeaturesWe include features from the relative rank ofa word according to KL(A ?
G), KL(G ?A), PrA(w)?PrG(w), PrA(w)/PrG(w) andPrA(w), derived from the NYT as described inSection 6.
If the rank of a word is within top-kor bottom-k by one metric, we would label it as1, where k is selected from a set of pre-definedvalues6.
We have in total 70 features in this6100, 200, 500, 1000, 2000, 5000, 10000 in this case.category, of which 56 are significant, 47 havinga p-value less than 10?7.
The predictive power ofthose global indicators are only behind the featureswhich indicates frequency and word positions.7.4 UnigramsThis is a binary feature corresponding to eachof the words that appeared at least twice in thetraining data.
The idea is to learn which wordsfrom the input tend to be mentioned in the humansummaries.
There are in total 8, 691 unigrams,among which 1, 290 are significant.
Despite thehigh number of significant unigram features, mostof them are not as significant as the more generalones we described so far.
It is interesting tocompare the significant unigrams identified in theDUC abstract/input data with those derived fromthe NYT corpus.
Unigrams that tend to appear inDUC summaries include president, government,political.
We also find the same unigrams amongthe top words from NYT corpus according toKL(A ?
G) .
As for words unlikely to appear insummaries, we see Wednesday, added, thing, etc,which again rank high according to KL(G ?
A).7.5 Dictionary Features: MPQA and LIWCUnigram features are notoriously sparse.
Tomitigate the sparsity problem, we resort tomore general groupings to words according tosalient semantic and functional categories.
Weemploy two hand-crafted dictionaries, MPQA forsubjectivity analysis and LIWC for topic analysis.The MPQA dictionary (Wiebe and Cardie,2005) contains words with different polarities(positive, neutral, negative) and intensities (strong,weak).
The combinations correspond to sixfeatures.
It turns out that words with strongpolarity, either positive or negative, are seldomlyincluded in the summaries.
Most strikingly,the p-value from significance test for the strongnegative words is less than 10?4?these wordsare rarely included in summaries.
There is nosignificant difference on weak polarity categories.Another dictionary we use is LIWC (Tausczikand Pennebaker, 2007), which contains manuallyconstructed dictionaries for multiple categoriesof words.
The value of the feature is 1 forone word if the word appears in the particulardictionary for the category.
34 out of 64 LIWCfeatures are significant.
Interesting categorieswhich appear at higher rate in summaries includeevents about death, anger, achievements, money717and negative emotions.
Those that appear at lowerrate in the summaries include auxiliary verbs, hear,pronouns, negation, function words, social words,swear, adverbs, words related to families, etc.7.6 Context FeaturesWe use context features here, based on theassumption that context importance around a wordaffects the importance of this word.
For contextwe consider the words before and after the targetword.
We extend our feature space by calculatingthe weighted average of the feature values of thecontext words.
For word w, we denote Lwas theset of words before w, Rwas the set of wordsafter w. We denote the feature for one word asw.fi, the way of calculating the newly extendedword-before feature w.lficould be written as:w.lfi=?ip(wl) ?
wl.fi, ?wl?
LwHere p(wl) is the probability word wlappearsbefore w among all words in Lw.For context features, we calculate the weightedaverage of the most widely used basic features,including frequency, location and capitalizationfor surrounding contexts.
There are in total220 features of this kind, among which 117 aresignificant, 74 having a p-value less than 10?4.8 ExperimentsThe performance of our logistic regression modelis evaluated on two tasks: keyword identificationand extractive summarization.
We name oursystem REGSUM.8.1 Regression for Keyword IdentificationFor each input, we define the set of keywordsas the top k words according to the scoresgenerated from different models.
We compareour regression system with three unsupervisedsystems: PROB, LLR, MRW.
To show theeffectiveness of new features, we compare ourresults with a regression system trained onlyon word frequency and location related featuresdescribed in Section 7.
Those features are theones standardly used for ranking the importanceof words in recent summarization works (Yih etal., 2007; Takamura and Okumura, 2009; Sipos etal., 2012), and we name this system REGBASIC.Figure 1 shows the performance of systemswhen selecting the 100 words with highest weightsFigure 1: Precision, Recall and F-score ofkeyword identification, 100 words selected, G1asgold-standardas keywords.
Each word from the input thatappeared in any of the four human summaries isconsidered as a gold-standard keyword.
Amongthe unsupervised approaches, word probabilityidentifies keywords better than LLR and MRWby at least 4% on F-score.
REGBASIC does notgive better performance at keyword identificationcompared with PROB, even though it includeslocation information.
Our system gets 2.2%F-score improvement over PROB, 5.2% overREGBASIC, and more improvement over theother approaches.
All of these improvements arestatistically significant by Wilcoxon test.Table 5 shows the performance of keywordidentification for different Giand differentnumber of keywords selected.
The regressionsystem has no advantage over PROB whenidentifying keywords that appeared in all of thefour human summaries.
However our systemachieves significant improvement for predictingwords that appeared in more than one or twohuman summaries.78.2 Regression for SummarizationWe now show that the performance of extractivesummarization can be improved by betterestimation of word weights.
We compare ourregression system with the four models introducedin Section 8.1.
We also include PEER-65, the bestsystem in DUC-2004, as well as KLSUM forcomparison.
Apart from these, we compare ourmodel with two state-of-the-art systems, includingthe submodular approach (SUBMOD) (Lin and7We also apply a weighted keyword evaluation approach,similar to the pyramid method for summarization.
Stillour system shows significant improvement over the others.See https://www.seas.upenn.edu/~hongkai1/regsum.html fordetails.718Gi#words PROB LLR MRW REGBASIC REGSUMG180 43.6 37.9 38.9 39.9 45.7G1100 44.3 38.7 39.2 41.0 46.5G1120 44.6 38.5 39.2 40.9 46.4G230 47.8 44.0 42.4 47.4 50.2G235 47.1 43.3 42.1 47.0 49.5G240 46.5 42.4 41.8 46.4 49.2G310 51.2 46.2 43.8 46.9 50.2G315 51.4 47.5 43.7 49.8 52.9G320 49.7 47.6 42.5 49.3 51.5G45 50.0 48.8 44.9 43.6 45.1G46 51.4 46.9 43.7 45.2 47.6G47 50.9 48.2 43.7 45.8 47.8Table 5: Keyword identification F-score (%) for different Giand different number of words selected.Bilmes, 2012) and the determinantal point process(DPP) summarizer (Kulesza and Taskar, 2012).The summaries were kindly provided by theauthors of these systems (Hong et al., 2014).As can been seen in Table 6, our systemoutperforms PROB, LLR, MRW, PEER-65,KLSUM and REGBASIC.
These improvementsare significant on ROUGE-2 recall.
Interestingly,although the supervised system REGBASIC whichuses only frequency and positions achievelow performance in keyword identification, thesummaries it generates are of high quality.
Theinclusion of position features negatively affects theperformance in summary keyword identificationbut boosts the weights for the words which appearclose to the beginning of the documents, which ishelpful for identifying informative sentences.
Byincluding other features we greatly improve overREGBASIC in keyword identification.
Similarlyhere the richer set of features results in betterquality summaries.We also examined the ROUGE-1, -2, -4recall compared with the SUBMOD and DPPsummarizers8.
There is no significant differenceon R-2 and R-4 recall compared with thesetwo state-of-the-art systems.
DPP performedsignificantly better than our system on R-1 recall,but that system is optimizing on R-1 F-score intraining.
Overall, our conceptually simple systemis on par with the state of the art summarizers andpoints to the need for better models for estimatingword importance.8The results are slightly different from the ones reportedin the original papers due to the fact that we truncated to 100words, while they truncated to 665 bytes.System R-1 R-2 R-4PROB 35.14 8.17 1.06LLR 34.60 7.56 0.83MRW 35.78 8.15 0.99REGBASIC 37.56 9.28 1.49KL 37.97 8.53 1.26PEER-65 37.62 8.96 1.51SUBMOD 39.18 9.35 1.39DPP 39.79 9.62 1.57REGSUM 38.57 9.75 1.60Table 6: System performance comparison (%)9 ConclusionWe presented a series of experiments whichshow that keyword identification can be improvedin a supervised framework which incorporatesa rich set of indicators of importance.
Wealso show that the better estimation of wordimportance leads to better extractive summaries.Our analysis of features related to globalimportance, sentiment and topical categoriesreveals rather unexpected results and confirms thatword importance estimation is a worthy researchdirection.
Success in the task is likely to improvesophisticated summarization approaches too, aswell as sentence compression systems which useonly crude frequency related measures to decidewhich words should be deleted from a sentence.99The work is partially funded by NSF CAREER awardIIS 0953445.719ReferencesTaylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.2011.
Jointly learning to extract and compress.
InProceedings of ACL-HLT, pages 481?490.Asli Celikyilmaz and Dilek Hakkani-Tur.
2010.A hybrid hierarchical model for multi-documentsummarization.
In Proceedings of ACL, pages815?824.Asli Celikyilmaz and Dilek Hakkani-T?ur.
2011.Discovery of topically coherent sentences forextractive summarization.
In Proceedings ofACL-HLT, pages 491?499.John M. Conroy, Judith D. Schlesinger, and Dianne P.O?Leary.
2006.
Topic-focused multi-documentsummarization using an approximate oracle score.In Proceedings of COLING/ACL, pages 152?159.Hal Daum?e, III and Daniel Marcu.
2006.
Bayesianquery-focused summarization.
In Proceedings ofACL, pages 305?312.Gunes Erkan and Dragomir R. Radev.
2004.
Lexrank:graph-based lexical centrality as salience in textsummarization.
Journal of Artificial IntelligenceResearch, 22(1):457?479.Elena Filatova and Vasileios Hatzivassiloglou.
2004.A formal model for information selection inmulti-sentence text extraction.
In Proceedings ofCOLING.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-localinformation into information extraction systems bygibbs sampling.
In Proceedings of ACL, pages363?370.D.
Graff, J. Kong, K. Chen, and K. Maeda.
2007.English gigaword third edition.
Linguistic DataConsortium, Philadelphia, PA.Surabhi Gupta, Ani Nenkova, and Dan Jurafsky.2007.
Measuring importance and query relevancein topic-focused multi-document summarization.
InProceedings of ACL, pages 193?196.Aria Haghighi and Lucy Vanderwende.
2009.Exploring content models for multi-documentsummarization.
In Proceedings of HLT-NAACL,pages 362?370.Sanda Harabagiu and Finley Lacatusu.
2005.
Topicthemes for multi-document summarization.
InProceedings of SIGIR 2005, pages 202?209.Kai Hong, John M. Conroy, Benoit Favre, AlexKulesza, Hui Lin, and Ani Nenkova.
2014.
Arepositary of state of the art and competitive baselinesummaries for generic news summarization.
InProceedings of LREC, May.Alex Kulesza and Ben Taskar.
2012.
Determinantalpoint processes for machine learning.
Foundationsand Trends in Machine Learning, 5(2?3).Julian Kupiec, Jan Pedersen, and Francine Chen.
1995.A trainable document summarizer.
In Proceedingsof SIGIR, pages 68?73.Page Lawrence, Brin Sergey, Rajeev Motwani, andTerry Winograd.
1998.
The pagerank citationranking: Bringing order to the web.
Technicalreport, Stanford University.Hui Lin and Jeff Bilmes.
2012.
Learning mixturesof submodular shells with application to documentsummarization.
In UAI, pages 479?490.Chin-Yew Lin and Eduard Hovy.
2000.
Theautomated acquisition of topic signatures for textsummarization.
In Proceedgins of COLING, pages495?501.Chin-Yew Lin.
2004.
Rouge: A package forautomatic evaluation of summaries.
In TextSummarization Branches Out: Proceedings of theACL-04 Workshop, pages 74?81.Marina Litvak, Mark Last, and Menahem Friedman.2010.
A new approach to improving multilingualsummarization using a genetic algorithm.
InProceedings of ACL, pages 927?936.Fei Liu, Feifan Liu, and Yang Liu.
2011.
A supervisedframework for keyword extraction from meetingtranscripts.
Transactions on Audio Speech andLanguage Processing, 19(3):538?548.H.
P. Luhn.
1958.
The automatic creation ofliterature abstracts.
IBM Journal of Research andDevelopment, 2(2):159?165, April.M.
Marneffe, B. Maccartney, and C. Manning.
2006.Generating Typed Dependency Parses from PhraseStructure Parses.
In Proceedings of LREC-06, pages449?454.Rebecca Mason and Eugene Charniak.
2011.Extractive multi-document summaries shouldexplicitly not contain document-specific content.In Proceedings of the Workshop on AutomaticSummarization for Different Genres, Media, andLanguages, pages 49?54.Ryan McDonald.
2007.
A study of global inferencealgorithms in multi-document summarization.
InProceedings of ECIR, pages 557?564.Rada Mihalcea and Paul Tarau.
2004.
Textrank:Bringing order into text.
In Proceedings of EMNLP,pages 404?411.Ani Nenkova and Lucy Vanderwende.
2005.
Theimpact of frequency on summarization.
Technicalreport, Microsoft Research.720Ani Nenkova, Lucy Vanderwende, and KathleenMcKeown.
2006.
A compositional context sensitivemulti-document summarizer: exploring the factorsthat influence summarization.
In Proceedings ofSIGIR, pages 573?580.Jahna Otterbacher, G?unes Erkan, and Dragomir R.Radev.
2009.
Biased lexrank: Passageretrieval using random walks with question-basedpriors.
Information Processing and Management,45(1):42?54.Paul Over, Hoa Dang, and Donna Harman.
2007.
Ducin context.
Inf.
Process.
Manage., 43(6):1506?1520.Karolina Owczarzak, John M. Conroy, Hoa TrangDang, and Ani Nenkova.
2012.
An assessmentof the accuracy of automatic evaluation insummarization.
In NAACL-HLT 2012: Workshopon Evaluation Metrics and System Comparison forAutomatic Summarization, pages 1?9.Peter Rankel, John Conroy, Eric Slud, and DianneO?Leary.
2011.
Ranking human and machinesummarization systems.
In Proceedings of EMNLP,pages 467?473.Korbinian Riedhammer, Beno?
?t Favre, and DilekHakkani-T?ur.
2010.
Long story short -global unsupervised models for keyphrase basedmeeting summarization.
Speech Communication,52(10):801?815.G.
Salton.
1971.
The SMART Retrieval System:Experiments in Automatic Document Processing.Prentice-Hall, Inc., Upper Saddle River, NJ, USA.Evan Sandhaus.
2008.
The new york times annotatedcorpus.
Linguistic Data Consortium, Philadelphia,PA.Chao Shen and Tao Li.
2010.
Multi-documentsummarization via the minimum dominating set.
InProceedings of Coling, pages 984?992.Ruben Sipos, Pannaga Shivaswamy, and ThorstenJoachims.
2012.
Large-margin learning ofsubmodular summarization models.
In Proceedingsof EACL, pages 224?233.Andreas Stolcke.
2002.
SRILM ?
an extensiblelanguage modeling toolkit.
In Proceedings ofICSLP, volume 2, pages 901?904.Hiroya Takamura and Manabu Okumura.
2009.
Textsummarization model based on maximum coverageproblem and its variant.
In Proceedings of EACL,pages 781?789.Yla R Tausczik and James W Pennebaker.
2007.The Psychological Meaning of Words: LIWC andComputerized Text Analysis Methods.
Journal ofLanguage and Social Psychology, 29:24?54.Kristina Toutanova, Dan Klein, Christopher D.Manning, and Yoram Singer.
2003.
Feature-richpart-of-speech tagging with a cyclic dependencynetwork.
In Proceedings of the NAACL-HLT, pages173?180.XiaojunWan and Jianwu Yang.
2008.
Multi-documentsummarization using cluster-based link analysis.
InProceedings of SIGIR, pages 299?306.Xiaojun Wan, Jianwu Yang, and Jianguo Xiao.2007.
Towards an iterative reinforcement approachfor simultaneous document summarization andkeyword extraction.
In Proceedings of ACL, pages552?559.Furu Wei, Wenjie Li, Qin Lu, and Yanxiang He.
2008.Query-sensitive mutual reinforcement chain andits application in query-oriented multi-documentsummarization.
In Proceedings of SIGIR, pages283?290.Janyce Wiebe and Claire Cardie.
2005.
Annotatingexpressions of opinions and emotions in language.language resources and evaluation.
In LanguageResources and Evaluation (formerly Computers andthe Humanities), page 1(2).Wen-tau Yih, Joshua Goodman, Lucy Vanderwende,and Hisami Suzuki.
2007.
Multi-documentsummarization by maximizing informativecontent-words.
In Proceedings of IJCAI, pages1776?1782.Hongyuan Zha.
2002.
Generic summarization andkeyphrase extraction using mutual reinforcementprinciple and sentence clustering.
In Proceedingsof SIGIR, pages 113?120.721
