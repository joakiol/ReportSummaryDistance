LAW VIII - The 8th Linguistic Annotation Workshop, pages 159?168,Dublin, Ireland, August 23-24 2014.Focus Annotation in Reading Comprehension DataRamon Ziai Detmar MeurersSonderforschungsbereich 833Eberhard Karls Universit?at T?ubingen{rziai,dm}@sfs.uni-tuebingen.deAbstractWhen characterizing the information structure of sentences, the so-called focus identifies the partof a sentence addressing the current question under discussion in the discourse.
While this notionis precisely defined in formal semantics and potentially very useful in theoretical and practicalterms, it has turned out to be difficult to reliably annotate focus in corpus data.We present a new focus annotation effort designed to overcome this problem.
On the one hand, itis based on a task-based corpus providing more explicit context.
The annotation study is basedon the CREG corpus (Ott et al., 2012), which consists of answers to explicitly given readingcomprehension questions.
On the other hand, we operationalize focus annotation as an incrementalprocess including several substeps which provide guidance, such as explicit answer typing.We evaluate the focus annotation both intrinsically by calculating agreement between annotatorsand extrinsically by showing that the focus information substantially improves the automaticmeaning assessment of answers in the CoMiC system (Meurers et al., 2011).1 IntroductionThis paper discusses the interplay of linguistic and computational linguistic aspects in the analysis offocus as a core notion of information structure.
Empirically, our work focuses on analyzing the responsesto reading comprehension questions.
In computational linguistics, automatic meaning assessment deter-mining whether a response appropriately answers a given question about a given text has developed intoan active field of research.
Short Answer Assessment recently was also highlighted by the Joint StudentResponse Analysis and Textual Entailment Challenge (Dzikovska et al., 2013).
Some research in thisdomain has pointed out the relevance of identifying which parts of a response are given by the question(Bailey and Meurers, 2008; Mohler et al., 2011), with recent work pointing out that the relevant notionhere is that of focus as discussed in formal pragmatics (Meurers et al., 2011; Hahn and Meurers, 2012).Figure 1 provides an example of answer comparison for meaning assessment, where the focus (markedby square brackets) can effectively be used to zoom in on the information that is relevant for comparing atarget answer (TA) with a student answer (SA) given a question (Q).Figure 1: Answer comparison with the help of focusThis work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footerare added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/159To support this line of research, one needs to be able to identify the focus in a response.
As a first step,we have designed an annotation scheme and manual annotation process for identifying the focus in acorpus of reading comprehension responses.
Focus here is understood in the sense of Krifka (2007) asindicating the presence of alternatives in the context and being a direct answer to the Question UnderDiscussion (QUD, Roberts 1996).
This semantic view of focus is essentially language-independent.Some attempts at systematically identifying focus in authentic data have been made in the past (Dipperet al., 2007; Calhoun et al., 2010).
However, most approaches either capture a notion of focus moreclosely related to particular language features, such as the Topic-Focus Articulation and its relation to theword order in Czech (Bur?a?nov?a et al., 2000), or the approaches were not rewarded with much success(Ritz et al., 2008).
The latter have tried to identify focus in newspaper text or other data types where noexplicit questions are available, making the task of determining the QUD, and thus reliably annotatingfocus, very hard.
In contrast, in the research presented here, we work with responses to explicitly givenquestions that are asked about an explicitly given text.
Thus, we can make use of the characteristics of thequestions and text to obtain reliable focus annotation for the responses.Theoretical linguists have discussed the notion of focus for decades, cf., e.g., Jackendoff (1972),Stechow (1981), Rooth (1992), Schwarzschild (1999) and B?uring (2007).
However, for insights andarguments from theoretical work to be applicable in computational linguistics, they need to be linkedto thorough empirical work ?
an area where some work remains to be done (cf., e.g., De Kuthy andMeurers, 2012), with some recent research making significant headway (Riester and Baumann, 2013).As it stands, computational linguists have not yet been able to fully profit from the theoretical debate onfocus.
An important reason complementing the one just mentioned is the fact that the context in whichthe text to be analyzed is produced has rarely been explicitly taken into account and encoded.
Yet, manyof the natural tasks in which focus annotation would be relevant actually do contain explicit task andcontext information of relevance to determining focus.
To move things forward, this paper builds onthe availability and relevance of task-based language data and presents an annotation study of focus onauthentic reading comprehension data.
As a second component of our proposal, we operationalize thefocus annotation in terms of several incremental steps, such as explicit answer typing, which providerelevant information guiding the focus annotation as such.Overall, the paper tries to accomplish two goals, which are also reflected in the way the annotationis evaluated: i) to present an effective focus annotation scheme and to evaluate how consistently it canbe applied, and ii) to explore the possible impact of focus annotation on Short Answer Assessment.Establishing a focus annotation scheme for question-response pairs from authentic reading comprehensiondata involves sharpening and linking the concepts and tests from theoretical linguistic with the wide rangeof properties realized in the authentic reading comprehension data.
The work thus stands to contributeboth to an empirical evaluation and enrichment of the linguistic concepts as well as to the development ofautomatic focus annotation approaches in computational linguistics.The paper is organized as follows: Section 2 presents the corpus data on which we base the annotationeffort and the annotation process.
Section 3 introduces the scheme we developed for annotating thereading comprehension data.
Section 4 then launches into both intrinsic and extrinsic evaluation of themanual annotation, before section 5 concludes the paper.2 Data and Annotation SetupWe base our work on the CREG corpus (Ott et al., 2012), a task-based corpus consisting of answers toreading comprehension questions written by learners of German at the university level.
The overall corpusincludes 164 reading texts, 1,517 reading comprehension questions, 2,057 target answers provided by theteachers, and 36,335 learner answers.
We use the CREG-1032 data subset (Meurers et al., 2011) for thepresent annotation work in order to enable comparison to previously published results on that data set(Meurers et al., 2011; Hahn and Meurers, 2012; Horbach et al., 2013).
The CREG-1032 data set consistsof two sub-corpora, which correspond to the sites they were collected at, Kansas University (KU) andOhio State University (OSU).
For the present work, we limited ourselves to the OSU portion of the databecause it contains longer answers and more answers per question.160The OSU subset consists of 422 student answers to 60 questions, for which 87 target answers areavailable.
The student answers were produced by 175 intermediate learners of German in the US, who onaverage wrote about 15 tokens per answer.
All student answers were rated by two annotators with respectto whether they answer the question or not.
The subset is balanced, i.e.
it contains the same number ofcorrect and incorrect answers, and both annotators agreed on the meaning assessment.To obtain a gold-standard focus annotation for this data set, we set out to manually annotate bothtarget answers and student answers with focus.
We also annotated the question forms in the question.The annotation was performed by two graduate research assistants in linguistics using the brat1rapidannotation tool directly on the token level.
Each annotator was given a separate directory containingidentical source files to annotate.
In order to sharpen distinctions and refine the annotation scheme to itscurrent state, we drew a random sample of 100 questions, target answers and student answers from eachsub-corpus of CREG and trained our two annotators on them.
During this piloting process, the first authormet with the annotators to discuss difficult cases and decide how the scheme would accommodate them.Figure 2 shows a sample screenshot of the brat tool.
The question asks for a person, namely the one?wandering through the dark outskirts?.
The target response provides an answer with an appropriate focus.The student response instead appears to answer a question about the reason for this person?s action, suchas ?Why did he wander through the dark outskirts?
?.Q: ?Who wandered through the dark outskirts?
?TA: ?The child?s father wandered through the dark outskirts.
?SA: ?He searched for wood.
?Figure 2: Example with a who-question and a different QUD for the student answer3 Annotation SchemeIn this section, we introduce the annotation scheme we developed.
An important characteristic of ourannotation scheme is that it is applied incrementally: annotators first look at the surface question form, thendetermine the set of alternatives (Krifka, 2007, sec.
3), and finally they mark instances of the alternativeset in answers.
The rich task context of reading comprehension data with its explicit questions allowsus to circumvent the problem of guessing an implicit QUD, except in the cases where students answer adifferent question (which we account for separately, see below).
In the following, we present the threetypes of categories our scheme is built on.Question Form is used to mark the surface form of a question, where we distinguish wh-questions,polarity questions, alternative questions, imperatives and noun phrase questions.
In themselves, questionforms do not encode any semantics, but merely act as an explicit marker of the surface question form.Table 1 gives an overview and examples of this dimension.Focus is used to mark the focused words or phrases in an answer.
We do not distinguish betweencontrastive and new information focus, as this is not relevant for assessing an answer.
Multiple foci can beencoded and in fact do occur in the data.1http://brat.nlplab.org161Category Example TranslationWhPhrase ?Warum hatte Schorlemmer zu Beginn Angst??
?Why was Schorlemmer afraid in the beginning?
?YesNo ?Muss man deutscher Staatsb?urger sein??
?Does one have to be a German citizen?
?Alternative ?Ist er f?ur oder gegen das EU-Gesetz??
?Is he for or against the EU law?
?Imperative ?Begr?unden Sie diesen anderen Spitznamen.?
?Give reasons for this other nickname.
?NounPhrase ?Wohnort??
?Place of residence?
?Table 1: Question Forms in the annotation schemeThe starting point of our focus annotation is Krifka (2007)?s understanding of focus as the part of anutterance that indicates the presence of alternatives relevant to the interpretation.
We operationalize thisby testing whether a given part of the utterance is needed to distinguish between alternatives in the QUD.Concretely, we train annotators to perform substitution tests in which they compare two potential extentsof the focus to identify whether the difference in the extent of the focus also selects a different validalternative in the sense of discriminating between alternatives in the QUD.
For instance, consider theexample in (1), where the focus is made explicit by the square brackets.
(1) Where does Heike live?She lives [[in Berlin.
]]FHere ?in?
needs to be part of the focus because exchanging it for another word with the same POSchanges the meaning of the phrase in a way picking another alternative, as in ?She lives near Berlin?.Consider the same answer to a slightly different question in (2).
Here the set of alternatives is moreconstrained and hence ?in?
is not focused.
(2) In what city does Heike live?She lives in [[Berlin]]F.Other criteria we defined to guide focus annotation include the following:?
Coordination: If several foci are coordinated, each should be marked separately.?
Givenness: Avoid marking given material except where needed to distinguish between alternatives.?
Each sentence is assumed to include at least one focus.
If it does not answer the explicit question, itmust be annotated with a different QUD (discussed below).?
Focus never crosses sentence boundaries.?
Focus does not apply to sub-lexical units, such as syllables.?
Punctuation at focus boundaries is to be excluded.In addition to marking focus, we annotate the relation between the explicitly given question and theQuestion Under Discussion actually answered by a given response.
In the most straightforward case, theQUD is identical to the explicit question given, which in the annotation scheme is encoded as questionanswered.
In cases where the QUD differs from the explicitly given question, we distinguish three cases:In the cases related to the implicit moves discussed in B?uring (2003, p. 525) exemplified by (3), the QUDanswered can be a subquestion of the explicit question, which we encode as question narrowed down.When it addresses a more general QUD, as in (4), the response is annotated as question generalized.
(3) What did the pop stars wear?The female pop stars wore caftans.
(4) Would you like a Coke or a Sprite?I?d like a beer.Finally, we also mark complete failures of question answer congruence with question ignored.
In allcases where the QUD being answered differs from the question explicitly given, the annotator is requiredto specify the QUD apparently being answered.162Answer Type expresses the semantic category of the focus in relation to the question form.
It furtherdescribes the nature of the question-answer congruence by specifying the semantic class of the set ofalternatives.
The answer types discussed in the computational linguistic literature generally are specific toparticular content domains, so that we developed our own taxonomy.
Examples include Time/Date,Location, Entity, and Reason.
In addition to semantically restricting the focus to a specific type,answer types can also provide syntactic cues restricting focus marking.
For example, an Entity willtypically be encoded as a nominal expression.
For annotation, the advantage of answer types is that theyforce annotators to make an explicit commitment to the semantic nature of the focus they are annotating,leading to potentially higher consistency and reliability of annotation.
On the conceptual side, the semanticrestriction encoded in the answer type bears an interesting resemblance to what in a Structured Meaningapproach to focus (Krifka, 1992) is referred to as restriction of the question (Krifka, 2001, p. 3).Category Description Example (translated)Time Date time/date expression, usually incl.
preposition The movie starts at 5:50Living Being individual, animal or plant The father of the child padded through the darkoutskirts.Thing concrete object which is not alive For the Spaniards toilet and stove are moreimportant than the internet.Abstract Entity entity that is not concrete The applicant needs a completed vocationaltraining as a cook.Report reported incident or statement The speaker says ?We ask all youths to havetheir passports ready.
?Reason reason or cause for a statement The maintenance of a raised garden bed iseasier because one does not need to stoop.Location place or relative location She is from Berlin.Action activity or happening.
In the vegetable garden one needs to hoe andwater.Property attribute of something Reputation and money are important for Til.Yes No polar answer, including whole statementif not ellipticThe mermaid does not marry the prince.Manner way in which something is done The word is used ironically in this story.Quantity/Duration countable amount of something The company seeks 75 employees.State state something is in, or result of some action If he works hard now, he won?t have to workin the future.Table 2: Answer Types with examples4 EvaluationThe approach is evaluated in two ways.
First, the consistency with which the focus annotation schemewas applied is evaluated in section 4.1 by calculating inter-annotator agreement.
In section 4.2 we thenexplore the effect of focus annotation on Short Answer Assessment.
For both evaluations, we provide aqualitative discussion of characteristic examples.4.1 Intrinsic Evaluation4.1.1 Quantitative ResultsHaving carried out the manual annotation experiment, the question arises how to compare and calculateagreement of spans of tokens in focus annotation.
While comparing individual spans and calculating somekind of overlap measure is certainly possible, it is hard to interpret the meaning of such numbers.
Wetherefore decided to make as few assumptions as possible and treat each token as a markable for whichthe annotator needs to make a decision.
On that basis, we then follow standard evaluation procedures incalculating percentage agreement and Cohen?s Kappa (Artstein and Poesio, 2009).Table 3 summarizes the agreement results.
For both student and target answers, we report the granularityof the distinction being made (focus/background vs. all answer types), the number of tokens the distinctionapplies to, and finally percentage and Kappa agreement.163Type of distinction Type of answers # tokens % ?Binary Student 6329 82.8 .65(focus/background) Target 6983 84.9 .69Detailed Student 5198 72.6 .61(13 Answer Types + background) Target 6839 76.5 .67Table 3: Inter-annotator agreement on student and target answersThe results show that all numbers are in the area of substantial agreement (?
> .6).
This is a noticeablyimprovement over the results obtained by Ritz et al.
(2008), who report ?
= .51 on tokens in questionnairedata, and it is on a par with the results reported by Calhoun et al.
(2010).
Annotation was easier on themore well-formed target answers than on the often ungrammatical student answers.
Moving from thebinary focus/background distinction to the one involving all Answer Types, we still obtain relatively goodagreement.
This indicates that the semantic characterization of foci via Answer Types works quite well,with the gap between student and target answers being even more apparent here.In order to assess the effect of answer length, we also computed macro-average versions of percentageagreement and ?
for the binary focus distinction, following Ott et al.
(2012, p. 55) but averaging overanswers.
We obtained 84.0% and ?
= .67 for student answers, and 87.4% and ?
= .74 for target answers.A few longer answers which are harder to annotate thus noticeably affected the agreement results ofTable 3 negatively.4.1.2 ExamplesTo explore the nature of the disagreements, we showcase two characteristic issues here based on examplesfrom the corpus.
Consider the following case where the annotators disagreed on the annotation of astudent answer:Q: Warum nennt der Autor Hamburg das ?Tor zur Welt der Wissenschaft??
?Why does the author call Hamburg the ?gate to the world of science??
?SA: [[Hamburg hat viel renommierte Universit?aten]]F(annotator 1)Hamburg hat [[viel renommierte Universit?aten]]F(annotator 2)?Hamburg has many renowned universities?Figure 3: Disagreement involving given materialWhereas annotator 1 marks the whole answer on the grounds that the focus is of Answer Type Reasonand needs to include the whole proposition, annotator 2 excludes material given in the question.
Both canin theory be justified, but annotator 1 is closer to our guidelines here, taking into account that ?Hamburg?indeed discriminates between alternatives (one could give reasons that do not include ?Hamburg?)
andthus needs to be part of the focus.The second example illustrates the issue of deciding where the boundary of a focus is:Q: Wof?ur ist der Aufsichtsrat verantwortlich?
?What is the supervisory board responsible for?
?SA: Der Aufsichtsrat ist f ?ur [[die Bestellung]]Fverantwortlich.
(annotator 1)Der Aufsichtsrat ist [[f ?ur die Bestellung]]Fverantwortlich.
(annotator 2)?The supervisory board is responsible for the appointment.
?Figure 4: Disagreement on a prepositionAnnotator 1 correctly excluded ?f?ur?
(?for?)
from the focus, only marking ?die Bestellung?
(?theappointment?)
given that ?f?ur?
is only needed for reasons of well-formedness.
Annotator 2 apparentlythought that ?f?ur?
makes a semantic difference here, but it is hard to construct a grammatical examplewith a different preposition that changes the meaning of the focused expression.1644.2 Extrinsic EvaluationIt has been pointed out that evaluating an expert annotation of a theoretical linguistic notion onlyintrinsically is problematic because there is no non-theoretical grounding involved (Riezler, 2014).Therefore, besides calculating agreement measures, we also evaluated the resulting annotation in a largercomputational task, the automatic meaning assessment of answers to reading comprehension questions.We used the CoMiC system (Comparing Meaning in Context, Meurers et al., 2011) as a testbed for ourexperiment.
CoMiC is an alignment-based system operating in three stages:1.
Annotating linguistic units (words, chunks and dependencies) in student and target answer on variouslevels of abstraction2.
Finding alignments of linguistic units between student and target answer based on annotation3.
Classifying the student answer based on number and type of alignments, using a supervised machinelearning setup with 13 features in totalIn stage 2, CoMiC integrates a simplistic approach to givenness, excluding all words from alignmentthat are mentioned in the question.
We transferred the underlying method to the notion of focus andimplemented a component that excludes all non-focused words from alignment, resulting in alignmentsbetween focused parts of answers only.
We only used the foci where students did not ignore the questionaccording to the annotators.For the present evaluation, we experimented with three different settings involving the basic givennessfilter and our focus annotations: i) using the givenness filter by itself as a baseline, ii) aligning onlyfocused tokens as described above and iii) combining both by producing a givenness and a focus versionof each classification feature.
All three settings were tried out for annotator 1 and 2.4.2.1 Quantitative ResultsTable 4 summarizes the quantitative results.
It shows that focus beats the basic givenness baseline of84.6% on its own, pushing the classification accuracy to 86.7% for annotator 1 and 87.2% for annotator 2.Annotator 1 Annotator 2Basic givenness only 84.6Focus only 86.7 87.2Focus + givenness 90.3 89.3Table 4: Answer classification accuracy with the CoMiC systemWhile this is an encouraging result already, the combination of basic givenness and focus performssubstantially better, reaching 90.3% accuracy for annotator 1 and 89.3% for annotator 2.In terms of the conceptual notions of formal pragmatics, this is an interesting result.
While the notionof givenness implemented here is surface-based and mechanistic and thus could be improved, the resultssupport the idea that both of the commonly discussed dimensions, focus/background and new/given, areuseful and informative information-structural dimensions that complement each other in assessing themeaning of answers.Interestingly, the focus annotation of annotator 2 on its own performed better than that of annotator 1,but worse when combined with basic givenness.
We suspect that annotator 2?s understanding of focusrelied more on the concept of givenness than annotator 1?s, causing the combination of the two to be lessinformative than for annotator 1.4.2.2 Alignment ExampleThe possible benefits of using focus to constrain alignment can take different forms: focus can lead us toexclude extra, irrelevant material, but it can also uncover the fact that the relevant piece of information hasin fact not been included, as in the following corpus example:165Q: Was machen sie, um die Brunnen im Winter zu sch?utzen?
?What do they do to protect the wells in winter?
?TA: Zw?olf der 47 Brunnen werden im Winter aus Schutz vor dem Frost undWitterungssch?aden [[eingehaust]]F?Twelve of the 47 wells are encased in winter for protection from freezing and damage fromweather conditions?SA: im Winter gibt es Frost und Witterungssch?aden?in winter there is freezing and damage from weather conditions?Figure 5: No alignments because the student answer ignores the questionThe question asks what is being done to protect the wells in winter, for which the text states that twelveof wells are encased for protection (technically, this is an answer to a sub-question since nothing is assertedabout the other wells).
Additional new information such as ?vor dem Frost und Witterungssch?aden?
doesnot distinguish between alternatives to the question ?Was machen sie.
.
.
?
?, which clearly asks for anAction.
The target and student answer have high token overlap due to the presence of such extrainformation, but only the target answer contains the relevant focus ?eingehaust?.
Without the focus filter,CoMiC wrongly classifies this answer as correct, but with the added focus information, it has the meansto judge this answer adequately.5 Conclusion and OutlookWe presented a focus annotation study based on reading comprehension data, which we view as acontribution to the general goal of analyzing and annotating focus.
Motivated by the limited success ofapproaches trying to tackle focus annotation from a general conceptual level, we aim to proceed fromthe concrete task to the more general setting.
This allows us to separate a) identifying the QUD and b)determining the location and extent of the focus in the language material, where a) is informed and greatlysimplified by the explicit question.Using this approach in combination with semantically motivated annotation guidelines, we showed thatfocus annotation can be carried out systematically with Kappa values in the range of .61 to .69, dependingon the well-formedness of the language and the number of classes distinguished.With respect to the practical goal of improving automatic assessment of short student answers, weshowed that information structural distinctions are relevant and able to quantitatively improve the results,as demonstrated by an increase from 84.6% to 90.3% accuracy in a binary classification task on a balanceddata set.While the manual annotation showcases the relevance and impact of focus annotation, we see thedesign of an automatic focus/background classification system on the basis of our annotated data as thelogical next step.
As such a system cannot perform the kind of introspective language analysis our humanannotators employed, we will have to approximate focus through surface criteria such as word order,syntactic categories and focus sensitive particles.
It remains to be seen how much of the potential benefitof focus annotation can be reached by automatic focus annotation using machine learning.Finally, in order to obtain more human-annotated data, we are planning to turn focus annotation ofanswers to questions into a feasible crowd-sourcing task.AcknowledgementsWe are grateful to Heike Cardoso and Stefanie Wolf for carrying out the manual annotation and providingvaluable feedback.
We also would like to thank Kordula De Kuthy, Verena Henrich, Niels Ott and thethree anonymous reviewers for their helpful comments.166ReferencesRon Artstein and Massimo Poesio.
2009.
Survey article: Inter-coder agreement for computational linguistics.Computational Linguistics, 34(4):1?42.Stacey Bailey and Detmar Meurers.
2008.
Diagnosing meaning errors in short answers to reading comprehensionquestions.
In Joel Tetreault, Jill Burstein, and Rachele De Felice, editors, Proceedings of the 3rd Workshop onInnovative Use of NLP for Building Educational Applications (BEA-3) at ACL?08, pages 107?115, Columbus,Ohio.Eva Bur?a?nov?a, Eva Haji?cov?a, and Petr Sgall.
2000.
Tagging of very large corpora: topic-focus articulation.
InProceedings of the 18th conference on Computational linguistics - Volume 1, COLING ?00, pages 139?144,Stroudsburg, PA, USA.
Association for Computational Linguistics.Daniel B?uring.
2003.
On d-trees, beans, and b-accents.
Linguistics and Philosophy, 26(5):511?545.Daniel B?uring.
2007.
Intonation, semantics and information structure.
In Gillian Ramchand and Charles Reiss,editors, The Oxford Handbook of Linguistic Interfaces.
Oxford University Press.Sasha Calhoun, Jean Carletta, Jason Brenier, Neil Mayo, Dan Jurafsky, Mark Steedman, and David Beaver.
2010.The NXT-format switchboard corpus: A rich resource for investigating the syntax, semantics, pragmatics andprosody of dialogue.
Language Resources and Evaluation, 44:387?419.Kordula De Kuthy and Detmar Meurers.
2012.
Focus projection between theory and evidence.
In Sam Featherstonand Britta Stolterfoth, editors, Empirical Approaches to Linguistic Theory ?
Studies in Meaning and Structure,volume 111 of Studies in Generative Grammar, pages 207?240.
De Gruyter.Stefanie Dipper, Michael G?otze, and Stavros Skopeteas, editors.
2007.
Information Structure in Cross-LinguisticCorpora: Annotation Guidelines for Phonology, Morphology, Syntax, Semantics and Information Structure,volume 7 of Interdisciplinary Studies on Information Structure.
Universit?atsverlag Potsdam, Potsdam, Germany.Myroslava Dzikovska, Rodney Nielsen, Chris Brew, Claudia Leacock, Danilo Giampiccolo, Luisa Bentivogli,Peter Clark, Ido Dagan, and Hoa Trang Dang.
2013.
Semeval-2013 task 7: The joint student response analysisand 8th recognizing textual entailment challenge.
In Second Joint Conference on Lexical and ComputationalSemantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation(SemEval 2013), pages 263?274, Atlanta, Georgia, USA, June.
Association for Computational Linguistics.Michael Hahn and Detmar Meurers.
2012.
Evaluating the meaning of answers to reading comprehension ques-tions: A semantics-based approach.
In Proceedings of the 7th Workshop on Innovative Use of NLP for BuildingEducational Applications (BEA-7) at NAACL-HLT 2012, pages 94?103, Montreal.Andrea Horbach, Alexis Palmer, and Manfred Pinkal.
2013.
Using the text to evaluate short answers for readingcomprehension exercises.
In Second Joint Conference on Lexical and Computational Semantics (*SEM), Vol-ume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity, pages 286?295,Atlanta, Georgia, USA, June.
Association for Computational Linguistics.Ray Jackendoff.
1972.
Semantic Interpretation in Generative Grammar.
MIT Press, Cambridge, MA.Manfred Krifka.
1992.
A compositional semantics for multiple focus constructions.
In Joachim Jacobs, editor,Informationsstruktur und Grammatik, pages 17?54.
Westdeutscher Verlag, Opladen.Manfred Krifka.
2001.
For a structured meaning account of questions and answers.
In C. Fery and W. Sternefeld,editors, Audiatur Vox Sapientia.
A Festschrift for Arnim von Stechow, volume 52 of studia grammatica, pages287?319.
Akademie Verlag, Berlin.Manfred Krifka.
2007.
Basic notions of information structure.
In Caroline Fery, Gisbert Fanselow, and ManfredKrifka, editors, The notions of information structure, volume 6 of Interdisciplinary Studies on InformationStructure (ISIS), pages 13?55.
Universit?atsverlag Potsdam, Potsdam.Detmar Meurers, Ramon Ziai, Niels Ott, and Janina Kopp.
2011.
Evaluating answers to reading comprehensionquestions in context: Results for german and the role of information structure.
In Proceedings of the TextInfer2011 Workshop on Textual Entailment, pages 1?9, Edinburgh, Scotland, UK, July.
Association for Computa-tional Linguistics.Michael Mohler, Razvan Bunescu, and Rada Mihalcea.
2011.
Learning to grade short answer questions usingsemantic similarity measures and dependency graph alignments.
In Proceedings of the 49th Annual Meetingof the Association for Computational Linguistics: Human Language Technologies, pages 752?762, Portland,Oregon, USA, June.
Association for Computational Linguistics.167Niels Ott, Ramon Ziai, and Detmar Meurers.
2012.
Creation and analysis of a reading comprehension exercisecorpus: Towards evaluating meaning in context.
In Thomas Schmidt and Kai W?orner, editors, Multilingual Cor-pora and Multilingual Corpus Analysis, Hamburg Studies in Multilingualism (HSM), pages 47?69.
Benjamins,Amsterdam.Arndt Riester and Stefan Baumann.
2013.
Focus triggers and focus types from a corpus perspective.
Dialogue &Discourse, 4(2):215?248.Stefan Riezler.
2014.
On the problem of theoretical terms in empirical computational linguistics.
ComputationalLinguistics, 40(1):235?245.Julia Ritz, Stefanie Dipper, and Michael G?otze.
2008.
Annotation of information structure: An evaluation acrossdifferent types of texts.
In Proceedings of the 6th International Conference on Language Resources and Evalu-ation, pages 2137?2142, Marrakech, Morocco.Craige Roberts.
1996.
Information structure in discourse: Towards an integrated formal theory of pragmatics.
InJae-Hak Yoon and Andreas Kathol, editors, OSU Working Papers in Linguistics No.
49: Papers in Semantics.The Ohio State University.Mats Rooth.
1992.
A theory of focus interpretation.
Natural Language Semantics, 1(1):75?116.Roger Schwarzschild.
1999.
GIVENness, AvoidF and other constraints on the placement of accent.
NaturalLanguage Semantics, 7(2):141?177.Arnim von Stechow.
1981.
Topic, focus, and local relevance.
In Wolfgang Klein and W. Levelt, editors, Crossingthe Boundaries in Linguistics, pages 95?130.
Reidel, Dordrecht.168
