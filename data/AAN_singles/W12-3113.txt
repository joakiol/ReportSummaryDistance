Proceedings of the 7th Workshop on Statistical Machine Translation, pages 114?119,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsLORIA System for the WMT12 Quality Estimation Shared TaskLanglois Davidlanglois@loria.frRaybaud SylvainLORIA, Universite?
de Lorraine615 rue du Jardin Botanique54602 Villers les Nancy, Franceraybauds@loria.frSma?
?li Kamelsmaili@loria.frAbstractIn this paper we present the system we sub-mitted to the WMT12 shared task on QualityEstimation.
Each translated sentence is givena score between 1 and 5.
The score is ob-tained using several numerical or boolean fea-tures calculated according to the source andtarget sentences.
We perform a linear regres-sion of the feature space against scores in therange [1:5].
To this end, we use a Support Vec-tor Machine.
We experiment with two kernels:linear and radial basis function.
In our submis-sion we use the features from the shared taskbaseline system and our own features.
Thisleads to 66 features.
To deal with this largenumber of features, we propose an in-housefeature selection algorithm.
Our results showthat a lot of information is already present inbaseline features, and that our feature selec-tion algorithm discards features which are lin-early correlated.1 IntroductionMachine translation systems are not reliable enoughto be used directly.
They can only be used to graspthe general meaning of texts or help human transla-tors.
Confidence measures detect erroneous wordsor sentences.
Such information could be useful forusers to decide whether or not to post-edit translatedsentences (Specia, 2011; Specia et al, 2010) or se-lect documents mostly correctly translated (Soricutand Echihabi, 2010).
Moreover, it is possible to useconfidence measures to compare outputs from dif-ferent systems and to recommend the best one (Heet al, 2010).
One can also imagine that confidencemeasures at word-level could be also useful for amachine translation system to automatically correctparts of output: for example, a translation systemtranslates the source sentence, then, this output istranslated with another translation system (Simardet al, 2007).
This last step could be driven by confi-dence measures.In previous works (Raybaud et al, 2011; Raybaudet al, 2009a; Raybaud et al, 2009b) we used state-of-the-art features to predict the quality of a transla-tion at sentence- and word-level.
Moreover, we pro-posed our own features based on previous works oncross-lingual triggers (Lavecchia et al, 2008; Latiriet al, 2011).
We evaluated our work in terms of Dis-crimination Error Trade-off, Equal Error Rate andNormalised Mutual Information.In this article, we compare the features used in theshared task baseline system and our own features.This leads to 66 features which will be detailed insections 3 and 4.
We therefore deal with many fea-tures.
We used a machine learning approach to per-form regression of the feature space against scoresgiven by humans.
Machine learning algorithms maynot efficiently deal with high dimensional spaces.Moreover, some features may be less discriminantdescriptors and then in some cases could add morenoise than information.
That is why, in this articlewe propose an in-house feature selection algorithmto remove useless features.The article is structured as follows.
In Section 2,we give an overview of our quality estimation sys-tem.
Then, in Sections 3 and 4, we describe thefeatures we experimented with.
In section 6, we de-scribe the algorithm we propose for feature selec-114tion.
Then we give the results of several configura-tions in Section 7.2 Overview of our quality estimationsubmissionEach translated sentence is assigned a score between1 and 5.
5 means that the machine translation outputis perfectly clear and intelligible and 1 means that itis incomprehensible.
The score is calculated usingseveral numerical or boolean features extracted ac-cording to the source and target sentences.
We per-form a regression of the feature space against [1 : 5].3 The baseline featuresThe quality estimation shared task organizers pro-vided a baseline system including several interestingfeatures.
Among them, several are yet used in (Ray-baud et al, 2011) but we give below a brief reviewof the whole baseline features set1:?
Source and target sentences lengths: there is acorrelation between the sizes of source and tar-get sentences.?
Average source token length: this is the averagenumber of letters of the words in the sentence.We guess that this feature can be useful becauseshort words have more chance to be tool words.?
Language model likelihood of source and targetsentences: a source sentence with low likeli-hood is certainly far from training corpus statis-tics.
There is a risk it is badly translated.
A tar-get sentence with low likelihood is not suitablein terms of target language.?
Average number of occurrences of the wordswithin the target sentence: too many occur-rences of the same word in the target sentencemay indicate a bad translation.?
Average number of translations per sourceword in the sentence: for each word in thesource sentence, the feature indicates howmany words of the target sentence are indeedtranslations of this word in the IBM1 table(with probability higher than 0.2).1Indeed, our system takes into input a set of features, and isable to discard redundant features (see Section 6).?
Weighted average number of translations persource word in the sentence: this feature is sim-ilar to the previous one, but a frequent word isgiven a low weight in the averaging.?
n-gram frequency based features: the baselinesystem proposes to group the n-gram frequen-cies into 4 quartiles.
The features indicate howmany n-gram (unigram to trigram) in sourcesentence are in quartiles 1 and 4.
These fea-tures indicate if the source sentence containsn-grams relevant to the training corpus.?
Punctuation based features: there may exist acorrelation between punctuation of source andtarget sentences.
The count of punctuationmarks in both sentences may then be useful.Overall, the baseline system proposes 17 features.4 The LORIA featuresIn a previous work (Raybaud et al, 2011), we testedseveral confidence measures.
The Quality MeasureTask campaign constitutes a good opportunity for usto compare our approach to others.
We give belowa brief review of our features (we cite again featureswhich are yet presented in baseline features becausesometimes, we use a variant of them):?
lengths: three features are generated, lengths ofsource and target sentences (already presentedin baseline features), and ratio of target oversource length?
n-gram based features (Duchateau et al, 2002):each word in the source and target sentencesis given its 5-gram probability.
Then, thesentence-level score is the average of the scoresacross all words in the sentence.
There are 4features: one for each language (source and tar-get) and one for each direction (left-to-right andright-to-left 5-gram).?
backoff n-gram based features: in the sameway, a score is assigned to a word accordingto how many times the language model had toback off in order to assign a probability to thesequence (Uhrik and Ward, 1997).
Here too,word scores are averaged and we get 4 scores.115?
averaged features: a common property of all n-gram based and backoff based features is that aword can get a low score if it is actually correctbut its neighbours are wrong.
To compensatefor this phenomenon we took into account theaverage score of the neighbours of the word be-ing considered.
More precisely, for every rele-vant feature x. defined at word level we alsocomputed:xleft.
(wi) = x.
(wi?2) ?
x.
(wi?1) ?
x.(wi)xcentred.
(wi) = x.
(wi?1) ?
x.
(wi) ?
x.(wi+1)xright.
(wi) = x.
(wi) ?
x.
(wi+1) ?
x.
(wi+2)A sentence level feature is then calculated ac-cording to the average of each new ?averagedfeature?.?
intra-lingual features: the intra-lingual scoreof a word in a sentence is the average of themutual information between that word and theother words in that sentence.
Mutual informa-tion is defined by:I(w1, w2) = P (w1, w2)?log(P (w1, w2)P (w1)P (w2))(1)The intra-lingual score of a sentence is the av-erage of the intra-lingual scores of the words inthis sentence.
There are two features, one foreach language.?
cross-lingual features: the cross-lingual scoreof a word in a target sentence is the average ofthe mutual information between that word andthe words in the source sentence.
The cross-lingual score of a target sentence is the averageof its constituents.?
IBM1 features: the score of the target sentenceis the average translation probability providedby the IBM1 model.?
basic parser: this produces two scores, a bi-nary flag indicating whether any bracketing in-side the target sentence is correct, and one in-dicating if the sentence ends with an end ofsentence symbol (period, colon, semi-colon,question/exclamation/quotation mark, comma,apostrophe, close parenthese)?
out-of-vocabulary: this generates two scores,the number of out-of-vocabulary words in thesentence, and the same one but normalized bythe length of the sentence.
These scores areused for both sides.This leads to 49 features.
A few ones are equiv-alent to or are strongly correlated to baseline ones.As we want to be able to integrate several sets of fea-tures without prior knowledge, our system is able todiscard redundant features (see Section 6).5 RegressionOur system predicts a score between 1 and 5 for eachtest sentence.
For that, we used the training corpusto perform the linear regression of the input featuresagainst scores given by humans.
We used SVM al-gorithm to perform this regression (LibSVM toolkit(Chang and Lin, 2011)).
We experimented two ker-nels: linear, and radial basis function.
For the radialbasis function, we used grid search to optimise pa-rameters.6 Feature SelectionWe experimented with many features.
Some of themmay be very poor predictors.
Then, these featuresmay disturb the convergence of the training algo-rithm of SVM.
To prevent this drawback, we appliedan in-house feature selection algorithm.
A featureselection algorithm selects the most relevant featuresby maximizing a criterion.
Feature selection algo-rithms can be divided into two classes: backwardand forward (Guyon and Elisseeff, 2003).
Backwardalgorithms remove useless features from a set.
For-ward algorithms start with an empty feature set andinsert useful features.
We implemented a greedybackward elimination algorithm for feature selec-tion.
It discards features until a quality criterionstops to decrease.
The criterion used is the Mean Av-erage Error (MAE) calculated on the developmentcorpus:MAE(s, r) =?ni=1 |si ?
ri|n(2)where s is the list of scores predicted by the sys-tem, r is the list of scores given by experts, n is thesize of these lists.The algorithm is described below:116Algorithm 1: Feature Selection algorithmbeginStart with a set S of featureswhile two features in S are linearlycorrelated (more than 0.999) dodiscard one of them from SCalculate MAE for SrepeatDecreaseMax?
0forall the feature f ?
S doS?
?
S \ fCalculate newMAE for S?if MAE-newMAE>DecreaseMax thenDecreaseMax?MAE-newMAEfchosen?
fif DecreaseMax> 0 thenS ?
S\ fchosenMAE?
MAE-DecreaseMaxuntil DecreaseMax=0;For calculating the MAE for a feature set, severalsteps are necessary: performing the regression be-tween the features and the expert scores on the train-ing corpus, using this regression to predict the scoreson the development corpus, calculate the MAE be-tween the predicted scores and the expert scores onthis development corpus.7 ResultsWe used the data provided by the shared taskon Quality Estimation2, without additional corpus.This data is composed of a parallel English-Spanishtraining corpus.
This corpus is made of the con-catenation of europarl-v5 and news-commentary10corpora (from WMT-2010), followed by tokeniza-tion, cleaning (sentences with more than 80 tokensremoved) and truecasing.
It has been used for base-line models provided in the baseline package by theshared task organizers.
We used the same train-ing corpus to train additional language models (for-ward and backward 5-gram with kneyser-ney dis-counting, obtained with the SRILM toolkit) and trig-gers required for our features.
For feature extrac-2http://dl.dropbox.com/u/6447503/resources.tbztion, we used the files provided by the organizers:1832 source english sentences, their translations bythe baseline translation system, and the score givenby humans to these translations.
We split these filesinto a training part (1000 sentences) and a develop-ment part (832 sentences).
We used the train partto perform the regression between the features andthe scores.
We used the development corpus to opti-mise the parameters of the regression and for featureselection.
We did not use additional provided infor-mation such as phrase alignment, word alignment,word graph, etc.Table 1 presents our results in terms of MAEand Root Mean Squared Error (RMSE).
MAE is de-scribed in Formula 2, and RMSE is defined by:RMSE(s, r) =?
?ni=1(si ?
ri)2n(3)Each line of Table 1 gives the performance for aset of features.
BASELINE+LORIA constitutes theunion of both features BASELINE (Section 3) andLORIA (Section 4).
the ?feature selection?
columnindicates if feature selection algorithm is applied.We experimented the SVM with two kernels: lin-ear (LIN in Table 1) and radial basis function (RBFin Table 1).
As the radial basis function uses pa-rameters, we proposed results with default values(DEF) and with values optimised by grid search onthe development corpus (OPT).
MAE and RMSE aregiven for development corpus and for the test cor-pus.
This test corpus (and its reference scores givenby humans) is the one released for the shared 2012task3.
MAE and RMSE has been computed againstthe scores given by humans to the translations in thistest corpus4.The results show that the performance on devel-opment corpus are always confirmed by those of thetest corpus.
The BASELINE features alone achievealready good performance, better than ours.
Al-though the differences are well inside the confidenceinterval, the fusion of both sets outperforms slightlythe BASELINE.
The feature selection algorithm al-lows to gain 0.01 point.
The gain is the same for3https://github.com/lspecia/QualityEstimation/blob/master/test set.tar.gz4available at https://github.com/lspecia/QualityEstimation/blob/master/test set.likert117the optimisation of the radial basis function param-eters.
Surprisingly, the linear kernel, simpler thanother kernels, yields the same performance as radialbasis function.In addition to MAE and RMSE results, we stud-ied the linear correlations between features: our ob-jective is to check if BASELINE and LORIA com-plement each other.
We computed the linear cor-relation between all features (BASELINE+LORIA).This leads to 2145 values.
Table 2 shows in line +/-the number of features pairs which correlate with anabsolute score higher than thresholds 0.9, 0.8 or 0.7.Among these pairs we give in line + the number ofpairs with positive correlation, and in line - the num-ber of pairs with negative correlation.
For lines +and -, we give 4 numbers: number of pairs, num-ber of LORIA-LORIA (e.g.
the number of correla-tions between a LORIA feature and another LORIAfeature) pairs, number of BASELINE-BASELINEpairs, number of LORIA-BASELINE pairs.
We re-mark that only 6% of the pairs correlates (column0.7, line +/-) and that the correlations are mostly be-tween LORIA features.
This last point is not sur-prising because there are more LORIA features thanBASELINE ones.
There are very few correlationsbetween LORIA and BASELINE features.
We stud-ied precisely the correlated pairs.
There is a strong(more than 0.9) positive correlation between n-gramand backoff based features and their averaged fea-ture versions.
Sometimes, there is also a strong cor-relation between ?forward?
and ?backward?
features.Source and target sentences lengths linearly corre-late (0.98).
This is the same case for source and tar-get language model likelihoods.
There is also a highcorrelation between forward and backward 5-gramscores (0.89).
There are very few negative correla-tions between features.
As they are not numerous,one can list these pairs with correlation between -1 and -0.7: target sentence length and target lan-guage model probability; source sentence length andsource language model probability; ratio of OOVwords over sentence length in source sentence andpercentage of unigrams in the source sentence seenin the SMT training corpus; and number of OOVwords in source sentence and percentage of uni-grams in the source sentence seen in the SMT train-ing corpus.
These correlations are not surprising.First, language model probability is not normalized?
0.9 ?
0.8 ?
0.7+/- 64 103 127+ 56/49/3/4 94/87/3/4 117/105/6/6- 8/0/4/4 9/0/4/5 10/0/4/6Table 2: Statistics on the linear correlations between LO-RIA+BASELINE featuresby the number of tokens: the more tokens, the lowerprobability.
Second, the more OOV in the sentence,the fewer known unigrams.Last, we present the set of features discarded byour feature selection algorithm.
We give only thisdescription for the LORIA+BASELINE set, withlinear kernel.
The algorithm discards 18 LORIAfeatures out of 49 (37%) and 3 BASELINE out of17 (18%).
The features discarded from LORIA aremostly averaged features based on n-gram and back-off.
This is consistent with the fact that these fea-tures are strongly correlated with n-gram and back-off features.
We remark that very few BASELINEfeatures are discarded: lengths of source and targetlanguage because these features are yet included inLORIA features, and ?average number of transla-tions per source word in the sentence?
maybe be-cause the LORIA feature giving the average IBM1probabilities is more precise.
Last, we remark thatthe target length feature is discarded, and only ratiobetween target and source length is kept.8 ConclusionIn this paper, we present our system to evaluate thequality of machine translated sentences.
A sentenceis given a score between 1 and 5.
This score is pre-dicted using a machine learning approach.
We usethe training data provided by the organizers to per-form the regression between numerical features cal-culated from source and target sentences and scoresgiven by human experts.
The features are the base-line ones provided by the organizers and our ownfeatures.
We proposed a feature selection algorithmto discard useless features.
Our results show thatbaseline features contain already the main part of in-formation for prediction.
Concerning our own fea-tures, a study of the linear correlations shows thataveraged features do not provide new informationcompared to n-gram and backoff features.
This last118Dev TestSet of features feature kernel MAE RMSE MAE RMSEselectionBASELINE no RBF DEF 0.63 0.79 0.69 0.83LORIA no RBF DEF 0.66 0.82 0.73 0.87BASELINE+LORIA no RBF DEF 0.62 0.78 0.69 0.82BASELINE+LORIA yes RBF DEF 0.61 0.77 0.69 0.83BASELINE+LORIA no RBF OPT 0.62 0.77 0.68 0.82BASELINE+LORIA no LIN 0.62 0.78 0.69 0.83BASELINE+LORIA yes LIN 0.61 0.77 0.68 0.82Table 1: Results of the various sets of features in terms of MAE and RMSEremark is confirmed by our feature selection algo-rithm.
Our feature selection algorithm seems to dis-card features linearly correlated with others whilekeeping relevant features for prediction.
Last, weremark that the choice of kernel, optimisation of pa-rameters and feature selection have not a strong ef-fect on performance.
The main effort may have tobe concentrated on features in the future.ReferencesC.-C. Chang and C.-J.
Lin.
2011.
LIBSVM:A library for support vector machines.
ACMTransactions on Intelligent Systems and Tech-nology, 2:27:1?27:27.
Software available athttp://www.csie.ntu.edu.tw/ cjlin/libsvm.J.
Duchateau, K. Demuynck, and P. Wambacq.
2002.Confidence scoring based on backward language mod-els.
In Proceedings of IEEE International Confer-ence on Acoustics, Speech, and Signal Processing, vol-ume 1, pages 221?224.I.
Guyon and A. Elisseeff.
2003.
An introduction to vari-able and feature selection.
Journal of Machine Learn-ing Research (Special Issue on Variable and FeatureSelection), pages 1157?1182.Y.
He, Y. Ma, J. van Genabith, and A.
Way.
2010.
Bridg-ing SMT and TM with translation recommendation.
InProceedings of the 48th Annual Meeting of the Associ-ation for Computational Linguistics, pages 622?630.C.
Latiri, K.
Sma?
?li, C. Lavecchia, C. Nasri, and D. Lan-glois.
2011.
Phrase-based machine translation basedon text mining and statistical language modeling tech-niques.
In Proceedings of the 12th International Con-ference on Intelligent Text Processing and Computa-tional Linguistics.C.
Lavecchia, K.
Sma?
?li, and D. Langlois.
2008.
Dis-covering phrases in machine translation by simulatedannealing.
In Proceedings of the Eleventh InterspeechConference.S.
Raybaud, C. Lavecchia, D. Langlois, and K.
Sma??li.2009a.
New confidence measures for statistical ma-chine translation.
In Proceedings of the Interna-tional Conference on Agents and Artificial Intelli-gence, pages 61?68.S.
Raybaud, C. Lavecchia, D. Langlois, and K.
Sma??li.2009b.
Word- and sentence-level confidence measuresfor machine translation.
In Proceedings of the 13thAnnual Conference of the European Association forMachine Translation, pages 104?111.S.
Raybaud, D. Langlois, and K.
Sma??li.
2011.
?Thissentence is wrong.?
Detecting errors in machine-translated sentences.
Machine Translation, 25(1):1?34.M.
Simard, N. Ueffing, P. Isabelle, and R. Kuhn.
2007.Rule-based translation with statistical phrase-basedpost-editing.
In Proceedings of the ACL-2007 Work-shop on Statistical Machine Translation (WMT-07),pages 203?206.R.
Soricut and A. Echihabi.
2010.
Trustrank: Inducingtrust in automatic translations via ranking.
In Proceed-ings of the 48th Annual Meeting of the Association forComputational Linguistics, pages 612?621.L.
Specia, N. Hajlaoui, C. Hallett, and W. Aziz.
2010.Predicting machine translation adequacy.
In Proceed-ings of the Machine Translation Summit XIII, pages612?621.L.
Specia.
2011.
Exploiting objective annotations formeasuring translation post-editing effort.
In Proceed-ings of the 15th Conference of the European Associa-tion for Machine Translation, pages 73?80.C.
Uhrik and W. Ward.
1997.
Confidence metrics basedon n-gram language model backoff behaviors.
In FifthEuropean Conference on Speech Communication andTechnology, pages 2771?2774.119
