Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1443?1448,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsWhere Not to Eat?
Improving Public Policy by Predicting HygieneInspections Using Online ReviewsJun Seok Kang?
Polina Kuznetsova?
?Department of Computer ScienceStony Brook UniversityStony Brook, NY 11794-4400{junkang,pkuznetsova,ychoi}@cs.stonybrook.eduMichael Luca?
Yejin Choi?
?Harvard Business SchoolSoldiers Field RoadBoston, MA 02163mluca@hbs.eduAbstractThis paper offers an approach for governmentsto harness the information contained in socialmedia in order to make public inspections anddisclosure more efficient.
As a case study, weturn to restaurant hygiene inspections ?
whichare done for restaurants throughout the UnitedStates and in most of the world and are a fre-quently cited example of public inspectionsand disclosure.
We present the first empiri-cal study that shows the viability of statisticalmodels that learn the mapping between tex-tual signals in restaurant reviews and the hy-giene inspection records from the Departmentof Public Health.
The learned model achievesover 82% accuracy in discriminating severeoffenders from places with no violation, andprovides insights into salient cues in reviewsthat are indicative of the restaurant?s sanitaryconditions.
Our study suggests that publicdisclosure policy can be improved by miningpublic opinions from social media to target in-spections and to provide alternative forms ofdisclosure to customers.1 IntroductionPublic health inspection records help customers tobe wary of restaurants that have violated healthcodes.
In some counties and cities, e.g., LA, NYC,it is required for restaurants to post their inspec-tion grades at their premises, which have shownto affect the revenue of the business substantially(e.g., Jin and Leslie (2005), Henson et al(2006)),thereby motivating restaurants to improve their sani-tary practice.
Other studies have reported correlationbetween the frequency of unannounced inspectionsper year, and the average violation scores, confirm-ing the regulatory role of inspections in improvingthe hygiene quality of the restaurants and decreasingfood-borne illness risks (e.g., Jin and Leslie (2003),Jin and Leslie (2009), Filion and Powell (2009),NYC-DoHMH (2012)).However, one practical challenge in the currentinspection system is that the department of healthhas only limited resources to dispatch inspectors,leaving out a large number of restaurants with un-known hygiene grades.
We postulate that online re-views written by the very citizens who have visitedthose restaurants can serve as a proxy for predictingthe likely outcome of the health inspection of anygiven restaurant.
Such a prediction model can com-plement the current inspection system by enlight-ening the department of health to make a more in-formed decision when allocating inspectors, and byguiding customers when choosing restaurants.Our work shares the spirit of recently emergingstudies that explores social media analysis for pub-lic health surveillance, in particular, monitoring in-fluenza or food-poisoning outbreaks from micro-blogs (e.g., Aramaki et al(2011), Sadilek et al(2012b), Sadilek et al(2012a), Sadilek et al(2013),Lamb et al(2013), Dredze et al(2013), von Etteret al(2010)).
However, no prior work has examinedthe utility of review analysis as a predictive tool foraccessing hygiene of restaurants, perhaps becausethe connection is not entirely conspicuous: after all,customers are neither familiar with inspection codes,nor have the full access to the kitchen, nor have beenasked to report on the hygiene aspects of their expe-1443review count*review count (filtered)*Coefficient0.050.10(a)0 10 20 30 40 50np review count*np review count (filtered)*0.050.10(b)0 10 20 30 40 50avg review rating*avg review rating (filtered)*Coefficient?0.05?0.03(c)0 10 20 30 40 50avg review length*avg review length(filtered)*00.05(d)0 10 20 30 40 50Inspection Penalty Score ThresholdFigure 1: Spearman?s coefficients of factors & inspectionpenalty scores.
?*?
: statistically significant (p ?
0.05)rience.In this work, we report the first empirical studydemonstrating the utility of review analysis for pre-dicting health inspections, achieving over 82% accu-racy in discriminating severe offenders from placeswith no violation, and find predictive cues in reviewsthat correlate with the inspection results.2 DataWe scraped entire reviews written for restaurants inSeattle from Yelp over the period of 2006 to 2013.1The inspection records of Seattle is publicly avail-able at www.datakc.org.
More than 50% of therestaurants listed under Yelp did not have inspectionrecords, implying the limited coverage of inspec-tions.
We converted street addresses into canonicalforms when matching restaurants between Yelp andinspection database.
After integrating reviews withinspection records, we obtained about 13k inspec-1Available at http://www.cs.stonybrook.edu/?junkang/hygiene/bimodality*bimodality (filtered)*Coefficient00.050.10(a)0 10 20 30 40 50fake review count*fake review count (filtered)00.050.10(b)0 10 20 30 40 50Inspection Penalty Score ThresholdFigure 2: Spearman?s coefficients of factors & inspectionpenalty scores.
?*?
: statistically significant (p ?
0.05)tions over 1,756 restaurants with 152k reviews.
Foreach restaurant, there are typically several inspec-tion records.
We defined an ?inspection period?
ofeach inspection record as the period of time start-ing from the day after the previous inspection to theday of the current inspection.
If there is no previ-ous inspection, then the period stretches to the past6 months in time.
Each inspection period corre-sponds to an instance in the training or test set.
Wemerge all reviews within an inspection period intoone document when creating the feature vector.Note that non-zero penalty scores may not nec-essarily indicate alarming hygiene issues.
For ex-ample, violating codes such as ?proper labeling?
or?proper consumer advisory posted for raw or under-cooked foods?
seem relatively minor, and unlikely tobe noted and mentioned by reviewers.
Therefore, wefocus on restaurants with severe violations, as theyare exactly the set of restaurants that inspectors andcustomers need to pay the most attention to.
To de-fine restaurants with ?severe violations?
we experi-ment with a varying threshold t, such that restaurantswith score ?
t are labeled as ?unhygienic?.23 Correlates of Inspection Penalty ScoresWe examine correlation between penalty scores andseveral statistics of reviews:I.
Volume of Reviews:2For restaurants with ?hygienic?
labels, we only considerthose without violation, as there are enough number of suchrestaurants to keep balanced distribution between two classes.144461.42 61.46 66.6170.83 77.1681.37Accuracy (%)6080Inspection Penalty Score Threshold0 10 20 30 40 50Figure 3: Trend of penalty score thresholds & accuracies.?
count of all reviews?
average length of all reviewsII.
Sentiment of Reviews: We examine whetherthe overall sentiment of the customers correlateswith the hygiene of the restaurants based on follow-ing measures:?
average review rating?
count of negative (?
3) reviewsIII.
Deceptiveness of Reviews: Restaurants withbad hygiene status are more likely to attract negativereviews, which would then motivate the restaurantsto solicit fake reviews.
But it is also possible thatsome of the most assiduous restaurants that abideby health codes strictly are also diligent in solicit-ing fake positive reviews.
We therefore examine thecorrelation between hygiene violations and the de-gree of deception as follows.?
bimodal distribution of review ratingsThe work of Feng et al(2012) has shownthat the shape of the distribution of opinions,overtly skewed bimodal distributions in partic-ular, can be a telltale sign of deceptive review-ing activities.
We approximately measure thisby computing the variance of review ratings.?
volume of deceptive reviews based on linguisticpatternsWe also explore the use of deception classifiersbased on linguistic patterns (Ott et al 2011)to measure the degree of deception.
Since nodeception corpus is available in the restaurantdomain, we collected a set of fake reviews andtruthful reviews (250 reviews for each class),following Ott et al(2011).3310 fold cross validation on this dataset yields 79.2% accu-racy based on unigram and bigram features.Features Acc.
MSE SCC- *50.00 0.500 -review count *50.00 0.489 0.0005np review count *52.94 0.522 0.0017cuisine *66.18 0.227 0.1530zip code *67.32 0.209 0.1669avrg.
rating *57.52 0.248 0.0091inspection history *72.22 0.202 0.1961unigram 78.43 0.461 0.1027bigram *76.63 0.476 0.0523unigram + bigram 82.68 0.442 0.0979all 81.37 0.190 0.2642Table 1: Feature Compositions & Respective Accuracies,Respective Mean Squared Errors(MSE) & Squared Cor-relation Coefficients (SCC), np=non-positiveFiltering Reviews: When computing above statis-tics over the set of reviews corresponding to eachrestaurant, we also consider removing a subset of re-views that might be dubious or just noise.
In partic-ular, we remove reviews that are too far away (delta?
2) from the average review rating.
Another filter-ing rule can be removing all reviews that are clas-sified as deceptive by the deception classifier ex-plained above.
For brevity, we only show resultsbased on the first filtering rule, as we did not findnotable differences in different filtering strategies.Results: Fig 1 and 2 show Spearman?s rank corre-lation coefficient with respect to the statistics listedabove, with and without filtering, computed at dif-ferent threshold cutoffs ?
{0, 10, 20, 30, 40, 50} ofinspection scores.
Although coefficients are notstrong,4 they are mostly statistically significant withp ?
0.05 (marked with ?*?
), and show interestingcontrastive trends as highlighted below.In Fig 1, as expected, average review rating is neg-atively correlated with the inspection penalty scores.Interestingly, all three statistics corresponding to thevolume of customer reviews are positively corre-lated with inspection penalty.
What is more inter-esting is that if potentially deceptive reviews are fil-tered, then the correlation gets stronger, which sug-gests the existence of deceptive reviews covering upunhappy customers.
Also notice that correlation is4Spearman?s coefficient assumes monotonic correlation.
Wesuspect that the actual correlation of these factors and inspectionscores are not entirely monotonic.1445Hygienic gross, mess, sticky, smell, restroom, dirtyBasic Ingredients: beef, pork, noodle, egg, soy,ramen, pho,Cuisines Vietnamese, Dim Sum, Thai, Mexican,Japanese, Chinese, American, Pizza, Sushi, Indian,Italian, AsianSentiment: cheap, never,Service & Atmosphere cash, worth, district, delivery,think, really, thing, parking, always, usually, definitely- door: ?The wait is always out the door when Iactually want to go there?,- sticker: ?I had sticker shock when I saw the prices.
?,- student: ?heap, large portions and tasty = the perfectstudent food!
?,- the size: ?i was pretty astonished at the size of all theplates for the money.
?,- was dry: ?The beef was dry, the sweet soy andanise-like sauce was TOO salty (almost inedible).
?,- pool: ?There are pool tables, TV airing soccer gamesfrom around the globe and of course - great drinks!
?Table 2: Lexical Cues & Examples - Unhygienic (dirty)generally stronger when higher cutoffs are used (x-axis), as expected.
Fig 2 looks at the relation be-tween the deception level and the inspection scoresmore directly.
As suspected, restaurants with highpenalty scores show increased level of deceptive re-views.Although various correlates of hygiene scores ex-amined so far are insightful, these alone are not in-formative enough to be used as a predictive tool,hence we explore content-based classification next.4 Content-based PredictionWe examine the utility of the following features:Features based on customers?
opinion:1.
Aggregated opinion: average review rating2.
Content of the reviews: unigram, bigramFeatures based on restaurant?s metadata:3.
Cuisine: e.g., Thai, Italian, as listed under Yelp4.
Location: first 5 digits of zip code5.
Inspection History: a boolean feature (?hy-gienic?
or ?unhygienic?
), a numerical feature(previous penalty score rescaled ?
[0, 1]), a nu-meric feature (average penalty score over allprevious inspections)Hygienic:Cooking Method & Garnish: brew, frosting, grill,crush, crust, taco, burrito, toastHealthy or Fancier Ingredients: celery, calamity,wine, broccoli, salad, flatbread, olive, pestoCuisines : Breakfast, Fish & Chips, Fast Food,German, Diner, Belgian, European, Sandwiches,VegetarianWhom & When: date, weekend, our, husband,evening, nightSentiment: lovely, yummy, generous, friendly, great,niceService & Atmosphere: selection, attitude,atmosphere, ambiance, pretentiousTable 3: Lexical Cues & Examples - Hygienic (clean)6. Review Count7.
Non-positive Review CountClassification Results We use liblinear?s SVM(Fan et al 2008) with L1 regularization and 10 foldcross validation.
We filter reviews that are fartherthan 2 from the average rating.
We also run Sup-port Vector Regression (SVR) using liblinear.
Fig 3shows the results.
As we increase the threshold, theaccuracy also goes up in most cases.
Table 1 showsfeature ablation at threshold t = 50, and ?*?
denotesstatistically significant (p?0.05) difference over theperformance with all features based on student t-test.We find that metadata information of restaurantssuch as location and cuisine alone show good predic-tive power, both above 66%, which are significantlyhigher than the expected accuracy of random guess-ing (50%).Somewhat unexpected outcome is aggregatedopinion, which is the average review rating duringthe corresponding inspection period, as it performsnot much better than chance (57.52%).
This resultsuggest that the task of hygiene prediction from re-views differs from the task of sentiment classifica-tion of reviews.Interestingly, the inspection history feature aloneis highly informative, reaching accuracy upto 72%,suggesting that the past performance is a good pre-dictor of the future performance.Textual content of the reviews (unigram+bigram)turns out to be the most effective features, reachingupto 82.68% accuracy.
Lastly, when all the features1446are combined together, the performance decreasesslightly to 81.37%, perhaps because n-gram featuresperform drastically better than all others.4.1 Insightful CuesTable 2 and 3 shows representative lexical cues foreach class with example sentences excerpted fromactual reviews when context can be helpful.Hygiene: Interestingly, hygiene related words areoverwhelmingly negative, e.g., ?gross?, ?mess?,?sticky?.
What this suggests is that reviewers docomplain when the restaurants are noticeably dirty,but do not seem to feel the need to complement oncleanliness as often.
Instead, they seem to focus onother positive aspects of their experience, e.g., de-tails of food, atmosphere, and their social occasions.Service and Atmosphere: Discriminative fea-tures reveal that it is not just the hygiene relatedwords that are predictive of the inspection results ofrestaurants.
It turns out that there are other quali-ties of restaurants, such as service and atmosphere,that also correlate with the likely outcome of inspec-tions.
For example, when reviewers feel the needto talk about ?door?, ?student?, ?sticker?, or ?thesize?
(see Table 2 and 3), one can extrapolate thatthe overall experience probably was not glorious.
Incontrast, words such as ?selection?, ?atmosphere?,?ambiance?
are predictive of hygienic restaurants,even including those with slightly negative connota-tion such as ?attitude?
or ?pretentious?.Whom and When: If reviewers talk about detailsof their social occasions such as ?date?, ?husband?,it seems to be a good sign.The way food items are described: Another in-teresting aspect of discriminative words are the wayfood items are described by reviewers.
In general,mentions of basic ingredients of dishes, e.g., ?noo-dle?, ?egg?, ?soy?
do not seem like a good sign.
Incontrast, words that help describing the way dish isprepared or decorated, e.g., ?grill?, ?toast?, ?frost-ing?, ?bento box?
?sugar?
(as in ?sugar coated?
)are good signs of satisfied customers.Cuisines: Finally, cuisines have clear correlationswith inspection outcome, as shown in Table 2 and 3.5 Related WorkThere have been several recent studies that probe theviability of public health surveillance by measuringrelevant textual signals in social media, in particu-lar, micro-blogs (e.g., Aramaki et al(2011), Sadileket al(2012b), Sadilek et al(2012a), Sadilek et al(2013), Lamb et al(2013), Dredze et al(2013), vonEtter et al(2010)).
Our work joins this line of re-search but differs in two distinct ways.
First, mostprior work aims to monitor a specific illness, e.g.,influenza or food-poisoning by paying attention toa relatively small set of keywords that are directlyrelevant to the corresponding sickness.
In contrast,we examine all words people use in online reviews,and draw insights on correlating terms and conceptsthat may not seem immediately relevant to the hy-giene status of restaurants, but nonetheless are pre-dictive of the outcome of the inspections.
Second,our work is the first to examine online reviews in thecontext of improving public policy, suggesting addi-tional source of information for public policy mak-ers to pay attention to.Our work draws from the rich body of researchthat studies online reviews for sentiment analysis(e.g., Pang and Lee (2008)) and deception detec-tion (e.g., Mihalcea and Strapparava (2009), Ott etal.
(2011), Feng et al(2012)), while introducingthe new task of public hygiene prediction.
We ex-pect that previous studies for aspect-based sentimentanalysis (e.g., Titov and McDonald (2008), Brodyand Elhadad (2010), Wang et al(2010)) would be afruitful venue for further investigation.6 ConclusionWe have reported the first empirical study demon-strating the promise of review analysis for predictinghealth inspections, introducing a task that has poten-tially significant societal benefits, while being rele-vant to much research in NLP for opinion analysisbased on customer reviews.AcknowledgmentsThis research was supported in part by the StonyBrook University Office of the Vice President forResearch, and in part by gift from Google.
We thankanonymous reviewers and Adam Sadilek for helpfulcomments and suggestions.1447ReferencesEiji Aramaki, Sachiko Maskawa, and Mizuki Morita.2011.
Twitter catches the flu: Detecting influenza epi-demics using twitter.
In Proceedings of the 2011 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1568?1576, Edinburgh, Scotland,UK., July.
Association for Computational Linguistics.Samuel Brody and Noemie Elhadad.
2010.
An unsu-pervised aspect-sentiment model for online reviews.In Human Language Technologies: The 2010 AnnualConference of the North American Chapter of theAssociation for Computational Linguistics, HLT ?10,pages 804?812, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Mark Dredze, Michael J. Paul, Shane Bergsma, and HieuTran.
2013.
Carmen: A twitter geolocation systemwith applications to public health.
In AAAI Workshopon Expanding the Boundaries of Health InformaticsUsing AI (HIAI).Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
Liblinear: A libraryfor large linear classification.
The Journal of MachineLearning Research, 9:1871?1874.Song Feng, Longfei Xing, Anupam Gogar, and YejinChoi.
2012.
Distributional footprints of deceptiveproduct reviews.
In ICWSM.Katie Filion and Douglas A Powell.
2009.
The use ofrestaurant inspection disclosure systems as a means ofcommunicating food safety information.
Journal ofFoodservice, 20(6):287?297.Spencer Henson, Shannon Majowicz, Oliver Masakure,Paul Sockett, Anria Johnes, Robert Hart, Debora Carr,and Lewinda Knowles.
2006.
Consumer assessmentof the safety of restaurants: The role of inspectionnotices and other information cues.
Journal of FoodSafety, 26(4):275?301.Ginger Zhe Jin and Phillip Leslie.
2003.
The effect ofinformation on product quality: Evidence from restau-rant hygiene grade cards.
The Quarterly Journal ofEconomics, 118(2):409?451.Ginger Zhe Jin and Phillip Leslie.
2005.
The case insupport of restaurant hygiene grade cards.Ginger Zhe Jin and Phillip Leslie.
2009.
Reputationalincentives for restaurant hygiene.
American EconomicJournal: Microeconomics, pages 237?267.Alex Lamb, Michael J. Paul, and Mark Dredze.
2013.Separating fact from fear: Tracking flu infections ontwitter.
In the North American Chapter of the Associa-tion for Computational Linguistics: Human LanguageTechnologies (NAACL-HLT).Rada Mihalcea and Carlo Strapparava.
2009.
The liedetector: Explorations in the automatic recognitionof deceptive language.
In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 309?312, Suntec, Singapore, August.
Association for Com-putational Linguistics.NYC-DoHMH.
2012.
Restaurant grading in new yorkcity at 18 months.
New York City Department ofHealth and Mental Hygiene.Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T. Han-cock.
2011.
Finding deceptive opinion spam by anystretch of the imagination.
In Proceedings of the 49thAnnual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages309?319, Portland, Oregon, USA, June.
Associationfor Computational Linguistics.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and trends in infor-mation retrieval, 2(1-2):1?135.Adam Sadilek, Henry Kautz, and Vincent Silenzio.2012a.
Predicting disease transmission from geo-tagged micro-blog data.
In Twenty-Sixth AAAI Con-ference on Artificial Intelligence.Adam Sadilek, Henry A. Kautz, and Vincent Silenzio.2012b.
Modeling spread of disease from social in-teractions.
In John G. Breslin, Nicole B. Ellison,James G. Shanahan, and Zeynep Tufekci, editors,ICWSM.
The AAAI Press.Adam Sadilek, Sean Brennan, Henry Kautz, and VincentSilenzio.
2013. nemesis: Which restaurants shouldyou avoid today?
First AAAI Conference on HumanComputation and Crowdsourcing.Ivan Titov and Ryan McDonald.
2008.
A joint modelof text and aspect ratings for sentiment summariza-tion.
In Proceedings of ACL-08: HLT, pages 308?316,Columbus, Ohio, June.
Association for ComputationalLinguistics.Peter von Etter, Silja Huttunen, Arto Vihavainen, MattiVuorinen, and Roman Yangarber.
2010.
Assess-ment of utility in web mining for the domain of pub-lic health.
In Proceedings of the NAACL HLT 2010Second Louhi Workshop on Text and Data Mining ofHealth Documents, pages 29?37, Los Angeles, Cal-ifornia, USA, June.
Association for ComputationalLinguistics.Hongning Wang, Yue Lu, and Chengxiang Zhai.
2010.Latent aspect rating analysis on review text data: a rat-ing regression approach.
In Proceedings of the 16thACM SIGKDD international conference on Knowl-edge discovery and data mining, pages 783?792.ACM.1448
