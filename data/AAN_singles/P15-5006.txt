Proceedings of the Tutorials of the 53rd Annual Meeting of the ACL and the 7th IJCNLP, pages 19?20,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsScalable Large-Margin Structured Learning:Theory and AlgorithmsLiang Huang Kai ZhaoThe City University of New York{liang.huang.sh, kzhao.hf, lemaoliu}@gmail.com1 MotivationsMuch of NLP tries to map structured input (sen-tences) to some form of structured output (tag se-quences, parse trees, semantic graphs, or trans-lated/paraphrased/compressed sentences).
Thusstructured prediction and its learning algorithmare of central importance to us NLP researchers.However, when applying machine learning tostructured domains, we often face scalability is-sues for two reasons:1.
Even the fastest exact search algorithms formost NLP problems (such as parsing andtranslation) is too slow for repeated use on thetraining data, but approximate search (suchas beam search) unfortunately breaks downthe nice theoretical properties (such as con-vergence) of existing machine learning algo-rithms.2.
Even with inexact search, the scale of thetraining data in NLP still makes pure onlinelearning (such as perceptron and MIRA) tooslow on a single CPU.This tutorial reviews recent advances that ad-dress these two challenges.
In particular, we willcover principled machine learning methods thatare designed to work under vastly inexact search,and parallelization algorithms that speed up learn-ing on multiple CPUs.
We will also extend struc-tured learning to the latent variable setting, wherein many NLP applications such as translation andsemantic parsing the gold-standard derivation ishidden.2 Contents1.
Overview of Structured Learning(a) key challenge 1: search efficiency(b) key challenge 2: interactions betweensearch and learning2.
Structured Perceptron(a) the basic algorithm(b) convergence proof ?
a purely geometricapproach (updated in 2015)(c) voted and averaged perceptrons, and ef-ficient implementation tricks(d) applications in tagging, parsing, etc.
(e) inseparability and generalizationbounds (new in 2015)3.
Structured Perceptron under Inexact Search(a) convergence theory breaks under inex-act search(b) early update(c) violation-fixing perceptron(d) applications in tagging, parsing, etc.
?coffee break?4.
Large-Margin Structured Learning with La-tent Variables(a) examples: machine translation, seman-tic parsing, transliteration(b) separability condition and convergenceproof (updated in 2015)(c) latent-variable perceptron under inexactsearch(d) applications in machine translation5.
Parallelizing Large-Margin StructuredLearning(a) iterative parameter mixing (IPM)(b) minibatch perceptron and MIRA193 Instructor BiographiesLiang Huang is an Assistant Professor at the CityUniversity of New York (CUNY).
He receivedhis Ph.D. in 2008 from Penn and has workedas a Research Scientist at Google and a Re-search Assistant Professor at USC/ISI.
His workis mainly on the theoretical aspects (algorithmsand formalisms) of computational linguistics, aswell as theory and algorithms of structured learn-ing.
He has received a Best Paper Award at ACL2008, several best paper nominations (ACL 2007,EMNLP 2008, and ACL 2010), two Google Fac-ulty Research Awards (2010 and 2013), and a Uni-versity Graduate Teaching Prize at Penn (2005).He has given three tutorials at COLING 2008,NAACL 2009 and ACL 2014.Kai Zhao is a Ph.D. candidate at the City Univer-sity of New York (CUNY), working with LiangHuang.
He received his B.S.
from the Univer-sity of Science and Technology in China (USTC).He has published on structured prediction, onlinelearning, machine translation, and parsing algo-rithms.
He was a summer intern with IBM TJ Wat-son Research Center in 2013, Microsoft ResearchRedmond in 2014, and Google Research NYC in2015.20
