Proceedings of the Third Workshop on Statistical Machine Translation, pages 175?178,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsCan we relearn an RBMT system?Lo?c Dugast (1,2)dugast@systran.frJean Senellart (1)senellart@systran.frPhilipp Koehn (2)pkoehn@inf.ed.ac.uk(1) SYSTRAN S.A.La Grande Arche1, Parvis de la D?fense92044 ParisLa D?fense CedexFrance(2) School of InformaticsUniversity of Edinburgh2 Buccleuch PlaceEdinburgh, EH8 9LWUnited KingdomAbstractThis paper describes SYSTRAN submissionsfor the shared task of the third Workshop onStatistical Machine Translation at ACL.
Ourmain contribution consists in a French-Englishstatistical model trained without the use of anyhuman-translated parallel corpus.
In substitu-tion, we translated a monolingual corpus withSYSTRAN rule-based translation engine toproduce the parallel corpus.
The results areprovided herein, along with a measure of erroranalysis.1 IntroductionCurrent machine translation systems follow twodifferent lines of research: (1) manually writtenrules associated with bilingual dictionaries (rule-based systems), (2) a statistical framework (statis-tical machine translation) based on large amount ofmonolingual and parallel corpora.
The first lineuses linguistically generalized information basedon what humans understand from what happens ina given language (source and target) and what hap-pens in the translation process.
The translationprocess is building a translation from a givensource sentence based on this knowledge.
The sec-ond line exploits implicit information present inalready translated corpora and more generally anytext production in the target language to automati-cally find the most likely translation for a givensource sentence.
This approach has proven to becompetitive with the rule-based approach whenprovided with enough resources on a specific do-main.
Though based on fundamentally differentparadigms and exploiting different types of infor-mation, these two research lines are not in opposi-tion and may be combined to produce improvedresults.
For instance, serial combination of the twoapproaches has produced very good results inWMT07 (Simard, 2007), (Dugast, 2007) andNIST07 (Ueffing, 2008).
(Schwenk et al, 2008)also combines both approaches and resources tobuild a better system.The SYSTRAN?s R&D team actually works tomerge these two approaches, drawing benefit fromtheir respective strengths.
Initially, the SYSTRANsystem was a pure rule-based system that in recentyears began integrating statistical features and cor-pus-based model (Senellart, 2006).
It must benoted that, for sake of simplification of the ex-periment and its interpretation, the base systemmentioned in this paper is a purely rule-based ver-sion.
In the framework of this research effort, vari-ous exploratory experiments are being run whichaim both at finding efficient combination setupsand at discriminating strengths and weaknesses ofrule-based and statistical systems.We had performed a first analysis on a statisticalpost-editing system (Dugast, 2007).
The systemsubmitted for Czech-English follows this setup.We present also here an original French-Englishstatistical model which doesn?t make use of thetarget side of the parallel data to train its phrase-table, but rather uses the rule-based translation ofthe source side.
We call this system ?SYSTRANRelearnt?
because, as far as the translation modelis concerned, this system is a statistical model ofthe rule-based engine.
In addition to the submittedsystem which only makes use of the Europarlmonolingual data, we present additional results175using unrelated monolingual data in the news do-main.
Though human evaluation of these systemswill provide additional insight, we try here to startanalyzing the specificities of those systems.2 Training without any human referencetranslationIf the need in terms of monolingual corpus tobuild language models can most of the time be ful-filled without much problem, the reliance of statis-tical models on parallel corpora is much moreproblematic.
Work on domain adaptation for statis-tical machine translation (Koehn and Schroeder,2007) tries to bring solutions to this issue.
Statisti-cal Post-Editing may well be another way to per-form efficient domain-adaptation, but still requiresparallel corpora.
We try here to open a new path.Our submitted system for French-English on theEuroparl task is a phrase based system, whosephrase table was trained on the rule based transla-tion of the French Europarl corpus.
The Frenchside of the Europarl parallel corpus was translatedwith the baseline rule-based translation engine toproduce the target side of the training data.
How-ever, the language model was trained on the realEnglish Europarl data provided for the shared task.Training was otherwise performed according tobaseline recommendations.Corpus Size(sentences)Size(words)Parallel FR-EN 0.94 M 21 MMonolingual EN (LM) 1.4 M 38 MTable 1: Corpus sizes for the submitted Eu-roparl-domain translationAn additional (non-submitted) system wastrained using two monolingual news corpora ofapproximately a million sentences.
The Frenchcorpus was built from a leading French newspaper,the English from a leading American newspaper,both of the same year (1995).
In the previousmodel, the English corpus used to train the lan-guage model actually contained the referencetranslations of the source corpus.
This is not thecase here.
As for the previous model, the Frenchcorpus was translated by the rule-based system toproduce the parallel training data, while the Eng-lish corpus was used to train a language model,.This same language model is used in both statisti-cal models: a relearnt system and a baselinephrase-based model whose phrase table was learntfrom the Europarl parallel data.
Both trainings fol-lowed the baseline recommendations of the sharedtask.Corpus Size(sen-tences)Size(words)Parallel FR-EN (Europarlv3)0.94M 21MMonolingual FR (LeMonde 1995)0.96M 18MMonolingual EN (NYT1995)3.8M 19MTable 2: Corpus sizes for the additional model,trained on news domain3 Results for the SYSTRAN-relearnt sys-temsWe provide here results on evaluation metrics,an initial error analysis and results on the addi-tional relearnt model.Table 3 provides metrics results for four differentsystems : purely rule based, purely statistical, andthe relearnt systems: Relearnt-0 is a plain statisti-cal model of systran, while Relearnt uses a realEnglish language model and is tuned on real Eng-lish.Model BLEU(tun-ing,dev2006)BLEU(test, dev-test2006)BaselineSYSTRANn.a.
21.27Relearnt-0, withSYSTRAN EnglishLM, tuned onSYSTRAN English20.54 20.92Relearnt 26.74 26.57Baseline Moses  29.98 29.86Table 3: Results of systems on Europarl task,trained (when relevant) on Europarl-only dataThe score of the Relearnt-0 model is slightlylower than the rule-based original (absence of mor-phological analysis and some non-local ruleswhich failed to be modelled may explain this).
The176use of a real English language model and tuningset gives a more than 5 BLEU points improvement,which is only 3 BLEU points below the Mosesbaseline, which uses the Europarl phrase table.Comparing these three systems may helpus discriminate between the statistical nature of atranslation system and the fact it was trained on therelevant domain.
For this purpose, we defined 11error types and counted occurrences for 100 ran-dom-picked sentences of the devtest2006 test cor-pus for the three following systems : a baselinephrase-based system, a SYSTRAN relearnt phrase-based system and the baseline SYSTRAN rule-based system.
Results are displayed in tables 5.aand 5.b.MC Missing ContentMO Missing OtherTCL Translation Choice (content, lemma)TCI Translation Choice (content, inflection)TCO Translation Choice (other)EWC Extra Word ContentEWO Extra Word OtherUW Unknown wordWOS Word Order, shortWOL Word Order, long (distance>=3 words)PNC PunctuationTable 4 : Short definition of error typesSystem MC MO TCL TCI TCOSYSTRAN 0.02 0.2 1.11 0.14 0.48Relearnt 0.22 0.39 0.77 0.22 0.38Moses 0.35 0.46 0.63 0.27 0.25Table 5.a : Average number of errors/sentenceSystem EWC EW0 UW WOS WOL PNCSYSTRAN 0 0.72 0.06 0.41 0.02 0Relearnt 0.05 0.35 0.09 0.41 0.05 0Moses 0.17 0.4 0.12 0.3 0.08 0.02Table 5.b : Average number of errors/sentenceSuch results lead us to make the following com-ments, regarding the various error types:?
Missing wordsThis type of error seems to be specific to statis-tical systems (counts are close between re-learnt and baseline Moses) .
Although we donot have evidence for that, we guess that it isespecially impairing adequacy when contentwords are concerned.?
Extra wordsObviously, the rule-based output producesmany useless functional words (determiners,prepositions?)
while statistical systems do nothave this problem that much.
However, theymay also produce extra content words..?
Unknown wordsFew words are out of the rule-based dictionar-ies?
vocabulary.
Morphological analysis mayexplain at least part of this.?
Translation choiceTranslation choice is the major strength of thestatistical model.
Note that the relearnt systemgains a great deal of the difference betweenSystran and Moses in this category.
We wouldexpect the remaining difference to requiremore translation choices (which may be learntfrom a parallel corpus).
Inflection errors re-main low for the rule-based system only,thanks to its morphological module.?
Word OrderThe language model couldn?t lower the num-ber of short-distance word-order errors (no dif-ference between SYSTRAN and SYSTRANrelearnt).
Long-distance word order is, as ex-pected, better for the rule-based output, thoughFrench-English is not known to be especiallysensitive to this issue.Additionally, table 6 shows the results of the re-learnt system we trained using only monolingualcorpus.
It performed better than both the europarl-trained phrase-based model and the baseline rule-based engine.
Table 7 shows the three differenttranslations of a same example French sentence.Model BLEU (tuning,nc-dev2007)BLEU (test,nctest2007)SYSTRAN n.a.
21.32Relearnt  22.8 23.15BaselineMoses22.7 22.19Table 6 : Results of systems on News task177SOURCECes politiques sont consid?r?es commeun moyen d'offrir des r?parations pour lesinjustices du pass?
et, plus important, decr?er des mod?les de r?le et de surmon-ter la discrimination restante et peut-?treinvolontaire.SYSTRANThese policies are regarded as a meansof offering repairs for the injustices of thepast and, more important, of creatingmodels of role and of overcoming re-maining and perhaps involuntary dis-crimination.Mosesthese policies are regarded as a way tooffer of repairs for past injustices and ,more important , to create a role modelsand remaining discrimination and per-haps involuntary .Relearntthese policies are regarded as a meansto offer repairs for the past injustices and, more important , creating role modelsand overcome remaining discriminationand perhaps involuntary .REFThese policies are seen as a way of of-fering reparation for past injustices and,more importantly, for creating role mod-els and for overcoming residual and per-haps involuntary discrimination.Table 7 : Example outputs for the news domainmodels (example taken from the nc-test2007 cor-pus)4 ConclusionThe relearnt experiment primary goal was toset-up a comparison between three different sys-tems, with equivalent resources.
This experimentshowed that a statistical translation system may begranted a high BLEU score, even if its translationmodel was not extracted from corpus.
It remainsto be seen how this correlates with human judg-ment (Callison-Burch, 2006), but the detailed erroranalysis we performed already shows improve-ments for important categories of errors.This experiment provided us with some new in-sight on the strengths and weaknesses of rule-based and phrase-based systems.
As an intermedi-ate between a purely corpus-based statistical sys-tem and a rule-based system, this setup couldbenefit from some of the strengths of a phrase-based statistical system, though at the expense ofits known drawbacks.As future work, we may pursue in this directionby exploring the effect of the size of the monolin-gual corpus used for training the translation model.We may also refine the model by using the targetside of the parallel training data when building thelanguage model corpus (to avoid a mismatch ofvocabularies) and also combine such a model withthe translation model(s) trained on whatever paral-lel data is available.
This would then be interestingto compare this strategy with the corpus-based-only strategies that make use of smaller in-domainparallel corpora.ReferencesChris Callison-Burch, Miles Osborne and PhilippKoehn, 2006.
Re-evaluating the Role of Bleu in Ma-chine Translation Research.
In Proceedings ofEACL-2006L.
Dugast, J. Senellart and P. Koehn.
Statistical Post-Editing on SYSTRAN?s Rule-Based Translation Sys-tem.
Proc.
2nd ACL Workshop on Statistical MachineTranslation, pp.
220-223, June 2007.Philipp Koehn &al.
Moses: Open Source Toolkit forStatistical Machine Translation, ACL 2007, demon-stration sessionPhilipp Koehn and Josh Schroeder.
Experiments in Do-main Adaptation for Statistical Machine Translation,ACL Workshop on Statistical Machine Translation2007Holger Schwenk, Jean-Baptiste Fouet and Jean Senel-lart.
First Steps towards a general purposeFrench/English Statistical Machine Translation Sys-tem.
Submitted at the 3rd ACL Workshop on Statisti-cal Machine Translation, 2008Jean Senellart.
2006.
Boosting linguistic rule-based MTsystem with corpus-based approaches.
In Presenta-tion.
GALE PI Meeting, Boston, MAM.
Simard, C. Goutte, and P. Isabelle.
StatisticalPhrase-based Post-Editing.
Proc.
HLT-NAACL, pp.508-515, April 2007.Simard Michel & al.
2007.
Rule-based Translation WithStatistical Phrase-based Post-editing.
In Proceedingsof WMT07Nicola Ueffing, Jens Stephan, Evgeny Matu-sov, Lo?c Dugast, George Foster, Roland Kuhn,Jean Senellart, and Jin Yang  Tighter Integration ofRule-based and Statistical MT in Serial SystemCombination.
Submitted178
