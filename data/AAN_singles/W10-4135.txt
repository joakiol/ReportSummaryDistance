High OOV-Recall Chinese Word SegmenterXiaoming Xu, Muhua Zhu, Xiaoxu Fei, and Jingbo ZhuSchool ofInformation Science and EngineeringNortheastern University{xuxm, zhumh, feixx}@ics.neu.edu.cnzhujingbo@mail.neu.edu.cnAbstractFor the competition of Chinese word seg-mentation held in the first CIPS-SIGHNAjoint conference.
We applied a subword-based word segmenter using CRFs and ex-tended the segmenter with OOV wordsrecognized by Accessor Variety.
More-over, we proposed several post-processingrules to improve the performance.
Oursystem achieved promising OOV recallamong all the participants.1 IntroductionChinese word segmentation is deemed to be a pre-requisite for Chinese language processing.
Thecompetition in the first CIPS-SIGHAN joint con-ference put the task of Chinese word segmenta-tion in a more challengeable setting, where train-ing and test data are obtained from different do-mains.
This setting is widely known as domainadaptation.For domain adaptation, either a large-scale un-labeled target domain data or a small size of la-beled target domain data is required to adapt asystem built on source domain data to the tar-get domain.
In this word segmentation competi-tion, unfortunately, only a small size of unlabeledtarget domain data is available.
Thus we focuson handling out-of-vocabulary (OOV) words.
Forthis purpose, our system is based on a combina-tion of subword-based tagging method (Zhang etal., 2006) and accessor variety-based new wordrecognition method (Feng et al, 2004).
In moredetail, we adopted and extended subword-basedmethod.
Subword list is augmented with new-word list recognized by accessor variety method.Feature Template Descriptiona) cn(?2,?1, 0, 1, 2) unigram of charactersb) cncn+1(?2,?1, 0, 1) bigram of charactersc) cn?1cncn+1(?1, 0, 1) trigram of charactersd) Pu(C0) whether punctuatione) T (C?1)T (C0)T (C+1) type of charactersTable 1: Basic Features for CRF-based SegmenterWe participated in the close track of the wordsegmentation competition, on all the four testdatasets, in two of which our system is ranked atthe 1st position with respect to the metric of OOVrecall.2 System Description2.1 Subword-based Tagging with CRFsThe backbone of our system is a character-basedsegmenter with the application of ConditionalRandom Fields (CRFs) (Zhao and Kit, 2008).
Indetail, we apply a six-tag tagging scheme, as in(Zhao et al, 2006).
That is , each Chinese char-acter can be assigned to one of the tags in {B,B2, B3, M , E, S }.
Refer to (Zhao et al, 2006)for detailed meaning of the tags.
Table 1 showsbasic feature templates used in our system, wherefeature templates a, b, d, e are also used in (Zhu etal., 2006) for SVM-based word segmentation.In order to extend basic CRF-based segmenter,we first collect 2k most frequent words from train-ing data.
Hereafter, the list of such words isreferred to as subword list.
Moreover, single-character words 1, if they are not contained inthe subword list, are also added.
Such proce-1By single-character word, we refer to words that consistsolely of a Chinese character.Feature Template Descriptionf) in(str, subword-list) is str in subword listg) in(str, confident-word-list) is str in confident-wordlistTable 2: Subword Features for CRF-based Seg-menterdure for constructing a subword list is similar tothe one used in (Zhang et al, 2006).
To en-hance the effect of subwords, we go one stepfurther to build a list, named confident-word listhere and below, which contains words that arenot a portion of other words and are never seg-mented in the training data.
In the competition,400 most frequent words in the confident-word listare used.
With subword list and confident-wordlist, both training and test data are segmentedwith forward maximum match method by usingthe union of subword list and confident-word list.Each segmentation unit (single-character or multi-character unit) in the segmentation results are re-garded as ?pseudo character?
and thus can be rep-resented with the basic features in Table 1 andtwo additional features as shown in Table 2.
Seethe details of subword-based Chinese word seg-mentation in (Zhang et al, 2006)2.2 OOV Recognition with Accessor VarietyAccessor variety (AV) (Feng et al, 2004) is a sim-ple and effective unsupervised method for extrac-tion of new Chinese words.
Given a unsegmentedtext, each substring (candidate word) in the textcan be assigned a value according to the follow-ing equation:AV (s) = min{Lav(s), Rav(s)} (1)where the left and right AV values, Lav(s) andRav(s) are defined to be the number of distinctcharacter types appearing on the left and right,respectively.
Candidate words are sorted in thedescending order of AV values and most highlyranked ones can be chosen as new words.
Inpractical applications, heuristic filtering rules aregenerally needed (Feng et al, 2004).
We re-implemented the AV method and filtering rules,as in (Feng et al, 2004).
Moreover, we filter outcandidate words that have AV values less than 3.Unfortunately, candidate word list generated thisway still contains many noisy words (substringsthat are not words).
One possible reason is thatunlabeled data (test data) used in the competitionis extremely small in size.
In order to refine theresults derived from the AV method, we make useof the training data to filter the results from twodifferent perspectives.?
Segment test data with the CRF-based seg-menter described above.
Then we collect(candidate) words that are in the CRF-basedsegmentation results, but not appear in thetraining data.
Such words are called CRF-OOV words hereafter.
We retain the intersec-tion of CRF-OOV words and AV-based re-sults as the set of candidate words to be pro-cessed by the following step.?
Any candidate word in the intersection ofCRF-based and AV-based results will be fil-tered out if they satisfy one of the followingconditions: 1) the candidate word is a part ofsome word in the training data; 2) the candi-date word is formed by connection of consec-utive words in the training data; 3) the candi-date word contains position words, such as?
(up), ?
(down),?
(left),?
(right), etc.Moreover, we take all English words in test dataas OOV words.
A simple heuristic rule is definedfor the purpose of English word recognition: anEnglish word is a consecutive sequence of Englishcharacters and punctuations between two Englishcharacters (including these two characters).We finally add all the OOV words into subwordlist and confident-word list.3 Post-Processing RulesIn the results of subword-based word segmenta-tion with CRFs, we found some errors could becorrected with heuristic rules.
For this purpose,we propose following post-processing rules, forhandling OOV and in-vocabulary (IV) words, re-spectively.3.1 OOV Rules3.1.1 Annotation-Standard IndependentRulesWe assume the phenomena discussed in the fol-lowing are general across all kinds of annotationstandards.
Thus corresponding rules can be ap-plied without considering annotation standards oftraining data.?
A punctuation tends to be a single-characterword.
If a punctation?s previous characterand next character are both Chinese charac-ters, i.e.
not punctuation, digit, or Englishcharacter, we always regard the punctuationas a word.?
Consecutive and identical punctuations tendto be joined together as a word.
For exam-ple, ???
represents a Chinese hyphen whichconsists of three ?-?, and ?!!!?
is used toshow emphasizing.
Inspired by this obser-vations, we would like to unite consecutiveand identical punctuations as a single word.?
When the character ???
appears in the train-ing data, it is generally used as a connec-tions symbol in a foreign person name, suchas ?????
(Saint John)?.
Taking this ob-servation into consideration, we always unitethe character ???
and its previous and nextsegment units into a single word.
A similarrule is designed to unite consecutive digits onthe sides of the symbol ?.
?, ex.
?1.11?.?
We notice that four consecutive characterswhich are in the pattern of AABB generallyform a single word in Chinese, for example?????
(dull)?.
Taking this observationinto account, we always unite consecutivecharacters in the AABB into a single word.3.1.2 Templates with Generalized DigitsWords containing digits generally belong to aopen class, for example, the word ?2012?
(AD2012??
means a date.
Thus CRF-based seg-menter has difficulties in recognizing such wordssince they are frequently OOV words.
To attackthis challenge, we first generalize digits in thetraining data.
In detail, we replaced consecutivedigits with ?*?.
For example, the word ?2012?
?will be transformed into ?*??.
Second, we col-lect word templates which consist of three con-secutive words on condition that at least one ofthe words in a template contains the character ?
*?and that the template appears in the training datamore than 4 times.
For example, we can get atemplate like ?*?
(month) *?(day)?
(publish)?.With such templates, we are able to correct errors,say ?10?
17???
into ?10?
17??
?.3.2 IV RulesWe notice that long words have less ambiguitythan short words in the sense of being words.For example, characters in ?????
?fullof talents)?
always form a word in the trainingdata, whereas ????
have two plausible split-ting forms, as ???
(talent)?
or ??
(people) ?(only)?.
In our system, we collect words that haveat least four characters and filter out words whichbelong to one of following cases: 1) the word isa part of other words; 2) the word consists solelyof punctation and/or digit.
For example, ?????
(materialism)?
and ?????
(120)?
arediscarded, since the former is a substring of theword ??????
(materialist)?
and the latter isa word of digits.
Finally we get a list containingabout 6k words.
If a character sequence in the testdata is a member in the list, it is retained as a wordin the final segmentation results.Another group of IV rules concern charactersequences that have unique splitting in the train-ing data.
For example, ????
(women)?
is al-ways split as ???
(woman) ?
(s)?.
Hereafter,we refer to such character sequences as unique-split-sequence (USS).
In our system, we are con-cerned with UUSs which are composed of lessthan 5 words.
In order to apply UUSs for post-processing, we first collect word sequence of vari-able length (word number) from training data.
Indetail, we collect word sequences of two words,three words, and four words.
Second, word se-quences that have more than one splitting casesin the training data are filtered out.
Third, spacesbetween words are removed to form USSs.
Forexample, the words ???
(woman) ?
(s)?
willform the USS ????
?.
Finally, we search thetest data for each USS.
If the searching succeeds,the USS will be replaced with the correspondingword sequence.4 Evaluation ResultsWe evaluated our Chinese word segmenter in theclose track, in four domain: literature (Lit), com-Domain Basic +OOV +OOV+IVROV RIV F ROV RIV F ROV RIV FLit .643 .946 .927 .652 .947 .929 .648 .952 .934Com .839 .961 .938 .850 .961 .941 .852 .965 .947Med .725 .938 .912 .754 .939 .917 .756 .944 .923Fin .761 .956 .932 .854 .958 .950 .871 .961 .955Table 3: Effectiveness of post-processing rulesputer (Com), medicine (Med) and finance (Fin).The results are depicted in Table 4, where R,P and F refer to Recall, Precision, F measurerespectively, and ROOV and RIV refer to recallof OOV and IV words respectively.
Since OOVwords are the obstacle for practical Chinese wordsegmenters to achieve high accuracy, we have spe-cial interest in the metric of OOV recall.
Wefound that our system achieved high OOV recall2.
Actually, OOV recall of our system in the do-mains of computer and finance are both ranked atthe 1st position among all the participants.
Com-pared with the systems ranked second in thesetwo domains, our system achieved OOV recall.853 vs. .827 and .871 vs. .857 respectively.We also examined the effectiveness of post-processing rules, as shown in Table 3, whereBasic represents the performance achieved be-fore post-processing, +OOV represents the resultsachieved after applying OOV post-processingrules, and +OOV+IV denotes the results achievedafter using all the post-processing rules, includingboth OOV and IV rules.
As the table shows, de-signed post-processing rules can improve both IVand OOV recall significantly.Domain R P F ROOV RIVLit .931 .936 .934 .648 .952Com .948 .945 .947 .853 .965Med .924 .922 .923 .756 .944Fin .953 .956 .955 .871 .961Table 4: Performance of our system in the compe-tition2For the test data from the domain of literature, we actu-ally use combination of our system and forward maximummatch, so we will omit the results on this test dataset in ourdiscussion.5 Conclusions and Future WorkWe proposed an approach to refine new words rec-ognized with the accessor variety method, and in-corporated such words into a subword-based wordsegmenter.
We found that such method couldachieve high OOV recall.
Moreover, we designedeffective post-processing rules to further enhancethe performance of our systems.
Our system fi-nally achieved satisfactory results in the competi-tion.AcknowledgmentsThis work was supported in part by the NationalScience Foundation of China (60873091).ReferencesFeng, Haodi, Kang Chen, Xiaotie Deng, and Weiminzhang.
2004.
Accessor Variety Criteriafor ChineseWord Extraction.
Computational Linguistics 2004,30(1), pages 75-93.Zhang, Ruiqiang, Genichiro Kikui, and EiichiroSumita.
2006.
Subword-based Tagging by Condi-tional Random Fileds for Chinese Word Segmenta-tion.
In Proceedings of HLT-NAACL 2006, pages193-196.Zhao, Hai, Chang-Ning Huang, and Mu Li.
2006.Improved Chinese Word Segmentation System withConditional Random Field.
In Proceedings ofSIGHAN-5 2006, pages 162-165.Zhao, Hai and Chunyu Kit.
2008.
Unsupervised Seg-mentation Helps Supervised Learning of CharacterTagging for Word Segmentation and Named EntityRecognition.
In Proceedings of SIGHAN-6 2008,pages 106-111.Zhu, Muhua, Yiling Wang, Zhenxing Wang, HuizhenWang, and Jingbo Zhu.
2006.
Designing Spe-cial Post-Processing Rules for SVM-based ChineseWord Segmentation.
In Proceedigns of SIGHAN-52006, pages 217-220.
