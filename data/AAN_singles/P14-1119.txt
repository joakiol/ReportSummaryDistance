Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1262?1273,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsAutomatic Keyphrase Extraction: A Survey of the State of the ArtKazi Saidul Hasan and Vincent NgHuman Language Technology Research InstituteUniversity of Texas at DallasRichardson, TX 75083-0688{saidul,vince}@hlt.utdallas.eduAbstractWhile automatic keyphrase extraction hasbeen examined extensively, state-of-the-art performance on this task is still muchlower than that on many core natural lan-guage processing tasks.
We present a sur-vey of the state of the art in automatickeyphrase extraction, examining the majorsources of errors made by existing systemsand discussing the challenges ahead.1 IntroductionAutomatic keyphrase extraction concerns ?the au-tomatic selection of important and topical phrasesfrom the body of a document?
(Turney, 2000).
Inother words, its goal is to extract a set of phrasesthat are related to the main topics discussed in agiven document (Tomokiyo and Hurst, 2003; Liuet al, 2009b; Ding et al, 2011; Zhao et al, 2011).Document keyphrases have enabled fast and ac-curate searching for a given document from a largetext collection, and have exhibited their potentialin improving many natural language processing(NLP) and information retrieval (IR) tasks, suchas text summarization (Zhang et al, 2004), textcategorization (Hulth and Megyesi, 2006), opin-ion mining (Berend, 2011), and document index-ing (Gutwin et al, 1999).Owing to its importance, automatic keyphraseextraction has received a lot of attention.
However,the task is far from being solved: state-of-the-artperformance on keyphrase extraction is still muchlower than that on many core NLP tasks (Liu et al,2010).
Our goal in this paper is to survey the stateof the art in keyphrase extraction, examining themajor sources of errors made by existing systemsand discussing the challenges ahead.2 CorporaAutomatic keyphrase extraction systems havebeen evaluated on corpora from a variety ofsources ranging from long scientific publicationsto short paper abstracts and email messages.
Ta-ble 1 presents a listing of the corpora grouped bytheir sources as well as their statistics.1There areat least four corpus-related factors that affect thedifficulty of keyphrase extraction.Length The difficulty of the task increases withthe length of the input document as longer doc-uments yield more candidate keyphrases (i.e.,phrases that are eligible to be keyphrases (see Sec-tion 3.1)).
For instance, each Inspec abstract hason average 10 annotator-assigned keyphrases and34 candidate keyphrases.
In contrast, a scientificpaper typically has at least 10 keyphrases and hun-dreds of candidate keyphrases, yielding a muchbigger search space (Hasan and Ng, 2010).
Conse-quently, it is harder to extract keyphrases from sci-entific papers, technical reports, and meeting tran-scripts than abstracts, emails, and news articles.Structural consistency In a structured doc-ument, there are certain locations where akeyphrase is most likely to appear.
For instance,most of a scientific paper?s keyphrases should ap-pear in the abstract and the introduction.
Whilestructural information has been exploited to ex-tract keyphrases from scientific papers (e.g., title,section information) (Kim et al, 2013), web pages(e.g., metadata) (Yih et al, 2006), and chats (e.g.,dialogue acts) (Kim and Baldwin, 2012), it is mostuseful when the documents from a source exhibitstructural similarity.
For this reason, structural in-formation is likely to facilitate keyphrase extrac-tion from scientific papers and technical reportsbecause of their standard format (i.e., standardsections such as abstract, introduction, conclusion,etc.).
In contrast, the lack of structural consistencyin other types of structured documents (e.g., webpages, which can be blogs, forums, or reviews)1Many of the publicly available corpora can be foundin http://github.com/snkim/AutomaticKeyphraseExtraction/and http://code.google.com/p/maui-indexer/downloads/list.1262Source Dataset/ContributorStatisticsDocuments Tokens/doc Keys/docPaper abstracts Inspec (Hulth, 2003)?
2,000 <200 10Scientific papersNUS corpus (Nguyen and Kan, 2007)?
211 ?8K 11citeulike.org (Medelyan et al, 2009)?
180 - 5SemEval-2010 (Kim et al, 2010b)?
284 >5K 15Technical reports NZDL (Witten et al, 1999)?
1,800 - -News articlesDUC-2001 (Wan and Xiao, 2008b)?
308 ?900 8Reuters corpus (Hulth and Megyesi, 2006) 12,848 - 6Web pages Yih et al (2006) 828 - -Hammouda et al (2005)?
312 ?500 -Blogs (Grineva et al, 2009) 252 ?1K 8Meeting transcripts ICSI (Liu et al, 2009a) 161 ?1.6K 4Emails Enron corpus (Dredze et al, 2008)?
14,659 - -Live chats Library of Congress (Kim and Baldwin, 2012) 15 - 10Table 1: Evaluation datasets.
Publicly available datasets are marked with an asterisk (?
).may render structural information less useful.Topic change An observation commonly ex-ploited in keyphrase extraction from scientific ar-ticles and news articles is that keyphrases typicallyappear not only at the beginning (Witten et al,1999) but also at the end (Medelyan et al, 2009)of a document.
This observation does not neces-sarily hold for conversational text (e.g., meetings,chats), however.
The reason is simple: in a conver-sation, the topics (i.e., its talking points) change asthe interaction moves forward in time, and so dothe keyphrases associated with a topic.
One wayto address this complication is to detect a topicchange in conversational text (Kim and Baldwin,2012).
However, topic change detection is not al-ways easy: while the topics listed in the form of anagenda at the beginning of formal meeting tran-scripts can be exploited, such clues are absent incasual conversations (e.g., chats).Topic correlation Another observation com-monly exploited in keyphrase extraction fromscientific articles and news articles is that thekeyphrases in a document are typically related toeach other (Turney, 2003; Mihalcea and Tarau,2004).
However, this observation does not nec-essarily hold for informal text (e.g., emails, chats,informal meetings, personal blogs), where peoplecan talk about any number of potentially uncorre-lated topics.
The presence of uncorrelated topicsimplies that it may no longer be possible to exploitrelatedness and therefore increases the difficulty ofkeyphrase extraction.3 Keyphrase Extraction ApproachesA keyphrase extraction system typically operatesin two steps: (1) extracting a list of words/phrasesthat serve as candidate keyphrases using someheuristics (Section 3.1); and (2) determiningwhich of these candidate keyphrases are correctkeyphrases using supervised (Section 3.2) or un-supervised (Section 3.3) approaches.3.1 Selecting Candidate Words and PhrasesAs noted before, a set of phrases and words istypically extracted as candidate keyphrases usingheuristic rules.
These rules are designed to avoidspurious instances and keep the number of candi-dates to a minimum.
Typical heuristics include (1)using a stop word list to remove stop words (Liu etal., 2009b), (2) allowing words with certain part-of-speech tags (e.g., nouns, adjectives, verbs) to becandidate keywords (Mihalcea and Tarau, 2004;Wan and Xiao, 2008b; Liu et al, 2009a), (3) al-lowing n-grams that appear in Wikipedia articletitles to be candidates (Grineva et al, 2009), and(4) extracting n-grams (Witten et al, 1999; Hulth,2003; Medelyan et al, 2009) or noun phrases(Barker and Cornacchia, 2000; Wu et al, 2005)that satisfy pre-defined lexico-syntactic pattern(s)(Nguyen and Phan, 2009).Many of these heuristics have proven effectivewith their high recall in extracting gold keyphrasesfrom various sources.
However, for a long docu-ment, the resulting list of candidates can be long.Consequently, different pruning heuristics havebeen designed to prune candidates that are un-likely to be keyphrases (Huang et al, 2006; Kumarand Srinathan, 2008; El-Beltagy and Rafea, 2009;You et al, 2009; Newman et al, 2012).3.2 Supervised ApproachesResearch on supervised approaches to keyphraseextraction has focused on two issues: task refor-mulation and feature design.12633.2.1 Task ReformulationEarly supervised approaches to keyphrase extrac-tion recast this task as a binary classification prob-lem (Frank et al, 1999; Turney, 1999; Witten etal., 1999; Turney, 2000).
The goal is to train aclassifier on documents annotated with keyphrasesto determine whether a candidate phrase is akeyphrase.
Keyphrases and non-keyphrases areused to generate positive and negative examples,respectively.
Different learning algorithms havebeen used to train this classifier, including na?
?veBayes (Frank et al, 1999; Witten et al, 1999),decision trees (Turney, 1999; Turney, 2000), bag-ging (Hulth, 2003), boosting (Hulth et al, 2001),maximum entropy (Yih et al, 2006; Kim and Kan,2009), multi-layer perceptron (Lopez and Romary,2010), and support vector machines (Jiang et al,2009; Lopez and Romary, 2010).Recasting keyphrase extraction as a classifica-tion problem has its weaknesses, however.
Recallthat the goal of keyphrase extraction is to identifythe most representative phrases for a document.In other words, if a candidate phrase c1is morerepresentative than another candidate phrase c2, c1should be preferred to c2.
Note that a binary clas-sifier classifies each candidate keyphrase indepen-dently of the others, and consequently it does notallow us to determine which candidates are betterthan the others (Hulth, 2004; Wang and Li, 2011).Motivated by this observation, Jiang et al(2009) propose a ranking approach to keyphraseextraction, where the goal is to learn a rankerto rank two candidate keyphrases.
This pairwiseranking approach therefore introduces competi-tion between candidate keyphrases, and has beenshown to significantly outperform KEA (Wittenet al, 1999; Frank et al, 1999), a popular su-pervised baseline that adopts the traditional super-vised classification approach (Song et al, 2003;Kelleher and Luz, 2005).3.2.2 FeaturesThe features commonly used to represent an in-stance for supervised keyphrase extraction can bebroadly divided into two categories.3.2.2.1 Within-Collection FeaturesWithin-collection features are computed basedsolely on the training documents.
These featurescan be further divided into three types.Statistical features are computed based on sta-tistical information gathered from the trainingdocuments.
Three such features have been exten-sively used in supervised approaches.
The firstone, tf*idf (Salton and Buckley, 1988), is com-puted based on candidate frequency in the giventext and inverse document frequency (i.e., numberof other documents where the candidate appears).2The second one, the distance of a phrase, is de-fined as the number of words preceding its firstoccurrence normalized by the number of words inthe document.
Its usefulness stems from the factthat keyphrases tend to appear early in a docu-ment.
The third one, supervised keyphraseness,encodes the number of times a phrase appears asa keyphrase in the training set.
This feature is de-signed based on the assumption that a phrase fre-quently tagged as a keyphrase is more likely to bea keyphrase in an unseen document.
These threefeatures form the feature set of KEA (Witten et al,1999; Frank et al, 1999), and have been shown toperform consistently well on documents from var-ious sources (Yih et al, 2006; Kim et al, 2013).Other statistical features include phrase length andspread (i.e., the number of words between the firstand last occurrences of a phrase in the document).Structural features encode how different in-stances of a candidate keyphrase are located indifferent parts of a document.
A phrase is morelikely to be a keyphrase if it appears in the ab-stract or introduction of a paper or in the metadatasection of a web page.
In fact, features that en-code how frequently a candidate keyphrase occursin various sections of a scientific paper (e.g., in-troduction, conclusion) (Nguyen and Kan, 2007)and those that encode the location of a candidatekeyphrase in a web page (e.g., whether it appearsin the title) (Chen et al, 2005; Yih et al, 2006)have been shown to be useful for the task.Syntactic features encode the syntactic pat-terns of a candidate keyphrase.
For example, acandidate keyphrase has been encoded as (1) aPoS tag sequence, which denotes the sequence ofpart-of-speech tag(s) assigned to its word(s); and(2) a suffix sequence, which is the sequence ofmorphological suffixes of its words (Yih et al,2006; Nguyen and Kan, 2007; Kim and Kan,2009).
However, ablation studies conducted onweb pages (Yih et al, 2006) and scientific articles2A tf*idf-based baseline, where candidate keyphrases areranked and selected according to tf*idf, has been widely usedby both supervised and unsupervised approaches (Zhang etal., 2005; Dredze et al, 2008; Paukkeri et al, 2008; Grinevaet al, 2009).1264(Kim and Kan, 2009) reveal that syntactic featuresare not useful for keyphrase extraction in the pres-ence of other feature types.3.2.2.2 External Resource-Based FeaturesExternal resource-based features are computedbased on information gathered from resourcesother than the training documents, such as lex-ical knowledge bases (e.g., Wikipedia) or theWeb, with the goal of improving keyphrase extrac-tion performance by exploiting external knowl-edge.
Below we give an overview of the exter-nal resource-based features that have proven use-ful for keyphrase extraction.Wikipedia-based keyphraseness is computed asa candidate?s document frequency multiplied bythe ratio of the number of Wikipedia articles wherethe candidate appears as a link to the number ofarticles where it appears (Medelyan et al, 2009).This feature is motivated by the observation thata candidate is likely to be a keyphrase if it occursfrequently as a link in Wikipedia.
Unlike super-vised keyphraseness, Wikipedia-based keyphrase-ness can be computed without using documentsannotated with keyphrases and can work even ifthere is a mismatch between the training domainand the test domain.Yih et al (2006) employ a feature that en-codes whether a candidate keyphrase appears inthe query log of a search engine, exploiting the ob-servation that a candidate is potentially importantif it was used as a search query.
Terminologicaldatabases have been similarly exploited to encodethe salience of candidate keyphrases in scientificpapers (Lopez and Romary, 2010).While the aforementioned external resource-based features attempt to encode how salient acandidate keyphrase is, Turney (2003) proposesfeatures that encode the semantic relatedness be-tween two candidate keyphrases.
Noting that can-didate keyphrases that are not semantically re-lated to the predicted keyphrases are unlikely tobe keyphrases in technical reports, Turney em-ploys coherence features to identify such can-didate keyphrases.
Semantic relatedness is en-coded in the coherence features as two candidatekeyphrases?
pointwise mutual information, whichTurney computes by using the Web as a corpus.3.3 Unsupervised ApproachesExisting unsupervised approaches to keyphraseextraction can be categorized into four groups.3.3.1 Graph-Based RankingIntuitively, keyphrase extraction is about findingthe important words and phrases from a docu-ment.
Traditionally, the importance of a candi-date has often been defined in terms of how relatedit is to other candidates in the document.
Infor-mally, a candidate is important if it is related to (1)a large number of candidates and (2) candidatesthat are important.
Researchers have computed re-latedness between candidates using co-occurrencecounts (Mihalcea and Tarau, 2004; Matsuo andIshizuka, 2004) and semantic relatedness (Grinevaet al, 2009), and represented the relatedness in-formation collected from a document as a graph(Mihalcea and Tarau, 2004; Wan and Xiao, 2008a;Wan and Xiao, 2008b; Bougouin et al, 2013).The basic idea behind a graph-based approachis to build a graph from the input document andrank its nodes according to their importance us-ing a graph-based ranking method (e.g., Brin andPage (1998)).
Each node of the graph correspondsto a candidate keyphrase from the document andan edge connects two related candidates.
Theedge weight is proportional to the syntactic and/orsemantic relevance between the connected candi-dates.
For each node, each of its edges is treatedas a ?vote?
from the other node connected by theedge.
A node?s score in the graph is defined recur-sively in terms of the edges it has and the scores ofthe neighboring nodes.
The top-ranked candidatesfrom the graph are then selected as keyphrases forthe input document.
TextRank (Mihalcea and Ta-rau, 2004) is one of the most well-known graph-based approaches to keyphrase extraction.This instantiation of a graph-based approachoverlooks an important aspect of keyphrase ex-traction, however.
A set of keyphrases for a doc-ument should ideally cover the main topics dis-cussed in it, but this instantiation does not guaran-tee that all the main topics will be represented bythe extracted keyphrases.
Despite this weakness, agraph-based representation of text was adopted bymany approaches that propose different ways ofcomputing the similarity between two candidates.3.3.2 Topic-Based ClusteringAnother unsupervised approach to keyphraseextraction involves grouping the candidatekeyphrases in a document into topics, such thateach topic is composed of all and only thosecandidate keyphrases that are related to that topic(Grineva et al, 2009; Liu et al, 2009b; Liu et1265al., 2010).
There are several motivations behindthis topic-based clustering approach.
First, akeyphrase should ideally be relevant to one ormore main topic(s) discussed in a document(Liu et al, 2010; Liu et al, 2012).
Second, theextracted keyphrases should be comprehensivein the sense that they should cover all the maintopics in a document (Liu et al, 2009b; Liu et al,2010; Liu et al, 2012).
Below we examine threerepresentative systems that adopt this approach.KeyCluster Liu et al (2009b) adopt aclustering-based approach (henceforth KeyClus-ter) that clusters semantically similar candidatesusing Wikipedia and co-occurrence-based statis-tics.
The underlying hypothesis is that each ofthese clusters corresponds to a topic covered inthe document, and selecting the candidates closeto the centroid of each cluster as keyphrasesensures that the resulting set of keyphrases coversall the topics of the document.While empirical results show that KeyClusterperforms better than both TextRank and Hulth?s(2003) supervised system, KeyCluster has a poten-tial drawback: by extracting keyphrases from eachtopic cluster, it essentially gives each topic equalimportance.
In practice, however, there couldbe topics that are not important and these topicsshould not have keyphrase(s) representing them.Topical PageRank (TPR) Liu et al (2010) pro-pose TPR, an approach that overcomes the afore-mentioned weakness of KeyCluster.
It runs Tex-tRank multiple times for a document, once foreach of its topics induced by a Latent Dirichlet Al-location (Blei et al, 2003).
By running TextRankonce for each topic, TPR ensures that the extractedkeyphrases cover the main topics of the document.The final score of a candidate is computed as thesum of its scores for each of the topics, weightedby the probability of that topic in that document.Hence, unlike KeyCluster, candidates belonging toa less probable topic are given less importance.TPR performs significantly better than bothtf*idf and TextRank on the DUC-2001 and Inspecdatasets.
TPR?s superior performance strength-ens the hypothesis of using topic clustering forkeyphrase extraction.
However, though TPR isconceptually better than KeyCluster, Liu et al didnot compare TPR against KeyCluster.CommunityCluster Grineva et al (2009) pro-pose CommunityCluster, a variant of the topicclustering approach to keyphrase extraction.
LikeTPR, CommunityCluster gives more weight tomore important topics, but unlike TPR, it extractsall candidate keyphrases from an important topic,assuming that a candidate that receives little focusin the text should still be extracted as a keyphraseas long as it is related to an important topic.
Com-munityCluster yields much better recall (withoutlosing precision) than extractors such as tf*idf,TextRank, and the Yahoo!
term extractor.3.3.3 Simultaneous LearningSince keyphrases represent a dense summary of adocument, researchers hypothesized that text sum-marization and keyphrase extraction can poten-tially benefit from each other if these tasks are per-formed simultaneously.
Zha (2002) proposes thefirst graph-based approach for simultaneous sum-marization and keyphrase extraction, motivated bya key observation: a sentence is important if it con-tains important words, and important words ap-pear in important sentences.
Wan et al (2007) ex-tend Zha?s work by adding two assumptions: (1)an important sentence is connected to other im-portant sentences, and (2) an important word islinked to other important words, a TextRank-likeassumption.
Based on these assumptions, Wan etal.
(2007) build three graphs to capture the asso-ciation between the sentences (S) and the words(W) in an input document, namely, a S?S graph,a bipartite S?W graph, and a W?W graph.
Theweight of an edge connecting two sentence nodesin a S?S graph corresponds to their content simi-larity.
An edge weight in a S?W graph denotes theword?s importance in the sentence it appears.
Fi-nally, an edge weight in a W?W graph denotes theco-occurrence or knowledge-based similarity be-tween the two connected words.
Once the graphsare constructed for an input document, an itera-tive reinforcement algorithm is applied to assignscores to each sentence and word.
The top-scoredwords are used to form keyphrases.The main advantage of this approach is that itcombines the strengths of both Zha?s approach(i.e., bipartite S?W graphs) and TextRank (i.e., W?W graphs) and performs better than both of them.However, it has a weakness: like TextRank, it doesnot ensure that the extracted keyphrases will coverall the main topics.
To address this problem, onecan employ a topic clustering algorithm on the W?W graph to produce the topic clusters, and then en-sure that keyphrases are chosen from every maintopic cluster.12663.3.4 Language ModelingMany existing approaches have a separate, heuris-tic module for extracting candidate keyphrasesprior to keyphrase ranking/extraction.
In contrast,Tomokiyo and Hurst (2003) propose an approach(henceforth LMA) that combines these two steps.LMA scores a candidate keyphrase based ontwo features, namely, phraseness (i.e., the ex-tent to which a word sequence can be treated asa phrase) and informativeness (i.e., the extent towhich a word sequence captures the central idea ofthe document it appears in).
Intuitively, a phrasethat has high scores for phraseness and informa-tiveness is likely to be a keyphrase.
These featurevalues are estimated using language models (LMs)trained on a foreground corpus and a backgroundcorpus.
The foreground corpus is composed ofthe set of documents from which keyphrases areto be extracted.
The background corpus is a largecorpus that encodes general knowledge about theworld (e.g., the Web).
A unigram LM and an n-gram LM are constructed for each of these twocorpora.
Phraseness, defined using the foregroundLM, is calculated as the loss of information in-curred as a result of assuming a unigram LM (i.e.,conditional independence among the words of thephrase) instead of an n-gram LM (i.e., the phraseis drawn from an n-gram LM).
Informativeness iscomputed as the loss that results because of theassumption that the candidate is sampled from thebackground LM rather than the foreground LM.The loss values are computed using Kullback-Leibler divergence.
Candidates are ranked accord-ing to the sum of these two feature values.In sum, LMA uses a language model rather thanheuristics to identify phrases, and relies on the lan-guage model trained on the background corpus todetermine how ?unique?
a candidate keyphrase isto the domain represented by the foreground cor-pus.
The more unique it is to the foreground?s do-main, the more likely it is a keyphrase for that do-main.
While the use of language models to iden-tify phrases cannot be considered a major strengthof this approach (because heuristics can identifyphrases fairly reliably), the use of a backgroundcorpus to identify candidates that are unique to theforeground?s domain is a unique aspect of this ap-proach.
We believe that this idea deserves furtherinvestigation, as it would allow us to discover akeyphrase that is unique to the foreground?s do-main but may have a low tf*idf value.4 EvaluationIn this section, we describe metrics for evaluatingkeyphrase extraction systems as well as state-of-the-art results on commonly-used datasets.4.1 Evaluation MetricsDesigning evaluation metrics for keyphrase ex-traction is by no means an easy task.
To scorethe output of a keyphrase extraction system, thetypical approach, which is also adopted by theSemEval-2010 shared task on keyphrase extrac-tion, is (1) to create a mapping between thekeyphrases in the gold standard and those in thesystem output using exact match, and then (2)score the output using evaluation metrics such asprecision (P), recall (R), and F-score (F).Conceivably, exact match is an overly strict con-dition, considering a predicted keyphrase incor-rect even if it is a variant of a gold keyphrase.For instance, given the gold keyphrase ?neuralnetwork?, exact match will consider a predictedphrase incorrect even if it is an expanded versionof the gold keyphrase (?artificial neural network?
)or one of its morphological (?neural networks?)
orlexical (?neural net?)
variants.
While morphologi-cal variations can be handled using a stemmer (El-Beltagy and Rafea, 2009), other variations maynot be handled easily and reliably.Human evaluation has been suggested as a pos-sibility (Matsuo and Ishizuka, 2004), but it is time-consuming and expensive.
For this reason, re-searchers have experimented with two types ofautomatic evaluation metrics.
The first type ofmetrics addresses the problem with exact match.These metrics reward a partial match between apredicted keyphrase and a gold keyphrase (i.e.,overlapping n-grams) and are commonly usedin machine translation (MT) and summarizationevaluations.
They include BLEU, METEOR, NIST,and ROUGE.
Nevertheless, experiments show thatthese MT metrics only offer a partial solution toproblem with exact match: they can only detect asubset of the near-misses (Kim et al, 2010a).The second type of metrics focuses on how asystem ranks its predictions.
Given that two sys-tems A and B have the same number of correctpredictions, binary preference measure (Bpref)and mean reciprocal rank (MRR) (Liu et al, 2010)will award more credit to A than to B if the ranksof the correct predictions in A?s output are higherthan those in B?s output.
R-precision (Rp) is an1267IR metric that focuses on ranking: given a docu-ment with n gold keyphrases, it computes the pre-cision of a system over its n highest-ranked can-didates (Zesch and Gurevych, 2009).
The motiva-tion behind the design of Rpis simple: a systemwill achieve a perfect Rpvalue if it ranks all thekeyphrases above the non-keyphrases.4.2 The State of the ArtTable 2 lists the best scores on some popular evalu-ation datasets and the corresponding systems.
Forexample, the best F-scores on the Inspec test set,the DUC-2001 dataset, and the SemEval-2010 testset are 45.7, 31.7, and 27.5, respectively.3Two points deserve mention.
First, F-scores de-crease as document length increases.
These re-sults are consistent with the observation we madein Section 2 that it is more difficult to extractkeyphrases correctly from longer documents.
Sec-ond, recent unsupervised approaches have rivaledtheir supervised counterparts in performance (Mi-halcea and Tarau, 2004; El-Beltagy and Rafea,2009; Liu et al, 2009b).
For example, KP-Miner(El-Beltagy and Rafea, 2010), an unsupervisedsystem, ranked third in the SemEval-2010 sharedtask with an F-score of 25.2, which is comparableto the best supervised system scoring 27.5.5 AnalysisWith the goal of providing directions for futurework, we identify the errors commonly made bystate-of-the-art keyphrase extractors below.5.1 Error AnalysisAlthough a few researchers have presented a sam-ple of their systems?
output and the correspondinggold keyphrases to show the differences betweenthem (Witten et al, 1999; Nguyen and Kan, 2007;Medelyan et al, 2009), a systematic analysis ofthe major types of errors made by state-of-the-artkeyphrase extraction systems is missing.To fill this gap, we ran four keyphrase extrac-tion systems on four commonly-used datasets ofvarying sources, including Inspec abstracts (Hulth,2003), DUC-2001 news articles (Over, 2001), sci-entific papers (Kim et al, 2010b), and meetingtranscripts (Liu et al, 2009a).
Specifically, we ran-domly selected 25 documents from each of these3A more detailed analysis of the results of the SemEval-2010 shared task and the approaches adopted by the partici-pating systems can be found in Kim et al (2013).DatasetApproach and System[Supervised?
]ScoreP R FAbstracts(Inspec)Topic clustering(Liu et al, 2009b) [?
]35.0 66.0 45.7BlogsTopic community detection(Grineva et al, 2009) [?
]35.1 61.5 44.7News(DUC-2001)Graph-based rankingfor extended neighborhood(Wan and Xiao, 2008b) [?
]28.8 35.4 31.7Papers(SemEval-2010)Statistical, semantic, anddistributional features(Lopez and Romary, 2010) [X]27.2 27.8 27.5Table 2: Best scores achieved on various datasets.four datasets and manually analyzed the output ofthe four systems, including tf*idf, the most fre-quently used baseline, as well as three state-of-the-art keyphrase extractors, of which two are unsu-pervised (Wan and Xiao, 2008b; Liu et al, 2009b)and one is supervised (Medelyan et al, 2009).Our analysis reveals that the errors fall into fourmajor types, each of which contributes signifi-cantly to the overall errors made by the four sys-tems, despite the fact that the contribution of eachof these error types varies from system to system.Moreover, we do not observe any significant dif-ference between the types of errors made by thefour systems other than the fact that the super-vised system has the expected tendency to predictkeyphrases seen in the training data.
Below wedescribe these four major types of errors.Overgeneration errors are a major type of pre-cision error, contributing to 28?37% of the overallerror.
Overgeneration errors occur when a systemcorrectly predicts a candidate as a keyphrase be-cause it contains a word that appears frequently inthe associated document, but at the same time er-roneously outputs other candidates as keyphrasesbecause they contain the same word.
Recall thatfor many systems, it is not easy to reject a non-keyphrase containing a word with a high term fre-quency: many unsupervised systems score a can-didate by summing the score of each of its compo-nent words, and many supervised systems use un-igrams as features to represent a candidate.
To bemore concrete, consider the news article on athleteBen Johnson in Figure 1, where the keyphrases areboldfaced.
As we can see, the word Olympic(s)has a significant presence in the document.
Con-sequently, many systems not only correctly predictOlympics as a keyphrase, but also erroneously pre-dict Olympic movement as a keyphrase, yieldingovergeneration errors.Infrequency errors are a major type of re-1268Canadian Ben Johnson left the Olympics today ?in acomplete state of shock,?
accused of cheating with drugsin the world?s fastest 100-meter dash and stripped ofhis gold medal.
The prize went to American CarlLewis.
Many athletes accepted the accusation that John-son used a muscle-building but dangerous and illegal an-abolic steroid called stanozolol as confirmation of whatthey said they know has been going on in track and field.Two tests of Johnson?s urine sample proved positive andhis denials of drug use were rejected today.
?This isa blow for the Olympic Games and the Olympic move-ment,?
said International Olympic Committee PresidentJuan Antonio Samaranch.Figure 1: A news article on Ben Johnson from theDUC-2001 dataset.
The keyphrases are boldfaced.call error contributing to 24?27% of the overallerror.
Infrequency errors occur when a systemfails to identify a keyphrase owing to its infre-quent presence in the associated document (Liuet al, 2011).
Handling infrequency errors is achallenge because state-of-the-art keyphrase ex-tractors rarely predict candidates that appear onlyonce or twice in a document.
In the Ben Johnsonexample, many keyphrase extractors fail to iden-tify 100-meter dash and gold medal as keyphrases,resulting in infrequency errors.Redundancy errors are a type of precision er-ror contributing to 8?12% of the overall error.
Re-dundancy errors occur when a system correctlyidentifies a candidate as a keyphrase, but at thesame time outputs a semantically equivalent can-didate (e.g., its alias) as a keyphrase.
This typeof error can be attributed to a system?s failureto determine that two candidates are semanticallyequivalent.
Nevertheless, some researchers mayargue that a system should not be penalized for re-dundancy errors because the extracted candidatesare in fact keyphrases.
In our example, Olympicsand Olympic games refer to the same concept, soa system that predicts both of them as keyphrasescommits a redundancy error.Evaluation errors are a type of recall error con-tributing to 7?10% of the overall error.
An evalu-ation error occurs when a system outputs a can-didate that is semantically equivalent to a goldkeyphrase, but is considered erroneous by a scor-ing program because of its failure to recognizethat the predicted phrase and the correspondinggold keyphrase are semantically equivalent.
Inother words, an evaluation error is not an errormade by a keyphrase extractor, but an error dueto the naivety of a scoring program.
In our exam-ple, while Olympics and Olympic games refer tothe same concept, only the former is annotated askeyphrase.
Hence, an evaluation error occurs if asystem predicts Olympic games but not Olympicsas a keyphrase and the scoring program fails toidentify them as semantically equivalent.5.2 RecommendationsWe recommend that background knowledge beextracted from external lexical databases (e.g.,YAGO2 (Suchanek et al, 2007), Freebase (Bol-lacker et al, 2008), BabelNet (Navigli andPonzetto, 2012)) to address the four types of er-rors discussed above.First, we discuss how redundancy errors couldbe addressed by using the background knowledgeextracted from external databases.
Note that if wecan identify semantically equivalent candidates,then we can reduce redundancy errors.
The ques-tion, then, is: can background knowledge be usedto help us identify semantically equivalent candi-dates?
To answer this question, note that Freebase,for instance, has over 40 million topics (i.e., real-world entities such as people, places, and things)from over 70 domains (e.g., music, business, ed-ucation).
Hence, before a system outputs a set ofcandidates as keyphrases, it can use Freebase todetermine whether any of them is mapped to thesame Freebase topic.
Referring back to our run-ning example, both Olympics and Olympic gamesare mapped to a Freebase topic called Olympicgames.
Based on this information, a keyphrase ex-tractor can determine that the two candidates arealiases and should output only one of them, thuspreventing a redundancy error.Next, we discuss how infrequency errorscould be addressed using background knowledge.A natural way to handle this problem would beto make an infrequent keyphrase frequent.
To ac-complish this, we suggest exploiting an influen-tial idea in the keyphrase extraction literature: theimportance of a candidate is defined in terms ofhow related it is to other candidates in the text (seeSection 3.3.1).
In other words, if we could relatean infrequent keyphrase to other candidates in thetext, we could boost its importance.We believe that this could be accomplished us-ing background knowledge.
The idea is to boostthe importance of infrequent keyphrases usingtheir frequent counterparts.
Consider again ourrunning example.
All four systems have managedto identify Ben Johnson as a keyphrase due to its1269significant presence.
Hence, we can boost the im-portance of 100-meter dash and gold medal if wecan relate them to Ben Johnson.To do so, note that Freebase maps a candi-date to one or more pre-defined topics, each ofwhich is associated with one or more types.
Typesare similar to entity classes.
For instance, thecandidate Ben Johnson is mapped to a Freebasetopic with the same name, which is associatedwith Freebase types such as Person, Athlete, andOlympic athlete.
Types are defined for a specificdomain in Freebase.
For instance, Person, Ath-lete, and Olympic athlete are defined in the People,Sports, and Olympics domains, respectively.
Next,consider the two infrequent candidates, 100-meterdash and gold medal.
100-meter dash is mappedto the topic Sprint of type Sports in the Sports do-main, whereas gold medal is mapped to a topicwith the same name of type Olympic medal in theOlympics domain.
Consequently, we can relate100-meter dash to Ben Johnson via the Sports do-main (i.e., they belong to different types under thesame domain).
Additionally, gold medal can berelated to Ben Johnson via the Olympics domain.As discussed before, the relationship betweentwo candidates is traditionally established usingco-occurrence information.
However, using co-occurrence windows has its shortcomings.
First,an ad-hoc window size cannot capture related can-didates that are not inside the window.
So itis difficult to predict 100-meter dash and goldmedal as keyphrases: they are more than 10 tokensaway from frequent words such as Johnson andOlympics.
Second, the candidates inside a windoware all assumed to be related to each other, but it isapparently an overly simplistic assumption.
Therehave been a few attempts to design Wikipedia-based relatedness measures, with promising ini-tial results (Grineva et al, 2009; Liu et al, 2009b;Medelyan et al, 2009).4Overgeneration errors could similarly be ad-dressed using background knowledge.
Recall thatOlympic movement is not a keyphrase in our ex-ample although it includes an important word (i.e.,Olympic).
Freebase maps Olympic movement toa topic with the same name, which is associatedwith a type called Musical Recording in the Mu-sic domain.
However, it does not map Olympic4Note that it may be difficult to employ our recommen-dations to address infrequency errors in informal text withuncorrelated topics because the keyphrases it contains maynot be related to each other (see Section 2).movement to any topic in the Olympics domain.The absence of such a mapping in the Olympicsdomain could be used by a keyphrase extractor asa supporting evidence against predicting Olympicmovement as a keyphrase.Finally, as mentioned before, evaluation errorsshould not be considered errors made by a sys-tem.
Nevertheless, they reveal a problem with theway keyphrase extractors are currently evaluated.To address this problem, one possibility is to con-duct human evaluations.
Cheaper alternatives in-clude having human annotators identify semanti-cally equivalent keyphrases during manual label-ing, and designing scoring programs that can au-tomatically identify such semantic equivalences.6 Conclusion and Future DirectionsWe have presented a survey of the state of the artin automatic keyphrase extraction.
While unsu-pervised approaches have started to rival their su-pervised counterparts in performance, the task isfar from being solved, as reflected by the fairlypoor state-of-the-art results on various commonly-used evaluation datasets.
Our analysis revealedthat there are at least three major challenges ahead.1.
Incorporating background knowledge.While much recent work has focused on algo-rithmic development, keyphrase extractors needto have a deeper ?understanding?
of a documentin order to reach the next level of performance.Such an understanding can be facilitated by theincorporation of background knowledge.2.
Handling long documents.
While it may bepossible to design better algorithms to handle thelarge number of candidates in long documents, webelieve that employing sophisticated features, es-pecially those that encode background knowledge,will enable keyphrases and non-keyphrases to bedistinguished more easily even in the presence ofa large number of candidates.3.
Improving evaluation schemes.
To more ac-curately measure the performance of keyphraseextractors, they should not be penalized for evalu-ation errors.
We have suggested several possibili-ties as to how this problem can be addressed.AcknowledgmentsWe thank the anonymous reviewers for their de-tailed and insightful comments on earlier drafts ofthis paper.
This work was supported in part byNSF Grants IIS-1147644 and IIS-1219142.1270ReferencesKen Barker and Nadia Cornacchia.
2000.
Using nounphrase heads to extract document keyphrases.
InProceedings of the 13th Biennial Conference of theCanadian Society on Computational Studies of In-telligence, pages 40?52.G?abor Berend.
2011.
Opinion expression mining byexploiting keyphrase extraction.
In Proceedings ofthe 5th International Joint Conference on NaturalLanguage Processing, pages 1162?1170.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet alocation.
Journal of Ma-chine Learning Research, 3:993?1022.Kurt Bollacker, Colin Evans, Praveen Paritosh, TimSturge, and Jamie Taylor.
2008.
Freebase: A col-laboratively created graph database for structuringhuman knowledge.
In Proceedings of the 2008 ACMSIGMOD International Conference on Managementof Data, pages 1247?1250.Adrien Bougouin, Florian Boudin, and B?eatrice Daille.2013.
Topicrank: Graph-based topic ranking forkeyphrase extraction.
In Proceedings of the 6th In-ternational Joint Conference on Natural LanguageProcessing, pages 543?551.Sergey Brin and Lawrence Page.
1998.
The anatomyof a large-scale hypertextual Web search engine.Computer Networks, 30(1?7):107?117.Mo Chen, Jian-Tao Sun, Hua-Jun Zeng, and Kwok-YanLam.
2005.
A practical system of keyphrase ex-traction for web pages.
In Proceedings of the 14thACM International Conference on Information andKnowledge Management, pages 277?278.Zhuoye Ding, Qi Zhang, and Xuanjing Huang.
2011.Keyphrase extraction from online news using binaryinteger programming.
In Proceedings of the 5th In-ternational Joint Conference on Natural LanguageProcessing, pages 165?173.Mark Dredze, Hanna M. Wallach, Danny Puller, andFernando Pereira.
2008.
Generating summary key-words for emails using topics.
In Proceedings of the13th International Conference on Intelligent UserInterfaces, pages 199?206.Samhaa R. El-Beltagy and Ahmed A. Rafea.
2009.KP-Miner: A keyphrase extraction system for En-glish and Arabic documents.
Information Systems,34(1):132?144.Samhaa R. El-Beltagy and Ahmed Rafea.
2010.
KP-Miner: Participation in SemEval-2.
In Proceedingsof the 5th International Workshop on Semantic Eval-uation, pages 190?193.Eibe Frank, Gordon W. Paynter, Ian H. Witten, CarlGutwin, and Craig G. Nevill-Manning.
1999.Domain-specific keyphrase extraction.
In Proceed-ings of 16th International Joint Conference on Arti-ficial Intelligence, pages 668?673.Maria Grineva, Maxim Grinev, and Dmitry Lizorkin.2009.
Extracting key terms from noisy and multi-theme documents.
In Proceedings of the 18th In-ternational Conference on World Wide Web, pages661?670.Carl Gutwin, Gordon Paynter, Ian Witten, Craig Nevill-Manning, and Eibe Frank.
1999.
Improving brows-ing in digital libraries with keyphrase indexes.
De-cision Support Systems, 27:81?104.Khaled M. Hammouda, Diego N. Matute, and Mo-hamed S. Kamel.
2005.
CorePhrase: Keyphrase ex-traction for document clustering.
In Proceedings ofthe 4th International Conference on Machine Learn-ing and Data Mining in Pattern Recognition, pages265?274.Kazi Saidul Hasan and Vincent Ng.
2010.
Conun-drums in unsupervised keyphrase extraction: Mak-ing sense of the state-of-the-art.
In Proceedingsof the 23rd International Conference on Computa-tional Linguistics: Posters, pages 365?373.Chong Huang, Yonghong Tian, Zhi Zhou, Charles X.Ling, and Tiejun Huang.
2006.
Keyphrase extrac-tion using semantic networks structure analysis.
InProceedings of the 6th International Conference onData Mining, pages 275?284.Anette Hulth and Be?ata B. Megyesi.
2006.
A studyon automatically extracted keywords in text catego-rization.
In Proceedings of the 21st InternationalConference on Computational Linguistics and the44th Annual Meeting of the Association for Compu-tational Linguistics, pages 537?544.Anette Hulth, Jussi Karlgren, Anna Jonsson, HenrikBostr?om, and Lars Asker.
2001.
Automatic key-word extraction using domain knowledge.
In Pro-ceedings of the 2nd International Conference onComputational Linguistics and Intelligent Text Pro-cessing, pages 472?482.Anette Hulth.
2003.
Improved automatic keyword ex-traction given more linguistic knowledge.
In Pro-ceedings of the 2003 Conference on Empirical Meth-ods in Natural Language Processing, pages 216?223.Anette Hulth.
2004.
Enhancing linguistically ori-ented automatic keyword extraction.
In Proceedingsof the Human Language Technology Conference ofthe North American Chapter of the Association forComputational Linguistics: Short Papers, pages 17?20.Xin Jiang, Yunhua Hu, and Hang Li.
2009.
A rank-ing approach to keyphrase extraction.
In Proceed-ings of the 32nd International ACM SIGIR Confer-ence on Research and Development in InformationRetrieval, pages 756?757.Daniel Kelleher and Saturnino Luz.
2005.
Automatichypertext keyphrase detection.
In Proceedings of the19th International Joint Conference on Artificial In-telligence, pages 1608?1609.1271Su Nam Kim and Timothy Baldwin.
2012.
Extractingkeywords from multi-party live chats.
In Proceed-ings of the 26th Pacific Asia Conference on Lan-guage, Information, and Computation, pages 199?208.Su Nam Kim and Min-Yen Kan. 2009.
Re-examiningautomatic keyphrase extraction approaches in scien-tific articles.
In Proceedings of the ACL-IJCNLPWorkshop on Multiword Expressions, pages 9?16.Su Nam Kim, Timothy Baldwin, and Min-Yen Kan.2010a.
Evaluating n-gram based evaluation metricsfor automatic keyphrase extraction.
In Proceedingsof the 23rd International Conference on Computa-tional Linguistics, pages 572?580.Su Nam Kim, Olena Medelyan, Min-Yen Kan, andTimothy Baldwin.
2010b.
SemEval-2010 Task 5:Automatic keyphrase extraction from scientific arti-cles.
In Proceedings of the 5th International Work-shop on Semantic Evaluation, pages 21?26.Su Nam Kim, Olena Medelyan, Min-Yen Kan, andTimothy Baldwin.
2013.
Automatic keyphraseextraction from scientific articles.
Language Re-sources and Evaluation, 47(3):723?742.Niraj Kumar and Kannan Srinathan.
2008.
Automatickeyphrase extraction from scientific documents us-ing n-gram filtration technique.
In Proceedings ofthe 8th ACM Symposium on Document Engineering,pages 199?208.Feifan Liu, Deana Pennell, Fei Liu, and Yang Liu.2009a.
Unsupervised approaches for automatic key-word extraction using meeting transcripts.
In Pro-ceedings of Human Language Technologies: TheAnnual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 620?628.Zhiyuan Liu, Peng Li, Yabin Zheng, and MaosongSun.
2009b.
Clustering to find exemplar terms forkeyphrase extraction.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing, pages 257?266.Zhiyuan Liu, Wenyi Huang, Yabin Zheng, andMaosong Sun.
2010.
Automatic keyphrase extrac-tion via topic decomposition.
In Proceedings of the2010 Conference on Empirical Methods in NaturalLanguage Processing, pages 366?376.Zhiyuan Liu, Xinxiong Chen, Yabin Zheng, andMaosong Sun.
2011.
Automatic keyphrase extrac-tion by bridging vocabulary gap.
In Proceedings ofthe 15th Conference on Computational Natural Lan-guage Learning, pages 135?144.Zhiyuan Liu, Chen Liang, and Maosong Sun.
2012.Topical word trigger model for keyphrase extraction.In Proceedings of the 24th International Conferenceon Computational Linguistics, pages 1715?1730.Patrice Lopez and Laurent Romary.
2010.
HUMB:Automatic key term extraction from scientific arti-cles in GROBID.
In Proceedings of the 5th Inter-national Workshop on Semantic Evaluation, pages248?251.Yutaka Matsuo and Mitsuru Ishizuka.
2004.
Key-word extraction from a single document using wordco-occurrence statistical information.
InternationalJournal on Artificial Intelligence Tools, 13.Olena Medelyan, Eibe Frank, and Ian H. Witten.2009.
Human-competitive tagging using automatickeyphrase extraction.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing, pages 1318?1327.Rada Mihalcea and Paul Tarau.
2004.
TextRank:Bringing order into texts.
In Proceedings of the2004 Conference on Empirical Methods in NaturalLanguage Processing, pages 404?411.Roberto Navigli and Simone Paolo Ponzetto.
2012.BabelNet: The automatic construction, evaluationand application of a wide-coverage multilingual se-mantic network.
Artificial Intelligence, 193:217?250.David Newman, Nagendra Koilada, Jey Han Lau, andTimothy Baldwin.
2012.
Bayesian text segmenta-tion for index term identification and keyphrase ex-traction.
In Proceedings of the 24th InternationalConference on Computational Linguistics, pages2077?2092.Thuy Dung Nguyen and Min-Yen Kan. 2007.Keyphrase extraction in scientific publications.
InProceedings of the International Conference onAsian Digital Libraries, pages 317?326.Chau Q. Nguyen and Tuoi T. Phan.
2009.
Anontology-based approach for key phrase extraction.In Proceedings of the Joint Conference of the 47thAnnual Meeting of the Association for Computa-tional Linguistics and the 4th International JointConference on Natural Language Processing: ShortPapers, pages 181?184.Paul Over.
2001.
Introduction to DUC-2001: An in-trinsic evaluation of generic news text summariza-tion systems.
In Proceedings of the 2001 DocumentUnderstanding Conference.Mari-Sanna Paukkeri, Ilari T. Nieminen, Matti P?oll?a,and Timo Honkela.
2008.
A language-independentapproach to keyphrase extraction and evaluation.
InProceedings of the 22nd International Conferenceon Computational Linguistics: Companion Volume:Posters, pages 83?86.Gerard Salton and Christopher Buckley.
1988.
Term-weighting approaches in automatic text retrieval.
In-formation Processing and Management, 24(5):513?523.1272Min Song, Il-Yeol Song, and Xiaohua Hu.
2003.KPSpotter: A flexible information gain-basedkeyphrase extraction system.
In Proceedings of the5th ACM International Workshop on Web Informa-tion and Data Management, pages 50?53.Fabian M. Suchanek, Gjergji Kasneci, and GerhardWeikum.
2007.
YAGO: A core of semantic knowl-edge.
In Proceedings of the 16th InternationalWorld Wide Web Conference, pages 697?706.Takashi Tomokiyo and Matthew Hurst.
2003.
A lan-guage model approach to keyphrase extraction.
InProceedings of the ACL Workshop on Multiword Ex-pressions, pages 33?40.Peter Turney.
1999.
Learning to extract keyphrasesfrom text.
National Research Council Canada, In-stitute for Information Technology, Technical ReportERB-1057.Peter Turney.
2000.
Learning algorithms for keyphraseextraction.
Information Retrieval, 2:303?336.Peter Turney.
2003.
Coherent keyphrase extractionvia web mining.
In Proceedings of the 18th Inter-national Joint Conference on Artificial Intelligence,pages 434?439.Xiaojun Wan and Jianguo Xiao.
2008a.
Col-labRank: Towards a collaborative approach tosingle-document keyphrase extraction.
In Proceed-ings of the 22nd International Conference on Com-putational Linguistics, pages 969?976.Xiaojun Wan and Jianguo Xiao.
2008b.
Singledocument keyphrase extraction using neighborhoodknowledge.
In Proceedings of the 23rd AAAI Con-ference on Artificial Intelligence, pages 855?860.Xiaojun Wan, Jianwu Yang, and Jianguo Xiao.
2007.Towards an iterative reinforcement approach for si-multaneous document summarization and keywordextraction.
In Proceedings of the 45th Annual Meet-ing of the Association of Computational Linguistics,pages 552?559.Chen Wang and Sujian Li.
2011.
CoRankBayes:Bayesian learning to rank under the co-trainingframework and its application in keyphrase extrac-tion.
In Proceedings of the 20th ACM InternationalConference on Information and Knowledge Man-agement, pages 2241?2244.Ian H. Witten, Gordon W. Paynter, Eibe Frank, CarlGutwin, and Craig G. Nevill-Manning.
1999.
KEA:Practical automatic keyphrase extraction.
In Pro-ceedings of the 4th ACM Conference on Digital Li-braries, pages 254?255.Yi-Fang Brook Wu, Quanzhi Li, Razvan Stefan Bot,and Xin Chen.
2005.
Domain-specific keyphraseextraction.
In Proceedings of the 14th ACM Inter-national Conference on Information and KnowledgeManagement, pages 283?284.Wen-Tau Yih, Joshua Goodman, and Vitor R. Carvalho.2006.
Finding advertising keywords on web pages.In Proceedings of the 15th International Conferenceon World Wide Web, pages 213?222.Wei You, Dominique Fontaine, and Jean-Paul Barth`es.2009.
Automatic keyphrase extraction with arefined candidate set.
In Proceedings of theIEEE/WIC/ACM International Joint Conference onWeb Intelligence and Intelligent Agent Technology,pages 576?579.Torsten Zesch and Iryna Gurevych.
2009.
Approxi-mate matching for evaluating keyphrase extraction.In Proceedings of the International Conference onRecent Advances in Natural Language Processing2009, pages 484?489.Hongyuan Zha.
2002.
Generic summarization andkeyphrase extraction using mutual reinforcementprinciple and sentence clustering.
In Proceedingsof 25th Annual International ACM SIGIR Confer-ence on Research and Development in InformationRetrieval, pages 113?120.Yongzheng Zhang, Nur Zincir-Heywood, and Evange-los Milios.
2004.
World Wide Web site summariza-tion.
Web Intelligence and Agent Systems, 2:39?53.Yongzheng Zhang, Nur Zincir-Heywood, and Evange-los Milios.
2005.
Narrative text classification forautomatic key phrase extraction in web documentcorpora.
In Proceedings of the 7th ACM Interna-tional Workshop on Web Information and Data Man-agement, pages 51?58.Xin Zhao, Jing Jiang, Jing He, Yang Song, PalakornAchanauparp, Ee-Peng Lim, and Xiaoming Li.2011.
Topical keyphrase extraction from Twitter.In Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 379?388.1273
