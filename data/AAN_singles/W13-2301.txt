Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 1?10,Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational LinguisticsAutomatic Correction and Extension of Morphological AnnotationsRamy Eskander, Nizar HabashCenter for Computational Learning Systems, Columbia University{reskander,habash}@ccls.columbia.eduAnn Bies, Seth Kulick, Mohamed MaamouriLinguistic Data Consortium, University of Pennsylvania{bies,skulick,maamouri}@ldc.upenn.eduAbstractFor languages with complex morpholo-gies, limited resources and tools, and/orlack of standard grammars, developing an-notated resources can be a challengingtask.
Annotated resources developed un-der time/money constraints for such lan-guages tend to tradeoff depth of represen-tation with degree of noise.
We presenttwo methods for automatic correction andextension of morphological annotations,and demonstrate their success on three di-vergent Egyptian Arabic corpora.1 IntroductionAnnotated corpora are essential for most researchin natural language processing (NLP).
For exam-ple, the development of treebanks, such as thePenn Treebank and the Penn Arabic Treebank,has been essential in pushing research on part-of-speech (POS) tagging and parsing of Englishand Arabic (Marcus et al 1993; Maamouri et al2004).
The creation of such resources tends to bequite expensive and time consuming: guidelinesneed to be developed, annotators hired, trained,and regularly evaluated for quality control.
Forlanguages with complex morphologies, limited re-sources and tools, and/or lack of standard gram-mars, such as any of the Dialectal Arabic (DA)varieties, developing annotated resources can be achallenging task.
As a result, annotated resourcesdeveloped under time/money constraints for suchlanguages tend to tradeoff depth of representationwith degree of noise.
In the extremes, we find richmorphological representations that may be noisyand inconsistent or simple by highly consistentand reliable annotations that have limited usabil-ity.
Furthermore, such resources are often devel-oped by different research groups leading to manyinconstancies that make pooling these resourcesnot a very easy task.In this paper, we describe two general tech-niques to address the limitations of the two typesof annotations: corrections of rich noisy annota-tions and extensions of clean but shallow ones.We present our work on Egyptian Arabic, an im-portant Arabic dialect with limited resources, andrich and ambiguous morphology.
Resulting fromthis effort is the largest Egyptian Arabic corpusannotated in one common representation by pool-ing resources from three very different sources:a non-final, pre-release version of the ARZ1 cor-pora from the Linguistic Data Consortium (LDC)(Maamouri et al 2012g), the LDC?s CallHomeEgypt transcripts (Gadalla et al 1997) and CMU?sEgyptian Arabic corpus (CMUEAC) (Mohamed etal., 2012).Although the paper focuses on Arabic, the ba-sic problem is relevant to other languages, espe-cially spontaneously written colloquial languageforms such as those used in social media.
Thegeneral solutions we propose are language inde-pendent given availability of specific language re-sources.Next we discuss some related work and rel-evant linguistic facts (Sections 2 and 3, respec-tively).
Section 4 presents our annotation cor-rection technique; and Section 5 presents out an-notation extension technique.
Finally, Section 6presents some statistics on the Egyptian Arabiccorpus annotated in one unified representation re-sulting from our correction and extension work.2 Related WorkMuch work has been done on automatic spellingcorrection.
Both supervised and unsupervised ap-proaches have been used employing a variety of1ARZ is the language code for Egyptian Ara-bic, http://www-01.sil.org/iso639-3/documentation.asp?id=arz1tools, resources, and heuristics, e.g., morpholog-ical analyzers, language models, annotated dataand edit-distance measures, respectively (Kukich,1992; Oflazer, 1996; Shaalan et al 2003; Hassanet al 2008; Kolak and Resnik, 2002; Magdy andDarwish, 2006).
Our work is different from theseapproaches in that it extends beyond spelling ofword forms to deeper annotations.
However, weuse some of these techniques to correct not justthe words, but also malformed POS tags.A number of efforts exist on treebank en-richment for many languages including Arabic(Palmer et al 2008; Hovy et al 2006; Alkuh-lani and Habash, 2011; Alkuhlani et al 2013).Our morphological extension effort is similar toAlkuhlani et al(2013)?s work except that theystart with tokenizations, reduced POS tags and de-pendency trees and extend them to full morpho-logical information.There has been a lot of work on Arabic POS tag-ging and morphological disambiguation (Habashand Rambow, 2005; Smith et al 2005; Hajic?
etal., 2005; Habash, 2010; Habash et al 2013).The work by Habash et al(2013) uses one of theresources we improve on in this paper.
In theirwork, they simply attempt to ?synchronize?
un-known/malformed annotations with the morpho-logical analyzer they use, thus forcing a reading onthe word to make the unknown/malformed annota-tion usable.
In our work, we address the cleaningissue directly.
We intend to make these automaticcorrections and extensions available in the futureso that they can be used in future disambiguationtools.Maamouri et al(2009) described a set of man-ual and automatic techniques used to improve onthe quality of the Penn Arabic Treebank.
Theirwork is most similar to ours except in the follow-ing aspects: we work only on morphology and fordialectal Arabic, whereas their work is primarilyon syntax and standard Arabic.
Furthermore, thechallenge of malformed tags is not a major prob-lem for them, while it is a core problem for us.Furthermore, we work with data that has partialannotations that we extend, while their work wasfor very rich syntax/morphology annotations.3 Linguistic FactsThe Arabic language is a collection of variants,most prominent amongst which is Modern Stan-dard Arabic (MSA), the official language of themedia and education.
The other variants, the Ara-bic dialects, are the day-to-day native vernacularsspoken in the Arab World.
While MSA is the of-ficial language, it is not the native language of anymodern day Arabic speakers.
Their differencesfrom MSA are comparable to the differences be-tween Romance languages and Latin.2Egyptian Arabic poses many challenges forNLP.
Arabic in general is a morphologically com-plex language which includes rich inflectionalmorphology, expressed both templatically and af-fixationally, and several classes of attachable cl-itics.
For example, the Egyptian Arabic wordA??J.J?J??
wi+ha+yi-ktib-uw+hA3 ?and they willwrite it?
has two proclitics (+?
wi+ ?and?
and + ?ha+ ?will?
), one prefix -?yi- ?3rd person?, onesuffix ?- -uw ?masculine plural?
and one pronom-inal enclitic A?+ +hA ?it/her?.
The word is consid-ered an inflected form of the lemma katab ?write[lit.
he wrote]?.
An important challenge for NLPwork on dialectal Arabic in general is the lack ofan orthographic standard.
Egyptian Arabic writ-ers are often inconsistent even in their own writ-ing (Habash et al 2012a), e.g., the future particleh Ha appears as a separate word or as a proclitic+h/+?
Ha+/ha+, reflecting different pronuncia-tions.
Arabic orthography in general drops dia-critical marks that mark short vowels and gemi-nation.
However in analyses, we want these dia-critics to be indicated.
Moreover, some letters inArabic (in general) are often spelled inconsistentlywhich leads to an increase in both sparsity (multi-ple forms of the same word) and ambiguity (sameform corresponding to multiple words), e.g., vari-ants of Hamzated Alif,@ ?
or @A?, are often writ-ten without their Hamza (Z ?
): @ A; and the Alif-Maqsura (or dotless Ya) ?
?
and the regular dottedYa ?y are often used interchangeably in word fi-nal position (El Kholy and Habash, 2010).
For thepurposes of normalizing the representations usedin computational models, we follow the work ofHabash et al(2012a) who devised a conventionalorthography for dialectal Arabic (CODA) for usein computational processing of Arabic dialects..An analysis of an Egyptian word for our workconsists of a surface form that may not be in2Habash and Rambow (2006) reported that a state-of-the-art MSA morphological analyzer has only 60% coverage ofLevantine Arabic verb forms.3Arabic orthographic transliteration is presented in theHabash-Soudi-Buckwalter scheme (Habash et al 2007):@ H.H H h. h p XXP 	P ?
?
??
??
????
?
?
??
?
?
?A b t ?
j H x d?r z s ?
S D T D?
?
?
f q k l m n hw yin addition to ?
Z, ?
@, A?
@, A?@, w??
', y?
Z?
', ~ ?, ?
?.2CODA (henceforth, RAW), a fully diacritizedCODA form (henceforth, DIAC), a morphemesplit form (henceforth, MORPH), which mayslightly differ from the allomorphic DIAC surfaceforms, a POS tag for each morpheme and stem,and a lemma (henceforth LEM).
For instance,the Egyptian Arabic example used above has thefollowing analysis:RAW whyktbuwhADIAC wiHayiktibuwhAMORPH wi+Ha+yi+ktib+uwA+hAPOS CONJ+FUT_PART+IV3P+IV+IVSUFF_SUBJ:3P+IVSUFF_DO:3FSLEM katabThe morphological analyzers we use in the pa-per, CALIMA (Habash et al 2012b) and SAMA(Graff et al 2009), both generate the different lev-els of representation discussed above.4 Automatic Morphological CorrectionIn this section, we present the effort on auto-matic morphological correction of rich noisy an-notations.
We next describe the data set we workwith and the problems it has.
This is followed bya discussion of our approach and results includingan error analysis.4.1 DataWe use a non-final, pre-release version of six man-ually annotated Egyptian Arabic corpora devel-oped by the LDC, and labeled as ?ARZ?, parts onethrough six.
The published versions of these cor-pora (Maamouri et al 2012a-f) do not include theannotation errors discussed in this paper.
Rather,in the official releases of the data from the LDC,such problematic cases with an unknown POS tagsequence (as in the example at the end of Sec-tion 4.2) were caught and given a NO_FUNC POStag instead, in order to allow syntactic annotationof the data to proceed, and in order to meet datapublication deadlines.
The combined corpus con-sists of about 274K words.
The annotations arevery detailed contextually selected morphologicalanalyses that include for each RAW word its LEM,POS, MORPH and DIAC as described earlier.
TheLDC used the CALIMA4 Egyptian Arabic mor-phological analyzer (Habash et al 2012b) to pro-vide the annotators with sets of analyses to se-lect from.5 CALIMA?s non-lexical morphologi-4Columbia Arabic Language and dIalect MorphologicalAnalyzer5SAMA, the Standard Arabic Morphological Analyzer(Graff et al 2009), was used to provide the annotators withcal coverage (i.e.
model of affixes and stem POScombinations) is almost complete; and its lexicalentries are of high precision.
However, CALIMAlacks some lexical items, i.e., its lexical recall isnot perfect ?
Habash et al(2012b) report coverageof 84% for basic CALIMA and 92% for CALIMAextended with SAMA (Graff et al 2009) (hence-forth, CALIMA+SAMA or simply the analyzer).6Many missing entries are a result of spelling vari-ants that are not modeled in CALIMA.
In caseswhen CALIMA fails to provide analyses or theannotators disagree with all the provided analy-ses, the annotators enter the information manuallyor copy and modify CALIMA provided analyses,which sometimes introduces errors.For the purpose of this work, we considerall analyses in the corpus that are in the CAL-IMA+SAMA morphological analyzer to be cor-rect.
We will not attempt to modify them.
Al-most 30% of the corpus analyses are not in theanalyzer, i.e.
analyzer out-of-vocabulary (OOV).We discuss next the general patterns of these anal-yses.
We refer to the original corpus analyses asthe ?Baseline?
analyses.4.2 Patterns of OOV Analyses in BaselineAbout 3.3% of all OOV analyses (and 1% of allcorpus words) are tagged as TYPOs.7 We do notaddress these cases in this paper.Over half of the POS OOVs (56%) in thepre-release data involve a different category ofa nominal (NOUN/NOUN_PROP/ADJ).
This isa well known issue even in MSA.
The restof the cases involve incorrect feature combina-tions such as giving the unaccusative verbY?JK @Aitnaf?i?
?be performed?
the POS PV_PASS(passive perfective).8 Another example is assign-ing the feminine singular pronoun ?X diy thePOS DEM_PRON instead of DEM_PRON_FS.Or the imperative verb @?
??
@ AilguwA ?cancel [youplural]?
the POS CV+CVSUFF_SUBJ:2MS (for?you masculine singular?)
instead of the correctCV+CVSUFF_SUBJ:2MP.
A tiny percentage ofall POS tags in the corpus (0.02%) include case-related variation (e.g.
CONJ vs Conj); these addto type sparsity, but are trivial to handle.analyses for the MSA tokens.6In our work, we distinguish between morphological anal-ysis, which refers to producing the various readings of a wordout of context, and morphological tagging (or disambigua-tion), which identifies the appropriate analysis in context.7The rate of TYPO words in the ARZ data is almost 18times the rate in the MSA PATB data sets.8The inflected verb Aitnaf?i?
is the passive voice of theverb with the lemma naf?a?
or the active voice of the verbwith the lemma Aitnaf?i?.3Among LEMs and DIACs, there is consider-able variation in the Arabic spelling, particularlyinvolving the spelling of Alif/Hamza forms, theEgyptian long vowels /e:/ and /o:/ and often re-quiring adjustment to conform to CODA guide-lines.9 The following are some examples.
SpecificCODA cases include spelling ?Y?
kidah ?as such?as @Y?
kdA or spelling ???
qawiy [pronounced/awi/] ?very?
as ??
@ Awy.
The preposition ?J?
fiyh?in it?
is incorrectly spelled as fiyuh (allomorphicform is incorrect).
The word IK.
bayt ?house?
isspelled biyt (long vowel spelling error).
And fi-nally the interjectionB l?
?no!?
is spelled as (theimplausible form) Z?
la?.Among LEMs, over 63% of the errors is dueto inconsistency in assigning lemmas of punctu-ation and digit, a trivial challenge.
29% of thecases are spelling errors such as those discussedabove.
The remaining 10% are due to not follow-ing the specific format guidelines of lemmas (e.g.,must be singular, uncliticized, and with a sense idnumber).
Among DIACs, almost all of the mis-matches are non-CODA-compliant spelling varia-tions.
One third is Alif/Hamza forms, and anotherquarter is long vowel spelling.
One eighth involvesdiacritic choice.Combinations of these error types occur,of course.
One extreme case is the pro-gressive particle prefix bi, which should betagged as bi/PROG_PART, but appears addi-tionally as b/PROG_PART, ba/PROG_PART,bi/PART_PROG, bi/PRO_PART, andbi/FUT_PART.Example For the rest of this section, we con-sider the example word @?
?g.AJk Hy?jlwA ?andthey will postpone?.
Figure 1 contrasts an erro-neous analysis in the pre-release data with a cor-rected version of it.
There are multiple problemsin this example.
First, the POS tag is both in-ternally inconsistent and is inconsistent with theMORPH choice.
The POS has a singular subjectprefix (IV3MS) and a plural subject suffix (IV-SUFF_SUBJ:P); and the plural subject suffix iswritten using the morpheme (+uh), which corre-sponds to a direct object enclitic.
The two mor-phemes, +uh and +uwA, are homophonous, whichis the most likely cause for this error.
Second, thefuture marker (Ha+) is written in a non-CODA-9LDC annotators were not asked to comply with CODAguidelines during the annotation task.
Therefore, multiplespelling variants for OOV Egyptian Arabic words were to beexpected.compliant way (ha+) in the analysis.
And finally,the lemma is malformed, containing multiple ex-tra sense id digits.
It is important to point outthat there are multiple ways to correct the anal-ysis.
For example, it can be Ha+yi+?aj?il+uhFUT_PART+IV3MS+IV+IVSUFF_DO:3MS ?hewill postpone it?.104.3 ApproachOur target is to provide correct morphologicalanalyses for the OOV annotations in the pre-release version of the ARZ corpus.
Since notall of the OOV annotations are wrong in prin-ciple, we do not force map them all to CAL-IMA+SAMA in-vocabulary variants, especiallyfor open class categories, where we know CAL-IMA+SAMA may be deficient.
As such, our gen-eral solution focuses on correcting closed classes(some stems and all of the affixes) by mappingthem to in-vocabulary variants.
We also use a setof language-specific preprocessing corrections forcommon orthographic variations (for all open andclosed classes).
An important tool we use through-out to rank choices and break ties is modified Lev-enshtein edit distance.11Next, we present the four steps of our correctionprocess: annotation preprocessing, morpheme-POS correction, lemma correction and surfaceDIAC generation.Annotation Preprocessing When first readingthe pre-release annotations, we perform a prepro-cessing step that includes a set of deterministiccorrections for common non-CODA-compliant or-thographic variations and errors, and POS taggingtypos.
The corrections apply to the POS tags, lem-mas, morphemes and surface forms.
Examples ofthese corrections include the following: reorder-ing diacritics, e.g., saji?l?
saj?il; removing du-plicate diacritics, e.g., saj?iil?
saj?il; adjustingAlif-Hamza forms to match the diacritics that fol-10Since our approach currently considers words out of con-text, such a correction is not preferred because it requiresmore character edits (see Figure 2).
We acknowledge thisto be a limitation and plan to address it in the future.11The Levenshtein edit distance is defined as the minimumnumber of single-character edits (insertion, deletion and sub-stitution) required to change one string into the other.
ForArabic words and morphemes, we modify the cost of sub-stitutions involving two phonologically or orthographicallysimilar letters to count as half edits.
We acquire the list ofsuch letter substitutions from Eskander et al(2013), who re-port them as the most frequent source of errors in EgyptianArabic orthography.
We map all diacritic-only morphemesto empty morphemes in both ways at a cost of half edit also.For POS tag edit distance, we use the standard definition ofLevenshtein edit distance.
Edit cost is an area where a lot oftuning could be done and we plan to explore it in the future.4RAW ??g.
AJ?
hyAjlwAnalysis Incorrect Annotation Correct AnnotationDIAC hayi?aj?iluh Hayi?aj?iluwAMORPH ha+yi+?aj?il+uh Ha+yi+?aj?il+uwAPOS FUT_PART+IV3MS+IV+IVSUFF_SUBJ:P FUT_PART+IV3P+IV+IVSUFF_SUBJ:PLEM ?aj?ill1 ?aj?il_1Figure 1: An incorrect annotation example with a possible correction.low them, e.g., A?aSl?
?aSl; and POS tag capital-ization, e.g., Fut_Part?
FUT_PART.Morpheme-POS Correction For morphemecorrection purposes, we define an abstract rep-resentation that combines all the closed-classmorphemes and POS tags.
For open-classstems, we simply use the POS tag.
For exam-ple, the abstract morpheme representation forthe correct version of the word in Figure 1 isHa/FUT_PART+yi/IV3P+IV+uwA/IVSUFF_SUBJ:P.We will refer to this representation as the inflec-tional morph-tag (IMT).We build two models for this task.
First, webuild an IMT language model from the CAL-IMA+SAMA databases.
This models all possibleinflections in the analyzer without the open classstems.
This model includes 304K sequences.
Sec-ond, we construct a map from all the seen IMTsin the ARZ corpus to all the in-vocabulary IMTsin the IMT language model.
The mapping in-cludes a cost that is based on the edit distance dis-cussed earlier.
Figure 2 shows the top mappingsfor the IMTs in our example.
Both models are im-plemented as finite state machines using the ATTFSM toolkit (Mohri et al 1998).The input, possibly incorrect, IMT is con-verted into an FSM that is then composedwith the mapping transducer and the languagemodel automaton to generate a cost-ranked listof mappings.
The output for our example islisted in Figure 3.
We then replace the inputPOS and MORPH with the top ranked correction:Ha/FUT_PART+yi/IV3MS+IV+uh/IVSUFF_SUBJ:Pat a cost of 4.0.
The open class stem is not modi-fied.Lemma Correction We generate a map that in-cludes all the possible lemmas for every possi-ble stem morpheme in CALIMA+SAMA.
For agiven ARZ word analysis, if the stem morphemeis in CALIMA+SAMA, then we pick the lemmafrom its corresponding lemma set.
When there ismore than one possible lemma, we pick the lemmathat is closest to the provided pre-release ARZBase IMTMorphemeMapped IMTMorphemes Costha/FUT_PART Ha/FUT_PART 0.5sa/FUT_PART 1.0yi/IV3MSyi/IV3MS 0.0ya/IV3MS 1.0y/IV3MS 1.0yu/IV3MS 1.0yi/IV3P 2.0IVIV 0.0PV 1.0CV 1.0uh/IVSUFF_SUBJ:P uwA/IVSUFF_SUBJ:P 1.5na/IVSUFF_SUBJ:FP 3.0Figure 2: Top mappings for the IMT morphemesha/FUT_PART, yi/IV3P, IV and uh/IVSUFF_SUBJ:PInput: ha/FUT_PART+yi/IV3P+IV+uh/IVSUFF_SUBJ:PFSM Output CostHa/FUT_PART+yi/IV3P+IV+uwA/IVSUFF_SUBJ:P 4.0Ha/FUT_PART+y/IV3P+IV+uwA/IVSUFF_SUBJ:P 5.0Ha/FUT_PART+ti/IV2P+IV+uwA/IVSUFF_SUBJ:P 6.0Ha/FUT_PART+yi/IV3MS+IV+uh/IVSUFF_DO:3MS 6.5Ha/FUT_PART+yi/IV3MS+IV+kuw/IVSUFF_DO:2P 7.0Ha/FUT_PART+yi/IV3MS+IV+nA/IVSUFF_DO:1P 7.0Ha/FUT_PART+tu/IV2P+IV+uwA/IVSUFF_SUBJ:P 7.0sa/FUT_PART+ya/IV3FP+IV+na/IVSUFF_SUBJ:FP 7.0sa/FUT_PART+yu/IV3FP+IV+na/IVSUFF_SUBJ:FP 7.0Ha/FUT_PART+yi/IV3MS+IV+kum/IVSUFF_DO:2P 7.5Figure 3: Top corrections for the inputha/FUT_PART+yi/IV3P+IV+uh/IVSUFF_SUBJ:Plemma, based on their string edit distance as de-fined earlier.
If the stem morpheme is not in CAL-IMA+SAMA (e.g., open class), then we keep theARZ lemma as it is.In our example, the stem morpheme ?aj?il/IVis paired in CALIMA+SAMA with the lemma?aj?il_1.
Accordingly, ?aj?il_1 replaces the in-put pre-release ARZ lemma.Surface DIAC Generation After correcting themorphemes and POS tags in the input word,we use them to generate a new surface DIACform.
For all the closed-class morphemes andin-vocabulary open-class stems, we use CAL-IMA+SAMA to identify all the MORPH+POS toDIAC mappings.
For open-class stems that are5OOVs, we use their corresponding DIAC form inthe input word.12 This may lead to many possiblesequences.
We rank them by their edit distance(defined above) to the surface DIAC of the inputword.In our example, this process is rather trivial:every morpheme is paired with only one surfaceDIAC in the morphological analyzer.
The surfaceDIACs corresponding to Ha/FUT_PART, yi/IV3P,?aj?il/IV and uwA/IVSUFF_SUBJ:P are Ha, yi,?aj?il and uwA, respectively.
The final combinedsurface is Hayi?aj?iluwA.A more interesting example is the word A 	JJ??
?alay+nA ?upon us?
which has the analysis?ala?/PREP+nA/PRON_1P.
The MORPH stem?ala?
has two DIAC forms: ?ala?
and ?alay.
Thesecond form is only used when an enclitic ispresent.
It is selected in this example because ithas a smaller edit distance to the full word inputDIAC form than the surface stem ?ala?.
In thefuture, we plan to use more sophisticated genera-tion and detokenization techniques (El Kholy andHabash, 2010).4.4 Results and Error AnalysisResults We conducted a manual evaluation for1,000 words from the internal, pre-release ARZafter applying the automatic correction process.This set is a blind test set, i.e., not used as partof the development.
The results are listed in Ta-ble 1 for the lemmas, POS tags, diacritized mor-phemes and diacritized surface forms, in additionto the complete morphological analyses (token-based), where the correction output is comparedto the pre-release ARZ annotations (the baseline).The results are listed for different subsets ofthe data.
The first row lists the results consider-ing the complete 1,000 words, where all the in-vocabulary words are considered correct.
This isonly intended to give an overall estimate of thecorrectness of the set.
The second row lists the re-sults for CALIMA+SAMA OOV words only.
Thethird row is the same as the second, but exclud-ing punctuations, digits and typos.
Focusing onthe last row, we see that we achieve between 58%and 24% error reduction on different features, andreach almost 40% error reduction on all featurescombined.Error Analysis For POS, 99.7% of all the cor-rect cases in the Baseline were not changed.
Only12Since the surface DIAC splits are not provided, we deter-mine the exact boundary of the surface DIAC stem by mini-mizing the edit distance between the prefixing/suffixing mor-phemes and the full input surface DIAC form.one case was changed and it was caused by an er-ror in the input MORPH splits.
Of the erroneouscases in the Baseline, 40% were not changed.Among the attempted changes, 71% successfullyfixed the baseline problem.
Almost all of the failedchanges are due to implausible null pronouns inthe Baseline that were not handled in the cur-rent implementation, which only considered cor-rect null pronouns.
We plan to address these inthe future.
Among the errors that were not ad-dressed, the most common case involves nominalform (41%) followed by hard features to resolveand open class passive-voice inconsistency (each27%).Regarding lemmas, 93.9% of all correct base-line lemmas remained correct.
In the rest, over-correction attempts resulting from matching theOOV lemma to the wrong in-vocabulary lemmabackfired.
Around 8.7% of the erroneous baselinelemmas were not modified and 1.6% were mod-ified incorrectly.
The rest, 92.8%, were success-fully fixed.
Almost all of the system errors result-ing from changes involve over correction by map-ping to incorrect INV lemma forms.Finally, as for diacritized forms, 96.9% of thecorrect baseline DIACs remained correct; the restfell victim to over-correction.
Among incorrectbaseline cases, 43% remained unchanged; and45% were fixed; 4% were over-corrected and 8%only partially corrected.
Remaining DIAC errorsare mostly in open classes where the analyzer re-call problems cannot help.5 Automatic Morphological ExtensionIn this section, we present the general techniquewe use to extend shallow annotations.
We discussthe data sets, the approach and evaluation resultsnext.5.1 DataWe conduct our experiments on two differ-ent Egyptian Arabic corpora: the CALLHOMEEgypt (CHE) corpus (Gadalla et al 1997) andCarnegie Mellon University Egyptian Arabic cor-pus (CMUEAC) (Mohamed et al 2012).CHE The CHE corpus contains 140 telephoneconversation transcripts of about 179K words.Each word is represented by its phonological formand undiacritized Arabic script orthography.
Theorthography used is quite similar to the CODAstandard we use.
Being a transcript corpus, it isquite clean and free of spelling variations.
We usea technique described in more detail in Habash et6POSLEM POS MORPH DIAC +MORPH AllAll wordsBaseline 79.8% 93.2% 92.2% 91.1% 87.3% 72.7%System 95.7% 95.5% 93.8% 93.6% 91.5% 90.0%Analyzer OOVBaseline 47.1% 82.4% 79.7% 76.8% 66.8% 28.4%System 88.9% 88.42% 83.9% 83.4% 77.9% 73.9%Analyzer OOV, no Baseline 71.3% 82.5% 74.1% 69.7% 59.0% 43.0%Punc/Digit/Typos System 88.0% 87.3% 80.5% 79.7% 71.3% 65.3%Table 1: Accuracy of the automatic morphological correction of internal, pre-release ARZ data.al.
(2012b) to combine the phonological form andundiacritized Arabic script into diacritized Arabicscript, i.e.
DIAC.
For example, the undiacritizedword ?
JJ?
?ynh ?his eye?
is combined with its pro-nunciation /?e:nu/ producing the diacritized form?aynuh.CMUEAC The CMUEAC corpus includesabout 23K words that are only annotated formorph splits.
The corpus text includes sponta-neously written Egyptian Arabic text collected offthe web.
To use the same example as above, theword ?
JJ?
?ynh ?his eye?
is segmented as ?yn+h in-dicating that there is a base word plus an enclitic.5.2 ApproachOur approach to morphological extension is to au-tomatically annotate the corpus using a very richmorphological tagger, and then use the limitedmanual annotations to adjust the morphologicalchoice.
We use a morphological tagger, MADA-ARZ (Morphological Analysis and Disambigua-tion for Egyptian Arabic) (Habash et al 2013).MADA-ARZ produces, for each input word, acontextually ranked list of analyses specifying allthe morphological interpretations of that word asprovided by the CALIMA+SAMA morphologicalanalyzer.CHE In the case of CHE, we select the firstchoice from the ranked list of analyses whoseDIAC matches the diacritized word in CHE.
Forexample, for the word ?
JJ?
?ynh MADA-ARZgenerates 45 different morphological analyseswith different lemmas, POS, orthographies anddiacritics: ?ayn+uh ?his eye?, ?ay?in+a~ ?sam-ple?
and ?ay?in+uh ?he appointed him?.
Thediacritized word ?ayn+uh allows us to select thefollowing full analysis:Metric CHE CMUEACLEM 97.2 82.0POS 95.2 79.6MORPH 96.8 77.6DIAC 97.2 78.4POS+MORPH 92.8 74.0All 92.8 72.0Table 2: Accuracy of automatic morphological ex-tension of CHE and CMUEAC.RAW EynhDIAC EaynuhMORPH Eayn+uhPOS NOUN+POSS_PRON_3MSLEM Eayn_1Although this example may not require the fullpower of a tagger, but just the out-of-context an-alyzer, other cases involving POS ambiguity un-realized through diacritization necessitate the useof a tagger, e.g., the word I.KA?
kAtib can bean ADJ meaning ?writing?
or a NOUN meaning?writer/author?.CMUEAC In the case of CMUEAC, we se-lect the first choice from the ranked list of anal-yses whose undiacritized MORPH splits match theword tokenization.
In the case of the word ?
JJ?
?yn+h, the tokenization cannot distinguish be-tween the noun reading ?ayn+uh ?his eye?
andthe verbal reading ?ay?in+uh ?he appointed him?.MADA-ARZ effectively selects in such cases.We expect the performance on CMUEAC tobe worse than CHE given the difference in theamount of information between the two corpora.5.3 Results and Error AnalysisWe evaluate the accuracy of the morphologicalextension process on both CHE and CMUEACusing two 300 word samples that were manu-ally enriched.
Table 2 presents the accuracies ofthe assigned LEMs, POS tags, DIAC forms and7MORPHs, in addition to the complete morpholog-ical analysis.
All results are token-based.CHE CHE analyses have high accuracies rang-ing between 95.2% and 97.2% for the differentanalysis features, with the complete analysis hav-ing an accuracy of 92.8%.
One third of the er-rors is due to gold diacritization errors in theCHE corpus.
28% of the errors are due to wrongverbal features (person, number and gender) forforms that are not distinguishable in DIAC, e.g.,I.J?
katabt ?I/you wrote?
and I.J?K tiktib ?youwrite/she writes?.
The rest of the errors are be-cause of failure in assigning the correct POS tagsfor nouns, particles and verbs with percentages of22%, 11% and 6%, respectively.CMUEAC CMUEAC analyses have muchlower accuracies compared to CHE, ranging be-tween 77.6% and 82.0% for different features,with the complete analysis accuracy at 72.0%.
TheCMUEAC is much harder to extend for two rea-sons: the text, being naturally occurring, con-tains a lot of orthographic noise; and tokeniza-tion information is not sufficient to disambiguatemany analyses.
For CMUEAC, a quarter of theerrors is due to gold tokenization errors in theoriginal CMUEAC corpus.
Another quarter ofthe errors results from MADA-ARZ assigning anMSA analysis instead of an Egyptian Arabic anal-ysis.13 Failure to assign the correct POS tags forparticles, verbs and nouns represents 14%, 10%and 7% of the errors, respectively.
Other errorsare because of wrong verbal features (13%) andwrong diacritization (6%).As expected, relatively richer annotations (i.e.,diacritics) are easier to extend to full morpholog-ical information that relatively poorer annotations(i.e., tokenization).
Of course, the tradeoff is stillthere as tokenizations are much easier and cheaperto annotate.
We plan to explore the question ofwhat would be an optimal set of poor annotationsthat can help us extend to the full morphology athigh accuracy in the future.6 Egyptian CorpusAfter applying morphological corrections to pre-release ARZ and morphological extensions toCHE and CMUEAC, we have now three big cor-pora that are automatically adjusted to includethe same rich morphological information, that is:13MADA-ARZ is trained on a combination of MSA andEgyptian Arabic text and as such may select an MSA analysisin cases that are ambiguous.lemma, POS tag, diacritized morphemes, and dia-critized surface.
We combine the three resourcestogether in one morphologically rich corpus thatcontains about 46K sentences and 447K words,representing 61K unique lemmas.
We intend tomake these automatic corrections and extensionsavailable in the future to provide extensive sup-port for Egyptian Arabic processing for differentpurposes.7 Conclusion and Future WorkWe presented two methods for automatic correc-tion and extension of morphological annotationsand demonstrated their success on three differentEgyptian Arabic corpora, which now have annota-tions that are automatically adjusted to include thesame rich morphological information although atdifferent degrees of quality that correspond to theamount of initial information.We presented two methods for automatic cor-rection and extension of morphological annota-tions and demonstrated their success on three dif-ferent Egyptian Arabic corpora, which now haveannotations that are automatically adjusted to in-clude the same rich morphological information al-though at different degrees of quality that corre-spond to the amount of initial information.In the future, we plan to study how to optimizethe amount of basic information to annotate man-ually in order to maximize the benefit of auto-matic extensions.
We also plan to provide feed-back to the annotation process to reduce the per-centage of errors generated by the annotators, per-haps through a tighter integration of the correc-tion/extension techniques with the annotation pro-cess.
We also plan on using the cleaned up corpusto extend the existing analyzer for Egyptian Ara-bic.AcknowledgmentThis paper is based upon work supported bythe Defense Advanced Research Projects Agency(DARPA) under contracts No.
HR0011-12-C-0014 and HR0011-11-C-0145.
Any opinions,findings and conclusions or recommendations ex-pressed in this paper are those of the authors anddo not necessarily reflect the views of DARPA.
Wealso would like to thank Emad Mohamed and Ke-mal Oflazer for providing us with the CMUEACcorpus.
We thank Ryan Roth for help withMADA-ARZ.
Finally, we thank Owen Rambow,Mona Diab and Warren Churchill for helpful dis-cussions.8ReferencesSarah Alkuhlani and Nizar Habash.
2011.
A Corpusfor Modeling Morpho-Syntactic Agreement in Ara-bic: Gender, Number and Rationality.
In Proceed-ings of the 49th Annual Meeting of the Associationfor Computational Linguistics (ACL?11), Portland,Oregon, USA.Sarah Alkuhlani, Nizar Habash, and Ryan Roth.
2013.Automatic morphological enrichment of a morpho-logically underspecified treebank.
In Proceedings ofthe 2013 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, pages 460?470, At-lanta, Georgia, June.
Association for ComputationalLinguistics.Ahmed El Kholy and Nizar Habash.
2010.
Techniquesfor Arabic Morphological Detokenization and Or-thographic Denormalization.
In Proceedings of theseventh International Conference on Language Re-sources and Evaluation (LREC), Valletta, Malta.Ramy Eskander, Nizar Habash, Owen Rambow, andNadi Tomeh.
2013.
Processing spontaneous orthog-raphy.
In Proceedings of the 2013 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 585?595, Atlanta, Georgia, June.Association for Computational Linguistics.Hassan Gadalla, Hanaa Kilany, Howaida Arram,Ashraf Yacoub, Alaa El-Habashi, Amr Shalaby,Krisjanis Karins, Everett Rowson, Robert MacIn-tyre, Paul Kingsbury, David Graff, and CynthiaMcLemore.
1997.
CALLHOME Egyptian Ara-bic Transcripts.
In Linguistic Data Consortium,Philadelphia.David Graff, Mohamed Maamouri, Basma Bouziri,Sondos Krouna, Seth Kulick, and Tim Buckwal-ter.
2009.
Standard Arabic Morphological Analyzer(SAMA) Version 3.1.
Linguistic Data ConsortiumLDC2009E73.Nizar Habash and Owen Rambow.
2005.
Arabic Tok-enization, Part-of-Speech Tagging and Morphologi-cal Disambiguation in One Fell Swoop.
In Proceed-ings of the 43rd Annual Meeting of the Associationfor Computational Linguistics (ACL?05), pages 573?580, Ann Arbor, Michigan.Nizar Habash and Owen Rambow.
2006.
MAGEAD:A Morphological Analyzer and Generator for theArabic Dialects.
In Proceedings of the 21st Interna-tional Conference on Computational Linguistics and44th Annual Meeting of the Association for Compu-tational Linguistics, pages 681?688, Sydney, Aus-tralia.Nizar Habash, Abdelhadi Soudi, and Tim Buckwalter.2007.
On Arabic Transliteration.
In A. van denBosch and A. Soudi, editors, Arabic Computa-tional Morphology: Knowledge-based and Empiri-cal Methods.
Springer.Nizar Habash, Mona Diab, and Owen Rabmow.
2012a.Conventional Orthography for Dialectal Arabic.
InProceedings of the Language Resources and Evalu-ation Conference (LREC), Istanbul.Nizar Habash, Ramy Eskander, and Abdelati Hawwari.2012b.
A Morphological Analyzer for EgyptianArabic.
In NAACL-HLT 2012 Workshop on Com-putational Morphology and Phonology (SIGMOR-PHON2012), pages 1?9, Montr?al, Canada.Nizar Habash, Ryan Roth, Owen Rambow, Ramy Es-kander, and Nadi Tomeh.
2013.
MorphologicalAnalysis and Disambiguation for Dialectal Arabic.In Proceedings of the 2013 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies(NAACL-HLT), Atlanta, GA.Nizar Habash.
2010.
Introduction to Arabic NaturalLanguage Processing.
Morgan & Claypool Publish-ers.Jan Hajic?, Otakar Smr?, Tim Buckwalter, and HubertJin.
2005.
Feature-based tagger of approximationsof functional Arabic morphology.
In Proceedings ofthe Workshop on Treebanks and Linguistic Theories(TLT), Barcelona, Spain.Ahmed Hassan, Sara Noeman, and Hany Hassan.2008.
Language Independent Text Correction us-ing Finite State Automata.
In Proceedings of the In-ternational Joint Conference on Natural LanguageProcessing (IJCNLP 2008).Eduard Hovy, Mitchell Marcus, Martha Palmer,Lance Ramshaw, and Ralph Weischedel.
2006.OntoNotes: The 90% Solution.
In NAACL ?06: Pro-ceedings of the Human Language Technology Con-ference of the NAACL, Companion Volume: ShortPapers on XX, pages 57?60, Morristown, NJ, USA.Okan Kolak and Philip Resnik.
2002.
OCR error cor-rection using a noisy channel model.
In Proceed-ings of the second international conference on Hu-man Language Technology Research.Karen Kukich.
1992.
Techniques for AutomaticallyCorrecting Words in Text.
ACM Computing Sur-veys, 24(4).Mohamed Maamouri, Ann Bies, Tim Buckwalter, andWigdan Mekki.
2004.
The Penn Arabic Treebank:Building a Large-Scale Annotated Arabic Corpus.In NEMLAR Conference on Arabic Language Re-sources and Tools, pages 102?109, Cairo, Egypt.Mohamed Maamouri, Ann Bies, and Seth Kulick.2009.
Creating a methodology for large-scale cor-rection of treebank annotation: The case of the ara-bic treebank.
In MEDAR Second International Con-ference on Arabic Language Resources and Tools,Egypt.
Citeseer.Mohamed Maamouri, Ann Bies, Seth Kulick, SondosKrouna, Dalila Tabassi, and Michael Ciul.
2012a.Egyptian Arabic Treebank DF Part 1 V2.0.
LDCcatalog number LDC2012E93.Mohamed Maamouri, Ann Bies, Seth Kulick, SondosKrouna, Dalila Tabassi, and Michael Ciul.
2012b.Egyptian Arabic Treebank DF Part 2 V2.0.
LDCcatalog number LDC2012E98.9Mohamed Maamouri, Ann Bies, Seth Kulick, SondosKrouna, Dalila Tabassi, and Michael Ciul.
2012c.Egyptian Arabic Treebank DF Part 3 V2.0.
LDCcatalog number LDC2012E89.Mohamed Maamouri, Ann Bies, Seth Kulick, SondosKrouna, Dalila Tabassi, and Michael Ciul.
2012d.Egyptian Arabic Treebank DF Part 4 V2.0.
LDCcatalog number LDC2012E99.Mohamed Maamouri, Ann Bies, Seth Kulick, SondosKrouna, Dalila Tabassi, and Michael Ciul.
2012e.Egyptian Arabic Treebank DF Part 5 V2.0.
LDCcatalog number LDC2012E107.Mohamed Maamouri, Ann Bies, Seth Kulick, SondosKrouna, Dalila Tabassi, and Michael Ciul.
2012f.Egyptian Arabic Treebank DF Part 6 V2.0.
LDCcatalog number LDC2012E125.Mohamed Maamouri, Sondos Krouna, Dalila Tabessi,Nadia Hamrouni, and Nizar Habash.
2012g.
Egyp-tian Arabic Morphological Annotation Guidelines.Walid Magdy and Kareem Darwish.
2006.
Ara-bic OCR Error Correction Using Character SegmentCorrection, Language Modeling, and Shallow Mor-phology.
In Proceedings of 2006 Conference onEmpirical Methods in Natural Language Processing(EMNLP 2006), pages 408?414, Sydney, Austrailia.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: The Penn treebank.
Computa-tional Linguistics, 19(2):313?330, June.Emad Mohamed, Behrang Mohit, and Kemal Oflazer.2012.
Annotating and Learning Morphological Seg-mentation of Egyptian Colloquial Arabic.
In Pro-ceedings of the Language Resources and EvaluationConference (LREC), Istanbul.Mehryar Mohri, Fernando C. N. Pereira, and MichaelRiley.
1998.
A rational design for a weighted finite-state transducer library.
In D. Wood and S. Yu, ed-itors, Automata Implementation, Lecture Notes inComputer Science 1436, pages 144?58.
Springer.Kemal Oflazer.
1996.
Error-tolerant finite-state recog-nition with applications to morphological analysisand spelling correction.
Computational Linguistics,22:73?90.Martha Palmer, Olga Babko-Malaya, Ann Bies, MonaDiab, Mohamed Maamouri, Aous Mansouri, andWajdi Zaghouani.
2008.
A Pilot Arabic Prop-bank.
In Proceedings of LREC, Marrakech, Mo-rocco, May.Khaled Shaalan, Amin Allam, and Abdallah Gomah.2003.
Towards Automatic Spell Checking for Ara-bic.
In Conference on Language Engineering,ELSE, Cairo, Egypt.Noah Smith, David Smith, and Roy Tromble.
2005.Context-Based Morphological Disambiguation withRandom Fields.
In Proceedings of the 2005 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP05), pages 475?482, Vancou-ver, Canada.10
