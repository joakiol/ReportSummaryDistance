Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 430?439,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsFast Online Lexicon Learning for Grounded Language AcquisitionDavid L. ChenDepartment of Computer ScienceThe University of Texas at Austin1616 Guadalupe, Suite 2.408Austin, TX 78701, USAdlcc@cs.utexas.eduAbstractLearning a semantic lexicon is often an impor-tant first step in building a system that learnsto interpret the meaning of natural language.It is especially important in language ground-ing where the training data usually consist oflanguage paired with an ambiguous perceptualcontext.
Recent work by Chen and Mooney(2011) introduced a lexicon learning methodthat deals with ambiguous relational data bytaking intersections of graphs.
While the al-gorithm produced good lexicons for the task oflearning to interpret navigation instructions, itonly works in batch settings and does not scalewell to large datasets.
In this paper we intro-duce a new online algorithm that is an orderof magnitude faster and surpasses the state-of-the-art results.
We show that by changingthe grammar of the formal meaning represen-tation language and training on additional datacollected from Amazon?s Mechanical Turk wecan further improve the results.
We also in-clude experimental results on a Chinese trans-lation of the training data to demonstrate thegenerality of our approach.1 IntroductionLearning to understand the semantics of human lan-guages has been one of the ultimate goals of naturallanguage processing (NLP).
Traditional learning ap-proaches have relied on access to parallel corpora ofnatural language sentences paired with their mean-ings (Mooney, 2007; Zettlemoyer and Collins, 2007;Lu et al, 2008; Kwiatkowski et al, 2010).
How-ever, constructing such semantic annotations can bedifficult and time-consuming.
More recently, therehas been work on learning from ambiguous super-vision where a set of potential sentence meaningsare given, only one (or a small subset) of which arecorrect (Chen and Mooney, 2008; Liang et al, 2009;Bordes et al, 2010; Chen andMooney, 2011).
Giventhe training data, the system needs to infer the cor-recting meaning for each training sentence.Building a lexicon of the formal meaning repre-sentations of words and phrases, either implicitlyor explicitly, is usually an important step in infer-ring the meanings of entire sentences.
In particu-lar, Chen and Mooney (2011) first learned a lexiconto help them resolve ambiguous supervision of re-lational data in which the number of choices is ex-ponential.
They represent the perceptual context asa graph and allow each sentence in the training datato align to any connected subgraph.
Their lexiconlearning algorithm finds the common connected sub-graph that occurs with a word by taking intersectionsof the graphs that represent the different contexts inwhich the word appears.
While the algorithm pro-duced a good lexicon for their application of learn-ing to interpret navigation instructions, it only worksin batch settings and does not scale well to largedatasets.
In this paper we introduce a novel onlinealgorithm that is an order of magnitude faster andalso produces better results on their navigation task.In addition to the new lexicon learning algorithm,we also look at modifying the meaning representa-tion grammar (MRG) for their formal semantic lan-guage.
By using a MRG that correlates better to thestructure of natural language, we further improve theperformance on the navigation task.
Since our al-430gorithm can scale to larger datasets, we present re-sults on collecting and training on additional datafrom Amazon?s Mechanical Turk.
Finally, we showthe generality of our approach by demonstrating oursystem?s ability to learn from a Chinese translationof the training data.2 BackgroundA common way to learn a lexicon across many dif-ferent contexts is to find the common parts of the for-mal representations associated with different occur-rences of the same words or phrases (Siskind, 1996).For graphical representations, this involves find-ing the common subgraph between multiple graphs(Thompson and Mooney, 2003; Chen and Mooney,2011).
In this section we review the lexicon learningalgorithm introduced by Chen and Mooney (2011)as well as the overall task they designed to test se-mantic understanding of navigation instructions.2.1 Navigation TaskThe goal of the navigation task is to build a sys-tem that can understand free-form natural-languageinstructions and follow them to move to the in-tended destination (MacMahon et al, 2006; Shimizuand Haas, 2009; Matuszek et al, 2010; Kollar etal., 2010; Vogel and Jurafsky, 2010; Chen andMooney, 2011).
Chen and Mooney (2011) de-fined a learning task in which the only supervi-sion the system receives is in the form of observ-ing how humans behave when following samplenavigation instructions in a virtual world.
For-mally, the system is given training data in theform: {(e1, a1, w1), (e2, a2, w2), .
.
.
, (en, an, wn)},where ei is a written natural language instruction, aiis an observed action sequence, and wi is a descrip-tion of the virtual world.
The goal is then to build asystem that can produce the correct aj given a pre-viously unseen (ej , wj) pair.Since the observed actions ai only consists oflow-level actions (e.g.
turn left, turn right, walk for-ward) and not high-level concepts (e.g.
turn yourback against the wall and walk to the couch), Chenand Mooney first use a set of rules to automaticallyconstruct the space of reasonable plans using the ac-tion trace and knowledge about the world.
The spaceis represented compactly using a graph as shown inFigure 1: Examples of landmarks plans constructed byChen and Mooney (2011) and how they computed the in-tersection of two graphs.Figure 1.
This is what they called a landmarks planand consists of the low-level observed actions in-terleaved with verification steps indicating what ob-jects should be observed after each action.Given that these landmarks plans contain a lot ofextraneous details, Chen andMooney learn a lexiconand use it to identify and remove the irrelevant partsof the plans.
They use a greedy method to removenodes from the graphs that are not associated withany of the words in the instructions.
The remain-ing refined landmarks plans are then treated as su-pervised training data for a semantic-parser learner,KRISP (Kate and Mooney, 2006).
Once a seman-tic parser is trained, it can be used at test time totransform novel instructions into formal navigationplans which are then carried out by a virtual robot(MacMahon et al, 2006).2.2 Lexicon LearningThe central component of the system is the lexi-con learning process which associates words andshort phrases (n-grams) to their meanings (con-nected graphs).
To learn the meaning of an n-gramw, Chen and Mooney first collect all navigationplans g that co-occur with w. This forms the ini-tial candidate meaning set for w. They then repeat-edly take the intersections between the candidatemeanings to generate additional candidate mean-ings.
They use the term intersection to mean a max-imal common subgraph (i.e.
it is not a subgraph ofany other common subgraphs).
In general, there are431multiple possible intersections between two graphs.In this case, they bias toward finding large connectedcomponents by greedily removing the largest com-mon connected subgraph from both graphs until thetwo graphs have no overlapping nodes.
The out-put of the intersection process consists of all the re-moved subgraphs.
An example of the intersectionoperation is shown in Figure 1.Once the list of candidate meanings are generated,they are ranked by the following scoring metric foran n-gram w and a graph g:Score(w, g) = p(g|w) ?
p(g|?w)Intuitively, the score measures how much morelikely a graph g appears whenw is present comparedto when it is not.
The probabilities are estimated bycounting how many examples contain the word w orgraph g, ignoring multiple occurrences in a singleexample.3 Online Lexicon Learning AlgorithmWhile the algorithm presented by Chen and Mooney(2011) produced good lexicons, it only works inbatch settings and does not scale well to largedatasets.
The bottleneck of their algorithm is the in-tersection process which is time-consuming to per-form.
Moreover, their algorithm requires takingmany intersections between many different graphs.Even though they use beam-search to limit the sizeof the candidate set, if the initial candidate meaningset for a n-gram is large, it can take a long time totake just one pass through the list of all candidates.Moreover, reducing the beam size could also hurt thequality of the lexicon learned.In this section, we present another lexicon learn-ing algorithm that is much faster and works in an on-line setting.
The main insight is that most words orshort phrases correspond to small graphs.
Thus, weconcentrate our attention on only candidate mean-ings that are less than a certain size.
Using this con-straint, we generate all the potential small connectedsubgraphs for each navigation plan in the trainingexamples and discard the original graph.
Pseudo-code for the new algorithm, Subgraph GenerationOnline Lexicon Learning (SGOLL) algorithm, isshown in Algorithm 1.As we encounter each new training examplewhich consists of a written navigation instructionAlgorithm 1 SUBGRAPH GENERATION ONLINELEXICON LEARNING (SGOLL)input A sequence of navigation instructionsand the corresponding navigation plans(e1, p1), .
.
.
, (en, pn)output Lexicon , a set of phrase-meaning pairs1: main2: for training example (ei, pi) do3: Update((ei, pi))4: end for5: OutputLexicon()6: end main7:8: function Update(training example (ei, pi))9: for n-gram w that appears in ei do10: for connected subgraph g of pi such thatthe size of g is less than or equal to m do11: Increase the co-occurrence count of gand w by 112: end for13: end for14: Increase the count of examples, each n-gramw and each subgraph g15: end function16:17:18: function OutputLexicon()19: for n-gram w that has been observed do20: if Number of times w has been observed isless than minSup then21: skip w22: end if23: for subgraph g that has co-occurred with wdo24: if score(w, g) > threshold t then25: add (w, g) to Lexicon26: end if27: end for28: end for29: end function432and the corresponding navigation plan, we first seg-ment the instruction into word tokens and constructn-grams from them.
From the corresponding navi-gation plan, we find all connected subgraphs of sizeless than or equal to m. We then update the co-occurrence counts between all the n-grams w andall the connected subgraphs g. We also update thecounts of how many examples we have encounteredso far and counts of the n-grams w and subgraphsg.
At any given time, we can compute a lexiconusing these various counts.
Specifically, for eachn-gram w, we look at all the subgraphs g that co-occurred with it, and compute a score for the pair(w, g).
If the score is higher than the threshold t, weadd the entry (w, g) to the lexicon.
We use the samescoring function as Chen and Mooney, which can becomputed efficiently using the counts we keep.
Incontrast to Chen and Mooney?s algorithm though,we add the constraint of minimum support by notcreating lexical entries for any n-gram w that ap-peared in less than minSup training examples.
Thisis to prevent rarely seen n-grams from receiving highscores in our lexicon simply due to their sparsity.Unless otherwise specified, we compute lexical en-tries for up to 4-grams with threshold t = 0.4, max-imum subgraph size m = 3, and minimum supportminSup = 10.It should be noted that SGOLL can also becomecomputationally intractable if the sizes of the nav-igations plans are large or if we set the maximumsubgraph size m to a large number.
Moreover, thememory requirement can be quite high if there aremany different subgraphs g associated with each n-gram w. To deal with such scalability issues, wecould use beam-search and only keep the top k can-didates associated with each w. Another importantstep is to define canonical orderings of the nodes inthe graphs.
This allows us to determine if two graphsare identical in constant time and also lets us use ahash table to quickly update the co-occurrence andsubgraph counts.
Thus, even given a large numberof subgraphs for each training example, each sub-graph can be processed very quickly.
Finally, thisalgorithm readily lends itself to being parallelized.Each processor would get a fraction of the trainingdata and compute the counts individually.
Then thecounts can be merged together at the end to producethe final lexicon.3.1 Changing the Meaning RepresentationGrammarIn addition to introducing a new lexicon learningalgorithm, we also made another modification tothe original system proposed by Chen and Mooney(2011).
To train a semantic parser using KRISP(Kate and Mooney, 2006), they had to supply aMRG, a context-free grammar, for their formal nav-igation plan language.
KRISP learns string-kernelclassifiers that maps natural language substrings toMRG production rules.
Consequently, it is impor-tant that the production rules in the MRG mirror thestructure of natural language (Kate, 2008).The original MRG used by Chen and Mooney is acompact grammar that contains many recursive rulesthat can be used to generate an infinite number of ac-tions or arguments.
While these rules are quite ex-pressive, they often do not correspond well to anywords or phrases in natural language.
To alleviatethis problem, we designed another MRG by expand-ing out many of the rules.
For example, the originalMRG contained the following production rules forgenerating an infinite number of travel actions fromthe root symbol S.*S -> *Action*Action -> *Action, *Action*Action -> *Travel*Travel -> Travel( )*Travel -> Travel( steps: *Num )We expand out the production rules as shown be-low to map S directly to specific travel actions sothey correspond better to patterns such as ?go for-ward?
or ?walk N steps?.
*S -> Travel( )*S -> Travel( steps: *Num )*S -> Travel( ), *Action*S -> Travel( steps: *Num ), *Action*Action -> *Action, *Action*Action -> Travel( )*Action -> Travel( steps: *Num )While this process of expanding the produc-tion rules resulted in many more rules, these ex-panded rules usually correspond better with wordsor phrases in natural language.
We still retain someof the recursive rules to ensure that the formal lan-guage remains as expressive as before.4334 Collecting Additional Data withMechanical TurkOne of the motivations for studying ambiguous su-pervision is the potential ease of acquiring largeamounts of training data.
Without requiring seman-tic annotations, a human only has to demonstratehow language is used in context which is generallysimple to do.
We validate this claim by collectingadditional training data for the navigation domainusing Mechanical Turk (Snow et al, 2008).There are two types of data we are interested incollecting: natural language navigation instructionsand follower data.
Thus, we created two tasks onMechanical Turk.
The first one asks the workersto supply instructions for a randomly generated se-quence of actions.
The second one asks the workersto try to follow a given navigation instruction in ourvirtual environment.
The latter task is used to gener-ate the corresponding action sequences for instruc-tions collected from the first task.4.1 Task DescriptionsTo facilitate the data collection, we first recreatedthe 3D environments used to collect the original data(MacMahon et al, 2006).
We built a Java appli-cation that allows the user to freely navigate thethree virtual worlds constructed by MacMahon etal.
(2006) using the discrete controls of turning left,turning right, and moving forward one step.The follower task is fairly straightforward usingour application.
The worker is given a navigationinstruction and placed at the starting location.
Theyare asked to follow the navigation instruction as bestas they could using the three discrete controls.
Theycould also skip the problem if they did not under-stand the instruction or if the instruction did not de-scribe a viable route.
For each Human IntelligenceTask (HIT), we asked the worker to complete 5 fol-lower problems.
We paid them $0.05 for each HIT,or 1 cent per follower problem.
The instructionsused for the follower problems were mainly col-lected from ourMechanical Turk instructor task withsome of the instructions coming from data collectedbyMacMahon (2007) that was not used by Chen andMooney (2011).The instructor task is slightly more involved be-cause we ask the workers to provide new navigationChen & Mooney MTurk# instructions 3236 1011Vocabulary size 629 590Avg.
# words 7.8 (5.1) 7.69 (7.12)Avg.
# actions 2.1 (2.4) 1.84 (1.24)Table 1: Statistics about the navigation instruction cor-pora.
The average statistics for each instruction areshown with standard deviations in parentheses.instructions.
The worker is shown a 3D simulationof a randomly generated action sequence betweenlength 1 to 4 and asked to write short, free-form in-structions that would lead someone to perform thoseactions.
Since this task requires more time to com-plete, each HIT consists of only 3 instructor prob-lems.
Moreover, we pay the workers $0.10 for eachHIT, or about 3 cents for each instruction they write.To encourage quality contributions, we use atiered payment structure (Chen and Dolan, 2011)that rewards the good workers.
Workers who havebeen identified to consistently provide good instruc-tions were allowed to do higher-paying version ofthe same HITs that paid $0.15 instead of $0.10.4.2 Data StatisticsOver a 2-month period we accepted 2,884 followerHITs and 810 instructor HITs from 653 workers.This corresponds to over 14,000 follower traces and2,400 instructions with most of them consisting ofsingle sentences.
For instructions with multiple sen-tences, we merged all the sentences together andtreated it as a single sentence.
The total cost ofthe data collection was $277.92.
While there were2,400 instructions, we filtered them to make surethey were of reasonable quality.
First, we discardedany instructions that did not have at least 5 followertraces.
Then we looked at all the follower traces anddiscarded any instruction that did not have majorityagreement on what the correct path is.Using our strict filter, we were left with slightlyover 1,000 instructions.
Statistics about the newcorpus and the one used by Chen and Mooney canbe seen in Table 1.
Overall, the new corpus has aslightly smaller vocabulary, and each instruction isslightly shorter both in terms of the number of wordsand the number of actions.4345 ExperimentsWe evaluate our new lexicon learning algorithm aswell as the other modifications to the navigation sys-tem using the same three tasks as Chen and Mooney(2011).
The first task is disambiguating the train-ing data by inferring the correct navigation plans as-sociated with each training sentence.
The secondtask is evaluating the performance of the semanticparsers trained on the disambiguated data.
We mea-sure the performance of both of these tasks by com-paring to gold-standard data using the same partialcorrectness metric used by Chen and Mooney whichgives credit to a parse for producing the correct ac-tion type and additional credit if the arguments werealso correct.
Finally, the third task is to complete theend-to-end navigation task.
There are two versionsof this task, the complete task uses the original in-structions which are several sentences long and theother version uses instructions that have been man-ually split into single sentences.
Task completionis measured by the percentage of trials in which thesystem reached the correct destination (and orienta-tion in the single-sentence version).We follow the same evaluation scheme as Chenand Mooney and perform leave-one-map-out exper-iments.
For the first task, we build a lexicon usingambiguous training data from two maps, and thenuse the lexicon to produce the best disambiguatedsemantic meanings for those same data.
For the sec-ond and third tasks, we train a semantic parser on theautomatically disambiguated data, and test on sen-tences from the third, unseen map.For all comparisons to the Chen and Mooney re-sults, we use the performance of their refined land-marks plans system which performed the best over-all.
Moreover, it provides the most direct compari-son to our approach since both use a lexicon to re-fine the landmarks plans.
Other than the modifi-cations discussed, we use the same components astheir system including using KRISP to train the se-mantic parsers and using the execution module fromMacMahon et al (2006) to carry out the navigationplans.5.1 Inferring Navigation PlansFirst, we examine the quality of the refined naviga-tion plans produced using SGOLL?s lexicon.
ThePrecision Recall F1Chen and Mooney 78.54 78.10 78.32SGOLL 87.32 72.96 79.49Table 2: Partial parse accuracy of how well each algo-rithm can infer the gold-standard navigation plans.Precision Recall F1Chen and Mooney 90.22 55.10 68.37SGOLL 92.22 55.70 69.43SGOLL withnew MRG 88.36 57.03 69.31Table 3: Partial parse accuracy of the semantic parserstrained on the disambiguated navigation plans.precision, recall, and F1 (harmonic mean of preci-sion and recall) of these plans are shown in Table 2.Compared to Chen and Mooney, the plans producedby SGOLL has higher precision and lower recall.This is mainly due to the additional minimum sup-port constraint we added which discards many noisylexical entries from infrequently seen n-grams.5.2 Training Semantic ParsersNext we look at the performance of the semanticparsers trained on the inferred navigation plans.
Theresults are shown in Table 3.
Here SGOLL per-forms almost the same as Chen and Mooney, withslightly better precision.
We also look at the effect ofchanging the MRG.
Using the new MRG for KRISPto train the semantic parser produced slightly lowerprecision but higher recall, with similar overall F1score.5.3 Executing Navigation PlansFinally, we evaluate the system on the end-to-endnavigation task.
In addition to SGOLL and SGOLLwith the newMRG, we also look at augmenting eachof the training splits with the data we collected usingMechanical Turk.Completion rates for both the single-sentencetasks and the complete tasks are shown in Table 4.Here we see the benefit of each of our modifications.SGOLL outperforms Chen and Mooney?s system onboth versions of the navigation task.
Using the newMRG to train the semantic parsers further improvedperformance on both tasks.
Finally, augmenting the435Single-sentence CompleteChen and Mooney 54.40% 16.18%SGOLL 57.09% 17.56%SGOLL with newMRG 57.28% 19.18%SGOLL with newMRG andMTurk data 57.62% 20.64%Table 4: End-to-end navigation task completion rates.Computation TimeChen and Mooney (2011) 2,227.63SGOLL 157.30SGOLL with MTurk data 233.27Table 5: The time (in seconds) it took to build the lexicon.training data with additional instructions and fol-lower traces collected from Mechanical Turk pro-duced the best results.5.4 Computation TimesHaving established the superior performance of ournew system compared to Chen and Mooney?s, wenext look at the computational efficiency of SGOLL.The average time (in seconds) it takes for each al-gorithm to build a lexicon is shown in Table 5.All the results are obtained running the algorithmson Dell PowerEdge 1950 servers with 2x XeonX5440 (quad-core) 2.83GHz processors and 32GBof RAM.
Here SGOLL has a decidedly large advan-tage over the lexicon learning algorithm from Chenand Mooney, requiring an order of magnitude lesstime to run.
Even after incorporating the new Me-chanical Turk data into the training set, SGOLL stilltakes much less time to build a lexicon.
This showshow inefficient it is to perform graph intersection op-erations and how our online algorithm can more re-alistically scale to large datasets.5.5 Experimenting with Chinese DataIn addition to evaluating the system on English data,we also translated the corpus used by Chen andMooney into Mandarin Chinese.1 To run our sys-1The translation can be downloaded at http://www.cs.utexas.edu/?ml/clamp/navigation/tem, we first segmented the sentences using theStanford Chinese Word Segmenter (Chang et al,2008).
We evaluated using the same three tasks asbefore.
This resulted in a precision, recall, and F1of 87.07, 71.67, and 78.61, respectively for the in-ferred plans.
The trained semantic parser?s preci-sion, recall, and F1 were 88.87, 58.76, and 70.74, re-spectively.
Finally, the system completed 58.70% ofthe single-sentence task and 20.13% of the completetask.
All of these numbers are very similar to the En-glish results, showing the generality of the system inits ability to learn other languages.5.6 DiscussionWe have introduced a novel, online lexicon learn-ing algorithm that is much faster than the one pro-posed by Chen and Mooney and also performs bet-ter on the navigation tasks they devised.
Havinga computationally efficient algorithm is critical forbuilding systems that learn from ambiguous super-vision.
Compared to systems that train on super-vised semantic annotations, a system that only re-ceives weak, ambiguous training data is expected tohave to train on much larger datasets to achieve sim-ilar performance.
Consequently, such system mustbe able to scale well in order to keep the learningprocess tractable.
Not only is SGOLLmuch faster inbuilding a lexicon, it can also be easily parallelized.Moreover, the online nature of SGOLL allows thelexicon to be continually updated while the systemis in use.
A deployed navigation system can gathernew instructions from the user and receive feedbackabout whether it is performing the correct actions.As new training examples are collected, we can up-date the corresponding n-gram and subgraph countswithout rebuilding the entire lexicon.One thing to note though is that while SGOLLmakes the lexicon learning step much faster andscalable, another bottleneck in the overall systemis training the semantic parser.
Existing semantic-parser learners such as KRISP were not designed toscale to very large datasets and have trouble trainingon more than a few thousand examples.
Thus, de-signing new scalable algorithms for learning seman-tic parsers is critical to scaling the entire system.We have performed a pilot data collection of newtraining examples using Mechanical Turk.
Eventhough the instructions were collected from very dif-436ferent sources (paid human subjects from a univer-sity for the original data versus workers recruitedover the Internet), we showed that adding the newdata into the training set improved the system?s per-formance on interpreting instructions from the orig-inal corpus.
It verified that we are indeed collectinguseful information and that non-experts are fully ca-pable of training the system by demonstrating howto use natural language in relevant contexts.6 Related WorkThe earliest work on cross-situational word learningwas by Siskind (1996) who developed a rule-basedsystem to solve the referential ambiguity problem.However, it did not handle noise and was tested onlyon artificial data.
More recently, Fazly et al (2010)proposed a probabilistic incremental model that canlearn online similar to our algorithm and was testedon transcriptions of child-directed speech.
However,they generated the semantic representations from thetext itself rather than from the environment.
More-over, the referential ambiguity was introduced artifi-cially by including the correct semantic representa-tion of the neighboring sentence.Our work falls into the larger framework of learn-ing the semantics of language from weak supervi-sion.
This problem can be seen as an alignmentproblem where each sentence in the training dataneeds to be aligned to one or more records that rep-resent its meaning.
Chen and Mooney (2008) previ-ously introduced another task that aligns sportscast-ing commentaries to events in a simulated soccergame.
Using an EM-like retraining method, theyalternated between building a semantic parser andestimating the most likely alignment.
Liang et al(2009) developed an unsupervised approach using agenerative model to solve the alignment problem.They demonstrated improved results on matchingsentences and events on the sportscasting task andalso introduced a new task of aligning weather fore-casts to weather information.
Kim and Mooney(2010) further improved the generative alignmentmodel by incorporating the full semantic parsingmodel from Lu et al (2008).
This resulted in ajoint generative model that outperformed all previ-ous results.
In addition to treating the ambiguoussupervision problem as an alignment problem, therehave been other approaches such as treating it as aranking problem (Bordes et al, 2010), or a PCFGlearning problem (Borschinger et al, 2011).Parallel to the work of learning from ambigu-ous supervision, other recent work has also lookedat training semantic parsers from supervision otherthan logical-form annotations.
Clarke et al (2010)and Liang et al (2011) trained systems on questionand answer pairs by automatically finding semanticinterpretations of the questions that would generatethe correct answers.
Artzi and Zettlemoyer (2011)use conversation logs between a computer systemand a human user to learn to interpret the humanutterances.
Finally, Goldwasser et al (2011) pre-sented an unsupervised approach of learning a se-mantic parser by using an EM-like retraining loop.They use confidence estimation as a proxy for themodel?s prediction quality, preferring models thathave high confidence about their parses.7 ConclusionLearning the semantics of language from the per-ceptual context in which it is uttered is a useful ap-proach because only minimal human supervision isrequired.
In this paper we presented a novel onlinealgorithm for building a lexicon from ambiguouslysupervised relational data.
In contrast to the pre-vious approach that computed common subgraphsbetween different contexts in which an n-gram ap-peared, we instead focus on small, connected sub-graphs and introduce an algorithm, SGOLL, that isan order of magnitude faster.
In addition to beingmore scalable, SGOLL also performed better on thetask of interpreting navigation instructions.
In addi-tion, we showed that changing the MRG and collect-ing additional training data from Mechanical Turkfurther improve the performance of the overall nav-igation system.
Finally, we demonstrated the gener-ality of the system by using it to learn Chinese navi-gation instructions and achieved similar results.AcknowledgmentsThe research in this paper was supported by the Na-tional Science Foundation (NSF) under the grantsIIS-0712097 and IIS-1016312.
We thank Lu Guo forperforming the translation of the corpus into Man-darin Chinese.437ReferencesYoav Artzi and Luke Zettlemoyer.
2011.
Bootstrappingsemantic parsers from conversations.
In Proceedingsof the 2011 Conference on Empirical Methods in Nat-ural Language Processing (EMNLP-11).Antoine Bordes, Nicolas Usunier, and Jason Weston.2010.
Label ranking under ambiguous supervision forlearning semantic correspondences.
In Proceedings ofthe 27th International Conference on Machine Learn-ing (ICML-2010).Benjamin Borschinger, Bevan K. Jones, and Mark John-son.
2011.
Reducing grounded learning tasks to gram-matical inference.
In Proceedings of the 2011 Confer-ence on Empirical Methods in Natural Language Pro-cessing (EMNLP-11).Pi-Chuan Chang, Michel Galley, and Chris Manning.2008.
Optimizing Chinese word segmentation for ma-chine translation performance.
In Proceedings of theACL Third Workshop on Statistical Machine Transla-tion.David L. Chen and William B. Dolan.
2011.
Collectinghighly parallel data for paraphrase evaluation.
In Pro-ceedings of the 49th Annual Meeting of the Associationfor Computational Linguistics (ACL-2011), Portland,OR, June.David L. Chen and Raymond J. Mooney.
2008.
Learn-ing to sportscast: A test of grounded language ac-quisition.
In Proceedings of 25th International Con-ference on Machine Learning (ICML-2008), Helsinki,Finland, July.David L. Chen and Raymond J. Mooney.
2011.
Learn-ing to interpret natural language navigation instruc-tions from observations.
In Proceedings of the 25thAAAI Conference on Artificial Intelligence (AAAI-11).James Clarke, Dan Goldwasser, Ming-Wei Chang, andDan Roth.
2010.
Driving semantic parsing fromthe worlds response.
In Proceedings of the Four-teenth Conference on Computational Natural Lan-guage Learning (CoNLL-2010), pages 18?27.Afsaneh Fazly, Afra Alishahi, and Suzanne Steven-son.
2010.
A probabilistic computational model ofcross-situational word learning.
Cognitive Science,34(6):1017?1063.Dan Goldwasser, Roi Reichart, James Clarke, and DanRoth.
2011.
Confidence driven unsupervised semanticparsing.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics (ACL-11).Rohit J. Kate and Raymond J. Mooney.
2006.
Us-ing string-kernels for learning semantic parsers.
InProceedings of the 21st International Conference onComputational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguis-tics (COLING/ACL-06), pages 913?920, Sydney, Aus-tralia, July.Rohit J. Kate.
2008.
Transforming meaning repre-sentation grammars to improve semantic parsing.
InProceedings of the Twelfth Conference on Compu-tational Natural Language Learning (CoNLL-2008),pages 33?40, Manchester, UK, August.Joohyun Kim and Raymond J. Mooney.
2010.
Genera-tive alignment and semantic parsing for learning fromambiguous supervision.
In Proceedings of the 23rd In-ternational Conference on Computational Linguistics(COLING-10).Thomas Kollar, Stefanie Tellex, Deb Roy, and NicholasRoy.
2010.
Toward understanding natural languagedirections.
In Proceedings of the 5th ACM/IEEE In-ternational Conference on Human-Robot Interaction(HRI).Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-ter, and Mark Steedman.
2010.
Inducing probabilisticCCG grammars from logical form with higher-orderunification.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Processing(EMNLP-10).Percy Liang, Michael I. Jordan, and Dan Klein.
2009.Learning semantic correspondences with less supervi-sion.
In Joint Conference of the 47th Annual Meetingof the Association for Computational Linguistics andthe 4th International Joint Conference on Natural Lan-guage Processing of the Asian Federation of NaturalLanguage Processing (ACL-IJCNLP).Percy Liang, Michael I. Jordan, and Dan Klein.
2011.Learning dependency-based compositional semantics.In Proceedings of the 49th Annual Meeting of the As-sociation for Computational Linguistics (ACL-11).Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S. Zettle-moyer.
2008.
A generative model for parsing naturallanguage to meaning representations.
In Proceedingsof the 2008 Conference on Empirical Methods in Natu-ral Language Processing (EMNLP-08), Honolulu, HI,October.Matt MacMahon, Brian Stankiewicz, and BenjaminKuipers.
2006.
Walk the talk: Connecting language,knowledge, and action in route instructions.
In Pro-ceedings of the Twenty-First National Conference onArtificial Intelligence (AAAI-06).Matt MacMahon.
2007.
Following Natural LanguageRoute Instructions.
Ph.D. thesis, Electrical and Com-puter Engineering Department, University of Texas atAustin.Cynthia Matuszek, Dieter Fox, and Karl Koscher.
2010.Following directions using statistical machine transla-tion.
In Proceedings of the 5th ACM/IEEE Interna-tional Conference on Human-Robot Interaction (HRI).438Raymond J. Mooney.
2007.
Learning for semantic pars-ing.
In A. Gelbukh, editor, Computational Linguisticsand Intelligent Text Processing: Proceedings of the 8thInternational Conference, CICLing 2007, Mexico City,pages 311?324.
Springer Verlag, Berlin.Nobuyuki Shimizu and Andrew Haas.
2009.
Learning tofollow navigational route instructions.
In Proceedingsof the Twenty-first International Joint Conference onArtificial Intelligence (IJCAI-2009).Jeffrey M. Siskind.
1996.
A computational studyof cross-situational techniques for learning word-to-meaning mappings.
Cognition, 61(1):39?91, October.Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-drew Y. Ng.
2008.
Cheap and fast - but is it good?evaluating non-expert annotations for natural languagetasks.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Processing(EMNLP-08).Cynthia A. Thompson and Raymond J. Mooney.
2003.Acquiring word-meaning mappings for natural lan-guage interfaces.
Journal of Artificial Intelligence Re-search, 18:1?44.Adam Vogel and Dan Jurafsky.
2010.
Learning to fol-low navigational directions.
In Proceedings of the 48thAnnual Meeting of the Association for ComputationalLinguistics (ACL-10).Luke S. Zettlemoyer and Michael Collins.
2007.
Onlinelearning of relaxed CCG grammars for parsing to logi-cal form.
In Proceedings of the 2007 Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning(EMNLP-CoNLL-07), pages 678?687, Prague, CzechRepublic, June.439
