Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 42?52,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsDescribing Images using Inferred Visual Dependency RepresentationsDesmond Elliott and Arjen P. de VriesInformation Access GroupCentrum Wiskunde & InformaticaAmsterdam, The Netherlandselliott@cwi.nl, arjen@acm.orgAbstractThe Visual Dependency Representation(VDR) is an explicit model of the spa-tial relationships between objects in an im-age.
In this paper we present an approachto training a VDR Parsing Model withoutthe extensive human supervision used inprevious work.
Our approach is to findthe objects mentioned in a given descrip-tion using a state-of-the-art object detec-tor, and to use successful detections to pro-duce training data.
The description of anunseen image is produced by first predict-ing its VDR over automatically detectedobjects, and then generating the text witha template-based generation model usingthe predicted VDR.
The performance ofour approach is comparable to a state-of-the-art multimodal deep neural network inimages depicting actions.1 IntroductionHumans typically write the text accompanying animage, which is a time-consuming and expen-sive activity.
There are many circumstances inwhich people are well-suited to this task, such ascaptioning news articles (Feng and Lapata, 2008)where there are complex relationships between themodalities (Marsh and White, 2003).
In this pa-per we focus on generating literal descriptions,which are rarely found alongside images becausethey describe what can easily be seen by others(Panofsky, 1939; Shatford, 1986; Hodosh et al,2013).
A computer that can automatically gen-erate these literal descriptions, filling the gap leftby humans, may improve access to existing imagecollections or increase information access for vi-sually impaired users.There has been an upsurge of research in thisarea, including models that rely on spatial rela-tionships (Farhadi et al, 2010), corpus-based rela-tionships (Yang et al, 2011), spatial and visual at-tributes (Kulkarni et al, 2011), n-gram phrase fu-sion fromWeb-scale corpora (Li et al, 2011), tree-substitution grammars (Mitchell et al, 2012), se-lecting and combining phrases from large image-description collections (Kuznetsova et al, 2012),using Visual Dependency Representations to cap-ture spatial and corpus-based relationships (El-liott and Keller, 2013), and in a generative frame-work over densely-labelled data (Yatskar et al,2014).
The most recent developments have fo-cused on deep learning the relationships betweenvisual feature vectors and word-embeddings withlanguage generation models based on recurrentneural networks or long-short term memory net-works (Karpathy and Fei-Fei, 2015; Vinyals et al,2015; Mao et al, 2015; Fang et al, 2015; Don-ahue et al, 2015; Lebret et al, 2015).
An alter-native thread of research has focused on directlypairing images with text, based on kCCA (Hodoshet al, 2013) or multimodal deep neural networks(Socher et al, 2014; Karpathy et al, 2014).We revisit the Visual Dependency Representa-tion (Elliott and Keller, 2013, VDR), an intermedi-ate structure that captures the spatial relationshipsbetween objects in an image.
Spatial context hasbeen shown to be useful in object recognition andnaming tasks because humans benefit from the vi-sual world conforming to their expectations (Bie-derman et al, 1982; Bar and Ullman, 1996).
Thespatial relationships defined in VDR are closely,but independently, related to cognitively plausiblespatial templates (Logan and Sadler, 1996) and re-gion connection calculus (Randell et al, 1992).In the image description task, explicitly modellingthe spatial relationships between observed objectsconstrains how an image should be described.
Anexample can be seen in Figure 1, where the train-ing VDR identifies the defining relationship be-tween the man and the laptop, which may be re-42A man is using a laptop- - -nsubj dobjperson laptopbesideVDRParserR-CNNperson laptopbesideVDRParserA person isusing a laptopLanguageGeneratorFigure 1: We present an approach to inferring VDR training data from images paired with descriptions(top), and for generating descriptions from VDR (bottom).
Candidates for the subject and object in theimage are extracted from the description.
An object detector1searches for the objects and determinis-tically produces a training instance, which is used to train a VDR Parser to predict the relationshipsbetween objects in unseen images.
When an unseen image is presented to the model, we first extractN-candidate objects for the image.
The detected objects are then parsed into a VDR structure, which ispassed into a template-based language generator to produce a description of the image.alised as a ?using?, ?typing?, or ?working?
rela-tionship between the objects.The main limitation of previous research onVDR has been the reliance on gold-standard train-ing annotations, which requires trained annota-tors.
We present the first approach to automati-cally inferring VDR training examples from nat-ural scenes using only an object detector and animage description.
Ortiz et al (2015) have re-cently presented an alternative treatment of VDRwithin the context of abstract scenes and phrase-based machine translation.
Figure 1 shows a de-tailed overview of our approach.
At training time,we learn a VDR Parsing model from representa-tions that are constructed by searching for the sub-ject and object in the image.
The description ofan unseen image is generated using a template-based generation model that leverages the VDRpredicted over the top-N objects extracted from anobject detector.We evaluate our method for inferring VDRs inan image description experiment on the Pascal1K(Rashtchian et al, 2010) and VL2K data sets (El-liott and Keller, 2013) against two models: thebi-directional recurrent neural network (Karpathyand Fei-Fei, 2015, BRNN) and MIDGE (Mitchellet al, 2012).
The main finding is that the qual-ity of the descriptions generated by our method1The image of the R-CNN object detector was modifiedwith permission from Girshick et al (2014).depends on whether the images depict an action.In the VLT2K data set of people performing ac-tions, the performance of our approach is compa-rable to the BRNN; in the more diverse Pascal1Kdataset, the BRNN is substantially better than ourmethod.
In a second experiment, we transfer theVDR-based model from the VLT2K data set to thePascal1K data set without re-training, which im-proves the descriptions generated in the Pascal1Kdata set.
This suggests that refining how we ex-tract training data may yield further improvementsto VDR-based image description.The code and generated descriptions are avail-able at http://github.com/elliottd/vdr/.2 Automatically Inferring VDRsThe Visual Dependency Representation is a struc-tured representation of an image that explicitlymodels the spatial relationships between objects.In this representation, the spatial relationship be-tween a pair of objects is encoded with one of thefollowing eight options: above, below, beside, op-posite, on, surrounds, infront, and behind.
Pre-vious work on VDR-based image description hasrelied on training data from expert human anno-tators, which is expensive and difficult to scaleto other data sets.
In this paper, we describe anapproach to automatically inferring VDRs usingonly an object detector and the description of animage.
Our aim is to define an automated version43Relation DefinitionBeside The angle between the subject andthe object is either between 315?and 45?or 135?and 225?.Above The angle between the subject andobject is between 225?and 315?.Below The angle between the subject andobject is between 45?and 135?.On More than 50% of the subjectoverlaps with the object.Surrounds More than 90% of the subjectoverlaps with the object.Table 1: The cascade of spatial relationships be-tween objects in VDR.
We always use the lastrelationship that matches.
These definitions aremostly taken from (Elliott and Keller, 2013), ex-cept that we remove the 3D relationships.
Anglesare defined with respect to the unit circle, whichhas 0?on the right.
All relations are specific withrespect to the centroid of the bounding boxes.of the human process used to create gold-standarddata (Elliott and Keller, 2013).An inferred VDR is constructed by searchingfor the subject and object referred to in the descrip-tion of an image using an object detector.
If boththe subject and object can be found in the image,a VDR is created by attaching the detected subjectto the detected object, given the spatial relation-ship between the object bounding boxes.
The spa-tial relationships that can be applied between sub-jects and objects are defined in the cascade definedin Table 1.
The set of relationships was reducedfrom eight to six due to the difficulty in predict-ing the 3D relationships in 2D images (Eigen etal., 2014).
The spatial relation selected for a pairof objects is determined by applying each tem-plate defined in Table 1 to the object pair.
We useonly the final matching relationship, although fu-ture work may consider applying multiple match-ing relationships between objects.Given a set of inferred VDR training examples,we train a VDR Parsing Model with the VDR+IMGfeature set using only the inferred examples (El-liott et al, 2014).
We tried training a model bycombining the inferred and gold-standard VDRsbut this lead to an erratic parsing model that wouldregularly predict flat structures instead of object?Figure 2: An example of the most confident objectdetections from the R-CNN object detector.object relationships.
One possibility for this be-haviour is the mismatch caused by removing theinfront and behind relationships in the inferredtraining data.
Another possible explanation isthe gold-standard data contains deeper and morecomplex structures than the simple object?objectstructures we infer.2.1 Linguistic ProcessingThe description of an image is processed to extractcandidates for the mentioned objects.
We extractcandidates from the nsubj and dobj tokens inthe dependency parsed description2.
If the parseddescription does not contain both a subject and anobject, as defined here, the example is discarded.2.2 Visual ProcessingIf the dependency parsed description containscandidates for the subject and object of an im-age, we attempt to find these objects in the im-age.
We use the Regions with ConvolutionalNeural Network features object detector (Gir-shick et al, 2014, R-CNN) with the pre-trainedbvlc reference ilsrvc13 detection modelimplemented in Caffe (Jia et al, 2014).
This ob-ject detection model is able to detect 200 differenttypes of objects, with a mean average precision of31.4% in the ImageNet Large-Scale Visual Recog-nition Challenge3(Russakovsky et al, 2014).
Theoutput of the object detector is a bounding boxwith real-valued confidence scores, as shown in2The descriptions are Part-of-Speech tagged using theStanford POS Tagger v3.1.0 (Toutanova et al, 2003) withthe english-bidirectional-distsim pre-trainedmodel.
The tagged descriptions are then Dependency Parsedusing Malt Parser v 1.7.2 (Nivre et al, 2007) with theengmalt.poly-1.7 pre-trained model.3The state-of-the-art result for this task is 37.2% using aNetwork in Network architecture (Lin et al, 2014a); a pre-trained detection model was not available in the Caffe ModelZoo at the time of writing.44A boy is using a laptop(a) onA man is riding a bike(b) aboveA woman is riding a bike(c) surroundsA woman is riding a horse(d) surroundsA man is playing a sax(e) surroundsA man is playing a guitar(f) besideThe woman is wearing a helmet(g) surroundsFigure 3: Examples of the object detections and automatically inferred VDR.
In each example, the objectdetector candidates were extracted from the description and the VDR relationships were determined bythe cascade in Table 1.
Automatically inferring VDR allows us to learn differences in spatial relationshipsfrom different camera viewpoints, such as people riding bicycles.Figure 2.
The confidence scores are not probabili-ties and can vary widely across images.The words in a description that refer to objectsin an image are not always within the constrainedvocabulary of the object labels in the object de-tection model.
We increase the chance of findingobjects with two simple back-offs: by lemmatis-ing the token, and transforming the token into itsWordNet hypernym parent.
If the subject and theobject can be found in the image, we create an in-ferred VDR from the detections, otherwise we dis-card this training example.Figure 3 shows a collection of automatically in-ferred VDRs.
One of the immediate benefits ofVDR, as a representation, is that we can easily in-terpret the structures extracted from images.
Anexample of helpful object orientation invariancecan be seen in 3 (b) and (c), where VDR capturesthe two different types of spatial relationships be-tween people and bicycles that are grounded in theverb ?riding?.
This type of invariance is usefuland it suggests VDR can model interacting objectsfrom various viewpoints.
We note here the sim-ilarities between automatically inferred VDR andVisual Phrases (Sadeghi and Farhadi, 2011).
Themain difference between these models is that VDRis primarily concerned with object?object interac-tions for generation and retrieval tasks, whereasVisual Phrases were intended to model person?object interactions for activity recognition.2.3 Building a Language ModelWe build a language model using the subjects,verbs, objects, and spatial relationships from thesuccessfully constructed training examples.
Thesubjects and objects take the form of the object de-tector labels to reduce the effects of sparsity.
Theverbs are found as the direct common verb parentof the subject and object in the dependency parsedsentence.
We stem the verbs using morpha, to re-duce sparsity, and inflect them in a generated de-scription with +ing using morphg (Minnen et al,2001).
The spatial relationship between the sub-ject and object region is used to help constrain lan-guage generation to produce descriptions, givenobserved spatial contexts in a VDR.45person laptop sofa banjo vacuumc=3.12 c=0.77 c=0.61 c=0.14 c=-0.40besiderootbesidebesideVDRParserA person is using a laptop (0.84)A person is playing a banjo (0.71)A person is beside a vacuum (0.38)?A person is in the image (0.96)?LanguageGeneratorFigure 4: An overview of VDR-constrained language generation.
We extract the top-N objects from animage using an object detector and predict the spatial relationships between the objects using a VDRParser trained over the inferred training data.
Descriptions are generated for all parent?child subtrees inthe VDR, and the final text has the highest combined corpus and visual confidence.
?
: only generatedis there are no verbs between the objects in the language model; ?
: only generated if there are no verbsbetween any pairs of objects in the image.3 Generating DescriptionsThe description of an image is generated usinga template-based language generation model de-signed to exploit the structure encoded in VDR.The language generation model extends Elliottand Keller (2013) with the visual confidencescores from the object detector.
Figure 4 showsan overview of the generation process.The top-N objects are extracted from an imageusing the pre-trained R-CNN object detector (seeSection 2.2 for more details).
We remove non-maximal detections with the same class label thatoverlap by more than 30%.
The objects are thenparsed into a VDR structure using the VDR Parsertrained on the automatically inferred training data.Given the VDR over the set of detected objects, wegenerate all possible descriptions of the image thatcan be produced in a depth-first traversal of theVDR.
A description is assigned a score that com-bines the corpus-based evidence and visual con-fidence of the objects selected for the description.The descriptions are generated using the followingtemplate:DT head is V DT child.In this template, head and child are the labelsof the objects that appear in the head and child po-sitions of a specific VDR subtree.
V is a verb de-termined from a subject-verb-object-spatial rela-tion model derived from the training data descrip-tions.
This model captures statistics about nounsthat appear as subjects and objects, the verbs be-tween them, and spatial relationships observed inthe inferred training VDRs.
The verb v that satis-fies the V field is determined as follows:v = argmaxvp(v|head, child, spatial)(1)p(v|head,child, spatial) =p(v|head) ?
p(child|v, head)?p(spatial|child, v, head)(2)If no verbs were observed between a particularobject?object pair in the training corpus, V is filledusing a back-off that uses the spatial relationshiplabel between the objects in the VDR.The object detection confidence values, whichare not probabilities and can vary substantially be-tween images, are transformed into the range [0,1]using sgm(conf) =11+e?conf.
The final score as-signed to a description is then used to rank all ofthe candidate descriptions, and the highest-scoringdescription is assigned to an image:score(head, v,child, spatial) =p(v|head, child, spatial)?sgm(head) ?
sgm(child)(3)If the VDR Parser does not predict any rela-tionships between objects in an image, which mayhappen if all of the objects have never been ob-served in the training data, we use a back-off tem-plate to generate the description.
In this case, themost confidently detected object in the image isused with the following template:A/An object is in the image.The number of objectsN objects extracted froman unseen image is optimised by maximising thesentence-level Meteor score of the generated de-scriptions in the development data.4 ExperimentsWe evaluate our approach to automatically infer-ring VDR training data in an automatic image de-scription experiment.
The aim in this task is to46generate a natural language description of an im-age, which is evaluated directly against multiplereference descriptions.4.1 ModelsWe compare our approach against two state-of-the-art image description models.
MIDGE gener-ates text based on tree-substitution grammar andrelies on discrete object detections (Mitchell et al,2012) for visual input.
We make a small modi-fication to MIDGE so it uses all of the top-N de-tected objects, regardless of the confidence of thedetections4.
BRNN is a multimodal deep neuralnetwork that generates descriptions directly fromvector representations of the image and the de-scription (Karpathy and Fei-Fei, 2015).
The im-ages are represented by the visual feature vectorextracted from the FC7layer of the VGG 16-layerconvolutional neural network (Simonyan and Zis-serman, 2015) and the descriptions are representedas a word-embedding vector.4.2 Evaluation MeasuresWe evaluate the generated descriptions usingsentence-level Meteor (Denkowski and Lavie,2011) and BLEU4 (Papineni et al, 2002), whichhave been shown to have moderate correlationwith humans (Elliott and Keller, 2014).
We adopta jack-knifing evaluation methodology, which en-ables us to report human?human results (Lin andOch, 2004), using MultEval (Clark et al, 2011).4.3 Data SetsWe perform our experiments on two data sets: Pas-cal1K and VLT2K.
The Pascal1K data set contains1,000 images sampled from the PASCAL ObjectDetection Challenge data set (Everingham et al,2010); each image is paired with five reference de-scriptions collected from Mechanical Turk.
It con-tains a wide variety of subject matter drawn fromthe original 20 PASCAL Detection classes.
TheVLT2K data set contains 2,424 images taken fromthe trainval 2011 portion of the PASCAL ActionRecognition Challenge; each image is paired withthree reference descriptions, also collected fromMechanical Turk.
We randomly split the imagesinto 80% training, 10% validation, and 10% test.4In personal communication with Margaret Mitchell, sheexplained that the object confidence thresholds for MIDGEwere determined by visual inspection on held-out data, whichwe decided was not feasible for 200 new detectors.VLT2K Pascal1KMeteor BLEU Meteor BLEUVDR 16.0 14.8 7.4 9.0BRNN 18.6 23.7 12.6 16.0-genders 16.6 17.4 12.1 15.1MIDGE 5.5 8.2 3.6 9.1Human 26.4 23.3 21.7 20.6Table 2: Sentence-level evaluation of the gen-erated descriptions.
VDR is comparable toBRNN when the images exclusively depict actions(VLT2K).
In a more diverse data set, BRNN gener-ates better descriptions (Pascal1K).4.4 ResultsTable 2 shows the results of the image descriptionexperiment.
The main finding of our experimentsis that the performance of our proposed approachVDR depends on the type of images.
We foundthat VDR is comparable to the deep neural networkBRNN on the VLT2K data set of people perform-ing actions.
This is consistent with the hypothesisunderlying VDR: it is useful to encode the spa-tial relationships between objects in images.
Thedifference between the models is increased by theinability of the object detector used by VDR to pre-dict bounding boxes for three objects (cameras,books, and phones) crucial to describing 30% ofthe images in this data set.
In the more diversePascal1K data set, which does not necessarily de-pict people performing actions, the deep neuralnetwork generates substantially better descriptionsthan VDR and MIDGE.
The tree-substitution gram-mar approach to generating descriptions used byMIDGE does not perform well on either data set.There is an obvious discrepancy between theBLEU4 and Meteor scores for the models.
BLEU4relies on lexical matching between sentences andthus penalises semantically equivalent descrip-tions.
For example, identifying the gender ofa person is important for generating a good de-scription.
However, object recognizers are not(yet) able to reliably achieve this distinction, andwe only have a single recogniser for ?persons?.The BRNN generates descriptions with ?man?
and?woman?, which leads to higher BLEU scores thanour VDR model, but this is based on corpus statis-tics than the observed visual information.
Me-47VDR is betterVDR: A person is playing a saxophone.BRNN: A man is playing a guitarVDR: A person is playing a guitar.BRNN: A man is jumping off a cliffVDR: A person is playing a drum.BRNN: A man is standing on aBRNN is betterVDR: A person is using a computer.BRNN: A man is jumping on a trampolineVDR: A person is riding a horse.BRNN: A group of people riding horsesVDR: A person is below sunglasses.BRNN: A man is reading a bookEqually goodVDR: A person is sitting a table.BRNN: A man is sitting on a chairVDR: A person is using a laptop.BRNN: A man is using a computerVDR: A person is riding a horse.BRNN: A man is riding a horseEqually badVDR: A person is holding a microphone.BRNN: A man is taking a pictureVDR: A person is driving a car.BRNN: A man is sitting on a phoneVDR: A person is driving a car.BRNN: A man is riding a bikeFigure 5: Examples of descriptions generated using VDR and the BRNN in the VLT2K data set.
Keenreaders are encouraged to inspect the second image with a magnifying glass or an object detector.48Figure 6: Optimising the number of detected ob-jects against generated description Meteor scoresfor our model.
Improvements are seen until eightobjects, which suggests good descriptions do notalways need the most confident detections.teor is able to back-off from ?man?
or ?woman?to ?person?
and still give partial credit to the de-scription.
If we replace the gendered referents inthe descriptions generated by the BRNN, its perfor-mance on the VLT2K data set drops by 2.0 Meteorpoints and 6.3 BLEU points.Figure 6 shows the effect of optimising thenumber of objects extracted from an image againstthe eventual Meteor score of a generated descrip-tion in the validation data.
It can be seen thatthe most confidently predicted objects are not al-ways the most useful objects for generating de-scriptions.
Interestingly, the quality of the de-scriptions does not significantly decrease with anincreased number of detected objects, suggestingour model formulation is appropriately discardingunsuitable detections.Figure 5 shows examples of the descriptionsgenerated by VDR and BRNN on the VLT2K val-idation set.
The examples where VDR generatesbetter descriptions than BRNN are because theVDR Parser makes good decisions about whichobjects are interacting in an image.
In the ex-amples where the BRNN is better than VDR, wesee that the multimodal RNN language modelsucceeds at describing intransitive verbs, groupevents, and objects not present in the R-CNN ob-ject detector.
Both models generate bad descrip-tions when the visual input pushes them in thewrong direction, seen at the bottom of the figure.VLT ?
PascalMeteor BLEUVDR 7.4 ?
8.2 9.1 ?
9.2BRNN 12.6 ?
8.1 16.0 ?
10.2Table 3: Sentence-level scores when transferringmodels directly between data sets with no retrain-ing.
The VDR-based approach generates better de-scriptions in the Pascal1K data set if we transferthe model from the VLT2K data set.4.5 Transferring ModelsThe main reason for the low performance of VDRon the Pascal1K data set is that the linguistic andvisual processing steps (Section 2) discard toomany training examples.
We found that only 190of the 4,000 description in the training data wereused to infer VDRs.
This was because most ofthe descriptions did not contain both a subject andan object, as required by our method.
This ob-servation led us to perform a second experimentwhere we transferred the VDR Parsing and Lan-guage Generation models between data sets.
Theaim of this experiment was to determine whetherVDR simply cannot work on more widely diversedata sets, or whether the process we defined toreplicate human VDR annotation was too strict.Table 3 shows the results of the model trans-fer experiment.
In general, neither model is par-ticularly good at transferring between data sets.This could be attributed to the shift in the types ofscenes depicted in each data set.
However, trans-ferring VDR from the VLT2K to the Pascal1K dataset improves the generated descriptions from 7.4?
8.2 Meteor points.
The performance of BRNNsubstantially decreases when transferring betweendata sets, suggesting that the model may be over-fitting its training domain.4.6 DiscussionNotwithstanding the conceptual differences be-tween multi-modal deep learning and learning anexplicit spatial model of object?object relation-ships, two key differences between the BRNN andour approach are the nature visual input and thelanguage generation models.The neural network model can readily use thepre-softmax visual feature vector from any of thepre-trained models available in the Caffe Model49Zoo, whereas VDR is currently restricted to dis-crete object detector outputs from those models.The implication of this is that the VDR-based ap-proach is unable to describe 30% of the data inthe VLT2K data set.
This is due to the object de-tection model not recognising crucial objects forthree of the action classes: cameras, books, andtelephones.
We considered using the VGG-16 pre-trained model from the ImageNet Recognition andLocalization task in the RCNN object detector,thus mirroring the detection model used by theneural network.
Frustratingly, this does not seempossible because none of the 1,000 types of objectsin the recognition task correspond to a person-typeof entity.
One approach to alleviating this problemcould be to use weakly-supervised object localisa-tion (Oquab et al, 2014).The template-based language generation modelused by VDR lacks the flexibility to describe in-teresting prepositional phrases or variety withinits current template.
An n-gram language gener-ator, such as the phrase-based approaches of (Or-tiz et al, 2015; Lebret et al, 2015), that workswithin the constraints imposed by VDR structuremay generate better descriptions of images thanthe current template.5 ConclusionsIn this paper we showed how to infer useful and re-liable Visual Dependency Representations of im-ages without expensive human supervision.
Ourapproach was based on searching for objects inimages, given a collection of co-occurring descrip-tions.
We evaluated the utility of the representa-tions on a downstream automatic image descrip-tion task on two data sets, where the quality of thegenerated text largely depended on the data set.
Ina large data set of people performing actions, thedescriptions generated by our model were com-parable to a state-of-the-art multimodal deep neu-ral network.
In a smaller and more diverse dataset, our approach produced poor descriptions be-cause it was unable to extract enough useful train-ing examples for the model.
In a follow-up exper-iment that transferred the VDR Parsing and Lan-guage Generation model between data, we foundimprovements in the diverse data set.
Our exper-iments demonstrated that explicitly encoding thespatial relationships between objects is a usefulway of learning how to describe actions.There are several fruitful opportunities for fu-ture work.
The most immediate improvement maybe found with broader coverage object detectors.It would be useful to search for objects usingmultiple pre-trained visual detection models, suchas a 200-class ImageNet Detection model and a1,000-class ImageNet Recognition and Localisa-tion model.
A second strand of further work wouldbe to relax the strict mirroring of human annota-tor behaviour when searching for subjects and ob-jects in an image.
It may be possible to learn goodrepresentations using only the nouns in the POStagged description.
Our current approach strictlylimits the inferred VDRs to transitive verbs; im-ages with descriptions such as ?A large cow in afield?
or ?A man is walking?
are also a focus forfuture relaxations of the process for creating train-ing data.
Another direction for future work wouldbe to use a n-gram based language model con-strained by the structured predicted in VDR.
Thecurrent template based method is limiting the gen-eration of objects that are being correctly realisedin images.Tackling the aforementioned future work opensup opportunities to working with larger and morediverse data sets such as the Flickr8K (Hodosh etal., 2013), Flickr30K (Young et al, 2014), and MSCOCO (Lin et al, 2014b) or larger action recogni-tion data sets such as TUHOI (Le et al, 2014) orMPII Human Poses (Andriluka et al, 2014).AcknowledgementsWe thank the anonymous reviewers for their com-ments, and members of LaCo at ILLC and WIL-LOW at INRIA for comments on an earlier versionof the work.
We thank the Database ArchitecturesGroup and the Life Sciences Group at CWI for ac-cess to their NVIDIA Tesla K20 GPUs.
D. El-liott is funded by an Alain Bensoussain Career De-velopment Fellowship, A. P. de Vries is partiallyfunded by COMMIT/.ReferencesMykhaylo Andriluka, Leonid Pishchulin, Peter Gehler,and Bernt Schiele.
2014.
2D Human Pose Estima-tion: New Benchmark and State of the Art Anal-ysis.
In CVPR ?14, pages 3686?3693, Columbus,OH, US.Moshe Bar and Shimon Ullman.
1996.
Spatial Contextin Recognition.
Perception, 25(3):343?52.Irving Biederman, Robert J Mezzanotte, and Jan CRabinowitz.
1982.
Scene perception: Detecting50and judging objects undergoing relational violations.Cognitive Psychology, 14(2):143?177.JH Clark, Chris Dyer, Alon Lavie, and NA Smith.2011.
Better hypothesis testing for statistical ma-chine translation: Controlling for optimizer instabil-ity.
In ACL-HTL ?11, pages 176?181, Portland, OR,U.S.A.Michael Denkowski and Alon Lavie.
2011.
Meteor1.3: Automatic metric for reliable optimization andevaluation of machine translation systems.
In SMTat EMNLP ?11, Edinburgh, Scotland, U.K.Jeff Donahue, Lisa Anne Hendricks, Sergio Guadar-rama, Marcus Rohrbach, Subhashini Venugopalan,Kate Saenko, and Trevor Darrell.
2015.
Long-term Recurrent Convolutional Networks for VisualRecognition and Description.
In CVPR ?15, Boston,MA, U.S.A.David Eigen, Christian Puhrsch, and Rob Fergus.2014.
Depth Map Prediction from a Single Imageusing a Multi-Scale Deep Network.
In NIPS 27,Lake Tahoe, CA, U.S.A, June.Desmond Elliott and Frank Keller.
2013.
ImageDescription using Visual Dependency Representa-tions.
In EMNLP ?13, pages 1292?1302, Seattle,WA, U.S.A.Desmond Elliott and Frank Keller.
2014.
ComparingAutomatic Evaluation Measures for Image Descrip-tion.
In ACL ?14, pages 452?457, Baltimore, MD,U.S.A.Desmond Elliott, Victor Lavrenko, and Frank Keller.2014.
Query-by-Example Image Retrieval using Vi-sual Dependency Representations.
In COLING ?14,pages 109?120, Dublin, Ireland.Mark Everingham, Luc Van Gool, ChristopherWilliams, John Winn, and Andrew Zisserman.2010.
The PASCAL Visual Object Classes Chal-lenge.
IJCV, 88(2):303?338.Hao Fang, Saurabh Gupta, Forrest Iandola, RupeshSrivastava, Li Deng, Piotr Doll?ar, Jianfeng Gao,Xiaodong He, Margaret Mitchell, John C. Platt,C.
Lawrence Zitnick, and Geoffrey Zweig.
2015.From Captions to Visual Concepts and Back.
InCVPR ?15, Boston, MA, U.S.A.Ali Farhadi, Mohsen Hejrati, Mohammad AminSadeghi, Peter Young, Cyrus Rashtchian, JuliaHockenmaier, and David Forsyth.
2010.
Everypicture tells a story: generating sentences from im-ages.
In ECCV ?10, pages 15?29, Heraklion, Crete,Greece.Yansong Feng and Mirella Lapata.
2008.
AutomaticImage Annotation Using Auxiliary Text Informa-tion.
In ACL ?08, pages 272?280, Colombus, Ohio.Ross Girshick, Jeff Donahue, Trevor Darrell, and Ji-tendra Malik.
2014.
Rich feature hierarchies for ac-curate object detection and semantic segmentation.CoRR, abs/1311.2.Micah Hodosh, Peter Young, and Julia Hockenmaier.2013.
Framing Image Description as a RankingTask: Data, Models and Evaluation Metrics.
JAIR,47:853?899.Yangqing Jia, Evan Shelhamer, Jeff Donahue, SergeyKarayev, Jonathan Long, Ross B. Girshick, SergioGuadarrama, and Trevor Darrell.
2014.
Caffe: Con-volutional Architecture for Fast Feature Embedding.In MM ?14, pages 675?678, Orlando, FL, U.S.A.Andrej Karpathy and Li Fei-Fei.
2015.
Deep Visual-Semantic Alignments for Generating Image De-scriptions.
In CVPR ?15, Boston, MA, U.S.A.Andrej Karpathy, Armand Joulin, and Li Fei-Fei.
2014.Deep Fragment Embeddings for Bidirectional ImageSentence Mapping.
In NIPS 28, Montreal, Quebec,Canada.Girish Kulkarni, Visruth Premraj, Sagnik Dhar, SimingLi, Yejin Choi, Alexander C. Berg, and Tamara L.Berg.
2011.
Baby talk: Understanding and generat-ing simple image descriptions.
In CVPR ?11, pages1601?1608, Colorado Springs, CO, U.S.A.Polina Kuznetsova, Vicente Ordonez, Alexander C.Berg, Tamara L. Berg, and Yejin Choi.
2012.
Col-lective Generation of Natural Image Descriptions.In ACL ?12, pages 359?368, Jeju Island, South Ko-rea.Dieu-thu Le, Jasper Uijlings, and Raffaella Bernardi.2014.
TUHOI : Trento Universal Human Object In-teraction Dataset.
In WVL at COLING ?14, pages17?24, Dublin, Ireland.Remi Lebret, Pedro O. Pinheiro, and Ronan Collobert.2015.
Phrase-based Image Captioning.
In ICML?15, Lille, France, February.Siming Li, Girish Kulkarni, Tamara L. Berg, Alexan-der C. Berg, and Yejin Choi.
2011.
Composing sim-ple image descriptions using web-scale n-grams.
InCoNLL ?11, pages 220?228, Portland, OR, U.S.A.Chin-Yew Lin and Franz Josef Och.
2004.
Auto-matic evaluation of machine translation quality us-ing longest common subsequence and skip-bigramstatistics.
In ACL ?04, pages 605?612, Barcelona,Spain.Min Lin, Qiang Chen, and Shuicheng Yan.
2014a.Network In Network.
In ICLR ?14, volumeabs/1312.4, Banff, Canada.Tsung-Yi Lin, Michael Maire, Serge Belongie,Lubomir Bourdev, Ross Girshick, James Hays,Pietro Perona, Deva Ramanan, C. Lawrence Zitnick,and Piotr Doll?ar.
2014b.
Microsoft COCO: Com-mon Objects in Context.
In ECCV ?14, pages 740?755, Zurich, Switzerland.51GD Logan and DD Sadler.
1996.
A computationalanalysis of the apprehension of spatial relations.
InPaul Bloom, Mary A. Peterson, Lynn Nadel, andMerrill F. Garrett, editors, Language and Space,pages 492?592.
MIT Press.Junhua Mao, Wei Xu, Yi Yang, Yiang Wang, andAlan L. Yuille.
2015.
Deep captioning with mul-timodal recurrent neural networks (m-rnn).
In ICLR?15, volume abs/1412.6632, San Diego, CA, U.S.A.Emily E. Marsh and Marilyn Domas White.
2003.
Ataxonomy of relationships between images and text.Journal of Documentation, 59(6):647?672.Guido Minnen, John Carroll, and Darren Pearce.
2001.Applied morphological processing of English.
Nat-ural Language Engineering, 7(3):207?223.Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Ya-maguchi, Karl Stratos, Alyssa Mensch, Alex Berg,Tamara Berg, and Hal Daum.
2012.
Midge : Gen-erating Image Descriptions From Computer VisionDetections.
In EACL ?12, pages 747?756, Avignon,France.Joakim Nivre, Johan Hall, Jens Nilsson, AtanasChanev, G?ulsen Eryigit, Sandra K?ubler, SvetoslavMarinov, and Erwin Marsi.
2007.
MaltParser:A language-independent system for data-driven de-pendency parsing.
Natural Language Engineering,13(2):1.Maxime Oquab, Leon Bottou, Ivan Laptev, and JosefSivic.
2014.
Learning and Transferring Mid-levelImage Representations Using Convolutional NeuralNetworks.
In CVPR ?14, pages 1717?1724, Colum-bus, OH, US.Luis M. G. Ortiz, Clemens Wolff, and Mirella Lapata.2015.
Learning to Interpret and Describe AbstractScenes.
In NAACL ?15, Denver, CO, U.S.A.Erwin Panofsky.
1939.
Studies in Iconology.
OxfordUniversity Press.Kishore Papineni, Salim Roukos, Todd Ward, andWJ Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In ACL ?02, pages311?318, Philadelphia, PA, U.S.A.DA Randell, Z Cui, and AG Cohn.
1992.
A spatiallogic based on regions and connection.
In Principlesof Knowledge Representation and Reasoning, pages165?176.Cyrus Rashtchian, Peter Young, Micah Hodosh, andJulia Hockenmaier.
2010.
Collecting image anno-tations using Amazon?s Mechanical Turk.
In AMTat NAACL ?10, pages 139?147, Los Angeles, CA,U.S.A.Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,Sanjeev Satheesh, Sean Ma, Zhiheng Huang, An-drej Karpathy, Aditya Khosla, Michael Bernstein,Alexander C Berg, and Li Fei-Fei.
2014.
ImageNetLarge Scale Visual Recognition Challenge.Mohammad A Sadeghi and Ali Farhadi.
2011.
Recog-nition Using Visual Phrases.
In CVPR ?11, pages1745?1752, Colorado Springs, CO, U.S.A.Sara Shatford.
1986.
Analysing the Subject of a Pic-ture: A Theoretical Approach.
Cataloging & Clas-sification Quarterly, 6(3):39?62.Karen Simonyan and Andrew Zisserman.
2015.
VeryDeep Convolutional Networks for Large-Scale Im-age Recognition.
In ICLR ?15, volume abs/1409.1,San Diego, CA, U.S.A.Richard Socher, Andrej Karpathy, Q Le, C Manning,and A Ng.
2014.
Grounded Compositional Seman-tics for Finding and Describing Images with Sen-tences.
TACL, 2:207?218.Kristina Toutanova, Dan Klein, and Christopher DManning.
2003.
Feature-Rich Part-of-Speech Tag-ging with a Cyclic Dependency Network.
In HLT-NAACL ?03, pages 173?180, Edmonton, Canada.Oriol Vinyals, Alexander Toshev, Samy Bengio, andDumitru Erhan.
2015.
Show and tell: A neural im-age caption generator.
In CVPR ?15, Boston, MA,U.S.A.Yezhou Yang, Ching Lik Teo, Hal Daum?e III, andYiannis Aloimonos.
2011.
Corpus-Guided Sen-tence Generation of Natural Images.
In EMNLP ?11,pages 444?454, Edinburgh, Scotland, UK.Mark Yatskar, Michel Galley, L Vanderwende, andL Zettlemoyer.
2014.
See No Evil, Say No Evil:Description Generation from Densely Labeled Im-ages.
In *SEM, pages 110?120, Dublin, Ireland.Peter Young, Alice Lai, Micah Hodosh, and JuliaHockenmaier.
2014.
From image descriptions to vi-sual denotations: New similarity metrics for seman-tic inference over event descriptions.
TACL, 2:67?78.52
