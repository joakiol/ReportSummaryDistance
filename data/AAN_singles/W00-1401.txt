Evaluation Metrics for GenerationSr in ivas  Banga lore  and  Owen Rambow and Steve  Whi t takerAT&T Labs  - Research180 Park  Ave,  PO Box 971F lo rham Park ,  N J  07932-0971, USA{srini,r~mbow, stevew}@reSearch, art tomAbst rac tCertain generation applications may profit from theuse of stochastic methods.
In developing stochasticmethods, it is crucial to be able to quickly assessthe relative merits of different approaches or mod-els.
In this paper, we present several types of in-trinsic (system internal) metrics which we have usedfor baseline quantitative assessment.
This quanti-tative assessment should then be augmented to afuller evaluation that examines qualitative aspects.To this end, we describe an experiment that testscorrelation between the quantitative metrics and hu-man qualitative judgment.
The experiment confirmsthat intrinsic metrics cannot replace human evalu-ation, but some correlate significantly with humanjudgments of quality and understandability and canbe used for evaluation during development.1 In t roduct ionFor many applications in natural language genera-tion (NLG), the range of linguistic expressions thatmust be generated is quite restricted, and a gram-mar for a surface realization component can be fullyspecified by hand.
Moreover, iLL inany cases it isvery important not to deviate from very specific out-put in generation (e.g., maritime weather reports),in which case hand-crafted grammars give excellentcontrol.
In these cases, evaluations of the generatorthat rely on human judgments (Lester and Porter,I997) or on human annotation of the test corpora(Kukich, 1983) are quite sufficient .
.
.
.However.
in other NLG applications the variety ofthe output is much larger, and the demands onthe quality of the output are solnewhat less strin-gent.
A typical example is NLG in the context of(interlingua- or transfer-based) inachine translation.Another reason for relaxing the quality of the out-put may be that not enough time is available to de-velop a full gramnlar for a new target, language inNLG.
ILL all these cases, stochastic methods providean alternative to hand-crafted approaches to NLG.1To our knowledge, the first to use stochastic tech-niques in an NLG realization module were Langkildeand Knight (1998a) and (~998b) (see also (Langk-ilde, 2000)).
As is the case for stochastic approachesin natural anguage understanding, the research anddevelopment itself requires an effective intrinsic met-ric in order to be able to evaluate progress.In this paper, we discuss several evaluation metricsthat we are using during the development of FERGUS(Flexible Empiricist/Rationalist Generation UsingSyntax).
FERCUS, a realization module, followsKnight and Langkilde's eminal work in using ann-gram language model, but we augment it with atree-based stochastic model and a lexicalized syntac-tic grammar.
The metrics are useful to us as rela-tive quantitative assessments of different models weexperiment with; however, we do not pretend thatthese metrics in themselves have any validity.
In-stead, we follow work done in dialog systems (Walkeret al, 1997) and attempt o find metrics which ontim one hand can be computed easily but on theother hand correlate with empirically verified humanjudgments in qualitative categories such as readabil-ity.The structure of the paper is as follows.
In Section 2,we briefly describe the architecture of FEacUS, andsome of the modules.
In Section 3 we present fourmetrics and some results obtained with these met-rics.
In Section 4 we discuss the for experimentalvalidation of the metrics using human judgments,and present a new metric based on the results ofthese experiments.
In Section 5 we discuss some ofthe 'many problematic issues related to  the use  Ofmetrics and our metrics in particular, and discusson-going work.2 Sys tem Overv iewFERGUS is composed of three mQdules: .the TreeChooser, tile Unraveler, and the Linear Precedence(LP) Chooser (Figure 1).
Tile input to the system isa dependency tree as shown in Figure 2. t Note thatthe nodes are unordered and are labeled only withlexemes, not with any sort of syntactic annotations.
2The Tree Chooser uses a stochastic tree model tochoose syntactic properties (expressed as trees in aTree Adjoining Grammar) for the nodes in the in-put structure.
This step can be seen as analogous to"supertagging" -(Bangalore-und doshh 1:999);.
exceptthat now supertags (i.e., names of trees which en-code the syntactic properties of a lexical head) mustbe found for words in a tree rather than for wordsin a linear sequence.
The Tree Chooser makes thesiinplifying assumptions that the choice of a tree fora node depends only on its daughter nodes, thus al-lowing for a top-down algorithm.
The Tree Chooserdraws on a tree model, which is a analysis in termsof syntactic dependency for 1,000,000 words of theWall Street Journal (WSJ).
3The supertagged tree which is output from the TreeChooser still does not fully determine the surfacestring, because there typically are different ways toattach a daughter node to her mother (for example,an adverb can be placed in different positions withrespect o its verbal head).
The Unraveler thereforeuses the XTAG grammar of English (XTAG-Group,1999) to produce a lattice of all possible lineariza-tions that are compatible with the supertagged tree.Specifically, the daughter nodes are ordered with re-spect to the head at each level of the derivation tree.In cases where the XTAG grammar allows a daugh-ter node to be attached at more than one place inthe mother supertag (as is the case in our exam-ple for was and for; generaUy, such underspecifica-tion occurs with adjuncts and with arguments if theirsyntactic role is not specified), a disjunction of allthese positions is assigned to the daughter node.
Abottom-up algorithm then constructs a lattice thatencodes the strings represented by each level of thederivation tree.
The lattice at the root of the deriva-tion tree is the result of the Unraveler.Finally.
the LP Chooser chooses the most likelytraversal of this lattice, given a linear language1The sentence generated by this tree is a predicativenounconstruction.
The XTAG grammar analyzes these as beingheaded by the noun,rather-than by.the copula, and we fol-low the XTAG analysis.
However, it would of course also bepossible to use a graminar that allows for the copula-headedanalysis.21n the system that we used in the experiments describedin Section 3. all words (including function words) need to bepresent in the input representation, fully inflected.
Further-more, there is no indication of syntactic role at all.
This is ofcourse unrealistic f~r applications see ,Section 5 for furtherrenlarks.
:3This wa~s constructed from the Penn Tree Bank usingsome heuristics, sirice the.
l)enn Tree Bank does not containfull head-dependerit information; as a result of the tlse ofheuristics, the Tree Model is tint fully correct.2ITAG Derivation Treewithout SupertagsiOne single semi-specif ied~TAG Deri~tion TreesWord Latticei\[ cPc.oo=, \]lStringFigure 1: Architecture of FERGUSestimatethere was no cost forIphasethe secondFigure 2: Input to FERGUSmodel (n-gram).
The lattice output from the Un-raveler encodes all possible word sequences permit-ted by the supertagged ependency structure.
\Verank these word sequences in the order of their likeN-hood by composing the lattice with a finite-state ma-chine representing a trigram language model.
Thismodel has been constructed from the 1.000,0000words WSJ training corpus.
We pick the best paththrough the lattice resulting from the compositionusing the Viterbi algorithm, and this top rankingword sequence is the output of the LP Chooser andthe generator.When we tally the results we obtain the score shownin the first column of Table 1.Note that if there are insertions and deletions, thenumber of operations may be larger than the numberof tokens involved for either one of the two strings.As a result, the simple string accuracy metric may3 Base l ine -Qua_nt i tmt ive ,Met r i cs  ...,:-~.--..~,-:..,be.
:..~eg~i~ee (t:hoagk:it, As, nevel:-greater._than 1, ofWe have used four different baseline quantitativemetrics for evaluating our generator.
The first twometrics are based entirely on the surface string.
Thenext two metrics are based on a syntactic represen-tation of the sentence.3.1 S t r ing -Based  Met r i csWe employ two metrics that measure the accuracyof a generated string.
The first metric, s imple ac-curacy,  is the same string distance metric used formeasuring speech recognition accuracy.
This met-ric has also been used to measure accuracy of MTsystems (Alshawi et al, 1998).
It is based on stringedit distance between the output of the generationsystem and the reference corpus string.
Simple ac-curacy is the number of insertion (I), deletion (D)and substitutions (S) errors between the referencestrings in the test corpus and the strings produced bythe generation model.
An alignment algorithm us-ing substitution, insertion and deletion of tokens asoperations attempts to match the generated stringwith the reference string.
Each of these operationsis assigned a cost value such that a substitution op-eration is cheaper than the combined cost of a dele-tion and an insertion operation.
The alignment al-gorithm attempts to find the set of operations thatminimizes the cost of aligning the generated stringto tile reference string.
Tile metric is summarizedin Equation (1).
R is the number of tokens in thetarget string.course).The simple string accuracy metric penalizes a mis-placed token twice, as a deletion from its expectedposition and insertion at a different position.
This isparticularly worrisome in our case, since in our eval-uation scenario the generated sentence is a permuta-tion of the tokens in the reference string.
We there-fore use a second metric, Generat ion  Str ing Ac-curacy,  shown in Equation (3), which treats dele-tion of a token at one location in the string and theinsertion of the same token at another location inthe string as one single movement error (M).
Thisis in addition to the remaining insertions (I ') anddeletions (D').
(3) Generat ion  St r ing  Accuracy  =( 1 -- M~-/~.P-~--~-~)In our example sentence (2), we see that the inser-tion and deletion of no can be collapsed into onemove.
However, the wrong positions of cost and ofphase are not analyzed as two moves, since one takesthe place of the other, and these two tokens still re-sult in one deletion, one substitution, and one inser-tion.
5 Thus, the generation string accuracy depe-nalizes simple moves, but still treats complex moves(involving more than one token) harshly.
Overall,the scores for the two metrics introduced so far areshown in the first two columns of Table 1.3.2 Tree-Based Metr ics(1) Simple Str ing Accuracy  = (1 I+*)+s I? )
\Vhile tile string-b~u~ed metrics are very easy to ap-ply, they have the disadvantage that they do notreflect the intuition that all token moves are not Consider tile fifth)wing example.
The target sentenceis on top, tile generated sentence below.
Tile third equally "bad".
Consider the subphrase stimate forline represents the operation needed to.
transfor m .. phase the second of the sentence in (2).
\Vhile this isone sentence into another: a period is used t.o indi- bad; i t  seems better:tiara rt alternative such as es-cate that no operation is needed.
4(2) There was no cost estimate for tileThere was estimate for l)hase tiled (1 isecond phasesecond no costi s?
I Note that the metric is symmetric,timate phase for tile second.
Tile difference betweenthe two strings is that the first scrambled string, butnot tile second,  can be read off fl'om tile dependencytree for the sentence (as shown ill Figure 2) with-out violation of projectivity, i.e., without (roughlySTiffs shows the importance of the alignment algorithm inthe definition of Ihese two metrics: had it.
not, aligned phaseand cost as a substitution (but each with an empty positionin the other~string-:instead),, then ~khe simple string accuracywould have 6 errors instead of 5, but the generation stringaccuracy would have 3 errors instead of ,1,speaking) creating discontinuous constituents.
Ithas long been observed (though informally) that thedependency trees of a vast majority of sentences inthe languages of the world are projective (see e.g.
(Mel'euk, 1988)), so that a violation of projectivityis presumably a more severe rror than a word ordervariation that does not violate projectivity.We designed thet ree-based ' -acet t rucymetr i cs  inorder to account for this effect.
Instead of compar-ing two strings directly, we relate the two stringsto a dependency tree of the reference string.
Foreach treelet (i.e., non-leaf node with all of its daugh-ters) of the reference dependency tree, we constructstrings of the head and its dependents in the orderthey appear in the reference string, and in the orderthey appear in the result string.
We then calculatethe number of substitutions, deletions, and inser-tions as for the simple string accuracy, and the num-ber of substitutions, moves, and remaining deletionsand insertions as for the generation string metrics,for all treelets that form the dependency tree.
Wesum these scores, and then use the values obtainedin the formulas given above for the two string-basedmetrics, yielding the S imple  Tree Accuracy  andGenerat ion  Tree Accuracy .
The scores for ourexample sentence are shown in the last two columnsof Table 1.3.3 Eva luat ion  Resu l tsThe simple accuracy, generation accuracy, simpletree accuracy and generation tree accuracy for thetwo experiments are tabulated in Table 2.
The testcorpus is a randomly chosen subset of 100 sentencesfrom the Section 20 of WSJ.
The dependency struc-tures for the test sentences were obtained automat-ically from converting the Penn TreeBank phrasestructure trees, in the same way as was done toCreate the training corpus.
The average length ofthe test sentences i 16.7 words with a longest sen-tence being 24 words in length.
As can be seen, thesupertag-based model improves over the baseline LRmodel on all four baseline quantitative metrics.4 Qua l i ta t ive  Eva luat ion  o f  theQuant i ta t ive  Met r i cs4.1 The  Exper imentsWe have presented four metrics which we can com-pute automatically.
In order to determine whetherthe metrics correlate with independent notions un-derstandability or quality, we have performed eval-uation experiments with human subjects.In the web-based experiment, we ask human sub-jects to read a short paragraph from the WSJ.
Wepresent hree or five variants of the last sentence ofthis paragraph on the same page, and ask the sub-ject to judge them along two dimensions:Here we summarize two experiments that we haveperformed that use different tree nmdels.
(For amore detailed comparisons of different tree models,see (Bangalore and Rainbow, 2000).
)o For the baseline experiment, we impose a ran-dom tree structure for each sentence of the cor-pus and build a Tree Model whose parametersconsist of whether a lexeme ld precedes or fol-lows her mother lexeme \[ .... We call this theBaseline Left-Right (LR) Model.
This modelgenerates There was est imate for  phase the sec-ond no cost .
for our example input.o In the second experiment we use the-systemas described in Section 2.
We employ thesupertag-based tree model whose parametersconsist of whether a lexeme ld with supertagsd is a dependent of lexeme 1,,, with supertags,,,.
Furthermore we use the information pro-vided by the XTAG grammar to order the de-pendents.
This model generates There was nocost est imate for" the second phase .
for our ex-ample input, .which is indeed.the sentence foundin the WS.I.o Unders tandab i l i ty :  How easy is this sentenceto understand?
Options range from "Extremelyeasy" (= 7) to "Just barely possible" (=4) to"Impossible" (=1).
(Intermediate numeric val-ues can also be chosen but have no descriptionassociated with them.
)o Qual i ty:  How well-written is this sentence?Options range from "Extremely well-written'"(= 7) to "Pretty bad" (=4) to "Horrible (=1).(Again.
intermediate numeric values can also t)echosen, but have no description associated withthem.
)The 3-5 variants of each of 6 base sentences are con-strutted by us (most of the variants lraxre not actu-ally been generated by FERGUS) to sample multiplevalues of each intrinsic metric as well as to contrastdifferences between the intrinsic measures.
Thus forone sentence "tumble", two of the five variants haveapproximately identical values for each of the met-rics but with the absolute values being high (0.9)and medium (0.7) respectively.
For two other sen-\[,('II('(}S ~ve have contrasting intrinsic values for treetrod string based measures.
For .the final sentencewe have contrasts between the string measures withMetric Simple Generation Simple GenerationString Accuracy String Accuracy Tree Accuracy Tree AccuracyTotal number of tokens 9 9 9 9UnchangedSubstitutionsInsertionsDeletionsMoves612206033O..6000.3Total number of problems 5 4 " 6 3Score 0.44 0.56 0.33 0.67Table 1: Scores for the sample sentence according to the four metricsTree Model Simple Generation Simple GenerationString Accuracy String Accuracy Tree Accuracy Tree Accuracy iBaseline LR Model 0.41 0.56 0.41 0.63. .
.
.
.
iSupertag-based Model 0.58 0.72 0.65 0.76 ITable 2: Performance resultstree measures being approximately equal.
Ten sub-jects who were researchers from AT&T carried outthe experiment.
Each subject made a total of 24judgments.Given the variance between subjects we first nor-malized the data.
We subtracted the mean scorefor each subject from each observed score and thendivided this by standard eviation of the scores forthat subject.
As expected our data showed strongcorrelations between ormalized understanding andquality judgments for each sentence variant (r(22) =0.94, p < 0.0001).Our main hypothesis i that the two tree-based met-rics correlate better with both understandability andquality than the string-based metrics.
This was con-firmed.
Correlations of the two string metrics withnormalized understanding for each sentence variantwere not significant (r(22) = 0.08 and rl.2.21 = 0.23, forsimple accuracy and generation accuracy: for bothp > 0.05).
In contrast both of the tree metrics weresignificant (r(2.2) = 0.51 and r(22) = 0.48: for treeaccuracy and generation tree accuracy, for both p< 0.05).
Similar results were achieved--for thegor-realized quality metric: (r(.2.21 = 0.16 and r(221 =0,33: for simple accuracy and generation accuracy,for both p > 0.05), (r(ee) = 0.45 and r(.2.2) = 0.42,for tree accuracy and generation tree accuracy, forboth p < 0.05).A second aim of ()Lit" qualitative valuation was tolest various models of the relationship between in-trinsic variables and qualitative user judgments.
\Veproposed a mmlber-of'models:in which various conL-from the two tree modelsbinations of intrinsic metrics were used to predictuser judgments of understanding and quality.
.Weconducted a series of linear regressions with nor-malized judgments of understanding and quality asthe dependent measures and as independent mea-sures different combinations of one of our four met-rics with sentence length, and with the "problem"variables that we used to define the string metrics(S, I, D, M, I ' ,  D' - see Section 3 for definitions).One sentence variant was excluded from the data set,on the grounds that the severely "mangled" sentencehappened to turn out well-formed and with nearlythe same nleaning as the target sentence.
The re-sults are shown in Table 3.We first tested models using one of our metrics as asingle intrinsic factor to explain the dependent vari-able.
We then added the "problem" variables.
6 andcould boost tile explanatory power while maintain-ing significance.
In Table 3, we show only some con>binations, which show that tile best results were ob-tained by combining the simple tree accuracy withthe number of Substitutions (S) and the sentencelength.
As we can see, the number of substitutions..... has an.important effecVon explanatory.power,, whilethat of sentence length is much more modest (butmore important for quality than for understanding).Furthermore, the number of substitutions has moreexplanatory power than the number of moves (andin fact.
than any of the other "problem" variables).The two regressions for understanding and writingshow very sinlilar results.
Normalized understand-6None of tile "problem" variables have much explanatorypower on their own (nor (lid they achieve significance).Model User Metric Explanatory Power Statistical Significance(R 2) (p value)Simple String Accuracy Understanding 0.02 0.571Simple String Accuracy Quality 0.00 0.953Generation String AccuracyGeneration String AccuracyS imple  T ree  Accuracy .
.
.
.
.
.
.
.
- ~,Simple Tree AccuracyGeneration Tree AccuracyGeneration Tree AccuracySimple Tree Accuracy + SSimple Tree Accuracy + SSimple Tree Accuracy + MSimple Tree Accuracy + MSimple Tree Accuracy + LengthSimple Tree Accuracy + LengthSimple Tree Accuracy + S + LengthSimple Tree Accuracy + S + LengthUnderstandingQuality::Unders~aatdiagQualityUnderstandingQuality0.020.05: .
,  .
.
.
.
0.360.340.350.350.5840.327. .
?
.
.
.
.
.
.
.
.
.
.
".0;003.. - .
:0.0030.0030.003Understanding 0.48 0.001Quality 0.47 0.002Understanding 0.38 0.008Quality 0.34 0.015Understanding 0.40 0.006Quality 0.42 0.0060.510.53UnderstandingQuality0.0030.002Table 3: Testing different models of user judgments (S is number of substitutions, M number of movedelements)ing was best modeled as:Normalized understanding = 1.4728*sim-ple tree accuracy - 0.1015*substitutions-0.0228 * length - 0.2127.This model was significant: F(3,1 .9  ) = 6.62, p < 0.005.Tile model is plotted in Figure 3. with the data pointrepresenting the removed outlier at the top of thediagram.This model is also intuitively plausible.
The simpletree metric was designed to measure the quality of asentence and it has a positive coefficient.
A substitu-tion represents a case in the string metrics in whichnot only a word is in the wrong place, but the wordthat should have been in that place is somewhereelse, Therefore, substitutions, more than moves orinsertions or deletions, represent grave cases of wordorder anomalies.
Thus, it is plausible to penalizethem separately.
(,Note that tile simple tree accuracyis bounded by 1, while the number of substitutions il/ounded by the length of the sentence.
In practice,in our sentences S ranges between 0 and 10 witha mean of 1,583.)
Finally, it is also plausible thatlonger sentem:es are more difficult to understand, sothat length has a (small) negative coefficient.We now turn to model for quality,Normalized quality = 1.2134*simple treeaccuracy- 0.0839*substitutions - 0.0280 *length - 0.0689.This model was also significant: F(3A9) = 7.23, p <0.005.
The model is plotted in Figure 4, with thedata point representing the removed outlier at thetop of the diagram.
The quality model is plausiblefor the same reasons that the understanding modelis.L2PPj ,1".i /.
/ /// /?
.
, j --05 O0 05I a728"SLmo~eTteeMel~ - 0 I015"S - 0 0228"lerN~hFigure 3: Regression for Understanding6oDu~h~- (0  -O5 0.0 05  I 01 4728*S,mpleT~eeMetr ?
- 0 I015"S - 0 0228"len~l~hFigure 4: Regression for Quality (Well-Formedness)4.2 Two New Metr i csA further goal of these experiments was to obtainone or two metrics which can be automatically com-puted, and which have been shown to significantlycorrelate with relevant human judgments?
We use asa starting point the two linear models for normalizedunderstanding and quality given above, but we maketwo changes.
First, we observe that while it is plau-sible to model human judgments by penalizing longsentences, this seems unmotivated in an accuracymetric: we do not want to give a perfectly generatedlonger sentence a lower score than a perfectly gener-ated shorter sentence.
We therefore use models thatjust use the simple tree accuracy and the numberof substitutions as independent variables?
Second,we note that once we have done so, a perfect sen-tence gets a score of 0.8689 (for understandability)or 0.6639 (for quality).
We therefore divide by thisscore to assure that a perfect sentence gets a scoreof 1.
(As for the previously introduced metrics, thescores may be less than 0.
)\Ve obtain the following new metrics:(4) Unders tandab i l i ty  ?
Accuracy  =(1.3147*simple tree accuracy 0.1039*sub-stitutions - 0.4458) / 0.8689 -(5) Qua l i ty  Accuracy  = (1.0192*simple tree ac-curacy-  0.0869*substitutions - 0.3553) / 0.6639\ \e  reevahtated our system and the baseline modelusing the new metrics, in order to veri(v whetherthe nloro motivated metrics we have developed stillshow that FER(;I:S improves l)erforniance over thebaseline.
This is indeed the  case: the resuhs areSlllnm.arized ill Tabh'-t.Tree Model Understandability QualityAccuracy AccuracyBaseline -0.08 -0.12Supertag-based 0.44 0.42.
Table 4: Performance results from the .two tree mod-..... els:using the:new metrics .
.
.
.
.
.
.5 D iscuss ionWe have devised the baseline quantitative metricspresented in this paper for internal use during re-search and development, in order to evaluate dif-ferent versions of FERGUS.
However, the questionalso arises whether they can be used to compare twocompletely different realization modules.
In eithercase, there are two main issues facing the proposedcorpus-based quantitative valuation: does it gener-alize and is it fair?The problem in generalization is this: can we usethis method to evaluate anything other than ver-sions of FERGUS which generate sentences from theWSJ?
We claim that we can indeed use the quan-titative evaluation procedure to evaluate most real-ization modules generating sentences from any cor-pus of unannotated English text.
The fact that thetree-based metrics require dependency parses of thecorpus is not a major impediment.
Using exist-ing syntactic parsers plus ad-hoc postprocessors asneeded, one can create the input representations tothe generator as well as the syntactic dependencytrees needed for the tree-based metrics.
The factthat the parsers introduce errors should not affectthe way the scores are used, namely as relative scores(they have no real value absolutely).
Which realiza-tion modules can be evaluated?
First, it is clearthat our approach can only evaluate single-sentencerealization modules which may perform some sen-tence planning tasks, but cruciaUy not including sen-tence scoping/aggregation.
Second, this approach:only works for generators whose input representa-tion is fairly "syntactic".
For example, it may bedifficult to evaluate in this manner a generator that-uses semanzic roles in-its inpntrepresent~ion,  sincewe currently cannot map large corpora of syntac-tic parses onto such semantic representations, andtherefore cannot create the input representation forthe evaluation.The second question is that of fairness of the evalu-ation.
FE\[,tGt.
'S as described in this paper is of lim-ited use.
since it only chooses word order (and, to acertain extent, syntactic structure).
Other realiza-tion and sentence planning tin{ks-which are neededfor most applications and which may profit from astochastic model include lexical choice, introductionof function words and punctuation, and generationof morphology.
(See (Langkilde and Knight, 1998a)for a relevant discussion.
FERGUS currently can per-form punctuation and function word insertion, andmorphology and lexical choice are under develop-ment.)
The question arises whether our metrics will.
fairly measure the:quality,~of,a, more comp!ete real~ ....ization module (with some sentence planning).
Oncethe range of choices that the generation componentmakes expands, one quickly runs into the problemthat, while the gold standard may be a good way ofcommunicating the input structure, there are usu-ally other good ways of doing so as well (using otherwords, other syntactic constructions, and so on).Our metrics will penalize such variation.
However,in using stochastic methods one is of course preciselyinterested in learning from a corpus, so that the factthat there may be other ways of expressing an inputis less relevant: the whole point of the stochastic ap-proach is precisely to express the input in a mannerthat resembles as much as possible the realizationsfound in the corpus (given its genre, register, id-iosyncratic hoices, and so on).
Assuming the testcorpus is representative of the training corpus, wecan then use our metrics to measure deviance fromthe corpus, whether it be merely in word order or interms of more complex tasks such as lexical choiceas well.
Thus, as long as the goal of the realizeris to enmlate as closely as possible a given corpus(rather than provide a maximal range of paraphras-tic capability), then our approach can be used forevaluation, rAs in the case of machine translation, evaluation ingeneration is a complex issue.
(For a discussion, see(Mellish and Dale, 1998).)
Presumably, the qual-ity of most generation systems can only be assessedat a system level in a task-oriented setting (ratherthan by taking quantitative measures or by askinghumans for quality assessments).
Such evaluationsare costly, and they cannot be the basis of work instochastic generation, for which evaluation is a fre-quent step in research and development.
An advan-tage of our approach is that our quantitative metricsallow us to evaluate without human intervention, au-tomatically and objectively (objectively with respectto the defined metric,-that is).- Independently, theuse of the metrics has been validated using humansubjects (as discussed in Section 4): once this hashappened, the researcher can have increased confi-dence that choices nlade in research and develop-ment based on the quantitative metrics will in fact7We could also assume a set of acceptable paraphrases foreach sentence in the test corpus.
Our metrics are run on allparaphrases, and the best score chosen.
However.
for manyapplications it will not be emsy to construct such paraphrasesets, be it by hand or automatically.8correlate with relevant subjective qualitative mea-sures.ReferencesHiyan Alshawi, Srinivas Bangalore, and Shona Dou-glas.
1998.
Automatic acquisition of hierarchical~traalsduatian.
:models :for ~machine.
tr:anslation, tnProceedings of the 36th Annual Meeting Associationfor Computational Linguistics, Montreal, Canada.Srinivas Bangalore and Aravind Joshi.
1999.
Su-pertagging: An approach to almost parsing.
Com-putational Linguistics, 25(2).Srinivas Bangalore and Owen Rambow.
2000.
Ex-ploiting a probabilistic hierarchical model for gemeration.
In Proceedings of the 18th InternationalConference on Computational Linguistics (COLING2000), Saarbriicken, Germany.Karen Kukich.
1983.
Knowledge-Based Report Gen-eration: A Knowledge Engineering Approach to Nat-ural Language Report Generation.
Ph.D. thesis, Uni-versity of Pittsuburgh.Irene Langkilde and Kevin Knight.
1998a.
Gener-ation that exploits corpus-based statistical knowl-edge.
In 36th Meeting of the Association for Com-putational Linguistics and 17th International Con-ference on Computational Linguistics (COLING-ACL'98), pages 704-710, Montreal, Canada.Irene Langkilde and Kevin Knight.
1998b.
Thepractical value of n-grams in generation.
In Proceed-ings of the Ninth International Natural LanguageGeneration Workshop (INLG'98), Niagara-on-the-Lake, Ontario.Irene Langkilde.
2000.
Forest-based statistical sen-tence generation.
In 6th Applied Natural LanguageProcessing Conference (ANLP'2000), pages 170-177, Seattle, WA.James C. Lester and Bruce W. Porter.
1997.
De-veloping and empirically evaluating robust explana-tion generators: The KNIGHT experiments.
Compu-tational Linguistics.
23(1):65-102.Igor A. Mel'~uk.
19S8.
Dependency Syntax: Theoryand Practice.
State University of New ~%rk Press.New York.Chris Mellish and Robert Dale.
1998.
Evahlation inthe context of natural language generation.
Corn=puter Speech and Language, 12:349-373.M.
A. Walker, D. Litman, C. A. Kamm.
andA.
Abella.
1997.
PARADISE: A general frameworkfor evahlating spoken dialogue agents.
In Proceed-ings of the 35th Annual Meeting of the Associationof Computational Linguistics, A CL/EA CL 97. pages271-280.The XTAG-Group.
1999.
A lexicalized Tree Adjoin-ing Gralnmar for English.
Technical report, Insti-- tu;te for 1Research in Cognitive Science, University ofPennsylvania.
