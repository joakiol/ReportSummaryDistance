Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 187?195,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsJoint Training and Decoding Using Virtual Nodes for CascadedSegmentation and Tagging TasksXian Qian, Qi Zhang, Yaqian Zhou, Xuanjing Huang, Lide WuSchool of Computer Science, Fudan University825 Zhangheng Road, Shanghai, P.R.China{qianxian, qz, zhouyaqian, xjhuang, ldwu}@fudan.edu.cnAbstractMany sequence labeling tasks in NLP requiresolving a cascade of segmentation and tag-ging subtasks, such as Chinese POS tagging,named entity recognition, and so on.
Tradi-tional pipeline approaches usually suffer fromerror propagation.
Joint training/decoding inthe cross-product state space could cause toomany parameters and high inference complex-ity.
In this paper, we present a novel methodwhich integrates graph structures of two sub-tasks into one using virtual nodes, and per-forms joint training and decoding in the fac-torized state space.
Experimental evaluationson CoNLL 2000 shallow parsing data set andFourth SIGHAN Bakeoff CTB POS taggingdata set demonstrate the superiority of ourmethod over cross-product, pipeline and can-didate reranking approaches.1 IntroductionThere is a typical class of sequence labeling tasksin many natural language processing (NLP) applica-tions, which require solving a cascade of segmenta-tion and tagging subtasks.
For example, many Asianlanguages such as Japanese and Chinese whichdo not contain explicitly marked word boundaries,word segmentation is the preliminary step for solv-ing part-of-speech (POS) tagging problem.
Sen-tences are firstly segmented into words, then eachword is assigned with a part-of-speech tag.
Bothsyntactic parsing and dependency parsing usuallystart with a textual input that is tokenized, and POStagged.The most commonly approach solves cascadedsubtasks in a pipeline, which is very simple to im-plement and allows for a modular approach.
While,the key disadvantage of such method is that er-rors propagate between stages, significantly affect-ing the quality of the final results.
To cope with thisproblem, Shi and Wang (2007) proposed a rerank-ing framework in which N-best segment candidatesgenerated in the first stage are passed to the tag-ging model, and the final output is the one with thehighest overall segmentation and tagging probabil-ity score.
The main drawback of this method is thatthe interaction between tagging and segmentation isrestricted by the number of candidate segmentationoutputs.
Razvan C. Bunescu (2008) presented animproved pipeline model in which upstream subtaskoutputs are regarded as hidden variables, togetherwith their probabilities are used as probabilistic fea-tures in the downstream subtasks.
One shortcom-ing of this method is that calculation of marginalprobabilities of features may be inefficient and someapproximations are required for fast computation.Another disadvantage of these two methods is thatthey employ separate training and the segmentationmodel could not take advantages of tagging infor-mation in the training procedure.On the other hand, joint learning and decodingusing cross-product of segmentation states and tag-ging states does not suffer from error propagationproblem and achieves higher accuracy on both sub-tasks (Ng and Low, 2004).
However, two problemsarises due to the large state space, one is that theamount of parameters increases rapidly, which is aptto overfit on the training corpus, the other is thatthe inference by dynamic programming could be in-efficient.
Sutton (2004) proposed Dynamic Con-ditional Random Fields (DCRFs) to perform jointtraining/decoding of subtasks using much fewer pa-rameters than the cross-product approach.
How-187ever, DCRFs do not guarantee non-violation of hard-constraints that nodes within the same segment geta single consistent tagging label.
Another draw-back of DCRFs is that exact inference is generallytime consuming, some approximations are requiredto make it tractable.Recently, perceptron based learning frameworkhas been well studied for incorporating node leveland segment level features together (Kazama andTorisawa, 2007; Zhang and Clark, 2008).
The mainshortcoming is that exact inference is intractablefor those dynamically generated segment level fea-tures, so candidate based searching algorithm isused for approximation.
On the other hand, Jiang(2008) proposed a cascaded linear model which hasa two layer structure, the inside-layer model usesnode level features to generate candidates with theirweights as inputs of the outside layer model whichcaptures non-local features.
As pipeline models, er-ror propagation problem exists for such method.In this paper, we present a novel graph structurethat exploits joint training and decoding in the fac-torized state space.
Our method does not sufferfrom error propagation, and guards against viola-tions of those hard-constraints imposed by segmen-tation subtask.
The motivation is to integrate twoMarkov chains for segmentation and tagging sub-tasks into a single chain, which contains two types ofnodes, then standard dynamic programming basedexact inference is employed on the hybrid struc-ture.
Experiments are conducted on two differenttasks, CoNLL 2000 shallow parsing and SIGHAN2008 Chinese word segmentation and POS tagging.Evaluation results of shallow parsing task showthe superiority of our proposed method over tradi-tional joint training/decoding approach using cross-product state space, and achieves the best reportedresults when no additional resources at hand.
ForChinese word segmentation and POS tagging task, astrong baseline pipeline model is built, experimentalresults show that the proposed method yields a moresubstantial improvement over the baseline than can-didate reranking approach.The rest of this paper is organized as follows: InSection 2, we describe our novel graph structure.
InSection 3, we analyze complexity of our proposedmethod.
Experimental results are shown in Section4.
We conclude the work in Section 5.2 Multi-chain integration using VirtualNodes2.1 Conditional Random FieldsWe begin with a brief review of the Conditional Ran-dom Fields(CRFs).
Let x = x1x2 .
.
.
xl denote theobserved sequence, where xi is the ith node in thesequence, l is sequence length, y = y1y2 .
.
.
yl is alabel sequence over x that we wish to predict.
CRFs(Lafferty et al, 2001) are undirected graphic mod-els that use Markov network distribution to learn theconditional probability.
For sequence labeling task,linear chain CRFs are very popular, in which a firstorder Markov assumption is made on the labels:p(y|x) = 1Z(x)?i?
(x,y, i),where?
(x,y, i) = exp(wT f(x, yi?1, yi, i))Z(x) =?y?i?
(x,y, i)f(x, yi?1, yi, i) =[f1(x, yi?1, yi, i), .
.
.,fm(x, yi?1, yi, i)]T , each ele-ment fj(x, yi?1, yi, i) is a real valued feature func-tion, here we simplify the notation of state featureby writing fj(x, yi, i) = fj(x, yi?1, yi, i), m is thecardinality of feature set {fj}.
w = [w1, .
.
.
, wm]Tis a weight vector to be learned from the trainingset.
Z(x) is the normalization factor over all labelsequences for x.In the traditional joint training/decoding approachfor cascaded segmentation and tagging task, eachlabel yi has the form si-ti, which consists of seg-mentation label si and tagging label ti.
Let s =s1s2 .
.
.
sl be the segmentation label sequence overx.
There are several commonly used label sets suchas BI, BIO, IOE, BIES, etc.
To facilitate our dis-cussion, in later sections we will use BIES label set,where B,I,E represents Beginning, Inside and End ofa multi-node segment respectively, S denotes a sin-gle node segment.
Let t = t1t2 .
.
.
tl be the tagginglabel sequence over x.
For example, in named entityrecognition task, ti ?
{PER, LOC, ORG, MISC,O} represents an entity type (person name, loca-tion name, organization name, miscellaneous entity188x2s?t22x1s?t11S-P S-Ox3s?t33S-Ox4s?t44B-Px5s?t55E-PHendrix ?s girlfriend Kathy EtchinghamFigure 1: Graphical representation of linear chain CRFsfor traditional joint learning/decodingname and other).
Graphical representation of lin-ear chain CRFs is shown in Figure 1, where tagginglabel ?P?
is the simplification of ?PER?.
For nodesthat are labeled as other, we define si =S, ti =O.2.2 Hybrid structure for cascaded labelingtasksDifferent from traditional joint approach, ourmethod integrates two linear markov chains for seg-mentation and tagging subtasks into one that con-tains two types of nodes.
Specifically, we firstregard segmentation and tagging as two indepen-dent sequence labeling tasks, corresponding chainstructures are built, as shown in the top and mid-dle sub-figures of Figure 2.
Then a chain of twicelength of the observed sequence is built, wherenodes x1, .
.
.
, xl on the even positions are originalobserved nodes, while nodes v1, .
.
.
, vl on the oddpositions are virtual nodes that have no content in-formation.
For original nodes xi, the state space isthe tagging label set, while for virtual nodes, theirstates are segmentation labels.
The label sequenceof the hybrid chain is y = y1 .
.
.
y2l = s1t1 .
.
.
sltl,where combination of consecutive labels siti repre-sents the full label for node xi.Then we let si be connected with si?1 and si+1, so that first order Markov assumption is madeon segmentation states.
Similarly, ti is connectedwith ti?1 and ti+1.
Then neighboring tagging andsegmentation states are connected as shown in thebottom sub-figure of Figure 2.
Non-violation ofhard-constraints that nodes within the same seg-ment get a single consistent tagging label is guar-anteed by introducing second order transition fea-tures f(ti?1, si, ti, i) that are true if ti?1 6= ti andsi ?
{I,E}.
For example, fj(ti?1, si, ti, i) is de-fined as true if ti?1 =PER, si =I and ti =LOC.In other words, it is true, if a segment is partiallytagging as PER, and partially tagged as LOC.
Sincesuch features are always false in the training corpus,their corresponding weights will be very low so thatinconsistent label assignments impossibly appear indecoding procedure.
The hybrid graph structure canbe regarded as a special case of second order Markovchain.Hendrix ?s girlfriend Kathy Etchinghamx1 x2 x3 x4 x5s1 s2 s3 s4 s5S S S B Ex1 x2 x3 x4 x5t1 t2 t3 t4 t5P O O P Px1 x2 x3 x4 x5t1 t2 t3 t4 t5P O O P Ps2s1 s3 s4 s5S S S B Ev1 v2 v3 v4 v5IntegrateFigure 2: Multi-chain integration using Virtual Nodes2.3 Factorized featuresCompared with traditional joint model that exploitscross-product state space, our hybrid structure usesfactorized states, hence could handle more flexiblefeatures.
Any state feature g(x, yi, i) defined inthe cross-product state space can be replaced by afirst order transition feature in the factorized space:f(x, si, ti, i).
As for the transition features, weuse f(si?1, ti?1, si, i) and f(ti?1, si, ti, i) insteadof g(yi?1, yi, i) in the conventional joint model.Features in cross-product state space require thatsegmentation label and tagging label take on partic-ular values simultaneously, however, sometimes we189want to specify requirement on only segmentation ortagging label.
For example, ?Smith?
may be an endof a person name, ?Speaker: John Smith?
; or a sin-gle word person name ?Professor Smith will .
.
.
?.
Insuch case, our observation is that ?Smith?
is likely a(part of) person name, we do not care about its seg-mentation label.
So we could define state featuref(x, ti, i) = true, if xi is ?Smith?
with tagging la-bel ti=PER.Further more, we could define features likef(x, ti?1, ti, i), f(x, si?1, si, i), f(x, ti?1, si, i),etc.
The hybrid structure facilitates us to usevarieties of features.
In the remainder of thepaper, we use notations f(x, ti?1, si, ti, i) andf(x, si?1, ti?1, si, i) for simplicity.2.4 Hybrid CRFsA hybrid CRFs is a conditional distribution that fac-torizes according to the hybrid graphical model, andis defined as:p(s, t|x) = 1Z(x)?i?
(x, s, t, i)?i?
(x, s, t, i)Where?
(x, s, t, i) = exp(wT1 f(x, si?1, ti?1, si))?
(x, s, t, i) = exp(wT2 f(x, ti?1, si, ti))Z(x) =?s,t(?i?
(x, s, t, i)?i?
(x, s, t, i))Where w1, w2 are weight vectors.Luckily, unlike DCRFs, in which graph structurecan be very complex, and the cross-product statespace can be very large, in our cascaded labelingtask, the segmentation label set is often small, sofar as we known, the most complicated segmenta-tion label set has only 6 labels (Huang and Zhao,2007).
So exact dynamic programming based algo-rithms can be efficiently performed.In the training stage, we use second order forwardbackward algorithm to compute the marginal proba-bilities p(x, si?1, ti?1, si) and p(x, ti?1, si, ti), andthe normalization factor Z(x).
In decoding stage,we use second order Viterbi algorithm to find thebest label sequence.
The Viterbi decoding can beTable 1: Time ComplexityMethod Training DecodingPipeline (|S|2cs + |T |2ct)L (|S|2 + |T |2)UCross-Product (|S||T |)2cL (|S||T |)2UReranking (|S|2cs + |T |2ct)L (|S|2 + |T |2)NUHybrid (|S| + |T |)|S||T |cL (|S| + |T |)|S||T |Uused to label a new sequence, and marginal compu-tation is used for parameter estimation.3 Complexity AnalysisThe time complexity of the hybrid CRFs train-ing and decoding procedures is higher than that ofpipeline methods, but lower than traditional cross-product methods.
Let?
|S| = size of the segmentation label set.?
|T | = size of the tagging label set.?
L = total number of nodes in the training dataset.?
U = total number of nodes in the testing dataset.?
c = number of joint training iterations.?
cs = number of segmentation training itera-tions.?
ct = number of tagging training iterations.?
N = number of candidates in candidate rerank-ing approach.Time requirements for pipeline, cross-product, can-didate reranking and hybrid CRFs are summarizedin Table 1.
For Hybrid CRFs, original node xi hasfeatures {fj(ti?1, si, ti)}, accessing all label subse-quences ti?1siti takes |S||T |2 time, while virtualnode vi has features {fj(si?1, ti?1, si)}, accessingall label subsequences si?1ti?1si takes |S|2|T | time,so the final complexity is (|S|+ |T |)|S||T |cL.In real applications, |S| is small, |T | could bevery large, we assume that |T | >> |S|, so foreach iteration, hybrid CRFs is about |S| times slowerthan pipeline and |S| times faster than cross-product190Table 2: Feature templates for shallow parsing taskCross Product CRFs Hybrid CRFswi?2yi, wi?1yi, wiyi wi?1si, wisi, wi+1siwi+1yi, wi+2yi wi?2ti, wi?1ti, witi, wi+1ti, wi+2tiwi?1wiyi, wiwi+1yi wi?1wisi, wiwi+1siwi?1witi, wiwi+1tipi?2yi, pi?1yi, piyi pi?1si, pisi, pi+1sipi+1yi, pi+2yi pi?2ti, pi?1ti, pi+1ti, pi+2tipi?2pi?1yi, pi?1piyi, pipi+1yi,pi+1pi+2yipi?2pi?1si, pi?1pisi, pipi+1si, pi+1pi+2sipi?3pi?2ti, pi?2pi?1ti, pi?1piti, pipi+1ti,pi+1pi+2ti, pi+2pi+3ti, pi?1pi+1tipi?2pi?1piyi, pi?1pipi+1yi,pipi+1pi+2yipi?2pi?1pisi, pi?1pipi+1si, pipi+1pi+2siwipitiwisi?1siwi?1ti?1ti, witi?1ti, pi?1ti?1ti, piti?1tiyi?1yi si?1ti?1si, ti?1sitimethod.
When decoding, candidate reranking ap-proach requires more time if candidate number N >|S|.Though the space complexity could not be com-pared directly among some of these methods, hybridCRFs require less parameters than cross-productCRFs due to the factorized state space.
This is sim-ilar with factorized CRFs (FCRFs) (Sutton et al,2004).4 Experiments4.1 Shallow ParsingOur first experiment is the shallow parsing task.
Weuse corpus from CoNLL 2000 shared task, whichcontains 8936 sentences for training and 2012 sen-tences for testing.
There are 11 tagging labels: nounphrase(NP), verb phrase(VP) , .
.
.
and other (O), thesegmentation state space we used is BIES label set,since we find that it yields a little improvement overBIO set.We use the standard evaluation metrics, which areprecision P (percentage of output phrases that ex-actly match the reference phrases), recall R (percent-age of reference phrases returned by our system),and their harmonic mean, the F1 score F1 = 2PRP+R(which we call F score in what follows).We compare our approach with traditional cross-product method.
To find good feature templates,development data are required.
Since CoNLL2000does not provide development data set, we dividethe training data into 10 folds, of which 9 folds fortraining and 1 fold for developing.
After selectingfeature templates by cross validation, we extract fea-tures and learn their weights on the whole trainingdata set.
Feature templates are summarized in Table2, where wi denotes the ith word, pi denotes the ithPOS tag.Notice that in the second row, feature templatesof the hybrid CRFs does not contain wi?2si, wi+2si,since we find that these two templates degrade per-formance in cross validation.
However, wi?2ti,wi+2ti are useful, which implies that the proper con-text window size for segmentation is smaller thantagging.
Similarly, for hybrid CRFs, the windowsize of POS bigram features for segmentation is 5(from pi?2 to pi+2, see the eighth row in the sec-ond column); while for tagging, the size is 7 (frompi?3 to pi+3, see the ninth row in the second col-umn).
However for cross-product method, their win-dow sizes must be consistent.For traditional cross-product CRFs and our hybridCRFs, we use fixed gaussian prior ?
= 1.0 for bothmethods, we find that this parameter does not signifi-191Table 3: Results for shallow parsing task, Hybrid CRFssignificantly outperform Cross-Product CRFs (McNe-mar?s test; p < 0.01)Method Cross-ProductCRFsHybridCRFsTraining Time 11.6 hours 6.3 hoursFeature Num-ber13 million 10 mil-lionIterations 118 141F1 93.88 94.31cantly affect the results when it varies between 1 and10.
LBFGS(Nocedal and Wright, 1999) method isemployed for numerical optimization.
Experimen-tal results are shown in Table 3.
Our proposed CRFsachieve a performance gain of 0.43 points in F-scoreover cross-product CRFs that use state space whilerequire less training time.For comparison, we also listed the results of pre-vious top systems, as shown in Table 4.
Our pro-posed method outperforms other systems when noadditional resources at hand.
Though recently semi-supervised learning that incorporates large mountsof unlabeled data has been shown great improve-ment over traditional supervised methods, such asthe last row in Table 4, supervised learning is funda-mental.
We believe that combination of our methodand semi-supervised learning will achieve furtherimprovement.4.2 Chinese word segmentation and POStaggingOur second experiment is the Chinese word seg-mentation and POS tagging task.
To facilitate com-parison, we focus only on the closed test, whichmeans that the system is trained only with a des-ignated training corpus, any extra knowledge is notallowed, including Chinese and Arabic numbers, let-ters and so on.
We use the Chinese Treebank (CTB)POS corpus from the Fourth International SIGHANBakeoff data sets (Jin and Chen, 2008).
The train-ing data consist of 23444 sentences, 642246 Chinesewords, 1.05M Chinese characters and testing dataconsist of 2079 sentences, 59955 Chinese words,0.1M Chinese characters.We compare our hybrid CRFs with pipeline andcandidate reranking methods (Shi and Wang, 2007)Table 4: Comparison with other systems on shallow pars-ing taskMethod F1 Additional Re-sourcesCross-Product CRFs 93.88Hybrid CRFs 94.31SVM combination 93.91(Kudo and Mat-sumoto, 2001)Voted Perceptrons 93.74 none(Carreras and Mar-quez, 2003)ETL (Milidiu et al,2008)92.79(Wu et al, 2006) 94.21 Extended featuressuch as token fea-tures, affixesHySOL 94.36 17M words unla-beled(Suzuki et al, 2007) dataASO-semi 94.39 15M words unla-beled(Ando and Zhang,2005)data(Zhang et al, 2002) 94.17 full parser output(Suzuki and Isozaki,2008)95.15 1G words unla-beled datausing the same evaluation metrics as shallow pars-ing.
We do not compare with cross-product CRFsdue to large amounts of parameters.For pipeline method, we built our word segmenterbased on the work of Huang and Zhao (2007),which uses 6 label representation, 7 feature tem-plates (listed in Table 5, where ci denotes the ithChinese character in the sentence) and CRFs for pa-rameter learning.
We compare our segmentor withother top systems using SIGHAN CTB corpus andevaluation metrics.
Comparison results are shownin Table 6, our segmenter achieved 95.12 F-score,which is ranked 4th of 26 official runs.
Except forthe first system which uses extra unlabeled data, dif-ferences between rest systems are not significant.Our POS tagging system is based on linear chainCRFs.
Since SIGHAN dose not provide develop-ment data, we use the 10 fold cross validation de-scribed in the previous experiment to turning featuretemplates and Gaussian prior.
Feature templates arelisted in Table 5, where wi denotes the ith word in192Table 5: Feature templates for Chinese word segmentation and POS tagging taskSegmentation feature templates(1.1) ci?2si, ci?1si, cisi, ci+1si, ci+2si(1.2) ci?1cisi, cici+1si, ci?1ci+1si(1.3) si?1siPOS tagging feature templates(2.1) wi?2ti, wi?1ti, witi, wi+1ti, wi+2ti(2.2) wi?2wi?1ti, wi?1witi, wiwi+1ti, wi+1wi+2ti, wi?1wi+1ti(2.3) c1(wi)ti, c2(wi)ti, c3(wi)ti, c?2(wi)ti, c?1(wi)ti(2.4) c1(wi)c2(wi)ti, c?2(wi)c?1(wi)ti(2.5) l(wi)ti(2.6) ti?1tiJoint segmentation and POS tagging feature templates(3.1) ci?2si, ci?1si, cisi, ci+1si, ci+2si(3.2) ci?1cisi, cici+1si, ci?1ci+1si(3.3) ci?3ti, ci?2ti, ci?1ti, citi, ci+1ti, ci+2ti, ci+3ti(3.4) ci?3ci?2ti, ci?2ci?1ti, ci?1citi, cici+1ti ci+1ci+2ti, ci+2ci+3ti, ci?2citi, cici+2ti(3.5) cisiti(3.6) citi?1ti(3.7) si?1ti?1si, ti?1sitiTable 6: Word segmentation results on Fourth SIGHANBakeoff CTB corpusRank F1 Description1/26 95.89?
official best, using extra un-labeled data (Zhao and Kit,2008)2/26 95.33 official second3/26 95.17 official third4/26 95.12 segmentor in pipeline sys-temTable 7: POS results on Fourth SIGHAN Bakeoff CTBcorpusRank Accuracy Description1/7 94.29 POS tagger in pipeline sys-tem2/7 94.28 official best3/7 94.01 official second4/7 93.24 official thirdthe sentence, cj(wi), j > 0 denotes the jth Chinesecharacter of word wi, cj(wi), j < 0 denotes the jthlast Chinese character, l(wi) denotes the word lengthof wi.
We compare our POS tagger with other topsystems on Bakeoff CTB POS corpus where sen-tences are perfectly segmented into words, our POStagger achieved 94.29 accuracy, which is the best of7 official runs.
Comparison results are shown in Ta-ble 7.For reranking method, we varied candidate num-bers n among n ?
{10, 20, 50, 100}.
For hybridCRFs, we use the same segmentation label set asthe segmentor in pipeline.
Feature templates arelisted in Table 5.
Experimental results are shownin Figure 3.
The gain of hybrid CRFs over thebaseline pipeline model is 0.48 points in F-score,about 3 times higher than 100-best reranking ap-proach which achieves 0.13 points improvement.Though larger candidate number can achieve higherperformance, such improvement becomes trivial forn > 20.Table 8 shows the comparison between our workand other relevant work.
Notice that, such com-parison is indirect due to different data sets and re-1930 20 40 60 80 10090.390.490.590.690.790.890.9candidate numberF scorecandidate rerankingHybrid CRFsFigure 3: Results for Chinese word segmentation andPOS tagging task, Hybrid CRFs significantly outperform100-Best Reranking (McNemar?s test; p < 0.01)Table 8: Comparison of word segmentation and POS tag-ging, such comparison is indirect due to different datasets and resources.Model F1Pipeline (ours) 90.40100-Best Reranking (ours) 90.53Hybrid CRFs (ours) 90.88Pipeline (Shi and Wang, 2007) 91.6720-Best Reranking (Shi and Wang,2007)91.86Pipeline (Zhang and Clark, 2008) 90.33Joint Perceptron (Zhang and Clark,2008)91.34Perceptron Only (Jiang et al, 2008) 92.5Cascaded Linear (Jiang et al, 2008) 93.4sources.
One common conclusion is that joint mod-els generally outperform pipeline models.5 ConclusionWe introduced a framework to integrate graph struc-tures for segmentation and tagging subtasks into oneusing virtual nodes, and performs joint training anddecoding in the factorized state space.
Our approachdoes not suffer from error propagation, and guardsagainst violations of those hard-constraints imposedby segmentation subtask.
Experiments on shal-low parsing and Chinese word segmentation tasksdemonstrate our technique.6 AcknowledgementsThe author wishes to thank the anonymous review-ers for their helpful comments.
This work waspartially funded by 973 Program (2010CB327906),The National High Technology Research and De-velopment Program of China (2009AA01A346),Shanghai Leading Academic Discipline Project(B114), Doctoral Fund of Ministry of Education ofChina (200802460066), National Natural ScienceFunds for Distinguished Young Scholar of China(61003092), and Shanghai Science and TechnologyDevelopment Funds (08511500302).ReferencesR.
Ando and T. Zhang.
2005.
A high-performance semi-supervised learning method for text chunking.
In Pro-ceedings of ACL, pages 1?9.Razvan C. Bunescu.
2008.
Learning with probabilisticfeatures for improved pipeline models.
In Proceedingsof EMNLP, Waikiki, Honolulu, Hawaii.X Carreras and L Marquez.
2003.
Phrase recognition byfiltering and ranking with perceptrons.
In Proceedingsof RANLP.Changning Huang and Hai Zhao.
2007.
Chinese wordsegmentation: A decade review.
Journal of ChineseInformation Processing, 21:8?19.Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Lu.2008.
A cascaded linear model for joint chinese wordsegmentation and part-of-speech tagging.
In Proceed-ings of ACL, Columbus, Ohio, USA.Guangjin Jin and Xiao Chen.
2008.
The fourth interna-tional chinese language processing bakeoff: Chineseword segmentation, named entity recognition and chi-nese pos tagging.
In Proceedings of Sixth SIGHANWorkshop on Chinese Language Processing, India.Junichi Kazama and Kentaro Torisawa.
2007.
A newperceptron algorithm for sequence labeling with non-local features.
In Proceedings of EMNLP, pages 315?324, Prague, June.Taku Kudo and Yuji Matsumoto.
2001.
Chunking withsupport vector machines.
In Proceedings of NAACL.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proceedingsof ICML.Ruy L. Milidiu, Cicero Nogueira dos Santos, and Julio C.Duarte.
2008.
Phrase chunking using entropy guidedtransformation learning.
In Proceedings of ACL, pages647?655.194Hwee Tou Ng and Jin Kiat Low.
2004.
Chinese part-ofspeech tagging: One-at-a-time or all-at-once?
word-based or character-based?
In Proceedings of EMNLP.J.
Nocedal and S. J. Wright.
1999.
Numerical Optimiza-tion.
Springer.Yanxin Shi and Mengqiu Wang.
2007.
A dual-layer crfsbased joint decoding method for cascaded segmenta-tion and labeling tasks.
In Proceedings of IJCAI, pages1707?1712, Hyderabad, India.C.
Sutton, K. Rohanimanesh, and A. McCallum.
2004.Dynamic conditional random fields: Factorized prob-abilistic models for labeling and segmenting sequencedata.
In Proceedings of ICML.Jun Suzuki and Hideki Isozaki.
2008.
Semi-supervisedsequential labeling and segmentation using giga-wordscale unlabeled data.
In Proceedings of ACL, pages665?673.Jun Suzuki, Akinori Fujino, and Hideki Isozaki.
2007.Semi-supervised structured output learning based ona hybrid generative and discriminative approach.
InProceedings of EMNLP, Prague.Yu-Chieh Wu, Chia-Hui Chang, and Yue-Shi Lee.
2006.A general and multi-lingual phrase chunking modelbased on masking method.
In Proceedings of Intel-ligent Text Processing and Computational Linguistics,pages 144?155.Yue Zhang and Stephen Clark.
2008.
Joint word seg-mentation and pos tagging using a single perceptron.In Proceedings of ACL, Columbus, Ohio, USA.T.
Zhang, F. Damerau, and D. Johnson.
2002.
Textchunking based on a generalization of winnow.
ma-chine learning research.
Machine Learning Research,2:615?637.Hai Zhao and Chunyu Kit.
2008.
Unsupervised segmen-tation helps supervised learning of character taggingforword segmentation and named entity recognition.In Proceedings of Sixth SIGHAN Workshop on ChineseLanguage Processing, pages 106?111.195
