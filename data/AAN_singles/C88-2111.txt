Using a Logic Grammar to Learna LexiconManny Rayner, ~sa Hugosson, G6ran HagertSwedish Institute of Computer ScienceBox 1263S-I64 28 KISTASwedenTel: +46-8-7521500SummaryIt is suggested that the concept of "logic grammar" asrelation between a string and a parse-tree can beextended by admitt ing the lexicon as part of therelation.
This makes it possible to give a simple andelegant formulation of the process of infering a lexiconfrom example  sentences in conjunct ion with agrammar .
Var ious  prob lems ar i s ing  f romimplementation and complexity factors are considered,and examples are shown to support the claim that themethod shows potential as a practical tool for automaticlexicon acquisition.Keywords: Logic programming, Prolog, logic grammar,learning, lexicon.Topic Area: Theoretical issues1.
IntroductionThe basic idea is as follows: a logic grammar \[1\] can beviewed as the definition of a relation between a stringand a parse-tree.
You can run it two ways: finding' theparse-trees that correspond to a given string (parsing),or f inding the strings that correspond to a givenparse-tree (generating).
However, if we view the lexiconas part of this relation, we get new possibilities.
Morespecif ical ly,  we can compute  the lexicons thatcorrespond to a given string; this can in a natural waybe viewed as a formalization of "lexicon learning fromexample sentences".
In terms of the "explanation-basedlearning" parad igm,  this makes the associatedparse-tree the "explanation" (See diagram 1).lexiconlearning j~  ~,~ y \] ~explanations t r 'g  Lgrammar ~- - - -  parse-treeparsing w4 generatingDiagram I524In what comes below, we are going to consider thefollowing questions:1) We are learning from positive-only examples~ Whatcan't be learned like this?2) The basic structural constraint, the thing that makesit all work, is the assumption that a word can usuallyonly be interpreted as one part of speech.
If we assumethat this is always going to be true, then things really gopretty well  (Section 2).
However,  this rule is brokensufficiently often that a realistic system has to able todeal with it.
How?3) How important is the order in which examples arepresented?
Can the system select a good order itself, if itis important?4) What kind of complexity features are there?
Howscalable is it in terms of number of sentences, numberof grammar ules, number of words to learn?2.
Learning with the "one entry per word" assumption.This is the simplest variant of the idea: assume thatthere is one entry per word, and represent the lexicon asan association-list (alist) with one entry for each word.Each sentence now constrains the possible values ofthese entries to be ones.
which allow it to be parsed; thehope is that a conjunction of a suitably large number ofsuch constraints wi l l  be enough to determine thelexicon uniquely.In concrete Prolog programming terms, what thismeans is the following.
In theinitial lexicon, the entries are all uninstantiated.
Weuse this to parse the first sentence, which fills in someentries; the resulting partially instantiated lexicon issent to the second sentence, which either refutes it orinstantiates it some more, and the process is repreateduntil we get to the end.
If at any stage we are unable toparse a sentence, we just backtrack.
If we want to, wecan continue even after we've got to the end, togenerate all possible lexicons that are consistent withthe input sentences and the grammar (and in fact weought to do this, so as to know which words are sti l lambiguous).
This procedure can be embodied as aone-page Prolog program (see diagram 2), but despitethis it is still surprisingly fast on small examples (agrammar with 15-30 rules, 10-15 sentences with a totalof 30-40 words to learn).
We per formed someexperiments with this kind of setup, and drew these iconclusions:1) Certain things can't be learned from positive-onlyexamples.
For example (at least with the grammars wehave tried), it is impossible to determine whetherbelongs is a verb which takes a PP complement withpreposit ion to, or is an intransitive verb which justhappens to have a PP modif ier in all the sentenceswhere it turns up.
However,  things of this kind seemfairly rare.2) Order is fairly critical.
When examples are presentedat random, a run time of about 100 seconds for a 10-12sentence group is typical; ordering them so that not toomany new words are introduced at once drops this toabout 5 seconds, a factor of 20.
This gets worse withmore sentences, since a lot of work can be done beforethe system realizes it's got  a wrong hypothesis andbacktracks\].earn (Sents~ L) :-s ta r t  lex (SentsvL) ,learn  :\[(Sents~L) .learn  1 ( J I LL ) .learn  I ( \ [F IR \ ]~L)  : -parse(F ,L )  rlearn  \] (RgL) ,parse(gent~\]-.)
:- s (Sent~ \[\],L)s ta r t  lex (Sents~L)  :-seto{?
(\[W, \ ]vS^(member(SvSents ) ,member(W,S) ) ,L ) .lex  lookup(WordvLex ,C lass )  :~"member( \ [Word ,  C lass \ ] , Lex) .% Example  grammar :s(L) ---~> np(L ) ,vp(L )  .np(L)  .... > det  (L) , noun (L) .vp(L) .... > iv(L) .vp(L) -.-> tv (L ) ,np(L )  .det(L)  .~-> \[D\], { lex lookup(D,L rdet )  }.noun(L)  - -> \[N\], { lex lookup(N,  Lrnoun)  } .iv (L) ~.-> \[V\] r { lex .lookup (V, L, iv) } .tv(L) ~-> \[V\], { lex lookup (V, L, tv) } .Diagram 23) A mo~:e important complexity point: structuralambiguities needn't be lexical ambiguities; in otherwo~'ds, it is quite possible to parse a sentence in twodistinct ways which still both demand the same lexicalentries (in practice, the most common case by far isNP/VP ~l'.
:tachment ambiguity).
Every such ambiguityintroduce:; a spurious duplication of the lexicon, andsince these.,, multiply we get an exponential dependencyon the number of sentences.
We could conceivablyhave tried to construct a grammar which doesn'tproduce this kind of ambiguity (cf.
\[2\], pp.
64-71), butinstead we reorganized the algorithm so as to collectaftex' each step the set of all possible lexicons compatiblewith the input so far.
Duplicates are then eliminatedfrom this, and the result is passed to the next step.Although the resulting program is actually considerablyxnore expensive for small examples, it wins in the longrun.
Moreover, it seems the right method to build onwhen we relax the "one entry per word" assumption.3?
~.emov;\[ng the "one curry per word" assumption.We doxft actually remove the assumption totally, butjust weaken it; for each new.
sentence, we now assumethat, of tlle words already possessed of one or moreentries, a'~ most one may have an unknown alternate..Multiple entries are sufficiently rare to make thisreasonable.
9o we extend the methods from the end ofsection 2; first we try and parse the current sentence byh~k ing  up known entries and filling in entries fox"words we so far know nothing about.
If we don't geta~y result this way, we try again, this time with theadded possibility of once assuming that a word whichalready has known entries in fact has one more.Tids is t~sually OK, but sometimes produces strangei'esults, as witness the following example.
Suppose thefirst three sentences are John drives a car, John driveswell, and John drives.
Aftex' the first sentence, thesystem gaesses that drives is a transitive verb, and it isable to maintain this belief after the second sentence ifit also assumes that well is a pronoun.
However, thethird sentence forces it to realize that drives can also bean intransitive verb.
Later on, it will presumably meeta sentence which forces well to be an adverb; we nowhave an anomalous lexicon where well has an extraentry (as pronoun), which is not actually used toexplain anything any longer.
To correct situations likethis one, a two-pass method is necessary; we parsethrough all the sentences a second time with the finallexicon, keeping count of which entries are actuallyused.
If we find some way of going through the wholelot without using some entry, it can be discarded.4.
Ordering the sentencesAs remarked above, order is a critical factor; if words areintroduced too quickly, so that the system has nod~ance to disambiguate them before moving on to newones, then the number of alternate lexicons growsexponentially.
Some way of ordering the sentencesautomatically is essential.
()ur initial effort in this direction is very simple, butstill seems reasonably efficient; sentences arepre-ordered so as to minimize the number of newwords introduced at each stage.
So the first sentence isthe one that contains the smallest number of distinctwords, the second is the one which the smallestnumber of words not present in the first one, and so on.We have experimented with this approach, usinggroups of between 20 and 40 sentences and a grammarcontaining about 40 rules.
If the sentences are randomlyordered, the number of alternate lexicons typicallygrows to over 400 within the first 6 to 10 sentences; thisslows things down to thepoint where further progress is in practice impossible.Using the above strategy, we get a fairly dramaticimprovement; the number of alternates remains small,reaching peak values of about 30.
This is sufficient o beable to process the groups within sensible times (lessthan 15 seconds per sentence average).
In the next twosections, we discuss the limitations of this method andsuggest some more sophisticated alternatives.5.
Increasing efficiencyIt is rather too early to say how feasible the methodsdescribed here can be in the long term.
As far as we cansee, scalability is good as far as grammar.~size isconcerned; we have increased the number of rules from15 in the first version to about 40 in the current onewith little performance degradation.
Scalability withrespect to number of sentences is more difficult toestimate.
Using the methods described in sections 3 and4, we have sucessfully processed groups of up to 50sentences (about equally many words), with run timestypically in the region of 10-15 minutes.
An example isshown in the appendix.
It is reasonable to suppose thatthe system as it stands would be capable of dealing withgroups up to four or five times this size (i.e.
200-250words to learn), but it has a limit; the problem is thatthere are always going to be a few words in any givencorpus which occur insufficiently often for their lexicalclass to be determinable.
Although these words aretypically fairly rare, the ambiguities they introducemultiply in the usual way, leading to an eventual525breakdown of the system.
The following tentative ideasrepresent some approaches to this problem which weare currently investigating.What appears to be necessary is to find some intelligentway of utilising the fact that the various alternatelexicons all agree on the majority of entries; typically,less than 10% are ambiguous after any given step in theprocessing.
The current system completely ignores this,representing each lexicon as a separate ntity.
If we areto improve this state of affairs, we can envisage twopossible plans.
Firstly, we could simply remove the"difficult" words, hoping that there are sufficiently fewfor this not to matter.
More ambitiously, we can try toshare structure between lexicons, so that the commonpart is not duplicated.
We now expand on these twoideas in more detail.5.1.
Removing "difficult" entriesAt regular intervals the group of alternate lexicons isanalyzed: the normal state of affairs is that they areidentical excepting the entries for a few words, thepotential "troublemakers".
What one could do wouldbe simply to remove these entries, making them onceagain uninstantiated; then all sentences containing theoffending words would be removed from the subgroupmarked as already having been processed, and saved forpossible future use.
The overall effect would be toreduce the group of alternate lexicons to a single"lowest common denominator",  which ~ wouldrepresent the "reliable" information so far acquired,this at the expense of losing some partial informationon the "dubious" words.We have carried out a few simple experiements alongthese lines, using a variant of the ,dea which at each"check-point" removes all ambigous words for whichthere are no further sentences awaiting processing.
Thisseems at first sight very reasonable, but unfortunately itturns out that there are problems.
Although one mighteasiIy think that an ambiguous word is going to stayambiguous if it doesn't occur in any of the remainingsentences, in actual fact this is not so; a word can bedisambiguated "indirectly", as a result of other wordsbeing disambiguated.
To give a simple example:suppose that the first sentence is The zebra laughed.This can give rise to a number of possibilities: forexample, the and laughed could be pronouns, and zebraa transitive verb.
If the word zebra didn't occur again,one would thus wrongly conclude that there was noway of determining whether it was a common oun ora transitive verb.
But this can easily be accomplished ifthe or laughed are later assigned to their proper classes,which will then remove the incorrect interpretationand indirectly make zebra unambiguous too.
Clearly, amore sophisticated implementation is required if thisidea is going to work.5.2.
"Lexicon compaction" using Prolog constraintsHere, we discuss the idea of exploiting the similaritybetween different alternate lexicons to "merge" or"compact" them.
The technical tool we will be using toperform this operation is the Prolog "constraint"mechanism \[3\], \[4\].
What we propose is illustrated indiagram 3, which shows two alternate lexicons,differing in a single entry.
These can be combined intothe third lexicon without any loss of information.526Simple compaction of two lexiconsTwo alternate lexicons for the sentence: the dogbe longs  to  the  man\[ \ [ the :d \ ] ,  \ [dog :n \ ] ,  \ [be  longs :v ( in t rans)  \],\ [ to :prep \ ] ,  \ [man:n \ ]  \]\[ \ [ the :d \ ] ,  \ [dog :n \ ] ,  \ [be longs :v (prep( to ) )  \],\ [ to :prep \ ] ,  \ [man:n \ ]  \]These can be compacted into the following singlelexicon\ [ \ [ the :d \ ] ,  \ [dog :n \ ] ,\ [be longs :<X:X=v(prep( to )  ;X=v( in t rans)>\ ] ,\ [ to :prep \ ] ,  \ [man:n \ ] lDiagram 3The technique is potentially very powerful, and infavourable circumstances can be used to compacttogether large numbers of alternates, as diagram 4illustrates.Compacting four lexicons into one in a two-stageprocess.lexl: \[ .
.
.
\ [be longs :v ( in t rans) \ ] ,  .
.
.\ [p lays :v ( in t rans)  \], .
.
.\]lex2: \[ .
.. \ [be longs  : v (p rep  (to) )\]r  ?
?
.\ [p lays :v ( in t rans)  \], .
.
.\]lex3: \[ ?
.. \ [be longs :v ( in t rans)  \]~ ...\ [p lays :v (prep(w i th ) ) \ ] ,  .
.
.
\ ]lex4: \[ .
.
.
\ [be longs :v (prep( to ) ) \ ] ,  ...\ [p lays :v (prep(w i th ) )  \], .
.
.
\ ]In the first stage, we compact lexl and lex2 to makelex12, and lex3 and lex4 to make lex34.lex12: \[ .
.
.
\ [be longs  :<X:X=v(prep( to ) ;X=v( in t rans)> \], ...\ [p lays :v ( in t rans) \ ] ,  .
.
.
\ ]lex34: \[ .
.
.
\ [be longs  :<X : X=v (prep  (to) ;X=v( in t rans)> \], .
.
.\ [p lays :v (prep(w i th ) )  \], .
.
.\]Then we compact lex12 and lex34 to get the final result.\[ .
.
.
\ [be longs  :<X:X=v(prep( to )  ;X=v( in t rans)> \], ...\ [p lays :  <Y:Y=v(prep(wi th )  ;Y=v( in t rans)> \],?
..\]Diagram 4What makes the "compaction" method so attractive isthat it appears to get the best of both worlds: noinformation is lost, but substantial efficency gains can beattained.
The method raws its power from the fact that itis "intelligent" about divergences between lexicons: if thesentence to be parsed contains none of the "constrained"words, then the compacted lexicon will behave as thoughit were a single, unambiguous,  lexicon; but if"constrained" words are present, then the lexicon will be"split" again, to exactly the extent required by the variousparsings of the sentence.
It is to be noted that all this ofcourse requires a Prolog constraint mechanism which isboth efficient and logically complete, something that hasonly recently become possible \[4\].
We are currently in theprocess of in~plementing the method within our system.6o Conclusioas and further directionsWe have described a series of experiments whichinvestigate the feasibility of automatically infering alexicon frora a logic grammar and a set of examplesentences; this stands in fairly sharp contrast o mostwork done so far within the field of automatic languageacquisition, where the emphasis has been either ongrammar induction e.g.
\[51, \[6\], \[7\], or learning of wordsenses \[8\]: Ia view of the fact that much recent linguisticresearch has been moving towards unification-basedformalisms where the bulk of the information is storedin the lexicon, we think that ideas like the ones wepropound here should have a rich field of application.For example, Pollard and Sag's HPSG framework \[9\] hasat only a couple of dozen grammatical rules, all of whichare extremely general; the rest of the information islexical in nature.Although we think that progress to date has beenextremely encouraging, it is still a little too early to makeany firm claim that our methods are going to be usable ina practical system.
As discussed above, there are somenon-trivial efficiency problems to be overco-ae: it alsoseems likely that we will need a more sophisticatedordering algorithm than that described in section 4,probably incorporating some notion of giving higherpriority to sentences containing ambiguous words.
Otherimportant opics which we so far have not had time todevote attention to are the use of morphologicalinformation and the development of some way ofhandling incorrect sentences (maybe just ignoring themis enough; but our feeling is that things will be a littletrickier).
These and other related questions will, we hope,provide fruitful ground for continued research in thisarea,References\[1\] F.C.N.
Pereira, Logic for Natural Language AnalysisSRI Technical Note 275, 1983\[2\] F.C.N.
Pereira & D.H.D.
Warren, Definite ClauseGramn,ars Compared with Augmented TransitionNetworks, Research Report, Dept.
of AI, EdinburghUniversity 1978 (also in Artificial Intelligence, 1980)113\] A. Colmerauer,  Prolog-II, Manuel de reference tmodel theorique, Groupe d'Intelligence Artificielle,Universite Aix-Marseille, 1982\[4\] M. Carlsson, An Implementation of "dif" and"freeze" in the WAM, SICS Research Report, 1986\[5\] S.F.
Pilato & R. Berwick Reversible Automata andInduction of the English Auxiliary System, Proc.23rd ACL, Chicago, 1985\[6\] R.M.
Wharton,  Grammar Enumeration andInference, Information and Control, Vol 33, 253~272,1977\[~/J }.I;\[.
Aadersson, A Theory of Language LearningBased on General Learning Principles, Proc.
7thIJCAI, Vancouver, 1981\[8\] R.C.
Berwick, Learning Word Meanings FromExamples IJCAI 1983\[9\] C. Pollard & I.
Sag Information-Based Syntax andSemantics, Vol.
1,CSLI 1987We enclose two appendices.
The first shows somesample runs; the second, the grammar used in theexamples.Append ix  1S ICStus V0.5 - Ju ly 31, 1987Copyr ight  (C) 1987,Swedish Inst i tute  of Computer  Science.Al l  r ights  reserved.I ?~- \[ 'start.pl ' \ ] .\ [consult ing /khons /asa / learn ing/s tar t .p l .
.
\ ]\ [compi l ing /khons /asa / learn ing/xgproc .p l .
.
.
\ ]\[xgproc compi led  in 14480 msec.\]\ [consult ing /khons /asa / learn ing /xgrun .p l .
.
.
\ ]\[xgrun reconsu l ted  in 159 msec.\]\ [consult ing /khons /asa / learn ing /ut i l i t i es .p l .
\ ]\[utilities.pl reconsulted in 1360 msec.\]\[compiling /khons/asa/learning/prettyprint.pl.\]\[prettyprint.pl compiled in 4680 msec.\]\[consulting /khons/asa/learning/top.pl...\]\[top.pl reconsulted in 5920 msec.\]\[consulting /khons/asa/learning/sent.pl.. .\]\[sent.pl reconsulted in 2340 msec.\]** Grammar from file grammar.pl : 0 words **\[consulting /khons/asa/learning/read-fi le.pl.
l\[read-file.pl reconsulted in 1420 msec.\]\[start.pl eonsulted in 32100 msec.\]% .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.% A s imple test  with six sentences.. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.yes1 ?- test qroup(5).Order  before  sort ing: \[1,26,2,3,4,5\]Order  after  sort ing: \[1,2,26,3,4,5\]% .........................................................% The format of each l ine is:% Sentence number (in test  sentence),% sentence ,number  of lex icons left.. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.i.
the cat saw the dog 82. the dog saw a cat 226. that man saw the dog 33. a man saw the nice dog 24. the nice dog likes the man 25. the man likes the dog that the cat saw 1Run time = 13420.
Compil ing statistics .... .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.% The system asks the user  which of the% a l ternate  lexicons is the correct one.% Here there  is only one poss ib i l i ty  left.% .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.a: detcat: noun(_48268)dog: noun(48270)likes: verb(trans)man: noun(_48273)nice: adjsaw: verb(trans)that: det rel prothe: detIs this correct?
yes.No mistakesyes% .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.% A rather  more compl icated  example.% .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.527I ?
-  test_grouP(0)"Order  before  sort ing:\ [ i ,2 ,3 ,4 ,5 ,6 ,7 ,8 ,9 ,10 ,11 ,12 ,13 ,14 ,15 ,16e17 ,18 ,19 ,20 ,21 ,22 ,23 ,24 ,25 ,26 ,27 ,28 ,29 ,30 ,31 ,32 ,33 ,34 ,35 ,36 ,37 ,38 ,39 ,40 ,41 ,42 ,43 \ ]Order  after  sort ing:\ [1 ,2 ,3 ,4 ,5 ,26 ,27 ,13 ,14 ,6 ,15 ,39 ,11 ,9 ,19 ,18 ,21 ,20 ,10 ,12 ,17 ,7 ,23 ,33 ,16 ,8 ,22 ,28 ,29 ,30 ,31 ,32 ,25 ,24 ,35 ,38 ,34 ,36 ,37 ,40 ,41 ,42 ,43 \ ]I. the cat saw the dog 82. the dog saw a cat 23. a man saw the nice dog 24. the n ice dog l ikes the man 25. the man l ikes the dog that the cat saw 126. that man saw the dog 127. the man has a cat 113. the dog be longs  to the man 414. the dog l ikes most  men 86. most  men l ike the dog 415. the men l ike john 439. the man hoped that john l ikes the dog 24ii.
the dog hoped that the manread the newspaper  169. the man read the newspaper  1619. john has read the newspaper  today 1618. john read  the newspaper  today 1621. the man read the newspaper  beforejohn saw the cat 1620. john has not read the newspaper  1610. the dog brought  the man the newspaper  1612. john threw the newspaper  to the dog 1217. john threw the newspaper  on the table 127. the dog sat on the table 2023. the cat  sat on the car 2033. john saw a g lass  on the table 1616. the dog sat with john 88. the table  be longs  to the man whoowns the dog B22.
the man who owns the cat dr ives the car 828. the man who has a cat has no dog 829. the cat ate a f ish 8 ~30.
john ate the beans 831. the man ate a can of beans 1632. the man brought  the cat a canof cat food 1625. the man can dr ive the car 7224. the dog can not dr ive the car 1635. the man drank the whisky  1638. john hoped the dog drank the water  434. john drank  a g lass  of water  236. john poured  the water  on the cat 237. john poured  a can of water  on the cat 240. mary  knows that john owns a dog 241; john be l ieves  that  mary dr ives a car 242. mary  be l ieves  John knows that peterhas a eat 443. peter  can not be l ieve  that  maryate the f ish 2Run t ime = 949120.
Compi l ing  s tat is t ics~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.% This is the f i rst  lex icon of two.
The% d ivergences  are summar ized  by the system% fur ther  down.
Note that "can" and "has"% are cor recte ly  ass igned to two d i f ferent% classes, and  "that" to three.% .. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.% Nouns can be c lass i f ied  as e i ther% "count"  or "measure".
Most  of them% cou ld  be either,  but nouns occur ing% in par ta t ive  const ruct ions  ("can of% catfood",  "glass of whisky") are% forced  to be "measure".% .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.e: detatef  verb(trans)beans: noun(measure)before:  sub_conJbe l ieve:  verb(s_comp)bel ieves:  verb(s  comp)belongs:  verb( int rans)528brought:  verb(doub ly  txans)can: noun(342148)  verb(aux)car: noun(342150)cat: noun(342152)catfood: noun(measure)dog: noun(342155)drank: verb(trans)drive: verb(trans)drives: Verb(trans)fish: noun(342160)glass: noun(342162)has: verb(trans)  verb(aux)hoped: verb(s  comp)john: nameknows: verb(s_comp)like: verb(trans)l ikes: verb(trans)man: noun(342170)mary: namemen: noun(342173)most: detnewspaper:  noun(_342176)nice: adjno: detnot: negatorof: par ta t ive  markeron: prepowns: verb(trans)peter:  namepoured: verb(trans)read: verb(trans)sat: verb( intrans)saw: verb(trans)table: noun(342189)that: rel p ro  det compthe: detthrew: verb(trans)to: p reptoday: advwater: noun(measure)whisky: noun(342197)who: tel  prowith: p repI s  this  cor rect?
yes.be longs  : 1 mis takes  \ [verb(pobj( \ [ to~45\]) ) \ ]yesJ ?- halt.user t ime 983.600000Appendix  2% Here is the grammar to the learn ing system~s v-> nprvp.np --> det ,np l ( _ ) ..np --> name.npl(Type) --> adJs,n(Type),optional_lop,tel.adjs -->.
\[\].adjs --> adJ ,adJs.vp --> v(Verb),lex (Verb ,verb(V  type)),v_comps(V  type) ,v  mods.v comps( intrans)  --> \[\].v_comps(trans)  --> np.v_comps(doub ly  trans) --> np,np.v_comps(pobj (Prep) )  --> pp(Prep).v_comps(s_cOmp) --> comp, s.v_comps(s  comp) --> s.v mods- -> pp(Prep).v~mods --> adv.v mods --> sc.v -mods --> \[\].sc --> sub conJ,s.opt iona l_pp ~-> pp(_).opt ional  pp --> par tat ive  marker,npl(measure)?optional_io p --> \[ \] ?pp(Prep) --> \ [Prep\] , lex(Prep,prep) ,np.rel ~-> \[ \] .tel - -> rel  pro, s.det --> \[Word\] , lex(Word, det).adv --> \[Word\] , lex(Word, adv).adj -~-> \[Word\] , lex(Word, adj).sub_conj  - -> \[Word\] , lex(Word, sub conj).n(Type) --> \[Word\] , lex(Word, noun(Type)) .name --> \[Word\], lex(Word, name).oomp --> \[Word\] , lex(Word, comp).par ta t ive_marker  --> \[Word\],lex(Word,partat ive_marker) .tel loro ... np --> \[Word\], lex(Word, rel_pro).v(Verb) --> \ [Verb \ ] , lex (Verb ,verb( ) ) .v(Verb) --> aux, \ [Verb \ ] , lex (Verb ,verb( ) ) .aux --> \ [Verb\ ] , lex(Verb,verb(aux)) .aux --> \[Verb,Negator\] ,lex(Verb, verb(aux)) ,lex(Negator ,negator ) .529
