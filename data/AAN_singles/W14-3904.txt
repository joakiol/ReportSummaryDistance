Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 34?41,October 25, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsExploration of the Impact of Maximum Entropy in Recurrent NeuralNetwork Language Models for Code-Switching SpeechNgoc Thang Vu1,2and Tanja Schultz11Karlsruhe Institute of Technology (KIT),2University of Munich (LMU), Germanythangvu@cis.lmu.de, tanja.schultz@kit.eduAbstractThis paper presents our latest investiga-tions of the jointly trained maximum en-tropy and recurrent neural network lan-guage models for Code-Switching speech.First, we explore extensively the integra-tion of part-of-speech tags and languageidentifier information in recurrent neu-ral network language models for Code-Switching.
Second, the importance ofthe maximum entropy model is demon-strated along with a various of experi-mental results.
Finally, we propose toadapt the recurrent neural network lan-guage model to different Code-Switchingbehaviors and use them to generate artifi-cial Code-Switching text data.1 IntroductionThe term Code-Switching (CS) denotes speechwhich contains more than one language.
Speakersswitch their language while they are talking.
Thisphenomenon appears very often in multilingualcommunities, such as in India, Hong Kong or Sin-gapore.
Furthermore, it increasingly occurs in for-mer monolingual cultures due to the strong growthof globalization.
In many contexts and domains,speakers switch more often between their nativelanguage and English within their utterances thanin the past.
This is a challenge for speech recog-nition systems which are typically monolingual.While there have been promising approaches tohandle Code-Switching in the field of acousticmodeling, language modeling is still a great chal-lenge.
The main reason is a shortage of trainingdata.
Whereas about 50h of training data mightbe sufficient for the estimation of acoustic mod-els, the transcriptions of these data are not enoughto build reliable language models.
In this paper,we focus on exploring and improving the languagemodel for Code-switching speech and as a resultimprove the automatic speech recognition (ASR)system on Code-Switching speech.The main contribution of the paper is the exten-sive investigation of jointly trained maximum en-tropy (ME) and recurrent neural language models(RNN LMs) for Code-Switching speech.
We re-visit the integration of part-of-speech (POS) tagsand language identifier (LID) information in recur-rent neural network language models and the im-pact of maximum entropy on the language modelperformance.
As follow-up to our previous workin (Adel, Vu et al., 2013), here we investigatewhether a recurrent neural network alone withoutusing ME is a suitable model for Code-Switchingspeech.
Afterwards, to directly use the RNN LMin the decoding process of an ASR system, weconvert the RNN LM into the n-gram languagemodel using the text generation approach (Deoraset al., 2011; Adel et al., 2014); Furthermore moti-vated by the fact that Code-Switching is speakerdependent (Auer, 1999b; Vu et al., 2013), wefirst adapt the recurrent neural network languagemodel to different Code-Switching behaviors andthen generate artificial Code-Switching text data.This allows us to train an accurate n-gram modelwhich can be used directly during decoding to im-prove ASR performance.The paper is organized as follows: Section 2gives a short overview of related works.
In Sec-tion 3, we describe the jointly trained maximumentropy and recurrent neural network languagemodels and their extension for Code-Switchingspeech.
Section 4 gives a short description of theSEAME corpus.
In Section 5, we summarize themost important experiments and results.
The studyis concluded in Section 6 with a summary.2 Related WorkThis section gives a brief introduction about therelated research regarding Code-Switching and re-34current language models.In (Muysken, 2000; Poplack, 1978; Bokamba,1989), the authors observed that code switchesoccur at positions in an utterance following syn-tactical rules of the involved languages.
Code-Switching can be regarded as a speaker depen-dent phenomenon (Auer, 1999b; Vu et al., 2013).However, several particular Code-Switching pat-terns are shared across speakers (Poplack, 1980).Furthermore, part-of-speech tags might be usefulfeatures to predict Code-Switching points.
Theauthors of (Solorio et al., 2008b; Solorio et al.,2008a) investigate several linguistic features, suchas word form, LID, POS tags or the position ofthe word relative to the phrase for Code-Switchingprediction.
Their best result is obtained by com-bining all those features.
(Chan et al., 2006)compare four different kinds of n-gram langua-ge models to predict Code-Switching.
They dis-cover that clustering all foreign words into theirPOS classes leads to the best performance.
In (Liet al., 2012; Li et al., 2013), the authors proposeto integrate the equivalence constraint into lan-guage modeling for Mandarin and English Code-Switching speech recorded in Hong Kong.In the last years, neural networks have beenused for a variety of tasks, including languagemodeling (Mikolov et al., 2010).
Recurrent neu-ral networks are able to handle long-term contextssince the input vector does not only contain thecurrent word but also the previous hidden layer.It is shown that these networks outperform tradi-tional language models, such as n-grams whichonly contain very limited histories.
In (Mikolovet al., 2011a), the network is extended by factoriz-ing the output layer into classes to accelerate thetraining and testing processes.
The input layercan be augmented to model features, such as POStags (Shi et al., 2011; Adel, Vu et al., 2013).
Fur-thermore, artificial text can be automatically gen-erated using recurrent neural networks to enlargethe amount of training data (Deoras et al., 2011;Adel et al., 2014).3 Joint maximum entropy and recurrentneural networks language models forCode-Switching3.1 Recurrent neural network languagemodelsThe idea of RNN LMs is illustrated in Figure 1.Vector w(t) forms the input of the recurrent neu-Figure 1: RNN language modelral network.
It represents the current word using1-of-N coding.
Thus, its dimension equals thesize of the vocabulary.
Vector s(t) contains thestate of the network - ?hidden layer?.
The networkis trained using back-propagation through time(BPTT), an extension of the back-propagationalgorithm for recurrent neural networks.
WithBPTT, the error is propagated through recurrentconnections back in time for a specific number oftime steps t. Hence, the network is able to capturea longer history than a traditional n-gram LM.
Thematrices U , V and W contain the weights for theconnections between the layers.
These weights arelearned during the training phase.To accelerate the training process, (Mikolov etal., 2011a) factorized the output layer into classesbased on simple frequency binning.
Every wordbelongs to exactly one class.
Vector c(t) containsthe probabilities for each class and vector w(t)provides the probabilities for each word given itsclass.
Hence, the probability P (wi|history) iscomputed as shown in equation 1.P (wi|history) = P (ci|s(t))P (wi|ci, s(t)) (1)Furthermore in (Mikolov et al., 2011b), the au-thors proposed to jointly train the RNN with ME- RMM-ME - to improve the language model andalso ASR performance.
The ME can be seen asa weight matrix which directly connects the in-put with the output layer as well as the input withthe class layer.
This weight matrix can be trainedjointly with the recurrent neural network.
?Direct-order?
and ?direct connection?
are the two impor-tant parameters which define the length of historyand the number of the trained connections.353.2 Code-Switching language modelsTo adapt RNN LMs to the Code-Switching task,(Adel, Vu et al., 2013) analyzed the SEAME cor-pus and observed that there are words and POStags which might have a high potential to predictCode-Switching points.
Therefore, it has beenproposed to integrate the POS and LID informa-tion into the RNN LM.
The idea is to factorizethe output layer into classes which provide lan-guage information.
By doing that, it is intendedto not only predict the next word but also thenext language.
Hence according to equation 1, theprobability of the next language is computed firstand then the probability of each word given thelanguage.
In that work, four classes were used:English, Mandarin, other languages and particles.Moreover, a vector f(t) which contains the POSinformation is added to the input layer.
This vec-tor provides the corresponding POS of the currentword.
Thus, not only the current word is activatedbut also its features.
Since the POS tags are in-tegrated into the input layer, they are also propa-gated into the hidden layer and back-propagatedinto its history s(t).
Hence, not only the previousfeatures are stored in the history but also featuresfrom several time steps in the past.In addition to that previous work, the experi-ments in this paper aim to explore the source ofthe improvements observed in (Adel, Vu et al.,2013).
We now clearly distinguish between theimpacts due to the long but unordered history ofthe RNN and the effects of the maximum entropymodel which also captures information about themost recent word and POS tag in the history.4 SEAME corpusTo conduct research on Code-Switching speechwe use the SEAME corpus (South East AsiaMandarin-English).
It is a conversationalMandarin-English Code-Switching speech corpusrecorded by (D.C. Lyu et al., 2011).
Originally, itwas used for the research project ?Code-Switch?which was jointly performed by Nanyang Tech-nological University (NTU) and Karlsruhe Insti-tute of Technology (KIT) from 2009 until 2012.The corpus contains 63 hours of audio data whichhas been recorded and manually transcribed inSingapore and Malaysia.
The recordings consistof spontaneously spoken interviews and conver-sations.
The words can be divided into four lan-guage categories: English words (34.3% of all to-kens), Mandarin words (58.6%), particles (Singa-porean and Malayan discourse particles, 6.8% ofall tokens) and others (other languages, 0.4% ofall tokens).
In total, the corpus contains 9,210unique English and 7,471 unique Mandarin words.The Mandarin character sequences have been seg-mented into words manually.
The language dis-tribution shows that the corpus does not contain aclearly predominant language.
Furthermore, thenumber of Code-Switching points is quite high:On average, there are 2.6 switches between Man-darin and English per utterance.
Additionally, theduration of the monolingual segments is rathershort: More than 82% of the English segments and73% of the Mandarin segments last less than onesecond.
The average duration of English and Man-darin segments is only 0.67 seconds and 0.81 sec-onds, respectively.
This corresponds to an averagelength of monolingual segments of 1.8 words inEnglish and 3.6 words in Mandarin.For the task of language modeling and speechrecognition, the corpus has been divided into threedisjoint sets: training, development and evaluationset.
The data is assigned to the three different setsbased on the following criteria: a balanced distri-bution of gender, speaking style, ratio of Singa-porean and Malaysian speakers, ratio of the fourlanguage categories, and the duration in each set.Table 1 lists the statistics of the SEAME corpus.Training Dev Eval# Speakers 139 8 8Duration(hours) 59.2 2.1 1.5# Utterances 48,040 1,943 1,029# Words 575,641 23,293 11,541Table 1: Statistics of the SEAME corpus5 Experiments and ResultsThis section presents all the experiments and re-sults regarding language models and ASR on thedevelopment and the evaluation set of the SEAMEcorpus.
However, the parameters were tuned onlyon the development set.5.1 LM experiments5.1.1 Baseline n-gramThe n-gram language model served as the baselinein this work.
We used the SRI language modeltoolkit (Stolcke, 2002) to build the CS 3-grambaseline from the SEAME training transcriptions36containing all words of the transcriptions.
Modi-fied Kneser-Ney smoothing (Rosenfeld, 2000) wasapplied.
In total, the vocabulary size is around16k words.
The perplexities (PPLs) are 268.4 and282.9 on the development and evaluation set re-spectively.5.1.2 Exploration of ME and of theintegration of POS and LID in RNNTo investigate the effect of POS and LID integra-tion into the RNN LM and the importance of theME, different RNN LMs were trained.The first experiment aims at investigating theimportance of using LID information for outputlayer factorization.
All the results are summarizedin table 2.
The first RNNLM was trained with ahidden layer of 50 nodes and without using outputfactorization and ME.
The PPLs were 250.8 and301.1 on the development and evaluation set, re-spectively.
We observed some gains in terms ofPPL on the development set but not on the eval-uation set compared to the n-gram LM.
Even us-ing ME and factorizing the output layer into fourclasses based on frequency binning (fb), the sametrend could be noticed - only the PPL on the devel-opment set was improved.
Four classes were usedto have a fair comparison with the output factor-ization with LID.
However after including the LIDinformation into the output layer, the PPLs wereimproved on both data sets.
On top of that, usingME provides some additional gains.
The resultsindicate that LID is a useful information sourcefor the Code-Switching task.
Furthermore, the im-provements are independent of the application ofME.Model Dev EvalCS 3-gram 268.4 282.9RNN LM 250.8 301.1RNN-ME LM 246.6 287.9RNN LM with fb 246.0 287.3RNN-ME LM with fb 256.0 294.0RNN LM with LID 241.5 274.4RNN-ME LM with LID 237.9 269.3Table 2: Effect of output layer factorizationIn the second experiment we investigated theuse of POS information and the effect of the ME.The results in Table 3 show that an integration ofPOS without ME did not give any further improve-ment compared to RNN LM.
The reason could liein the fact that a RNN can capture a long historybut not the information of the word order.
Notethat in the syntactic context, the word order is oneof the most important information.
However us-ing ME allows using the POS of the previous timestep to predict the next language and also the nextword, the PPL was improved significantly on de-velopment and evaluation set.
These results revealthat POS is a reasonable trigger event which canbe used to support Code-Switching prediction.Model Dev EvalCS 3-gram 268.4 282.9RNN LM 250.8 301.1RNN-ME LM 246.6 287.9RNN LM with POS 250.6 298.3RNN-ME LM with POS 233.5 268.0Table 3: Effect of ME on the POS integration intothe input layerFinally, we trained an LM by integrating thePOS tags and factorizing the output layer with LIDinformation.
Again without applying ME, we ob-served that POS information is not helpful to im-prove the RNN LM.
Using the ME provides a biggain in terms of PPL on both data sets.
We ob-tained a PPL of 219.8 and 239.2 on the develop-ment and evaluation set respectively.Model Dev EvalCS 3-gram 268.4 282.9RNN LM 250.8 301.1RNN-ME LM 246.6 287.9RNN LM with POS + LID 243.9 277.1RNN-ME LM with POS+ LID 219.8 239.2Table 4: Effect of ME on the integration of POSand the output layer factorization using LID5.1.3 Training parametersMoreover, we investigated the effect of differentparameters, such as the backpropagation throughtime (BPTT) step, the direct connection order andthe amount of direct connections on the perfor-mance of the RNN-ME LMs.
Therefore, differentLMs were trained with varying values for theseparameters.
For each parameter change, the re-maining parameters were fixed to the most suitablevalue which has been found so far.First, we varied the BPTT step from 1 to 5.
TheBPTT step defines the length of the history whichis incorporated to update the weight matrix of the37RNN.
The larger the BPTT step is, the longer is thehistory which is used for learning.
Table 5 showsthe perplexities on the SEAME development andevaluation sets with different BPTT steps.
Theresults indicate that increasing BPTT might im-prove the PPL.
The best PPL can be obtained witha BPTT step of 4.
The big loss in terms of PPLby using a BPTT step of 5 indicates that too longhistories might hurt the language model perfor-mance.
Another reason might be the limitation ofthe training data.BPTT 1 2 3 4 5Dev 244.7 224.6 222.8 219.8 266.8Eval 281.1 241.4 242.8 239.2 284.5Table 5: Effect of the BPTT stepIt has been shown in the previous section, thatME is very important to improve the PPL espe-cially for the Code-Switching task, we also trainedseveral RNN-ME LMs with various values for ?di-rect order?
and ?direct connection?.
Table 6 and7 summarize the PPL on the SEAME develop-ment and evaluation set.
The results reveal thatthe larger the direct order is, the lower is the PPL.We observed consistent PPL improvement by in-creasing the direct order.
However, the gain seemsto be saturated after a direct order of 3 or 4.
In thispaper, we choose to use a direct order of 4 to trainthe final model.Direct order 1 2 3 4Dev 238.6 231.7 220.5 219.8Eval 271.8 261.4 240.7 239.2Table 6: Effect of the direct orderSince the ?direct order?
is related to the lengthof the context, the size of the ?direct connection?
isa trade off between the size of the language modeland also the amount of the training data.
Higher?direct connection?
leads to a larger model andmight improve the PPL if the amount of trainingdata is enough to train all the direct connectionweights.
The results with four different data points(50M, 100M, 150M and 200M) show that the bestmodel can be obtained on SEAME data set by us-ing 100M of direct connection.5.1.4 Artificial Code-Switching textgeneration using RNNThe RNN LM demonstrates a great improvementover the traditional n-gram language model.
How-#Connection 50M 100M 150M 200MDev 226.2 219.8 224.7 224.6Eval 244.7 239.2 243.7 242.0Table 7: Effect of the number of direct connectionsever, it is inefficient to use the RNN LM directlyin the decoding process of an ASR system.
In or-der to convert the RNN into a n-gram languagemodel, a text generation method which was pro-posed in (Deoras et al., 2011) can be applied.Moreover, it allows to generate more training datawhich might be useful to improve the data sparsityof the language modeling task for Code-Switchingspeech.
In (Deoras et al., 2011), the authors ap-plied the Gibb sampling method to generate artifi-cial text based on the probability distribution pro-vided by the RNNs.
We applied that techniquein (Adel et al., 2014) to generate Code-Switchingdata and were able to improve the PPL and ASRperformance on CS speech.
In addition to that pre-vious work, we now propose to use several Code-Switching attitude dependent language models in-stead of the final best RNN LM.Code-Switching attitude dependent languagemodeling Since POS tags might have a potentialto predict Code-Switch points, (Vu et al., 2013)performed an analysis of these trigger POS tagson a speaker level.
The CS rate for each tag wascomputed for each speaker.
Afterwards, we calcu-lated the minimum, maximum and mean values aswell as standard deviations.
We observed that thespread between minimum and maximum values isquite high for most of the tags.
It indicates that al-though POS information may trigger a CS event,it is rather speaker dependent.Motivated by this observation, we performed k-mean clustering of the training text into three dif-ferent portions of text data which describe differ-ent Code-Switching behaviors (Vu et al., 2013).Afterwards, the LM was adapted with each textportion to obtain Code-Switching attitude depen-dent language models.
By using these models, wecould improve both PPL and ASR performance foreach speaker.Artificial text generation To generate artificialtext, we first adapted the best RNN-ME LM de-scribed in the previous section to three differentCode-Switching attitudes.
Afterwards, we gen-erated three different text corpora based on thesespecific Code-Switching attitudes.
Each corpus38contains 100M tokens.
We applied the SRILMtoolkit (Stolcke, 2002) to train n-gram languagemodel and interpolated them linearly with theweight =13.
Table 8 shows the perplexity of theresulting n-gram models on the SEAME develop-ment and evaluation set.
To make a comparison,we also used the unadapted best RNN-ME LM togenerate two different texts, one with 300M to-kens and another one with 235M tokens (Adel etal., 2014).
The results show that the n-gram LMstrained with only the artificial text data can notoutperform the baseline CS 3-gram.
However theyprovide some complementary information to thebaseline CS 3-gram LM.
Therefore, when we in-terpolated them with the baseline CS 3-gram, thePPL was improved all the cases.
Furthermore byusing the Code-Switching attitude dependent lan-guage models to generate artificial CS text data,the PPL was slightly improved compared to usingthe unadapted one.
The final 3-gram model (Final3-gram) was built by interpolating all the Code-Switching attitude dependent 3-gram and the base-line CS 3-gram.
It has a PPL of 249.3 and 266.9on the development set and evaluation set.Models Dev EvalCS 3-gram 268.4 282.9300M words text 391.3 459.5+ CS 3-gram 250.0 270.9235M words text 385.1 454.6+ CS 3-gram 249.5 270.5100M words text I 425.4 514.4+ CS 3-gram 251.4 274.5100M words text II 391.8 421.6+ CS 3-gram 251.6 266.4100M words text III 390.3 428.1+ CS 3-gram 250.6 266.9Interpolation of I, II and III 377.5 416.1+ CS 3-gram (Final n-gram) 249.3 266.9RNN-ME LM + POS + LID 219.8 239.2Table 8: PPL of the N-gram models trained withartificial text data5.2 ASR experimentsFor the ASR experiments, we applied BioKIT, adynamic one-pass decoder (Telaar et al., 2014).The acoustic model is speaker independent andhas been trained with all the training data.
To ex-tract the features, we first trained a multilayer per-ceptron (MLP) with a small hidden layer with 40nodes.
The output of this hidden layer is calledbottle neck features and is used to train the acous-tic model.
The MLP has been initialized with amultilingual multilayer perceptron as described in(Vu et al., 2012).
The phone set contains Englishand Mandarin phones, filler models for continu-ous speech (+noise+, +breath+, +laugh+) and anadditional phone +particle+ for Singaporean andMalayan particles.
The acoustic model applieda fully-continuous 3-state left-to-right HMM.
Theemission probabilities were modeled with Gaus-sian mixture models.
We used a context dependentacoustic model with 3,500 quintphones.
Merge-and-split training was applied followed by six it-erations of Viterbi training.
To obtain a dictio-nary, the CMU English (CMU Dictionary, 2014)and Mandarin (Hsiao et al., 2008) pronunciationdictionaries were merged into one bilingual pro-nunciation dictionary.
Additionally, several rulesfrom (Chen et al., 2010) were applied which gen-erate pronunciation variants for Singaporean En-glish.As a performance measure for decoding Code-Switching speech, we used the mixed error rate(MER) which applies word error rates to En-glish and character error rates to Mandarin seg-ments (Vu et al., 2012).
With character errorrates for Mandarin, the performance can be com-pared across different word segmentations.
Ta-ble 9 shows the results of the baseline CS 3-gramLM, the 3-gram LM trained with 235M artificialwords interpolated with CS 3-gram LM and the fi-nal 3-gram LM described in the previous section.Compared to the baseline system, we are able toimprove the MER by up to 3% relative.
Further-more, a very small gain can be observed by usingthe Code-Switching attitude dependent languagemodel compared to the unadapted best RNN-MELM.Model Dev EvalCS 3-gram 40.0% 34.3%235M words text + CS-3gram 39.4% 33.4%Final 3-gram 39.2% 33.3%Table 9: ASR results on SEAME data6 ConclusionThis paper presents an extensive investigation ofthe impact of maximum entropy in recurrent neu-ral network language models for Code-Switching39speech.
The experimental results reveal that fac-torization of the output layer of the RNN us-ing LID always improved the PPL independentwhether the ME is used.
However, the integra-tion of the POS tags into the input layer only im-proved the PPL in combination with ME.
The bestLM can be obtained by jointly training the MEand the RNN LM with POS integration and fac-torization using LID.
Moreover, using the RNN-ME LM allows generating artificial CS text dataand therefore training an n-gram LM which car-ries the information of the RNN-ME LM.
This canbe directly used during decoding to improve ASRperformance on Code-Switching speech.
On theSEAME development and evaluation set, we ob-tained an improvement of up to 18% relative interms of PPL and 3% relative in terms of MER.7 AcknowledgmentThis follow-up work on exploring the impact ofmaximum entropy in recurrent neural network lan-guage models for Code-Switching speech was mo-tivated by the very useful comments and sugges-tions of the SLSP reviewers, for which we are verygrateful.ReferencesH.
Adel, N.T.
Vu, F. Kraus, T. Schlippe, and T. Schultz.Recurrent Neural Network Language Modeling forCode Switching Conversational Speech In: Pro-ceedings of ICASSP 2013.H.
Adel, K. Kirchhoff, N.T.
Vu, D.Telaar, T. SchultzComparing Approaches to Convert Recurrent Neu-ral Networks into Backoff Language Models For Ef-ficient Decoding In: Proceedings of Interspeech2014.P.
Auer Code-Switching in Conversation Routledge1999.P.
Auer From codeswitching via language mixing tofused lects toward a dynamic typology of bilingualspeech In: International Journal of Bilingualism,vol.
3, no.
4, pp.
309-332, 1999.E.G.
Bokamba Are there syntactic constraints on code-mixing?
In: World Englishes, vol.
8, no.
3, pp.
277-292, 1989.J.Y.C.
Chan, PC Ching, T. Lee, and H. Cao Au-tomatic speech recognition of Cantonese-Englishcode-mixing utterances In: Proceeding of Inter-speech 2006.W.
Chen, Y. Tan, E. Chng, H. Li The development of aSingapore English call resource In: Proceedings ofOriental COCOSDA, 2010.Carnegie Mellon University CMU pronoun-cation dictionary for English Online:http://www.speech.cs.cmu.edu/cgi-bin/cmudict,retrieved in July 2014D.C.
Lyu, T.P.
Tan, E.S.
Cheng, H. Li An Analysis ofMandarin-English Code-Switching Speech Corpus:SEAME In: Proceedings of Interspeech 2011.A.
Deoras, T. Mikolov, S. Kombrink, M. Karafiat, S.Khudanpur Variational approximation of long-spanlanguage models for LVCSR In: Proceedings ofICASSP 2011.R.
Hsiao, M. Fuhs, Y. Tam, Q. Jin, T. Schultz TheCMU-InterACT 2008 Mandarin transcription sys-tem In: Procceedings of ICASSP 2008.Y.
Li, P. Fung Code-Switch Language Model withInversion Constraints for Mixed Language SpeechRecognition In: Proceedings of COLING 2012.Y.
Li, P. Fung Improved mixed language speech recog-nition using asymmetric acoustic model and lan-guage model with Code-Switch inversion constraintsIn: Proceedings of ICASSP 2013.M.P.
Marcus, M.A.
Marcinkiewicz, and B. Santorini.Building a large annotated corpus of english: Thepenn treebank In: Computational Linguistics, vol.19, no.
2, pp.
313-330, 1993.T.
Mikolov, M. Karafiat, L. Burget, J. Jernocky and S.Khudanpur.
Recurrent Neural Network based Lan-guage Model In: Proceedings of Interspeech 2010.T.
Mikolov, S. Kombrink, L. Burget, J. Jernocky andS.
Khudanpur.
Extensions of Recurrent Neural Net-work Language Model In: Proceedings of ICASSP2011.T.
Mikolov, A. Deoras, D. Povey, L. Burget, J.H.
Cer-nocky Strategies for Training Large Scale Neu-ral Network Language Models In: Proceedings ofASRU 2011.P.
Muysken Bilingual speech: A typology of code-mixing In: Cambridge University Press, vol.
11.S.
Poplack Syntactic structure and social functionof code-switching , Centro de Estudios Puertor-riquenos, City University of New York.S.
Poplack Sometimes i?ll start a sentence in spanishy termino en espanol: toward a typology of code-switching In: Linguistics, vol.
18, no.
7-8, pp.
581-618.D.
Povey, A. Ghoshal, et al.
The Kaldi speech recogni-tion toolkit In: Proceedings of ASRU 2011.R.
Rosenfeld Two decades of statistical language mod-eling: Where do we go from here?
In: Proceedingsof the IEEE 88.8 (2000): 1270-1278.T.
Schultz, P. Fung, and C. Burgmer, Detecting code-switch events based on textual features.40Y.
Shi, P. Wiggers, M. Jonker Towards Recurrent Neu-ral Network Language Model with Linguistics andContextual Features In: Proceedings of Interspeech2011.T.
Solorio, Y. Liu Part-of-speech tagging for English-Spanish code-switched text In: Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing.
Association for ComputationalLinguistics, 2008.T.
Solorio, Y. Liu Learning to predict code-switchingpoints In: Proceedings of the Conference on Empir-ical Methods in Natural Language Processing.
As-sociation for Computational Linguistics, 2008.A.
Stolcke SRILM-an extensible language modelingtoolkit.
In: Proceedings of Interspeech 2012.D.
Telaar, et al.
BioKIT - Real-time Decoder ForBiosignal Processing In: Proceedings of Inter-speech 2014.N.T.
Vu, D.C. Lyu, J. Weiner, D. Telaar, T. Schlippe,F.
Blaicher, E.S.
Chng, T. Schultz, H. Li A FirstSpeech Recognition System For Mandarin-EnglishCode-Switch Conversational Speech In: Proceed-ings of Interspeech 2012.N.T.
Vu, H. Adel, T. Schultz An Investigation of Code-Switching Attitude Dependent Language ModelingIn: In Statistical Language and Speech Processing,First International Conference, 2013.N.T.
Vu, F. Metze, T. Schultz Multilingual bottleneckfeatures and its application for under-resourced lan-guages In: Proceedings of SLTU, 2012.41
