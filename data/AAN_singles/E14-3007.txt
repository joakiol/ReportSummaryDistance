Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 56?64,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsModelling Irony in TwitterFrancesco BarbieriPompeu Fabra UniversityBarcelona, Spainfrancesco.barbieri@upf.eduHoracio SaggionPompeu Fabra UniversityBarcelona, Spainhoracio.saggion@upf.eduAbstractComputational creativity is one of thecentral research topics of Artificial Intel-ligence and Natural Language Process-ing today.
Irony, a creative use oflanguage, has received very little atten-tion from the computational linguisticsresearch point of view.
In this studywe investigate the automatic detection ofirony casting it as a classification prob-lem.
We propose a model capable of de-tecting irony in the social network Twit-ter.
In cross-domain classification experi-ments our model based on lexical featuresoutperforms a word-based baseline previ-ously used in opinion mining and achievesstate-of-the-art performance.
Our featuresare simple to implement making the ap-proach easily replicable.1 IntroductionIrony, a creative use of language, has received verylittle attention from the computational linguisticsresearch point of view.
It is however considered animportant aspect of language which deserves spe-cial attention given its relevance in fields such assentiment analysis and opinion mining (Pang andLee, 2008).
Irony detection appears as a difficultproblem since ironic statements are used to ex-press the contrary of what is being said (Quintilienand Butler, 1953), therefore being a tough nut tocrack by current systems.
Being a creative form oflanguage, there is no consensual agreement in theliterature on how verbal irony should be defined.Only recently irony detection has been approachedfrom a computational perspective.
Reyes et al.
(2013) cast the problem as one of classificationtraining machine learning algorithms to sepatareironic from non-ironic statements.
In a similarvein, we propose and evaluate a new model todetect irony, using seven sets of lexical features,most of them based on our intuitions about ?un-expectedness?, a key component of ironic state-ments.
Indeed, Lucariello (1994) claims that ironyis strictly connected to surprise, showing that un-expectedness is the feature most related to situa-tional ironies.In this paper we reduce the complexity of theproblem by studying irony detection in the micro-blogging service Twitter1that allows users to sendand read text messages (shorter than 140 charac-ters) called tweets.We do not adopt any formal definition of irony,instead we rely on a dataset created for the studyof irony detection which allows us to compare ourfindings with recent state-of-the-art approaches(Reyes et al., 2013).The contributions of this paper are as follows:?
a novel set of linguistically motivated, easy-to-compute features?
a comparison of our model with the state-of-the-art; and?
a novel set of experiments to demonstratecross-domain adaptation.The paper will show that our model outperformsa baseline, achieves state-of-the-art performance,and can be applied to different domains.The rest of the paper is organised as follows: inthe next Section we describe related work.
In Sec-tion 3 we described the corpus and text process-ing tools used and in Section 4 we present our ap-proach to tackle the irony detection problem.
Sec-tion 5 describes the experiments while Section 6interprets the results.
Finally we close the paper inSection 7 with conclusions and future work.1https://twitter.com/562 Related WorkVerbal irony has been defined in several ways overthe years but there is no consensual agreementon the definition.
The standard definition is con-sidered ?saying the opposite of what you mean?
(Quintilien and Butler, 1953) where the opposi-tion of literal and intended meanings is very clear.Grice (1975) believes that irony is a rhetorical fig-ure that violates the maxim of quality: ?Do notsay what you believe to be false?.
Irony is also de-fined (Giora, 1995) as any form of negation withno negation markers (as most of the ironic utter-ances are affirmative, and ironic speakers use in-direct negation).
Wilson and Sperber (2002) de-fined it as echoic utterance that shows a negativeaspect of someone?s else opinion.
For example ifsomeone states ?the weather will be great tomor-row?
and the following day it rains, someone withironic intents may repeat the sentence ?the weatherwill be great tomorrow?
in order to show the state-ments was incorrect.
Finally irony has been de-fined as form of pretence by Utsumi (2000) andVeale and Hao (2010b).
Veale states that ?ironicspeakers usually craft their utterances in spite ofwhat has just happened, not because of it.
Thepretence alludes to, or echoes, an expectation thathas been violated?.Past computational approaches to irony detec-tion are scarce.
Carvalho et.
al (2009) createdan automatic system for detecting irony relying onemoticons and special punctuation.
They focusedon detection of ironic style in newspaper articles.Veale and Hao (2010a) proposed an algorithm forseparating ironic from non-ironic similes, detect-ing common terms used in this ironic comparison.Reyes et.
al (2013) have recently proposed a modelto detect irony in Twitter, which is based on fourgroups of features: signatures, unexpectedness,style, and emotional scenarios.
Their classificationresults support the idea that textual features cancapture patterns used by people to convey irony.Among the proposed features, skip-grams (part ofthe style group) which captures word sequencesthat contain (or skip over) arbitrary gaps, seems tobe the best one.There are also a few computational model thatdetect sarcasm ((Davidov et al., 2010); (Gonz?alez-Ib?a?nez et al., 2011); (Liebrecht et al., 2013)) onTwitter and Amazon, but even if one may arguethat sarcasm and irony are the same linguistic phe-nomena, the latter is more similar to mocking ormaking jokes (sometimes about ourselves) in asharp and non-offensive manner.
On the otherhand, sarcasm is a meaner form of irony as it tendsto be offensive and directed towards other people(or products like in Amazon reviews).
Textual ex-amples of sarcasm lack the sharp tone of an ag-gressive speaker, so for textual purposes we thinkirony and sarcasm should be considered as differ-ent phenomena and studied separately (Reyes etal., 2013).3 Data and Text ProcessingThe dataset used for the experiments reportedin this paper has been prepared by Reyes et al.(2013).
It is a corpus of 40.000 tweets equally di-vided into four different topics: Irony, Education,Humour, and Politics where the last three topicsare considered non-ironic.
The tweets were au-tomatically selected by looking at Twitter hash-tags (#irony, #education, #humour, and #politics)added by users in order to link their contribution toa particular subject and community.
The hashtagsare removed from the tweets for the experiments.According to Reyes et.
al (2013), these hashtagswere selected for three main reasons: (i) to avoidmanual selection of tweets, (ii) to allow irony anal-ysis beyond literary uses, and because (iii) ironyhashtag may ?reflect a tacit belief about what con-stitutes irony.
?Another corpora is employed in our approach tomeasure the frequency of word usage.
We adoptedthe Second Release of the American National Cor-pus Frequency Data2(Ide and Suderman, 2004),which provides the number of occurrences of aword in the written and spoken ANC.
From nowon, we will mean with ?frequency of a term?
theabsolute frequency the term has in the ANC.3.1 Text ProcessingIn order to process the tweets we use the freelyavailable vinhkhuc Twitter Tokenizer3which al-lows us to recognise words in each tweet.
To part-of-speech tag the words, we rely on the Rita Word-Net API (Howe, 2009) that associates to a wordwith its most frequently used part of speech.
Wealso adopted the Java API for WordNet Searching2The American National Corpus (http://www.anc.org/) is,as we read in the web site, a massive electronic collection ofAmerican English words (15 million)3https://github.com/vinhkhuc/Twitter-Tokenizer/blob/master/src/Twokenizer.java57(Spell, 2009) to perform some operation on Word-Net synsets.
It is worth noting that although ourapproach to text processing is rather superficial forthe moment, other tools are available to performdeeper tweet linguistic analysis (Bontcheva et al.,2013; Derczynski et al., 2013).4 MethodologyWe approach the detection of irony as a classifica-tion problem applying supervised machine learn-ing methods to the Twitter corpus described inSection 3.
When choosing the classifiers we hadavoided those requiring features to be independent(e.g.
Naive Bayes) as some of our features are not.Since we approach the problem as a binary deci-sion (deciding if a tweet is ironic or not) we pickedtwo tree-based classifiers: Random Forest and De-cision tree (the latter allows us to compare ourfindings directly to Reyes et.
al (2013)).
We usethe implementations available in the Weka toolkit(Witten and Frank, 2005).To represent each tweet we use six groups offeatures.
Some of them are designed to detect im-balance and unexpectedness, others to detect com-mon patterns in the structure of the ironic tweets(like type of punctuation, length, emoticons).
Be-low is an overview of the group of features in ourmodel:?
Frequency (gap between rare and commonwords)?
Written-Spoken (written-spoken style uses)?
Intensity (intensity of adverbs and adjectives)?
Structure (length, punctuation, emoticons)?
Sentiments (gap between positive and nega-tive terms)?
Synonyms (common vs. rare synonyms use)?
Ambiguity (measure of possible ambiguities)In our knowledge Frequency, Written Spoken, In-tensity and Synonyms groups have not been usedbefore in similar studies.
The other groups havebeen used already (for example by Carvalho et.
al(2009) or Reyes et al.
(2013)) yet our implemen-tation is different in most of the cases.In the following sections we describe the the-oretical motivations behind the features and howthem have been implemented.4.1 FrequencyAs said previously unexpectedness can be a sig-nal of irony and in this first group of features wetry to detect it.
We explore the frequency imbal-ance between words, i.e.
register inconsistenciesbetween terms of the same tweet.
The idea is thatthe use of many words commonly used in English(i.e.
high frequency in ANC) and only a few termsrarely used in English (i.e.
low frequency in ANC)in the same sentence creates imbalance that maycause unexpectedness, since within a single tweetonly one kind of register is expected.
We are ableto explore this aspect using the ANC FrequencyData corpus.Three features belong to this group: frequencymean, rarest word, frequency gap.
The first oneis the arithmetic average of all the frequencies ofthe words in a tweet, and it is used to detect thefrequency style of a tweet.
The second one, rarestword, is the frequency value of the rarest word,designed to capture the word that may create im-balance.
The assumption is that very rare wordsmay be a sign of irony.
The third one is the abso-lute difference between the first two and it is usedto measure the imbalance between them, and cap-ture a possible intention of surprise.
We have ver-ified that the mean of this gap in each tweet of theirony corpus is higher than in the other corpora.4.2 Written-SpokenTwitter is composed of written text, but an infor-mal spoken English style is often used.
We de-signed this set of features to explore the unexpect-edness created by using spoken style words in amainly written style tweet or vice versa (formalwords usually adopted in written text employed ina spoken style context).
We can analyse this aspectwith ANC written and spoken, as we can see us-ing this corpora whether a word is more often usedin written or spoken English.
There are three fea-tures in this group: written mean, spoken mean,written spoken gap.
The first and second ones arethe means of the frequency values, respectively, inwritten and spoken ANC corpora of all the wordsin the tweet.
The third one, written spoken gap,is the absolute value of the difference between thefirst two, designed to see if ironic writers use bothstyles (creating imbalance) or only one of them.
Alow difference between written and spoken stylesmeans that both styles are used.584.3 StructureWith this group of features we want to study thestructure of the tweet: if it is long or short (length),if it contains long or short words (mean of wordlength), and also what kind of punctuation is used(exclamation marks, emoticons, etc.).
This is apowerful feature, as ironic tweets in our corporapresent specific structures: for example they areoften longer than the tweets in the other corpora,they contain certain kind of punctuation and theyuse only specific emoticons.
This group includesseveral features that we describe below.The length feature consists of the number ofcharacters that compose the tweet, n. words isthe number of words, and words length mean isthe mean of the words length.
Moreover, we usethe number of verbs, nouns, adjectives and adverbsas features, naming them n. verbs, n. nouns, n.adjectives and n. adverbs.
With these last fourfeatures we also computed the ratio of each partof speech to the number of words in the tweet; wecalled them verb ratio, noun ratio, adjective ra-tio, and adverb ratio.
All these features have thepurpose of capturing the style of the writer.
Someof them seem to be significant; for example theaverage length of an ironic tweet is 94.8 charac-ters and the average length of education, humour,and politics tweets are respectively 82.0, 86.6, and86.5.
The words used in the irony corpus are usu-ally shorter than in the other corpora, but theyamount to more.The punctuation feature is the sum of the num-ber of commas, full stops, ellipsis and exclama-tion that a tweet presents.
We also added a featurecalled laughing which is the sum of all the internetlaughs, denoted with hahah, lol, rofl, and lmao thatwe consider as a new form of punctuation: insteadof using many exclamation marks internet usersmay use the sequence lol (i.e.
laughing out loud) orjust type hahaha.
As the previous features, punc-tuation and laughing occur more frequently in theironic tweets than in the other topics.The emoticon feature is the sum of the emoti-cons :), :D, :( and ;) in a tweet.
This feature workswell in the humour corpus because is the one thatpresents a very different number of them, it hasfour times more emoticons than the other corpora.The ironic corpus is the one with the least emoti-cons (there are only 360 emoticons in the Ironycorpus, while in Humour, Education, and Poli-tics tweets they are 2065, 492, 397 respectively).In the light of these statistics we can argue thatironic authors avoid emoticons and leave words tobe the central thing: the audience has to under-stand the irony without explicit signs, like emoti-cons.
Another detail is the number of winks ;).
Inthe irony corpus one in every five emoticon is awink, whereas in the Humour, Education and Pol-itics corpora the number of winks are 1 in every30, 22 and 18 respectively.
Even if the wink is nota usual emoticon, ironic authors use it more of-ten because they mean something else when writ-ing their tweets, and a wink is used to suggest thatsomething is hidden behind the words.4.4 IntensityA technique ironic authors may employ is sayingthe opposite of what they mean (Quintilien andButler, 1953) using adjectives and adverbs to, forexample, describe something very big to denotesomething very small (e.g.
saying ?Do we hikethat tiny hill now??
before going on top of a veryhigh mountain).
In order to produce an ironic ef-fect some authors might use an expression whichis antonymic to what they are trying to describe,we believe that in the case the word being an ad-jective or adverb its intensity (more or less exag-gerated) may well play a role in producing the in-tended effect.
We adopted the intensity scores ofPotts (2011) who uses naturally occurring meta-data (star ratings on service and product reviews)to construct adjectives and adverbs scales.
An ex-ample of adjective scale (and relative scores inbrackets) could be the following: horrible (-1.9)?
bad (-1.1)?
good (0.2)?
nice (0.3)?
great(0.8).With these scores we evaluate four features foradjective intensity and four for adverb intensity(implemented in the same way): adj (adv) tot,adj (adv) mean, adj (adv) max, and adj (adv)gap.
The sum of the AdjScale scores of all the ad-jectives in the tweet is called adj tot.
adj mean isadj tot divided by the number of adjectives in thetweet.
The maximum AdjScale score within a sin-gle tweet is adj max.
Finally, adj gap is the differ-ence between adj max and adj mean, designed tosee ?how much?
the most intense adjective is outof context.4.5 SynonymsIronic authors send two messages to the audienceat the same time, the literal and the figurative one(Veale, 2004).
It follows that the choice of a term59(rather than one of its synonyms) is very impor-tant in order to send the second, not obvious, mes-sage.
For example if the sky is grey and it isabout to rain, someone with ironic intents may say?sublime weather today?, choosing sublime overmany different, more common, synonyms (likenice, good, very good and so on, that according toANC are more used in English) to advise the lis-tener that the literal meaning may not be the onlymeaning present.
A listener will grasp this hid-den information when he asks himself why a rareword like sublime was used in that context whereother more common synonyms were available toexpress the same literal meaning.For each word of a tweet we get its synonymswith WordNet (Miller, 1995), then we calculatetheir ANC frequencies and sort them into a de-creasing ranked list (the actual word is part of thisranking as well).
We use these rankings to definethe four features which belong to this group.
Thefirst one is syno lower which is the number of syn-onyms of the word wiwith frequency lower thanthe frequency of wi.
It is defined as in Equation 1:slwi= |syni,k: f(syni,k) < f(wi)| (1)where syni,kis the synonym of wiwith rank k,and f(x) the ANC frequency of x.
Then we alsodefined syno lower mean as mean of slwi(i.e.
thearithmetic average of slwiover all the words of atweet).We also designed two more features: synolower gap and syno greater gap, but to definethem we need two more parameters.
The first oneis word lowest syno that is the maximum slwiin atweet.
It is formally defined as:wlst= maxwi{|syni,k: f(syni,k) < f(wi)|}(2)The second one is word greatest syno defined as:wgst= maxwi{|syni,k: f(syni,k) > f(wi)|}(3)We are now able to describe syno lower gapwhich detects the imbalance that creates a com-mon synonym in a context of rare synonyms.
It isthe difference between word lowest syno and synolower mean.
Finally, we detect the gap of veryrare synonyms in a context of common ones withsyno greater gap.
It is the difference betweenword greatest syno and syno greater mean, wheresyno greater mean is the following:sgmt=|syni,k: f(syni,k) > f(wi)|n.
words of t(4)The arithmetic averages of syno greater gapand of syno lower gap in the irony corpus arehigher than in the other corpora, suggesting that avery common (or very rare) synonym is often usedout of context i.e.
a very rare synonym when mostof the words are common (have a high rank in ourmodel) and vice versa.4.6 AmbiguityAnother interesting aspect of irony is ambiguity.We noticed that the arithmetic average of the num-ber of WordNet synsets in the irony corpus isgreater than in all the other corpora; this indi-cates that ironic tweets presents words with moremeanings.
Our assumption is that if a word hasmany meanings the possibility of ?saying some-thing else?
with this word is higher than in a termthat has only a few meanings, then higher possibil-ity of sending more then one message (literal andintended) at the same time.There are three features that aim to capturethese aspects: synset mean, max synset, andsynset gap.
The first one is the mean of the num-ber of synsets of each word of the tweet, to see ifwords with many meanings are often used in thetweet.
The second one is the greatest number ofsynsets that a single word has; we consider thisword the one with the highest possibility of beingused ironically (as multiple meanings are availableto say different things).
In addition, we calculatesynset gap as the difference between the numberof synsets of this word (max synset) and the av-erage number of synsets (synset mean), assumingthat if this gap is high the author may have usedthat inconsistent word intentionally.4.7 SentimentsWe think that sign of irony could also be foundusing sentiment analysis.
The SentiWordNet sen-timent lexicon (Esuli and Sebastiani, 2006) as-signs to each synset of WordNet sentiment scoresof positivity and negativity.
We used these scoresto examine what kind of sentiments characterisesirony.
We explore ironic sentiments with two dif-ferent views: the first one is the simple analysisof sentiments (to identify the main sentiment thatarises from ironic tweets) and the second one con-cerns sentiment imbalances between words, de-60Training SetEducation Humour PoliticsTest set P R F1 P R F1 P R F1Education .85/.73 .84/.73 .84/.73 .57/.61 .53/.61 .46/.61 .61/.67 .56/.67 .51/.67Humour .64/.62 .51/.62 .58/.62 .85/.75 .85/.75 .85/.75 .65/.61 .59/.61 .55/.60Politics .61/.67 .58/.67 .55/.67 .55/.61 .60/.60 .56/.60 .87/.75 .87/.75 .87/.75Table 1: Precision, Recall and F-Measure of each topic combination for word based algorithm and ouralgorithm in the form ?Word Based / Ours?.
Decision Tree has been used as classifier for both algorithms.We marked in bold the results that, according to the t-test, are significantly better.signed to explore unexpectedness from a senti-ment prospective.There are six features in the Sentiments group.The first one is named positive sum and it is thesum of all the positive scores in a tweet, the sec-ond one is negative sum, defined as sum of all thenegative scores.
The arithmetic average of the pre-vious ones is another feature, named positive neg-ative mean, designed to reveal the sentiment thatbetter describe the whole tweet.
Moreover, thereis positive-negative gap that is the difference be-tween the first two features, as we wanted also todetect the positive/negative imbalance within thesame tweet.The imbalance may be created using only onesingle very positive (or negative) word in thetweet, and the previous features will not be ableto detect it, thus we needed to add two more.
Forthis purpose the model includes positive singlegap defined as the difference between most posi-tive word and the mean of all the sentiment scoresof all the words of the tweet and negative singlegap defined in the same way, but with the mostnegative one.4.8 Bag of Words BaselineBased on previous work on sentiment analysis andopinon classification (see (Pang et al., 2002; Daveet al., 2003) for example) we also investigate thevalue of using bag of words representations forirony classification.
In this case, each tweet is rep-resented as a set of word features.
Because of thebrevity of tweets, we are only considering pres-ence/absence of terms instead of frequency-basedrepresentations based on tf ?
idf .5 Experiments and ResultsIn order to carry out experimentation and to beable to compare our approach to that of (Reyes etal., 2013) we use three datasets derived from thecorpus in Section 3.
Irony vs Education, Ironyvs Humour and Irony vs Politics.
Each topiccombination was balanced with 10.000 ironicand 10.000 of non-ironic examples.
The task athand it to train a classifier to identify ironic andnon-ironic tweets.Figure 1: Information gain value of each group(mean of the features belonged to each group) overthe three balanced corpus.We perform two types of experiments:?
we run in each of the datasets a 10-fold cross-validation classification;?
across datasets, we train the classifier in onedataset and apply it to the other two datasets.To perform these experiments, we createthree balanced datasets containing each onethird of the original 10.000 ironic tweets (sothat the datasets are disjoint) and one third ofthe original domain tweets.The experimental framework is executed for theword-based baseline model and our model.
In Ta-ble 1 we present precision, recall, and F-measure61Figure 2: Information gain of each feature of the model.
Irony corpus is compared to Education, Humor,and Politics corpora.
High values of information gain help to better discriminate ironic from non-ironictweets.figures for the different runs of the experiments.Table 3 shows precision, recall, and F-measurefigures for our approach compared to (Reyes etal., 2013).
Table 2 compares two different algo-rithms: Decision Tree and Random Forest usingour model.In order to have a clear understanding about thecontribution of each set of features in our model,we also studied the behaviour of information gainin each dataset.
We compute information gainexperiments over the three balanced corpora andpresent the results in Figure 1.
The graphic showsthe mean information gain for each group of fea-tures.
We also report in Figure 2 the informationgain of each single feature, where one can under-stand if a feature will be important to distinguishironic from non-ironic tweets.6 DiscussionThe results obtained with the bag-of-words base-line seem to indicate that this approach is work-ing as a topic-based classifier and not as an ironydetection procedure.
Indeed, within each domainusing a 10 fold cross-validation setting, the bag-of-words approach seems to overtake our model.However, a clear picture emerges when a cross-domain experiment is performed.
In a settingwhere different topics are used for training andtesting our model performs significantly betterthan the baseline.
t-tests were run for each ex-periment and differences between baseline and ourmodel were observed for each cross-domain con-dition (with a 99% confidence level).
This couldbe an indication that our model is more able to cap-ture ironic style disregarding domain.Analysing the data on Figure 2, we observe thatfeatures which are more discriminative of ironicstyle are rarest value, synonym lower, synonymgreater gap, and punctuation, suggesting thatFrequency, Structure and choice of the Synonymare important aspects to consider for irony detec-tion in tweets (this latter statement can be appre-ciated in Figure 1 as well).
Note, however, thatthere is a topic or theme effect since features be-have differently depending on the dataset used:the Humour corpus seems to be the least consis-tent.
For instance punctuation well distinguishesironic from educational tweets, but behaves poorlyin the Humour corpus.
This imbalance may causeissues in a not controlled environment (e.g.
nopreselected topics, only random generic tweets).In spite of this, information gain values are fairlyhigh with four features having information gainvalues over 0.1.
Finding features that are signif-icant for any non-ironic topic is hard, this is whyour system includes several feature sets: they aimto distinguish irony from as many different topicsas possible.62Training SetEducation Humour PoliticsTest set P R F1 P R F1 P R F1Education .78/.73 .78/.73 .78/.73 .65/.61 .63/.61 .62/.61 .71/.67 .71/.67 .70/.67Humour .64/.62 .61/.62 .60/.62 .80/.75 .80/.75 .80/.75 .64/.61 .62/.61 .60/.60Politics .71/.67 .70/.67 .69/.67 .63/.61 .51/.60 .59/.60 .79/.75 .79/.75 .79/.75Table 2: Precision, Recall and F-Measure for each topic combination of our model when Decision Treeand Random Forest are used.
Data are in the format ?Random Forest / Decision Tree?.
We marked inbold the F-Measures that are better.Education Humour PoliticsModel P R F1 P R F1 P R F1Reyes et.
al .76 .66 .70 .78 .74 .76 .75 .71 .73Our model .73 .73 .73 .75 .75 .75 .75 .75 .75Table 3: Precision, Recall, and F-Measure over the three corpora Education, Humour, and Politics.
Bothour and Reyes et al.
results are shown; the classifier used is Decision Tree for both models.
We markedin bold the F-Measures that are better compared to the other model.With respect to results for two different classi-fiers trained with our model (Random Forest (RF)and Decision Trees (DT)) we observe that (see Ta-ble 2) RF is better in cross-validation but across-domains both algorithms are comparable.Turning now to the state of the art we compareour approach to (Reyes et al., 2013), the num-bers presented in Table 3 seem to indicate that (i)our approach is more balanced in terms of preci-sion and recall and that (ii) our approach performsslightly better in terms of F-Measure in two out ofthree domains.7 Conclusion and Future WorkIn this article we have proposed a novel linguisti-cally motivated set of features to detect irony in thesocial network Twitter.
The features take into ac-count frequency, written/spoken differences, senti-ments, ambiguity, intensity, synonymy and struc-ture.
We have designed many of them to be ableto model ?unexpectedness?, a key characteristic ofirony.We have performed controlled experiments withan available corpus of ironic and non-ironic tweetsusing classifiers trained with bag-of-words fea-tures and with our irony specific features.
We haveshown that our model performs better than a bag-of-words approach across-domains.
We have alsoshown that our model achieves state-of-the-art per-formance.There is however much space for improve-ments.
The ambiguity aspect is still weak in thisresearch, and it needs to be improved.
Also exper-iments adopting different corpora (Filatova, 2012)and different negative topics may be useful in or-der to explore the system behaviour in a real situa-tion.
Finally, we have relied on very basic tools forlinguistic analysis of the tweets, so in the near fu-ture we intend to incorporate better linguistic pro-cessors.
A final aspect we want to investigate isthe use of n-grams from huge collections to model?unexpected?
word usage.AcknowledgmentsWe are greatful to three anonymous reviewers fortheir comments and suggestions that help improveour paper.
The research described in this paper ispartially funded by fellowship RYC-2009-04291from Programa Ram?on y Cajal 2009 and projectnumber TIN2012-38584-C06-03 (SKATER-UPF-TALN) from Ministerio de Econom?
?a y Competi-tividad, Secretar?
?a de Estado de Investigaci?on, De-sarrollo e Innovac?
?on, Spain.ReferencesKalina Bontcheva, Leon Derczynski, Adam Funk,Mark A. Greenwood, Diana Maynard, and NirajAswani.
2013.
TwitIE: An Open-Source Informa-tion Extraction Pipeline for Microblog Text.
In Pro-ceedings of Recent Advances in Natural LanguageProcessing Conferemce.Paula Carvalho, Lu?
?s Sarmento, M?ario J Silva, and63Eug?enio de Oliveira.
2009.
Clues for detect-ing irony in user-generated contents: oh...!!
it?sso easy;-).
In Proceedings of the 1st internationalCIKM workshop on Topic-sentiment analysis formass opinion, pages 53?56.
ACM.Kushal Dave, Steve Lawrence, and David M. Pennock.2003.
Mining the peanut gallery: Opinion extractionand semantic classification of product reviews.
InProceedings of the 12th International Conference onWorld Wide Web, WWW ?03, pages 519?528, NewYork, NY, USA.
ACM.Dmitry Davidov, Oren Tsur, and Ari Rappoport.
2010.Semi-supervised recognition of sarcastic sentencesin twitter and amazon.
In Proceedings of the Four-teenth Conference on Computational Natural Lan-guage Learning, pages 107?116.
Association forComputational Linguistics.Leon Derczynski, Alan Ritter, Sam Clark, and KalinaBontcheva.
2013.
Twitter part-of-speech taggingfor all: Overcoming sparse and noisy data.
In Pro-ceedings of the International Conference on RecentAdvances in Natural Language Processing.Andrea Esuli and Fabrizio Sebastiani.
2006.
Sen-tiwordnet: A publicly available lexical resourcefor opinion mining.
In Proceedings of LanguageResources and Evaluation Conference, volume 6,pages 417?422.Elena Filatova.
2012.
Irony and sarcasm: Corpusgeneration and analysis using crowdsourcing.
InProceedings of Language Resources and EvaluationConference, pages 392?398.Rachel Giora.
1995.
On irony and negation.
Discourseprocesses, 19(2):239?264.Roberto Gonz?alez-Ib?a?nez, Smaranda Muresan, andNina Wacholder.
2011.
Identifying sarcasm in twit-ter: A closer look.
In ACL (Short Papers), pages581?586.
Citeseer.H Paul Grice.
1975.
Logic and conversation.
1975,pages 41?58.Daniel C Howe.
2009.
Rita wordnet.
java based api toaccess wordnet.Nancy Ide and Keith Suderman.
2004.
The Ameri-can National Corpus First Release.
In Proceedingsof the Language Resources and Evaluation Confer-ence.Christine Liebrecht, Florian Kunneman, and Antalvan den Bosch.
2013.
The perfect solution fordetecting sarcasm in tweets# not.
WASSA 2013,page 29.Joan Lucariello.
1994.
Situational irony: A concept ofevents gone awry.
Journal of Experimental Psychol-ogy: General, 123(2):129.George A Miller.
1995.
Wordnet: a lexicaldatabase for english.
Communications of the ACM,38(11):39?41.Bo Pang and Lillian Lee.
2008.
Opinion Mining andSentiment Analysis.
Found.
Trends Inf.
Retr., 2(1-2):1?135, January.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: Sentiment classification usingmachine learning techniques.
In Proceedings of theACL-02 Conference on Empirical Methods in Natu-ral Language Processing - Volume 10, EMNLP ?02,pages 79?86, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Christopher Potts.
2011.
Developing adjective scalesfrom user-supplied textual metadata.
NSF Work-shop on Restructuring Adjectives in WordNet.
Ar-lington,VA.Quintilien and Harold Edgeworth Butler.
1953.
TheInstitutio Oratoria of Quintilian.
With an EnglishTranslation by HE Butler.
W. Heinemann.Antonio Reyes, Paolo Rosso, and Tony Veale.
2013.A multidimensional approach for detecting irony intwitter.
Language Resources and Evaluation, pages1?30.Brett Spell.
2009.
Java api for wordnet searching(jaws).Akira Utsumi.
2000.
Verbal irony as implicit dis-play of ironic environment: Distinguishing ironicutterances from nonirony.
Journal of Pragmatics,32(12):1777?1806.Tony Veale and Yanfen Hao.
2010a.
Detecting ironicintent in creative comparisons.
In ECAI, volume215, pages 765?770.Tony Veale and Yanfen Hao.
2010b.
An ironic fistin a velvet glove: Creative mis-representation in theconstruction of ironic similes.
Minds and Machines,20(4):635?650.Tony Veale.
2004.
The challenge of creative informa-tion retrieval.
In Computational Linguistics and In-telligent Text Processing, pages 457?467.
Springer.Deirdre Wilson and Dan Sperber.
2002.
Relevancetheory.
Handbook of pragmatics.Ian H Witten and Eibe Frank.
2005.
Data Mining:Practical machine learning tools and techniques.Morgan Kaufmann.64
