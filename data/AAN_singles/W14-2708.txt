Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 58?67,Baltimore, Maryland USA, 27 June 2014.c?2014 Association for Computational LinguisticsGenerating Subjective Responses to Opinionated Articles in Social Media:An Agenda-Driven Architecture and a Turing-Like TestTomer CaganSchool of Computer ScienceThe Interdisciplinary CenterHerzeliya, Israelcagan.tomer@idc.ac.ilStefan L. FrankCentre for Language StudiesRadboud UniversityNijmegen, The Netherlandss.frank@let.ru.nlReut TsarfatyMathematics and Computer ScienceWeizmann Institute of ScienceRehovot, Israeltsarfaty@weizmann.ac.ilAbstractNatural language traffic in social media(blogs, microblogs, talkbacks) enjoys vastmonitoring and analysis efforts.
How-ever, the question whether computer sys-tems can generate such content in orderto effectively interact with humans hasbeen only sparsely attended to.
This pa-per presents an architecture for generat-ing subjective responses to opinionatedarticles based on users?
agenda, docu-ments?
topics, sentiments and a knowledgegraph.
We present an empirical evalua-tion method for quantifying the human-likeness and relevance of the generated re-sponses.
We show that responses gen-erated using world knowledge in the in-put are regarded as more human-like thanthose that rely on topic, sentiment andagenda only, whereas the use of worldknowledge does not affect perceived rel-evance.1 IntroductionDigital media, user-generated content and socialnetworks enable effective human interaction; somuch so that much of our day-to-day interactionis conducted online (Viswanath et al., 2009).
In-teraction in social media fundamentally changesthe way businesses and consumers behave (Qual-man, 2012), can be instrumental to the successof individuals and businesses (Haenlein and Ka-plan, 2009), and even affects the stability of polit-ical regimes (Howard et al., 2011; Lamer, 2012).These facts force organizations (businesses, gov-ernments, and non-profit organizations) to be con-stantly involved in the monitoring of, and the inter-action with, human agents in digital environments(Langheinrich and Karjoth, 2011).Automatic analysis of user-generated onlinecontent benefits from extensive research and com-mercial opportunities.
In natural language pro-cessing, there is ample research on the analysisof subjectivity and sentiment of content in socialmedia.
The development of tools for sentimentanalysis (Davidov et al., 2010), mood aggregation(Agichtein et al., 2008), opinion mining (Mishne,2006), and many more, now enjoys wide inter-est and exposure, as is also evident by the manyworkshops and dedicated tracks at ACL venues.1Methods are also developed for the analysis of po-litical texts (O?Connor et al., 2010; O?Connor etal., 2013) and for text-driven forecasting based onthese data (Yano et al., 2009).
A related strandof research uses computational methods to findout what kind of published utterances are influ-ential, and how they affect linguistic communi-ties (Danescu-Niculescu-Mizil et al., 2009).
Suchwork complements, and contributes to, studiesfrom sociology and sociolinguistics that aim to de-lineate the process of generating meaningful re-sponses (e.g., Amabile (1981)).In contrast to these analysis efforts, the topicof generating responses to content in social me-dia is only sparsely explored.
Commercially, thereis movement towards online response automation(Owyang, 2012; Mah, 2012).2Research on userinterfaces is trying to move away from script-based interaction towards the development of chatbots that attempt natural human-like interaction(Mori et al., 2003; Feng et al., 2006).
However,these chat bots are typically designed to providean automated one-size-fits-all type of interaction.A study by Ritter et al.
(2011) addressesthe generation of responses to natural languagetweets in a data-driven setup.
It applies amachine-translation approach to response gener-ation, where moods and sentiments already ex-1E.g., the ACL series LASM http://tinyurl.com/ludyrkz; WASSA http://tinyurl.com/kjjdhax.2There is a general debate on the efficiency of automatedtools (Nall, 2013) and whether such tools are desirable in so-cial media (McConnell (2012); responses to Owyang (2012)).58pressed in the past are replicated or reused.
A re-cent study by Hasegawa et al.
(2013) modifies Rit-ter?s approach to produce responses that elicit anemotion from the addressee.
Yet, these responsesdo not target particular topics and are not drivenby a user agenda.The present paper addresses the problem ofgenerating novel, subjective, responses to on-line opinionated articles.
We formally define thedocument-to-response mapping problem and sug-gest an end-to-end system to solve it.
Our sys-tem integrates a range of NLP and NLG technolo-gies (including topic models, sentiment analysis,and the integration of a knowledge graph) to de-sign a flexible generation mechanism that allowsus to vary the information in the input to the gen-eration procedure.
We then use a Turing-inspiredtest to study the different factors that contribute tothe perceived human-likeness and relevance of thegenerated responses, and show how the perceptionof responses depends on external knowledge andthe expressed sentiment.The remainder of this paper is organized as fol-lows.
The next section presents our proposal: Sec-tion 2.1 describes our approach, Section 2.2 for-malizes the proposal, and Section 2.3 presents ourend-to-end architecture.
This is followed by ourevaluation method and empirical results in Sec-tion 3.
We discuss related and future work in Sec-tion 4, and in Section 5 we conclude.2 The Proposal: Generating SubjectiveResponses2.1 Our ApproachNatural language is, above all, a communicativedevice that we employ to achieve certain goals.In social media, the driving force behind generat-ing responses is a responder?s disposition towardssome topic.
This topic could be a political cam-paign or a candidate, a product, or some abstractidea, which the responder has a motive to promote.Let us call this goal our user?s agenda.User response generation, like any other natu-ral language utterance generation, is triggered bya certain event that is related to the communica-tive goal.
In a social media setting, this eventis often a new online document.
The documentand the agenda thus form the input to our gener-ation system.
Each document and each agendacontain (possibly many) topics, each of which isassociated with a (positive or negative) sentiment.Document sentiments are attributed to the author,whereas agenda sentiments are attributed to theuser (henceforth: the responder).For each non-empty intersection of the topicsin the document and in the agenda, our response-generation system aims to generate utterances thatare fluent, human-like, and effectively engagereaders.
The generation is based on three assump-tions, roughly reflecting the Gricean maxims ofcooperative interaction (Grice, 1967).
Online userresponses should then be:?
Economic (Maxim of Quantity): Responsesare brief and concise;?
Relevant (Maxim of Relation): Responses di-rectly address the documents?
content.?
Opinionated (Maxim of Quality): Responsesexpress responders beliefs, sentiments, ordispositions towards the topic(s).2.2 The Formal ModelLet D be a set of documents and let A be a setof user agendas as we define shortly.
Let S be aset of English sentences over a finite vocabularyS = ??.
Our system implements a function thatmaps each ?document, agenda?
pair to a naturallanguage response sentence s ?
S.fresponse: D ?A?
SResponse generation takes place in two phases,roughly corresponding to macro and micro plan-ning in Reiter and Dale (1997):?
Macro Planning (below, the analysis phase):What are we going to talk about??
Micro Planning (below, the generationphase): How are we going to say it?The analysis function p : D ?
C maps a docu-ment to a subjective representation of its content.3The generation function g : C ?
A ?
S inter-sects the content elements in the document and inthe user agenda, and generates a response basedon the content of the intersection.
All in all, oursystem implements a composition of the analysisand the generation functions:fresponse(d, a) = g(p(d), a) = s3A content element may conceivably encompass a topic,its sentiment, its objectivity, its evidentiality, its perceivedtruthfulness, and so on.
In this paper we focus on topic andsentiment, and leave the rest for future research.59Each content element c ?
C or an agenda itema ?
A is composed of a topic t associated with asentiment value sentimentt?
[?n..n] that sig-nifies the (negative or positive) disposition of thedocument?s author (if c ?
C) or the user?s agenda(if a ?
A) towards the topic.
We assume here thata topic is simply a bag of words from our vocabu-lary ?.
Thus, we have the following:A,C ?
P(?)?
[?n..n]Our generation component accepts the result ofthe intersection as input and relies on a template-based grammar and a set of functions for generat-ing referring expressions in order to construct theoutput.
To make the responses economic, we limitthe content of a response to one statement aboutthe document or its author, followed by a state-ment on the relevant topic.
To make the responserelevant, the templates that generate the responsemake use of topics in the intersection of the docu-ment and the agenda.
To make the response opin-ionated, the sentiment of the response depends onthe (mis)match between the sentiment values forthe topic in the document and in the agenda.
Con-cretely, the response is positive if the sentimentsfor the topic in the document and agenda are thesame (both positive or both negative) and it is neg-ative otherwise.We suggest two variants of the generation func-tion g. The basic variant implements the baselinefunction defined above:gbase(c, a) = sc ?
C, a ?
A, s ?
?
?For the other variant we define a knowledgebase (KB) as a directed graph in which wordsw ?
?
from the topic models correspond to nodesin the graph, and relations r ?
R between thewords are predicates that hold in the real world.Our second generation function now becomes:gkb(c, a,KB) = sKB ?
{(wi, r, wj)|wi, wj?
?, r ?
R}with c ?
C, a ?
A, s ?
?
?as defined in gbaseabove.2.3 The ArchitectureThe system architecture from a bird?s eye viewis presented in Figure 1.
In a nutshell, a docu-ment enters the analysis phase, where topic infer-ence and sentiment scoring take place, resultingin ?topic, sentiment?-pairs.
During the subsequentgeneration phase, these are intersected with the?topic, sentiment?-pairs in the user agenda.
Thisintersection, possibly augmented with a knowl-edge graph, forms the input for a template-basedgeneration component.Analysis phase For the task of inferring the top-ics of the document we use topic modeling: aprobabilistic generative modeling technique thatallows for the discovery of abstract topics overa large body of documents (Papadimitriou et al.,1998; Hofmann, 1999; Blei et al., 2003).
Specif-ically, we use topic modeling based on LatentDirichlet Allocation (LDA) (Blei et al., 2003; Blei,2012).
Given a new document and a trainedmodel, the inference method provides a weightedmix of topics for that document, where each topicis represented as a vector containing keywords as-sociated with probabilities.
For training the topicmodel and inferring the topics in new documentswe use Gensim (Rehurek and Sojka, 2010), a fastand easy-to-use implementation of LDA.Next, we wish to infer the sentiment that is ex-pressed in the text with relation to the topic(s)identified in the document.
We use the seman-tic/lexical method as implemented in Kathuria(2012).
We rely on a WSD sentiment classifierthat uses the SentiWordNet (Baccianella et al.,2010) database and calculates the positivity andnegativity scores of a document based on the pos-itivity and negativity of individual words.
The re-sult of the sentiment analysis is a pair of values,indicating the positive and negative sentiments ofthe document-based scores for individual words.We use the larger of these two values as the senti-ment value for the whole document.4Generation phase Our generation function firstintersects the set of topics in the document and theset of topics in the agenda in order to discover rel-evant topics to which the system would generateresponses.
A response may in principle integratecontent from a range of topics in the topic modeldistribution, but, for the sake of generating conciseresponses, in the current implementation we focuson the single most prevalent, topic.
We pick thehighest scoring word of the highest scoring topic,and intersect it with topics in the agenda.
The sys-tem generates a response based on the identified4Clearly, this is a simplifying assumption.
We discuss thisassumption further in Section 4.60Figure 1: The system architecture from a bird?s eye view.
Components on gray background are executedoffline.topic, the sentiment for the topic in the document,and the sentiment for that topic in the user agenda.The generation component relies on a template-based approach similar to Reiter and Dale (1997)and Van Deemter et al.
(2005).
Templates areessentially subtrees with leaves that are place-holders for other templates or for functions gener-ating referring expressions (Theune et al., 2001).These functions receive (relevant parts of) the in-put and emit the sequence of fine-grained part-of-speech (POS) tags that realizes the relevant refer-ring expression.
The POS tags in the resultingsequences are ultimately place holders for wordsfrom a lexicon ?.
In order to generate a variety ofexpression forms ?
nouns, adjectives and verbs?
these items are selected randomly from a fine-grained lexicon we defined.
The sentiment (posi-tive or negative) is expressed in a similar fashionvia templates and randomly selected lexical en-tries for the POS slots, after calculating the over-all sentiment for the intersection as stated above.Our generation implementation is based on Sim-pleNLG (Gatt and Reiter, 2009) which is a surfacerealizer API that allows us to create the desiredtemplates and functions, and aggregates contentinto coherent sentences.
The templates and func-tions that we defined are depicted in Figure 2.In addition, we handcrafted a simple knowledgegraph (termed here KB) containing the words in aset of pre-defined user agendas.
Table 1 shows asnippet of the constructed knowledge graph.
Theknowledge graph can be used to expand the re-sponse in the following fashion: The topic of theresponse is a node in the KB.
We randomly se-lect one of its outgoing edges for creating a relatedSource Relation TargetApple CompetesWith SamsungApple CompetesWith GoogleApple Creates iOSTable 1: A knowledge graph snippet.statement that has the target node of this relationas its subject.
The related sentence generation usesthe same template-based mechanism as before.
Inprinciple, this process may be repeated any num-ber of times and express larger parts of the KB.Here we only add one single knowledge-base re-lation per response, to keep the responses concise.3 EvaluationWe set out to evaluate how computer-generated re-sponses compare to human responses in their per-ceived human-likeness and relevance.
More inparticular, we compare different system variantsin order to investigate what makes responses seemmore human-like or relevant.3.1 MaterialsOur empirical evaluation is restricted to topics re-lated to mobile telephones, specifically Apple?siPhone and devices based on the Android operat-ing system.
We collected 300 articles from lead-ing technology sites in the domain to train thetopic models on, settling on 10 topics models.Next, we generated a set of user agendas refer-ring to the same 10 topics.
Each agenda is rep-resented by a single keyword from a topic modeldistribution and a sentiment value sentimentt?
{?8,?4, 0, 4, 8}.
Finally, we selected 10 new ar-ticles from similar sites and generated a pool of61SresponseNPIVPV?beliefSBARthat (Sresponse)SresponseSarticleSitem(Srelation)SitemNP?itemRefVP?sentiiSarticleNP?articleRefVP?sentiaSrelationNP?relationRefVP?sentirarticleRef ?
ExpressArticle(...)itemRef ?
ExpressItem(...)relationRef ?
ExpressRelation(...)sentimenta?
ExpressArticleSentiment(...)sentimenti?
ExpressItemSentiment(...)sentimentr?
ExpressRelationSentiment(...)belief ?
ExpressBelief(...)Figure 2: Template-based response generation.
The templates are on the left.
The Express* functions onthe right uses regular expressions over the arguments and vocabulary items from a closed lexicon.1000 responses for each, comprising 100 uniqueresponses for each combination of sentimenttand system variant (i.e., with or without a knowl-edge base).
Table 2 presents an example responsefor each such combination.
In addition, we ran-domly collected 5 to 10 real, short or medium-length, online human responses for each article.3.2 SurveysWe collected evaluation data via two onlinesurveys on Amazon Mechanical Turk (www.mturk.com).
In Survey 1, participants judgedwhether responses to articles were written by hu-man or computer, akin to (a simplified version of)the Turing test (Turing, 1950).
In Survey 2, re-sponses were rated on their relevance to the ar-ticle, in effect testing whether they abide by theGricean Maxim of Relation.
This is comparableto the study by Ritter et al.
(2011) where peoplejudged which of two responses was ?best?.Each survey comprises 10 randomly ordered tri-als, corresponding to the 10 selected articles.
First,the participant was presented with a snippet fromthe article.
When clicking a button, the text wasremoved and its presentation duration recorded.Next, a multiple-choice question asked about thesnippet?s topic.
Data on a trial was discarded fromanalysis if the participant answered incorrectly orif the snippet was presented for less than 10 msecper character; we took these to be cases where thesnippet was not properly read.
Next, the partic-ipant was shown a randomly ordered list of re-sponses to the article.In Survey 1, four responses were presented foreach article: three randomly selected from thepool of human responses to that article and onegenerated by our system.
The task was to cate-gorize each response on a 7-point scale with la-bels ?Certainly human/computer?, ?Probably hu-man/computer?, ?Maybe human/computer?
and?Unsure?.
In Survey 2, five responses were pre-sented: three human responses and two computer-generated.
The task was to rate the responses?relevance on a 7-point scale labeled ?Completely(not) relevant?, ?Mostly (not) relevant?, ?Some-what (not) relevant?, and ?Unsure?.
As a con-trol condition, one of the human responses andone of the computer responses were actually takenfrom another article than the one just presented.In both surveys, the computer-generated responsespresented to each participant were balanced acrosssentiment levels and generation functions (gbaseand gkb).
After completing the 10 trials, partic-ipants provided basic demographic information,including native language.
Data from non-nativeEnglish speakers was discarded.
Surveys 1 and 2were completed by 62 and 60 native speakers, re-spectively.3.3 Analysis and ResultsSurvey 1: Computer-Likeness Rating.
Table 3shows the mean ?computer-likeness?-ratings from1 (?Certainly human?)
to 7 (?Certainly computer?
)for each response category.
Clearly, the humanresponses are rated as more human-like than thecomputer-generated ones: our model did not gen-erally mislead the participants.
This may be dueto the template-based response structure: over thecourse of the survey, human raters are likely tonotice this structure and infer that such responsesare computer-generated.
To investigate whethersuch learning indeed occurs, a linear mixed-effects model was fitted, with predictor variablesIS COMP (+1:computer-generated, ?1:human re-sponses), POS (position of the trial in the survey, 0to 9), and the interaction between the two.
Table 462Sent.
KB Response?8No Android is horrendous so I think that the writer is completely correct!!
!Yes Apple is horrendous so I feel that the author is not really right!!!
iOS is horrendous as well.
?4No I think that the writer is mistaken because apple actually is unexceptional.Yes I think that the author is wrong because Nokia is mediocre.
Apple on the other hand is pretty good ...0No The text is accurate.
Apple is okay.Yes Galaxy is okay so I think that the content is accurate.
All-in-all samsung makes fantastic gadgets.4No Android is pretty good so I feel that the author is right.Yes Nokia is nice.
The article is precise.
Samsung on the other hand is fabulous...8No Galaxy is great!!!
The text is completely precise.Yes Galaxy is awesome!!!
The author is not completely correct.
In fact I think that samsung makesawesome products.Table 2: Responses generated by the system with or without a knowledge-base (KB), with differentsentiment levels.Response Type Mean and CIHuman 3.33 ?
0.08Computer (all) 4.49 ?
0.15Computer (?KB) 4.66 ?
0.20Computer (+KB) 4.32 ?
0.22Table 3: Mean and 95% confidence interval ofcomputer-likeness rating per response category.
?KB indicates whether gbaseor gkbwas used.Factor b t P (b < 0)(intercept) 3.590IS COMP 0.193 2.11 0.015POS 0.069 4.76 0.000IS COMP ?
POS 0.085 6.27 0.000Table 4: Computer-likeness rating regression re-sults, comparing human to computer responses.presents, for each factor in the regression analysis,the coefficient b and its t-statistic.
The coefficientequals the increase in computer-likeness rating foreach unit increase in the predictor variable.
The t-statistic is indicative of how much variance in theratings is accounted for by the predictor.
We alsoobtained a probability distribution over each co-efficient by Markov Chain Monte Carlo samplingusing the R package lme4 version 0.99 (Bates,2005).
From each coefficient?s distribution, we es-timate the posterior probability that b is negative,which quantifies the reliability of the effect.The positive b value for POS shows that re-sponses drift towards the ?computer?-end of thescale.
More importantly, a positive interactionwith IS COMP indicates that the difference be-tween human and computer responses becomesmore noticeable as the survey progresses ?the participants did learn to identify computer-generated responses.
However, the positive coef-ficient for IS COMP means that even at the veryfirst trial, computer responses are considered to bemore computer-like than human responses.Factors Affecting Human-Likeness.
Our find-ing that the identifiability of computer-generatedresponses cannot be fully attributed to their repet-itiveness, raises the question: What makes a sucha response more human-like?
The results provideseveral insights into this matter.First, the mean scores in Table 3 suggest that in-cluding a knowledge base increases the responses?human-likeness.
To further investigate this, weperformed a separate regression analysis, usingonly the data on computer-generated responses.This analysis also included predictors KB (+1:knowledge base included, ?1: otherwise), SENT(sentimentt, from ?8 to +8), absolute value ofSENT, and the interaction between KB and POS.As can be seen in Table 5, there is no reliable in-teraction between KB and POS: the effect of in-cluding the KB on the human-likeness of responsesremained constant over the course of the survey.Furthermore, we see evidence that responseswith a more positive sentiment are consideredmore computer-like.
The (only weakly reliable)negative effect of the absolute value of senti-ment suggests that more extreme sentiments areconsidered more human-like.
Apparently, peoplecount on computer responses to be mildly positive,whereas human responses are expected to be moreextreme, and extremely negative in particular.Survey 2: Relevance Rating.
The mean rele-vance scores in Table 6 reveal that a response israted as more relevant to a snippet if it was actu-ally a response to that snippet, rather than to a dif-ferent snippet.
This reinforces our design choice63Factor b t P (b < 0)(intercept) 4.022KB ?0.240 ?2.13 0.987POS 0.144 5.82 0.000SENT 0.035 2.98 0.002abs(SENT) ?0.041 ?1.97 0.967KB ?
POS 0.023 1.03 0.121Table 5: Computer-likeness rating regression re-sults, comparing systems with and without KB.Response Type Source Mean and CIHumanthis 4.85 ?
0.11other 3.56 ?
0.18Computer (all)this 4.52 ?
0.16other 2.52 ?
0.15Computer (?KB)this 4.53 ?
0.23other 2.46 ?
0.21Computer (+KB)this 4.51 ?
0.23other 2.58 ?
0.22Table 6: Mean and 95% confidence interval ofrelevance rating per response category.
?Source?indicates whether the response is from the pre-sented text snippet or a random other snippet.
?KB indicates whether gbaseor gkbwas used.Factor b t P (b < 0)(intercept) 3.861IS COMP ?0.339 ?7.10 1.000SOURCE 0.824 16.80 0.000IS COMP ?
PRES 0.179 5.03 0.000Table 7: Relevance ratings regression results,comparing human to computer responses.Factor b t P (b < 0)(intercept) 3.603KB 0.026 0.49 0.322SOURCE 1.003 15.90 0.000SENT 0.023 1.94 0.029abs(SENT) ?0.017 ?0.93 0.819KB ?
SOURCE ?0.032 ?0.61 0.731Table 8: Relevance ratings regression results,comparing systems with and without KB.to include input items referring specifically to thetopic and sentiment of the author.
However, hu-man responses are considered more relevant thanthe computer-generated ones.
This is confirmedby a reliably negative regression coefficient forIS COMP (see regression results in Table 7).The analysis included the binary factor SOURCE(+1 if the response came from the presented snip-pet, ?1 if it came from a random article).
Wesee a positive interaction between SOURCE andIS COMP, indicating that presenting a responsefrom a random article is more detrimental to rel-evance of computer-generated responses than thatof the human responses.
This is not surprising, asthe computer-generated responses (unlike the hu-man responses) always includes the article?s topic.When analyzing only data on computer-generated responses, and including predictors foragenda sentiment and for presence of the knowl-edge base, we see that including the KB does notaffect response relevance (see Table 8).
Also, thereis no interaction between KB and SOURCE, thatis, the effect of presenting a response from a dif-ferent article does not differ between the modelswith and without the knowledge base.
Possibly,responses are considered as more relevant if theyhave more positive sentiment, but the evidence forthis is fairly weak.4 Related and Future WorkIn contrast to the vast amount of research on sen-timent and topic analysis, as well as generationtasks in which the input is artificial or pre-defined,our system implements a full end-to-end cyclefrom natural language analysis to natural languagegeneration with applications in social media andautomated interaction in real-world settings.The only two other studies on response gener-ation in social media we know of are Ritter et al.
(2011) and Hasegawa et al.
(2013).
Ritter?s andHasegawa?s approaches differ from ours in theirobjective and their approach to generation.
Specif-ically, Ritter?s approach is based on machine trans-lation, creating responses by directly re-using pre-vious content.
Their data-driven approach gener-ates relevant, but not opinionated responses.
Inaddition, both Ritter?s and Hasegawa?s systems re-spond to tweets, while our system analyzes and re-sponds to complete articles.
Hasegawa?s approachis closer to ours in that it generates responses thatare intended to elicit a specific emotion from theaddressee.
However, it still differs considerably insettings (dialogues versus online posting) and inthe goal itself (eliciting emotion versus expressingopinion).
Thus, we see these studies as comple-mentary to ours in the realm of response genera-tion in social media.64A natural contact point of our work with exist-ing work in social media analysis is the investiga-tion of how a change in the implementation of in-dividual components (e.g., topic inference or sen-timent scoring) would affect the result of the over-all generation.
In particular, it would be interestingto test whether a novel mechanism for joint infer-ence of topic/sentiment distributions could lead toimprovement in the human-likeness of the gener-ated responses.The syntactic and semantic means of expres-sion that we use are based on bare bone templatesand fine-grained POS tags (Theune et al., 2001).These may potentially be expanded with differentways to express subject/object relations, relationsbetween phrases, polarity of sentences, and so on.Additional approaches to generation can factor insuch aspects, e.g., the template-based methods inBecker (2002) and Narayan et al.
(2011), or gram-mar based methods, as in DeVault et al.
(2008).Using more sophisticated generation methods witha rich grammatical backbone may combat the sen-sitivity to computer-generated response patterns asacquired by our human raters over time.Furthermore, our result concerning the human-likeness of gkbclearly demonstrates that semanticknowledge must be brought in to support better,and more human-like, response generation.
Large-scale knowledge graphs such as Freebase supportmany semantic tasks (Jacobs, 1985), and can beused for providing richer context for automaticallygenerating human-like responses.From a theoretical viewpoint, the system willclearly benefit from rigorous analysis of humaninteraction in online media.
Responses to user-generated content on the Internet share somelinguistic characteristics in structure, length andmanner of expression.
Studying these features the-oretically and then examining them empiricallyusing a Turing-like evaluation as presented herecan take us a big step in the direction of better gen-eration, and also better understanding of the pro-cesses underlying human response generation.This latter understanding may be complementedwith insights into the causes, motivations and in-tricacies of human interaction in such environ-ments, as studied by sociologists and psychol-ogists.
In particular, our preliminary interac-tion with colleagues from communication stud-ies suggests that the present endeavor nicely com-plements that of ?persuasive computing?
(Fogg,1998; Fogg, 2002), and we hope that this collabo-ration will lead to valuable synergies.Finally, bridging the gap between the technicaland the theoretical, it would be fascinating to testthe responses in the context for which they aregenerated ?
social media.
Generated texts maybe posted as a response to the original article, orshared with a link of the original article, followedby measuring the responses to, and shares of, thatresponse.
Such real-world evaluation could indi-cate that generated responses are indeed believableand engaging, and may better simulate a Turing-like test in which machine-generated responsescannot be distinguished from human responses.5 ConclusionWe presented a system for generating responsesthat are directly tied to responders?
agendas anddocument content.
To the best of our knowledge,this is the first system to generate subjective re-sponses directly reflecting users?
agendas.
Our re-sponse generation architecture provides an easy-to-use and easy-to-extend solution encompassinga range of NLP and NLG techniques.
We evalu-ated both the human-likeness and the relevance ofthe generated content, thereby empirically quan-tifying the efficacy of computer-generated re-sponses compared head-to-head against human re-sponses.Generating concise, relevant, and opinionatedresponses that are also human-like is hard ?
itrequires the integration of text-understanding andsentiment analysis, and it is also contingent on theexpression of the agents?
prior knowledge, reasonsand motives.
We suggest our architecture and eval-uation method as a baseline for future researchon generated content that would effectively passa Turing-like test, and successfully convince hu-mans of the authenticity of generated responses.5AcknowledgmentsWe thank Yoav Francis for his contribution in theearly stages of this research.
We further thankour anonymous reviewers for their insightful com-ments on an earlier draft.5Our code, training data, experimental data (computer andhuman responses) and analysis scripts are publicly availablevia www.tsarfaty.com/nlg-sd/.65ReferencesEugene Agichtein, Carlos Castillo, Debora Donato,Aristides Gionis, and Gilad Mishne.
2008.
Findinghigh-quality content in social media.
In Proceed-ings of the international conference on Web searchand web data mining, pages 183?194.
ACM.Teresa M. Amabile.
1981.
Brilliant but Cruel: Per-ceptions of Negative Evaluators.
Washington, DC:ERIC Clearinghouse.Stefano Baccianella, Andrea Esuli, and Fabrizio Se-bastiani.
2010.
SentiWordNet 3.0: An enhancedlexical resource for sentiment analysis and opinionmining.
In Proceedings of the Seventh InternationalConference on Language Resources and Evaluation(LREC?10), Valletta, Malta.
European Language Re-sources Association (ELRA).Douglas M. Bates.
2005.
Fitting linear mixed modelsin R. R News, 5:27?30.Tilman Becker.
2002.
Practical, template-based natu-ral language generation with TAG.
In Proceedingsof the 6th International Workshop on Tree AdjoiningGrammars and Related Frameworks (TAG+6).David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet Allocation.
J. Mach.
Learn.Res., 3:993?1022.David M. Blei.
2012.
Probabilistic topic models.Commun.
ACM, 55(4):77?84.Cristian Danescu-Niculescu-Mizil, Gueorgi Kossinets,Jon Kleinberg, and Lillian Lee.
2009.
How opinionsare received by online communities: A case studyon amazon.com helpfulness votes.
In Proceedingsof the 18th International Conference on World WideWeb, WWW ?09, pages 141?150, New York, NY,USA.
ACM.Dmitry Davidov, Oren Tsur, and Ari Rappoport.
2010.Enhanced sentiment learning using Twitter hashtagsand smileys.
In Proceedings of the 23rd Inter-national Conference on Computational Linguistics,pages 241?249, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.David DeVault, David Traum, and Ron Artstein.
2008.Practical grammar-based NLG from examples.
InProceedings of the Fifth International Natural Lan-guage Generation Conference, INLG ?08, pages 77?85, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Donghui Feng, Erin Shaw, Jihie Kim, and EduardHovy.
2006.
An intelligent discussion-bot foranswering student queries in threaded discussions.In Proceedings of Intelligent User Interface (IUI-2006), pages 171?177.B.
J. Fogg.
1998.
Persuasive computers: Perspec-tives and research directions.
In Proceedings of theSIGCHI Conference on Human Factors in Comput-ing Systems, CHI ?98, pages 225?232, New York,NY, USA.
ACM Press/Addison-Wesley PublishingCo.B.
J. Fogg.
2002.
Persuasive technology: Using com-puters to change what we think and do.
Ubiquity,December.Albert Gatt and Ehud Reiter.
2009.
SimpleNLG: Arealisation engine for practical applications.
In Pro-ceedings of the 12th European Workshop on Natu-ral Language Generation, ENLG ?09, pages 90?93,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.H.
P. Grice.
1967.
Logic and conversation.
In H. P.Grice, editor, Studies in the ways of words, pages22?40.
Harvard University Press.Michael Haenlein and Andreas M. Kaplan.
2009.Flagship brand stores within virtual worlds: The im-pact of virtual store exposure on real-life attitudetoward the brand and purchase intent.
Rechercheet Applications en Marketing (English Edition),24(3):57?79.Takayuki Hasegawa, Nobuhiro Kaji, Naoki Yoshinaga,and Masashi Toyoda.
2013.
Predicting and elicitingaddressee?s emotion in online dialogue.
In Proceed-ings of the 51st Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Pa-pers), pages 964?972, Sofia, Bulgaria, August.
As-sociation for Computational Linguistics.Thomas Hofmann.
1999.
Probabilistic Latent Seman-tic Indexing.
In Proceedings of the 22nd AnnualInternational ACM SIGIR Conference on Researchand Development in Information Retrieval, SIGIR?99, pages 50?57, New York, NY, USA.
ACM.Philip N. Howard, Aiden Duffy, Deen Freelon, Muza-mmil Hussain, Will Mari, and Marwa Mazaid.2011.
Opening closed regimes: What was the roleof social media during the Arab spring?
Project onInformation Technology and Political Islam.Paul S Jacobs.
1985.
A knowledge-based approach tolanguage production.
Technical report, Universityof California at Berkeley, Berkeley, CA, USA.Pulkit Kathuria.
2012.
Sentiment Clas-sification using WSD, Maximum En-tropy and Naive Bayes Classifiers.https://github.com/kevincobain2000/sentiment classifier.Visited March 2014.Wiebke Lamer.
2012.
Twitter and tyrants: New me-dia and its effects on sovereignty in the Middle East.Arab Media and Society.Marc Langheinrich and G?unter Karjoth.
2011.
Socialnetworking and the risk to companies and institu-tions.
Information Security Technical Report.
Spe-cial Issue: Identity Reconstruction and Theft, pages51?56.66Paul Mah.
2012.
Tools to automate yourcustomer service response on social me-dia.
http://www.itbusinessedge.com/blogs/smb-tech/tools-to-automate-your-customer-service-response-on-social-media.html.
Visited August2013.Chris McConnell.
2012.
When brands auto-mate Twitter and Facebook responses I?ll re-volt.
http://dailytekk.com/2012/06/07/brands-automating-social-media/.
Visited August 2013.Gilad Mishne.
2006.
Multiple ranking strategies foropinion retrieval in blogs.
In Proceedings of the 15thText Retrieval Conference.Kyoshi Mori, Adam Jatowt, and Mitsuru Ishizuka.2003.
Enhancing conversational flexibility in multi-modal interactions with embodied lifelike agent.
InProceedings of the 8th International Conference onIntelligent User Interfaces, IUI ?03, pages 270?272,New York, NY, USA.
ACM.Mickey Nall.
2013.
You can?t automate so-cial media engagement, argues PRSA?s MickeyNall.
http://www.prmoment.com/1359/you-cant-automate-social-media-engagement-argues-prsas-mickey-nall.aspx.
Visited August 2013.Karthik Sankaran Narayan, Charles Lee Isbell Jr., andDavid L. Roberts.
2011.
Dextor: Reduced effortauthoring for template-based natural language gen-eration.
In Vadim Bulitko and Mark O. Riedl, ed-itors, Proceedings of the Seventh Artificial Intelli-gence and Interactive Digital Entertainment Confer-ence.
The AAAI Press.Brendan O?Connor, Ramnath Balasubramanyan,Bryan R. Routledge, and Noah A. Smith.
2010.From tweets to polls: Linking text sentiment topublic opinion time series.
In William W. Cohenand Samuel Gosling, editors, ICWSM.
The AAAIPress.Brendan O?Connor, Brandon M. Stewart, and Noah A.Smith.
2013.
Learning to extract international rela-tions from political context.
In Proceedings of the51st Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages1094?1104.
The Association for Computer Linguis-tics.Jeremiah Owyang.
2012.
Brands Start AutomatingSocial Media Responses on Facebook and Twitter.http://techcrunch.com/2012/06/07/brands-start-automating-social-media-responses-on-facebook-and-twitter/.
Visited August 2013.Christos H. Papadimitriou, Hisao Tamaki, PrabhakarRaghavan, and Santosh Vempala.
1998.
La-tent Semantic Indexing: A probabilistic analy-sis.
In Proceedings of the Seventeenth ACMSIGACT-SIGMOD-SIGART Symposium on Princi-ples of Database Systems, PODS ?98, pages 159?168, New York, NY, USA.
ACM.Erik Qualman.
2012.
Socialnomics: How social mediatransforms the way we live and do business.
JohnWiley & Sons, Hoboken, NJ, USA, 2nd edition.Radim Rehurek and Petr Sojka.
2010.
Software frame-work for topic modelling with large corpora.
In Pro-ceedings of the LREC 2010 Workshop on New Chal-lenges for NLP Frameworks, pages 45?50, Valletta,Malta, May.
ELRA.Ehud Reiter and Robert Dale.
1997.
Building appliednatural language generation systems.
Nat.
Lang.Eng., 3(1):57?87.Alan Ritter, Colin Cherry, and William B. Dolan.
2011.Data-driven response generation in social media.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing, EMNLP ?11,pages 583?593, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.M.
Theune, E. Klabbers, J. R. De Pijper, E. Krahmer,and J. Odijk.
2001.
From data to speech: A generalapproach.
Nat.
Lang.
Eng., 7(1):47?86.Alan M. Turing.
1950.
Computing machinery and in-telligence.
Mind, LIX:433?460.Kees Van Deemter, Emiel Krahmer, and Mari?et The-une.
2005.
Real versus template-based natural lan-guage generation: A false opposition?
Comput.
Lin-guist., 31(1):15?24.Bimal Viswanath, Alan Mislove, Meeyoung Cha, andKrishna P. Gummadi.
2009.
On the evolution ofuser interaction in Facebook.
In Proceedings ofthe 2nd ACM Workshop on Online Social Networks,WOSN ?09, pages 37?42, New York, NY, USA.ACM.Tae Yano, William W. Cohen, and Noah A. Smith.2009.
Predicting response to political blog postswith topic models.
In Proceedings of Human Lan-guage Technologies: The 2009 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics, NAACL ?09, pages477?485, Stroudsburg, PA, USA.
Association forComputational Linguistics.67
