LAW VIII - The 8th Linguistic Annotation Workshop, pages 54?58,Dublin, Ireland, August 23-24 2014.Optimizing annotation efforts to build reliable annotated corporafor training statistical modelsCyril Grouin1Thomas Lavergne1,2Aur?elie N?ev?eol11LIMSI?CNRS, 91405 Orsay, France2Universit?e Paris Sud 11, 91400 Orsay, Francefirstname.lastname@limsi.frAbstractCreating high-quality manual annotations on text corpus is time-consuming and often requires thework of experts.
In order to explore methods for optimizing annotation efforts, we study three keytime burdens of the annotation process: (i) multiple annotations, (ii) consensus annotations, and(iii) careful annotations.
Through a series of experiments using a corpus of clinical documentsannotated for personally identifiable information written in French, we address each of theseaspects and draw conclusions on how to make the most of an annotation effort.1 IntroductionStatistical and Machine Learning methods have become prevalent in Natural Language Processing (NLP)over the past decades.
These methods sucessfully address NLP tasks such as part-of-speech tagging ornamed entity recognition by relying on large annotated text corpora.
As a result, developping high-quality annotated corpora representing natural language phenomena that can be processed by statisticaltools has become a major challenge for the scientific community.
Several aspects of the annotation taskhave been studied in order to ensure corpus quality and affordable cost.
Inter-annotator agreement (IAA)has been used as an indicator of annotation quality.
Early work showed that the use of automatic pre-annotation tools improved annotation consistency (Marcus et al., 1993).
Careful and detailed annotationguideline definition was also shown to have positive impact on IAA (Wilbur et al., 2006).Efforts have investigated methods to reduce the human workload while annotating corpora.
In par-ticular, active learning (Settles et al., 2008) sucessfully selects portions of corpora that yield the mostbenefit when annotated.
Alternatively, (Dligach and Palmer, 2011) investigated the need for double an-notation and found that double annotation could be limited to carefully selected portions of a corpus.They produced an algorithm that automatically selects portions of a corpus for double annotation.
Theirapproach allowed to reduce the amount of work by limiting the portion of doubly annotated data andmaintained annotation quality to the standard of a fully doubly annotated corpus.
The use of automaticpre-annotations was shown to increase annotation consistency and result in producing quality annotationwith a time gain over annotating raw data (Fort and Sagot, 2010; N?ev?eol et al., 2011; Rosset et al., 2013).With the increasing use of crowdsourcing for obtaining annotated data, (Fort et al., 2011) show that thereare ethic aspects to consider in addition to technical and monetary cost when using a microworking plat-form for annotation.
While selecting the adequate methods for computing IAA is important (Artsteinand Poesio, 2008) for interpreting the IAA for a particular task, annotator disagreement is inherent toall annotation tasks.
To address this situation (Rzhetsky et al., 2009) designed a method to estimateannotation confidence based on annotator modeling.
Overall, past work shows that creating high-qualitymanual annotations is time-consuming and often requires the work of experts.
The time burden is dis-tributed between the sheer creation of the annotations, the act of producing multiple annotations for thesame data and the subsequent analysis of multiple annotations to resolve conflicts, viz.
the creation of aconsensus.
Research has addressed methods for reducing the time burden associated to these annotationThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/54activities (for example, adequate annotation tools such as automatic pre-annotations can reduce the timeburden of annotation creation) with the final goal of producing the highest quality of annotations.In contrast, our hypothesis in this work is that annotations are being developed for the purpose of train-ing a machine learning model.
Therefore, our experiments consist in training a named entity recognizeron a training set comprising annotations of varying quality to study the impact of training annotationquality on model performance.
In order to explore methods for optimizing annotation efforts for the de-velopment of training corpora, we revisit the three key time burdens of the annotation process on textualcorpora: (i) careful annotations, (ii) multiple annotations, and (iii) consensus annotations.
Through aseries of experiments using a corpus of French clinical documents annotated for personally identifiableinformation (PHI), we address each of these aspects and draw conclusions on how to make the most ofan annotation effort.2 Material and methods2.1 Annotated corpusExperiments were conducted with a corpus of clinical documents in French annotated for 10 categories ofPHI.
The distribution of the categories over the corpus varies with some categories being more prevalentthan others.
In addition, the performance of entity recognition for each type of PHI also varies (Grouinand N?ev?eol, 2014).
The datasets were split to obtain a training corpus (200 documents) and a testcorpus (100 documents).
For all documents in the training corpus, three types of human annotationsare available: annotations performed independently by two human annotators and consensus annotationsobtained after adjudication to resolve conflicts between the two annotators.
Inter-annotator agreementon the training corpus was above 85% F-measure, which is considered high (Artstein and Poesio, 2008).The distribution of annotations over all PHI categories on both corpora (train/dev) is: address(188/100), zip code (197/97), date (1025/498), e-mail (119/57), hospital (448/208), identifier (135/76),last name (1855/855), first name (1568/724), telephone (802/386) and city (450/217).2.2 Automatic Annotation MethodsWe directly applied the MEDINA rule-based de-identification tool (Grouin, 2013) to obtain baselineautomatic annotations.
We used the CRF toolkit Wapiti (Lavergne et al., 2010) to train a series of modelson the various sets of annotations available for the training corpus.Features set For each CRF experiment, we used the following set of features with a l1 regularization:?
Lexical features: unigram and bigram of tokens;?
Morphological features: (i) the token case (all in upper/lower case, combination of both), (ii) thetoken is a digit, (iii) the token is a punctuation mark, (iv) the token belongs to a specific list (firstname, last name, city), (v) the token was not identified in a dictionary of inflected forms, (vi) thetoken is a trigger word for specific categories (hospital, last name) ;?
Syntactic features: (i) the part-of-speech (POS) tag of the token, as provided by the Tree Taggertool (Schmid, 1994), (ii) the syntactic chunk the token belongs to, from a home made chunkerbased upon the previouses POS tags;?
External features: (i) we created 320 classes of tokens using Liang?s implementation (Liang, 2005)of the Brown clustering algorithm (Brown et al., 1992), (ii) the position of the token within thedocument (begining, middle, end).Design of experiments The models were built to assess three annotation time-saving strategies:1.
Careful annotation: (i) AR=based on automatic annotations from the rule-based system,(ii) AR?H2=intersection of automatic annotations from the rule-based system with annotationsfrom annotator 2.
This model captures a situation where the human annotator would quickly revisethe automatic annotations by removing errors: some annotations would be missing (average recall),but the annotations present in the set would be correct (very high precision), (iii) ARH2=automaticannotations from the rule-based system, with replacement of the three most difficult categories by55annotations from annotator 2.
This model captures a situation where the human annotator wouldfocus on revising targeted categories, and (iv) ARHC=automatic annotations from the rule-basedsystem, with replacement of the three most difficult categories by consensus annotations;2.
Double annotation: (i) H1=annotations from annotator 1, (ii) H2=annotations from annotator 2,(iii) H12=first half of the annotations from annotator 1, second half from annotator 2, and(iv) H21=first half of the annotations from annotator 2, second half from annotator 1;3.
Consensus annotation: (i) H1?H2=all annotations from annotator 1 and 2 (concatenation withoutadjudication), and (ii) HC=consensus annotations (after adjudication between annotator 1 and 2).3 ResultsTable 1 presents an overview of the global performance of each annotation run (H12 and H21 achievedsimilar results) across all PHI categories in terms of precision, recall and F1-measure (Manning andSch?utze, 2000).
Table 2 presents the detailed performance of each annotation run for individual PHIcategories in terms of F-measure.Baseline AR AR?H2 ARH2* ARHC H1* H12 H1?H2 H2 HCPrecision .820 .868 .920 .942 .943 .959 .962 .969 .974 .974Recall .806 .796 .763 .854 .854 .927 .934 .935 .936 .942F-measure .813 .830 .834 .896 .896 .943 .948 .951 .955 .958Table 1: Overall performance for all automatic PHI detection.
A star indicates statistically significantdifference in F-measure over the previous model (Wilcoxon test, p<0.05)Category Baseline AR AR?H2 ARH2 ARHC H1 H12 H1?H2 H2 HCAddress .648 .560 .000 .800 .800 .716 .744 .789 .795 .791Zip code .950 .958 .947 .964 .958 .974 .984 .974 .984 .990Date .958 .968 .962 .963 .967 .965 .963 .963 .959 .970E-mail .937 .927 .927 .927 .927 1.000 1.000 1.000 1.000 1.000Hospital .201 .248 .039 .856 .868 .789 .809 .856 .861 .867Identifier .000 .000 .000 .762 .797 .870 .892 .823 .836 .876Last name .816 .810 .834 .832 .828 .953 .957 .954 .961 .963First name .849 .858 .900 .901 .902 .960 .956 .961 .965 .960Telephone 1.000 .980 .978 .983 .980 .987 .994 .999 .999 1.000City .869 .874 .883 .887 .887 .948 .972 .962 .965 .972Table 2: Performance per PHI category (F-measure)4 Discussion4.1 Model performanceOverall, the task of automatic PHI recognition has been well studied and the rule-based tool provides astrong baseline with .813 F-measure on the test set.
Table 1 shows that there are three different typesof models, in terms of performance: the lower-performing category corresponds to models with no hu-man input.
The next category corresponds to models with some human input, and the higher-performingmodels correspond to models with the most human input.
This reflects the expectation that model per-formance increases with training corpus quality.
However, it also shows that, within the two categoriesthat include human input, there is no statistical difference in model performance with respect to the typeof human input.
We observed that the model trained on annotations from the H2 human annotator per-formed better (F=0.955) than the model trained on annotations from the H1 annotator (F=0.943).
Thisobservation reflects the agreement of the annotators with consensus annotations, where H2 had higheragreement than H1 (Grouin and N?ev?eol, 2014).
This is also true at the category level: H2 achieved56higher agreement with the consensus compared to H1 on categories ?address?
(F=0.985>0.767) and?hospital?
(F=0.947>0.806) but H2 had lower agreement with the consensus on the category ?identifier?
(F=0.840<0.933).4.2 Error AnalysisThe performance of CRF models depends on the size of the training corpus and the level of diversity ofthe mentions.
Error analysis on our test data shows that a few specific mentions are not tagged in the testcorpus, even though they occur in the training corpus.
For example, some hospital names occur in theclinical narratives either as acronyms or as full forms (e.g.
?GWH?
for ?George Washington Hospital?in transfer patient from GWH).
The acronyms are overall much less prevalent than the full forms andalso happen to be difficult to identify for human annotators (depending on the context, a given acronymcould refer to either a medical procedure, a physician or a hospital).
We observed that the only hospitalacronym present in the test corpus was not annotated by any of the CRF models.
Nevertheless, only fiveoccurrences of this acronym were found in the training corpus which is not enough for the CRF to learn.Other errors occur in recognizing sequences of doctors?
names that appear without separators in sig-natures lines at the end of documents (e.g.
?Jane BROWN John DOE Mary SMITH?).
In our test setwe observed that models trained on automatic annotations correctly predicted the beginning of such se-quences and then produced erroneous predictions for the rest of the sequence (models AR, AR?H2,ARHC and ARH2).
In contrast, models built on human annotations produced correct predictions on theentire sequence (models H1, H12, H1?H2, H2 and HC).
Similarly, for last names containing a nobiliaryparticle, the models trained on automatic annotations only identified part of the last name as a PHI.
Wealso observed that spelling errors (e.g.
?Jhn DOE?)
only resulted in correct predictions from the mod-els trained on the human annotations.
We did not find cases where the models built on the automaticannotations performed better than the models built on the human annotations.4.3 Annotation strategyTable 1 indicates that for the purpose of training a machine learning entity recognizer, all types of humaninput are equivalent.
In practice, this means that double annotations or consensus annotations are notnecessary.
The high inter-annotator agreement on our dataset may be a contributing factor for this finding.Indeed, (Esuli et al., 2013) found that with low inter-annotator agreement, models are biased towardsthe annotation style of the annotator who produced the training data.
Therefore, we believe that inter-annotator should be established on a small dataset before annotators work independently.
Table 2 showsthat using human annotations for selected categories results in strong improvement of the performanceover these categories (?address?, ?hospital?
and ?identifier?
categories in ARHC and ARH2 vs. AR) withlittle impact on the performance of the model on other categories.
Therefore, careful human annotationsare not necessarily needed for the entire corpus.
Targeting ?hard?
categories for human annotations canbe a good time-saving strategy.
While the difference between the models using some human input vs.all human input is statistically significant, the performance gain is lower than between models withouthuman input and some human input.
Using data with partial human input for training statistical modelscan cut annotation cost.5 Conclusion and future workHerein we have shown that full double annotation of a corpus is not necessary for the purpose of training acompetitive CRF-based model.
Our results suggest that a three-step annotation strategy can optimize theannotation effort: (i) double annotate a small subset of the corpus to ensure human annotators understandthe guidelines; (ii) have annotators work independently on different sections of the corpus to obtain widecoverage; and (iii) train a machine-learning based model on the human annotations and apply this modelon a new dataset.In future work, we plan to re-iterate these experiments on a different type of entity recognition taskwhere inter-annotator agreement may be more difficult to achieve, and may vary more between categoriesin order to investigate the influence of inter-annotator-agreement on our conclusions.57AcknowledgementsThis work was supported by the French National Agency for Research under grant CABeRneT1ANR-13-JS02-0009-01 and by the French National Agency for Medicines and Health Products Safety undergrant Vigi4MED2ANSM-2013-S-060.ReferencesRon Artstein and Massimo Poesio.
2008.
Inter-coder agreement for computational linguistics.
ComputationalLinguistics, 34(4):555?596.Peter F Brown, Vincent J Della Pietra, Peter V de Souza, Jenifer C Lai, and Robert L Mercer.
1992.
Class-basedn-gram models of natural language.
Computational Linguistics, 18(4):467?79.Dmitriy Dligach and Martha Palmer.
2011.
Reducing the need for double annotation.
In Proc of LinguisticAnnotation Workshop (LAW), pages 65?73.
Association for Computational Linguistics.Andrea Esuli, Diego Marcheggiani, and Fabrizio Sebastiani.
2013.
An enhanced CRFs-based system for informa-tion extraction from radiology reports.
J Biomed Inform, 46(3):425?35, Jun.Kar?en Fort and Beno?
?t Sagot.
2010.
Influence of pre-annotation on POS-tagged corpus development.
In Proc ofLinguistic Annotation Workshop (LAW), pages 56?63.
Association for Computational Linguistics.Kar?en Fort, Gilles Adda, and Kevin Bretonnel Cohen.
2011.
Amazon Mechanical Turk: Gold Mine or Coal Mine?Computational Linguistics, pages 413?420.Cyril Grouin and Aur?elie N?ev?eol.
2014.
De-identification of clinical notes in french: towards a protocol forreference corpus development.
J Biomed Inform.Cyril Grouin.
2013.
Anonymisation de documents cliniques : performances et limites des m?ethodes symboliqueset par apprentissage statistique.
Ph.D. thesis, University Pierre et Marie Curie, Paris, France.Thomas Lavergne, Olivier Capp?e, and Franc?ois Yvon.
2010.
Practical very large scale CRFs.
In Proceedings the48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 504?513.
Association forComputational Linguistics.Percy Liang.
2005.
Semi-supervised learning for natural language.
Master?s thesis, MIT.Chiristopher D. Manning and Hinrich Sch?utze.
2000.
Foundations of Statistical Natural Language Processing.MIT Press, Cambridge, Massachusetts.Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini.
1993.
Building a large annotated corpus ofEnglish: the Penn TreeBank.
Computational Linguistics, 19(2):313?330.Aur?elie N?ev?eol, Rezarta Islamaj Do?gan, and Zhiyong Lu.
2011.
Semi-automatic semantic annotation of PubMedqueries: a study on quality, efficiency, satisfaction.
J Biomed Inform, 44(2):310?8.Sophie Rosset, Cyril Grouin, Thomas Lavergne, Mohamed Ben Jannet, J?er?emy Leixa, Olivier Galibert, and PierreZweigenbaum.
2013.
Automatic named entity pre-annotation for out-of-domain human annotation.
In Proc ofLinguistic Annotation Workshop (LAW), pages 168?177.
Association for Computational Linguistics.Andrey Rzhetsky, Hagit Shatkay, and W John Wilbur.
2009.
How to get the most out of your curation effort.
PLoSComput Biol, 5(5):e1000391.Helmut Schmid.
1994.
Probabilistic part-of-speech tagging using decision trees.
In Proc of International Confer-ence on New Methods in Language.Burr Settles, Mark Craven, and Lewis Friedland.
2008.
Active learning with real annotation costs.
In Proc of theNIPS Workshop on Cost-Sensitive Learning.W John Wilbur, Andrey Rzhetsky, and Hagit Shatkay.
2006.
New directions in biomedical text annotation:definitions, guidelines and corpus construction.
BMC Bioinformatics, 25(7):356.1CABeRneT: Compr?ehension Automatique de Textes Biom?edicaux pour la Recherche Translationnelle2Vigi4MED: Vigilance dans les forums sur les M?edicaments58
