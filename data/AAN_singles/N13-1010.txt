Proceedings of NAACL-HLT 2013, pages 95?105,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsAn Analysis of Frequency- and Memory-Based Processing CostsMarten van SchijndelThe Ohio State Universityvanschm@ling.osu.eduWilliam SchulerThe Ohio State Universityschuler@ling.osu.eduAbstractThe frequency of words and syntactic con-structions has been observed to have a sub-stantial effect on language processing.
Thisbegs the question of what causes certain con-structions to be more or less frequent.
A the-ory of grounding (Phillips, 2010) would sug-gest that cognitive limitations might cause lan-guages to develop frequent constructions insuch a way as to avoid processing costs.
Thispaper studies how current theories of workingmemory fit into theories of language process-ing and what influence memory limitationsmay have over reading times.
Measures ofsuch limitations are evaluated on eye-trackingdata and the results are compared with predic-tions made by different theories of processing.1 IntroductionFrequency effects in language have been isolatedand observed in many studies (Trueswell, 1996;Jurafsky, 1996; Hale, 2001; Demberg and Keller,2008).
These effects are important because they il-luminate the ontogeny of language (how individualspeakers have acquired language), but they do notanswer questions about the phylogeny of language(how the language came to its current form).Phillips (2010) has hypothesized that grammarrule probabilities may be grounded in memory lim-itations.
Increased delays in processing center-embedded sentences as the number of embeddingsincreases, for example, are often explained in termsof a complexity cost associated with maintaining in-complete dependencies in working memory (Gib-son, 2000; Lewis and Vasishth, 2005).
Other stud-ies have shown a link between processing delaysand the low frequency of center-embedded construc-tions like object relatives (Hale, 2001), but theyhave not explored the source of this low frequency.A grounding hypothesis would claim that the lowprobability of generating such a structure may arisefrom an associated memory load.
In this account,while these complexity costs may involve language-specific concepts such as referent or argument link-ing, the underlying explanation would be one ofmemory limitations (Gibson, 2000) or neural acti-vation (Lewis and Vasishth, 2005).This paper seeks to explore the different predic-tions made by these theories on a broad-coveragecorpus of eye-tracking data (Kennedy et al 2003).In addition, the current experiment seeks to isolatememory effects from frequency effects in the sametask.
The results show that memory load measuresare a significant factor even when frequency mea-sures are residualized out.The remainder of this paper is organized as fol-lows: Sections 2 and 3 describe several frequencyand memory measures.
Section 4 describes a proba-bilistic hierarchic sequence model that allows all ofthese measures to be directly computed.
Section 5describes how these measures were used to predictreading time durations on the Dundee eye-trackingcorpus.
Sections 6 and 7 present results and discuss.2 Frequency Measures2.1 SurprisalOne of the strongest predictors of processing com-plexity is surprisal (Hale, 2001).
It has been shownin numerous studies to have a strong correlationwith reading time durations in eye-tracking and self-paced reading studies when calculated with a variety95of models (Levy, 2008; Roark et al 2009; Wu et al2010).Surprisal predicts the integration difficulty that aword xt at time step t presents given the precedingcontext and is calculated as follows:surprisal(xt) = ?
log2(?s?S(x1...xt) P (s)?s?S(x1...xt?1) P (s))(1)where S(x1 .
.
.
xt) is the set of syntactic trees whoseleaves have x1 .
.
.
xt as a prefix.1In essence, surprisal measures how unexpectedconstructions are in a given context.
What it doesnot provide is an explanation for why certain con-structions would be less common and thus more sur-prising.2.2 Entropy ReductionProcessing difficulty can also be measured in termsof entropy (Shannon, 1948).
A larger entropy over arandom variable corresponds to greater uncertaintyover the observed value it will take.
The entropy ofa syntactic derivation over the sequence x1 .
.
.
xt iscalculated as:2H(x1...t) =?s?S(x1...xt)?P (s) ?
log2 P (s) (2)Reduction in entropy has been found to predictprocessing complexity (Hale, 2003; Hale, 2006;Roark et al 2009; Wu et al 2010; Hale, 2011):?H(x1...t) = max(0, H(x1...t?1)?H(x1...t)) (3)This measures the change in uncertainty about thediscourse as each new word is processed.3 Memory Measures3.1 Dependency LocalityIn Dependency Locality Theory (DLT) (Gibson,2000), complexity arises from intervening referentsintroduced between a predicate and its argument.Under the original formulation of DLT, there is a1The parser in this study uses a beam.
However, given highparser accuracy, Roark (2001) showed that calculating com-plexity metrics over a beam should obtain similar results to thefull complexity calculation.2The incremental formulation used here was first proposedin Wu et al(2010).storage cost for each new referent introduced and anintegration cost for each referent intervening in a de-pendency projection.
This is a simplification madefor ease of computation, and subsequent work hasfound DLT to be more accurate cross-linguisticallyif the intervening elements are structurally definedrather than defined in terms of referents (Kwon etal., 2010).
That is, simply having a particular ref-erent intervene in a dependency projection may nothave as great an effect on processing complexity asthe syntactic construction the referent appears in.Therefore, this work reinterprets the costs of depen-dency locality to be related to the events of begin-ning a center embedding (storage) and completinga center embedding (integration).
Note that anti-locality effects (where longer dependencies are eas-ier to process) have also been observed in some lan-guages, and DLT is unable to account for these phe-nomena (Vasishth and Lewis, 2006).3.2 ACT-RProcessing complexity has also been attributed toconfusability (Lewis and Vasishth, 2005) as definedin domain-general cognitive models like ACT-R(Anderson et al 2004).ACT-R is based on theories of neural activation.Each new word is encoded and stored in workingmemory until it is retrieved at a later point for mod-ification before being re-encoded into the parse.
Anewly observed sign (word) associatively activatesany appropriate arguments from working memory,so multiple similarly appropriate arguments wouldslow processing as the parser must choose betweenthe highly activated hypotheses.
Any interveningsigns (words or phrases) that modify a previouslyencoded sign re-activate it and raise its resting acti-vation potential.
This can ease later retrieval of thatsign in what is termed an anti-locality effect, con-tra predictions of DLT.
In this way, returning out ofan embedded clause can actually speed processingby having primed the retrieved sign before it wasneeded.
ACT-R attributes locality phenomena to fre-quency effects (e.g.
unusual constructions) overrid-ing such priming and to activation decay if embed-ded signs do not prime the target sign through mod-ification (as in parentheticals).
Finally, ACT-R pre-dicts something like DLT?s storage cost due to theneed to differentiate each newly encoded sign from96SVPNP??NPNDtheVboughtNPNstudioDthe??????
?S/NP}NP/NFigure 1: Two disjoint connected components of a phrasestructure tree for the sentence The studio bought the pub-lisher?s rights, shown immediately prior to the word pub-lisher.those previously encoded (similarity-based encod-ing interference) (Lewis et al 2006).3.3 Hierarchic Sequential PredictionCurrent models of working memory in structuredtasks are defined in terms of hierarchies of sequen-tial processes, in which superordinate sequences canbe interrupted by subordinate sequences and resumewhen the subordinate sequences have concluded(Botvinick, 2007).
These models rely on temporalcueing as well as content-based cueing to explainhow an interrupted sequence may be recalled forcontinuation.Temporal cueing is based on a context of temporalfeatures for the current state (Howard and Kahana,2002).
The temporal context in which the subor-dinate sequence concludes must be similar enoughto the temporal context in which it was initiated torecall where in the superordinate sequence the sub-ordinate sequence occurred.
For example, the actof making breakfast may be interrupted by a phonecall.
Once the call is complete, the temporal contextis sufficiently similar to when the call began that oneis able to continue preparing breakfast.
The associ-ation between the current temporal context and thetemporal context prior to the interruption is strongenough to cue the next action.Temporal cueing is complemented by sequential(content-based) cueing (Botvinick, 2007) in whichthe content of an individual element is associatedwith, and thus cues, the following element.
For ex-ample, recalling the 20th note of a song is difficult,but when playing the song, each note cues the fol-lowing note, leading one to play the 20th note with-out difficulty.Hierarchic sequential prediction may be directlyapplicable to processing syntactic center embed-dings (van Schijndel et al in press).
An ongoingparse may be viewed graph-theoretically as one ormore connected components of incomplete phrasestructure trees (see Figure 1).
Beginning a new sub-ordinate sequence (a center embedding) introducesa new connected component, disjoint from that ofthe superordinate sequence.
As the subordinate se-quence proceeds, the new component gains asso-ciated discourse referents, each sequentially cuedfrom the last, until finally it merges with the super-ordinate connected component at the end of the em-bedded clause, forming a single connected compo-nent representing the parse up to that point.
Sinceit is not connected to the subordinate connectedcomponent prior to merging, the superordinate con-nected component must be recalled through tempo-ral cueing.McElree (2001; 2006) has found that retrievalof any non-focused (or in this case, unconnected)element from memory leads to slower processing.Therefore, integrating two disjoint connected com-ponents should be expected to incur a processingcost due to the need to recall the current state of thesuperordinate sequence to continue the parse.
Sucha cost would corroborate a DLT-like theory whereintegration slows processing.3.4 Dynamic Recruitment of AdditionalProcessing ResourcesLanguage processing is typically centered in the lefthemisphere of the brain (for right-handed individ-uals).
Just and Varma (2007) provide fMRI re-sults suggesting readers dynamically recruit addi-tional processing resources such as the right-side ho-mologues of the language processing areas of thebrain when processing center-embedded construc-tions.
Once an embedded construction terminates,the reader may still have temporary access to theseextra processing resources, which may briefly speedprocessing.This hypothesis would, therefore, predict an en-coding cost when a center embedding is initiated.The resulting inhibition would trigger recruitment ofadditional processing resources, which would then97allow the rest of the embedded structure to be pro-cessed at the usual speed.
Upon completing an em-bedding, the difficulty arising frommemory retrieval(McElree, 2001) would be ameliorated by these ex-tra processing resources, and the reduced process-ing complexity arising from reduced memory loadwould yield a temporary facilitation in processing.No longer requiring the additional resources to copewith the increased embedding, the processor wouldrelease them, returning the processor to its usualspeed.
Unlike anti-locality, where processing isfacilitated in longer passages due to accumulatingprobabilistic evidence, a model of dynamic recruit-ment of additional processing resources would pre-dict universal facilitation after a center embeddingof any length, modulo frequency effects.3.5 Embedding DifferenceWu et al(2010) propose an explicit measure ofthe difficulty associated with processing center-embedded constructions, which is similar to the pre-dictions of dynamic recruitment and is defined interms of changes in memory load.
They calcu-late a probabilistically-weighted average embeddingdepth as follows:?emb(x1 .
.
.
xt) =?s?S(x1...xt)d(s) ?
P (s) (4)where d(s) returns the embedding depth of thederivation s at xt in a variant of a left-corner pars-ing process.3 Embedding difference may then be de-rived as:EmbDiff (x1 .
.
.
xt) =?emb(x1 .
.
.
xt)?
(5)?emb(x1 .
.
.
xt?1)This is hypothesized to correlate positively withprocessing load: increasing the embedding depth in-creases processing load and decreasing it reducesprocessing load.
Note that embedding differencemakes the opposite prediction from DLT in that in-tegrating an embedded clause is predicted to speedprocessing.
In fact, the predictions of embedding3As pointed out by Wu et al(2010), in practice this can becomputed over a beam of potential parses in which case it mustbe normalized by the total probability of the beam.difference are such that it may be viewed as an im-plementation of the predictions of a hierarchic se-quential processing model with dynamic recruitmentof additional resources.4 ModelThis paper uses a hierarchic sequence model imple-mentation of a left-corner parser variant (van Schijn-del et al in press), which represents connected com-ponents of phrase structure trees in hierarchies ofhidden random variables.
This requires, at each timestep t:?
a hierarchically-organized set of N connectedcomponent states qnt , each consisting of an ac-tive sign of category aqnt , and an awaited signof category bqnt , separated by a slash ?/?
; and?
an observed word xt.Each connected component state in this model thenrepresents a contiguous portion of a phrase structuretree (see Figure 1 on preceding page).The operations of this parser can be defined as adeductive system (Shieber et al 1995) with an inputsequence consisting of a top-level connected com-ponent state ?/?, corresponding to an existing dis-course context, followed by a sequence of observedwords x1, x2, .
.
.
4 If an observation xt can attach asthe awaited sign of the most recent (most subordi-nate) connected component a/b, it is hypothesizedto do so, turning this incomplete sign into a com-plete sign a (F?, below); or if the observation canserve as a lower descendant of this awaited sign, itis hypothesized to form the first complete sign a?
ina newly initiated connected component (F+):a/b xta b ?
xt (F?
)a/b xta/b a?
b+?
a?
... ; a?
?
xt (F+)Then, if either of these complete signs (a or a?above, matched to a??
below) can attach as an initial4A deductive system consists of inferences or productionsof the form:PQR, meaning premise P entails conclusion Q ac-cording to rule R.98?/?
the?/?, D F+?/?, NP/N L?
studio?/?, NP F?
?/?, S/VP L?
bought?/?, S/VP, V F+?/?, S/NP L+ the?/?, S/NP, D F+?/?, S/NP, NP/N L?
publisher?/?, S/NP, NP F?
?/?, S/NP, D/G L?
?s?/?, S/NP, D F?
?/?, S/N L+ rights?/?, S F??/?
L+Figure 2: Example parse (in the form of a deductive proof) of the sentence The studio bought the publisher?s rights,using F+, F?, L+, and L?
productions.
Each pair of deductions combines a context of one or more connected compo-nent states with a sign (word) observed in that context.
By applying the F and L rules to the observed sign and context,the parser is able to generate a consequent context.
Initially, the context corresponds to a connected pre-sententialdialogue state ?/?.
When the is observed, the parser applies F+ to begin a new connected component state D. Byapplying L?, the parser determines that this new connected component is unfinished and generates an appropriateincomplete connected component state NP/N, encoding the superordinate state ?/?
for later retrieval.
Further on, theparser observes ?s and uses F?
to avoid generating a new connected component, which completes the sign D. Theparser follows this up with L+ to recall the superordinate connected component state S/NP and integrate it into themost deeply embedded connected component, which results in a less deeply embedded structure.child of the awaited sign of the immediately superor-dinate connected component state a/b, it is hypoth-esized to do so and terminate the subordinate con-nected component state, with xt as the last observa-tion of the terminated connected component (L+); orif the observation can serve as a lower descendant ofthis awaited sign, it is hypothesized to remain dis-joint and form its own connected component (L?
):a/b a??a/b??
b ?
a??
b??
(L+)a/b a?
?a/b a?/b??
b+?
a?
... ; a?
?
a??
b??
(L?
)These operations can be made probabilistic.
Theprobability ?
of a transition at time step t is definedin terms of (i) a probability ?
of initiating a new con-nected component state with xt as its first observa-tion, multiplied by (ii) the probability ?
of terminat-ing a connected component state with xt as its lastobservation, multiplied by (iii) the probabilities ?and ?
of generating categories for active and awaitedsigns aqnt and bqnt in the resulting most subordinateconnected component state qnt .
This kind of modelcan be defined directly on PCFG probabilities andtrained to produce state-of-the-art accuracy by usingthe latent variable annotation of Petrov et al(2006)(van Schijndel et al in press).5An example parse is shown in Figure 2.
Sincetwo binary structural decisions (F and L) must bemade in order to generate each word, there are fourpossible structures that may be generated (see Ta-ble 1).
The F+L?
transition initiates a new levelof embedding at word xt and so requires the super-ordinate state to be encoded for later retrieval (e.g.on observing the in Figure 2).
The F?L+ transi-tion completes the deepest level of embedding andtherefore requires the recall of the current superor-dinate connected component state with which the5The model has been shown to achieve an F-score of 87.8,within .2 points of the Petrov and Klein (2007) parser, whichobtains an F-score of 88.0 on the same task.
Because the se-quence model is defined over binary-branching phrase structure,both parsers were evaluated on binary-branching phrase struc-ture trees to provide a fair comparison.99F?L?
Cue Active SignF+L?
Initiate/EncodeF?L+ Terminate/IntegrateF+L+ Cue Awaited SignTable 1: The hierarchical structure decisions and the op-erations they represent.
F+L?
initiates a new connectedcomponent, F?L+ integrates two disjoint connected com-ponents into a single connected component, and F?L?and F+L+ sequentially cue, respectively, a new activesign (along with an associated awaited sign) and a newawaited sign from the most recent connected component.subordinate connected component state will be in-tegrated.
For example, in Figure 2, upon observ-ing ?s, the parser must use temporal cueing to re-call that it is in the middle of processing an NP (tocomplete an S), which sequentially cues a predictionof N. F?L?
transitions complete the awaited sign ofthe most subordinate state and so sequentially cuea following connected component state at the sametier of the hierarchy.
For example, in Figure 2, afterobserving studio, the parser uses the completed NPto sequentially cue the prediction that it has finishedthe left child of an S. F+L+ transitions locally ex-pand the awaited sign of the most subordinate stateand so should also not require any recall or encod-ing.
For example, in Figure 2, observing boughtwhile awaiting a VP sequentially cues a predictionof NP.F+L?, then, loosely corresponds to a storage ac-tion under DLT as more hierarchic levels must nowbe maintained at each future step of the parse.
Asstated before, it differs from DLT in that it is sensi-tive to the depth of embedding rather than a partic-ular subset of syntactic categories.
Wu et al(2010)found that increasing the embedding depth led tolonger reading times in a self-paced reading experi-ment.
In ACT-R terms, F+L?
corresponds to an en-coding action, potentially causing processing diffi-culty resulting from the similarity of the current signto previously encoded signs.F?L+, by contrast, is similar to DLT?s integra-tion action since a subordinate connected compo-nent is integrated into the rest of the parse structure.This represents a temporal cueing event in whichthe awaited category of the superordinate connectedTheory F+L?
F?L+DLT positive positiveACT-R positive positiveHier.
Sequential Prediction positiveDynamic Recruitment positive negativeEmbedding Difference positive negativeTable 2: Each theory?s prediction of the direction ofthe correlation between each hierachical structure predic-tor and reading times.
Hierarchic sequential predictionis agnostic about the processing speed of F+L?
opera-tions, and none of the theories make any predictions as tothe sign associated with the within-embedding measuresF?L?
and F+L+.component is recalled.
In contrast to DLT, embed-ding difference and dynamic recruitment would pre-dict a shorter reading time in the F?L+ case be-cause of the reduction in memory load.
In an ACT-Rframework, reading time durations can increase atthe retrieval site because the retrieval causes compe-tition among similarly encoded signs in the contextset.
While it is possible for reading times to decreasewhen completing a center embedding in ACT-R (Va-sishth and Lewis, 2006), this would be expressedas a frequency effect due to certain argument typescommonly foreshadowing their predicates (Jaeger etal., 2008).
Since frequency effects are factored sep-arately from memory effects in this study, ACT-Rwould predict longer residual (memory-based) read-ing times when completing an embedding.Predicted correlations to reading times for the Fand L transitions are summarized in Table 2.5 Eye-trackingEye-tracking and reading time data are often used totest complexity measures (Gibson, 2000; Dembergand Keller, 2008; Roark et al 2009) under the as-sumption that readers slow down when reading morecomplex passages.
Readers saccade over portions oftext and regress back to preceding text in complexpatterns, but studies have correlated certain mea-sures with certain processing constraints (see Cliftonet al2007 for a review).
For example, the initiallength of time fixated on a single word is correlatedwith word identification time; whereas regressiondurations after a word is fixated (but prior to a fix-ation in a new region) are hypothesized to correlate100with integration difficulty.Since this work focuses on incremental process-ing, all processing that occurs up to a given point inthe sentence is of interest.
Therefore, in this study,predictions will be compared to go-past durations.Go-past durations are calculated by summing all fix-ations in a region of text, including regressions, un-til a new region is fixated, which accounts for addi-tional processing that may take place after initial lex-ical access, but before the next region is processed.For example, if one region ends at word 5 in a sen-tence, and the next fixation lands on word 8, then thego-past region consists of words 6-8 and the go-pastduration sums all fixations until a fixation occurs af-ter word 8.6 EvaluationThe measures presented in this paper were evaluatedon the Dundee eye-tracking corpus (Kennedy et al2003).
The corpus consists of 2388 sentences of nat-urally occurring news text written in standard BritishEnglish.
The corpus also includes eye-tracking datafrom 10 native English speakers, which providesa test corpus of 260,124 subject-duration pairs ofreading time data.
Of this, any fixated words ap-pearing fewer than 5 times in the training data wereconsidered unknown and were filtered out to obtainaccurate predictions.
Fixations on the first or lastwords of a line were also filtered out to avoid any?wrap-up?
effects resulting from preparing to sac-cade to the beginning of the next line or resultingfrom orienting to a new line.
Additionally, followingDemberg and Keller (2008), any fixations that skipmore than 4 words were attributed to track loss bythe eyetracker or lack of attention of the reader andso were excluded from the analysis.
This left the fi-nal evaluation corpus with 151,331 subject-durationpairs.The evaluation consisted of fitting a linear mixed-effects model (Baayen et al 2008) to reading timedurations using the lmer function of the lme4 Rpackage (Bates et al 2011; R Development CoreTeam, 2010).
This allowed by-subject and by-itemvariation to be included in the initial regression asrandom intercepts in addition to several baseline pre-dictors.6 Before fitting, the durations extracted from6Each fixed effect was centered to reduce collinearity.the corpus were log-transformed, producing morenormally distributed data to obey the assumptions oflinear mixed effects models.7Included among the fixed effects were the posi-tion in the sentence that initiated the go-past region(SENTPOS) and the number of characters in the ini-tiating word (NRCHAR).
The difficulty of integrat-ing a word may be seen in whether the immediatelyfollowing word was fixated (NEXTISFIX), and sim-ilarly if the immediately previous word was fixated(PREVISFIX) the current word probably need not befixated for as long.
Finally, unigram (LOGPROB)and bigram probabilities are included.
The bigramprobabilities are those of the current word given theprevious word (LOGFWPROB) and the current wordgiven the following word (LOGBWPROB).
Fossumand Levy (2012) showed that for n-gram probabili-ties to be effective predictors on the Dundee corpus,they must be calculated from a wide variety of texts,so following them, this study used the Brown corpus(Francis and Kucera, 1979), the WSJ Sections 02-21(Marcus et al 1993), the written text portion of theBritish National Corpus (BNC Consortium, 2007),and the Dundee corpus (Kennedy et al 2003).
Thisamounted to an n-gram training corpus of roughly87 million words.
These statistics were smoothedusing the SRILM (Stolcke, 2002) implementation ofmodified Kneser-Ney smoothing (Chen and Good-man, 1998).
Finally, total surprisal (SURP) was in-cluded to account for frequency effects in the base-line.The preceding measures are commonly used inbaseline models to fit reading time data (Dembergand Keller, 2008; Frank and Bod, 2011; Fossum andLevy, 2012) and were calculated from the final wordof each go-past region.
The following measurescreate a more sophisticated baseline by accumulat-ing over the entire go-past region to capture whatmust be integrated into the discourse to continue theparse.
One factor (CWDELTA) simply counts thenumber of words in each go-past region.
Cumula-7In particular, these models assume the noise in the data isnormally distributed.
Initial exploratory trials showed that theresiduals of fitting any sensible baseline also become more nor-mally distributed if the response variable is log-transformed.
Fi-nally, the directions of the effects remain the same whether ornot the reading times are log-transformed, though significancecannot be ascertained without the transform.101tive total surprisal (CUMUSURP) and cumulative en-tropy reduction (ENTRED) give the surprisal (Hale,2001) and entropy reduction (Hale, 2003) summedover the go-past region.
To avoid convergence is-sues, each of the cumulative measures is residual-ized from the next simpler model in the followingorder: CWDELTA from the standard baseline, CU-MUSURP from the baseline with CWDELTA, and EN-TRED from the baseline with all other effects.Residualization was accomplished by using thesimpler mixed-effects model to fit the measure of in-terest.
The residuals from that model fit were thenused in place of the factor of interest.
All joint inter-actions were included in the baseline model as well.Finally, to account for spillover effects (Just et al1982) where processing from a previous region con-tributes to the following duration, the above baselinepredictors from the previous go-past region were in-cluded as factors for the current region.Having SURP as a predictor with CUMUSURP mayseem redundant, but initial analyses showed SURPwas a significant predictor over CUMUSURP whenCWDELTA was a separate factor in the baseline (cur-rent: p = 2.2 ?
10?16 spillover: p = 2 ?
10?15)and vice versa (current: p = 2.2 ?
10?16 spillover:p = 6 ?
10?5).
One reason for this could be thatgo-past durations conflate complexity experiencedwhen initially fixating on a region with the difficultyexperienced during regressions.
By including bothversions of surprisal, the model is able to accountfor frequency effects occurring in both conditions.This study is only interested in how well the pro-posed memory-based measures fit the data over thebaseline, so to avoid fitting to the test data or weak-ening the baseline by overfitting to training data, thefull baseline was used in the final evaluation.Each measure proposed in this paper was summedover go-past regions to make it cumulative andwas residualized from all non-spillover factors be-fore being included on top of the full baseline as amain effect.
Likewise, the spillover version of eachproposed measure was residualized from the otherspillover factors before being included as a main ef-fect.
Only a single proposed measure (or its spillovercorrollary) was included in each model.
The resultsshown in Table 3 reflect the probability of the fullmodel fit being obtained by the model lacking eachfactor of interest.
This was found via posterior sam-Factor Operation t-score p-valueF?L?
Cue Active 0.60 0.55F+L?
Initiate 7.10 2.22?10?14F?L+ Integrate -5.44 5.23?10?8F+L+ Cue Awaited -1.55 0.12Table 3: Significance of each of the structure generationoutcomes at predicting log-transformed durations whenadded to the baseline as a main effect after being residu-alized from it.
The sign of the t-score indicates the direc-tion of the correlation between the residualized factor andgo-past durations.
Note that these factors are all basedon the current go-past region; the spillover corollaries ofthese were not significant predictors of reading times.pling of each factor using the Markov chain MonteCarlo implementation of the languageR R package(Baayen, 2008).The results indicate that the F+L?
and F?L+ mea-sures were both significant predictors of duration asexpected.
Further, F?L?
and F+L+, which both sim-ply reflect sequential cueing, were not significantpredictors of go-past duration, also as expected.7 Discussion and ConclusionThe fact that F+L?
was strongly predictive over thebaseline is encouraging as it suggests that memorylimitations could provide at least a partial explana-tion of why certain constructions are less frequent incorpora and thus yield a high surprisal.
Moreover,it indicates that the model corroborates the sharedprediction of most of the memory-based models thatinitiating a new connected component slows pro-cessing.The fact that F?L+ is predictive but has a neg-ative coefficient could be evidence of anti-locality,or it could be an indication of some sort of pro-cessing momentum due to dynamic recruitment ofadditional processing resources (Just and Varma,2007).
Since anti-locality is an expectation-basedfrequency effect, and since this study controlled forfrequency effects with n-grams, surprisal, and en-tropy reduction, an anti-locality explanation wouldrely on either (i) more precise variants of the met-rics used in this study or (ii) other frequency metricsaltogether.
Future work could investigate the possi-bility of anti-locality by looking at the distance be-tween an encoding operation and its corresponding102integration action to see if the integration facilita-tion observed in this study is driven by longer em-beddings or if there is simply a general facilitationeffect when completing embeddings.The finding of a negative integration cost was pre-viously observed by Wu et al(2010) as well asDemberg and Keller (2008), although Demberg andKeller calculated it using the original referent-baseddefinitions of Gibson (1998; 2000) and varied whichparts of speech counted for calculating integrationcost.
Ultimately, Demberg and Keller (2008) con-cluded that the negative coefficient was evidencethat integration cost was not a good broad-coveragepredictor of reading times; however, this study hasreplicated the effect and showed it to be a very strongpredictor of reading times, albeit one that is corre-lated with facilitation rather than inhibition.It is interesting that many studies have foundnegative integration cost using naturalistic stimuliwhile others have consistently found positive inte-gration cost when using constructed stimuli withmultiple center embeddings presented without con-text (Gibson, 2000; Chen et al 2005; Kwon et al2010).
It may be the case that any dynamic re-cruitment is overwhelmed by the memory demandsof multiply center-embedded stimuli.
Alternatively,it may be that the difficulty of processing multiplycenter-embedded sentences containing ambiguitiesproduces anxiety in subjects, which slows process-ing at implicit prosodic boundaries (Fodor, 2002;Mitchell et al 2008).
In any case, the source of thisdiscrepancy presents an attractive target for futureresearch.In general, sequential prediction does not seemto present people with any special ease or difficultyas evidenced by the lack of significance of F?L?and F+L+ predictions when frequency effects arefactored out.
This supports a theory of sequential,content-based cueing (Botvinick, 2007) that predictsthat certain states would directly cue other states andthus avoid recall difficulty.
An example of this maybe seen in the case of a transitive verb triggeringthe prediction of a direct object.
This kind of cue-ing would show up as a frequency effect predictedby surprisal rather than as a memory-based cost,due to frequent occurrences becoming ingrained asa learned skill.
Future work could use these sequen-tial cueing operations to investigate further claimsof the dynamic recruitment hypothesis.
One of theimplications of the hypothesis is that recruitment ofresources alleviates the initial encoding cost, whichallows the parser to continue on as before the em-bedding.
DLT, on the other hand, predicts that thereis a storage cost for maintaining unresolved depen-dencies during a parse (Gibson, 2000).
By weight-ing each of the sequential cueing operations with theembedding depth at which it occurs, an experimentmay be able to test these two predictions.This study has shown that measures based onworking memory operations have strong predictivityover other previously proposed measures includingthose associated with frequency effects.
This sug-gests that memory limitations may provide a partialexplanation of what gives rise to frequency effects.Lastly, this paper provides evidence that there is arobust facilitation effect in English that arises fromcompleting center embeddings.The hierarchic sequence model, all evaluationscripts, and regression results for all baseline pre-dictors used in this paper are freely available athttp://sourceforge.net/projects/modelblocks/.AcknowledgementsThanks to Peter Culicover, Micha Elsner, and threeanonymous reviewers for helpful suggestions.
Thiswork was funded by an OSU Department of Lin-guistics Targeted Investment for Excellence (TIE)grant for collaborative interdisciplinary projectsconducted during the academic year 2012-13.ReferencesJohn R. Anderson, Dan Bothell, Michael D. Byrne,S.
Douglass, Christian Lebiere, and Y. Qin.
2004.
Anintegrated theory of the mind.
Psychological Review,111(4):1036?1060.R.
Harald Baayen, D. J. Davidson, and Douglas M. Bates.2008.
Mixed-effects modeling with crossed randomeffects for subjects and items.
Journal of Memory andLanguage, 59:390?412.R.
Harald Baayen.
2008.
Analyzing Linguistic Data:A Practical Introduction to Statistics using R. Cam-bridge University Press, New York, NY.Douglas Bates, Martin Maechler, and Ben Bolker, 2011.lme4: Linear mixed-effects models using S4 classes.BNC Consortium.
2007.
The british national corpus.103Matthew Botvinick.
2007.
Multilevel structure in behav-ior and in the brain: a computational model of Fuster?shierarchy.
Philosophical Transactions of the Royal So-ciety, Series B: Biological Sciences, 362:1615?1626.Stanley F. Chen and Joshua Goodman.
1998.
An empir-ical study of smoothing techniques for language mod-eling.
Technical report, Harvard University.Evan Chen, Edward Gibson, and Florian Wolf.
2005.Online syntactic storage costs in sentence comprehen-sion.
Journal of Memory and Language, 52(1):144?169.Charles Clifton, Adrian Staub, and Keith Rayner.
2007.Eye movements in reading words and sentences.
InEye movements: A window on mind and brain, pages341?372.
Elsevier.Vera Demberg and Frank Keller.
2008.
Data from eye-tracking corpora as evidence for theories of syntacticprocessing complexity.
Cognition, 109(2):193?210.Janet Fodor.
2002.
Prosodic disambiguation in silentreading.
In M. Hirotani, editor, In Proceedings ofNELS 32.Victoria Fossum and Roger Levy.
2012.
Sequentialvs.
hierarchical syntactic models of human incremen-tal sentence processing.
In Proceedings of CMCL-NAACL 2012.
Association for Computational Linguis-tics.W.
Nelson Francis and Henry Kucera.
1979.
The browncorpus: A standard corpus of present-day edited amer-ican english.Stefan Frank and Rens Bod.
2011.
Insensitivity ofthe human sentence-processing system to hierarchicalstructure.
Psychological Science.Edward Gibson.
1998.
Linguistic complexity: Localityof syntactic dependencies.
Cognition, 68(1):1?76.Edward Gibson.
2000.
The dependency locality theory:A distance-based theory of linguistic complexity.
InImage, language, brain: Papers from the first mind ar-ticulation project symposium, pages 95?126.John Hale.
2001.
A probabilistic earley parser as apsycholinguistic model.
In Proceedings of the secondmeeting of the North American chapter of the Associ-ation for Computational Linguistics, pages 159?166,Pittsburgh, PA.John Hale.
2003.
Grammar, Uncertainty and Sen-tence Processing.
Ph.D. thesis, Cognitive Science,The Johns Hopkins University.John Hale.
2006.
Uncertainty about the rest of the sen-tence.
Cognitive Science, 30(4):609?642.John Hale.
2011.
What a rational parser would do.
Cog-nitive Science, 35(3):399?443.Marc W. Howard and Michael J. Kahana.
2002.
A dis-tributed representation of temporal context.
Journal ofMathematical Psychology, 45:269?299.F.
T. Jaeger, E. Fedorenko, P. Hofmeister, and E. Gib-son.
2008.
Expectation-based syntactic processing:Antilocality outside of head-final languages.
In The21st CUNY Sentence Processing Conference.Daniel Jurafsky.
1996.
A probabilistic model of lexicaland syntactic access and disambiguation.
CognitiveScience: A Multidisciplinary Journal, 20(2):137?194.Marcel Adam Just and Sashank Varma.
2007.
The or-ganization of thinking: What functional brain imagingreveals about the neuroarchitecture of complex cogni-tion.
Cognitive, Affective, & Behavioral Neuroscience,7:153?191.Marcel Adam Just, Patricia A. Carpenter, and Jacque-line D. Woolley.
1982.
Paradigms and processes inreading comprehension.
Journal of Experimental Psy-chology: General, 111:228?238.Alan Kennedy, James Pynte, and Robin Hill.
2003.
TheDundee corpus.
In Proceedings of the 12th Europeanconference on eye movement.Nayoung Kwon, Yoonhyoung Lee, Peter C. Gordon,Robert Kluender, and Maria Polinsky.
2010.
Cog-nitive and linguistic factors affecting subject/objectasymmetry: An eye-tracking study of pre-nominal rel-ative clauses in korean.
Language, 86(3):561.Roger Levy.
2008.
Expectation-based syntactic compre-hension.
Cognition, 106(3):1126?1177.Richard L. Lewis and Shravan Vasishth.
2005.An activation-based model of sentence processingas skilled memory retrieval.
Cognitive Science,29(3):375?419.Richard L. Lewis, Shravan Vasishth, and Jane A. VanDyke.
2006.
Computational principles of workingmemory in sentence comprehension.
Trends in Cog-nitive Science, 10(10):447?454.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: the Penn Treebank.
ComputationalLinguistics, 19(2):313?330.Brian McElree.
2001.
Working memory and focal atten-tion.
Journal of Experimental Psychology, LearningMemory and Cognition, 27(3):817?835.Brian McElree.
2006.
Accessing recent events.
The Psy-chology of Learning and Motivation, 46:155?200.D.
Mitchell, X. Shen, M. Green, and T. Hodgson.
2008.Accounting for regressive eye-movements in modelsof sentence processing: A reappraisal of the selectivereanalysis hypothesis.
Journal of Memory and Lan-guage, 59:266?293.Slav Petrov and Dan Klein.
2007.
Improved infer-ence for unlexicalized parsing.
In Proceedings ofNAACL HLT 2007, pages 404?411, Rochester, NewYork, April.
Association for Computational Linguis-tics.104Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proceedings of the 44thAnnual Meeting of the Association for ComputationalLinguistics (COLING/ACL?06).Colin Phillips.
2010.
Some arguments and non-arguments for reductionist accounts of syntactic phe-nomena.
Language and Cognitive Processes, 28:156?187.R Development Core Team, 2010.
R: A Language andEnvironment for Statistical Computing.
R Foundationfor Statistical Computing, Vienna, Austria.
ISBN 3-900051-07-0.Brian Roark, Asaf Bachrach, Carlos Cardenas, andChristophe Pallier.
2009.
Deriving lexical and syn-tactic expectation-based measures for psycholinguisticmodeling via incremental top-down parsing.
Proceed-ings of the 2009 Conference on Empirical Methods inNatural Langauge Processing, pages 324?333.Brian Roark.
2001.
Probabilistic top-down parsingand language modeling.
Computational Linguistics,27(2):249?276.Claude Shannon.
1948.
A mathematical theory of com-munication.
Bell System Technical Journal, 27:379?423, 623?656.Stuart M. Shieber, Yves Schabes, and Fernando C.N.Pereira.
1995.
Principles and implementation of de-ductive parsing.
Journal of Logic Programming, 24:3?36.Andreas Stolcke.
2002.
Srilm ?
an extensible languagemodeling toolkit.
In Seventh International Conferenceon Spoken Language Processing.John Trueswell.
1996.
The role of lexical frequencyin syntactic ambiguity resolution.
Journal of Memoryand Language, 35:566?585.Marten van Schijndel, Andy Exley, and William Schuler.in press.
A model of language processing as hierarchicsequential prediction.
Topics in Cognitive Science.Shravan Vasishth and Richard L. Lewis.
2006.Argument-head distance and processing complexity:Explaining both locality and antilocality effects.
Lan-guage, 82(4):767?794.Stephen Wu, Asaf Bachrach, Carlos Cardenas, andWilliam Schuler.
2010.
Complexity metrics in an in-cremental right-corner parser.
In Proceedings of the48th Annual Meeting of the Association for Computa-tional Linguistics (ACL?10), pages 1189?1198.105
