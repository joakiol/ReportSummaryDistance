Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 953?960,Sydney, July 2006. c?2006 Association for Computational LinguisticsInducing Word Alignments with Bilexical Synchronous TreesHao Zhang and Daniel GildeaComputer Science DepartmentUniversity of RochesterRochester, NY 14627AbstractThis paper compares different bilexicaltree-based models for bilingual alignment.EM training for the new model bene-fits from the dynamic programming ?hooktrick?.
The model produces improved de-pendency structure for both languages.1 IntroductionA major difficulty in statistical machine translationis the trade-off between representational powerand computational complexity.
Real-world cor-pora for language pairs such as Chinese-Englishhave complex reordering relationships that are notcaptured by current phrase-based MT systems, de-spite their state-of-the-art performance measuredin competitive evaluations.
Synchronous gram-mar formalisms that are capable of modelingsuch complex relationships while maintaining thecontext-free property in each language have beenproposed for many years, (Aho and Ullman, 1972;Wu, 1997; Yamada and Knight, 2001; Melamed,2003; Chiang, 2005), but have not been scaled tolarge corpora and long sentences until recently.In Synchronous Context Free Grammars, thereare two sources of complexity, grammar branch-ing factor and lexicalization.
In this paper we fo-cus on the second issue, constraining the gram-mar to the binary-branching Inversion Transduc-tion Grammar of Wu (1997).
Lexicalization seemslikely to help models predict alignment patternsbetween languages, and has been proposed byMelamed (2003) and implemented by Alshawi etal.
(2000) and Zhang and Gildea (2005).
However,each piece of lexical information considered by amodel multiplies the number of states of dynamicprogramming algorithms for inference, meaningthat we must choose how to lexicalize very care-fully to control complexity.In this paper we compare two approaches tolexicalization, both of which incorporate bilexicalprobabilities.
One model uses bilexical probabil-ities across languages, while the other uses bilex-ical probabilities within one language.
We com-pare results on word-level alignment, and investi-gate the implications of the choice of lexicaliza-tion on the specifics of our alignment algorithms.The new model, which bilexicalizes within lan-guages, allows us to use the ?hook trick?
(Eis-ner and Satta, 1999) and therefore reduces com-plexity.
We describe the application of the hooktrick to estimation with Expectation Maximization(EM).
Despite the theoretical benefits of the hooktrick, it is not widely used in statistical monolin-gual parsers, because the savings do not exceedthose obtained with simple pruning.
We speculatethat the advantages may be greater in an EM set-ting, where parameters to guide pruning are not(initially) available.In order to better understand the model, we an-alyze its performance in terms of both agreementwith human-annotated alignments, and agreementwith the dependencies produced by monolingualparsers.
We find that within-language bilexical-ization does not improve alignment over cross-language bilexicalization, but does improve recov-ery of dependencies.
We find that the hook tricksignificantly speeds training, even in the presenceof pruning.Section 2 describes the generative model.
Thehook trick for EM is explained in Section 3.
InSection 4, we evaluate the model in terms of align-ment error rate and dependency error rate.
Weconclude with discussions in Section 5.9532 Bilexicalization of InversionTransduction GrammarThe Inversion Transduction Grammar of Wu(1997) models word alignment between a transla-tion pair of sentences by assuming a binary syn-chronous tree on top of both sides.
Using EMtraining, ITG can induce good alignments throughexploring the hidden synchronous trees from in-stances of string pairs.ITG consists of unary production rules that gen-erate English/foreign word pairs e/f :X ?
e/fand binary production rules in two forms that gen-erate subtree pairs, written:X ?
[Y Z]andX ?
?Y Z?The square brackets indicate the right hand siderewriting order is the same for both languages.The pointed brackets indicate there exists a type ofsyntactic reordering such that the two right handside constituents rewrite in the opposite order inthe second language.The unary rules account for the alignment linksacross two sides.
Either e or f may be a specialnull word, handling insertions and deletions.
Thetwo kinds of binary rules (called straight rules andinverted rules) build up a coherent tree structureon top of the alignment links.
From a modelingperspective, the synchronous tree that may involveinversions tells a generative story behind the wordlevel alignment.An example ITG tree for the sentence pair Jeles vois / I see them is shown in Figure 1(left).
Theprobability of the tree is the product rule probabil-ities at each node:P (S ?
A)?
P (A ?
[C B])?
P (C ?
I/Je)?
P (B ?
?C C?)?
P (C ?
see/vois)?
P (C ?
them/les)The structural constraint of ITG, which is thatonly binary permutations are allowed on eachlevel, has been demonstrated to be reasonableby Zens and Ney (2003) and Zhang and Gildea(2004).
However, in the space of ITG-constrainedsynchronous trees, we still have choices in makingthe probabilistic distribution over the trees morerealistic.
The original Stochastic ITG is the coun-terpart of Stochastic CFG in the bitext space.
Theprobability of an ITG parse tree is simply a prod-uct of the probabilities of the applied rules.
Thus,it only captures the fundamental features of wordlinks and reflects how often inversions occur.2.1 Cross-Language BilexicalizationZhang and Gildea (2005) described a model inwhich the nonterminals are lexicalized by Englishand foreign language word pairs so that the inver-sions are dependent on lexical information on theleft hand side of synchronous rules.
By introduc-ing the mechanism of probabilistic head selectionthere are four forms of probabilistic binary rulesin the model, which are the four possibilities cre-ated by taking the cross-product of two orienta-tions (straight and inverted) and two head choices:X(e/f) ?
[Y (e/f) Z]X(e/f) ?
[Y Z(e/f)]X(e/f) ?
?Y (e/f) Z?X(e/f) ?
?Y Z(e/f)?where (e/f) is a translation pair.A tree for our example sentence under thismodel is shown in Figure 1(center).
The tree?sprobability is again the product of rule probabil-ities:P (S ?
A(see/vois))?
P (A(see/vois) ?
[C B(see/vois)])?
P (C ?
C(I/Je))?
P (B(see/vois) ?
?C(see/vois) C?)?
P (C ?
C(them/les))2.2 Head-Modifier BilexicalizationOne disadvantage of the model above is that itis not capable of modeling bilexical dependen-cies on the right hand side of the rules.
Thus,while the probability of a production being straightor inverted depends on a bilingual word pair, itdoes not take head-modifier relations in either lan-guage into account.
However, modeling completebilingual bilexical dependencies as theorized inMelamed (2003) implies a huge parameter spaceof O(|V |2|T |2), where |V | and |T | are the vo-cabulary sizes of the two languages.
So, in-stead of modeling cross-language word transla-tions and within-language word dependencies in954CBCAsee/vois them/lesI/JeSCC(I/Je)CC(them/les)CB(see/vois)A(see/vois)C(see/vois)S SC(I)C(them)them/lesI/Je C(see)see/voisB(see)A(see)Figure 1: Parses for an example sentence pair under unlexicalized ITG (left), cross-language bilexicaliza-tion (center), and head-modifier bilexicaliztion (right).
Thick lines indicate head child; crossbar indicatesinverted production.a joint fashion, we factor them apart.
We lexical-ize the dependencies in the synchronous tree usingwords from only one language and translate thewords into their counterparts in the other languageonly at the bottom of the tree.
Formally, we havethe following patterns of binary dependency rules:X(e) ?
[Y (e) Z(e?
)]X(e) ?
[Y (e?)
Z(e)]X(e) ?
?Y (e) Z(e?
)?X(e) ?
?Y (e?)
Z(e)?where e is an English head and e?
is an Englishmodifier.Equally importantly, we have the unary lexicalrules that generate foreign words:X(e) ?
e/fTo make the generative story complete, we alsohave a top rule that goes from the unlexicalizedstart symbol to the highest lexicalized nonterminalin the tree:S ?
X(e)Figure 1(right), shows our example sentence?stree under the new model.
The probability of abilexical synchronous tree between the two sen-tences is:P (S ?
A(see))?
P (A(see) ?
[C(I) B(see)])?
P (C(I) ?
I/Je)?
P (B(see) ?
?C(see) C(them)?)?
P (C(see) ?
see/vois)?
P (C(them) ?
them/les)Interestingly, the lexicalized B(see) predictsnot only the existence of C(them), but also thatthere is an inversion involved going from C(see)to C(them).
This reflects the fact that direct ob-ject pronouns come after the verb in English, butbefore the verb in French.
Thus, despite condi-tioning on information about words from only onelanguage, the model captures syntactic reorderinginformation about the specific language pair it istrained on.
We are able to discriminate betweenthe straight and inverted binary nodes in our ex-ample tree in a way that cross-language bilexical-ization could not.In terms of inferencing within the framework,we do the usual Viterbi inference to find the bestbilexical synchronous tree and treat the depen-dencies and the alignment given by the Viterbiparse as the best ones, though mathematically thebest alignment should have the highest probabil-ity marginalized over all dependencies constrainedby the alignment.
We do unsupervised training toobtain the parameters using EM.
Both EM andViterbi inference can be done using the dynamicprogramming framework of synchronous parsing.3 Inside-Outside Parsing with the HookTrickITG parsing algorithm is a CYK-style chart pars-ing algorithm extended to bitext.
Instead of build-ing up constituents over spans on a string, an ITGchart parser builds up constituents over subcellswithin a cell defined by two strings.
We use?
(X(e), s, t, u, v) to denote the inside probabil-ity of X(e) which is over the cell of (s, t, u, v)where (s, t) are indices into the source languagestring and (u, v) are indices into the target lan-guage string.
We use ?
(X(e), s, t, u, v) to de-note its outside probability.
Figure 2 shows howsmaller cells adjacent along diagonals can be com-bined to create a large cell.
We number the sub-cells counterclockwise.
To analyze the complex-ity of the algorithm with respect to input string955S                                                                                                                   uUvs t2 13 4U                              us SeFigure 2: Left: Chart parsing over the bitext cell of (s, t, u, v).
Right: One of the four hooks built forfour corners for more efficient parsing.length, without loss of generality, we ignore thenonterminal symbols X , Y , and Z to simplify thederivation.The inside algorithm in the context of bilexicalITG is based on the following dynamic program-ming equation:?
(e, s, t, u, v)=?S,U,e??????
?1(e) ?
?3(e?)
?
P ([e?e] | e)+?2(e) ?
?4(e?)
?
P (?ee??
| e)+?3(e) ?
?1(e?)
?
P ([ee?]
| e)+?4(e) ?
?2(e?)
?
P (?e?e?
| e)????
?So, on the right hand side, we sum up all possi-ble ways (S, U ) of splitting the left hand side celland all possible head words (e?)
for the non-headsubcell.
e, e?, s, t, u, v, S, and U all eight vari-ables take O(n) values given that the lengths ofthe source string and the target string are O(n).Thus the entire DP algorithm takes O(n8) steps.Fortunately, we can reduce the maximum num-ber of interacting variables by factorizing the ex-pression.Let us keep the results of the summations overe?
as:?+1 (e) =?e??1(e?)
?
P ([ee?]
| e)?+2 (e) =?e??2(e?)
?
P (?e?e?
| e)?+3 (e) =?e??3(e?)
?
P ([e?e] | e)?+4 (e) =?e??4(e?)
?
P (?ee??
| e)The computation of each ?+ involves fourboundary indices and two head words.
So, we canrely on DP to compute them in O(n6).
Based onthese intermediate results, we have the equivalentDP expression for computing inside probabilities:?
(e, s, t, u, v)=?S,U?????
?1(e) ?
?+3 (e)+ ?2(e) ?
?+4 (e)+ ?3(e) ?
?+1 (e)+ ?4(e) ?
?+2 (e)????
?We reduced one variable from the original ex-pression.
The maximum number of interactingvariables throughout the algorithm is 7.
So the im-proved inside algorithm has a time complexity ofO(n7).The trick of reducing interacting variables in DPfor bilexical parsing has been pointed out by Eis-ner and Satta (1999).
Melamed (2003) discussedthe applicability of the so-called hook trick forparsing bilexical multitext grammars.
The namehook is based on the observation that we combinethe non-head constituent with the bilexical rule tocreate a special constituent that matches the headlike a hook as demonstrated in Figure 2.
How-ever, for EM, it is not clear from their discussionshow we can do the hook trick in the outside pass.The bilexical rules in all four directions are anal-ogous.
To simplify the derivation for the outsidealgorithm, we just focus on the first case: straightrule with right head word.The outside probability of the constituent(e, S, t, U, v) in cell 1 being a head of such rulesis:?s,u,e?(?
(e) ?
?3(e?)
?
P ([e?e] | e))=?s,u(?
(e) ?(?e??3(e?)
?
P ([e?e] | e)))=?s,u(?
(e) ?
?+3 (e))which indicates we can reuse ?+ of the lower leftneighbors of the head to make the computationfeasible in O(n7).On the other hand, the outside probability for(e?, s, S, u, U) in cell 3 acting as a modifier of such956a rule is:?t,v,e(?
(e) ?
?1(e) ?
P ([e?e] | e))=?e?
?P ([e?e] | e) ????t,v?
(e) ?
?1(e)???
?=?e(P ([e?, e] | e) ?
?+3 (e))in which we memorize another kind of intermedi-ate sum to make the computation no more complexthan O(n7).We can think of ?+3 as the outside probabilityof the hook on cell 3 which matches cell 1.
Gener-ally, we need outside probabilities for hooks in allfour directions.
?+1 (e) =?s,u?
(e) ?
?3(e)?+2 (e) =?t,u?
(e) ?
?4(e)?+3 (e) =?t,v?
(e) ?
?1(e)?+4 (e) =?s,v?
(e) ?
?2(e)Based on them, we can add up the outside prob-abilities of a constituent acting as one of the twochildren of each applicable rule on top of it to getthe total outside probability.We finalize the derivation by simplifying the ex-pression of the expected count of (e ?
[e?e]).EC(e ?
[e?e])=?s,t,u,v,S,U(P ([e?e] | e) ?
?3(e?)
?
?
(e) ?
?1(e))=?s,S,u,U?
?P ([e?e] | e) ?
?3(e?)
????t,v?
?
?1???
?=?s,S,u,U(P ([e?e] | e) ?
?3(e?)
?
?+3 (e))which can be computed in O(n6) as long as wehave ?+3 ready in a table.
Overall we can do theinside-outside algorithm for the bilexical ITG inO(n7), by reducing a factor of n through interme-diate DP.The entire trick can be understood very clearlyif we imagine the bilexical rules are unary rulesthat are applied on top of the non-head con-stituents to reduce it to a virtual lexical constituent(a hook) covering the same subcell while sharingthe head word with the head constituent.
However,if we build hooks looking for all words in a sen-tence whenever a complete constituent is added tothe chart, we will build many hooks that are neverused, considering that the words outside of largercells are fewer and pruning might further reducethe possible outside words.
Blind guessing of whatmight appear outside of the current cell will off-set the saving we can achieve.
Instead of activelybuilding hooks, which are intermediate results, wecan build them only when we need them and thencache them for future use.
So the construction ofthe hooks will be invoked by the heads when theheads need to combine with adjacent cells.3.1 Pruning and SmoothingWe apply one of the pruning techniques used inZhang and Gildea (2005).
The technique is gen-eral enough to be applicable to any parsing algo-rithm over bitext cells.
It is called tic-tac-toe prun-ing since it involves an estimate of both the insideprobability of the cell (how likely the words withinthe box in both dimensions are to align) and theoutside probability (how likely the words outsidethe box in both dimensions are to align).
By scor-ing the bitext cells and throwing away the bad cellsthat fall out of a beam, it can reduce over 70% ofO(n4) cells using 10?5 as the beam ratio for sen-tences up to 25 words in the experiments, withoutharming alignment error rate, at least for the un-lexicalized ITG.The hook trick reduces the complexity of bilex-ical ITG from O(n8) to O(n7).
With the tic-tac-toe pruning reducing the number of bitext cells towork with, also due to the reason that the grammarconstant is very small for ITG.
the parsing algo-rithm runs with an acceptable speed,The probabilistic model has lots of parametersof word pairs.
Namely, there are O(|V |2) de-pendency probabilities and O(|V ||T |) translationprobabilities, where |V | is the size of English vo-cabulary and |T | is the size of the foreign lan-guage vocabulary.
The translation probabilities ofP (f |X(e)) are backed off to a uniform distribu-tion.
We let the bilexical dependency probabilitiesback off to uni-lexical dependencies in the follow-ing forms:P ([Y (?)
Z(e?)]
| X(?
))P ([Y (e?)
Z(?)]
| X(?
))P (?Y (?)
Z(e?)?
| X(?
))P (?Y (e?)
Z(?)?
| X(?
))95701002003004005006007000  5  10  15  20secondssentence lengthwithout-hookwith-hook0204060801001201400  5  10  15  20  25secondssentence lengthwithout-hookwith-hook(a) (b)Figure 3: Speedup for EM by the Hook Trick.
(a) is without pruning.
In (b), we apply pruning on thebitext cells before parsing begins.The two levels of distributions are interpolatedusing a technique inspired by Witten-Bell smooth-ing (Chen and Goodman, 1996).
We use the ex-pected count of the left hand side lexical nontermi-nal to adjust the weight for the EM-trained bilexi-cal probability.
For example,P ([Y (e) Z(e?)]
| X(e)) =(1 ?
?
)PEM ([Y (e) Z(e?)]
| X(e))+ ?P ([Y (?)
Z(e?)]
| X(?))where?
= 1/(1 + Expected Counts(X(e)))4 ExperimentsFirst of all, we are interested in finding out howmuch speedup can be achieved by doing the hooktrick for EM.
We implemented both versions inC++ and turned off pruning for both.
We ran thetwo inside-outside parsing algorithms on a smalltest set of 46 sentence pairs that are no longer than25 words in both languages.
Then we put the re-sults into buckets of (1 ?
4), (5 ?
9), (10 ?
14),(15?19), and (20?24) according to the maximumlength of two sentences in each pair and took av-erages of these timing results.
Figure 3 (a) showsclearly that as the sentences get longer the hooktrick is helping more and more.
We also tried toturn on pruning for both, which is the normal con-dition for the parsers.
Both are much faster dueto the effectiveness of pruning.
The speedup ratiois lower because the hooks will less often be usedagain since many cells are pruned away.
Figure 3(b) shows the speedup curve in this situation.We trained both the unlexicalized and the lex-icalized ITGs on a parallel corpus of Chinese-English newswire text.
The Chinese data wereautomatically segmented into tokens, and Englishcapitalization was retained.
We replaced wordsoccurring only once with an unknown word token,resulting in a Chinese vocabulary of 23,783 wordsand an English vocabulary of 27,075 words.We did two types of comparisons.
In the firstcomparison, we measured the performance of fiveword aligners, including IBM models, ITG, thelexical ITG (LITG) of Zhang and Gildea (2005),and our bilexical ITG (BLITG), on a hand-alignedbilingual corpus.
All the models were trained us-ing the same amount of data.
We ran the ex-periments on sentences up to 25 words long inboth languages.
The resulting training corpus had18,773 sentence pairs with a total of 276,113 Chi-nese words and 315,415 English words.For scoring the Viterbi alignments of each sys-tem against gold-standard annotated alignments,we use the alignment error rate (AER) of Ochand Ney (2000), which measures agreement at thelevel of pairs of words:AER = 1 ?
|A ?
GP | + |A ?
GS ||A| + |GS |where A is the set of word pairs aligned by theautomatic system, GS is the set marked in thegold standard as ?sure?, and GP is the set markedas ?possible?
(including the ?sure?
pairs).
In ourChinese-English data, only one type of alignmentwas marked, meaning that GP = GS .In our hand-aligned data, 47 sentence pairs areno longer than 25 words in either language andwere used to evaluate the aligners.A separate development set of hand-alignedsentence pairs was used to control overfitting.
Thesubset of up to 25 words in both languages wasused.
We chose the number of iterations for EM958AlignmentPrecision Recall Error RateIBM-1 .56 .42 .52IBM-4 .67 .43 .47ITG .68 .52 .41LITG .69 .51 .41BLITG .68 .51 .42DependencyPrecision Recall Error RateITG-lh .11 .11 .89ITG-rh .22 .22 .78LITG .13 .12 .88BLITG .24 .22 .77Table 1: Bilingual alignment and English dependency results on Chinese-English corpus (?
25 words onboth sides).
LITG stands for the cross-language Lexicalized ITG.
BLITG is the within-English BilexicalITG.
ITG-lh is ITG with left-head assumption on English.
ITG-rh is with right-head assumption.Precision Recall AERITG .59 .60 .41LITG .60 .57 .41BLITG .58 .55 .44Precision Recall DERITG-rh .23 .23 .77LITG .11 .11 .89BLITG .24 .24 .76Table 2: Alignment and dependency results on a larger Chinese-English corpus.training as the turning point of AER on the de-velopment data set.
The unlexicalized ITG wastrained for 3 iterations.
LITG was trained for only1 iteration, partly because it was initialized withfully trained ITG parameters.
BLITG was trainedfor 3 iterations.For comparison, we also included the resultsfrom IBM Model 1 and Model 4.
The numbersof iterations for the training of the IBM modelswere also chosen to be the turning points of AERchanging on the development data set.We also want to know whether or not BLITGcan model dependencies better than LITG.
Forthis purpose, we also used the AER measurement,since the goal is still getting higher precision/recallfor a set of recovered word links, although the de-pendency word links are within one language.
Forthis reason, we rename AER to Dependency ErrorRate.
Table 1(right) is the dependency results onEnglish side of the test data set.
The dependencyresults on Chinese are similar.The gold standard dependencies were extractedfrom Collins?
parser output on the sentences.
TheLITG and BLITG dependencies were extractedfrom the Viterbi synchronous trees by followingthe head words.For comparison, we also included two base-lineresults.
ITG-lh is unlexicalized ITG with left-headassumption, meaning the head words always comefrom the left branches.
ITG-rh is ITG with right-head assumption.To make more confident conclusions, we alsodid tests on a larger hand-aligned data set used inLiu et al (2005).
We used 165 sentence pairs thatare up to 25 words in length on both sides.5 DiscussionThe BLITG model has two components, namelythe dependency model on the upper levels of thetree structure and the word-level translation modelat the bottom.
We hope that the two componentswill mutually improve one another.
The currentexperiments indicate clearly that the word levelalignment does help inducing dependency struc-tures on both sides.
The precision and recall onthe dependency retrieval sub-task are almost dou-bled for both languages from LITG which onlyhas a kind of uni-lexical dependency in each lan-guage.
Although 20% is a low number, given thefact that the dependencies are learned basicallythrough contrasting sentences in two languages,the result is encouraging.
The results slightly im-prove over ITG with right-head assumption forEnglish, which is based on linguistic insight.
Ourresults also echo the findings of Kuhn (2004).They found that based on the guidance of wordalignment between English and multiple other lan-guages, a modified EM training for PCFG on En-glish can bootstrap a more accurate monolingualprobabilistic parser.
Figure 4 is an example of thedependency tree on the English side from the out-put of BLITG, comparing against the parser out-put.We did not find that the feedback from the de-959areaccomplishmentsEconomic reformbrightforcitiesChina?s14 open frontieraccomplishmentsEconomic reform frontieropen cities 14brightfor are?sChinaFigure 4: Dependency tree extracted from parser output vs. Viterbi dependency tree from BLITGpendencies help alignment.
To get the reasons, weneed further and deeper analysis.
One might guessthat the dependencies are modeled but are not yetstrong and good enough given the amount of train-ing data.
Since the training algorithm EM has theproblem of local maxima, we might also need toadjust the training algorithm to obtain good pa-rameters for the alignment task.
Initializing themodel with good dependency parameters is a pos-sible adjustment.
We would also like to point outthat alignment task is simpler than decoding wherea stronger component of reordering is required toproduce a fluent English sentence.
Investigatingthe impact of bilexical dependencies on decodingis our future work.Acknowledgments This work was supportedby NSF ITR IIS-09325646 and NSF ITR IIS-0428020.ReferencesAlbert V. Aho and Jeffery D. Ullman.
1972.
TheTheory of Parsing, Translation, and Compiling, vol-ume 1.
Prentice-Hall, Englewood Cliffs, NJ.Hiyan Alshawi, Srinivas Bangalore, and Shona Dou-glas.
2000.
Learning dependency translation mod-els as collections of finite state head transducers.Computational Linguistics, 26(1):45?60.Stanley F. Chen and Joshua Goodman.
1996.
An em-pirical study of smoothing techniques for languagemodeling.
In Proceedings of the 34th Annual Con-ference of the Association for Computational Lin-guistics (ACL-96), pages 310?318, Santa Cruz, CA.ACL.David Chiang.
2005.
A hierarchical phrase-basedmodel for statistical machine translation.
In Pro-ceedings of the 43rd Annual Conference of the As-sociation for Computational Linguistics (ACL-05),pages 263?270, Ann Arbor, Michigan.Jason Eisner and Giorgio Satta.
1999.
Efficient pars-ing for bilexical context-free grammars and head au-tomaton grammars.
In 37th Annual Meeting of theAssociation for Computational Linguistics.Jonas Kuhn.
2004.
Experiments in parallel-text basedgrammar induction.
In Proceedings of the 42nd An-nual Conference of the Association for Computa-tional Linguistics (ACL-04).Yang Liu, Qun Liu, and Shouxun Lin.
2005.
Log-linear models for word alignment.
In Proceedingsof the 43rd Annual Conference of the Associationfor Computational Linguistics (ACL-05), Ann Ar-bor, Michigan.I.
Dan Melamed.
2003.
Multitext grammars and syn-chronous parsers.
In Proceedings of the 2003 Meet-ing of the North American chapter of the Associ-ation for Computational Linguistics (NAACL-03),Edmonton.Franz Josef Och and Hermann Ney.
2000.
Improvedstatistical alignment models.
In Proceedings of the38th Annual Conference of the Association for Com-putational Linguistics (ACL-00), pages 440?447,Hong Kong, October.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?403.Kenji Yamada and Kevin Knight.
2001.
A syntax-based statistical translation model.
In Proceedingsof the 39th Annual Conference of the Associationfor Computational Linguistics (ACL-01), Toulouse,France.Richard Zens and Hermann Ney.
2003.
A compara-tive study on reordering constraints in statistical ma-chine translation.
In Proceedings of the 40th AnnualMeeting of the Association for Computational Lin-guistics, Sapporo, Japan.Hao Zhang and Daniel Gildea.
2004.
Syntax-basedalignment: Supervised or unsupervised?
In Pro-ceedings of the 20th International Conference onComputational Linguistics (COLING-04), Geneva,Switzerland, August.Hao Zhang and Daniel Gildea.
2005.
Stochastic lex-icalized inversion transduction grammar for align-ment.
In Proceedings of the 43rd Annual Confer-ence of the Association for Computational Linguis-tics (ACL-05), Ann Arbor, MI.960
