Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 278?282,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsBuilding Comparable Corpora Based on Bilingual LDA ModelZede ZhuUniversity of Science and Technologyof China, Institute of Intelligent Ma-chines Chinese Academy of SciencesHefei, Chinazhuzede@mail.ustc.edu.cnMiao Li, Lei Chen, Zhenxin YangInstitute of Intelligent Machines ChineseAcademy of SciencesHefei, Chinamli@iim.ac.cn,alan.cl@163.com,xinzyang@mail.ustc.edu.cnAbstractComparable corpora are important basic re-sources in cross-language information pro-cessing.
However, the existing methods ofbuilding comparable corpora, which use inter-translate words and relative features, cannotevaluate the topical relation between documentpairs.
This paper adopts the bilingual LDAmodel to predict the topical structures of thedocuments and proposes three algorithms ofdocument similarity in different languages.Experiments show that the novel method canobtain similar documents with consistent top-ics own better adaptability and stability per-formance.1 IntroductionComparable corpora can be mined fine-grainedtranslation equivalents, such as bilingual termi-nologies, named entities and parallel sentences,to support the bilingual lexicography, statisticalmachine translation and cross-language infor-mation retrieval (AbduI-Rauf et al, 2009).
Com-parable corpora are defined as pairs of monolin-gual corpora selected according to the criteria ofcontent similarity but non-direct translation indifferent languages, which reduces limitation ofmatching source language and target languagedocuments.
Thus comparable corpora have theadvantage over parallel corpora in which they aremore up-to-date, abundant and accessible (Ji,2009).Many works, which focused on the exploita-tion of building comparable corpora, were pro-posed in the past years.
Tao et al (2005) ac-quired comparable corpora based on the truththat terms are inter-translation in different lan-guages if they have similar frequency correlationat the same time periods.
Talvensaari et al (2007)extracted appropriate keywords from the sourcelanguage documents and translated them into thetarget language, which were regarded as the que-ry words to retrieve similar target documents.Thuy et al (2009) analyzed document similaritybased on the publication dates, linguistic inde-pendent units, bilingual dictionaries and wordfrequency distributions.
Otero et al (2010) tookadvantage of the translation equivalents insertedin Wikipedia by means of interlanguage links toextract similar articles.
Bo et al (2010) proposeda comparability measure based on the expecta-tion of finding the translation for each word.The above studies rely on the high coverage ofthe original bilingual knowledge and a specificdata source together with the translation vocabu-laries, co-occurrence information and languagelinks.
However, the severest problem is that theycannot understand semantic information.
Thenew studies seek to match similar documents ontopic level to solve the traditional problems.
Pre-iss (2012) transformed the source language topi-cal model to the target language and classifiedprobability distribution of topics in the same lan-guage, whose shortcoming is that the effect ofmodel translation seriously hampers the compa-rable corpora quality.
Ni et al (2009) adaptedmonolingual topic model to bilingual topic mod-el in which the documents of a concept unit indifferent languages were assumed to share iden-tical topic distribution.
Bilingual topic model iswidely adopted to mine translation equivalentsfrom multi-language documents (Mimno et al,2009; Ivan et al, 2011).Based on the bilingual topic model, this paperpredicts the topical structure of documents indifferent languages and calculates the similarityof topics over documents to build comparablecorpora.
The paper concretely includes: 1) Intro-duce the Bilingual LDA (Latent Dirichlet Alloca-tion) model  which builds comparable corporaand improves the efficiency of matching similardocuments; 2) Design a novel method of TFIDF(Topic Frequency-Inverse Document Frequency)to enhance the distinguishing ability of topicsfrom different documents; 3) Propose a tailored278method of conditional probability to calculatedocument similarity; 4) Address a language-independent study which isn?t limited to a par-ticular data source in any language.2 Bilingual LDA Model2.1 Standard LDALDA model (Blei et al, 2003) represents the la-tent topic of the document distribution by Di-richlet distribution with a K-dimensional implicitrandom variable, which is transformed into acomplete generative model when ?
is exerted toDirichlet distribution (Griffiths et al, 2004)(Shown in Fig.
1),?
m?
,m n?
,m n?
?
[1, ]mn N?
[1, ]m M?k?
[1, ]k K?Figure 1: Standard LDA modelwhere ?
and ?
denote the parameters distribut-ed by Dirichlet; K denotes the topic numbers; k?
denotes the vocabulary probability distribution inthe topic k; M denotes the document number; m?
denotes the topic probability distribution in thedocument m; Nm denotes the length of m; ,m n?
and ?m,n denote the topic and the word in m re-spectively.2.2 Bilingual LDABilingual LDA is a bilingual extension of astandard LDA model.
It takes advantage of thedocument alignment which shares the same topicdistribution m?
and uses different word distribu-tions for each topic (Shown in Fig.
2), where Sand T denote source language and target lan-guage respectively.
?,Sm n?,Tm n?,Sm n?,Tm n?
Tk?Sk?
S?m?
[1, ]m M?
[1, ]S Smn N?
[1, ]T Tmn N?
[1, ]k K?T?Figure 2: Bilingual LDA modelFor each language l ( { , }l S T?
), ,lm n?
and,lm n?
are drawn using , ( | )l lm n n mP ??
??
and, ,( | , )l l l lm n n m nP?
?
???
.Giving the comparable corpora M, the distri-bution ,k v?
can be obtained by sampling a newtoken as word v from a topic k. For new collec-tion of documents M?
, keeping ,k v?
, the distri-bution ,lm k?
?
of sampling a topic k from documentm?
can be obtained as follows:( ), ( )1( | )( )lllkkl mk Km k kkmknP mn??????
?
???????
,     (1)where ( )lkmn?
denotes the total number of times that the document m?
is assigned to the topic k.3 Building comparable corporaBased on the bilingual LDA model, buildingcomparable corpora includes several steps togenerate the bilingual topic model ,k v?
from the given bilingual corpora, predict the topic distri-bution ,lm k?
?
of the new documents, calculate thesimilarity of documents and select the largestsimilar document pairs.
The key step is that thedocument similarity is calculated to align thesource language document Sm?
with relevanttarget language document Tm?
.As one general way of expressing similarity,the Kullback-Leibler (KL) Divergence is adoptedto measure the document similarity by topic dis-tributions ?
,Sm k?
and ?
,Tm k?
as follows:?
?
??
?, , ,1( , ) [ ( | ), ( | )]log .
(2)S S TS T S TKLKm k m k m kkSim m m KL P m P m?
?
???
?
??
??
?
??
???
?
?
?The remainder section focuses on other twomethods of calculating document similarity.3.1 Cosine SimilarityThe similarity between Sm?
and Tm?
can be meas-ured by Topic Frequency-Inverse DocumentFrequency.
It gives high weights to the topicwhich appears frequently in a specific documentand rarely appears in other documents.
Then therelation between ,SmTFIDF ??
and ,TmTFIDF ??
ismeasured by Cosine Similarity (CS).Similar to Term Frequency-Inverse DocumentFrequency (Manning et al,1999), Topic Fre-quency (TF) denoting frequency of topic ?
forthe document lm?
is denoted by ( | )lP m?
?
.
Givena constant value?
, Inverse Document Frequency(IDF) is defined as the total number of docu-ments M?
divided by the number of documents279: ( | )l lm P m ??
??
?
containing a particular topic, and then taking the logarithm, which is calculat-ed as follows:log1 : ( | )l lMIDF m P m ??
?
?
???
?
.
(3)The TFIDF is calculated as follows:*( | ) log1 : ( | )ll lTFIDF TF IDFMP m m P m ???
?
?
?
???
?
?.
(4)Thus, the TFIDF score of the topic k overdocument lm?
is given by:,,,( | ) log 1 : ( | )log .
(5)1 :lllm klk l lkm k lm kTFIDFMP m m P mMm??
?
??
?
?
?
??
?
??????
?
??
?The similarity between Sm?
and Tm?
is given by:, ,, ,12 2, ,1 1( , ) ( , ).
(6)S TS TS TS TCS m mKm k m kkK Km k m kk kSim m m Cos TFIDF TFIDFTFIDF TFIDFTFIDF TFIDF?
???
?????
??
??
??
??
?3.2 Conditional ProbabilityThe similarity between Sm?
and Tm?
is defined asthe Conditional Probability (CP) of documents( | )T SP m m?
?
that Tm?
will be generated as a re-sponse to the cue Sm?
.
( )P ?
as prior topic distribution is assumed auniform distribution and satisfied the condition( ) ( )kP P?
?
?
.
According to the total probabil-ity formula, the document Tm?
is given as:11( ) ( | ) ( )( ) ( | ).KT Tk kkK TkkP m P m PP P m???
?
??
?
????
??
(7)Based on the Bayesian formula, the probabil-ity that a given topic ?
is assigned to a particu-lar target language document Tm?
is expressed:1( | ) ( | ) ( ) ( )= ( | ) ( | ).T T TKT TkkP m P m P m PP Z m P m??
??
?
?
??
????
?
??
?
(8)The sum of all probabilities1( | )K TkkP m???
?that all topics ?
are assigned to a particular doc-ument Tm?
is a constant?
, thus equation (8) isconverted as follows:( | ) ( | )T TP m P m?
?
?
??
?
.
(9)According to the total probability formula, thesimilarity between Sm?
and Tm?
is given by:?
?11, ,1(10)( , ) ( | )[ ( | ) ( | )][ ( | ) ( | )][ ].S TS T T SCPK T Sk kk K T Sk kkKm k m kkSim m m P m mP m P mP m P m?
??????
?
??
?
?
??
?????
?
?
??
??
?4 Experiments and analysis4.1 Datasets and EvaluationThe experiments are conducted on two sets ofChinese-English comparable corpora.
The firstdataset is news corpora with 3254 comparabledocument pairs, from which 200 pairs are ran-domly selected as the test dataset News-Test andthe remainder is the training dataset News-Train.The second dataset contains 8317 bilingual Wik-ipedia entry pairs, from which 200 pairs are ran-domly selected as the test dataset Wiki-Test andthe remainder is the training dataset Wiki-Train.Then News-Train and Wiki-Train are mergedinto the training dataset NW-Train.
And thehand-labeled gold standard namely NW-Test iscomposed of News-Test and Wiki-Test.Braschler et al (1998) used five levels of rele-vance to assess the alignments as follows: SameStory, Related Story, Shared Aspect, CommonTerminology and Unrelated.
The paper selectsthe documents with Same Story and Related Sto-ry as comparable corpora.
Let Cp be the compa-rable corpora in the building result and Cl be thecomparable corpora in the labeled result.
ThePrecision (P), Recall (R) and F-measure (F) aredefined as:= ,p l p llpC C C CP R CC ??
?
, 2PRF P R?
?
.
(11)4.2 Results and analysisTwo groups of validation experiments are setwith sampling frequency of 1000, parameter ?280of 50/K, parameter ?
of 0.01 and topic number K of 600.Group 1: Different data sourceWe learn bilingual LDA models by taking differ-ent training datasets.
The performance of threeapproaches (KL, CS and CP) is examined on dif-ferent test datasets.
Tab.
1 demonstrates theseresults with the winners for each algorithm inbold.Train Test KL CS CP P F P F P FNews News 0.62 0.52 0.73 0.59 0.69 0.56News Wiki 0.60 0.47 0.68 0.56 0.66 0.52Wiki News 0.61 0.48 0.71 0.58 0.68 0.55Wiki Wiki 0.63 0.50 0.75 0.60 0.71 0.59NW NW 0.66 0.55 0.76 0.62 0.73 0.60Table 1: Sensitivity of Data SourceThe results indicate the robustness and effec-tiveness of these algorithms.
The performance ofalgorithms on Wiki-Train is much better thanNews-Train.
The main reason is that Wiki-Trainis an extensive snapshot of human knowledgewhich can cover most topics talked in News-Train.
The probability of vocabularies among thetest dataset which have not appeared in the train-ing data is very low.
And then the document top-ic can effectively concentrate all the vocabular-ies?
expressions.
The topic model slightly faceswith the problem of knowledge migration issue,so the performance of the topic model trained byWiki-Train shows a slight decline in the experi-ments on News-Test.CS shows the strongest performance amongthe three algorithms to recognize the documentpairs with similar topics.
CP has almost equiva-lent performance with CS.
Comparing the equa-tion (5) and (6) with (10), we can find out thatCP is similar to a simplified CS.
CP can improvethe operating efficiency and decrease the perfor-mance.
The performance achieved by KL is theweakest and there is a large gap between KL andothers.
In addition, the shortage of KL is thatwhen the exchange between the source languageand the target language documents takes place,different evaluations will occur in the same doc-ument pairs.Group 2: Existing Methods ComparisonWe adopt the NW-Train and NW-Test as trainingset and test set respectively, and utilize the CSalgorithm to calculate the document similarity toverify the excellence of methods in the study.Then we compare its performance with the exist-ing representative approaches proposed by Thuyet al (2009) and Preiss (2012) (Shown in Tab.
2).Algorithm P R FThuy 0.45 0.32 0.37Preiss 0.67 0.44 0.53CS 0.76 0.53 0.62Table 2: Existing Methods ComparisonThe table shows CS outperforms other algo-rithms, which indicates that bilingual LDA isvalid to construct comparable corpora.
Thuy et al(2009) matches similar documents in the view ofinter-translated vocabulary and co-occurrenceinformation features, which cannot understandthe content effectively.
Preiss (2012) uses mono-lingual training dataset to generate topic modeland translates source language topic model intotarget language topic model respectively.
Yet thetranslation accuracy constrains the matching ef-fectiveness of similar documents, and the cosinesimilarity is directly used to calculate document-topic similarity failing to highlight the topic con-tributions of different documents.5 ConclusionThis study proposes a new method of using bi-lingual topic to match similar documents.
WhenCS is used to match the documents, TFIDF isproposed to enhance the topic discrepanciesamong different documents.
The method of CP isalso addressed to measure document similarity.Experimental results show that the matchingalgorithm is superior to the existing algorithms.It can utilize comprehensively large scales ofdocument information in training set to avoid theinformation deficiency of the document itself andover-reliance on bilingual knowledge.
The algo-rithm makes the document match on the basis ofunderstanding the document.
This study does notcalculate similar contents existed in the monolin-gual documents.
However, a large number ofdocuments in the same language describe thesame event.
We intend to incorporate monolin-gual document similarity into bilingual topicsanalysis to match multi-documents in differentlanguages perfectly.AcknowledgmentsThe work is supported by the National NaturalScience Foundation of China under No.61070099 and the project of MSR-CNIC Win-dows Azure Theme.281ReferencesAbduI-Rauf S, Schwenk H. On the use of comparablecorpora to improve SMT perfor-mance[C]//Proceedings of the 12th Conference ofthe European Chapter of the Association for Com-putational Linguistics.
Association for Computa-tional Linguistics, 2009: 16-23.Ji H. Mining name translations from comparable cor-pora by creating bilingual information networks[C]// Proceedings of BUCC 2009.
Suntec, Singapore,2009: 34-37.Braschler M, Schauble P. Multilingual InformationRetrieval based on document alignment tech-niques[C] // Proceedings of the Second EuropeanConference on Research and Advanced Technolo-gy for Digital Libraries.
Heraklion, Greece.
1998:183-197.Tao Tao, Chengxiang Zhai.
Mining comparable bilin-gual text corpora for cross-language informationintegration[C] // Proceedings of ACM SIGKDD,Chicago, Illinois, USA.
2005:691-696.Talvensaari T, Laurikkala J, Jarvelin K, et al Creatingand Exploiting a Comparable Corpus in Cross-Language Information Retrieval[J].
ACM Transac-tions on Information Systems.
2007, 25(1): 322-334.Thuy Vu, Ai Ti Aw, Min Zhang.
Feature-based meth-od for document alignment in comparable newscorpora[C] // Proceedings of the 12th Conferenceof the European Chapter of the ACL, Athens,Greece.
2009: 843-851.Otero P G, L?opez I G. Wikipedia as MultilingualSource of Comparable Corpora[C] // Proceedingsof the 3rd Workshop on BUCC, LREC2010.
Malta.2010: 21-25.Li B, Gaussier E. Improving corpus comparability forbilingual lexicon extraction from comparable cor-pora[C]//Proceedings of the 23rd InternationalConference on Computational Linguistics.
Associ-ation for Computational Linguistics, 2010: 644-652.Judita Preiss.
Identifying Comparable Corpora UsingLDA[C]//2012 Conference of the North AmericanChapter of the Association for Computational Lin-guistics: Human Language Technologies.
Mon-tre?al, Canada, June 3-8, 2012: 558-562.Mimno D, Wallach H, Naradowsky J et al Polylin-gual topic models[C]//Proceedings of the EMNLP.Singapore, 2009: 880-889.Vulic I, De Smet W, Moens M F, et al Identifyingword translations from comparable corpora usinglatent topic models[C]//Proceedings of ACL.
2011:479-484.Ni X, Sun J T, Hu J, et al Mining multilingual topicsfrom wikipedia[C]//Proceedings of the 18th inter-national conference on World wide web.
ACM,2009: 1155-1156.Blei D M, Ng A Y, Jordan M I.
Latent dirichlet alo-cation[J].
the Journal of machine Learning research,2003, 3: 993-1022.Griffiths T L, Steyvers M. Finding scientific topics[J].Proceedings of the National academy of Sciencesof the United States of America, 2004, 101: 5228-5235.Manning C D, Sch?tze H. Foundations of statisticalnatural language processing[M].
MIT press, 1999.282
