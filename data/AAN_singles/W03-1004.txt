Sentence Alignment for Monolingual Comparable CorporaRegina BarzilayDepartment of Computer ScienceCornell UniversityIthaca, NY 14853regina@cs.cornell.eduNoemie ElhadadDepartment of Computer ScienceColumbia UniversityNew York, NY 10027noemie@cs.columbia.eduAbstractWe address the problem of sentence align-ment for monolingual corpora, a phe-nomenon distinct from alignment in par-allel corpora.
Aligning large compara-ble corpora automatically would providea valuable resource for learning of text-to-text rewriting rules.
We incorporate con-text into the search for an optimal align-ment in two complementary ways: learn-ing rules for matching paragraphs usingtopic structure and further refining thematching through local alignment to findgood sentence pairs.
Evaluation showsthat our alignment method outperformsstate-of-the-art systems developed for thesame task.1 IntroductionText-to-text generation is an emerging area of re-search in NLP (Chandrasekar and Bangalore, 1997;Caroll et al, 1999; Knight and Marcu, 2000; Jingand McKeown, 2000).
Unlike in traditional concept-to-text generation, text-to-text generation applica-tions take a text as input and transform it into a newtext satisfying specific constraints, such as length insummarization or style in text simplification.
Oneexciting new research direction is the automatic in-duction of such transformation rules.
This is a par-ticularly promising direction given that there are nat-urally occurring examples of comparable texts thatconvey the same information yet are written in dif-ferent styles.
Presented with two such texts, onecan pair sentences that convey the same information,thereby building a training set of rewriting examplesfor the domain to which the texts belong.
We believethat automating this process will provide researchersin text-to-text generation with valuable training andtesting resources, just as techniques to align multi-lingual parallel corpora boosted research in MachineTranslation (MT).In this paper, we address the task of aligning sen-tences in text pairs.
We focus on monolingual com-parable corpora, that is, texts in the same language(e.g., English) that overlap in the information theyconvey.
Stories about the same events from differentpress agencies and texts presenting the same infor-mation to experts and lay people are two examples.In MT, the task of sentence alignment was exten-sively studied for parallel corpora.1 A typical sen-tence alignment algorithm can be roughly describedas a two-step process: (1) for each sentence paircompute a local similarity value, independently ofthe other sentences; (2) find an overall sequence ofmapped sentences, using both the local similarityvalues and additional features.In the case of monolingual corpora, step (2) mightseem unnecessary.
Since the texts share the samelanguage, it would be enough to choose for localsimilarity a function based on lexical cues only andselect sentence pairs with high lexical similarity.Even a simple lexical function (e.g., one that countsword overlap) could produce an accurate alignment.1Sentence alignment for comparable multilingual corporawas not addressed in previous research.
Comparable cor-pora have primarily been used to build bilingual lexical re-sources (Fung and Yee, 1998).After all, two sentences which share most of theirwords are likely to paraphrase each other.
The prob-lem is that there are many sentences that conveythe same information but have little surface resem-blance.
As a result, simple word counts cannot dis-tinguish the matching pair (A) in Figure 1 from theunrelated pair (B).
An accurate local similarity mea-sure would have to account for many complex para-phrasing phenomena.
A simple, weak lexical simi-larity function alone is not sufficient.(A)?
Petersburg served as the capital of Russia for 200years.?
For two centuries Petersburg was the capital of theRussian Empire.(B)?
The city is also the country?s leading port and centerof commerce.?
And yet, as with so much of the city, the port facili-ties are old and inefficient.Figure 1: Sentence pairs from our corpus sharingtwo content words.
(A) is a matching pair, (B) is not.In MT, a weak similarity function is compensatedfor by searching for a globally optimal alignment,using dynamic programming or taking advantageof the geometric/positional or contextual propertiesof the text pair (Gale and Church, 1991; Shemtov,1993; Melamed, 1999).
But these techniques oper-ate on the assumptions that there are limited inser-tions and deletions between the texts and that theorder of the information is roughly preserved fromone text to another.Texts from comparable corpora, as opposed toparallel corpora, contain a great deal of ?noise.?
InFigure 2 which plots the manually identified align-ment for a text pair in our corpus, only a small frac-tion of the sentences got aligned (35 out of 31 ?270 sentence pairs), which illustrates that there is nocomplete information overlap.
Consider two textswritten by different press agencies: while both re-port on the same events, one may contain additionalinterviews and the other, background information.Another distinguishing characteristic of comparablecorpora is that the order in which the informationis presented can differ greatly from one text to an-other.
Analysis of comparable texts in different do-mains (Paris, 1993; Barzilay et al, 2002) showedthat there is wide variability in the order in whichthe same information can be presented.
This is alsoillustrated in Figure 2.0501001502002500 5 10 15 20 25 30SentencesinText 2Sentences in Text 1Manual AlignmentFigure 2: Manual alignment for a text pair in ourcorpus.
A point in (x,y) indicates that the sentencesx and y match.We investigate a novel approach informed by textstructure for sentence alignment.
Our method em-phasizes the search for an overall alignment, whilerelying on a simple local similarity function.
Weincorporate context into the search process in twocomplementary ways: (1) we map large text frag-ments using hypotheses learned in a supervised fash-ion and (2) we further refine the match through lo-cal alignment within mapping fragments to find sen-tence pairs.
When the documents in the collectionbelong to the same domain and genre, the fragmentmapping takes advantage of the topical structure ofthe texts.
This structure is derived in an unsuper-vised fashion by analyzing commonalities amongtexts on each side of the comparable corpora sep-arately.
Our experiments show that our overall ap-proach identifies even pairs with low lexical sim-ilarity.
We also found that a fully unsupervisedmethod using a minimalist representation of contex-tual information, viz., paragraph-level lexical simi-larity, outperforms existing methods based on com-plex local similarity functions.In the next section, we provide an overview ofexisting work on monolingual sentence alignment.Section 3 describes our algorithm.
In sections 4and 5, we report on data collection and evaluation.2 Related WorkMost of the work in monolingual corpus alignmentis in the context of summarization.
In single docu-ment summarization, alignment between full docu-ments and summaries written by humans is used tolearn rules for text compression.
Marcu (1999) com-putes sentence similarity using a cosine-based met-ric.
Jing (2002) identifies phrases that were cut andpasted together using a Hidden Markov Model withfeatures incorporating word identity and positioningwithin sentences, thereby providing an alignment ofthe document and its summary.
However, both ofthese methods construct an alignment by looking atsentences one at a time, independently of the de-cisions made about other sentences.
Because sum-maries often reuse original document text to a largeextent, these methods achieve good results.In the context of multidocument summarization,SimFinder (Hatzivassiloglou et al, 1999) identifiessentences that convey similar information across in-put documents to select the summary content.
Eventhough the input documents are about the same sub-ject, they exhibit a great deal of lexical variability.To address this issue, SimFinder employs a com-plex similarity function, combining features that ex-tend beyond a simple word count and include nounphrase, proper noun, and WordNet sense overlap.Since many documents are processed in parallel,clustering is used to combine pairwise alignments.In contrast to our approach, SimFinder does not takethe context around sentences into account.3 AlgorithmGiven a comparable corpus consisting of two collec-tions and a training set of manually aligned text pairsfrom the corpus, the algorithm follows four mainsteps.
Steps 1 and 2 take place at training time.
Steps3 and 4 are carried out when a new text pair (Text1,Text2) is to be aligned.1.
Topical structure induction: by analyzingmultiple instances of paragraphs within thetexts of each collection, the topics characteris-tic of the collections are identified through clus-tering.
Each paragraph in the training set getsassigned the topic it verbalizes (Section 3.1.1.)2.
Learning of structural mapping rules: us-ing the training set, rules for mapping para-graphs are learned in a supervised fashion (Sec-tion 3.1.2).3.
Macro alignment: given a new unseen pair(Text1, Text2), each paragraph is automati-cally assigned its topic.
Paragraphs are mappedfollowing the learned rules (Section 3.2).4.
Micro alignment: for each mapped paragraphpair, a local alignment is computed at the sen-tence level.
The final alignment for the text pairis the union of all the aligned sentence pairs(Section 3.3).3.1 Off-Line ProcessingGiven two sentences with moderate lexical similar-ity, we may not have enough evidence to decide ac-curately whether they should be aligned.
Looking atthe broader context they appear in can provide addi-tional insight: if the types of information expressedin the contexts are similar, then the specific infor-mation expressed in the sentences is more likely tobe the same.
On the other hand, if the types of in-formation in the two contexts are unrelated, chancesare that the sentences should not be aligned.
In ourimplementation, context is represented by the para-graphs to which the sentences belong.2 Our goal inthis phase is to learn rules for determining whethertwo paragraphs are likely to contain sentences thatshould be aligned, or whether, on the contrary, twoparagraphs are unrelated and, therefore, should notbe considered for further processing.A potentially fruitful way to do so is to take ad-vantage of the topical structure of texts.
In a givendomain and genre, while the texts relate differentsubjects, they all use a limited set of topics to con-vey information; these topics are also known as theDomain Communication Knowledge (Kittredge etal., 1991).
For instance, most texts describing dis-eases will have topics such as ?symptoms?
or ?treat-ment.
?3 If the task is to align a disease descriptionwritten for physicians and a text describing the samedisease for lay people, it is most likely that sentenceswithin the topic ?symptoms?
in the expert versionwill map to sentences describing the symptoms inthe lay version rather than those describing treat-ment options.
If we can automatically identify thetopic each paragraph conveys, we can decide moreaccurately whether two paragraphs are related andshould be mapped for further processing.2Texts without adequate paragraph marking could be seg-mented using tools such as TextTiling (Hearst, 1994).3We use the term topic differently than it is commonly usedin the topic detection task?
there, a ?topic?
would designatewhich disease is described.In the field of text generation, methods forrepresenting the semantic structure of texts havebeen investigated through text schemata (McKeown,1985) or rhetorical structures (Mann and Thompson,1987).
In our framework, we want to identify thedifferent topics of the text, but we are not concernedwith the relations holding between them or the orderin which they typically appear.
We propose to iden-tify the topics typical to each collection in the com-parable corpus by using clustering, such that eachcluster represents a topic in the collection.The process of learning paragraph mapping rulesis accomplished in two stages: first, we identify thetopics of each collection, Corpus1 and Corpus2,and label each paragraph with its specific topic.
Sec-ond, using a training set of manually aligned textpairs, we learn rules for mapping paragraphs fromCorpus1 to Corpus2.
Two paragraphs are consid-ered mapped if they are likely to contain sentencesthat should be aligned.3.1.1 Vertical Paragraph ClusteringWe perform a clustering at the paragraph level foreach collection.
We call this stage Vertical Clus-tering because all the paragraphs of all the doc-uments in Corpus1 get clustered, independentlyof Corpus2; the same goes for the paragraphs inCorpus2.
At this stage, we are only interested inidentifying the topics of the texts in each collection,each cluster representing a topic.We apply a hierarchical complete link clustering.Similarity is a simple cosine measure based on theword overlap of the paragraphs, ignoring functionwords.
Since we want to group together paragraphsthat convey the same type of information across thedocuments in the same collection, we replace all thetext-specific attributes, such as proper names, datesand numbers, by generic tags.4 This way, we ensurethat two paragraphs are clustered not because theyrelate the same specific information, but rather, be-cause they convey the same type of information (anexample of two automatically clustered paragraphsis shown in Figure 3).
The number of clusters foreach collection is a parameter tuned on our trainingset (see Section 4).4We crudely consider any words with a capital letter a propername, except for each sentence?s first word.Lisbon has a mild and equable climate, with a mean annualtemperature of 63 degree F (17 degree C).
The proximity ofthe Atlantic and the frequency of sea fogs keep the atmospherehumid, and summers can be somewhat oppressive, althoughthe city has been esteemed as a winter health resort since the18th century.
Average annual rainfall is 26.6 inches (666 mil-limetres).Jakarta is a tropical, humid city, with annual temperaturesranging between the extremes of 75 and 93 degree F (24 and34 degree C) and a relative humidity between 75 and 85 per-cent.
The average mean temperatures are 79 degree F (26 de-gree C) in January and 82 degree F (28 degree C) in October.The annual rainfall is more than 67 inches (1,700 mm).
Tem-peratures are often modified by sea winds.
Jakarta, like anyother large city, also has its share of air and noise pollution.Figure 3: Two automatically clustered paragraphs inthe same collection (without date, number, and namesubstitution).Cluster1Text TextPar.2Par.1Par.3Corpus1 Corpus2Par.3Par.1Par.2ClusterEClusterBText Textj1 j2i1 i2Figure 4: The training set for the paragraph mappingstep.
An arrow between two paragraphs indicatesthey contain at least one aligned sentence pair.3.1.2 Horizontal Paragraph MappingOnce the different topics, or clusters, are identi-fied inside each collection, we can use this informa-tion to learn rules for paragraph mapping (Horizon-tal Mapping between texts from Corpus1 and textsfrom Corpus2).
Using a training set of text pairs,manually aligned at the sentence level, we considertwo paragraphs to map each other if they contain atleast one aligned sentence pair (see Figure 4).Our problem can be framed as a classificationtask: given training instances of paragraph pairs (P ,Q) from a text pair, classify them as mapping or not.The features for the classification are the lexical sim-ilarity of P and Q, the cluster number of P , and thecluster number of Q.
Here, similarity is again a sim-ple cosine measure based on the word overlap of thetwo paragraphs.5 These features are weak indicatorsby themselves.
Consequently, we use the publicly-available classification tool BoosTexter (Singer andSchapire, 1998) to combine them accurately.63.2 Macro Alignment: Find CandidateParagraph(s)At this stage, the clustering and training are com-pleted.
Given a new unseen text pair (Text1,Text2), the goal is to find a sentence alignment be-tween them.
Two sentences with very high lexicalsimilarity are likely to be aligned.
We allow suchpairs in the alignment independently of their context.This step allows us to catch the ?easy?
paraphrases.We focus next on how our algorithm identifies theless obvious matching sentence pairs.For each paragraph in each text, we identify thecluster in its collection it is the closest to.
Similaritybetween the paragraph and each cluster is computedthe same way as in the Vertical Clustering step.
Wethen apply mapping classification to find the map-ping paragraphs in the text pair (see Figure 5).Text 2Text 1Cluster GPar.
13Cluster EPar.
7Cluster BPar.
2Par.
2Cluster 3Figure 5: Macro Alignment: a paragraph in Text1and its mapped candidate paragraphs in Text2.3.3 Micro Alignment: Find Sentence Pair(s)Once the paragraph pairs are identified in (Text1,Text2), we want to find, for each paragraph pair,the (possibly empty) subsets of sentence pairs whichconstitute a good alignment.
Context is used in thefollowing way: given two sentences with moder-ate similarity, their proximity to sentence pairs withhigh similarity can help us decide whether to alignthem or not.To combine the lexical similarity (again using co-sine measure) and the proximity feature, we com-5At this stage, we want to match on text-specific informa-tion, unlike in the Vertical Clustering.
We therefore use theoriginal text, without any substitution, to compute the similarity.6Because BoosTexter cannot form conjunctive hypotheses,we add a feature which encodes the combination of two clusternumbers.pute local alignments on each paragraph pair, us-ing dynamic programming.
The local alignment weconstruct fits the characteristics of the data we areconsidering.
In particular, we adapt it to our frame-work to allow many-to-many alignments and someflips of order among aligned sentences.
Given sen-tences i and j, their local similarity sim(i, j) is:sim(i, j) = cos(i, j) ?
mismatch penaltyThe weight s(i, j) of the optimal alignment betweenthe two sentences is computed as follows:s(i, j) = max?????????
?s(i, j?1) ?
skip penaltys(i?1, j) ?
skip penaltys(i?1, j?1) + sim(i, j)s(i?1, j?2) + sim(i, j) + sim(i, j?1)s(i?2, j?1) + sim(i, j) + sim(i?1, j)s(i?2, j?2) + sim(i, j?1) + sim(i?1, j)The mismatch penalty penalizes sentence pairs withvery low similarity measure, while the skip penaltyprevents only sentence pairs with high similarityfrom getting aligned.4 Evaluation SetupThe Data.
We compiled two collections from theEncyclopedia Britannica and Britannica Elementary.In contrast to the long (up to 15-page) detailed arti-cles of the Encyclopedia Britannica, Britannica Ele-mentary contains one- to two-page entries targetedtowards children.
The elementary version gener-ally contains a subset of the information presented inthe comprehensive version, but there are numerouscases when the elementary entry contains additionalor more up-to-date pieces of information.7 The twocollections together exhibit many instances of com-plex rewriting.We collected 103 pairs of comprehensive/ele-mentary city descriptions.
We set aside a testing setof 11 text pairs.
The rest (92 pairs) was used for theVertical Clustering.
Nine text pairs were used fortraining (see Table 1 for statistics).Human Annotation.
Each text pair in the train-ing and testing sets was annotated by two annota-tors.8 In our guidelines to them, we defined two sen-tences as aligned if they contain at least one clause7Britannica Elementary is a new feature of the encyclopedia,not all entries in the original Britannica have been fully updated.8All the annotators were native speakers of English.
Theauthors did not take part in the annotation.Sentences ParagraphsMin Max Avg Min Max AvgComp.
Train 87 313 180 19 59 37Comp.
Test 138 308 200 32 63 43Elem.
Train 34 64 47 8 12 10Elem.
Test 27 75 45 6 16 10Table 1: Statistics for the training and testing sets forthe comprehensive and elementary versions.Range Training Testing0%?40% 149 (46.6%) 127 (45.2%)40%?70% 103 (32.2%) 96 (34.2%)70%?100% 68 (21.2%) 58 (20.6%)Table 2: Distribution of manually aligned sentencepairs among different similarity ranges.that expresses the same information.
We allowedmany-to-many alignments.
On average, each anno-tator spent 50 minutes per text pair.
While the an-notators agreed for most of the sentence pairs theyidentified, there were some cases of disagreement.Alignment is a tedious task, and sentence pairs caneasily be missed even by a careful human annotator.For each text pair, a third annotator went throughcontested sentence pairs, deciding on a case-by-casebasis whether to include it in the alignment.
Over-all, 320 sentence pairs were aligned in the trainingset and 281 in the testing set.
The other sentencepairs which were not aligned served as negative ex-amples, yielding a total of 4192 training instancesand 3884 testing instances.9As a confirmation that there is no order preserva-tion in comparable corpora, there were up to nine or-der shifts in each of the annotated text pairs.
Table 2shows that a large fraction of manually aligned sen-tence pairs have low lexical similarity.
Similarity ismeasured here by the number of words in common,normalized by the number of types in the shortersentence.Parameter Tuning.
We tuned all the parameterson our training set, obtaining the following values:the skip penalty is 0.001, and the cosine thresholdfor selecting pairs with high lexical similarity is 0.5.BoosTexter was trained in 200 iterations.
To find theoptimal number of clusters for each collection, Ver-tical Clustering was performed with different num-bers of clusters, ranging from 10 to 40; we selected9Our corpus is available at http://www.cs.columbia.edu/?noemie/alignment.0.10.20.30.40.50.60.70.80.910.1 0.2 0.3 0.4 0.5 0.6 0.7PrecisionRecallfullSimFindercosinedecompositionFigure 6: Precision/Recall for SimFinder, Cosine,Decomposition, and our full method.the alternatives with the best performance on thetraining set: 20 for both collections.5 ResultsWe first present the comparison of our method withother systems developed for the same task.
Next wefocus on the impact of individual components on theperformance of our method.5.1 Comparison with Other SystemsAn obvious choice for a baseline in this task is thefollowing: any two sentences are considered alignedif their cosine similarity exceeds a certain threshold.We also compare our algorithm with two state-of-the-art systems, SimFinder (Hatzivassiloglou et al,1999) and Decomposition (Jing, 2002).10 Figure 6shows Precision/Recall curves for the different sys-tems.
For our full system, we obtain different valuesof Recall keeping constant the skip penalty and thecosine threshold and varying the value of the mis-match penalty from 0 to 0.45.11 This setup resultsin recall values in a 25%?65% range.
The curve forSimFinder was obtained by running SimFinder withdifferent similarity thresholds, ranging from 0.1 to0.95.
In the case of Decomposition, there are sev-eral hard-coded parameters which are not trainable.As a result, we were able to obtain results for De-composition only at a 55.8% Recall level.
Table 3reports Precision values at this level of Recall.10Jing (2002) reports that Decomposition outperforms the al-gorithm of Marcu (1999); we, therefore, did not compare ourmethod against his system.11Varying the mismatch penalty is a natural choice: varyingthe skip penalty produces a narrow range of Recall values, whilethe cosine threshold controls only a small portion of the sen-tence pairs that can be identified (the ones with high similarity).System PrecisionSimFinder 24%Cosine 57.9%Decomposition 64.3%Without Topic Mapping 71.6%Without Local Alignment 73.3%Full Method 76.9%Full Method (with ideal clusters) 80.9%Table 3: Precision at 55.8% Recall.
(*) Gradually the German culture and language became morewidespread in the city.Capping Prague?s rebirth, it was designated a EuropeanCity of Culture in 2000.Prague is a centuries-old city with a wealth of historiclandmarks.The physical attractions and landmarks of Prague aremany.Figure 7: Aligned pairs, (*) denotes an incorrectalignment.Our full method outperforms both the base-line (?Cosine?)
and the more complex systems(?SimFinder?
and ?Decomposition?).
Interestingly,methods that use simple local similarity functionssignificantly outperform SimFinder (SimFinder wastrained on newswire texts; we did not have accessto SimFinder?s training component for retraining onour corpus).
This confirms our hypothesis that whileit is an appealing idea, putting all one?s eggs in thebasket of a sophisticated local similarity measure toachieve good performance may be too hard a task.The simple cosine baseline is competitive with theHidden Markov Model of Decomposition (Decom-position was specifically developed to identify sen-tence pairs with cut-and-paste transformations, notall possible paraphrase pairs).
This suggests thatwhen looking for an alignment, Cosine is a good,yet simple, starting local similarity measure.
Addingon top of it an explicit search mechanism relyingon the context surrounding the sentences, as in ourmethod, results in a performance improvement of19% at 55.8% Recall.
Figure 7 shows examples ofpairs identified by our method.5.2 AnalysisImpact of Horizontal Paragraph Mapping.
Wehypothesize that exploiting the regularity in map-ping between semantic units, such as topics, im-proves the alignment.
We compared the perfor-mance of our full method with a variation that doesnot take any topical information into account.
Forthe paragraph mapping, we replaced the learnedrules by a single rule based on lexical similarity:two paragraphs are mapped if their cosine measureis above a pre-specified threshold.12 This new map-ping is a good point of comparison because it doesnot rely on any knowledge inferred from the othertexts in the corpus.
The results confirm our hypoth-esis: learning paragraph mapping based on topicalstructures improves the performance (see ?WithoutTopic Mapping?
vs. ?Full Method?, Table 3).This experiment also shows that representing con-text, even simply using only the paragraphs and theirlexical similarity, achieves higher performance thanmethods based on more complex local similarityfunctions.
It is an important finding, because thissimplified method can be used when topic struc-ture cannot be derived (e.g., in heterogeneous col-lections) or when no training data is available, sinceit is unsupervised.Impact of Cluster Quality.
Our method usesclustering to identify the different topics of eachcollection.
It is important to know how sensitiveour overall algorithm is to the quality of the iden-tified clusters.
Fortunately, in our corpus, some ofthe texts contain section headings (e.g., ?Climate?or ?Demography?).
Even though our method ig-nores this piece of information, we used it to derivemanually ?ideal?
clusters.13 We obtained eight clus-ters for the elementary version and 11 for the com-prehensive one.
When feeding these ideal clustersinstead of the automatically identified ones to thelearning module for paragraph mapping, we achieve4% improvement in Precision (at 55.8% Recall).
Weinterpret this as a sign that the algorithm handles im-perfect, automatically induced clusters fairly well.Impact of Local Alignment.
Our hypothesisfor computing local alignments between pairs ofmapped paragraphs is that our approach allows us toidentify additional matching sentence pairs: if twosentences paraphrase each other but have a low co-sine measure, looking at the sentence pairs around12The threshold was tuned on our training data.13The process was performed manually because the sectionsare different from one text to another, both in names and levelsof detail, and because we needed to infer clusters for the para-graphs that did not have section headings.Full Dec. Cos.Range Prec.
Rec.
Prec.
Rec.
Prec.
Rec.0%?40% 50% 25% 34% 28% 23% 15%40%?70% 85% 73% 82% 74% 66% 86%70%?100% 95% 95% 93% 88% 90% 95%Table 4: Precision and Recall for different rangesof lexical similarity for Decomposition, Cosine, andour full method.them may increase their chances of getting selected.We compared our full method with a version ofour algorithm that does not perform local alignment(?Without Local Alignment?).
Instead, it simply se-lects sentence pairs inside the mapped paragraphsbased on their cosine measure.
This incomplete ver-sion of the algorithm achieves 73.3% Precision (at55.8% Recall), 3% lower than the full method, vali-dating our hypothesis.Impact of Lexical Similarity.
We investigatedhow the performance of our method depends on thelexical similarity of the input sentences.
Table 4shows Precision and Recall for our method and oth-ers at three sentence-similarity ranges based on wordoverlap counts (at the overall Recall of 55.8%).
Ourmethod outperforms the Cosine baseline and De-composition on all similarity ranges.6 ConclusionsThe central finding of our work is that context playsan important role in the task of sentence alignmentfor monolingual comparable corpora.
A weak sen-tence similarity measure combined with contextualinformation outperforms methods based on sophis-ticated sentence similarity functions.
Experimentsshow that a simple representation of context is help-ful.
Relying on a more elaborate representation,such as topical text structure, has an even strongerimpact on performance.AcknowledgmentsWe would like to thank Michael Elhadad, Klara Kedem, KathyMcKeown and Becky Passonneau for the useful discussions.Thanks to Mirella Lapata, Lillian Lee, Vincent Ng and Bo Pangfor their helpful comments on previous drafts.
We are gratefulto Hongyan Jing for giving us access to her code for Decompo-sition.
Finally we would like to thank all the annotators (they aremany).
This paper is based upon work supported in part by theNational Science Foundation under ITR/IM grant IIS-0081334,Digital Library Initiative Phase II Grant No.
IIS-98-17434, anda Sloan Research Fellowship to Lillian Lee.
Any opinions, find-ings, and conclusions or recommendations expressed above arethose of the authors and do not necessarily reflect the views ofthe National Science Foundation or the Sloan Foundation.ReferencesRegina Barzilay, Noemie Elhadad, and Kathleen McKeown.2002.
Inferring strategies for sentence ordering in multi-document news summarization.
Journal of Artificial Intel-ligence Research, 17:35?55.John Carroll, Guido Minnen, Darren Pearce, Yvonne Canning,Siobhan Devlin, and John Tait.
1999.
Simplifying text forlanguage-impaired readers.
In Proceedings of EACL.Raman Chandrasekar and Srinivas Bangalore.
1997.Automatic Induction of Rules for Text Simplification.Knowledge-Based Systems, 10(3):183?190.Pascale Fung and Lo Yuen Yee.
1998.
An IR approach fortranslating new words from nonparallel, comparable texts.In Proceedings of ACL.William Gale and Kenneth Church.
1991.
A program for align-ing sentences in bilingual corpora.
In Proceedings of ACL.Vasileios Hatzivassiloglou, Judith Klavans, and Eleazar Eskin.1999.
Detecting text similarity over short passages: Explor-ing linguistic feature combinations via machine learning.
InProceedings of EMNLP.Marti Hearst.
1994.
Multi-paragraph segmentation of exposi-tory text.
In Proceedings of ACL.Hongyan Jing and Kathleen McKeown.
2000.
Cut and pastebased summarization.
In Proceedings of NAACL.Hongyan Jing.
2002.
Using hidden Markov modeling to de-compose human-written summaries.
Computational Lin-guistics, 28(4):527?543.Richard Kittredge, Tanya Korelsky, and Owen Rambow.
1991.On the need for domain communication knowledge.
Com-putational Intelligence, 7(4):305?314.Kevin Knight and Daniel Marcu.
2000.
Statistics-based sum-marization - step one: Sentence compression.
In Proceed-ings of AAAI.William Mann and Sandra Thompson.
1987.
Rhetorical struc-ture theory: description and construction of text structures.In G. Kempen, editor, Natural Language Generation: RecentAdvances in Artificial Intelligence, Psychology, and Linguis-tics.
Kluwer Academic.Daniel Marcu.
1999.
The automatic construction of large-scalecorpora for summarization research.
In Proceedings of SI-GIR.Kathleen McKeown.
1985.
Text Generation.
Cambridge Uni-versity Press.Dan Melamed.
1999.
Bitext maps and alignment via patternrecognition.
Computational Linguistics, 25(1):107?130.Cecile Paris.
1993.
User Modelling in Text Generation.Frances Pinter Publishers.Hadar Shemtov.
1993.
Text alignment in a tool for translatingrevised documents.
In Proceedings of EACL.Yoram Singer and Robert Schapire.
1998.
Improved boostingalgorithms using confidence-rated predictions.
In Proceed-ings of COLT.
