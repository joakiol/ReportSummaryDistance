Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 181?184,Sydney, July 2006. c?2006 Association for Computational LinguisticsChinese Word Segmentation and Named Entity Recognition Based onConditional Random Fields ModelsYuanyong Feng Le Sun Yuanhua LvInstitute of Software, Chinese Academy of Sciences, Beijing, 100080, China{yuanyong02, sunle, yuanhua04}@ios.cnAbstractThis paper mainly describes a Chinesenamed entity recognition (NER) systemNER@ISCAS, which integrates text,part-of-speech and a small-vocabulary-character-lists feature for MSRA NERopen track under the framework of Con-ditional Random Fields (CRFs) model.The techniques used for the close NERand word segmentation tracks are alsopresented.1 IntroductionThe system NER@ISCAS is designed under theConditional Random Fields (CRFs.
Lafferty etal., 2001) framework.
It integrates multiple fea-tures based on single Chinese character or spaceseparated ASCII words.
The early designed sys-tem (Feng et al, 2005) is used for the MSRANER open track this year.
The output of an ex-ternal part-of-speech tagging tool and some care-fully collected small-scale-character-lists areused as outer knowledge.The close word segmentation and named en-tity recognition tracks are also based on this sys-tem by some adjustments.The remaining of this paper is organized asfollows.
Section 2 introduces Conditional Ran-dom Fields model.
Section 3 presents the detailsof our system on Chinese NER integrating mul-tiple features.
Section 4 describes the featuresextraction for close track.
Section 5 gives theevaluation results.
We end our paper with someconclusions and future works.2 Conditional Random Fields ModelConditional random fields are undirected graphi-cal models for calculating the conditional prob-ability for output vertices based on input ones.While sharing the same exponential form withmaximum entropy models, they have more effi-cient procedures for complete, non-greedy finite-state inference and training.Given an observation sequence o=<o1, o2, ...,oT>, linear-chain CRFs model based on the as-sumption of first order Markov chains definesthe corresponding state sequence s?
probability asfollows (Lafferty et al, 2001):111( | ) exp( ( , , , ))Tk k t tt kp f s s tZ??
?== ?
?os o o(1)Where ?
is the model parameter set, Zo is thenormalization factor over all state sequences, fk isan arbitrary feature function, and ?k is the learnedfeature weight.
A feature function defines itsvalue to be 0 in most cases, and to be 1 in somedesignated cases.
For example, the value of afeature named ?MAYBE-SURNAME?
is 1 ifand only if st-1 is OTHER, st is PER, and the t-thcharacter in o is a common-surname.The inference and training procedures ofCRFs can be derived directly from those equiva-lences in HMM.
For instance, the forward vari-able ?t(si) defines the probability that state attime t being si at time t given the observationsequence o.
Assumed that we know the proba-bilities of each possible value si for the beginningstate  ?0(si), then we have1( ) ( ) exp( ( , , , )t i t k k is ks s f s s t?
?
?+??
?= ?
?
o (2)In similar ways, we can obtain the backwardvariables and Baum-Welch algorithm.3 Chinese NER Using CRFs Model Inte-grating Multiple Features for OpenTrackIn our system the text feature, part-of-speech(POS) feature, and small-vocabulary-character-lists (SVCL) feature are combined under a uni-fied CRFs framework.181The text feature includes single Chinese char-acter, some continuous digits or letters.POS feature is an important feature which car-ries some syntactic information.
Our POS tag setfollows the criterion of modern Chinese corporaconstruction (Yu, 1999), which contains 39 tags.The last feature is based on lists.
We first listall digits and English letters in Chinese.
Thenmost frequently used character feature in ChineseNER are collected, including 100 single charac-ter surnames, 100 location tail characters, and 40organization tail characters.
The total number ofthese items in our lists is less than 600.
The listsaltogether make up a list feature (SVCL).
Someexamples of this list are given in Table 1.Each token is presented by its feature vector,which is combined by these features we just dis-cussed.
Once all token feature (Maybe includingcontext features) values are determined, an ob-servation sequence is feed into the model.Each token state is a combination of the typeof the named entity it belongs to and the bound-ary type it locates within.
The entity types areperson name (PER), location name (LOC), or-ganization name (ORG), date expression (DAT),time expression (TIM), numeric expression(NUM), and not named entity (OTH).
Theboundary types are simply Beginning, Inside,and Outside (BIO).4 Feature Extraction for Close TracksIn close tracks, only character and word list fea-tures which are extracted from training data areapplied for word segmentation.
In NER track wealso include a named entity list extracted fromthe training data.To extract the list feature, we simply searcheach text string among the list items in maximumlength forward way.Taking the word segmentation task for in-stance, when a text string c1c2?cn is given, wetag each character into a BIO-WL style.
Ifcici+1?cj matches an item I of length j-i+1 and noother item I?
of length k (k>j-i+1) in the listmatches cici+1?cj?ck+i-1, then the characters aretagged as follows:ci ci+1 ?
cjB-WL I-WL ?
I-WLIf no item in the list matches head subpart ofthe string, then ci is tagged as 0.The tagging operation iterates on theremaining part until all characters are tagged.5 Evaluation5.1 ResultsThe system for our MSRA NER open tracksubmission has some bugs and was trained on amuch smaller training data set than the full setthe organizer provided.
The results are very low,see Table 2:Accuracy 96.28%Precision 83.20%Recall 67.03%FB1 74.24%Table 2.
MSRA NER OpenWhen we fixed the bug and retrained on thefull training corpus, the result comes out to be asfollows:Accuracy 98.24%Precision 89.38%Recall 83.07%FB1 86.11%Table 3.
MSRA NER Open (retrained)All the submissions on close tracks are trainedon 80% of the training corpora, the remaining20% parts are used for development.
The resultsare shown in Table 4 and Table 5:Value Description Examplesdigit Arabic digit(s) 1,2,3letter Letter(s) A,B,C,...,a, b, cContinuous digits and/or letters (The sequence isregarded as a single token)chseq Chinese order 1 ?
?
?
?, , ,chdigit Chinese digit ?
?
?, ,tianseq Chinese order 2 ?
?
?
?, , ,chsurn Surname ?
?
?
?, , ,notname Not name ?
?
?
?
?
?, , , , ,loctch LOC tail char-acter?
?
?
?
?, , , , ,?
?,orgtch ORG tail char-acter?
?
?
?
?, , , , ,?, ?other Other case ?
?, ?, ,   ?, ?Table 1.
Some Examples of SVCL Feature182Corpus MeasureUPUC  CityU  CKIP MSRARecall 0.922 0.952 0.939 0.933Precision 0.912 0.954 0.929 0.942FB1 0.917 0.953 0.934 0.937OOVRecall 0.680 0.747 0.606 0.640IV Recall 0.945 0.960 0.954 0.943Table 4.
WS CloseMeasure MSRA CityU LDCAccuracy 92.44 97.80 93.82Precision 81.64 92.76 81.43Recall 31.24 81.81 59.53FB1 45.19 86.94 68.78Table 5.
NER CloseThe reason for low measure on MSRA NERtrack exists in that we chose a much smallertraining data file encoded in CP936 (about 7% ofthe full data set).
This file may be an incompleteoutput when the organizer transfers from anotherencoding scheme.5.2 Errors  from NER TrackThe NER errors in our system are mainly as fol-lows:?
AbbreviationsAbbreviations are very common among the er-rors.
Among them, a significant part of abbrevia-tions are mentioned before their correspondingfull names.
Some common abbreviations has nocorresponding full names appeared in document.Here are some examples:R1:??????????
?
?
???[????????????
ORG][??
GPE]?[??
GPE]?????????K:??????????
[?
GPE][?
GPE]???[????????????
ORG][??
GPE]?[??GPE]????????
?R: ??[????
LOC]????
?K: [??????
LOC]????
?R: [?
?
LOC]?
?K: [?
LOC][?
LOC]?
?In current system, the recognition is fully de-pended on the linear-chain CRFs model, which isheavily based on local window observation fea-tures; no abbreviation list or special abbreviation1 R stands for system response, K for key.recognition involved.
Because lack of constraintchecking on distant entity mentions, the systemfails to catch the interaction among similar textfragments cross sentences.?
Concatenated NamesFor many reasons, Chinese names in titles andsome sentences, especially in news, are not sepa-rated.
The system often fails to judge the rightboundaries and the reasonable type classification.For example:R:????[??
??
PER]??[??
PER] ????K:????[??
PER][??PER][??
PER][??
PER] ????R:?[????
LOC]?[????PER]??K:?[????
PER]?[????PER]???
HintsThough it helps to recognize an entity at mostcases, the small-vocabulary-list hint feature mayrecommend a wrong decision sometimes.
Forinstance, common surname character ???
in thefollowing sentence is wrongly labeled when noword segmentation information given:R:[??
LOC]?[?
????????
PER]K:[??
LOC]?
?[????????
PER]Other errors of this type may result from fail-ing to identify verbs and prepositions, such as:R:[????
?
????????ORG]??????[???
ORG]??????K:[????
ORG]?[????????
ORG]??????[???
ORG]??????R:???????????
?????K:[???????????
ORG]?????R:??
??K:[??
PER] ???
Other Types:R:????
???
???????K:????[???
PER]???????R:?????
?
?
???183K:?????[?
LOC][?
LOC]??
?6 Conclusions and Future WorkWe mainly described a Chinese named entityrecognition system NER@ISCAS, which inte-grates text, part-of-speech and a small-vocabulary-character-lists feature for MSRANER open track under the framework of Condi-tional Random Fields (CRFs) model.
Although itprovides a unified framework to integrate multi-ple flexible features, and to achieve global opti-mization on input text sequence, the popular lin-ear chained Conditional Random Fields modeloften fails to catch semantic relations among re-occurred mentions and adjoining entities in acatenation structure.The situations containing exact reoccurrenceand shortened occurrence enlighten us to takemore effort on feature engineering or post proc-essing on abbreviations / recurrence recognition.Another effort may be poured on the commonpatterns, such as paraphrase, counting, and con-straints on Chinese person name lengths.From current point of view, enriching the hintlists is also desirable.AcknowledgmentThis work is supported by the National ScienceFund of China under contract 60203007.ReferencesChinese 863 program.
2005.
Results on NamedEntity Recognition.
The 2004HTRDP ChineseInformation Processing and Intelligent Hu-man-Machine Interface Technology Evalua-tion.Yuanyong Feng, Le Sun and Junlin Zhang.
2005.Early Results for Chinese Named Entity Rec-ognition Using Conditional Random FieldsModel, HMM and Maximum Entropy.
IEEENatural Language Processing & KnowledgeEngineering.
Beijing: Publishing House,BUPT.
pp.
549~552.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional Random Fields:Probabilistic Models for Segmenting and La-beling Sequence Data.
ICML.Shiwen Yu.
1999.
Manual on Modern ChineseCorpora Construction.
Institute of Computa-tional Language, Peking Unversity.
Beijing.184
