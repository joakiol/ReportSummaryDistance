Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1098?1107,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsSimple, Accurate Parsing with an All-Fragments GrammarMohit Bansal and Dan KleinComputer Science DivisionUniversity of California, Berkeley{mbansal,klein}@cs.berkeley.eduAbstractWe present a simple but accurate parserwhich exploits both large tree fragmentsand symbol refinement.
We parse withall fragments of the training set, in con-trast to much recent work on tree se-lection in data-oriented parsing and tree-substitution grammar learning.
We re-quire only simple, deterministic grammarsymbol refinement, in contrast to recentwork on latent symbol refinement.
More-over, our parser requires no explicit lexi-con machinery, instead parsing input sen-tences as character streams.
Despite itssimplicity, our parser achieves accuraciesof over 88% F1 on the standard EnglishWSJ task, which is competitive with sub-stantially more complicated state-of-the-art lexicalized and latent-variable parsers.Additional specific contributions center onmaking implicit all-fragments parsing effi-cient, including a coarse-to-fine inferencescheme and a new graph encoding.1 IntroductionModern NLP systems have increasingly used data-intensive models that capture many or even allsubstructures from the training data.
In the do-main of syntactic parsing, the idea that all train-ing fragments1 might be relevant to parsing has along history, including tree-substitution grammar(data-oriented parsing) approaches (Scha, 1990;Bod, 1993; Goodman, 1996a; Chiang, 2003) andtree kernel approaches (Collins and Duffy, 2002).For machine translation, the key modern advance-ment has been the ability to represent and memo-rize large training substructures, be it in contigu-ous phrases (Koehn et al, 2003) or syntactic trees1In this paper, a fragment means an elementary tree in atree-substitution grammar, while a subtree means a fragmentthat bottoms out in terminals.
(Galley et al, 2004; Chiang, 2005; Deneefe andKnight, 2009).
In all such systems, a central chal-lenge is efficiency: there are generally a combina-torial number of substructures in the training data,and it is impractical to explicitly extract them all.On both efficiency and statistical grounds, muchrecent TSG work has focused on fragment selec-tion (Zuidema, 2007; Cohn et al, 2009; Post andGildea, 2009).At the same time, many high-performanceparsers have focused on symbol refinement ap-proaches, wherein PCFG independence assump-tions are weakened not by increasing rule sizesbut by subdividing coarse treebank symbols intomany subcategories either using structural anno-tation (Johnson, 1998; Klein and Manning, 2003)or lexicalization (Collins, 1999; Charniak, 2000).Indeed, a recent trend has shown high accura-cies from models which are dedicated to inducingsuch subcategories (Henderson, 2004; Matsuzakiet al, 2005; Petrov et al, 2006).
In this paper,we present a simplified parser which combines thetwo basic ideas, using both large fragments andsymbol refinement, to provide non-local and lo-cal context respectively.
The two approaches turnout to be highly complementary; even the simplest(deterministic) symbol refinement and a basic useof an all-fragments grammar combine to give ac-curacies substantially above recent work on tree-substitution grammar based parsers and approach-ing top refinement-based parsers.
For example,our best result on the English WSJ task is an F1of over 88%, where recent TSG parsers2 achieve82-84% and top refinement-based parsers3 achieve88-90% (e.g., Table 5).Rather than select fragments, we use a simplifi-cation of the PCFG-reduction of DOP (Goodman,2Zuidema (2007), Cohn et al (2009), Post and Gildea(2009).
Zuidema (2007) incorporates deterministic refine-ments inspired by Klein and Manning (2003).3Including Collins (1999), Charniak and Johnson (2005),Petrov and Klein (2007).10981996a) to work with all fragments.
This reductionis a flexible, implicit representation of the frag-ments that, rather than extracting an intractablylarge grammar over fragment types, indexes allnodes in the training treebank and uses a com-pact grammar over indexed node tokens.
This in-dexed grammar, when appropriately marginalized,is equivalent to one in which all fragments are ex-plicitly extracted.
Our work is the first to applythis reduction to full-scale parsing.
In this direc-tion, we present a coarse-to-fine inference schemeand a compact graph encoding of the training set,which, together, make parsing manageable.
Thistractability allows us to avoid selection of frag-ments, and work with all fragments.Of course, having a grammar that includes alltraining substructures is only desirable to the ex-tent that those structures can be appropriatelyweighted.
Implicit representations like thoseused here do not allow arbitrary weightings offragments.
However, we use a simple weight-ing scheme which does decompose appropriatelyover the implicit encoding, and which is flexibleenough to allow weights to depend not only on fre-quency but also on fragment size, node patterns,and certain lexical properties.
Similar ideas havebeen explored in Bod (2001), Collins and Duffy(2002), and Goodman (2003).
Our model empir-ically affirms the effectiveness of such a flexibleweighting scheme in full-scale experiments.We also investigate parsing without an explicitlexicon.
The all-fragments approach has the ad-vantage that parsing down to the character levelrequires no special treatment; we show that an ex-plicit lexicon is not needed when sentences areconsidered as strings of characters rather thanwords.
This avoids the need for complex un-known word models and other specialized lexicalresources.The main contribution of this work is to showpractical, tractable methods for working with anall-fragments model, without an explicit lexicon.In the parsing case, the central result is that ac-curacies in the range of state-of-the-art parsers(i.e., over 88% F1 on English WSJ) can be ob-tained with no sampling, no latent-variable mod-eling, no smoothing, and even no explicit lexicon(hence negligible training overall).
These tech-niques, however, are not limited to the case ofmonolingual parsing, offering extensions to mod-els of machine translation, semantic interpretation,and other areas in which a similar tension existsbetween the desire to extract many large structuresand the computational cost of doing so.2 Representation of Implicit Grammars2.1 All-Fragments GrammarsWe consider an all-fragments grammar G (seeFigure 1(a)) derived from a binarized treebankB.
G is formally a tree-substitution grammar(Resnik, 1992; Bod, 1993) wherein each subgraphof each training tree in B is an elementary tree,or fragment f , in G. In G, each derivation d isa tree (multiset) of fragments (Figure 1(c)), andthe weight of the derivation is the product of theweights of the fragments: ?
(d) =?f?d ?(f).
Inthe following, the derivation weights, when nor-malized over a given sentence s, are interpretableas conditional probabilities, soG induces distribu-tions of the form P (d|s).In models like G, many derivations will gen-erally correspond to the same unsegmented tree,and the parsing task is to find the tree whosesum of derivation weights is highest: tmax =arg maxt?d?t ?(d).
This final optimization is in-tractable in a way that is orthogonal to this pa-per (Sima?an, 1996); we describe minimum Bayesrisk approximations in Section 4.2.2 Implicit Representation of GExplicitly extracting all fragment-rules of a gram-marG is memory and space intensive, and imprac-tical for full-size treebanks.
As a tractable alter-native, we consider an implicit grammar GI (seeFigure 1(b)) that has the same posterior probabil-ities as G. To construct GI , we use a simplifi-cation of the PCFG-reduction of DOP by Good-man (1996a).4 GI has base symbols, which arethe symbol types from the original treebank, aswell as indexed symbols, which are obtained byassigning a unique index to each node token inthe training treebank.
The vast majority of sym-bols in GI are therefore indexed symbols.
Whileit may seem that such grammars will be overlylarge, they are in fact reasonably compact, beinglinear in the treebank size B, while G is exponen-tial in the length of a sentence.
In particular, wefound that GI was smaller than explicit extractionof all depth 1 and 2 unbinarized fragments for our4The difference is that Goodman (1996a) collapses ourBEGIN and END rules into the binary productions, giving alarger grammar which is less convenient for weighting.1099!SYMBOLS: X, for all types in treebank "RULES: X?#, for all fragments in "!
$SYMBOLS:?Base: X   for all types in treebank "?Indexed: Xi for all tokens of X in "RULES:?Begin: ;?
;i for all Xi in "?Continue: Xi?<j Zk for all rule-tokens in "?End: Xi?;IRUDOO;i in " %$FRAGMENTSDERIVATIONS(a)(b)GRAMMAR%#$AXAlCONTINUEENDXiZkYjBEGINBBmCCn#AXZYB CCBAXwordsXCBAwordsEXPLICITIMPLICITMAP  ?Figure 1: Grammar definition and sample derivations and fragments in the grammar for (a) the explicitly extracted all-fragmentsgrammar G, and (b) its implicit representation GI .treebanks ?
in practice, even just the raw treebankgrammar grows almost linearly in the size of B.5There are 3 kinds of rules inGI , which are illus-trated in Figure 1(d).
The BEGIN rules transitionfrom a base symbol to an indexed symbol and rep-resent the beginning of a fragment from G. TheCONTINUE rules use only indexed symbols andcorrespond to specific depth-1 binary fragment to-kens from training trees, representing the internalcontinuation of a fragment in G. Finally, ENDrules transition from an indexed symbol to a basesymbol, representing the frontier of a fragment.By construction, all derivations in GI will seg-ment, as shown in Figure 1(d), into regions corre-sponding to tokens of fragments from the trainingtreebank B.
Let pi be the map which takes appro-priate fragments in GI (those that begin and endwith base symbols and otherwise contain only in-dexed symbols), and maps them to the correspond-ing f in G. We can consider any derivation dI inGI to be a tree of fragments f I , each fragment atoken of a fragment type f = pi(f I) in the orig-inal grammar G. By extension, we can thereforemap any derivation dI in GI to the correspondingderivation d = pi(dI) in G.The mapping pi is an onto mapping from GI to5Just half the training set (19916 trees) itself had 1.7 mil-lion depth 1 and 2 unbinarized rules compared to the 0.9 mil-lion indexed symbols in GI (after graph packing).
Even ex-tracting binarized fragments (depth 1 and 2, with one orderof parent annotation) gives us 0.75 million rules, and, practi-cally, we would need fragments of greater depth.G.
In particular, each derivation d in G has a non-empty set of corresponding derivations {dI} =pi?1(d) in GI , because fragments f in d corre-spond to multiple fragments f I in GI that differonly in their indexed symbols (one f I per occur-rence of f in B).
Therefore, the set of derivationsin G is preserved in GI .
We now discuss howweights can be preserved under pi.2.3 Equivalence for Weighted GrammarsIn general, arbitrary weight functions ?
on frag-ments in G do not decompose along the increasedlocality of GI .
However, we now consider a use-fully broad class of weighting schemes for whichthe posterior probabilities under G of derivationsd are preserved in GI .
In particular, assume thatwe have a weighting ?
on rules in GI which doesnot depend on the specific indices used.
There-fore, any fragment f I will have a weight in GI ofthe form:?I(fI) = ?BEGIN(b)?r?C?CONT(r)?e?E?END(e)where b is the BEGIN rule, r are CONTINUE rules,and e are END rules in the fragment f I (see Fig-ure 1(d)).
Because ?
is assumed to not depend onthe specific indices, all f I which correspond to thesame f under pi will have the same weight ?I(f)in GI .In this case, we can define an induced weight1100XiBEGINAXAlCONTINUEENDZkYjBmwordDOP1MIN-FRAGMENTS OUR MODEL!!"
#$ !"%#$%!!
!CONTINUERULE TYPES WEIGHTSFigure 2: Rules defined for grammar GI and weight schemafor the DOP1 model, the Min-Fragments model (Goodman(2003)) and our model.
Here s(X) denotes the total numberof fragments rooted at base symbol X .for fragments f in G by?G(f) =?fI?pi?1(f)?I(fI) = n(f)?I(f)= n(f)?BEGIN(b?)?r??C?CONT(r?)?e??E?END(e?
)where now b?, r?
and e?
are non-indexed type ab-stractions of f ?s member productions in GI andn(f) = |pi?1(f)| is the number of tokens of f inB.Under the weight function ?G(f), any deriva-tion d in G will have weight which obeys?G(d) =?f?d?G(f) =?f?dn(f)?I(f)=?dI?d?I(dI)and so the posterior P (d|s) of a derivation d fora sentence s will be the same whether computedin G or GI .
Therefore, provided our weightingfunction on fragments f in G decomposes overthe derivational representation of f in GI , we canequivalently compute the quantities we need forinference (see Section 4) using GI instead.3 Parameterization of ImplicitGrammars3.1 Classical DOP1The original data-oriented parsing model ?DOP1?
(Bod, 1993) is a particular instance of the generalweighting scheme which decomposes appropri-ately over the implicit encoding, described in Sec-tion 2.3.
Figure 2 shows rule weights for DOP1in the parameter schema we have defined.
TheEND rule weight is 0 or 1 depending on whetherA is an intermediate symbol or not.6 The localfragments in DOP1 were flat (non-binary) so thisweight choice simulates that property by not al-lowing switching between fragments at intermedi-ate symbols.The original DOP1 model weights a fragment fin G as ?G(f) = n(f)/s(X), i.e., the frequencyof fragment f divided by the number of fragmentsrooted at base symbol X .
This is simulated by ourweight choices (Figure 2) where each fragment f IinGI has weight ?I(f I) = 1/s(X) and therefore,?G(f) =?fI?pi?1(f) ?I(fI) = n(f)/s(X).Given the weights used for DOP1, the recursiveformula for the number of fragments s(Xi) rootedat indexed symbol Xi (and for the CONTINUE ruleXi ?
Yj Zk) iss(Xi) = (1 + s(Yj))(1 + s(Zk)), (1)where s(Yj) and s(Zk) are the number of frag-ments rooted at indexed symbols Yj and Zk (non-intermediate) respectively.
The number of frag-ments s(X) rooted at base symbol X is thens(X) =?Xis(Xi).Implicitly parsing with the full DOP1 model (nosampling of fragments) using the weights in Fig-ure 2 gives a 68% parsing accuracy on the WSJdev-set.7 This result indicates that the weight of afragment should depend on more than just its fre-quency.3.2 Better ParameterizationAs has been pointed out in the literature, large-fragment grammars can benefit from weights offragments depending not only on their frequencybut also on other properties.
For example, Bod(2001) restricts the size and number of wordsin the frontier of the fragments, and Collins andDuffy (2002) and Goodman (2003) both givelarger fragments smaller weights.
Our model canincorporate both size and lexical properties.
Inparticular, we set ?CONT(r) for each binary CON-TINUE rule r to a learned constant ?BODY, and weset the weight for each rule with a POS parent to a6Intermediate symbols are those created during binariza-tion.7For DOP1 experiments, we use no symbol refinement.We annotate with full left binarization history to imitate theflat nature of fragments in DOP1.
We use mild coarse-passpruning (Section 4.1) without which the basic all-fragmentschart does not fit in memory.
Standard WSJ treebank splitsused: sec 2-21 training, 22 dev, 23 test.1101Rule score: r(A?
B C, i, k, j) =?x?y?zO(Ax, i, j)?
(Ax ?
By Cz)I(By, i, k)I(Cz, k, j)Max-Constituent: q(A, i, j) =?x O(Ax,i,j)I(Ax,i,j)?r I(rootr,0,n)tmax = argmaxt?c?tq(c)Max-Rule-Sum: q(A?
B C, i, k, j) = r(A?B C,i,k,j)?r I(rootr,0,n)tmax = argmaxt?e?tq(e)Max-Variational: q(A?
B C, i, k, j) = r(A?B C,i,k,j)?x O(Ax,i,j)I(Ax,i,j)tmax = argmaxt?e?tq(e)Figure 3: Inference: Different objectives for parsing with posteriors.
A, B, C are base symbols, Ax, By , Cz are indexedsymbols and i,j,k are between-word indices.
Hence, (Ax, i, j) represents a constituent labeled with Ax spanning words ito j. I(Ax, i, j) and O(Ax, i, j) denote the inside and outside scores of this constituent, respectively.
For brevity, we writec ?
(A, i, j) and e ?
(A?
B C, i, k, j).
Also, tmax is the highest scoring parse.
Adapted from Petrov and Klein (2007).constant ?LEX (see Figure 2).
Fractional values ofthese parameters allow the weight of a fragment todepend on its size and lexical properties.Another parameter we introduce is a?switching-penalty?
csp for the END rules(Figure 2).
The DOP1 model uses binary values(0 if symbol is intermediate, 1 otherwise) asthe END rule weight, which is equivalent toprohibiting fragment switching at intermediatesymbols.
We learn a fractional constant aspthat allows (but penalizes) switching betweenfragments at annotated symbols through theformulation csp(Xintermediate) = 1 ?
asp andcsp(Xnon?intermediate) = 1 + asp.
This featureallows fragments to be assigned weights based onthe binarization status of their nodes.With the above weights, the recursive formulafor s(Xi), the total weighted number of fragmentsrooted at indexed symbol Xi, is different fromDOP1 (Equation 1).
For rule Xi ?
Yj Zk, it iss(Xi) = ?BODY.
(csp(Yj)+s(Yj))(csp(Zk)+s(Zk)).The formula uses ?LEX in place of ?BODY if r is alexical rule (Figure 2).The resulting grammar is primarily parameter-ized by the training treebank B.
However, eachsetting of the hyperparameters (?BODY, ?LEX, asp)defines a different conditional distribution ontrees.
We choose amongst these distributions bydirectly optimizing parsing F1 on our develop-ment set.
Because this objective is not easily dif-ferentiated, we simply perform a grid search onthe three hyperparameters.
The tuned values are?BODY = 0.35, ?LEX = 0.25 and asp = 0.018.For generalization to a larger parameter space, wewould of course need to switch to a learning ap-proach that scales more gracefully in the numberof tunable hyperparameters.88Note that there has been a long history of DOP estima-tors.
The generative DOP1 model was shown to be inconsis-dev (?
40) test (?
40) test (all)Model F1 EX F1 EX F1 EXConstituent 88.4 33.7 88.5 33.0 87.6 30.8Rule-Sum 88.2 34.6 88.3 33.8 87.4 31.6Variational 87.7 34.4 87.7 33.9 86.9 31.6Table 1: All-fragments WSJ results (accuracy F1 and exactmatch EX) for the constituent, rule-sum and variational ob-jectives, using parent annotation and one level of markoviza-tion.4 Efficient InferenceThe previously described implicit grammarGI de-fines a posterior distribution P (dI |s) over a sen-tence s via a large, indexed PCFG.
This distri-bution has the property that, when marginalized,it is equivalent to a posterior distribution P (d|s)over derivations in the correspondingly-weightedall-fragments grammar G. However, even withan explicit representation of G, we would not beable to tractably compute the parse that maxi-mizes P (t|s) =?d?t P (d|s) =?dI?t P (dI |s)(Sima?an, 1996).
We therefore approximatelymaximize over trees by computing various exist-ing approximations to P (t|s) (Figure 3).
Good-man (1996b), Petrov and Klein (2007), and Mat-suzaki et al (2005) describe the details of con-stituent, rule-sum and variational objectives re-spectively.
Note that all inference methods dependon the posterior P (t|s) only through marginal ex-pectations of labeled constituent counts and an-chored local binary tree counts, which are easilycomputed from P (dI |s) and equivalent to thosefrom P (d|s).
Therefore, no additional approxima-tions are made in GI over G.As shown in Table 1, our model (an all-fragments grammar with the weighting schemetent by Johnson (2002).
Later, Zollmann and Sima?an (2005)presented a statistically consistent estimator, with the basicinsight of optimizing on a held-out set.
Our estimator is notintended to be viewed as a generative model of trees at all,but simply a loss-minimizing conditional distribution withinour parametric family.1102shown in Figure 2) achieves an accuracy of88.5% (using simple parent annotation) which is4-5% (absolute) better than the recent TSG work(Zuidema, 2007; Cohn et al, 2009; Post andGildea, 2009) and also approaches state-of-the-art refinement-based parsers (e.g., Charniak andJohnson (2005), Petrov and Klein (2007)).94.1 Coarse-to-Fine InferenceCoarse-to-fine inference is a well-established wayto accelerate parsing.
Charniak et al (2006) in-troduced multi-level coarse-to-fine parsing, whichextends the basic pre-parsing idea by adding morerounds of pruning.
Their pruning grammarswere coarse versions of the raw treebank gram-mar.
Petrov and Klein (2007) propose a multi-stage coarse-to-fine method in which they con-struct a sequence of increasingly refined gram-mars, reparsing with each refinement.
In par-ticular, in their approach, which we adopt here,coarse-to-fine pruning is used to quickly com-pute approximate marginals, which are then usedto prune subsequent search.
The key challengein coarse-to-fine inference is the construction ofcoarse models which are much smaller than thetarget model, yet whose posterior marginals areclose enough to prune with safely.Our grammar GI has a very large number of in-dexed symbols, so we use a coarse pass to pruneaway their unindexed abstractions.
The simple,intuitive, and effective choice for such a coarsegrammar GC is a minimal PCFG grammar com-posed of the base treebank symbols X and theminimal depth-1 binary rules X ?
Y Z (andwith the same level of annotation as in the fullgrammar).
If a particular base symbolX is prunedby the coarse pass for a particular span (i, j) (i.e.,the posterior marginal P (X, i, j|s) is less than acertain threshold), then in the full grammar GI ,we do not allow building any indexed symbolXl of type X for that span.
Hence, the pro-jection map for the coarse-to-fine model is piC :Xl (indexed symbol)?
X (base symbol).We achieve a substantial improvement in speedand memory-usage from the coarse-pass pruning.Speed increases by a factor of 40 and memory-usage decreases by a factor of 10 when we go9All our experiments use the constituent objective ex-cept when we report results for max-rule-sum and max-variational parsing (where we use the parameters tuned formax-constituent, therefore they unsurprisingly do not per-form as well as max-constituent).
Evaluations use EVALB,see http://nlp.cs.nyu.edu/evalb/.87.888.088.288.4-4.0 -4.5 -5.0 -5.5 -6.0 -6.5 -7.0 -7.5Coarse-pass Log Posterior Threshold (PT)F1-6.2Figure 4: Effect of coarse-pass pruning on parsing accuracy(for WSJ dev-set, ?
40 words).
Pruning increases to the leftas log posterior threshold (PT) increases.86.086.587.087.588.088.589.089.590.0-1 -3 -5 -7 -9 -11 -13Coarse-pass Log Posterior Threshold (PT)-6F189.6 No Pruning (PT = -inf)89.8Figure 5: Effect of coarse-pass pruning on parsing accuracy(WSJ, training ?
20 words, tested on dev-set ?
20 words).This graph shows that the fortuitous improvement due topruning is very small and that the peak accuracy is almostequal to the accuracy without pruning (the dotted line).from no pruning to pruning with a ?6.2 log pos-terior threshold.10 Figure 4 depicts the variationin parsing accuracies in response to the amountof pruning done by the coarse-pass.
Higher pos-terior pruning thresholds induce more aggressivepruning.
Here, we observe an effect seen in previ-ous work (Charniak et al (1998), Petrov and Klein(2007), Petrov et al (2008)), that a certain amountof pruning helps accuracy, perhaps by promotingagreement between the coarse and full grammars(model intersection).
However, these ?fortuitous?search errors give only a small improvement andthe peak accuracy is almost equal to the pars-ing accuracy without any pruning (as seen in Fig-ure 5).11 This outcome suggests that the coarse-pass pruning is critical for tractability but not forperformance.10Unpruned experiments could not be run for 40-word testsentences even with 50GB of memory, therefore we calcu-lated the improvement factors using a smaller experimentwith full training and sixty 30-word test sentences.11To run experiments without pruning, we used trainingand dev sentences of length ?
20 for the graph in Figure 5.1103tree-to-graph encodingFigure 6: Collapsing the duplicate training subtrees convertsthem to a graph and reduces the number of indexed symbolssignificantly.4.2 Packed Graph EncodingThe implicit all-fragments approach (Section 2.2)avoids explicit extraction of all rule fragments.However, the number of indexed symbols in ourimplicit grammar GI is still large, because ev-ery node in each training tree (i.e., every symboltoken) has a unique indexed symbol.
We havearound 1.9 million indexed symbol tokens in theword-level parsing model (this number increasesfurther to almost 12.3 million when we parse char-acter strings in Section 5.1).
This large symbolspace makes parsing slow and memory-intensive.We reduce the number of symbols in our im-plicit grammar GI by applying a compact, packedgraph encoding to the treebank training trees.
Wecollapse the duplicate subtrees (fragments thatbottom out in terminals) over all training trees.This keeps the grammar unchanged because in antree-substitution grammar, a node is defined (iden-tified) by the subtree below it.
We maintain ahashmap on the subtrees which allows us to eas-ily discover the duplicates and bin them together.The collapsing converts all the training trees in thetreebank to a graph with multiple parents for somenodes as shown in Figure 6.
This technique re-duces the number of indexed symbols significantlyas shown in Table 2 (1.9 million goes down to 0.9million, reduction by a factor of 2.1).
This reduc-tion increases parsing speed by a factor of 1.4 (andby a factor of 20 for character-level parsing, seeSection 5.1) and reduces memory usage to under4GB.We store the duplicate-subtree counts for eachindexed symbol of the collapsed graph (using ahashmap).
When calculating the number of frag-Parsing Model No.
of Indexed SymbolsWord-level Trees 1,900,056Word-level Graph 903,056Character-level Trees 12,280,848Character-level Graph 1,109,399Table 2: Number of indexed symbols for word-level andcharacter-level parsing and their graph versions (for all-fragments grammar with parent annotation and one level ofmarkovization).Figure 7: Character-level parsing: treating the sentence as astring of characters instead of words.ments s(Xi) parented by an indexed symbol Xi(see Section 3.2), and when calculating the insideand outside scores during inference, we accountfor the collapsed subtree tokens by expanding thecounts and scores using the corresponding multi-plicities.
Therefore, we achieve the compactionwith negligible overhead in computation.5 Improved Treebank Representations5.1 Character-Level ParsingThe all-fragments approach to parsing has theadded advantage that parsing below the word levelrequires no special treatment, i.e., we do not needan explicit lexicon when sentences are consideredas strings of characters rather than words.Unknown words in test sentences (unseen intraining) are a major issue in parsing systems forwhich we need to train a complex lexicon, withvarious unknown classes or suffix tries.
Smooth-ing factors need to be accounted for and tuned.With our implicit approach, we can avoid traininga lexicon by building up the parse tree from char-acters instead of words.
As depicted in Figure 7,each word in the training trees is split into its cor-responding characters with start and stop bound-ary tags (and then binarized in a standard right-branching style).
A test sentence?s words are splitup similarly and the test-parse is built from train-ing fragments using the same model and inferenceprocedure as defined for word-level parsing (seeSections 2, 3 and 4).
The lexical items (alphabets,digits etc.)
are now all known, so unlike word-levelparsing, no sophisticated lexicon is needed.We choose a slightly richer weighting scheme1104dev (?
40) test (?
40) test (all)Model F1 EX F1 EX F1 EXConstituent 88.2 33.6 88.0 31.9 87.1 29.8Rule-Sum 88.0 33.9 87.8 33.1 87.0 30.9Variational 87.6 34.4 87.2 32.3 86.4 30.2Table 3: All-fragments WSJ results for the character-levelparsing model, using parent annotation and one level ofmarkovization.for this representation by extending the two-weight schema for CONTINUE rules (?LEX and?BODY) to a three-weight one: ?LEX, ?WORD, and?SENT for CONTINUE rules in the lexical layer, inthe portion of the parse that builds words fromcharacters, and in the portion of the parse thatbuilds the sentence from words, respectively.
Thetuned values are ?SENT = 0.35, ?WORD = 0.15,?LEX = 0.95 and asp = 0.
The character-levelmodel achieves a parsing accuracy of 88.0% (seeTable 3), despite lacking an explicit lexicon.12Character-level parsing expands the trainingtrees (see Figure 7) and the already large indexedsymbol space size explodes (1.9 million increasesto 12.3 million, see Table 2).
Fortunately, thisis where the packed graph encoding (Section 4.2)is most effective because duplication of characterstrings is high (e.g., suffixes).
The packing shrinksthe symbol space size from 12.3 million to 1.1 mil-lion, a reduction by a factor of 11.
This reductionincreases parsing speed by almost a factor of 20and brings down memory-usage to under 8GB.135.2 Basic Refinement: Parent Annotationand Horizontal MarkovizationIn a pure all-fragments approach, compositionsof units which would have been independent ina basic PCFG are given joint scores, allowingthe representation of certain non-local phenom-ena, such as lexical selection or agreement, whichin fully local models require rich state-splittingor lexicalization.
However, at substitution sites,the coarseness of raw unrefined treebank sym-bols still creates unrealistic factorization assump-tions.
A standard solution is symbol refinement;Johnson (1998) presents the particularly simplecase of parent annotation, in which each node is12Note that the word-level model yields a higher accuracyof 88.5%, but uses 50 complex unknown word categoriesbased on lexical, morphological and position features (Petrovet al, 2006).
Cohn et al (2009) also uses this lexicon.13Full char-level experiments (w/o packed graph encoding)could not be run even with 50GB of memory.
We calcu-late the improvement factors using a smaller experiment with70% training and fifty 20-word test sentences.Parsing Model F1No Refinement (P=0, H=0)?
71.3Basic Refinement (P=1, H=1)?
80.0All-Fragments + No Refinement (P=0, H=0) 85.7All-Fragments + Basic Refinement (P=1, H=1) 88.4Table 4: F1 for a basic PCFG, and incorporation of basicrefinement, all-fragments and both, for WSJ dev-set (?
40words).
P = 1 means parent annotation of all non-terminals,including the preterminal tags.
H = 1 means one level ofmarkovization.
?Results from Klein and Manning (2003).marked with its parent in the underlying treebank.It is reasonable to hope that the gains from us-ing large fragments and the gains from symbol re-finement will be complementary.
Indeed, previouswork has shown or suggested this complementar-ity.
Sima?an (2000) showed modest gains from en-riching structural relations with semi-lexical (pre-head) information.
Charniak and Johnson (2005)showed accuracy improvements from composedlocal tree features on top of a lexicalized baseparser.
Zuidema (2007) showed a slight improve-ment in parsing accuracy when enough fragmentswere added to learn enrichments beyond manualrefinements.
Our work reinforces this intuition bydemonstrating how complementary they are in ourmodel (?20% error reduction on adding refine-ment to an all-fragments grammar, as shown in thelast two rows of Table 4).Table 4 shows results for a basic PCFG, and itsaugmentation with either basic refinement (parentannotation and one level of markovization), withall-fragments rules (as in previous sections), orboth.
The basic incorporation of large fragmentsalone does not yield particularly strong perfor-mance, nor does basic symbol refinement.
How-ever, the two approaches are quite additive in ourmodel and combine to give nearly state-of-the-artparsing accuracies.5.3 Additional Deterministic RefinementBasic symbol refinement (parent annotation), incombination with all-fragments, gives test-set ac-curacies of 88.5% (?
40 words) and 87.6% (all),shown as the Basic Refinement model in Table 5.Klein and Manning (2003) describe a broad setof simple, deterministic symbol refinements be-yond parent annotation.
We included ten of theirsimplest annotation features, namely: UNARY-DT,UNARY-RB, SPLIT-IN, SPLIT-AUX, SPLIT-CC, SPLIT-%,GAPPED-S, POSS-NP, BASE-NP and DOMINATES-V.None of these annotation schemes use any headinformation.
This additional annotation (see Ad-1105838485868788890 20 40 60 80 100F1Percentage of WSJ sections 2-21 used for trainingFigure 8: Parsing accuracy F1 on the WSJ dev-set (?
40words) increases with increasing percentage of training data.ditional Refinement, Table 5) improves the test-set accuracies to 88.7% (?
40 words) and 88.1%(all), which is equal to a strong lexicalized parser(Collins, 1999), even though our model does notuse lexicalization or latent symbol-split induc-tion.6 Other Results6.1 Parsing Speed and Memory UsageThe word-level parsing model using the wholetraining set (39832 trees, all-fragments) takes ap-proximately 3 hours on the WSJ test set (2245trees of ?40 words), which is equivalent toroughly 5 seconds of parsing time per sen-tence; and runs in under 4GB of memory.
Thecharacter-level version takes about twice the timeand memory.
This novel tractability of an all-fragments grammar is achieved using both coarse-pass pruning and packed graph encoding.
Micro-optimization may further improve speed and mem-ory usage.6.2 Training Size VariationFigure 8 shows how WSJ parsing accuracy in-creases with increasing amount of training data(i.e., percentage of WSJ sections 2-21).
Even if wetrain on only 10% of the WSJ training data (3983sentences), we still achieve a reasonable parsingaccuracy of nearly 84% (on the development set,?
40 words), which is comparable to the full-system results obtained by Zuidema (2007), Cohnet al (2009) and Post and Gildea (2009).6.3 Other Language TreebanksOn the French and German treebanks (using thestandard dataset splits mentioned in Petrov andtest (?
40) test (all)Parsing Model F1 EX F1 EXFRAGMENT-BASED PARSERSZuidema (2007) ?
?
83.8?
26.9?Cohn et al (2009) ?
?
84.0 ?Post and Gildea (2009) 82.6 ?
?
?THIS PAPERAll-Fragments+ Basic Refinement 88.5 33.0 87.6 30.8+ Additional Refinement 88.7 33.8 88.1 31.7REFINEMENT-BASED PARSERSCollins (1999) 88.6 ?
88.2 ?Petrov and Klein (2007) 90.6 39.1 90.1 37.1Table 5: Our WSJ test set parsing accuracies, comparedto recent fragment-based parsers and top refinement-basedparsers.
Basic Refinement is our all-fragments grammar withparent annotation.
Additional Refinement adds determinis-tic refinement of Klein and Manning (2003) (Section 5.3).
?Results on the dev-set (?
100).Klein (2008)), our simple all-fragments parserachieves accuracies in the range of top refinement-based parsers, even though the model parameterswere tuned out of domain on WSJ.
For German,our parser achieves an F1 of 79.8% comparedto 81.5% by the state-of-the-art and substantiallymore complex Petrov and Klein (2008) work.
ForFrench, our approach yields an F1 of 78.0% vs.80.1% by Petrov and Klein (2008).147 ConclusionOur approach of using all fragments, in combi-nation with basic symbol refinement, and evenwithout an explicit lexicon, achieves results in therange of state-of-the-art parsers on full scale tree-banks, across multiple languages.
The main take-away is that we can achieve such results in a veryknowledge-light way with (1) no latent-variabletraining, (2) no sampling, (3) no smoothing be-yond the existence of small fragments, and (4) noexplicit unknown word model at all.
While thesemethods offer a simple new way to construct anaccurate parser, we believe that this general ap-proach can also extend to other large-fragmenttasks, such as machine translation.AcknowledgmentsThis project is funded in part by BBN underDARPA contract HR0011-06-C-0022 and the NSFunder grant 0643742.14All results on the test set (?
40 words).1106ReferencesRens Bod.
1993.
Using an Annotated Corpus as aStochastic Grammar.
In Proceedings of EACL.Rens Bod.
2001.
What is the Minimal Set of Frag-ments that Achieves Maximum Parse Accuracy?
InProceedings of ACL.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminativereranking.
In Proceedings of ACL.Eugene Charniak, Sharon Goldwater, and Mark John-son.
1998.
Edge-Based Best-First Chart Parsing.In Proceedings of the 6th Workshop on Very LargeCorpora.Eugene Charniak, Mark Johnson, et al 2006.
Multi-level Coarse-to-fine PCFG Parsing.
In Proceedingsof HLT-NAACL.Eugene Charniak.
2000.
A Maximum-Entropy-Inspired Parser.
In Proceedings of NAACL.David Chiang.
2003.
Statistical parsing with anautomatically-extracted tree adjoining grammar.
InData-Oriented Parsing.David Chiang.
2005.
A Hierarchical Phrase-BasedModel for Statistical Machine Translation.
In Pro-ceedings of ACL.Trevor Cohn, Sharon Goldwater, and Phil Blunsom.2009.
Inducing Compact but Accurate Tree-Substitution Grammars.
In Proceedings of NAACL.Michael Collins and Nigel Duffy.
2002.
New RankingAlgorithms for Parsing and Tagging: Kernels overDiscrete Structures, and the Voted Perceptron.
InProceedings of ACL.Michael Collins.
1999.
Head-Driven Statistical Mod-els for Natural Language Parsing.
Ph.D. thesis, Uni-versity of Pennsylvania, Philadelphia.Steve Deneefe and Kevin Knight.
2009.
SynchronousTree Adjoining Machine Translation.
In Proceed-ings of EMNLP.Michel Galley, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a translation rule?In Proceedings of HLT-NAACL.Joshua Goodman.
1996a.
Efficient Algorithms forParsing the DOP Model.
In Proceedings of EMNLP.Joshua Goodman.
1996b.
Parsing Algorithms andMetrics.
In Proceedings of ACL.Joshua Goodman.
2003.
Efficient parsing of DOP withPCFG-reductions.
In Bod R, Scha R, Sima?an K(eds.)
Data-Oriented Parsing.
University of ChicagoPress, Chicago, IL.James Henderson.
2004.
Discriminative Training ofa Neural Network Statistical Parser.
In Proceedingsof ACL.Mark Johnson.
1998.
PCFG Models of LinguisticTree Representations.
Computational Linguistics,24:613?632.Mark Johnson.
2002.
The DOP Estimation Method IsBiased and Inconsistent.
In Computational Linguis-tics 28(1).Dan Klein and Christopher Manning.
2003.
AccurateUnlexicalized Parsing.
In Proceedings of ACL.Philipp Koehn, Franz Och, and Daniel Marcu.
2003.Statistical Phrase-Based Translation.
In Proceed-ings of HLT-NAACL.Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.2005.
Probabilistic CFG with latent annotations.
InProceedings of ACL.Slav Petrov and Dan Klein.
2007.
Improved Infer-ence for Unlexicalized Parsing.
In Proceedings ofNAACL-HLT.Slav Petrov and Dan Klein.
2008.
Sparse Multi-ScaleGrammars for Discriminative Latent Variable Pars-ing.
In Proceedings of EMNLP.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning Accurate, Compact, andInterpretable Tree Annotation.
In Proceedings ofCOLING-ACL.Slav Petrov, Aria Haghighi, and Dan Klein.
2008.Coarse-to-Fine Syntactic Machine Translation usingLanguage Projections.
In Proceedings of EMNLP.Matt Post and Daniel Gildea.
2009.
Bayesian Learningof a Tree Substitution Grammar.
In Proceedings ofACL-IJCNLP.Philip Resnik.
1992.
Probabilistic Tree-AdjoiningGrammar as a Framework for Statistical NaturalLanguage Processing.
In Proceedings of COLING.Remko Scha.
1990.
Taaltheorie en taaltechnologie;competence en performance.
In R. de Kort andG.L.J.
Leerdam (eds.
): Computertoepassingen in deNeerlandistiek.Khalil Sima?an.
1996.
Computational Complexityof Probabilistic Disambiguation by means of Tree-Grammars.
In Proceedings of COLING.Khalil Sima?an.
2000.
Tree-gram Parsing: Lexical De-pendencies and Structural Relations.
In Proceedingsof ACL.Andreas Zollmann and Khalil Sima?an.
2005.
AConsistent and Efficient Estimator for Data-OrientedParsing.
Journal of Automata, Languages and Com-binatorics (JALC), 10(2/3):367?388.Willem Zuidema.
2007.
Parsimonious Data-OrientedParsing.
In Proceedings of EMNLP-CoNLL.1107
