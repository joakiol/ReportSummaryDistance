11Automatic SummarizationAni Nenkova University of PennsylvaniaSameer Maskey IBM ResearchYang Liu University of Texas at Dallas2Why summarize?23Text summarizationNews articlesScientific ArticlesEmailsBooksWebsitesSocial MediaStreams4Speech summarizationMeetingPhone ConversationClassroomRadio NewsBroadcast NewsTalk ShowsLectureChat35How tosummarizeText & Speech?-Algorithms-Issues-Challenges-SystemsTutorial6Motivation & DefinitionTopic ModelsGraph Based MethodsSupervised TechniquesOptimization MethodsSpeech SummarizationEvaluationManual (Pyramid), Automatic (Rouge, F-Measure)Fully AutomaticFrequency, Lexical chains, TF*IDF,Topic Words, Topic Models [LSA, EM, Bayesian]Features, Discriminative TrainingSampling, Data, Co-trainingIterative, Greedy, Dynamic ProgrammingILP, Sub-Modular SelectionSegmentation, ASRAcoustic Information, Disfluency47Motivation: where does summarizationhelp? Single document summarization Simulate the work of intelligence analyst Judge if a document is relevant to a topic of interest?Summaries as short as 17% of the full text length speed updecision making twice, with no significant degradation inaccuracy.?
?Query-focused summaries enable users to find more relevantdocuments more accurately, with less need to consult the full textof the document.?
[Mani et al, 2002]8Motivation: multi-document summarizationhelps in compiling and presenting Reduce search time, especially when the goal of theuser is to find as much information as possible about agiven topic Writing better reports, finding more relevant information,quicker Cluster similar articles and provide a multi-documentsummary of the similarities Single document summary of the information unique toan article[Roussinov and Chen, 2001; Mana-Lopez et al, 2004; McKeown et al, 2005 ]59Benefits from speech summarization Voicemail Shorter time spent on listening (call centers) Meetings Easier to find main points Broadcast News Summary of story from mulitiple channels Lectures Useful for reviewing of course materials[He et al, 2000; Tucker and Whittaker, 2008; Murray et al, 2009]10Assessing summary quality: overview Responsiveness Assessor directly rate each summary on a scale In official evaluations but rarely reported in papers Pyramid Assessors create model summaries Assessors identifies semantic overlap between summaryand models ROUGE Assessors create model summaries ROUGE automatically computes word overlap611Tasks in summarizationContent (sentence) selection Extractive summarizationInformation ordering In what order to present the selected sentences, especiallyin multi-document summarizationAutomatic editing, information fusion and compression Abstractive summaries12Extractive (multi-document) summarizationInput text2Input text1 Input text3Summary1.
Selection2.
Ordering3.
FusionCompute Informativeness713Computing informativeness Topic models (unsupervised) Figure out what the topic of the input Frequency, Lexical chains, TF*IDF LSA, content models (EM, Bayesian) Select informative sentences based on the topic Graph models (unsupervised) Sentence centrality Supervised approaches Ask people which sentences should be in a summary Use any imaginable feature to learn to predict humanchoices14Motivation & DefinitionTopic ModelsGraph Based MethodsSupervised TechniquesGlobal Optimization MethodsSpeech SummarizationEvaluationFrequency, Lexical chains, TF*IDF,Topic Words,Topic Models [LSA, EM, Bayesian]Manual (Pyramid), Automatic (Rouge, F-Measure)Fully AutomaticFeatures, Discriminative TrainingSampling, Data, Co-trainingIterative, Greedy, Dynamic ProgrammingILP, Sub-Modular SelectionSegmentation, ASRAcoustic Information, Disfluency815Frequency as document topic proxy10 incarnations of an intuition Simple intuition, look only at the document(s) Words that repeatedly appear in the document are likely tobe related to the topic of the document Sentences that repeatedly appear in different inputdocuments represent themes in the input But what appears in other documents is also helpfulin determining the topic Background corpus probabilities/weights for word16What is an article about? Word probability/frequency Proposed by Luhn in 1958 [Luhn 1958] Frequent content words would be indicative of thetopic of the article In multi-document summarization, words orfacts repeated in the input are more likely toappear in human summaries [Nenkova et al, 2006]917Word probability/weightsLibyabombingtrailGadafhisuspectsLibya refusesto surrendertwo Pan AmbombingsuspectsPan AmINPUTSUMMARYWORD PROBABILITY TABLEWord Probabilitypan 0.0798am 0.0825libya 0.0096suspects 0.0341gadafhi 0.0911trail 0.0002?.usa 0.0007HOW?UK andUSA18HOW: Main steps in sentence selectionaccording to word probabilitiesStep 1 Estimate word weights (probabilities)Step 2 Estimate sentence weightsStep 3 Choose best sentenceStep 4 Update word weightsStep 5 Go to 2 if desired length not reached)()( SentwCFSentWeight i ?=1019More specific choices [Vanderwende et al, 2007; Yih et al,2007; Haghighi and Vanderwende, 2009] Select highest scoring sentence Update word probabilities for the selected sentenceto reduce redundancy Repeat until desired summary length?
?=SwwpSSScore )(||1)(pnew (w) = pold (w).pold (w)20Is this a reasonable approach: yes, peopleseem to be doing something similar Simple test Compute word probability table from the input Get a batch of summaries written by H(umans) and S(ystems) Compute the likelihood of the summaries given the wordprobability table Results Human summaries have higher likelihoodHSSSSSSSSSSHSSSHSSHHSHHHHHHIGH LIKELIHOODLOW1121Obvious shortcomings of the purefrequency approaches Does not take account of related words suspects -- trail Gadhafi ?
Libya Does not take into account evidence fromother documents Function words: prepositions, articles, etc. Domain words: ?cell?
in cell biology articles Does not take into account many otheraspects22Two easy fixes Lexical chains [Barzilay and Elhadad, 1999, Silber and McCoy,2002, Gurevych and Nahnsen, 2005] Exploits existing lexical resources (WordNet) TF*IDF weights [most summarizers] Incorporates evidence from a background corpus1223Lexical chains and WordNet relations Lexical chains Word sense disambiguation is performed Then topically related words represent a topic Synonyms, hyponyms, hypernyms Importance is determined by frequency of the words in atopic rather than a single word One sentence per topic is selected Concepts based on WordNet [Schiffman et al, 2002, Ye et al,2007] No word sense disambiguation is performed {war, campaign, warfare, effort, cause, operation} {concern, carrier, worry, fear, scare}24TF*IDF weights for wordsCombining evidence for document topics from theinput and from a background corpus Term Frequency (TF) Times a word occurs in the input Inverse Document Frequency (IDF) Number of documents (df) from a backgroundcorpus of N documents that contain the word)/log(* dfNtfIDFTF ?=1325Motivation & DefinitionTopic ModelsGraph Based MethodsSupervised TechniquesGlobal Optimization MethodsSpeech SummarizationEvaluationFrequency, TF*IDF, Topic WordsTopic Models [LSA, EM, Bayesian]Manual (Pyramid), Automatic (Rouge, F-Measure)Fully AutomaticFeatures, Discriminative TrainingSampling, Data, Co-trainingIterative, Greedy, Dynamic ProgrammingILP, Sub-Modular SelectionSegmentation, ASRAcoustic Information, Disfluency26Topic words (topic signatures) Which words in the input are most descriptive? Instead of assigning probabilities or weights to all words,divide words into two classes: descriptive or not For iterative sentence selection approach, the binarydistinction is key to the advantage over frequency andTF*IDF Systems based on topic words have proven to be the mostsuccessful in official summarization evaluations1427Example input and associated topic words Input for summarization: articles relevant to thefollowing user needTitle: Human Toll of TropicalStorms Narrative: What has been the human toll in death or injuryof tropical storms in recent years?
Where and when have each ofthe storms caused human casualties?
What are the approximatetotal number of casualties attributed to each of the storms?ahmed, allison, andrew, bahamas, bangladesh, bn, caribbean, carolina, caused, cent,coast, coastal, croix, cyclone, damage, destroyed, devastated, disaster, dollars, drowned,flood, flooded, flooding, floods, florida, gulf, ham, hit, homeless, homes, hugo, hurricane,insurance, insurers, island, islands, lloyd, losses, louisiana, manila, miles, nicaragua,north, port, pounds, rain, rains, rebuild, rebuilding, relief, remnants, residents, roared, salt,st, storm, storms, supplies, tourists, trees, tropical, typhoon, virgin, volunteers, weather,west, winds, yesterday.Topic Words28Formalizing the problem of identifying topicwords Given t: a word that appears in the input T: cluster of articles on a given topic (input) NT: articles not on topic T (background corpus) Decide if t is a topic word or not Words that have (almost) the same probability in Tand NT are not topic words1529Computing probabilities View a text as a sequence of Bernoulli trails A word is either our term of interest t or not The likelihood of observing term t which occurs withprobability p in a text consisting of N words is given by Estimate the probability of t in three ways Input + background corpus combines Input only Background onlyt30Testing which hypothesis is morelikely: log-likelihood ratio testhas a known statistical distribution: chi-squareAt a given significance level, we can decide if a word isdescriptive of the input or not.This feature is used in the best performing systems formulti-document summarization of news [Lin and Hovy,2000; Conroy et al, 2006]Likelihood of the data given H1Likelihood of the data given H2?
=-2 log ?1631Motivation & DefinitionTopic ModelsGraph Based MethodsSupervised TechniquesGlobal Optimization MethodsSpeech SummarizationEvaluationFrequency, TF*IDF, Topic WordsTopic Models [LSA, EM, Bayesian]Manual (Pyramid), Automatic (Rouge, F-Measure)Fully AutomaticFeatures, Discriminative TrainingSampling, Data, Co-trainingIterative, Greedy, Dynamic ProgrammingILP, Sub-Modular SelectionSegmentation, ASRAcoustic Information, Disfluency32The background corpus takes morecentral stage Learn topics from the background corpus topic ~ themes often discusses in the background topic representation ~ word probability tables Usually one time training step To summarize an input Select sentences from the input that correspondto the most prominent topics1733Latent semantic analysis (LSA) [Gong and Liu,2001, Hachey et al, 2006, Steinberger et al, 2007] Discover topics from the background corpus with n uniquewords and d documents Represent the background corpus as nxd matrix A Rows correspond to words Aij=number of times word I appears in document j Use standard change of coordinate system and dimensionalityreduction techniques In the new space each row corresponds to the most importanttopics in the corpus Select the best sentence to cover each topicTUPVA =34Notes on LSA and other approaches The original article that introduced LSA forsingle document summarization of news didnot find significant difference with TF*IDF For multi-document summarization of newsLSA approaches have not outperformed topicwords or extensions of frequency approaches Other topic/content models have been muchmore influential1835Domain dependent content models Get sample documents from the domain background corpus Cluster sentences from these documents Implicit topics Obtain a word probability table for each topic Counts only from the cluster representing thetopic Select sentences from the input with highestprobability for main topics36Text structure can be learnt Human-written examples from a domainLocation, timerelief effortsmagnitudedamage1937Topic = cluster of similar sentences fromthe background corpus Sentences cluster from earthquake articles Topic ?earthquake location? The Athens seismological institute said the temblor?s epicenterwas located 380 kilometers (238 miles) south of the capital. Seismologists in Pakistan?s Northwest Frontier Province said thetemblor?s epicenter was about 250 kilometers (155 miles) north ofthe provincial capital Peshawar. The temblor was centered 60 kilometers (35 miles) north- west ofthe provincial capital of Kunming, about 2,200 kilometers (1,300miles) southwest of Beijing, a bureau seismologist said.38Content model [Barzilay and Lee, 2004, Pascale et al, 2003] Hidden Markov Model (HMM)-based States - clusters of related sentences ?topics? Transition prob.
- sentence precedence in corpus Emission prob.
- bigram language modellocation,magnitude casualtiesrelief efforts)|()|(),|,( 11111 +++++ ?=><>< iieiitiiii hsphhphshspEarthquake reportsTransition from previoustopicGeneratingsentence incurrent topic2039Learning the content model Many articles from the same domain Cluster sentences: each cluster represents a topic fromthe domain Word probability tables for each topic Transitions between clusters can be computed fromsentence adjacencies in the original articles Probabilities of going from one topic to another Iterate between clustering and transition probabilityestimation to obtain domain model40To select a summary Find main topics in the domain using a small collection of summary-input pairs Find the most likely topic for each sentence inthe input Select the best sentence per main topic2141Historical note Some early approaches to multi-documentsummarization relied on clustering thesentences in the input alone [McKeown et al, 1999,Siddharthan et al, 2004] Clusters of similar sentences represent a theme inthe input Clusters with more sentences are more important Select one sentence per important cluster42Example clusterChoose one sentence to represent the cluster1.
PAL was devastated by a pilots' strike in June and by theregion's currency crisis.2.
In June, PAL was embroiled in a crippling three-weekpilots' strike.3.
Tan wants to retain the 200 pilots because they stood byhim when the majority of PAL's pilots staged adevastating strike in June.2243Bayesian content models Takes a batch of inputs for summarization Many word probability tables One for general English One for each of the inputs to be summarized One for each document in any inputTo select a summary S with L words fromdocument collection D given as inputThe goal is to select the summary, not asentence.
Greedy selection vs. global willbe discussed in detail laterS* = minS:words(S)?LKL(PD||PS)44KL divergence Distance between two probability distributions: P, Q P, Q: Input and summary word distributionsKL (P || Q) = pP (w) log2 pP (w)pQ (w)w?2345Intriguing side note In the full Bayesian topic models, wordprobabilities for all words is more importantthan binary distinctions of topic and non-topicword Haghighi and Vanderwende report that asystem that chooses the summary withhighest expected number of topic wordsperforms as SumBasic46Review Frequency based informativeness has beenused in building summarizers Topic words probably more useful Topic models Latent Semantic Analysis Domain dependent content model Bayesian content model2447Motivation & DefinitionTopic ModelsGraph Based MethodsSupervised TechniquesOptimization MethodsSpeech SummarizationEvaluationFrequency, TF*IDF, Topic WordsTopic Models [LSA, EM, Bayesian]Manual (Pyramid), Automatic (Rouge, F-Measure)Fully AutomaticFeatures, Discriminative TrainingSampling, Data, Co-trainingIterative, Greedy, Dynamic ProgrammingILP, Sub-Modular SelectionSegmentation, ASRAcoustic Information, Disfluency48Using graph representations [Erkan and Radev,2004; Mihalcea and Tarau, 2004; Leskovec et al, 2005 ] Nodes Sentences Discourse entities Edges Between similar sentences Between syntactically related entities Computing sentence similarity Distance between their TF*IDF weighted vectorrepresentations254950Sentence :Iraqi vice president?Sentence :Ivanov contended?Sim(d1s1, d3s2)2651Advantages of the graph model Combines word frequency and sentenceclustering Gives a formal model for computingimportance: random walks Normalize weights of edges to sum to 1 They now represent probabilities of transitioningfrom one node to another52Random walks for summarization Represent the input text as graph Start traversing from node to node following the transition probabilities occasionally hopping to a new node What is the probability that you are in anyparticular node after doing this process for acertain time? Standard solution (stationary distribution) This probability is the weight of the sentence2753Motivation & DefinitionTopic ModelsGraph Based MethodsSupervised TechniquesGlobal Optimization MethodsSpeech SummarizationEvaluationFrequency, TF*IDF, Topic WordsTopic Models [LSA, EM, Bayesian]Manual (Pyramid), Automatic (Rouge, F-Measure)Fully AutomaticFeatures, Discriminative TrainingSampling, Data, Co-trainingIterative, Greedy, Dynamic ProgrammingILP, Sub-Modular SelectionSegmentation, ASRAcoustic Information, Disfluency54Supervised methods For extractive summarization, the task can berepresented as binary classification A sentence is in the summary or not Use statistical classifiers to determine the score of asentence: how likely it?s included in the summary Feature representation for each sentence Classification models trained from annotated data Select the sentences with highest scores (greedy fornow, see other selection methods later)2855Features Sentence length long sentences tend to be more important Sentence weight cosine similarity with documents sum of term weights for all words in a sentence calculate term weight after applying LSA56Features Sentence position beginning is often more important some sections are more important (e.g., inconclusion section) Cue words/phrases frequent n-grams cue phrases (e.g., in summary, as a conclusion) named entities2957Features Contextual features features from context sentences difference of a sentence and its neighboring ones Speech related features (more later): acoustic/prosodic features speaker information (who said the sentence, is thespeaker dominant?
) speech recognition confidence measure58Classifiers Can classify each sentence individually, oruse sequence modeling Maximum entropy [Osborne, 2002] Condition random fields (CRF) [Galley, 2006] Classic Bayesian Method [Kupiec et al, 1995] HMM [Conroy and O'Leary, 2001; Maskey, 2006 ] Bayesian networks SVMs [Xie and Liu, 2010] Regression [Murray et al, 2005] Others3059So that is it with supervised methods? It seems it is a straightforward classificationproblem What are the issues with this method? How to get good quality labeled training data How to improve learning Some recent research has explored a fewdirections Discriminative training, regression, sampling, co-training, active learning60Motivation & DefinitionTopic ModelsGraph Based MethodsSupervised TechniquesGlobal Optimization MethodsSpeech SummarizationEvaluationFrequency, TF*IDF, Topic WordsTopic Models [LSA, EM, Bayesian]Manual (Pyramid), Automatic (Rouge, F-Measure)Fully AutomaticFeatures, Discriminative TrainingSampling, Data, Co-trainingIterative, Greedy, Dynamic ProgrammingILP, Sub-Modular SelectionSegmentation, ASRAcoustic Information, Disfluency3161Improving supervised methods: differenttraining approaches What are the problems with standard trainingmethods? Classifiers learn to determine a sentence?s label(in summary or not) Sentence-level accuracy is different fromsummarization evaluation criterion (e.g.,summary-level ROUGE scores) Training criterion is not optimal Sentences?
labels used in training may be toostrict (binary classes)62Improving supervised methods: MERTdiscriminative training Discriminative training based on MERT [Aker etal., 2010] In training, generate multiple summary candidates(using A* search algorithm) Adjust model parameters (feature weights)iteratively to optimize ROUGE scoresNote: MERT has been used for machine translation discriminative training3263Improving supervised methods: rankingapproaches Ranking approaches [Lin et al 2010] Pair-wise training Not classify each sentence individually Input to learner is a pair of sentences Use Rank SVM to learn the order of two sentences Direct optimization Learns how to correctly order/rank summary candidates(a set of sentences) Use AdaRank [Xu and Li 2007] to combine weak rankers64Improving supervised methods: regressionmodel Use regression model [Xie and Liu, 2010] In training, a sentence?s label is not +1 and -1 Each one is labeled with numerical values torepresent their importance Keep +1 for summary sentence For non-summary sentences (-1), use their similarity tothe summary as labels Train a regression model to better discriminatesentence candidates3365Improving supervised methods: sampling Problems -- in binary classification setup forsummarization, the two classes areimbalanced Summary sentences are minority class. Imbalanced data can hurt classifier training How can we address this? Sampling to make distribution more balanced totrain classifiers Has been studied a lot in machine learning66Improving supervised methods: sampling Upsampling: increase minority samples Replicate existing minority samples Generate synthetic examples (e.g., by some kindof interpolation) Downsampling: reduce majority samples Often randomly select from existing majoritysamples3467Improving supervised methods: sampling Sampling for summarization [Xie and Liu, 2010] Different from traditional upsampling and downsampling Upsampling select non-summary sentences that are like summarysentences based on cosine similarity or ROUGE scores change their label to positive Downsampling: select those that are different from summary sentences These also address some human annotation disagreement The instances whose labels are changed are often the onesthat humans have problems with68Motivation & DefinitionTopic ModelsGraph Based MethodsSupervised TechniquesGlobal Optimization MethodsSpeech SummarizationEvaluationFrequency, TF*IDF, Topic WordsTopic Models [LSA, EM, Bayesian]Manual (Pyramid), Automatic (Rouge, F-Measure)Fully AutomaticFeatures, Discriminative TrainingSampling, Data, Co-rainingIterative, Greedy, Dynamic ProgrammingILP, Sub-Modular SelectionSegmentation, ASRAcoustic Information, Disfluency3569Supervised methods: data issues Need labeled data for model training How do we get good quality training data? Can ask human annotators to select extractivesummary sentences However, human agreement is generally low What if data is not labeled at all?
or it onlyhas abstractive summary?70 Distributions of content units and words are similar Few units are expressed by everyone; many unitsare expressed by only one personDo humans agree on summary sentenceselection?
Human agreement on word/sentence/fact selection3671Supervised methods: semi-supervisedlearning Question ?
can we use unlabeled data tohelp supervised methods? A lot of research has been done on semi-supervised learning for various tasks Co-training and active learning have beenused in summarization72Co-training Use co-training to leverage unlabeled data Feature sets represent different views They are conditionally independent given theclass label Each is sufficient for learning Select instances based on one view, to help theother classifier3773Co-training in summarization In text summarization [Wong et al, 2008] Two classifiers (SVM, na?ve Bayes) are used onthe same feature set In speech summarization [Xie et al, 2010] Two different views: acoustic and lexical features They use both sentence and document asselection units74Active learning in summarization Select samples for humans to label Typically hard samples, machines are notconfident, informative ones Active learning in lecture summarization [Zhanget al 2009] Criterion: similarity scores between the extractedsummary sentences and the sentences in thelecture slides are high3875Supervised methods: using labeledabstractive summaries Question -- what if I only have abstractivesummaries, but not extractive summaries? No labeled sentences to use for classifiertraining in extractive summarization Can use reference abstract summary toautomatically create labels for sentences Use similarity of a sentence to the human writtenabstract (or ROUGE scores, other metrics)76Comment on supervised performance Easier to incorporate more information At the cost of requiring a large set of humanannotated training data Human agreement is low, therefore labeledtraining data is noisy Need matched training/test conditions may not easily generalize to different domains Effective features vary for different domains e.g., position is important for news articles3977Comments on supervised performance Seems supervised methods are moresuccessful in speech summarization than intext Speech summarization is almost never multi-document There are fewer indications about the topic of theinput in speech domains Text analysis techniques used in speechsummarization are relatively simpler78Motivation & DefinitionTopic ModelsGraph Based MethodsSupervised TechniquesOptimization MethodsSpeech SummarizationEvaluationFrequency, TF*IDF, Topic WordsTopic Models [LSA, EM, Bayesian]Manual (Pyramid), Automatic (Rouge, F-Measure)Fully AutomaticFeatures, Discriminative TrainingSampling, Data, Co-trainingIterative, Greedy, Dynamic ProgrammingILP, Sub-Modular SelectionSegmentation, ASRAcoustic Information, Disfluency4079Parameters to optimize In summarization methods we try to find1.
Most significant sentences2.
Remove redundant ones3.
Keep the summary under given length Can we combine all 3 steps in one? Optimize all 3 parameters at once80Summarization as an optimization problem Knapsack Optimization ProblemSelect boxes such that amount of money ismaximized while keeping total weight under X Kg Summarization ProblemSelect sentences such that summary relevance ismaximized while keeping total length under X words Many other similar optimization problems General Idea: Maximize a function given a set ofconstraints4181Optimization methods for summarization Different flavors of solutions Greedy Algorithm Choose highest valued boxes Choose the most relevant sentence Dynamic Programming algorithm Save intermediate computations Look at both relevance and length Integer Linear Programming Exact Inference Scaling IssuesWe will now discuss these 3 types of optimization solutions82Motivation & DefinitionTopic ModelsGraph Based MethodsSupervised TechniquesOptimization MethodsSpeech SummarizationEvaluationFrequency, TF*IDF, Topic WordsTopic Models [LSA, EM, Bayesian]Manual (Pyramid), Automatic (Rouge, F-Measure)Fully AutomaticFeatures, Discriminative TrainingSampling, Data, Co-trainingGreedy, Dynamic ProgrammingILP, Sub-Modular SelectionSegmentation, ASRAcoustic Information, Disfluency4283Greedy optimization algorithms Greedy solution is an approximate algorithm whichmay not be optimal Choose the most relevant + least redundantsentence if the total length does not exceed thesummary length Maximal Marginal Relevance is one such greedy algorithmproposed by [Carbonell et al, 1998]84Maximal Marginal Relevance (MMR)[Carbonell et al, 1998] Summary: relevant and non-redundant information Many summaries are built based on sentences ranked byrelevance E.g.
Extract most relevant 30% of sentencesRelevance Redundancyvs. Summary should maximize relevant information aswell as reduce redundancy4385Marginal relevance ?Marginal Relevance?
or ?Relevant Novelty? Measure relevance and novelty separately Linearly combine these two measures High Marginal relevance if Sentence is relevant to story (significant information) Contains minimal similarity to previously selected sentences(new novel information) Maximize Marginal Relevance to get summary thathas significant non-redundant information86Relevance with query or centroid We can compute relevance of text snippetwith respect to query or centroid Centroid as defined in [Radev, 2004] based on the content words of  a document TF*IDF vector of all documents in corpus Select words above a threshold : remaining vectoris a centroid vector4487Maximal Marginal Relevance (MMR)[Carbonell et al, 1998] Q ?
document centroid/user query D ?
document collection R ?
ranked listed S ?
subset of documents in R already selected Sim ?
similarity metric Lambda =1 produces most significant ranked list Lambda = 0 produces most diverse ranked listMMR?
Argmax(Di?R?S)[?
(Sim1(Di, Q))?(1??
)max(Dj?S)Sim2(Di, Dj)]88MMR based Summarization [Zechner, 2000]Iteratively select next sentenceNext Sentence =Frequency Vectorof all content wordscentroid4589MMR based summarization Why this iterative sentence selection processworks? 1st Term: Find relevant sentences similar tocentroid of the document 2nd Term: Find redundancy ?
sentences that aresimilar to already selected sentences are notselected90 MMR is an iterative sentence selectionprocess decision made for each sentence Is this selected sentence globally optimal?Sentence selection in MMRSentence with same level of relevance but shorter may not beselected if a longer relevant sentence is already selected4691Motivation & DefinitionTopic ModelsGraph Based MethodsSupervised TechniquesOptimization MethodsSpeech SummarizationEvaluationFrequency, TF*IDF, Topic WordsTopic Models [LSA, EM, Bayesian]Manual (Pyramid), Automatic (Rouge, F-Measure)Fully AutomaticFeatures, Discriminative TrainingSampling, Data, Co-trainingIterative, Greedy, Dynamic ProgrammingILP, Sub-Modular SelectionSegmentation, ASRAcoustic Information, Disfluency92Global inferenceD=t1, t2, , tn?1, tn Modify our greedy algorithm add constraints for sentence length as well Let us define document D with tn textualunits4793Global inference Let us defineRelevance of ti to be in thesummaryRedundancy between ti and tjLength of til(i)Red(i,j)Rel(i)94Inference problem [McDonald, 2007] Let us define inference problem asSummary ScorePairwise RedundancyMaximum Length4895Greedy solution [McDonald, 2007]Sort by RelevanceSelect Sentence Sorted list may have longer sentences at the top Solve it using dynamic programming Create table and fill it based on length and redundancyrequirementsNo consideration ofsentence length96Dynamic programming solution [McDonald, 2007]High scoring summaryof length k and i-1text unitsHigh scoringsummary oflength k-l(i) +tiHigher ?4997 Better than the previously shown greedyalgorithm Maximizes the space utilization by notinserting longer sentences These are still approximate algorithms:performance loss?Dynamic programming algorithm [McDonald, 2007]98Inference algorithms comparison[McDonald, 2007]System 50 100 200Baseline 26.6/5.3 33.0/6.8 39.4/9.6Greedy 26.8/5.1 33.5/6.9 40.1/9.5Dynamic Program 27.9/5.9 34.8/7.3 41.2/10.0Summarization results: Rouge-1/Rouge-2Sentence Length5099Motivation & DefinitionTopic ModelsGraph Based MethodsSupervised TechniquesOptimization MethodsSpeech SummarizationEvaluationFrequency, TF*IDF, Topic WordsTopic Models [LSA, EM, Bayesian]Manual (Pyramid), Automatic (Rouge, F-Measure)Fully AutomaticFeatures, Discriminative TrainingSampling, Data, Co-trainingIterative, Greedy, Dynamic ProgrammingILP, Sub-Modular SelectionSegmentation, ASRAcoustic Information, Disfluency100Integer Linear Programming (ILP) [Gillickand Favre, 2009; Gillick et al, 2009; McDonald, 2007] Greedy algorithm is an approximate solution Use exact solution algorithm with ILP (scaling issuesthough) ILP is constrained optimization problem Cost and constraints are linear in a set of integer variables Many solvers on the web Define the constraints based on relevance andredundancy for summarization Sentence based ILP N-gram based ILP51101Sentence-level ILP formulation [McDonald,2007]1 if ti in summaryConstraintsOptimization Function102N-gram ILP formulation [Gillick and Favre, 2009;Gillick et al, 2009] Sentence-ILP constraint on redundancy isbased on sentence pairs Improve by modeling n-gram-levelredundancy Redundancy implicitly definedCi indicates presenceof n-gram i in summaryand its weight is wi?i wici52103N-gram ILP formulation [Gillick and Favre, 2009]ConstraintsOptimization Function n-gram level ILP has different  optimizationfunction than one shown before104Sentence vs. n-gram ILPSystem ROUGE-2 PyramidBaseline 0.058 0.186Sentence ILP[McDonald, 2007] 0.072 0.295N-gram ILP[Gillick and Favre, 2009] 0.110 0.34553105Other optimization based summarizationalgorithms Submodular selection [Lin et al, 2009] Submodular set functions for optimization Modified greedy algorithm [Filatova, 2004] Event based features Stack decoding algorithm [Yih et al, 2007] Multiple stacks, each stack represents hypothesis of differentlength A* Search [Aker et al, 2010] Use scoring and heuristic functions106Submodular selection for summarization[Lin et al, 2009] Summarization Setup V ?
set of all sentences in document S ?
set of extraction sentences f(.)
scores the quality of the summary Submodularity been used in solving manyoptimization problems in near polynomial time For summarization:Select subset S (sentences) representative of Vgiven the constraint |S| =< K (budget)54107Submodular selection [Lin et al, 2009] If V are nodes in a Graph G=(V,E) representingsentences And E represents edges (i,j) such that w(i,j)represents similarity between sentences i and j Introduce submodular set functions which measures?representative?
S of entire set V [Lin et al, 2009] presented 4 submodular set functions108Submodular selection for summarization[Lin et al, 2009]Comparison of results using different methods55109Review: optimization methods Global optimization methods have shown to besuperior than 2-step selection process and reduceredundancy 3 parameters are optimized together Relevance Redundancy Length Various Algorithms for Global Inference Greedy Dynamic Programming Integer Linear Programming Submodular Selection110Motivation & DefinitionTopic ModelsGraph Based MethodsSupervised TechniquesOptimization MethodsSpeech SummarizationEvaluationFrequency, TF*IDF, Topic WordsTopic Models [LSA, EM, Bayesian]Manual (Pyramid), Automatic (Rouge, F-Measure)Fully AutomaticFeatures, Discriminative TrainingSampling, Data, Co-trainingIterative, Greedy, Dynamic ProgrammingILP, Sub-Modular SelectionSegmentation, ASRAcoustic Information, Disfluency56111Speech summarization Increasing amount of data available inspeech form meetings, lectures, broadcast, youtube, voicemail Browsing is not as easy as for text domains users need to listen to the entire audio Summarization can help effective informationaccess Summary output can be in the format of textor speech112Domains Broadcast news Lectures/presentations Multiparty meetings Telephone conversations Voicemails57113ExampleMeeting transcripts and summary sentences (in red)so it?s possible that we could do something like asummary node of some sort thatme003but there is some technology you could try to applyme010yeahme010now I don?t know that any of these actually apply inthis caseme010uh so if you co- you could ima- and i-me010mmmme003there?re ways to uh sort of back off on the purity ofyour bayes-net-ednessme010andme010uh i- i slipped a paper to bhaskara and about noisy-or?s and noisy-maxesme010which is there are technical ways of doing itme010uh let me just mention something that i don?t wantto pursue todayme010there there are a variety of ways of doing itme010Broadcast news transcripts and summary (in red)try to use electrical appliances before p.m. and after p.m. andturn off computers, copiers and lights when they're not beingusedset your thermostat at 68 degrees when you're home, 55degrees when  you're awayenergy officials are offering tips to conserve electricity, they say,to delay holiday lighting until after at nightthe area shares power across many statesmeanwhile, a cold snap in the pacific northwest is putting anadded strain on power suppliescoupled with another unit, it can provide enough power for about2 million peopleit had been shut down for maintenancea unit at diablo canyon nuclear plant is expected to resumeproduction todaycalifornia's strained power grid is getting a boost today whichmight help increasingly taxed power supplies114Speech vs. text summarization: similarities When high quality transcripts are available Not much different from text summarization Many similar approaches have been used Some also incorporate acoustic information For genres like broadcast news, style is alsosimilar to text domains58115Speech vs. text summarization: differences Challenges in speech summarization Speech recognition errors can be very high Sentences are not as well formed as in most textdomains: disfluencies, ungrammatical There are not clearly defined sentences Information density is also low (off-topicdiscussions, chit chat, etc.
) Multiple participants116Motivation & DefinitionTopic ModelsGraph Based MethodsSupervised TechniquesOptimization MethodsSpeech SummarizationEvaluationFrequency, TF*IDF, Topic WordsTopic Models [LSA, EM, Bayesian]Manual (Pyramid), Automatic (Rouge, F-Measure)Fully AutomaticFeatures, Discriminative TrainingSampling, Data, Co-trainingIterative, Greedy, Dynamic ProgrammingILP, Sub-Modular SelectionSegmentation, ASRAcoustic Information, Disfluency59117What should be extraction units in speechsummarization? Text domain Typically use sentences (based on punctuationmarks) Speech domain Sentence information is not available Sentences are not as clearly definedUtterance from previous example:there there are a variety of ways of doing it uh let me just mention somethingthat i don?t want to pursue today which is there are technical ways of doing it118Automatic sentence segmentation (side note) For a word boundary, determine whether it?s a sentenceboundary Different approaches: Generative: HMM Discriminative: SVM, boosting, maxent, CRF Information used: word n-gram, part-of-speech, parsinginformation, acoustic info (pause, pitch, energy)60119What is the effect of differentunits/segmentation on summarization? Research has used different units in speechsummarization Human annotated sentences or dialog acts Automatic sentence segmentation Pause-based segments Adjacency pairs Intonational phrases Words120What is the effect of differentunits/segmentation on summarization? Findings from previous studies Using intonational phrases (IP) is better thanautomatic sentence segmentation, pause-basedsegmentation [Maskey, 2008 ] IPs are generally smaller than sentences, alsolinguistically meaningful Using sentences is better than words, betweenfiller segments [Furui et al, 2004] Using human annotated dialog acts is better thanautomatically generated ones [Liu and Xie, 2008]61121Motivation & DefinitionTopic ModelsGraph Based MethodsSupervised TechniquesOptimization MethodsSpeech SummarizationEvaluationFrequency, TF*IDF, Topic WordsTopic Models [LSA, EM, Bayesian]Manual (Pyramid), Automatic (Rouge, F-Measure)Fully AutomaticFeatures, Discriminative TrainingSampling, Data, Co-trainingIterative, Greedy, Dynamic ProgrammingILP, Sub-Modular SelectionSegmentation, ASRAcoustic Information, Disfluency122Using acoustic information insummarization Acoustic/prosodic features: F0 (max, min, mean, median, range) Energy (max, min, mean, median, range) Sentence duration Speaking rate (# of words or letters) Need proper normalization Widely used in supervised methods, incombination with textual features62123Using acoustic information insummarization Are acoustic features useful when combiningit with lexical information? Results vary depending on the tasks anddomains Often lexical features are ranked higher But acoustic features also contribute to overallsystem performance Some studies showed little impact when addingspeech information to textual features [Penn and Zhu,2008]124Using acoustic information insummarization Can we use acoustic information only for speechsummarization? Transcripts may not be available Another way to investigate contribution of acousticinformation Studies showed using just acoustic information canachieve similar performance to using lexicalinformation [Maskey and Hirschberg, 2005; Xie et al, 2009; Zhu et al,2009] Caveat: in some experiments, lexical information is used(e.g., define the summarization units)63125Speech recognition errors ASR is not perfect, often high word error rate 10-20% for read speech 40% or even higher for conversational speech Recognition errors generally have negativeimpact on summarization performance Important topic indicative words are incorrectlyrecognized Can affect term weighting and sentence scores126Speech recognition errors Some studies evaluated effect of recognitionerrors on summarization by varying worderror rate [Christensen et al, 2003; Penn and Zhu, 2008; Lin et al,2009] Degradation is not much when word errorrate is not too low (similar to spokendocument retrieval) Reason: better recognition accuracy in summarysentences than overall64127What can we do about ASR errors? Deliver summary using original speech Can avoid showing recognition errors in thedelivered text summary But still need to correctly identify summarysentences/segments Use recognition confidence measure andmultiple candidates to help better summarize128Address problems due to ASR errors Re-define summarization task: selectsentences that are most informative, at thesame time have high recognition accuracy Important words tend to have high recognitionaccuracy Use ASR confidence measure or n-gramlanguage model scores in summarization Unsupervised methods [Zechner, 2002; Kikuchi et al, 2003;Maskey, 2008] Use as a feature in supervised methods65129Address problems due to ASR errors Use multiple recognition candidates n-best lists [Liu et al, 2010] Lattices [Lin et al, 2010] Confusion network [Xie and Liu, 2010] Use in MMR framework Summarization segment/unit contains all the wordcandidates (or pruned ones based on probabilities) Term weights (TF, IDF) use candidate?s posteriors Improved performance over using 1-best recognitionoutput130Motivation & DefinitionTopic ModelsGraph Based MethodsSupervised TechniquesOptimization MethodsSpeech SummarizationEvaluationFrequency, TF*IDF, Topic WordsTopic Models [LSA, EM, Bayesian]Manual (Pyramid), Automatic (Rouge, F-Measure)Fully AutomaticFeatures, Discriminative TrainingSampling, Data, Co-trainingIterative, Greedy, Dynamic ProgrammingILP, Sub-Modular SelectionSegmentation, ASRAcoustic Information, Disfluency66131Disfluencies and summarization Disfluencies (filler words, repetitions, revisions,restart, etc) are frequent in conversational speech Example from meeting transcript:so so does i- just remind me of what what you were going to do with thewhat what what what'sy- you just described what you've been doing Existence may hurt summarization systems, alsoaffect human readability of the summaries132Disfluencies and summarization Natural thought: remove disfluenices Word-based selection can avoid disfluentwords Using n-gram scores tends to select fluentparts [Hori and Furui, 2001] Remove disfluencies first, then performsummarization Does it work?
not consistent results Small improvement [Maskey, 2008; Zechner, 2002] No improvement [Liu et al, 2007]67133Disfluencies and summarization In supervised classification, information related todisfluencies can be used as features forsummarization Small improvement on Switchboard data [Zhu and Penn, 2006] Going beyond disfluency removal, can performsentence compression in conversational speech toremove un-necessary words [Liu and Liu, 2010] Help improve sentence readability Output is more like abstractive summaries Compression helps summarization134Review on speech summarization Speech summarization has been performedfor different domains A lot of text-based approaches have beenadopted Some speech specific issues have beeninvestigated Segmentation ASR errors Disfluencies Use acoustic information68135Motivation & DefinitionTopic ModelsGraph Based MethodsSupervised TechniquesGlobal Optimization MethodsSpeech SummarizationEvaluationFrequency, TF*IDF, Topic WordsTopic Models [LSA, EM, Bayesian]Manual (Pyramid), Automatic (Rouge, F-Measure)Fully AutomaticFeatures, Discriminative TrainingSampling, Data, Co-trainingIterative, Greedy, Dynamic ProgrammingILP, Sub-Modular SelectionSegmentation, ASRAcoustic Information, Disfluency136Manual evaluations Task-based evaluations too expensive Bad decisions possible, hard to fix Assessors rate summaries on a scale Responsiveness Assessors compare with gold-standards Pyramid69137Automatic and fully automaticevaluation Automatically compare with gold-standard Precision/recall (sentence level) ROUGE (word level) No human gold-standard is used Automatically compare input and summary138Precision and recall for extractivesummaries Ask a person to select the most importantsentencesRecall: system-human choiceoverlap/sentences chosen by humanPrecision: system-human choiceoverlap/sentences chosen by system70139Problems? Different people choose different sentences The same summary can obtain a recall scorethat is between 25% and 50% differentdepending on which of two available humanextracts is used for evaluation Recall more important/informative thanprecision?140More problems? GranularityWe need help.
Fires have spread in the nearbyforest and threaten several villages in this remotearea. Semantic equivalence Especially in multi-document summarization Two sentences convey almost the sameinformation: only one will be chosen in the humansummary71141PyramidResponsivenessROUGEFully automaticModelsummariesManual comparison/ratingsEvaluation methods for content142Pyramid method [Nenkova and Passonneau, 2004; Nenkova et al,2007] Based on Semantic Content Units (SCU) Emerge from the analysis of several texts Link different surface realizations with thesame meaning72143SCU exampleS1 Pinochet arrested in London on Oct 16 at aSpanish judge?s request for atrocities againstSpaniards in Chile.S2 Former Chilean dictator Augusto Pinochet hasbeen arrested in London at the request of theSpanish government.S3 Britain caused international controversy andChilean turmoil by arresting former Chileandictator Pinochet in London.144SCU: label, weight, contributorsLabel London was where Pinochet wasarrestedWeight=3S1 Pinochet arrested in London on Oct 16 at a Spanishjudge?s request for atrocities against Spaniards in Chile.S2 Former Chilean dictator Augusto Pinochet has beenarrested in London at the request of the Spanishgovernment.S3 Britain caused international controversy and Chileanturmoil by arresting former Chilean dictator Pinochet inLondon.73145Ideally informative summary Does not include an SCU from a lower tierunless all SCUs from higher tiers areincluded as well146Ideally informative summary Does not include an SCU from a lower tierunless all SCUs from higher tiers areincluded as well74147Ideally informative summary Does not include an SCU from a lower tierunless all SCUs from higher tiers areincluded as well148Ideally informative summary Does not include an SCU from a lower tierunless all SCUs from higher tiers areincluded as well75149Ideally informative summary Does not include an SCU from a lower tierunless all SCUs from higher tiers areincluded as well150Ideally informative summary Does not include an SCU from a lower tierunless all SCUs from higher tiers areincluded as well76151Different equally good summaries Pinochet arrested Arrest in London Pinochet is a formerChilean dictator Accused of atrocitiesagainst Spaniards152Different equally good summaries Pinochet arrested Arrest in London On Spanish warrant Chile protests77153Diagnostic ?
why is a summary bad? Good  Less relevantsummary154Importance of content Can observe distribution in humansummaries Assign relative importance Empirical rather than subjective The more people agree, the more important78155Pyramid score for evaluation New summary with n content units Estimates the percentage of information that ismaximally importantIdealWightightObservedWeIdealWeightniinii=?
?==11156ROUGE [Lin, 2004] De facto standard for evaluation in textsummarization High correlation with manual evaluations in thatdomain More problematic for some other domains,particularly speech Not highly correlated with manual evaluations May fail to distinguish human and machinesummaries79157ROUGE details In fact a suite of evaluation metrics Unigram Bigram Skip bigram Longest common subsequence Many settings concerning Stopwords Stemming Dealing with multiple models158How to evaluate without humaninvolvement?
[Louis and Nenkova, 2009] A good summary should be similar to theinput Multiple ways to measure similarity Cosine similarity KL divergence JS divergence Not all work!80159 Distance between two distributions asaverage KL divergence from their meandistributionJS divergence between input andsummary)]||()||([)||( 21 ASummKLAInpKLSummInpJS +=SummaryandInputofondistributimeanSummInpA ,2+=160Summary likelihood given the input Probability that summary is generated according toterm distribution in the inputHigher likelihood ~ better summary Unigram Model Multinomial ModeliinrInpnInpnInpwwordofsummaryincountnvocabularysummaryrwpwpwp r=?
)()()( 21 21 KsizesummarynNwpwpwpiinrInpnInpnInpnnN rr==?
)()()( 211 21!!!
KK81161 Fraction of summary = input?s topic words % of input?s topic words also appearing in summary Capture variety Cosine similarity: input?s topic words and all summarywords Fewer dimensions, more specific vectorsTopic words identified by log-likelihoodtest162How good are these metrics?48 inputs, 57 systemsJSD -0.880 -0.7360.795 0.627-0.763 -0.6940.712 0.6470.712 0.602-0.688 -0.585-0.188 -0.1010.222 0.235% input?s topic in summaryKL div summ-inputCosine similarity% of summary = topic wordsKL div input-summUnigram summ prob.Multinomial summ prob.-0.699 0.629Topic word similarityPyramid ResponsivenessSpearman correlation on macro level for the query focused task.82163 JSD correlations with pyramid scores even better thanR1-recall R2-recall is consistently better Can extend features using higher order n-gramsHow good are these metrics?0.870.90R2-recall0.800.85R1-recall-0.73-0.88JSDResp.Pyramid164Motivation & DefinitionTopic ModelsGraph Based MethodsSupervised TechniquesGlobal Optimization MethodsSpeech SummarizationEvaluationFrequency, TF*IDF, Topic WordsTopic Models [LSA, EM, Bayesian]Manual (Pyramid), Automatic (Rouge, F-Measure)Fully AutomaticFeatures, Discriminative TrainingSampling, Data, Co-trainingIterative, Greedy, Dynamic ProgrammingILP, Sub-Modular SelectionSegmentation, ASRAcoustic Information, Disfluency83165Current summarization research Summarization for various new genres Scientific articles Biography Social media (blog, twitter) Other text and speech data New task definition Update summarization Opinion summarization New summarization approaches Incorporate more information (deep linguistic knowledge, informationfrom the web) Adopt more complex machine learning techniques Evaluation issues Better automatic metrics Extrinsic evaluationsAnd more?166 Check out summarization papers at ACL thisyear Workshop at ACL-HLT 2011: Automatic summarization for different genres,media, and languages [June 23, 2011] http://www.summarization2011.org/84167References Ahmet Aker, Trevor Cohn, Robert Gaizauska.
2010.
Multi-document summarization using A* search anddiscriminative training.
Proc.
of EMNLP. R. Barzilay and M. Elhadad.
2009.
Text summarizations with lexical chains.
In: I. Mani and M. Maybury (eds.
):Advances in Automatic Text Summarization. Jaime Carbonell and Jade Goldstein.
1998.
The Use of MMR, Diversity-Based reranking for ReorderingDocuments and Producing Summaries.
Proceedings of the 21st Annual International ACM SIGIR Conference onResearch and Development in Information Retrieval. H. Christensen, Y. Gotoh, B. Killuru, and S. Renals.
2003.
Are Extractive Text Summarization TechniquesPortable to Broadcast News?
Proc.
of ASRU. John Conroy and Dianne O'Leary.
2001.
Text Summarization via Hidden Markov Models.
Proc.
of SIGIR. J. M. Conroy, J. D. Schlesinger, and D. P. OLeary.
2006.
Topic-Focused Multi-Document Summarization Using anApproximate Oracle Score.
Proc.
COLING/ACL 2006. pp.
152-159. Thomas Cormen, Charles E. Leiserson, and Ronald L. Rivest.1990.
Introduction to algorithms.
MIT Press. G. Erkan and D. R. Radev.2004.
LexRank: Graph-based Centrality as Salience in Text Summarization.
Journal ofArtificial Intelligence Research (JAIR). Pascale Fung, Grace Ngai, and Percy Cheung.
2003.
Combining optimal clustering and hidden Markov models forextractive summarization.
Proceedings of ACL Workshop on Multilingual Summarization.Sadoki Furui, T. Kikuchi,Y.
Shinnaka, and C. Hori.
2004.
Speech-to-text and Speech-to-speech Summarization of Spontaneous Speech.IEEE Transactions on Audio, Speech, and Language Processing.
12(4), pages 401-408. Michel Galley.
2006.
A Skip-Chain Conditional Random Field for Ranking Meeting Utterances by Importance.Proc.
of EMNLP. Dan Gillick, Benoit Favre.
2009.
A scalable global model for summarization.
Proceedings of the Workshop onInteger Linear Programming for Natural Language Processing. Dan Gillick, Korbinian Riedhammer, Benoit Favre, Dilek Hakkani-Tur.
2009.
A global optimization framework formeeting summarization.
Proceedings of ICASSP. Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and Mark Kantrowitz.
2000.
Multi-document summarization bysentence extraction.
Proceedings of the 2000 NAACL-ANLP Workshop on Automatic Summarization.168References Y. Gong and X. Liu.
2001.
Generic text summarization using relevance measure and latent semantic analysis.Proc.
ACM SIGIR. I. Gurevych and T. Nahnsen.
2005.
Adapting Lexical Chaining to Summarize Conversational Dialogues.
Proc.RANLP. B. Hachey, G. Murray, and D. Reitter.2006.
Dimensionality reduction aids term co-occurrence based multi-document summarization.
In: SumQA 06: Proceedings of the Workshop on Task-Focused Summarization andQuestion Answering. Aria Haghighi and Lucy Vanderwende.
2009.
Exploring content models for multi-document summarization.
Proc.
ofNAACL-HLT. L. He, E. Sanocki, A. Gupta, and J. Grudin.
2000.
Comparing presentation summaries: Slides vs. reading vs.listening.
Proc.
of SIGCHI on Human factors in computing systems. C. Hori and Sadaoki Furui.
2001.
Advances in Automatic Speech Summarization.
Proc.
of Eurospeech. T. Kikuchi, S. Furui, and C. Hori.
2003.
Automatic Speech Summarization based on Sentence Extractive andCompaction.
Proc.
of ICSLP. Julian Kupiec, Jan Pedersen, and Francine Chen.
1995.
A Trainable Document Summarizer.
Proc.
of SIGIR. J. Leskovec, N. Milic-frayling, and M. Grobelnik.
2005.
Impact of Linguistic Analysis on the Semantic GraphCoverage and Learning of Document Extracts.
Proc.
AAAI. Chin-Yew Lin.
2004.
ROUGE: A Package for Automatic Evaluation of Summaries, Workshop on TextSummarization Branches Out. C.Y.
Lin and E. Hovy.
2000.
The automated acquisition of topic signatures for text summarization.
Proc.
COLING. Hui Lin and Jeff Bilmes.
2010.
Multi-document summarization via budgeted maximization of submodular functions.Proc.
of NAACL. Hui Lin and Jeff Bilmes and Shasha Xie.
2009.
Graph-based Submodular Selection for Extractive Summarization.Proceedings of ASRU. Shih-Hsiang Lin and Berlin Chen.
2009.
Improved Speech Summarization with Multiple-hypothesisRepresentations and Kullback-Leibler Divergence Measures.
Proc.
of Interspeech. Shih-Hsiang Lin, Berlin Chen, and H. Min Wang.
2009.
A Comparative Study of Probabilistic Ranking Models forChinese Spoken Document Summarization.
ACM Transactions on Asian Language Information Processing.85169References Shih Hsiang Lin, Yu Mei Chang, Jia Wen Liu, Berlin Chen.
2010 Leveraging Evaluation Metric-related TrainingCriteria for Speech Summarization.
Proc.
of ICASSP. Fei Liu and Yang Liu.
2009.
From Extractive to Abstractive Meeting Summaries: Can it be done by sentencecompression?
Proc.
of ACL. Fei Liu and Yang Liu.
2010.
Using Spoken Utterance Compression for Meeting Summarization: A pilot study.
Proc.of IEEE SLT. Yang Liu and Shasha Xie.
2008.
Impact of Automatic Sentence Segmentation on Meeting Summarization.
Proc.
ofICASSP. Yang Liu, Feifan Liu, Bin Li, and Shasha Xie.
2007.
Do Disfluencies Affect Meeting Summarization: A pilot studyon the impact of disfluencies.
Poster at MLMI. Yang Liu, Shasha Xie, and Fei Liu.
2010.
Using n-best Recognition Output for Extractive Summarization andKeyword Extraction in Meeting Speech.
Proc.
of ICASSP. Annie Louis and Ani Nenkova.
2009.
Automatically evaluating content selection in summarization withouthuman models.
Proceedings of EMNLP H.P.
Luhn.
1958.
The Automatic Creation of Literature Abstracts.
IBM Journal of Research and Development 2(2). Inderjeet Mani, Gary Klein, David House, Lynette Hirschman, Therese Firmin, and Beth Sundheim.
2002.SUMMAC: a text summarization evaluation.
Natual Language Engineering.
8,1 (March 2002), 43-68. Manuel J. Mana-Lopez, Manuel De Buenaga, and Jose M. Gomez-Hidalgo.
2004.
Multidocument summarization:An added value to clustering in interactive retrieval.
ACM Trans.
Inf.
Systems. Sameer Maskey.
2008.
Automatic Broadcast News Summarization.
Ph.D thesis.
Columbia University. Sameer Maskey and Julia Hirschberg.
2005.
Comparing lexical, acoustic/prosodic, discourse and structuralfeatures for speech summarization.
Proceedings of Interspeech. Sameer Maskey and Julia Hirschberg.
2006.
Summarizing Speech Without Text Using Hidden Markov Models.Proc.
of HLT-NAACL. Ryan McDonald.
2007.
A Study of Global Inference Algorithms in Multi-document Summarization.
Lecture Notes inComputer Science.
Advances in Information Retrieval. Kathleen McKeown, Rebecca J. Passonneau, David K. Elson, Ani Nenkova, and Julia Hirschberg.
2005.
Dosummaries help?.
Proc.
of SIGIR. K. McKeown, J. L. Klavans, V. Hatzivassiloglou, R. Barzilay, and E. Eskin.1999.
Towards multidocumentsummarization by reformulation: progress and prospects.
Proc.
AAAI 1999.170References R. Mihalcea and P. Tarau .2004.
Textrank: Bringing order into texts.
Proc.
of EMNLP 2004. G. Murray, S. Renals, J. Carletta, J. Moore.
2005.
Evaluating Automatic Summaries of Meeting Recordings.
Proc.of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation. G. Murray, T. Kleinbauer, P. Poller, T. Becker, S. Renals, and J. Kilgour.
2009.
Extrinsic SummarizationEvaluation: A Decision Audit Task.
ACM Transactions on Speech and Language Processing. A. Nenkova and R. Passonneau.
2004.
Evaluating Content Selection in Summarization: The Pyramid Method.Proc.
HLT-NAACL. A. Nenkova, L. Vanderwende, and K. McKeown.
2006.
A compositional context sensitive multi-documentsummarizer: exploring the factors that influence summarization.
Proc.
ACM SIGIR. A. Nenkova, R. Passonneau, and K. McKeown.
2007.
The Pyramid Method: Incorporating human contentselection variation in summarization evaluation.
ACM Trans.
Speech Lang.
Processing. Miles Osborne.
2002.
Using maximum entropy for sentence extraction.
Proc.
of ACL Workshop on AutomaticSummarization. Gerald Penn and Xiaodan Zhu.
2008.
A critical Reassessement of Evaluation Baselines for SpeechSummarization.
Proc.
of ACL-HLT. Dmitri G. Roussinov and Hsinchun Chen.
2001.
Information navigation on the web by clustering and summarizingquery results.
Inf.
Process.
Manage.
37, 6 (October 2001), 789-816. B. Schiffman, A. Nenkova, and K. McKeown.
2002.
Experiments in Multidocument Summarization.
Proc.
HLT. A. Siddharthan, A. Nenkova, and K. Mckeown.2004.
Syntactic Simplification for Improving Content Selection inMulti-Document Summarization.
Proc.
COLING. H. Grogory Silber and Kathleen F. McCoy.
2002.
Efficiently computed lexical chains as an intermediaterepresentation for automatic text summarization.
Computational.
Linguist.
28, 4 (December 2002), 487-496. J. Steinberger, M. Poesio, M. A. Kabadjov, and K. Jeek.
2007.
Two uses of anaphora resolution in summarization.Inf.
Process.
Manage.
43(6). S. Tucker and S. Whittaker.
2008.
Temporal compression of speech: an evaluation.
IEEE Transactions on Audio,Speech and Language Processing, pages 790-796. L. Vanderwende, H. Suzuki, C. Brockett, and A. Nenkova.
2007.
Beyond SumBasic: Task-focused summarizationwith sentence simplification and lexical expansion.
Information Processing and Management 43.86171References Kam-Fai Wong, Mingli Wu, and Wenjie Li.
2008.
Extractive Summarization using Supervised and Semi-supervisedlearning.
Proc.
of ACL. Shasha Xie and Yang Liu.
2010.
Improving Supervised Learning for Meeting Summarization using Sampling andRegression.
Computer Speech and Language.
V24, pages 495-514. Shasha Xie and Yang Liu.
2010.
Using Confusion Networks for Speech Summarization.
Proc.
of NAACL. Shasha Xie, Dilek Hakkani-Tur, Benoit Favre, and Yang Liu.
2009.
Integrating Prosodic Features in ExtractiveMeeting Summarization.
Proc.
of ASRU. Shasha Xie, Hui Lin, and Yang Liu.
2010.
Semi-supervised Extractive Speech Summarization via Co-trainingAlgorithm.
Proc.
of Interspeech. S. Ye, T.-S. Chua, M.-Y.
Kan, and L. Qiu.
2007.
Document concept lattice for text understanding andsummarization.
Information Processing and Management 43(6). W. Yih, J. Goodman, L. Vanderwende, and H. Suzuki.
2007.
Multi-Document Summarization by MaximizingInformative Content-Words.
Proc.
IJCAI 2007. Klaus Zechner.
2002.
Automatic Summarization of Open-domain Multiparty Dialogues in Diverse Genres.Computational Linguistics.
V28, pages 447-485. Klaus Zechner and Alex Waibel.
2000.
Minimizing word error rate in textual summaries of spoken language.Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference. Justin Zhang and Pascale Fung.
2009.
Extractive Speech Summarization by Active Learning.
Proc.
of ASRU. Xiaodan Zhu and Gerald Penn.
2006.
Comparing the Roles of Textual, Acoustic and Spoken-language Featureson Spontaneous Conversation Summarization.
Proc.
of HLT-NAACL. Xiaodan Zhu, Gerald Penn, and F. Rudzicz.
2009.
Summarizing Multiple Spoken Documents: Finding Evidencefrom Untranscribed Audio.
Proc.
of ACL.
