Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
561?570, Prague, June 2007. c?2007 Association for Computational LinguisticsGenerating Lexical Analogies Using Dependency RelationsAndy Chiu, Pascal Poupart, and Chrysanne DiMarcoDavid R. Cheriton School of Computer ScienceUniversity of WaterlooWaterloo, Ontario, Canada{pachiu,ppoupart,cdimarco}@uwaterloo.caAbstractA lexical analogy is a pair of word-pairsthat share a similar semantic relation.
Lex-ical analogies occur frequently in text andare useful in various natural language pro-cessing tasks.
In this study, we present asystem that generates lexical analogies au-tomatically from text data.
Our system dis-covers semantically related pairs of wordsby using dependency relations, and appliesnovel machine learning algorithms to matchthese word-pairs to form lexical analogies.Empirical evaluation shows that our systemgenerates valid lexical analogies with a pre-cision of 70%, and produces quality outputalthough not at the level of the best human-generated lexical analogies.1 IntroductionAnalogy discovery and analogical reasoning are ac-tive research areas in a multitude of disciplines, in-cluding philosophy, psychology, cognitive science,linguistics, and artificial intelligence.
A type of anal-ogy that is of particular interest in natural languageprocessing is lexical analogy.
A lexical analogy is apair of word-pairs that share a similar semantic rela-tion.
For example, the word-pairs (dalmatian, dog)and (trout, fish) form a lexical analogy because dal-matian is a subspecies of dog just as trout is a sub-species of fish, and the word-pairs (metal, electric-ity) and (air, sound) form a lexical analogy becausein both cases the initial word serves as a conductorfor the second word.
Lexical analogies occur fre-quently in text and are useful in various natural lan-guage processing tasks.
For example, understandingmetaphoric language such as ?the printer died?
re-quires the recognition of implicit lexical analogies,in this case between (printer, malfunction) and (per-son, death).
Lexical analogies also have applica-tions in word sense disambiguation, information ex-traction, question-answering, and semantic relationclassification (see (Turney, 2006)).In this study, we present a novel system for gen-erating lexical analogies directly from a text cor-pus without relying on dictionaries or other seman-tic resources.
Our system uses dependency relationsto characterize pairs of semantically related words,then compares the similarity of their semantic rela-tions using two machine learning algorithms.
Wealso present an empirical evaluation that shows oursystem generates valid lexical analogies with a pre-cision of 70%.
Section 2 provides a list of defini-tions, notations, and necessary background materi-als.
Section 3 describes the methods used in oursystem.
Section 4 presents our empirical evalua-tion.
Section 5 reviews selected related work.
Fi-nally, Section 6 concludes the paper with suggestedfuture work and a brief conclusion.2 DefinitionsA word-pair is a pair of entities, where each entityis a single word or a multi-word named entity.
Theunderlying relations of a word-pair (w1, w2) are thesemantic relations1 between w1 and w2.
For exam-1Here ?semantic relations?
include both classical relationssuch as synonymy and meronymy, and non-classical relationsas defined by Morris and Hirst (2004).561ple, the underlying relations of (poet, poem) includeproduces, writes, enjoys, and understands.
A lexicalanalogy is a pair of word-pairs that share at least oneidentical or similar underlying relation.A key linguistic formalism we use is dependencygrammar (Tesnie`re, 1959).
A dependency gram-mar describes the syntactic structure of a sentencein a manner similar to the familiar phrase-structuregrammar.
However, unlike phrase-structure gram-mars which associate each word of a sentence tothe syntactic phrase in which the word is contained,a dependency grammar associates each word to itssyntactic superordinate as determined by a set ofrules.
Each pair of depending words is called adependency.
Within a dependency, the word beingdepended on is called the governor, and the worddepending on the governor is called the dependent.Each dependency is also labelled with the syntac-tic relation between the governor and the dependent.Dependency grammars require that each word of asentence have exactly one governor, except for oneword called the head word which has no governor atall.
A proposition p that is governor to exactly oneword w1 and dependent of exactly one word w2 isoften collapsed (Lin and Pantel, 2001); that is, thetwo dependencies involving p are replaced by a sin-gle dependency between w1 and w2 labelled p.The dependency structure of a sentence can beconcisely represented by a dependency tree, inwhich each word is a node, each dependent is a childof its governor, and the head word is the root.
A de-pendency path is an undirected path through a de-pendency tree, and a dependency pattern is a depen-dency path with both ends replaced by slots (Lin andPantel, 2001).
Figure 1 illustrates various depen-dency structures of the sentence, rebels fired rocketsat a military convoy, after each word is lemmatized.3 MethodsWe consider lexical analogy generation as a se-quence of two key problems: data extraction andrelation-matching.
Data extraction involves theidentification and extraction of pairs of semanticallyrelated words, as well as features that characterizetheir relations.
Relation-matching involves match-ing word-pairs with similar features to form lexi-cal analogies.
We describe our methods for solvingthese two problems in the following subsections.3.1 Data ExtractionExtracting Word-PairsTo identify semantically related words, we relyon the assumption that highly syntactically relatedwords also tend to be semantically related ?
a hy-pothesis that is supported by works such as Levin?s(1993) study of English verbs.
As such, the de-pendency structure of a sentence can be used to ap-proximate the semantic relatedness between its con-stituent words.
Our system uses a dependency parserto parse the input text into a set of dependency trees,then searches through these trees to extract depen-dency paths satisfying the following constraints:1.
The path must be of the form noun-verb-noun.2.
One of the nouns must be the subject of theclause to which it belongs.Each of these paths is then turned into a word-pairby taking its two nouns.
The path constraints that weuse are suggested by the subject-verb-object (SVO)pattern commonly used in various relation extractionalgorithms.
However, our constraints allow signifi-cantly more flexibility than the SVO pattern in twoimportant aspects.
First, our constraints allow anarbitrary relation between the verb and the secondnoun, not just the object relation.
Hence, word-pairscan be formed from a clause?s subject and its loca-tion, time, instrument, and other arguments, whichare clearly semantically related to the subject.
Sec-ondly, searching in the space of dependency trees in-stead of raw text data means that we are able to findsemantically related words that are not necessarilyadjacent to each other in the sentence.It is important to note that, although these con-straints improve the precision of our system and tendto identify effectively the most relevant word-pairs,they are not strictly necessary.
Our system would befully functional using alternative sets of constraintstailored for specific applications, or even with noconstraints at all.Using the sentence in Figure 1 as an example, oursystem would extract the dependency paths ?rebelsubj?
fireobj?
rocket?
and ?rebelsubj?
fire at?
con-voy?, and would thus generate the word-pairs (rebel,rocket) and (rebel, convoy).562Figure 1: Dependency structures of ?rebels fired rockets at a military convoy?
after lemmatizationExtracting FeaturesRecall that each word-pair originates from a de-pendency path.
The path, and in particular the mid-dle verb, provides a connection between the twowords of the word-pair, and hence is a good in-dication of their semantic relation.
Therefore, foreach word-pair extracted, we also extract the depen-dency pattern derived from the word-pair?s depen-dency path as a feature for the word-pair.
We furtherjustify this choice of feature by noting that the useof dependency patterns have previously been shownto be effective at characterizing lexico-syntactic re-lations (Lin and Pantel, 2001; Snow et al, 2004).Using Figure 1 as an example again, the depen-dency patterns ?subj?
fireobj?
?
and ?subj?fire at?
?
would be extracted as a feature of (rebel,rocket) and (rebel, convoy), respectively.FilteringWord-pairs and features extracted using only de-pendency relations tend to be crude in several as-pects.
First, they contain a significant amount ofnoise, such as word-pairs that have no meaningfulunderlying relations.
Noise comes from grammati-cal and spelling mistakes in the original input data,imperfect parsing, as well as the fact that depen-dency structure only approximates semantic related-ness.
Secondly, some of the extracted word-pairscontain underlying relations that are too general ortoo obscure for the purpose of lexical analogy gen-eration.
For example, consider the word-pair (com-pany, right) from the sentence ?the company exer-cised the right to terminate his contract?.
The twowords are clearly semantically related, however therelation (have or entitled-to) is very general and itis difficult to construct satisfying lexical analogiesfrom the word-pair.
Lastly, some features are alsosubject to the same problem.
The feature ?subj?sayobj?
?, for example, has very little characteri-zation power because almost any pair of words canoccur with this feature.In order to retain only the most relevant word-pairs and features, we employ a series of refiningfilters.
All of our filters rely on the occurrencestatistics of the word-pairs and features.
Let W ={wp1, wp2, ..., wpn} be the set of all word-pairs andF = {f1, f2, ..., fm} the set of all features.
Let Fwpbe the set of features of word-pair wp, and let Wfbe the set of word-pairs associated with feature f .Let O(wp) be the total number of occurrences ofword-pair wp, O(f) be the total number of occur-rences of feature f , and O(wp, f) be the number ofoccurrences of word-pair wp with feature f .
Thefollowing filters are used:1.
Occurrence filter: Eliminate word-pair wp ifO(wp) is less than some constant Kf1 , andeliminate feature f if O(f) is less than someconstant Kf2 .
This filter is inspired by the sim-ple observation that valid word-pairs and fea-tures tend to occur repeatedly.2.
Generalization filter: Eliminate feature f if|Wf | is greater than some constant Kf3 .
Thisfilter ensures that features associated with toomany word-pairs are not kept.
A feature thatoccurs with many word-pairs tend to describeoverly general relations.
An example of sucha feature is ?subj?
sayobj?
?, which inour experiment occurred with several thousandword-pairs while most features occurred withless than a hundred.5633.
Data sufficiency filter: Eliminate word-pair wpif |Fwp| is less than some constant Kf4 .
Thisfilter ensures that all word-pairs have sufficientfeatures to be compared meaningfully.4.
Entropy filter: Eliminate word-pair wp if itsnormalized entropy is greater than some con-stant Kf5 .
We compute a word-pair?s entropyby considering it as a distribution over features,in a manner that is analogous to the feature en-tropy defined in (Turney, 2006).
Specifically,the normalized entropy of a word-pair wp is:?
?f?Fwp p(f |wp) log (p(f |wp))log |Fwp|where p(f |wp) = O(wp,f)O(wp) is the conditionalprobability of f occurring in the context of wp.The normalized entropy of a word-pair rangesfrom zero to one, and is at its highest when thedistribution of the word-pair?s occurrences overits features is the most random.
The justifica-tion behind this filter is that word-pairs withstrong underlying relations tend to have just afew dominant features that characterize thoserelations, whereas word-pairs that have manynon-dominant features tend to have overly gen-eral underlying relations that can be character-ized in many different ways.3.2 Relation-MatchingCentral to the problem of relation-matching is that ofa relational similarity function: a function that com-putes the degree of similarity between two word-pairs?
underlying relations.
Given such a function,relation-matching reduces to simply computing therelational similarity between every pair of word-pairs, and outputting the pairs scoring higher thansome threshold Kth as lexical analogies.
Our sys-tem incorporates two relational similarity functions,as discussed in the following subsections.Latent Relational AnalysisThe baseline algorithm that we use to computerelational similarity is a modified version of LatentRelational Analysis (LRA) (Turney, 2006), that con-sists of the following steps:1.
Construct an n-by-m matrix A such that theith row maps to word-pair wpi, the jth columnmaps to feature fj , and Ai,j = O(wpi, fj).2.
Reduce the dimensionality of A to a con-stant Ksvd using Singular Value Decomposi-tion (SVD) (Golub and van Loan, 1996).
SVDproduces a matrix A?
of rank Ksvd that is thebest approximation of A among all matrices ofrank Ksvd.
The use of SVD to compress thefeature space was pioneered in Latent SemanticAnalysis (Deerwester et al, 1990) and has be-come a popular technique in feature-based sim-ilarity computation.
The compressed space isbelieved to be a semantic space that minimizesartificial surface differences.3.
The relational similarity between two word-pairs is the cosine measure of their correspond-ing row vectors in the reduced feature space.Specifically, let A?i denote the ith row vector ofA?, then the relational similarity between word-pairs wpi1 and wpi2 is:A?i1 ?
A?i2???A?i1???2+???A?i2??
?2The primary difference between our algorithmand LRA is that LRA also includes each word?ssynonyms in the computation.
Synonym inclusiongreatly increases the size of the problem space,which leads to computational issues for our systemas it operates at a much larger scale than previouswork in relational similarity.
Turney?s (2006) exten-sive evaluation of LRA on SAT verbal analogy ques-tions, for example, involves roughly ten thousand re-lational similarity computations2.
In contrast, oursystem typically requires millions of relational sim-ilarity computations because every pair of extractedword-pairs needs to be compared.
We call our algo-rithm LRA-S (LRA Without Synonyms) to differen-tiate it from the original LRA.Similarity Graph TraversalWhile LRA has been shown to perform well incomputing relational similarity, it suffers from two2The study evaluated 374 SAT questions, each involving 30pairwise comparisons, for a total of 11220 relational similaritycomputations.564limitations.
First, the use of SVD is difficult to inter-pret from an analytical point of view as there is noformal analysis demonstrating that the compressedspace really corresponds to a semantic space.
Sec-ondly, even LRA-S does not scale up well to largedata sets due to SVD being an expensive operation?
computing SVD is in generalO(mn?min(m,n))(Koyuturk et al, 2005), where m, n are the numberof matrix rows and columns, respectively.To counter these limitations, we propose an alter-native algorithm for computing relational similarity?
Similarity Graph Traversal (SGT).
The intuitionbehind SGT is as follows.
Suppose we know thatwp1 and wp2 are relationally similar, and that wp2and wp3 are relationally similar.
Then, by transi-tivity, wp1 and wp3 are also likely to be relation-ally similar.
In other words, the relational similar-ity between two word-pairs can be reinforced byother word-pairs through transitivity.
The actual al-gorithm involves the following steps:1.
Construct a similarity graph as follows.
Eachword-pair corresponds to a node in the graph.An edge exists from wp1 to wp2 if and onlyif the cosine measure of the two word-pairs?feature vectors is greater than or equal to somethreshold Ksgt, in which case, the cosine mea-sure is assigned as the strength of the edge.2.
Define a similarity path of length k, or k-path, from wp1 to wp2 to be a directed acyclicpath of length k from wp1 to wp2, and de-fine the strength s(p) of a path p to be theproduct of the strength of all of the path?sedges.
Denote the set of all k-paths from wp1to wp2 as P(k,wp1, wp2), and denote the sumof the strength of all paths in P(k,wp1, wp2)as S(k,wp1, wp2).3.
The relational similarity between word-pairswpi1 and wpi2 is:?1S(1, wp1, wp2) +?2S(2, wp1, wp2) +.
.
.
?KlS(Kl, wp1, wp2)where Kl is the maximum path length to con-sider, and ?1, .
.
., ?Kl are weights that arelearned using least-squares regression on asmall set of hand-labelled lexical analogies.A natural concern for SGT is that relational simi-larity is not always transitive, and hence some pathsmay be invalid.
For example, although (teacher,student) is relationally similar to both (shepherd,sheep) and (boss, employee), the latter two word-pairs are not relationally similar.
The reason thatthis is not a problem for SGT is because truly simi-lar word-pairs tend to be connected by many transi-tive paths, while invalid paths tend to occur in iso-lation.
As such, while a single path may not be in-dicative, a collection of many paths likely signifiesa true common relation.
The weights in step 3 en-sure that SGT assigns a high similarity score to twoword-pairs only if there are sufficiently many tran-sitive paths (which are sufficiently strong) betweenthem.Analogy FiltersAs a final step in both LSA-R and SGT, we fil-ter out lexical analogies of the form (w1,w2) and(w1,w3), as such lexical analogies tend to expressthe near-synonymy between w2 and w3 more thanthey express the relational similarity between thetwo word-pairs.
We also keep only one permuta-tion of each lexical analogy: (w1,w2) and (w3,w4),(w3,w4) and (w1,w2), (w2,w1) and (w4,w3), and(w4,w3) and (w2,w1) are different permutations ofthe same lexical analogy.4 EvaluationOur evaluation consisted of two parts.
First, we eval-uated the performance of the system, using LRA-Sfor relation-matching.
Then, we evaluated the SGTalgorithm, in particular, how it compares to LRA-S.4.1 System EvaluationExperimental SetupWe implemented our system in Sun JDK 1.5.
Wealso used MXTerminator (Reynar and Ratnaparkhi,1997) for sentence segmentation, MINIPAR (Lin,1993) for lemmatization and dependency parsing,and MATLAB3 for SVD computation.
The exper-iment was conducted on a 2.1 GHz processor, with3http://www.mathworks.com565the exception of SVD computation which was car-ried out in MATLAB running on a single 2.4 GHzprocessor within a 64-processor cluster.
The inputcorpus consisted of the following collections in theText Retrieval Conference Dataset4: AP Newswire1988?1990, LA Times 1989?1990, and San JoseMercury 1991.
In total, 1196 megabytes of text datawere used for the experiment.
Table 1 summarizesthe running times of the experiment.Process TimeSentence Segmentation 20 minDependency Parsing 2232 minData Extraction 138 minRelation-Matching 65 minTable 1: Experiment Running TimesThe parameter values selected for the experimentare listed in Table 2.
The filter parameters were se-lected mostly through trial-and-error ?
various pa-rameter values were tried and filtration results exam-ined.
We used a threshold valueKth = 0.80 to gener-ate the lexical analogies, but the evaluation was per-formed at ten different thresholds from 0.98 to 0.80in 0.02 decrements.Kf1 Kf2 Kf3 Kf4 Kf5 Ksvd35 10 100 10 0.995 600Table 2: Experiment Parameter ValuesEvaluation ProtocolAn objective evaluation of our system is difficultfor two reasons.
First, lexical analogies are by defi-nition subjective; what constitutes a ?good?
lexicalanalogy is debatable.
Secondly, there is no goldstandard of lexical analogies to which we can com-pare.
For these reasons, we adopted a subjectiveevaluation protocol that involved human judges rat-ing the quality of the lexical analogies generated.Such a manual evaluation protocol, however, meantthat it was impractical to evaluate the entire outputset (which was well in the thousands).
Instead, weevaluated random samples from the output and in-terpolated the results.4http://trec.nist.gov/In total, 22 human judges participated in the eval-uation.
All judges were graduate or senior under-graduate students in English, Sociology, or Psychol-ogy, and all were highly competent English speak-ers.
Each judge was given a survey containing 105lexical analogies, 100 of which were randomly sam-pled from our output, and the remaining five weresampled from a control set of ten human-generatedlexical analogies.
All entries in the control set weretaken from the Verbal Analogy section of the Stan-dard Aptitude Test5 and represented the best possi-ble lexical analogies.
The judges were instructed tograde each lexical analogy with a score from zero to10, with zero representing an invalid lexical analogy(i.e., when the two word-pairs share no meaningfulunderlying relation) and ten representing a perfectlexical analogy.
To minimize inter-judge subjectiv-ity, all judges were given detailed instructions con-taining the definition and examples of lexical analo-gies.
In all, 1000 samples out of the 8373 generatedwere graded, each by at least two different judges.We evaluated the output at ten threshold values,from 0.98 to 0.80 in 0.02 decrements.
For eachthreshold, we collected all samples down to thatthreshold and computed the following metrics:1.
Coverage: The number of lexical analogiesgenerated at the current threshold over thenumber of lexical analogies generated at thelowest threshold (8373).2.
Precision: The proportion of samples at thecurrent threshold that scored higher than three.These are considered valid lexical analogies.Note that this is significantly more conservativethan the survey scoring.
Wewant to ensure verypoor lexical analogies were excluded, even ifthey were ?valid?
according to the judges.3.
Quality: The average score of all samples at thecurrent threshold, divided by ten to be in thesame scale as the other metrics.4.
Goodness: The proportion of samples at thecurrent threshold that scored within 10% of theaverage score of the control set.
These are con-sidered human quality.5http://www.collegeboard.com/566Note that recall was not an evaluation metric be-cause there does not exist a method to determine thetrue number of lexical analogies in the input corpus.ResultTable 3 summarizes the result of the control set,and Figure 2:Left summarizes the result of the lex-ical analogies our system generated.
Table 4 listssome good and some poor lexical analogies our sys-tem generated, along with some of their shared fea-tures.Coverage Precision Quality GoodnessN/A 1.00 0.97 0.90Table 3: Result of the Control SetAs Figure 2 shows, our system performed fairlywell, generating valid lexical analogies with a preci-sion around 70%.
The quality of the generated lex-ical analogies was reasonable, although not at thelevel of human-generation.
On the other hand, asmall portion (19% at the highest threshold) of ouroutput was of very high quality, comparable to thebest human-generated lexical analogies.Our result also showed that there was a correspon-dence between the score our system assigned to eachgenerated lexical analogy and its quality.
Precision,quality, and goodness all declined steadily towardlower thresholds: precision 0.70?0.66, quality 0.54?0.49, and goodness 0.19?0.14.Error AnalysisDespite our aggressive filtration of irrelevantword-pairs and features, noise was still the most sig-nificant problem in our output.
Most low-scoringsamples contained at least one word-pair that did nothave a meaningful and clear underlying relation; forexamples, (guy, ball) and (issue, point).
As men-tioned, noise originated from mistakes in the inputdata, errors in sentence segmentation and parsing,as well as mismatches between dependencies andsemantic relatedness.
An example of the latter in-volved the frequent usage of the proposition ?of ?in various constructs.
In the sentence ?the com-pany takes advantage of the new legislation?, forexample, the dependency structure associates com-pany with advantage, whereas the semantic relationclearly lies between company and legislation.
Allthree of our evaluation metrics (precision, quality,and goodness) were negatively affected by noise.Polysemic words, as well as words which wereheavily context-dependent, also posed a problem.For example, one of the lexical analogies generatedin the experiment was (resolution, house) and (leg-islation, senate).
This lexical analogy only makessense if ?house?
is recognized as referring to theHouse of Representatives, which is often abbrevi-ated as ?the House?
in news articles.
Polysemy alsonegatively affected all three of our evaluation met-rics, although to a lesser extent for precision.Finally, our system had difficulties differentiat-ing semantic relations of different granularity.
Theunderlying relations of (relation, country) and (tie,united states), for example, are similar, yet they donot form a good lexical analogy because the rela-tions are at different levels of granularity (countriesin general in the former, and a particular countryin the latter).
Undifferentiated granularity affectedquality and goodness, but it did not have a signifi-cant effect on precision.4.2 SGT EvaluationTo evaluate how SGT compares to LRA-S, werepeated the experiment using SGT for relation-matching.
We set Kl (maximum path length) to 3,andKsgt (cosine threshold) to 0.2; these values wereagain determined largely through trial-and-error.
Totrain SGT, we used 90 lexical analogies graded byhuman judges from the previous experiment.
In or-der to facilitate a fair comparison to LRA-S, we se-lected Kth values that allowed SGT to generate thesame number of lexical analogies as LRA-S did ateach threshold interval.Running on the same 2.1 GHz processor, SGTfinished in just over eight minutes, which is almosta magnitude faster than LRA-S?
65 minutes.
SGTalso used significantly less memory, as the similar-ity graph was efficiently stored in an adjacency list.The sets of lexical analogies generated by the twoalgorithms were quite similar, overlapping approxi-mately 50% at all threshold levels.The significant overlap between SGT and LRA-S?outputs allowed us to evaluate SGT using the sam-ples collected from the previous surveys instead ofconducting a new round of human grading.
Specifi-cally, we identified previously graded samples that567Figure 2: System Evaluation ResultsGood Examples Shared Features(vietnam, cambodia) and (iraq, kuwait)subj?
invadeobj?
,subj?
pull outof?
(building, office) and (museum, collection)subj?
houseobj?
,subj?
consolidateobj?
(stock market, rally) and (student, march)subj?
stageobj?
(researcher, experiment) and (doctor, surgery)subj?
performobj?
(gainer, loser) and (decline, advance)subj?
outnumberobj?
(book, shelf ) and (picture, wall) with?
linesubj?
,subj?
remain on?
(blast, car) and (sanction, economy)subj?
damageobj?
,by?
destroysubj?Poor Examples Shared Features(president, change) and (bush, legislation)subj?
vetoobj?
(charge, death) and (lawsuit, federal court)subj?
file in?
(relation, country) and (tie, united states)obj?
severesubj?
(judge, term) and (member, life)subj?
sentence to?
(issue, point) and (stock, cent)subj?
be down?
,subj?
beup?Table 4: Examples of Good and Poor Lexical Analogies Generatedhad also been generated by SGT, and used thesesamples as the evaluation data points for SGT.
At thelowest threshold (where 8373 lexical analogies weregenerated), we were able to reuse 533 samples outof the original 1000 samples.
Figure 2:Right sum-marizes the performance of the system using SGTfor relation-matching.As the figure shows, SGT performed very simi-larly to LRA-S.
Both SGT?s precision and qualityscores were slightly higher than LRA-S, but the dif-ferences were very small and hence were likely dueto sample variation.
The goodness scores betweenthe two algorithms were also comparable.
In thecase of SGT, however, the score fluctuated insteadof monotonically decreased.
We attribute the fluctu-ation to the smaller sample size.As the samples were drawn exclusively from theportion of SGT?s output that overlapped with LRA-S?
output, we needed to ensure that the samples werenot strongly biased and that the reported result wasnot better than SGT?s actual performance.
To val-idate the result, we conducted an additional experi-ment involving a single human judge.
The judge wasgiven a survey with 50 lexical analogies, 25 of whichwere sampled from the overlapping portion of SGTand LRA-S?
outputs, and 25 from lexical analogiesgenerated only by SGT.
Table 5 summarizes the re-sult of this experiment.
As the table demonstrates,568the results from the two sets were comparable withsmall differences.
Moreover, the differences werein favour of the SGT-only portion.
Therefore, eitherthere was no sampling bias at all, or the samplingbias negatively affected the result.
As such, SGT?sactual performance was at least as good as reported,and may have been slightly higher.Precision Quality GoodnessOverlap 0.76 0.56 0.28SGT-Only 0.88 0.62 0.2Table 5: Overlap vs. SGT-OnlyWe conclude that SGT is indeed a viable alter-native to LRA-S. SGT generates lexical analogiesthat are of the same quality as LRA-S, while be-ing significantly faster and more scalable.
On theother hand, an obvious limitation of SGT is that it isa supervised algorithm requiring manually labelledtraining data.
We claim this is not a severe limitationbecause there are only a few variables to train (i.e.,the weights), hence only a small set of training datais required.
Moreover, a supervised algorithm canbe advantageous in some situations; for example, itis easier to tailor SGT to a particular input corpus.5 Related WorkThe study of analogy in the artificial intelligencecommunity has historically focused on computa-tional models of analogy-making.
French (2002)and Hall (1989) provide two of the most completesurveys of such models.
Veale (2004; 2005) gen-erates lexical analogies from WordNet (Fellbaum,1998) and HowNet (Dong, 1988) by dynamicallycreating new type hierarchies from the semanticinformation stored in these lexicons.
Unlike ourcorpus-based generation system, Veale?s algorithmsare limited by the lexicons in which they oper-ate, and generally are only able to generate near-analogies such as (Christian, Bible) and (Muslim,Koran).
Turney?s (2006) Latent Relational Analy-sis is a corpus-based algorithm that computes the re-lational similarity between word-pairs with remark-ably high accuracy.
However, LRA is focused solelyon the relation-matching problem, and by itself is in-sufficient for lexical analogy generation.6 Conclusion and Future WorkWe have presented a system that is, to the best of ourknowledge, the first system capable of generatinglexical analogies from unstructured text data.
Em-pirical evaluation shows that our system performedfairly well, generating valid lexical analogies witha precision of about 70%.
The quality of the gen-erated lexical analogies was reasonable, althoughnot at the level of human performance.
As partof the system, we have also developed a novel al-gorithm for computing relational similarity that ri-vals the performance of the current state-of-the-artwhile being significantly faster and more scalable.One of our immediate tasks is to complement depen-dency patterns with additional features.
In particu-lar, we expect semantic features such as word defini-tions from machine-readable dictionaries to improveour system?s ability to differentiate between differ-ent senses of polysemic words, as well as differentgranularities of semantic relations.
We also plan totake advantage of our system?s flexibility and relaxthe constraints on dependency paths so as to gen-erate more-varied lexical analogies, e.g., analogiesinvolving verbs and adjectives.A potential application of our system, and theoriginal inspiration for this research, would be touse the system to automatically enrich ontologiesby spreading semantic relations between lexical ana-logues.
For example, if words w1 and w2 are relatedby relation r, and (w1, w2) and (w3, w4) form a lex-ical analogy, then it is likely that w3 and w4 are alsorelated by r. A dictionary of lexical analogies there-fore would allow an ontology to grow from a smallset of seed relations.
In this way, lexical analogiesbecome bridges through which semantic relationsflow in a sea of ontological concepts.AcknowledgmentsWe thank the reviewers of EMNLP 2007 for valu-able comments and suggestions.
This work was sup-ported in part by the Ontario Graduate ScholarshipProgram, Ontario Innovation Trust, Canada Foun-dation for Innovation, and the Natural Science andEngineering Research Council of Canada.569ReferencesScott Deerwester, Susan T. Dumais, George W. Furnas,Thomas K. Landauer, and Richard Harshman.
1990.Indexing by latent semantic analysis.
Journal of theAmerican Society for Information Science, 41:391?407.Dong Zhen Dong.
1988.
What, how and who?
Pro-ceedings of the International Symposium on ElectronicDictionaries.
Tokyo, Japan.Christine Fellbaum, editor.
1998.
WordNet ?
An Elec-tronic Lexical Database.
MIT Press.Robert French.
2002.
The computational modelingof analogy-making.
Trends in Cognitive Sciences,6(5):200?205.Gene Golub and Charles van Loan.
1996.
Matrix Com-putations.
Johns Hopkins University Press, third edi-tion.Rogers Hall.
1989.
Computational approaches to ana-logical reasoning: A comparative analysis.
ArtificialIntelligence, 39:39?120.Mehmet Koyuturk, Ananth Grama, and Naren Ramakr-ishnan.
2005.
Compression, clustering, and patterndiscovery in very high-dimensional discrete-attributedata sets.
IEEE Transactions on Knowledge and DataEngineering, 17(4):447?461.Beth Levin 1993.
English Verb Classes and Alter-nations: A Preliminary Investigation.
University ofChicago Press.Dekang Lin.
1993.
Principle-based parsing withoutovergeneration.
Proceedings of the 31st Annual Meet-ing on ACL, pp 112?120.
Columbus, USA.Dekang Lin and Patrick Pantel.
2001.
Discovery of infer-ence rules for question answering.
Natural LanguageEngineering, 7(4):343?360.Jane Morris and Graeme Hirst.
2004.
Non-classical lex-ical semantic relations.
Proceedings of the Compu-tational Lexical Semantics Workshop at HLT-NAACL2004, pp 46?51.
Boston, USA.Jeffrey C. Reynar and Adwait Ratnaparkhi.
1997.
Amaximum entropy approach to identifying sentenceboundaries.
Proceedings of the 5th Conference on Ap-plied Natural Language Processing, pp 16?19.
Wash-ington, USA.Gerard Salton, A. Wong, and C.S.
Yang.
1975.
A vectorspace model for automatic indexing.
Communicationsof the ACM, 13(11):613?620.Rion Snow, Daniel Jurafsky, and Andrew Ng.
2004.Learning syntactic patterns for automatic hypernymdiscovery.
Proceedings of the 2004 Neural Infor-mation Processing Systems Conference.
Vancouver,Canada.Lucien Tesnie`re.
1959.
E?le?ments de Syntaxe Structurale.Librairie C. Klincksieck, Paris.Peter D. Turney.
2006.
Similarity of semantic relations.Computational Linguistics, 32(3):379?416.Tony Veale, Jer Hayes, and Nuno Seco.
2004.
The Bibleis the Christian Koran: Discovering simple analogicalcompounds.
Proceedings of the Workshop on Com-putational Creativity in 2004 European Conference onCase-Based Reasoning.
Madrid, Spain.Tony Veale.
2005.
Analogy generation with HowNet.Proceedings of the 2005 International Joint Confer-ence on Artificial Intelligence.
Edinburgh, Scotland.570
