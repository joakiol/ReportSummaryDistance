A C lass i f i ca t ion  Approach  to  Word  Pred ic t ion*Ya i r  Even-Zohar  Dan RothDepar tment  of Computer  ScienceUniversity of Illinois at Urbana-Champaign{evenzoha, danr}~uiuc, eduAbst rac tThe eventual goal of a language model is to accu-rately predict he value of a missing word given itscontext.
We present an approach to word predictionthat is based on learning a representation for eachword as a function of words and linguistics pred-icates in its context.
This approach raises a fewnew questions that we address.
First, in order tolearn good word representations it is necessary touse an expressive representation f the context.
Wepresent away that uses external knowledge to gener-ate expressive context representations, along with alearning method capable of handling the large num-ber of features generated this way that can, poten-tially, contribute to each prediction.
Second, sincethe number of words "competing" for each predic-tion is large, there is a need to "focus the attention"on a smaller subset of these.
We exhibit he contri-bution of a "focus of attention" mechanism to theperformance of the word predictor.
Finally, we de-scribe a large scale experimental study in which theapproach presented is shown to yield significant im-provements in word prediction tasks.1 In t roduct ionThe task of predicting the most likely word based onproperties of its surrounding context is the archetyp-ical prediction problem in natural language process-ing (NLP).
In many NLP tasks it is necessary to de-termine the most likely word, part-of-speech (POS)tag or any other token, given its history or context.Examples include part-of speech tagging, word-sensedisambiguation, speech recognition, accent restora-tion, word choice selection in machine translation,context-sensitive spelling correction and identifyingdiscourse markers.
Most approaches to these prob-lems are based on n-gram-like modeling.
Namely,the learning methods make use of features which areconjunctions of typically (up to) three consecutivewords or POS tags in order to derive the predictor.In this paper we show that incorporating addi-tional information into the learning process is very* This research is supported by NSF grants IIS-9801638 andSBR-987345.beneficial.
In particular, we provide the learner witha rich set of features that combine the informationavailable in the local context along with shallowparsing information.
At the same time, we studya learning approach that is specifically tailored forproblems in which the potential number of featuresis very large but only a fairly small number of themactually participates in the decision.
Word predic-tion experiments hat we perform show significantimprovements in error rate relative to the use of thetraditional, restricted, set of features.BackgroundThe most influential problem in motivating statis-tical learning application in NLP tasks is that ofword selection in speech recognition (Jelinek, 1998).There, word classifiers are derived from a probabilis-tic language model which estimates the probabilityof a sentence s using Bayes rule as the product ofconditional probabilities,Pr(s) - =-- H~=lPr(wi \ ]wl , .
.
.Wi_ l  )-- H?=lPr(wi\[hi )where hi is the relevant history when predicting wi.Thus, in order to predict he most likely word in agiven context, a global estimation of the sentenceprobability is derived which, in turn, is computedby estimating the probability of each word given itslocal context or history.
Estimating terms of theform Pr (wlh  ) is done by assuming some generativeprobabilistic model, typically using Markov or otherindependence assumptions, which gives rise to es-timating conditional probabilities of n-grams typefeatures (in the word or POS space).
Machine learn-ing based classifiers and maximum entropy modelswhich, in principle, are not restricted to features ofthese forms have used them nevertheless, perhapsunder the influence of probabilistic methods (Brill,1995; Yarowsky, 1994; Ratnaparkhi et al, 1994).It has been argued that the information availablein the local context of each word should be aug-mented by global sentence information and even in-formation external to the sentence in order to learn124better classifiers and language models.
Efforts inthis directions consists of (1) directly adding syn-tactic information, as in (Chelba and Jelinek, 1998;Rosenfeld, 1996), and (2) indirectly adding syntac-tic and semantic information, via similarity models;in this case n-gram type features are used when-ever possible, and when they cannot be used (dueto data sparsity), additional information compiledinto a similarity measure is used (Dagan et al,1999).
Nevertheless, the efforts in this direction sofar have shown very insignificant improvements, ifany (Chelba and Jelinek, 1998; Rosenfeld, 1996).We believe that the main reason for that is that in-corporating information sources in NLP needs to becoupled with a learning approach that is suitable forit.Studies have shown that both machine learningand probabilistic learning methods used in NLPmake decisions using a linear decision surface overthe feature space (Roth, 1998; Roth, 1999).
In thisview, the feature space consists of simple functions(e.g., n-grams) over the the original data so as toallow for expressive nough representations u ing asimple functional form (e.g., a linear function).
Thisimplies that the number of potential features thatthe learning stage needs to consider may be verylarge, and may grow rapidly when increasing the ex-pressivity of the features.
Therefore a feasible com-putational approach needs to be feature-efficient.
Ineeds to tolerate a large number of potential featuresin the sense that the number of examples requiredfor it to converge should depend mostly on the num-ber features relevant o the decision, rather than onthe number of potential features.This paper addresses the two issues mentionedabove.
It presents a rich set of features that is con-structed using information readily available in thesentence along with shallow parsing and dependencyinformation.
It then presents a learning approachthat can use this expressive (and potentially large)intermediate r presentation a d shows that it yieldsa significant improvement in word error rate for thetask of word prediction.The rest of the paper is organized as follows.
Insection 2 we formalize the problem, discuss the in-formation sources available to the learning systemand how we use those to construct features.
In sec-tion 3 we present he learning approach, based onthe SNoW learning architecture.
Section 4 presentsour experimental study and results.
In section 4.4we discuss the issue of deciding on a set of candi-date words for each decision.
Section 5 concludesand discusses future work.2 In fo rmat ion  Sources  and  FeaturesOur goal is to learn a representation for each wordin terms of features which characterize the syntacticand semantic ontext in which the word tends toappear.
Our features are defined as simple relationsover a collection of predicates that capture (some of)the information available in a sentence.2.1 In fo rmat ion  SourcesDef in i t ion  1 Let s =< wl,w2,.
.
.
,wn > be a sen-tence in which wi is the i-th word.
Let :?
be a col-lection of predicates over a sentence s. IS(s)) 1, theIn fo rmat ion  source(s)  available for the sentences is a representation ors as a list of predicates I E :r,XS(S) = {II(Wll  , ...Wl,),-.., /~g(W~l , ..-Wk,)}.Ji is the arity of the predicate I j .Example  2 Let s be the sentence< John, X, at,  the, clock, to, see, what, time, i t ,  i s  >Let ~={word, pos, subj-verb}, with the interpreta-tion that word is a unary predicate that returns thevalue of the word in its domain; pos is a unarypredicate that returns the value of the pos of theword in its domain, in the context of the sentence;sub j -  verb is a binary predicate that returns thevalue of the two words in its domain if the second isa verb in the sentence and the first is its subject; itreturns ?
otherwise.
Then,IS(s)  = {word(wl) = John, ..., word(w3) = at,... ,word(wn) = is,  pos(w4) = DET,.
.
.
,s bj - verb(w , w2) = {John, x}...}.The IS representation f s consists only of the pred-icates with non-empty values.
E.g., pos(w6) =modal is not part of the IS for the sentence above.subj - verb might not exist at all in the IS even if thepredicate is available, e.g., in The ba l l  was givento Mary.Clearly the I S  representation f s does not containall the information available to a human reading s;it captures, however, all the input that is availableto the computational process discussed in the restof this paper.
The predicates could be generated byany external mechanism, even a learned one.
Thisissue is orthogonal to the current discussion.2.2  Generat ing  FeaturesOur goal is to learn a representation for each wordof interest.
Most efficient learning methods knowntoday and, in particular, those used in NLP, makeuse of a linear decision surface over their featurespace (Roth, 1998; Roth, 1999).
Therefore, in or-der to learn expressive representations one needs tocompose complex features as a function of the in-formation sources available.
A linear function ex-pressed irectly in terms of those will not be expres-sive enough.
We now define a language that allows1We denote IS(s) as IS wherever it is obvious what thereferred sentence we is, or whenever we want to indicate In-formation Source in general.125one to define "types" of features 2 in terms of theinformation sources available to it.Def in i t ion 3 (Basic Features )  Let I E Z be ak-ary predicate with range R. Denote w k =(Wjl , .
.
.
, wjk).
We define two basic binary relationsas follows.
For a e R we define:1 iffI(w k)=af ( I (wk) ,  a) = 0 otherwise (1)An existential version of the relation is defined by:l i f f3aERs .
t I (w  k)=af ( I (wk) ,x )  = 0 otherwise (2)Features, which are defined as binary relations, canbe composed to yield more complex relations interms of the original predicates available in IS.Def in i t ion 4 (Compos ing  features)  Let f l ,  f2be feature definitions.
Then fand(fl,  f2)  for(f1, f2)fnot(fl) are defined and given the usual semantic:liff:----h=lfand(fl, f2) = 0 otherwisel i f f := l  o r f2=lfob(f1, f2)  = 0 otherwise{ l~f f l=Ofnot(fx) = 0 otherwiseIn order to learn with features generated using thesedefinitions as input, it is important hat featuresgenerated when applying the definitions on differentISs are given the same identification.
In this pre-sentation we assume that the composition operatoralong with the appropriate IS element (e.g., Ex.
2,Ex.
9) are written explicitly as the identification ofthe features.
Some of the subtleties in defining theoutput representation are addressed in (Cumby andRoth, 2000).2.3 S t ructured  FeaturesSo far we have presented features as relations overIS(s)  and allowed for Boolean composition opera-tors.
In most cases more information than just a listof active predicates is available.
We abstract hisusing the notion of a structural information source(SIS(s))  defined below.
This allows richer class offeature types to be defined.2We note that we do not define the features will be used inthe learning process.
These are going to be defined in a datadriven way given the definitions discussed here and the inputISs.
The importance of formally defining the "types" is dueto the fact that some of these are quantified.
Evaluating themon a given sentence might be computationally intractable anda formal definition would help to flesh out the difficulties andaid in designing the language (Cumby and Roth, 2000).2.4 S t ructured  InstancesDef in i t ion 5 (S t ructura l  In fo rmat ion  Source)Let s =< wl,w2, ...,Wn >.
SIS(s)), the St ructura lIn fo rmat ion  source(s)  available for the sentences, is a tuple (s, E1, .
.
.
,Ek) of directed acyclicgraphs with s as the set of vertices and Ei 's, a setof edges in s.Example  6 (L inear  S t ructure)  The simplestSIS is the one corresponding to the linear structureof the sentence.
That is, S IS(s )  = (s ,E)  where(wi, wj) E E iff the word wi occurs immediatelybefore wj in the sentence (Figure 1 bottom leftpart).In a linear structure (s =< Wl,W2,...,Wn >,E) ,where E = {(wi,wi+l); i  = 1, .
.
.n -  1}, we definethe chainc(w j ,  \[l, r\]) = {w,_ , , .
.
.
,  w j , .
.
,  n s.We can now define a new set of features thatmakes use of the structural information.
Structuralfeatures are defined using the SIS.
When defining afeature, the naming of nodes in s is done relative toa distinguished node, denoted wp, which we call thefocus word of the feature.
Regardless of the arityof the features we sometimes denote the feature fdefined with respect o wp as f(wp).Def in i t ion  7 (P rox imi ty )  Let S IS (s )  = (s, E) bethe linear structure and let I E Z be a k-ary predicatewith range R. Let Wp be a focus word and C =C(wp, \[l, r\]) the chain around it.
Then, the proximityfeatures for I with respect o the chain C are definedas:fc(l(w), a) = { 1 i f I (w)  = a ,a  E R ,w E C 0 otherwise(3)The second type of feature composition definedusing the structure is a collocation operator.Def in i t ion 8 (Col locat ion)  Let f l , .
.
.
f k  be fea-ture definitions, col locc ( f l , f 2, .
.
.
f k ) is a restrictedconjunctive operator that is evaluated on a chainC of length k in a graph.
Specifically, let C ={wj,, wj=, .. .
, wjk } be a chain of length k in S IS(s) .Then, the collocation feature for f l , .
.
,  fk with re-spect to the chain C is defined ascollocc(fl, .
.
.
, fk) = {1 ifVi = 1 , .
.
.
k ,  f i(wj,) = 10 otherwise(4)The following example defines features that areused in the experiments described in Sec.
4.126Example  9 Let s be the sentence in Example 2.
Wedefine some of the features with respect o the linearstructure of the sentence.
The word X is used asthe focus word and a chain \[-10, 10\] is defined withrespect o it.
The proximity features are defined withrespect o the predicate word.
We get, for example:fc(word) ---- John; fc(word) = at; fc(word) = clock.Collocation features are defined with respect o achain \[-2, 2\] centered at the focus word X.
They aredefined with respect o two basic features f l ,  f2 eachof which can be either f(word, a) or f(pos, a).
Theresulting features include, for example:collocc(word, word)= { John-  X};collocc(word, word) = {X - at};collocc(word, pos) = {at -  DET}.2.5 Non-L inear  S t ructureSo far we have described feature definitions whichmake use of the linear structure of the sentence andyield features which are not too different from stan-dard features used in the literature e.g., n-gramswith respect o pos or word can be defined as collocfor the appropriate chain.
Consider now that we aregiven a general directed acyclic graph G = (s, E)on the the sentence s as its nodes.
Given a distin-guished focus word wp 6 s we can define a chain inthe graph as we did above for the linear structureof the sentence.
Since the definitions given above,Def.
7 and Def.
8, were given for chains they wouldapply for any chain in any graph.
This generaliza-tion becomes interesting if we are given a graph thatrepresents a more involved structure of the sentence.Consider, for example the graph DG(s) in Fig-ure 1.
DG(s) described the dependency graph ofthe sentence s. An edge (wi,wj) in DG(s) repre-sent a dependency between the two words.
In ourfeature generation language we separate the infor-mation provided by the dependency grammar 3 totwo parts.
The structural information, provided inthe left side of Figure 1, is used to generate SIS(s) .The labels on the edges are used as predicates andare part of IS(s).
Notice that some authors (Yuret,1998; Berger and Printz, 1998) have used the struc-tural information, but have not used the informationgiven by the labels on the edges as we do.The following example defines features that areused in the experiments described in Sec.
4.Example  10 Let s be the sentence in Figure 1along with its IS that is defined using the predicatesword, pos, sub j ,  obj ,  aux_vrb.
A sub j -verb3This information can be produced by a functional de-pendency grammar (FDG), which assigns each word a spe-cific function, and then structures the sentence hierarchical lybased on it, as we do here (Tapanainen and Jrvinen, 1997),but can also be generated by an external  rule-based parser ora learned one.feature, fsubj-verb, can be defined as a collocationover chains constructed with respect to the focusword jo in .
Moreover, we can define fsubj-verb tobe active also when there is an aux_vrb betweenthe subj and verb, by defining it as a disjunctionof two collocation features, the sub j -verb  and thesubj-aux_vrb-verb.
Other features that we use areconjunctions of words that occur before the focusverb (here: j o in )  along all the chains it occurs in(here: wi l l ,  board,  as) and collocations of objand verb.As a final comment on feature generation, we notethat the language presented is used to define "types"of features.
These are instantiated in a data drivenway given input sentences.
A large number of fea-tures is created in this way, most of which might notbe relevant o the decision at hand; thus, this pro-cess needs to be followed by a learning process thatcan learn in the presence of these many features.3 The  Learn ing  ApproachOur experimental investigation is done using theSNo W learning system (Roth, 1998).
Earlier ver-sions of SNoW (Roth, 1998; Golding and Roth,1999; Roth and Zelenko, 1998; Munoz et al, 1999)have been applied successfully to several natural an-guage related tasks.
Here we use SNo W for the taskof word prediction; a representation is learned foreach word of interest, and these compete at evalua-tion time to determine the prediction.3.1 The  SNOW Arch i tec tureThe SNo W architecture is a sparse network of linearunits over a common pre-defined or incrementallylearned feature space.
It is specifically tailored forlearning in domains in which the potential number offeatures might be very large but only a small subsetof them is actually relevant o the decision made.Nodes in the input layer of the network representsimple relations on the input sentence and are beingused as the input features.
Target nodes representwords that are of interest; in the case studied here,each of the word candidates for prediction is repre-sented as a target node.
An input sentence, alongwith a designated word of interest in it, is mappedinto a set of features which are active in it; this rep-resentation is presented to the input layer of SNoWand propagates to the target nodes.
Target nodesare linked via weighted edges to (some of) the inputfeatures.
Let At = {Q, .
.
.
, i,~} be the set of featuresthat are active in an example and are linked to thetarget node t. Then the linear unit corresponding tot is active ifft E w i > Ot,iEAtwhere w~ is the weight on the edge connecting the ithfeature to the target node t, and Ot is the threshold127~ , Nov.29.will- board -~?Vi ~ ,~ t!e d i re~aPierr/e Ye~ars old l-~ non-executive61Pierre -*Vinken-.-, -~ 61 -*-years -'*old ~ -" will - -  \]?join ---the -,-board -.- as--- a +non-executive I?director-.- 29.
-=-Nov.~.
,, Nov. 29. oldaux_vrb obj~sub?"
will J  b?~rd corped~%~det pcomp det.. a-.. Vir~.nattr "moc l  tl~e director,Pier're years attr* non-eXecutive qnt6tPierre Vinken, 61 years old, will join the board as anonexecutive director Nov. 29.Figure 1: A sentence  w i th  a l inear and a dependency  grammar  s t ructurefor the target node t. In this way, SNo W providesa collection of word representations rather than justdiscriminators.A given example is treated autonomously by eachtarget subnetwork; an example labeled t may betreated as a positive example by the subnetworkfor t and as a negative xample by the rest of thetarget nodes.
The learning policy is on-line andmistake-driven; several update rules can be usedwithin SNOW.
The most successful update rule isa variant of Littlestone's Winnow update rule (Lit-tlestone, 1988), a multiplicative update rule that istailored to the situation in which the set of inputfeatures is not known a priori, as in the infiniteattribute model (Blum, 1992).
This mechanism isimplemented via the sparse architecture of SNOW.That is, (1) input features are allocated in a datadriven way - an input node for the feature i is al-located only if the feature i was active in any inputsentence and (2) a link (i.e., a non-zero weight) ex-ists between a target node t and a feature i if andonly if i was active in an example labeled t.One of the important properties of the sparse ar-chitecture is that the complexity of processing anexample depends only on the number of features ac-tive in it, na, and is independent of the total num-ber of features, nt, observed over the life time of thesystem.
This is important in domains in which thetotal number of features is very large, but only asmall number of them is active in each example.4 Exper imenta l  S tudy4.1 Task def in i t ionThe experiments were conducted with four goals inmind:1.
To compare mistake driven algorithms withnaive Bayes, trigram with backoff and a simplemaximum likelihood estimation (MLE) base-line.2.
To create a set of experiments which is compa-rable with similar experiments hat were previ-ously conducted by other researchers.3.
To build a baseline for two types of extensions ofthe simple use of linear features: (i) Non-Linearfeatures (ii) Automatic focus of attention.4.
To evaluate word prediction as a simple lan-guage model.We chose the verb prediction task which is sim-ilar to other word prediction tasks (e.g.,(Goldingand Roth, 1999)) and, in particular, follows theparadigm in (Lee and Pereira, 1999; Dagan et al,1999; Lee, 1999).
There, a list of the confusion sets isconstructed first, each consists of two different verbs.The verb vl is coupled with v2 provided that theyoccur equally likely in the corpus.
In the test set,every occurrence of vl or v2 was replaced by a set{vl, v2} and the classification task was to predict hecorrect verb.
For example, if a confusion set is cre-ated for the verbs "make" and "sell", then the datais altered as follows:Once target subnetworks have been learned andthe network is being evaluated, a decision sup-port mechanism is employed, which selects thedominant active target node in the SNoW unitvia a winner-take-all mechanism to produce a fi-nal prediction.
SNoW is available publicly athttp ://L2R.
cs.
uiuc.
edu/- cogcomp, html.make the paper --+ {make,sell} the papersell sensitive data --~ {make,sell} sensitive dataThe evaluated predictor chooses which of the twoverbs is more likely to occur in the current sentence.In choosing the prediction task in this way, wemake sure the task in difficult by choosing between128competing words that have the same prior proba-bilities and have the same part of speech.
A fur-ther advantage of this paradigm is that in futureexperiments we may choose the candidate verbs sothat they have the same sub-categorization, pho-netic transcription, etc.
in order to imitate the firstphase of language modeling used in creating can-didates for the prediction task.
Moreover, the pre-transformed data provides the correct answer so that(i) it is easy to generate training data; no supervi-sion is required, and (ii) it is easy to evaluate theresults assuming that the most appropriate word isprovided in the original text.Results are evaluated using word-error rate(WER).
Namely, every time we predict the wrongword it is counted as a mistake.4.2 DataWe used the Wall Street Journal (WSJ) of the years88-89.
The size of our corpus is about 1,000,000words.
The corpus was divided into 80% trainingand 20% test.
The training and the test data wereprocessed by the FDG parser (Tapanainen and Jrvi-nen, 1997).
Only verbs that occur at least 50 timesin the corpus were chosen.
This resulted in 278 verbsthat we split into 139 confusion sets as above.
Af-ter filtering the examples of verbs which were not inany of the sets we use 73, 184 training examples and19,852 test examples.4.3 Results4.3.1 FeaturesIn order to test the advantages of different featuresets we conducted experiments using the followingfeatures ets:1.
Linear features: proximity of window size 4-10words, conjunction of size 2 using window size4-2.
The conjunction combines words and partsof speech.2.
Linear + Non linear features: using the lin-ear features defined in (1) along with nonlinear features that use the predicates ub j ,obj ,  word, pos, the collocations ubj -verb,verb-obj  linked to the focus verb via the graphstructure and conjunction of 2 linked words.The over all number of features we have generatedfor all 278 target verbs was around 400,000.
In alltables below the NB columns represent results of thenaive Bayes algorithm as implemented within SNoWand the SNoW column represents the results of thesparse Winnow algorithm within SNOW.Table 1 summarizes the results of the experimentswith the features ets (1), (2) above.
The baselineexperiment uses MLE, the majority predictor.
Inaddition, we conducted the same experiment usingtrigram with backoff and the WER is 29.3%.
FromLinearNon LinearBline NB SNoW49.6 13.54 11.5649.6 12.25 9.84Table 1: Word Error Rate results for linearand non-linear featuresthese results we conclude that using more expressivefeatures helps significantly in reducing the WER.However, one can use those types of features onlyif the learning method handles large number of pos-sible features.
This emphasizes the importance ofthe new learning method.Similarity NB SNoW54.6% 59.1% WSJ dataAP news 47.6%Table 2: Compar i son  o f  the improvementachieved using similarity methods (Dagan etal., 1999) and using the methods presented inthis paper.
Results are shown in percentageof improvement in accuracy over the baseline.Table 2 compares our method to methods that usesimilarity measures (Dagan et al, 1999; Lee, 1999).Since we could not use the same corpus as in thoseexperiments, we compare the ratio of improvementand not the WER.
The baseline in this studies isdifferent, but other than that the experiments areidentical.
We show an improvement over the bestsimilarity method.
Furthermore, we train using only73,184 examples while (Dagan et al, 1999) trainusing 587, 833 examples.
Given our experience withour approach on other data sets we conjecture thatwe could have improved the results further had weused that many training examples.4.4 Focus of  attentionSNoW is used in our experiments as a multi-classpredictor - a representation is learned for each wordin a given set and, at evaluation time, one of theseis selected as the prediction.
The set of candidatewords is called the confusion set (Golding and Roth,1999).
Let C be the set of all target words.
In previ-ous experiments we generated artificially subsets ofsize 2 of C in order to evaluate the performance ofour methods.
In general, however, the question ofdetermining a good set of candidates i interesting init own right.
In the absence, of a good method, onemight end up choosing a verb from among a largerset of candidates.
We would like to study the effectsthis issue has on the performance of our method.In principle, instead of working with a single largeconfusion set C, it might be possible to,split C intosubsets of smaller size.
This process, which we callthe focus of attention (FOA) would be beneficialonly if we can guarantee that, with high probability,129given a prediction task, we know which confusionset to use, so that the true target belongs to it.
Infact, the FOA problem can be discussed separatelyfor the training and test stages.1.
Training: Given our training policy (Sec.
3) ev-ery positive xample serves as a negative xam-ple to all other targets in its confusion set.
Fora large set C training might become computa-tionally infeasible.2.
Testing: considering only a small set of wordsas candidates at evaluation time increases thebaseline and might be significant from the pointof view of accuracy and efficiency.To evaluate the advantage of reducing the size ofthe confusion set in the training and test phases, weconducted the following experiments u ing the samefeatures et (linear features as in Table 1).Bline NB SNoWTra in  All Test All 87.44 65.22 65.05Train All Test 2 49.6 13.54 13.15Train 2 Test 2 49.6 13.54 11.55Table 3: Evaluat ing Focus of  Attent ion:  WordEr ror  Rate  for Training and testing usingall the words together  against using pairs ofwords.
"Train All" means training on all 278 targets to-gether.
"Test all" means that the confusion set isof size 278 and includes all the targets.
The resultsshown in Table 3 suggest that, in terms of accuracy,the significant factor is the confusion set size in thetest stage.
The effect of the confusion set size ontraining is minimal (although it does affect rainingtime).
We note that for the naive Bayes algorithmthe notion of negative xamples does not exist, andtherefore regardless of the size of confusion set intraining, it learns exactly the same representations.Thus, in the NB column, the confusion set size intraining makes no difference.The application in which a word predictor is usedmight give a partial solution to the FOA problem.For example, given a prediction task in the contextof speech recognition the phonemes that constitutethe word might be known and thus suggest a wayto generate a small confusion set to be used whenevaluating the predictors.Tables 4,5 present he results of using artificiallysimulated speech recognizer using a method of gen-eral phonetic lasses.
That is, instead of transcrib-ing a word by the phoneme, the word is transcribedby the phoneme classes(Jurafsky and Martin, 200).Specifically, these experiments deviate from the taskdefinition given above.
The confusion sets used areof different sizes and they consist of verbs with dif-ferent prior probabilities in the corpus.
Two sets ofexperiments were conducted that use the phonetictranscription ofthe words to generate confusion sets.Bl ine NB SNoWTrain All Test PC  19.84 11.6 12.3Train PC  Test PC  19.84 11.6 11.3Table 4: Simulating Speech Recognizer:  WordEr ror  Rate  for Training and testing withconfusion sets determined based on phonet icclasses (PC) f rom a s imulated speech recog-nizer.In the first experiment (Table 4), the transcriptionof each word is given by the broad phonetic groupsto which the phonemes belong i.e., nasals, fricative,etc.
4.
For example, the word "b_u_y" is transcribedusing phonemes as "b_Y" and here we transcribe itas "P_VI" which stands for "Plosive_Vowell".
Thispartition results in a partition of the set of verbsinto several confusions sets.
A few of these confusionsets consist of a single word and therefore have 100%baseline, which explains the high baseline.Bline NB SNoWTrain All Test PC  45.63 26.36 27.54Train PC  Test PC  45.63 26.36 25.55Table 5: Simulat ing Speech Recognizer:  WordEr ror  Rate  for Training and testing withconfusion sets determined based on phonet icclasses (PC) f rom a s imulated speech recog-nizer.
In this case only confusion sets thathave less than 98% baseline are used, whichexplains the overall  lower baseline.Table 5 presents the results of a similar exper-iment in which only confusion sets with multiplewords were used, resulting in a lower baseline.As before, Train All means that training is donewith all 278 targets together while Train PC meansthat the PC confusion sets were used also in train-ing.
We note that for the case of SNOW, used herewith the sparse Winnow algorithm, that size of theconfusion set in training has some, although small,effect.
The reason is that when the training is donewith all the target words, each target word repre-sentation with all the examples in which it does notoccur are used as negative xamples.
When a smallerconfusion set is used the negative xamples are morelikely to be "true" negative.5 Conc lus ionThis paper presents a new approach to word predic-tion tasks.
For each word of interest, a word repre-sentation is learned as a function of a common, but4In this experiment, he vowels phonemes were dividedinto two different groups to account for different sounds.130potentially very large set of expressive (relational)features.
Given a prediction task (a sentence witha missing word) the word representations are evalu-ated on it and compete for the most likely word tocomplete the sentence.We have described a language that allows one todefine expressive feature types and have exhibitedexperimentally the advantage ofusing those on wordprediction task.
We have argued that the success ofthis approach inges on the combination of using alarge set of expressive f atures along with a learningapproach that can tolerate it and converges quicklydespite the large dimensionality of the data.
Webelieve that this approach would be useful for otherdisambiguation tasks in NLP.We have also presented a preliminary study of areduction in the confusion set size and its effectson the prediction performance.
In future work weintend to study ways that determine the appropriateconfusion set in a way to makes use of the currenttask properties.AcknowledgmentsWe gratefully acknowledge helpful comments andprogramming help from Chad Cumby.ReferencesA.
Berger and H. Printz.
1998.
Recognition perfor-mance of a large-scale dependency-grammar lan-guage model.
In Int'l Conference on Spoken Lan-guage Processing (ICSLP'98), Sydney, Australia.A.
Blum.
1992.
Learning boolean functions inan infinite attribute space.
Machine Learning,9(4):373-386.E.
Brill.
1995.
Transformation-based error-drivenlearning and natural language processing: A casestudy in part of speech tagging.
ComputationalLinguistics, 21(4):543-565.C.
Chelba and F. Jelinek.
1998.
Exploiting syntac-tic structure for language modeling.
In COLING-A CL '98.C.
Cumby and D. Roth.
2000.
Relational repre-sentations that facilitate learning.
In Proc.
ofthe International Conference on the Principles ofKnowledge Representation a d Reasoning.
To ap-pear.I.
Dagan, L. Lee, and F. Pereira.
1999.
Similarity-based models of word cooccurrence probabilities.Machine Learning, 34(1-3):43-69.A.
R. Golding and D. Roth.
1999.
A Winnow basedapproach to context-sensitive spelling correction.Machine Learning, 34(1-3):107-130.
Special Issueon Machine Learning and Natural Language.F.
Jelinek.
1998.
Statistical Methods for SpeechRecognition.
MIT Press.D.
Jurafsky and J. H. Martin.
200.
Speech and Lan-guage Processing.
Prentice Hall.L.
Lee and F. Pereira.
1999.
Distributional similar-ity models: Clustering vs. nearest neighbors.
InA CL 99, pages 33-40.L.
Lee.
1999.
Measure of distributional similarity.In A CL 99, pages 25-32.N.
Littlestone.
1988.
Learning quickly when irrel-evant attributes abound: A new linear-thresholdalgorithm.
Machine Learning, 2:285-318.M.
Munoz, V. Punyakanok, D. Roth, and D. Zimak.1999.
A learning approach to shallow parsing.
InEMNLP-VLC'99, the Joint SIGDAT Conferenceon Empirical Methods in Natural Language Pro-cessing and Very Large Corpora, June.A.
Ratnaparkhi, J. Reynar, and S. Roukos.
1994.
Amaximum entropy model for prepositional phraseattachment.
In ARPA, Plainsboro, N J, March.R.
Rosenfeld.
1996.
A maximum entropy approachto adaptive statistical language modeling.
Com-puter, Speech and Language, 10.D.
Roth and D. Zelenko.
1998.
Part of speechtagging using a network of linear separators.In COLING-ACL 98, The 17th InternationalConference on Computational Linguistics, pages1136-1142.D.
Roth.
1998.
Learning to resolve natural languageambiguities: A unified approach.
In Proc.
Na-tional Conference on Artificial Intelligence, pages806-813.D.
Roth.
1999.
Learning in natural anguage.
InProc.
of the International Joint Conference of Ar-tificial Intelligence, pages 898-904.P.
Tapanainen and T. Jrvinen.
1997.
A non-projective dependency parser.
In In Proceedingsof the 5th Conference on Applied Natural Lan-guage Processing, Washington DC.D.
Yarowsky.
1994.
Decision lists for lexical ambi-guity resolution: application to accent restorationin Spanish and French.
In Proc.
of the AnnualMeeting of the A CL, pages 88-95.D.
Yuret.
1998.
Discovery of Linguistic RelationsUsing Lexical Attraction.
Ph.D. thesis, MIT.131
