Statistical Models for Topic SegmentationJeffrey C. Reynar lMicrosoft CorporationOne Microsoft WayRedmond, WA 98052 USAjreynar@microsoft.comAbstractMost documents are about more than onesubject, but many NLP and IR techniquesimplicitly assume documents have just onetopic.
We describe new clues that mark shiftsto new topics, novel algorithms foridentifying topic boundaries and the uses ofsuch boundaries once identified.
We reporttopic segmentation performance on severalcorpora as well as improvement on an IR taskthat benefits from good segmentation.IntroductionDividing documents into topically-coherentsections has many uses, but the primarymotivation for this work comes from informationretrieval (IR).
Documents in many collectionsvary widely in length and while the shortest mayaddress one topic, modest length and longdocuments are likely to address multiple topics orbe comprised of sections that address variousaspects of the primary topic.
Despite this fact,most IR systems treat documents as indivisibleunits and index them in their entirety.This is problematic for two reasons.
First, mostrelevance metrics are based on word frequency,which can be viewed as a function of the topicbeing discussed (Church and Gale, 1995).
(Forexample, the word header is rare in generalEnglish, but it enjoys higher frequency indocuments about soccer.)
In general, wordfrequency is a good indicator of whether adocument is relevant o a query, but consider along document containing only one sectionrelevant o a query.
If a keyword is used only inthe pertinent section, its overall frequency in thedocument will be low and, as a result, thedocument as a whole may be judged irrelevantdespite the relevance of one section.The second reason it would be beneficial to indexsections of documents i that, once a search enginehas identified a relevant document, users wouldbenefit from direct access to the relevant sections.This problem is compounded when searchingmultimedia documents.
If a user wants to find aparticular news item in a database of radio ortelevision news programs, they may not have thepatience to suffer through a 30 minute broadcast tofind the one minute clip that interests them.Dividing documents into sections based on topicaddresses both of these problems.
IR engines canindex the resulting sections just like documentsand subsequently users can peruse those sectionstheir search engine deems relevant.
In the nextsection we will discuss the nature of our approach,then briefly describe previous work, discussvarious indicators of topic shifts, outline novelalgorithms based on them and present our results.I Our ApproachWe treat the process of creating documents as aninstance of the noisy channel model.
In thisidealization, prior to writing, the author has inmind a collection of disjoint topics that she intendsto address.
During the writing process, due to thegoals of writing smooth prose and knitting herdocument into a coherent whole, she blurs theboundaries between these topics.
Thus, we assumethere is a correct segmentation that has beenhidden from our view.
Our goal, therefore, is tomodel the clues about the original segmentationthat were not obliterated while writing.We view segmentation as a labeling task.
Giventhe text of a document and a collection of putativetopic boundary locations--which couldcorrespond to sentence boundaries, paragraphboundaries, pauses between utterances, changes inspeaker or some arbitrary list of choice points--This work was conducted as part of my Ph.D. thesis work at the University of Pennsylvania.357we label each of them as either the location of atopic boundary or not.
We perform this labelingusing statistical algorithms that combine diversesources of evidence to determine the likelihood ofa topic boundary.2 Previous WorkMuch research has been devoted to the task ofstructuring text--that is dividing texts into unitsbased on information within the text.
This workfalls roughly into two categories.
Topicsegmentation focuses on identifying topically-coherent blocks of text several sentences throughseveral paragraphs in length (e.g.
see Hearst,1994).
The prime motivation for identifying suchunits is to improve performance on language-processing or IR tasks.
Discourse segmentation,on the other hand, is often finer-grained, andfocuses on identifying relations betweenutterances (e.g.
Grosz and Sidner, 1986 orHirschberg and Grosz, 1992).Many topic segmentations algorithms have beenproposed in the literature.
There is not enoughspace to review them all here, so we will focus ondescribing a representative sample that coversmost of the features used to predict the locationof boundaries.
See (Reynar, 1998) for a morethorough review.Youmans devised a technique called theVocabulary Management Profile based on thelocation of first uses of word types.
He positedthat large clusters of first uses frequentlyfollowed topic boundaries since new topicsgenerally introduce new vocabulary items(Youmans, 1991).Morris and Hirst developed an algorithm (Morrisand Hirst, 1991) based on lexical cohesionrelations (Halliday and Hasan, 1976).
They usedRoget's 1977 Thesaurus to identify synonymsand other cohesion relations.Kozima defined a measure called the LexicalCohesion Profile (LCP) based on spreadingactivation within a semantic network derivedfrom.
a machine-readable dictionary.
Heidentified topic boundaries where the LCP scorewas low (Kozima, 1993).Hearst developed a technique called TextTilingthat automatically divides expository texts intomulti-paragraph segments using the vector spacemodel from IR (Hearst, 1994).
Topic boundarieswere positioned where the similarity between theblock Of text before and after the boundary waslow.In previous work (Reynar, 1994), we described amethod of finding topic boundaries using anoptimisation algorithm based on word repetitionthat was inspired by a visualization techniqueknown as dotplotting (Helfman, 1994).Ponte and Croft predict topic boundaries using amodel of likely topic length and a query expansiontechnique called Local Content Analysis that mapssets of words into a space of concepts (Ponte andCroft, 1997).Richmond, Smith and Amitay designed analgorithm for topic segmentation that weightedwords based on their frequency within a documentand subsequently used these weights in a formulabased on the distance between repetitions of wordtypes (Richmond et al, 1997).Beeferman, Berger and Lafferty used the relativeperformance of two statistical language modelsand cue words to identify topic boundaries(Beeferman et al, 1997).3 New Clues for Topic SegmentationPrior work on topic segmentation has exploitedmany different hints about where topic boundarieslie.
The algorithms we present use many cues fromthe literature as well as novel ones.
Our approachis statistical in nature and weights evidence basedon its utility in segmenting a training corpus.
As aresult, we do not use clues to form hard and fastrules.
Instead, they all contribute vidence used toeither increase or decrease the likelihood ofproposing a topic boundary between two regionsof text.3.1 Domain-specific Cue PhrasesMany discourse segmentation techniques (e.g.Hirschberg and Litman, 1993) as well as sometopic segmentation algorithms rely on cue wordsand phrases (e.g.
Beeferman et al, 1997), but thetypes of cue words used vary greatly.
Those weemploy are highly domain specific.
Taking an: 358example from the broadcast news domain wherewe will demonstrate the effectiveness of ouralgorithms, the phrase joining us is a goodindicator that a topic shift has just occurredbecause news anchors frequently say things suchas joining us to discuss the crisis in Kosovo isCongressman... when beginning new stories.Consequently, our algorithms use the presence ofphrases uch as this one to boost the probabilityof a topic boundary having occurred.joining usgood eveningbrought o you bythis just inwelcome back<person ame> <station>this is <person ame>Table 1: A sampling of domain-specific cue phraseswe employ.Some cue phrases are more complicated andcontain word sequences of particular types.
Notsurprisingly, the phrase this is is common inbroadcast news.
When it is followed by aperson's name, however, it serves as a good cluethat a topic is about to end.
This is <personname> is almost always said when a reporter issigning off after finishing an on-location report.Generally such signoffs are followed by the startof new news stories.
A sampling of the cuephrases we use is found in Table 1.
Since ourtraining corpus was relatively small we identifiedthese by hand, but on a different corpus weinduced them automatically (Reynar, 1998).
Theresults we present later in the paper ely solely onmanually identified cues phrases.Identifying complex cue phrases involves patternmatching and determining whether particularword sequences belong to various classes.
Toaddress this, we built a named entity recognitionsystem in the spirit of those used for the MessageUnderstanding Conference evaluations (e.g.
Bikelet al, 1997).
Our named entity recognizer used amaximum entropy model, built with AdwaitRatnaparkhi's tools (Ratnaparkhi, 1996) to labelword sequences as either person, place, companyor none of the above based on local cuesincluding the surrounding words and whetherhonorifics (e.g.
Mrs. or Gen.) or corporatedesignators (e.g.
Corp. or Inc.) were present.
Ouralgorithm's labelling accuracy of 96.0% by tokenwas sufficient for our purposes, but performance isnot directly comparable to the MUC competitors'.Though we trained from the same data, wepreprocessed the data to remove punctuation andcapitalization so the model could be applied tobroadcast news data that lacked these helpfulclues.
We separately identified television etworkacronyms using simple regular expressions.3.2 Word Bigram FrequencyMany topic segmentation algorithms in theliterature use word frequency (e.g.
Hearst, 1994;Reynar, 1994; Beeferman et al, 1997).
Anobvious extension to using word frequency is touse the frequency of multi-word phrases.
Suchphrases are useful because they approximate wordsense disambiguation techniques.
Algorithms thatrely exclusively on word frequency might befooled into suggesting that two stretches of textcontaining the word plant were part of the samestory simply because of the rarity of plant and thelow odds that two adjacent stories contained it dueto chance.
However, if plant in one sectionparticipated in bigrams such as wild plant, nativeplant and woody plant but in the other section wasonly in the bigrams chemical plant, manufacturingplant and processing plant, the lack of overlapbetween sets of bigrams could be used to decreasethe probability that the two sections of text were inthe same story.
We limited the bigrams we used tothose containing two content words.3.3 Repetition of Named EntitiesThe named entities we identified for use in cuephrases are also good indicators of whether twosections are likely to be in the same story or not.Companies, people and places figure prominentlyin many documents, particularly those in thedomain of broadcast news.
The odds that differentstories discuss the same entities are generally low.There are obviously exceptions--the President ofthe U.S. may figure in many stories in a singlebroadcast--but nonetheless the presence of thesame entities in two blocks of text suggest hatthey are likely to be part of the same story.3.4 Pronoun UsageIn her dissertation, Levy described a study of theimpact of the type of referring expressions used,the location of first mentions of people and thegestures peakers make upon the cohesiveness of359discourse (Levy, 1984).
She found a strongcorrelation between the types of referringexpressions people used, in particular howexplicitthey were, and the degree of cohesiveness withthe preceding context.
Less cohesive utterancesgenerally contained more explicit referringexpressions, such as definite noun phrases orphrases consisting of a possessive followed by anoun, while more cohesive utterances more.frequently contained zeroes and pronouns.We will use the converse of Levy's observationabout pronouns to gauge the likelihood of a topicshift.
Since Levy generally found pronouns inutterances that exhibited a high degree ofcohesion with the prior context, we assume thatthe presence of a pronoun among the first wordsimmediately following a putative topic boundaryprovides some evidence that no topic boundaryactually exists there.4 Our AlgorithmsWe designed two algorithms for topicsegmentation.
The first is based solely on wordfrequency and the second combines the results ofthe first with other sources of evidence.
Both ofthese algorithms are applied to text followingsome preprocessing including tokenization,conversion to lowercase and the application of alemmatizer (Karp et al.
,  1992).4.1 Word Frequency AlgorithmOur word frequency algorithm uses Katz's Gmodel (Katz, 1996).
The G model stipulates thatwords occur in documents either topically or non-topically.
The model defines topical words asthose that occur more than 1 time, while non-topical words occur only once.
Counterexamplesof these uses o f  topical and nontopical, of course,abound.We use the G model, shown below, to determinethe probability that a particular word, w, occurredk times in a document.
We trained the modelfrom a corpus of 78 million words of Wal l  S t reetJourna l  text and smoothed .the parameters usingDan Melamed's implementation f Good-Turingsmoothing (Gale and Sampson, 1995) andadditional ad hoc  smoothing to account forunknown words.Pr(k, w) = (1 - ct,.
)8~.
o + a w (1 - y w )Sk,l +( awrw (1 "----~1 ")k-2)(l-St. 0 -St .
l )B w - 1 B w - 1ot w is the probability that a document contains atleast 1 occurrence of word w.Y w is the probability that w is used topically in adocument given that it occurs at all.B w is the average number of occurrences indocuments with more than l occurrence of w.6 is a function with value 1 if x = y and 0 x,votherwise.The simplest way to view the G model is todecompose it into 3 separate terms that aresummed.
The first term is the probablility of zerooccurrences of a word, the second is theprobability of one occurrence and the third is theprobability of any number of occurrences greaterthan one.To detect opic boundaries, we used the model toanswer this simple question.
Is it more or lesslikely that the words following a putative topicboundary were generated independently of thosebefore it?Given a potential topic boundary, we call the textbefore the boundary region 1 and the text after itregion 2.
For the sake of our algorithm, the size ofthese regions was fixed at 230 words--the averagesize of a topic segment in our training corpus, 30files from the HUB-4 Broadcast News Corpusannotated with topic boundaries by the LDC(HUB-4, 1996).
Since the G model, unlikelanguage models used for speech recognition,computes the probability of a bag of words ratherthan a word sequence, we can use it to computethe probability of some text given knowledge ofwhat words have occurred before that text.
Wecomputed two probabilities with the model.
P,,,,, isthe probability that region 1 and region 2 discussthe same subject matter and hence that there is notopic boundary between them.
P ..... is theprobability that they discuss different subjects andare separated by a topic boundary.
P ....... therefore,is the probability of seeing the words in region 2given the context, called C, of region 1.
P ,  is the360probability of seeing the words in region 2independent of the words in region 1.
Formulaefor P ..... and P ,  are shown below.
Boundarieswere placed where P ,  was greater than P,,,,, by acertain threshold.
The threshold was used totrade precision for recall and vice versa whenidentifying topic boundaries.
The most naturalthreshold is a very small nonzero value, which isequivalent to placing a boundary wherever P.,, isgreater than P,,,,vP,,~ : 1--\[ Pr(k, w \[ C) Pw,, = l--I Pr(k, w)W w?
How many named entities were commonto both regions??
How many content words in both regionswere synonyms according to WordNet(Miller et al, 1990)??
What percentage of content words in theregion after the putative boundary werefirst uses??
Were pronouns used in the first five wordsafter the putative topic boundary?We trained this model from 30 files of HUB-4data that was disjoint from our test data.Computing Pov,, is straightforward, but P,,,requirescomputing conditional probabilities of thenumber of occurrences of each word in region 2given the number in region 1.
The formulae forthe conditional probabilities are shown in Table2.
We do not have space to derive these formulaehere, but they can be found in (Reynar, 1998).
Mis a normalizing term required to make theconditional probabilities um to 1.
In the table,x+ means x occurrences or more.Occurrencesin region 1002+Occurrencesin region 202+1+0+Conditional probability(x(l-y)~-y  1 (1 - )~-2B- I  B - Il-yy 1(1 - )~-2B- I  B - I1 1 - -  (1 - ~)k -2M(B - 1) B - 1Table 2: Conditional probabilities used to computeP nn~"4.2 A Maximum Entropy ModelOur second algorithm is a maximum entropymodel that uses these features:?
Did our word frequency algorithmsuggest a topic boundary??
Which domain cues (such as Joining usor This is <person>) were present??
How many content word bigrams werecommon to both regions adjoining theputative topic boundary?5 EvaluationWe will present results for broadcast news dataand for identifying chapter boundaries labelled byauthors.5.1 HUB-4 Corpus PerformanceTable 3 shows the results of segmenting the testportion of the HUB-4 coqgus, which consisted oftranscribed broadcasts divided into segments bythe LDC.
We measured performance bycomparing our segmentation to the gold standardannotation produced by the LDC.The row labelled Random guess shows theperformance of a baseline algorithm that randomlyguessed boundary locations with probability equalto the fraction of possible boundary sites that wereboundaries in the gold standard.
The rowTextTiling shows the performance of the publiclyavailable version of that algorithm (Hearst, 1994).Optimization is the algorithm we proposed in(Reynar, 1994).
Word frequency and Max.
Ent.Model are the algorithms we described above.
Ourword frequency algorithm does better than chance,TextTiling and our previous work and ourmaximum entropy model does better still.
See(Reynar, 1998) for graphs showing the effects oftrading precision for recall with these models.Algorithm Precision RecallRandom Iguess 0.16 0.16TextTiling 0.21 0.41Optimization 0.36 0.20Word Frequency 0.55 0.52Max.
Ent.
Model 0.59 0.60Table 3: Performance on the HUB-4 English corpus.361We also tested our models on speech-recognizedbroadca.sts from the 1997 TREC spokendocument retrieval corpus.
We did not havesufficient data to train the maximum entropymodel, but our word frequency algorithmachieved precision of 0.36 and recall of 0.52,considerably better, than the baseline of 0.19precision and recall.
Using manually producedtranscripts of the same data naturally yieldedbetter performance--precision was 0.50 and.recall 0.58.Our performance on broadcast data wassurprisingly good considering we trained theword frequency model from newswire data.Given a large corpus of broadcast data, we expectour algorithms would perform even better.We were curious, however, how much of theperformance was attributable to having numerousparameters (3 per word) in the G model and howmuch comes from the nature of the model.
Toaddress this, we discarded the or, ~, and Bparameters particular to each word and insteadused the same parameter values for each word--namely, those assigned to unknown wordsthrough our smoothing process.
This reduced thenumber of parameters from 3 .per word to only 3parameters total.
Performance of this hobbledversion of our word frequency algorithm was sogood on the HUB-4 English corpuswachievingprecision of 0.42 and recall of 0.50---that wetested it on Spanish broadcast news data from theHUB-4 corpus.
Even for that corpus we foundmuch better than baseline performance.
Baselinefor Spanish was precision and recall of 0.28, yetour 3-parameter word frequency model achieved0.50 precision and recall of 0.62.
To reiterate, weused our word frequency model with a total of 3parameters trained from English newswire text tosegment Spanish broadcast news dataWe believe that the G model, which captures thenotion of burstiness very well, is a good modelfor segmentation.
However, the more importantlesson from this work is that the concept ofburstiness alone can be used to segment exts.Segmentation performance is better when modelshave accurate measures of the likelihood of 0, 1and 2 or more occurrences of a word.
However,the mere fact that content words are bursty andare relatively unlikely to appear in neighboringregions of a document unless those two regions areabout the same topic is sufficient o segment manytexts.
This explains our ability to segment Spanishbroadcast news using a 3 parameter model trainedfrom English newswire data.5.2 Recovering Authorial StructureAuthors endow some types of documents withstructure as they write.
They may dividedocuments into chapters, chapters into sections,sections into subsections and so forth.
Weexploited these structures to evalUate topicsegmentation techniques by comparingalgorithmic determinations of structure to theauthor's original divisions.
This method ofevaluation is especially useful because numerousdocuments are now available in electronic form.We tested our word frequency algorithm on fourrandomly selected texts from Project Gutenberg.The four texts were Thomas Paine's pamphletCommon Sense which was published in 1791, thefirst .volume of Decline and Fall of the RomanEmpire by Edward Gibbon, G.K. Chesterton'sbook Orthodoxy.
and Herman Melville's classicMoby Dick.
We permitted the algorithm to guessboundaries only between paragraphs, which weremarked by blank lines in each document.To assess performance, we set the number ofboundaries to be guessed to the number theauthors themselves had identified.
As a result, thisevaluation focuses olely on the algorithm's abilityto rank candidate boundaries and not on itsadeptness at determining how many boundaries toselect.
To evaluate performance, we computed theaccuracy of the algorithm's guesses compared tothe chapter boundaries the authors identified.
Thedocuments we used for this evaluation may havecontained legitimate topic boundaries which didnot correspond to chapter boundaries, but wescored guesses at those boundaries incorrect.Table 4 presents results for the four works.
Ouralgorithm performed better than randomlyassigning boundaries for each of the documentsexcept he pamphlet Common Sense.
Performanceon the other three works was significantly betterthan chance and ranged from an improvement of afactor of three in accuracy over the baseline to afactor of nearly 9 for the lengthy Decline and Fallof the Roman Empire.362WorkCommonSenseDeclineand FallMobyDickOrthodoxyCombined# ofBoundaries7WordFrequency0.00Random0.3653 0.21 0.0024132 0.55 0.1738 0.25 0.033200 0.059 0.43Table 4: Accuracy of the Word Frequencyalgorithm on identifying chapter boundaries.5.3 IR Task PerformanceThe data from the HUB-4 corpus was also usedfor the TREC Spoken document retrieval task.We tested the utility of our segmentations bycomparing IR performance when we indexeddocuments, the segments annotated by the LDCand the segments identified by our algorithms.We modified SMART (Buckley, 1985) toperform better normalization for variations indocument length (Singhal et al, 1996) prior toconducting our IR experiments.This IR task is atypical in that there is only 1relevant document in the collection for eachquery.
Consequently, performance is measuredby determining the average rank determined bythe IR system for the document relevant o eachquery.
Perfect performance would be an averagerank of 1, hence lower average ranks are better.Table 5 presents our results.
Note that indexingthe segments identified by our algorithms wasbetter than indexing entire documents and thatour best algorithm even outperformed indexingthe gold standard annotation produced by theLDC.MethodDocumentsAnnotator segmentsWord frequency modelMax.
Ent.
ModelAverage Rank9.528.429.487.54Table 5: Performance on an IR task.
Lowernumbers are better.ConclusionWe described two new algorithms for topicsegmentation.
The first, based solely on wordfrequency, performs better than previousalgorithms on broadcast news data.
It performswell on speech recognized English despiterecognition errors.
Most surprisingly, a version ofour first model that requires little training datacould segment Spanish broadcast news documentsas well---even with parameters estimated fromEnglish documents.
Our second technique, astatistical model that combined numerous cluesabout segmentation, performs better than the first,but requires egmented training data.We showed an improvement on a simple IR taskto demonstrate he potential of topic segmentationalgorithms for improving IR.
Other potential usesof these algorithms include better languagemodeling by building topic-based languagemodels, improving NLP algorithms (e.g.coreference resolution), summarization, hypertextlinking (Salton and Buckley, 1992), automatedessay grading (Burstein et al, 1997) and topicdetection and tracking (TDT program committee,1998).
Some of these are discussed in (Reynar,1998), and others will be addressed in future work.AcknowledgementsMy thanks to the anonymous reviewers and themembers of my thesis committee, Mitch Marcus,Aravind Joshi, Mark Liberman, Julia Hirschberg andLyle Ungar for useful feedback.
Thanks also to DanMelamed for use of his smoothing tools and to AdwaitRatnaparkhi for use of his maximum entropy modellingsoftware.ReferencesBeeferman, D., Berger, A., and Lafferty, J.
(1997).
Textsegmentation using exponential models.
InProceedings of the Second Conference onEmpirical Methods in Natural LanguageProcessing, pages 35-46, Providence, RhodeIsland.Bikel, D.M., Miller, S., Schwartz, R., and Weischedel,R.
(1997).
Nymble: a high-performance learningname-finder.
In Proceedings of the FifthConference on Applied Natural LanguageProcessing, pages 194-201, Washington, D.C.Buckley, C. (1985).
Implementation of the SMARTinformation retrieval system.
Technical ReportTechnical Report 85-686, Cornell University.363Burstein, J., Wolff, S., Lu, C., and Kaplan, R. (1997).An automatic scoring system for advancedplacement biology essays.
In Proceedings of theFifth Conference on Applied Natural LanguageProcessing, pages 174-181, Washington, D.C.Church, K.W.
and Gale, W.A.
(1995).
Inversedocument frequency (IDF): A measure ofdeviations from Poisson.
tn Yarowsky, D. andChurch, K., editors, Proceedings of the ThirdWorkshop on Very Large Corpora, pages 121-130.
Association for Computational Linguistics.Gale, W. and Sampson, G. (1995).
Good-Turingsmoothing without ears.
Journal of QuantitativeLinguistics, 2.Grosz, B. J. and Sidner, C.L.
(1986).
Attention,Intentions and the Structure of Discourse.Computational Linguistics, 12 (3): 175-204.Halliday, M. and Hasan, R. (1976).
Cohesion inEnglish.
Longman Group, New York.Hearst, M.A.
(1994).
Multi-paragraph segmentation fexpository text.
In Proceedings of the 32 ~" AnnualMeeting of the Association for ComputationalLinguistics, pages 9-16, Las Cruces, New Mexico.Helfman, J.I.
(1994).
Similarity patterns in language.In IEEE Symposium on Visual Languages.Hirschberg, J. and Grosz, B.
(1992).
Intonationalfeatures of local and global discourse.
InProceedings of the Workshop on SpokenLanguage Systems, pages 441-446.
DARPA.Hirschberg, J. and Litman, D. (1993).
Empiricalstudies on the disambiguation f cue phrases.Computational Linguistics, 19(3):501-530.HUB-4 Program Committee (1996).
The 1996 HUB-4annotation specification for evaluation of speechrecognition on broadcast news, version 3.5.Karp, D., Schabes, Y., Zaidel, M. and Egedi, D.(1992).
A Freely Available Wide CoverageMorphological Analyzer for English.
Proceedingsof the 15 'h International Conference onComputational Linguistics.
Nantes, France.Katz, S.M.
(1996).
Distribution of content words andphrases in text and language modeling.
NaturalLanguage Engineering, 2(1): 15-59.Kozima, H. (1993).
Text segmentation based onsimilarity between words.
In Proceedings of the31 ~' Annual Meeting of the Association forComputational Linguistics, Student Session,pages 286-288.Levy, E.T.
(1984).
Communicating ThematicStructure in Narrative Discourse: The Use ofReferring Terms and Gestures.
Ph.D. thesis,University of Chicago.Miller, G.A., Beckwith, R., Fellbaum, C., Gross, D.,and Miller, K. (1990).
Five papers on WordNet.Technical report, Cognitive Science Laboratory,Princeton University.Morris, J. and Hirst, G. (1991).
Lexical cohesioncomputed by thesaural relations as an indicator ofthe structure of text.
Computational Linguistics,17(I):21-42.Ponte, J.M.
and Croft, W.B.
(1997).
Text segmentationby topic.
In European Conference on DigitalLibraries, pages 113-125, Pisa, Italy.Ratnaparkhi, A.
(1996).
A maximum entropy model forpart-of-speech tagging.
In Proceedings of the FirstConference on Empirical Methods in NaturalLanguage Processing, pages 133-142, Universityof Pennsylvania.Reynar, J.C. (1994).
An automatic method of findingtopic boundaries.
In Proceedings of the 32 nd AnnualMeeting of the Association for ComputationalLinguistics, Student Session, pages 331-333, LasCruces, New Mexico.Reynar, J.C. (1998).
Topic Segmentation: Algorithmsand Applications.
Ph.D. thesis, University ofPennsylvania, Department of Computer Science.Richmond, K., Smith, A., and Amitay, E. (1997).Detecting subject boundaries within text: Alanguage independent statistical approach.
InExploratory Methods in Natural LanguageProcessing, pages 47-54, Providence, RhodeIsland.Salton, G. and Buckley, C. (1992).
Automatic textstructuring experiments.
In Jacobs, P.S., editor,Text-Based Intelligent Systems: Current Researchand Practice in Information Extraction andRetrieval, pages 199-210.
Lawrence ErlbaumAssociates, Hillsdale, New Jersey.Singhal, A., Buckley, C., and Mitra, M. (1996).
Pivoteddocument length normalization.
In Proceedings ofthe A CM-SIGIR Conference on Research andDevelopment i  Information Retrieval, pages 21-29, Zurich, Switzerland.
ACM.TDT Program Committee (1998).
Topic Detection andTracking Phase 2 Evaluation Plan, version 2.1.Youmans, G. (1991).
A new tool for discourse analysis:The vocabulary management profile.
Language,67(4):763-789.364
