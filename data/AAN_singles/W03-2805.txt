Colouring Summaries BLEUKaterina PastraDepartment of Computer ScienceUniversity of Sheffieldkaterina@dcs.shef.ac.ukHoracio SaggionDepartment of Computing ScienceUniversity of Sheffieldsaggion@dcs.shef.ac.ukAbstractIn this paper we attempt to apply theIBM algorithm, BLEU, to the output offour different summarizers in order toperform an intrinsic evaluation of theiroutput.
The objective of this experimentis to explore whether a metric, originallydeveloped for the evaluation of machinetranslation output, could be used for as-sessing another type of output reliably.Changing the type of text to be evalu-ated by BLEU into automatically gener-ated extracts and setting the conditionsand parameters of the evaluation exper-iment according to the idiosyncrasiesof the task, we put the feasibility ofporting BLEU in different Natural Lan-guage Processing research areas undertest.
Furthermore, some important con-clusions relevant to the resources neededfor evaluating summaries have come upas a side-effect of running the whole ex-periment.1 IntroductionMachine Translation and Automatic Summariza-tion are two very different Natural Language Pro-cessing (NLP) tasks with -among others- differ-ent implementation needs and goals.
They bothaim at generating text; however, the properties andcharacteristics of these target texts vary consider-ably.
Simply put, in Machine Translation, the gen-erated document should be an accurate and fluenttranslation of the original document, in the targetlanguage.
In Summarization, the generated textshould be an informative, reduced version of theoriginal document (single-document summary), orsets of documents (multi-document summary) inthe form of an abstract, or an extract.
Abstractspresent an overview of the main points expressedin the original document, while extracts consist ofa number of informative sentences taken directlyfrom the source document.
The fact that, by theirvery nature, automatically generated extracts carrythe single sentence qualities of the source docu-ments1, may lead one to the conclusion that eval-uating this type of text is trivial, as compared tothe evaluation of abstracts or even machine trans-lation, since in the latter, one needs to be able toevaluate the content of the generated translation interms of grammaticality, semantic equivalence tothe source document and other quality character-istics (Hovy et al, 2002).Though the evaluation of generated extracts isnot as demanding as the evaluation of MachineTranslation, it does have two critical idiosyncraticaspects that render the evaluation task difficult:?
the compression level (word or sentencelevel) and the compression rate of the sourcedocument must be determined for the selec-tion of the contents of the extract ; the val-ues of these variables may greatly affect thewhole evaluation setup and the results ob-tained1Even if coherence issues may arise beyond the sentenceboundaries i.e.
at the text level?
the very low agreement among human eval-uators on what is considered to be ?impor-tant information?
for inclusion in the extract,reaching sometimes the point of total dis-agreement on the focus of the extract (Mani,2001; Mani et al, 2001).
The nature of thisdisagreement on the adequacy of the extractsis such that - by definition - cannot manifestitself in Machine Translation; this is becauseit refers to the adequacy of the contents cho-sen to form the extract, rather than what con-stitutes an adequate way of expressing all thecontents of the source document in a targetlanguage.The difference on the parameters to be takeninto consideration when performing evaluationwithin these two NLP tasks presents a challengefor porting evaluation metrics from the one re-search area to the other.
Given the relatively re-cent success in achieving high correlations withhuman judgement for Machine Translation evalua-tion, using the IBM content-based evaluation met-ric, BLEU (Papineni et al, 2001), we attempt torun this same metric on system generated extracts;this way we explore whether BLEU can be usedreliably in this research area and if so, which test-ing parameters need to be taken into considera-tion.
First, we refer briefly to BLEU and its useacross different NLP areas, then we locate our ex-periments relatively to this related work and wedescribe the resources we used, the tools we de-veloped and the parameters we set for running theexperiments.
The description of these experimentsand the interpretation of the results follows.
Thepaper concludes with some preliminary observa-tions we make as a result of this restricted, firstexperimentation.2 Using BLEU in NLPBeing an intrinsic evaluation measure(Sparck Jones and Galliers, 1995), BLEUcompares the content of a machine translationagainst an ?ideal?
translation.
It is based ona ?weighted average of similar length phrasematches?
(n-grams), it is sensitive to longern-grams (the baseline being the use of up to 4-grams) and it also includes a brevity penalty factorfor penalising shorter than the ?gold standard?translations (Papineni et al, 2001; Doddington,2002).
The metric has been found to highlycorrelate with human judgement, being at thesame time reliable even when run on differentdocuments and against different number of modelreferences.
Experiments run by NIST (Dodding-ton, 2002), checking the metric for consistencyand sensitivity, verified these findings and showedthat the metric distinguishes, indeed, betweenquite similar systems.
A slightly different versionof BLEU has been suggested by the same people,which still needs to be put into comparative testingwith BLEU before any claims for its performanceare made.BLEU has been used for evaluating differenttypes of NLP output to a small extent.
In (Za-jic et al, 2002), the algorithm has been used ina specific Natural Language Generation applica-tion: headline generation.
The purpose of thiswork was to use an automated metric for evalu-ating a system generated headline against a hu-man generated one, in order to draw conclusionson the parameters that affect the performance ofa system and improve scoring similarity.
In (Linand Hovy, 2002) BLEU has been applied on sum-marization.
The authors argue on the unstableand unreliable nature of manual evaluation andthe low agreement among humans on the con-tents of a reference summary.
Lin and Hovy makethe case that automated metrics are necessary andtest their own modified recall metric, along withBLEU itself, on single and multi-document sum-maries and compare the results with human judge-ment.
Modified recall seems to reach very highcorrelation scores, though direct comparative ex-perimentation is needed for drawing conclusionson its performance in relation to BLEU.
The lat-ter, has been shown to achieve 0.66 correlationin single-document summaries at 100 words com-pression rate and against a single reference sum-mary.
The correlation achieved by BLEU climbsup to 0.82 when BLEU is run over and comparedagainst multiply judged document units, that couldbe thought of as a sort of multiple reference sum-maries.
The correlation scores for multi-documentsummaries are similar.
Therefore, BLEU has beenfound to correlate quite highly with human judge-ment for the summarization task when multiplejudgement is involved, while -as Lin and Hovyindicate- using a single reference is not adequatefor getting reliable results with high correlationwith the human evaluators.It is this conclusion that Lin and Hovy havedrawn, that contradicts findings by the IBM andNIST people for the importance of using multiplereferences when using BLEU in Machine Trans-lation.
The use of either multiple references orjust a single reference has been proved not to af-fect the reliability of the results provided by BLEU(Papineni et al, 2001; Doddington, 2002), whichseems not to be the case in summarization.
Thisis not a surprise; comparisons of content-basedmetrics for summarization in (Donaway et al,2000) have led the authors to the conclusion thatsuch metrics correlate highly with human judge-ment when the humans do not disagree substan-tially.
The fact that more than one reference sum-maries are needed because of the low agreementbetween human evaluators has been repeatedlyindicated in automatic summarization evaluation(Mani, 2001).We attempt to test BLEU?s reliability whenchanging various evaluation parameters such asthe source documents, the reference summariesused and even parameters unique to the evaluationof summaries, such as the compression rate of theextract.
In doing so, we explore whether the met-ric is indeed reliable only when using more thana single reference and whether any other testingparameter could compensate for lack of multiplereferences, if used appropriately.3 Evaluation ExperimentIn this section, we will present a description of theexperiments themselves, along with the results ob-tained and their analysis, preceded by informationon the corpus we used for our experiments and thetools we developed for setting their parameters andrunning them automatically.3.1 Testing corpusWe make use of part of the language resources(HKNews Corpus) developed during the 2001Workshop on Automatic Summarization of Mul-tiple (Multilingual) Documents (Saggion et al,2002).The documents of each cluster are all relevantto a specific topic-query, so that they form, in fact,thematic clusters.
The texts are marked up on theparagraph, sentence and word level.
Annotationswith linguistic information (Part of speech tagsand morphological information), though markedup on the documents have not been used in ourexperiments at all.
Three judges have assessedthe sentences in each cluster and have provided ascore on a scale from 0 to 10 (i.e.
utility judge-ment), expressing how important the sentence isfor the topic of the cluster (Radev et al, 2000).In our experiments, we have used three documentclusters, each consisting of ten documents in En-glish.3.2 SummarizersIt is important to note, that our objective is notto demonstrate how a particular summarizationmethodology performs, but to analyse an evalua-tion metric.
The summaries used for the evalu-ation were produced as extracts at different ?sen-tence?
(and not word) compression rates2.
In or-der to produce summarizers for our evaluation,we use a robust summarisation system (Saggion,2002) that makes use of components for seman-tic tagging and coreference resolution developedwithin the GATE architecture (Cunningham et al,2002).
The system combines GATE componentswith well established statistical techniques devel-oped for the purpose of text summarisation re-search.
The system supports ?generic?
and query-based summarisation addressing the need for useradaptation3.
For each sentence, the system com-putes values for a number of ?shallow?
summariza-tion features: position of the sentence, term distri-bution analysis, similarity of the sentence with thedocument, similarity with the sentence at the lead-ing part of the document, similarity of the sentencewith the query, named entity distribution analysis,statistic cohesion, etc.
The values of these featuresare linearly combined to produce the sentence fi-2We have to note that the level of compression i.e sentenceor word level, affects probably the evaluation of the summa-rizers?
output.
Comparative testing could indicate whetherthis is a crucial parameter for system evaluation.3The software can be obtained from http://www.dcs.shef.ac.uk/?saggionnal score.
Top-ranked sentences are annotated un-til the target n% compression is achieved (an an-notation set is produced for each summary that isgenerated).
Different summarization systems canbe deployed by setting-up the weights that par-ticipate in the scoring formula.
Note that as thesummarization components are not aware of thecompression parameter, one would expect specificconfigurations to produce good extracts at differ-ent compression rates and across documents.We have configured four different summariz-ers, namely, the ?query-based system?
that com-putes the similarity of each sentence of the sourcedocument with the documents topic-query, in or-der to decide whether to include a sentence in thegenerated extract or not.
We also have the ?Sim-ple 1 system?, whose main feature is that it com-putes the similarity of a sentence with the wholedocument, the ?Simple 2 system?
which is a leadbased summarizer and the ?Simple 3 system?
thatblindly extracts the last part of the source docu-ment.3.3 Judge-based SummariesFollowing the same methodology used in (Saggionet al, 2002), we implemented a judge-based sum-marization system that given a judge number (1,2, 3, or all), it scores sentences based on a combi-nation of the utility that the sentence has accord-ing to the judge (or the sum of the utilities if ?all?
)and the position of the sentence (leading sentencesare preferred).
These ?extracts?
represent our gold-standards for evaluation in our experiments.
Inorder to use the documents in a stand-alone way,we have enriched the initial corpus mark-up andadded to each document information about clusternumber, cluster topic (or query) and all the infor-mation about utility judgement (that informationwas kept in separate files in the original HKNewscorpus).3.4 Evaluation SoftwareWe have developed a number of software compo-nents to facilitate the evaluation and we make useof the GATE development environment for testingand processing.
The evaluation package allows theuser to specify different reference extracts (judge-based summarizers) and summarization systems tobe compared.Co-selection comparison (i.e., precision and re-call) is being done with modules obtained fromthe GATE library (AnnotationDiff components).Content-based comparison by the Bleu algorithmwas implemented as a Java class.
The exact for-mula provided by the developers of BLEU hasbeen implemented following the baseline config-urations i.e use of 4-grams and uniform weightssumming to 1:Bleu(S,R) = K(S,R) ?
eBleu1(S,R)Bleu1(S,R) =?i=1,2,...nwi ?
lg( |(Si?Ri)||Si| )K(S,R) ={1 if |S| > |R|e(1?|R||S| ) otherwisewi = i?j=1,2,...n jfor i = 1, 2, ..., nwhere S and R are the system and referencesets.
Si and Ri are the ?bags?
of i-grams for sys-tem and reference.
n is a parameter of our imple-mentation, but for the purpose of our experimentswe have set n to 4.3.5 ExperimentsIn our experiments we have treated compressionrates and clusters as variables each one being acondition for the other and both dependent to athird variable, the gold standard summary.
Weran BLEU in all different combinations in order tosee the main effects of each combination and theinteractions among them.
In particular, we haveused three different text clusters, consisting oftexts that refer to the same topic: cluster 1197 on?Museum exhibits and hours?, cluster 125 whichdeals with ?Narcotics and rehabilitation?
and clus-ter 241 which refers to ?Fire safety and buildingmanagement?.
For the texts of each cluster wehave three different reference summaries (createdaccording to the utility judgement score assignedby human evaluators cf.
3.1 and 3.2).
We willrefer to these as Reference1, Reference2 and Ref-erence3.
The judges behind these references areall the same for the three text clusters with one ex-ception: Reference1 in cluster 241 has not beencreated by the same human evaluator as the Refer-ence 1 summaries for the other two clusters.
Last,we ran the experiments at five different compres-sion rates 4: 10%, 20%, 30%, 40% and 50%.We first ran BLEU on the reference summariesin order to check whether BLEU is consistentin the data it produces concerning the agreementamong human evaluators.
We tried all possiblecombinations for comparing the reference sum-maries; using at first Reference 1 as the gold stan-dard, we ran BLEU over References 2 and 3 andwe did this for two clusters (since the third?s -241-Reference 1 set of summaries had been created byanother judge - a fourth one).
We did this for allfive compression rates separately.
We repeated theexperiment changing the gold standard and the ref-erences to be scored accordingly (i.e Reference 1and 3 against 2, Reference 1 and 2 against 3).
Theresults we got were consistent neither across clus-ters, nor within clusters across compression rates;however the latter, did show a general tendency forconsistency which allows for some observationsto be made.
In cluster 1197, References 1 and 2are generally in higher agreement than with 3, afact verified regardless the reference chosen as agold standard.
The fact that References 1 and 2 arevery close was also evident when both comparedagainst Reference 3; though the latter is generallycloser to Reference 2, the scores assigned to Ref-erence 1 and 2 are extremely close.
In cluster 125,Reference 1 is consistently closer to 3, while 2 iscloser to 1 at some compression rates and closerto 3 at others.
These very close scores indicatethat all three references are similarly ?distant?
onefrom another, and no groupings of agreement canactually be made.
Agreement between referencesummaries augments as the compression rate alsoincreases, with the higher similarity scores alwaysfound at the 50% compression rate and the lowerones consistently found at 10%.
Table 1 showsa consistent ranking across compression rates incluster 1197 and an inconsistent one in cluster 125,using in both cases Reference 2 as the gold stan-dard.
From this first experiment, the rankings of4In our experiments compression is always performed atthe sentence levelthe reference summaries seem to depend on thedifferent values of the variables used.
If that isthe case, then one should use BLEU in summa-rization only when determining specific values forthe evaluation experiment, that will guarantee re-liable results; but how could one determine whichvalue(s) should be chosen?
To explore things fur-ther we decided to proceed with a second experi-ment set up in a similar way.In our second experiment we try to comparethe system generated extracts (and therefore theperformance of the four summarizers) against thedifferent human references.
Again, the differ-ent rounds of the experiment involve multiple pa-rameters; the generated extracts of all three textclusters are compared against each reference sum-mary, against all reference summaries (integratedsummary) and at all five compression rates.
Goingthrough the different stages of this experiment weobserve that:?
For Reference X within Cluster Y acrossCompressions, the ranking of the systems isnot consistentOne does not get the same system ranking at dif-ferent compression rates.
The similarity of a gen-erated extract to a specific reference summary isthe same at some compression rates, similar at oth-ers (e.g the order of two of the systems swaps)and totally different at other rates.
No patternsarise in the way that rankings are similar at spe-cific compression rates; for example, in table 2,there seems to be a prevailing ranking common infour compression rates; however, the ranking pro-vided at 10% is totally different, and no apparentreason seems to justify this deviation (e.g.
veryclose scores).
Furthermore, this agreement amongthe four highest compression rates does not forma pattern i.e it does not appear as such across clus-ters or references.?
For Reference X at Compression Y acrossClusters, the ranking of the systems is notconsistentIn our experiments we were able to observe 15 dif-ferent realisations of these testing configurationsand hardly did a case of consistency at a compres-sion rate across clusters appeared.Ref 2 - 1197 10% 20% 30% 40% 50%Reference 1 0.50 - 1 0.67 - 1 0.73 - 1 0.73 - 1 0.79 - 1Reference 3 0.34 - 2 0.51 - 2 0.52 - 2 0.63 - 2 0.69 - 2Ref 2 - 125 10% 20% 30% 40% 50%Reference 1 0.36 - 1 0.41 - 1 0.59 - 2 0.67 - 2 0.78 - 1Reference 3 0.20 - 2 0.46 - 2 0.66 - 1 0.73 - 1 0.73 - 2Table 1: Reference summary similarity scores and rankings across clusters and compression ratesReference 3 10% 20% 30% 40% 50%Query-based 0.44 - 2 0.50 - 1 0.58 - 1 0.66 - 1 0.71 - 1Simple 1 0.10 - 3 0.23 - 3 0.48 - 3 0.57 - 3 0.64 - 3Simple 2 0.52 - 1 0.45 - 2 0.53 - 2 0.62 - 2 0.68 - 2Simple 3 0.03 - 4 0.07 - 4 0.08 - 4 0.11 - 4 0.11 - 4Table 2: System scores and rankings for cluster 241, against Reference 3, at different compression rates?
For Reference All across Clusters at multipleCompressions, the ranking of the systems isconsistentEstimating similarity scores against ReferenceAll (use of multiple references cf.
3.2), proves toprovide reliable, consistent results across clustersand compression rates.
Table 3 presents the scoresand corresponding system rankings for two differ-ent clusters and at the five different compressionrates.
The prevailing system ranking is [1324],which is what we would intuitively expect accord-ing to the features of the summarizers we compare.Some deviations from this ranking are due to verysmall differences in the similarity scores assignedto the systems5, which indicates the need for usinga larger testing corpus for the experiments.So, the need for multiple references is evident;BLEU is a consistent, reliable metric, but whenused in summarization, one has to apply it to mul-tiple references in order to get reliable results.This is not just a way to improve correlation withhuman judgement (Lin and Hovy, 2002); it is acrucial evaluation parameter that affects the qual-ity of the automatic evaluation results.
In our casewe had a balanced set of reference summaries towork with, i.e none of them was too similar to an-other.
The more reference summaries one has andthe larger one?s testing corpus, the safer the con-clusions drawn will be.
However, what happenswhen there is lack of such resources and especially5For example, at the 10% compression rate, cluster 1197,systems Simple 1 and Simple 2 swap places in the final rank-ing with a 0.005 difference in their similarity scoresof multiple reference summaries?
Is there a wayto use BLEU with a single reference summary andstill get reliable results back?Looking at the results of our experiments, whenusing each reference summary separately as a goldstandard, we realised that estimating the averageranking of each system across multiple compres-sion rates might lead to consistent rankings.
Fol-lowing the average rank aggregation techique (Ra-jman and Hartley, 2001), we transfered the aver-age scores each system got per text cluster at eachcompression rate into ranks and computed the av-erage rank of each system across all five compres-sion rates per text cluster and against each refer-ence summary.
Table 4, shows the average systemrankings we got for each system at clusters 1197and 125, using Reference 1, 2, and 3 separately.
[1324] is the average system ranking that is clearlyindicated in the vast majority of cases.
The twoexceptions to this are due to extremely small dif-ferences in average scores at specific compressionrates and indicate the need for scaling up our ex-periment, a fact that has already been indicated bythe results of our experiment using multiple refer-ences (Reference All).4 Conclusions and Future WorkBLEU has been developed for measuring con-tent similarity in terms of length and wordingbetween texts.
For the evaluation of automati-cally generated extracts, the metric is expected tocapture similarities between sentences not sharedby both the generated text and the model sum-Ref All - 1197 10% 20% 30% 40% 50%Query based 0.55 - 1 0.47 - 1 0.49 - 1 0.62 - 1 0.63 - 2Simple 1 0.3184 - 2 0.32 - 3 0.40 - 3 0.49 - 3 0.62 - 3Simple 2 0.3134 - 3 0.39 - 2 0.44 - 2 0.56 - 2 0.67 - 1Simple 3 0.02 - 4 0.03 - 4 0.07 - 4 0.11 - 4 0.13 - 4Ref All - 125 10% 20% 30% 40% 50%Query based 0.44 - 1 0.43 - 1 0.57 - 1 0.72 - 1 0.7641 - 2Simple 1 0.18 - 3 0.3684 - 2 0.54 - 2 0.60 - 3 0.68 - 3Simple 2 0.32 - 2 0.3673 - 3 0.44 - 3 0.66 - 2 0.7691 - 1Simple 3 0.03 - 4 0.06 - 4 0.07 - 4 0.10 - 4 0.14 - 4Table 3: Systems?
similarity scores and rankings using Reference All as gold standard10% 20% 30% 40% 50% Average RankRef 1 - 125 1324 1234 2134 1324 1234 1234Ref 2 - 125 1324 1324 1324 1324 2314 1324Ref 3 - 125 2314 2314 1324 1324 2314 2314Ref 1 - 1197 1324 2314 1324 1324 2314 1324Ref 2 - 1197 1324 1324 1324 1324 2314 1324Ref 3 - 1197 1324 1324 1324 1324 2314 1324Table 4: Systems?
average rankings resulting from ranks at multiple compression rates in clusters 125 and1197.
(Systems assumed to be listed in alphabetical order: Query-based, Simple1, Simple2, Simple3)mary.
Going through the texts scored in the aboveexperiments, we found cases in which BLEUdoes not actually capture content similarity tosuch a granularity that a human would.
Some-times, this is because the order of the wordsforming n-grams differs slightly but still conveysthe same meaning (e.g.
?...abusers reported...?vs.
?...reported abusers...?)
and most of thetimes because there is no way to capture casesof synonymy, paraphrasing (e.g.
?downwardtendency?/?falling trend?/?decrease?)
and otherdeeper semantic equivalence (e.g.
?number of X?vs.
?9,000 of X?).
Such phenomena are -of course-expected from a statistical metric which involvesno linguistic knowledge at all.
Our aim in this pa-per was to shed some light on the conditions underwhich the metric performs reliably within summa-rization, given the different parameters that affectevaluation in this NLP research area.
From the re-sults obtained by our preliminary experiments, wehave generally concluded that:?
Running BLEU over system generated sum-maries using a single reference affects the re-liability of the results provided by the metric.The use of multiple references is a sine quanon for reliable results?
Running BLEU over system generated sum-maries at multiple compression rates and esti-mating the average rank of each system mightyield consistent and reliable results even witha single reference summary and thereforecompensate for lack of multiple referencesummariesIn order to draw more safe conclusions, we needto scale our experiments considerably, and this isalready in progress.
Many research questions needstill to be answered, such as how BLEU scorescorrelate with results produced by other content-based metrics used in summarization and else-where.
We hope that this preliminary, experimen-tal work on porting evaluation metrics across dif-ferent NLP research areas will function as a stim-ulus for extensive and thorough research in this di-rection.ReferencesH.
Cunningham, D. Maynard, K. Bontcheva, andV.
Tablan.
2002.
GATE: A framework and graph-ical development environment for robust NLP toolsand applications.
In ACL 2002.G.
Doddington.
2002.
Automatic evaluation of ma-chine translation quality using n-gram co-occurencestatistics.
In Proceedings of HLT 2002, Human Lan-guage Technology Conference, San Diego, CA.R.
Donaway, K. Drummey, and L. Mather.
2000.
Acomparison of rankings produced by summariza-tion evaluation measures.
In Proceedings of theANLP-NAACL 2000 Workshop on Automatic Sum-marization, Advanced Natural Language Processing- North American of the Association for Computa-tional Linguistics Conference, Seattle, DC.E.
Hovy, M. King, and A. Popescu-Belis.
2002.
An in-troduction to machine translation evaluation.
In Pro-ceedings of the LREC 2002 Workshop on MachineTranslation Evaluation: Human Evaluators MeetAutomated Metrics, Language Resources and Eval-uation Conference.
European Language ResourcesAssociation (ELRA).Ch.
Lin and E. Hovy.
2002.
Manual and automaticevaluation of summariess.
In Proceedings of theACL 2002 Workshop on Automatic Summarization,Association for Computation Linguistics, Philadel-phia, PA.I.
Mani, T. Firmin, and B. Sundheim.
2001.
Summac:A text summarization evaluation.
Natural LanguageEngineering.I.
Mani.
2001.
Summarization evaluation: anoverview.
In Proceedings of the NAACL 2001 Work-shop on Automatic Summarization, North Chapterof the Association for Computational Linguistics,Pittsburgh, PA.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2001.Bleu: a method for automatic evaluation of machinetranslation.
Technical Report RC22176 (W0 109-022), IBM Research Division.Dr.
Radev, J. Hongyan, and M. Budzikowska.
2000.Centroid-based summarization of multiple docu-ments: sentence extraction, utility-based evaluation,and user studies.
In ANLP/NAACL Workshop onSummarization, Seattle, WA, April.M.
Rajman and A. Hartley.
2001.
Automatically pre-dicting mt system rankings compatible with fluency,adequacy or informativeness scores.
In Proceed-ings of the MT Summit 2001 Workshop on MachineTranslation Evaluation: Who did what to whom, Eu-ropean Association for Machine Translation, Santi-ago de Compostella, Spain.H.
Saggion, D.
Radev., S. Teufel, L. Wai, andS.
Strassel.
2002.
Developing infrastructure forthe evaluation of single and multi-document sum-marization systems in a cross-lingual environment.In 3rd International Conference on Language Re-sources and Evaluation (LREC 2002), pages 747?754, Las Palmas, Gran Canaria, Spain.H.
Saggion.
2002.
Shallow-based Robust Summariza-tion.
In Automatic Summarization: Solutions andPerspectives, ATALA, December, 14.Karen Sparck Jones and Julia R. Galliers.
1995.
Eval-uating Natural Language Processing Systems: AnAnalysis and Review.
Number 1083 in LectureNotes in Artificial Intelligence.
Springer.D.
Zajic, B. Dorr, and R. Schwartz.
2002.
Automaticheadline generation for newspaper stories.
In Pro-ceedings of the ACL 2002 Workshop on AutomaticSummarization, Association for Computation Lin-guistics, Philadelphia, PA.
