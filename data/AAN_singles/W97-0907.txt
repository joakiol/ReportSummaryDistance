A Language Ident i f i ca t ion  App l i ca t ion  Bu i l t  on the  JavaC l ient /Server  P la t fo rmGary AdamsSun Microsystems LaboratoriesTwo Elizabeth DriveChelmsford, MA 01824-4195, USAgary.
adams?east, sun.
tomPhilip ResnikDept.
of Linguistics and UMIACSUniversity of MarylandCollege Park, MD 20742, USAresnik~umiacs, umd.
eduAbstractWe describe an experimental system im-plemented using the Java(TM) program-ming language which demonstrates a va-riety of application-level tradeoffs availableto distributed natural language processing(NLP) applications.
In the context of theWorld Wide Web (WWW), it is possible toprovide value added functionality to legacydocuments in a client side browser, a docu-ment server or an intermediary agent.
Us-ing a well-known ngram-based algorithmfor automatic language identification, wehave constructed a system to dynamicallyadd language labels for whole documentsand text fragments.
We have experi-mented with several client/server configu-rations, and present he results of tradeoffsmade between labelling accuracy and thesize/completeness of the language models.1 IntroductionIn the 1.1 release of the Java Developers Kit(TM), awide selection of text processing and international-ization interfaces have been added to the base Javapackage 1 making the package usable for multilingual1A few pointers to online Java resources.Java Developer Kit :< U RL:http://www.javasoft.com/products/jdk/1.1/index.html>Java Server(TM) Product Family :< URL:http://jeeves.javasoft.com/products/java-server/index.html>HotJava(TM) Browser :< U RL:http://www.javasoft.com/products/HotJava/index.html>Java internationalization :< U RL:http://java.sun.com/products/jdk/1.1/docs/guide/intl/index.html>Java Workshop :< U RL:http://www.sun.com/workshop/index.html>Java JIT :< U RL:http://www.sun.com/workshop/java/jit/index.html>text processing.
The Java programming language,the portable Java virtual machine and the basic webinfrastructure of client web browsers and documentresource protocols provide a widely deployed plat-form suitable for distributing NLP applications.
Ourresearch is targeted at shallow machine translationand summarization of multilingual web pages.
2 Toproperly bootstrap this technology we require ap-propriate language labels on documents.
Languagelabels may be present at a whole document or collec-tion of documents level for large granularity appli-cations or at a structural component (SGML entitylevel) for fine grained uses.
(Yergeau et al, 1997)Using an ngram language model (Dunning, 1994),we have explored avariety of mechanisms for addinglanguage labels to legacy documents as part of thenormal end user experience of the World Wide Web.Three obvious places the language labels could beadded to legacy documents are within an end userweb browser, within a document repository serverand within an intermediary proxy server.
We haveexperimented with several client/server configura-tions, and present he results of tradeoffs made be-tween labelling accuracy and the size/completenessof the language models .2 Automatic Language IdentificationAlthough the general framework will support a va-riety of algorithms for automatic language identifi-cation, our implementation is based on Dunning's(1994) character ngram approach, which is concep-tually quite simple and achieves good performanceeven given relatively small training sets (50K oftraining text is more than enough, and one canmake do with as little as 1-2K), and even whenthe strings to be classified are quite short (e.g., 50characters).
Essentially, the method involves con-structing a probabilistic model (or "profile") basedon character ngrams for each language in the classi-fication set, and then performing classification of an2Sun Microsystems Laboratories efforts inInternational Linguistic Applications:< URL:http: / /www.sunlabs.com/research /ila/ >.L.
)43unknown string by selecting the model most likelyto have generated that string.Although the model itself is quite simple, somesubtle and not-so-subtle issues do arise in puttingthe algorithm into practice.
First among these is theproblem of matching the character set of the inputtext to the character set assumed by the languageprofiles - -  for example, Shift-JIS and EUC-Japanare frequently used to encode Japanese documentsin the PC and Unix worlds, respectively.
Docu-ments currently found on the WWW are often in-sufficiently labeled to indicate the language of textor the encoding of the characters within the docu-ment.
We believe that use of Unicode will becomeincreasingly widespread, obviating this problem, al-though for the time being we avail ourselves of thereasonable tools available 3 for identifying and con-verting among character sets.Second, sparse training data is a significant fac-tor in ngram modeling, even at the level of charac-ter co-occurrences, ince we consider character se-quences up to 5 characters in length.
We have ex-perimented with simple add-k smoothing and, not-ing known problems with that method (Gale andChurch, 1990), we have also experimented withGood-Turing smoothing (Good, 1953) - finding, toour surprise, that the simple "add ?"
is only slightlyless accurate.Table 2 shows the performance of the languageidentification algorithm when run on Dunning's(1994) English/Spanish test set, using language pro-files for English and Spanish constructed from train-ing sets of 50K characters each and varying the sizeof the ngrams and length of the test strings.
Thisexperiment used Good-Turing smoothing and alsoadopted a simplified approximation of conditionalprobabilities used by Dunning (personal communi-cation) in his experiments.
Each row of the tableshows the language and length of the test strings,the ngram size, the number of test strings classifiedcorrectly and incorrectly, and the percentage correct.Third, in an environment where computation maymore efficiently be done on the client side ratherthan the server side, the size of the language pro-files becomes relevant, since computation cost mustbe traded off against communication cost for thedata needed in order to perform the classification.Given that probabilities for low frequency items canbe poorly estimated anyway, we have experimentedwith eliminating low-frequency items from the pro-file - -  e.g., treating singletons (ngrams appearingjust once in the training data) as if they never ap-peared at all and using the smoothed-zero valuefor them instead, thus trading model size againstclassification accuracy.
Again to our surprise, we~Xemacs internationalization :< U RL:http://www.xemacs.org/xemacs-faq.html~interna.tion Miza.tion >have found that quite reasonable classification per-formance is sustained even when filtering out notonly singletons but even ngrams that appear twiceand even three times in the training set .
(see Ta-bles 3-5).Table 1 shows the dramatic size reduction thattakes place as smaller window sizes are used in train-ing the language models.
In the current implemen-tation plain text files are used for maximum porta-bility of the resources.
An application that uses a5-gram model without filtering any of the trainingdata would use a 220K model containing 13K ob-served ngrams, with an average accuracy of 98.68%for 100-500 character length strings.
If the same ap-plication can function effectively with a marginallylower accuracy rate of 98.32%, then the same train-ing data can be used to produce a profile a full orderof magnitude smaller (a 23K profile containing only1.6K ngrams), by using a trigram model and filter-ing out those trigrams whose observed frequency isless than 4.
This 10X reduction in size for this par-ticular resource could mean supporting 10 times asmany languages with the same memory footprint ordelivering the linguistic resource 10 times as fast fora client side computation.Finally, standardization of language labels mustbe addressed; this work follows the ISO standardsfor language and country codes for internationaliza-tion (\[SO, 1988b; ISO, 1988a; Yergeau et al, 1997;Alvestrand, 1995).3 C l ient /Server  Arch i tec tureIn designing a distributed application several deci-sions can be built into the architecture of the prod-uct or left as runtime decisions.
By using a Java vir-tual machine as the target platform, the same codecan run on a server machine or within the clientgraphical user interface.
A sophisticated programcan determine at startup whether the computationresources (memory and CPU) on the server machineor on the client workstation are better for the morecomplex algorithms.
(In our testing we work witha SparcStation(TM) 10 file server, an Ultral(TM)with 500M of memory as a high end client and aSparcStation 2 remote client over a 28.8 Kb modemconnection as a low end client.
)In addition to compute resources, it is also impor-tant to consider the network bandwidth resources.The local area network configuration can make somesimplifying assumptions that may not be appropri-ate for wide area network and remotely connectedclients.
We have explored the possibility of degradedapplication performance in exchange for reasonableresponse times for the remotely connected client, i.e.,we have allowed a higher error rate on language la-bels of short text fragments in exchange for smallerlanguage models which can easily be down-loadedover slower network connections by remote sites.
In44this section, we discuss the incorporation of lan-guage identification at three possible locations: theclient's Web browser, the document server, and be-tween them at a proxy Web server.3.1 C l ient  Web BrowserWe have experimented with two extreme client con-figurations.
Our high end client has fast CPU anda large memory pool.
Our low end client has botha slow CPU and small memory footprint.
The highend client easily caches large language profiles andis capable of computing the best possible languagelabels.
When the network resources are available tothe high end client, it makes the most sense to per-form the language labeling within the client browser.On the low end client, the available networkbandwidth was the driving architectural consider-ation.
When high bandwidth was available, delegat-ing computations to the server system provides thebest language labels and the best throughput to endusers.
In disconnected or low bandwidth situations,the client must perform its own labeling.
In thesesituations, less accurate language labels with reason-able responsiveness is preferred over slow but morecorrect results.Three pr imary techniques were used to improvethe responsiveness of the client side language la-belling interfaces.
Basically, they all attempt o min-imize the work that is performed and to overlap thework whenever possible with other end user interac-tions.?
Asynchronous processing for perceived respon-siveness.
End users perceive system responsive-ness in terms of its ability to react to their re-quests when they are presented to the system.Within our application there are clear pointsduring system initialization and end user pa-rameter selection when large amounts of net-work bandwidth and computation resources areneeded.
Using the builtin threading capabilitiesof the Java environment, we start the resourceintensive operations when they are indicated,but allow the user to continue interacting withthe user interface.
If the user requests an op-eration that requires an uninitialized resource amessage is presented and the application blocksuntil the resource is available.?
Degraded language profiles for smaller footprint.Our language identification profiles have beenbuilt with 3, 4 and 5 character ngram windows.In addition to varying the ngram window sizewe have experimented with removal of singletonand doubleton observations in the training data.While this amplifies the sparse data problem, itdoes not significantly impact the end user per-ceived error rates for large granularity text ob-jects, e.g., labeling a typical webpage with 1000characters of textual information.?
Subset of language profiles for specific userneeds.
We have been working with a mixture ofwestern European and Asian languages 4.
Forremote clients it is worth the extra effort topreselect he languages that will be most bene-ficial to distinguish on the client machine.
Fordemonstration purposes we use a dozen westernlanguages and preload a few profiles during theinitialization of the sample configuration.3.2 Document  ServerA typical document server is designed to service alarge number of end user requests.
While they areusually configured with large amounts of disk stor-age, they are not always the best computational re--sources available on the network.
For static web-pages, it is easy to include a language labeling toolfor off-line document management.
The labelingtool would be used to convert ext/htmlfile into mes-sage/http files.
5For real time information such as news wires orother database generated replies the same off linelanguage labeling tools could be used with CommonGateway Interface (CGI) 6 scripts to automatical lyadd language labels to dynamically generated web-pages.3.3 P roxy  Web ServerA proxy server is an intermediary program that pro-vides value added functionality to documents as partof the transmission process.
A proxy server could beconfigured close to the end user or close to the datasource depending on the network topology.
Proxyservers may also be employed as a shared work-group or enterprise wide facility, e.g., departmentlevel proxies can share cached webpages, or an en-terprise wide proxy could add an extra level of ac-cess controls.
By introducing language labeling ata proxy server it is possible to combine the benefitsof webpage caching and transparent content negoti-ation to reuse previously computed headers.
74We selected 200K of sample text from the WWWfor the following languages: Chinese, Czech, Danish,Dutch, English, Finnish, French, German, Greek, Hun-garian, Italian, Japanese, Korean, Norwegian, Polish,Portuguese, Spanish, Swedish, and Turkish.
We use 50Kof text for training the models, 50K for use in entropycalculations and 100K heldout for testing purposes.
Pre-liminary experiments indicate that performance compa-rable to what we have seen with the English/Spanishtest set will also be achieved with other language pairs.5Apache server documentation for variant files:< U RL:http://www.apache.org/docs/mod/mod_a~is.html>< U RL:http://www.apache.org/docs/mod/mod_negotiation.html>8Common Gateway Interface 1.1 :< U RL:http://hoohoo.ncsa.uiuc.edu/cgi/>rTarnsparent content negotiation in HTTP :< U RL:http://gewis.win.tue.nl/454 Java  C lassesThe primary reusable Java module written for lan-guage labeling in our system is called a frequencytable class.
A main() routine is provided in the classto provide a stand-alone interface for generating newlanguage profiles from training data.
The generatedlanguage profiles are self documenting text files indi-cating the parameters used in creating the languagemodel and the algorithms used for smoothing andfiltering the training data.
Methods are providedin the frequency table class for saving and loadingthe profiles to disk and for scoring individual stringsfrom a loaded profile.Specialized classes were also written to provideconnections within a client environment (in Javalingo an "applet") and within a proxy HTTP  server(again in Java lingo a "servlet").
In both the servletand applet applications of the language labeling classthe Java platform provided the basic class loadinginfrastructure to allow a common shared moduleand the distributed platform for running those al-gorithms transparently on a client or server system.5 DiscussionIn this paper, we have described a Java s implemen-tation of a character ngram language labeling algo-rithm.
This NLP module was successfully reusedin a client side Java application, in an offiine docu-ment management system and embedded within anHTTP proxy server.
With the rapid deployment ofthe globally available Java infrastructure, a tremen-dous opportunity exists for resusable NLP compo-nents.The distributed nature of our particular applica-tion, led us to explore possible tradeoffs between theaccuracy needed for client side language labeling andthe size of the language models.
By selecting smallerngram windows sizes and by disgarding infrequentlyobserved ngrams from our language profiles we canreduce the size of the models by an order of mag-nitude with an insignificant loss of precision for ourtarget application.The tradeoffs we have explored in the context ofautomatic language identification are relevant moregenerally to natural language processing in the dis-tributed setting made possible by the Java infras-tructure.
At a minimum, our observations with re-spect to character-based language models are likelyto be applicable to the word-based language mod-els used in other statistically-driven NLP applica-koen/conneg/>IETF - HTTP Working Group :< U RL:http://www.ics.uci.edu/pub/ietf/http/>aSun, Java, Java Developers Kit, Hot Java, and Ul-tral are trademarks or registered trademarks of SunMicrosystems, \[nc.
in the United States and othercountries.N Filter Lines5 0 134234 0 81963 0 35575 1 60034 1 46073 1 23935 2 38994 2 33773 2 19525 3 28634 3 26853 3 1693Bytes 1%Correct221996 98.68127917 98.6652144 98.3895858 98.6470496 98.5434688 98.3662704 98.5850973 98.5227646 98.3245782 97.8439464 98.5023527 98.32Table 1: Language profile sizestions.
Beyond that, similar client/server tradeoffsare likely to be important even in strictly knowledgebased systems.
Part-of-speech tagging and phraseidentification, foreign word translation, and topic la-beling are among the operations that promise to en-hance intelligent search and browsing on the Web,and the present paper represents a beginning step to-ward making decisions about where to locate theseoperations' computations and data.ReferencesH.
Alvestrand.
1995.
RFC 1766: Tags for the iden-tification of languages, March.Ted Dunning.
1994.
Statistical identification of lan-guage.
Computing Research Laboratory Techni-cal Memo MCCS 94-273, New Mexico State Uni-versity, Las Cruces, New Mexico.W.
Gale and K. Church.
1990.
What's wrong withadding one?
IEEE transactions on Acoustics,Speech, and Signal Processing.I.J.
Good.
1953.
The population frequencies ofspecies and the estimation of population parame-ters.
Biometrika, 40(3 and 4):237-264.ISO.
1988a.
ISO3166 - codes for the representationof names of countries.ISO.
1988b.
ISO639 - code for the representation fnames of languages.F.
Yergeau, G. Nicol, G. Adams, and M. Duerst.1997.
RFC 2070: Internationalization f the hy-pertext markup language, January.46Lang IEnSpEnSpEnSpEnSpEnSpEnSpEnSpEnSpEnSpLength I N Correct Wrongt00 3 983100 3 973200 3 984200 3 981500 3 4985OO 3 5OO100 4 988100 4 976200 4 986200 4 983500 4 500500 4 500100 5 991100 5 973200 5 986200 5 9845OO 5 50O500 5 500% Correct17 98.37 99.316 98.419 98.12 99.60 t00.012 98.84 99.614 98.617 98.30 100.00 100.09 99.17 99.314 98.616 98.40 100.00 100.0Table 2: Language identification performance usingfull profilesLang Length N CorrectEn 100 3 986Sp 100 3 972En 200 3 979Sp 20O 3 981En 500 3 498Sp 5O0 3 5OOEn 100 4 989Sp 100 4 973En 200 4 983Sp 200 4 982En 500 4 499Sp 500 4 500En 100 5 989Sp 100 5 973En 200 5 982Sp 2OO 5 985En 500 5 500Sp 50O 5 500Wrong % Correct14 98.608 99.1821 97.9019 98.102 99.600 100.0011 98.907 99.2917 98.3018 98.201 99.800 100.0011 98:907 99.2918 98.2015 98.5O0 100.000 100.00Table 4: Language identification performance usingreduced profiles (filtered doubletons)Lang Length N CorrectEn 100 3 986Sp 100 3 973En 200 3 980Sp 200 ,3 981En 500 3 498Sp 500 i3 500En 100 ~4 989Sp 100 4 973En 200 4 982Sp 200 4 983En 500 4 500Sp 500 4 500En 100 5 990Sp 100 5 974En 200 5 984Sp 2O0 5 984En 500 5 500Sp 5OO 5 5OOWrong % Correct14720192011718170010616160098.6099.2998.0098.1099.60100.0098.9099.2998.2098.30100.00100.0099.0099.3998.4098.40100.00100.00Table 3: Language identification performance usingreduced profiles (filtered singletons)Lang Length N CorrectEn 100 3 984Sp 100 3 973En 200 3 979Sp 200 3 981En 500 3 499Sp 5OO 3 5OOEn 100 4 990Sp 100 4 972En 200 4 983Sp 200 4 981En 500 4 499Sp 5OO 4 50OEn 100 5 979Sp 100 5 961En 200 5 974Sp 2OO 5 985En 500 5 497Sp 500 5 496Table 5:reducedWrong % Correct16 98.407 99.2921 97.9019 98.101 99.800 100.0010 99.008 99.1817 98.3019 98.101 99.800 100.0021 97.9019 98.0626 97.4015 98.503 99.404 99.20Language identification performance usingprofiles (filtered tripletons)4748
