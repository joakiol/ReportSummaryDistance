Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 155?163,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsJointly Identifying Predicates, Arguments and Senses using Markov LogicIvan Meza-Ruiz?
Sebastian Riedel??
?School of Informatics, University of Edinburgh, UK?Department of Computer Science, University of Tokyo, Japan?Database Center for Life Science, Research Organization of Information and System, Japan?I.V.Meza-Ruiz@sms.ed.ac.uk ?
sebastian.riedel@gmail.comAbstractIn this paper we present a Markov Logic Net-work for Semantic Role Labelling that jointlyperforms predicate identification, frame dis-ambiguation, argument identification and ar-gument classification for all predicates in asentence.
Empirically we find that our ap-proach is competitive: our best model wouldappear on par with the best entry in theCoNLL 2008 shared task open track, and atthe 4th place of the closed track?right be-hind the systems that use significantly betterparsers to generate their input features.
More-over, we observe that by fully capturing thecomplete SRL pipeline in a single probabilis-tic model we can achieve significant improve-ments over more isolated systems, in particu-lar for out-of-domain data.
Finally, we showthat despite the joint approach, our system isstill efficient.1 IntroductionSemantic Role Labelling (SRL, Ma?rquez et al,2008) is generally understood as the task of iden-tifying and classifying the semantic arguments andmodifiers of the predicates mentioned in a sentence.For example, in the case of the following sentence:we are to find out that for the predicate token ?plays?with sense ?play a role?
(play.02) the phrase headedby the token ?Haag?
is referring to the player (A0)of the play event, and the phrase headed by the token?Elianti?
is referring to the role (A1) being played.SRL is considered as a key task for applications thatrequire to answer ?Who?, ?What?, ?Where?, etc.questions, such as Information Extraction, QuestionAnswering and Summarization.Any real-world SRL system needs to make sev-eral decisions, either explicitly or implicitly: whichare the predicate tokens of a sentence (predicateidentification), which are the tokens that have se-mantic roles with respect to these predicates (argu-ment identification), which are the roles these to-kens play (argument classification), and which is thesense of the predicate (sense disambiguation).In this paper we use Markov Logic (ML), a Statis-tical Relational Learning framework that combinesFirst Order Logic and Markov Networks, to developa joint probabilistic model over all decisions men-tioned above.
The following paragraphs will moti-vate this choice.First, it allows us to readily capture global cor-relations between decisions, such as the constraintthat a predicate can only have one agent.
This typeof correlations has been successfully exploited inseveral previous SRL approaches (Toutanova et al,2005; Punyakanok et al, 2005).Second, we can use the joint model to evaluatethe benefit of incorporating decisions into the jointmodel that either have not received much attentionwithin the SRL community (predicate identificationand sense disambiguation), or been largely made inisolation (argument identification and classificationfor all predicates of a sentence).Third, our ML model is essentially a template thatdescribes a class of Markov Networks.
Algorithmscan perform inference in terms of this template with-155out ever having to fully instantiate the completeMarkov Network (Riedel, 2008; Singla and Domin-gos, 2008).
This can dramatically improve the effi-ciency of an SRL system when compared to a propo-sitional approach such as Integer Linear Program-ming (ILP).Finally, when it comes to actually building anSRL system with ML there are ?only?
four thingsto do: preparing input data files, converting out-put data files, and triggering learning and inference.The remaining work can be done by an off-the-shelf Markov Logic interpreter.
This is to be con-trasted with pipeline systems where several compo-nents need to be trained and connected, or IntegerLinear Programming approaches for which we needto write additional wrapper code to generate ILPs.Empirically we find that our system iscompetitive?our best model would appear onpar with the best entry in the CoNLL 2008 sharedtask open track, and at the 4th place of the closedtrack?right behind systems that use significantlybetter parsers1 to generate their input features.We also observe that by integrating frame disam-biguation into the joint SRL model, and by extract-ing all arguments for all predicates in a sentencesimultaneously, significant improvements comparedto more isolated systems can be achieved.
Theseimprovements are particularly large in the case ofout-of-domain data, suggesting that a joint approachhelps to increase the robustness of SRL.
Finally, weshow that despite the joint approach, our system isstill efficient.Our paper is organised as follows: we first intro-duce ML (section 2), then we present our model interms of ML (section 3) and illustrate how to per-form learning and inference with it (section 4).
Howthis model will be evaluated is explained in section 5with the corresponding evaluation presented in sec-tion 6.
We conclude in section 7.2 Markov LogicMarkov Logic (ML, Richardson and Domingos,2005) is a Statistical Relational Learning languagebased on First Order Logic and Markov Networks.It can be seen as a formalism that extends First Or-der Logic to allow formulae that can be violated with1Our unlabelled accuracy for syntactic dependencies is atleast 3% points under theirs.some penalty.
From an alternative point of view, it isan expressive template language that uses First Or-der Logic formulae to instantiate Markov Networksof repetitive structure.Let us describe ML by considering the predicateidentification task.
In ML we can model this task byfirst introducing a set of logical predicates2 such asisPredicate(Token) or word(Token,Word).
Then wespecify a set of weighted first order formulae thatdefine a distribution over sets of ground atoms ofthese predicates (or so-called possible worlds).Ideally, the distribution we define with theseweighted formulae assigns high probability to possi-ble worlds where SRL predicates are correctly iden-tified and a low probability to worlds where this isnot the case.
For example, a suitable set of weightedformulae would assign a high probability to theworld3{word (1,Haag) , word(2, plays),word(3,Elianti), isPredicate(2)}and a low one to{word (1,Haag) , word(2, plays),word(3,Elianti), isPredicate(3)}In Markov Logic a set of weighted formulae is calleda Markov Logic Network (MLN).
Formally speak-ing, an MLN M is a set of pairs (?,w) where ?
is afirst order formula and w a real weight.
M assignsthe probabilityp (y) = 1Z exp??
?
(?,w)?Mw?c?C?f?c (y)??
(1)to the possible world y.
Here C?
is the set of allpossible bindings of the free variables in ?
with theconstants of our domain.
f?c is a feature functionthat returns 1 if in the possible world y the groundformula we get by replacing the free variables in ?by the constants in c is true and 0 otherwise.
Zis a normalisation constant.
Note that this distri-bution corresponds to a Markov Network (the so-called Ground Markov Network) where nodes repre-sent ground atoms and factors represent ground for-mulae.2In the cases were is not obvious whether we refer to SRLor ML predicates we add the prefix SRL or ML, respectively.3?Haag plays Elianti?
is a segment of a sentence in the train-ing corpus.156For example, if M contains the formula ?word (x, take) ?
isPredicate (x)then its corresponding log-linear model has, amongothers, a feature f?t1 for which x in ?
has been re-placed by the constant t1 and that returns 1 ifword (1, take) ?
isPredicate (1)is true in y and 0 otherwise.We will refer predicates such as word as observedbecause they are known in advance.
In contrast, is-Predicate is hidden because we need to infer it at testtime.3 ModelConceptually we divide our SRL system into threestages: one stage that identifies the predicates ofa sentence, one stage that identifies and classifiesthe arguments of these predicates, and a final stagethat predicts the sense of each predicate.
We shouldstress that this architecture is intended to illustratea typical SRL system, and to describe the pipeline-based approach we will compare our models to.However, it does not correspond to the way in-ference is performed in our proposed model?wejointly infer all decisions described above.Note that while the proposed division into con-ceptual stages seems somewhat intuitive, it is by nomeans uncontroversial.
In fact, for the CoNLL 2008shared task slightly more than one half of the par-ticipants performed sense disambiguation before ar-gument identification and classification; most otherparticipants framed the problem in the reverse or-der.4We define five hidden predicates for the threestages of the task.
Figure 1 illustrates these pred-icates and the stage they belong to.
For predicateidentification, we use the predicate isPredicate.
is-Predicate(p) indicates that the word in the positionp is an SRL predicate.
For argument identifica-tion and classification, we use the predicates isAr-gument, hasRole and role.
The atom isArgument(a)signals that the word in the position a is a SRL ar-gument of some (unspecified) SRL predicate whilehasRole(p,a) indicates that the token at position a is4However, for almost all pipeline based systems, predicateidentification was the first stage of the role labelling process.isPredicatesenseisArgumenthasRolerolePredicateIdentificationArgumentIdentification &clasificationSenseDisambiguationBottom-upTop-DownPipeline directionFigure 1: MLN hidden predicates divided in stagesan argument of the predicate in position p. The pred-icate role(p,a,r) corresponds to the decision that theargument at position a has the role r with respect tothe predicate in position p. Finally, for sense disam-biguation we define the predicate sense(p,e) whichsignals that the predicate in position p has the sensee.Before we continue to describe the formulae ofour Markov Logic Network we would like to high-light the introduction of the isArgument predicatementioned above.
This predicate corresponds to adecision that is usually made implicitly: a token isan argument if there exists a predicate for which itplays a semantic role.
Here we model this decisionexplicitly, assuming that there exist cases where atoken clearly has to be an argument of some pred-icate, regardless of which predicate in the sentencethis might be.
It is this assumption that requires us toinfer the arguments for all predicates of a sentenceat once?otherwise we cannot make sure that for amarked argument there exists at least one predicatefor which the argument plays a semantic role.In addition to the hidden predicates, we defineobservable predicates to represent the informationavailable in the corpus.
Table 1 presents these pred-icates.3.1 Local formulaeA formula is local if its groundings relate any num-ber of observed ground atoms to exactly one hiddenground atom.
For example, two groundings of thelocal formulalemma(p,+l1)?lemma(a,+l2) ?
hasRole(p, a)can be seen in the Factor Graph of Figure 2.
Bothconnect a single hidden hasRole ground atom with157word(i,w) Token i has word wlemma(i,l) Token i has lemma lppos(i,p) Token i has POS tag pcpos(i,p) Token i has coarse POS tag pvoice(i,v) Token i is verb and has voice v(Active/Passive).subcat(i,f) Token i has subcategorizationframe fdep(i,j,d) Token h is head of token m andhas dependency label dpalmer(i,j) Token j can be semantic argu-ment for token i according tohigh recall heuristic?depPath(i,j,p) Dependency path between to-kens i and j is p?depFrame(i,j,f) f is a syntactic (dependency)frame in which tokens i and jare designated as ?pivots?
?Table 1: Observable predicates; predicates marked with?
are dependency parsing-based versions for features ofXue and Palmer (2004).two observed lemma ground atoms.
The + notationindicates that the MLN contains one instance of therule, with a separate weight, for each assignment ofthe variables with a plus sign (?
).The local formulae for isPredicate, isArgumentand sense aim to capture the relation of the tokenswith their lexical and syntactic surroundings.
Thisincludes formulae such assubcat(p,+f) ?
isPredicate(p)which implies that a certain token is a predicatewith a weight that depends on the subcategorizationframe of the token.
Further local formulae are con-structed using those observed predicates in table 1that relate single tokens and their properties.The local formulae for role and hasRole focus onproperties of the predicate and argument token?theformula illustrated in figure 2 is an example of this?and on the relation between the two tokens.
An ex-ample of the latter type is the formuladepPath(p, a,+d) ?
role(p, a,+r)which implies that token a plays the semantic role rwith respect to token p, and for which the weight de-pends on the syntactic (dependency) path d betweenp and a and on the actual role to assign.
Again,further formulae are constructed using the observedFigure 2: Factor graph for the first local formula in sec-tion 3.1.
Here round nodes represent variables (corre-sponding to the states of ground atoms) and the rectan-gular nodes represent the factor and their parameters at-tached to the ground formulae.predicates in table 1; however, this time we considerboth predicates that relate tokens to their individualproperties and predicates that describe the relationbetween tokens.Unfortunately, the complete set of local formulaeis too large to be exhaustively described in this pa-per.
Its size results from the fact that we also con-sider conjunctions of several atoms as conditions,and lexical windows around tokens.
Hence, insteadof describing all local formulae we refer the readerto our MLN model files.5 They can be used both asa reference and as input to our Markov Logic En-gine,6 and thus allow the reader to easily reproduceour results.3.2 Global formulaeGlobal formulae relate several hidden ground atoms.We use this type of formula for two purposes: toensure consistency between the predicates of allSRL stages, and to capture some of our backgroundknowledge about SRL.
We will refer to formulaethat serve the first purpose as structural constraints.For example, a structural constraint is given by the(deterministic) formularole(p, a, r) ?
hasRole(p, a)which ensures that, whenever the argument a isgiven a label r with respect to the predicate p, thisargument must be an argument of a as denoted byhasRole(p,a).
Note that this formula by itself modelsthe traditional ?bottom-up?
argument identificationand classification pipeline (Xue and Palmer, 2004):5http://code.google.com/p/thebeast/source/browse/#svn/mlns/naacl-hlt6http://code.google.com/p/thebeast158it is possible to not assign a role r to an predicate-argument pair (p, a) proposed by the identificationstage; however, it is impossible to assign a role rto token pairs (p, a) that have not been proposed aspotential arguments.An example of another class of structural con-straints ishasRole(p, a) ?
?r.role(p, a, r)which, by itself, models an inverted or ?top-down?pipeline.
In this architecture the argument classifi-cation stage can assign roles to tokens that have notbeen proposed by the argument identification stage.However, it must assign a label to any token pair theprevious stage proposes.For the SRL predicates that perform a labellingtask (role and sense) we also need a structural con-straint which ensures that not more than one label isassigned.
For instance,(role(p, a, r1) ?
r1 6= r2 ?
?role(p, a, r2))forbids two different semantic roles for a pair ofwords.There are three global formulae that capture ourlinguistic background knowledge.
The first one isa deterministic constraint that had been frequentlyapplied in the SRL literature.
It forbids cases wheredistinct arguments of a predicate have the same roleunless the role describes a modifier:role (p, a1, r) ?
?mod (r) ?
a1 6= a2 ?
?role (p, a2, r)The second ?linguistic?
global formula isrole(p, a,+r) ?
lemma(p,+l) ?
sense(p,+s)which implies that when a predicate p with lemma lhas an argument awith role r it has to have the senses.
Here the weight depends on the combination ofrole r, lemma l and sense s.The third and final ?linguistic?
global formula islemma(p,+l) ?
ppos(a,+p)?hasRole(p, a) ?
sense(p,+f)It implies that if a predicate p has the lemma l and anargument a with POS tag p it has to have the senses.
This time the weight depends on the combinationof POS tag p, lemma l and sense s.Note that the final two formulae evaluate the se-mantic frame of a predicate and become local for-mulae in a pipeline system that performs sense dis-ambiguation after argument identification and clas-sification.Table 2 summarises the global formulae we use inthis work.4 Inference and LearningAssuming that we have an MLN, a set of weightsand a given sentence then we need to predict thechoice of predicates, frame types, arguments androle labels with maximal a posteriori probabil-ity (MAP).
To this end we apply a method thatis both exact and efficient: Cutting Plane Infer-ence (CPI, Riedel, 2008) with Integer Linear Pro-gramming (ILP) as base solver.Instead of fully instantiating the Markov Networkthat a Markov Logic Network describes, CPI beginswith a subset of factors/edges?in our case we usethe factors that correspond to the local formulae ofour model?and solves the MAP problem for thissubset using the base solver.
It then inspects thesolution for ground formulae/features that are notyet included but could, if added, lead to a differentsolution?this process is usually referred to as sep-aration.
The ground formulae that we have foundare added and the network is solved again.
This pro-cess is repeated until the network does not changeanymore.This type of algorithm could also be realised foran ILP formulation of SRL.
However, it would re-quire us to write a dedicated separation routine foreach type of constraint we want to add.
In MarkovLogic, on the other hand, separation can be gener-ically implemented as the search for variable bind-ings that render a weighted first order formulae true(if its weight is negative) or false (if its weight ispositive).
In practise this means that we can try newglobal formulae/constraints without any additionalimplementation overhead.We learn the weights associated with each MLNusing 1-best MIRA (Crammer and Singer, 2003)Online Learning method.
As MAP inferencemethod that is applied in the inner loop of the on-line learner we apply CPI, again with ILP as base159Bottom-upsense(p, s) ?
isPredicate(p)hasRole(p, a) ?
isPredicate(p)hasRole(p, a) ?
isArgument(a)role(p, a, r) ?
hasLabel(p, a)Top-DownisPredicate(p) ?
?s.sense(p, s)isPredicate(p) ?
?a.hasRole(p, a)isArgument(a) ?
?p.hasRole(p, a)hasLabel(p, a) ?
?r.role(p, a, r)Unique Labels role(p, a, r1) ?
r1 6= r2 ?
?role(p, a, r2)sense(p, s1) ?
s1 6= s2 ?
?sense(p, r2)Linguisticrole (p, a1, r) ?
?mod (r) ?
a1 6= a2 ?
?role (p, a2, r)lemma(p,+l) ?
ppos(a,+p) ?
hasRole(p, a) ?
sense(p,+f)lemma(p,+l) ?
role(p, a,+r) ?
sense(p,+f)Table 2: Global formulae for ML modelsolver.5 Experimental SetupFor training and testing our SRL systems we used aversion of the CoNLL 2008 shared task (Surdeanuet al, 2008) dataset that only mentions verbal predi-cates, disregarding the nominal predicates availablein the original corpus.7 While the original (opentrack) corpus came with MALT (Nivre et al, 2007)dependencies, we observed slightly better resultswhen using the dependency parses generated witha Charniak parser (Charniak, 2000).
Hence we usedthe latter for all our experiments.To assess the performance of our model, and it toevaluate the possible gains to be made from consid-ering a joint model of the complete SRL pipeline,we set up several systems.
The full system uses aMarkov Logic Network with all local and global for-mulae described in section 3.
For the bottom-up sys-tem we removed the structural top-down constraintsfrom the complete model?previous work Riedeland Meza-Ruiz (2008) has shown that this can leadto improved performance.
The bottom-up (-arg) sys-tem is equivalent to the bottom-up system, but itdoes not include any formulae that mention the hid-den isArgument predicate.For the systems presented so far we perform jointinference and learning.
The pipeline system dif-fers in this regard.
For this system we train a sep-arate model for each stage in the pipeline of figure1.
The predicate identification stage identifies thepredicates (using all local isPredicate formulae) of7The reason for this choice where license problems.a sentence.
The next stage predicts arguments andtheir roles for the identified predicates.
Here we in-clude all local and global formulae that involve onlythe predicates of this stage.
In the last stage we pre-dict the sense of each identified predicate using allformulae that involve the sense, without the struc-tural constraints that connect the sense predicate tothe previous stages of the pipeline (these constraintsare enforced by architecture).6 ResultsTable 3 shows the results of our systems for theCoNLL 2008 development set and the WSJ andbrown test sets.
The scores are calculated using thesemantic evaluation metric of the CoNLL-08 sharedtask (Surdeanu et al, 2008).
This metric measuresthe precision, recall and F1 score of the recoveredsemantic dependencies.
A semantic dependency iscreated for each predicate and its arguments, thelabel of such dependency is the role of the argu-ment.
Additionally, there is a semantic dependencyfor each predicate and aROOT argument which hasthe sense of the predicate as label.To put these results into context, let us comparethem to those of the participants of the CoNLL 2008shared task (see the last three rows of table 3).8 Ourbest model, Bottom-up, would reach the highest F1WSJ score, and second highest Brown score, forthe open track.
Here the best-performing participantwas Vickrey and Koller (2008).Table 3 also shows the results of the best (Jo-hansson and Nugues, 2008) and fourth best sys-8Results of other systems were extracted from Table 16 ofthe shared task overview paper (Surdeanu et al, 2008).160tem (Zhao and Kit, 2008) of the closed track.
Wenote that we do significantly worse than Johanssonand Nugues (2008), and roughly equivalent to Zhaoand Kit (2008); this places us on the fourth rank of19 participants.
However, note that all three sys-tems above us, as well as Zhao and Kit (2008), useparsers with at least about 90% (unlabelled) accu-racy on the WSJ test set (Johansson?s parser hasabout 92% unlabelled accuracy).9 By contrast, withabout 87% unlabelled accuracy our parses are sig-nificantly worse.Finally, akin to Riedel and Meza-Ruiz (2008) weobserve that the bottom-up joint model performsbetter than the full joint model.System Devel WSJ BrownFull 76.93 79.09 67.64Bottom-up 77.96 80.16 68.02Bottom-up (-arg) 77.57 79.37 66.70Pipeline 75.69 78.19 64.66Vickrey N/A 79.75 69.57Johansson N/A 86.37 71.87Zhao N/A 79.40 66.38Table 3: Semantic F1 scores for our systems and threeCoNLL 2008 shared task participants.
The Bottom-upresults are statistically significantly different to all others(i.e., ?
?
0.05 according to the sign test).6.1 Joint Model vs. PipelineTable 3 suggests that by including sense disam-biguation into the joint model (as is the case for allsystems but the pipeline) significant improvementscan be gained.
Where do these improvements comefrom?
We tried to answer this question by taking acloser look at how accurately the pipeline predictsthe isPredicate, isArgument, hasRole, role andsense relations, and how this compares to the resultof the joint full model.Table 4 shows that the joint model mainly doesbetter when it comes to predicting the right predi-cate senses.
This is particularly true for the case ofthe Brown corpus?here we gain about 10% points.These results suggest that a more joint approach maybe particularly useful in order to increase the robust-ness of an SRL system in out-of-domain scenarios.109Since our parses use a different label set we could not com-WSJ BrownPipe.
Fu.
Pipe.
Fu.isPredicate 96.6 96.5 92.2 92.5isArgument 90.3 90.6 85.9 86.9hasRole 88.0 87.9 83.6 83.8role 75.4 75.5 64.2 64.6sense 85.5 88.5 67.3 77.1Table 4: F1 scores for M predicates; Pipe.
refers to thePipeline system, Fu.
to the full system.6.2 Modelling if a Token is an ArgumentIn table 3 we also observe that improvements can bemade if we explicitly model the decision whether atoken is a semantic argument of some predicate ornot.
As we mentioned in section 3, this aspect of ourmodel requires us to jointly perform inference forall predicates of a sentence, and hence our resultsjustify the per-sentence SRL approach proposed inthis paper.In order to analyse where these improvementscome from, we again list our results on a per-SRL-predicate basis.
Table 5 shows that by including theisArgument predicate and the corresponding for-mulae we gain around 0.6% and 1.0% points acrossthe board for WSJ and Brown, respectively.11 Asshown in table 3, these improvements result in about1.0% improvements for both WSJ and Brown interms of the CoNLL 2008 metric.
Hence, an ex-plicit model of the ?is an argument?
decision helpsthe SRL at all levels.How the isArgument helps to improve the over-all role labelling score can be illustrated with theexample in figure 3.
Here the model without ahidden isArgument predicate fails to attach thepreposition ?on?
to the predicate ?start.01?
(here 01refers to the sense of the predicate).
Apparentlythe model has not enough confidence to assign thepreposition to either ?start.01?
or ?get.03?, so it justdrops the argument altogether.
However, becausethe isArgument model knows that most preposi-tions have to be modifying some predicate, pres-pare labelled accuracy.10The differences between results of the full and joint modelare statistically significant with the exception of the results forthe isPredicate predicate for the WSJ test set.11The differences between results of the w/ and w/o modelare statistically significant with the exception of the results forthe sense predicate for the Brown test set.161Figure 3: Segment of the CoNLL 2008 development setfor which the bottom-up model w/o isArgument predi-cate fails to attach the preposition ?on?
as an ?AM-LOC?for ?started?.
The joint bottom-up model attaches thepreposition correctly.sure is created that forces a decision between thetwo predicates.
And because for the role model?start.01?
looks like a better fit than ?get.03?, thecorrect attachment is found.WSJ Brownw/o w/ w/o w/isPredicate 96.3 96.5 91.4 92.5hasRole 87.1 87.7 82.5 83.6role 76.9 77.5 65.2 66.2sense 88.3 89.0 76.1 77.5Table 5: F1 scores for ML predicates; w/o refers toa Bottom-up system without isArgument predicate, w/refers to a Bottom-up system with isArgument predicate.6.3 EfficiencyIn the previous sections we have shown that our jointmodel indeed does better than an equivalent pipelinesystem.
However, usually most joint approachescome at a price: efficiency.
Interestingly, in our casewe observe the opposite: our joint model is actuallyfaster than the pipeline.
This can be seen in table 6,where we list the time it took for several differentsystem to process the WSJ and Brown test corpus,respectively.
When we compare the times for thebottom-up model to those of the pipeline, we notethat the joint model is twice as fast.
While the indi-vidual stages within the pipeline may be faster thanthe joint system (even when we sum up inferencetimes), extracting results from one system and feed-ing them into another creates overhead which offsetsthis potential reduction.Table 6 also lists the run-time of a bottom-upsystem that solves the inference problem by fullygrounding the Markov Network that the MarkovLogic (ML) model describes, mapping this networkto an Integer Linear Program, and finding the mostlikely assignment using an ILP solver.
This sys-tem (Bottom-up (-CPI)) is four times slower than theequivalent system that uses Cutting Plane Inference(Bottom-up).
This suggests that if we were to imple-ment the same joint model using ILP instead of ML,our system would either be significantly slower, orwe would need to implement a Cutting Plane algo-rithm for the corresponding ILP formulation?whenwe use ML this algorithm comes ?for free?.System WSJ BrownFull 9.2m 1.5mFull (-CPI) 38.4m 7.47mBottom-up 9.5m 1.6mBottom-up (-CPI) 38.8m 6.9mPipeline 18.9m 2.9mTable 6: Testing times for full model and bottom-up whenCPI algorithm is not used.
TheWSJ test set contains 2414sentences, the Brown test set 426.
Our best systems thustakes on average 230ms per WSJ sentence (on a 2.4Ghzsystem).7 ConclusionIn this paper we have presented aMarkov Logic Net-work that jointly models all predicate identification,argument identification and classification and sensedisambiguation decisions for a sentence.
We haveshown that this approach is competitive, in particularif we consider that our input parses are significantlyworse than those of the top CoNLL 2008 systems.We demonstrated the benefit of jointly predictingsenses and semantic arguments when compared to apipeline system that first picks arguments and thensenses.
We also showed that by modelling whethera token is an argument of some predicate and jointlypicking arguments for all predicates of a sentence,further improvements can be achieved.Finally, we demonstrated that our system is effi-cient, despite following a global approach.
This ef-ficiency was also shown to stem from the first orderinference method our Markov Logic engine applies.AcknowledgementsThe authors are grateful to Mihai Surdeanu for pro-viding the version of the corpus used in this work.162ReferencesEugene Charniak.
A maximum-entropy-inspiredparser.
In Proceedings of NAACL-2000, 2000.Koby Crammer and Yoram Singer.
Ultraconserva-tive online algorithms for multiclass problems.Journal of Machine Learning Research, 3:951?991, 2003.Richard Johansson and Pierre Nugues.
Dependency-based semantic role labeling of propbank.
In Pro-ceedings of EMNLP-2008., 2008.Llu?
?s Ma?rquez, Xavier Carreras, Ken Litkowski, andSuzanne Stevenson.
Semantic role labeling.
Com-putational Linguistics, 34(2), 2008.
Introductionto the Special Issue on Semantic Role Labeling.J.
Nivre, J.
Hall, J. Nilsson, A. Chanev, G. Eryigit,S.
Kuebler, S. Marinov, and E. Marsi.
Malt-Parser: A language-independent system for data-driven dependency parsing.
Natural LanguageEngineering, 13(02):95?135, 2007.V.
Punyakanok, D. Roth, and W. Yih.
General-ized inference with multiple semantic role label-ing systems.
In Ido Dagan and Dan Gildea, ed-itors, CoNLL ?05: Proceedings of the AnnualConference on Computational Natural LanguageLearning, pages 181?184, 2005.Matthew Richardson and Pedro Domingos.
Markovlogic networks.
Technical report, University ofWashington, 2005.Sebastian Riedel.
Improving the accuracy and ef-ficiency of map inference for markov logic.
InUAI ?08: Proceedings of the Annual Conferenceon Uncertainty in AI, 2008.Sebastian Riedel and Ivan Meza-Ruiz.
Collectivesemantic role labelling with markov logic.
InConference on Computational Natural LanguageLearning, 2008.P.
Singla and P. Domingos.
Lifted First-Order BeliefPropagation.
Association for the Advancement ofArtificial Intelligence (AAAI), 2008.Mihai Surdeanu, Richard Johansson, Adam Meyers,Llu?
?s Ma`rquez, and Joakim Nivre.
The CoNLL-2008 shared task on joint parsing of syntactic andsemantic dependencies.
In Proceedings of the12th Conference on Computational Natural Lan-guage Learning (CoNLL-2008), 2008.Kristina Toutanova, Aria Haghighi, and Christo-pher D. Manning.
Joint learning improves seman-tic role labeling.
In ACL ?05: Proceedings of the43rd Annual Meeting on Association for Compu-tational Linguistics, Morristown, NJ, USA, 2005.David Vickrey and Daphne Koller.
Applying sen-tence simplification to the conll-2008 shared task.In Proceedings of CoNLL-2008., 2008.Nianwen Xue and Martha Palmer.
Calibrating fea-tures for semantic role labeling.
In EMNLP ?04:Proceedings of the Annual Conference on Em-pirical Methods in Natural Language Processing,2004.Hai Zhao and Chunyu Kit.
Parsing syntactic and se-mantic dependencies with two single-stage max-imum entropy models.
In CoNLL 2008: Pro-ceedings of the Twelfth Conference on Computa-tional Natural Language Learning, Manchester,England, 2008.163
