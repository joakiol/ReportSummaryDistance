Learning the Countability of English Nouns from Corpus DataTimothy BaldwinCSLIStanford UniversityStanford, CA, 94305tbaldwin@csli.stanford.eduFrancis BondNTT Communication Science LaboratoriesNippon Telegraph and Telephone CorporationKyoto, Japanbond@cslab.kecl.ntt.co.jpAbstractThis paper describes a method for learn-ing the countability preferences of Englishnouns from raw text corpora.
The methodmaps the corpus-attested lexico-syntacticproperties of each noun onto a featurevector, and uses a suite of memory-basedclassifiers to predict membership in 4countability classes.
We were able to as-sign countability to English nouns with aprecision of 94.6%.1 IntroductionThis paper is concerned with the task of knowledge-rich lexical acquisition from unannotated corpora,focusing on the case of countability in English.Knowledge-rich lexical acquisition takes unstruc-tured text and extracts out linguistically-precise cat-egorisations of word and expression types.
Bycombining this with a grammar, we can buildbroad-coverage deep-processing tools with a min-imum of human effort.
This research is closein spirit to the work of Light (1996) on classi-fying the semantics of derivational affixes, andSiegel and McKeown (2000) on learning verb as-pect.In English, nouns heading noun phrases are typ-ically either countable or uncountable (also calledcount and mass).
Countable nouns can be modi-fied by denumerators, prototypically numbers, andhave a morphologically marked plural form: onedog, two dogs.
Uncountable nouns cannot be modi-fied by denumerators, but can be modified by unspe-cific quantifiers such as much, and do not show anynumber distinction (prototypically being singular):*one equipment, some equipment, *two equipments.Many nouns can be used in countable or uncountableenvironments, with differences in interpretation.We call the lexical property that determines whichuses a noun can have the noun?s countability prefer-ence.
Knowledge of countability preferences is im-portant both for the analysis and generation of En-glish.
In analysis, it helps to constrain the inter-pretations of parses.
In generation, the countabil-ity preference determines whether a noun can be-come plural, and the range of possible determin-ers.
Knowledge of countability is particularly im-portant in machine translation, because the closesttranslation equivalent may have different countabil-ity from the source noun.
Many languages, suchas Chinese and Japanese, do not mark countability,which means that the choice of countability will belargely the responsibility of the generation compo-nent (Bond, 2001).
In addition, knowledge of count-ability obtained from examples of use is an impor-tant resource for dictionary construction.In this paper, we learn the countability prefer-ences of English nouns from unannotated corpora.We first annotate them automatically, and then trainclassifiers using a set of gold standard data, takenfrom COMLEX (Grishman et al, 1998) and the trans-fer dictionaries used by the machine translation sys-tem ALT-J/E (Ikehara et al, 1991).
The classifiersand their training are described in more detail inBaldwin and Bond (2003).
These are then run overthe corpus to extract nouns as members of fourclasses ?
countable: dog; uncountable: furniture; bi-partite: [pair of] scissors and plural only: clothes.We first discuss countability in more detail (?
2).Then we present the lexical resources used in our ex-periment (?
3).
Next, we describe the learning pro-cess (?
4).
We then present our results and evalu-ation (?
5).
Finally, we discuss the theoretical andpractical implications (?
6).2 BackgroundGrammatical countability is motivated by the se-mantic distinction between object and substancereference (also known as bounded/non-bounded orindividuated/non-individuated).
It is a subject ofcontention among linguists as to how far grammat-ical countability is semantically motivated and howmuch it is arbitrary (Wierzbicka, 1988).The prevailing position in the natural languageprocessing community is effectively to treat count-ability as though it were arbitrary and encode it asa lexical property of nouns.
The study of countabil-ity is complicated by the fact that most nouns canhave their countability changed: either converted bya lexical rule or embedded in another noun phrase.An example of conversion is the so-called universalpackager, a rule which takes an uncountable nounwith an interpretation as a substance, and returns acountable noun interpreted as a portion of the sub-stance: I would like two beers.
An example of em-bedding is the use of a classifier, e.g.
uncountablenouns can be embedded in countable noun phrasesas complements of classifiers: one piece of equip-ment.Bond et al (1994) suggested a division of count-ability into five major types, based on Allan (1980)?snoun countability preferences (NCPs).
Nouns whichrarely undergo conversion are marked as either fullycountable, uncountable or plural only.
Fully countablenouns have both singular and plural forms, and can-not be used with determiners such as much, little, alittle, less and overmuch.
Uncountable nouns, suchas furniture, have no plural form, and can be usedwith much.
Plural only nouns never head a singularnoun phrase: goods, scissors.Nouns that are readily converted are marked as ei-ther strongly countable (for countable nouns that canbe converted to uncountable, such as cake) or weaklycountable (for uncountable nouns that are readilyconvertible to countable, such as beer).NLP systems must list countability for at leastsome nouns, because full knowledge of the refer-ent of a noun phrase is not enough to predict count-ability.
There is also a language-specific knowl-edge requirement.
This can be shown most sim-ply by comparing languages: different languages en-code the countability of the same referent in dif-ferent ways.
There is nothing about the conceptdenoted by lightning, e.g., that rules out *a light-ning being interpreted as a flash of lightning.
In-deed, the German and French translation equivalentsare fully countable (ein Blitz and un e?clair respec-tively).
Even within the same language, the samereferent can be encoded countably or uncountably:clothes/clothing, things/stuff , jobs/work.Therefore, we must learn countability classesfrom usage examples in corpora.
There are severalimpediments to this approach.
The first is that wordsare frequently converted to different countabilities,sometimes in such a way that other native speak-ers will dispute the validity of the new usage.
Wedo not necessarily wish to learn such rare examples,and may not need to learn more common conver-sions either, as they can be handled by regular lexi-cal rules (Copestake and Briscoe, 1995).
The secondproblem is that some constructions affect the appar-ent countability of their head: for example, nounsdenoting a role, which are typically countable, canappear without an article in some constructions (e.g.We elected him treasurer).
The third is that differentsenses of a word may have different countabilities:interest ?a sense of concern with and curiosity?
isnormally countable, whereas interest ?fixed chargefor borrowing money?
is uncountable.There have been at several earlier approachesto the automatic determination of countabil-ity.
Bond and Vatikiotis-Bateson (2002) determinea noun?s countability preferences from its seman-tic class, and show that semantics predicts (5-way)countability 78% of the time with their ontology.O?Hara et al (2003) get better results (89.5%) usingthe much larger Cyc ontology, although they onlydistinguish between countable and uncountable.Schwartz (2002) created an automatic countabil-ity tagger (ACT) to learn noun countabilities fromthe British National Corpus.
ACT looks at deter-miner co-occurrence in singular noun chunks, andclassifies the noun if and only if it occurs with a de-terminer which can modify only countable or un-countable nouns.
The method has a coverage ofaround 50%, and agrees with COMLEX for 68% ofthe nouns marked countable and with the ALT-J/Elexicon for 88%.
Agreement was worse for uncount-able nouns (6% and 44% respectively).3 ResourcesInformation about noun countability was obtainedfrom two sources.
One was COMLEX 3.0 (Grish-man et al, 1998), which has around 22,000 nounentries.
Of these, 12,922 are marked as being count-able (COUNTABLE) and 4,976 as being uncountable(NCOLLECTIVE or :PLURAL *NONE*).
The remainderare unmarked for countability.The other was the common noun part of ALT-J/E?s Japanese-to-English semantic transfer dictio-nary (Bond, 2001).
It contains 71,833 linkedJapanese-English pairs, each of which has a valuefor the noun countability preference of the Englishnoun.
Considering only unique English entries withdifferent countability and ignoring all other informa-tion gave 56,245 entries.
Nouns in the ALT-J/E dic-tionary are marked with one of the five major count-ability preference classes described in Section 2.
Inaddition to countability, default values for numberand classifier (e.g.
blade for grass: blade of grass)are also part of the lexicon.We classify words into four possible classes, withsome words belonging to multiple classes.
The firstclass is countable: COMLEX?s COUNTABLE and ALT-J/E?s fully, strongly and weakly countable.
The sec-ond class is uncountable: COMLEX?s NCOLLECTIVE or:PLURAL *NONE* and ALT-J/E?s strongly and weaklycountable and uncountable.The third class is bipartite nouns.
These can onlybe plural when they head a noun phrase (trousers),but singular when used as a modifier (trouser leg).When they are denumerated they use pair: a pair ofscissors.
COMLEX does not have a feature to markbipartite nouns; trouser, for example, is listed ascountable.
Nouns in ALT-J/E marked plural only witha default classifier of pair are classified as bipartite.The last class is plural only nouns: those that onlyhave a plural form, such as goods.
They can nei-ther be denumerated nor modified by much.
Manyof these nouns, such as clothes, use the plural formeven as modifiers (a clothes horse).
The wordclothes cannot be denumerated at all.
Nouns marked:SINGULAR *NONE* in COMLEX and nouns in ALT-J/E marked plural only without the default classifierpair are classified as plural only.
There was somenoise in the ALT-J/E data, so this class was hand-checked, giving a total of 104 entries; 84 of thesewere attested in the training data.Our classification of countability is a subset ofALT-J/E?s, in that we use only the three basic ALT-J/E classes of countable, uncountable and plural only,(although we treat bipartite as a separate class, not asubclass).
As we derive our countability classifica-tions from corpus evidence, it is possible to recon-struct countability preferences (i.e.
fully, strongly, orweakly countable) from the relative token occurrenceof the different countabilities for that noun.In order to get an idea of the intrinsic difficulty ofthe countability learning task, we tested the agree-ment between the two resources in the form of clas-sification accuracy.
That is, we calculate the averageproportion of (both positive and negative) countabil-ity classifications over which the two methods agree.E.g., COMLEX lists tomato as being only countablewhere ALT-J/E lists it as being both countable and un-countable.
Agreement for this one noun, therefore, is34 , as there is agreement for the classes of countable,plural only and bipartite (with implicit agreement asto negative membership for the latter two classes),but not for uncountable.
Averaging over the total setof nouns countability-classified in both lexicons, themean was 93.8%.
Almost half of the disagreementscame from words with two countabilities in ALT-J/Ebut only one in COMLEX.4 Learning CountabilityThe basic methodology employed in this research isto identify lexical and/or constructional features as-sociated with the countability classes, and determinethe relative corpus occurrence of those features foreach noun.
We then feed the noun feature vectorsinto a classifier and make a judgement on the mem-bership of the given noun in each countability class.In order to extract the feature values from corpusdata, we need the basic phrase structure, and partic-ularly noun phrase structure, of the source text.
Weuse three different sources for this phrase structure:part-of-speech tagged data, chunked data and fully-parsed data, as detailed below.The corpus of choice throughout this paper is thewritten component of the British National Corpus(BNC version 2, Burnard (2000)), totalling around90m w-units (POS-tagged items).
We chose this be-cause of its good coverage of different usages of En-glish, and thus of different countabilities.
The onlycomponent of the original annotation we make useof is the sentence tokenisation.Below, we outline the features used in this re-search and methods of describing feature interac-tion, along with the pre-processing tools and ex-traction techniques, and the classifier architecture.The full range of different classifier architecturestested as part of this research, and the experi-ments to choose between them are described inBaldwin and Bond (2003).4.1 Feature spaceFor each target noun, we compute a fixed-lengthfeature vector based on a variety of features intendedto capture linguistic constraints and/or preferencesassociated with particular countability classes.
Thefeature space is partitioned up into feature clusters,each of which is conditioned on the occurrence ofthe target noun in a given construction.Feature clusters take the form of one- or two-dimensional feature matrices, with each dimensiondescribing a lexical or syntactic property of theconstruction in question.
In the case of a one-dimensional feature cluster (e.g.
noun occurring insingular or plural form), each component featurefeats in the cluster is translated into the 3-tuple:Feature cluster(base feature no.)
Countable Uncountable Bipartite Plural onlyHead number (2) S,P S P PModifier number (2) S,P S S PSubj?V agreement (2 ?
2) [S,S],[P,P] [S,S] [P,P] [P,P]Coordinate number (2?
2) [S,S],[P,S],[P,P] [S,S],[S,P] [P,S],[P,P] [P,S],[P,P]N of N (11 ?
2) [100s,P], .
.
.
[lack,S], .
.
.
[pair,P], .
.
.
[rate,P], .
.
.PPs (52 ?
2) [per,-DET], .
.
.
[in,-DET], .
.
.
?
?Pronoun (12 ?
2) [it,S],[they,P], .
.
.
[it,S], .
.
.
[they,P], .
.
.
[they,P], .
.
.Singular determiners (10) a,each, .
.
.
much, .
.
.
?
?Plural determiners (12) many, few, .
.
.
?
?
many, .
.
.Neutral determiners (11?
2) [less,P], .
.
.
[BARE,S], .
.
.
[enough,P], .
.
.
[all,P], .
.
.Table 1: Predicted feature-correlations for each feature cluster (S=singular, P=plural)?freq(feats|word),freq(feats|word)freq(word) ,freq(feats|word)?ifreq(feati|word)?In the case of a two-dimensional feature cluster(e.g.
subject-position noun number vs. verb numberagreement), each component feature feat s,t is trans-lated into the 5-tuple:?freq(feats,t|word),freq(feats,t|word)freq(word) ,freq(feats,t|word)?i,j freq(feati,j |word),freq(feats,t|word)?ifreq(feati,t|word),freq(feats,t|word)?j freq(feats,j |word)?See Baldwin and Bond (2003) for further details.The following is a brief description of each fea-ture cluster and its dimensionality (1D or 2D).
Asummary of the number of base features and predic-tion of positive feature correlations with countabilityclasses is presented in Table 1.Head noun number:1D the number of the targetnoun when it heads an NP (e.g.
a shaggy dog= SINGULAR)Modifier noun number:1D the number of the tar-get noun when a modifier in an NP (e.g.
dogfood = SINGULAR)Subject?verb agreement:2D the number of the tar-get noun in subject position vs. number agree-ment on the governing verb (e.g.
the dog barks= ?SINGULAR,SINGULAR?
)Coordinate noun number:2D the number of thetarget noun vs. the number of the headnouns of conjuncts (e.g.
dogs and mud =?PLURAL,SINGULAR?
)N of N constructions:2D the number of the targetnoun (N?)
vs. the type of the N?
in an N?of N?
construction (e.g.
the type of dog =?TYPE,SINGULAR?).
We have identified a totalof 11 N?
types for use in this feature cluster(e.g.
COLLECTIVE, LACK, TEMPORAL).Occurrence in PPs:2D the presence or absence ofa determiner (?DET) when the target noun oc-curs in singular form in a PP (e.g.
per dog= ?per,?DET?).
This feature cluster exploitsthe fact that countable nouns occur determin-erless in singular form with only very partic-ular prepositions (e.g.
by bus, *on bus, *withbus) whereas with uncountable nouns, there arefewer restrictions on what prepositions a targetnoun can occur with (e.g.
on furniture, with fur-niture, ?by furniture).Pronoun co-occurrence:2D what personal andpossessive pronouns occur in the same sen-tence as singular and plural instances of thetarget noun (e.g.
The dog ate its dinner =?its,SINGULAR?).
This is a proxy for pronounbinding effects, and is determined over a totalof 12 third-person pronoun forms (normalisedfor case, e.g.
he, their, itself ).Singular determiners:1D what singular-selectingdeterminers occur in NPs headed by the tar-get noun in singular form (e.g.
a dog = a).All singular-selecting determiners consideredare compatible with only countable (e.g.
an-other, each) or uncountable nouns (e.g.
much,little).
Determiners compatible with either areexcluded from the feature cluster (cf.
this dog,this information).
Note that the term ?deter-miner?
is used loosely here and below to denotean amalgam of simplex determiners (e.g.
a), thenull determiner, complex determiners (e.g.
allthe), numeric expressions (e.g.
one), and adjec-tives (e.g.
numerous), as relevant to the partic-ular feature cluster.Plural determiners:1D what plural-selecting deter-miners occur in NPs headed by the target nounin plural form (e.g.
few dogs = few).
Aswith singular determiners, we focus on thoseplural-selecting determiners which are compat-ible with a proper subset of count, plural onlyand bipartite nouns.Non-bounded determiners:2D what non-boundeddeterminers occur in NPs headed by the targetnoun, and what is the number of the target nounfor each (e.g.
more dogs = ?more,PLURAL?
).Here again, we restrict our focus to non-bounded determiners that select for singular-form uncountable nouns (e.g.
sufficient furni-ture) and plural-form countable, plural onlyand bipartite nouns (e.g.
sufficient dogs).The above feature clusters produce a combinedtotal of 1,284 individual feature values.4.2 Feature extractionIn order to extract the features described above,we need some mechanism for detecting NP andPP boundaries, determining subject?verb agreementand deconstructing NPs in order to recover con-juncts and noun-modifier data.
We adopt three ap-proaches.
First, we use part-of-speech (POS) taggeddata and POS-based templates to extract out the nec-essary information.
Second, we use chunk datato determine NP and PP boundaries, and medium-recall chunk adjacency templates to recover inter-phrasal dependency.
Third, we fully parse the dataand simply read off all necessary data from the de-pendency output.With the POS extraction method, we first Penn-tagged the BNC using an fnTBL-based tagger (Ngaiand Florian, 2001), training over the Brown andWSJ corpora with some spelling, number and hy-phenation normalisation.
We then lemmatised thisdata using a version of morph (Minnen et al, 2001)customised to the Penn POS tagset.
Finally, weimplemented a range of high-precision, low-recallPOS-based templates to extract out the features fromthe processed data.
For example, NPs are in manycases recoverable with the following Perl-style reg-ular expression over Penn POS tags: (PDT)* DT(RB|JJ[RS]?|NNS?
)* NNS?
[?N].For the chunker, we ran fnTBL over the lem-matised tagged data, training over CoNLL 2000-style (Tjong Kim Sang and Buchholz, 2000) chunk-converted versions of the full Brown and WSJ cor-pora.
For the NP-internal features (e.g.
determin-ers, head number), we used the noun chunks directly,or applied POS-based templates locally within nounchunks.
For inter-chunk features (e.g.
subject?verbagreement), we looked at only adjacent chunk pairsso as to maintain a high level of precision.As the full parser, we used RASP (Briscoe andCarroll, 2002), a robust tag sequence grammar-based parser.
RASP?s grammatical relation outputfunction provides the phrase structure in the formof lemmatised dependency tuples, from which it ispossible to read off the feature information.
RASPhas the advantage that recall is high, although pre-cision is potentially lower than chunking or taggingas the parser is forced into resolving phrase attach-ment ambiguities and committing to a single phrasestructure analysis.Although all three systems map onto an identi-cal feature space, the feature vectors generated for agiven target noun diverge in content due to the dif-ferent feature extraction methodologies.
In addition,we only consider nouns that occur at least 10 timesas head of an NP, causing slight disparities in thetarget noun type space for the three systems.
Therewere sufficient instances found by all three systemsfor 20,530 common nouns (out of 33,050 for whichat least one system found sufficient instances).4.3 Classifier architectureThe classifier design employed in this research isfour parallel supervised classifiers, one for eachcountability class.
This allows us to classify a sin-gle noun into multiple countability classes, e.g.
de-mand is both countable and uncountable.
Thus,rather than classifying a given target noun accord-ing to the unique most plausible countability class,we attempt to capture its full range of countabilities.Note that the proposed classifier design is that whichwas found by Baldwin and Bond (2003) to be opti-mal for the task, out of a wide range of classifierarchitectures.In order to discourage the classifiers from over-training on negative evidence, we constructed thegold-standard training data from unambiguouslynegative exemplars and potentially ambiguous pos-itive exemplars.
That is, we would like classifiersto judge a target noun as not belonging to a givencountability class only in the absence of positive ev-idence for that class.
This was achieved in the caseof countable nouns, for instance, by extracting allcountable nouns from each of the ALT-J/E and COM-LEX lexicons.
As positive training exemplars, wethen took the intersection of those nouns listed ascountable in both lexicons (irrespective of member-ship in alternate countability classes); negative train-ing exemplars, on the other hand, were those con-tained in both lexicons but not classified as count-Class Positive data Negative data BaselineCountable 4,342 1,476 .746Uncountable 1,519 5,471 .783Bipartite 35 5,639 .994Plural only 84 5,639 .985Table 2: Details of the gold-standard dataable in either.1 The uncountable gold-standard datawas constructed in a similar fashion.
We used theALT-J/E lexicon as our source of plural only and bi-partite nouns, using all the instances listed as ourpositive exemplars.
The set of negative exemplarswas constructed in each case by taking the intersec-tion of nouns not contained in the given countabilityclass in ALT-J/E, with all annotated nouns with non-identical singular and plural forms in COMLEX.Having extracted the positive and negative exem-plar noun lists for each countability class, we filteredout all noun lemmata not occurring in the BNC.The final make-up of the gold-standard data foreach of the countability classes is listed in Table 2,along with a baseline classification accuracy foreach class (?Baseline?
), based on the relative fre-quency of the majority class (positive or negative).That is, for bipartite nouns, we achieve a 99.4% clas-sification accuracy by arbitrarily classifying everytraining instance as negative.The supervised classifiers were built usingTiMBL version 4.2 (Daelemans et al, 2002), amemory-based classification system based on the k-nearest neighbour algorithm.
As a result of exten-sive parameter optimisation, we settled on the de-fault configuration for TiMBL with k set to 9.
25 Results and EvaluationEvaluation is broken down into two components.First, we determine the optimal classifier configura-tion for each countability class by way of stratifiedcross-validation over the gold-standard data.
Wethen run each classifier in optimised configurationover the remaining target nouns for which we havefeature vectors.5.1 Cross-validated resultsFirst, we ran the classifiers over the full feature setfor the three feature extraction methods.
In eachcase, we quantify the classifier performance by way1Any nouns not annotated for countability in COMLEX wereignored in this process so as to assure genuinely negativeexemplars.2We additionally experimented with the kernel-basedTinySVM system, but found TiMBL to be superior in all cases.Class System Accuracy (e.r.)
F-scoreTagger?
.928 (.715) .953Chunker .933 (.734) .956CountableRASP?
.923 (.698) .950Combined .939 (.759) .960Tagger .945 (.746) .876Chunker?
.945 (.747) .876UncountableRASP?
.944 (.743) .872Combined .952 (.779) .892Tagger .997 (.489) .752Chunker .997 (.460) .704BipartiteRASP .997 (.488) .700Combined .996 (.403) .722Tagger .989 (.275) .558Chunker .990 (.299) .568Plural onlyRASP?
.989 (.227) .415Combined .990 (.323) .582Table 3: Cross-validation resultsof 10-fold stratified cross-validation over the gold-standard data for each countability class.
The fi-nal classification accuracy and F-score3 are averagedover the 10 iterations.The cross-validated results for each classifier arepresented in Table 3, broken down into the differ-ent feature extraction methods.
For each, in addi-tion to the F-score and classification accuracy, wepresent the relative error reduction (e.r.)
in classifi-cation accuracy over the majority-class baseline forthat gold-standard set (see Table 2).
For each count-ability class, we additionally ran the classifier overthe concatenated feature vectors for the three basicfeature extraction methods, producing a 3,852-valuefeature space (?Combined?
).Given the high baseline classification accuraciesfor each gold-standard dataset, the most revealingstatistics in Table 3 are the error reduction and F-score values.
In all cases other than bipartite, thecombined system outperformed the individual sys-tems.
The difference in F-score is statistically sig-nificant (based on the two-tailed t-test, p < .05) forthe asterisked systems in Table 3.
For the bipartiteclass, the difference in F-score is not statistically sig-nificant between any system pairing.There is surprisingly little separating the tagger-,chunker- and RASP-based feature extraction meth-ods.
This is largely due to the precision/recall trade-off noted above for the different systems.5.2 Open data resultsWe next turn to the task of classifying all unseencommon nouns using the gold-standard data and thebest-performing classifier configurations for each3Calculated according to: 2?precision ?recallprecision+recall00.20.40.60.8110  100  1000  100000.20.40.60.81precisionrecallPrecisionRecallMean frequencyFigure 1: Precision?recall curve for countable nounscountability class (indicated in bold in Table 3).4Here, the baseline method is to classify every nounas being uniquely countable.There were 11,499 feature-mapped commonnouns not contained in the union of the gold-standard datasets.
Of these, the classifiers were ableto classify 10,355 (90.0%): 7,974 (77.0%) as count-able (e.g.
alchemist), 2,588 (25.0%) as uncountable(e.g.
ingenuity), 9 (0.1%) as bipartite (e.g.
head-phones), and 80 (0.8%) as plural only (e.g.
dam-ages).
Only 139 nouns were assigned to multiplecountability classes.We evaluated the classifier outputs in two ways.In the first, we compared the classifier output to thecombined COMLEX and ALT-J/E lexicons: a lexiconwith countability information for 63,581 nouns.
Theclassifiers found a match for 4,982 of the nouns.
Thepredicted countability was judged correct 94.6% ofthe time.
This is marginally above the level of matchbetween ALT-J/E and COMLEX (93.8%) and substan-tially above the baseline of all-countable at 89.7%(error reduction = 47.6%).To gain a better understanding of the classifierperformance, we analysed the correlation betweencorpus frequency of a given target noun and its pre-cision/recall for the countable class.5 To do this,we listed the 11,499 unannotated nouns in increas-ing order of corpus occurrence, and worked throughthe ranking calculating the mean precision and re-call over each partition of 500 nouns.
This resultedin the precision?recall graph given in Figure 1, fromwhich it is evident that mean recall is proportionaland precision inversely proportional to corpus fre-4In each case, the classifier is run over the best-500 features as selected by the method described inBaldwin and Bond (2003) rather than the full feature set, purelyin the interests of reducing processing time.
Based on cross-validated results over the training data, the resultant differencein performance is not statistically significant.5We similarly analysed the uncountable class and found thesame basic trend.quency.
That is, for lower-frequency nouns, the clas-sifier tends to rampantly classify nouns as count-able, while for higher-frequency nouns, the classi-fier tends to be extremely conservative in positivelyclassifying nouns.
One possible explanation for thisis that, based on the training data, the frequencyof a noun is proportional to the number of count-ability classes it belongs to.
Thus, for the morefrequent nouns, evidence for alternate countabilityclasses can cloud the judgement of a given classifier.In secondary evaluation, the authors used BNCcorpus evidence to blind-annotate 100 randomly-selected nouns from the test data, and tested the cor-relation with the system output.
This is intendedto test the ability of the system to capture corpus-attested usages of nouns, rather than independentlexicographic intuitions as are described in the COM-LEX and ALT-J/E lexicons.
Of the 100, 28 were clas-sified by the annotators into two or more groups(mainly countable and uncountable).
On this set,the baseline of all-countable was 87.8%, and theclassifiers gave an agreement of 92.4% (37.7% e.r.
),agreement with the dictionaries was also 92.4%.Again, the main source of errors was the classi-fier only returning a single countability for eachnoun.
To put this figure in proper perspective, wealso hand-annotated 100 randomly-selected nounsfrom the training data (that is words in our com-bined lexicon) according to BNC corpus evidence.Here, we tested the correlation between the manualjudgements and the combined ALT-J/E and COMLEXdictionaries.
For this dataset, the baseline of all-countable was 80.5%, and agreement with the dic-tionaries was a modest 86.8% (32.3% e.r.).
Basedon this limited evaluation, therefore, our automatedmethod is able to capture corpus-attested count-abilities with greater precision than a manually-generated static repository of countability data.6 DiscussionThe above results demonstrate the utility of theproposed method in learning noun countabilityfrom corpus data.
In the final system configu-ration, the system accuracy was 94.6%, compar-ing favourably with the 78% accuracy reportedby Bond and Vatikiotis-Bateson (2002), 89.5% ofO?Hara et al (2003), and also the noun token-basedresults of Schwartz (2002).At the moment we are merely classifying nounsinto the four classes.
The next step is to store thedistribution of countability for each target noun andbuild a representation of each noun?s countabilitypreferences.
We have made initial steps in this direc-tion, by isolating token instances strongly support-ing a given countability class analysis for that targetnoun.
We plan to estimate the overall frequency ofthe different countabilities based on this evidence.This would represent a continuous equivalent of thediscrete 5-way scale employed in ALT-J/E, tunable todifferent corpora/domains.For future work we intend to: investigate furtherthe relation between meaning and countability, andthe possibility of using countability information toprune the search space in word sense disambigua-tion; describe and extract countability-idiosyncraticconstructions, such as determinerless PPs and role-nouns; investigate the use of a grammar that distin-guishes between countable and uncountable uses ofnouns; and in combination with such a grammar, in-vestigate the effect of lexical rules on countability.7 ConclusionWe have proposed a knowledge-rich lexical acqui-sition technique for multi-classifying a given nounaccording to four countability classes.
The tech-nique operates over a range of feature clusters draw-ing on pre-processed corpus data, which are then fedinto independent classifiers for each of the count-ability classes.
The classifiers were able to selec-tively classify the countability preference of Englishnouns with a precision of 94.6%.AcknowledgementsThis material is based upon work supported by the NationalScience Foundation under Grant No.
BCS-0094638 and alsothe Research Collaboration between NTT Communication Sci-ence Laboratories, Nippon Telegraph and Telephone Corpora-tion and CSLI, Stanford University.
We would like to thankLeonoor van der Beek, Ann Copestake, Ivan Sag and the threeanonymous reviewers for their valuable input on this research.ReferencesKeith Allan.
1980.
Nouns and countability.
Language,56(3):541?67.Timothy Baldwin and Francis Bond.
2003.
A plethora of meth-ods for learning English countability.
In Proc.
of the 2003Conference on Empirical Methods in Natural Language Pro-cessing (EMNLP 2003), Sapporo, Japan.
(to appear).Francis Bond and Caitlin Vatikiotis-Bateson.
2002.
Using anontology to determine English countability.
In Proc.
of the19th International Conference on Computational Linguistics(COLING 2002), Taipei, Taiwan.Francis Bond, Kentaro Ogura, and Satoru Ikehara.
1994.Countability and number in Japanese-to-English machinetranslation.
In Proc.
of the 15th International Conferenceon Computational Linguistics (COLING ?94), pages 32?8,Kyoto, Japan.Francis Bond.
2001.
Determiners and Number in English, con-trasted with Japanese, as exemplified in Machine Transla-tion.
Ph.D. thesis, University of Queensland, Brisbane, Aus-tralia.Ted Briscoe and John Carroll.
2002.
Robust accurate statisticalannotation of general text.
In Proc.
of the 3rd InternationalConference on Language Resources and Evaluation (LREC2002), pages 1499?1504, Las Palmas, Canary Islands.Lou Burnard.
2000.
User Reference Guide for the British Na-tional Corpus.
Technical report, Oxford University Comput-ing Services.Ann Copestake and Ted Briscoe.
1995.
Semi-productive poly-semy and sense extension.
Journal of Semantics, pages 15?67.Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and An-tal van den Bosch.
2002.
TiMBL: Tilburg memory basedlearner, version 4.2, reference guide.
ILK technical report02-01.Ralph Grishman, Catherine Macleod, and Adam Myers, 1998.COMLEX Syntax Reference Manual.
Proteus Project, NYU.
(http://nlp.cs.nyu.edu/comlex/refman.ps).Satoru Ikehara, Satoshi Shirai, Akio Yokoo, and HiromiNakaiwa.
1991.
Toward an MT system without pre-editing?
effects of new methods in ALT-J/E?.
In Proc.
of the ThirdMachine Translation Summit (MT Summit III), pages 101?106, Washington DC.Marc Light.
1996.
Morphological cues for lexical semantics.In Proc.
of the 34th Annual Meeting of the ACL, pages 25?31, Santa Cruz, USA.Guido Minnen, John Carroll, and Darren Pearce.
2001.
Ap-plied morphological processing of English.
Natural Lan-guage Engineering, 7(3):207?23.Grace Ngai and Radu Florian.
2001.
Transformation-basedlearning in the fast lane.
In Proc.
of the 2nd Annual Meetingof the North American Chapter of Association for Compu-tational Linguistics (NAACL2001), pages 40?7, Pittsburgh,USA.Tom O?Hara, Nancy Salay, Michael Witbrock, Dave Schnei-der, Bjoern Aldag, Stefano Bertolo, Kathy Panton, FritzLehmann, Matt Smith, David Baxter, Jon Curtis, and PeterWagner.
2003.
Inducing criteria for mass noun lexical map-pings using the Cyc KB and its extension to WordNet.
InProc.
of the Fifth International Workshop on ComputationalSemantics (IWCS-5), Tilburg, the Netherlands.Lane O.B.
Schwartz.
2002.
Corpus-based acquisition of headnoun countability features.
Master?s thesis, Cambridge Uni-versity, Cambridge, UK.Eric V. Siegel and Kathleen McKeown.
2000.
Learning meth-ods to combine linguistic indicators: Improving aspectualclassification and revealing linguistic insights.
Computa-tional Linguistics, 26(4):595?627.Erik F. Tjong Kim Sang and Sabine Buchholz.
2000.
Introduc-tion to the CoNLL-2000 shared task: Chunking.
In Proc.of the 4th Conference on Computational Natural LanguageLearning (CoNLL-2000), Lisbon, Portugal.Anna Wierzbicka.
1988.
The Semantics of Grammar.
JohnBenjamin.
