QA better than IR ?Dominique LaurentSynapse D?veloppement33 rue MaynardToulouse, Francedlaurent@synapse-fr.comPatrick S?gu?laSynapse D?veloppement33 rue MaynardToulouse, Francepatrick.seguela@synapse-fr.comSophie N?greSynapse D?veloppement33 rue MaynardToulouse, Francesophie.negre@synapse-fr.comAbstractA Question Answering (QA) system allowsthe user to ask questions in natural languageand to obtain one or several answers.
Ifcompared with a classical IR engine likeGoogle, what kind of key benefits QA bringto users and how to measure their distinctiveperformances.
This is what we shall attempthere to determine, specially in providing acomparative weak and strong points table ofeach system, along with showing how QAsystems, in particular our Qristal QA system,requires up two to six time less ?user effort?.1 IntroductionAsking questions in natural language and obtainshort answers (if possible the exact answer)make QA systems the paramount of InformationRetrieval.Through TREC and CLEF internationalcampaigns, Question Answering systems areevaluated both in monolingual and multilingualuse (e.g.
Voorhees, 2005; Vallin, 2005).Nevertheless, very few comparisons ofperformances took place between QuestionAnswering systems and Information Retrievalengines (Kwok, 2001; Radev, 2002; Buccholz,2002; McGowan, 2005).
If so, they mainly focuson the quality of the supplied answers and on thetime for user to obtain the answer.We hereby attempt to define an evaluationmethod to compare performances and user-friendliness of both Question Answering systemsand Information Retrieval engines.
We willapply this method to our system Qristal and tothe Google Desktop Search1 engine..1Google Desktop Search : http://desktop.google.com/2 Background2.1 Qristal pedigreeQristal (French acronym of "Questions-R?ponsesInt?grant un Syst?me de Traitement Automatiquedes Langues", which can be translated by"Question Answering System using NLP") is, asfar as we know, the first Multilingual QuestionAnswering System available on the consumermarket (B2C).
It handles French, English, Italian,Portuguese or Polish.Qristal allows the user to query on a staticcorpus or on the Web.
It supplies answers in oneor any of the 4 languages.Our system is described in detail in otherpapers (Amaral, 2004; Laurent 2004; Laurent2005-1; Laurent 2005-2).
Qristal is based on ourCordial syntactic analyzer and extensively usesall the usual constituents of the natural languageprocessing, while, as seldom found, remarkablyfeaturing anaphora resolution and metaphordetection.Originally developed within the framework ofthe European project TRUST2 and M-CAST3,our system has evolved, over the last five years,from a monolingual single-user program into amultilingual multi-user system.2.2 Qristal benchmarks and performancesA beta version of Qristal was evaluated in July2004 in the EQueR4 evaluation campaignorganized in France by several ministries(Ayache, 2004; Ayache, 2005).
With a MRR("Mean Reciprocal Rank"; cf.
Ayache, 2004) of0.58 for the exact answers and 0.70 for thesnippets, our system ranked first out of the seven2IST-1999-56416 http://www.trustsemantics.com3EDC 22249, http://www.m-cast.infovide.pl4http://www.technolangue.net/article61.htmlEACL 2006 Workshop on Multilingual Question Answering - MLQA061Question Answering systems evaluated.The marketed version of Qristal was evaluatedduring the CLEF 2005 (Laurent, 2005-2) andobtained 64% of exact answers for French toFrench, 39.5% from English to French and36.5% from Portuguese to French.
Once again,Qristal ranked first in this evaluation, for Frenchengines and for all cross language systems, allpairs considered.Since this evaluation, the resources wereincreased and some algorithms revised, so ourlast tests brought us a 70% of exacted answersand 45 % for cross language.3 QA and IRIt is true that, intrinsically, IR engines and QAsystems differ in design, objectives andprocesses.
An IR engine is geared to deliversnippets or docs from a query, a QA systemstrive to deliver the exact answer to a question.If one is to differentiate 3 key features of bothsystems, one of the first difference concerns thequery mode : natural language for the QAsystems and ?Boolean like?
for the IR engines.We define ?Boolean like?
extensively as the useof Boolean operators associated to underlyingconstraints induced by the word matchingtechniques.
The table 1 gives the results ofGoogle Desktop for natural language requestsand Boolean requests (set of questions detailedbelow) and we can see that results with naturallanguage requests are not so good:rankofanswersGooglesnippetsnaturallanguagerequestsGooglesnippetsBooleanrequestsGooglesnippetsor docsnaturallanguagerequestsGooglesnippets ordocsBooleanrequests1 4.2 % 9.7 % 7.3 % 30.6 %1-5 6.7 % 18.5 % 14.2 % 52.1 %Table 1.
Results with Google Desktop Search onnatural language and Boolean requestsThis performance table shows that classicalengines are not suited to answer questions innatural language.
To quote Google ?A Googlesearch is an easy, honest and objective way tofind high-quality websites with informationrelevant to your search.
?The Google technology considers, at least inFrench, equally and of same ?weight, words like,"de" or "le" and the highly semantically-loadedwords of the query.
This leads to a dramaticupsurge of noise in their results.
Therefore, usingclassic engines require a good knowledge of theirsyntax and their underlying word matchingtechniques, like the necessity of groupingbetween quotation marks, the ?noun phrases?and the expressions.The second difference concerns what isdelivered to the user.
Question Answeringsystems deliver one or more exact answers to aquestion and their context whereas classicalengines return snippets with links to the textsthose snippets were extracted from.The third difference relates to the dynamic andopenness status of the corpora.
Usually QAsystems use confined or close corpora with lowup-date rate, while classical IR engines are tunedto the Web queries and their reference file arecontinuously updated.Qristal QA is able to deliver answers fromboth web-based queries and closed corpora.
Wewere eager to apply our proposed metrics on theweb-based deliveries, but unfortunately, we hadnot at our disposal the appropriate web referencefile of questions and answers, probably impossi-ble to elaborate considering the extremely highup-dating rate of the web pages.Therefore we had but no choice to use a closedcorpus, Google Desktop being able to managethis type of corpus (see note 1).
We used thereference file of questions and answersestablished for EQueR.3.1 Question Answering systemsQristal interface mimics almost all the usualscreen template of IR engine (see figure 2).
Itdisplays results in different languages, keeps thetrack or trace of the precedent requests, allowsthe user to choose the requested corpus and, ifthought necessary, to make a semanticdisambiguation of the question terms.Figure 2.
QRISTALResults (answer, sentences, links) are display-ed at three levels: if an exact answer is found, itEACL 2006 Workshop on Multilingual Question Answering - MLQA062is displayed in the top right part of the window,sentences in the lower right part of the windowand links on top of the sentences.
Note that thewords or phrases supporting the inferred answerare put in bold in the text.
These words aresometimes pronouns (anaphora) and, frequently,synonyms or derivate forms of the request words.3.2 Information Retrieval enginesNumerous Information Retrieval engines areavailable for closed or web-based corpora.For our evaluation we selected the Googleengine, as it is available both for a web andclosed PC desktop usage (in version "DesktopSearch"), although, regrettably, this beta versionhad a few minor defects.
The snippets suppliedby Google are generally fragments of sentences,sometimes with cut words, stemming fromexcerpts of text seeming to correspond the best tothe query.
All you can expect is a help in theselection of the text(s) in which is likely to bepresent the answer to your query rather than apinpointed and elaborated exact answer.3.3 Evaluation method of the performancesCorpus of requests and answersThe corpus selected for this evaluation is thecorpus used for the EQueR evaluation campaign.This choice was justified by the size of thecorpus (over half million texts for about 1.5 Gb)and, especially, by the fact that we have animportant corpus of questions (500) with manyanswers and references for all these questions.To generate a comprehensive package of testsof Question Answering systems, ELDA,organizer of the EQueR campaign, made acompilation of all the results returned by all theparticipants.
Then, thanks to a thoroughexamination by several specialists (one of whombeing an author of this paper), this corpus hasbeen verified, increased and validated.
In doingso, most certainly, the immense majority of thepossible answers are inventoried, the greatmajority of the references of these answers isknown5 to the extent that this corpus of questionsand answers can be automatically run, the resultssubsequently requiring only a reduced amount ofchecking.The initial corpus of 500 questions has beenreduced to 330 questions for this evaluation.
In5a set comprising textual corpora, questions andanswers is available at ELDA (http://www.elda.org/article139.html)fact, the last 100 questions were onlyreformulations of previous questions and offerednot enough interest.
30 questions concernedbinary answers YES/NO and 40 questionsconcerned lists as answers.
As InformationRetrieval engines are not able to return binary orlist answers, including them within theevaluation would have biased it.
Finally, fivequestions were without any answer.
They wereremoved by the organizers of EQueR and wealso did so accordingly.
For these five ?no-answer?questions, Qristal systematically returnedcorrect answers i.e.
NIL, where as a classicalsearch engine like Google would systematicallyreturn at least one answer.
We decided not toinclude these ?NIL?
questions so as not furtherpenalize the IR Google engine.Evaluation of the "user effort"We have two competing systems on the samecorpora and a reference file with questions andanswers.
We need now to define the basis oftheir comparison.The main comparative evaluation between theQuestion Answering systems and theInformation Retrieval engines (Kwok, 2001)considered only the reading time while countingcharacters to be read to reach the answer.Knowing the delay needed to obtain the results inmost Question Answering systems (McGowan,2005), it seems necessary to take also in accountthis delay if we want to measure the global usereffort to obtain an answer to his question.We consider that the user wants a correctanswer to his question and we consider that theanswer is correct if this answer can be found inthe snippet or in the text linked to the snippet forGoogle (to the sentence for Qristal).
So wecompared the quality of the systems as follows:percentage of correct answers ranked first,ranked within the five first, the ten first and thehundred first.
We considered both the answer aspart of the snippets or part or the snippets anddocuments.For the user, the quality of the answersreturned, especially in the first results page, isparamount.
But, we think another item has to betaken into account: the time needed to obtain theanswer.
This time is the compound of threeelements:?
the time to key in the question,?
the delay before the results display,?
the reading time of the snippets orsentences to reach a correct answer.EACL 2006 Workshop on Multilingual Question Answering - MLQA063Addition of these three elements provides themeasure of the "user effort".Time to key in the questionThis time is shorter for a ?Boolean like?engine.
Typing a question in Qristal needs inaverage nine seconds more than with the query inGoogle.
However it supposes that the user typesthe Boolean like request at the same speed than anatural language request.
This implies that theuser is very familiar with the Google syntax.
Forexample the question 6:Quel ?ge a l'abb?
Pierre ?will be converted into Google syntax by:"abb?
Pierre" ansThis Boolean request increases the probabilityto obtain the effective age, not only snippets withthe words "?ge" and "abb?
Pierre".
Otherexample, the question 37:Quel ?v?nement a eu lieu le 27 d?cembre 1978 enAlg?rie ?will be converted into Google syntax:"27 d?cembre 1978" Alg?rieThis Boolean request is needed either wordslike "?v?nement" or "avoir lieu" will bring outmore noise than correct answers.So, we translated the complete list of questionin the Google syntax in addition to the questionsset in natural language.
Compared results wereshown on Table 1.To measure the time necessary to enterquestions, we counted the number of characterstyped (always inferior in Google) and multipliedthis number by an average speed of 150characters by minute.
We know that aprofessional typist types at a speed of 300 to 400characters per minute, so our chosen speedcorresponds to a user keying in with two fingers.The following table gives the numbers ofcharacters and the times for the two systems:characters  mean timeQristal 49.1 19.6 secondsGoogle Desktop 27.2 10.9 secondsTable 3: mean time of question enteringDelay to display the resultsThis is the elapsed time between the click onthe button "OK" and the display of the results.Note here that, strangely, Google Desktop has aresponse time distinctly bigger than the responsetime of Google on the Web, especially when therequest contains a group of words betweenquotes.Reading time to reach one answerTo fix a reading speed, we tested severalusers.
An average speed of 40 characters bysecond (2 400 characters per minute, or also 400words per minute) seems a fair measure.
Itcorresponds to a reader with a higher-educationbackground, according to Richaudeau, 1977.While making these tests, we noted that, if theuser knows the answer, the reading speed of boththe snippets and texts would increase to 100characters per second (6 000 characters perminute, or 1 000 words per minute).
Even if veryfew questions have an obvious answer, wedecided to calculate the reading times with bothspeeds of 40 characters per second and 100characters per second.The speed of 100 characters per second willhowever be considered as a superior limit thatfavours clearly Google where the snippets,constituted by fragments of sentences, sometimesfragments of words, are more difficult and longerto read than the sentences returned by Qristal.4 Results of the benchmark4.1 Evaluation on the 330 questionsFor the 330 questions of the evaluation, resultsare:AnswerrankGoogleDesktopsnippetsQristalsnippetsGoogleDesktopsnippetsor docsQristalsnippetsor docsExactanswer69.7 %  69.7 %1 or exact 9.7 % 81.8 % 30.6 % 86.7 %1-5 18.5 % 88.2 % 51.9 % 94.5 %1-10 21.5 % 88.8 % 58.5 % 96.1 %1-100 27.0 % 90.9 % 70.0 % 98.5 %Not found 73.0 % 9.1 % 30.
0 % 1.5 %Table 4: Percentage of correct answers / 330Qristal returns an exact answer for nearly 70%of questions and a correct answer is returned asexact answer or in the first sentence for 82% ofthe 330 questions.
This has to be compared withthe 10% of answer found in first position byGoogle Desktop.
If we consider the snippets andthe documents, Qristal returns a correct answerin first rank for 86% of the questions and Googlein more than 30%.These results give a clear advantage to theQuestion Answering system on the InformationRetrieval engine.
This superiority in qualityexists also in quantity.
Here is the table of userefforts to obtain a correct answer:EACL 2006 Workshop on Multilingual Question Answering - MLQA064GoogleDesktopQristalType the question 10.9 s 19.6 sDisplay results 3.0 s 2.1 sReading 40 char/second 59.3 s 7.1 sReading 100 char/second 23.7 s 2.8 sDisplay results + Reading 40characters/second62.3 s 9.2 sDisplay results + Reading 100characters/second26.7 s 4.9 sType the question + Displayresults + Reading 40 c./second73.2 s 28.8 sType the question + Displayresults + Reading 100 c./second37.6 s 24.5 sTable 5: Mean times compared for the two systems(on 330 questions)It appears that the time to type down the questionin Qristal is nearly 9 seconds longer than withGoogle.
The elapsed time before display issimilar.
On the other hand, as Google gives acorrect answer in a higher rank or in a document,not a snippet, the number of characters to be readbefore reaching an answer is finally moreimportant.
Finally, if we consider the averagereading speed of 40 characters per second,Qristal needs in average 29 seconds against 73seconds, in other words the user effort to obtain agood answer is 2.5 times higher with Googlethan with Qristal.If we don't take into account the time to enterthe question, Google requires a user effort 6 to 7time higher than Qristal to reach an answer.
Thiscomparison would be effective in the case ofvoice-based submitted query.
In that case, theacquisition of the question would become moredifficult according to the syntax of the Booleanengine ("open the quotes", "close the quotes"...)4.2 Evaluation on 231 questionsLooking carefully at each answer returned byGoogle Desktop, we discovered that it ignoredsome texts or, more exactly, some parts of texts,especially the end of these texts.
The help pagesof this software point out this "bug" :However, if you're searching for a word within the file,please note that Google Desktop searches only about thefirst 10,000 words.
In a few cases, Google Desktop mayindex slightly fewer words to save space in your searchindex and on your hard drive.6Of course this default impacted on the results andthe comparisons.
Thus, we decided, in a seconditeration of this evaluation, to consider only the231 questions where Google Desktop found at6http://desktop.google.com/support/bin/answer.py?answer=24755&topic=209least one correct answer.
Google Desktop foundno answer with those 99 (330-231) removedquestions for two main reasons.
Firstly, as itdoesn't manage a full indexation of documents.Secondly, as some complex questions like "why"or "how" questions often lead it to silence on thisevaluation.This selection of 231 questions favours GoogleDesktop but it allows a more accuratecomparison.
Here are the results for those 231questions:Scale GoogleDesktopbribesQristalbribesGoogleDesktopbribesou docsQristalbribesou docsExactanswer73.6 %  73.6 %1 13.9 % 89.6 % 43.7 % 90.9 %1-5 26.4 % 94.8 % 74.0 % 97.4 %1-10 30.7 % 94.8 % 83.5 % 97.8 %1-100 38.5 % 96.1 % 100.0 % 99.6 %Not found  3.9 %  0.4 %Table 6: Number of answers and percentages / 231The corpus of 231 questions is thus "easier"than that of 325 questions.
This confirms thescore of Qristal for the exact answers: 73.6%versus 69.7%, and the score for the correctanswer in first rank: 89.6% versus 81.8%.
Butthe results of Google are of course better: 13.9%in the first snippet against 9.7%, 43.7% in thefirst snippet or the first document, against 30.6%.However the advantage of the QA system overthe IR engine is clear in terms of quality,especially if we consider only the snippets.
Thisadvantage is also clear for the user effort, even ifany of the answer not found by Qristal penalizesthis system as the reading time of this question isthe consolidation of all reading times of all thesnippets displayed for this question!Google QristalType the question 10.1 s 18.7 sDisplay results 3.5 s 2.0 sReading 40 char/second 28.4 s 3.3 sReading 100 char/second 11.4 s 1.3 sDisplay results + Reading 40char/second31.9 s 5.3 sDisplay results + Reading100 char/second14.9 s 3.3 sType + Display results +Reading 40 char/second42.0 s 23.9 sType + Display results +Reading 100 char/second24.9 s 21.9 sTable 7: Mean times compared for the two systems(on 231 questions)EACL 2006 Workshop on Multilingual Question Answering - MLQA065The mean times of question entering anddisplaying results are nearly the same for those231 questions than for the 325 questions.
But,because Google Desktop finds an answer to allquestions, the reading time before a correctanswer is, in that case, reduced for Google.The mean times of question entering anddisplaying results are nearly the same for those231 questions than for the 325 questions.
But,because Google Desktop finds an answer to allquestions, the reading time before a correctanswer is, in that case, reduced for Google.Finally, with an average reading speed of 40characters by second, the user effort is two timeshigher with Google Desktop than it is withQristal.
And if we don't take into account thetime to type in the question, the user effort withGoogle is 6 times higher than with Qristal.Using the same presentation than Kwok, 2001,the following graph gives the compared resultsof the two systems.
On Y-axis is the number ofcorrect answers and in X-axis the number ofcharacters read, for the 231 questions:Figure 8 : number of correct answers by charactersThe interest of Question Answering systems isparticularly noticeable at the beginning of thegraph, seeing that Qristal displays a correctanswer as exact answer at the top of the screen inmore than 70% of the questions while GoogleDesktop needs to read about 1000 characters inthe snippets and in the documents to obtain asimilar success rate.4.3 Comparison by type of question(s)The above statistics concern all types of queries.In fact, 25 questions wait a definition, the othersbeing factual requests.
The following table of the231 questions corpus gives the results for thesetwo categories:AnswerrankGooglesnippetsQristalsnippetsGooglesnippetsor docsQristalsnippetsor docsExactdefinition64.0 %  64.0 %Definitionin rank 132.0 % 88.0 % 48.0 % 92.0 %Definitionin rank 1-552.0 % 96.0 % 68.0 % 100 %Exactfactual74.8 %  74.8 %Factual rank111.7 % 89.8 % 43.2 % 90.8 %Factual rank1-523.3 % 94.7 % 74.8 % 97.6 %Table 9 : Percentages of answers by type of questionsThe only significant gap in these results is thatGoogle provides better results for definitions(32% of correct answers in the first snippetagainst 12% for the factual questions).We also looked at the questions beginning by"comment" ("how"), but we excluded thosebeginning by "comment s'appelle" ("how iscalled") or "comment est mort" ("how didsomebody die"), i.e.
16 questions (3, 50, 59, 90,93, 117, 148, 154, 165, 196, 199, 234, 247, 249,263, 295).
The results are :Rank GooglesnippetsQristalsnippetsGooglesnippetsor docsQristalsnippetsor docsExact  25.0 %  25.0 %1 0.0 % 56.3 % 18.8 % 56.3 %1-5 6.3 % 68.8 % 25.0 % 81.3 %1-10 6.3 % 68.8 % 25.0 % 81.3 %1-100 12.5 % 68.8 % 37.5 % 81.3 %Not found 87.5 % 31.2 % 62.5 % 18.7 %Table 10: Percentage of correct answers for questionsbeginning by "comment" ("how")These results are not satisfying for any of the twosystems but Qristal displays a correct answer in 9cases on 16, versus 0 for Google Desktop.
Thisunderlines that the Question Answering systemsare more successful when the queries are notpurely factual requests.
Most certainly this couldbe caused by the fact that those questions requirea deeper analysis.A closer examination of factual questionsrevealed that the most difficult questions for theInformation Retrieval engines are the questionsabout location.
For example Google Desktopfinds the country related to the Vilvorde townonly at the 23rd rank, and the country related toJohannesburg is given only at the 13th rank; theregion of Cancale is displayed at the 18th rank;EACL 2006 Workshop on Multilingual Question Answering - MLQA066and the department (county) where is locatedAnnemasse only at the 23rd rank.More generally, a search engine finds answersmore easily when these answers contain thewords of the query.
For example, to the query252 ("?
quelle peine fut condamn?
Jean-MarieVillemin le 16 d?cembre 1993?"
["What was thesentence received by Jean-Marie Villemin on 16December 1993 ?
"]), Google Desktop does notfind any answer because the acceptable answers"cinq ans de prison" (five years of prison) or"cinq ann?es d'emprisonnement" (a five-yearprison sentence) does not contain, in French, anyword of the query.
Similarly the search enginehas many difficulties to display the developmentof acronyms like those of the question 141 ("Quesignifie CGT?"
["What is the significance ofCGT?
"]), or of the question 319 ("Qu'est-ce quel'EEE?"
["What is EEE?"]).
Because the answersare developments of these capital letters whichare not so frequent, except when the acronym israre, as in this case the acronym is often followedor preceded by his significance, like for thequestions 327 ("Qu'est-ce que le Cermoc ?
"["What is Cermoc?"])
or 330 ("Qu'est-ce quel'OACI ?"
[What is OACI?"]).
For these twoquestions, Google Desktop returns the correctanswer in the first snippet.5 PerspectivesThe evaluation was designed in such fair way totake into account all the differences between theInformation Retrieval engines and the QuestionAnswering systems.We made all possible efforts not to favour theQA systems and avoid non equitable compari-son.
For example, the evaluation includes therequests to Google Desktop made with the mostsophisticated achievable query syntax to generatea return of the best answers, knowing that if theywere keyed in as for the natural languagerequests, their success rate would have droppedconsiderably (see Table 1).It is most unlikely that one is able to formulatequeries in Boolean like style as quickly as in NLquestions.
Conversely, for a same given numberof characters, reading Google snippets requiresmost likely far more time than reading completeNL sentences.
However, despite all thesemetrical choices more favourable to the classicalsearch engine, the Question Answering systemobtains better results with regard to the quality ofthe answers and to the user effort.If we were able to compare the Web versionsof Google and Qristal, the results would beprobably different.First, because Qristal uses the search enginesas a meta-engine without any indexation.
Next,because GoogleWeb is really faster at displayingthe results from the Web than Google Desktop.At last because the redundancy, due to the largevolume of indexed pages on the Web, allows theimplementation of some very successfultechniques.For example it may happen that you find thequestions in natural language followed by theiranswers inside Web pages and this in such a waythat asking a request in natural language inGoogle, you can obtain sometimes a verypertinent answer.
To the question "Pourquoi leciel est bleu?"
("Why the sky is blue?")
or to thequestion "Pourquoi la mer est bleue?"
("Why thesea is blue?
"), Google Web returns in first ranksnippets and documents very accurately.However the analysis of the documents andcontained answers permit to the QuestionAnswering systems to return more accurateanswers.
For example, with the request "capitaleanglaise" ("English capital"), Google returns alot of snippets containing the phrase "capitaleanglaise" ("English capital") but not the wordLondres or London in these snippets.
In anInformation Retrieval engine the answers arevery often less justified by the context than it isthe case with Question Answering systems.
Thisis because the snippets group essentially wordscontained in the query.
For example, to thequestion 26 ("Qui a ?crit Germinal?"
["Whowrote Germinal?
"], converted in Google syntaxby : "auteur Germinal" ["writer Germinal"]), thesearch engine returns "?mile Zola" in the secondsnippet but the snippet "L'exposition "EmileZola, photographe" fait escale" (The exhibition"Emile Zola, photograph" stops at) would beconsidered as an answer out of its context andnon receivable within a campaign like TREC,even if we can read in the text : "L'auteur de"Germinal", l'?crivain fran?ais Emile Zola(1840-1902), ?tait aussi un photographe detalent" ("The author of Germinal, the writerEmile Zola (1840-1902), was also a talentedphotograph").
We could almost say that theclassical search engines return far better resultswhen the user already knows the answer to hisquery.A complete compared benchmark and exhaustiveevaluation of search engines and questionanswering systems needs to be made on the Web.EACL 2006 Workshop on Multilingual Question Answering - MLQA067The evaluation method described above could beapplied but, knowing the difficulty to validate aWeb corpus of answers, specially the difficultyto keep it referentially constant, the effort toestimate the quality of the returned answerswould be far much enormous than the oneengaged for this evaluation on a closed corpus.6 ConclusionWe described a metrical method to compareQuestion Answering systems and InformationRetrieval engines on a hard disk-based corpus.Applied to our system Qristal and to the searchengine Google Desktop, this method shows thatthe improvements due to question answeringsystems, especially in terms of user effort, areboth qualitative and quantitative.
Taking intoaccount or not the query keying time, thequestion answering system is 2 to 6 times fasterthan the Information Retrieval engine.This evaluation, made almost automaticallywith the corpus EQueR, focuses mainly onfactual or definition types of requests whichdefines the majority of the requests concerned inthe QA system evaluation campaigns.
On morecomplex questions, like those beginning by"comment" ("how..."), the QA systems obtainedless satisfying results than for factual questionsbut, on this type of questions, the search engineslike Google proved to be also less accurate.
Amore exhaustive and thorough study on thesetypes of requests or on questions beginning by"pourquoi" ("why") would possibly confirmthese results, although here only a few questionswere of these types.At last, a similar metrics and applied evaluationremain to be endeavoured on a web-based corpusdespite the entailing difficulties.ReferencesAmaral C., Laurent D., Martins A., Mendes A.,Pinto C. (2004), Design & Implementation of aSemantic Search Engine for Portuguese, InProceedings of the Fourth Conference on LanguageResources and Evaluation.Amaral C., Figueira H., Martins A., Mendes A.,Mendes P., Pinto C., (2005) Priberam's QuestionAnswering System for Portuguese.
In WorkingNotes for the CLEF 2005 Workshop, 21-23September, Vienna, AustriaAyache C., Choukri K., Grau B.
(2004) CampagneEVALDA/EQueR ?valuation en Question-R?ponse.http://www.technolangue.net/IMG/pdf/rapport_EQUER_1.2.pdfAyache C., Grau B., Vilnat A., (2005) Campagned'?valuation EQueR-EVALDA : ?valuation enquestion-r?ponse.
In TALN & RECITAL 2005,Tome 2 - Ateliers et tutoriels, pp.
63-72Buchholz S., (2002) Open-Domain QuestionAnswering on the World Wide Web.
Tutorial,http://tcc.itc.it/research/textec/topics/ question-answering/Tut-Bucholtz.html.Kwok C., Etzioni O., Weld D.S., (2001) ScalingQuestion Answering to the Web.
In ProceedingsInternational WWW Conference(10), Hong-KongLaurent D., Varone M., Amaral C., Fuglewicz P.(2004) Multilingual Semantic and CognitiveSearch Engine for Text Retrieval Using SemanticTechnologies, First International Workshop onProofing Tools and Language Technologies,Patras, Gr?ce.Laurent D., S?gu?la P., N?gre S., (2005-1) QRISTAL,syst?me de Questions-R?ponses.
In TALN &RECITAL 2005, Tome 1 - Conf?rences princi-pales, pp.
53-62Laurent D., S?gu?la P., N?gre S., (2005-2) Cross-Lingual Question Answering using Qristal forCLEF 2005.
In Working Notes for the CLEF 2005Workshop, 21-23 September, Vienna, AustriaMcGowan K., (2005) Emma : A Natural LanguageQuestion Answering System for umich.edu.http://www-personal.umich.edu/~clunis/emma/Radev D.R., Qi H., Wu H., Fan W., (2002) EvaluatingWeb-based Question Answering Systems.
In theProceedings of the 11th WWW conference,Hawaii.Richaudeau F., Gauquelin M. et F., (1977) Lam?thode compl?te de lecture rapide "Richaudeau",?d.
Retz-CEPL, Paris.Vallin A. Giampiccolo D., AunimoL., Ayache C.,Osenova P., Pe?as A., de Rijke M., Sacaleanu B.,Santos D., Sutcliffe R. (2005) Overview of theCLEF 2005 Multilingual Question AnsweringTrack.
In Working Notes for the CLEF 2005Workshop, 21-23 September, Vienna, AustriaVoorhees E.M., (2005) Overview of the TREC 2004Question Answering Track.
In Proceedings of theThirteenth Text REtrieval Conference (TREC2004).EACL 2006 Workshop on Multilingual Question Answering - MLQA068
