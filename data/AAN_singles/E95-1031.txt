A Robust  Parser Based on Syntact ic  Informat ionKong Joo Lee Cheol Jung Kweon Jungyun Seo Gil Chang KimDepartment of Computer Scinence and CAIRKorea Advanced Institute of Science and TechnologyTaejon, Korea 305-701{kjlee,cjkwn}@csone.kaist.ac.krAbstractAn extragrammatical sentence is whata normal parser fails to analyze.
It isimportant to recover it using only syn-tactic information although results ofrecovery are better if semantic factorsare considered.
A general algorithm .forleast-errors recognition, which is basedonly on syntactic information, was pro-posed by G. Lyon to deal with the ex-tragrammaticality.
We extended thisalgorithm to recover extragrammaticalsentence into grammatical one in run-ning text.
Our robust parser with re-covery mechanism - extended generalalgorithm for least-errors recognition -can be easily scaled up and modified be-cause it utilize only syntactic informa-tion.
To upgrade this robust parser weproposed heuristics through the analy-sis on the Penn treebank corpus.
Theexperimental result shows 68% ,~ 77%accuracy in error recovery.1 In t roduct ionExtragrammaticM sentences include patently un-grammatical constructions as well as utterancesthat may be grammaticMly acceptable but are be-yond the syntactic overage of a parser, and anyother difficult ones that are encountered in parsing(Carbonell and Hayes, 1983).I am sure this is what  he means.This  is, I am sure, what  he means.The progress of machine does not stop even a day.Not even a day does the progress of machine stop.Above examples how that people are used towrite same meaningful sentences differently.
Inaddition, people are prone to mistakes in writingsentences.
So, the bulk of written sentences areopen to the extragrammaticality.In the Penn treebank tree-tagged cor-pus(Marcus, 1991), for instance, about 80 per-cents of the rules are concerned with peculiarsentences which include inversive, elliptic, paren-thetic, or emphatic phrases.
For example, we candrive a rule VP ---, vb NP comma rb comma PPfrom the following sentence.
((SThe same jealousy can breed confusion,however, in the absence of any authoriza-tion bill this year.
(NP The/dr(ADJP ea~e/ j j )  jealousy/nn) can/md(VP breed/vb(NP confusion/nn) , / ,  however/rb , / ,(PP in / in(NP(NP the/dr absence/nn)(PP orlon(NP anyldt auZhorization/nnbi11/nn))(NP this/dt  yearlnn)))))./.
)A robust parser is one that can analyze theseextragrammaticalsentences without failure.
How-ever, if we try to preserve robustness by addingsuch rules whenever we encounter an extra-grammatical sentence, the rulebase will grow uprapidly, and thus processing and maintaining theexcessive number of rules will become inefficientand impractical.
Therefore, extragrammaticalsentences hould be handled by some recoverymechanism(s) rather than by a set of additionalrules.Many researchers have attempted several tech-niques to deal with extragrammatical sentencessuch as Augmented Transition Network(ATN)(Kwasny and Sondheimer, 1981), network-basedsemantic grammar (Hendrix, 1977), partial pat-tern matching (Hayes and Mouradian, 1981), con-ceptual case frame (Schank et al, 1980), and mul-tiple cooperating methods (Hayes and Carbonell,1981).
Above mentioned techniques take into ac-count various emantic factors depending on spe-cific domains on question in recovering extragram-matical sentences.
W\]lereas they can provide venbetter solutions intrinsically, they are usually ad-hoc and are lack of extensibility.
Therefore, it is223important o recover extragrammatical sentencesusing syntactic factors only, which are indepen-dent of any particular system and any particulardomain.Mellish (Mellish, 1989) introduced some chart-based techniques using only syntactic informationfor extragrammatical sentences.
This techniquehas an advantage that there is no repeating workfor the chart to prevent he parser from generat-ing the same edge as the previously existed edge.Also, because the recovery process runs when anormal parser terminates unsuccessfully, the per-formance of the normal parser does not decrease incase of handling grammatical sentences.
However,his experiment was not based on the errors in run-ning texts but on artificial ones which were ran-domly generated by human.
Moreover, only oneword error was considered though several word er-rors can occur simultaneously in the running text.A general algorithm for least-errors recognition(Lyon, 1974), proposed by G. Lyon, is to find outthe least number of errors necessary to success-ful parsing and recover them.
Because this algo-rithm is also syntactically oriented and baaed ona chart, it has the same advantzrge as that of Mel-lish's parser.
When the original parsing algorithmterminates unsuccessfully, the algorithm begins toassume rrors of insertion, deletion and mutationof a word.
For any input, including grammat-ical and extragrammatical sentences, this algo-rithm can generate the resultant parse tree.
Atthe cost of the complete robustness, however, thisalgorithm degrades the efficiency of parsing, andgenerates many intermediate edges.In this paper, we present a robust parser witha recovery mechanism.
We extend the general al-gorithm for least-errors recognition to adopt it asthe recovery mechanism in our robust parser.
Be-cause our robust parser handle extragrammaticalsentences with this syntactic information orientedrecovery mechanism, it can be independent of aparticular system or particular domain.
Also, wepresent the heuristics to reduce the number ofedges so that we can upgrade the performance ofour parser.This paper is organized as follows : We firstreview a general algorithm for least-errors recog-nition.
Then we present he extension of this al-gorithm, and the heuristics adopted by the robustparser.
Next, we describe the implementation fthe system and the result of the experiment ofparsing real sentences.
Finally, we make conclu-sion with future direction.2 Algorithm and Heuristics2.1 Genera l  a lgor i thm for  leas t -e r rorsrecogn i t ionThe general algorithm for least-errors recognition(Lyon, 1974), which is based on Earley's algo-rithm, assumes that sentences may have insertion,RULE : T ~ aINPUT : t ( i )l nutat ion -er ror  ?," .
.
.
.
.
.
.
.
.
.
.
.
.
".,';" ................ ".
hypothesis .
:' ".?
perfec~ match :deletion-error ~ "' ' ' ," T --* u .
O !hypothe,is  : ':~T --* a.  , 1 } insertion-error .
'~T --*.
a ,  1 i,, ..' hypothes is  ", :"" .
.
.
.
S( i)  .
.
.
.
.
.  ""
t ( i  ) ..... ~'( i+1)  - '""Figure 1: SCAN processingdeletion, and mutation errors of terminal symbols.The objective of this algorithm is to parse inputstring with the least number of errors.A state used in this algorithm is quadruple (p,j, f, e J, where p is a production umber in gram-mar, j marks a position in RHS(p), f is  a start po-sition of the state in input string, and e is an errorvalue.
1 A final state (p, p_-I-1, f, e) denotes recog-nition of a phrase RHS(p) with e errors where _p isa number of components in rule p. A stateset S(i),where i is the position of the input, is an orderedset of states.
States within a stateset are orderedby ascending value of 3", within a p within a f ; ftakes descending value.When adding to statesets, ff state (p, j, f, e)is a candidate for admission to a stateset whichalready has a similar member (p, j, f,, e') and e'_~ e, then (p, j, f, e) is rejected.
However, if e'  >e, then (p, j, f, e') is replaced by (p, j, f, e).The algorithm works as follows : A procedureSCAN is carried out for each state in S(i).
SCANchecks various correspondences of input token t(i)against terminal symbols in RHS of rules.
OnceSCAN is done, COMPLETER substitutes all finalstates of S(i) into all other analyses which can usethem as components.SCANSCAN handles states of S(i), checking each in-put terminal against requirements of states in S(i)and various error hypotheses.
Figure 1 shows howSCAN processes.Let c(p,j) be j-th component of RHS(p) and t(i)be i-th word of input string.?
perfect match :i f  c(p,j) = t(i) then add (p, j+l,  f, e) toS(i+l) if possible.?
insertion-error hypothesis :Add (p, j, f ,  ?- / 'ce i , .
, .er .on)  to S(i-t-1) if pos-sible.a~n,er~ion is the cost of an insertion-error fora terminal symbol.?
de le t ion -er ror  hypothes is  :1 Lyon  sa id  thzLt e is an ezzor  count224I f  c(p,j) is terminal, then add (p, j-l-l, .f,e+OtdeZe.~) to S(i) if possible.od~z~.., is the cost of a deletion-error for aterminal symbol.?
mutation-error hypothesis :I f  c(p,j) is terminal but not equal to t(i), thenadd (p, j+ l, f, e+~,,,to,on) toS(i+ l) if pos-sible.~muta,on is the cost of a mutation-error forS", \[VP-> vb.
NP PPIS' " \[,.NP I \[PP|?
They ~ ~ t h e  r e p o ~s.\[VP->vb NP PP\]< Phrase Perfect Ma~:h >s"= IS-> NP.
md VPIa terminal symbol.2 ~ ~ !
_COMPLETER ~ .
.
-  c~ rles " IneludlngW(mt G@rmany ~mlq "havell hltrd lime,.. ~ .COMPLETER handles ubstitution of final states " k .
/  ,in S(i) like that of original Earley's algorithm.
,-\[s->,P , ,  vP|Each final state means the recognition of a non-terminal.
?
Ph.~ ,..=r~,~-,.r~ .yp~-=, .2.2 Extens ion  o f  leas t -e r rors  recogn i t iona lgor i thmThe algorithm in section 2.1 can analyze any in-put string with the least number of errors.
Butthis algorithm can handle only the errors of termi-nal symbols because it doesn't consider the errorsof nonterminal nodes.
In the real text, however,the insertion, deletion, or inversion of a phrase- namely, nonterminal node - occurs more fre-quently.
So, we extend the original algorithm in"order to handle the errors of nonterminal symbolsas well.In our extended algorithm, the same SCAN asthat of the original algorithm is used, while COM-PLETER is modified and extended.
Figure 2shows the processing of extended-COMPLETER.In figure 2, \[NP\] denotes the final state whose rulehas NP as its LHS.
In other words, it means therecognition of a noun phrase.extended-COMPLETERIf there is a final state s' = (p',p~ + 1, k, e') inS(i),?
phrase perfect matchI f  there exists a state s" = (p, j ,  x, e) in S(k), t < i and j )  = L /S(f) then add s =(p, j + 1, z, e + e') into S(i).?
phrase insertion-error hypothesis aI f  there exists a state s" = (p, j, z, e) in S(k)then add s = (p, j ,z ,e+/~,, ,r ,o,)  into S(i)if possible./Yinaertion is the cost of a insertion-error for anonterminal symbol.2ain.ertion, Otdeletion , Ofmutation lEES st|\] strictly 1 inLyon 's  o r i~-~l  p~per~In fact, there axe cases that an inserted phrasecannot be constructed to form a nonterminal node.
Inphrase insertion-error hypothesis of figure 2, the orig-inal sentence is ~Other countries, including West Ger-many, m~y hgve .
.
. '
,  where the inserted phrase VPis surrounded by commas.
So, the substring( commaV~ comma ) should be dealt with as a constituentin extended-COMPLETER.
In fact, we implementedthe algorithm to allow substring insertions ~, well asinsertions of nontermlnal nodes.S"- IS-> NP.
VP PP\]~ s e e m ,  r n s d ~  \[VP' \[PP\] / ~  S' -,PP \].ands= \[S-> NP VP.PP|< Phrm?
I~eaon-error Ilypoe,cds >Figure 2: Examples of extended-COMPLETERprocessing?
phrase deletion-error hypothesisI f  there exists a state s" = (p, j, z, e) in S(k)and e(p,j) is a nonterminal then add s :(p, j "~- 1, Z, e "\]'t~dele|ion) into S(k) if possible.~dele,~ is the cost of a deletion-error for anonterminal symbol.?
phrase mutation-error hypothesis 4I f  there ~ts  a state 8" = (V, J, x, e) in S(k)and c(p, j) is a nonterminal but not equal toL(p') then add s = (p, j + 1, z, e + ~me*a.o.
)into S(i) if possible.~m.ta.o.
is the cost of a mutation-error fora nonterminM symbol.The extended least-errors recognition algorithmcan handle not only terminal errors but also non-terminal errors.2.3 Heur i s t i csThe robust pa~ser using the extended least-errorsrecognition algorithm overgenerates many error-hypothesis edges during parsing process.
To copewith this problem, we adjust error values accord-ing to the following heuristics.
Edges with moreerror values are regarded as less important ones,so that those edges are processed later than thoseof less error values.tWe know that the phrase mutation-error hypothe-sis is not meaningful in the red text because we cannotfind out any example of phrase mutation-error in thecorpus.
So we didn't implement the phrase mutation-error hypothesis.225?
Heur i s t i cs  1: e r ro r  typesThe analysis on 3,538 sentences of the Penntreebank corpus WS:I shows that there are498 sentences with phrase deletions and 224sentences with phrase insertions.
So, weassign less error value to the deletion-errorhypothesis edge than to the insertion- andmutation-errors.a <~~deletion <= Oeinsertion ~ ~mutat ion~deletion ~ ~insergiortwhere ~ is the error cost of a terminal sym-bol,/~ is the error cost of a nonterminal sym-bol.?
Heur i s t i cs  2: f iduc ia l  nontermina lPeople often make mistakes in writing En-glish.
These mistakes usually take placerather between small constituents uch asa verbal phrase, an adverbial phrase andnoun phrase than within small constituentsthemselves.
The possibility of error occur-rence within noun phrases are lower than be-tween a noun phrase and a verbal phrase,a preposition phrase, an adverbial phrase.So, we assume some phrases, for examplenoun phrases, as fiducial nonterminals, whichmeans error-free nonterminals.
When han-dling sentences, the robust parser assingsmore error values(61) to the error hypothesisedge occurring within a fiducial nonterminal.?
Heur i s t i cs  3: k inds  o f  te rmina l  symbo lsSome terminal symbols like punctuationsymbols, conjunctions and particles are of-ten misused.
So, the robust parser assignsless error values(-52) to the error hypothesisedges with these symbols than to the otherterminal symbols.?
Heur i s t i cs  4:  i nser ted  phrases  betweencommas or  parenthesesMost of inserted phrases are surrounded bycommas or parentheses.
For example,a.
They're active , genera l ly  , at night or ondamp, cloudy days.b.
All refrigerators , whether they are defrostedmanually or not ,  need to be cleaned.c.
I was a last-minute ( read intedopin9 ) at-tendee at a French journalism convention .-.We will assign less error values(-6a) to theinsertion-error hypothesis edges of nontermi-nals which are embraced by comma or paren-thesis.61 and 62 are weights for the error of terminalnodes, and 68 is a weight for the error of nonter-minal nodes.The error value e of an edge is calculated asfollows.
All error values are additive.The error value e for a rule X ~ a lA la2 .
.
,  a~Aj,where a is a terminal node and A is a nonterminalnode, is1.
e= eT +e + 61 - 62 if terminal error2.
eT : 0 otherwise{ /~ - 6s -t- ech,d if nonterminal 3. eNT - - "  errorechild otherwisewhere a E {ain..r:ion, adele.on, amutation}, fl E{/~in.er.o.,/~&/etion} and ech.d is an error valueof a child edge.By these heuristics, our robust parser can pro-cess only plausible edges first, inste~i of process-ing all generated edges at the same time, so thatwe can enhance the performance of the robustparser and result in the great reduction in thenumber of resultant rees.3 Imp lementat ion  and  Eva luat ion3.1 The  robust  parserOur robust parsing system is composed of twomodules.
One module is a normal parser whichis the bottom-up chart parser.
The other is arobust parser with the error recovery mechanismproposed herein.
At first, an input sentence isprocessed by the normal parser.
I f  the sentenceis within the grammatical  coverage of the system,the normal parser succeed to analyze it.
Other-wise, the normal parser fails, and then the robustparser starts to execute with edges generated bythe normal parser.
The result of the robust parseris the parse trees which are within the grammat-ical coverage of the system.
The overview of thesystem is shown in figure 3.. ,  t !
, , .
.
o , ,Figure 3: The overview of the system3.2 Exper imenta l  resu l tTo show usefulness of the robust parser proposedin this paper, we made some experiments.?
RuleWe can derive 4,958 rules and their frequen-cies out of 14,137 sentences in the Penn226Table 1: The results of the robust parser on WSJExperiment 1 : WSJ 410 sentenceswith Heuristics without HeuristicsAverage sentence lengthAverage processing timeAverage number of edgesAccuracy (%)no-crossing sentences% of < 1-crossing sentences% of < 2-crossing sentences16.27 words (2-25 words)6.52 sec7726.0377.123.28%40.52%55.17%16.27 words (2-25 words)22.47 sec10346.672.820.28%37.14%48.57%treebank tree-tagged corpus, the Wall StreetJournal.
The average frequency of each ruleis 48 times in the corpus.
Of these rules, weremove rules which occurs fewer times thanthe average frequency in the corpus, and thenonly 192 rules are left.
These removed rulesare almost for peculiar sentences and the leftrules are very general rules.
We can showthat our robust parser can compensate forlack of rules using only 192 rules with therecovery mechanism and heuristics.?
Test setFirst, 1,000 sentences are selected randomlyfrom the WSJ corpus, which we have referredto in proposing the robust parser.
Of thesesentences, 410 are failed in normal parsing,and are processed again by the robust parser.To show the validity of these heuristics, wecompare the result of the robust parser us-ing heuristics with one not using heuristics.Second, to show the adaptability of our ro-bust parser, same experiments are carriedout on 1,000 sentences from the ATIS cor-pus in Penn treebank, which we haven't re-ferred to when we propose the robust parser.Among 1,000 sentences from the ATIS, 465sentences are processed by the robust parserafter the failure of the normal parsing.?
Parameter  adjustmentWe chose the best parameters of heuristicsby executing several experiments.c~in0ert i~ : 10.2 fii,~,e~t~on : 15.0Ctd~:~,io, : 10.4 ~a,~,t~ : 20.0otmuto.~ : 10.861 : 0.01 62 : 5.0~3 : 1.0Accuracy is measured as the percentage of con-stituents in the test sentences which do not crossany Penn treebank constituents (Black, 1991).Table 1 shows the results of the robust parseron WSJ.
In table 1, 5th, 6th and 7th raw meanthat the percentage of sentences which have nocrossing constituents, less than one crossing andless than two crossing respectively.
With heuris-tics, our robust parser can enhance the processingtime and reduce the number of edges.
Also, theaccuracy is improved from 72.8% to 77.1% even ifthe heuristics differentiate edges and prefer someedges.
It shows that the proposed heuristics isvalid in parsing the real sentences.
The experi-ment says that our robust parser with heuristicscan recover perfectly about 23 sentences out of 100sentences which axe just failed in normal parsing,as the percentage ofno-crossing sentences i about23.28%.Table 2 is the results of the robust parser onATIS which we did not refer to before.
The accu-racy of the result on ATIS is lower than WSJ be-cause the parameters ofthe heuristics are a~justednot by ATIS itself but by WSJ.
However, thepercentage ofsentences with constituents crossingless than 2 is higher than the WSJ, as sentencesof ATIS are more or less simple.The experimental results of our robust parsershow high accuracy in recovery even though 96%of total rules are removed.
It is impossible to con-struct complete grammar rules in the real parsingsystem to succeed in analyzing every real sentence.So, parsing systems are likely to have extragram-matical sentences which cannot be analyzed bythe systems.
Our robust parser can recover theseextragrammatical sentences with 68 ~ 77% accu-racy.It is very interesting that parameters of heuris-tics reflect the characteristics of the test corpus.For example, if people tend to write sentences withinserted phrases, then the parameter fli,~sert~onmust increase.
Therefore we can get better esultsif the parameter are fitted to the characteristics ofthe corpus.4 ConclusionIn this paper, we have presented the robust parserwith the extended least-errors recognition algo-rithm as the recovery mechanism.
This robustparser can easily be scaled up and applied to var-ious domains because this parser depends only onsyntactic factors.
To enhance the performance ofthe robust parser for extragrainmatical sentences,227Table 2: The results of the robust parser on ATISExperiment 2 : ATIS 465 sentencesAverage sentence l ngthAverage processing timeAverage number of edgesAccuracy (%)no-crossing sentences% of _< 1-crossing sentences% of <_ 2-crossing sentenceswith Heuristics without Heuristics10.55 words (2-25 words)8.68 sec12974.268.526.02%47.10%66.24%10.55 words (2-25 words)71.98 sec25652.559.413.28%36.06%52.46%we proposed several heuristics.
The heuristics as-sign the error values to each error-hypothesis edge,and edges which has less error values axe processedfirst.
So, not all the generated edges are processedby the robust parser, but the most plausible parsetrees can be generated first.
The accuracy of therecovery in our robust parser is about 68% ,,~ 77~.Hence, this parser is suitable for systems in realapplication areas.Our short term goal is to propose an automaticmethod that can learn parameter values of heuris-tics by analyzing the corpus.
We expect hat au-tomatically leaxned values of parameters can up-grade the performance of the parser.AcknowledgementThis work was supported(in part) by KoreaScience and Engineering Foundation(KOSEF)through Center for Artificial Intelligence Ke-search(CAIR), the Engineering Research Cen-ter(EKC) of Excellence Program.References\[Black, 1991\] E. Black eta l .
A Procedure forquantitatively comparing the syntactic over-age of English grammars.
Proceedings of FourthDARPA Speech and Natural Language Work-shop, pp.
306-311, 1991.\[Carbonell and Hayes, 1983\] J. G. Carbonell andP.
:I. Hayes.
Recovery Strategies for ParsingExtragrsmmatical Language.
American Jour-nal of Computational Linguistics, vol.
9, no.
3-4, pp.
123-146, 1983.\[Hayes and C~rbonell, 1981\] P. Hayes and J. Car-bonell.
Multi-strategy Construction-SpecificParsing for Flexible Data Base Query Update.Proceedings of the 7th International Joint Con-ference on Artificial Intelligence, pp.
432-439,1981.\[Hayes and Mouradian, 1981\] P. J. Hayes andG.
V. Mouradian.
Flexible Parsing.
AmericanJournal of Computational Linguistics, vol.
7,no.
4, pp.
232-242, 1981.\[Hendrix, 1977\] G. Hendrix.
Human Engineer-ing for Applied Natural Language Processing.Proceedings of the 5th International Joint Con-ference on Artificial Intelligence, pp.
183-191,1977.\[Kwasny and Sondheimer, 1981\] S. Kwasnyand N. Sondheimer.
Relaxation Techniquesfor Parsing Grammatically Ill-Formed Inputin Natural Language Understanding Systems.American Journal of Computational Linguis-tics, vol.
7, no.
2, pp.
99-108, 1981.\[Lyon, 1974\] G. Lyon.
Syntax-Directed Least-Errors Analysis for Context-Free Languages.Communications of the ACM, vol.
17, no.
1,pp.
3-14, 1974.\[Marcus, 1991\] M. P. Marcus.
Building very Largenatural language corpora : the Penn Treebank,1991.\[Mellish, 1989\] C. S. Mellish.
Some Chart-BasedTechniques for Parsing Ill-Formed Input.
Asso-ciation for Computational Linguistics, pp.
102-109, 1989.\[Schank etal.,  1980\] R. C. Schank, M. Lebowitzand L. Brinbaum.
An Intergrated Under-stander.
American ?ournal of ComputationalLinguistics, vol.
6, no.
1, pp.
13-30, 1980.228
