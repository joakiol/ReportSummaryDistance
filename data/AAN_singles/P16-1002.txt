Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 12?22,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsData Recombination for Neural Semantic ParsingRobin JiaComputer Science DepartmentStanford Universityrobinjia@stanford.eduPercy LiangComputer Science DepartmentStanford Universitypliang@cs.stanford.eduAbstractModeling crisp logical regularities is cru-cial in semantic parsing, making it difficultfor neural models with no task-specificprior knowledge to achieve good results.In this paper, we introduce data recom-bination, a novel framework for inject-ing such prior knowledge into a model.From the training data, we induce a high-precision synchronous context-free gram-mar, which captures important conditionalindependence properties commonly foundin semantic parsing.
We then train asequence-to-sequence recurrent network(RNN) model with a novel attention-basedcopying mechanism on datapoints sam-pled from this grammar, thereby teachingthe model about these structural proper-ties.
Data recombination improves the ac-curacy of our RNN model on three se-mantic parsing datasets, leading to newstate-of-the-art performance on the stan-dard GeoQuery dataset for models withcomparable supervision.1 IntroductionSemantic parsing?the precise translation of nat-ural language utterances into logical forms?hasmany applications, including question answer-ing (Zelle and Mooney, 1996; Zettlemoyer andCollins, 2005; Zettlemoyer and Collins, 2007;Liang et al, 2011; Berant et al, 2013), instruc-tion following (Artzi and Zettlemoyer, 2013b),and regular expression generation (Kushman andBarzilay, 2013).
Modern semantic parsers (Artziand Zettlemoyer, 2013a; Berant et al, 2013)are complex pieces of software, requiring hand-crafted features, lexicons, and grammars.Meanwhile, recurrent neural networks (RNNs)what are the major cities in utah ?what states border maine ?Original ExamplesTrain ModelSequence-to-sequence RNNSample New ExamplesSynchronous CFGInduce Grammarwhat are the major cities in [states border [maine]] ?
what are the major cities in [states border [utah]]  ?what states border [states border [maine]] ?what states border [states border [utah]] ?Recombinant ExamplesFigure 1: An overview of our system.
Given adataset, we induce a high-precision synchronouscontext-free grammar.
We then sample from thisgrammar to generate new ?recombinant?
exam-ples, which we use to train a sequence-to-sequenceRNN.have made swift inroads into many structured pre-diction tasks in NLP, including machine trans-lation (Sutskever et al, 2014; Bahdanau et al,2014) and syntactic parsing (Vinyals et al, 2015b;Dyer et al, 2015).
Because RNNs make very fewdomain-specific assumptions, they have the poten-tial to succeed at a wide variety of tasks with min-imal feature engineering.
However, this flexibil-ity also puts RNNs at a disadvantage comparedto standard semantic parsers, which can generalizenaturally by leveraging their built-in awareness oflogical compositionality.In this paper, we introduce data recombina-tion, a generic framework for declaratively inject-12GEOx: ?what is the population of iowa ?
?y: _answer ( NV , (_population ( NV , V1 ) , _const (V0 , _stateid ( iowa ) ) ) )ATISx: ?can you list all flights from chicago to milwaukee?y: ( _lambda $0 e ( _and( _flight $0 )( _from $0 chicago : _ci )( _to $0 milwaukee : _ci ) ) )Overnightx: ?when is the weekly standup?y: ( call listValue ( callgetProperty meeting.weekly_standup( string start_time ) ) )Figure 2: One example from each of our domains.We tokenize logical forms as shown, thereby cast-ing semantic parsing as a sequence-to-sequencetask.ing prior knowledge into a domain-general struc-tured prediction model.
In data recombination,prior knowledge about a task is used to build ahigh-precision generative model that expands theempirical distribution by allowing fragments ofdifferent examples to be combined in particularways.
Samples from this generative model arethen used to train a domain-general model.
In thecase of semantic parsing, we construct a genera-tive model by inducing a synchronous context-freegrammar (SCFG), creating new examples suchas those shown in Figure 1; our domain-generalmodel is a sequence-to-sequence RNN with anovel attention-based copying mechanism.
Datarecombination boosts the accuracy of our RNNmodel on three semantic parsing datasets.
On theGEO dataset, data recombination improves test ac-curacy by 4.3 percentage points over our baselineRNN, leading to new state-of-the-art results formodels that do not use a seed lexicon for predi-cates.2 Problem statementWe cast semantic parsing as a sequence-to-sequence task.
The input utterance x is a sequenceof words x1, .
.
.
, xm?
V(in), the input vocabulary;similarly, the output logical form y is a sequenceof tokens y1, .
.
.
, yn?
V(out), the output vocab-ulary.
A linear sequence of tokens might appearto lose the hierarchical structure of a logical form,but there is precedent for this choice: Vinyals et al(2015b) showed that an RNN can reliably predicttree-structured outputs in a linear fashion.We evaluate our system on three existing se-mantic parsing datasets.
Figure 2 shows sampleinput-output pairs from each of these datasets.?
GeoQuery (GEO) contains natural languagequestions about US geography paired withcorresponding Prolog database queries.
Weuse the standard split of 600 training exam-ples and 280 test examples introduced byZettlemoyer and Collins (2005).
We prepro-cess the logical forms to De Brujin index no-tation to standardize variable naming.?
ATIS (ATIS) contains natural languagequeries for a flights database paired withcorresponding database queries written inlambda calculus.
We train on 4473 examplesand evaluate on the 448 test examples usedby Zettlemoyer and Collins (2007).?
Overnight (OVERNIGHT) contains logicalforms paired with natural language para-phrases across eight varied subdomains.Wang et al (2015) constructed the dataset bygenerating all possible logical forms up tosome depth threshold, then getting multiplenatural language paraphrases for each logi-cal form from workers on Amazon Mechan-ical Turk.
We evaluate on the same train/testsplits as Wang et al (2015).In this paper, we only explore learning from log-ical forms.
In the last few years, there has anemergence of semantic parsers learned from de-notations (Clarke et al, 2010; Liang et al, 2011;Berant et al, 2013; Artzi and Zettlemoyer, 2013b).While our system cannot directly learn from deno-tations, it could be used to rerank candidate deriva-tions generated by one of these other systems.3 Sequence-to-sequence RNN ModelOur sequence-to-sequence RNN model is basedon existing attention-based neural machine trans-lation models (Bahdanau et al, 2014; Luong et al,2015a), but also includes a novel attention-basedcopying mechanism.
Similar copying mechanismshave been explored in parallel by Gu et al (2016)and Gulcehre et al (2016).3.1 Basic ModelEncoder.
The encoder converts the input se-quence x1, .
.
.
, xminto a sequence of context-13sensitive embeddings b1, .
.
.
, bmusing a bidirec-tional RNN (Bahdanau et al, 2014).
First, a wordembedding function ?
(in)maps each word xito afixed-dimensional vector.
These vectors are fed asinput to two RNNs: a forward RNN and a back-ward RNN.
The forward RNN starts with an initialhidden state hF0, and generates a sequence of hid-den states hF1, .
.
.
, hFmby repeatedly applying therecurrencehFi= LSTM(?
(in)(xi), hFi?1).
(1)The recurrence takes the form of an LSTM(Hochreiter and Schmidhuber, 1997).
The back-ward RNN similarly generates hidden stateshBm, .
.
.
, hB1by processing the input sequence inreverse order.
Finally, for each input position i,we define the context-sensitive embedding bito bethe concatenation of hFiand hBiDecoder.
The decoder is an attention-basedmodel (Bahdanau et al, 2014; Luong et al, 2015a)that generates the output sequence y1, .
.
.
, ynonetoken at a time.
At each time step j, it writesyjbased on the current hidden state sj, then up-dates the hidden state to sj+1based on sjand yj.Formally, the decoder is defined by the followingequations:s1= tanh(W(s)[hFm, hB1]).
(2)eji= s>jW(a)bi.
(3)?ji=exp(eji)?mi?=1exp(eji?).
(4)cj=m?i=1?jibi.
(5)P (yj= w | x, y1:j?1) ?
exp(Uw[sj, cj]).
(6)sj+1= LSTM([?
(out)(yj), cj], sj).
(7)When not specified, i ranges over {1, .
.
.
,m} andj ranges over {1, .
.
.
, n}.
Intuitively, the ?ji?s de-fine a probability distribution over the input words,describing what words in the input the decoder isfocusing on at time j.
They are computed fromthe unnormalized attention scores eji.
The matri-ces W(s), W(a), and U , as well as the embeddingfunction ?
(out), are parameters of the model.3.2 Attention-based CopyingIn the basic model of the previous section, the nextoutput word yjis chosen via a simple softmax overall words in the output vocabulary.
However, thismodel has difficulty generalizing to the long tail ofentity names commonly found in semantic parsingdatasets.
Conveniently, entity names in the inputoften correspond directly to tokens in the output(e.g., ?iowa?
becomes iowa in Figure 2).1To capture this intuition, we introduce a newattention-based copying mechanism.
At each timestep j, the decoder generates one of two types ofactions.
As before, it can write any word in theoutput vocabulary.
In addition, it can copy any in-put word xidirectly to the output, where the prob-ability with which we copy xiis determined bythe attention score on xi.
Formally, we define alatent action ajthat is either Write[w] for somew ?
V(out)or Copy[i] for some i ?
{1, .
.
.
,m}.We then haveP (aj= Write[w] | x, y1:j?1) ?
exp(Uw[sj, cj]),(8)P (aj= Copy[i] | x, y1:j?1) ?
exp(eji).
(9)The decoder chooses ajwith a softmax over allthese possible actions; yjis then a deterministicfunction of ajand x.
During training, we maxi-mize the log-likelihood of y, marginalizing out a.Attention-based copying can be seen as a com-bination of a standard softmax output layer of anattention-based model (Bahdanau et al, 2014) anda Pointer Network (Vinyals et al, 2015a); in aPointer Network, the only way to generate outputis to copy a symbol from the input.4 Data Recombination4.1 MotivationThe main contribution of this paper is a novel datarecombination framework that injects importantprior knowledge into our oblivious sequence-to-sequence RNN.
In this framework, we induce ahigh-precision generative model from the trainingdata, then sample from it to generate new trainingexamples.
The process of inducing this generativemodel can leverage any available prior knowledge,which is transmitted through the generated exam-ples to the RNN model.
A key advantage of ourtwo-stage approach is that it allows us to declaredesired properties of the task which might be hardto capture in the model architecture.1On GEO and ATIS, we make a point not to rely on or-thography for non-entities such as ?state?
to _state, sincethis leverages information not available to previous models(Zettlemoyer and Collins, 2005) and is much less language-independent.14Examples(?what states border texas ?
?,answer(NV, (state(V0), next_to(V0, NV), const(V0, stateid(texas)))))(?what is the highest mountain in ohio ?
?,answer(NV, highest(V0, (mountain(V0), loc(V0, NV), const(V0, stateid(ohio))))))Rules created by ABSENTITIESROOT?
?
?what states border STATEID ?
?,answer(NV, (state(V0), next_to(V0, NV), const(V0, stateid(STATEID ))))?STATEID?
?
?texas?, texas ?ROOT?
?
?what is the highest mountain in STATEID ?
?,answer(NV, highest(V0, (mountain(V0), loc(V0, NV),const(V0, stateid(STATEID )))))?STATEID?
?
?ohio?, ohio?Rules created by ABSWHOLEPHRASESROOT?
?
?what states border STATE ?
?, answer(NV, (state(V0), next_to(V0, NV), STATE ))?STATE?
?
?states border texas?, state(V0), next_to(V0, NV), const(V0, stateid(texas))?ROOT?
?
?what is the highest mountain in STATE ?
?,answer(NV, highest(V0, (mountain(V0), loc(V0, NV), STATE )))?Rules created by CONCAT-2ROOT?
?SENT1</s> SENT2, SENT1</s> SENT2?SENT?
?
?what states border texas ?
?,answer(NV, (state(V0), next_to(V0, NV), const(V0, stateid(texas)))) ?SENT?
?
?what is the highest mountain in ohio ?
?,answer(NV, highest(V0, (mountain(V0), loc(V0, NV), const(V0, stateid(ohio))))) ?Figure 3: Various grammar induction strategies illustrated on GEO.
Each strategy converts the rules ofan input grammar into rules of an output grammar.
This figure shows the base case where the inputgrammar has rules ROOT ?
?x, y?
for each (x, y) pair in the training dataset.Our approach generalizes data augmentation,which is commonly employed to inject priorknowledge into a model.
Data augmenta-tion techniques focus on modeling invariances?transformations like translating an image oradding noise that alter the inputs x, but do notchange the output y.
These techniques haveproven effective in areas like computer vision(Krizhevsky et al, 2012) and speech recognition(Jaitly and Hinton, 2013).In semantic parsing, however, we would like tocapture more than just invariance properties.
Con-sider an example with the utterance ?what statesborder texas ??.
Given this example, it should beeasy to generalize to questions where ?texas?
isreplaced by the name of any other state: simplyreplace the mention of Texas in the logical formwith the name of the new state.
Underlying thisphenomenon is a strong conditional independenceprinciple: the meaning of the rest of the sentenceis independent of the name of the state in ques-tion.
Standard data augmentation is not sufficientto model such phenomena: instead of holding yfixed, we would like to apply simultaneous trans-formations to x and y such that the new x stillmaps to the new y.
Data recombination addressesthis need.4.2 General SettingIn the general setting of data recombination, westart with a training set D of (x, y) pairs, whichdefines the empirical distribution p?
(x, y).
We thenfit a generative model p?
(x, y) to p?
which gener-alizes beyond the support of p?, for example bysplicing together fragments of different examples.We refer to examples in the support of p?
as re-combinant examples.
Finally, to train our actualmodel p?
(y | x), we maximize the expected valueof log p?
(y | x), where (x, y) is drawn from p?.4.3 SCFGs for Semantic ParsingFor semantic parsing, we induce a synchronouscontext-free grammar (SCFG) to serve as thebackbone of our generative model p?.
An SCFGconsists of a set of production rules X ?
?
?, ?
?,whereX is a category (non-terminal), and ?
and ?are sequences of terminal and non-terminal sym-bols.
Any non-terminal symbols in ?
must bealigned to the same non-terminal symbol in ?, andvice versa.
Therefore, an SCFG defines a set ofjoint derivations of aligned pairs of strings.
In ourcase, we use an SCFG to represent joint deriva-15tions of utterances x and logical forms y (whichfor us is just a sequence of tokens).
After weinduce an SCFG G from D, the correspondinggenerative model p?
(x, y) is the distribution overpairs (x, y) defined by sampling from G, wherewe choose production rules to apply uniformly atrandom.It is instructive to compare our SCFG-baseddata recombination with WASP (Wong andMooney, 2006; Wong and Mooney, 2007), whichuses an SCFG as the actual semantic parsingmodel.
The grammar induced by WASP must havegood coverage in order to generalize to new in-puts at test time.
WASP also requires the imple-mentation of an efficient algorithm for computingthe conditional probability p(y | x).
In contrast,our SCFG is only used to convey prior knowl-edge about conditional independence structure, soit only needs to have high precision; our RNNmodel is responsible for boosting recall over theentire input space.
We also only need to forwardsample from the SCFG, which is considerably eas-ier to implement than conditional inference.Below, we examine various strategies for induc-ing a grammar G from a dataset D. We first en-code D as an initial grammar with rules ROOT?
?x, y?
for each (x, y) ?
D. Next, we willdefine each grammar induction strategy as a map-ping from an input grammar Ginto a new gram-mar Gout.
This formulation allows us to composegrammar induction strategies (Section 4.3.4).4.3.1 Abstracting EntitiesOur first grammar induction strategy, ABSENTI-TIES, simply abstracts entities with their types.We assume that each entity e (e.g., texas) hasa corresponding type e.t (e.g., state), which weinfer based on the presence of certain predicatesin the logical form (e.g.
stateid).
For eachgrammar rule X ?
?
?, ??
in Gin, where ?
con-tains a token (e.g., ?texas?)
that string matchesan entity (e.g., texas) in ?, we add two rulesto Gout: (i) a rule where both occurrences are re-placed with the type of the entity (e.g., state),and (ii) a new rule that maps the type to the en-tity (e.g., STATEID ?
??texas?,texas?
; we re-serve the category name STATE for the next sec-tion).
Thus, Goutgenerates recombinant examplesthat fuse most of one example with an entity foundin a second example.
A concrete example from theGEO domain is given in Figure 3.4.3.2 Abstracting Whole PhrasesOur second grammar induction strategy, ABSW-HOLEPHRASES, abstracts both entities and wholephrases with their types.
For each grammar ruleX ?
?
?, ??
in Gin, we add up to two rules toGout.
First, if ?
contains tokens that string matchto an entity in ?, we replace both occurrences withthe type of the entity, similarly to rule (i) from AB-SENTITIES.
Second, if we can infer that the entireexpression ?
evaluates to a set of a particular type(e.g.
state) we create a rule that maps the typeto ?
?, ??.
In practice, we also use some simplerules to strip question identifiers from ?, so thatthe resulting examples are more natural.
Again,refer to Figure 3 for a concrete example.This strategy works because of a more generalconditional independence property: the meaningof any semantically coherent phrase is condition-ally independent of the rest of the sentence, thecornerstone of compositional semantics.
Note thatthis assumption is not always correct in general:for example, phenomena like anaphora that in-volve long-range context dependence violate thisassumption.
However, this property holds in mostexisting semantic parsing datasets.4.3.3 ConcatenationThe final grammar induction strategy is a surpris-ingly simple approach we tried that turns out towork.
For any k ?
2, we define the CONCAT-kstrategy, which creates two types of rules.
First,we create a single rule that has ROOT going toa sequence of k SENT?s.
Then, for each root-level rule ROOT ?
?
?, ??
in Gin, we add the ruleSENT ?
?
?, ??
to Gout.
See Figure 3 for an ex-ample.Unlike ABSENTITIES and ABSWHOLE-PHRASES, concatenation is very general, and canbe applied to any sequence transduction problem.Of course, it also does not introduce additionalinformation about compositionality or indepen-dence properties present in semantic parsing.However, it does generate harder examples for theattention-based RNN, since the model must learnto attend to the correct parts of the now-longerinput sequence.
Related work has shown thattraining a model on more difficult examples canimprove generalization, the most canonical casebeing dropout (Hinton et al, 2012; Wager et al,2013).16function TRAIN(dataset D, number of epochs T ,number of examples to sample n)Induce grammar G from DInitialize RNN parameters ?
randomlyfor each iteration t = 1, .
.
.
, T doCompute current learning rate ?tInitialize current dataset Dtto Dfor i = 1, .
.
.
, n doSample new example (x?, y?)
from GAdd (x?, y?)
to Dtend forShuffle Dtfor each example (x, y) in Dtdo?
?
?
+ ?t?
log p?
(y | x)end forend forend functionFigure 4: The training procedure with data recom-bination.
We first induce an SCFG, then samplenew recombinant examples from it at each epoch.4.3.4 CompositionWe note that grammar induction strategies canbe composed, yielding more complex grammars.Given any two grammar induction strategies f1and f2, the composition f1?
f2is the grammarinduction strategy that takes in Ginand returnsf1(f2(Gin)).
For the strategies we have defined,we can perform this operation symbolically on thegrammar rules, without having to sample from theintermediate grammar f2(Gin).5 ExperimentsWe evaluate our system on three domains: GEO,ATIS, and OVERNIGHT.
For ATIS, we reportlogical form exact match accuracy.
For GEO andOVERNIGHT, we determine correctness based ondenotation match, as in Liang et al (2011) andWang et al (2015), respectively.5.1 Choice of Grammar Induction StrategyWe note that not all grammar induction strate-gies make sense for all domains.
In particular,we only apply ABSWHOLEPHRASES to GEO andOVERNIGHT.
We do not apply ABSWHOLE-PHRASES to ATIS, as the dataset has little nestingstructure.5.2 Implementation DetailsWe tokenize logical forms in a domain-specificmanner, based on the syntax of the formal lan-guage being used.
On GEO and ATIS, we dis-allow copying of predicate names to ensure a faircomparison to previous work, as string matchingbetween input words and predicate names is notcommonly used.
We prevent copying by prepend-ing underscores to predicate tokens; see Figure 2for examples.On ATIS alone, when doing attention-basedcopying and data recombination, we leveragean external lexicon that maps natural languagephrases (e.g., ?kennedy airport?)
to entities (e.g.,jfk:ap).
When we copy a word that is part ofa phrase in the lexicon, we write the entity asso-ciated with that lexicon entry.
When performingdata recombination, we identify entity alignmentsbased on matching phrases and entities from thelexicon.We run all experiments with 200 hidden unitsand 100-dimensional word vectors.
We initial-ize all parameters uniformly at random withinthe interval [?0.1, 0.1].
We maximize the log-likelihood of the correct logical form usingstochastic gradient descent.
We train the modelfor a total of 30 epochs with an initial learning rateof 0.1, and halve the learning rate every 5 epochs,starting after epoch 15.
We replace word vectorsfor words that occur only once in the training setwith a universal <unk> word vector.
Our modelis implemented in Theano (Bergstra et al, 2010).When performing data recombination, we sam-ple a new round of recombinant examples fromour grammar at each epoch.
We add these ex-amples to the original training dataset, randomlyshuffle all examples, and train the model for theepoch.
Figure 4 gives pseudocode for this trainingprocedure.
One important hyperparameter is howmany examples to sample at each epoch: we foundthat a good rule of thumb is to sample as many re-combinant examples as there are examples in thetraining dataset, so that half of the examples themodel sees at each epoch are recombinant.At test time, we use beam search with beam size5.
We automatically balance missing right paren-theses by adding them at the end.
On GEO andOVERNIGHT, we then pick the highest-scoringlogical form that does not yield an executor errorwhen the corresponding denotation is computed.On ATIS, we just pick the top prediction on thebeam.5.3 Impact of the Copying MechanismFirst, we measure the contribution of the attention-based copying mechanism to the model?s overall17GEO ATIS OVERNIGHTNo Copying 74.6 69.9 76.7With Copying 85.0 76.3 75.8Table 1: Test accuracy on GEO, ATIS, andOVERNIGHT, both with and without copying.
OnOVERNIGHT, we average across all eight domains.GEO ATISPrevious WorkZettlemoyer and Collins (2007) 84.6Kwiatkowski et al (2010) 88.9Liang et al (2011)291.1Kwiatkowski et al (2011) 88.6 82.8Poon (2013) 83.5Zhao and Huang (2015) 88.9 84.2Our ModelNo Recombination 85.0 76.3ABSENTITIES 85.4 79.9ABSWHOLEPHRASES 87.5CONCAT-2 84.6 79.0CONCAT-3 77.5AWP + AE 88.9AE + C2 78.8AWP + AE + C2 89.3AE + C3 83.3Table 2: Test accuracy using different data recom-bination strategies on GEO and ATIS.
AE is AB-SENTITIES, AWP is ABSWHOLEPHRASES, C2 isCONCAT-2, and C3 is CONCAT-3.performance.
On each task, we train and evalu-ate two models: one with the copying mechanism,and one without.
Training is done without data re-combination.
The results are shown in Table 1.On GEO and ATIS, the copying mechanismhelps significantly: it improves test accuracy by10.4 percentage points on GEO and 6.4 pointson ATIS.
However, on OVERNIGHT, adding thecopying mechanism actually makes our modelperform slightly worse.
This result is somewhatexpected, as the OVERNIGHT dataset contains avery small number of distinct entities.
It is alsonotable that both systems surpass the previous bestsystem on OVERNIGHT by a wide margin.We choose to use the copying mechanism in allsubsequent experiments, as it has a large advan-tage in realistic settings where there are many dis-tinct entities in the world.
The concurrent work ofGu et al (2016) and Gulcehre et al (2016), both ofwhom propose similar copying mechanisms, pro-vides additional evidence for the utility of copyingon a wide range of NLP tasks.5.4 Main Results2The method of Liang et al (2011) is not comparable toFor our main results, we train our model with a va-riety of data recombination strategies on all threedatasets.
These results are summarized in Tables 2and 3.
We compare our system to the baseline ofnot using any data recombination, as well as tostate-of-the-art systems on all three datasets.We find that data recombination consistentlyimproves accuracy across the three domains weevaluated on, and that the strongest results comefrom composing multiple strategies.
Combin-ing ABSWHOLEPHRASES, ABSENTITIES, andCONCAT-2 yields a 4.3 percentage point improve-ment over the baseline without data recombina-tion on GEO, and an average of 1.7 percentagepoints on OVERNIGHT.
In fact, on GEO, weachieve test accuracy of 89.3%, which surpassesthe previous state-of-the-art, excluding Liang et al(2011), which used a seed lexicon for predicates.On ATIS, we experiment with concatenating morethan 2 examples, to make up for the fact that wecannot apply ABSWHOLEPHRASES, which gen-erates longer examples.
We obtain a test accu-racy of 83.3 with ABSENTITIES composed withCONCAT-3, which beats the baseline by 7 percent-age points and is competitive with the state-of-the-art.Data recombination without copying.
Forcompleteness, we also investigated the effectsof data recombination on the model withoutattention-based copying.
We found that recom-bination helped significantly on GEO and ATIS,but hurt the model slightly on OVERNIGHT.
OnGEO, the best data recombination strategy yieldedtest accuracy of 82.9%, for a gain of 8.3 percent-age points over the baseline with no copying andno recombination; on ATIS, data recombinationgives test accuracies as high as 74.6%, a 4.7 pointgain over the same baseline.
However, no data re-combination strategy improved average test accu-racy on OVERNIGHT; the best one resulted in a0.3 percentage point decrease in test accuracy.
Wehypothesize that data recombination helps less onOVERNIGHT in general because the space of pos-sible logical forms is very limited, making it morelike a large multiclass classification task.
There-fore, it is less important for the model to learngood compositional representations that general-ize to new logical forms at test time.ours, as they as they used a seed lexicon mapping words topredicates.
We explicitly avoid using such prior knowledgein our system.18BASKETBALL BLOCKS CALENDAR HOUSING PUBLICATIONS RECIPES RESTAURANTS SOCIAL Avg.Previous WorkWang et al (2015) 46.3 41.9 74.4 54.0 59.0 70.8 75.9 48.2 58.8Our ModelNo Recombination 85.2 58.1 78.0 71.4 76.4 79.6 76.2 81.4 75.8ABSENTITIES 86.7 60.2 78.0 65.6 73.9 77.3 79.5 81.3 75.3ABSWHOLEPHRASES 86.7 55.9 79.2 69.8 76.4 77.8 80.7 80.9 75.9CONCAT-2 84.7 60.7 75.6 69.8 74.5 80.1 79.5 80.8 75.7AWP + AE 85.2 54.1 78.6 67.2 73.9 79.6 81.9 82.1 75.3AWP + AE + C2 87.5 60.2 81.0 72.5 78.3 81.0 79.5 79.6 77.5Table 3: Test accuracy using different data recombination strategies on the OVERNIGHT tasks.Depth-2 (same length)x: ?rel:12 of rel:17 of ent:14?y: ( _rel:12 ( _rel:17 _ent:14 ) )Depth-4 (longer)x: ?rel:23 of rel:36 of rel:38 of rel:10 of ent:05?y: ( _rel:23 ( _rel:36 ( _rel:38( _rel:10 _ent:05 ) ) ) )Figure 5: A sample of our artificial data.0 100 200 300 400 500020406080100Number of additional examplesTest accuracy (%)Same length, independentLonger, independentSame length, recombinantLonger, recombinantFigure 6: The results of our artificial data exper-iments.
We see that the model learns more fromlonger examples than from same-length examples.5.5 Effect of Longer ExamplesInterestingly, strategies like ABSWHOLE-PHRASES and CONCAT-2 help the model eventhough the resulting recombinant examples aregenerally not in the support of the test distribution.In particular, these recombinant examples are onaverage longer than those in the actual dataset,which makes them harder for the attention-basedmodel.
Indeed, for every domain, our bestaccuracy numbers involved some form of concate-nation, and often involved ABSWHOLEPHRASESas well.
In comparison, applying ABSENTITIESalone, which generates examples of the samelength as those in the original dataset, wasgenerally less effective.We conducted additional experiments on artifi-cial data to investigate the importance of addinglonger, harder examples.
We experimented withadding new examples via data recombination, aswell as adding new independent examples (e.g.
tosimulate the acquisition of more training data).
Weconstructed a simple world containing a set of enti-ties and a set of binary relations.
For any n, we cangenerate a set of depth-n examples, which involvethe composition of n relations applied to a singleentity.
Example data points are shown in Figure 5.We train our model on various datasets, then testit on a set of 500 randomly chosen depth-2 exam-ples.
The model always has access to a small seedtraining set of 100 depth-2 examples.
We then addone of four types of examples to the training set:?
Same length, independent: New randomlychosen depth-2 examples.3?
Longer, independent: Randomly chosendepth-4 examples.?
Same length, recombinant: Depth-2 exam-ples sampled from the grammar induced byapplying ABSENTITIES to the seed dataset.?
Longer, recombinant: Depth-4 examplessampled from the grammar induced by apply-ing ABSWHOLEPHRASES followed by AB-SENTITIES to the seed dataset.To maintain consistency between the independentand recombinant experiments, we fix the recombi-nant examples across all epochs, instead of resam-pling at every epoch.
In Figure 6, we plot accu-racy on the test set versus the number of additionalexamples added of each of these four types.
As3Technically, these are not completely independent, as wesample these new examples without replacement.
The sameapplies to the longer ?independent?
examples.19expected, independent examples are more help-ful than the recombinant ones, but both help themodel improve considerably.
In addition, we seethat even though the test dataset only has short ex-amples, adding longer examples helps the modelmore than adding shorter ones, in both the inde-pendent and recombinant cases.
These results un-derscore the importance training on longer, harderexamples.6 DiscussionIn this paper, we have presented a novel frame-work we term data recombination, in which wegenerate new training examples from a high-precision generative model induced from the orig-inal training dataset.
We have demonstratedits effectiveness in improving the accuracy of asequence-to-sequence RNN model on three se-mantic parsing datasets, using a synchronouscontext-free grammar as our generative model.There has been growing interest in applyingneural networks to semantic parsing and relatedtasks.
Dong and Lapata (2016) concurrently de-veloped an attention-based RNN model for se-mantic parsing, although they did not use data re-combination.
Grefenstette et al (2014) proposeda non-recurrent neural model for semantic pars-ing, though they did not run experiments.
Mei etal.
(2016) use an RNN model to perform a relatedtask of instruction following.Our proposed attention-based copying mech-anism bears a strong resemblance to two mod-els that were developed independently by othergroups.
Gu et al (2016) apply a very similar copy-ing mechanism to text summarization and single-turn dialogue generation.
Gulcehre et al (2016)propose a model that decides at each step whetherto write from a ?shortlist?
vocabulary or copy fromthe input, and report improvements on machinetranslation and text summarization.
Another pieceof related work is Luong et al (2015b), who traina neural machine translation system to copy rarewords, relying on an external system to generatealignments.Prior work has explored using paraphrasing fordata augmentation on NLP tasks.
Zhang et al(2015) augment their data by swapping out wordsfor synonyms from WordNet.
Wang and Yang(2015) use a similar strategy, but identify similarwords and phrases based on cosine distance be-tween vector space embeddings.
Unlike our datarecombination strategies, these techniques onlychange inputs x, while keeping the labels y fixed.Additionally, these paraphrasing-based transfor-mations can be described in terms of grammarinduction, so they can be incorporated into ourframework.In data recombination, data generated by a high-precision generative model is used to train a sec-ond, domain-general model.
Generative oversam-pling (Liu et al, 2007) learns a generative modelin a multiclass classification setting, then uses itto generate additional examples from rare classesin order to combat label imbalance.
Uptraining(Petrov et al, 2010) uses data labeled by an ac-curate but slow model to train a computationallycheaper second model.
Vinyals et al (2015b) gen-erate a large dataset of constituency parse treesby taking sentences that multiple existing systemsparse in the same way, and train a neural model onthis dataset.Some of our induced grammars generate ex-amples that are not in the test distribution, butnonetheless aid in generalization.
Related workhas also explored the idea of training on alteredor out-of-domain data, often interpreting it as aform of regularization.
Dropout training has beenshown to be a form of adaptive regularization(Hinton et al, 2012; Wager et al, 2013).
Guu et al(2015) showed that encouraging a knowledge basecompletion model to handle longer path queriesacts as a form of structural regularization.Language is a blend of crisp regularities andsoft relationships.
Our work takes RNNs, whichexcel at modeling soft phenomena, and uses ahighly structured tool?synchronous context freegrammars?to infuse them with an understandingof crisp structure.
We believe this paradigm for si-multaneously modeling the soft and hard aspectsof language should have broader applicability be-yond semantic parsing.Acknowledgments This work was supported bythe NSF Graduate Research Fellowship underGrant No.
DGE-114747, and the DARPA Com-municating with Computers (CwC) program underARO prime contract no.
W911NF-15-1-0462.Reproducibility.
All code, data, andexperiments for this paper are avail-able on the CodaLab platform at https://worksheets.codalab.org/worksheets/0x50757a37779b485f89012e4ba03b6f4f/.20ReferencesY.
Artzi and L. Zettlemoyer.
2013a.
UW SPF: TheUniversity of Washington semantic parsing frame-work.
arXiv preprint arXiv:1311.3011.Y.
Artzi and L. Zettlemoyer.
2013b.
Weakly super-vised learning of semantic parsers for mapping in-structions to actions.
Transactions of the Associ-ation for Computational Linguistics (TACL), 1:49?62.D.
Bahdanau, K. Cho, and Y. Bengio.
2014.
Neuralmachine translation by jointly learning to align andtranslate.
arXiv preprint arXiv:1409.0473.J.
Berant, A. Chou, R. Frostig, and P. Liang.
2013.Semantic parsing on Freebase from question-answerpairs.
In Empirical Methods in Natural LanguageProcessing (EMNLP).J.
Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pas-canu, G. Desjardins, J. Turian, D. Warde-Farley, andY.
Bengio.
2010.
Theano: a CPU and GPU mathexpression compiler.
In Python for Scientific Com-puting Conference.J.
Clarke, D. Goldwasser, M. Chang, and D. Roth.2010.
Driving semantic parsing from the world?s re-sponse.
In Computational Natural Language Learn-ing (CoNLL), pages 18?27.L.
Dong and M. Lapata.
2016.
Language to logicalform with neural attention.
In Association for Com-putational Linguistics (ACL).C.
Dyer, M. Ballesteros, W. Ling, A. Matthews, andN.
A. Smith.
2015.
Transition-based dependencyparsing with stack long short-term memory.
In As-sociation for Computational Linguistics (ACL).E.
Grefenstette, P. Blunsom, N. de Freitas, and K. M.Hermann.
2014.
A deep architecture for seman-tic parsing.
In ACL Workshop on Semantic Parsing,pages 22?27.J.
Gu, Z. Lu, H. Li, and V. O. Li.
2016.
Incorporatingcopying mechanism in sequence-to-sequence learn-ing.
In Association for Computational Linguistics(ACL).C.
Gulcehre, S. Ahn, R. Nallapati, B. Zhou, and Y. Ben-gio.
2016.
Pointing the unknown words.
In Associ-ation for Computational Linguistics (ACL).K.
Guu, J. Miller, and P. Liang.
2015.
Travers-ing knowledge graphs in vector space.
In Em-pirical Methods in Natural Language Processing(EMNLP).G.
E. Hinton, N. Srivastava, A. Krizhevsky,I.
Sutskever, and R. R. Salakhutdinov.
2012.Improving neural networks by preventing co-adaptation of feature detectors.
arXiv preprintarXiv:1207.0580.S.
Hochreiter and J. Schmidhuber.
1997.
Long short-term memory.
Neural Computation, 9(8):1735?1780.N.
Jaitly and G. E. Hinton.
2013.
Vocal tractlength perturbation (vtlp) improves speech recog-nition.
In International Conference on MachineLearning (ICML).A.
Krizhevsky, I. Sutskever, and G. E. Hinton.
2012.Imagenet classification with deep convolutional neu-ral networks.
In Advances in Neural InformationProcessing Systems (NIPS), pages 1097?1105.N.
Kushman and R. Barzilay.
2013.
Using semanticunification to generate regular expressions from nat-ural language.
In Human Language Technology andNorth American Association for Computational Lin-guistics (HLT/NAACL), pages 826?836.T.
Kwiatkowski, L. Zettlemoyer, S. Goldwater, andM.
Steedman.
2010.
Inducing probabilistic CCGgrammars from logical form with higher-order uni-fication.
In Empirical Methods in Natural LanguageProcessing (EMNLP), pages 1223?1233.T.
Kwiatkowski, L. Zettlemoyer, S. Goldwater, andM.
Steedman.
2011.
Lexical generalization inCCG grammar induction for semantic parsing.
InEmpirical Methods in Natural Language Processing(EMNLP), pages 1512?1523.P.
Liang, M. I. Jordan, and D. Klein.
2011.
Learn-ing dependency-based compositional semantics.
InAssociation for Computational Linguistics (ACL),pages 590?599.A.
Liu, J. Ghosh, and C. Martin.
2007.
Generativeoversampling for mining imbalanced datasets.
In In-ternational Conference on Data Mining (DMIN).M.
Luong, H. Pham, and C. D. Manning.
2015a.Effective approaches to attention-based neural ma-chine translation.
In Empirical Methods in NaturalLanguage Processing (EMNLP), pages 1412?1421.M.
Luong, I. Sutskever, Q. V. Le, O. Vinyals, andW.
Zaremba.
2015b.
Addressing the rare wordproblem in neural machine translation.
In Associa-tion for Computational Linguistics (ACL), pages 11?19.H.
Mei, M. Bansal, and M. R. Walter.
2016.
Listen,attend, and walk: Neural mapping of navigationalinstructions to action sequences.
In Association forthe Advancement of Artificial Intelligence (AAAI).S.
Petrov, P. Chang, M. Ringgaard, and H. Alshawi.2010.
Uptraining for accurate deterministic ques-tion parsing.
In Empirical Methods in Natural Lan-guage Processing (EMNLP).H.
Poon.
2013.
Grounded unsupervised semantic pars-ing.
In Association for Computational Linguistics(ACL).21I.
Sutskever, O. Vinyals, and Q. V. Le.
2014.
Se-quence to sequence learning with neural networks.In Advances in Neural Information Processing Sys-tems (NIPS), pages 3104?3112.O.
Vinyals, M. Fortunato, and N. Jaitly.
2015a.
Pointernetworks.
In Advances in Neural Information Pro-cessing Systems (NIPS), pages 2674?2682.O.
Vinyals, L. Kaiser, T. Koo, S. Petrov, I. Sutskever,and G. Hinton.
2015b.
Grammar as a foreign lan-guage.
In Advances in Neural Information Process-ing Systems (NIPS), pages 2755?2763.S.
Wager, S. I. Wang, and P. Liang.
2013.
Dropouttraining as adaptive regularization.
In Advances inNeural Information Processing Systems (NIPS).W.
Y. Wang and D. Yang.
2015.
That?s so annoying!!!
:A lexical and frame-semantic embedding based dataaugmentation approach to automatic categorizationof annoying behaviors using #petpeeve tweets.
InEmpirical Methods in Natural Language Processing(EMNLP).Y.
Wang, J. Berant, and P. Liang.
2015.
Building asemantic parser overnight.
In Association for Com-putational Linguistics (ACL).Y.
W. Wong and R. J. Mooney.
2006.
Learning for se-mantic parsing with statistical machine translation.In North American Association for ComputationalLinguistics (NAACL), pages 439?446.Y.
W. Wong and R. J. Mooney.
2007.
Learningsynchronous grammars for semantic parsing withlambda calculus.
In Association for ComputationalLinguistics (ACL), pages 960?967.M.
Zelle and R. J. Mooney.
1996.
Learning toparse database queries using inductive logic pro-gramming.
In Association for the Advancement ofArtificial Intelligence (AAAI), pages 1050?1055.L.
S. Zettlemoyer and M. Collins.
2005.
Learning tomap sentences to logical form: Structured classifica-tion with probabilistic categorial grammars.
In Un-certainty in Artificial Intelligence (UAI), pages 658?666.L.
S. Zettlemoyer and M. Collins.
2007.
Online learn-ing of relaxed CCG grammars for parsing to log-ical form.
In Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP/CoNLL), pages 678?687.X.
Zhang, J. Zhao, and Y. LeCun.
2015.
Character-level convolutional networks for text classification.In Advances in Neural Information Processing Sys-tems (NIPS).K.
Zhao and L. Huang.
2015.
Type-driven incremen-tal semantic parsing with polymorphism.
In NorthAmerican Association for Computational Linguis-tics (NAACL).22
