Proceedings of the SIGDIAL 2014 Conference, pages 194?198,Philadelphia, U.S.A., 18-20 June 2014.c?2014 Association for Computational LinguisticsKnowledge Acquisition Strategies for Goal-Oriented Dialog SystemsAasish Pappu Alexander I. RudnickySchool of Computer ScienceCarnegie Mellon University{aasish, air}@cs.cmu.eduAbstractMany goal-oriented dialog agents are ex-pected to identify slot-value pairs in aspoken query, then perform lookup ina knowledge base to complete the task.When the agent encounters unknown slot-values, it may ask the user to repeat or re-formulate the query.
But a robust agentcan proactively seek new knowledge froma user, to help reduce subsequent task fail-ures.
In this paper, we propose knowledgeacquisition strategies for a dialog agentand show their effectiveness.
The acquiredknowledge can be shown to subsequentlycontribute to task completion.1 IntroductionMany spoken dialog agents are designed to per-form specific tasks in a specified domain e.g., in-formation about public events in a city.
To carryout its task, an agent parses an input utterance, fillsin slot-value pairs, then completes the task.
Some-times, information on these slot-value pairs maynot be available in its knowledge base.
In suchcases, typically the agent categorizes utterances asnon-understanding errors.
Ideally the incident isrecorded and the missing knowledge is incorpo-rated into the system with a developer?s assistance?
a slow offline process.There are other sources of knowledge: automat-ically crawling the web, as done by NELL [Carl-son et al., 2010], and community knowledgebases such as Freebase [Bollacker et al., 2008].These approaches provide globally popular slot-values [Araki, 2012] and high-level semantic con-texts [Pappu and Rudnicky, 2013].
Despite theirsize, these knowledge bases may not contain in-formation about the entities in a specific targetdomain.
However, users in the agent?s domaincan potentially provide specific information onslot/values that are unavailable on the web, e.g.,regarding a recent interest/hobby of the user?sfriend.
Lasecki et al.
[2013] have elicited natu-ral language dialogs from humans to build NLUmodels for the agent and Bigham et al.
[2010]have elicited answers to visual questions by in-tegrating users into the system.
One observationfrom this work is that both users and non-userscan impart useful knowledge to system.
In thispaper we propose spoken language strategies thatallow an agent to elicit new slot-value pairs fromits own user population to extend its knowledgebase.
Open-domain knowledge may be elicitedthrough text-based questionnaires from non-usersof the system, but in a situated interaction scenariospoken strategies may be more effective.
We ad-dress the following research questions:1.
Can an agent elicit reliable knowledge aboutits domain from users?
Particularly knowl-edge it cannot locate elsewhere (e.g., on-lineknowledge bases).
Is the collective knowl-edge of the users sufficient to allow the agentto augment its knowledge through interactivemeans?2.
What strategies elicit useful knowledge fromusers?
Based on previous work in com-mon sense knowledge acquisition [Von Ahn,2006, Singh et al., 2002, Witbrock et al.,2003], we devise spoken language strategiesthat allow the system to solicit information bypresenting concrete situations and by askinguser-centric questions.We address these questions in the context of theEVENTSPEAK dialog system, an agent that providesinformation about seminars and talks in an aca-demic environment.
This paper is organized asfollows.
In Section 2, we discuss knowledge ac-quisition strategies.
In Section 3, we describe auser study on these strategies.
Then, we presentan evaluation on system acquired knowledge andfinally we make concluding remarks.194Table 1: System initiated strategies used by the agent for knowledge acquisition in the EVENTSPEAK system.StrategyType Strategy Example PromptQUERYDRIVENQUERYEVENT I know events on campus.
What do you want to know?QUERYPERSON I know some of the researchers on campus.Whom do you want to know about?PERSONALBUZZWORDS What are some of the popular phrases in your research?FAMOUSPEOPLE Tell me some well-known people in your research areaSHOW&ASKTWEET How would you describe this talk in a sentence, say a tweet.KEYWORDS Give keywords for this talk in your own words.PEOPLE Do you know anyone who might be interested in this talk?2 Knowledge Acquisition StrategiesWe posit three different circumstances that cantrigger knowledge acquisition behavior: (1) initi-ated by expert users of the system [Holzapfel et al.,2008, Spexard et al., 2006, L?utkebohle et al., 2009,Rudnicky et al., 2010], (2) triggered by ?misun-derstanding?
of the user?s input [Chung et al.,2003, Filisko and Seneff, 2005, Prasad et al., 2012,Pappu et al., 2014], or (3) triggered by the system.They are described below:QUERYDRIVEN.
The system prompts a userwith an open-ended question akin to ?how-may-I-help-you?
to learn what ?values?
of a slot are ofinterest to the user.
This strategy does not grounduser about system?s knowledge limitations.
How-ever, it allows the system to acquire information(slot-value pairs) from user?s input.
The systemcan choose to respond to the input or ignore theinput depending on its knowledge about the slot-value pairs in the input.
Table 1 shows strategiesof this kind i.e., QUERYEVENT and QUERYPERSON.PERSONAL.
The system asks a user about theirown interests and people who may share those in-terests.
This is an open-ended request as well, butthe system expects the response to be confined tothe user?s knowledge about specific entities in theenvironment.
BUZZWORDS and FAMOUSPEOPLE ex-pects the user to provide values for the slots.SHOW&ASK.
The system provides a descrip-tion of an event and asks questions to grounduser?s responses in relation to that event.
E.g.,given the title and abstract of a technical talk,the system asks the user questions about the talk.TWEET strategy is expected to elicit a concise de-scription of the event, which eventually may helpthe agent to both summarize events for other usersand identify keywords for an event.
KEYWORDSstrategy expects the user to explicitly supply key-words for an event.
PEOPLE strategy expects theuser to provide names of likely event participants.We hypothesized that these strategies may allowthe agent to learn new slot-value pairs that mayhelp towards better task performance.3 Knowledge Acquisition StudyWe conducted a user study to determine reliabilityof the information acquired by the system.
We per-formed this study using the EVENTSPEAK1dialogsystem, which provides information about upcom-ing talks and other events that might be of inter-est, and about ongoing research on campus.
Thesystem presents material on a screen and acceptsspoken input, in a context similar to a kiosk.The study evaluated performance of the sevenstrategies described above.
For SHOW&ASK strate-gies, we had users respond regarding a specificevent.
We used descriptions of research talks col-lected from the university?s website.
We used aweb-based interface for data collection; the inter-face presented the prompt material and recordedthe subject?s voice response.
Testvox2was usedto setup the experiments and Wami3for audiorecording.3.1 User Study DesignWe recruited 40 researchers (graduate students)from the School of Computer Science, at CarnegieMellon, representative of the user population forthe EVENTSPEAK dialog system.
Each subject re-sponded to prompts from the QUERYDRIVEN, PER-SONAL and SHOW&ASK strategies.In the QUERYDRIVEN tasks, the QUERYEVENTstrategy, the system responds to the user?s querywith a list of talks.
The user?s response isrecorded, then sent to an open-vocabulary speechrecognizer; the result is used as a query to adatabase of talks.
The results are then displayed onthe screen.
The system applies the QUERYPERSONstrategy in a similar way.
In the PERSONAL tasks,the system applies the BUZZWORDS strategy to askthe user about popular keyphrases in their research1http://www.speech.cs.cmu.edu/apappu/kacq2https://bitbucket.org/happyalu/testvox/wiki/Home3https://code.google.com/p/wami-recorder/195Figure 1: Time per Task for all strategiesQueryEventQueryPersonBuzzwordsFamousPeopleTweetPeopleKeywords012341.512.230.910.712.510.690.97TimeinminutesFigure 2: Time per Task vs ExpertiseExpertLevel1ExpertLevel2ExpertLevel3ExpertLevel401234Timeinminutestweetpeoplekeywordsarea.
The system then asks about well-known re-searchers (FAMOUSPEOPLE) in the user?s area.In the SHOW&ASK tasks, we use two seminardescriptions per subject (in our pilot study, wefound that people provide more diverse responses(in term of entities) in the SHOW&ASK based onthe event abstract, compared to PERSONAL, QUERY-DRIVEN).
We used a set of 80 research talk an-nouncements (consisting of a title, abstract andother information).
For each talk, the system usedall three strategies viz., TWEET, KEYWORDS and PEO-PLE.
For the TWEET tasks, subjects were asked toprovide a one sentence description.
They were al-lowed to give a non-technical/high-level descrip-tion if they were unfamiliar with the topic.
Forthe PEOPLE task, subjects had to give names of col-leagues who might be interested in the talk.
Forthe KEYWORDS task, subjects provided keywords,either their own words or ones selected from theabstract.Since the material is highly technical, we wereinterested whether the tasks are cognitively de-manding for people who are less familiar with thesubject of a talk.
Therefore, users were asked toindicate their familiarity with a particular talk (re-search area in general) using a scale of 1?4: 4 be-ing more familiar and 1 being less familiar.3.2 Corpus DescriptionThis user study produced 64 minutes of audio data,on average 1.6 minutes per subject.
We tran-scribed the speech then annotated the corpus forpeople names, and for research interests.
Table 2shows the number of unique slot-values found inthe corpus.
We observe that the number of uniqueresearch interests produced during SHOW&ASK ishigher than for other strategies.
This confirmsour initial observations that this strategy elicitsdiverse responses.
The PERSONAL task produceda relatively higher number of researcher names(FAMOUSPEOPLE strategy) than other tasks.
One ex-planation might be that people may find it easierto recall names in their own research area, as com-pared to other areas.
Overall, we identified 139unique researcher names and 485 interests.Table 2: Corpus StatisticsStrategyTypeUniqueResearcherNamesUniqueResearchInterestsQUERYDRIVEN 21 30PERSONAL 77 107SHOW&ASK 76 390Overall 139 4853.3 Corpus AnalysisOne of the objectives of this work is to determineWhat strategies can the agent use to elicit knowl-edge from users?
Although, time-cost will varywith task and domain, a usable strategy should, ingeneral, be less demanding.
We analyzed the time-per-task for each strategy, shown in Figure 1.
Wefound that the TWEET strategy is not only more de-manding, it has higher variance than other tasks.One explanation is that people would attempt tosummarize the entire abstract including technicaldetails, despite the instructions indicated that anon-technical description was acceptable.
We cansee a similar trend in Figure 2 that irrespectiveof expertise-level, subjects take more time to giveone sentence descriptions.
We also observe highvariance and higher time-per-task for QUERYPER-SON; this is due to the system deliberately not re-turning any results for this task.
This was done to196Table 3: Mean Precision for 200 researchers, broken down by the ?source?
strategy used to acquire their nameNote: Only 85 of 200 researchers had Google Scholar pages, GScholar Accuracy is computed for only those 85.Metric Description Text SHOW&ASK PERSONAL QUERYDRIVEN meanMean Precision 89.5% 86.9% 93.6% 86.2% 90.5%GScholar Acc.
78.3% 82.3% 86.1% 100% 80.0%find out whether subjects would repeat the task onfailure.
Ideally the system needs to only rarely usethis strategy to not lose user?s trust and solicit mul-tiple values for a given slot (e.g., person name) asopposed to requesting list of values as in FAMOUS-PEOPLE and PEOPLE strategies.
We find that PEOPLE,KEYWORDS, FAMOUSPEOPLE and BUZZWORDS strate-gies are efficient with a time-per-task of less thanone minute.
As shown in Figure 2, subjects do nottake much time to speak a list of names or key-words.4 Evaluation of Acquired KnowledgeTo answer Can an agent elicit reliable knowl-edge about its domain from users?
we analyzedthe relevance of acquired knowledge.
We havetwo disjoint list of entities, (a) researchers and(b) research interests; in addition we have speakernames from the talk descriptions.
Our goal isto implicitly infer a list of interests for each re-searcher without soliciting the user for the inter-ests of every researcher exhaustively.
To each re-searcher in the list, we attribute list of interests thatwere mentioned in the same context as researcherwas mentioned.
We tag list of names acquiredfrom the FAMOUSPEOPLE strategy with list of key-words acquired from the BUZZWORDS strategy ?both lists acquired from same user.
We repeat thisprocess for each name mentioned in relation to atalk in the SHOW&ASK strategy.
We tag keywordsmentioned in the KEYWORDS strategy to researchersmentioned in the PEOPLE strategy.4.1 AnalysisWe produced 200 entries for researchers and theirset of interests.
We then had two annotators (se-nior graduate students) mark whether the system-predicted interests were relevant/accurate.
The an-notators were allowed to use information found onresearchers?
home pages and Google Scholar4toevaluate the system-predicted interests.This can be seen as an information retrieval (IR)problem, where researcher is ?query?
and interestsare ?documents?.
So, we use Mean Precision, a4scholar.google.comcommon metric in IR, to evaluate retrieval.
In ourcase, the ground truth for relevant interests comesfrom the annotators.
The results are shown in Ta-ble 3.
Our approach has high precision, 90.5%,for all 200 researchers.
We see that irrespectiveof the strategy used to acquire entities, precisionis good.
We also compared our predicted inter-ests with interests listed by researchers themselveson Google Scholar.
There are only 85 researchersfrom our list with a Google Scholar page; for theseour accuracy is 80%, again good.
Moreover, sig-nificant knowledge is absent from the web (at leastin our domain) yet can be elicited from users fa-miliar with the domain.5 ConclusionWe describe a set of knowledge acquisition strate-gies that allow a system to solicit novel informa-tion from users in a situated environment.
To in-vestigate the usability of these strategies, we con-ducted a user study in the domain of research talks.We analyzed a corpus of system-acquired knowl-edge and have made the material available5.
Ourdata show that users on average take less than aminute to provide new information using the pro-posed elicitation strategies.
The reliability of ac-quired knowledge in predicting relationships be-tween researchers and interests is quite good, witha mean precision of 90.5%.
We note that the PER-SONAL strategy, which tries to tap personal knowl-edge, appears to be particularly effective.
Moregenerally, automated elicitation appears to be apromising technique for continuous learning inspoken dialog systems.6 AppendixSystem Predicted Researcher-Interests 1rich stern deep neural networks, speech recog-nition, signal processing, neural networks, machinelearning, speech synthesis5www.speech.cs.cmu.edu/apappu/pubdl/eventspeak corpus.zip197System Predicted Researcher-Interests 2kishore prahallad dialogue systems, prosody,speech synthesis, text to speech, pronunciation mod-eling, low resource languagesSystem Predicted Researcher-Interests 3carolyn rose crowdsourcing, meta discourse clas-sification, statistical analysis, presentation skills in-struction, man made system, education models, humanlearningSystem Predicted Researcher-Interests 4florian metze dialogue systems, speech recogni-tion, nlp, prosody, speech synthesis, text to speech,pronunciation modeling, low resource languages, au-tomatic accent identificationSystem Predicted Researcher-Interests 5madhavi ganapathiraju protein structure, contin-uous graphical models, generative models, structuralbiology, protein structure dynamics, molecular dy-namicsSystem Predicted Researcher-Interests 6alexander hauptmann discriminatively trainedmodels, deep learning, computer vision, big dataSystem Predicted Researcher-Interests 7jamie callan learning to rank, search, large scalesearch, web search, click prediction, information re-trieval, web mining, user activity, recommendation,relevance, machine learning, web crawling, distributedsystems, structural similaritySystem Predicted Researcher-Interests 8lori levin natural language understanding, knowl-edge reasoning, construction grammar, knowledgebases, natural language processingReferencesMasahiro Araki.
Rapid development process of spoken dia-logue systems using collaboratively constructed semanticresources.
In Proceedings of the SIGDIAL 2012 Confer-ence, pages 70?73.
ACL, 2012.Jeffrey P Bigham, Chandrika Jayant, Hanjie Ji, Greg Little,Andrew Miller, Robert C Miller, Robin Miller, AubreyTatarowicz, Brandyn White, Samual White, et al.
Vizwiz:nearly real-time answers to visual questions.
In Proceed-ings of the 23rd ACM Symposium on User Interface soft-ware and technology, pages 333?342.
ACM, 2010.Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge,and Jamie Taylor.
Freebase: a collaboratively createdgraph database for structuring human knowledge.
Pro-ceedings of the SIGMOD, pages 1247?1249, 2008.Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Set-tles, Estevam R Hruschka Jr., and Tom M Mitchell.
To-ward an Architecture for Never-Ending Language Learn-ing.
Artificial Intelligence, 2(4):1306?1313, 2010.Grace Chung, Stephanie Seneff, and Chao Wang.
Automaticacquisition of names using speak and spell mode in spo-ken dialogue systems.
In Proceedings of the NAACL-HLT,pages 32?39.
ACL, 2003.Edward Filisko and Stephanie Seneff.
Developing city nameacquisition strategies in spoken dialogue systems via usersimulation.
In 6th SIGdial Workshop on Discourse andDialogue, 2005.Hartwig Holzapfel, Daniel Neubig, and Alex Waibel.
A dia-logue approach to learning object descriptions and seman-tic categories.
Robotics and Autonomous Systems, 56(11):1004?1013, November 2008.Walter Stephen Lasecki, Ece Kamar, and Dan Bohus.
Con-versations in the crowd: Collecting data for task-orienteddialog learning.
In First AAAI Conference on HumanComputation and Crowdsourcing, 2013.Ingo L?utkebohle, Julia Peltason, Lars Schillingmann,Christof Elbrechter, Britta Wrede, Sven Wachsmuth, andRobert Haschke.
The Curious Robot: Structuring Inter-active Robot Learning.
In ICRA?09, pages 4156?4162.IEEE, 2009.Aasish Pappu and Alexander Rudnicky.
Predicting tasksin goal-oriented spoken dialog systems using semanticknowledge bases.
In Proceedings of the SIGDIAL, pages242?250.
ACL, 2013.Aasish Pappu, Teruhisa Misu, and Rakesh Gupta.
Investi-gating critical speech recognition errors in spoken shortmessages.
In Proceedings of IWSDS, pages 39?49, 2014.Rohit Prasad, Rohit Kumar, Sankaranarayanan Ananthakr-ishnan, Wei Chen, Sanjika Hewavitharana, Matthew Roy,Frederick Choi, Aaron Challenner, Enoch Kan, ArvindNeelakantan, et al.
Active error detection and resolu-tion for speech-to-speech translation.
In Proceedings ofIWSLT, 2012.Alexander I Rudnicky, Aasish Pappu, Peng Li, and MatthewMarge.
Instruction Taking in the TeamTalk System.
InProceedings of the AAAI Fall Symposium on Dialog withRobots, pages 173?174, 2010.Push Singh, Thomas Lin, Erik T Mueller, Grace Lim, Trav-ell Perkins, and Wan Li Zhu.
Open mind commonsense: Knowledge acquisition from the general public.
InCoopIS, DOA, and ODBASE, pages 1223?1237.
Springer,2002.Thorsten Spexard, Shuyin Li, Britta Wrede, Jannik Fritsch,Gerhard Sagerer, Olaf Booij, Zoran Zivkovic, Bas Ter-wijn, and Ben Krose.
BIRON, where are you?
Enablinga robot to learn new places in a real home environment byintegrating spoken dialog and visual localization.
Integra-tion The VLSI Journal, (section II):934?940, 2006.Luis Von Ahn.
Games with a purpose.
Computer, 39(6):92?94, 2006.Michael Witbrock, David Baxter, Jon Curtis, Dave Schneider,Robert Kahlert, Pierluigi Miraglia, Peter Wagner, KathyPanton, Gavin Matthews, and Amanda Vizedom.
An inter-active dialogue system for knowledge acquisition in cyc.In Proceedings of the 18th IJCAI, pages 138?145, 2003.198
