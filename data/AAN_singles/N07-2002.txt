Proceedings of NAACL HLT 2007, Companion Volume, pages 5?8,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsAutomatic acquisition of grammatical types for nounsN?ria Bel Sergio Espeja Montserrat MarimonIULAUniversitat Pompeu FabraP.
de la Merc?, 10-12ES-08002 ?
Barcelona{nuria.bel,sergio.espeja,montserrat.marimon}@upf.eduAbstractThe work1 we present here is concernedwith the acquisition of deep grammati-cal information for nouns in Spanish.The aim is to build a learner that canhandle noise, but, more interestingly,that is able to overcome the problem ofsparse data, especially important in thecase of nouns.
We have based our workon two main points.
Firstly, we haveused distributional evidences as fea-tures.
Secondly, we made the learnerdeal with all occurrences of a word as asingle complex unit.
The obtained re-sults show that grammatical features ofnouns is a level of generalization thatcan be successfully approached with aDecision Tree learner.1 IntroductionOur work aims to the acquisition of deep gram-matical information for nouns, because having in-formation such as countability and complementa-tion is necessary for different applications, espe-cially for deep analysis grammars, but also forquestion answering, topic detection and tracking,etc.Most successful systems of deep lexical acquisi-tion are based on the idea that distributional fea-tures (i.e.
the contexts where words occur) are as-sociated to concrete lexical  types.
The difficulties1 This research was supported by the Spanish Ministerio de Educaci?n y Cien-cia: project AAILE, HUM2004-05111-C02-01/FILO, Ram?n y Cajal, Juan de laCierva Programs and PTA-CTE/1370/2003 with Fondo Social Europeo,.are, on the one hand, that some filtering must beapplied to get rid of noise, that is, contexts wronglyassessed as cues of a given type and, on the otherhand, that for a pretty large number of words, theiroccurrences in a corpus of any length are very few,making statistical treatment very difficult.The phenomenon of noise is related to the factthat one particular context can be a cue of differentlexical types.
The problem of sparse data is pre-dicted by the Zipfian distribution of words in texts:there is a large number of words likely to occur avery reduced number of times in any corpus.
Bothof these typical problems are maximized in thecase of nouns.The aim of the work we present here is to builda learner that can handle noise, but, more interest-ingly, that is able to overcome the problem ofsparse data.
The learner must predict the correcttype both when there is a large number of occur-rences as well as when there are only few occur-rences, by learning on features that maximize gen-eralization capacities of the learner while control-ling overfitting phenomena.We have based our work on two main points.Firstly, we have used morphosyntactic informationas features.
Secondly, we made the learner dealwith all occurrences of a word as a complex unit.In our system, linguistic cues of every occurrenceare collected in the signature of the word (moretechnically a pair lema + part of speech) in a par-ticular corpus.
In the next sections we give furtherdetails about the features used, as well as about theuse of signatures.The rest of the paper is as follows.
Section 2presents an overview of the state of the art in deeplexical acquisition.
In section 3, we introduce de-tails about our selection of linguistically motivated5cues to be used as features for training a DecisionTree (DT).
Section 4 shortly introduces the meth-odology and data used in the experiments whoseresults are presented in section 5.
And in section 6we conclude by comparing with the published re-sults for similar tasks and we sketch future re-search.2 State of the artMost of the work on deep lexical informationacquisition has been devoted to verbs.
The existingacquisition systems learn very specialized linguis-tic information such as verb subcategorizationframe2.
The results for verb subcategorization aremostly around the 0.8 of precision.
Briscoe & Car-roll (1997) reported a type precision of 0,76 and atype recall of 0.43.
Their results were improved bythe work of Korhonen (2002) with a type precisionof 0.87 and a recall of 0.68 using external re-sources to filter noise.
Shulte im Walde (2002) re-ports a precision of 0.65 and a recall of 0.58.Chesley & Salmon-Alt (2006) report a precision of0.86 and a recall of 0.54 for verb subcategorizationacquisition for French.Lexical acquisition for nouns has been con-cerned mainly with ontological classes and hasmainly worked on measuring semantic similarityon the basis of occurrence contexts.
As for gram-matical information, the work of Baldwin andBond (2003) in acquisition of countability featuresfor English nouns also tackles the very importantproblem of feature selection.
Other work like Car-roll and Fang?s (2004) and Baldwin?s (2005) havefocused on grammatical information acquisitionfor HPSG based computational grammars.
Thelatter is the most similar exercises to our work.Baldwin (2005) reports his better results in termsof type accuracy has been obtained by using syn-tactic information in a chunked and parsed corpus.The type F-scores for the different tested catego-ries for English were: for verbs 0.47, for nouns 0.6and for adjectives 0.832.3 Feature selectionOne of the most important tasks in developingmachine learning applications is the selection of2 Given the argument-adjunct distinction, subcategorizationconcerns the specification for a predicate of the number andtype of arguments which it requires for well-formedness.the features that leads to the smallest classificationerror.
For our system, we have looked at distribu-tional motivated features that can help in discrimi-nating the different types that we ultimately use toclassify words.The lexical types used in deep analysis gram-mars are linguistic generalizations drawn from thedistributional characteristics of particular sets ofwords.
For the research we present here, we havetaken the lexicon of a HPSG-based grammars de-veloped in the LKB platform (Copestake, 2002) forSpanish, similarly to the work of Baldwin (2005).In the LKB grammatical framework, lexical typesare defined as a combination of features.
Lexicaltypology of nouns for Spanish, for instance, can beseen as a cross-classification of noun countabilityvs.
mass distinctions, and subcategorization frameor valence, including prepositional selection.
Forexample nouns as ?temor?
(?fear?)
and ?adicci?n?
(?adiction) belong to the typen_ppde_pcomp_a_count as they take two com-plements: one with de and the other with a boundpreposition a, as in ?El temor de la ni?a a los fan-tasmas?
(?The girl?s fear to ghosts?)
vs. ?La adic-ci?n a la coca?na?
(?The addiction to cocaine?
).We decided to carry out the classification foreach of the grammatical features that conform thecross-classified types as a better level of generali-zation than the type: mass and countable, on theone hand and, on the other hand, for subcategoriza-tion information three further basic features: trans,for nouns with thematic complements introducedby the preposition de, intrans, when the noun canappear with no complements and pcomp for nounshaving complements introduced by a bound prepo-sition.
The complete type can be recomposed withthe assigned features.
?Temor?
and ?adicci?n?
willbe examples of trans and pcomp_a.
They bothhave also to be assigned the feature countable.
Thecombination of features assigned corresponds tothe final type which is a definition of the completebehaviour of the noun with respect, for instance,optional complements.We have used 23 linguistic cues, that is, the pat-terns of contexts that can be indicative of a particu-lar feature.
The most frequent cue that can be re-lated to countable is for the noun to be found withplural morphology.
A singular noun without de-terminer after a verb or a preposition is a cue of thenoun being mass: ?hay barro en el sal?n?
(?there ismud in the living room?)
vs. ?hay hombres en el6sal?n?
(?there are men in the living room?).
A fur-ther cue for mass is the presence of particularquantifiers, such as ?m?s?
(?more?
), ?menos?(?less?
), etc.
But these cues, based on a collectionof lexical items, are less productive than othercharacteristics such as morphological number orpresence of determiners, as they appear veryscarcely in texts.
Nevertheless, we should mentionthat most of mass nouns in Spanish can also appearin the contexts of countables, as in the case of?beer?
when in constructions such as ?three beers,please?.More difficult was to find cues for identifyingthe transitive nature of a noun.
After some empiri-cal work, we found a tendency of argumental com-plements to have a definite article: ?temor de lani?a?
(?fear of the girl?
), while modifiers tend toappear without determiners: ?mesa de juegos?
(?ta-ble of games?).
Besides, we have taken as a cue themorphological characteristics of deverbal nouns.Suffixes such as ?-ci?n?, ?-si?n?, and ?-miento?,are very much indicative of transitive nouns.
Fi-nally, to find the bound preposition of comple-ments, we used a pattern for each possible preposi-tion found after the noun in question.We used Regular Expressions to implement thelinguistic motivated patterns that check for the in-formation just mentioned in a part of speech taggedcorpus.
The various patterns determine whether thelinguistic cues that we have related to syntacticfeatures are found in each occurrence of a particu-lar word in a corpus.
The positive or negative re-sults of the n pattern checking are stored as binaryvalues of a n dimensional vector, one for each oc-currence.
All vectors produced, one per occurrenceof the word in question, are stored then in a kind ofvector of vectors that we have called its signature.The term signature wants to capture the notion thatthe data it embodies is truly representative of a par-ticular item, and that shows the details of its typicalbehavior.
Particularly, we wanted linguistic cuesappearing in different occurrences of the sameword to be observed as related information.
Wehave not dealt with ambiguity at all, however.
Oneof the reasons was our focus on low frequencynouns.4 Methodology and dataWe have worked with the Corpus T?cnic del?IULA, a multilingual part of speech tagged corpuswhich consists of domain specific texts.
The sec-tion used for our evaluation was the Spanish with1,091,314 words in the domain of economy and4,301,096 for medicine.
A dataset of 289 nouns,present in both subcorpora, was selected.
It wasimportant to compare the behavior of the samenouns in both corpus to check whether the learnerwas subject to unwanted overfitting.We used the data for building a C4.5 DT clas-sifier3.
DT?s are one well known and successfultechnique for this class of tasks when there isenough pre-annotated data available.
DT?s havethe additional benefit that the results can be in-spected.
The signatures of the words in the Gold-Standard lists were extracted from the corpus ofmedicine and of the economy one.
There was afurther test set of 50 nouns with a single occur-rence in the corpus of economy for testing pur-poses.
The DT was trained with the signatures ofthe economy corpus, and the medicine ones as wellas the singles set were used for testing.5 EvaluationThe purpose of the evaluation was to validate oursystem with respect to the two problems men-tioned: noise filtering and generalization capacityby measuring type precision and type recall.
Weunderstand type precision as a measure of the noisefiltering success, and recall as a measure of thegeneralization capacity.In the following tables we present the results ofthe different experiments.
In Table 1, there is aview of the results of the experiment after trainingand testing with the signatures got in the smallercorpus.
The results are for the assignment of thegrammatical feature for the two values, yes and no.And the column named global refers to the totalpercentage of correctly classified instances.yes nolt global prec.
rec.
F prec.
rec.
FMASS 0.67 0.4 0.26 0.31 0.73 0.83 0.78COUNT 0.96 0.97 0.99 0.98 0 0 0TRANS 0.85 0.73 0.45 0.55 0.86 0.95 0.91INT 0.81 0.84 0.94 0.89 0.64 0.32 0.48PCOMP 0.9 0.4 0.08 0.13 0.91 0.98 0.95Table 1.
DT results of economy signatures fortraining and test3 We have used WEKA J48 decision tree classifier (Witten and Frank, 2005).7The most difficult task for the learner is to iden-tify nouns with bound prepositions.
Note that thereare only 20 nouns with prepositional complementsof the 289 test nouns, and that the occurrence ofthe preposition is not mandatory, and hence thesignatures are presented to the learner with verylittle information.Table 2 shows the results for 50 nouns with onlyone occurrence in the corpus.
The performancedoes not change significantly, showing that thegeneralization capacity of the learner can copewith low frequency words, and that noise in largersignatures has been adequately filtered.yes nolt global prec.
rec.
F prec.
rec.
FMASS 0.71 0.5 0.16 0.25 0.73 0.93 0.82COUNT 0.97 0.97 1 0.98 0 0 0TRANS 0.85 0.75 0.46 0.57 0.87 0.96 0.91INT 0.83 0.85 0.95 0.89 0.70 0.41 0.52PCOMP 0.91 0 0 0 0.91 1 0.95Table 2.
DT results for training with signatures ofthe economy corpus and testing 50 unseen nounswith a single occurrence as testTable 3 shows that there is little variation in theresults of training with signatures of the economycorpus and testing with ones of the medicine cor-pus.
As expected, no variation due to domain isrelevant as the information learnt should be validin all domains.yes nolt global prec.
rec.
F prec.
rec.
FMASS 0.65 0.44 0.53 0.48 0.77 0.70 0.73COUNT 0.97 0.97 1 0.98 0 0 0TRANS 0.82 0.62 0.47 0.54 0.86 0.92 0.89INT 0.78 0.82 0.92 0.86 0.58 0.35 0.43PCOMP 0.81 0.31 0.28 0.29 0.92 0.93 0.93Table 3.
DT results for training with economy sig-natures and testing with medicine signatures6  ConclusionsThe obtained results show that the learning ofgrammatical features of nouns are learned success-fully when using distributional linguistic informa-tion as learning features that allow the learner togeneralize so as to maintain the performance incases of nouns with just one occurrence.There are however issues that should be furtherinvestigated.
Grammatical features with low preci-sion and recall results (mass and pcomp) show thatsome more research should be carried out for find-ing relevant linguistic cues to be used as learningfeatures.
In that respect, the local cues based onmorphosyntactic tagging have proved to be useful,minimizing the text preprocessing requirements forgetting usable results.AcknowledgementsThe authors would like to thank Jordi Porta,Daniel Chicharro and the anonymous reviewers forhelpful comments and suggestions.ReferencesBaldwin, T. 2005.
?Bootstrapping Deep Lexical Re-sources: Resources for Courses?, ACL-SIGLEX 2005.Workshop on Deep Lexical Acquisition.Baldwin, T. and F. Bond.
2003.
?Learning the Count-ability of English Nouns from Corpus Data?.
Pro-ceedings of the 41st.
Annual Meeting of the ACL.Briscoe, T. and J. Carroll.
1997.
?Automatic extractionof subcategorization from corpora?.
In Proceedingsof the Fifth Conference on Applied Natural Process-ing, Washington.Carroll, J. and A. Fang.
2004.
?The automatic acquisi-tion of verb subcategorisations and their impact onthe performance of an HPSG parser?.
In Proceedingsof the 1st International Joint Conference on NaturalLanguage Processing (IJCNLP), Sanya City, China.Chesley, P and S. Salmon-Alt.
2006.
?Automatic extrac-tion of subcategorization frames for French?.
InProc.
of the LREC Conference, Genoa.Copestake, A.. 2002.
Implementing Typed FeatureStructure Grammars.
CSLI Publications.Korhonen, A.
2002.
?Subcategorization acquisition?.
AsTechnical Report UCAM-CL-TR-530, University ofCambridge, UK.Shulte im Walde, S. 2002.
?Evaluating verb subcate-gorization frames learned by a German statisticalgrammar against manual definitions in the DudenDictionary?.
In Proceedings of the 10th EURALEX In-ternational Congress, 187-197.Witten, Ian H. and Eibe Frank.
2005.
Data Mining:Practical machine learning tools and techniques.
2ndEdition, Morgan Kaufmann, San Francisco.8
