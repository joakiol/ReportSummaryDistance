Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing, pages 40?44,Donostia?San Sebastia?n, July 23?25, 2012. c?2012 Association for Computational LinguisticsDAGGER: A Toolkit for Automata on Directed Acyclic GraphsDaniel QuernheimInstitute for Natural Language ProcessingUniversita?t Stuttgart, GermanyPfaffenwaldring 5b, 70569 Stuttgartdaniel@ims.uni-stuttgart.deKevin KnightUniversity of Southern CaliforniaInformation Sciences InstituteMarina del Rey, California 90292knight@isi.eduAbstractThis paper presents DAGGER, a toolkit forfinite-state automata that operate on directedacyclic graphs (dags).
The work is based on amodel introduced by (Kamimura and Slutzki,1981; Kamimura and Slutzki, 1982), with afew changes to make the automata more ap-plicable to natural language processing.
Avail-able algorithms include membership checkingin bottom-up dag acceptors, transduction ofdags to trees (bottom-up dag-to-tree transduc-ers), k-best generation and basic operationssuch as union and intersection.1 IntroductionFinite string automata and finite tree automata haveproved to be useful tools in various areas of naturallanguage processing (Knight and May, 2009).
How-ever, some applications, especially in semantics, re-quire graph structures, in particular directed acyclicgraphs (dags), to model reentrancies.
For instance,the dags in Fig.
1 represents the semantics of the sen-tences ?The boy wants to believe the girl?
and ?Theboy wants the girl to believe him.?
The double roleof ?the boy?
is made clear by the two parent edges ofthe BOY node, making this structure non-tree-like.Powerful graph rewriting systems have been usedfor NLP (Bohnet and Wanner, 2010), yet we con-sider a rather simple model: finite dag automata thathave been introduced by (Kamimura and Slutzki,1981; Kamimura and Slutzki, 1982) as a straight-forward extension of tree automata.
We present thetoolkit DAGGER (written in PYTHON) that can beused to visualize dags and to build dag acceptors(a)WANTBELIEVEBOY GIRL (b)WANTBELIEVEBOY GIRLFigure 1: (a) ?The boy wants to believe the girl.?
and(b) ?The boy wants the girl to believe him.?
First edgerepresents :agent role, second edge represents :patientrole.and dag-to-tree transducers similar to their model.Compared to those devices, in order to use them foractual NLP tasks, our machines differ in certain as-pects:?
We do not require our dags to be planar, and wedo not only consider derivation dags.?
We add weights from any commutative semir-ing, e.g.
real numbers.The toolkit is available under an open source li-cence.12 Dags and dag acceptorsDAGGER comes with a variety of example dags andautomata.
Let us briefly illustrate some of them.
Thedag of Fig.
1(a) can be defined in a human-readableformat called PENMAN (Bateman, 1990):(1 / WANT:agent (2 / BOY):patient (3 / BELIEVE:agent 2:patient (4 / GIRL)))1http://www.ims.uni-stuttgart.de/?daniel/dagger/40ss -> (WANT :agent i :patient s)s -> (BELIEVE :agent i :patient s)i -> (0)s -> (0)i -> (GIRL)i -> (BOY)s -> (GIRL)s -> (BOY)i i -> (GIRL)i i -> (BOY)i s -> (GIRL)i s -> (BOY)s s -> (GIRL)s s -> (BOY)Figure 2: Example dag acceptor example.bda.In this format, every node has a unique identifier,and edge labels start with a colon.
The tail node ofan edge is specified as a whole subdag, or, in thecase of a reentrancy, is referred to with its identifier.Fig.
2 shows a dag acceptor.
The first line con-tains the final state, and the remaining lines containrules.
Mind that the rules are written in a top-downfashion, but are evaluated bottom-up for now.
Let usconsider a single rule:s -> (WANT :agent i :patient s)The right-hand side is a symbol (WANT :agent:patient) whose tail edges are labeled with states (iand s), and after applying the rule, its head edges arelabeled with new states (s).
All rules are height one,but in the future we will allow for larger subgraphs.In order to deal with symbols of arbitrary headrank (i.e.
symbols that can play multiple roles), wecan use rules using special symbols such as 2=1 and3=1 that split one edge into more than one:i s -> (2=1 :arg e)Using these state-changing rules, the ruleset canbe simplified (see Fig.
3), however the dags look abit different now:(1 / WANT:agent (2 / 2=1:arg (3 / BOY)):patient (4 / BELIEVE:agent 2:patient (5 / GIRL)))Note that we also added weights to the ruleset now.Weights are separated from the rest of a rule by the @sign.
The weight semantics is the usual one, whereweights are multiplied along derivation steps, whilethe weights of alternative derivations are added.ss -> (WANT :agent i :patient s) @ 0.6s -> (BELIEVE :agent i :patient s) @ 0.4i -> (0) @ 0.2s -> (0) @ 0.4i -> (GIRL) @ 0.3s -> (GIRL) @ 0.3i -> (BOY) @ 0.2s -> (BOY) @ 0.2i i -> (2=1 :arg e) @ 0.3i s -> (2=1 :arg e) @ 0.3s s -> (2=1 :arg e) @ 0.3e -> (GIRL) @ 0.4e -> (BOY) @ 0.6Figure 3: Simplified dag acceptor simple.bda.2.1 Membership checking and derivationforestsDAGGER is able to perform various operations ondags.
The instructions can be given in a simple ex-pression language.
The general format of an expres-sion is:(command f1 .. fm p1 .. pn)Every command has a number of (optional) featuresfi and a fixed number of arguments pi.
Most com-mands have a short and a long name; we will use theshort names here to save space.
In order to evaluatea expression, you can either?
supply it on the command-line:./dagger.py -e EXPRESSION?
or read from a file:./dagger.py -f FILEWe will now show a couple of example expres-sions that are composed of smaller expressions.Assume that the dag acceptor of Fig.
2 is savedin the file example.bda, and the file boywants.dagcontains the example dag in PENMAN format.We can load the dag with the expression (g (fboywants.dag)), and the acceptor with the expres-sion (a w (f example.bda)) where w means that theacceptor is weighted.
We could also specify the dagdirectly in PENMAN format using p instead of f. Wecan use the command r:(r (a w (f example.bda)) (g (fboywants.dag)))to check whether example.bda recognizesboywants.dag.
This will output one list item41qWANTBELIEVEBOY GIRL=?Sqnomb wants qinfbBELIEVEBOY GIRL=?Sqnomb wants INFqaccg to believe qaccbBOY GIRL=?SINFNP NP NPthe boy wants the girl to believe himFigure 4: Derivation from graph to tree ?the boy wants the girl to believe him?.qq.S(x1 wants x2)) -> (WANT :agent nomb.x1 :patient inf.x2)inf.INF(x1 to believe x2) -> (BELIEVE :agent accg.x1 :patient accb.x2)accg.NP(the girl) -> (GIRL)nomb.NP(the boy) accb.
(him) -> (BOY)Figure 5: Example dag-to-tree-transducer example.bdt.for each successful derivation (and, if the acceptoris weighted, their weights), in this case: (?s?,?0.1?, 0, ?0?
), which means that the acceptor canreach state s with a derivation weighted 0.1.
Therest of the output concerns dag-to-tree transducersand will be explained later.Note that in general, there might be multiplederivations due to ambiguity (non-determinism).Fortunately, the whole set of derivations can be effi-ciently represented as another dag acceptor with thed command.
This derivation forest acceptor has theset of rules as its symbol and the set of configura-tions (state-labelings of the input dag) as its state set.
(d (a w (f example.bda)) (g fboywants.dag)))will write the derivation forest acceptor to the stan-dard output.2.2 k-best generationTo obtain the highest-weighted 7 dags generated bythe example dag acceptor, run:(k 7 (a w (f example.bda)))(1 / BOY)(1 / GIRL)(1 / BELIEVE :agent (2 / GIRL) :patient 2)(1 / WANT :agent (2 / GIRL) :patient 2)(1 / 0)(1 / BELIEVE :agent (2 / BOY) :patient 2)(1 / WANT :agent (2 / BOY) :patient 2)If the acceptor is unweighted, the smallest dags(in terms of derivation steps) are returned.
(1 / 0)(1 / BOY)(1 / GIRL)(1 / BELIEVE :agent (2 / GIRL) :patient 2)(1 / BELIEVE :agent (2 / BOY) :patient 2)(1 / BELIEVE :agent (2 / GIRL) :patient(3 / 0))(1 / BELIEVE :agent (2 / GIRL) :patient(3 / GIRL))2.3 Visualization of dagsBoth dags and dag acceptors can be visualized usingGRAPHVIZ2.
For this purpose, we use the q (query)command and the v feature:(v (g (f boywants.dag)) boywants.pdf)(v (a (f example.bda)) example.pdf)Dag acceptors are represented as hypergraphs,where the nodes are the states and each hyperedgerepresents a rule labeled with a symbol.2.4 Union and intersectionIn order to construct complex acceptors from sim-pler building blocks, it is helpful to make use ofunion (u) and intersection (i).
The following codewill intersect two acceptors and return the 5 bestdags of the intersection acceptor.
(k 5 (i (a (f example.bda)) (a (fsomeother.bda))))Weighted union, as usual, corresponds to sum,weighted intersection to product.2available under the Eclipse Public Licence from http://www.graphviz.org/42string automata tree automata dag automatacompute .
.
.
strings (sentences) .
.
.
(syntax) trees .
.
.
semantic representationsk-best .
.
.
paths through a WFSA (Viterbi,1967; Eppstein, 1998).
.
.
derivations in a weighted forest(Jime?nez and Marzal, 2000; Huang andChiang, 2005)3EM training Forward-backward EM (Baum et al,1970; Eisner, 2003)Tree transducer EM training (Graehl etal., 2008)?Determinization .
.
.
of weighted string acceptors (Mohri,1997).
.
.
of weighted tree acceptors (Bor-chardt and Vogler, 2003; May andKnight, 2006a)?Transducer composi-tionWFST composition (Pereira and Riley,1997)Many transducers not closed under com-position (Maletti et al, 2009)?General tools AT&T FSM (Mohri et al, 2000),Carmel (Graehl, 1997), OpenFST (Rileyet al, 2009)Tiburon (May and Knight, 2006b),ForestFIRE (Cleophas, 2008; Strolen-berg, 2007)DAGGERTable 1: General-purpose algorithms for strings, trees and feature structures.3 Dag-to-tree transducersDag-to-tree transducers are dag acceptors with treeoutput.
In every rule, the states on the right-handsides have tree variables attached that are used tobuild one tree for each state on the left-hand side.
Afragment of an example dag-to-tree transducer canbe seen in Fig.
5.Let us see what happens if we apply this trans-ducer to our example dag:(r (a t (f example.bdt)) (g (fboywants.dag)))All derivations including output trees will be listed:(?q?, ?1.0?,S(NP(the boy) wants INF(NP(the girl)to believe NP(him))),?the boy wants the girl to believehim?
)A graphical representation of this derivation (top-down instead of bottom-up for illustrative purposes)can be seen in Fig.
4.3.1 Backward application and force decodingSometimes, we might want to see which dags mapto a certain input tree in a dag-to-tree transducer.This is called backward application since we use thetransducer in the reverse direction: We are currentlyimplementing this by ?generation and checking?, i.e.a process that generates dags and trees at the sametime.
Whenever a partial tree does not match theinput tree, it is discarded, until we find a derivationand a dag for the input tree.
If we also restrict thedag part, we have force decoding.4 Future workThis work describes the basics of a dag automatatoolkit.
To the authors?
knowledge, no such im-plementation already exists.
Of course, many algo-rithms are missing, and there is a lot of room for im-provement, both from the theoretical and the practi-cal viewpoint.
This is a brief list of items for futureresearch (Quernheim and Knight, 2012):?
Complexity analysis of the algorithms.?
Closure properties of dag acceptors and dag-to-tree transducers as well as composition withtree transducers.?
Extended left-hand sides to condition on alarger semantic context, just like extended top-down tree transducers (Maletti et al, 2009).?
Handling flat, unordered, sparse sets of rela-tions that are typical of feature structures.
Cur-rently, rules are specific to the rank of thenodes.
A first step in this direction could begone by getting rid of the explicit n=m symbols.?
Hand-annotated resources such as (dag, tree)pairs, similar to treebanks for syntactic repre-sentations as well as a reasonable probabilisticmodel and training procedures.?
Useful algorithms for NLP applications thatexist for string and tree automata (cf.
Ta-ble 1).
The long-term goal could be to build asemantics-based machine translation pipeline.AcknowledgementsThis research was supported in part by ARO grant W911NF-10-1-0533.
The first author was supported by the German ResearchFoundation (DFG) grant MA 4959/1?1.43ReferencesJohn A. Bateman.
1990.
Upper modeling: organizingknowledge for natural language processing.
In Proc.Natural Language Generation Workshop, pages 54?60.L.
E. Baum, T. Petrie, G. Soules, and N. Weiss.
1970.A maximization technique occurring in the statisticalanalysis of probabilistic functions of Markov chains.Ann.
Math.
Statist., 41(1):164171.Bernd Bohnet and Leo Wanner.
2010.
Open sourcegraph transducer interpreter and grammar develop-ment environment.
In Proc.
LREC.Bjo?rn Borchardt and Heiko Vogler.
2003.
Determiniza-tion of finite state weighted tree automata.
J. Autom.Lang.
Comb., 8(3):417?463.Loek G. W. A. Cleophas.
2008.
Tree Algorithms: TwoTaxonomies and a Toolkit.
Ph.D. thesis, Department ofMathematics and Computer Science, Eindhoven Uni-versity of Technology.Jason Eisner.
2003.
Learning non-isomorphic tree map-pings for machine translation.
In Proc.
ACL, pages205?208.David Eppstein.
1998.
Finding the k shortest paths.SIAM J.
Comput., 28(2):652?673.Jonathan Graehl, Kevin Knight, and Jonathan May.2008.
Training tree transducers.
Comput.
Linguist.,34(3):391?427.Jonathan Graehl.
1997.
Carmel finite-state toolkit.http://www.isi.edu/licensed-sw/carmel.Liang Huang and David Chiang.
2005.
Better k-bestparsing.
In Proc.
IWPT.V?
?ctor M. Jime?nez and Andre?s Marzal.
2000.
Computa-tion of the n best parse trees for weighted and stochas-tic context-free grammars.
In Proc.
SSPR/SPR, pages183?192.Tsutomu Kamimura and Giora Slutzki.
1981.
Paral-lel and two-way automata on directed ordered acyclicgraphs.
Inf.
Control, 49(1):10?51.Tsutomu Kamimura and Giora Slutzki.
1982.
Transduc-tions of dags and trees.
Math.
Syst.
Theory, 15(3):225?249.Kevin Knight and Jonathan May.
2009.
Applicationsof weighted automata in natural language processing.In Manfred Droste, Werner Kuich, and Heiko Vogler,editors, Handbook of Weighted Automata.
Springer.Andreas Maletti, Jonathan Graehl, Mark Hopkins, andKevin Knight.
2009.
The power of extended top-downtree transducers.
SIAM J.
Comput., 39(2):410?430.Jonathan May and Kevin Knight.
2006a.
A better n-bestlist: Practical determinization of weighted finite treeautomata.
In Proc.
HLT-NAACL.Jonathan May and Kevin Knight.
2006b.
Tiburon: Aweighted tree automata toolkit.
In Oscar H. Ibarra andHsu-Chun Yen, editors, Proc.
CIAA, volume 4094 ofLNCS, pages 102?113.
Springer.Mehryar Mohri, Fernando C. N. Pereira, and MichaelRiley.
2000.
The design principles of a weightedfinite-state transducer library.
Theor.
Comput.
Sci.,231(1):17?32.Mehryar Mohri.
1997.
Finite-state transducers in lan-guage and speech processing.
Computational Linguis-tics, 23(2):269?311.Fernando Pereira and Michael Riley.
1997.
Speechrecognition by composition of weighted finite au-tomata.
In Finite-State Language Processing, pages431?453.
MIT Press.Daniel Quernheim and Kevin Knight.
2012.
To-wards probabilistic acceptors and transducers for fea-ture structures.
In Proc.
SSST.
(to appear).Michael Riley, Cyril Allauzen, and Martin Jansche.2009.
OpenFST: An open-source, weighted finite-state transducer library and its applications to speechand language.
In Proc.
HLT-NAACL (Tutorial Ab-stracts), pages 9?10.Roger Strolenberg.
2007.
ForestFIRE and FIREWood.a toolkit & GUI for tree algorithms.
Master?s thesis,Department of Mathematics and Computer Science,Eindhoven University of Technology.Andrew Viterbi.
1967.
Error bounds for convolutionalcodes and an asymptotically optimum decoding al-gorithm.
IEEE Transactions on Information Theory,13(2):260?269.44
