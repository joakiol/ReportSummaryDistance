PARAMETER EST IMATION FOR CONSTRAINEDCONTEXT-FREE LANGUAGE MODELSKevin Mark, Michael Miller, Ulf Grenander~ Steve Abney tE lect ron ic  Sys tems and  Signals Research  LaboratoryWash ington  Un ivers i tySt.
Louis,  M issour i  63130ABSTRACTA new language model incorporating both N-gram andcontext-free ideas is proposed.
This constrained context-freemodel is specified by a stochastic context-free prior distribu-tion with N-gram frequency constraints.
The resulting dis-tribution is a Markov random field.
Algorithms for samplingfrom this distribution and estimating the parameters of themodel are presented.1.
INTRODUCTIONThis paper introduces the idea of N-gram constrainedcontext-free language models.
This class of languagemodels merges two prevalent ideas in language modeling:N-grams and context-free grammars.
In N-gram lan-guage models, the underlying probability distributionsare Markov chains on the word string.
N-gram mod-els have advantages in their simplicity.
Both parameterestimation and sampling from the distribution are sim-ple tasks.
A disadvantage of these models is their weakmodeling of linguistic structure.Context-free language models are instances of randombranching processes.
The major advantage of this classof models is its ability to capture linguistic structure.In the following section, notation for stochastic ontext-free language models and the probability of a word stringunder this model are presented.
Section 3 reviews a pa-rameter estimation algorithm for SCF language models.Section 4 introduces the bigram-constrained context-freelanguage model.
This language model is seen to be aMarkov random field.
In Section 5, a random samplingalgorithm is stated.
In Section 6, the problem of param-eter estimation in the constrained context-free languagemodel is addressed.
*Division of Applied Mathematics, Brown University, Provi-dence, Rhode Island 02904tBell Communications Research, Morristown, New Jersey079622.
STOCHASTIC  CONTEXT-FREEGRAMMARSA stochastic ontext-free grammar G is specified by thequintuple < VN, VT, R, S, P > where VN is a finite setof non-terminal symbols, VT is a finite set of terminalsymbols, R is a set of rewrite rules, S is a start symbolin VN, and P is a parameter vector.
If r 6 R, then Pr isthe probability of using the rewrite rule r.For our experiments, we are using a 411 rule grammarwhich we will refer to as the Abney-2 grammar.
Thegrammar has 158 syntactic variables, i.e., IVNI = 158.The rules of the Abney-2 grammar are of the formH -+ G1,G2 .. .
.
Gk where H, Gi 6 VN and k = 1,2 .
.
.
.
.Hence, this grammar is not expressed in Chomsky Nor-mal Form.
We maintain this more general form for thepurposes of linguistic analysis.An important measure is the probability of a deriva-tion tree T. Using ideas from the random branchingprocess literature \[2, 4\], we specify a derivation tree Tby its depth L and the counting statistics zt(i,k),l =1 .. .
.
,n , i  = 1 .
.
.
.
,IVNI, and k = 1 .
.
.
.
.
IRI.
The count-ing statistic zz(i, k) is the number of non-terminals at 6VN rewritten at level I with rule rk 6 R. With thesestatistics the probability of a tree T is given byL IVN\] IRI= H H H (1)l=l i=l k=lIn this model, the probability of a word string W1,N =w:w2...  WN, fl(Wl,N), is given byZ(W:,N) = =(T) (2)TEParses(W,,N)where Parses(W1,N) is the set of parse trees for thegiven word string.
For an unambiguous grammar,Parses(Wl,N) consists of a single parse.1463.
PARAMETER EST IMATION FORSCFGSAn important problem in stochastic language models isthe estimation of model parameters.
In the parameterestimation problem for SCFGs, we observe a word stringW1,N of terminal symbols.
With this observation, wewant to estimate the rule probabilities P. For a grammarin Chomsky Normal Form, the familiar Inside/OutsideAlgorithm is used to estimate P. However, the Abney-2 grammar is not in this normal form.
Although thegrammar could be easily converted to CNF, we prefer toretain its original form for linguistic relevance.
Hence,we need an algorithm that can estimate the probabilitiesof rules in our more general form given above.The algorithm that we have derived is a specific case ofKupiec's trellis-based algorithm \[3\].
Kupiec's algorithmestimates parameters for general recursive transition et-works.
In our case, we only have rules of the followingtwo types:1.
H ---~ G1G2"..Gk where H, Gi E VN and k =1,2 .
.
.
.2.
H -+TwhereHEVN andTEVw.For this particular topology, we derived the followingtrellis-based algorithm.Tre l l i s -based a lgor i thm1.
Compute inner probabilities a( i , j ,a)  = Pr\[o"Wij\] where a E VN and Wij denotes the substringwi ?
?
?
wj.o~(i,i,o') = .p?ldo__wi -I- E "p?ld?--?
'~^eii'trl)'G 1 :G- -~G Io~(i,j,o') = E o~n,e(i,j,o'n,a)Gn :G---~ .
.
.G  n"n,~(i, j, ~m, ~) =bold ari  g o""~ O" -~ 'Qm , .
.
\ , J ,  rn , \ ]ifo" ~ o'm..,  or m = 1. i -1  ?
Ek=,+l  "rite(', k, fire-l, ff) .
(k, J, ~m)if o" ~.
.
.
o'ra- 1 am ?
?
?2.
Compute outer probabilities fl(i,j,o') = Pr\[S :~Wl,i- i  o" W/+X,N\] where o" e VN.fl(1, N, S) = 1.0~(i,  j, if) ---- E bold ,~ ri ~ n) J n_ ,o .
.
.
I Jn tek  , J ,  i f ,i -1+ E E a"'e(k' i 'p'n)f l" '~(k' j '? '
'n)n.-.t .
.
.
.pa.. ,  k=0tints(i, j, crm, o') ={ f~(i, j  o.
)if a ~ .
.
.
a~L E~=~+i ~(j, k, o'~+t)f~.tdi, k, o'm+t, o-)if a ~ ...OynO'rn+l .
.
.3.
Re-estimate P.pnew0"--+ 0"10"2 , .
.a  nN-1 NE/N=i N ?
~j=i  a(z, j, a)fl(i, j, a)new _PaI ._~ T - -Ei:w,=T ca(i, i, o')fl( i, i, a)E/N=1 N ?
Ej=, ~(~, J ~)~(i, j ~)For CNF grammars, the trellis-based algorithm reducesto the Inside-Outside algorithm.
We have tested the al-gorithm on both CNF grammars and non-CNF gram-mars.
In either case, the estimated probabilities areasymptotically unbiased.4.
SCFGS WITH B IGRAMCONSTRAINTSWe now consider adding bigram relative frequencies asconstraints on our stochastic ontext-free trees.
The sit-uation is shown in Figure 1.
In this figure, a word stringis shown with its bigram relationships and its underlyingparse tree structure.In this model, we assume a given prior context-free dis-tribution as given by fl(W1,N) (Equation 2).
This priordistribution may be obtained via the trellis-based esti-mation algorithm (Section 3) applied to a training textor, alternatively, from a hand-parsed training text.
Weare also given bigram relative frequencies,N--1hai,aj(Wl,g) = ~ lq,,aj(wk, w~+l) (3)k=lwhere tri, aj E VT.Given this type of structure involving both hierarchicaland bigram relationships, what probability distributionon word strings should we consider?
The following the-orem states the maximum entropy solution.147SNP VPAr t  NI I v .ths  boy \]Figure 1: Stochastic ontext-free tree with bigram rela-tionships.Theorem 1distribution maximizing the generalized entropy-- E p(c) log p(c) (4)f(c)subjectto the constraints {E\[ha,,as(W1,N)\] : Ha,,aj}ai,a~CVwisLet c = W1,N and f(c) = fl(W1,N).
ThePr(W1,N) = p*(c) = (5)Z-lexp ( E EOtal,a~hal,a2(W1,N))fl(Wl,N)oI~VT G2~VTwhere Z is the normalizing constant.Remarks  The specification of bigram constraints forh(.)
is not necessary for the derivation of this theorem.The constraint function h(.)
may be any function on theword string including general N-grams.
Also, note thatif the parameters o~a1,~,2 are all zero, then this distribu-tion reduces to the unconstrained stochastic context-freemodel.5.
S IMULAT IONFor simulation purposes, we would like to be able todraw sample word strings from the maximum entropydistribution.
The generation of such sentences for thislanguage model cannot be done directly as in the un-constrained context-free model.
In order to generatesentences, a random sampling algorithm is needed.
Asimple Metropolis-type algorithm is presented to samplefrom our distribution.The distribution must first be expressed in Gibbs form:1 -E(W~.N) Pr(Wl,g) = ~e (6)whereE(WI,N) = -- E E ha,, a2h'',a2(Wl,g)oa EVT a2EVT- log 3(Wa,N).
(7)Given this 'energy' E, the following algorithm generatesa sequence of samples, {W 1, W 2, W3,.
.
.
},  from this dis-tribution.Random sampl ing a lgor i thm1.
perturb W i to W new2.
compute AE  = E(W new) - E(W i)3. if AE  < 0 thenWi+T +_ wnewelsewi+l ~ W newp( new W ) = e_AE  with probability = P(W)4. increment i and repeat step 1.In the first step, the perturbation of a word string is doneas follows:1. generate parses of the string W2.
choose one of these parses3.
choose a node in the parse tree4.
generate a subtree rooted at this node according tothe prior rule probabilities5.
let the terminal sequence of the modified tree be thenew word string W new.This method of perturbation satisfies the detailed bal-ance conditions in random sampling.P ropos i t ion  Given a sequenceof samples {W 1, W 2, W3,.
.
.}
generated with the ran-dom sampling algorithm above.
The sequence convergesweakly to the distribution Pr(W1,N).1486.
PARAMETER EST IMATION FORTHE CONSTRAINEDCONTEXT-FREE MODELIn the parameter estimation problem for the constrainedcontext-free model, we are given an observed word stringW1,N of terminal symbols and want to estimate thec~ parameters in the maximum entropy distribution,Pr(W1,N).
One criterion in estimating these parametersis maximizing the likelihood given the observed data.Maximum likelihood estimation yields the following con-dition for the optimum (ML) estimates:0 Pr(W1,N) I = 0 (8)~Olaa ,ab I &~a ,~t~Evaluating the left hand side gives the following maxi-mum likelihood conditionEa .
.
.
.
b \[ha',ab(Wl,g)\] = h?.,?b(W1,N) (O)One method to obtain the maximum likelihood estimatesis given by Younes \[5\].
His estimation algorithm usesa random sampling algorithm to estimate the expectedvalue of the constraints in a gradient descent framework.Another method is the pseudolikelihood approach whichwe consider here.In the pseudolikelihood approach, an approximation tothe likelihood is derived from local probabilities \[1\].
Inour problem, these local probabilities are given by:Pr(wilwl .
.
.
.
.
wi-1, Wi?l .
.
.
.
.
WN) =exp(~,_,,~, + ~,,~,+,)Z(W1,N)EWj:~V T exp(aw,_,,w; + aw:,w,+,)~ti(Wl,N, w~ 10)where ,~i(W1,N,  W~) = ETEParses(w,  ..... w i - l ,w: ,w i+t  ..... wN) 7r(T).The pseudolikelihood ?
is given in terms of these localprobabilities byN?
-- IXPr (w i lw l ' " "Wi - l 'W'+l ' " "WN)  (11)i= lMaximizing the pseudolikelihood ?
is equivalent to max-imizing the log-pseudolikelihood,N-1logi = Nlog~(W1,N)+ 2 ~ ~wk,~_.
(12)k=l- ~ log ~-w,_~,~: + :,,~,+, ~(w1,N, w~)i= l  .
Lw:eVT JWe can estimate the oL parameters by maximizing thelog-pseudolikelihood with respect o the c,'s.
The algo-rithm that we use to do this is a gradient descent al-gorithm.
The gradient descent algorithm is an iterativealgorithm in which the parameters are updated by a fac-tor of the gradient, i.e.,0 logO~(i+1) = ~( i )  "Jr #0~,o2  (13)a I t0"2 O" 1 ~0~where # is the step size and the gradient is given by0 log ?
N- I--2 E awk'wk+ ' l? '
'?~(wk'wk+l)' k= lg ~ 0 CftVi--l,W~ ?Otto~,~i..Ll ~ /TIT l~L.~wlEV.p ET '~ -e * * ~ pi~ VV l ,N ,Wi \ ] .Ottvi--ltw{?Otw{,wi..~.
1 f J  /T I?
- .
I \  \ - -  Ii=l  L..~w~EVT e * ' P i~VVl ,N ,Wi )The gradient descent algorithm is sensitive to the choiceof step size #.
This choice is typically made by trial anderror.7.
CONCLUSIONThis paper introduces a new class of language modelsbased on Markov random field ideas.
The proposedcontext-free language model with bigram constraints of-fers a rich linguistic structure.
In order to facilitate x-ploring this structure, we have presented a random sam-pling algorithm and a parameter estimation algorithm.The work presented here is a beginning.
Further work isbeing done in improving the efficiency of the algorithmsand in investigating the correlation of bigram relativefrequencies and estimated a parameters in the model.References1.
Besag, J., "Spatial Interaction and the Statistical Anal-ysis of Lattice Systems," J. R. Statist.
Soc.
B, Vol.
36,1974, pp.
192-236.2.
Harris, T. E., The Theory of Branching Processes,Springer-Verlag, Berlin, 1963.3.
Kupiec, J., "A trellis-based algorithm for estimating theparameters of a hidden stochastic ontext-free gram-mar," 1991.4.
Miller, M. I., and O'Sullivan, J.
A., "Entropies and Com-binatorics of Random Branching Processes and Context-Free Languages," IEEE Trans.
on Information Theory,March, 1992.5.
Younes, L., "Maximum likelihood estimation for Gibb-sian fields," 1991.149
