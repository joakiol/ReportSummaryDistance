Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 12?23,Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational LinguisticsSupervised Learning of German Qualia RelationsYannick VersleySFB 833Universita?t Tu?bingenversley@sfs.uni-tuebingen.deAbstractIn the last decade, substantial progress hasbeen made in the induction of semantic rela-tions from raw text, especially of hypernymyand meronymy in the English language andin the classification of noun-noun relations incompounds or other contexts.
We investigatethe question of learning qualia-like semanticrelations that cross part-of-speech boundariesfor German, by first introducing a hand-taggeddataset of associated noun-verb pairs for thistask, and then provide classification results us-ing a general framework for supervised classi-fication of lexical relations.1 IntroductionEver since the introduction of wordnets (Millerand Fellbaum, 1991) or more generally machine-readable dictionaries containing semantic relations,researchers have investigated ways to learn such ex-amples automatically from large text corpora, orgeneralize them from existing instances.
Substan-tial research exists on the learning of hyperonymyrelations (Hearst, 1992; Snow et al, 2005; TjongKim Sang and Hofmann, 2009), meronymy relations(Hearst, 1998; Berland and Charniak, 1999; Girjuet al, 2003) and selectional preferences (Erk et al,2010; Bergsma et al, 2008; O?
Se?aghdha, 2010).Both lexicographic research (Chaffin and Her-rmann, 1987; Morris and Hirst, 2004) and researchin cognitive psychology (Vigliocco et al, 2004;McRae et al, 2005), argue that it is important toconsider relations beyond the classical inventoryof hyperonymy and meronymy relations; further-more psychological research on priming (Hare et al,2009) suggests different processing for different re-lations, which would entail that cognitively plau-sible modeling of human language should modelthese relations explicitly rather than simply record-ing untyped associations between concepts (as in the?evocation?
relation proposed for WordNet by Boyd-Graber et al, 2006).One set of suggestions for an extended inventoryof relations can be found in the telic and agentivequalia relations of Pustejovsky (1991) which havebeen shown to be useful in recognizing discourse re-lations (Wellner et al, 2006), or metonymy/coercionphenomena (Verspoor, 1997; Ru?d and Zarcone,2011), and have the property of linking differentparts-of-speech groups, unlike meronymy and hy-peronymy/troponymy.The work we present in this paper consists of adataset of noun-verb associations for German con-crete nouns, which we present in more detail in sec-tion 3, and a state-of-the-art approach to the super-vised classification of such cross-part-of-speech re-lations using informative features from large collec-tions of unannotated text, which we present in sec-tion 4.
Experimental results are discussed in section6.2 Related WorkMost of earlier work on discovering novel instancesof semantic relations was based on surface patternmatching, as presented by Hearst (1998).
In the do-main of finding qualia relations, Cimiano and Wen-deroth (2005) propose patterns such as ?.
.
.
purpose12of X is .
.
.
?
or ?.
.
.
X is used to .
.
.
?, whereas theyargue that agentive qualia are best chosen from asmall, fixed inventory of verbs (e.g., make, bake,create .
.
.
).
Katrenko and Adriaans (2008a) addi-tionally propose ?to Y a (new|complete) X?
and ?a(new|complete) X has been Y?d?
as patterns for agen-tive qualia.Some of the more recent work starts out frommatches extracted by means of such a pattern, butuse supervised training data to learn semantic con-straints that improve the precision by filtering theextracted examples.
Berland and Charniak (1999)use some handcrafted rules to exclude abstract ob-jects from the part-of relations they extract from acorpus, and additionally rank pattern extractions bycollocation strength.
Girju et al (2003) propose aniterative refinement scheme based on taxonomic in-formation from WordNet: In this learning approach,general constraints using top-level semantic classes(entity, abstraction, causal-agent) are passed to a de-cision tree learner and iteratively refined until the se-mantic constraints induced from the classes are nolonger ambiguous.Katrenko and Adriaans (2008b, 2010) present ap-proaches to learn semantic constraints for the use inrecognizing semantic relations between word tokens(SemEval 2007 shared task, see Girju et al, 2009),either in a graph-based generalization of Girju?s it-erative refinement approach that is able to handlesense ambiguities more gracefully, or by clusteringpairs of words by the joint similarity of both relationarguments.A complementary aspect is to improving recallbeyond the possibilities of a few hand-selected pat-terns.
Following Hearst (1998), Girju et al (2003)show that it is possible to find usable patterns byexploiting known positive examples and looking forco-occurrences of these relation arguments in a cor-pus.
However, these patterns usually have low preci-sion and/or very limited recall, meaning that a moreelaborate approach (such as Girju et al?s inductionof semantic constraints) is needed to make the bestuse of them.Yamada and Baldwin (2004) propose to use acombination of templates typical of telic and agen-tive qualia relations (X is worth Y ing, X deservesY ing, a well-Y ed X) and a statistical ranking com-bining association and a classifier learned on pos-itive and negative examples for that role.
Theyfind that the combination of association statistic andclassification worked somewhat better than the tem-plates alone.One approach targeted at exploiting a greaternumber of patterns for hyperonymy relations can befound in the work of Snow et al (2005): they ex-tract patterns consisting of the shortest path in thedependency graph plus an optional satellite and usethe set of all found paths as features in a linear clas-sifier.
The resulting classifier for hyperonymy re-lations outperforms single patterns both in terms ofprecision and in terms of recall; a further improve-ment can be achieved if the frequency of pattern in-stances is binned instead of just occurrence or non-occurrence being recorded.Tjong Kim Sang and Hofmann (2009) investigatethe question whether it is necessary to use syntac-tic (rather than surface) patterns for the hyperonymclassification approach of Snow et al They comparea method of extracting features based on syntax asin Snow et al?s approach with a surface-based al-ternative where the string between two words, plusoptionally one word to the left or right side of theword, is extracted.
Tjong Kim Sang and Hofmannargue that the benefit of the parser (additional recalldue to the better generalization capability of the syn-tactic patterns) is mostly negated by parsing errors:In some informative contexts that the system basedon POS patterns is able to find without problems,parsing errors lead to a parse tree that does not ex-hibit the intended (dependency path) pattern.Several researchers have applied such patternclassification approaches to a larger set of relations,and have demonstrated that extracting a pattern dis-tribution between occurrences and performing su-pervised classification based on this distribution isa promising solution for semantic relations that gobeyond hyperonymy.O?
Se?aghdha and Copestake (2007) use a super-vised classification approach for noun-noun com-pounds combining context features for each of thesingle words with features characterizing the jointoccurrences of the two nouns that are part of the tar-get compound.
In their experiments, they found thatlinear classification using informative (bag-of-wordsand bag-of-triples) features in conjunction with fea-tures aimed at the similarity of each word of the tar-13get pair yields good results.
In particular, the resultsof using a linear classifier with informative corpus-based features that are quite close to those that canbe achieved using a (more accurate, but computa-tionally quite expensive) string kernel or those thatO?
Se?aghdha (2007) achieves using taxonomic infor-mation from WordNet.Turney (2008) presents a general approach forclassifying word pairs into semantic relations by ex-tracting the strings occurring between the two wordsof a pair (up to three words in-between, up to oneword on either side) and using a frequency-basedselection process to select sub-patterns where wordsfrom the extracted context pattern may have been re-placed by a wildcard.
Using standard machine learn-ing tools (a support vector machine with radial basefunction kernel), he is able to reach results that areclose to those possible with previous more special-ized approaches.Similarly, Herdag?delen and Baroni (2009) tacklea variety of problems in semantic relation classifi-cation using a unified approach where frequent uni-grams and bigrams are extracted from co-occurrencecontexts of the target word pair (in addition to fea-tures extracted from general occurrence contexts ofeach word).
Herdag?delen and Baroni?s approachuses a linear SVM (which is faster and better-suitedto large data sets in general than either kernelizedsupport vector machines or nearest-neighbour ap-proaches) yet is able to reach competitive accuracy.In contrast to approaches using generic machinelearning, O?
Se?aghdha and Copestake (2009) andNakov and Kozareva (2011) model the similaritiesbetween related word pairs more explicitly in termsof distributional kernels (O?
Se?aghdha and Copes-take), or as a similarity metric between word pairs(Nakov and Kozareva).
Such approaches allow moreflexibility in the modeling of similarity and the com-bination of lexical and relational similarity mea-sures, but are less well-suited for scaling up to moretraining data.1Because of the need for sufficient training data,purely supervised approaches to learning relations1O?
Se?aghdha and Copestake (2009) reports training timesof slightly more than one day for their most efficient methodwhereas a ten-fold crossvalidation run using SVMperf ?
see thepresentation on p. 6 ?
takes under an hour, i.e., using linearclassification is more efficient by a factor of about 100.in morphologically-rich languages are often lim-ited to the classical relations found in wordnets.Tjong Kim Sang and Hofmann (2009) use a Dutchcorpus and hyperonymy relations from the DutchCornetto wordnet and mention relatively few dif-ferences to approaches on English such as Snowet al (2005).
Kurc and Piasecki (2008) applythe semi-supervised approach of Pantel and Pen-nachiotti (2006) for learning hyperonymy relations,but modify the patterns used to enforce morphosyn-tactic agreement and accommodate a more flexi-ble word order.
Versley (2007) uses Web patternqueries for finding hyperonymy relations and men-tions the fact that greater morphological richnessand the smaller size of the German Web make theuse of Web queries more complex than for English.Outside the realm of hyperonymy, Regneri (2006)uses Web-based pattern search to classify verb-verbassociations into the semantic classes proposed forEnglish by Chklovski and Pantel (2004).
Ru?d andZarcone (2011) perform a corpus study of patternsindicative of telic and agentive qualia relations in aGerman Web corpus, but perform no automatic clas-sification.In summary, the research of Tjong Kim Sang andHofmann (2009) seems to indicate that at least hy-peronymy relations can be found using a shallowpattern approach despite greater word order flex-ibility of languages such as Dutch and German.For cross-part-of-speech relations, such as telic andagentive qualia, such a question has been unad-dressed as of yet, which prompted us to create adataset that is suitable for such an investigation.3 MaterialIn order to investigate general-domain Noun-Verbrelations in German, we first had to create an ap-propriate dataset that captures a realistic notion ofthe relationships that humans infer in a text.
Exist-ing datasets that explore this space (most of themfor English) use a variety of approaches: One ap-proach starts from examples (such as the popularanalogy dataset for English introduced by Turneyand Littman, 2003); other approaches such as thedata collection for the SemEval task on identifyingrelations between nominals (Girju et al, 2009; Hen-drickx et al, 2010) start from common semantic re-14lations and use patterns to gather positive and nega-tive examples by Web queries.In our case, we started from noun-verb associa-tions found in a sample of human-produced asso-ciations to concrete noun stimuli (Melinger et al,2006); starting from the original association data, weexcluded items that were produced by less than threesubjects and used the part-of-speech information at-tached to the data to retrieve only the verb associates.The classification scheme was motivated by exist-ing generative lexicon research (Pustejovsky, 1991;Lenci et al, 2003), but was modeled to achieve agood fit to the associations present in the data ratherthan to force a good fit to any particular theory.?
agentive relations exist between an artifact andan event that creates or procures it (e.g.
bread-bake)?
the telic relations exist between an entity andan event that is related to its purpose or (actualor intended) role:?
telic-artifact holds between an artifact andits intended usage (e.g.
plane-fly)?
telic-role holds between a role (i.e., a pro-fession, organizational position etc.)
andactivities related to that role (e.g.
cowboy-ride)?
telic-bodypart holds between a body partand its intended uses (e.g.
eye-see)?
the behaviour group of relations hold betweenan entity and events that are caused by it, butare not necessarily intentional or related to arole that it fulfills:?
behaviour-animate are typical activitiesperformed by animate entities that are un-related to the role that they fulfill for hu-mans (e.g., dog-bark)?
behaviour-artifact relates artifacts to (usu-ally) unintended behaviour associatedwith them (e.g., moped-rattle)?
behaviour-environment relates elementsof the environment to events that go onaround them (e.g., sun-shine)?
location relations hold between elements of theenvironment and activities typically performedin or at them (e.g., mountain-climb)?
grooming relations hold between artifacts andactivities that contribute to the readiness of anartifact (or body part) for its intended use butare not directly related to it (e.g., plant-water,hair-dye)In comparison to standard schemes such as SIM-PLE (Lenci et al, 2003), we have extended the set oftelic and agentive qualia from the original generativelexicon approach by supplementing it with relationsthat describe the affordances of objects or guides theinterpretative linking of objects and events, namelylocation for affordances of elements of the environ-ment and grooming for object-related actions thatmay not be necessary for a differently-built objectwith that same function, and finally behaviour de-scribes events that co-occur with objects but are usu-ally not part of a human agent?s action plan.As a refinement, we subdivided the telic qualiaand behaviour relations, in particular specifying anytelic relation with the reason a concrete object maybe relevant for goal-directed processing ?
either byteleological interpretation of body parts, by the cre-ation of artifacts with a specific purpose, or the es-tablishment of roles with social conventions sup-porting certain types of actions.Among the responses collected by Melinger et al(2006), we found relatively few instances that weregenuinely ambiguous (Drachen - fliegen, which mayeither be interpreted as ?kite/fly?, in which case itwould be a telic-artifact relation, or as ?dragon/fly?,in which case it would be a behaviour-animate rela-tion), but found that domestic animals (cows, horses,dogs) have affordances such as horse-ride or dog-bark that indicate they are conceptualized as instru-ments serving a particular goal (which means thatthe relation should be labeled as telic-artifact ratherthan as behaviour-animate).In the associated word pairs, we also found re-lations such as Zwiebel-schneiden (?onion-cut?)
orHandtuch-duschen (?towel-shower?)
where the ac-tion is related to a thing?s purpose but not identicalto it (towels are used to dry yourself after showering,and people acquire onions to eat them after havingcut them).
Our initial annotation included a com-bination between the qualia-like relations presentedhere and an additional event-semantic relation link-ing the elicited event and the intended affordance of15the object.
However, the event relation was left outof the dataset used in the experiments to avoid datasparsity.In our dataset with 641 items, the most fre-quent relations are telic-artifact (425 instances),behaviour-animate (94 instances), telic-role (35 in-stances), telic-bodypart (24 instances).
The other re-lations have between 2 and 17 instances each (seetable 3).
The relationship data is therefore heavilyskewed.4 Classification ApproachOur classification approach is aimed at a practi-cal toolkit for supervised classification of lexical-semantic relations, similar in spirit to the BagPackapproach of Herdag?delen and Baroni (2009) butadapted for the use in morphologically-rich lan-guages, in particular German.In addition to the surface-based unigram and bi-gram features, we use features based on dependencysyntax, which is more robust against variation inword order, and allows to reattach separable verbprefixes.4.1 PreprocessingTo see why a very shallow approach may be less use-ful for German, let us consider a simple direct (ac-cusative) object relation such as between aufessen(eat up) and Kuchen (cake): this relation could berealized in a variety of ways depending on clausetype and constituent order, as illustrated in example(1).
(1) a. Peter isst den Kuchen auf.Peter eats the cake up.
?Peter eats up the cake?.b.
Den Kuchen hat Peter aufgegessen.The cakeacc has Peter eaten-up.
?Peter has eaten up the cake?.c.
.
.
.
dass Peter den Kuchen aufisst.. .
.
that Peter the cake up-eat.?.
.
.
that Peter eats up the cake?.In German, clause type decides whether the verbis in verb-second position (1a) or at the end of theclause (1b,1c); additionally, as in (1a), prefixes ofverbs may be stranded at the end of a clause with theverb in verb-second position.In addition to morphological analysis, hence, reat-tachment is necessary in such cases as (1a), andparsing is necessary to reattach prefix and verb.
Incases such as (1b), word order variation also needsto be taken into account in order to recover the directobject relation, unlike in languages with less-flexibleword order.As a text collection that furnishes contexts for thewords or word pairs that interest us, we use the web-news corpus, a collection of online news articles col-lected by Versley and Panchenko (2012).
For theprocessing of this 1.7 billion word corpus, we usea pipeline that relies on deterministic dependencyparsing to provide complete dependency parses at aspeed that is suitable for the processing of Web-scalecorpora.The parsing model is based on MALTParser, atransition-based parser, and uses part-of-speech andmorphological information as input.
Morphologicalinformation is annotated using RFTagger (Schmidand Laws, 2008), a state-of-the-art morphologicaltagger based on decision trees and a large con-text window (which allows it to model morpho-logical agreement more accurately than a normaltrigram-based sequence tagger).
While transition-based parsers are quite fast in general, an SVM clas-sifier (which is used in MALTParser by default) be-comes slower with increasing training set.
In con-trast, using the MALTParser interface to LibLinearby Cassel (2009), we were able to reach a muchlarger speed of 55 sentences per second (against 0.4sentences per second for a more feature-rich SVM-based model that reaches state of the art perfor-mance).For lemmatization, we use the syntax-basedTu?Ba-D/Z lemmatizer (Versley et al, 2010), whichuses a separate morphological analyzer and somefallback heuristics.
The SMOR morphology(Schmid et al, 2004) serves to provide morpholog-ical analyses for novel words, covering inflection,derivation and composition processes.
For unana-lyzed novel words that are not covered by SMOR,the lemmatizer falls back to surface-based guessingheuristics.
It uses morphological and syntactic in-formation to provide more accurate lemmas; In ad-dition to dependency structures, the morphologicaltags from RFTagger as well as global frequency in-formation are used.164.2 ClassificationFor classification, we use the following learningmethods:?
For the SVMperf classifier, the set of possiblelabels is decomposed into binary problems us-ing the one-vs-all scheme (for each possible la-bel, a classifier is trained that receives the in-stances of this label as positive instances andthe others as negative instances).
SVMperf al-lows the training of models that either optimize(an upper bound for) the accuracy (SVMacc)or the f-measure (SVMF) of positive instances(Joachims, 2005).?
The Maximum Entropy (MaxEnt) classifierdirectly learns the multiclass decision.
Here,we used the AMIS package by Miyao and Tsu-jii (2002).All experiments are run in a ten-fold crossvalidationsetup where the data is split ten portions and eachportion (fold) is tagged using a classifier trained onthe remaining nine folds.
This setup leads to de-creased variationAs noted in section 6, SVMperf using optimizationfor accuracy (i.e., a standard linear kernel SVM withhinge loss and a one-versus-all reduction to handlethe multiclass problem) performs best on the two ag-gregate measures that we used (accuracy and macro-averaged F).
Hence, most results we report in thelater part only use the standard SVM learner.4.3 FeaturesThe first group of surface-based features usesa similar technique to Herdag?delen and Baroni(2009): given the co-occurrences of two words Xand Y with at most 4 words in-between, we extractfrequent unigrams and bigrams.
Because we canmaintain the sparsity of the resulting feature vector(see section 5), we can use a larger list of 10 000each of the most frequent unigrams and bigrams(w12) alternatively to a list with only 2 000 entrieseach (w12:2k).
The lem12 feature uses the sameapproach, but uses lemmas instead.A second group of features uses a path-basedrepresentation based on a modified version of the de-pendency parse (where the main verb, and not theauxiliary verb is the head of a clause and is con-nected to both the subject and its other arguments).In the path-based representation, we can extractthe (shortest) path between the two target words inthe dependency graph.
The rel feature records thecomplete path (labeled dependency edges as wellas lemmas of intervening nodes) between the targetwords.
In contrast, the sat feature records labeleddependency edges as well as lemmas of the depen-dents of one of the target words.Because the rel feature yields relatively large(and therefore sparse) strings, we also decomposethe dependency path in triples consisting of labeleddependency edge in the path and the two nodes ad-jacent to it (with the endpoints replaced by ?w1?
and?w2?, respectively) for the triples feature.In order to emulate the feature extraction of Snowet al (2005), we introduce a relsat feature, whichpairs the path (as in the rel feature) with one de-pendent of either target word.
The relsat featurewould be able to model patterns such as ?w1 andother w2?, where a modifier (?other?)
is not part ofthe shortest dependency path between w1 and w2.In addition, a feature based on GermaNet (Hen-rich and Hinrichs, 2010) uses taxonomic informa-tion: possible hypernyms of the noun and verb inthe pair are extracted, and are used by themselves(e.g.
?noun is a hyponym of ?thing?
?, or ?verb is ahyponym of ?communicate?
?)
and in combinationsof up to two of these possible hypernym labels.In addition to taxonomic information from Ger-maNet, we use distributional similarity featuresfor single words.
For the nouns, we use distribu-tional features based on the co-occurrence of pre-modifying adjectives, which Versley and Panchenko(2012) found to work better than other grammatical-relation-based collocates (attr1), while we usePado?
and Lapata?s (2007) method of gathering andweighting collocates based on distance in the depen-dency graph for the verbs (pl2).
Herdag?delen andBaroni (2009) simply use a window-based approachfor gathering collocates, which we reimplemented asa simpler way of capturing distributional similarity.The resulting features are named w1 and w2.17Seine Tante backt ta?glich leckeren KuchenDET NSUBJ ADVOBJAAMODHis aunt bakes daily luscious cake?his aunt bakes luscious cake every day?w12 Seinew2,w1 Tantew2,w1 w2 ta?glichw1lem12 seinw2,w1 Tantew2,w1 w2 ta?glichw1rel ?OBJAsat w2ADV:ta?glich w1AMOD:leckerw1NSUBJ:Tantetriples w1 ?OBJAw2relsat ?OBJA/w2ADV:ta?glich?OBJA/w1AMOD:lecker?OBJA/w1NSUBJ:TanteDue to the short path between w1 and w2, the triples andrel features are not very different in the example.
In case ofmore complicated constructions, the triples approach wouldyield multiple simpler features whereas rel would yield onesingle complex string.Figure 1: Kinds of features5 Count TransformationsIt is a well-known fact in distributional semanticsthat raw observation counts for context items (bethey elements surrounding single word occurrencesor elements extracted from the occurences of twowords together) are incomparable for different targetwords/target pairs (since their frequency can differ)as well as for different context items.
As a result, re-searchers have proposed different approaches to pro-duce transformed vectors using more sophisticatedassociation statistics (see Dumais, 1991, Weedset al, 2004, Turney and Pantel, 2010, inter alia).In our case, we implemented L1 normalization(which normalizes for target word frequency), a con-servative estimate for pointwise mutual information(which normalizes for the frequencies of both targetword and feature), and the G2 log-likelihood mea-sure of Dunning (1993), which gives significancescores (i.e., numbers that invariably grow both withtarget and feature frequency, even if the associationstrength ?
the relation between actual occurrencesand those that would be expected when assuming noassociation ?
is constant).
In both cases, very fre-quent features would be emphasized in comparisonto medium- and low-frequency features.In the realm of supervised learning, an additionalchoice has to be made among learning methods thatcan classify words or word pairs using large featurevectors ?
most commonly using nearest-neighbourclassification (Nakov and Kozareva, 2011), us-ing custom kernels in support vector classification(O?
Se?aghdha and Copestake, 2009; Turney, 2008),or by using appropriate techniques to represent thefeature vectors in linear classification.In comparison to the former methods, linear clas-sification scales better with the number of exam-ples (where nearest-neigbour and kernel-based tech-niques both show strongly superlinear behaviour)and would be the method of choice for large-scaleclassification.Herdag?delen and Baroni (2009) propose to mapthe values computed by association statistics bycomputing mean and standard deviation of each fea-ture and mapping the range [?
?2?, ?+2?]
of asso-ciation scores for that feature (seen over the valuesof that feature for all target pairs) to the range [0, 1]in the input for the classifier, clamping values out-side that range to 0 or 1, respectively.Unfortunately, the approach proposed byHerdag?delen and Baroni has the property that anassociation score of 0 is mapped to a non-zerofeature value for the classifier, which means thatfeature vectors are no longer sparse (i.e., instead ofonly storing non-zero values for context items thatare informative, values for all context items have tobe processed).To keep the sparsity of the transformed counts,we always use 0 as the lower bound of the mapping(such that zero values stay zero values).
In addi-tion to the Herdagdelen and Baroni?s mean/variance-based threshold, we investigated the following pos-sibilities for fixing the upper bound:?
MI scale: use a constant upper bound of 1 on (aconservative estimate of) the pointwise mutualinformation.22To yield a conservative MI estimate, we use the discountingfactor introduced by Pantel and Lin (2002).
The pointwise mu-tual information value normalizes the frequency of both wordsof a pair, hence all mutual information values are on a commonscale.
A threshold of 1 in this case corresponds to two items oc-18baselines/single features Acc MacroFrandom 0.463 0.090telic-artifact 0.663 0.080w12/L1-norm/AMIS 0.677 0.181w12/L1-norm/SVMacc 0.715 0.212w12/L1-norm/SVMF 0.674 0.120w12/L1-norm 0.715 0.212lem12/G2-quant 0.703 0.204rel/L1-quant 0.722 0.154sat/L1-norm 0.700 0.185triples/L1-quant 0.741 0.192triples/G2-norm 0.739 0.212relsat/L1-quant 0.698 0.154attr1+pl2/MI-thr 0.800 0.460w1+w2/MI-thr 0.807 0.468GermaNet, no combination 0.846 0.450GermaNet, degree=2 0.851 0.516Table 1: Trivial and single-feature baselines (using SVM-acc unless noted otherwise)?
norm: use a value based on mean and standarddeviation of the occurring values for one givenfeature (?+ 2?).?
quant: use a fixed quantile (99%) of all valuesfor a feature for the upper bound of the map-ping interval.In addition, to mapping feature values onto theunit interval [0, 1], we investigated the usefulness ofmaking the features binary-valued by mapping allvalues lower than the threshold to zero.
While intu-itively a continuous-valued feature should be moreinformative, the high dimensionality of the featurespace may mean that noisy feature extraction ulti-mately leads to a worse model in the continuous-feature case.6 Results and DiscussionBecause of the skewed distribution, it is useful tolook not only at the overall accuracy (Acc) but alsoat the macro-average of the F-measure of all rela-tions (MacroF).
The macro-averaged F-measure re-flects the ability of the system to recognize all re-curring together about exp(1) ?
2.7 times as often as would beexpected from the marginal distribution for that co-occurrencerelation.combinations Acc MacroFtriples/G2-norm 0.739 0.212triples+w12/G2-norm 0.733 0.206triples+rel/G2-norm 0.725 0.190triples+sat/G2-norm 0.738 0.200triples+relsat/G2-norm 0.729 0.184triples+w1+w2/MI-thr 0.816 0.469triples+attr1+pl2/MI-thr 0.807 0.431GermaNet 0.851 0.516GermaNet+triples/G2-norm 0.853 0.482GermaNet+triples/MI-thr 0.855 0.484GermaNet+w12/G2-norm 0.855 0.496GermaNet+w12/MI-thr 0.858 0.510GWN+w12+triples/G2-norm 0.852 0.462GWN+w12+triples/MI-thr 0.849 0.478GermaNet+w1+w2/MI-thr 0.828 0.496Table 2: Combination results (using SVMacc)lations since it weighs all relation types equally, in-stead of (implicitly) weighting by token count whereunder-predicting rare relation types normally yieldsa higher accuracy.
As is evident from table 1, theaccuracy baseline for the most frequent label (telic-artifact) is already quite high.Looking at results with various scaling methodsand learners on single features (table 1), we foundthat the SVMacc learner consistently yields betteraccuracy and macro-averaged F-measure than theother two learners.
For the weighting functions, wefound that none of the measures was consistentlybetter than the others; results for the single featuresin table 1 are reported for a weighting function thatworks best for either accuracy or macro-averaged F-measure using.
(For space reasons, table 1 showsnumbers only for the w12 feature and L1-norm scal-ing; other features and settings show a similar rela-tion between the scores for different learners).As in the investigation by O?
Se?aghdha and Copes-take (2007), dependency triples from the path be-tween the two target words are the most effectivefeature representation and yields both the great-est accuracy value (with L1 scaling and quantile-based setting of thresholds) and the greatest F-measure macroaverage (with G2 scaling and settingof thresholds based on average and standard devi-ation).
Combination of the triples feature with19agentive beh-anim beh-artif beh-body beh-env grooming location telic-artif telic-body telic-rolecount 14 94 13 2 5 17 12 425 24 35w12 0.105 0.513 0.000 0.000 0.000 0.000 0.154 0.834 0.214 0.255triples 0.125 0.601 0.000 0.000 0.000 0.000 0.153 0.853 0.153 0.238attr1+pl2 0.333 0.826 0.258 0.000 0.000 0.500 0.421 0.874 0.636 0.754w1+w2 0.385 0.834 0.222 0.000 0.000 0.400 0.571 0.877 0.619 0.767GermaNet 0.480 0.859 0.190 0.000 0.000 0.451 0.636 0.909 0.773 0.857GWN+w12 0.400 0.857 0.133 0.000 0.333 0.384 0.600 0.916 0.600 0.873Table 3: Results by relationother features based on paired co-occurrences doesnot lead to further improvements, especially withthose features that also express information from thedependency path (rel,relsat).In comparison, the accuracy of the GermaNet hy-pernyms feature (which includes combinations ofthe hypernyms of first and second word) is muchhigher than the versions that do not make use ofhand-crafted taxonomic knowledge, which is sur-prising since it uses only taxonomic and no rela-tional information.
The pairwise feature combina-tion for GermaNet features yields another small im-provement over these already very good results.
Dis-tributional information on single words, both thestrictly window-based w1+w2 feature and the onethat is based on more elaborated distributional mod-eling (attr1+pl2) show quite good results thatshow further (but relatively small) improvementswhen combined with the triples feature.The importance of taxonomic (or, alternatively,distributional semantic) information for the task pro-posed here - namely, the supervised classification ofqualia-like relations - partly mirrors results for thesupervised classification of relations between nomi-nals, where O?
Se?aghdha and Copestake (2007) findthat their best system for distributional similaritybased on the BNC performs at about the same levelas a (somewhat simpler) approach using WordNet-based classification (O?
Se?aghdha, 2007), with onlymuch more sophisticated approaches such as the oneof O?
Se?aghdha and Copestake (2009), which alsomakes use of a considerably larger textual basis toimprove results over the level of the WordNet-basedapproach.Another reason for the importance of taxonomicinformation in this task may lie in the fact that thedifferent relations have relatively strong selectionalrestrictions (for animate objects, roles/professions,body parts, or artifacts on the noun side, and certaintypes of actions or events on the verb side).Looking at the results for each relation in table3, we see that both telic-artifact and behaviour-animate, the two relations with the largest counts,are classified quite reliably, while behaviour-bodypart and behaviour-environment, the two rela-tions with very few examples, are never found bythe system.
Among the other relations, taxonomi-cal information for nouns and verbs seems to be in-strumental for adequate classification of the groom-ing relation and possibly also for location, telic-bodypart and telic-role.7 SummaryIn this paper, we have presented a dataset contain-ing cross-part-of-speech relations between concretenouns and human verb associates and demonstrateda state-of-the-art approach for the supervised mul-ticlass classification of the qualia relations in thisdataset.3 Our results show that taxonomic informa-tion from GermaNet is much superior to all otherfeatures, while corpus-based dependency triples arestill visibly superior to shallow surface-based fea-tures.Important questions for future research would in-clude a more direct comparison to other languages(ideally using a similar data set and informationsources) to tease apart the influences of word order,taxonomic organization, and data sparsity, respec-tively.3The dataset and future corrected/improved versions, areavailable on request.
Please feel free to send an email to theauthor if you want to use it or produce a create a version foranother language.20Acknowledgements The work described in thispaper was funded by the Deutsche Forschungsge-meinschaft (DFG) as part of Collaborative ResearchCentre (SFB) 833.
The author gratefully acknowl-edges Melike Heubach?s help with the annotation ofthe dataset.ReferencesBergsma, S., Lin, D., and Goebel, R. (2008).Discriminative learning of selectional preferencefrom unlabeled text.
In EMNLP 2008.Berland, M. and Charniak, E. (1999).
Finding partsin very large corpora.
In Proceedings of ACL-1999.Boyd-Graber, J., Fellbaum, C., Osherson, D., andSchapire, R. (2006).
Adding dense, weightedconnections to WordNet.
In Proceedings of theGlobal WordNet Conference.Cassel, S. (2009).
MaltParser and LIBLINEAR- transition-based dependency parsing with lin-ear classification for feature model optimization.Master?s thesis, Uppsala University.Chaffin, R. and Herrmann, D. J.
(1987).
Relationelement theory: A new account of the represen-tation and processing of semantic relations.
InGorfein, D. S., editor, Memory and Learning: theEbbinghaus Centennial Conference.
Erlbaum.Chklovski, T. and Pantel, P. (2004).
VerbOcean:Mining the web for fine-grained semantic verb re-lations.
In Proc.
EMNLP 2004.Cimiano, P. and Wenderoth, J.
(2005).
Automati-cally learning qualia structures from the web.
InProceedings of the ACL?05 Workshop on DeepLexical Acquisition.Dumais, S. (1991).
Improving the retrieval of infor-mation from external sources.
Behavior ResearchMethods, Instruments and Computers, 23(2):229?236.Dunning, T. (1993).
Accurate methods for the statis-tics of surprise and coincidence.
ComputationalLinguistics, 19(1):61?74.Erk, K., Pado?, S., and Pado?, U.
(2010).
A flexi-ble, corpus-driven model of regular and inverseselectional preferences.
Computational Linguis-tics, 36(4):723?761.Girju, R., Badulescu, A., and Moldovan, D. (2003).Learning semantic constraints for the automaticdiscovery of part-whole relations.
In NAACL-HLT2003.Girju, R., Nakov, P., Nastase, V., Szpakowicz, S.,and Turney, P. (2009).
Classification of semanticrelations between nominals.
Language Resourcesand Evaluation, 43(2):105?121.Hare, M., Jones, M., Thomson, C., Kelly, S., andMcRae, K. (2009).
Activating event knowledge.Cognition, 111:151?167.Hearst, M. (1992).
Automatic acquisition of hy-ponyms from large text corpora.
In Proc.
of the14th International Conference on ComputationalLinguistics (COLING 92).Hearst, M. (1998).
Automated discovery of word-net relations.
In WordNet: An Electronic LexicalDatabase.
MIT Press, Cambridge (MA), USA.Hendrickx, I., Kim, S. N., Kozareva, Z., Nakov, P.,Se?aghdha, D. O., Pado?, S., Pennacchiotti, M., Ro-mano, L., and Szpakowicz, S. (2010).
Semeval-2010 task 8: Multi-way classification of semanticrelations between pairs of nominals.
In SemEval2010.Henrich, V. and Hinrichs, E. (2010).
GernEdiT -the GermaNet editing tool.
In Proceedings of theSeventh Conference on International LanguageResources and Evaluation (LREC 2010), pages2228?2235.Herdag?delen, A. and Baroni, M. (2009).
BagPack:A general framework to represent semantic rela-tions.
In ACL09 Workshop on Geometric Modelsof Natural Language Semantics (GEMS09).Joachims, T. (2005).
A support vector method formultivariate performance measures.
In Proceed-ings of the International Conference on MachineLearning (ICML).Katrenko, S. and Adriaans, P. (2008a).
Qualia struc-tures and their impact on the concrete noun cat-egorization task.
In ESSLLI 2008 workshop onDistributional Lexical Semantics.Katrenko, S. and Adriaans, P. (2008b).
Semantictypes of some generic relation arguments: Detec-tion and evaluation.
In ACL/HLT 2008.21Katrenko, S. and Adriaans, P. (2010).
Finding con-straints for semantic relations via clustering.
InCLIN 2010.Kurc, R. and Piasecki, M. (2008).
Automatic ac-quisition of wordnet relations by the morpho-syntactic patterns extracted from the corpora inPolish.
In Proceedings of the International Multi-conference on Computer Science and InformationTechnology - Third International Symposium Ad-vances in Artificial Intelligence and Applications(IMCSIT 2008).Lenci, A., Calzolari, N., and Zampolli, A.
(2003).SIMPLE: Plurilingual semantic lexicons for nat-ural language processing.
Linguistica Compu-tatazionale, 16?17:323?352.McRae, K., Cree, G., Seidenberg, M., and Mc-Norgan, C. (2005).
Semantic feature produc-tion norms for a large set of living and nonliv-ing things.
Behaviour Research Methods, 37:547?559.Melinger, A., Schulte im Walde, S., and Weber, A.(2006).
Characterizing response types and reveal-ing noun ambiguity in german association norms.In Workshop on Making Sense of Sense: BringingPsycholinguistics and Computational LinguisticsTogether.Miller, G. A. and Fellbaum, C. (1991).
Semanticnetworks of English.
Cognition, 41:197?229.Miyao, Y. and Tsujii, J.
(2002).
Maximum entropyestimation for feature forests.
In HLT 2002.Morris, J. and Hirst, G. (2004).
Non-classical lexicalsemantic relations.
In HLT/NAACL Workshop onComputational Lexical Semantics.Nakov, P. and Kozareva, Z.
(2011).
Combining re-lational and attributional similarity for semanticrelation classification.
In RANLP 2011.O?
Se?aghdha, D. and Copestake, A.
(2009).
Usinglexical and relational similarity to classify seman-tic relations.
In EACL 2009.O?
Se?aghdha, D. (2007).
Annotating and learningcompound noun semantics.
In Proceedings of theACL07 Student Research Workshop.O?
Se?aghdha, D. (2010).
Latent variable models ofselectional preference.
In ACL 2010.O?
Se?aghdha, D. and Copestake, A.
(2007).
Co-occurrence contexts for noun compound interpre-tation.
In ACL 2007 Workshop on A Broader Per-spective on Multiword Expressions.O?
Se?aghdha, D. and Copestake, A.
(2009).
Us-ing lexical and relational similarity to classify se-mantic relations.
In 12th Conference of the Eu-ropean Chapter of the Association for Computa-tional Linguistics (EACL).Pado?, S. and Lapata, M. (2007).
Dependency-basedconstruction of semantic space models.
Compu-tational Linguistics, 33(2):161?199.Pantel, P. and Lin, D. (2002).
Discovering wordsenses from text.
In Proceedings of ACM Confer-ence on Knowledge Discovery and Data Mining(KDD-02), pages 613?619.Pantel, P. and Pennachiotti, M. (2006).
Espresso:Leveraging generic patterns for automatically har-vesting semantic relations.
In COLING/ACL2006.Pustejovsky, J.
(1991).
The generative lexicon.Computational Linguistics, 17(4):409?441.Regneri, M. (2006).
VerbOzean - maschinellesLernen von semantischen Relationen zwischendeutschen Verben.
Bachelorarbeit, Universita?t desSaarlandes.Ru?d, S. and Zarcone, A.
(2011).
Covert events andqualia structures for German verbs.
In Metonymy2011.Schmid, H., Fitschen, A., and Heid, U.
(2004).SMOR: A German computational morphologycovering derivation, composition and inflection.In Proceedings of LREC 2004.Schmid, H. and Laws, F. (2008).
Estimation of con-ditional probabilities with decision trees and anapplication to fine-grained POS tagging.
In COL-ING 2008.Snow, R., Jurafsky, D., and Ng, A. Y.
(2005).
Learn-ing syntactic patterns for automatic hypernym dis-covery.
In NIPS 2005.Tjong Kim Sang, E. and Hofmann, K. (2009).
Lexi-cal patterns or dependency patterns: Which is bet-ter for hypernym extraction.
In CoNLL-2009.22Turney, P. (2008).
A uniform approach to analogies,synonyms, antonyms and associations.
In Coling2008.Turney, P. and Littman, M. (2003).
Learning analo-gies and semantic relations.
Technical ReportERB-1103, National Research Council, Institutefor Information Technology.Turney, P. and Pantel, P. (2010).
From frequencyto meaning: Vector space models of seman-tics.
Journal of Artificial Intelligence Research,37:141?188.Versley, Y.
(2007).
Using the Web to resolvecoreferent bridging in German newspaper text.In Proceedings of GLDV-Fru?hjahrstagung 2007,Tu?bingen.
Narr.Versley, Y., Beck, A. K., Hinrichs, E., and Telljo-hann, H. (2010).
A syntax-first approach to high-quality morphological analysis and lemma disam-biguation for the Tu?Ba-D/Z treebank.
In Proceed-ings of the 9th Conference on Treebanks and Lin-guistic Theories (TLT9).Versley, Y. and Panchenko, Y.
(2012).
Not justbigger: Towards better-quality Web corpora.
InProceedings of the 7th Web as Corpus Workshop(WAC-7), pages 44?52.Verspoor, C. M. (1997).
Contextually-DependentLexical Semantics.
PhD thesis, University of Ed-inburgh.Vigliocco, G., Vinson, D., Lewis, W., and Garrett,M.
(2004).
Representing the meanings of objectand action words: The featural and unitary se-mantic space hypothesis.
Cognitive Psychology,48:422?488.Weeds, J., Weir, D., and McCarthy, D. (2004).
Char-acterizing measures of lexical distributional simi-larity.
In CoLing 2004.Wellner, B., Pustejovsky, J., Havasi, C., Rumshisky,A., and Sauri, R. (2006).
Classification of dis-course coherence relations: An exploratory studyusing multiple knowledge sources.
In Proc.
7thSIGdial Workshop on Discourse and Dialogue.Yamada, I. and Baldwin, T. (2004).
Automatic dis-covery of telic and agentive roles from corpusdata.
In Proceedings of the 18th Pacific Asia Con-ference on Language, Information and Computa-tion (PACLIC 18).23
