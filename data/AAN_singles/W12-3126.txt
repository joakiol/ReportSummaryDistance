Proceedings of the 7th Workshop on Statistical Machine Translation, pages 210?221,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsCCG Syntactic Reordering Models for Phrase-based Machine TranslationDennis N. MehayThe Ohio State UniversityColumbus, OH, USAmehay@ling.ohio-state.eduChris BrewEducational Testing ServicePrinceton, NJ, USAcbrew@ets.orgAbstractStatistical phrase-based machine translationrequires no linguistic information beyondword-aligned parallel corpora (Zens et al,2002; Koehn et al, 2003).
Unfortunately,this linguistic agnosticism often produces un-grammatical translations.
Syntax, or sentencestructure, could provide guidance to phrase-based systems, but the ?non-constituent?
wordstrings that phrase-based decoders manipu-late complicate the use of most recursive syn-tactic tools.
We address these issues byusing Combinatory Categorial Grammar, orCCG, (Steedman, 2000), which has a muchmore flexible notion of constituency, therebyproviding more labels for putative non-constituent multiword translation phrases.
Us-ing CCG parse charts, we train a syntacticanalogue of a lexicalized reordering model bylabelling phrase table entries with multiwordlabels and demonstrate significant improve-ments in translating between Urdu and En-glish, two language pairs with divergent sen-tence structure.1 IntroductionStatistical phrase-based machine translation (PMT)is attractive, as it requires no linguistic informa-tion beyond word-aligned parallel corpora (Zens etal., 2002; Koehn et al, 2003).
Unfortunately, thislinguistic agnosticism leaves phrase-based systemswith no precise characterization of the word orderrelationships between languages, often leading toungrammatical translations.
Syntax could provideguidance to phrase-based systems, by steering themtowards reorderings that reflect the structural rela-tionships between languages, but using syntax toguide a phrase-based system is problematic.
Phrase-based systems build the result incrementally fromthe beginning of the target string to the end, andthe intermediate strings need not constitute completetraditional syntactic constituents.
It is difficult toreconcile traditional recursive syntactic processingwith this regime, because not all intermediate stringsconsidered by the decoder would even have a syntac-tic category to assess.
As a result, most phrase-baseddecoders control reordering using simple distance-based distortion models, which penalize all reorder-ing equally, and lexicalized reordering models (Till-mann, 2004; Axelrod et al, 2005), which probabilis-tically score various reordering configurations con-ditioned on specific lexical translations.
While un-doubtedly better than nothing, these models performpoorly when languages diverge considerably in sen-tence structure.
Distance-based distortion modelsare too coarse-grained to distinguish correct fromincorrect reordering, while lexical reordering mod-els suffer from data sparsity and fail to capture moregeneral patterns.
We argue that finding a way tolabel translation phrases with syntactic labels willabstract over the observed reordering configurationsthereby address both all three deficiencies of granu-larity, data sparsity and lack of generality.The present work presents a novel syntactic ana-logue of the lexicalized reordering model that usesmultiword syntactic labels to capture the general re-ordering patterns between two languages with verydifferent word order.
We accomplish this by usingCombinatory Categorial Grammar, or CCG (Steed-210man, 2000), a word-centered syntax that allows agreat deal of flexibility in how sentence analysesare formed.
Syntactic derivations in CCG are mas-sively spuriously ambiguous, i.e., there are manyways to derive the same semantic analysis of a sen-tence, similar to how a mathematical equation canbe reduced by canceling out variables in differentorders.
Despite its name, spurious ambiguity is abenefit to us, as it provides many different labelledbracketings for the same dependency graph of thesame sentence, thereby increasing the chance thatany substring of that sentence will have a syntacticlabel.
Our approach exploits this property of CCGto derive multiword CCG syntactic labels for targettranslation strings in a phrase table, thus providing afirmer basis on which to collect syntactic reorderingstatistics.
In particular:?
We show how CCG can derive constituent la-bels for target-side phrase-table entries thatare often lamented as ?non-constituents?
or as?crossing a phrase boundary?.?
Our CCG categories are not limited to single-word supertags.
Rather, as these labels aredrawn from CCG parse charts, they can spanmultiple words.
Further, the labels are tailoredspecifically to each translation constituent?sboundaries (Section 2.1).
As a consequence,?70% of phrase table entries receive a singlesyntactic label (Section 5), largely removingthe terminological inconsistency of calling lex-ical translation constituents ?phrases?.
Now,more of them actually are syntactic phrases.?
We use these labels to train a target-languagebidirectional reordering model over CCG syn-tactic sequences (Section 3), which, whenadded to the baseline system, is found to be su-perior to systems that use both lexicalized re-ordering models and supertag reordering mod-els (Section 5).With only minor modifications, we incorporate theseenhancements into a state-of-the-art PMT decoder(Koehn et al, 2007), achieving significant improve-ments over two competitive baselines in an Urdu-English translation task (Sections 5).
This languagepair was chosen to highlight the promise of this ap-proach for languages with considerable, but syntac-tically governed, word-order differences to one an-other.
Finally, in a small discussion we provide qual-itative evidence that the improvements in automaticmetric scores correspond to real gains in target lan-guage fluency.2 Syntax, Constituency and Phrase-basedMTConsider the following German-English PMTphrase pair that we have extracted from a parallelEuropean parliamentary transcript:1Ich hoffe, da?
?
I hope thatNeither word string is a well-formed constituent intraditional theories of syntax.
But tradition is at oddswith the intuition that that such ?non-constituent?sequences are still well-formed substrings, governedby rules of how they can be combined with otherword strings ?
e.g., declarative sentence translationrules like es mo?glich sein wird ?
it will be possiblecan grammatically extend each, but a noun phraserule cannot.As Figure 1 illustrates, putative non-constituentword sequences abound in phrase-based MT.
Here atranslation ?phrase?
is simply any contiguous wordstring that is consistent with a word alignment (arelation between source and target words), usuallyproduced by a language-independent alignment pro-cedure (Zens et al, 2002).
The figure also high-lights the need for linguistic syntax in controllinghow translations are assembled; the successful trans-lation is merely one among many possible reorder-ings, many of which (despite their ungrammatical-ity) might score well on a word n-gram model.
Butrather than changing the word alignments or PMT?phrase?
boundaries to fit a syntactic theory, wechoose to use a flexible syntax which can produce awider range of bracketings to accommodate the re-sults of alignment-derived translations.
To this end,we use Combinatory Categorial Grammar, or CCG,(Steedman, 2000).
To understand how CCG allowsthis, we illustrate its use with some simple examples.1Throughout this paper, the term ?PMT phrase?
refers to anunbroken sequence of words used by a PMT system, whereas?phrase?
(without context) refers to a syntactic constituent.211Wiederaufnahme der SitungsperiodeResumption of the sessionIch hoffe , da?
es mo?glich sein wirdI hope that it will be possibleIch hoffe, da?
das den Weg fu?r eine baldige Wiederaufnahme der Debatte ebnen wirdI hope that this will pave the way for an early resumption of the debateFigure 1: Two phrase-based MT word groups are extracted from aligned words (the dashed outlines) and then used to form a newtranslation (bottom).
[Adapted from parallel sentences in the Europarl German-English corpus, v6.
]2.1 CCG, Spurious Ambiguity and PMT:Turning ?Phrases?
into PhrasesCCG is a derivational syntax, where words are as-signed a lexical category2 and sentence structuresare then recursively built using a small set of de-ductive rule schemata known as combinators (Steed-man, 2000).
Lexical syntactic categories can berichly structured in CCG, indicating how words cancombine.
A syntactic category of the form X/Y,e.g., states that a category of type X can be formed ifcombined with a Y to its right ?
i.e., a function fromrightward Ys to X.
This can be accomplished withthe forward function application combinator (>),3which is written in derivational form as follows:4X/Y Y>XThis derivation of the symbol X is known as thenormal-form derivation (Steedman, 2000), since ituses function application whenever possible.
ButCCG has the ability to construct the same resultby using a different, non-normal-form sequence ofcombinatory inferences.
For example, by using thebackward type-raising combinator (T<) and thenbackward function application (<), we can arrive atthe same result:2When represented by a strings, lexical categories are calledsupertags.3CCG actually respects the rule-to-rule hypothesis (Bach,l976), where, for every syntactic term built, there is a corre-sponding semantic term, but, for simplicity of exposition, wefocus only on syntax here.4The reader will notice that CCG derivations are in facttrees, but that they ?grow?
in the direction opposite to how parsetrees are often depicted in NLP.X/Y YT<X\(X/Y)<XThis derivation shows how the argument Y to thefunctional type X/Y5 can ?raise?
its type to be-come a function that consumes that functional type,X\(X/Y), only to produce same result as before,namely X.
This property of CCG is often referredto as ?spurious ambiguity?, because there are manyways of reaching the same result as the canonical,normal-form derivation.Despite the name, this property is useful for ourpurposes.
Considering the target translation in Fig-ure 1, we then observe in Figure 2 how CCG canderive not only a bracketing similar to a more tra-ditional Penn Treebank-style parse, but also a non-normal-form variant that gives us a single categoryfor the English translation string I hope that ?namely the category S[dcl]/S[dcl] (a declarative sen-tence lacking a declarative sentence complement toits right).We use this fact about CCG to label a widerrange of PMT phrases with genuine syntactic con-stituent labels.
First we parse the English sen-tences in our training data with the C&C parser, astate-of-the-art, treebank-trained CCG parser (Clarkand Curran, 2007), producing normal-form CCGderivations.
We then enumerate all non-normal-form derivations that result in the same top-levelsymbol, packing all derivations (normal-form andnon-normal-form) into a parse chart (see Figure 4).5Also referred to as a functor.212SNPIVPVBPhopeSBARWNPWDTthatSit will...I hope that it will ...NP (S[dcl]\NP)/S[em] S[em]/S[dcl] S[dcl]>S[em]>S[dcl]\NP<S[dcl]I hope that it will ...NP (S[dcl]\NP)/S[em] S[em]/S[dcl] S[dcl]T>S[dcl]/(S[dcl]\NP)B>S[dcl]/S[em]B>S[dcl]/S[dcl]>S[dcl]Figure 2: Left: a traditional syntactic derivation; top right: a normal-form CCG derivation with the same subject+predicatebracketing; bottom right: one of many non-normal-form variants.
Combinator symbol key: >=forward function application,<=backward function application, T>=forward type-raising, B>=forward composition.
Note: the CCG dependencies that aredischarged in different orders are indicated by color-coding (if available in your medium) and underlining the appropriate categories(type-raising discharges no dependencies).
Both CCG derivations lead to the same symbol (S[dcl]), and dependencies.UR.-EN.SINGLE-LABEL COVERAGE 69%AVE.
EN.
PHRASE LEN.
2.8 wdsAVE.
CCG LABEL SPAN 2.3 wdsAVE.
CCG LABS/ENTRY 1.4Table 1: Training data statistics (top to bottom): (1) % of sin-gle CCG labels spanning entire English translation phrases, (2)average length of English translation phrase, (3) average CCGlabel span and (4) average CCG labels per English translationphrase.
(Maximum translation phrase length is 7 words.
)For the English string of each phrase table entry, weinspect the chart for the English-side sentence thatit came from and extract a list of labels as in Fig-ure 3.
For each span, this procedure either (lines5?9) finds the topmost single label, only using type-raised categories when no others exist,6 or (lines 10?19) recursively and greedily finds the longest span-ning labels from left to right, if no single label ex-ists.
The degenerate case is the single-word level(supertags).
In this way we find single labels for69% of the English-side phrase training instances.Table 1 gives more details.6Type-raisings are almost always possible, and will alwaysbe closer to the top-level symbol.
Many type-raisings, however,are superfluous ?
i.e., produce no novel bracketings.
Thereforewe only use type-raised symbols to derive a label for a span ofwords when necessary.GETLABELS(C,s)1 B C: a packed chart of derivations of E2 B s = (el, er): a span in target sentence E3 B RETURN: a list of labels covering all words4 B from E in span s5 if EXISTSSINGLESPANNINGLABEL(C,S)6 then B Get the topmost label7 B non-type-raised, if possible8 lb ?
GETTOPMOSTLABEL(C,s)9 return [ lb ]10 else B Get the longest label starting at el11 for i?
(er ?
1) to (el + 1)12 do lbs ?
GETLABELS(C,(el, i))13 if LENGTH(lbs)=114 then el?
?
i+ 115 lb ?
HEAD(lbs)16 BREAK17 else CONTINUE18 return19 CONS(lb,GETLABELS(C,(el?
, er)))Figure 3: Algorithm for labeling English sides of phrasetable instances.2130 I 1 hope 2 that 3 it 4 will 5 rain 60 Ich 1 hoffe 2 , 3 da?
4 es 5 regnen 6 wird 7NPS/(S\NP)S[dcl]/S[em]S[dcl]/S[dcl](S[dcl]\NP)/S[em]S[em]/S[dcl]NPS/(S\NP)S[em]/(S[dcl]\NP)S[dcl]/(S[b]\NP)S[dcl]S[em]S[dcl]\NPS[em]/(S[b]\NP)(S[dcl]\NP)/(S[b]\NP) S[b]\NP(S[dcl]\NP)/(S[b]\NP)S[dcl]\NPS[dcl]Figure 4: A packed CCG parse chart with multiple semantically equivalent derivations and two word-aligned strings.
(Not allderivations are depicted.
)3 Reordering Models: from Words toSupertags to ParsesIn phrase-based MT systems, the standard reorder-ing model that controls the order in which thesource string is translated is the lexicalized reorder-ing model (Tillmann, 2004; Axelrod et al, 2005).
Inits simplest form, a lexicalized reordering model es-timates, for each translation phrase pair (fi...j , ek...l)(where the indices sit ?in-between?
words, as in Fig-ure 4), the probability of p(O | fi...j , ek...l), whereO ?
{MONO, SWAP,DISCONTINUOUS} (abbrevi-ated M, S and D) is the orientation of the phrase pair(fi...j , ek...l) w.r.t.
the previously translated sourcephrase fu...v. If v = i, then O = M; if u = j, thenO = S; otherwise O = D. This model, known asa unidirectional MSD lexicalized reordering model,can also be enriched with statistics over orientationsto the next source phrase translated (i.e., it can bea bidirectional model), as well as with more fine-grained distinctions in the third class D (i.e., whetherit is DLEFT or DRIGHT).
All models in the presentwork are bidirectional MSD models.During decoding, orientations are predicted basedon previously translated (or following) phrases inthe decoder?s search state, but, when extracting ori-entation statistics, there are many different possi-ble phrasal segmentations of both strings.
A sim-ple solution, known as word-based extraction, is tolook for neighboring alignment points that supportthe various orientations.
In Figure 4, e.g., a word-based extraction regime would count the phrasehoffe ?
hope as being in orientation D w.r.t.
towhat follows, because its rightmost index, 2, is dis-contiguous with the next aligned source point, (3,4).Another approach, known as phrase-based extrac-tion aims to remedy this situation by conditioningthe extraction of orientations on translation phrasesconsistent with the alignment.
In Figure 4 there is atranslation phrase that follows the phrase in question?
viz., , da?
?
that ?
and an orientation of Mis therefore tallied.Regardless of the method of extraction, lexi-calized reordering model statistics rely on exactword-string pairs, (f, e), which can lead problemswith data sparsity.
Moreover, even given ampledata, cross-phrasal reordering generalizations willbe missed.
E.g., the fact that regnen ?
rain hasorientation S w.r.t.
the previous phrase pair does notsupport the fact that other infinitival German verbsshould also behave similarly in relative clausal envi-ronments.To remedy this we might substitute abstract sym-bols for each word in e, and train a syntactic bidirec-tional MSD reordering model.
For this we use CCGsupertags (cf.
the single-word labels in the parse214chart in Figure 4), which are richly structured partsof speech that describe their potential to combinewith other words (cf.
Section 2.1).
Given the samephrase from Figure 4, we can estimate the proba-bility of orientation S, given regnen ?
S[b]\NP .A further level of abstraction is to use CCG parsecharts packed with all derivations.
The phraseda?
es ?
that it can therefore be abstracted toda?
es ?
S[em]/(S[dcl]\NP) (a ?that?
clauselacking a verb phrase to the right).Except in cases of high ambiguity, the sourcephrase effectively encodes the target phrase, mean-ing that these extensions will suffer from data spar-sity similarly to the baseline lexicalized model.
Wetherefore omit the source phrase in our syntacticreordering models, estimating probability distribu-tions p(O|LAB(e)) where LAB(e) is the syntactic la-bel sequence derived from the chart (or supertaggedstring, as the case may be) using the algorithm inFigure 3.7 Orientations are determined using thephrase-based extraction regime described in (Till-mann, 2004), but statistics are tallied only for thesyntactic label sequence of the target string.
Moreprecisely, for phrase pair (fi...j , ek...l), if a phrase(fa...i, eb...k) exists in the alignment grid, an orien-tation of M is assigned to LAB(ek...l) .
Otherwise,if a phrase (fj...p, el...m) exists in the alignment grid,an orientation of S is assigned.
In all other cases, anorientation of D is assigned.Using these statistics, we deploy target-side re-ordering models, as described below.4 Related WorkAs noted, lexicalized reordering models can betrained and configured in many different ways.
Inaddition to the standard word-based extraction (Ax-elrod et al, 2005) and phrase-based extraction (Till-mann, 2004) cases, more recent work has exploredusing dynamic programming to extract and laterscore orientations based on hierarchical configura-tions of phrases consistent with an alignment (Gal-ley and Manning, 2008).
This means that the re-ordering model can be conditioned on an unboundedamount of context and can capture the fact that7Note that a tagged string can be viewed as a very impover-ished parse chart, and so the algorithm defined in Figure 3 canbe applied to the supertagging case as well.many translations are monotonic w.r.t.
the previ-ously translated block, but are mistakenly identifiedas having orientation S or D.Su and colleagues (2010) observe that the spaceof phrase pairs consistent with an alignment canbe viewed in its entirety, as a graph of phrases,thereby collecting reordering statistics w.r.t.
the en-tire space of surrounding phrases.
Ling and col-leagues (2011) extend this approach by weightingorientation counts with multiple scored alignments.All of these more sophisticated reordering extrac-tion approaches are compatible with the current ap-proach, and could be straightforwardly applied toour labelled target-side word strings.Syntax-driven reordering approaches in phrase-based MT abound, but, perhaps due to the incom-patibility of phrase table entries and traditional syn-tactic constituency, most research has avoided usingrecursive target-side syntax during decoding.
Till-mann (2008) presents an algorithm that reorders us-ing part-of-speech based permutation patterns dur-ing the decoding process.
Others have side-steppedthe issue by restructuring the source language be-fore decoding to resemble the target language usingsyntactic rules, either automatically extracted (Xiaand McCord, 2004), or hand-crafted (Collins et al,2005; Wang et al, 2007; Xu and Seneff, 2008).The flexibility of CCG syntax is also gainingrecognition as a useful tool for constraining statis-tical MT decoders.
Hassan (2009) describes an in-cremental CCG parsing language model, althoughhis model does not beat a supertag factored PMTapproach.
Almaghout and colleagues (2010) alsouse a CCG chart to improve translation, augment-ing SCFG rules by consulting the multiple deriva-tions in the parse chart of Clark and Curran?s (2007)CCG parser.
We note two key differences to ouruse of spurious ambiguity.
First, they use a chartpacked with multiple dependency analyses, unlikeour spuriously ambiguous reworkings of the parser?ssingle-best analysis.
Second, the C&C parser re-strains type-raising to a small number of possi-bilities, thereby blocking many non-normal-formderivations that we do not.Two SCFG approaches that employ catego-rial syntax that resembles CCG are the syntax-augmented MT (SAMT) system described in (Venu-gopal et al, 2007), and the target dependency lan-215guage model of of (Shen et al, 2008).
(Venu-gopal et al, 2007) uses a Penn Treebank-trainedCFG parser to label target strings and then re-works the CFG parse trees, if needed,x to ac-count for non-traditional constituents.
This on-demand reworking process, however, is bounded bytree depth, and sometimes produces conjoined cat-egories, rather than consistently produce the func-tional ?slash?
categories that a full CCG would ?e.g., a subject + transitive verb string might some-times be labelled NP+ V and other times S/NP .The approach in (Shen et al, 2010) uses a simplecategorial grammar with only a single atomic sym-bol ?
i.e., every functional category has the formC\X or C/X, where X is either C or another slashcategory C\X or C/X.
In contrast to these two ap-proaches, the CCG parser we use is trained on aCCG treebank that is the result of a carefully engi-neered Penn Treebank-to-CCG conversion (Hocken-maier and Steedman, 2007) and we impose no limitson deriving categorial functional categories (X/Y).We view our reworking of CCG charts as a poten-tially useful extension to such approaches.5 Experimental ResultsWe empirically validate our technique by translat-ing from Urdu into English.
Urdu has a canoni-cal word order of SOV ?
subject, object(s), verb?
whereas English has SVO, leading to indefinitelylong distances between corresponding verbs and ob-jects.
This language pair is therefore a strong testcase for a reordering model.For decoding we use Moses (Koehn et al, 2007),a state-of-the-art PMT decoder, with IRST LM (Fed-erico and Cettolo, 2007) for language model infer-ence.
For Urdu-English parallel data, we use theOpenMT 2008 training set which consists of 88thousand sentence-level translations and a transla-tion dictionary of ?114 thousand word and phrasetranslations.
We use half of the OpenMT 2008 Urdu-English evaluation data for development and per-form development testing on the other half.
Bothhalves are ?900 sentences long and were balancedto contain approximately the same number of to-kens.
Our blind test set is the entire OpenMT 2009Urdu-English evaluation set.
All evaluation sets had4 reference translations for each tuning or testing in-stance.
All system component weights were tunedusing minimum error-rate training (Och, 2003), withthree tuning runs for each condition.
The data wasnormalized, tokenized and the English sentenceswere lowercased,8As a baseline, we train a standard phrase-basedsystem with a bidirectional MSD lexicalized re-ordering model using word-based extraction.
OurCCG-augmented reordering system has all of themodel components of the baseline, as well as a bidi-rectional orientation reordering model over target-side multiword syntactic labels.
To directly test theeffect of using CCG parse charts ?
as opposed tosimply using a CCG supertagger ?
we also added aCCG supertag bidirectional MSD reordering modelto the baseline set-up.
All systems were tuned andtested with distortion limit of 15 words, and testruns were performed with and without 200-best min-imum Bayes?
risk (MBR) hypothesis selection (Ku-mar and Byrne, 2004).To acquire CCG labels for our English paralleldata, we use the C&C CCG toolkit of Clark andCurran (2007).
We build CCG parse charts by re-working the normal-form derivations from the C&Cparser in all spuriously ambiguous ways, as de-scribed in Section 2.1.
For supertags, we tag withthe C&C supertagger.
Rather than training sepa-rate phrase tables for our CCG systems, however,we instead decorate the baseline phrase tables withCCG multiword labels or supertags.
To smooth overparsing and tagging errors, we only use those la-bels whose relative frequency (rf) is sufficiently highw.r.t.
the most frequent label for that phrase pairLAB*[f?e].
More precisely, for each phrase pair, weuse the set of labels:9{LAB[f?e]|rf(LAB[f?e]) ?
?
?
rf(LAB*[f?e])}This is reminiscent of the ?-best tagging approachof (Clark and Curran, 2004), but performed in abatch process when creating the syntactic phrase ta-bles (both supertag and CCG chart-derived).
We set8N.B.
We use Penn Treebank III-compatible tokenization forEnglish and a specially designed tokenization script for Urdu,cf.
(Baker et al, 2010), Appendix C9Recalling that ?31% of the time, a phrase pair might havea list of labels, rather than a single label, the word ?label?
hererefers to a single token that can be the concatenation of multiplesymbols.216DEVTEST (NIST-08) (MBR/NON-MBR) NIST-09 TEST (MBR/NON-MBR)BLEU-4 METEOR TER LENGTH BLEU-4 METEOR TER LENGTHLR 25.3/24.7 28.3/28.2 64.2/64.4 98.2/97.6 29.1/28.8 30.0/28.8 60.0/60.1 98.2/97.8NO-LR 22.5/22.1 27.5/27.3 66.3/66.3 97.6/97.1 26.2/25.8 29.2/29.1 61.9/62.0 97.1/96.6ST+LR 24.5/24.2 28.4/28.3 64.6/64.5 97.9/97.3 28.5/28.2 30.0/30.0 60.3/60.2 97.9/97.3CCG+LR 25.6/25.2 28.7/28.5 64.3/64.5 98.7/98.1 29.1/29.2 30.1/30.2 59.5/59.8 97.4/97.9Table 2: Case-insensitive BLEU-4, METEOR, TER and hypothesis/reference length ratio (LENGTH) for a lexicalized reorderingbaseline (LR), a system with only a distance-based distortion model (NO-LR), a system with an additional CCG supertag reorderingmodel (ST+LR) and our system with an additional CCG chart-derived reordering model (CCG+LR).
Systems were run with (leftof slash) and without (right of slash) 200-best-list MBR hypothesis selection.
All boldfaced results were found to be significantlybetter than the baseline at ?
the 95% confidence level using method described in (Clark et al, 2011) with 3 separate MERT tuningruns for each system.
Non-boldfaced numbers are statistically indistinguishable from (or worse than) the baseline.?
= 0.5 in all of our CCG experiments.To minimize disruption to the Moses decoder(which only supports single-word labels in phrase-based mode), we project multiword labels across thewords they label as single-word factors with book-keeping characters, similar to the ?microtag?
anno-tations of asynchronous factored translation mod-els (Cettolo et al, 2008).
We modified to the de-coder to reassemble the multiple single-word fac-tors into a single label before querying the reorder-ing model.
As an example, we might have the phrasepair le ve?lo rouge ?
the|NP( red|NP+ bike|NP) .Before querying the reordering model, the fac-tor sequence NP( NP+ NP) is collapsed into thesingle, multiword label ?NP?
by the rule schemaX( .
.
.
X+ .
.
.
X) ?
X.We train a language model using all of the WMT2011 NEWSCRAWL, NEWSCOMENTARY and EU-ROPARL monolingual data,10 tokenized and lower-cased as above, but de-duplicated to address the re-dundancy of the Web-crawled portion of that dataset.
We also train a separate language model on theEnglish portion of the Urdu-English parallel corpus(minus the dictionary entries), and interpolate thetwo models by optimizing perplexity on our tuningset.Table 2 lists our results, where we see significantimprovement over both of our baselines, lexicalizedreordering (LR) and supertag reordering plus lexi-calized reordering (ST+LR).
To test the effects ofthe lexicalized reordering model itself, we also eval-uate a system with no lexicalized reordering model10http://www.statmt.org/wmt11/translation-task.html(only a distance-based distortion model).
This lastsystem (a system which almost always prefers notto reorder) is considerably worse than all other sys-tems, demonstrating the need for non-monotonicreordering configurations when accounting for theUrdu-English data.6 Analysis and DiscussionOur CCG system (CCG+LR) outperforms bothbaseline systems (LR and ST+LR) in a majority ofmetrics in both MBR and non-MBR conditions.
Wesee that, even though MBR decoding closes the per-formance gap somewhat, our system continues tomatch or outperform (if sometimes insignificantly)in all areas.
Note that the CCG+LR non-MBRconfiguration outperforms both LR and ST+LR inMBR and non-MBR decoding conditions in its ME-TEOR score on the NIST-09 test set.
We note alsothat, in the NIST-09 test case, the CCG+LR sys-tem?s poorer performance is perhaps due to a mis-match in hypothesis length, which could be harmingits scores, particularly the BLEU brevity penalty.6.1 Poor Performance of CCG Supertag ModelWe have no firm explanation for the poor per-formance of the CCG supertag model (ST-LR), but it is important to note that the su-pertag reordering model does not unify statis-tics across phrases of different lengths, as theCCG chart-derived model does.
E.g., thephrase pair den Weg fu?r eine ?
the way for anwill query the CCG chart-derived reorderingmodel with the same symbol as the phrase pairden Weg fu?r eine baldige ?
the way for an early217twenty-seven year old abdullah britain blasts in hatcheries planning accused of is .CCG+LR: twenty-seven year old abdullah is accused of planning hatcheries blasts in britain .LR: twenty-seven years on charges of planning bombings hatcheries , abdullah in britain .Reference 1: 27 years old abdullah is accused of planning explosions in britain .Reference 2: twenty-seven years old abdullah is blamed for planning attacks in britain .Reference 3: abdullah , 27 , has been blamed for planning the blasts in britain .Reference 4: abdullah , 27 , has been blamed for planning the blasts in britain .now musharraf resignation give should .CCG+LR: now musharraf should give resignation .LR: now musharraf resignation should be given .Reference 1: now musharraf should resign .Reference 2: now , musharraf should resign .Reference 3: now , musharraf should resign .Reference 4: musharraf should resign .Figure 5: Sample devtest (NIST-08) translations of the median-performing tuned CCG syntactic reordering model(CCG+LR) compared to the median-performing baseline lexicalized reordering model (LR).?
viz., NP/N.
The CCG supertag model, how-ever, will have two distinct label sequences for thesephrases ?
viz., NP/N N (NP\NP)/NP NP/N andNP/N N (NP\NP)/NP NP/N N/N, resp.
?
bothof which could be reduced to the single label, NP/N,using CCG?s syntactic combinators.
The supertagsystem does not have the means of relating thereordering patterns of strings of symbols such asthis.11 Such data fragmentation may be leading todecreased performance, which would indicate theuse of recursive CCG syntax.6.2 Qualitative ImprovementsIn addition to improved metric scores, we noted realqualitative improvements in some examples, as Fig-ure 5 shows.
These examples demonstrate the abil-ity of the reordering model to navigate the massive,structure-governed reorderings needed to approxi-mate the correct answer with the phrase inventoryit is given.11Its reordering table has more than twice as many entries asthat of the chart-derived model.6.3 Comparison to the State of the ArtTo our knowledge, the state of the art in Urdu-English translation using the OpenMT data islisted in the NIST OpenMT 2009 evaluation re-sults (http://www.itl.nist.gov/iad/mig/tests/mt/2009/ResultsRelease/currentUrdu.html).
This evaluation acceptedonly single system outputs, and used cased refer-ences.
Therefore we had to choose a single systemoutput and recase its text.For system selection, we picked the tuned sys-tem that performed best on the development testset.
For recasing, we trained a lowercased-to-casedmonolingual phrase-based ?translation model?
withno reordering and a cased language model, similar towhat is described in (Baker et al, 2010).
The train-ing text is simply the non-dictionary portion of theUrdu-English parallel corpus, with its lowercasedversion as the source and the original cased text asthe target, both halves tokenized as above.
We tunedon a similar version of the English half of our tuning218references.
The lowercased output of our system isfed to this model and the first token of each casing?translation?
is capitalized (if not already).The official metric of the NIST 2009 evaluationis BLEU (as implemented in the NIST-distributedmteval-v13a.pl script).12 The best-performingsystem in the constrained data evaluation scored0.312 w.r.t.
the cased references, with the secondand third place systems scoring 0.2395 and 0.2322,respectively.13 Our best performing MERT-tunedsystem (as determined on the devtest data) scores0.2734 on the test set, putting it between the top twosystems.
For comparison, our devtest-best baselineLR system scores 0.2683 on the test set.While is generally not useful to test experimentalmanipulations based on a single tuning run (Clark etal., 2011) and with different monolingual languagemodelling data, we note these figures simply to situ-ate our results within the state of the art.7 ConclusionWe have argued for the use of CCG in phrase-based translation, due to its flexibility in providinga wealth of different bracketings that better accom-modate lexical translation strings.
We have also pre-sented a novel method for using CCG constituent la-bels in a syntactic reordering model where the syn-tactic labels span multiple words, do not cross trans-lation constituent boundaries and are tailored specif-ically to each translation constituent.
The result is asignificant improvement in Urdu-English (SOV ?SVO) translation scores over two baselines: a tra-ditional phrase-based baseline with a lexicalized re-ordering model and a phrase-based baseline with anadditional supertag reordering model.
Moreover, wehave provided qualitative examples that confirm theimprovements in automatic metrics.In future work we would like explore whetherfurther improvements can be gained by using moresophisticated reordering models, such as reorderinggraphs (Su et al, 2010) and hierarchical reorderingmodels (Galley and Manning, 2008) both for ourword-based and syntactic reordering models.
Fur-ther, as in prior work (Zollmann et al, 2006; Shen12ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13a-20091001.tar.gz.13We exclude combination entries that are combinations ofmultiple systems with different algorithmic approaches.et al, 2010; Almaghout et al, 2010), our categoriallabels could also be used to derive CCG-augmentedSCFG rules, both lexicalized and unlexicalized, cf.
(Zhao and Al-onaizan, 2008) ?
the latter being theSCFG analogue of our current model.AcknowledgmentsThe authors would like to thank Chong Min Lee,Aoife Cahill and Nitin Madnani at ETS for takingthe time to read earlier drafts of this (and closely re-lated) work.
Their comments and suggestions madethis a better paper.
We would also like to thankthe anonymous reviewers for their very helpful feed-back.
The views expressed in this paper do not nec-essarily reflect those of The Ohio State University orof Educational Testing Service.ReferencesHala Almaghout, Jie Jiang, and Andy Way.
2010.CCG Augmented Hierarchical Phrase-based MachineTranslation.
In Proceedings of the 7th InternationalWorkshop on Spoken Language Translation, Paris,France.Amittai Axelrod, Ra Birch Mayne, Chris Callison-Burch,Miles Osborne, and David Talbot.
2005.
Edin-burgh System Description for the 2005 IWSLT SpeechTranslation Evaluation.
In Proceedings of the Inter-national Workshop on Spoken Language Translation(IWSLT-05), Pittsburgh, PA, USA.Emmon Bach.
l976.
An Extension of Classical Transfor-mational Grammar.
In Proceedings of the 1976 Con-ference on Problems of Linguistic Metatheory, pages183?224, East Lansing, MI, USA.Kathy Baker, Steven Bethard, Michael Bloodgood, RalfBrown, Chris Callison-Burch, Glen Coppersmith,Bonnie Dorr, Wes Filardo, Kendall Giles, Anni Irvine,Mike Kayser, Lori Levin, Justin Martineau, JimMayeld, Scott Miller, Aaron Phillips, Andrew Philpot,Christine Piatko, Lane Schwartz, and David Zajic.2010.
Semantically informed machine translation.Technical Report 002, Johns Hopkins University, Bal-timore, MD, Human Language Technology Center ofExcellence.Mauro Cettolo, Marcello Federico, Daniele Pighin, andNicola Bertoldi.
2008.
Shallow-syntax Phrase-basedTranslation: Joint versus Factored String-to-chunkModels.
In Proceedings of AMTA 2008, Honolulu, HI,USA.Stephen Clark and James R. Curran.
2004.
The Impor-tance of Supertagging for Wide-Coverage CCG Pars-ing.
In Proceedings of the 20th International Con-219ference on Computational Linguistics (COLING-04),Geneva, Switzerland.Stephen Clark and James R. Curran.
2007.
Wide-Coverage Efficient Statistical Parsing with CCG andLog-Linear Models.
Computational Linguistics,33(4):493?552.Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.Smith.
2011.
Better Hypothesis Testing for Ma-chine Translation: Controlling for Optimizer Insta-bility.
In Proceedings of the Meeting of the Associ-ation for Computational Linguistics (ACL-11), Port-land, OR, USA.Michael Collins, Philipp Koehn, and Ivona Kucerova.2005.
Clause Restructuring for Statistical MachineTranslation.
In Proceedings of the Association forComputational Linguistics (ACL-05), Ann Arbor, MI,USA.Marcello Federico and Mauro Cettolo.
2007.
EfficientHandling of n-gram Language Models for StatisticalMachine Translation.
In Proceedings of Associationfor Computational Linguistics, Prague, The Czech Re-public.Michel Galley and Christopher D. Manning.
2008.
ASimple and Effective Hierarchical Phrase ReorderingModel.
In Proceedings of EMNLP-08.Hany Hassan.
2009.
Lexical Syntax for Statistical Ma-chine Translation.
Ph.D. thesis, Dublin City Univer-sity, Dublin, Ireland.Julia Hockenmaier and Mark Steedman.
2007.
CCG-bank: A Corpus of CCG Derivations and DependencyStructures Extracted from the Penn Treebank.
Com-putational Linguistics, 33(3):355?396.Philipp Koehn, Franz Joseph Och, and Daniel Marcu.2003.
Statistical Phrase-based Translation.
In Pro-ceedings of NAACL-HLT, pages 48?54, Edmonton, Al-berta, CA.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open SourceToolkit for Statistical Machine Translation.
In Pro-ceedings of the Association for Computational Lin-guistics, Companion Volume Proceedings of the Demoand Poster Sessions, Prague, Czech Republic, June.Shankar Kumar and William Byrne.
2004.
MinimumBayes-Risk Decoding for Statistical Machine Transla-tion.
In Proceedings of HLT-NAACL.Wang Ling, Jo ao Grac?a, David Martins de Matos, Is-abel Trancoso, and Alan Black.
2011.
Discrimi-native Phrase-based Lexicalized Reordering Modelsusing Weighted Reordering Graphs.
In Proceedingsof the 5th International Joint Conference on NaturalLanguage Processing.Franz Joseph Och.
2003.
Minimum Error Rate Train-ing in Statistical Machine Translation.
In Proceed-ings of the 41st Annual Meeting of the Association forComputational Linguistics, pages 160?167, Sapporo,Japan.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.
ANew String-to-dependency Machine Translation Algo-rithm with a Target Dependency Language Model.
InProceedings of the Joint Meeting of the Associationfor Computational Linguistics and Human LanguageTechnologies (ACL-08:HLT), Columbus, OH, USA.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2010.String-to-Dependency Statistical Machine Translation.Computational Linguistics, 36(4):649?671.Mark Steedman.
2000.
The Syntactic Process.
MITPress, Cambridge, MA, USA.Jinsong Su, Yang Liu, Yajuan Lu?, Haitao Mi, and QunLiu.
2010.
Learning Lexicalized Reordering Modelsfrom Reordering Graphs.
In Proceedings of the ACL2010; Short Papers.Christoph Tillmann.
2004.
A Unigram OrientationModel for Statistical Machine Translation.
In Pro-ceedings of HLT-NAACL 2004: Short Papers, HLT-NAACL-Short ?04.Christoph Tillmann.
2008.
A Rule-Driven Dynamic Pro-gramming Decoder for Statistical MT.
In Proceedingsof the Second Workshop on Syntax and Structure inStatistical Translation (SSST-08).Ashish Venugopal, Andreas Zollmann, and Stephan Vo-gel.
2007.
An Efficient Two-pass Approach toSynchronous-CFG Driven Statistical MT.
In Proceed-ings of the Human Language Technology and NorthAmerican Association for Computational LinguisticsConference (HLT/NAACL-07), Rochester, NY.Chao Wang, Michael Collins, and Philipp Koehn.
2007.Chinese Syntactic Reordering for Statistical MachineTranslation.
In Proceedings of EMNLP/CoNLL-07,Prague, The Czech Republic.Fei Xia and Michael McCord.
2004.
Improving a Statis-tical MT System with Automatically Learned RewritePatterns.
In Proceedings of International Conferenceon Computational Linguistics (COLING-04), Geneva,Switzerland.Yushi Xu and Stephanie Seneff.
2008.
Two-stage Trans-lation: A Combined Linguistic and Statistical MachineTranslation Framework.
In Proceedings of the 8thConference of the Association for Machine Transla-tion in the Americas (AMTA-08), Waikiki, Honolulu,HI, USA.Richard Zens, Franz Josef Och, and Hermann Ney.2002.
Phrase-Based Statistical Machine Translation.In M. Jarke, J. Koehler, and G. Lakemeyer, editors, KI-2002: Advances in Artificial Intelligence, Proceedingsof the 25th Annual German Conference on AI, (KI-2002), pages 18?32.
Springer Verlag, Aachen, Ger-many.220Bing Zhao and Yaser Al-onaizan.
2008.
GeneralizingLocal and Non-Local Word-Reordering Patterns forSyntax-Based Machine Translation.
In Proceedingsof The Conference on Empirical Methods in NaturalLanguage Processig (EMNLP-08).Andreas Zollmann, Ashish Venugopal, Stephan Vogel,and Alex Waibel.
2006.
The CMU-UKA Syntax Aug-mented Machine Translation System for IWSLT-06.In Proceedings of International Workshop on SpokenLanguage Translation (IWSLT-06), Kyoto, Japan.221
