Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 513?520Manchester, August 2008Authorship Attribution and Verification with Many Authors and LimitedDataKim Luyckx and Walter DaelemansCNTS Language Technology GroupUniversity of AntwerpPrinsstraat 13, 2000 Antwerp, Belgium{kim.luyckx,walter.daelemans}@ua.ac.beAbstractMost studies in statistical or machinelearning based authorship attribution focuson two or a few authors.
This leads toan overestimation of the importance of thefeatures extracted from the training dataand found to be discriminating for thesesmall sets of authors.
Most studies alsouse sizes of training data that are unreal-istic for situations in which stylometry isapplied (e.g., forensics), and thereby over-estimate the accuracy of their approach inthese situations.
A more realistic interpre-tation of the task is as an authorship ver-ification problem that we approximate bypooling data from many different authorsas negative examples.
In this paper, weshow, on the basis of a new corpus with145 authors, what the effect is of manyauthors on feature selection and learning,and show robustness of a memory-basedlearning approach in doing authorship at-tribution and verification with many au-thors and limited training data when com-pared to eager learning methods such asSVMs and maximum entropy learning.1 IntroductionIn traditional studies on authorship attribution, thefocus is on small sets of authors.
Trying to classifyan unseen text as being written by one of two orof a few authors is a relatively simple task, whichc?Kim Luyckx & Walter Daelemans, 2008.
Li-censed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license(http://creativecommons.org/licenses/by-nc-sa/3.0/).
Somerights reserved.in most cases can be solved with high reliabil-ity and accuracies over 95%.
An early statisticalstudy by Mosteller and Wallace (1964) adopteddistributions of function words as a discriminat-ing feature to settle the disputed authorship of theFederalist Papers between three candidate authors(Alexander Hamilton, James Madison, and JohnJay).
The advantage of distributions of functionwords and syntactic features is that they are notunder the author?s conscious control, and there-fore provide good clues for authorship (Holmes,1994).
Frequencies of rewrite rules (Baayen etal., 1996), n-grams of syntactic labels from par-tial parsing (Hirst and Feiguina, 2007), n-grams ofparts-of-speech (Diederich et al, 2000), functionwords (Miranda Garc?
?a and Calle Mart?
?n, 2007),and functional lexical features (Argamon et al,2007) have all been claimed to be reliable markersof style.
There is of course a difference betweenclaims about types of features and claims about in-dividual features of that type.
E.g., it may be cor-rect to claim that distributions of function wordsare important markers of author identity, but thedistribution of a particular function word, whileuseful to distinguish between one particular pairof authors, may be irrelevant when comparing an-other pair of authors.The field of authorship attribution is howeverdominated by studies potentially overestimatingthe importance of these specific predictive featuresin experiments discriminating between only two ora few authors.
Taking into account a larger set ofauthors allows the computation of the degree ofvariability encountered in text on a single topic ofdifferent (types of) features.
Recently, research hasstarted to focus on authorship attribution on largersets of authors: 8 (Van Halteren, 2005), 20 (Arga-mon et al, 2003), 114 (Madigan et al, 2005), or up513to thousands of authors (Koppel et al, 2006) (seeSection 5).A second problem in traditional studies are theunrealistic sizes of training data, which also makesthe task considerably easier.
Researchers tend touse over 10,000 words per author (Argamon et al,2007; Burrows, 2007; Gamon, 2004; Hirst andFeiguina, 2007; Madigan et al, 2005; Stamatatos,2007), which is regarded to be ?a reliable mini-mum for an authorial set?
(Burrows, 2007).
Whenno long texts are available, for example in po-ems (Coyotl-Morales et al, 2006) or student es-says (Van Halteren, 2005), a large number of shorttexts is selected for training for each author.
One ofthe few studies focusing on small texts is Feiguinaand Hirst (2007), but they select hundreds of theseshort texts (here 100, 200 or 500 words).
The ac-curacy of any of these studies with unrealistic sizesof training data is overestimated when comparedto realistic situations.
When only limited data isavailable for a specific author, the author attribu-tion task becomes much more difficult.
In foren-sics, where often only one small text per candidateauthor is available, traditional approaches are lessreliable than expected from reported results.In this paper, we present a more realistic inter-pretation of the authorship attribution task, viz.
asa problem of authorship verification.
This is amuch more natural task, since the group of poten-tial authors for a document is essentially unknown.Forensic experts not only want to identify the au-thor given a small set of suspects, they also wantto make sure the author is not someone else notunder investigation.
They often deal with short e-mails or letters and have only limited data avail-able.
The central question in authorship verifica-tion is Did candidate author x write the document?Of the three basic approaches to authorship veri-fication - also including a one-class learning ap-proach (Koppel et al, 2007) - we selected a one vs.all approach.
This approach is similar to the oneinvestigated by Argamon et al (2003), which al-lows for a better comparison of results.
With onlyfew positive instances and a large number of neg-ative instances to learn from, we are dealing withhighly skewed class distributions.We show, on the basis of a new corpus with 145authors, what the effect is of many authors on fea-ture selection and learning, and show robustnessof a memory-based learning approach in doing au-thorship attribution and verification with many au-thors and limited training data when compared toeager learning methods such as SVMs and max-imum entropy learning.
As far as feature selec-tion is concerned, we find that similar types of fea-tures tend to work well for small and large sets ofauthors, but that no generalisations can be madeabout individual features.
Classification accuracyis clearly overestimated in authorship attributionwith few authors.
Experiments in authorship veri-fication with a one vs. all approach reveal that ma-chine learning methods are able to correctly clas-sify up to 56% of the positive instances in test data.For our experiments, we use the Personae cor-pus, a collection of student essays by 145 authors(see Section 2).
Most studies in stylometry focuson English, whereas our focus is on Dutch writtenlanguage.
Nevertheless, the techniques used aretransferable to other languages.2 CorpusThe 200,000-word Personae corpus1used in thisstudy consists of 145 student (BA level) essays ofabout 1400 words about a documentary on Artifi-cial Life, thereby keeping markers of genre, regis-ter, topic, age, and education level relatively con-stant.
These essays contain a factual description ofthe documentary and the students?
opinion aboutit.
The task was voluntary and students produc-ing an essay were rewarded with two cinema tick-ets.
The students also took an online Myers-BriggsType Indicator (MBTI) (Briggs Myers and Myers,1980) test and submitted their profile, the text andsome user information via a website.
All studentsreleased the copyright of their text and explicitlyallowed the use of their text and associated per-sonality profile for research, which makes it pos-sible to distribute the corpus.
The corpus cannotonly be used for authorship attribution and verifi-cation experiments, but also for personality predic-tion.
More information about the motivation be-hind the corpus and results from exploratory ex-periments in personality prediction can be foundin Luyckx & Daelemans (2008).3 MethodologyWe approach authorship attribution and verifica-tion as automatic text categorization tasks that la-bel documents according to a set of predefined cat-egories (Sebastiani, 2002, 3).
Like in most text cat-1The Personae corpus can be downloaded fromhttp://www.cnts.ua.ac.be/?kim/Personae.html514egorization systems, we take a two-step approachin which our system (i) achieves automatic selec-tion of features that have high predictive value forthe categories to be learned (see Section 3.1), and(ii) uses machine learning algorithms to learn tocategorize new documents by using the features se-lected in the first step (see Section 3.2).3.1 Feature ExtractionSyntactic features have been proposed as more re-liable style markers than for example token-levelfeatures since they are not under the consciouscontrol of the author (Baayen et al, 1996; Arga-mon et al, 2007).
To allow the selection of lin-guistic features rather than (n-grams of) terms, ro-bust and accurate text analysis tools such as lem-matizers, part of speech taggers, chunkers etc.,are needed.
We use the Memory-Based ShallowParser (MBSP) (Daelemans and van den Bosch,2005), which gives an incomplete parse of theinput text, to extract reliable syntactic features.MBSP tokenizes the input, performs a part-of-speech analysis, looks for noun phrase, verb phraseand other phrase chunks and detects subject andobject of the sentence and a number of other gram-matical relations.Word or part-of-speech (n-grams) occurringmore often than expected with either of the cate-gories are extracted automatically for every docu-ment.
We use the ?2metric (see Figure 1), whichcalculates the expected and observed frequency forevery item in every category, to identify featuresthat are able to discriminate between the categoriesunder investigation.?2=k?i=1(?i?
?i)2?iFigure 1: Chi-square formulaDistributions of n-grams of lexical features (lex)are represented numerically in the feature vectors,as well as of n-grams of both fine-grained (pos)and coarse-grained parts-of-speech (cgp).
Themost predictive function words are present in thefwd feature set.
For all of these features, the ?2value is calculated.An implementation of the Flesch-Kincaid met-ric indicating the readability of a text, along withits components (viz., mean word and sentencelength) and the type-token ratio (which indicatesvocabulary richness) are also represented (tok).3.2 Experimental Set-UpThis paper focuses on three topics, each with theirown experimental set-up:(a) the effect of many authors on feature selectionand learning;(b) the effect of limited data in authorship attri-bution;(c) the results of authorship verification usingmany authors and limited data on learning.For (a), we perform experiments in authorshipattribution while gradually increasing the numberof authors.
First, we select a hundred random sam-ples of 2, 5 and 10 authors in order to minimize theeffect of chance, then select one random sample of20, 50, 100 authors and finally experiment with all145 authors (Section 5.1).We investigate (b) by performing authorship at-tribution on 2 and 145 authors while gradually in-creasing the amount of training data, keeping testset size constant at 20% of the entire corpus.
Theresulting learning curve will be used to compareperformance of eager and lazy learners (see Sec-tion 5.1).The authorship verification task (c) - which iscloser to a realistic situation in e.g.
forensics -using limited data and many authors is approxi-mated as a skewed binary classification task (onevs.
all).
For each of the 145 authors, we have 80%of the text in training and 20% in test.
The neg-ative class contains 80% of each of the other 144author?s training data in training and 20% in test(see Section 5.2).All experiments for (a), (b) and (c) are per-formed using 5-fold cross-validation.
This allowsus to get a reliable indication of how well thelearner will do when it is asked to make new pre-dictions on the held-out test set.
The data set is di-vided into five subsets containing two fragments ofequal size per author.
Five times one of the subsetsis used as test set and the other subsets as trainingset.The feature vectors that are fed into the ma-chine learning algorithm contain the top-n features(n=50) with highest ?2value.
Every text fragmentis split into ten equal parts, each part being repre-sented by means of a feature vector, resulting in1450 vectors per fold (divided over training andtest).515For classification, we experimented with bothlazy and eager supervised learning methods.As an implementation of the lazy learning ap-proach we used TiMBL (Tilburg Memory-BasedLearner) (Daelemans et al, 2007), a supervised in-ductive algorithm for learning classification tasksbased on the k-nn algorithm with various exten-sions for dealing with nominal features and fea-ture relevance weighting.
Memory-based learningstores feature representations of training instancesin memory without abstraction and classifies newinstances by matching their feature representationto all instances in memory.
From these ?nearestneighbors?, the class of the test item is extrapo-lated.As eager learners, we selected SMO, an im-plementation of Support-Vector Machines (SVM)using Sequential Minimal Optimization (Platt,1998), and Maxent, an implementation of Maxi-mum Entropy learning (Le, 2006).
SMO is em-bedded in the WEKA (Waikato Environment forKnowledge Analysis) software package (Wittenand Frank, 1999).Our expectation is that eager learners will tendto overgeneralize for this task when dealing withlimited training data, while lazy learners, by de-laying generalization over training data until thetest phase, will be at an advantage when dealingwith limited data.
Unlike eager learners, they willnot ignore - i.e.
not abstract away from - the fre-quently occurring infrequent or untypical patternsin the training data, that will nevertheless be usefulin generalization.4 Results and DiscussionIn this section, we present results of experimentsconcerning the three main issues of this paper (seeSection 3.2 for the experimental set-up):(a) the effect of many authors on feature selectionand learning;(b) the effect of limited data in authorship attri-bution;(c) the results of authorship verification usingmany authors and limited data on learning.4.1 Authorship Attribution(a) Figure 2 shows the effect of many authors inauthorship attribution experiments using memory-based learning (TiMBL) (k=1) and separate fea-ture sets.
Most authorship attribution studies fo-Number of authorsAccuracy in%20406080100x2 100x5 100x10 20 50 100 145cgp1cgp2cgp3 fwd1lex1lex2 lex3pos1pos2 pos3tokFigure 2: The effect of many authors using singlefeature setscus on a small set of authors and report good re-sults, but systematically increasing the amount ofauthors under investigation leads to a significantdecrease in performance.
In the 2-author task (100experiments with random samples of 2 authors),we achieve an average accuracy of 96.90%, whichis in line with results reported in other studies onsmall sets of authors.
The 5-, 10- (both in 100experiments with random samples) and 20-authortasks show a gradual decrease in performance withresults up to 88%, 82% and 76% accuracy, respec-tively.
A significant fall in accuracy comes with the50- and 100-author attribution task, where accu-racy drops below 52% for the best performing fea-ture sets.
Experiments with all 145 authors fromthe corpus (as a multiclass problem) show an ac-curacy up to 34%.
Studies reporting on accuraciesover 95% are clearly overestimating their perfor-mance on a small set of authors.Incremental combinations of feature sets per-forming well in authorship attribution lead to anaccuracy of almost 50% in the 145-author case, asis shown in Figure 3.
This indicates that provid-ing a more heterogeneous set of features improvesthe system significantly.
Memory-based learningshows robustness for a large set of authors in au-thorship attribution.As far as feature selection is concerned, we findthat similar types of features tend to work well forsmall and large sets of authors in our corpus, butthat no generalisations can be made about individ-ual features towards other corpora or studies, sincethis is highly dependent of the specific authors se-lected.516Number of authorsAccuracy in%20406080100x2 100x5 100x10 20 50 100 145lex1+pos1lex1+pos1+toklex1+pos1+tok+fwdlex1+pos2lex1+pos2+toklex1+pos2+tok+fwdlex1+tokFigure 3: The effect of many authors using combi-nations of feature setsNumber of authorsEvolution of chi?square value010203040100x2 100x5 100x10 20 50 100 145?conclusieerg iklangsman miermijnstrengen wasFigure 4: The effect of many authors on ?2valuefor 2-author discriminating featuresFigure 4 shows the top-ten features with highest?2value in one of the randomly selected 2-authorsamples.
In 5-author cases, we see that some ofthese features have some discriminating power, butwith the increase of the number of authors comesa decrease in importance.
(b) The effect of limited data is demonstratedby means of a learning curve.
The performanceof lazy learner TiMBL is compared to that of ea-ger learners Maxent (Maximum Entropy Learning)and SMO (Support-Vector Machines) when com-paring different training set sizes.
Figure 5 showsthe evolution of learning in authorship attributionusing the lex1 feature set.
Although memory-basedlearning does show robustness when dealing withlimited data, we cannot show a clear superiorityon this aspect to the eager learning methods in thisexperiment.
However, results are good enough toPercentage of training dataAccuracy in%2040608020 40 60 80TiMBL 2 authorsMaxentSMO TiMBL 145 authorsMaxentSMOFigure 5: The effect of limited data in authorshipattribution on lex1warrant continuing the experiments on authorshipverification with this method.4.2 Authorship Verification(c) We now focus on a more realistic interpreta-tion of the authorship attribution task, viz.
as aauthorship verification problem.
Forensic expertswant to answer both questions of authorship attri-bution (Which of the n candidate authors wrotethe document?)
and verification (Did candidateauthor x write the document?).
They often dealwith limited data like short e-mails or letters, andthe amount of candidate authors is essentially un-known.
With only few positive instances (of 1 au-thor) and a large amount of negative instances (of144 authors in our corpus), we are dealing withhighly skewed class distributions.We approximate the author verification problemby defining a binary classification task with the au-thor fragments as positive training data, and thefragments of all the other authors as negative train-ing data.
A more elegant formulation would be asa one-class problem (providing only positive train-ing data), but in exploratory experiments, theseone-class learning approaches did not yield usefulresults.We evaluate authorship verification experimentsby referring to precision and recall of the positiveclass.
Recall represents the proportional numberof times an instance of the positive class has cor-rectly been classified as positive.
Precision showsthe proportion of test instances predicted by thesystem to be positive that was correctly classifiedas such.517Feature set Precision Recall F-scoretok 20.66% 15.93% 17.99%fwd 37.89% 8.41% 13.76%lex1 56.04% 7.03% 12.49%lex2 47.95% 5.66% 10.12%lex3 34.05% 8.73% 13.90%cgp1 25.70% 24.55% 25.11%cgp2 36.35% 18.28% 24.33%cgp3 33.13% 3.79% 6.80%pos1 42.42% 0.97% 1.90%pos2 42.66% 4.21% 7.66%pos3 38.75% 2.14% 4.06%Table 1: Results of one vs. all Authorship Verifi-cation experiments using MBLTable 1 shows the results for the positive class ofone vs. all authorship verification using memory-based learning.
We see that memory-based learn-ing on the authorship verification task is able tocorrectly classify up to 56% of the positive classwhich is highly underrepresented in both trainingand test data.
Despite the very skewed class distri-butions, memory-based learning scores reasonablywell on this approximation of authorship verifica-tion with limited data.
The most important lessonis that in a realistic set-up of the task of authorshipverification, the accuracy to be expected is muchlower than what in general can be found in the pub-lished literature.5 Related ResearchAs mentioned earlier, most research in author-ship attribution starts from unrealistic assumptionsabout numbers of authors and amount of trainingdata available.
We list here the exceptions to thisgeneral rule.
These studies partially agree with ourown results.
Argamon et al (2003) report on re-sults in authorship attribution on twenty authors ina corpus of Usenet newsgroups on a variety of top-ics.
Depending on the topic, results vary from 25%(books, computer theory) to 45% accuracy (com-puter language) for the 20-author task.
Linguis-tic profiling, a technique presented by Van Hal-teren (2005), takes large numbers of linguistic fea-tures to compare separate authors to average pro-files.
In a set of eight authors, a linguistic pro-filing system correctly classifies 97% of the testdocuments.
Madigan et al (2005) use a collec-tion of data released by Reuters consisting of 114authors, each represented by a minimum of 200texts.
Results of Bayesian multinomial logistic re-gression on this corpus show error rates between97% and 20%, depending on the type of featuresapplied.
This is only partially comparable to theauthorship attribution results on 145 authors pre-sented in this paper because of the large amount ofdata in the Madigan et al (2005) study, while oursystem works on limited data.
In a study of weblogcorpora, Koppel et al (2006) show that authorshipattribution with thousands of candidate authors isreasonably reliable, since the system gave an an-swer in 31.3% of the cases, while the answer iscorrect in almost 90% of the cases.
Whereas thesecases show similar results as ours, we believe thisstudy is the first to study the effect of trainingset size and number of authors involved system-atically.When applied to author verification on eightauthors, the linguistic profiling system (Van Hal-teren, 2005) has a False Reject Rate (FRR) of 0%and a False Accept Rate (FAR) of 8.1%.
Argamonet al (2003) also report on one vs. all learning ina set of twenty authors.
Results vary from 19%(books, computer theory) to 43% (computer lan-guage) accuracy, depending on the topics.
Madi-gan et al (2005) also did authorship verificationexperiments on their corpus of 114 authors we de-scribed above.
They vary the number of target,decoy, and test authors to find that the ideal splitis 10-50-54, which produces an error rate of 24%.Koppel et al (2007) also report on results in onevs.
all experiments.
Using a corpus of 21 booksby 10 authors in different genres (including essays,plays, and novels), their system scores a precisionof 22.30% and recall of 95%.
Our system performsbetter in precision and worse in recall.
Their cor-pus nevertheless consists of 21 books (each rep-resented by more than forty 500-word chunks) by10 authors, which makes the task considerably lessdifficult.6 Conclusions and Further ResearchA lot of the research in authorship attribution isperformed on a small set of authors and unrealisticsizes of data, which is an artificial situation.
Mostof these studies not only overestimate the perfor-mance of their system, but also the importance oflinguistic features in experiments discriminatingbetween only two or a small number of authors.In this paper, we have shown the effect of manyauthors and limited data in authorship attribution518and verification.
When systematically increasingthe number of authors in authorship attribution, wesee that performance drops significantly.
Similartypes of features work well for different amountsof authors in our corpus, but generalizations aboutindividual features are not useful.Memory-based learning shows robustness whendealing with limited data, which is essential in e.g.forensics.
Results from experiments in authorshipattribution on 145 authors indicate that in almost50% of the cases, a text from one of the 145 au-thors is classified correctly.
Using combinations ofgood working lexical and syntactic features leadsto significant improvements.
The authorship veri-fication task is a much more difficult task, which,in our approximation of it, leads to a correct clas-sification in 56% of the test cases.
It is clear thatstudies reporting over 95% accuracy on a 2-authorstudy are overestimating their performance and theimportance of the features selected.Further research with the 145-author corpus willinvolve a study of handling with imbalanced dataand experimenting with other machine learning al-gorithms for authorship attribution and verifica-tion and a more systematic study of the behaviorof different types of learning methods (includingfeature selection and other optimization issues) onthis problem.AcknowledgementsThis study has been carried out in the frameworkof the ?Computational Techniques for Stylometryfor Dutch?
project, funded by the National Fundfor Scientific Research (FWO) in Belgium.
Wewould like to thank the reviewers for their com-ments which helped improve this paper.ReferencesArgamon, Shlomo, Marin Saric, and Sterling S. Stein.2003.
Style mining of electronic messages for mul-tiple authorship discrimination: First results.
In Pro-ceedings of the 2003 Association for Computing Ma-chinery Conference on Knowledge Discovery andData Mining (ACM SIGKDD), pages 475?480.Argamon, Shlomo, Casey Whitelaw, Paul Chase,Sushant Dawhle, Sobhan R. Hota, Navendu Carg,and Shlomo Levitan.
2007.
Stylistic text classifica-tion using functional lexical features.
Journal of theAmerican Society of Information Science and Tech-nology, 58(6):802?822.Baayen, Harald R., Hans Van Halteren, and FionaTweedie.
1996.
Outside the cave of shadows: Usingsyntactic annotation to enhance authorship attribu-tion.
Literary and Linguistic Computing, 11(3):121?131.Briggs Myers, Isabel and Peter B. Myers.
1980.
Giftsdiffering: Understanding personality type.
Moun-tain View, CA: Davies-Black Publishing.Burrows, John.
2007.
All the way through: Testing forauthorship in different frequency data.
Literary andLinguistic Computing, 22(1):27?47.Coyotl-Morales, Rosa M., Luis Villase?nor Pineda,Manuel Montes-y G?omez, and Paolo Rosso.
2006.Authorship attribution using word sequences.
InProceedings of the Iberoamerican Congress on Pat-tern Recognition (CIARP), pages 844?853.Daelemans, Walter and Antal van den Bosch.
2005.Memory-Based Language Processing.
Studiesin Natural Language Processing.
Cambridge, UK:Cambridge University Press.Daelemans, Walter, Jakub Zavrel, Ko van der Sloot, andAntal van den Bosch.
2007.
TiMBL: Tilburg Mem-ory Based Learner, version 6.1, Reference Guide.Technical Report ILK Research Group Technical Re-port Series no.
07-07, ILK Research Group, Univer-sity of Tilburg.Diederich, Joachim, J?org.
Kindermann, Edda Leopold,and Gerhard Paass.
2000.
Authorship attributionwith Support Vector Machines.
Applied Intelligence,19(1-2):109?123.Feiguina, Ol?ga and Graeme Hirst.
2007.
Authorshipattribution for small texts: literary and forensic ex-periments.
In Proceedings of the 30th InternationalConference of the Special Interest Group on Infor-mation Retrieval: Workshop on Plagiarism Analysis,Authorship Identification, and Near-Duplicate De-tection (SIGIR).Gamon, Michael.
2004.
Linguistic correlates of style:Authorship classification with deep linguistic anal-ysis features.
In Proceedings of the 2004 Inter-national Conference on Computational Linguistics(COLING), pages 611?617.Hirst, Graeme and Ol?ga Feiguina.
2007.
Bigramsof syntactic labels for authorship discrimination ofshort texts.
Literary and Linguistic Computing,22(4):405?417.Holmes, D. 1994.
Authorship Attribution.
Computersand the Humanities, 28(2):87?106.Koppel, Moshe, Jonathan Schler, Shlomo Argamon,and Eran Messeri.
2006.
Authorship attributionwith thousands of candidate authors.
In Proceed-ings of the 29th International Conference of the Spe-cial Interest Group on Information Retrieval (SI-GIR), pages 659?660.519Koppel, Moshe, Jonathan Schler, and ElishevaBonchek-Dokow.
2007.
Measuring differentiabil-ity: Unmasking pseudonymous authors.
Journal ofMachine Learning Research, 8:1261?1276.Le, Zhang.
2006.
Maximum Entropy Modeling Toolkitfor Python and C++.
Version 20061005.Luyckx, Kim and Walter Daelemans.
2008.
Per-sonae: a corpus for author and personality predictionfrom text.
In Proceedings of the 6th InternationalConference on Language Resources and Evaluation(LREC).Madigan, David, Alexander Genkin, David D. Lewis,Shlomo Argamon, Dmitriy Fradkin, and Li Ye.2005.
Author identification on the large scale.
InProceedings of the 2005 Meeting of the Classifica-tion Society of North America (CSNA).Miranda Garc?
?a, Antonio and Javier Calle Mart??n.2007.
Function words in authorship attribution stud-ies.
Literary and Linguistic Computing, 22(1):49?66.Mosteller, F. and D. Wallace.
1964.
Inference and dis-puted authorship: the Federalist.
Series in Behav-ioral Science: Quantitative Methods Edition.Platt, John, 1998.
Advances in Kernel Methods - Sup-port Vector Learning, chapter Fast training of Sup-port Vector Machines using Sequential Minimal Op-timization, pages 185?208.Sebastiani, Fabrizio.
2002.
Machine learning in auto-mated text categorization.
Association for Comput-ing Machinery (ACM) Computing Surveys, 34(1):1?47.Stamatatos, Efstathios.
2007.
Author identification us-ing imbalanced and limited training texts.
In Pro-ceedings of the 18th International Conference onDatabase and Expert Systems Applications (DEXA),pages 237?241.Van Halteren, Hans.
2005.
Linguistic profiling for au-thor recognition and verification.
In Proceedings ofthe 2005 Meeting of the Association for Computa-tional Linguistics (ACL).Witten, Ian and Eibe Frank.
1999.
Data Mining: Prac-tical Machine Learning Tools with Java Implemen-tations.
San Fransisco: Morgan Kaufmann.520
