Proceedings of the ACL-08: HLT Demo Session (Companion Volume), pages 1?4,Columbus, June 2008. c?2008 Association for Computational LinguisticsDemonstration of a POMDP Voice DialerJason WilliamsAT&T Labs ?
Research, Shannon Laboratory180 Park Ave., Florham Park, NJ 07932, USAjdw@research.att.comAbstractThis is a demonstration of a voice di-aler, implemented as a partially observableMarkov decision process (POMDP).
A real-time graphical display shows the POMDP?sprobability distribution over different possi-ble dialog states, and shows how system out-put is generated and selected.
The systemdemonstrated here includes several recent ad-vances, including an action selection mecha-nism which unifies a hand-crafted controllerand reinforcement learning.
The voice dialeritself is in use today in AT&T Labs and re-ceives daily calls.1 IntroductionPartially observable Markov decision processes(POMDPs) provide a principled formalism for plan-ning under uncertainty, and past work has arguedthat POMDPs are an attractive framework for build-ing spoken dialog systems (Williams and Young,2007a).
POMDPs differ from conventional dialogsystems in two respects.
First, rather than main-taining a single hypotheses for the dialog state,POMDPs maintain a probability distribution calleda belief state over many possible dialog states.
Adistribution over a multiple dialog state hypothe-ses adds inherent robustness, because even if an er-ror is introduced into one dialog hypothesis, it canlater be discarded in favor of other, uncontaminateddialog hypotheses.
Second, POMDPs choose ac-tions using an optimization process, in which a de-veloper specifies high-level goals and the optimiza-tion works out the detailed dialog plan.
Becauseof these innovations, POMDP-based dialog systemshave, in research settings, shown more resilienceto speech recognition errors, yielding shorter di-alogs with higher task completion rates (Williamsand Young, 2007a; Williams and Young, 2007b).Because POMDPs differ significantly from con-ventional techniques, their operation can be difficultto conceptualize.
This demonstration provides anaccessible illustration of the operation of a state-of-the-art POMDP-based dialog system.
The systemitself is a voice dialer, which has been operationalfor several months in AT&T Labs.
The system in-corporates several recent advances, including effi-cient large-scale belief monitoring (akin to Young etal., 2006), policy compression (Williams and Young,2007b), and a hybrid hand-crafted/optimized dialogmanager (Williams, 2008).
All of these elementsare depicted in a graphical display, which is updatedin real time, as a call is progressing.
Whereas pre-vious demonstrations of POMDP-based dialog sys-tems have focused on showing the probability distri-bution over dialog states (Young et al, 2007), thisdemonstration adds new detail to convey how ac-tions are chosen by the dialog manager.In the remainder of this paper, Section 2 presentsthe dialog system and explains how the POMDP ap-proach has been applied.
Then, section 3 explainsthe graphical display which illustrates the operationof the POMDP.2 System descriptionThis application demonstrated here is a voice dialerapplication, which is accessible within the AT&T re-search lab and receives daily calls.
The dialer?s vo-1cabulary consists of 50,000 AT&T employees.The dialog manager in the dialer is implementedas a POMDP.
In the POMDP approach, a distribu-tion called a belief state is maintained over manypossible dialog states, and actions are chosen us-ing reinforcement learning (Williams and Young,2007a).
In this application, a distribution is main-tained over all of the employees?
phone listings inthe dialer?s vocabulary, such as Jason Williams?
of-fice phone or Srinivas Bangalore?s cell phone.
Asspeech recognition results are received, this distri-bution is updated using probability models of howusers are likely to respond to questions and how thespeech recognition process is likely to corrupt userspeech.
The benefit of tracking this belief state isthat it synthesizes all of the ASR N-Best lists overthe whole dialog ?
i.e., it makes the most possibleuse of the information from the speech recognizer.POMDPs then choose actions based on this be-lief state using reinforcement learning (Sutton andBarto, 1998).
A developer writes a reward func-tion which assigns a real number to each state/actionpair, and an optimization algorithm determines howto choose actions in order to maximize the expectedsum of rewards.
In other words, the optimizationperforms planning and this allows a developer tospecify the trade-off to use between task comple-tion and dialog length.
In this system, a simple re-ward function assigns -1 per system action plus +/-20 for correctly/incorrectly transferring the caller atthe end of the call.
Optimization was performedroughly following (Williams and Young, 2007b), byrunning dialogs in simulation.Despite their theoretical elegance, applying aPOMDP to this spoken dialog system has presentedseveral interesting research challenges.
First, scal-ing the number of listings quickly prevents the be-lief state from being updated in real-time, and herewe track a distribution over partitions, which is akinto a beam search in ASR (Young et al, 2006).
Atfirst, all listings are undifferentiated in a single mas-ter partition.
If a listing appears on the N-Best list,it is separated into its own partition and tracked sep-arately.
If the number of partitions grows too large,then low-probability partitions are folded back intothe master undifferentiated partition.
This techniqueallows a well-formed distribution to be maintainedover an arbitrary number of concepts in real-time.Second, the optimization process which choosesactions is also difficult to scale.
To tackle this,the so-called ?summary POMDP?
has been adopted,which performs optimization in a compressed space(Williams and Young, 2007b).
Actions are mappedinto clusters called mnemonics, and states are com-pressed into state feature vectors.
During opti-mization, a set of template state feature vectors aresampled, and values are computed for each actionmnemonic at each template state feature vector.Finally, in the classical POMDP approach there isno straightforward way to impose rules on systembehavior because the optimization algorithm con-siders taking any action at any point.
This makesit impossible to impose design constraints or busi-ness rules, and also needlessly re-discovers obviousdomain properties during optimization.
In this sys-tem, a hybrid POMDP/hand-crafted dialog manageris used (Williams, 2008).
The POMDP and con-ventional dialog manager run in parallel; the con-ventional dialog manager nominates a set of one ormore allowed actions, and the POMDP chooses theoptimal action from this set.
This approach enablesrules to be imposed and allows prompts to easily bemade context-specific.The POMDP dialer has been compared to a con-vention version in dialog simulation, and improvedtask completion from 92% to 97% while keeping di-alog length relatively stable.
The system has beendeployed in the lab and we are currently collectingdata to assess performance with real callers.3 DemonstrationA browser-based graphical display has been createdwhich shows the operation of the POMDP dialerin real time, shown in Figure 1.
The page is up-dated after the user speech has been processed, andbefore the next system action has been played tothe user.
The left-most column shows the systemprompt which was just played to the user, and theN-Best list of recognized text strings, each with itsconfidence score.The center column shows the POMDP beliefstate.
Initially, all of the belief is held by the mas-ter, undifferentiated partition, which is shown as agreen bar and always shown first.
As names are rec-ognized, they are tracked separately, and the top 102Previoussystem actionN-BestrecognitionwithconfidencescoresPOMDP beliefstateFeatures of thecurrent dialogstateAllowedactionsValues of theallowedactionsResultingsystem action,output to TTSFigure 1: Overview of the graphical display.
Contents are described in the text.names are shown as blue bars, sorted by their belief.If the system asks for the phone type (office or mo-bile), then the bars sub-divide into a light blue (foroffice) and dark blue (for mobile).The right column shows how actions are selected.The top area shows the features of the current stateused to choose actions.
Red bars show the two con-tinuous features: the belief in the most likely nameand most likely type of phone.
Below that, threediscrete features are shown: how many phones areavailable (none, one, or both); whether the mostlikely name has been confirmed (yes or no); andwhether the most likely name is ambiguous (yesor no).
Below this, the allowed actions (i.e., thosewhich are nominated by the hand-crafted dialogmanager) are shown.
Each action is preceded by theaction mnemonic, shown in bold.
Below the allowedactions, the action selection process is shown.
Thevalues of the action mnemonic at the closest tem-plate point are shown next to each action mnemonic.Finally the text of this action, which is output to thecaller, is shown at the bottom of the right-hand col-umn.
Figure 2 shows the audio and video transcrip-tion of an interaction with the demonstration.4 ConclusionThis demonstration has shown the operation of aPOMDP-based dialog system, which incorporatesrecent advances including efficient large-scale beliefmonitoring, policy compression, and a unified hand-crafted/optimized dialog manager.
A graphical dis-play shows the operation of the system in real-time,as a call progresses, which helps make the POMDPapproach accessible to a non-specialist.AcknowledgmentsThanks to Iker Arizmendi and Vincent Goffin forhelp with the implementation.ReferencesR Sutton and A Barto.
1998.
Reinforcement Learning:an Introduction.
MIT Press.JD Williams and SJ Young.
2007a.
Partially observableMarkov decision processes for spoken dialog systems.Computer Speech and Language, 21(2):393?422.JD Williams and SJ Young.
2007b.
Scaling POMDPs forspoken dialog management.
IEEE Trans.
on Audio,Speech, and Language Processing, 15(7):2116?2129.JD Williams.
2008.
The best of both worlds: Unifyingconventional dialog systems and POMDPs.
In (In sub-mission).SJ Young, JD Williams, J Schatzmann, MN Stuttle, andK Weilhammer.
2006.
The hidden information stateapproach to dialogue management.
Technical Re-port CUED/F-INFENG/TR.544, Cambridge Univer-sity Engineering Department.SJ Young, J Schatzmann, B R M Thomson, KWeilham-mer, and H Ye.
2007.
The hidden information statedialogue manager: A real-world POMDP-based sys-tem.
In Proc NAACL-HLT, Rochester, New York, USA.3Transcript of audioScreenshotsofgraphicaldisplayS1: Sorry,first andlast name?U1: JunlanFengS1: DialingS1: JunlanFeng.U1: YesS1: First andlast name?U1: JunlanFengFigure2:Thedemonstration?sgraphicaldisplayduringacall.Thegraphicaldisplayhasbeencroppedandre-arrangedforreadability.Thecallersays?JunlanFeng?twice,andalthougheachnamerecognitionalonecarriesalowconfidencescore,thebeliefstateaggregatesthisinformation.ThisnovelbehaviorenablesthecalltoprogressfasterthanintheconventionalsystemandillustratesonebenefitofthePOMDPapproach.Wehaveobservedseveralothernovelstrategiesnotinabaselineconventionaldialer:forexample,thePOMDP-basedsystemwillconfirmacallee?snameatdifferentconfidencelevelsdependingonwhetherthecalleehasaphonenumberlistedornot;andusesyes/noconfirmationquestionstodisambiguatewhentherearetwoambiguouscallees.4
