Coling 2010: Poster Volume, pages 1382?1390,Beijing, August 2010Chasing the ghost: recovering empty categories in the Chinese TreebankYaqin YangComputer Science DepartmentBrandeis Universityyaqin@cs.brandeis.eduNianwen XueComputer Science DepartmentBrandeis Universityxuen@cs.brandeis.eduAbstractEmpty categories represent an impor-tant source of information in syntacticparses annotated in the generative linguis-tic tradition, but empty category recoveryhas only started to receive serious atten-tion until very recently, after substantialprogress in statistical parsing.
This paperdescribes a unified framework in recover-ing empty categories in the Chinese Tree-bank.
Our results show that given skele-tal gold standard parses, the empty cate-gories can be detected with very high ac-curacy.
We report very promising resultsfor empty category recovery for automaticparses as well.1 IntroductionThe use of empty categories to represent the syn-tactic structure of a sentence is the hallmark of thegenerative linguistics and they represent an im-portant source of information in treebanks anno-tated in this linguistic tradition.
The use of emptycategories in the annotation of treebanks startedwith the Penn Treebank (Marcus et al, 1993), andthis practice is continued in the Chinese Treebank(CTB) (Xue et al, 2005) and the Arabic Tree-bank, the Penn series of treebanks.
Empty cat-egories come in a few different varieties, serv-ing different purposes.
One use of empty cate-gories is to mark the extraction site of an dislo-cated phrase, thus effectively reconstructing thecanonical structure of a sentence, allowing easyextraction of its predicate-argument structure.
Forexample, in Figure 1, the empty category *T*-1 is coindexed with the dislocated topic NP ??
(?Ningbo?
), indicating that the canonical po-sition of this NP is next to the verb ?
(?come?
).The empty category effectively localizes the syn-tactic dependency between the verb and this NP,making it easier to detect and extract this relation.Marking the extraction site of a dislocated itemis not the only use of empty categories.
For lan-guages like Chinese, empty categories are alsoused to represent dropped pronouns.
Chinese isa pro-drop language (Huang, 1989) and subjectpronouns are routinely dropped.
Recovering theseelliptical elements is important to many naturallanguage applications.
When translated into an-other language, for example, these dropped pro-nouns may have to be made explicit and replacedwith overt pronouns or noun phrases if the targetlanguage does not allow dropped pronouns.Although empty categories have been an inte-gral part of the syntactic representation of a sen-tence ever since the Penn Treebank was first con-structed, it is only recently that they are startingto receive the attention they deserve.
Works onautomatic detection of empty categories startedto emerge (Johnson, 2002; Dienes and Dubey,2003; Campbell, 2004; Gabbard et al, 2006) af-ter substantial progress has been made in statis-tical syntactic parsing.
This progress has beenachieved after over a decade of intensive researchon syntactic parsing that has essentially left theempty categories behind (Collins, 1999; Char-niak, 2000).
Empty categories were and still areroutinely pruned out in parser evaluations (Blacket al, 1991).
They have been excluded from theparser development and evaluation cycle not somuch because their importance was not under-stood, but because researchers haven?t figured out1382IPNP-PN-TPC-1 NP-SBJ VPNR PN VC VP?
?ningboNingbo?woI?shibeQP-ADV VPOD CLP VV NP-OBJ?
?disanthirdM ?laicome-NONE-?ci*T*-1?Ningbo, this is the third time I came here.
?Figure 1: A CTB tree with empty categoriesa way to incorporate the empty category detectionin the parsing process.
In fact, the detection ofempty categories relies heavily on the other com-ponents of the syntactic representation, and as aresult, empty category recovery is often formu-lated as postprocessing problem after the skeletalstructure of a syntactic parse has been determined.As work on English has demonstrated, empty cat-egory detection can be performed with high accu-racy given high-quality skeletal syntactic parses asinput.Because Chinese allows dropped pronouns andthus has more varieties of empty categories thanlanguages like English, it can be argued that thereis added importance in Chinese empty categorydetection.
However, to our knowledge, there hasbeen little work in this area, and the work wereport here represents the first effort in Chineseempty category detection.
Our results are promis-ing, but they also show that Chinese empty cat-egory detection is a very challenging problemmostly because Chinese syntactic parsing is dif-ficult and still lags significantly behind the stateof the art in English parsing.
We show that givenskeletal gold-standard parses (with empty cate-gories pruned out), the empty detection can beperformed with a fairly high accuracy of almost89%.
The performance drops significantly, to63%, when the output of an automatic parser isused.The rest of the paper is organized as follows.In Section 2, we formulate the empty category de-tection as a binary classification problem whereeach word is labeled as either having a empty cat-egory before it or not.
This makes it possible touse any standard machine learning technique tosolve this problem.
The key is to find the appro-priate set of features.
Section 3 describes the fea-tures we use in our experiments.
We present ourexperimental results in Section 4.
There are twoexperimental conditions, one with gold standardtreebank parses (stripped of empty categories) asinput and the other with automatic parses.
Section5 describes related work and Section 6 concludeour paper.2 Formulating the empty categorydetection as a tagging problemIn the CTB, empty categories are marked in aparse tree which represents the hierarchical struc-ture of a sentence, as illustrated in Figure 1.There are eight types of empty categories anno-tated in the CTB, and they are listed in Table 1.Among them, *pro* and *PRO* are used to rep-resent nominal empty categories, *T* and *NP*are used to represent traces of dislocated items,*OP* is used to represent empty relative pronounsin relative clauses, and *RNR* is used to repre-sent pseudo attachment.
The reader is referred tothe CTB bracketing manual (Xue and Xia, 2000)for detailed descriptions and examples.
As canbe seen from Table 1, the distribution of theseempty categories is very uneven, and many ofthese empty categories do not occur very often.1383EC Type count Description*pro* 2024 small pro*PRO* 2856 big pro*T* 4486 trace for extraction*RNR* 217 right node raising*OP* 879 operator* 132 trace for raisingTable 1: Empty categories in CTB.As a first step of learning an empty categorymodel, we treat all the empty categories as a uni-fied type, and for each word in the sentence, weonly try to decide if there is an empty categorybefore it.
This amounts to an empty category de-tection task, and the objective is to first locate theempty categories without attempting to determinethe specific empty category type.
Instead of pre-dicting the locations of the empty categories in aparse tree and having a separate classifier for eachsyntactic construction where an empty category islikely to occur, we adopt a linear view of the parsetree and treat empty categories, along with overtword tokens, as leaves in the tree.
This allows usto identify the location of the empty categories inrelation to overt word tokens in the same sentence,as illustrated in Example (1):(1) ??
?
?
??
?
?
*T*?In this representation, the position of the emptycategory can be defined either in relation to theprevious or the next word, or both.
To makethis even more amenable to machine learning ap-proaches, we further reformulate the problem as atagging problem so that each overt word is labeledeither with EC, indicating there is an empty cate-gory before this word, or NEC, indicating there isno empty category.
This reformulated representa-tion is illustrated in Example (2):(2) ?
?/NEC ?/NEC ?/NEC ?
?/NEC?/NEC?/NEC?/ECIn (2), the EC label attached to the final periodindicates that there is an empty category beforethis punctuation mark.
There is a small price topay with this representation: when there is morethan one empty category before a word, it is indis-tinguishable from cases where there is only oneempty category.
What we have gained is a sim-ple unified representation for all empty categoriesthat lend itself naturally to machine learning ap-proaches.
Another advantage is that for naturallanguage applications that do not need the fullparse trees but only need the empty categories,this representation provides an easy-to-use repre-sentation for those applications.
Since this linear-lized representation is still aligned with its parsetree, we still have easy access to the full hierar-chical structure of this tree from which useful fea-tures can be extracted.3 FeaturesHaving modeled empty category detection as amachine learning task, feature selection is crucialto successfully finding a solution to this problem.The machine learning algorithm scans the wordsin a sentence from left to right one by one anddetermine if there is an empty category before it.When the sentence is paired with its parse tree,the feature space is all the surrounding words ofthe target word as well as the syntactic parse forthe sentence.
The machine learning algorithm alsohas access to the empty category labels (EC orNEC) of all the words before the current word.Figure 2 illustrates the feature space for the lastword (a period) in the sentence.NP VPNR PN VCVPQPODCLPMVPVV PUNPIP!"
# $% & ' !NEC NEC NEC NEC NEC NEC ECNingbo I be third time come .
("Ningbo, this is the third time I came here.
"Figure 2: Feature space of empty category detec-tionFor purposes of presentation, we divide ourfeatures into lexical and syntactic features.
The1384lexical features are different combinations of thewords and their parts of speech (POS), while syn-tactic features are the structural information gath-ered from the nonterminal phrasal labels and theirsyntactic relations.3.1 Lexical featuresThe lexical features are collected from a narrowwindow of five words and their POS tags.
If thetarget word is a verb, the lexical features also in-clude transitivity information of this verb, whichis gathered from the CTB.
A transitivity lexicon isinduced from the CTB by checking whether a verbhas a right NP or IP sibling.
Each time a verb isused as a transitive verb (having a right NP or IPsibling), its transitive count is incremented by one.Conversely, each time a verb is used as an intran-sitive verb (not having a right NP or IP sibling), itsintransitive use is incremented by one.
The result-ing transitivity lexicon after running through theentire Chinese Treebank consists of a list of verbswith frequencies of their transitive and intransitiveuses.
A verb is considered to be transitive if its in-transitive count in this lexicon is zero or if its tran-sitive use is more than three times as frequent asits intransitive use.
Similarly, a verb is consideredto be intransitive if its transitive count is zero orif its intransitive use is at least three times as fre-quent as its transitive use.
The full list of lexicalfeatures is presented in Table 2.3.2 Syntactic featuresSyntactic features are gathered from the CTBparses stripped of function tags and empty cate-gories when the gold standard trees are used asinput.
The automatic parses used as input to oursystem are produced by the Berkeley parser.
Likemost parsers, the Berkeley parser does not repro-duce the function tags and empty categories in theoriginal trees in the CTB.
Syntactic features cap-ture the syntactic context of the target word, andas we shall show in Section 4, the syntactic fea-tures are crucial to the success of empty categorydetection.
The list of syntactic features we use inour system include:1.
1st-IP-child: True if the current word is thefirst word in the lowest IP dominating thisword.Feature Names Descriptionword(0) Current wordword(-1) Previous wordpos(0) POS of current wordpos(-1,0) POS of previous and cur-rent wordpos(0, 1) POS of current and nextwordpos(0, 1, 2) POS of current & nextword, & word 2 afterpos(-2, -1) POS of previous word &word 2 beforeword(-1), pos(0) Previous word & POS ofcurrent wordpos(-1),word(0) POS of previous word&current wordtrans(0) current word is transitiveor intransitive verbprep(0) true if POS of currentword is a prepositionTable 2: Feature set.2.
1st-word-in-subjectless-IP: True if the cur-rent word starts an IP with no subject.
Sub-ject is detected heuristically by looking at leftsisters of a VP node.
Figure 3 illustrates thisfeature for the first word in a sentence wherethe subject is a dropped pronoun.3.
1st-word-in-subjectless-IP+POS: POS ofthe current word if it starts an IP with no sub-ject.4.
1st-VP-child-after-PU: True if the currentword is the first terminal child of a VP fol-lowing a punctuation mark.5.
NT-in-IP: True if POS of current word is NT,and it heads an NP that does not have a sub-ject NP as its right sister.6.
verb-in-NP/VP: True if the current word is averb in an NP/VP.7.
parent-label: Phrasal label of the parent ofthe current node, with the current node al-ways corresponding to a terminal node in theparse tree.8.
has-no-object: True If the previous word isa transitive verb and this verb does not takean object.1385!"#$%&'("#)*+,-%.
*/0("#.123 4("#*)/.*2%56("#*77898)*,:;.)%-<=("#*>>/?;.
@ABCD("#EFGH("#ILCPPPAD ADVVCD MLCNTP QPVPADVPADVPVPIPBy the end of  last year, (Shanghai) has approved 216 ...Figure 3: First word in a subject-less IPEmpty categories generally occur in clausal orphrasal boundaries, and most of the features aredesigned to capture such information.
For exam-ple, the five feature types, 1st-IP-child, 1st-word-in-subjectless-IP, 1st-word-in-subjectless-IP, 1st-VP-child-after-PU and NT-in-IP all represent theleft edge of a clause (IP) with some level of gran-ularity.
parent label and verb-in-NP/VP representphrases within which empty categories typicallyoccur do not occur.
The has-no-object feature isintended to capture transitive uses of a verb whenthe object is missing.4 ExperimentsGiven that our approach is independent of specificmachine learning techniques, many standard ma-chine learning algorithms can be applied to thistask.
For our experiment we built a Maximum En-tropy classifier with the Mallet toolkit1.4.1 DataIn our experiments, we use a subset of the CTB6.0.
This subset is further divided into train-ing (files chtb 0081 thorough chtb 0900), devel-opment (files chtb 0041 through chtb 0080) andtest sets (files chtb 0001 through chtb 0040, fileschtb 0901 through chtb 0931).
The reason for notusing the entire Chinese Treebank is that the datain the CTB is from a variety of different sourcesand the automatic parsing accuracy is very unevenacross these different sources.1http://mallet.cs.umass.edu4.2 Experimental conditionsTwo different kinds of data sets were used in theevaluation of our method: 1) gold standard parsetrees from the CTB; and 2) automatic parses pro-duced by the Berkeley parser2 .4.2.1 Gold standard parsesThere are two experimental conditions.
In ourfirst experiment, we use the gold standard parsetrees from the CTB as input to our classifier.
Theversion of the parse tree that we use as input toour classifier is stripped of the empty categoryinformation.
What our system effectively doesis to restore the empty categories given a skele-tal syntactic parse.
The purpose of this experi-ment is to establish a topline and see how accu-rately the empty categories can be restored givena?correct?parse.4.2.2 Automatic parsesTo be used in realistic scenarios, the parse treesneed to be produced automatically from raw textusing an automatic parser.
In our experiments weuse the Berkeley Parser as a representative of thestate-of-the-art automatic parsers.
The input to theBerkeley parser is words that have already beensegmented in the CTB.
Obviously, to achieve fullyautomatic parsing, the raw text should be auto-matically segmented as well.
The Berkeley parsercomes with a fully trained model, and to makesure that none of our test and development data isincluded in the training data in the original model,we retrained the parser with our training set andused the resulting model to parse the documentsin the development and test sets.When training our empty category model usingautomatic parses, it is important that the qualityof the parses match between the training and testsets.
So the automatic parses in the training setare acquired by first training the parser with 4/5of the data and using the resulting model to parsethe remaining 1/5 of the data that has been heldout.
Measured by the ParsEval metric (Black etal., 1991), the parser accuracy stands at 80.3% (F-score), with a precision of 81.8% and a recall of78.8% (recall).2http://code.google.com/p/berkeleyparser13864.3 Evaluation metricsWe use precision, recall and F-measure as ourevaluation metrics for empty category detection.Precision is defined as the number of correctlyidentified Empty Categories (ECs) divided by thetotal number of ECs that our system produced.Recall is defined as the number of correctly iden-tified ECs divided by the total number of EC la-bels in the CTB gold standard data.
F-measureis defined as the geometric mean of precision andrecall.R = # of correctly detected EC# of EC tagged in corpus (1)P = # of correctly detected EC# of EC reported by the system (2)F = 21/R + 1/P (3)4.4 Overall EC detection performanceWe report our best result for the gold standardtrees and the automatic parses produced by theBerkeley parser in Table 3.
These results areachieved by using all lexical and syntactic featurespresented in Section 3.Data Prec.
(%) Rec.
(%) F(%)Gold 95.9 (75.3) 83.0 (70.5) 89.0 (72.8)Auto 80.3 (57.9) 52.1 (50.2) 63.2 (53.8)Table 3: Best results on the gold tree.As shown in Table 3, our feature set workswell for the gold standard trees.
Not surprisingly,the accuracy when using the automatic parses islower, with the performance gap between usingthe gold standard trees and the Berkeley parserat 25.8% (F-score).
When the automatic parseris used, although the precision is 80.3%, the re-call is only 52.1%.
As there is no similar work inChinese empty category detection using the samedata set, for comparison purposes we establisheda baseline using a rule-based approach.
The rule-based algorithm captures two most frequent loca-tions of empty categories: the subject and the ob-ject positions.
Our algorithm labels the first wordwithin a VP with EC if the VP does not have asubject NP.
Similarly, it assigns the EC label to theword immediately following a transitive verb if itdoes not have an NP or IP object.
Since the miss-ing subjects and objects account for most of theempty categories in Chinese, this baseline coversmost of the empty categories.
The baseline resultsare also presented in Table 3 (in brackets).
Thebaseline results using the gold standard trees are75.3% (precision), 70.5% (recall), and 72.8% (F-score).
Using the automatic parses, the results are57.9% (precision), 50.2% (recall), and 53.8% (F-score) respectively.
It is clear from our results thatour machine learning model beats the rule-basedbaseline by a comfortable margin in both exper-imental conditions.
Table 4 breaks down our re-sults by empty category types.
Notice that we didnot attempt to predict the specific empty categorytype.
This only shows the percentage of emptycategories our model is able to recover (recall) foreach type.
As our model does not predict the spe-cific empty category type, only whether there is anempty category before a particular word, we can-not compute the precision for each empty categorytype.
Nevertheless, this breakdown gives us asense of which empty category is easier to recover.For both experimental conditions, the empty cate-gory that can be recovered with the highest accu-racy is *PRO*, an empty category often used insubject/object control constructions.
*pro* seemsto be the category that is most affected by parsingaccuracy.
It has the widest gap between the twoexperimental conditions, at more than 50%.EC Type Total Correct Recall(%)*pro* 290 274/125 94.5/43.1*PRO* 299 298/196 99.7/65.6*T* 578 466/338 80.6/58.5*RNR* 32 22/20 68.8/62.5*OP* 134 53/20 40.0/14.9* 19 9/5 47.4/26.3Table 4: Results of different types of empty cate-gories.4.5 Comparison of feature typesTo investigate the relative importance of lexicaland syntactic features, we experimented with us-ing just the lexical or syntactic features underboth experimental conditions.
The results are pre-1387sented in Table 5.
Our results show that whenusing only the lexical features, the drop in accu-racy is small when automatic parses are used inplace of gold standard trees.
However, when us-ing only the syntactic features, the drop in accu-racy is much more dramatic.
In both experimentalconditions, however, syntactic features are moreeffective than the lexical features, indicating thecrucial importance of high-quality parses to suc-cessful empty category detection.
This makes in-tuitive sense, given that all empty categories oc-cupy clausal and phrasal boundaries that can onlydefined in syntactic terms.Data Prec.
(%) Rec.
(%) F(%)Lexical 79.7/77.3 47.6/39.9 59.6/52.7Syntactic 95.9/78.0 70.0/44.5 81.0/56.7Table 5: Comparison of lexical and syntactic fea-tures.4.6 Comparison of individual featuresGiven the importance of syntactic features, weconducted an experiment trying to evaluate theimpact of each individual syntactic feature on theoverall empty category detection performance.
Inthis experiment, we kept the lexical feature setconstant, and switched off the syntactic featuresone at a time.
The performance of the differentsyntactic features is shown in Table 6.
The re-sults here assume that automatic parses are used.The first row is the result of using all features(both syntactic and lexical) while the last row isthe result of using only the lexical features.
Itcan be seen that syntactic features contribute morethan 10% to the overall accuracy.
The results alsoshow that features (e.g., 1st-IP-child) that captureclause boundary information tend to be more dis-criminative and they occupy the first few rows ofa table that sorted based on feature performance.5 Related workThe problem of empty category detection has beenstudied both in the context of reference resolutionand syntactic parsing.
In the reference resolutionliterature, empty category detection manifests it-self in the form of zero anaphora (or zero pronoun)Feature Name Prec.
(%) Rec.
(%) F(%)all 80.3 52.1 63.21st-IP-child 79.8 49.2 60.81st-VP-child-after-PU79.7 50.5 61.8NT-in-IP 79.4 50.8 61.91st-word-in-subjectless-IP+Pos79.5 51.1 62.2has-no-object 80.0 51.1 62.41st-word-in-subjectless-IP79.4 51.5 62.5verb-in-NP/VP 79.9 52.0 63.0parent-label 79.4 52.4 63.1only lexical 77.3 39.9 52.7Table 6: Performance for individual syntactic fea-tures with automatic parses.detection and resolution.
Zero anaphora resolu-tion has been studied as a computational prob-lem for many different languages.
For example,(Ferra?ndez and Peral, 2000) describes an algo-rithm for detecting and resolving zero pronounsin Spanish texts.
(Seki et al, 2002) and (Lida etal., 2007) reported work on zero pronoun detec-tion and resolution in Japanese.Zero anaphora detection and resolution forChinese has been studied as well.
Converse(2006) studied Chinese pronominal anaphora res-olution, including zero anaphora resolution, al-though there is no attempt to automatically de-tect the zero anaphors in text.
Her work onlydeals with anaphora resolution, assuming the zeroanaphors have already been detected.
Chinesezero anaphora identification and resolution havebeen studied in a machine learning framework-ing in (Zhao and Ng, 2007) and (Peng and Araki,2007).The present work studies empty category re-covery as part of the effort to fully parse naturallanguage text and as such our work is not lim-ited to just recovering zero anaphors.
We arealso interested in other types of empty categoriessuch as traces.
Our work is thus more closely re-lated to the work of (Johnson, 2002), (Dienes andDubey, 2003), (Campbell, 2004) and (Gabbard et1388al., 2006).Johnson (2002) describes a pattern-matchingalgorithm for recovering empty nodes from phrasestructure trees.
The idea was to extract minimalconnected tree fragments that contain an emptynode and its antecedent(s), and to match the ex-tracted fragments against an input tree.
He eval-uated his approach both on Penn Treebank goldstandard trees stripped of the empty categories andon the output of the Charniak parser (Charniak,2000).
(Dienes and Dubey, 2003) describes an emptydetection method that is similar to ours in that ittreats empty detection as a tagging problem.
Thedifference is that the tagging is done without ac-cess to any syntactic information so that the iden-tified empty categories along with word tokens inthe sentence can then be fed into a parser.
The suc-cess of this approach depends on strong local cuessuch as infinitive markers and participles, whichare non-existent in Chinese.
Not surprisingly, ourmodel yields low accuracy if only lexical featuresare used.Cambell (2004) proposes an algorithm that useslinguistic principles in empty category recovery.He argues that a rule-based approach might per-form well for this problem because the locationsof the empty categories, at least in English, are in-serted by annotators who follow explicit linguisticprinciples.Yuqing(2007) extends (Cahill et al, 2004) ?sapproach for recovering English non-local depen-dencies and applies it to Chinese.
This paper pro-poses a method based on the Lexical-FunctionalGrammar f-structures, which differs from our ap-proach.
Based on parser output trees including610 files from the CTB, the authors of this pa-per claimed they have achieved 64.71% f-score fortrace insertion and 54.71% for antecedent recov-ery.
(Gabbard et al, 2006) describes a more recenteffort to fully parse the Penn Treebank, recoveringboth the function tags and the empty categories.Their approach is similar to ours in that they treatempty category recovery as a post-processing pro-cess and use a machine learning algorithm thathas access to the skeletal information in the parsetree.
Their approach is different from ours in thatthey have different classifiers for different types ofempty categories.Although generally higher accuracies are re-ported in works on English empty category re-covery, cross-linguistic comparison is difficult be-cause both the types of empty categories andthe linguistic cues that are accessible to machinelearning algorithms are different.
For example,there are no empty complementizers annotated inthe CTB while English does not allow droppedpronouns.6 Conclusion and future workWe describe a unified framework to recover emptycategories for Chinese given skeletal parse trees asinput.
In this framework, empty detection is for-mulated as a tagging problem where each wordin the sentence receives a tag indicating whetherthere is an empty category before it.
This ad-vantage of this approach is that it is amenable tolearning-based approaches and can be addressedwith a variety of machine learning algorithms.Our results based on a Maximum Entropy modelshow that given skeletal gold standard parses,empty categories can be recovered with very highaccuracy (close to 90%).
We also report promis-ing results (over 63%).
when automatic parsesproduced by an off-the-shelf parser is used as in-put.Detecting empty categories is only the first steptowards fully reproducing the syntactic represen-tation in the CTB, and the obvious next step is toalso classify these empty categories into differenttypes and wherever applicable, link the empty cat-egories to their antecedent.
This is the line of re-search we intend to pursue in our future work.AcknowledgmentThis work is supported by the National Sci-ence Foundation via Grant No.
0910532 enti-tled ?Richer Representations for Machine Trans-lation?.
All views expressed in this paper arethose of the authors and do not necessarily repre-sent the view of the National Science Foundation.1389ReferencesBlack, E., S. Abney, D. Flickinger, C. Gdaniec, R. Gr-ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,J.
Klavans, M. Liberman, M. Marcus, S. Roukos,B.
Santorini, and T. Strzalkowski.
1991.
A proce-dure for quantitively comparing the syntactic cov-erage of English grammars.
In Proceedings of theDARPA Speech and Natural Language Workshop,pages 306?311.Cahill, Aoife, Michael Burke, Ruth O?Donovan,Josef van Genabith, and Andy Way.
2004.
Long-Distance Dependency Resolution in AutomaticallyAcquired Wide-Coverage PCFG-Based LFG Ap-proximations.
In In Proceedings of the 42nd An-nual Meeting of the Association for ComputationalLinguistics.Campbell, Richard.
2004.
Using linguistic principlesto recover empty categories.
In Proceedings of the42nd Annual Meeting on Association For Computa-tional Linguistics.Charniak, E. 2000.
A Maximum-Entropy-InspiredParser.
In Proceedings of NAACL-2000, pages 132?139, Seattle, Washington.Collins, Michael.
1999.
Head-driven Statistical Mod-els for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania.Converse, Susan.
2006.
Pronominal anaphora resolu-tion for Chinese.
Ph.D. thesis.Dienes, Pe?ter and Amit Dubey.
2003.
Deep syntac-tic processing by combining shallow methods.
InProceedings of the 41st Annual Meeting of the As-sociation for Computational Linguistics, volume 1.Ferra?ndez, Antonio and Jesu?s Peral.
2000.
A compu-tational approach to zero-pronouns in Spanish.
InProceedings of the 38th Annual Meeting on Associ-ation For Computational Linguistics.Gabbard, Ryan, Seth Kulick, and Mitchell Marcus.2006.
Fully parsing the penn treebank.
In Proceed-ings of HLT-NAACL 2006, pages 184?191, NewYork City.Guo, Yuqing, Haifeng Wang, and Josef van Genabith.2007.
Recovering Non-Local Dependencies forChinese.
In Proceedings of the 2007 Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning.Huang, James C.-T. 1989.
Pro drop in Chinese, ageneralized control approach.
In O, Jaeggli andK.
Safir, editors, The Null Subject Parameter.
D.Reidel Dordrecht.Johnson, Mark.
2002.
A simple pattern-matching al-gorithm for recovering empty nodes and their an-tecedents.
In Proceedings of the 40th Annual Meet-ing of the Association for Computational Linguis-tics.Lida, Ryu, Kentaro Inui, and Yuji Matsumoto.
2007.Zero-anaphora resolution by learning rich syntacticpattern features.
ACM Transactions on Asian Lan-guage Information Processing, pages 1?22.Marcus, Mitchell P., Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of english: The penn treebank.
Computa-tional Linguistics, 19(2):313?330.Peng, Jing and Kenji Araki.
2007.
Zero-anaphora res-olution in chinese using maximum entropy.
IEICE- Trans.
Inf.
Syst., E90-D(7):1092?1102.Seki, Kazuhiro, Atsushi Fujii, and Tetsuya Ishikawa.2002.
A probabilistic method for analyzingJapanese anaphora integrating zero pronoun detec-tion and resolution.
In Proceedings of the 19th in-ternational Conference on Computational Linguis-tics, volume 1.Xue, Nianwen and Fei Xia.
2000.
The Bracket-ing Guidelines for Penn Chinese Treebank Project.Technical Report IRCS 00-08, University of Penn-sylvania.Xue, Nianwen, Fei Xia, Fu dong Chiou, and MarthaPalmer.
2005.
The Penn Chinese TreeBank: PhraseStructure Annotation of a Large Corpus.
NaturalLanguage Engineering, 11(2):207?238.Zhao, Shanheng and Hwee Tou Ng.
2007.
Identifi-cation and Resolution of Chinese Zero Pronouns:A Machine Learning Approach.
In Proceedings ofEMNLP-CoNLL Joint Conference, Prague, CzechRepublic.1390
