Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 873?880,Sydney, July 2006. c?2006 Association for Computational LinguisticsAdvances in Discriminative ParsingJoseph Turian and I. Dan Melamed{lastname}@cs.nyu.eduComputer Science DepartmentNew York UniversityNew York, New York 10003AbstractThe present work advances the accu-racy and training speed of discrimina-tive parsing.
Our discriminative parsingmethod has no generative component, yetsurpasses a generative baseline on con-stituent parsing, and does so with mini-mal linguistic cleverness.
Our model canincorporate arbitrary features of the in-put and parse state, and performs fea-ture selection incrementally over an ex-ponential feature space during training.We demonstrate the flexibility of our ap-proach by testing it with several pars-ing strategies and various feature sets.Our implementation is freely available at:http://nlp.cs.nyu.edu/parser/.1 IntroductionDiscriminative machine learning methods haveimproved accuracy on many NLP tasks, includingPOS-tagging, shallow parsing, relation extraction,and machine translation.
Some advances have alsobeen made on full syntactic constituent parsing.Successful discriminative parsers have relied ongenerative models to reduce training time andraise accuracy above generative baselines (Collins& Roark, 2004; Henderson, 2004; Taskar et al,2004).
However, relying on information from agenerative model might prevent these approachesfrom realizing the accuracy gains achieved by dis-criminative methods on other NLP tasks.
Anotherproblem is training speed: Discriminative parsersare notoriously slow to train.In the present work, we make progress towardsovercoming these obstacles.
We propose a flexi-ble, end-to-end discriminative method for trainingparsers, demonstrating techniques that might alsobe useful for other structured prediction problems.The proposed method does model selection with-out ad-hoc smoothing or frequency-based featurecutoffs.
It requires no heuristics or human effortto optimize the single important hyper-parameter.The training regime can use all available informa-tion from the entire parse history.
The learning al-gorithm projects the hand-provided features into acompound feature space and performs incremen-tal feature selection over this large feature space.The resulting parser achieves higher accuracy thana generative baseline, despite not using a genera-tive model as a feature.Section 2 describes the parsing algorithm.
Sec-tion 3 presents the learning method.
Section 4presents experiments with discriminative parsersbuilt using these methods.
Section 5 compares ourapproach to related work.2 Parsing AlgorithmThe following terms will help to explain our work.A span is a range over contiguous words in the in-put.
Spans cross if they overlap but neither con-tains the other.
An item is a (span, label) pair.
Astate is a partial parse, i.e.
a set of items, noneof whose spans may cross.
A parse inference isa (state, item) pair, i.e.
a state and an item to beadded to it.
The frontier of a state consists of theitems with no parents yet.
The children of a candi-date inference are the frontier items below the itemto be inferred, and the head of a candidate infer-ence is the child item chosen by English head rules(Collins, 1999, pp.
238?240).
A parse path is asequence of parse inferences.
For some input sen-tence and training parse tree, a state is correct ifthe parser can infer zero or more additional itemsto obtain the training parse tree, and an inference873is correct if it leads to a correct state.Given input sentence s, the parser searches forparse p?
out of the possible parses P(s):p?
= arg minp?P(s)C?
(p) (1)where C?
(p) is the cost of parse p under model ?:C?
(p) =?i?pc?
(i) (2)Section 3.1 describes how to compute c?(i).
Be-cause c?
(i) ?
R+, the cost of a partial parse mono-tonically increases as we add items to it.The parsing algorithm considers a successionof states.
The initial state contains terminal items,whose labels are the POS tags given by the taggerof Ratnaparkhi (1996).
Each time we pop a statefrom the agenda, c?
computes the costs for thecandidate bottom-up inferences generated fromthat state.
Each candidate inference results in asuccessor state to be placed on the agenda.The cost function c?
can consider arbitraryproperties of the input and parse state.
We are notaware of any tractable solution to Equation 1, suchas dynamic programming.
Therefore, the parserfinds p?
using a variant of uniform-cost search.The parser implements the search using an agendathat stores entire states instead of single items.Each time a state is popped from the agenda, theparser uses depth-first search starting from thestate that was popped until it (greedily) finds acomplete parse.
In preliminary experiments, thissearch strategy was faster than standard uniform-cost search (Russell & Norvig, 1995).3 Training Method3.1 General SettingOur training set I consists of candidate inferencesfrom the parse trees in the training data.
Fromeach training inference i ?
I we generate the tuple?X(i), y(i), b(i)?.
X(i) is a feature vector describingi, with each element in {0, 1}.
We will use X f (i) torefer to the element of X(i) that pertains to featuref .
y(i) = +1 if i is correct, and y(i) = ?1 if not.Some training examples might be more importantthan others, so each is given a bias b(i) ?
R+, asdetailed in Section 3.3.The goal during training is to induce a hypothe-sis h?
(i), which is a real-valued inference scoringfunction.
In the present work, h?
is a linear modelparameterized by a real vector ?, which has oneentry for each feature f :h?
(i) = ?
?
X(i) =?f?
f ?
X f (i) (3)The sign of h?
(i) predicts the y-value of i and themagnitude gives the confidence in this prediction.The training procedure optimizes ?
to minimizethe expected risk R?
over training set I. R?
is theobjective function, a combination of loss functionL?
and regularization term ??:R?
(I) = L?
(I) + ??
(4)The loss of the inference set decomposes into theloss of individual inferences:L?
(I) =?i?Il?
(i) (5)In principle, l?
can be any loss function, but in thepresent work we use the log-loss (Collins et al,2002):l?
(i) = b(i) ?
ln(1 + exp(???
(i))) (6)and ??
(i) is the margin of inference i:??
(i) = y(i) ?
h?
(i) (7)Inference cost c?
(i) in Equation 2 is l?
(i) com-puted using y(i) = +1 and b(i) = 1, i.e.:c?
(i) = ln(1 + exp(?h?
(i))) (8)??
in Equation 4 is a regularizer, which penal-izes complex models to reduce overfitting and gen-eralization error.
We use the `1 penalty:??
=?f?
?
|?
f | (9)where ?
is a parameter that controls the strengthof the regularizer.
This choice of objective R?
ismotivated by Ng (2004), who suggests that, givena learning setting where the number of irrelevantfeatures is exponential in the number of train-ing examples, we can nonetheless learn effectivelyby building decision trees to minimize the `1-regularized log-loss.
On the other hand, Ng (2004)suggests that most of the learning algorithms com-monly used by discriminative parsers will overfitwhen exponentially many irrelevant features arepresent.1Learning over an exponential feature space isthe very setting we have in mind.
A priori, we de-fine only a set A of simple atomic features (given1including the following learning algorithms:?
unregularized logistic regression?
logistic regression with an `2 penalty (i.e.
a Gaussian prior)?
SVMs using most kernels?
multilayer neural nets trained by backpropagation?
the perceptron algorithm874in Section 4).
The learner then induces compoundfeatures, each of which is a conjunction of possi-bly negated atomic features.
Each atomic featurecan have one of three values (yes/no/don?t care),so the size of the compound feature space is 3|A|,exponential in the number of atomic features.
Itwas also exponential in the number of training ex-amples in our experiments (|A| ?
|I|).3.2 Boosting `1-Regularized Decision TreesWe use an ensemble of confidence-rated decisiontrees (Schapire & Singer, 1999) to represent h?.2The path from the root to each node n in a decisiontree corresponds to some compound feature f , andwe write ?
(n) = f .
To score an inference i usinga decision tree, we percolate the inference?s fea-tures X(i) down to a leaf n and return confidence??(n).
An inference i percolates down to node n iffX?
(n) = 1.
Each leaf node n keeps track of the pa-rameter value ??
(n).3 The score h?
(i) given to aninference i by the whole ensemble is the sum of theconfidences returned by the trees in the ensemble.Listing 1 Outline of training algorithm.1: procedure T????
(I)2: ensemble?
?3: ??
?4: while dev set accuracy is increasing do5: t ?
tree with one (root) node6: while the root node cannot be split do7: decay `1 parameter ?8: while some leaf in t can be split do9: split the leaf to maximize gain10: percolate every i ?
I to a leaf node11: for each leaf n in t do12: update ??
(n) to minimize R?13: append t to ensembleListing 1 presents our training algorithm.
Atthe beginning of training, the ensemble is empty,?
= 0, and the `1 parameter ?
is set to?
(Steps 1.2and 1.3).
We train until the objective cannot be fur-ther reduced for the current choice of ?.
We thendetermine the accuracy of the parser on a held-outdevelopment set using the previous ?
value (be-fore it was decreased), and stop training when this2Turian and Melamed (2005) reported that decision treesapplied to parsing have higher accuracy and training speedthan decision stumps, so we build full decision trees ratherthan stumps.3Any given compound feature can appear in more than onetree, but each leaf node has a distinct confidence value.
Forsimplicity, we ignore this possibility in our discussion.accuracy reaches a plateau (Step 1.4).
Otherwise,we relax the regularization penalty by decreasing?
(Steps 1.6 and 1.7) and continue training.
In thisway, instead of choosing the best ?
heuristically,we can optimize it during a single training run(Turian & Melamed, 2005).Each training iteration (Steps 1.5?1.13) has sev-eral steps.
First, we choose some compound fea-tures that have high magnitude gradient with re-spect to the objective function.
We do this bybuilding a new decision tree, whose leaves rep-resent the chosen compound features (Steps 1.5?1.9).
Second, we confidence-rate each leaf to min-imize the objective over the examples that per-colate down to that leaf (Steps 1.10?1.12).
Fi-nally, we append the decision tree to the ensem-ble and update parameter vector ?
accordingly(Step 1.13).
In this manner, compound feature se-lection is performed incrementally during train-ing, as opposed to a priori.Our strategy minimizing the objective R?
(I)(Equation 4) is a variant of steepest descent(Perkins et al, 2003).
To compute the gradient ofthe unpenalized loss L?
with respect to the param-eter ?
f of feature f , we have:?L?(I)??
f=?i?I?l?(i)???
(i) ????(i)??
f(10)where:???(i)??
f= y(i) ?
X f (i) (11)Using Equation 6, we define the weight of an ex-ample i under the current model as the rate atwhich loss decreases as the margin of i increases:w?
(i) = ?
?l?(i)???
(i) = b(i) ?11 + exp(??
(i)) (12)Recall that X f (i) is either 0 or 1.
Combining Equa-tions 10?12 gives:?L?(I)??
f= ?
?i?IX f (i)=1y(i) ?
w?
(i) (13)We define the gain of feature f as:G?
(I; f ) = max(0,???????L?(I)??
f???????
?
)(14)Equation 14 has this form because the gradient ofthe penalty term is undefined at ?
f = 0.
This dis-continuity is why `1 regularization tends to pro-duce sparse models.
If G?
(I; f ) = 0, then the ob-jective R?
(I) is at its minimum with respect to pa-rameter ?
f .
Otherwise, G?
(I; f ) is the magnitude875of the gradient of the objective as we adjust ?
f inthe appropriate direction.To build each decision tree, we begin with a rootnode.
The root node corresponds to a dummy ?al-ways true?
feature.
We recursively split nodes bychoosing a splitting feature that will allow us to in-crease the gain.
Node n with corresponding com-pound feature ?
(n) = f can be split by atomic fea-ture a if:G?
(I; f ?
a) + G?
(I; f ?
?a) > G?
(I; f ) (15)If no atomic feature satisfies the splitting crite-rion in Equation 15, then n becomes a leaf nodeof the decision tree and ??
(n) becomes one of thevalues to be optimized during the parameter up-date step.
Otherwise, we choose atomic feature a?to split node n:a?
= arg maxa?A(G?
(I; f ?
a) + G?
(I; f ?
?a))(16)This split creates child nodes n1 and n2, with?
(n1) = f ?
a?
and ?
(n2) = f ?
?a?.Parameter update is done sequentially on onlythe most recently added compound features, whichcorrespond to the leaves of the new decision tree.After the entire tree is built, we percolate exam-ples down to their appropriate leaf nodes.
We thenchoose for each leaf node n the parameter ??
(n)that minimizes the objective over the examples inthat leaf.
A convenient property of decision treesis that the leaves?
compound features are mutuallyexclusive.
Their parameters can be directly opti-mized independently of each other using a linesearch over the objective.3.3 The Training SetWe choose a single correct path from each trainingparse tree, and the training examples correspond toall candidate inferences considered in every statealong this path.4 In the deterministic setting thereis only one correct path, so example generationis identical to that of Sagae and Lavie (2005).
Ifparsing proceeds non-deterministically then theremight be multiple paths that lead to the same finalparse, so we choose one randomly.
This methodof generating training examples does not require aworking parser and can be run prior to any train-ing.
The disadvantage of this approach is that itminimizes the error of the parser at correct statesonly.
It does not account for compounded error or4Nearly all of the examples generated are negative (y = ?1).teach the parser to recover from mistakes grace-fully.Turian and Melamed (2005) observed that uni-form example biases b(i) produced lower accuracyas training progressed, because the induced clas-sifiers minimized the error per example.
To min-imize the error per state, we assign every train-ing state equal value and share half the value uni-formly among the negative examples for the ex-amples generated from that state and the other halfuniformly among the positive examples.We parallelize training by inducing 26 labelclassifiers (one for each non-terminal label in thePenn Treebank).
Parallelization might not uni-formly reduce training time because different la-bel classifiers train at different rates.
However, par-allelization uniformly reduces memory usage be-cause each label classifier trains only on inferenceswhose consequent item has that label.4 ExperimentsDiscriminative parsers are notoriously slow totrain.
For example, Taskar et al (2004) took sev-eral months to train on the ?
15 word sentencesin the English Penn Treebank (Dan Klein, p.c.
).The present work makes progress towards fasterdiscriminative parser training: our slowest classi-fier took fewer than 5 days to train.
Even so, itwould have taken much longer to train on the en-tire treebank.
We follow Taskar et al (2004) intraining and testing on ?
15 word sentences inthe English Penn Treebank (Taylor et al, 2003).We used sections 02?21 for training, section 22for development, and section 23 for testing, pre-processed as per Table 1.
We evaluated our parserusing the standard PARSEVAL measures (Black etal., 1991): labelled precision, labelled recall, andlabelled F-measure (Prec., Rec., and F1, respec-tively), which are based on the number of non-terminal items in the parser?s output that matchthose in the gold-standard parse.5As mentioned in Section 2, items are inferredbottom-up and the parser cannot infer any itemthat crosses an item already in the state.
Althoughthere are O(n2) possible (span, label) pairs over afrontier containing n items, we reduce this to the?
5 ?
n inferences that have at most five children.65The correctness of a stratified shuffling test has been calledinto question (Michael Collins, p.c.
), so we are not aware ofany valid significance tests for observed differences in PAR-SEVAL scores.6Only 0.57% of non-terminals in the preprocessed develop-876Table 1 Steps for preprocessing the data.
Starred steps are performed only when parse trees are availablein the data (e.g.
not on test data).1.
* Strip functional tags and trace indices, and remove traces.2.
* Convert PRT to ADVP.
(This convention was established by Magerman (1995).)3.
Remove quotation marks (i.e.
terminal items tagged ??
or ??).
(Bikel, 2004)4.
* Raise punctuation.
(Bikel, 2004)5.
Remove outermost punctuation.a6.
* Remove unary projections to self (i.e.
duplicate items with the same span and label).7.
POS tag the text using the tagger of Ratnaparkhi (1996).8.
Lowercase headwords.aAs pointed out by an anonymous reviewer of Collins (2003), removing outermost punctuation might discard useful infor-mation.
Collins and Roark (2004) saw a LFMS improvement of 0.8% over their baseline discriminative parser after addingpunctuation features, one of which encoded the sentence-final punctuation.To ensure the parser does not enter an infinite loop,no two items in a state can have both the samespan and the same label.
Given these restrictionson candidate inferences, there were roughly 40million training examples generated in the train-ing set.
These were partitioned among the 26 con-stituent label classifiers.
Building a decision tree(Steps 1.5?1.9 in Listing 1) using the entire ex-ample set I can be very expensive.
We estimateloss gradients (Equation 13) using a sample of theinference set, which gives a 100-fold increase intraining speed (Turian & Melamed, 2006).Our atomic feature set A contains 300K fea-tures, each of the form ?is there an item in groupJ whose label/headword/headtag/headtagclass is?X??
?.7 Possible values of ?X?
for each predicateare collected from the training data.
For 1 ?
n ?
3,possible values for J are:?
the first/last n child items?
the first n left/right context items?
the n children items left/right of the head?
the head item.The left and right context items are the frontieritems to the left and right of the children of thecandidate inference, respectively.4.1 Different Parsing StrategiesTo demonstrate the flexibility of our learn-ing procedure, we trained three differentparsers: left-to-right (l2r), right-to-left (r2l),ment set have more than five children.7The predicate headtagclass is a supertype of the headtag.Given our compound features, these are not strictly necessary,but they accelerate training.
An example is ?proper noun,?which contains the POS tags given to singular and pluralproper nouns.
Space constraints prevent enumeration of theheadtagclasses, which are instead provided at the URL givenin the abstract.Table 2 Results on the development set, trainingand testing using only ?
15 word sentences.active?
features % Rec.
% Prec.
F1l2r 0.040 11.9K 89.86 89.63 89.74b.u.
0.020 13.7K 89.92 89.84 89.88r2l 0.014 14.0K 90.66 89.81 90.23and non-deterministic bottom-up (b.u.).
Thenon-deterministic parser was allowed to chooseany bottom-up inference.
The other two parserswere deterministic: bottom-up inferences hadto be performed strictly left-to-right or right-to-left, respectively.
We stopped training wheneach parser had 15K active features.
Figure 1shows the accuracy of the different runs over thedevelopment set as training progressed.
Table 2gives the PARSEVAL scores of these parsers attheir optimal `1 penalty setting.
We found thatthe perplexity of the r2l model was low so that,in 85% of the sentences, its greedy parse was theoptimal one.
The l2r parser does poorly becauseits decisions were more difficult than those of theother parsers.
If it inferred far-right items, it wasmore likely to prevent correct subsequent infer-ences that were to the left.
But if it inferred far-leftitems, then it went against the right-branchingtendency of English sentences.
The left-to-rightparser would likely improve if we were to use aleft-corner transform (Collins & Roark, 2004).Parsers in the literature typically choose somelocal threshold on the amount of search, such asa maximum beam width.
With an accurate scor-ing function, restricting the search space usinga fixed beam width might be unnecessary.
In-stead, we imposed a global threshold on explo-ration of the search space.
Specifically, if the877Figure 1 F1 scores on the development set of thePenn Treebank, using only ?
15 word sentences.The x-axis shows the number of non-zero param-eters in each parser, summed over all classifiers.85%86%87%88%89%90%15K10K5K2.5K1.5KDevel.
F-measuretotal number of non-zero parametersright-to-leftleft-to-rightbottom upparser has found some complete parse and hasexplored at least 100K states (i.e.
scored at least100K inferences), search stopped prematurely andthe parser would return the (possibly sub-optimal)current best complete parse.
The l2r and r2lparsers never exceeded this threshold, and al-ways found the optimal complete parse.
However,the non-deterministic bottom-up parser?s searchwas cut-short in 28% of the sentences.
The non-deterministic parser can reach each parse statethrough many different paths, so it searches alarger space than a deterministic parser, with moreredundancy.To gain a better understanding of the weak-nesses of our parser, we examined a sample of50 development sentences that the r2l parser didnot get entirely correct.
Roughly half the errorswere due to noise and genuine ambiguity.
The re-maining errors fell into three types, occurring withroughly the same frequency:?
ADVPs and ADJPs The r2l parser had F1 =81.1% on ADVPs, and F1 = 71.3% on ADJPs.
An-notation of ADJP and ADVP in the PTB is inconsis-tent, particularly for unary projections.?
POS Tagging Errors Many of the parser?s er-rors were due to incorrect POS tags.
In future workwe will integrate POS-tagging as inferences of theparser, allowing it to entertain competing hypothe-ses about the correct tagging.?
Bilexical dependencies Although compoundfeatures exist to detect affinities between words,the parser had difficulties with bilexical depen-dency decisions that were unobserved in the train-ing data.
The classifier would need more trainingdata to learn these affinities.Figure 2 F1 scores of right-to-left parsers with dif-ferent atomic feature sets on the development setof the Penn Treebank, using only ?
15 word sen-tences.85%86%87%88%89%90%91%30K20K10K5K2.5K1.5KDevel.
F-measuretotal number of non-zero parameterskitchen sinkbaseline4.2 More Atomic FeaturesWe compared our right-to-left parser with thebaseline set of atomic features to one with a farricher atomic feature set, including unboundedcontext features, length features, and features ofthe terminal items.
This ?kitchen sink?
parsermerely has access to many more item groups J, de-scribed in Table 3.
All features are all of the formgiven earlier, except for length features (Eisner &Smith, 2005).
Length features compute the size ofone of the groups of items in the indented list inTable 3.
The feature determines if this length isequal to/greater than to n, 0 ?
n ?
15.
The kitchensink parser had 1.1 million atomic features, 3.7times the number available in the baseline.
In fu-ture work, we plan to try linguistically more so-phisticated features (Charniak & Johnson, 2005)as well as sub-tree features (Bod, 2003; Kudo etal., 2005).Figure 2 shows the accuracy of the right-to-left parsers with different atomic feature sets overthe development set as training progressed.
Eventhough the baseline training made progress morequickly than the kitchen sink, the kitchen sink?s F1surpassed the baseline?s F1 early in training, and at6.3K active parameters it achieved a developmentset F1 of 90.55%.4.3 Test Set ResultsTo situate our results in the literature, we compareour results to those reported by Taskar et al (2004)and Turian and Melamed (2005) for their dis-criminative parsers, which were also trained andtested on ?
15 word sentences.
We also compareour parser to a representative non-discriminative878Table 3 Item groups available in the kitchen sink run.?
the first/last n child items, 1 ?
n ?
4?
the first n left/right context items, 1 ?
n ?
4?
the n children items left/right of the head, 1 ?
n ?
4?
the nth frontier item left/right of the leftmost/head/rightmost child item, 1 ?
n ?
3?
the nth terminal item left/right of the leftmost/head/rightmost terminal item dominated by the itembeing inferred, 1 ?
n ?
3?
the leftmost/head/rightmost child item of the leftmost/head/rightmost child item?
the following groups of frontier items:?
all items?
left/right context items?
non-leftmost/non-head/non-rightmost child items?
child items left/right of the head item, inclusive/exclusive?
the terminal items dominated by one of the item groups in the indented list aboveTable 4 Results of parsers on the test set, trainingand testing using only ?
15 word sentences.% Rec.
% Prec.
F1Turian and Melamed (2005) 86.47 87.80 87.13Bikel (2004) 87.85 88.75 88.30Taskar et al (2004) 89.10 89.14 89.12kitchen sink 89.26 89.55 89.40parser (Bikel, 2004)8, the only one that we wereable to train and test under exactly the same ex-perimental conditions (including the use of POStags from the tagger of Ratnaparkhi (1996)).
Ta-ble 4 shows the PARSEVAL results of these fourparsers on the test set.5 Comparison with Related WorkOur parsing approach is based upon a single end-to-end discriminative learning machine.
Collinsand Roark (2004) and Taskar et al (2004) beatthe generative baseline only after using the stan-dard trick of using the output from a generativemodel as a feature.
Henderson (2004) finds thatdiscriminative training was too slow, and reportsaccuracy higher than generative models by dis-criminatively reranking the output of his genera-tive model.
Unlike these state-of-the-art discrimi-native parsers, our method does not (yet) use anyinformation from a generative model to improvetraining speed or accuracy.
As far as we know, wepresent the first discriminative parser that does notuse information from a generative model to beat a8Bikel (2004) is a ?clean room?
reimplementation of theCollins (1999) model with comparable accuracy.generative baseline (the Collins model).The main limitation of our work is that we cando training reasonably quickly only on short sen-tences because a sentence with n words gener-ates O(n2) training inferences in total.
Althoughgenerating training examples in advance with-out a working parser (Turian & Melamed, 2005)is much faster than using inference (Collins &Roark, 2004; Henderson, 2004; Taskar et al,2004), our training time can probably be de-creased further by choosing a parsing strategy witha lower branching factor.
Like our work, Ratna-parkhi (1999) and Sagae and Lavie (2005) gener-ate examples off-line, but their parsing strategiesare essentially shift-reduce so each sentence gen-erates only O(n) training examples.An advantage of our approach is its flexibility.As our experiments showed, it is quite simple tosubstitute in different parsing strategies.
Althoughwe used very little linguistic information (the headrules and the POS tag classes), our model couldalso start with more sophisticated task-specificfeatures in its atomic feature set.
Atomic featuresthat access arbitrary information are representeddirectly without the need for an induced interme-diate representation (cf.
Henderson, 2004).Other papers (Clark & Curran, 2004; Kaplanet al, 2004, e.g.)
have applied log-linear mod-els to parsing.
These works are based upon con-ditional models, which include a normalizationterm.
However, our loss function forgoes normal-ization, which means that it is easily decomposedinto the loss of individual inferences (Equation 5).879Decomposition of the loss allows the objective tobe optimized in parallel.
This might be an ad-vantage for larger structured prediction problemswhere there are more opportunities for paralleliza-tion, for example machine translation.The only important hyper-parameter in ourmethod is the `1 penalty factor.
We optimize itas part of the training process, choosing the valuethat maximizes accuracy on a held-out develop-ment set.
This technique stands in contrast to moread-hoc methods for choosing hyper-parameters,which may require prior knowledge or additionalexperimentation.6 ConclusionOur work has made advances in both accuracyand training speed of discriminative parsing.
Asfar as we know, we present the first discriminativeparser that surpasses a generative baseline on con-stituent parsing without using a generative compo-nent, and it does so with minimal linguistic clev-erness.
Our approach performs feature selectionincrementally over an exponential feature spaceduring training.
Our experiments suggest that thelearning algorithm is overfitting-resistant, as hy-pothesized by Ng (2004).
If this is the case, itwould reduce the effort required for feature engi-neering.
An engineer can merely design a set ofatomic features whose powerset contains the req-uisite information.
Then, the learning algorithmcan perform feature selection over the compoundfeature space, avoiding irrelevant compound fea-tures.In future work, we shall make some standardimprovements.
Our parser should infer its ownPOS tags to improve accuracy.
A shift-reduceparsing strategy will generate fewer training in-ferences, and might lead to shorter training times.Lastly, we plan to give the model linguisticallymore sophisticated features.
We also hope to ap-ply the model to other structured prediction tasks,such as syntax-driven machine translation.AcknowledgmentsThe authors would like to thank Chris Pike,Cynthia Rudin, and Ben Wellington, as well asthe anonymous reviewers, for their helpful com-ments and constructive criticism.
This researchwas sponsored by NSF grants #0238406 and#0415933.ReferencesBikel, D. M. (2004).
Intricacies of Collins?
parsing model.Computational Linguistics, 30(4).Black, E., Abney, S., Flickenger, D., Gdaniec, C., Grishman,R., Harrison, P., et al (1991).
A procedure for quantitativelycomparing the syntactic coverage of English grammars.
InSpeech and Natural Language.Bod, R. (2003).
An efficient implementation of a new DOPmodel.
In EACL.Charniak, E., & Johnson, M. (2005).
Coarse-to-fine n-bestparsing and MaxEnt discriminative reranking.
In ACL.Clark, S., & Curran, J. R. (2004).
Parsing the WSJ usingCCG and log-linear models.
In ACL.Collins, M. (1999).
Head-driven statistical models for natu-ral language parsing.
Doctoral dissertation.Collins, M. (2003).
Head-driven statistical models for naturallanguage parsing.
Computational Linguistics, 29(4).Collins, M., & Roark, B.
(2004).
Incremental parsing withthe perceptron algorithm.
In ACL.Collins, M., Schapire, R. E., & Singer, Y.
(2002).
Logis-tic regression, AdaBoost and Bregman distances.
MachineLearning, 48(1-3).Eisner, J., & Smith, N. A.
(2005).
Parsing with soft and hardconstraints on dependency length.
In IWPT.Henderson, J.
(2004).
Discriminative training of a neuralnetwork statistical parser.
In ACL.Kaplan, R. M., Riezler, S., King, T. H., Maxwell, III, J. T.,Vasserman, A., & Crouch, R. (2004).
Speed and accuracyin shallow and deep stochastic parsing.
In HLT/NAACL.Kudo, T., Suzuki, J., & Isozaki, H. (2005).
Boosting-basedparse reranking with subtree features.
In ACL.Magerman, D. M. (1995).
Statistical decision-tree modelsfor parsing.
In ACL.Ng, A. Y.
(2004).
Feature selection, `1 vs. `2 regularization,and rotational invariance.
In ICML.Perkins, S., Lacker, K., & Theiler, J.
(2003).
Grafting: Fast,incremental feature selection by gradient descent in func-tion space.
Journal of Machine Learning Research, 3.Ratnaparkhi, A.
(1996).
A maximum entropy part-of-speechtagger.
In EMNLP.Ratnaparkhi, A.
(1999).
Learning to parse natural languagewith maximum entropy models.
Machine Learning, 34(1-3).Russell, S., & Norvig, P. (1995).
Artificial intelligence: Amodern approach.Sagae, K., & Lavie, A.
(2005).
A classifier-based parser withlinear run-time complexity.
In IWPT.Schapire, R. E., & Singer, Y.
(1999).
Improved boosting us-ing confidence-rated predictions.
Machine Learning, 37(3).Taskar, B., Klein, D., Collins, M., Koller, D., & Manning, C.(2004).
Max-margin parsing.
In EMNLP.Taylor, A., Marcus, M., & Santorini, B.
(2003).
The PennTreebank: an overview.
In A. Abeille?
(Ed.
), Treebanks:Building and using parsed corpora (chap.
1).Turian, J., & Melamed, I. D. (2005).
Constituent parsing byclassification.
In IWPT.Turian, J., & Melamed, I. D. (2006).
Computational chal-lenges in parsing by classification.
In HLT-NAACL work-shop on computationally hard problems and joint inferencein speech and language processing.880
