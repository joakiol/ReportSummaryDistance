Proceedings of the ACL Interactive Poster and Demonstration Sessions,pages 93?96, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsThe Wild Thing!Kenneth Church Bo ThiessonMicrosoft ResearchRedmond, WA, 98052, USA{church, thiesson}@microsoft.comAbstractSuppose you are on a mobile device withno keyboard (e.g., a cell or PDA).
Howcan you enter text quickly?
T9?
Graffiti?This demo will show how language model-ing can be used to speed up data entry, bothin the mobile context, as well as the desk-top.
The Wild Thing encourages users touse wildcards (*).
A language model findsthe k-best expansions.
Users quickly figureout when they can get away with wild-cards.
General purpose trigram languagemodels are effective for the general case(unrestricted text), but there are importantspecial cases like searching over popularweb queries, where more restricted lan-guage models are even more effective.1 Motivation: Phone AppCell phones and PDAs are everywhere.
Users lovemobility.
What are people doing with their phone?You?d think they would be talking on their phones,but a lot of people are typing.
It is considered rudeto talk on a cell in certain public places, especiallyin Europe and Asia.
SMS text messaging enablespeople to communicate, even when they can?t talk.It is bizarre that people are typing on theirphones given how painful it is.
?Talking on thephone?
is a collocation, but ?typing on the phone?is not.
Slate (slate.msn.com/id/2111773) recentlyran a story titled: ?A Phone You Can ActuallyType On?
with the lead:?If you've tried to zap someone a text mes-sage recently, you've probably discoveredthe huge drawback of typing on your cellphone.
Unless you're one of those cyborgScandinavian teenagers who was born witha Nokia in his hand, pecking out even asimple message is a thumb-twisting chore.
?There are great hopes that speech recognitionwill someday make it unnecessary to type on yourphone (for SMS or any other app), but speech rec-ognition won?t help with the rudeness issue.
Ifpeople are typing because they can?t talk, thenspeech recognition is not an option.
Fortunately,the speech community has developed powerfullanguage modeling techniques that can help evenwhen speech is not an option.2 K-Best String MatchingSuppose we want to search for MSN using a cellphone.
A standard approach would be to type 6<pause> 777 <pause> 66, where 6  M, 777  Sand 66  N.  (The pauses are necessary for disam-biguation.)
Kids these days are pretty good at typ-ing this way, but there has to be a better solution.T9 (www.t9.com) is an interesting alternative.The user types 676 (for MSN).
The system uses a(unigram) language model to find the k-bestmatches.
The user selects MSN from this list.Some users love T9, and some don?t.The input, 676, can be thought of as short handfor the regular expression:/^[6MNOmno][7PRSprs][6MNOmno]$/using standard Unix notation.
Regular expressionsbecome much more interesting when we considerwildcards.
So-called ?word wheeling?
can bethought of as the special case where we add awildcard to the end of whatever the user types.Thus, if the user types 676 (for MSN), we wouldfind the k-best matches for:/^[6MNOmno][7PRSprs][6MNOmno].
*/93See Google Suggests1 for a nice example ofword wheeling.
Google Suggests makes it easy tofind popular web queries (in the standard non-mobile desktop context).
The user types a prefix.After each character, the system produces a list ofthe k most popular web queries that start with thespecified prefix.Word wheeling not only helps when you knowwhat you want to say, but it also helps when youdon?t.
Users can?t spell.
And things get stuck onthe tip of their tongue.
Some users are just brows-ing.
They aren?t looking for anything in particular,but they?d like to know what others are looking at.The popular query application is relatively easyin terms of entropy.
About 19 bits are needed tospecify one of the 7 million most popular web que-ries.
That is, if we assign each web query a prob-ability based on query logs collected at msn.com,then we can estimate entropy, H, and discover thatH?19.
(About 23 bits would be needed if thesepages were equally likely, but they aren?t.)
It isoften said that the average query is between twoand three words long, but H is more meaningfulthan query length.General purpose trigram language models areeffective for the general case (unrestricted text),but there are important special cases like popularweb queries, where more restricted language mod-els are even more effective than trigram models.Our language model for web queries is simply alist of queries and their probabilities.
We considerqueries to be a finite language, unlike unrestrictedtext where the trigram language model allows sen-tences to be arbitrarily long.Let?s consider another example.
The MSNquery was too easy.
Suppose we want to findCondoleezza Rice, but we can?t spell her name.And even if we could, we wouldn?t want to.
Typ-ing on a phone isn?t fun.We suggest spelling Condoleezza as 2*, where2  [ABCabc2] and * is the wildcard.
We thentype ?#?
for space.
Rice is easy to spell: 7423.Thus, the user types, 2*#7423, and the systemsearches over the MSN query log to produce a listof k-best (most popular) matches (k defaults to 10):1.
Anne Rice2.
Book of Shadows3.
Chris Rice4.
Condoleezza Rice1 http://www.google.com/webhp?complete=15.
Ann Rice?8.
Condoleeza RiceThe letters matching constants in the regular ex-pression are underlined.
The other letters matchwildcards.
(An implicit wildcard is appended tothe end of the input string.
)Wildcards are very powerful.
Strings withwildcards are more expressive than prefix match-ing (word wheeling).
As mentioned above, itshould take just 19 bits on average to specify oneof the 7 million most popular queries.
The query2*#7423 contains 7 characters in an 12-characteralphabet (2-9  [A-Za-z2-9] in the obvious way,except that 0  [QZqz0]; #  space; * is wild).
7characters in a 12 character alphabet is 7 log212 =25 bits.
If the input notation were optimal (whichit isn?t), it shouldn?t be necessary to type muchmore than this on average to specify one of the 7million most popular queries.Alphabetic ordering causes bizarre behavior.Yellow Pages are full of company names startingwith A, AA, AAA, etc..
If prefix matching tools likeGoogle Suggests take off, then it is just a matter oftime before companies start to go after valuableprefixes: mail, maps, etc.
Wildcards can help soci-ety avoid that non-sense.
If you want to find a topmail site, you can type, ?*mail?
and you?ll find:Gmail, Hotmail, Yahoo mail, etc..3 Collaboration & PersonalizationUsers quickly learn when they can get away withwildcards.
Typing therefore becomes a collabora-tive exercise, much like Palm?s approach to hand-writing recognition.
Recognition is hard.
Ratherthan trying to solve the general case, Palm encour-ages users to work with the system to write in away that is easier to recognize (Graffiti).
The sys-tem isn?t trying to solve the AI problem by itself,but rather there is a man-machine collaborationwhere both parties work together as a team.Collaboration is even more powerful in theweb context.
Users issue lots of queries, making itclear what?s hot (and what?s not).
The system con-structs a language model based on these queries todirect users toward good stuff.
More and moreusers will then go there, causing the hot query tomove up in the language model.
In this way, col-laboration can be viewed as a positive feedback94loop.
There is a strong herd instinct; all partiesbenefit from the follow-the-pack collaboration.In addition, users want personalization.
Whentyping names of our friends and family, technicalterms, etc., we should be able to get away withmore wildcards than other users would.
There areobvious opportunities for personalizing the lan-guage model by integrating the language modelwith a desktop search index (Dumais et al 2003).4 Modes, Language Models and AppsThe Wild Thing demo has a switch for turning onand off phone mode to determine whether inputcomes from a phone keypad or a standard key-board.
Both with and without phone mode, thesystem uses a language model to find the k-bestexpansions of the wildcards.The demo contains a number of different lan-guage models, including a number of standard tri-gram language models.
Some of the languagemodels were trained on large quantities (6 Billionwords) of English.
Others were trained on largesamples of Spanish and German.
Still others weretrained on small sub-domains (such as ATIS,available from www.ldc.upenn.edu).
The demoalso contains two special purpose language modelsfor searching popular web queries, and popularweb domains.Different language models are different.
Witha trigram language model trained on general Eng-lish (containing large amounts of newswire col-lected over the last decade),pres* rea* *d y* t* it is v*imp*  President Reagan saidyesterday that it is very impor-tantWith a Spanish Language Model,pres* rea*  presidente ReaganIn the ATIS domain,pres* rea*  <UNK> <UNK>The tool can also be used to debug languagemodels.
It turns out that some French slipped intothe English training corpus.
Consequently, theEnglish language model expanded the * in en * deto some common French words that happen to beEnglish words as well: raison, circulation, oeuvre,place, as well as <OOV>.
After discovering this,we discovered quite a few more anomalies in thetraining corpus such as headers from the AP news.There may also be ESL (English as a SecondLanguage) applications for the tool.
Many usershave a stronger active vocabulary than passive vo-cabulary.
If the user has a word stuck on the tip oftheir tongue,  they can type a suggestive contextwith appropriate wildcards and there is a goodchance the system will propose the word the user islooking for.Similar tricks are useful in monolingual con-texts.
Suppose you aren?t sure how to spell a ce-lebrity?s name.
If you provide a suggestivecontext, the language model is likely to get it right:ron* r*g*n  Ronald Reagandon* r*g*n  Donald Reganc* rice  Condoleezza RiceTo summarize, wildcards are helpful in quite afew apps:?
No keyboard: cell phone, PDA, Tablet PC.?
Speed matters: instant messaging, email.?
Spelling/ESL/tip of the tongue.?
Browsing: direct users toward hot stuff.5 Indexing and CompressionThe k-best string matching problem raises a num-ber of interesting technical challenges.
We havetwo types of language models: trigram languagemodels and long lists (for finite languages such asthe 7 million most popular web queries).The long lists are indexed with a suffix array.Suffix arrays2 generalize very nicely to phonemode, as described below.
We treat the list of webqueries as a text of N bytes.
(Newlines are re-placed with end-of-string delimiters.)
The suffixarray, S, is a sequence of N ints.
The array is ini-tialized with the ints from 0 to N?1.
Thus, S[i]=i,for 0?i<N.
Each of these ints represents a string,starting at position i in the text and extending to theend of the string.
S is then sorted alphabetically.Suffix arrays make it easy to find the frequencyand location of any substring.
For example, giventhe substring ?mail,?
we find the first and last suf-fix in S that starts with ?mail.?
The gap betweenthese two is the frequency.
Each suffix in the gappoints to a super-string of ?mail.
?To generalize suffix arrays for phone mode wereplace alphabetical order (strcmp) with phone or-der (phone-strcmp).
Both strcmp and phone-strcmp consider each character one at a time.
Instandard alphabetic ordering, ?a?<?b?<?c?, but in2 An excellent discussion of suffix arrays including sourcecode can be found at www.cs.dartmouth.edu/~doug.95phone-strcmp, the characters that map to the samekey on the phone keypad are treated as equivalent.We generalize suffix arrays to take advantageof popularity weights.
We don?t want to find allqueries that contain the substring ?mail,?
butrather, just the k-best (most popular).
The standardsuffix array method will work, if we add a filter onthe output that searches over the results for the k-best.
However, that filter could take O(N) time ifthere are lots of matches, as there typically are forshort queries.An improvement is to sort the suffix array byboth popularity and alphabetic ordering, alternatingon even and odd depths in the tree.
At the firstlevel, we sort by the first order and then we sort bythe second order and so on, using a construction,vaguely analogous to KD-Trees (Bentley, 1975).When searching a node ordered by alphabeticalorder, we do what we would do for standard suffixarrays.
But when searching a node ordered bypopularity, we search the more popular half beforethe second half.
If there are lots of matches, asthere are for short strings, the index makes it veryeasy to find the top-k quickly, and we won?t haveto search the second half very often.
If the prefixis rare, then we might have to search both halves,and therefore, half the splits (those split by popu-larity) are useless for the worst case, where theinput substring doesn?t match anything in the table.Lookup is O(sqrt N).3Wildcard matching is, of course, a differenttask from substring matching.
Finite State Ma-chines (Mohri et al 2002) are the right way tothink about the k-best string matching problemwith wildcards.
In practice, the input strings oftencontain long anchors of constants (wildcard freesubstrings).
Suffix arrays can use these anchors togenerate a list of candidates that are then filteredby a regex package.3 Let F(N) be the work to process N items on thefrequency splits and let A(N) be the work to proc-ess N items on the alphabetical splits.
In the worstcase, F(N) = 2A(N/2) + C1 and A(N) = F(N/2) + C2,where C1  and C2 are two constants.
In otherwords, F(N) = 2F(N/4) + C, where C = C1 + 2C2.We guess that F(N) = ?
sqrt(N) + ?, where ?
and ?are constant.
Substituting this guess into the recur-rence, the dependencies on N cancel.
Thus, weconclude, F(N) = O(sqrt N).Memory is limited in many practical applica-tions, especially in the mobile context.
Much hasbeen written about lossless compression of lan-guage models.
For trigram models, we use a lossymethod inspired by the Unix Spell program (McIl-roy, 1982).
We map each trigram <x, y, z> into ahash code h = (V2 x + V y + z) % P, where V is thesize of the vocabulary and P is an appropriateprime.
P trades off memory for loss.
The cost tostore N trigrams is: N [1/loge2 + log2(P/N)] bits.The loss, the probability of a false hit, is 1/P.The N trigrams are hashed into h hash codes.The codes are sorted.
The differences, x, are en-coded with a Golomb code4 (Witten et al 1999),which is an optimal Huffman code, assuming thatthe differences are exponentially distributed, whichthey will be, if the hash is Poisson.6 ConclusionsThe Wild Thing encourages users to make use ofwildcards, speeding up typing, especially on cellphones.
Wildcards are useful when you want tofind something you can?t spell, or something stuckon the tip of your tongue.
Wildcards are moreexpressive than standard prefix matching, great forusers, and technically challenging (and fun) for us.ReferencesJ.
L. Bentley (1975), Multidimensional binary searchtrees used for associative searching, Commun.
ACM,18:9, pp 509-517.S.
T. Dumais, E. Cutrell, et al(2003).
Stuff I've Seen: Asystem for personal information retrieval and re-use,SIGIR.M.
D. McIlroy (1982), Development of a spelling list,IEEE Trans.
on Communications 30, 91-99.M.
Mohri, F. C. N. Pereira, and M. Riley.
WeightedFinite-State Transducers in Speech Recognition.Computer Speech and Language, 16(1):69-88, 2002.I.
H. Witten, A. Moffat and T. C. Bell, (1999),  Manag-ing Gigabytes: Compressing and Indexing Docu-ments and Images, by Morgan Kaufmann Publishing,San Francisco, ISBN 1-55860-570-3.4 In Golomb, x = xq m + xr, where xq = floor(x/m)and xr = x mod m.  Choose m to be a power of twonear ceil(?
E[x])=ceil(?
P/N).
Store quotients xqin unary and remainders xr in binary.
z in unary isa sequence of z?1 zeros followed by a 1.
Unary isan optimal Huffman code when Pr(z)=(?)z+1.
Stor-age costs are: xq bits for xq + log2m bits for xr.96
