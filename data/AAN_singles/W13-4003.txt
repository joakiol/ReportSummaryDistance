Proceedings of the SIGDIAL 2013 Conference, pages 12?20,Metz, France, 22-24 August 2013. c?2013 Association for Computational LinguisticsUnsupervised structured semantic inference for spoken dialog reservationtasksAlejandra LorenzoUniversite?
de LorraineLORIA, UMR 7503Nancy, Francealejandra.lorenzo@loria.frLina M. Rojas-BarahonaLORIA, UMR 7503Nancy, Francelina.rojas@loria.frChristophe CerisaraLORIA, UMR 7503Nancy, Francecerisara@loria.frAbstractThis work proposes a generative modelto infer latent semantic structures on topof manual speech transcriptions in a spo-ken dialog reservation task.
The proposedmodel is akin to a standard semantic rolelabeling system, except that it is unsuper-vised, it does not rely on any syntactic in-formation and it exploits concepts derivedfrom a domain-specific ontology.
Thesemantic structure is obtained with un-supervised Bayesian inference, using theMetropolis-Hastings sampling algorithm.It is evaluated both in terms of attachmentaccuracy and purity-collocation for clus-tering, and compared with strong baselineson the French MEDIA spoken-dialog cor-pus.1 IntroductionMany concrete applications that involve human-machine spoken dialogues exploit some hand-crafted ontology that defines and relates the con-cepts that are useful for the application.
The mainchallenge for the dialog manager used in the appli-cation is then to interpret the user?s spoken inputin order to correctly answer the user?s expectationsand conduct a dialogue that shall be satisfactoryfor the user.
This whole process may be decom-posed into the following stages:?
Automatic speech recognition, to transformthe acoustic signal into a sequence of words(or sequences of word hypotheses);?
Spoken language understanding, to segmentand map these sequences of words into con-cepts of the ontology;?
Semantic analysis, to relate these conceptstogether and interpret the semantic of the userinput at the level of the utterance, or of thespeaker turn;?
Dialogue act recognition?
Dialogue planning?
Text generation?
...Note that the process sketched here often furtherinvolves several other important steps that are usedinternally within one or several of these broadstages, for instance named entity recognition, co-reference resolution, syntactic parsing, marcov de-cision process, reinforcement learning, etc.This work focuses mainly on the second andthird stages, since we assume that segmentationis given and we want to discover the underly-ing concepts and relations in the data.
The thirdstage is very important because it exhibits the la-tent semantic structure hidden in the user utter-ance: what is the object affected by a given pred-icate ?
What are the modifiers that may alter themeaning of a predicate ?
Without such a structure,the system can hardly push understanding beyondlexical semantics and reach fine-grained seman-tic representations, which are thus often limitedto well-formed inputs and cannot handle sponta-neous speech as considered here.
But still, despiteits importance, most spoken dialog systems do notmake use of such structure.We propose an approach here to address thisissue by directly inferring the semantic structurefrom the flat sequence of concepts using the un-supervised Bayesian learning framework.
Hence,the proposed model does not rely on any prede-fined corpus annotated with semantic structure,which makes it much more robust to spoken inputsand adaptable to new domains than traditional su-pervised approaches.122 Related workIn recent years, an increasing number of workshave addressed robustness and adaptability issuesin most of standard Natural Language Processingtasks with unsupervised or semi-supervised ma-chine learning approaches.
Unsupervised learn-ing attempts to induce the annotations from largeamounts of unlabeled data.
Several approacheshave recently been proposed in this context for thesemantic role labeling task.
(Swier and Stevenson,2004) were the first to introduce an unsupervisedsemantic parser, followed by (Grenager and Man-ning, 2006), (Lang and Lapata, 2010), (Lang andLapata, 2011b) and (Lang and Lapata, 2011a).
Fi-nally, (Titov and Klementiev, 2012), introducedtwo new Bayesian models that achieve the bestcurrent state-of-the-art results.
However, all theseworks use some kind of supervision (namely averb lexicon or a supervised syntactic system,which is the case in most of the approaches).
(Abend et al 2009) proposed an unsupervisedalgorithm for argument identification that usesa fully unsupervised syntactic parser and wherethe only supervised annotation is part-of-speech(POS) tagging.Semi-supervised learning attempts to improvethe performance of unsupervised algorithms byusing both labeled and unlabeled data for train-ing, where typically the amount of labeled data issmaller.
A variety of algorithms have been pro-posed for semi-supervised learning1.
In the con-text of semantic role labeling, (He and Gildea,2006) and (Lee et al 2007) hence tested self-training and co-training, while (Fu?rstenau and La-pata, 2009) used a graph-alignment method tosemantic role labeling (SRL).
Finally, in (De-schacht and Moens, 2009) the authors present asemi-supervised Latent Words Language Model,which outperforms a state-of-the-art supervisedbaseline.
Although semi-supervised learning ap-proaches minimize the manual effort involved,they still require some amount of annotation.
Thisannotation is not always available, sometimes ex-pensive to create and often domain specific.
More-over, these systems assume a specific role labeling(e.g.
PropBank, FrameNet or VerbNet) and are notgenerally portable from one framework to another.A number of works related to semantic infer-ence have already been realized on the French1We refer the reader to (Zhu, 2005) or (Pise and Kulkarni,2008) for an overview on semi-supervised learning methods.MEDIA corpus.
Hence, dynamic Bayesian net-works were proposed for semantic compositionin (Meurs et al 2009), however their model re-lies on manual semantic annotation (i.e.
concept-value pairs) and supervised training through thedefinition of 70 rules.
In (Huet and Lefe`vre, 2011;Camelin et al 2011) unsupervised models wereproposed that use stochastic alignment and LatentDirichlet Allocation respectively, but these mod-els infer a flat concept-value semantic representa-tion.
Compared to these works, we rather proposea purely unsupervised approach for structured se-mantic Metropolis-Hastings inference with a gen-erative model specifically designed for this task.3 Proposed model3.1 PrincipleWe consider a human-machine dialog, with the ob-jective of automatically building a semantic struc-ture on top of the user?s spoken utterances thatshall help the dialog system to interpret the userinputs.
This work focuses on inferring the seman-tic structure, and it assumes that a segmentation ofusers?
utterances into concepts is given.
More pre-cisely, we exploit as input a manual segmentationof each utterance into word segments, where eachsegment represents a single concept that belongsto MEDIA ontology (Denis et al 2006) (see Fig-ure 1).AttributesPrice GeneralParkRelativeNearRestaurantLocation Person TimeHotel RoomObjectThingFigure 1: Excerpt of MEDIA ontologyThis ontology identifies the concepts that canhave arguments, and we thus use this informa-tion to further distinguish between head segmentsthat can have arguments (noted Wh2 in Figure 3)and argument segments that cannot govern anotherconcept (noted Wa).
From these two classes of2Wh actually represents one word in a segment composedof Nh words, but by extension, we implicitly refer here to thefull segment.13segments and the words?
inflected forms that com-pose each segment we infer:?
A semantic structure composed of triplets(Wa,Wh, A) where A is the type of argu-ment, or, in other words, the type of semanticrelation between both segments;?
A semantic class Ct for the head segmentAn example of the target structure we want to ob-tain is shown in Figure 2.Inference of these structure and classes is real-ized with an unsupervised Bayesian model, i.e.,without training the model on any corpus anno-tated with such relations.
Instead, the model istrained on an unlabeled dialog corpus composedof raw manual speech transcriptions, which havealso been manually segmented into utterances andwords?
segments as described above.
Training isactually realized on this corpus using an approxi-mate Bayesian inference algorithm that computesthe posterior distribution of the model?s param-eters given the dataset.
We have used for thispurpose the Metropolis-Hastings Markov ChainMonte Carlo algorithm.3.2 Bayesian modelFigure 3 shows the plate diagram of the proposedmodel.
The plate Nh (respectively Nw) that sur-rounds a shaded node represents a single words?segment of length Nh (respectively Nw).
Theouter plate Nu indicates that the graphical modelshall be repeated for each of the Nu utterances inthe corpus.Variable DescriptionCt latent semantic type assigned to predicate tWh observed words in each head segment.P (Wh|Ct) encodes lexical preferences for thesemantic inferenceAi latent semantic type assigned to the ith argu-ment of predicate tRpi latent relative position assigned to the ith argu-ment of predicate tWa observed words in each argument segment.P (Wa|Ai) encodes lexical preferences for thesemantic inferenceTable 1: Variables of the modelEach head word segment has a latent semantictype Ct, and governs Na arguments.
Each argu-ment is represented by an argument words?
seg-ment, which has a latent semantic typeA.
Each ar-gument is further characterized by its relative po-sition Rp with respect to its head segment.
RpC1 ?
?
?
Ct?1 Ct Ct+1 ?
?
?
CNcWhNhAWaNwRpNaNuFigure 3: Plate diagram of the proposed model.Nu represents the number of utterances; Nh, thenumber of words in a head segment; Nw, the num-ber of words in an argument segment; and Na thenumber of arguments assigned to predicate t.can have 4 values, depending on whether the argu-ment is linked to the closest (1) or another (2) ver-bal3 head, or the closest (3) or another (4) nominalhead.
Rp is derived from the argument-to-headassignment, which is latent.
So, Rp is also latent.The sequence of Nc head segments in utterance uis captured by the HMM shown on top of the platediagram, which models the temporal dependencybetween successive ?semantic actions?
of the user.The variables of the model are explained in Ta-ble 1.The most important property of this model isthat the number of arguments Na is not known be-forehand.
In fact, every argument segment can begoverned by any of the Nc head segments in theutterance, and it is the role of the inference pro-cess to actually decide with which head it shouldbe linked.
This is why the model performs struc-tured inference.Concretely, at any time during training, everyargument is governed by a single head.
Then, in-ference explores a new possible head attachmentfor an argument Wa, which impacts the model asfollows:?
The number of arguments Na of the previoushead is decreased by one;?
The number of argumentsNa of the new headis increased by one;3Morphosyntactic classes are obtained with the Treetag-ger14Je voudrais le prix en fait je euh une chambre pas che`reI ?d like the price well in fact I uh a room not expensiveReserve RoomAgentPrice PriceBooked objectFigure 2: Example of inferred semantic structure for a sentence in the MEDIA corpus.
Traditionaldependency notations are used: the head segment points to the argument segment, where segments areshown with boxes (arrows link segments, not words !).
The semantic class assigned to each head segmentis shown in bold below the translated text.?
The relative position Rp of the argument isrecomputed based on its new head position;?
The argument typeA is also re sampled giventhe new head type Ct.This reassignment process, which is at the heart ofour inference algorithm, is illustrated in Figure 4.3.3 Metropolis inferenceBayesian inference aims at computing the poste-rior distribution of the model?s parameters, giventhe observed data.
We assume that all distributionsin our model are multinomial with uniform priors.The parameters are thus:P (Wh|Ct) ?M(?HCt)Distribution of thewords for a givenhead semantic classP (Ct|Ct?1) ?M(?CCt?1)Transition prob-abilities betweensemantic classesP (Wa|A) ?M(?WA )Distribution of thewords for a givenargument typeP (Rp|A) ?M(?RA)Distrib.
of the rel-ative position of agiven argument toits head given theargument typeP (A|Ct) ?M(?ACt)Distrib.
of the ar-gument types givena head semanticclass3.3.1 Inference algorithmTo perform inference, we have chosen a MarkovChain Monte Carlo algorithm.
As our model isfinite, parametric and identifiable, Doob?s theo-rem guarantees the consistency of its posterior,and thus the convergence of MCMC algorithmstowards the true posterior.
Because changing thehead of one argument affects several variables si-multaneously in the model, it is problematic touse the basic Gibbs sampling algorithm.
A block-Gibbs sampling would have been possible, but thiswould have increased the computational complex-ity and we also wanted to keep as much flexibilityas possible in the jumps that could be realized inthe search space, in order to prevent slow-mixingand avoid (nearly) non-ergodic Markov chains,which are likely to occur in such structured infer-ence problems.We have thus chosen a Metropolis-Hastingssampling algorithm, which allows us to design anefficient proposal distribution that is adapted to ourtask.
The algorithm proceeds by first initializingthe variables with a random assignment of argu-ments to one of the heads in the utterance, and auniform sampling of the class variables.
Then, ititerates through the following steps:1.
Sample uniformly one utterance u2.
Sample one jump following the proposal dis-tribution detailed in Section 3.3.2.3.
Because the proposal is uniform, compute theacceptance ratio between the model?s jointprobability at the proposed (noted with a ?
)and current states:r = P (C?,W ?h,W ?a, Rp?, A?
)P (C,Wh,Wa, Rp,A)4.
Accept the new sample with probabilitymin(1, r); while the sample is not accepted,iterate from step 2.15Je voudrais le prix en fait je euh une chambre pas che`reI ?d like the price well in fact I uh a room not expensiveAgentPrice PriceBooked objectAgentPriceBooked objectPriceFigure 4: Illustration of the reassignment process following the expample presented in Figure 2.
Thisexample illustrates the third Metropolis proposed move, which changes the head of argument ?le prix?
:arcs above the text represent the initial state, while arcs below the text represent the new proposed state.5.
When the sample is accepted, update themultinomials accordingly and iterate fromstep 1 until convergence.This process is actually repeated for 2,000,000iterations, and the sample that gives the largestjoint probability is chosen.3.3.2 Metropolis proposal distributionThe proposal distribution is used to explore thesearch space in an efficient way for the targetapplication.
Each state in the search space isuniquely defined by a value assignment to everyvariable in the model, for every utterance in thecorpus.
It corresponds to one possible sample ofall variables, or in other words, to the choice ofone possible semantic structure and class assign-ment to all utterances in the corpus.Given a current state in this search space, theproposal distribution ?proposes?
to jump to anew state, which will then be evaluated by theMetropolis algorithm.
Our proposal samples anew state in the following successive steps:1.
Sample uniformly one of the three possiblemoves:Move1: Change the semantic class of a head;Move2: Change the argument type of an argu-ment segment;Move3: Change the assignment of an argumentto a new head;2.
If Move1 is chosen, sample uniformly onehead segment and one target semantic class;3.
If Move2 is chosen, sample uniformly oneargument segment and one target argumenttype;4.
If Move3 is chosen, sample uniformly oneargument segment Wa and ?detach?
it fromits current head.
Then, sample uniformly onetarget head segment W ?h, and reattach Wa toits new head W ?h.
Because the distribution ofargument types differ from one head class toanother, it would be interesting at this stageto resample the argument type of Wa fromthe new head class distribution.
But in thiswork, we resample the argument type fromthe uniform distribution.This proposal distribution Q(x ?
x?)
is re-versible, i.e., Q(x?
x?)
> 0?
Q(x?
?
x) > 0.We can show that it is further symmetric, i.e.,Q(x ?
x?)
= Q(x?
?
x), because the samemove is sampled to jump from x to x?
than to jumpfrom x?
to x, and because the proposal distributionwithin each move is uniform.4 Experimental validation4.1 Experimental setupThe French MEDIA corpus collects about 70hours of spontaneous speech (1258 dialogues,46k utterances, 494.048 words and 4068 dis-tinct words) for the task of hotel reservationand tourist information (Bonneau-Maynard et al2005).
Calls from 250 speakers to a simulatedreservation system (i.e.
the Wizard-of-Oz) wererecorded and transcribed.
Dialogues are full ofdisfluencies, hesitations, false starts, truncations orfillers words (e.g., euh or ben).16Gold Standard AnnotationSemantic Relation FrequencyAgent 320Booked object 298Location 285Time 209Coordination 134Beneficiary 117Price 108Reference Location 66Table 2: Most frequent semantic relations in thegold annotation.This corpus has been semantically annotatedas part of the French ANR project PORT-MEDIA (Rojas-Barahona et al 2011).
We areusing a set of 330 utterances manually annotatedwith gold semantic relations (i.e.
High-Level Se-mantics).
This gold corpus gathers 653 head seg-ments and 1555 argument segments, from whicharound 20% are both arguments and heads, suchas une chambre in Figure 4.
Table 2 shows thesemantic relations frequencies in the gold annota-tion.
12 head segment types and 19 different argu-ment segment types are defined in the gold anno-tations.
In the evaluation, we assume the numberof both classes is given.
A possible extension ofthe approach to automatically infer the number ofclasses would be to use a non-parametric model,but this is left for future work.4.2 Evaluation metricsThe proposed method infers three types of seman-tic information:?
The semantic relation between an argumentand its head;?
The argument type A?
The semantic class of the head Ct.The three outcomes are evaluated as follows.?
The output structure is a forest of trees thatis similar to a partial syntactic dependencystructure.
We thus use a classical unsuper-vised dependency parsing metric, the Un-labeled Attachment Score (UAS), which issimply the accuracy of argument attachment:an argument is correctly attached if and onlyif its inferred head matches the gold head.?
Both argument and head classes correspondto the outcome of a clustering process intosemantic classes, akin to the semantic classesobtained in unsupervised semantic role la-beling tasks.
We then evaluate them with aclassical metric used to evaluate these classesin unsupervised SRL (as done for instancein (Lang and Lapata, 2011a) and (Titov andKlementiev, 2012)): purity and collocation.Purity measures the degree to which each clus-ter contains instances that share the same goldclass, while collocation measures the degree towhich instances with the same gold class are as-signed to a single cluster.More formally, the purity of argument seg-ments?
(head segment?)
clusters for the whole cor-pus is computed as follows:PU = 1N?imaxj|Gj ?
Ci|whereCi is the set of argument (head) segmentsin the ith cluster found, Gj is the set of argument(head) segments in the jth gold class, and N isthe number of gold argument (head) segment in-stances.
In a similar way, the collocation of argu-ment segments?
(head segment?)
clusters is com-puted as follows:CO = 1N?jmaxi|Gj ?
Ci|Finally the F1 measure is the harmonic mean ofthe purity and collocation:F1 = 2 ?
CO ?
PUCO + PU4.3 Experimental resultsWe compare the proposed approach against twobaselines:?
An argument-head ?attachment?
baseline,which attaches each argument to the closesthead segment.?
A strong clustering baseline, which respec-tively clusters the head and argument seg-ments using a very effective topic model:the Latent Dirichlet Allocation (LDA) ap-proach (Blei et al 2003).17Table 3 shows the UAS obtained for the pro-posed model on the MEDIA corpus, while Table 4shows the obtained Purity, Collocation and F1-measure.
In both cases, we compare the perfor-mances of the proposed model with the respectivebaseline.
Our system outperforms both baselinesby a large margin.System UASClosest attachment 68%(?2%)Proposed - UAS 74%(?2%)Table 3: Experimental results for UAS on the ME-DIA database.
The statistical confidence intervalat 95% with Gaussian approximation is reported.System Purity Col. F-mesLDA - Heads 51.7% 25.5% 34.2%LDA - Args 31.7% 22.2% 26.1%Proposed - Heads 78.7% 50.8% 61.8%Proposed - Args 61.8% 53.3% 59.3%Table 4: Experimental results on the MEDIAdatabase for purity, collocation and F1-measure.4.3.1 Qualitative EvaluationWe further carried out a qualitative evaluation,where we inspected the inferred clusters and com-pared them with the baseline.
Figures 7 and 8show, for every head class Ct in each stacked col-umn, the distribution of instances from all goldclusters.
Each column can also be viewed as agraphical representation of the intersection of oneinferred class with all gold clusters.
Figure 7 illus-trates this for our model, and Figure 8 for LDA.The same comparison for the argument types isshown, respectively, in Figure 5 and Figure 6.For head segment clusters, we can observe thatmost inferred clusters contain many instances ofthe Reservation type (in dark blue), both in theLDA baseline and in the proposed system.
Themain reason for that is that the corpus is very un-balanced in favor of the Reservation class, whilewe do not assume any prior knowledge about thedata and thus use a uniform prior.
Still, every othergold type that occurs with a reasonnably highenough frequency, apart from two special typesthat are discussed next, is well captured by one ofFigure 5: Distribution of the gold types (one percolor) into the clusters inferred by our system(shown on the X-axis) for argument segments.our inferred class: this is the case for ?Room?
thatmainly intersects with our class 1, ?Place?
withour class 2 and ?Hotel?
with our class 9.Some examples of instances for each case are:?
Reservation: ?voudrais re?server?, ?aimeraispartir?, ?voudrais une *re?servation unere?servation?, ?prends?, ?recherche?
,?
*de?sire de?sire?, ?il me faudrait?, ?opte?,?aimerais s?
il vous pla?
?t si c?
est possibleavoir prendre?.?
Room: ?deux chambres pour un coup(le)avec trois enfants avec bon standing?, ?troissingles?, ?deux chambres de bon standinga` peu pre`s niveau trois e?toiles?, ?trois dou-bles?.?
Place: ?Paris?, ?a` Saintes?, ?a`Charleville?, ?dans le dix huitie`me ar-rondissement de Paris?.?
Hotel: ?un ho?tel deux e?toiles?, ?dans unho?tel beau standing?, ?un ho?tel formule un?,?l?
ho?t(el) le l?
ho?tel?, ?un autre ho?tel dansles me?mes conditions?, ?le Beaugency?, ?l?autre?, ?au Novotel?, ?le premier?.Two ?special?
head segment types that are nei-ther nicely captured by our system nor LDA areCoordination and Inform, which are instead as-signed to the clusters corresponding to the goldsegments that they coordinate or inform about.For argument segments we also observed thatthe inferred clusters are semantically related to thegold types.
We found, for instance, four clusters18Figure 6: Distribution of the gold types (one percolor) into the clusters inferred by the LDA base-line (shown on the X-axis) for argument segments.Figure 7: Distribution of the gold types (one percolor) into the clusters inferred by our system(shown on the X-axis) for head segments.
(2, 5, 12 and 15) containing mainly ?Time?
ar-guments (?du premier au trois Novembre?, ?dixnuit?, ?le festival du film?, ?au seize Novembre?,etc.
), two (3 and 14) dedicated to ?Location?
argu-ments (?a` Menton?, ?au festival lyrique de belleeuh Belle Ile En mer?, ?bastille?, ?sur le ville deParis?, ?parking prive??
), one (10) for ?Price?
ar-guments (?pas plus de cent euros par personne?,?un tarif infe?rieur a` quatre vingts euros?, ?pastrop che`re?, ?a` cent vingt euros?, ?moins de cent*cent euros?)
etc.Finally, as noted for the head segments, we canobserve that the most frequent gold types largelyintersect with several inferred clusters, for thesame reason: data is very unbalanced and we donot assume any prior knowledge about the dataFigure 8: Distribution of the gold types (one percolor) into the clusters inferred by the LDA base-line (shown on the X-axis) for head segments.and thus use an uniform prior.
Nevertheless, sev-eral other important classes such as Event, Priceand Agent are well captured by our system.5 ConclusionsThis work proposes an unsupervised generativemodel to infer latent semantic structures on topof user spontaneous utterances.
It relies on theMetropolis-Hastings sampling algorithm to jointlyinfer both the structure and semantic classes.
Itis evaluated in the context of the French MEDIAcorpus for the hotel reservation task.
Although thesystem proposed in this work is evaluated on a spe-cific spoken dialog reservation task, it actually re-lies on a generic unsupervised structured inferencemodel and can thus be applied to many other struc-tured inference tasks, as long as observed wordsegments are given.An interesting future direction of researchwould be to modify this model so that it jointlyinfers both the latent syntactic and semantic struc-tures, which are known to be closely related butstill carry complementary information.
We ofcourse also plan to evaluate the proposed modelwith automatic speech transcriptions and conceptsdecoding.
Another advantage of the proposedmodel is the possibility to build better Metropolis-Hastings proposals, which may greatly improvethe convergence rate of the algorithm.
In partic-ular, we would like to investigate the use of somenon-uniform proposal distributions when reattach-ing an argument to a new head, which shall im-prove mixing.19ReferencesOmri Abend, Roi Reichart, and Ari Rappoport.
2009.Unsupervised argument identification for semanticrole labeling.
In Proceedings of the Joint Confer-ence of the 47th Annual Meeting of the ACL andthe 4th International Joint Conference on Natu-ral Language Processing of the AFNLP, ACL ?09,pages 28?36, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.D.M.
Blei, A.Y.
Ng, and M.I.
Jordan.
2003.
Latentdirichlet alcation.
the Journal of machine Learn-ing research, 3:993?1022.Helene Bonneau-Maynard, Sophie Rosset, ChristelleAyache, Anne Kuhn, and Djamel Mostefa.
2005.Semantic annotation of the french MEDIA dialogcorpus.
In INTERSPEECH-2005, 3457-3460.N.
Camelin, B. Detienne, S. Huet, D. Quadri, andF.
Lefe`vre.
2011.
Unsupervised concept annota-tion using latent dirichlet alcation and segmentalmethods.
In EMNLP 1st Workshop on UnsupervisedLearning in NLP, Edinburgh (UK).Alexandre Denis, Matthieu Quignard, and GuillaumePitel.
2006.
A Deep-Parsing Approach to NaturalLanguage Understanding in Dialogue System: Re-sults of a Corpus-Based Evaluation.
In Proceedingsof the 5th international Conference on Language Re-sources and Evaluation (LREC 2006) Proceedingsof Language Resources and Evaluation Conference,pages 339?344, Genoa Italie.Koen Deschacht and Marie-Francine Moens.
2009.Semi-supervised semantic role labeling using the la-tent words language model.
In Proc.
EMNLP, pages21?29.Hagen Fu?rstenau and Mirella Lapata.
2009.
Graphalignment for semi-supervised semantic role label-ing.
In Proc.
EMNLP, pages 11?20.Trond Grenager and Christopher D. Manning.
2006.Unsupervised discovery of a statistical verb lexicon.In Proceedings of the 2006 Conference on EmpiricalMethods in Natural Language Processing, EMNLP?06, pages 1?8, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.S.
He and H. Gildea.
2006.
Self-training and Cotrain-ing for Semantic Role Labeling: Primary Report.Technical report, TR 891, University of Colorado atBoulder.Ste?phane Huet and Fabrice Lefe`vre.
2011.
Unsuper-vised alignment for segmental-based language un-derstanding.
In Proceedings of the First Workshopon Unsupervised Learning in NLP, EMNLP ?11,pages 97?104, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Joel Lang and Mirella Lapata.
2010.
Unsuper-vised induction of semantic roles.
In Human Lan-guage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics, HLT ?10, pages 939?947, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Joel Lang and Mirella Lapata.
2011a.
Unsupervisedsemantic role induction via split-merge clustering.In Proc.
ACL, pages 1117?1126.Joel Lang and Mirella Lapata.
2011b.
Unsupervisedsemantic role induction with graph partitioning.
InEMNLP, pages 1320?1331.
Association for Com-puter Linguistics.Joo-Young Lee, Young-In Song, and Hae-Chang Rim.2007.
Investigation of weakly supervised learningfor semantic role labeling.
In ALPIT, pages 165?170.
IEEE Computer Society.Marie-Jean Meurs, Fabrice Lefe`vre, and Renatode Mori.
2009.
Spoken language interpretation: Onthe use of dynamic bayesian networks for semanticcomposition.
In Proc.
ICASSP, pages 4773?4776.Nitin Namdeo Pise and Parag Kulkarni.
2008.
A sur-vey of semi-supervised learning methods.
In Pro-ceedings of the 2008 International Conference onComputational Intelligence and Security - Volume02, CIS ?08, pages 30?34, Washington, DC, USA.IEEE Computer Society.Lina Maria Rojas-Barahona, Thierry Bazillon,Matthieu Quignard, and Fabrice Lefevre.
2011.Using MMIL for the high level semantic annotationof the french MEDIA dialogue corpus.
In Pro-ceedings of the Ninth International Conference onComputational Semantics (IWCS 2011).Robert S. Swier and Suzanne Stevenson.
2004.
Un-supervised Semantic Role Labelling.
In EMNLP,pages 95?102.
Association for Computational Lin-guistics.Ivan Titov and Alexandre Klementiev.
2012.
Abayesian approach to unsupervised semantic role in-duction.
In Proceedings of the Conference of theEuropean Chapter of the Association for Computa-tional Linguistics, Avignon, France, April.Xiaojin Zhu.
2005.
Semi-Supervised Learning Liter-ature Survey.
Technical report, Computer Sciences,University of Wisconsin-Madison.20
