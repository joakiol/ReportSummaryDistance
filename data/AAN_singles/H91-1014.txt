Development  and Prel iminary Evaluationof the MIT ATIS System 1Stephanie Seneff, James Glass, David Goddeau, David Goodine, Lynette Hirschman,Hong Leung, Michael Phillips, Joseph Polifroni, and Victor ZueSpoken Language Systems GroupLaboratory for Computer ScienceMassachusetts Institute of TechnologyCambridge, Massachusetts 02139ABSTRACTThis paper represents a status report on the MIT ATIS sys-tem.
The most significant new achievement is that we now havea speech-input mode.
It is based on the MIT SUMMIT system us-ing context independent phone models, and includes a word-pairgrammar with perplexity 92 (on the June-90 test set).
In addi-tion, we have completely redesigned the back-end component, inorder to emphasize portability and extensibility.
The parser nowproduces an intermediate s mantic frame representation, whichserves as the focal point for all back-end operations, uch as his-tory management, text generation, and SQL query generation.Most of those aspects of the system that are tied to a partic-ular domain are now entered through a set of tables associatedwith a small artificial anguage for decoding them.
We have alsoimproved the display of the database table, making it consider-ably easier for a subject o comprehend the information given.We report here on the results of the official DARPA February-91evaluation, as well as on results of an evaluation on data collectedat MIT, for both speech input and text input.INTRODUCTIONIn June 1990, we reported on the initial development ofthe MIT ATIS system, and participated in the first round ofDARPA common evaluation using text input \[5\].
Since then,a number of changes have been made to our system, particu-larly the back-end component that transforms the parse treeinto a representation that can be used to maintain discourse,generate confirmation messages, and produce SQL queriesfor accessing the OAG database.
We have also connected theSUMMIT speech recognizer to our ATIS system, so that it cannow accept verbal input.This paper gives a progress report on the MIT ATIS de-velopment, with particular emphasis on how the system pro-cesses an input utterance to achieve understanding and gen-erate responses.
We will also report on the evaluation ofthe system for both text and speech input, using the dataprovided by TI through NIST, as well as the data that weXThis research was supported by DARPA under Contract N00014-89-J-1332, monitored through the Office of Naval Research.have collected over the past few months \[3\].
Aspects of thesystem involving discourse and dialogue are based on simi-lar principles as before, but has been modified to reflect thenew semantic representations.
A detailed escription of ourdiscourse model can be found in a companion paper \[4\].SYSTEM DESCRIPT IONIn this section we will describe those aspects of the systemthat have changed significantly since our report last June \[5\].The most significant change has been the incorporation ofthe speech recognition component.
We begin by describingthe recognizer configuration and the interface mechanism weare currently using.
In the natural language component, theparser and grammar remain unchanged, except for augmen-tations to improve coverage.
However, we have completelyredesigned the component that translates from a parse treeto executable SQL queries, and the component that gener-ates verbal responses.
Both of these areas are described herein more detail.Speech  Recogn i t ion  ComponentThe speech recognition configuration is similar to the oneused in the VOYAGER system and is based on the SUMMIT sys-tem \[6\].
For the ATIS task, we used 76 context-independentphone models trained on speaker-independent data collectedat TI and MIT \[3\].
There were 1284 TI sentences (read andspontaneous versions of 642 sentences) and 1146 spontaneoussentences taken from the MIT training corpus.
The lexiconwas derived from the vocabulary used by the ATIS naturallanguage component and consisted of 577 words.
In order toprovide some conservative natural language constraints, thespeech recognition component used a generalized word-pairgrammar derived from the speech training data augmentedwith a large number of additional sentences pooled from allavailable sources of nTIS related text material.
The word-pair grammar was generated by parsing each sentence, andthen generalizing each word in a terminal node to all wordsin the same semantic lass.
Thus for example, an instanceof the word "Boston" would generalize to all cities.
In the88case where a sentence did not parse, no additions were madeto the word-pair grammar.
When evaluated on the TI June-90 data set of 138 sentences, the word-pair grammar had acoverage of 70% and a perplexity of 92.
Overall, 3.6% of thesentences that parsed failed to pass the word-pair grammar.The interface to the natural anguage component was im-plemented with the N-best mechanism we have describedpreviously for the VOYACER.
system \[6\].
In our original im-plementation, the first N-best output which parsed was usedby the back-end to generate a response.
Since our naturallanguage component (TINA) is able to produce a parse prob-ability derived from training data, we have tried to make useof the probability in the selection of the N-best output.
Inboth the VOYAGER.
and ATIS domains we have found that alinear combination of the acoustic score produced by s v M M ITand the parse score produced by TINA improved the overallsystem performance \[1\].
In ATIS the improvement in recog-nition accuracy was about 2% on the TI June-90 data set.In order to control the number of false alarms producedby the system, we investigated the use of severn,1 pruningmeasures which could be applied to the N-best outputs.
Todate we have found the N-best rank and the relative acous-tic score (relative to the first choice output) to be effectiveparameters.The  ATIS back -endAfter reassessing the status of our ATIS system last June,we were concerned that the design of the back-end componentmight not be as easily extended or ported to new domains aswe would like.
We therefore decided to redesign the system,with the goal of emphasizing both system modularity andsystem portability.
In choosing a design for the new system,we had two major goals.
One was to design a semantic framerepresentation that would capture all necessary informationfrom the sentence and serve as a focal point for all compo-nents of the back-end.
The frame design should be flexibleenough to be able to extend to other domains.
The sec-ond goal was to provide a mechanism that would permit thedomain-dependent aspects of the system to be entered com-pletely through table-driven mechanisms, without requiringany explicit programming.Processing of a sentence involves several steps.
The firststep is to provide a parse tree for the input word stream.A second-pass treewalk through the parse tree yields a se-mantic frame, which is then integrated with available framesfrom the history.
Both an SQL query and a generated textresponse are derived from the completed frame.
The verbalresponse is spoken to the subject and a table is retrieved fromthe database through the database management system O R.-ACLE.
A table post-processing step converts the table to amuch more readable and informative form prior to display.Finally, the system examines the goal plan and optionallyinitiates an additional response, based on its assessment of, Hi.qtnrvR=nk"Parser I = SemanticI FrameInput_1 Tea I - I  Generator +VerbalResponse= Display ~TableFigure 1: Block diagram of system structure showing centralrole of semantic frame.a likely next step.
A thorough description of dialogue anddiscourse aspects of the system along with an example flightreservations dialogue can be found in \[4\].The  Semant ic  Frame:  The parse outputs of TINA arefirst converted to a semantic frame representation which servesthree critical roles, as shown in Figure 1: it is translated toSQL through table-driven pattern matching devices, it is de-livered to a text-generation program to construct appropri-ate verbal responses, and it serves as input to the discoursehistory used to restore implicit information in subsequentqueries and resolve explicit anaphoric references.Each frame is associated with a name, a type, and aset of (key: value) pairs.
The value can be an integer, astring, a symbol, another frame, or a set of frames.
Thereare only a small number of possible types of frames, such asclause, predicate, qset (for common noun phrases), reference(for proper nouns), and quantifier.
The type reference alwayshas a special key reflype associated with it, identifying theclass of proper nouns it belongs in (i.e., city-name would bethe reflype for "Boston.
")Convers ion of Parse  Tree to Semant ic  F rame:  Theprocess of producing a semantic frame involves a second-passtree walk through a completed parse tree.
Only the namesof the nodes are needed, because of the semantic nature ofthe grammar.
In the tree walk, nodes pass along frames,modifying them if necessary according to the node's seman-89tic significance.
A completed semantic frame is ultimatelyreturned to the top-level sentence node and delivered as-is tothe back end for further processing.About half of the nodes in the ATIS grammar have nosemantic significance, and hence they simply pass along totheir children and later to their right sibling whatever wasdelivered to them.
Each of the active nodes is associatedby name to a particular semantic name, which is often thesame as its "given" name.
Each semantic name is in turnassociated with a particular functionality.
There are fewerthan twenty possible functions, and during the tree walk, theparticular function to choose is dictated by the association.Each function is called with three arguments: the semanticname, the subparse tree and the current frame.A simple example may help to clarify this process.
Thenode named dir-object is associated with the semantic nametheme which calls the function process-noun-phrase.
Thisfunction, during the top-down cycle, creates an empty frameof type qset and inserts it into the current frame under the keytheme as specified by the argument.
It then passes the emptyframe along to its children, who will fill it in.
Finally, it passesthe original frame to its right siblings, with a completed entryunder the key theme.Decod ing  the  F rame:  A completed semantic frameis passed to the back-end for interpretation.
The top-levelframe is always of type clause, and its name determines aparticular clause-level analysis function to be executed.
Op-tions include request, statement, yes-no-question, clarifier,etc.
For example, the function for a yes-no question makestwo separate calls to the database.
The first one determinesthe set of all objects as specified by the topic, and the sec-ond one finds the set defined by the topic restricted by thepredicate (or complement).
A final step seeks a non-nullintersection between the two sets.
There are three possi-ble types of response, namely "There is no <topic>," Yes,<topic> does do <predicate>," and "No, <topic> does notdo <predicate>."
Thus, to answer the question, "Does theearliest flight serve lunch?"
the system finds both the earliestflight and the earliest flight that serves lunch, and determineswhether they are the same flight.In addition to the high-level interpretation of clauses,some low level routines serve to reorganize certain informa-tion in the frame as delivered by the parser.
For example,there are many modifiers which can be attached to eitherflights or fares.
We decided that it would be easier for laterprocessing if all fare modifiers are physically transferred toa flight object, which is created if it didn't exist explicitlyin the sentence.
Thus if the person says, "Show fares fromBoston to Denver," the sentence is converted into: "Showfares forflights from Boston to Denver."
In addition, phrasesabout time and date are regularized and turned into abso-lute references.
Thus "the following Wednesday," is decodedas "the date which is on the subsequent Wednesday to theFrame Format :\[name typekeyl: valuelkey2 : value2.
.
.
\ ]Frame :\[request c lausepredicate: \[display predicatefor-poss : \["me" reference reftype: pronoun\]theme: \[fare qsetfor: \[auto qsetcar-type: limousineI;o: \["oakland" referencereftype: city-name\] \] \] \] \]Figure 2: Semantic frame representation f the sentence, "Showme the price of a limousine to Oakland.
"date stored in the history table."
After the frame is properlyrestructured, it is sent off to the discourse module, whichaugments noun phrases (mainly flights and fares) with ap-propriate modifiers from the history.SQL Query  Generat ion  Mechan ism:  All of the domain-dependent information eeded to map frames into SQL queriesis contained in a small set of tables, which are decoded througha simple artificial anguage involving a small number of spe-cial operations.
The basic unit of recognition is a patterncontaining (name (key value-type)), where name is the nameof the parent frame, and value-type is the uniquely definedidentifier for the value associated with the key.
For example,the value-type of a qset is simply its name~ the value-typeof a reference is its reflype, and the value-type of a string isSTRING.We will explain the interface between the semantic frameand the back end by walking through a simple example.
Thesemantic frame derived from the sentence, "Show me theprice of a limousine to Oakland," is given in Figure 2, andthe table entries needed to decode that frame are shown inFigure 3.
The final SQL query generated is given in Figure 4.The top-level display-table defines a set of elements to bedisplayed and the set of database tables in which to find theseelements.
For our simple example, the instructions are todisplay all elements in the ground_service table, given a qsetnamed fare with a for key whose value is a qset named auto.The final set of elements and tables to be displayed is con-structed as the union of all sets whose patterns are matchedin display-table.
In some cases, entries from multiple tablesmust be displayed, and for these cases there is an additionaltable that defines how to link the two database tables.The qset-table contains a set of patterns particular toframes of type qset, which trigger the augmentation of a sim-ple database SQL query with a set of where-clause's.
Thesystem processes a top-level qset through recursive process-ing of possible nested qsets.
In our example, both the top-90One entry in the "display-table":((fare (for AUTO)) ground_service (*))Three entries in the "qset-table":((fare (for AUTO))(add-where-clauses 0 $1))((auto (car-type STRING))(= transport_code (use-table auto-table)))((auto ((from to in) (CITY CITY-CODE CITY-NAME)))(in city_code (cvt CITY-CODE $i)))One entry in the "conversion-table":((CITY-NAME CITY-CODE)(sql city city_code O ((= city_name $1))))An auto-table :(("taxi" T) ("limousine" L)("air-taxi" A)("rental-car" R) ("car" R))Figure 3: Table entries needed to decode the sentence, "Showme the price of a limousine to Oakland," whose semantic frameis shown in Figure 1level fare and the auto entry under the for slot are qsets.
Theentry under fare that matches this pattern instructs the sys-tem to add to the parent query all the where clauses that aregenerated by the auto qset where the special code $1 standsfor the argument.There are two entries under auto that are activated by ourframe.
The one matched by car-type constructs the where-clause for the unit: "where transport_code = 'L' " and theone under the key to constructs the where-clause for thecity_code.
The decoding of the city "Denver" is done throughthe conversion-table, keyed by the special operator cvt (con-vert).
The operator sql in conversion-table triggers the con-struction of another SQL query, "select distinct city_codefrom city where city_name = 'DENVER,' " which is insertedinto the where-clause for city_code in ground_transport.
Whatis constructed through this decoding step is not the actualstring appropriate for calling the database, but rather a hi-erarchy of structures representing queries and where clauses,which can be converted to the query string through a print-query function, resulting in the SQL command shown in Fig-ure 4.The Table Display:  We felt that in many cases the rawinformation from the database would not be readily com-prehended without a further transformation.
Therefore, wewrote a set of conversion routines associated with each col-umn heading that would make the table easier to understand.Thus a clock time would be converted from "1426" to "2:26P.M.
", an airline name from "DL" to "Delta", and a fareclass from "QX" to "QX: coach class discounted weekday.
"In some cases, we felt the database column was sufficientlyselect distinct * from ground_servicewhere transport_code = 'L'and city_code in(select distinct city_code from citywhere city_name = 'OAKLAND')Figure 4: The SQL query for the sentence "Show me the priceof a limousine to Oakland.
"confusing that it was better to leave it out altogether, espe-cially in cases where the text response redundantly carriedthe information.
For instance, we never display the column"flight days," since the verbal response will always say, "onTuesday" when appropriate.
Likewise, we omit the flight-code column because it invites the user to refer to flightsby their flight code using unpredictable language constructs.Our paper on database collection \[3\] discusses the effects ofthis transformation on solicited speech.Verbal  Response:  A completed semantic frame is sentto a text generation program along with the database ta-ble indicating the answer.
Text generation is mostly guidedthrough tables, associating keys with both a print functionand a positional specification within the parent frame's over-all scheme.
For example, adjectival modifiers precede themain noun, a flight-number immediately follows the mainnoun, and a post-modlfier such as a relative clause or agerund occurs at the end.
Clause level generation is donethrough specialized functions, each associating with a partic-ular clause type, such as yes-no-question.
The database tableis used both to infer what should be said at the top-level, andto determine whether the noun phrase is singular or plural.Thus, for example, an existential clause would be requiredto produce one of, "There is" There are" or "There are no"preceding a noun-phrase describing the intended flight set.When a person asks a wh-query, such as "What meals dothese flights serve?"
the system detects the trace under theobject of the verb "serve" and inserts the canned phrase, "thefollowing meals" into the verbal response.
The database tableis then displayed providing the answer in a meals column.Other  Aspects  of the  System:  There are two othermajor components of the system that have not yet been dis-cussed.
These are the discourse history management systemand the dialogue component.
Both of these are described indetail in \[4\] and therefore will only be briefly mentioned here.Discourse is managed through a history table contain-ing several types of elements derived from the previous en-tences, including both semantic frames identifying namedobjects such as flights and dates, display tables from thedatabase, and, in the case of bookings, previous tates of theticket.
Most of the history revolves around a flight-event ob-ject.
Modifiers are inherited from the history either if theyare not explicitly mentioned in the current frame or if no91"masker" modifiers are present.
For each history modifier,a set of maskers is specified in a table.
We determined themasking conditions based on experience with real data.
Forexample, if the subject asks about "non-stop" flights, then aconnection-place would not be inherited.
The most complexhistory management involves references to "return flights," inwhich a previously mentioned source and destination must be"swapped," unless the previous sentence also concerned re-turn flights.
In addition, only fare restrictions and airlineshould be inherited, along with source and destination.
Anyprevious references to a date or a flight number would bedropped when talking about return flights.The computer essentially always gives a verbal responseto the subject's question identifying the contents of the dis-played table.
Dialogue is maintained through a dialogue statestack which is popped and evaluated after each input sentenceis fully processed.
A clear division is kept in the computercode between the subject's half of the conversation and thecomputer's half.
During the analysis of the subject's con-tribution, the dialogue state may be modified, but none ofthe dialogue xecution routines are called.
Most of the timethe dialogue stack is empty, and it rarely contains more thanone previous tate.
Dialogue is used mostly during bookings,which involve a complex interplay between the subject andthe computer.
For example, if the subject says, "Book thecheapest flight."
the system must remember that a bookingis underway, but must first ask whether the subject wants aone-way or round-trip fare.
Hence the stack becomes two-deep at this point.EVALUATIONTable 1 summarizes our results for the three obligatorysystem evaluations using the February-91 test set provided byNIST.
The first test takes as input the transcriptions of theso-called Class A sentences, i.e., sentences that are contextindependent, and produces a CAS 2 output.
The second testis the same as the first one, except that the sentences areClass D1, i.e., their interpretation depends upon a previoussentence, which is provided as additional input.
The last testis the same as the first, except hat the input is speech ratherthan text.
For each data set, we give the percent correct,percent incorrect, percent with no answer, and the overallscore, where the score penalizes incorrect answers weightedequally against correct answers.Comparing the first row of Table i with last June, our cur-rent implementation makes considerably fewer false alarmsfor text input.
The two errors that the system made weredue to a minor system bug; while the correct answer was dis-played to the user locally, we inadvertently sent the wrongone to the comparator.
We are encouraged by this result,2CAS, or Common Answer Specification, is a standardized format forthe information retrieved from the OAG database, which is comparedagainst a "reference" CAS using a comparator provided by NIST.Data No.
of I Correct Incorrect No Answer Score ISet Sentences (%) (%) I (%) (%) IClass A Text 145 I 56.6 1.4 i 42.1 55.2Class D1 Text 38 I 47.4 5.3 I 47.4 42.1I\[ Class A Speech \]45 I 31.7 13.1 \[ 55.2 \[ 18.6 HTable 1: Results for the standard ATIS test sets for three testconditions.since the errors were all due to factors unrelated to the de-velopment of the natural language technology, and as suchcan be fixed trivially.The results for the context-dependent sentences are givenin column 2 of Table 1.
Our system provided correct answersfor 18 of the 38 context pairs, and made only 2 errors.
Thisis a more stringent est than the first one, since providingthe correct answer in this case demands that both sentencesbe correctly understood.
One of the errors was due to thesame system bug describe above, i.e., the right answer wasdisplayed but not sent.
In the second one, which we con-sidered to he the only error made by the natural anguagesystem, the system simply ignored the context.The results for the Class A sentences with speech inputare given in Column 3 of Table 1.
Of the 19 sentences thatprovided an incorrect answer, 2 were correctly recognized,but failed due to the system bug mentioned above.We recently collected a sizable amount of spontaneousspeech data, using a paradigm very different from the oneused at TI.
Our preliminary analyses of the two data setshave indicated significant differences in many dimensions, in-cluding the speaking rate, vocabulary growth, and amountof spontaneous speech disfluencies \[3\].
We thought it mightbe interesting to compare our system's performance on thetwo data sets.
To this end, we asked B. Bly of SRI to helpus generate the CAS reference answers for the designateddevelopment-test set of our database.
The test set consists'of 371 sentences, of which 198 were classified by Ms. Bly asClass A.
Since'no aspects of our system had been trained onthese data, we consider it to be a legitimate test set for pur-poses of this experiment, although we plan to use it in thefuture as a development test set.The results for CAS output with both text and speech in-put for the MIT data are given in Table 2.
We should pointout that in an initial run of the text-input condition, severalanswers marked as "incorrect" were judged by us to be du-bious.
We submitted these questionable answers to Ms. Bly,as part of the customary follow-up process of "human aju-dication" established at NIST.
The results in Table 2 thusrepresent the final outcome.
Some of the discrepancies weredue to an error on the part of the reference answer, severalwere due to the "yes/no" vs. table problem, several were dueto the fact that SRI assumed 1990 for all dates, whereas most92I Data No.
of !Correct Incorrect No Answer Score }\]Set Sentences (%) (%) (%) (%)\]Class A Text 198 74.2 1.5 24.2 72.7\[\[ Class A Speech I 198 39.9 I 8.1 52.0 I 31.8 \[ITable 2: Results for 198 Class A utterances taken from the MITDevelopment Test Set, with CAS reference answers provided bySRI, for both text and speech input.of the dates were actually in January of 1991.For the text-in/CAS-out test condition, we obtained anoverall score of 72.7%, which is a dramatic improvement overour results on the TI data.
In two of the three errors, theback-end ignored certain critical modifiers in the frame.
Thethird error was fairly subtle: we interpreted "I'd like to book aflight between Boston and San Francisco with stops in Denverand Atlanta," to mean or rather than and for the stops.We produced a correct answer for almost 40% of the ut-terances when the speech recognizer was included in the sys-tem, with an 8% false alarm rate.
This gave an overall scoreof 32%, which is again substantially higher than the 18.6%score we received for the recognizer results on the standardtest set.
The MIT test was run after some bug fixes, whichwould have improved the score for the TI data to 24% (seeTable 3).
However, this is still substantially ower than thescore for the MIT set.
This is all the more surprising since theMIT test data were not prescreened for speech disfluencies s -we included all of the Class A sentences of each test speaker.There are several possible xplanations for the discrepancy.We believe that the MIT sentences are spoken more fluently,as suggested by the results of a statistical analysis reported in\[3\].
We also suspect that MIT subjects tend to use constructsthat are more straightforward and conform more closely tostandard English.
Finally~ the MIT sentences include veryfew table clarification questions, a feature which allowed usto reduce the size and perplexity of our grammar.In general, the speech recognition error rate for our sys-tem is significantly higher in the ATIS domain than what wehave experienced with the Resource Management domain.One conclusion we may draw is that spontaneous speech,with out-of-domain words and novel linguistic constructs,can combine to degrade recognition performance drastically.There are at least several other reasons that contribute to ourincreased speech recognition error rate.
The version of ourrecognizer in the ATIS system used only context-independentphoneme models.
This is because we have focused our re-search attention in speech recognition primarily on the Re-source Management task, and did not really devote any effortto the ATIS domain until late January.
The word-pair lan-guage model that we developed has a coverage of only 50% ofSThe TI class A sentences were screened to exclude disfiuencies forthe basic test set.Data No.
of CorlSub Del Ins ErrorSet Sentences (%) (%) (%) (%)(%)TI Feb 91 145 65.2 28.1 6.8 8.8 43.6MIT Development 198 78 18 4 3 25.6Table 3: Speech recognition resultsthe TI test set sentences.
It also has a perplexity of over 90,which is higher by a factor of at least four compared to whatothers have used (see, for example, \[2\]).
In the next fewmonths, we intend to incorporate context-dependent mod-elling to the ATIS domain.
We will also replace the word-pairlanguage model with a bigram so as to increase the coverageand lower the perplexity.SUMMARYThis paper gives a current status of our ATIS developmenteffort.
A significant change since the last workshop is that thesystem can now accept verbal input.
We have also rewrittenthe back-end component, adopting an approach that shouldpromote extension and portability.
We have extended ourevaluations this time to include both Class A and Class D1sentences.
Our evaluations for both speech and text inputwere performed on two different data sets, collected at TI andMIT.
The system's performance appears to depend stronglyon the conditions under which the data were collected.REFERENCES\[1\] Hirschman, L., Seneff, S., Goodine, D., Phillips, M., "Inte-grating Syntax and Semantics into Spoken Language Under-standing," These Proceedings.\[2\] Paul, D. B., "New Results with the Lincoln Tied-MixturettMM CSR System," These Proceedings.\[3\] Polifroni, J., Seneff, S., and Zue, V. W., "Collection of Spon-taneous Speech for the hwIS Domain and Comparative Anal-yses of Data Collected at MIT and TI," These Proceedings.\[4\] Seneff, S., IIirschman, L., and Zue, V. W., "Interactive Prob-lem Solving and Dialogue in the ATIS Domain," These Pro-ceedings.\[5\] Zne, V.,Glass, J., Goodine, D., Lenng, I-I., PhiUips, M., Po-lifroni, J., and Seneff, S., "Preliminary ATIS Development aMIT," Third DARPA Speech and Natural Language Work-shop, Hidden Vailey, PA, June 1990.\[6\] Zue, V., Glass, J., Goodine, D., Leung, If., McCandleas, M.,Phillips, M., Polifroni, J., and Seneff~ S., "Recent Progress onthe VOYAGER system," Third DAI~PA Speech and NaturalLanguage Workshop, IIidden Valley, PA, June 1990.93
