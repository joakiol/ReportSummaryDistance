Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 576?584,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPJoint Decoding with Multiple Translation ModelsYang Liu and Haitao Mi and Yang Feng and Qun LiuKey Laboratory of Intelligent Information ProcessingInstitute of Computing TechnologyChinese Academy of SciencesP.O.
Box 2704, Beijing 100190, China{yliu,htmi,fengyang,liuqun}@ict.ac.cnAbstractCurrent SMT systems usually decode withsingle translation models and cannot ben-efit from the strengths of other models indecoding phase.
We instead propose jointdecoding, a method that combines multi-ple translation models in one decoder.
Ourjoint decoder draws connections amongmultiple models by integrating the trans-lation hypergraphs they produce individu-ally.
Therefore, one model can share trans-lations and even derivations with othermodels.
Comparable to the state-of-the-artsystem combination technique, joint de-coding achieves an absolute improvementof 1.5 BLEU points over individual decod-ing.1 IntroductionSystem combination aims to find consensus trans-lations among different machine translation sys-tems.
It proves that such consensus translationsare usually better than the output of individual sys-tems (Frederking and Nirenburg, 1994).Recent several years have witnessed the rapiddevelopment of system combination methodsbased on confusion networks (e.g., (Rosti et al,2007; He et al, 2008)), which show state-of-the-art performance in MT benchmarks.
A confusionnetwork consists of a sequence of sets of candidatewords.
Each candidate word is associated with ascore.
The optimal consensus translation can beobtained by selecting one word from each set ofcandidates to maximizing the overall score.
Whileit is easy and efficient to manipulate strings, cur-rent methods usually have no access to most infor-mation available in decoding phase, which mightbe useful for obtaining further improvements.In this paper, we propose a framework for com-bining multiple translation models directly in de-coding phase.
1 Based on max-translation decod-ing and max-derivation decoding used in conven-tional individual decoders (Section 2), we go fur-ther to develop a joint decoder that integrates mul-tiple models on a firm basis:?
Structuring the search space of each modelas a translation hypergraph (Section 3.1),our joint decoder packs individual translationhypergraphs together by merging nodes thathave identical partial translations (Section3.2).
Although such translation-level combi-nation will not produce new translations, itdoes change the way of selecting promisingcandidates.?
Two models could even share derivationswith each other if they produce the samestructures on the target side (Section 3.3),which we refer to as derivation-level com-bination.
This method enlarges the searchspace by allowing for mixing different typesof translation rules within one derivation.?
As multiple derivations are used for findingoptimal translations, we extend the minimumerror rate training (MERT) algorithm (Och,2003) to tune feature weights with respectto BLEU score for max-translation decoding(Section 4).We evaluated our joint decoder that integrateda hierarchical phrase-based model (Chiang, 2005;Chiang, 2007) and a tree-to-string model (Liu etal., 2006) on the NIST 2005 Chinese-English test-set.
Experimental results show that joint decod-1It might be controversial to use the term ?model?, whichusually has a very precise definition in the field.
Someresearchers prefer to saying ?phrase-based approaches?
or?phrase-based systems?.
On the other hand, other authors(e.g., (Och and Ney, 2004; Koehn et al, 2003; Chiang, 2007))do use the expression ?phrase-based models?.
In this paper,we use the term ?model?
to emphasize that we integrate dif-ferent approaches directly in decoding phase rather than post-processing system outputs.576S ?
?X1,X1?X ?
?fabiao X1, give a X1?X ?
?yanjiang, talk?Figure 1: A derivation composed of SCFG rulesthat translates a Chinese sentence ?fabiao yan-jiang?
into an English sentence ?give a talk?.ing with multiple models achieves an absolute im-provement of 1.5 BLEU points over individual de-coding with single models (Section 5).2 BackgroundStatistical machine translation is a decision prob-lem where we need decide on the best of targetsentence matching a source sentence.
The processof searching for the best translation is convention-ally called decoding, which usually involves se-quences of decisions that translate a source sen-tence into a target sentence step by step.For example, Figure 1 shows a sequence ofSCFG rules (Chiang, 2005; Chiang, 2007) thattranslates a Chinese sentence ?fabiao yanjiang?into an English sentence ?give a talk?.
Such se-quence of decisions is called a derivation.
Inphrase-based models, a decision can be translatinga source phrase into a target phrase or reorderingthe target phrases.
In syntax-based models, deci-sions usually correspond to transduction rules.
Of-ten, there are many derivations that are distinct yetproduce the same translation.Blunsom et al (2008) present a latent vari-able model that describes the relationship betweentranslation and derivation clearly.
Given a sourcesentence f , the probability of a target sentence ebeing its translation is the sum over all possiblederivations:Pr(e|f) =?d??
(e,f)Pr(d, e|f) (1)where ?
(e, f) is the set of all possible derivationsthat translate f into e and d is one such derivation.They use a log-linear model to define the con-ditional probability of a derivation d and corre-sponding translation e conditioned on a sourcesentence f :Pr(d, e|f) = exp?m ?mhm(d, e, f)Z(f) (2)where hm is a feature function, ?m is the asso-ciated feature weight, and Z(f) is a constant fornormalization:Z(f) =?e?d??
(e,f)exp?m?mhm(d, e, f) (3)A feature value is usually decomposed as theproduct of decision probabilities: 2h(d, e, f) =?d?dp(d) (4)where d is a decision in the derivation d.Although originally proposed for supportinglarge sets of non-independent and overlapping fea-tures, the latent variable model is actually a moregeneral form of conventional linear model (Ochand Ney, 2002).Accordingly, decoding for the latent variablemodel can be formalized ase?
= argmaxe{?d??
(e,f)exp?m?mhm(d, e, f)}(5)where Z(f) is not needed in decoding because itis independent of e.Most SMT systems approximate the summa-tion over all possible derivations by using 1-bestderivation for efficiency.
They search for the 1-best derivation and take its target yield as the besttranslation:e?
?
argmaxe,d{?m?mhm(d, e, f)}(6)We refer to Eq.
(5) as max-translation decodingand Eq.
(6) as max-derivation decoding, which arefirst termed by Blunsom et al (2008).By now, most current SMT systems, adoptingeither max-derivation decoding or max-translationdecoding, have only used single models in decod-ing phase.
We refer to them as individual de-coders.
In the following section, we will presenta new method called joint decoding that includesmultiple models in one decoder.3 Joint DecodingThere are two major challenges for combiningmultiple models directly in decoding phase.
First,they rely on different kinds of knowledge sources2There are also features independent of derivations, suchas language model and word penalty.577Sgive0-1talk1-2give a talk0-2give talks0-2Sgive0-1speech1-2give a talk0-2make a speech0-2Sgive0-1talk1-2speech1-2give a talk0-2give talks0-2make a speech0-2packing(a) (b)(c)Figure 2: (a) A translation hypergraph produced by one model; (b) a translation hypergraph produced byanother model; (c) the packed translation hypergraph based on (a) and (b).
Solid and dashed lines denotethe translation rules of the two models, respectively.
Shaded nodes occur in both (a) and (b), indicatingthat the two models produce the same translations.and thus need to collect different information dur-ing decoding.
For example, taking a source parseas input, a tree-to-string decoder (e.g., (Liu et al,2006)) pattern-matches the source parse with tree-to-string rules and produces a string on the tar-get side.
On the contrary, a string-to-tree decoder(e.g., (Galley et al, 2006; Shen et al, 2008)) is aparser that applies string-to-tree rules to obtain atarget parse for the source string.
As a result, thehypothesis structures of the two models are funda-mentally different.Second, translation models differ in decodingalgorithms.
Depending on the generating orderof a target sentence, we distinguish between twomajor categories: left-to-right and bottom-up.
De-coders that use rules with flat structures (e.g.,phrase pairs) usually generate target sentencesfrom left to right while those using rules with hier-archical structures (e.g., SCFG rules) often run ina bottom-up style.In response to the two challenges, we first ar-gue that the search space of an arbitrary model canbe structured as a translation hypergraph, whichmakes each model connectable to others (Section3.1).
Then, we show that a packed translation hy-pergraph that integrates the hypergraphs of indi-vidual models can be generated in a bottom-uptopological order, either integrated at the transla-tion level (Section 3.2) or the derivation level (Sec-tion 3.3).3.1 Translation HypergraphDespite the diversity of translation models, they allhave to produce partial translations for substringsof input sentences.
Therefore, we represent thesearch space of a translation model as a structurecalled translation hypergraph.Figure 2(a) demonstrates a translation hyper-graph for one model, for example, a hierarchicalphrase-based model.
A node in a hypergraph de-notes a partial translation for a source substring,except for the starting node ?S?.
For example,given the example source sentence0 fabiao 1 yanjiang 2the node ?
?give talks?, [0, 2]?
in Figure 2(a) de-notes that ?give talks?
is one translation of thesource string f21 = ?fabiao yanjiang?.The hyperedges between nodes denote the deci-sion steps that produce head nodes from tail nodes.For example, the incoming hyperedge of the node?
?give talks?, [0, 2]?
could correspond to an SCFGrule:X ?
?X1 yanjiang,X1 talks?Each hyperedge is associated with a number ofweights, which are the feature values of the corre-sponding translation rules.
A path of hyperedgesconstitutes a derivation.578Hypergraph Decodingnode translationhyperedge rulepath derivationTable 1: Correspondence between translation hy-pergraph and decoding.More formally, a hypergraph (Klein and Man-ning., 2001; Huang and Chiang, 2005) is a tuple?V,E,R?, where V is a set of nodes, E is a setof hyperedges, and R is a set of weights.
For agiven source sentence f = fn1 = f1 .
.
.
fn, eachnode v ?
V is in the form of ?t, [i, j]?, which de-notes the recognition of t as one translation of thesource substring spanning from i through j (thatis, fi+1 .
.
.
fj).
Each hyperedge e ?
E is a tuplee = ?tails(e), head(e), w(e)?, where head(e) ?V is the consequent node in the deductive step,tails(e) ?
V ?
is the list of antecedent nodes, andw(e) is a weight function from R|tails(e)| to R.As a general representation, a translation hyper-graph is capable of characterizing the search spaceof an arbitrary translation model.
Furthermore,it offers a graphic interpretation of decoding pro-cess.
A node in a hypergraph denotes a translation,a hyperedge denotes a decision step, and a pathof hyperedges denotes a derivation.
A translationhypergraph is formally a semiring as the weightof a path is the product of hyperedge weights andthe weight of a node is the sum of path weights.While max-derivation decoding only retains thesingle best path at each node, max-translation de-coding sums up all incoming paths.
Table 1 sum-marizes the relationship between translation hy-pergraph and decoding.3.2 Translation-Level CombinationThe conventional interpretation of Eq.
(1) is thatthe probability of a translation is the sum over allpossible derivations coming from the same model.Alternatively, we interpret Eq.
(1) as that thederivations could come from different models.3This forms the theoretical basis of joint decoding.Although the information inside a derivationdiffers widely among translation models, the be-ginning and end points (i.e., f and e, respectively)must be identical.
For example, a tree-to-string3The same for all d occurrences in Section 2.
For exam-ple, ?
(e, f) might include derivations from various modelsnow.
Note that we still use Z for normalization.model first parses f to obtain a source tree T (f)and then transforms T (f) to the target sentencee.
Conversely, a string-to-tree model first parsesf into a target tree T (e) and then takes the surfacestring e as the translation.
Despite different inside,their derivations must begin with f and end with e.This situation remains the same for derivationsbetween a source substring f ji and its partial trans-lation t during joint decoding:Pr(t|f ji ) =?d??
(t,fji )Pr(d, t|f ji ) (7)where d might come from multiple models.
Inother words, derivations from multiple modelscould be brought together for computing the prob-ability of one partial translation.Graphically speaking, joint decoding creates apacked translation hypergraph that combines in-dividual hypergraphs by merging nodes that haveidentical translations.
For example, Figure 2 (a)and (b) demonstrate two translation hypergraphsgenerated by two models respectively and Fig-ure 2 (c) is the resulting packed hypergraph.
Thesolid lines denote the hyperedges of the first modeland the dashed lines denote those of the secondmodel.
The shaded nodes are shared by both mod-els.
Therefore, the two models are combined at thetranslation level.
Intuitively, shared nodes shouldbe favored in decoding because they offer consen-sus translations among different models.Now the question is how to decode with multi-ple models jointly in just one decoder.
We believethat both left-to-right and bottom-up strategies canbe used for joint decoding.
Although phrase-baseddecoders usually produce translations from left toright, they can adopt bottom-up decoding in prin-ciple.
Xiong et al (2006) develop a bottom-up de-coder for BTG (Wu, 1997) that uses only phrasepairs.
They treat reordering of phrases as a binaryclassification problem.
On the other hand, it ispossible for syntax-based models to decode fromleft to right.
Watanabe et al (2006) propose left-to-right target generation for hierarchical phrase-based translation.
Although left-to-right decod-ing might enable a more efficient use of languagemodels and hopefully produce better translations,we adopt bottom-up decoding in this paper just forconvenience.Figure 3 demonstrates the search algorithm ofour joint decoder.
The input is a source languagesentence fn1 , and a set of translation models M5791: procedure JOINTDECODING(fn1 , M )2: G?
?3: for l ?
1 .
.
.
n do4: for all i, j s.t.
j ?
i = l do5: for all m ?M do6: ADD(G, i, j,m)7: end for8: PRUNE(G, i, j)9: end for10: end for11: end procedureFigure 3: Search algorithm for joint decoding.
(line 1).
After initializing the translation hyper-graph G (line 2), the decoder runs in a bottom-up style, adding nodes for each span [i, j] and foreach model m. For each span [i, j] (lines 3-5),the procedure ADD(G, i, j,m) add nodes gener-ated by the model m to the hypergraph G (line 6).Each model searches for partial translations inde-pendently: it uses its own knowledge sources andvisits its own antecedent nodes, just running likea bottom-up individual decoder.
After all mod-els finishes adding nodes for span [i, j], the pro-cedure PRUNE(G, i, j) merges identical nodes andremoves less promising nodes to control the searchspace (line 8).
The pruning strategy is similar tothat of individual decoders, except that we requirethere must exist at least one node for each modelto ensure further inference.Although translation-level combination will notoffer new translations as compared to single mod-els, it changes the way of selecting promising can-didates in a combined search space and might po-tentially produce better translations than individ-ual decoding.3.3 Derivation-Level CombinationIn translation-level combination, different modelsinteract with each other only at the nodes.
Thederivations of one model are unaccessible to othermodels.
However, if two models produce the samestructures on the target side, it is possible to com-bine two models within one derivation, which werefer to as derivation-level combination.For example, although different on the sourceside, both hierarchical phrase-based and tree-to-string models produce strings of terminals andnonterminals on the target side.
Figure 4 showsa derivation composed of both hierarchical phraseIP(x1:VV, x2:NN) ?
x1 x2X ?
?fabiao, give?X ?
?yanjiang, a talk?Figure 4: A derivation composed of both SCFGand tree-to-string rules.pairs and tree-to-string rules.
Hierarchical phrasepairs are used for translating smaller units andtree-to-string rules for bigger ones.
It is appealingto combine them in such a way because the hierar-chical phrase-based model provides excellent rulecoverage while the tree-to-string model offers lin-guistically motivated non-local reordering.
Sim-ilarly, Blunsom and Osborne (2008) use both hi-erarchical phrase pairs and tree-to-string rules indecoding, where source parse trees serve as condi-tioning context rather than hard constraints.Depending on the target side output, we dis-tinguish between string-targeted and tree-targetedmodels.
String-targeted models include phrase-based, hierarchical phrase-based, and tree-to-string models.
Tree-targeted models includestring-to-tree and tree-to-tree models.
All modelscan be combined at the translation level.
Modelsthat share with same target output structure can befurther combined at the derivation level.The joint decoder usually runs as max-translation decoding because multiple derivationsfrom various models are used.
However, if allmodels involved belong to the same category, ajoint decoder can also adopt the max-derivationfashion because all nodes and hyperedges are ac-cessible now (Section 5.2).Allowing derivations for comprising rules fromdifferent models and integrating their strengths,derivation-level combination could hopefully pro-duce new and better translations as compared withsingle models.4 Extended Minimum Error RateTrainingMinimum error rate training (Och, 2003) is widelyused to optimize feature weights for a linear model(Och and Ney, 2002).
The key idea of MERT isto tune one feature weight to minimize error rateeach time while keep others fixed.
Therefore, each580xf(x)t1t2t3(0, 0) x1 x2Figure 5: Calculation of critical intersections.candidate translation can be represented as a line:f(x) = a?
x + b (8)where a is the feature value of current dimension,x is the feature weight being tuned, and b is thedotproduct of other dimensions.
The intersectionof two lines is where the candidate translation willchange.
Instead of computing all intersections,Och (2003) only computes critical intersectionswhere highest-score translations will change.
Thismethod reduces the computational overhead sig-nificantly.Unfortunately, minimum error rate training can-not be directly used to optimize feature weights ofmax-translation decoding because Eq.
(5) is not alinear model.
However, if we also tune one dimen-sion each time and keep other dimensions fixed,we obtain a monotonic curve as follows:f(x) =K?k=1eak?x+bk (9)where K is the number of derivations for a can-didate translation, ak is the feature value of cur-rent dimension on the kth derivation and bk is thedotproduct of other dimensions on the kth deriva-tion.
If we restrict that ak is always non-negative,the curve shown in Eq.
(9) will be a monotoni-cally increasing function.
Therefore, it is possibleto extend the MERT algorithm to handle situationswhere multiple derivations are taken into accountfor decoding.The key difference is the calculation of criti-cal intersections.
The major challenge is that twocurves might have multiple intersections whiletwo lines have at most one intersection.
Fortu-nately, as the curve is monotonically increasing,we need only to find the leftmost intersection ofa curve with other curves that have greater valuesafter the intersection as a candidate critical inter-section.Figure 5 demonstrates three curves: t1, t2, andt3.
Suppose that the left bound of x is 0, we com-pute the function values for t1, t2, and t3 at x = 0and find that t3 has the greatest value.
As a result,we choose x = 0 as the first critical intersection.Then, we compute the leftmost intersections of t3with t1 and t2 and choose the intersection closestto x = 0, that is x1, as our new critical intersec-tion.
Similarly, we start from x1 and find x2 as thenext critical intersection.
This iteration continuesuntil it reaches the right bound.
The bold curve de-notes the translations we will choose over differentranges.
For example, we will always choose t2 forthe range [x1, x2].To compute the leftmost intersection of twocurves, we divide the range from current criticalintersection to the right bound into many bins (i.e.,smaller ranges) and search the bins one by onefrom left to right.
We assume that there is at mostone intersection in each bin.
As a result, we canuse the Bisection method for finding the intersec-tion in each bin.
The search process ends immedi-ately once an intersection is found.We divide max-translation decoding into threephases: (1) build the translation hypergraphs, (2)generate n-best translations, and (3) generate n?-best derivations.
We apply Algorithm 3 of Huangand Chiang (2005) for n-best list generation.
Ex-tended MERT runs on n-best translations plus n?-best derivations to optimize the feature weights.Note that feature weights of various models aretuned jointly in extended MERT.5 Experiments5.1 Data PreparationOur experiments were on Chinese-to-Englishtranslation.
We used the FBIS corpus (6.9M +8.9M words) as the training corpus.
For lan-guage model, we used the SRI Language Mod-eling Toolkit (Stolcke, 2002) to train a 4-grammodel on the Xinhua portion of GIGAWORD cor-pus.
We used the NIST 2002 MT Evaluation testset as our development set, and used the NIST2005 test set as test set.
We evaluated the trans-lation quality using case-insensitive BLEU metric(Papineni et al, 2002).Our joint decoder included two models.
The581Max-derivation Max-translationModel Combination Time BLEU Time BLEUhierarchical N/A 40.53 30.11 44.87 29.82tree-to-string N/A 6.13 27.23 6.69 27.11translation N/A N/A 55.89 30.79both derivation 48.45 31.63 54.91 31.49Table 2: Comparison of individual decoding and joint decoding on average decoding time (sec-onds/sentence) and BLEU score (case-insensitive).first model was the hierarchical phrase-basedmodel (Chiang, 2005; Chiang, 2007).
We obtainedword alignments of training data by first runningGIZA++ (Och and Ney, 2003) and then applyingthe refinement rule ?grow-diag-final-and?
(Koehnet al, 2003).
About 2.6M hierarchical phrase pairsextracted from the training corpus were used onthe test set.Another model was the tree-to-string model(Liu et al, 2006; Liu et al, 2007).
Based onthe same word-aligned training corpus, we ran aChinese parser on the source side to obtain 1-bestparses.
For 15,157 sentences we failed to obtain1-best parses.
Therefore, only 93.7% of the train-ing corpus were used by the tree-to-string model.About 578K tree-to-string rules extracted from thetraining corpus were used on the test set.5.2 Individual Decoding Vs. Joint DecodingTable 2 shows the results of comparing individ-ual decoding and joint decoding on the test set.With conventional max-derivation decoding, thehierarchical phrase-based model achieved a BLEUscore of 30.11 on the test set, with an average de-coding time of 40.53 seconds/sentence.
We foundthat accounting for all possible derivations in max-translation decoding resulted in a small negativeeffect on BLEU score (from 30.11 to 29.82), eventhough the feature weights were tuned with respectto BLEU score.
One possible reason is that weonly used n-best derivations instead of all possi-ble derivations for minimum error rate training.Max-derivation decoding with the tree-to-stringmodel yielded much lower BLEU score (i.e.,27.23) than the hierarchical phrase-based model.One reason is that the tree-to-string model failsto capture a large amount of linguistically unmo-tivated mappings due to syntactic constraints.
An-other reason is that the tree-to-string model onlyused part of the training data because of pars-ing failure.
Similarly, accounting for all possible0.00.10.20.30.40.50.60.70.80.91.00  1  2  3  4  5  6  7  8  9  10  11percentagespan widthFigure 6: Node sharing in max-translation de-coding with varying span widths.
We retain atmost 100 nodes for each source substring for eachmodel.derivations in max-translation decoding failed tobring benefits for the tree-to-string model (from27.23 to 27.11).When combining the two models at the trans-lation level, the joint decoder achieved a BLEUscore of 30.79 that outperformed the best result(i.e., 30.11) of individual decoding significantly(p < 0.05).
This suggests that accounting forall possible derivations from multiple models willhelp discriminate among candidate translations.Figure 6 demonstrates the percentages of nodesshared by the two models over various span widthsin packed translation hypergraphs during max-translation decoding.
For one-word source strings,89.33% nodes in the hypergrpah were shared byboth models.
With the increase of span width, thepercentage decreased dramatically due to the di-versity of the two models.
However, there still ex-ist nodes shared by two models even for sourcesubstrings that contain 33 words.When combining the two models at the deriva-tion level using max-derivation decoding, the jointdecoder achieved a BLEU score of 31.63 that out-performed the best result (i.e., 30.11) of individ-582Method Model BLEUhierarchical 30.11individual decodingtree-to-string 27.23system combination both 31.50joint decoding both 31.63Table 3: Comparison of individual decoding, sys-tem combination, and joint decoding.ual decoding significantly (p < 0.01).
This im-provement resulted from the mixture of hierarchi-cal phrase pairs and tree-to-string rules.
To pro-duce the result, the joint decoder made use of8,114 hierarchical phrase pairs learned from train-ing data, 6,800 glue rules connecting partial trans-lations monotonically, and 16,554 tree-to-stringrules.
While tree-to-string rules offer linguisticallymotivated non-local reordering during decoding,hierarchical phrase pairs ensure good rule cover-age.
Max-translation decoding still failed to sur-pass max-derivation decoding in this case.5.3 Comparison with System CombinationWe re-implemented a state-of-the-art system com-bination method (Rosti et al, 2007).
As shownin Table 3, taking the translations of the two indi-vidual decoders as input, the system combinationmethod achieved a BLEU score of 31.50, slightlylower than that of joint decoding.
But this differ-ence is not significant statistically.5.4 Individual Training Vs. Joint TrainingTable 4 shows the effects of individual training andjoint training.
By individual, we mean that the twomodels are trained independently.
We concatenateand normalize their feature weights for the jointdecoder.
By joint, we mean that they are trainedtogether by the extended MERT algorithm.
Wefound that joint training outperformed individualtraining significantly for both max-derivation de-coding and max-translation decoding.6 Related WorkSystem combination has benefited various NLPtasks in recent years, such as products-of-experts(e.g., (Smith and Eisner, 2005)) and ensemble-based parsing (e.g., (Henderson and Brill, 1999)).In machine translation, confusion-network basedcombination techniques (e.g., (Rosti et al, 2007;He et al, 2008)) have achieved the state-of-the-art performance in MT evaluations.
From a dif-Training Max-derivation Max-translationindividual 30.70 29.95joint 31.63 30.79Table 4: Comparison of individual training andjoint training.ferent perspective, we try to combine different ap-proaches directly in decoding phase by using hy-pergraphs.
While system combination techniquesmanipulate only the final translations of each sys-tem, our method opens the possibility of exploit-ing much more information.Blunsom et al (2008) first distinguish betweenmax-derivation decoding and max-translation de-coding explicitly.
They show that max-translationdecoding outperforms max-derivation decodingfor the latent variable model.
While they train theparameters using a maximum a posteriori estima-tor, we extend the MERT algorithm (Och, 2003)to take the evaluation metric into account.Hypergraphs have been successfully used inparsing (Klein and Manning., 2001; Huang andChiang, 2005; Huang, 2008) and machine trans-lation (Huang and Chiang, 2007; Mi et al, 2008;Mi and Huang, 2008).
Both Mi et al (2008) andBlunsom et al (2008) use a translation hyper-graph to represent search space.
The difference isthat their hypergraphs are specifically designed forthe forest-based tree-to-string model and the hier-archical phrase-based model, respectively, whileours is more general and can be applied to arbi-trary models.7 ConclusionWe have presented a framework for including mul-tiple translation models in one decoder.
Repre-senting search space as a translation hypergraph,individual models are accessible to others via shar-ing nodes and even hyperedges.
As our decoderaccounts for multiple derivations, we extend theMERT algorithm to tune feature weights with re-spect to BLEU score for max-translation decod-ing.
In the future, we plan to optimize featureweights for max-translation decoding directly onthe entire packed translation hypergraph ratherthan on n-best derivations, following the lattice-based MERT (Macherey et al, 2008).583AcknowledgementThe authors were supported by National NaturalScience Foundation of China, Contracts 60873167and 60736014, and 863 State Key Project No.2006AA010108.
Part of this work was done whileYang Liu was visiting the SMT group led byStephan Vogel at CMU.
We thank the anonymousreviewers for their insightful comments.
We arealso grateful to Yajuan Lu?, Liang Huang, NguyenBach, Andreas Zollmann, Vamshi Ambati, andKevin Gimpel for their helpful feedback.ReferencesPhil Blunsom and Mile Osborne.
2008.
Probabilis-tic inference for machine translation.
In Proc.
ofEMNLP08.Phil Blunsom, Trevor Cohn, and Miles Osborne.
2008.A discriminative latent variable model for statisticalmachine translation.
In Proc.
of ACL08.David Chiang.
2005.
A hierarchical phrase-basedmodel for statistical machine translation.
In Proc.of ACL05.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2).Robert Frederking and Sergei Nirenburg.
1994.
Threeheads are better than one.
In Proc.
of ANLP94.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Proc.of ACL06.Xiaodong He, Mei Yang, Jianfeng Gao, PatrickNguyen, and Robert Moore.
2008.
Indirect-HMM-based hypothesis alignment for combining outputsfrom machine translation systems.
In Proc.
ofEMNLP08.John C. Henderson and Eric Brill.
1999.
Exploitingdiversity in natural language processing: Combiningparsers.
In Proc.
of EMNLP99.Liang Huang and David Chiang.
2005.
Better k-bestparsing.
In Proc.
of IWPT05.Liang Huang and David Chiang.
2007.
Forest rescor-ing: Faster decoding with integrated language mod-els.
In Proc.
of ACL07.Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In Proc.
of ACL08.Dan Klein and Christopher D. Manning.
2001.
Parsingand hypergraphs.
In Proc.
of ACL08.Phillip Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proc.
ofNAACL03.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machinetranslation.
In Proc.
of ACL06.Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin.2007.
Forest-to-string statistical translation rules.
InProc.
of ACL07.Wolfgang Macherey, Franz J. Och, Ignacio Thayer, andJakob Uszkoreit.
2008.
Lattice-based minimum er-ror rate training for statistical machine translation.In Proc.
of EMNLP08.Haitao Mi and Liang Huang.
2008.
Forest-based trans-lation rule extraction.
In Proc.
of EMNLP08.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-based translation.
In Proc.
of ACL08.Franz J. Och and Hermann Ney.
2002.
Discriminativetraining and maximum entropy models for statisticalmachine translation.
In Proc.
of ACL02.Franz J. Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.Computational Linguistics, 29(1).Franz J. Och and Hermann Ney.
2004.
The alignmenttemplate approach to statistical machine translation.Computational Linguistics, 30(4).Franz J. Och.
2003.
Minimum error rate training instatistical machine translation.
In Proc.
of ACL03.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proc.
of ACL02.Antti-Veikko Rosti, Spyros Matsoukas, and RichardSchwartz.
2007.
Improved word-level system com-bination for machine translation.
In Proc.
of ACL07.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.
Anew string-to-dependency machine translation algo-rithm with a target dependency language model.
InProc.
of ACL08.Noah A. Smith and Jason Eisner.
2005.
Contrastiveestimation: Training log-linear models on unlabeleddata.
In Proc.
of ACL05.Andreas Stolcke.
2002.
Srilm - an extension languagemodel modeling toolkit.
In Proc.
of ICSLP02.Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.2006.
Left-to-right target generation for hierarchicalphrase-based translation.
In Proc.
of ACL06.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23.Deyi Xiong, Qun Liu, and Shouxun Lin.
2006.
Maxi-mum entropy based phrase reordering model for sta-tistical machine translation.
In Proc.
of ACL06.584
