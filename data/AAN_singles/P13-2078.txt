Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 440?445,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsMeasuring semantic content in distributional vectorsAure?lie HerbelotEB KognitionswissenschaftUniversita?t PotsdamGolm, Germanyaurelie.herbelot@cantab.netMohan GanesalingamTrinity CollegeUniversity of CambridgeCambridge, UKmohan0@gmail.comAbstractSome words are more contentful than oth-ers: for instance, make is intuitively moregeneral than produce and fifteen is more?precise?
than a group.
In this paper,we propose to measure the ?semantic con-tent?
of lexical items, as modelled bydistributional representations.
We inves-tigate the hypothesis that semantic con-tent can be computed using the Kullback-Leibler (KL) divergence, an information-theoretic measure of the relative entropyof two distributions.
In a task focus-ing on retrieving the correct ordering ofhyponym-hypernym pairs, the KL diver-gence achieves close to 80% precision butdoes not outperform a simpler (linguis-tically unmotivated) frequency measure.We suggest that this result illustrates therather ?intensional?
aspect of distributions.1 IntroductionDistributional semantics is a representation of lex-ical meaning that relies on a statistical analysisof the way words are used in corpora (Curran,2003; Turney and Pantel, 2010; Erk, 2012).
Inthis framework, the semantics of a lexical item isaccounted for by modelling its co-occurrence withother words (or any larger lexical context).
Therepresentation of a target word is thus a vector in aspace where each dimension corresponds to a pos-sible context.
The weights of the vector compo-nents can take various forms, ranging from sim-ple co-occurrence frequencies to functions such asPointwise Mutual Information (for an overview,see (Evert, 2004)).This paper investigates the issue of comput-ing the semantic content of distributional vectors.That is, we look at the ways we can distribution-ally express that make is a more general verb thanproduce, which is itself more general than, forinstance, weave.
Although the task is related tothe identification of hyponymy relations, it aimsto reflect a more encompassing phenomenon: wewish to be able to compare the semantic content ofwords within parts-of-speech where the standardnotion of hyponymy does not apply (e.g.
preposi-tions: see with vs. next to or of vs. concerning)and across parts-of-speech (e.g.
fifteen vs. group).The hypothesis we will put forward is that se-mantic content is related to notions of relative en-tropy found in information theory.
More specif-ically, we hypothesise that the more specific aword is, the more the distribution of the wordsco-occurring with it will differ from the baselinedistribution of those words in the language as awhole.
(A more intuitive way to phrase this is thatthe more specific a word is, the more informationit gives us about which other words are likely tooccur near it.)
The specific measure of differencethat we will use is the Kullback-Leibler divergenceof the distribution of words co-ocurring with thetarget word against the distribution of those wordsin the language as a whole.
We evaluate our hy-pothesis against a subset of the WordNet hierar-chy (given by (Baroni et al 2012)), relying on theintuition that in a hyponym-hypernym pair, the hy-ponym should have higher semantic content thanits hypernym.The paper is structured as follows.
We firstdefine our notion of semantic content and moti-vate the need for measuring semantic content indistributional setups.
We then describe the im-plementation of the distributional system we usein this paper, emphasising our choice of weight-ing measure.
We show that, using the compo-440nents of the described weighting measure, whichare both probability distributions, we can calculatethe relative entropy of a distribution by insertingthose probability distributions in the equation forthe Kullback-Leibler (KL) divergence.
We finallyevaluate the KL measure against a basic notion offrequency and conclude with some error analysis.2 Semantic contentAs a first approximation, we will define seman-tic content as informativeness with respect to de-notation.
Following Searle (1969), we will takea ?successful reference?
to be a speech act wherethe choice of words used by the speaker appropri-ately identifies a referent for the hearer.
Glossingover questions of pragmatics, we will assume thata more informative word is more likely to lead toa successful reference than a less informative one.That is, if Kim owns a cat and a dog, the identify-ing expression my cat is a better referent than mypet and so cat can be said to have more semanticcontent than pet.While our definition relies on reference, it alsoposits a correspondence between actual utterancesand denotation.
Given two possible identifying ex-pressions e1 and e2, e1 may be preferred in a par-ticular context, and so, context will be an indicatorof the amount of semantic content in an expres-sion.
In Section 5, we will produce an explicithypothesis for how the amount of semantic con-tent in a lexical item affects the contexts in whichit appears.A case where semantic content has a direct cor-respondence with a lexical relation is hyponymy.Here, the correspondence relies entirely on a basicnotion of extension.
For instance, it is clear thathammer is more contentful than tool because theextension of hammer is smaller than that of tool,and therefore more discriminating in a given iden-tifying expression (See Give me the hammer ver-sus Give me the tool).
But we can also talk aboutsemantic content in cases where the notion of ex-tension does not necessarily apply.
For example,it is not usual to talk of the extension of a prepo-sition.
However, in context, the use of a preposi-tion against another one might be more discrim-inating in terms of reference.
Compare a) Sandyis with Kim and b) Sandy is next to Kim.
Given aset of possible situations involving, say, Kim andSandy at a party, we could show that b) is morediscriminating than a), because it excludes the sit-uations where Sandy came to the party with Kimbut is currently talking to Kay at the other end ofthe room.
The fact that next to expresses physi-cal proximity, as opposed to just being in the samesituation, confers it more semantic content accord-ing to our definition.
Further still, there may be aneed for comparing the informativeness of wordsacross parts of speech (compare A group of/Fifteenpeople was/were waiting in front of the town hall).Although we will not discuss this in detail, thereis a notion of semantic content above the wordlevel which should naturally derive from compo-sition rules.
For instance, we would expect thecomposition of a given intersective adjective anda given noun to result into a phrase with a seman-tic content greater than that of its components (orat least equal to it).3 MotivationThe last few years have seen a growing interest indistributional semantics as a representation of lex-ical meaning.
Owing to their mathematical inter-pretation, distributions allow linguists to simulatehuman similarity judgements (Lund, Burgess andAtchley, 1995), and also reproduce some of thefeatures given by test subjects when asked to writedown the characteristics of a given concept (Ba-roni and Lenci, 2008).
In a distributional semanticspace, for instance, the word ?cat?
may be close to?dog?
or to ?tiger?, and its vector might have highvalues along the dimensions ?meow?, ?mouse?
and?pet?.
Distributional semantics has had great suc-cesses in recent years, and for many computationallinguists, it is an essential tool for modelling phe-nomena affected by lexical meaning.If distributional semantics is to be seen asa general-purpose representation, however, weshould evaluate it across all properties which wedeem relevant to a model of the lexicon.
We con-sider semantic content to be one such property.
Itunderlies the notion of hyponymy and naturallymodels our intuitions about the ?precision?
(as op-posed to ?vagueness?)
of words.Further, semantic content may be crucial insolving some fundamental problems of distribu-tional semantics.
As pointed out by McNally(2013), there is no easy way to define the notionof a function word and this has consequences fortheories where function words are not assigneda distributional representation.
McNally suggeststhat the most appropriate way to separate function441from content words might, in the end, involve tak-ing into account how much ?descriptive?
contentthey have.4 An implementation of a distributionalsystemThe distributional system we implemented for thispaper is close to the system of Mitchell and La-pata (2010) (subsequently M&L).
As backgrounddata, we use the British National Corpus (BNC) inlemmatised format.
Each lemma is followed by apart of speech according to the CLAWS tagset for-mat (Leech, Garside, and Bryant, 1994).
For ourexperiments, we only keep the first letter of eachpart-of-speech tag, thus obtaining broad categoriessuch as N or V. Furthermore, we only retain wordsin the following categories: nouns, verbs, adjec-tives and adverbs (punctuation is ignored).
Eacharticle in the corpus is converted into a 11-wordwindow format, that is, we are assuming that con-text in our system is defined by the five words pre-ceding and the five words following the target.To calculate co-occurrences, we use the follow-ing equations:freqci =?tfreqci,t (1)freqt =?cifreqci,t (2)freqtotal =?ci,tfreqci,t (3)The quantities in these equations represent thefollowing:freqci,t frequency of the context word ciwith the target word tfreqtotal total count of word tokensfreqt frequency of the target word tfreqci frequency of the context word ciAs in M&L, we use the 2000 most frequentwords in our corpus as the semantic space dimen-sions.
M&L calculate the weight of each contextterm in the distribution as follows:vi(t) =p(ci|t)p(ci)= freqci,t ?
freqtotalfreqt ?
freqci(4)We will not directly use the measure vi(t) as itis not a probability distribution and so is not suit-able for entropic analysis; instead our analysis willbe phrased in terms of the probability distributionsp(ci|t) and p(ci) (the numerator and denominatorin vi(t)).5 Semantic content as entropy: twomeasuresResnik (1995) uses the notion of information con-tent to improve on the standard edge countingmethods proposed to measure similarity in tax-onomies such as WordNet.
He proposes that theinformation content of a term t is given by the self-information measure ?
log p(t).
The idea behindthis measure is that, as the frequency of the termincreases, its informativeness decreases.
Althougha good first approximation, the measure cannot besaid to truly reflect our concept of semantic con-tent.
For instance, in the British National Corpus,time and see are more frequent than thing or mayand man is more frequent than part.
However, itseems intuitively right to say that time, see andman are more ?precise?
concepts than thing, mayand part respectively.
Or said otherwise, there isno indication that more general concepts occur inspeech more than less general ones.
We will there-fore consider self-information as a baseline.As we expect more specific words to be moreinformative about which words co-occur withthem, it is natural to try to measure the specificityof a word by using notions from information the-ory to analyse the probability distribution p(ci|t)associated with the word.
The standard notionof entropy is not appropriate for this purpose, be-cause it does not take account of the fact that thewords serving as semantic space dimensions mayhave different frequencies in language as a whole,i.e.
of the fact that p(ci) does not have a uniformdistribution.
Instead we need to measure the de-gree to which p(ci|t) differs from the context worddistribution p(ci).
An appropriate measure for thisis the Kullback-Leibler (KL) divergence or rela-tive entropy:DKL(P?Q) =?iln(P (i)Q(i))P (i) (5)By taking P (i) to be p(ci|t) and Q(i) to be p(ci)(as given by Equation 4), we calculate the rela-tive entropy of p(ci|t) and p(ci).
The measure isclearly informative: it reflects the way that t mod-ifies the expectation of seeing ci in the corpus.We hypothesise that when compared to the distri-bution p(ci), more informative words will have a442more ?distorted?
distribution p(ci|t) and that theKL divergence will reflect this.16 EvaluationIn Section 2, we defined semantic content as a no-tion encompassing various referential properties,including a basic concept of extension in caseswhere it is applicable.
However, we do not knowof a dataset providing human judgements over thegeneral informativeness of lexical items.
So in or-der to evaluate our proposed measure, we inves-tigate its ability to retrieve the right ordering ofhyponym pairs, which can be considered a subsetof the issue at hand.Our assumption is that if X is a hypernym ofY , then the information content in X will be lowerthan in Y (because it has a more ?general?
mean-ing).
So, given a pair of words {w1, w2} in aknown hyponymy relation, we should be able totell which of w1 or w2 is the hypernym by com-puting the respective KL divergences.We use the hypernym data provided by (Baroniet al 2012) as testbed for our experiment.2 Thisset of hyponym-hypernym pairs contains 1385 in-stances retrieved from the WordNet hierarchy.
Be-fore running our system on the data, we makeslight modifications to it.
First, as our distributionsare created over the British National Corpus, somespellings must be converted to British English: forinstance, color is replaced by colour.
Second, fiveof the nouns included in the test set are not in theBNC.
Those nouns are brethren, intranet, iPod,webcam and IX.
We remove the pairs containingthose words from the data.
Third, numbers such aseleven or sixty are present in the Baroni et alset asnouns, but not in the BNC.
Pairs containing sevensuch numbers are therefore also removed from thedata.
Finally, we encounter tagging issues withthree words, which we match to their BNC equiv-alents: acoustics and annals are matched to acous-tic and annal, and trouser to trousers.
These mod-ifications result in a test set of 1279 remainingpairs.We then calculate both the self-informationmeasure and the KL divergence of all terms in-1Note that KL divergence is not symmetric:DKL(p(ci|t)?p(ci))) is not necessarily equal toDKL(p(ci)?p(ci|t)).
The latter is inferior as a fewvery small values of p(ci|t) can have an inappropriately largeeffect on it.2The data is available at http://clic.cimec.unitn.it/Files/PublicData/eacl2012-data.zip.cluded in our test set.
In order to evaluate the sys-tem, we record whether the calculated entropiesmatch the order of each hypernym-hyponym pair.That is, we count a pair as correctly representedby our system if w1 is a hypernym of w2 andKL(w1) < KL(w2) (or, in the case of thebaseline, SI(w1) < SI(w2) where SI is self-information).Self-information obtains 80.8% precision on thetask, with the KL divergence lagging a little be-hind with 79.4% precision (the difference is notsignificant).
In other terms, both measures per-form comparably.
We analyse potential reasonsfor this disappointing result in the next section.7 Error analysisIt is worth reminding ourselves of the assumptionwe made with regard to semantic content.
Ourhypothesis was that with a ?more general?
targetword t, the p(ci|t) distribution would be fairlysimilar to p(ci).Manually checking some of the pairs whichwere wrongly classified by the KL divergence re-veals that our hypothesis might not hold.
For ex-ample, the pair beer ?
beverage is classified in-correctly.
When looking at the beverage distri-bution, it is clear that it does not conform to ourexpectations: it shows high vi(t) weights alongthe food, wine, coffee and tea dimensions, for in-stance, i.e.
there is a large difference betweenp(cfood) and p(cfood|t), etc.
Although beverageis an umbrella word for many various types ofdrinks, speakers of English use it in very partic-ular contexts.
So, distributionally, it is not a ?gen-eral word?.
Similar observations can be made for,e.g.
liquid (strongly associated with gas, presum-ably via coordination), anniversary (linked to theverb mark or the noun silver), or again projectile(co-occurring with weapon, motion and speed).The general point is that, as pointed out else-where in the literature (Erk, 2013), distributionsare a good representation of (some aspects of) in-tension, but they are less apt to model extension.3So a term with a large extension like beveragemay have a more restricted (distributional) inten-sion than a word with a smaller extension, such as3We qualify ?intension?
here, because in the sense of amapping from possible worlds to extensions, intension can-not be said to be provided by distributions: the distribution ofbeverage, it seems, does not allow us to successfully pick outall beverages in the real world.443beer.4Contributing to this issue, fixed phrases, namedentities and generally strong collocations skew ourdistributions.
So for instance, in the jewelry distri-bution, the most highly weighted context is mental(with vi(t) = 395.3) because of the music albumMental Jewelry.
While named entities could eas-ily be eliminated from the system?s results by pre-processing the corpus with a named entity recog-niser, the issue is not so simple when it comes tofixed phrases of a more compositional nature (e.g.army ant): excluding them might be detrimentalfor the representation (it is, after all, part of themeaning of ant that it can be used metaphoricallyto refer to people) and identifying such phrases isa non-trivial problem in itself.Some of the errors we observe may also berelated to word senses.
For instance, the wordmedium, to be found in the pair magazine ?medium, can be synonymous with middle, clair-voyant or again mode of communication.
In thesense of clairvoyant, it is clearly more specificthan in the sense intended in the test pair.
As dis-tributions do not distinguish between senses, thiswill have an effect on our results.8 ConclusionIn this paper, we attempted to define a mea-sure of distributional semantic content in or-der to model the fact that some words have amore general meaning than others.
We com-pared the Kullback-Leibler divergence to a sim-ple self-information measure.
Our experiments,which involved retrieving the correct ordering ofhyponym-hypernym pairs, had disappointing re-sults: the KL divergence was unable to outperformself-information, and both measures misclassifiedaround 20% of our testset.Our error analysis showed that several factorscontributed to the misclassifications.
First, distri-butions are unable to model extensional propertieswhich, in many cases, account for the feeling that aword is more general than another.
Second, strongcollocation effects can influence the measurementof information negatively: it is an open questionwhich phrases should be considered ?words-with-spaces?
when building distributions.
Finally, dis-4Although it is more difficult to talk of the extension ofe.g.
adverbials (very) or some adjectives (skillful), the generalpoint is that text is biased towards a certain usage of words,while the general meaning a competent speaker ascribes tolexical items does not necessarily follow this bias.tributional representations do not distinguish be-tween word senses, which in many cases is a de-sirable feature, but interferes with the task we sug-gested in this work.To conclude, we would like to stress that we donot think another information-theoretic measurewould perform hugely better than the KL diver-gence.
The point is that the nature of distributionalvectors makes them sensitive to word usage andthat, despite the general assumption behind dis-tributional semantics, word usage might not suf-fice to model all aspects of lexical semantics.
Weleave as an open problem the issue of whether amodified form of our ?basic?
distributional vectorswould encode the right information.AcknowledgementsThis work was funded by a postdoctoral fellow-ship from the Alexander von Humboldt Founda-tion to the first author, and a Title A Fellowshipfrom Trinity College, Cambridge, to the secondauthor.ReferencesBaroni, Marco, and Lenci, Alessandro.
2008.
Con-cepts and properties in word spaces.
In Alessan-dro Lenci (ed.
), From context to meaning: Distribu-tional models of the lexicon in linguistics and cog-nitive science (Special issue of the Italian Journal ofLinguistics 20(1)), pages 55?88.Baroni, Marco, Raffaella Bernardi, Ngoc-Quynh Doand Chung-chieh Shan.
2012.
Entailment above theword level in distributional semantics.
In Proceed-ings of the 13th Conference of the European Chap-ter of the Association for Computational Linguistics(EACL2012), pages 23?32.Baroni, Marco, Raffaella Bernardi, and Roberto Zam-parelli.
2012.
Frege in Space: a Program for Com-positional Distributional Semantics.
Under review.Curran, James.
2003.
From Distributional to SemanticSimilarity.
Ph.D. thesis, University of Edinburgh,Scotland, UK.Erk, Katrin.
2012.
Vector space models of word mean-ing and phrase meaning: a survey.
Language andLinguistics Compass, 6:10:635?653.Erk, Katrin.
2013.
Towards a semantics for distribu-tional representations.
In Proceedings of the TenthInternational Conference on Computational Seman-tics (IWCS2013).Evert, Stefan.
2004.
The statistics of word cooccur-rences: word pairs and collocations.
Ph.D. thesis,University of Stuttgart.444Leech, Geoffrey, Roger Garside, and Michael Bryant.1994.
Claws4: The tagging of the british nationalcorpus.
In Proceedings of the 15th InternationalConference on Computational Linguistics (COLING94), pages 622?628, Kyoto, Japan.Lund, Kevin, Curt Burgess, and Ruth Ann Atchley.1995.
Semantic and associative priming in high-dimensional semantic space.
In Proceedings of the17th annual conference of the Cognitive Science So-ciety, Vol.
17, pages 660?665.McNally, Louise.
2013.
Formal and distributional se-mantics: From romance to relationship.
In Proceed-ings of the ?Towards a Formal Distributional Seman-tics?
workshop, 10th International Conference onComputational Semantics (IWCS2013), Potsdam,Germany.
Invited talk.Mitchell, Jeff and Mirella Lapata.
2010.
Compositionin Distributional Models of Semantics.
CognitiveScience, 34(8):1388?1429, November.Resnik, Philipp.
1995.
Using information contentto evaluate semantic similarity in a taxonomy.
InProceedings of the 14th International Joint Con-ference on Artificial Intelligence (IJCAI-95), pages448?453.Searle, John R. 1969.
Speech acts: An essay in the phi-losophy of language.
Cambridge University Press.Turney, Peter D. and Patrick Pantel.
2010.
Fromfrequency to meaning: Vector space models of se-mantics.
Journal of Artificial Intelligence Research,37:141?188.445
