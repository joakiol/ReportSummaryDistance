Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 144?147,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsA Simple Ensemble Method for Hedge IdentificationFerenc P. Szidarovszky1, Ille?s Solt1, Domonkos Tikk1,21 Budapest University of Technology and Economics, Budapest, Hungary2 Humboldt-Universita?t zu Berlin, Berlin, Germanyferenc.szidarovszky@hotmail.com,{solt,tikk}@tmit.bme.huAbstractWe present in this paper a simple hedgeidentification method and its applicationon biomedical text.
The problem at handis a subtask of CoNLL-2010 shared task.Our solution consists of two classifiers, astatistical one and a CRF model, and asimple combination schema that combinestheir predictions.
We report in detail oneach component of our system and discussthe results.
We also show that a more so-phisticated combination schema could im-prove the F-score significantly.1 Problem definitionThe CoNLL-2010 Shared Task focused on theidentification and localization of uncertain infor-mation and its scope in text.
In the first task, abinary classification of sentences had to be per-formed, based on whether they are uncertain ornot.
The second task concentrated on the identi-fication of the source of uncertainty ?
specifyingthe keyword/phrase that makes its context uncer-tain ?, and the localization of its scope.
The orga-nizers provided training data from two applicationdomains: biomedical texts and Wikipedia articles.For more details see the overview paper by the or-ganizers (Farkas et al, 2010).
We focused on task1 and worked with biomedical texts exclusively.The biomedical training corpus contains se-lected abstracts and full text articles from the Bio-Scope corpus (Vincze et al, 2008).
The corpuswas manually annotated for hedge cues on thephrase level.
Sentences containing at least one cueare considered as uncertain, while sentences withno cues are considered as factual.
Though cue tag-ging was given in the training data, their markingin the submission was not mandatory.The evaluation of systems at task 1 was per-formed on the sentence level with the F-measureof the uncertain class being the official evaluationmetric.
For evaluation, corpora also from both do-mains were provided that allowed for in-domainand cross-domain experiments as well.
Neverthe-less, we restricted the scope of our system to thein-domain biomedical subtask.2 BackgroundAutomatic information extraction methods mayincorrectly extract facts that are mentioned in anegated or speculative context.
If aiming at highaccuracy, it is therefore crucial to be able to clas-sify assertions to avoid such false positives.
Theimportance of assertion classification has been re-cently recognized by the text mining community,which yielded several text-mining challenges cov-ering this task.
For example, the main task ofObesity Challenge (Uzuner, 2008) was to iden-tify based on a free text medical record whethera patient is known to, speculated to or knownnot to have a disease; in the BioNLP ?09 SharedTask (Kim et al, 2009), mentions of bio-molecularevents had to be classified as either positive or neg-ative statements or speculations.Approaches to tackle assertion classificationcan be roughly organized into following classes:rule based models (Chapman et al, 2001), sta-tistical models (Szarvas, 2008), machine learning(Medlock and Briscoe, 2007), though most con-tributions can be seen as a combination of these(Uzuner et al, 2009).
Even when classifying sen-tences, the most common approach is to look forcues below the sentence-level (O?zgu?r and Radev,2009).
The common in these approaches is thatthey use a text representation richer than bag-of-words, usually tokens from a fixed-width windowwith additional surface features.Evaluation of assertion classification is mostlyperformed at the sentence level, where state-of-the-art systems have been reported to achievean F-measure of 83?85% for hedge detection in144biomedical literature (Medlock and Briscoe, 2007;Szarvas, 2008).3 MethodsAlthough the problem itself is a binary categoriza-tion problem, we approach the problem at the to-ken/phrase level.
We search for hedge cues andused the decision model also applied by the an-notators of the training corpus: when a sentencecontains at least one uncertainty cue then it is un-certain, otherwise factual.We applied two different models to identifyhedge cues:?
a statistical model that creates a candidate listof cue words/phrases from the training sam-ples, and cuts off the list based on the preci-sion measured on the trial set;?
a sequence tagger CRF model, trained againwith hedge cues using various feature sets.Finally, we combined the outputs of the meth-ods at the sentence level.
Here we applied twovery simple ways of combination: the aggres-sive one assigns a sentence to the uncertain classif any of the models finds a cue phrase therein(OR merger), while the conservative only if bothmodels predict the sentence as uncertain (ANDmerger).
We submitted the version which pro-duced better result on the trial set.
The overviewof our system is depicted on Figure 1.3.1 PreprocessingThe biomedical corpus was provided in twotrain/trial pairs (abstracts and full texts), see alsoTable 1.
Because the ratio of uncertain sentencesis similar in both train and trial sets, we mergedthe two train sets and the two trial sets, respec-tively, to obtain a single train/trial pair.
Since thetrial set was originally included also in the trainset, we removed the elements of the merged trialset from the merged train set.
In the following, werefer to them as train and trial sets.
All data (train,trial, evaluation) were given as separate sentences;therefore no sentence segmentation had to be per-formed.Merging train and trial sets was also motivatedby the sparsity of data and the massively differ-ent train/trial ratio observed for the two types ofbiomedical texts (Table 1).
Therefore buildingseparate models for abstracts and full texts mayCRF model Statistical modelInputCRFclassificationStatisticalclassificationOR mergerFinalclassificationFigure 1: System overviewonly yield overfitting, particularly because such adistinction is not available for the evaluation set.3.2 Statistical modelThe statistical model considers a sentence uncer-tain, if it contains at least one cue from a validatedset of cue phrases.
To determine the set of cuephrases to be used, we first collected all annotatedcues from the training data.
From this candidatecue set we retained those ones that had a precisionover a predefined threshold.
To this end we mea-sured on the training set the precision of each cuephrase.
We depicted on Figure 2 the precision, re-call and F-measure values obtained on the trial setwith different cue phrase precision thresholds.The candidate cue set contains 186 cue phrases,among which 83 has precision 1.0 and 141 hasprecision greater or equal 0.5.
Best cue phrasesinclude words/phrases like cannot + verb phrase,hypothesis, indicate, may, no(t) + verb/noun, raisethe + noun, seem, suggest, whether etc., while lowprecision cues are, e.g., assume, not fully under-stood, not, or, prediction, likelihood.3.3 CRF modelIdentifying entities such as speculation cues can beefficiently solved by training conditional randomfield (CRF) models.
As a general sequence tagger,a CRF can be naturally extended to incorporate to-ken features and features of neighboring tokens.The trained CRF model is then applied to unseen145Train set Trial set Evaluation setsentences uncertain ratio sentences uncertain ratio sentences uncertain ratioAbstract 11 832 2 091 17.7% 39 10 25.6% ?
?
?Full text 2 442 468 19.2% 228 51 22.4% ?
?
?Total 14 274 2 559 17.9% 267 61 22.9% 5 003 790 15.8%Table 1: Basic statistics of the provided train, trial, and evaluation sets73.6285.71 86.4078.57304050607080901000.00 0.20 0.40 0.60 0.80 1.00PercentageThresholdF-MeasurePrecisionRecallFigure 2: Cue phrase threshold selectiontext, whenever a speculation cue is found the con-taining sentence is annotated as being speculative.In our experiments, we used MALLET (McCal-lum, 2002) to train CRF models using custom to-kenization (Section 3.3.1) and feature sets (Sec-tion 3.3.2).
We included features of 2?2 neigh-boring tokens in each direction, not surpassing thesentence limits.3.3.1 TokenizationWe split text into tokens using punctuation andwhite-space tokenization, keeping punctuationsymbols as separate tokens.3.3.2 Feature setsWe experimented with the following binary sur-face features:1. token text2.
token text in lowercase3.
stem of token in lowercase4.
indicator of the token being all lowercase5.
indicator whether the token is in sentencecase (first character upper-, others lowercase)6. indicator whether the token contains at leastone digit7.
indicator of token being a punctuation sym-bolThese features were evaluated both in isolationand in combination on the trial set.
The best per-forming combination was then used to train the fi-nal model.3.3.3 Feature selectionEvaluating all combinations of the above features,we found that the combination of features 2 and 4produced the best results on the trial set.
For com-putational efficiency, when selecting the best per-forming feature subset, we considered lower fea-ture count to overrule a slight increase in perfor-mance.4 ResultsTable 2 and Table 3 summarize the results for thestatistical and CRF models and their AND and ORcombinations on the trial and on the evaluationsets, respectively.
For the latter, we used naturallyall available labeled data (train and trial sets) fortraining.
Numbers shown correspond to the out-put of the official evaluation tool.
Results on thecombination OR represent our official shared taskevaluation.5 DiscussionIn the development scenario (Table 2), the maindifference between the statistical and CRF modelwas that the former was superior in recall while thelatter in precision.
It was thus unclear which of thecombinations OR and AND would perform better,we chose OR, the combination method which per-formed better on the trial set.
Unfortunately, therank of combination methods was different whenmeasured on the evaluation set (Table 3).
A possi-ble explanation for this non-extrapolability is thedifferent prior probability of speculative sentencesin each set, e.g., 17.9% on the train set while22.9% on the trial set and 15.8% on the evaluationset.While using only a minimal amount of features,both of our models were on par with other partic-ipants?
solutions.
Overfitting was observed by thestatistical model only (14% drop in precision onthe evaluation set), the CRF model showed moreconsistent behavior across the datasets.146ModelStatistical CRF Combination AND Combination ORPrecision (%) 84.4 92.3 93.9 83.6Recall (%) 88.6 78.7 75.4 91.8F-measure (%) 86.4 85.0 83.6 87.5Table 2: Results on trial set (development)ModelStatistical CRF Combination AND Combination ORPrecision (%) 70.5 87.0 88.0 70.1Recall (%) 89.4 82.7 81.0 91.0F-measure (%) 78.8 84.8 84.4 79.2Table 3: Results on evaluation set6 ConclusionWe presented our method to identify hedging inbiomedical literature, and its evaluation at theCoNLL-2010 shared task.
We solved the sen-tence level assertion classification problem by us-ing an ensemble of statistical and CRF mod-els that identify speculation cue phrases.
Thenon-extrapolability of the combination methods?performance observed emphasizes the sensitivityof ensemble methods to the distributions of thedatasets they are applied to.
While using only aminimal set of standard surface features, our CRFmodel was on par with participants?
systems.AcknowledgementD.
Tikk was supported by the Alexander-von-Humboldt Foundation.ReferencesWendy W. Chapman, Will Bridewell, Paul Hanbury,Gregory F. Cooper, and Bruce G. Buchanan.
2001.A simple algorithm for identifying negated findingsand diseases in discharge summaries.
Journal ofBiomedical Informatics, 2001:34?301.Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nosCsirik, and Gyo?rgy Szarvas.
2010.
The CoNLL-2010 Shared Task: Learning to Detect Hedges andtheir Scope in Natural Language Text.
In Proceed-ings of the Fourteenth Conference on ComputationalNatural Language Learning (CoNLL-2010): SharedTask, pages 1?12, Uppsala, Sweden.
ACL.Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-nobu Kano, and Jun?ichi Tsujii.
2009.
Overviewof BioNLP?09 shared task on event extraction.
InBioNLP ?09: Proc.
of the Workshop on BioNLP,pages 1?9, Morristown, NJ, USA.
ACL.Andrew K. McCallum.
2002.
MALLET: A Ma-chine Learning for Language Toolkit.
http://mallet.cs.umass.edu.Ben Medlock and Ted Briscoe.
2007.
Weakly super-vised learning for hedge classification in scientificliterature.
In Proc.
of the 45th Annual Meeting ofthe Association of Computational Linguistics, pages992?999, Prague, Czech Republic, June.
ACL.Arzucan O?zgu?r and Dragomir R. Radev.
2009.
Detect-ing speculations and their scopes in scientific text.In EMNLP ?09: Proc.
of Conf.
on Empirical Meth-ods in Natural Language Processing, pages 1398?1407, Morristown, NJ, USA.
ACL.Gyo?rgy Szarvas.
2008.
Hedge Classification inBiomedical Texts with a Weakly Supervised Selec-tion of Keywords.
In Proceedings of ACL-08: HLT,pages 281?289, Columbus, Ohio, June.
ACL.O?zlem Uzuner, Xiaoran Zhang, and Tawanda Sibanda.2009.
Machine Learning and Rule-based Ap-proaches to Assertion Classification.
Journalof the American Medical Informatics Association,16(1):109?115.O?zlem Uzuner.
2008.
Second i2b2 workshop onnatural language processing challenges for clinicalrecords.
In AMIA Annual Symposium Proceedings,pages 1252?3.Veronika Vincze, Gyo?rgy Szarvas, Richa?rd Farkas,Gyo?rgy Mo?ra, and Ja?nos Csirik.
2008.
The Bio-Scope corpus: biomedical texts annotated for uncer-tainty, negation and their scopes.
BMC Bioinformat-ics, 9(Suppl 11):S9.147
