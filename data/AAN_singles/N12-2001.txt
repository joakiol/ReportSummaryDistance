Proceedings of the NAACL HLT 2012 Student Research Workshop, pages 1?6,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsFinding the Right Supervisor: Expert-Finding in a University DomainFawaz Alarfaj, Udo Kruschwitz, David Hunter and Chris FoxSchool of Computer Science and Electronic EngineeringUniversity of EssexColchester, CO4 3SQ, UK{falarf, udo, dkhunter, foxcj}@essex.ac.ukAbstractEffective knowledge management is a key fac-tor in the development and success of any or-ganisation.
Many different methods have beendevised to address this need.
Applying thesemethods to identify the experts within an or-ganisation has attracted a lot of attention.
Welook at one such problem that arises withinuniversities on a daily basis but has attractedlittle attention in the literature, namely theproblem of a searcher who is trying to iden-tify a potential PhD supervisor, or, from theperspective of the university?s research office,to allocate a PhD application to a suitable su-pervisor.
We reduce this problem to identify-ing a ranked list of experts for a given query(representing a research area).We report on experiments to find experts in auniversity domain using two different meth-ods to extract a ranked list of candidates:a database-driven method and a data-drivenmethod.
The first one is based on a fixed listof experts (e.g.
all members of academic staff)while the second method is based on auto-matic Named-Entity Recognition (NER).
Weuse a graded weighting based on proximity be-tween query and candidate name to rank thelist of candidates.
As a baseline, we use asystem that ranks candidates simply based onfrequency of occurrence within the top docu-ments.1 IntroductionThe knowledge and expertise of individuals are sig-nificant resources for organisation.
Managing thisintangible resource effectively and efficiently con-stitutes an essential and very important task (Non-aka and Takeuchi, 1995; Law and Ngai, 2008).
Ap-proaching experts is the primary and most direct wayof utilising their knowledge (Yang and Huh, 2008;Li et al, 2011).
Therefore, it is important to have ameans of locating the right experts within organisa-tions.
The expert-finding task can be categorised asan information retrieval task similar to a web search,but where the results are people rather than docu-ments.
An expert-finding system allows users to in-put a query, and it returns a ranked list of experts.Here we look at a university context.
We startwith a real-world problem which is to identify a listof experts within an academic environment, e.g.
auniversity intranet.
The research reported here isbased on an empirical study of a simple but effectivemethod in which a system that applies the concept ofexpert-finding has been designed and implemented.The proposed system will contribute to provide anexpert-search service to all of the university?s stake-holders.Expert-finding systems require two main re-sources in order to function: a list of candidates anda collection of data from which the evidence of ex-pertise can be extracted.
We present two approachesto address this problem, a database-driven and adata-driven method using NER.
The main differ-ence between the two methods is the way in whichthe candidates?
list is constructed.
In the databasemethod, the candidates are simply selected from aknown list of experts, e.g.
the university?s academicstaff.
In the NER method, the candidates are ex-tracted automatically from the pages returned by an1underlying search engine.
This method promises tobe more useful for finding experts from a wider (andpossibly more up-to-date) range of candidates.
Bothmethods apply the same ranking function(s), as willbe discussed below.This paper will survey related work in Section 2and introduce the expert-finding task in a universitydomain in Section 3.
The process of ranking expertswill be discussed in Section 4.
The evaluation willbe described in Section 4, followed by a discussionof the experiment?s results in Section 5.2 Related WorkThe expert-finding task addresses the problem of re-trieving a ranked list of people who are knowledge-able about a given topic.
This task has found itsplace in the commercial environment as early as the1980?s, as discussed in Maybury (2006); however,there was very limited academic research on findingand ranking experts until the introduction of the en-terprise track at the 2005 Text REtrieval Conference(TREC) (Craswell et al, 2005).When expert-finding we must know the experts?profiles, These profiles may be generated manuallyor automatically.
Manually created profiles may beproblematic.
If, for example, experts enter their owninformation, they may exaggerate or downplay theirexpertise.
In addition, any changes of expertise forany expert requires a manual update to the expert?sprofile.
Thus incurring high maintenance costs.
Anexample of manually generated profiles is the workof Dumais and Nielsen (1992).
Although their sys-tem automatically assigns submitted manuscripts toreviewers, the profiles of the reviewers or experts arecreated manually.The alternative is to generate the profiles automat-ically, for example by extracting relevant informa-tion from a document collection.
The assumption isthat individuals will tend to be expert in the topicsof documents with which they are associated.
Ex-perts can be associated with the documents in whichthey are mentioned (Craswell et al, 2001) or withe-mails they have sent or received (Balog and de Ri-jke, 2006a; Campbell et al, 2003; Dom et al, 2003).They can also be associated with their home pagesor CVs (Maybury et al, 2001), and with documentsthey have written (Maybury et al, 2001; Becerra-Fernandez, 2000).
Finally, some researchers usesearch logs to associate experts with the web pagesthey have visited (Wang et al, 2002; Macdonald andWhite, 2009).After associating candidate experts with one ormore of the kinds of textual evidence mentionedabove, the next step is to find and rank candidatesbased on a user query.
Many methods have beenproposed to perform this task.
Craswell et al (2001)create virtual documents for each candidate (or em-ployee).
These virtual documents are simply con-catenated texts of all documents from the corpus as-sociated with a particular candidate.
Afterwards,the system indexes and processes queries for theemployee?s documents.
The results would show alist of experts based on the ten best matching em-ployee documents.
Liu et al (2005) have appliedexpert-search in the context of a community-basedquestion-answering service.
Based on a virtual doc-ument approach, their work applied three languagemodels: the query likelihood model, the relevancemodel and the cluster-based language model.
Theyconcluded that combining language models can en-hance the retrieval performance.Two principal approaches recognised for expert-finding can be found in the literature.
Both werefirst proposed by Balog et al (2006b).
The mod-els are called the candidate model and the doc-ument model, or Model 1 and Model 2, respec-tively.
Different names have been used for thetwo methods.
Fang and Zhai (2007) refer to themas ?Candidate Generation Models and Topic Gen-eration Models?.
Petkova and Croft (2006) callthem the ?Query-Dependent Model?
and the ?Query-Independent Model?.
The main difference betweenthe models is that the candidate-based approaches(Model 1) build a textual representation of candi-date experts, and then rank the candidates based onthe given query, whereas the document-based ap-proaches (Model 2) first find documents that are rel-evant to the query, and then locate the associated ex-perts in these documents.Balog et al (2006b) have compared the two mod-els and concluded that Model 2 outperforms Model1 on all measures (for this reason, we will adoptModel 2).As Model 2 proved to be more efficient, it formedthe basis of many other expert-search systems (Fang2and Zhai, 2007; Petkova and Croft, 2007; Yaoet al, 2008).
Fang and Zhai developed a mixturemodel using proximity-based document representa-tion.
This model makes it possible to put differentweights on different representations of a candidateexpert (Fang and Zhai, 2007).
Another mixture ofpersonal and global language models was proposedby Serdyukov and Hiemstra (2008).
They combinedtwo criteria for personal expertise in the final rank-ing: the probability of generation of the query by thepersonal language model and a prior probability ofcandidate experts that expresses their level of activ-ity in the important discussions on the query topic.Zhu et al (2010) claimed that earlier languagemodels did not consider document features.
Theyproposed an approach that incorporates: internaldocument structure; document URLs; page rank;anchor texts; and multiple levels of association be-tween experts and topics.All of the proposed frameworks assume that themore documents associated with a candidate thatscore highly with respect to a query, the more likelythe candidate is to have relevant expertise for thatquery.
Macdonald and Ounis (2008) developed adifferent approach, called the Voting Model.
In theirmodel, candidate experts are ranked first by consid-ering a ranking of documents with respect to theusers?
query.
Then, using the candidate profiles,votes from the ranked documents are converted intovotes for candidates.There have been attempts to tackle the expert-finding problem using social networks.
This hasmainly been investigated from two directions.
Thefirst direction uses graph-based measures on socialnetworks to produce a ranking of experts (Campbellet al, 2003; Dom et al, 2003).
The second direc-tion assumes similarities among the neighbours in asocial network and defines a smoothing procedureto rank experts (Karimzadehgan et al, 2009; Zhanget al, 2007).Some have argued that it is not enough to find ex-perts by looking only at the queries?
without tak-ing the users into consideration.
They claim thatthere are several factors that may play a role indecisions concerning which experts to recommend.Some of these factors are the users?
expertise level,social proximity and physical proximity (Borgattiand Cross, 2003; McDonald and Ackerman, 1998;Shami et al, 2008).
McDonald and Ackerman(1998) emphasised the importance of the accessi-bility of the expert.
They argued that people usu-ally prefer to contact the experts who are physicallyor organisationally close to them.
Moreover, Shamiet al (2008) found that people prefer to contact ex-perts they know, even when they could potentiallyreceive more information from other experts who arelocated outside their social network.Woudstra and van den Hooff (2008) identified anumber of factors in selecting experts that are re-lated to quality and accessibility.
They argued thatthe process of choosing which candidate expert tocontact might differ depending on the specific situa-tion.Hofmann et al (2010) showed that many of thesefactors can be modelled.
They claimed that integrat-ing them with retrieval models can improve retrievalperformance.
Smirnova and Balog (2011) provideda user-oriented model for expert-finding where theyplaced an emphasis on the social distance betweenthe user and the expert.
They considered a numberof social graphs based on organisational hierarchy,geographical location and collaboration.3 Expert-Finding in a UniversityIn any higher educational institution, finding an ap-propriate supervisor is a critical task for researchstudents, a task that can be very time consuming,especially if academics describe their work usingterms that a student is not familiar with.
A searchermay build up a picture of who is likely to havethe relevant expertise by looking for university aca-demic staff who have written numerous documentsabout the general topic, who have authored docu-ments exactly related to the topic, or who list thetopic as one of their research interest areas.
Au-tomating this process will not only help research stu-dents find the most suitable supervisors, but it alsoallow the university to allocate applications to super-visors, and help researchers find other people inter-ested in the particular topics.3.1 MethodThe two approaches we apply, database-driven, anddata-driven using NER1 are illustrated in Figure 1.1We use OpenNLP to identify named entities.3Search API URL FilterPage ReaderHTML ParserNormalizerCandidateExtractorCandidateEvaluatorNamed-EntityRecognitionCandidateEvaluatorDB ConnectionUpdateCandidate RankDisplay ExpertsQuery URLs Filtered URLsWeb PageStringParsed HtmlPageNormalizedWeb PageStringCandidate ListNormalizedWeb PageStringCandidateListCandidate RankFor PageCandidate RankFor PageNamed-Entity RecognitionMethod Database MethodFigure 1: System Architecture.The main difference between the two methods is theway in which the candidates?
list is constructed.
Weargue that each method has its advantages.
In thedatabase method, the candidates are simply the uni-versity?s academic staff.
This avoids giving resultsunrelated to the university.
It would be appropriate ifthe aim is to find the experts from among the univer-sity academics.
In the data-driven method, the can-didates are extracted from the pages returned by theunderlying search engine.
The experts found by thismethod are not necessarily university staff.
Theycould be former academics, PhD students, visitingprofessors, or newly appointed staff.Both methods apply the same ranking functions,one baseline function which is purely based on fre-quency and one which takes into account proximityof query terms with matches of potential candidatesin the retrieved set of documents.3.2 The Baseline ApproachThe baseline we chose for ranking candidates is thefrequency of appearance of names in the top twentyretrieved documents.
The system counts how manytimes the candidate?s name appears in the documentd(cc).
Then it calculates the candidate metric cm bydividing the candidate count d(cc) by the number oftokens in the document d(nt).Equation 1 defines the metric, where cm is thefinal candidate?s metric for all documents and n isthe number of documents.cm =n?d=1d(cc)d(nt)(1)3.3 Our ApproachOur approach takes into account the proximity be-tween query terms and candidate names in thematching documents in the form of a distanceweight.
This measure will adds a distance weightvalue to the main candidate?s metric that was gener-ated earlier.
Similar approaches have been proposedin the literature for different expert search applica-tions Lu et al (2006); Cao et al (2005).
The dis-tance weight will be higher whenever the name ap-pears closer to the query term, within a +/- 10 wordwindow.We experiment with two different formulae.
Thefirst formula is as follows:cm1 =n?i=1m?j=1(cm+1?
?
?ij), ?ij ={dij if dij ?
100 Otherwise(2)where n is the number of times the candidate?s namehas been found in the matching documents, m is thenumber of times the (full) query has been identified,and dij is the distance between the name positionand query position (?
has been set empirically to 3).The second formula is:cm2 =n?i=1m?j=1(cm+1cm?ij), ?ij ={dij if dij ?
100 Otherwise(3)This equation is designed to return a smaller valueas the distance x increases, and to give the candidatewith lower frequency a higher weight.In both cases, candidates are ranked according tothe final score and displayed in order so that the can-didates who are most likely to be experts are dis-played at the top of the list.4 EvaluationAs with any IR system, evaluation can be difficult.In the given context one might argue that precision4is more important than recall.
In any case, recall canbe difficult to measure precisely.
To address these is-sues we approximate a gold standard as follows.
Weselected one school within the university for whicha page of research topics with corresponding aca-demics exists In this experiment we take this map-ping as a complete set of correct matches.
In thispage, there are 371 topics (i.e.
potential queries) di-vided among 28 more general research topics.
Eachtopic/query is associated with one or more of theschool?s academic staff.
It is presumed that thosenames belong to experts on the corresponding top-ics.Table 1 illustrates some general topics with thenumber of (sub)topic they contain.
Table 2 list someof the topics.Topic NAnalogue and Digital Systems Architectures 2Artificial Intelligence 26Audio 12Brain Computer Interface 18Computational Finance Economics and Management 1Computational Intelligence 10. .
.
.
.
.Table 1: Distribution of topics - N denotes the number oftopics for the corresponding general topic area.High-Speed Lasers And PhotodetectorsHuman Behaviour And The PsychologyHuman Motion TrackingHuman-Centred RoboticsHybrid HeuristicsHybrid Intelligent Systems Which Include Neuro-Fuzzy SystemsHypercomplex Algebras And Fourier TransformsHypercomplex Fourier Transforms And FiltersTable 2: Some topics/queriesThe measure used to test the system is recall atthe following values {3, 5, 7, 10, 15, 20}.
Wealso measure Mean Average Precision at rank 20(MAP@20).5 Results and DiscussionTable 3 shows the system results where BL is thebaseline result.
There are two main findings.
Firstof all, the database-driven approach outperformsthe data-driven approach.
Secondly, our approachwhich applies a grading of results based on prox-imity between queries and potential expert namessignificantly outperforms the baseline approach thatonly considers frequency, that is true for both formu-lae we apply when ranking the results (using pairedt-tests applied to MAP with p<0.0001).
However,the differences between cm1 and cm2 tend not to besignificantly different.BL cm1 cm2NER DB NER DB NER DBR@3 0.47 0.48 0.49 0.76 0.58 0.79R@5 0.56 0.60 0.58 0.83 0.68 0.86R@7 0.61 0.64 0.62 0.87 0.72 0.88R@10 0.65 0.69 0.68 0.89 0.78 0.90R@15 0.69 0.72 0.74 0.91 0.80 0.91R@20 0.71 0.75 0.76 0.92 0.82 0.93MAP 0.20 0.28 0.50 0.61 0.52 0.66Table 3: Performance MeasuresIt is perhaps important to mention that our data isfairly clean.
More noise would make the creation ofrelational database more difficult.
In that case thedata-driven approach may become more appropri-ate.6 ConclusionThe main objective of this work was to exploreexpert-finding in a university domain, an area thathas to the best of our knowledge so far attractedlittle attention in the literature.
The main findingis that a database-driven approach (utilising a fixedset of known experts) outperforms a data-driven ap-proach which is based on automatic named-entityrecognition.
Furthermore, exploiting proximity be-tween query and candidate outperforms a straightfrequency measure.There are a number of directions for future work.For example, modelling the user background andinterests could increase the system?s effectiveness.Some more realistic end-user studies could be usedto evaluate the systems.
Consideration could begiven to term dependence and positional models asin Metzler and Croft (2005), which might improveour proximity-based scoring function.
Finaly, ourgold standard collection penalises a data-driven ap-proach, which might offer a broader range of ex-perts.
We will continue this line of work usingboth technical evaluation measures as well as user-focused evaluations.5ReferencesK.
Balog and M. de Rijke.
Finding experts and their details in e-mailcorpora.
In Proceedings of the 15th international conference onWorld Wide Web, pages 1035?1036.
ACM, 2006a.K.
Balog, L. Azzopardi, and M. de Rijke.
Formal models for expertfinding in enterprise corpora.
In Proceedings of the 29th annualinternational ACM SIGIR conference on Research and developmentin information retrieval, pages 43?50.
ACM, 2006b.I.
Becerra-Fernandez.
Facilitating the online search of experts at NASAusing expert seeker people-finder.
In Proceedings of the 3rd Interna-tional Conference on Practical Aspects of Knowledge Management(PAKM), Basel, Switzerland, 2000.S.P.
Borgatti and R. Cross.
A relational view of information seeking andlearning in social networks.
Management science, 49(4):432?445,2003.C.S.
Campbell, P.P.
Maglio, A. Cozzi, and B. Dom.
Expertise identifica-tion using email communications.
In Proceedings of the twelfth in-ternational conference on Information and knowledge management,pages 528?531.
ACM, 2003.Y.
Cao, J. Liu, S. Bao, and H. Li.
Research on expert search at enterprisetrack of trec 2005.
In 14th Text Retrieval Conference (TREC 2005),2005.N.
Craswell, D. Hawking, A.M. Vercoustre, and P. Wilkins.
P@ nopticexpert: Searching for experts not just for documents.
In AuswebPoster Proceedings, Queensland, Australia, 2001.N.
Craswell, A.P.
de Vries, and I. Soboroff.
Overview of the TREC-2005 enterprise track.
In TREC 2005 Conference Notebook, pages199?205, 2005.B.
Dom, I. Eiron, A. Cozzi, and Y. Zhang.
Graph-based ranking algo-rithms for e-mail expertise analysis.
In Proceedings of the 8th ACMSIGMOD workshop on Research issues in data mining and knowl-edge discovery, pages 42?48.
ACM, 2003.S.T.
Dumais and J. Nielsen.
Automating the assignment of submittedmanuscripts to reviewers.
In Proceedings of the 15th annual inter-national ACM SIGIR conference on Research and development ininformation retrieval, pages 233?244.
ACM, 1992.H.
Fang and C.X.
Zhai.
Probabilistic models for expert finding.
Ad-vances in Information Retrieval, pages 418?430, 2007.K.
Hofmann, K. Balog, T. Bogers, and M. de Rijke.
Contextual fac-tors for finding similar experts.
Journal of the American Society forInformation Science and Technology, 61(5):994?1014, 2010.M.
Karimzadehgan, R. White, and M. Richardson.
Enhancing expertfinding using organizational hierarchies.
Advances in InformationRetrieval, pages 177?188, 2009.C.
Law and E. Ngai.
An empirical study of the effects of knowledgesharing and learning behaviors on firm performance.
Expert Systemswith Applications, 34(4):2342?2349, 2008.M.
Li, L. Liu, and C. Li.
An approach to expert recommendation basedon fuzzy linguistic method and fuzzy text classification in knowl-edge management systems.
Expert Systems with Applications, 38(7):8586?8596, 2011.X.
Liu, W.B.
Croft, and M. Koll.
Finding experts in community-basedquestion-answering services.
In Proceedings of the 14th ACM in-ternational conference on Information and knowledge management,pages 315?316.
ACM, 2005.W.
Lu, S. Robertson, A. MacFarlane, and H. Zhao.
Window-basedenterprise expert search.
In Proceeddings of the 15th Text REtrievalConference (TREC 2006).
NIST, 2006.C.
Macdonald and I. Ounis.
Voting techniques for expert search.
Knowl-edge and information systems, 16(3):259?280, 2008.C.
Macdonald and R.W.
White.
Usefulness of click-through data inexpert search.
In SIGIR, volume 9, pages 816?817, 2009.M.
Maybury, R. D?Amore, and D. House.
Expert finding for collabo-rative virtual environments.
Communications of the ACM, 44(12):55?56, 2001.M.T.
Maybury.
Expert finding systems.
MITRE Center for IntegratedIntelligence Systems Bedford, Massachusetts, USA, 2006.D.W.
McDonald and M.S.
Ackerman.
Just talk to me: a field studyof expertise location.
In Proceedings of the 1998 ACM conferenceon Computer supported cooperative work, pages 315?324.
ACM,1998.D.
Metzler and W.B.
Croft.
A markov random field model for term de-pendencies.
In Proceedings of the 28th annual international ACMSIGIR conference on Research and development in information re-trieval, pages 472?479.
ACM, 2005.I.
Nonaka and H. Takeuchi.
The knowledge creating company: HowJapanese The knowledge creating company: How Japanese compa-nies create the dynamics of innovation.
Oxford University Press,New York, 1995.D.
Petkova and W.B.
Croft.
Hierarchical language models for ex-pert finding in enterprise corpora.
In Tools with Artificial Intel-ligence, 2006.
ICTAI?06.
18th IEEE International Conference on,pages 599?608.
IEEE, 2006.D.
Petkova and W.B.
Croft.
Proximity-based document representationfor named entity retrieval.
In Proceedings of the sixteenth ACM con-ference on Conference on information and knowledge management,pages 731?740.
ACM, 2007.P.
Serdyukov and D. Hiemstra.
Modeling documents as mixtures ofpersons for expert finding.
Advances in Information Retrieval, pages309?320, 2008.N.S.
Shami, K. Ehrlich, and D.R.
Millen.
Pick me: link selection inexpertise search results.
In Proceeding of the twenty-sixth annualSIGCHI conference on Human factors in computing systems, pages1089?1092.
ACM, 2008.E.
Smirnova and K. Balog.
A user-oriented model for expert finding.Advances in Information Retrieval, pages 580?592, 2011.J.
Wang, Z. Chen, L. Tao, W.Y.
Ma, and L. Wenyin.
Ranking user?srelevance to a topic through link analysis on web logs.
In Proceed-ings of the 4th international workshop on Web information and datamanagement, pages 49?54.
ACM, 2002.L.
Woudstra and B. van den Hooff.
Inside the source selection pro-cess: Selection criteria for human information sources.
InformationProcessing & Management, 44(3):1267?1278, 2008.K.
Yang and S. Huh.
Automatic expert identification using a text au-tomatic expert identification using a text categorization technique inknowledge management systems.
Expert Systems with Expert Sys-tems with Applications, 34(2):1445?1455, 2008.J.
Yao, J. Xu, and J. Niu.
Using role determination and expert min-ing in the enterprise environment.
In Proceedings of the 2008 TextREtrieval Conference (TREC 2008), 2008.J.
Zhang, J. Tang, and J. Li.
Expert finding in a social network.
Ad-vances in Databases: Concepts, Systems and Applications, pages1066?1069, 2007.J.
Zhu, X. Huang, D. Song, and S. Ru?ger.
Integrating multiple doc-ument features in language models for expert finding.
Knowledgeand Information Systems, 23(1):29?54, 2010.6
