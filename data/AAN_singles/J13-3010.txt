Book ReviewLanguage and ComputersMarkus Dickinson*, Chris Brew?, and Detmar Meurers?
(*Indiana University, ?Educational Testing Service, and ?University of Tu?bingen)Wiley-Blackwell, 2013, xviii+232 pp; paperbound, ISBN 978-1-4051-8305-5, $34.95;hardbound, ISBN 978-4051-8306-2, $87.95; e-book, ISBN 978-1-1183-2316-8, $22.99Reviewed byMats Wire?nStockholm UniversityAny textbook on computational linguistics today must position itself relative to Jurafskyand Martin (2009), which, by virtue of its depth and comprehensiveness, is the standardintroduction and reference for speech and language processing.
Readers without sub-stantial background may find that book too demanding, however, and then Languageand Computers, by Dickinson, Brew, and Meurers, may be a better choice.
Roughly, thisis how Language and Computers distinguishes itself from Jurafsky and Martin (2009):1.
It does not presuppose any computing background.
More specifically, asstated in the Overview for Instructors, the book assumes ?no mathematicalor linguistic background beyond normal high-school experience.?
Thebook certainly has the ambition to explain some of the inner workings ofthe technology, but does so starting from a general level.2.
It is structured according to major applications (treated in six chapters),with the theoretical material being introduced along the way as it isneeded.
Jurafsky and Martin (2009) is instead structured accordingto linguistic description levels, starting with words and graduallyproceeding through higher levels (albeit with a final part on applications).The idea in Language and Computers is that readers without a backgroundin computing will at least be familiar with real-world tasks in whichcomputers deal with language, and will find a textbook structured inthis way more accessible.
It also gives the authors an opportunity toshow how the underlying techniques recur across different applications.3.
Its topic is processing of text.
There is some discussion in the first chapterof the nature and representation of speech, but the applications (with thepossible exception of dialogue) are text-oriented.4.
It is introductory, typically aimed at a quarter-length course according tothe Overview for Instructors.
At 232 pages, it is about one-quarter of thelength of Jurafsky and Martin (2009), which has enough material for afull-year course.?
2013 Association for Computational Linguisticsdoi:10.1162/COLI r 00165Computational Linguistics Volume 39, Number 3Chapter 1 is a prologue with a refreshingly broad perspective, dealing with howwritten and spoken language are represented in computers.
First, the basic writ-ing systems are described: alphabetical (roughly, one character?one phoneme), syl-labic (roughly, one character?one syllable), logographic (roughly, one character?onemorpheme/word), and major encodings are addressed (in particular, Unicode).
A de-scription of the nature of speech and its representation in waveforms and spectrogramsfollows, including sections on how to read spectrograms and on language modelingusing n-grams.
The two latter sections come under the heading of ?Under the Hood.
?Sections so headed (typically, one or two per chapter) provide digressions into selectedtechnical material that provide bonuses for the interested reader, but which can beomitted without losing the gist of each chapter.Chapter 2 brings up the first application, ?Writer?s Aids,?
which consists of twoparts: checking spelling and checking grammar.
The first part includes an overview ofdifferent kinds of spelling errors, and methods for detection and correction of errors.These include a description of dynamic programming for calculating the minimum editdistance between a misspelled word and a set of candidate corrections.
The secondpart deals with grammar checking.
To set the scene for this, there is first an exposi-tion of context-free grammar for the purpose of specifying the norm (the well-formedsentences) of the language.
(The possibility of enriching the formalism with featuresis mentioned later in the same chapter.)
It is then stated that ?
[w]hen a parser fails toparse a sentence, we have an indication that something is grammatically wrong withthe sentence?
(page 58).
At the same time, it is recognized that the most one can dois to build grammar fragments.
So what about undergeneration, and the challenges ofmaintaining a large, manually encoded grammar?
Instead of elaborating on this, thechapter goes on with a brief description of methods for correcting errors.
These includerelaxation-based techniques that discharge grammar rules, and special-purpose rulesor n-grams that trigger directly on erroneous sequences of words.
The reader is leftwondering what one could expect from a grammar fragment.
Although this may notbe relevant to grammar checking, it would have been instructive to have a mentionsomewhere of the possibility of augmenting context-free rules or other formalisms withprobabilities, and of how this has affected wide-coverage parsing (Clark and Curran2007).Chapter 3 deals with language tutoring systems and how to make them linguis-tically aware.
To this end, the concepts (but not the inner workings) of tokenization,part-of-speech tagging, and syntactic parsing are described.
An example languagetutoring system for learners of Portuguese is then addressed.
Compared with traditionalworkbook exercises, the system gives immediate feedback on orthographic, syntactic,and semantic errors, and also contains audio.The topic of Chapter 4, ?Searching,?
is mainly related to information retrieval.
Mostof the chapter is contained in two comprehensive sections that deal with searching inunstructured data (typically the Web) and semi-structured data (such as Wikipedia).The former section contains a relatively detailed description of search engines andPageRank, as well as an overview of HTML.
Evaluation of search results is mentioned,but the measures are described in more detail in the next chapter.
The section on semi-structured data contains a description of regular expressions, Unix grep, and finite-state automata.
The chapter also contains brief sections on searching of structured data(databases, using Boolean expressions) and of text corpora (mainly discussing corpusannotation).
Given that many of the readers of this book will be linguists, it is perhapssurprising that the book has so little material related to corpus linguistics, but it could beargued that this topic would require a text of its own (and that it is not an ?application?
).778Book ReviewAnyway, the ?Further Reading?
section of the chapter includes some good suggestionson this topic.Chapter 5 brings up document classification, and begins with an overview ofconcepts in machine learning.
Then there is a detailed description of how to measuresuccess in classification, with precision/recall, true/false positives/negatives, andthe related measures of sensitivity and specificity used in medicine.
After this, twoexamples of document classifiers are described in some detail: naive Bayes and theperceptron.
Finally, there is a short section on sentiment analysis.
This chapter has agood balance between applications-oriented and theoretical material, and gives a goodgrasp of each of them.Chapter 6 deals with dialogue systems.
After some motivation, the chapterexamines in detail an example spoken dialogue transcript from the Carnegie MellonUniversity Let?s Go!
Bus Information System, followed by a thorough description ofdialogue moves, speech acts, and Grice?s conversational maxims.
After this ambitiousbackground, one would expect some (even superficial) material on the anatomy of adialogue system founded on these principles, but instead there is a detailed descriptionof Eliza.
The motivation is that ?
[Let?s Go!]
is .
.
.
too complicated to be explained fully inthis textbook?
(page 167).
The ambition to explain a working system in full is admirable,but seems misdirected here.
Why not try to work out the basic principles of a simplerquestion-answering system?
As it stands, the applications-oriented material (Eliza) isnot connected to the theoretical parts of the chapter.
It is also potentially misleadingwhen the authors say: ?
[Eliza] works reasonably well using simple means, and this canbe useful if your application calls for a straightforward but limited way of creating theillusion of an intelligent being at the other end of the wire?
(page 170).
Here, the authorsmust be referring to chatbots, but for applications in general, it would have been valu-able to have a pointer to principles of user interface design, such as trying to maintainconsistency (Cohen, Giangola, and Balogh 2004).
On a different note, this chapter wouldprofit from picking up the thread on speech from Chapter 1.
For example, it might beinstructive to have an explanation of word-error rate and the degradation of input typi-cally caused by a speech recognizer.
This would also give an opportunity to elaborateon one of the problems mentioned at the outset of the chapter: ?
[f]ixing confusions andmisunderstandings before they cause the conversation to break down?
(page 154).Chapter 7 brings up the final application, machine translation.
This is a chapterthat gives a good grasp of both applications-oriented and theoretical material.
Example-based translation and translation memories are briefly discussed, and, after some back-ground, word alignment, IBM Model 1, the noisy channel model, and phrase-basedstatistical translation are explained.
Commercial translation is also addressed.
In con-nection with the translation triangle, the possibility of an interlingua is discussed, theconclusion being that ?
[d]espite great efforts, nobody has ever managed to design orbuild a suitable interlingua?
(page 189) and ?we probably cannot have one any timesoon?
(page 190).
This is true for a universal interlingua in the sense of a fully abstractlanguage-independent representation, but what might be more relevant is domain-specific interlinguas, which have proved to be feasible in recent years (Ranta 2011).Interlinguas are also mentioned later in the chapter, at the end of ?Under the Hood12?
(page 205) on phrase-based statistical translation, where it is stated that ?
[t]here iscertainly no interlingua in sight.?
Although this is different, Google Translate actuallyuses English as an interlingua for a large majority of its language pairs (Ranta 2010).Surely, the reason for this is that Google typically has much more parallel data forlanguage pairs where English is one of the members than for language pairs wherethis is not the case.779Computational Linguistics Volume 39, Number 3The final chapter (?Epilogue?)
brings up some philosophical questions: the im-pact of language technology on society (effects of automatization of natural-languagetasks), the human communication system as opposed to animal communication,human self-perception when computers begin to use language to interact, and ethicalconsiderations.A good index is crucial in a book like this, also because it helps to fulfill the aim ofshowing the reader how the underlying techniques recur across applications.
However,for numerous terms such as n-gram, part-of-speech tagging, parsing, supervised learning,and so forth, many (sometimes a majority) of the occurrences in the book are not coveredby the index.
It would be highly desirable to improve this for a future edition.In sum, although a couple of chapters could have had a better balance or connectionbetween the applications-oriented and theoretical material, the overall impression isthat Language and Computers successfully fills a niche: It serves its purpose well in beingan introductory textbook in computational linguistics for people without a computingbackground.
It is possible to debate both the applications chosen and the theory selectedfor presentation, but it is nonetheless good to see a general introduction with a clearambition to explain some of the inner workings of the technology.I teach an introductory course in computational linguistics for Master?s studentsin linguistics who lack a computing background.
My experience is that the primaryneed of these students is corpus linguistics, because, above all, they need to learn howto practice linguistics as an empirical science.
They soon reach a point, however, wherethey also need a broader introduction to the applications and methods of computationallinguistics.
Such a broad introduction is also needed by Bachelor?s students in linguis-tics, and by those studying to become language consultants, translators, and so on.
Thisbook would then be the natural choice.ReferencesClark, Stephen and James R. Curran.2007.
Wide-coverage efficient statisticalparsing with CCG and log-linearmodels.
Computational Linguistics33(4):493?552.Cohen, Michael H., James P. Giangola, andJennifer Balogh.
2004.
Voice User InterfaceDesign.
Addison-Wesley.Jurafsky, Daniel and James H. Martin.2009.
Speech and Language Processing:An Introduction to Natural LanguageProcessing, Speech Recognition, andComputational Linguistics.
2nd edition.Prentice-Hall.Ranta, Aarne.
2010.
Report from GoogleEMEA Faculty Summit 8?10 February2010.
http://www.molto-project.eu/node/842.Ranta, Aarne.
2011.
Grammatical Framework:Programming with Multilingual Grammars.CSLI Publications, Stanford, CA.Mats Wire?n is Associate Professor in computational linguistics at Stockholm University.
He re-ceived his M.Sc.
and Ph.D. from Linko?ping University.
His current research focuses on languageacquisition, whereas a long-time interest of his is parsing.
He has also carried out research inspeech translation and spoken dialogue.
Wire?n?s address is Department of Linguistics, StockholmUniversity, SE-106 91 Stockholm, Sweden; e-mail: mats.wiren@ling.su.se.780
