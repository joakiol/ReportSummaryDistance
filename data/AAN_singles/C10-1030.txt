Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 259?267,Beijing, August 2010Generating Learner-Like Morphological Errors in RussianMarkus DickinsonIndiana Universitymd7@indiana.eduAbstractTo speed up the process of categorizinglearner errors and obtaining data for lan-guages which lack error-annotated data,we describe a linguistically-informedmethod for generating learner-like mor-phological errors, focusing on Russian.We outline a procedure to select likely er-rors, relying on guiding stem and suffixcombinations from a segmented lexicon tomatch particular error categories and rely-ing on grammatical information from theoriginal context.1 IntroductionWork on detecting grammatical errors in the lan-guage of non-native speakers covers a range oferrors, but it has largely focused on syntax ina small number of languages (e.g., Vandeven-ter Faltin, 2003; Tetreault and Chodorow, 2008).In more morphologically-rich languages, learn-ers naturally make many errors in morphology(Dickinson and Herring, 2008).
Yet for many lan-guages, there is a major bottleneck in system de-velopment: there are not enough error-annotatedlearner corpora which can be mined to discoverthe nature of learner errors, let alne enough datato train or evaluate a system.
Our perspective isthat one can speed up the process of determin-ing the nature of learner errors via semi-automaticmeans, by generating plausible errors.We set out to generate linguistically-plausiblemorphological errors for Russian, a language withrich inflections.
Generating learner-like errors haspractical and theoretical benefits.
First, there isthe issue of obtaining training data; as Foster andAndersen (2009) state, ?The ideal situation for agrammatical error detection system is one where alarge amount of labelled positive and negative ev-idence is available.?
Generated errors can bridgethis gap by creating realistic negative evidence(see also Rozovskaya and Roth, 2010).
As forevaluation data, generated errors have at least oneadvantage over real errors, in that we know pre-cisely what the correct form is supposed to be, aproblem for real learner data (e.g., Boyd, 2010).By starting with a coarse error taxonomy, gen-erating errors can improve categorization.
Gener-ated errors provide data for an expert?e.g., a lan-guage teacher?to search through, expanding thetaxonomy with new error types or subtypes and/ordeprecating error types which are unlikely.
Giventhe lack of real learner data, this has the potentialto speed up error categorization and subsequentsystem development.
Furthermore, error genera-tion techniques can be re-used, adjusting the er-rors for different learner levels, first languages,and so forth.The error generation process can benefit by us-ing linguistic properties to mimic learner varia-tions.
This can lead to more realistic errors, a ben-efit for machine learning (Foster and Andersen,2009), and can also provide feedback for the lin-guistic representation used to generate errors by,e.g., demonstrating under which linguistic condi-tions certain error types are generated and underwhich they are not.We are specifically interested in generatingRussian morphological errors.
To do this, we needa knowledge base representing Russian morphol-ogy, allowing us to manipulate linguistic proper-ties.
After outlining the coarse error taxonomy259(section 2), we discuss enriching a part-of-speech(POS) tagger lexicon with segmentation informa-tion (section 3).
We then describe the steps in er-ror generation (section 4), highlighting decisionswhich provide insight for the analysis of learnerlanguage, and show the impact on POS tagging insection 5.2 Error taxonomyRussian is an inflecting language with relativelyfree word order, meaning that morphological syn-tactic properties are often encoded by affixes.
In(1a), for example, the verb ??????
needs a suf-fix to indicate person and number, and ??
is thethird person singular form.1 By contrast, (1b) il-lustrates a paradigm error: the suffix ??
is thirdsingular, but not the correct one.
Generating sucha form requires having access to individual mor-phemes and their linguistic properties.
(1) a.
??????+??begin-3s[nachina+et]b.
*??????+??begin-3s[nachina+it](diff.
verb paradigm)This error is categorized as a suffix error in fig-ure 1, expanding the taxonomy in Dickinson andHerring (2008).
Stem errors are similarly catego-rized, with Semantic errors defined with respectto a particular context (e.g., using a different stemthan required by an activity).For formation errors (#3), one needs to knowhow stems relate.
For instance, some verbschange their form depending on the suffix, as in(2).
In (2c), the stem and suffix are morpholog-ically compatible, just not a valid combination.One needs to know that ???
is a variant of ???.
(2) a.
???+??can-3p[mog+ut]b.
???+??can-3s[mozh+et]c.
*???+?
?can-3p[mozh+ut] (#3)(wrong formation)Using a basic lexicon without such knowledge,it is hard to tell formation errors apart from lex-1For examples, we write the Cyrillic form and include aRoman transliteration (SEV 1362-78) for ease of reading.0.
Correct: The word is well-formed.1.
Stem errors:(a) Stem spelling error(b) Semantic error2.
Suffix errors:(a) Suffix spelling error(b) Lexicon error:i. Derivation error: The wrong POS isused (e.g., a noun as a verb).ii.
Inherency error: The ending is for adifferent subclass (e.g., inanimate asan animate noun).
(c) Paradigm error: The ending is from thewrong paradigm.3.
Formation errors: The stem does not followappropriate spelling/sound change rules.4.
Syntactic errors: The form is correct, butused in an in appropriate syntactic context(e.g., nominative case in a dative context)?
Lexicon incompleteness: The form may bepossible, but is not attested.Figure 1: Error taxonomyicon incompleteness (see section 4.2.2).
If ??-???
(2c) is generated and is not in the lexicon,we do not know whether it is misformed or simplyunattested.
In this paper, we group together suchcases, since this allows for a simpler and morequickly-derivable lexicon.We have added syntactic errors, whereas Dick-inson and Herring (2008) focused on strictly mor-phological errors.
Learners make syntactic errors(e.g., Rubinstein, 1995; Rosengrant, 1987), andwhen creating errors, a well-formed word may re-sult.
In the future, syntactic errors can be subdi-vided (Boyd, 2010).This classification is of possible errors, makingno claim about the actual distribution of learnererrors, and does not delve into issues such aserrors stemming from first language interference(Rubinstein, 1995).
Generating errors from thepossible types allows one to investigate whichtypes are plausible in which contexts.260It should be noted that we focus on inflec-tional morphology in Russian, meaning that wefocus on suffixes.
Prefixes are rarely used in Rus-sian as inflectional markers; for example, prefixesmark semantically-relevant properties for verbs ofmotion.
The choice of prefix is thus related tothe overall word choice, an issue discussed underRandom stem generation in section 4.2.4.3 Enriching a POS lexiconTo create errors, we need a segmented lexiconwith morphological information, as in (3).
Here,the word ????
(mogu, ?I am able to?)
is split intostem and suffix, with corresponding POS tags.2(3) a.
???,Vm-----a-p,?,Vmip1s-a-pb.
???,Vm-----a-p,??,Vmip3s-a-pc.
??
?,Vm-----a-p,NULL,Vmis-sma-pThe freely-available POS lexicon from Sharoffet al (2008), specifically the file for the POStagger TnT (Brants, 2000), contains full words(239,889 unique forms), with frequency informa-tion.
Working with such a rich database, we onlyneed segmentation, providing a quickly-obtainedlexicon (cf.
five years for a German lexicon inGeyken and Hanneforth, 2005).In the future, one could switch to a differenttagset, such as that in Hana and Feldman (2010),which includes reflexivity, animacy, and aspectfeatures.
One could also expand the lexicon, byadapting algorithms for analyzing unknown words(e.g., Mikheev, 1997), as suggested by Feldmanand Hana (2010).
Still, our lexicon continues thetrend of linking traditional categories used for tag-ging with deeper analyses (Sharoff et al, 2008;Hana and Feldman, 2010).33.1 Finding segments/morphemesWe use a set of hand-crafted rules to segmentwords into morphemes, of the form: if the tag is xand the word ends with y, make y the suffix.
Suchrules are easily and quickly derivable from a text-book listing of paradigms.
For certain exceptional2POS tags are from the compositional tagset inSharoff et al (2008).
A full description is at: http://corpus.leeds.ac.uk/mocky/msd-ru.html.3This lexicon now includes lemma information, but eachword is not segmented (Erjavec, 2010).cases, we write word-specific rules.
Additionally,we remove word, tag pairs indicating punctuationor non-words (PUNC, SENT, -).One could use a sophisticated method for lem-matizing words (e.g., Chew et al, 2008; Schoneand Jurafsky, 2001), but we would likely haveto clean the lexicon later; as Feldman and Hana(2010) point out, it is difficult to automaticallyguess the entries for a word, without POS in-formation.
Essentially, we write precise rules tospecify part of the Russian system of suffixes; thelexicon then provides the stems for free.We use the lexicon for generating errors, butit should be compatible with analysis.
Thus, wefocus on suffixes for beginning and intermediatelearners.
We can easily prune or add to the ruleset later.
From an analysis perspective, we need tospecify that certain grammatical properties are ina tag (see below), as an analyzer is to support theprovision of feedback.
Since the rules are freelyavailable,4 changing these criteria for other pur-poses is straightforward.3.1.1 Segmentation rulesWe have written 1112 general morphologyrules and 59 rules for the numerals ?one?
through?four,?
based on the Nachalo textbooks (Ervinet al, 1997).
A rule is simply a tag, suffix pair.For example, in (4), Ncmsay (Noun, common,masculine, singular, accusative, animate [yes])words should end in either ?
(a) or ?
(ya).
(4) a. Ncmsay, ?b.
Ncmsay, ?A program consults this list and segments aword appropriately, requiring at least one charac-ter in the stem.
In the case where multiple suffixesmatch (e.g., ???
(eni) and ?
(i) for singular neuterlocative nouns), the longer one is chosen, as it isunambiguously correct.We add information in 101 of the 1112rules.
All numerals, for instance, are tagged asMc-s (Numeral, cardinal, [unspecified gender],singular).
The tagset in theory includes propertiessuch as case; they just were not marked (see foot-note 6, though).
Based on the ending, we add all4http://cl.indiana.edu/?boltundevelopment/261possible analyses.
Using an optional output tag,in (5), Mc-s could be genitive (g), locative (l),or dative (d) when it ends in ?
(i).
These rulesincrease ambiguity, but are necessary for learnerfeedback.
(5) a. Mc-s, ?, Mc-sgb.
Mc-s, ?, Mc-slc.
Mc-s, ?, Mc-sdIn applying the rules, we generate stem tags, en-coding properties constant across suffixes.
Basedon the word?s tag (e.g., Ncmsay, cf.
(4)) a stemis given a more basic tag (e.g., Ncm--y).3.2 Lexicon statisticsTo be flexible for future use, we have only en-riched 90% of the words (248,014), removing ev-ery 10th word.
Using the set of 1112 rules resultsin a lexicon with 190,450 analyses, where analy-ses are as in (3).
For these 190,450 analyses, thereare 117 suffix forms (e.g., ?, ya) corresponding to808 suffix analyses (e.g., <?, Ncmsay>).
On av-erage 3.6 suffix tags are observed with each stem-tag pair, but 22.2 tags are compatible, indicatingincomplete paradigms.4 Generating errors4.1 Basic procedureTaking the morpheme-based lexicon, we generateerrors by randomly combining morphemes intofull forms.
Such randomness must be constrained,taking into account what types of errors are likelyto occur.The procedure is given in figure 2 and de-tailed in the following sections.
First, we use thecontextually-determined POS tag to restrict thespace of possibilities.
Secondly, given that ran-dom combinations of a stem and a suffix can resultin many unlikely errors, we guide the combina-tions, using a loose notion of likelihood to ensurethat the errors fall into a reasonable distribution.After examining the generated errors, one couldrestrict the errors even further.
Thirdly, we com-pare the stem and suffix to determine the possibletypes of errors.
A full form may have several dif-ferent interpretations, and thus, lastly, we selectthe best interpretation(s).1.
Determine POS properties of the word to begenerated (section 4.2.1).2.
Generate a full-form, via guided randomstem and suffix combination (section 4.2.4).3.
Determine possible error analyses for the fullform (section 4.2.2).4.
Select the error type(s) from among multiplepossible interpretations (section 4.2.3).Figure 2: Error generation procedureBy trying to determine the best error type instep 4, the generation process can provide in-sight into error analysis.
This is important, giventhat suffixes are highly ambiguous; for example,??
(-oj) has at least 6 different uses for adjec-tives.
Analysis is not simply generation in reverse,though.
Importantly, error generation relies uponthe context POS tag for the intended form, forthe whole process.
To morphologically analyzethe corrupted data, one has to POS tag corruptedforms (see section 5).4.2 CorruptionWe use a corpus of 5 million words automaticallytagged by TnT (Brants, 2000) and freely avail-able online (Sharoff et al, 2008).5 Because wewant to make linguistically-informed corruptions,we corrupt only the words we have informationfor, identifying the words in the corpus which arefound in the lexicon with the appropriate POStag.6 We also select only words which have in-flectional morphology: nouns, verbs, adjectives,pronouns, and numerals.74.2.1 Determining word properties (step 1)We use the POS tag to restrict the properties ofa word, regardless of how exactly we corrupt it.Either the stem and its tag or the suffix and its tag5See http://corpus.leeds.ac.uk/mocky/.6We downloaded the TnT lexicon in 2008, but the corpusin 2009; although no versions are listed on the website, thereare some discrepancies in the tags used (e.g., numeral tagsnow have more information).
To accommodate, we use alooser match for determining whether a tag is known, namelychecking whether the tags are compatible.
In the future, onecan tweak the rules to match the newer lexicon.7Adverbs inflect for comparative forms, but we do notconsider them here.262can be used as an invariant, to guide the gener-ated form (section 4.2.4).
In (6a), for instance, theadjective (Af) stem or plural instrumental suffix(Afp-pif) can be used as the basis for genera-tion.
(6) a.
Original: ??????
(serymi, ?gray?)7?
???/Af+???/Afp-pifb.
Corrupted: ???+??
(seroj)The error type is defined in terms of the originalword?s POS tag.
For example, when we generate acorrectly-formed word, as in (6b), it is a syntacticerror if it does not match this POS tag.4.2.2 Determining error types (step 3)Before discussing word corruption in step 2(section 4.2.4), we need to discuss how error typesare determined (this section) and how to han-dle multiple possibilities (section 4.2.3), as thesesteps help guide step 2.
After creating a corruptedword, we elucidate all possible interpretations instep 3 by comparing each suffix analysis with thestem.
If the stem and suffix form a legitimateword (in the wrong context), it is a syntactic er-ror.
Incompatible features means a derivation orinherency error, depending upon which featuresare incompatible.
If the features are compati-ble, but there is no attested form, it is either aparadigm error?if we know of a different suffixwith the same grammatical features?or a forma-tion/incompleteness issue, if not.This is a crude morphological analyzer (cf.Dickinson and Herring, 2008), but bases its anal-yses on what is known about the invariant part ofthe original word.
If we use ???
(ymi) from (6a)as an invariant, for instance, we know to treat it asa plural instrumental adjective ending, regardlessof any other possible interpretations, because thatis how it was used in this context.4.2.3 Selecting the error type (step 4)Corrupted forms may have many possible anal-yses.
For example, in (6b), the suffix ??
(oj)has been randomly attached to the stem ???
(ser).With the stem fixed as an adjective, the suf-fix could be a feminine locative adjective (syn-tactic error), a masculine nominative adjective(paradigm error), or an instrumental femininenoun (derivation error).
Given what learners arelikely to do, we can use some heuristics to restrictthe set of possible error types.First, we hypothesize that a correctly-formedword is more likely a correct form than a mis-formed word.
This means that correct wordsand syntactic errors?correctly-formed words inthe wrong context?have priority over other errortypes.
For (6b), for instance, the syntactic erroroutranks the paradigm and derivation errors.Secondly, we hypothesize that a contextually-appropriate word, even if misformed, ismore likely the correct interpretation than acontextually-inappropriate word.
When we havecases where there is: a) a correctly-formed wordnot matching the context (a syntactic error), andb) a malformed word which matches the context(e.g., a paradigm error), we list both possibilities.Finally, derivation errors seem less likely thanthe others (a point confirmed by native speakers),giving them lower priority.
Given these heuristics,not only can we rule out error types after gener-ating new forms, but we can also split the errorgeneration process into different steps.4.2.4 Corrupting selected words (step 2)Using these heuristics, we take a known wordand generate errors based on a series of choices.For each choice, we randomly generate a num-ber between 0 and 1 and choose based on a giventhreshold.
Thresholds should be reset when moreis known about error frequency, and more deci-sions added as error subtypes are added.Decision #1: Correct forms The first choice iswhether to corrupt the word or not.
Currently, thethreshold is set at 0.5.
If we corrupt the word, wecontinue on to the next decision.Decision #2: Syntactic errors We can eithergenerate a syntactic or a morphological error.
Onthe assumption that syntactic errors are more com-mon, we currently set a threshold of 0.7, generat-ing syntactic errors 70% of the time and morpho-logical form errors 30% of the time.To generate a correct form used incorrectly, weextract the stem from the word and randomly se-lect a new suffix.
We keep selecting a suffix until263we obtain a valid form.8 An example is given in(7): the original (7a) is a plural instrumental ad-jective, unspecified for gender; in (7b), it is singu-lar nominative feminine.
(7) a.
??????grayAfp-pif???????eyesNcmpin..SENTb.
?????Afpfsnf??????
?Ncmpin.SENTOne might consider ensuring that each errordiffers from the original in only one property.
Orone might want to co-vary errors, such that, inthis case, the adjective and noun both change frominstrumental to nominative.
While this is eas-ily accomplished algorithmically, we do not knowwhether learners obey these constraints.
Generat-ing errors in a relatively unbounded way can helppinpoint these types of constraints.While the form in (7b) is unambiguous, syntac-tic errors can have more than one possible analy-sis.
In (8), for instance, this word could be cor-rupted with an -??
(-oj) ending, indicating fem-inine singular genitive, instrumental, or locative.We include all possible forms.
(8) ?????Afpfsg.Afpfsi.Afpfsl??????
?Ncmpin.SENTLikewise, considering the heuristics in sec-tion 4.2.3, generating a syntactic error may leadto a form which may be contextually-appropriate.Consider (9): in (9a), the verb-preposition com-bination requires an accusative (Ncnsan).
Bychanging -?
to -?, we generate a form which couldbe locative case (Ncnsln, type #4) or, since -?
can be an accusative marker, a misformed ac-cusative with the incorrect paradigm (#2c).
Welist both possibilities.
(9) a. .
.
.
???????.
.
.
(he) looked.
.
.
Vmis-sma-p?intoSp-a???
?the skyNcnsanb.
.
.
.
?.
.
.
Sp-a???
?Ncnsan+2c.Ncnsln+4Syntactic errors obviously conflate many dif-ferent error types.
The taxonomy for German8We ensure that we do not generate the original form, sothat the new form is contextually-inappropriate.from Boyd (2010), for example, includes selec-tion, agreement, and word order errors.
Our syn-tactic errors are either selection (e.g., wrong caseas object of preposition) or agreement errors (e.g.,subject-verb disagreement in number).
However,without accurate syntactic information, we cannotdivvy up the error space as precisely.
With thePOS information, we can at least sort errors basedon the ways in which they vary from the original(e.g., incorrect case).Finally, if no syntactic error can be derived, werevert to the correct form.
This happens when thelexicon contains only one form for a given stem.Without changing the stem, we cannot generate anew form which is verifiably correct.Decision #3: Morphological errors The nextdecision is: should we generate a true morpholog-ical error or a spelling error?
We currently biasthis by setting a 0.9 threshold.
The process forgenerating morphological errors (0.9) is describedin the next few sections, after which spelling er-rors (0.1) are described.
Surely, 10% is an un-derestimate of the amount of spelling errors (cf.Rosengrant, 1987); however, for refining a mor-phological error taxonomy, biasing towards mor-phological errors is appropriate.Decision #4: Invariant morphemes When cre-ating a context-dependent morphological error,we have to ask what the unit, or morpheme, isupon which the full form is dependent.
The finalchoice is thus to select whether we keep the stemanalysis constant and randomize the suffix or keepthe suffix and randomize the stem.
Consider thatthe stem is the locus of a word?s semantic proper-ties, and the (inflectional) suffix reflects syntacticproperties.
If we change the stem of a word, wecompletely change the semantics (error type #1b).Changing the suffix, on the other hand, creates amorphological error with the same basic seman-tics.
We thus currently randomly generate a suffix90% of the time.Random suffix generation Randomly attach-ing a suffix to a fixed stem is the same procedureused above to generate syntactic errors.
Here,however, we force the form to be incorrect, notallowing syntactic errors.
If attaching a suffix re-264sults in a correct form (contextually-appropriateor not), we re-select a random suffix.Similarly, the intention is to generate inherency(#2bii), paradigm (#2c), and formation (#3) errors(or lexicon incompleteness).
All of these seemto be more likely than derivation (#2bi) errors, asdiscussed in section 4.2.3.
If we allow any suffixto combine, we will overwhelmingly find deriva-tion errors.
As pointed out in Dickinson and Her-ring (2008), such errors can arise when a learnertakes a Russian noun, e.g., ???
(dush, ?shower?
)and attempts to use it as a verb, as in English, e.g.,????
(dushu) with first person singular morphol-ogy.
In such cases, we have the wrong stem be-ing used with a contextually-appropriate ending.Derviation errors are thus best served with ran-dom stem selection, as described in the next sec-tion.
To rule out derivation errors, we only keepsuffix analyses which have the same major POS asthe stem.For some stems, particular types of errors areimpossible to generate.
a) Inherency errors do notoccur for underspecified stems, as happens withadjectives.
For example, ??
?- (nov-, ?new?)
is anadjective stem which is compatible with any ad-jective ending.
b) Paradigm errors cannot occurfor words whose suffixes in the lexicon have no al-ternate forms; for instance, there is only one wayto realize a third singular nominative pronoun.
c)Lexicon incompleteness cannot be posited for aword with a complete paradigm.
These facts showthat the generated error types are biased, depend-ing upon the POS and the completeness of the lex-icon.Random stem generation Keeping the suffixfixed and randomly selecting a stem ties the gen-erated form to the syntactic context, but changesthe semantics.
Thus, these generated errors arefirstly semantic errors (#1b), featuring stems in-appropriate for the context, in addition to havingsome other morphological error.
The fact that,given a context, we have to generate two errorslends weight to the idea that these are less likely.A randomly-generated stem will most likelybe of a different POS class than the suffix, re-sulting in a derivation error (#2bi).
Further, aswith all morphological errors, we restrict the gen-erated word not to be a correctly-formed word,and we do not allow the stem or the suffix to beclosed class items.
It makes little sense to putnoun inflections on a preposition, for example,and derivation errors involve open class words.9Spelling errors For spelling errors, we create anerror simply by randomly inserting, deleting, orsubstituting a single character in the word.10 Thiswill either be a stem (#1a) or a suffix (#2a) error.
Itis worth noting that since we know the process ofcreating this error, we are able to compartmental-ize spelling errors from morphological ones.
Anerror analyzer, however, will have a harder timedistinguishing them.5 Tagging the corpusFigure 3 presents the distribution of error typesgenerated, where Word refers to the number ofwords with a particular error type, as opposed tothe count of error type+POS pairs, as each wordcan have more than one POS for an error type (cf.(9b)).
For the 780,924 corrupted words, there are2.67 error type+POS pairs per corrupted word.
In-herency (#2bii) errors in particular have many tagsper word, since the same suffix can have multiplesimilar deviations from the original (cf.
(8)).
Fig-ure 3 shows that we have generated roughly thedistribution we wanted, based on our initial ideasof linguisic plausibility.Type Word POS Type Word POS1a 19,661 19,661 1b-2bi 11,772 11,7722a 6,560 6,560 1b-2bii 5,529 5,5292bii 150,710 749,292 1b-2c 279 2792c 94,211 94,211 1b-3+ 1,770 1,7704 524,269 721,0513+ 83,763 208,208 1b-all 19,350 19,350Figure 3: Distribution of generated errorsWithout an error detection system, it is hard togauge the impact of the error generation process.Although it is not a true evaluation of the errorgeneration process, as a first step, we test a POS9Learners often misuse, e.g., prepositions, but these er-rors do not affect morphology.
Future work should examinethe relation between word choice and derivation errors, in-cluding changes in prefixes.10One could base spelling errors on known or assumedphonological confusions (cf.
Hovermale and Martin, 2008).265tagger against the newly-created data.
This helpstest the difficulty of tagging corrupted forms, aneeded step in the process of analyzing learnerlanguage.
Note that for providing feedback, itseems desirable to have the POS tagger matchthe tag of the corrupted form.
This is a differentgoal than developing POS taggers which are ro-bust to noise (e.g., Bigert et al, 2003), where thetag should be of the original word.To POS tag, we use the HMM tagger TnT(Brants, 2000) with the model from http://corpus.leeds.ac.uk/mocky/.
The re-sults on the generated data are in figure 4, usinga lenient measure of accuracy: a POS tag is cor-rect if it matches any of the tags for the hypoth-esized error types.
The best performance is foruncorrupted known words,11 but notable is that,out of the box, the tagger obtains 79% precisionon corrupted words when compared to the gener-ated tags, but is strongly divergent from the orig-inal (no longer correct) tags.
Given that 67%(524,269780,924 ) of words have a syntactic error?i.e., awell-formed word in the wrong context?this in-dicates that the tagger is likely relying on the formin the lexicon more than the context.Gold TagsOriginal Error # wordsCorrupted 3.8% 79.0% 780,924Unchanged:Known 92.1% 92.1% 965,280Unknown 81.9% 81.9% 3,484,909Overall 72.1% 83.4% 5,231,113Figure 4: POS tagging results, comparing taggeroutput to Original tags and Error tagsIt is difficult to break down the results for cor-rupted words by error type, since many words areambiguous between several different error types,and each interpretation may have a different POStag.
Still, we can say that words which are syn-tactic errors have the best tagging accuracy.
Ofthe 524,269 words which may be syntactic er-rors, TnT matches a tag in 96.1% of cases.
Suffixspelling errors are particularly in need of improve-11Known here refers to being in the enriched lexicon, asthese are the cases we specificaly did not corrupt.ment: only 17.3% of these words are correctlytagged (compared to 62% for stem spelling er-rors).
With an ill-formed suffix, the tagger simplydoes not have reliable information.
To improvetagging for morphological errors, one should in-vestigate which linguistic properties are being in-correctly tagged (cf.
sub-tagging in Hana et al,2004) and what roles distributional, morphologi-cal, or lexicon cues should play in tagging learnerlanguage (see also D?
?az-Negrillo et al, 2010).6 Conclusions and OutlookWe have developed a general method for gener-ating learner-like morphological errors, and wehave demonstrated how to do this for Russian.While many insights are useful for doing erroranalysis (including our results for POS taggingthe resulting corpus), generation proceeds fromknowing grammatical properties of the originalword.
Generating errors based on linguistic prop-erties has the potential to speed up the process ofcategorizing learner errors, in addition to creatingrealistic data for machine learning systems.
As aside effect, we also added segmentation to a wide-coverage POS lexicon.There are several directions to pursue.
Themost immediate step is to properly evaluate thequality of generated errors.
Based on this analysis,one can refine the taxonomy of errors, and therebygenerate even more realistic errors in a future iter-ation.
Additionally, building from the initial POStagging results, one can work on generally analyz-ing the morphology of learner language, includ-ing teasing apart what information a POS taggerneeds to examine and dealing with multiple hy-potheses (Dickinson and Herring, 2008).AcknowledgementsI would like to thank Josh Herring, Anna Feld-man, Jennifer Foster, and three anonymous re-viewers for useful comments on this work.266ReferencesBigert, Johnny, Ola Knutsson and Jonas Sjo?bergh(2003).
Automatic Evaluation of Robustnessand Degradation in Tagging and Parsing.
InProceedings of RANLP-2003.
Borovets, Bul-garia, pp.
51?57.Boyd, Adriane (2010).
EAGLE: an Error-Annotated Corpus of Beginning Learner Ger-man.
In Proceedings of LREC-10.
Malta.Brants, Thorsten (2000).
TnT ?
A Statistical Part-of-Speech Tagger.
In Proceedings of ANLP-00.Seattle, WA, pp.
224?231.Chew, Peter A., Brett W. Bader and Ahmed Abde-lali (2008).
Latent Morpho-Semantic Analysis:Multilingual Information Retrieval with Char-acter N-Grams and Mutual Information.
In Pro-ceedings of Coling 2008.
Manchester, pp.
129?136.D?
?az-Negrillo, Ana, Detmar Meurers, SalvadorValera and Holger Wunsch (2010).
Towardsinterlanguage POS annotation for effectivelearner corpora in SLA and FLT.
Language Fo-rum .Dickinson, Markus and Joshua Herring (2008).Developing Online ICALL Exercises for Rus-sian.
In The 3rd Workshop on Innovative Useof NLP for Building Educational Applications.Columbus, OH, pp.
1?9.Erjavec, Tomaz?
(2010).
MULTEXT-East Ver-sion 4: Multilingual Morphosyntactic Specifi-cations, Lexicons and Corpora.
In Proceedingsof LREC-10.
Malta.Ervin, Gerard L., Sophia Lubensky and Donald K.Jarvis (1997).
Nachalo: When in Russia .
.
.
.New York: McGraw-Hill.Feldman, Anna and Jirka Hana (2010).
AResource-light Approach to Morpho-syntacticTagging.
Amsterdam: Rodopi.Foster, Jennifer and Oistein Andersen (2009).GenERRate: Generating Errors for Use inGrammatical Error Detection.
In The 4th Work-shop on Innovative Use of NLP for Building Ed-ucational Applications.
Boulder, CO, pp.
82?90.Geyken, Alexander and Thomas Hanneforth(2005).
TAGH: A Complete Morphology forGerman Based on Weighted Finite State Au-tomata.
In FSMNLP 2005.
Springer, pp.
55?66.Hana, Jirka and Anna Feldman (2010).
A Posi-tional Tagset for Russian.
In Proceedings ofLREC-10.
Malta.Hana, Jirka, Anna Feldman and Chris Brew(2004).
A Resource-light Approach to RussianMorphology: Tagging Russian using Czechresources.
In Proceedings of EMNLP-04.Barcelona, Spain.Hovermale, DJ and Scott Martin (2008).
Devel-oping an Annotation Scheme for ELL SpellingErrors.
In Proceedings of MCLC-5 (MidwestComputational Linguistics Colloquium).
EastLansing, MI.Mikheev, Andrei (1997).
Automatic Rule Induc-tion for Unknown-Word Guessing.
Computa-tional Linguistics 23(3), 405?423.Rosengrant, Sandra F. (1987).
Error Patterns inWritten Russian.
The Modern Language Jour-nal 71(2), 138?145.Rozovskaya, Alla and Dan Roth (2010).
TrainingParadigms for Correcting Errors in Grammarand Usage.
In Proceedings of HLT-NAACL-10.Los Angeles, California, pp.
154?162.Rubinstein, George (1995).
On Case Errors Madein Oral Speech by American Learners of Rus-sian.
Slavic and East European Journal 39(3),408?429.Schone, Patrick and Daniel Jurafsky (2001).Knowledge-Free Induction of Inflectional Mor-phologies.
In Proceedings of NAACL-01.
Pitts-burgh, PA.Sharoff, Serge, Mikhail Kopotev, Tomaz?
Erjavec,Anna Feldman and Dagmar Divjak (2008).
De-signing and evaluating Russian tagsets.
In Pro-ceedings of LREC-08.
Marrakech.Tetreault, Joel and Martin Chodorow (2008).
TheUps and Downs of Preposition Error Detectionin ESL Writing.
In Proceedings of COLING-08.
Manchester.Vandeventer Faltin, Anne (2003).
Syntactic errordiagnosis in the context of computer assistedlanguage learning.
The`se de doctorat, Univer-site?
de Gene`ve, Gene`ve.267
