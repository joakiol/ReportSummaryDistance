Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 36?43Manchester, August 2008Large Scale Production of Syntactic Annotations to Move ForwardPatrick Paroubek, Anne Vilnat, Sylvain LoiseauLIMSI-CNRSBP 133 91403 Orsay CedexFranceprenom.nom@limsi.frGil FrancopouloTagmatica126 rue de Picpus 75012 ParisFrancegil.francopoulo@tagmatica.comOlivier HamonELDA and LIPN-P1355-57 rue Brillat-Savarin 75013 Paris,Francehamon@elda.orgEric Villemonte de la ClergerieAlpage-INRIADom.
de Voluceau Rocquencourt,B.P.
105, 78153 Le Chesnay, FranceEric.De La Clergerie@inria.frAbstractThis article presents the methodology ofthe PASSAGE project, aiming at syntacti-cally annotating large corpora by compos-ing annotations.
It introduces the anno-tation format and the syntactic annotationspecifications.
It describes an importantcomponent of the methodolgy, namely anWEB-based evaluation service, deployedin the context of the first PASSAGE parserevaluation campaign.1 IntroductionThe last decade has seen, at the international level,the emergence of a very strong trend of researcheson statistical methods in Natural Language Pro-cessing.
In our opinion, one of its origins, inparticular for English, is the availability of largeannotated corpora, such as the Penn Treebank(1M words extracted from the Wall Street journal,with syntactic annotations; 2ndrelease in 19951,the British National Corpus (100M words cover-ing various styles annotated with parts of speech2),or the Brown Corpus (1M words with morpho-syntactic annotations).
Such annotated corporawere very valuable to extract stochastic grammarsor to parametrize disambiguation algorithms.
Forinstance (Miyao et al, 2004) report an experimentwhere an HPSG grammar is semi-automaticallyaquired from the Penn Treebank, by first annotat-ing the treebank with partially specified derivationc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.1http://www.cis.upenn.edu/?treebank/2http://www.natcorp.ox.ac.uk/trees using heuristic rules , then by extracting lex-ical entries with the application of inverse gram-mar rules.
(Cahill et al, 2004) managed to ex-tract LFG subcategorisation frames and paths link-ing long distance dependencies reentrancies fromf-structures generated automatically for the Penn-II treebank trees and used them in an long distancedependency resolution algorithm to parse new text.They achieved around 80% f-score for fstructuresparsing on the WSJ part of the Penn-II treebank,a score comparable to the ones of the state-of-the-art hand-crafted grammars.
With similar re-sults, (Hockenmaier and Steedman, 2007) trans-lated the Penn Treebank into a corpus of Combina-tory Categorial Grammar (CCG) derivations aug-mented with local and long-range word to worddependencies and used it to train wide-coveragestatistical parsers.
The development of the PennTreebank have led to many similar proposals ofcorpus annotations3.
However, the development ofsuch treebanks is very costly from an human pointof view and represents a long standing effort, inparticular for getting of rid of the annotation errorsor inconsistencies, unavoidable for any kind of hu-man annotation.
Despite the growing number ofannotated corpora, the volume of data that can bemanually annotated remains limited thus restrict-ing the experiments that can be tried on automaticgrammar acquisition.
Furthermore, designing anannotated corpus involves choices that may blockfuture experiments from acquiring new kinds oflinguistic knowledge because they necessitate an-notation incompatible or difficult to produce fromthe existing ones.With PASSAGE (de la Clergerie et al, 2008b),we believe that a new option becomes possible.3http://www.ims.uni-stuttgart.de/projekte/TIGER/related/links.shtml36Funded by the French ANR program on DataWarehouses and Knowledge, PASSAGE is a 3-year project (2007?2009), coordinated by INRIAproject-team Alpage.
It builds up on the re-sults of the EASy French parsing evaluation cam-paign, funded by the French Technolangue pro-gram, which has shown that French parsing sys-tems are now available, ranging from shallow todeep parsing.
Some of these systems were nei-ther based on statistics, nor extracted from a tree-bank.
While needing to be improved in robustness,coverage, and accuracy, these systems has nev-ertheless proved the feasibility to parse mediumamount of data (1M words).
Preliminary experi-ments made by some of the participants with deepparsers (Sagot and Boullier, 2006) indicate thatprocessing more than 10 M words is not a prob-lem, especially by relying on clusters of machines.These figures can even be increased for shallowparsers.
In other words, there now exists sev-eral French parsing systems that could parse (andre-parse if needed) large corpora between 10 to100 M words.Passage aims at pursuing and extending theline of research initiated by the EASy campaignby using jointly 10 of the parsing systems thathave participated to EASy.
They will be used toparse and re-parse a French corpus of more than100 M words along the following feedback loopbetween parsing and resource creation as follows(de la Clergerie et al, 2008a):1.
Parsing creates syntactic annotations;2.
Syntactic annotations create or enrich linguis-tic resources such as lexicons, grammars orannotated corpora;3.
Linguistic resources created or enriched onthe basis of the syntactic annotations are thenintegrated into the existing parsers;4.
The enriched parsers are used to create richer(e.g., syntactico-semantic) annotations;5. etc.
going back to step 1In order to improve the set of parameters ofthe parse combination algorithm (inspired fromthe Recognizer Output Voting Error Reduction,i.e.
ROVER, experiments), two parsing evalu-ation campaigns are planned during PASSAGE,the first of these already took place at the end of2007 (de la Clergerie et al, 2008b).
In the follow-ing, we present the annotation format specificationand the syntactic annotation specifications of PAS-SAGE, then give an account of how the syntacticannotations were compared in the first evaluationcampaign, by first describing the evaluation met-rics and the web server infrastructure that was de-ployed to process them.
We conclude by showinghow the results so far achieved in PASSAGE willcontribute to the second part of the project, extract-ing and refining enriched linguistic annotations.2 PASSAGE Annotation FormatThe aim is to allow an explicit representation ofsyntactic annotations for French, whether such an-notations come from human annotators or parsers.The representation format is intended to be usedboth in the evaluation of different parsers, so theparses?
representations should be easily compara-ble, and in the construction of a large scale anno-tation treebank which requires that all French con-structions can be represented with enough details.The format is based on three distinct specifica-tions and requirements:1.
MAF (ISO 24611)4and SynAF (ISO 24615)5which are the ISO TC37 specifications formorpho-syntactic and syntactic annotation(Ide and Romary, 2002) (Declerck, 2006)(Francopoulo, 2008).
Let us note that thesespecifications cannot be called ?standards?because they are work in progress and thesedocuments do not yet have the status Pub-lished Standard.
Currently, their official sta-tus is only Committee Draft.2.
The format used during the previous TECH-NOLANGUE/EASY evaluation campaignin order to minimize porting effort for the ex-isting tools and corpora.3.
The degree of legibility of the XML tagging.From a technical point of view, the format is acompromise between ?standoff?
and ?embedded?notation.
The fine grain level of tokens and wordsis standoff (wrt the primary document) but higherlevels use embedded annotations.
A standoff nota-tion is usually considered more powerful but less4http://lirics.loria.fr/doc pub/maf.pdf5http://lirics.loria.fr/doc pub/N421 SynAF CD ISO 24615.pdf37Figure 1: UML diagram of the structure of an an-notated documentreadable and not needed when the annotations fol-low a (unambiguous) tree-like structure.
Let usadd that, at all levels, great care has been taken toensure that the format is mappable onto MAF andSynAF, which are basically standoff notations.The structure of a PASSAGE annotated docu-ment may be summarized with the UML diagramin Figure1.
The document begins by the declara-tion of all the morpho-syntactic tagsets (MSTAG)that will be used within the document.
These dec-larations respect the ISO Standard Feature Struc-ture Representation (ISO 24610-1).
Then, tokensare declared.
They are the smallest unit address-able by other annotations.
A token is unsplittableand holds an identifier, a character range, and acontent made of the original character string.
Aword form is an element referencing one or sev-eral tokens.
It has has two mandatory attributes:an identifier and a list of tokens.
Some optional at-tributes are allowed like a part of speech, a lemma,an inflected form (possibly after spelling correc-tion or case normalization) and morpho-syntactictags.
The following XML fragment shows howthe original fragment ?Les chaises?
can be repre-sented with all the optional attributes offered bythe PASSAGE annotation format :<T id="t0" start="0" end="3">Les</T><W id="w0" tokens="t0"pos="definiteArticle"lemma="le"form="les"mstag="nP"/><T id="t1" start="4" end="11">chaises</T><W id="w1" tokens="t1"pos="commonNoun"lemma="chaise"form="chaises"mstag="nP gF"/>Note that all parts of speech are taken from theISO registry6(Francopoulo et al, 2008).
As inMAF, a word may refer to several tokens in or-der to represent multi-word units like ?pomme deterre?.
Conversely, a unique token may be referedby two different words in order to represent resultsof split based spelling correction like when ?un-etable?
is smartly separated into the words ?une?and ?table?.
The same configuration is required torepresent correctly agglutination in fused preposi-tions like the token ?au?
that may be rewritten intothe sequence of two words ?`a?
?le?.
On the con-trary of MAF, cross-reference in token-word linksfor discontiguous spans is not allowed for the sakeof simplicity.
Let us add that one of our require-ment is to have PASSAGE annotations mappableonto the MAF model and not to map all MAF an-notations onto PASSAGE model.
A G element de-notes a syntactic group or a constituent (see detailsin section 3).
It may be recursive or non-recursiveand has an identifier, a type, and a content made ofword forms or groups, if recursive.
All group typevalues are taken from the ISO registry.
Here is anexample :<T id="t0" start="0" end="3">Les</T><T id="t1" start="4" end="11">chaises</T><G id="g0" type="GN"><W id="w0" tokens="t0"/><W id="w1" tokens="t1"/></G>A group may also hold optional attributes like syn-tactic tagsets of MSTAG type.
The syntactic re-lations are represented with a standoff annotationswhich refer to groups and word forms.
A relationis defined by an identifier, a type, a source, and atarget (see details in section 3.
All relation types,like ?subject?
or ?direct object?
are mappable ontothe ISO registry.
An unrestricted number of com-ments may be added to any element by means ofthe mark element (i.e.
M).
Finally, a ?Sentence?6Data Category Registry, see http://syntax.inist.fr38element gathers tokens, word forms, groups, rela-tions and marks and all sentences are included in-side a ?Document?
element.3 PASSAGE Syntactic AnnotationSpecification3.1 IntroductionThe annotation formalism used in PASSAGE7isbased on the EASY one(Vilnat et al, 2004) whichwhose first version was crafted in an experimentalproject PEAS (Gendner et al, 2003), with inspira-tion taken from the propositions of (Carroll et al,2002).
The definition has been completed with theinput of all the actors involved in the EASY evalu-ation campaign (both parsers?
developers and cor-pus providers) and refined with the input of PAS-SAGE participants.
This formalism aims at mak-ing possible the comparison of all kinds of syn-tactic annotation (shallow or deep parsing, com-plete or partial analysis), without giving any ad-vantage to any particular approach.
It has sixkinds of syntactic ?chunks?, we call constituentsand 14 kinds of relations The annotation formal-ism allows the annotation of minimal, continuousand non recursive constituents, as well as the en-coding of relations wich represent syntactic func-tions.
These relations (all of them being binary, ex-cept for the ternary coordination) have sources andtargets which may be either forms or constituents(grouping several forms).
Note that the PASSAGEannotation formalism does not postulate any ex-plicit lexical head.3.2 Constituent annotationsFor the PASSAGE campaigns, 6 kinds of con-stituents (syntactic ?chunks?)
have been consid-ered and are illustrated in Table 3.2:?
the Noun Phrase (GN for Groupe Nominal)may be made of a noun preceded by a de-terminer and/or by an adjective with its ownmodifiers, a proper noun or a pronoun;?
the prepositional phrase (GP, for groupepr?epositionnel ) may be made of a preposi-tion and the GN it introduces, a contracteddeterminer and preposition, followed by theintroduced GN, a preposition followed by anadverb or a relative pronoun replacing a GP;7Annotation guide: http://www.limsi.fr/Recherche/CORVAL/PASSAGE/eval 1/2007 1005PEAS reference annotations v11.12.html?
the verb kernel (NV for noyau verbal ) in-cludes a verb, the clitic pronouns and possibleparticles attached to it.
Verb kernels may havedifferent forms: conjugated tense, present orpast participle, or infinitive.
When the con-jugation produces compound forms, distinctNVs are identified;?
the adjective phrase (GA for groupe adjec-tival) contains an adjective when it is notplaced before the noun, or past or present par-ticiples when they are used as adjectives;?
the adverb phrase (GR for groupe adverbial )contains an adverb;?
the verb phrase introduced by a preposition(PV) is a verb kernel with a verb not inflected(infinitive, present participle,...), introducedby a preposition.
Some modifiers or adverbsmay also be included in PVs.GN - la tr`es grande porte8(the very big door);- Rouletabille- eux (they), qui (who)GP - de la chambre (from the bedroom),- du pavillon (from the lodge)- de l`a (from there), dont (whose)NV - j?entendais (I heared)- [on ne l?entendait]9plus(we could no more hear her)- Jean [viendra] (Jean will come)- [d?esob?eissant] `a leurs parents(disobeying their parents),- [ferm?ee] `a clef (key closed)- Il [ne veut] pas [venir](He doesn?t want to come),- [ils n?
?etaient] pas [ferm?es](they were not closed),GA - les barreaux [intacts] (the intact bars)- la solution [retenue] fut...(the chosen solution has been...),- les enfants [d?esob?eissants](the disobeying children)GR - aussi (also)- vous n?auriez [pas] (you would not)PV - [pour aller] `a Paris (for going to Paris),- de vraiment bouger (to really move)Table 1: Constituent examples393.2.1 Syntactic Relation annotationsThe dependencies establish all the links betweenthe minimal constituents described above.
All par-ticipants, corpus providers and campaign organiz-ers agreed on a list of 14 kinds of dependencieslisted below:1. subject-verb (SUJ V): may be inside thesame NV as between elle and ?etait in elle?etait (she was), or between a GN and a NV asbetween mademoiselle and appelait in Made-moiselle appelait (Miss was calling);2. auxiliary-verb (AUX V), between two NVsas between a and construit in: on a construitune maison (we have built a house);3. direct object-verb (COD V): the relation isannotated between a main verb (NV) and anoun phrase (GN), as between construit andla premi`ere automobile in: on a construit lapremi`ere automobile (we have built the firstcar);4. complement-verb (CPL V): to link to theverb the complements expressed as GP or PVwhich may be adjuncts or indirect objects, asbetween en quelle ann?ee and construit in enquelle ann?ee a-t on construit la premi`ere au-tomobile (In which year did we build the firstcar);5. modifier-verb (MOD V): concerns the con-stituants which certainly modify the verb,and are not mandatory, as adverbs or adjunctclauses, as between profond?ement or quandla nuit tombe and dort in Jean dort pro-fond?ement quand la nuit tombe (Jean deeplysleeps when the night falls);6. complementor (COMP): to link the intro-ducer and the verb kernel of a subordinateclause, as between qu?
and viendra in Jepense qu?il viendra (I think that he willcome); it is also used to link a preposition anda noun phrase when they are not contiguous,preventing us to annotate them as GP;7. attribute-subject/object (ATB SO): betweenthe attribute and the verb kernel, and precis-ing that the attribute is relative to (a) the sub-ject as between grand and est in il est grand), or (b) the object as between ?etrange andtrouve in il trouve cette explication ?etrange;8. modifier-noun (MOD N): to link to the nounall the constituents which modify it, as the ad-jective, the genitive, the relative clause... Thisdependency is annotated between unique andfen?etre in l?unique fen?etre (the unique win-dow) or between de la chambre and la portein la porte de la chambre (the bedroom door);9. modifier-adjective (MOD A): to relate to theadjective the constituents which modify it, asbetween tr`es et belle in ?la tr`es belle collec-tion (the very impressive collection) or be-tween de son fils and fi`ere in elle est fi`ere deson fils (she is proud of her son);10. modifier-adverb (MOD R): the same kind ofdependency than MOD A for the adverbs, asbetween tr`es and gentiment in elle vient tr`esgentiment (she comes very kindly);11. modifier-preposition (MOD P): to relate toa preposition what modifies it, as betweenpeu and avant in elle vient peu avant lui (shecomes just before him);12. coordination (COORD): to relate the coor-dinate and the coordinated elements, as be-tween Pierre, Paul and et in Pierre et Paularrivent (Paul and Pierre are arriving);13. apposition (APP): to link the elements whichare placed side by side, when they refer to thesame object, as between le d?eput?e and YvesTavernier in Le d?eput?e Yves Tavernier ... (theDeputy Yves Tavernier...);14. juxtaposition (JUXT): to link constituentswhich are neither coordinate nor in an appo-sition relation, as in enumeration.
It also linksclauses as on ne l?entendait et elle ?etait inon ne l?
entendait plus ... elle ?etait peut-?etremorte (we did not hear her any more... per-haps she was dead).Some dependencies are illustrated in the two an-notated sentences illutrated in figure .
These anno-tations have been made using EasyRef, a specificWeb annotation tool developed by INRIA.4 PASSAGE First Evaluation Campaign4.1 Evalution ServiceThe first PASSAGE evaluation campaign wascarried out in two steps.
During the ini-tial one-month development phase, a develop-ment corpus was used to improve the quality of40Figure 2: Example of two sentences annotationsparsers.
This development corpus from the TECH-NOLANGUE/EASY is composed of 40,000 sen-tences, out of which 4,000 sentences have beenmanually annotated for the gold standard.
Basedon these annotated sentences, an automatic WEB-based evaluation server provides fast performancefeedback to the parsers?
developers.
At the endof this first phase, each participant indicated whathe thought was his best parser run and got evalu-ated on a new set of 400 sentences selected fromanother part of the developement corpus whichmeanwhile had been manually annotated for thepurpose and kept undisclosed.The two phases represent a strong effort for theevaluators.
To avoid adding the cost of managingthe distribution and installation of the evaluationpackage at each developer?s site, the solution of theWEB evaluation service was chosen.
A few infras-tructures have been already experimented in NLP,like GATE (Cunningham et al, 2002) infrastruc-tures, but to our knowledge none has been used toprovide an WEB-based evaluation service as PAS-SAGE did.
The server was designed to managetwo categories of users: parser developers and or-ganizers.
To the developers, it provides, almost inreal time, confidential and secure access to the au-tomatic evaluation of their submitted parses.
Tothe organizers, it give access to statistics enablingthem to follow the progress made by the develop-ers, and easy management of the test phase.
Theevaluation server provides, through a simple WEBbrowser, access to both coarse and fine grain statis-tics to a developer?s performance evaluation, glob-ally for the whole corpus, at the level of a partic-ular syntactic annotation or of a particular genrespecific subcorpus, and also at the level of a singleannotation for a particular word form.Figure 3: Overall functional relations results4.2 Performance ResultsTen systems participated to the constituents anno-tation task.
For most of the systems, F-measure isup to 90% and only three systems are between 80%and 90%.
The trend is quite the same for Recalland Precision.
Around 96.5% of the constituentsreturned by the best system are correct and it found95.5% of the constituents present in gold standard.Figure 3 shows the results of the seven systems thatparticipated to the functional relations annotationtask.
Performance is lower than for constituentsand differences between systems are larger, an evi-dence that the task remains more difficult.
No sys-tems gets a performance above 70% in F-measure,three are above 60% and two above 50%.
The lasttwo systems are above 40%.4.3 Systems ImprovementsThe higher system gets increasing results from thebeginning of the development phase to the testphase for both constituents and relations.
How-ever, although the increase for relations is rathercontinuous, constituents results grow during thefirst few development evaluations, then reach athreshold from which results do not vary.
Thiscan be explained by the fact that the constituentscores are rather high, while for relations, scoresare lower and starting from low scores.Using the evaluation server, system improvesits performance by 50% for the constituents and600% for the relations, although performance varyaccording to the type of relation or constituent.Moreover, in repeating development evaluations,another consequence was the convergence of pre-cision and recall.415 Parser?s outputs combinationThe idea to combine the output of systems partic-ipating to an evalauation campaign in order to ob-tain a combination with better performance thanthe best one was invented to our knowledge by J.Fiscus (Fiscus, 1997) in a DARPA/NIST speechrecognition evaluation (ROVER/Reduced OutputVoting Error Reduction).
By aligning the out-put of the participating speech transcription sys-tems and by selecting the hypothesis which wasproposed by the majority of the systems, he ob-tained better performances than these of the bestsystem.
The idea gained support in the speech pro-cessing community(L?o?of et al, 2007) and in gen-eral better results are obtained with keeping onlythe output of the two or three best performing sys-tems, in which case the relative improvement cango up to 20% with respect to the best performance(Schwenk and Gauvain, 2000).
For text process-ing, the ROVER procedure was applied to POStagging (Paroubek, 2000) and machine translation(Matusov et al, 2006).In our case, we will use the text itself to realignthe annotations provided by the various parser be-fore computing their combination, as we did forour first experiments with the EASY evaluationcampaign data (Paroubek et al, 2008).
Since itis very likely taht the different parsers do not usethe same word and sentence segmentation, we willrealign all the data along a common word and sen-tence segmentation obtained by majority vote fromthe different outputs.But our motivation for using such procedureis not only concerned with performance improve-ment but also with the obtention of a confidencemeasure for the annotation since if all systemsagree on a particular annotation, then it is verylikely to be true.At this stage many options are open for the waywe want to apply the ROVER algorithm, since wehave both constituents and relations in our anno-tations.
We could vary the selection order (be-tween constituents and relations), or use differ-ent comparison functions for the sources/targets ofconstituents/relations(Patrick Paroubek, 2006), orperform incremental/global merging of the annoa-tions, or explore different weightings/thresholdingstrategies etc.
In passage, ROVER experimentsare only beginning and we have yet to determinewhich is the best strategy before applying it toword and sentence free segmentation data.
In theearly experiment we did with the ?EASy classic?PASSAGE track which uses a fixed word and sen-tence segmentation, we measured an improvementin precision for some specific subcorpora and an-notations but improvement in recall was harder toget.6 ConclusionThe definition of a common interchange syntacticannotation format is an essential element of anymethodology aiming at the creation of large an-notated corpora from the cooperation of parsingsystems to acquire new linguistic knowledge.
Butthe formalism aquires all of its value when backed-up by the deployment of a WEB-based evaluationservice as the PASSAGE examples shows.
167experiments were carried out during the develop-ment phase (around 17 experiments per participantin one month).
The results of the test phase wereavailable less than one hour after the end of the de-velopment phase.
The service proved so success-ful that the participants asked after the evaluation,that the evaluation service be extended to supportevaluation as a perennial serviceReferencesCahill, Aoife, Michael Burke, Ruth O?Donovan, JosefVan Genabith, and Andy Way.
2004.
Long-distancedependency resolution in automatically acquiredwide-coverage pcfg-based lfg approximations.
InProceedings of the 42nd Meeting of the Associationfor Computational Linguistics (ACL?04), Main Vol-ume, pages 319?326, Barcelona, Spain, July.Carroll, J., D. Lin, D. Prescher, and H. Uszkoreit.2002.
Proceedings of the workshop beyond parse-val - toward improved evaluation measures for pars-ing systems.
In Proceedings of the 3rd InternationalConference on Language Resources and Evaluation(LREC), Las Palmas, Spain.Cunningham, Hamish, Diana Maynard, KalinaBontcheva, and Valentin Tablan.
2002.
Gate:an architecture for development of robust hlt ap-plications.
In ACL ?02: Proceedings of the 40thAnnual Meeting on Association for ComputationalLinguistics, pages 168?175, Morristown, NJ, USA.Association for Computational Linguistics.Declerck, T. 2006.
Synaf: towards a standard for syn-tactic annotation.
In In proceedings of the fifth in-ternational conference on Language Resources andEvaluation (LREC 2006), Genoa, Italy, May.
ELRA.Fiscus, Jonathan G. 1997.
A post-processing systemto yield reduced word error rates: recognizer outputvoting error reduction (rover).
In In proceedings of42the IEEE Workshop on Automatic Speech Recogni-tion and Understanding, pages 347?357, Santa Bar-bara, CA.Francopoulo, G., T. Declerck, V. Sornlertlamvanich,E.
de la Clergerie, and M. Monachini.
2008.
Datacategory registry: Morpho-syntactic and syntacticprofiles.
Marrakech.
LREC.Francopoulo, Gil.
2008.
Tagparser: Well on the wayto iso-tc37 conformance.
In In proceedings of theInternational Conference on Global Interoperabilityfor Language Resources (ICGL), pages 82?88, HongKong, January.Gendner, V?eronique, Gabriel Illouz, Mich`ele Jardino,Laura Monceaux, Patrick Paroubek, Isabelle Robba,and Anne Vilnat.
2003.
Peas the first instanciationof a comparative framework for evaluating parsers offrench.
In Proceedings of the 10thConference of theEuropean Chapter fo the Association for Computa-tional Linguistics, pages 95?98, Budapest, Hungary,April.
ACL.
Companion Volume.Hockenmaier, Julia and Mark Steedman.
2007.
Ccg-bank: A corpus of ccg derivations and dependencystructures extracted from the penn treebank.
Com-putational Linguistics, 33(3):355?396.Ide, N. and L. Romary.
2002.
Standards for languageressources.
Las Palmas.
LREC.L?o?of, J., C. Gollan, S. Hahn, G. Heigold, B. Hoffmeis-ter, C. Plahl, D. Rybach, R. Schl?uter, , and H. Ney.2007.
The rwth 2007 tc-star evaluation system foreuropean english and spanish.
In In proceedings ofthe Interspeech Conference, pages 2145?2148.Matusov, Evgeny, N. Ueffing, and Herman Ney.
2006.Automatic sentence segmentation and punctuationprediction for spoken language translation.
In Pro-ceedings of the International Workshop on Spo-ken Language Translation (IWSLT), pages 158?165,Trento, Italy.de la Clergerie, Eric, Christelle Ayache, Ga?el de Chal-endar, Gil Francopoulo, Claire Gardent, and PatrickParoubek.
2008a.
Large scale production of syntac-tic annotations for french.
In In proceedings of theFirst Workshop on Automated Syntactic Annotationsfor Interoperable Language Resources at IGCL?08,pages 45?52, Hong Kong, January.de la Clergerie, Eric, Olivier Hamon, Djamel Mostefa,Christelle Ayache, Patrick Paroubek, and Anne Vil-nat.
2008b.
Passage: from french parser evalua-tion to large sized treebank.
In ELRA, editor, Inproceedings of the sixth international conference onLanguage Resources and Evaluation (LREC), Mar-rakech, Morroco, May.
ELRA.Miyao, Yusuke, Takashi Ninomiya, and Jun?ichi Tsu-jii.
2004.
Corpus-oriented grammar developmentfor acquiring a head-driven phrase structure gram-mar from the penn treebank.
In In Proceedings ofthe First International Joint Conference on NaturalLanguage Processing (IJCNLP-04).Paroubek, Patrick, Isabelle Robba, Anne Vilnat, andChristelle Ayache.
2008.
Easy, evaluation of parsersof french: what are the results?
In Proceedings ofthe 6thInternational Conference on Language Re-sources and Evaluation (LREC), Marrakech, Mor-roco.Paroubek, Patrick.
2000.
Language resources as by-product of evaluation: the multitag example.
InIn proceedings of the Second International Con-ference on Language Resources and Evaluation(LREC2000), volume 1, pages 151?154.Patrick Paroubek, Isabelle Robba, Anne Vilnat Chris-telle Ayache.
2006.
Data, annotations and mea-sures in easy - the evaluation campaign for parsersof french.
In ELRA, editor, In proceedings ofthe fifth international conference on Language Re-sources and Evaluation (LREC 2006), pages 315?320, Genoa, Italy, May.
ELRA.Sagot, Beno?
?t and Pierre Boullier.
2006.
Efficientparsing of large corpora with a deep lfg parser.
InIn proceedings of the sixth international conferenceon Language Resources and Evaluation (LREC),Genoa, Italy, May.
ELDA.Schwenk, Holger and Jean-Luc Gauvain.
2000.
Im-proved rover using language model information.
InIn proceedings of the ISCA ITRW Workshop on Au-tomatic Speech Recognition: Challenges for the newMillenium, pages 47?52, Paris, September.Vilnat, A., P. Paroubek, L. Monceaux, I. Robba,V.
Gendner, G. Illouz, and M. Jardino.
2004.
Theongoing evaluation campaign of syntactic parsing offrench: Easy.
In Proceedings of the 4thInternationalConference on Language Resources and Evaluation(LREC), pages 2023?2026, Lisbonne, Portugal.43
