Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 737?742,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsConstrained Semantic Forests for Improved DiscriminativeSemantic ParsingWei LuInformation Systems Technology and DesignSingapore University of Technology and Designluwei@sutd.edu.sgAbstractIn this paper, we present a model forimproved discriminative semantic parsing.The model addresses an important limi-tation associated with our previous state-of-the-art discriminative semantic parsingmodel ?
the relaxed hybrid tree modelby introducing our constrained semanticforests.
We show that our model is able toyield new state-of-the-art results on stan-dard datasets even with simpler features.Our system is available for download fromhttp://statnlp.org/research/sp/.1 IntroductionThis paper addresses the problem of parsing natu-ral language sentences into their corresponding se-mantic representations in the form of formal logi-cal representations.
Such a task is also known assemantic parsing (Kate and Mooney, 2006; Wongand Mooney, 2007; Lu et al., 2008; Kwiatkowskiet al., 2010).One state-of-the-art model for semantic pars-ing is our recently introduced relaxed hybrid treemodel (Lu, 2014), which performs integrated lexi-con acquisition and semantic parsing within a sin-gle framework utilizing efficient algorithms fortraining and inference.
The model allows naturallanguage phrases to be recursively mapped to se-mantic units, where certain long-distance depen-dencies can be captured.
It relies on representa-tions called relaxed hybrid trees that can jointlyrepresent both the sentences and semantics.
Themodel is essentially discriminative, and allowsrich features to be incorporated.Unfortunately, the relaxed hybrid tree modelhas an important limitation: it essentially doesnot allow certain sentence-semantics pairs to bejointly encoded using the proposed relaxed hy-brid tree representations.
Thus, the model is un-able to identify joint representations for certainsentence-semantics pairs during the training pro-cess, and is unable to produce desired outputs forcertain inputs during the evaluation process.
Inthis work, we propose a solution addressing theabove limitation, which makes our model more ro-bust.
Through experiments, we demonstrate thatour improved discriminative model for semanticparsing, even when simpler features are used, isable to obtain new state-of-the-art results on stan-dard datasets.2 Related WorkSemantic parsing has recently attracted a signif-icant amount of attention in the community.
Inthis section, we provide a relatively brief discus-sion of prior work in semantic parsing.
The hy-brid tree model (Lu et al., 2008) and the Bayesiantree transducer based model (Jones et al., 2012)are generative frameworks, which essentially as-sume natural language and semantics are jointlygenerated from an underlying generative process.Such models are efficient, but are limited in theirpredictive power due to the simple independenceassumptions made.On the other hand, discriminative models areable to exploit arbitrary features and are usuallyable to give better results.
Examples of such mod-els include the WASP system (Wong and Mooney,2006) which regards the semantic parsing prob-lem as a statistical machine translation problem,the UBL system (Kwiatkowski et al., 2010) whichperforms CCG-based semantic parsing using alog-linear model, as well as the relaxed hybrid treemodel (Lu, 2014) which extends the generativehybrid tree model.
This extension results in a dis-criminative model that incorporates rich featuresand allows long-distance dependencies to be cap-tured.
The relaxed hybrid tree model has achievedthe state-of-the-art results on standard benchmarkdatasets across different languages.Performing semantic parsing under other forms737mambmdmcw1w2w3w4w5w6w7w8w9w10(a)maw10mbmdw9w7w8mcw6w3w4w5w1w2(b)ma(w1w2) w3w4w5w6w7w8w9(w10)mb(w3w4w5) w6(w7w8) w9md(w9)mc(w6)(c)Figure 1: The semantics-sentence pair (a), an example hybrid tree (b), and an example relaxed hybridtree (c).of supervision is also possible.
Clarke et al.
(2010)proposed a model that learns a semantic parser foranswering questions without relying on semanticannotations.
Goldwasser et al.
(2011) presenteda confidence-driven approach to semantic parsingbased on self-training.
Liang et al.
(2013) in-troduced semantic parsers based on dependencybased semantics (DCS) that map sentences intotheir denotations.
In this work, we focus on pars-ing sentences into their formal semantic represen-tations.3 Relaxed Hybrid TreesWe briefly discuss our previously proposed re-laxed hybrid tree model (Lu, 2014) in this section.The model is a discriminative semantic parsingmodel which extends the generative hybrid treemodel (Lu et al., 2008).
Both systems are publiclyavailable1.Let us use m to denote a complete semanticrepresentation, n to denote a complete natural lan-guage sentence, and h to denote a complete latentstructure that jointly represents both m and n. Themodel defines the conditional probability for ob-serving a (m,h) pair for a given natural languagesentence n using a log-linear approach:P?
(m,h|n) =e???(n,m,h)?m?,h??H(n,m?)e???(n,m?,h?
)(1)where ?
is the set of parameters (weights of fea-tures) used by the model.
Figure 1 (a) gives anexample sentence-semantics pair.
A real exampletaken from the GeoQuery dataset is shown in Fig-ure 2.Note that h is a complete latent structure thatjointly represents a natural language sentence and1http://statnlp.org/research/sp/QUERY : answer(RIVER)RIVER : exclude(RIVER, RIVER)RIVER : traverse(STATE)STATE : stateid(STATENAME)STATENAME : (?tn?
)RIVER : river(all)What rivers do not run through Tennessee ?Figure 2: An example tree-structured semanticrepresentation (above) and its corresponding nat-ural language sentence (below).its corresponding semantic representation.
Typi-cally, to limit the space of latent structures, certainassumptions have to be made to h. In our work,we assume that h must be from a space consistingof relaxed hybrid tree structures (Lu, 2014).The relaxed hybrid trees are analogous to thehybrid trees, which was earlier introduced as agenerative framework.
One major distinction be-tween these two types of representations is thatthe relaxed hybrid tree representations are able tocapture unbounded long-distance dependencies ina principled way.
Such dependencies were un-able to be captured by hybrid tree representationslargely due to their generative settings.
Figure 1gives an example of a hybrid tree and a relaxedhybrid tree representation encoding the sentencew1w2w3w4w5w6w7w8w9w10and the se-mantics ma(mb(mc,md)).In the hybrid tree structure, each word is strictlyassociated with a semantic unit.
For example theword w3is associated with the semantic unit mb.In the relaxed hybrid tree, however, each word isnot only directly associated with exactly one se-mantic unit m, but also indirectly associated with738mambmdmcmaw1w2.
.
.mbw1w2md(w2)mc(w1)mambmdw2mcw1w1w2(a) (b) (c)Figure 3: (a) Example semantics-sentence pairthat cannot be jointly represented with relaxed hy-brid trees if pattern X is disallowed.
(b) Exam-ple relaxed hybrid tree that consists of an infinitenumber of nodes when pattern X is allowed.
(c)Example hybrid tree jointly representing both thesemantics and the sentence (where pattern X is al-lowed).all other semantic units that are predecessors of m.For example, the word w3now is directly associ-ated with mb, but is also indirectly associated withma.
These indirect associations allow the long-distance dependencies to be captured.Both the hybrid tree and relaxed hybrid treemodels define patterns at each level of their latentstructure which specify how the words and childsemantic units are organized at each level.
Forexample, within the semantic unit ma, we havea pattern wXw which states that we first havewords that are directly associated with ma, fol-lowed by some words covered by its first child se-mantic unit, then another sequence of words di-rectly associated with ma.3.1 LimitationsOne important difference between the hybrid treerepresentations and the relaxed hybrid tree repre-sentations is the exclusion of the pattern X in thelatter.
This ensured relaxed hybrid trees with aninfinite number of nodes were not considered (Lu,2014) when computing the denominator term ofEquation 1.
In relaxed hybrid tree, H(n,m) wasimplemented as a packed forest representation forexponentially many possible relaxed hybrid treeswhere pattern X was excluded.By allowing pattern X, we allow certain seman-tic units with no natural language word counter-#Args Patterns0 w1 [w]X[w]2 [w]X[w]Y[w], [w]Y[w]X[w]Table 1: The patterns allowed for our model.
[w]denotes an optional sequence of natural languagewords.
E.g., [w]X[w] refers to the following 4patterns: wX, Xw, wXw, and X (the pattern ex-cluded by the relaxed hybrid tree model).part to exist in the joint relaxed hybrid tree repre-sentation.
This may lead to possible relaxed hy-brid tree representations consisting of an infinitenumber of internal nodes (semantic units), as seenin Figure 3 (b).
When pattern X is allowed, bothmaand mbare not directly associated with anynatural language word, so we are able to furtherinsert arbitrarily many (compatible) semantic unitsbetween the two units maand mbwhile the re-sulting relaxed hybrid tree remains valid.
There-fore we can construct a relaxed hybrid tree repre-sentation that contains the given natural languagesentence w1w2with an infinite number of nodes.This issue essentially prevents us from comput-ing the denominator term of Equation 1 since itinvolves an infinite number of possible m?and h?.To eliminate relaxed hybrid trees consisting ofan infinite number of nodes, pattern X is dis-allowed in the relaxed hybrid trees model (Lu,2014).
However, disallowing pattern X has ledto other issues.
Specifically, for certain semantics-sentence pairs, it is not possible to find relaxed hy-brid trees that jointly represent them.
In the exam-ple semantics-sentence pair given in Figure 3 (a),it is not possible to find any relaxed hybrid tree thatcontains both the sentence and the semantics sinceeach semantic unit which takes one argument mustbe associated with at least one word.
On the otherhand, it is still possible to find a hybrid tree repre-sentation for both the sentence and the semanticswhere pattern X is allowed (see Figure 3 (c)).In practice, we can alleviate this issue by ex-tending the lengths of the sentences.
For example,we can append the special beginning-of-sentencesymbol ?s?
and end-of-sentence symbol ?/s?
toall sentences to increase their lengths, allowingthe relaxed hybrid trees to be constructed for cer-tain sentence-semantics pairs with short sentences.However, such an approach does not resolve thetheoretical limitation of the model.7394 Constrained Semantic ForestsTo address this limitation, we allow pattern X tobe included when building our new discrimina-tive semantic parsing model.
However, as men-tioned above, doing so will lead to latent struc-tures (relaxed hybrid tree representations) of infi-nite heights.
To resolve such an issue, we insteadadd an additional constraint ?
limiting the heightof a semantic representation to a fixed constant c,where c is larger than the maximum height of allthe trees appearing in the training set.Table 1 summarizes the list of patterns that ourmodel considers.
This is essentially the same asthose considered by the hybrid tree model.Our new objective function is as follows:P?(m,h|n)=e???(n,m,h)?m??M,h??H?(n,m?)e???(n,m?,h?
)(2)where M refers to the set of all possible seman-tic trees whose heights are less than or equal to c,and H?(n,m?)
refers to the set of possible relaxedhybrid tree representations where the pattern X isallowed.The main challenge now becomes the compu-tation of the denominator term in Equation 2, asthe set M is still very large.
To properly handleall such semantic trees in an efficient way, we in-troduce a constrained semantic forest (CSF) rep-resentation of M here.
Such a constrained seman-tic forest is a packed forest representation of ex-ponentially many possible unique semantic trees,where we set the height of the forest to c. By con-trast, it was not possible in our previous relaxedhybrid tree model to introduce such a compactrepresentation over all possible semantic trees.
Inour previous model?s implementation, we directlyconstructed for each sentence n a different com-pact representation over all possible relaxed hy-brid trees containing n.Setting the maximum height to c effectivelyguarantees that all semantic trees contained inthe constrained semantic forest have a height nogreater than c. We then constructed the (exponen-tially many) relaxed hybrid tree representationsbased on the constrained semantic forest M andeach input sentence n. We used a single packedforest representation to represent all such relaxedhybrid tree representations.
This allows the com-putation of the denominator to be performed ef-ficiently using similar dynamic programming al-gorithms described in (Lu, 2014).
Optimizationof the model parameters were done by using L-BFGS (Liu and Nocedal, 1989), where the gradi-ents were computed efficiently using an analogousdynamic programming algorithm.5 ExperimentsOur experiments were conducted on the publiclyavailable multilingual GeoQuery dataset.
Vari-ous previous works on semantic parsing used thisdataset for evaluations (Wong and Mooney, 2006;Kate and Mooney, 2006; Lu et al., 2008; Joneset al., 2012).
The dataset consists of 880 naturallanguage sentences where each sentence is cou-pled with a formal tree-structured semantic repre-sentation.
The early version of this dataset wasannotated with English only (Wong and Mooney,2006; Kate and Mooney, 2006), and Jones et al.
(2012) released a version that is annotated withthree additional languages: German, Greek andThai.
To make our system directly comparable toprevious works, we used the same train/test splitused in those works (Jones et al., 2012; Lu, 2014)for evaluation.
We also followed the standard ap-proach for evaluating the correctness of an outputsemantic representation from our system.
Specifi-cally, we used a standard script to construct Prologqueries based on the outputs, and used the queriesto retrieve answers from the GeoQuery database.Following previous works, we regarded an out-put semantic representation as correct if and onlyif it returned the same answers as the gold stan-dard (Jones et al., 2012; Lu, 2014).The results of our system as well as those ofseveral previous systems are given in Table 2.We compared our system?s performance againstthose of several previous works.
The WASP sys-tem (Wong and Mooney, 2006) is based on statis-tical machine translation technique while the HY-BRIDTREE+ system (Lu et al., 2008) is based onthe generative hybrid tree model augmented witha discriminative re-ranking stage where certainglobal features are used.
UBL-S (Kwiatkowski etal., 2010) is a CCG-based semantic parsing sys-tem.
TREETRANS (Jones et al., 2012) is the sys-tem based on tree transducers.
RHT (Lu, 2014) isthe discriminative semantic parsing system basedon relaxed hybrid trees.In practice, we set c (the maximum height ofa semantic representation) to 20 in our experi-740SystemEnglish Thai German GreekAcc.
F Acc.
F Acc.
F Acc.
FWASP 71.1 77.7 71.4 75.0 65.7 74.9 70.7 78.6HYBRIDTREE+ 76.8 81.0 73.6 76.7 62.1 68.5 69.3 74.6UBL-S 82.1 82.1 66.4 66.4 75.0 75.0 73.6 73.7TREETRANS 79.3 79.3 78.2 78.2 74.6 74.6 75.4 75.4RHT (all features) 83.6 83.6 79.3 79.3 74.3 74.3 78.2 78.2This work 86.8 86.8 80.7 80.7 75.7 75.7 79.3 79.3Table 2: Performance of various works across four different languages.
Acc.
: accuracy percentage, F:F1-measure percentage.ments, which we determined based on the heightsof the semantic trees that appear in the trainingdata.
Results showed that our system consistentlyyielded higher results than all the previous sys-tems, including our state-of-the-art relaxed hybridtree system (the full model, when all the featuresare used), in terms of both accuracy score and F1-measure.
We would like to highlight two potentialadvantages of our new model over the old RHTmodel.
First, our model is able to handle certainsentence-semantics pairs which could not be han-dled by RHT during both training and evaluationas discussed in Section 3.1.
Second, our modelconsiders the additional pattern X and thereforehas the capability to capture more accurate depen-dencies between the words and semantic units.We note that in our experiments we used a smallsubset of the features used by our relaxed hy-brid tree work.
Specifically, we did not use anylong-distance features, and also did not use anycharacter-level features.
As we have mentionedin (Lu, 2014), although the RHT model is ableto capture unbounded long-distance dependencies,for certain languages such as German such long-distance features appeared to be detrimental tothe performance of the system (Lu, 2014, Table4).
Here in this work, we only used simple un-igram features (concatenation of a semantic unitand an individual word that appears directly belowthat unit in the joint representation), pattern fea-tures (concatenation of a semantic unit and the pat-tern below that unit) as well as transition features(concatenation of two semantic units that form aparent-child relationship) described in (Lu, 2014).While additional features could potentially lead tobetter results, using simpler features would makeour model more compact and more interpretable.We summarized in Table 3 the number of featuresused in both the previous RHT system and our sys-tem across four different languages.
It can be seenthat our system only required about 2-3% of theSystem English Thai German GreekRHT 2.1?1062.3?1062.7?1062.6?106This work 5.4?1045.2?1047.5?1046.9?104Table 3: Number of features involved for boththe RHT system and our new system using con-strained semantic forests, across four different lan-guages.features used in the previous system.We also note that the training time for ourmodel is longer than that of the relaxed hybrid treemodel since the space for H?(n,m?)
is now muchlarger than the space for H(n,m?).
In practice,to make the overall training process faster, we im-plemented a parallel version of the original RHTalgorithm.6 ConclusionIn this work, we presented an improved discrim-inative approach to semantic parsing.
Our ap-proach does not have the theoretical limitationassociated with our previous state-of-the-art ap-proach.
We demonstrated through experimentsthat our new model was able to yield new state-of-the-art results on a standard dataset across fourdifferent languages, even though simpler featureswere used.
Since our new model involves simplerfeatures, including unigram features defined overindividual semantic unit ?
word pairs, we believeour new model would aid the joint modeling ofboth distributional and logical semantics (Lewisand Steedman, 2013) within a single framework.We plan to explore this avenue in the future.AcknoledgmentsThe author would like to thank the anonymousreviewers for their helpful comments.
Thiswork was supported by SUTD grant SRG ISTD2013 064 and was partially supported by project61472191 under the National Natural ScienceFoundation of China.741ReferencesJames Clarke, Dan Goldwasser, Ming-Wei Chang, andDan Roth.
2010.
Driving semantic parsing fromthe world?s response.
In Proc.
of CONLL ?10, pages18?27.Dan Goldwasser, Roi Reichart, James Clarke, and DanRoth.
2011.
Confidence driven unsupervised se-mantic parsing.
In Proc.
of ACL ?11, pages 1486?1495.Bevan Keeley Jones, Mark Johnson, and Sharon Gold-water.
2012.
Semantic parsing with bayesian treetransducers.
In Proc.
of ACL ?12, pages 488?496.Rohit J. Kate and Raymond J. Mooney.
2006.
Us-ing string-kernels for learning semantic parsers.
InProc.
of COLING/ACL, pages 913?920.Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-ter, and Mark Steedman.
2010.
Inducing proba-bilistic ccg grammars from logical form with higher-order unification.
In Proc.
EMNLP?10, pages 1223?1233.Mike Lewis and Mark Steedman.
2013.
Combin-ing distributional and logical semantics.
Transac-tions of the Association for Computational Linguis-tics, 1:179?192.Percy Liang, Michael I Jordan, and Dan Klein.
2013.Learning dependency-based compositional seman-tics.
Computational Linguistics, 39(2):389?446.D.
C. Liu and J. Nocedal.
1989.
On the limited mem-ory bfgs method for large scale optimization.
Math.Program., 45(3):503?528, December.Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S.Zettlemoyer.
2008.
A generative model for pars-ing natural language to meaning representations.
InProc.
of EMNLP ?08, pages 783?792.Wei Lu.
2014.
Semantic parsing with relaxed hybridtrees.
In Proc.
of EMNLP ?14.Yuk Wah Wong and Raymond J. Mooney.
2006.Learning for semantic parsing with statistical ma-chine translation.
In Proc.
of HLT/NAACL ?06,pages 439?446.Yuk Wah Wong and Raymond J Mooney.
2007.Learning synchronous grammars for semantic pars-ing with lambda calculus.
In Proc.
of ACL ?07.742
