Query Translation by Text CategorizationPatrick RuchSIM, University Hospital of Geneva24 Micheli du Crest1201 Geneva, SwitzerlandandLITH, Swiss Federal Institute of Technology1015 Lausanne, Switzerlandpatrick.ruch@sim.hcuge.chAbstractWe report on the development of a cross lan-guage information retrieval system, whichtranslates user queries by categorizing thesequeries into terms listed in a controlled vo-cabulary.
Unlike usual automatic text cat-egorization systems, which rely on data-intensive models induced from large train-ing data, our automatic text categorizationtool applies data-independent classifiers: avector-space engine and a pattern matcherare combined to improve ranking of Med-ical Subject Headings (MeSH).
The cate-gorizer also benefits from the availabilityof large thesauri, where variants of MeSHterms can be found.
For evaluation, we usean English collection of MedLine records:OHSUMED.
French OHSUMED queries -translated from the original English queriesby domain experts- are mapped into FrenchMeSH terms; then we use the MeSH con-trolled vocabulary as interlingua to trans-late French MeSH terms into English MeSHterms, which are finally used to query theOHSUMED document collection.
The firstpart of the study focuses on the text toMeSH categorization task.
We use a setof MedLine abstracts as input documentsin order to tune the categorization system.The second part compares the performanceof a machine translation-based cross lan-guage information retrieval (CLIR) systemwith the categorization-based system: theformer results in a CLIR ratio close to 60%,while the latter achieves a ratio above 80%.A final experiment, which combines bothapproaches, achieves a result above 90%.1 IntroductionCross Language Information Retrieval (CLIR)is increasingly relevant as network-based re-sources become commonplace.
In the med-ical domain, it is of strategic importance inorder to fill the gap between clinical records,written in national languages and research re-ports, massively written in English.
There areseveral ways for handling CLIR.
Historically,the most traditional approach to IR in gen-eral and to multilingual retrieval in particular,uses a controlled vocabulary for indexing andretrieval.
In this approach, a librarian selectsfor each document a few descriptors taken froma closed list of authorized terms.
A good ex-ample of such a human indexing is found inthe MedLine database, whose records are man-ually annotated with Medical Subject Headings(MeSH).
Ontological relations (synonyms, re-lated terms, narrower terms, broader terms) canbe used to help choose the right descriptors,and solve the sense problems of synonyms andhomographs.
The list of authorized terms andsemantic relations between them are containedin a thesaurus.
A problem remains, however,since concepts expressed by one single term inone language sometime are expressed by dis-tinct terms in another.
We can observe thatterminology-based CLIR is a common approachin well-delimited fields for which multilingualthesauri already exist (not only in medicine,but also in the legal domain, energy, etc.)
aswell as in multinational organizations or coun-tries with several official languages.
This con-trolled vocabulary approach is often associatedwith Boolean-like engines, and it gives accept-able results but prohibits precise queries thatcannot be expressed with these authorized key-words.
The two main problems are:?
it can be difficult for users to think in termsof a controlled vocabulary, therefore theuse of these systems -like most Boolean-supported engines- is often performed byprofessionals rather than general users;?
this retrieval method ignores the free-textportions of documents during indexing.1.1 Translation-based approachA second approach to multilingual interroga-tion is to use existing machine translation(MT) systems to automatically translate thequeries (Davis, 1998), or even the entire textualdatabase (Oard and Hackett, 1998) (McCarley,1999) from one language to another, therebytransforming the CLIR problem into a mono-lingual information retrieval (MLIR) problem.This kind of method would be satisfactory ifcurrent MT systems did not make errors.
Acertain amount of syntactic error can be ac-cepted without perturbing results of informa-tion retrieval systems, but MT errors in trans-lating concepts can prevent relevant documents,indexed on the missing concepts, from beingfound.
For example, if the word traitement inFrench is translated by processing instead ofprescription, the retrieval process would yieldwrong results.
This drawback is limited in MTsystems that use huge transfer lexicons of nounphrases by taking advantage of frequent colloca-tions to help disambiguation, but in any collec-tion of text, ambiguous nouns will still appearas isolated nouns phrases untouched by this ap-proach.1.2 Using parallel resourcesA third approach receiving increasing attentionis to automatically establish associations be-tween queries and documents independent oflanguage differences.
Seminal researches wereusing latent semantic indexing (Dumais et al,1997).
The general strategy when working withparallel or comparable texts is the following:if some documents are translated into a sec-ond language, these documents can be observedboth in the subspace related to the first lan-guage and the subspace related to the secondone; using a query expressed in the secondlanguage, the most relevant documents in thetranslated subset are extracted (usually usinga cosine measure of proximity).
These rele-vant documents are in turn used to extractclose untranslated documents in the subspace ofthe first language.
This approach use implicitdependency links and co-occurrences that bet-ter approximate the notion of concept.
Sucha strategy has been tested with success onthe English-French language pair using a sam-ple of the Canadian Parliament bilingual cor-pus.
It is reported that for 92% of the En-glish text documents the closest document re-turned by the method was its correct Frenchtranslation.
Such an approach presupposes thatthe sample used for training is representativeof the full database, and that sufficient par-allel/comparable corpora are available or ac-quired.Other approaches are usually based on bilin-gual dictionaries and terminologies, sometimescombined with parallel corpora.
These ap-proaches attempt to infer a word by word trans-fer function: they typically begin by deriving atranslation dictionary, which is then applied toquery translation.
To synthesize, we can con-sider that performances of CLIR systems typi-cally range between 60% and 90% of the corre-sponding monolingual run (Scha?uble and Sheri-dan, 1998).
CLIR ratio above 100% have beenreported (Xu et al, 2001), however such resultswere obtained by computing a weak monolin-gual baseline.2 Our strategySoergel describes a general framework for theuse of multilingual thesauri in CLIR (Soergel,1997), noting that a number of operational Eu-ropean systems employ multilingual thesauri forindexing and searching.
However, except forvery early work (Salton, 1970), there has beenlittle empirical evaluation of multilingual the-sauri in the context of free-text based CLIR,particularly when compared to dictionary andcorpus-based methods.
This may be due to theexpense of constructing multilingual thesauri,but this expense is unlikely to be any more thanthat of creating bilingual dictionaries or even re-alistic parallel collections.
In fact, it seems thatmultilingual thesauri can be built quite effec-tively by merging existing monolingual thesauri,as shown by the current development of the Uni-fied Medical Language System (UMLS1).Our approach to CLIR in MedLine exploitthe UMLS resources and its multilingual com-ponents.
The core technical component of ourcross language engine is an automatic text cat-egorizer, which associates a set of MeSH termsto any input text.
The experimental design isthe following:1. original English OHSUMED (Hersh et al,1994) queries have been translated intoFrench queries by domain experts;2. the OHSUMED document collection is in-dexed using a standard engine;3.
French queries are mapped to a set of1In our experiments, we used the MeSH as dis-tributed in the 2002 release of the UMLS.
Seehttp://umlsks.nlm.nih.gov.French MeSH terms using an automatictext categorizer;4. the top-N returned French terms are trans-lated into English MeSH terms, usingMeSH unique identifiers as interlingua: dif-ferent values of N terms are tested;5. these English MeSH terms are concate-nated to query the OHSUMED documentcollection.2.1 MeSH-driven Text CategorizationAutomatic text categorization has been largelystudied and has led to an impressive amountof papers.
A partial list2 of machine learn-ing approaches applied to text categorizationincludes naive Bayes (McCallum and Nigam,1998), k-nearest neighbors (Yang, 1999), boost-ing (Schapire and Singer, 2000), and rule-learning algorithms (Apte?
et al, 1994).
How-ever, most of these studies apply text classifica-tion to a small set of classes; usually a few hun-dred, as in the Reuters collection (Hayes andWeinstein, 1990).
In comparison, our system isdesigned to handle large class sets (Ruch et al,2003): retrieval tools, which are used, are onlylimited by the size of the inverted file, but 105?6is still a modest range 3 .Our approach is data-poor because it onlydemands a small collection of annotated textsfor fine tuning: instead of inducing a complexmodel using large training data, our catego-rizer indexes the collection of MeSH terms as ifthey were documents and then it treats the in-put as if it was a query to be ranked regardingeach MeSH term.
The classifier is tuned by us-ing English abstracts and English MeSH terms.Then, we apply the indexing system on theFrench MeSH to categorize French queries intoFrench MeSH terms.
The category set rangesfrom about 19 936 -if only unique canonic En-glish MeSH terms are taken into account- upto 139 956 -if synonym strings are consideredin addition to their canonic class.
For evaluat-ing the categorizer, the top 15 returned termsare selected, because it is the average number2See http://faure.iei.pi.cnr.it/?fabrizio/ for an up-dated bibliography.3In text categorization based on learning methods,the scalability issue is twofold: it concerns both the abil-ity of these data-driven systems to work with large con-cept sets, and their ability to learn and generalize reg-ularities for rare events: (Larkey and Croft, 1996) showhow the frequency of concepts in the collection is a majorparameter for learning systems.of MeSH terms per abstract in the OHSUMEDcollection.2.2 Collection and MetricsThe mean average precision (noted Av.
Prec.in the following tables): is the main measurefor evaluating ad hoc retrieval tasks (for bothmonolingual and bilingual runs).
Following(Larkey and Croft, 1996), we also use this mea-sure to tune the automatic text categorizationsystem.Among the 348 566 MedLine citations of theOHSUMED collection4, we use the 233 445records provided with an abstract and anno-tated with MeSH keywords.
We tune the cate-gorization system on a small set of OHSUMEDabstracts: 1200 randomly selected abstractswere used to select the weighting parameters ofthe vector space classifier, and the best com-bination of these parameters with the regularexpression-based classifier.3 MethodsWe first present the MeSH categorizer and itstuning, then the query translation system.3.1 CategorizationIn this section, we present the basic classifiersand their combination for the categorizationtask.
Two main modules constitute the skele-ton of our system: the regular expression(RegEx) component, and the vector space(VS) component.
Each of the basic classifiersimplement known approaches to documentretrieval.
The first tool is based on a regularexpression pattern matcher (Manber and Wu,1994), it is expected to perform well whenapplied on very short documents such askeywords: MeSH terms do not contains morethan 5 tokens.
The second classifier is basedon a vector space engine5.
This second tool isexpected to provide high recall in contrast withthe regular expression-based tool, which shouldprivilege precision.
The former component usestokens as indexing units and can be mergedwith a thesaurus, while the latter uses stems(Porter).
Table 1 shows the results of each4As for queries, we use the corrected version of theOHSUMED queries.
For 5 of the 106 OHSUMED queriesrelevant document sets are not known so only 101 querieswere used.5The IR engine, which has used last year for TREC(Ruch et al, 2004), and the automatic categorizationtoolkit are available on the author?s pages: http://lithwww.epfl.ch/?ruch/softs/softs.htmlSystem or Relevant Prec.
at Av.parameters retrieved Rec.
= 0 Prec.RegEx 3986 .7128 .1601lnc.atn 3838 .7733 .1421anc.atn 3813 .7733 .1418ltc.atn 3788 .7198 .1341ltc.lnn 2946 .7074 .111Table 1: Categorization results.
For the VS en-gine, tf.idf parameters are provided: the firsttriplet indicates the weighting applied to the?document?, i.e.
the concept, while the secondis for the?query?, i.e.
the abstract.
The totalnumber of relevant terms is 15193.classifiers.Regular expressions and MeSH the-saurus.
The regular expression search toolis applied on the canonic MeSH collectionaugmented with the MeSH thesaurus (120020 synonyms).
In this system, string nor-malization is mainly performed by the MeSHterminological resources when the thesaurus isused.
Indeed, the MeSH provides a large set ofrelated terms, which are mapped to a uniqueMeSH representative in the canonic collection.The related terms gather morpho-syntacticvariants, strict synonyms, and a last class ofrelated terms, which mixes up generic and spe-cific terms: for example, Inhibition is mappedto Inhibition (Psychology).
The system cutsthe abstract into 5 token-long phrases andmoves the window through the abstract: theedit-distance is computed between each ofthese 5 token sequence and each MeSH term.Basically, the manually crafted finite-stateautomata allow two insertions or one deletionwithin a MeSH term, and ranks the proposedcandidate terms based on these basic editoperations: insertion costs 1, while deletioncosts 2.
The resulting pattern matcher behaveslike a term proximity scoring system (Rasolofoand Savoy, 2003), but restricted to a 5 tokenmatching window.Vector space classifier.
The vector spacemodule is based on a general IR engine withtf.idf 6 weighting schema.
The engine uses a listof 544 stop words.As for setting the weighting factors, we ob-6We use the SMART representation for expressingstatistical weighting factors: a formal description can befound in (Ruch, 2002).served that cosine normalization was especiallyeffective for our task.
This is not surprising,considering the fact that cosine normalizationperforms well when documents have a similarlength (Singhal et al, 1996).
As for the respec-tive performance of each basic classifiers, table1 shows that the RegEx system performs betterthan any tf.idf schema used by the VS engine,so the pattern matcher provide better resultsthan the vector space engine for automatictext categorization.
However, we also observein table 1 that the VS system gives betterprecision at high ranks (Precisionat Recall=0or mean reciprocal rank) than the RegExsystem: this difference suggests that mergingthe classifiers could be a effective.
The idffactor seems also an important parameter,as shown in table 1, the four best weightingschema use the idf factor.
This observationsuggests that even in a controlled vocabulary,the idf factor is able to discriminate betweencontent and non-content bearing features (suchas syndrome and disease).Classifiers?
fusion.
The hybrid systemcombines the regular expression classifier withthe vector-space classifier.
Unlike (Larkey andCroft, 1996) we do not merge our classifiers bylinear combination, because the RegEx mod-ule does not return a scoring consistent withthe vector space system.
Therefore the combi-nation does not use the RegEx?s edit distance,and instead it uses the list returned by the vec-tor space module as a reference list (RL), whilethe list returned by the regular expression mod-ule is used as boosting list (BL), which servesto improve the ranking of terms listed in RL.A third factor takes into account the length ofterms: both the number of characters (L1) andthe number of tokens (L2, with L2 > 3) are com-puted, so that long and compound terms, whichappear in both lists, are favored over single andshort terms.
We assume that the reference listhas good recall, and we do not set any thresholdon it.
For each concept t listed in the RL, thecombined Retrieval Status Value (cRSV , equa-tion 1) is:cRSVt ={ RSVV S(t) ?
Ln(L1(t) ?
L2(t) ?
k) if t ?
BL,RSVV S(t) otherwise.The value of the k parameter is set empir-ically.
Table 2 shows that the optimal tf.idfparameters (lnc.atn) for the basic VS classi-fier does not provide the optimal combinationWeighting function Relevant Prec.
at Av.concepts.abstracts retrieved Rec.
= 0 Prec.Hybrids: tf.idf + RegExltc.lnn 4308 .8884 .1818lnc.lnn 4301 .8784 .1813anc.ntn 4184 .8746 .1806anc.ntn 4184 .8669 .1795atn.ntn 3763 .9143 .1794Table 2: Combining VS with RegEx.with RegEx.
Measured by mean average preci-sion, the optimal combination is obtained withltc.lnn settings (.1818) 7, whereas atn.ntn max-imizes the Precisionat Recall=0 (.9143).
For ageneral purpose system, we prefer to maximizeaverage precision, since this is the only measurethat summarizes the performance of the full or-dering of concepts, so ltc.lnn factors will be usedfor the following CLIR experiments.3.2 TranslationTo translate user queries, we transform theEnglish MeSH mapping tool described above,which attributes MeSH terms to English ab-stracts in a French mapping tool for mappingFrench OHSUMED queries into French MeSHterms.
The English version of the MeSH issimply replaced by the accented French version(Zweigenbaum and Grabar, 2002) of the MeSH.We use the weighting schema and system com-bination (ltc.lnn + RegEx) as selected in theabove experiments, so we assume that the bestweighting schema regarding average precisionfor mapping abstracts to MeSH terms is ap-propriate for categorizing OHSUMED queries.The only technical differences concern: 1) thethesaural resources, 2) the stemming algorithm.The former are provided by the Unified MedicalLexicon for French consortium (Zweigenbaumet al, 2003) and contains about 20000 Frenchmedical lexemes, with synonyms, while the lat-ter is based on Savoy?s stemmer (Savoy, 1999).An additional parameter is used, in order toavoid translating too many irrelevant concepts,we try to take advantage of the concept rank-ing.
Depending on the length of the query, abalance must be found between having a cou-ple of high precision concepts and missing animportant one.
To evaluate this aspect we donot select the top 15 terms, as in text catego-rization, but we vary this number and we allow7For the augmented term frequency factor (noted a,which is defined by the function ?
+ ?
?
(tf/max(tf)),the value of the parameters is ?
= ?
= 0.5.System Av.
precision CLIR Ratio (%)MLIR (baseline) .2406 100THR-3 .1925 80.0MT .1637 59.7THR-3 + MT .2209 91.8THR-F .1978 82.2Table 3: Average precision and CLIR ratio.different thresholds: 1, 2, 3, 5, 10, and 25.
Fi-nally, by linear regression, we also attempt todetermine a linear fit between the length of thequery (in byte) and the optimal threshold.4 Results and DiscussionEvaluations are computed by retrieving the first1000 documents for each query.
In figure 1,we provide the average precision of each CLIRrun depending on the threshold value.
Themaximum of the average precision is reachedwhen three MeSH terms are selected per query(0.1925), but we can notice that selecting onlytwo terms is as effective (0.19).
On the contrary,selecting the unique top returned term is notsufficient (average precision is below 0.145), andadding more than three terms smoothly degradethe precision, so that with 25 terms, precisionfalls below 0.15.
Table 3 compares the results tothe baseline, i.e.
the score of the monolingualinformation retrieval system (MLIR).
The rel-ative score (CLIR Ratio) of the system whichselects only three terms is 80% (THR-3), andshould be contrasted with the score obtainedby the MT system8 (59.7%).
In the same table,we observe that using a linear function (THR-F), to compute the number of terms to select,results in a very modest improvement as com-pared to using the best performing static value(82.2% vs. 80%): it means that using a dy-namic threshold is not really more effective thantranslating only the top 3 MeSH concepts.
Thismoderate effectiveness may be due to the factthat OHSUMED queries roughly have a simi-lar length.
In contrast, we could expect thatquerying with very short (one word) and verylong queries (querying by documents) could jus-tify the use of a length-dependent threshold.In a last experiment, we try to combine thetwo translation strategies: the translation pro-vided by selecting three terms is simply addedto the translation provided by the MT system.In table 3, a significant improvement (THR3 +8The SysTran system was used.0.140.1450.150.1550.160.1650.170.1750.180.1850.190.1950 5 10 15 20 2511 ptAvergage PrecisionThreshold (# terms)Concept MappingFigure 1: Average precision: different numberof terms are translated by concept mapping.MT = 91.8%) is observed as compared to eachsingle strategies.
It seems to confirm that atleast some of the words, which are not trans-lated or not properly translated by the text cat-egorizer are well translated by the commercialsystem.For example, if we consider a French querysuch as ?ane?mie - ane?mie ferriprive, quelexamen est le meilleur?
(OHSUMED ID =97: ?anemia - iron deficiency anemia, whichtest is best?
), the ranked list of English MeSHterm returned by the categorizer is (mostsimilar terms first, with N = 3): anemia;anemia, iron-deficiency ; anemia, neonatal.We also observe that an important wordlike test is missing from the list of terms,while on the opposite a less relevant termlike anemia, neonatal is provided.
Now, ifwe consider the translation supplied by MT,the above query becomes ?weaken - weakensferriprive, which examination is the best?
:although this translation is far from perfect,it is interesting to remark that part of thesense expressed by the word test in the Englishquery can be somehow found in words suchas examination and best.
Further, it is also ofinterest to notice that most of the erroneouslytranslated content (weaken - ferriprive) is veryunlikely to affect the document retrieval forthis query: ferriprive as a French word willbe ignored, while weaken is of marginal content.Volk et al (2002) works with a related col-lection but using German queries, they observethat morphological analysis was effective andreport on a CLIR ratio above 80% (MLIR =0.3543; CLIR = 0.2955).
Directly related toour experiments, Eichmann et al (1998) usethe same benchmarks and similar terminologi-cal resources, but rely on a word-by-word trans-fer lexicon constructed from the UMLS.
Theaverage precision of their system using Frenchqueries is 0.1493, what results in a CLIR ra-tio of 62% 9.
Because we use the same bench-marks and resources and because our monolin-gual baselines are quite similar, the methodolog-ical difference must be underlined: while Eich-mann and al.
rely on a word to word transferlexicon, our system aims at breaking the bagof word limitation by translating multiwordsterms.
Finally, we also observe that the com-bined system is able to take advantage of ex-isting multilingual vocabulary without assum-ing any prior terminological knowledge from theuser, so that usual problems associated withcontrolled vocabularies (cf.
the introduction)are mutually solved in the proposed architec-ture.5 Conclusion and future workWe have presented a cross language informationretrieval engine, which capitalizes on the avail-ability of multilingual controlled vocabulary totranslate user requests.
The system relies ona text categorizer, which maps queries into aset of predefined concepts.
The automatic textcategorizer is tuned to perform a keyword as-signment task before being used to translateFrench queries into English MeSH terms.
ForOHSUMED queries, optimal precision is ob-tained when selecting three MeSH terms, butresults are improved when the system is mergedwith a commercial machine translation system,what suggest that text categorization can beopportunely combined with other query trans-lation approaches.
As future investigation, weplan to take into account the retrieval statusvalue obtained by each of the ranked MeSHterms instead of simply setting a threshold onthe ranking of the terms.9They report on surprisingly better results (CLIR ra-tion = 71%) for Spanish queries and suggest that Frenchis more difficult to translate than Spanish !AcknowledgementsThe study has been supported by the SwissNational Foundation (Grant 3200-065228).
Iwould like to thank Arnaud Gaudinat for histechnical help and Dr. Paul Fabry (MD) forthe translation of OHSUMED queries.ReferencesC Apte?, F Damerau, and S Weiss.
1994.
Auto-mated learning of decision rules for text cat-egorization.
ACM Transactions on Informa-tion Systems (TOIS), 12(3):233?251.M Davis.
1998.
Free resources and advancedalignment for cross-language text retrieval.In In proceedings of The Sixth Text RetrievalConference (TREC6).S Dumais, T Letsche, M Littman, and T Lan-dauer.
1997.
Automatic cross-language re-trieval using latent semantic indexing.
InD Hull and D Oard, editors, AAAI Sympo-sium on Cross-Language Text and Speech Re-trieval.D Eichmann, M Ruiz, and P Srinivasan.
1998.Cross-Language Information Retrieval withthe UMLS Metathesaurus.
pages 72?80.P Hayes and S Weinstein.
1990.
A systemfor content-based indexing of a database ofnews stories.
Proceedings of the Second An-nual Conference on Innovative Applicationsof Intelligence.W Hersh, C Buckley, T Leone, and D Hickam.1994.
OHSUMED: An interactive retrievalevaluation and new large test collection forresearch.
In SIGIR, pages 192?201.L Larkey and W Croft.
1996.
Combining clas-sifiers in text categorization.
In SIGIR, pages289?297.
ACM Press, New York, US.U Manber and S Wu.
1994.
GLIMPSE: A toolto search through entire file systems.
In Pro-ceedings of the USENIX Winter 1994 Tech-nical Conference, pages 23?32, San FransiscoCA USA, 17-21.A McCallum and K Nigam.
1998.
A compari-son of event models for Naive Bayes text clas-sification.
In AAAI-98 Workshop on Learn-ing for Text Categorization.J McCarley.
1999.
Should we translate the doc-uments or the queries in cross-language infor-mation retrieval.
ACL.D Oard and P Hackett.
1998.
Document trans-lation for cross-language text retrieval atthe university of Maryland.
In In Proceed-ings of The Sixth Text Retrieval Conference(TREC6).Y Rasolofo and J Savoy.
2003.
Term proximityscoring for keyword-based retrieval systems.In ECIR, pages 101?116.P Ruch, R Baud, and A Geissbu?hler.
2003.Learning-Free Text Categorization.
LNAI2780, pages 199?208.P Ruch, C Chichester, G Cohen, G Coray,F Ehrler, H Ghorbel, H Mu?ller, and V Pal-lotta.
2004.
Report on the TREC 2003 Ex-periment: Genomic Track.
In TREC-12.P Ruch.
2002.
Using contextual spelling correc-tion to improve retrieval effectiveness in de-graded text collections.
COLING 2002.G Salton.
1970.
Automatic processing of for-eign language documents.
JASIS, 21(3):187?194.J Savoy.
1999.
A stemming procedure and stop-word list for general french corpora.
Journalof the American Society for Information Sci-ence, 50(10):944?952.R Schapire and Y Singer.
2000.
BoosTexter:A boosting-based system for text categoriza-tion.
Machine Learning, 39(2/3):135?168.P Scha?uble and P Sheridan.
1998.
Cross-language information retrieval (CLIR) trackoverview.
In In Proceedings of The Sixth TextRetrieval Conference (TREC6).A Singhal, C Buckley, and M Mitra.
1996.
Piv-oted document length normalization.
ACM-SIGIR, pages 21?29.D Soergel.
1997.
Multilingual thesauri in cross-language text and speech retrieval.
In D Hulland D Oard, editors, AAAI Symposium onCross-Language Text and Speech Retrieval.M Volk, B Ripplinger, S Vintar, P Buitelaar,D Raileanu, and B Sacaleanu.
2002.
Se-mantic Annotation for Concept-Based Cross-Language Medical Information Retrieval.
IntJ Med Inf, 67 (1-3):75?83.J Xu, A Fraser, and R Weischedel.
2001.
Cross-lingual retrieval at bbn.
In TREC.Y Yang.
1999.
An evaluation of statistical ap-proaches to text categorization.
Journal ofInformation Retrieval, 1:67?88.P Zweigenbaum and N Grabar.
2002.
Restoringaccents in unknown biomedical words: appli-cation to the french mesh thesaurus.
Int JMed Inf, pages 113?126.Pierre Zweigenbaum, Robert Baud, A Burgun,F Namer, E Jarrousse, N Grabar, P Ruch,F Le Duff, B Thirion, and S Darmoni.2003.
Towards a Unified Medical Lexicon forFrench.
MIE 2003.
