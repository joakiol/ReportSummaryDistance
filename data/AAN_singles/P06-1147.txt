Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1169?1176,Sydney, July 2006. c?2006 Association for Computational LinguisticsUtilizing Co-Occurrence of Answers in Question AnsweringAbstractIn this paper, we discuss how to utilizethe co-occurrence of answers in buildingan automatic question answering systemthat answers a series of questions on aspecific topic in a batch mode.
Experi-ments show that the answers to the manyof the questions in the series usually havea high degree of co-occurrence in rele-vant document passages.
This featuresometimes can?t be easily utilized in anautomatic QA system which processesquestions independently.
However it canbe utilized in a QA system that processesquestions in a batch mode.
We have usedour pervious TREC QA system as base-line and augmented it with new answerclustering and co-occurrence maximiza-tion components to build the batch QAsystem.
The experiment results show thatthe QA system running under the batchmode get significant performance im-provement over our baseline TREC QAsystem.1 IntroductionQuestion answering of a series of questions onone topic has gained more and more researchinterest in the recent years.
The current TRECQA test set contains factoid and list questionsgrouped into different series, where each serieshas the target of a definition associated with it(Overview of the TREC 2004 Question Answer-ing Track, Voorhees 2005).
Usually, the target isalso called ?topic?
by QA researchers.
One of therestrictions of TREC QA is that ?questionswithin a series must be processed in order, with-out looking ahead.?
That is, systems are allowedto use answers to earlier questions to help answerlater questions in the same series, but can not uselater questions to help answer earlier questions.This requirement models the dialogue discoursebetween the user and the QA system.
Howeverour experiments on interactive QA system showthat some impatient QA users will throw a bunchof questions to the system and waiting for theanswers returned in all.
This prompted us to con-sider building a QA system which can accept asmany questions as possible from users once in alland utilizing the relations between these ques-tions to help find answers.
We would also like toknow the performance difference between theQA system processing the question series in anorder and the QA system processing the questionseries as a whole.
We call the second type of QAsystem as batch QA system to avoid the ambigu-ity in the following description in this paper.What kind of relations between questionscould be utilized is a key problem in building thebatch QA system.
By observing the test ques-tions of TREC QA, we found that the questionsgiven under the same topic are not independentat all.
Figure-1 shows a series of three questionsproposed under the topic ?Russian submarineKursk Sinks?
and some relevant passages to thistopic found in the TREC data set.
These passagescontain answers not to just one but to two orthree of the questions.
This indicates that the an-swers to these questions have high co-occurrence.In an automatic QA system which processesthe questions independently, the answers to thequestions may or may not always be extracteddue to algorithmic limitations or noisy informa-tion around the correct answer.
However inbuilding a batch QA system, the inter-dependence between the answers could be util-ized to help to filter out the noisy informationand pinpoint the correct answer for each questionin the series.Min Wu1 and Tomek Strzalkowski1,21 ILS Institute, University at Albany, State University of New York1400 Washington Ave SS261, Albany NY, 122222Institute of Computer Science, Polish Academy of Sciencesminwu@cs.albany.edu, tomek@csc.albany.edu1169We will discuss later in this paper how to util-ize the co-occurrence of answers to a series ofquestions in building a batch QA system.
Theremainder of this paper is organized as follows.In the next section, we review the current tech-niques used in building an automatic QA system.Section 3 introduces the answers co-occurrenceand how to cluster questions by the co-occurrence of their answers.
Section 4.1 de-scribes our TREC QA system and section 4.2describes how to build a batch QA system byaugmenting the TREC QA system with questionclustering and answer co-occurrence maximiza-tion.
Section 4.3 describes the experiments andexplains the experimental results.
Finally weconclude with the discussion of future work.2 Related WorkDuring recent years, many automatic QA sys-tems have been developed and the techniquesused in these systems cover logic inference, syn-tactic relation analysis, information extractionand proximity search, some systems also utilizepre-compiled knowledge base and externalonline knowledge resource.The LCC system (Moldovan & Rus, 2001;Harabagiu et al 2004) uses a logic prover to se-lect answer from related passages.
With the aidof extended WordNet and knowledge base, thetext terms are converted to logical forms that canbe proved to match the question logical forms.The IBM?s PIQUANT system (Chu-Carroll et al2003; Prager et al 2004) adopts a QA-by-Dossier-with-Constraints approach, which util-izes the natural constraints between the answer tothe main question and the answers to the auxil-iary questions.
Syntactic dependency matchinghas also been applied in many QA systems (Cuiet al 2005; Katz and Lin 2003).
The syntacticdependency relations of a candidate sentence arematched against the syntactic dependency rela-tions in the question in order to decide if the can-didate sentence contains the answer.
Althoughsurface text pattern matching is a comparativelysimple method, it is very efficient for simple fac-toid questions and is used by many QA systems(Hovy et al2001; Soubbotin, M. and S. Soub-botin 2003).
As a powerful web search engineand external online knowledge resource, Googlehas been widely adopted in QA systems (Hovy etal 2001; Cui 2005) as a tool to help passage re-trieval and answer validation.Current QA systems mentioned above andrepresented at TREC have been developed toanswer one question at the time.
This may par-tially be an artifact of the earlier TREC QAevaluations which used large sets of independentquestions.
It may also partially reflect the inten-tion of the current TREC QA Track that thequestion series introduced in TREC QA 2004(Voorhees 2005) simulate an interaction with ahuman, thus expected to arrive one at a time.The co-occurrence of answers of a series ofhighly related questions has not yet been fullyutilized in current automatic QA systems partici-pating TREC.
In this situation, we think itworthwhile to find out whether a series of highlyrelated questions on a specific topic such as theTREC QA test questions can be answered to-gether in a batch mode by utilizing the co-occurrences of the answers and how much it willhelp improve the QA system performance.3 Answer Co-Occurrence and QuestionClusteringMany QA systems utilize the co-occurrence ofquestion terms in passage retrieval (Cui 2005).Topic Russian submarine Kursk sinks1.
When did the submarine sink?
August 122.
How many crewmen were lost in the disaster?
1183.
In what sea did the submarine sink?
Barents SeaSome Related PassagesRussian officials have speculated that the Kursk col-lided with another vessel in the Barents Sea, and usu-ally blame an unspecified foreign submarine.
All 118officers and sailors aboard were killed.The Russian governmental commission on the acci-dent of the submarine Kursk sinking in the BarentsSea on August 12 has rejected 11 original explana-tions for the disaster..... as the same one carried aboard the nuclear subma-rine Kursk, which sank in the Barents Sea on Aug. 12,killing all 118 crewmen aboard.The navy said Saturday that most of the 118-mancrew died Aug. 12 when a huge explosion ....Chief of Staff of the Russian Northern Fleet MikhailMotsak Monday officially confirmed the deaths of118 crewmen on board the Kursk nuclear submarinethat went to the bottom of the Barents Sea on August12.Figure-1 Questions and Related Passages1170Some QA systems utilize the co-occurrence ofquestion terms and answer terms in answer vali-dation.
These methods are based on the assump-tion that the co-occurrences of question termsand answer terms are relatively higher than theco-occurrences of other terms.
Usually the co-occurrence are measured by pointwise mutualinformation between terms.During the development of our TREC QA sys-tem, we found the answers of some questions ina series have higher co-occurrence.
For example,in a series of questions on a topic of disasterevent, the answers to questions such as ?whenthe event occurred?, ?where the event occurred?and ?how many were injured in the event?
havehigh co-occurrence in relatively short passages.Also, in a series of questions on a topic of someperson, the answers to questions such as ?whendid he die?, ?where did he die?
and ?how did hedie?
have high co-occurrence.
To utilize this an-swers co-occurrence effectively in a batch QAsystem, we need to know which questions areexpected to have higher answers co-occurrenceand cluster these questions to maximize the an-swers co-occurrence among the questions in thecluster.Currently, the topics used in TREC QA testquestions fall into four categories: ?Person?,?Organization?, ?Event?
and ?Things?.
The topiccan be viewed as an object and the series ofquestions can be viewed as asking for the attrib-utes of the object.
In this point of view, to findout which questions have higher answers co-occurrence is to find out which attributes of theobject (topic) have high co-occurrence.We started with three categories of TREC QAtopics: ?Event?, ?Person?
and ?Organization?.For ?Event?
topic category, we divided it intotwo sub-categories: ?Disaster Event?
and ?SportEvent?.
From the 2004 & 2005 TREC QA testquestions, we manually collected frequentlyasked questions on each topic category andmapped these questions to the correspondingattributes of the topic.
We focused on frequentlyasked questions because these questions are eas-ier to be classified and thus served as a goodstarting point for our work.
However for thistechnique to scale in the future, we are expectingto integrate automatic topic model detection intothe system.
For topic category ?Person?, the at-tributes and corresponding named entity (NE)tags list as follows.For each topic category, we collected 20 sam-ple topics as well as the corresponding attributesinformation about these topics.
The sample topic?Rocky Marciano?
and the attributes are listed asfollows:From each attribute of the sample topic, anappropriate question can be formulated and rele-vant passages about this question were retrievedfrom TREC data (AQUAINT Data) and the web.A topic-related passages collection was formedby the relevant passages of questions on all at-tributes under the topic.
Among the topic-relatedpassages, the pointwise mutual information (PMI)of attribute values were calculated which conse-quently formed a symmetric mutual informationmatrix.
The PMI of two attribute values x and ywas calculated by the following equation.
)()(),(log),(ypxpyxpyxPMI =All the mutual information matrixes under thetopic category were added up and averaged inorder to get one mutual information matrixwhich reflects the general co-occurrence rela-Attribute      Attribute ValueBirth Date   September 1, 1923Birth Place  Brockton, MADeath Date  August 31, 1969Death Place  IowaDeath Reason  airplane crashDeath Age   45Buried Place  Fort Lauderdale, FLNationality   AmericanOccupation  heavyweight champion boxerFather    Pierino MarchegianoMother    Pasqualena MarchegianoWife   Barbara CousinsChildren   Mary Ann, Rocco KevinNo.
of Children  twoReal Name  Rocco Francis MarchegianoNick Name  noneAffiliation   noneEducation   noneAttribute                                     Attribute?s NE tagBirth Date   DateBirth Place  LocationDeath Date  DateDeath Place  LocationDeath Reason  Disease, AccidentDeath Age   NumberNationality  NationalityOccupation  OccupationFather   PersonMother   PersonWife   PersonChildren   PersonNumber of Children  NumberReal Name  Person, OtherNick Name  Person, OtherAffiliation   OrganizationEducation   Organization1171tions between attributes under the topic category.We clustered the attributes by their mutual in-formation value.
Our clustering strategy was tocluster attributes whose pointwise mutual infor-mation is greater than a threshold ?.
We choose ?as equal to 60% of the maximum value in thematrix.The operations described above were auto-matically carried out by our carefully designedtraining system.
The clusters learned for eachtopic category is listed as follows.The reason for the clustering of attributes oftopic category is for the convenience of buildinga batch QA system.
When a batch QA system isprocessing a series of questions under a topic,some of the questions in the series are mapped tothe attributes of the topic and thus grouped to-gether according to the attribute clusters.
Thenquestions in the same group are processed to-gether to obtain a maximum of answers co-occurrence.
More details are given in section 4.2.4 Experiment  Setup and Evaluation4.1 Baseline SystemThe baseline system is an automatic IE-driven(Information Extraction) QA system.
We call itIE-driven because the main techniques used inthe baseline system: surface pattern matchingand N-gram proximity search need to be appliedto NE-tagged (Named Entity) passages.
The sys-tem architecture is illustrated in Figure-2.
Thecomponents indicated by dash lines are not in-cluded in the baseline system and they are addedto the baseline system to build a batch QA sys-tem.
As shown in the figure with light color, thetwo components are question classification andco-occurrence maximization.
Both our baselinesystem and batch QA system didn?t utilize anypre-compiled knowledge base.In the question analysis component, questionsare classified by their syntactic structure and an-swer target.
The answer targets are classified asnamed entity types.
The retrieved documents aresegmented into passages and filtered by topickeywords, question keywords and answer target.The answer selection methods we used aresurface text pattern matching and n-gram prox-imity search.
We build a pattern learning systemto automatically extract answer patterns from theTREC data and the web.
These answer patternsare scored by their frequency, sorted by questiontype and represented as regular expressions withterms of ?NP?, ?VP?, ?VPN?, ?ADVP?, ?be?,?in?, ?of?, ?on?, ?by?, ?at?, ?which?, ?when?,?where?, ?who?, ?,?, ?-?, ?(?.
Some sample an-swer patterns of question type ?when_be_np_vp?are listed as follows.When applying these answer patterns to ex-tract answer from candidate passages, the termssuch as ?NP?, ?VP?, ?VPN?, ?ADVP?
and ?be?are replaced with the corresponding questionterms.
The replaced patterns can be matched di-rectly to the candidate passages and answer can-didate be extracted.Some similar proximity search methods havebeen applied in document and passage retrievalin the previous research.
We applied n-gramproximity search to answer questions whose an-swers can?t be extracted by surface text patternmatching.
Around every named entity in the fil-tered candidate passages, question terms as wellas topic terms are matched as n-grams.
A ques-tion term is tokenized by word.
We matched thelongest possible sequence of tokenized wordwithin the 100 word sliding window around thenamed entity.
Once a sequence is matched, thecorresponding word tokens are removed from theADVP1 VP in <Date>([^<>]+?)<\/Date>NP1.{1,15}VP.
{1,30} in <Date>([^<>]+?)<\/Date>NP1.
{1,30} be VP in <Date>([^<>]+?
)<\/Date>NP1, which be VP in <Date>([^<>]+?
)<\/Date>VP NP1.
{1,15} at .{1,15}<Date>([^<>]+?)<\/Date>ADVP1.{1,80}NP1.{1,80}<Date>([^<>]+?
)<\/Date>NP1, VP in <Date>([^<>]+?
)<\/Date>NP1 of <Date>([^<>]+?
)<\/Date>NP1 be VP in <Date>([^<>]+?)<\/Date>?Person?
TopicCluster1: Birth Date; Birth PlaceCluster2a: Death Date; Death Place;Death Reason; Death AgeCluster2b: Death Date; Birth DateCluster3: Father; MotherCluster4: Wife; Children; Number of ChildrenCluster5: Nationality; Occupation?Disaster Event?
TopicCluster1: Event Date; Event Location; Event Casualty;Cluster2:  Organization Involved, Person Involved?Sport Event?
TopicCluster1: Winner; Winning ScoreCluster2: Location, Date?Organization?
TopicCluster1: Founded Date; Founded Location; FounderCluster2: Headquarters; Number of Members1172token list and the same searching and matching isrepeated until the token list is empty or no se-quence of tokenized word can be matched.
Thenamed entity is scored by the average weighteddistance score of question terms and topic terms.Let Num(ti...tj) denotes the number of allmatched n-grams, d(E, ti...tj) denotes the worddistance between the named entity and thematched n-gram, W1(ti...tj) denotes the topicweight of the matched n-gram, W2(ti...tj) denotesthe length weight of the matched n-gram.
If ti...tjcontains topic terms or question verb phrase, 0.5is assigned to W1, otherwise 1.0 is assigned.
Thevalue assigned to length weight W2 is deter-mined by ?, the ratio value of matched n-gramlength to question term length.
How to assign W2is illustrated as follows.The weighted distance score D(E,QTerm) ofthe question term and the final score S(E) of thenamed entity are calculated by the followingequations.)...()...(2)...
(1)...,(),( ...jitt jijijittNumttWttWttEdQTermED ji?
?=NQTermEDESNii?=),()(4.2 Batch QA SystemThe batch QA system is built from the base-line system and two added components: questionclassification and co-occurrence maximization.In a batch QA system, questions are classifiedbefore they are syntactically and semanticallyanalyzed.
The classification process consists oftwo steps: topic categorization and questionmapping.
Firstly the topic of the series questionsis classified into appropriate topic category andthen the questions can be mapped to the corre-sponding attribute and clustered according to themapped attributes.
Since the attributes of topiccategory is collected from frequently asked ques-tions, there are some questions in the questionseries which can?t be mapped to any attribute.These unmapped questions are processed indi-vidually.The topic categorization is done by a Na?veBayes classifier which employs features such asstemmed question terms and named entities inthe question.
The training data is a collection of85 question series labeled as one of four topiccategories: ?Person?, ?Disaster Event?, ?SportEvent?
and ?Organization?.
The mapping ofquestion to topic attribute is an example-basedsyntactic pattern matching and keywords match-ing.The questions grouped together are processedas a question cluster.
After the processing of an-swer selection and ranking, each question in thecluster gets top 10 scored candidate answerswhich forms an answer vector A(a1, ?, a10).W2(ti...tj)=0.4   if ?<0.4;W2(ti...tj)=0.6       if 0.4?
??
0.6;W2(ti...tj)=0.8        if ?>0.6;W2(ti...tj)= 0.9      if ?>0.75.AnswersSyntactic ChunkingType CategorizationQuery GenerationTarget ClassificationQuestions DocumentRetrievalPassage FilteringSurface Text Pattern MatchingN-Gram Proximity SearchAnswer RankingPattern FilesTagged Corpus(AQUAINT/Web)QuestionClusteringCo-occurrenceMaximizationFigure-2  Baseline QA System & Batch QA System (dashed lines and light colored component)1173Suppose there are n questions in the cluster, thetask of answer co-occurrence maximization is toretrieve a combination of n answers which hasmaximum pointwise mutual information (PMI).This combination is assumed to be the answers tothe questions in the cluster.There are a total of 10n possible combinationsamong all the candidate answers.
If the PMI ofevery combination should be calculated, it iscomputationally inefficient.
Also, some combi-nations containing noisy information may havehigher co-occurrence than the correct answercombination.
For example, the correct answerscombination to questions showed in figure-1 is?August 12; 118; Barents Sea?.
However, thereis also a combination of ?Aug.
12, two; U.S.?which has higher pointwise mutual informationdue to the frequently occurred noisy informationof ?two U.S. submarines?
and ?two explosions inthe area Aug. 12 at the time?.To reduce this negative effect brought by thenoisy information, we started from the highestscored answer and put it in the final answer list.Then we added the answers one by one to thefinal answer list.
The added answer has the high-est PMI with the answers in the final answer list.It is important here to choose the first answeradded to the final answer list correctly.
Other-wise, the following added answers will be nega-tively affected.
So in our batch QA system, acorrect answer should be scored highest amongall the answer candidates of the questions in thecluster.
Although this can?t be always achieved,it can be approximated by setting higher thresh-old both in passage scoring and answer ranking.However, in the baseline system, passages arenot scored.
They are equally processed becausewe wanted to retrieve as many answer candidatesas possible and answer candidates are ranked bytheir matching score and redundancy score.4.3 Performance EvaluationThe data corpus we used is TREC QA data(AQUAINT Corpus).
The test questions areTREC QA 2004 and TREC QA 2005 questions.Each topic is followed with a series of factoidquestions.
The number of questions selectedfrom TREC 2004 collection is 230 and the num-ber of question series is 65.
The number of ques-tions selected from TREC 2005 collection is 362and the number of question series is 75.We performed 4 different experiments: (1).Baseline system.
(2).
Batch QA system (Baselinesystem with co-occurrence maximization).
(3).Baseline system with web supporting.
(4).
BatchQA with web supporting.
We introduced websupporting into the experiments because usuallythe information on the web tends to share moreco-occurrence and redundancy which is alsoproved by our results.Compared between the baseline system andbatch system, the experiment results show thatthe overall accuracy score has been improvedfrom 0.34 to 0.39 on TREC 2004 test questionsand from 0.31 to 0.37 on TREC 2005 test ques-tions.
Compared between the baseline systemand batch system with web supporting, the accu-racy score can be improved up to 0.498.
We alsonoticed that the average number of questions un-der each topic in TREC 2004 test questions is3.538, which is significantly lower than the4.8267 average in TREC 2005 questions series.This may explain why the improvement we ob-tained on TREC2004 data is not as significant asthe improvement obtained on TREC 2005 ques-tions.The accuracy score of each TREC2005 ques-tion series is also calculated.
Figure3-4 shows thecomparisons between 4 different experimentmethods.
We also calculate the number of ques-tion series with accuracy increased, unchangedand decreased.
It is also shown in the followingtable.
(?+?
means number of question series withaccuracy increased, ?=?
unchanged and ?-?
de-creased.
)TREC2005 Question Series(75 question series)+ - =Baseline + Co-occurrence 25 5 45Baseline + Web 40 2 33Baseline + Co-occurrence +Web49 2 24Accuracy Comparison on DifferentMethods00.10.20.30.40.50.61 2 3 4TREC2004 TREC20051174Some question series get unchanged accuracybecause the questions can?t be clustered accord-ing to our clustering template so that it can?t util-ize the co-occurrence of answers in the cluster.Some question series get decreased accuracy be-cause the questions because the noisy informa-tion had even higher co-occurrence, the erroroccurred during the question clustering and theanswers didn?t show any co-relations in the re-trieved passages at all.
A deep and further erroranalysis is necessary for this answer co-occurrence maximization technique to be appliedtopic independently.5 Discussion and Future WorkWe have demonstrated that in a QA system,answering a series of inter-related questions canbe improved by grouping the questions by ex-pected co-occurrence of answers in text.
The im-provement can be made without exploiting thepre-compiled knowledge base.Although our system can cluster frequentlyasked questions on topics of ?Events?, ?Persons?and ?Organizations?, there are still some highlyrelated questions which can?t be clustered by ourmethod.
Here are some examples.To cluster these questions, we plan to utilizeevent detection techniques and set up an eventtopic ?Carlos the Jackal captured?
during theanswering process, which will make it easier tocluster ?When was the Carlos the Jackal cap-tured??
and ?Where was the Carlos the Jackalcaptured?
?Can this answers co-occurrence maximizationapproach be applied to improve QA performanceTopic Carlos the Jackal1.
When was he captured?2.
Where was he captured?Topic boxer Floyd Patterson1.
When did he win the title?2.
How old was he when he won the title?3.
Who did he beat to win the title?Accuracy on TREC2005 Test Questions00.20.40.60.811.21 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49 52 55 58 61 64 67 70 73question seriesaccuracybaseline baseline+co_occurrence baseline+w eb baseline+w eb+co_occurrenceAccuracy on TREC2004 Test Questions00.20.40.60.811.21 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49 52 55 58 61 64question seriesaccuracybaseline baseline+co_occurrence baseline+w eb baseline+w eb+co_occurrenceFigure 3-4 Comparison of TREC2004/2005 Question Series Accuracy1175on single questions (i.e.
1-series)?
As suggestedin the reference paper (Chu-Carrol and Prager),we may be able to add related (unasked) ques-tions to form a cluster around the single question.Another open issue is what kind of effect willthis technique bring to answering series of ?list?questions, i.e., where each question expects a listof items as answer.
As we know that the an-swers of some ?list?
questions have pretty highco-occurrence while others don?t have co-occurrence at all.
Future work involves experi-ments conducted on these aspects.AcknowledgementThe Authors wish to thank BBN for the use ofNE tagging software IdentiFinder, CIIR atUniversity of Massachusetts for the use ofInquery search engine, Stanford University NLPgroup for the use of Stanford parser.
Thanks alsoto the anonymous reviewers for their helpfulcomments.ReferencesChu-Carrol, J., J. Prager, C. Welty, K. Czuba andD.
Ferrucci.
?A Multi-Strategy and Multi-Source Approach to Question Answering?, InProceedings of the 11th TREC, 2003.Cui, H., K. Li, R. Sun, T.-S. Chua and M.-Y.Kan.
?National University of Singapore at theTREC 13 Question Answering Main Task?.
InProceedings of the 13th TREC, 2005.Han, K.-S., H. Chung, S.-B.
Kim, Y.-I.
Song, J.-Y.
Lee, and H.-C. Rim.
?Korea UniversityQuestion Answering System at TREC 2004?.In Proceedings of the 13th TREC, 2005.Harabagiu, S., D. Moldovan, C. Clark, M. Bow-den, J. Williams and J. Bensley.
?AnswerMining by Combining Extraction Techniqueswith Abductive Reasoning?.
In Proceedings of12th TREC, 2004.Hovy, E. L. Gerber, U. Hermjakob, M. Junk andC.-Y.
Lin.
?Question Answering  in Webclo-pedia?.
In Proceedings of the 9th TREC, 2001.Lin, J., D. Quan, V. Sinha, K. Bakshi, D. Huynh,B.
Katz and D. R. Karger.
?The Role of Con-text in Question Answering Systems?.
In CHI2003.Katz, B. and J. Lin.
?Selectively Using Relationsto Improve Precision in Question Answering?.In Proceedings of the EACL-2003 Workshopon Natural Language Processing for QuestionAnswering.
2003.Moldovan, D. and V. Rus.
?Logical Form Trans-formation of WordNet and its Applicability toQuestion Answering?.
In Proceedings of theACL, 2001.Monz.
C. ?Minimal Span Weighting Retrievalfor Question Answering?
In Proceedings ofthe SIGIR Workshop on Information Retrievalfor Question Answering.
2004.Prager, J., E. Brown, A. Coden and D.
Radev.
?Question-Answering by Predictive Annota-tion?.
In Proceedings of SIGIR 2000, pp.
184-191.
2000.Prager, J., J. Chu-Carroll and K. Czuba.
?Ques-tion Answering Using Constraint Satisfaction:QA-By-Dossier-With-Constraints?.
In Pro-ceedings of the 42nd ACL.
2004.Ravichandran, D. and E. Hovy.
?Learning Sur-face   Text Patterns for a Question AnsweringSystem?.
In Proceedings of 40th ACL.
2002.Soubbotin, M. and S. Soubbotin.
?Patterns ofPotential Answer Expressions as Clues to theRight Answers?.
In Proceedings of 11th TREC.2003.Voorhees, E. ?Using Question Series to EvaluateQuestion Answering System Effectiveness?.In Proceedings of HLT 2005.
2005.1176
