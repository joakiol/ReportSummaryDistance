Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 355?362,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsDomain Adaptation via Pseudo In-Domain Data SelectionAmittai AxelrodUniversity of WashingtonSeattle, WA 98105amittai@uw.eduXiaodong HeMicrosoft ResearchRedmond, WA 98052xiaohe@microsoft.comJianfeng GaoMicrosoft ResearchRedmond, WA 98052jfgao@microsoft.comAbstractWe explore efficient domain adaptation for thetask of statistical machine translation basedon extracting sentences from a large general-domain parallel corpus that are most relevantto the target domain.
These sentences maybe selected with simple cross-entropy basedmethods, of which we present three.
Asthese sentences are not themselves identicalto the in-domain data, we call them pseudoin-domain subcorpora.
These subcorpora ?1% the size of the original ?
can then usedto train small domain-adapted Statistical Ma-chine Translation (SMT) systems which out-perform systems trained on the entire corpus.Performance is further improved when we usethese domain-adapted models in combinationwith a true in-domain model.
The resultsshow that more training data is not alwaysbetter, and that best results are attained viaproper domain-relevant data selection, as wellas combining in- and general-domain systemsduring decoding.1 IntroductionStatistical Machine Translation (SMT) system per-formance is dependent on the quantity and qualityof available training data.
The conventional wisdomis that more data is better; the larger the training cor-pus, the more accurate the model can be.The trouble is that ?
except for the few all-purposeSMT systems ?
there is never enough training datathat is directly relevant to the translation task athand.
Even if there is no formal genre for the textto be translated, any coherent translation task willhave its own argot, vocabulary or stylistic prefer-ences, such that the corpus characteristics will nec-essarily deviate from any all-encompassing model oflanguage.
For this reason, one would prefer to usemore in-domain data for training.
This would em-pirically provide more accurate lexical probabilities,and thus better target the task at hand.
However, par-allel in-domain data is usually hard to find1, and soperformance is assumed to be limited by the quan-tity of domain-specific training data used to build themodel.
Additional parallel data can be readily ac-quired, but at the cost of specificity: either the datais entirely unrelated to the task at hand, or the data isfrom a broad enough pool of topics and styles, suchas the web, that any use this corpus may provide isdue to its size, and not its relevance.The task of domain adaptation is to translate a textin a particular (target) domain for which only a smallamount of training data is available, using an MTsystem trained on a larger set of data that is not re-stricted to the target domain.
We call this larger setof data a general-domain corpus, in lieu of the stan-dard yet slightly misleading out-of-domain corpus,to allow a large uncurated corpus to include sometext that may be relevant to the target domain.Many existing domain adaptation methods fallinto two broad categories.
Adaptation can be done atthe corpus level, by selecting, joining, or weightingthe datasets upon which the models (and by exten-sion, systems) are trained.
It can be also achieved atthe model level by combining multiple translation orlanguage models together, often in a weighted man-ner.
We explore both categories in this work.1Unless one dreams of translating parliamentary speeches.355First, we present three methods for ranking thesentences in a general-domain corpus with respect toan in-domain corpus.
A cutoff can then be applied toproduce a very small?yet useful?
subcorpus, whichin turn can be used to train a domain-adapted MTsystem.
The first two data selection methods are ap-plications of language-modeling techniques to MT(one for the first time).
The third method is noveland explicitly takes into account the bilingual na-ture of the MT training corpus.
We show that it ispossible to use our data selection methods to subse-lect less than 1% (or discard 99%) of a large generaltraining corpus and still increase translation perfor-mance by nearly 2 BLEU points.We then explore how best to use these selectedsubcorpora.
We test their combination with the in-domain set, followed by examining the subcorporato see whether they are actually in-domain, out-of-domain, or something in between.
Based on this, wecompare translation model combination methods.Finally, we show that these tiny translation mod-els for model combination can improve system per-formance even further over the current standard wayof producing a domain-adapted MT system.
The re-sulting process is lightweight, simple, and effective.2 Related Work2.1 Training Data SelectionAn underlying assumption in domain adaptation isthat a general-domain corpus, if sufficiently broad,likely includes some sentences that could fall withinthe target domain and thus should be used for train-ing.
Equally, the general-domain corpus likely in-cludes sentences that are so unlike the domain of thetask that using them to train the model is probablymore harmful than beneficial.
One mechanism fordomain adaptation is thus to select only a portion ofthe general-domain corpus, and use only that subsetto train a complete system.The simplest instance of this problem can befound in the realm of language modeling, usingperplexity-based selection methods.
The sentencesin the general-domain corpus are scored by their per-plexity score according to an in-domain languagemodel, and then sorted, with only the lowest onesbeing retained.
This has been done for languagemodeling, including by Gao et al(2002), and morerecently by Moore and Lewis (2010).
The rankingof the sentences in a general-domain corpus accord-ing to in-domain perplexity has also been applied tomachine translation by both Yasuda et al(2008), andFoster et al(2010).
We test this approach, with thedifference that we simply use the source side per-plexity rather than computing the geometric meanof the perplexities over both sides of the corpus.
Wealso reduce the size of the training corpus far moreaggressively than Yasuda et als 50%.
Foster et al(2010) do not mention what percentage of the cor-pus they select for their IR-baseline, but they con-catenate the data to their in-domain corpus and re-port a decrease in performance.
We both keep themodels separate and reduce their size.A more general method is that of (Matsoukas etal., 2009), who assign a (possibly-zero) weight toeach sentence in the large corpus and modify the em-pirical phrase counts accordingly.
Foster et al(2010)further perform this on extracted phrase pairs, notjust sentences.
While this soft decision is more flex-ible than the binary decision that comes from includ-ing or discarding a sentence from the subcorpus, itdoes not reduce the size of the model and comesat the cost of computational complexity as well asthe possibility of overfitting.
Additionally, the mosteffective features of (Matsoukas et al, 2009) werefound to be meta-information about the source doc-uments, which may not be available.Another perplexity-based approach is that takenby Moore and Lewis (2010), where they use thecross-entropy difference as a ranking function ratherthan just cross-entropy.
We apply this criterion forthe first time to the task of selecting training datafor machine translation systems.
We furthermore ex-tend this idea for MT-specific purposes.2.2 Translation Model CombinationIn addition to improving the performance of a sin-gle general model with respect to a target domain,there is significant interest in using two translationmodels, one trained on a larger general-domain cor-pus and the other on a smaller in-domain corpus, totranslate in-domain text.
After all, if one has ac-cess to an in-domain corpus with which to selectdata from a general-domain corpus, then one mightas well use the in-domain data, too.
The expectationis that the larger general-domain model should dom-356inate in regions where the smaller in-domain modellacks coverage due to sparse (or non-existent) ngramcounts.
In practice, most practical systems also per-form target-side language model adaptation (Eck etal., 2004); we eschew this in order to isolate the ef-fects of translation model adaptation alone.Directly concatenating the phrase tables into onelarger one isn?t strongly motivated; identical phrasepairs within the resulting table can lead to unpre-dictable behavior during decoding.
Nakov (2008)handled identical phrase pairs by prioritizing thesource tables, however in our experience identicalentries in phrase tables are not very common whencomparing across domains.
Foster and Kuhn (2007)interpolated the in- and general-domain phrase ta-bles together, assigning either linear or log-linearweights to the entries in the tables before combiningoverlapping entries; this is now standard practice.Lastly, Koehn and Schroeder (2007) reportedimprovements from using multiple decoding paths(Birch et al, 2007) to pass both tables to the MosesSMT decoder (Koehn et al, 2003), instead of di-rectly combining the phrase tables to perform do-main adaptation.
In this work, we directly com-pare the approaches of (Foster and Kuhn, 2007) and(Koehn and Schroeder, 2007) on the systems gener-ated from the methods mentioned in Section 2.1.3 Experimental Framework3.1 CorporaWe conducted our experiments on the Interna-tional Workshop on Spoken Language Translation(IWSLT) Chinese-to-English DIALOG task 2, con-sisting of transcriptions of conversational speech ina travel setting.
Two corpora are needed for theadaptation task.
Our in-domain data consisted of theIWSLT corpus of approximately 30,000 sentencesin Chinese and English.
Our general-domain cor-pus was 12 million parallel sentences comprising avariety of publicly available datasets, web data, andprivate translation texts.
Both the in- and general-domain corpora were identically segmented (in Chi-nese) and tokenized (in English), but otherwise un-processed.
We evaluated our work on the 2008IWSLT spontaneous speech Challenge Task3 test2http://iwslt2010.fbk.eu/node/333Correct-Recognition Result (CRR) conditionset, consisting of 504 Chinese sentences with 7 En-glish reference translations each.
This is the mostrecent IWSLT test set for which the reference trans-lations are available.3.2 System DescriptionIn order to highlight the data selection work, weused an out-of-the-box Moses framework usingGIZA++ (Och and Ney, 2003) and MERT (Och,2003) to train and tune the machine translation sys-tems.
The only exception was the phrase tablefor the large out-of-domain system trained on 12msentence pairs, which we trained on a cluster us-ing a word-dependent HMM-based alignment (He,2007).
We used the Moses decoder to produce allthe system outputs, and scored them with the NISTmt-eval31a 4 tool used in the IWSLT evalutation.3.3 Language ModelsOur work depends on the use of language models torank sentences in the training corpus, in addition totheir normal use during machine translation tuningand decoding.
We used the SRI Language Model-ing Toolkit (Stolcke, 2002) was used for LM train-ing in all cases: corpus selection, MT tuning, anddecoding.
We constructed 4gram language mod-els with interpolated modified Kneser-Ney discount-ing (Chen and Goodman, 1998), and set the Good-Turing threshold to 1 for trigrams.3.4 Baseline SystemThe in-domain baseline consisted of a translationsystem trained using Moses, as described above, onthe IWSLT corpus.
The resulting model had a phrasetable with 515k entries.
The general-domain base-line was substantially larger, having been trained on12 million sentence pairs, and had a phrase tablecontaining 1.5 billion entries.
The BLEU scores ofthe baseline single-corpus systems are in Table 1.Corpus Phrases Dev TestIWSLT 515k 45.43 37.17General 1,478m 42.62 40.51Table 1: Baseline translation results for in-domain andgeneral-domain systems.4http://www.itl.nist.gov/iad/mig/tools/3574 Training Data Selection MethodsWe present three techniques for ranking and select-ing subsets of a general-domain corpus, with an eyetowards improving overall translation performance.4.1 Data Selection using Cross-EntropyAs mentioned in Section 2.1, one establishedmethod is to rank the sentences in the general-domain corpus by their perplexity score accord-ing to a language model trained on the small in-domain corpus.
This reduces the perplexity of thegeneral-domain corpus, with the expectation thatonly sentences similar to the in-domain corpus willremain.
We apply the method to machine trans-lation, even though perplexity reduction has beenshown to not correlate with translation performance(Axelrod, 2006).
For this work we follow the proce-dure of Moore and Lewis (2010), which applies thecosmetic change of using the cross-entropy ratherthan perplexity.The perplexity of some string s with empirical n-gram distribution p given a language model q is:2?
?x p(x) log q(x) = 2H(p,q) (1)where H(p, q) is the cross-entropy between p andq.
We simplify this notation to just HI(s), mean-ing the cross-entropy of string s according to a lan-guage model LMI which has distribution q. Se-lecting the sentences with the lowest perplexity istherefore equivalent to choosing the sentences withthe lowest cross-entropy according to the in-domainlanguage model.
For this experiment, we used a lan-guage model trained (using the parameters in Sec-tion 3.3) on the Chinese side of the IWSLT corpus.4.2 Data Selection using Cross-EntropyDifferenceMoore and Lewis (2010) also start with a languagemodel LMI over the in-domain corpus, but then fur-ther construct a language modelLMO of similar sizeover the general-domain corpus.
They then rank thegeneral-domain corpus sentences using:HI(s)?HO(s) (2)and again taking the lowest-scoring sentences.
Thiscriterion biases towards sentences that are both likethe in-domain corpus and unlike the average of thegeneral-domain corpus.
For this experiment we re-used the in-domain LM from the previous method,and trained a second LM on a random subset of35k sentences from the Chinese side of the generalcorpus, except using the same vocabulary as the in-domain LM.4.3 Data Selection using BilingualCross-Entropy DifferenceIn addition to using these two monolingual criteriafor MT data selection, we propose a new methodthat takes in to account the bilingual nature of theproblem.
To this end, we sum cross-entropy differ-ence over each side of the corpus, both source andtarget:[HI?src(s)?HO?src(s)]+[HI?tgt(s)?HO?tgt(s)](3)Again, lower scores are presumed to be better.
Thisapproach reuses the source-side language modelsfrom Section 4.2, but requires similarly-trained onesover the English side.
Again, the vocabulary of thelanguage model trained on a subset of the general-domain corpus was restricted to only cover thosetokens found in the in-domain corpus, followingMoore and Lewis (2010).5 Results of Training Data SelectionThe baseline results show that a translation systemtrained on the general-domain corpus outperforms asystem trained on the in-domain corpus by over 3BLEU points.
However, this can be improved fur-ther.
We used the three methods from Section 4 toidentify the best-scoring sentences in the general-domain corpus.We consider three methods for extracting domain-targeted parallel data from a general corpus: source-side cross-entropy (Cross-Ent), source-side cross-entropy difference (Moore-Lewis) from (Moore andLewis, 2010), and bilingual cross-entropy difference(bML), which is novel.Regardless of method, the overall procedure isthe same.
Using the scoring method, We rank theindividual sentences of the general-domain corpus,select only the top N .
We used the top N ={35k, 70k, 150k} sentence pairs out of the 12 mil-358lion in the general corpus 5.
The net effect is that ofdomain adaptation via threshhold filtering.
New MTsystems were then trained solely on these small sub-corpora, and compared against the baseline modeltrained on the entire 12m-sentence general-domaincorpus.
Table 2 contains BLEU scores of the sys-tems trained on subsets of the general corpus.Method Sentences Dev TestGeneral 12m 42.62 40.51Cross-Entropy 35k 39.77 40.66Cross-Entropy 70k 40.61 42.19Cross-Entropy 150k 42.73 41.65Moore-Lewis 35k 36.86 40.08Moore-Lewis 70k 40.33 39.07Moore-Lewis 150k 41.40 40.17bilingual M-L 35k 39.59 42.31bilingual M-L 70k 40.84 42.29bilingual M-L 150k 42.64 42.22Table 2: Translation results using only a subset of thegeneral-domain corpus.All three methods presented for selecting a sub-set of the general-domain corpus (Cross-Entropy,Moore-Lewis, bilingual Moore-Lewis) could beused to train a state-of-the-art machine transla-tion system.
The simplest method, using only thesource-side cross-entropy, was able to outperformthe general-domain model when selecting 150k outof 12 million sentences.
The other monolingualmethod, source-side cross-entropy difference, wasable to perform nearly as well as the general-domain model with only 35k sentences.
The bilin-gual Moore-Lewis method proposed in this paperworks best, consistently boosting performance by1.8 BLEU while using less than 1% of the availabletraining data.5.1 Pseudo In-Domain DataThe results in Table 2 show that all three meth-ods (Cross-Entropy, Moore-Lewis, bilingual Moore-Lewis) can extract subsets of the general-domaincorpus that are useful for the purposes of statisticalmachine translation.
It is tempting to describe theseas methods for finding in-domain data hidden in a5Roughly 1x, 2x, and 4x the size of the in-domain corpus.general-domain corpus.
Alas, this does not seem tobe the case.We trained a baseline language model on the in-domain data and used it to compute the perplexityof the same (in-domain) held-out dev set used totune the translation models.
We extracted the topN sentences using each ranking method, varying Nfrom 10k to 200k, and then trained language modelson these subcorpora.
These were then used to alsocompute the perplexity of the same held-out dev set,shown below in Figure 1.020406080100120140'020253035405070100125150175Top-rankedgeneral-domain sentences (in k)Devset PerplexityIn-domain baselineCross-EntropyMoore-Lewisbilingual M-LFigure 1: Corpus Selection ResultsThe perplexity of the dev set according to LMstrained on the top-ranked sentences varied from 77to 120, depending on the size of the subset and themethod used.
The Cross-Entropy method was con-sistently worse than the others, with a best perplex-ity of 99.4 on 20k sentences, and bilingual Moore-Lewis was consistently the best, with a lowest per-plexity of 76.8.
And yet, none of these scores areanywhere near the perplexity of 36.96 according tothe LM trained only on in-domain data.From this it can be deduced that the selectionmethods are not finding data that is strictly in-domain.
Rather they are extracting pseudo in-domain data which is relevant, but with a differingdistribution than the original in-domain corpus.As further evidence, consider the results of con-catenating the in-domain corpus with the best ex-tracted subcorpora (using the bilingual Moore-Lewis method), shown in Table 3.
The change in359both the dev and test scores appears to reflect dissim-ilarity in the underlying data.
Were the two datasetsmore alike, one would expect the models to rein-force each other rather than cancel out.Method Sentences Dev TestIWSLT 30k 45.43 37.17bilingual M-L 35k 39.59 42.31bilingual M-L 70k 40.84 42.29bilingual M-L 150k 42.64 42.22IWSLT + bi M-L 35k 47.71 41.78IWSLT + bi M-L 70k 47.80 42.30IWSLT + bi M-L 150k 48.44 42.01Table 3: Translation results concatenating the in-domainand pseudo in-domain data to train a single model.6 Translation Model CombinationBecause the pseudo in-domain data should be keptseparate from the in-domain data, one must trainmultiple translation models in order to advanta-geously use the general-domain corpus.
We now ex-amine how best to combine these models.6.1 Linear InterpolationA common approach to managing multiple transla-tion models is to interpolate them, as in (Foster andKuhn, 2007) and (Lu?
et al, 2007).
We tested thelinear interpolation of the in- and general-domaintranslation models as follows: Given one modelwhich assigns the probability P1(t|s) to the trans-lation of source string s into target string t, and asecond model which assigns the probability P2(t|s)to the same event, then the interpolated translationprobability is:P (t|s) = ?P1(t|s) + (1?
?
)P2(t|s) (4)Here ?
is a tunable weight between 0 and 1, whichwe tested in increments of 0.1.
Linear interpolationof phrase tables was shown to improve performanceover the individual models, but this still may not bethe most effective use of the translation models.6.2 Multiple ModelsWe next tested the approach in (Koehn andSchroeder, 2007), passing the two phrase tables di-rectly to the decoder and tuning a system using bothphrase tables in parallel.
Each phrase table receivesa separate set of weights during tuning, thus thiscombined translation model has more parametersthan a normal single-table system.Unlike (Nakov, 2008), we explicitly did not at-tempt to resolve any overlap between the phrase ta-bles, as there is no need to do so with the multipledecoding paths.
Any phrase pairs appearing in bothmodels will be treated separately by the decoder.However, the exact overlap between the phrase ta-bles was tiny, minimizing this effect.6.3 Translation Model Combination ResultsTable 4 shows baseline results for the in-domaintranslation system and the general-domain system,evaluated on the in-domain data.
The table alsoshows that linearly interpolating the translationmodels improved the overall BLEU score, as ex-pected.
However, using multiple decoding paths,and no explicit model merging at all, produced evenbetter results, by 2 BLEU points over the best indi-vidual model and 1.3 BLEU over the best interpo-lated model, which used ?
= 0.9.System Dev TestIWSLT 45.43 37.17General 42.62 40.51Interpolate IWSLT, General 48.46 41.28Use both IWSLT, General 49.13 42.50Table 4: Translation model combination resultsWe conclude that it can be more effective to notattempt translation model adaptation directly, andinstead let the decoder do the work.7 Combining Multi-Model and DataSelection ApproachesWe presented in Section 5 several methods to im-prove the performance of a single general-domaintranslation system by restricting its training corpuson an information-theoretic basis to a very smallnumber of sentences.
However, Section 6.3 showsthat using two translation models over all the avail-able data (one in-domain, one general-domain) out-performs any single individual translation model sofar, albeit only slightly.360Method Dev TestIWSLT 45.43 37.17General 42.62 40.51both IWSLT, General 49.13 42.50IWSLT, Moore-Lewis 35k 48.51 40.38IWSLT, Moore-Lewis 70k 49.65 40.45IWSLT, Moore-Lewis 150k 49.50 41.40IWSLT, bi M-L 35k 48.85 39.82IWSLT, bi M-L 70k 49.10 43.00IWSLT, bi M-L 150k 49.80 43.23Table 5: Translation results from using in-domain andpseudo in-domain translation models together.It is well and good to use the in-domain datato select pseudo in-domain data from the general-domain corpus, but given that this requires accessto an in-domain corpus, one might as well use it.As such, we used the in-domain translation modelalongside translation models trained on the subcor-pora selected using the Moore-Lewis and bilingualMoore-Lewis methods in Section 4.
The results arein Table 5.A translation system trained on a pseudo in-domain subset of the general corpus, selected withthe bilingual Moore-Lewis method, can be furtherimproved by combining with an in-domain model.Furthermore, this system combination works betterthan the conventional multi-model approach by upto 0.7 BLEU on both the dev and test sets.Thus a domain-adapted system comprising twophrase tables trained on a total of 180k sen-tences outperformed the standard multi-model sys-tem which was trained on 12 million sentences.
Thistiny combined system was also 3+ points better thanthe general-domain system by itself, and 6+ pointsbetter than the in-domain system alone.8 ConclusionsSentence pairs from a general-domain corpus thatseem similar to an in-domain corpus may not actu-ally represent the same distribution of language, asmeasured by language model perplexity.
Nonethe-less, we have shown that relatively tiny amounts ofthis pseudo in-domain data can prove more usefulthan the entire general-domain corpus for the pur-poses of domain-targeted translation tasks.This paper has also explored three simple yeteffective methods for extracting these pseudo in-domain sentences from a general-domain corpus.
Atranslation model trained on any of these subcorporacan be comparable ?
or substantially better ?
than atranslation system trained on the entire corpus.In particular, the new bilingual Moore-Lewismethod, which is specifically tailored to the ma-chine translation scenario, is shown to be more ef-ficient and stable for MT domain adaptation.
Trans-lation models trained on data selected in this wayconsistently outperformed the general-domain base-line while using as few as 35k out of 12 million sen-tences.
This fast and simple technique for discardingover 99% of the general-domain training corpus re-sulted in an increase of 1.8 BLEU points.We have also shown in passing that the linear in-terpolation of translation models may work less wellfor translation model adaptation than the multiplepaths decoding technique of (Birch et al, 2007).These approaches of data selection and model com-bination can be stacked, resulting in a compact, twophrase-table, translation system trained on 1% of theavailable data that again outperforms a state-of-the-art translation system trained on all the data.Besides improving translation performance, thiswork also provides a way to mine very large corporain a computationally-limited environment, such ason an ordinary computer or perhaps a mobile device.The maximum size of a useful general-domain cor-pus is now limited only by the availability of data,rather than by how large a translation model can befit into memory at once.ReferencesAmittai Axelrod.
2006.
Factored Language Models forStatistical Machine Translation.
M.Sc.
Thesis.
Univer-sity of Edinburgh, Scotland.Alexandra Birch, Miles Osborne and Philipp Koehn.2007.
CCG Supertags in Factored Translation Models.Workshop on Statistical Machine Translation, Associ-ation for Computational Linguistics.Stanley Chen and Joshua Goodman.
1998.
An Em-pirical Study of Smoothing Techniques for LanguageModeling.
Technical Report 10-98, Computer ScienceGroup, Harvard University.Matthias Eck, Stephan Vogel, and Alex Waibel.
2004.Language Model Adaptation for Statistical Machine361Translation based on Information Retrieval.
LanguageResources and Evaluation.George Foster and Roland Kuhn.
2007.
Mixture-ModelAdaptation for SMT.
Workshop on Statistical MachineTranslation, Association for Computational Linguis-tics.George Foster, Cyril Goutte, and Roland Kuhn.
2010.Discriminative Instatnce Weighting for Domain Adap-tation in Statistical Machine Translation.
EmpiricalMethods in Natural Language Processing.Jianfeng Gao, Joshua Goodman, Mingjing Li, and Kai-FuLee.
2002.
Toward a Unified Approach to StatisticalLanguage Modeling for Chinese.
ACM Transactionson Asian Language Information Processing.Xiaodong He.
2007.
Using Word-Dependent TransitionModels in HMM-based Word Alignment for Statisti-cal Machine Translation.
Workshop on Statistical Ma-chine Translation, Association for Computational Lin-guistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2003.
Moses: Open SourceToolkit for Statistical Machine Translation.
Demo Ses-sion, Association for Computational Linguistics.Philipp Koehn and Josh Schroeder.
2007.
Experimentsin Domain Adaptation for Statistical Machine Trans-lation.
Workshop on Statistical Machine Translation,Association for Computational Linguistics.Yajuan Lu?, Jin Huang and Qun Liu.
2007.
ImprovingStatistical Machine Translation Performance by Train-ing Data Selection and Optimization.
Empirical Meth-ods in Natural Language Processing and Computa-tional Natural Language Learning.Spyros Matsoukas, Antti-Veikko Rosti, Bing Zhang.2009.
Discriminative Corpus Weight Estimation forMachine Translation.
Empirical Methods in NaturalLanguage Processing.Robert Moore and William Lewis.
2010.
Intelligent Se-lection of Language Model Training Data.
Associationfor Computational Linguistics.Preslav Nakov.
2008.
Improving English-Spanish Sta-tistical Machine Translation: Experiments in DomainAdaptation, Sentence Paraphrasing, Tokenization, andRecasing.
Workshop on Statistical Machine Transla-tion, Association for Computational Linguistics.Franz Och and Hermann Ney.
2003.
A SystematicComparison of Various Statistical Alignment Models.Computational LinguisticsFranz Och.
2003.
Minimum Error Rate Training in Sta-tistical Machine Translation.
Association for Compu-tational LinguisticsAndreas Stolcke.
2002.
SRILM ?
An Extensible Lan-guage Modeling Toolkit.
Spoken Language Process-ing.Keiji Yasuda, Ruiqiang Zhang, Hirofumi Yamamoto, Ei-ichiro Sumita.
2008.
Method of Selecting Train-ing Data to Build a Compact and Efficient Transla-tion Model.
International Joint Conference on NaturalLanguage Processing.362
