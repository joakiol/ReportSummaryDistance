Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1724?1733,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsFriends with Motives: Using Text to Infer Influence on SCOTUSYanchuan SimLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USAysim@cs.cmu.eduBryan R. RoutledgeTepper School of BusinessCarnegie Mellon UniversityPittsburgh, PA 15213, USAroutledge@cmu.eduNoah A. SmithComputer Science & EngineeringUniversity of WashingtonSeattle, WA 98195, USAnasmith@cs.washington.eduAbstractWe present a probabilistic model of the in-fluence of language on the behavior of theU.S.
Supreme Court, specifically influence ofamicus briefs on Court decisions and opin-ions.
The approach assumes that amici arerational, utility-maximizing agents who try towin votes or affect the language of court opin-ions.
Our model leads to improved predictionsof justices?
votes and perplexity of opinionlanguage.
It is amenable to inspection, allow-ing us to explore inferences about the persua-siveness of different amici and influenceabilityof different justices; these are consistent withearlier findings.
?Language is the central tool of our trade.
?John G. Roberts, 2007 (Garner, 2010)1 IntroductionThe Supreme Court of the United States (SCOTUS),the highest court in the American judiciary, makesdecisions with far-reaching effects.
In a typical case,there are four participating parties: petitioners andrespondents who file briefs arguing the merits oftheir sides of a case (?merits briefs?
); third-party en-tities with an interest (but not a direct stake) in thecase, who file amicus curiae1 briefs to provide fur-ther arguments and recommendations on either side;and justices who, after oral arguments and discus-1Amicus curiae is Latin for ?friends of the court.?
Hereafter,we use amicus in singular and amici in plural to refer to theseinterested third parties.
It is common for several amici to co-author a single brief, which we account for in our model.sions, vote on the case and write ?opinions?
to ex-plain the Court?s decisions.2In recent years, amicus briefs are increasingly be-ing employed as a lobbying tool to influence theCourt?s decision-making process (Franze and An-derson, 2015; Kearney and Merrill, 2000).
The con-tent of these briefs reveals explicit attempts to per-suade justices and provides a fascinating setting forempirical study of influence through language.
Assuch, we take the perspective of an amicus, propos-ing a probabilistic model of the various parties to acase that accounts for the amicus?
goals.Our model of SCOTUS is considerably morecomprehensive than past work in political science,which has focused primarily on ideal point modelsthat use votes as evidence.
Text has been incorpo-rated more recently as a way of making such modelsmore interpretable, but without changing the funda-mental assumptions (Lauderdale and Clark, 2014).Here, we draw on decision theory to posit amici asrational agents.
We assume these amici-agents max-imize their expected utility by framing their argu-ments to sway justices towards favorable outcomes.We build directly on Sim et al (2015), who usedutility functions to explicitly model the goals of am-ici in a probabilistic setting.
Their approach onlyconsidered amici in aggregate, inferring nothingabout any specific amicus, such as experience or mo-tivation for filing briefs.
Here, we enrich their modelto allow such analysis and also introduce Court opin-ions as evidence.
By modeling the justices?
author-2We use the term opinions to refer to these decision-explaining documents, not to abstract positions as the term isused in much NLP research.1724ing process as well, we can capture an importantaspect of amici?s goals: influencing the text of theopinions.In ?3, we demonstrate the effectiveness of our ap-proach on vote prediction and perplexity.
Further-more, we present analyses that reveal the persuasive-ness of amici and influenceability of justices that areconsistent with past findings.2 Generative Models of SCOTUSOur approach builds on a series of probabilisticmodels only recently considered in NLP research.To keep the discussion self-contained, we begin withclassical models of votes alone and build up towardour novel contributions.2.1 Modeling VotesIdeal point (IP) models are a mainstay in quantitativepolitical science, often applied to voting records toplace voters (lawmakers, justices, etc.)
in a continu-ous space.
A justice?s ?ideal point?
is a latent vari-able positioning him in this space.
Martin and Quinn(2002) introduced the unidimensional IP model forjudicial votes, which posits an IP ?j ?
R for eachjustice j.
Often the ?j values are interpreted as po-sitions along a liberal-conservative ideological spec-trum.
Each case i is represented by popularity (ai)and polarity (bi) parameters.3 A probabilistic viewof the unidimensional IP model is that justice j votesin favor of case i?s petitioner (as opposed to the re-spondent) with probabilityp(vi,j = petitioner | ?j , ai, bi) = ?
(ai + bi?j)where ?
(x) = exp(x)1+exp(x) is the logistic function.When the popularity parameter ai is high enough,every justice is more likely to favor the petitioner.The polarity bi captures the importance of a justice?sideology: polarizing cases (i.e., |bi|  0) push jus-tice j more strongly to the side of the petitioner (ifbi has the same sign as ?j) or the respondent (other-wise).Amici IP models.
Sim et al (2015) introduceda multidimensional IP model that incorporated text3This model is also known as a two parameter logistic modelin item-response theory (Fox, 2010), where ai is ?difficulty?and bi is ?discrimination.
?from merits and amicus briefs as evidence.
They in-ferred dimensions of IP that are grounded in ?top-ical?
space, where topics are learned using latentDirichlet alocation (Blei et al, 2003).
In their pro-posed model, the merits briefs describe the issuesand facts of the case, while amicus briefs were hy-pothesized to ?frame?
the facts and potentially influ-ence the outcome of the case.
For case i and justicej, the vote probability isp(vi,j = petitioner | ?j ,?i,?i, ai, bi, ci) (1)= ?
(ai + bi?>j(?i + 1|Ai|?k?Ai csi,ki ?i,k?
??
?case IP))whereAi is the set of amicus briefs filed on this case,si,k denotes the side (?
{petitioner, respondent})supported by the kth brief, and cpi , and cri are the ami-cus polarities for briefs on either side.
The case IP isinfluenced by merits briefs (embedded in ?i) and bythe amicus briefs (embedded in ?i,k), both of whichare rescaled independently by the case discrimina-tion parameters to generate the vote probability.
Themodel assumes that briefs on the same side share asingle embedding and that individual briefs on oneside influence the vote-specific IP equally.New IP model: Persuasive amici.
Lynch (2004)and others have argued that some amici are moreeffective than others, with greater influence on jus-tices.
We therefore propose a new model whichconsiders amici as individual actors.
Starting fromEq.
1, we consider two additional variables: eachamicus e?s persuasiveness (pie > 0) and each jus-tice j?s influenceability (?j > 0).4p(vi,j = petitioner | ?j , ?j ,?i,?i, ai, bi,pi) (2)= ?
(ai + bi?>j(?i + ?j|Ai|?k?Ai p?ii,k?i,k))where p?ii,k =?e?Ei,k pie|Ei,k| is the average of their pi-values, with Ei,k denoting the set of entities who co-authored the kth amicus brief for case i.Intuitively, a larger value of ?j will shift the caseIP more towards the contents of the amicus briefs,4Note that the amici IP model of Sim et al (2015), Eq.
1, isa special case of this model where ?j = 1 and each case has po-larity parameters for each side; no information is shared acrossbriefs written by the same amicus-entity for different cases.1725thus making the justice seem more ?influenced?
byamicus.
Likewise, briefs co-authored by groups ofamici who are more effective (i.e., larger p?ii,k), will?frame?
the case towards their biases.
Unlike Sim etal.
(2015), we eschew the amicus polarity parame-ters (ci) and instead rely on the influenceability andpersuasiveness parameters.
Furthermore, we notethat they performed a post-hoc analysis of amici in-fluence on justices but we do so directly through ?j .With appropriate priors on the latent variables, thegenerative story for votes is:1.
For each topic t ?
{1, .
.
.
, T}, draw topic-worddistributions ?t ?
Dirichlet(?).2.
For each justice j ?
J , draw justice IP ?j ?N (0, ?2JI + ?1)5 and influenceability ?j ?logN (0, ?2I I).3.
For each amicus-entity e ?
E , draw its persua-siveness pie ?
logN (0, ?2P I).4.
For each case i ?
C:(a) Draw case parameters ai, bi ?
N (0, ?2C).
(b) Draw topic proportions for merits ?i ?Dirichlet(?).
(c) For each word w(m)i,n in the merits briefs, drawtopic indicators z(m)i,n ?
Categorical(?i) andw(m)i,n ?
Categorical(?z(m)i,n ).
(d) For each amicus brief indexed by k:i.
Draw topic proportions ?i,k according to adistribution discussed in ?2.3.ii.
For each word w(a)i,n in the brief, draw topicindicators z(a)i,k,n ?
Categorical(?i,k) andw(a)i,k,n ?
Categorical(?z(a)i,k,n).
(e) For each participating justice j ?
Ji, draw votevi,j according to Eq.
2.2.2 Modeling OpinionsIn most SCOTUS cases, a justice is assigned to au-thor a majority opinion, and justices voting in themajority ?join?
in the opinion.
Justices may authoradditional opinions concurring or dissenting withthe majority, and they may choose to join concurring5The positive off-diagonal elements of the covariance ma-trix for justice IPs (?j) orient the issue-specific dimensions inthe same direction (i.e., with conservatives at the same end)and provide shrinkage of IP in each dimension to their commonmean across dimensions (Lauderdale and Clark, 2014).and dissenting opinions written by others.
Here, weextend the IP model of votes to generate the opinionsof a case; this marks the second major extension be-yond the IP model of Sim et al (2015).SCOTUS justices often incorporate languagefrom merits (Feldman, 2016b; Feldman, 2016a) andamicus (Collins et al, 2015; Ditzler, 2011) briefsinto their opinions.
While amicus briefs are notusually used directly in legal analyses, the back-ground and technical information they provide areoften quoted in opinions.
As such, we model opin-ions as a mixture of its justice-authors?
topic pref-erences, topic proportions of the merits briefs (?
),and topic proportions of the amicus briefs (?).
Thiscan also be viewed as an author-topic model (Rosen-Zvi et al, 2004) where justices, litigants, and groupsof amici are all effective authors.
To accomplishthis, we introduce an explicit switching variable xfor each word, which selects between the differentsources of topics, to capture the mixture proportions.Since any justice can author additional opinionsexplaining the rationale behind their votes, we con-catenate all opinions supporting the same side of acase into a single document.6 However, we notethat concurring opinions often contain perspectivesthat are different from the majority opinion andby concatenating them, we may lose some infor-mation about individual justices?
styles or prefer-ences.
Building on the generative model for votes,the generative story for each case i?s two opinions-documents is:5.
For each justice j ?
J , draw topics ?j ?Dirichlet(?).6.
For each case i ?
C:(a) For each side s ?
{petitioner, respondent},draw ?author?-mixing proportions:?
si ?
Dirichlet?????????????
?p(vi,1 = s)...p(vi,|J | = s)11??????????????
(3)where the last two dimensions are for choos-ing topics from the merits and amicus briefs, re-6Opinions where justices dissent from the majority are con-catenated together, and those where justices concur with the ma-jority are concatenated with the majority opinion.1726spectively.7 Intuitively, our model assumes thatopinions will incorporate more language fromjustices who agree with it.
(b) For each side s ?
{petitioner, respondent} andeach word w(o)i,s,n in the opinion for side s,i.
Draw xi,s,n ?
Categorical(?
si ).ii.
If xi,s,n ?
Ji, draw z(o)i,s,n ?Categorical(?xi,s,n), the justice?s topicdistribution.iii.
If xi,s,n = merits, draw z(o)i,s,n ?Categorical(?i), the merits topic distribu-tion.iv.
If xi,s,n = amici, draw z(o)i,s,n ?Categorical(?si ), side s?s amicus briefstopic distribution.v.
Draw word w(o)i,s,n ?
Categorical(?z(o)i,s,n).Unlike in the Court, where an opinion is mainlyauthored by a single justice, all the participating jus-tices contribute to an opinion in our generative story,with different proportions.
This approach simpli-fies the computational model and reflects the closed-door nature of discussions held by justices prior towriting their opinions.
Our model assumes that jus-tices debate together, and that the arguments are re-flected in the final opinions.
In future work, wemight extend the model to infer an authoring pro-cess that separates an initial author from ?joiners.
?2.3 Amici UtilityOur approach assumes that amici are rational andpurposeful decisionmakers who write briefs to in-fluence the outcome of a case; this assumption leadsto the design of the distribution over ?
(generativemodel step 4(d)i).
When writing a brief ?, an am-icus seeks to increase the response to her brief (i.e.,votes), while keeping her costs low.
We encode herobjectives as a utility function, which she aims tomaximize with respect to the decision variable ?:U(?)
= R(?)?
C(?)
(4)where R(?)
is the extrinsic response (reward) thatan amicus gets from filing brief ?
and C(?)
is the?cost?
of filing the brief; dependency on other latent7In cases where there are less than nine justices voting, thesize of ?
pi and ?
ri may be smaller.variables is notationally suppressed.
When author-ing her brief, we assume that the amicus writer hasknowledge of the justices (IP and topic preferences),case parameters, and merits, but not the other amiciparticipating in the case.8Amicus curiae are motivated to position them-selves (through their briefs) in such a way as to im-prove the likelihood that their arguments will per-suade SCOTUS justices.
This is reflected in the waya justice votes or through the language of the opin-ions.
Hence, we investigate two response functions.First, an amicus supporting side s seeks to win votesfor s,Rvote(?)
= 1|J |?j?J p(vj = s | .
.
.
), (5)which is the expected number of votes for side s,under the model.
This follows Sim et al (2015).An alternative is to maximize the (topical) simi-larity between her brief and the Court?s opinion(s)siding with s,Ropinion(?)
= 1?H2(?,?s), (6)where H2(P,Q) = 12?
?P ?
?Q?22 is the squaredHellinger (1909) distance between two distributions,and ?s is the expected topic mixture under themodel assumptions in ?2.2 (which has a closedform).
In short, the amicus gains utility by ac-curately predicting the expected opinion, therebygaining publicity and demonstrating to members,donors, potential clients, and others that the lan-guage of the highly visible SCOTUS opinion wasinfluenced.
Both Eqs.
5 and 6 reward amici whenjustices ?agree?
with them, for different definitionsof agreement.We assume the cost C(?)
= H2(?,?
), thesquared Hellinger distance between the mixture pro-portions of the amicus brief and merits briefs.9 Thecost term defines the ?budget?
set of the amicus:briefs cannot be arbitrary text, as there is disutility or8Capturing strategic amici agents (a petitioner amicuschoosing brief topics considering a respondent amicus?
brief)would require a game-theoretic model and, we conjecture,would require a much richer representation of policy and goals.That idea is left for future research.9Sim et al (2015) used a Euclidean distance for cost ratherthan Hellinger distance, which we believe is a better fit for prob-ability distributions without sacrificing symmetry (cf.
KL diver-gence).1727effort required to carefully frame a case, and mone-tary cost to hiring legal counsel.
The key assumptionis that framing is costly, while simply matching themerits is cheap (and presumably unnecessary).Notationally, we use Uvote to refer to modelswhere Eq.
5 is in the utility function (in Eq.
4) andUopinion where it is Eq.
6.Random utility models Recall our assumptionthat amici are purposeful writers whose briefs areoptimized for their utility function.
In an ideal set-ting, the ?
which we observe will be utility maxi-mizing.
We simplify computation by assuming thatthese amici agents?
preferences also contain an id-iosyncratic random component that is unobserved tous.
This is a common assumption in discrete choicemodels known as a ?random utility model?
(McFad-den, 1974).
We view the utility function as a prioron ?,putil(?
| .
.
.)
?
exp ?U(?
),where our functional equations for utility imply?1 ?
U(?)
?
1. ?
is a hyperparameter tuned us-ing cross validation.
The behavior which we observe(i.e., the amicus?
topic mixture proportions) has alikelihood that is proportional to utility.2.4 Parameter EstimationThe models we described above can be estimatedwithin a Bayesian framework.
We decoupled theestimation of the votes model from the opinionsmodel; we first estimate the parameters for the votesmodel and hold them fixed while we estimate thenew latent variables in the opinions model.
In ourpreliminary experiments, we found that estimatingparameters for both votes and opinions jointly led toslow mixing and poor predictive performance.
Sep-arating the estimation procedure into two stages al-lows the model to find better parameters for the votesmodel, which are then fed into the opinions model aspriors through the vote probabilities.We used Metropolis within Gibbs, a hybridMCMC algorithm, to sample the latent parametersfrom their posterior distributions (Tierney, 1994).10For the Metropolis-Hastings proposal distributions,we used a Gaussian for the case parameters a, b, andjustice IPs ?, log-normal distributions for ?
and pi,10The details of our sampler and hyperparameter settings canbe found in ?A and ?B of the supplementary materials.and logistic-normal distribution for the variables onthe simplex ?,?, ?
, and ?.
We tuned the hyperpa-rameters of the proposal distributions at each itera-tion to achieve a target acceptance rate of 15?45%.We used T = 128 topics for model and initializedtopic proportions (?,?)
and topic-word distribu-tions (?)
using online LDA (Hoffman et al, 2010).3 ExperimentsData.
In our experiments, we use SCOTUS casesbetween 1985?2014; votes and metadata are fromSpaeth et al (2013) and brief texts come from Sim etal.
(2015).
We concatenate each of the 2,643 cases?merits briefs from both parties to form a single doc-ument, where the text is used to infer the represen-tation of the case in topical space (?
; i.e., meritsbriefs are treated as ?facts of the case?).
Likewise,opinions supporting the same side of the case (i.e.,majority and concurring vs. dissents) were concate-nated to form a single document.
In our dataset, theopinions are explicitly labeled with the justice whoauthored them (as well as other justices who decideto ?join?
it).As the amicus briefs in the dataset were not ex-plicitly labeled with the side that they support, Simet al (2015) built a binary classifier with bag-of-n-gram features that took advantage of cues in thebrief content that strongly signal the side that theamici supports (e.g., ?in support of petitioner?).
Weused their classifier to label the amici?s support-ing side.
Additionally, we created regular expres-sion rules to identify and standardize amicus au-thors from the header of briefs.
We filtered am-ici who have participated in fewer than 5 briefs11and merged regional chapters of amicus organiza-tions together (i.e., ?ACLU of Kansas?
and ?ACLUof Kentucky?
are both labeled ?ACLU?).
On theother hand, we separated labeled amicus briefs bythe U.S.
Solicitor General according to the presi-dential administration when the brief is filed (i.e.,an amicus brief filed during Obama?s administrationwill be labeled ?USSG-Obama?).
The top three am-ici by number of briefs filed are American Civil Lib-erties Union (463), Utah (376), and National Asso-11Briefs which have no authors as a result of the filtering pro-cess are removed from our dataset.
This occurred in about 24%of amicus briefs.1728Cases / Votes 2,643 / 23,465Merits / Amicus briefs 16,416 / 16,303Opinions 4,187Phrases 18,207,326Table 1: Corpus statistics.ciation of Criminal Defense Lawyers (359).We represent a document as a bag of n-grams withpart of speech tags that follow the simple but effec-tive pattern (Adjective|Cardinal|Noun)+ Noun (Juste-son and Katz, 1995).
We filter phrases appearingfewer than 100 times or in more than 8,500 docu-ments, obtaining a final set of 48,589 phrase types.Table 1 summarizes the details of our corpus.Predicting Votes.
We quantify the performance ofour vote model using 5-fold cross validation and onpredicting future votes from past votes.
The utilityfunction in the vote model uses the response func-tion in Eq.
5.
Due to the specification of IP models,we need the case parameters of new cases to predictthe direction of the votes.
Gerrish and Blei (2011)accomplished this by using regression on legislativetext to predict the case parameters (a, b).
Here, wefollow a similar approach, fitting ridge regressionmodels on the merits brief topic mixtures ?
to pre-dict a and b for each case.12 On the held-out testcases, we sampled the mixture proportions for themerits and amicus briefs directly using latent Dirich-let alocation with parameters learned while fittingour vote model.
With the parameters from our fittedvote model and ridge regression, we can predict thevotes of every justice for every case.We compared the performance of our model withtwo strong baselines: (i) a random forest trained oncase-centric metadata coded by Spaeth et al (2013)to make predictions on how justices would vote(Katz et al, 2014) and (ii) Sim et al (2015)?s amiciIP model, which uses amicus briefs and their versionof utility; it is a simpler version of our vote modelthat does not consider the persuasiveness of differ-ent amici or the influenceability of different justices.For prediction in Sim et al (2015), we used the sameapproach described above to estimate the case pa-rameters a, b, and regressing on amicus brief topics(?)
instead for amicus polarities cp and cr.
Table 212We tuned the parameters of the regression using 5-foldcross-validation on the training data.Model 5-fold 2013 2014Most frequent 0.597 0.694 0.650Random forest 0.651 0.648 0.633Vote model without U vote 0.661 0.655 0.660Sim et al (2015) 0.675 0.658 0.661Vote model with U vote 0.685 0.664 0.672Table 2: Accuracy of vote prediction.
There are 70 cases(625 votes) and 69 cases (619 votes) in the 2013 and 2014test sets, respectively.shows performance on vote prediction.We evaluated the models using 5-fold cross vali-dation, as well as on forecasting votes in 2013 and2014 (trained using data from 1985 to the precedingyear).
Our model outperformed the baseline models.The improvement in accuracy over Sim et al (2015)is small; most likely because both models are verysimilar, the main difference being the parametriza-tion of amicus briefs.
In the 2013 test set, the distri-bution of votes is significantly skewed towards thepetitioner (compared to the training data), which re-sulted in the most frequent class classifier perform-ing much better than everything else.
Fig.
1 illus-trates our model?s estimated ideal points for selectedtopics.Predicting Opinions.
We also estimated the opin-ion model using the utility function with responsefunction in Eq.
6.
We use perplexity as a proxy tomeasure the opinion content predictive ability of ourmodel.
Perplexity on a test set is commonly usedto quantify the generalization ability of probabilisticmodels and make comparisons among models overthe same observation space.
For a case with opinionw supporting side s, the perplexity is defined asexp(?
log p(w | s, .
.
.
)N),whereN is the number of tokens in the opinion and alower perplexity indicates better generalization per-formance.
The likelihood term can be approximatedusing samples from the inference step.Table 3 shows the perplexity of our model onopinions in the test set.
As described in ?2.4, welearn the vote model in the first stage before esti-mating the opinion model.
Here, we compare ourmodel against using vote models that do not in-clude Uvote to evaluate the sensitivity of our opinion17294 2 0 2 4 6GinsburgSotomayorKaganBreyerKennedyRobertsAlitoScaliaThomas17: juror, prosecutor,death penalty4 2 0 2 4 632: speech, firstamendment, free speech4 2 0 2 4 661: eeoc, title vii,discrimination4 2 0 2 4 6120: marriage, samesex, manFigure 1: Justices?
ideal points for selected topics.
Justices whose topic IPs are close to each other are more likely tovote in the same direction on cases involving those topics.
The IP estimated by our model is consistent with publiclyavailable knowledge regarding justices?
ideological stances on these issues.model to the vote model parameters.
Additionally,we compared against two baselines trained on justthe opinions: one using LDA13 and another using theauthor-topic model (Rosen-Zvi et al, 2004).
For theauthor-topic model, we treat each opinion as being?authored?
by the participating justices, a pseudo-author representing the litigants which is shared be-tween opinions in a case, and a unique amicus au-thor for each side.
Our model with Uopinion achievesbetter generalization performance than the simplerbaselines, while we do not see significant differencesin whether the first stage vote models useUvote.
Thisis not surprising since the vote model?s results aresimilar with or without Uvote and it influences theopinion model indirectly through priors andUopinion.In our model, the latent variable ?j captures theproportion of topics that justice j is likely to con-tribute to an opinion.
When j has a high probabilityof voting for a particular side, our informed prior in-creases the likelihood that j?s topics will be selectedfor words in the opinion.
While ?j serves a similarpurpose to ?j in characterizing j through her ideo-logical positions, ?j relies on votes and gives us a?direction?
of j?s ideological standing, whereas ?jis estimated from text produced by the justices andonly gives us the ?magnitude?
of her tendency to au-thor on a particular issue.
In Table 4, we identify thetop topics in ?j by considering the deviation fromthe mean of all justice?s ?, i.e., ?j,k ?
1|J |?j ?j,k.Amici Persuasiveness.
The latent variable pie cap-tures the model?s belief about amicus e?s brief?s ef-13We used scikit-learn?s LDA module (Pedregosa etal., 2011) which implements the online variational Bayes algo-rithm (Hoffman et al, 2010).Model 5-fold 2013 2014LDA 2.86 2.67 2.63Author-Topic 2.62 2.36 2.25Opinion model without U opinion?2.43 ?2.26 ?2.132.45 2.27 2.11Opinion model with U opinion?2.10 ?1.91 ?1.962.07 1.98 1.94Table 3: Perplexity of Court?s opinions (?103).
Thereare 30,133 phrases (98 opinions) and 23,706 phrases (109opinions) in the 2013 and 2014 test set, respectively.
Re-sults marked ?
are initialized with a vote model U vote.fect on the case IP, which we call ?persuasiveness.
?A large pie indicates that across the dataset, e exertsa larger effect on the case IPs, that is, according toour model, she has a larger impact on the Court?sdecision than other amici.
Fig.
2 is a swarm plotillustrating the distribution of pi values for differenttypes of amicus writers.Our model infers that governmental offices tend tohave larger pi values than private organizations, es-pecially the U.S.
Solicitor General.14 In fact, Lynch(2004) found through interviews with SCOTUS lawclerks that ?amicus briefs from the solicitor generalare ?head and shoulders?
above the rest, and are of-ten considered more carefully than party briefs.
?Another interesting observation from Fig.
2 is thelow pi value for ACLU and ABA, despite being pro-lific amicus brief filers.
While it is tempting to saythat amici with low pi values are ineffective, we findthat there is almost no correlation between pi and theproportion of cases where they were on the winningside.15 Note that our model does not assume that a14The average pi for Federal, State/Local and Others are 2.35,1.11, and 0.929 respectively.15The Spearman?s ?
between pi and the proportion of winning1730John G. Roberts32: speech, first amendment, free speech, message, expres-sion61: eeoc, title vii, discrimination, woman, civil rights act52: sec, fraud, security, investor, section ##bRuth B. Ginsburg61: eeoc, title vii, discrimination, woman, civil rights act80: class, settlement, rule ##, class action, r civ96: taxpayer, bank, corporation, fund, irsAntonin Scalia94: 42 USC 1983, qualified immunity, immunity, official,section ####57: president, senate, executive, article, framer80: class, settlement, rule ##, class action, r civTable 4: Top three topics contributed to Court opinionsfor selected justices (?).
The full list can be found insupplementary ?C.?persuasive?
amicus tends to win.
Instead, an am-icus with large pi will impact the case IP most, andthus explain a justice?s vote or opinion (even dissent-ing) more than the other components in a case.Insofar as pi explains a vote, we must exercise cau-tion; it is possible that the amicus played no rolein the decision-making process and the values of piesimply reflect our modeling assumptions and/or ar-tifacts of the data.
Without entering the minds ofSCOTUS justices, or at least observing their closed-door deliberations, it is difficult to measure the in-fluence of amicus briefs on justices?
decisions.Justice Influenceability.
The latent variable ?jmeasures the relative effect of amicus briefs on jus-tice j?s vote IP; when ?j is large, justice j?s voteprobability is affected by amicus briefs more.
Since?j is shared between all cases that a justice partic-ipates in, ?j should correspond to how much theyvalue amicus briefs.
Some justices, such as the lateScalia, are known to be dubious of amicus briefs,preferring to leave the task of reading these briefs totheir law clerks, who will pick out any notable briefsfor them; we would expect Scalia to have a smaller?
than other justices.
In Table 5, we compare the ?values of justices with how often they cite an amicusbrief in any opinion they wrote (Franze and Ander-son, 2015).
The ?
values estimated by our model aresides is ?0.0549.
On average, an amicus supports the winningside in 55% of cases.
For the ACLU, ABA, CAC, and CWFA,the proportions are 44%, 50%, 47%, and 50% respectively.Federal State/Local Others432101234logpiBushClintonObamaReaganBush Sr.NYC,RI,MSACLUABACWFACACFigure 2: Amici ?persuasiveness?
by organization type.Federal refers to different presidential administration?sfederal government (and represented by the U.S. SolicitorGeneral) and State/Local refers to state and local govern-ments.
The abbreviated amici are New York City (NYC),Rhode Island (RI), Mississippi (MS), Concerned WomenFor America (CWFA), Constitution Accountability Cen-ter (CAC), American Bar Association (ABA), and Amer-ican Civil Liberties Union (ACLU).consistent with our expectations.16We note that the ?
values correlate considerablywith the general ideological leanings of the justices.This might be a coincidence or an inability of themodel?s specification to discern between ideologicalextremeness and influenceability.4 Related WorkThe ideal points model was first introduced by Pooleand Rosenthal (1985) and has inspired a variety of IPmodels in SCOTUS (Lauderdale and Clark, 2014;Martin and Quinn, 2002) and Congressional bills(Clinton et al, 2004; Gerrish and Blei, 2011; Heck-man and Snyder, 1996).
IP has provided a usefulframework to characterize voters using roll call in-formation and textual evidence.We view amicus briefs as ?purposeful?
texts,where authors are writing to maximize their utilityfunction.
This is related to work investigating newsmedia for ?slant?
to maximize profit (Gentzkow andShapiro, 2010) and economists choosing researchtopics maximize certain career outcomes (Jelveh etal., 2015).
More generally, extensive literature in16The Spearman?s ?
between ?j and citation rates is 0.678.1731Justice ?j Citation rate (%)Sonia Sotomayor 1.590 45Elena Kagan 0.714 40Stephen G. Breyer 0.637 38Ruth B. Ginsburg 0.515 41John G. Roberts 0.495 42Anthony M. Kennedy 0.468 42Samuel A. Alito 0.286 27Antonin Scalia 0.268 22Clarence Thomas 0.162 25Table 5: Justice ?
values and their average amicus cita-tion rates between 2010?2015, provided by Franze andAnderson (2015).econometrics estimates structural utility-based deci-sions (Berry et al, 1995, inter alia).Researchers have used SCOTUS texts to studyauthorship (Li et al, 2013), historical changes(Wang et al, 2012), power relationships (Danescu-Niculescu-Mizil et al, 2012; Prabhakaran et al,2013), and pragmatics (Goldwasser and Daume?,2014).5 ConclusionWe presented a random utility model of the SupremeCourt that is more comprehensive than earlier work.We considered an individual amicus?
persuasivenessand motivations through two different utility func-tions.
On the vote prediction task, our results areconsistent with earlier work, and we can infer andcompare the relative effectiveness of an individualamicus.
Moreover, our opinions model and opinionutility function achieved better generalization per-formance than simpler methods.AcknowledgmentsThe authors thank the anonymous reviewers for theirthoughtful feedback and Tom Clark, Philip Resnik,and members of the ARK group for their valuablecomments.
This research was supported in part byan A*STAR fellowship to Y. Sim, by a Google re-search award, and by computing resources from thePittsburgh Supercomputing Center.ReferencesSteven Berry, James Levinsohn, and Ariel Pakes.
1995.Automobile prices in market equilibrium.
Economet-rica: Journal of the Econometric Society, pages 841?890.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet alocation.
Journal of MachineLearning Research, 3:993?1022.Joshua Clinton, Simon Jackman, and Douglas Rivers.2004.
The statistical analysis of roll call data.
Ameri-can Political Science Review, 98:355?370.Paul M. Collins, Pamela C. Corley, and Jesse Hamner.2015.
The influence of amicus curiae briefs on U.S.Supreme Court opinion content.
Law & Society Re-view, 49(4):917?944.Cristian Danescu-Niculescu-Mizil, Lillian Lee, Bo Pang,and Jon Kleinberg.
2012.
Echoes of power: Languageeffects and power differences in social interaction.
InProc.
of WWW.Megan Ann Ditzler.
2011.
Language overlap betweensolicitor general amicus curiae and Supreme Court ma-jority opinions: An analysis.
Master?s thesis, SouthernIllinois University Carbondale.Adam Feldman.
2016a.
All copying is not created equal:Examining Supreme Court opinions?
borrowed lan-guage.
Journal of Appellate Practice and Process, 17.Adam Feldman.
2016b.
A brief assessment of SupremeCourt opinion language, 1946?2013.
Mississippi LawJournal, 85.J.
P. Fox.
2010.
Bayesian Item Response Modeling: The-ory and Applications.
Statistics for Social and Behav-ioral Sciences.
Springer-Verlag New York.Anthony J. Franze and R. Reeves Anderson.2015.
Record breaking term for amicus cu-riae in Supreme Court reflects new norm.National Law Journal, Supreme Court Brief.http://www.nationallawjournal.com/supremecourtbrief/id=1202735095655/,August 19, 2015.Bryan A. Garner.
2010.
Interviews with United StatesSupreme Court justices.
In Joseph Kimble, editor, TheScribes Journal of Legal Writing, volume 13.
Ameri-can Society of Legal Writers.Matthew Gentzkow and Jesse M Shapiro.
2010.
Whatdrives media slant?
Evidence from U.S. daily newspa-pers.
Econometrica, 78(1):35?71.Sean Gerrish and David Blei.
2011.
Predicting legisla-tive roll calls from text.
In Proc.
of ICML.Dan Goldwasser and Hal Daume?.
2014.
?I object!?
mod-eling latent pragmatic effects in courtroom dialogues.In Proc.
of EACL.James J. Heckman and James M. Snyder.
1996.
Linearprobability models of the demand for attributes withan empirical application to estimating the preferencesof legislators.
Working Paper 5785, National Bureauof Economic Research.1732Ernst D. Hellinger.
1909.
Neue Begru?ndung derTheorie quadratischer Formen von unendlichvielenVera?nderlichen.
Journal fu?r die reine und angewandteMathematik (Crelle?s Journal), 1909(136):210?271.Matthew Hoffman, Francis R. Bach, and David M. Blei.2010.
Online learning for latent Dirichlet alocation.In Advances in Neural Information Processing Sys-tems 23.Zubin Jelveh, Bruce Kogut, and Suresh Naidu.
2015.Political language in economics.
Columbia BusinessSchool Research Paper Series, 14(57).John S. Justeson and Slava M. Katz.
1995.
Technical ter-minology: Some linguistic properties and an algorithmfor identification in text.
Natural Language Engineer-ing, 1:9?27.Daniel Martin Katz, Michael James Bommarito, andJosh Blackman.
2014.
Predicting the behavior ofthe Supreme Court of the United States: A gen-eral approach.
http://ssrn.com/abstract=2463244.Joseph D. Kearney and Thomas W. Merrill.
2000.
Theinfluence of amicus curiae briefs on the SupremeCourt.
University of Pennsylvania Law Review, pages743?855.Benjamin E. Lauderdale and Tom S. Clark.
2014.Scaling politically meaningful dimensions using textsand votes.
American Journal of Political Science,58(3):754?771.William Li, Pablo Azar, David Larochelle, Phil Hill,James Cox, Robert C. Berwick, and Andrew W. Lo.2013.
Using algorithmic attribution techniques todetermine authorship in unsigned judicial opinions.Stanford Technology Law Review, pages 503?534.Kelly J. Lynch.
2004.
Best friends ?
Supreme Court lawclerks on effective amicus curiae briefs.
Journal ofLaw & Politics, 20.Andrew D. Martin and Kevin M. Quinn.
2002.
Dynamicideal point estimation via Markov Chain Monte Carlofor the U.S. Supreme Court, 19531999.
Political Anal-ysis, 10(2):134?153.Daniel McFadden.
1974.
Conditional logit analysis ofqualitative choice behavior.
In Paul Zarembka, editor,Frontiers in Econometrics, pages 105?142.
AcademicPress.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Prettenhofer,R.
Weiss, V. Dubourg, J. Vanderplas, A. Passos,D.
Cournapeau, M. Brucher, M. Perrot, and E. Duches-nay.
2011.
Scikit-learn: Machine learning in Python.Journal of Machine Learning Research, 12:2825?2830.
Available at http://scikit-learn.org/.Keith T. Poole and Howard Rosenthal.
1985.
A spatialmodel for legislative roll call analysis.
American Jour-nal of Political Science, 29(2):357?384.Vinodkumar Prabhakaran, Ajita John, and Dore?e D.Seligmann.
2013. Who had the upper hand?
rank-ing participants of interactions based on their relativepower.
In Proc.
of IJCNLP.Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers, andPadhraic Smyth.
2004.
The author-topic model forauthors and documents.
In Proc.
of UAI.Yanchuan Sim, Bryan Routledge, and Noah A. Smith.2015.
The utility of text: The case of amicus briefsand the Supreme Court.
In Proc.
of AAAI.Harold J. Spaeth, Sara Benesh, Lee Epstein, Andrew D.Martin, Jeffrey A. Segal, and Theodore J. Ruger.
2013.Supreme Court Database, Version 2013 Release 01.Database at http://supremecourtdatabase.org.Luke Tierney.
1994.
Markov chains for explor-ing posterior distributions.
The Annals of Statistics,22(4):1701?1728.William Yang Wang, Elijah Mayfield, Suresh Naidu, andJeremiah Dittmar.
2012.
Historical analysis of le-gal opinions with a sparse mixed-effects latent variablemodel.
In Proc.
of ACL.1733
