CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 81?88Manchester, August 2008Baby SRL: Modeling Early Language Acquisition.Michael ConnorDepartment of Computer ScienceUniversity of Illinoisconnor2@uiuc.eduYael GertnerBeckman InstituteUniversity of Illinoisygertner@cyrus.psych.uiuc.eduCynthia FisherDepartment of PsychologyUniversity of Illinoiscfisher@cyrus.psych.uiuc.eduDan RothDepartment of Computer ScienceUniversity of Illinoisdanr@uiuc.eduAbstractA fundamental task in sentence compre-hension is to assign semantic roles to sen-tence constituents.
The structure-mappingaccount proposes that children start witha shallow structural analysis of sentences:children treat the number of nouns in thesentence as a cue to its semantic predicate-argument structure, and represent languageexperience in an abstract format that per-mits rapid generalization to new verbs.
Inthis paper, we tested the consequences ofthese representational assumptions via ex-periments with a system for automatic se-mantic role labeling (SRL), trained on asample of child-directed speech.
Whenthe SRL was presented with representa-tions of sentence structure consisting sim-ply of an ordered set of nouns, it mim-icked experimental findings with toddlers,including a striking error found in children.Adding features representing the positionof the verb increased accuracy and elim-inated the error.
We show the SRL sys-tem can use incremental knowledge gainto switch from error-prone noun order fea-tures to a more accurate representation,demonstrating a possible mechanism forthis process in child development.1 IntroductionHow does the child get started in learning to in-terpret sentences?
The structure-mapping viewof early verb and syntax acquisition proposes thatc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.children start with a shallow structural analysis ofsentences: children treat the number of nouns inthe sentence as a cue to its semantic predicate-argument structure (Fisher, 1996), and representlanguage experience in an abstract format that per-mits rapid generalization to new verbs (Gertner etal., 2006).The structure-mapping account makes strongpredictions.
First, as soon as children can identifysome nouns, they should interpret transitive and in-transitive sentences differently, simply by assign-ing a distinct semantic role to each noun in the sen-tence.
Second, language-specific syntactic learn-ing should transfer rapidly to new verbs.
Third,some striking errors of interpretation can occur.In ?Fred and Ginger danced?, an intransitive verbis presented with two nouns.
If children interpretany two-noun sentence as if it were transitive, theyshould be fooled into interpreting the order of twonouns in such conjoined-subject intransitive sen-tences as conveying agent-patient role information.Experiments with young children support thesepredictions.
First, 21-month-olds use the numberof nouns to understand sentences containing newverbs (Yuan et al, 2007).
Second, 21-month-oldsgeneralize what they have learned about Englishtransitive word-order to sentences containing newverbs: Children who heard ?The girl is gorping theboy?
interpreted the girl as an agent and the boy asa patient (Gertner et al, 2006).
Third, 21-month-olds make the predicted error, treating intransitivesentences containing two nouns as if they weretransitive: they interpret the first noun in ?The girland the boy are gorping?
as an agent and the sec-ond as a patient (Gertner and Fisher, 2006).
Thiserror is short-lived.
By 25 months, children addnew features to their representations of sentences,and interpret conjoined-subject intransitives differ-81ently from transitives (Naigles, 1990).These experimental results shed light on whatsyntactic information children might have avail-able for early sentence comprehension, but do notrule out the possibility that children?s early per-formance is based on a more complex underlyingsystem.
In this paper, we tested the consequencesof our representational assumptions by perform-ing experiments with a system for automatic se-mantic role labeling (SRL), whose knowledge ofsentence structure is under our control.
Com-putational models of semantic role labeling learnto identify, for each verb in a sentence, all con-stituents that fill a semantic role, and to determinetheir roles.
We adopt the architecture proposedby Roth and colleagues (Punyakanok et al, 2005),limiting the classifier?s features to a set of lexicalfeatures and shallow structural features suggestedby the structure-mapping account.
Learning abil-ity is measured by the level of SRL accuracy and,more importantly, the types of errors made by thesystem on sentences containing novel verbs.
Test-ing these predictions on the automatic SRL pro-vides us with a demonstration that it is possible tolearn how to correctly assign semantic roles basedonly on these very simple cues.From an NLP perspective this feature study pro-vides evidence for the efficacy of alternative, sim-pler syntactic representations in gaining an initialfoothold on sentence interpretation.
It is clear thathuman learners do not begin interpreting sentencesin possession of full part-of-speech tagging, or fullparse trees.
By building a model that uses shal-low representations of sentences and mimics fea-tures of language development in children, we canexplore the nature of initial representations of syn-tactic structure and build more complex featuresfrom there, further mimicking child development.2 Learning ModelWe trained a simplified SRL classifier (Baby SRL)with sets of features derived from the structure-mapping account.
Our test used novel verbs tomimic sentences presented in experiments withchildren.
Our learning task is similar to the fullSRL task (Carreras and M`arquez, 2004), exceptthat we classify the roles of individual words ratherthan full phrases.
A full automatic SRL system(e.g.
(Punyakanok et al, 2005)) typically involvesmultiple stages to 1) parse the input, 2) identify ar-guments, 3) classify those arguments, and then 4)run inference to make sure the final labeling for thefull sentence does not violate any linguistic con-straints.
Our simplified SRL architecture (BabySRL) essentially replaces the first two steps withheuristics.
Rather than identifying arguments viaa learned classifier with access to a full syntac-tic parse, the Baby SRL treats each noun in thesentence as a candidate argument and assigns asemantic role to it.
A simple heuristic collapsedcompound or sequential nouns to their final noun:an approximation of the head noun of the nounphrase.
For example, ?Mr.
Smith?
was treatedas the single noun ?Smith?.
Other complex nounphrases were not simplified in this way.
Thus,a phrase such as ?the toy on the floor?
would betreated as two separate nouns, ?toy?
and ?floor?.This represents the assumption that young childrenknow ?Mr.
Smith?
is a single name, but they do notknow all the predicating terms that may link mul-tiple nouns into a single noun phrase.
The simpli-fied learning task of the Baby SRL implements akey assumption of the structure-mapping account:that at the start of multiword sentence comprehen-sion children can tell which words in a sentence arenouns (Waxman and Booth, 2001), and treat eachnoun as a candidate argument.Feedback is provided based on annotation inPropbank style: in training, each noun receives therole label of the phrase that noun is part of.
Feed-back is given at the level of the macro-role (agent,patient, etc., labeled A0-A4 for core arguments,and AM-* adjuncts).
We also introduced a NO la-bel for nouns that are not part of any argument.For argument classification we use a linear clas-sifier trained with a regularized perceptron updaterule (Grove and Roth, 2001).
This learning algo-rithm provides a simple and general linear clas-sifier that has been demonstrated to work well inother text classification tasks, and allows us to in-spect the weights of key features to determine theirimportance for classification.
The Baby SRL doesnot use inference for the final classification.
In-stead it classifies every argument independently;thus multiple nouns can have the same role.2.1 TrainingThe training data were samples of parental speechto one child (?Eve?
; (Brown, 1973), availablevia Childes (MacWhinney, 2000)).
We trainedon parental utterances in samples 9 through 20,recorded at child age 21-27 months.
All verb-82containing utterances without symbols indicatinglong pauses or unintelligible words were automat-ically parsed with the Charniak parser (Charniak,1997) and annotated using an existing SRL sys-tem (Punyakanok et al, 2005).
In this initial pass,sentences with parsing errors that misidentified ar-gument boundaries were excluded.
Final role la-bels were hand-corrected using the Propbank an-notation scheme (Kingsbury and Palmer, 2002).The child-directed speech (CDS) training set con-sisted of about 2200 sentences, of which a majorityhad a single verb and two nouns to be labeled1.
Weused the annotated CDS training data to train ourBaby SRL, converting labeled phrases to labelednouns in the manner described above.3 Experimental ResultsTo evaluate the Baby SRL we tested it with sen-tences like those used for the experiments withchildren described above.
All test sentences con-tained a novel verb (?gorp?).
We constructed twotest sentence templates: ?A gorps B?
and ?A and Bgorp?, where A and B were replaced with nounsthat appeared more than twice in training.
Wefilled the A and B slots by sampling nouns thatoccurred roughly equally as the first and secondof two nouns in the training data.
This procedurewas adopted to avoid ?building in?
the predicted er-ror by choosing A and B nouns biased toward anagent-patient interpretation.
For each test sentencetemplate we built a test set of 100 sentences by ran-domly sampling nouns in this fashion.The test sentences with novel verbs ask whetherthe classifier transfers its learning about argumentrole assignment to unseen verbs.
Does it as-sume the first of two nouns in a simple transi-tive sentence (?A gorps B?)
is the agent (A0) andthe second is the patient (A1)?
Does it over-generalize this rule to two-noun intransitives (?Aand B gorp?
), mimicking children?s behavior?
Weused two measures of success, one to assess clas-sification accuracy, and the other to assess thepredicted error.
We used a per argument F1 forclassification accuracy, with F1 based on correctidentification of individual nouns rather than fullphrases.
Here precision is defined as the propor-tion of nouns that were given the correct labelbased on the argument they belong to, and recallis the proportion of complete arguments for which1Corpus available at http://L2R.cs.uiuc.edu/?cogcomp/data.phpsome noun in that argument was correctly labeled.The desired labeling for ?A gorps B?
is A0 for thefirst argument and A1 for the second; for ?A andB gorp?
both arguments should be A0.
To mea-sure predicted errors we also report the proportionof test sentences classified with A0 first and A1second (%A0A1).
This labeling is a correct gener-alization for the novel ?A gorps B?
sentences, butis an overgeneralization for ?A and B gorp.
?3.1 Noun PatternThe basic feature we propose is the noun patternfeature.
We hypothesize that children use the num-ber and order of nouns to represent argument struc-ture.
To encode this we created a feature (NPat-tern) that indicates how many nouns there are inthe sentence and which noun the target is.
For ex-ample, in our two-noun test sentences noun A hasthe feature ?
N?
active indicating that it is the firstnoun of two.
Likewise for B the feature ?N ?
is ac-tive, indicating that it is the second of two nouns.This feature is easy to compute once nouns areidentified, and does not require fine-grained dis-tinctions between types of nouns or any other partof speech.
Table 1 shows the initial feature pro-gression that involves this feature.
The baselinesystem (feature set 1) uses lexical features only:the target noun and the root form of the predicate.We first tested the hypothesis that children usethe NPattern features to distinguish different nounarguments, but only for specific verbs.
The NPat-tern&V features are conjunctions of the target verband the noun pattern, and these are added to theword features to form feature set 2.
Now everyexample has three features active: target noun, tar-get predicate, and a NPattern&V feature indicating?the target is the first of two nouns and the verbis X.?
This feature does not improve results on thenovel ?A gorps B?
test set, or generate the predictederror with the ?A and B gorp?
test set, because theverb-specific NPattern&V features provide no wayto generalize to unseen verbs.We next tested the NPattern feature alone, with-out making it verb-specific (feature set 3).
Thenoun pattern feature was added to the word fea-tures and again each example had three features ac-tive: target noun, target predicate, and the target?snoun-pattern feature (first of two, second of three,etc.).
The abstract NPattern feature allows theBaby SRL to generalize to new verbs: it increasesthe system?s tendency to predict that the first of two83CHILDES WSJUnbiased Noun Choice Biased Noun Choice Biased Noun ChoiceA gorps B A and B gorp A gorps B A and B gorp A gorps B A and B gorpFeatures F1 %A0A1 F1 %A0A1 F1 %A0A1 F1 %A0A1 F1 %A0A1 F1 %A0A11.
Words 0.59 0.38 0.46 0.38 0.80 0.65 0.53 0.65 0.57 0.31 0.37 0.312.
NPattern&V 0.53 0.28 0.54 0.28 0.81 0.67 0.53 0.67 0.56 0.31 0.39 0.313.
NPattern 0.83 0.65 0.33 0.65 0.96 0.92 0.46 0.92 0.67 0.44 0.37 0.444.
NPattern + NPattern&V 0.83 0.65 0.33 0.65 0.95 0.90 0.45 0.90 0.73 0.53 0.44 0.535.
+ VPosition 0.99 0.96 0.98 0.00 1.00 1.00 0.99 0.01 0.94 0.88 0.69 0.39Table 1: Experiments showing the efficacy of Noun Pattern features for determining agent/patient roles insimple two-noun sentences.
The novel verb test sets assess whether the Baby SRL generalizes transitiveargument prediction to unseen verbs in the case of ?A gorps B?
(increasing %A0A1 and thus F1), andovergeneralizes in the case of ?A and B gorp?
(increasing %A0A1, which is an error).
By varying thesampling method for creating the test sentences we can start with a biased or unbiased lexical baseline,demonstrating that the noun pattern features still improve over knowledge that can be contained intypical noun usage.
The simple noun pattern features are still effective at learning this pattern whentrained with more complex Wall Street Journal training data.nouns is A0 and the second of two nouns is A1 forverbs not seen in training.
Feature set 4 includesboth the abstract, non-verb-specific NPattern fea-ture and the verb-specific version.
This feature setpreserves the ability to generalize to unseen verbs;thus the availability of the verb-specific NPatternfeatures during training did not prevent the abstractNPattern features from gathering useful informa-tion.3.2 Lexical Cues for Role-LabelingThus far, the target nouns?
lexical features pro-vided little help in role labeling, allowing us toclearly see the contribution of the proposed sim-ple structural features.
Would our structural fea-tures produce any improvement above a more re-alistic lexical baseline?
We created a new set oftest sentences, sampling the A nouns based on thedistribution of nouns seen as the first of two nounsin training, and the B nouns based on the distri-bution of nouns seen as the second of two nouns.Given this revised sampling of nouns, the words-only baseline is strongly biased toward A0A1 (bi-ased results for feature set 1 in table 1).
This highbaseline reflects a general property of conversa-tion: Lexical choices provide considerable infor-mation about semantic roles.
For example, the 6most common nouns in the Eve corpus are pro-nouns that are strongly biased in their positionsand in their semantic roles (e.g., ?you?, ?it?).
De-spite this high baseline, however, we see the samepattern in the unbiased and biased experiments intable 1.
The addition of the NPattern features (fea-ture set 3) substantially improves performance on?A gorps B?
test sentences, and promotes over-generalization errors on ?A and B gorp?
sentences.3.3 More Complex Training DataFor comparison purposes we also trained the BabySRL on a subset of the Propbank training dataof Wall Street Journal (WSJ) text (Kingsbury andPalmer, 2002).
To approximate the simpler sen-tences of child-directed speech we selected onlythose sentences with 8 or fewer words.
Thisprovided a training set of about 2500 sentences,most with a single verb and two nouns to be la-beled.
The CDS and WSJ data pose similar prob-lems for learning abstract and verb-specific knowl-edge.
However, newspaper text differs from ca-sual speech to children in many ways, includingvocabulary and sentence complexity.
One couldargue that the WSJ corpus presents a worst-casescenario for learning based on shallow representa-tions of sentence structure: Full passive sentencesare more common in written corpora such as theWSJ than in samples of conversational speech, forexample (Roland et al, 2007).
As a result of suchdifferences, two-noun sequences are less likely todisplay an A0-A1 sequence in the WSJ (0.42 A0-A1 in 2-noun sentences) than in the CDS trainingdata (0.67 A0-A1).
The WSJ data provides a moredemanding test of the Baby SRL.We trained the Baby SRL on the WSJ data, andtested it using the biased lexical choices as de-scribed above, sampling A and B nouns for novel-verb test sentences based on the distribution ofnouns seen as the first of two nouns in training, andas the second of two nouns, respectively.
The WSJtraining produced performance strikingly similarto the performance resulting from CDS training(last 4 columns of Table 1).
Even in this morecomplex training set, the addition of the NPattern84features (feature set 3) improves performance on?A gorps B?
test sentences, and promotes over-generalization errors on ?A and B gorp?
sentences.3.4 Tests with Familiar VerbsFeatures Total A0 A1 A2 A41.
Words 0.64 0.83 0.74 0.33 0.002.
NPattern&V 0.67 0.86 0.77 0.45 0.443.
NPattern 0.66 0.87 0.76 0.37 0.224.
NPattern + NPattern&V 0.68 0.87 0.80 0.47 0.445.
+ VPosition 0.70 0.88 0.83 0.50 0.50Table 2: Testing NPattern features on full SRL taskof heldout section 8 of Eve when trained on sec-tions 9 through 20.
Each result column reflects aper argument F1.Learning to interpret sentences depends on bal-ancing abstract and verb-specific structural knowl-edge.
Natural linguistic corpora, including ourCDS training data, have few verbs of very high fre-quency and a long tail of rare verbs.
Frequent verbsoccur with differing argument patterns.
For exam-ple, ?have?
and ?put?
are frequent in the CDS data.?Have?
nearly always occurs in simple transitivesentences that display the canonical word order ofEnglish (e.g., ?I have cookies?).
?Put?, in contrast,tends to appear in non-canonical sentences that donot display an agent-patient ordering, includingimperatives (?Put it on the floor?).
To probe theBaby SRL?s ability to learn the argument-structurepreferences of familiar verbs, we tested it on aheld-out sample of CDS from the same source(Eve sample 8, approximately 234 labeled sen-tences).
Table 2 shows the same feature progres-sion shown previously, with the full SRL test set.The words-only baseline (feature set 1 in Table 2)yields fairly accurate performance, showing thatconsiderable success in role assignment in thesesimple sentences can be achieved based on theargument-role biases of the target nouns and thefamiliar verbs.
Despite this high baseline, how-ever, we still see the benefit of simple structuralfeatures.
Adding verb-specific (feature set 2) orabstract NPattern features (feature set 3) improvesclassification performance, and the combination ofboth verb-specific and abstract NPattern features(feature set 4) yields higher performance than ei-ther alone.
The combination of abstract NPatternfeatures with the verb-specific versions allows theBaby SRL both to generalize to unseen verbs, asseen in earlier sections, and to learn the idiosyn-crasies of known verbs.3.5 Verb PositionThe noun pattern feature results show that theBaby SRL can learn helpful rules for argument-role assignment using only information about thenumber and order of nouns.
It also makes the errorpredicted by the structure-mapping account, anddocumented in children, because it has no way torepresent the difference between the ?A gorps B?and ?A and B gorp?
test sentences.
At some pointthe learner must develop more sophisticated syn-tactic representations that could differentiate thesetwo.
These could include many aspects of the sen-tence, including noun-phrase and verb-phrase mor-phological features, and word-order features.
As afirst step in examining recovery from the predictederror, we focused on word-order features.
We didthis by adding a verb position feature (VPosition)that specifies whether the target noun is before orafter the verb.
Now simple transitive sentences intraining should support the generalization that pre-verbal nouns tend to be agents, and post-verbalnouns tend to be patients.
In testing, the BabySRL?s classification of the ?A gorps B?
and ?A andB gorp?
sentences should diverge.When we add verb position information (fea-ture set 5 in table 1 and 2), performance improvesstill further for transitive sentences, both with bi-ased and unbiased test sentences.
Also, for the firsttime, the A0A1 pattern is predicted less often for?A and B gorp?
sentences.
This error diminishedbecause the classifier was able to use the verb po-sition features to distinguish these from ?A gorpsB?
sentences.Unbiased LexicalA gorps B A and B gorpFeatures F1 %A0A1 F1 %A0A11.
Words 0.59 0.38 0.46 0.383.
NPattern 0.83 0.65 0.33 0.656.
VPosition 0.99 0.95 0.97 0.00Table 3: Verb Position vs. Noun Pattern featuresalone.
Verb position features yield better overallperformance, but do not replicate the error on ?Aand B gorp?
sentences seen with children.Verb position alone provides another simple ab-stract representation of sentence structure, so itmight be proposed as an equally natural initialrepresentation for human learners, rather than thenoun pattern features we proposed.
The VPo-sition features should also support learning andgeneralization of word-order rules for interpret-ing transitive sentences, thus reproducing some of85the data from children that we reviewed above.In table 3 we compared the words-only baseline(set 1), words and NPattern features (set 3), and anew feature set, words and VPosition (set 6).
Interms of correct performance on novel transitiveverbs (?A gorps B?
), the VPosition features out-perform the NPattern features.
This may be partlybecause the same VPosition features are used inall sentences during training, while the NPatternfeatures partition sentences by number of nouns,but is also due to the fact that the verb positionfeatures provide a more sophisticated representa-tion of English sentence structure.
Verb positionfeatures can distinguish transitive sentences fromimperatives containing multiple post-verbal nouns,for example.
Although verb position is ultimatelya more powerful representation of word order forEnglish sentences, it does not accurately reproducea 21-month-old?s performance on all aspects ofthis task.
In particular, the VPosition feature doesnot support the overgeneralization of the A0A1pattern to the ?A and B gorp?
test sentences.
Thissuggests that children?s very early sentence com-prehension is dominated by less sophisticated rep-resentations of word order, akin to the NPatternfeatures we proposed.3.6 Informativeness vs. AvailabilityIn the preceding sections, we modeled increasesin syntactic knowledge by building in more so-phisticated features.
The Baby SRL escaped thepredicted error on two-noun intransitive sentenceswhen given access to features reflecting the posi-tion of the target noun relative to the verb.
Thisimposed sequence of features is useful as a startingpoint, but a more satisfying approach would be touse the Baby SRL to explore possible reasons whyNPattern features might dominate early in acquisi-tion, even though VPosition features are ultimatelymore useful for English.In theory, a feature might be unavailable early inacquisition because of its computational complex-ity.
For example, lexical features are presumablyless complex than relative position features such asNPattern and VPosition.
In practice, features canalso be unavailable at first because of an informa-tional lack.
Here we suggest that NPattern featuresmight dominate VPosition features early in acqui-sition because the early lexicon is dominated bynouns, and it is easier to compute position relativeto a known word than to an unknown word.
Manystudies have shown that children?s early vocabu-lary is dominated by names for objects and peo-ple (Gentner and Boroditsky, 2001).-0.500.511.522.533.50  5000  10000  15000  200000306090120150180A0-A1#VerbsExamples_N_VKnown Verbs(a) Verb threshold = 5-0.500.511.522.533.50  5000  10000  15000  200000306090120150180A0-A1#VerbsExamples_N_VKnown Verbs(b) Verb threshold = 20-0.500.511.522.533.50  5000  10000  15000  200000306090120150180A0-A1#VerbsExamples_N_VKnown Verbs(c) Verb threshold = 20, +verb-specific featuresFigure 1: Testing the consequences of the assump-tion that Verb Position features are only active forfamiliar verbs.
The figure plots the bias of the fea-tures ?
N?
and ?
V?
to predict A0 over A1, as thedifference between the weights of these connec-tions in the learned network.
Verb position fea-tures win out over noun pattern features as theverb vocabulary grows.
Varying the verb familiar-ity threshold ((a) vs. (b)) and the presence versusabsence of verb-specific versions of the structuralfeatures ((b) vs. (c)) affects how quickly the verbposition features become dominant.To test the consequences of this proposed infor-86mational bottleneck on the relative weighting ofNPattern and VPosition features during training,we modified the Baby SRL?s training proceduresuch that NPattern features were always active, butVPosition features were active during training onlywhen the verb in the current example had been en-countered a critical number of times.
This repre-sents the assumption that the child can recognizewhich words in the sentence are nouns, based onlexical familiarity or morphological context (Wax-man and Booth, 2001), but is less likely to be ableto represent position relative to the verb withoutknowing the verb well.Figure 1 shows the tendency of the NPattern fea-ture ?
N?
(first of two nouns) and the VPositionfeature ?
V?
(pre-verbal noun) to predict the roleA0 as opposed to A1 as the difference betweenthe weights of these connections in the learned net-work.
Figure 1(a) shows the results when VPosi-tion features were active whenever the target verbhad occurred at least 5 times; in Figure 1(b) thethreshold for verb familiarity was 20.
In both fig-ures we see that the VPosition features win outover the NPattern features as the verb vocabularygrows.
Varying the degree of verb familiarity re-quired to accurately represent VPosition featuresaffects how quickly the VPosition features winout (compare Figures 1(a) and 1(b)).
Figure 1(c)shows the same analysis with a threshold of 20,but with verb-specific as well as abstract versionsof the NPattern and the VPosition features.
In thisprocedure, every example started with three fea-tures: target noun, target predicate, NPattern, andif the verb was known, added NPattern&V, VPo-sition, and VPosition&V.
Comparing Figures 1(b)and 1(c), we see that the addition of verb-specificversions of the structural features also affects therate at which the VPosition features come to dom-inate the NPattern features.Thus, in training the VPosition features becomedominant as the SRL learns to recognize moreverbs.
However, the VPosition features are inac-tive when the Baby SRL encounters the novel-verbtest sentences.
Since the NPattern features are ac-tive in test, the system generates the predicted erroruntil the bias of the NPattern features reaches 0.Note in figure 1(c) that when verb-specific struc-tural features were added, the Baby SRL neverlearned to entirely discount the NPattern featureswithin the range of training provided.
This resultis reminiscent of suggestions in the psycholinguis-00.20.40.60.810  0.2  0.4  0.6  0.8  1A0A1NoiseWords+NPattern+NPattern&V+VPositionFigure 2: Testing the ability of simple featuresto cope with varying amounts of noisy feedback.Even with noisy feedback, the noun pattern fea-tures support learning and generalization to newverbs of a simple agent-patient template for un-derstanding transitive sentences.
These results arelower than those found in table 1 due to slightlydifferent training assumptions.tics literature that shallow representations of syn-tax persist in the adult parser, alongside more so-phisticated representations (e.g., (Ferreira, 2003)).3.7 Noisy TrainingSo far, the Baby SRL has only been trained withperfect feedback.
Theories of human language ac-quisition assume that learning to understand sen-tences is naturally a partially-supervised task: thechild uses existing knowledge of words and syntaxto assign a meaning to a sentence; the appropriate-ness of this meaning for the referential context pro-vides the feedback (e.g., (Pinker, 1989)).
But thisfeedback must be noisy.
Referential scenes pro-vide useful but often ambiguous information aboutthe semantic roles of sentence participants.
For ex-ample, a participant could be construed as an agentof fleeing or as a patient being chased.
In a finalset of experiments, we examined the generaliza-tion abilities of the Baby SRL as a function of theintegrity of semantic feedback.We provided noisy semantic-role feedback dur-ing training by giving a randomly-selected argu-ment label on 0 to 100% of examples.
Followingthis training, we tested with the ?A gorps B?
testsentences, using the unbiased noun choices.As shown in Figure 2, feature sets includingNPattern or VPosition features yield reasonableperformance on the novel verb test sentences up to50% noise, and promote an A0-A1 sequence over87the words-only baseline even at higher noise lev-els.
Thus the proposed simple structural featuresare robust to noisy feedback.4 ConclusionThe simplified SRL classifier mimicked experi-mental results with toddlers.
We structured thelearning task to ask whether shallow representa-tions of sentence structure provided a useful ini-tial representation for learning to interpret sen-tences.
Given representations of the number andorder of nouns in the sentence (noun pattern fea-tures), the Baby SRL learned to classify the firstof two nouns as an agent and the second as a pa-tient.
When provided with both verb-general andverb-specific noun pattern features, the Baby SRLlearned to balance verb-specific and abstract syn-tactic knowledge.
By treating each noun as anargument, it also reproduced the errors childrenmake.
Crucially, verb-position features improvedperformance when added to the noun-pattern fea-ture, but when presented alone failed to producethe error found with toddlers.
We believe thatour model can be naturally extended to supportthe case in which the arguments are noun phrasesrather than single noun words and this extension isone of the first steps we will explore next.AcknowledgmentsWe would like to thank our annotators, espe-cially Yuancheng Tu.
This research is supportedby NSF grant BCS-0620257 and NIH grant R01-HD054448.ReferencesBrown, R. 1973.
A First Language.
Harvard Univer-sity Press, Cambridge, MA.Carreras, X. and L. M`arquez.
2004.
Introduction tothe CoNLL-2004 shared tasks: Semantic role label-ing.
In Proceedings of CoNLL-2004, pages 89?97.Boston, MA, USA.Charniak, E. 1997.
Statistical parsing with a context-free grammar and word statistics.
In Proc.
NationalConference on Artificial Intelligence.Ferreira, F. 2003.
The misinterpretation of noncanoni-cal sentences.
Cognitive Psychology, 47:164?203.Fisher, C. 1996.
Structural limits on verb mapping:The role of analogy in children?s interpretation ofsentences.
Cognitive Psychology, 31:41?81.Gentner, D. and L. Boroditsky.
2001.
Individuation,relativity and early word learning.
In Bowerman, M.and S. C. Levinson, editors, Language acquisitionand conceptual development, pages 215?256.
Cam-bridge University Press, New York.Gertner, Y. and C. Fisher.
2006.
Predicted errors inearly verb learning.
In 31st Annual Boston Univer-sity Conference on Language Development.Gertner, Y., C. Fisher, and J. Eisengart.
2006.
Learningwords and rules: Abstract knowledge of word orderin early sentence comprehension.
Psychological Sci-ence, 17:684?691.Grove, A. and D. Roth.
2001.
Linear concepts andhidden variables.
Machine Learning, 42(1/2):123?141.Kingsbury, P. and M. Palmer.
2002.
From Treebank toPropBank.
In Proceedings of LREC-2002, Spain.MacWhinney, B.
2000.
The CHILDES project: Toolsfor analyzing talk.
Third Edition.
Lawrence ElrbaumAssociates, Mahwah, NJ.Naigles, L. R. 1990.
Children use syntax to learn verbmeanings.
Journal of Child Language, 17:357?374.Pinker, S. 1989.
Learnability and Cognition.
Cam-bridge: MIT Press.Punyakanok, V., D. Roth, and W. Yih.
2005.
The ne-cessity of syntactic parsing for semantic role label-ing.
In Proc.
of the International Joint Conferenceon Artificial Intelligence (IJCAI), pages 1117?1123.Roland, D., F. Dick, and J. L. Elman.
2007.
Fre-quency of basic english grammatical structures: Acorpus analysis.
Journal of Memory and Language,57:348?379.Waxman, S. R. and A. Booth.
2001.
Seeing pinkelephants: Fourteen-month-olds?s interpretations ofnovel nouns and adjectives.
Cognitive Psychology,43:217?242.Yuan, S., C. Fisher, Y. Gertner, and J. Snedeker.
2007.Participants are more than physical bodies: 21-month-olds assign relational meaning to novel tran-sitive verbs.
In Biennial Meeting of the Society forResearch in Child Development, Boston, MA.88
