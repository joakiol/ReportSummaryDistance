Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 947?954, Vancouver, October 2005. c?2005 Association for Computational LinguisticsSEARCHING THE AUDIO NOTEBOOK:KEYWORD SEARCH IN RECORDED CONVERSATIONSPeng Yu, Kaijiang Chen, Lie Lu, and Frank SeideMicrosoft Research Asia, 5F Beijing Sigma Center, 49 Zhichun Rd., 100080 Beijing, P.R.C.
{rogeryu,kaijchen,llu,fseide}@microsoft.comAbstractMIT?s Audio Notebook added great value to thenote-taking process by retaining audio record-ings, e.g.
during lectures or interviews.
The keywas to provide users ways to quickly and easilyaccess portions of interest in a recording.
Sev-eral non-speech-recognition based techniqueswere employed.
In this paper we present asystem to search directly the audio record-ings by key phrases.
We have identified theuser requirements as accurate ranking of phrasematches, domain independence, and reasonableresponse time.
We address these requirementsby a hybrid word/phoneme search in lattices,and a supporting indexing scheme.
We will in-troduce the ranking criterion, a unified hybridposterior-lattice representation, and the index-ing algorithm for hybrid lattices.
We presentresults for five different recording sets, includ-ing meetings, telephone conversations, and in-terviews.
Our results show an average searchaccuracy of 84%, which is dramatically betterthan a direct search in speech recognition tran-scripts (less than 40% search accuracy).1 IntroductionLisa Stifelman proposed in her thesis the idea of the?Audio Notebook,?
where audio recordings of lecturesand interviews are retained along with the notes (Stifel-man, 1997).
She has shown that the audio recordings arevaluable to users if portions of interest can be accessedquickly and easily.Stifelman explored various techniques for this, includ-ing user-activity based techniques (most noteworthy time-stamping notes so they can serve as an index into therecording) and content-based ones (signal processing foraccelerated playback, ?snap-to-grid?
(=phrase boundary)based on prosodic cues).
The latter are intended for sit-uations where the former fail, e.g.
when the user has notime for taking notes, does not wish to pay attention to it,or cannot keep up with complex subject matter, and as aconsequence the audio is left without index.
In this pa-per, we investigate technologies for searching the spokencontent of the audio recording.Several approaches have been reported in the litera-ture for the problem of indexing spoken words in au-dio recordings.
The TREC (Text REtrieval Conference)Spoken-Document Retrieval (SDR) track has fostered re-search on audio-retrieval of broadcast-news clips.
MostTREC benchmarking systems use broadcast-news recog-nizers to generate approximate transcripts, and apply text-based information retrieval to these.
They achieve re-trieval accuracy similar to using human reference tran-scripts, and ad-hoc retrieval for broadcast news is consid-ered a ?solved problem?
(Garofolo, 2000).
Noteworthyare the rather low word-error rates (20%) in the TRECevaluations, and that recognition errors did not lead tocatastrophic failures due to redundancy of news segmentsand queries.However, in our scenario, requirements are rather dif-ferent.
First, word-error rates are much higher (40-60%).
Directly searching such inaccurate speech recog-nition transcripts suffers from a poor recall.
Second, un-like broadcast-news material, user recordings of conver-sations will not be limited to a few specific domains.
Thisnot only poses difficulties for obtaining domain-specifictraining data, but also implies an unlimited vocabulary ofquery phrases users want to use.
Third, audio recordingswill accumulate.
When the audio database grows to hun-dreds or even thousands of hours, a reasonable responsetime is still needed.A successful way to deal with high word error rates isthe use of recognition alternates (lattices).
For example,(Seide and Yu, 2004; Yu and Seide, 2004) reports a sub-stantial 50% improvement of FOM (Figure Of Merit) fora word-spotting task in voicemails.
Improvements fromusing lattices were also reported by (Saraclar and Sproat,2004) and (Chelba and Acero, 2005).To address the problem of domain independence, asubword-based approach is needed.
In (Logan, 2002)the authors address the problem by indexing phonetic orword-fragment based transcriptions.
Similar approaches,e.g.
using overlapping M -grams of phonemes, are dis-cussed in (Scha?uble, 1995) and (Ng, 2000).
(Jamesand Young, 1994) introduces the approach of searchingphoneme lattices.
(Clements, 2001) proposes a similaridea called ?phonetic search track.?
In previous work(Seide and Yu, 2004), promising results were obtainedwith phonetic lattice search in voicemails.
In (Yu and947wordrecognizerphoneticrecognizerposteriorconversion & mergingwordlattice latticeindexerlatticestoreindexlookupresult list linear search queryphonemelatticeaudiostreampromisingsegmentsrankerindexingsearchhit listhybridlatticequeryinvertedindex letter to soundword stringphonemestringhybridlatticeFigure 1: System architecture.Seide, 2004), it was found that even better re ult can beachieved by combining a phonetic search with a word-level search.For the third problem, quick response time is com-monly achieved by indexing techniques.
However, inthe context of phonetic lattice search, the concept of ?in-dexing?
becomes a non-trivial problem, because due tothe unknown-word nature, we need to deal with an openset of index keys.
(Saraclar and Sproat, 2004) proposesto store the individual lattice arcs (inverting the lattice).
(Allauzen et al, 2004) introduces a general indexationframework by indexing expected term frequencies (?ex-pected counts?)
instead of each individual keyword oc-currence or lattice arcs.
In (Yu et al, 2005), a similar ideaof indexing expected term frequencies is proposed, sug-gesting to approximate expected term frequencies by M -gram phoneme language models estimated on segmentsof audio.In this paper, we combine previous work on pho-netic lattice search, hybrid search and lattice indexinginto a real system for searching recorded conversationsthat achieves high accuracy and can handle hundreds ofhours of audio.
The main contributions of this paperare: a real system for searching conversational speech, anovel method for combining phoneme and word lattices,and experimental results for searching recorded conver-sations.The paper is organized as follows.
Section 2 gives anoverview of the system.
Section 3 introduces the over-all criterion, based on which the system is developed,Section 4 introduces our implementation for a hybridword/phoneme search system, and Section 5 discusses thelattice indexing mechanism.
Section 6 presents the exper-imental results, and Section 7 concludes.2 A System For Searching ConversationsA system for searching the spoken content of recordedconversations has several distinct properties.
Users aresearching their own meetings, so most searches will beknown-item searches with at most a few correct hits in thearchive.
Users will often search for specific phrases thatthey remember, possibly with boolean operators.
Rele-vance weighting of individual query terms is less of anissue in this scenario.We identified three user requirements:?
high recall and accurate ranking of phrase matches;?
domain independence ?
it should work for any topic,ideally without need to adapt vocabularies or lan-guage models;?
reasonable response time ?
a few seconds at most,independent of the size of the conversation archive.We address them as follows.
First, to increase recallwe search recognition alternates based on lattices.
Lat-tice oracle word-error rates1 are significantly lower thanword-error rates of the best path.
For example, (Chelbaand Acero, 2005) reports a lattice oracle error rate of 22%for lecture recordings at a top-1 word-error rate of 45%2.To utilize recognizer scores in the lattices, we formulat-ing the ranking problem as one of risk minimization andderive that keyword hits should be ranked by their word(phrase) posterior probabilities.Second, domain independence is achieved by combin-ing large-vocabulary recognition with a phonetic search.This helps especially for proper names and specializedterminology, which are often either missing in the vocab-ulary or not well-predicted by the language model.Third, to achieve quick response time, we use an M -gram based indexing approach.
It has two stages, wherethe first stage is a fast index lookup to create a short-list ofcandidate lattices.
In the second stage, a detailed latticematch is applied to the lattices in the short-list.
We callthe second stage linear search because search time growslinearly with the duration of the lattices searched.1The ?oracle word-error rate?
of a lattice is the word errorrate of the path through the lattice that has the least errors.2Note that this comparison was for a reasonably well-tunedrecognizer setup.
Any arbitrary lattice oracle error rate can beobtained by adjusting the recognizer?s pruning setup and in-vesting enough computation time (plus possibly adapting thesearch-space organization).948The resulting system architecture is shown in Fig.
1.
Inthe following three sections, we will discuss our solutionsin these three aspects in details respectively.3 Ranking CriterionFor ranking search results according to ?relevance?
to theuser?s query, several relevance measures have been pro-posed in the text-retrieval literature.
The key element ofthese measures is weighting the contribution of individualkeywords to the relevance-ranking score.
Unfortunately,text ranking criteria are not directly applicable to retrievalof speech because recognition alternates and confidencescores are not considered.Luckily, this is less of an issue in our known-item stylesearch, because the simplest of relevance measures canbe used: A search hit is assumed relevant if the queryphrase was indeed said there (and fulfills optional booleanconstraints), and it is not relevant otherwise.This simple relevance measure, combined with a vari-ant of the probability ranking principle (Robertson,1977), leads to a system where phrase hits are ranked bytheir phrase posterior probability.
This is derived througha Bayes-risk minimizing approach as follows:1.
Let the relevance be R(Q,hiti) of a returned audiohit ?
hiti to a user?s query Q formally defined is 1(match) if the hit is an occurrence of the query termwith time boundaries (thitis , thitie ), or 0 if not.2.
The user expects the system to return a list of audiohits, ranked such that the accumulative relevance ofthe top n hits (hit1...hitn), averaged over a range ofn = 1...nmax, is maximal:1nmaxnmax?n=1n?i=1R(Q,hiti) != max .
(1)Note that this is closely related to popular word-spotting metrics, such as the NIST (National Insti-tute of Standards & Technology) Figure Of Merit.To the retrieval system, the true transcription of eachaudio file is unknown, so it must maximize Eq.
(1) in thesense of an expected valueEWT |O{1nmaxnmax?n=1n?i=1RWT (Q,hiti)}!= max,where O denotes the totality of all audio files (Ofor observation), W = (w1, w2, ..., wN ) a hypothe-sized transcription of the entire collection, and T =(t1, t2, ..., tN+1) the associated time boundaries on ashared collection-wide time axis.RWT (?)
shall be relevance w.r.t.
the hypothesized tran-scription and alignment.
The expected value is takenw.r.t.
the posterior probability distribution P (WT |O)provided by our speech recognizer in the form of scoredlattices.
It is easy to see that this expression is max-imal if the hits are ranked by their expected relevanceEWT |O{RWT (Q,hiti)}.
In our definition of relevance,RWT (Q,hiti) is written asRWT (Q,hiti) =??????
?1 ?k, l : tk = thitis?tk+l = thitie?wk, ..., wk+l?1 = Q0 otherwiseand the expected relevance is computed asEWT |O{RWT (Q,hiti)} =?WTRWT (Q,hiti)P (WT |O)= P (?, thitis , Q, thitie , ?|O)withP (?, ts, Q, te, ?|O) =?WT :?k,l:tk=ts?tk+l=te?wk,...,wk+l?1=QP (WT |O).
(2)For single-word queries, this is the well-known word pos-terior probability (Wessel et al, 2000; Evermann et al,2000).
To cover multi-label phrase queries, we will call itphrase posterior probability.The formalism in this section is applicable to all sortsof units, such as fragments, syllables, or words.
The tran-scription W and its units wk, as well as the query stringQ, should be understood in this sense.
For a regular word-level search, W and Q are just strings of words In the con-text of phonetic search, W and Q are strings of phonemes.For simplicity of notation, we have excluded the issue ofmultiple pronunciations of a word.
Eq.
(2) can be triviallyextended by summing up over all alternative pronuncia-tions of the query.
And in a hybrid search, there wouldbe multiple representations of the query, which are just aspronunciation variants.4 Word/Phoneme Hybrid SearchFor a combined word and phoneme based search, twoproblems need to be considered:?
Recognizer configuration.
While established solu-tions exist for word-lattice generation, what needsto be done for generating high-quality phoneme lat-tices??
How should word and phoneme lattices be jointlyrepresented for the purpose of search, and howshould they be searched?4.1 Speech Recognition4.1.1 Large-Vocabulary RecognitionWord lattices are generated by a common speaker-independent large-vocabulary recognizer.
Because thespeaking style of conversations is very different from, say,949your average speech dictation system, specialized acous-tic models are used.
These are trained on conversationalspeech to match the speaking style.
The vocabulary andthe trigram language model are designed to cover a broadrange of topics.The drawback of large-vocabulary recognition is, ofcourse, that it is infeasible to have the vocabulary coverall possible keywords that a user may use, particularlyproper names and specialized terminology.One way to address this out-of-vocabulary problem isto mine the user?s documents or e-mails to adapt the rec-ognizer?s vocabulary.
While this is workable for somescenarios, it is not a good solution e.g.
when new wordsare frequently introduced in the conversations themselvesrather than preceding written conversations, where thespelling of a new word is not obvious and thus inconsis-tent, or when documents with related documents are noteasily available on the user?s hard disk but would have tobe specifically gathered by the user.A second problem is that the performance of state-of-the-art speech recognition relies heavily on a well-traineddomain-matched language model.
Mining user data canonly yield a comparably small amount of training data.Adapting a language model with it would barely yield arobust language model for newly learned words, and theirusage style may differ in conversational speech.For the above reasons, we decided not to attempt toadapt vocabulary and language model.
Instead, we usea fixed broad-domain vocabulary and language modelfor large-vocabulary recognition, and augment this sys-tem with maintenance-free phonetic search to cover newwords and mismatched domains.4.1.2 Phonetic RecognitionThe simplest phonetic recognizer is a regular recog-nizer with the vocabulary replaced by the list of phone-mes of the language, and the language model replaced bya phoneme M -gram.
However, such phonetic languagemodel is much weaker than a word language model.
Thisresults in poor accuracy and inefficient search.Instead, our recognizer uses ?phonetic word frag-ments?
(groups of phonemes similar to syllables or half-syllables) as its vocabulary and in the language model.This provides phonotactic constraints for efficient decod-ing and accurate phoneme-boundary decisions, while re-maining independent of any specific vocabulary.
A setof about 600 fragments was automatically derived fromthe language-model training set by a bottom-up group-ing procedure (Klakow, 1998; Ng, 2000; Seide and Yu,2004).
Example fragments are /-k-ih-ng/ (the syllable -king), /ih-n-t-ax-r-/ (inter-), and /ih-z/ (the word is).With this, lattices are generated using the commonViterbi decoder with word-pair approximation (Schwartzet al, 1994; Ortmanns et al, 1996).
The decoder has beenmodified to keep track of individual phoneme boundariesand scores.
These are recorded in the lattices, whilefragment-boundary information is discarded.
This way,phoneme lattices are generated.In the results section we will see that, even with a well-trained domain-matching word-level language model,searching phoneme lattices can yield search accuraciescomparable with word-level search, and that the best per-formance is achieved by combining both into a hybridword/phoneme system.4.2 Unified Hybrid Lattice RepresentationCombining word and phonetic search is desirable becausethey are complementary: Word-based search yields bet-ter precision, but has a recall issue for unknown and rarewords, while phonetic search has very good recall but suf-fers from poor precision especially for short words.Combining the two is not trivial.
Several strategies arediscussed in (Yu and Seide, 2004), including using a hy-brid recognizer, combining lattices from two separate rec-ognizers, and combining the results of two separate sys-tems.
Both hybrid recognizer configuration and latticecombination turned out difficult because of the differentdynamic range of scores in word and phonetic paths.We found it beneficial to convert both lattices intoposterior-based representations called posterior latticesfirst, which are then merged into a hybrid posterior lat-tice.
Search is performed in a hybrid lattice in a unifiedmanner using both phonetic and word representations as?alternative pronunciation?
of the query, and summing upthe resulting phrase posteriors.Posterior lattices are like regular lattices, except thatthey do not store acoustic likelihoods, language modelprobabilities, and precomputed forward/backward proba-bilities, but arc and node posteriors.
An arc?s posterioris the probability that the arc (with its associated wordor phoneme hypothesis) lies on the correct path, while anode posterior is the probability that the correct path con-nects two word/phoneme hypotheses through this node.In our actual system, a node is only associated with apoint in time, and the node posterior is the probabilityof having a word or phoneme boundary at its associatedtime point.The inclusion of node posteriors, which to our knowl-edge is a novel contribution of this paper, makes an exactcomputation of phrase posteriors from posterior latticespossible.
In the following we will explain this in detail.4.2.1 Arc and Node PosteriorsA lattice L = (N ,A, nstart, nend) is a directed acyclicgraph (DAG) with N being the set of nodes, A is theset of arcs, and nstart, nend ?
N being the unique ini-tial and unique final node, respectively.
Nodes representtimes and possibly context conditions, while arcs repre-sent word or phoneme hypotheses.3Each node n ?
N has an associated time t[n] and pos-sibly an acoustic or language-model context condition.Arcs are 4-tuples a = (S[a], E[a], I[a], w[a]).
S[a], E[a]3Alternative definitions of lattices are possible, e.g.
nodesrepresenting words and arcs representing word transitions.950?
N denote the start and end node of the arc.
I[a] isthe arc label4, which is either a word (in word lattices)or a phoneme (in phonetic lattices).
Last, w[a] shall bea weight assigned to the arc by the recognizer.
Specifi-cally, w[a] = pac(a)1/?
?PLM(a) with acoustic likelihoodpac(a), language model probability PLM, and language-model weight ?.In addition, we define paths pi = (a1, ?
?
?
, aK) assequences of connected arcs.
We use the symbols S,E, I , and w for paths as well to represent the respec-tive properties for entire paths, i.e.
the path start nodeS[pi] = S[a1], path end node E[pi] = E[aK ], path la-bel sequence I[pi] = (I[a1], ?
?
?
, I[aK ]), and total pathweight w[pi] = ?Kk=1 w[ak].Finally, we define ?
(n1, n2) as the entirety of all pathsthat start at node n1 and end in node n2: ?
(n1, n2) ={pi|S[pi] = n1 ?
E[pi] = n2}.With this, the phrase posteriors defined in Eq.
2 can bewritten as follows.In the simplest case, Q is a single word token.
Then,the phrase posterior is just the word posterior and, asshown in e.g.
(Wessel et al, 2000) or (Evermann et al,2000), can be computed asP (?, ts, Q, te, ?|O) =?pi=(a1,???
,aK )??(nstart,nend):?l:[S[al]]=ts?t[E[al]]=te?I[al]=Qw[pi]?pi??
(nstart,nend)w[pi]=?a?A:t[S[a]]=ts?t[E[a]]=te?I[a]=QParc[a] (3)with Parc[a] being the arc posterior defined asParc[a] =?S[a] ?
w[a] ?
?E[a]?nendwith the forward/backward probabilities ?n and ?n de-fined as:?n =?pi??
(nstart,n)w[pi]?n =?pi??(n,nend)w[pi].
?n and ?n can conveniently be computed from the wordlattices by the well-known forward/backward recursion:?n ={ 1.0 n = nstart?a:E[a]=n?S[a] ?
w[a] otherwise?n ={ 1.0 n = nend?a:S[a]=nw[a] ?
?E[a] otherwise.4Lattices are often interpreted as weighted finite-state accep-tors, where the arc labels are the input symbols, hence the sym-bol I .Now, in the general case of multi-label queries, the phraseposterior can be computed asP (?, ts, Q, te, ?|O)=?pi=(a1,???
,aK ):t[S[pi]]=ts?t[E[pi]]=te?I[pi]=QParc[a1] ?
?
?Parc[aK ]Pnode[S[a2]] ?
?
?Pnode[S[aK ]]with Pnode[n], the node posterior5, defined asPnode[n] = ?n ?
?n?nend.
(4)4.2.2 Advantages of Posterior LatticesThe posterior-lattice representation has several advan-tages over traditional lattices.
First, lattice storage is re-duced because only one value (node posterior) needs to bestored per node instead of two (?, ?)6.
Second, node andarc posteriors have a smaller and similar dynamic rangethan ?n, ?n, and w[a], which is beneficial when the val-ues should be stored with a small number of bits.Further, for the case of word-based search, the summa-tion in Eq.
3 can also be precomputed by merging all lat-tice nodes that carry the same time label, and merging thecorresponding arcs by summing up their arc posteriors.In such a ?pinched?
lattice, word posteriors for single-label queries can now be looked up directly.
However,posteriors for multi-label strings cannot be computed pre-cisely anymore.
Our experiments have shown that the im-pact on ranking accuracy caused by this approximation isneglectable.
Unfortunately, we have also found that thesame is not true for phonetic search.The most important advantage of posterior lattices forour system is that they provide a way of combining theword and phoneme lattices into a single structure ?
bysimply merging their start nodes and their end nodes.
Thisallows to implement hybrid queries in a single unifiedsearch, treating the phonetic and the word-based repre-sentation of the query as alternative pronunciations.5 Lattice IndexingSearching lattices is time-consuming.
It is not feasible tosearch large amounts of audio.
To deal with hundreds oreven thousands of hours of audio, we need some form ofinverted indexing mechanism.This is comparably straight-forward when indexingtext.
It is also not difficult for indexing word lattices.
Inboth case, the set of words to index is known.
However,indexing phoneme lattices is very different, because the-oretically any phoneme string could be an indexing item.5Again, mind that in our lattice formulation word/phonemehypotheses are represented by arcs, while nodes just representconnection points.
The node posterior is the probability that thecorrect path passes through a connection point.6Note, however, that storage for the traditional lattice canalso be reduced to a single number per node by weight push-ing (Saraclar and Sproat, 2004), using an algorithm that is verysimilar to the forward/backward procedure.951We address this by our M -gram lattice-indexingscheme.
It was originally designed for phoneme lattices,but can be ?
and is actually ?
used in our system for in-dexing word lattices.First, audio files are clipped into homogeneous seg-ments.
For an audio segment i, we define the expectedterm frequency (ETF) of a query string Q as summationof phrase posteriors of all hits in this segment:ETFi(Q) =?
?ts,teP (?, ts, Q, te, ?|Oi)=?pi?
?i:I[pi]=Qp[pi]with ?i being the set of all paths of segment i.At indexing time, ETFs of a list of M -grams for eachsegment are calculated.
They are stored in an invertedstructure that allows retrieval by M -gram.In search time, the ETFs of the query string are es-timated by the so-called ?M -gram approximation?.
Inorder to explain this concept, we need to first introduceP (Q|Oi) ?
the probability of observing query string Q atany word boundary in the recording Oi.
P (Q|Oi) has arelationship with ETF asETFi(Q) = N?i ?
P (Q|Oi)with N?i being the expected number of words in the seg-ment i.
It can also be computed asN?i =?n?Nip[n],where Ni is the node set for segment i.Like the M -gram approximation in language-modeltheory, we approximate P (Q|Oi) asP (Q|Oi) ?
P?
(Q|Oi)=l?k=1P?
(qk|qk?M+1, ?
?
?
, qk?1, Oi),while the right-hand items can be calculated from M -gram ETFs:P?
(qk|qk?M+1, ?
?
?
, qk?1, Oi)= ETFi(qk?M+1, ?
?
?
, qk)ETFi(qk?M+1, ?
?
?
, qk?1) .The actual implementation uses only M -grams extractedfrom a large background dictionary, with a simple backoffstrategy for unseen M -grams, see (Yu et al, 2005) fordetails.The resulting index is used in a two stage-search man-ner: The index itself is only used as the first stage to de-termine a short-list of promising segments that may con-tain the query.
The second stage involves a linear latticesearch to get final results.Table 1: Test corpus summary.test set dura- #seg- keyword settion ments (incl.
OOV)ICSI meetings 2.0h 429 1878 (96)SWBD eval2000 3.6h 742 2420 (215)SWBD rt03s 6.3h 1298 2325 (236)interviews (phone) 1.1h 267 1057 (49)interviews (lapel) 1.0h 244 1629 (107)6 Results6.1 SetupWe have evaluated our system on five different corpora ofrecorded conversations:?
one meeting corpus (NIST ?RT04S?
developmentdata set, ICSI portion, (NIST, 2000-2004))?
two eval sets from the switchboard (SWBD) datacollection (?eval 2000?
and ?RT03S?, (NIST, 2000-2004))?
two in-house sets of interview recordings of aboutone hour each, one recorded over the telephone, andone using a single microphone mounted in the inter-viewee?s lapel.For each data set, a keyword list was selected by anautomatic procedure (Seide and Yu, 2004).
Words andmulti-word phrases were selected from the reference tran-scriptions if they occurred in at most two segments.
Ex-ample keywords are overseas, olympics, and ?automatedaccounting system?.
For the purpose of evaluation, thosedata sets are cut into segments of about 15 seconds each.The size of the corpora, their number of segments, andthe size of the selected keyword set are given in Table 1.The acoustic model we used is trained on 309h of theSwitchboard corpus (SWBD-1).
The LVCSR languagemodel was trained on the transcriptions of the Switch-board training set, the ICSI-meeting training set, and theLDC Broadcast News 96 and 97 training sets.
No ded-icated training data was available for the in-house inter-view recordings.
The recognition dictionary has 51388words.
The phonetic language model was trained on thephonetic version of the transcriptions of SWBD-1 andBroadcast News 96 plus about 87000 background dictio-nary entries, a total of 11.8 million phoneme tokens.To measure the search accuracy, we use the ?FigureOf Merit?
(FOM) metric defined by NIST for word-spotting evaluations.
In its original form, it is the aver-age of detection/false-alarm curve taken over the range[0..10] false alarms per hour per keyword.
Because man-ual word-level alignments of our test sets were not avail-able, we modified the FOM such that a correct hit is a15-second segment that contains the key phrase.Besides FOM, we use a second metric ?
?Top Hit Pre-cision?
(THP), defined as the correct rate of the bestranked hit.
If no hit is returned for an existing query term,it is counted as an error.
Both of these metrics are relevantmeasures in our known-item search.952Table 2: Baseline transcription word-error rates (WER)as well as precision (P), recall (R), FOM and THP forsearching the transcript.test set WER P R FOM THP[%] [%] [%] [%] [%]ICSI meetings 44.1 80.6 43.8 43.6 43.6SWBD eval2000 39.0 79.6 41.1 41.1 41.1SWBD rt03s 45.2 72.6 36.3 36.3 36.0interviews (phone) 57.7 68.8 31.6 29.3 31.3interviews (lapel) 62.8 80.1 32.0 30.2 32.1average 49.8 76.3 37.0 36.1 36.8Table 3: Comparison of search accuracy for word,phoneme, and hybrid lattices.test set word phoneme hybridFigure Of Merit (FOM) [%]ICSI meetings 72.1 81.2 88.2SWBD eval2000 71.3 80.4 87.3SWBD rt03s 66.4 76.9 84.2interviews (phone) 60.6 73.7 83.3interviews (lapel) 59.0 70.2 77.7average 65.9 76.5 84.1INV words only 69.4 77.0 84.7OOV words only 0 73.8 73.8Top Hit Precision (THP) [%]ICSI meetings 67.2 65.0 78.7SWBD eval2000 67.1 63.6 77.9SWBD rt03s 59.6 59.1 71.7interviews (phone) 55.7 64.4 73.1interviews (lapel) 55.6 59.7 71.2average 61.0 62.4 74.5INV words only 64.5 62.4 75.3OOV words only 0 60.5 60.56.2 Word/Phoneme Hybrid SearchTable 2 gives the LVSCR transcription word-error ratesfor each set.
Almost all sets have a word-error rates above40%.
Searching those speech recognition transcriptionsresults in FOM and THP values below 40%.Table 3 gives results of searching in word, phoneme,and hybrid lattices.
First, for all test sets, word-latticesearch is drastically better than transcription-only search.Second, comparing word-lattice and phoneme-latticesearch, phoneme lattices outperforms word lattices onall tests in terms of FOM.
This is because phoneme lat-tice has better recall rate.
For THP, word lattice searchis slightly better except on the interview sets for whichthe language model is not well matched.
Hybrid searchleads to a substantial improvement over each (27.6% av-erage FOM improvement and 16.2% average THP im-provement over word lattice search).
This demonstratesthe complementary nature of word and phoneme search.We also show results separately for known words (in-vocabulary, INV) and out-of-vocabulary words (OOV).Interestingly, even for known words, hybrid search leadsto a significant improvement (get 22.0% for FOM and16.7% for THP) compared to using word lattices only.6.3 Effect of Node PosteriorIn Section 4.2, we have shown that phrase posteriors canbe computed from posterior lattices if they include botharc and node posteriors (Eq.
4).
However, posterior rep-resentations of lattices found in literature only includeword (arc) posteriors, and some posterior-based systemssimply ignore the node-posterior term, e.g.
(Chelba andAcero, 2005).
In Table 4, we evaluate the impact on ac-curacy when this term is ignored.
(In this experiment,we bypassed the index-lookup step, thus the numbers areslightly different from Table 3.
)We found that for word-level search, the effect of nodeposterior compensation is indeed neglectable.
However,for phonetic search it is not: We observe a 4% relativeFOM loss.6.4 Index Lookup and Linear SearchSection 5 introduced a two-stage search approach usingan M -gram based indexing scheme.
How much accuracyis lost from incorrectly eliminating correct hits in the first(index-based) stage?
Table 5 compares three setups.
Thefirst column shows results for linear search only: no indexlookup used at all, a complete linear search is performedon all lattices.
This search is optimal but does not scaleup to large database.
The second column shows indexlookup only.
Segments are ranked by the approximateM -gram based ETF score obtained from the index.
Thethird column shows the two-stage results.The index-based two-stage search is indeed very closeto a full linear search (average FOM loss of 1.2% andTHP loss of 0.2% points).
A two-stage search takes undertwo seconds and is mostly independent of the databasesize.
In other work, we have applied this technique suc-cessfully to search a database of nearly 200 hours.6.5 The SystemFig.
2 shows a screenshot of a research prototype for asearch-enabled audio notebook.
In addition to a note-taking area (bottom) and recording controls, it includesa rich audio browser showing speaker segmentation andautomatically identified speaker labels (both not scope ofthis paper).
Results of keyword searches are shown ascolor highlights, which are clickable to start playback atthat position.7 ConclusionIn this paper, we have presented a system for searchingrecordings of conversational speech, particularly meet-Table 4: Effect of ignoring the node-posterior term inphrase-posterior computation (shown for ICSI meetingset only).FOM word phonemeexact computation 72.1 82.3node posterior ignored 72.0 79.2relative change [%] -0.1 -3.8953Table 5: Comparing the effect of lattice indexing.
Shownis unindexed ?linear search,?
index lookup only (seg-ments selected via the index without subsequent linearsearch), and the combination of both.test set linear index two-search lookup stageFigure Of Merit (FOM) [%]ICSI meetings 88.6 86.4 88.2SWBD eval2000 88.7 86.5 87.3SWBD rt03s 87.3 85.1 84.2interviews (phone) 83.8 81.2 83.3interviews (lapel) 78.3 76.1 77.7average 85.3 83.1 84.1Top Hit Precision (THP) [%]ICSI meetings 78.8 70.7 78.7SWBD eval2000 78.0 71.4 77.9SWBD rt03s 71.9 65.7 71.7interviews (phone) 73.8 64.6 73.1interviews (lapel) 70.8 65.9 71.2average 74.7 67.7 74.5ings and telephone conversations.
We identified user re-quirements as accurate ranking of phrase matches, do-main independence, and reasonable response time.
Wehave addressed these by hybrid word/phoneme latticesearch and a supporting indexing scheme.
Unlike manyother spoken-document retrieval systems, we searchrecognition alternates instead of only speech recognitiontranscripts.
This yields a significant improvement of key-word spotting accuracy.
We have combined word-levelsearch with phonetic search, which not only enables thesystem to handle the open-vocabulary problem, but alsosubstantially improves in-vocabulary accuracy.
We haveproposed a posterior-lattice representation that allows forunified word and phoneme indexing and search.
To speedup the search process, we proposed M -gram based lat-tice indexing, which extends our open vocabulary searchability for large collection of audio.
Tested on five dif-ferent recording sets including meetings, conversations,and interviews, a search accuracy (FOM) of 84% hasbeen achieved ?
dramatically better than searching speechrecognition transcripts (under 40%).Figure 2: Screenshot of our research prototype of asearch-enabled audio notebook.8 AcknowledgementsThe authors wish to thank our colleagues Asela Gunawar-dana, Patrick Nguyen, Yu Shi, and Ye Tian for sharingtheir Switchboard setup and models with us; and FrankSoong for his valuable feedback on this paper.ReferencesC.
Allauzen, M. Mohri, M. Saraclar, General indexation ofweighted automata ?
application to spoken utterance re-trieval.
Proc.
HLT?2004, Boston, 2004.C.
Chelba and A. Acero, Position specific posterior lattices forindexing speech.
Proc.
ACL?2005, Ann Arbor, 2005.Mark Clements et al, Phonetic Searching vs. LVCSR:How to find what you really want in audio archives.Proc.
AVIOS?2001, San Jose, 2001.G.
Evermann et al, Large vocabulary decoding and con-fidence estimation using word posterior probabilities.Proc.
ICASSP?2000, Istanbul, 2000J.
Garofolo, TREC-9 Spoken Document Retrieval Track.National Institute of Standards and Technology, http://trec.nist.gov/pubs/trec9/sdrt9_slides/sld001.htm.D.
A. James and S. J.
Young, A fast lattice-based approach tovocabulary-independent wordspotting.
Proc.
ICASSP?1994,Adelaide, 1994.D.
Klakow.
Language-model optimization by mapping of cor-pora.
Proc.
ICASSP?1998.Beth Logan et al, An experimental study of an audio indexingsystem for the web.
Proc.
ICSLP?2000, Beijing, 2000.Beth Logan et al, Word and subword indexing approachesfor reducing the effects of OOV queries on spoken audio.Proc.
HLT?2002, San Diego, 2002.Kenney Ng, Subword-based approaches for spoken documentretrieval.
PhD thesis, Massachusetts Institute of Technology,2000.NIST Spoken Language Technology Evaluations, http://www.nist.gov/speech/tests/.S.
Ortmanns, H. Ney, F. Seide, and I. Lindam, A comparison ofthe time conditioned and word conditioned search techniquesfor large-vocabulary speech recognition.
Proc.
ICSLP?1996,Philadelphia, 1996.S.
E. Robertson, The probability ranking principle in IR.
Journalof Documentation 33 (1977).M.
Saraclar, R. Sproat, Lattice-based search for spoken utter-ance retrieval.
Proc.
HLT?2004, Boston, 2004.P.
Scha?uble et al, First experiences with a system for con-tent based retrieval of information from speech recordings.Proc.
IJCAI?1995, Montreal, 1995.R.
Schwartz et al, A comparison of several approximate al-gorithms for finding multiple (n-best) sentence hypotheses.Proc.
ICSLP?1994, Yokohama, 1994.F.
Seide, P. Yu, et al, Vocabulary-independent search in sponta-neous speech.
Proc.
ICASSP?2004, Montreal, 2004.Lisa Joy Stifelman, The Audio Notebook.
PhD thesis, Mas-sachusetts Institute of Technology, 1997.F.
Wessel, R. Schlu?ter, and H. Ney, Using posteriorword probabilities for improved speech recognition.Proc.
ICASSP?2000, Istanbul, 2000.P.
Yu, F. Seide, A hybrid word / phoneme-based approachfor improved vocabulary-independent search in spontaneousspeech.
Proc.
ICLSP?04, Jeju, 2004.P.
Yu, K. J. Chen, C. Y. Ma, F. Seide, Vocabulary-IndependentIndexing of Spontaneous Speech, to appear in IEEE transac-tion on Speech and Audio Processing, Special Issue on DataMining of Speech, Audio and Dialog.954
