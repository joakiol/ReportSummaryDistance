Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1268?1277,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsMinimum Bayes-risk System CombinationJesu?s Gonza?lez-RubioInstituto Tecnolo?gico de Informa?ticaU.
Polite`cnica de Vale`ncia46022 Valencia, Spainjegonzalez@iti.upv.esAlfons Juan Francisco CasacubertaD.
de Sistemas Informa?ticos y Computacio?nU.
Polite`cnica de Vale`ncia46022 Valencia, Spain{ajuan,fcn}@dsic.upv.esAbstractWe present minimum Bayes-risk system com-bination, a method that integrates consen-sus decoding and system combination intoa unified multi-system minimum Bayes-risk(MBR) technique.
Unlike other MBR meth-ods that re-rank translations of a single SMTsystem, MBR system combination uses theMBR decision rule and a linear combina-tion of the component systems?
probabilitydistributions to search for the minimum risktranslation among all the finite-length stringsover the output vocabulary.
We introduce ex-pected BLEU, an approximation to the BLEUscore that allows to efficiently apply MBR inthese conditions.
MBR system combination isa general method that is independent of spe-cific SMT models, enabling us to combinesystems with heterogeneous structure.
Exper-iments show that our approach bring sig-nificant improvements to single-system-basedMBR decoding and achieves comparable re-sults to different state-of-the-art system com-bination methods.1 IntroductionOnce statistical models are trained, a decoding ap-proach determines what translations are finally se-lected.
Two parallel lines of research have shownconsistent improvements over the max?derivationdecoding objective, which selects the highest prob-ability derivation.
Consensus decoding proceduresselect translations for a single system with a mini-mum Bayes risk (MBR) (Kumar and Byrne, 2004).System combination procedures, on the other hand,generate translations from the output of multiplecomponent systems by combining the best frag-ments of these outputs (Frederking and Nirenburg,1994).
In this paper, we present minimum Bayesrisk system combination, a technique that unifiesthese two approaches by learning a consensus trans-lation over multiple underlying component systems.MBR system combination operates directly on theoutputs of the component models.
We perform anMBR decoding using a linear combination of thecomponent models?
probability distributions.
In-stead of re-ranking the translations provided by thecomponent systems, we search for the hypothesiswith the minimum expected translation error amongall the possible finite-length strings in the target lan-guage.
By using a loss function based on BLEU (Pa-pineni et al, 2002), we avoid the hypothesis align-ment problem that is central to standard system com-bination approaches (Rosti et al, 2007).
MBR sys-tem combination assumes only that each translationmodel can produce expectations of n-gram counts;the latent derivation structures of the component sys-tems can differ arbitrary.
This flexibility allows us tocombine a great variety of SMT systems.The key contributions of this paper are three: theusage of a linear combination of distributions withinthe MBR decoding, which allows multiple SMTmodels to be involved in, and makes the computa-tion of n-grams statistics to be more accurate; thedecoding in an extended search space, which allowsto find better hypotheses than the evidences pro-vided by the component models; and the use of anexpected BLEU score instead of the sentence-wiseBLEU, which allows to efficiently apply MBR de-coding in the huge search space under consideration.We evaluate in a multi-source translation task ob-taining improvements of up to +2.0 BLEU abs.
overthe best single system max-derivation, and state-of-the-art performance in the system combination taskof the ACL 2010 workshop on SMT.12682 Related WorkMBR system combination is a multi-system gener-alization of MBR decoding where the space of hy-potheses is not constrained to the space of evidences.We expand the space of hypotheses following someunderlying ideas of system combination techniques.2.1 Minimum Bayes riskIn SMT, MBR decoding allows to minimize theloss of the output for a single translation system.MBR is generally implemented by re-ranking an N -best list of translations produced by a first pass de-coder (Kumar and Byrne, 2004).
Different tech-niques to widen the search space have been de-scribed (Tromble et al, 2008; DeNero et al, 2009;Kumar et al, 2009; Li et al, 2009).
These worksextend the traditional MBR algorithms based on N -best lists to work with lattices.The use of MBR to combine the outputs of vari-ous MT systems has also been explored previously.Duan et al (2010) present an MBR decoding thatmakes use of a mixture of different SMT systems toimprove translation accuracy.
Our technique differsin that we use a linear combination instead of a mix-ture, which avoids the problem of component sys-tems not sharing the same search space; perform thedecoding in a search space larger than the outputsof the component models; and optimize an expectedBLEU score instead of the linear approximation toit described in (Tromble et al, 2008).DeNero et al (2010) present model combination,a multi-system lattice MBR decoding on the con-joined evidences spaces of the component systems.Our technique differs in that we perform the searchin an extended search space not restricted to the pro-vided evidences, have fewer parameters to learn, andoptimizes an expected BLEU score instead of thelinear BLEU approximation.Another MBR-related technique to combine theoutputs of various MT systems was presented byGonza?lez-Rubio and Casacuberta (2010).
They usedifferent median string (Fu, 1982) algorithms tocombine various machine translation systems.
Ourapproach differs in that we take into account the pos-terior distribution over translations instead of con-sidering each translation equally likely, optimize theexpected BLEU score instead of a sentence-wisemeasure such as the edit distance or the sentence-level BLEU, and take into account the quality dif-ferences by associating a tunable scaling factor toeach system.2.2 System CombinationSystem combination techniques in MT take as in-put the outputs {e1, ?
?
?
, eN} of N translation sys-tems, where en is a structured translation object(or N -best lists thereof), typically viewed as a se-quence of words.
The dominant approach in thefield chooses a primary translation ep as a backbone,then finds an alignment an to the backbone for eachen.
A new search space is constructed from thesebackbone-aligned outputs and then a voting proce-dure of feature-based model predicts a final consen-sus translation (Rosti et al, 2007).
MBR systemcombination entirely avoids this alignment prob-lem by considering hypotheses as n-gram occur-rence vectors rather than word sequences.
MBR sys-tem combination performs the decoding in a largersearch space and includes statistics from the compo-nents?
posteriors, whereas system combination tech-niques typically do not.Despite these advantages, system combinationmay be more appropriate in some settings.
In par-ticular, MBR system combination is designed pri-marily for statistical systems that generate N -bestor lattice outputs.
MBR system combination can in-tegrate non-statistical systems that generate either asingle or an unweighted output.
However, we wouldnot expect the same strong performance from MBRsystem combination in these constrained settings.3 Minimum Bayes risk DecodingMBR decoding aims to find the candidate hypothesisthat has the least expected loss under a probabilitymodel (Bickel and Doksum, 1977).
We begin with areview of MBR for SMT.SMT can be described as a mapping of a word se-quence f in a source language to a word sequencee in a target language; this mapping is produced bythe MT decoder D(f).
If the reference translatione is known, the decoder performance can be mea-sured by the loss function L(e,D(f)).
Given such aloss function L(e, e?)
between an automatic transla-tion e?
and a reference e, and an underlying proba-1269bility model P (e|f), MBR decoding has the follow-ing form (Goel and Byrne, 2000; Kumar and Byrne,2004):e?
= arg mine??ER(e?)
(1)= arg mine?
?E?e?EP (e|f) ?
L(e, e?)
, (2)where R(e?)
denotes the Bayes risk of candidatetranslation e?
under loss function L, and E repre-sents the space of translations.If the loss function between any two hypothesescan be bounded: L(e, e?)
?
Lmax, the MBR de-coder can be rewritten in term of a similarity func-tion S(e, e?)
= Lmax ?
L(e, e?).
In this case, in-stead of minimizing the Bayes risk, we maximizethe Bayes gain G(e?):e?
= arg maxe??EG(e?)
(3)= arg maxe?
?E?e?EP (e|f) ?
S(e, e?)
.
(4)MBR decoding can use different spaces for hy-pothesis selection and gain computation (arg maxand summatory in Eq.
(4)).
Therefore, the MBR de-coder can be more generally written as follows:e?
= arg maxe?
?Eh?e?EeP (e|f) ?
S(e, e?)
, (5)where Eh refers to the hypotheses space form wherethe translations are chosen and Ee refers to the evi-dences space that is used to compute the Bayes gain.We will investigate the expansion of the hypothesesspace while keeping the evidences space as providedby the decoder.4 MBR System CombinationMBR system combination is a multi-system gener-alization of MBR decoding.
It uses the MBR de-cision rule on a linear combination of the probabil-ity distributions of the component systems.
Unlikeexisting MBR decoding methods that re-rank trans-lation outputs, MBR system combination search forthe minimum risk hypotheses on the complete set offinite-length hypotheses over the output vocabulary.We assume the component systems to be statisticallyindependent and define the Bayes gain as a linearcombination of the Bayes gains of the components.Each system provides its own space of evidencesDn(f) and its posterior distribution over translationsPn(e|f).
Given a sentence f in the source language,MBR system combination is written as follows:e?
= arg maxe??EhG(e?)
(6)?
arg maxe?
?EhN?n=1?n ?
Gn(e?)
(7)= arg maxe?
?EhN?n=1?n ?
?e?Dn(f)Pn(e|f) ?
S(e, e?)
, (8)where N is the total number of component systems,Eh represents the hypotheses space where the searchis performed, Gn(e?)
is the Bayes gain of hypothe-sis e?
given by the nth component system and ?n isa scaling factor introduced to take into account thedifferences in quality of the component models.
It isworth mentioning that by using a linear combinationinstead of a mixture model, we avoid the problemof component systems not sharing the same searchspace (Duan et al, 2010).MBR system combination parameters trainingand decoding in the extended hypotheses space aredescribed below.4.1 Model TrainingWe learn the scaling factors in Eq.
(8) using min-imum error rate training (MERT) (Och, 2003).MERT maximizes the translation quality of e?
on aheld-out set, according to an evaluation metric thatcompares to a reference set.
We used BLEU, choos-ing the scaling factors to maximize BLEU scoreof the set of translations predicted by MBR sys-tem combination.
We perform the maximization bymeans of the down-hill simplex algorithm (Nelderand Mead, 1965).4.2 Model DecodingIn most MBR algorithms, the hypotheses space isequal to the evidences space.
Following the underly-ing idea of system combination, we are interested inextend the hypotheses space by including new sen-tences created using fragments of the hypotheses inthe evidences spaces of the component models.
Weperform the search (argmax operation in Eq.
(8))1270Algorithm 1 MBR system combination decoding.Require: Initial hypothesis eRequire: Vocabulary the evidences ?1: e??
e2: repeat3: ecur ?
e?4: for j = 1 to |ecur| do5: e?s ?
ecur6: for a ?
?
do7: e?s ?
Substitute(ecur, a, j)8: if G(e?s) > G(e?s) then9: e?s ?
e?s10: e?d ?
Delete(ecur, j)11: e?i ?
ecur12: for a ?
?
do13: e?i ?
Insert(ecur, a, j)14: if G(e?i) > G(e?i) then15: e?i ?
e?i16: e??
arg maxe??
{ecur,e?s,e?d,e?i} G(e?
)17: until G(e?)
6> G(ecur)18: return ecurEnsure: G(ecur) ?
G(e)using the approximate median string (AMS) algo-rithm (Mart?
?nez et al, 2000).
AMS algorithm per-form a search on a hypotheses space equal to thefree monoid ??
of the vocabulary of the evidences?
= V oc(Ee).The AMS algorithm is shown in Algorithm 1.AMS starts with an initial hypothesis e that is mod-ified using edit operations until there is no improve-ment in the Bayes gain (Lines 3?16).
On each posi-tion j of the current solution ecur, we apply all thepossible single edit operations: substitution of thejth word of ecur by each word a in the vocabulary(Lines 5?9), deletion of the jth word of ecur (Line10) and insertion of each word a in the vocabulary inthe jth position of ecur (Lines 11?15).
If the Bayesgain of any of the new edited hypotheses is higherthan the Bayes gain of the current hypothesis (Line17), we repeat the loop with this new hypotheses e?,in other case, we return the current hypothesis.AMS algorithm takes as input an initial hypothe-sis e and the combined vocabulary of the evidencesspaces ?.
Its output is a possibly new hypothesiswhose Bayes gain is assured to be higher or equalthan the Bayes gain of the initial hypothesis.The complexity of the main loop (lines 2-17) isO(|ecur| ?
|?| ?
CG), where CG is the cost of com-puting the gain of a hypothesis, and usually only amoderate number of iterations (< 10) is needed toconverge (Mart?
?nez et al, 2000).5 Computing BLEU-based GainWe are interested in performing MBR system com-bination under BLEU.
BLEU behaves as a scorefunction: its value ranges between 0 and 1 and alarger value reflects a higher similarity.
Therefore,we rewrite the gain function G(?)
using single evi-dence (or reference) BLEU (Papineni et al, 2002)as the similarity function:Gn(e?)
=?e?Dn(f)Pn(e|f) ?
BLEU(e, e?)
(9)BLEU =4?k=1(mkck) 14?min(e1?rc , 1.0), (10)where r is the length of the evidence, c the length ofthe hypothesis, mk the number of n-gram matchesof size k, and ck the count of n-grams of size k inthe hypothesis.The evidences space Dn(f) may contain a hugenumber of hypotheses1 which often make impracti-cal to compute Eq.
(9) directly.
To avoid this prob-lem, Tromble et al (2008) propose linear BLEU, anapproximation to the BLEU score to efficiently per-form MBR decoding when the search space is repre-sented with lattices.
However, our hypotheses spaceis the full set of finite-length strings in the target vo-cabulary and can not be represented in a lattice.In Eq.
(9), we have one hypothesis e?
that is to becompared to a set of evidences e ?
Dn(f) whichfollow a probability distribution Pn(e|f).
Insteadof computing the expected BLEU score by calcu-lating the BLEU score with respect to each of theevidences, our approach will be to use the expectedn-gram counts and sentence length of the evidencesto compute a single-reference BLEU score.
We re-place the reference statistics (r and mn in Eq.
(10))by the expected statistics (r?
and m?n) given the pos-1For example, in a lattice the number of hypotheses may beexponential in the size of its state set.1271terior distribution Pn(e|f) over the evidences:Gn(e?)
=4?k=1(m?kck) 14?min(e1?r?c , 1.0)(11)r?
=?e?Dn(f)|e| ?
Pn(e|f) (12)m?k =?ng?Nk(e?)min(Ce?
(ng), C?
(ng)) (13)C ?
(ng) =?e?Dn(f)Ce(ng) ?
Pn(e|f) , (14)where Nk(e?)
is the set of n-grams of size k in thehypothesis, Ce?
(ng) is the count of the n-gram ng inthe hypothesis and C ?
(ng) is the expected count ofng in the evidences.
To compute the n-gram match-ings m?k, the count of each n-gram is truncated, ifnecessary, to not exceed the expected count for thatn-gram in the evidences.We have replaced a summation over a possibly ex-ponential number of items (e?
?
Dn(f) in Eq.
(9))with a summation over a polynomial number of n-grams that occur in the evidences2.
Both, the ex-pected length of the evidences r?
and their expectedn-gram counts m?k can be pre-computed efficientlyfrom N -best lists and translation lattices (Kumar etal., 2009; DeNero et al, 2010).6 ExperimentsWe report results on a multi-source translationtask.
From the Europarl corpus released for theACL 2006 workshop on MT (WMT2006), we se-lect those sentence pairs from the German?English(de?en), Spanish?English (es?en) and French?English (fr?en) sub-corpora that share the same En-glish translation.
We obtain a multi-source corpuswith German, Spanish and French as source lan-guages and English as target language.
All the ex-periments were carried out with the lowercased andtokenized version of this corpus.We report results using BLEU (Papineni et al,2002) and translation edit rate (Snover et al, 2006)(TER).
We measure statistical significance using2If Dn(f) is represented by a lattice, the number of n-grams ispolynomial in the number of edges in the lattice.Systemdev testBLEU TER BLEU TERde?enMAX 25.3 60.5 25.6?
60.3MBR 25.1 60.7 25.4?
60.5es?enMAX 30.9?
53.3?
30.4?
53.9?MBR 31.0?
53.4?
30.4?
54.0?fr?enMAX 30.7?
53.9?
30.8?
53.4?MBR 30.7?
53.8?
30.9?
53.4?Table 1: Performance of base systems.Approachdev testBLEU TER BLEU TERBest MAX 30.9?
53.3?
30.8?
53.4?Best MBR 31.0?
53.4?
30.9?
53.4?MBR-SC 32.3 52.5 32.8 52.3Table 2: Performance from best single system max-derivation decoding (Best MAX), the best single systemminimum Bayes risk decoding (Best MBR) and mini-mum Bayes risk system combination (MBR-SC) combin-ing three systems.95% confidence intervals computed using pairedbootstrap re-sampling (Zhang and Vogel, 2004).
Inall table cells (except for Table 3) systems withoutstatistically significant differences are marked withthe same superscript.6.1 Base SystemsWe combine outputs from three systems, each onetranslating from one source language (German,Spanish or French) into English.
Each individualsystem is a phrase-based system trained using theMoses toolkit (Koehn et al, 2007).
The parame-ters of the systems were tuned using MERT (Och,2003) to optimize BLEU on the development set.Each base system yields state-of-the-art perfor-mance, summarized in Table 1.
For each system,we report the performance of max-derivation decod-ing (MAX) and 1000-best3 MBR decoding (Kumarand Byrne, 2004).6.2 Experimental ResultsTable 2 compares MBR system combination (MBR-SC) to the best MAX and MBR systems.
Both Best3Ehling et al (2007) studied up to 10000-best and show that theuse of 1000-best candidates is sufficient for MBR decoding.1272Setup BLEU TERBest MBR 30.9 53.4MBR-SC Expected 30.9 53.5MBR-SC E/Conjoin 32.4 52.1MBR-SC E/C/evidences-best 30.9 53.5MBR-SC E/C/hypotheses-best 31.8 52.5MBR-SC E/C/Extended 32.7 52.3MBR-SC E/C/Ex/MERT 32.8 52.3Table 3: Results on the test set for different setups ofminimum Bayes risk system combination.MBR and MBR-SC were computed on 1000-bestlists.
MBR-SC uses expected BLEU as gain func-tion using the conjoined evidences spaces of thethree systems to compute expected BLEU statistics.It performs the search in the free monoid of the out-put vocabulary, and its model parameters were tunedusing MERT on the development set.
This is thestandard setup for MBR system combination, andwe refer to it as MBR-SC-E/C/Ex/MERT in Table 3.MBR system combination improves single BestMAX system by +2.0 BLEU points in test, and al-ways improves over MBR.
This improvement couldarise due to multiple reasons: the expected BLEUgain, the larger evidences space, the extended hy-potheses space, or the MERT tuned scaling factorvalues.
Table 3 teases apart these contributions.We first apply MBR-SC to the best system (MBR-SC-Expected).
Best MBR and MBR-SC-Expecteddiffer only in the gain function: MBR uses sentencelevel BLEU while MBR-SC-Expected uses the ex-pected BLEU gain described in Section 5.
MBR-SC-Expected performance is comparable to MBRdecoding on the 1000-best list from the single bestsystem.
The expected BLEU approximation per-forms as well as sentence-level BLEU and addition-ally requires less total computation.We now extend the evidences space to the con-joined 1000-best lists (MBR-SC-E/Conjoin).
MBR-SC-E/Conjoin is much better than the best MBR ona single system.
This implies that either the ex-pected BLEU statistics computed in the conjoinedevidences space are stronger or the larger conjoinedevidences spaces introduce better hypotheses.When we restrict the BLEU statistics to be com-puted from only the best system?s evidences space(MBR-SC-E/C/evidences-best), BLEU scores dra-matically decrease relative to MBR-SC-E/Conjoin.This implies that the expected BLEU statistics com-puted over the conjoined 1000-best lists are strongerthan the corresponding statistics from the single bestsystem.
On the other hand, if we restrict the searchspace to only the 1000-best list of the best sys-tem (MBR-SC-E/C/hypotheses-best), BLEU scoresalso decrease relative to MBR-SC-E/Conjoin.
Thisimplies that the conjoined search space also con-tains better hypotheses than the single best system?ssearch space.These results validate our approach.
The linearcombination of the probability distributions in theconjoined evidences spaces allows to compute muchstronger statistics for the expected BLEU gain andalso contains some better hypotheses than the singlebest system?s search space does.We next expand the conjoined evidences spacesusing the decoding algorithm described in Sec-tion 4.2 (MBR-SC-E/C/Extended).
In this case, theexpected BLEU statistics are computed from theconjoined 1000-best lists of the three systems, butthe hypotheses space where we perform the decod-ing is expanded to the set of all possible finite-length hypotheses over the vocabulary of the evi-dences.
We take the output of MBR-SC-E/Conjoinas the initial hypotheses of the decoding (see Algo-rithm 1).
MBR-SC-E/C/Extended improves BLEUscore of MBR-SC-E/Conjoin but obtains a slightlyworse TER score.
Since these two systems are iden-tical in their expected BLEU statistics, the improve-ments in BLEU imply that the extended search spacehas introduced better hypotheses.
The degradationin TER performance can be explained by the use of aBLEU-based gain function in the decoding process.We finally compute the optimum values forthe scaling factors of the different system us-ing MERT (MBR-SC-E/C/Ex/MERT).
MBR-SC-E/C/Ex/MERT slightly improves BLEU score ofMBR-SC-E/C/Extended.
This implies that the op-timal values of the scaling factors do not deviatemuch from 1.0; a similar result was reported in (Ochand Ney, 2001).
We hypothesize that this is becausethe three component systems share the same SMTmodel, pre-process and decoding.
We expect to ob-tain larger improvements when combining systemsimplementing different MT paradigms.127330.53131.53232.533100 101 102 103BLEUNumber of hypotheses in the N-best listsBest MAXMBR-SCMBR-SC C/ExtendedMBR-SC ConjoinFigure 1: Performance of minimum Bayes risk systemcombination (MBR-SC) for different sizes of the evi-dences space in comparison to other MBR-SC setups.MBR-SC-E/C/Ex/MERT is the standard setup forMBR system combination and, from now, on we willrefer to it as MBR-SC.We next evaluate performance of MBR systemcombination on N -best lists of increasing sizes, andcompare it to MBR-SC-E/C/Extended and MBR-SC-E/Conjoin in the same N -best lists.
We list theresults of the Best MAX system for comparison.Results in Figure 1 confirm the conclusions ex-tracted from results displayed in Table 3.
MBR-SC-Conjoin is consistently better than the Best MAXsystem, and differences in BLEU increase withthe size of the evidences space.
This implies thatthe linear combination of posterior probabilities al-low to compute stronger statistics for the expectedBLEU gain, and, in addition, the larger the evi-dences space is, the stronger the computed statisticsare.
MBR-SC-C/Extended is also consistently betterthan MBR-SC-Conjoin with an almost constant im-provement of +0.4 BLEU points.
This result showthat the extended search space always contains bet-ter hypotheses than the conjoined evidences spaces;also confirms the soundness of Algorithm 1 that al-lows to reach them.
Finally, MBR-SC also slightlyimproves MBR-SC-C/Extended.
The optimizationof the scaling factors allows only small improve-ments in BLEU.Figure 2 display the MBR system combinationtranslation and compare it to the max-derivationtranslations of the three component systems.
Refer-ence translation is also listed for comparison.
MBR-MAX de?en i will return later .MAX es?en i shall come back to that later .MAX fr?en i will return to this later .MBR-SC i will return to this point later .Reference i will return to this point later .Figure 2: MBR system combination example.SC adds word ?point?
to create a new translationequal to the reference.
MBR-SC is able to detectthat this is valuable word even though it does notappear in the max-derivation hypotheses.6.3 Comparison to System CombinationFigure 3 compares MBR system combination(MBR-SC) with state-of-the-art system combinationtechniques presented to the system combination taskof the ACL 2010 workshop on MT (WMT2010).All system combination techniques build a ?wordsausage?
from the outputs of the different compo-nent systems and choose a path trough the sausagewith the highest score under different models.
A de-scription of these systems can be found in (Callison-Burch et al, 2010).In this task, the output of the component systemsare single hypotheses or unweighted lists thereof.Therefore, we lack of the statistics of the com-ponents?
posteriors which is one of the main ad-vantages of MBR system combination over sys-tem combination techniques.
However, we find that,even in these constrained setting, MBR system com-bination performance is similar to the best sys-tem combination techniques for all translation di-rections.
These experiments validate our approach.MBR system combination yields state-of-the-artperformance while avoiding the challenge of align-ing translation hypotheses.7 ConclusionMBR system combination integrates consensus de-coding and system combination into a unified multi-system MBR technique.
MBR system combinationuses the MBR decision rule on a linear combina-tion of the component systems?
probability distri-butions to search for the sentence with the mini-mum Bayes risk on the complete set of finite-length1274161820222426283032cz-en en-cz de-en en-de es-en en-es fr-en en-frBLEUMBR-SCBBNCMUDCUJHUKOCLIUMRWTHFigure 3: Performance of minimum Bayes risk system combination (MBR-SC) for different language directions incomparison to the rest of system combination techniques presented in the WMT2010 system combination task.strings in the output vocabulary.
Component sys-tems can have varied decoding strategies; we onlyrequire that each system produce an N -best list (ora lattice) of translations.
This flexibility allows thetechnique to be applied quite broadly.
For instance,Leusch et al (2010) generate intermediate transla-tions in several pivot languages, translate them sep-arately into the target language, and generate a con-sensus translation out of these using a system combi-nation technique.
Likewise, these pivot translationscould be combined via MBR system combination.MBR system combination has two significant ad-vantages over current approaches to system combi-nation.
First, it does not rely on hypothesis align-ment between outputs of individual systems.
Align-ing translation hypotheses can be challenging andhas a substantial effect on combination perfor-mance (He et al, 2008).
Instead of aligning the sen-tences, we view the sentences as vectors of n-gramcounts and compute the expected statistics of theBLEU score to compute the Bayes gain.
Second, wedo not need to pick a backbone system for combina-tion.
Choosing a backbone system can also be chal-lenging and also affects system combination per-formance (He and Toutanova, 2009).
MBR systemcombination sidesteps this issue by working directlyon the conjoined evidences space produced by theoutputs of the component systems, and allows theconsensus model to express system preferences viascaling factors.Despite its simplicity, MBR system combinationprovides strong performance by leveraging differentconsensus, decoding and training techniques.
It out-performs best MAX or MBR derivation on each ofthe component systems.
In addition, it obtains state-of-the-art performance in a constrained setting bettersuited for dominant system combination techniques.AcknowledgementsWork supported by the EC (FEDER/FSE) and theSpanish MEC/MICINN under the MIPRCV ?Con-solider Ingenio 2010?
program (CSD2007-00018),the iTrans2 (TIN2009-14511) project, the UPV1275under grant 20091027 and the FPU scholarshipAP2006-00691.
Also supported by the SpanishMITyC under the erudito.com (TSI-020110-2009-439) project and by the Generalitat Valenciana undergrant Prometeo/2009/014.ReferencesPeter J. Bickel and Kjell A Doksum.
1977.
Mathe-matical statistics : basic ideas and selected topics.Holden-Day, San Francisco.Chris Callison-Burch, Philipp Koehn, Christof Monz,Kay Peterson, Mark Przybocki, and Omar F. Zaidan.2010.
Findings of the 2010 joint workshop on sta-tistical machine translation and metrics for machinetranslation.
In Proceedings of the Joint Fifth Workshopon Statistical Machine Translation and MetricsMATR,pages 17?53, Morristown, NJ, USA.
Association forComputational Linguistics.John DeNero, David Chiang, and Kevin Knight.
2009.Fast consensus decoding over translation forests.
InProceedings of the Joint Conference of the 47th An-nual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processing ofthe AFNLP: Volume 2 - Volume 2, pages 567?575,Morristown, NJ, USA.
Association for ComputationalLinguistics.John DeNero, Shankar Kumar, Ciprian Chelba, and FranzOch.
2010.
Model combination for machine trans-lation.
In Human Language Technologies: The 2010Annual Conference of the North American Chapter ofthe Association for Computational Linguistics, pages975?983, Morristown, NJ, USA.
Association for Com-putational Linguistics.Nan Duan, Mu Li, Dongdong Zhang, and Ming Zhou.2010.
Mixture model-based minimum bayes risk de-coding using multiple machine translation systems.
InProceedings of the 23rd International Conference onComputational Linguistics (Coling 2010), pages 313?321, Beijing, China, August.
Coling 2010 OrganizingCommittee.Nicola Ehling, Richard Zens, and Hermann Ney.
2007.Minimum bayes risk decoding for bleu.
In Proceed-ings of the 45th Annual Meeting of the ACL on Inter-active Poster and Demonstration Sessions, pages 101?104, Morristown, NJ, USA.
Association for Computa-tional Linguistics.Robert Frederking and Sergei Nirenburg.
1994.
Threeheads are better than one.
In Proceedings of the fourthconference on Applied natural language processing,pages 95?100, Morristown, NJ, USA.
Association forComputational Linguistics.K.S.
Fu.
1982.
Syntactic Pattern Recognition and Appli-cations.
Prentice Hall.Vaibhava Goel and William J. Byrne.
2000.
Minimumbayes-risk automatic speech recognition.
ComputerSpeech & Language, 14(2):115?135.Jesu?s Gonza?lez-Rubio and Francisco Casacuberta.
2010.On the use of median string for multi-source transla-tion.
In In Proceedings of the International Confer-ence on Pattern Recognition (ICPR2010), pages 4328?4331.Xiaodong He and Kristina Toutanova.
2009.
Joint opti-mization for machine translation system combination.In Proceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing: Volume 3- Volume 3, pages 1202?1211, Morristown, NJ, USA.Association for Computational Linguistics.Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen,and Robert Moore.
2008.
Indirect-hmm-based hy-pothesis alignment for combining outputs from ma-chine translation systems.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, pages 98?107, Morristown, NJ, USA.
As-sociation for Computational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: open sourcetoolkit for statistical machine translation.
In Proceed-ings of the 45th Annual Meeting of the ACL on Inter-active Poster and Demonstration Sessions, pages 177?180, Morristown, NJ, USA.
Association for Computa-tional Linguistics.Shankar Kumar and William J. Byrne.
2004.
Minimumbayes-risk decoding for statistical machine translation.In HLT-NAACL, pages 169?176.Shankar Kumar, Wolfgang Macherey, Chris Dyer, andFranz Och.
2009.
Efficient minimum error rate train-ing and minimum bayes-risk decoding for translationhypergraphs and lattices.
In Proceedings of the JointConference of the 47th Annual Meeting of the ACL andthe 4th International Joint Conference on Natural Lan-guage Processing of the AFNLP: Volume 1 - Volume 1,pages 163?171, Morristown, NJ, USA.
Association forComputational Linguistics.Gregor Leusch, Aure?lien Max, Josep Maria Crego, andHermann Ney.
2010.
Multi-pivot translation by sys-tem combination.
In International Workshop on Spo-ken Language Translation, Paris, France, December.Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.
2009.Variational decoding for statistical machine transla-tion.
In Proceedings of the Joint Conference of the47th Annual Meeting of the ACL and the 4th Interna-tional Joint Conference on Natural Language Process-1276ing of the AFNLP: Volume 2 - Volume 2, pages 593?601, Morristown, NJ, USA.
Association for Computa-tional Linguistics.C.
D.
Mart?
?nez, A. Juan, and F. Casacuberta.
2000.
Useof Median String for Classification.
In Proceedings ofthe 15th International Conference on Pattern Recog-nition, volume 2, pages 907?910, Barcelona (Spain),September.John A. Nelder and Roger Mead.
1965.
A SimplexMethod for Function Minimization.
The ComputerJournal, 7(4):308?313, January.Franz Josef Och and Hermann Ney.
2001.
Statisticalmulti-source translation.
In In Machine TranslationSummit, pages 253?258.Franz J. Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics - Volume 1, pages 160?167, Morris-town, NJ, USA.
Association for Computational Lin-guistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedings ofthe 40th Annual Meeting on Association for Compu-tational Linguistics, pages 311?318, Morristown, NJ,USA.
Association for Computational Linguistics.Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, Spy-ros Matsoukas, Richard Schwartz, and Bonnie Dorr.2007.
Combining outputs from multiple machinetranslation systems.
In Human Language Technolo-gies 2007: The Conference of the North AmericanChapter of the Association for Computational Lin-guistics; Proceedings of the Main Conference, pages228?235, Rochester, New York, April.
Association forComputational Linguistics.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and Ralph Weischedel.
2006.
A studyof translation error rate with targeted human annota-tion.
In In Proceedings of the Association for MachineTransaltion in the Americas.Roy W. Tromble, Shankar Kumar, Franz Och, and Wolf-gang Macherey.
2008.
Lattice minimum bayes-riskdecoding for statistical machine translation.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing, pages 620?629, Mor-ristown, NJ, USA.
Association for Computational Lin-guistics.Ying Zhang and Stephan Vogel.
2004.
Measuring confi-dence intervals for the machine translation evaluationmetrics.
In In Proceedings of the 10th InternationalConference on Theoretical and Methodological Issuesin Machine Translation (TMI-2004, pages 4?6.1277
