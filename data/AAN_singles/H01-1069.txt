Toward Semantics-Based Answer PinpointingEduard Hovy, Laurie Gerber, Ulf Hermjakob, Chin-Yew Lin, Deepak RavichandranInformation Sciences InstituteUniversity of Southern California4676 Admiralty WayMarina del Rey, CA 90292-6695USAtel: +1-310-448-8731{hovy,gerber,ulf,cyl,ravichan}@isi.eduABSTRACTWe describe the treatment of questions (Question-AnswerTypology, question parsing, and results) in the Weblcopediaquestion answering system.1.
INTRODUCTIONSeveral research projects have recently investigated theproblem of automatically answering simple questions that havebrief phrasal answers (?factoids?
), by identifying and extractingthe answer from a large collection of text.The systems built in these projects exhibit a fairly standardstructure: they create a query from the user?s question, performIR with the query to locate (segments of) documents likely tocontain an answer, and then pinpoint the most likely answerpassage within the candidate documents.
The most commondifference lies in the pinpointing.
Many projects employ awindow-based word scoring method that rewards desirablewords in the window.
They move the window across thecandidate answers texts/segments and return the window at theposition giving the highest total score.
A word is desirable ifit is a content word and it is either contained in the question, oris a variant of a word contained in the question, or if it matchesthe words of the expected answer.
Many variations of thismethod are possible?of the scores, of the treatment of multi-word phrases and gaps between desirable words, of the range ofvariations allowed, and of the computation of the expectedanswer words.Although it works to some degree (giving results of up to 30%in independent evaluations), the window-based method hasseveral quite serious limitations:?
it cannot pinpoint answer boundaries precisely (e.g., anexact name or noun phrase),?
it relies solely on information at the word level, andhence cannot recognize information of the desired type(such as Person or Location),?
it cannot locate and compose parts of answers that aredistributed over areas wider than the window.Window-based pinpointing is therefore not satisfactory in thelong run, even for factoid QA.
In this paper we describe workin our Webclopedia project on semantics-based answerpinpointing.
Initially, though, recognizing the simplicity andpower of the window-based technique for getting started, weimplemented a version of it as a fallback method.
We thenimplemented two more sophisticated methods: syntactic-semantic question analysis and QA pattern matching.
Thisinvolves classification of QA types to facilitate recognition ofdesired answer types, a robust syntactic-semantic parser toanalyze the question and candidate answers, and a matcher thatcombines word- and parse-tree-level information to identifyanswer passages more precisely.
We expect that the twomethods will really show their power when more complex non-factoid answers are sought.
In this paper we describe how wellthe three methods did relative to each other.
Section 2 outlinesthe Webclopedia system.
Sections 3, 4, and 5 describe thesemantics-based components: a QA Typology, question andanswer parsing, and matching.
Finally, we outline currentwork on automatically learning QA patterns using the NoisyChannel Model.2.
WEBCLOPEDIAWebclopedia?s architecture (Figure 1) follows the patternoutlined above:Question parsing: Using BBN?s IdentiFinder [1], ourparser CONTEX (Section 4) produces a syntactic-semanticanalysis of the question and determines the QA type (Section3).Query formation : Single- and multi-word units (contentwords) are extracted from the analysis, and WordNet synsets areused for query expansion.
A Boolean query is formed.
See [9].IR: The IR engine MG [12] returns the top-ranked 1000documents.Segmentat ion : To decrease the amount of text to beprocessed, the documents are broken into semanticallycoherent segments.
Two text segmenter?TexTiling [5] andC99 [2]?were tried; the first is used; see [9].Ranking segments : For each segment, each sentence i sscored using a formula that rewards word and phrase overlapwith the question and its expanded query words.
Segments areranked.
See [9]Parsing segments : CONTEX parses each sentence of thetop-ranked 100 segments (Section 4).Pinpointing: For each sentence, three steps of matching areperformed (Section 5); two compare the analyses of thequestion and the sentence; the third uses the window method tocompute a goodness score.Ranking of answers : The candidate answers?
scores arecompared and the winner(s) are output.3.
THE QA TYPOLOGYIn order to perform pinpointing deeper than the word level, thesystem has to produce a representation of what the user i sasking.
Some previous work in automated question answeringhas categorized questions by question word or by a mixture ofquestion word and the semantic class of the answer [11, 10].
Toensure full coverage of all forms of simple question and answer,and to be able to factor in deviations and special requirements,we are developing a QA Typology.We motivate the Typology (a taxonomy of QA types) asfollows.There are many ways to ask the same thing: What is the age o fthe Queen of Holland?
How old is the Netherlands?
queen?
Howlong has the ruler of Holland been alive?
Likewise, there aremany ways of delivering the same answer: about 60; 63 yearsold; since January 1938.
Such variations form a sort ofsemantic equivalence class of both questions and answers.Since the user may employ any version of his or her question,and the source documents may contain any version(s) of theanswer, an efficient system should group together equivalentquestion types and answer types.
Any specific question canthen be indexed into its type, from which all equivalent formsof the answer can be ascertained.
These QA equivalence typescan help with both query expansion and answer pinpointing.However, the equivalence is fuzzy; even slight variationsintroduce exceptions: who invented the gas laser?
can beanswered by both Ali Javan and a scientist at MIT, while whatis the name of the person who invented the gas laser?
requiresthe former only.
This inexactness suggests that the QA typesbe organized in an inheritance hierarchy, allowing the answerrequirements satisfying more general questions to beoverridden by more specific ones ?lower down?.These considerations help structure the Webclopedia QATypology.
Instead of focusing on question word or semantictype of the answer, our classes attempt to represent the user?sintention, including for example the classes Why-Famous (forWho was Christopher Columbus?
but not Who discoveredIR?
Steps: create query from question (WordNet-expand)retrieve top 1000 documents?
Engines: MG (Sydney)?
(Lin)AT&T (TREC)?(Lin)Segmentation?
Steps:segment each document into topical segments?
Engines: fixed-length (not used)TexTiling (Hearst 94)?
(Lin)C99 (Choi 00)?
(Lin)MAXNET (Lin 00, not used)Ranking?
Steps: score each sentence in each segment,using WordNet expansionrank segments?
Engines: FastFinder (Junk)Matching?
Steps: match general constraint patterns against parse treesmatch desired semantic type against parse tree elementsmatch desired words against words in sentences?
Engines: matcher (Junk)Ranking and answer extraction?
Steps: rank candidate answersextract and format them?
Engines: part of matcher (Junk)Question parsing?
Steps: parse questionfind desired semantic type?
Engines: IdentiFinder (BBN)CONTEX (Hermjakob)QA typology?
Categorize QA types in taxonomy (Gerber)Constraint patterns?
Identify likely answers in relation to otherparts of the sentence (Gerber)Retrieve documentsSegment documentsRank segmentsParse top segmentsParse questionInput questionMatch segments against questionRank and prepare answersCreate queryOutput answersSegment Parsing?
Steps: parse segment sentences?
Engines: CONTEX (Hermjakob)Figure 1.
Webclopedia architecture.America?, which is the QA type Proper-Person) andAbbreviation-Expansion (for What does HLT stand for?).
Inaddition, the QA Typology becomes increasingly specific asone moves from the root downward.To create the QA Typology, we analyzed 17,384 questions andtheir answers (downloaded from answers.com); see (Gerber, inprep.).
The Typology (Figure 2) contains 72 nodes, whose leafnodes capture QA variations that can in many cases be furtherdifferentiated.Each Typology node has been annotated with examples andtypical patterns of expression of both Question and Answer,using a simple template notation that expressed configurationsof words and parse tree annotations (Figure 3).
Questionpattern information (specifically, the semantic type of theanswer required, which we call a Qtarget) is produced by theCONTEX parser (Section 4) when analyzing the question,enabling it to output its guess(s) for the QA type.
Answerpattern information is used by the Matcher (Section 5) topinpoint likely answer(s) in the parse trees of candidate answersentences.Question examples and question templatesWho was Johnny Mathis' high school track coach?Who was Lincoln's Secretary of State?who be <entity>'s <role>Who was President of Turkmenistan in 1994?Who is the composer of Eugene Onegin?Who is the chairman of GE?who be <role> of <entity>Answer templates and actual answers<person>, <role> of  <entity>Lou Vasquez, track coach of?and Johnny Mathis<person> <role-title*> of <entity>Signed Saparmurad Turkmenbachy [Niyazov],president of Turkmenistan<entity>?s <role> <person>...Turkmenistan?s President Saparmurad Niyazov<person>'s <entity>...in Tchaikovsky's Eugene Onegin...<role-title> <person> ... <entity> <role>Mr.
Jack Welch, GE chairman...<subject>|<psv object> of related role-verb...Chairman John Welch said ...GE'sFigure 3.
Some QA Typology node annotations forProper-Person.At the time of the TREC-9 Q&A evaluation, we had producedapprox.
500 patterns by simply cross-combining approx.
20Question patterns with approx.
25 Answer patterns.
To ourdisappointment (Section 6), these patterns were both toospecific and too few to identify answers frequently?when theyapplied, they were quite accurate, but they applied too seldom.We therefore started work on automatically learning QApatterns in parse trees (Section 7).
On the other hand, thesemantic class of the answer (the Qtarget) is used to good effect(Sections 4 and 6).4.
PARSINGCONTEX is a deterministic machine-learning based grammarlearner/parser that was originally built for MT [6].
ForEnglish, parses of unseen sentences measured 87.6% labeledprecision and 88.4% labeled recall, trained on 2048 sentencesfrom the Penn Treebank.
Over the past few years it has beenextended to Japanese and Korean [7].4.1 Parsing QuestionsAccuracy is particularly important for question parsing,because for only one question there may be several answers in alarge document collection.
In particular, it is important toidentify as specific a Qtarget as possible.
But grammar rulesERACITY YES:NOTRUE:FALSENTIT Y A GENT NAME LAST-NAMEFIRST-NAMEORGANIZATIONGROUP-OF-PEOPLEA NIMALPERSON OCCUPATION-PERSONGEOGRAPHICA L-PERSONPROPER-NAMED-ENTITY PROPER-PERSONPROPER-ORGANIZATIONPROPER-PLACE CITYCOUNTRYSTATE-DISTRICTQUANTITY NU MERICAL-QUANTI TYMONETARY-QUANTITYTEMPORAL-QUANTITYMASS-QUANTI TYSPATIAL-QUANTITY DISTANCE-QUANTITYAREA-QUANTITYVOLUME-QUANTI TYTEMP-LOC DATEDATE-RANGELOCATOR ADDRESSEMAIL-ADDRESSPHONE-NUMBERURLTANGIBLE-OBJECT HU MAN-FOODSUBS TANCE LIQUIDBODY-PARTINSTRUMENTGARMENTTITLED-WORKABSTRACT SHAPEADJECTIVE COLORDISEASETEXTNARRATIVE GENERAL-INFO DEFINITION USEEXPRESSION-ORIGINHISTORY WHY-FAMOUS BIOANTECEDENTINFLUENCE CONSEQUENTCAUSE-EFFECT METHOD-MEANSCIRCUMSTANCE-MEANS REASONEVALUATION PRO-CONCONTRASTRATINGCOUNSEL-ADVICEFigure 2.
Portion of Webclopedia QA Typology.for declarative sentences do not apply well to questions, whichalthough typically shorter than declaratives, exhibit markedlydifferent word order, preposition stranding (?What universitywas Woodrow Wilson President of??
), etc.Unfortunately for CONTEX, questions to train on were notinitially easily available; the Wall Street Journal sentencescontain a few questions, often from quotes, but not enough andnot representative enough to result in an acceptable level ofquestion parse accuracy.
By collecting and treebanking,however, we increased the number of questions in the trainingdata from 250 (for our TREC-9 evaluation version ofWebclopedia) to 400 on Oct 16 to 975 on Dec 9.
The effect i sshown in Table 1.
In the first test run (?
[trained] without[additional questions]?
), CONTEX was trained mostly ondeclarative sentences (2000 Wall Street Journal sentences,namely the enriched Penn Treebank, plus a few other non-question sentences such as imperatives and short phrases).
Inlater runs (?
[trained] with [add.
questions]?
), the system wastrained on the same examples plus a subset of the 1153questions we have treebanked at ISI (38 questions from the pre-TREC-8 test set, all 200 from TREC-8 and 693 TREC-9, and222 others).The TREC-8 and TREC-9 questions were divided into 5 subsets,used in a five-fold cross validation test in which the system wastrained on all but the test questions, and then evaluated on thetest questions.Reasons for the improvement include (1) significantly moretraining data; (2) a few additional features, some more treebankcleaning, a bit more background knowledge etc.
; and (3) the251 test questions on Oct. 16 were probably a little bit harderon average, because a few of the TREC-9 questions initiallytreebanked (and included in the October figures) were selectedfor early treebanking because they represented particularchallenges, hurting subsequent Qtarget processing.4.2 Parsing Potential AnswersThe semantic type ontology in CONTEX was extended toinclude 115 Qtarget types, plus some combined types; moredetails in [8].
Beside the Qtargets that refer to concepts inCONTEX?s concept ontology (see first example below),Qtargets can also refer to part of speech labels (first example),to constituent roles or slots of parse trees (second and thirdexamples), and to more abstract nodes in the QA Typology(later examples).
For questions with the Qtargets Q-WHY-FAMOUS, Q-WHY-FAMOUS-PERSON, Q-SYNONYM, andothers, the parser also provides Qargs?information helpful formatching (final examples).Semantic ontology types (I-EN-CITY)and part of speech labels (S-PROPER-NAME):What is the capital of Uganda?QTARGET: (((I-EN-CITY S-PROPER-NAME))((EQ I-EN-PROPER-PLACE)))Parse tree roles:Why can't ostriches fly?QTARGET: (((ROLE REASON)))Name a film in which Jude Law acted.QTARGET: (((SLOT TITLE-P TRUE)))QA Typology nodes:What are the Black Hills known for?Q-WHY-FAMOUSWhat is Occam's Razor?Q-DEFINITIONWhat is another name for nearsightedness?Q-SYNONYMShould you exercise when you're sick?Q-YES-NO-QUESTIONQargs for additional information:Who was Betsy Ross?QTARGET: (((Q-WHY-FAMOUS-PERSON)))QARGS: (("Betsy Ross"))How is "Pacific Bell" abbreviated?QTARGET: (((Q-ABBREVIATION)))QARGS: (("Pacific Bell"))What are geckos?QTARGET: (((Q-DEFINITION)))QARGS: (("geckos" "gecko") ("animal"))These Qtargets are determined during parsing using 276 hand-written rules.
Still, for approx.
10% of the TREC-8&9questions there is no easily determinable Qtarget (?What doesthe Peugeot company manufacture??
; ?What is caliente inEnglish??).
Strategies for dealing with this are underinvestigation.
More details appear in (Hermjakob, 2001).
Thecurrent accuracy of the parser on questions and resultingQtargets sentences is shown in Table 2.5.
ANSWER MATCHINGThe Matcher performs three independent matches, in order:?
match QA patterns in the parse tree,?
match Qtargets and Qwords in the parse tree,?
match over the answer text using a word window.Details appear in [9].Table 1.
Improvement in parsing of questions.Labeled Labeled Tagging CrossingPrecision Recall Precision Recall Accuracy BracketsWithout, Oct 16 90.74% 90.72% 84.62% 83.48% 94.95% 0.6With, Oct 16 94.19% 94.86% 91.63% 91.91% 98.00% 0.48With, Dec 9 97.33% 97.13% 95.40% 95.13% 98.64% 0.19Table 1.
Improvement in parsing of questions.6.
RESULTSWe entered the TREC-9 short form QA track, and received anoverall Mean Reciprocal Rank score of 0.318, which putWebclopedia in essentially tied second place with two others.
(The best system far outperformed those in second place.
)In order to determine the relative performance of the modules,we counted how many correct answers their output contained,working on our training corpus.
Table 3 shows the evolutionof the system over a sample one-month period, reflecting theamount of work put into different modules.
The modules QApattern, Qtarget, Qword, and Window were all run in parallelfrom the same Ranker output.The same pattern, albeit with lower scores, occurred in theTREC test (Table 4).
The QA patterns made only a smallcontribution, the Qtarget made by far the largest contribution,and, interestingly, the word-level window match laysomewhere in between.Table 4.
TREC-9 test: correct answersattributable to each module.IR hits QA pattern Qtarget Window Total78.1 5.5 26.2 10.4 30.3We are pleased with the performance of the Qtarget match.
Thisshows that CONTEX is able to identify to some degree thesemantic type of the desired answer, and able to pinpoint thesetypes also in candidate answers.
The fact that it outperformsthe window match indicates the desirability of looking deeperthan the surface level.
As discussed in Section 4, we arestrengthening the parser?s ability to identify Qtargets.We are disappointed in the performance of the 500 QA patterns.Analysis suggests that we had too few patterns, and the ones wehad were too specific.
When patterns matched, they were ratheraccurate, both in finding correct answers and more preciselypinpointing the boundaries of answers.
However, they weretoo sensitive to variations in phrasing.
Furthermore, it wasdifficult to construct robust and accurate question and answerphraseology patterns manually, for several reasons.
First,manual construction relies on the inventiveness of the patternbuilder to foresee variations of phrasing, for both question andanswer.
It is however nearly impossible to think of allpossible variations when building patterns.Second, it is not always clear at what level of representation toformulate the pattern: when should one specify using words?Parts of speech?
Other parse tree nodes?
Semantic classes?
Thepatterns in Figure 3 include only a few of these alternatives.Specifying the wrong elements can result in non-optimalcoverage.
Third, the work is simply tedious.
We thereforedecided to try to learn QA patterns automatically.7.
TOWARD LEARNING QA PATTERNSAUTOMATICALLYTo learn corresponding question and answer expressions, wepair up the parse trees of a question and (each one of) itsanswer(s).
We then apply a set of matching criteria to identifypotential corresponding portions of the trees.
We then use theEM algorithm to learn the strengths of correspondencecombinations at various levels of representation.
This work i sstill in progress.In order to learn this information we observe the truism thatthere are many more answers than questions.
This holds for thetwo QA corpora we have access to?TREC and an FAQ website(since discontinued).
We therefore use the familiar version ofthe Noisy Channel Model and Bayes?
Rule.
For each basic QAtype (Location, Why-Famous, etc.
):Table 2.
Question parse tree and Qtarget accuracies.# Penn # Question Crossing Qtarget QtargetTreebank sentences Labeled Labele d Tagging brackets accuracy accuracysentences added Precision Recall Accuracy (/ sent) (strict) (lenient)2000 0 83.47% 82.49% 94.65% 0.34 63.00% 65.50%3000 0 84.74% 84.16% 94.51% 0.35 65.30% 67.40%2000 38 91.20% 89.37% 97.63% 0.26 85.90% 87.20%3000 38 91.52% 90.09% 97.29% 0.26 86.40% 87.80%2000 975 95.71% 95.45% 98.83% 0.17 96.10% 97.30%Date NumberQsIRhitsRankerhitsQApatternQtgtmatchQwordfallbackWindowfallbackTotal2-Jul 52 1.00 0.61 0.12 0.49 0.15 0.19 0.628-Jul 38 0.89 0.40 0.28 0.40 0.12 n/a 0.5313-Jul 52 1.00 0.61 0.04 0.48 0.15 0.22 0.533-Aug 55 n/a n/a 0.04 0.32 0.15 0.19 0.41Table 3.
Relative performance of Webclopedia modules on training corpus.P(A|Q)  =  argmax P(Q|A) .
P(A)P(A)  =   ?all trees (# nodes that may express a true A)/  (number of nodes in tree)P(Q|A)  =  ?all QA tree pairs (number of covarying nodesin Q and A trees)/ (number of nodes in A tree)As usual, many variations are possible, including how todetermine likelihood of expressing a true answer; whether toconsider all nodes or just certain major syntactic ones (N, NP,VP, etc.
); which information within each node to consider(syntactic?
semantic?
lexical?
); how to define ?covaryinginformation?
?node identity?
individual slot value equality?
;what to do about the actual answer node in the A trees; if (andhow) to represent the relationships among A nodes that havebeen found to be important; etc.
Figure 4 provides an answerparse tree that indicates likely Location nodes, determined byappropriate syntactic class, semantic type, and syntactic rolein the sentence.Our initial model focuses on bags of corresponding QA parsetree nodes, and will help to indicate for a given question whattype of node(s) will contain the answer.
We plan to extend thismodel to capture structured configurations of nodes that, whenmatched to a question, will help indicate where in the parse treeof a potential answer sentence the answer actually lies.
Suchbags or structures of nodes correspond, at the surface level, toimportant phrases or words.
However, by using CONTEXoutput we abstract away from the surface level, and learn toinclude whatever syntactic and/or semantic information is bestsuited for predicting likely answers.8.
REFERENCES[1] Bikel, D., R. Schwartz, and R. Weischedel.
1999.
AnAlgorithm that Learns What s in a Name.
MachineLearning Special Issue on NL Learning, 34, 1?3.
[2] Choi, F.Y.Y.
2000.
Advances in independent linear textsegmentation.
Proceedings of the 1st Conference of theNorth American Chapter of the Association forComputational Linguistics (NAACL-00), 26?33.
[3] Fellbaum, Ch.
(ed).
1998.
WordNet: An Electronic LexicalDatabase.
Cambridge: MIT Press.
[4] Gerber, L. 2001.
A QA Typology for Webclopedia.
In prep.
[5] Hearst, M.A.
1994.
Multi-Paragraph Segmentation ofExpository Text.
Proceedings of the Annual Conferenceof the Association for Computational Linguistics (ACL-94).
[6] Hermjakob, U.
1997.
Learning Parse and TranslationDecisions from Examples with Rich Context.
Ph.D.dissertation, University of Texas at Austin.file://ftp.cs.utexas.edu/pub/ mooney/papers/hermjakob-dissertation-97.ps.gz.
[7] Hermjakob, U.
2000.
Rapid Parser Development: AMachine Learning Approach for Korean.
Proceedings ofthe 1st Conference of the North American Chapter of theAssociation for Computational Linguistics (ANLP-NAACL-2000).http://www.isi.edu/~ulf/papers/kor_naacl00.ps.gz.
[8] Hermjakob, U.
2001.
Parsing and Question Classificationfor Question Answering.
In prep.
[9] Hovy, E.H., L. Gerber, U. Hermjakob, M. Junk, and C.-Y.Lin.
2000.
Question Answering in Webclopedia.Proceedings of the TREC-9 Conference.
NIST.Gaithersburg, MD.
[10] Moldovan, D., S. Harabagiu, M. Pasca, R. Mihalcea,, R.Girju, R. Goodrum, and V. Rus.
2000.
The Structure andPerformance of an Open-Domain Question AnsweringSystem.
Proceedings of the Conference of the Associationfor Computational Linguistics (ACL-2000), 563?570.
[11] Srihari, R. and W. Li.
2000.
A Question Answering SystemSupported by Information Extraction.
In Proceedings ofthe 1st Conference of the North American Chapter of theAssociation for Computational Linguistics (ANLP-NAACL-00), 166?172.
[12] Witten, I.H., A. Moffat, and T.C.
Bell.
1994.
ManagingGigabytes: Compressing and Indexing Documents andImages.
New York: Van Nostrand Reinhold.SURFLuxorisfamedfor itsValleyoftheKingsPharaonicnecropolisandtheKarnaktemplecomplex.CATS-SNTCLASSI-EV-BECLASSES(I-EV-BE)LEXbeSCORE0SURFLuxorCATS-NPCLASSI-EN-LUXORCLASSES(I-EN-LUXORI-EN-CITYI-EN-PLACEI-EN-AGENTI-EN-PROPER-NAMED-ENTITY)LEXLuxorROLES(SUBJ)SCORE4SURFisCATS-AUXCLASSI-EV-BECLASSES(I-EV-BE)LEXbeROLES(PRED)SCORE1SURFfamedCATS-ADJPCLASSI-EADJ-FAMEDCLASSES(I-EADJ-FAMED)LEXfamedROLES(COMPL)GRADEUNGRADEDSCORE0SURFforits ValleyoftheKingsPharaonicnecropolisandtheKarnaktemplecomplexCATS-PPCLASSI-EN-NECROPOLISCLASSES(I-EN-NECROPOLIS)LEXnecropolisROLES(MOD)SCORE3SURF.CATD-PERIODLEX.ROLES(DUMMY)SCORE0SURFLuxorCATS-PROPER-NAMECLASSI-EN-LUXORCLASSES(I-EN-LUXORI-EN-CITYI-EN-PLACEI-EN-AGENTI-EN-PROPER-NAMED-ENTITY)LEXLuxorROLES(PRED)SCORE5SURFfamedCATS-ADJCLASSI-EADJ-FAMEDCLASSES(I-EADJ-FAMED)LEXfamedROLES(PRED)GRADEUNGRADEDSCORE1SURFforCATS-PREPCLASSI-EP-FORCLASSES(I-EP-FOR)LEXforROLES(P)SCORE0SURFitsValleyoftheKingsPharaonicnecropolisandtheKarnaktemplecomplexCATS-NPCLASSI-EN-NECROPOLISCLASSES(I-EN-NECROPOLIS)LEXnecropolisROLES(PRED)SCORE3SURFitsValleyoftheKingsPharaonicnecropolisCATS-NPCLASSI-EN-NECROPOLISCLASSES(I-EN-NECROPOLIS)LEXnecropolisROLES(PRED)SCORE3SURFandCATS-COORD-CONJCLASSI-EC-ANDCLASSES(I-EC-AND)LEXandROLES(CONJ)SCORE0SURFtheKarnaktemplecomplexCATS-NPCLASSI-EN-COMPLEXCLASSES(I-EN-COMPLEX)LEXcomplexROLES(COORD)SCORE2SURFitsCATS-POSS-PRONCLASSI-EN-POSS-PRONOUNCLASSES(I-EN-POSS-PRONOUN)LEXPOSS-PRONROLES(DET)SCORE0SURFValleyoftheKingsPharaonicnecropolisCATS-NOUNCLASSI-EN-NECROPOLISCLASSES(I-EN-NECROPOLIS)LEXnecropolisROLES(PRED)SCORE1SURFValleyoftheKingsCATS-PROPER-NAMECLASSI-EN-PROPER-ORGANIZATIONCLASSES(I-EN-PROPER-ORGANIZATIONI-EN-ORGANIZATIONI-EN-AGENTI-EN-PROPER-NAMED-ENTITY)LEXValleyoftheKingsROLES(MOD)NAMED-ENTITY-UNIT-PTRUESCORE4SURFPharaonicCATS-PROPER-NAMECLASSI-EN-PHARAONICCLASSES(I-EN-PHARAONIC)LEXPharaonicROLES(MOD)SCORE3SURFnecropolisCATS-NOUNCLASSI-EN-NECROPOLISCLASSES(I-EN-NECROPOLIS)LEXnecropolisROLES(PRED)SCORE1SURFValleyCATS-NPCLASSI-EN-VALLEYCLASSES(I-EN-VALLEYI-EN-PLACE)LEXvalleyROLES(PRED)SCORE5SURFof theKingsCATS-PPCLASSI-EN-KING-NAMECLASSES(I-EN-KING-NAMEI-EN-AGENT)LEXKingROLES(MOD)SCORE3SURFValleyCATS-COUNT-NOUNCLASSI-EN-VALLEYCLASSES(I-EN-VALLEYI-EN-PLACE)LEXvalleyROLES(PRED)SCORE3SURFofCATS-PREPCLASSI-EP-OFCLASSES(I-EP-OF)LEXofROLES(P)SCORE0SURFtheKingsCATS-NPCLASSI-EN-KING-NAMECLASSES(I-EN-KING-NAMEI-EN-AGENT)LEXKingROLES(PRED)SCORE3SURFtheCATS-DEF-ARTCLASSI-EART-DEF-ARTCLASSES(I-EART-DEF-ART)LEXtheROLES(DET)SCORE0SURFKingsCATS-PROPER-NAMECLASSI-EN-KING-NAMECLASSES(I-EN-KING-NAMEI-EN-AGENT)LEXKingROLES(PRED)SCORE3SURFtheCATS-DEF-ARTCLASSI-EART-DEF-ARTCLASSES(I-EART-DEF-ART)LEXtheROLES(DET)SCORE0SURFKarnaktemplecomplexCATS-COUNT-NOUNCLASSI-EN-COMPLEXCLASSES(I-EN-COMPLEX)LEXcomplexROLES(PRED)SCORE1SURFKarnaktempleCATS-NOUNCLASSI-EN-TEMPLECLASSES(I-EN-TEMPLE)LEXtempleROLES(MOD)SCORE1SURFcomplexCATS-COUNT-NOUNCLASSI-EN-COMPLEXCLASSES(I-EN-COMPLEX)LEXcomplexROLES(PRED)SCORE1SURFKarnakCATS-NOUNCLASSI-EN-KARNAKCLASSES(I-EN-KARNAK)LEXkarnakROLES(MOD)SCORE1SURFtempleCATS-NOUNCLASSI-EN-TEMPLECLASSES(I-EN-TEMPLE)LEXtempleROLES(PRED)SCORE1Figure 4.
Candidate answer tree showing likely Location answers.
