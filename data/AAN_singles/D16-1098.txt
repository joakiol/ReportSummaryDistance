Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 968?974,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsExploiting Mutual Benefits between Syntax and Semantic Roles usingNeural NetworkPeng Shi??
Zhiyang Teng?
Yue Zhang?
?Singapore University of Technology and Design (SUTD)?Zhejiang University, Chinaimpavidity@zju.edu.cnzhiyang teng@mymail.sutd.edu.sg, yue zhang@sutd.edu.sgAbstractWe investigate mutual benefits between syn-tax and semantic roles using neural networkmodels, by studying a parsing?SRL pipeline,a SRL?parsing pipeline, and a simple jointmodel by embedding sharing.
The integra-tion of syntactic and semantic features givespromising results in a Chinese Semantic Tree-bank, demonstrating large potentials of neuralmodels for joint parsing and semantic role la-beling.1 IntroductionThe correlation between syntax and semantics hasbeen a fundamental problem in natural languageprocessing (Steedman, 2000).
As a shallow seman-tic task, semantic role labeling (SRL) models havetraditionally been built upon syntactic parsing re-sults (Gildea and Jurafsky, 2002; Gildea and Palmer,2002; Punyakanok et al, 2005).
It has been shownthat parser output features play a crucial role for ac-curate SRL (Pradhan et al, 2005; Surdeanu et al,2007).On the reverse direction, semantic role featureshave been used to improve parsing (Boxwell et al,2010).
Existing methods typically use semantic fea-tures to rerank n-best lists of syntactic parsing mod-els (Surdeanu et al, 2008; Hajic?
et al, 2009).
Therehas also been attempts to learn syntactic parsing andsemantic role labeling models jointly, but most suchefforts have led to negative results (Sutton and Mc-Callum, 2005; Van Den Bosch et al, 2012; Boxwellet al, 2010).
?Work done while the first author was visiting SUTD.With the rise of deep learning, neural networkmodels have been used for semantic role label-ing (Collobert et al, 2011).
Recently, it has beenshown that a neural semantic role labeler can givestate-of-the-art accuracies without using parser out-put features, thanks to the use of recurrent neuralnetwork structures that automatically capture syn-tactic information (Zhou and Xu, 2015; Wang et al,2015).
In the parsing domain, neural network mod-els have also been shown to give state-of-the-art re-sults recently (Dyer et al, 2015; Weiss et al, 2015;Zhou et al, 2015).The availability of parser-independent neural SRLmodels allows parsing and SRL to be performedby both parsing?SRL and SRL?parsing pipelines,and gives rise to the interesting research questionwhether mutual benefits between syntax and seman-tic roles can be better exploited under the neuralsetting.
Different from traditional models that relyon manual feature combinations for joint learningtasks (Sutton and McCallum, 2005; Zhang andClark, 2008a; Finkel and Manning, 2009; Lewis etal., 2015), neural network models induce non-linearfeature combinations automatically from input wordand Part-of-Speech (POS) embeddings.
This al-lows more complex feature sharing between multi-ple tasks to be achieved effectively (Collobert et al,2011).We take a first step1 in such investigation by cou-1Recently, Swayamdipta et al (2016) independently pro-posed a similar idea to perform joint syntactic and semanticdependency parsing.
Their work mainly focuses on extendingactions of a greedy transition-based parser to support the jointtask, achieving good performance on an English shared task,while we use a neural network for multi-task learning and we968pling a state-of-the-art neural semantic role labeler(Wang et al, 2015) and a state-of-the-art neuralparser (Dyer et al, 2015).
First, we propose a novelparsing?SRL pipeline using a tree Long Short-Term Memory (LSTM) model (Tai et al, 2015) torepresent parser outputs, before feeding them to theneural SRL model as inputs.
Second, we investigatea SRL?parsing pipeline, using semantic role labelembeddings to enrich parser features.
Third, webuild a joint training model by embedding sharing,which is the most shallow level of parameter sharingbetween deep neural networks.
This simple strat-egy is immune to significant differences between thenetwork structures of the two models, which pre-vent direct sharing of deeper network parameters.We choose a Chinese semantic role treebank (Qiu etal., 2016) for preliminary experiments, which offersconsistent dependency between syntax and seman-tic role representations, thereby facilitates the ap-plication of standard LSTM models.
Results showthat the methods give improvements to both parsingand SRL accuracies, demonstrating large potentialsof neural networks for the joint task.Our contributions can be summarized as:?We show that the state-of-the-art LSTM seman-tic role labeler of Zhou and Xu (2015), which hasbeen shown to be able to induce syntactic featuresautomatically, can still be improved using parseroutput features via tree LSTM (Tai et al, 2015);?We show that state-of-the-art neural parsing canbe improved by using semantic role features;?We show that parameter sharing between neuralparsing and SRL improves both sub tasks, which isin line with the observation of Collobert et al (2011)between POS tagging, chunking and SRL.?
Our code and all models are released athttps://github.com/ShiPeng95/ShallowJoint.2 Models2.1 Semantic Role LabelerWe employ the SRL model of Wang et al (2015),which uses a bidirectional Long Short-term Mem-ory (Hochreiter and Schmidhuber, 1997; Gravesand Schmidhuber, 2005; Graves et al, 2013) for se-quential labeling.work on a Chinese dataset.Figure 1: Bi-LSTM Semantic Role LabelerGiven the sentence ???
(human) ?
(de) ??
(development) ??
(face) ??
(challenge)?, thestructure of the model is shown in Figure 1.
Foreach word wt, the LSTM model uses a set of vec-tors to control information flow: an input gate it, aforget gate ft, a memory cell ct, an output gate ot,and a hidden state ht.
The computation of each vec-tor is as follows:it = ?
(W (i)xt + U (i)ht?1 + V (i)ct?1 + b(i))ft = 1.0?
itct = ft  ct?1 + it  tanh(W (u)xt + U (u)ht?1 + b(u))ot = ?
(W (o)xt + U (o)ht?1 + V (o)ct + b(o))ht = ot  tanh(ct)Here ?
denotes component-wise sigmoid functionand  is component-wise multiplication.The representation of xt is from four sources: anembedding for the word wt, two hidden states ofthe last LSTM cells in a character-level bidirectionalLSTM (Ballesteros et al, 2015) (denoted as ??chtand?
?cht, respectively), and a learned vector Part-of-Speech (POS) representation (post).
A linear trans-formation is applied to the vector representationsbefore feeding them into a component-wise ReLU(Nair and Hinton, 2010) function.xt = max{0, V (x)[wt;??cht;?
?cht; post] + b(x)}The hidden state vectors at the t-th word from bothdirections (denote as ?
?ht and ?
?ht , respectively) arepassed through the ReLU function, before a softmaxlayer for semantic role detection.2.2 Stack-LSTM Dependency ParserWe employ the Stack-LSTM model of Dyer et al(2015) for dependency parsing.
As shown in Figure2, it uses a buffer (B) to order input words, a stack(S) to store partially constructed syntactic trees, and969Figure 2: Stack-LSTM Parsertakes the following types of actions to build treesfrom input.?
SHIFT, which pops the top element off thebuffer, pushing it into stack.?
REDUCE-LEFT/REDUCE-RIGHT, whichpop the top two elements off the stack, pushing backthe composition of the two elements with a depen-dent relation.The parser is initialized by pushing input embed-dings into the buffer in the reverse order.
The repre-sentation of the token is same as the previous bidi-rectional LSTM (Bi-LSTM) model.
The buffer (B),stack (S) and action history sequence (A) are all rep-resented by LSTMs, with S being represented by anovel stack LSTM.
At a time step t, the parser pre-dicts an action according to current parser state pt:pt = max{0,W (parser)[st; bt; at] + dp},y(parser)t = softmax(V (parser)pt + dy)W , V and d are model parameters.2.3 DEP?SRL PipelineIn this pipeline model, we apply Stack-LSTM pars-ing first and feed the results as additional featuresfor SRL.
For each word wt to the SRL system, thecorresponding input becomes,x(dep)t = max{0, V (dep)[wt;??cht;?
?cht; post;dept]}where dept is the t-th word?s dependency informa-tion from parser output and V (dep) is a weight ma-trix.
There are multiple ways to define dept.
A sim-ple method is to use embeddings of the dependencylabel at wt.
However, this input does not embodyfull arc information.We propose a novel way of defining dept?, by us-ing hidden vector ht?
of a dependency tree LSTM(Tai et al, 2015) at wt?
as dept?.
Given a depen-dency tree output, we define tree LSTM inputs xt?in the same way as Section 2.1.
The tree LSTM isa bottom-up generalization of the sequence LSTM,with a node ht?
having multiple predecessors hkt?
?1,which corresponding to the syntactic dependents ofthe word wt?.
The computation of ht?
for each wt?
is(unlike t, which is a left-to-right index, t?
is a bottom-up index, still with one ht?
being computed for eachwt?):h?t?
?1 =?khkt??1it?
= ?
(W (i)xt?
+ U (i)h?t?
?1 + b(i))ft?k=?
(W (f)xt?+U(f)hkt?
?1+b(f))ct?=?k fkt?
ckt??1+it?
tanh(W (u)xt?+U(u)h?t??1+b(u))ot?=?
(W (o)xt?+U(o)h?t??1+b(o))ht?=ot?
tanh(ct?
)For training, we construct a corpus with all wordsbeing associated with automatic dependency labelsby applying 10-fold jackknifing.2.4 SRL?DEP PipelineIn this pipeline model, we conduct SRL first, andfeed the output semantic roles to the Stack-LSTMparser in the token level.
The representation of atoken becomes:x(srl)t = max{0, V (srl)[wt;??cht;?
?cht; post; srlt]}where srlt is the t-th word?s predicted semantic roleembedding and V (srl) is a weight matrix.For training, we construct a training corpus withautomatically tagged semantic role labels by using10-fold jackknifing.2.5 Joint Model by Parameter SharingThe structure of the joint system is shown in Fig-ure 3.
Here the parser and semantic role labeler arecoupled in the embedding layer, sharing the vectorlookup tables for characters, words and POS.
Morespecifically, the Bi-LSTM model of Section 2.1 andthe Stack-LSTM model of Section 2.2 are used forthe SRL task and the parsing task, respectively.
TheBi-LSTM labeler and Stack-LSTM parser share theembedding layer.
During training, we maximize the970Figure 3: Joint Multi-task Modelsum of log-likelihood for the two different tasks.The loss from the semantic role labeler and theparser both propagate to the embedding layer, re-sulting in a better vector representation of each to-ken, which benefits both tasks at the same time.
Onthe other hand, due to different neural structures,there is no sharing of other parameters.
The jointmodel offers the simplest version of shared training(Collobert et al, 2011), but does not employ shareddecoding (Sutton and McCallum, 2005; Zhang andClark, 2008b).
Syntax and semantic roles are as-signed separately, avoiding error propagation.3 Experiments3.1 Experimental SettingsDatasets We choose Chinese Semantic Tree-bank (Qiu et al, 2016) for our experiments.
Similarto the CoNLL corpora (Surdeanu et al, 2008; Hajic?et al, 2009) and different from PropBank (Kings-bury and Palmer, 2002; Xue and Palmer, 2005),it is a dependency-based corpus rather than aconstituent-based corpus.
The corpus contains syn-tactic dependency arc and semantic role annotationsin a consistent form, hence facilitating the joint task.We follow the standard split for the training, devel-opment and test sets, as shown in Table 1.Training Details.
There is a large number of sin-gletons in the training set and a large number ofout-of-vocabulary (OOV) words in the developmentset.
We use the mechanism of Dyer et al (2015) tostochastically set singletons as UNK token in eachtraining iteration with a probability punk.
The hyper-parameter punk is set to 0.2.For parameters used in Stack-LSTM, we followDyer et al (2015).
We set the number of embed-dings by intuition, and decide to have the size ofword embedding twice as large as that of charac-Dataset Words Types Singletons OOVTrain 280,043 24,866 12,012 -Dev 23,724 5,492 - 1,505Test 32,326 6,989 - 1,893Table 1: Statistics of Chinese Semantic Treebank.ter embedding, and the size of character embeddinglarger than the size of POS embedding.
More specif-ically, we fix the size of word embeddings nw to 64,character embeddings nchar to 32, POS embeddingsnpos to 30, action embeddings ndep to 30, and se-mantic role embeddings nsrl to 30.
The LSTM inputsize is set to 128 and the LSTM hidden size to 128.We randomly initialize each parameter to a realvalue in [?
?6r+c ,?6r+c ], where r is the number ofinput unit and c is the number of output unit (Glo-rot and Bengio, 2010).
To minimize the influenceof external information, we did not pretrain the em-bedding values.
In addition, we apply a Gaussiannoise N(0, 0.2) to word embeddings during trainingto prevent overfitting.We optimize model parameters using stochasticgradient descent with momentum.
The same learn-ing rate decay mechanism of Dyer et al (2015) isused.
The best model parameters are selected ac-cording to a score metric on the development set.For different tasks, we use different score metrics toevaluate the parameters.
Since there are there met-rics, F1, UAS and LAS, possibly reported at thesame time, we use the weighted average to con-sider the effect of all metrics when choosing the bestmodel on the dev set.
In particular, we use F1 forSRL, 0.5 ?
LAS + 0.5 ?
UAS for parsing, and0.5?F1 + 0.25?UAS+ 0.25?LAS for the jointtask.3.2 ResultsThe final results are shown in Table 2, where F1 rep-resents the F1-score of semantic roles, and UAS andLAS represent parsing accuracies.
The Bi-LSTMrow represents the bi-directional semantic role la-beler, the S-LSTM row represents the Stack-LSTMparser, the DEP?SRL row represents the depen-dency parsing?
SRL pipeline, the SRL?DEP rowrepresents the SRL?
dependency parsing pipeline,and the Joint row represents the parameter-sharedmodel.
For the DEP?SRL pipeline, lab and lstm971Model F1 UAS LASBi-LSTM 72.71 - -S-LSTM - 84.33 82.10DEP?SRL(lab/lstm) 73.00/74.18 84.33 82.10SRL?DEP 72.71 84.75 82.62Joint 73.84 85.15 82.91Table 2: Results.
Bi-LSTM and S-LSTM are two baselinemodels for SRL and parsing, respectively.
DEP?SRL andSRL?DEP are two pipeline models.
?Joint?
denotes the pro-posed model for joint parsing and semantic role labeling.
labuses only the dependency label as features, while lstm appliesfeatures extracted from dependency trees using tree LSTMs.represents the use of dependency label embeddingsand tree LSTM hidden vectors for the additionalSRL features dept, respectively.Comparison between Bi-LSTM and DEP?SRLshows that slight improvement is brought by intro-ducing dependency label features to the semanticrole labeler (72.71?73.00).
By introducing fulltree information, the lstm integration leads to muchhigher improvements (72.71?74.18).
This demon-strates that the LSTM SRL model of Zhou and Xu(2015) can still benefit from parser outputs, despitethat it can learn syntactic information independently.In the reverse direction, comparison betweenS-LSTM and SRL?DEP shows improvement toUAS/LAS by integrating semantic role features(82.10?82.62).
This demonstrates the usefulnessof semantic roles to parsing and is consistent withobservations on discrete models (Boxwell et al,2010).
To our knowledge, we are the first to reportresults using a SRL ?
Parsing pipeline, which isenabled by the neural SRL model.Using shared embeddings, the joint model givesimprovements on both SRL and parsing.
The mostsalient difference between the joint model and thetwo pipelines is the shared parameter space.These results are consistent with the finds of Col-lobert et al (2011) who show that POS, chunkingand semantic role information can bring benefit toeach other in joint neural training.
In contrast to theirresults (SRL 74.15?74.29, POS 97.12?97.22,CHUNK 93.37?93.75), we find that parsing andSRL benefit relatively more from each other (SRL72.72?73.84, DEP 84.33?85.15).
This is intuitivebecause parsing offers deeper syntactic informationcompared to POS and shallow syntactic chunking.4 ConclusionWe investigated the mutual benefits between depen-dency syntax and semantic roles using two state-of-the-art LSTM models, finding that both can be fur-ther improved.
In addition, simple multitask learn-ing is also effective.
These results demonstrate po-tentials for deeper joint neural models between thesetasks.AcknowledgmentsYue Zhang is the corresponding author.
Thisresearch is supported by NSFC61572245 andT2MOE201301 from Singapore Ministry of Educa-tion.
We appreciate anonymous reviewers for theirinsightful comments.ReferencesMiguel Ballesteros, Chris Dyer, and Noah A Smith.2015.
Improved transition-based parsing by modelingcharacters instead of words with lstms.
arXiv preprintarXiv:1508.00657.Stephen A Boxwell, Dennis N Mehay, and Chris Brew.2010.
What a parser can learn from a semantic role la-beler and vice versa.
In Proceedings of the 2010 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 736?744.
Association for Compu-tational Linguistics.Ronan Collobert, Jason Weston, Le?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural language processing (almost) from scratch.The Journal of Machine Learning Research, 12:2493?2537.Chris Dyer, Miguel Ballesteros, Wang Ling, AustinMatthews, and Noah A. Smith.
2015.
Transition-based dependency parsing with stack long short-termmemory.
In Proc.
ACL.Jenny Rose Finkel and Christopher D Manning.
2009.Joint parsing and named entity recognition.
In Pro-ceedings of Human Language Technologies: The 2009Annual Conference of the North American Chapter ofthe Association for Computational Linguistics, pages326?334.
Association for Computational Linguistics.Daniel Gildea and Daniel Jurafsky.
2002.
Automatic la-beling of semantic roles.
Computational linguistics,28(3):245?288.Daniel Gildea and Martha Palmer.
2002.
The necessityof parsing for predicate argument recognition.
In Pro-ceedings of the 40th Annual Meeting on Associationfor Computational Linguistics, pages 239?246.
Asso-ciation for Computational Linguistics.972Xavier Glorot and Yoshua Bengio.
2010.
Understand-ing the difficulty of training deep feedforward neuralnetworks.
In International conference on artificial in-telligence and statistics, pages 249?256.Alex Graves and Ju?rgen Schmidhuber.
2005.
Frame-wise phoneme classification with bidirectional lstmand other neural network architectures.
Neural Net-works, 18(5):602?610.A.
Graves, A. Mohamed, and G. Hinton.
2013.
Speechrecognition with deep recurrent neural networks.Jan Hajic?, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Anto`nia Mart?
?, Llu?
?sMa`rquez, Adam Meyers, Joakim Nivre, SebastianPado?, Jan S?te?pa?nek, et al 2009.
The conll-2009shared task: Syntactic and semantic dependenciesin multiple languages.
In Proceedings of the Thir-teenth Conference on Computational Natural Lan-guage Learning: Shared Task, pages 1?18.
Associa-tion for Computational Linguistics.Sepp Hochreiter and Ju?rgen Schmidhuber.
1997.
Longshort-term memory.
Neural computation, 9(8):1735?1780.Paul Kingsbury and Martha Palmer.
2002.
From tree-bank to propbank.
In LREC.
Citeseer.Mike Lewis, Luheng He, and Luke Zettlemoyer.
2015.Joint a* ccg parsing and semantic role labelling.In Proceedings of the 2015 Conference on Empiri-cal Methods in Natural Language Processing, pages1444?1454, Lisbon, Portugal, September.
Associationfor Computational Linguistics.Vinod Nair and Geoffrey E Hinton.
2010.
Rectified lin-ear units improve restricted boltzmann machines.
InProceedings of the 27th International Conference onMachine Learning (ICML-10), pages 807?814.Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James HMartin, and Daniel Jurafsky.
2005.
Semantic rolechunking combining complementary syntactic views.In Proceedings of the Ninth Conference on Compu-tational Natural Language Learning, pages 217?220.Association for Computational Linguistics.Vasin Punyakanok, Dan Roth, and Wen-tau Yih.
2005.The necessity of syntactic parsing for semantic role la-beling.
In IJCAI, volume 5, pages 1117?1123.Likun Qiu, Yue Zhang, and Meishan Zhang.
2016.
De-pendency tree representations of predicate-argumentstructures.
In Proc.
AAAI.Mark Steedman.
2000.
The syntactic process, vol-ume 24.
MIT Press.Mihai Surdeanu, Llu?
?s Ma`rquez, Xavier Carreras, andPere R Comas.
2007.
Combination strategies for se-mantic role labeling.
Journal of Artificial IntelligenceResearch, pages 105?151.Mihai Surdeanu, Richard Johansson, Adam Meyers,Llu?
?s Ma`rquez, and Joakim Nivre.
2008.
Theconll-2008 shared task on joint parsing of syntac-tic and semantic dependencies.
In Proceedings ofthe Twelfth Conference on Computational NaturalLanguage Learning, pages 159?177.
Association forComputational Linguistics.Charles Sutton and Andrew McCallum.
2005.
Jointparsing and semantic role labeling.
In Proceedingsof the Ninth Conference on Computational NaturalLanguage Learning, pages 225?228.
Association forComputational Linguistics.Swabha Swayamdipta, Miguel Ballesteros, Chris Dyer,and Noah A Smith.
2016.
Greedy, joint syntactic-semantic parsing with stack lstms.
arXiv preprintarXiv:1606.08954.Kai Sheng Tai, Richard Socher, and Christopher D. Man-ning.
2015.
Improved semantic representations fromtree-structured long short-term memory networks.
InProceedings of the 53rd Annual Meeting of the Associ-ation for Computational Linguistics and the 7th Inter-national Joint Conference on Natural Language Pro-cessing (Volume 1: Long Papers), pages 1556?1566,Beijing, China, July.
Association for ComputationalLinguistics.Antal Van Den Bosch, Roser Morante, and Sander Cani-sius.
2012.
Joint learning of dependency parsing andsemantic role labeling.
Computational Linguistics inthe Netherlands Journal, 2:97?117.Zhen Wang, Tingsong Jiang, Baobao Chang, and Zhi-fang Sui.
2015.
Chinese semantic role labeling withbidirectional recurrent neural networks.
In Proc.
ofthe 2015 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 1626?1631.David Weiss, Chris Alberti, Michael Collins, and SlavPetrov.
2015.
Structured training for neural networktransition-based parsing.
In Proceedings of the 53rdAnnual Meeting of the Association for ComputationalLinguistics and the 7th International Joint Conferenceon Natural Language Processing (Volume 1: Long Pa-pers), pages 323?333, Beijing, China, July.
Associa-tion for Computational Linguistics.Nianwen Xue and Martha Palmer.
2005.
Automatic se-mantic role labeling for chinese verbs.
In IJCAI, vol-ume 5, pages 1160?1165.
Citeseer.Yue Zhang and Stephen Clark.
2008a.
Joint word seg-mentation and pos tagging using a single perceptron.In ACL, pages 888?896.Yue Zhang and Stephen Clark.
2008b.
A tale oftwo parsers: investigating and combining graph-basedand transition-based dependency parsing using beam-search.
In Proceedings of the Conference on Empir-ical Methods in Natural Language Processing, pages562?571.
Association for Computational Linguistics.973Jie Zhou and Wei Xu.
2015.
End-to-end learning of se-mantic role labeling using recurrent neural networks.In Proceedings of the Annual Meeting of the Associa-tion for Computational Linguistics.Hao Zhou, Yue Zhang, and Jiajun Chen.
2015.
Aneural probabilistic structured-prediction model fortransition-based dependency parsing.
In Proceedingsof the 53rd Annual Meeting of the Association forComputational Linguistics, pages 1213?1222.974
