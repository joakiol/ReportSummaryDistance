Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 551?560,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPAn Empirical Study of Semi-supervised Structured Conditional Modelsfor Dependency ParsingJun Suzuki, Hideki IsozakiNTT CS Lab., NTT Corp.Kyoto, 619-0237, Japanjun@cslab.kecl.ntt.co.jpisozaki@cslab.kecl.ntt.co.jpXavier Carreras, and Michael CollinsMIT CSAILCambridge, MA 02139, USAcarreras@csail.mit.edumcollins@csail.mit.eduAbstractThis paper describes an empirical studyof high-performance dependency parsersbased on a semi-supervised learning ap-proach.
We describe an extension of semi-supervised structured conditional models(SS-SCMs) to the dependency parsingproblem, whose framework is originallyproposed in (Suzuki and Isozaki, 2008).Moreover, we introduce two extensions re-lated to dependency parsing: The first ex-tension is to combine SS-SCMs with an-other semi-supervised approach, describedin (Koo et al, 2008).
The second exten-sion is to apply the approach to second-order parsing models, such as those de-scribed in (Carreras, 2007), using a two-stage semi-supervised learning approach.We demonstrate the effectiveness of ourproposed methods on dependency parsingexperiments using two widely used testcollections: the Penn Treebank for En-glish, and the Prague Dependency Tree-bank for Czech.
Our best results ontest data in the above datasets achieve93.79% parent-prediction accuracy for En-glish, and 88.05% for Czech.1 IntroductionRecent work has successfully developed depen-dency parsing models for many languages us-ing supervised learning algorithms (Buchholz andMarsi, 2006; Nivre et al, 2007).
Semi-supervisedlearning methods, which make use of unlabeleddata in addition to labeled examples, have the po-tential to give improved performance over purelysupervised methods for dependency parsing.
Itis often straightforward to obtain large amountsof unlabeled data, making semi-supervised ap-proaches appealing; previous work on semi-supervised methods for dependency parsing in-cludes (Smith and Eisner, 2007; Koo et al, 2008;Wang et al, 2008).In particular, Koo et al (2008) describe asemi-supervised approach that makes use of clus-ter features induced from unlabeled data, and givesstate-of-the-art results on the widely used depen-dency parsing test collections: the Penn Tree-bank (PTB) for English and the Prague Depen-dency Treebank (PDT) for Czech.
This is a verysimple approach, but provided significant perfor-mance improvements comparing with the state-of-the-art supervised dependency parsers such as(McDonald and Pereira, 2006).This paper introduces an alternative method forsemi-supervised learning for dependency parsing.Our approach basically follows a framework pro-posed in (Suzuki and Isozaki, 2008).
We extend itfor dependency parsing, which we will refer to asa Semi-supervised Structured Conditional Model(SS-SCM).
In this framework, a structured condi-tional model is constructed by incorporating a se-ries of generative models, whose parameters areestimated from unlabeled data.
This paper de-scribes a basic method for learning within this ap-proach, and in addition describes two extensions.The first extension is to combine our method withthe cluster-based semi-supervised method of (Kooet al, 2008).
The second extension is to apply theapproach to second-order parsing models, morespecifically the model of (Carreras, 2007), usinga two-stage semi-supervised learning approach.We conduct experiments on dependency parsingof English (on Penn Treebank data) and Czech (onthe Prague Dependency Treebank).
Our experi-ments investigate the effectiveness of: 1) the basicSS-SCM for dependency parsing; 2) a combina-tion of the SS-SCM with Koo et al (2008)?s semi-supervised approach (even in the case we used thesame unlabeled data for both methods); 3) the two-stage semi-supervised learning approach that in-551corporates a second-order parsing model.
In ad-dition, we evaluate the SS-SCM for English de-pendency parsing with large amounts (up to 3.72billion tokens) of unlabeled data .2 Semi-supervised StructuredConditional Models for DependencyParsingSuzuki et al (2008) describe a semi-supervisedlearning method for conditional random fields(CRFs) (Lafferty et al, 2001).
In this paper weextend this method to the dependency parsingproblem.
We will refer to this extended methodas Semi-supervised Structured Conditional Mod-els (SS-SCMs).
The remainder of this section de-scribes our approach.2.1 The Basic ModelThroughout this paper we will use x to denote aninput sentence, and y to denote a labeled depen-dency structure.
Given a sentence x with n words,a labeled dependency structure y is a set of n de-pendencies of the form (h,m, l), where h is theindex of the head-word in the dependency, m isthe index of the modifier word, and l is the labelof the dependency.
We use h = 0 for the root ofthe sentence.
We assume access to a set of labeledtraining examples, {xi,yi}Ni=1, and in addition aset of unlabeled examples, {x?i}Mi=1.In conditional log-linear models for dependencyparsing (which are closely related to conditionalrandom fields (Lafferty et al, 2001)), a distribu-tion over dependency structures for a sentence xis defined as follows:p(y|x) =1Z(x)exp{g(x,y)}, (1)where Z(x) is the partition function, w is a pa-rameter vector, andg(x,y) =?
(h,m,l)?yw ?
f(x, h,m, l)Here f(x, h,m, l) is a feature vector represent-ing the dependency (h,m, l) in the context of thesentence x (see for example (McDonald et al,2005a)).In this paper we extend the definition of g(x,y)to include features that are induced from unlabeleddata.
Specifically, we defineg(x,y) =?
(h,m,l)?yw ?
f(x, h,m, l)+?
(h,m,l)?yk?j=1vjqj(x, h,m, l).
(2)In this model v1, .
.
.
, vkare scalar parameters thatmay be positive or negative; q1.
.
.
qkare func-tions (in fact, generative models), that are trainedon unlabeled data.
The vjparameters will dictatethe relative strengths of the functions q1.
.
.
qk, andwill be trained on labeled data.For convenience, we will use v to refer to thevector of parameters v1.
.
.
vk, and q to refer to theset of generative models q1.
.
.
qk.
The full modelis specified by values for w,v, and q.
We willwrite p(y|x;w,v,q) to refer to the conditionaldistribution under parameter values w,v,q.We will describe a three-step parameter estima-tion method that: 1) initializes the q functions(generative models) to be uniform distributions,and estimates parameter values w and v from la-beled data; 2) induces new functions q1.
.
.
qkfromunlabeled data, based on the distribution definedby the w,v,q values from step (1); 3) re-estimatesw and v on the labeled examples, keeping theq1.
.
.
qkfrom step (2) fixed.
The end result is amodel that combines supervised training with gen-erative models induced from unlabeled data.2.2 The Generative ModelsWe now describe how the generative modelsq1.
.
.
qkare defined, and how they are inducedfrom unlabeled data.
These models make directuse of the feature-vector definition f(x,y) used inthe original, fully supervised, dependency parser.The first step is to partition the d fea-tures in f(x,y) into k separate feature vectors,r1(x,y) .
.
.
rk(x,y) (with the result that f is theconcatenation of the k feature vectors r1.
.
.
rk).
Inour experiments on dependency parsing, we parti-tioned f into up to over 140 separate feature vec-tors corresponding to different feature types.
Forexample, one feature vector rjmight include onlythose features corresponding to word bigrams in-volved in dependencies (i.e., indicator functionstied to the word bigram (xm, xh) involved in a de-pendency (x, h,m, l)).We then define a generative model that assignsa probabilityq?j(x, h,m, l) =dj?a=1?rj,a(x,h,m,l)j,a(3)to the dj-dimensional feature vector rj(x, h,m, l).The parameters of this model are ?j,1.
.
.
?j,dj;552they form a multinomial distribution, with the con-straints that ?j,a?
0, and?a?j,a= 1.
Thismodel can be viewed as a very simple (naive-Bayes) model that defines a distribution over fea-ture vectors rj?
Rdj.
The next section describeshow the parameters ?j,aare trained on unlabeleddata.Given parameters ?j,a, we can simply define thefunctions q1.
.
.
qkto be log probabilities under thegenerative model:qj(x, h,m, l) = log q?j(x, h,m, l)=dj?a=1rj,a(x, h,m, l) log ?j,a.We modify this definition slightly, be introducingscaling factors cj,a> 0, and definingqj(x, h,m, l) =dj?a=1rj,a(x, h,m, l) log?j,acj,a(4)In our experiments, cj,ais simply a count of thenumber of times the feature indexed by (j, a) ap-pears in unlabeled data.
Thus more frequent fea-tures have their contribution down-weighted in themodel.
We have found this modification to be ben-eficial.2.3 Estimating the Parameters of theGenerative ModelsWe now describe the method for estimating theparameters ?j,aof the generative models.
Weassume initial parameters w,v,q, which definea distribution p(y|x?i;w,v,q) over dependencystructures for each unlabeled example x?i.
We willre-estimate the generative models q, based on un-labeled examples.
The likelihood function on un-labeled data is defined asM?i=1?yp(y|x?i;w,v,q)?
(h,m,l)?ylog q?j(x?i, h,m, l),(5)where q?jis as defined in Eq.
3.
This function re-sembles the Q function used in the EM algorithm,where the hidden labels (in our case, dependencystructures), are filled in using the conditional dis-tribution p(y|x?i;w,v,q).It is simple to show that the estimates ?j,athatmaximize the function in Eq.
5 can be defined asfollows.
First, define a vector of expected countsbased on w,v,q as?rj=M?i=1?yp(y|x?i;w,v,q)?
(h,m,l)?yrj(x?i, h,m, l).Note that it is straightforward to calculate these ex-pected counts using a variant of the inside-outsidealgorithm (Baker, 1979) applied to the (Eisner,1996) dependency-parsing data structures (Paskin,2001) for projective dependency structures, or thematrix-tree theorem (Koo et al, 2007; Smith andSmith, 2007; McDonald and Satta, 2007) for non-projective dependency structures.The estimates that maximize Eq.
5 are then?j,a=r?j,a?dja=1r?j,a.In a slight modification, we employ the follow-ing estimates in our model, where ?
> 1 is a pa-rameter of the model:?j,a=(?
?
1) + r?j,adj?
(?
?
1) +?dja=1r?j,a.
(6)This corresponds to a MAP estimate under aDirichlet prior over the ?j,aparameters.2.4 The Complete Parameter-EstimationMethodThis section describes the full parameter estima-tion method.
The input to the algorithm is a setof labeled examples {xi,yi}Ni=1, a set of unla-beled examples {x?i}Mi=1, a feature-vector defini-tion f(x,y), and a partition of f into k feature vec-tors r1.
.
.
rkwhich underly the generative mod-els.
The output from the algorithm is a parametervector w, a set of generative models q1.
.
.
qk, andparameters v1.
.
.
vk, which define a probabilisticdependency parsing model through Eqs.
1 and 2.The learning algorithm proceeds in three steps:Step 1: Estimation of a Fully SupervisedModel.
We choose the initial value q0of thegenerative models to be the uniform distribution,i.e., we set ?j,a= 1/djfor all j, a.
We then de-fine the regularized log-likelihood function for thelabeled examples, with the generative model fixedat q0, to be:L(w,v;q0) =n?i=1log p(yi|xi;w,v,q0)?C2(||w||2+ ||v||2)553This is a conventional regularized log-likelihoodfunction, as commonly used in CRF models.
Theparameter C > 0 dictates the level of regular-ization in the model.
We define the initial pa-rameters (w0,v0) = argmaxw,vL(w,v;q0).These parameters can be found using conventionalmethods for estimating the parameters of regu-larized log-likelihood functions (in our case weuse LBFGS (Liu and Nocedal, 1989)).
Note thatthe gradient of the log-likelihood function can becalculated using the inside-outside algorithm ap-plied to projective dependency parse structures, orthe matrix-tree theorem applied to non-projectivestructures.Step 2: Estimation of the Generative Mod-els.
In this step, expected count vectors?r1.
.
.
?rkare first calculated, based on the distributionp(y|x;w0,v0,q0).
Generative model parameters?j,aare calculated through the definition in Eq.
6;these estimates define updated generative modelsq1jfor j = 1 .
.
.
k through Eq.
4.
We refer to thenew values for the generative models as q1.Step 3: Re-estimation of w and v. Inthe final step, w1and v1are estimated asargmaxw,vL(w,v;q1) where L(w,v;q1) is de-fined in an analogous way to L(w,v;q0).
Thus wand v are re-estimated to optimize log-likelihoodof the labeled examples, with the generative mod-els q1estimated in step 2.The final output from the algorithm is the set ofparameters (w1,v1,q1).
Note that it is possible toiterate the method?steps 2 and 3 can be repeatedmultiple times (Suzuki and Isozaki, 2008)?butin our experiments we only performed these stepsonce.3 Extensions3.1 Incorporating Cluster-Based FeaturesKoo et al (2008) describe a semi-supervisedapproach that incorporates cluster-based features,and that gives competitive results on dependencyparsing benchmarks.
The method is a two-stageapproach.
First, hierarchical word clusters are de-rived from unlabeled data using the Brown et alclustering algorithm (Brown et al, 1992).
Sec-ond, a new feature set is constructed by represent-ing words by bit-strings of various lengths, corre-sponding to clusters at different levels of the hier-archy.
These features are combined with conven-tional features based on words and part-of-speechtags.
The new feature set is then used within aconventional discriminative, supervised approach,such as the averaged perceptron algorithm.The important point is that their approach usesunlabeled data only for the construction of a newfeature set, and never affects to learning algo-rithms.
It is straightforward to incorporate cluster-based features within the SS-SCM approach de-scribed in this paper.
We simply use the cluster-based feature-vector representation f(x,y) intro-duced by (Koo et al, 2008) as the basis of our ap-proach.3.2 Second-order Parsing ModelsPrevious work (McDonald and Pereira, 2006; Car-reras, 2007) has shown that second-order parsingmodels, which include information from ?sibling?or ?grandparent?
relationships between dependen-cies, can give significant improvements in accu-racy over first-order parsing models.
In principleit would be straightforward to extend the SS-SCMapproach that we have described to second-orderparsing models.
In practice, however, a bottle-neck for the method would be the estimation ofthe generative models on unlabeled data.
Thisstep requires calculation of marginals on unlabeleddata.
Second-order parsing models generally re-quire more costly inference methods for the cal-culation of marginals, and this increased cost maybe prohibitive when large quantities of unlabeleddata are employed.We instead make use of a simple ?two-stage?
ap-proach for extending the SS-SCM approach to thesecond-order parsing model of (Carreras, 2007).In the first stage, we use a first-order parsingmodel to estimate generative models q1.
.
.
qkfromunlabeled data.
In the second stage, we incorpo-rate these generative models as features within asecond-order parsing model.
More precisely, inour approach, we first train a first-order parsingmodel by Step 1 and 2, exactly as described inSection 2.4, to estimate w0, v0and q1.
Then,we substitute Step 3 as a supervised learning suchas MIRA with a second-order parsing model (Mc-Donald et al, 2005a), which incorporates q1as areal-values features.
We refer this two-stage ap-proach to as two-stage SS-SCM.In our experiments we use the 1-best MIRAalgorithm (McDonald and Pereira, 2006)1as a1We used a slightly modified version of 1-best MIRA,whose difference can be found in the third line in Eq.
7,namely, including L(yi,y).554(a) English dependency parsingData set (WSJ Sec.
IDs) # of sentences # of tokensTraining (02?21) 39,832 950,028Development (22) 1,700 40,117Test (23) 2,012 47,377Unlabeled 1,796,379 43,380,315(b) Czech dependency parsingData set # of sentences # of tokensTraining 73,088 1,255,590Development 7,507 126,030Test 7,319 125,713Unlabeled 2,349,224 39,336,570Table 1: Details of training, development, test data(labeled data sets) and unlabeled data used in ourexperimentsparameter-estimation method for the second-orderparsing model.
In particular, we perform the fol-lowing optimizations on each update t = 1, ..., Tfor re-estimating w and v:min ||w(t+1)?w(t)||+ ||v(t+1)?
v(t)||s.t.
S(xi,yi)?
S(xi,?y) ?
L(yi,?y)?y = argmaxyS(xi,y) + L(yi,y),(7)where L(yi,y) represents the loss between correctoutput of i?th sample yiand y.
Then, the scoringfunction S for each y can be defined as follows:S(x,y) =w ?
(f1(x,y) + f2(x,y))+Bk?j=1vjqj(x,y),(8)where B represents a tunable scaling factor, andf1and f2represent the feature vectors of first andsecond-order parsing parts, respectively.4 ExperimentsWe now describe experiments investigating the ef-fectiveness of the SS-SCM approach for depen-dency parsing.
The experiments test basic, first-order parsing models, as well as the extensionsto cluster-based features and second-order parsingmodels described in the previous section.4.1 Data SetsWe conducted experiments on both English andCzech data.
We used the Wall Street Journalsections of the Penn Treebank (PTB) III (Mar-cus et al, 1994) as a source of labeled data forEnglish, and the Prague Dependency Treebank(PDT) 1.0 (Haji?c, 1998) for Czech.
To facili-tate comparisons with previous work, we used ex-actly the same training, development and test setsCorpus article name (mm/yy) # of sent.
# of tokensBLLIP wsj 00/87?00/89 1,796,379 43,380,315Tipster wsj 04/90?03/92 1,550,026 36,583,547North wsj 07/94?12/96 2,748,803 62,937,557American reu 04/94?07/96 4,773,701 110,001,109Reuters reu 09/96?08/97 12,969,056 214,708,766English afp 05/94?12/06 21,231,470 513,139,928Gigaword apw 11/94?12/06 46,978,725 960,733,303ltw 04/94?12/06 10,524,545 230,370,454nyt 07/94?12/06 60,752,363 1,266,531,274xin 01/95?12/06 12,624,835 283,579,330total 175,949,903 3,721,965,583Table 2: Details of the larger unlabeled data setused in English dependency parsing: sentences ex-ceeding 128 tokens in length were excluded forcomputational reasons.as those described in (McDonald et al, 2005a;McDonald et al, 2005b; McDonald and Pereira,2006; Koo et al, 2008).
The English dependency-parsing data sets were constructed using a stan-dard set of head-selection rules (Yamada and Mat-sumoto, 2003) to convert the phrase structure syn-tax of the Treebank to dependency tree repre-sentations.
We split the data into three parts:sections 02-21 for training, section 22 for de-velopment and section 23 for test.
The Czechdata sets were obtained from the predefined train-ing/development/test partition in the PDT.
The un-labeled data for English was derived from theBrown Laboratory for Linguistic Information Pro-cessing (BLLIP) Corpus (LDC2000T43)2, givinga total of 1,796,379 sentences and 43,380,315tokens.
The raw text section of the PDT wasused for Czech, giving 2,349,224 sentences and39,336,570 tokens.
These data sets are identicalto the unlabeled data used in (Koo et al, 2008),and are disjoint from the training, developmentand test sets.
The datasets used in our experimentsare summarized in Table 1.In addition, we will describe experiments thatmake use of much larger amounts of unlabeleddata.
Unfortunately, we have no data availableother than PDT for Czech, this is done only forEnglish dependency parsing.
Table 2 shows thedetail of the larger unlabeled data set used in ourexperiments, where we eliminated sentences thathave more than 128 tokens for computational rea-sons.
Note that the total size of the unlabeled datareaches 3.72G (billion) tokens, which is approxi-2We ensured that the sentences used in the PTB wereexcluded from the unlabeled data, since sentences used inBLLIP corpus are a super-set of the PTB.555mately 4,000 times larger than the size of labeledtraining data.4.2 Features4.2.1 Baseline FeaturesIn general we will assume that the input sentencesinclude both words and part-of-speech (POS) tags.Our baseline features (?baseline?)
are very simi-lar to those described in (McDonald et al, 2005a;Koo et al, 2008): these features track word andPOS bigrams, contextual features surrounding de-pendencies, distance features, and so on.
En-glish POS tags were assigned by MXPOST (Rat-naparkhi, 1996), which was trained on the train-ing data described in Section 4.1.
Czech POS tagswere obtained by the following two steps: First,we used ?feature-based tagger?
included with thePDT3, and then, we used the method described in(Collins et al, 1999) to convert the assigned richPOS tags into simplified POS tags.4.2.2 Cluster-based FeaturesIn a second set of experiments, we make use of thefeature set used in the semi-supervised approachof (Koo et al, 2008).
We will refer to this as the?cluster-based feature set?
(CL).
The BLLIP (43Mtokens) and PDT (39M tokens) unlabeled data setsshown in Table 1 were used to construct the hierar-chical clusterings used within the approach.
Notethat when this feature set is used within the SS-SCM approach, the same set of unlabeled data isused to both induce the clusters, and to estimatethe generative models within the SS-SCM model.4.2.3 Constructing the Generative ModelsAs described in section 2.2, the generative mod-els in the SS-SCM approach are defined througha partition of the original feature vector f(x,y)into k feature vectors r1(x,y) .
.
.
rk(x,y).
Wefollow a similar approach to that of (Suzuki andIsozaki, 2008) in partitioning f(x,y), where thek different feature vectors correspond to differentfeature types or feature templates.
Note that, ingeneral, we are not necessary to do as above, thisis one systematic way of a feature design for thisapproach.4.3 Other Experimental SettingsAll results presented in our experiments are givenin terms of parent-prediction accuracy on unla-3Training, development, and test data in PDT already con-tains POS tags assigned by the ?feature-based tagger?.beled dependency parsing.
We ignore the parent-predictions of punctuation tokens for English,while we retain all the punctuation tokens forCzech.
These settings match the evaluation settingin previous work such as (McDonald et al, 2005a;Koo et al, 2008).We used the method proposed by (Carreras,2007) for our second-order parsing model.
Sincethis method only considers projective dependencystructures, we ?projectivized?
the PDT trainingdata in the same way as (Koo et al, 2008).
Weused a non-projective model, trained using an ap-plication of the matrix-tree theorem (Koo et al,2007; Smith and Smith, 2007; McDonald andSatta, 2007) for the first-order Czech models, andprojective parsers for all other models.As shown in Section 2, SS-SCMs with 1st-orderparsing models have two tunable parameters, Cand ?, corresponding to the regularization con-stant, and the Dirichlet prior for the generativemodels.
We selected a fixed value ?
= 2, whichwas found to work well in preliminary experi-ments.4The value of C was chosen to optimizeperformance on development data.
Note that Cfor supervised SCMs were also tuned on develop-ment data.
For the two-stage SS-SCM for incor-porating second-order parsing model, we have ad-ditional one tunable parameter B shown in Eq.
8.This was also chosen by the value that providedthe best performance on development data.In addition to providing results for modelstrained on the full training sets, we also performedexperiments with smaller labeled training sets.These training sets were either created throughrandom sampling or by using a predefined subsetof document IDs from the labeled training data.5 Results and DiscussionTable 3 gives results for the SS-SCM method un-der various configurations: for first and second-order parsing models, with and without the clus-ter features of (Koo et al, 2008), and for varyingamounts of labeled data.
The remainder of thissection discusses these results in more detail.5.1 Effects of the Quantity of Labeled DataWe can see from the results in Table 3 that oursemi-supervised approach consistently gives gains4An intuitive meaning of ?
= 2 is that this adds onepseudo expected count to every feature when estimating newparameter values.556(a) English dependency parsing: w/ 43M token unlabeled data (BLLIP)WSJ sec.
IDs wsj 21 random selection random selection wsj 15?18 wsj 02-21(all)# of sentences / tokens 1,671 / 40,039 2,000 / 48,577 8,000 / 190,958 8,936 / 211,727 39,832 / 950,028feature type baseline CL baseline CL baseline CL baseline CL baseline CLSupervised SCM (1od) 85.63 86.80 87.02 88.05 89.23 90.45 89.43 90.85 91.21 92.53SS-SCM (1od) 87.16 88.40 88.07 89.55 90.06 91.45 90.23 91.63 91.72 93.01(gain over Sup.
SCM) (+1.53) (+1.60) (+1.05) (+1.50) (+0.83) (+1.00) (+0.80) (+0.78) (+0.51) (+0.48)Supervised MIRA (2od) 87.99 89.05 89.20 90.06 91.20 91.75 91.50 92.14 93.02 93.542-stage SS-SCM(+MIRA) (2od) 88.88 89.94 90.03 90.90 91.73 92.51 91.95 92.73 93.45 94.13(gain over Sup.
MIRA) (+0.89) (+0.89) (+0.83) (+0.84) (+0.53) (+0.76) (+0.45) (+0.59) (+0.43) (+0.59)(b) Czech dependency parsing: w/ 39M token unlabeled data (PDT)PDT Doc.
IDs random selection c[0-9]* random selection l[a-i]* (all)# of sentences / tokens 2,000 / 34,722 3,526 / 53,982 8,000 / 140,423 14,891 / 261,545 73,008 /1,225,590feature type baseline CL baseline CL baseline CL baseline CL baseline CLSupervised SCM (1od) 75.67 77.82 76.88 79.24 80.61 82.85 81.94 84.47 84.43 86.72SS-SCM (1od) 76.47 78.96 77.61 80.28 81.30 83.49 82.74 84.91 85.00 87.03(gain over Sup.
SCM) (+0.80) (+1.14) (+0.73) (+1.04) (+0.69) (+0.64) (+0.80) (+0.44) (+0.57) (+0.31)Supervised MIRA (2od) 78.19 79.60 79.58 80.77 83.15 84.39 84.27 85.75 86.82 87.762-stage SS-SCM(+MIRA) (2od) 78.71 80.09 80.37 81.40 83.61 84.87 84.95 86.00 87.03 88.03(gain over Sup.
MIRA) (+0.52) (+0.49) (+0.79) (+0.63) (+0.46) (+0.48) (+0.68) (+0.25) (+0.21) (+0.27)Table 3: Dependency parsing results for the SS-SCM method with different amounts of labeled trainingdata.
Supervised SCM (1od) and Supervised MIRA (2od) are the baseline first and second-order ap-proaches; SS-SCM (1od) and 2-stage SS-SCM(+MIRA) (2od) are the first and second-order approachesdescribed in this paper.
?Baseline?
refers to models without cluster-based features, ?CL?
refers to modelswhich make use of cluster-based features.in performance under various sizes of labeled data.Note that the baseline methods that we have usedin these experiments are strong baselines.
It isclear that the gains from our method are larger forsmaller labeled data sizes, a tendency that was alsoobserved in (Koo et al, 2008).5.2 Impact of Combining SS-SCM withCluster FeaturesOne important observation from the results in Ta-ble 3 is that SS-SCMs can successfully improvethe performance over a baseline method that usesthe cluster-based feature set (CL).
This is in spiteof the fact that the generative models within theSS-SCM approach were trained on the same un-labeled data used to induce the cluster-based fea-tures.5.3 Impact of the Two-stage ApproachTable 3 also shows the effectiveness of the two-stage approach (described in Section 3.2) that inte-grates the SS-SCM method within a second-orderparser.
This suggests that the SS-SCM methodcan be effective in providing features (generativemodels) used within a separate learning algorithm,providing that this algorithm can make use of real-valued features.91.592.092.593.093.510 100 1,000 10,000CLbaseline43.4M 143M468M 1.38G3.72G(Mega tokens)Unlabeled data size: [Log-scale]Parent-predictionAccuracy(BLLIP)Figure 1: Impact of unlabeled data size for the SS-SCM on development data of English dependencyparsing.5.4 Impact of the Amount of Unlabeled DataFigure 1 shows the dependency parsing accuracyon English as a function of the amount of unla-beled data used within the SS-SCM approach.
(Asdescribed in Section 4.1, we have no unlabeleddata other than PDT for Czech, hence this sectiononly considers English dependency parsing.)
Wecan see that performance does improve as moreunlabeled data is added; this trend is seen bothwith and without cluster-based features.
In addi-tion, Table 4 shows the performance of our pro-posed method using 3.72 billion tokens of unla-557feature type baseline CLSS-SCM (1st-order) 92.23 93.23(gain over Sup.
SCM) (+1.02) (+0.70)2-stage SS-SCM(+MIRA) (2nd-order) 93.68 94.26(gain over Sup.
MIRA) (+0.66) (+0.72)Table 4: Parent-prediction accuracies on develop-ment data with 3.72G tokens unlabeled data forEnglish dependency parsing.beled data.
Note, however, that the gain in perfor-mance as unlabeled data is added is not as sharpas might be hoped, with a relatively modest dif-ference in performance for 43.4 million tokens vs.3.72 billion tokens of unlabeled data.5.5 Computational EfficiencyThe main computational challenge in our ap-proach is the estimation of the generative mod-els q = ?q1.
.
.
qk?
from unlabeled data, partic-ularly when the amount of unlabeled data usedis large.
In our implementation, on the 43M to-ken BLLIP corpus, using baseline features, it takesabout 5 hours to compute the expected counts re-quired to estimate the parameters of the generativemodels on a single 2.93GHz Xeon processor.
Ittakes roughly 18 days of computation to estimatethe generative models from the larger (3.72 billionword) corpus.
Fortunately it is simple to paral-lelize this step; our method takes a few hours onthe larger data set when parallelized across around300 separate processes.Note that once the generative models have beenestimated, decoding with the model, or train-ing the model on labeled data, is relatively in-expensive, essentially taking the same amount ofcomputation as standard dependency-parsing ap-proaches.5.6 Results on Test DataFinally, Table 5 displays the final results on testdata.
There results are obtained using the bestsetting in terms of the development data perfor-mance.
Note that the English dependency pars-ing results shown in the table were achieved us-ing 3.72 billion tokens of unlabeled data.
The im-provements on test data are similar to those ob-served on the development data.
To determinestatistical significance, we tested the difference ofparent-prediction error-rates at the sentence levelusing a paired Wilcoxon signed rank test.
All eightcomparisons shown in Table 5 are significant with(a) English dependency parsing: w/ 3.72G token ULDfeature set baseline CLSS-SCM (1st-order) 91.89 92.70(gain over Sup.
SCM) (+0.92) (+0.58)2-stage SS-SCM(+MIRA) (2nd-order) 93.41 93.79(gain over Sup.
MIRA) (+0.65) (+0.48)(b) Czech dependency parsing: w/ 39M token ULD (PDT)feature set baseline CLSS-SCM (1st-order) 84.98 87.14(gain over Sup.
SCM) (+0.58) (+0.39)2-stage SS-SCM(+MIRA) (2nd-order) 86.90 88.05(gain over Sup.
MIRA) (+0.15) (+0.36)Table 5: Parent-prediction accuracies on test datausing the best setting in terms of development dataperformances in each condition.
(a) English dependency parsers on PTBdependency parser test description(McDonald et al, 2005a) 90.9 1od(McDonald and Pereira, 2006) 91.5 2od(Koo et al, 2008) 92.23 1od, 43M ULDSS-SCM (w/ CL) 92.70 1od, 3.72G ULD(Koo et al, 2008) 93.16 2od, 43M ULD2-stage SS-SCM(+MIRA, w/ CL) 93.79 2od, 3.72G ULD(b) Czech dependency parsers on PDTdependency parser test description(McDonald et al, 2005b) 84.4 1od(McDonald and Pereira, 2006) 85.2 2od(Koo et al, 2008) 86.07 1od, 39M ULD(Koo et al, 2008) 87.13 2od, 39M ULDSS-SCM (w/ CL) 87.14 1od, 39M ULD2-stage SS-SCM(+MIRA, w/ CL) 88.05 2od, 39M ULDTable 6: Comparisons with the previous top sys-tems: (1od, 2od: 1st- and 2nd-order parsingmodel, ULD: unlabeled data).p < 0.01.6 Comparison with Previous MethodsTable 6 shows the performance of a number ofstate-of-the-art approaches on the English andCzech data sets.
For both languages our ap-proach gives the best reported figures on thesedatasets.
Our results yield relative error reduc-tions of roughly 27% (English) and 20% (Czech)over McDonald and Pereira (2006)?s second-ordersupervised dependency parsers, and roughly 9%(English) and 7% (Czech) over the previous bestresults provided by Koo et.
al.
(2008)?s second-order semi-supervised dependency parsers.Note that there are some similarities betweenour two-stage semi-supervised learning approachand the semi-supervised learning method intro-duced by (Blitzer et al, 2006), which is an exten-sion of the method described by (Ando and Zhang,5582005).
In particular, both methods use a two-stageapproach; They first train generative models orauxiliary problems from unlabeled data, and then,they incorporate these trained models into a super-vised learning algorithm as real valued features.Moreover, both methods make direct use of exist-ing feature-vector definitions f(x,y) in inducingrepresentations from unlabeled data.7 ConclusionThis paper has described an extension of thesemi-supervised learning approach of (Suzuki andIsozaki, 2008) to the dependency parsing problem.In addition, we have described extensions that in-corporate the cluster-based features of Koo et al(2008), and that allow the use of second-orderparsing models.
We have described experimentsthat show that the approach gives significant im-provements over state-of-the-art methods for de-pendency parsing; performance improves whenthe amount of unlabeled data is increased from43.8 million tokens to 3.72 billion tokens.
The ap-proach should be relatively easily applied to lan-guages other than English or Czech.We stress that the SS-SCM approach requiresrelatively little hand-engineering: it makes di-rect use of the existing feature-vector representa-tion f(x,y) used in a discriminative model, anddoes not require the design of new features.
Themain choice in the approach is the partitioningof f(x,y) into components r1(x,y) .
.
.
rk(x,y),which in our experience is straightforward.ReferencesR.
Kubota Ando and T. Zhang.
2005.
A Framework forLearning Predictive Structures from Multiple Tasksand Unlabeled Data.
Journal of Machine LearningResearch, 6:1817?1853.J.
K. Baker.
1979.
Trainable Grammars for SpeechRecognition.
In Speech Communication Papers forthe 97th Meeting of the Acoustical Society of Amer-ica, pages 547?550.J.
Blitzer, R. McDonald, and F. Pereira.
2006.
DomainAdaptation with Structural Correspondence Learn-ing.
In Proc.
of EMNLP-2006, pages 120?128.P.
F. Brown, P. V. deSouza, R. L. Mercer, V. J. DellaPietra, and J. C. Lai.
1992.
Class-based n-gramModels of Natural Language.
Computational Lin-guistics, 18(4):467?479.S.
Buchholz and E. Marsi.
2006.
CoNLL-X SharedTask on Multilingual Dependency Parsing.
In Proc.of CoNLL-X, pages 149?164.X.
Carreras.
2007.
Experiments with a Higher-OrderProjective Dependency Parser.
In Proc.
of EMNLP-CoNLL, pages 957?961.M.
Collins, J. Hajic, L. Ramshaw, and C. Tillmann.1999.
A Statistical Parser for Czech.
In Proc.
ofACL, pages 505?512.J.
Eisner.
1996.
Three New Probabilistic Models forDependency Parsing: An Exploration.
In Proc.
ofCOLING-96, pages 340?345.Jan Haji?c.
1998.
Building a Syntactically AnnotatedCorpus: The Prague Dependency Treebank.
In Is-sues of Valency and Meaning.
Studies in Honor ofJarmila Panevov?a, pages 12?19.
Prague Karolinum,Charles University Press.T.
Koo, A. Globerson, X. Carreras, and M. Collins.2007.
Structured Prediction Models via the Matrix-Tree Theorem.
In Proc.
of EMNLP-CoNLL, pages141?150.T.
Koo, X. Carreras, and M. Collins.
2008.
SimpleSemi-supervised Dependency Parsing.
In Proc.
ofACL-08: HLT, pages 595?603.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Condi-tional Random Fields: Probabilistic Models for Seg-menting and Labeling Sequence Data.
In Proc.
ofICML-2001, pages 282?289.D.
C. Liu and J. Nocedal.
1989.
On the LimitedMemory BFGS Method for Large Scale Optimiza-tion.
Math.
Programming, Ser.
B, 45(3):503?528.M.
P. Marcus, B. Santorini, and M. A. Marcinkiewicz.1994.
Building a Large Annotated Corpus of En-glish: The Penn Treebank.
Computational Linguis-tics, 19(2):313?330.R.
McDonald and F. Pereira.
2006.
Online Learning ofApproximate Dependency Parsing Algorithms.
InProc.
of EACL, pages 81?88.R.
McDonald and G. Satta.
2007.
On the Com-plexity of Non-Projective Data-Driven DependencyParsing.
In Proc.
of IWPT, pages 121?132.R.
McDonald, K. Crammer, and F. Pereira.
2005a.
On-line Large-margin Training of Dependency Parsers.In Proc.
of ACL, pages 91?98.R.
McDonald, F. Pereira, K. Ribarov, and J. Haji?c.2005b.
Non-projective Dependency Parsing us-ing Spanning Tree Algorithms.
In Proc.
of HLT-EMNLP, pages 523?530.J.
Nivre, J.
Hall, S. K?ubler, R. McDonald, J. Nilsson,S.
Riedel, and D. Yuret.
2007.
The CoNLL 2007Shared Task on Dependency Parsing.
In Proc.
ofEMNLP-CoNLL, pages 915?932.Mark A. Paskin.
2001.
Cubic-time Parsing and Learn-ing Algorithms for Grammatical Bigram.
Technicalreport, University of California at Berkeley, Berke-ley, CA, USA.559A.
Ratnaparkhi.
1996.
A Maximum Entropy Modelfor Part-of-Speech Tagging.
In Proc.
of EMNLP,pages 133?142.D.
A. Smith and J. Eisner.
2007.
BootstrappingFeature-Rich Dependency Parsers with Entropic Pri-ors.
In Proc.
of EMNLP-CoNLL, pages 667?677.D.
A. Smith and N. A. Smith.
2007.
Probabilis-tic Models of Nonprojective Dependency Trees.
InProc.
of EMNLP-CoNLL, pages 132?140.J.
Suzuki and H. Isozaki.
2008.
Semi-supervisedSequential Labeling and Segmentation Using Giga-Word Scale Unlabeled Data.
In Proc.
of ACL-08:HLT, pages 665?673.Q.
I. Wang, D. Schuurmans, and D. Lin.
2008.
Semi-supervised Convex Training for Dependency Pars-ing.
In Proc.
of ACL-08: HLT, pages 532?540.H.
Yamada and Y. Matsumoto.
2003.
Statistical De-pendency Analysis with Support Vector Machines.In Proc.
of IWPT.560
