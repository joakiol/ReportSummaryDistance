Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 129?135,Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational LinguisticsLearning to Model Multilingual Unrestricted Coreference in OntoNotesBaoli LIDepartment of Computer ScienceHenan University of Technology1 Lotus Street, High&New TechnologyIndustrial Development Zone, Zhengzhou,Henan, China, 450001csblli@gmail.comAbstractCoreference resolution, which aims atcorrectly linking meaningful expressions intext, is a much challenging problem inNatural Language Processing community.This paper describes the multilingualcoreference modeling system of WebInformation Processing Group, HenanUniversity of Technology, China, for theCoNLL-2012 shared task (closed track).The system takes a supervised learningstrategy, and consists of two cascadedcomponents: one for detecting mentions,and the other for clustering mentions.
Tomake the system applicable for multiplelanguages, generic syntactic and semanticfeatures are used to model coreference intext.
The system obtained combinedofficial score 41.88 over three languages(Arabic, Chinese, and English) and ranked7th among the 15 systems in the closedtrack.1 IntroductionCoreference resolution, which aims at correctlylinking meaningful expressions in text, has becomea central research problem in natural languageprocessing community with the advent of varioussupporting resources (e.g.
corpora and differentkinds of knowledge bases).
OntoNotes (Pradhan etal.
2007), compared to MUC (Chinchor, 2001;Chinchor and Sundheim, 2003) and ACE(Doddington et al 2000) corpora, is a large-scale,multilingual corpus for general anaphoriccoreference that covers entities and events notlimited to noun phrases or a limited set of entitytypes.
It greatly stimulates the research on thischallenging problem ?
Coreference Resolution.Moreover, resources like WordNet (Miller, 1995)and the advancement of different kinds of syntacticand semantic analysis technologies, make itpossible to do in-depth research on this topic,which is demanded in most of natural languageprocessing applications, such as informationextraction, machine translation, question answering,summarization, and so on.Our group is exploring how to extractinformation from grain/cereal related Chinese textfor business intelligence.
This shared task providesa good platform for advancing our research on IErelated topics.
We experiment with a machinelearning strategy to model multilingual coreferencefor the CoNLL-2012 shared task (Pradhan et al2012).
Two steps are taken to detect coreference intext: mention detection and mention clustering.
Weconsider mentions that correspond to a word or aninternal node in a syntactic tree and ignore the restmentions, as we think a mention should be a validmeaningful unit of a sentence.
Maximal entropyalgorithm is used to model what a mention is andhow two mentions link to each other.
Genericfeatures are designed to facilitate these modeling.129Our official submission obtained combinedofficial score 41.88 over three languages (Arabic,Chinese, and English), which ranked the system 7thamong 15 systems participating the closed track.Our system performs poor on the Arabic data, andhas relatively high precision but low recall.The rest of this paper is organized as follows.Section 2 gives the overview of our system, whileSection 3 discusses the first component of oursystem for mention detection.
Section 4 explainshow our system links mentions.
We present ourexperiments and analyses in Section 5, andconclude in Section 6.Pre-processingMention DetectionMention ClusteringPost-processingPipelined Processing ModulesFigure 1.
System Architecture.2 System DescriptionFigure 1 gives the architecture of our CoNLL-2012system, which consists of four pipelinedprocessing modules: pre-processing, mentiondetection, mention clustering, and post-processing.Pre-processing: this module reads in the datafiles in CoNLL format and re-builds the syntacticand semantic analysis trees in memory.Mention Detection: this module choosespotential sub-structures on the syntactic parsingtrees and determines whether they are realmentions.Mention Clustering: this module comparespairs of mentions and links them together.Post-processing: this module removes singletonmentions and produces the final results.To facilitate the processing, the data files of thesame languages are combined together to form bigfiles for training, development, and testrespectively.Compared to the CoNLL-2011 shared task, thetask of this year focuses on the multilingualcapacity of a corefernece resolution system.
Weplan to take a generic solution for differentlanguages rather than customized approach tosome languages with special resources.
In otherwords, our official system didn?t take any specialprocessing for data of different languages but usedthe same strategy and feature sets for all threelanguages.Stanford?s Rule-based method succeeded inresolving the coreferences in English text last year(Pradhan et al 2011; Lee et al 2011).
Therefore,we planed to incorporate the results of a rule-basedsystem (simple or complex as the Stanford?ssystem) if available and derive some relevantfeatures for our machine learning engine.
However,due to limited time and resources, we failed toimplement in our official system such a solutionintegrating rules within the overall statistical model.Intuitively, mentions are meaningful sub-structures of sentences.
We thus assume that amention should be a word or a phrasal sub-structure of a parsing tree.
Mention detectionmodules focus on these mentions and ignore othersthat do not correspond to a valid phrasal sub-structure.A widely used machine learning algorithm insolving different NLP problems, Maximal Entropy(Berger et al1996), is used to model mentions anddetect links between them.
Compared with NaiveBayes algorithm, Maximum entropy does notassume statistical independence of the differentfeatures.
In our system, Le Zhang?s maximumentropy package (Zhang, 2006) is integrated.In the following two sections, we will detail thetwo critical modules: mention detection andmention clustering.3 Mention DetectionThis module determines all mentions in text.
Wetake the assumption that a mention should be avalid sub-structure of a sentence.3.1 Methods130We first choose potential mentions in text and thenuse statistical machine learning method to makefinal decisions.From the train and development datasets, wecould obtain a list of POS and syntactic structuretags that a mention usually has.
For example,below is given such a list for English data:POS_TAG     "NP" /*145765*/POS_TAG     "NML" /*910*/POS_TAG     "S" /*207*/POS_TAG     "VP" /*189*/POS_TAG     "ADVP" /*75*/POS_TAG     "FRAG" /*73*/POS_TAG     "WHNP" /*67*/POS_TAG     "ADJP" /*65*/POS_TAG     "QP" /*62*/POS_TAG     "INTJ" /*40*/POS_TAG     "PP" /*16*/POS_TAG     "SBAR" /*10*/POS_TAG     "WHADVP" /*7*/POS_TAG     "UCP" /*5*///POS_TAG     "SINV" /*1*///POS_TAG     "SBARQ" /*1*///POS_TAG     "RRC" /*1*///POS_TAG     "SQ" /*1*///POS_TAG     "LST" /*1*/SYN_TAG     "PRP$" /*14734*/SYN_TAG     "NNP" /*3642*/SYN_TAG     "VB" /*733*/SYN_TAG     "VBD" /*669*/SYN_TAG     "VBN" /*384*/SYN_TAG     "VBG" /*371*/SYN_TAG     "NN" /*306*/SYN_TAG     "VBZ" /*254*/SYN_TAG     "VBP" /*235*/SYN_TAG     "PRP" /*137*/SYN_TAG     "CD" /*132*/SYN_TAG     "DT" /*77*/SYN_TAG     "IN" /*64*/SYN_TAG     "NNS" /*57*/SYN_TAG     "JJ" /*52*/SYN_TAG     "RB" /*19*/SYN_TAG     "NNPS" /*17*/SYN_TAG     "UH" /*7*/SYN_TAG     "CC" /*7*/SYN_TAG     "NFP" /*5*/SYN_TAG     "XX" /*4*/SYN_TAG     "MD" /*3*/SYN_TAG     "JJR" /*2*/SYN_TAG     "POS" /*2*///SYN_TAG     "FW" /*1*///SYN_TAG     "ADD" /*1*/We remove tags rarely occurring in the datasets,such as FW and ADD for English and consider allwords and syntactic structures of the restcategories as potential mentions.To make a decision about whether a potentialmention is a real one or not, we use a maximalentropy classifier with a set of generic featuresconcerning the word or sub-structure itself and itssyntactic and semantic contexts.3.2 FeaturesThe features we used in this step for each potentialword or sub-structure include:a.
Source and Genre of a document; Speaker of asentence;b.
Level of the Node in the syntactic parsing tree;c. Named entity tag of the word or sub-structure;d. Its head predicates and types;e. Syntactic tag path to the root;f. Whether it?s part of a mention, named entity,or an argument;g. Features from its parent: syntactic tag, namedentity tag, how many children it has, whetherthe potential word or sub-structure is the leftmost child of it, the right most child, or middlechild; binary syntactic tag feature;h. Features from its direct left and right siblings:their syntactic tags, named entity tags, andbinary syntactic tag features;i.
Features from its children: its total  tokenlength, words, pos tags, lemma, frameset ID,and word sense, tag paths to the left and rightmost child;j.
Features from its direct neighbor (before andafter) tokens: words, pos tags, lemma,frameset ID, and word sense, and binaryfeatures of  pos tags;4 Mention ClusteringThis component clusters the detected mentions intogroup.4.1 MethodsFor each pair of detected mentions, we determinewhether they could be linked together with amaximal entropy classifier.
The clustering takes abest-of-all strategy and works as the followingalgorithm:INPUT: a list of mentions;OUTPUT: a splitting of the mentions intogroups;ALGORIHTM:1311.
For each detected mention ANAP from the last tothe first:1.1 Find its most likely linked antecedantANTE before ANAP1.2 if FOUND1.2.1 link all anaphors of ANAP to ANTE;1.2.2 link ANAP to ANTEFigure 2.
Algorithm for Clustering Detected MentionsWe used the probability value of the maximalentropy classifier?s output for weighting the linksbetween mentions.4.2 FeaturesThe features we used in this step include:a.
Source and Genre of a document; Speaker of asentence;b.
Sentence distance between the potentialantecedent and anaphor;c. Syntactic tag of them, whether they are leafnode or not in the parsing tree;d. Syntactic tag bi-grams of them, and whethertheir syntactic tags are identical;e. Named entity tags of them, bi-gram of thesetags, and whether they are identical;f. Syntactic tag path to root of them, bi-gram ofthese paths, and whether they are identical;g. Whether they are predicates;h. Features of anaphor: Its head predicates andtypes, words, pos tags, the words and pos tagsof the left/right 3 neighbor tokens, and bi-grams;i.
Features of antecedent: Its head predicates andtypes, words, pos tags, the words and pos tagsof the left/right 3 neighbor tokens, and bi-grams;j.
The number of identical words of theantecedent and the anaphor;k. The number of identical words in theneighbors (3 tokens before and after) of theantecedent and the anaphor.The above features include not only thosesuggested by Soon et al (2001), but also somecontext features, such as words within and out ofthe antecedent and the anaphor, and theoverlapping number of the context words.
Featuresabout Gender and number agreements are notconsidered in our official system, as we failed towork out a generic solution to include them for alldata of three different languages.5 Experiments5.1 DatasetsThe datasets of the CoNLL-2012 shared taskcontain three languages: Arabic (ARB), Chinese(CHN), and English (ENG).
No predicted namesand propositions are provided in the Arabic data,while no predicted names are given in the Chinesedata.Tables 1 and 2 show statistical information ofboth training and development datasets for eachlanguage.Language # of Doc.# ofSent.# ofMent.# ofmentionsthat do notcorrespondto a validphrasal sub-structureDev 44 950 3,317 262(7.9%)ARBTrain 359 7,422 27,590 2,176(7.9%)Dev 252 6,083 14,183 677(4.8%)CHNTrain 1,810 36,487 102,854 6,345(6.2%)Dev 343 9,603 19,156 661(3.5%)ENGTrain 2,802 75,187 155,560 4,639(3.0%)Table 1.
Statistical information of the three languagedatasets (train and development) (part 1).# ofsentences perdocument# of tokensper sentence LanguageAvg.
Max Avg.
MaxDev 21.59 41 29.82 160ARBTrain 20.67 78 32.70 384Dev 24.14 144 18.09 190CHNTrain 20.16 283 20.72 242Dev 28.00 127 16.98 186ENGTrain 26.83 188 17.28 210Table 2.
Statistical information of the three languagedatasets (train and development) (part 2).The total size of the uncompressed original datais about 384MB.
The English dataset is the largestone containing 3,145 documents (343+2802),84,790 sentences, and 174,716 mentions.
TheArabic dataset is the smallest one containing 403documents, 8,372 sentences, and 30,907 mentions.In the Arabic datasets, about 7.9% mentions do not132correspond to a valid phrasal sub-structure.
Thisnumber of the Chinese dataset is 6%, while that ofEnglish 3%.
These small percentages verify thatour assumption that a mention is expected to be avalid phrasal sub-structure is reasonable.The average numbers of sentences in adocument in the three language datasets areroughly 21, 22, and 27 respectively, while thelongest document that has 283 sentences is foundin the Chinese train dataset.
The average numbersof tokens in a sentence in the three languagedatasets are roughly 31, 19, and 17 respectively,while the longest sentence with 384 tokens isfound in the Arabic train dataset.5.2 Experimental ResultsFor producing the results on the test datasets, wecombined both train and development datasets fortraining maximal entropy classifiers.The official score adopted by CoNLL-2012 isthe unweighted average of scores on threelanguages, while for each language, the score isderived by averaging the three metrics MUC(Vilain et al 1995), B-CUBED (Bagga andBaldwin, 1998), and CEAF(E) (Constrained EntityAligned F-measure)(Luo, 2005) as follows:MUC + B-CUBED + CEAF (E)OFFICIAL SCORE =  ----------------------------------------3Our system achieved the combined official score42.32 over three languages (Arabic, Chinese, andEnglish).
On each of the three languages, thesystem obtained scores 33.53, 46.27, and 45.85respectively.
It performs poor on the Arabic dataset,but equally well on the Chinese and Englishdatasets.Tables 3, 4, and 5 give the detailed results onthree languages respectively.Metric Recall Precision F1MUC 10.77 55.60 18.05B-CUBED 36.17 93.34 52.14CEAF (M) 37.03 37.03 37.03CEAF (E) 55.45 20.95 30.41BLANC1 52.91 73.93 54.12OFFICIALSCORE NA NA 33.53Table 3.
Official results of our system on the Arabic testdataset.1For this metric, please refer to (Recasens and Hovy, 2011).Metric Recall Precision F1MUC 32.48 71.44 44.65B-CUBED 45.51 86.06 59.54CEAF (M) 45.70 45.70 45.70CEAF (E) 55.11 25.24 34.62BLANC 64.99 76.63 68.92OFFICIALSCORE NA NA 46.27Table 4.
Official results of our system on the Chinesetest dataset.Metric Recall Precision F1MUC 39.12 72.57 50.84B-CUBED 43.03 80.06 55.98CEAF (M) 41.97 41.97 41.97CEAF (E) 49.44 22.30 30.74BLANC 64.01 66.86 65.24OFFICIALSCORE NA NA 45.85Table 5.
Official results of our system on the Englishtest dataset.Comparing the detailed scores, we found thatour submitted system performs much poor on theMUC metric on the Arabic data.
It can onlyrecover 10.77% valid mentions.
As a whole, thesystem works well in precision perspective butpoor in recall perspective.Language Recall Precision F1Arabic 18.17 80.43 29.65Chinese 36.60 87.01 51.53English 45.78 86.72 59.93Table 6.
Mention Detection Scores on the test datasets.Table 6 shows the official mention detectionscores on the test datasets, which could beregarded as the performance upper bounds (MUCmetric) of the mention clustering component.Taking the mention detection results as a basis, themention clustering component could achieveroughly 60.88 (18.05/29.65), 86.65 (44.65/51.53),and 84.83 (50.84/59.93) for the Arabic, Chinese,and English data respectively.
It seems that theperformance of the whole system is highlybottlenecked by that of the mention detectioncomponent.
However, it may not be true as the taskrequires removing singleton mentions that do notrefer to any other mentions.
To examine how133singleton mentions affect the final scores, weconducted additional experiments on thedevelopment datasets.
Table 7 shows the mentiondetection scores on the dev datasets.
When weinclude the singletons, the mention detectionscores become 59, 63.75, and 71.27 from 31.46,53.99, and 59.16 for the three language datasetsrespectively.
They are reasonable and close tothose that we can get at the mention clusteringcomponent.
These analyses tell us that therequirement of removing singletons for scoringmay deserve further study.
At the same time, werealize that to get better performance we may needto re-design the feature sets (e.g.
including moreuseful features like gender and number) and trysome more powerful machine learning algorithmssuch as linear classification or Tree CRF (Bradleyand Guestrin, 2010).Recall Precision F1 Language-Sing +Sing -Sing +Sing -Sing +SingArabic 19.42 47.58 82.88 77.61 31.46 59Chinese 39.05 53.78 87.43 78.24 53.99 63.75English 44.9 65.2 86.67 78.58 59.16 71.27Table 7.
Mention Detection Scores on the development(Dev) datasets.
?-Sing?
means without singletons, whichis required by the task specification, while ?+Sing?means including singletons.Table 8.
F1 scores of the two supplementarysubmissions with additional gold mention boundariesand gold mentions respectively.Besides the official submission for the task withpredicted data, we also provide two supplementarysubmissions with gold mention boundaries andgold mentions respectively.
Table 8 summarizesthe scores of these two submissions.With gold mentions, our official system doesachieve better performance with gain of 8.77(50.65-41.88).
On Chinese data, we get the highestscore 61.61.
However, the system performs worsewhen the gold mention boundaries are available.The F1 score drops 2.62 from 41.88 to 39.26.
Weguess that more candidate mentions bring moredifficulties for the maximal entropy classifier tomake decisions.
The best-of-all strategy may notbe a good choice when a large number ofcandidates are available.
More efforts are requiredto explore the real reason behind the results.6 ConclusionsIn this paper, we describe our system for theCoNLL-2012 shared task ?
Modeling MultilingualUnrestricted Coreference in OntoNotes (closedtrack).
Our system was built on machine learningstrategy with a pipeline architecture, whichintegrated two cascaded components: mentiondetection and mention clustering.
The system relieson successful syntactic analyses, which means thatonly valid sub-structures of sentences areconsidered as potential mentions.Due to limited time and resources, we had notconducted thorough enough experiments to deriveoptimal solutions, but the system and theinvolvement in this challenge do provide a goodfoundation for further study.
It?s a success for us tofinish all the submissions on time.
In the future, weplan to focus on those mentions that do notcorrespond to a syntactic structure and considerintroducing virtual nodes for them.
We may alsoexplore different strategies when linking ananaphor and its antecedent.
In addition, maximalentropy may not be good enough for this kind oftask.
Therefore, we also plan to explore otherpowerful algorithms like large linear classificationand tree CRF (Bradley and Guestrin, 2010; Ramand Devi, 2012) in the future.AcknowledgmentsThis research was funded by HeNan ProvincialResearch Project on Fundamental and Cutting-Edge Technologies (No.
112300410007).
We usedthe Amazon Elastic Compute Cloud (Amazon EC2)web service in our experiments.
We thankAmazon.com for providing such great service fornot only industrial applications, but also academicresearch.With gold mentionboundaries (39.26)With gold mentions(50.65)ARB CHN ENG ARB CHN ENGMUC 11.30 38.70 38.21 33.31 66.13 60.45B-CUBED 54.25 59.27 59.51 53.74 66.84 57.18CEAF (M) 33.68 41.06 39.30 42.25 57.50 47.82CEAF (E) 28.84 31.86 31.39 34.81 46.83 36.58BLANC 51.46 61.47 61.33 57.96 73.47 67.12MD Score 29.78 51.90 51.08 52.58 77.73 72.75OfficialScore 31.46 43.28 43.04 40.62 59.93 51.40134ReferencesAmit Bagga and Breck Baldwin.
1998.
Algorithms forScoring Coreference Chains.
In Proceedings of theFirst International Conference on LanguageResources and Evaluation Workshop on LinguisticsCoreference.Adam L. Berger, Stephen A. Della Pietra, and Vincent J.Della Pietra.
1996.
A Maximal Entropy Approach toNatural Language Processing.
ComputationalLinguistics, 22(1):39-42.Joseph K. Bradley and Carlos Guestrin.
2010.
LearningTree Conditional Random Fields.
In Proceedings ofthe International Conference on Machine Learning(ICML-2010).Nancy Chinchor.
2001.
Message UnderstandingCoference (MUC) 7.
In LDC2001T02.Nancy Chinchor and Beth Sundheim.
2003.
MessageUnderstanding Conference (MUC) 6.
InLDC2003T13.G.G.
Doddington, A. Mitchell, M. Przybocki, L.Ramshaw, S. Strassell, and R. Weischedel.
2000.
TheAutomatic Content Extraction (ACE) Program: Tasks,Data, and Evaluation.
In Proceedings of LREC-2000.Heeyoung Lee, Yves Peirsman, Angei Chang,Nathanael Chambers, Mihai Surdeanu, Dan Jurafsky.2011.
Stanford?s Multi-Pass Sieve CoreferenceResolution System at the CoNLL-2011 Shared Task.In Proceedings of the 15th Conference onComputational Natural Language Learning: SharedTask.Xiaoqiang Luo.
2005.
On coreference resolutionperformance metrics.
In Proceedings of the HumanLanguage Technology Conference and Conferenceon Empirical Methods in Natural LanguageProcessing.George A. Miller.
1995.
WordNet: a Lexical Databasefor English.
Communications of the ACM.
38(11):39-41.Sameer Pradhan, Eduard Hovy, Mitch Marcus, MarthaPalmer, Lance Ramshaw, Ralph Weischedel.
2007.OntoNotes: A Unified Relational SemanticRepresentation.
International Journal of SemanticComputing, 1(4): 405-419.Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,Olga Uryupina, and Yuchen Zhang.
2012.
CoNLL-2012 Shared Task: Modeling MultilingualUnrestricted Coreference in OntoNotes.
InProceedings of the Sixteenth Conference onComputational Natural Language Learning (CoNLL2012): Shared Task.Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,Martha Palmer, Ralph Weischedel and Nianwen Xue.2011.
CoNLL-2011 Shared Task: ModelingUnrestricted Coreference in OntoNotes.
InProceedings of the 15th Conference on ComputationalNatural Language Learning: Shared Task, pp.
1-27.Vijay Sundar Ram R. and Sobha Lalitha Devi.
2012.Coreference Resolution Using Tree CRFs.
InProceedings of the 13th Conference on ComputationalLinguistics and Intelligent Text Processing(CICLing-2012).Marta Recasens and Eduard Hovy.
2011.
Blanc:Implementing the rand index for coreferenceevaluation.
Natural Language Engineering, 17(4):485-510.Wee Meng Soon, Hwee Tou Ng, and Daniel ChungYong Lim.
2001.
A machine learning approach tocoreference resolution of noun phrase.Computational Linguistics, 27(4): 521-544.M.
Vilain, J. Burger, J. Aberdeen, D. Connolly, and L.Hirschman.
1995.
A model theoretic coreferencescoring scheme.
In Proceedings of the Sixth MessageUnderstanding Conference (MUC-6).Le Zhang.
2006.
Maximum Entropy Modeling Toolkitfor Python and C++.
Software available athttp://homepages.inf.ed.ac.uk/s0450736/maxent_toolkit.html.135
