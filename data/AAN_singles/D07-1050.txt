Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
477?485, Prague, June 2007. c?2007 Association for Computational LinguisticsWord Sense DisambiguationIncorporating Lexical and Structural Semantic InformationTakaaki Tanaka?
Francis Bond?
Timothy Baldwin?
Sanae Fujita?
Chikara Hashimoto??
{takaaki, sanae}@cslab.kecl.ntt.co.jp ?
bond@nict.go.jp?
tim@csse.unimelb.edu.au ?
ch@yz.yamagata-u.ac.jp?
NTT Communication Science Laboratories, Nippon Telegraph and Telephone Corporation?
National Institute of Information and Communications Technology?
The University of Melbourne ?
Yamagata UniversityAbstractWe present results that show that incorporat-ing lexical and structural semantic informa-tion is effective for word sense disambigua-tion.
We evaluated the method by using pre-cise information from a large treebank andan ontology automatically created from dic-tionary sentences.
Exploiting rich semanticand structural information improves preci-sion 2?3%.
The most gains are seen withverbs, with an improvement of 5.7% over amodel using only bag of words and n-gramfeatures.1 IntroductionRecently, significant improvements have been madein combining symbolic and statistical approachesto various natural language processing tasks.
Inparsing, for example, symbolic grammars are be-ing combined with stochastic models (Riezler et al,2002; Oepen et al, 2002; Malouf and van Noord,2004).
Statistical techniques have also been shownto be useful for word sense disambiguation (Steven-son, 2003).
However, to date, there have beenfew combinations of sense information together withsymbolic grammars and statistical models.
Kleinand Manning (2003) show that much of the gain instatistical parsing using lexicalized models comesfrom the use of a small set of function words.Features based on general relations provide littleimprovement, presumably because the data is toosparse: in the Penn treebank normally used to trainand test statistical parsers stocks and skyrocket neverappear together.
They note that this should motivatethe use of similarity and/or class based approaches:the superordinate concepts capital (?
stocks) andmove upward (?
sky rocket) frequently appear to-gether.
However, there has been little success in thisarea to date.
For example, Xiong et al (2005) use se-mantic knowledge to parse Chinese, but gain only amarginal improvement.
Focusing on WSD, Steven-son (2003) and others have shown that the use ofsyntactic information (predicate-argument relations)improve the quality of word sense disambiguation(WSD).
McCarthy and Carroll (2003) have shownthe effectiveness of the selectional preference infor-mation for WSD.
However, there is still little workon combining WSD and parse selection.We hypothesize that one of the reasons for thelack of success is that there has been no resourceannotated with both syntactic (or structural seman-tic information) and lexical semantic information.For English, there is the SemCor corpus (Fellbaum,1998) which is annotated with parse trees and Word-Net senses, but it is fairly small, and does not ex-plicitly include any structural semantic information.Therefore, we decided to construct and use a tree-bank with both syntactic information (e.g.
HPSGparses) and lexical semantic information (e.g.
sensetags): the Hinoki treebank (Bond et al, 2004).
Thiscan be used to train word sense disambiguation andparse ranking models using both syntactic and lexi-cal semantic features.
In this paper, we discuss onlyword sense disambiguation.
Parse ranking is dis-cussed in Fujita et al (2007).2 The Hinoki CorpusThe Hinoki corpus consists of the Lexeed Seman-tic Database of Japanese (Kasahara et al, 2004) andcorpora annotated with syntactic and semantic infor-477mation.2.1 LexeedLexeed is a database built from on a dictionary,which defines word senses used in the Hinoki cor-pus and has around 49,000 dictionary definition sen-tences and 46,000 example sentences which are syn-tactically and semantically annotated.
Lexeed con-sists of all words with a familiarity greater than orequal to five on a scale of one to seven.
This givesa fundamental vocabulary of 28,000 words, dividedinto 46,347 different senses.
Each sense has a defi-nition sentence and example sentence written usingonly these 28,000 familiar words (and some functionwords).
Many senses have more than one sentencein the definition: there are 75,000 defining sentencesin all.A (simplified) example of the entry for?U3 un-tenshu ?chauffeur?
is given in Figure 1.
Each wordcontains the word itself, its part of speech (POS) andlexical type(s) in the grammar, and the familiarityscore.
Each sense then contains definition and ex-ample sentences, links to other senses in the lexicon(such as hypernym), and links to other resources,such as the Goi-Taikei (Ikehara et al, 1997) andWordNet (Fellbaum, 1998).
Each content word inthe definition and example sentences is annotatedwith sense tags from the same lexicon.2.2 Lexical Semantics AnnotationThe lexical semantic annotation uses the sense in-ventory from Lexeed.
All words in the fundamentalvocabulary are tagged with their sense.
For example,the word d& ookii ?big?
(in ookiku naru ?growup?)
is tagged as sense 5 in the example sentence(Figure 1), with the meaning ?elder, older?.Each word was annotated by five annotators.
Weuse the majority choice in case of disagreements(Tanaka et al, 2006).
Inter-annotator agreementsamong the five annotators range from 78.7% to83.3%: the lowest agreement is for the Lexeed def-inition sentences and the highest is for Kyoto cor-pus (newspaper text).
These agreements reflect thedifficulties in disambiguating word sense over eachcorpus and can be considered as the upper bound ofprecision for WSD.Table 1 shows the distribution of word senses ac-cording to the word familiarity in Lexeed.Fam #WordsPoly-semous #WS#Mono-semous(%)6.5 - 368 182 4.0 186 (50.5)6.0 - 4,445 1,902 3.4 2,543 (57.2)5.5 - 9,814 3,502 2.7 6,312 (64.3)5.0 - 11,430 3,457 2.5 7,973 (69.8)Table 1: Word Senses in Lexeed2.3 OntologyThe Hinoki corpus comes with an ontology semi-automatically constructed from the parse results ofdefinitions in Lexeed (Nichols and Bond, 2005).
Theontology includes more than 80 thousand relation-ships between word senses, e.g.
synonym, hyper-nym, abbreviation, etc.
The hypernym relation for?U3 untenshu ?chauffeur?
is shown in Figure 1.Hypernym or synonym relations exist for almost allcontent words.2.4 ThesaurusAs part of the ontology verification, all nominal andmost verbal word senses in Lexeed were linked tosemantic classes in the Japanese thesaurus, NihongoGoi-Taikei (Ikehara et al, 1997).
These were thenhand verified.
Goi-Taikei has about 400,000 wordsincluding proper nouns, most nouns are classifiedinto about 2,700 semantic classes.
These seman-tic classes are arranged in a hierarchical structure(11 levels).
The Goi-Taikei Semantic Class for ?U3 untenshu ?chauffeur?
is shown in Figure 1:?C292:driver?
at level 9 which is subordinate to?C4:person?.2.5 Syntactic and Structural SemanticsAnnotationSyntactic annotation is done by selecting the bestparse (or parses) from the full analyses derived bya broad-coverage precision grammar.
The gram-mar is an HPSG implementation (JACY: Siegel andBender, 2002), which provides a high level of de-tail, marking not only dependency and constituentstructure but also detailed semantic relations.
As thegrammar is based on a monostratal theory of gram-mar (HPSG: Pollard and Sag, 1994) it is possibleto simultaneously annotate syntactic and semanticstructure without overburdening the annotator.
Us-ing a grammar enforces treebank consistency ?
allsentences annotated are guaranteed to have well-478?????????????????????
?INDEX ?U3 untenshuPOS nounLEX-TYPE noun-lexFAMILIARITY 6.2 [1?7] (?
5)SENSE 1???????????
?DEFINITION[\1 ?
?
1 k?U1 2d04 a person who drives trains and cars]EXAMPLE[d&(5 C<8b\1 G?U31 Dod6 G%?3 2I dream of growing up and becoming a train driver]HYPERNYM 04 hito ?person?SEM.
CLASS ?292:driver?
(?
?4:person?
)WORDNET motorman1?????????????????????????????????
?Figure 1: Dictionary Entry for ?U31 untenshu ?chauffeur?formed parses.
The flip side to this is that any sen-tences which the parser cannot parse remain unan-notated, at least unless we were to fall back on fullmanual mark-up of their analyses.
The actual anno-tation process uses the same tools as the Redwoodstreebank of English (Oepen et al, 2002).There were 4 parses for the definition sentenceshown in Figure 1.
The correct parse, shown as aphrase structure tree, is shown in Figure 2.
The twosources of ambiguity are the conjunction and the rel-ative clause.
The parser also allows the conjunctionto join to \ densha and 0 hito.
In Japanese, rel-ative clauses can have gapped and non-gapped read-ings.
In the gapped reading (selected here), 0 hitois the subject of ?U unten ?drive?.
In the non-gapped reading there is some underspecified relationbetween the thing and the verb phrase.
This is sim-ilar to the difference in the two readings of the dayhe knew in English: ?the day that he knew about?
(gapped) vs ?the day on which he knew (some-thing)?
(non-gapped).
Such semantic ambiguity isresolved by selecting the correct derivation tree thatincludes the applied rules in building the tree.The parse results can be automatically given bythe HPSG parser PET (Callmeier, 2000) with theJapanese grammar JACY.
The current parse rankingmodel has an accuracy of 70%: the correct tree isranked first 70% of the time (for Lexeed definitionsentences) (Fujita et al, 2007).The full parse is an HPSG sign, containing bothsyntactic and semantic information.
A view of thesemantic information is given in Figure 31.1The specific meaning representation language used inUTTERANCENPVP NPP VNPPPN CONJ N CASE-P V V\ ?
?
k ?U 2d 0densha ya jidousha o unten suru hitotrain or car ACC drive do person?U31 ?chauffeur?
: ?a person who drives a train or car?Figure 2: Syntactic View of the Definition of ?U31 untenshu ?chauffeur?The semantic view shows some ambiguity hasbeen resolved that is not visible in the purely syn-tactic view.The semantic view can be further simplified into adependency representation, further abstracting awayfrom quantification, as shown in Figure 4.
One ofthe advantages of the HPSG sign is that it containsall this information, making it possible to extract theparticular view needed.
In order to make linking toother resources (such as the sense annotation) easier,predicates are labeled with pointers back to their po-sition in the original surface string.
For example, thepredicate densha n 1 links to the surface charactersbetween positions 0 and 3: \.JACY is Minimal Recursion Semantics (Copestake et al, 2005).479???????????????????????????????
?TEXT \?
?
k?U2d0TOP h1RELS??????????????????????????????
?proposition m relLBL h1ARG0 e2MARG h3?????
?unknown relLBL h4ARG0 e2ARG x5????
?densha nLBL h6ARG0 x7??????
?udef relLBL h8ARG0 x7RSTR h9BODY h10?????????
?ya pLBL h11ARG0 x13L-INDEX x7R-INDEX x12?????????
?udef relLBL h15ARG0 x12RSTR h16BODY h17??????
?jidousha nLBL h18ARG0 x12??????
?udef relLBL h19ARG0 x12RSTR h20BODY h21?????????
?unten sLBL h22ARG0 e23 tense=presentARG1 x5ARG2 x13??????
?hito nLBL h24ARG0 x5??????
?udef relLBL h25ARG0 x5RSTR h26BODY h27???????
?proposition m relLBL h10001ARG0 e23 tense=presentMARG h28??????????????????????????????
?HCONS {h3 qeq h4,h9 qeq h6,h16 qeq h11,h20 qeq h18,h26 qeq h24,h28 qeq h22}ING {h24 ing h10001}???????????????????????????????
?Figure 3: Semantic View of the Definition of ?U31 untenshu ?chauffeur?_1:proposition_m<0:13>[MARG e2:unknown]e2:unknown<0:13>[ARG x5:_hito_n]x7:udef<0:3>[]x7:densha_n_1<0:3>x12:udef<4:7>[]x12:_jidousha_n<4:7>x13:_ya_p_conj<0:4>[L-INDEX x7:_densha_n_1, R-INDEX x12:_jidousha_n]e23:_unten_s_2<8:10>[ARG1 x5:_hito_n, ARG2 x13:_ya_p_conj]x5:udef<12:13>[]_2:proposition_m<0:13>[MARG e23:_unten_s_2]Figure 4: Dependency View of the Definition of ?U31 untenshu ?chauffeur?3 TaskWe define the task in this paper as ?allocating theword sense tags for all content words included inLexeed as headwords, in each input sentence?.
Thistask is a kind of all-words task, however, a uniquepoint is that we focus on fundamental vocabulary(basic words) in Lexeed and ignore other words.
Weuse Lexeed as the sense inventory.
There are twoproblems in resolving the task: how to build themodel and how to assign the word sense by usingthe model for disambiguating the senses.
We de-scribe the word sense selection model we use in sec-tion 4 and the method of word sense assignment insection 5.4 Word Sense Selection ModelAll content words (i.e.
basic words) in Lexeed areclassified into six groups by part-of-speech: noun,verb, verbal noun, adjective, adverb, others.
Wetreat the first five groups as targets of disambiguat-ing senses.
We build five words sense models corre-sponding to these groups.
A model contains sensesfor various words, however, features for a word arediscriminated from those for other words so that thesenses irrelevant to a target word are not selected.For example, an n-gram feature following a targetword ?has-a-tail?
for dog is distinct from that for cat.In the remainder of this section, we describe thefeatures used in the word sense disambiguation.First we used simple n-gram collocations, then a bagof words of all words occurring in the sentence.
Thiswas then enhanced by using ontological informationand predicate argument relations.4.1 Word CollocationsWord collocations (WORD-Col) are basic and effec-tive cues for WSD.
They can be modelled by n-gram and bag of words features, which are easilyextracted from a corpus.
We used all unigrams, bi-grams and trigrams which precede and follow thetarget words (N-gram) and all content words in thesentences where the target words occur (BOW).480# sample featuresC1 ?COLWS:04?C2 ?COLWSSC:C33:other person?C3 ?COLWSHYP:0/1?C4 ?COLWSHYPSC:C5:person?C1 ?COLWS:\1?C2 ?COLWSSC:C988:land vehicle?C3 ?COLWSHYP:?1?C4 ?COLWSHYPSC:C988:land vehicle?C1 ?COLWS:?
1?C2 ?COLWSSC:C988:land vehicle?C3 ?COLWSHYP:2?C4 ?COLWSHYPSC:C988:land vehicle?Table 2: Example semantic collocation features(SEM-Col) extracted from the word sense tagged cor-pus and the dictionary (Lexeed and GoiTaikei) andthe ontology which have the word senses and the se-mantic classes linked to the semantic tags.
The firstcolumn numbers the feature template correspondingto each example.4.2 Semantic FeaturesWe use the semantic information (sense tags and on-tologies) in two ways.
One is to enhance the collo-cations and the other is to enhance dependency rela-tions.4.2.1 Semantic CollocationsWord surface features like N-gram and BOW in-evitably suffer from data sparseness, therefore, wegeneralize them to more abstract words or conceptsand also consider words having the same mean-ings.
We used the ontology described in Sec-tion 2.3 to get hypernyms and synonyms and theGoi-Taikei thesaurus to abstract the words to the se-mantic classes.
The superordinate classes at level3, 4 and 5 are also added in addition to the originalsemantic class.
For example, \ densha ?train?and ?
jidousha ?automobile?
are both gener-alized to the semantic class ?C988:land vehicle?
(level 7).
The superordinate classes are also used:?C706:inanimate?
(level 3), ?C760:artifact?
(level 4) and ?C986:vehicle?
(level 5).4.2.2 Semantic DependenciesThe semantic dependency features are based ona predicate and its arguments taken from the ele-mentary dependencies.
For example, consider thesemantic dependency representation for densha ya# sample features for ?U2d1D1 ?PRED:?U2d, ARG1:0?D1 ?PRED:?U2d, ARG2:\?D1 ?PRED:?U2d, ARG2:?
?D2 ?PRED:?U2d, ARG1:04?D2 ?PRED:?U2d, ARG2:\1?D2 ?PRED:?U2d, ARG2:?
1?D3 ?PRED:?U2d, ARG1SC:C33?D3 ?PRED:?U2d, ARG2SC:C988?D4 ?PRED:?U2d, ARG2SYN:???1?D5 ?PRED:?U2d, ARG1HYP:0/1?D5 ?PRED:?U2d, ARG2HYP:?1?D5 ?PRED:?U2d, ARG2HYP:2?D6 ?PRED:?U2d, ARG1HYPSC:C5?D6 ?PRED:?U2d, ARG2HYPSC:C988?D11 ?PRED:?U2d, ARG1:0, ARG2:\?D22 ?PRED:?U2d, ARG1:04, ARG2:\1?D23 ?PRED:?U2d, ARG1:04, ARG2:C1460 ?D24 ?PRED:?U2d, ARG1:04, ARG2SYN:???1?D32 ?PRED:?U2d, ARG1:C5, ARG2:\1?D33 ?PRED:?U2d, ARG1:C5, ARG2:C988?D55 ?PRED:?U2d, ARG1HYP:0/4, ARG2HYP:?1?D56 ?PRED:?U2d, ARG1HYP:0/4, ARG2HYPSC:C988?D65 ?PRED:?U2d, ARG1HYPSC:C5 , ARG2HYP:?1?D322 ?PRED:C2003, ARG1:04, ARG2:\1?Table 3: Example semantic features extracted fromthe dependency tree in Figure 4.
The first columnnumbers the feature template corresponding to eachexample.jidousha-wo unten suru hito ?a person who drives atrain or car?
given in Figure 4.
The predicate un-ten ?drive?, has two arguments: ARG1 hito ?person?and ARG2 ya ?or?.
The coordinate conjunction isexpanded out into its children, giving ARG2 densha?train?
and jidousha ?automobile?.From these, we produce several features, a sam-ple of them are shown in Table 3.
One has all argu-ments and their labels (D11).
We also produce var-ious back offs, for example the predicate with onlyone argument at a time (D1-D3).
Each combinationof predicate and its related argument(s) becomes afeature.For the next class of features, we used the senseinformation from the corpus combined with the se-mantic classes in the dictionary to replace each pred-481icate by its disambiguated sense, its hypernym, itssynonym (if any) and its semantic class.
The seman-tic classes for\1 and?
1 are both ?988:landvehicle?, while ?U1 is ?2003:motion?
and 04is ?4:human?.
We also expand ?
1 into its syn-onym ???1 mo?ta?ka?
?motor car?.The semantic class features provide a seman-tic smoothing, as words are binned into the 2,700classes.
The hypernym/synonym features provideeven more smoothing.
Both have the effect of mak-ing more training data available for the disambigua-tor.4.3 DomainDomain information is a simple and sometimesstrong cue for disambiguating the target words(Gliozzo et al, 2005).
For instance, the sense ofthe word ?record?
is likey to be different in the mu-sical context, which is recalled by domain-specificwords like ?orchestra?, ?guitar?, than in the sport-ing context.
We use 12 domain categories like ?cul-ture/art?, ?sport?, etc.
which are similar to ones usedin directory search web sites.
About 6,000 wordsare automatically classified into one of 12 domaincategories by distributions in web sites (Hashimotoand Kurohashi, 2007) and 10% of them are manuallychecked.
Polysemous words which belong to multi-ple domains and neutral words are not classified intoany domain.5 Search AlgorithmThe conditional probability of the word sense foreach word is given by the word sense selectionmodel described in Section 4.
In the initial state,some of the semantic features, e.g.
semantic col-locations (SEM-Col) and word sense extensions forsemantic dependencies (SEM-Dep) are not available,since no word senses for polysemous words havebeen determined.
It is not practical to count all com-binations of word senses for target words, therefore,we first try to decide the sense for that word whichis most plausible among all the ambiguous words,then, disambiguate the next word by using the sense.We use the beam search algorithm, which is sim-ilar to that used for decoder in statistical machinetranslation (Watanabe, 2004), for finding the plausi-ble combination of word sense tags.The algorithm is described as follows.
For a pol-ysemous word set in an input sentence {w1, .
.
.
,wn},twik is the k-th word sense of word wi, W is a sethaving words to be disambiguated, T is a list of re-solved word senses.
A search node N is defined as[W,T ] and a score of a node N, s(N) is defined asthe probability that the word sense set T occurs inthe context.
The beam search can be done as fol-lows (beam width is b):1.
Create an initial node N0 = [T0,W0] (T0 = {},W0 = {}) and insert the node into an initialqueue Q0.2.
For each node N in the queue Q, do the follow-ing steps.?
For each wi (?
W ), create W ?i by pickingout wi from W?
Create new lists T ?1, .
.
.
,T ?l by adding oneof word sense candidates twi1,.
.
.
,twil for wito T?
Create new nodes [W ?i ,T ?0 ], .
.
.
,[W ?i ,T ?l ] andinsert them into the queue Q?3.
Sort the nodes in Q?
by the score s(N)4.
If the top node W in the queue Q?
is empty,adopt T as the combination of word senses andterminate.
Otherwise, pick out the top b nodesfrom Q?
and insert them into new queue Q, thengo back to 26 EvaluationWe trained and tested on the Lexeed Dictionary Def-inition (LXD-DEF) and Example sections (LXD-EX) ofthe Hinoki corpus (Bond et al, 2007).
These haveabout 75,000 definition and 46,000 example sen-tences respectively.
Some 54,000 and 36,000 sen-tences of them are treebanked, i.e., they have thesyntactic trees and structural semantic information.We used these sentences with the complete informa-tion and selected 1,000 sentences out of each sen-tence class as test sets (LXD-DEFtest, LXD-EXtest), andthe remainder is combined and used as a trainingset (LXD-ALL).
We also tested 1,000 sentences fromthe Kyoto Corpus of newspaper text (KYOTOtest).These sentences have between 3.4 (LXD-EXtest) ?
5.2(KYOTOtest) polysemous words per sentence on av-erage.482We use a maximum entropy / minimum diver-gence (MEMD) modeler to train the word sense se-lection model.
We use the open-source MaximunEntropy Modeling Toolkit2 for training, determiningbest-performing convergence thresholds and priorsizes experimentally.
The models for five differ-ent POSs were trained with each training sets: thebase model is word collocation model (WORD-Col),and the semantic models built by semantic colloca-tion (SEM-Col), semantic dependency (SEM-Dep) ordomain with WORD-Col (+SEM-Col, +SEM-Dep and+DOMAIN).Figure 5: Learning Curve7 Results and DiscussionTable 4 shows the precision as the results of the wordsense disambiguation on the combination of LXD-DEF and LXD-EX (LXD-ALL).
The baseline methodselects the senses occurring most frequently in thetraining corpus.
Each row indicates the results us-ing the baseline, word collocation (WORD-Col), thecombinations of WORD-Col and one of the seman-tic features (+SEM-Col, +SEM-Dep and +DOMAIN),e.g, +SEM-Col gives the results using WORD-Col andSEM-Col, and all features (FULL).There are significant improvements over the base-line and the other results on all corpora.
Basic word2http://homepages.inf.ed.ac.uk/s0450736/maxent_toolkit.htmlcollocation features (WORD-Col) give a vast improve-ment.
Extending this by using the ontological in-formation (+SEM-Col) gives a further improvementover the WORD-Col.
Adding the predicate-argumentrelationships (+SEM-Dep) improves the results evenmore.Table 6 shows the statistics of the target corpora.The best result of LXD-DEFtest (80.7%) surpasses theinter-annotator agreement (78.7%) in building theHinoki Sensebank.
However, there is a wide gapbetween the best results of KYOTOtest (60.4%) andthe inter-annotator agreement (83.3%), this suggestsother information such as the semantic classes fornamed entities (including proper nouns and multi-word expressions (MWE)) and broader contexts arerequired.
However, a model built on dictionary sen-tences lacks these features.
Even, so there is someimprovement.The domain features (+DOMAIN) give small con-tribution to the precision, since only intra-sentencecontext is counted in this experiment.
Unfortunatelydictiory definition and example sentences do not re-ally have a useful context.
We expect broader con-text should make the domain features more effectivefor the newspaper text (e.g.
as in Stevenson (2003)),Table 5 shows comparison of results of differentPOSs.
The semantic features (+SEM-Col and +SEM-Dep) are particularly effective for verb and also givemoderate improvements on the results of the otherPOSs.Figure 5 shows the precisions of LXD-DEFtest inchanging the size of a training corpus, which is di-vided into five partitions.
The precision is saturatedin using four partitions (264,000 tokens).These results of the dictionary sentences are closeto the best published results for the SENSEVAL-2task (79.3% by Murata et al (2003) using a com-bination of simple Bayes learners).
However, weare using a different sense inventory (Lexeed notIwanami (Nishio et al, 1994)) and testing over a dif-ferent corpus, so the results are not directly compa-rable.
In future work, we will test over SENSEVAL-2 data so that we can compare directly.None of the SENSEVAL-2 systems used onto-logical information, despite the fact that the dic-tionary definition sentences were made available,and there are several algorithms describing how toextract such information from MRDs (Tsurumaru483Model Test Baseline WORD-Col +SEM-Col +SEM-Dep +DOMAIN FULLLXD-ALL LXD-DEFtest 72.8 78.4 79.8 80.2 78.1 80.7LXD-EXtest 70.4 75.6 78.7 77.9 76.0 78.8KYOTOtest 55.6 58.5 60.0 58.8 59.8 60.4Table 4: The Precision of WSDPOS Baseline WORD-Col +SEM-Col +SEM-Dep +DOMAIN FULLNoun 65.5 68.7 69.6 69.4 68.9 69.8Verb 60.3 66.9 71.0 70.6 67.7 72.6VN 72.6 76.2 77.7 74.6 77.6 77.5Adj 59.9 67.2 69.5 68.9 68.9 69.5Adv 74.4 78.6 79.8 79.2 78.6 79.8Table 5: The Precision of WSD (per Part-of-Speech)et al, 1991; Wilkes et al, 1996; Nichols et al, 2005).We hypothesize that this is partly due to the way thetask is presented: there was not enough time to ex-tract and debug an ontology as well as build a dis-ambiguation system, and there was no ontology dis-tributed.
The CRL system (Murata et al, 2003) useda syntactic dependency parser as one source of fea-tures (KNP: Kurohashi and Nagao (2003)), remov-ing it decreased performance by around 0.6%.8 ConclusionsWe used the Hinoki corpus to test the importance oflexical and structural information in word sense dis-ambiguation.
We found that basic n-gram featuresand collocations provided a great deal of useful in-formation, but that better results could be gained byusing ontological information and semantic depen-dencies.AcknowledgementsWe would like to thank the other members of theNTT Natural Language Research Group NTT Com-munication Science laboratories for their support.We would also like to express gratitude to the re-viewers for their valuable comments and ProfessorZeng Guangping, Wang Daliang and Shen Bin ofthe University of Science and Technology Beijing(USTB) for building the demo system.ReferencesFrancis Bond, Sanae Fujita, Chikara Hashimoto, KanameKasahara, Shigeko Nariyama, Eric Nichols, Akira Ohtani,Takaaki Tanaka, and Shigeaki Amano.
2004.
The Hinokitreebank: A treebank for text understanding.
In Proceed-ings of the First International Joint Conference on NaturalLanguage Processing (IJCNLP-04), pages 554?559.
HainanIsland.Francis Bond, Sanae Fujita, and Takaaki Tanaka.
2007.
The Hi-noki syntactic and semantic treebank of Japanese.
LanguageResources and Evaluation.
(Special issue on Asian languagetechnology).Ulrich Callmeier.
2000.
PET - a platform for experimentationwith efficient HPSG processing techniques.
Natural Lan-guage Engineering, 6(1):99?108.Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan A. Sag.2005.
Minimal Recursion Semantics.
An introduction.
Re-search on Language and Computation, 3(4):281?332.Christine Fellbaum, editor.
1998.
WordNet: An Electronic Lex-ical Database.
MIT Press.Sanae Fujita, Francis Bond, Stephan Oepen, and TakaakiTanaka.
2007.
Exploiting semantic information for HPSGparse selection.
In ACL 2007 Workshop on Deep LinguisticProcessing, pages 25?32.
Prague, Czech Republic.Alfio Massimiliano Gliozzo, Claudio Giuliano, and CarloStrapparava.
2005.
Domain kernels for word sense disam-biguation.
In Proceedings of the 43rd Annual Meeting of theAssociation for Computational Linguistics (ACL 2005).
AnnArbor, U.S.Chikara Hashimoto and Sadao Kurohashi.
2007.
Constructionof domain dictionary for fundamental vocaburalry.
In Pro-ceedings of the ACL 2007 Main Conference Poster Sessions.Association for Computational Linguistics, Prague, CzechRepublic.Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, AkioYokoo, Hiromi Nakaiwa, Kentaro Ogura, YoshifumiOoyama, and Yoshihiko Hayashi.
1997.
Goi-Taikei ?A Japanese Lexicon.
Iwanami Shoten, Tokyo.
5 vol-umes/CDROM.Kaname Kasahara, Hiroshi Sato, Francis Bond, TakaakiTanaka, Sanae Fujita, Tomoko Kanasugi, and ShigeakiAmano.
2004.
Construction of a Japanese semantic lexicon:Lexeed.
In IPSG SIG: 2004-NLC-159, pages 75?82.
Tokyo.
(in Japanese).Dan Klein and Christopher D. Manning.
2003.
Accurate un-lexicalized parsing.
In Erhard Hinrichs and Dan Roth, edi-tors, Proceedings of the 41st Annual Meeting of the Associ-ation for Computational Linguistics, pages 423?430.
URLhttp://www.aclweb.org/anthology/P03-1054.pdf.484CorpusAnnotatedTokens #WSAgreementtoken (type) %Other Sense %Homonym %MWE %Proper NounLXD-DEF 199,268 5.18 .787 (.850) 4.2 0.084 1.5 0.046LXD-EX 126,966 5.00 .820 (.871) 2.3 0.035 0.4 0.0018KYOTO 268,597 3.93 .833 (.828) 9.8 3.3 7.9 5.5Table 6: Corpus StatisticsSadao Kurohashi and Makoto Nagao.
2003.
Building aJapanese parsed corpus ?
while improving the parsing sys-tem.
In Anne Abeille?, editor, Treebanks: Building and UsingParsed Corpora, chapter 14, pages 249?260.
Kluwer Aca-demic Publishers.Robert Malouf and Gertjan van Noord.
2004.
Wide cover-age parsing with stochastic attribute value grammars.
InIJCNLP-04 Workshop: Beyond shallow analyses - For-malisms and statistical modeling for deep analyses.
JSTCREST.
URL http://www-tsujii.is.s.u-tokyo.ac.jp/bsa/papers/malouf.pdf.Diana McCarthy and John Carroll.
2003.
Disambiguat-ing nouns, verbs and adjectives using automatically ac-quired selectional preferences.
Computational Linguistics,29(4):639?654.Masaaki Murata, Masao Utiyama, Kiyotaka Uchimoto, QingMa, and HItoshi Isahara.
2003.
CRL at Japanese dictionary-based task of SENSEVAL-2.
Journal of Natural LanguageProcessing, 10(3):115?143.
(in Japanese).Eric Nichols and Francis Bond.
2005.
Acquiring ontologiesusing deep and shallow processing.
In 11th Annual Meetingof the Association for Natural Language Processing, pages494?498.
Takamatsu.Eric Nichols, Francis Bond, and Daniel Flickinger.
2005.
Ro-bust ontology acquisition from machine-readable dictionar-ies.
In Proceedings of the International Joint Conference onArtificial Intelligence IJCAI-2005, pages 1111?1116.
Edin-burgh.Minoru Nishio, Etsutaro Iwabuchi, and Shizuo Mizutani.
1994.Iwanami Kokugo Jiten Dai Go Han [Iwanami Japanese Dic-tionary Edition 5].
Iwanami Shoten, Tokyo.
(in Japanese).Stephan Oepen, Kristina Toutanova, Stuart Shieber,Christoper D. Manning, Dan Flickinger, and ThorstenBrant.
2002.
The LinGO redwoods treebank: Motivationand preliminary applications.
In 19th International Confer-ence on Computational Linguistics: COLING-2002, pages1253?7.
Taipei, Taiwan.Carl Pollard and Ivan A.
Sag.
1994.
Head Driven Phrase Struc-ture Grammar.
University of Chicago Press, Chicago.Stefan Riezler, Tracy H. King, Ronald M. Kaplan, RichardCrouch, John T. Maxwell, and Mark Johnson.
2002.
Parsingthe Wall Street Journal using a Lexical-Functional Grammarand discriminative estimation techniques.
In 41st AnnualMeeting of the Association for Computational Linguistics:ACL-2003, pages 271?278.Melanie Siegel and Emily M. Bender.
2002.
Efficient deep pro-cessing of Japanese.
In Proceedings of the 3rd Workshop onAsian Language Resources and International Standardiza-tion at the 19th International Conference on ComputationalLinguistics, pages 1?8.
Taipei.Mark Stevenson.
2003.
Word Sense Disambiguation.
CSLI Pub-lications.Takaaki Tanaka, Francis Bond, and Sanae Fujita.
2006.
The Hi-noki sensebank ?
a large-scale word sense tagged corpus ofJapanese ?.
In Proceedings of the Workshop on Frontiers inLinguistically Annotated Corpora 2006, pages 62?69.
Syd-ney.
URL http://www.aclweb.org/anthology/W/W06/W06-0608, (ACL Workshop).Hiroaki Tsurumaru, Katsunori Takesita, Itami Katsuki, Toshi-hide Yanagawa, and Sho Yoshida.
1991.
An approach tothesaurus construction from Japanese language dictionary.In IPSJ SIGNotes Natural Language, volume 83-16, pages121?128.
(in Japanese).Taro Watanabe.
2004.
Example-based Statistical MachineTranslation.
Ph.D. thesis, Kyoto University.Yorick A. Wilkes, Brian M. Slator, and Louise M. Guthrie.1996.
Electric Words.
MIT Press.Deyi Xiong, Qun Liu Shuanglong Li and, Shouxun Lin, andYueliang Qian.
2005.
Parsing the Penn Chinese treebankwith semantic knowledge.
In Robert Dale, Jian Su Kam-FaiWong and, and Oi Yee Kwong, editors, Natural LanguageProcessing ?
IJCNLP 005: Second International Joint Con-ference Proceedings, pages 70?81.
Springer-Verlag.485
