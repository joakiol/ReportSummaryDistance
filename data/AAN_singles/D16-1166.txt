Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1598?1607,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsCharacter-Level Question Answering with AttentionDavid GolubUniversity of Washingtongolubd@cs.washington.eduXiaodong HeMicrosoft Researchxiaohe@microsoft.comAbstractWe show that a character-level encoder-decoder framework can be successfully ap-plied to question answering with a structuredknowledge base.
We use our model for single-relation question answering and demonstratethe effectiveness of our approach on the Sim-pleQuestions dataset (Bordes et al, 2015),where we improve state-of-the-art accuracyfrom 63.9% to 70.9%, without use of ensem-bles.
Importantly, our character-level modelhas 16x fewer parameters than an equivalentword-level model, can be learned with signif-icantly less data compared to previous work,which relies on data augmentation, and is ro-bust to new entities in testing.
11 IntroductionSingle-relation factoid questions are the most com-mon form of questions found in search query logsand community question answering websites (Yih etal., 2014; Fader et al, 2013).
A knowledge-base(KB) such as Freebase, DBpedia, or Wikidata canhelp answer such questions after users reformulatethem as queries.
For instance, the question ?Wherewas Barack Obama born??
can be answered by is-suing the following KB query:?
(x).place of birth(Barack Obama, x)However, automatically mapping a natural languagequestion such as ?Where was Barack Obama born?
?1Our code is publicly available at https://github.com/davidgolub/simpleqato its corresponding KB query remains a challeng-ing task.There are three key issues that make learning thismapping non-trivial.
First, there are many para-phrases of the same question.
Second, many of theKB entries are unseen during training time; however,we still need to correctly predict them at test time.Third, a KB such as Freebase typically contains mil-lions of entities and thousands of predicates, mak-ing it difficult for a system to predict these entities atscale (Yih et al, 2014; Fader et al, 2014; Bordes etal., 2015).
In this paper, we address all three of theseissues with a character-level encoder-decoder frame-work that significantly improves performance overstate-of-the-art word-level neural models, while alsoproviding a much more compact model that can belearned from less data.First, we use a long short-term memory (LSTM)(Hochreiter and Schmidhuber, 1997) encoder to em-bed the question.
Second, to make our model ro-bust to unseen KB entries, we extract embeddingsfor questions, predicates and entities purely fromtheir character-level representations.
Character-level modeling has been previously shown to gen-eralize well to new words not seen during training(Ljubes?ic?
et al, 2014; Chung et al, 2016), whichmakes it ideal for this task.
Third, to scale our modelto handle the millions of entities and thousands ofpredicates in the KB, instead of using a large out-put layer in the decoder to directly predict the entityand predicate, we use a general interaction functionbetween the question embeddings and KB embed-dings that measures their semantic relevance to de-termine the output.
The combined use of character-1598W h e r e ...    b o r n ??
?<?>Q: Where was Barack Obama born?O b a m a B a r a c k  O b a m aObama 0.18Barack Obama 0.60...CNNE CNNE CNNE???????
?????
?a) Question encoder(character-level LSTM)c) Entity & Predicate encoder(character-level CNNs)b) KB query decoder(LSTM with an attention mechanism)?people/?/spouse 0.2people/?/?birth 0.58...CNNP CNNP CNNP?h0</s>Entity attentionsPredicate attentions p e o p l e/?/ s p o u s eh1h2h3p e o p l e/?/ p l ac e _ o f _ b i r t hs1 ?
s4          ?
sn{?1}{?2}?????????????????
?1?2?Figure 1: Our encoder-decoder architecture that generates a query against a structured knowledge base.We encode our question via a long short-term memory (LSTM) network and an attention mechanism toproduce our context vector.
During decoding, at each time step, we feed the current context vector and anembedding of the English alias of the previously generated knowledge base entry into an attention-baseddecoding LSTM to generate the new candidate entity or predicate.level modeling and a semantic relevance functionallows us to successfully produce likelihood scoresfor the KB entries that are not present in our vo-cabulary, a challenging task for standard encoder-decoder frameworks.Our novel, character-level encoder-decodermodel is compact, requires significantly less data totrain than previous work, and is able to generalizewell to unseen entities in test time.
In particular,without use of ensembles, we achieve 70.9% accu-racy in the Freebase2M setting and 70.3% accuracyin the Freebase5M setting on the SimpleQuestionsdataset, outperforming the previous state-of-arts of62.7% and 63.9% (Bordes et al, 2015) by 8.2%and 6.4% respectively.
Moreover, we only use thetraining questions provided in SimpleQuestions totrain our model, which cover about 24% of words inentity aliases on the test set.
This demonstrates therobustness of the character-level model to unseenentities.
In contrast, data augmentation is usuallynecessary to provide more coverage for unseenentities and predicates, as done in previous work(Bordes et al, 2015; Yih et al, 2014).2 Related WorkOur work is motivated by three major threadsof research in machine learning and natural lan-guage processing: semantic-parsing for open-domain question answering, character-level lan-guage modeling, and encoder-decoder methods.Semantic parsing for open-domain question an-swering, which translates a question into a struc-tured KB query, is a key component in question an-swering with a KB.
While early approaches relied onbuilding high-quality lexicons for domain-specificdatabases such as GeoQuery (Tang and Mooney,2001), recent work has focused on building seman-tic parsing frameworks for general knowledge basessuch as Freebase (Yih et al, 2014; Bordes et al,2014a; Bordes et al, 2015; Berant and Liang, 2014;Fader et al, 2013; Dai et al, 2016).Semantic parsing frameworks for large-scaleknowledge bases have to be able to successfully gen-erate queries for the millions of entities and thou-sands of predicates in the KB, many of which areunseen during training.
To address this issue, recentwork relies on producing embeddings for predicatesand entities in a KB based on their textual descrip-tions (Bordes et al, 2014a; Bordes et al, 2015; Yihet al, 2014; Yih et al, 2015; Bishan Yang, 2015).A general interaction function can then be used tomeasure the semantic relevance of these embeddedKB entries to the question and determine the mostlikely KB query.1599Most of these approaches use word-level embed-dings to encode entities and predicates, and there-fore might suffer from the out-of-vocabulary (OOV)problem when they encounter unseen words duringtest time.
Consequently, they often rely on signif-icant data augmentation from sources such as Par-alex (Fader et al, 2013), which contains 18 millionquestion-paraphrase pairs scraped from WikiAn-swers, to have sufficient examples for each wordthey encounter (Bordes et al, 2014b; Yih et al,2014; Bordes et al, 2015).As opposed to word-level modeling, character-level modeling can be used to handle the OOV is-sue.
While character-level modeling has not beenapplied to factoid question answering before, it hasbeen successfully applied to information retrieval,machine translation, sentiment analysis, classifica-tion, and named entity recognition (Huang et al,2013; Shen et al, 2014; Chung et al, 2016; Zhanget al, 2015; Santos and Zadrozny, 2014; dos Santosand Gatti, 2014; Klein et al, 2003; dos Santos, 2014;dos Santos et al, 2015).
Moreover, Chung et al(2015) demonstrate that gated-feedback LSTMs ontop of character-level embeddings can capture long-term dependencies in language modeling.Lastly, encoder-decoder networks have been ap-plied to many structured machine learning tasks.First introduced in Sutskever et al (2014), in anencoder-decoder network, a source sequence is firstencoded with a recurrent neural network (RNN) intoa fixed-length vector which intuitively captures its?meaning?, and then decoded into a desired tar-get sequence.
This approach and related memory-based or attention-based approaches have been suc-cessfully applied in diverse domains such as speechrecognition, machine translation, image captioning,parsing, executing programs, and conversational di-alogues (Amodei et al, 2015; Venugopalan et al,2015; Bahdanau et al, 2015; Vinyals et al, 2015;Zaremba and Sutskever, 2014; Xu et al, 2015;Sukhbaatar et al, 2015).Unlike previous work, we formulate question an-swering as a problem of decoding the KB querygiven the question and KB entries which are en-coded in embedding spaces.
We therefore inte-grate the learning of question and KB embeddingsin a unified encoder-decoder framework, where thewhole system is optimized end-to-end.3 ModelSince we focus on single-relation question answer-ing in this work, our model decodes every ques-tion into a KB query that consists of exactly twoelements?the topic entity, and the predicate.
Moreformally, our model is a function f(q, {e}, {p}) thattakes as input a question q, a set of candidate enti-ties {e} = e1, ..., en, a set of candidate predicates{p} = p1, ..., pm, and produces a likelihood scorep(ei, pj |q) of generating entity ei and predicate pjgiven question q for all i ?
1...n, j ?
1...m.As illustrated in Figure 1, our model consists ofthree components:1.
A character-level LSTM-based encoder for thequestion which produces a sequence of embed-ding vectors, one for each character (Figure1a).2.
A character-level convolutional neural net-work (CNN)-based encoder for the predi-cates/entities in a knowledge base which pro-duces a single embedding vector for each pred-icate or entity (Figure 1c).3.
An LSTM-based decoder with an attentionmechanism and a relevance function for gener-ating the topic entity and predicate to form theKB query (Figure 1b).The details of each component are described in thefollowing sections.3.1 Encoding the QuestionTo encode the question, we take two steps:1.
We first extract one-hot encoding vectors forcharacters in the question, x1, ..., xn, where xirepresents the one-hot encoding vector for theith character in the question.
We keep thespace, punctuation and original cases withouttokenization.2.
We feed x1, ..., xn from left to right into a two-layer gated-feedback LSTM, and keep the out-puts at all time steps as the embeddings for thequestion, i.e., these are the vectors s1, ..., sn.16003.2 Encoding Entities and Predicates in the KBTo encode an entity or predicate in the KB, we taketwo steps:1.
We first extract one-hot encoding vectors forcharacters in its English alias, x1, ..., xn, wherexi represents the one-hot encoding vector forthe ith character in the alias.2.
We then feed x1, ..., xn into a temporal CNNwith two alternating convolutional and fully-connected layers, followed by one fully-connected layer:f(x1, ..., xn) = tanh(W3 ?max(tanh(W2?conv(tanh(W1 ?
conv(x1, ..., xn))))))where f(x1...n) is an embedding vector of sizeN , W3 has size RN?h, conv represents a tem-poral convolutional neural network, and maxrepresents a max pooling layer in the temporaldirection.We use a CNN as opposed to an LSTM to embedKB entries primarily for computational efficiency.Also, we use two different CNNs to encode enti-ties and predicates because they typically have sig-nificantly different styles (e.g., ?Barack Obama?
vs.?/people/person/place of birth?
).3.3 Decoding the KB QueryTo generate the single topic entity and predicate toform the KB query, we use a decoder with two keycomponents:1.
An LSTM-based decoder with attention.
Itshidden states at each time step i, hi, have thesame dimensionality N as the embeddings ofentities/predicates.
The initial hidden state h0is set to the zero vector: ~0.2.
A pairwise semantic relevance function thatmeasures the similarity between the hiddenunits of the LSTM and the embedding of an en-tity or predicate candidate.
It then returns themostly likely entity or predicate based on thesimilarity score.In the following two sections, we will first de-scribe the LSTM decoder with attention, followedby the semantic relevance function.3.3.1 LSTM-based Decoder with AttentionThe attention-based LSTM decoder uses a similararchitecture as the one described in Bahdanau et al(2015).
At each time step i, we feed in a contextvector ci and an input vector vi into the LSTM.
Attime i = 1 we feed a special input vector v<S> =~0 into the LSTM.
At time i = 2, during training,the input vector is the embedding of the true entity,while during testing, it is the embedding of the mostlikely entity as determined at the previous time step.We now describe how we produce the contextvector ci.
Let hi?1 be the hidden state of the LSTMat time i?1, sj be the jth question character embed-ding, n be the number of characters in the question,r be the size of sj , andm be a hyperparameter.
Thenthe context vector ci, which represents the attention-weighted content of the question, is recomputed ateach time step i as follows:ci =n?j=1?ijsj ,?ij =exp (eij)?Txk=1 exp (eik)eij =v>a tanh (Wahi?1 + Uasj) ,where {?}
is the attention distribution that is ap-plied over each hidden unit sj , Wa ?
Rm?N , Ua ?Rm?r, and va ?
R1?m.3.3.2 Semantic Relevance FunctionUnlike machine translation and language model-ing where the vocabulary is relatively small, thereare millions of entries in the KB.
If we try to di-rectly predict the KB entries, the decoder will needan output layer with millions of nodes, which iscomputationally prohibitive.
Therefore, we resortto a relevance function that measures the semanticsimilarity between the decoder?s hidden state andthe embeddings of KB entries.
Our semantic rel-evance function takes two vectors x1, x2 and re-turns a distance measure of how similar they are toeach other.
In current experiments we use a simplecosine-similarity metric: cos(x1, x2).Using this similarity metric, the likelihoods ofgenerating entity ej and predicate pk are:1601RESULTS ON SIMPLEQUESTIONS DATASETKB TRAIN SOURCES AUTOGEN.
EMBED MODEL ENSEMBLE SQ # TRAINWQ SIQ PRP QUESTIONS TYPE Accuracy EXAMPLESFB2M no yes no no Char Ours 1 model 70.9 76KFB2M no yes no no Word Ours 1 model 53.9 76KFB2M yes yes yes yes Word MemNN 1 model 62.7 26MFB5M no yes no no Char Ours 1 model 70.3 76KFB5M no yes no no Word Ours 1 model 53.1 76KFB5M yes yes yes yes Word MemNN 5 models 63.9 27MFB5M yes yes yes yes Word MemNN Subgraph 62.9 27MFB5M yes yes yes yes Word MemNN 1 model 62.2 27MTable 1: Experimental results on the SimpleQuestions dataset.
MemNN results are from Bordes et al(2015).
WQ, SIQ and PRP stand for WebQuestions, SimpleQuestions and paraphrases from WikiAnswers.P (ej) =exp(?cos(h1, ej))?ni=1 exp(?cos(h1, ei))P (pk) =exp(?cos(h2, pk))?mi=1 exp(?cos(h2, pi))where ?
is a constant, h1, h2 are the hidden states ofthe LSTM at times t = 1 and t = 2, e1, ..., en are theentity embeddings, and p1, ..., pm are the predicateembeddings.
A similar likelihood function was usedto train the semantic similarity modules proposed inYih et al (2014) and Yih et al (2015), Palangi et al(2016), Huang et al (2013).During inference, e1, ..., en and p1, ..., pm are theembeddings of candidate entities and predicates.During training e1, ..., en, p1, ..., pm are the embed-dings of the true entity and 50 randomly-sampledentities, and the true predicate and 50 randomly-sampled predicates, respectively.3.4 InferenceFor each question q, we generate a candidate setof entities and predicates, {e} and {p}, and feed itthrough the model f(q, {e}, {p}).
We then decodethe most likely (entity, predicate) pair:(e?, p?)
= argmaxei,pj (P (ei) ?
P (pj))which becomes our semantic parse.We use a similar procedure as the one describedin Bordes et al (2015) to generate candidate entities{e} and predicates {p}.
Namely, we take all entitieswhose English alias is a substring of the question,and remove all entities whose alias is a substring ofanother entity.
For each English alias, we sort eachentity with this alias by the number of facts that ithas in the KB, and append the top 10 entities fromthis list to our set of candidate entities.
All predi-cates pj for each entity in our candidate entity setbecome the set of candidate predicates.3.5 LearningOur goal in learning is to maximize the joint likeli-hood P (ec)?P (pc) of predicting the correct entity ecand predicate pc pair from a set of randomly sampledentities and predicates.
We use back-propagation tolearn all of the weights in our model.All the parameters of our model are learnedjointly without pre-training.
These parameters in-clude the weights of the character-level embeddings,CNNs, and LSTMs.
Weights are randomly initial-ized before training.
For the ith layer in our network,each weight is sampled from a uniform distributionbetween ?
1|li| and 1|li| , where |li| is the number ofweights in layer i.4 Dataset and Experimental SettingsWe evaluate the proposed model on the SimpleQues-tions dataset (Bordes et al, 2015).
The dataset con-sists of 108,442 single-relation questions and theircorresponding (topic entity, predicate, answer en-tity) triples from Freebase.
It is split into 75,910train, 10,845 validation, and 21,687 test questions.Only 10,843 of the 45,335 unique words in entityaliases and 886 out of 1,034 unique predicates inthe test set were present in the train set.
For theproposed dataset, there are two evaluation settings,called FB2M and FB5M, respectively.
The formeruses a KB for candidate generation which is a sub-1602set of Freebase and contains 2M entities, while thelatter uses subset of Freebase with 5M entities.In our experiments, the Memory Neural Networks(MemNNs) proposed in Bordes et al (2015) serveas the baselines.
For training, in addition to the 76Kquestions in the training set, the MemNNs use 3Ktraining questions from WebQuestions (Berant et al,2013), 15M paraphrases from WikiAnswers (Faderet al, 2013), and 11M and 12M automatically gener-ated questions from the KB for the FB2M and FB5Msettings, respectively.
In contrast, our models aretrained only on the 76K questions in the training set.For our model, both layers of the LSTM-basedquestion encoder have size 200.
The hidden layersof the LSTM-based decoder have size 100, and theCNNs for entity and predicate embeddings have ahidden layer of size 200 and an output layer of size100.
The CNNs for entity and predicate embeddingsuse a receptive field of size 4, ?
= 5, and m = 100.We train the models using RMSProp with a learningrate of 1e?4.In order to make the input character sequencelong enough to fill up the receptive fields of mul-tiple CNN layers, we pad each predicate or entityusing three padding symbols P , a special start sym-bol, and a special end symbol.
For instance, Obamawould become SstartPPPObamaPPPSend.
Forconsistency, we apply the same padding to the ques-tions.5 Results5.1 End-to-end Results on SimpleQuestionsFollowing Bordes et al (2015), we report resultson the SimpleQuestions dataset in terms of SQ ac-curacy, for both FB2M and FB5M settings in Ta-ble 1.
SQ accuracy is defined as the percentageof questions for which the model generates a cor-rect KB query (i.e., both the topic entity and predi-cate are correct).
Our single character-level modelachieves SQ accuracies of 70.9% and 70.3% onthe FB2M and FB5M settings, outperforming theprevious state-of-art results by 8.2% and 6.4%, re-spectively.
Compared to the character-level model,which only has 1.2M parameters, our word-levelmodel has 19.9M parameters, and only achieves abest SQ accuracy of 53.9%.
In addition, in contrastto previous work, the OOV issue is much more se-vere for our word-level model, since we use no dataaugmentation to cover entities unseen in the trainset.5.2 Ablation and Embedding ExperimentsWe carry out ablation studies in Sections 5.2.1 and5.2.2 through a set of random-sampling experi-ments.
In these experiments, for each question, werandomly sample 200 entities and predicates fromthe test set as noise samples.
We then mix the goldentity and predicate into these negative samples, andevaluate the accuracy of our model in predicting thegold predicate or entity from this mixed set.5.2.1 Character-Level vs. Word-Level ModelsWe first explore using word-level models as an al-ternative to character-level models to construct em-beddings for questions, entities and predicates.Both word-level and character-level models per-form comparably well when predicting the predi-cate, reaching an accuracy of around 80% (Table3).
However, the word-level model has considerabledifficulty generalizing to unseen entities, and is onlyable to predict 45% of the entities accurately fromthe mixed set.
These results clearly demonstrate thatthe OOV issue is much more severe for entities thanpredicates, and the difficulty word-level models havewhen generalizing to new entities.In contrast, character-level models have no suchissues, and achieve a 96.6% accuracy in predictingthe correct entity on the mixed set.
This demon-strates that character-level models encode the se-mantic representation of entities and can match en-tity aliases in a KB with their mentions in naturallanguage questions.5.2.2 Depth Ablation StudyWe also study the impact of the depth of neuralnetworks in our model.
The results are presentedin Table 2.
In the ablation experiments we comparethe performance of a single-layer LSTM to a two-layer LSTM to encode the question, and a single-layer vs. two-layer CNN to encode the KB entries.We find that a two-layer LSTM boosts joint accu-racy by over 6%.
The majority of accuracy gains area result of improved predicate predictions, possiblybecause entity accuracy is already saturated in thisexperimental setup.1603# of LSTM Layers # of CNN Layers Joint Accuracy Predicate Accuracy Entity Accuracy2 2 78.3 80.0 96.62 1 77.7 79.4 96.81 2 71.5 73.9 95.01 1 72.2 74.7 94.9Table 2: Results for a random sampling experiment where we varied the number of layers used for convolutions andthe question-encoding LSTM.
We terminated training models after 14 epochs and 3 days on a GPU.Embedding Type Joint Accuracy Predicate Accuracy Entity AccuracyCharacter 78.3 80.0 96.6Word 37.6 78.8 45.5Table 3: Results for a random sampling experiment where we varied the embedding type (word vs. character-level).We used 2 layered-LSTMs and CNNs for all our experiments.
Our models were trained for 14 epochs and 3 days.5.3 Attention MechanismsIn order to further understand how the model per-forms question answering, we visualize the attentiondistribution over question characters in the decodingprocess.
In each sub-figure of Figure 2, the x-axisis the character sequence of the question, and the y-axis is the attention weight distribution {?i}.
Theblue curve is the attention distribution when gener-ating the entity, and green curve is the attention dis-tribution when generating the predicate.Interestingly, as the examples show, the attentiondistribution typically peaks at empty spaces.
Thisindicates that the character-level model learns that aspace defines an ending point of a complete linguis-tic unit.
That is, the hidden state of the LSTM en-coder at a space likely summarizes content about thecharacter sequence before that space, and thereforecontains important semantic information that the de-coder needs to attend to.Also, we observe that entity attention distribu-tions are usually less sharp and span longer portionsof words, such as ?john?
or ?rutters?, than predicateattention distributions (e.g., Figure 2a).
For enti-ties, semantic information may accumulate gradu-ally when seeing more and more characters, whilefor predicates, semantic information will becomeclear only after seeing the complete word.
For ex-ample, it may only be clear that characters such as?song by?
refer to a predicate after a space, as op-posed to the name of a song such as ?song bye byelove?
(Figures 2a, 2b).
In contrast, a sequence ofcharacters starts to become a likely entity after see-ing an incomplete name such as ?joh?
or ?rutt?.In addition, a character-level model can identifyentities whose English aliases were never seen intraining, such as ?phrenology?
(Figure 2d).
Themodel apparently learns that words ending with thesuffix ?nology?
are likely entity mentions, which isinteresting because it reads in the input one characterat a time.Furthermore, as observed in Figure 2d, the atten-tion model is capable of attending disjoint regions ofthe question and capture the mention of a predicatethat is interrupted by entity mentions.
We also notethat predicate attention often peaks at the paddingsymbols after the last character of the question, pos-sibly because sentence endings carry extra informa-tion that further help disambiguate predicate men-tions.
In certain scenarios, the network may onlyhave sufficient information to build a semantic rep-resentation of the predicate after being ensured thatit reached the end of a sentence.Finally, certain words in the question help identifyboth the entity and the predicate.
For example, con-sider the word ?university?
in the question ?Whattype of educational institution is eastern new mexicouniversity?
(Figure 2c).
Although it is a part of theentity mention, it also helps disambiguate the predi-cate.
However, previous semantic parsing-based QAapproaches (Yih et al, 2015; Yih et al, 2014) as-sume that there is a clear separation between pred-icate and entity mentions in the question.
In con-trast, the proposed model does not need to make thishard categorization, and attends the word ?univer-sity?
when predicting both the entity and predicate.16046 Error AnalysisWe randomly sampled 50 questions where thebest-performing model generated the wrong KBquery and categorized the errors.
For 46 out of the50 examples, the model predicted a predicate witha very similar alias to the true predicate, i.e.
?/mu-sic/release/track?
vs. ?/music/release/track list?.For 21 out of the 50 examples, the model predictedthe wrong entity, e.g., ?Album?
vs. ?Still Here?
forthe question ?What type of album is still here?
?.Finally, for 18 of the 50 examples, the model pre-dicted the wrong entity and predicate, i.e.
(?Play?,?/freebase/equivalent topic/equivalent type?)
forthe question ?which instrument does amapolacabase play??
Training on more data, augment-ing the negative sample set with words from thequestion that are not an entity mention, and havingmore examples that disambiguate between similarpredicates may ameliorate many of these errors.7 ConclusionIn this paper, we proposed a new character-level,attention-based encoder-decoder model for questionanswering.
In our approach, embeddings of ques-tions, entities, and predicates are all jointly learnedto directly optimize the likelihood of generating thecorrect KB query.
Our approach improved the state-of-the-art accuracy on the SimpleQuestions bench-mark significantly, using much less data than pre-vious work.
Furthermore, thanks to character-levelmodeling, we have a compact model that is robustto unseen entities.
Visualizations of the attentiondistribution reveal that our model, although built oncharacter-level inputs, can learn higher-level seman-tic concepts required to answer a natural languagequestion with a structured KB.
In the future we wantextend our system to multi-relation questions.8 AcknowledgementsWe thank the anonymous reviewers, Luke Zettle-moyer, Yejin Choi, Joel Pfeiffer, and members of theUW NLP group for helpful feedback on the paper.a)b)c)d)Figure 2: Attention distribution over outputs of aleft-to-right LSTM on question characters.1605References[Amodei et al2015] Dario Amodei, Rishita Anubhai,Eric Battenberg, Carl Case, Jared Casper, Bryan C.Catanzaro, Jingdong Chen, Mike Chrzanowski, AdamCoates, Greg Diamos, Erich Elsen, Jesse Engel, LinxiFan, Christopher Fougner, Tony Han, Awni Y. Han-nun, Billy Jun, Patrick LeGresley, Libby Lin, SharanNarang, Andrew Y. Ng, Sherjil Ozair, Ryan Prenger,Jonathan Raiman, Sanjeev Satheesh, David Seetapun,Shubho Sengupta, Yi Wang, Zhiqian Wang, ChongWang, Bo Xiao, Dani Yogatama, Jun Zhan, andZhenyao Zhu.
2015.
Deep speech 2: End-to-endspeech recognition in english and mandarin.
CoRR,abs/1512.02595.
[Bahdanau et al2015] Dzmitry Bahdanau, KyunghyunCho, and Yoshua Bengio.
2015.
Neural machinetranslation by jointly learning to align and translate.In ICLR.
[Berant and Liang2014] J. Berant and P. Liang.
2014.
Se-mantic parsing via paraphrasing.
In Association forComputational Linguistics (ACL).
[Berant et al2013] Jonathan Berant, Andrew Chou, RoyFrostig, and Percy Liang.
2013.
Semantic parsingon freebase from question-answer pairs.
In EmpiricalMethods in Natural Language Processing (EMNLP).
[Bishan Yang2015] Xiaodong He Jianfeng Gao Li DengBishan Yang, Scott Wen-tau Yih.
2015.
Embed-ding entities and relations for learning and infer-ence in knowledge bases.
In Proceedings of the In-ternational Conference on Learning Representations(ICLR) 2015, May.
[Bordes et al2014a] Antoine Bordes, Sumit Chopra, andJason Weston.
2014a.
Question answering with sub-graph embeddings.
In Proceedings of the 2014 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 615?620, Doha, Qatar,October.
Association for Computational Linguistics.
[Bordes et al2014b] Antoine Bordes, Jason Weston, andNicolas Usunier.
2014b.
Open question answer-ing with weakly supervised embedding models.
InProceedings of the European Conference on MachineLearning and Knowledge Discovery in Databases -Volume 8724, ECML PKDD 2014, pages 165?180,New York, NY, USA.
Springer-Verlag New York, Inc.[Bordes et al2015] Antoine Bordes, Nicolas Usunier,Sumit Chopra, and Jason Weston.
2015.
Large-scalesimple question answering with memory networks.
InProc.
NIPS.
[Chung et al2015] Junyoung Chung, C?aglar Gu?lc?ehre,Kyunghyun Cho, and Yoshua Bengio.
2015.
Gatedfeedback recurrent neural networks.
In Proceedings ofthe 32nd International Conference on Machine Learn-ing, ICML 2015, Lille, France, 6-11 July 2015, pages2067?2075.
[Chung et al2016] Junyoung Chung, Kyunghyun Cho,and Yoshua Bengio.
2016.
A character-level de-coder without explicit segmentation for neural ma-chine translation.
arXiv preprint arXiv:1603.06147.
[Dai et al2016] Zihang Dai, Lei Li, and Wei Xu.
2016.Cfo: Conditional focused neural question answeringwith large-scale knowledge bases.
In ACL.
[dos Santos and Gatti2014] Cicero dos Santos and MairaGatti.
2014.
Deep convolutional neural networks forsentiment analysis of short texts.
In Proceedings ofCOLING 2014, the 25th International Conference onComputational Linguistics: Technical Papers, pages69?78, Dublin, Ireland, August.
Dublin City Univer-sity and Association for Computational Linguistics.
[dos Santos et al2015] C?cero dos Santos, VictorGuimaraes, RJ Nitero?i, and Rio de Janeiro.
2015.Boosting named entity recognition with neural char-acter embeddings.
In Proceedings of NEWS 2015 TheFifth Named Entities Workshop, page 25.
[dos Santos2014] Cicero dos Santos.
2014.
Think posi-tive: Towards twitter sentiment analysis from scratch.In Proceedings of the 8th International Workshop onSemantic Evaluation (SemEval 2014), pages 647?651,Dublin, Ireland, August.
Association for Computa-tional Linguistics and Dublin City University.
[Fader et al2013] Anthony Fader, Luke Zettlemoyer, andOren Etzioni.
2013.
Paraphrase-driven learning foropen question answering.
In Proceedings of the 51stAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 1608?1618, Sofia, Bulgaria, August.
Association for Com-putational Linguistics.
[Fader et al2014] Anthony Fader, Luke Zettlemoyer, andOren Etzioni.
2014.
Open question answering overcurated and extracted knowledge bases.
In The 20thACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining, KDD ?14, New York,NY, USA - August 24 - 27, 2014, pages 1156?1165.
[Hochreiter and Schmidhuber1997] Sepp Hochreiter andJu?rgen Schmidhuber.
1997.
Long short-term memory.Neural Comput., 9(8):1735?1780, November.
[Huang et al2013] Po-Sen Huang, Xiaodong He, Jian-feng Gao, Li Deng, Alex Acero, and Larry Heck.2013.
Learning deep structured semantic models forweb search using clickthrough data.
In Proceedingsof the 22nd ACM international conference on Confer-ence on information & knowledge management, pages2333?2338.
ACM.
[Klein et al2003] Dan Klein, Joseph Smarr, Huy Nguyen,and Christopher D Manning.
2003.
Named entityrecognition with character-level models.
In Proceed-ings of the seventh conference on Natural language1606learning at HLT-NAACL 2003-Volume 4, pages 180?183.
Association for Computational Linguistics.[Ljubes?ic?
et al2014] Nikola Ljubes?ic?, Tomaz?
Erjavec,and Darja Fis?er.
2014.
Standardizing tweets withcharacter-level machine translation.
In Proceedings ofthe 15th International Conference on ComputationalLinguistics and Intelligent Text Processing - Volume8404, CICLing 2014, pages 164?175, New York, NY,USA.
Springer-Verlag New York, Inc.[Palangi et al2016] Hamid Palangi, Li Deng, YelongShen, Jianfeng Gao, Xiaodong He, Jianshu Chen,Xinying Song, and Rabab Ward.
2016.
Deep sen-tence embedding using long short-term memory net-works: Analysis and application to information re-trieval.
IEEE/ACM Transactions on Audio, Speech,and Language Processing, 24(4):694?707.
[Santos and Zadrozny2014] Cicero D Santos and BiancaZadrozny.
2014.
Learning character-level represen-tations for part-of-speech tagging.
In Proceedings ofthe 31st International Conference on Machine Learn-ing (ICML-14), pages 1818?1826.
[Shen et al2014] Yelong Shen, Xiaodong He, JianfengGao, Li Deng, and Gregoire Mesnil.
2014.
A latentsemantic model with convolutional-pooling structurefor information retrieval.
CIKM, November.
[Sukhbaatar et al2015] Sainbayar Sukhbaatar, Jason We-ston, Rob Fergus, et al 2015.
End-to-end memorynetworks.
In Advances in Neural Information Process-ing Systems, pages 2431?2439.
[Sutskever et al2014] Ilya Sutskever, Oriol Vinyals, andQuoc V Le.
2014.
Sequence to sequence learning withneural networks.
In Advances in neural informationprocessing systems, pages 3104?3112.
[Tang and Mooney2001] Lappoon R Tang and Ray-mond J Mooney.
2001.
Using multiple clause con-structors in inductive logic programming for semanticparsing.
In Machine Learning: ECML 2001, pages466?477.
Springer.
[Venugopalan et al2015] Subhashini Venugopalan, Hui-juan Xu, Jeff Donahue, Marcus Rohrbach, RaymondMooney, and Kate Saenko.
2015.
Translating videosto natural language using deep recurrent neural net-works.
In Proceedings of the 2015 Conference of theNorth American Chapter of the Association for Com-putational Linguistics: Human Language Technolo-gies, pages 1494?1504, Denver, Colorado, May?June.Association for Computational Linguistics.
[Vinyals et al2015] Oriol Vinyals, ?
ukasz Kaiser, TerryKoo, Slav Petrov, Ilya Sutskever, and Geoffrey Hinton.2015.
Grammar as a foreign language.
In C. Cortes,N.
D. Lawrence, D. D. Lee, M. Sugiyama, and R. Gar-nett, editors, Advances in Neural Information Process-ing Systems 28, pages 2773?2781.
Curran Associates,Inc.
[Xu et al2015] Kelvin Xu, Jimmy Ba, Ryan Kiros,Kyunghyun Cho, Aaron Courville, Ruslan Salakhudi-nov, Rich Zemel, and Yoshua Bengio.
2015.
Show,attend and tell: Neural image caption generation withvisual attention.
In David Blei and Francis Bach, ed-itors, Proceedings of the 32nd International Confer-ence on Machine Learning (ICML-15), pages 2048?2057.
JMLR Workshop and Conference Proceedings.
[Yih et al2014] Wen-tau Yih, Xiaodong He, and Christo-pher Meek.
2014.
Semantic parsing for single-relationquestion answering.
In Proceedings of ACL.
Associa-tion for Computational Linguistics, June.
[Yih et al2015] Wen-tau Yih, Ming-Wei Chang, Xi-aodong He, and Jianfeng Gao.
2015.
Semantic pars-ing via staged query graph generation: Question an-swering with knowledge base.
In Proceedings of theJoint Conference of the 53rd Annual Meeting of theACL and the 7th International Joint Conference onNatural Language Processing of the AFNLP.
ACLAssociation for Computational Linguistics, July.
[Zaremba and Sutskever2014] Wojciech Zaremba andIlya Sutskever.
2014.
Learning to execute.
arXivpreprint arXiv:1410.4615.
[Zhang et al2015] Xiang Zhang, Junbo Zhao, and YannLeCun.
2015.
Character-level convolutional networksfor text classification.
In C. Cortes, N. D. Lawrence,D.
D. Lee, M. Sugiyama, and R. Garnett, editors, Ad-vances in Neural Information Processing Systems 28,pages 649?657.
Curran Associates, Inc.1607
