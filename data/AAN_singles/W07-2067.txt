Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 308?313,Prague, June 2007. c?2007 Association for Computational LinguisticsSWAT-MP: The SemEval-2007 Systems for Task 5 and Task 14Phil Katz, Matthew Singleton, Richard WicentowskiDepartment of Computer ScienceSwarthmore CollegeSwarthmore, PA{katz,msingle1,richardw}@cs.swarthmore.eduAbstractIn this paper, we describe our two SemEval-2007 entries.
Our first entry, for Task 5:Multilingual Chinese-English Lexical Sam-ple Task, is a supervised system that decidesthe most appropriate English translation ofa Chinese target word.
This system uses acombination of Na?
?ve Bayes, nearest neigh-bor cosine, decision lists, and latent seman-tic analysis.
Our second entry, for Task 14:Affective Text, is a supervised system thatannotates headlines using a predefined list ofemotions.
This system uses synonym expan-sion and matches lemmatized unigrams inthe test headlines against a corpus of hand-annotated headlines.1 IntroductionThis paper describes our two entries in SemEval-2007.
The first entry, a supervised system used in theMultilingual Chinese-English Lexical Sample task(Task 5), is an extension of the system described in(Wicentowski et al, 2004).
We implement five dif-ferent classifiers: a Na?
?ve Bayes classifier, a decisionlist classifier, two different nearest neighbor cosineclassifiers, and a classifier based on Latent Seman-tic Analysis.
Section 2.2 describes each of the in-dividual classifiers, Section 2.3 describes our clas-sifier combination system, and Section 2.4 presentsour results.The second entry, a supervised system used in theAffective Text task (Task 14), uses a corpus of head-lines hand-annotated by non-experts.
It also uses anonline thesaurus to match synonyms and antonymsof the sense labels (Thesaurus.com, 2007).
Section3.1 describes the creation of the annotated trainingcorpus, Section 3.2 describes our method for assign-ing scores to the headlines, and Section 3.3 presentsour results.2 Task 5: Multilingual Chinese-English LSThis task presents a single Chinese word in contextwhich must be disambiguated.
Rather than askingparticipants to provide a sense label correspondingto a pre-defined sense inventory, the goal here is tolabel each ambiguous word with its correct Englishtranslation.
Since the task is quite similar to moretraditional lexical sample tasks, we extend an ap-proach used successfully in multiple Senseval-3 lex-ical sample tasks (Wicentowski et al, 2004).2.1 FeaturesEach of our classifiers uses the same set of contextfeatures, taken directly from the data provided by thetask organizers.
The features we used included:?
Bag-of-words (unigrams)?
Bigrams and trigrams around the target word?
Weighted unigrams surrounding the targetwordThe weighted unigram features increased the fre-quencies of the ten words before and after the tar-get word by inserting them multiple times into thebag-of-words.308Many words in the Chinese data were broken upinto ?subwords?
: since we were unsure how to han-dle these and since their appearance seemed incon-sistent, we decided to simply treat each subword as aword for the purposes of creating bigrams, trigrams,and weighted unigrams.2.2 ClassifiersOur system consists of five unique classifiers.
Threeof the classifiers were selected by our combinationsystem, while the other two were found to be detri-mental to its performance.
We describe the con-tributing classifiers first.
Table 1 shows the resultsof each classifier, as well as our classifier combina-tion system.2.2.1 Na?
?ve BayesThe Na?
?ve Bayes classifier is based on Bayes?
the-orem, which allows us to define the similarity be-tween an instance, I, and a sense class, Sj , as:Sim(I, Sj) = Pr(I, Sj) = Pr(Sj) ?
Pr(I|Sj)We then choose the sense with the maximum sim-ilarity to the test instance.Additive SmoothingAdditive smoothing is a technique that is usedto attempt to improve the information gained fromlow-frequency words, in tasks such as speech pat-tern recognition (Chen and Goodman, 1998).
Weused additive smoothing in the Na?
?ve Bayes classi-fier.
To implement additive smoothing, we added avery small number, ?, to the frequency count of eachfeature (and divided the final product by this ?
valuetimes the size of the feature set to maintain accurateprobabilities).
This small number has almost no ef-fect on more frequent words, but boosts the scoreof less common, yet potentially equally informative,words.2.2.2 Decision ListThe decision list classifier uses the log-likelihoodof correspondence between each context feature andeach sense, using additive smoothing (Yarowsky,1994).
The decision list was created by orderingthe correspondences from strongest to weakest.
In-stances that did not match any rule in the decisionlist were assigned the most frequent sense, as calcu-lated from the training data.2.2.3 Nearest Neighbor CosineThe nearest neighbor cosine classifier required thecreation of a term-document matrix, which containsa row for each training instance of an ambiguousword, and a column for each feature that can occurin the context of an ambiguous word.
The rows ofthis matrix are referred to as sense vectors becauseeach row represents a combination of the features ofall ambiguous words that share the same sense.The nearest neighbor cosine classifier compareseach of the training vectors to each ambiguous in-stance vector.
The cosine between the ambiguousvector and each of the sense vectors is calculated,and the sense that is the ?nearest?
(largest cosine, orsmallest angle) is selected by the classifier.TF-IDFTF-IDF (Term Frequency-Inverse Document Fre-quency) is a method for automatically adjusting thefrequency of words based on their semantic impor-tance to a document in a corpus.
TF-IDF decreasesthe value of words that occur in more different doc-uments.
The equation we used for TF-IDF is:tfi ?
idfi = ni ?
log( |D||D : ti?D|)where ni is the number of occurrences of a term ti,and D is the set of all training documents.TF-IDF is used in an attempt to minimize thenoise from words such as ?and?
that are extremelycommon, but, since they are common across alltraining instances, carry little semantic content.2.2.4 Non-contributing ClassifiersWe implemented a classifier based on Latent Se-mantic Analysis (Landauer et al, 1998).
To dothe calculations required for LSA, we used theSVDLIBC library1.
Because this classifier actu-ally weakened our combination system (in cross-validation), our classifier combination (Section 2.3)does not include it.We also implemented a k-Nearest Neighbors clas-sifier, which treats each individual training instance1http://tedlab.mit.edu/?dr/SVDLIBC/309as a separate vector (instead of treating each set oftraining instances that makes up a given sense as asingle vector), and finds the k-nearest training in-stances to the test instance.
The most frequent senseamong the k-nearest to the test instance is the se-lected sense.
Unfortunately, the k-NN classifier didnot improve the results of our combined system andso it is not included in our classifier combination.2.3 Classifier CombinationThe classifier combination algorithm that we imple-ment is based on a simple voting system.
Each clas-sifier returns a score for each sense: the Na?
?ve Bayesclassifier returns a probability, the cosine-based clas-sifiers (including LSA) return a cosine distance, andthe decision list classifier returns the weight asso-ciated with the chosen feature (if no feature is se-lected, the frequency of the most frequent sense isused).
The scores from each classifier are normal-ized to the range [0,1], multiplied by an empiricallydetermined weight for that classifier, and summedfor each sense.
The combiner then chooses the sensewith the highest score.
We used cross validation todetermine the weight for each classifier, and it wasduring that test that we discovered that the best con-stant for the LSA and k-NN classifiers was zero.
Themost likely explanation for this is that the LSA andk-NN are doing similar, only less accurate, classi-fications as the nearest neighbor classifier, and sohave little new knowledge to add to the combiner.We also implemented a simple majority voting sys-tem, where the chosen sense is the sense chosen bythe most classifiers, but found it to be less accurate.2.4 EvaluationTo increase the accuracy of our system, we needed tooptimize various parameters by running the trainingdata through 10-way cross-validation and averagingthe scores from each set.
Table 2 shows the results ofthis cross-validation in determining the ?
value usedin the additive smoothing for both the Na?
?ve Bayesclassifier and for the decision list classifier.We also experimented with different feature sets.The results of these experiments are shown in Ta-ble 3.Classifier Cross-Validation ScoreMFS 34.99%LSA 38.61%k-NN Cosine 61.54%Na?
?ve Bayes 58.60%Decision List 64.37%NN Cosine 65.56%Simple Combined 65.89%Weighted Combined 67.38%Classifier Competition ScoreSWAT-MP 65.78%Table 1: The (micro-averaged) precision of each ofour classifiers in cross-validation, plus the actual re-sults from our entry in SemEval-2007.Na?
?ve Bayes?
precision10?1 53.01%10?2 58.60%10?3 60.80%10?4 61.09%10?5 60.95%10?6 61.06%10?7 61.08%Decision List?
precision1.0 64.14%0.5 64.37%0.1 64.59%0.05 64.48%0.005 64.37%0.001 64.37%Table 2: On cross-validated training data, systemprecision when using different smoothing parame-ters in the Na?
?ve Bayes and decision list classifiers.2.5 ConclusionWe presented a supervised system that used simplen-gram features and a combination of five differentclassifiers.
The methods used are applicable to anylexical sample task, and have been applied to lexicalsample tasks in previous Senseval competitions.3 Task 14The goal of Task 14: Affective Text is to take a list ofheadlines and meaningfully annotate their emotionalcontent.
Each headline was scored along seven axes:six predefined emotions (Anger, Disgust, Fear, Joy,Sadness, and Surprise) on a scale from 0 to 100, andthe negative/positive polarity (valence) of the head-line on a scale from ?100 to +100.310Na?
?ve Bayes Feature Dec. List55.36% word trigrams 59.98%55.55% word bigrams 59.98%58.50% weighted unigrams 62.77%58.60% all features 64.37%NN-Cosine Feature Combined60.39% word trigrams 62.03%60.42% word bigrams 62.66%65.56% weighted unigrams 64.56%62.92% all features 67.38%Table 3: On cross-validated training data, the preci-sion when using different features with each classi-fier, and with the combination of all classifiers.
Allfeature sets include a simple, unweighted bag-of-words in addition to the feature listed.3.1 Training Data CollectionOur system is trained on a set of pre-annotated head-lines, building up a knowledge-base of individualwords and their emotional significance.We were initially provided with a trial-set of 250annotated headlines.
We ran 5-way cross-validationwith a preliminary version of our system, and foundthat a dataset of that size was too sparse to effec-tively tag new headlines.
In order to generate amore meaningful knowledge-base, we created a sim-ple web interface for human annotation of headlines.We used untrained, non-experts to annotate an addi-tional 1,000 headlines for use as a training set.
Theheadlines were taken from a randomized collectionof headlines from the Associated Press.We included a subset of the original test set inthe set that we put online so that we could get arough estimate of the consistency of human annota-tion.
We found that consistency varied greatly acrossthe emotions.
As can be seen in Table 4, our annota-tors were very consistent with the trial data annota-tors on some emotions, while inconsistent on others.In ad-hoc, post-annotation interviews, our anno-tators commented that the task was very difficult.What we had initially expected to be a tedious butmindless exercise turned out to be rather involved.They also reported that some emotions were consis-tently harder to annotate than others.
The results inTable 4 seem to bear this out as well.Emotion CorrelationValence 0.83Sadness 0.81Joy 0.79Disgust 0.38Anger 0.32Fear 0.19Surprise 0.19Table 4: Pearson correlations between trial data an-notators and our human annotators.One difficulty reported by our annotators was de-termining whether to label the emotion experiencedby the reader or by the subject of the headline.
Forexample, the headline ?White House surprised atreaction to attorney firings?
clearly states that theWhite House was surprised, but the reader might nothave been.Another of the major difficulties in properly an-notating headlines is that many headlines can be an-notated in vastly different ways depending on theviewpoint of the annotator.
For example, while theheadline ?Hundreds killed in earthquake?
would beuniversally accepted as negative, the headline ?Italydefeats France in World Cup Final,?
can be seen aspositive, negative, or even neutral depending on theviewpoint of the reader.
These types of problemsmade it very difficult for our annotators to provideconsistent labels.3.2 Data ProcessingBefore we can process a headline and determine itsemotions and valence, we convert our list of taggedheadlines into a useful knowledge base.
To this end,we create a word-emotion mapping.3.2.1 Pre-processingThe first step is to lemmatize every word in everyheadline, in an attempt to reduce the sparseness ofour data.
We use the CELEX2 (Baayen et al, 1996)data to perform this lemmatization.
There are unfor-tunate cases where lemmatizing actually changes theemotional content of a word (unfortunate becomesfortunate), but without lemmatization, our data issimply too sparse to be of any use.
Once we haveour list of lemmatized words, we score the emotionsand valence of each word as the average of the emo-311tions and valence of every headline, H, in which thatword, w, appears, ignoring non-content words:Score(Em,w) =?H: w ?
HScore(Em,H)In the final step of pre-processing, we add thesynonyms and antonyms of the sense labels them-selves to our word-emotion mapping.
We queriedthe web interface for Roget?s New Millennium The-saurus (Thesaurus.com, 2007) and added every wordin the first 8 entries for each sense label to our map,with a score of 100 (the maximum possible score)for that sense.
We also added every word in the first4 antonym entries with a score of ?40.
For exam-ple, for the emotion Joy, we added alleviation andamusement with a score of 100, and we added de-spair and misery with a score of ?40.3.2.2 ProcessingAfter creating our word-emotion mapping, pre-dicting the emotions and valence of a given headlineis straightforward.
We treat each headline as a bag-of-words and lemmatize each word.
Then we lookup each word in the headline in our word-emotionmap, and average the emotion and valence scores ofeach word in our map that occurs in the headline.We ignore words that were not present in the train-ing data.3.3 EvaluationEmotion Training Size (Headlines)100 250 1000Valence 19.07 32.07 35.25Anger 8.42 13.38 24.51Disgust 11.22 23.45 18.55Fear 14.43 18.56 32.52Joy 31.87 46.03 26.11Sadness 16.32 35.09 38.98Surprise 1.15 11.12 11.82Table 5: A comparison of results on the providedtrial data as headlines are added to the trainingset.
The scores are given as Pearson correlations ofscores for training sets of size 100, 250, and 1000headlines.As can be seen in Table 5, four out of six emotionsand the valence increase along with training set size.This leads us to believe that further increases in thesize of the training set would continue to improveresults.
Lack of time prevents a full analysis thatcan explain the sudden drop of Disgust and Joy.Table 6 shows our full results from this task.
Oursystem finished third out of five in the valence sub-task and second out of three in the emotion sub-task.Emotion Fine Coarse-GrainedA P RValence 35.25 53.20 45.71 3.42Anger 24.51 92.10 12.00 5.00Disgust 18.55 97.20 0.00 0.00Fear 32.52 84.80 25.00 14.40Joy 26.11 80.60 35.41 9.44Sadness 38.98 87.70 32.50 11.92Surprise 11.82 89.10 11.86 10.93Table 6: Our full results from SemEval-2007, Task14, as reported by the task organizers.
Fine-grainedscores are given as Pearson correlations.
Coarse-grained scores are given as accuracy (A), preci-sion (P), and recall (R).3.4 ConclusionWe presented a supervised system that used a un-igram model to annotate the emotional content ofheadlines.
We also used synonym expansion on theemotion label words.
Our annotators encounteredsignificant difficulty while tagging training data, dueto ambiguity in definition of the task.ReferencesR.H.
Baayen, R. Piepenbrock, and L. Gulikers.
1996.CELEX2.
LDC96L14, Linguistic Data Consortium,Philadelphia.S.
F. Chen and J. Goodman.
1998.
An empirical study ofsmoothing techniques for language modeling.
Techni-cal Report TR-10-98, Harvard University.T.K.
Landauer, Foltz P.W, and D. Laham.
1998.
Intro-duction to latent semantic analysis.
Discourse Pro-cesses, 25:259?284.Thesaurus.com.
2007.
Roget?s New Millennium The-saurus, 1st ed.
(v 1.3.1).
Lexico Publishing Group,LLC, http://thesaurus.reference.com.312Richard Wicentowski, Emily Thomforde, andAdrian Packel.
2004.
The Swarthmore CollegeSENSEVAL-3 System.
In Proceedings of Senseval-3,Third International Workshop on Evaluating WordSense Disambiguation Systems.David Yarowsky.
1994.
Decision lists for lexical am-biguity resolution: Application to accent restorationin Spanish and French.
In Proceedings of the 32ndAnnual Meeting of the Association for ComputationalLinguistics, pages 88?95.313
