Word Segmentation, Unknown-wordResolution, and Morphological Agreementin a Hebrew Parsing SystemYoav Goldberg?Ben Gurion University of the NegevMichael Elhadad?
?Ben Gurion University of the NegevWe present a constituency parsing system for Modern Hebrew.
The system is based on thePCFG-LA parsing method of Petrov et al(2006), which is extended in various ways in orderto accommodate the specificities of Hebrew as a morphologically rich language with a smalltreebank.
We show that parsing performance can be enhanced by utilizing a language resourceexternal to the treebank, specifically, a lexicon-based morphological analyzer.
We present acomputational model of interfacing the external lexicon and a treebank-based parser, also in thecommon case where the lexicon and the treebank follow different annotation schemes.
We showthat Hebrew word-segmentation and constituency-parsing can be performed jointly using CKYlattice parsing.
Performing the tasks jointly is effective, and substantially outperforms a pipeline-based model.
We suggest modeling grammatical agreement in a constituency-based parser as afilter mechanism that is orthogonal to the grammar, and present a concrete implementation ofthe method.
Although the constituency parser does not make many agreement mistakes to beginwith, the filter mechanism is effective in fixing the agreement mistakes that the parser does make.These contributions extend outside of the scope of Hebrew processing, and are of generalapplicability to the NLP community.
Hebrew is a specific case of a morphologically rich language,and ideas presented in this work are useful also for processing other languages, includingEnglish.
The lattice-based parsing methodology is useful in any case where the input is uncertain.Extending the lexical coverage of a treebank-derived parser using an external lexicon is relevantfor any language with a small treebank.1.
IntroductionDifferent languages have different syntactic properties.
In English, word order is rela-tively fixed, whereas in other languages word order is much more flexible (in Hebrew,the subject may appear either before or after a verb).
In languages with a flexible wordorder, the meaning of the sentence is realized using other structural elements, like word?
Computer Science Department, Ben Gurion University of the Negev, Israel.E-mail: yoav.goldberg@gmail.com.??
Computer Science Department, Ben Gurion University of the Negev, Israel.E-mail: elhadad@cs.bgu.ac.il.Submission received: 30 September 2011; revised submission received: 19 May 2012; accepted for publication:3 August 2012.?
2013 Association for Computational LinguisticsComputational Linguistics Volume 39, Number 1inflections or markers, which are referred to as morphology (in Hebrew, the marker !?
?is used to mark definite objects, distinguishing them from subjects in the same position.In addition, verbs and nouns are marked for gender and number, and subject and verbmust share the same gender and number).
A limited form of morphology also exists inEnglish: the -s and -ed suffixes are examples of Englishmorphological markings.
In otherlanguages, morphological processes may be much more involved.
The lexical units(words) in English are always separated by white space.
In Chinese, such separationis not available.
In Hebrew (and Arabic), most words are separated by white space,but many of the function words (determiners like the, conjunctions such as and, andprepositions like in or of ) do not stand on their own but are instead attached to thefollowing words.A large part of the parsing literature is devoted to automatic parsing of English, alanguage with a relatively simple morphology, relatively fixed word order, and a largetreebank.
Data-driven English parsing is now at the state where naturally occurring textin the news domain can be automatically parsed with accuracies of around 90% (accord-ing to standard parsing evaluation measures).
When moving from English to languageswith richer morphologies and less-rigid word orders, however, the parsing algorithmsdeveloped for English exhibit a large drop in accuracy.
In addition, whereas English hasa large treebank, containing over one million annotated words, many other languageshave much smaller treebanks, which also contribute to the drop in the accuracies ofthe data-driven parsers.
A similar drop in parsing accuracy is also exhibited in Englishwhen moving from the news domain, on which parsers have traditionally been trained,to other genres such as prose, blogs, poetry, product reviews, or biomedical texts, whichuse different vocabularies and, to some extent, different syntactic rules.This work focuses on constituency parsing of Modern Hebrew, a Semitic languagewith a rich and productive morphology, relatively free word order,1 and a small tree-bank.
Several natural questions arise: Can the small size of the treebank be compensatedfor using other available resources or sources of information?
How should the wordsegmentation issue (that function words do not appear in isolation but attach to the nextword, forming ambiguous letter patterns) be handled?
Can morphological informationbe used effectively in order to improve parsing accuracy?We present a system which is based on a state-of-the-art model for constituencyparsing, namely, the probabilistic context-free grammar (PCFG) with latent annotations(PCFG-LA) model of Petrov et al(2006), as implemented in the BerkeleyParser.
Afterevaluating the out-of-the-box performance of the BerkeleyParser on the Hebrew tree-bank, we discuss some of its limitations and then go on to extend the PCFG-LA parsingmodel in several directions, making it more suitable for parsing Hebrew and relatedlanguages.
Our extensions are based on the following themes.Separation of lexical and syntactic knowledge.
There are two kinds of knowledge inherentin a parsing system.
One of them is syntactic knowledge governing the way in whichwords can be combined to form structures, which, in turn, can be combined to formever larger structures.
The other is lexical knowledge about the identities of individualwords, the word classes they belong to, and the kinds of syntactic structures they canparticipate in.
We argue that the amount of syntactic knowledge needed for a parsingsystem is relatively limited, and that sufficiently large parts of it can be captured also1 To be more precise, in Hebrew the order of constituents is relatively free, whereas the order of the wordswithin certain constituents is relatively fixed.122Goldberg and Elhadad Parsing System for Hebrewbased on a relatively small treebank.
Lexical knowledge, on the other hand, is muchmore vast, and we should not rely on a treebank (small or large) to provide adequatelexical coverage.
Instead, we should aim to find ways of integrating lexical knowledge,which is external to the treebank, into the parsing process.We extend the lexical coverage of a treebank-based parser using a dictionary-basedmorphological analyzer.
We present a way of integrating the two resources also for thecommon case where their annotations schemes diverge.
This method is very effective inimproving parsing accuracy.Encoding input uncertainty using a lattice-based representation.
Sometimes, the languagesignal (the input to the parser) may be uncertain.
This happens in Hebrew when aspace-delimited token such as !????
can represent either a single word (?
[an] onion?)
or asequence of two words or three words (?in shadow?
and ?in the shadow,?
respectively).When computationally feasible, it is best to let the uncertainty be resolved by the parserrather than in a separate preprocessing step.We propose encoding the input-uncertainty in a word lattice, and use lattice parsing(Chappelier et al1999; Hall 2005) to perform joint word segmentation and syntacticdisambiguation (Cohen and Smith 2007; Goldberg and Tsarfaty 2008).
Performing thetasks jointly is effective, and substantially outperforms a pipeline-based model.Using morphological information to improve parsing accuracy.
Morphology provides usefulhints for resolving syntactic ambiguity, and the parsing model should have a way ofutilizing these hints.
There is a range of morphological hints than can be utilized: fromfunctional marking elements (such as the !??
marker indicating a definite direct object);to elements marking syntactic properties such as definiteness (such as the Hebrew !
?marker); to agreement patterns requiring a compatibility in properties such as gender,number, and person between syntactic constituents (such as a verb and its subject oran adjective and the noun it modifies).We suggest modeling agreement as a filtering process that is orthogonal to thegrammar.
Although the constituency parser does not make many agreement mistakesto begin with, the filter mechanism is effective in fixing the agreement mistakes that theparser does make, without introducing new mistakes.Aspects of the work presented in this article are discussed in earlier publica-tions.
Goldberg and Tsarfaty (2008) suggest the lattice-parsing mechanism, Goldberget al(2009) discuss ways of interfacing a treebank-derived PCFG-parser with an exter-nal lexicon, and Goldberg and Elhadad (2011) present experiments using the PCFG-LABerkeleyParser.
Here we provide a cohesive presentation of the entire system, as well asa more detailed description and an expanded evaluation.
We also extend the previouswork in several dimensions: We introduce a new method of interfacing the parser andthe external lexicon, which contributes to an improved parsing accuracy, and suggestincorporating agreement information as a filter.The methodologies we suggest extend outside the scope of Hebrew processing,and are of general applicability to the NLP community.
Hebrew is a specific case ofa morphologically rich language, and ideas presented in this work are useful also forprocessing other languages, including English.
The lattice-based parsing methodologyis useful in any case where the input is uncertain.
Indeed, we have used it to solvethe problem of parsing while recovering null elements in both English and Chinese(Cai, Chiang, and Goldberg 2011), and others have used it for the joint segmentationand parsing of Arabic (Green and Manning 2010).
Extending the lexical coverage ofa treebank-derived parser using an external lexicon is relevant for any language with123Computational Linguistics Volume 39, Number 1a small treebank, and also for domain adaptation scenarios for English.
Finally, theagreement-as-filter methodology is applicable to any morphologically rich language,and although its contribution to the parsing task may be limited, it is of wide applica-bility to syntactic generation tasks, such as target-side-syntax machine translation in amorphologically rich language.2.
Modern Hebrew2.1 Lexical and Syntactic PropertiesSome relevant lexical and syntactic properties of Modern Hebrew are highlighted in thissection.2.1.1 Unvocalized Orthography.
Most vowels are not marked in everyday Hebrewtext, which results in a very high level of lexical and morphological ambiguity.
Sometokens can admit as many as 15 distinct readings, and the average number of pos-sible morphological analyses per token in Hebrew text is 2.7, compared with 1.4 inEnglish (Adler 2007).
The word !???????
can be read in at least eight different ways(?spoons,?
?square cotton headkerchiefs,?
?coercions,?
?as mouths,?
?as spouts,?
?as fairies,??ungratefulness,?
?fun/adjectivefeminine,plural?
), the word !????
in at least six ways (?ajournalist,?
?writing,?
?script,?
?wrote,?
?added someone as a recipient,?
?was added asa recipient?)
and the word !??
can be read as a very common case-marker (appearingbefore definite direct objects), a very common pronoun (?you/feminine?
), and a noun(?shovel?
).2.1.2 Affixation.
Eight common prepositions, conjunctions, and articles may neverappear in isolation and must always be attached as prefixes to the following word.2These include the function words !??
(?from?
), !?
(?which?/?who?/?that?
), !???
(?when?),!?
(?the?
), !?
(?and?
), !??
(?like?
), !?
(?to?
), and !?
(?in?),.
Several such elements may attachtogether, producing forms such as !?????????
( !?-?-??-?-????
?and-that-from-the-sun?).
Noticethat when it appears by itself, the last part of the token, the noun !????
(?sun?
), can alsobe interpreted as the sequence !?-???
(?who moved?).
The linear order of such elementswithin a token is fixed (disallowing the reading !?-?-??-?-?-???
in the previous example).The syntactic relations of these elements with respect to the rest of the sentenceare rather free, however.
The relativizer !?
(?that?
), for example, may attach to anarbitrarily long relative clause that goes beyond token boundaries.
The attachment insuch cases encompasses a long-distance dependency that cannot be captured by local-context (or Markovian) sequential processes that are typically used for morphologicaldisambiguation.
The same argument holds for resolving PP attachment of a prefixedpreposition or marking conjunction of elements of any kind.To further complicate matters, the definite article !?
(?the?)
is not realized in writingwhen following the particles !?
(?in?
), !??
(?like?
), and !?
(?to?).
Thus, the form !????
can beinterpreted as either !?-???
(?in house?)
or !?-?-???
(?in the house?
).32 In what follows, we indicate the correct segmentations of the different forms.
Naturally occurringHebrew text does not have such indications.3 This overt element is in fact indicated by vocalization, but is not realized in standard written text.124Goldberg and Elhadad Parsing System for HebrewIn addition, pronominal elements (clitics) may attach to nouns, verbs, adverbs,prepositions, and others as suffixes (e.g., !?????
[ !????-?
?, ?brought-them?
], !?????
[ !-?????,?onthem?
]).These affixations result in highly ambiguous token segmentations: !???????
(?
[they]assigned numbers?)
vs.
!??????-?
(?his number?
or ?the one who cuts his hair?)
vs.
!-???-????
(?from his book?
or ?from his barber?
), !??????
(?putting together?)
vs.
!?-?????
(?the train?
),and !????
(?an onion?)
vs.
!?-???
(?in the shadow?)
are only a few examples of ambiguitiesthat may arise.
Quantitatively, 99,896 out of 567,483 forms (17%) in a wide-coveragelexicon of Hebrew can admit both segmented and unsegmented analyses.In many cases the correct segmentation cannot be determined from local contextalone, but can be disambiguated by more global syntactic constraints (in ?????
?????!??????
?, the middle token is ambiguous between !?????
[?sky?]
and !?-????
[?that/rel water?
],and the sequence can be interpreted as either ?I saw blue skies?
or ?I saw that blue water.
?On the other hand, !????
???
??????
???????
?????
?????
is unambiguous because the pastverb !??????
requires the relativizer !
?, allowing only the segmented !?-????
reading ?I sawthat blue water broke from the well?.
In the other direction, ???????
???????
?????
?????!?????
is also unambiguous, allowing only the unsegmented reading ?I saw blue skiesand went to sleep?.
)2.1.3 Rich Templatic Morphology.
Hebrew words follow a complex morphological struc-ture, which is based on a root + template system, with both derivational and inflectionalelements.
Word forms can encode gender, number, person, and tense, and in additionnoun-compounding is also morphologically marked (see Section 2.1.7).
Although theexact details of the system are irrelevant (but see Adler [2007] and Glinert [1989] for agood overview), we note that this word formation mechanism results in a very highnumber of possible word forms, and that it is hard to guess the part-of-speech of wordsbased on prefixes and suffixes alone, a method frequently used in other languages.2.1.4 The Participle Form.
The Hebrew participle form ( !???????
?, literally the ?middle form?of verbs) is a form that shares morphological and syntactic properties of nouns, verbs,and adjectives.
This form causes many disagreements between human annotators, andlarge disagreement is found also between major Hebrew dictionaries regarding manyword forms (see Adler et al[2008b] for a discussion from tag set design and annotationguidelines, including many syntactic, semantic, and lexical considerations).
For thepurpose of this work, this form is of interest as it highlights the inherent ambiguitybetween adjectival, nominal, and verbal readings of many words, which are hard todisambiguate even in context.2.1.5 Relatively Free Constituent Order.
The ordering of constituents inside a phrase isrelatively free.
This is most notably apparent in verbal phrases and sentential levels.
Inparticular, whereas most sentences follow a subject-verb-object order (SVO), OVS andVSO configurations are also possible (counting in the Hebrew Treebank reveals 5,720SV cases and 2,613 VS cases, compared with 81,135 SV and 3,018 VS constructions inthe English WSJ Treebank).
In addition, verbal arguments can appear before or afterthe verb, and in many orders.
Such variations in constituent order are easy to captureusing ?flat?
S structures putting the verbs and all of its arguments on the same clausallevel, and this is the annotation approach adopted by the Hebrew Treebank (as wellas by treebanks of other languages, such as French [Abeille?, Cle?ment, and Toussenel2003]).
These flat structures result in the grammar having more and longer rules and thetreebank having fewer instances of each rule type, however, causing a data sparseness125Computational Linguistics Volume 39, Number 1problem for statistical estimation methods based on treebank counts, and making itmore difficult to reliably estimate the grammar parameters.2.1.6 Verbless Constructions.
Several constructions in which the verb is not realized arecommon in Hebrew.
These include the possessive constructions such as ?????????
?????!????
(?to-Ido toys many?
meaning ?ido has many toys?
), which also feature a flexibleconstituent order !?????
????
?????????
(?toys many to-Ido?, ?ido has many toys?
), andcopular constructions such as !?????
????
(?the-boy cute?
?the boy is cute?)
and !??????
????
(?the-boy crazy?
?the boy is crazy?
).2.1.7 NP Structure and Construct-State.
Although constituent order may vary, NPinternal structure is rigid.
A special morphological marker (construct state, or !????????
)is used to mark noun-compounds as well as similar phenomena (this is similar to theidafa construction in Arabic).4 Noun compounding in Modern Hebrew is productiveand very frequent?about a quarter of the noun tokens in the Hebrew Treebank are inthe construct state.
Construct-state nouns can be highly ambiguous with non-construct-state nouns.
Some forms are morphologically marked but the marking is not present inunvocalized text ( !????
?/banot vs.
!????
?/bnot), and some forms are not marked at all ( !????
).The construct-state marker, although ambiguous, is essential for analyzing NP internalstructure.
Where regular nouns are marked for definiteness using the definite marker!
?, construct-nouns acquire the definite status of the noun-phrase they compound to.Construct constructions may be nested, as in !????????
??????
??????
????
????
(?shadeconstcolorconst lidconst boxconst the apples,?
meaning ?the shade of the color of the lid of the boxof the apples?
).2.1.8 Definiteness.
Definiteness is spread across many elements in the NP.
All elementsin a definite NP, except for construct-nouns and proper-names, are explicitly markedusing the functional element !?
that is prefixed to the token.
Proper-names are inher-ently definite and cannot take the definite marker, and construct-nouns acquire theirdefiniteness status from the NP they dominate (definiteness is not explicitly marked onconstruct-nouns).2.1.9 Case Marking.
Definite direct objects are marked.
The case marker in this case isthe function word !??
appearing before the direct object.
Subjects, indirect objects, andnon-definite direct objects are not marked.2.1.10 Agreement.
Hebrew grammar forces morphological agreement between adjec-tives and nominals (adjectives appear after the noun, and agree in gender, number, anddefiniteness), and between subjects and verbs (including the verbless copular construc-tions), which agree in gender, number, and person.
Agreement in the predicative caseis a bit complex: When the verb is overt and the predicative-complement is a noun,as in !?????
???
???????
(?the-tripfem isfem an-excusemasc?
), gender and number agreementare required between the subject and the verb (but not the predicative-complement),but in the verbless case, the subject and the predicate-complement noun must agree( !?????
???????
* ?the-tripfem an-excusemasc?).
When the predicate-complement is an adjec-tive, gender and number agreement between the subject and the predicate-complement4 The construct state is not restricted to nouns, and can also appear on numbers (e.g., !?????
!?????/?tens-ofkids?)
and adjectives ( !????????
!???
?/?biggest-of authors?
).126Goldberg and Elhadad Parsing System for Hebrewis required regardless of the realization of the verb/copular element: !????
???
?, ?????!????
*, !????
???
???
?, !????
???
?????
* (?the-boy tallmasc?, ?
*the-boy tallfem?, ?the-boy ismasctallmasc?, ?
*the-girl isfem tallmasc?
).2.2 Implications for ParsingAfter surveying some lexical and syntactic properties of Modern Hebrew, we turnto highlight some aspects in which Modern Hebrew differs from English from theperspective of parsing system design.2.2.1 Small Amount of Annotated Data.
Whereas the English Treebank is relatively large(49,208 sentences, or 1,173,766 words), the Hebrew Treebank (Guthmann et al2009) ismuch smaller, containing only 6,220 sentences, or 115,661 tokens (156,316 words5).The small size of the Hebrew Treebank implies a smaller training set for learning-algorithms used to construct the parser.2.2.2 Ambiguous Word Segmentation.
Syntactic parsing systems treat the input sentenceas observed data?the leaves (in constituency parsing) of the tree are known in advance,and the parser is expected to build a parse tree around them.
This is not the case inHebrew, where many function words are not separated by white space but instead areprefixed to the next word and appear within the same token.
This makes the wordsequence unobserved to the parser, which has to infer both the syntactic-structure andthe token segmentation.6One possible solution to the unobserved word-sequence problem is a pipelinesystem in which an initial model is in charge of token-segmentation, and the output ofthe initial model is fed as the input to a second stage parser.
This is a popular approachin parsing systems for Arabic and Chinese (Jiang, Huang, and Liu 2009; Green andManning 2010).
As discussed in Section 2.1.2 (as well as in Tsarfaty [2006a], Goldbergand Tsarfaty [2008], and Cohen and Smith [2007]), however, the token-segmentation andsyntactic-parsing tasks are closely intertwined and are better performed jointly insteadof in a pipeline fashion, which is the approach we explore in this work.2.2.3 Morphological Variation and High Out-of-Vocabulary Rate.
The intrinsic deficiencycaused by the small amount of training data is made even more severe due to Hebrew?srich morphological inflection patterns.
The high amount of morphological variationmeans that many word forms will not be observed in the training data, making it harderto reliably estimate lexical probabilities based on the annotated resources alone.Unlike English, where parts-of-speech for words are relatively easy to guess basedon simple orthographic features (words starting with capital letters are proper nouns,words ending in -ed are usually verbs, etc.
), this is not the case for Hebrew.
Amongthe 773 words appearing in English test data but not in the training data, 269 startwith a capital letter, 58 end with -ed, and 49 end with -ing.
Together, these three simpleheuristics cover almost half of the unobserved tokens.
Such heuristics are not availablefor Hebrew in the common case of unvocalized text: Proper names are not marked5 Because of agglutination, a Hebrew token may consist of several words, for example the token !???
?comprises the two words !?(?in?)
and !???(?house?
).6 Token segmentation is sometimes (erroneously) referred to as morphological segmentation.127Computational Linguistics Volume 39, Number 1in writing, and word prefixes and suffixes are not indicative of the part-of-speechtags.7 Thus, the out-of-vocabulary (OOV) problem is much harder in Hebrew than inEnglish and other European languages: On the one hand many words are unobservedin training, and on the other, it is more difficult to guess the analysis of such unknownwords.A system for handling automatic processing of Hebrew text cannot rely solely onmanually annotated corpora, as such corpora cannot provide adequate lexical coverage.Systems that attempt to perform disambiguation on the lexical level (such as sequence-based morphological disambiguators, or syntactic parsers that perform morphologicaldisambiguation as part of the parsing process) should be designed to incorporate lexicalknowledge from sources external to the annotated corpora.
We discuss methods of en-hancing the system?s performance based on a resource that is external to the treebank: Alexicon-based broad-coverage morphological analyzer enhanced with semi-supervisedprobability estimates based on expectation maximization (EM) training of a hiddenMarkov model (HMM) tagger on a large amount of unannotated text.2.2.4 Morphological Agreement.
The rich morphological system also means that wordscarry large amounts of extra information: definiteness, gender, number, tense, andperson.
Some of this information interacts with syntax through agreement constraints.Specifically, nouns and adjectives should agree in gender and number, and subjectsand verbs should agree in gender, number, and person.
Agreement constraints canprovide useful hints for disambiguating the syntactic structure.
Consider for examplethe sentence !??????
??
??????
????
???
(?wife of the man who ate the apple?).
TheEnglish sentence is ambiguous with respect to the entity who ate the apple, but theHebrew version is not?the verb !?????
(?ate?)
is in feminine form, indicating that it wasthe wife who did the eating.
Can a parsing system make use of such information?
Thisissue is investigated further in Section 8.2.2.3 Existing Resources for Hebrew Text ProcessingSeveral linguistic resources are available for Hebrew, and are used as building blocksfor the parsing systems described in this work.2.3.1 The Hebrew Constituency-Treebank.
A constituency treebank of Modern Hebrew,incrementally developed at the Technion over the course of more than eight years(Sima?an et al2001; Guthmann et al2009), is maintained by MILA, the knowledgecenter for processing Hebrew.8 The current version of the treebank (Version 2) contains6,220 sentences taken from the Israeli daily newspaper !????
(Ha?aretz).
The sentencesare manually annotated on both the lexical and the syntactic levels.
Each token9 issegmented into words, and eachword is assigned a part of speech tag that also captures,7 Although the suffixes are good indicators of gender and number ( !??
is usually plural masculine, !?
isusually singular feminine), they are not good at indicating the core part-of-speech ( !?
is a suffix canappear in adjectives !???
?, verbs !????
?, nouns !????
?, and similarly for !??
( !????
?, !????????
?, !?????????).
Furthermore,due to the root+template system, in most cases the first and last letters of the word are part of the rootand not of the pattern !??????!,????
?, making the suffixes even less indicative.8 http://www.mila.cs.technion.ac.il/mila/eng/index.html.9 As discussed in Section 2.1.2, Hebrew tokens (entities separated by white space and punctuation symbols)do not necessarily correspond to Hebrew words.
A single token may contain several words.128Goldberg and Elhadad Parsing System for Hebrewwhere applicable, the morphological properties of the word such as number, gender,and person.
Then a constituency tree is built on top of the segmented words.
Theannotation of NPs is relatively nested, and the sentence level structures are relativelyflat (the verb and all of its arguments reside on one level under S).
The treebank has115,661 tokens and 156,764 words.The POS tagging scheme in the treebank is highly syntactic in nature: A part-of-speech is chosen to reflect the syntactic function of the given word in context.
For exam-ple, demonstrative pronouns are tagged in the treebank as adjectives when appearingin an adjectival position ( !??
!??
?, ?this/JJ child/NN?
), and a special MOD tag is used tomark non-adverbial clausal level modification (that is, modifications that can be treatedas adverbial, but that are used to modify something other than a verb).
For a moredetailed description of the Constituency Treebank see Sima?an et al(2001), Guthmannet al(2009), and Tsarfaty (2010, pages 199?216), as well as the annotation guidelines.102.3.2 Train/dev/test Splits.
Throughout the article, we follow the established train/dev/test split for the treebank, namely, sentences 1?483 are used for development,sentences 484?5,740 are used for training the parser, and sentences 5,741 to 6,220 areused as the final test set.2.3.3 The MILA Broad-Coverage Lexicon.
Aside from the Constituency Treebank, Hebrewhas a wide-coverage, lexicon-based morphological analyzer which can assign morpho-logical analyses (prefixes, suffixes, core POS, gender, number, person, etc.)
to Hebrewtokens.
The lexicon (henceforth the KC Analyzer) is developed and maintained bythe Knowledge Center for Processing Hebrew (Itai and Wintner 2008).
It is based on alexicon of roughly 25,000word lemmas and their inflection patterns.
From these, 562,439unique word forms are derived.
These are then prefixed (subject to constraints) by 73prepositional prefixes.
Even with this seemingly large vocabulary, the KC Analyzer?scoverage is not perfect.
In Adler et al(2008a), we present a machine-learning methodthat is trained on the basis of the analyzer and that can guess possible analyses forwords unknown to the analyzer with reasonable accuracies.
Using this extension, theanalyzer has perfect coverage (even though the quality is obviously better for wordsthat are present in the analyzer?s database).The tag set used by the lexicon/analyzer is lexicographic in nature, and is discussedin depth in BGU Computational Linguistics Group (2008).Creating a resource such as the morphological analyzer for a morphologically richlanguage is a worthwhile and cost-effective effort: After establishing the tag set, it isrelatively straightforward to add lemmas to the lexicon, and the automatic inflectionprocess guarantees good coverage of all the possible inflections.
This is much moreefficient than annotating enough text to obtain a similar coverage.2.3.4 Hebrew Morphological Disambiguator.
The morphological analyzer provides thepossible set of analyses for each token, but does not disambiguate the correct analy-sis in context.
A morphological disambiguator (henceforth ?the Hebrew tagger?
or?tagger?)
was developed by Meni Adler at Ben-Gurion University of the Negev(Adler and Elhadad 2006; Adler 2007; Goldberg, Adler, and Elhadad 2008).
After the(extended) morphological analyzer assigns the possible analyses for each token in an10 http://www.mila.cs.technion.ac.il/mila/files/treebank/Decisions-Corpus1-5001.v1.pdf.129Computational Linguistics Volume 39, Number 1input sentence, the tagger takes the output of the analyzer as input and chooses the sin-gle best analysis for the entire sentence (performing both token segmentation of wordsand part-of-speech assignment for each word).
The tagger is an HMM-based sequentialmodel that is trained in a semi-supervised fashion using EM based on the output of themorphological analyzer on a large amount (about 70M words) of unannotated Hebrewtext.
The tagger is described in Adler and Elhadad (2006) and Adler (2007).The tagger is relatively accurate: It achieves 93% accuracy in predicting segmen-tation and tagging when measured on the POS accuracy, and 90% accuracy whenmeasured on the complete tag set, which includes the complete set of morphologicalfeatures.
Because the tagger is not trained on a particular annotated training set butinstead on a very large corpus of text spanning multiple genres, its performance isrobust across domains.The tagger?s success is due in part to a smart initialization procedure to the EMtraining process.
This initialization procedure takes the output of the analyzer andassigns a conditional probability distribution P(tag|word) for each word.
In other words,it assigns an a priori, context-free likelihood for each analysis of a word (although theword broke can be either a verb in the past tense or an adjective, it is more likely to be theformer; such preferences can be modeled as probability distributions, and the initializa-tion procedure attempts to learn the values of these distributions automatically fromraw data).
This initialization procedure is described in Goldberg, Adler, and Elhadad(2008).A side effect of the EM?HMM training of the tagger is pseudo-counts for ?word, tag?events, which are based on patterns observed in the unannotated training data.
We usethese counts in order to improve the lexical-disambiguation capacity of the parser.2.3.5 A Resource Incompatibility Issue.
Unfortunately, the KC Analyzer adopted a dif-ferent tag set than the one used in the treebank, and analyses produced by the KCAnalyzer (and hence by the morphological disambiguator) are incompatible with theHebrew Treebank.
These are not mere technical differences, but derive from differentperspectives on the data.
The Hebrew Treebank (TB) tag set is syntactic in nature (?ifthe word in this particular position functions as an adverb, tag it as an adverb, eventhough it is listed in the dictionary only as a noun?
), whereas the KC tag set (Adler2007; Netzer et al2007; Adler et al2008b) takes a lexical approach to POS tagging(?a word can assume only POS tags that would be assigned to it in a dictionary?).
Thelexical approach does not accommodate generic modification POS tags such as MOD,nor does it allow listing of demonstrative pronouns as adjectives.These divergent perspectives are reflected in different guidelines to human taggers,different principles underlying tag definitions, and different verification procedures.This difference in perspective yields different performances for parsers induced fromtagged data, and a simple mapping between the two schemes is impossible to define.Some Hebrew forms, particularly the present participle and modal forms, are in-herently hard to define, and the wide disagreement about their status is reflected inpractically all Hebrew dictionaries.
This kind of disagreement naturally appears alsobetween the KC and TB.
See Adler et al(2008b) and Netzer et al(2007) for furtherdiscussion on these two interesting cases.Bridging the discrepancy between the two resources is an important aspect in thecreation of a successful parsing system.
On the one hand the syntactic annotations in thetreebank are needed in order to train the parser, and on the other hand the informationprovided by the morphological analyzer is needed in order to provide a good lexicalcoverage.
We discuss an approach to bridging this discrepancy in Section 6.130Goldberg and Elhadad Parsing System for Hebrew2.4 Section SummaryTo summarize, the Hebrew language and its analysis poses several challenges to parserdesign: The amount of annotated material is relatively small, precluding the possibilityof learning robust lexical parameters from the annotated corpora.
The productivenature of the morphology results in many word forms, adding another obstacle toestimating lexical parameters from annotated data.
The nature of the word-formationmechanism in Hebrew makes it hard to guess the morphological analysis of a wordbased on its prefix and suffix alone as is done in other languages, requiring the use of amore complex system for handling unknown words.
Many function words in Hebreware not separated by white space but are instead attached to the next token, makingthe observed word sequence ambiguous.
Word segmentation needs to be performedin addition to syntactic disambiguation.
Successful word segmentation may rely onsyntactic disambiguation, suggesting that it is better to perform the segmentationand syntactic-disambiguation tasks jointly.
Finally, Hebrew grammar requires variousforms of morphological agreement, a fact which hopefully can help disambiguateotherwise ambiguous syntactic structures.
The syntactic parser should be able to makeuse of agreement information.In terms of existing resources, Hebrew has a small treebank annotated with con-stituency structure and a broad-coverage, manually constructed, lexicon-based mor-phological analyzer.
The morphological analyzer is capable of providing the possiblemorphological analyses for many lexical forms, and it is extended using a machine-learning technique to also provide possible analyses for word-forms not covered bythe lexicon.
The extended lexicon provides a good lexical coverage of Hebrew.
Alsoavailable is a morphological disambiguator that is capable of associating probabilities tothe possible analyses of the lexical forms in the lexicon, and disambiguating the analysesof a sequence of lexical items in context based on a sequential model.
The constituencytreebank can be used to learn the parameters of a syntactic-model of Hebrew, andthe morphological analyzer can be used to provide broad-coverage lexical knowledge.Unfortunately, the treebank and the lexicon/disambiguator follow different annotationschemes, and are therefore incompatible with each other.
The annotation gap betweenthe two resources must be bridged before they can be used together.We now turn to survey the components of our Hebrew parsing system.3.
Latent-Annotation State-Split Grammars (PCFG-LA)Klein and Manning (2003) demonstrated that linguistically informed splitting of non-terminal symbols in treebank-derived grammars can result in accurate grammars.
Theirwork triggered investigations in automatic grammar refinement and state-splitting(Matsuzaki, Miyao, and Tsujii 2005; Prescher 2005), which was then perfected in workby Petrov and colleagues (Petrov et al2006; Petrov and Klein 2007; Petrov 2009).State-split models assume that each non-terminal label has a latent annotation thatshould be recovered.
Instead of a single NP symbol, these models hypothesize that thereare many different NP symbols, NP1, .
.
.
,NPk, and each is used in a different context.The labels are hidden, however, and we can only observe the core category label (NP).The job of the training process is to come up with the hidden set of label assignmentsto non-terminals, such that the resulting grammar assigns a high probability to theobserved treebank data.
Such models are called PCFG with latent annotations (PCFG-LA) and are shown empirically to produce very accurate parsing results.131Computational Linguistics Volume 39, Number 1The model of Petrov et al(2006) and its publicly available implementation, theBerkeleyParser,11 learns the latent annotations by starting with a bare-bones treebank-derived grammar and automatically refining it in split-merge-smooth cycles, setting theparameters using EM.
We provide a brief description of the model and learning process(refer to Petrov et al2006; Petrov and Klein 2007; Petrov 2009 for the full details).The learning works by following an iterative split-merge-smooth cycle, in whichthe following steps are performed repetitively:Splitting each non-terminal category in two All of the grammar symbols are split.
Inthe first round, NP is split into NP1 and NP2.
In the second round these aresplit into NP11, NP12, NP21, NP22, and so forth.
Each splitting round results innew grammar in which a rule of the form A ?
BC is replaced by eight rules, theresult of splitting each A, B, and C in two.
An EM procedure is then used to setthe probabilities of each of the split rules.
The EM training is constrained by thegrammar on the one hand and by the annotated tree structures on the other.Merging back non-effective splits Not all of the splits are useful.
For example, thepunctuation POS tag will always result in punctuation, and there is no reasonto split it into two punctuation POS tags.
Having a grammar with too many statesis difficult to manage in terms of memory, storage, and parsing time, and is alsoprone to overfitting the data.
Thus, the model aims to undo splits if they are notuseful.
The splits are evaluated based on an information gain criteria, and splitsthat are not useful are merged back into their parent symbol, resulting in a smallergrammar (if the symbols B1 and B2 are merged back into B, the rules A ?
B1 Cand A ?
B2 C are merged into A ?
B C).
The merging step is also followed by anEM procedure for setting the rule probabilities for the resulting grammar.Smoothing the split non-terminals toward their shared ancestor Finally, split sym-bols may still share some information (although an NP in subject position andan NP in object position behave differently, they also retain some common prop-erties).
The smoothing procedure joggles the probability mass of the grammarand moves some probability from the split symbol to its parent.
This step is alsofollowed by parameter re-estimation using EM.Performing five or six such split-merge-smooth cycles results in accurate grammars,with annotations that capture many latent syntactic interactions.
Six cycles mean thatsymbols can have as many as 64 different substates.At inference time, the latent annotations are (approximately) marginalized out,resulting in the (approximate) most probable unannotated tree according to the refinedgrammar (the score of the unsplit rule A ?
B C is taken to be?x?y?zAx ?
By Cz).The grammar learning process is applied to binarized parse trees, with first-ordervertical and zeroth-order horizontal markovization (Klein and Manning 2003).
Thismeans that in the initial grammar, each of the non-terminal symbols is effectivelyconditioned on its parent alone, and is independent of its sisters.
For example, the ruleS ?
NP VP NP PP is binarized as:S ?
NP @S@S ?
VP @S@S ?
NP @S@S ?
PP11 http://code.google.com/p/berkeleyparser/.132Goldberg and Elhadad Parsing System for Hebrewindicating that S rules start with an NP, can be followed by a sequence of zero ormore NPs and VPs, and end with a PP.
Such an extreme markovization suggests avery strong independence assumption, and is too permissive on its own.
It allows theresulting refined grammar to encode its own set of dependencies between a node andits sisters, however, as well as ordering preferences in long, flat rules.
For example,the binarized grammar allows the production S ?
NP NP PP, which may be incorrect.However, by annotating the symbols as follows:S ?
NP @S1@S1 ?
VP @S2@S2 ?
NP @S2@S2 ?
PPthe grammar now forces the VP to be produced before the NP, but still allows the NP tobe dropped.
Similarly, by annotating the symbols as:S ?
NP @S1@S1 ?
VP @S2@S2 ?
NP @S3@S3 ?
PPthe grammar effectively allows only the original rule to be produced.Initial experiments on Hebrew confirm that moving to higher order horizontalmarkovization (encoding more context in the initial binarized rules) degrades parsingperformance, while producing much larger grammars.The PCFG-LA parsing methodology is very robust, producing state-of-the-art accu-racies for English, as well as many other languages including German (Petrov and Klein2008), French (Candito, Crabbe?, and Seddah 2009), and Chinese (Huang and Harper2009).4.
Baseline ExperimentsThe baseline system is an ?out-of-the-box?
PCFG-LA parser, as described in Petrovet al(2006) and Petrov and Klein (2007) and implemented in the BerkeleyParser.12The parser is trained on the Modern Hebrew Treebank (see Section 9 for the exactexperimental settings) after stripping all the functional and morphological informationfrom the non-terminals.We evaluate the resulting models on the development set, and consider threesettings:Seg+POS Oracle: The parser has access to the gold segmentation and POS tags.Seg Oracle: The parser has access to the gold segmentation, but not the POS tags.Pipeline: A POS-tagger is used to perform word segmentation, which is then used asparser input.A better tag set.
Glossing over the parses revealed that the parser failed to learnthe distinction between finite and non-finite verbs.
The importance of this linguistic12 http://code.google.com/p/berkeleyparser/.133Computational Linguistics Volume 39, Number 1Table 1Baseline: Out-of-the-box BerkeleyParser performance on the dev-set.Setting Tag set F1 (4 cycles) F1 (5 cycles)Seg+POS Oracle Core 89.7 89.5Seg Oracle Core 82.6 83.6Pipeline Core 76.3 77.2Seg+POS Oracle Core+Verbs 89.9 90.9Seg Oracle Core+Verbs 83.3 83.6Pipeline Core+Verbs 77.1 77.3distinction for parsing is obvious, and was also noted in Klein and Manning (2003) forEnglish and in our previous work on parsing Hebrew (Goldberg and Tsarfaty 2008).Finite and non-finite verbs are easily distinguishable from each other based on surfaceform alone.
Although finiteness is clearly annotated in the treebank, it is not on the?core?
part of the POS tags and was removed prior to training the parser.
In a secondset of experiments the core tag set of the parser was modified to distinguish finite verbs,infinitives, and modals.13 The original core?tag set alady includes some importantdistinctions, such as construct from non-construct nouns.Results and discussion.
Table 1 presents the parsing results on the development set.
Withgold POS tags and segmentation, the results are very high.
Accuracy drops considerablywhen the parser is not given access to the gold tags (from about 90 to less than 84 F1),indicating that the POS tags are both informative and ambiguous.
Results drop evenfurther (from 84 to 77) in the pipeline case where the gold segmentation is not available,indicating that correct segmentation also provides valuable information to the parserand that segmentation mistakes are costly.Enriching the tag set to distinguish modals and finite and infinite verbs proveduseful, with an increase of about 1 F1 points (absolute) after four split-merge-smoothcycles, and a smaller increase after five cycles.
This stresses the importance of the corerepresentation: The automatic learning procedure goes a long way, but it can be aidedby linguistically motivated manual interventions in some cases.4.1 Analyzing the Learned PCFG-LA Grammar4.1.1 Terminal-Level (Lexical) Splits.
We begin by inspecting the splits at the part-of-speech level.
Table 2 displays the number of splits learned for each of the parts-of-speechsymbols.
Prepositions are the most heavily split, followed closely by the somewhat-generic MOD tag and the nouns.Nouns and adjectives.
The noun and adjective splits are somewhat hard to decipher.Some of the groups are obvious (things appearing after numbers, last names, parts-of-dates,time related, places, etc.).
Others are are much harder to interpret.13 Unlike previous work, the distinction is retained only at the POS tag level and not propagated to thephrase level.
The tag-level information is sufficient for the parser to learn the phrase-level distinctions onits own.
Similar observations regarding the usefulness and sufficiency of linguistically motivated manualstate-splitting of preterminals (as opposed to tree-internal nodes) prior to training a latent-variablegrammar were also made by Crabbe?
and Candito (2008).134Goldberg and Elhadad Parsing System for HebrewTable 2Number of learned splits per POS category after five split-merge cycles.Tag # Splits Tag # SplitsH 1 CDT 6HAM 1 CC 7POS 1 DT 7REL 1 JJ 7VB 1 VB-INF 7AT 2 PRP 8COM 2 CD 10JJT 2 RB 13QW 2 NN 16RBR 2 NNP 17VBMD 2 NNT 22WDT 2 MOD 24AGR 4 IN 26AUX 6MOD.
For the general-modification POS tags, most categories clearly single out oneor two words with very specific usage patterns, such as !??
(?no?
), !??
(?also?
), !??
(?only?),!??????
(?even?
), !?????
(?former?
), and so forth.
The other categories are harder to interpret.Verbs.
Finite-verbs are not split at all, even though they form an open-class category.Modal verbs are split into two groups: One of them is dominated by nine modals( !????
?, !?
?, !????
?, !??
?, !??
?, !????
?, !????
?, !???
?, !?????
?, roughly corresponding to the English could,should, seem/appear, hard, shouldn?t, possible, appear/seem, important, fitting/required); andthe second contains all the others.
This is an interesting distinction, as the nine singled-out modals never take a subject, whereas the modals in the other group do.14 Infinitiveverbs are split into seven categories, six of which are dominated by one or two wordseach, and the last is a catch-all category.Coordination and question-words.
Coordination words are heavily split, each of thecategories dominated by one or two words, indicating different usage patterns.
Thequestion words !???
(?what?)
and !???
(?who?)
are singled out from the rest.Gender/number agreement.
The verbs are not split at all, indicating that the learnedgrammar cannot model subject?verb agreement.
Pronouns are split by type (personals,demonstrative, and subtypes of demonstratives), but not by gender and number.
Nounand adjective splits are sometimes hard to decipher, but they do not exhibit any group-ing based on gender or number properties, indicating that the grammar cannot modeladjective?noun agreement.
Category splits for the AGR tag do show a clear divisionthat follows gender and number, but it is unclear what is captured by this division asthe information cannot interact with nouns, adjectives, verbs, or pronouns.14 In fact, the nine modals are very similar in characterization to the words identified in Netzer et al(2007)as modals, whereas many of the modals in the other group are not necessarily considered as modaloutside of the treebank guidelines.135Computational Linguistics Volume 39, Number 1Table 3Number of learned splits per NT-category after five split-merge cycles.Tag # Splits Tag # SplitsFRAGQ 1 ADVP 16INTJ 6 S 16FRAG 7 PP 22SQ 7 VP 22PRN 8 PREDP 25ADJP 14 NP 32SBAR 144.1.2 Grammar-Level Splits.
Table 3 shows the number of splits learned for each gram-mar non-terminal.
The NP category is the most heavily split, followed by predicativephrases, verb phrases, and PPs.
With the exception of the FRAGQ category, all symbolsare split into at least six substates.
What information is encapsulated in the state splits?As noted by Petrov et al(2006), the latent state-splits learned for the grammar symbolsare harder to analyze.One way of shedding some light on the meanings of the split-states is by using thegrammar in generationmode and by samplingword sequences from each of the states.15By looking at the resulting strings, one can sometimes infer the kind of informationencoded in the grammar.NP.
The split-NPs encode phrase length (some splits result in very long NPs, somein very short, some in very specific one- or two-word patterns).
They also encode thedefiniteness rules (either an NP is definite or not), the interaction between definitenessand the AT marker, and a limited interaction between definiteness and construct nouns.Other NP splits are dedicated to pronouns or to question words, or encode propernames, monetary units, and numbers.SBAR.
The split-SBARs are split according to the word introducing the SBAR.
Inaddition, some split-SBARs encode quoted and parenthetical items.S.
The split-Ss differ by length.
In addition, some S splits seem to be modeling verb-lesssentences, variations in word order, and sentence-level coordination.4.2 Limitation of PCFG-LA Parsing of Modern HebrewThe PCFG-LA baseline is a strong one, and is substantially higher than all previousreported results for Hebrew parsing in each of the setups (Seg+POS oracle, Seg Oracle,and no Oracle).
We also identify some of its limitations, namely:Missed splits.
The learning procedure is not perfect, and fails to capture some linguis-tically meaningful state-splits.
When such splits are manually supplied (i.e., the trivialsplit of verbal types) accuracy improves.15 Sampling a word sequence is performed by starting at a given state (a split grammar symbol), randomlychoosing a right-hand-side based on the PCFG-induced distribution, expanding the state into the chosenright-hand side, and continuing recursively until we are left with only strings.136Goldberg and Elhadad Parsing System for HebrewSensitivity to non-gold POS.
The substantial drop in accuracy when the POS tags areunobserved and need to be predicted is staggering, which suggests that it is difficultfor the parser to assign part-of-speech tags.
Of the 698 part-of-speech errors, 314 are onwords not seen in training.Sensitivity to non-gold segmentation.
The accuracy drops even further when the parseris presented with predicted segmentation.
Segmentation errors are detrimental to theparser.Not encoding grammatical agreement.
Finally, the learned grammar does not encodegrammatical agreement.
Whereas the majority of the parser mistakes are due to theflexible constituent order or ?standard?
ambiguities such as coordination and PPattachment, a handful of them could be resolved using agreement information.In what follows, we address these four limitations, and substantially increase theparser accuracy for the realistic case where gold segmentation and POS tags are notavailable.5.
Manual State-SplitsWe experimented with several linguistically motivated state-splits which were added astree-annotations prior to running the parser.
Most of them did not help on their own andslightly degraded parser performance when combined with other splits.
These includesplits which were proven useful in previous work, such as marking of definite NPs, anddistinguishing possessive from other PPs.
We also experimented with splits based onmorphological agreement features, which are discussed in Section 8.1.Overall, the learning procedure is capable of producing good splits on its own.
Wedid, however, manage to improve upon it with the following annotation (the annota-tions were removed prior to evaluation).Subject NPs.
Hebrew phrase order is rather flexible, and the subject can appear beforeor after the verb.
Identifying the subject can thus help in grounding the overall structureof the sentence.
The subject is also dependent on agreement constraints with the verb.Following Johnson (1998), Klein and Manning (2003) implicitly annotate subject-NPsin English using parent annotation (distinguishing NPs under S from other NPs), withgood results.
When applied to English, the PCFG-LA also learns to model subject NPswell.
Hebrew?s non-configurationality, however, put both Subjects and Objects directlyunder S, making it much harder to learn the distinction automatically.Explicit marking of subject NPs contributes slightly to the accuracy of the parser.Perhapsmore important than the small increase in accuracy is the fact that the parser canidentify subjects relatively well.
In contrast, marking of object NPs did not help by itselfand slightly degraded the parsing accuracy when combined with other annotations.Note, however, that Hebrew definite objects are already clearly marked using the !?
?marker, making them an easy target for the parser.6.
Better Lexical Coverage with an External LexiconThe drop in parsing accuracy when gold core POS tags are not available and need to beinferred by the parser is huge (from above 90 to less than 84 F1).137Computational Linguistics Volume 39, Number 1The large number of possible word forms make it very difficult for manually annotatedcorpora to provide adequate lexical coverage.
The problem is even more severe withthe case of the Hebrew Treebank, which is especially small.
Although it is big enough tolearn meaningful syntactic generalizations (as demonstrated by the high performanceof the baseline system) it is far too small to learn a good lexical model (as evidenced bythe drop in accuracy when gold tags are not available).We suggest increasing the lexical coverage of the parser using an external resource,namely, a lexicon-based morphological analyzer.
We further extend the utility of theanalyzer with lexical tagging probabilities learned from an unannotated corpus.6.1 A Unified Lexical Probability ModelWe would like to use the KC Analyzer (Section 2.3.3) to increase the lexical coverageof the treebank-trained parser.
That is, we would like to improve the lexical modelP(T ?
W) of the generative parser.
As discussed in Section 2.3.5, however, the tag setsused by the two resources differ.
How can this difference be reconciled?One possibility is to re-tag the treebank with the KC tag set and then train on thisunified resource.
In Goldberg et al(2009), we show that this procedure degrades parserperformance.
Instead, Goldberg et alsuggest a layered generative approach that retainsthe benefits of the treebank tagging for frequent words and resorts to the KC tag set onlyfor rare and unseen words.
Under this approach, frequent words are generated fromtreebank POS tags as usual, but rare words follow a generative process in which firstthe treebank tag generates a KC tag, and then the KC-tag generates the word.
A samplederivation using this layered representation is presented in Figure 1.The Treebank-to-KC tag generation probabilities represent a fuzzy, probabilisticmapping between the two resources.
In Goldberg et al(2009), the estimation of theseprobabilities was done based on a re-tagging of the treebank to use the KC tag set.The re-tagging process was far from trivial, and many tagging cases required extensivedebates between human annotators.Here, we present a new procedure which does not require the treebank to be re-tagged with a new tag set.
It still uses the layered representation, but instead of forcingone unique KC analysis for each location, it embraces the uncertainty and allows all ofthem.
This is done by treating the KC-tag assignments as hidden variables, learningthe TB-KC mapping probabilities as part of the grammar training EM process, andmarginalizing the KC tags out for the final tree.
The procedure is based on the followingassumptions:r We have access to trees in which the POS tags ttb are taken from a given tagset TTB....JJTBPRP-M-S-3-DEMExt!?
?Figure 1A layered POS tag representation.138Goldberg and Elhadad Parsing System for Hebrew...NNTBNN-M-SExt NNT-M-SExt VB-M-S-3-PastExt VB-M-S-3-ImpExt!??
?Figure 2A latent layered POS tag representation.r We have additional access to an external resource (lexicon) mappingwords to tags text from a different tag set TExt.r Probabilities involving words which are frequent in the treebank can andshould be based on treebank counts.r Probabilities involving less frequent words should be smoothed in withinformation from the external lexicon.r Smoothing should have a greater effect on less-frequent words.r Probabilities for unseen words should be based solely on the externallexicon.Figure 2 illustrates the representation used for words which are rare or unseen in thetreebank training data.
The treebank tag NNTB (upper level) generates the word-form!???
(lower level) by considering all the possible KC POS tags allowed for the word inthe morphological analyzer (the middle level).
The probabilities related to generatingthe KC POS tags are summed, and all the other probabilities are multiplied.
The exactequations are detailed in the following.Although the needed quantity is the emission probability P(TTB ?
W) = P(W|TTB),it is more convenient (for a reason which will be discussed later) to work with thetagging probability P(TTB|W).
Once the tagging probabilites P(TTB|W) are available,they can easily be converted to emission probabilities using Bayesian inversion, basedon the relative-frequency estimates of P(W) and P(TTB) which are calculated from thetreebank:16P(ttb|w)P(w)P(ttb)= P(w|ttb) = P(ttb ?
w) (1)16 Our notation uses capital letters to denote random variables, and lower-case letters to denote specificevents.
Thus, P(T|W) refers the distributions in which a tag ?
T is condition on a word ?
W, P(T|w) refersto the conditional distribution of tags t ?
T given a specific word w, and P(t|w) refers to the probabilitymass of the specific tag t given word w.139Computational Linguistics Volume 39, Number 1Let us now focus on estimating the tagging probabilities P(TTB|W) for the cases offrequent, rare, and OOV words.For frequent words that are seen more than K times in the treebank, we simply usetreebank-based relative-frequency estimates:17Ptb(ttb|w) =c(w, ttb)c(w) (2)where c(?)
is a counting function.For OOV words that are not seen in the treebank, the tagging probability is estimatedusing:Poov(ttb|w) =?text?TExtP(text|w)P(ttb|text) (3)where P(TExt|W) is a tagging probability using the external tag set, and P(TTB|TExt) isa transfer probability relating the tags from the two tag sets (the estimation of thesetwo probabilities is discussed subsequently).
What this does is assume a process inwhich the word is tagged by first choosing a tag according to the external lexicon, andthen choosing a tag from the TB tag set based on the external one.
The external tagassignments are then treated as latent variables, and are marginalized out.Finally, for rare words that are seen only a few times in the treebank, we interpolate thetwo quantities, weighted by the word?s frequency in the treebank:Prare(ttb|w) =c(w)Ptb(ttb|w)+ Poov(ttb|w)1+ c(w) (4)We now turn to describing the estimation of the external tagging probability P(TExt|W)and the tag transfer probability P(TTB|TExt).Estimating P(TExt|W).
The tagging probability follows the morphological analyzer.The analyzer provides the possible analyses, but does not provide probabilities forthem.
One simple option would be to assign each possible analysis (tag) a uniformprobability, and assign 0 probability for tags not allowed by the lexicon for the givenword.
This method is referred to as Punif(TExt|W).
We know that not all the possibleanalyses for a given word are equally likely, however, and in practice, the actualtagging distribution is usually biased toward one or two of the tags.
These taggingpreferences can be learned in an unsupervised manner given the lexicon and a largecorpus of unannotated text, using EM training of an HMM tagging model.
Adler andElhadad (2006) suggest such a model for accurate tagging of Hebrew, and Adler (2007)and Goldberg, Adler, and Elhadad (2008) extend it to provide state-of-the-art taggingaccuracies for Hebrew using a smart initialization.
Here, we use the pseudo-counts from17 In practice, a small amount of smoothing is added to allow tagging a word with open-class tags if itwasn?t seen within the treebank: Ptb(ttb|w) = (c(w, ttb )+ 0.0001 ?
P(ttb ))/(c(w)+ 0.0001).140Goldberg and Elhadad Parsing System for Hebrewthe final round of EM training in this tagging model in order to compute Pem(TExt|W).We show in Section 9 that this unsupervised lexical probabilities estimation doesindeed provide better parsing results.Estimating P(TTB|TExt).
The tagset-transfer probabilities capture the patterns of transferbetween the syntactic tagging scheme of the treebank and the other tagging schemeof the external resource.
They are estimated using treebank counts and the taggingdistribution P(TExt|W):P(ttb|text) =c(ttb, text)c(text)=?w c(ttb,w)P(text|w)?w c(w)P(text|w)(5)Integration into the PCFG-LA model.
The estimation procedure is incorporated into thetraining process of the PCFG-LA model.
Note that in the PCFG-LA model the treebanktag set TTB is gradually split, and each tag takes the form ?tag, substate?, where substateis a latent variable indicating a specific split of the given tag.
This means that thetreebank tagging probability and the tag set?transfer probabilities are also defined overthese split tags.
Whereas the external tagging probabilities P(TExt|W) are fixed prior toPCFG-LA training, the other distributions (P(TTBsubstate |W) and P(TTBsubstate |TExt)) are re-estimated in the EM process following each of the split, merge, and smooth stages.
Thisis done by replacing the corpus counts c(?)
in Equations (2) and (5) with pseudo-counts(expectations, marginal scores) of the same events in the E step of the EM procedure.The main reason for using the Bayesian inversion (Equation (1)) instead of workingwith the emission probability P(W|T) directly is that the emission probability ishighly dependent on the vocabulary size.
The treebank estimates are based on a smallvocabulary, the external lexicon estimates are based on a very large vocabulary, anda proper combination of the two emission probabilities is not trivial.
In contrast, thetagging probabilities do not depend on the vocabulary size, allowing a very simplecombination.
We can then base the counts for the emission probability on the treebankvocabulary alone, and estimate P(W) for words unseen in training as if they were seenonce.7.
Joint Segmentation and ParsingWhen applied to real text (for which the gold word-segmentation is not available), thebaseline PCFG-LA parser is supplied with word segmentation produced by a separatetagging process.18 This seriously degrades parsing performance.
A major reason for theperformance drop is that the word-segmentation task and the syntactic-disambiguationtask are highly related.
Segmentation mistakes drive the parser toward wrong syntacticstructures, and many segmentation decisions require long-distance information that isnot available to a sequential process (Tsarfaty 2006a).
For these reasons, we claim thatparsing and segmentation should be performed jointly.18 Although the tagger also produces POS tag assignments, we ignore them and use only the wordsegmentation.
This is done for two reasons: first, the tag set of the tagger is the one used by themorphological analyzer, and is not compatible with the treebank.
Second, we believe it is better forthe parser to produce its own tag assignments.141Computational Linguistics Volume 39, Number 1Figure 3The lattice for the Hebrew sequence !??????
?????
(see footnote 19).Joint segmentation and parsing can be achieved using lattice parsing.
Instead ofparsing over a fixed input string, the parser operates on a lattice?a structure encodingall the possible segmentations.7.1 Lattice RepresentationFormally, a lattice is a directed acyclic graph in which all paths lead from the initial stateto the end state.For the Hebrew segmentation task, all word segmentations of a given sentence arerepresented using a lattice structure.
Each lattice arc corresponds to a word and itscorresponding POS tag, and a path through the lattice corresponds to a specific word-segmentation and POS tagging of the sentence.
This is by now a fairly standard repre-sentation for multiple morphological segmentations of Hebrew utterances (Adler 2001;Bar-Haim, Sima?an, and Winter 2005; Adler 2007; Cohen and Smith 2007; Goldberg,Adler, and Elhadad 2008; Goldberg and Tsarfaty 2008; Goldberg and Elhadad 2011).
Itis also used for Arabic (Green and Manning 2010) and other languages (Smith, Smith,and Tromble 2005).Figure 3 depicts the lattice for the two-words sentence !??????
19.?????
Double-circlesindicate the space-delimited token boundaries.
Note that in this construction arcs cannever cross token boundaries.
Every token is independent of the others, and the sen-tence lattice is in fact a concatenation of smaller lattices, one for each token.
Further-more, some of the arcs represent lexemes not present in the input tokens (e.g., !?/DT,!?
?/POS), although these are parts of valid analyses of the token.
Segments with the samesurface form but different POS tags are treated as different lexemes, and are representedas separate arcs (e.g., the two arcs labeled !?????
from node 6 to 7).A similar structure is used in speech recognition.
There, a lattice is used to representthe possible sentences resulting from an interpretation of an acoustic model.
In speechrecognition the arcs of the lattice are typically weighted in order to indicate the probabil-ity of specific transitions.
Given that weights on all outgoing arcs sum up to one, weightsinduce a probability distribution on the lattice paths.
In sequential tagging models suchas Smith, Smith, and Tromble (2005), Adler and Elhadad (2006), and Bar-Haim, Sima?an,and Winter (2008) weights are assigned according to a tagging model based on linearcontext.
For the case of parsing, context-free weighting of lattice arcs is used: each arc19 Whereas Hebrew is written right-to-left, the lattice is to be read left-to-right.
The words on each arcfollow the Hebrew writing directions, and are written right-to-left.142Goldberg and Elhadad Parsing System for Hebrewcorresponds to a ?tag,word?
pair, and is weighted according to the emission distributionP(tag ?
word).207.2 Lattice ParsingThe CKY parsing algorithm can be extended to accept a lattice, instead of a predefinedlist of tokens, as its input (Chappelier et al1999).
The CKY search then finds a treespanning from the start-state to the end-state of the lattice, where the leaves of the treeare lattice arcs.
The lattice extension of the CKY algorithm is performed by indexinglexical items according to their start- and end-states in the lattice instead of by theirsentence position, and changing the initialization procedure of CKY to allow terminaland preterminal symbols of spans of sizes > 1.
It is then relatively straightforward tomodify the parsing mechanism to support this change: not giving special treatmentsfor spans of size 1, and distinguishing lexical items from non-terminals by a specifiedmarking instead of by their position in the chart.Figure 4 shows the CKY chart for the lattice in Figure 3, together with an (incorrect)parse over the lattice.
The chart is initialized with parts of speech corresponding tothe lattice arcs.
Phrase-structures are then built on top of the POS tags (in blue).
Theproposed structure must span the entire chart, and correspond to a path through thelattice from the initial state (0) to the last one (7).At training time the correct segmentation is fully observed, and the generativeparser is trained as usual over the treebank.
At inference (test) time, the correct seg-mentation is unknown, and the decoding is applied to the segmentation lattice.
The bestderivation returned by the parser forces a specific segmentation.
The returned parse treeis the most probable ?segmentation, tree?
pair according to the grammar.21 We modifiedthe PCFG-LA BerkeleyParser to accept lattice input at inference time.Lattice parsing allows us to preserve the segmentation ambiguity and present itto the parser, instead of committing to a specific segmentation prior to parsing.
Thisway segmentation decisions are performed in the parser as part of the global searchfor the most probable structure, and can be affected by global syntactic considera-tions.
We show in Section 9 that this methodology is indeed superior to the pipelineapproach.Early descriptions of algorithms for parsing over word lattices can be found inLang (1974, 1988) and Billott and Lang (1989).
Lattice parsing was explored in thecontext of parsing of speech signals by Chappelier et al(1999), Sima?an (1999), andHall (2005), and in the context of joint word-segmentation and syntactic disambiguationin Cohen and Smith (2007), Goldberg and Tsarfaty (2008), and Green and Manning(2010).20 Lattice parsing for Hebrew is explored also in Cohen and Smith (2007).
There, lattice arc weightsare assigned based on aggregate quantities (forward-backward tagging marginals) derived from adiscriminative CRF tagging model.
This approach is not ideal from a modeling perspective, as it makeseach POS tag be accounted for twice: once by the syntactic model, and once by the sequential one.In this work, a sequential tagging model is not used at all.
If the use of a sequential model is desired,an alternative method for integrating a sequence model and a syntactic model is making the models?negotiate?
an agreed upon structure that maximizes the score under both models, using optimizationtechniques such as dual decomposition (Dantzig and Wolfe 1960), which was recently introduced intonatural language processing (Rush et al2010).21 Note that finding the most probable segmentation requires summing over all the trees resulting in eachsegmentation?a much harder task, proven to be NP-complete in Sima?an (1996).143Computational Linguistics Volume 39, Number 1Figure 4Lattice initialization of the CKY chart.8.
Incorporating Morphological AgreementInspecting the learned grammars reveal that they do not encode any knowledge ofmorphological agreement: The split categories for nouns, verbs, and adjectives do notgroup words according to any relevant morphological property such as gender ornumber, making it impossible for the grammar to model agreement patterns.
At thesame time, inspecting some of the bad parses reveals several clear cases of agreementmistakes.
Can morphological agreement be incorporated in the parsing model?8.1 Forcing Morphologically Motivated SplitsOur initial attempts focused on making the PCFG-LA learning procedure pick up onagreement-relevant state-splits.
When neither the core tag set nor the non-terminalsencode gender and number information, it is very hard for the parser to pick up onagreement patterns.22We attempted to train the parser on trees which mark the agreement features (eitherthe gender, the number, or both) either on the POS tags, the relevant constituents, or22 In the external lexicon case, the external lexicon tags do encode the morphological features, makingit possible in principle for the parser to learn to map certain substates to certain agreement features.This did not happen in practice, arguably because other structural factors were more powerful thanthe agreement ones.144Goldberg and Elhadad Parsing System for Hebrewboth.
Annotating agreement features on the POS tag?level made the parsing muchslower, but did make the parser assign certain split categories to certain gender?numbercombinations, and sampling utterances from the learned grammar did indicate a notionof grammatical agreement.
This did not improve parsing accuracy, however?and evenslightly degraded it.When propagating the agreement features and annotating them on the constituentlevel, parsing accuracy dropped considerably.
When inspecting the learned grammarwe observe that most of the agreement-annotated constituents (e.g., NPMasc,Plural) werestill fully split, indicating that the parser picked on patterns which were orthogonal tothe agreement mechanism.
The pre-splitting according to agreement-features propertiescaused data sparseness, aided over-fitting, and hurt parsing performance: The smooth-ing procedure of the BerkeleyParser shares some probability-mass between varioussplits of the same symbol, but was not applied in our case (no information flowedbetween, for example, NPMasc,Plural and NPMasc,Singular).
We attempted to counter thiseffect by changing the smoothingmechanism of the BerkeleyParser to share informationalso between the manually split symbols.
This brought parsing accuracy back to theinitial level, but also caused the parser to, again, not model agreement very well.
Thereason for this is clear in hindsight: Morphological agreement is an absolute concept,not a fuzzy one (things can either agree or not).
Smoothing the probabilities betweenthe different morphology-based split-licensed grammar rules that allow morphologicaldisagreement, and made the grammar lose its discrimination power.
This was thenreinforced by the training process, which picked on other syntactic factors instead, andfurther phased out the agreement knowledge.A note on product-grammars.
In recent work, Petrov (2010) showed that a committee oflatent-variable grammars encoding different grammatical preferences can be combinedinto a product-grammar that is better than the individual ensemble members.
Petrovcreated the ensemble by training several PCFG-LA parsers on the same data, but usingdifferent random seeds when initializing the EM starting point.
We attempted to cre-ate a similar ensemble by providing the learning process with different linguisticallymotivated tree annotations (with and without encoding agreement features, with andwithout encoding definiteness, etc.).
The combined parser did increase the performancelevel over that of the individual parsers, but an ensemble with the same number ofcomponents that was produced using the random-seeds approach produced far su-perior results.
This reinforces the findings of Petrov (2010) who also reports that theensemble creation using random initialization is exceptionally strong and outperformsother methods of ensemble creation.238.2 Agreement as FilterWe now turn to suggest an approach to modeling agreement, which rests on the follow-ing principles:r Agreement can be modeled as a set of hard (not probabilistic) constraints.r Agreement is completely orthogonal to the other aspects of the grammar.23 The product grammar approach with random seeds works well and is effective for improving theaccuracy of Hebrew parsing.
As it is completely orthogonal to the approaches presented in this article,however, we chose not to discuss it further other than commenting on its applicability.145Computational Linguistics Volume 39, Number 1Based on these principles, we suggest treating agreement as a filter, a device that canrule out illegal parses.
Under the agreement-as-filter framework, we want the parser toproduce the most probable parse according to its grammar and subject to hard agreementconstraints.
This approach completely decouples the grammar from the agreement ver-ification mechanism.
The agreement information is not modeled in the grammar andis not used to guide the search for the best parse.
Instead, it is a separate process thatimposes hard constraints on the search space and rules out parts of it completely.
Thatis, agreement is a part of the parser and not of the grammar.
This is similar in spirit toideas from constraint-based grammars such as LFG (Falk 2001) and HPSG (Pollard andSag 1994), which also model aspects of the syntax as Boolean constraints.Grammatical agreement is a relation between constituents.
The relevant morpho-logical features are propagated from one of the leaves up to the constituent level.When constituents are combined to form a larger constituent, their morphologicalfeatures are assigned to the newly created constituent according to language-specificrules (it is possible that different morphological features will be assigned by differentconstituents).
An agreement violation occurs when two or more constituents assignconflicting features to their parent.Implementation.
In the implementation, an agreement-verification mechanism is man-ually constructed (not learned) based on a set of simple, language-dependent rules.First, we provide a set of rules to propagate the morphological agreement features fromthe leaves to the constituents.
Then, we specify an additional set of rules to inspectlocal tree configuration and identify agreement violations (the Hebrew set of rules isdescribed later, along with a concrete example).
The feature-propagation mechanismworks bottom?up and the agreement verification rules are very local, making it possibleto integrate the filtering mechanism into a bottom?up CKY parsing algorithm (refusingto complete a constituent if it violates an agreement constraint).
We did not pursue thisroute for the experiments in this work, however.
Instead, we opted for an approximationin which we take the 100-best trees for each sentence, and choose the first tree thatdoes not have an agreement violation (this is an approximation because the 100-besttrees may not contain a valid tree, in which case we accept the agreement violation andchoose the first-best tree).
The specific details of the Hebrew agreement filter are givenin the appendix.Verifying the hard-constraint property.
We verified that the hard constraint assumptionworks and that the agreement verification mechanism is valid by applying the proce-dure to the gold-standard trees in the training-set and checking that (1) the propagatedfeatures agree with the manually marked ones, and (2) none of the training-set treeswere filtered due to agreement violation.
We did find a few cases in which the prop-agated features disagreed with the manually marked ones, and a few gold-standardtrees that the mechanism marked as containing an agreement violation.
All of thesecases were due to mistakes in the manual annotation.Connections to parse-reranking.
Our implementation is similar to parse-reranking(Charniak and Johnson 2005; Collins and Koo 2005).
Indeed, if we were to modelagreement as soft constraints, we could have incorporated this information as featuresin a reranking model.
The filter approach differs in that it poses hard constraints andnot soft ones, pruning away parts of the search space entirely.
Thus, the use of k-bestlist is merely a technical detail in our implementation?the agreement information is146Goldberg and Elhadad Parsing System for Hebreweasily decomposable and the hard constraints can be efficiently incorporated into theCKY search procedure.9.
Evaluation and ResultsData set.
For all the experiments we use Version 2 of the Hebrew Treebank (Guthmannet al2009), with the established test-train-dev splits: Sentences 484?5,740 are used fortraining, sentences 1?483 are the development set, and sentences 5,741?6,220 are usedfor the final test set.Evaluation Measure.
In the cases where the gold segmentation is given, we use the well-known evalb F1 score.
Namely, each tree is treated as a set of labeled constituents.24Each constituent is represented as a 3-tuple ?i, j,L?, in which i and j are the indices ofthe first and the last words in the constituent, respectively, and L is the constituencylabel.
For example, (2, 4,NP) indicates an NP spanning from word 2 to word 4.
Theperformance of a parser is evaluated based on the amount of constituents it recoveredcorrectly.
Let G denote the set of constituents in a gold-standard constituency tree, andP denote the set of constituents in a predicted tree.
Precision (P), recall (R), and F1 aredefined as:precision = |G ?
P||P| recall =|G ?
P||G|F1 = 21precision +1recallF1 ranges from 0 to 1, and it is 1 iff both precision and recall are 1, indicating the treesare identical.
We report numbers in precentages rather than fractions.When measuring the performance of models in which the token-segmentation ispredicted and can contradict the gold-standard, a generalization of these measures isused.
Instead of representing a constituent by a triplet ?i, j,L?, each constituent is repre-sented by a pair containing the concatenation of the words at its yield, and its label L.This measure was suggested by Tsarfaty (2006a) and used in subsequent work (Tsarfaty2006b; Goldberg and Tsarfaty 2008; Goldberg et al2009; Goldberg and Elhadad 2011).This is equivalent to reassigning the i and j indices to represent character positionsinstead of word numbers.
When the yields of the gold standard and the predicted treesare the same, this is equivalent to the standard evaluation measure using the ?i, j,L?triplets of word indices and a label, and it will produce the same precision, recall, andF1 as above.Effect of external lexicon.
We start by evaluating the effect of extending the parser?s lexicalmodel with an external lexicon, as described in Section 6.1.
The rare-word thresholdis set to 100.
We use the morphological analyzer described in Section 2.3.3.
We testtwo conditions: UNIFORM, in which the P(Text|w) distribution is uniform over all the24 This assumes unary-chains do not contain cycles.147Computational Linguistics Volume 39, Number 1Table 4Dev-set results when incorporating an external lexicon.Setting Ext-Lexicon/Probs F1 (4 cycles) F1 (5 cycles)Seg Oracle NONE 83.13 83.39Pipeline NONE 75.98 76.65Seg Oracle UNIFORM 84.92 84.56Pipeline UNIFORM 77.53 77.35Seg Oracle HMM-BASED 86.17 85.79Pipeline HMM-BASED 78.75 78.78analyses suggested by the morphological analyzer for the word, and HMM-BASED inwhich the P(Text|w) distribution is based on pseudo-counts from the final round of EM?HMM training of the semi-supervised POS tagger described in Section 2.3.4.
Results arepresented in Table 4.Incorporating the external lexicon helps both in the case where the correct segmen-tation is assumed to be known, as well as in the pipeline case where the segmentation isautomatically induced by a sequential tagger.
Incorporating the semi-supervised lexicalprobabilities learned over large unannotated corpora (HMM-BASED) further improvesthe results, up to 86.1 F1 for the gold-segmentation case and 78.7 F1 for the pipelinecase.
The pipeline model still lags behind the gold-segmentation case, indicating thatthe correct segmentation is very informative for the parser.Joint segmentation and parsing.
Having established that the external lexicon can be effec-tively incorporated into the parser, we turn to evaluate the method for joint segmenta-tion and parsing.
We follow the same conditions as before (UNIFORM and HMM-BASEDlexical probabilities), but in this set of experiments the parser is allowed to choose itspreferred segmentation using the lattice-parsing methodology presented in Section 7.2.The lattice is constructed according to the analyses licensed by the morphologicalanalyzer.
Table 5 lists the results.
Lattice parsing is effective, leading to an improvementof about 2?3 F1 points over the pipeline model.Agreement filter.
We now turn to add the agreement filtering on top of the lexicon-enhanced models.
In this setting, the model outputs its 100-best trees for each sentence,agreement features are propagated, and agreement violations are checked as describedTable 5Dev-set results when using lattice parsing on top of an external lexicon/analyzer.Setting Ext-Lexicon/Probs F1 (4 cycles) F1 (5 cycles)Pipeline UNIFORM 77.53 77.35Lattice (Joint) UNIFORM 80.35 80.31Pipeline HMM-BASED 78.75 78.78Lattice (Joint) HMM-BASED 80.91 80.46148Goldberg and Elhadad Parsing System for HebrewTable 6Dev-set results of using the agreement-filter on top of the lexicon-enhanced parser (starting fromgold segmentation).Setting Ext-Lexicon/Probs F1 (4 cycles) F1 (5 cycles)No Agreement UNIFORM 84.92 84.56Agreement as Filter UNIFORM 85.30 84.52No Agreement HMM-BASED 86.17 85.79Agreement as Filter HMM-BASED 86.55 86.25in Section 12, and the first tree that does not contain any agreement violation is returnedas the final parse for the sentence (or the first-best tree in case that all of the outputtrees contain an agreement violation).
Table 6 lists the results when agreement filteringis performed on top of parses based on gold segmentation, and Table 7 lists the resultswhen agreement filtering is performed on top of a lattice-based parsing model that doesnot assume gold segmentation is available.Discussion of agreement filter results.
Although the agreement filter does not hurtthe parser performance, the benefits from it are very small.
To understand why thatis the case, we analyzed the 1-best parses produced by the 5-cycles-trained grammar onthe gold-segmented development set (these conditions corresponds to the last columnof the third row in Table 6).
The analysis revealed the following reasons for the lowimpact of the agreement filter: (1) The grammar is strong enough to produce fairlyaccurate structures, which have very few agreement mistakes to begin with, and (2)fixing an agreement mistake does not necessarily mean fixing the entire parse?in somecases it is very easy for the parser to fix the agreement mistake and still produce anincorrect parse for other parts of the structure.The 1-best trees of the 480 sentences of the development set contain 22,500 parse-tree nodes.
Of these 22,500 nodes, 2,368 nodes triggered a gender-agreement check:about 10% of the parsing decisions could benefit from gender agreement.
Of the 2,368relevant nodes, however, 130 nodes involved conjunctions or possessives, and wereoutside of the scope of our agreement verification rules.
Of the remaining 2,238 parse-tree nodes, 2,204 passed the agreement check, and only 34 nodes (1.5% of the relevantnodes, and 0.15% of the total number of nodes) were flagged as gender-agreementviolations.
Similarly for number agreement, 2,244 nodes triggered an agreement check,of which 2,131 nodes could be handled by our system.
Of these relevant nodes, 2,109nodes passed the gender-agreement check, and only 23 nodes (1.07% of relevant nodes,Table 7Dev-set results of using the agreement-filter on top of the lexicon-enhanced lattice parser (parserdoes both segmentation and parsing).Setting Ext-Lexicon/Probs F1 (4 cycles) F1 (5 cycles)No Agreement UNIFORM 80.35 80.31Agreement as Filter UNIFORM 80.55 80.74No Agreement HMM-BASED 80.91 80.46Agreement as Filter HMM-BASED 81.04 80.72149Computational Linguistics Volume 39, Number 1Table 8Numbers of parse-tree nodes in the 1-best parses of the development set that triggered gender ornumber agreement checks, and the results of these checks.Gender Agreement Number AgreementTriggered agreement check 2,368 2,244Could be handled by the system 2,238 2,131No agreement violation 2,204 2,109Agreement violation 34 23and 0.1% of the total nodes) were flagged as agreement violations.
The numbers aresummarized in Table 8.
It is clear that the vast majority of the parser decisions arecompatible with the agreement constraints.Turning to inspect the cases in which the agreement filter caught an agreementviolation, we note that the agreement filter marked 51 of the 480 development sentencesas having an agreement violation in the 1-best parse?about 10% of the sentences couldpotentially benefit from the agreement filter.
For 38 of the 51 agreement violations, theagreement violation was fixed in the tree suggested in the 100-best list.
We manuallyinspected these 51 parse trees, and highlight some the trends we observed.
In the13 cases in which the 100-best list did not contain a fix to the agreement violation,the cause was usually that the 1-best parse had many mistakes that were not relatedto the agreement violation, and diversity in the 100-best list reflected fixes to thesemistakes without affecting the agreement violation.
Another cause of error was an erro-neous agreement mistake due to an omission in the lexicon.
Of the 38 fixable agreementviolations, 25 were local to a noun-phrase, 10 were cases of subject?verb agreement,and the remaining three were either corner-cases or harder to categorize.
The subject?verb agreement violations were handled almost exclusively by keeping the structuremostly intact and changing the NPSUBJ label to some other closely related label that doesnot require verb agreement, usually NP.
This is a good strategy for fixing subject-lesssentences (about half of the cases), but it is only a partial fix in case the subject shouldbe assigned to a different NP (which does not happen in practice) or in case a moredrastic structural change to the parse-structure is needed.
In one of the 10 cases, thesubject?verb agreement mistake indeed resulted in a structural change that improvedthe overall parse quality.
The NP internal agreement violations include many cases ofnoun-compound attachments, and some cases involving coordination.
The correctionsto the agreement violation were mostly local, and usually resulted in correct structure,but sometimes introduced new errors.
Figure 5 presents some examples of the differentcases.
Our overall impression is that for NP internal mistakes the agreement-filteringmethod was mostly doing the right thing.To conclude, the agreement filter is useful in overcoming some errors and providingbetter parses, especially with respect to noun-compound construct-state constructions.Due to the limited number of parsing mistakes involving agreement violations, how-ever, and because of the local nature of the agreement-violation mistakes, the total effectof the agreement filter on the final parsing score is small.10.
The Final ModelFinally, we evaluate the best performing model on the test set.
Table 9 presents theresults of parsing the test set while incorporating the external lexicon and using the150Goldberg and Elhadad Parsing System for HebrewFigure 5NP agreement violations that were caught by the agreement filter system.
(a) Noun-compoundcase that was correctly handled.
(b) Case involving conjunction that was correctly handled.
(c) A case where fixing the agreement violation introduces a PP-attachment mistake.Table 9Test-set results of the best-performing models.Setting Model F1 (4 cycles)Gold Segmentation HMM-Based External Lexicon 85.67+ Agreement 85.70Lattice-parsing HMM-Based External Lexicon 76.87+ Agreement 76.95151Computational Linguistics Volume 39, Number 1HMM-based probabilities, for a grammar trained for four split-merge iterations.
Thisgrammar is applied both to the gold-segmentation case and to the realistic case wheresegmentation and parsing are performed jointly using lattice-parsing.
We also test theeffectiveness of the agreement-filter in both situations.Agreement information does not hurt performance, but contributes very little to thefinal accuracy?additionally on the test sentences, the parser makes very few agreementmistakes to begin with.Consistent with previous reports (Tsarfaty 2010), the test set is somewhat harderthan the development set.
With gold-segmentation, the models achieve accuracies of85.70% F1.
In the realistic scenario in which the segmentation is induced by the parser,the accuracies are around 76.9% F1.
We verified that the HMM-based lexical probabili-ties also outperform the Uniform probabilities on the test set (the F1 scores when usinguniform lexical probabilities are 84.06 and 76.30 for the gold and induced segmenta-tions, respectively).
These are the best reported results for parsing the test-set of theHebrew Treebank.11.
Related Work in Parsing of Morphologically Rich LanguagesCoping with unknown words.
Several papers show that the handling of unknown wordsis a major component to be considered when adapting a parser to a new language.For example, the work in Attia et al(2010) uses language-specific unknown-wordsignatures for several languages based on various indicative prefixes and suffixes, andHuang and Harper (2009) suggest a Chinese-specific model based on the geometricaverage of the emission probabilities of the individual characters in the rare or unknownword.
Another method of coping with lexical sparsity is word clustering.
In Canditoand Crabbe?
(2009), the authors demonstrate that replacing words by a combination ofa morphological signature and a word-cluster (based on the linear context of a word ina large unannotated corpus) improves parsing performance for French.
The techniqueprovides more reliable estimates for in-vocabulary words (a given cluster appears morefrequently than the actual word form), and it also increases the known vocabulary:Unknown words may share a cluster with known words.Arabic.
Arabic is similar to Hebrew in the challenges it presents for automatic pars-ing.
Most early work on constituency parsing of Arabic focused on straightforwardadaptations of Bikel?s parser to Arabic, with little empirical success.
Attia et al(2010)show that parsing accuracies of around 81% F1 can be achieved for Arabic (assuminggold word segmentation) by using a PCFG-LA parser with Arabic-specific unknown-word signatures.
Recently, Green and Manning (2010) report on an extensive set ofexperiments with several kinds of tree annotations and refinements, and report pars-ing accuracies of 79% F1 using the Stanford-parser and 82% F1 using the PCFG-LABerkeleyParser, both when assuming gold word segmentation.
The work of Green andManning also explored the use of lattice-parsing as suggested in Section 7 of this article,as well as earlier in Goldberg and Tsarfaty (2008) and Cohen and Smith (2007), andreport promising results for joint segmentation and parsing of Arabic (an F1 score of76% for sentences of up to 70 words).
The best reported results for parsing Arabicwhen the gold word segmentation is not known, however, are obtained using a pipelinemodel in which a tagger and word-segmenter is applied prior to a manually state-splitconstituency parser, resulting in an F-score of 79% F1 (for sentences of up to 70 words)(Green and Manning 2010).152Goldberg and Elhadad Parsing System for HebrewHebrew and relational-realizational parsing.
Some related work deals directly with con-stituency parsing of Modern Hebrew.
The work of Tsarfaty and Sima?an (2007) experi-ments with grammar refinement for Hebrew, and shows that annotating definitenessand accusativity of constituents, together with parent annotation, improves parsingaccuracy when gold word segmentation is available.The Relational Realizational (RR) line of work presented in Tsarfaty et al(Tsarfatyand Sima?an 2008; Tsarfaty, Sima?an, and Scha 2009; Tsarfaty and Sima?an 2010; Tsarfaty2010) handles the constituent-order variation in Hebrew by presenting a separationbetween the form and function aspects of the grammar.
Briefly, whereas plain treebank-derived grammars have rules such as S ?
NP VP PP NP PP that are applied in asingle step, the RR approach suggests a generative model in which the generation offlat clausal structures is decomposed into three distinct steps.
First, in the projection step,a non-terminal generates the kinds of its children without specifying their form or theorder between them, using rules of the form S ?
{OBJ,SBJ,PRED,COM,Adjunct}@S.Second, in the configuration step, an order is chosen based on a separate orderingdistribution, using rules of the form{OBJ,SBJ,PRED,COM,Adjunct}@S ?
SBJ@S PRED@S Adj@S OBJ@S COM@S.Third, in the realization step, each functional element receives a specific form, using rulesof the form SBJ@S ?
NP or Adj@S ?
PP.
The realization rules can encode syntacticproperties that are required by the grammar for the given function?for example, arule such as OBJ@S ?
NPdef,acc captures the requirement that definite objects in Hebrewmust be marked for accusativity using the !??
marker, and the rest if the generativeprocess will generate the object NP according to this specified constraint.
This kind oflinguistically motivated separation of form and function is shown to produce modelswith fewer parameters and result in better parsing accuracies than plain (or head-driven) PCFGs derived from the same trees.The relational-realizational model can accommodate agreement information.
It isshown in Tsarfaty and Sima?an (2010) that, given gold-standard POS tags that includethe gender and number information for individual words, RR models enriched withgender and number agreement information can provide Modern Hebrew parsing ac-curacies of 84% F1 for sentences of up to 40 words, the highest reported number forModern Hebrew parsing based on gold POS tags and word-segmentation by the timeof its publication.Although the RR framework is well motivated linguistically and appealing aesthet-ically, in the current work we chose to rely on the extreme markovization employed bythe PCFG-LA BerkeleyParser in order to cope with the constituent order variation, andtomodel agreement as an external filter that is orthogonal to the grammar.
The approachtaken in this article provides state-of-the-art results for Hebrew constituency parsing.We leave the question of integrating the RR approach with the approach presented hereto future work.12.
ConclusionsWe presented experiments on Hebrew Constituency Parsing based on the PCFG-LAmethodology of Petrov et al(2006).
The PCFG-LA model performs well out-of-the-box,especially when the gold POS tags are available to the parser.
It is possible to improvethe learned grammar, however, by specifying some manual state-splits, specifically153Computational Linguistics Volume 39, Number 1distinguishing between modal, finite, and infinitive verbs, and explicit marking ofsubject-NPs.Parsing accuracies drop considerably when the gold POS tags are not available, anddrop even further when using non-gold segmentation.
A large part of the drop whenthe gold POS tags are not available is due to the large percentage of lexical events thatare unseen or seen only a few times in the training set.
This drop can be mitigatedby extending the lexical coverage of the parser using an external lexical resource suchas a wide-coverage morphological analyzer for mapping lexical items to their possiblePOS tags.
The POS-tagging schemes assumed by the treebank and the morphologicalanalyzer need not be compatible with each other: We present a method for bridgingthe POS tags differences between the two resources.
The morphological analyzer doesnot provide lexical probabilities.
Parsing accuracies can be further improved by usinglexical probabilities which are derived in a semi-supervised fashion based on the mor-phological analyzer and a large corpus of unannotated text.The correct token-segmentation is very important for achieving high-quality parses,and when the gold segmentation is not available, parsing results drop considerably.It is better to let the parser induce its preferred segmentation in interaction with theparsing process rather than to use a segmentation based on an external sequence modelin a pipeline fashion.
The joint induction of both the syntactic structure and the token-segmentation can be performed by representing the possible segmentations in latticestructure, and using lattice parsing.
Joint parsing and segmentation is shown to outper-form the pipeline approach.
The parsing accuracies with non-gold segmentation are stillfar below the accuracies when the gold-segmentation is assumed to be known, however,and accurate parsing with non-gold segmentation remains a challenging open researchproblem.The learned PCFG-LA grammar is not capable of modeling agreement information.We considered methods of using morphological agreement information to improveparsing accuracy.
We propose modeling agreement information as a filtering processthat is orthogonal to the grammar used for parsing.
The approach works in the sensethat, in contrast to other methods of using agreement information, it does not degradeparsing accuracy and even improves it slightly.
The benefit from the agreement filteringis small, however: With the strong grammar induced by the PCFG-LA training pro-cedure, the parser makes very few agreement mistakes to begin with.
Modeling mor-phological agreement is probably more useful in syntactic generation than in syntacticparsing.
We expect the filtering approach we propose to be proven useful for tasksinvolving syntactic generation, such as target-side-syntax machine translation into amorphologically rich language.Overall, we presented four enhancements to the PCFG-LA mechanism in orderto adapt it to parsing Hebrew: the introduction of manual, linguistically motivatedstate-splits; extending the lexical coverage of the parser using an external morpho-logical analyzer; performing segmentation and parsing jointly using a lattice parser;and incorporating agreement information in a filtering framework.
Together, theseenhancements result in the best published results for Hebrew Constituency Parsing todate.Appendix A: The Hebrew Agreement FilterHebrew syntax requires agreement in gender, number, and person.
The implementationconsiders only the gender and number features, which are the most common.
Each of154Goldberg and Elhadad Parsing System for Hebrewthe features can take one of five values Masculine, Feminine, Both, Unknown, and NA forGender, and Singular, Plural, Both, Unknown and NA for Number.
Masculine, Feminine,Singular, and Plural are self-explanatory, and are assigned when the feature value isobvious.
NA means that the feature is irrelevant for the given constituent (adverbsand PPs do not carry gender or number features).
Both and Unknown are assignedwhen we are uncertain about the corresponding feature value.
Both and Unknown areidentical in the sense that they leave the feature value unspecified, and have the sameeffect on the filtering process.
From a practical perspective they could be collapsedinto the same category.
We chose to maintain the distinction between the two casesbecause they have slightly different semantics.
Both indicates that both options arepossible (for example, the form !?????
is ambiguous between the plural girls and thesingular childhood, and the titular !?
?, Dr. can refer both to males and females), whereasUnknown means that the feature value could not be computed due to a limitationof the model (for example, there is no clear rule as to the gender of a conjunctionwhich coordinates masculine and feminine NPs, and we are currently unable to accu-rately infer the gender and number associated with certain complex quantifiers suchas !???
(most).
Compare: !?????
??????
??
?, !???????
?????
??
?, !?????
???
??????
(?most ofthe classfem stayedmasc, most of the cakefem was eatenfem, most of the cakefem waseatenmasc?
).Feature values are said to agree if they are compatible with each other.
Feminine iscompatible with NA, Both, and Unknown but not with Masculine.
Similarly, Singular iscompatible with NA, Both, and Unknown, but not with Plural.Agreement cases.
The system is designed to handle the following cases of morphologicalagreement:NP level agreement between nouns and adjectives.
!??????
??????
???????
????
(?box-ofSgapplesPl greenPl bigPl?)
, !????
??????
???????
????
(?box-ofSg applesPl greenPl bigSg?
)S level agreement between subject and verbs.
!???
??????
???
(?
[one-of the-kids]SgwalkedSg?
)Predicative agreement between the subject, ADJP, and copular element.
!????
???
(?he[is] smartmasc?
), !????????
????
???
(?she was amazing/fem?
), but not with nouns ????!????
???
(?she was a-symbolmasc?
).Agreement between the Verb in a relativized SBAR and the realization of the Null-subject in the external NP.!??????
????
?
??????
(?the-committeefem which [*] discussedfem the-matter?
)Morphological feature propagation.
The first step of determining agreement is propagatingthe relevant features from the leaves up to the constituent level.The procedure begins by assigning each leaf gender and number features.
Theseare assigned based either on the TB tag assigned for the word if training on goldPOS tags, or on the morphological analyzer entries for the given word (in most casesthe number and gender features are easy to predict, even in cases where the corePOS is not clear.
In the relatively rare cases where the analyzer contains both a fem-inine and masculine (alt.
singular and plural) analyses, feature value is marked asBoth).155Computational Linguistics Volume 39, Number 1Table A.1Gender and number percolation rules.
FC = first child with non-NA gender/number.
Rules foreach constituent type are applied in order, until a condition holds.
Rules for gender and numberare applied independently of each other.Constituent Condition Feature ValuesSBAR has REL and S children S.featuresSBAR otherwise NAPREDP has ADJP child ADJP.featuresPREDP has AGR child and no NP child AGR.featuresPREDP otherwise NAS has VP child and no NP-Subj child VP.featuresS has VB child and no NP-Subj child VB.featuresS otherwise NANNPG always UNP has NNT child NNT.featuresNP has CDT and NP children CDT.number NP.genderNP is a conjunction gender=U number=PluralNP has a ?
child UNP first child is NP, second is POS NP.featuresNP has IN child FC.gender number=UNP has child with non-NA gen/num FC.gender FC.numberNP otherwise NAADJP has JJT child JJT.featuresADJP has child with non-NA gen/num FC.gender FC.numberADJP otherwise NAVP has VB child VB.featuresVP has VB-Modal child VB-Modal.featuresVP has VP child VP.featuresVP otherwise NAother always NAAfter each leaf is assigned feature values, the features are propagated up the treeaccording to a set of rules such as the following (the complete set of rules is given inTable A.1):r If the constituent is an NP and has a Construct-noun child, it is assignedthe gender of the Construct-noun.r If the constituent is a coordinated NP (has a CC child), set its numberfeature to plural.r If the constituent is an S and it has VP child but no NP-Subject child, takethe gender from the VP.Agreement rules.
Once the features are propagated from the leaves to a constituent,agreement is verified at the constituent level according to the following rules:NP agreement rules:r Agreement for coordinated NPs and Possessive NPs is not checked.156Goldberg and Elhadad Parsing System for Hebrewr If NP has an SBAR child, all the children up to the SBAR whose type isnominal or adjectival must agree in gender and number.r If NP has an ADJP child, all the children up to the ADJP whose type isnominal or adjectival must agree in gender and number.(a)NPNPNNTFem,Sg!??????NPNNMs,Pl!???????JJMs,Pl!??????JJFem,Sg!?????(b)NPNPNNTFem,Sg!??????NPNNMs,Pl!???????JJMs,Pl!??????JJFem,Sg!?????(c)NPFem,SgNPFem,SgNNTFem,Sg!??????NPMs,PlNNMs,Pl!???????JJMs,Pl!??????JJFem,Sg!?????(d)NPFem,SgNPFem,SgNNTFem,Sg!??????NPMs,PlNNMs,Pl!???????JJMs,Pl!??????JJFem,Sg!????
?Figure A.1Agreement annotation and validation example: correct tree.
The sentence words translate tobox-of apples green big, literally, a big box of green apples.(a)NPNNTFem,Sg!??????NPNPNNMs,Pl!???????JJMs,Pl!??????JJFem,Sg!?????(b)NPNNTFem,Sg!??????NPNPNNMs,Pl!???????JJMs,Pl!??????JJFem,Sg!?????(c)NPFem,SgNNTFem,Sg!??????NPMs,PlNPMs,PlNNMs,Pl!???????JJMs,Pl!??????JJFem,Sg!?????(d)NPFem,SgNNTFem,Sg!??????NPMs,PlNPMs,PlNNMs,Pl!???????JJMs,Pl!??????JJFem,Sg!????
?Figure A.2Agreement annotation and validation example: incorrect tree, agreement violation.
box-of applesgreen big, literally, a big box of green apples, though the parse tree suggests the interpretation a boxof big green apples.157Computational Linguistics Volume 39, Number 1S agreement rule:r All children of S with type in {NP-Subject, VP, VB, AUX, PREDP} mustagree in their gender and number features.ADJP agreement rule:r All children of ADJP with type in {NP, NP-Subject, NN, JJ, ADJP} mustagree in their gender and number features.An example.
Consider the tree in Figure A.1a.
In the first stage (Figure A.1b), agreementfeatures are propagated according to the rules in Table A.1, resulting in the annotatedtree in Figure A.1c.
Agreement is then validated in Figure A.1d (nodes in which anagreement rule applied and passed are marked in green).
In contrast, the tree in Fig-ure A.2a has an agreement mistake.
As before, the agreement features are propagatedaccording to the rules (Figure A.2b) resulting in Figure A.2c.
Agreement validationfails at Figure A.2d (the node in which agreement validation was applied and failedis marked in red).ReferencesAbeille?, Anne, Lionel Cle?ment, and Franc?oisToussenel.
2003.
Building a treebank forFrench.
In A.
Abeille?, editor.
Treebanks:Building and Using Parsed Corpora.Springer, Berlin, pages 165?188.Adler, Meni.
2001.
Hidden Markov model forHebrew part-of-speech tagging.
Master?sthesis, Ben-Gurion University of theNegev.Adler, Meni.
2007.
Hebrew MorphologicalDisambiguation: An Unsupervised StochasticWord-based Approach.
Ph.D. thesis,Ben-Gurion University of the Negev.Adler, Meni and Michael Elhadad.
2006.An unsupervised morpheme-basedHMM for Hebrew morphologicaldisambiguation.
In Proceedings of the 21stInternational Conference on ComputationalLinguistics and 44th Annual Meeting of theAssociation for Computational Linguistics,pages 665?672, Sydney.Adler, Meni, Yoav Goldberg, DavidGabay, and Michael Elhadad.
2008a.Unsupervised lexicon-based resolution ofunknown words for full morphologicalanalysis.
In Proceedings of ACL-08: HLT,pages 728?736, Columbus, OH.Adler, Meni, Yael Netzer, David Gabay,Yoav Goldberg, and Michael Elhadad.2008b.
Tagging a Hebrew corpus: The caseof participles.
In Proceedings of LREC 2008,pages 3167?3174, Marrakech.Attia, Mohammed, Jennifer Foster, DeirdreHogan, Joseph Le Roux, Lamia Tounsi,and Josef van Genabith.
2010.
Handlingunknown words in statisticallatent-variable parsing models for Arabic,English and French.
In Proceedings of theNAACL HLT 2010 First Workshop onStatistical Parsing of Morphologically-RichLanguages, pages 67?75, Los Angeles, CA.Bar-Haim, Roy, Khalil Sima?an, and YoadWinter.
2005.
Choosing an optimalarchitecture for segmentation andPOS-tagging of Modern Hebrew.
InProceedings of the ACL Workshop onComputational Approaches to SemiticLanguages, pages 39?46, Ann Arbor, MI.Bar-Haim, Roy, Khalil Sima?an, and YoadWinter.
2008.
Part-of-speech tagging ofModern Hebrew text.
Natural LanguageEngineering, 14(2):223?251.Billott, Sylvie and Bernard Lang.
1989.
Thestructure of shared forests in ambiguousparsing.
In Proceedings of the 27th AnnualMeeting of the Association for ComputationalLinguistics, pages 143?151, Vancouver.BGU Computational Linguistics Group.2008.
Hebrew morphological taggingguidelines.
Technical report, Ben GurionUniversity of the Negev.Cai, Shu, David Chiang, and Yoav Goldberg.2011.
Language-independent parsing withempty elements.
In Proceedings of the49th Annual Meeting of the Association forComputational Linguistics: Human LanguageTechnologies, pages 212?216, Portland, OR.Candito, Marie and Beno?
?t Crabbe?.
2009.Improving generative statistical parsingwith semi-supervised word clustering.In Proceedings of the 11th International158Goldberg and Elhadad Parsing System for HebrewConference on Parsing Technologies(IWPT?09), pages 138?141, Paris.Candito, Marie, Beno?
?t Crabbe?, andDjame?
Seddah.
2009.
On statisticalparsing of French with supervisedand semi-supervised strategies.In EACL 2009 Workshop GrammaticalInference for Computational Linguistics,pages 49?57, Athens.Chappelier, J., M. Rajman, R. Aragues, andA.
Rozenknop.
1999.
Lattice parsing forspeech recognition.
In Sixth Conferencesur le Traitement Automatique du LangageNaturel (TANL?99), pages 95?104, Carge?se.Charniak, Eugene and Mark Johnson.2005.
Coarse-to-fine n-best parsing andmaxent discriminative reranking.In Proceedings of the 43rd Annual Meetingof the Association for ComputationalLinguistics (ACL?05), pages 173?180,Ann Arbor, MI.Cohen, Shay B. and Noah A. Smith.
2007.Joint morphological and syntacticdisambiguation.
In Proceedings of the2007 Joint Conference on Empirical Methodsin Natural Language Processing andComputational Natural Language Learning(EMNLP-CoNLL), pages 208?217, Prague.Collins, Michael and Terry Koo.
2005.Discriminative reranking for naturallanguage parsing.
ComputationalLinguistics, 31(1):25?69.Crabbe?, Beno?
?t and Marie Candito.
2008.Expe?riences d?analyses syntaxiquestatistique du franc?ais.
In Proceedingsof TALN, pages 45?54, Avignon.Dantzig, G. B. and P. Wolfe.
1960.Decomposition principle for linearprograms.
Operations Research, 8:101?111.Falk, Yehuda N. 2001.
Lexical-FunctionalGrammar: An Introduction to ParallelConstraint-Based Syntax.
CSLI Publications,Stanford, CA.Glinert, Lewis.
1989.
The Grammar of ModernHebrew.
Cambridge University Press.Goldberg, Yoav, Meni Adler, and MichaelElhadad.
2008.
EM can find pretty goodHMM POS-taggers (when given a goodstart).
In Proceedings of ACL-08: HLT,pages 746?754, Columbus, OH.Goldberg, Yoav and Michael Elhadad.2011.
Joint Hebrew segmentation andparsing using a PCFGLA lattice parser.
InProceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics:Human Language Technologies,pages 704?709, Portland, OR.Goldberg, Yoav and Reut Tsarfaty.
2008.A single generative model for jointmorphological segmentation and syntacticparsing.
In Proceedings of ACL-08: HLT,pages 371?379, Columbus, OH.Goldberg, Yoav, Reut Tsarfaty, Meni Adler,and Michael Elhadad.
2009.
Enhancingunlexicalized parsing performance usinga wide coverage lexicon, fuzzy tag-setmapping, and EM-HMM-based lexicalprobabilities.
In Proceedings of the 12thConference of the European Chapter of theACL (EACL 2009), pages 327?335,Athens.Green, Spence and Christopher D. Manning.2010.
Better Arabic parsing: Baselines,evaluations, and analysis.
In Proceedingsof the 23rd International Conference onComputational Linguistics (Coling 2010),pages 394?402, Beijing.Guthmann, Noemie, Yuval Krymolowski,Adi Milea, and Yoad Winter.
2009.Automatic annotation of morpho-syntacticdependencies in a Modern HebrewTreebank.
In Proceedings of the 1st Workshopon Treebanks and Linguistic Theories (TLT),pages 1?12, Groningen.Hall, Keith.
2005.
Best-first Word-latticeParsing: Techniques for Integrated SyntacticLanguage Modeling.
Ph.D. thesis, BrownUniversity.Huang, Zhongqiang and Mary Harper.2009.
Self-training PCFG grammars withlatent annotations across languages.In Proceedings of the 2009 Conference onEmpirical Methods in Natural LanguageProcessing, pages 832?841, Singapore.Itai, Alon and Shuly Wintner.
2008.
Languageresources for Hebrew.
Language Resourcesand Evaluation, 42(1):75?98.Jiang, Wenbin, Liang Huang, and Qun Liu.2009.
Automatic adaptation of annotationstandards: Chinese word segmentationand POS tagging?a case study.
InProceedings of the Joint Conference of the 47thAnnual Meeting of the ACL and the 4thInternational Joint Conference on NaturalLanguage Processing of the AFNLP,pages 522?530, Suntec.Johnson, Mark.
1998.
PCFG models oflinguistic tree representations.Computational Linguistics, 24:613?632.Klein, Dan and Christopher D. Manning.2003.
Accurate unlexicalized parsing.
InProceedings of the 41st Annual Meeting of theAssociation for Computational Linguistics,pages 423?430, Sapporo.Lang, Bernard.
1974.
Deterministictechniques for efficient non-deterministicparsers.
In J. Loeckx, editor, Automata,Languages and Programming, volume 14 of159Computational Linguistics Volume 39, Number 1Lecture Notes in Computer Science.
Springer,Berlin Heidelberg, pages 255?269.Lang, Bernard.
1988.
Parsing incompletesentences.
In Proceedings of COLING,pages 365?371, Budapest.Matsuzaki, Takuya, Yusuke Miyao, andJun?ichi Tsujii.
2005.
Probabilistic CFGwith latent annotations.
In Proceedings ofthe 43rd Annual Meeting of the Associationfor Computational Linguistics (ACL?05),pages 75?82, Ann Arbor, MI.Netzer, Yael, Meni Adler, David Gabay, andMichael Elhadad.
2007.
Can you tag themodal?
You should!
In Proceedings of the2007 Workshop on Computational Approachesto Semitic Languages: Common Issues andResources, pages 57?64, Prague.Petrov, Slav.
2009.
Coarse-to-Fine NaturalLanguage Processing.
Ph.D. thesis,University of California at Berkeley.Petrov, Slav.
2010.
Products of random latentvariable grammars.
In Proceedings ofNAACL, pages 19?27, Los Angeles, CA.Petrov, Slav, Leon Barrett, Romain Thibaux,and Dan Klein.
2006.
Learning accurate,compact, and interpretable treeannotation.
In Proceedings of the 21stInternational Conference on ComputationalLinguistics and 44th Annual Meeting of theAssociation for Computational Linguistics,pages 433?440, Sydney.Petrov, Slav and Dan Klein.
2007.
Improvedinference for unlexicalized parsing.In Human Language Technologies 2007:The Conference of the North AmericanChapter of the Association for ComputationalLinguistics; Proceedings of the MainConference, pages 404?411, Rochester, NY.Petrov, Slav and Dan Klein.
2008.
ParsingGerman with latent variable grammars.In Proceedings of the Workshop on ParsingGerman, pages 33?39, Columbus, OH.Pollard, Carl and Ivan A.
Sag.
1994.Head-driven phrase structure grammar.University of Chicago Press.Prescher, Detlef.
2005.
Inducing head-drivenPCFGs with latent heads: Refining a tree-bank grammar for parsing.
In Proceedingsof the European Conference on MachineLearning (ECML), pages 292?304, Porto.Rush, Alexander M, David Sontag, MichaelCollins, and Tommi Jaakkola.
2010.On dual decomposition and linearprogramming relaxations for naturallanguage processing.
In Proceedings ofEMNLP, pages 1?11, Cambridge, MA.Sima?an, Khalil.
1996.
Computationalcomplexity of probabilistic disambiguationby means of tree grammars.
In Proceedingsof COLING, pages 1175?1180, Copenhagen.Sima?an, Khalil.
1999.
Learning EfficientDisambiguation.
Ph.D. thesis, ILLCDissertation Series, University ofAmsterdam.Sima?an, Khalil, Alon Itai, Yoad Winter,Alon Altman, and Noa Nativ.
2001.Building a tree-bank of Modern Hebrewtext.
Traitement Automatique des Langues,42(2):1?32.Smith, Noah A., David A. Smith, andRoy W. Tromble.
2005.
Context-basedmorphological disambiguation withrandom fields.
In Proceedings of EMNLP,pages 475?482, Vancouver.Tsarfaty, Reut.
2006a.
Integratedmorphological and syntacticdisambiguation for Modern Hebrew.In Proceedings of the COLING/ACL 2006Student Research Workshop, pages 49?54,Sydney.Tsarfaty, Reut.
2006b.
The Interplay of Syntaxand Morphology in Building ParsingModels for Modern Hebrew.
In Proceedingsof ESSLI Student Session, pages 263?274,Malaga.Tsarfaty, Reut.
2010.
Relational-RealizationalParsing.
Ph.D. thesis, ILLC DissertationSeries, University of Amsterdam.Tsarfaty, Reut and Khalil Sima?an.
2007.Three-dimensional parametrization forparsing morphologically rich languages.In Proceedings of the Tenth InternationalConference on Parsing Technologies,pages 156?167, Prague.Tsarfaty, Reut and Khalil Sima?an.
2008.Relational-realizational parsing.
InProceedings of CoLING, pages 889?896,Manchester.Tsarfaty, Reut and Khalil Sima?an.
2010.Modeling morphosyntactic agreementin constituency-based parsing ofModern Hebrew.
In Proceedings of theNAACL HLT 2010 First Workshop onStatistical Parsing of Morphologically-RichLanguages, pages 40?48, Los Angeles, CA.Tsarfaty, Reut, Khalil Sima?an, and RemkoScha.
2009.
An alternative to head-drivenapproaches for parsing a (relatively) freeword-order language.
In Proceedingsof the 2009 Conference on EmpiricalMethods in Natural Language Processing,pages 842?851, Singapore.160
