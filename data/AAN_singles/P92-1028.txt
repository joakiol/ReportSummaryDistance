CORPUS-BASED ACQUISITION OF RELATIVE PRONOUNDISAMBIGUATION HEURISTICSClaire CardieDepartment of  Computer ScienceUniversity of  MassachusettsAmherst, MA 01003E-mail: cardie@cs.umass.eduABSTRACTThis paper presents a corpus-based approach forderiving heuristics to locate the antecedents of relativepronouns.
The technique dupficates the performanceof hand-coded rules and requires human interventiononly during the training phase.
Because the traininginstances are built on parser output rather than wordcooccurrences, the technique requires a small numberof training examples and can be used on small tomedium-sized corpora.
Our initial results uggest thatthe approach may provide a general method for theautomated acquisition of a variety of disambiguationheuristics for natural language systems, especially forproblems that require the assimilation of syntactic andsemantic knowledge.1 INTRODUCTIONState-of-the-art natural anguage processing (NLP)systems typically rely on heuristics to resolve manyclasses of ambiguities, e.g., prepositional phraseattachment, part of speech disambiguation, wordsense disambiguation, conjunction, pronounresolution, and concept activation.
However, themanual encoding of these heuristics, either as part ofa formal grammar or as a set of disarnbiguation rules,is difficult because successful heuristics demand theassimilation of complex syntactic and semanticknowledge.
Consider, for example, the problem ofprepositional phrase attachment.
A number of purelystructural solutions have been proposed including thetheories of Minimal Attachment (Frazier, 1978) andRight Association (Kimball, 1973).
While thesemodels may suggest the existence of strong syntacticpreferences in effect during sentence understanding,other studies provide clear evidence that purelysyntactic heuristics for prepositional phraseattachment will not work (see (Whittemore, Ferrara,& Brunner, 1990), (Taraban, & McClelland, 1988)).However, computational linguists have found themanual encoding of disarnbiguation rules - -especially those that merge syntactic and semanticconstraints - -  to be difficult, time-consuming, andprone to error.
In addition, hand-coded heuristics areoften incomplete and perform poorly in new domainscomprised of specialized vocabularies or a differentgenre of text.In this paper, we focus on a single ambiguity insentence processing: locating the antecedents ofrelative pronouns.
We present an implementedcorpus-based approach for the automatic acquisition ofdisambiguation heuristics for that task.
The techniqueuses an existing hierarchical clustering system todetermine the antecedent ofa relative pronoun given adescription of the clause that precedes it and requiresonly minimal syntactic parsing capabilities and a verygeneral semantic feature set for describing nouns.Unlike other corpus-based techniques, only a smallnumber of training examples is needed, making theapproach practical even for small to medium-sized on-line corpora.
For the task of relative pronoundisambignation, the automated approach duplicatesthe performance of hand-coded rules and makes itpossible to compile heuristics tuned to a new corpuswith little human intervention.
Moreover, we believethat the technique may provide a general approach forthe automated acquisition of disambiguationheuristics for additional problems in naturallanguage processing.In the next section, we briefly describe the task ofrelative pronoun disambiguation.
Sections 3 and 4give the details of the acquisition algorithm andevaluate its performance.
Problems with theapproach and extensions required for use with largecorpora of unrestricted text are discussed in Section 5.2 D ISAMBIGUATING RELAT IVEPRONOUNSAccurate disambiguation of relative pronouns isimportant for any natural language processing systemthat hopes to process real world texts.
It is especiallya concern for corpora where the sentences tend to belong and information-packed.
Unfortunately, tounderstand a sentence containing a relative pronoun,an NLP system must solve two difficult problems:the system has to locate the antecedent of the relativepronoun and then determine the antecedent's implicitposition in the embedded clause.
Although finding thegap in the embedded clause is an equally difficult216problem, the work we describe here focuses onlocating the relative pronoun antecedent.1This task may at first seem relatively simple: theantecedent of a relative pronoun is just the mostrecent constituent that is a human.
This is the casefor sentences S1-$7 in Figure 1, for example.However, this strategy assumes that the NLP systemproduces a perfect syntactic and semantic parse of theclause preceding the relative pronoun, includingprepositional phrase attachment (e.g., $3, $4, and$7) and interpretation f conjunctions (e.g., $4, $5,and $6) and appositives (e.g., $6).
In $5, forexample, the antecedent is the entire conjunction ofphrases (i.e., "Jim, Terry, and Shawn"), not just themost recent human (i.e., "Shawn").
In $6, eithers1.
Tony saw the boy who won the award.$2.
The boy who gave me the book had red hair.$3.
Tony ate dinner with the men from Detroit whosold computers.$4.
I spoke to the woman with the black shirt andgreen hat over in the far comer of the room whcwanted asecond interview.SS.
I'd like to thank Jim.
Terry, and Shawn, whoprovided the desserts.$6.
I'd like to thank our sponsors, GE andNSF,  whoprovide financial support.ST.
The woman from Philadelphia who played soccerwas my sister.$8.
The awards for the children who pass the test arein the drawer.$9.
We wondered who stole the watch.S10.
We talked with the woman and the man whodanced.Figure 1.
Examples o f  Relat ivePronoun Antecedents"our sponsors" or its appositive "GE and NSF" is asemantically valid antecedent.
Because pp-attachmentand interpretation of conjunctions and appositivesremain difficult for current systems, it is oftenunreasonable to expect reliable parser output forclauses containing those constructs.Moreover, the parser must access both syntacticand semantic knowledge in finding the antecedent of arelative pronoun.
The syntactic structure of the clausepreceding "who" in $7 and $8, for example, isidentical (NP-PP) but the antecedent in each case isdifferent.
In $7, the antecedent is the subject, "thewoman;" in $9, it is the prepositional phrase1For a solution to the gap-finding problem that isconsistent with the simplified parsing strategypresented below, see (Cardie & Lehnert, 1991).modifier, "the children."
Even if we assume aperfectparse, there can be additional complications.
In somecases the antecedent is not the most recentconstituent, but is a modifier of that constituent (e.g.,$8).
Sometimes there is no apparent antecedent a all(e.g., $9).
Other times the antecedent is trulyambiguous without seeing more of the surroundingcontext (e.g., S10).As a direct result of these difficulties, NLP systembuilders have found the manual coding of rules thatfind relative pronoun antecedents to be very hard.
Inaddition, the resulting heuristics are prone to errorsof omission and may not generalize to new contexts.For example, the UMass/MUC-3 system 2 began with19 rules for finding the antecedents of relativepronouns.
These rules included both structural andsemantic knowledge and were based on approximately50 instances of relative pronouns.
As counter-examples were identified, new rules were added(approximately 10) and existing rules changed.
Overtime, however, we became increasingly reluctant tomodify the rule set because the global effects of localrule changes were difficult o measure.
Moreover, theoriginal rules were based on sentences thatUMass/MUC-3 had found to contain importantinformation.
As a result, the rules tended to workwell for relative pronoun disambiguation in sentencesof this class (93% correct for one test set of 50 texts),but did not generalize to sentences outside of the class(78% correct on the same test set of 50 texts).2.1 CURRENT APPROACHESAlthough descriptions of NLP systems do notusually include the algorithms used to find relativepronoun antecedents, current high-coverage parsersseem to employ one of 3 approaches for relativepronoun disambiguation.
Systems that use a formalsyntactic grammar often directly encode informationfor relative pronoun disambiguation i the grammar.Alternatively, a syntactic filter is applied to the parsetree and any noun phrases for which coreference withthe relative pronoun is syntactically legal (or, insome cases, illegal) are passed to a semanticcomponent which determines the antecedent usinginference or preference rules (see (Correa, 1988),(Hobbs, 1986), (Ingria, & Stallard, 1989), (Lappin,& McCord, 1990)).
The third approach employs hand-coded disambiguation heuristics that rely mainly on2UMass/MUC-3 is a version of the CIRCUS parser(Lehnert, 1990) developed for the MUC-3performance evaluation.
See (Lehnert et.
al., 1991)for a description of UMass/MUC-3.
MUC-3 is theThird Message Understanding System Evaluation andMessage Understanding Conference (Sundheim,1991).217semantic knowledge but also include syntacticconstraints (e.g., UMass/MUC-3).However, there are problems with all 3 approachesin that 1) the grammar must be designed to findrelative pronoun antecedents for all possible syntacticcontexts; 2) the grammar and/or inference rules requiretuning for new corpora; and 3) in most cases, theapproach unreasonably assumes a completely correctparse of the clause preceding the relative pronoun.
Inthe remainder of the paper, we present an automatedapproach for deriving relative pronoun disambigu_a6onrules.
This approach avoids the problems associatedwith the manual encoding of heuristics and grammarsand automatically tailors the disambiguationdecisions to the syntactic and semantic profile of thecorpus.
Moreover, the technique requires only a verysimple parser because input to the clustering systemthat creates the disambiguation heuristics presumesneither pp-attachment nor interpretation ofconjunctions and appositives.3 AN AUTOMATED APPROACHOur method for deriving relative pronoundisambiguation heuristics consists of the followingsteps:1.
Select from a subset of the corpus allsentences containing a particular relativepronoun.
(For the remainder of the paper, wewill focus on the relative pronoun "who.")2.
For each instance of the relative pronoun inthe selected sentences,a.
parse the portion of the sentence thatprecedes it into low-level syntactic onstituentsb.
use the results of the parse to create atraining instance that represents thedisambiguation decision for this occurrence ofthe relative pronoun.3.
Provide the training instances as input to anexisting conceptual c ustering system.During the training phase outlined above, theclustering system creates a hierarchy of relativepronoun disambiguation decisions that replace thehand-coded heuristics.
Then, for each new occurrenceof the wh-word encountered after training, we retrievethe most similar disambiguation decision from thehierarchy using a representation of the clausepreceding the wh-word as the probe.
Finally, theantecedent of the retrieved decision guides theselection of the antecedent for the new occurrence ofthe relative pronoun.
Each step of the training andtesting phases will be explained further in thesections that follow.3.1 SELECT ING SENTENCESFROM THE CORPUSFor the relative pronoun disambiguation task, weused the MUC-3 corpus of 1500 articles that rangefrom a single paragraph to over one page in length.In theory, each article describes one or more terroristincidents in Latin America.
In practice, however,about half of the texts are actually irrelevant to theMUC task.
The MUC-3 articles consist of a varietyof text types including newspaper articles, TV newsreports, radio broadcasts, rebel communiques,speeches, and interviews.
The corpus is relativelysmall - it contains approximately 450,000 words and18,750 sentences.
In comparison, most corpus-basedalgorithms employ substantially arger corpora (e.g.,1 million words (de Marcken, 1990), 2.5 millionwords (Brent, 1991), 6 million words (Hindle, 1990),13 million words (Hindle, & Rooth, 1991)).Relative pronoun processing is especiallyimportant for the MUC-3 corpus becauseapproximately 25% of the sentences contain at leastone relative pronoun.
3 In fact, the relative pronoun"who" occurs in approximately 1 out of every 10sentences.
In the experiment described below, we use100 texts containing 176 instances of the relativepronoun "who" for training.
To extract sentencescontaining a specific relative pronoun, we simplysearch the selected articles for instances of the relativepronoun and use a preprocessor to locate sentenceboundaries.3.2 PARSING REQUIREMENTSNext, UMass/MUC-3 parses each of the selectedsentences.
Whenever the relative pronoun "who" isrecognized, the syntactic analyzer returns a list of thelow-level constituents of the preceding clause prior toany attachment decisions (see Figure 2).UMass/MUC-3 has a simple, deterministic, stack-oriented syntactic analyzer based on the McEli parser(Schank, & Riesbeck, 1981).
It employs lexically-indexed local syntactic knowledge to segmentincoming text into noun phrases, prepositionalphrases, and verb phrases, ignoring all unexpectedconstructs and unknown words.
4Each constituent3There are 4707 occurrences of wh-words (i.e., who,whom, which, whose, where, when, why) in theapproximately 18,750 sentences that comprise theMUC-3 corpus.4Although UMass/MUC-3 can recognize othersyntactic lasses, only noun phrases, prepositionalphrases, and verb phrases become part of the traininginstance.218Sources in downtown Lima report thatthe police last night detained JuanBautista and Rogoberto Matute, who ...~ U Mass/MUC-3 syntacticanalyzerthe  po l ice  : \[subject, human\]deta ined  : \[verb\]Juan Bautista : \[np, proper-name\]Rogober to  Matute  : \[np, proper-name\]Figure 2.
Syntactic Analyzer Outputreturned by the parser (except the verb) is tagged withthe semantic lassification that best describes thephrase's head noun.
For the MUC-3 corpus, we use aset of 7 semantic features to categorize ach noun inthe lexicon: human, proper-name, location, entity,physical-target, organization, and weapon.
Inaddition, clause boundaries are detected using amethod escribed in (Cardie, & Lehnert, 1991).It should be noted that all difficult parsingdecisions are delayed for subsequent processingcomponents.
For the task of relative pronoundisambiguation, this means that the conceptualclustering system, not the parser, is responsible forrecognizing all phrases that comprise aconjunction ofantecedents and for specifying at least one of thesemantically valid antecedents in the case ofappositives.
In addition, pp-attachment is moreeasily postponed until after the relative pronounantecedent has been located.
Consider the sentence "Iate with the men from the restaurant in the club.
"Depending on the context, "in the club" modifieseither "ate" or "the restaurant."
If we know that "themen" is the antecedent ofa relative pronoun, however(e.g., "I ate with the men from the restaurant in theclub, who offered me the job"), it is probably the casethat "in the club" modifies "the men.
"Finally, because the MUC-3 domain is sufficientlynarrow in scope, lexical disambiguation problems areinfrequent.
Given this rather simplistic view ofsyntax, we have found that a small set of syntacticpredictions covers the wide variety of constructs inthe MUC-3 corpus.3.3 CREAT ING THE TRAIN INGINSTANCESOutput from the syntactic analyzer is used togenerate a training instance for each occurrence of therelative pronoun in the selected sentences.
A traininginstance represents a single disambiguation decisionand includes one attribute-value pair for every low-level syntactic onstituent in the preceding clause.The attributes of a training instance describe thesyntactic class of the constituent as well as itsposition with respect o the relative pronoun.
Thevalue associated with an attribute is the semanticfeature of the phrase's head noun.
(For verb phrases,we currently note only their presence or absence usingthe values tand nil, respectively.
)Consider the training instances in Figure 3.
In S 1,for example, "of the 76th district court" is representedwith the attribute ppl because it is a prepositionalphrase and is in the first position to the left of "who.
"Its value is "physical-target" because "court" isclassified as a physical-target in the lexicon.
Thesubject and verb constituents (e.g., "her DASbodyguard" in $3 and "detained" in $2) retain theirtraditional s and v labels, however - -  no positionalinformation is included for those attributes.S1: \[The judge\] \[of the 76th court\] \[,\] who ...I ITraining instance: \[ (s human) (pp l physical-rargeO (v nil) (antecedent ((s) ) \]f12: \[The police\] \[detained\] Uuan Bautista\] [and\] [Rogoberto Matute\] \[,\] who ...Training instanoa: \[ (s human) (v 0 (np2 proper-name) (npl proper-name)(antecedent ((rip2 npl))) \]S8: \[Her DAS bodyguard\] \[,\] \[Dagoberto Rodriquez\] [,\] who...I ITraining instance: \[( s human) (npl proper-name) (v nil)(antecedent ((npl )(s npl )(s)))\]Figure 3.
Tra in ing  Ins tances219In addition to the constituent a tribute-value pairs,a training instance contains an attribute-value pairthat represents he correct antecedent.
As shown inFigure 3, the value of the antecedent attribute is a listof the syntactic constituents that contain theantecedent (or (none) if the relative pronoun has noanteceden0.
In S 1, for example, the antecedent of"who" is "the judge."
Because this phrase is locatedin the subject position, the value of the antecedentattribute is (s).
Sometimes, however, the antecedentis actually a conjunction of phrases.
In these cases,we represent he antecedent as a list of theconstituents associated with each element of theconjunction.
Look, for example, at the antecedent in$2.
Because "who" refers to the conjunction "JuanBautista nd Rogoberto Matute," and because thosephrases occur as rip1 and rip2, the value of theantecedent attribute is (np2 npl).
$3 shows yetanother variation of the antecedent attribute-valuepair.
In this example, an appositive creates threeequivalent antecedents: 1) "Dagoberto Rodriguez"(rip1), 2) "her DAS bodyguard" m (s), and 3) "herDAS bodyguard, Dagoberto Rodriguez" - -  (s npl).UMass/MUC-3 automatically generates thetraining instances as a side effect of parsing.
Onlythe desired antecedent is specified by a humansupervisor via a menu-driven i terface that displaysthe antecedent options.3.4 BUILDING THE HIERARCHYOF DISAMBIGUATIONHEURISTICSAs the training instances become available they areinput to an existing conceptual clustering systemcalled COBWEB (Fisher, 1987).
5COBWEB employsan evaluation metric called category utility (Gluck,& Corter, 1985) to incrementally discover aclassification hierarchy that covers the traininginstances.
6 It is this hierarchy that replaces the hand-coded disambiguation heuristics.
While the details ofCOBWEB are not necessary, it is important to knowthat nodes in the hierarchy represent concepts thatincrease in generality as they approach the root of thetree.
Given a new instance to classify, COBWEB5 For these experiments, we used a version ofCOBWEB developed by Robert Williams at theUniversity of Massachusetts at Amherst.6Conceptual clustering systems typically discoverappropriate classes as well as the the concepts foreach class when given a set of examples that havenot been preclassified by a teacher.
Our unorthodoxuse of COBWEB to perform supervised learning isprompted by plans to use the resulting hierarchy fortasks other than relative pronoun disambiguation.220retrieves the most specific concept hat adequatelydescribes the instance.3.5 US ING THEDISAMBIGUATION HEURIST ICSH IERARCHYAfter training, the resulting hierarchy of relativepronoun disambiguation decisions supplies theantecedent of the wh-word in new contexts.
Given anovel sentence containing "who," UMass/MUC-3generates a set of attribute-value pairs that representthe clause preceding the wh-word.
This probe is justa training instance without he antecedent a tribute-value pair.
Given the probe, COBWEB retrievesfrom the hierarchy the individual instance or abstractclass that is most similar and the antecedent of theretrieved example guides selection of the antecedentfor the novel case.
We currently use the followingselection heuristics to 1) choose an antex~ent for thenovel sentence that is consistent with the context ofthe probe; or to 2) modify the retrieved antecedent sothat it is applicable in the current context:1.
Choose the first option whose constituentsare all present in the probe.2.
Otherwise, choose the first option thatcontains at least one constituent present in theprobe and ignore those constituents in theretrieved antex~ent that are missing from theprobe.3.
Otherwise, replace the np constituents in theretrieved antecedent that are missing from theprobe with pp constituents (and vice versa),and try 1 and 2 again.In S 1 of Figure 4, for example, the first selectionheuristic applies.
The retrieved instance specifies thenp2 constituent as the location of the antecedent andthe probe has rip2 as one of its constituents.Therefore, UMass/MUC-3 infers that the antecedentof "who" for the current sentence is "the hardliners,"i.e., the contents of the np2 syntactic onstituent.
In$2, however, the retrieved concept specifies anantecedent from five constituents, only two of whichare actually present in the probe.
Therefore, weignore the missing constituents pp5, rip4, and pp3,and look to just np2 and rip1 for the antecedent.
For$3, selection heuristics 1and 2 fail because the probecontains no pp2 constituent.
However, if we replacepp2 with np2 in the retrieved antecedent, henheuristic 1 applies and "a specialist" is chosen as theantecedent.Sl: \[It\] [encourages\] \[the military men\] \[,\] [and\] \[the hardliners\] \[in ARENA\] who...I I I\[(s enaty) (vO (np3 human) (np2 human) (ppl org)\]Antecedent of Retrieved Instance: ((np2))Antecedent of Probe:.
(np2) = "the hardliners"S2: \[There\] \[are\] \[also\] \[criminals\] \[like\] \[Vice President Merino\] \[,\] [a man\] who...\[(s entity) (v t) (rip3 human) (rip2 proper-name) (rip1 human)\]Antecedent of Retrieved Instance: ((pp5 np4 pp3 np2 np1))Antecedent of Probe:.
(np2 np1) = Wice President Merino, a man"$3: \[It\] [coincided\] \[with the arrival\] [of Smith\] \[,\] [a specialist\] \[from the UN\] \[,\] who...~ (pp4Jntity) \[ \[ (plplentity)\] \[(s entity) (v 0 (pp3 proper-name) (rip2 human)Antecedent of Retrieved Instance: ((pp2))Antecedent of Probe: (np2) = "a specialist"Figure 4.
Us ing  the  D isambiguat ion  Heur i s t i cs  H ierarchy4 RESULTSAs described above, we used 100 texts(approximately 7% of the corpus) containing 176instances of the relative pronoun "who" for training.Six of those instances were discarded when theUMass/MUC-3 syntactic analyzer failed to include thedesired antecedent as part of its constituentrepresentation, making it impossible for the humansupervisor to specify the location of the antecedent.
7After training, we tested the resulting disambiguationhierarchy on 71 novel instances extracted from anadditional 50 texts in the corpus.
Using the selectionheuristics described above, the correct antecedent wasfound for 92% of the test instances.
Of the 6 errors, 3involved probes with antecedent combinations neverseen in any of the training cases.
This usuallyindicates that the semantic and syntactic structure ofthe novel clause differs significantly from those inthe disambiguation hierarchy.
This was, in fact, thecase for 2 out of 3 of the errors.
The third errorinvolved a complex conjunction and appositivecombination.
In this case, the retrieved antecedentspecified 3out of 4 of the required constituents.If we discount the errors involving unknownantecedents, our algorithm correctly classifies 94%of the novel instances (3 errors).
In comparison, theoriginal UMass/MUC-3 system that relied on hand-coded heuristics for relative pronoun disambiguationfinds the correct antecedent 87% of the time (9 errors).However, a simple heuristic that chooses the mostrecent phrase as the antecedent succeeds 86% of thetime.
(For the training sets, this heuristic worksonly 75% of the time.)
In cases where the antecedentwas not the most recent phrase, UMass/MUC-3 errs67% of the time.
Our automated algorithm errs 47%of the time.It is interesting that of the 3 errors that did notspecify previously unseen an~exlents, one was causedby parsing blunders.
The remaining 2 errors involvedrelative pronoun antecedents that are difficult even forpeople to specify: 1) "... 9 rebels died at the hands ofmembers of the civilian militia, who resisted theattacks" and 2) "... the government expelled a groupof foreign drug traffickers who had establishedthemselves in northern Chile".
Our algorithm chose"the civilian militia" and "foreign drug traffickers" asthe antecedents of "who" instead of the preferredantecedents "members of the civilian militia" and"group of foreign drug traffickers.
"85 CONCLUSIONSWe have described an automated approach for theacquisition of relative pronoun disambiguationheuristics that duplicates the performance of hand-ceded rules.
Unfortunately, extending the techniquefor use with unrestricted texts may be difficult.
TheUMass/MUC-3 parser would clearly need additionalmechanisms tohandle the ensuing part of speech and7Other parsing errors occurred throughout the trainingset, but only those instances where the antecedent wasnot recognized as a constituent (and the wh-word hadan anteceden0 were discarded.8Interestingly, in work on the automatedclassification of nouns, (Hindle, 1990) also notedproblems with "empty" words that depend on theircomplements for meaning.221word sense disambiguation problems.
However,recent research in these areas indicates that automatedapproaches for these tasks may be feasible (see, forexample, (Brown, Della Pietra, Della Pietra, &Mercer, 1991) and (l-Iindle, 1983)).
In addition,although our simple semantic feature set seemsadequate for the current relative pronoundisambiguntion task, it is doubtful that a singlesemantic feature set can be used across all domainsand for all disambignation tasks.
9In related work on pronoun disambig~_~_afion, Daganand Itai (1991) successfully use statisticalcooccurrence patterns to choose among thesyntactically valid pronoun referents posed by theparser.
Their approach is similar in that thestatistical database depends on parser output.However, it differs in a variety of ways.
First,human intervention is required not to specify thecorrect pronoun antecedent, but to check that thecomplete parse tree supplied by the parser for eachtraining example is correct and to rule out potentialexamples that are inappropriate for their approach.More importantly, their method requires very largeCOrlxra of data.Our technique, on the other hand, requires fewtraining examples because ach training instance isnot word-based, but created from higher-level parseroutput.
10 Therefore, unlike other corpus-basedtechniques, our approach is practical for use withsmall to medium-sized corpora in relatively narrowdomains.
((Dagan & Itai, 1991) mention the use ofsemantic feature-based cooccurrences a  one way tomake use of a smaller corpus.)
In addition, becausehuman intervention is required only to specify theantecedent during the training phase, creatingdisambiguation heuristics for a new domain requireslittle effort.
Any NLP system that uses semanticfeatures for describing nouns and has minimalsyntactic parsing capabilities can generate he requiredtraining instances.
The parser need only recognizenoun phrases, verbs, and prepositional phrasesbecause the disambiguation heuristics, not the parser,are responsible for recognizing the conjunctions andappositives that comprise a relative pronounantecedent.
Moreover, the success of the approach forstructurally complex antecedents suggests that thetechnique may provide a general approach for the9 In recent work on the disambiguation ofstructurally, but not semantically, restricted phrases,however, a set of 16 predefined semantic ategoriessufficed (Ravin, 1990).10Although further work is needed to determine theoptimal number of training examples, it is probablythe case that many fewer than 170 instances wererequired even for the experiments described here.222automated acquisition of disambiguation rules forother problems in natural language processing.6 ACKNOWLEDGMENTSThis research was supported by the Office of NavalResearch, under a University Research InitiativeGrant, Contract No.
N00014-86-K-0764 and NSFPresidential Young Investigators Award NSFIST-8351863 (awarded to Wendy Lehnert) and theAdvanced Research Projects Agency of theDepartment of Defense monitored by the Air ForceOffice of Scientific Research under Contract No.F49620-88-C-0058.7 REFERENCESBrent, M. (1991).
Automatic acquisition ofsubcategorization frames from untagged text.Proceedings, 29th Annual Meeting of the Associationfor Computational Linguists.
University ofCalifornia, Berkeley.
Association for ComputationalLinguists.Brown, P. F., Della Pietra, S. A., Della Pietra, V.J., & Mercer, R. L. (1991).
Word-sensedisambiguation using statistical methods.Proceedings, 29th Annual Meeting of the Associationfor Computational Linguists.
University ofCalifornia, Berkeley.
Association for ComputationalLinguists.Cardie, C., & Lehnert, W. (1991).
A CognitivelyPlausible Approach to Understanding ComplexSyntax.
Proceedings, Eighth National Conference onArtificial Intelligence.
Anaheim, CA.
AAAI Press \]The MIT Press.Correa, N. (1988).
A Binding Rule forGovernment-Binding Parsing.
Proceedings, COLING'88.
Budapest.Dagan, I. and Itai, A.
(1991).
A Statistical Filterfor Resolving Pronoun References.
In Y.A.
Feldmanand A.Bruckstein (Eds.
), Artificial Intelligence andComputer Vision (pp.
125-135).
North-Holland:Elsevier.de Marcken, C. G. (1990).
Parsing the LOBcorpus.
Proceedings, 28th Annual Meeting of theAssociation for Computational Linguists.
Universityof Pittsburgh.
Association for ComputationalLinguists.Fisher, D. H. (1987).
Knowledge Acquisition ViaIncremental Conceptual Clustering.
MachineLearning, 2, 139-172.Frazier, L. (1978).
On comprehending sentences:Syntactic parsing strategies.
Ph.D. Thesis.
Universityof Connecticut.Gluck, M. A., & Corter, J. E. (1985).Information, uncertainty, and the utility of categories.Proceedings, Seventh Annual Conference of theCognitive Science Society.
Lawrence ErlbaumAssociates.Hindle, D. (1983).
User manual for Fidditch(7590-142).
Naval Research Laboratory.Hindle, D. (1990).
Noun classification frompredicate-argument structures.
Proceedings, 28thAnnual Meeting of the Association forComputational Linguists.
University of Pittsburgh.Association for Computational Linguists.Hindle, D., & Rooth, M. (1991).
Structuralambiguity and lexical relations.
Proceedings, 29thAnnual Meeting of the Association forComputational Linguists.
University of California,Berkeley.
Association for Computational Linguists.Hobbs, J.
(1986).
Resolving Pronoun References.In B. J. Grosz, K. Sparck Jones, & B. L.
Webber(Eds.
), Readings in Natural Language Processing (pp.339-352).
Los Altos, CA: Morgan KaufmannPublishers, Inc.Ingria, R., & Stallard, D. (1989).
A computationalmechanism for pronominal reference.
Proceedings,27th Annual Meeting of the Association forComputational Linguistics.
Vancouver.Kimball, J.
(1973).
Seven principles of surfacestructure parsing in natural anguage.
Cognition, 2,15-47.Lappin, S., & McCord, M. (1990).
A syntacticfilter on pronominal anaphora for slot grammar.Proceedings, 28th Annual Meeting of the Associationfor Computational Linguistics.
University ofPittsburgh.
Association for ComputationalLinguistics.Lehnert, W. (1990).
Symbolic/SubsymbolicSentence Analysis: Exploiting the Best of TwoWorlds.
In J. Bamden, & J. Pollack (Eds.
), Advancesin Connectionist and Neural Computation Theory.Norwood, NJ: Ablex Publishers.Lehnert, W., Cardie, C., Fisher, D., Riloff, E., &Williams, R. (1991).University of Massachusetts:Description of the CIRCUS System as Used forMUC-3.
Proceedings, Third Message UnderstandingConference (MUC-3).
San Diego, CA.
MorganKaufmann Publishers.Ravin, Y.
(1990).
Disambignating and interpretingverb definitions.
Proceedings, 28th Annual Meetingof the Association for Computational Linguists.University of Pittsburgh.
Association forComputational Linguists.Schank, R., & Riesbeck, C. (1981).
InsideComputer Understanding: Five Programs PlusMiniatures.
Hillsdale, NJ: Lawrence Erlbaum.Sundheim, B. M. (May,1991).
Overview of theThird Message Understanding Evaluation andConference.
Proceedings,Third Message Understand-ing Conference (MUC-3).
San Diego, CA.
MorganKanfmann Publishers.Taraban, R., & McClelland, J. L. (1988).Constituent attachment and thematic role assignmentin sentence processing: influences of content-basedexpectations.
Journal of Memory and Language, 27,597-632.Whittemore, G., Ferrara, K., & Brunner, H.(1990).
Empirical study of predictive powers ofsimple attachment schemes for post-modifierprepositional phrases.
Proceedings, 28th AnnualMeeting of the Association for ComputationalLinguistics.
University of Pittsburgh.
Association forComputational Linguistics.223
