Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 522?530,Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational LinguisticsNegation Focus Identification with Contextual Discourse InformationBowei Zou        Qiaoming Zhu       Guodong Zhou*Natural Language Processing Lab, School of Computer Science and TechnologySoochow University, Suzhou, 215006, Chinazoubowei@gmail.com, {qmzhu, gdzhou}@suda.edu.cnAbstractNegative expressions are common in naturallanguage text and play a critical role in in-formation extraction.
However, the perfor-mances of current systems are far from satis-faction, largely due to its focus on intra-sentence information and its failure to con-sider inter-sentence information.
In this paper,we propose a graph model to enrich intra-sentence features with inter-sentence featuresfrom both lexical and topic perspectives.Evaluation on the *SEM 2012 shared taskcorpus indicates the usefulness of contextualdiscourse information in negation focus iden-tification and justifies the effectiveness of ourgraph model in capturing such global infor-mation.
*1 IntroductionNegation is a grammatical category which com-prises various kinds of devices to reverse thetruth value of a proposition (Morante andSporleder, 2012).
For example, sentence (1)could be interpreted as it is not the case that hestopped.
(1) He didn't stop.Negation expressions are common in naturallanguage text.
According to the statistics on bio-medical literature genre (Vincze et al, 2008),19.44% of sentences contain negative expres-sions.
The percentage rises to 22.5% on ConanDoyle stories (Morante and Daelemans, 2012).
Itis interesting that a negative sentence may haveboth negative and positive meanings.
For exam-ple, sentence (2) could be interpreted as Hestopped, but not until he got to Jackson Holewith positive part he stopped and negative partuntil he got to Jackson Hole.
Moreover, a nega-* Corresponding authortive expression normally interacts with somespecial part in the sentence, referred as negationfocus in linguistics.
Formally, negation focus isdefined as the special part in the sentence, whichis most prominently or explicitly negated by anegative expression.
Hereafter, we denote nega-tive expression in boldface and negation focusunderlined.
(2) He didn't stop until he got to Jackson Hole.While people tend to employ stress or intona-tion in speech to emphasize negation focus andthus it is easy to identify negation focus inspeech corpora, such stress or intonation infor-mation often misses in the dominating text cor-pora.
This poses serious challenges on negationfocus identification.
Current studies (e.g., Blancoand Moldovan, 2011; Rosenberg and Bergler,2012) sort to various kinds of intra-sentence in-formation, such as lexical features, syntactic fea-tures, semantic role features and so on, ignoringless-obvious inter-sentence information.
Thislargely defers the performance of negation focusidentification and its wide applications, sincesuch contextual discourse information plays acritical role on negation focus identification.Take following sentence as an example.
(3) Helen didn?t allow her youngest son toplay the violin.In sentence (3), there are several scenarios onidentification of negation focus, with regard tonegation expression n?t, given different contexts:Scenario A: Given sentence But her husband didas next sentence, the negation focus should beHelen, yielding interpretation the person whodidn?t allow the youngest son to play the violin isHelen but not her husband.Scenario B: Given sentence She thought that hedidn?t have the artistic talent like her eldest sonas next sentence, the negation focus should bethe youngest son, yielding interpretation Helen522thought that her eldest son had the talent to playthe violin, but the youngest son didn?t.Scenario C: Given sentence Because of herneighbors?
protests as previous sentence, the ne-gation focus should be play the violin, yieldinginterpretation Helen didn?t allow her youngestson to play the violin, but it didn?t show whetherhe was allowed to do other things.In this paper, to well accommodate such con-textual discourse information in negation focusidentification, we propose a graph model to en-rich normal intra-sentence features with variouskinds of inter-sentence features from both lexicaland topic perspectives.
Besides, the standardPageRank algorithm is employed to optimize thegraph model.
Evaluation on the *SEM 2012shared task corpus (Morante and Blanco, 2012)justifies our approach over several strong base-lines.The rest of this paper is organized as follows.Section 2 overviews the related work.
Section 3presents several strong baselines on negation fo-cus identification with only intra-sentence fea-tures.
Section 4 introduces our topic-drivenword-based graph model with contextual dis-course information.
Section 5 reports the exper-imental results and analysis.
Finally, we con-clude our work in Section 6.2 Related WorkEarlier studies of negation were almost in lin-guistics (e.g.
Horn, 1989; van der Wouden,1997), and there were only a few in natural lan-guage processing with focus on negation recog-nition in the biomedical domain.
For example,Chapman et al (2001) developed a rule-basednegation recognition system, NegEx, to deter-mine whether a finding mentioned within narra-tive medical reports is present or absent.
Sincethe release of the BioScope corpus (Vincze et al,2008), a freely available resource consisting ofmedical and biological texts, machine learningapproaches begin to dominate the research onnegation recognition (e.g.
Morante et al, 2008;Li et al, 2010).Generally, negation recognition includes threesubtasks: cue detection, which detects and identi-fies possible negative expressions in a sentence,scope resolution, which determines the grammat-ical scope in a sentence affected by a negativeexpression, and focus identification, which iden-tifies the constituent in a sentence most promi-nently or explicitly negated by a negative expres-sion.
This paper concentrates on the third subtask,negation focus identification.Due to the increasing demand on deep under-standing of natural language text, negationrecognition has been drawing more and moreattention in recent years, with a series of sharedtasks and workshops, however, with focus on cuedetection and scope resolution, such as the Bi-oNLP 2009 shared task for negative event detec-tion (Kim et al, 2009) and the ACL 2010 Work-shop for scope resolution of negation and specu-lation (Morante and Sporleder, 2010), followedby a special issue of Computational Linguistics(Morante and Sporleder, 2012) for modality andnegation.The research on negation focus identificationwas pioneered by Blanco and Moldovan (2011),who investigated the negation phenomenon insemantic relations and proposed a supervisedlearning approach to identify the focus of a nega-tion expression.
However, although Morante andBlanco (2012) proposed negation focus identifi-cation as one of the *SEM?2012 shared tasks,only one team (Rosenberg and Bergler, 2012) 1participated in this task.
They identified negationfocus using three kinds of heuristics andachieved 58.40 in F1-measure.
This indicatesgreat expectation in negation focus identification.The key problem in current research on nega-tion focus identification is its focus on intra-sentence information and large ignorance of in-ter-sentence information, which plays a criticalrole in the success of negation focus identifica-tion.
For example, Ding (2011) made a qualita-tive analysis on implied negations in conversa-tion and attempted to determine whether a sen-tence was negated by context information, fromthe linguistic perspective.
Moreover, a negationfocus is always associated with authors?
intentionin article.
This indicates the great challenges innegation focus identification.3 BaselinesNegation focus identification in *SEM?2012shared tasks is restricted to verbal negations an-notated with MNEG in PropBank, with only theconstituent belonging to a semantic role selectedas negation focus.
Normally, a verbal negationexpression (not or n?t) is grammatically associat-ed with its corresponding verb (e.g., He didn?tstop).
For details on annotation guidelines and1 In *SEM?2013, the shared task is changed with focus on"Semantic Textual Similarity".523examples for verbal negations, please refer toBlanco and Moldovan (2011).For comparison, we choose the state-of-the-artsystem described in Blanco and Moldovan(2011), which employed various kinds of syntac-tic features and semantic role features, as one ofour baselines.
Since this system adopted C4.5 fortraining, we name it as BaselineC4.5.
In order toprovide a stronger baseline, besides those fea-tures adopted in BaselineC4.5, we added more re-fined intra-sentence features and adopted rankingSupport Vector Machine (SVM) model for train-ing.
We name it as BaselineSVM.Following is a list of features adopted in thetwo baselines, for both BaselineC4.5 and Base-lineSVM,?
Basic features: first token and its part-of-speech (POS) tag of the focus candidate; thenumber of tokens in the focus candidate;relative position of the focus candidateamong all the roles present in the sentence;negated verb and its POS tag of the negativeexpression;?
Syntactic features: the sequence of wordsfrom the beginning of the governing VP tothe negated verb; the sequence of POS tagsfrom the beginning of the governing VP tothe negated verb; whether the governing VPcontains a CC; whether the governing VPcontains a RB.?
Semantic features: the syntactic label of se-mantic role A1; whether A1 contains POStag DT, JJ, PRP, CD, RB, VB, and WP, asdefined in Blanco and Moldovan (2011);whether A1 contains token any, anybody, an-ymore, anyone, anything, anytime, anywhere,certain, enough, full, many, much, other,some, specifics, too, and until, as defined inBlanco and Moldovan (2011); the syntacticlabel of the first semantic role in the sentence;the semantic label of the last semantic role inthe sentence; the thematic role forA0/A1/A2/A3/A4 of the negated predicate.and for BaselineSVM only,?
Basic features: the named entity and its typein the focus candidate; relative position of thefocus candidate to the negative expression(before or after).?
Syntactic features: the dependency path andits depth from the focus candidate to the neg-ative expression; the constituent path and itsdepth from the focus candidate to the nega-tive expression;4 Exploring Contextual Discourse In-formation for Negation Focus Identi-ficationWhile some of negation focuses could be identi-fied by only intra-sentence information, othersmust be identified by contextual discourse in-formation.
Section 1 illustrates the necessity ofsuch contextual discourse information in nega-tion focus identification by giving three scenariosof different discourse contexts for negation ex-pression n?t in sentence (3).For better illustration of the importance ofcontextual discourse information, Table 1 showsthe statistics of intra- and inter-sentence infor-mation necessary for manual negation focusidentification with 100 instances randomly ex-tracted from the held-out dataset of *SEM'2012shared task corpus.
It shows that only 17 instanc-es can be identified by intra-sentence information.It is surprising that inter-sentence information isindispensable in 77 instances, among which 42instances need only inter-sentence informationand 35 instances need both intra- and inter-sentence information.
This indicates the greatimportance of contextual discourse informationon negation focus identification.
It is also inter-esting to note 6 instances are hard to determineeven given both intra- and inter-sentence infor-mation.Info Number#Intra-Sentence Only 17#Inter-Sentence Only 42#Both 35#Hard to Identify 6(Note: "Hard to Identify" means that it is hard for ahuman being to identify the negation focus evengiven both intra- and inter-sentence information.
)Table 1.
Statistics of intra- and inter-sentenceinformation on negation focus identification.Statistically, we find that negation focus is al-ways related with what authors repeatedly statesin discourse context.
This explains why contex-tual discourse information could help identifynegation focus.
While inter-sentence informationprovides the global characteristics from the dis-course context perspective and intra-sentenceinformation provides the local features from lex-ical, syntactic and semantic perspectives, bothhave their own contributions on negation focusidentification.In this paper, we first propose a graph modelto gauge the importance of contextual discourse524information.
Then, we incorporate both intra-and inter-sentence features into a machine learn-ing-based framework for negation focus identifi-cation.4.1 Graph ModelGraph models have been proven successful inmany NLP applications, especially in represent-ing the link relationships between words or sen-tences (Wan and Yang, 2008; Li et al, 2009).Generally, such models could construct a graphto compute the relevance between documenttheme and words.In this paper, we propose a graph model torepresent the contextual discourse informationfrom both lexical and topic perspectives.
In par-ticular, a word-based graph model is proposed torepresent the explicit relatedness among words ina discourse from the lexical perspective, while atopic-driven word-based model is proposed toenrich the implicit relatedness between words, byadding one more layer to the word-based graphmodel in representing the global topic distribu-tion of the whole dataset.
Besides, the PageRankalgorithm (Page et al, 1998) is adopted to opti-mize the graph model.Word-based Graph Model:A word-based graph model can be defined asGword (W, E), where W={wi} is the set of words inone document and E={eij|wi, wj ?W} is the set ofdirected edges between these words, as shown inFigure 1.Figure 1.
Word-based graph model.In the word-based graph model, word node wiis weighted to represent the correlation of theword with authors?
intention.
Since such correla-tion is more from the semantic perspective thanthe grammatical perspective, only content wordsare considered in our graph model, ignoringfunctional words (e.g., the, to,?).
Especially, thecontent words limited to those with part-of-speech tags of JJ, NN, PRP, and VB.
For sim-plicity, the weight of word node wi is initializedto 1.In addition, directed edge eij is weighted torepresent the relatedness between word wi andword wj in a document with transition probabilityP(j|i) from i to j, which is normalized as follows:???|??
?
??????,????
??????,????
(1)where k represents the nodes in discourse, andSim(wi,wj) denotes the similarity between wi andwj.
In this paper, two kinds of information areused to calculate the similarity between words.One is word co-occurrence (if word wi and wordwj occur in the same sentence or in the adjacentsentences, Sim(wi,wj) increases 1), and the otheris WordNet (Miller, 1995) based similarity.Please note that Sim(wi,wi) = 0 to avoid self-transition, and Sim(wi,wj) and Sim(wj,wi) may notbe equal.Finally, the weights of word nodes are calcu-lated using the PageRank algorithm as follows:????????????
?
1??????????????
?
?
?
????????????
????
???|??
?
?1 ?
??
(2)where d is the damping factor as in the PageRankalgorithm.Topic-driven Word-based Graph ModelWhile the above word-based graph model canwell capture the relatedness between contentwords, it can only partially model the focus of anegation expression since negation focus is moredirectly related with topic than content.
In orderto reduce the gap, we propose a topic-drivenword-based model by adding one more layer torefine the word-based graph model over theglobal topic distribution, as shown in Figure 2.Figure 2.
Topic-driven word-based graph model.525Here, the topics are extracted from all the doc-uments in the *SEM 2012 shared task using theLDA Gibbs Sampling algorithm (Griffiths, 2002).In the topic-driven word-based graph model, thefirst layer denotes the relatedness among contentwords as captured in the above word-based graphmodel, and the second layer denotes the topicdistribution, with the dashed lines between thesetwo layers indicating the word-topic model re-turn by LDA.Formally, the topic-driven word-based two-layer graph is defined as Gtopic (W, T, Ew, Et),where W={wi} is the set of words in one docu-ment and T={ti} is the set of topics in all docu-ments; Ew={ewij|wi, wj ?W} is the set of directededges between words and Et ={etij|wi?W, tj ?T}is the set of undirected edges between words andtopics; transition probability Pw(j|i) of ewij is de-fined as the same as P(j|i) of the word-basedgraph model.
Besides, transition probability Pt(i,m) of etij in the word-topic model is defined as:???
?, ??
?
??????,????
??????,????
(3)where Rel(wi, tm) is the weight of word wi in top-ic tm calculated by the LDA Gibbs Sampling al-gorithm.
On the basis, the transition probabilityPw (j|i) of ewij is updated by calculating as fol-lowing:?????|??
?
?
?
????|??
?
?1 ?
??
?
????,???????,???
????,???????,???
(4)where k represents all topics linked to both wordwi and word wj, and ??
[0,1] is the coefficientcontrolling the relative contributions from thelexical information in current document and thetopic information in all documents.Finally, the weights of word nodes are calcu-lated using the PageRank algorithm as follows:????????????
?
1??????????????
?
?
?
????????????
????
?????|??
?
?1 ?
??
(5)where d is the damping factor as in the PageRankalgorithm.4.2 Negation Focus Identification viaGraph ModelGiven the graph models and the PageRank opti-mization algorithm discussed above, four kindsof contextual discourse information are extractedas inter-sentence features (Table 2).In particular, the total weight and the maxweight of words in the focus candidate are calcu-lated as follows:???????????
?
?
?????????????????
(6)?????????
?
max?
????????????????
(7)where i represents the content words in the focuscandidate.
These two kinds of weights focus ondifferent aspects about the focus candidate withthe former on the contribution of content words,which is more beneficial for a long focus candi-date, and the latter biased towards the focus can-didate which contains some critical word in adiscourse.No Feature1 Total weight of words in the focus candi-date using the co-occurrence similarity.2 Max weight of words in the focus candi-date using the co-occurrence similarity.3 Total weight of words in the focus candi-date using the WordNet similarity.4 Max weight of words in the focus candi-date using the WordNet similarity.Table 2.
Inter-sentence features extracted fromgraph model.For evaluating the contribution of contextualdiscourse information on negation focus identifi-cation directly, we incorporate the four inter-sentence features from the topic-driven word-based graph model into a negation focus identifi-er.5 ExperimentationIn this section, we describe experimental settingsand systematically evaluate our negation focusidentification approach with focus on exploringthe effectiveness of contextual discourse infor-mation.5.1 Experimental SettingsDatasetIn all our experiments, we employ the*SEM'2012 shared task corpus (Morante andBlanco, 2012)2 .
As a freely downloadable re-source, the *SEM shared task corpus is annotatedon top of PropBank, which uses the WSJ sectionof the Penn TreeBank.
In particular, negationfocus annotation on this corpus is restricted toverbal negations (with corresponding mark2 http://www.clips.ua.ac.be/sem2012-st-neg/526MNEG in PropBank).
On 50% of the corpus an-notated by two annotators, the inter-annotatoragreement was 0.72 (Blanco and Moldovan,2011).
Along with negation focus annotation,this corpus also contains other annotations, suchas POS tag, named entity, chunk, constituent tree,dependency tree, and semantic role.In total, this corpus provides 3,544 instancesof negation focus annotations.
For fair compari-son, we adopt the same partition as *SEM?2012shared task in all our experiments, i.e., with2,302 for training, 530 for development, and 712for testing.
Although for each instance, the cor-pus only provides the current sentence, the pre-vious and next sentences as its context, we sort tothe Penn TreeBank3 to obtain the correspondingdocument as its discourse context.Evaluation MetricsSame as the *SEM'2012 shared task, the evalua-tion is made using precision, recall, and F1-score.Especially, a true positive (TP) requires an exactmatch for the negation focus, a false positive (FP)occurs when a system predicts a non-existingnegation focus, and a false negative (FN) occurswhen the gold annotations specify a negationfocus but the system makes no prediction.
Foreach instance, the predicted focus is consideredcorrect if it is a complete match with a gold an-notation.Beside, to show whether an improvement issignificant, we conducted significance testingusing z-test, as described in Blanco and Moldo-van (2011).ToolkitsIn our experiments, we report not only the de-fault performance with gold additional annotatedfeatures provided by the *SEM'2012 shared taskcorpus and the Penn TreeBank, but also the per-formance with various kinds of features extractedautomatically, using following toolkits:?
Syntactic Parser: We employ the StanfordParser4 (Klein and Manning, 2003; De Marn-effe et al, 2006) for tokenization, constituentand dependency parsing.?
Named Entity Recognizer: We employ theStanford NER5 (Finkel et al, 2005) to obtainnamed entities.3 http://www.cis.upenn.edu/~treebank/4 http://nlp.stanford.edu/software/lex-parser.shtml5 http://nlp.stanford.edu/ner/?
Semantic Role Labeler: We employ the se-mantic role labeler, as described in Punyaka-nok et al(2008).?
Topic Modeler: For estimating transitionprobability Pt(i,m), we employGibbsLDA++6, an LDA model using GibbsSampling technique for parameter estimationand inference.?
Classifier: We employ SVMLight 7 with defaultparameters as our classifier.5.2 Experimental ResultsWith Only Intra-sentence InformationTable 3 shows the performance of the two base-lines, the decision tree-based classifier as inBlanco and Moldovan (2011) and our rankingSVM-based classifier.
It shows that our rankingSVM-based baseline slightly improves the F1-measure by 2.52% over the decision tree-basedbaseline, largely due to the incorporation of morerefined features.System P(%) R(%) F1BaselineC4.5 66.73 49.93 57.12BaselineSVM 60.22 59.07 59.64Table 3.
Performance of baselines with onlyintra-sentence information.Error analysis of the ranking SVM-basedbaseline on development data shows that 72% ofthem are caused by the ignorance of inter-sentence information.
For example, among the42 instances listed in the category of ?#Inter-Sentence Only?
in Table 1, only 7 instances canbe identified correctly by the ranking SVM-based classifier.
With about 4 focus candidates inone sentence on average, this percentage is evenlower than random.With Only Inter-sentence InformationFor exploring the usefulness of pure contextualdiscourse information in negation focus identifi-cation, we only employ inter-sentence featuresinto ranking SVM-based classifier.
First of all,we estimate two parameters for our topic-drivenword-based graph model: topic number T fortopic model and coefficient ?
between Pw(j|i) andPt (i,m) in Formula 4.Given the LDA Gibbs Sampling model withparameters ?
= 50/T and ?
= 0.1, we vary T from20 to 100 with an interval of 10 to find the opti-6 http://gibbslda.sourceforge.net/7 http://svmlight.joachims.org527mal T. Figure 3 shows the experiment results ofvarying T (with ?
= 0.5) on development data.
Itshows that the best performance is achievedwhen T = 50 with 51.11 in F1).
Therefore, we setT as 50 in our following experiments.Figure 3.
Performance with varying T.For parameter ?, a trade-off between the tran-sition probability Pw(j|i) (word to word) and thetransition probability Pt (i,m) (word and topic) toupdate P?w(j|i), we vary it from 0 to 1 with aninterval of 0.1.
Figure 4 shows the experimentresults of varying ?
(with T=50) on developmentdata.
It shows that the best performance isachieved when ?
= 0.6, which are adopted here-after in all our experiments.
This indicates thatdirect lexical information in current documentcontributes more than indirect topic informationin all documents on negation focus identification.It also shows that direct lexical information incurrent document and indirect topic informationin all documents are much complementary onnegation focus identification.Figure 4.
Performance with varying ?.System P(%) R(%) F1using word-based graphmodel  45.62 42.02 43.75using topic-driven word-based graph model 54.59 50.76 52.61Table 4.
Performance with only inter-sentenceinformation.Table 4 shows the performance of negationfocus identification with only inter-sentence fea-tures.
It also shows that the system with inter-sentence features from the topic-driven word-based graph model significantly improves theF1-measure by 8.86 over the system with inter-sentence features from the word-based graphmodel, largely due to the usefulness of topic in-formation.In comparison with Table 3, it shows that thesystem with only intra-sentence features achievesbetter performance than the one with only inter-sentence features (59.64 vs. 52.61 in F1-measure).With both Intra- and Inter-sentence In-formationTable 5 shows that enriching intra-sentence fea-tures with inter-sentence features significantly(p<0.01) improve the performance by 9.85 in F1-measure than the better baseline.
This indicatesthe usefulness of such contextual discourse in-formation and the effectiveness of our topic-driven word-based graph model in negation fo-cus identification.System P(%) R(%) F1BaselineC4.5 with intrafeat.
only 66.73 49.93 57.12BaselineSVM with intrafeat.
only 60.22 59.07 59.64Ours with Both feat.using word-based GM 64.93 62.47 63.68Ours  with  Both   feat.using    topic-drivenword-based GM71.67 67.43 69.49(Note: ?feat.?
denotes features; ?GM?
denotes graph model.
)Table 5.
Performance comparison of systems onnegation focus identification.System P(%) R(%) F1BaselineC4.5 with intrafeat.
only (auto) 60.94 44.62 51.52BaselineSVM with intrafeat.
Only (auto) 53.81 51.67 52.72Ours with Both feat.using word-based GM(auto)58.77 57.19 57.97Ours  with  Both   feat.using    topic-drivenword-based GM (auto)66.74 64.53 65.62Table 6.
Performance comparison of systems onnegation focus identification with automaticallyextracted features.528Besides, Table 6 shows the performance ofour best system with all features automaticallyextracted using the toolkits as described in Sec-tion 5.1.
Compared with our best system employ-ing gold additional annotated features (the lastline in Table 5), the homologous system withautomatically extracted features (the last line inTable 6) only decrease of less than 4 in F1-measure.
This demonstrates the achievability ofour approach.In comparison with the best-reported perfor-mance on the *SEM?2012 shared task (Rosen-berg and Bergler, 2012), our system performsbetter by about 11 in F-measure.5.3 DiscussionWhile this paper verifies the usefulness of con-textual discourse information on negation focusidentification, the performance with only inter-sentence features is still weaker than that withonly intra-sentence features.
There are two mainreasons.
On the one hand, the former employs anunsupervised approach without prior knowledgefor training.
On the other hand, the usefulness ofinter-sentence features depends on the assump-tion that a negation focus relates to the meaningof which is most relevant to authors?
intention ina discourse.
If there lacks relevant information ina discourse context, negation focus will becomedifficult to be identified only by inter-sentencefeatures.Error analysis also shows that some of the ne-gation focuses are very difficult to be identified,even for a human being.
Consider the sentence (3)in Section 1, if given sentence because of herneighbors' protests, but her husband doesn?tthink so as its following context, both Helen andto play the violin can become the negation focus.Moreover, the inter-annotator agreement in thefirst round of negation focus annotation can onlyreach 0.72 (Blanco and Moldovan, 2011).
Thisindicates inherent difficulty in negation focusidentification.6 ConclusionIn this paper, we propose a graph model to enrichintra-sentence features with inter-sentence fea-tures from both lexical and topic perspectives.
Inthis graph model, the relatedness between wordsis calculated by word co-occurrence, WordNet-based similarity, and topic-driven similarity.Evaluation on the *SEM 2012 shared task corpusindicates the usefulness of contextual discourseinformation on negation focus identification andour graph model in capturing such global infor-mation.In future work, we will focus on exploringmore contextual discourse information via thegraph model and better ways of integrating intra-and inter-sentence information on negation focusidentification.AcknowledgmentsThis research is supported by the National Natu-ral Science Foundation of China, No.61272260,No.61331011, No.61273320, the Natural ScienceFoundation of Jiangsu Province, No.
BK2011282,the Major Project of College Natural ScienceFoundation of Jiangsu Province,No.11KIJ520003, and the Graduates Project ofScience and Innovation, No.
CXZZ12_0818.
Theauthors would like to thank the anonymous re-viewers for their insightful comments and sug-gestions.
Our sincere thanks are also extended toDr.
Zhongqing Wang for his valuable discus-sions during this study.ReferenceEduardo Blanco and Dan Moldovan.
2011.
SemanticRepresentation of Negation Using Focus Detection.In Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics, pages581-589, Portland, Oregon, June 19-24, 2011.Wendy W. Chapman, Will Bridewell, Paul Hanbury,Gregory F. Cooper, and Bruce G. Buchanan.
2001.A simple algorithm for identifying negated find-ings and diseases in discharge summaries.
Journalof Biomedical Informatics, 34:301-310.Marie-Catherine De Marneffe, Bill MacCartney andChristopher D. Manning.
2006.
Generating TypedDependency Parses from Phrase Structure Parses.In Proceedings of  LREC?2006.Yun Ding.
2011.
Implied Negation in Discourse.Journal of Theory and Practice in Language Stud-ies, 1(1): 44-51, Jan 2011.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local infor-mation into information extraction systems bygibbs sampling.
In Proceedings of the 43rd AnnualMeeting on Association for Computational Lin-guistics, pages 363-370, Stroudsburg, PA, USA.Tom Griffiths.
2002.
Gibbs sampling in the generativemodel of Latent Dirichlet Allocation.
Tech.
rep.,Stanford University.Laurence R Horn.
1989.
A Natural History of Nega-tion.
Chicago University Press, Chicago, IL.529Fangtao Li, Yang Tang, Minlie Huang, and XiaoyanZhu.
2009.
Answering Opinion Questions withRandom Walks on Graphs.
In Proceedings of the47th Annual Meeting of the ACL and the 4thIJCNLP of the AFNLP, pages 737-745, Suntec,Singapore, 2-7 Aug 2009.Junhui Li, Guodong Zhou, Hongling Wang, and Qi-aoming Zhu.
2010.
Learning the Scope of Negationvia Shallow Semantic Parsing.
In Proceedings ofthe 23rd International Conference on Computa-tional Linguistics.
Stroudsburg, PA, USA: Associa-tion for Computational Linguistics, 671-679.Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo,Yoshinobu Kano, and Jun'ichi Tsujii.
2009.
Over-view of BioNLP'09 Shared Task on Event Extrac-tion.
In Proceedings of the BioNLP'2009 WorkshopCompanion Volume for Shared Task.
Stroudsburg,PA, USA: Association for Computational Linguis-tics, 1-9.Dan Klein and Christopher D. Manning.
2003.
Accu-rate Unlexicalized Parsing.
In Proceedings of the41st Meeting of the Association for ComputationalLinguistics, pages 423-430.George A. Miller.
1995.
Wordnet: a lexical databasefor english.
Commun.
ACM, 38(11):39-41.Roser Morante, Anthony Liekens and Walter Daele-mans.
2008.
Learning the Scope of Negation in Bi-omedical Texts.
In Proceedings of the 2008 Con-ference on Empirical Methods in Natural Lan-guage Processing, pages 715-724, Honolulu, Oc-tober 2008.Roser Morante and Caroline Sporleder, editors.
2010.In Proceedings of the Workshop on Negation andSpeculation in Natural Language Processing.
Uni-versity of Antwerp, Uppsala, Sweden.Roser Morante and Eduardo Blanco.
2012.
*SEM2012 Shared Task: Resolving the Scope and Focusof Negation.
In Proceedings of the First Joint Con-ference on Lexical and Computational Semantics(*SEM), pages 265-274, Montreal, Canada, June 7-8, 2012.Roser Morante and Caroline Sporleder.
2012.
Modali-ty and Negation: An Introduction to the Special Is-sue.
Computational Linguistics, 2012, 38(2): 223-260.Roser Morante and Walter Daelemans.
2012.
ConanDoyle-neg: Annotation of negation cues and theirscope in Conan Doyle stories.
In Proceedings ofLREC 2012, Istambul.Lawrence Page, Sergey Brin, Rajeev Motwani, andTerry Winograd.
1998.
The pagerank citation rank-ing: Bringing order to the web.
Technical report,Stanford University.Vasin Punyakanok, Dan Roth, and Wen-tau Yih.
2008.The importance of syntactic parsing and inferencein semantic role labeling.
Computational Linguis-tics, 34(2):257-287, June.Sabine Rosenberg and Sabine Bergler.
2012.
UCon-cordia: CLaC Negation Focus Detection at *Sem2012.
In Proceedings of the First Joint Conferenceon Lexical and Computational Semantics (*SEM),pages 294-300, Montreal, Canada, June 7-8, 2012.Ton van der Wouden.
1997.
Negative Contexts: Col-location, Polarity, and Multiple Negation.Routledge, London.Veronika Vincze, Gy?rgy Szarvas, Rich?rd Farkas,Gy?rgy M?ra, and J?nos Csirik.
2008.
The Bio-Scope corpus: biomedical texts annotated for un-certainty,negation and their scopes.
BMC Bioin-formatics, 9(Suppl 11):S9.Xiaojun Wan and Jianwu Yang.
2008.
Multi-document summarization using cluster-based linkanalysis.
In Proceedings of the 31st annual inter-national ACM SIGIR conference on Research anddevelopment in information retrieval, pages 299-306.530
