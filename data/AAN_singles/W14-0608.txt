Proceedings of the 8th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH) @ EACL 2014, pages 56?61,Gothenburg, Sweden, April 26 2014.c?2014 Association for Computational LinguisticsAutomated Error Detection in Digitized Cultural Heritage DocumentsKata G?aborINRIA & Universit?e Paris 7Domaine de Voluceau - BP 10578153 Le Chesnay CedexFRANCEkata.gabor@inria.frBeno?
?t SagotINRIA & Universit?e Paris 7Domaine de Voluceau - BP 10578153 Le Chesnay CedexFRANCEbenoit.sagot@inria.frAbstractThe work reported in this paper aimsat performance optimization in the di-gitization of documents pertaining tothe cultural heritage domain.
A hybridmethod is proposed, combining statisticalclassification algorithms and linguisticknowledge to automatize post-OCR errordetection and correction.
The currentpaper deals with the integration of lin-guistic modules and their impact on errordetection.1 IntroductionProviding wider access to national cultural her-itage by massive digitization confronts the actorsof the field with a set of new challenges.
Stateof the art optical character recognition (OCR)software currently achieve an error rate of around1 to 10% depending on the age and the layoutof the text.
While this quality may be adequatefor indexing, documents intended for readingneed to meet higher standards.
A reduction ofthe error rate by a factor of 10 to 100 becomesnecessary for the diffusion of digitized booksand journals through emerging technologies suchas e-books.
Our paper deals with the automaticpost-processing of digitized documents with theaim of reducing the OCR error rate by usingcontextual information and linguistic processing,by and large absent from current OCR engines.
Inthe current stage of the project, we are focusingon French texts from the archives of the FrenchNational Library (Biblioth`eque Nationale deFrance) covering the period from 1646 to 1990.We adopted a hybrid approach, making useof both statistical classification techniques andlinguistically motivated modules to detect OCRerrors and generate correction candidates.
Thetechnology is based on a symbolic linguistic pre-processing, followed by a statistical module whichadpots the noisy channel model (Shannon, 1948).Symbolic methods for error correction allow totarget specific phenomena with a high precision,but they typically strongly rely on presumptionsabout the nature of errors encountered.
This draw-back can be overcome by using the noisy channelmodel (Kernighan et al., 1990; Brill and Moore,2000; Kolak and Resnik, 2002; Mays et al., 1991;Tong and Evans, 1996).
However, error models insuch systems work best if they are created frommanually corrected training data, which are notalways available.
Other alternatives to OCR errorcorrection include (weighted) FSTs (Beaufortand Mancas-Thillou, 2007), voting systems usingthe output of different OCR engines (Klein andKope, 2002), textual alignment combined withdictionary lookup (Lund and Ringger, 2009), orheuristic correction methods (Alex et al., 2012).While correction systems rely less and less onpre-existing external dictionaries, a shift can beobserved towards methods that dinamically createlexicons either by exploiting the Web (Cucerzanand Brill, 2004; Strohmaier et al., 2003) or fromthe corpus (Reynaert, 2004).As to linguistically enhanced models, POStagging was succesfully applied to spelling cor-rection (Golding and Schabes, 1996; Schaback,2007).
However, to our knowledge, very littlework has been done to exploit linguistic analysisfor post-OCR correction (Francom and Hulden,2013).
We propose to apply a shallow processingmodule to detect certain types of named entities(NEs), and a POS tagger trained specifically todeal with NE-tagged input.
Our studies aim todemonstrate that linguistic preprocessing canefficiently contribute to reduce the error rate by1) detecting false corrections proposed by the56statistical correction module, 2) detecting OCRerrors which are unlikely to be detected by thestatistical correction module.
We argue thatnamed entity grammars can be adapted to thecorrection task at a low cost and they allow totarget specific types of errors with a very highprecision.In what follows, we present the global architec-ture of the post-OCR correction system (2), thenamed entity recognition module (3), as well asour experiments in named entity-aware POS tag-ging (4).
The predicted impact of the linguisticmodules is illustrated in section 5.
Finally, wepresent ongoing work and the conclusion (6).2 System ArchitectureOur OCR error detection and correction systemuses a hybrid methodology with a symbolic mod-ule for linguistic preprocessing, a POS tagger, fol-lowed by statistical decoding and correction mod-ules.
The SxPipe toolchain (Sagot and Boullier,2008) is used for shallow processing tasks (to-kenisation, sentence segmentation, named entityrecognition).
The NE-tagged text is input to POStagging with MElt-h, a hybrid version of the MElttagger (Denis and Sagot, 2010; Denis and Sagot,2012).
MELT-h can take both NE tagged texts andraw text as input.The decoding phase is based on the noisy channelmodel (Shannon, 1948) adapted to spell checking(Kernighan et al., 1990).
In a noisy channel model,given an input string s, we want to find the word wwhich maximizes P (w|s).
Using Bayes theorem,this can be written as:argmax(w)P (s|w) ?
P (w) (1)where P(w) is given by the language model ob-tained from clean corpora.
Both sentence-level(Tong and Evans, 1996; Boswell, 2004) and word-level (Mays et al., 1991) language models can beused.
P (s|w) is given by the error model, repre-sented as a confusion matrix calculated from ourtraining corpus in which OCR output is alignedwith its manually corrected, noiseless equivalent.The post-correction process is summarized in 1.The integration of a symbolic module for NErecognition and the use of part of speech andnamed entity tags constitute a novel aspect in ourmethod.
Moreover, linguistic preprocessing al-lows us to challenge tokenisation decisions priorOCR outputPreprocessing with Sx-Pipe (character level)POS and NE taggingtokenization revisiteddecoding: creationof hypothesis latticecorrection: ranking of the hypothesesFigure 1: Architectureto and during the decoding phase (similarly to Ko-lak (2005)) ; this constitutes a significant featureas OCR errors often boil down to a fusion or splitof tokens.The corpus we use comes from the archives ofthe French National Library and contains 1 500documents (50 000 000 tokens).
This corpus isavailable both as a ?reference corpus?, i.e., in amanually corrected, clean version, and as a ?con-trast corpus?, i.e., a noisy OCR output version.These variants are aligned at the sentence level.3 Named entity tagging3.1 NE recognition methodologyAs a first step in error detection, the OCR out-put is analysed in search of ?irregular?
charac-ter sequences such as named entities.
This pro-cess is implemented with SxPipe (Sagot and Boul-lier, 2008), a freely available1, robust and modu-lar multilingual processing chain for unrestrictedtext.
SxPipe contains modules for named en-tity recognition, tokenization, sentence segmenta-tion, non-deterministic multi-word expression de-tection, spelling correction and lexicon-based pat-terns detection.
The SxPipe chain is fully cus-tomizable with respect to input language, domain,text type and the modules to be used.
Users arealso free to add their own modules to the chain.In accordance with our purposes, we definednamed entities as sequences of characters whichcannot be analysed morphologically or syntacti-cally, yet follow productive patterns.
Such enti-ties do not adhere to regular tokenization patterns1https://gforge.inria.fr/projects/lingwb/57since they often include punctuation marks, usu-ally considered as separators.
As compared to theconsensual use of the term (Maynard et al., 2001;Chinchor, 1998; Sang and Meulder, 2003), ourdefinition covers a wider range of entities, e.g., nu-merals, currency units, dimensions.2The correctannotation of these entities has a double relevancefor our project:?
NE tagging prior to POS tagging helps to im-prove the accuracy of the latter.?
NE tagging allows to detect and, eventu-ally, correct OCR errors which occur insideNEs.
Conversely, it can also contribute to de-tect false correction candidates when the se-quence of characters forming the NE wouldotherwise be assigned a low probability bythe language model.The named entity recognition module is imple-mented in Perl as a series of local grammars.
Localgrammars constitute a simple and powerful tool torecognize open classes of entities (Friburger andMaurel, 2004; Maynard et al., 2002; Bontchevaet al., 2002); we are concerned with time ex-pressions, addresses, currency units, dimensions,chemical formulae and legal IDs.
Named entitygrammars are applied to the raw corpus before to-kenization and segmentation.
Our grammars arerobust in the sense that they inherently recognizeand correct some types of frequent OCR errors inthe input.3SxPipe?s architecture allows to definean OCR-specific correction mode as an input pa-rameter and hence apply robust recognition andcorrection to noisy output, while requiring exactmatching for clean texts.
However, maximizingprecision remains our primary target, as a falsecorrection is more costly than the non-correctionof an eventual error at this stage.
Therefore, ourgrammars are built around unambiguous markers.3.2 Evaluation of NE taggingA manual, application-independent evaluationwas carried out, concentrating primarily on preci-sion for the reasons mentioned in 3.
For four typesof NEs, we collected a sample of 200 sentencesexpected to contain one or more instances of the2Our current experiments do not cover single-word propernames.3E.g., A numerical 0 inside a chemical formula is pre-sumed in most cases to be an erroneous hypothesis for alpha-betical O.given entity category, based on the presence ofcategory-specific markers (lexical units, acronymsetc.)4.
However, chemical formulae were eval-uated directly on sentences extracted from thearchives of the European Patent Office; no filter-ing was needed due to the density of formulae inthese documents.Legal IDs were evaluated on a legal corpus fromthe Publications Office of the European Union,while the rest of the grammars were evaluated us-ing the BNF corpus.Entity Type Precision RecallDATE 0.98 0.97ADDRESS 0.83 0.86LEGAL 0.88 0.82CHEMICAL 0.94 -Table 1: Evaluation of NE grammars4 POS tagging4.1 MEltFRand MElt-hThe following step in the chain is POS taggingusing a named entity-aware version of the MElttagger.
MElt (Denis and Sagot, 2010; Denis andSagot, 2012) is a maximum entropy POS taggerwhich differs from other systems in that it usesboth corpus-based features and a large-coveragelexicon as an external source of information.
ItsFrench version, MElt-FR was trained on the Leffflexicon (Sagot, 2010) and on the French TreeBank(FTB) (Abeill?e et al., 2003).
The training corpususes a tagset consisting of 29 tags.
MEltFRyieldsstate of the art results for French, namely 97.8%accuracy on the test set.In order to integrate MElt into our toolchain,the tagger needed to be trained to read NE-taggedtexts as output by SxPipe.
We thus extendedthe FTB with 332 manually annotated sentences(15 500 tokens) containing real examples for eachtype of NE covered by our version of SxPipe.
Sx-Pipe?s output format was slightly modified to fa-cilitate learning: entities covered by the grammarswere replaced by pseudo-words corresponding totheir category.
The training corpus is the union4Although this sampling is biased towards entities witha certain type of marker, it gives an approximation on therecall, as opposed to simply extracting hits of our grammarsand evaluating only their precision.58of the FTB and the small NE corpus annotatedwith 35 categories (29 POS and 6 named entitycategories).
We used this corpus to train MElt-h,a hybrid tagger compatible with our OCR post-processing toolchain.
MElt-h can tag both rawcorpora (using the 29 POS categories learnt fromthe FTB), and NE-annotated texts (preprocessedwith SxPipe or any other tool, as long as the for-mat is consistent with the output of SxPipe).Training a tagger on a heterogeneous corpus likethe one we used is theoretically challengeable.Therefore, careful attention was paid to evaluat-ing it on both NE-annotated data and on the FTBtest corpus.
The latter result is meant to indi-cate whether there is a decrease in performancecompared to the ?original?
MEltFRtagger, trainedsolely on FTB data.4.2 Evaluation of POS and NE taggingA set of experiments were performed usingdifferent sections of the NE-annotated trainingdata.
First, we cut out 100 sentences at randomand used them as a test corpus.
From the restof the sentences, we created diverse randompartitionings using 50, 100, 150 and all the 232sentences as training data.
We trained MElt-hon each training corpus and evaluated it on thetest section of the FTB as well as on the 100NE-annotated sentences.#sentences Prec on FTB Prec on PACTE-NE0 97.83 ?50 97.82 95.61100 97.80 95.71150 97.78 95.76200 97.78 95.84232 97.75 96.20Table 2: Evaluation of MElt-h on the FTB and onthe NE-annotated corpusThe results confirm that adding NE-annotatedsentences to the training corpus does not decreaseprecision on the FTB itself.
Furthermore, we notethat the results on the NE corpus are slightly infe-rior to the results on the FTB, but the figures sug-gest that the learning curve did not reach a limit forNE-annotated data: adding more NE-annotatedsentences will probably increase precision.5 Expected impact on OCR errorreductionWhile the major impact of named entity taggingand NE-enriched POS tagging is expected to re-sult from their integration into the language model,series of experiments are currently being carriedout to estimate the efficiency of the symbolic cor-rection module and the quantity of the remain-ing OCR errors inside named entities.
A sampleof 500.000 sentences (15.500.000 tokens) was ex-tracted from the BNF corpus to be used for a com-parison and case studies, both in the noisy OCRoutput version and in the editorial quality version.Both types of texts were tagged for NEs with Sx-Pipe, using the ?clean input?
mode (without tol-erance for errors and correction candidates).
Only65% of the recognized NEs are identical, implyingthat 35% of the named entities are very likely tocontain an OCR error.5To investigate further, weapplied the grammars one by one in ?noisy input?mode.
This setting allows to detect certain typesof typical OCR errors, with an efficiency rangingfrom 0 (no tolerance) to 10% i.e., up to this quan-tity of erroneous input can be detected and cor-rectly tagged with certain named entity grammars.Detailed case studies are currently being carriedout to determine the exact precision of the correc-tion module.6 Conclusion and Future WorkWe described an architecture for post-OCR errordetection in documents pertaining to the culturalheritage domain.
Among other characteristics, thespecificity of our model consists in a combina-tion of linguistic analysis and statistical modules,which interact at different stages in the error detec-tion and correction process.
The first experimentscarried out within the project suggest that linguis-tically informed modules can efficiently comple-ment statistical methods for post-OCR error detec-tion.
Our principal future direction is towards theintegration of NE-enriched POS tagging informa-tion into the language models, in order to provide afiner grained categorization and account for thesephenomena.
A series of experiences are plannedto be undertaken, using different combinations oftoken-level information.5In the less frequent case, divergences can also be due toerrors in the editorial quality text.59ReferencesAnne Abeill?e, Lionel Cl?ement, and Franc?ois Toussenel.2003.
Building a treebank for French.
In AnneAbeill?e, editor, Treebanks.
Kluwer, Dordrecht.Bea Alex, Claire Grover, Ewan Klein, and Richard To-bin.
2012.
Digitised historical text: Does it haveto be mediOCRe?
In Proceedings of KONVENS2012 (LThist 2012 workshop), pages 401?409, Vi-enna, Austria.Richard Beaufort and C?eline Mancas-Thillou.
2007.A weighted finite-state framework for correcting er-rors in natural scene OCR.
In Proceedings of theNinth International Conference on Document Anal-ysis and Recognition, pages 889?893, Washington,DC, USA.Kalina Bontcheva, Marin Dimitrov, Diana Maynard,Valentin Tablan, and Hamish Cunningham.
2002.Shallow methods for named entity coreference res-olution.
In Proceedings of the TALN 2002 Confer-ence.Dustin Boswell.
2004.
Language models for spellingcorrection.
CSE, 256.Eric Brill and Robert C. Moore.
2000.
An ImprovedError Model for Noisy Channel Spelling Correction.In Proceedings of the 38th ACL Conference, pages286?293.Nancy Chinchor.
1998.
Muc-7 named entity task def-inition.
In Seventh Message Understanding Confer-ence (MUC-7).Silviu Cucerzan and Eric Brill.
2004.
Spelling correc-tion as an iterative process that exploits the collec-tive knowledge of web users.
In Proceedings of Pro-ceedings of the 2004 Conference on Empirical Meth-ods in Natural Language Processing (EMNLP?04),pages 293?300, Barcelona, Spain.Pascal Denis and Beno?
?t Sagot.
2010.
Exploita-tion d?une ressource lexicale pour la constructiond?un ?etiqueteur morphosyntaxique ?etat-de-l?art dufranc?ais.
In Traitement Automatique des LanguesNaturelles : TALN 2010, Montr?eal, Canada.Pascal Denis and Beno?
?t Sagot.
2012.
Coupling anannotated corpus and a lexicon for state-of-the-artPOS tagging.
Language Resources and Evaluation,46(4):721?736.Jerid Francom and Mans Hulden.
2013.
Diacritic er-ror detection and restoration via part-of-speech tags.In Proceedings of the 6th Language and TechnologyConference.Nathalie Friburger and Denis Maurel.
2004.
Finite-state transducer cascades to extract named entities intexts.
Theoretical Computer Science, 313:94?104.Andrew Golding and Yves Schabes.
1996.
Com-bining trigram-based and feature-based methods forcontext-sensitive spelling correction.
In ACL, pages71?78.Mark Kernighan, Kenneth Church, and William Gale.1990.
A spelling correction program based ona noisy channel model.
In Proceedings of the13th conference on Computational linguistics, pages205?210.Samuel Klein and Miri Kope.
2002.
A voting systemfor automatic OCR correction.
In Proceedings of theWorkshop On Information Retrieval and OCR: FromConverting Content to Grasping Meaning, pages 1?21, Tampere, Finland.Okan Kolak and Philip Resnik.
2002.
OCR errorcorrection using a noisy channel model.
In Pro-ceedings of the Second International Conference onHuman Language Technology Research (HLT?02),pages 257?262, San Diego, USA.Okan Kolak and Philip Resnik.
2005.
OCR post-processing for low density languages.
In Proceed-ings of the HLT-EMNLP Conference, pages 867?874.William Lund and Eric Ringger.
2009.
Improv-ing optical character recognition through efficientmultiple system alignment.
In Proceedings of the9th ACM/IEEE-CS Joint Conference on Digital Li-braries (JCDL?09), pages 231?240, Austin, USA.Diana Maynard, Valentin Tablan, Cristian Ursu,Hamish Cunningham, and Yorick Wilks.
2001.Named entity recognition from diverse text types.
InIn Proceedings of the Recent Advances in NaturalLanguage Processing Conference, pages 257?274.Diana Maynard, Valentin Tablan, Hamish Cunning-ham, Cristian Ursu, Horacio Saggion, KalinaBontcheva, and Yorick Wilks.
2002.
Architecturalelements of language engineering robustness.
Jour-nal of Natural Language Engineering - Special Issueon Robust Methods in Analysis of Natural LanguageData, 8:257?274.Eric Mays, Fred Damerau, and Robert Mercer.
1991.Context based spelling correction.
Information Pro-cessing and Management, 23 (5):517?522.Martin Reynaert.
2004.
Multilingual text inducedspelling correction.
In Proceedings of the Workshopon Multilingual Linguistic Ressources (MLR?04),pages 117?117.Beno?
?t Sagot and Pierre Boullier.
2008.
SxPipe 2: ar-chitecture pour le traitement pr?e-syntaxique de cor-pus bruts.
Traitement Automatique des Langues,49(2):155?188.Beno?
?t Sagot.
2010.
The lefff, a freely availableand large-coverage morphological and syntactic lex-icon for french.
In Proceedings of LREC 2010, LaValette, Malte.Erik Tjong Kim Sang and Fien De Meulder.
2003.
In-troduction to the conll-2003 shared task: Language-independent named entity recognition.
In InProceedings of Computational Natural LanguageLearning, pages 142?147.
ACL Press.60Johannes Schaback.
2007.
Multi-level feature extrac-tion for spelling correction.
In IJCAI Workshop onAnalytics for Noisy Unstructured Text Data, pages79?86.Claude Shannon.
1948.
A Mathematical Theory ofCommunication.
Bell System Technical Journal, 27(3):379?423.Christan Strohmaier, Cristoph Ringlstetter, KlausSchulz, and Stoyan Mihov.
2003.
Lexical post-correction of OCR-results: the web as a dynamicsecondary dictionary?
In Proceedings of the Sev-enth International Conference on Document Anal-ysis and Recognition (ICDAR?03), page 11331137,Edinburgh, Royaume-Uni.Xiang Tong and David Evans.
1996.
A statistical ap-proach to automatic OCR error correction in context.In Proceedings of the Fourth Workshop on Very largeCorpora, pages 88?100.61
