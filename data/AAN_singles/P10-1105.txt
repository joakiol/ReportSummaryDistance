Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1030?1039,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsFinding Cognate Groups using PhylogeniesDavid Hall and Dan KleinComputer Science DivisionUniversity of California, Berkeley{dlwh,klein}@cs.berkeley.eduAbstractA central problem in historical linguisticsis the identification of historically relatedcognate words.
We present a generativephylogenetic model for automatically in-ducing cognate group structure from un-aligned word lists.
Our model representsthe process of transformation and trans-mission from ancestor word to daughterword, as well as the alignment betweenthe words lists of the observed languages.We also present a novel method for sim-plifying complex weighted automata cre-ated during inference to counteract theotherwise exponential growth of messagesizes.
On the task of identifying cognatesin a dataset of Romance words, our modelsignificantly outperforms a baseline ap-proach, increasing accuracy by as much as80%.
Finally, we demonstrate that our au-tomatically induced groups can be used tosuccessfully reconstruct ancestral words.1 IntroductionA crowning achievement of historical linguisticsis the comparative method (Ohala, 1993), whereinlinguists use word similarity to elucidate the hid-den phonological and morphological processeswhich govern historical descent.
The comparativemethod requires reasoning about three importanthidden variables: the overall phylogenetic guidetree among languages, the evolutionary parame-ters of the ambient changes at each branch, andthe cognate group structure that specifies whichwords share common ancestors.All three of these variables interact and informeach other, and so historical linguists often con-sider them jointly.
However, linguists are cur-rently required to make qualitative judgments re-garding the relative likelihood of certain soundchanges, cognate groups, and so on.
Several re-cent statistical methods have been introduced toprovide increased quantitative backing to the com-parative method (Oakes, 2000; Bouchard-Co?te?
etal., 2007; Bouchard-Co?te?
et al, 2009); others havemodeled the spread of language changes and spe-ciation (Ringe et al, 2002; Daume?
III and Camp-bell, 2007; Daume?
III, 2009; Nerbonne, 2010).These automated methods, while providing ro-bustness and scale in the induction of ancestralword forms and evolutionary parameters, assumethat cognate groups are already known.
In thiswork, we address this limitation, presenting amodel in which cognate groups can be discoveredautomatically.Finding cognate groups is not an easy task,because underlying morphological and phonolog-ical changes can obscure relationships betweenwords, especially for distant cognates, where sim-ple string overlap is an inadequate measure of sim-ilarity.
Indeed, a standard string similarity met-ric like Levenshtein distance can lead to falsepositives.
Consider the often cited example ofGreek /ma:ti/ and Malay /mata/, both meaning?eye?
(Bloomfield, 1938).
If we were to rely onLevenshtein distance, these words would seem tobe a highly attractive match as cognates: they arenearly identical, essentially differing in only a sin-gle character.
However, no linguist would positthat these two words are related.
To correctly learnthat they are not related, linguists typically relyon two kinds of evidence.
First, because soundchange is largely regular, we would need to com-monly see /i/ in Greek wherever we see /a/ inMalay (Ross, 1950).
Second, we should look atlanguages closely related to Greek and Malay, tosee if similar patterns hold there, too.Some authors have attempted to automaticallydetect cognate words (Mann and Yarowsky, 2001;Lowe and Mazaudon, 1994; Oakes, 2000; Kon-drak, 2001; Mulloni, 2007), but these methods1030typically work on language pairs rather than onlarger language families.
To fully automate thecomparative method, it is necessary to considermultiple languages, and to do so in a model whichcouples cognate detection with similarity learning.In this paper, we present a new generative modelfor the automatic induction of cognate groupsgiven only (1) a known family tree of languagesand (2) word lists from those languages.
A prioron word survival generates a number of cognategroups and decides which groups are attested ineach modern language.
An evolutionary modelcaptures how each word is generated from its par-ent word.
Finally, an alignment model maps theflat word lists to cognate groups.
Inference re-quires a combination of message-passing in theevolutionary model and iterative bipartite graphmatching in the alignment model.In the message-passing phase, our model en-codes distributions over strings as weighted finitestate automata (Mohri, 2009).
Weighted automatahave been successfully applied to speech process-ing (Mohri et al, 1996) and more recently to mor-phology (Dreyer and Eisner, 2009).
Here, wepresent a new method for automatically compress-ing our message automata in a way that can takeinto account prior information about the expectedoutcome of inference.In this paper, we focus on a transcribed wordlist of 583 cognate sets from three Romance lan-guages (Portuguese, Italian and Spanish), as wellas their common ancestor Latin (Bouchard-Co?te?et al, 2007).
We consider both the case wherewe know that all cognate groups have a surfaceform in all languages, and where we do not knowthat.
On the former, easier task we achieve iden-tification accuracies of 90.6%.
On the latter task,we achieve F1 scores of 73.6%.
Both substantiallybeat baseline performance.2 ModelIn this section, we describe a new generativemodel for vocabulary lists in multiple related lan-guages given the phylogenetic relationship be-tween the languages (their family tree).
The gener-ative process factors into three subprocesses: sur-vival, evolution, and alignment, as shown in Fig-ure 1(a).
Survival dictates, for each cognate group,which languages have words in that group.
Evo-lution describes the process by which daughterwords are transformed from their parent word.
Fi-nally, alignment describes the ?scrambling?
of theword lists into a flat order that hides their lineage.We present each subprocess in detail in the follow-ing subsections.2.1 SurvivalFirst, we choose a number G of ancestral cognategroups from a geometric distribution.
For eachcognate group g, our generative process walksdown the tree.
At each branch, the word may ei-ther survive or die.
This process is modeled in a?death tree?
with a Bernoulli random variable S`gfor each language ` and cognate group g specify-ing whether or not the word died before reachingthat language.
Death at any node in the tree causesall of that node?s descendants to also be dead.
Thisprocess captures the intuition that cognate wordsare more likely to be found clustered in sibling lan-guages than scattered across unrelated languages.2.2 EvolutionOnce we know which languages will have an at-tested word and which will not, we generate theactual word forms.
The evolution component ofthe model generates words according to a branch-specific transformation from a node?s immediateancestor.
Figure 1(a) graphically describes ourgenerative model for three Romance languages:Italian, Portuguese, and Spanish.1 In each cog-nate group, each word W` is generated from itsparent according to a conditional distribution withparameter ?`, which is specific to that edge in thetree, but shared between all cognate groups.In this paper, each ?` takes the form of a pa-rameterized edit distance similar to the standardLevenshtein distance.
Richer models ?
such as theones in Bouchard-Co?te?
et al (2007) ?
could in-stead be used, although with an increased infer-ential cost.
The edit transducers are representedschematically in Figure 1(b).
Characters x andy are arbitrary phonemes, and ?
(x, y) representsthe cost of substituting x with y. ?
represents theempty phoneme and is used as shorthand for inser-tion and deletion, which have parameters ?
and ?,respectively.As an example, see the illustration in Fig-ure 1(c).
Here, the Italian word /fwOko/ (?fire?)
isgenerated from its parent form /fokus/ (?hearth?
)1Though we have data for Latin, we treat it as unobservedto represent the more common case where the ancestral lan-guage is unattested; we also evaluate our system using theLatin data.1031GWVLWPI???
?
?WLA?SLASVLSPISITSESSPTLLwptwesL?wITwITwITwITwITwITWITWITSurvivalEvolutionf u skf w?okAlignment(a)(b)(c)x:y/?(x,y)x:?/?x?
:y/?yoFigure 1: (a) The process by which cognate words are generated.
Here, we show the derivation of Romance language wordsW` from their respective Latin ancestor, parameterized by transformations ?` and survival variables S`.
Languages shownare Latin (LA), Vulgar Latin (VL), Proto-Iberian (PI), Italian (IT), Portuguese (PT), and Spanish (ES).
Note that only modernlanguage words are observed (shaded).
(b) The class of parameterized edit distances used in this paper.
Each pair of phonemeshas a weight ?
for deletion, and each phoneme has weights ?
and ?
for insertion and deletion respectively.
(c) A possiblealignment produced by an edit distance between the Latin word focus (?hearth?)
and the Italian word fuoco (?fire?
).by a series of edits: two matches, two substitu-tions (/u/?
/o/, and /o/?/O/), one insertion (w)and one deletion (/s/).
The probability of eachindividual edit is determined by ?.
Note that themarginal probability of a specific Italian word con-ditioned on its Vulgar Latin parent is the sum overall possible derivations that generate it.2.3 AlignmentFinally, at the leaves of the trees are the observedwords.
(We take non-leaf nodes to be unobserved.
)Here, we make the simplifying assumption that inany language there is at most one word per lan-guage per cognate group.
Because the assign-ments of words to cognates is unknown, we spec-ify an unknown alignment parameter pi` for eachmodern language which is an alignment of cognategroups to entries in the word list.
In the case thatevery cognate group has a word in each language,each pi` is a permutation.
In the more general casethat some cognate groups do not have words fromall languages, this mapping is injective from wordsto cognate groups.
From a generative perspective,pi` generates observed positions of the words insome vocabulary list.In this paper, our task is primarily to learn thealignment variables pi`.
All other hidden variablesare auxiliary and are to be marginalized to thegreatest extent possible.3 Inference of Cognate AssignmentsIn this section, we discuss the inference methodfor determining cognate assignments under fixedparameters ?.
We are given a set of languages anda list of words in each language, and our objec-tive is to determine which words are cognate witheach other.
Because the parameters pi` are eitherpermutations or injections, the inference task is re-duced to finding an alignment pi of the respectiveword lists to maximize the log probability of theobserved words.pi?
= arg maxpi?glog p(w(`,pi`(g))|?, pi,w?`)w(`,pi`(g)) is the word in language ` that pi` hasassigned to cognate group g. Maximizing thisquantity directly is intractable, and so instead weuse a coordinate ascent algorithm to iteratively1032maximize the alignment corresponding to asingle language ` while holding the others fixed:pi?` = arg maxpi`?glog p(w(`,pi`(g))|?, pi?`, pi`,w?`)Each iteration is then actually an instance ofbipartite graph matching, with the words in onelanguage one set of nodes, and the current cognategroups in the other languages the other set ofnodes.
The edge affinities aff between thesenodes are the conditional probabilities of eachword w` belonging to each cognate group g:aff(w`, g) = p(w`|w?`,pi?`(g), ?, pi?`)To compute these affinities, we perform in-ference in each tree to calculate the marginaldistribution of the words from the language `.For the marginals, we use an analog of the for-ward/backward algorithm.
In the upward pass, wesend messages from the leaves of the tree towardthe root.
For observed leaf nodes Wd, we have:?d?a(wa) = p(Wd = wd|wa, ?d)and for interior nodes Wi:?i?a(wa) =?wip(wi|wa, ?i)?d?child(wi)?d?i(wi)(1)In the downward pass (toward the lan-guage `), we sum over ancestral words Wa:?a?d(wd)=?wap(wd|wa, ?d)?a??a(wa)?d??child(wa)d?
6=d?d?
?a(wa)where a?
is the ancestor of a. Computing thesemessages gives a posterior marginal distribution?`(w`) = p(w`|w?`,pi?`(g), ?, pi?`), which is pre-cisely the affinity score we need for the bipartitematching.
We then use the Hungarian algorithm(Kuhn, 1955) to find the optimal assignment forthe bipartite matching problem.One important final note is initialization.
In ourearly experiments we found that choosing a ran-dom starting configuration unsurprisingly led torather poor local optima.
Instead, we started withempty trees, and added in one language per itera-tion until all languages were added, and then con-tinued iterations on the full tree.4 LearningSo far we have only addressed searching forViterbi alignments pi under fixed parameters.
Inpractice, it is important to estimate better para-metric edit distances ?` and survival variablesS`.
To motivate the need for good transducers,consider the example of English ?day?
/deI/ andLatin ?die?s?
/dIe:s/, both with the same mean-ing.
Surprisingly, these words are in no way re-lated, with English ?day?
probably coming from averb meaning ?to burn?
(OED, 1989).
However,a naively constructed edit distance, which for ex-ample might penalize vowel substitutions lightly,would fail to learn that Latin words that are bor-rowed into English would not undergo the soundchange /I/?/eI/.
Therefore, our model must learnnot only which sound changes are plausible (e.g.vowels turning into other vowels is more commonthan vowels turning into consonants), but whichchanges are appropriate for a given language.2At a high level, our learning algorithm is muchlike Expectation Maximization with hard assign-ments: after we update the alignment variables piand thus form new potential cognate sets, we re-estimate our model?s parameters to maximize thelikelihood of those assignments.3 The parameterscan be learned through standard maximum likeli-hood estimation, which we detail in this section.Because we enforce that a word in language dmust be dead if its parent word in language a isdead, we just need to learn the conditional prob-abilities p(Sd = dead|Sa = alive).
Given fixedassignments pi, the maximum likelihood estimatecan be found by counting the number of ?deaths?that occurred between a child and a live parent,applying smoothing ?
we found adding 0.5 to bereasonable ?
and dividing by the total number oflive parents.For the transducers ?, we learn parameterizededit distances that model the probabilities of dif-ferent sound changes.
For each ?` we fit a non-uniform substitution, insertion, and deletion ma-trix ?
(x, y).
These edit distances define a condi-2We note two further difficulties: our model does not han-dle ?borrowings,?
which would be necessary to capture asignificant portion of English vocabulary; nor can it seam-lessly handle words that are inherited later in the evolution oflanguage than others.
For instance, French borrowed wordsfrom its parent language Latin during the Renaissance andthe Enlightenment that have not undergone the same changesas words that evolved ?naturally?
from Latin.
See Bloom-field (1938).
Handling these cases is a direction for futureresearch.3Strictly, we can cast this problem in a variational frame-work similar to mean field where we iteratively maximize pa-rameters to minimize a KL-divergence.
We omit details forclarity.1033tional exponential family distribution when condi-tioned on an ancestral word.
That is, for any fixedwa:?wdp(wd|wa, ?)
=?wd?z?align(wa,wd)score(z;?)=?wd?z?align(wa,wd)?(x,y)?z?
(x, y) = 1where align(wa, wd) is the set of possible align-ments between the phonemes in words wa and wd.We are seeking the maximum likelihood esti-mate of each ?, given fixed alignments pi:?
?` = arg max?`p(w|?, pi)To find this maximizer for any given pi`, weneed to find a marginal distribution over theedges connecting any two languages a andd.
With this distribution, we calculate theexpected ?alignment unigrams.?
That is, foreach pair of phonemes x and y (or emptyphoneme ?
), we need to find the quantity:Ep(wa,wd)[#(x, y; z)] =?wa,wd?z?align(wa,wd)#(x,y; z)p(z|wa, wd)p(wa, wd)where we denote #(x, y; z) to be the num-ber of times the pair of phonemes (x, y) arealigned in alignment z.
The exact method forcomputing these counts is to use an expectationsemiring (Eisner, 2001).Given the expected counts, we now need to nor-malize them to ensure that the transducer repre-sents a conditional probability distribution (Eis-ner, 2002; Oncina and Sebban, 2006).
We havethat, for each phoneme x in the ancestor language:?y =E[#(?, y; z)]E[#(?, ?
; z)]?
(x, y) = (1??y??y?
)E[#(x, y; z)]E[#(x, ?
; z)]?x = (1??y??y?
)E[#(x, ?
; z)]E[#(x, ?
; z)]Here, we have #(?, ?
; z) =?x,y #(x, y; z) and#(x, ?
; z) =?y #(x, y; z).
The (1 ??y?
?y?
)term ensure that for any ancestral phoneme x,?y ?y+?y ?
(x, y)+?x = 1.
These equations en-sure that the three transition types (insertion, sub-stitution/match, deletion) are normalized for eachancestral phoneme.5 Transducers and AutomataIn our model, it is not just the edit distancesthat are finite state machines.
Indeed, the wordsthemselves are string-valued random variables thathave, in principle, an infinite domain.
To representdistributions and messages over these variables,we chose weighted finite state automata, whichcan compactly represent functions over strings.Unfortunately, while initially compact, these au-tomata become unwieldy during inference, and soapproximations must be used (Dreyer and Eisner,2009).
In this section, we summarize the standardalgorithms and representations used for weightedfinite state transducers.
For more detailed treat-ment of the general transducer operations, we di-rect readers to Mohri (2009).A weighted automaton (resp.
transducer) en-codes a function over strings (resp.
pairs ofstrings) as weighted paths through a directedgraph.
Each edge in the graph has a real-valuedweight4 and a label, which is a single phonemein some alphabet ?
or the empty phoneme ?
(resp.pair of labels in some alphabet ???).
The weightof a string is then the sum of all paths through thegraph that accept that string.For our purposes, we are concerned with threefundamental operations on weighted transducers.The first is computing the sum of all paths througha transducer, which corresponds to computing thepartition function of a distribution over strings.This operation can be performed in worst-casecubic time (using a generalization of the Floyd-Warshall algorithm).
For acyclic or feed-forwardtransducers, this time can be improved dramati-cally by using a generalization of Djisktra?s algo-rithm or other related algorithms (Mohri, 2009).The second operation is the composition of twotransducers.
Intuitively, composition creates a newtransducer that takes the output from the first trans-ducer, processes it through the second transducer,and then returns the output of the second trans-ducer.
That is, consider two transducers T1 andT2.
T1 has input alphabet ?
and output alpha-bet ?, while T2 has input alphabet ?
and out-put alphabet ?.
The composition T1 ?
T2 returnsa new transducer over ?
and ?
such that (T1 ?T2)(x, y) =?u T1(x, u) ?
T2(u, y).
In this paper,we use composition for marginalization and fac-tor products.
Given a factor f1(x, u;T1) and an-4The weights can be anything that form a semiring, but forthe sake of exposition we specialize to real-valued weights.1034other factor f2(u, y;T2), composition correspondsto the operation ?
(x, y) =?u f1(x, u)f2(u, y).For two messages ?1(w) and ?2(w), the same al-gorithm can be used to find the product ?
(w) =?1(w)?2(w).The third operation is transducer minimization.Transducer composition produces O(nm) states,where n and m are the number of states in eachtransducer.
Repeated compositions compound theproblem: iterated composition of k transducersproduces O(nk) states.
Minimization alleviatesthis problem by collapsing indistinguishable statesinto a single state.
Unfortunately, minimizationdoes not always collapse enough states.
In the nextsection we discuss approaches to ?lossy?
mini-mization that produce automata that are not ex-actly the same but are much smaller.6 Message ApproximationRecall that in inference, when summing out in-terior nodes wi we calculated the product overincoming messages ?d?i(wi) (Equation 1), andthat these products are calculated using transducercomposition.
Unfortunately, the maximal numberof states in a message is exponential in the num-ber of words in the cognate group.
Minimizationcan only help so much: in order for two states tobe collapsed, the distribution over transitions fromthose states must be indistinguishable.
In practice,for the automata generated in our model, mini-mization removes at most half the states, which isnot sufficient to counteract the exponential growth.Thus, we need to find a way to approximate a mes-sage ?
(w) using a simpler automata ??
(w; ?)
takenfrom a restricted class parameterized by ?.In the context of transducers, previous authorshave focused on a combination of n-best listsand unigram back-off models (Dreyer and Eis-ner, 2009), a schematic diagram of which is inFigure 2(d).
For their problem, n-best lists aresensible: their nodes?
local potentials already fo-cus messages on a small number of hypotheses.In our setting, however, n-best lists are problem-atic; early experiments showed that a 10,000-bestlist for a typical message only accounts for 50%of message log perplexity.
That is, the posteriormarginals in our model are (at least initially) fairlyflat.An alternative approach might be to simplytreat messages as unnormalized probability distri-butions, and to minimize the KL divergence be-egufeofuuueuguouffffeeeeegggggooooof2 3eugof0 1feo4goeufueofg5oguffu e g ofeu g ofe u gfe efuegg(a)(b)(c)(d)ugoeufoFigure 2: Various topologies for approximating topologies:(a) a unigram model, (b) a bigram model, (c) the anchoredunigram model, and (d) the n-best plus backoff model used inDreyer and Eisner (2009).
In (c) and (d), the relative heightof arcs is meant to convey approximate probabilities.tween some approximating message ??
(w) and thetrue message ?(w).
However, messages are notalways probability distributions and ?
because thenumber of possible strings is in principle infinite ?they need not sum to a finite number.5 Instead, wepropose to minimize the KL divergence betweenthe ?expected?
marginal distribution and the ap-proximated ?expected?
marginal distribution:??
= arg min?DKL(?(w)?(w)||?(w)??
(w; ?
))= arg min??w?(w)?
(w) log?(w)?(w)?(w)??
(w; ?
)= arg min??w?(w)?
(w) log?(w)??
(w; ?
)(2)where ?
is a term acting as a surrogate for the pos-terior distribution over w without the informationfrom ?.
That is, we seek to approximate ?
not onits own, but as it functions in an environment rep-resenting its final context.
For example, if ?
(w) isa backward message, ?
could be a stand-in for aforward probability.6In this paper, ?
(w) is a complex automaton withpotentially many states, ??
(w; ?)
is a simple para-metric automaton with forms that we discuss be-low, and ?
(w) is an arbitrary (but hopefully fairlysimple) automaton.
The actual method we use is5As an extreme example, suppose we have observed thatWd = wd and that p(Wd = wd|wa) = 1 for all ancestralwords wa.
Then, clearlyPwd?
(wd) =PwdPp(Wd =wd|wa) = ?
whenever there are an infinite number of pos-sible ancestral strings wa.6This approach is reminiscent of Expectation Propaga-tion (Minka, 2001).1035as follows.
Given a deterministic prior automa-ton ?
, and a deterministic automaton topology ??
?,we create the composed unweighted automaton?
???
?, and calculate arc transitions weights to min-imize the KL divergence between that composedtransducer and ?
?
?.
The procedure for calcu-lating these statistics is described in Li and Eis-ner (2009), which amounts to using an expectationsemiring (Eisner, 2001) to compute expected tran-sitions in ?
?
???
under the probability distribution?
?
?.From there, we need to create the automaton?
?1 ?
?
?
??.
That is, we need to divide out theinfluence of ?(w).
Since we know the topologyand arc weights for ?
ahead of time, this is oftenas simple as dividing arc weights in ?
?
??
by thecorresponding arc weight in ?(w).
For example,if ?
encodes a geometric distribution over wordlengths and a uniform distribution over phonemes(that is, ?
(w) ?
p|w|), then computing ??
is as sim-ple as dividing each arc in ?
?
??
by p.7There are a number of choices for ?
.
One is ahard maximum on the length of words.
Another isto choose ?
(w) to be a unigram language modelover the language in question with a geometricprobability over lengths.
In our experiments, wefind that ?
(w) can be a geometric distribution overlengths with a uniform distribution over phonemesand still give reasonable results.
This distributioncaptures the importance of shorter strings whilestill maintaining a relatively weak prior.What remains is the selection of the topologiesfor the approximating message ??.
We considerthree possible approximations, illustrated in Fig-ure 2.
The first is a plain unigram model, thesecond is a bigram model, and the third is an an-chored unigram topology: a position-specific un-igram model for each position up to some maxi-mum length.The first we consider is a standard unigrammodel, which is illustrated in Figure 2(a).
Ithas |?| + 2 parameters: one weight ?a for eachphoneme a ?
?, a starting weight ?, and a stop-ping probability ?.
??
then has the form:??
(w) = ??
?i?|w|?wiEstimating this model involves only computingthe expected count of each phoneme, along with7Also, we must be sure to divide each final weight in thetransducer by (1 ?
|?|p), which is the stopping probabilityfor a geometric transducer.the expected length of a word, E[|w|].
We thennormalize the counts according to the maximumlikelihood estimate, with arc weights set as:?a ?
E[#(a)]Recall that these expectations can be computed us-ing an expectation semiring.Finally, ?
can be computed by ensuring that theapproximate and exact expected marginals havethe same partition function.
That is, with the otherparameters fixed, solve:?w?(w)??
(w) =?w?(w)?
(w)which amounts to rescaling ??
by some constant.The second topology we consider is the bigramtopology, illustrated in Figure 2(b).
It is similarto the unigram topology except that, instead ofa single state, we have a state for each phonemein ?, along with a special start state.
Each statea has transitions with weights ?b|a = p(b|a) ?E[#(b|a)].
Normalization is similar to the un-igram case, except that we normalize the transi-tions from each state.The final topology we consider is the positionalunigram model in Figure 2(c).
This topology takespositional information into account.
Namely, foreach position (up to some maximum position), wehave a unigram model over phonemes emitted atthat position, along with the probability of stop-ping at that position (i.e.
a ?sausage lattice?).
Es-timating the parameters of this model is similar,except that the expected counts for the phonemesin the alphabet are conditioned on their position inthe string.
With the expected counts for each posi-tion, we normalize each state?s final and outgoingweights.
In our experiments, we set the maximumlength to seven more than the length of the longestobserved string.7 ExperimentsWe conduct three experiments.
The first is a ?com-plete data?
experiment, in which we reconstitutethe cognate groups from the Romance data set,where all cognate groups have words in all threelanguages.
This task highlights the evolution andalignment models.
The second is a much harder?partial data?
experiment, in which we randomlyprune 20% of the branches from the dataset ac-cording to the survival process described in Sec-tion 2.1.
Here, only a fraction of words appear1036in any cognate group, so this task crucially in-volves the survival model.
The ultimate purposeof the induced cognate groups is to feed richerevolutionary models, such as full reconstructionmodels.
Therefore, we also consider a proto-wordreconstruction experiment.
For this experiment,using the system of Bouchard-Co?te?
et al (2009),we compare the reconstructions produced fromour automatic groups to those produced from goldcognate groups.7.1 BaselineAs a novel but heuristic baseline for cognate groupdetection, we use an iterative bipartite matchingalgorithm where instead of conditional likelihoodsfor affinities we use Dice?s coefficient, defined forsets X and Y as:Dice(X,Y ) =2|X ?
Y ||X|+ |Y |(3)Dice?s coefficients are commonly used in bilingualdetection of cognates (Kondrak, 2001; Kondrak etal., 2003).
We follow prior work and use sets ofbigrams within words.
In our case, during bipar-tite matching the set X is the set of bigrams in thelanguage being re-permuted, and Y is the union ofbigrams in the other languages.7.2 Experiment 1: Complete DataIn this experiment, we know precisely how manycognate groups there are and that every cognategroup has a word in each language.
While thisscenario does not include all of the features of thereal-world task, it represents a good test case ofhow well these models can perform without thenon-parametric task of deciding how many clus-ters to use.We scrambled the 583 cognate groups in theRomance dataset and ran each method to conver-gence.
Besides the heuristic baseline, we tried ourmodel-based approach using Unigrams, Bigramsand Anchored Unigrams, with and without learn-ing the parametric edit distances.
When we did notuse learning, we set the parameters of the edit dis-tance to (0, -3, -4) for matches, substitutions, anddeletions/insertions, respectively.
With learningenabled, transducers were initialized with thoseparameters.For evaluation, we report two metrics.
The firstis pairwise accuracy for each pair of languages,averaged across pairs of words.
The other is accu-Pairwise ExactAcc.
MatchHeuristicBaseline 48.1 35.4ModelTransducers MessagesLevenshtein Unigrams 37.2 26.2Levenshtein Bigrams 43.0 26.5Levenshtein Anch.
Unigrams 68.6 56.8Learned Unigrams 0.1 0.0Learned Bigrams 38.7 11.3Learned Anch.
Unigrams 90.3 86.6Table 1: Accuracies for reconstructing cognate groups.
Lev-enshtein refers to fixed parameter edit distance transducer.Learned refers to automatically learned edit distances.
Pair-wise Accuracy means averaged on each word pair; ExactMatch refers to percentage of completely and accurately re-constructed groups.
For a description of the baseline, see Sec-tion 7.1.Prec.
Recall F1HeuristicBaseline 49.0 43.5 46.1ModelTransducers MessagesLevenshtein Anch.
Unigrams 86.5 36.1 50.9Learned Anch.
Unigrams 66.9 82.0 73.6Table 2: Accuracies for reconstructing incomplete groups.Scores reported are precision, recall, and F1, averaged overall word pairs.racy measured in terms of the number of correctly,completely reconstructed cognate groups.Table 1 shows the results under various config-urations.
As can be seen, the kind of approxima-tion used matters immensely.
In this application,positional information is important, more so thanthe context of the previous phoneme.
Both Un-igrams and Bigrams significantly under-performthe baseline, while Anchored Unigrams easily out-performs it both with and without learning.An initially surprising result is that learning ac-tually harms performance under the unanchoredapproximations.
The explanation is that thesetopologies are not sensitive enough to context, andthat the learning procedure ends up flattening thedistributions.
In the case of unigrams ?
which havethe least context ?
learning degrades performanceto chance.
However, in the case of positional uni-grams, learning reduces the error rate by more thantwo-thirds.7.3 Experiment 2: Incomplete DataAs a more realistic scenario, we consider the casewhere we do not know that all cognate groups havewords in all languages.
To test our model, we ran-1037domly pruned 20% of the branches according thesurvival process of our model.8Because only Anchored Unigrams performedwell in Experiment 1, we consider only it and theDice?s coefficient baseline.
The baseline needs tobe augmented to support the fact that some wordsmay not appear in all cognate groups.
To do this,we thresholded the bipartite matching process sothat if the coefficient fell below some value, westarted a new group for that word.
We experi-mented on 10 values in the range (0,1) for thebaseline?s threshold and report on the one (0.2)that gives the best pairwise F1.The results are in Table 2.
Here again, we seethat the positional unigrams perform much betterthan the baseline system.
The learned transduc-ers seem to sacrifice precision for the sake of in-creased recall.
This makes sense because the de-fault edit distance parameter settings strongly fa-vor exact matches, while the learned transducerslearn more realistic substitution and deletion ma-trices, at the expense of making more mistakes.For example, the learned transducers enableour model to correctly infer that Portuguese/d1femdu/, Spanish /defiendo/, and Italian/difEndo/ are all derived from Latin /de:fendo:/?defend.?
Using the simple Levenshtein transduc-ers, on the other hand, our model keeps all threeseparated, because the transducers cannot know ?among other things ?
that Portuguese /1/, Span-ish /e/, and Italian /i/ are commonly substitutedfor one another.
Unfortunately, because the trans-ducers used cannot learn contextual rules, cer-tain transformations can be over-applied.
For in-stance, Spanish /nombRar/ ?name?
is grouped to-gether with Portuguese /num1RaR/ ?number?
andItalian /numerare/ ?number,?
largely because therule Portuguese /u/?
Spanish /o/ is applied out-side of its normal context.
This sound change oc-curs primarily with final vowels, and does not usu-ally occur word medially.
Thus, more sophisti-cated transducers could learn better sound laws,which could translate into improved accuracy.7.4 Experiment 3: ReconstructionsAs a final trial, we wanted to see how each au-tomatically found cognate group faired as com-pared to the ?true groups?
for actual reconstruc-tion of proto-words.
Our model is not optimized8This dataset will be made available athttp://nlp.cs.berkeley.edu/Main.html#Historicalfor faithful reconstruction, and so we used the An-cestry Resampling system of Bouchard-Co?te?
et al(2009).
To evaluate, we matched each Latin wordwith the best possible cognate group for that word.The process for the matching was as follows.
Iftwo or three of the words in an constructed cognategroup agreed, we assigned the Latin word associ-ated with the true group to it.
With the remainder,we executed a bipartite matching based on bigramoverlap.For evaluation, we examined the Levenshteindistance between the reconstructed word and thechosen Latin word.
As a kind of ?skyline,?we compare to the edit distances reported inBouchard-Co?te?
et al (2009), which was based oncomplete knowledge of the cognate groups.
Onthis task, our reconstructed cognate groups hadan average edit distance of 3.8 from the assignedLatin word.
This compares favorably to the editdistances reported in Bouchard-Co?te?
et al (2009),who using oracle cognate assignments achieved anaverage Levenshtein distance of 3.0.98 ConclusionWe presented a new generative model of wordlists that automatically finds cognate groups fromscrambled vocabulary lists.
This model jointlymodels the origin, propagation, and evolution ofcognate groups from a common root word.
Wealso introduced a novel technique for approximat-ing automata.
Using these approximations, ourmodel can reduce the error rate by 80% over abaseline approach.
Finally, we demonstrate thatthese automatically generated cognate groups canbe used to automatically reconstruct proto-wordsfaithfully, with a small increase in error.AcknowledgmentsThanks to Alexandre Bouchard-Co?te?
for the manyinsights.
This project is funded in part by the NSFunder grant 0915265 and an NSF graduate fellow-ship to the first author.ReferencesLeonard Bloomfield.
1938.
Language.
Holt, NewYork.9Morphological noise and transcription errors contributeto the absolute error rate for this data set.1038Alexandre Bouchard-Co?te?, Percy Liang, Thomas Grif-fiths, and Dan Klein.
2007.
A probabilistic ap-proach to diachronic phonology.
In EMNLP.Alexandre Bouchard-Co?te?, Thomas L. Griffiths, andDan Klein.
2009.
Improved reconstruction of pro-tolanguage word forms.
In NAACL, pages 65?73.Hal Daume?
III and Lyle Campbell.
2007.
A Bayesianmodel for discovering typological implications.
InConference of the Association for ComputationalLinguistics (ACL).Hal Daume?
III.
2009.
Non-parametric Bayesian modelareal linguistics.
In NAACL.Markus Dreyer and Jason Eisner.
2009.
Graphicalmodels over multiple strings.
In EMNLP, Singa-pore, August.Jason Eisner.
2001.
Expectation semirings: FlexibleEM for finite-state transducers.
In Gertjan van No-ord, editor, FSMNLP.Jason Eisner.
2002.
Parameter estimation for proba-bilistic finite-state transducers.
In ACL.Grzegorz Kondrak, Daniel Marcu, and Keven Knight.2003.
Cognates can improve statistical translationmodels.
In NAACL.Grzegorz Kondrak.
2001.
Identifying cognates byphonetic and semantic similarity.
In NAACL.Harold W. Kuhn.
1955.
The Hungarian method forthe assignment problem.
Naval Research LogisticsQuarterly, 2:83?97.Zhifei Li and Jason Eisner.
2009.
First- and second-order expectation semirings with applications tominimum-risk training on translation forests.
InEMNLP.John B. Lowe and Martine Mazaudon.
1994.
The re-construction engine: a computer implementation ofthe comparative method.
Computational Linguis-tics, 20(3):381?417.Gideon S. Mann and David Yarowsky.
2001.
Mul-tipath translation lexicon induction via bridge lan-guages.
In NAACL, pages 1?8.
Association forComputational Linguistics.Thomas P. Minka.
2001.
Expectation propagation forapproximate bayesian inference.
In UAI, pages 362?369.Mehryar Mohri, Fernando Pereira, and Michael Riley.1996.
Weighted automata in text and speech pro-cessing.
In ECAI-96 Workshop.
John Wiley andSons.Mehryar Mohri, 2009.
Handbook of Weighted Au-tomata, chapter Weighted Automata Algorithms.Springer.Andrea Mulloni.
2007.
Automatic prediction of cog-nate orthography using support vector machines.
InACL, pages 25?30.John Nerbonne.
2010.
Measuring the diffusion of lin-guistic change.
Philosophical Transactions of theRoyal Society B: Biological Sciences.Michael P. Oakes.
2000.
Computer estimation ofvocabulary in a protolanguage from word lists infour daughter languages.
Quantitative Linguistics,7(3):233?243.OED.
1989.
?day, n.?.
In The Oxford English Dictio-nary online.
Oxford University Press.John Ohala, 1993.
Historical linguistics: Problemsand perspectives, chapter The phonetics of soundchange, pages 237?238.
Longman.Jose Oncina and Marc Sebban.
2006.
Learningstochastic edit distance: Application in handwrittencharacter recognition.
Pattern Recognition, 39(9).Don Ringe, Tandy Warnow, and Ann Taylor.
2002.Indo-european and computational cladistics.
Trans-actions of the Philological Society, 100(1):59?129.Alan S.C. Ross.
1950.
Philological probability prob-lems.
Journal of the Royal Statistical Society SeriesB.David Yarowsky, Grace Ngai, and Richard Wicen-towski.
2000.
Inducing multilingual text analysistools via robust projection across aligned corpora.In NAACL.1039
