Answer  Ext rac t ionSteven Abney  Michae l  Co l l ins  Ami t  S ingha lAT&T Shannon Laboratory180 Park  Ave.F lo rharn  Park ,  N J  07932{abney,  mco l l ins , s ingha l}@research .a t t .
cornAbst ractInformation retrieval systems have typically concen-trated on retrieving a set of documents which are rel-evant to a user's query.
This paper describes a sys-tem that attempts to retrieve a much smaller sectionof text, namely, a direct answer to a user's question.The SMART IR system is used to extract a rankedset of passages that are relevant o the query.
En-tities are extracted from these passages as potentialanswers to the question, and ranked for plausibilityaccording to how well their type matches the query,and according to their frequency and position in thepassages.
The system was evaluated at the TREC-8question answering track: we give results and erroranalysis on these queries.1 IntroductionIn this paper, we describe and evaluate a question-answering system based on passage retrieval andentity-extraction technology.There has long been a concensus in the Informa-tion Retrieval (IR) community that natural anguageprocessing has little to offer for retrieval systems.Plausibly, this is creditable to the preeminence of adhoc document retrieval as the task of interest in IR.However, there is a growing recognition of the lim-itations of ad hoc retrieval, both in the sense thatcurrent systems have reached the limit of achievableperformance, and in the sense that users' informa-tion needs are often not well characterized by docu-ment retrieval.In many cases, a user has a question with a spe-cific answer, such as What city is it where the Euro-pean Parliament meets?
or Who discovered Pluto?In such cases, ranked answers with links to support-ing documentation are much more useful than theranked list of documents that standard retrieval en-gines produce.The ability to answer specific questions also pro-vides a foundation for addressing quantitative in-quiries such as How many times has the Fed raisedinterest rates this year?
which can be interpretedas the cardinality of the set of answers to a specificquestion that happens to have multiple correct an-swers, like On what date did the Fed raise interestrates this year?We describe a system that extracts specific an-swers from a document collection.
The system's per-formance was evaluated in the question-answeringtrack that has been introduced this year at theTREC information-retrieval conference.
The majorpoints of interest are the following.?
Comparison of the system's performance to asystem that uses the same passage retrievalcomponent, but no natural language process-ing, shows that NLP provides ignificant perfor-mance improvements on the question-answeringtask.?
The system is designed to build on the strengthsof both IR and NLP technologies.
This makesfor much more robustness than a pure NLP sys-tem would have, while affording much greaterprecision than a pure IR system would have.?
The task is broken into subtasks that admit ofindependent development and evaluation.
Pas-sage retrieval and entity extraction are both re-cognized independent tasks.
Other subtasks areentity classification and query classification--both being classification tasks that use featuresobtained by parsing--and entity ranking.In the following section, we describe the question-answering system, and in section 3, we quantify itsperformance and give an error analysis.2 The  Quest ion -Answer ing  SystemThe system takes a natural-language query as inputand produces a list of answers ranked in order ofconfidence.
The top five answers were submitted tothe TREC evaluation.Queries are processed in two stages.
In the infor-mation retrieval stage, the most promising passagesof the most promising documents are retrieved.
Inthe linguistic processing stage, potential answers areextracted from these passages and ranked.The system can be divided into five main compo-nents.
The information retrieval stage consists of a296single component, passage retrieval, and the linguis-tic processing stage circumscribes four components:entity extraction, entity classification, query classi-fication, and entity ranking.Passage Ret r ieva l  Identify relevant documents,and within relevant documents, identify thepassages most likely to contain the answer tothe question.Ent i ty  Ext ract ion  Extract a candidate set of pos-sible answers from the passages.Ent i ty  Classification The candidate set is a list ofentities falling into a number of categories, in-cluding people, locations, organizations, quan-tities, dates, and linear measures.
In some cases(dates, quantities, linear measures), entity clas-sification is a side effect of entity extraction,but in other cases (proper nouns, which maybe people, locations, or organizations), there isa separate classification step after extraction.Query  Classi f icat ion Determine what category ofentity the question is asking for.
For example,if the query isWho is the author of the book, TheIron Lady: A Biography of MargaretThatcher?the answer should be an entity of type Person.Ent i ty  Ranking Assign scores to entities, repre-senting roughly belief that the entity is the cor-rect answer.
There are two components of thescore.
The most-significant bit is whether ornot the category of the entity (as determinedby entity classification) matches the categorythat the question is seeking (as determined byquery classification).
A finer-grained ranking isimposed on entities with the correct category,through the use of frequency and other infor-mation.The following sections describe these five compo-nents in detail.2.1 Passage RetrievalThe first step is to find passages likely to contain theanswer to the query.
We use a modified version ofthe SMART information retrieval system (Buckleyand Lewit, 1985; Salton, 1971) to recover a set ofdocuments which are relevant o the question.
Wedefine passages as overlapping sets consisting of asentence and its two immediate neighbors.
(Pas-sages are in one-one correspondence with with sen-tences, and adjacent passages have two sentences incommon.)
The score for passage i was calculated as1 ?Si-z + ?Si + ~'S,+1 (1)where Sj, the score for sentence j, is the sum of IDFweights of non-stop terms that it shares with thequery, plus an additional bonus for pairs of words(bigrams) that the sentence and query have in com-mon.The top 50 passages are passed on as input tolinguistic processing.2.2 Ent i ty  Ext ract ionEntity extraction is done using the Cass partial pars-er (Abney, 1996).
From the Cass output, we takedates, durations, linear measures, and quantities.In addition, we constructed specialized code forextracting proper names.
The proper-name extrac-tor essentially classifies capitalized words as intrinsi-cally capitalized or not, where the alternatives to in-trinsic capitalization are sentence-initial capitaliza-tion or capitalization in titles and headings.
Theextractor uses various heuristics, including whetherthe words under consideration appear unambiguous-ly capitalized elsewhere in the document.2.3 Ent i ty  Classif icationThe following types of entities were extracted as po-tential answers to queries.Person, Locat ion,  Organization, OtherProper names were classified into these cate-gories using a classifier built using the methoddescribed in (Collins and Singer, 1999).
1 Thisis the only place where entity classification wasactually done as a separate step from entityextraction.Dates Four-digit numbers starting with 1 .
.
.
or20.
.
were taken to be years.
Cass was used toextract more complex date expressions ( uch asSaturday, January 1st, 2000).Quant i t ies  Quantities include bare numbers andnumeric expressions' like The Three Stooges, 41//2 quarts, 27~o.
The head word of complex nu-meric expressions was identified (stooges, quartsor percent); these entities could then be lateridentified as good answers to How many ques-tions such as How many stooges were there ?Durat ions,  Linear Measures  Durations and lin-ear measures are essentially special cases ofquantities, in which the head word is a timeunit or a unit of linear measure.
Examples ofdurations are three years, 6 1/2 hours.
Exam-ples of linear measures are 140 million miles,about 12 feet.We should note that this list does not exhaust hespace of useful categories.
Monetary amounts (e.g.,~The classifier makes a three way distinction betweenPerson, Location and Organization; names where the classi-fier makes no decision were classified as Other Named E~tity.297$25 million) were added to the system shortly afterthe Trec run, but other gaps in coverage remain.
Wediscuss this further in section 3.2.4 Query  Classif icationThis step involves processing the query to identifythe category of answer the user is seeking.
We parsethe query, then use the following rules to determinethe category of the desired answer:?
Who, Whom -+ Person.?
Where, Whence, Whither--+ Locat ion.?
When -+ Date.?
How few, great, little, many, much -+Quemtity.
We also extract the head word ofthe How expression (e.g., stooges in how manystooges) for later comparison to the head wordof candidate answers.?
How long --+ Duration or Linear Measure.How tall, wide, high, big, far --+ LinearMeasure.?
The wh-words Which or What typically appearwith a head noun that describes the categoryof entity involved.
These questions fall into twoformats: What  X where X is the noun involved,and What  is the ... X.
Here are a couple ofexamples:What  company is the largest Japaneseship builder?What  is the largest city in Germany?For these queries the head noun (e.g., compa-ny or city) is extracted, and a lexicon map-ping nouns to categories is used to identify thecategory of the query.
The lexicon was partlyhand-built (including some common cases suchas number --+ Quant i ty  or year --~ Date).
Alarge list of nouns indicating Person, Locat ionor Organ izat ion  categories was automatical-ly taken from the contextual (appositive) cueslearned in the named entity classifier describedin (Collins and Singer, 1999).?
In queries containing no wh-word (e.g., Namethe largest city in Germany), the first nounphrase that is an immediate constituent of thematrix sentence is extracted, and its head isused to determine query category, as for WhatX questions.?
Otherwise, the category is the wildcard Any.2.5 Ent i ty  Rank ingEntity scores have two components.
The first, most-significant, component is whether or not the entity'scategory matches the query's category.
(If the querycategory is Any, all entities match it.
)In most cases, the matching is boolean: either anentity has the correct category or not.
However,there are a couple of special cases where finer distinc-tions are made.
If a question is of the Date type, andthe query contains one of the words day or month,then "full" dates are ranked above years.
Converse-ly, if the query contains the word year, then years areranked above full dates.
In How many X questions(where X is a noun), quantified phrases whose headnoun is also X are ranked above bare numbers orother quantified phrases: for example, in the queryHow many lives were lost in the Lockerbie air crash,entities such as 270 lives or almost 300 lives wouldbe ranked above entities such as 200 pumpkins or150.
2The second component of the entity score is basedon the frequency and position of occurrences of agiven entity within the retrieved passages.
Each oc-currence of an entity in a top-ranked passage counts10 points, and each occurrence of an entity in anyother passage counts 1 point.
("Top-ranked pas-sage" means the passage or passages that receivedthe maximal score from the passage retrieval compo-nent.)
This score component is used as a secondarysort key, to impose a ranking on entities that are notdistinguished by the first score component.In counting occurrences of entities, it is necessaryto decide whether or not two occurrences are to-kens of the same entity or different entities.
To thisend, we do some normalization of entities.
Datesare mapped to the format year-month-day: that is,last Tuesday, November 9, 1999 and 11/9/99 areboth mapped to the normal form 1999 Nov 9 beforefrequencies are counted.
Person names axe aliasedbased on the final word they contain.
For example,Jackson and Michael Jackson are both mapped tothe normal form Jackson.
a3 Eva luat ion3.1 Resul ts  on the TREC-8  Evaluat ionThe system was evaluated in the TREC-8 question-answering track.
TREC provided 198 questions as ablind test set: systems were required to provide fivepotential answers for each question, ranked in or-der of plausibility.
The output from each systemwas then scored by hand by evaluators at NIST,each answer being marked as either correct or in-correct.
The system's core on a particular questionis a function of whether it got a correct answer in thefive ranked answers, with higher scores for the an-swer appearing higher in the ranking.
The systemreceives a score of 1, 1/2, 1/3, 1/4, 1/5, or 0, re-2perhaps less desirably, people would not be recognizedas a synonym of lives in this example: 200 people would beindistinguishable from 200 pumpkins.3This does introduce occasional errors, when two peoplewith the same last name appear in retrieved passages.298System Mean Answer MeanAns Len in Top 5 ScoreEntity 10.5 B 46% 0.356Passage 50 50 B 38.9% 0.261Passage 250 250 B 68% 0.545Figure 1: Results on the TREC-8 Evaluationspectively, according as the correct answer is ranked1st, 2nd, 3rd, 4th, 5th, or lower in the system out-put.
The final score for a system is calculated as itsmean score on the 198 questions.The TREC evaluation considered two question-answering scenarios: one where answers were lim-ited to be less than 250 bytes in length, the otherwhere the limit was 50 bytes.
The output from thepassage retrieval component (section 2.1), with sometrimming of passages to ensure they were less than250 bytes, was submitted to the 250 byte scenario.The output of the full entity-based system was sub-mitted to the 50 byte track.
For comparison, we alsosubmitted the output of a 50-byte system based onIR techniques alone.
In this system single-sentencepassages were retrieved as potential answers, theirscore being calculated using conventional IR meth-ods.
Some trimming of sentences so that they wereless than 50 bytes in length was performed.Figure 1 shows results on the TREC-8 evaluation.The 250-byte passage-based system found a correctanswer somewhere in the top five answers on 68% ofthe questions, with a final score of 0.545.
The 50-byte passage-based system found a correct answeron 38.9% of all questions, with an average score of0.261.
The reduction in accuracy when moving fromthe 250-byte limit to the 50-byte limit is expected,because much higher precision is required; the 50-byte limit allows much less extraneous material tobe included with the answer.
The benefit of theincluding less extraneous material is that the usercan interpret the output with much less effort.Our entity-based system found a correct answer inthe top five answers on 46% of the questions, witha final score of 0.356.
The performance is not asgood as that of the 250-byte passage-based system.But when less extraneous material is permitted, theentity-based system outperforms the passage-basedapproach.
The accuracy of the entity-based sys-tem is significantly better than that of the 50-bytepassage-based system, and it returns virtually no ex-traneous material, as reflected in the average answerlength of only 10.5 bytes.
The implication is thatNLP techniques become increasingly useful whenshort answers are required.3.2 Error Analysis of the Ent i ty-BasedSystem3.2.1 Ranking of AnswersAs a first point, we looked at the performance oftheentity-based system, considering the queries wherethe correct answer was found somewhere in the top5 answers (46% of the 198 questions).
We found thaton these questions, the percentage ofanswers ranked1, 2, 3, 4, and 5 was 66%, 14%, 11%, 4%, and 4%respectively.
This distribution is by no means uni-form; it is clear that when the answer is somewherein the top five, it is very likely to be ranked 1st or2nd.
The system's performance is quite bimodahit either completely fails to get the answer, or elserecovers it with a high ranking.3.2.2 Accuracy on Different CategoriesFigure 2 shows the distribution of question typesin the TREC-8 test set ("Percentage of Q's"), andthe performance ofthe entity-based system by ques-tion type ("System Accuracy").
We categorized thequestions by hand, using the eight categories de-scribed in section 2.3, plus two categories that es-sentially represent types that were not handled bythe system at the time of the TREC competition:Monetary Amount and Miscellaneous.
"System Accuracy" means the percentage ofques-tions for which the correct answer was in the top fivereturned by the system.
There is a sharp division inthe performance on different question types.
Thecategories Person, Location, Date and Quantityare handled fairly well, with the correct answer ap-pearing in the top five 60% of the time.
These fourcategories make up 67% of all questions.
In contrast,the other question types, accounting for 33% of thequestions, are handled with only 15% accuracy.Unsurprisingly, the Miscellaneous and OtherNamed Ent i ty  categories are problematic; unfortu-nately, they are also rather frequent.
Figure 3 showssome examples of these queries.
They include a largetail of questions eeking other entity types (moun-tain ranges, growth rates, films, etc.)
and questionswhose answer is not even an entity (e.g., "Why didDavid Koresh ask the FBI for a word processor?
")For reference, figure 4 gives an impression of thesorts of questions that the system does well on (cor-rect answer in top five).3.2.3 Errors by ComponentFinally, we performed an analysis to gauge whichcomponents represent performance bottlenecks inthe current system.
We examined system logs fora 50-question sample, and made a judgment of whatcaused the error, when there was an error.
Figure 5gives the breakdown.
Each question was assigned toexactly one line of the table.The largest body of errors, accounting for 18% ofthe questions, are those that are due to unhandled299Question I Rank I Output from SystemWho is the author of the book, The Iron Lady: A Biography of 2Margaret Thatcher?What is the name of the managing director of Apricot Computer?
iWhat country is the biggest producer of tungsten?Who was the first Taiwanese President?When did Nixon visit China?How many calories are there in a Big Mac?
4What is the acronym for the rating system for air conditioner effi- 1ciency?Hugo YoungDr Peter HorneChinaTaiwanese President LiTeng hui1972562 caloriesEERFigure 4: A few TREC questions answered correctly by the system.Type Percentof Q'sSystemAccuracyPerson 28 62.5Location 18.5 67.6Date 11 45.5Quantity 9.5 52.7TOTAL 67 60Other Named EntMiscellaneousLinear MeasureMonetary AmtOrganizationDuration14.58.53.5321.533 TOTAL315.9000015ErrorsPassage retrieval failedAnswer is not an entityAnswer of unhandled type: moneyAnswer of unhandled type: miscEntity extraction failedEntity classification failedQuery classification failedEntity ranking failed16%4%10%8%2%4%4%4%SuccessesAnswer at Rank 2-5 I 16%Answer at Rank 1 I 32%TOTALFigure 2: Performance ofthe entity-based system ondifferent question types.
"System Accuracy" meanspercent of questions for which the correct answerwas in the top five returned by the system.
"Good"types are in the upper block, "Bad" types are in thelower block.What does the Peugeot company manufacture?Why did David Koresh ask the FBI for a wordprocessor?What are the Valdez Principles?What was the target rate for M3 growth in 1992?What does El Nino mean in spanish?Figure 5: Breakdown of questions by error type, inparticular, by component responsible.
Numbers arepercent of questions in a 50-question sample.five, but not at rank one, are almost all due to fail-ures of entity ranking) Various factors contributingto misrankings are the heavy weighting assigned toanswers in the top-ranked passage, the failure to ad-just frequencies by "complexity" (e.g., it is signifi-cant if 22.5 million occurs everal times, but not if 3occurs several times), and the failure of the systemto consider the linguistic context in which entitiesappear.Figure 3: Examples of "Other Named Entity" andMiscellaneous questions.types, of which half are monetary amounts.
(Ques-tions with non-entity answers account for another4%.)
Another large block (16%) is due to the pas-sage retrieval component: the correct answer wasnot present in the retrieved passages.
The linguisticcomponents ogether account for the remaining 14%of error, spread evenly among them.The cases in which the correct answer is in the top4 Conc lus ions  and  Future  WorkWe have described a system that handles arbi-trary questions, producing a candidate list of an-swers ranked by their plausibility.
Evaluation onthe TREC question-answering track showed that thecorrect answer to queries appeared in the top five an-swers 46% of the time, with a mean score of 0.356.The average length of answers produced by the sys-tem was 10.5 bytes.4The sole exception was a query misclassification causedby a parse failure---miraculously, the correct answer made itto rank five despite being of the "wrong" type.300There are several possible areas for future work.There may be potential for improved performancethrough more sophisticated use of NLP techniques.In particular, the syntactic ontext in which a par-ticular entity appears may provide important infor-mation, but it is not currently used by the system.Another area of future work is to extend theentity-extraction component of the system to han-dle arbitrary types (mountain ranges, films etc.
).The error analysis in section 3.2.2 showed that thesequestion types cause particular difficulties for thesystem.The system is largely hand-built.
It is likely thatas more features are added a trainable statistical ormachine learning approach to the problem will be-come increasingly desirable.
This entails developinga training set of question-answer pairs, raising thequestion of how a relatively large corpus of questionscan be gathered and annotated.Re ferencesSteven Abney.
1996.
Partial parsing via finite-state cascades.
J Natural Language Engineering,2(4):337-344, December.C.
Buckley and A.F.
Lewit.
1985.
Optimization ofinverted vector searches.
In Proe.
Eighth Interna-tional ACM SIGIR Conference, pages 97-110.Michael Collins and Yoram Singer.
1999.
Unsuper-vised models for named entity classification.
InEMNLP.G.
Salton, editor.
1971.
The Smart Retrieval Sys-tem - Experiments in Automatic Document Pro-cessing.
Prentice-Hall, Inc., Englewood Cliffs, NJ.301
