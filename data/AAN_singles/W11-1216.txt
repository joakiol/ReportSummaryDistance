Towards a Data Model for the Universal CorpusSteven AbneyUniversity of Michiganabney@umich.eduSteven BirdUniversity of Melbourne andUniversity of Pennsylvaniasbird@unimelb.edu.auAbstractWe describe the design of a comparable cor-pus that spans all of the world?s languages andfacilitates large-scale cross-linguistic process-ing.
This Universal Corpus consists of textcollections aligned at the document and sen-tence level, multilingual wordlists, and a smallset of morphological, lexical, and syntactic an-notations.
The design encompasses submis-sion, storage, and access.
Submission pre-serves the integrity of the work, allows asyn-chronous updates, and facilitates scholarly ci-tation.
Storage employs a cloud-hosted file-store containing normalized source data to-gether with a database of texts and annota-tions.
Access is permitted to the filestore, thedatabase, and an application programming in-terface.
All aspects of the Universal Corpusare open, and we invite community participa-tion in its design and implementation, and insupplying and using its data.1 IntroductionWe have previously proposed a community datasetof annotated text spanning a very large number oflanguages, with consistent annotation and formatthat enables automatic cross-linguistic processingon an unprecedented scale (Abney and Bird, 2010).Here we set out the data model in detail, and invitemembers of the computational linguistics commu-nity to begin work on the first version of the dataset.The targeted annotation generalizes over threewidely-used kinds of data: (1) simple bitexts, thatis, tokenized texts and their translations, which arewidely used for training machine translation sys-tems; (2) interlinear glossed text (IGT), which addslemmas, morphological features and parts of speech,and is the de facto standard in the documentary lin-guistics literature; and (3) dependency parses, whichadd a head pointer and relation name for each word,and are gaining popularity as representations of syn-tactic structure.
We do not expect all texts to haveequal richness of annotation; rather, these are thedegrees of annotation we wish to explicitly accom-modate.
Keeping the annotation lightweight is a pri-mary desideratum.We strive for inclusion of as many languages aspossible.
We are especially interested in languagesoutside of the group of 30 or so for which therealready exist non-trivial electronic resources.
Op-timistically, we aim for a universal corpus, in thesense of one that covers a widely representative setof the world?s languages and supports inquiry intouniversal linguistics and development of languagetechnologies with universal applicability.We emphasize, however, that even if completelysuccessful, it will be a universal corpus and not theuniversal corpus.
The term ?universal?
should em-phatically not be understood in the sense of encom-passing all language annotation efforts.
We are notproposing a standard or a philosophy of languagedocumentation, but rather a design for one partic-ular resource.
Though the goals with regard to lan-guage coverage are unusually ambitious, for the sakeof achievability we keep the targeted annotation assimple as possible.
The result is intended to be a sin-gle, coherent dataset that is very broad in languagecoverage, but very thin in complexity of annotation.120Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 120?127,49th Annual Meeting of the Association for Computational Linguistics,Portland, Oregon, 24 June 2011. c?2011 Association for Computational LinguisticsFinally, the development of the corpus is an un-funded, all-volunteer effort.
It will only come aboutif it wins community buy-in, in the spirit of collab-orative efforts like Project Gutenberg.
We formulateit as a cooperation among data providers and host-ing services to provide data in a manner that createsa single, seamless dataset from the user perspective.This paper is a first draft of a ?cooperative agree-ment?
that could achieve that goal.2 A lightweight model for multilingual text2.1 Media and annotationIn documentary linguistics, a distinction is madebetween language documentation, whose concernis the collection of primary documentation such asspeech recordings and indigenous written works,and language description, whose concern is the an-notation and organization of the primary material(Himmelmann, 1998).
We make a similar distinc-tion between media files and annotation, where ?an-notation?
is understood broadly to include all pro-cessing steps that make the linguistic contents moreexplicit, including plain text rendering, sentencesegmentation, and alignment of translations.The Corpus consists of annotated documents, inthe sense of primary documents with accompany-ing annotation.
There are many efforts at collectingdocumentation for a broad range of languages; whatmakes this Corpus distinct is its focus on annotation.Accordingly, we assume that media files and anno-tation are handled separately.For media, the Language Commons collection inthe Internet Archive is a recently-established repos-itory for redistributable language data that we viewas the primary host.1 For the annotation database, aprimary data host remains to be established, but wehave identified some options.
For example, AmazonWeb Services and the Talis Connected Commonshave free hosting services for public data sets.2.2 The data model in briefIn order to keep the barriers to participation as low aspossible, we have made our target for annotation assimple as possible.
The data model is summarizedin Figure 1.
We distinguish between aligned texts1http://www.archive.org/details/LanguageCommons(or parallel texts) and analyzed texts (comparabletexts).Semantically, the entire collection of aligned textsconstitutes a matrix whose columns are languagesand whose rows are texts.
We limit attention to threelevels of granularity: document, sentence, and word.Each cell is occupied by a string, the typical lengthof the string varying with the granularity.
We expectthe matrix to be quite sparse: most cells are empty.The collection of analyzed texts consists, semanti-cally, of one table per language.
The rows representwords and the columns are properties of the words.The words may either be tokens in a sentence anal-ysis, as suggested by the examples, or types repre-senting dictionary information.
The tables are com-parable, in the sense that they have a common formatand are conducive to language-independent process-ing, but they are not parallel: the i-th word in theGerman table has nothing to do with the i-th wordin the Spanish table.The tables in Figure 1 constitute the bulk of thedata model.
In addition, we assume some auxiliaryinformation (not depicted) that is primarily organi-zational.
It includes an association between docu-ments and sentences, the location of documents andsentences within media files (if applicable), a group-ing of table rows into ?files,?
and a grouping of filesinto ?works.?
Metadata such as revision informationis attached to files and works.
We return below tothe characterization of this auxiliary information.In contrast to current standard practice, we wishto emphasize the status of aligned and analyzed textas annotation of primary documents represented bymedia files such as speech recordings or page im-ages, and we wish to maintain explicit connectionsbetween annotations and primary documents.
Wedo not insist that the underlying media files be avail-able in all cases, but we hope to identify them whenpossible.
However, we focus on storage of the anno-tation; we assume that media files are in a separatestore, and referenced by external URIs.2.3 Two implementations: filestore anddatabaseThe data model is abstract, and is implemented in acouple of ways for different purposes.
For distribu-tion on physical medium or by download, it is mostconvenient to implement the data model as actual121Aligned Texts Analyzed Textsdeu spa fra eng .
.
.d1 sie.. ella.. elle.. she..d2...s1s2...w1w2...deusent form lemma morph pos gloss head relw1 s1 Ku?he Kuh PL N cow 2 SBJw2 s1 sind sein PL V be 0 ROOT...spasent form lemma morph pos gloss head relw1 s2 estas este F.PL D this 2 SPCw2 s2 floras flora F.PL N flower 3 SBJ......Figure 1: An overview of the targeted annotation: Aligned Texts in a single matrix having three levels of granularity(document, sentence, word), and Analyzed Texts grouped by language and annotated down to the word level withmorphological, lexical and syntactic information.files.
Each file contains information correspondingto some slice of a table, and the structure of the tableis encoded in the file format.
On the other hand,web services are often implemented as databases,making an implementation of the abstract model asa database desirable.A file-based implementation is most familiar, andmost existing resources are available as file collec-tions.
However, even when different existing re-sources have similar semantics, such as differentparallel text collections, there is considerable varietyin the organization and representation of the infor-mation.
In order to work with multiple such sources,a substantial amount of housekeeping is required.One can view our proposed filestore as a normal-ized form that removes the diversity that only gets inthe way of efficient cross-language processing.
In-deed, our proposed format for analyzed text hewsintentionally close to the format used in the CoNLLdependency-parsing shared tasks, which provideda normal form into which data from multiple tree-banks was mapped (Buchholz et al, 2006).When an existing resource is included in the Cor-pus, we assume that it remains externally availablein its original form, but a copy is imported into theCorpus filestore in which every file has been pre-processed into one of a set of simple file formatsimplementing the model of Figure 1, following aconsistent scheme for filenames, with utf8 charac-ter encoding, and capturing any available alignmentinformation in an auxiliary table.
Distribution of theCorpus via physical media or download simply in-volves copying the filestore.The filestore is organized around material pro-vided by individual data providers, or ?authors,?
andmaintains the identity of a data provider?s contribu-tion as a distinct intellectual ?work.?
Works providean appropriate unit to which to attach edition andrights metadata.In addition to the filestore, the texts and align-ments are imported into a collection of database ta-bles that can be queried efficiently.In section 3 we describe a simple file-based im-plementation of the data model, and show the varietyof familiar file types that find a natural place in themodel.
In section 4 we describe the tabular storagemodel.3 Filestore implementationDespite the simplicity of the data model, it capturesa substantial, even surprising, variety of commonly-used textual data file types.Document-aligned text.
Parallel corpora are mostcommonly aligned at the document level.
Typically,each translation of a document is contained in a file,and there is some way of indicating which files aremutual translations of the same document.
The con-122tents of a file, as a single string, represents one cellin the Aligned Text matrix in Figure 1 (at the ?doc-ument?
level of granularity).
A document, compris-ing a collection of mutual translations, correspondsto a row of the matrix.As normal form, we propose the convention ofusing filenames that incorporate a language iden-tifier and a document identifier.
For example,1001-eng.txt and 1001-deu.txt are the En-glish and German files representing mutual transla-tions of some hypothetical document 1001.Language identifiers are ISO 639-3 languagecode, supplemented by the Linguist List local-usecodes and subgroup and dialect identifiers.Sentence-aligned text.
At a finer grain, paral-lel corpora may be aligned at the sentence level.Each file contains the translation of one document,segmented into one sentence per line.
Our nor-mal form uses the same filename convention asfor document-aligned text, to indicate which filesare mutual translations.
We use the file suffix?.snt?
to indicate a file with one sentence perline.
This incidentally indicates which documenta set of sentences came from, since the filenamesshare a document identifier.
For example, the file1001-deu.snt contains the sentence-segmentedversion of 1001-deu.txt.In the canonical case, each file in a group ofaligned files contains the same number of sentences,and the sentences line up one-to-one.
The groupof aligned files corresponds to a set of rows in theAligned Text matrix, at the ?sentence?
level of gran-ularity.There are cases in which the sentence alignmentbetween documents is not one-to-one.
Even in thiscase, we can view the alignment as consisting of asequence of ?beads?
that sometimes contain multi-ple sentences in one language.
If we normalize thefile to one in which the group of sentences belong-ing to a single bead are concatenated together as a?translational unit,?
we reduce this case to the one-to-one case, though we do lose the information aboutorthographic sentence boundaries internal to a bead.Preserving the original sentences would necessi-tate an extension to the data model.
A typical ap-proach is to store the alignments in a table, wheren-way alignments are indicated using n-tuples of in-tegers.
We leave this as a point for future consider-ation.
We also put aside consideration of word-leveldocument alignment.Translation dictionaries.
A translation dictionarycontains word translations in multiple languages.One representation looks just like sentence-alignedtext, except that each file contains one entry per lineinstead of one sentence per line.
Each file in analigned set contains the same number of entries, andthe entries line up one-to-one across files.
This isthe representation we take as our normal form.
Wealso use the same filename convention, but with suf-fix .tdi for translation dictionary.A translation dictionary corresponds to a set ofrows in the Aligned Text matrix, at the ?word?
levelof granularity.
A translation dictionary would typ-ically be derived from a large number of text doc-uments, so each translation dictionary will typicallyhave a unique document identifier, and will not alignwith files at the sentence or document granularity.Transcriptions and segmentations.
When onebegins with a sound recording or with page imagesfrom a print volume that has been scanned, a firststep is conversion to plain text.
We will call this a?transcription?
both for the case where the originalwas a sound file and for the case where the origi-nal was a page image.
Transcriptions fit into ourdata model as the special case of ?document-alignedtext?
in which only one language is involved.
Weassume that the Aligned Text matrix is sparse, andthis is the extreme case in which only one cell in arow is occupied.
The connection between the tran-script?s document identifier and the original mediafile is recorded in an auxiliary metadata file.After transcription, the next step in processing isto identify the parts of the text that are natural lan-guage (as opposed to markup or tables or the like),and to segment the natural language portion intosentences.
The result is sentence-segmented text.Again, we treat this as the special case of sentence-aligned text in which only one language is involved.Analyzed text.
A variety of different text file typescan be grouped together under the heading of an-alyzed text.
The richest example we consider isdependency parse structure.
One widely-used filerepresentation has one word token per line.
Each123line consists of tab-separated fields containing at-tributes of the word token.
There is some varia-tion in the attributes that are specified, but the onesused in the Analyzed Text tables of our data modelare typical, namely: sentence identifier, wordform,lemma, morphological form, gloss, part of speech,head (also called governor), and relation (also calledrole).
Sentence boundaries are not represented as to-kens; rather, tokens belonging to the same sentenceshare the same value for sentence identifier.
We con-tinue with the same filename convention as before;for Analyzed Text files, the suffix is .tab.Many different linguistic annotations are natu-rally represented as special cases of Analyzed Text.?
Tokenized text in ?vertical format?
is the spe-cial case in which the only column is the word-form column.
We include the sentence ID col-umn as well, in lieu of sentence-boundary to-kens.?
POS-tagged text adds the part of speech col-umn.?
The information in the word-by-word part ofinterlinear glossed text (IGT) typically includesthe wordform, lemma, morph, and gloss; againwe also include the sentence ID column.?
A dependency parse, as already indicated, is thecase in which all columns are present.In addition, the format accommodates a varietyof monolingual and multilingual lexical resources.Such lexical resources are essential, whether manu-ally curated or automatically extracted.?
A basic dictionary consists of a sequence of en-tries, each of which contains a lemma, part ofspeech, and gloss.
Hence a dictionary is nat-urally represented as analyzed text containingjust those three columns.
The entries in a dic-tionary are word types rather than word tokens,so the wordform and sentence ID columns areabsent.?
If two or more lexicons use the same glosses,the lexicons are implicitly aligned by virtue ofthe glosses and there is no need for overt align-ment information.
This is a more flexible repre-sentation than a translation dictionary: unlike atranslation dictionary, it permits multiple wordsto have the same gloss (synonyms), and it addsparts of speech.4 Database implementationAn alternative implementation, appropriate for de-ployment of the Corpus as a web service, is as anormalized, multi-table database.
In this sectionwe drill down and consider the kinds of tables andrecords that would be required in order to representour abstract data model.
We will proceed by wayof example, for each of the kinds of data we wouldlike to accommodate.
Each example is displayed asa record consisting of a series of named fields.Note that we make no firm commitment as to thephysical format of these records.
They could be se-rialized as XML when the database is implementedas a web service.
Equally, they could be representedusing dictionaries or tuples when the database is ac-cessed via an application program interface (API).We will return to this later.4.1 The Aligned Text matrixThe Aligned Text matrix is extremely sparse.
Weuse the more flexible representation in which eachmatrix cell is stored using a separate record, wherethe record specifies (index, column) pairs.
For ex-ample, the matrix rowdeu spa frad1 Sie... Ella...d2 Mein... Mon...is represented asDocument TableDID LANG TEXT1 deu Sie...1 spa Ella...2 deu Mein...2 fra Mon...(The ellipses are intended to indicate that each cellcontains the entire text of a document.)
We have alsoadded an explicit document ID.When we consider entries at the sentence andword levels, we require both a document ID and sen-tence or word IDs within the document.
Figure 2shows an example of two sentences from the samedocument, translated into two languages.
Note thatwe can think of DID + LANG as an identifier for amonolingual document instance, and DID + LANG +SID identifies a particular sentence in a monolingualdocument.124DID LANG SID TEXT1 deu 1 Der Hund bellte.1 eng 1 the dog barked.1 deu 2 Mein Vater ist Augenarzt.1 eng 2 My father is an optometrist.Figure 2: Two sentences with two translations.
These aresentence table records.In short, we implement the Aligned Text matrix asthree database tables.
All three tables have columnsDID, LANG, and TEXT.
The sentence table adds SID,and the word table adds WID instead of SID.
(Thewords are types, not tokens, hence are not associatedwith any particular sentence.
)4.2 The Analyzed Text tablesThe implementation of the Analyzed Text tables isstraightforward.
We add a column for the documentID, and we assume that sentence ID is relative tothe document.
We also represent the word token IDexplicitly, and take it to be relative to the sentence.Finally, we add a column for LANG, so that we havea single table rather than one per language.The first record from the German table in Figure 1is implemented as in Figure 3.
This is a record froma dependency parse.
Other varieties of analyzed textleave some of the columns empty, as discussed in theprevious section.There is a subtlety to note.
In the sentence table,the entry with DID 1, SID 1, and LANG ?deu?
is un-derstood to be a translation of the entry with DID 1,SID 1, and LANG ?eng.?
That is not the case withrecords in the analyzed-text table.
Word 1 in the En-glish sentence 1 of document 1 is not necessarily atranslation of word 1 in the German sentence 1 ofdocument 1.A few comments are in order about the meaningsof the columns.
The wordform is the attested, in-flected form of the word token.
The LEMMA pro-vides the lexical form, which is the headword un-der which one would find the word in a dictionary.The MORPH field provides a symbolic indicator ofthe relationship between the lemma and the word-form.
For example, ?Ku?he?
is the PL form of thelemma ?Kuh.
?This approach encompasses arbitrary morpholog-ical processes.
For example, Hebrew lomedet maybe represented as the PRESPTC.FEM.SG form oflmd, (?to learn?
).When we represent dictionaries, the records areword types rather than word tokens.
We assign adocument ID to the dictionary as a whole, but byconvention take the SID to be uniformly 0.Ultimately, the POS and GLOSS fields are in-tended to contain symbols from controlled vocab-ularies.
For the present, the choice of controlledvocabulary is up to the annotator.
For the GLOSSfield, an option that has the benefit of simplicity isto use the corresponding word from a reference lan-guage, but one might equally well use synset identi-fiers from WordNet, or concepts in some ontology.4.3 The auxiliary tablesThe auxiliary tables were not shown in the abstractdata model as depicted in Figure 1.
They primar-ily include metadata.
We assume a table that asso-ciates each document ID with a work, and a tablethat provides metadata for each work.
The Corpusas a whole is the sum of the works.In the spirit of not duplicating existing efforts, we?outsource?
the bulk of the metadata to OLAC (Si-mons and Bird, 2003).
If a work has an OLAC entry,we only need to associate the internal document IDto the OLAC identifier.There is some metadata information that wewould like to include for which we cannot refer toOLAC.?
Provenance: how the annotation was con-structed, e.g., who the annotator was, or whatsoftware was used if it was automatically cre-ated.?
Rights: copyright holder, license category cho-sen from a small set of interoperable licenses.?
Standards: allows the annotator to indicatewhich code sets are used for the MORPH, POS,and GLOSS fields.
We would like to be ableto specify a standard code set for each, in thesame way that we have specified ISO 639-3 forlanguage codes.
Consensus has not yet crystal-lized around any one standard, however.The auxiliary tables also associate documentswith media files.
We assume a table associatingdocument IDs with a media files, represented by125DID LANG SID WID FORM LEMMA MORPH POS GLOSS HEAD REL123 deu 1 1 Ku?he Kuh PL N cow 2 SBJFigure 3: A single word from a dependency parse.
This is a record from the analyzed-text table.their URLs, and a table associating sentences (DID+ SID) with locations in media files.Note that, as we have defined the file and tabu-lar implementations, there is no need for an explicitmapping between document IDs and filenames.
Afilename is always of the form did-lang.suffix,where the suffix is .txt for the document table,.snt for the sentence table, .tdi for the word ta-ble, and .tab for the analyzed-text table.
Each filecorresponds to a set of records in one of the tables.5 Cloud Storage and InterfaceA third interface to the Corpus is via an applica-tion programming interface.
We illustrate a possi-ble Python API using Amazon SimpleDB, a cloud-hosted tuple store accessed via a web services in-terface.2 An ?item?
is a collection of attribute-value pairs, and is stored in a ?domain.?
Items,attributes, and domains are roughly equivalent torecords, fields, and tables in a relational database.Unlike relational databases, new attributes and do-mains can be added at any time.Boto is a Python interface to Amazon Web Ser-vices that includes support for SimpleDB.3 The fol-lowing code shows an interactive session in which aconnection is established and a domain is created:>>> import boto>>> sdb = boto.connect_sdb(PUBLIC_KEY, PRIVATE_KEY)>>> domain = sdb.create_domain(?analyzed_text?
)We can create a new item, then use Python?s dic-tionary syntax to create attribute-value pairs, beforesaving it:>>> item = domain.new_item(?123?
)>>> item[?DID?]
= ?123?>>> item[?LANG?]
= ?deu?>>> item[?FORM?]
= ?Ku?he?>>> item[?GLOSS?]
= ?cow?>>> item[?HEAD?]
= ?2?>>> item.save()Finally, we can retrieve an item by name, or submita query using SQL-like syntax.2http://aws.amazon.com/simpledb/3http://code.google.com/p/boto/>>> sdb.get_attributes(domain, ?123?)?LANG?
: ?deu?, ?HEAD?
: ?2?, ?DID?
: ?123?,?FORM?
: ?Ku?he?, ?GLOSS?
: ?cow?>>> sdb.select(domain,... ?select DID, FORM from analyzed_text... where LANG = "deu"?)[?DID?
: ?123?, ?FORM?
: ?Ku?he?
]We have developed an NLTK ?corpus reader?which understands the Giza and NAACL03 formatsfor bilingual texts, and creates a series of records forinsertion into SimpleDB using the Boto interface.Other formats will be added over time.Beyond the loading of corpora, a range of queryand report generation functions are needed, as illus-trated in the following (non-exhaustive) list:?
lookup(lang=ENG, rev="1.2b3", ...): find allitems which have the specified attribute val-ues, returning a list of dictionaries; followingPython syntax, we indicate this variable num-ber of keyword arguments with **kwargs.?
extract(type=SENT, lang=[ENG, FRA, DEU],**kwargs): extract all aligned sentences involv-ing English, French, and German, which meetany further constraints specified in the keywordarguments.
(When called extract(type=SENT)this will extract all sentence alignments acrossall 7,000 languages, cf Figure 1.)?
dump(type=SENT, format="giza", lang=[ENG,FRA], **kwargs): dump English-French bitextin Giza format.?
extract(type=LEX, lang=[ENG, FRA, ...],**kwargs): produce a comparative wordlist forthe specified languages.?
dump(type=LEX, format="csv", lang=[ENG,FRA, ...], **kwargs): produce the wordlist incomma-separated values format.Additional functions will be required for discov-ery (which annotations exist for an item?
), naviga-tion (which file does an item come from?
), citation(which publications should be cited in connectionwith these items?
), and report generation (what typeand quantity of material exists for each language?
).126Further functionality could support annotation.We do not wish to enable direct modification ofdatabase fields, since everything in the Corpuscomes from contributed corpora.
Instead, we couldfoster user input and encourage crowdsourcing ofannotations by developing software clients that ac-cess the Corpus using methods such as the ones al-ready described, and which save any new annota-tions as just another work to be added to the Corpus.6 Further design considerationsVersioning.
When a work is contributed, it comeswith (or is assigned) a version, or ?edition.?
Multi-ple editions of a work may coexist in the Corpus, andeach edition will have distinct filenames and identi-fiers to avoid risk of collision.
Now, it may hap-pen that works reference each other, as when a basetext from one work is POS-tagged in another.
Forthis reason, we treat editions as immutable.
Modi-fications to a work are accumulated and released asa new edition.
When a new edition of a base textis released, stand-off annotations of that text (suchas the POS-tagging in our example) will need to beupdated in turn, a task that should be largely auto-mated.
A new edition of the annotation, anchored tothe new edition of the base text, is then released.
Theold editions remain unchanged, though they may beflagged as obsolete and may eventually be deleted.Licensing.
Many corpora come with license con-ditions that prevent them from being included.
Insome cases, this is due to license fees that are paidby institutional subscription.
Here, we need to ex-plore a new subscription model based on access.
Insome cases, corpus redistribution is not permitted,simply in order to ensure that all downloads occurfrom one site (and can be counted as evidence ofimpact), and so that users agree to cite the scholarlypublication about the corpus.
Here we can offer dataproviders a credible alternative: anonymized usagetracking, and an automatic way for authors to iden-tify the publications associated with any slice of theCorpus, facilitating comprehensive citation.Publication.
The Corpus will be an online publi-cation, with downloadable dated snapshots, evolv-ing continually as new works and editions are added.An editorial process will be required, to ensure thatcontributions are appropriate, and to avoid spam-ming.
A separate staging area would facilitatechecking of incoming materials prior to release.7 ConclusionWe have described the design and implementationof a Universal Corpus containing aligned and anno-tated text collections for the world?s languages.
Wefollow the same principles we set out earlier (Abneyand Bird, 2010, 2.2), promoting a community-leveleffort to collect bilingual texts and lexicons for asmany languages as possible, in a consistent formatthat facilitates machine processing across languages.We have proposed a normalized filestore model thatintegrates with current practice on the supply side,where corpora are freestanding works in a varietyof formats and multiple editions.
We have also de-vised a normalized database model which encom-passes the desired range of linguistic objects, align-ments, and annotations.
Finally, we have argued thatthis model scales, and enables a view of the Univer-sal Corpus as a vast matrix of aligned and analyzedtexts spanning the world?s languages, a radical de-parture from existing resource creation efforts in lan-guage documentation and machine translation.We invite participation by the community in elab-orating the design, implementing the storage model,and populating it with data.
Furthermore, we seekcollaboration in using such data as the basis forlarge-scale cross-linguistic analysis and modeling,and in facilitating the creation of easily accessiblelanguage resources for the world?s languages.ReferencesSteven Abney and Steven Bird.
2010.
The HumanLanguage Project: building a universal corpus of theworld?s languages.
In Proc.
48th ACL, pages 88?97.Association for Computational Linguistics.Sabine Buchholz, Erwin Marsi, Yuval Krymolowski, andAmit Dubey.
2006.
CoNLL-X shared task: Multi-lingual dependency parsing.
http://ilk.uvt.nl/conll/.
Accessed May 2011.Nikolaus P. Himmelmann.
1998.
Documentary and de-scriptive linguistics.
Linguistics, 36:161?195.Gary Simons and Steven Bird.
2003.
The Open Lan-guage Archives Community: An infrastructure for dis-tributed archiving of language resources.
Literary andLinguistic Computing, 18:117?128.127
