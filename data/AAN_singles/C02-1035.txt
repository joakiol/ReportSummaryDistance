AbstractTo support context-based multimodal interpre-tation in conversational systems, we have devel-oped a semantics-based representation tocapture salient information from user inputs andthe overall conversation.
In particular, wepresent three unique characteristics: fine-grained semantic models, flexible compositionof feature structures, and consistent representa-tion at multiple levels.
This representationallows our system to use rich contexts to resolveambiguities, infer unspecified information, andimprove multimodal alignment.
As a result, oursystem is able to enhance understanding of mul-timodal inputs including those abbreviated,imprecise, or complex ones.1 IntroductionInspired by earlier works on multimodal interfaces(e.g., Bolt, 1980; Cohen el al., 1996; Wahlster, 1991;Zancanaro et al, 1997), we are currently building anintelligent infrastructure, called Responsive Informa-tion Architect (RIA) to aid users in their informa-tion-seeking process.
Specifically, RIA engagesusers in a full-fledged multimodal conversation,where users can interact with RIA through multiplemodalities (speech, text, and gesture), and RIA canact/react through automated multimedia generation(speech and graphics) (Zhou and Pan 2001).
Cur-rently, RIA is embodied in a testbed, called RealHunterTM, a real-estate application to help users findresidential properties.As a part of this effort, we are building a seman-tics-based multimodal interpretation frameworkMIND (Multimodal Interpretation for Natural Dia-log) to identify meanings of user multimodal inputs.Traditional multimodal interpretation has beenfocused on integrating multimodal inputs togetherwith limited consideration on the interaction context.In a conversation setting, user inputs could be abbre-viated or imprecise.
Only by combining multipleinputs together often cannot reach a full understand-ing.
Therefore, MIND applies rich contexts (e.g.,conversation context and domain context) toenhance multimodal interpretation.
In support of thiscontext-based approach, we have designed a seman-tics-based representation to capture salient informa-tion from user inputs and the overall conversation.In this paper, we will first give a brief overview onmultimodal interpretation in MIND.
Then we willpresent our semantics-based representation and dis-cuss its characteristics.
Finally, we will describe theuse of this representation in context-based multimo-dal interpretation and demonstrate that, with this rep-resentation, MIND is able to process a variety of userinputs including those ambiguous, abbreviated andcomplex ones.2 Multimodal InterpretationTo interpret user multimodal inputs, MIND takesthree major processes as in Figure 1: unimodalunderstanding, multimodal understanding, and dis-course understanding.
During unimodal understand-ing, MIND applies modality specific recognition andunderstanding components (e.g., a speech recognizerand a language interpreter) to identify meaningsfrom each unimodal input, and captures those mean-ings in a representation called modality unit.
Duringmultimodal understanding, MIND combines seman-tic meanings of unimodal inputs (i.e., modalityunits), and uses contexts (e.g., conversation contextand domain context) to form an overall understand-ing of user multimodal inputs.
Such an overallunderstanding is then captured in a representationcalled conversation unit.
Furthermore, MIND alsoidentifies how an input relates to the overall conver-sation discourse through discourse understanding.
Inparticular, MIND uses a representation called con-versation segment to group together inputs that con-tribute to a same goal or sub-goal (Grosz and Sidner,1986).
The result of discourse understanding is anevolving conversation history that reflects the over-all progress of a conversation.Figure 2 shows a conversation fragment between auser and MIND.
In the first user input U1, the deicticFigure 1.
MIND componentsgesturespeechtextMultimodalInterpreterDiscourseInterpreterLanguageInterpreterGestureInterpreterSpeechRecognizerGestureRecognizerModality Unit(Speech& Text)Modality Unit(Gesture)Conversation UnitUnimodalUnderstandingDiscourseUnderstandingMultimodalUnderstandingOther RIA ComponentsConversationHistoryConversationSegmentMINDDomain,VisualContextsSemantics-based Representation for Multimodal Interpretation inConversational SystemsJoyce ChaiIBM T. J. Watson Research Center19 Skyline DriveHawthorne, NY 10532, USA{jchai@us.ibm.com}gesture (shown in Figure 3) is ambiguous.
It is notclear which object the user is pointing at: two housesnearby or the town of Irvington1.
The third user inputU3 by itself is incomplete since the purpose of theinput is not specified.
Furthermore, in U4, a singledeictic gesture overlaps (in terms of time) with both?this style?
and ?here?
from the speech input, it is hardto determine which one of those two referencesshould be aligned and fused with the gesture.
Finally,U5 is also complex since multiple objects (?these twohouses?)
specified in the speech input need to be uni-fied with a single deictic gesture.This example shows that user multimodal inputsexhibit a wide range of varieties.
They could beabbreviated, ambiguous or complex.
Fusing inputstogether often cannot reach a full understanding.
Toprocess these inputs, contexts are important.3 Semantics-based RepresentationTo support context-based multimodal interpretation,both representation of user inputs and representationof contexts are crucial.
Currently, MIND uses threetypes of contexts: domain context, conversation con-text, and visual context.
The domain context providesdomain knowledge.
The conversation context reflectsthe progress of the overall conversation.
The visualcontext gives the detailed semantic and syntacticstructures of visual objects and their relations.
In thispaper, we focus on representing user inputs and theconversation context.
In particular, we discuss twoaspects of representation: semantic models that cap-ture salient information and structures that representthose semantic models.3.1 Semantic ModelsWhen two people participate in a conversation, theirunderstanding of each other?s purposes forms strongconstraints on how the conversation is going to pro-ceed.
Especially, in a conversation centered aroundinformation seeking, understanding each other?sinformation needs is crucial.
Information needs canbe characterized by two main aspects: motivation forseeking the information of interest and the informa-tion sought itself.
Thus, MIND uses an intentionmodel to capture the first aspect and an attentionmodel to capture the second.
Furthermore, since userscan use different ways to specify their information ofinterest, MIND also uses a constraint model to cap-ture different types of constraints that are importantfor information seeking.3.1.1 Intention and AttentionIntention describes the purpose of a message.
In aninformation seeking environment, intention indicatesthe motivation or task related to the information ofinterest.
An intention is modeled by three dimensions:Motivator indicating one of the three high level pur-poses: DataPresentation, DataAnalysis (e.g., compari-son), and ExceptionHandling (e.g., clarification), Actspecifying whether the input is a request or a reply,and Method indicating a specific task, e.g., Search(activating the relevant objects based on some crite-ria) or Lookup (evaluating/retrieving attributes ofobjects).Attention relates to objects, relations that aresalient at each point of a conversation.
In an informa-tion seeking environment, it relates to the informationsought.
An attention model is characterized by sixdimensions.
Base indicates the semantic type of theinformation of interest (e.g., House, School, or Citywhich are defined in our domain ontology).
Topicspecifies the granularity of the information of interest(e.g., Instance or Collection).
Focus identifies the scopeof the topic as to whether it is about a particular fea-ture (i.e., SpecficAspect) or about all main features(i.e., MainAspect).
Aspect provides specific features ofthe topic.
Constraint describes constraints to be satis-fied (described later).
Content points to the actual data.The intention and attention models were derivedbased on preliminary studies of user informationneeds in seeking for residential properties.
The detailsare described in (Chai et al, 2002).For example, Figure 4(a-b) shows the Intention andAttention identified from U1 speech and gesture inputrespectively.
Intention in Figure 4(a) indicates the useris requesting RIA (Act: Request) to present her somedata (Motivator: DataPresentation) about attributes of1 The generated display has multiple layers, where the house iconsare on top of the Irvington town map.
Thus this deictic gesture couldeither refer to the town of Irvington or houses.Figure 2.
A conversation fragmentSpeech: Here is the comparison chart.Graphics: Show a chartR5:Speech: Compare these two houses with the previous house.Graphics: Point to the corner of the screen where two house icons aredisplayedU5:Speech: This is a Victorian style house.
I find seven Victorian housesin White Plains.Graphics: Show seven houses in White PlainsR4:Speech: Show me houses with this style around hereGesture: Point to a position east of Irvington on the mapU4:Speech: This house costs 320,000 dollars.Graphics: Highlight the house icon and show a pictureR3:Speech: What about this one?Gesture: Point to a house icon on the screenU3:Speech: The green house costs 250,000 dollars.R2:Speech: The green one.U2:Speech: Which house are you interested in?Graphics: Highlight two house iconsR1:Speech: How much is this?Gesture: Point to the screen (not directly on any object)U1:A collection of houses are shown on the map of IrvingtonFigure 3.
An example of graphics outputuser pointsherecertain object(s) (Method: Lookup).
The Attention indi-cates that the information of interest is about the price(Aspect: Price) of a certain object (Focus: Instance).
Theexact object is not known but is referred by a demon-strative ?this?
(in Constraint).
Intention in Figure 4(b)does not have any information since the high levelpurpose and the specific task cannot be identifiedfrom the gesture input.
Furthermore, because of theambiguity of the deictic gesture, three Attentions areidentified.
The first two Attentions are about houseinstances MLS0234765 and MLS0876542 (ID from Mul-tiple Listing Service) and the third is about the townof Irvington.3.1.2 ConstraintsIn an information seeking environment, based on theconversation context and the graphic display, userscan refer to objects using different types of refer-ences, for example, through temporal or spatial rela-tions, visual cues, or simply a deictic gesture.Furthermore, users can also search for objects usingdifferent constraints on data properties.
Therefore,MIND models two major types of constraints: refer-ence constraints and data constraints.
Reference con-straints characterize different types of references.Data constraints specify relations of data properties.A summary of our constraint model is shown inFigure 5.
Both reference constraints and data con-straints are characterized by six dimensions.
Categorysub-categorizes constraints (described later).
Mannerindicates the specific way such a constraint isexpressed.
Aspect indicates a feature (features) thisconstraint is concerned about.
Relation specifies therelation to be satisfied between the object of interestand other objects or values.
Anchor provides a particu-lar value, object or a reference point this constraintrelates to.
Number specifies cardinal numbers that areassociated with the constraint.Reference ConstraintsReference constraints are further categorized intofour categories: Anaphora, Temporal, Visual, and Spatial.An anaphora reference can be expressed through pro-nouns such as ?it?
or ?them?
(Pronoun), demonstra-tives such as ?this?
or ?these?
(Demonstrative), here orthere (Here/There), or proper names such as ?Lyn-hurst?
(ProperNoun).
An example is shown inFigure 4(a), where a demonstrative ?this?
(Manner:Demonstrative-This) is used in the utterance ?this house?to refer to a single house object (Number: 1).
Note thatManner also keeps track of the specific type of theterm.
The subtle difference between terms can pro-vide additional cues for resolving references.
Forexample, the different use of ?this?
and ?that?
mayindicate the recency of the referent in the user mentalmodel of the discourse, or the closeness of the refer-ent to the user?s visual focus.Temporal references use temporal relations to referto entities that occurred in the prior conversation.Manner is characterized by Relative and Absolute.
Rela-tive indicates a temporal relation with respect to a cer-tain point in a conversation, and Absolute specifies atemporal relation regarding to the whole interaction.Relation indicates the temporal relations (e.g., Precedeor Succeed) or ordinal relations (e.g., first).
Anchorindicates a reference point.
For example, as inFigure 6(a), a Relative temporal constraint is usedsince ?the previous house?
refers to the house that pre-cedes the current focus (Anchor: Current) in the conver-sation history.
On the other hand, in the input: ?thefirst house you showed me,?
an Absolute temporal con-straint is used since the user is interested in the firsthouse shown to her at the beginning of the entire con-versation.Spatial references describe entities on the graphicdisplay in terms of their spatial relations.
Manner isagain characterized by Absolute and Relative.
Absoluteindicates that entities are specified through orienta-tions (e.g., left or right, captured by Relation) withrespect to the whole display screen (Anchor: Display-Frame).
In contrast, Relative specifies that entities aredescribed through orientations with respect to a par-ticular sub-frame (Anchor: FocusFrame, e.g., an areaFigure 4.
Intention and Attention for U1 unimodal inputsMotivator: DataPresentationAct: RequestMethod: Lookup(b) U1 gesture: pointingBase: HouseTopic: InstanceContent: {MLS0234765}IntentionAttentionIntentionTopic: InstanceFocus: SpecificAspectAspect: PriceConstraint:AttentionCategory: AnaphoraManner: Demonstrative(THIS)Number: 1Base: CityTopic: InstanceContent: {?Irvington?
}(a) U1 speech: ?How much is this?Base: HouseTopic: InstanceContent: {MLS0876542}Figure 5.
Constraint modelMannerCategory Aspect Relation Anchor NumberAnaphora Demonstrative,Pronoun,Here/There,ProperNoun,Temporal Relative,AbsoluteSpatialProcede,Succeed,Ordinal(e.g., first)VisualAttributiveCurrent,ObjectDisplayFrame,FocusFrame,ObjectOrientation(e.g., Left,Right )Multiple,Cardinal-number(e.g., 1, 2)Relative,AbsoluteComparativeComparative,Superlative,FuzzyVisual-Properties(e.g., Color,Highlight)Data Features(e.g., Price,Size)Less-Than,Equals,Greater-ThanEqualsDataValue,ValueOfObject,ObjectDataValue,ValueOfObject,ObjectReferenceConstraintsDataConstraints-----Figure 6.
Temporal and visual reference constraintsBase: HouseTopic: InstanceConstraint:AttentionCateogry: TemporalManner: RelativeRelation: PrecedeAnchor: CurrentNumber: 1(a) ?
the previous house?
(b) ?
the green house?Base: HouseTopic: InstanceConstraint:AttentionCateogry: VisualManner: ComparativeAspect: ColorRelation: EqualsAnchor: ?Green?Number: 1with highlighted objects) or another object.Visual references describe entities on the graphicoutput using visual properties (such as displaying col-ors or shapes) or visual techniques (such as high-light).
Manner of Comparative indicates a visual entityis compared with another value (captured by Anchor).Aspect indicates the visual entity used (such as Colorand Shape, which are defined in our domain ontol-ogy).
Relation specifies the relation to be satisfiedbetween the visual entity and some value.
For exam-ple, constraint used in the input ?the green house?
isshown in Figure 6(b).
It is worth mentioning that dur-ing reference resolution, the color Green will be fur-ther mapped to the internal color encoding used bygraphics generation.Data ConstraintsData constraints describe objects in terms of theiractual data attributes (Category: Attributive).
The Man-ner of Comparative indicates the constraint is about acomparative relation between (aspects of) the desiredentities with other entities or values.
Superlative indi-cates the constraint is about minimum or maximumrequirement(s) for particular attribute(s).
Fuzzy indi-cates a fuzzy description on the attributes (e.g.,?cheap house?).
For example, for the input ?housesunder 300,000 dollars?
in Figure 7(a), Manner is Compar-ative since the constraint is about a ?less than?
rela-tionship (Relation: Less-Than) between the price(Aspect: Price) of the desired object(s) and a particularvalue (Anchor: ?300000 dollars?).
For the input ?3 largesthouses?
in Figure 7(b), Manner is Superlative since it isabout the maximum (Relation: Max) requirement onthe size of the houses (Aspect: Size).The refined characterization of different constraintsprovides rich cues for MIND to identify objects ofinterest.
In an information seeking environment, theobjects sought can come from different sources.
Theycould be entities that have been described earlier inthe conversation, entities that are visible on the dis-play, or entities that have never been mentioned orseen but exist in a database.
Thus, fine-grained con-straints allow MIND to determine where and how tofind the information of interest.
For example, tempo-ral constraints help MIND navigate the conversationhistory by providing guidance on where to start,which direction to follow in the conversation history,and how many to look for.Our fine-grained semantic models of intention,attention and constraints characterize user informa-tion needs and therefore enable the system to comeup with an intelligent response.
Furthermore, thesemodels are domain independent and can be applied toany information seeking applications (for structuredinformation).3.1.3 Representing User InputsGiven the semantic models of intention, attention andconstraints, MIND represents those models using acombination of feature structures (Carpenter, 1992).This representation is inspired by the earlier works(Johnston et al, 1997; Johnston, 1998) and offers aflexibility to accommodate complex inputs.
Specifi-cally, MIND represents intention, attention and con-straints identified from user inputs as a result of bothunimodal understanding and multimodal understand-ing.During unimodal understanding, MIND applies adecision tree based semantic parser on natural lan-guage inputs (Jelinek et al, 1994) to identify salientinformation.
For the gesture input, MIND applies asimple geometry-based recognizer.
As a result, infor-mation from each unimodal input is represented in amodality unit.
We have seen several modality units(in Figure 4, Figure 6, and Figure 7), where intention,attention and constraints are represented in featurestructures.
Note that only features that can be instan-tiated by information from the user input are includedin the feature structure.
For example, since the exactobject cannot be identified from U1 speech input, theContent feature is not included in its Attention structure(Figure 4a).
In addition to intention, attention andconstraints, a modality unit also keeps a time stampthat indicates when a particular input takes place.This time information is used for multimodal align-ment which we do not discuss here.Depending on the complexity of user inputs, therepresentation can be composed by a flexible combi-Figure 7.
Attributive data constraintsBase: HouseTopic: CollectionConstraint:AttentionCateogry: AttributiveManner: ComparativeAspect: PriceRelation: EqualsAnchor: ?300000 dollars?
(a) ?houses under 300,000 dollars?Base: HouseTopic: CollectionConstraint:AttentionCateogry: AttributiveManner: SuperlativeAspect: SizeRelation: MaxNumber: 3(a) ?3 largest houses?Figure 8.
Attention structures for U4Base: HouseTopic: CollectionConstraint:Attention (A1)Category: AttributeiveManner: ComparativeAspect: StyleRelation: EqualsAnchor: *Topic: InstanceConstraint:Category: AnaphoraManner: Demonstrative(THIS)Number: 1Attention (A2)Category: AttributiveManner: ComparativeAspect: LocationRelation: EqualsAnchor: *Base: GeoLocationTopic: InstanceConstraint:Category: AnaphoraManner: HEREAttention (A3)Constraint:(a) Attention structure in the modality unit for U4 speech inputBase: HouseTopic: CollectionConstraint:Attention (A1)Category: AttributeiveManner: ComparativeAspect: StyleRelation: EqualsAnchor: ?Victorian?Category: AttributiveManner: ComparativeAspect: LocationRelation: EqualsAnchor: ?White Plains?Constraint:(b) Attention structure in the conversation unit for U4 speech inputnation of different feature structures.
Specifically, anattention structure may have a constraint structure asits feature, and on the other hand, a constraint struc-ture may also include another attention structure.For example, U4 in Figure 2 is a complex input,where the speech input ?what about houses with thisstyle around here?
consists of multiple objects with dif-ferent relations.
The modality unit created for U4speech input is shown in Figure 8(a).
The Attentionfeature structure (A1) contains two attributive con-straints indicating that the objects of interest are acollection of houses that satisfy two attributive con-straints.
The first constraint is about the style (Aspect:Style), and the second is about the location.
Both ofthese constraints are related to other objects (Manner:Comparative), which are represented by Attention struc-tures A2 and A3 through Anchor respectively.
A2 indi-cates an unknown object that is referred by aDemonstrative reference constraint (this style), and A3indicates a geographic location object referred byHERE.
Since these two references are overlapped witha single deictic gesture, it is hard to decide which oneshould be unified with the gesture input.
We willshow in Section 4.3 that the fine-grained representa-tion in Figure 8(a) allows MIND to use contexts toresolve these two references and improve alignment.During multimodal understanding, MIND com-bines information from modality units together andgenerates a conversation unit that represents the over-all meaning of user multimodal inputs.
A conversa-tion unit also has the same type of intention andattention feature structures, as well as the featurestructure for data constraints.
Since references areresolved during the multimodal understanding pro-cess, the reference constraints are no longer present inconversation units.
For example, once two referencesin Figure 8(a) are resolved during multimodal under-standing (details are described in Section 4.3), andMIND identifies ?this style?
is ?Victorian?
and ?here?
is?White Plains?, it creates a conversation unit represent-ing the overall meanings of this input in Figure 8(b).3.2 Representing Conversation ContextMIND uses a conversation history to represent theconversation context based on the goals or sub-goalsof user inputs and RIA outputs.
For example, in theconversation fragment mentioned earlier (Figure 2),the first user input (U1) initiates a goal of looking upthe price of a particular house.
Due to the ambiguousgesture input, in the next turn, RIA (R2) initiates asub-goal of disambiguating the house of interest.
Thissub-goal contributes to the goal initiated by U1.
Oncethe user replies with the house of interest (U2), thesub-goal is fulfilled.
Then RIA gives the price infor-mation (R2), and the goal initiated by U1 is accompol-ished.
To reflect this progress, our conversationhistory is a hierarchical structure which consists ofconversation segments and conversation units (inFigure 9).
As mentioned earlier, a conversation unitrecords user (rectangle U1, U2) or RIA (rectangle R1,R2) overall meanings at a single turn in the conversa-tion.
These units can be grouped together to form aconversation segment (oval DS1, DS2) based on theirgoals and sub-goals.
Furthermore, a conversation seg-ment contains not only intention and attention, butalso other information such as the conversation initi-ating participant (Initiator).
In addition to conversationsegments and conversation units, a conversation his-tory also maintains different relations between seg-ments and between units.
Details can be found in(Chai et al, 2002).Another main characteristic of our representation isthe consistent representation of intention and atten-tion across different levels.
Just like modality unitsand conversation units, conversation segments alsoconsist of the same type of intention and attentionfeature structures (as shown in Figure 9).
This consis-tent representation not only supports unificationbased multimodal fusion, but also enables context-based inference to enhance interpretation (describedlater).We have described our semantics-based representa-tion and presented three characteristics: fine-grainedsemantic models, flexible composition, and consis-tent representation.
Next we will show that how thisrepresentation is used effectively in the multimodalinterpretation process.4 The Use of Representation in MultimodalInterpretationAs mentioned earlier, multimodal interpretation inMIND consists of three processes: unimodal under-standing, multimodal understanding and discourseunderstanding.
Here we focus on multimodal under-standing.
The key difference between MIND and ear-lier works is the use of rich contexts to improveunderstanding.
Specifically, multimodal understand-ing consists of two sub-processes: multimodal fusionand context-based inference.
Multimodal fusion fusesintention and attention structures (from modalityunits) for unimodal inputs and forms a combined rep-resentation.
Context-based inference uses rich con-Figure 9.
A fragment of a conversation historyMotivator: DataPresentationAct: RequestMethod: LookupIntentionBase: HouseTopic: InstanceFocus: SpecificAspectAspect: PriceContent: {MLS0234765|MLS0876542}AttentionU1Motivator: DataPresentationMethod: LookupIntentionBase: HouseTopic: InstanceFocus: SpecificAspectAspect: PriceContent: {MLS0234765}AttentionDS1R2Initiator: UserMotivator: ExceptionHandlingMethod: DisambiguateIntentionBase: HouseTopic: InstanceContent: {MLS0234765 |MLS0876542}AttentionDS1Initiator: RIAR1 U2Intention?.Attention?.texts to improve interpretation by resolvingambiguities, deriving unspecified information, andimproving alignment.4.1 Resolving AmbiguitiesUser inputs could be ambiguous.
For example, in U1,the deictic gesture is not directly on a particularobject.
Fusing intention and attention structures fromeach individual inputs presents some ambiguities.
Forexample, in Figure 4(b), there are three Attentionstructures for U1 gesture input.
Each of them can beunified with the Attention structure from U1 speechinput (in Figure 4a).
The result of fusion is shown inFigure 10(a).
Since the reference constraint in thespeech input (Number: 1 in Figure 4a) indicates thatonly one attention structure is allowed, MIND usescontexts to eliminate inconsistent structures.
In thiscase, A3 in Figure 10(a) indicates the information ofinterest is about the price of the city Irvington.
Basedon the domain knowledge that the city object cannothave the price feature, A3 is filtered out.
As a result,both A1 and A2 are potential interpretation.
Therefore,the Content in those structures are combined using adisjunctive relation as in Figure 10(b).
Based on thisrevised conversation unit, RIA is able to arrange thefollow-up question to further disambiguate the houseof interest (R2 in Figure 2).
This example shows that,modeling semantic information by fine-graineddimensions supports the use of domain knowledge incontext-based inference, and can therefore resolvesome ambiguities.4.2 Deriving Unspecified InformationIn a conversation setting, user inputs are often abbre-viated.
Users tend to only provide new informationwhen it is their turn to interact.
Sometimes, fusingindividual modalities together still cannot provideoverall meanings of those inputs.
For example, aftermultimodal fusion, the conversation unit for U3(?What about this one?)
does not give enough informa-tion on what the user exactly wants.
The motivationand task of this input is not known as in Figure 11(a).Only based on the conversation context, is MINDable to identify the overall meaning of this input.
Inthis case, based on the most recent conversation seg-ment (DS1) in Figure 9 (also as in Figure 11b), MINDis able to derive Motivator and Method features fromDS1 to update the conversation unit for U3(Figure 11c).
As a result, this revised conversationunit provides the overall meaning that the user isinterested in finding out the price information aboutanother house MLS7689432.
Note that it is importantto maintain a hierarchical conversation history basedon goals and subgoals.
Without such a hierarchicalstructure, MIND would not be able to infer the moti-vation of U3.
Furthermore, because of the consistentrepresentation of intention and attention at both thediscourse level (in conversation segments) and theinput level (in conversation units), MIND is able todirectly use conversation context to infer unspecifiedinformation and enhance interpretation.4.3 Improving AlignmentIn a multimodal environment, users could use differ-ent ways to coordinate their speech and gestureinputs.
In some cases, one reference/object men-tioned in the speech input coordinates with one deic-tic gesture (U1, U3).
In other cases, several references/objects in the speech input are coordinated with onedeictic gesture (U4, U5).
In the latter cases, only usingtime stamps often cannot accurately align and fusethe respective attention structures from each modal-ity.
Therefore, MIND uses contexts to improve align-ment based on our semantics-based representation.For example, from the speech input in U4 (?show mehouses with this style around here?
), three Attention struc-tures are generated as shown in Figure 8(a).
From thegesture input, only one Attention structure is generatedwhich corresponds to the city of White Plains.
Sincethe gesture input overlaps with both ?this style?
(corre-sponding to A2) and ?here?
(corresponding to A3),there is no obvious temporal relation indicatingwhich of these two references should be unified withthe deictic gesture.
In fact, both A2 and A3 are poten-tial candidates.
Based on the domain context that acity cannot have a feature Style, MIND determinesthat the deictic gesture is actually resolving the refer-Figure 10.
Resolving ambiguity for U1Motivator: DataPresentationAct: RequestMethod: LookupIntentionBase: HouseTopic: InstanceFocus: SpecificAspectAspect: PriceContent:{MLS0234765}AttentionBase: HouseTopic: InstanceFocus: SpecificAspectAspect: PriceContent:{MLS0876542}Base: CityTopic: InstanceFocus: SpecificAspectAspect: PriceContent:{?Irvington?
}Motivator: DataPresentationAct: RequestMethod: LookupIntentionBase: HouseTopic: InstanceFocus: SpecificAspectAspect: PriceContent:{MLS0234765 |MLS0876542}AttentionA1A2A3(a) Conversation unit for U1 as aresult of multimodal fusion(b) Revised conversation unit for U1 as aresult of context-based inferenceFigure 11.
Deriving unspecified information for U3Act: RequestIntentionBase: HouseTopic: InstanceContent: {MLS7689432}AttentionU3Motivator: DataPresentationMethod: LookupIntentionBase: HouseTopic: InstanceFocus: SpecificAspectAspect: PriceContent: {MLS0234765}AttentionDS1Initiator: User(a) Conversation unit for U3 as aresult of multimodal fusion(b) Conversation segment DS1 inthe conversation historyMotivator: DataPresentationAct: RequestMethod: LookupIntentionBase: HouseTopic: InstanceFocus: SpecificAspectAspect: PriceContent: {MLS7689432}AttentionU1(c) Revised conversation unit for U3 asa result of context-based inferenceence of ?here?.
To resolve the reference of ?this style?,MIND uses the visual context which indicates ahouse is highlighted on the screen.
A recent study(Kehler, 2000) shows that objects in the visual focusare often referred by pronouns, rather than by fullnoun phrases or deictic gestures.
Based on this study,MIND is able to infer that most likely ?this style?refers to the style of the highlighted house(MLS7689432).
Suppose the style is ?Victorian?, thenMIND is able to figure out that the overall meaningof U4 is looking for houses with a Victorian style andlocated in White Plains (as shown in Figure 8b).Furthermore, for U5 (?Comparing these two houseswith the previous house?
), there are two Attention struc-tures (A1 and A2) created for the speech input as inFigure 12(a).
A1 corresponds to ?these two houses?,where the Number feature in the reference constraint isset 2.
Although there is only one deictic gesturewhich points to two potential houses (Figure 12b),MIND is able to figure out that this deictic gesture isactually referring to a group of two houses rather thanan ambiguous single house.
Although the gestureinput in U5 is the same kind as that in U1, because ofthe fine-grained information captured from thespeech input (i.e., Number feature), MIND processesthem differently.
For the second reference of ?previoushouse?
(A2 in Figure 12a), based on the informationcaptured in the temporal constraint, MIND searchesthe conversation history and finds the most recenthouse explored (MLS7689432).
Therefore, MIND isable to reach an overall understanding of U5 that theuser is interested in comparing three houses (as inFigure 12c).5 ConclusionTo facilitate multimodal interpretation in conversa-tional systems, we have developed a semantics-basedrepresentation to capture salient information fromuser inputs and the overall conversation.
In this paper,we have presented three unique characteristics of ourrepresentation.
First, our representation is based onfine grained semantic models of intention, attentionand constraints that are important in informationseeking conversation.
Second, our representation iscomposed by a flexible combination of feature struc-tures and thus supports complex user inputs.
Third,our representation of intention and attention is consis-tent at different levels and therefore facilitates con-text-based interpretation.
This semantics-basedrepresentation allows MIND to use contexts toresolve ambiguities, derive unspecified informationand improve alignment.
As a result, MIND is able toprocess a large variety of user inputs including thoseincomplete, ambiguous or complex ones.6 AcknowledgementThe author would like to thank Shimei Pan andMichelle Zhou for their contributions on semanticmodels.ReferencesBolt, R. (1980) Voice and gesture at the graphics inter-face.
Computer Graphics, pages 262-270.Carpenter, R. (1992) The logic of typed feature struc-tures.
Cambridge University Press.Chai, J.; Pan, S.; and Zhou, M. X.
(2002) MIND: A Se-mantics-based multimodal interpretation frameworkfor conversational systems.
To appear in Proceedingsof International CLASS Workshop on Natural, Intelli-gent and Effective Interaction in Multimodal DialogSystems.Cohen, P.; Johnston, M.; McGee, D.; S. Oviatt, S.; Pitt-man, J.; Smith, I.; Chen, L; and Clow, J.
(1996) Quick-set: Multimodal interaction for distributedapplications.
Proc.
ACM MM'96, pages 31-40.Grosz, B. J. and Sidner, C. (1986) Attention, intentions,and the structure of discourse.
Computational Linguis-tics, 12(3):175-204.Jelinek, F.; Lafferty, J.; Magerman, D. M.; Mercer, R.and Roukos, S. (1994) Decision tree parsing using ahidden derivation model.
Proc.
Darpa Speech and Nat-ural Language Workshop.Johnston, M.; Cohen, P. R.; McGee, D.; Oviatt, S. L.;Pittman, J.
A.; and Smith, I.
(1997) Unification basedmultimodal integration.
Proc.
35th ACL, pages 281-288.Johnston, M. (1998) Unification-based multimodal pars-ing.
Proc.
COLING-ACL'98.Kehler, A.
(2000) Cognitive status and form of referencein multimodal human-computer interaction.
Proc.AAAI?01, pages 685?689.Wahlster, W. (1998) User and discourse models for mul-timodal communication.
In M. Maybury and W. Wahl-ster, editors, Intelligent User Interfaces, pages 359-370.Zancanaro, M.; Stock, O.; and Strapparava, C. (1997)Multimodal interaction for information access: Ex-ploiting cohesion.
Computational Intelligence,13(4):439-464.Zhou, M. X. and Pan, S. (2001) Automated authoring ofcoherent multimedia discourse for conversation sys-tems.
Proc.
ACM MM?01, pages 555?559.Figure 12.
Improving alignment for U5Motivator: DataAnalysisAct: RequestMethod: CompareIntentionBase: HouseTopic: CollectionFocus: MainAspectConstraint:AttentionBase: HouseTopic: InstanceContent:{MLS0765489}AttentionA1(a) Modality unit for U5 speech input(b) Modality unit for U5 gesture inputCategory: AnaphoraManner: DemonstrativeNumber: 2Base: HouseTopic: InstanceFocus: MainAspectConstraint:A2Category: TemporalManner: RelativeRelation: PrecedeAnchor: CurrentNumber: 1Base: HouseTopic: InstanceContent:{MLS0468709}Motivator: DataAnalysisAct: RequestMethod: CompareIntentionBase: HouseTopic: CollectionFocus: MainAspectContent: {MLS0468709,MLS0765489,MLS7689432}AttentionA1(c) Conversation unit for U5
