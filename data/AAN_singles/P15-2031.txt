Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 186?191,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsA Unified Learning Framework of Skip-Grams and Global VectorsJun Suzuki and Masaaki NagataNTT Communication Science Laboratories, NTT Corporation2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237 Japan{suzuki.jun, nagata.masaaki}@lab.ntt.co.jpAbstractLog-bilinear language models such asSkipGram and GloVe have been proven tocapture high quality syntactic and seman-tic relationships between words in a vectorspace.
We revisit the relationship betweenSkipGram and GloVe models from a ma-chine learning viewpoint, and show thatthese two methods are easily merged intoa unified form.
Then, by using the unifiedform, we extract the factors of the config-urations that they use differently.
We alsoempirically investigate which factor is re-sponsible for the performance differenceoften observed in widely examined wordsimilarity and analogy tasks.1 IntroductionNeural-network-inspired word embedding meth-ods such as Skip-Gram (SkipGram) have beenproven to capture high quality syntactic and se-mantic relationships between words in a vectorspace (Mikolov et al, 2013a).
A similar embed-ding method, called ?Global Vector (GloVe)?, wasrecently proposed.
It has demonstrated significantimprovements over SkipGram on the widely used?Word Analogy?
and ?Word Similarity?
benchmarkdatasets (Pennington et al, 2014).
Unfortunately,a later deep re-evaluation has revealed that GloVedoes not consistently outperform SkipGram (Levyet al, 2015); both methods provided basically thesame level of performance, and SkipGram evenseems ?more robust (not yielding very poor re-sults)?
than GloVe.
Moreover, some other papers,i.e., (Shi and Liu, 2014), and some researchersin the community have discussed a relationship,and/or which is superior, SkipGram or GloVe.From this background, we revisit the relation-ship between SkipGram and GloVe from a ma-chine learning viewpoint.
We show that it is nat-V : set of vocabulary (set of words)|V| : vocabulary size, or number of words in Vi : index of the input vector, where i ?
{1, .
.
.
, |V|}j : index of the output vector, where j ?
{1, .
.
.
, |V|}ei: input vector of the i-th word in Voj: output vector of the j-th word in VIf i = j, then eiand ojare the input and output vec-tors of the same word in V , respectively.D : number of dimensions in input and output vectorsmi,j: (i, j)-factor of matrix Msi,j: dot product of input and output vectors, si,j= ei?
ojD : training data, D = {(in, jn)}Nn=1?(?)
: objective function?(?)
: sigmoid function, ?
(x) =11+exp(?x)ci,j: co-occurrence of the i-th and j-th words in DD?
: (virtual) negative sampling datac?i,j: co-occurrence of the i-th and j-th words in D?k : hyper-parameter of the negative sampling?(?)
: ?weighting factor?
of loss function?(?)
: loss functionTable 1: List of notations used in this paper.ural to think that these two methods are essen-tially identical, with the chief difference beingtheir learning configurations.The final goal of this paper is to provide a uni-fied learning framework that encompasses the con-figurations used in SkipGram and GloVe to gain adeeper understanding of the behavior of these em-bedding methods.
We also empirically investigatewhich learning configuration most clearly eluci-dates the performance difference often observed inword similarity and analogy tasks.2 SkipGram and GloVeTable 1 shows the notations used in this paper.2.1 Matrix factorization view of SkipGramSkipGram can be categorized as one of thesimplest neural language models (Mnih andKavukcuoglu, 2013).
It generally assigns two dis-tinct D-dimensional vectors to each word in vo-cabulary V; one is ?input vector?, and the other is?output vector?1.1These two vectors are generally referred to as ?word (ortarget) vector?
and ?context vector?.
We use the terms ?in-186Roughly speaking, SkipGram models word-to-word co-occurrences, which are extracted withinthe predefined context window size, by the in-put and output vectors.
Recently, SkipGramhas been interpreted as implicitly factorizing thematrix, where the factors are calculated fromco-occurrence information (Levy and Goldberg,2014).
Let mi,jbe the (i, j)-factor of matrix Mto be ?implicitly?
factorized by SkipGram.
Skip-Gram approximates each mi,jby the inner prod-uct of the corresponding input and output vectors,that is:mi,j?
ei?
oj, (1)2.1.1 SkipGram with negative samplingThe primitive training sample for SkipGram is apair of a target word and its corresponding con-text word.
Thus, we can represent the trainingdata of SkipGram as a list of input and output in-dex pairs, that is, D = {(in, jn)}Nn=1.
Thus theestimation problem of ?SkipGram with negativesampling (SGNS)?
is defined as the minimizationproblem of objective function ?:?
=??(in,jn)?Dlog(?(ein?
ojn))??(in,jn)?D?log(1?
?(ein?
ojn)),(2)where the optimization parameters are eiand ojfor all i and j.
Note that we explicitly represent thenegative sampling data D?
(Goldberg and Levy,2014).Let us assume that, in a preliminary step, wecount all co-occurrences in D. Then, the SGNSobjective in Eq.
2 can be rewritten as follows by asimple reformulation:?
=??i?j(ci,jlog(?(ei?
oj))+c?i,jlog(1?
?(ei?
oj))).
(3)Here, let us substitute ei?
ojin Eq.
3 for si,j,and then assume that all si,jare free parameters.Namely, we can freely select the value of si,jin-dependent from any other si?,j?, where i 6= i?andj 6= j?, respectively.
The partial derivatives of ?with respect to si,jtake the following form:?si,j?
=?(ci,j(1?
?(si,j))?
c?i,j?(si,j)).(4)put?
and ?output?
to reduce the ambiguity since ?word?
and?context?
are exchangeable by the definition of model (i.e.,SkipGram or CBoW).The minimizer can be obtained when ?si,j?
= 0for all si,j.
By using this relation, we can obtainthe following closed form solution:si,j= log(ci,jc?i,j).
(5)Overall, SGNS approximates the log of the co-occurrence ratio between ?real?
training data Dand ?virtual?
negative sampling data D?by the in-ner product of the corresponding input and outputvectors in terms of minimizing the SGNS objec-tive written in Eq.
2, and Eq.
3 as well.
Therefore,we can obtain the following relation for SGNS:mi,j= log(ci,jc?i,j)?
ei?
oj.
(6)Note that the expectation of c?i,jiskcicj|D|if thenegative sampling is assumed to follow unigramprobabilitycj|D|, and the negative sampling data isk-times larger than the training dataD, where ci=?jci,jand cj=?ici,j2.
The above matches?shifted PMI?
as described in (Levy and Goldberg,2014) when we substitute c?i,jforkcicj|D|in Eq.
6,In addition, the word2vec implementationuses a smoothing factor ?
to reduce the selec-tion of high-occurrence-frequency words duringthe negative sampling.
The expectation of c?i,jcan then be written as: kci(cj)??j?(cj?)?.
We referto log(ci,j?j?(cj?)?kci(cj)?
)as ?
?-parameterized shiftedPMI (SPMIk,?
)?.2.2 Matrix factorization view of GloVeThe GloVe objective is defined in the followingform (Pennington et al, 2014):?
=?i?j?(ci,j)(ei?
oj?
log(ci,j))2,(7)where ?(?)
represent a ?weighting function?.
Inparticular, ?(?)
satisfies the relations 0 ?
?
(x) <?, and ?
(x) = 0 if x = 0.
For example, thefollowing weighting function has been introducedin (Pennington et al, 2014):?
(x) = min(1,(x/xmax)?).
(8)This is worth noting here that the original GloVeintroduces two bias terms, biand bj, and defines2Every input of the i-th word samples k words.
Therefore,the negative sampling number is kci.
Finally, the expectationcan be obtained by multiplying count kciby probabilitycj|D|.187configuration SGNS GloVetraining unit sample-wise co-occurrenceloss function logistic (Eq.
11) squared (Eq.
12)neg.
sampling explicit no samplingweight.
func.
?(?)
fixed to 1 Eq.
8fitting function SPMIk,?log(ci,j)bias none biand bjTable 2: Comparison of the different configura-tions used in SGNS and GloVe.ei?oj+ bi+ bjinstead of just ei?ojin Eq.
7.
Forsimplicity and ease of discussion, we do not ex-plicitly introduce bias terms in this paper.
This isbecause, without loss of generality, we can embedthe effect of the bias terms in the input and outputvectors by introducing two additional dimensionsfor all eiand oj, and fixing parameters ei,D+1= 1and oj,D+2= 1.According to Eq.
7, GloVe can also be viewedas a matrix factorization method.
Differentfrom SGNS, GloVe approximates the log of co-occurrences:mi,j= log(ci,j)?
ei?
oj, (9)3 Unified Form of SkipGram and GloVeAn examination of the differences between Eqs.
6and 9 finds that Eq.
6 matches Eq.
9 if c?i,j= 1.Recall that c?i,jis the number of co-occurrencesof (i, j) in negative sampling data D?.
Therefore,what GloVe approximates is SGNS when the neg-ative sampling data D?is constructed as 1 for allco-occurrences.
From the viewpoint of matrix fac-torization, GloVe can be seen as a special case ofSGNS, in that it utilizes a sort of uniform negativesampling method.Our assessment of the original GloVe papersuggests that the name ?Global Vector?
mainlystands for the architecture of the two stage learn-ing framework.
Namely, it first counts all theco-occurrences in D, and then, it leverages thegathered co-occurrence information for estimating(possibly better) parameters.
In contrast, the name?SkipGram?
stands mainly for the model type;how it counts the co-occurrences in D. The keypoints of these two methods seems different anddo not conflict.
Therefore, it is not surprising totreat these two similar methods as one method; forexample, SkipGram model with two-stage globalvector learning.
The following objective functionis a generalized form that subsumes Eqs.
3 and 7:?
=?i?j?(ci,j)?
(ei,oj, ci,j, c?i,j).
(10)hyper-parameter selected valueword2vec glovecontext window (W ) 10sub (Levy et al, 2015) dirty, t = 10?5?del (Levy et al, 2015) use 400,000 most frequent wordscds (Levy et al, 2015) ?
= 3/4 ?w+c (Levy et al, 2015) e + oweight.
func.
(?, xmax) ?
3/4, 100initial learning rate (?)
0.025 0.05# of neg.
sampling (k) 5 ?# of iterations (T ) 5 20# of threads 56# of dimensions (D) 300Table 3: Hyper-parameters in our experiments.In particular, the original SGNS uses ?
(ci,j) = 1for all (i, j), and logistic loss function:?
(ei,oj, ci,j, c?i,j) = ci,jlog(?(ei?
oj))+c?i,jlog(1?
?(ei?
oj)).
(11)In contrast, GloVe uses a least squared loss func-tion:?
(ei,oj, ci,j, c?i,j) =(ei?
oj?
log(ci,jc?i,j))2.
(12)Table 2 lists the factors of each configuration useddifferently in SGNS and GloVe.Note that this unified form also includesSkipGram with noise contrastive estimation(SGNCE) (Mnih and Kavukcuoglu, 2013), whichapproximatesmi,j= log(ci,jkcj) in matrix factoriza-tion view.
This paper omits a detailed discussionof SGNCE for space restrictions.4 ExperimentsFollowing the series of neural word embedding pa-pers, our training data is taken from a Wikipediadump (Aug. 2014).
We tokenized and lowercasedthe data yielding about 1.8B tokens.For the hyper-parameter selection, we mostlyfollowed the suggestion made in (Levy et al,2015).
Table 3 summarizes the default values ofhyper-parameters used consistently in all our ex-periments unless otherwise noted.4.1 Benchmark datasets for evaluationWe prepared eight word similarity benchmarkdatasets (WSimilarity), namely, R&G (Ruben-stein and Goodenough, 1965), M&C (Miller andCharles, 1991), WSimS (Agirre et al, 2009),WSimR (Agirre et al, 2009), MEM (Bruniet al, 2014), MTurk (Radinsky et al, 2011),SCWS (Huang et al, 2012), and RARE (Luong188method time WSimilarity WAnalogySGNS (original) 8856 65.4 (65.2, 65.7) 63.0 (62.2, 63.8)GloVe (original) 8243 57.6 (57.5, 57.9) 64.8 (64.6, 65.0)w/o bias terms 8027 57.6 (57.5, 57.7) 64.8 (64.5, 65.0)fitting=SPMIk,?8332 57.5 (57.2, 57.8) 65.0 (64.8.
65.1)Table 4: Results: the micro averages of Spear-man?s rho (WSimilarity) and accuracy (WAnal-ogy) for all benchmark datasets.et al, 2013).
Moreover, we also prepared threeanalogy benchmark datasets (WAnalogy), that is,GSEM (Mikolov et al, 2013a), GSYN (Mikolovet al, 2013a), and MSYN (Mikolov et al, 2013b).4.2 SGNS and GloVe ResultsTable 4 shows the training time and performanceresults gained from our benchmark data.
The col-umn ?time?
indicates average elapsed time (sec-ond) for model learning.
All the results are the av-erage performance of ten runs.
This is becausethe comparison methods have some randomizedfactors, such as initial value (since they are non-convex optimization problems) and (probabilistic)sampling method, which significantly impact theresults.At first, we compared the original SGNS as im-plemented in the word2vec package3and theoriginal GloVe as implemented in the glovepackage4.
These results are shown in the first andsecond rows in Table 4.
In our experiments, SGNSsignificantly outperformed GloVe in WSimilaritywhile GloVe significantly outperformed SGNS inWAnalogy.
As we explained, these two methodscan be easily merged into a unified form.
Thus,there must be some differences in their configura-tions that yields such a large difference in the re-sults.
Next, we tried to determine the clues as thedifferences.4.3 Impact of incorporating bias termsThe third row (w/o bias terms) in Table 4 showsthe results of the configuration without using thebias terms in the glove package.
A comparisonwith the results of the second row, finds no mean-ingful benefit to using the bias terms.
In contrast,obviously, the elapsed time for model learning isconsistently shorter since we can discard the biasterm update.3https://code.google.com/p/word2vec/4http://nlp.stanford.edu/projects/glove/(a) WSimilaritymethod W=2 3 5 10 20SGNS (original) 64.9 65.1 65.4 65.4 64.9GloVe (original) 53.6 55.7 57.0 57.6 57.8w/o harmonic func.
54.6 56.9 57.8 58.2 57.9(b) WAnalogymethod W=2 3 5 10 20SGNS (original) 62.8 63.5 63.9 63.0 61.3GloVe (original) 51.7 58.4 62.3 64.8 66.1w/o harmonic func.
52.6 58.0 60.5 61.6 60.7Table 5: Impact of the context window size, andharmonic function.W=2 3 5 10 20(1) 0<ci,j<1 104M 213M 377M 649M 914M(2) 1?ci,j167M 184M 207M 234M 251Mnon-zero ci,j271M 398M 584M 883M 1165Mratio of (1) 38.5% 53.6% 64.5% 73.5% 78.4%Table 6: The ratio of entries less than one in co-occurrence matrix.4.4 Impact of fitting functionThe fourth row (fitting=SPMIk,?)
in Table 4 showsthe performance when we substituted the fit-ting function of GloVe, namely, log(ci,j), forSPMIk=5,?=3/4used in SGNS.
Clearly, the per-formance becomes nearly identical to the originalGloVe.
Accordingly, the selection of fitting func-tion has only a small impact.4.5 Impact of context window size andharmonic functionTable 5 shows the impact of context window sizeW .
The results of SGNS seem more stable againstW than those of GloVe.Additionally, we investigated the impact of the?harmonic function?
used in GloVe.
The ?har-monic function?
uses the inverse of context dis-tance, i.e., 1/a if the context word is a-word awayfrom the target word, instead of just count 1 re-gardless of the distance when calculating the co-occurrences.
Clearly, GloVe without using theharmonic function shown in the third row of Ta-ble 5 yielded significantly degraded performanceon WAnalogy, and slight improvement on WSimi-larity.
This fact may imply that the higher WAnal-ogy performance of GloVe was derived by the ef-fect of this configuration.4.6 Link between harmonic function andnegative samplingThis section further discusses a benefit of har-monic function.Recall that GloVe does not explicitly consider?negative samples?.
It fixes c?i,j= 1 for all (i, j)as shown in Eq.
7.
However, the co-occurrence189count given by using the harmonic function cantake values less than 1, i.e., ci,j= 2/3, if the i-th word and the j-th word co-occurred twice withdistance 3.
As a result, the value of the fitting func-tion of GloVe becomes log(2/3).
Interestingly,this is essentially equivalent to co-occur 3 times inthe negative sampling data and 2 times in the realdata since the fitting function of the unified formshown in Eq.
12 is log(ci,j/c?i,j) = log(2/3) whenci,j= 2 and c?i,j= 3.
It is not surprising that rareco-occurrence words that occur only in long rangecontexts may have almost no correlation betweenthem.
Thus treating them as negative samples willnot create a problem in most cases.
Therefore, theharmonic function seems to ?unexpectedly?
mimica kind of a negative sampling method; it is inter-preted as ?implicitly?
generating negative data.Table 6 shows the ratio of the entries ci,jwhosevalue is less than one in matrix M. Rememberthat vocabulary size was 400,000 in our experi-ments.
Thus, we had a total of 400K?400K=160Belements in M, and most were 0.
Here, we con-sider only non-zero entries.
It is clear that longercontext window sizes generated many more en-tries categorized in 0 < ci,j< 1 by the har-monic function.
One important observation is thatthe ratio of 0 < ci,j< 1 is gradually increas-ing, which offers a similar effect to increasing thenumber of negative samples.
This can be a rea-son why GloVe demonstrated consistent improve-ments in WAnalogy performance as context win-dow increased since larger negative sampling sizeoften improves performance (Levy et al, 2015).Note also that the number of 0 < ci,j< 1 alwaysbecomes 0 in the configuration without the har-monic function.
This is equivalent to using uni-form negative sampling c?i,j= 1 as described inSec.
3.
This fact also indicates the importance ofthe negative sampling method.4.7 Impact of weighting functionTable 7 shows the impact of weighting functionused in GloVe, namely, Eq 8.
Note that ??(?
)=1?column shows the results when we fixed 1 forall non-zero entries5.
This is also clear that theweighting function Eq 8 with appropriate param-eters significantly improved the performance ofboth WSimilarity and WAnalogy tasks.
How-ever unfortunately, the best parameter values for5This is equivalent to set 0 to -x-max option in gloveimplementation.
(a) WSimilarityhyper param.
?(?
)=1 xmax= 1 10 100 10000?
= 0.75 59.4 60.1 60.9 57.7 49.5w/o harmonic func.
58.2 58.0 60.7 58.2 56.0?
= 1.0 (59.4) 60.1 59.4 55.9 36.1w/o harmonic func.
(58.2) 58.3 60.7 57.7 46.7(b) WAnalogyhyper param.
?(?
)=1 xmax= 1 10 100 10000?
= 0.75 55.7 61.1 64.3 64.8 28.4w/o harmonic func.
53.4 52.6 60.3 61.6 42.5?
= 1.0 (55.7) 61.0 63.8 59.1 7.5w/o harmonic func.
(53.4) 54.1 60.8 60.1 20.3Table 7: Impact of the weighting function.WSimilarity and WAnalogy tasks looks different.We emphasize that harmonic function discussedin the previous sub-section was still a necessarycondition to obtain the best performance, and bet-ter performance in the case of ??(?)=1?
as well.5 ConclusionThis paper reconsidered the relationship betweenSkipGram and GloVe models in machine learn-ing viewpoint.
We showed that SGNS and GloVecan be easily merged into a unified form.
Wealso extracted the factors of the configurationsthat are used differently.
We empirically inves-tigated which learning configuration is responsi-ble for the performance difference often observedin widely examined word similarity and analogytasks.
Finally, we found that at least two config-urations, namely, the weighting function and har-monic function, had significant impacts on the per-formance.
Additionally, we revealed a relation-ship between harmonic function and negative sam-pling.
We hope that our theoretical and empiricalanalyses will offer a deeper understanding of theseneural word embedding methods6.AcknowledgmentWe thank three anonymous reviewers for theirhelpful comments.ReferencesEneko Agirre, Enrique Alfonseca, Keith Hall, JanaKravalova, Marius Pas?ca, and Aitor Soroa.
2009.A study on similarity and relatedness using distribu-tional and wordnet-based approaches.
In Proceed-ings of Human Language Technologies: The 2009Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,NAACL ?09, pages 19?27, Stroudsburg, PA, USA.Association for Computational Linguistics.6The modified codes for our experiments will be availablein author?s homepage190Elia Bruni, Nam Khanh Tran, and Marco Baroni.
2014.Multimodal distributional semantics.
J. Artif.
Int.Res., 49(1):1?47, January.Yoav Goldberg and Omer Levy.
2014. word2vecExplained: Deriving Mikolov et al?s Negative-sampling Word-embedding Method.
CoRR,abs/1402.3722.Eric H. Huang, Richard Socher, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Improving wordrepresentations via global context and multiple wordprototypes.
In Proceedings of the 50th Annual Meet-ing of the Association for Computational Linguis-tics: Long Papers - Volume 1, pages 873?882.
Asso-ciation for Computational Linguistics.Omer Levy and Yoav Goldberg.
2014.
NeuralWord Embedding as Implicit Matrix Factorization.In Z. Ghahramani, M. Welling, C. Cortes, N.D.Lawrence, and K.Q.
Weinberger, editors, Advancesin Neural Information Processing Systems 27, pages2177?2185.
Curran Associates, Inc.Omer Levy, Yoav Goldberg, and Ido Dagan.
2015.Improving Distributional Similarity with LessonsLearned from Word Embeddings.
Transactions ofthe Association for Computational Linguistics, 3.Thang Luong, Richard Socher, and Christopher Man-ning.
2013.
Better word representations with recur-sive neural networks for morphology.
In Proceed-ings of the Seventeenth Conference on Computa-tional Natural Language Learning, pages 104?113,Sofia, Bulgaria, August.
Association for Computa-tional Linguistics.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013a.
Efficient estimation of word represen-tations in vector space.
CoRR, abs/1301.3781.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013b.
Linguistic Regularities in Continuous SpaceWord Representations.
In Proceedings of the 2013Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 746?751, Atlanta,Georgia, June.
Association for Computational Lin-guistics.George A. Miller and Walter G. Charles.
1991.
Con-textual Correlates of Semantic Similarity.
Language& Cognitive Processes, 6(1):1?28.Andriy Mnih and Koray Kavukcuoglu.
2013.
Learningword embeddings efficiently with noise-contrastiveestimation.
In C.J.C.
Burges, L. Bottou, M. Welling,Z.
Ghahramani, and K.Q.
Weinberger, editors, Ad-vances in Neural Information Processing Systems26, pages 2265?2273.
Curran Associates, Inc.Jeffrey Pennington, Richard Socher, and ChristopherManning.
2014.
Glove: Global vectors for wordrepresentation.
In Proceedings of the 2014 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 1532?1543, Doha,Qatar, October.
Association for Computational Lin-guistics.Kira Radinsky, Eugene Agichtein, EvgeniyGabrilovich, and Shaul Markovitch.
2011.
Aword at a time: Computing word relatedness usingtemporal semantic analysis.
In Proceedings of the20th International Conference on World Wide Web,WWW ?11, pages 337?346, New York, NY, USA.ACM.Herbert Rubenstein and John B. Goodenough.
1965.Contextual correlates of synonymy.
Commun.
ACM,8(10):627?633, October.Tianze Shi and Zhiyuan Liu.
2014.
Linking GloVewith word2vec.
CoRR, abs/1411.5595.191
