Transactions of the Association for Computational Linguistics, 1 (2013) 243?254.
Action Editor: Philipp Koehn.Submitted 12/2012; Revised 3/2013; Published 5/2013.
c?2013 Association for Computational Linguistics.Unsupervised Tree Induction for Tree-based TranslationFeifei Zhai, Jiajun Zhang, Yu Zhou and Chengqing ZongNational Laboratory of Pattern Recognition, Institute of Automation,Chinese Academy of Sciences, Beijing, China{ffzhai,jjzhang,yzhou,cqzong}@nlpr.ia.ac.cnAbstractIn current research, most tree-based translationmodels are built directly from parse trees.
Inthis study, we go in another direction and builda translation model with an unsupervised treestructure derived from a novel non-parametricBayesian model.
In the model, we utilizesynchronous tree substitution grammars (STSG)to capture the bilingual mapping betweenlanguage pairs.
To train the model efficiently,we develop a Gibbs sampler with three novelGibbs operators.
The sampler is capable ofexploring the infinite space of tree structures byperforming local changes on the tree nodes.Experimental results show that the string-to-tree translation system using our Bayesian treestructures significantly outperforms the strongbaseline string-to-tree system using parse trees.1 IntroductionIn recent years, tree-based translation models1 aredrawing more and more attention in thecommunity of statistical machine translation(SMT).
Due to their remarkable ability toincorporate context structure information and longdistance reordering into the translation process,tree-based translation models have shownpromising progress in improving translationquality (Liu et al 2006, 2009; Quirk et al 2005;Galley et al 2004, 2006; Marcu et al 2006; Shenet al 2008; Zhang et al 2011b).However, tree-based translation models alwayssuffer from two major challenges: 1) They areusually built directly from parse trees, which aregenerated by supervised linguistic parsers.1 A tree-based translation model is defined as a modelusing tree structures on one side or both sides.However, for many language pairs, it is difficult toacquire such corresponding linguistic parsers dueto the lack of Tree-bank resources for training.
2)Parse trees are actually only used to model andexplain the monolingual structure, rather than thebilingual mapping between language pairs.
Thisindicates that parse trees are usually not theoptimal choice for training tree-based translationmodels (Wang et al 2010).Based on the above analysis, we can concludethat the tree structure that is independent fromTree-bank resources and simultaneously considersthe bilingual mapping inside the bilingual sentencepairs would be a good choice for building tree-based translation models.Therefore, complying with the above conditions,we propose an unsupervised tree structure for tree-based translation models in this study.
In thestructures, tree nodes are labeled by combining theword classes of their boundary words rather thanby syntactic labels, such as NP, VP.
Furthermore,using these node labels, we design a generativeBayesian model to infer the final tree structurebased on synchronous tree substitution grammars(STSG) 2 .
STSG is derived from the wordalignments and thus can grasp the bilingualmapping effectively.Training the Bayesian model is difficult due tothe exponential space of possible tree structures foreach training instance.
We therefore develop anefficient Gibbs sampler with three novel Gibbsoperators for training.
The sampler is capable ofexploring the infinite space of tree structures byperforming local changes on the tree nodes.2 We believe it is possible to design a model to infer thenode label and tree structure jointly.
We plan this asfuture work, and here, we focus only on inferring thetree structure in terms of the node labels derived fromword classes.243The tree structure formed in this way isindependent from the Tree-bank resources andsimultaneously exploits the bilingual mappingeffectively.
Experiments show that the proposedunsupervised tree (U-tree) is more effective andreasonable for tree-based translation than the parsetree.The main contributions of this study are asfollows:1) Instead of the parse tree, we propose aBayesian model to induce a U-tree for tree-based translation.
The U-tree exploits thebilingual mapping effectively and does notrely on any Tree-bank resources.2) We design a Gibbs sampler with three novelGibbs operators to train the Bayesian modelefficiently.The remainder of the paper is organized asfollows.
Section 2 introduces the related work.Section 3 describes the STSG generation process,and Section 4 depicts the adopted Bayesian model.Section 5 describes the Gibbs sampling algorithmand Gibbs operators.
In Section 6, we analyze theachieved U-trees and evaluate their effectiveness.Finally, we conclude the paper in Section 7.2 Related WorkIn this study, we move in a new direction to build atree-based translation model with effectiveunsupervised U-tree structures.For unsupervised tree structure induction,DeNero and Uszkoreit (2011) adopted a parallelparsing model to induce unlabeled trees of sourcesentences for syntactic pre-reordering.
Ourprevious work (Zhai et al 2012) designed an EM-based method to construct unsupervised trees fortree-based translation models.
This work differsfrom the above work in that we design a novelBayesian model to induce unsupervised U-trees,and prior knowledge can be encoded into themodel more freely and effectively.Blunsom et al(2008, 2009, 2010) utilizedBayesian methods to learn synchronous contextfree grammars (SCFG) from a parallel corpus.
Theobtained SCFG is further used in a phrase-basedand hierarchical phrase-based system (Chiang,2007).
Levenberg et al(2012) employed aBayesian method to learn discontinuous SCFGrules.
This study differs from their work becausewe concentrate on constructing tree structures fortree-based translation models.
Our U-trees arelearned based on STSG, which is more appropriatefor tree-based translation models than SCFG.Burkett and Klein (2008) and Burkett et al(2010) focused on joint parsing and alignment.They utilized the bilingual Tree-bank to train ajoint model for both parsing and word alignment.Cohn and Blunsom (2009) adopted a Bayesianmethod to infer an STSG by exploring the space ofalignments based on parse trees.
Liu et al(2012)re-trained the linguistic parsers bilingually basedon word alignment.
Burkett and Klein (2012)utilized a transformation-based method to learn asequence of monolingual tree transformations fortranslation.
Compared to their work, we do not relyon any Tree-bank resources and focus ongenerating effective unsupervised tree structuresfor tree-based translation models.Zollmann and Venugopal (2006) substituted thenon-terminal X in hierarchical phrase-based modelby extended syntactic categories.
Zollmann andVogel (2011) further labeled the SCFG rules withPOS tags and unsupervised word classes.
Our workdiffers from theirs in that we present a Bayesianmodel to learn effective STSG translation rules andU-tree structures for tree-based translation models,rather than designing a labeling strategy fortranslation rules.3 The STSG Generation ProcessIn this work, we induce effective U-trees for thestring-to-tree translation model, which is based ona synchronous tree substitution grammar (STSG)between source strings and target tree fragments.We take STSG as the generation grammar to matchthe translation model.
Typically, such an STSG3 isa 5-tuple as follows:( , , , , )s t t tG N S P ?
?where:i s?
and t?
represent the set of source andtarget words, respectively,i tN  is the set of target non-terminals,i t tS N?
is the start root non-terminal, andi P  is the production rule set.3 Generally, an STSG involves tree fragments on bothsides.
Here we only consider the special case where thesource side is actually a string.244Apart from the start non-terminal tS , we defineall the other non-terminals in tN  by word classes.Inspired by (Zollmann and Vogel, 2011), wedivide these non-terminals into three categories:one-word, two-word and multi-word non-terminals.The one-word non-terminal is a word class, such asC, meaning that it dominates a word whose wordclass is C. Two-word non-terminals are used tostand for two word strings.
They are labeled in theform of C1+C2, where C1 and C2 are the wordclasses of the two words separately.
Accordingly,multi-word non-terminals represent the stringscontaining more than two words.
They are labeledas C1?Cn, demanding that the word classes of theleftmost word and the rightmost word are C1 andCn, respectively.We use POS tag to play the role of word class4.For example, the head node of the rule in Figure 1is a multi-word non-terminal PRP?RB.
It requiresthat the POS tags of the leftmost and rightmostword must be PRP and RB, respectively.
Xiong etal.
(2006) showed that the boundary word is aneffective indicator for phrase reordering.
Thus, webelieve that combining the word class of boundarywords can denote the whole phrase well.PRP...RBwePRPVBP:x0 RB:x1VBP+RB??
x1   x0wo-menFigure 1.
An example of an STSG production rule.Each production rule in P  consists of a sourcestring and a target tree fragment.
In the target treefragment, each internal node is labeled with a non-terminal in tN , and each leaf node is labeled witheither a target word in t?
or a non-terminal in tN .The source string in a production rule comprisessource words and variables.
Each variablecorresponds to a leaf non-terminal in the target treefragment.
In the STSG, the production rule is usedto rewrite the root node into a string and a treefragment.
For example, in Figure 1, the rulerewrites the head node PRP?RB into thecorresponding string and fragment.An STSG derivation refers to the process ofgenerating a specific source string and target tree4 The demand of a POS tagger impairs the independencefrom manual resources to some extent.
In future, weplan to design a method to learn effective unsupervisedlabels for the non-terminals.structure by production rules.
This process beginswith the start non-terminal tS  and an empty sourcestring.
We repeatedly choose production rules torewrite the leaf non-terminals and expand thestring until no leaf non-terminal is left.
Finally, weacquire a source string and a target tree structuredefined by the derivation.
The probability of aderivation is given as follows:1( ) ( | )ni iip d p r N?
(1)where the derivation comprises a sequence of rulesd=(r1,?,rn), and Ni represents the root node of ruleri.
Hence, for a specific bilingual sentence pair, wecan generate the best target-side tree structurebased on the STSG, independent from the Tree-bank resources.
The STSG used in the aboveprocess is learned by the Bayesian model that isdetailed in the next section.Actually, SCFG can also be used to build the U-trees.
We do not use SCFG because most of thetree-based models are based on STSG.
In ourBayesian model, the U-trees are optimized throughselecting a set of STSG rules.
These STSG rulesare consistent with the translation rules used in thetree-based models.Another reason is that STSG has a strongerexpressive power on tree construction than SCFG.In a STSG-based U-tree or a STSG rule, althoughnot linguistically informed, the nodes labeled byPOS tags are also effective on distinguishingdifferent ones.
However, with SCFG, we have todiscard all the internal nodes (i.e., flattening the U-trees or rules) to express the same sequence,leading to a poor ability of distinguishing differentU-trees and production rules.
Thus, using STSG,we can build more specific U-trees for translation.In addition, we find that the Bayesian SCFGgrammar cannot even significantly outperform theheuristic SCFG grammar (Blunsom et al2009)5.This would indicate that the SCFG-basedderivation tree as by-product is also not such goodfor tree-based translation models.
Considering theabove reasons, we believe that the STSG-basedlearning procedure would result in a bettertranslation grammar for tree-based models.5 In (Blunsom et al 2009), for Chinese-to-Englishtranslation, the Bayesian SCFG grammar onlyoutperform the heuristic SCFG grammar by 0.1 BLEUpoints on NIST MT 2004 and 0.6 BLEU points on NISTMT 2005 in the NEWS domain.2454 Bayesian ModelIn this section, we present a Bayesian model tolearn STSG defined in section 3.
In the model, weuse ?N to denote the probability distribution( | )p r N  in Equation (1).
?N follows a multinomialdistribution and we impose a Dirichlet prior (DP)on it:0 0| ~ ( )| , ~ ( , ( | ) )NN N Nr N MultiP DP P NTT D D <  (2)where 0 ( | )P N<  (base distribution) is used to assignprior probabilities to the STSG production rules.
?Ncontrols the model?s tendency to either reuseexisting rules or create new ones using the basedistribution 0 ( | )P N< .Instead of denoting the multinomial distributionexplicitly with a specific ?N, we integrate over allpossible values of ?N to achieve the probabilities ofrules.
This integration results in the followingconditional probability for rule ri given thepreviously observed rules r-i = r1 ,?, ri-1:00( | )( | , , , ) iir N iii N iN Nn P r Np r r N PnDD D  (3)Where n-i ri  denotes the number of ri in ir , and n-iNrepresents the total count of rules rewriting non-terminal N in ir .
Thanks to the exchangeability ofthe model, all permutations of the rules are actuallyequiprobable.
This means that we can compute theprobability of each rule based on the previous andsubsequent rules (i.e.
consider each rule as the lastone).
This characteristic allows us to design anefficient Gibbs sampling algorithm to train theBayesian model.4.1  Base DistributionThe base distribution 0 ( | )P r N  is designed toassign prior probabilities to the STSG productionrules.
Because each rule r consists of a target treefragment frag and a source string str in the model,we follow Cohn and Blunsom (2009) anddecompose the prior probability 0 ( | )P r N  into twofactors as follows:0 ( | ) ( | ) ( | )P r N P frag N P str frag ?
(4)where ( | )P frag N  is the probability ofproducing the target tree fragment frag.
Togenerate frag, Cohn and Blunsom (2009) used ageometric prior to decide how many child nodes toassign each node.
Differently, we require that eachmulti-word non-terminal node must have two childnodes.
This is because the binary structure hasbeen verified to be very effective for tree-basedtranslation (Wang et al 2007; Zhang et al 2011a).The generation process starts at root node N. Atfirst, root node N is expanded into two child nodes.Then, each newly generated node will be checkedto expand into two new child nodes withprobability pexpand.
This process repeats until all thenew non-terminal nodes are checked.
Obviously,pexpand controls the scale of tree fragments, where alarge pexpand corresponds to large fragments6.
Thenew terminal nodes (words) are drawn uniformlyfrom the target-side vocabulary, and the non-terminal nodes are created by asking two questionsas follows:1) What type is the node, one-word, two-word or multi-word non-terminal?2) What tag is used to label the node?The answer to question 1) is chosen from auniform distribution, i.e., the probability is 1/3 foreach type of non-terminal.
The entire generationprocess is in a top-down manner, i.e., generating aparent node first and then its children.With respect to question 2), because the fathernode has determined the POS tags of boundarywords, we only need one POS tag to generate thelabel of the current node.
For example, in Figure 1,as the father node PRP?RB demands that the POStag of the rightmost word is RB, the right child ofPRP?RB must also satisfy this condition.Therefore, we choose a POS tag VBP and obtainthe label VBP+RB.
The POS tag is drawnuniformly from the POS tag set.
If the current nodeis a one-word non-terminal, question 2) isunnecessary.
Similarly, with respect to the two-word non-terminal node, questions 1) and 2) areboth unnecessary for its two child nodes becausethey have already been defined by their father node.As an example of the generative process, thetree fragment in Figure 1 is created as follows:a.
Determine that the left child of PRP?RB isa one-word non-terminal (labeled with PRP);b.
Expand PRP and generate the word ?we?
forPRP;6 In our experiment, we set pexpand to 1/3 to encouragesmall tree fragments.246c.
Determine that the right child of PRP?RB isa two-word non-terminal;d. Utilize the predetermined RB and a POS tagVBP to form the tag of the two-word non-terminal: VBP+RB;e. Expand VBP+RB (to VBP and RB);f. Do not expand VBP and RB.
( | )P str frag  in Equation (4) is the probability ofgenerating the source string, which containsseveral source words and variables.
Inspired by(Blunsom et al 2009) and (Cohn and Blunsom,2009), we define ( | )P str frag  as follows:var11 1( | ) ( ;1)| |poisson swswswccs iP str frag P cc iu u?
?
(5)where csw is the number of words in the sourcestring.
?s means the source vocabulary set.
Further,cvar denotes the number of variables, which isdetermined by the tree fragment frag.As shown in Equation(5), we first determinehow many source words to generate using aPoisson distribution Ppoisson(csw;1), which imposes astable preference for short source strings.
Then, wedraw each source word from a uniform distributionover ?s.
Afterwards, we insert the variables intothe string.
The variables are inserted one at a timeusing a uniform distribution over the possiblepositions.
This factor discourages more variables.For the example rule in Figure 1, the generativeprocess of the source string is:a.
Decide to generate one source word;b.
Generate the source word ???
(wo-men) ?;c.
Insert the first variable after the word;d. Insert the second variable between the wordand the first variable.Intuitively, a good translation grammar shouldcarry both small translation rules with enoughgenerality and large rules with enough contextinformation.
DeNero and Klein (2007) proposedthis statement, and Cohn and Blunsom (2009) hasverified it in their experiments with parse trees.Our base distribution is also designed based onthis intuition.
Considering the two factors in ourbase distribution, we penalize both large target treefragments with many nodes and long source stringswith many words and variables.
The Bayesianmodel tends to select both small and frequentSTSG production rules to construct the U-trees.With these types of trees, we can extract smallrules with good generality and simultaneouslyobtain large rules with enough context informationby composition.
We will show the effectiveness ofour U-trees in the verification experiment.5 Model Training by Gibbs SamplingIn this section, we introduce a collapsed Gibbssampler, which enables us to train the Bayesianmodel efficiently.5.1 Initialization StateAt first, we use random binary trees to initialize thesampler.
To get the initial U-trees, we recursivelyand randomly segment a sentence into two partsand simultaneously create a tree node to dominateeach part.
The created tree nodes are labeled by thenon-terminals described in section 3.Using the initial target U-trees, source sentencesand word alignment, we extract minimal GHKMtranslation rules7 in terms of frontier nodes (Galleyet al 2004).
Frontier nodes are the tree nodes thatcan map onto contiguous substrings on the sourceside via word alignment.
For example, the bolditalic nodes with shadows in Figure 2 are frontiernodes.
In addition, it should be noted that the wordalignment is fixed8, and we only explore the entirespace of tree structures in our sampler.
Differently,Cohn and Blunsom (2009) designed a sampler toinfer an STSG by fixing the tree structure andexploring the space of alignment.
We believe thatit is possible to investigate the space of both treestructure and alignment simultaneously.
Thissubject will be one of our future work topics.For each training instance (a pair of sourcesentence and target U-tree structure), the extractedGHKM minimal translation rules compose aunique STSG derivation9.
Moreover, all the rulesdeveloped from the training data constitute aninitial STSG for the Gibbs sampler.7 We attach the unaligned word to the lowest frontiernode that can cover it in terms of word alignment.8 The sampler might reinforce the frequent alignmenterrors (AE), which would harm the translation model(TM).
Actually, the frequent AEs also greatly impair theconventional TM.
Besides, our sampler encourages thecorrect alignments and simultaneously discourages theinfrequent AEs.
Thus, compared with the conventionalTMs, we believe that our final TM would not be worsedue to AEs.
Our final experiments verify this point andwe will conduct a much detailed analysis in future.9 We only use the minimal GHKM rules (Galley et al2004) here to reduce the complexity of the sampler.247jin-tian jian-mianwo-men zai-ciPRP+VBPtodayNNwePRPmeetVBPagainRB??
??
??
?
?PRP...RBNN...RBFigure 2.
Illustration of an initial U-tree structure.
Thebold italic nodes with shadows are frontier nodes.Under this initial STSG, the sampler modifiesthe initial U-trees (initial sample) to create a seriesof new ones (new samples) by the Gibbs operators.Consequently, new STSGs are created based on thenew U-trees simultaneously and used for the nextsampling operation.
Repeatedly and after a numberof iterations, we can obtain the final U-trees forbuilding translation models.5.2 The Gibbs OperatorsIn this section, we develop three novel Gibbsoperators for the sampler.
They explore the entirespace of the U-tree structures by performing localchanges on the tree nodes.For a U-tree of a given sentence, we define s-node as the non-root node covering at least twowords.
Thus, the set of s-node contains all the treenodes except the root node, the pre-terminal nodesand leaf nodes, which we call non-s-node.
Forexample, in Figure 2, PRB?RB and PRP+VBP ares-nodes, while NN and NN?RB are non-s-nodes.Since the POS tag sequence of the sentence isfixed, all non-s-nodes would stay unchanged in allpossible U-trees of the sentence.
Based on this fact,our Gibbs operators work only on s-nodes.Further, we assign 3 descendant candidates (DC)for each s-node: its left child, right child and itssibling.
For example, in Figure 3, the 3 DCs for thes-node are node PRP, VBP and RB respectively.According to the different DCs it governs, every s-node might be in one of the two different states:1) Left state: as Figure 3(a) shows, the s-nodegoverns the left two DCs, PRP and VBP,and is labeled PRP+VBP.2) Right state: as Figure 3(b) shows, the s-nodegoverns the right two DCs, VBP and RB, andis labeled VBP+RB.For a specific U-tree, the states of s-nodes are fixed.Thus, by changing an s-node?s state, we can easilytransform this U-tree to another one, i.e., from thecurrent sample to a new one.To formulate the U-tree transformation process,we associate a binary variable ??
{0,1} with eachs-node, indicating whether the s-node is in the left?
 or right state ?
 Then we can changethe U-tree by changing value of the ?
parameters.Our first Gibbs operator, Rotate, just works bysampling value of the ?parameters, one at a time,and changing the U-tree accordingly.
For example,in Figure 3(a), the s-node is currently in the leftVWDWH?
:HVDPSOHWKH?RIWKLVQRGHDQGLIWKHVDPSOHGYDOXHRI?LVZHNHHSWKHVWUXFWXUHunchanged, i.e., in the left state.
Otherwise, wechange its state to the right state ?
, andtransform the U-tree to Figure 3(b) accordingly.jian-mianwo-men zai-cis-nodewePRPmeetVBPagainRB??
??
?
?PRP...RBPRP+VBPjian-mianwo-men zai-cis-nodewePRPmeetVBPagainRB??
??
?
?PRP...RBVBP+RB(b) ?=1(a) ?=0RotateFigure 3.
Illustration of the Rotate operator.
In thefigure, (a) and (b) denote the s-node?s left state and rightstate respectively.
The bold italic nodes with shadows inthe figure are frontier nodes.Obviously, towards an s-node for sampling, thetwo values of ?
would define two different U-trees.Using the GHKM algorithm (Galley et al2004),we can get two different STSG derivations fromthe two U-trees based on the fixed word alignment.Each derivation carries a set of STSG rules (i.e.,minimal GHKM translation rules) of its own.
Inthe two derivations, the STSG rules defined by thetwo states include the one rooted at the s-node?slowest ancestor frontier node, and the one rooted atthe s-node if it is a frontier node.
For instance, inFigure 3(a), as the s-node is not a frontier node, theleft state (?
) defines only one rule:0 2 10 1 2:... ( ( : : ) : )leftr x x xPRP RB PRP VBP x PRP x VBP x RBoDifferently, in Figure 3(b), the s-node is afrontier node and thus the right state (?
1) definestwo rules:2480 0 1 0 11 1 0 0 1: ... ( : : ): ( : : )rightrightr x x PRP RB x PRP x VBP RBr x x VBP RB x VBP x RBo o Using these STSG rules, the two derivations areevaluated as follows (We use the value of ?
todenote the corresponding STSG derivation):0 10 1 0( 0) ( | )( 1) ( , | )( | ) ( | , )leftright rightright right rightp p r rp p r r rp r r p r r r    <  v<  vWhere r  refers to the conditional context, i.e., theset of all other rules in the training data.
All theprobabilities in the above formulas are computedby Equation(3).
We then normalize the two scoresand sample a value of ?
based on them.
With theBayesian model described in section 4, the samplerZLOOSUHIHUWKH?WKDWSURGXFHVVPDOODQGIUHTXHQWSTSG rules.
This tendency results in more frontiernodes in the U-tree (i.e., the s-node tends to be inthe state that is a frontier node), which will factorthe training instance into more small STSG rules.In this way, the overall likelihood of the bilingualdata is improved by the sampler.Theoretically, the Rotate operator is capable ofarriving at any possible U-tree from the initial U-tree.
This is because we can first convert the initialU-tree to a left branch tree by the Rotate operator,and then transform it to any other U-tree.
However,it may take a long time to do so.
Thus, to speed upthe structure transformation process, we employ aTwo-level-Rotate operator, which takes a pair of s-nodes in a parent-child relationship as a unit forsampling.
Similar to the Rotate operator, we alsoassign a binary variable ??
{0,1} to each unit andupdate the U-tree by sampling the value of ?.
Themethod of sampling ?
is similar to the one used for?.
Figure 4 shows an example of the operator.
Asshown in Figure 4(a), the unit NN?VBP andPRP+VBP is in the left state (?=0), and governsthe left three descendants: NN, PRP, and VBP.
Bythe Two-level-Rotate operator, we can convert theunit to Figure 4(b), i.e., the ULJKWVWDWH?=1).
Just asFigure 4(b) shows, the governed descendants of theunit are turned to PRP, VBP, and RB.It may be confusing when choosing the parent-child s-node pair for sampling because the parentnode always faces two choices: combining the leftchild or right child for sampling.
To avoidconfusion, we split the Two-level-Rotate operatorinto two operators: Two-level-left-Rotate operator,which works with the parent node and its left child,and Two-level-right-Rotate operator, which onlyconsiders the parent node and its right child 10 .Therefore, the operator used in Figure 4 is a Two-level-right-Rotate operator.jin-tian jian-mianwo-men zai-ciPRP+VBPTodayNNwePRPmeetVBPagainRB??
??
??
?
?NN...VBPNN...RBjin-tian jian-mianwo-men zai-ciVBP+RBTodayNNwePRPmeetVBPagainRB??
??
??
?
?PRP...RBNN...RB(a) ?=0 (b) ?=1Two-level-right-RotateFigure 4.
Illustration of the Two-level-Rotate operator.The bold italic nodes with shadows in the Figure arefrontier nodes.During sampling, for each training instance, thesampler first applies the Two-level-left-Rotateoperator to all candidate pairs of s-nodes (parent s-node and its left child s-node) in the U-tree.
Afterthat, the Two-level-right-Rotate operator is appliedto all the candidate pairs of s-nodes (parent s-nodeand its right child s-node).
Then, we use the Rotateoperator on every s-node in the U-tree.
By utilizingthe operators separately, we can guarantee that oursampler satisfies detailed balance.
We visit all thetraining instances in a random order (one iteration).After a number of iterations, we can obtain thefinal U-tree structures and build the tree-basedtranslation model accordingly.6 Experiments6.1 Experimental SetupThe experiments are conducted on Chinese-to-English translation.
The training data are the FBIScorpus with approximately 7.1 million Chinesewords and 9.2 million English words.
We obtainthe bidirectional word alignment with GIZA++,and then adopt the grow-diag-final-and strategy toobtain the final symmetric alignment.
We train a 5-gram language model on the Xinhua portion of theEnglish Gigaword corpus and the English part of10 We can also take more nodes as a unit for sampling,but this would make the algorithm much more complex.249the training data.
For tuning and testing, we use theNIST MT 2003 evaluation data as the developmentset, and use the NIST MT04 and MT05 data as thetest set.
We use MERT (Och, 2004) to tuneparameters.
Since MERT is prone to search errors,we run MERT 5 times and select the best tuningparameters in the tuning set.
The translation qualityis evaluated by case-insensitive BLEU-4 with theshortest length penalty.
The statistical significancetest is performed by the re-sampling approach(Koehn, 2004).To create the baseline system, we use the open-source Joshua 4.0 system (Ganitkevitch et al 2012)to build a hierarchical phrase-based (HPB) system,and a syntax-augmented MT (SAMT) 11  system(Zollmann and Venugopal, 2006) respectively.The translation system used for testing theeffectiveness of our U-trees is our in-house string-to-tree system (abbreviated as s2t).
The system isimplemented based on (Galley et al 2006) and(Marcu et al2006).
In the system, we extract boththe minimal GHKM rules (Galley et al 2004), andthe rules of SPMT Model 1 (Galley et al 2006)with phrases up to length L=5 on the source side.We then obtain the composed rules by composingtwo or three adjacent minimal rules.To build the above s2t system, we first use theparse tree, which is generated by parsing theEnglish side of the bilingual data with the Berkeleyparser (Petrov et al 2006).
Then, we binarize theEnglish parse trees using the head binarizationapproach (Wang et al 2007) and use the resultingbinary parse trees to build another s2t system.For the U-trees, we run the Gibbs sampler for1000 iterations on the whole corpus.
The sampleruses 1,087s per iteration, on average, using a singlecore, 2.3 GHz Intel Xeon machine.
For thehyperparameters, we set ?
to 0.1 and pexpand = 1/3to give a preference to the rules with smallfragments.
We built an s2t translation system withthe achieved U-trees after the 1000th iteration.
Weonly use one sample to extract the translationgrammar because multiple samples would result ina grammar that would be too large.11 From (Zollmann and Vogel, 2011), we find that theperformance of SAMT system is similar with themethod of labeling SCFG rules with POS tags.
Thus, tobe convenient, we only conduct experiments with theSAMT system.6.2 Analysis of The Gibbs SamplerTo evaluate the effectiveness of the Gibbs sampler,we explore the change of the training data?slikelihood with increasing sampling iterations.1.239E+081.243E+081.247E+081.251E+081.255E+081.259E+08100 200 300 400 500 600 700 800 900 1000Number of Sampling IterationsNegative-LogLikelihood random 1random 2random 3Figure 5.
Histograms of the training data?s likelihood vs.the number of sampling iterations.
In the figure, random1 to 3 refers to three independent runs of the samplerwith different initial U-trees as initialization states.Figure 5 depicts the negative-log likelihood ofthe training data after several sampling iterations.The results show that the overall likelihood of thetraining data is improved by the sampler.
Moreover,comparing the three independent runs, we see thatalthough the sampler begins with different initialU-trees, the training data?s likelihood is alwayssimilar during sampling.
This demonstrates thatour sampler is not sensitive to the random initialU-trees and can always arrive at a good final statebeginning from different initialization states.
Thus,we only utilize the U-trees from random 1 forfurther analysis hereafter.1.035E+071.040E+071.045E+071.050E+071.055E+071.060E+07100 200 300 400 500 600 700 800 900 1000Number of Sampling IterationsTotalNumberofFrontierNodesrandom 1random 2random 3Figure 6.
The total number of frontier nodes for thethree independent runs.6.3 Analysis of the U-tree StructureAcquiring better U-trees for translation is our finalpurpose.
However, are the U-trees achieved by the250Gibbs sampler appropriate for the tree-basedtranslation model?To answer this question, we first analyze theeffect of the sampler on the U-trees.
Figure 6shows the total number of frontier nodes in thetraining data during sampling.
The results showthat the number of frontier nodes increases withincreased sampling.
This tendency indicates thatour sampler prefers the tree structure with morefrontier nodes.
Consequently, the final U-treestructures can always be factored into many smallminimal translation rules.
Just as we have arguedin section 4.1, this is beneficial for a goodtranslation grammar.To demonstrate the above analysis, Figure 7shows a visual comparison between our U-tree(from random 1) and the binary parse tree (foundby head binarization).
Because the traditional parsetree is not binarized, we do not consider it for thisanalysis.
Figure 7 shows that whether it is thetarget tree fragment or the source string of the rule,our U-trees always tend to obtain the smallerones12.
This comparison verifies that our Bayesiantree induction model is effective in shifting the treestructures away from complex minimal rules,which tend to negatively affect translation.0200k400k600k800k1000k2 3 4 5 6 7 8 9 10 >=11U-Treebinary parse treeNumber of Nodes in the Target Tree FragmentNumberofRulesNumber of Words and Variables in the Source String0300k600k900k1200k1 2 3 4 5 6 7NumberofRulesFigure 7.
Histograms over minimal translation rulestatistics comparing our U-trees and binary parse trees.12 Binary parse trees get more tree fragments with twonodes than U-trees.
This is because there are manyunary edges in the binary parse trees, while no unaryedge exists in our U-trees.Specifically, we show an example of a binaryparse tree and our U-tree in Figure 8.
The exampleU-tree is more conducive to extracting effectivetranslation rules.
For example, to translate theChinese phrase ??
?
?, we can extract a rule (R2in Figure 9) directly from the U-tree because thephrase ??
??
is governed by a frontier node, i.e.,node ?VBD+RB?.
However, because no nodegoverns ??
??
in the binary parse tree, we canonly obtain a rule (R1 in Figure 9) with many extranodes and edges, such as node CD in R1.
Due tothese extra things, R1 is too large to show goodgenerality.wasQPdollarsUS1500onlyVBD NNSNNPCDRBNPNP?
??????
?NP-COMP(a) binary parse tree(b) U-treewas dollarsUS1500onlyVBD NNSNNPCDRB?
??????
?VBD+RB NNP+NNSCD...NNSVBD...NNSFigure 8.
Example of different tree structures.
The nodeNP-COMP is achieved by head binarization.
The bolditalic nodes with shadows denote frontier nodes.was QPonlyVBDCD:x0RBNPNPNP-COMP:x1was onlyVBD RB?
?VBD+RB?
x1x0?R1:R2:Figure 9.
Example rules to translate the Chinese phrase??
?
.?
R1 is extracted from Figure 8(a), i.e., thebinary parse tree.
R2 is from Figure 8(b), i.e., the U-tree.251Based on the above analysis, we can concludethat our proposed U-tree structures are conduciveto extracting small, minimal translation rules.
Thisindicates that the U-trees are more consistent withthe word alignment and are good at capturingbilingual mapping information.
Therefore, becauseparse trees are always constrained by cross-lingualstructure divergence, we believe that the proposedU-trees would result in a better translationgrammar.
We demonstrate this conclusion in thenext sub-section.6.4 Final Translation ResultsThe final translation results are shown in Table 1.In the table, lines 3-6 refer to the string-to-treesystems built with different types of tree structures.Table 1 shows that all our s2t systemsoutperform the Joshua (HPB) and Joshua (SAMT)system significantly.
This comparison verifies thesuperiority of our in-house s2t system.
Moreover,the results shown in Table 1 also demonstrate theeffectiveness of head binarization, which helps toimprove the s2t system using parse trees in alltranslation tasks.To test the effectiveness of our U-trees, we givethe s2t translation system using the U-trees (fromrandom 1).
The results show that the system usingU-trees achieves the best translation result from allof the systems.
It surpasses the s2t system usingparse trees by 1.47 BLEU points on MT04 and1.44 BLEU points on MT05.
Moreover, even usingthe binary parse trees, the achieved s2t system isstill lower than our U-tree-based s2t system by0.97 BLEU points on the combined test set.
Fromthe translation results, we can validate our formeranalysis that the U-trees generated by our Bayesiantree induction model are more appropriate forstring-to-tree translation than parse trees.System MT04 MT05 AllJoshua (HPB) 31.73 28.82 30.64Joshua (SAMT) 32.48 29.77 31.56s2t (parse-tree) 33.73* 30.25* 32.75*s2t (binary-parse-tree) 34.09* 30.99*# 32.92*s2t (U-tree) 35.20*# 31.69*# 33.89*#Table 1.
Results (in case-insensitive BLEU-4 scores) ofs2t systems using different types of trees.
The ?*?
and?#?
denote that the results are significantly better thanthe Joshua (SAMT) system and the s2t system usingparse trees (p<0.01).6.5 Large DataWe also conduct an experiment on a largerbilingual training data from the LDC corpus13.
Thetraining corpus contains 2.1M sentence pairs withapproximately 27.7M Chinese words and 31.9MEnglish words.
Similarly, we train a 5-gramlanguage model using the Xinhua portion of theEnglish Gigaword corpus and the English part ofthe training corpus.
With the same settings asbefore, we run the Gibbs sampler for 1000iterations and utilize the final U-tree structure tobuild a string-to-tree translation system.The final BLEU score results are shown in Table2.
In the scenario with a large data, the string-to-tree system using our U-trees still significantlyoutperforms the system using parse trees.System MT04 MT05 AllJoshua (HPB) 34.55 33.11 34.01Joshua (SAMT) 34.76 33.72 34.37s2t (parse-tree) 36.40* 34.53* 35.70*s2t (binary-parse-tree) 37.38*# 35.14*# 36.54*#s2t (U-tree) 38.02*# 36.12*# 37.34*#Table 2.
Results (in case-insensitive BLEU-4 scores) forthe large training data.
The meaning of ?*?
and ?#?
aresimilar to Table 1.7 Conclusion and Future WorkIn this paper, we explored a new direction to builda tree-based model based on unsupervisedBayesian trees rather than supervised parse trees.To achieve this purpose, we have made two majorefforts in this paper:(1) We have proposed a novel generativeBayesian model to induce effective U-trees fortree-based translation.
We utilized STSG in themodel to grasp bilingual mapping information.
Wefurther imposed a reasonable hierarchical prior onthe tree structures, encouraging small and frequentminimal rules for translation.
(2) To train the Bayesian tree inductionmodel efficiently, we developed a Gibbs samplerwith three novel Gibbs operators.
The operators aredesigned specifically to explore the infinite spaceof tree structures by performing local changes onthe tree structure.13 LDC category number : LDC2000T50, LDC2002E18,LDC2003E07, LDC2004T07, LDC2005T06,LDC2002L27, LDC2005T10 and LDC2005T34.252Experiments on the string-to-tree translationmodel demonstrated that our U-trees are betterthan the parse trees.
The translation results verifythat the well-designed unsupervised trees areactually more appropriate for tree-based translationthan parse trees.
Therefore, we believe that theunsupervised tree structure would be a promisingresearch direction for tree-based translation.In future, we plan to testify our sampler withvarious initial trees, such as the tree structureformed by (Zhang et al 2008).
We also plan toperform a detailed empirical comparison betweenSTST and SCFG under our settings.
Moreover, wewill further conduct experiments to compare ourmethods with other relevant works, such as (Cohnand Blunsom, 2009) and (Burkett and Klein, 2012).AcknowledgmentsWe would like to thank Philipp Koehn and threeanonymous reviewers for their valuable commentsand suggestions.
The research work has beenfunded by the Hi-Tech Research and DevelopmentProgram (?863?
Program) of China under GrantNo.
2011AA01A207, 2012AA011101, and2012AA011102.ReferencesPhil Blunsom, Trevor Cohn, Miles Osborne.
2008.Bayesian synchronous grammar induction.
InAdvances in Neural Information Processing Systems,volume 21, pages 161-168.Phil Blunsom, Trevor Cohn, Chris Dyer, and MilesOsborne.
2009.
A gibbs sampler for phrasalsynchronous grammar induction.
In Proc.
of ACL2009, pages 782-790.Phil Blunsom and Trevor Cohn.
2010.
Inducingsynchronous grammars with slice sampling.
In Proc.of NAACL 2010, pages 238-241.David Burkett and Dan Klein.
2008.
Two languages arebetter than one (for syntactic Parsing).
In Proc.
ofEMNLP 2008, pages 877-886.David Burkett, John Blitzer, and Dan Klein.
2010.
Jointparsing and alignment with weakly synchronizedgrammars.
In Proc.
of NAACL 2010, pages 127-135.David Burkett and Dan Klein.
2012.
Transforming treesto improve syntactic convergence.
In Proc.
ofEMNLP 2012, pages 863-872.David Chiang.
2007.
Hierarchical phrase-basedtranslation.
Computational Linguistics, 33 (2).
pages201-228.Dekai Wu.
1996.
A polynomial-time algorithm forstatistical machine translation.
In Proc.
of ACL 1996,pages 152-158.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23:377-404.Trevor Cohn and Phil Blunsom.
2009.
A bayesianmodel of syntax-directed tree to string grammarinduction.
In Proc.
of EMNLP 2009, pages 352-361.Trevor Cohn, Phil Blunsom, and Sharon Goldwater.2010.
Inducing tree-substitution grammars.
Journalof Machine Learning Research, pages 3053-3096.Brooke Cowan, Ivona Kucerova and Michael Collins.2006.
A discriminative model for tree-to-treetranslation.
In Proc.
of EMNLP 2006, pages 232-241.John DeNero and Dan Klein.
2007.
Tailoring wordalignments to syntactic machine translation.
In Proc.of ACL 2007, pages 17-24.John DeNero and Jakob Uszkoreit.
2011.
Inducingsentence structure from parallel corpora forreordering.
In Proc.
of EMNLP 2011, pages 193-203.Chris Dyer.
2010.
Two monolingual parses are betterthan one (synchronous parse).
In Proc.
of NAACL2010, pages 263-266.Jason Eisner.
2003.
Learning non-isomorphic treemappings for machine translation.
In Proc.
of ACL2003, pages 205-208.Michel Galley, Mark Hopkins, Kevin Knight and DanielMarcu.
2004.
What?s in a translation rule.
In Proc.
ofHLT-NAACL 2004, pages 273?280.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Proc.
ofACL-COLING 2006, pages 961-968.Jonathan Weese, Juri Ganitkevitch, Chris Callison-Burch, Matt Post and Adam Lopez.
2011.
Joshua 3.0:syntax-based machine translation with the thraxGrammar Extractor.
In Proc of WMT11, pages 478-484.Liang Huang, Kevin Knight and Aravind Joshi.
2006.
Asyntax-directed translator with extended domain oflocality.
In Proc.
of AMTA 2006, pages 65-73.Philipp Koehn, Franz Och, and Daniel Marcu.
2003.Statistical phrase-based translation, In Proc.
ofHLT/NAACL 2003, pages 48-54.253Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proc.
of EMNLP2004, pages 388?395.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,RichDUG =HQV &KULV '\HU DQG 2QG?HM %RMDU.
2007.Moses: open source toolkit for statistical machinetranslation.
In Proc.
of ACL 2007, pages 177-180.Abby Levenberg, Chris Dyer and Phil Blunsom.
2012.A bayesian model for learning SCFGs withdiscontiguous Rules.
In Proc.
of EMNLP 2012, pages223-232.Zhifei Li, Chris Callison-Burch, Chris Dyer, JuriGanitkevitch, Sanjeev Khudanpur, Lane Schwartz,Wren N.G.
Thornton, Jonathan Weese and Omar F.Zaidan.
2009.
Joshua: An open source toolkit forparsing-based machine translation.
In Proc.
of ACL2009, pages 135-139.Shujie Liu, Chi-Ho Li, Mu Li, Ming Zhou.
2012.
Re-training monolingual parser bilingually for syntacticSMT.
In Proc.
of EMNLP 2012, pages 854-862.Yang Liu, Qun Liu and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machinetranslation.
In Proc.
of ACL-COLING 2006, pages609-616.Yang Liu, Yajuan Lv and Qun Liu.
2009.
Improvingtree-to-tree translation with packed forests.
In Proc.of ACL-IJCNLP 2009, pages 558-566.Daniel Marcu, Wei Wang, Abdessamad Echihabi andKevin Knight.
2006.
SPMT: Statistical machinetranslation with syntactified target language phrases.In Proc.
of EMNLP 2006, pages 44-52.Franz Och, 2003.
Minimum error rate training instatistical machine translation.
In Proc.
of ACL 2003,pages 160-167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automaticevaluation of machine translation.
In Proc.
of ACL2002, pages 311-318.Slav Petrov, Leon Barrett, Romain Thibaux and DanKlein.
2006.
Learning accurate, compact, andinterpretable tree annotation.
In Proc.
of COLING-ACL 2006, pages 433-440.Chris Quirk, Arul Menezes and Colin Cherry.
2005.Dependency treelet translation: syntacticallyinformed phrasal SMT.
In Proc.
of ACL 2005, pages271-279.Libin Shen, Jinxi Xu and Ralph Weischedel.
2008.
Anew string-to-dependency machine translationalgorithm with a target dependency language model.In Proc.
of ACL-08, pages 577-585.Wei Wang, Kevin Knight, and Daniel Marcu.
2007.Binarizing syntax trees to improve syntax-basedmachine translation accuracy.
In Proc.
of EMNLP2007, pages 746-754.Wei Wang, Jonathan May, Kevin Knight, and DanielMarcu.
2010.
Re-structuring, re-labeling, and re-aligning for syntax-based machine translation.Computational Linguistics, 36(2):247?277.Feifei Zhai, Jiajun Zhang, Yu Zhou and ChengqingZong.
2012.
Tree-based translation without usingparse trees.
In Proc.
of COLING 2012, pages 3037-3054.Hao Zhang, Liang Huang, Daniel Gildea and KevinKnight.
2006.
Synchronous binarization for machinetranslation.
In Proc.
of HLT-NAACL 2006, pages256-263.Hao Zhang, Daniel Gildea, and David Chiang.
2008.Extracting synchronous grammars rules from wordlevel alignments in linear time.
In Proc.
of COLING2008, pages 1081-1088.Hao Zhang, Licheng Fang, Peng Xu, Xiaoyun Wu.2011a.
Binarized forest to string translation.
In Proc.of ACL 2011, pages 835-845.Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw, ChewLim Tan.
2009.
Forest-based tree sequence to stringtranslation model.
In Proc.
of ACL-IJCNLP 2009,pages 172-180.Jiajun Zhang, Feifei Zhai and Chengqing Zong.
2011b.Augmenting string-to-tree translation models withfuzzy use of source-side syntax.
In Proc.
of EMNLP2011, pages 204-215.Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, ChewLim Tan and Sheng Li.
2007.
A tree-to-treealignment-based model for statistical Machinetranslation.
MT-Summit-07.
pages 535-542Min Zhang, Hongfei Jiang, Ai ti Aw, Haizhou Li, ChewLim Tan and Sheng Li.
2008.
A tree sequencealignment-based tree-to-tree translation model.
InProc.
of ACL 2008, pages 559-567.Andreas Zollmann and Ashish Venugopal.
2006.
Syntaxaugmented machine translation via chart parsing.
InProc.
of Workshop on Statistical MachineTranslation 2006, pages 138-141.Andreas Zollmann and Stephan Vogel.
2011.
A word-class approach to labeling PSCFG rules for machinetranslation.
In Proc.
of ACL 2011, pages 1-11.254
