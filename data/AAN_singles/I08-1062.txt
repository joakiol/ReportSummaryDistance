Japanese-Spanish Thesaurus ConstructionUsing English as a PivotJessica  Ram?rez, Masayuki Asahara, Yuji MatsumotoGraduate  School of Information ScienceNara Institute of Science and TechnologyIkoma, Nara, 630-0192 Japan{jessic-r,masayu-a,matsu}@is.naist.jpAbstractWe present the results of research with thegoal of automatically creating a multilin-gual thesaurus based on the freely availableresources of Wikipedia and WordNet.
Ourgoal is to increase resources for naturallanguage processing tasks such as machinetranslation targeting the Japanese-Spanishlanguage pair.
Given the scarcity of re-sources, we use existing English resourcesas a pivot for creating a trilingual Japanese-Spanish-English thesaurus.
Our approachconsists of extracting the translation tuplesfrom Wikipedia, disambiguating them bymapping them to WordNet word senses.We present results comparing two methodsof disambiguation, the first using VSM onWikipedia article texts and WordNet defi-nitions, and the second using categoricalinformation extracted from Wikipedia, Wefind that mixing the two methods producesfavorable results.
Using the proposedmethod, we have constructed a multilingualSpanish-Japanese-English thesaurus con-sisting of 25,375 entries.
The same methodcan be applied to any pair of languages thatare linked to English in Wikipedia.1 IntroductionAligned data resources are indispensable compo-nents of many Natural Language Processing (NLP)applications; however lack of annotated data is themain obstacle for achieving high performance NLPsystems.
Current success has been moderate.
Thisis because for some languages there are few re-sources that are usable for NLP.Manual construction of resources is expensiveand time consuming.
For this reason NLP re-searchers have proposed semi-automatic or auto-matic methods for constructing resources such asdictionaries, thesauri, and ontologies, in order tofacilitate NLP tasks such as word sense disam-biguation, machine translation and other tasks.Hoglan Jin and Kam-Fai Wong (2002) automati-cally construct a Chinese dictionary from differentChinese corpora, and Ahmad Khurshid et al(2004) automatically develop a thesaurus for aspecific domain by using text that is related to animage collection to aid in image retrieval.With the proliferation of the Internet and theimmense amount of data available on it, a numberof researchers have proposed using the WorldWide Web as a large-scale corpus (Rigau et al,2002).
However due to the amount of redundantand ambiguous information on the web, we mustfind methods of extracting only the informationthat is useful for a given task.1.1 GoalsThis research deals with the problem of developinga multilingual Japanese-English-Spanish thesaurusthat will be useful to future Japanese-Spanish NLPresearch projects.A thesaurus generally means a list of wordsgrouped by concepts; the resource that we create issimilar because we group the words according tosemantic relations.
However, our resource is also473composed of three languages ?
Spanish, English,and Japanese.
Thus we call the resource we createda multilingual thesaurus.Our long term goal is the construction of a Japa-nese-Spanish MT system.
This thesaurus will beused for word alignments and building comparablecorpus.We construct our multilingual thesaurus by fol-lowing these steps:?
Extract the translation tuples from Wikipediaarticle titles?
Align the word senses of these tuples withthose of English WordNet (disambigua-tion)?
Construct a parallel thesaurus of Spanish-English-Japanese from these tuples1.2 Method summaryWe extract the translation tuples using Wikipedia?shyperlinks to articles in different languages andalign these tuples to WordNet by measuring cosinevector similarity measures between Wikipedia arti-cle texts and WordNet glosses.
We also use heuris-tics comparing the Wikipedia categories of a wordwith its hypernyms in WordNet.A fundamental step in the construction of a the-saurus is part of speech (POS) identification ofwords and word sense disambiguation (WSD) ofpolysemous entries.For POS identification, we cannot use Wikipedia,because it does not contain POS information.
Sowe use another well-structured resource, WordNet,to provide us with the correct POS for a word.These two resources, Wikipedia and WordNet,contain polysemous entries.
We also introduceWSD method to align these entries.We focus on the multilingual application ofWikipedia to help transfer information across lan-guages.
This paper is restricted mainly to nouns,noun phrases, and to a lesser degree, named enti-ties, because we only use Wikipedia article titles.2 Resources2.1 WikipediaWikipedia is an online multilingual encyclopediawith articles on a wide range of topics, in whichthe texts are aligned across different languages.Wikipedia has some features that make it suitablefor research such as:Each article has a title, with a unique ID.
?Redi-rect pages?
handle synonyms, and ?disambiguationpages?
are used when a word has several senses.
?Category pages?
contain a list of words that sharethe same semantic category.
For example the cate-gory page for ?Birds?
contains links to articles like?parrot?, ?penguin?, etc.
Categories are assignedmanually by users and therefore not all pages havea category label.Some articles belong to multiple categories.
Forexample, the article ?Dominican Republic?
be-longs to three categories: ?Dominican Republic?,?Island countries?
and ?Spanish-speaking coun-tries?.
Thus, the article Dominican Republic ap-pears in three different category pages.The information in redirect pages, disambigua-tion pages and Category pages combines to form akind of Wikipedia taxonomy, where entries areidentified by semantic category and word sense.2.2 WordNetWordNet (C. Fellbaum, 1998) ?is considered to beone of the most important resources in computa-tional linguistics and is a lexical database, inwhich concepts have been grouped into sets ofsynonyms (words with different spellings, but thesame meaning), called synsets, recording differentsemantic relations between words?.WordNet can be considered to be a kind of ma-chine-readable dictionary.
The main differencebetween WordNet and conventional dictionaries isthat WordNet groups the concepts into synsets, andeach concept has a small definition sentence call a?gloss?
with one or more sample sentences foreach synset.When we look for a word in WordNet it presentsa finite number of synsets, each one representing aconcept or idea.The entries in WordNet have been classified ac-cording to the syntactic category such as: nouns,verbs, adjectives and adverbs, etc.
These syntacticcategories are known as part of speech (POS).3 Related WorkCompared to well-established resources such asWordNet, there are currently comparatively fewerresearchers using Wikipedia as a data resource in474NLP.
There are, however, works showing promis-ing results.The work most closely related to this paper is(M. Ruiz et al, 2005), which attempts to create anontology by associating the English Wikipedialinks with English WordNet.
They use the ?Sim-ple English Wikipedia?
and WordNet version 1.7to measure similarity between concepts.
Theycompared the WordNet glosses and Wikipedia byusing the Vector Space Model, and presented re-sults using the cosine similarity.Our approach differs in that we disambiguatethe Wikipedia category tree using WordNet hyper-/hyponym tree.
We compare our approach to M.Ruiz et al, (2005) using it as the baseline in sec-tion 7.Oi Yee Kwong (1998) integrates different re-sources to construct a thesaurus by using WordNetas a pivot to fill gaps between thesaurus and a dic-tionary.Strube and Ponzetto (2006) present some ex-periments using Wikipedia for the computing se-mantic relatedness of words (a measure of degreeto which two concepts are related in a taxonomymeasured using all semantic relations), and com-pare the results with WordNet.
They also integrateGoogle hits, in addition to Wikipedia and WordNetbased measures.4 General DescriptionFirst we extract from Wikipedia all the alignedlinks i.e.
Wikipedia article titles.
We map these onto WordNet to determine if a word has more thanone sense (polysemous) and extract the ambiguousarticles.
We use two methods to disambiguate byassigning the WordNet sense to the polysemouswords, we use two methods: Measure the cosine similarity between eachWikipedia article?s content and the WordNetglosses. Compare the Wikipedia category to which thearticle belongs with the corresponding word inWordNet?s ontologyFinally, we substitute the target word into Japa-nese and Spanish.5 Extracting links from WikipediaThe goal is the acquisition of Japanese-Spanish-English tuples of Wikipedia?s article titles.
EachWikipedia article provides links to correspondingarticles in different languages.Every article page in Wikipedia has on the lefthand side some boxes labeled: ?navigation?,?search?, ?toolbox?
and finally ?in other languages?.This has a list of all the languages available for thatarticle, although the articles in each language donot all have exactly the same contents.
In mostcases English articles are longer or have more in-formation than their counterparts in other lan-guages, because the majority of Wikipedia collabo-rators are native English speakers.Pre-processing procedure:Before starting with the above phases, we firsteliminate the irrelevant information from Wikipe-dia articles, to make processing easy and faster.Thesteps applied are as follows:1.
Extract the Wikipedia web articles2.
Remove from the pages all irrelevant informa-tion, such as images, menus, and specialmarkup such as: ?
()?, ?&quot;?, ?
*?, etc...3.
Verify if a link is a redirected article and ex-tract the original article4.
Remove all stopwords and function words thatdo not give information about a specific topicsuch as ?the?, ?between?, ?on?, etc.MethodologyFigure 1.
The article ?bird?
in English, Spanish andJapaneseTake all articles titles that are nouns or named enti-ties and look in the articles?
contents for the box475called ?In other languages?.
Verify that it has atleast one link.
If the box exists, it links to the samearticle in other languages.
Extract the titles in theseother languages and align them with the originalarticle title.For instance, Figure 1. shows the English articletitled ?bird?
translated into Spanish as ?ave?, andinto Japanese as ?chourui?
(??).
When we clickSpanish or Japanese ?in other languages?
box, weobtain an article about the same topic in the otherlanguage.
This gives us the translation as its title,and we proceed to extract it.6 Aligning Wikipedia entries to WordNetsensesThe goal of aligning English Wikipedia entries toWordNet 2.1 senses is to disambiguate thepolysemous words in Wikipedia by means of com-parison with each sense of a given word existing inWordNet.A gloss in WordNet contains both an associationof POS and word sense.
For example, the entry?bark#n#1?
is different than ?bark#v#1?
becausetheir POSes are different.
In this example, ?n?
de-notes noun and ?v?
denotes verb.
So when wealign a Wikipedia article to a WordNet gloss, weobtain both POS and word sense information.MethodologyWe assign WordNet senses to Wikipedia?s polyse-mous articles.
Firstly, after extracting all links andtheir corresponding translations in Spanish andJapanese, we look up the English words in Word-Net and count the number of senses that each wordhas.
If the word has more than one sense, the wordis polysemous.We use two methods to disambiguate the am-biguous articles, the first uses cosine similarity andthe second uses Wikipedia?s category tree andWordNet?s ontology tree.6.1 Disambiguation using Vector Space ModelWe use a Vector Space Model (VSM) on Wikipe-dia and WordNet to disambiguate the POS and-word sense of Wikipedia article titles.
This givesus a correspondence to a WordNet gloss.Where V1 represents the Wikipedia article?s wordvector and V2  represents the WordNet gloss?
wordvector.In order to transfer the POS and word sense in-formation, we have to measure similarity metricbetween a Wikipedia article and a WordNet gloss.BackgroundVSM is an algebraic model, in which we convert aWikipedia article into a vector and compares it to aWordNet gloss (that has also been converted into avector) using the cosine similarity measure.
It takesthe set of words in some Wikipedia article andcompares them with the set of words of WordNetgloss.
Wikipedia articles which have more wordsin common are considered similar documents.In Figure 2 shows the vector of the word ?bank?,we want to compare the similitude between theWikipedia article ?bank-1?
with the EnglishWordNet ?bank-1?
and ?bank-2?.Figure 2.
Vector Space Model with the word ?bank?VSM Algorithm:1.
Encode the Wikipedia article as a vector,where each dimension represents a wordin the text of the article2.
Encode the WordNet gloss of each senseas a vector in the same mannerbank -1(a)bank -1(b)bank -2(a)bank -2(b)(a) Wikipedia(b) WordNet?||.||cos 2121VVVV ?=?4763.
Compute the similarity between theWikipedia vector and WordNet senses?vectors for a given word using the cosinemeasure4.
Link the Wikipedia article to the Word-Net gloss with the highest similarity6.2   Disambiguation by mapping the WordNetontological tree to Wikipedia categoriesThis method consists of mapping the WikipediaCategory tree to the WordNet ontological tree, bycomparing hypernyms and hyponyms.
The mainassumption is that there should be overlap betweenthe hypernyms and hyponyms of Wikipedia arti-cles and their correct WordNet senses.
We willrefer to this method as MCAT (?Map CATego-ries?)
throughout the rest of this paper.Wikipedia has in the bottom of each page a boxcontaining the category or categories to which thepage belongs, as we can see in Figure 3.
Each cate-gory links to the corresponding category page towhich the title is affiliated.
This means that the?category page?
contains a list of all articles thatshare a common category.Figure 3.
Relation between WordNet ontological treeand Wikipedia categoriesMethodology1.
We extract ambiguous Wikipedia articletitles (links) and the corresponding cate-gory pages2.
Extract the category pages, containing allpages which belong to that category, itssubcategories, and other category pagesthat have a branch in the tree and categoriesto which it belongs.3.
If the page has a category:3.1 Construct an n-dimensional vector con-taining the links and their categories3.2 Construct an n-dimensional vector of thecategory pages, where every dimensionrepresents a link which belongs to thatcategory4.
For each category that an article belongsto:4.1Map the categoryto the WordNet hy-pernym-/hyponym tree by looking ineach place that the given word ap-pears and verify if any of its branchesexist in the category page vector.4.2If a relation cannot be found then con-tinue with other categories4.3If there is no correspondence at allthen take the category pages vectorand look to see if any of the links hasrelation with the WordNet tree5.
If there is at least one correspondence thenassign this sense6.3 Constructing the multilingual thesaurusAfter we have obtained the English words with itscorresponding English WordNet sense aligned inthe three languages, we construct a thesaurus fromthese alignments.The thesaurus contains a unique ID for every tu-ple of word and POS that it will have informationabout the syntactic category.It also contains the sense of the word (obtain inthe disambiguation process) and finally a smalldefinition, which have the meaning of the word inthe three languages. We assign a unique ID to every tuple of words For Spanish and Japanese we assign for de-fault sense 1 to the first occurrence of theword if there exists more than 1 occurrencewe continue incrementing Extract a small definition from the corre-sponding Wikipedia articlesanimalbirdlife formWordNet tree                Wikipedia articleCategories: Birds477The definition of title word in Wikipedia tends tobe in the first sentence of the article.Wikipedia articles often include sentences defin-ing the meaning of the article?s title.
We mineWikipedia for these sentences include them in ourthesaurus.
There is a large body of research dedi-cate to identifying definition sentences (Wilks etal., 1997), However, we currently rely on verysimple patterns to this (e.g.
?X is a/are Y?,  ?X esun/a Y?, ?X ?/?
Y ?????).
Incorporatingmore sophisticated methods remains an area offuture work.7 Experiments7.1 Extracting links from WikipediaWe use the articles titles from Wikipedia which aremostly nouns (including named entities) in Spanish,English and Japanese; (es.wikipedia.org,en.wikipedia.org, and ja.wikipedia.org), specifi-cally ?the latest all titles?
and ?the latest pages ar-ticles?
files retrieved in April of 2006, and EnglishWordNet version 2.1.Our Wikipedia data contains a total of 377,621articles in Japanese; 2,749,310 in English; and194,708 in Spanish.
We got a total of 25,379 wordsaligned in the three languages.7.2 Aligning Wikipedia  entries to WordNetsensesIn WordNet there are 117,097 words and 141,274senses.
In Wikipedia (English) there are 2,749,310article titles.
78,247 word types exist in WordNet.There are 14,614 polysemous word types that willalign with one of the 141,274 senses in WordNet.We conduct our experiments using 12,906 am-biguous articles from Wikipedia.Table 1 shows the results obtained for WSD.The first column is the baseline (M. Ruiz et al,2005) using the whole article; the second column isthe baseline using only the first part of the article.The third column (MCAT) shows the results ofthe second disambiguation method (disambigua-tion by mapping the WordNet ontological tree toWikipedia categories).
Finally the last columnshows the results of combined method of takingthe MCAT results when available and falling backto MCAT otherwise.
The first row shows the senseassignments, the second row shows the incorrectsense assignment, and the last row shows the num-ber of word used for testing.7.2.1 Disambiguation using VSMIn the experiment using VSM, we used humanevaluation over a sample of 507 words to verify ifa given Wikipedia article corresponds to a givenWordNet gloss.
We took a the stratified sample ofour data selecting the first 5 out of every 66 entriesas ordered alphabetically for a total of 507 entries.We evaluate the effectiveness of using wholearticles in Wikipedia versus only a part (the firstpart up to the first subtitle), we found that the bestscore was obtained when using the whole articles81.5% (410 words) of them are correctly assignedand 18.5% (97 words) incorrect.DiscussionIn this experiment because we used VSM the resultwas strongly affected by the length of the glossesin WordNet, especially in the case of related defi-nitions because the longer the gloss the greater theprobability of it having more words in common.An example of related definitions in EnglishWordNet is the word ?apple?.
It has two senses asfollows: apple#n#1: fruit with red or yellow or greenskin and sweet to tart crisp whitish flesh. apple#n#2: native Eurasian tree widely culti-vated in many varieties for its firm roundededible fruits.The Wikipedia article ?apple?
refers to bothsenses, and so selection of either WordNet sense iscorrect.
It is very difficult for the algorithm to dis-tinguish between them.7.2.2 Disambiguation by mapping the WordNetontological tree to Wikipedia categoriesOur 12,906 articles taken from Wikipedia belongto a total of 18,810 associated categories.
Thus,clearly some articles have more than one category;however some articles also do not have any cate-gory.In WordNet there are 107,943 hypernym relations.478Baseline Our methodsVSM VSM (using firstpart of the article)MCAT VSM+ MCATCorrect sense identification 410(81.5%)403(79.48%)380(95%)426(84.02%)Incorrect sense identification 97(18.5%)104(20.52%)20(5%)81(15.98%)Total ambiguous words 507(100%)400(100%)507(100%)Table 1.
Results of disambiguationResults:We successfully aligned 2,239 Wikipedia articletitles with a WordNet sense.
400 of the 507 arti-cles in our test data have Wikipedia categorypages allowing us apply MCAT.
Our humanevaluation found that 95% (380 words) were cor-rectly disambiguated.
This outperformed disam-biguation using VSM, demonstrating the utility ofthe taxonomic information in Wikipedia andWordNet.
However, because not all words inWikipedia have categories, and there are very fewnamed entities in WordNet, the number of disam-biguated words that can be obtained with MCAT(2,239) is less than when using VSM, (12,906).Using only MCAT reduces the size of the Japa-nese-Spanish thesaurus.
We had the intuition thatby combining both disambiguation methods wecan achieve a better balance between coverageand accuracy.
VSM+MCAT use the MCAT WSDresults when available falling back to VSM resultsotherwise.We got an accuracy of 84.02% (426 of 507 to-tal words) with VSM+MCAT, outperforming thebaselines.Evaluating the coverage over Comparable cor-pus Corpus constructionWe construct comparable corpus by extractingfrom Wikipedia articles content information asfollows:Choose the articles whose content belongs to thethesaurus.
We only took the first part of the articleuntil a subtitle and split into sentences. Evaluation of  coverageWe evaluate the coverage of the thesaurus over anautomated comparable corpus automatically ex-tracted from Wikipedia.
The comparable corpusconsists of a total of 6,165 sentences collectedfrom 12,900 articles of Wikipedia.We obtained 34,525 types of words; we mapthem with 15,764 from the Japanese-English-Spanish thesaurus.
We found 10,798 types ofwords that have a coincidence that it is equivalentto 31.27%.We found this result acceptable for find informa-tion inside Wikipedia.8 Conclusion and future workThis paper focused on the creation of a Japanese-Spanish-English thesaurus and ontological rela-tions.
We demonstrated the feasibility of usingWikipedia?s features for aligning several lan-guages.We present the results of three sub-tasks:The first sub-task used pattern matching toalign the links between Spanish, Japanese, andEnglish articles?
titles.The second sub-task used two methods to dis-ambiguate the English article titles by assigningthe WordNet senses to each English word; thefirst method compares the disambiguation usingcosine similarity.
The second method usesWikipedia categories.
We established that usingWikipedia categories and the WordNet ontologygives promising results, however the number ofwords that can be disambiguated with this method479is small compared to the VSM method.
However,we showed that combining the two methodsachieved a favorable balance of coverage and ac-curacy.Finally, the third sub-task involved translatingEnglish thesaurus entries into Spanish and Japa-nese to construct a multilingual aligned thesaurus.So far most of research on Wikipedia focuseson using only a single language.
The main contri-bution of this paper is that by using a huge multi-lingual data resource (in our case Wikipedia)combined with a structured monolingual resourcesuch as WordNet, we have shown that it is possi-ble to extend a monolingual resource to other lan-guages.
Our results show that the method is quiteconsistent and effective for this task.The same experiment can be repeated usingWikipedia and WordNet on languages others thanJapanese and Spanish offering useful results espe-cially for minority languages.In addition, the use of Wikipedia and WordNetin combination achieves better results than thosethat could be achieved using either resource inde-pendently.We plan to extent the coverage of the thesaurusto other syntactic categories such as verbs, adverb,and adjectives.
We also evaluate our thesaurus inreal world tasks such as the construction of com-parable corpora for use in MT.AcknowledgmentsWe would like to thanks to Eric Nichols for hishelpful comments.ReferencesK.
Ahmad, M. Tariq, B. Vrusias and C. Handy.
2003.Corpus-Based Thesaurus Construction for ImageRetrieval in Specialist Domains.
In Proceedings ofECIR 2003. pp.
502-510.R.
Bunescu and M. Pa?ca.
2006.
Using encyclopedicknowledge for named entity disambiguation.
In Pro-ceedings of EACL-06, pp.
9-16.C.
Fellbaum.
1998.
WordNet: An Electronic LexicalDatabase.
Cambridge, Mass.
: MIT press.
pp.
25-43.J.
Honglan and Kam-Fai-Won.
2002.
A Chinese dic-tionary construction algorithm for information re-trieval.
ACM Transactions on Asian Language In-formation Processing.
pp.
281-296.C.
Manning and H. Sch?tze.
2000.
Foundations ofStatistical Natural Language Processing.
Cam-bridge, Mass.
: MIT press.
pp.
230-259.K.
Oi Yee, 1998.
Bridging the Gap between Dictionaryand Thesaurus.
COLING-ACL.
pp.
1487-1489.R.
Rada, H. Mili, E. Bicknell and M. Blettner.
1989.Development and application of a metric semanticnets.
IEEE Transactions on Systems, Man and Cy-bernetics, 19(1):17-30.M.
Ruiz, E. Alfonseca and P. Castells.
2005.
Auto-matic assignment of Wikipedia encyclopedic entriesto WordNet synsets.
In Proceedings of AWIC-05.Lecture Notes in Computer Science 3528. pp.
380-386, Springer, 2005.M.
Strube and S. P. Ponzetto.
2006.
WikiRelate!
Com-puting semantic relatedness using Wikipedia.
21stNational Conference on Artificial Intelligence.L.
Urdang.
1991.
The Oxford Thesaurus.
Clarendonpress.
Oxford.480
