Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 705?712Manchester, August 2008A Method for Automatic POS Guessing of Chinese Unknown WordsLikun QiuDepartment of Chinese Language andLiterature, Peking University / No.5 Yi-heyuan Road, Haidian District, Beijing,China 100871NEC Laboratories, Chinaqiulk@pku.edu.cnChangjian Hu, Kai ZhaoNEC Laboratories, China / 14F, Build-ing.A, Innovation Plaza, No.1Tsinghua Science Park, ZhongguancunEast Road, Haidian District, Beijing,China 100084{huchangjian, zhaokai}@research.nec.com.cnAbstractThis paper proposes a method for auto-matic POS (part-of-speech) guessing ofChinese unknown words.
It contains twomodels.
The first model uses a machine-learning method to predict the POS ofunknown words based on their internalcomponent features.
The credibility ofthe results of the first model is thenmeasured.
For low-credibility words, thesecond model is used to revise the firstmodel?s results based on the global con-text information of those words.
The ex-periments show that the first modelachieves 93.40% precision for all wordsand 86.60% for disyllabic words, whichis a significant improvement over thebest results reported in previous studies,which were 89% precision for all wordsand 74% for disyllabic words.
Further,the second model improves the results by0.80% precision for all words and 1.30%for disyllabic words.
?1 IntroductionSince written Chinese does not use blank spacesto denote word boundaries, Chinese word seg-mentation becomes an essential task for naturallanguage processing, as in many other Asian lan-guages (Thai, Japanese, Tibetan, etc.).
It is diffi-cult to build a complete dictionary comprising allwords, for new words are constantly being cre-ated.
As such, unknown words may greatly influ-ence the effectiveness of text processing.
Studies?2008.
Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license(http://creativecommons.org/licenses/by-nc-sa/3.0/).
Somerights reserved.on unknown words include detection, POSguessing, sense classification, etc.
Current meth-ods for automatic unknown word detection havebeen relatively successful and widely used inmany systems, yet automatic POS guessing forunknown words still remains a challenge fornatural language processing research.The task of POS guessing is quite differentfrom traditional POS tagging.
Traditional POStagging involves assigning a single POS tag to aword token, provided that it is known what POStag this word can take on in principle.
This taskrequires a lexicon that lists possible POS tags forall words.
However, unknown words are not inthe lexicon, so the task of POS guessing of un-known words involves the guessing of a correctPOS for an unknown word from the whole POSset of the current language.
Obviously, tradi-tional methods of POS tagging cannot effectivelysolve the problem of POS guessing of unknownwords.In previous work, two types of features havebeen used for the task of POS guessing of un-known Chinese words.
One type is contextualfeature, including local contextual features andglobal contextual features, and the other is inter-nal component feature.
Previous work has mainlyused context information to guess the POS tagsof unknown Chinese words, while a few designslooked at internal component features.
Althoughthere have been some attempts to combine thetwo types of features together, no reasonable ex-planation of the relationship between the twotypes of features has been given.It is well known that the properties of a struc-ture always depend on its internal componentstructure.
As such, it is natural for us to wonderwhether models based on internal componentfeatures alone can perform the POS guessing taskfor unknown Chinese words with both high pre-705cision and high recall.
Here we present a modelbased on internal component features of un-known words using CRFs (conditional randomfields).
The results are very good, especially formulti-syllabic words (excluding disyllabicwords).While in the previous model the precision ofPOS guessing of disyllabic words is relativelyhigh, there is still much room for further im-provement.
Considering that the usages of aword in real text can show the properties of aword, and that these features may be a usefulcomplement to internal component features, wedesigned a scheme to effectively utilize the twotypes of features together.
In this scheme, credi-bility scores for former guessing results are com-puted, and only those words with relatively lowercredibility scores are revised by the model basedon global context information.
The model basedon global context information simulates the be-havior of linguists in judging the POS of a word.Though the recall of the latter model is low, itcan revise some incorrect guessing results of theinitial model.According to Lu (2005), there are six maintypes of unknown Chinese words:(1) Abbreviation (acronym): e.g., ??
zhong-mei (China-U.S).
(2) Proper names (person?s name, place name,company name): e.g., ???
Wang Anshi(person?s name), ??
Penang (an island inMalaysia; place name), ??
Huawei (com-pany name).
(3) Derived words (words with affixes): e.g., ???
zong-jingli (general manager), ??
?xiandai-hua (modernize).
(4) Compounds: e.g., ??
huoyun (obtainpermission), ??
nisha (mud), ??
tubian(sudden change)(5) Numeric compounds: e.g., ????
siqianriyuan (four thousand Japanese yen), ?????
2003nian (year 2003)(6) Reduplicated words: e.g., ?
?
?
?yingbuyinggai (should or should not), ????
chuchujinjin (go in and go out)Proper names and numeric compounds are allnouns, so they don?t have the problem of POSguessing.
We will focus on abbreviation, derivedwords, compounds, and reduplicated words.The remainder of this paper is organized asfollows: in Section 2, we introduce some previ-ous work on POS guessing of unknown Chinesewords.
Sections 3, 4, and 5 describe our proposedmethod, which includes two models and an addi-tional process linking the two models together.
Indetail, Section 3 considers POS guessing for un-known Chinese words as a sequence-labelingproblem and proposes a model based on internalcomponent features to solve this task.
In Section4, we compute a credibility score for each guess-ing result based on the sequence type of thewords?
internal component structure.
This linksthe model in Section 3 and that of Section 5 to-gether.
Section 5 describes a model based onglobal context information to revise the guessingresults of the initial model that have relativelylower credibility scores.
Section 6 shows the ex-periments and results of our methods and a com-parison with previous work.
Section 7 presentsour conclusions.2 Previous WorkConsidering the features used during POS guess-ing, we have classified previous studies on POSguessing of unknown words into three types.The first type use only contextual features, in-cluding local context and global context.
Forexample, Nakagawa and Matsumoto (2006) pro-posed a probabilistic model to guess the POStags of unknown words by considering all theoccurrences of unknown words with the samelexical form in a document.
The parameters wereestimated using Gibbs sampling.
They also at-tempted to apply the model to semi-supervisedlearning, and conducted experiments on multiplecorpora.
The highest precision in the Chinesecorpus of their experiments was 67.85%.The second type use only internal componentfeatures, such as that of Chen and Bai (1997) andWu and Jiang (2000).
Chen and Bai (1997) ex-amined all unknown nouns, verbs, and adjectivesand reported 69.13% precision using Dice met-rics to measure the affix-category associationstrength and an affix-dependent entropy weight-ing scheme for determining the weightings be-tween prefix-category and suffix-category asso-ciations.
This approach is effective in processingderived words such as ???
xiandai-hua (mod-ernize), but performs poorly when encounteringcompounds such as ??
baozhi (inflation-proof).Wu and Jiang (2000) calculated P(Cat,Pos,Len)for each character, where Cat is the POS of aword containing the character, Pos is the positionof the character in that word, and Len is thelength of that word.
They then calculated thePOS probabilities for each unknown word as thejoint probabilities of P(Cat,Pos,Len) for its com-706ponent characters.
This approach was applied tounknown nouns, verbs, and adjectives of two tofour characters in length.
This approach exhibitslower recall for multi-syllabic words even if thetraining corpus is significantly large.The third type attempt to combine internalcomponent features and context information,such as that of Lu (2005) and Goh et al (2006).Lu (2005) describes a hybrid model that com-bines a rule-based model with two statisticalmodels for the task of POS guessing of unknownChinese words.
The rule-based model includes35 manual rules concerning the type, length, andinternal structure of unknown words, and the twostatistical models utilize context information andthe likelihood for a character to appear in a par-ticular position of words of a particular lengthand POS category, one of which is Wu and Ji-ang?s (2000) model.
It achieves a precision of89.00%, a significant improvement over the bestresult reported in previous studies, which was69.00%.
Goh et al (2006) propose a method forguessing the part-of-speech tags of detected un-known words using contextual and internal com-ponent features with maximum entropy models.Both Lu (2005) and Goh et al (2006) use onlylocal context and not global context.
As far asinternal component features are concerned, Lu(2005) uses only the word category feature in hisrule-based model while Goh et al (2006) usesonly the first character and last character features.From the above studies, we may find that meth-ods based on internal component features arevery promising, but this kind of features stillneeds much more attention.
Moreover, none ofthem has proved that methods based on contextinformation can improve the results of methodsbased on internal component features.
They onlyattempted to utilize different types of featurestogether and to give simultaneous results for bothtypes of features.Our method is among the third type of studies,but is different from the rest in the scheme ofcombining the two types of features together.
Inour method, internal component features play amore important role.
We will prove that a modelbased on this type of features alone can performvery well.
The other type of features acts as auseful supplement and can improve the results ofsome words in a certain degree.
The two modelsare linked together by assigning a credibilityscore for each POS guessing result generated bythe initial model.
The results with a relativelylower credibility score are identified and putthrough reconsideration by a method based onglobal context information.3 Model Based on Internal ComponentFeaturesIn this model, we consider the task of POSguessing of unknown words as a problem of se-quence labeling.
The inspiration for this ap-proach came from our observations of how hu-mans understand a word.
Usually an unknownword is regarded by human as a sequence ofcharacters or morphemes that can be partitionedinto several segments, where each segment isrelatively coherent in meaning.3.1 Conditional Random FieldsIn contrast with other models for labeling se-quences, such as HMM and MEMM, CRFs aregood at avoiding the label bias problem.
Theycondition on the entire observation sequence,thus avoiding the need for independent assump-tions between observations and vastly expandingthe set of features that can be incorporated intothe model without violating its assumptions.
Oneof the main advantages of a conditional model isits ability to explore a diverse range of featuresrelevant to a specific task.
As many studies haveshown, CRFs are the best models for solving thesequence labeling problem (Lafferty et al, 2001;Vail et al, 2007).
So we chose to use CRFs tosolve the POS guessing problem.3.2 The POS Guessing ModelWhile training, we use the words of a dictionaryas training data.
Those words will be consideredas sentences and then segmented and assignedPOS-tags by a standard word segmentation andPOS tagging tool.
By training with this data, wewill obtain a POS guessing model for unknownwords.
While testing, we still consider an un-known word as a sentence and process it with thesame tool.In the dictionary, most words have one POS-tag while a few have more.
The monosyllabicwords were omitted from the training.Feature AnalysisIn our CRFs model, we employed three maintypes of features: the components of words, thelengths of those components, and the POS tags ofthose components.Before the CRFs training, we analyzed the in-ternal component structure of the dictionarywords and assigned a proper POS-tag to eachcomponent.
There are four analysis schemes that707are different from each other in two aspects.
Thefirst aspect is the type of the component, whichmay be a character or immediate constituent (IC).Here, ?immediate constituent?
means constitu-ents that directly form a word.
For example, ?????
kexuejishubu (department of scienceand technology) has the following constituents:?
ke, ?
xue, ?
ji, ?
shu, ?
bu, ??
kexue,and ??
jishu, in which only ??
kexue, ?
?jishu and ?
bu are the immediate constituents of?????
kexuejishubu.
The second aspect isregarding the consideration of the POS-tag of thecomponent.
The four analysis schemes are listedin Table 1.POSComponentWith  WithoutCharacter Scheme 1 Scheme 2ImmediateConstituentScheme 3 Scheme 4Table 1.
Analysis SchemesIn Scheme 1 and Scheme 2, the tool segmentsa dictionary word until all components are char-acters, and in Scheme 1 only, each component isgiven a POS tag.
In Scheme 3 and Scheme 4, thetool segments a dictionary word only once to getits immediate constituents, and in Scheme 3 only,each component is given a POS tag.
For instance,?????
kexuejishubu (department of sci-ence and technology) will be segmented as ?
?/N?/N ?/N ?/N ?/N?
in Scheme 1 and ???/N?
?/N ?/N?
in Scheme 3.Feature Template SelectionFor each type of feature, we used the five tem-plates in Figure 1.
So in Schemes 1 and 3 thereare 15 templates, and in Schemes 2 and 4 thereare 10 templates.For instance, when training, ?????
willbe transformed as ??/N/N_B?
?/N/N_M ?/N/N_E in Scheme 3, in which N denotes that thePOS of the word ?????
is noun, while B,M and E denote the beginning, middle and endpositions of the word.U01:%x[-1,i]: the former component?s ith featureU02:%x[0,i]: the current component?s ith featureU03:%x[1,i]: the next component?s ith featureU04:%x[-1,i]/%x[0,i]: the former component?sith feature and the current component?s ith fea-tureU05:%x[0,i]/%x[1,i]: the current component?sith feature and the next component?s ith featureFigure 1.
List of Templates(i=1-3)By using a dictionary with these feature tem-plates for the training, we obtain a POS guessingmodel for unknown Chinese words.
If an un-known word such as ??
yongdian (electricityused) is tested, it would be analyzed as ?/V ?/N (to use, electricity) for the feature extractionand then it would be tagged as ?
/V/N_B ?/N/N_E.
That is, the word is assigned a POS ofnoun.4 Credibility ComputationThe initial model is based on the hypothesis thatthe syntactical properties of a word depend on itsinternal structure.
But the internal structure ofsome words are ambiguous.
For instance, both ??
yongyu (vocabulary [literally, ?used words?
])?and ?
yongjing (exert [literally, ?usestrength?])
both have the sequence V1N1 (whichis combined by a POS sequence of ?VN?
and alength sequence of ?11?
), yet the former one is anoun and the latter one is a verb.There are some other sequences like V1N1.All the words (especially disyllabic words)fitting to these sequences bring difficulty forPOS guessing model in Section 3.
In this section,we attempt to identify those words by computinga credibility score for each type of sequence.
Thelower the score, less credible the result of themodel.In detail, Formula 1 is used to compute thecredibility of a word that has a certain type ofsequence, e.g., ?
?
yongyu (vocabulary[literally, ?used words?])
with the sequence?V1N1?.
)()|()|( 1kjkjkk SCountPPSCountPPSCountC +=?==(1)In Formula 1, kC  denotes the credibility scoreof words that have the kth type of sequence SK.SK denotes a sequence as P1L1P2L2?
?PnLn, inwhich n means the quantity of components in thesequence SK; Pn and Ln mean the POS and lengthof the nth component of any word that has thesequence SK, respectively; Count(SK) denotes thequantity of words in the dictionary that have thesequence SK; and Count(SK|P=Pj) andCount(SK|P=Pj+1) denote the quantity of words inthe dictionary that have the sequence SK  and aretagged as POS Pj and Pj+1, respectively, in whichPj and Pj+1 are the two POSs that make the valueof Count(SK|P=Pj) a maximum of two.
Some se-quences with lower credibility scores are listed inTable 2.708For instance, the sequence type of the word ??
yongdian (electricity used) is V1N1, so itscredibility score is 0.65.
If the threshold is 0.8,this word will be considered a low-credibilityword and put through reconsideration by the fol-lowing model.Sequence CredibilityScoreProportionVg1Ng1 0.7 0.50%V1Ng1 0.67 1.90%Vg1N1 0.67 0.55%V1N1 0.65 2.99%V1Vn2 0.57 0.04%A1A1 0.55 0.22%N1A1 0.48 0.15%V1V2 0.44 0.05%V1Vi2 0.25 0.08%Table 2.
Examples of Sequences with LowCredibility Scores5 Model Based on Global ContextualFeaturesWords with relatively lower credibility scores(given in Section 4) will be revised by a modelbased on global context.
In this paper, we im-plement a model of voting by syntactical tem-plates, which derived from research results oflinguists.This process requires a relatively large corpusthat can provide enough context instances foreach under-processed word.
It is difficult for usto find a corpus that can provide enough in-stances of most unknown words, because manyof such instances have only been used for a rela-tively short time.
In this paper, we use searchengine as the source of corpus, i.e., throw a wordto a search engine and pick out instances fromthe returned snippets.Linguists have summarized systematic rulesfor judging the POS of a Chinese word based onits global distribution in real text (Guo, 2002).For example, generally a verb or adjective can bemodified by the word ???
(not) while a nouncannot.
Based on this knowledge, we designed aset of syntactical templates listed in Table 3.
Thetemplates indicate whether a word can be used insuch ways.For every word, we build phrases based onthese templates (see Table 3 for instances of ??
xihuan (like)) and send the phrases to a searchengine as queries.
For each query, the searchengine returns some snippets, which aregenerally in sentence form.
Then each word getsthree scores through a voting process in whichthe sentences act as ?voters.?
The three scores,Score(N), Score(V), and Score(A), denote thelikelihood score for the word to be a noun, a verbor an adjective, respectively.
Each voter votes byfollowing the criteria given in Figure 2.
In Figure2, Value(N), Value(V), and Value(A) are  con-stant values that are used to balance the threescores.Table 3.
Syntactical Templates with Instances of???
?1If the unknown word follows a transitive verband is at the end of a sentence or subsentence,Score(N)+=Value(N);If the unknown word follows a quantitative wordand is at the end of a sentence or subsentence,Score(N)+=Value(N);If the unknown word follows the word ???,???
or ??
?, Score(V)+=Value(V);If the unknown word follows the word ????,????
or ????
and is at the end of a sentenceor subsentence, or there is a following word thatis not a verb, Score(V)+=Value(N);If the unknown word follows the word ???,???
or ???
?, Score(A)+=Value(A).Figure 2.
Criteria for VotingFor each instance, Score(N), Score(V), andScore(A) will be added to the scores Value(N),Value(V), and Value(A), respectively.Although these templates are effective, thereare some exceptions brought by morphologyanalysis errors or other reasons, so we use anoutstanding method to filter the exceptions.
We1 Here ?*?
means the structure is invalid.Templates~ ?+~??+~??+~??+~?+~?+~?+~?
?+~ Instance?????????????*????*????????????
?709compute an outstanding value with Formula 2 tojudge whether the voting result is acceptable.
))(())(('))((POSScoreMaxPOSScoreMaxPOSScoreMaxO?=(2)In Formula 2, O  means the outstanding valueof a voting result; ))(( POSScoreMax  meansthe maximum score among the three scores and))((' POSScoreMax means the maximum scorebetween the other scores.
If O  is larger than athreshold, we assume the voting result to be ac-ceptable and adopt the result to revise the POSguessing result of the initial model.For instance, Score(N), Score(V), and Score(A)of the word??
yongyu (vocabulary [literally,?used words?])
are 50, 5 and 3 respectively.
SoO(??
)=(50-5)/50=0.9.6 Experiments and Results6.1 Data PreparationThe model based on CRFs is trained on theModern Chinese Grammar InformationDictionary (Yu, 1998) and tested on theContemporary Chinese Corpus of PekingUniversity (Yu et al, 2002).
The corpus is seg-mented and POS-tagged.
Both the dictionary andcorpus were constructed by the Institute of Com-putational Linguistics, Peking University.
Thecorpus was built using the content of all the newsarticles of the People?s Daily newspaper pub-lished in China from January to June 1998.
Weselected all verbs, nouns and adjectives from thedictionary, excluding monosyllabic words, astraining data.
The nouns, verbs and adjectives inthe corpus but not in the dictionary were consid-ered to be unknown words and used as testingdata.
The distribution of word length of the train-ing and testing data is presented in Table 4.Word Length Training TestingDisyllabic 40,103 11,108Tri-syllabic 12,167 12,901Four-character 1,180 1,055Five-character 0 279Total 53,450 25,343Table 4.
Distribution of Word Length in Trainingand Testing DataWe used ICTCLAS 1.0 (Zhang, 2002) to doword segmentation and POS tagging, becauseICTCLAS is known as one of the best tools forthose functions.
?CRF++, Yet Another CRF?toolkit (Kudo, 2005) was used as the implemen-tation of CRFs model and www.Baidu.com asthe search engine for our model based on contex-tual features.6.2 Results of the Proposed MethodThe results for the four schemes of our methodbased on internal component features are listed inTable 52.
From these results we may see thatScheme 1 is the best and Scheme 3 the secondbest, which means POS-tag of internal compo-nents is very useful feature in the POS guessingwork.
The comparison between Scheme 1 andScheme 3 indicates that character-based schemeis good for processing tri-syllabic words andfive-character words while IC-based scheme isgood for processing disyllabic words and four-character words.
Considering that most tri-syllabic words and five-character words are de-rivative words, while disyllabic words and four-character words are compounds, the results showthat the character-based scheme is good for proc-essing derivative words while the IC-basedscheme is good for processing compounds.
Allthe following improvements will be based onScheme 1.We assign the threshold of credibility score as0.8, and then there are 2,234 words with a credi-bility score lower than the threshold.
Thesewords are then put through the revision process.In the revision model, we set the values ofValue(N), Value(V), Value(A) and the out-standing threshold as 4, 1, 1, and 0.5, respec-tively, based on experience.
All above thresholdsare experimentally determined.
Finally, 1,357 outof the 2,234 words pass the outstanding examina-tion.
Among them, 462 results were differentfrom the former results and 302 of those werecorrectly revised, which resulted in the precisionof disyllabic words reaching 87.90% (see Table6).
Moreover, other 895 words, which have thesame result in the two models, reaches the preci-sion of 91.2%.
That means the credibility will bevery high when the two models generate thesame result.Although we believe that the former methodmay have equal effectiveness to most man-maderules, there are still several rules that must beincorporated in order to simplify our machine-learning method.
Here we incorporated tworeduplication rules to process two types ofreduplicated unknown words, respectively.
Theform of the first kind of words is ?V1?V2,  suchas ????
yingbuyinggai (should or should2 Precision, recall, F-measure are the same.710Table 7.
Results of Wu & Jiang's (2000)Table 6.
Results of Revision by Voting Modeland Two Rulesnot) and the form of the second kind of words is?V1V1V2V2,?
such as ????
chuchujinjin (goin and go out).
If a four-character word isassociated with one of the two forms and the firstcharacter is a verb, we revise its POS as a verbtag.The two reduplication rules correctly revised68 four-character words, which increased theprecision of four-character words to 97.10%, asignificant improvement over the previous bestresult, which was 92.89% (see Table 6).6.3 Comparison with Previous WorkWu & Jiang?s (2000)3 method is the most analo-gous with our method, yet they did not directlyreport the results in their paper.
In this paper, we3 Lu (2005) implemented Wu & Jiang?s (2000) model witha relatively small corpus as the training data.
The precisionof Wu & Jiang?s method reported by the paper is 77.90%with a recall of 63.82%.implement their model using the same data asour method.
The results of Wu & Jiang?s (2000)model are listed in Table 7.
It shows that theirmodel can guess the POS for disyllabic wordswith a relatively good F-measure (83.60%).
How-ever, the recall is not high for disyllabic(79.11%) and tri-syllabic (82.70%) words, andquite low for four-character (20.95%) and five-character (0%) words.
Our model in Section 3not only improves F-measure to 93.40%, but alsoimproves recalls to 86.60%, 99.22%, 92.03% and100% for multi-syllabic words in turn (Table 5,Scheme 1).Lu (2005) proposed a hybrid model thatachieved a precision of 89% for all words and74% for disyllabic words.
Compared with thatmethod, the hybrid model in this paper improvesthe precision to 94.20% for all words and87.90% for disyllabic words.
Although the ex-periments were not taken on the same data, thefigures reflect the difference of power betweenmethods in a certain degree.7 Conclusion and Future WorkThe results of this experiment show that ourmodel based on internal component features canachieve quite good results in POS guessing forunknown Chinese words, both in precision andrecall.
This proves that the internal componentWord Length Precision ofScheme 1Precision ofScheme 2Precision ofScheme 3Precision ofScheme 4Best ResultDisyllabic 86.60% 86.01 86.65% 85.21% 86.65%Tri-syllabic 99.22% 99.17% 98.65% 97.48% 99.22%Four-character 92.03% 91.47% 92.89% 89.76% 92.89%Five-character 100.00% 98.20% 98.92% 98.92% 100.00%Total 93.40% 93.08% 93.15% 91.80% 93.40%WordLengthTotalnumberPrecision (Correspond-ing value of before)Disyllabic 11,108 87.90% (86.60%)Tri-syllabic 12,901 99.22% (99.22%)Four-character1,055 97.10% (92.89%)Five-character279 100%    (100%)Total 25,343 94.20% (93.40%)Word Length Total Num-berTaggedNumberPrecision Recall F-measure (Corre-sponding value ofour method)Disyllabic 11,108 10,408 84.43% 79.11% 81.68%  (87.90%)Tri-syllabic 12,901 11,091 96.20% 82.70% 88.94%  (99.22%)Four-character 1,055 225 98.22% 20.95% 34.53%  (97.10%)Five-character 279 0 0 0 0             (100%)Total 25,343 21,724 90.58% 77.65% 83.60%   (94.20%)Table 5.
Results for Four Schemes of The Model Based on Internal Component Features711features of unknown words can be very useful inPOS guessing.
Moreover, the trained modelbased on internal component features is universaland robust.
One evidence is that the model canidentify POS correctly for most five-characterwords, even when there is no training data forthat type of words.Our results also show that the contextual fea-tures of unknown words can be an importantcomplement to help improve POS guessing.
Al-though models based on contextual featuresalone can?t achieve the same precision and recallas models based on internal component featuresdo, we may use contextual features as a comple-ment in processing those words with ambiguousstructure.In contrast with Lu (2005), we don?t use manymanual rules.
This does not mean that we believethose rules are useless in POS guessing.
In fact,our initial model based on the CRFs model haslearned the structure rules of Chinese words andcan even give a credibility score for each rule.That is, most of the rules have been incorporatedby the utilization of the CRFs model.In the future, to improve the results, we at-tempt to manually revise the training data.
No-tice that the training data was formed by seg-menting and tagging POS of each word in a dic-tionary using an existing tool like ICTCLAS.However, these tools usually generate quite afew errors on the words, because they are de-signed to handle sentence but not word.
Theseerrors were not revised in the experiment, whichdamaged the performance.
Thus, by manuallyrevising the training data, we hope to improvethe results in a certain degree.Although our experiments are mainly based oncontemporary Chinese, we believe that thismethod will also be applicable to other Asianlanguages such as Japanese.ReferencesAndy Wu and Zixin Jiang.
2000.
Statistically-enhanced New Word Identification in a Rule-basedChinese System.
In Proceedings of the 2nd ChineseLanguage Processing Workshop, pages 46?51.Chao-Jan Chen, Ming-Hong Bai, and Keh-Jiann Chen.1997.
Category Guessing for Chinese UnknownWords.
In Proceedings of the Natural LanguageProcessing Pacific Rim Symposium, pages 35?40.Chooi-Ling Goh, Masayuki Asahara, and Yuji Ma-tsumoto.
2006.
Machine Learning-based Methods toChinese Unknown Word Detection and POS TagGuessing.
In Journal of Chinese Language and Com-puting 16 (4):185-206Douglas L. Vail, Manuela M. Veloso, and John D.Lafferty.
2007.
Conditional Random Fields for Activ-ity Recognition.
In Proceedings of 2007 InternationalJoint Conference on Autonomous Agents and Multi-agent Systems.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional Random Fields: Probabilis-tic Models for Segmenting and Labeling SequenceData.
In Proceedings of International Conference onMachine Learning.Kevin Zhang.
ICTCLAS1.0.
http://www.nlp.org.cn/project/project.php?proj_id=6.Rui Guo.
2002.
Studies on Part-of-speech of Contem-porary Chinese.
Commercial Press, Beijing, China.Shiwen Yu.
1998.
Dictionary of Modern ChineseGrammar Information.
Tsinghua University Press.Beijing, China.Shiwen Yu, Huiming Duan, Xuefeng Zhu, and BingSun.
2002.
The Basic Processing of ContemporaryChinese Corpus at Peking University.
Technical Re-port, Institute of Computational Linguistics, PekingUniversity, Beijing, China.T Nakagawa, Y Matsumoto.
2006.
Guessing Parts-of-speech of Unknown Words Using Global Information.In Proceedings of the 21st International Conferenceon Computational Linguistics and 44th Annual Meet-ing of Association for Computational Linguistics,pages 705?712.Taku Kudo.
2005.
CRF++: Yet Another CRF toolkit.http://chasen.org/~taku/software/CRF++.Xiaofei Lu.
2005.
Hybrid Methods for POS Guessingof Chinese Unknown Words.
In Proceedings of the43th Annual Meeting of Association for ComputationalLinguistics Student Research Workshop, pages 1?6.712
