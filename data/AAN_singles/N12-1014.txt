2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 131?141,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsUnsupervised Learning on an Approximate Corpus?Jason Smith and Jason EisnerCenter for Language and Speech ProcessingJohns Hopkins University3400 N. Charles St., Baltimore, MD 21218, USA{jsmith,jason}@cs.jhu.eduUnsupervised learning techniques can take advan-tage of large amounts of unannotated text, but thelargest text corpus (the Web) is not easy to use inits full form.
Instead, we have statistics about thiscorpus in the form of n-gram counts (Brants andFranz, 2006).
While n-gram counts do not directlyprovide sentences, a distribution over sentences canbe estimated from them in the same way that n-gram language models are estimated.
We treat thisdistribution over sentences as an approximate cor-pus and show how unsupervised learning can beperformed on such a corpus using variational infer-ence.
We compare hidden Markov model (HMM)training on exact and approximate corpora of vari-ous sizes, measuring speed and accuracy on unsu-pervised part-of-speech tagging.1 IntroductionWe consider the problem of training generative mod-els on very large datasets in sublinear time.
It is wellknown how to train an HMM to maximize the like-lihood of a corpus of sentences.
Here we show howto train faster on a distribution over sentences thatcompactly approximates the corpus.
The distribu-tion is given by an 5-gram backoff language modelthat has been estimated from statistics of the corpus.In this paper, we demonstrate our approach ona traditional testbed for new structured-predictionlearning algorithms, namely HMMs.
We focus onunsupervised learning.
This serves to elucidate thestructure of our variational training approach, whichstitches overlapping n-grams together rather thantreating them in isolation.
It also confirms that atleast in this case, accuracy is not harmed by thekey approximations made by our method.
In future,we hope to scale up to the Google n-gram corpus(Brants and Franz, 2006) and learn a more detailed,explanatory joint model of tags, syntactic dependen-cies, and topics.
Our intuition here is that web-scaledata may be needed to learn the large number of lex-ically and contextually specific parameters.
?Work was supported in part by NSF grant No.
0347822.1.1 FormulationLet w (?words?)
denote an observation sequence,and let t (?tags?)
denote a hidden HMM state se-quence that may explain w. This terminology istaken from the literature on inducing part-of-speech(POS) taggers using a first-order HMM (Merialdo,1994), which we use as our experimental setting.Maximum a posteriori (MAP) training of anHMM p?
seeks parameters ?
to maximizeN ?
?wc(w) log?tp?
(w, t) + log Pr prior(?)
(1)where c is an empirical distribution that assignsprobability 1/N to each of the N sentences in atraining corpus.
Our technical challenge is to gen-eralize this MAP criterion to other, structured dis-tributions c that compactly approximate the corpus.Specifically, we address the case where c is givenby any probabilistic FSA, such as a backoff lan-guage model?that is, a variable-order Markovmodel estimated from corpus statistics.
Similar sen-tences w share subpaths in the FSA and cannot eas-ily be disentangled.
The support of c is typically infi-nite (for a cyclic FSA) or at least exponential.
Henceit is no longer practical to compute the tagging distri-bution p(t | w) for each sentence w separately, as intraditional MAP-EM or gradient ascent approaches.We will maximize our exact objective, or a cheapervariational approximation to it, in a way that cru-cially allows us to retain the structure-sharing.1.2 MotivationsWhy train from a distribution rather than a corpus?First, the foundation of statistical NLP is distribu-tions over strings that are specified by weighted au-tomata and grammars.
We regard parameter estima-tion from such a distribution c (rather than from asample) as a natural question.
Previous work onmodeling c with a distribution from another fam-ily was motivated by approximating a grammar or131model rather than generalizing from a dataset, andhence removed latent variables while adding param-eters (Nederhof, 2000; Mohri and Nederhof, 2001;Liang et al, 2008), whereas we do the reverse.Second, in practice, one may want to incorporatemassive amounts of (possibly out-of-domain) datain order to get better coverage of phenomena.
Mas-sive datasets usually require a simple model (given atime budget).
We propose that it may be possible touse a lot of data and a good model by reducing theaccuracy of the data representation instead.
Whiletraining will become more complicated, it can stillresult in an overall speedup, because a frequent 5-gram collapses into a single parameter of the esti-mated distribution that only needs to be processedonce per training iteration.
By pruning low-countn-grams or reducing the maximum n below 5, onecan further increase data volume for the fixed timebudget at the expense of approximation quality.Third, one may not have access to the originalcorpus.
If one lacks the resources to harvest theweb, the Google n-gram corpus was derived fromover a trillion words of English web text.
Privacyor copyright issues may prevent access, but one maystill be able to work with n-gram statistics: Michelet al (2010) used such statistics from 5 millionscanned books.
Several systems use n-gram counts(Bergsma et al, 2009; Lin et al, 2009) or otherweb statistics (Lapata and Keller, 2005) as featureswithin a classifier.
A large language model from n-gram counts yields an effective prior over hypothe-ses in tasks like machine translation (Brants et al,2007).
We similarly construct an n-gram model, buttreat it as the primary training data whose structureis to be explained by the generative HMM.
Thus ourcriterion does not explain the n-grams in isolation,but rather tries to explain the likely full sentencesw that the model reconstructed from overlapping n-grams.
This is something like shotgun sequencing,in which likely DNA strings are reconstructed fromoverlapping short reads (Staden, 1979); however, wetrain an HMM on the resulting distribution ratherthan merely trying to find its mode.Finally, unsupervised HMM training discovers la-tent structure by approximating an empirical distri-bution c (the corpus) with a latent-variable distribu-tion p (the trained HMM) that has fewer parameters.We show how to do the same where the distributionc is not a corpus but a finite-state distribution.
Ingeneral, this finite-state c could represent some so-phisticated estimate of the population distribution,using shrinkage, word classes, neural-net predictors,etc.
to generalize in some way beyond the trainingsample before fitting p. For the sake of speed andclear comparison, however, our present experimentstake c to be a compact approximation to the sampledistribution, requiring only n-grams.Spectral learning of HMMs (Hsu et al, 2009)also learns from a collection of n-grams.
It has thestriking advantage of converging globally to the trueHMM parameters (under a certain reparameteriza-tion), with enough data and under certain assump-tions.
However, it does not exploit context beyonda trigram (it will not maximize, even locally, thelikelihood of a finite sample of sentences), and can-not exploit priors or structure?e.g., that the emis-sions are consistent with a tag dictionary or that thetransitions encode a higher-order or factorial HMM.Our more general technique extends to other latent-variable models, although it suffers from variationalEM?s usual local optima and approximation errors.2 A variational lower boundOur starting point is the variational EM algorithm(Jordan et al, 1999).
Recall that this maximizes alower bound on the MAP criterion of equation 1, bybounding the log-likelihood subterm as follows:log?t p?
(w, t) (2)= log?t q(t)(p?
(w, t)/q(t))?
?t q(t) log(p?
(w, t)/q(t))= Eq(t)[log p?
(w, t)?
log q(t)] (3)This use of Jensen?s inequality is valid for any distri-bution q.
As Neal and Hinton (1998) show, the EMalgorithm (Dempster et al, 1977) can be regardedas locally maximizing the resulting lower bound byalternating optimization, where q is a free parame-ter.
The E-step optimizes q for fixed ?, and the M-step optimizes ?
for fixed q.
These computations aretractable for HMMs, since the distribution q(t) =p?
(t | w) that is optimal at the E-step (which makesthe inequality tight) can be represented as a lattice(a certain kind of weighted DFA), and this makesthe M-step tractable via the forward-backward algo-rithm.
However, there are many extensions such as132factorial HMMs and Bayesian HMMs in which anexpectation under p?
(t | w) involves an intractablesum.
In this setting, one may use variational EM, inwhich q is restricted to some parametric family q?that will permit a tractable M-step.
In this case theE-step chooses the optimal values of the variationalparameters ?
; the inequality is no longer tight.There are two equivalent views of how this pro-cedure is applied to a training corpus.
One view isthat the corpus log-likelihood is just as in (2), wherew is taken to be the concatenation of all trainingsentences.
The other view is that the corpus log-likelihood is a sum over many terms of the form (2),one for each training sentence w, and we bound eachsummand individually using a different q?.However, neither view leads to a practical imple-mentation in our setting.
We can neither concatenateall the relevant w nor loop over them, since we wantthe expectation of (2) under some distribution c(w)such that {w : c(w) > 0} is very large or infinite.Our move is to make q be a conditional distributionq(t | w) that applies to all w at once.
The follow-ing holds by applying Jensen?s inequality separatelyto each w in the expectation (this is valid since foreach w, q(t | w) is a distribution):Ec(w) log?t p?
(w, t) (4)= Ec(w) log?t q(t | w)(p?
(w, t)/q(t | w))?
Ec(w)?t q(t | w) log(p?
(w, t)/q(t | w))= Ecq(w,t)[log p?
(w, t)?
log q(t | w)] (5)where we use cq(w, t) to denote the joint distribu-tion c(w) ?
q(t | w).
Thus, just as c is our approx-imate corpus, cq is our approximate tagged corpus.Our variational parameters ?
will be used to param-eterize cq directly.
To ensure that cq?
can indeedbe expressed as c(w) ?
q(t | w), making the abovebound valid, it suffices to guarantee that our varia-tional family preserves the marginals:(?w)?t cq?
(w, t) = c(w)3 Finite-state encodings and algorithmsIn the following, we will show how to maximize(5) for particular families of p, c, and cq that canbe expressed using finite-state machines (FSMs)?that is, finite-state acceptors (FSAs) and transducers(FSTs).
This general presentation of our method en-ables variations using other FSMs.A path in an FSA accepts a string.
In an FST,each arc is labeled with a ?word : tag?
pair, so that apath accepts a string pair (w, t) obtained by respec-tively concatenating the words and the tags encoun-tered along the path.
Our FSMs are weighted in the(+,?)
semiring: the weight of any path is the prod-uct (?)
of its arc weights, while the weight assignedto a string or string pair is the total weight (+) of allits accepting paths.
An FSM is unambiguous if eachstring or string pair has at most one accepting path.Figure 1 reviews how to represent an HMM POStagger as an FST (b), and how composing this withan FSA that accepts a single sentence gives us thefamiliar HMM tagging lattice as an FST (c).
Theforward-backward algorithm sums over paths in thelattice via dynamic programming (Rabiner, 1989).In section 3.1, we replace the straight-line FSAof Figure 1a with an FSA that defines a more gen-eral distribution c(w) over many sentences.
Notethat we cannot simply use this as a drop-in replace-ment in the construction of Figure 1.
That wouldcorrespond to running EM on a single but uncer-tain sentence (distributed as c(w)) rather than a col-lection of observed sentences.
For example, in thecase of an ordinary training corpus of N sentences,the new FSA would be a parallel union (sum) ofN straight-line paths?rather than a serial concate-nation (product) of those paths as in ordinary EM(see above).
Running the forward algorithm on theresulting lattice would compute Ec(w)?t p(w, t),whose log is logEc(w)?t p(w, t) rather than ourdesired Ec(w) log?t p(w, t).
Instead, we use c insection 3.2 to construct a variational family cq?.
Wethen show in sections 3.3?3.5 how to compute andlocally maximize the variational lower bound (5).3.1 Modeling a corpus with n-gram countsn-gram backoff language models have been used fordecades in automatic speech recognition and statis-tical machine translation.
We follow the usual FSAconstruction (Allauzen et al, 2003).
The state of a 5-gram FSA model c(w) must remember the previous4-gram.
For example, it would include an arc fromstate defg (the previous 4-gram) to state efgh withlabel h and weight c(h | defg).
Then, with appro-priate handling of boundary conditions, a sentencew = .
.
.
defghi .
.
.
is accepted along a single path ofweight c(w) = ?
?
?
c(h | defg) ?
c(i | efgh) ?
?
?
.
Arcs133(a) w Time flies like an arrow(b) p(w,t) Start Vw:VStopNw:NDTw:DTw:Vw:V(c) w o p(w,t) Start VTime : V NTime : NVflies : VNflies : Nflies : Vflies : NPreplike : PrepVlike : Vlike : Preplike : VDTan : DTan : DT Narrow : NFigure 1: Ordinary HMM tagging with finite-state machines.
An arc?s label may have up to three components:?word:tag / weight.?
(Weights are suppressed for space.
State labels are not part of the machine but suggest the historyrecorded by each state.)
(a) w is an FSA that generates the sentence ?Time flies like an arrow?
; all arcs have probability1.
(b) p(w, t) is an FST representing an HMM (many arcs are not shown and words are abbreviated as ?w?).
Each arcw : t is weighted by the product of transition and emission probabilities, p(t | previous t) ?
p(w | t).
Composing (a)with (b) yields (c), an FST that encodes the joint probabilities p(w, t) of all possible taggings of the sentence w.of weight 0 can be omitted from the FSA.1To estimate a conditional probability like c(h |defg) above, we simply take an unsmoothed ratio oftwo n-gram counts.
This ML estimation means thatc will approximate as closely as possible the train-ing sample from which the counts were drawn.
Thatgives a fair comparison with ordinary EM, whichtrains directly on that sample.
(See discussion at theend of section 1.2 for alternatives.
)Yet we decline to construct a full 5-gram model,which would not be as compact as desired.
A col-lection of all web 5-grams would be nearly as largeas the web itself (by Zipf?s Law).
We may not havesuch a collection.
For example, the Google n-gramcorpus version 2 contains counts only for 1-gramsthat appear at least 40 times and 2-, 3-, 4-, and 5-grams that appear at least 10 times (Lin et al, 2009).1The FSA?s initial state is the unigram history #, and its finalstates (which have no outgoing arcs) are the other states whosen-gram labels end in #.
Here # is a boundary symbol that fallsbetween sentences.
To compute the weighted transitions, sen-tence boundaries must be manually or automatically annotated,either on the training corpus as in our present experiments, ordirectly on the training n-grams if we have only those.To automatically find boundaries in an n-gram collection,one could apply a local classifier to each n-gram.
But in princi-ple, one could exploit more context and get a globally consistentannotation by stitching the n-grams together and applying themethods of this paper?replacing p?
with an existing CRF sen-tence boundary detector, replacing c with a document-level (notsentence-level) language model, and optimizing cq?
to be a ver-sion of c that is probabilistically annotated with sentence bound-aries, which yields our desired distribution over sentences.Instead, we construct a backoff language model.This FSA has one arc for each n-gram in the col-lection.
Our algorithm?s runtime (per iteration) willbe linear in the number of arcs.
If the 5-gram defghis not in our collection, then there can be no h arcleaving defg.
When encountering h in state defg, theautomaton will instead take a failure arc (Allauzenet al, 2003) to the ?backoff state?
efg.
It may beable to consume the h from that state, on an arc withweight c(h | efg); or it may have to back off furtherto fg.
Each state?s failure arc is weighted such thatthe state?s outgoing arcs sum to 1.
It is labeled withthe special symbol ?, which does not contribute tothe word string accepted along a path.We take care never to allow backoff to the emptystate ,2 since we find that c(w) is otherwise toocoarse an approximation to English: sampled sen-tences tend to be disjointed, with some words gener-ated in complete ignorance of their left context.3.2 The variational distribution cq(w, t)The ?variational gap?
between (4) and (5) isEc(w)KL(q(t | w) || p?
(t | w)).
That is, the boundis good if q does a good job of approximating p?
?stagging distribution on a randomly drawn sentence.Note that n?1 is the order of our n-gram Markov2To prevent such backoff, it suffices to include all 2-gramswith count > 0.
But where the full collection of 2-grams isunavailable or too large, one can remove the empty state (andrecursively remove all states that transition only to removedstates), and then renormalize the model locally or globally.134model c(w) (i.e., each word is chosen given the pre-vious n ?
1 words).
Let np ?
1 be the order of theHMM p?
(w, t) that we are training: i.e., each tag ischosen given the previous np ?
1 tags.
Our experi-ments take np = 2 (a bigram HMM) as in Figure 1.We will take q?
(t | w) to be a conditional Markovmodel of order nq ?
1.3 It will predict the tag at po-sition i using a multinomial conditioned on the pre-ceding nq?1 tags and on the word n-gram ending atposition i (where n is as large as possible such thatthis n-gram is in our training collection).
?
is thecollection of all multinomial parameters.If nq = np, then our variational gap can be made 0as in ordinary non-variational EM (see section 3.5).In our experiments, however, we save memory bychoosing nq = 1.
Thus, our variational gap is tightto the extent that a word?s POS tag under the modelp?
is conditionally independent of previous tags andthe rest of the sentence, given an n-word window.4This is the assumption made by local classificationmodels (Punyakanok et al, 2005; Toutanova andJohnson, 2007).
Note that it is milder than the ?onetagging per n-gram?
hypothesis (Dawborn and Cur-ran, 2009; Lin et al, 2009), which claims that each5-gram (and therefore each sentence!)
is unambigu-ous as to its full tagging.
In contrast, we allow thata tag may be ambiguous even given an n-word win-dow; we merely suppose that there is no further dis-ambiguating information accessible to p?.5We can encode the resulting cq(w, t) as an FST.With nq = 1, the states of cq are isomorphic to thestates of c. However, an arc in c from defg withlabel h and weight 0.2 is replaced in cq by severalarcs?one per tag t?with label h : t and weight0.2 ?
q?
(t | defgh).6 We remark that an encoding of3A conditional Markov model is a simple case of amaximum-entropy Markov model (McCallum et al, 2000).4At present, the word being tagged is the last word in thewindow.
We do have an efficient modification in which the win-dow is centered on the word, by using an FST cq that delays theemission of a tag until up to 2 subsequent words have been seen.5With difficulty, one can construct English examples thatviolate our assumption.
(1) ?Some monitor lizards fromAfrica .
.
.
?
versus ?Some monitor lizards from a distance .
.
.
?
:there are words far away from ?monitor?
that help disambiguatewhether ?monitor?
is a noun or a verb.
(?Monitor lizards?
area species, but some people like to monitor lizards.)
(2) ?Timeflies?
: ?flies?
is more likely to be a noun if ?time?
is a verb.6In the case nq > 1, the states of c would need to be splitin order to remember nq ?
1 tags of history.
For example, ifq(t | w) as an FST would be identical except fordropping the c factor (e.g., 0.2) from each weight.Composing c ?
q would then recover cq.This construction associates one variational pa-rameter in ?
with each arc in cq?that is, with eachpair (arc in c, tag t), if nq = 1.
There would be lit-tle point in sharing these parameters across arcs ofcq, as that would reduce the expressiveness of thevariational distribution without reducing runtime.7Notice that maximizing equation (5) jointly learnsnot only a compact slow HMM tagger p?, but also alarge fast tagger q?
that simply memorizes the likelytags in each n-gram context.
This is reminiscent ofstructure compilation (Liang et al, 2008).3.3 Computing the variational objectiveThe expectation in equation (5) can now be com-puted efficiently and elegantly by dynamic program-ming over the FSMs, for a given ?
and ?.We exploit our representation of cq?
as an FSMover the (+,?)
semiring.
The path weights repre-sent a probability distribution over the paths.
In gen-eral, it is efficient to compute the expected value ofa random FSM path, for any definition of value thatdecomposes additively over the path?s arcs.
The ap-proach is to apply the forward algorithm to a versionof cq?
where we now regard each arc as weightedby an ordered pair of real numbers.
The (+,?)
op-erations for combining weights (section 3) are re-placed with the operations of an ?expectation semir-ing?
whose elements are such pairs (Eisner, 2002).Suppose we want to find Ecq?
(w,t) log q?
(t | w).To reduce this to an expected value problem, wemust assign a value to each arc of cq?
such that thec is Figure 1a, splitting its states with nq = 2 would yield acq with a topology like Figure 1c, but with each arc having anindependent variational parameter.7One could increase the number of arcs and hence varia-tional parameters by splitting the states of cq to remember morehistory.
In particular, one could increase the width nq of the tagwindow, or one could increase the width of the word window bysplitting states of c (without changing the distribution c(w)).Conversely, one could reduce the number of variational pa-rameters by further restricting the variational family.
For exam-ple, requiring q(t | w) to have entropy 0 (analogous to ?hardEM?
or ?Viterbi EM?)
would associate a single deterministictag with each arc of c. This is fast, makes cq as compact as c,and is still milder than ?one tagging per n-gram.?
More gener-ously, one could allow up to 2 tags per arc of c, or use a low-dimensional representation of the arc?s distribution over tags.135total value of a path accepting (w, t) is log q?
(t |w).
Thus, let the value of each arc in cq?
be the logof its weight in the isomorphic FST q?
(t | w).8We introduce some notation to make this precise.A state of cq?
is a pair of the form [hc, hq], where hcis a state of c (e.g., an (n?
1)-word history) and hqis an (nq ?
1)-tag history.
We saw in the previoussection that an arc a leaving this state, and labeledwith w : t where w is a word and t is a tag, willhave a weight of the form kadef= c(w | hc)?a where?adef= q?
(t | hcw, hq).
We now let the value vadef=log ?a.9 Then, just as the weight of a path accepting(w, t) is?a ka = cq?
(w, t), the value of that pathis?a va = log q?
(t | w), as desired.To compute the expected value r?
over all paths,we follow a generalized forward-backward recipe(Li and Eisner, 2009, section 4.2).
First, run the for-ward and backward algorithms over cq?.10 Now theexpected value is a sum over all arcs of cq?, namelyr?
=?a ?akava?a, where ?a denotes the forwardprobability of arc a?s source state and ?a denotesthe backward probability of arc a?s target state.Now, in fact, the expectation we need to computeis not Ecq?
(w,t) log q?
(t | w) but rather equation (5).So the value va of arc a should not actually belog ?a but rather log ?a ?
log ?a where ?adef= p?
(t |8The total value is then the sum of the logs, i.e., the logof the product.
This works because q?
is unambiguous, i.e., itcomputes q?
(t | w) as a product along a single accepting path,rather than summing over multiple paths.9The special case of a failure arc a goes from [hc, hq] to[h?c, hq], where h?c is a backed-off version of hc.
It is labeledwith ?
: , which does not contribute to the word string ortag string accepted along a path.
Its weight ka is the weightc(?
| hc) of the corresponding failure arc in c from hc to h?c.We define vadef= 0, so it does not contribute to the total value.10Recall that the forward probability of each state is definedrecursively from the forward probabilities of the states that havearcs leading to it.
As our FST is cyclic, it is not possible to visitthe states in topologically sorted order.
We instead solve thesesimultaneous equations by a relaxation algorithm (Eisner, 2002,section 5): repeatedly sweep through all states, updating theirforward probability, until the total forward probability of all fi-nal states is close to the correct total of 1 =?w,t cq?
(w, t)(showing that we have covered all high-prob paths).
A corre-sponding backward relaxation is actually not needed yet (we doneed it for ??
in section 3.4): backward probabilities are just 1,since cq?
is constructed with locally normalized probabilities.When we rerun the forward-backward algorithm after a pa-rameter update, we use the previous solution as a starting pointfor the relaxation algorithm.
This greatly speeds convergence.hp) ?
p?
(w | t).
This is a minor change?except thatva now depends on hp, which is the history of np?1previous tags.
If np > nq, then a?s start state doesnot store such a long history.
Thus, the value of aactually depends on how one reaches a!
It is prop-erly written as vza, where za is a path ending witha and z is sufficiently long to determine hp.11Formally, let Za be a ?partitioning?
set of paths toa, such that any path in cq?
from an initial state tothe start state of a must have exactly one z ?
Za asa suffix, and each z ?
Za is sufficiently long so thatvza is well-defined.
We can now find the expectedvalue as r?
=?a?z?Za ?z(?z?z kz)kavza?a.The above method permits p?
to score the tag se-quences of length np that are hypothesized by cq?.One can regard it as implicitly running the general-ized forward-backward algorithm over a larger FSTthat marries the structure of cq?
with the np-gramHMM structure,12 so that each value is again local toa single arc za.
However, it saves space by workingdirectly on cq?
(which has manageable size becausewe deliberately kept nq small), rather than material-izing the larger FST (as bad as increasing nq to np).TheZa trick usesO(CTnq) rather thanO(CTnp)space to store the FST, where C is the number ofarcs in c (= number of training n-grams) and T isthe number of tag types.
With or without the trick,runtime isO(CTnp+BCTnq), whereB is the num-11By concatenating z?s start state?s hq with the tags along z.Typically z has length np ?
nq (and Za consists of the pathsof that length to a?s start state).
However, z may be longer if itcontains ?
arcs, or shorter if it begins with an initial state.12Constructed by lazy finite-state intersection of cq?
and p?
(Mohri et al, 2000).
These do not have to be n-gram taggers,but must be same-length FSTs (these are closed under inter-section) and unambiguous.
Define arc values in both FSTs suchthat for any (w, t), cq?
and p?
accept (w, t) along unique pathsof total values v = ?
log q?
(t | w) and v?
= log p?
(w, t), re-spectively.
We now lift the weights into the expectation semir-ing (Eisner, 2002) as follows.
In cq?, replace arc a?s weightka with the semiring weight ?ka, kava?.
In p?
, replace arc a?
?sweight with ?1, v?a??.
Then if k = cq?
(w, t), the intersectedFST accepts (w, t) with weight ?k, k(v + v?)?.
The expecta-tion of v+v?
over all paths is then a sum?za ?zarza?za overarcs za of the intersected FST?we are using za to denote thearc in the intersected FST that corresponds to ?a in cq?
whenreached via path z,?
and rza to denote the second componentof its semiring weight.
Here ?za and ?za denote the forwardand backward probabilities in the intersected FST, defined fromthe first components of the semiring weights.
We can get themmore efficiently from the results of running forward-backwardon the smaller cq?
: ?za = ?z?z?z kz and ?za = ?a = 1.136ber of forward-backward sweeps (footnote 10).
Theordinary forward algorithm requires nq = np andtakesO(CTnp) time and space on a length-C string.3.4 Computing the gradient as wellTo maximize our objective (5), we compute its gra-dient with respect to ?
and ?.
We follow an efficientrecipe from Li and Eisner (2009, section 5, case 3).The runtime and space match those of section 3.3,except that the runtime rises to O(BCTnp).13First suppose that each va is local to a single arc.We replace each weight ka with k?a = ?ka, kava?in the so-called expectation semiring, whose sumand product operations can be found in Li and Eis-ner (2009, Table 1).
Using these in the forward-backward algorithm yields quantities ?
?a and ?
?athat also fall in the expectation semiring.14 (Theirfirst components are the old ?a and ?a.)
Thedesired gradient15 ??k?,?r??
is?a ??a(?k?a)?
?a,16where?k?a = (?ka,?
(kava)) = (?ka, (?ka)va+ka(?va)).
Here?
gives the vector of partial deriva-tives with respect to all ?
and ?
parameters.
Yet each?k?a is sparse, with only 3 nonzero components, be-cause k?a depends on only one ?
parameter (?a) andtwo ?
parameters (via ?a as defined in section 3.3).When np > nq, we sum not over arcs a of cq?
butover arcs za of the larger FST (footnote 12).
Againwe can do this implicitly, by using the short path zain cq?
in place of the arc za.
Each state of cq?
mustthen store ??
and ??
values for each of the Tnp?nqstates of the larger FST that it corresponds to.
(In thecase np ?
nq = 1, as in our experiments, this fortu-nately does not increase the total asymptotic space,13An alternative would be to apply back-propagation(reverse-mode automatic differentiation) to section 3.3?s com-putation of the objective.
This would achieve the same runtimeas in section 3.3, but would need as much space as time.14This also computes our objective r?
: summing the ??
?s of thefinal states of cq?
gives ?k?, r??
where k?
= 1 is the total probabil-ity of all paths.
This alternative computation of the expectationr?, using the forward algorithm (instead of forward-backward)but over the expectation semiring, was given by Eisner (2002).15We are interested in ?r?.
?k?
is just a byproduct.
We re-mark that ?k?
6= 0, even though k?
= 1 for any valid parametervector ?
(footnote 14), as increasing ?
invalidly can increase k?.16By a product of pairs we always mean ?k, r?
?s, t?
def=?ks, kt+ rs?, just as in the expectation semiring, even thoughthe pair?k?a is not in that semiring (its components are vectorsrather than scalars).
See (Li and Eisner, 2009, section 4.3).
Wealso define scalar-by-pair products as k?s, t?
def= ?ks, kt?.since each state of cq?
already has to store T arcs.
)With more cleverness, one can eliminate thisextra storage while preserving asymptotic runtime(still using sparse vectors).
Find ?
?k?, (?r?)(1)?
=?a ??a?
?ka, 0???a.
Also find ?r?, (?r?)(2)?
=?a?z?Za?z(?z?z?kz,?kz?)?kavza,?(kavza)??a.
Now our desired gradient ?r?
emerges as(?r?
)(1) + (?r?)(2).
The computation of (?r?
)(1)uses modified definitions of ?
?a and ?
?a that dependonly on (respectively) the source and target states ofa?not za.17 To compute them, initialize ??
(respec-tively ??)
at each state to ?1, 0?
or ?0, 0?
according towhether the state is initial (respectively final).
Nowiterate repeatedly (footnote 10) over all arcs a: Add?
?a?ka, 0?
+?z?Za ?z(?z?z kz)?0, kavza?
to the??
at a?s target state.
Conversely, add ?ka, 0??
?a tothe ??
at a?s source state, and for each z ?
Za, add(?z?z kz)?0, kavza?
?a to the ??
at z?s source state.3.5 Locally optimizing the objectiveRecall that cq?
associates with each [hc, hq, w] ablock of ?
parameters that must be ?
0 and sum to1.
Our optimization method must enforce these con-straints.
A standard approach is to use a projectedgradient method, where after each gradient step on?, the parameters are projected back onto the prob-ability simplex.
We implemented another standardapproach: reexpress each block of parameters {?a :a ?
A} as ?adef= exp ?a/?b?A exp ?b, as is possi-ble iff the ?a parameters satisfy the constraints.
Wethen follow the gradient of r?
with respect to the new?
parameters, given by ?r?/?
?a = ?a(?r?/?
?a?EA)where EA =?b ?b(?r?/?
?b).Another common approach is block coordinateascent on ?
and ?
?this is ?variational EM.?
M-step: Given ?, we can easily find optimal esti-mates of the emission and transition probabilities ?.They are respectively proportional to the posteriorexpected counts of arcs a and paths za under cq?,namely N ?
?aka?a and N ?
?z(?z?z kz)ka?a.E-step: Given ?, we cannot easily find the opti-mal ?
(even if nq = np).18 This was the rea-17First components ?a and ?a remain as in cq?.
?
?a sumspaths to a.
?
?ka, 0??
?a can?t quite sum over paths starting witha (their early weights depend on z), but (?r?
)(2) corrects this.18Recall that cq?
must have locally normalized probabilities(to ensure that its marginal is c).
If nq = np, the optimal ?is as follows: we can reduce the variational gap to 0 by setting137son for gradient ascent.
However, for any singlesum-to-1 block of parameters {?a : a ?
A}, itis easy to find the optimal values if the others areheld fixed.
We maximize LAdef= r?
+ ?A?a?A ?a,where ?A is a Lagrange multiplier chosen so thatthe sum is 1.
The partial derivative ?r?/?
?a can befound using methods of section 3.4, restricting thesums to za for the given a.
For example, follow-ing paragraphs 2?3 of section 3.4, let ?
?a, ra?def=?z?Za ?
?za, rza?
where ?
?za, rza?def= ??za?
?za.19Setting ?LA/?
?a = 0 implies that ?a is propor-tional to exp((ra +?z?Za ?za log ?za)/?a).20Rather than doing block coordinate ascent by up-dating one ?
block at a time (and then recomputingra values for all blocks, which is slow), one can takean approximate step by updating all blocks in paral-lel.
We find that replacing the E-step with a singleparallel step still tends to improve the objective, andthat this approximate variational EM is faster thangradient ascent with comparable results.214 Experiments4.1 Constrained unsupervised HMM learningWe follow the unsupervised POS tagging setup ofMerialdo (1994) and many others (Smith and Eis-ner, 2005; Haghighi and Klein, 2006; Toutanova andJohnson, 2007; Goldwater and Griffiths, 2007; John-son, 2007).
Given a corpus of sentences, one seeksthe maximum-likelihood or MAP parameters of a bi-gram HMM (np = 2).
The observed sentences, forq?
(t | hcw, hq) to the probability that t begins with t if werandomly draw a suffix w ?
c(?
| hcw) and randomly tag wwwith t ?
p?(?
| ww, hq).
This is equivalent to using p?
with thebackward algorithm to conditionally tag each possible suffix.19The first component of ??za?
?za is ?za?za = ?za ?
1.20If a is an arc of cq?
then ?r?/?
?a is the second componentof?z?Za??za(?k?za/??a)??za.
Then ?LA/?
?a works out to?z?Zaca(rza+?za(log ?za?log ?a?1))+?A.
Set to 0 andsolve for ?a, noting that ca, ?a, ?A are constant over a ?
A.21In retrospect, an even faster strategy might be to do a seriesof block ?
and ??
updates, updating ??
at a state (footnote 10) im-mediately after updating ?
on the arcs leading from that state,which allows a better block update at predecessor states.
On anacyclic machine, a single backward pass of this sort will reducethe variational gap to 0 if nq = np (footnote 18).
This is be-cause, thanks to the up-to-date ?
?, each block of arcs gets new ?weights in proportion to relative suffix path probabilities underthe new ?.
After this backward pass, a single forward pass canupdate the ?
values and collect expected counts for the M-stepthat will update ?.
Standard EM is a special case of this strategy.us, are replaced by the faux sentences extrapolatedfrom observed n-grams via the language model c.The states of the HMM correspond to POS tags asin Figure 1.
All transitions are allowed, but not allemissions.
If a word is listed in a provided ?dictio-nary?
with its possible tags, then other tags are given0 probability of emitting that word.
The EM algo-rithm uses the corpus to learn transition and emis-sion probabilities that explain the data under thisconstraint.
The constraint ensures that the learnedstates have something to do with true POS tags.Merialdo (1994) spawned a long line of workon this task.
Ideas have included Bayesian learn-ing methods (MacKay, 1997; Goldwater and Grif-fiths, 2007; Johnson, 2007), better initial parame-ters (Goldberg et al, 2008), and learning how toconstrain the possible parts of speech for a word(Ravi and Knight, 2008), as well as non-HMM se-quence models (Smith and Eisner, 2005; Haghighiand Klein, 2006; Toutanova and Johnson, 2007).Most of this work has used the Penn Treebank(Marcus et al, 1993) as a dataset.
While thismillion-word Wall Street Journal (WSJ) corpus isone of the largest that is manually annotated withparts of speech, unsupervised learning methodscould take advantage of vast amounts of unannotatedtext.
In practice, runtime concerns have sometimesled researchers to use small subsets of the Penn Tree-bank (Goldwater and Griffiths, 2007; Smith and Eis-ner, 2005; Haghighi and Klein, 2006).
Our goal isto point the way to using even larger datasets.The reason for all this past research is that (Meri-aldo, 1994) was a negative result: while EM isguaranteed to improve the model?s likelihood, it de-grades the match between the latent states and trueparts of speech (if the starting point is a good oneobtained with some supervision).
Thus, for the taskof POS induction, there must be something wrongwith the HMM model, the likelihood objective, orthe search procedure.
It is clear that the model is fartoo weak: there are many latent variables in naturallanguage, so the HMM may be picking up on some-thing other than POS tags.
Ultimately, fixing thiswill require richer models with many more param-eters.
But learning these (lexically specific) param-eters will require large training datasets?hence ourpresent methodological exploration on whether it ispossible to scale up the original setting.1384.2 SetupWe investigate how much performance degradeswhen we approximate the corpus and train approx-imately with nq = 1.
We examine two measures:likelihood on a held-out corpus and accuracy in POStagging.
We train on corpora of three different sizes:?WSJ-big (910k words?
441k n-grams @ cutoff 3),?
Giga-20 (20M words?
2.9M n-grams @ cutoff 10),?
Giga-200 (200M wds?
14.4M n-grams @ cutoff 20).These were drawn from the Penn Treebank (sections2?23) and the English Gigaword corpus (Parker etal., 2009).
For held-out evaluation, we use WSJ-small (Penn Treebank section 0) or WSJ-big.We estimate backoff language models for thesecorpora based on collections of n-grams with n ?
5.In this work, we select the n-grams by simple countcutoffs as shown above,22 taking care to keep all 2-grams as mentioned in footnote 2.Similar to Merialdo (1994), we use a tag dictio-nary which limits the possible tags of a word to thoseit was observed with in the WSJ, provided that theword was observed at least 5 times in the WSJ.
Weused the reduced tagset of Smith and Eisner (2005),which collapses the original 45 fine-grained part-of-speech tags into just 17 coarser tags.4.3 ResultsIn all experiments, our method achieves similar ac-curacy though slightly worse likelihood.
Althoughthis method is meant to be a fast approximation ofEM, standard EM is faster on the smallest dataset(WSJ-big).
This is because this corpus is not muchbigger than the 5-gram language model built from it(at our current pruning level), and so the overheadof the more complex n-gram EM method is a netdisadvantage.
However, when moving to larger cor-pora, the iterations of n-gram EM become as fast asstandard EM and then faster.
We expect this trendto continue as one moves to much larger datasets, asthe compression ratio of the pruned language modelrelative to the original corpus will only improve.The Google n-gram corpus is based on 50?
moredata than our largest but could be handled in RAM.22Entropy-based pruning (Stolcke, 2000) may be a better se-lection method when one is in a position to choose.
However,count cutoffs were already used in the creation of the Googlen-gram corpus, and more complex methods of pruning may notbe practical for very large datasets.7274767880828486AccuracyTimeEM (WSJ-big)N-gram EM (WSJ-big)EM (Giga-20)N-gram EM (Giga-20)EM (Giga-200)N-gram EM (Giga-200)LikelihoodTimeEM (WSJ-big)N-gram EM (WSJ-big)EM (Giga-20)N-gram EM (Giga-20)EM (Giga-200)N-gram EM (Giga-200)Figure 2: POS-tagging accuracy and log-likelihood af-ter each iteration, measured on WSJ-big when trainingon the Gigaword datasets, else on WSJ-small.
Runtimeand log-likelihood are scaled differently for each dataset.Replacing EM with our method changes runtime per it-eration from 1.4s?
3.5s, 48s?
47s, and 506s?
321s.5 ConclusionsWe presented a general approach to training genera-tive models on a distribution rather than on a trainingsample.
We gave several motivations for this novelproblem.
We formulated an objective function simi-lar to MAP, and presented a variational lower bound.Algorithmically, we gave nontrivial general meth-ods for computing and optimizing our variationallower bound for arbitrary finite-state data distribu-tions c, generative models p, and variational fami-lies q, provided that p and q are unambiguous same-length FSTs.
We also gave details for specific usefulfamilies for c, p, and q.As proof of principle, we used a traditional HMMPOS tagging task to demonstrate that we can traina model from n-grams almost as accurately as fromfull sentences, and do so faster to the extent that then-gram dataset is smaller.
More generally, we offerour approach as an intriguing new tool to help semi-supervised learning benefit from very large datasets.139ReferencesCyril Allauzen, Mehryar Mohri, and Brian Roark.
2003.Generalized algorithms for constructing statistical lan-guage models.
In Proc.
of ACL, pages 40?47.Shane Bergsma, Dekang Lin, and Randy Goebel.
2009.Web-scale n-gram models for lexical disambiguation.In Proc.
of IJCAI.Thorsten Brants and Alex Franz.
2006.
Web 1T 5-gramversion 1.
Linguistic Data Consortium, Philadelphia.LDC2006T13.Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,and Jeffrey Dean.
2007.
Large language models inmachine translation.
In Proc.
of EMNLP.Tim Dawborn and James R. Curran.
2009.
CCGparsing with one syntactic structure per n-gram.
InAustralasian Language Technology Association Work-shop, pages 71?79.Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-bin.
1977.
Maximum likelihood from incomplete datavia the EM algorithm.
Journal of the Royal StatisticalSociety.
Series B (Methodological), 39(1):1?38.Jason Eisner.
2002.
Parameter estimation for probabilis-tic finite-state transducers.
In Proc.
of ACL, pages 1?8.Yoav Goldberg, Meni Adler, and Michael Elhadad.
2008.EM can find pretty good HMM POS-taggers (whengiven a good start).
In Proc.
of ACL, pages 746?754.Sharon Goldwater and Thomas Griffiths.
2007.
A fullyBayesian approach to unsupervised part-of-speech tag-ging.
In Proc.
of ACL, pages 744?751.Aria Haghighi and Dan Klein.
2006.
Prototype-drivenlearning for sequence models.
In Proc.
of NAACL,pages 320?327.Daniel Hsu, Sham M. Kakade, and Tong Zhang.
2009.
Aspectral algorithm for learning hidden Markov models.In Proc.
of COLT.Mark Johnson.
2007.
Why doesn?t EM find good HMMPOS-taggers?
In Proc.
of EMNLP-CoNLL, pages296?305.M.
I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K.Saul.
1999.
An introduction to variational methodsfor graphical models.
In M. I. Jordan, editor, Learningin Graphical Models.
Kluwer.Mirella Lapata and Frank Keller.
2005.
Web-based mod-els for natural language processing.
ACM Transac-tions on Speech and Language Processing.Zhifei Li and Jason Eisner.
2009.
First- and second-orderexpectation semirings with applications to minimum-risk training on translation forests.
In Proc.
ofEMNLP, pages 40?51.Percy Liang, Hal Daume?
III, and Dan Klein.
2008.Structure compilation: Trading structure for features.In International Conference on Machine Learning(ICML), Helsinki, Finland.D.
Lin, K. Church, H. Ji, S. Sekine, D. Yarowsky,S.
Bergsma, K. Patil, E. Pitler, R. Lathbury, V. Rao,K.
Dalwani, and S. Narsale.
2009.
Unsupervised ac-quisition of lexical knowledge from n-grams.
Sum-mer workshop technical report, Center for Languageand Speech Processing, Johns Hopkins University.David J. C. MacKay.
1997.
Ensemble learning for hid-den Markov models.
http://www.inference.phy.cam.ac.uk/mackay/abstracts/ensemblePaper.html.Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-rice Santorini.
1993.
Building a large annotated cor-pus of English: The Penn Treebank.
ComputationalLinguistics.Andrew McCallum, Dayne Freitag, and FernandoPereira.
2000.
Maximum entropy Markov models forinformation extraction and segmentation.
In Proc.
ofICML, pages 591?598.B.
Merialdo.
1994.
Tagging English text with a proba-bilistic model.
Computational Linguistics, 20(2):155?171.J.-B.
Michel, Y. K. Shen, A. P. Aiden, A. Veres, M. K.Gray, W. Brockman, The Google Books Team, J. P.Pickett, D. Hoiberg, D. Clancy, P. Norvig, J. Orwant,S.
Pinker, M. A. Nowak, and E. L. Aiden.
2010.Quantitative analysis of culture using millions of digi-tized books.
Science, 331(6014):176?182.Mehryar Mohri and Mark-Jan Nederhof.
2001.
Regu-lar approximation of context-free grammars throughtransformation.
In Jean-Claude Junqua and Gert-jan van Noord, editors, Robustness in Language andSpeech Technology, chapter 9, pages 153?163.
KluwerAcademic Publishers, The Netherlands, February.Mehryar Mohri, Fernando Pereira, and Michael Riley.2000.
The design principles of a weighted finite-state transducer library.
Theoretical Computer Sci-ence, 231(1):17?32, January.Radford M. Neal and Geoffrey E. Hinton.
1998.
A viewof the EM algorithm that justifies incremental, sparse,and other variants.
In M.I.
Jordan, editor, Learning inGraphical Models, pages 355?368.
Kluwer.Mark-Jan Nederhof.
2000.
Practical experimentswith regular approximation of context-free languages.Computational Linguistics, 26(1).Robert Parker, David Graff, Junbo Kong, Ke Chen, andKazuaki Maeda.
2009.
English Gigaword fourthedition.
Linguistic Data Consortium, Philadelphia.LDC2009T13.V.
Punyakanok, D. Roth, W. Yih, and D. Zimak.
2005.Learning and inference over constrained output.
InProc.
of IJCAI, pages 1124?1129.Lawrence R. Rabiner.
1989.
A tutorial on hiddenMarkov models and selected applications in speech140recognition.
Proc.
of the IEEE, 77(2):257?286, Febru-ary.Sujith Ravi and Kevin Knight.
2008.
Minimized modelsfor unsupervised part-of-speech tagging.
In Proc.
ofACL, pages 504?512.Noah A. Smith and Jason Eisner.
2005.
Contrastive esti-mation: Training log-linear models on unlabeled data.In Proc.
of ACL, pages 354?362.R.
Staden.
1979.
A strategy of DNA sequencing em-ploying computer programs.
Nucleic Acids Research,6(7):2601?2610, June.Andreas Stolcke.
2000.
Entropy-based pruning of back-off language models.
In DARPA Broadcast NewsTranscription and Understanding Workshop, pages270?274.Kristina Toutanova and Mark Johnson.
2007.
ABayesian LDA-based model for semi-supervised part-of-speech tagging.
In Proc.
of NIPS, volume 20.141
