Automatic Extraction of Semantic Relations from Specialized CorporaAr i s tomenis  Thanopou los ,  N ikos  Fakotak is  and  George  Kokk inak isWire Communications LaboratoryElectrical & Computer Engineering Dept., University of  Patras26500 Rion, Patras, Greecearistom@wcl, ee.
upatras.grAbstractIn this paper we address the problem ofdiscovering word semantic similarities viastatistical processing of text corpora.
Wepropose a knowledge-poor method thatexploits the sentencial context of words forextracting similarity relations between themas well as semantic in nature word clusters.The approach aims at full portability acrossdomains and languages and therefore isbased on minimal resources.1 MotivationProviding digital computers with the capability toacquire conceptual relations between lexical itemsby processing real-life text corpora is not only anexciting research activity but also a significanttask in the framework of many NLP systems.Specifically:1.
State-of-the-art Language Modeling techniques(McMahon and Smith., 1996) require lexicalintbrmation about word classes.2.
Thesauri creation in a (semi-) automatic mannerin any domain and language with minimaldependence on specialized tools and resourcesis very important.
Most thematic domainstoday in most of the languages lack semanticresources.
Adopting a knowledge-poor corpus-based method not only much less labor isnecessary in construction of conceptualstructures but also domain-dependent semanticrelations are obtained.
New resources can bereadily created in new domains or existingthesauri can be enlarged or refined by re-training on larger corpora as soon as theybecome available.3.
Many currently implemented, both spoken andwritten, NLP systems operate in a specificdomain and usually utilize a constrainedvocabulary related directly to their taskdomain.
Therefore semantic domain-dependentknowledge can be acquired directly fromrelevant corpora.4.
Autonomous computational intelligence shouldrely mainly on processing of tree flowelectronic texts for acquiring new semantic andworld knowledge.The present approach aims at corpus-basedautomatic extraction of domain-dependentsemantic similarity relations between lexical itemsand the formation of corresponding semanticclusters.
For this purpose, the usage of readilyavailable domain-specific text corpora isimperative.
The guideline of our approach was theadaptation to the special characteristics of thistype of corpora (specialization, restricted size)without imposing the need for other domain-dependent resources and obtaining portabilityacross languages.2 Related workThree main approaches have been proposed forthe automatic extraction of lexical semanticsknowledge: syntax-based, n-gram-based andwindow-based.
Syntax-based methods (referredalso as knowledge-rich in contrast to the others -knowledge-poor methods) (Pereira and Thishby,1992; Grefenstette, 1993; Li and Abe, 1997)represent the words under consideration asvectorscontaining statistic values of their syntacticproperties in relation to a given set of words (e.g.statistics of object syntax relations referring to aset of verbs) and cluster the considered wordsaccording to similarity of the correspondingvectors.
Methods that use bigrams (Brown et al,1992) or trigrams (Martin et al, 1998) clusterwords considering as a word's context he one ortwo immediately adjacent words and employ asclustering criteria the minimal loss of average836nmtual information and the perplexityimprovement respectively.
Such methods areoriented to language modeling and aim primarilyat rough but fast clustering of large vocabularies.Brown et al (1992) also proposed a windowmethod introducing the concept of "semanticstickiness" of two words as the relatively frequentclose occurrence between them (less than 500words distance).
Although this is an efficient andentirely knowledge-poor method tbr extractingboth semantic relations and clusters, the extractedrelations are not restricted to semantic similaritybut extend on thematic roles.
Moreover itsapplicability to small and specialized corpora isuncertain.3 A knowledge-poor approachIn order to achieve portability we approach theissue from a knowledge-poor perspective.
Syntax-based methods employ partial parsers whichrequire highly language-dependent resources(morphological/grammatical analysis), and/orproperly tagged training corpus in order to detectsyntactic relations between sentence constituents.On the other hand, n-gram methods operate onlarge corpora and, in order to reducecomputational resources, consider as contextwords only the immediately adjacent ones.Medium-distance word context is not exploited.Since large corpora are available only for fewdomains we aimed at developing a method forprocessing small or medium sized corporaexploiting the most of contextual information, thatis, the tifll sentential context of words.
Ourapproach was driven by the observation that indomain-constrained corpora, unlike fiction orgeneral journalese, the vocabulary is limited, thesyntactic structures are not complex and thatmedium-distance lexical patterns are frequentlyused to express imilar facts.Specifically we have developed two differentalgorithms in respect o the context considerationthey employ: Word-based and Pattern-based.
Theformer acquires word-based contextual data(extended up to sentence boundaries), according tothe distributional similarity of which, wordsimilarity relations are extracted.
The latter detectscommon patterns throughout he corpus thatindicate possible word similarities.
For example,consider the sentence fragments:"...while the S&I' index inched up 0.3%.
""The DAX #Mex inched up O.
70 point to close..."Although their syntactic structures are different,the common contextual pattern (appearing beyondimmediately adjacent words) indicates a possiblesimilarity between the tokens 'S&P' and 'DAX'.Word pairs that persistently appear such contextsimilarity throughout the corpus (frequentlyobserved in technical texts) are confidentlyindicated as semantically similar.
Our methodcaptures such context similarity and extracts aproportionate measure about semantic similaritybetween lexical items.Most approaches (Brown et al, 1992; Li & Abe,1997) inherently extract semantic knowledge inthe abstracted form of semantic clusters.
Ourmethod produces emantic similarity relations asan intermediate (and information-richer)semantics representation formalism, from whichcluster hierarchies can be generated.
Of greatimportance is that soft clustering methods can alsobe applied to this set of relations and clusterpolysemous words to more than one classes.Stock lnarket-financial news and Modem Greek,were used as domain and language test caserespectively, ltowever demonstrative xamplestaken from the WSJ corpus have been usedthroughout the paper as well.4 Context Similarity EstimationThe main idea supporting context-based wordclustering is that two words that can substitute oneanother in several different contexts alwaysproviding meaningful word sequences areprobably semantically similar.
Present n-grambased methods utilize this assumption consideringas a context of a focus word only the one or twoimmediately adjacent parameter words.In the present work, we consider as word contextthe whole sentence in which the examined wordappears, excluding only the semantically empty(i.e.
functional) words such as articles,conjunctions, particles, auxiliaries.
Adopting thisword context notion we proceed to tile ibllowinganalysis:Let us consider a text corpus Tc with vocabularyVc and Vs _c Vc the set of words that are of interestin extracting semantic similarity relations betweenthem.
Vx comprises the non-functional words of837Vc appearing in Tc with a frequency higher than athreshold (set to 20 in the presented experiments)in order to acquire sufficient data for every focusword.
Let Vr_~ Vc be the set of words that will beused as context parameters.
Ideally, any wordappearing at least twice in the corpus could beused as context parameter.
However we specifiedthis word frequency threshold to 10 in order todiminish computational time.
Consider a sentenceof Tc:Sm= WI,W2,..-,Wj-I,Wj,Wj+I,...,WkWe define as sentential context o fw~ in S,,, the setof the pairs of the sentence words which aremembers of Vp, accompanied by theircorresponding distance from wj:Cs.,(wi)= {( i - j ,w~), i=l .
.k,( i  ~j),  Vw~ e VI, }Equation (1): Sentential context of wi in S,,,More formally, Cs,.
(wi) can be represented as abinary-valued matrix defined over the set~t =Sxo)  where 8={ -l,l,-2,2,...,-Lm,Lm}, Lmbeing the maximum word distance we regard thatcarries useful contextual information (for fullsentence distance Lm=Lma~-I where L,,,x themaximum sentence length in Tc), and o~ theordered set Vs:Cs,,, (wi) = {ci,,, (d, w)}, where:de&wE~cj,,.
(d,w)=J'l, w=wj ,w i cco, d=i - j  (2)0, otherwiseSumming over all corpus sentences we obtain thecontextual data matrices for every wj c V s :(w/) : {cj (d, w)} = C,,, (w j) (s)de6, we0) mThe word semantic similarity estimation has beenreduced to matrices similarity estimation.
Theobtained contextual matrices are compared using aweighted Tanimoto measure (Chamiak, 1993) anda word similarity measure S,.
(wl,wj) is obtained:ZE\ [h (d) "  ci(d,w)" cj(d,w)ld wSm(wi,wj) = ZEmax{c i  (d,w), cj(d,w)} (4)d wThe weight function h(d) defines the desiredinfluence that the distance between words shouldhave to context similarity estimation.
In thisexperiment we set: h(d)=l/ld I.
In order to reducecomputational time the denominator was set toZEc i (d ,w)+ZZc j (d ,w) ,  a modificationd w d wthat has minimal effect on the final result.Experimental results of this method (Word-basedContext Similarity Es t imat ion -  WCSE), areshown in Table 1.
Note that, since the Cfc,(wj)matrix is sparse, (l) was used as data storingformula instead of (2), in order to diminishcomputational cost.The previously described algorithm is handling allcontextual data in a uniform way.
However, studyof the results showed that preference should begiven to hits derived from many different similarcontexts instead of few ones appearing manytilnes.
This would clearly give better esults sincethe latter case may be due to often-usedstereotyped expressions or repeated ~3cts.
In orderto achieve this we modified (4) to:ZZ\ [h (d) "  log2 \[ci (d,w) ?
cj (d,w)~d wd w d wIndeed the experimental results of this variation(Variant WCSE - VWCSE) show a significantimprovement (see Table 1 ).5 Dynamic pattern detection forcontext similarity estimationIn the previously described method the notion ofword context is based on independent intra-sentential word co-occurrences.
Itoweversimilarity of contextual patterns is much morereliable word similarity criterion than word-basedcontext similarity.
That is, if the sententialcontexts Cs,,,(w~) and Cs,(w/) have at least twocommon elements, we count this as a much moreconfident hit regarding the wi and w.i similarity.
Ameasure expressing the weight of the commonpattern is obtained.
Since the patterns underdetection vary across languages and domains weneed a method that extracts them dynamically,regardless of the text genre, domain or language.For this purpose we propose an algorithm thatperforms a sentence-by-sentence comparisonalong the corpus.
This comparison is based on the838cross-correlation concept as it is used in digitalsignal processing.
A sentence can be considered asa digital signal where every semantic tokencorresponds to a signal sample.
In order to detectwords with common contexts every sentence ischecked on matching every other one partially (i.e.matching the semantic ategory of one or moretokens) on every possible relative positionbetween the two sentences.
Wherever colnmonpatterns of semantic tokens are lbund theneighboring respective tokens on the twosentences are stored as candidate semanticrelatives.During this process contextual data are notmaintained in memory; instead the detection of acommon pattern in both sentences results to thestorage of several hits (i.e.
candidate silnilar wordpairs) or to the increase of their correspondingsimilarity measure according to the patternsimilarity ot'their contexts.Let Sm and S, be two sentences that undergo thecross-correlation procedure.
If 8~={dx, x=l..xl,xl>l }, is the set of word distances that satisfy theequality: ci,,, , (d, ,  Wy) = c./.,, (d,-, Wy ) = 1, then thepair (wi,wi) is stored as a hit accompanied by thel'ollowing context similarity measure:.v 1 .v 1Keeping only the first term we obtain the sameresult as in the WCSE method with weightfunction h(d) =l/\]d\[.
The second term augments thescore in proportion to the cohesion and the size o1'the: detected pattern depending on the position ofwi (or, equivalently, wi).
I)ividing (6) by the totallength of S,,, and S. (i.e.
1~.,,, =L,c?I.,, ) we obtain anormalized measure of the cross-correlation f thetwo sentences:1,,,,,, (lv,, w.i )F ",,,,,, (u,,, ~,j) : (7)LI I I l lThe total similarity measure is obtained from:(8)m n~napplied throughout the corpus.In order to reduce search thne and requiredmemory during the whole process a pruningmechanism is applied at regular time intervals toeliminate word pairs with a relatively very lowsemantic similarity score.Dividing (8) by the product of the wordprobabilities P(wi)-P(wi) we obtain the normalizedsimilarity measure FN(Wl,Wj).in order to constrain the degradation of our resultsdue to sparse data regarding less frequent words,we multiply (8) by Pc, a data sufficiency measurefunction of P(wi) and P(wi) , obtaining Fu, a morereliable measure, ltere we employed:.
r ,  J' <1' c(P, 1')= -1~ ' 1~.
./ (9)C 1 , otherwisewhere we used PTI,=30/ITcl, ITcl being the size ofthe corpus.Finally, sorting the resulting pairs by Fu andkeeping the N-best scoring pairs, we obtain thepreponderant semantically related candidates.6 PreprocessingIn order to apply the above described algorithmssome preprocessing is necessalT:1.
A trainable sentence splitter and a rule-basedchunker are applied.
Sentence boundaries confinethe scope of context while phrase boundariesdetermine the nmximunl extent of semantic tokens(see below).2.
The next step of the preprocessing is what wecall "xemantic lokenizalion".
We try to reducecontext parameters and simultaneously toincerease the volulne of contextual data either byreducing the volume of both the focus andparameter word set or by discaring or merginglexical items resulting in reduction of the distancebetween semantic tokens.
Words or wordsequences are thus classified in common semanticcategories employing syntactical, morphologicaland coltocational intbrmation:a.Functionals (auxiliaries, determiners) arediscarded since they do not modit) semanticallytheir head words.
Words of indeterminablesemantic content (pronouns, low fi'equencywords) are treated as empty tokens.b.Known domain-independent lexical patternsincorporating arithmetic and temporal839expressions (e.g.
dates, numbers, amounts, etc.
)are regarded as a single semantic token andtagged accordingly.
Their information content isindifferent o semantic kmowledge acquisition;therefore we preserve only class information.c.Frequently appearing lexical patterns whichrepresent single semantic entities in the specificdomain are treated as a single (albeit composite)"semantic token".
Their detection is based onthe following algorithm (cf.
Smadja, 1993):1.
Extract "significant bigrams" confined insidenoun phrases i.e.
immediately adjacent wordsthat contain a relatively high amount ofnmtual information:I ......... ; (w. ,w2)= log 2 P(w 'w2) (10)P(w t )P(w 2 )2.
Combine significant bigrams together toobtain "significant n-grams" found in thecorpus and confined inside noun phrases aswell.
Discard subsumed m-grams (re<n) onlyif they do not occur indepentently in thecorpus.3.
Tag throughout the corpus the significant n-grams as single semantic tokens, startingfrom the higher-order ones.Semantic entities that are lexically representedas sticky word chains may be either standard -in the framework of the information extractiontask - named entities, such as "Latin America"(location), "Russian president Boris Yeltsin"(person), "Tpdne~a Mct~c6ovk%-Qpdmlg"("Bank of Macedonia and Thrace";organization) or representations of donmin-specific typical events ("Gt6~qm 1 tm'coztKo6Keq)c0~cdon" = rise of equity capital), abstractconcepts ("Dow Jones industrials"), etc.
Toensure that the detected "sticky" phrases actuallyrepresent semantic entities, human inspection isnecessary for discarding the spurious ones, sincerepeated word sequences that do not constitutealways single semantic entities often appear inspecialized texts.From the above it is apparent that we use the term"semantic token" to refer to a recognized semanticpattern (e.g.
<date>), a rigid word chain (e.g.
"Dow Jones industrials') or a single contentword.
The context similarity estimation algorithmswere run using vocabularies of focus andparameter words derived from the extracted set ofsemantic tokens.7 Incorporating heuristicsFrom the study of the erroneously extractedsemantic relations certain systematic errors weredetected.
For example, adjectives, adverbs oradjunctive nouns that occur interpolating inotherwise similar contexts lead to the extraction ofspurious pairs.
Consider ~br example the phrases:"11 c~6~qml "nlg ztgl\]g "Cllg J~v~ixrll~" (= the increaseof the benzine price) and "11 c~6~qcn 1 zqg "cqa\]g~d0~31mlg zqg \[~cv~iwl?'
(=the increase of thedi,sposal benzine price).
Every algorithm based onword adjacency data outputs as erroneous hits thepairs {benzine-disposal} nd { increase-disposal}.A rule that was applied to deal with this problemis:If wiGS m and wjGSn have s imi larcontexts,  count the pair (wi,w j) asa hit on ly  if wi~wj+3 and wj~wi+1.Such contextual rules can be applied only usingthe cross-correlation method for the contextsimilarity estimation (either pattern-based orword-based).8 Word ClusteringAlthough the obtained semantically related N-bestpair list constitutes already a thesaurus-like andinformation-rich form of semantic knowledgerepresentation, many NLP applications (e.g.language modeling) require word clusters insteadof word relations.
However, since a wordsimilarity measure has been extracted, theformation of clusters is a rather trivial problem,although more complex for "soft clustering" (i.e.
aword can be classified in more than one classes).In order to construct word classes we applied theunsupervised agglomerative hard clusteringalgorithn3 shown in Figure 1 over the set ofsenmntic relations.
Each distinct lexical item isinitially assigned to a cluster and then clusters aremerged into larger ones according to the averagelinkage measure.
Merging of clusters tops whenthe distance between the more proximate clustersexceeds a threshold proportional to the averagedistance between words.
Tracking the successivemerges we obtain sub-cluster hierarchies, uch asthe one shown in l<igure 2.840Repeat until mJn(AvgDis tance  (C / ,C  I ) ) > k .
.
.
.for every  c lus ter  C:~for  every  c lus ter  Ca@CTca lcu la te  AvgDJs tance(C ; ,C \ [  ) : : -merge C',, Arg Inin (AvgDis tall c'e(C'.i,(\]l))C,/1 Z distance(w i, w j)Ivsl 2 v,,,,.,,,,~,,1 .
Z distance(w ,wj)Figure 1 : Unsupervised Itard Clustering Algorithm9 Experimental ResultsThe reported experinaents have been carried out ona 220.000 words corpus, comprised o1' financialnews of 1998, which was constructed in theframework of a currently carried out R&D projectfor Information Extraction from raw text ~.The methods and their variations described insections 4 and 5 for obtaining lexical senmnticrelations were tested and their accuracy pernurnber of best hits was measured by hunmninspection.
The VWCSE method was tested usingonly the previous and next word as contextparameters (N&P method), to sketch a methodbaseline lbr the particular corpus.
Using aMorphological Analyzer & Part-of Speech taggerto restrict semantic relations only between wordsof the same Part-of-Speech (PoS) we obtainapparently higher accuracy, though we loose someinteresting verb - noun pairs referring to the sameaction or condition, e.g.
(mOiOqKc (=increased)and dvogo.q (=increment).
The results indicate thatthe norlnalization factors indeed iinprove theaccuracy of the methods and that contextsimilarity detection based on dynamic pattern-matching yields significantly more reliable resultsthan the word-based method.
This demonstratesthe importance of the cross-correlation algorithm,which is the only suitable tbr pattern-basedcontext similarity detection.Regarding the clustering procedure, a set of 1300words was clustered to 84 hierarchicallystructured clusters.
Considering an interestedcluster formed (Figure 2) we note that from the 18lexical entities (words or rigid phrases) thatconstitute the cluster all but two refer to money1 Project "M1TOS" of the Greek General Sec|'etariat lbrReseach and Technologyinvestment o1' profit.
Froln the vocabulary subjectto clustering 4 words belonging to the same classwere not detected; therefore accuracy and recallfor the specific cluster were found at 88.9% and80% respectively.Although comparision with other knowledge-poormethods would be very useful it was not realized,nminly because our method produces semanticrelations while other methods produce semanticclusters and our clustering process is not yetelaborated enough to yield quality results.Lexical ltem English TransLKox'~o)d0)v outlays/gcapilals_ w~ot6tl m l\] ui d i t ) , /~___cxcx'66ocl~ investmentscn~:x'~ocn 1 investment*xpoTpd\[qtct'ro ~ program/g~kLmv &ltmo\[oo state stocks/goltoLd,/(0v income bonds/gc.wdwo)v_yptqt\[tat\[ow time notes/g~q\[tt?~ lossesK{;p& 1 profitsKfvmO~:oclc; deposils~q~u(ov losses/ga(0Lqcmt~ purchasesco680w incomes/g~c~o&t incomesm)vcO, Lfty?~ dealings*o6\[t~ctoq contractFigure 2: A derived sample hierarchial cluster oflexieal entities ('/g' denotes genitive case)841METHODN&PWCSEVWCSECCPMCCPM-NCCPM-N-FCCPM-N-F-PcPoS &VWCSECCPMCCPM-NCCPM-N-FCCPM-N-F-PcPrecision (%) per number ofbest hits100 1200 1300 140064 61 57.7 54.7572 65.5 61.7 5781 7O 66 62.574 59 54 50.589 81.5 70.3 6390 80.5 72.7 67.2593 82 71.3 66.7586 80 75 67.586 77.5 68 59.593 88 79 7495 88.5 83 7797 89 82.7 77N&P: Context = next and previous wordWCSE: every word into the sentence istaken as context parameter venly - Eq.
(4)VWCSE: contextual similarity varianceis favored - Eq.
(5)CCPM: Dynamic Pattern-Matchingbased on Cross-Correlation - Eq.
(6)&(8)CCPM-N: normalized by L ..... (7)&(8)CCPM-N-F: normalized by P(wi)' P(wj)CCPM-N-F-Pc: normalized by Pc, Eq.
(9)Table I: Comparative Results and Explanation Memo10 ConclusionInitiating from the conception of word similarityestimation in terms of context similarity we haveproposed an approach with several variations forextracting semantic similarity relationsbetweenlexical entities by processing wordadjacency data obtained from small or mediumsized corpora.
The described cross-correlationprocedure, offers the possibility to dynamicallydetect pattern context similarities offering strongevidence for semantic similarity.
The presentedalgorithm featureslanguage and domainportability and the ability to classify keywordsirrespective of their grammatical characteristics.The implementation of the soft clusteringalgorithm, the test of the method to a differentdomain and language and the quantifiedcomparison with other kamwledge-poor methodsare quite interesting matters belonging to futurework.ReferencesBrown P.F., DellaPietra V.J., DeSouza P.V., LaiJ.C., Mercer R.L.
: Class-Based n-gram Modelsof Natural Language.
ComputationalLinguistics, 18(4): pp.
467-479, 1992.Chamiak E.: Statistical Language Learning.
TheM1T Press, 1993.Grefenstette, G.: SEXTANT."
ExtractingSemantics fromRaw Text: ImplementationDetails.
The Journal of Knowledge Engineering,1993.Li H., Abe N.: Clustering Words with the MDLPrinciple.
Journal of Natural LanguageProcessing v.4, n.2, 1997.Martin S., Liermann J., Ney i1.
: Algorithms forbigram and trigram word clustering.
SpeechCommunication 24, pp.
19-37, 1998.McMahon J.G., Smith F.J.: Improving StatisticalLanguage Model Pelformance withAutomatically Generated Word Ilierarchies.Computational Linguistics, 22(2) 1996.Pereira F., Tishby N.: Distributional SimilariO~,Phrase Transitions and Itierarchical Clustering.In Working Notes, Fall Symposium Series.AAAI pp.108-112, 1992.Smadja F.: Retrieving Collocations from text:Xtract.
Computational Linguistics, 19(1): pp.143-177, 1993.842
