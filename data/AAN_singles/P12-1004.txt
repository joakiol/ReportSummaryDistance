Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 31?39,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsProbabilistic Integration of Partial Lexical Information for Noise RobustHaptic Voice RecognitionKhe Chai SimDepartment of Computer ScienceNational University of Singapore13 Computing Drive, Singapore 117417simkc@comp.nus.edu.sgAbstractThis paper presents a probabilistic frameworkthat combines multiple knowledge sources forHaptic Voice Recognition (HVR), a multi-modal input method designed to provide ef-ficient text entry on modern mobile devices.HVR extends the conventional voice input byallowing users to provide complementary par-tial lexical information via touch input to im-prove the efficiency and accuracy of voicerecognition.
This paper investigates the use ofthe initial letter of the words in the utteranceas the partial lexical information.
In additionto the acoustic and language models used inautomatic speech recognition systems, HVRuses the haptic and partial lexical models asadditional knowledge sources to reduce therecognition search space and suppress confu-sions.
Experimental results show that both theword error rate and runtime factor can be re-duced by a factor of two using HVR.1 IntroductionNowadays, modern portable devices, such as thesmartphones and tablets, are equipped with micro-phone and touchscreen display.
With these devicesbecoming increasingly popular, there is an urgentneed for an efficient and reliable text entry methodon these small devices.
Currently, text entry us-ing an onscreen virtual keyboard is the most widelyadopted input method on these modern mobile de-vices.
Unfortunately, typing with a small virtualkeyboard can sometimes be cumbersome and frus-tratingly slow for many people.
Instead of usinga virtual keyboard, it is also possible to use hand-writing gestures to input text.
Handwriting inputoffers a more convenient input method for writingsystems with complex orthography, including manyAsian languages such as Chinese, Japanese and Ko-rean.
However, handwriting input is not necessarilymore efficient compared to keyboard input for En-glish.
Moreover, handwriting recognition is suscep-tible to recognition errors, too.Voice input offers a hands-free solution for textentry.
This is an attractive alternative for text entrybecause it completely eliminates the need for typ-ing.
Voice input is also more natural and faster forhuman to convey messages.
Normally, the averagehuman speaking rate is approximately 100 wordsper minute (WPM).
Clarkson et al (2005) showedthat the typing speed for regular users reaches only86.79 ?
98.31 using a full-size keyboard and 58.61?
61.44 WPM using a mini-QWERTY keyboard.Evidently, speech input is the preferred text entrymethod, provided that speech signals can be reli-ably and efficiently converted into texts.
Unfortu-nately, voice input relies on automatic speech recog-nition (ASR) (Rabiner, 1989) technology, which re-quires high computational resources and is suscep-tible to performance degradation due to acoustic in-terference, such as the presence of noise.In order to improve the reliability and efficiencyof ASR, Haptic Voice Recognition (HVR) was pro-posed by Sim (2010) as a novel multimodal inputmethod combining both speech and touch inputs.Touch inputs are used to generate haptic events,which correspond to the initial letters of the words inthe spoken utterance.
In addition to the regular beam31pruning used in traditional ASR (Ortmanns et al,1997), search paths which are inconsistent with thehaptic events are also pruned away to achieve furtherreduction in the recognition search space.
As a re-sult, the runtime of HVR is generally more efficientthan ASR.
Furthermore, haptic events are not sus-ceptible to acoustic distortion, making HVR morerobust to noise.This paper proposes a probabilistic frameworkthat encompasses multiple knowledge sources forcombining the speech and touch inputs.
This frame-work allows coherent probabilistic models of dif-ferent knowledge sources to be tightly integrated.In addition to the acoustic model and languagemodel used in ASR, haptic model and partial lexi-cal model are also introduced to facilitate the inte-gration of more sophisticated haptic events, such asthe keystrokes, into HVR.The remaining of this paper is organised as fol-lows.
Section 2 gives an overview of existing tech-niques in the literature that aim at improving noiserobustness for automatic speech recognition.
Sec-tion 3 gives a brief introduction to HVR.
Section 4proposes a probabilistic framework for HVR thatunifies multiple knowledge sources as an integratedprobabilistic generative model.
Next, Section 5describes how multiple knowledge sources can beintegrated using Weighted Finite State Transducer(WFST) operations.
Experimental results are pre-sented in Section 6.
Finally, conclusions are givenin Section 7.2 Noise Robust ASRAs previously mentioned, the process of convertingspeech into text using ASR is error-prone, wheresignificant performance degradation is often due tothe presence of noise or other acoustic interference.Therefore, it is crucial to improve the robustnessof voice input in noisy environment.
There aremany techniques reported in the literature whichaim at improving the robustness of ASR in noisyenvironment.
These techniques can be largely di-vided into two groups: 1) using speech enhance-ment techniques to increase the signal-to-noise ratioof the noisy speech (Ortega-Garcia and Gonzalez-Rodriguez, 1996); and 2) using model-based com-pensation schemes to adapt the acoustic models tonoisy environment (Gales and Young, 1996; Aceroet al, 2000).From the information-theoretic point of view, inorder to achieve reliable information transmission,redundancies are introduced so that information lostdue to channel distortion or noise corruption can berecovered.
Similar concept can also be applied toimprove the robustness of voice input in noisy en-vironment.
Additional complementary informationcan be provided using other input modalities to pro-vide cues (redundancies) to boost the recognitionperformance.
The next section will introduce a mul-timodal interface that combines speech and touch in-puts to improve the efficiency and noise robustnessfor text entry using a technique known as HapticVoice Recognition (Sim, 2010).3 Haptic Voice Recognition (HVR)For many voice-enabled applications, users oftenfind voice input to be a black box that captures theusers?
voice and automatically converts it into textsusing ASR.
It does not provide much flexibility forhuman intervention through other modalities in caseof errors.
Certain applications may return multiplehypotheses, from which users can choose the mostappropriate output.
Any remaining errors are typi-cally corrected manually.
However, it may be moreuseful to give users more control during the inputstage, instead of having a post-processing step forerror correction.
This motivates the investigation ofmultimodal interface that tightly integrates speechinput with other modalities.Haptic Voice Recognition (HVR) is a multimodalinterface designed to offer users the opportunity toadd his or her ?magic touch?
in order to improvethe accuracy, efficiency and robustness of voice in-put.
HVR is designed for modern mobile devicesequipped with an embedded microphone to capturespeech signals and a touchscreen display to receivetouch events.
The HVR interface aims to combineboth speech and touch modalities to enhance speechrecognition.
When using an HVR interface, userswill input text verbally, at the same time provide ad-ditional cues in the form of Partial Lexical Infor-mation (PLI) to guide the recognition search.
PLIsare simplified lexical representation of words thatshould be easy to enter whilst speaking (e.g.
the32prefix and/or suffix letters).
Preliminary simulatedexperiments conducted by Sim (2010) show thatpotential performance improvements both in termsof recognition speed and noise robustness can beachieved using the initial letters as PLIs.
For ex-ample, to enter the text ?Henry will be in Bostonnext Friday?, the user will speak the sentence andenter the following letter sequence: ?H?, ?W?, ?B?,?I?, ?B?, ?N?
and ?F?.
These additional letter sequenceis simple enough to be entered whilst speaking; andyet they provide crucial information that can sig-nificantly improve the efficiency and robustness ofspeech recognition.
For instance, the number of let-ters entered can be used to constrain the number ofwords in the recognition output, thereby suppress-ing spurious insertion and deletion errors, which arecommonly observed in noisy environment.
Further-more, the identity of the letters themselves can beused to guide the search process so that partial wordsequences in the search graph that do not conform tothe PLIs provided by the users can be pruned away.PLI provides additional complementary informa-tion that can be used to eliminate confusions causedby poor speech signal.
In conventional ASR, acous-tically similar word sequences are typically resolvedimplicitly using a language model where contextsof neighboring words are used for disambiguation.On the other hand, PLI can also be very effectivein disambiguating homophones1 and similar sound-ing words and phrases that have distinct initial let-ters.
For example, ?hour?
versus ?our?, ?vary?
versus?marry?
and ?great wine?
versus ?grey twine?.This paper considers two methods of generatingthe initial letter sequence using a touchscreen.
Thefirst method requires the user to tap on the appropri-ate keys on an onscreen virtual keyboard to generatethe desired letter sequence.
This method is similarto that proposed in Sim (2010).
However, typing onsmall devices like smartphones may require a greatdeal of concentration and precision from the users.Alternatively, the initial letters can be entered usinghandwriting gestures.
A gesture recognizer can beused to determine the letters entered by the users.
Inorder to achieve high recognition accuracy, each let-ter is represented by a single-stroke gesture, so thatisolated letter recognition can be performed.
Fig-1Words with the same pronunciationure 1 shows the single-stroke gestures that are usedin this work.4 A Probabilistic Formulation for HVRLet O = {o1,o2, .
.
.
,oT } denote a sequence ofT observed acoustic features such as MFCC (Davisand Mermelstein, 1980) or PLP (Hermansky, 1990)and H = {h1,h2, .
.
.
,hN} denote a sequence ofN haptic features.
For the case of keyboard input,each hi is a discrete symbol representing one of the26 letters.
On the other hand, for handwriting input,each hi represents a sequence of 2-dimensional vec-tors that corresponds to the coordinates of the pointsof the keystroke.
Therefore, the haptic voice recog-nition problem can be defined as finding the jointoptimal solution for both the word sequence, W?
andthe PLI sequence, L?, given O and H. This can beexpressed using the following formulation:(W?, L?)
= arg maxW,LP (W,L|O,H) (1)where according to the Bayes?
theorem:P (W,L|O,H) =p(O,H|W,L)P (W,L)p(O,H)=p(O|W)p(H|L)P (W,L)p(O,H)(2)The joint prior probability of the observed inputs,p(O,H), can be discarded during the maximisationof Eq.
1 since it is independent ofW andL.
p(O|W)is the acoustic likelihood of the word sequence,W ,generating the acoustic feature sequence, O. Simi-larly, P (H|L) is the haptic likelihood of the lexicalsequence, L, generating the observed haptic inputs,H.
The joint prior probability, P (W,L), can be de-composed into:P (W,L) = P (L|W)P (W) (3)where P (W) can be modelled by the word-based n-gram language model (Chen and Goodman, 1996)commonly used in automatic speech recognition.Combining Eq.
2 and Eq.
3 yields:P (W,L|O,H) ?p(O|W)?
p(H|L)?
P (L|W)?
P (W)(4)It is evident from the above equation that the prob-abilistic formulation of HVR combines four knowl-edge sources:33Figure 1: Examples of single-stroke handwriting gestures for the 26 English letters?
Acoustic model score: p(O|W)?
Haptic model score: p(H|L)?
PLI model score: P (L|W)?
Language model score: P (W)Note that the acoustic model and language modelscores are already used in the conventional ASR.The probabilistic formulation of HVR incorporatedtwo additional probabilities: haptic model score,p(H|L) and PLI model score, P (L|W).
The roleof the haptic model and PLI model will be describedin the following sub-sections.4.1 Haptic ModelSimilar to having an acoustic model as a statisti-cal representation of the phoneme sequence generat-ing the observed acoustic features, a haptic model isused to model the PLI sequence generating the ob-served haptic inputs, H. The haptic likelihood canbe factorised asp(H|L) =N?i=1p(hi|li) (5)where L = {li : 1 ?
i ?
N}.
li is the ith PLIin L and hi is the ith haptic input feature.
In thiswork, each PLI represent the initial letter of a word.Therefore, li represents one of the 26 letters.
As pre-viously mentioned, for keyboard input, hi are dis-crete features whose values are also one of the 26letters.
Therefore, p(hi|li) forms a 26?26 matrix.
Asimple model can be derived by making p(hi|li) anidentity matrix.
Therefore, p(hi|li) = 1 if hi = li;otherwise, p(hi|li) = 0.
However, it is also possi-ble to have a non-diagonal matrix for p(hi|li) in or-der to accommodate typing errors, so that non-zeroprobabilities are assigned to cases where hi 6= li.For handwriting input, hi denote a sequence of 2-dimensional feature vectors, which can be modelledusing Hidden Markov Models (HMMs) (Rabiner,1989).
Therefore, (hi|li) is simply given by theHMM likelihood.
In this work, each of the 26 let-ters is represented by a left-to-right HMM with 3emitting states.4.2 Partial Lexical Information (PLI) ModelFinally, a PLI model is used to impose the com-patibility constraint between the PLI sequence, L,and the word sequence, W .
Let W = {wi : 1 ?i ?
M} denote a word sequence of length M .
IfM = N , the PLI model likelihood, P (L|W), canbe expressed in the following form:P (L|W) =N?i=1P (li|wi) (6)where P (li|wi) is the likelihood of the ith word, wi,generating the ith PLI, li.
Since each word is rep-resented by a unique PLI (the initial letter) in thiswork, the PLI model score is given byP (li|wi) = Csub ={1 if li = initial letter of wi0 otherwiseOn the other hand, if N 6= M , insertions and dele-tions have to be taken into consideration:P (li = |wi) = Cdel and P (li|wi = ) = Cinswhere  represents an empty token.
Cdel and Cinsdenote the deletion and insertion penalties respec-tively.
This work assumes Cdel = Cins = 0.This means that the word count of the HVR out-put matches the length of the initial letter sequenceentered by the user.
Assigning a non-zero value toCdel gives the users option to skip entering lettersfor certain words (e.g.
short words).34Figure 2: WSFT representation of PLI model, P?5 Integration of Knowledge SourcesAs previously mentioned, the HVR recognition pro-cess involves maximising the posterior probabilityin Eq.
4, which can be expressed in terms of fourknowledge sources.
It turns out that these knowl-edge sources can be represented as Weighted FiniteState Transducers (WFSTs) (Mohri et al, 2002) andthe composition operation (?)
can be used to inte-grate these knowledge sources into a single WFST:F?integrated = A?
?
L?
?
P?
?
H?
(7)where A?, L?, P?
and H?
denote the WFST repre-sentation of the acoustic model, language model,PLI model and haptic model respectively.
Mohriet al (2002) has shown that Hidden Markov Mod-els (HMMs) and n-gram language models can beviewed as WFSTs.
Furthermore, HMM-based hap-tic models are also used in this work to representthe single-stroke letters shown in Fig.
1.
Therefore,A?, L?, and H?
can be obtained from the respectiveprobabilistic models.
Finally, the PLI model de-scribed in Section 4.2 can also be represented usingthe WFST as shown in Fig.
2.
The transition weightsof these WFSTs are given by the negative log prob-ability of the respective models.
P?
can be viewedas a merger that defines the possible alignments be-tween the speech and haptic inputs.
Each completepath in F?
represents a valid pair of W and L suchthat the weight of the path is given by the negativelogP (L,W|O,H).
Therefore, finding the shortestpath in F?
is equivalent to solving Eq.
1.Direct decoding from the overall composedWFST, F?integrated, is referred to as integrated de-coding.
Alternatively, HVR can also operate in a lat-tice rescoring manner.
Speech input and haptic in-put are processed separately by the ASR system andthe haptic model respectively.
The ASR system mayFigure 3: Screenshot depicting the HVR prototype oper-ating with keyboard inputFigure 4: Screenshot depicting the HVR prototype oper-ating with keystroke inputgenerate multiple hypotheses of word sequences inthe form of a lattice.
Similarly, the haptic model mayalso generate a lattice containing the most probablyletter sequences.
Let L?
and H?
represent the wordand letter lattices respectively.
Then, the final HVRoutput can be obtained by searching for the shortestpath of the following merged WFST:F?rescore = L?
?
P?
?
H?
(8)Note that the above composition may yield an emptyWFST.
This may happen if the lattices generated bythe ASR system or the haptic model are not largeenough to produce any valid pair ofW and L.6 Experimental ResultsIn this section, experimental results are reportedbased on the data collected using a prototype HVRinterface implemented on an iPad.
This prototypeHVR interface allows both speech and haptic inputdata to be captured either synchronously or asyn-chronously and the partial lexical information canbe entered using either a soft keyboard or handwrit-ing gestures.
Figures 3 and 4 shows the screen-shot of the HVR prototype iPad app using the key-35Donna was in Cincinnati last Thursday.Adam will be visiting Charlotte tomorrowJanice will be in Chattanooga next month.Christine will be visiting Corpus Christinext Tuesday.Table 1: Example sentences used for data collection.board and keystroke inputs respectively.
Therefore,there are altogether four input configurations.
Foreach configuration, 250 sentences were collectedfrom a non-native fluent English speaker.
200 sen-tences were used as test data while the remaining 50sentences were used for acoustic model adaptation.These sentences contain a variety of given names,surnames and city names so that confusions can-not be easily resolved using a language model.
Ex-ample sentences used for data collection are shownin Table 1.
In order to investigate the robustnessof HVR in noisy environment, the collected speechdata were also artificially corrupted with additivebabble noise from the NOISEX database (Varga andSteeneken, 1993) to synthesise noisy speech signal-to-noise (SNR) levels of 20 and 10 decibels2.The ASR system used in all the experiments re-ported in this paper consists of a set of HMM-basedtriphone acoustic models and an n-gram languagemodel.
The HMM models were trained using 39-dimensional MFCC features.
Each HMM has aleft-to-right topology and three emitting states.
Theemission probability for each state is represented bya single Gaussian component 3.
A bigram languagemodel with a vocabulary size of 200 words was usedfor testing.
The acoustic models were also noise-compensated using VTS (Acero et al, 2000) in orderachieve a better baseline performance.6.1 Comparison of Input SpeedTable 2 shows the speech, letter and total inputspeed using different input configurations.
For syn-chronous HVR, the total input speed is the sameas the speech and letter input speed since both thespeech and haptic inputs are provided concurrently.According to this study, synchronous keyboard in-put speed is 86 words per minutes (WPM).
This is2Higher SNR indicates a better speech quality3A single Gaussian component system was used as a com-promise between speed and accuracy for mobile apps.Haptic HVR Input Speed (WPM)Input Mode Speech Letter TotalKeyboardSync 86 86 86ASync 100 105 51KeystrokeSync 78 78 78ASync 97 83 45Table 2: Comparison of the speech and letter inputspeed, measured in Words-Per-Minute (WPM), for dif-ferent HVR input configurationsslightly faster than keystroke input using handwrit-ing gestures, where the input speed is 78 WPM.
Thisis not surprising since key taps are much quickerto generate compared to handwriting gestures.
Onthe other hand, the individual speech and letter in-put speed are faster for asynchronous mode be-cause users do not need to multi-task.
However,since the speech and haptic inputs are provided con-currently, the resulting total input speed for asyn-chronous HVR is much slower compared to syn-chronous HVR.
Therefore, synchronous HVR is po-tentially more efficient than asynchronous HVR.6.2 Performance of ASRHVR Mode SNR WER (%) LER (%)ASyncClean 22.2 17.020 dB 30.2 24.210 dB 33.3 28.5SyncClean 25.9 20.2(Keyboard)20 dB 34.6 28.810 dB 35.5 29.9SyncClean 29.0 22.5(Keystroke)20 dB 40.1 32.010 dB 37.9 31.3Table 3: WER and LER performance of ASR in differentnoise conditionsFirst of all, the Word Error Rate (WER) andLetter Error Rate (LER) performances for standardASR systems in different noise conditions are sum-marized in Table 3.
These are results using pureASR, without adding the haptic inputs.
Speechrecorded using asynchronous HVR is considerednormal speech.
The ASR system achieved 22.2%,30.2% and 33.3% WER in clean, 20dB and 10dB36conditions respectively.
Note that the acoustic mod-els have been compensated using VTS (Acero et al,2000) for noisy conditions.
Table 3 also shows thesystem performance considering on the initial let-ter sequence of the recognition output.
This indi-cates the potential improvements that can be ob-tained with the additional first letter information.Note that the pure ASR system output contains sub-stantial initial letter errors.For synchronous HVR, the recorded speech is ex-pected to exhibit different characteristics since itmay be influenced by concurrent haptic input.
Ta-ble 3 shows that there are performance degradations,both in terms of WER and LER, when performingASR on these speech utterances.
Also, the degra-dations caused by simultaneous keystroke input aregreater.
The degradation may be caused by phenom-ena such as the presence of filled pauses and thelengthening of phoneme duration.
Other forms ofdisfluencies may have also been introduced to therealized speech utterances.
Nevertheless, the addi-tional information provided by the PLIs will out-weigh these degradations.6.3 Performance of Synchronous HVRHapticSNR WER (%) LER (%)InputKeyboardClean 11.8 1.120 dB 12.7 1.010 dB 15.0 1.0KeystrokeClean 11.4 0.320 dB 13.1 0.910 dB 14.0 1.0Table 4: WER and LER performance of synchronousHVR in different noise conditionsThe performance of synchronous HVR is shownin Table 4.
Compared to the results shown in Ta-ble 3, the WER performance of synchronous HVRimproved by approximately a factor of two.
Fur-thermore, the LER performance improved signifi-cantly.
For keyboard input, the LER reduced toabout 1.0% for all noise conditions.
Note that thetradeoffs between the WER and LER performancecan be adjusted by applying appropriately weights todifferent knowledge sources during integration.
Forkeystroke input, top five letter candidates returnedby the handwriting recognizer were used.
Therefore,in clean condition, the acoustic models are able torecover some of the errors introduced by the hand-writing recognizer, bringing the LER down to as lowas 0.3%.
However, in noisy conditions, the LERperformance is similar to those using keyboard in-put.
Overall, synchronous and asynchronous HVRachieved WER comparable performance.6.4 Performance of Asynchronous HVRHapticSNR WER (%) LER (%)InputKeyboardClean 10.2 0.620 dB 11.2 0.610 dB 13.0 0.6KeystrokeClean 10.7 0.420 dB 11.4 1.010 dB 13.4 1.1Table 5: WER and LER performance of asynchronousHVR in different noise conditionsSimilar to synchronous HVR, asynchronous HVRalso achieved significant performance improve-ments over the pure ASR systems.
Table 5 shows theWER and LER performance of asynchronous HVRin different noise conditions.
The WER perfor-mance of asynchronous HVR is consistently betterthan that of synchronous HVR (comparing Tables 4and 5).
This is expected since the speech quality forasynchronous HVR is higher.
However, consider-ing the much slower input speed (c.f.
Table 2) andthe marginal WER improvements for asynchronousHVR, synchronous HVR appears to be a better con-figuration.6.5 Integrated Decoding vs. Lattice RescoringSNRWER (%)Clean 20dB 10dBIntegrated 11.8 12.7 15.0Lat-rescore 11.2 18.6 18.1Table 6: WER performance of keyboard synchronousHVR using integrated decoding and lattice rescoringAs previously mentioned in Section 5, HVR canalso be performed in two stages using lattice rescor-ing technique.
Table 6 shows the performance37comparison between integrated decoding and lat-tice rescoring for HVR.
Both methods gave similarperformance in clean condition.
However, latticerescoring yielded significantly worse performancein noisy environment.
Therefore, it is important totightly integrate the PLI into the decoding process toavoid premature pruning away optimal paths.6.6 Runtime PerformanceASR system searches for the best word sequenceusing a dynamic programming paradigm (Ney andOrtmanns, 1999).
The complexity of the search in-creases with the vocabulary size as well as the lengthof the input speech.
A well-known concept of To-ken Passing (Young et al, 1989) can be used to de-scribe the recognition search process.
A set of ac-tive tokens are being propagated upon observing anacoustic feature frame.
The best token that survivedto the end of the utterance represents the best out-put.
Typically, beam pruning technique (Ortmannset al, 1997) is applied to improve the recognition ef-ficiency.
Tokens which are unlikely to yield the op-timal solution will be pruned away.
HVR performs amore stringent pruning, where paths that do not con-form to the PLI sequence are also be pruned away.System SNR RTActive TokensPer FrameASRClean 1.9 626020 dB 2.0 645010 dB 2.4 7168KeyboardClean 0.9 349020 dB 0.9 376410 dB 1.0 4442KeystrokeClean 1.1 405920 dB 1.2 419010 dB 1.5 4969Table 7: WER and LER performance of integrated andrescoring synchronous HVR in different noise conditionsTable 7 shows the comparison of the runtime fac-tors and the average number of active tokens perframe for ASR and HVR systems.
The standardASR system runs at 1.9, 2.0 and 2.4 times real-time (xRT)4.
The runtime factor increases with de-4Runtime factor is computed as the ratio between the recog-nition duration and the input speech durationcreasing SNR because the presence of noise intro-duces more confusions, which renders beam prun-ing (Ortmanns et al, 1997) less effective.
The num-ber of active tokens per frame also increases from6260 to 7168 as the SNR drops from the clean con-dition to 10dB.
On the other hand, there are sig-nificant speedup in the runtime of HVR systems.In particular, synchronous HVR achieved the bestruntime performance, which is roughly consistentacross different noise conditions (approximately 1.0xRT).
The average number of active tokens also re-duces to the range of 3490 ?
4442.
Therefore, thesynchronous HVR using keyboard input is robust tonoisy environment, both in terms of WER and run-time performance.
The runtime performance usingkeystroke input is also comparable to that using key-board input (only slightly worse).
Therefore, bothkeyboard and keystroke inputs are effective ways forentering the initial letters for HVR.
However, it isworth noting that the iPad was used for the studiesconducted in this work.
The size of the iPad screenis sufficiently large to allow efficient keyboard entry.However, for devices with smaller screen, keystrokeinputs may be easier to use and less error-prone.7 ConclusionsThis paper has presented a unifying probabilisticframework for the multimodal Haptic Voice Recog-nition (HVR) interface.
HVR offers users the optionto interact with the system using touchscreen duringvoice input so that additional cues can be providedto improve the efficiency and robustness of voicerecognition.
Partial Lexical Information (PLI), suchas the initial letter of the words, are used as cuesto guide the recognition search process.
Therefore,apart from the acoustic and language models usedin conventional ASR, HVR also combines the hap-tic model as well as the PLI model to yield an inte-grated probabilistic model.
This probabilistic frame-work integrates multiple knowledge sources usingthe weighted finite state transducer operation.
Suchintegration is achieved using the composition oper-ation which can be applied on-the-fly to yield ef-ficient implementation.
Experimental results showthat this framework can be used to achieve a moreefficient and robust multimodal interface for text en-try on modern portable devices.38ReferencesAlex Acero, Li Deng, Trausti Kristjansson, and JerryZhang.
2000.
HMM adaptation using vector Taylorseries for noisy speech recognition.
In Proc.
of ICSLP,volume 3, pages 869?872.Stanley F. Chen and Joshua Goodman.
1996.
An empir-ical study of smoothing techniques for language mod-eling.
In Proceedings of the 34th annual meeting onAssociation for Computational Linguistics, ACL ?96,pages 310?318, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Edward Clarkson, James Clawson, Kent Lyons, and ThadStarner.
2005.
An empirical study of typing rateson mini-qwerty keyboards.
In CHI ?05 extended ab-stracts on Human factors in computing systems, CHIEA ?05, pages 1288?1291, New York, NY, USA.ACM.S.
B. Davis and P. Mermelstein.
1980.
Comparisonof parametric representations for monosyllabic wordrecognition in continuously spoken sentences.
IEEETransactions on Acoustic, Speech and Signal Process-ing, 28(4):357?366.Mark Gales and Steve Young.
1996.
Robust continuousspeech recognition using parallel model combination.IEEE Transactions on Speech and Audio Processing,4:352?359.H.
Hermansky.
1990.
Perceptual Linear Predictive (PLP)analysis of speech.
Journal of the Acoustic Society ofAmerica, 87(4):1738?1752.Mehryar Mohri, Fernando Pereira, and Michael Ri-ley.
2002.
Weighted finite-state transducers inspeech recognition.
Computer Speech & Language,16(1):69?88.H.
Ney and S. Ortmanns.
1999.
Dynamic programmingsearch for continuous speech recognition.
IEEE Sig-nal Processing Magazine, 16(5):64?83.J.
Ortega-Garcia and J. Gonzalez-Rodriguez.
1996.Overview of speech enhancement techniques for au-tomatic speaker recognition.
In Proceedings of In-ternational Conference on Spoken Language (ICSLP),pages 929?932.S.
Ortmanns, H. Ney, H. Coenen, and Eiden A.
1997.Look-ahead techniques for fast beam search.
InICASSP.L.
A. Rabiner.
1989.
A tutorial on hidden Markov mod-els and selective applications in speech recognition.
InProc.
of the IEEE, volume 77, pages 257?286, Febru-ary.K.
C. Sim.
2010.
Haptic voice recognition: Augmentingspeech modality with touch events for efficient speechrecognition.
In Proc.
SLT Workshop.A.
Varga and H.J.M.
Steeneken.
1993.
Assessmentfor automatic speech recognition: II.
NOISEX-92: Adatabase and an experiment to study the effect of ad-ditive noise on speech recognition systems.
SpeechCommunication, 12(3):247?251.S.J.
Young, N.H. Russell, and J.H.S Thornton.
1989.
To-ken passing: a simple conceptual model for connectedspeech recognition systems.
Technical report.39
