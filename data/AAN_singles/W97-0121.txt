Collocation Lattices and Max imum Entropy ModelsAndre i  M ikheevHCRC,  Language Technology Group, University of Edinburgh,2 Buccleuch Place, Edinburgh EH8 9LW,  Scotland, UK .e-mail: Andrei.Mikheev@ed.ac.ukS teven  F inchThomson Technical Labs,1375 Piccard Drive, Suite 250, RockviUe Maryland, 20850e-mail: sfinch~thomtech.comJuly 9, 1997AbstractMaximum entropy framework proved to be expressive and powerful for the statistical lan-guage modelling, but it suffers from the computational expensiveness of the model building.
Theiterative scaling algorithm that is used for the parameter estimation is computationally expen-sive while the feature selection process requires to estimate parameters of the model for manycandidate features many times.
In this paper we present a novel approach for building maximumentropy models.
Our approach uses a features collocation lattice and selects the atomic featureswithout resorting to iterative scaling.
After the atomic features have been selected we, usingthe iterative scaling, compile a fully saturated model for the maximal constraint space and thenstart to eliminate the most specific constraints.
Since during constraint deselection at everypoint we have a fully fit maximum entropy model, we rank the constraints on the basis of theirweights in the model.
Therefore we don't have to use the iterative scaling during constraintranldng and apply it only for linear model regression.
Another important improvement is thatsince the simplified model deviates from the previous larger model only in a small number ofconstraints, we use the parameters of the old model as the initial values of the parameters forthe iterative scaling of the new one.
This proved to decrease the number of required iterationsby about tenfold.
As practical results we discuss how our method has been applied to severaltasks of language modelling such as sentence boundary disambiguation, part-of-speech taggingand automatic document abstracting.1 Int roduct ionMax imum entropy modelling has been recently introduced to the NLP community and proved to bean expressive and powerful framework.
The max imum entropy model is a model which fits to a setof pre-defined constraints and assumes m~ximum ignorance about everything which is not subjectto its constraints thus assigning such cases with the most uniform distribution.
The most uniformdistribution will have the entropy on its max im~n and the model is chosen accorrllng to:model = argrnaxpecon~.aints H(p) where H(p) = - ~P i  * log2 Pi (1)iFor instance, if we want to disambiguate part-of-speech of a word and we observed that in 50%of the times a noun is preceeded by a determiner and in 30% of the thnes it is preceeded by an216adjective, we can state these observations as constraints to the model.
Thus our model will haveto choose a probability distribution for parts-of-speech w ich will agree with our observations andwhich will assign all the cases when a word which can be a noun is preceeded with neither adeterminer nor an adjective with equal probabilities.One of the most popular maximum entropy distributions i known as Gibbs distribution.
Itdefines a model as a set of Lagrange multipliers (Z,)~0..?n) and has an exponential form:p(wi) = ?e E??Exxx?*'fx?
(w') where (2)* T is a set of instantiated by their values atomic features of the model;* w/ is a described by the model entity which can be represented as a configuration of theinstantiated atomic features from T. We will call wi a configuration from con.figuration space?
W. The configuration space W includes not only observed configurations (w) but rather allpossible in the domain configurations many of which might have not ever been observed;* X1 is a constraint from the constraint space X imposed to the model.
It is essentially acombination ofinstantiated atomic features too.
We can look at the constraints as at employedby the model features 1.
In the rest of the paper we will use the terms constraint feature,feature and constraint interchangeably;?
fx~ (wi) is the indicator function which indicates whether or not the j -th constraint (Xj) isactive for the configuration wi.
This function takes two values: 1 - if the constraint is activeand 0 otherwise;?
~ (Lagrange multiplier) is the weight of the j-th constraint (Xj);?
Z is the normalization constant which ensures that the probabilities for all configurationssum up to 1:z= (3)wiEWApart from being a distribution of maximum entropy this distribution also po6sesses a veryimportant property of model decomposition.
For instance, if our atomic feature set T includesword length (1, 2, 3, 4, 5..), its capitalization (Cap) and whether it ends with the full-stop (fstop),and we want to obtain the probability of spelling the word "Mr." , we will have the equation asfollows:p(Mr.) = ?cX1"0+~2"?+'~3"1+~*?+~5"?+'~c','1+~1,~, .I = ?e +~c',+~.f'~There is nothing interesting about the model above since it constrains only atomic non-overlappingi.e.
independent features.
The main strength of the Gibbs distribution is that it can handlecomplex overlapping features and therefore account for feature interaction.
For instance, we mightnotice that the capitalization, when it is seen particlll~rly with words of length 3, has a differentdistribution from its general one.
To model this observation we can introduce a comple?
feature(i.e.
we set a complex constraint) which is a logical conjunction or collocation of the two atomicfeatures 3 and Cap.
Now our model will predict he probability for the word "Mr." as:1Note here the difference between atomic features and constraint features: constraint features consist from atomicfeatures but we can have a set of constraints which does not include some or even all atomic features per se but onlytheir combinations.217p(Mr.) = ?e~+~*,+~s'~,+x(a.c=,)Important hing to note here is that we still constrain the atomic features A3 and Acap togetherwith their collocation feature A(3,can)- So A(a,cap) has only the excess weight which differentiatesp(3, Cap) from the product of p(3) and p(Cap) - the case ff there were no feature interaction.
Suchdecomposition of complex features into simpler ones provides an elegant way of representing caseswith interactions of many overlapping features of high complexity.Because of its ability to handle overlapping features the maximum entropy framework provides abetter way to incorporate multiple knowledge sources than traditionally used for this purpose linearinterpolation and Katz back-off method (Katz 1987).
Rosenfeld 1996 evaluates in detail a maximumentropy model which combines unigrams, bigrams, trigrams and long-distance trigger words for theprediction of the next word.
The linear interpolation combines uch knowledge sources imply byweighting them as Pcornbined = ~=Z AiPi for'k knowledge sources.
It does not, however, model theinteraction between different knowledge sources 2 and only provides the best weights for the themunder the assumption of independence.
The back-off method, in fact, does not combine differentknowledge sources but rather ran/; them.
This allows for using the most informative method firstand back-off to a less informative method if there is not enough information for a more informativeone.
For instance, we can try to use the trigram model first, and only when there is no suitabletrigram known to the model, we back-off to the bigram model.
As the linear interpolation theback-off method does not account for possible interactions between different knowledge sourceswhich can lead to overestimation f some events.The maximum entropy framework naturally combines the good sides of the two methods and atthe same time it accounts for the interactions between features.
Every knowledge source producesa set of constraints which are used together with constraints from other knowledge sources - sono interpolation eeded.
Simple and complex features together with their overlaps are naturallyincorporated into the model and all the interactions are naturally accounted for.
Because of thedecompositional nature of the maximum entropy model, it can act as a back-off model too -overlapping simpler features naturally coexist with more complex ones and the weights of thecomplex features are just the excess on which they different from their constituent simpler features.Applying the exponential distribution discussed above the maximum entropy approach devel-oped in Della Pietra et ai.
1995 defines a framework for the selection of the best performingconstraint set and for the estimation of the weights (As) for these constraints.
The model inductionprocedure has two parts: feature selection and parameter estimation, both of which agree with theprinciple of maximum entropy.
In this paper we present a novel approach to feature selectionfor the maximum entropy models.
This approach requires less computational load than the onedeveloped in Della Pietra et al 1995 at a price of being not yet suitable for building models witha very large (hundreds of thousands) set of parameters.
We also propose a slight modification tothe process of parameter estimation for the conditional maximum entropy models.
Our methoduses assumptions similar to Berger et al 1996 but is naturally suitable for distributed parallelcomputations.2 Parameter  Es t imat ionThe parameter estimation for the exponential maximum entropy distribution is based on the Im-proved Iterative Scaling algorithm presented in Della Pietra et ai.
1995.
Its task is to determineweights (A) for all the constraint features of a given constraint space so that the resulting dis-2For instance, the probabilities for bigrams and trigrams are clearly not independent from each other.218iftribution will closely model some reference distribution which is usually taken as the empiricaldistribution of the configuration space.
The essence of the parameter estimation is as follows:there is a model with a set of atomic features T and a constraint feature space X- Everyconstraint feature consists of one or more atomic feature from T. For instance, we candefine a model with atomic features T = (2, 3, 4, 5, Cap, fs$op) and a possible feature spaceX = {2, 3, Cap, (2, Cap), (5, Cap, fstop)}.there is a sample of entities w = {w0...wm} which are representable as configurations ofatomic features 3 from T. We define a function ?
: w ~ T which maps entities w into T, forinstance, ?(Mr.)
~ (3, Cap, fstop).
Here we will refer to w-s as configurations meaning thatthey are mapped into atomic features.?
all the features from the constraint feature space X have corresponding indicator functions?
fxo(W)...fx= (w) to flag whether a certain constraint feature is active for a particular configu-ration from the configuration space:1 if X~ C ~(wi) (4)= 0For instance: f (z,cap)( Mr.) = 1 since .?(Mr.)
--~ {3, Cap, f stop) and (3, Cap) C (3, Cap, \]stop).?
we choose a reference distribution to fit our model to, so we can associate very featurefrom the model's constraint space with a corresponding reference probability: 25(Xo)...iS(Xn ).Usually the reference distribution is simply the empirical distribution of our features in theconfiguration space.
In this case the reference probability for a feature is computed as:(xk) = (5)wiE~where ~(wi) is the frequency count of the i-th configuration over the total number of observedconfigurations.
Note here that the total sum of feature probabilities can be greater than 1:~x~?xi6(Xk) -> I since features can overlap with each other.?
we const r~ our features to their reference probabilities: P(Xk) ~ P(Xk) and using equation 5obtain:Cxk) = fx Cw ) ,vCw ) = vCxk) (6)~iEw w~EWwhere IS(wi) is estimated from the sample of configurations and p(w~) is computed usingequation 2.
Note here that in the second part of the equation we sum over all possibleconfigurations (W) in the domain.
* To  fit the mode l  to its reference distribution we use the Improved Iterative Scaling algorithm:1. we initialize all weights (lambdas) of the features from our constraint space with someinitial values.
Usually if we don't have other evidence we start with the uniform distri-bution so all lambdas are set to the same value e.g.
O: Ax0 = 0 ..... Axe = O.Slndeed, the feature space and the mapped configuration space can be identical if we want to memorize in ourmodel all and only seen cases.
This can lead, however, to the overlltting of the model and such model also will notpossess any generalization power, so its performance on unseen cases might be rather poor.2192.
we calculate the normalization constant Z using equation 3 and the maximum entropyprobabilities (p(wo)....p(wm) for each configuration from the total configuration spaceW using equation 2.3. for each feature from the model's constraint space we apply the constraint as in equa-tion 6 and compute how its weight should be adjusted (AA) to satisfy the constraint:i~(xk) ~ Z .f~ (w~) ?
p(w~) ?
e~*3C~ ~ I~c~,) (7)w~EWwhere fxk (wd) ensures that only those configurations (wd) which include the constrainedfeature Xk contribute to the mass probability, and ~.ex  fx~ (wd) is simply the number ofall the features from the model's feature space a which are active for the i-th contributingconfiguration (wi).
This ensures that we account only for that proportion which belongsto Xk in the contributing configurations.
In general there is no analytical solution tosuch equations and the most popular numerical method is Newton's method where wefit AA iteratively.4.
if the greatest AA computed at the previous tep is smaller than a certain threshold -the algorithm has converged and we exit.
Otherwise we update the model's weights as:Ax~ =- Axk + AAxk and go to step 2 to obtain a better fit of the model.?
As the result we have a fully specified maximum entropy model of the form: (Z, Axo ...... Ax,~)2.1 Comput ing  Cond i t iona l  Mode lsGeneralized Iterative Scaling algorithm presented above defines a way for the computing of Maxi-mum Entropy models for joint probability distributions.
With such models we can answer questionssuch as what is the probability of generating an entity described as a configuration of atomic fea-tures.
For instance, a joint model can predict how likely it is to generate a capitalised word withsuffix ~ing" : p( capi~al ized = YES ,  su f = ing ).
In the statistical language modelling we, however,are often concerned with the conditional probabilities: what will be the probability of Y to take acertain value y if we see a feature-configuration x.
For instance, a conditional model can predicthow likely that a word will be a verb if the previous word was a noun and the previous but oneword was an adjective: p(wordi = VB \[ wordi-1 = NN,  wordl-2 = ADJ ) .
For supervised trainingof conditional models the sample space consists of configurations which include features from twonon-overlapping sets: factor features (X) and behavior features (Y).
Using such joint configura-tions w -- (x, y) we have to estimate a conditional model which would predict a behaviour variable(Y) given a configuration of factor variables (X): p(y \[ x).There is an obvious simple way to compute a conditional model by computing a joint modelX, Y for every value of the behavior variable separately and then the conditional model is computedas:p(~ I z) = p(y' ~)iEY4Note that this is not just the number of atomic features which compose the i-th configuration but rather thenumber of all registered in the model features (atomic and complex) which are active for it.
For instance, ff our ourfeature space is \[3, Gap, \]stop, (3, fstop), (Gap, fstop), (3, Cap, fstop)\] the constraint for the feature (3) will look like:15(3) ~ p(w--+(3)) * e ~xs'* + p(w--+(3, fstop)) * e Ax3"a + p(w--+(3, Cap)) * e z~x~'2 +p(w.-+(3, Cap, fstop)) ?
e A~'3"e220IIIIIIIII!IIIIII!IIwith a suitable choice of Z for each joint model.
This approach is naturally suitable for distributedcomputation but as it was pointed out in Rosenfeld 1996 it is not a good way to proceed becauseevery behavior is activated only by a fraction of possible factors but we will estimate their jointmodel on the whole space of possible configurations (W).
Apart from the computational overloadthis will require training data well beyond usually available in the training samples.Berger et al 1996 presented a way of computing conditional maximum entropy models directlyby modifying equation 6 as follows (now instead of w we will explicitly use (x, y) ):i ~Cx~) = ~ f~(~, y) * ~(~, y) ~ ~ .~(~, y) * ~(~) * pCy I ~) = p(xk) (9)x6X yEY xEX yEYwhere ~(x, y) - is an empirical probability of a joint configuration (w) of certain instantiated factorI variables with certain instantiated behavior variables.
~(x) is the marginal empirical probability ofthe factor variables.
The constraint solving equation 7 is then correspondingly adjusted as:I ~Cxk) ~ ~ .~x~C~,y) *~(~) *pCy I ~) * e ~'Z~I~c~'~) (I0)xEX YEYand the normalization constant Z(x) ensures that ~erP(Y  \] x) = 1.
This approach differs fromthe standard approach for joint distribution (equation 7) in plugging in the empirical marginalestimate ~(x).
This restricts the constraint set only to those cases which were actually observed inthe training samples for a particular value of the behavior variable (y) and in solving a constraintwe only sum over seen conjurations rather than all possible ones.In this paper we propose to adjust the first method with restrictions imilar to that of thesecond method.
We will compute joint distributions separately thus benefitting from the possibilityof distributed computations - each joint model can be independently computed on a separateprocessor or machine using multi-threading together with remote process calls (RPC).
At thesame time we will restrict the configuration space from all possible configurations in the domain(W) only to the observed and logically implied configurations (w +) as it is described in section 6.We will require that the normalization constants Z for each joint model (X, y) ensure that allprobabilities in a joint model sum up to the empirical marginal probability of the behavior variable~(y), thus accounting only for the true proportions of the joint models.
We also adopt a furtheryet simplification suggested in Ristard 1996 to restrict he constraints only to the cases when theoverall joint frequency of a feature Xk = (x, y) is greater than a certain threshold, for instance 5.3 Feature SelectionThe iterative scaling algorithm applied for the parameter estimation provides us with a set ofAs which ensure that the model fits to the reference distribution and does not make spuriousassumptions (as required by the max imum entropy principle) about events beyond the referenceevents.
It, however, does not guarantee that the features employed by the model are good featuresand the model is useful.
Thus the most important part of the model building is the feature selectionprocedure.
The key idea of the feature selection is that if we notice an interaction between certainfeatures we should build a more complex feature which will account for this interaction.
Thenewly added feature should improve the model: its Kuliback-Leibler divergence from the referencedistribution should decrease:?
i~(~) (11)221For a conditional model Kullback-Leibler divergence is computed as:Iz) 11 P) = ?
log  (X2)zEXyEYand the conditional maximum entropy model will also have the greatest log-likelihood (L) value: \ [ \ ]model = argrnax L~(p) where L~(p) = ~ l~(x, y) * log p(y I x) (13) UxEXyEYThe basic constraint feature induction algorithm presented in Della Pietra et ai.
1995 startswith an empty feature space and iteratively tries all possible feature candidates which are eitheratomic features or complex features produced as a combination of an atomic feature with thefeatures already selected to the model's feature space.
For every feature from the candidate f atureset the algorithm prescribes to compute the maximum entropy model using the iterative scalingalgorithm described above, and select he feature which minimized the Kullback-Leibler divergenceor maximized the log-likelihood of the model in the largest way.
This approach, however, is notcomputationaily feasible since the iterative scaling is computationaUy expensive and to computemodels for many candidate f atures many times is unreal.
To make feature ranking computationallytractable in Della Pietra et al 1995 and Berger et al 1996 a simplified process proposed: at thefeature ranking stage when adding a new feature to the model all previously computed parametersare kept fixed and, thus, we have to fit only one new constraint imposed by a candidate feature.Then after the best ranked feature has been established it is added to the feature space and theweights for all the features are recomputed.
This approach allows for estimating ood featuresrelatively fast but it does not guarantee that at every single point we add the best feature becausewhen we add a new feature to the model all its parameters can che uge.In this paper we present a novel approach to feature selection for the maximum entropy models.Our approach uses a feature collocation lattice and selects the atomic features without resorting tothe iterative scaling.
After the atomic features have been selected we, using the iterative scaling,compute a fully saturated model for the maximal constraint space and then the algorithm startsto eliminate the most specific constraints.
Since during constraint deselection at every point wehave a fully fit maximum entropy model, we rank the features on the basis of their weights inthe model.
Therefore we don't have to use the iterative scaling for constraint ranking and applyit only for linear model regression.
Another important improvement is that since the simplifiedmodel deviates from the previous larger model only in a small number of constraints, we use theparameters of the old model as the initial values of the parameters for the iterative scaling of thenew one.
This proved to decrease the number of required iterations by about enfold.
In the rest ofthe paper we first introduce the feature collocation lattice as a graphical way to represent complexmodels, then we introduce a feature selection process using the collocation lattice and finally, wepresent some details of application of our method to the tasks of sentence boundary disarnbiguation,part-of-speech tagging and automatic document abstracting via sentence extraction.4 Feature  Co l locat ion Latt iceWhen we have a set of atomic features T and a training sample of configurations w, we can builda feature collocation lattice.
Such collocation lattice will represent, in fact, the factorial constraintspace (X) for the maximum entropy model and at the same time will contain all seen and logicallyimplied configurations (w+).
Formally, the feature collocation lattice is a 3-ple: (0, C_, ~> where2220 is a set of nodes of the lattice which corresponds to the  union of the feature space of themaximum entropy model and the configuration space: 8 = XU?(w).
In fact, the nodes in thelattice (0) can have dual interpretation - on one hand they can act as mapped configurationsfrom the extended configuration space (w +) and on the other hand they can act as featuresfrom the constraint space (X);C is a transitive, antisymmetric relation over 0 x 0 - a partial ordering.
We also will needthe indicator function similar to one in equation 4 to indicate whether the relation C holdsfrom node i to node k:I ~f 0{ _C Okfo,(OD = 0~tu is a set of configuration frequency counts of the nodes (8) of the lattice.
This representshow many times we saw this particular configuration i  our training samples.
Because of thedual interpretation of the nodes a node can also be associated with its feature frequency counti.e.
the number of times we see this feature combination anywhere in the lattice.
The featurefrequency of a node (similar to equation 5) will then be ~X(ek) = ~\]sieo fek (8i) * ~ which isthe sum of all the configuration frequency counts (~tO) of the descendant nodes.Suppose we have a lattice of nodes A, B, (AB) with obvious relations: A C (AB); B C (AB):A/z  (AB) ~ BThe configuration frequency ~a will be the number of times we saw A but not (A.B) and thenthe feature frequency of A will be: ~ = ~ + ~B i.e.
the number of times we saw A in allthe nodes.When we construct a feature collocation lattice from a set of samples, each sample representsa feature configuration which we must add to the lattice as a node (Ok) if is not already there.
Tosupport generMizations over domain we also want to add to the lattice those nodes which were notseen on their own but only as common parts of other nodes in the lattice.
Thus we add to thelattice all sub-configurations of a newly added configuration which are the intersections with theother nodes.
We increment he configuration frequency (~)  of a node each time we see in thetrair~ng samples this particular configuration on its own but not as a part of another configuration.For example, if a configuration (ABCD) comes from a training sample and it is still not in thetO lattice, we create a node (ABCD) and set its configuration frequency ~(ABCD) to 1.
If by that timethere is a node (ABDE) in the lattice, we then also create the node (ABD), relate it to the nodes(ABCD) and (ABDE) and set its configuration frequency to 0.
If (ABCD) had already been inthe lattice we would simply incremented its configuration frequency: ~(ABCD) = ~BCD) + 1.Thus in the feature lattice we have nodes with non-zero configuration frequencies, which wecall reference nodes and nodes with zero configuration frequencies which we call latent or hiddennodes.
Reference nodes actually represent he observed configuration space (w).
Hidden nodesare never observed on their own but only as parts of the reference nodes and represent possiblegeneralizations about domain: low-complexity constraints (X) and logically possible configurations(w+).Sometimes, there develops a class of hidden nodes which does not provide good generalizations- those hidden nodes that directly support less than two higher level nodes.
By support here wemean the _C relation and the direct support is that there is no intermediate nodes between two223III" |II Figure I: This figure shows a feature lattice where thick orcles repre~mt reference nodes and filledcircles represent obsolete hidden nodes.
3 indicates that a word has length 3, C indicates that aword is capitalised, Mr. indicates that a word has spelling "Mr3 and Dr. indicates that a wordhas spelling ~Dr2.nodes linked with the _C relation.
Such redundant nodes might develop because we explicitly putall the atomic features into the lattice but some of them never act on their own.
Figure 1 showsan excerpt from a feature lattice with the atomic features including: word length (1, 2, 3,4, 5..),capitalization (C), spellings (Mr., Dr...) and others.
The nodes (Dr., 3, C), and (Mr., 3, C) arethe reference nodes - they were observed in full 5 and have non-zero configuration frequency counts((ur.,s,c) > 0 (Mr.,S,C) > 0).
All other nodes on figure 1 are hidden nodes that were observedonly as parts of the higher level nodes 6.
Hidden nodes (Dr., 3), (Mr., 3), (Mr., G) and (.Dr., 6')directly support only one node each and thus do not provide any gener~J~.ations.
Therefore thesenodes are obsolete and can be safely removed from the lattice.
The nodes (Dr.) and (Mr.) thenbecome obsolete as well, since they will not support directly any node, so we can safely removethem from the lattice too.
Nodes (3) and (C) apart from supporting the node (3,C) support someother nodes not represented on figure 1, and thus should be retained in the lattice.This method of building the feature collocation lattice ensures that along with true observationsit contains hidden nodes which can provide generalizations about domain, and at the same timethere is no over-generation f the hidden nodes: no logically impossible feature combinations andno hidden nodes without generalization power are included.
Such collocation lattice was success-fully applied for representing queries for document retrieval (Finch 1997 ) where it behaved as acombination of the boolean and the vector space models.5 Aton~ic  Feature  Se lec t ionIIIIIIIIAfter from a set of samples we have constructed a feature collocation lattice (0, C,~ w) which wewill call the empirical attice, we try to estimate which atomic features contribute and which donot to the frequency distribution on the reference nodes.
Thus we will retain in the lattice onlythe predictive atomic features.
The optimized feature space can be seen as a feature lattice definedSIndeed, any time we see a configuration for the words ~'Mr3 and ~'Dr."
it always includes their length 3 andtheir capitalization feature C together with their spellings.eFor instance, it is impossible to see the configuration (Dr., C) without seeing the configuration (.Dr., 3, G).IIII224 IIIIIIa) b) c)'~Z = '~ + ,~B + '~c + ~Bc  ~ = ~ + '~c '~7 = ~ ~'  = ~c~Y.
= ~$ + '~c '~Y = ~$ ~Ec = ~~c ~'~  = '~  + ~ec  '~  = ~'B ~,~c = ~~c~c  = '~cFigure 2: This figure shows the redistribution ofconfiguration frequencies in the optimized featurelattice when adding new atomic nodes.
Case a) stands for adding the atomic feature A to theempty lattice, case b) stands for adding the atomic feature B to the lattice with the atomic featureA and case c) stands for adding the atomic feature C to the lattice with the atomic features Aand B and their collocations.
The unfilled nodes tand for the nodes in the empirical lattice whichdon't have reference in the optimized lattice.
The nodes in bold stand for the nodes decided by theoptimized lattice (i.e.
they can be assigned with some probabilities).over the empirical feature lattice: 8' C_ 0 and initially it is empty: 8' =,~.
We build the optimizedlattice by incrementally adding an atomic feature from the empirical lattice together with the nodeswhich are the minimal collocations of this atomic feature with the nodes already included into theoptimized lattice.
So if in the optimized feature lattice there is just one feature A, then when weadd the atomic feature B we also have to add the collocation (A.B) if it exists in the empiricallattice.
The configuration frequency of a node in the optimized lattice (~,w) then can be computedas:Thus a node in the optirnLzed lattice takes all cor~fi~ation frequencies (~w) of itself and the aboverelated nodes i?
these nodes do not belong to the optimized lattice themselves and there is no highernode in the optimized lattice related to them.Figure 2 shows how the configuration frequencies in the optimized lattice are redistributedwhen adding a new atomic feature.
First the lattice is empty.
When we add the atomic featureA to the optimized lattice (figure 2.a), because no other features are present in the optimizedlattice, it takes all the configuration frequencies of the nodes where we see the feature A: ~ =~ + ~B + ~c  ?
~Bo- Case b) of figure 2 represents he situation when we add the atomicfeature B to the optimized lattice which already includes the atomic feature A.
Apart from thenode B we also add to the optimized lattice the collocation of the nodes A and B.
Now we haveto redistribute the configuration frequencies in the optimized lattice.
The configuration frequencyof the node A now will become the number of times of seeing the node A but not the node .AB:~ = ~ + ~o.
The configuration frequency of the node B will be the number of times of seeingthe node B but not the node A.B: ~ = ~ + ~'~.
The configuration ~equency of the node AB will225be: ~'B = '~e + ~ABC" When we add the atomic feature C to the optimized lattice (figure 2.c) weproduce a fully saturated lattice identical to the empirical attice, since the node C will collocatewith the node A producing AC and will collocate with the node B producing BC.
These nodes intheir turn will collocate with each other and with the node AB producing the node ABC.During the building of the optimized lattice all the atomic features from the empirical atticecompete, and we include the one which results in a new optimized lattice with the smallest diver-gence D(p \[\[ p') (see equation 11 and equation 12) and therefore with the greatest log-likelihoodLp(p') according to the optimization task stated in equation 13, where:?
in the selection of features for a conditional model the only extra calculation is to convertjoint probabilities of nodes e = (x, y) to the conditional ones.
This is easily done by usingthe equations as follows:io(ylz) = p(e~ = (z,y))  /Cylz) = / (o~ = (x,~,)) (z5)Eok~o .fok (z) ?
p(e~) Eo~o Jo~ Cz) ?
fCOk)Note here that only reference nodes from the empirical attice contribute to D(p \[I P') andLp(p') estimations since hidden nodes always have their empirical probabilities (~) set to 0.?
p(Si) is the probability for the i-th node in the empirical lattice:N oj~o?
p'(ei) is the probability assigned to the i-th node using only the nodes included into theoptimized lattice.
{ 9~.
~I 8i e e'p'(e~) = ~ ~s e~ ?
e' ~ \[3e~ : e~ e e'(n)The optimized lattice assigns the probability to a node in the empirical lattice equal to thatof its most specific sub-node T from the optimized lattice.
For reference nodes which do nothave sub-nodes in the optimized lattice at all (undecided nodes) according to the maximumentropy principle we assign the uniform probability of making an arbitrary prediction.For instance, for the example on figure 2.b the optimized lattice includes only three nodes butthere is just one undecided node (C) which is not shown in bold.
So the probabilities for the nodeswill be:~(A) = -7- ~(B)= ~- p(AB)= ~ p(iC)= q- ~(~c)= ~- p(~c)= '@ ~(c)=N is the total count on the empirical attice and is calculated as shown in equation 16:~Since when we add an atomic feature to the optimized lattice we also add all its relevant collocations, for everynode in the empirical lattice which does not belong to the optimized lattice there is always no more than one mostspecific sub-node in the optlmi~ed lattice.
This can be the node itself.226IIIIIIIIIIIIIIIIIIIThe presented above method provides us with an efficient way of selecting only importantatomic features from the in/tiM set of candidate atomic features without resorting to iterativescaling.
When this way we add atomic features to the optimized lattice, some of the features raightturn out not to contribute or contribute only- on a very small scale to the probability distributionon the lattice.
Such redundant atomic features can be classified into three categories: a) featureswhich are simply not informative; b) features which are always seen with some other feature (co-founded features) and c) features which are mutually exclusive with some other feature.
In the firstcase such features are usually present randomly in the reference nodes of the empirical lattice andtherefore they do not have any discriminative power.
In the second case all the relevant referencenodes will be already correctly decided by the time we will try to add another co-founded feature.Imagine that in the example on figure 2 most of the times when we saw the feature B we sawthe feature C as well.
So the redistribution of the configuration frequencies as on (figure 2.c) willnot bring much advantage and we can safely leave the feature U out of the optimized lattice andtherefore use the atomic feature set as on figure 2.b.
In the third case the configuration frequencyremaiuing on the parent node will account for the case of having two mutually exclusive higher levelnodes.
Imagine that B and C in the example above are two mutually exclusive features and wenever see them on their own.
So our lattice will consist from the nodes A, B, C, AB  and AC.
Seeingthe node AC is equal of not seeing the node AB when seeing A, and this is already presupposedby the configuration frequency ~'  when the node AB but not the node AC is in the lattice.
Thesame stands for the features B and C - the frequency of seeing the feature C is the frequency of notseeing the feature B because they are mutually exclusive and thus all the configurations withoutfeature B will account for the presence of the feature C if we don't put it into the lattice.6 Mode l  Comput ing  and  Genera l i za t ionAfter we have chosen a subset of the atomic features for our model, we restrict our feature latticeto the optimized lattice.
Now we can compute the maximum entropy model taking the referenceprobabilities (which are configuration probabilities) as in equation 17.
Instead of using total possibleconfiguration space (W) as required for iterative scaling by equation 7 we restrict he configurationspace to that actually observed uring the lattice building.
Here we have two choices.
First asthe configuration space we can use only the reference nodes (w) from the lattice which makes itsimilar to the method of Berger et al 1996 described in section 2.1.
We can also use all the nodesfrom the lattice (reference and hidden) as the extended configuration space (w+).
This can be seenas the union of the observed and logically implied configuration spaces which still usually will bemuch smaller than the total possible configuration space (W).
In this case the computational loadwill increase proportionally to the number of hidden nodes but the model itself will be fit moreaccurately.
Thus depending on the s/ze of the lattice we can use either the first or the second way.The nodes from the lattice also serve as potential constraint features to our model.
However,even ~f the lattice is large, only a fraction of the nodes will be relevant as possible constraints for aparticular joint distributions 0 = (X, y) since most of the nodes will have zero or very small featurecount.,; ~x s Thus we will consider as possible constraints for the model only those nodes whose 0=(=,y) "feature frequency counts are greater than a certain threshold, e.g.
: ~ z,~ > 5.
This means that =( )we constrain only features with reliable estimates and at the same time we drastically decrease thecomputational load.Initially we constrain all the nodes which satisfy the above requirement.
Then for each behaviorvariable we run the Improved Iterative Scaling algorithm as described in section 1 and produce as~'~(o~) = ~o,~o, fo~ (el) * ~227joint model with parameters (Z, A0...An).
Then we use these joint models for the computing of aconditional model as described in section 2.1.
This model will closely fit the reference distributionof the optimized feature lattice but usually it will be too specific and might poorly predict theunseen cases.
In theory we want to constrain only some general hidden nodes so they wouldaccurately predict the reference nodes and we hope that they will be good as well for the unseenconfigurations.In order to generalize and simplify our maximum entropy model we unconstrain the mostspecific features, compute a new simplified maximum entropy model and if it still predicts well,we repeat he process.
So our aim is to remove from the constraints as many top level nodes aspossible without loosing the model ftness to the reference distribution ~) of the optimized featurelattice.
The necessary condition for a node to be taking as a candidate for being removed fromthe constraint set is that this node shouldn't have any constrained nodes above it.
There is alsoa natural ranking for the candidate nodes: the closer to 1 the weight (A) of a constrained node is,the 'less it is important for the model.
We can set a certain threshold on the weights, so all thecandidate nodes whose As differ from 1 less than this threshold will be unconstrained in one go.Therefore we don't have to use the iterative scaling for feature ranking and apply it only for linearmodel regression, possibly un-constraining several feature configurations (nodes) at once.
Thismethod, in fact, resembles the Backward Sequential Search (BSS) proposed in Pedersen&Bruce1997 for decomposable models.
Unlike there, however, we don't believe that models with complexoverlapping feature interactions can be estimated irectly from their feature distribution and usethe iterative scaling algorithm instead.
Another important improvement in our method is thatsince the generalized smaller model deviates from the previous larger model only in a small numberof constraints, we use the parameters of that larger model 9 as the initial values for the iterativescaling algorithm.
This proved to decrease the number of required iterations to fit the simplifiedmodel by about tenfold, which makes a tremendous saving in time.There can be many possible criteria when to stop the generalization algorithm.
The simplestone is just to set a predefmed threshold on the deviation D(~ II P) of the generalized model from thereference distribution.
Pedersen&Bruce 1997 suggest o use Alc~i~e's Information Criteria (AIC)to judge the acceptability of a new model.
AIC rewards good model fit and penalizes models withhigh complexity measured in the number of features.
We adopted the stop condition suggested inBerger et al 1996 - the maximization of the likelihood on a cross-validation set of samples whichis unseen at the parameter esti~_tion.7 App l i ca t ions  o f  the  MethodWe applied the above described method of building maximum entropy models to several tasks:sentence boundary disambiguation, part-of-speech tagging and document abstracting via sentenceextraction.
In this section we deliberately will not go into details of training and evaluation - thiswill be the subject for a separate paper - but rather we will concentrate on the feasibility of theproposed method for real world applications.Sentence boundary disambiguation has recently gained certain attention of the language ngi-neering community.
It is required for most text processing tasks such as, tagging, parsing, parallelcorpora lignment etc., and, as it turned out to be, it is a non-trivial task itself.
A period can act asthe end of sentence or be a part of an abbreviation, but when an abbreviation is the last word in asentence, the period denotes the end of sentence as well.
The simplest "period-space-capital_letter"approach works well for simple texts but is rather unreliable for texts with many proper names9instead of the uniform distribution as prescribed in the step 1 of the Improved Iterative Scaling algorithm.228and abbreviations at the end of sentence as, for instance, the Wall Street Journal (WSJ) corpus (Marcus et ai.1993 ).To tackle this problem we built two maximum entropy models.
The first model used a lexiconof words associated with one or more categories from the set: abbreviation, proper noun, contentword, closed-class word.
This model employed atomic features uch as the lexicon information forthe words before and after the period, their capitalization and spellings.
For training we collectedfrom the WSJ corpus 51,000 samples of the form (Y, F..F) and (N, F..F), where Y stands for theend of sentence, N stands for otherwise and Fs stand for the features of the model.
We built amodel out of 238 most frequent atomic features which gave us the collocation lattice of 8,245 nodesin 43 minutes of processor time on SUN Ultra-1 workstation.
When we appl/ed the atomic featureselection algorithm (section 5), we in 53 minutes bo/led the lattice down to 3,769 nodes.
Thenconstraining all the nodes we compiled a maximum entropy model in about three hours and thenusing the constraint removal process in two hours boiled the constraint space down to 619.
Forthe evaluation we usecl the same 27,294 sentences as in PaimerSzHearst 1994 and Palmer&Hearst1997 I?
which were also used by Reynar&Ratnaparkhi 1997 in the evaluation of their system.
Thesesentences, of course, were not seen at the training phase of our model.
Our model achieved 99,2%accuracy which is the highest quoted score on this test set known to the authors.We also built a maximum entropy model to deal with unknown abbreviations, i.e.
the modelclassifies whether or not an unknown to the lexicon word is an abbreviation.
This model relieson the surface lexical features of words such as, word capitalization (C), class of the character(consonant (c), vowel (v), punctuation (p) or digit (d)) in the four last positions of the word andthe length (1, 2,3,4,5, 6+) of the word.
From the test samples we collected the frequencies of allour features and included into the initial atomic feature set only those features which appearedmore than 1000 times in the positive training samples.
There were 49 of such features but they hada very high level of co-occurrence and produced the empirical feature collocation lattice of 7,626nodes.
This took about 67 minutes of the processor time.
Then we run the atomic feature selectionalgorithm and our atomic feature set in 46 minutes was boiled down to 42 atomic features andthe feature collocation lattice to 3,028 nodes.
Then the constraint removing algorithm boiled theconstraint space down to 1,031 in two hours.
The accuracy of the classification of the producedmodel reached 96,4% on unseen words.Using our method we built a maximum entropy model for part-of-speech tagging.
We consideredas features bigram and trigram combinations together with unigrams of possible parts-of-speech forwords in question.
We also included into the constraint set the actual spellings of the most frequentwords.
We collected training samples from the Brown Corpus distributed with the Penn Treebank(Marcus et al1993 ).
This gave us 1,763 atomic features, but the lattice itself was not-surprisinglyfiat and had 38,564 nodes.
It was boiled down to 16,078 in 40 hours of the processor t/me.
Wedidn't specifically evaluate the model but it is about 1.2% more accurate than a bigram HiddenMarkov Model which we used before.We also built a maximum entropy model for the task of extraction of the most informativesentences for automatic document abstracting.
Here we used about 120 atomic features uch assentence position, sentence length, q-phrases, etc.
The initial lattice was of 28,114 nodes.
Afterthe atomic feature selection it was reduced down to 3~792 nodes.
This was boiled down to 311nodes by the constraint removal algorithm.
It actually shows that there was only a very smallinterdependency among the features and not-surprisingly our model hnproved only about 1.1%over a simple Bayesian classifier achieving just under 70% precision.2?We want to thank David Palmer for making his test data available to us.2298 Conc lus ionIn this paper we presented a novel approach for building maximum entropy models.
Our approachuses a feature collocation lattice and selects the atomic features without resorting to iterativescaling..A_fter the atomic features have been selected we, using the iterative scaling, compute afully saturated model for the maximal constraint space and then start to eliminate the most specificconstraints.
Since during constraint deselection at every point we have a fully fit maximum entropymodel, we rank the constraints on the basis of their weights in the model.
Therefore we don't haveto use the iterative scaling for constraint ranking and apply it only for linear model regression.Another important improvement is that since the smaller model deviates from the previous largermodel only in a small number of constraints, we use the parameters of the old model as the initialvalues of the parameters for the iterative scaling of the new one.
This proved to decrease thenumber of required iterations by about tenfold.
We applied the described method to severallangnage modelling tasks and proved its feasibility for selecting and building the models with thecomplexity of tens of thousands constraints.
A potential drawback of our approach is that werequire to build a maximum entropy model for the whole observed feature-space which might notbe feasible for applications with hundreds of thousands of features.
So one of the directions in ourfuture work is to find efficient ways for a decomposition f the feature lattice into non-overlappingsub-lattices which then can be handled by our method.
Another avenue for further improvement isto introduce the "or" operation on the nodes of the lattice.
This can provide a further generalizationover the employed by the model features.ReferencesBerger et al 1996 A. Berger, S. Della Pietra, V. Della Pietra, 1996.
A Maximum Entropy Approach toNatural Language Processing In ComputationaZ Linguistics voi.22(1)Della Pietra et al 1995 S. Della Pietra, V.. Della Pietra, and J. Latferty 1995.
Inducing Features of RandomFields Technical report CMU-CS-95-144Finch 1997 S. Finch 1997.
Query Domains: Towards an Algebra for Collocation.
TTSG Technical ReportTTLNLP/97/003, Rockvine, MD.Katz 1987 S. Katz 1987.
Estimation of probabilities from sparse data for the language model component ofa speech recognizer.
In IEEE Transactions on Acoustics, Speech and Signal Processing, ASSP-35Marcus et a1.1993 M. Marcus, M.A.
Marcinkiewicz, and B. Santorini 1993.
Building a Large AnnotatedCorpus of English: The Penn Treebank.
In Computational Linguistics, vol 19/2 pp.313-329Palmer&Hearst 1994 D. D. Palmer and M. A. Hearst 1994.
Adaptive Sentence Boundary Disambiguation.In Proceedings of the Fourth A CL Con\]erence onApplied Natural Language Process~m3 (ANLP'9~), ACL.PaLmer&Hearst 1997 D. D. Palmer and M. A. Hearst 1997.
Adaptive Multilingual Sentence BoundaryDisambiguation.
In Computational Lingu?s~cs, ACL.Pedersen&Bruce 1997 T. Pedersen and R. Bruce 1997.
A New Supervised Learning Algorithm for WordSense Disarabiguation.
In Proceedings of the Fourteenth National Conference on Artificial In~lligence,Providence, RI.Reynar&Ratnaparkhi 1997 J. C. Reynar and A. Ratnaparkhi 1997.
A Maximum Entropy Approach toIdentifying Sentence Boundaries.
In Proceedings ofthe FiSh A CL Conference on Applied Natural LanguageProcessing (ANLP'97), Washington D.C., ACL.Ristard 1996 E. S. Ristaxd 1996.
Maximum Entropy Modelling Toolldt.
Documentat.ionfor Werswn 1.3 Beta,Dra~, available at cmp-lg/96120005Rosenfeld 1996 R. Rosenfeld 1996.
A Maximum Entropy Approach to Adaptive Statistical Language Learn-ing.
In Computer Speech and Language, vol.10(3) Academic Press Limited230
