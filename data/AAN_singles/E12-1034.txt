Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 336?344,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsSkip N-grams and Ranking Functions for Predicting Script EventsBram JansKU LeuvenLeuven, Belgiumbram.jans@gmail.comSteven BethardUniversity of Colorado BoulderBoulder, Colorado, USAsteven.bethard@colorado.eduIvan Vulic?KU LeuvenLeuven, Belgiumivan.vulic@cs.kuleuven.beMarie Francine MoensKU LeuvenLeuven, Belgiumsien.moens@cs.kuleuven.beAbstractIn this paper, we extend current state-of-the-art research on unsupervised acquisition ofscripts, that is, stereotypical and frequentlyobserved sequences of events.
We design,evaluate and compare different methods forconstructing models for script event predic-tion: given a partial chain of events in ascript, predict other events that are likelyto belong to the script.
Our work aimsto answer key questions about how bestto (1) identify representative event chainsfrom a source text, (2) gather statistics fromthe event chains, and (3) choose rankingfunctions for predicting new script events.We make several contributions, introducingskip-grams for collecting event statistics, de-signing improved methods for ranking eventpredictions, defining a more reliable evalu-ation metric for measuring predictiveness,and providing a systematic analysis of thevarious event prediction models.1 IntroductionThere has been recent interest in automatically ac-quiring world knowledge in the form of scripts(Schank and Abelson, 1977), that is, frequentlyrecurring situations that have a stereotypical se-quence of events, such as a visit to a restaurant.All of the techniques so far proposed for this taskshare a common sub-task: given an event or partialchain of events, predict other events that belongto the same script (Chambers and Jurafsky, 2008;Chambers and Jurafsky, 2009; Chambers and Ju-rafsky, 2011; Manshadi et al 2008; McIntyre andLapata, 2009; McIntyre and Lapata, 2010; Regneriet al 2010).
Such a model can then serve as inputto a system that identifies the order of the eventswithin that script (Chambers and Jurafsky, 2008;Chambers and Jurafsky, 2009) or that generatesa story using the selected events (McIntyre andLapata, 2009; McIntyre and Lapata, 2010).In this article, we analyze and compare tech-niques for constructing models that, given a partialchain of events, predict other events that belong tothe script.
In particular, we consider the followingquestions:?
How should representative chains of eventsbe selected from the source text??
Given an event chain, how should statisticsbe gathered from it??
Given event n-gram statistics, which rankingfunction best predicts the events for a script?In the process of answering these questions, thisarticle makes several contributions to the field ofscript and narrative event chain understanding:?
We explore for the first time the use of skip-grams for collecting narrative event statistics,and show that this approach performs betterthan classic n-gram statistics.?
We propose a new method for ranking eventsgiven a partial script, and show that it per-forms substantially better than ranking meth-ods from prior work.?
We propose a new evaluation procedure (us-ing Recall@N) for the cloze test, and advo-cate its usage instead of average rank usedpreviously in the literature.?
We provide a systematic analysis of the in-teractions between the choices made whenconstructing an event prediction model.336Section 2 gives an overview of the prior workrelated to this task.
Section 3 lists and briefly de-scribes different approaches that try to provideanswers to the three questions posed in this intro-duction, while Section 4 presents the results of ourexperiments and reports on our findings.
Finally,Section 5 provides a conclusive discussion alongwith ideas for future work.2 Prior WorkOur work is primarily inspired by the work ofChambers and Jurafsky, which combined a depen-dency parser with coreference resolution to col-lect event script statistics and predict script events(Chambers and Jurafsky, 2008; Chambers and Ju-rafsky, 2009).
For each document in their trainingcorpus, they used coreference resolution to iden-tify all the entities, and a dependency parser toidentify all verbs that had an entity as either a sub-ject or object.
They defined an event as a verb plusa dependency type (either subject or object), andcollected for each entity, the chain of events thatit participated in.
They then calculated pointwisemutual information (PMI) statistics over all thepairs of events that occurred in the event chains intheir corpus.
To predict a new script event givena partial chain of events, they selected the eventwith the highest sum of PMIs with all the eventsin the partial chain.The work of McIntyre and Lapata followed inthis same paradigm, (McIntyre and Lapata, 2009;McIntyre and Lapata, 2010), collecting chains ofevents by looking at entities and the sequence ofverbs for which they were a subject or object.
Theyalso calculated statistics over the collected eventchains, though they considered both event bigramand event trigram counts.
Rather than predictingan event for a script however, they used these sim-ple counts to predict the next event that should begenerated for a children?s story.Manshadi and colleagues were concerned aboutthe scalability of running parsers and coreferenceover a large collection of story blogs, and so useda simplified version of event chains ?
just the mainverb of each sentence (Manshadi et al 2008).Rather than rely on an ad-hoc summation of PMIs,they apply language modeling techniques (specifi-cally, a smoothed 5-gram model) over the sequenceof events in the collected chains.
However, theyonly tested these language models on sequencingtasks (e.g.
is the real sequence better than a ran-dom sequence?)
rather than on prediction tasks(e.g.
which event should follow these events?
).In the current article, we attempt to shed somelight on these previous works by comparing differ-ent ways of collecting and using event chains.3 MethodsModels that predict script events typically havethree stages.
First, a large corpus is processed tofind event chains in each of the documents.
Next,statistics over these event chains are gathered andstored.
Finally, the gathered statistics are used tocreate a model that takes as input a partial scriptand produces as output a ranked list of events forthat script.
The following sections give more de-tails about each of these stages and identify thedecisions that must be made in each step, and anoverview of the whole process with an examplesource text is displayed in Figure 1.3.1 Identifying Event ChainsEvent chains are typically defined as a sequenceof actions performed by some actor.
Formally, anevent chain C for some actor a, is a partially or-dered set of events (v, d) where each v is a verbthat has the actor a as its dependency d. Followingprior work (Chambers and Jurafsky, 2008; Cham-bers and Jurafsky, 2009; McIntyre and Lapata,2009; McIntyre and Lapata, 2010), these eventchains are identified by running a coreference sys-tem and a dependency parser.
Then for each en-tity identified by the coreference system, all verbsthat have a mention of that entity as one of theirdependencies are collected1.
The event chain isthen the sequence of (verb, dependency-type) tu-ples.
For example, given the sentence A Crowwas sitting on a branch of a tree when a Fox ob-served her, the event chain for the Crow would be(sitting, SUBJECT), (observed, OBJECT).Once event chains have been identified, the mostappropriate event chains for training the modelmust be selected.
The goal of this process is toselect the subset of the event chains identified bythe coreference system and the dependency parserthat look to be the most reliable.
Both the coref-erence system and the dependency parser makesome errors, so not all event chains are necessarilyuseful for training a model.
The three strategieswe consider for this selection process are:1Also following prior work, we consider only the depen-dencies subject and object.337John woke up.
He opened his eyes and yawned.
Then he crossed the room and walked to the door.There he saw Mary.
Mary smiled and kissed him.
Then they both blushed.JOHN(woke, SUBJ)(opened, SUBJ)(yawned, SUBJ)(crossed, SUBJ)(walked, SUBJ)(saw, SUBJ)(kissed, OBJ)(blushed, SUBJ) MARY(saw, OBJ)(smiled, SUBJ)(kissed, SUBJ)(blushed, SUBJ)all chains, long chains,the longest chain all chains 1.
Identifying event chains... [(saw, OBJ), (smiled, SUBJ)][(smiled, SUBJ), (kissed, SUBJ)][(kissed, SUBJ), (blushed, SUBJ)] [(saw, OBJ), (smiled, SUBJ)][(saw, OBJ), (kissed, SUBJ)][(smiled, SUBJ), (kissed, SUBJ)][(smiled, SUBJ), (blushed, SUBJ)][(kissed, SUBJ), (blushed, SUBJ)] [(saw, OBJ), (smiled, SUBJ)][(saw, OBJ), (kissed, SUBJ)][(saw, OBJ), (blushed, SUBJ)]...[(kissed, SUBJ), (blushed, SUBJ)]regular bigrams 2-skip bigrams1-skip bigrams 2.
Gathering event chain statistics(saw, OBJ)(smiled, SUBJ)(kissed, SUBJ)_________ (missing event)constructing a partial script (cloze test)1.
(looked, OBJ)2.
(gave, SUBJ)3.
(saw, SUBJ)... 1.
(kissed, OBJ)2.
(looked, OBJ)3.
(waited, SUBJ)... 1.
(blushed, SUBJ)2.
(kissed, OBJ)3.
(smiled, SUBJ)C&J PMIOrdered PMIBigram prob.
3.
Predicting script eventsFigure 1: An overview of the whole linear work flow showing the three key steps ?
identifying event chains,collecting statistics out of the chains and predicting a missing event in a script.
The figure also displays how apartial script for evaluation (Section 4.3) is constructed.
We show the whole process for Mary?s event chain only,but the same steps are followed for John?s event chain.?
Select all event chains, that is, all sequencesof two or more events linked by commonactors.
This strategy will produce the largestnumber of event chains to train a model from,but it may produce noisier training data asthe very short chains included by this strategymay be less likely to represent real scripts.?
Select all long event chains consisting of 5or more events.
This strategy will produce asmaller number of event chains, but as theyare longer, they may be more likely to repre-sent scripts.?
Select only the longest event chain.
Thisstrategy will produce the smallest number ofevent chains from a corpus.
However, theymay be of higher quality, since this strategylooks for the key actor in each story, and onlyuses the events that are tied together by thatkey actor.
Since this is the single actor thatplayed the largest role in the story, its actionsmay be the most likely to represent a realscript.3.2 Gathering Event Chain StatisticsOnce event chains have been collected from thecorpus, the statistics necessary for constructingthe event prediction model must be gathered.
Fol-lowing prior work (Chambers and Jurafsky, 2008;Chambers and Jurafsky, 2009; Manshadi et al2008; McIntyre and Lapata, 2009; McIntyre andLapata, 2010), we focus on gathering statisticsabout the n-grams of events that occur in thecollected event chains.
Specifically, we look atstrategies for collecting bigram statistics, the mostcommon type of statistics gathered in prior work.We consider three strategies for collecting bigramstatistics:?
Regular bigrams.
We find all pairs ofevents that are adjacent in an event chainand collect the number of times each eventpair was observed.
For example, given thechain of events (saw, SUBJ), (kissed, OBJ),(blushed, SUBJ), we would extract the twoevent bigrams: ((saw, SUBJ), (kissed, OBJ))338and ((kissed, OBJ), (blushed, SUBJ)).
In addi-tion to the event pair counts, we also collectthe number of times each event was observedindividually, to allow for various conditionalprobability calculations.
This strategy fol-lows the classic approach for most languagemodels.?
1-skip bigrams.
We collect pairs of eventsthat occur with 0 or 1 events intervening be-tween them.
For example, given the chain(saw, SUBJ), (kissed, OBJ), (blushed, SUBJ),we would extract three bigrams: the two regu-lar bigrams ((saw, SUBJ), (kissed, OBJ)) and((kissed, OBJ), (blushed, SUBJ)), plus the 1-skip-bigram, ((saw, SUBJ), (blushed, SUBJ)).This approach to collecting n-gram statisticsis sometimes called skip-gram modeling, andit can reduce data sparsity by extracting moreevent pairs per chain (Guthrie et al 2006).It has not previously been applied in the taskof predicting script events, but it may bequite appropriate to this task because in mostscripts it is possible to skip some events inthe sequence.?
2-skip bigrams.
We collect pairs of eventsthat occur with 0, 1 or 2 intervening events,similar to what was done in the 1-skip bi-grams strategy.
This will extract even morepairs of events from each chain, but it is pos-sible the statistics over these pairs of eventswill be noisier.3.3 Predicting Script EventsOnce statistics over event chains have been col-lected, it is possible to construct the model forpredicting script events.
The input of this modelwill be a partial script c of n events, where c =c1c2 .
.
.
cn = (v1, d1), (v2, d2), .
.
.
, (vn, dn), andthe output of this model will be a ranked list ofevents where the highest ranked events are the onesmost likely to belong to the event sequence in thescript.
Thus, the key issue for this model is to de-fine the function f for ranking events.
We considerthree such ranking functions:?
Chambers & Jurafsky PMI.
Chambers andJurafsky (2008) define their event rankingfunction based on pointwise mutual infor-mation.
Given a partial script c as definedabove, they consider each event e = (v?, d?
)collected from their corpus, and score it asthe sum of the pointwise mutual informationsbetween the event e and each of the events inthe script:f(e, c) =n?ilogP (ci, e)P (ci)P (e)Chambers and Jurafsky?s description of thisscore suggests that it is unordered, such thatP (a, b) = P (b, a).
Thus the probabilitiesmust be defined as:P (e1, e2) =C(e1, e2) + C(e2, e1)?ei?ejC(ei, ej)P (e) =C(e)?e?
C(e?
)where C(e1, e2) is the number of times thatthe ordered event pair (e1, e2) was counted inthe training data, and C(e) is the number oftimes that the event e was counted.?
Ordered PMI.
A variation on the approachof Chambers and Jurafsky is to have a scorethat takes the order of the events in the chaininto account.
In this scenario, we assume thatin addition to the partial script of events, weare given an insertion point, m, where thenew event should be added.
The score is thendefined as:f(e, c) =m?k=1logP (ck, e)P (ck)P (e)+n?k=m+1logP (e, ck)P (e)P (ck)where the probabilities are defined as:P (e1, e2) =C(e1, e2)?ei?ejC(ei, ej)P (e) =C(e)?e?
C(e?
)This approach uses pointwise mutual infor-mation but also models the event chain in theorder it was observed.?
Bigram probabilities.
Finally, a naturalranking function, which has not been appliedto the script event prediction task (but has339been applied to related tasks (Manshadi etal., 2008)) is to use the bigram probabilitiesof language modeling rather than pointwisemutual information scores.
Again, given aninsertion point m for the event in the script,we define the score as:f(e, c) =m?k=1logP (e|ck) +n?k=m+1logP (ck|e)where the conditional probability is definedas2:P (e1|e2) =C(e1, e2)C(e2)This approach scores an event based on theprobability that it was observed following allthe events before it in the chain and precedingall the events after it in the chain.
This ap-proach most directly models the event chainin the order it was observed.4 ExperimentsOur experiments aimed to answer three questions:Which event chains are worth keeping?
Howshould event bigram counts be collected?
Andwhich ranking method is best for predicting scriptevents?
To answer these questions we use twocorpora, the Reuters Corpus and the Andrew LangFairy Tale Corpus, to evaluate our three differ-ent chain selection methods, {all chains, longchains, the longest chain}, our three different bi-gram counting methods, {regular bigrams, 1-skipbigrams, 2-skip bigrams}, and our three differentranking methods, {Chambers & Jurafsky PMI, or-dered PMI, bigram probabilities}.4.1 CorporaWe consider two corpora for evaluation:?
Reuters Corpus, Volume 1 3 (Lewis etal., 2004) ?
a large collection of 806, 791news stories written in English concerninga number of different topics such as politics,2Note that predicted bigram probabilities are calculatedin this way for both classic language modeling and skip-grammodeling.
In skip-gram modeling, skips in the n-grams areonly used to increase the size of the training data; predictionis performed exactly as in classic language modeling.3http://trec.nist.gov/data/reuters/reuters.htmleconomics, sports, etc., strongly varying inlength, topics and narrative structure.?
Andrew Lang Fairy Tale Corpus 4 ?
asmall collection of 437 children stories withan average length of 125 sentences, and usedpreviously for story generation by McIntyreand Lapata (2009).In general, the Reuters Corpus is much larger andallows us to see how well script events can bepredicted when a lot of data is available, while theAndrew Lang Fairy Tale Corpus is much smaller,but has a more straightforward narrative structurethat may make identifying scripts simpler.4.2 Corpus ProcessingConstructing a model for predicting script eventsrequires a corpus that has been parsed with a de-pendency parser, and whose entities have beenidentified via a coreference system.
We there-fore processed our corpora by (1) filtering outnon-narrative articles, (2) applying a dependencyparser, (3) applying a coreference resolution sys-tem and (4) identifying event chains via entitiesand dependencies.First, articles that had no narrative content wereremoved from the corpora.
In the Reuters Corpus,we removed all files solely listing stock exchangevalues, interest rates, etc., as well as all articlesthat were simply summaries of headlines from dif-ferent countries or cities.
After removing thesefiles, the Reuters corpus was reduced to 788, 245files.
Removing files from the Fairy Tale corpuswas not necessary ?
all 437 stories were retained.We then applied the Stanford Parser (Klein andManning, 2003) to identify the dependency struc-ture of each sentence in each article in the corpus.This parser produces a constitutent-based syntacticparse tree for each sentence, and then converts thistree to a collapsed dependency structure via a setof tree patterns.Next we applied the OpenNLP coreference en-gine5 to identify the entities in each article, and thenoun phrases that were mentions of each entity.Finally, to identify the event chains, we tookeach of the entities proposed by the coreferencesystem, walked through each of the noun phrasesassociated with that entity, retrieved any subject4http://www.mythfolklore.net/andrewlang/5http://incubator.apache.org/opennlp/340or object dependencies that linked a verb to thatnoun phrase, and created an event chain from thesequence of (verb, dependency-type) tuples in theorder that they appeared in the text.4.3 Evaluation MetricsWe follow the approach of Chambers and Jurafsky(2008), evaluating our models for predicting scriptevents in a narrative cloze task.
The narrativecloze task is inspired by the classic psychologicalcloze task in which subjects are given a sentencewith a word missing and asked to fill in the blank(Taylor, 1953).
Similarly, in the narrative clozetask, the system is given a sequence of events froma script where one event is missing, and askedto predict the missing event.
The difficulty of acloze task depends a lot on the context aroundthe missing item ?
in some cases it may be quitepredictable, but in many cases there is no singlecorrect answer, though some answers are moreprobable than others.
Thus, performing well on acloze task is more about ranking the missing eventhighly, and not about proposing a single ?correct?event.In this way, narrative cloze is like perplexityin a language model.
However, where perplexitymeasures how good the model is at predicting ascript event given the previous events in the script,narrative cloze measures how good the model isat predicting what is missing between events inthe script.
Thus narrative cloze is somewhat moreappropriate to our task, and at the same time sim-plifies comparisons to prior work.Rather than manually constructing a set ofscripts on which to run the cloze test, we followChambers and Jurafsky in reserving a section ofour parsed corpora for testing, and then using theevent chains from that section as the scripts forwhich the system must predict events.
Given anevent chain of length n, we run n cloze tests, witha different one of the n events removed each timeto create a partial script from the remaining n?
1events (see Figure 1).
Given a partial script asinput, an accurate event prediction model shouldrank the missing event highly in the guess list thatit generates as output.We consider two approaches to evaluating theguess lists produced in response to narrative clozetests.
Both are defined in terms of a test collectionC, consisting of |C| partial scripts, where for eachpartial script c with missing event e, ranksys(c) isthe rank of e in the system?s guess list for c.?
Average rank.
The average rank of the miss-ing event across all of the partial scripts:1|C|?c?Cranksys(c)This is the evaluation metric used by Cham-bers and Jurafsky (2008).?
Recall@N. The fraction of partial scriptswhere the missing event is ranked N or less6in the guess list.1|C||{c : c ?
C ?
ranksys(c) ?
N}|In our experiments we use N = 50, but re-sults are roughly similar for lower and highervalues of N .Recall@N has not been used before for evaluat-ing models that predict script events, however wesuggest that it is a more reliable metric than Av-erage rank.
When calculating the average rank,the length of the guess lists will have a significantinfluence on results.
For instance, if a small modelis trained with only a small vocabulary of events,its guess lists will usually be shorter than a largermodel, but if both models predict the missing eventat the bottom of the list, the larger model will getpenalized more.
Recall@N does not have this is-sue ?
it is not influenced by length of the guesslists.An alternative evaluation metric would havebeen mean average precision (MAP), a metriccommonly used to evaluate information retrieval.Mean average precision reduces to mean recipro-cal rank (MRR) when there?s only a single answeras in the case of narrative cloze, and would havescored the ranked lists as:1|C|?c?C1ranksys(c)Note that mean reciprocal rank has the same issueswith guess list length that average rank does.
Thus,since it does not aid us in comparing to prior work,and it has the same deficiencies as average rank,we do not report MRR in this article.6Rank 1 is the event that the system predicts is most prob-able, so we want the missing event to have the smallest rankpossible.3412-skip + bigram prob.Chain selection Av.
rank Recall@50all chains 502 0.5179long chains 549 0.4951the longest chain 546 0.4984Table 1: Chain selection methods for the Reuters corpus- comparison of average ranks and Recall@50.2-skip + bigram prob.Chain selection Av.
rank Recall@50all chains 1650 0.3376long chains 452 0.3461the longest chain 1534 0.3376Table 2: Chain selection methods for the Fairy Talecorpus - comparison of average ranks and Recall@50.4.4 ResultsWe considered all 27 combinations of our chainselection methods, bigram counting methods, andranking methods: {all chains, long chains, thelongest chain}x{regular bigrams, 1-skip bigrams,2-skip bigrams}x{Chambers & Jurafsky PMI, or-dered PMI, bigram probabilities}.
The best amongthese 27 combinations for the Reuters corpus was{all chains}x{2-skip bigrams}x{bigram probabil-ities} achieving an average rank of 502 and a Re-call@50 of 0.5179.Since viewing all the combinations at oncewould be confusing, instead the following sec-tions investigate each decision (selection, counting,ranking) one at a time.
While one decision is var-ied across its three choices, the other decisions areheld to their values in the best model above.4.4.1 Identifying Event ChainsWe first try to answer the question: How shouldrepresentative chains of events be selected fromthe source text?
Tables 1 and 2 show perfor-mance when we vary the strategy for selectingevent chains, while fixing the counting method to2-skip bigrams, and fixing the ranking method tobigram probabilities.For the Reuters collection, we see that using allchains gives a lower average rank and a higherRecall@50 than either of the strategies that selecta subset of the event chains.
The explanation isprobably simple: using all chains produces morethan 700,000 bigrams from the Reuters corpus,while using only the long chains produces onlyaround 300,000.
So more data is better data forall chains + bigram prob.Bigram selection Av.
rank Recall@50regular bigrams 789 0.48861-skip bigrams 630 0.49512-skip bigrams 502 0.5179Table 3: Event bigram selection methods for theReuters corpus - comparison of average ranks and Re-call@50.all chains + bigram prob.Bigram selection Av.
rank Recall@50regular bigrams 2363 0.32271-skip bigrams 1690 0.34182-skip bigrams 1650 0.3376Table 4: Event bigram selection methods for the FairyTales corpus - comparison of average ranks and Re-call@50.predicting script events.For the Fairy Tale collection, long chains givesthe lowest average rank and highest Recall@50.
Inthis collection, there is apparently some benefit tofiltering the shorter event chains, probably becausethe collection is small enough that the noise in-troduced from dependency and coreference errorsplays a larger role.4.4.2 Gathering Event Chain StatisticsWe next try to answer the question: Given anevent chain, how should statistics be gathered fromit?
Tables 3 and 4 show performance when we varythe strategy for counting event pairs, while fixingthe selecting method to all chains, and fixing theranking method to bigram probabilities.For the Reuters corpus, 2-skip bigrams achievesthe lowest average rank and the highest Recall@50.For the Fairy Tale corpus, 1-skip bigrams and 2-skip bigrams perform similarly, and both havelower average rank and higher Recall@50 thanregular bigrams.Skip-grams probably outperform regular n-grams on both of these corpora because the skip-grams provide many more event pairs over whichto calculate statistics: in the Reuters corpus, regu-lar bigrams extracts 737,103 bigrams, while 2-skipbigrams extracts 1,201,185 bigrams.
Though skip-grams have not been applied to predicting scriptevents before, it seems that they are a good fit,and better capture statistics about narrative eventchains than regular n-grams do.342all bigrams + 2-skipRanking method Av.
rank Recall@50C&J PMI 2052 0.1954ordered PMI 3584 0.1694bigram prob.
502 0.5179Table 5: Ranking methods for the Reuters corpus -comparison of average ranks and Recall@50.all bigrams + 2-skipRanking method Av.
rank Recall@50C&J PMI 1455 0.1975ordered PMI 2460 0.0467bigram prob.
1650 0.3376Table 6: Ranking methods for the Fairy Tale corpus -comparison of average ranks and Recall@50.4.4.3 Predicting Script EventsFinally, we try to answer the question: Givenevent n-gram statistics, which ranking functionbest predicts the events for a script?
Tables 5 and6 show performance when we vary the strategy forranking event predictions, while fixing the selec-tion method to all chains, and fixing the countingmethod to 2-skip bigrams.For both Reuters and the Fairy Tale corpus, Re-call@50 identifies bigram probabilities as the bestranking function by far.
On the Reuters corpusthe Chambers & Jurafsky PMI ranking methodachieves Recall@50 of only 0.1954, while bigramprobabilities ranking method achieves 0.5179.
Thegap is also quite large on the Fairy Tales corpus:0.1975 vs. 0.3376.On the Reuters corpus, average rank also identi-fies bigram probabilities as the best ranking func-tion, yet for the Fairy Tales corpus, Chambers &Jurafsky PMI and bigram probabilities have simi-lar average ranks.
This inconsistency is probablydue to the flaws in the average rank evaluationmeasure that were discussed in Section 4.3 ?
themeasure is overly sensitive to the length of theguess list, particularly when the missing event isranked lower, as it is likely to be when training ona smaller corpus like the Fairy Tales corpus.5 DiscussionOur experiments have led us to several importantconclusions.
First, we have introduced skip-gramsand proved their utility for acquiring script knowl-edge ?
our models that employ skip bigrams scoreconsistently higher on event prediction.
By follow-ing the intuition that events do not have to appearstrictly one after another to be closely semanticallyrelated, skip-grams decrease data sparsity and in-crease the size of the training data.Second, our novel bigram probabilities rankingfunction outperforms the other ranking methods.In particular, it outperforms the state-of-the-artpointwise mutual information method introducedby Chambers and Jurafsky (2008), and it does soby a large margin, more than doubling the Re-call@50 on the Reuters corpus.
The key insighthere is that, when modeling events in a script, alanguage-model-like approach better fits the taskthan a mutual information approach.Third, we have discussed why Recall@N is abetter and more consistent evaluation metric thanAverage rank.
However, both evaluation metricssuffer from the strictness of the narrative cloze test,which accepts only one event being the correctevent, while it is sometimes very difficult, evenfor humans, to predict the missing events, andsometimes more solutions are possible and equallycorrect.
In future research, our goal is to designa better evaluation framework which is more suit-able for this task, where credit can be given forproposed script events that are appropriate but notidentical to the ones observed in a text.Fourth, we have observed some differences inresults between the Reuters and the Fairy Talecorpora.
The results for Reuters are consistentlybetter (higher Recall@50, lower average rank), al-though fairy tales contain a plainer narrative struc-ture, which should be more appropriate to our task.This again leads us to the conclusion that moredata (even with more noise as in Reuters) leads toa greater coverage of events, better overall modelsand, consequently, to more accurate predictions.Still, the Reuters corpus seems to be far from aperfect corpus for research in the automatic acqui-sition of scripts, since only a small portion of thecorpus contains true narratives.
Future work musttherefore gather a large corpus of true narratives,like fairy tales and children?s stories, whose sim-ple plot structures should provide better learningmaterial, both for models predicting script events,and for related tasks like automatic storytelling(McIntyre and Lapata, 2009).One of the limitations of the work presentedhere is that it takes a fairly linear, n-gram-based ap-proach to characterizing story structure.
We thinksuch an approach is useful because it forms a natu-343ral baseline for the task (as it does in many othertasks such as named entity tagging and languagemodeling).
However, story structure is seldomstrictly linear, and future work should considermodels based on grammatical or discourse linksthat can capture the more complex nature of scriptevents and story structure.AcknowledgmentsWe would like to thank the anonymous reviewersfor their constructive comments.
This researchwas carried out as a master thesis in the frame-work of the TERENCE European project (EU FP7-257410).ReferencesNathanael Chambers and Dan Jurafsky.
2008.
Un-supervised learning of narrative event chains.
InProceedings of the 46th Annual Meeting of the As-sociation for Computational Linguistics: HumanLanguage Technologies, pages 789?797.Nathanael Chambers and Dan Jurafsky.
2009.
Un-supervised learning of narrative schemas and theirparticipants.
In Proceedings of the Joint Conferenceof the 47th Annual Meeting of the Association forComputational Linguistics and the 4th InternationalJoint Conference on Natural Language Processingof the AFNLP, pages 602?610.Nathanael Chambers and Dan Jurafsky.
2011.Template-based information extraction without thetemplates.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics:Human Language Technologies, pages 976?986.David Guthrie, Ben Allison, W. Liu, Louise Guthrie,and Yorick Wilks.
2006.
A closer look at skip-grammodelling.
In Proceedings of the Fifth internationalConference on Language Resources and Evaluation(LREC), pages 1222?1225.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting of the Association for Compu-tational Linguistics, pages 423?430.David D. Lewis, Yiming Yang, Tony G. Rose, and FanLi.
2004.
RCV1: a new benchmark collection fortext categorization research.
Journal of MachineLearning Research, 5:361?397.Mehdi Manshadi, Reid Swanson, and Andrew S. Gor-don.
2008.
Learning a probabilistic model of eventsequences from internet weblog stories.
In Proceed-ings of the Twenty-First International Florida Artifi-cial Intelligence Research Society Conference.Neil McIntyre and Mirella Lapata.
2009.
Learning totell tales: A data-driven approach to story genera-tion.
In Proceedings of the Joint Conference of the47th Annual Meeting of the Association for Compu-tational Linguistics and the 4th International JointConference on Natural Language Processing of theAFNLP, pages 217?225.Neil McIntyre and Mirella Lapata.
2010.
Plot induc-tion and evolutionary search for story generation.In Proceedings of the 48th Annual Meeting of theAssociation for Computational Linguistics, pages1562?1572.Michaela Regneri, Alexander Koller, and ManfredPinkal.
2010.
Learning script knowledge with webexperiments.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Lin-guistics, pages 979?988.Roger C. Schank and Robert P. Abelson.
1977.
Scripts,plans, goals, and understanding: an inquiry intohuman knowledge structures.
Lawrence ErlbaumAssociates.Wilson L. Taylor.
1953.
Cloze procedure: a new toolfor measuring readibility.
Journalism Quarterly,30:415?433.344
