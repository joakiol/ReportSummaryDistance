Exper iments with Tree-Structured MMIEncoders on the RM TaskMark T. Anikst, William S. Meisel, Matthew C. StaresSpeech Systems Incorporated18356 Oxnard StreetTarzana, California 91356Kai-Fu LeeCarnegie Mellon UniversityPittsburgh, Pennsylvania 15213ABSTRACTThis paper describes the tree-structured maximum utualinformation (MMI) encoders used in SSrs Phonetic Engine ?to perform large-vocabulary, continuous speech recognition.The MMI encoders are arranged into a two-stage cascade.
Ateach stage, the encoder is trained to maximize the mutualinformation between a set of phonetic targets andcorresponding codes.
After each stage, the codes arecompressed into segments.
This step expands acoustic-phonetic ontext and reduces ubsequent computation.
Weevaluated these MMI encoders by comparing them against astandard minimum distortion (MD) vector quantizer(encoder).
Both encoders produced code streams, which wereused to train speaker-independent discrete hidden Markovmodels in a simplified version of the Sphinx system \[3\].
Weused data from the DARPA Resource Management (RM)task.
The two-stage cascade of MMI encoders ignificantlyoutperforms the standard MD encoder in both speed andaccuracy.INTRODUCTIONMost hidden Markov model systems use minimumdistortion (MD) vector quantizers (encoders) to convertcontinuously valued speech parameters into streams ofinteger codes.
However, MD encoders do not optimize acriterion that is directly related to recognition accuracy.Moreover, they use a single distortion measure that may notbe appropriate for all speech classes.
In this paper, wepropose the use of maximum mutual information (MMI)encoders that are trained to extract phonetic information andthereby minimize phonetic recognition errors.
We furthercompress the frames into larger segments and repeat heencoding.Our MMI encoders are binary decision trees built tomaximize the average mutual information between thephonetic targets and the codes assigned to them.
The task oftraining such encoders has been extensively addressed in thetheory of binary decision trees \[5, 8, 2\].
For example,Breiman et al systematically consider binary decision treesapplied to various classification tasks.
The decision (interior)nodes of the tree are allowed to use linear combinations offeature vectors, as well as unordered categorical features.Training criteria ("impurity" criteria) for the binary decisiontrees include the average leaf-node-conditional cl ss entropy.Training is performed in a top-down ode-at-a-time fashion,adding new leaf nodes and maximizing reduction in theaverage leaf node impurity attained by such additions.
It isdemonstrated on many practical classification problems thatthe above procedure results in a suboptimal, but sufficientlyaccurate tree.Labelled data necessary for the supervised training isobtained by aligning speech frames with phonetictranscriptions using dynamic programming.
We train a two-stage cascade of binary-tree ncoders.
In the first stage,frames are encoded to extract maximum information abouttheir target label classes.
Feature vectors used in the treeencoder are frame-based.
Contiguous runs of frames with thesame code are compressed into segments.
In the second stage,the resulting segments are encoded to extract maximuminformation about their target label classes (we assign asingle target label class per segment).
Segment-basedacoustic feature vectors are used in the second-stage treeencoder, along with some categorical features based on thephonetic identities uncovered by the first-stage tree encoder.Segment duration features are also used.
Resulting runs ofsegments with the same code are again compressed intolarger segments.Speech Systems Incorporated (SSI) has been using aversion of this two-stage cascade of the MMI encoders in thePhonetic Engine ?, an integral part of SSI's large-vocabulary, continuous speech recognition system \[6, 1\].The two-stage trees are very fast; they encode one second ofspeech in one-third of a second on a 16 mHz 68020microprocessor.
In this study, we apply these MMI encodersin a more limited sense -- as vector quantizers for the Sphinxspeech recognition system \[3\].
This enables a directcomparison of MMI encoders and standard MD encoders.
Inour experiments, for the sake of expediency, we used asimplified version of the Sphinx system limited to 48context-independent phonetic HMMs and 26 acoustic framefeatures.
The two-stage cascade of MMI encoders outperformsthe standard MD encoder: Word error ate drops by 33% andrecognition isperformed roughly 1.6 times faster.We also ran a preliminary evaluation of the MMI and MDencoders using the Sphinx 1100 context-dependent(generalized triphone) HMMs.
We used the same codeswithout re-growing the trees for context-dependent classtargets.
Error rate was reduced by more than half relative tono use of context.346SYSTEM OVERVIEWHere we briefly describe the system used in ourexperiments.
Figure 1 summarizes the encoding process andthe experiments performed.Acoustic ProcessingThe speech is sampled at 16 kHz and is converted into asequence of 10-msec frames of 26 acoustic parameters: 12cepstrum coefficients, 12 differenced cepstrum coefficients,power and differenced power \[3\].LabellingTraining of the tree-structured MMI encoders i performedusing labelled speech data.
The set of label classes used forlabelling contains 144 classes: there is a unique label classfor each of the three pdf's (roughly corresponding tobeginning, middle, and end) of each of the 48 Sphinxcontext-independent phones.
Labelled frame data for trainingis obtained via Viterbi alignment using the Sphinx system.First-Stage (Frame) MMI EncoderAt the first (frame-coding) stage, frames are encoded insuch a way as to convey maximum information about heirunderlying label class identities.
To perform frame ncoding,the frame time-sequence is scanned by a "sliding window"covenng W frames; in our experiments, we kept W = 1 (aconstraint imposed for the sake of a fair comparison betweenthe first-stage MMI encoder and the standard MD encoder;normally, we use a three-frame window).
A set of the 26acoustic parameters of a frame was used as a feature vectoraccessed by the window.
The tree frame encoder takes asinput this feature vector and outputs acode for the frame atthe center of the window.
The encoder is trained to maximizethe average mutual information between its code alphabet andthe alphabet comprised of the 144 target label classes.The resulting sequence of coded acoustic frames is furtherprocessed to form acoustic segments by merging time-contiguous blocks of frames with the same code.
Also, themost likely broad phonetic lass is assigned to each formedsegment.
The stream of the acoustic segments with theassigned segmentation classes constitutes the input to thesegment-coding stage.Second-Stage (Segment) MMI EncoderThe second (segment-coding) stage processing is similarto that of the frame-coding stage.
Namely, segments areencoded in such a way as to convey maximum informationabout heir underlying phonetic lasses.To perform segment encoding, the stream of segments iscanned by a sliding time window covenng three segments(W = 3).
A set of pre-defined feature vectors is extracted fromthe acoustic parameters of all the frames encountered in thesegments accessed by the window.
Also, the most-likelybroad phonetic lasses assigned after the first stage to each ofthe three segments in the window comprise additionalcategorical variables.
These variables provide phoneticfeatures complementing the acoustic features.
Segmentduration features are also computed.
The segment encoder treetakes as input these sets of features and outputs acode for thesegment in the center of the window.
The encoder is trainedto maximize the average mutual information between itscode alphabet and the alphabet comprised of the 144 targetlabel classes.
The target labels for segments were derivedfrom the labels of the constituent frames.To obtain a categorical feature for use in the tree based onthe phonetic lass of a segment, we combined 144 targetphonetic lasses into nine broad superclasses, and used themost likely superclass number for each code.
The selected setof broad phonetic superclasses i shown in Table 1 (in thestandard notation of the Sphinx phonetic system, \[3\]).Class Phones0 SIL1 S SH Z ZH TS JH CH2 WL3 VFTH4 TKPHTDPD KDG B DDHDX DD5 R ER6 N M NG7 AH AE AA AY AO OW OY AW8 EH EY IH IY AX IX Y UH UWTable 1: Superclasses u ed in a categorical variable for thesecond-stage tr e.The resulting sequence of coded segments i furtherprocessed to form larger segments as in the first stage.
Thestream of the enlarged segments with the assigned codesconstitutes the output of the second stage.MMI TrainingThe first- and second-stage MMI encoders are trainedusing labelled ata (supervised training).
The encoders aretrained as binary decision trees using maximization of theaverage mutual information I(classes, codes) between the setof target label classes and the set of leaf-node numbers(codes), as the training criterion:I( classes, codes) =E Pr(class,code)*log(Pr(class,code)/(Pr(class)*Pr(code))),class codewhere Pr(class,code) is the joint probability of the class andthe code assigned to a training sample, Pr(class) and Pr(code)are the marginal class and code probabilities, respectively.Training is performed top-down, starting from the root ofthe binary decision tree.
The decision function associatedwith each decision ode of the tree effects the split of thefeature space with a hyperplane (for the continuous-valuedfeature vectors) or a dichotomy of a discrete set (for acategorical feature).
The training samples at each node werethose which reached that node after passing throughpredecessor nodes.Training of the decision function at a node uses as anoptimization criterion the reduction in the node's averageclass entropy.
To find a decision hyperplane, we useconjugate gradient based search \[9\] where the gradient of thecriterion function with respect to the hyperplane coefficientsis computed by replacing the "hard limiter" decision functionwith a piecewise linear one (the threshold-logic type) andgradually annealing that non-linearity to the hard limiter.347A Cepstrum, Differenced Cepstrum, coustlc Frames (10 ms) /Power ,  Differenced PowerI I I I I I I I~ .
.
.
Ixt Baseline (VQ codes)1st stage ,1 Single-frame context(frame) Tdecision tree t')/ -N  x Linear combinations ofencoder O ... ~O, features at each nodeou  to os~r (Terminal nodes of tree)1?1?14?121~!12 19191 ...
I ~" Frame CodesSegmentationI C?ncatenate Runs Ist-1 st St+l2nd s t a g e ~(segment) ?~decision tree / \ Linear combinations of featuresencoder ~/  ... \O or / -~  ... categorical features at each node/ \~) ~ ~l-1 N Output codes"" (Terminal nodes of tree)I~  Is l i i l  I 4~ I... ISegmentationI C?ncatenate Runs IExperimentla (256 x 3codebooks)lb (1024 codes)2a (1024 codes)2b (256 x 3codebooks)Segment features based onCepstrum, Differenced Cepstrum,Power, Differenced Power,Duration of segment, andmost likely phonetic lassThree-segment context(moving window)___.L.__.
Segment Codes?
7 .
.
~ .- 4(1024codes)5 5 5 7 7 7 4 4 ... ~ 3(1024codes,Segment-Coded Frames N=1023)Figure 1: Overview of the Multi-Stage Decision Tree(MMI) Encoder and Experiments.348Once the optimal coefficients are estimated, we use the hardlimiter decision function to send the patterns to the left orthe right child node.
We don't split a node if the highestreduction in the class entropy attained by the "node split" isless than a certain fraction of the node's class entropy; a finalnode is a terminal node.After the entire binary tree is created, its performancecriterion (i.e.
the average mutual information between the setof target classes and the set of the terminal nodes) isevaluated with a combination of the training and independentsets of labelled ata.
Some nodes are then removed, startingwith the current erminal nodes, i.e., the tree is "pruned," toproduce a more robust subtree with more accurate stimatesof the node-class probabilities.
The resulting terminal nodenumbers are used as codes.The above training and pruning of the trees was performedutilizing SSI tree-growing software.EXPERIMENTSWe compared various MMI tree encoders with the standardMD encoders (quantizers), as used in Sphinx and otherdiscrete HMM-based systems.
Both MMI and MD encodersproduce codes, which were used as input to the SphinxSystem \[3\].
For this study, a simplified version of theSphinx system was used.
Instead of context-dependentmodeling, we used only context-independent models.
Insteadof 51 features (as used in the latest version), we used only 26features (12 cepstrum coefficients, 12 differenced cepstrumcoefficients, power and differenced power).
Therefore, theresults hould be evaluated relatively rather than absolutely.We evaluated both a three-codebook version (256 codes percodebook) and a one-codebook version (1024 codes).
For theone-codebook version, we also used co-occurrence smoothingand deleted interpolation \[4\] to smooth rarely observed codes.We used the standard inventory of 48 phonetic models, eachwith 7 states and 3 output pdt~s.We also started a preliminary evaluation of the second-stage segment MMI codes for a version of the Sphinxsystem using context-dependent HMMs.
Results are given atthe end of this section.The task for our study is the DARPA ResourceManagement (RM) task, with the perplexity 60 word-pairgrammar \[7\].
We used the standard "extended training set" of3990 sentences from 109 speakers for speaker-independenttraining.
We trained the phonetic HMMs on all 3990sentences.
All results were evaluated on 300 independent testsentences from 12 speakers (the June 88 test set).
Followingthat, selected cases were evaluated on the RM2 June 90 testset as a verification.We first generated a first-stage MMI tree encoder (MMI-1024).
This tree was grown using 144 target phonetic lasses(48 phones x 3 distributions).
All 26 features were accessibleat all nodes to form linear decision boundaries (via linearcombination splits).
We used half of the training sentencesto grow the MMI tree encoder, and all of the trainingsentences to prune it.
This tree was grown to 1430 codes,and then pruned to 1024 codes.
The average code-classmutual information and corresponding error rate (substitution+ deletion + insertion) on the RM task (after the Forward-Backward training with co-occurrence smoothing and deletedinterpolation) are shown in in Table 2.To evaluate this result, we also generated an MD encoder(quantizer) that used the same 26 features, utilizing aweighted Euclidean distance (MD-1024) \[3\].
The results ofthis encoder (again, after the Forward-Backward training withco-occurrence smoothing and deleted interpolation) are shownin Table 2.
In this experiment, the MMI-1024 encoder errorrate was 3.5% lower than the MD-1024 encoder (a 15%reduction in error aate).Experiment Encoder Info (bits) ErrorNo.
I Rate(%)2a MMI-1024 3.42 out of 6.63 19.2lb MD-1024 3.16 out of 6.63 22.71 See Fig.
1.Table 2: Comparison of an MD encoder with an MMIframe stage encoder: a single codebook.Since the standard Sphinx system uses three separate VQcodebooks, we also compared the performance of a 3-codebook MD encoder and a 3-codebook MMI encoder.
Ineach case, the encoder has access only to a subset of thefeatures (VQ1 - 12 cepstrum coefficients, VQ2 - 12differenced cepstrum coefficients, and VQ3 - power &differenced power).
The codebook size was the same for allthe encoders (256 codes).
Co-occurrence smoothing of theoutput code pdfs was not performed in these experiments,but deleted interpolation was done.
The results (see Table 3)indicate that the MMI encoder gives slightly higher errorthan the MD encoder (despite higher information extracted),and both were worse than the MMI-1024 encoder.
Weconclude that effective tree encoders require access to theentire feature vector, so as to exploit the between-featurerelationships.Experiment Encoder  Info (bits) ErrorNo.
1 (VO1.
VO2.VO33 Rate(%)2b MMI 3-VQ 2.23, 1.77, 1.79 20.5out of 6.63la MD3-VQ 2.09, 1.51, 1.68 20.0out of 6.631 See Fig.
1.Table 3: Comparison of MD encoders with MMIframe-stage encoders: three codebooks.Next, we evaluated the second-stage MMI tree encoder.We used a three-segment sliding window to compute featuresderived from the 26 frame acoustic parameters, andcategorical features derived from the segment phoneticidentities discovered by the first-stage tree encoder.
Segmentduration features were also computed.The target labels for segments were derived from thelabels of the constituent frames.
Using those targets, wegrew a second-stage MMI tree encoder to 1417 codes (usingall of the training sentences) and then pruned it to 1024349codes.
The codes output by the encoder were furthercompressed by combining runs of segments with the samecodes into larger segments.We evaluated the second-stage codes in two ways: asframe codes (every constituent frame of a segment wasassigned the segment code, MMI-SF), and as segment codes(one code per segment, MMI-SS).
Respectively, we trainedtwo sets of the phonetic HMMs (standard 48 phoneticmodels of the SPHINX system) and ran recognition testsusing streams of frame and segment codes.
The code-classmutual information and corresponding error rates are shownin Table 4 (after the Forward-Backward training with co-occu~ence smoothing and deleted interpolation).Although MMI-SF extracts substantially moreinformation, the performance was slightly lower.
However,switching to segment codes (MMI-SS) resulted in aperformance improvement of 4.0% (21% reduction i  error)relative to the first stage alone.
Performance was improved7.5% (33% reduction) over the MD-1024 baseline (Table 2).Exper iment  Fdl.C..9.d.fJ: ln fo  (bits) ErrorNo .
Ratef%)2a MMI-1024 3.42 out of 6.63 19.24 MMI-SF 3.85 out of 6.63 19.83 MMI-SS 3.52 out of 6.73 15.2Table 4: MMI encoders: different temporal units.
(MMI-SF is segment codes on frames; MMI-SS issegment codes on segments.
)It was also found that MMI segment codes lead tosignificant frame compression (on the average, 1.6frames/segment) and therefore to significant speed advantages(which should be roughly proportional to the reduction insegments).
Table 5 illustrates this phenomenon.
Thus, therewas a simultaneous improvement in speed and accuracyusing an MMI segment encoder rather than an MD vectorquantizer.Table 5 displays the average number of temporal units(frames or segments) per target label class in the Per Targetcolumn.
The number of segments decreases with each stageof successive temporal compression.
In the finalsegmentation, the number of temporal units per target labelclass is reduced by a factor of 1.6.
We can measure whetherthe temporal compression loses target class segmentboundaries, by examining the percentages of the target labelclasses which were merged into groups of two or morewithin single segments (Merged Targets column); only 3.2%of such targets were merged by the final segmentation stage.Segmentat ion  ~ MergedTargetsbefore 1st stage 3.18 frames 0%after 1st stage 2.48 segments 1.6%after 2nd stage 2.01 segments 3.2%Table 5: Effect of segmentation.We conjecture that the observed improvement in therecognition accuracy for the segment codes versus framecodes is mainly due to the following.
First, the underlyingassumption of independence of the output code distributionsgiven a transition in a phonetic lass model (made for use ofthe hidden Markov models of phonetic lasses) is satisfied toa greater extent when the runs of frames with the same codeare merged in a single segment code, thus absorbing short-time dependencies.
Therefore, the HMMs become moreadequate models of the phonetic lasses.
Second, thereremains a sufficient amount of training data for the segmentcodes after the data is compressed due to segmentation.Finally, segmentation does not lead to any significantmerging of the target label classes within the resultingsegments, thereby retaining temporal resolution of phonetictargets.We also made a preliminary evaluation of the 2nd-stagesegment MMI codes for a version of the Sphinx systemusing context-dependent HMMs (1100 generalized triphonemodels for within- and between-word triphones).
The resultsare shown in Table 6.
Results for a comparable Sphinxconfiguration using 3 MD codebooks (using subsets of the26 features) is shown for comparison.
In both cases, co-occurrence smoothing was performed along with deletedinterpolation.
Although the word accuracy is close for bothcases, the decoding speedup for the segment codes gives theadvantage to the MMI encoder.
We view these results asrather encouraging, in view of the following limitations: (a)the encoder tree's topology was not utilized for pdfsmoothing, and (b) training of the MMI encoders was doneon the pdf labels of the 48 phones, and not on the generalizedtriphones.
Further investigation f the use of MMI encoderswith context-dependent HMMs will be conducted in thefuture.EncoderMMI-SS-Context DependentMD 3-VQ-Context DependentErrorRate7.17.0Table 6: Context-dependent decoding.Results for the RM2 (June 90) test set are shown in table 7.They show the same trend.Fdllf.O.dg.i: ErrorRatef%)MD-1024 19.5MMI-SS 15.6MMI-S S -Context Dependent 8.0Table 7: Results on June 90 RM test set.CONCLUSIONWe compared vector quantization (the MD encoder) withno segmentation to a multi-stage decision-tree ncoder (theMMI encoder) with and without segmentation.
We found thatthe MMI encoder (1) extracts asignificantly arger amount ofinformation than the MD encoder; (2) works better with a350combined feature set (as a single tree); and (3) yields higheraccuracy with faster decoding time when segment codes areused.In order to make a controlled comparison, either the bestdecision tree technology nor the best Markov modeltechnology was used.
In decision trees, we did not use widercontext in the frame tree, as in previous work \[1\].
Inaddition, we have found that a third segmentation stagehelps, creating even larger yet "clean" segments (unpublishedwork at SSI).
The decision tree can easily use more featuressimultaneously, providing the prospect of more informativecodes.
Since the trees make dichotomous decisions, moreextensive smoothing of the codes (utilizing tree topology)should help.
Further, several iterations of the entire processof labelling the frames and tree-growing can be repeated toimprove accuracy (as long as the resulting recognizerprovides more accurate decoding than that of the previousiteration).
Finally, due to temporal compression of framesand resulting data reduction, a reduced topology of thephonetic HMMs (e.g., fewer states/transitions) may yield abetter fit to the segment codes.
Future research will includetrying some of these variations.In our experiments, we have not fully explored the contextdependency of the phonetic models.
Further investigation ofthe use of MMI encoders with context-dependent HMMs willbe conducted in the future.REFERENCES1.
Anikst, M.T., Meisel, W.S., Newstadt, R.E,, Pirzadeh,S.S., Schumacher, J.E., Shinn, P., Soares, M.C., Trawick,D.T.
A Continuous Speech Recognizer Using Two-StageEncoder Neural Nets.
Proc.
International Joint Conference onNeural Networks, Washington D.C., pp.
I1-306 - II-309,January 1990.2.
Breiman, L., Friedman, J.H., Olshen, R.A. and Stone,C.J.
Classification and Regression Trees, WadsworthInternational Group, Belmont, Calif., 1984.3.
Lee, K.-F. Automatic Speech Recognition: TheDevelopment of the SPHINX System.
Kluwer AcademicPublishers, Boston, 1989.4.
Lee, K.F., Hon, H.W.
Speaker-Independent PhoneRecognition Using Hidden Markov Models, IEEETransactions on ASSP, November, 1989.5.
Meisel, W.S., Michalopoulos, D.A.
A PartitioningAlgorithm with Application in Pattern Classification,Piecewise-Constant Approximation, and the Optimization ofDecision Trees, IEEE Trans.
on Computers, January 1973.6.
Meisel, W.S., Fortunato, M.P., Michalek, W.D.
APhonetically-Based Speech Recognition System, SpeechTechnology, pp.
44-48, Apr/May 1989.7.
Pallett, D. S. Benchmark Tests for DARPA PerformanceEvaluations, Proc.
ICASSP 98, pp.
536-539, May 1989.g.
Payne, H.J., Meisel, W.S.
An Algorithm forConstructing Optimal Binary Decision Trees, IEEE Trans.on Computers, September 1977.9.
Press, W.H., Flannery, B.P.,Teukolsky, S.A.,Vetterling, W.T.
Numerical Recipes, Cambridge UniversityPress, 1986.351
