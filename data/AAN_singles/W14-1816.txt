Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 134?142,Baltimore, Maryland USA, June 26, 2014.c?2014 Association for Computational LinguisticsAutomated Scoring of Speaking Items in an Assessment for Teachers ofEnglish as a Foreign LanguageKlaus Zechner, Keelan Evanini, Su-Youn Yoon, Lawrence Davis,Xinhao Wang, Lei Chen, Chong Min Lee, Chee Wee LeongEducational Testing Service (ETS)Princeton, NJ 08541, USA{kzechner,kevanini,syoon,ldavis,xwang002,lchen,clee001,cleong}@ets.orgAbstractThis paper describes an end-to-end proto-type system for automated scoring of spo-ken responses in a novel assessment forteachers of English as a Foreign Languagewho are not native speakers of English.The 21 speaking items contained in the as-sessment elicit both restricted and moder-ately restricted responses, and their aim isto assess the essential speaking skills thatEnglish teachers need in order to be effec-tive communicators in their classrooms.Our system consists of a state-of-the-artautomatic speech recognizer; multiple fea-ture generation modules addressing di-verse aspects of speaking proficiency, suchas fluency, pronunciation, prosody, gram-matical accuracy, and content accuracy; afilter that identifies and flags problematicresponses; and linear regression modelsthat predict response scores based on sub-sets of the features.
The automated speechscoring system was trained and evaluatedon a data set involving about 1,400 testtakers, and achieved a speaker-level cor-relation (when scores for all 21 responsesof a speaker are aggregated) with humanexpert scores of 0.73.1 IntroductionAs English has become increasingly important as alanguage of international business, trade, science,and communication, efforts to promote teachingEnglish as a Foreign Language (EFL) have seensubstantially more emphasis in many non-English-speaking countries worldwide in recent years.
Inaddition, the prevailing trend in English pedagogyhas been to promote the use of spoken English inthe classroom, as opposed to the respective nativelanguages of the EFL learners.
However, due tothe high demand for EFL teachers in many coun-tries, the training of these teachers has not alwayscaught up with these high expectations, so there isa need for both governmental and private institu-tions involved in the employment and training ofEFL teachers to assess their competence in the En-glish language, as well as in English pedagogy.Against this background, we developed a lan-guage assessment for EFL teachers who are notnative speakers of English that addresses the fourbasic English language skills of Reading, Listen-ing, Writing and Speaking.
This paper focusesonly on the speaking portion of the English assess-ment, and, in particular, on the system that we de-veloped to automatically compute scores for testtakers?
spoken responses.Several significant challenges needed to be ad-dressed during the course of building this auto-mated speech scoring system, including, but notlimited to:?
The 21 Speaking items belong to 8 differ-ent task types with different characteristics;therefore, we had to select features and buildscoring models for each task type separately.?
The test takers speak a variety of native lan-guages, and thus have very different non-native accents in their spoken English.
Fur-thermore, the test takers also exhibit a widerange of speaking proficiency levels, whichcontributes to the diversity of their spoken re-sponses.
Our speech recognizer therefore hadto be trained and adapted to a large databaseof non-native speech.?
Since content accuracy is very important forthe types of tasks contained in the test, evensmall error rates by the automatic speechrecognition (ASR) system can lead to a no-ticeable impact on feature performance.
Thisfact motivated the development of a set of134features that are robust to speech recognitionerrors.?
A significant amount of responses (more than7%) exhibit issues that make them hard orimpossible to score automatically, e.g., highnoise levels, background speech, etc.
Wetherefore implemented a filter to identifythese non-scorable responses automatically.The paper is organized as follows: Section 2discusses related work; in Section 3, we presentthe data used for system training and evaluation;Section 4 describes the system architecture of theautomated speech scoring system.
We detail themethods we used to build our system in Section 5,followed by an overview of the results in Section6.
Section 7 discusses our findings; finally, Sec-tion 8 concludes the paper.2 Related WorkAutomated speech processing and scoring tech-nology has been applied to a variety of domainsover the course of the past two decades, includ-ing evaluation and tutoring of children?s literacyskills (Mostow et al., 1994), preparation for highstakes English proficiency tests for institutions ofhigher education (Zechner et al., 2009), evalua-tion of English skills of foreign-based call centeragents (Chandel et al., 2007), and evaluation ofaviation English (Pearson Education, Inc., 2011),to name a few (for a comprehensive overview, see(Eskenazi, 2009)).Most of these applications elicit restrictedspeech from the participants, and the most com-mon item type by far is the Read Aloud, in whichthe speaker reads a sentence or collection of sen-tences out loud.
Due to the constrained natureof this task, it is possible to develop ASR sys-tems that are relatively accurate, even with heav-ily accented non-native speech.
Several types offeatures related to a non-native speaker?s abilityto produce English sounds and speech patternseffectively have been extracted from these typesof responses.
Some of the best performing ofthese types of features include pronunciation fea-tures, such as a phone?s spectral match to na-tive speaker acoustic models (Witt, 1999) and aphone?s duration compared to native speaker mod-els (Neumeyer et al., 2000); fluency features, suchas the rate of speech, mean pause length, and num-ber of disfluencies (Cucchiarini et al., 2000); andprosody features, such as F0 and intensity slope(Hoenig, 2002).In addition to the large majority of applicationsthat elicit restricted speech, a small number of ap-plications have also investigated automated scor-ing of non-native spontaneous speech, in orderto more fully evaluate a speaker?s communicativecompetence (e.g., (Cucchiarini et al., 2002) and(Zechner et al., 2009)).
In these systems, the sametypes of pronunciation, fluency, and prosody fea-tures can be extracted; furthermore, features re-lated to additional aspects of a speaker?s profi-ciency in the non-native language can be extracted,such as vocabulary usage (Yoon et al., 2012), syn-tactic complexity (Bernstein et al., 2010a; Chenand Zechner, 2011), and topical content (Xie et al.,2012).As described in Section 1, the domain for theautomated speaking assessment investigated inthis study is teachers of EFL around the world.Based on the fact that many of the item types aredesigned to assess the test taker?s ability to pro-ductively use English constructions and linguis-tic units that commonly recur in English teach-ing environments, several of the item types elicitsemi-restricted speech (see Table 1 below for a de-scription of the different item types).
These typesof responses fall somewhere between the heavilyrestricted speech elicited by a Read Aloud taskand unconstrained spontaneous speech.
In thesesemi-restricted responses, the test taker may beprovided with a set of lexical items that shouldbe used to form a sentence; in addition, the testtaker is often asked to make the sentence conformto a given grammatical template.
Thus, the re-sponses provided for a given prompt of this typeby multiple different speakers will often overlapwith each other; however, it is not possible tospecify a complete list of all possible responses.These types of items have only infrequently beenexamined in the context of automated speech scor-ing.
Some related item types that have beenexplored previously include the Sentence Buildand Short item types described in (Bernstein etal., 2010b); however, those item types typicallyelicited a much narrower range of responses thanthe semi-restricted ones in this study.3 DataThe data used in this study was drawn from a pilotadministration of a language assessment for teach-135ers of English as a Foreign Language.
This testis designed to assess the ability of a non-nativeteacher of English to use English in classroom set-tings.
The language forms and functions includedin this test are based on the materials included in acurriculum that the test takers studied prior to tak-ing the assessment.
The assessment includes itemsthat cover the four language skills: Reading, Lis-tening, Writing, and Speaking.
There are a total of8 different types of Speaking items included in theassessment.
These can be divided into the follow-ing two categories, depending on how constrainedthe test taker?s response is:?
Restricted Speech: In these item types, allof the linguistic content expected in thetest taker?s response is presented in the testprompt, and the test taker is asked to read orrepeat it aloud.?
Semi-restricted Speech: In these item types, aportion of the linguistic content is presentedin the prompt, and the test taker is required toprovide the remaining content to formulate acomplete response.Sets of 7 Speaking items are presented to thetest taker in thematic units, called ?lessons?, basedon their instructional goals; in total, each test takercompleted three lessons, and thus responded to 21Speaking items.
Table 1 presents descriptions ofthe 8 different item types included in the assess-ment.The numbers of responses provided by the testtakers to each type (along with their respective re-sponse durations) are as follows: four MultipleChoice (10 seconds each), six Read Aloud (four 40second responses and two 60 second responses),two Repeat Aloud (15 seconds each), one Incom-plete Sentence (20 seconds), one Key Words (15seconds), five Chart (four 20 seconds and one 40seconds), one Keyword Chart (15 seconds), andone Visuals (15 seconds).
Thus, each test takerprovided a total of approximately 9 minutes of au-dio.The responses were all double-scored by trainedhuman raters on a three-point scale (1 - 3).
Forthe Restricted Speech items, the raters assessedthe test taker?s pronunciation, pacing, and intona-tion.
For the Semi-restricted Speech items, the re-sponses were also scored holistically on a 3-pointscale, but raters were also asked to take into ac-count the appropriateness of the language usedRestricted SpeechType DescriptionMultipleChoice(MC)The test taker selects the correctoption and reads it aloudRead Aloud(RA)The test taker reads aloud a setof classroom instructionsRepeatAloud (RP)The test taker listens to a studentutterance twice and then repeatsitSemi-restricted SpeechType DescriptionIncompleteSentence(IS)The test taker is given a sentencefragment and completes the sen-tence according to the instruc-tionsKey Words(KW)The test taker uses the key wordsprovided to speak a sentence asinstructedChart (CH) The test taker uses an examplefrom a language chart and thenformulates a similar sentence us-ing a given grammatical patternKeywordChart (KC)The test taker constructs a sen-tence using keywords providedand information in a chartVisuals (VI) The test taker is given two visu-als and is asked to give instruc-tions to students based on thegraphical informationTable 1: Types of speaking items included in theassessment(e.g., grammatical accuracy and content correct-ness) in addition to aspects of fluency and pronun-ciation.
For some responses, the raters were notable to provide a score on the 1 - 3 scale, e.g.,because the audio response contained no speechinput, the test taker responded in their native lan-guage, etc.
These responses are labeled NS forNon-Scoreable.After receiving scores, all of the responseswere transcribed using standard English orthogra-phy (disfluencies, such as filled pauses and par-tial words are also included in the transcriptions).Then, the responses were partitioned (with nospeaker overlap) into five sets for the training andevaluation of the ASR system and the linear re-gression scoring models.
The amount of data and136human score distributions in each of these parti-tions are displayed in Table 2.4 System ArchitectureThe automated scoring system used for the teach-ers?
spoken language assessment consists of thefollowing four components, which are invokedone after the other in a pipeline fashion (ETSSpeechRaterSM, (Zechner et al., 2009; Higgins etal., 2011)):?
an automated speech recognizer, generatingword hypotheses from input audio recordingsof the test takers?
responses?
a feature computation module that generatesfeatures based on the ASR output, e.g., mea-suring fluency, pronunciation, prosody, andcontent accuracy?
a filtering model that flags responses thatshould not be scored automatically due to is-sues with audio quality, empty responses, etc.?
linear regression scoring models that predictthe score for each response based on a set ofselected featuresFurthermore, we use Praat (Boersma andWeenick, 2012) to extract power and pitch fromthe speech signal; this information is used forsome of the feature computation modules, as wellas for the filtering model.The ASR is an HMM-based triphone systemtrained on approximately 800 hours of non-nativespeech from a different data set; a backgroundLanguage Model (LM) was also trained on thesame data set.
Subsequently, 8 adapted LMs weretrained (with an interpolation weight of 0.9 for thein-domain data) using the responses in the ASRTraining partition for the 8 different item typeslisted in Table 1.
The ASR system obtained anoverall word error rate (WER) of 13.0% on theASR Evaluation partition and 15.6% on the ModelEvaluation partition.
As would be expected, theASR system performed best on the responses thatwere most restricted by the test item and per-formed worse on the responses that were less re-stricted.
The WER ranged from 11.4% for theRA responses to 41.4% for the IS responses in theModel Evaluation partition.5 Methodology5.1 Speech featuresThe feature computation components of ourspeech scoring system compute more than 100features based on a speaker?s response.
They be-long to the following broad dimensions of speak-ing proficiency: fluency, pronunciation, prosody,vocabulary usage, grammatical complexity andaccuracy, and content accuracy (Zechner et al.,2009; Chen and Yoon, 2012; Chen et al., 2009;Zechner et al., 2011; Yoon et al., 2012; Yoon andBhat, 2012; Zechner and Wang, 2013).After initial feature generation, we selected a setof about 10 features for each of the 8 item types,based on the following considerations1(Zechneret al., 2009; Xi et al., 2008):?
empirical performance, i.e., feature correla-tion with human scores?
construct2relevance, i.e., to what extent thefeature measures aspects of speaking profi-ciency that are considered to be relevant andimportant by content experts?
overall construct coverage, i.e., the feature setshould include features from all relevant con-struct dimensions?
feature independence, i.e., the inter-correlation between any two features of theset should be lowFurthermore, some features were transformed(e.g., by applying the inverse or log function), inorder to increase the normality of their distribu-tions (an assumption of linear regression classi-fiers).
All feature values that exceeded a thresh-old of 4 standard deviations from the mean werereplaced by the respective threshold (outlier trun-cation).The composition of feature sets is slightly dif-ferent for the two item type categories: for the 3restricted item types, features related to fluency,pronunciation, prosody and read/repeat accuracywere chosen, whereas for the 5 semi-restricteditem types, vocabulary and grammar features werealso added to the set.
Further, while accuracy1While automated feature selection is conceivable in prin-ciple, in our experience it typically does not result in a featureset that meets all of these criteria well.2A construct is the set of knowledge, skills, and abilitiesmeasured by a test.137Partition Spk.
Resp.
Dur.
1 2 3 NSASR Training 773 16,049 116.7 1,587 (9.9) 4,086 (25.5) 8,796 (54.8) 1,580 (9.8)ASR Development 25 525 3.8 53 (10.1) 133 (25.3) 327 (62.3) 12 (2.3)ASR Evaluation 25 525 3.8 31 (5.9) 114 (21.7) 326 (62.1) 54 (10.3)Model Training 300 6,300 45.8 675 (10.7) 1,715 (27.2) 3,577 (56.8) 333 (5.3)Model Evaluation 300 6,300 45.7 647 (10.3) 1,637 (26.0) 3,487 (55.3) 529 (8.4)Total 1,423 29,699 215.8 2,993 (9.38) 7,685 (25.14) 16,513 (58.26) 2,508 (7.22)Table 2: Amount of data contained in each partition (speakers, responses, hours of speech) and distribu-tion of human scores (percentages of scores per partition in brackets).features for the restricted items were based onlyon string alignment measures, content accuracyfeatures for the semi-restricted items were morediverse, e.g., based on regular expressions, key-words, and language model scores (Zechner andWang, 2013).
Table 3 lists the features that wereused in the scoring models for restricted and semi-restricted item types, along with sub-constructsthey measure and their description.5.2 Filtering modelIn order to automatically identify responses thathave technical issues (e.g., loud background noise)or are otherwise not scorable (e.g., empty re-sponses), a decision tree-based filtering model wasdeveloped using a combination of features derivedfrom ASR output and from pitch and energy in-formation (Yoon et al., 2011; Jeon and Yoon,2012).
The filtering model was tested on the scor-ing model evaluation data, and obtained an ac-curacy rate (the exact agreement between the fil-tering model and a human rater concerning thedistinction between scorable and non-scorable re-sponses) of 97%; it correctly identified 90% of thenon-scorable responses in the data set with a falsepositive rate of 21% (recall=0.90, precision=0.79,F-score=0.84).5.3 Scoring modelsWe used the Model Training set to train 8 linearregression models for the 8 different item types,using the previously determined feature sets.
Weused the features as independent variables in thesemodels and the summed scores of two humanraters as the dependent variable.
These trainedscoring models were then employed to score re-sponses of the Model Evaluation data (exclud-ing responses marked as non-scorable by humanraters) and rounded to the nearest integer to predictthe final scores for each response.
These scoreswere then evaluated against the first human raterscore (H1).Item N S-H1 H1-H2 WER (%)RA 1653 0.34 0.51 11.4RP 543 0.41 0.73 21.8MC 1036 0.67 0.83 17.1CH 1372 0.44 0.67 26.3KW 275 0.45 0.67 28.7KC 274 0.57 0.74 28.8IS 260 0.46 0.69 41.4VI 272 0.43 0.80 30.4Table 4: Correlations between system and first hu-man rater (S-H1) and between two human raters(H1-H2), for all responses of each item type in theModel Evaluation partition (N).
The last columnprovides the average ASR word error rate (WER)in percent.Additionally, for responses flagged as non-scorable by the automatic filtering model, the sec-ond human rater score (H2) was used as finalitem score in order to mimic the operational sce-nario where human raters score responses that areflagged by the filtering model.We also compute the agreement between sys-tem and human raters based on a set of all 21 re-sponses of a speaker.
Score imputation was usedfor responses that were labeled as non-scorable byboth the system and H2; in this case, the responsewas given the mean score of the total scorableresponses from the same speaker.
Similarly, thesame score imputation rule was applied to the H1scores.6 ResultsTable 4 presents the Pearson correlation coeffi-cients between human and automated scores forthe responses from the 8 different item types alongwith the human-human correlation for each itemtype.
Furthermore, we also provide the word errorrates of the ASR system for the same 8 item typesin the last column of the table.138Feature Sub-construct DescriptionContent Ed1 Read/repeat accu-racy / FluencyCorrectly read words per minuteContent Ed2 Read/repeat accu-racyRead/repeat word error rateContent RegEx Content accuracy Matching of regular expressionsContent WER Content accuracy Response discrepancy from high scoring responsesContent NGram Content accuracy N-grams in response matching high scoring response n-gramsFluency Rate Fluency Speaking rateFluency Chunk Fluency Average length of contiguous word chunksFluency Sil1 Fluency Frequency of long silencesFluency Sil2 Fluency / Grammar Proportion of long within-clause-silences to all within-clause-silencesFluency Sil3 Fluency Mean length of silences within a clauseFluency Disfl1 Fluency Frequency of interruption points (repair, repetition, falsestart)Fluency Disfl2 Fluency Number of disfluencies per secondFluency Disfl3 Fluency Frequency of repetitionsPron Vowels Pronunciation Average vowel duration differences relative to a native-speaker modelProsody1 Prosody Percentage of stressed syllablesProsody2 Prosody Mean deviation of time intervals between stressed syllablesProsody3 Prosody Mean distance between stressed syllablesVocab1 Vocabulary / Flu-encyNumber of word types divided by utterance durationGrammar POS Grammar Part-of-speech based distributional similarity score be-tween a response and responses with different score levelsGrammar LM Grammar Global language model score (normalized by responselength)Table 3: List of features used for item type scoring models, with the sub-constructs they represent anddescriptions.139Comparison Pearson rS-H1 0.725S-H2 0.742H1-H2 0.934Table 5: Speaker-level performance (Pearson rcorrelations) computed over the sum of all 21scores from each speaker, N=272Sub-construct Restricted Semi-restrictedContent 0.33?0.67 0.34?0.61Fluency 0.19?0.33 0.20?0.33Pronunciation 0.20?0.22 0.13?0.31Prosody 0.18?0.24 0.12?0.27Grammar ?
0.23?0.49Vocabulary ?
0.21?0.32Table 6: Range of Pearson r correlations for dif-ferent features with human scores (H1) by sub-construct for restricted and semi-restricted itemtypes.Table 5 presents the Pearson correlation coeffi-cients between the speaker-level scores producedby the automated scoring system (S) and the twosets of human scores (H1 and H2).
These speaker-level scores were computed based on the sum ofall 21 scores from each speaker in the Model Eval-uation partition.
Responses that received a non-scorable rating from the human raters were im-puted, as described above.
Furthermore, 28 speak-ers were excluded from this analysis because theyhad more than 7 non-scorable responses each.3Finally, Table 6 provides an overview of Pear-son correlation ranges with human rater scores(H1) for the different features used in the scoringmodels, summarized by the sub-constructs that thefeatures represent.7 DiscussionWhen looking at Table 4, we see that the inter-raterreliability for human raters ranges between 0.51(for RA items) and 0.83 (for MC items).
Inter-rater reliability varies less for the 5 semi-restricteditem types (0.67?0.80), compared to the 3 re-stricted item types (0.51?0.83).
As for automatedscore correlations with human raters, the Pearsonr coefficients range from 0.34 (RA) to 0.67 (MC).3In an operational setting, these test takers would not re-ceive a test score; instead, they would have the opportunity totake the test again.Again, the variability of Pearson r coefficients islarger for the 3 restricted item types (0.34?0.67)than for the 5 semi-restricted item types (0.43?0.57).
The degradation in correlation between theinter-human results and the machine-human re-sults varies from 0.16 (MC) to 0.37 (VI).Speech recognition word error rate does notseem to have a strong influence on model perfor-mance (RA items have the lowest WER with S-H1 r=0.34, but r=0.46 for IS items that have thehighest WER).
However, we found other factorsthat affect model performance negatively; for ex-ample, multiple repeats of responses by test tak-ers contribute to the large performance differencebetween S-H1 and H1-H2 for the RP items.
Ingeneral, we conjecture that using features for alarger set of sub-construct areas?in the case ofsemi-restricted item types?may contribute to thelower variation of scoring model performance forthis subset of the data.As for speaker-level results (Table 5), the over-all degradation between the inter-human correla-tion and the system-human correlations is of asimilar magnitude (around 0.2) as observed formost of the individual item types.
Still, thespeaker-level correlation of 0.73 is 0.26 higherthan the average item type correlation between thesystem and H1.When we look into more detail at the Pearsonr correlations between individual features used inthe item type scoring models and human scores(Table 6), we can see that features related to con-tent accuracy exhibit a substantially stronger per-formance (r=0.33?0.67) than features related tomost other sub-constructs of speaking proficiency,namely fluency, pronunciation, prosody, and vo-cabulary (r ?
0.2).
One exception is featuresrelated to grammar, where correlations with hu-man scores are as high as 0.49.
Since related workon scoring speech using features indicative of flu-ency, pronunciation, etc.
showed higher correla-tions (e.g., (Cucchiarini et al., 1997; Franco et al.,2000; Zechner et al., 2009)), we conjecture thatthe reason behind this difference is likely to befound in the fact that the responses in this assess-ment for teachers of English are quite short (6?14 words on average for all items except for ReadAloud items that are about 46 words on average).Since content features are less reliant on longerstretches of speech, they still work fairly well formost items in our corpus.140Finally, while the proportion of words containedin responses in restricted items is much larger thanthose contained in responses in semi-restricteditems, these two item type categories are moreevenly distributed over the whole test, i.e., eachtest taker responds to 9 semi-restricted and 12 re-stricted items, and the item scores are then aggre-gated for a final score with equal weight given toeach item score.8 ConclusionThis paper presented an overview of an automatedspeech scoring system that was developed for alanguage assessment for teachers of English as aForeign Language (EFL) whose native languageis not English.
We described the main compo-nents of this prototype system and their perfor-mance: the ASR system, features generated fromASR output, a filtering model to flag non-scorableresponses, and finally a set of linear regressionmodels, one for each of 8 different types of testitems.We found that overall, the correlation betweenour speech scoring system?s predicted scores andhuman rater scores range between 0.34 and 0.67,evaluated on responses from 8 item types.
Further-more, we found that correlations based on com-plete sets of 21 spoken responses per test taker im-prove to around r = 0.73.Given the many significant challenges of thiswork, including 8 different item types in the as-sessment, responses from speakers from differentnative languages and speaking proficiency levels,sub-optimal audio conditions for a part of the data,and a relatively small data set for both ASR systemadaptation and linear regression model training,we find that the overall performance achieved byour automated speech scoring system was a goodstarting point for an eventual deployment in a low-stakes assessment context.Future work will aim at improving the perfor-mance of the prediction models by the addition ofmore features addressing different aspects of theconstruct as well as an improved filtering modelfor flagging the different types of problematic re-sponses.
Furthermore, agreement between humanraters, in particular for read-aloud items, could beimproved by refining rater rubrics and additionalrater training and monitoring.AcknowledgmentsThe authors would like to thank Anastassia Louk-ina and Jidong Tao for their comments on an ear-lier version of this paper, and are also indebtedto the anonymous reviewers of BEA-9 and ASRU2013 for their valuable comments and suggestions.ReferencesJared Bernstein, Jian Cheng, and Masanori Suzuki.2010a.
Fluency and structural complexity as pre-dictors of L2 oral proficiency.
In Proceedings of In-terspeech.Jared Bernstein, Alistair Van Moere, and Jian Cheng.2010b.
Validating automated speaking tests.
Lan-guage Testing, 27(3):355?377.Paul Boersma and David Weenick.
2012.
Praat: Doingphonetics by computer, version 5.3.32. http://www.praat.org.Abhishek Chandel, Abhinav Parate, Maymon Ma-dathingal, Himanshu Pant, Nitendra Rajput, ShajithIkbal, Om Deshmuck, and Ashish Verma.
2007.Sensei: Spoken language assessment for call cen-ter agents.
In Proceedings of the IEEE Workshop onAutomatic Speech Recognition and Understanding(ASRU).Lei Chen and Su-Youn Yoon.
2012.
Application ofstructural events detected on ASR outputs for auto-mated speaking assessment.
In Proceedings of In-terspeech.Miao Chen and Klaus Zechner.
2011.
Computingand evaluating syntactic complexity features for au-tomated scoring of spontaneous non-native speech.In Proceedings of the 49th Annual Meeting of the As-sociation for Computational Linguistics, pages 722?731.Lei Chen, Klaus Zechner, and Xiaoming Xi.
2009.
Im-proved pronunciation features for construct-drivenassessment of non-native spontaneous speech.
InProceedings of NAACL-HLT.Catia Cucchiarini, Helmer Strik, and Lou Boves.
1997.Automatic evaluation of Dutch pronunciation by us-ing speech recognition technology.
In Proceedingsof the IEEE Workshop on Auotmatic Speech Recog-nition and Understanding (ASRU).Catia Cucchiarini, Helmer Strik, and Lou Boves.
2000.Quantitative assessment of second language learn-ers?
fluency by means of automatic speech recogni-tion technology.
Journal of the Acoustical Society ofAmerica, 107(2):989?999.Catia Cucchiarini, Helmer Strik, and Lou Boves.
2002.Quantitative assessment of second language learn-ers?
fluency: Comparisons between read and spon-taneous speech.
Journal of the Acoustical Society ofAmerica, 111(6):2862?2873.141Maxine Eskenazi.
2009.
An overview of spoken lan-guage technology for education.
Speech Communi-cation, 51(10):832?844.Horacio Franco, Leonardo Neumeyer, Vassilios Di-galakis, and Orith Ronen.
2000.
Combination ofmachine scores for automatic grading of pronuncia-tion quality.
Speech Communication, 30(1-2):121?130.Derrick Higgins, Xiaoming Xi, Klaus Zechner, andDavid M. Williamson.
2011.
A three-stage ap-proach to the automated scoring of spontaneous spo-ken responses.
Computer Speech and Language,25(2):282?306.Florian Hoenig.
2002.
Automatic assessment of non-native prosody ?
Annotation, modelling, and evalu-ation.
In Proceedings of the International Sympo-sium on Automatic Detection of Errors in Pronun-ciation Training (ISADEPT), pages 21?30, Stock-holm, Sweden.Je Hun Jeon and Su-Youn Yoon.
2012.
Acousticfeature-based non-scorable response detection for anautomated speaking proficiency assessment.
In Pro-ceedings of Interspeech.Jack Mostow, Steven F. Roth, Alexander G. Haupt-mann, and Matthew Kane.
1994.
A prototype read-ing coach that listens.
In Proceedings of the TwelfthNational Conference on Artificial Intelligence.Leonardo Neumeyer, Horacio Franco, Vassilios Di-galakis, and Mitchel Weintraub.
2000.
Automaticscoring of pronunciation quality.
Speech Communi-cation, 30:83?93.Pearson Education, Inc. 2011.
VersantTMAviation English Test.
http://www.versanttest.com/technology/VersantAviationEnglishTestValidation.pdf.Silke Witt.
1999.
Use of speech recognition incomputer-assisted language learning.
Ph.D. thesis,Cambridge University.Xiaoming Xi, Derrick Higgins, Klaus Zechner, andDavid M. Williamson.
2008.
Automated scoring ofspontaneous speech using SpeechRater v1.0.
Edu-cational Testing Service Research Report RR-08-62.Shasha Xie, Keelan Evanini, and Klaus Zechner.
2012.Exploring content features for automated speechscoring.
In Proceedings of the 2012 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 103?111, Montr?eal, Canada.
Asso-ciation for Computational Linguistics.Su-Youn Yoon and Suma Bhat.
2012.
Assessment ofESL learners?
syntactic competence based on sim-ilarity measures.
In Proceedings of the 2012 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning, pages 600?608, Jeju Island, Korea.Association for Computational Linguistics.Su-Youn Yoon, Keelan Evanini, and Klaus Zechner.2011.
Non-scorable response detection for auto-mated speaking proficiency assessment.
In Proceed-ings of NAACL-HLT Workshop on Innovative Use ofNLP for Building Educational Applications.Su-Youn Yoon, Suma Bhat, and Klaus Zechner.
2012.Vocabulary profile as a measure of vocabulary so-phistication.
In Proceedings of the 7th Workshop onInnovative Use of NLP for Building Educational Ap-plications, NAACL-HLT, Montr?eal, Canada.
Associ-ation for Computational Linguistics.Klaus Zechner and Xinhao Wang.
2013.
Automatedcontent scoring of spoken responses in an assess-ment for teachers of english.
In Proceedings ofthe 8th Workshop on Innovative Use of NLP forBuilding Educational Applications, NAACL-HLT,Atlanta.
Association for Computational Linguistics.Klaus Zechner, Derrick Higgins, Xiaoming Xi, andDavid M. Williamson.
2009.
Automatic scoringof non-native spontaneous speech in tests of spokenEnglish.
Speech Communication, 51(10):883?895.Klaus Zechner, Xiaoming Xi, and Lei Chen.
2011.Evaluating prosodic features for automated scoringof non-native read speech.
In Proceedings of theIEEE Workshop on Automatic Speech Recognitionand Understanding (ASRU).142
