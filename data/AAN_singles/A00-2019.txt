An Unsupervised Method for Detecting Grammatical ErrorsMartin ChodorowHunter College of CUNY695 Park AvenueNew York, NYmartin.chodorow @.hunter.cuny.eduClaudia LeacockEducational Testing ServiceRosedale RoadPrinceton, NJcleacock@ets.orgAbstractWe present an unsupervised method fordetecting rammatical errors by inferringnegative evidence from edited textualcorpora.
The system was developed andtested using essay-length responses toprompts on the Test of English as aForeign Language (TOEFL).
The error-recognition system, ALEK, performs withabout 80% precision and 20% recall.IntroductionA good indicator of whether aperson knows themeaning of a word is the ability to use itappropriately in a sentence (Miller and Gildea,1987).
Much information about usage can beobtained from quite a limited context: Chouekaand Lusignan (1985) found that people cantypically recognize the intended sense of apolysemous word by looking at a narrowwindow of one or two words around it.Statistically-based computer programs havebeen able to do the same with a high level ofaccuracy (Kilgarriff and Palmer, 2000).
The goalof our work is to automatically identifyinappropriate usage of specific vocabularywords in essays by looking at the localcontextual cues around a target word.
We havedeveloped a statistical system, ALEK (AssessingLe____xical Knowledge), that uses statisticalanalysis for this purpose.A major objective of this research is to avoid thelaborious and costly process of collecting errors(or negative vidence) for each word that wewish to evaluate.
Instead, we train ALEK on ageneral corpus of English and on edited textcontaining example uses of the target word.
Thesystem identifies inappropriate usage based ondifferences between the word's local contextcues in an essay and the models of context it hasderived from the corpora of well-formedsentences.A requirement for ALEK has been that all stepsin the process be automated, beyond choosingthe words to be tested and assessing the results.Once a target word is chosen, preprocessing,building a model of the word's appropriateusage, and identifying usage errors in essays isperformed without manual intervention.ALEK has been developed using the Test ofEnglish as a Foreign Language (TOEFL)administered bythe Educational TestingService.
TOEFL is taken by foreign studentswho are applying to US undergraduate andgraduate-level programs.1 BackgroundApproaches to detecting errors by non-nativewriters typically produce grammars that look forspecific expected error types (Schneider andMcCoy, 1998; Park, Palmer and Washburn,1997).
Under this approach, essays written byESL students are collected and examined forerrors.
Parsers are then adapted to identify thoseerror types that were found in the essaycollection.We take a different approach, initially viewingerror detection as an extension of the word sensedisambiguation (WSD) problem.
Corpus-basedWSD systems identify the intended sense of apolysemous word by (1) collecting a set ofexample sentences for each of its various ensesand (2) extracting salient contextual cues fromthese sets to (3) build a statistical model for eachsense.
They identify the intended sense of aword in a novel sentence by extracting itscontextual cues and selecting the most similarword sense model (e.g., Leacock, Chodorow andMiller (1998), Yarowsky (1993)).Golding (1995) showed how methods used forWSD (decision lists and Bayesian classifiers)could be adapted to detect errors resulting from140common spelling confusions among sets such asthere, their, and they 're.
He extracted contextsfrom correct usage of each confusable word in atraining corpus and then identified a newoccurrence as an error when it matched thewrong context.However, most grammatical errors are not theresult of simple word confusions.
Thiscomplicates the task of building a model ofincorrect usage.
One approach we consideredwas to proceed without such a model: representappropriate word usage (across enses) in asingle model and compare a novel example tothat model.
The most appealing part of thisformulation was that we could bypass theknowledge acquisition bottleneck.
Alloccurrences of the word in a collection of editedtext could be automatically assigned to a singletraining set representing appropriate usage.Inappropriate usage would be signaled bycontextual cues that do not occur in training.Unfortunately, this approach was not effectivefor error detection.
An example of a word usageerror is often very similar to the model ofappropriate usage.
An incorrect usage cancontain two or three salient contextual elementsas well as a single anomalous element.
Theproblem of error detection does not entailfinding similarities to appropriate usage, rather itrequires identifying one element among thecontextual cues that simply does not fit.2 ALEK ArchitectureWhat kinds of anomalous elements does ALEKidentify?
Writers sometimes produce rrors thatviolate basic principles of English syntax (e.g., adesks), while other mistakes how a lack ofinformation about a specific vocabulary item(e.g., a knowledge).
In order to detect hese twotypes of problems, ALEK uses a 30-millionword general corpus of English from the SanJose Mercury News (hereafter referred to as thegeneral corpus) and, for each target word, a setof 10,000 example sentences from NorthAmerican newspaper text I (hereafter referred toas the word-specific corpus).i The corpora re extracted from the ACL-DCIcorpora.
In selecting the sentences for the wordALEK infers negative vidence from thecontextual cues that do not co-occur with thetarget word - either in the word specific corpusor in the general English one.
It uses two kindsof contextual cues in a +2 word window aroundthe target word: function words (closed-classitems) and part-of-speech tags (Brill, 1994).
TheBrill tagger output is post-processed to "enrich"some closed class categories of its tag set, suchas subject versus object pronoun and definiteversus indefinite determiner.
The enriched tagswere adapted from Francis and Ku~era (I 982).After the sentences have been preprocessed,ALEK counts sequences of adjacent part-of-speech tags and function words (such asdeterminers, prepositions, and conjunctions).
Forexample, the sequence a/ATfull-time/JJjob/NNcontributes one occurrence ach to the bigramsAT+J J, JJ+NN, a+JJ, and to the part-of-speech tagtrigram AT+JJ+NN.
Each individual tag andfunction word also contributes to its ownunigram count.
These frequencies form the basisfor the error detection measures.From the general corpus, ALEK computes amutual information measure to determine whichsequences of part-of-speech tags and functionwords are unusually rare and are, therefore,likely to be ungrammatical in English (e.g.,singular determiner preceding plural noun, as in*a desks).
Mutual information has often beenused to detect combinations of words that occurmore frequently than we would expect based onthe assumption that the words are independent.Here we use this measure for the oppositepurpose - to find combinations that occur lessoften than expected.
ALEK also looks forsequences that are common in general butunusual in the word specific corpus (e.g., thesingular determiner a preceding a singular nounis common in English but rare when the noun isspecific orpora, we tried to minimize the mismatchbetween the domains of newspapers and TOEFLessays.
For example, in the newspaper domain,concentrate is usually used as a noun, as in orangejuice concentrate but in TOEFL essays it is a verb91% of the time.
Sentence selection for the wordspecific orpora was constrained to reflect hedistribution of part-of-speech tags for the target wordin a random sample of TOEFL essays.141knowledge).
These divergences between the twocorpora reflect syntactic properties that arepeculiar to the target word.2.1 Measures based on the generalcorpus:The system computes mutual informationcomparing the proportion of observedoccurrences ofbigrams in the general corpus tothe proportion expected based on the assumptionof independence, as shown below:P(A) ?
P(B))Here, P(AB) is the probability of the occurrenceof the AB bigram, estimated from its frequencyin the general corpus, and P(A) and P(B) are theprobabilities of the first and second elements ofthe bigram, also estimated from the generalcorpus.
Ungrammatical sequences shouldproduce bigram probabilities that are muchsmaller than the product of the unigramprobabilities (the value of MI will be negative).Trigram sequences are also used, but in this casethe mutual information computation comparesthe co-occurrence of ABC to a model in whichA and C are assumed to be conditionallyindependent given B (see Lin, 1998).M/= log 2 P( B) x P( A I B ) x P(C I B)Once again, a negative value is often indicativeof a sequence that violates a rule of English.2.2 Comparing the word-specific orpusto the general corpus:ALEK also uses mutual information to comparethe distributions of tags and function words inthe word-specific corpus to the distributions thatare expected based on the general corpus.
Themeasures for bigrams and trigrams are similar tothose given above except that the probability inthe numerator isestimated from the word-specific orpus and the probabilities in thedenominator come from the general corpus.
Toreturn to a previous example, the phrase aknowledge contains the tag bigram for singulardeterminer followed by singular noun (AT Nil).This sequence ismuch less common in theword-specific corpus for knowledge than wouldbe expected from the general corpus unigramprobabilities of AT and NN.In addition to bigram and trigram measures,ALEK compares the target word's part-of-speech tag in the word-specific corpus and in thegeneral corpus.
Specifically, it looks at theconditional probability of the part-of-speech taggiven the major syntactic ategory (e.g., pluralnoun given noun) in both distributions, bycomputing the following value.
( P=p~c~c _ corm(taglcategory) Io g 2 / ~ ~t.
p=o,e,o, _ co, =(tag I )For example, in the general corpus, about half ofall noun tokens are plural, but in the training setfor the noun knowledge, the plural knowledgesoccurs rarely, if at all.The mutual information measures providecandidate errors, but this approach overgenerates- it finds rare, but still quite grammatical,sequences.
To reduce the number of falsepositives, no candidate found by the MImeasures i considered an error if it appears inthe word-specific corpus at least wo times.
Thisincreases ALEK's precision at the price ofreduced recall.
For example, aknowledge willnot be treated as an error because it appears inthe training corpus as part of the longer aknowledge of sequence (as in a knowledge ofmathematics).ALEK also uses another statistical technique forfinding rare and possibly ungrammatical t g andfunction word bigrams by computing the %2 (chisquare) statistic for the difference between thebigram proportions found in the word-specificand in the general corpus:~ Pgeneral_corpu~ i -egerneral_corpus ) / Nword specificThe %2 measure faces the same problem ofovergenerating errors.
Due to the large samplesizes, extreme values can be obtained eventhough effect size may be minuscule.
To reducefalse positives, ALEK requires that effect sizesbe at least in the moderate-to-small r nge(Cohen and Cohen, 1983).142Direct evidence from the word specific corpuscan also be used to control the overgeneration ferrors.
For each candidate rror, ALEKcompares the larger context in which the bigramappears to the contexts that have been analyzedin the word-specific corpus.
From the word-specific corpus, ALEK forms templates,sequences ofwords and tags that represent thelocal context of the target.
If a test sentencecontains a low probability bigram (as measuredby the X2 test), the local context of the target iscompared to all the templates of which it is apart.
Exceptions to the error, that is longergrammatical sequences that contain rare sub-sequences, are found by examining conditionalprobabilities.
To illustrate this, consider theexample of a knowledge and a knowledge of.The conditional probability of of  given aknowledge is high, as it accounts for almost allof the occurrences ofa knowledge in the word-specific corpus.
Based on this high conditionalprobability, the system will use the template fora knowledge of to keep it from being marked asan error.
Other function words and tags in the +1position have much lower conditionalprobability, so for example, a knowledge iswillnot be treated as an exception to the error.2.3 Validity of  the n-gram measuresTOEFL essays are graded on a 6 point scale,where 6 demonstrates "clear competence" inwriting on rhetorical and syntactic levels and 1demonstrates "incompetence in writing".
If lowprobability n-grams ignal grammatical errors,then we would expect TOEFL essays thatreceived lower scores to have more of these n-grams.
To test this prediction, we randomlyselected from the TOEFL pool 50 essays foreach of the 6 score values from 1.0 to 6.0.
ForScore1.0% of bigrams3.6% O f trigrams1.42.0 3.4 0.83.0 2.6 0.64.0 1.9 0.35.0 1.3 0.46.0 1.5 0.3Table 1: Percent of n-grams with mutualinformation <-3.60, by score pointeach score value, all 50 essays wereconcatenated to form a super-essay.
In everysuper-essay, for each adjacent pair and triple oftags containing a noun, verb, or adjective, thebigram and trigram mutual information valueswere computed based on the general corpus.Table 1 shows the proportions ofbigrams andtrigrams with mutual information less than-3.60.
As predicted, there is a significantnegative correlation between the score and theproportion of low probability bigrams (rs = -.94,n=6, p<.01, two-tailed) and trigrams (r~= -.84,n=6, p<.05, two-tailed).2.4 System developmentALEK was developed using three target wordsthat were extracted from TOEFL essays:concentrate, interest, and knowledge.
Thesewords were chosen because they representdifferent parts of speech and varying degrees ofpolysemy.
Each also occurred in at least 150sentences inwhat was then a small pool ofTOEFL essays.
Before development began, eachoccurrence of these words was manually labeledas an appropriate or inappropriate usage -without aking into account grammatical errorsthat might have been present elsewhere in thesentence but which were not within the targetword's scope.Critical values for the statistical measures wereset during this development phase.
The settingswere based empirically on ALEK's performanceso as to optimize precision and recall on thethree development words.
Candidate rrors werethose local context sequences that produced amutual information value of less than -3.60based on the general corpus; mutual informationof less than -5.00 for the specific/generalcomparisons; ora X2 value greater than 12.82with an effect size greater than 0.30.
Precisionand recall for the three words are shown below.Target word n Precision RecallConcentrate 169 .875 .280Interest 416 .840 .330Knowledge 761 .918 .570Table 2: Development Words143Test Word Precision Recall Total Recall Test Word Precision Recall Total Recall(estimated) (estimated)Affect .848 .762 .343 .768 .666 .104AreaAspectBenefit.752.792.744.846.717.709.205.217.276EnergyFunctionIndividualJob.800.576.714.742.168.302.728 .679 .103Career .736 .671 .110 Period .832 .670 .102Communicate .784 .867 .274 Pollution .912 .780 .310Concentrate .848 .791 .415 Positive .784 .700 .091Conclusion .944 .756 .119 Role ' .728 .674 .098Culture .704 .656 .083 Stress .768 .578 .162.816 .728.779Economy .666 .674.716Technology ~Mean.235 .093.190Table 3: Precision and recall for 20 test words3 Experimental Design and ResultsALEK was tested on 20 words.
These wordswere randomly selected from those which mettwo criteria: (1) They appear in a universityword list ('Nation, 1990) as words that a studentin a US university will be expected to encounterand (2) there were at least 1,000 sentencescontaining the word in the TOEFL essay pool.To build the usage model for each target word,10,000 sentences containing it were extractedfrom the North American News Corpus.Preprocessing included etecting sentenceboundaries and part-of-speech tagging.
As in thedevelopment system, the model of generalEnglish was based on bigram and trigramfrequencies of function words and part-of-speech tags from 30-million words of the SanJose Mercury News.For each test word, all of the test sentences weremarked by ALEK as either containing an erroror not containing an error.
The size of the test setfor each word ranged from 1,400 to 20,000 witha mean of 8,000 sentences.3.1 ResultsTo evaluate the system, for each test word werandomly extracted 125 sentences that ALEKclassified as containing no error (C-set) and 125sentences which it labeled as containing an error(E-set).
These 250 sentences were presented toa linguist in a random order for blind evaluation.The linguist, who had no part in ALEK'sdevelopment, marked each usage of  the targetword as incorrect or correct and in the case ofincorrect usage indicated how far from the targetone would have to look in order to recognise thatthere was an error.
For example, in the case of"an period" the error occurs at a distance of oneword from period.
When the error is anomission, as in "lived in Victorian period", thedistance is where the missing word should haveappeared.
In this case, the missing determiner is2 positions away from the target.
When morethan one error occurred, the distance of the oneclosest o the target was marked.Table 3 lists the precision and recall for the 20test words.
The column labelled "Recall" is theproportion of human-judged rrors in the 250-sentence sample that were detected by ALEK.
"Total Recall" is an estimate that extrapolatesfrom the human judgements of the sample to theentire test set.
We illustrate this with the resultsfor pollution.
The human judge marked asincorrect usage 91.2% of the sample fromALEK's E-set and 18.4% of the sample from itsC-set.
To estimate overall incorrect usage, wecomputed a weighted mean of these two rates,where the weights reflected the proportion ofsentences that were in the E-set and C-set.
TheE-set contained 8.3% of the pollution sentencesand the C-set had the remaining 91.7%.
With thehuman judgements as the gold standard, theestimated overall rate of incorrect usage is (.083x .912 + .917 x .184) = .245.
ALEK's estimatedrecall is the proportion of sentences in the E-settimes its precision, divided by the overallestimated error rate (.083 ?
.912) / .245 = .310.144The precision results vary from word to word.Conclusion and pollution have precision in thelow to middle 90's while individual's precisionis 57%.
Overall, ALEK's predictions are about78% accurate.
The recall is limited in part by thefact that the system only looks at syntacticinformation, while many of the errors aresemantic.3.2 Analysis of  Hits and MissesNicholls (1999) identifies four error types: anunnecessary word (*affect o their emotions), amissing word (*opportunity of job.
), a word orphrase that needs replacing (*every jobs), a wordused in the wrong form (*pollutions).
ALEKrecognizes all of these types of errors.
For closedclass words, ALEK identified whether a wordwas missing, the wrong word was used (choice),and when an extra word was used.
Open classwords have a fourth error category, form,including inappropriate compounding and verbagreement.
During the development stage, wefound it useful to add additional error categories.Since TEOFL graders are not supposed to takepunctuation i to account, punctuation errorswere only marked when they caused the judge to"garden path" or initially misinterpret thesentence.
Spelling was marked either when afunction word was misspelled, causing part-of-speech tagging errors, or when the writer'sintent was unclear.The distributions of categories for hits andmisses, shown in Table 4, are not strikinglydifferent.
However, the hits are primarilysyntactic in nature while the misses are bothsemantic (as in open-class:choice) and syntactic(as in closed-class:missing).ALEK is sensitive to open-class wordconfusions (affect vs effect) where the part ofspeech differs or where the target word isconfused with another word (*ln this aspect,...instead ofln this respect, ...).
In both cases, thesystem recognizes that the target is in the wrongsyntactic environment.
Misses can also besyntactic - when the target word is confusedwith another word but the syntactic environmentfails to trigger an error.
In addition, ALEK doesnot recognize semantic errors when the errorinvolves the misuse of an open-class word inCategory % Hits % MissesClosed-class - choice 22.5 15.5--extra 15.5 13.0-missing.Open-class - choice8.0 8.512.0 19.0- extra .5 1.0- missing 15- form1.528.0 28.5Punctuation 5.5 1.51.55.5Sentence fragmentSpelling/typing errorWord order .52.08.51.0Table 4: Hits and misses based on a random sampleof 200 hits and 200 missescombination with the target (for example, makein "*they make benefits").Closed class words typically are either selectedby or agree with a head word.
So why are thereso many misses, especially with prepositions?The problem is caused in part by polysemy -when one sense of the word selects aprepositionthat another sense does not.
When concentrate isused spatially, it selects the preposition i , as"the stores were concentrated in the downtownarea".
When it denotes mental activity, it selectsthe preposition on, as in "Susan concentrated onher studies".
Since ALEK trains on all senses ofconcentrate, it does not detect he error in"*Susan concentrated in her studies".
Anothercause is that adjuncts, especially temporal andlocative adverbials, distribute freely in the word-specific corpora, as in "Susan concentrated inher room."
This second problem is moretractable than the polysemy problem - andwould involve training the system to recognizecertain types of adjuncts.3.3 Analysis of  False PositivesFalse positives, when ALEK "identifies" anerror where none exists, fall into six majorcategories.
The percentage of each false positivetype in a random sample of 200 false positives isshown in Table 5.Domain mismatch: Mismatch of thenewspaper-domain word-specific corpora ndessay-domain test corpus.
One notabledifference is that some TOEFL essay promptscall for the writer's opinion.
Consequently,145Error Type % OccurrenceDomain mismatch 12.517.0 TaggerSyntacticFree distribution14.516.5Punctuation 12.0Infrequent tagsOther9.018.5Table 5.
Distribution of false positive typesTOEFL essays often contain first personreferences, whereas newspaper a ticles arewritten in the third person.
We need tosupplement the word-specific corpora withmaterial that more closely resembles the testcorpus.Tagger: Incorrect analysis by the part-of-speechtagger.
When the part-of-speech tag is wrong,ALEK often recognizes the resulting n-gram asanomalous.
Many of these errors are caused bytraining on the Brown corpus instead of a corpusof essays.Syntactic analysis: Errors resulting from usingpart-of-speech tags instead of supertags or a fullparse, which would give syntactic relationsbetween constituents.
For example, ALEK falsealarms on arguments of ditransitive verbs suchas offer and flags as an error "you benefits" in"offers you benefits".Free distribution: Elements that distributefreely, such as adverbs and conjunctions, as wellas temporal and locative adverbial phrases, tendto be identified as errors when they occur insome positions.Punctuation: Most notably omission of periodsand commas.
Since these errors are notindicative of one's ability to use the target word,they were not considered as errors unless theycaused the judge to misanalyze the sentence.Infrequent tags.
An undesirable r sult of our"enriched" tag set is that some tags, e.g., thepost-determiner last, occur too infrequently inthe corpora to provide reliable statistics.Solutions to some of these problems will clearlybe more tractable than to others.4 Comparison of ResultsComparison of these results to those of othersystems i  difficult because there is no generallyaccepted test set or performance baseline.
Giventhis limitation, we compared ALEK'sperformance toa widely used grammar checker,the one incorporated in Microsoft's Word97.
Wecreated files of sentences used for the threedevelopment words concentrate, interest, andknowledge, and manually corrected any errorsoutside the local context around the target beforechecking them with Word97.
The performancefor concentrate showed overall precision of 0.89and recall of 0.07.
For interest, precision was0.85 with recall of 0.11.
In sentences containingknowledge, precision was 0.99 and recall was0.30.
Word97 correctly detected theungrammaticality ofknowledges as well as aknowledge, while it avoided flagging aknowledge of.In summary, Word97's precision in errordetection is impressive, but the lower recallvalues indicate that it is responding tofewererror types than does ALEK.
In particular,Word97 is not sensitive to inappropriateselection of prepositions for these three words(e.g., *have knowledge on history, *toconcentrate at science).
Of course, Word97detects many kinds of errors that ALEK doesnot.Research as been reported on grammarcheckers pecifically designed for an ESLpopulation.
These have been developed by hand,based on small training and test sets.
Schneiderand McCoy (1998) developed a system tailoredto the error productions of American SignLanguage signers.
This system was tested on 79sentences containing determiner and agreementerrors, and 101 grammatical sentences.
Wecalculate that their precision was 78% with 54%recall.
Park, Palmer and Washburn (1997)adapted a categorial grammar to recognize"classes of errors \[that\] dominate" in the nineessays they inspected.
This system was tested oneight essays, but precision and recall figures arenot reported.5 ConclusionThe unsupervised techniques that we havepresented for inferring negative vidence areeffective in recognizing rammatical errors inwritten text.146Preliminary results indicate that ALEK's errordetection is predictive of TOEFL scores.
IfALEK accurately detects usage errors, then itshould report more errors in essays with lowerscores than in those with higher scores.
We havealready seen in Table 1 that there is a negativecorrelation between essay score and two ofALEK's component measures, the generalcorpus n-grams.
However, the data in Table 1were not based on specific vocabulary items anddo not reflect overall system performance, whichincludes the other measures as well.Table 6 shows the proportion of test wordoccurrences that were classified by ALEK ascontaining errors within two positions of thetarget at each of 6 TOEFL score points.
Aspredicted, the correlation is negative (rs = -1.00,n = 6, p < .001, two-tailed).
These data supportthe validity of the system as a detector ofinappropriate usage, even when only a limitednumber of words are targeted and only theimmediate context of each target is examined.Score123456ALEK Human.091 .
.
.
.
..085 .375.067 .268.057 .293.048 .232.041 .164Table 6: Proportion of  test word occurrences, byscore point, classified as containing an error byALEK and by a human judgeFor comparison, Table 6 also gives the estimatedproportions of inappropriate usage by scorepoint based on the human judge's classification.Here, too, there is a negative correlation: rs =-.90, n = 5, p < .05, two-tailed.Although the system recognizes a wide range oferror types, as Table 6 shows, it detects onlyabout one-fifth as many errors as a human judgedoes.
To improve recall, research needs to focuson the areas identified in section 3.2 and, toimprove precision, efforts should be directed atreducing the false positives described in 3.3.ALEK is being developed as a diagnostic toolfor students who are learning English as aforeign language.
However, its techniques couldbe incorporated into a grammar checker fornative speakers.AcknowledgmentsWe thank Susanne Wolff for evaluating the testsentences, and Robert Kantor, Ken Sheppard and 3anonymous reviewers for their helpful suggestions.ReferencesBrill, E. 1994.
Some advances in rule-based part-of-speech tagging.
Proceedings of the TwelfthNational Conference on Artificial Intelligence,Seattle, AAAI.Choueka, Y. and S. Lusignan.
1985.
Disambiguationby short contexts.
Computers and the Humanities,19:147-158.Cohen, J. and P. Cohen.
1983.
Applied MultipleRegression~Correlation Analysis for theBehavioral Sciences.
Hillsdale, N J: Erlbaum.Francis, W. and H. Ku~era.
1982.
FrequencyAnalysis of English Usage: Lexicon and Grammar.Boston, Houghton Mifflin.Golding, A.
1995.
A Bayesian hybrid for context-sensitive spelling correction.
Proceedings of the 3 ~aWorkshop on Very Large Corpora.
Cambridge,MA.
39--53.Kilgarriff, A. and M. Palmer.
2000.
Introduction tothe special issue on SENSEVAL.
Computers andthe Humanities, 34:1----2.Leacock, C., M. Chodorow and G.A.
Miller.
1998.1998.
Using corpus tatistics and WordNet'slexical relations for sense identification.Computational Linguistics, 24:1.Lin, D. 1998.
Extracting collocations from textcorpora.
First Workshop on ComputationalTerminology.
Montreal, Canada.Miller, G.A.
and P. Gildea.
1987.
How children learnwords.
Scientific American, 257.Nation, I.S.P.
1990.
Teaching and learningvocabulary.
New York: Newbury House.Nicholls, D. 1999.
The Cambridge Learner Corpus -Error coding and analysis.
Summer Workshop onLearner Corpora.
TokyoPark, J.C., M. Palmer and G. Washburn.
1997.Checking rammatical mistakes for English-as-a-second-language (ESL) students.
Proceedings ofKSEA-NERC.
New Brunswick, NJ.Schneider, D.A.
and K.F.
McCoy.
1998.
Recognizingsyntactic errors in the writing of second languagelearners.
Proceedings of Coling-ACL-98, Montr6al.Yarowsky, D. 1993.
One sense per collocation.Proceedings of the ARPA Workshop on HumanLanguage Technology.
San Francisco.
MorganKaufman.147
