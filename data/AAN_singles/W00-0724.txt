In: Proceedings o/CoNLL-2000 and LLL-2000, pages 119-122, Lisbon, Portugal, 2000.A Default First Order Family Weight Determination Procedurefor WPDV ModelsHans  van  Ha l te renDept.
of Language and Speech, University of Ni jmegenP.O.
Box 9103, 6500 HD Nijmegen, The Netherlandshvh@let, kun.
nlAbst rac tWeighted Probability Distribution Voting(WPDV) is a newly designed machine learningalgorithm, for which research is currentlyaimed at the determination of good weightingschemes.
This paper describes a simple yeteffective weight determination procedure, whichleads to models that can produce competitiveresults for a number of NLP classificationtasks.1 The  WPDV a lgor i thmWeighted Probability Distribution Voting(WPDV) is a supervised learning approach toclassification.
A case which is to be classified isrepresented as a feature-value pair set:Fcase -- {{fl : Vl}, .
.
.
,  { fn  :Vn}}An estimation of the probabilities of the variousclasses for the case in question is then based onthe classes observed with similar feature-valuepair sets in the training data.
To be exact, theprobability of class C for Fcase is estimated asa weighted sum over all possible subsets Fsub ofFcase:w /req(CJF  b)P(C) = N(C) /req(F  b)FsubCFcasewith the frequencies (freq) measured on thetraining data, and N(C) a normalizing factorsuch that ~/5(C)  = 1.In principle, the weight factors WF,~,~ can beassigned per individual subset.
For the timebeing, however, they are assigned for groups ofsubsets.
First of all, it is possible to restrictthe subsets that are taken into account in themodel, using the size of the subset (e.g.
Fsubcontains at most 4 elements) and/or its fre-quency (e.g.
Fsub occurs at least twice in thetraining material).
Subsets which do not fulfilthe chosen criteria are not used.
For the sub-sets that are used, weight factors are not as-signed per individual subset either, but ratherper "family", where a family consists of thosesubsets which contain the same combination offeature types (i.e.
the same f/).The two components of a WPDV model, dis-tributions and weights, are determined sepa-rately.
In this paper, I will use the term trainingset for the data on which the distributions arebased and tuning set for the data on the basis ofwhich the weights are selected.
Whether thesetwo sets should be disjunct or can coincide isone of the subjects under investigation.2 Fami ly  we ightsThe various family weighting schemes can beclassified according to the type of use they makeof the tuning data.
Here, I use a very roughclassification, into weighting scheme orders.With 0 th order  weights,  no informationwhatsoever is used about the data in tuningset.
Examples of such rudimentary weightingschemes are the use of a weight of k!
for all sub-sets containing k elements, as has been used e.g.for wordclass tagger combination (van Halterenet al, To appear), or even a uniform weight forall subsets.With 1 st order  weights,  information is usedabout the individual feature types, i.e.WF,~b = IT WIt{il(f i :vi}eF, ub}First order weights ignore any possible inter-action between two or more feature types, but119have the clear advantage of corresponding to areasonably low number of weights, viz.
as manyas there are feature types.With n th order  weights,  interaction pat-terns are determined of up to n feature typesand the family weights are adjusted to compen-sate for the interaction.
When n is equal tothe total number of feature types, this corre-sponds to weight determination per individualfamily, n th order weighting generally requiresmuch larger numbers of weights, which can beexpected to lead to much slower tuning proce-dures.
In this paper, therefore, I focus on firstorder weighting.3 F i r s t  o rder  we ight  determinat ionAs argumented in an earlier paper (van Hal-teren, 2000a), a theory-based feature weight de-termination would have to take into accounteach feature's decisiveness and reliability.
How-ever, clear definitions of these qualities, andhence also means to measure them, are as yetsorely lacking.
As a result, a more pragmaticapproach will have to be taken.
Reliability is ig-nored altogether at the moment, 1 and decisive-ness replaced by an entropy-related measure.3.1 Initial weightsThe weight given to each feature type fi shouldpreferably increase with the amount of informa-tion it contributes to the classification process.A measure related to this is Information Gain,which represents the difference between the en-tropy of the choice with and without knowledgeof the presence of a feature (cf.
Quinlan (1986)).As do Daelemans et al (2000), I opt for a fac-tor proportional to the feature type's Gain Ra-tio, a normalising derivative of the InformationGain value.
The weight factors W/~ are set toan optimal multiplication constant C times themeasured Gain Ratio for fi- C is determined bycalculating the accuracies for various values ofC on the tuning set 2 and selecting the C whichyields the highest accuracy.lit may still be present, though, in the form of theabovementioned frequency threshold for features.2If the tuning set coincides with the training set, allparts of the tuning procedure are done in leave-one-outmode: in the WPDV implementation, it is possible to(virtually) remove the information about each individualinstance from the model when that specific instance hasto be classified.3.2 Hi l l -c l imbingSince the initial weight determination is basedon pragmatic rather than theoretical consider-ations, it is unlikely that the resulting weightsare already the optimal ones.
For this reason,an attempt is made to locate even better weightvectors in the n-dimensional weight space.
Thenavigation mechanism used in this search is hill-climbing.
This means that systematic variationsof the currently best vector are investigated.
Ifthe best variation is better than the currentlybest vector, that variation is taken as the bestvector and the process is repeated.
This repeti-tion continues until no better vector is found.In the experiments described here, the varia-tion consists of multiplication or division of eachindividual W/i by a variable V (i.e.
2n new vec-tors are tested each time), which is increased if abetter vector is found, and otherwise decreased.The process is halted as soon as V falls belowsome pre-determined threshold.Hill-climbing, as most other optimaliza-tion techniques, is vulnerable to overtraining.To lessen this vulnerability, the WPDV hill-climbing implementation splits its tuning mate-rial into several (normally five) parts.
A switchto a new weight vector is only taken if the ac-curacy increases on the tuning set as a wholeand does not decrease on more than one part,i.e.
some losses are accepted but only if theyare localized.4 Qua l i ty  o f  the  f i rs t  o rder  we ightsIn order to determine the quality of the WPDVsystem, using first order weights as describedabove, I run a series of experiments, using tasksintroduced by Daelemans et al (1999): 3The Par t -o f - speech  tagg ing  task (POS) isto determine a wordclass tag on the basis of dis-ambiguated tags of two preceding tokens andundisambiguated tags for the focus and two fol-lowing tokens.
4 5 features with 170-480 values;169 classes; 837Kcase training; 2xl05Kcase test.The Grapheme- to -phoneme convers ionwith stress  task (GS) is to determine the pro-nunciation of an English grapheme, includingaI only give a rough description of the tasks here.
Forthe exact details, I refer the reader to Daelemans et al(1999).4For a overall WPDV approach to wordclass tagging,see van Halteren (2000b).120Table h Accuracies for the POS task (with thetraining set ah tested in leave-one-out mode)Table 2: Accuracies for the GS task (with thetraining set ah tested in leave-one-out mode)Weighting scheme Test setah iComparisonNaive BayesTiMBL (k=l)Maccent (freq=2;iter=150)Maccent (freq=l;iter=300)WPDV 0 th order weights1kl96.4197.8398.0798.1397.66 97.7196.86 96.92WPDV initial 18t ordertune = ah (10GR) 98.14 98.16tune = i (12GR) 98.14 98.17tune = j (llGR) 98.14 98.16WPDV with hill-climbingtune = ah (30 steps) 98.17 98.21tune = i (20 steps) 98.15 98.20tune = j (20 steps) 98.15 98.1896.2497.7998.0398.1097.6396.8698.1298.1298.1398.1598.1298.16Weighting schemeComparisonNaive BayesTiMBL (k----l)Maccent (freq=2;iter=150)Maccent (freq=l;iter=300)Test setah i j50.05 49.9892.25 92.0279.41 79.3680.43 80.35WPDV 0 th order weights1 90.99 90.49 90.25k!
92.77 92.05 91.89WPDV initial 1 st ordertune = ah (30GR) 93.27 92.74 92.52tune = i (25GR) 93.24 92.76 92.54tune = j (25GR) 93.24 92.76 92.54WPDV with hill-climbingtune = ah (34 steps) 93.29 92.77 92.53tune = i (28 steps) 93.25 92.79 92.53tune = j (12 steps) 93.24 92.76 92.54presence of stress, on the basis of the focusgrapheme, three preceding and three followinggraphemes.
7 features with 42 values each; 159classes; 540Kcase training; 2x68Kcase test.The PP  a t tachment  ask (PP) is preposi-tional phrase attachment to either a precedingverb or a preceding noun, on the basis of theverb, the noun, the preposition in question andthe head noun of the prepositional complement.4 features with 3474, 4612, 68 and 5780 values;2 classes; 19Kcase training; 2x2Kcase test.The NP  chunk ing  task (NP) is the deter-ruination of the position of the focus token in abase NP chunk (at beginning of chunk, in chunk,or not in chunk), on the basis of the words andtags for two preceding tokens, the focus andone following token, and also the predictions bythree newfirst stage classifiers for the task.
5 11features with 3 (first stage classifiers), 90 (tags)and 20K (words) values; 3 classes; 201Kcasetraining; 2x25Kcase test.
6For each of the tasks, sections a to h of the dataset are used as the training set and sections i5For a WPDV approach to a more general chunkingtask, see my contribution to the CoNLL shared task,elsewhere in these proceedings.~The number of feature combinations for the NP taskis so large that the WPDV model has to be limited.
Forthe current experiments, I have opted for a maximumsize for fsub of four features and a threshold frequencyof two observations in the training set.and j as (two separate) test sets.
All three arealso used as tuning sets.
This allows a compari-son between tuning on the training set itself andon a held-out uning set.
For comparison withsome other well-known machine learning algo-rithms, I complement the WPDV experimentswith accuracy measurements forthree other sys-tems: 1) A system using a Na ive  Bayes  prob-ability estimation; 2) T iMBL ,  using memorybased learning and probability estimation basedon the nearest neighbours (Daelemans et al,2000), 7 for which I use the parameters whichyielded the best results according to Daelemanset al (1999); and 3) Maccent ,  a maximum en-tropy based system, s for which I use both thedefault parameters, viz.
a frequency thresholdof 2 for features to be used and 150 iterationsof improved iterative scaling, and a more am-bitious parameter setting, viz.
a threshold of 1and 300 iterations.The results for various WPDV weights, andthe other machine learning techniques are listedin Tables 1 to 4.
9 Except for one case (PP withtune on j and test on i), the first order weightWPDV results are all higher than those for the7http:// i lk.kub.nl/.Shttp://w~.cs.kuleuven.ac.be/~ldh.9The accuracy isshown in itMics wheneverthetuningset is equM to the test set, i.e.
when there is anunf~radvantage.121Table 3: Accuracies for the PP  task (with thetraining set ah tested in leave-one-out mode)Table 4: Accuracies for the NP task (with thetraining set ah tested in leave-one-out mode)Weighting scheme Test setah iComparisonNaive BayesTiMBL (k=l)Maccent (freq=2;iter=150)Maccent (freq=l;iter=300)WPDV 0 ~h order weights1k~82.68 82.6483.43 81.9781.00 80.2579.41 79.7980.83 82.26 81.4680.76 82.30 81.30WPDV initial 18t ordertune = ah (21GR) 82.89 83.64 82.38tune = i (15GR) 82.82 83.81 82.55tune = j (llGR) 82.60 83.26 82.76WPDV with hill-climbingtune --- ah (19 steps) 83.10 83.72 82.68tune = i (18 steps) 82.95 84.06 82.80tune = j (16 steps) 82.65 83.10 82.93Weighting scheme i Test setah i jComparisonNaive BayesTiMBL (k=3)Maccent (freq=2;iter=150)Maccent (freq=l;iter=300)WPDV 0 th order weights1k~96.52 96.4998.34 98.2297.89 97.7597.66 97.4597.56 97.77 97.6997.74 97.97 97.87WPDV initial I st ordertune = ah (380GR) 98.19 98.38 98.26tune = i (60GR) 98.14 98.39 98.17tune = j (360GR) 98.19 98.38 98.27WPDV with hill-climbingtune = ah (50 steps) 98.36 98.54 98.44tune = i (34 steps) 98.25 98.57 98.33tune = j (12 steps) 98.19 98.38 98.27comparison systems.
1?
0 th order weights gener-ally do not reach this level of accuracy.Hill-climbing with the tuning set equal to thetraining set produces the best results overall.It always leads to an improvement over initialweights of the accuracies on both test sets, al-though sometimes very small (GS).
Equally im-portant, the improvement on the test sets iscomparable to that on the tuning/training set.This is certainly not the case for hill-climbingwith the tuning set equal to the other test set,which generally does not reach the same level ofaccuracy and may even be detrimental (climb-ing on PPj) .Strangely enough, hill-climbing with the tun-ing set equal to the test set itself sometimes doesnot even yield the best quality for that test set(POS with test set i and especially NP with j).This shows that the weight-+accuracy functiondoes have local maxima~ and the increased riskfor smaller data sets to run into a sub-optimalone is high enough that it happens in at leasttwo of the eight test set climbs.1?The accuracies for TiMBL are lower than thosefound by Daelemans et ai.
(1999): POSi 97.95, POSj97.90, GS~ 93.75, GSj 93.58, PP~ 83.64, PPj 82.51, NP~98.38 and NPj 98.25.
This is due to the use of eight parttraining sets instead of nine.
The extreme differences forthe GS task show how much this task depends on indi-vidual observations rather than on generalizations, whichprobably also explains why Naive Bayes and MaximumExtropy (Maccent) handle this task so badly.In summary, hill-climbing should preferablybe done with the tuning set equal to the trainingset.
This is not surprising, as the leave-one-out mechanism allows the training set to behaveas held-out data, while containing eight timesmore cases than a test set turned tuning set.The disadvantage is a much more time-intensivehill-climbing procedure, but when developing anactual production model, the weights only haveto be determined once and the results appear tobe worth it most of the time.Referencesw.
Daelemans, A.
Van den Bosch, and J. Zavrel.1999.
Forgetting exceptions is harmful in lan-guage learning.
Machine Learning, Special issueon Natural Language Learning, 34:11-41.W.
Daelemans, J. Zavrel, K. Van der Sloot, andA.
Van den Bosch.
2000.
TiMBL: Tilburg Mem-ory Based Learner, version 3.0, reference manual.Tech.
Report ILK-00-01, ILK, Tilburg University.H.
van Halteren.
2000a.
Weighted Probability Dis-tribution Voting, an introduction.
In Computa-tional linguistics in the Netherlands, 1999.H.
van Halteren.
2000b.
The detection of in-consistency in manually tagged text.
In Proc.LINC2000.H.
van Halteren, J. Zavrel, and W. Daelemans.To appear.
Improving accuracy in NLP throughcombination of machine learning systems.
Com-putational Linguistics.J.R.
Quinlan.
1986.
Induction of Decision Trees.Machine Learning, 1:81-206.122
