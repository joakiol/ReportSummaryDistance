Bootstrapping Coreference Classifiers withMultiple Machine Learning AlgorithmsVincent Ng and Claire CardieDepartment of Computer ScienceCornell UniversityIthaca, NY 14853-7501{yung,cardie}@cs.cornell.eduAbstractSuccessful application of multi-view co-training algorithms relies on the ability tofactor the available features into views thatare compatible and uncorrelated.
This canpotentially preclude their use on problemssuch as coreference resolution that lack anobvious feature split.
To bootstrap coref-erence classifiers, we propose and eval-uate a single-view weakly supervised al-gorithm that relies on two different learn-ing algorithms in lieu of the two differentviews required by co-training.
In addition,we investigate a method for ranking un-labeled instances to be fed back into thebootstrapping loop as labeled data, aimingto alleviate the problem of performancedeterioration that is commonly observedin the course of bootstrapping.1 IntroductionCo-training (Blum and Mitchell, 1998) is a weaklysupervised paradigm that learns a task from a smallset of labeled data and a large pool of unlabeleddata using separate, but redundant views of the data(i.e.
using disjoint feature subsets to represent thedata).
To ensure provable performance guaran-tees, the co-training algorithm assumes as input aset of views that satisfies two fairly strict condi-tions.
First, each view must be sufficient for learn-ing the target concept.
Second, the views must beconditionally independent of each other given theclass.
Empirical results on artificial data sets byMuslea et al (2002) and Nigam and Ghani (2000)confirm that co-training is sensitive to these assump-tions.
Indeed, although the algorithm has been ap-plied successfully to natural language processing(NLP) tasks that have a natural view factorization(e.g.
web page classification (Blum and Mitchell,1998) and named entity classification (Collins andSinger, 1999)), there has been little success, and anumber of reported problems, when applying co-training to NLP data sets for which no natural fea-ture split has been found (e.g.
anaphora resolution(Mueller et al, 2002)).As a result, researchers have begun to investigateco-training procedures that do not require explicitview factorization.
Goldman and Zhou (2000) andSteedman et al (2003b) use two different learningalgorithms in lieu of the multiple views required bystandard co-training.1 The intuition is that the twolearning algorithms can potentially substitute for thetwo views: different learners have different rep-resentation and search biases and can complementeach other by inducing different hypotheses from thedata.
Despite their similarities, the principles under-lying the Goldman and Zhou and Steedman et alco-training algorithms are fundamentally different.In particular, Goldman and Zhou rely on hypothesistesting to select new instances to add to the labeleddata.
On the other hand, Steedman et al use twolearning algorithms that correspond to coarsely dif-ferent features, thus retaining in spirit the advantages1Steedman et al (2003b) bootstrap two parsers that use dif-ferent statistical models via co-training.
Hence, the two parserscan effectively be viewed as two different learning algorithms.provided by conditionally independent feature splitsin the Blum and Mitchell algorithm.The goal of this paper is two-fold.
First, wepropose a single-view algorithm for bootstrappingcoreference classifiers.
Like anaphora resolution,noun phrase coreference resolution is a problem forwhich a natural feature split is not readily available.In related work (Ng and Cardie, 2003), we com-pare the performance of the Blum and Mitchell co-training algorithm with that of two existing single-view bootstrapping algorithms ?
self-training withbagging (Banko and Brill, 2001) and EM (Nigam etal., 2000) ?
on coreference resolution, and showthat single-view weakly supervised learners are a vi-able alternative to co-training for the task.
This pa-per instead focuses on developing a single-view al-gorithm that combines aspects of each of the Gold-man and Zhou and Steedman et al algorithms.Second, we investigate a new method that, in-spired by Steedman et al (2003a), ranks unlabeledinstances to be added to the labeled data in an at-tempt to alleviate a problem commonly observed inbootstrapping experiments ?
performance deterio-ration due to the degradation in the quality of thelabeled data as bootstrapping progresses (Pierce andCardie, 2001; Riloff and Jones, 1999).In a set of baseline experiments, we first demon-strate that multi-view co-training fails to boost theperformance of the coreference system under var-ious parameter settings.
We then show that oursingle-view weakly supervised algorithm success-fully bootstraps the coreference classifiers, boost-ing the F-measure score by 9-12% on two standardcoreference data sets.
Finally, we present experi-mental results that suggest that our method for rank-ing instances is more resistant to performance dete-rioration in the bootstrapping process than Blum andMitchell?s ?rank-by-confidence?
method.2 Noun Phrase Coreference ResolutionNoun phrase coreference resolution refers to theproblem of determining which noun phrases (NPs)refer to each real-world entity mentioned in a doc-ument.2 In this section, we give an overview ofthe coreference resolution system to which the boot-2Concrete examples of the coreference task can be found inMUC-6 (1995) and MUC-7 (1998).strapping algorithms will be applied.The framework underlying the coreference sys-tem is a standard combination of classification andclustering (see Ng and Cardie (2002) for details).Coreference resolution is first recast as a classifica-tion task, in which a pair of NPs is classified as co-referring or not based on constraints that are learnedfrom an annotated corpus.
A separate clusteringmechanism then coordinates the possibly contradic-tory pairwise classifications and constructs a parti-tion on the set of NPs.
When the system operateswithin the weakly supervised setting, a weakly su-pervised algorithm bootstraps the coreference classi-fier from the given labeled and unlabeled data ratherthan from a much larger set of labeled instances.
Theclustering algorithm, however, is not manipulated bythe bootstrapping procedure.3 Learning AlgorithmsWe employ naive Bayes and decision list learnersin our single-view, multiple-learner framework forbootstrapping coreference classifiers.
This sectiongives an overview of the two learners.3.1 Naive BayesA naive Bayes (NB) classifier is a generative classi-fier that assigns to a test instance i with feature val-ues <x1, .
.
., xm> the maximum a posteriori (MAP)label y?, which is determined as follows:y?
= arg maxyP (y | i)= arg maxyP (y)P (i | y)= arg maxyP (y)m?i = 1P (xi| y)The first equality above follows from the definitionof MAP, the second one from Bayes rule, and the lastone from the conditional independence assumptionof the feature values.
We determine the class priorsP(y) and the class densities P(xi| y) directly fromthe training data using add-one smoothing.3.2 Decision ListsOur decision list (DL) algorithm is based on that de-scribed in Collins and Singer (1999).
For each avail-able feature fiand each possible value vjof fiin thetraining data, the learner induces an element of theObservations JustificationsMany feature-value pairs alone can de-termine the class value.3 For example,two NPs cannot be coreferent if they differin gender or semantic class.Decision lists draw a decision boundary based on a single feature-value pairand can take advantage of this observation directly.
On the other hand, naiveBayes classifiers make a decision based on a combination of features andthus cannot take advantage of this observation directly.The class distributions in coreferencedata sets are skewed.
Specifically, thefact that most NP pairs in a document arenot coreferent implies that the negative in-stances grossly outnumber the positives.Naive Bayes classifiers are fairly resistant to class skewness, which canonly exert its influence on classifier prediction via the class priors.
On theother hand, decision lists suffer from skewed class distributions.
Elementscorresponding to the negative class tend to aggregate towards the beginningof the list, causing the classifier to perform poorly on the minority class.Many instances contain redundant in-formation as far as classification is con-cerned.
For example, two NPs may dif-fer in both gender and semantic class, butknowing one of these two differences is suf-ficient for determining the class value.Both naive Bayes classifiers and decision lists can take advantage of dataredundancy.
Frequency counts of feature-value pairs in these classifiers areupdated independently, and thus a single instance can possibly contribute tothe discovery of more than one useful feature-value pair.
On the other hand,some classifiers such as decision trees are not able to take advantage of thisredundancy because of their intrinsic nature of recursive data partitioning.Table 1: The justifications (shown in the right column) for using naive Bayes and decision list learner asthe underlying learning algorithms for bootstrapping coreference classifiers are based on the correspondingobservations on the coreference task and the features used by the coreference system in the left column.decision list for each class y.
The elements in the listare sorted in decreasing order of the strength associ-ated with each element, which is defined as the con-ditional probability P(y | fi= vj) and is estimatedbased on the training data as follows:P (y | fi= vj) =N (fi= vj, y) + ?N (fi= vj) + k?N (x) is the frequency of event x in the trainingdata, ?
a smoothing parameter, and k the numberof classes.
In this paper, k = 2 and we set ?
to 0.01.A test instance is assigned the class associated withthe first element of the list whose predicate is satis-fied by the description of the instance.While generative classifiers estimate class densi-ties, discriminative classifiers like decision lists fo-cus on approximating class boundaries.
Table 1 pro-vides the justifications for choosing these two learn-ers as components in our single-view, multi-learnerbootstrapping algorithm.
Based on observations ofthe coreference task and the features employed byour coreference system, the justifications suggestthat the two learners can potentially compensate foreach other?s weaknesses.4 Multi-View Co-TrainingIn this section, we describe the Blum and Mitchell(B&M) multi-view co-training algorithm and applyit to coreference resolution.3This justifies the use of a decision list as a potential classi-fier for bootstrapping.
See Yarowsky (1995) for details.4.1 The Multi-View Co-Training AlgorithmThe intuition behind the B&M co-training algorithmis to train two classifiers that can help augment eachother?s labeled data by exploiting two separate butredundant views of the data.
Specifically, each clas-sifier is trained using one view of the labeled dataand predicts labels for all instances in the data pool,which consists of a randomly chosen subset of theunlabeled data.
Each then selects its most confidentpredictions, and adds the corresponding instanceswith their predicted labels to the labeled data whilemaintaining the class distribution in the labeled data.The number of instances to be added to the la-beled data by each classifier at each iteration is lim-ited by a pre-specified growth size to ensure thatonly the instances that have a high probability of be-ing assigned the correct label are incorporated.
Thedata pool is replenished with instances from the un-labeled data and the process is repeated.During testing, each classifier makes an indepen-dent decision for a test instance.
In this paper, thedecision associated with the higher confidence istaken to be the final prediction for the instance.4.2 Experimental SetupOne of the goals of the experiments is to enable afair comparison of the multi-view algorithm withour single-view bootstrapping algorithm.
Since theB&M co-training algorithm is sensitive not only tothe views employed but also to other input parame-MUC-6 MUC-7Naive Bayes Decision List Naive Bayes Decision ListExperiments R P F R P F R P F R P FBaseline 50.7 52.6 51.6 17.9 72.0 28.7 40.1 40.2 40.1 32.4 78.3 45.8Multi-view Co-Training 33.3 90.7 48.7 19.5 71.2 30.6 32.9 76.3 46.0 32.4 78.3 45.8Single-view Bootstrapping 53.6 79.0 63.9 40.1 83.1 54.1 43.5 73.2 54.6 38.3 75.4 50.8Self-Training 48.3 63.5 54.9 18.7 70.8 29.6 40.1 40.2 40.1 32.9 78.1 46.3Table 2: Results of multi-view co-training, single-view bootstrapping, and self-training.
Recall, Precision, andF-measure are provided.
Except for the baselines, the best results (F-measure) achieved by the algorithms are shown.ters such as the pool size and the growth size (Pierceand Cardie, 2001), we evaluate the algorithm underdifferent parameter settings, as described below.Evaluation.
We use the MUC-6 (1995) and MUC-7 (1998) coreference data sets for evaluation.
Thetraining set is composed of 30 ?dry run?
texts, fromwhich 491659 and 482125 NP pair instances aregenerated for the MUC-6 and MUC-7 data sets, re-spectively.
Unlike Ng and Cardie (2003) where wechoose one of the dryrun texts (contributing ap-proximately 3500?3700 instances) form the labeleddata set, however, here we randomly select 1000 in-stances.
The remaining instances are used as un-labeled data.
Testing is performed by applying thebootstrapped coreference classifier and the cluster-ing algorithm described in section 2 on the 20?30?formal evaluation?
texts for each of the MUC-6 andMUC-7 data sets.Two sets of experiments are conducted, one usingnaive Bayes as the underlying supervised learningalgorithm and the other the decision list learner.
Allresults reported are averages across five runs.Co-training parameters.
The co-training param-eters are set as follows.Views.
We used three methods to generate theviews from the 25 features used by the coreferencesystem: Mueller et al?s (2002) greedy method, ran-dom splitting of features into views, and splittingof features according to the feature type (i.e.
lexico-syntactic vs. non-lexico-syntactic features).4Pool size.
We tested values of 500, 1000, 5000.Growth size.
We tested values of 10, 50, 100, 200.4.3 Results and DiscussionResults are shown in Table 2, where performance isreported in terms of recall, precision, and F-measure4Space limitation precludes a detailed description of thesemethods.
See Ng and Cardie (2003) for details.0 50 100 150 200 250 300 350 400 450 5002030405060708090100Number of Co?Training IterationsScoreBaselineRecallPrecisionF?measureFigure 1: Learning curve for co-training (pool size= 500, growth size = 50, views formed by randomlysplitting the features) for MUC-6.using the model-theoretic MUC scoring program(Vilain et al, 1995).
The baseline coreference sys-tem, which is trained only on the initially labeleddata using all of the features, achieves an F-measureof 51.6 (NB) and 28.7 (DL) on the MUC-6 data setand 40.1 (NB) and 45.8 (DL) on MUC-7.The results shown in row 2 of Table 2 correspondto the best F-measure scores achieved by co-trainingacross all of the parameter combinations describedin the previous subsection.
In comparison to thebaseline, co-training is able to improve system per-formance in only two of the four classifier/data setcombinations: F-measure increases by 2% and 6%for MUC-6/DL and MUC-7/NB, respectively.
Nev-ertheless, co-training produces high-precision clas-sifiers in all four cases (at the expense of recall).
Inpractical applications in which precision is critical,the co-training classifiers may be preferable to thebaseline classifiers despite the fact that they achievesimilar F-measure scores.Figure 1 depicts the learning curve for the co-training run that gives rise to the best F-measure forthe MUC-6 data set using naive Bayes.
The hor-izontal (dotted) line shows the performance of thebaseline system, as described above.
As co-trainingprogresses, F-measure rises to 48.7 at iteration tenand gradually drops to and stabilizes at 42.9.
We ob-serve similar performance trends for the other clas-sifier/data set combinations.
The drop in F-measureis potentially due to the pollution of the labeled databy mislabeled instances (Pierce and Cardie, 2001).5 Single-View BootstrappingIn this section, we describe and evaluate our single-view, multi-learner bootstrapping algorithm, whichcombines ideas from Goldman and Zhou (2000) andSteedman et al (2003b).
We will start by giving anoverview of these two co-training algorithms.5.1 Related WorkThe Goldman and Zhou (G&Z) Algorithm.This single-view algorithm begins by training twoclassifiers on the initially labeled data using twodifferent learning algorithms; it requires that eachclassifier partition the instance space into a set ofequivalence classes (e.g.
in a decision tree, each leafnode defines an equivalence class).
Each classi-fier then considers each equivalence class and useshypothesis testing to determine if adding all unla-beled instances within the equivalence class to theother classifier?s labeled data will improve the per-formance of its counterparts.
The process is thenrepeated until no more instances can be labeled.The Steedman et al (Ste) Algorithm.
This algo-rithm is a variation of B&M applied to two diversestatistical parsers.
Initially, each parser is trained onthe labeled data.
Each then parses and scores allsentences in the data pool, and then adds the mostconfidently parsed sentences to the training data ofthe other parser.
The parsers are retrained, and theprocess is repeated for several iterations.The algorithm differs from B&M in three mainrespects.
First, the training data of the two parsersdiverge after the first co-training iteration.
Second,the data pool is flushed and refilled entirely with in-stances from the unlabeled data after each iteration.This reduces the possibility of having unreliably la-beled sentences accumulating in the pool.
Finally,the two parsers, each of which is assumed to hold aunique ?view?
of the data, are effectively two differ-ent learning algorithms.5.2 Our Single-View Bootstrapping AlgorithmAs mentioned before, our algorithm uses two dif-ferent learning algorithms to train two classifiers onthe same set of features (i.e.
the full feature set).At each bootstrapping iteration, each classifier la-bels and scores all instances in the data pool.
Thehighest scored instances labeled by one classifier areadded to the training data of the other classifier andvice versa.
Since the two classifiers are trained onthe same view, it is important to maintain a separatetraining set for each classifier: this reduces the prob-ability that the two classifiers converge to the samehypothesis at an early stage and hence implicitly in-creases the ability to bootstrap.
Like Ste, the entiredata pool is replenished with instances drawn fromthe unlabeled data after each iteration, and the pro-cess is repeated.
So our algorithm is effectively Steapplied to coreference resolution ?
instead of twoparsing algorithms that correspond to different fea-tures, we use two learning algorithms, each of whichrelies on the same set of features as in G&Z.
Thesimilarities and differences among B&M, G&Z, Ste,and our algorithm are summarized in Table 3.5.3 Results and DiscussionWe tested different pool sizes and growth sizes asspecified in section 4.2 to determine the best pa-rameter setting for our algorithm.
For both datasets, the best F-measure score is achieved using apool size of 5000 and a growth size of 50.
The re-sults under this parameter setting are given in row3 of Table 2.
In comparison to the baseline, we seedramatic improvement in F-measure for both clas-sifiers and both data sets.
In addition, we see si-multaneous gains in recall and precision in all casesexcept MUC-7/DL.
Furthermore, single-view boot-strapping beats co-training (in terms of F-measurescores) by a large margin in all four cases.
Theseresults provide suggestive evidence that single-view,multi-learner bootstrapping might be a better alter-native to its multi-view, single-learner counterpartsfor coreference resolution.The bootstrapping run that corresponds to this pa-rameter setting for the MUC-6 data set using naiveBayes is shown in Figure 2.
Again, we see a ?typi-Blum and Mitchell Goldman and Zhou Steedman et al OursBootstrapping basis Use different views Use different learners Use different parsers Use different learnersNumber of instancesadded per iterationFixed Variable Fixed FixedTraining sets for thetwo learners/parsersSame Different Different DifferentData pool flushed af-ter each iterationNo N/A (No data pool isused)Yes YesExample selectionmethodHighest scored in-stancesInstances in allequivalance classesthat are expected toimprove a classifierHighest scored sen-tencesHighest scored in-stancesTable 3: Summary of the major similarities and differences among four bootstrapping schemes: Blum andMitchell, Goldman and Zhou, Steedman et al, and ours.
Only the relevant dimensions are discussed here.0 500 1000 1500 2000 2500 3000 3500 400040455055606570758085Number of Co?Training IterationsScoreBaselineRecallPrecisionF?measureFigure 2: Learning curve for our single-view boot-strapping algorithm (pool size = 5000, growth size =50) for MUC-6.cal?
bootstrapping curve: an initial rise in F-measurefollowed by a gradual deterioration.
In comparisonto Figure 1, the recall level achieved by co-training ismuch lower than that of single-view bootstrapping.This appears to indicate that each co-training view isinsufficient for learning the target concept: the fea-ture split limits any interaction of features that canproduce better recall.Finally, Figure 2 shows that performance in-creases most rapidly in the first 200 iterations.
Thisprovides indirect evidence that the two classifiershave acquired different hypotheses from the ini-tial data and are exchanging information with eachother.
To ensure that the classifiers are indeed bene-fiting from each other, we conducted a self-trainingexperiment for each classifier separately: at eachself-training iteration, each classifier labels all 5000instances in the data pool using all available featuresand selects the most confidently labeled 50 instancesfor addition to its labeled data.5 The best F-measurescores achieved by self-training are shown in the lastrow of Table 2.
Overall, self-training only yieldsmarginal performance gains over the baseline.Nevertheless, self-training outperforms co-training in both cases where naive Bayes is used.While these results seem to suggest that co-trainingis inherently handicapped for coreference resolu-tion, there are two plausible explanations againstthis conclusion.
First, the fact that self-training hasaccess to all of the available features may accountfor its superior performance to co-training.
This isagain partially supported by the fact that the recalllevel achieved by co-training is lower than that ofself-training in both cases in which self-trainingoutperforms co-training.
Second, 1000 instancesmay simply not be sufficient for co-training to beeffective for this task: in related work (Ng andCardie, 2003), we find that starting with 3500?3700labeled instances instead of 1000 allows co-trainingto improve the baseline by 4.6% and 9.5% inF-measure using naive Bayes classifiers for theMUC-6 and MUC-7 data sets, respectively.6 An Alternative Ranking MethodAs we have seen before, F-measure scores ulti-mately decrease as bootstrapping progresses.
If thedrop were caused by the degradation in the qualityof the bootstrapped data, then a more ?conservative?instance selection method than that of B&M wouldhelp alleviate this problem.
Our hypothesis is thatselection methods that are based solely on the con-fidence assigned to an instance by a single classifier5Note that this is self-training without bagging, unlike theself-training algorithm discussed in Ng and Cardie (2003).i1> i2if any of the following is true:[?
(C1(i1)) = ?
(C2(i1))] ?
[?
(C1(i2)) = ?(C2(i2))][?
(C1(i1)) = ?
(C2(i1))] ?
[?
(C1(i2)) = ?
(C2(i2))] ?
[|C1(i1) ?
C2(i1)| > |C1(i2) ?
C2(i2)|][?
(C1(i1)) = ?
(C2(i1))] ?
[?
(C1(i2)) = ?
(C2(i2))] ?
[max(C1(i1), 1 ?
C1(i1)) > max(C1(i2), 1 ?
C1(i2))]Figure 3: The ranking method that a binary classifier C1uses to impose a partial ordering on the instancesto be selected and added to the training set of binary classifier C2.
i1and i2are arbitrary instances, and ?
isa function that rounds a number to its closest integer.may be too liberal.
In particular, these methods al-low the addition of instances with opposing labelsto the labeled data; this can potentially result in in-creased incompatibility between the classifiers.Consequently, we develop a new procedure forranking instances in the data pool.
The bootstrap-ping algorithm then selects the highest ranked in-stances to add to the labeled data in each iteration.The method favors instances whose label is agreedupon by both classifiers (Preference 1).
However,incorporating instances that are confidently labeledby both classifiers may reduce the probability ofacquiring new information from the data.
There-fore, the method imposes an additional preferencefor instances that are confidently labeled by one butnot both (Preference 2).
If none of the instancesreceives the same label from the classifiers, themethod resorts to the ?rank-by-confidence?
methodused by B&M (Preference 3).More formally, define a binary classifier as a func-tion that maps an instance to a value that indicatesthe probability that it is labeled as positive.
Now,let ?
be a function that rounds a number to its near-est integer.
Given two binary classifiers C1and C2and instances i1and i2, the ranking method shown inFigure 3 uses the three preferences described aboveto impose a partial ordering on the given instancesfor incorporation into C2?s labeled data.
The methodsimilarly ranks instances to be added to C1?s labeleddata, with the roles of C1and C2reversed.Steedman et al (2003a) also investigate instanceselection methods for co-training, but their goal isprimarily to use selection methods as a means toexplore the trade-off between maximizing coverageand maximizing accuracy.6 In contrast, our focus6McCallum and Nigam (1998) tackle this idea of balancing0 500 1000 1500 2000 2500 3000 3500 40004446485052545658606264Number of Co?Training IterationsF?measureBaselineUsing the B&M ranking methodUsing our ranking methodFigure 4: F-measure curves for our single-viewbootstrapping algorithm with different rankingmethods (pool size = 5000, growth size = 50) forMUC-6.here is on examining whether a more conservativeranking method can alleviate the problem of perfor-mance deterioration.
Nevertheless, Preference 2 isinspired by their Sint-n selection method, which se-lects an instance if it belongs to the intersection ofthe set of the n percent highest scoring instancesof one classifier and the set of the n percent lowestscoring instances of the other.
To our knowledge, noprevious work has examined a ranking method thatcombines the three preferences described above.To compare our ranking procedure with B&M?srank-by-confidence method, we repeat the boot-strapping experiment shown in Figure 2 except thatwe replace B&M?s ranking method with ours.
Thelearning curves generated using the two rankingmethods with naive Bayes for the MUC-6 data setare shown in Figure 4.
The results are consistentwith our intuition regarding the two ranking meth-accuracy and coverage by combining EM and active learning.ods.
The B&M ranking method is more liberal.In particular, each classifier always selects the mostconfidently labeled instances to add to the other?s la-beled data at each iteration.
If the underlying learn-ers have indeed induced two different hypothesesfrom the data, then each classifier can potentially ac-quire informative instances from the other and yieldperformance improvements very rapidly.In contrast, our ranking method is more conserva-tive in that it places more emphasis on maintaininglabeled data accuracy than the B&M method.
Asa result, the classifier learns at a slower rate whencompared to that in the B&M case: it is not until iter-ation 600 that we see a sharp rise in F-measure.
Dueto the ?liberal?
nature of the B&M method, however,its performance drops dramatically as bootstrappingprogresses, whereas ours just dips temporarily.
Thiscan potentially be attributed to the more rapid injec-tion of mislabeled instances into the labeled data inthe B&M case.
At iteration 2800, our method startsto outperform B&M?s.
Overall, our ranking methoddoes not exhibit the performance trend observedwith the B&M method: except for the spike betweeniterations 0 and 100, F-measure does not deteriorateas bootstrapping progresses.
Since it is hard to deter-mine a ?good?
stopping point for bootstrapping dueto the paucity of labeled data in a weakly supervisedsetting, our ranking method can potentially serve asan alternative to the B&M method.7 ConclusionsWe have proposed a single-view, multi-learner boot-strapping algorithm for coreference resolution andshown empirically that the algorithm is a better al-ternative to the Blum and Mitchell co-training al-gorithm for this task for which no natural featuresplit has been found.
In addition, we have investi-gated an example ranking method for bootstrappingthat, unlike Blum and Mitchell?s rank-by-confidencemethod, can potentially alleviate the problem of per-formance deterioration due to the pollution of the la-beled data in the course of bootstrapping.AcknowledgmentsWe thank the anonymous reviewers for their invalu-able and insightful comments.
This work was sup-ported in part by NSF Grant IIS?0208028.ReferencesMichele Banko and Eric Brill.
2001.
Scaling to very very largecorpora for natural language disambiguation.
In Proceed-ings of the ACL/EACL, pages 26?33.Avrim Blum and Tom Mitchell.
1998.
Combining labeled andunlabeled data with co-training.
In Proceedings of COLT,pages 92?100.Michael Collins and Yoram Singer.
1999.
Unsupervisedmodels for named entity classification.
In Proceedings ofEMNLP/VLC, pages 100?110.Sally Goldman and Yan Zhou.
2000.
Enhancing supervisedlearning with unlabeled data.
In Proceedings of ICML, pages327?334.Andrew McCallum and Kamal Nigam.
1998.
Employing EMand pool-based active learning for text classification.
In Pro-ceedings of ICML, pages 359?367.MUC-6.
1995.
Proceedings of the Sixth Message Understand-ing Conference (MUC-6).MUC-7.
1998.
Proceedings of the Seventh Message Under-standing Conference (MUC-7).Christoph Mueller, Stefan Rapp, and Michael Strube.
2002.Applying co-training to reference resolution.
In Proceedingsof the ACL, pages 352?359.Ion Muslea, Steven Minton, and Craig Knoblock.
2002.
Active+ Semi-Supervised Learning = Robust Multi-View Learning.In Proceedings of ICML.Vincent Ng and Claire Cardie.
2002.
Combining sample selec-tion and error-driven pruning for machine learning of coref-erence rules.
In Proceedings of EMNLP, pages 55?62.Vincent Ng and Claire Cardie.
2003.
Weakly supervised natu-ral language learning without redundant views.
In Proceed-ings of HLT-NAACL.Kamal Nigam and Rayid Ghani.
2000.
Analyzing the effec-tiveness and applicability of co-training.
In Proceedings ofCIKM, pages 86?93.Kamal Nigam, Andrew McCallum, Sabastian Thrun, andTom Mitchell.
2000.
Text classification from labeledand unlabeled documents using EM.
Machine Learning,39(2/3):103?134.David Pierce and Claire Cardie.
2001.
Limitations of co-training for natural language learning from large datasets.
InProceedings of EMNLP, pages 1?9.Ellen Riloff and Rosie Jones.
1999.
Learning dictionaries forinformation extraction by multi-level bootstrapping.
In Pro-ceedings of AAAI, pages 474?479.M.
Steedman, R. Hwa, S. Clark, M. Osborne, A. Sarkar,J.
Hockenmaier, P. Ruhlen, S. Baker, and J. Crim.
2003a.Example selection for bootstrapping statistical parsers.
InProceedings of HLT-NAACL.M.
Steedman, M. Osborne, A. Sarkar, S. Clark, R. Hwa,J.
Hockenmaier, P. Ruhlen, S. Baker, and J. Crim.
2003b.Bootstrapping statistical parsers from small datasets.
In Pro-ceedings of the EACL.M.
Vilain, J. Burger, J. Aberdeen, D. Connolly, andL.
Hirschman.
1995.
A model-theoretic coreference scoringscheme.
In Proceedings of the Sixth MessageUnderstandingConference (MUC-6), pages 45?52.David Yarowsky.
1995.
Unsupervised word sense disambigua-tion rivaling supervised methods.
In Proceedingsof the ACL,pages 189?196.
