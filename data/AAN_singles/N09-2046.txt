Proceedings of NAACL HLT 2009: Short Papers, pages 181?184,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsImproving SCL Model for Sentiment-Transfer LearningSongbo TanInstitute of Computing TechnologyBeijing, Chinatansongbo@software.ict.ac.cnXueqi ChengInstitute of Computing TechnologyBeijing, Chinacxq@ict.ac.cnABSTRACTIn recent years, Structural CorrespondenceLearning (SCL) is becoming one of the mostpromising techniques for sentiment-transferlearning.
However, SCL model treats eachfeature as well as each instance by anequivalent-weight strategy.
To address the twoissues effectively, we proposed a weightedSCL model (W-SCL), which weights thefeatures as well as the instances.
Morespecifically, W-SCL assigns a smaller weightto high-frequency domain-specific (HFDS)features and assigns a larger weight toinstances with the same label as the involvedpivot feature.
The experimental resultsindicate that proposed W-SCL model couldovercome the adverse influence of HFDSfeatures, and leverage knowledge from labelsof instances and pivot features.1   IntroductionIn the community of sentiment analysis (Turney2002; Pang et al, 2002; Tang et al, 2009),transferring a sentiment classifier from one sourcedomain to another target domain is still far from atrivial work, because sentiment expression oftenbehaves with strong domain-specific nature.Up to this time, many researchers haveproposed techniques to address this problem, suchas classifiers adaptation, generalizable featuresdetection and so on (DaumeIII et al, 2006; Jianget al, 2007; Tan et al, 2007; Tan et al, 2008; Tanet al, 2009).
Among these techniques, SCL(Structural Correspondence Learning) (Blitzer etal., 2006) is regarded as a promising method totackle transfer-learning problem.
The main ideabehind SCL model is to identify correspondencesamong features from different domains bymodeling their correlations with pivot features (orgeneralizable features).
Pivot features behavesimilarly in both domains.
If non-pivot featuresfrom different domains are correlated with manyof the same pivot features, then we assume themto be corresponded with each other, and treat themsimilarly when training a sentiment classifier.However, SCL model treats each feature as wellas each instance by an equivalent-weight strategy.From the perspective of feature, this strategy failsto overcome the adverse influence of high-frequency domain-specific (HFDS) features.
Forexample, the words ?stock?
or ?market?
occursfrequently in most of stock reviews, so these non-sentiment features tend to have a strongcorrespondence with pivot features.
As a result,the representative ability of the other sentimentfeatures will inevitably be weakened to somedegree.To address this issue, we proposed FrequentlyExclusively-occurring Entropy (FEE) to pick outHFDS features, and proposed a feature-weightedSCL model (FW-SCL) to adjust the influence ofHFDS features in building correspondence.
Themain idea of FW-SCL is to assign a smallerweight to HFDS features so that the adverseinfluence of HFDS features can be decreased.From the other perspective, the equivalent-weight strategy of SCL model ignores the labels(?positive?
or ?negative?)
of labeled instances.Obviously, this is not a good idea.
In fact, positivepivot features tend to occur in positive instances,so the correlations built on positive instances aremore reliable than that built on negative instances;and vice versa.
Consequently, utilization of labelsof instances and pivot features can decrease theadverse influence of some co-occurrences, such asco-occurrences involved with positive pivotfeatures and negative instances, or involved withnegative pivot features and positive instances.In order to take into account the labels oflabeled instances, we proposed an instance-weighted SCL model (IW-SCL), which assigns alarger weight to instances with the same label asthe involved pivot feature.
In this time, we obtaina combined model: feature-weighted and instance-weighted SCL model (FWIW-SCL).
For the sake181of convenience, we simplify ?FWIW-SCL?
as?W-SCL?
in the rest of this paper.2   Structural Correspondence LearningIn the section, we provide the detailed proceduresfor SCL model.First we need to pick out pivot features.
Pivotfeatures occur frequently in both the source andthe target domain.
In the community of sentimentanalysis, generalizable sentiment words are goodcandidates for pivot features, such as ?good?
and?excellent?.
In the rest of this paper, we use K tostand for the number of pivot features.Second, we need to compute the pivotpredictors (or mapping vectors) using selectedpivot features.
The pivot predictors are the key job,because they directly decide the performance ofSCL.
For each pivot feature k, we use a lossfunction Lk, ( ) 21)( wxwxpLi iTikk ?+?=?
(1)where the function pk(xi) indicates whether thepivot feature k occurs in the instance xi,otherwisexifxp ikik011)(>???
?= ,where the weight vector w encodes thecorrespondence of the non-pivot features with thepivot feature k (Blitzer et al, 2006).Finally we use the augmented space [xT, xTW]T totrain the classifier on the source labeled data andpredict the examples on the target domain, whereW=[w1,w2, ?, wK].3   Feature-Weighted SCL Model3.1 Measure to pick out HFDS featuresIn order to pick out HFDS features, we proposedFrequently Exclusively-occurring Entropy (FEE).Our measure includes two criteria: occur in onedomain as frequently as possible, while occur onanother domain as rarely as possible.
To satisfythis requirement, we proposed the followingformula:( )( ) ( )( ) ???????
?+=)(),(min)(),(maxlog)(),(maxlogwPwPwPwPwPwPfnononow(2)where Po(w) and Pn(w) indicate the probability ofword w in the source domain and the targetdomain respectively:( )( )??
?++=2)()(ooo NwNwP                     (3)( )( )??
?++=2)()(nnn NwNwP                     (4)where No(w) and Nn(w) is the number of exampleswith word w in the source domain and the targetdomain respectively; No and Nn is the number ofexamples in the source domain and the targetdomain respectively.
In order to overcomeoverflow, we set ?=0.0001 in our experimentreported in section 5.To better understand this measure, let?s take asimple example (see Table 1).
Given a sourcedataset with 1000 documents and a target datasetwith 1000 documents, 12 candidate features, and atask to pick out 2 HFDS features.
According toour understanding, the best choice is to pick outw4 and w8.
According to formula (2), fortunately,we successfully pick out w4, and w8.
This simpleexample validates the effectiveness of proposedFEE formula.Table 1: A simple example for FEEFEEWords No(w) Nn(w) Score Rankw1 100 100 -2.3025 6w2 100 90 -2.1971 4w3 100 45 -1.5040 3w4 100 4 0.9163 1w5 50 50 -2.9956 8w6 50 45 -2.8903 7w7 50 23 -2.2192 5w8 50 2 0.2231 2w9 4 4 -5.5214 11w10 4 3 -5.2337 10w11 4 2 -4.8283 9w12 1 1 -6.9077 123.2 Feature-Weighted SCL modelTo adjust the influence of HFDS features inbuilding correspondence, we proposed feature-weighted SCL model (FW-SCL), ( ) 21)( wxwxpLi ill llikk??
+?=?
?
(5)where the function pk(xi) indicates whether thepivot feature k occurs in the instance xi;otherwisexifxp ikik011)(>???
?= ,and ?l is the parameter to control the weight of theHFDS feature l,182otherwiseZlif HFDSl????=1?
?where ZHFDS indicates the HFDS feature set and ?is located in the range [0,1].
When ?
?=0?, itindicates that no HFDS features are used to buildthe correspondence vectors; while ??=1?
indicatesthat all features are equally used to build thecorrespondence vectors, that is to say, proposedFW-SCL algorithm is simplified as traditionalSCL algorithm.
Consequently, proposed FW-SCLalgorithm could be regarded as a generalizedversion of traditional SCL algorithm.4 Instance-Weighted SCL ModelThe traditional SCL model does not take intoaccount the labels (?positive?
or ?negative?)
ofinstances on the source domain and pivot features.Although the labels of pivot features are not givenat first, it is very easy to obtain these labelsbecause the number of pivot features is typicallyvery small.Obviously, positive pivot features tend to occurin positive instances, so the correlations built onpositive instances are more reliable than thecorrelations built on negative instances; and viceversa.
As a result, the ideal choice is to assign alarger weight to the instances with the same labelas the involved pivot feature, while assign asmaller weight to the instances with the differentlabel as the involved pivot feature.
This strategycan make correlations more reliable.
This is thekey idea of instance-weighted SCL model (IW-SCL).
Combining the idea of feature-weightedSCL model (FW-SCL), we obtain the feature-weighted and instance-weighted SCL model(FWIW-SCL),( )( ) ( )( )( ) ( )( )( ) ( )( )?
??
?????++?
?=1),(111),(2jll lljkjill llikikxwxpxkwxwxpxkL???????????
(6)where ?
is the instance weight and the functionpk(xi) indicates whether the pivot feature k occursin the instance xi;otherwisexifxp ikik011)(>???
?=and ?l is the parameter to control the weight of theHFDS feature l,otherwiseZlif HFDSl????=1??
,where ZHFDS indicates the HFDS feature set and ?is located in the range [0,1].In equation (6), the function ?
(z,y) indicateswhether the two variables z and y have the samenon-zero value,( )otherwise0y and zzifz,y?=???=01?
;and the function ?
(z) is a hinge function, whosevariables are either pivot features or instances,labelnegativez has aifunknownlabelpositivez has aifz101)(??????=?
.For the sake of convenience, we simplify?FWIW-SCL?
as ?W-SCL?.5   Experimental Results5.1 DatasetsWe collected three Chinese domain-specificdatasets: Education Reviews (Edu, fromhttp://blog.sohu.com/learning/), Stock Reviews (Sto,from http://blog.sohu.com/stock/) and ComputerReviews (Comp, from http://detail.zol.com.cn/).
All ofthese datasets are annotated by three linguists.
Weuse ICTCLAS (a Chinese text POS tool,http://ictclas.org/) to parse Chinese words.The dataset Edu includes 1,012 negativereviews and 254 positive reviews.
The averagesize of reviews is about 600 words.
The datasetSto consists of 683 negative reviews and 364positive reviews.
The average length of reviews isabout 460 terms.
The dataset Comp contains 390negative reviews and 544 positive reviews.
Theaverage length of reviews is about 120 words.5.2 Comparison MethodsIn our experiments, we run one supervisedbaseline, i.e., Na?ve Bayes (NB), which only usesone source-domain labeled data as training data.For transfer-learning baseline, we implementtraditional SCL model (T-SCL) (Blitzer et al,2006).
Like TSVM, it makes use of the source-domain labeled data as well as the target-domainunlabeled data.5.3 Does proposed method work?To conduct our experiments, we use source-domain data as unlabeled set or labeled trainingset, and use target-domain data as unlabeled set ortesting set.
Note that we use 100 manual-annotated pivot features for T-SCL, FW-SCL andW-SCL in the following experiments.
We select183pivot features use three criteria: a) is a sentimentword; b) occurs frequently in both domains; c) hassimilar occurring probability.
For T-SCL, FW-SCL and W-SCL, we use prototype classifier(Sebastiani, 2002) to train the final model.Table 2 shows the results of experimentscomparing proposed method with supervisedlearning, transductive learning and T-SCL.
ForFW-SCL, the ZHFDS is set to 200 and ?
is set to 0.1;For W-SCL, the ZHFDS is set to 200, ?
is set to 0.1,and ?
is set to 0.9.As expected, proposed method FW-SCL doesindeed provide much better performance thansupervised baselines, TSVM and T-SCL model.For example, the average accuracy of FW-SCLbeats supervised baselines by about 12 percents,beats TSVM by about 11 percents and beats T-SCL by about 10 percents.
This result indicatesthat proposed FW-SCL model could overcome theshortcomings of HFDS features in buildingcorrespondence vectors.More surprisingly, instance-weighting strategycan further boost the performance of FW-SCL byabout 4 percents.
This result indicates that thelabels of instances and pivot features are veryuseful in building the correlation vectors.
Thisresult also verifies our analysis in section 4:positive pivot features tend to occur in positiveinstances, so the correlations built on positiveinstances are more reliable than the correlationsbuilt on negative instances, and vice versa.Table 2: Accuracy of different methodsNB T-SCL FW-SCL W-SCLEdu->Sto 0.6704 0.7965 0.7917 0.8108Edu->Comp 0.5085 0.8019 0.8993 0.9025Sto->Edu 0.6824 0.7712 0.9072 0.9368Sto->Comp 0.5053 0.8126 0.8126 0.8693Comp->Sto 0.6580 0.6523 0.7010 0.7717Comp->Edu 0.6114 0.5976 0.9112 0.9408Average 0.6060 0.7387 0.8372 0.8720Although SCL is a method designed for transferlearning, but it cannot provide better performancethan TSVM.
This result verifies the analysis insection 3: a small amount of HFDS featuresoccupy a large amount of weight in classificationmodel, but hardly carry corresponding sentiment.In another word, very few top-frequency wordsdegrade the representative ability of SCL modelfor sentiment classification.6 Conclusion RemarksIn this paper, we proposed a weighted SCLmodel (W-SCL) for domain adaptation in thecontext of sentiment analysis.
On six domain-transfer tasks, W-SCL consistently produces muchbetter performance than the supervised, semi-supervised and transfer-learning baselines.
As aresult, we can say that proposed W-SCL modeloffers a better choice for sentiment-analysisapplications that require high-precisionclassification but hardly have any labeled trainingdata.7 AcknowledgmentsThis work was mainly supported by two funds,i.e., 0704021000 and 60803085, and one anotherproject, i.e., 2004CB318109.ReferencesBlitzer, J. and McDonald, R. and Fernando Pereira.Domain adaptation with structural correspondencelearning.
EMNLP 2006.DaumeIII, H. and Marcu, D. Domain adaptation forstatistical classifiers.
Journal of ArtificialIntelligence Research, 2006, 26: 101-126.Jiang, J., Zhai, C. A Two-Stage Approach to DomainAdaptation for Statistical Classifiers.
CIKM 2007.Pang, B., Lee, L. and Vaithyanathan, S. Thumbs up?Sentiment classification using machine learningtechniques.
EMNLP 2002.Sebastiani, F. Machine learning in automated textcategorization.
ACM Computing Surveys.
2002,34(1): 1-47.S.
Tan, G. Wu, H. Tang and X. Cheng.
A novelscheme for domain-transfer problem in the contextof sentiment analysis.
CIKM 2007.S.
Tan, Y. Wang, G. Wu and X. Cheng.
Usingunlabeled data to handle domain-transfer problem ofsemantic detection.
SAC 2008.S.
Tan, X. Cheng, Y. Wang and H. Xu.
AdaptingNaive Bayes to Domain Adaptation for SentimentAnalysis.
ECIR 2009.H.
Tang, S. Tan, and X. Cheng.
A Survey onSentiment Detection of Reviews.
Expert Systemswith Applications.
Elsevier.
2009,doi:10.1016/j.eswa.2009.02.063.Turney, P. D. Thumbs Up or Thumbs Down?
SemanticOrientation Applied to Unsupervised Classificationof Reviews.
ACL 2002.184
