Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1155?1165,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsA Semiparametric Gaussian Copula Regression Modelfor Predicting Financial Risks from Earnings CallsWilliam Yang WangSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213yww@cs.cmu.eduZhenhao HuaSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213zhua@cs.cmu.eduAbstractEarnings call summarizes the financialperformance of a company, and it is animportant indicator of the future financialrisks of the company.
We quantitativelystudy how earnings calls are correlatedwith the financial risks, with a special fo-cus on the financial crisis of 2009.
In par-ticular, we perform a text regression task:given the transcript of an earnings call, wepredict the volatility of stock prices fromthe week after the call is made.
We pro-pose the use of copula: a powerful statis-tical framework that separately models theuniform marginals and their complex mul-tivariate stochastic dependencies, whilenot requiring any prior assumptions on thedistributions of the covariate and the de-pendent variable.
By performing probabil-ity integral transform, our approach movesbeyond the standard count-based bag-of-words models in NLP, and improves pre-vious work on text regression by incor-porating the correlation among local fea-tures in the form of semiparametric Gaus-sian copula.
In experiments, we showthat our model significantly outperformsstrong linear and non-linear discriminativebaselines on three datasets under varioussettings.1 IntroductionPredicting the risks of publicly listed companies isof great interests not only to the traders and ana-lysts on the Wall Street, but also virtually anyonewho has investments in the market (Kogan et al,2009).
Traditionally, analysts focus on quantita-tive modeling of historical trading data.
Today,even though earnings calls transcripts are abun-dantly available, their distinctive communicativepractices (Camiciottoli, 2010), and correlationswith the financial risks, in particular, future stockperformances (Price et al, 2012), are not wellstudied in the past.Earnings calls are conference calls where alisted company discusses the financial perfor-mance.
Typically, a earnings call contains twoparts: the senior executives first report the oper-ational outcomes, as well as the current financialperformance, and then discuss their perspectiveson the future of the company.
The second part ofthe teleconference includes a question answeringsession where the floor will be open to investors,analysts, and other parties for inquiries.
The ques-tion we ask is that, even though each earnings callhas distinct styles, as well as different speakersand mixed formats, can we use earnings calls topredict the financial risks of the company in thelimited future?Given a piece of earnings call transcript, weinvestigate a semiparametric approach for auto-matic prediction of future financial risk1.
To dothis, we formulate the problem as a text regres-sion task, and use a Gaussian copula with prob-ability integral transform to model the uniformmarginals and their dependencies.
Copula mod-els (Schweizer and Sklar, 1983; Nelsen, 1999)are often used by statisticians (Genest and Favre,2007; Liu et al, 2012; Masarotto and Varin, 2012)and economists (Chen and Fan, 2006) to study thebivariate and multivariate stochastic dependencyamong random variables, but they are very newto the machine learning (Ghahramani et al, 2012;Han et al, 2012; Xiang and Neville, 2013; Lopez-paz et al, 2013) and related communities (Eick-hoff et al, 2013).
To the best of our knowledge,even though the term ?copula?
is named for theresemblance to grammatical copulas in linguistics,copula models have not been explored in the NLPcommunity.
To evaluate the performance of ourapproach, we compare with a standard squaredloss linear regression baseline, as well as strongbaselines such as linear and non-linear support1In this work, the risk is defined as the measured volatil-ity of stock prices from the week following the earnings callteleconference.
See details in Section 5.1155vector machines (SVMs) that are widely used intext regression tasks.
By varying different exper-imental settings on three datasets concerning dif-ferent periods of the Great Recession from 2006-2013, we empirically show that our approach sig-nificantly outperforms the baselines by a widemargin.
Our main contributions are:?
We are among the first to formally study tran-scripts of earnings calls to predict financialrisks.?
We propose a novel semiparametric Gaussiancopula model for text regression.?
Our results significantly outperform standardlinear regression and strong SVM baselines.?
By varying the number of dimensions of thecovariates and the size of the training data,we show that the improvements over thebaselines are robust across different param-eter settings on three datasets.In the next section, we outline related work inmodeling financial reports and text regression.
InSection 3, the details of the semiparametric cop-ula model are introduced.
We then describe thedataset and dependent variable in this study, andthe experiments are shown in Section 6.
We dis-cuss the results and findings in Section 7 and thenconclude in Section 8.2 Related WorkFung et al (2003) are among the first to studySVM and text mining methods in the market pre-diction domain, where they align financial newsarticles with multiple time series to simulate the33 stocks in the Hong Kong Hang Seng Index.However, text regression in the financial domainhave not been explored until recently.
Kogan etal.
(2009) model the SEC-mandated annual re-ports, and performs linear SVM regression with-insensitive loss function to predict the mea-sured volatility.
Another recent study (Wang etal., 2013) uses exactly the same max-margin re-gression technique, but with a different focus onthe financial sentiment.
Using the same dataset,Tsai and Wang (2013) reformulate the regressionproblem as a text ranking problem.
Note thatall these regression studies above investigate theSEC-mandated annual reports, which are very dif-ferent from the earnings calls in many aspects suchas length, format, vocabulary, and genre.
Mostrecently, Xie et al (2013) have proposed the useof frame-level semantic features to understand fi-nancial news, but they treat the stock movementprediction problem as a binary classification task.Broadly speaking, our work is also aligned to re-cent studies that make use of social media datato predict the stock market (Bollen et al, 2011;Zhang et al, 2011).Despite our financial domain, our approach ismore relevant to text regression.
Traditional dis-criminative models, such as linear regression andlinear SVM, have been very popular in varioustext regression tasks, such as predicting movie rev-enues from reviews (Joshi et al, 2010), under-standing the geographic lexical variation (Eisen-stein et al, 2010), and predicting food prices frommenus (Chahuneau et al, 2012).
The advantage ofthese models is that the estimation of the parame-ters is often simple, the results are easy to inter-pret, and the approach often yields strong perfor-mances.
While these approaches have merits, theysuffer from the problem of not explicitly model-ing the correlations and interactions among ran-dom variables, which in some sense, correspond-ing to the impractical assumption of independentand identically distributed (i.i.d) of the data.
Forexample, when bag-of-word-unigrams are presentin the feature space, it is easier if one does not ex-plicitly model the stochastic dependencies amongthe words, even though doing so might hurt thepredictive power, while the variance from the cor-relations among the random variables is not ex-plained.3 Copula Models for Text RegressionIn NLP, many statistical machine learning meth-ods that capture the dependencies among ran-dom variables, including topic models (Blei et al,2003; Lafferty and Blei, 2005; Wang et al, 2012),always have to make assumptions with the under-lying distributions of the random variables, andmake use of informative priors.
This might berather restricting the expressiveness of the modelin some sense (Reisinger et al, 2010).
On theother hand, once such assumptions are removed,another problem arises ?
they might be prone toerrors, and suffer from the overfitting issue.
There-fore, coping with the tradeoff between expressive-ness and overfitting, seems to be rather importantin statistical approaches that capture stochastic de-pendency.Our proposed semiparametric copula regressionmodel takes a different perspective.
On one hand,copula models (Nelsen, 1999) seek to explicitlymodel the dependency of random variables by sep-arating the marginals and their correlations.
Onthe other hand, it does not make use of any as-1156sumptions on the distributions of the random vari-ables, yet, the copula model is still expressive.This nice property essentially allows us to fusedistinctive lexical, syntactic, and semantic featuresets naturally into a single compact model.From an information-theoretic point ofview (Shannon, 1948), various problems in textanalytics can be formulated as estimating theprobability mass/density functions of tokensin text.
In NLP, many of the probabilistic textmodels work in the discrete space (Church andGale, 1995; Blei et al, 2003), but our model isdifferent: since the text features are sparse, wefirst perform kernel density estimates to smoothout the zeroing items, and then calculate theempirical cumulative distribution function (CDF)of the random variables.
By doing this, weare essentially performing probability integraltransform?
an important statistical techniquethat moves beyond the count-based bag-of-wordsfeature space to marginal cumulative densityfunctions space.
Last but not least, by usinga parametric copula, in our case, the Gaussiancopula, we reduce the computational cost fromfully nonparametric methods, and explicitlymodel the correlations among the covariate andthe dependent variable.In this section, we first briefly look at thetheoretical foundations of copulas, including theSklar?s theorem.
Then we describe the proposedsemiparametric Gaussian copula text regressionmodel.
The algorithmic implementation of our ap-proach is introduced at the end of this section.3.1 The Theory of CopulaIn the statistics literature, copula is widely knownas a family of distribution function.
The idea be-hind copula theory is that the cumulative distri-bution function (CDF) of a random vector can berepresented in the form of uniform marginal cu-mulative distribution functions, and a copula thatconnects these marginal CDFs, which describesthe correlations among the input random variables.However, in order to have a valid multivariate dis-tribution function regardless of n-dimensional co-variates, not every function can be used as a copulafunction.
The central idea behind copula, there-fore, can be summarize by the Sklar?s theorem andthe corollary.Theorem 1 (Sklar?s Theorem (1959)) Let Fbe the joint cumulative distribution functionof n random variables X1, X2, ..., Xn.
Letthe corresponding marginal cumulative dis-tribution functions of the random variable beF1(x1), F2(x2), ..., Fn(xn).
Then, if the marginalfunctions are continuous, there exists a uniquecopula C, such thatF (x1, ..., xn) = C[F1(x1), ..., Fn(xn)].
(1)Furthermore, if the distributions are continuous,the multivariate dependency structure and themarginals might be separated, and the copula canbe considered independent of the marginals (Joe,1997; Parsa and Klugman, 2011).
Therefore, thecopula does not have requirements on the marginaldistributions, and any arbitrary marginals can becombined and their dependency structure can bemodeled using the copula.
The inverse of Sklar?sTheorem is also true in the following:Corollary 1 If there exists a copula C : (0, 1)nand marginal cumulative distribution func-tions F1(x1), F2(x2), ..., Fn(xn), thenC[F1(x1), ..., Fn(xn)] defines a multivariatecumulative distribution function.3.2 Semiparametric Gaussian Copula ModelsThe Non-Parametric EstimationWe formulate the copula regression model as fol-lows.
Assume we have n random variables of textfeatures X1, X2, ..., Xn.
The problem is that textfeatures are sparse, so we need to perform non-parametric kernel density estimation to smooth outthe distribution of each variable.
Let f1, f2, ..., fnbe the unknown density, we are interested in de-riving the shape of these functions.
Assume wehave m samples, the kernel density estimator canbe defined as:?fh(x) =1mm?i=1Kh(x?
xi) (2)=1mhm?i=1K(x?
xih)(3)Here, K(?)
is the kernel function, where in ourcase, we use the Box kernel2K(z):K(z) =12, |z| ?
1, (4)= 0, |z| > 1.
(5)Comparing to the Gaussian kernel and other ker-nels, the Box kernel is simple, and computation-ally inexpensive.
The parameter h is the band-width for smoothing3.2It is also known as the original Parzen windows (Parzen,1962).3In our implementation, we use the default h of the Boxkernel in the ksdensity function in Matlab.1157Now, we can derive the empiri-cal cumulative distribution functions?FX1(?f1(X1)),?FX2(?f2(X2)), ...,?FXn(?fn(Xn)) ofthe smoothed covariates, as well as the dependentvariable y and its CDF?Fy(?f(y)).
The empiricalcumulative distribution functions are defined as:?F (?)
=1mm?i=1I{xi?
?}
(6)where I{?}
is the indicator function, and ?
in-dicates the current value that we are evaluating.Note that the above step is also known as prob-ability integral transform (Diebold et al, 1997),which allows us to convert any given continuousdistribution to random variables having a uniformdistribution.
This is of crucial importance to mod-eling text data: instead of using the classic bag-of-words representation that uses raw counts, we arenow working with uniform marginal CDFs, whichhelps coping with the overfitting issue due to noiseand data sparsity.The Parametric Copula EstimationNow that we have obtained the marginals, and thenthe joint distribution can be constructed by apply-ing the copula function that models the stochasticdependencies among marginal CDFs:?F (?f1(X1), ...,?f1(Xn),?f(y)) (7)= C[?FX1(?f1(X1)), ...,?FXn(?fn(Xn)),?Fy(?fy(y))] (8)In this work, we apply the parametric Gaussiancopula to model the correlations among the textfeatures and the label.
Assume xiis the smoothedversion of random variable Xi, and y is thesmoothed label, we have:F (x1, ..., xn, y) (9)= ??(?
?1[Fx1(x1)], ..., ,??1[Fxn(xn)],?
?1[Fy(y)])(10)where ?
?is the joint cumulative distribution func-tion of a multivariate Gaussian with zero mean and?
variance.
?
?1is the inverse CDF of a standardGaussian.
In this parametric part of the model, theparameter estimation boils down to the problem oflearning the covariance matrix ?
of this Gaussiancopula.
In this work, we perform standard maxi-mum likelihood estimation for the ?
matrix.To calibrate the ?
matrix, we make use ofthe power of randomness: using the initial ?from MLE, we generate random samples fromthe Gaussian copula, and then concatenate previ-ously generated joint of Gaussian inverse marginalCDFs with the newly generated random copulanumbers, and re-estimate using MLE to derive thefinal adjusted ?.
Note that the final ?
matrix hasto be symmetric and positive definite.Computational ComplexityOne important question regarding the proposedsemiparametric Gaussian copula model is the cor-responding computational complexity.
This boilsdown to the estimation of the??
matrix (Liu et al,2012): one only needs to calculate the correla-tion coefficients of n(n ?
1)/2 pairs of randomvariables.
Christensen (2005) shows that sort-ing and balanced binary trees can be used to cal-culate the correlation coefficients with complex-ity of O(n log n).
Therefore, the computationalcomplexity of MLE for the proposed model isO(n log n).Efficient Approximate InferenceIn this regression task, in order to performexact inference of the conditional probabilitydistribution p(Fy(y)|Fx1(x1), ..., Fxn(xn)),one needs to solve the mean response?E(Fy(y)|Fx1(x1), ..., Fx1(x1)) from a jointdistribution of high-dimensional Gaussian copula.Assume in the simple bivariate case of Gaussiancopula regression, the covariance matrix ?
is:?
=[?11?12?22]We can easily derive the conditional density thatcan be used to calculate the expected value of theCDF of the label:C(Fy(y)|Fx1(x1); ?)
=1|?22?
?T12??111?12|12exp(?12?T([?22?
?T12??111?12]?1?
I)?
)(11)where ?
= ??1[Fy(y)]?
?T12??111?
?1[Fx1(x1)].Unfortunately, the exact inference can be in-tractable in the multivariate case, and approximateinference, such as Markov Chain Monte Carlosampling (Gelfand and Smith, 1990; Pitt et al,2006) is often used for posterior inference.
In thiswork, we propose an efficient sampling methodto derive y given the text features ?
we sampleFy(y) s.t.
it maximizes the joint high-dimensionalGaussian copula density:?Fy(y) ?
arg maxFy(y)?
(0,1)1?det ?exp(?12?T?(??1?
I)??)(12)1158where?
=???????1(Fx1(x1))...??1(Fxn(xn))??1(Fy(y))????
?Again, the reason why we perform approxi-mated inference is that: exact inference in thehigh-dimensional Gaussian copula density is non-trivial, and might not have analytical solutions,but approximate inference using maximum den-sity sampling from the Gaussian copula signifi-cantly relaxes the complexity of inference.
Fi-nally, to derive y?, the last step is to compute theinverse CDF of?Fy(y).3.3 Algorithmic ImplementationThe algorithmic implementation of our semipara-metric Gaussian copula text regression model isshown in Algorithm 1.
Basically, the algorithmcan be decomposed into four parts:?
Perform nonparametric Box kernel densityestimates of the covariates and the dependentvariable for smoothing.?
Calculate the empirical cumulative distribu-tion functions of the smoothed random vari-ables.?
Estimate the parameters (covariance ?)
of theGaussian copula.?
Infer the predicted value of the dependentvariable by sampling the Gaussian copulaprobability density function.4 DatasetsWe use three datasets4of transcribed quarterlyearnings calls from the U.S. stock market, focus-ing on the period of the Great Recession.The pre-2009 dataset consists of earnings callsfrom the period of 2006-2008, which includescalls from the beginning of economic downturn,the outbreak of the subprime mortgage crisis, andthe epidemic of collapses of large financial insti-tutions.
The 2009 dataset contains earnings callsfrom the year of 2009, which is a period where thecredit crisis spreads globally, and the Dow JonesIndustrial Average hit the lowest since the begin-ning of the millennium.
The post-2009 dataset in-cludes earnings calls from the period of 2010 to2013, which concerns the recovery of global econ-omy.
The detailed statistics is shown in Table 1.4http://www.cs.cmu.edu/?yww/data/earningscalls.zipAlgorithm 1 A Semi-parametric Gaussian CopulaModel Based Text Regression AlgorithmGiven:(1) training data (X(tr), ~y(tr));(2) testing data (X(te), ~y(te));Learning:for i = 1?
n dimensions doX(tr)?i?
BoxKDE(X(tr)i, X(tr)i);U(tr)i?
EmpiricalCDF (X(tr)?i);X(te)?i?
BoxKDE(X(tr)i, X(te)i);U(te)i?
EmpiricalCDF (X(te)?i);end fory(tr)??
BoxKDE(y(tr), y(tr));v(tr)?
EmpiricalCDF (y(tr)?);Z(tr)?
GaussianInverseCDF ([U(tr)v(tr)]);???
CorrelationCoefficients(Z(tr));r ?MultiV ariateGaussianRandNum(0,?
?, n);Z(tr)?= GaussianCDF (r);???
CorrelationCoefficients([Z(tr)Z(tr)?
]);Inference:for j = 1?
m instances domaxj?
0;?Y?= 0;for k = 0.01?
1 doZ(te)?
GaussianInverseCDF ([U(te)k]);pj=MultiV ariateGaussianPDF (Z(te),??
)?nGaussianPDF (Z(te));if pj?
maxjthenmaxj= pj;?Y?= k;end ifend forend fory?
?
InverseCDF (~y(tr),?Y?
);Dataset #Calls #Companies #Types #TokensPre-2009 3694 2746 371.5K 28.7M2009 3474 2178 346.2K 26.4MPost-2009 3726 2107 377.4K 28.6MTable 1: Statistics of three datasets.
Types: uniquewords.
Tokens: word tokens.Note that unlike the standard news corpora inNLP or the SEC-mandated financial report, Tran-scripts of earnings call is a very special genreof text.
For example, the length of WSJ docu-ments is typically one to three hundreds (Harman,1995), but the averaged document length of ourthree earnings calls datasets is 7677.
Dependingon the amount of interactions in the question an-swering session, the complexities of the calls vary.This mixed form of formal statement and informalspeech brought difficulties to machine learning al-gorithms.5 Measuring Financial RisksVolatility is an important measure of the financialrisk, and in this work, we focus on predicting thefuture volatility following the earnings teleconfer-1159ence call.
For each earning call, we have a week ofstock prices of the company after the day on whichthe earnings call is made.
The Return of Day t is:rt=xtxt?1?
1 (13)where xtrepresents the share price of Day t, andthe Measured Stock Volatility from Day t to t+ ?
:y(t,t+?)=???i=0(rt+i?
r?)2?
(14)Using the stock prices, we can use the equationsabove to calculate the measured stock volatility af-ter the earnings call, which is the standard measureof risks in finance, and the dependent variable y ofour predictive task.6 Experiments6.1 Experimental SetupIn all experiments throughout this section, we use80-20 train/test splits on all three datasets.Feature sets:We have extracted lexical, named entity, syntactic,and frame-semantics features, most of which havebeen shown to perform well in previous work (Xieet al, 2013).
We use the unigrams and bigramsto represent lexical features, and the Stanford part-of-speech tagger (Toutanova et al, 2003) to extractthe lexicalized named entity and part-of-speechfeatures.
A probabilistic frame-semantics parser,SEMAFOR (Das et al, 2010), is used to providethe FrameNet-style frame-level semantic annota-tions.
For each of the five sets, we collect the top-100 most frequent features, and end up with a totalof 500 features.Baselines:The baselines are standard squared-loss linearregression, linear kernel SVM, and non-linear(Gaussian) kernel SVM.
They are all standardalgorithms in regression problems, and havebeen shown to have outstanding performances inmany recent text regression (Kogan et al, 2009;Chahuneau et al, 2012; Xie et al, 2013; Wanget al, 2013; Tsai and Wang, 2013).
We usethe Statistical Toolbox?s linear regression imple-mentation in Matlab, and LibSVM (Chang andLin, 2011) for training and testing the SVM mod-els.
The hyperparameter C in linear SVM, andthe ?
and C hyperparameters in Gaussian SVMare tuned on the training set using 10-fold cross-validation.
Note that since the kernel density esti-mation in the proposed copula model is nonpara-metric, and we only need to learn the ?
in theGaussian copula, there is no hyperparameters thatneed to be tuned.Evaluation Metrics:Spearman?s correlation (Hogg and Craig, 1994)and Kendall?s tau (Kendall, 1938) have beenwidely used in many regression problems in NLP(Albrecht and Hwa, 2007; Yogatama et al, 2011;Wang et al, 2013; Tsai and Wang, 2013), and herewe use them to measure the quality of predictedvalues?y by comparing to the vector of groundtruth y.
In contrast to Pearson?s correlation, Spear-man?s correlation has no assumptions on the rela-tionship of the two measured variables.
Kendall?stau is a nonparametric statistical metric that haveshown to be inexpensive, robust, and represen-tation independent (Lapata, 2006).
We also usepaired two-tailed t-test to measure the statisticalsignificance between the best and the second bestapproaches.6.2 Comparing to Various BaselinesIn the first experiment, we compare the proposedsemiparametric Gaussian copula regression modelto three baselines on three datasets with all fea-tures.
The detailed results are shown in Table 2.On the pre-2009 dataset, we see that the linear re-gression and linear SVM perform reasonably well,but the Gaussian kernel SVM performs less well,probably due to overfitting.
The copula modeloutperformed all three baselines by a wide mar-gin on this dataset with both metrics.
Similar per-formances are also obtained in the 2009 dataset,where the result of linear SVM baseline falls be-hind.
On the post-2009 dataset, none of resultsfrom the linear and non-linear SVM models canmatch up with the linear regression model, butour proposed copula model still improves over allbaselines by a large margin.
Comparing to second-best approaches, all improvements obtained by thecopula model are statistically significant.6.3 Varying the Amount of Training DataTo understand the learning curve of our proposedcopula regression model, we use the 25%, 50%,75% subsets from the training data, and evaluateall four models.
Figure 1 shows the evaluationresults.
From the experiments on the pre-2009dataset, we see that when the amount of trainingdata is small (25%), both SVM models have ob-tained very impressive results.
This is not surpris-ing at all, because as max-margin models, soft-margin SVM only needs a handful of examplesthat come with nonvanishing coefficients (supportvectors) to find a reasonable margin.
When in-1160Method Pre-2009 2009 Post-2009Spearman Kendall Spearman Kendall Spearman Kendalllinear regression: 0.377 0.259 0.367 0.252 0.314 0.216linear SVM: 0.364 0.249 0.242 0.167 0.132 0.091Gaussian SVM: 0.305 0.207 0.280 0.192 0.152 0.104Gaussian copula: 0.425* 0.315* 0.422* 0.310* 0.375* 0.282*Table 2: Comparing the learning algorithms on three datasets with all features.
The best result is high-lighted in bold.
* indicates p < .001 comparing to the second best result.Figure 1: Varying the amount of training data.
Left column: pre-2009 dataset.
Middle column: 2009dataset.
Right column: post-2009 dataset.
Top row: Spearman?s correlation.
Bottom row: Kendall?s tau.creasing the amount of training data to 50%, we dosee the proposed copula model catches up quickly,and lead all baseline methods undoubtably at 75%training data.
On the 2009 dataset, we observevery similar patterns.
Interestingly, the proposedcopula regression model has dominated all meth-ods for both metrics throughout all proportions ofthe ?post-2009?
earnings calls dataset, where in-stead of financial crisis, the economic recovery isthe main theme.
In contrast to the previous twodatasets, both linear and non-linear SVMs fail toreach reasonable performances on this dataset.6.4 Varying the Amount of FeaturesFinally, we investigate the robustness of the pro-posed semiparametric Gaussian copula regressionmodel by varying the amount of features in the co-variate space.
To do this, we sample equal amountof features from each feature set, and concatenatethem into a feature vector.
When increasing theamount of total features from 100 to 400, the re-sults are shown in Figure 2.
On the pre-2009dataset, we see that the gaps between the best-perform copula model and the second-best linearregression model are consistent throughout all fea-ture sizes.
On the 2009 dataset, we see that theperformance of Gaussian copula is aligned withthe linear regression model in terms of Spearman?scorrelation, where the former seems to performbetter in terms of Kendall?s tau.
Both linear andnon-linear SVM models do not have any advan-tages over the proposed approach.
On the post-2009 dataset that concerns economic growth andrecovery, the boundaries among all methods arevery clear.
The Spearman?s correlation for bothSVM baselines is less than 0.15 throughout all set-tings, but copula model is able to achieve 0.4 whenusing 400 features.
The improvements of copula1161Figure 2: Varying the amount of features.
Left column: pre-2009 dataset.
Middle column: 2009 dataset.Right column: post-2009 dataset.
Top row: Spearman?s correlation.
Bottom row: Kendall?s tau.Pre-2009 2009 Post-20092008/CD 2008 first quarter2008 million/CD revenue/NNthird quarter 2008/CD revenuethird million quarter ofthird/JJ million in compared tothe third the fourth million inmillion/CD fourth quarter Peter/PERSONcapital fourth callmillion fourth/JJ first/JJFE Trajector entity $/$ million/CDTable 3: Top-10 features that have positive corre-lations with stock volatility in three datasets.model over squared loss linear regression modelare increasing, when working with larger featurespaces.6.5 Qualitative AnalysisLike linear classifiers, by ?opening the hood?
tothe Gaussian copula regression model, one can ex-amine features that exhibit high correlations withthe dependent variable.
Table 3 shows the top fea-tures that are positively correlated with the futurestock volatility in the three datasets.
On the topfeatures from the ?pre-2009?
dataset, which pri-marily (82%) includes calls from 2008, we canclearly observe that the word ?2008?
has strongcorrelation with the financial risks.
Interestingly,the phrase ?third quarter?
and its variations, notonly play an important role in the model, but alsohighly correlated to the timeline of the financialcrisis: the Q3 of 2008 is a critical period in therecession, where Lehman Brothers falls on theSept.
15 of 2008, filing $613 billion of debt ?the biggest bankruptcy in U.S. history (Mamudi,2008).
This huge panic soon broke out in vari-ous financial institutions in the Wall Street.
Onthe top features from ?2009?
dataset, again, we seethe word ?2008?
is still prominent in predicting fi-nancial risks, indicating the hardship and extendedimpacts from the center of the economic crisis.After examining the transcripts, we found sen-tences like: ?...our specialty lighting business thatwe discontinued in the fourth quarter of 2008...?,?...the exception of fourth quarter revenue whichwas $100,000 below our guidance target...?, and?...to address changing economic conditions andtheir impact on our operations, in the fourth quar-ter we took the painful but prudent step of de-creasing our headcount by about 5%...?, show-ing the crucial role that Q4 of 2008 plays in 2009earnings calls.
Interestingly, after the 2008-2009crisis, in the recovery period, we have observednew words like ?revenue?, indicating the ?back-to-normal?
trend of financial environment, and newfeatures that predict financial volatility.7 DiscussionsIn the experimental section, we notice that theproposed semiparametric Gaussian copula modelhas obtained promising results in various setupson three datasets in this text regression task.
The1162main questions we ask are: how is the pro-posed model different from standard text regres-sion/classification models?
What are the advan-tages of copula-based models, and what makes itperform so well?One advantage we see from the copula modelis that it does not require any assumptions onthe marginal distributions.
For example, in latentDirichlet alocation (Blei et al, 2003), the topicproportion of a document is always drawn froma Dirichlet(?)
distribution.
This is rather re-stricted, because the possible shapes from a K?1simplex of Dirichlet is always limited in somesense.
In our copula model, instead of using somepriors, we just calculate the empirical cumulativedistribution function of the random variables, andmodel the correlation among them.
This is ex-tremely practical, because in many natural lan-guage processing tasks, we often have to deal withfeatures that are extracted from many different do-mains and signals.
By applying the ProbabilityIntegral Transform to raw features in the copulamodel, we essentially avoid comparing apples andoranges in the feature space, which is a commonproblem in bag-of-features models in NLP.The second hypothesis is about the semiparam-etirc parameterization, which contains the non-parametric kernel density estimation and the para-metric Gaussian copula regression components.The benefit of a semiparametric model is that herewe are not interested in performing completelynonparametric estimations, where the infinite di-mensional parameters might bring intractability.In contrast, by considering the semiparametriccase, we not only obtain some expressiveness fromthe nonparametric models, but also reduce thecomplexity of the task: we are only interested inthe finite-dimensional components ?
in the Gaus-sian copula with O(n log n) complexity, whichis not as computationally difficult as the com-pletely nonparametric cases.
Also, by modelingthe marginals and their correlations seperately, ourapproach is cleaner, easy-to-understand, and al-lows us to have more flexibility to model the un-certainty of data.
Our pilot experiment also alignswith our hypothesis: when not performing the ker-nel density estimation part for smoothing out themarginal distributions, the performances droppedsignificantly when sparser features are included.The third advantage we observe is the power ofmodeling the covariance of the random variables.Traditionally, in statistics, independent and identi-cally distributed (i.i.d) assumptions among the in-stances and the random variables are often used invarious models, such that the correlations amongthe instances or the variables are often ignored.However, this might not be practical at all: in im-age processing, the ?cloud?
pixel of a pixel show-ing the blue sky of a picture are more likelihood toco-occur in the same picture; in natural languageprocessing, the word ?mythical?
is more likely toco-occur with the word ?unicorn?, rather than theword ?popcorn?.
Therefore, by modeling the cor-relations among marginal CDFs, the copula modelhas gained the insights on the dependency struc-tures of the random variables, and thus, the perfor-mance of the regression task is boosted.In the future, we plan to apply the proposedapproach to large datasets where millions of fea-tures and millions of instances are involved.
Cur-rently we have not experienced the difficulty whenestimating the Gaussian copula model, but paral-lel methods might be needed to speedup learningwhen significantly more marginal CDFs are in-volved.
The second issue is about overfitting.
Wesee that when features are rather noisy, we mightneed to investigate regularized copula models toavoid this.
Finally, we plan to extend the proposedapproach to text classification and structured pre-diction problems in NLP.8 ConclusionIn this work, we have demonstrated that the morecomplex quarterly earnings calls can also be usedto predict the measured volatility of the stocks inthe limited future.
We propose a novel semipara-metric Gausian copula regression approach thatmodels the dependency structure of the languagein the earnings calls.
Unlike traditional bag-of-features models that work discrete features fromvarious signals, we perform kernel density esti-mation to smooth out the distribution, and useprobability integral transform to work with CDFsthat are uniform.
The copula model deals withmarginal CDFs and the correlation among themseparately, in a cleaner manner that is also flexibleto parameterize.
Focusing on the three financialcrisis related datasets, the proposed model signif-icantly outperform the standard linear regressionmethod in statistics and strong discriminative sup-port vector regression baselines.
By varying thesize of the training data and the dimensionality ofthe covariates, we have demonstrated that our pro-posed model is relatively robust across differentparameter settings.AcknowledgementWe thank Alex Smola, Barnab?as P?oczos, SamThomson, Shoou-I Yu, Zi Yang, and anonymousreviewers for their useful comments.1163ReferencesJoshua Albrecht and Rebecca Hwa.
2007.
Regressionfor sentence-level mt evaluation with pseudo refer-ences.
In Proceedings of Annual Meeting of the As-sociation for Computational Linguistics.David Blei, Andrew Ng, and Michael Jordan.
2003.Latent dirichlet alocation.
Journal of machineLearning research.Johan Bollen, Huina Mao, and Xiaojun Zeng.
2011.Twitter mood predicts the stock market.
Journal ofComputational Science.Belinda Camiciottoli.
2010.
Earnings calls: Exploringan emerging financial reporting genre.
Discourse &Communication.Victor Chahuneau, Kevin Gimpel, Bryan R Routledge,Lily Scherlis, and Noah A Smith.
2012.
Wordsalad: Relating food prices and descriptions.
In Pro-ceedings of the 2012 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning.Chih-Chung Chang and Chih-Jen Lin.
2011.
Libsvm:a library for support vector machines.
ACM Trans-actions on Intelligent Systems and Technology.Xiaohong Chen and Yanqin Fan.
2006.
Estimationof copula-based semiparametric time series models.Journal of Econometrics.David Christensen.
2005.
Fast algorithms for the cal-culation of kendalls ?
.
Computational Statistics.Kenneth Church and William Gale.
1995.
Poissonmixtures.
Natural Language Engineering.Dipanjan Das, Nathan Schneider, Desai Chen, andNoah A Smith.
2010.
Probabilistic frame-semanticparsing.
In Human language technologies: The2010 annual conference of the North Americanchapter of the association for computational linguis-tics.Francis X Diebold, Todd A Gunther, and Anthony STay.
1997.
Evaluating density forecasts.Carsten Eickhoff, Arjen P. de Vries, and KevynCollins-Thompson.
2013.
Copulas for informationretrieval.
In Proceedings of the 36th InternationalACM SIGIR Conference on Research and Develop-ment in Information Retrieval.Jacob Eisenstein, Brendan O?Connor, Noah A Smith,and Eric P Xing.
2010.
A latent variable model forgeographic lexical variation.
In Proceedings of the2010 Conference on Empirical Methods in NaturalLanguage Processing.Pui Cheong Fung, Xu Yu, and Wai Lam.
2003.
Stockprediction: Integrating text mining approach usingreal-time news.
In Proceedings of IEEE Interna-tional Conference on Computational Intelligence forFinancial Engineering.Alan Gelfand and Adrian Smith.
1990.
Sampling-based approaches to calculating marginal densities.Journal of the American statistical association.Christian Genest and Anne-Catherine Favre.
2007.Everything you always wanted to know about copulamodeling but were afraid to ask.
Journal of Hydro-logic Engineering.Zoubin Ghahramani, Barnab?as P?oczos, and JeffSchneider.
2012.
Copula-based kernel dependencymeasures.
In Proceedings of the 29th InternationalConference on Machine Learning.Fang Han, Tuo Zhao, and Han Liu.
2012.
Coda: Highdimensional copula discriminant analysis.
Journalof Machine Learning Research.Donna Harman.
1995.
Overview of the second text re-trieval conference (trec-2).
Information Processing& Management.Robert V Hogg and Allen Craig.
1994.
Introduction tomathematical statistics.Harry Joe.
1997.
Multivariate models and dependenceconcepts.Mahesh Joshi, Dipanjan Das, Kevin Gimpel, andNoah A Smith.
2010.
Movie reviews and revenues:An experiment in text regression.
In Human Lan-guage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics.Maurice Kendall.
1938.
A new measure of rank corre-lation.
Biometrika.Shimon Kogan, Dimitry Levin, Bryan Routledge, Ja-cob Sagi, and Noah Smith.
2009.
Predicting riskfrom financial reports with regression.
In Proceed-ings of Human Language Technologies: The 2009Annual Conference of the North American Chapterof the Association for Computational Linguistics.John Lafferty and David Blei.
2005.
Correlated topicmodels.
In Advances in neural information process-ing systems.Mirella Lapata.
2006.
Automatic evaluation of infor-mation ordering: Kendall?s tau.
Computational Lin-guistics.Han Liu, Fang Han, Ming Yuan, John Lafferty, andLarry Wasserman.
2012.
High-dimensional semi-parametric gaussian copula graphical models.
TheAnnals of Statistics.David Lopez-paz, Jose M Hern?andez-lobato, andGhahramani Zoubin.
2013.
Gaussian process vinecopulas for multivariate dependence.
In Proceed-ings of the 30th International Conference on Ma-chine Learning.Sam Mamudi.
2008.
Lehman folds with record $613billion debt.
MarketWatch.com.1164Guido Masarotto and Cristiano Varin.
2012.
Gaussiancopula marginal regression.
Electronic Journal ofStatistics.Roger B Nelsen.
1999.
An introduction to copulas.Springer Verlag.Rahul A Parsa and Stuart A Klugman.
2011.
Copularegression.
Variance Advancing and Science of Risk.Emanuel Parzen.
1962.
On estimation of a probabilitydensity function and mode.
The annals of mathe-matical statistics.Michael Pitt, David Chan, and Robert Kohn.
2006.Efficient bayesian inference for gaussian copula re-gression models.
Biometrika.McKay Price, James Doran, David Peterson, and Bar-bara Bliss.
2012.
Earnings conference calls andstock returns: The incremental informativeness oftextual tone.
Journal of Banking & Finance.Joseph Reisinger, Austin Waters, Bryan Silverthorn,and Raymond J Mooney.
2010.
Spherical topicmodels.
In Proceedings of the 27th InternationalConference on Machine Learning.Berthold Schweizer and Abe Sklar.
1983.
Probabilis-tic metric spaces.Claude Shannon.
1948.
A mathematical theory ofcommunication.
In The Bell System Technical Jour-nal.Abe Sklar.
1959.
Fonctions de r?epartition `a n dimen-sions et leurs marges.
Universit?e Paris 8.Kristina Toutanova, Dan Klein, Christopher D Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology.Ming-Feng Tsai and Chuan-Ju Wang.
2013.
Riskranking from financial reports.
In Advances in In-formation Retrieval.William Yang Wang, Elijah Mayfield, Suresh Naidu,and Jeremiah Dittmar.
2012.
Historical analysisof legal opinions with a sparse mixed-effects latentvariable model.
In Proceedings of the 50th AnnualMeeting of the Association for Computational Lin-guistics.Chuan-Ju Wang, Ming-Feng Tsai, Tse Liu, and Chin-Ting Chang.
2013.
Financial sentiment analysis forrisk prediction.
In Proceedings of the Sixth Interna-tional Joint Conference on Natural Language Pro-cessing.Rongjing Xiang and Jennifer Neville.
2013.
Collec-tive inference for network data with copula latentmarkov networks.
In Proceedings of the sixth ACMinternational conference on Web search and datamining.Boyi Xie, Rebecca J. Passonneau, Leon Wu, andGerm?an G. Creamer.
2013.
Semantic frames to pre-dict stock price movement.
In Proceedings of the51st Annual Meeting of the Association for Compu-tational Linguistics.Dani Yogatama, Michael Heilman, Brendan O?Connor,Chris Dyer, Bryan R Routledge, and Noah A Smith.2011.
Predicting a scientific community?s responseto an article.
In Proceedings of the Conference onEmpirical Methods in Natural Language Process-ing.Xue Zhang, Hauke Fuehres, and Peter A Gloor.
2011.Predicting stock market indicators through twitter ?ihope it is not as bad as i fear?.
Procedia-Social andBehavioral Sciences.1165
