Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1326?1334,Beijing, August 2010Leveraging Multiple MT Engines for Paraphrase GenerationShiqi Zhao?
?, Haifeng Wang?, Xiang Lan?, and Ting Liu?
?Baidu Inc.?HIT Center for Information Retrieval, Harbin Institute of Technology{zhaoshiqi, wanghaifeng}@baidu.com,{xlan, tliu}@ir.hit.edu.cnAbstractThis paper proposes a method that lever-ages multiple machine translation (MT)engines for paraphrase generation (PG).The method includes two stages.
Firstly,we use a multi-pivot approach to acquirea set of candidate paraphrases for a sourcesentence S. Then, we employ two kindsof techniques, namely the selection-basedtechnique and the decoding-based tech-nique, to produce a best paraphrase T forS using the candidates acquired in the firststage.
Experimental results show that:(1) The multi-pivot approach is effectivefor obtaining plenty of valuable candi-date paraphrases.
(2) Both the selection-based and decoding-based techniques canmake good use of the candidates and pro-duce high-quality paraphrases.
Moreover,these two techniques are complementary.
(3) The proposed method outperforms astate-of-the-art paraphrase generation ap-proach.1 IntroductionThis paper addresses the problem of paraphrasegeneration (PG), which seeks to generate para-phrases for sentences.
PG is important in manynatural language processing (NLP) applications.For example, in machine translation (MT), asentence can be paraphrased so as to make itmore translatable (Zhang and Yamamoto, 2002;Callison-Burch et al, 2006).
In question answer-ing (QA), a question can be paraphrased to im-prove the coverage of answer extraction (Duboueand Chu-Carroll, 2006; Riezler et al, 2007).
Innatural language generation (NLG), paraphrasingcan help to increase the expressive power of theNLG systems (Iordanskaja et al, 1991).In this paper, we propose a novel PG method.For an English sentence S, the method first ac-quires a set of candidate paraphrases with a multi-pivot approach, which uses MT engines to auto-matically translate S into multiple pivot languagesand then translate them back into English.
Fur-thermore, the method employs two kinds of tech-niques to produce a best paraphrase T for S us-ing the candidates, i.e., the selection-based anddecoding-based techniques.
The former selectsa best paraphrase from the candidates based onMinimum Bayes Risk (MBR), while the lattertrains a MT model using the candidates and gen-erates paraphrases with a MT decoder.We evaluate our method on a set of 1182 En-glish sentences.
The results show that: (1) al-though the candidate paraphrases acquired by MTengines are noisy, they provide good raw ma-terials for further paraphrase generation; (2) theselection-based technique is effective, which re-sults in the best performance; (3) the decoding-based technique is promising, which can generateparaphrases that are different from the candidates;(4) both the selection-based and decoding-basedtechniques outperform a state-of-the-art approachSPG (Zhao et al, 2009).2 Related Work2.1 Methods for Paraphrase GenerationMT-based method is the mainstream method onPG.
It regards PG as a monolingual machine trans-lation problem, i.e., ?translating?
a sentence Sinto another sentence T in the same language.1326Quirk et al (2004) first presented the MT-basedmethod.
They trained a statistical MT (SMT)model on a monolingual parallel corpus extractedfrom comparable news articles and applied themodel to generate paraphrases.
Their work showsthat SMT techniques can be extended to PG.
How-ever, its usefulness is limited by the scarcity ofmonolingual parallel data.To overcome the data sparseness problem, Zhaoet al (2008a) improved the MT-based PG methodby training the paraphrase model using multi-ple resources, including monolingual parallel cor-pora, monolingual comparable corpora, bilingualparallel corpora, etc.
Their results show that bilin-gual parallel corpora are the most useful amongthe exploited resources.
Zhao et al (2009) furtherimproved the method by introducing a usabilitysub-model into the paraphrase model so as to gen-erate varied paraphrases for different applications.The main disadvantage of the MT-basedmethod is that its performance heavily depends onthe fine-grained paraphrases, such as paraphrasephrases and patterns, which provide paraphraseoptions in decoding.
Hence one has to first ex-tract fine-grained paraphrases from various cor-pora with different methods (Zhao et al, 2008a;Zhao et al, 2009), which is difficult and time-consuming.In addition to the MT-based method, re-searchers have also investigated other methods forparaphrase generation, such as the pattern-basedmethods (Barzilay and Lee, 2003; Pang et al,2003), thesaurus-based methods (Bolshakov andGelbukh, 2004; Kauchak and Barzilay, 2006),and NLG-based methods (Kozlowski et al, 2003;Power and Scott, 2005).2.2 Pivot Approach for ParaphrasingBannard and Callison-Burch (2005) introducedthe pivot approach to extracting paraphrasephrases from bilingual parallel corpora.
Their ba-sic assumption is that two English phrases alignedwith the same phrase in a foreign language (alsocalled a pivot language) are potential paraphrases.Zhao et al (2008b) extended the approach andused it to extract paraphrase patterns.
Both of theabove works have proved the effectiveness of thepivot approach in paraphrase extraction.Pivot approach can also be used in paraphrasegeneration.
It generates paraphrases by translatingsentences from a source language to one (single-pivot) or more (multi-pivot) pivot languages andthen translating them back to the source language.Duboue et al (2006) first proposed the multi-pivot approach for paraphrase generation, whichwas specially designed for question expansion inQA.
In addition, Max (2009) presented a single-pivot approach for generating sub-sentential para-phrases.
A clear difference between our methodand the above works is that we propose selection-based and decoding-based techniques to gener-ate high-quality paraphrases using the candidatesyielded from the pivot approach.3 Multi-pivot Approach for AcquiringCandidate ParaphrasesA single-pivot PG approach paraphrases a sen-tence S by translating it into a pivot languagePL with a MT engine MT1 and then translat-ing it back into the source language with MT2.In this paper, a single-pivot PG system is repre-sented as a triple (MT1, PL, MT2).
A multi-pivot PG system is made up of a set of single-pivotsystems with various pivot languages and MT en-gines.
Given m pivot languages and n MT en-gines, we can build a multi-pivot PG system con-sisting of N (N ?
n ?
m ?
n) single-pivot ones,where N = n ?
m ?
n iff all the n MT enginescan perform bidirectional translation between thesource and each pivot language.In this work, we experiment with 6 pivot lan-guages (Table 1) and 3 MT engines (Table 2) inthe multi-pivot approach.
All the 3 MT enginesare off-the-shelf systems, in which Google andMicrosoft translators are SMT engines, while Sys-tran translator is a rule-based MT engine.
EachMT engine can translate English to all the 6 pivotlanguages and back to English.
We thereby con-struct a multi-pivot PG system consisting of 54(3*6*3) single-pivot systems.The advantages of the multi-pivot PG approachlie in two aspects.
First, it effectively makes useof the vast bilingual data and translation rules un-derlying the MT engines.
Second, the approach issimple, which just sends sentences to the onlineMT engines and gets the translations back.1327Source Sentence he said there will be major cuts in the salaries of high-level civil servants .
(GG, G, MS) he said there are significant cuts in the salaries of high-level officials .
(GG, F , GG) he said there will be significant cuts in the salaries of top civil level .
(MS, C, MS) he said that there will be a major senior civil service pay cut .
(MS, F , ST ) he said there will be great cuts in the wages of the high level civils servant .
(ST , G, GG) he said that there are major cuts in the salaries of senior government officials .Table 3: Examples of candidate paraphrases obtained using the multi-pivot approach.1 French (F) 4 Italian (I)2 German (G) 5 Portuguese (P)3 Spanish (S) 6 Chinese (C)Table 1: Pivot languages used in the approach.1 Google Translate (GG)(translate.google.com)2 Microsoft Translator (MS)(www.microsofttranslator.com)3 Systran Online Translation (ST)(www.systransoft.com)Table 2: MT engines utilized in the approach.4 Producing High-quality Paraphrasesusing the CandidatesTable 3 shows some examples of candidate para-phrases for a sentence.
As can be seen, the can-didates do provide some correct and useful para-phrase substitutes (in bold) for the source sen-tence.
However, they also contain quite a few er-rors (in italic) due to the limited translation qual-ity of the MT engines.
The problem is evenworse when the source sentences get longer andmore complicated.
Therefore, we need to com-bine the outputs of the multiple single-pivot PGsystems and produce high-quality paraphrases outof them.
To this end, we investigate two tech-niques, namely, the selection-based and decoding-based techniques.4.1 Selection-based TechniqueGiven a source sentence S along with a set D ofcandidate paraphrases {T1, T2, ..., Ti, ...TN}, thegoal of the selection-based technique is to selectthe best paraphrase T?i for S from D. The para-phrase selection technique we propose is based onMinimum Bayes Risk (MBR).
In detail, the MBRbased technique first measures the quality of eachcandidate paraphrase Ti ?
D in terms of Bayesrisk (BR), and then selects the one with the min-imum BR as the best paraphrase.
In detail, givenS, a candidate Ti ?
D, a reference paraphraseT 1, and a loss function L(T, Ti) that measures thequality of Ti relative to T , we define the Bayesrisk as follows:BR(Ti) = EP (T,S)[L(T, Ti)], (1)where the expectation is taken under the true dis-tribution P (T, S) of the paraphrases.
Accordingto (Bickel and Doksum, 1977), the candidate para-phrase that minimizes the Bayes risk can be foundas follows:T?i = arg minTi?D?T?TL(T, Ti)P (T |S), (2)where T represents the space of reference para-phrases.
In practice, however, the collection ofreference paraphrases is not available.
We thusconstruct a set D?
= D?
{S} to approximate T 2.In addition, we cannot estimate P (T |S) in Equa-tion (2), either.
Therefore, we make a simplifica-tion by assigning a constant c to P (T |S) for eachT ?
D?, which can then be removed:T?i = arg minTi?D?T?D?L(T, Ti).
(3)Equation (3) can be further rewritten using a gainfunction G(T, Ti) instead of the loss function:1Here we assume that we have the collection of all possi-ble paraphrases of S, which are used as references.2The source sentence S is included in D?
based on theconsideration that a sentence is allowed to keep unchangedduring paraphrasing.1328T?i = arg maxTi?D?T?D?G(T, Ti).
(4)We define the gain function based on BLEU:G(T, Ti) = BLEU(T, Ti).
BLEU is awidely used metric in the automatic evaluation ofMT (Papineni et al, 2002).
It measures the sim-ilarity of two sentences by counting the overlap-ping n-grams (n=1,2,3,4 in our experiments):BLEU(T, Ti) = BP ?exp(4?n=1wn log pn(T, Ti)),where pn(T, Ti) is the n-gram precision of Ti andwn = 1/4.
BP (?
1) is a brevity penalty thatpenalizes Ti if it is shorter than T .In summary, for each sentence S, the MBRbased technique selects a paraphrase that is themost similar to all candidates and the source sen-tence.
The underlying assumption is that correctparaphrase substitutes should be common amongthe candidates, while errors committed by thesingle-pivot PG systems should be all different.We denote this approach as S-1 hereafter.Approaches for comparison.
In the experiments,we also design another two paraphrase selectionapproaches S-2 and S-3 for comparison with S-1.S-2: S-2 selects the best single-pivot PGsystem from all the 54 ones.
The selectionis also based on MBR and BLEU.
For eachsingle-pivot PG system, we sum up its gainfunction values over a set of source sentences(i.e., ?S?TS?D?S G(TS , TSi)).
Then we se-lect the one with the maximum gain value asthe best single-pivot system.
In our experi-ments, the selected best single-pivot PG system is(ST, P,GG), the candidate paraphrases acquiredby which are then returned as the best paraphrasesin S-2.S-3: S-3 is a simple baseline, which just ran-domly selects a paraphrase from the 54 candidatesfor each source sentence S.4.2 Decoding-based TechniqueThe selection-based technique introduced abovehas an inherent limitation that it can only selecta paraphrase from the candidates.
That is to say, itmajor cuts high-level civil servantssignificant cuts senior officialsmajor cuts* high-level officialsimportant cuts senior civil servantsbig cutsgreat cutsTable 4: Extracted phrase pairs.
(*This is calleda self-paraphrase of the source phrase, whichis generated when a phrase keeps unchanged insome of the candidate paraphrases.
)can never produce a perfect paraphrase if all thecandidates have some tiny flaws.
To solve thisproblem, we propose the decoding-based tech-nique, which trains a MT model using the can-didate paraphrases of each source sentence S andgenerates a new paraphrase T for S with a MTdecoder.In this work, we implement the decoding-basedtechnique using Giza++ (Och and Ney, 2000) andMoses (Hoang and Koehn, 2008), both of whichare commonly used SMT tools.
For a sentenceS, we first construct a set of parallel sentencesby pairing S with each of its candidate para-phrases: {(S,T1),(S,T2),...,(S,TN )} (N = 54).We then run word alignment on the set usingGiza++ and extract aligned phrase pairs as de-scribed in (Koehn, 2004).
Here we only keep thephrase pairs that are aligned ?3 times on the set,so as to filter errors brought by the noisy sentencepairs.
The extracted phrase pairs are stored in aphrase table.
Table 4 shows some extracted phrasepairs.Note that Giza++ is sensitive to the data size.Hence it is interesting to examine if the alignmentcan be improved by augmenting the parallel sen-tence pairs.
To this end, we have tried augmentingthe parallel set for each sentence S by pairing anytwo candidate paraphrases.
In this manner, C2Nsentence pairs are augmented for each S. We con-duct word alignment using the (N+C2N ) sentencepairs and extract aligned phrases from the originalN pairs.
However, we have not found clear im-provement after observing the results.
Therefore,we do not adopt the augmentation strategy in ourexperiments.1329Using the extracted phrasal paraphrases, weconduct decoding for the sentence S with Moses,which is based on a log-linear model.
The defaultsetting of Moses is used, except that the distortionmodel for phrase reordering is turned off3.
Thelanguage model in Moses is trained using a 9 GBEnglish corpus.
We denote the above approach asD-1 in what follows.Approach for comparison.
The main advantageof the decoding-based technique is that it allowsus to customize the paraphrases for different re-quirements through tailoring the phrase table ortuning the model parameters.
As a case study,this paper shows how to generate paraphrases withvaried paraphrase rates4.D-2: The extracted phrasal paraphrases (in-cluding self-paraphrases) are stored in a phrase ta-ble, in which each phrase pair has 4 scores mea-suring their alignment confidence (Koehn et al,2003).
Our basic idea is to control the paraphraserate by tuning the scores of the self-paraphrases.We thus extend D-1 to D-2, which assigns aweight ?
(?
> 0) to the scores of the self-paraphrase pairs.
Obviously, if we set ?
< 1,the self-paraphrases will be penalized and the de-coder will prefer to generate a paraphrase withmore changes.
If we set ?
> 1, the decoder willtend to generate a paraphrase that is more similarto the source sentence.
In our experiments, we set?
= 0.1 in D-2.5 Experimental SetupOur test sentences are extracted from the paral-lel reference translations of a Chinese-to-EnglishMT evaluation5, in which each Chinese sentencec has 4 English reference translations, namely e1,e2, e3, and e4.
We use e1 as a test sentence to para-phrase and e2, e3, e4 as human paraphrases of e1for comparison with the automatically generatedparaphrases.
We process the test set by manuallyfiltering ill-formed sentences, such as the ungram-matical or incomplete ones.
1182 out of 13573We conduct monotone decoding as previous work(Quirk et al, 2004; Zhao et al, 2008a, Zhao et al, 2009).4The paraphrase rate reflects how different a paraphraseis from the source sentence.52008 NIST Open Machine Translation Evaluation: Chi-nese to English Task.Score Adequacy Fluency5 All Flawless English4 Most Good English3 Much Non-native English2 Little Disfluent English1 None IncomprehensibleTable 5: Five point scale for human evaluation.test sentences are retained after filtering.
Statisticsshow that about half of the test sentences are fromnews and the other half are from essays.
The aver-age length of the test sentences is 34.12 (words).Manual evaluation is used in this work.
A para-phrase T of a sentence S is manually scored basedon a five point scale, which measures both the ?ad-equacy?
(i.e., how much of the meaning of S ispreserved in T ) and ?fluency?
of T (See Table 5).The five point scale used here is similar to that inthe human evaluation of MT (Callison-Burch etal., 2007).
In MT, adequacy and fluency are eval-uated separately.
However, we find that there is ahigh correlation between the two aspects, whichmakes it difficult to separate them.
Thus we com-bine them in this paper.We compare our method with a state-of-the-art approach SPG6 (Zhao et al, 2009), whichis a statistical approach specially designed forPG.
The approach first collects a large volume offine-grained paraphrase resources, including para-phrase phrases, patterns, and collocations, fromvarious corpora using different methods.
Then itgenerates paraphrases using these resources witha statistical model7.6 Experimental ResultsWe evaluate six approaches, i.e., S-1, S-2, S-3, D-1, D-2 and SPG, in the experiments.
Each ap-proach generates a 1-best paraphrase for a testsentence S. We randomize the order of the 6 para-phrases of each S to avoid bias of the raters.6SPG: Statistical Paraphrase Generation.7We ran SPG under the setting of baseline-2 as describedin (Zhao et al, 2009).133000.511.522.533.544.5score 3.92 3.52 2.78 3.62 3.36 3.47S-1 S-2 S-3 D-1 D-2 SPGFigure 1: Evaluation results of the approaches.6.1 Human Evaluation ResultsWe have 6 raters in the evaluation, all of whomare postgraduate students.
In particular, 3 ratersmajor in English, while the other 3 major in com-puter science.
Each rater scores the paraphrasesof 1/6 test sentences, whose results are then com-bined to form the final scoring result.
The av-erage scores of the six approaches are shown inFigure 1.
We can find that among the selection-based approaches, the performance of S-3 is theworst, which indicates that randomly selecting aparaphrase from the candidates works badly.
S-2 performs much better than S-3, suggesting thatthe quality of the paraphrases acquired with thebest single-pivot PG system are much higher thanthe randomly selected ones.
S-1 performs the bestin all the six approaches, which demonstrates theeffectiveness of the MBR-based selection tech-nique.
Additionally, the fact that S-1 evidentlyoutperforms S-2 suggests that it is necessary to ex-tend a single-pivot approach to a multi-pivot one.To get a deeper insight of S-1, we randomlysample 100 test sentences and manually score allof their candidates.
We find that S-1 successfullypicks out a paraphrase with the highest score for72 test sentences.
We further analyze the remain-ing 28 sentences for which S-1 fails and find thatthe failures are mainly due to the BLEU-basedgain function.
For example, S-1 sometimes se-lects paraphrases that have correct phrases but in-correct phrase orders, since BLEU is weak in eval-uating phrase orders and sentence structures.
Inthe next step we shall improve the gain functionby investigating other features besides BLEU.In the decoding-based approaches, D-1 ranksthe second in the six approaches only behind S-1.00.511.522.533.544.55S-1 S-2 S-3 D-1 D-2 SPGr1 r2 r3 r4 r5 r6Figure 2: Evaluation results from each rater.We will further improve D-1 in the future ratherthan simply use Moses in decoding with the de-fault setting.
However, the value of D-1 lies inthat it enables us to break down the candidatesand generate new paraphrases flexibly.
The per-formance decreases when we extend D-1 to D-2to achieve a larger paraphrase rate.
This is mainlybecause more errors are brought in when moreparts of a sentence are paraphrased.We can also find from Figure 1 that S-1, S-2,and D-1 all get higher scores than SPG, whichshows that our method outperforms this state-of-the-art approach.
This is more important if weconsider that our method is lightweight, whichmakes no effort to collect fine-grained paraphraseresources beforehand.
After observing the results,we believe that the outperformance of our methodcan be mainly ascribed to the selection-based anddecoding-based techniques, since we avoid manyerrors by voting among the candidates.
For in-stance, an ambiguous phrase may be incorrectlyparaphrased by some of the single-pivot PG sys-tems or the SPG approach.
However, our methodmay obtain the correct paraphrase through statis-tics over all candidates and selecting the mostcredible one.The human evaluation of paraphrases is subjec-tive.
Hence it is necessary to examine the coher-ence among the raters.
The scoring results fromthe six raters are depicted in Figure 2.
As it can beseen, they show similar trends though the ratershave different degrees of strictness.133100.10.20.30.40.50.60.70.8PR1 0.116 0.138 0.232 0.149 0.206 0.139 0.366 0.379 0.386PR2 0.211 0.272 0.427 0.22 0.3 0.234 0.607 0.602 0.694S-1 S-2 S-3 D-1 D-2 SPG HP1 HP2 HP3Figure 3: Paraphrase rates of the approaches.6.2 Paraphrase RateHuman evaluation assesses the quality of para-phrases.
However, the paraphrase rates cannot bereflected.
A paraphrase that is totally transformedfrom the source sentence and another that is al-most unchanged may get the same score.
There-fore, we propose two strategies, i.e., PR1 and PR2,to compute the paraphrase rate:PR1(T ) = 1?
OL(S, T )L(S) ; PR2(T ) =ED(S, T )L(S) .Here, PR1 is defined based on word overlappingrate, in which OL(S, T ) denotes the number ofoverlapping words between a paraphrase T and itssource sentence S, L(S) denotes the number ofwords in S. PR2 is defined based on edit distance,in which ED(S, T ) denotes the edit distance be-tween T and S. Obviously, PR1 only measuresthe percentage of words that are changed fromS to T , whereas PR2 further takes word orderchanges into consideration.
It should be noted thatPR1 and PR2 not only count the correct changesbetween S and T , but also count the incorrectones.
We compute the paraphrase rate for eachof the six approaches by averaging the paraphraserates over the whole test set.
The results are shownin the left part of Figure 3.On the whole, the paraphrase rates of the ap-proaches are not high.
In particular, we can seethat the paraphrase rate of D-2 is clearly higherthan D-1, which is in line with our intention of de-signing D-2.
We can also see that the paraphraserate of S-3 is the highest among the approaches.We find it is mainly because the paraphrases gen-erated with S-3 contain quite a lot of errors, whichcontribute most of the changes.7 Analysis7.1 Effectiveness of the Proposed MethodOur analysis starts from the candidate paraphrasesacquired with the multi-pivot approach.
Actu-ally, the results of S-3 reflect the average qual-ity of the candidate paraphrases.
A score of 2.78(See Figure 1) indicates that the candidates areunacceptable according to the human evaluationmetrics.
This is in line with our expectation thatthe automatically acquired paraphrases through atwo-way translation are noisy.
However, the re-sults of S-1 and D-1 demonstrate that, using theselection-based and decoding-based techniques,we can produce paraphrases of good quality.
Es-pecially, S-1 gets a score of nearly 4, which sug-gests that the paraphrases are pretty good accord-ing to our metrics.
Moreover, our method out-performs SPG built on pre-extracted fine-grainedparaphrases.
It shows that our method makes gooduse of the paraphrase knowledge from the largevolume of bilingual data underlying the multipleMT engines.7.2 How to Choose Pivot Languages and MTEngines in the Multi-pivot ApproachIn our experiments, besides the six pivot lan-guages used in the multi-pivot system, we havealso tried another five pivot languages, includingArabic, Japanese, Korean, Russian, and Dutch.They are finally abandoned since we find that theyperform badly.
Our experience on choosing pivotlanguages is that: (1) a pivot language should bea language whose translation quality can be wellguaranteed by the MT engines; (2) it is better tochoose a pivot language similar to the source lan-guage (e.g., French - English), which is easier totranslate; (3) the translation quality of a pivot lan-guage should not vary a lot among the MT en-gines.
On the other hand, it is better to chooseMT engines built on diverse models and corpora,which can provide different paraphrase options.We plan to employ a syntax-based MT engine inour further experiments besides the currently usedphrase-based SMT and rule-based MT engines.1332S he said there will be major cuts in the salaries of high-level civil servants .S-1 he said that there will be significant cuts in the salaries of senior officials .S-2 he said there will be major cuts in salaries of civil servants high level .S-3 he said that there will be significant cuts in the salaries of senior officials .D-1 he said , there will be significant cuts in salaries of senior civil servants .D-2 he said , there will be significant cuts in salaries of senior officials .SPG he said that there will be the main cuts in the wages of high-level civil servants .HP1 he said there will be a big salary cut for high-level government employees .HP2 he said salaries of senior public servants would be slashed .HP3 he claimed to implement huge salary cut to senior civil servants .Table 6: Comparing the automatically generated paraphrases with the human paraphrases.7.3 Comparing the Selection-based andDecoding-based TechniquesIt is necessary to compare the paraphrases gener-ated via the selection-based and decoding-basedtechniques.
As stated above, the selection-basedtechnique can only select a paraphrase from thecandidates, while the decoding-based techniquecan generate a paraphrase different from all can-didates.
In our experiments, we find that forabout 90% test sentences, the paraphrases gener-ated by the decoding-based approach D-1 are out-side the candidates.
In particular, we compare theparaphrases generated by S-1 and D-1 and findthat, for about 40% test sentences, S-1 gets higherscores than D-1, while for another 21% test sen-tences, D-1 gets higher scores than S-18.
Thisindicates that the selection-based and decoding-based techniques are complementary.
In addition,we find examples in which the decoding-basedtechnique can generate a perfect paraphrase forthe source sentence, even if all the candidate para-phrases have obvious errors.
This also shows thatthe decoding-based technique is promising.7.4 Comparing Automatically GeneratedParaphrases with Human ParaphrasesWe also analyze the characteristics of the gener-ated paraphrases and compare them with the hu-man paraphrases (i.e., the other 3 reference trans-lations in the MT evaluation, see Section 5, whichare denoted as HP1, HP2, and HP3).
We find that,compared with the automatically generated para-phrases, the human paraphrases are more com-8For the rest 39%, S-1 and D-1 get identical scores.plicated, which involve not only phrase replace-ments, but also structure reformulations and eveninferences.
Their paraphrase rates are also muchhigher, which can be seen in the right part of Fig-ure 3.
We show the automatic and human para-phrases for the example sentence of this paper inTable 6.
To narrow the gap between the automaticand human paraphrases, it is necessary to learnstructural paraphrase knowledge from the candi-dates in the future work.8 Conclusions and Future WorkWe put forward an effective method for para-phrase generation, which has the following con-tributions.
First, it acquires a rich fund of para-phrase knowledge through the use of multiple MTengines and pivot languages.
Second, it presentsa MBR-based technique that effectively selectshigh-quality paraphrases from the noisy candi-dates.
Third, it proposes a decoding-based tech-nique, which can generate paraphrases that aredifferent from the candidates.
Experimental re-sults show that the proposed method outperformsa state-of-the-art approach SPG.In the future work, we plan to improve theselection-based and decoding-based techniques.We will try some standard system combinationstrategies, like confusion networks and consensusdecoding.
In addition, we will refine our evalu-ation metrics.
In the current experiments, para-phrase correctness (adequacy and fluency) andparaphrase rate are evaluated separately, whichseem to be incompatible.
We plan to combinethem together and propose a uniform metric.1333ReferencesColin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with Bilingual Parallel Corpora.
In Pro-ceedings of ACL, pages 597-604.Regina Barzilay and Lillian Lee.
2003.
Learningto Paraphrase: An Unsupervised Approach UsingMultiple-Sequence Alignment.
In Proceedings ofHLT-NAACL, pages 16-23.Peter J. Bickel and Kjell A. Doksum.
1977.
Mathe-matical Statistics: Basic Ideas and Selected Topics.Holden-Day Inc., Oakland, CA, USA.Igor A. Bolshakov and Alexander Gelbukh.
2004.Synonymous Paraphrasing Using WordNet and In-ternet.
In Proceedings of NLDB, pages 312-323.Chris Callison-Burch, Cameron Fordyce, PhilippKoehn, Christof Monz and Josh Schroeder.
2007.
(Meta-) Evaluation of Machine Translation.
In Pro-ceedings of ACL-2007 Workshop on Statistical Ma-chine Translation, pages 136-158.Chris Callison-Burch, Philipp Koehn, and Miles Os-borne.
2006.
Improved Statistical Machine Trans-lation Using Paraphrases.
In Proceedings of HLT-NAACL, pages 17-24.Pablo Ariel Duboue and Jennifer Chu-Carroll.
2006.Answering the Question You Wish They HadAsked: The Impact of Paraphrasing for QuestionAnswering.
In Proceedings of HLT-NAACL, pages33-36.Hieu Hoang and Philipp Koehn.
2008.
Design of theMoses Decoder for Statistical Machine Translation.In Proceedings of ACL Workshop on Software en-gineering, testing, and quality assurance for NLP,pages 58-65.Lidija Iordanskaja, Richard Kittredge, and AlainPolgue`re.
1991.
Lexical Selection and Paraphrasein a Meaning-Text Generation Model.
In Ce?cile L.Paris, William R. Swartout, and William C.
Mann(Eds.
): Natural Language Generation in ArtificialIntelligence and Computational Linguistics, pages293-312.David Kauchak and Regina Barzilay.
2006.
Para-phrasing for Automatic Evaluation.
In Proceedingsof HLT-NAACL, pages 455-462.Philipp Koehn.
2004.
Pharaoh: a Beam Search De-coder for Phrase-Based Statistical Machine Transla-tion Models: User Manual and Description for Ver-sion 1.2.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical Phrase-Based Translation.
In Pro-ceedings of HLT-NAACL, pages 127-133.Raymond Kozlowski, Kathleen F. McCoy, and K.Vijay-Shanker.
2003.
Generation of single-sentence paraphrases from predicate/argumentstructure using lexico-grammatical resources.
InProceedings of IWP, pages 1-8.Aure?lien Max.
2009.
Sub-sentential Paraphrasing byContextual Pivot Translation.
In Proceedings of the2009 Workshop on Applied Textual Inference, ACL-IJCNLP 2009, pages 18-26.Franz Josef Och and Hermann Ney.
2000.
ImprovedStatistical Alignment Models.
In Proceedings ofACL, pages 440-447.Bo Pang, Kevin Knight, and Daniel Marcu.
2003.Syntax-based Alignment of Multiple Translations:Extracting Paraphrases and Generating New Sen-tences.
In Proceedings of HLT-NAACL, pages 102-109.Kishore Papineni, Salim Roukos, ToddWard, Wei-JingZhu.
2002.
BLEU: a Method for Automatic Eval-uation of Machine Translation.
In Proceedings ofACL, pages 311-318.Richard Power and Donia Scott.
2005.
Automaticgeneration of large-scale paraphrases.
In Proceed-ings of IWP, pages 73-79.Chris Quirk, Chris Brockett, andWilliamDolan.
2004.Monolingual Machine Translation for ParaphraseGeneration.
In Proceedings of EMNLP, pages 142-149.Stefan Riezler, Alexander Vasserman, IoannisTsochantaridis, Vibhu Mittal and Yi Liu.
2007.Statistical Machine Translation for Query Expan-sion in Answer Retrieval.
In Proceedings of ACL,pages 464-471.Yujie Zhang and Kazuhide Yamamoto.
2002.
Para-phrasing of Chinese Utterances.
In Proceedings ofCOLING, pages 1163-1169.Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li.
2009.Application-driven Statistical Paraphrase Genera-tion.
In Proceedings of ACL-IJCNLP 2009, pages834-842.Shiqi Zhao, Cheng Niu, Ming Zhou, Ting Liu, andSheng Li.
2008a.
CombiningMultiple Resources toImprove SMT-based Paraphrasing Model.
In Pro-ceedings of ACL-08:HLT, pages 1021-1029.Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.2008b.
Pivot Approach for Extracting ParaphrasePatterns from Bilingual Corpora.
In Proceedings ofACL-08:HLT, pages 780-788.1334
