Improving Language Models by Clustering Training SentencesDav id  Car terSRI International23 Millers YardCambridge CB2 1RQ, UKdmc@cam, sri.
comAbst rac tMany of the kinds of language model usedin speech understanding suffer from imper-fect modeling of intra-sentential contextualinfluences.
I argue that this problem can beaddressed by clustering the sentences in atraining corpus automatically into subcor-pora on the criterion of entropy reduction,and calculating separate language modelparameters for each cluster.
This kind ofclustering offers a way to represent impor-tant contextual effects and can thereforesignificantly improve the performance of amodel.
It also offers a reasonably auto-matic means to gather evidence on whethera more complex, context-sensitive modelusing the same general kind of linguistic in-formation is likely to reward the effort thatwould be required to develop it: if cluster-ing improves the performance of a model,this proves the existence of further contextdependencies, not exploited by the unclus-tered model.
As evidence for these claims, Ipresent results showing that clustering im-proves some models but not others for theATIS domain.
These results are consistentwith other findings for such models, sug-gesting that the existence or otherwise ofan improvement brought about by cluster-ing is indeed a good pointer to whether itis worth developing further the unclusteredmodel.1 In t roduct ionIn speech recognition and understanding systems,many kinds of language model may be used to choosebetween the word and sentence hypotheses for whichthere is evidence in the acoustic data.
Some words,word sequences, yntactic onstructions and seman-tic structures are more likely to occur than others,and the presence of more likely objects in a sen-tence hypothesis is evidence for the correctness ofthat hypothesis.
Evidence from different knowledgesources  can be combined in an attempt o optimizethe selection of correct hypotheses; ee e.g.
Alshawiand Carter (1994); Rayner et al(1994); Rosenfeld(1994).Many of the knowledge sources used for this pur-pose score a sentence hypothesis by calculating asimple, typically linear, combination of scores asso-ciated with objects, such as N-grams and grammarrules, that characterize the hypothesis or its pre-ferred linguistic analysis.
When these scores areviewed as log probabilities, taking a linear sum corre-sponds to making an independence assumption thatis known to be at best only approximately true, andthat may give rise to inaccuracies that reduce theeffectiveness of the knowledge source.The most obvious way to make a knowledge sourcemore accurate is to increase the amount of structureor context that it takes account of.
For example,a bigram model may be replaced by a trigram one,and the fact that dependencies xist among the like-lihoods of occurrence of grammar ules at differentlocations in a parse tree can be modeled by associat-ing probabilities with states in a parsing table ratherthan simply with the rules themselves (Briscoe andCarroll, 1993).However, such remedies have their drawbacks.Firstly, even when the context is extended, some im-portant influences may still not be modeled.
For ex-ample, dependencies between words exist at separa-tions greater than those allowed for by trigrams (forwhich long-distance N-grams \[Jelinek et al 1991\] area partial remedy), and associating scores with pars-ing table states may not model all the importantcorrelations between grammar ules.
Secondly, ex-tending the model may greatly increase the amountof training data required if sparseness problems areto be kept under control, and additional data maybe unavailable or expensive to collect.
Thirdly, onecannot always know in advance of doing the workwhether extending a model in a particular directionwill, in practice, improve results.
If it turns out notto, considerable ingenuity and effort may have beenwasted.In this paper, I argue for a general method for59extending the context-sensitivity of any knowledgesource that calculates sentence hypothesis coresas linear combinations of scores for objects.
Themethod, which is related to that of Iyer, Osten-dorf and Rohlicek (1994), involves clustering thesentences in the training corpus into a number ofsubcorpora, each predicting a different probabilitydistribution for linguistic objects.
An utterance hy-pothesis encountered at run time is then treated asif it had been selected from the subpopulation ofsentences represented by one of these subcorpora.This technique addresses as follows the three draw-backs just alluded to.
Firstly, it is able to capturethe most important sentence-internal contextual ef-fects regardless of the complexity of the probabilis-tic dependencies between the objects involved.
Sec-ondly, it makes only modest additional demands ontraining data.
Thirdly, it can be applied in a stan-dard way across knowledge sources for very differentkinds of object, and if it does improve on the unclus-tered model this constitutes proof that additional, asyet unexploited relationships exist between linguis-tic objects of the type the model is based on, andthat therefore it is worth looking for a more specific,more powerful way to model them.The use of corpus clustering often does not boostthe power of the knowledge source as much as a spe-cific hand-coded extension.
For example, a clusteredbigram model will probably not be as powerful as atrigram model.
However, clustering can have twoimportant uses.
One is that it can provide someimprovement to a model even in the absence of theadditional (human or computational) resources re-quired by a hand-coded extension.
The other use isthat the existence or otherwise of an improvementbrought about by clustering can be a good indica-tor of whether additional performance can in fact begained by extending the model by hand without fur-ther data collection, with the possibly considerableadditional effort that extension would entail.
And,of course, there is no reason why clustering shouldnot, where it gives an advantage, also be used inconjunction with extension by hand to produce yetfurther improvements.As evidence for these claims, I present experimen-tal results showing how, for a particular task andtraining corpus, clustering produces a sizeable im-provement in unigram- and bigram-based models,but not in trigram-based ones; this is consistent withexperience in the speech understanding communitythat while moving from bigrams to trigrams usuallyproduces a definite payoff, a move from trigrams to4-grams yields less clear benefits for the domain inquestion.
I also show that, for the same task andcorpus, clustering produces improvements when sen-tences are assessed not according to the words theycontain but according to the syntax rules used intheir best parse.
This work thus goes beyond thatof Iyer et alby focusing on the methodological im-portance of corpus clustering, rather than just itsusefulness in improving overall systemperformance,and by exploring in detail the way its effectivenessvaries along the dimensions of language model type,language model complexity, and number of clustersused.
It also differs from Iyer et als work by clus-tering at the utterance rather than the paragraphlevel, and by using a training corpus of thousands,rather than millions, of sentences; in many speechapplications, available training data is likely to bequite limited, and may not always be chunked intoparagraphs.2 Cluster-based Language ModelingMost other work on clustering for language model-ing (e.g.
Pereira, Tishby and Lee, 1993; Ney, Es-sen and Kneser, 1994) has addressed the problemof data sparseness by clustering words into classeswhich are then used to predict smoothed probabil-ities of occurrence for events which may seldom ornever have been observed uring training.
Thus con-ceptually at least, their processes are agglomerative:a large initial set of words is clumped into a smallernumber of clusters.
The approach described here isquite different.
Firstly, it involves clustering wholesentences, not words.
Secondly, its aim is not totackle data sparseness by grouping a large numberof objects into a smaller number of classes, but to in-crease the precision of the model by dividing a singleobject (the training corpus) into some larger num-ber of sub-objects (the clusters of sentences).
Thereis no reason why clustering sentences for predictionshould not be combined with clustering words to re-duce sparseness; the two operations are orthogonal.Our type of clustering, then, is based on the as-sumption that the utterances to be modeled, as sam-pled in a training corpus, fall more or less naturallyinto some number of clusters o that words or otherobjects associated with utterances have probabil-ity distributions that differ between clusters.
Thusrather than estimating the relative likelihood of anutterance interpretation simply by combining fixedprobabilities associated with its various characteris-tics, we view these probabilities as conditioned bythe initial choice of a cluster or subpopulation fromwhich the utterance is to be drawn.
In both cases,many independence assumptions that are known tobe at best reasonable approximations will have tobe made.
However, if the clustering reflects signif-icant dependencies, ome of the worst inaccuraciesof these assumptions may be reduced, and systemperformance may improve as a result.Some domains and tasks lend themselves more ob-viously to a clustering approach than others.
Anobvious and trivial case where clustering is likely tobe useful is a speech understander for use by trav-elers in an international airport; here, an utterancewill typically consist of words from one, and onlyone, natural anguage, and clusters for different lan-60guages will be totally dissimilar.
However, clusteringmay also give us significant leverage in monolingualcases.
If the dialogue handling capabilities of a sys-tem are relatively rigid, the system may only ask theuser a small number of different questions (modulothe filling of slots with different values).
For ex-ample, the CLARE interface to the Autoroute PCpackage (Lewin et al 1993) has a fairly simple dia-logue model which allows it to ask only a dozen orso different ypes of question of the user.
A Wizardof Oz exercise, carried out to collect data for thistask, was conducted in a similarly rigid way; thus itis straightforward to divide the training corpus intoclusters, one cluster for utterances immediately fol-lowing each kind of system query.
Other corpora,such as Wall Street Journal articles, might also beexpected to fall naturally into clusters for differentsubject areas, and indeed Iyer el al (1994) reportpositive results from corpus clustering here.For some applications, though, there is no obviousextrinsic basis for dividing the training corpus intoclusters.
The ARPA air travel information (ATIS)domain is an example.
Questions can mention con-cepts such as places, times, dates, fares, meals, air-lines, plane types and ground transportation, butmost utterances mention several of these, and thereare few obvious restrictions on which of them can oc-cur in the same utterance.
Dialogues between a hu-man and an ATIS database access ystem are there-fore likely to be less clearly structured than in theAutoroute case.However, there is no reason why automatic lus-tering should not be attempted even when thereare no grounds to expect clearly distinct underly-ing subpopulations to exist.
Even a clustering thatonly partly reflects the underlying variability of thedata may give us more accurate predictions of ut-terance likelihoods.
Obviously, the more clustersare assumed, the more likely it is that the increasein the number of parameters to be estimated willlead to worsened rather than improved performance.But this trade-off, and the effectiveness of differentclustering algorithms, can be monitored and opti-mized by applying the resulting cluster-based lan-guage models to unseen test data.
In Section 4 be-low, 1 report results of such experiments with ATISdata, which, for the reasons given above, would atfirst sight seem relatively unlikely to yield useful re-suits from a clustering approach.
Since, as we willsee, clustering does yield benefits in this domain, itseems very plausible that it will also do so for other,more naturally clustered omains.3 C lus ter ing  A lgor i thmsThere are many different criteria for quantifying the(dis)similarity between (analyses of) two sentencesor between two clusters of sentences; Everitt (1993)provides a good overview.
Unfortunately, whateverthe criterion selected, it is in general impractical tofind the optimal clustering of the data; instead, oneof a variety of algorithms must be used to find alocally optimal solution.Let us for the moment consider the case wherethe language model consists only of a unigram prob-ability distribution for the words in the vocabulary,with no N-gram (for N > 1) or fuller linguisticconstraints considered.
Perhaps the most obviousmeasure of the similarity between two sentences orclusters is then Jaccard's coefficient (Everitt, 1993,p41), the ratio of the number of words occurring inboth sentences to the number occurring in either orboth.
Another possibility would be Euclidean dis-tance, with each word in the vocabulary defininga dimension in a vector space.
However, it makessense to choose as a similarity measure the quan-tity we would like the final clustering arrangementto minimize: the expected entropy (or, equivalently,perplexity) of sentences from the domain.
This goalis analogous to that used in the work described ear-lier on finding word classes by clustering.For our simple unigram language model withoutclustering, the training corpus perplexity is mini-mized (and its likelihood is maximized) by assigningeach word wi a probability Pi = f i /N ,  where f/ isthe frequency of wi and N is the total size of the cor-pus.
The corpus likelihood is then P1 = l-\[i P{', andthe per-word entropy, -Y'\],L,, pi log(pi) ,  is thus min-imized.
(See e.g.
Cover and Thomas, 1991, chapter2 for the reasoning behind this).If now we model the language as consisting of sen-tences drawn at random from K different subpopu-lations, each with its own unigram probability dis-tribution for words, then the estimated corpus prob-ability isP~ = I-I~j ~ck qk l-I~,~j pk,~where the iterations are over each utterance uj inthe corpus, each cluster c l .
.
.
cg  from which ujmight arise, and each word wi in utterance uj.qk = Ickl /~ Ic~l is the likelihood of an utterancearising from cluster (or subpopulation) ck, and pk,iis the likelihood assigned to word wi by cluster k,i.e.
its relative frequency in that cluster.Our ideal, then, is the set of clusters that maxi-mizes the cluster-dependent corpus likelihood PK.As with nearly all clustering problems, finding aglobal maximum is impractical.
To derive a goodapproximation to it, therefore, we adopt the follow-ing algorithm.?
Select a random ordering of the training corpus,and initialize each cluster ck ,k  = 1 .
.
.K ,  tocontain just the kth sentence in the ordering.?
Present each remaining training corpus sentencein turn, initially creating an additional singletoncluster cK+l for it.
Merge that pair of clustersc l .
?.
CK+I that entails the least additional cost,i.e.
the smallest reduction in the value of PK forthe subcorpus een so far.61?
When all training utterances have been incor-porated, find all the triples (u, ci,cj), i ~ j,such that u E ci but the probability of u ismaximized by cj.
Move all such u's (in paral-lel) between clusters.
Repeat until no furthermovements are required.In practice, we keep track not of PK but of theoverall corpus entropy HK = -log( Pl,: ).
We recordthe contribution each cluster c~ makes to HK asHK(ek) = -- ~wieck fiklog(fik/Fk)where fik is the frequency of wi in ck and Fk =~wjeck fjk, and find the value of this quantity forall possible merged clusters.
The merge in the sec-ond step of the algorithm is chosen to be the oneminimizing the increase in entropy between the un-merged and the merged clusters.The adjustment process in the third step of thealgorithm does not attempt directly to decrease n-tropy but to achieve a clustering with the obviouslydesirable property that each training sentence is bestpredicted by the cluster it belongs to rather thanby another cluster.
This heightens the similaritieswithin clusters and the differences between them.It also reduces the arbitrariness introduced into theclustering process by the order in which the trainingsentences are presented.
The approach is applica-ble with only a minor modification to N-grams forN > 1: the probability of a word within a cluster isconditioned on the occurrence of the N-1  words pre-ceding it, and the entropy calculations take this intoaccount.
Other cases of context dependence mod-eled by a knowledge source can be handled similarly.And there is no reason why the items characterizingthe sentence have to be (sequences of) words; occur-rences of grammar ules, either without any contextor in the context of, say, the rules occurring justabove them in the parse tree, can be treated in justthe same way.4 Experimental  ResultsExperiments were carried out to assess the effective-ness of clustering, and therefore the existence of un-exploited contextual dependencies, for instances oftwo general types of language model.
In the firstexperiment, sentence hypotheses were evaluated onthe N-grams of words and word classes they con-tained.
In the second experiment, evaluation was onthe basis of grammar ules used rather than wordoccurrences.4.1 N-gram ExperimentIn the first experiment, reference versions of a setof 5,873 domain-relevant (classes A and D) ATIS-2 sentences were allocated to K clusters for K =2, 3, 5, 6, 10 and 20 for the unigram, bigram and tri-gram conditions and, for unigrams and bigrams only,K = 40 and 100 as well.
Each run was repeatedfor ten different random orders for presentation ofthe training data.
The unclustered (K = 1) versionof each language model was also evaluated.
Somewords, and some sequences of words such as "SanFrancisco", were replaced by class names to improveperformance.The improvement (if any) due to clustering wasmeasured by using the various language models tomake selections from N-best sentence hypothesislists; this choice of test was made for conveniencerather than out of any commitment to the N-bestparadigm, and the techniques described here couldequally well be used with other forms of speech-language interface.Specifically, each clustering was tested against1,354 hypothesis lists output by a version of theDECIPHER (TM) speech recognizer (Murveit etal, 1993) that itself used a (rather simpler) bigrammodel.
Where more then ten hypothesis were out-put for a sentence, only the top ten were considered.These 1,354 lists were the subset of two 1,000 sen-tence sets (the February and November 1992 ATISevaluation sets) for which the reference sentence it-self occurred in the top ten hypotheses.
The clus-tered language model was used to select the mostlikely hypothesis from the list without paying anyattention either to the score that DECIPHER as-signed to each hypothesis on the basis of acousticinformation or its own bigram model, or to the or-dering of the list.
In a real system, the DECIPHERscores would of course be taken into account, butthey were ignored here in order to maximize the dis-criminatory power of the test in the presence of onlya few thousand test utterances.To avoid penalizing longer hypotheses, the prob-abilities assigned to hypotheses were normalized bysentence length.
The probability assigned by a clus-ter to an N-gram was taken to be the simple maxi-mum likelihood (relative frequency) value where thiswas non-zero.
When an N-gram in the test data hadnot been observed at all in the training sentencesassigned to a given cluster, a "failure", represent-ing a vanishingly small probability, was assigned.A number of backoff schemes of various degrees ofsophistication, including that of Katz (1987), weretried, but none produced any improvement in per-formance, and several actuMly worsened it.The average percentages of sentences correctlyidentified by clusterings for each condition were asgiven in Table 1.
The maximum possible score was100%; the baseline score, that expected from a ran-dom choice of a sentence from each list, was 11.4%.The unigram and bigram scores show a steadyand, in fact, statistically significant 1 increase withthe number of clusters.
Using twenty clusters forbigrams (score 43.9%) in fact gi~ces more than halfthe advantage over unclustered bigrams that is given1 Details of significance tests are omitted for space rea-sons.
They are included in a longer version of this paperavailable on request from the author.62Clusters Unigram Bigram Trigram1 12.4 34.3 51.62 13.8 37.9 51.03 15.3 39.5 50.84 16.1 41.2 50.45 16.8 41.2 51.06 17.2 41.8 50.710 17.8 43.1 51.220 19.9 43.9 50.340 22.3 45.0100 24.4 46.4Table 1: Average percentage scores for cluster-basedN-gram modelsClusters 1-rule 2-rule1 29.4 34.32 31.4 35.53 31.8 36.24 31.7 37.05 32.3 37.26 31.9 37.310 32.8 37.520 35.1 38.340 35.8 38.9Table 2: Average percentage scores for cluster-basedN-rule modelsby moving from unclustered bigrams to unclusteredtrigrams.
However, clustering trigrams producesno improvement in score; in fact, it gives a smallbut statistically significant deterioration, presum-ably due to the increase in the number of parametersthat need to be calculated.The random choice of a presentation order for thedata meant that different clusterings were arrivedat on each run for a given condition ((N, K) for N-grams and K clusters).
There was some limited ev-idence that some clusterings for the same conditionwere significantly better than others, rather thanjust happening to perform better on the particulartest data used.
More trials would be needed to es-tablish whether presentation order does in generalmake a genuine difference to the quality of a cluster-ing.
If there is one, however, it would appear to befairly small compared to the improvements available(in the unigram and bigram cases) from increasingthe numbers of clusters.4.2 Grammar Rule ExperimentIn the second experiment, each training sentence andeach test sentence hypothesis was analysed by theCore Language Engine (Alshawi, 1992) trained onthe ATIS domain (Agn~ et al 1994).
Unanalysablesentences were discarded, as were sentences of over15 words in length (the ATIS adaptation had concen-trated on sentences of 15 words or under, and anal-ysis of longer sentences was less reliable and slower).When a sentence was analysed successfully, severalsemantic analyses were, in general, created, and aselection was made from among these on the basisof trained preference functions (Alshawi and Carter,1994).
For the purpose of the experiment, clusteringand hypothesis election were performed on the ba-sis not of the words in a sentence but of the grammarrules used to construct its most preferred analysis.The simplest condition, hereafter referred to as "l-rule", was analogous to the unigram case for word-based evaluation.
A sentence was modeled simply asa bag of rules, and no attempt (other than the clus-tering itself) was made to account for dependenciesbetween rules.Another condition, henceforth "2-rule" because ofits analogy to bigrams, was also tried.
Here, eachrule occurrence was represented not in isolation butin the context of the rule immediately above it inthe parse tree.
Other choices of context might haveworked as well or better; our purpose here is simplyto illustrate and assess ways in which explicit contextmodeling can be combined with clustering.The training corpus consisted of the 4,279 sen-tences in the 5,873-sentence s t that were analysableand consisted of fifteen words or less.
The test cor-pus consisted of 1,106 hypothesis lists, selected inthe same way (on the basis of length and analysabil-ity of their reference sentences) from the 1,354 usedin the first experiment.
The "baseline" score forthis test corpus, expected from a random choiceof (analysable) hypothesis, was 23.2%.
This wasrather higher than the 11.4% for word-based selec-tion because the hypothesis lists used were in gen-eral shorter, unanalysable hypotheses having beenexcluded.The average percentages of correct hypotheses(actual word strings, not just the rules used to rep-resent them) selected by the 1-rule and 2-rule con-ditions were as given in Table 2.These results show that clustering ives a signif-icant advantage for both the 1-rule and the 2-ruletypes of model, and that the more clusters are cre-ated, the larger the advantage is, at least up toK = 20 clusters.
As with the N-gram experiment,there is weak evidence that some clusterings are gen-uinely better than others for the same condition.5 Conc lus ionsI have suggested that training corpus clustering canbe used both to extend the effectiveness of a verygeneral class of language models, and to provide ev-idence of whether a particular language model couldbenefit from extending it by hand to allow it to takebetter account of context.
Clustering can be usefuleven when there is no reason to believe the training63corpus naturally divides into any particular numberof clusters on any extrinsic grounds.The experimental results presented show thatclustering increases the (absolute) success rate of un-igram and bigram language modeling for a particu-lar ATIS task by up to about 12%, and that perfor-mance improves teadily as the number of clustersclimbs towards 100 (probably a reasonable upperlimit, given that there are only a few thousand train-ing sentences).
However, clusters do not improve tri-gram modeling at all.
This is consistent with experi-ence (Rayner et al 1994) that, for the ATIS domain,trigrams model inter-word effects much better thanbigrams do, but that extending the N-gram modelbeyond N = 3 is much less beneficial.For N-rule modeling, clustering increases the suc-cess rate for both N = 1 and N = 2, although onlyby about half as much as for N-grams.
This sug-gests that conditioning the occurrence of a grammarrule on the identity of its mother (as in the 2-rulecase) accounts for some, but not all, of the contex-tual influences that operate.
From this it is sensibleto conclude, consistently with the results of Briscoeand Carroll (1993), that a more complex model ofgrammar rule interaction might yield better esults.Either conditioning on other parts of the parse treethan the mother node could be included, or a ratherdifferent scheme such as Briscoe and Carroll's couldbe used.Neither the observation that trigrams may repre-sent the limit of usefulness for N-gram modeling inATIS, nor that non-trivial contextual influences ex-ist between occurrences of grammar ules, is verynovel or remarkable in its own right.
Rather, whatis of interest is that the improvement (or otherwise)in particular language models from the applicationof clustering is consistent with those observations.This is important evidence for the main hypothe-sis of this paper: that enhancing a language modelwith clustering, which once the software is in placecan be done largely automatically, can give us im-portant clues about whether it is worth expendingresearch, programming, data-collection a d machineresources on hand-coded improvements to the way inwhich the language model in question models con-text, or whether those resources are best devoted todifferent, additional kinds of language model.AcknowledgementsThis research was partly funded by the DefenceResearch Agency, Malvern, UK, under assignmentM85T51XX.I am grateful to Manny Rayner and Ian Lewin foruseful comments on earlier versions of this paper.Responsibility for any remaining errors or unclaritiesrests in the customary place.ReferencesAgn~s, M-S., et al1994).
Spoken Language Transla-tor First Year Report.
SRI International Cam-bridge Technical Report CRC-043.Alshawi, H., and D.M.
Carter (1994).
"Training andScaling Preference Functions for Disambigua-tion".
Computational Linguistics (to appear).Briscoe, T., and J. Carroll (1993).
"GeneralizedProbabilistic LR Parsing of Natural Language(Corpora) with Unification-Based Grammars",Computational Linguistics, Vol 19:1, 25-60.Cover, T.M., and J.A.
Thomas (1991).
Elements ofInformation Theory.
New York: Wiley.Everitt, B.S.
(1993).
Cluster Analysis, Third Edi-tion.
London: Edward Arnold.Iyer, R., M. Ostendorf and J.R. Rohlicek (1994).
"Language Modeling with Sentence-Level Mix-tures".
Proceedings of the ARPA Workshop onHuman Language Technology.Jelinek, F., B. Merialdo, S. Roukos and M. Strauss(1991).
"A Dynamic Language Model forSpeech Recognition", Proceedings of the Speechand Natural Language DARPA Workshop, Feb1991, 293-295.Katz, S.M.
(1987).
"Estimation of Probabilitiesfrom Sparse Data for the Language Model Com-ponent of a Speech Recognizer", IEEE Transac-tions on Acoustics, Speech and Signal Process-ing, Vol ASSP-35:3.Lewin, I., D.M.
Carter, S. Pulman, S. Brown-ing, K. Ponting and M. Russell (1993).
"ASpeech-Based Route Enquiry System BuiltFrom General-Purpose Components", Proceed-ings of Eurospeech-93.Murveit, H., J. Butzberger, V. Digalakis andM.
Weintraub (1993).
"Large Vocabulary Dic-tation using SRI's DECIPHER(TM) SpeechRecognition System: Progressive Search Tech-niques", Proceedings of the International Con-ference on Acoustics, Speech and Signal Pro-cessing, Minneapolis, Minnesota.Ney, H., U. Essen and R. Kneser (1994).
"On Struc-turing Probabilistic Dependencies in Stochas-tic Language Modeling".
Computer Speech andLanguage, vol 8:1, 1-38.Pereira, F., N. Tishby and L. Lee (1993).
"Distribu-tional Clustering of English Words".
Proceed-ings of ACL-93, 183-190.Rayner, M., D. Carter, V. Digalakis and P. Price(1994).
"Combining Knowledge Sources to Re-order N-best Speech Hypothesis Lists".
Pro-ceedings of the ARPA Workshop on HumanLanguage Technology.Rosenfeld, R. (1994).
"A Hybrid Approach to Adap-tive Statistical Language Modeling".
Proceed-ings of the ARPA Workshop on Human Lan-guage Technology.64
