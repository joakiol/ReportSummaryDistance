Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 101?110,Atlanta, Georgia, June 13 2013. c?2013 Association for Computational LinguisticsApproved for Public Release; Distribution Unlimited.
13-1876Discriminating Non-Native English with 350 WordsJohn Henderson, Guido Zarrella, Craig Pfeifer and John D. BurgerThe MITRE Corporation202 Burlington RoadBedford, MA 01730-1420, USA{jhndrsn,jzarrella,cpfeifer,john}@mitre.orgAbstractThis paper describes MITRE?s participation inthe native language identification (NLI) taskat BEA-8.
Our best effort performed at an ac-curacy of 82.6% in the eleven-way NLI task,placing it in a statistical tie with the best per-forming systems.
We describe the varietyof machine learning approaches that we ex-plored, including Winnow, language model-ing, logistic regression and maximum-entropymodels.
Our primary features were word andcharacter n-grams.
We also describe severalensemble methods that we employed for com-bining these base systems.1 IntroductionInvestigations into the effect of authors?
latent at-tributes on language use have a long history in lin-guistics (Labov, 1972; Biber and Finegan, 1993).The rapid growth of social media has sparked in-creased interest in automatically identifying authorattributes such as gender and age (Schler et al 2006;Burger and Henderson, 2006; Argamon et al 2007;Mukherjee and Liu, 2010; Rao et al 2010).
Thereis also a long history of computational aids for lan-guage pedagogy, both for first- and second-languageacquisition.
In particular, automated native languageidentification (NLI) is a useful aid to second lan-guage learning.
This is our first foray into NLI,although we have recently described experimentsaimed at identifying the gender of unknown Twit-ter authors (Burger et al 2011).
We performed wellusing only character and word n-grams as evidence.In the present work, we apply that same approachto NLI, and combine it with several other baselineclassifiers.In the remainder of this paper, we describe ourhigh-performing system for identifying the nativelanguage of English writers.
We explore a variedset of learning algorithms and present two ensem-ble methods used to produce a better system thanany of the individuals.
In Section 2 we describe thedata and task in detail as well as the evaluation met-ric.
In Section 3 we discuss details of the particularsystem configuration that scored best for us.
We de-scribe our experiments in Section 4, including ourexploration of several different classifier types andparametrizations.
In Section 5 we present and an-alyze performance results, and inspect some of thefeatures that were useful in discrimination.
Finallyin Section 6 we summarize our findings, and de-scribe possible extensions to the work.2 Task, data and evaluationNative Language Identification was a shared task or-ganized as part of the Eighth Workshop on Innova-tive Use of NLP for Building Educational Applica-tions, 2013.
The task was to identify an author?snative language based on an English essay.The data provided consisted of a set of 12,100Test of English as a Foreign Language (TOEFL) ex-aminations contributed by the Educational TestingService (Blanchard et al to appear).
These wereEnglish essays written by native speakers of Arabic,Chinese, French, German, Hindi, Italian, Japanese,Korean, Spanish, Telugu, and Turkish.
A set of 1000essays for each language was identified as trainingdata, along with 100 per language for development,101and another 100 per language for a final test set.
Themean length of an essay is 348 words.The primary evaluation metric for shared tasksubmissions was simple accuracy: the fraction of thetest essays for which the correct native language wasidentified.
A baseline accuracy would thus be about9% (one out of eleven).
Results were also reportedin terms of F-measure on a per-language basis.
F-measure is a harmonic mean of precision and recall:F = 2PRP+R .
For the evaluation, the precision de-nominator was the number of items labeled with aparticular language by the system and the recall de-nominator was the number of items marked with aparticular language in the reference set.The training, development, and test sets all hadbalanced distributions across the native languages,so error rates and accuracy did not favor any partic-ular language in any set.3 System overviewThe systems we used to generate results for the NLIcompetition were all machine-learning-based, withno handwritten rules or features.
The final submittedsystems were ensembles built from the outputs andconfidence scores of independent eleven-way multi-nomial classifiers.3.1 FeaturesThe features used to build these systems werelanguage-independent and were generated using thesame infrastructure designed for the experiments de-scribed in Burger et al(2011).We incorporated a variety of binary features intoour systems, each of which was hashed into a 64-bitnumeric representation using MurmurHash3 (Ap-pleby, 2011).
The bulk of our features were case-sensitive word- and character-based n-grams, inwhich a feature was turned ?on?
if its sequence ofwords or characters appeared at least once in the textof an essay.
We also added binary features describ-ing surface characteristics of the text such as averageword length and word count.
Features were sepa-rated into tracks such that the word unigram ?i?
andthe character unigram ?i?
would each generate a dis-tinct feature.Part of speech tag n-grams were added to thefeature set after reviewing performance results inBrooke and Hirst (2012).
We used the Stan-ford log-linear part of speech tagger described inToutanova et al(2003), with the english-left3words-distsim.tagger pretrained model and the Penn Tree-bank tagset.
The tagger was run on each essay andoutputs were incorporated as sequence features withn-grams up to length 5.3.2 ClassifiersCarnie1 is a MITRE-developed linear classifierthat implements the Winnow2 algorithm of Carvalhoand Cohen (2006), generalized for multinomial clas-sification.
Carnie was developed to perform clas-sification of short, noisy texts with many trainingexamples.
It maintains one weight per feature peroutput class, and performs multiplicative updatesthat reinforce weights corresponding to the correctclass while penalizing weights associated with thetop-scoring incorrect class.
The learner is mistake-driven and performs an update of size  after an erroror when the ratio of weight masses of the correct andtop incorrect classes is below 1 + ?.
It iterates overthe training data, cooling its updates after each itera-tion.
For the purposes of these experiments, an inputto Carnie was the text of a single TOEFL essay, andthe output was the highest scoring class and severalrelated scores.SRI?s Language Modeling Toolkit (SRILM) isa toolkit for sequence modeling that continues tobe relevant after more than a decade of develop-ment (Stolcke, 2002).
It can be used to both buildmodels of sequence likelihoods and to evaluate like-lihoods of previously unseen sequences.
Buildinga multinomial text classifier with a language modeltoolkit involves building one model for each targetclass and choosing the label whose model gives thehighest probability.Many smoothing methods are implemented bySRILM, along with a variety of n-gram filter-ing techniques.
The out-of-the-box default con-figuration produces trigram models with Good-Turing smoothing.
It worked well for this com-petition.
Using open vocabulary models (-unk),turning off sentence boundary insertion (-no-sos-no-eos) and treating each essay as one sentence1It is named for entertainers who guess personal character-istics of carnival goers.102worked best in our development environment.LIBLINEAR is a popular open source library forclassification of large, sparse data.
We experimentedwith several of their standard Support Vector Ma-chine and logistic regression configurations (Fan etal., 2008).
We selected multiclass `2-regularizedlogistic regression with the dual-form solver anddefault parameters.
Inputs to the model were bi-nary features generated from a single TOEFL essay.Features for this model were generated by Carnie.The model provided probability estimates for eachcandidate output class (L1) for each essay, whichwere then combined with the outputs of Carnie andSRILM in an ensemble to produce a single predic-tion.3.3 EnsemblesThe classifiers described above were selected for in-clusion as components in a larger ensemble on thebasis of their performance and the observation thaterrors committed by these systems were not highlycorrelated.
We used the entirety of our training datafor construction of each component system, leavingscant data available for estimating parameters of en-sembles.
This scenario led us to choose naive Bayesto combine the outputs of the original components.Given h1, .
.
.
, hk hypothesis labels from k differ-ent systems, one approximates the conditional like-lihood of the reference label P (R|H1 .
.
.
Hk) usingthe Bayes transform and the development set esti-mates of P (Hi|R).
One investigates all possible la-bels to decode r?
= argmaxr P (r)?i P (hi|r).
Theclass balance in every set we operated on made theprior P (r) irrelevant for maximization and simpli-fied many of the denominators along the way.
Thisis a typical formulation of naive Bayes.Confidence All of our component systems pro-duce scores as well as a predicted label.
Carnie pro-duces (non-probability) scores for all of the candi-date labels, SRILM produces log-probabilities andperplexities, and LIBLINEAR produces P (h|r), thelikelihood of each of the possible labels.
We ex-perimented with several transformations of thosescores to best use them to predict correctness oftheir hypothesis.
There were several graphical mod-els we could use for folding these scores into theBayes ensemble, and we chose a simple, discretizedP (H,S|R).
We evenly partitioned and relabeled oursystem outputs according to their scores (S), andused those partition labels in the Bayes ensemble.Thus when a particular reference label was scoredin the ensemble during decoding, both its predictionand score contributed to the label in the naive Bayestable lookup.3.4 Best configurationWe submitted five systems with a variety of con-figurations.
One of our systems was our individualCarnie system on its own for calibration.
The otherfour were ensembles.The best system we submitted was a Bayes en-semble of the Carnie, SRILM, and LIBLINEARcomponents each trained on the train+developmentsets.
Carnie was trained for twelve iterations with = 0.03, ?
= 0.05, and a cooling rate of 0.1.SRILM models were trained for open vocabularyand the default trigram, Good-Turing setting.
Lo-gistic regression from LIBLINEAR was run with `2regularization and using the dual form solver.Parameters for the Bayes model were collectedfrom the development set when the componentswere trained only on the training set.
A grid searchwas performed over likely candidates for ?, theDirichlet parameter, and ?, the number of score-based partitions, resulting in ?
= 0.03125 and ?
=2.
The grid search was performed with the compo-nent models trained only on the training set and us-ing 10-fold cross validation on the development set.4 ExperimentsIn all experiments described below, systems weretrained initially on the 9900 training examples alone,with the 1100 item development set held back to al-low for hyperparameter estimation.
When prepar-ing our final test set submissions, the developmentset was folded into the training data, and all modelswere re-trained on this new dataset containing 11000items.4.1 BaselinesHow hard is the NLI task?
Simple baselines of-ten give us a quick glimpse into what matters in aNLP task.
In Figure 1, we give accuracy resultson ten different baselines we trained on the training103Baseline Accuracy(%)random 9.1char length 9.6SRILM(letter unigram) 10.8word length 12.0proficiency 14.9SRILM(letter bigram) 15.1JS(vowels) 20.6JS(consonants) 33.8JS(vowels+consonants) 34.1JS(bag-of-words) 52.5Figure 1: Simple baseline development set scores.set and evaluated on the development set.
Predic-tions based on simple character and word lengthsshow only slight gains over random.
Using thehigh/medium/low proficiency score that accompa-nied the data similarly gives a tiny amount of infor-mation over baseline (14.9%).
We ignored those rat-ings elsewhere in our work, to focus on the core taskof prediction based on essay content.We collected some simple distributions of voweland consonant clusters and used them for predic-tion, scoring with Jensen-Shannon divergence.
JSdivergence is a symmetrized form of KL divergenceto alleviate the mathematical problem involved withmissing observations.
It has behaved well in thecontext of language processing applications (Lee,1999).
The score progression from consonant clus-ters, to vowel clusters, to words suggests that thereis NLI information scattered at various levels of sur-face features.4.2 Varied Carnie configurationsCarnie?s out-of-the-box configuration is one that hasbeen optimized for application to micro-blogs andother ungrammatical short texts.
While our hypoth-esis was that this configuration would be well suitedto analysis of English TOEFL essays, we investi-gated a number of possible techniques to help Carnieadapt to the new domain.We began by performing a grid search to selectmodel hyperparameters that enabled our standardconfiguration to generalize well from the trainingdataset to the development dataset.
These values of, ?, and cooling rate were then applied to variousnew feature configurations.The standard configuration included binary fea-tures for word unigrams and bigrams, character n-grams of sizes 1 to 5, and surface features.
Weexperimented here with word trigrams, character 6-grams, and lowercased character n-grams of sizes 1to 6.
We also added skip bigrams, which were or-dered word pairs in which 1 to 6 intervening wordswere omitted.
We incorporated part of speech tags ina number of ways, including POS n-grams of lengths1 to 5, POS k-skip bigrams with k ranging from 1 to6, and POS n-grams in which closed-class POS tagswere replaced with the actual content word used.We also measured the impact of using frequency-weighted features.Our standard approach with Carnie is to performmultinomial classification using one model trainedon all the data simultaneously.
We experimentedwith other ways of framing the NLI problem, suchas building eleven binary classifiers, each of whichwas trained on all of the data but with the sole taskof accepting or rejecting a single candidate L1.
Wealso partitioned the training data to build 55 binaryclassifiers for all possible pairs of L1s.
These bi-nary classifiers were then combined via a votingmechanism to select a single winner.
This allowedus to apply focused efforts to improve discrimina-tion in language pairs which Carnie found challeng-ing, such as Hindi-Telugu or Japanese-Korean.
Tothis end, we collected a substantial amount of ad-ditional out-of-domain training data from the web-sites lang8.com (70,000 entries) and gohackers.com(40,000 entries).
Although we did not use thisdata in our final submission, we performed experi-ments to measure the value of this new data in theTOEFL11 domain with no adaptation, with featurefiltering to limit training features to items observedin the test sets, and with ?frustratingly easy?
do-main adaptation, EasyAdapt, described in Daum?and Marcu (2007).4.3 Varied SRILM configurationsSRILM offers a number of parameters for ex-perimentation.
We hill-climbed on the train-ing/development split to select a good configura-tion.
We experimented with n-gram lengths from1-5 (bag of words through word 5-grams), using thetokenization given by the NLI organizers.
We triedthe lighter weight smoothing techniques offered by104System Confidence MRDCarnie s(h1)/s(h2) 343s(h1)/?i s(hi) 268s(h1)?
s(h2) 72SRILM log p(h1)/ log p(h2) 315.7log p(h1)?
log p(h2) 315.3ppl1(h1)/ppl1(h2) 315.12ppl1(h1)?
ppl1(h2) 260ppl1 77log p(h1) 40MaxEnt?i p(hi) log p(hi) 385.7(JCarafe) p(h1) 383.15log p(h1) 383.15p(h1)/p(h2) 373.75log p(h1)/ log p(h2) 379.8LIBLINEAR?i p(hi) log p(hi) 379.8Figure 2: Confidence candidates measured in Mean RankDifference between correct and incorrect labels.SRILM including Good-Turing, Witten-Bell, Ris-tad?s natural discounting, both modified and originalKneser-Ney.
We built both closed vocabulary andopen vocabulary language models and with specialsymbols added for sentence boundaries.4.4 Component confidence experimentsOur components generate scores, but those scoreswere not always scaled in the same way.
Winnow(in Carnie) is a margin-based, mistake-driven learnergenerating scores which are interpretable only assums of weights.
SRILM produces log p(dj |hi),but renormalizing those (with priors) into estimatesof p(hi|dj) is unreliable because the different sub-models are not connected with smoothing.
Logisticregression produces a distribution for p(hi|dj).
Weaimed to express these notions of confidence in away that was common to all systems.
We did this byrelabeling system hypotheses after sorting by confi-dence, but not all metrics were equally good at thissorting.We performed an ad hoc assessment of severalcandidate scoring functions.
Our goal was to findfunctions that best separated correct answers fromincorrect answers in a sorted ranking.
We ran severalcandidates on our development set and measured thedifference between the mean rank of correct answersand the mean rank of incorrect answers.
Figure 2displays the results.
In each case h1 was the best hy-pothesis generated by the system and h2 is secondbest.
p(?)
indicates probabilities, s(?)
indicates non-probability scores.
We chose those functions withthe highest values.4.5 Simple models for combinationIn this work, we focused our ensembles only on theoutput of our individual components, ignoring thefeatures from the original data that they attempt tomodel.
The base systems are all trained to minimizeerrors, and did not appear to have any particularpreferential capabilities.
Thus we rely on them en-tirely for the primary processing and focus on theiroutputs.In our naive Bayes formulation, the random vari-ables produced by the component systems (H) neednot take on values directly comparable with the ref-erence labels to be predicted (R).
We experimentedwith folding in several one-shot systems that pro-duced labels in {L, L?
}, for particular native lan-guage groups, but none of these proved to be goodcomplements for the components described above.To cope with decode-time configurations of Hthat hadn?t been seen during estimation, we useda Dirichlet prior on R in this ensemble.
A sin-gle parameter, ?, was introduced.
Thus our esti-mates for P (hi|r) were based on smoothed counts:c(hi,r)+?c(r)+?|R| .
The search for ?
was performed usingcross-validation on the development set.Assignment In many prediction settings, we knowthat our evaluation data consists of examples drawnfrom a particular allocation of candidate classes.One can take advantage of this in a probabilisticsetting by doing a global search for the maximumlikelihood assignment of the test documents to theL1 languages under the constraint that each L1 lan-guage must have a particular occupancy by the doc-uments ?
in this case, an even split.
More generally,once we have p(hi|dj) for each candidate languagehi and document dj , we can find an assignmentA ={(i, j) : ?i,j = 1} that maximizes the likelihoodP (H|D) =?
(i,j)?A p(hi|dj) =?i,j p(hi|dj)?i,junder the constraints that?i ?i,j = |D|/|H| and?j ?i,j = 1.
The first constraint says that each lan-guage should get an even allocation of documentsassigned to it and the second constraint says that105each document should be assigned to only one lan-guage.
This reduces to a maximum weight match-ing on?i,j ?i,j log p(hi|dj).
This problem is di-rectly convertible into a max flow problem or a lin-ear program.
It can be solved with methods suchas the Hungarian algorithm, Ford-Fulkerson, or lin-ear programming.
In our case, we used LPSOLVE2to find this global maximum.
This looks at firstglance like an integer programming problem, butone can relax the constraints into inequalities andstill be guaranteed that the solution will end up withall ?i,j landing on either zero or one in the rightamounts.
We applied this assignment combinationas a post-processing step to the probabilities gener-ated in the naive Bayes ensemble and also to the rawLIBLINEAR outputs.
The hope in doing this is thatthe optimizer will move the less likely assignmentsaround appropriately while preserving the assign-ments where it has more confidence.
We observedmixed results on our development set and submittedtwo systems using this ensemble technique.4.6 Other components exploredLIBLINEAR provides an implementation of a linearSVM as well as a logistic regression package.
Weexperimented with various combinations of `1- and`2 -loss SVMs, with both `1 and `2-regularization,but in the end opted to use the `2-regularized logisticregression due to slightly superior performance andthe ease with which we could extract eleven valuesof P (H) for inclusion in our ensemble.Another component that was tested in develop-ment of our ensemble systems was a maximum en-tropy classifier.
This particular effort used the imple-mentation from JCarafe,3 which uses L-BFGS foroptimization.We approached the NLI task as document classi-fication, following a typical JCarafe recipe (Gibsonet al 2007).
The class of the document is the nativelanguage of the author.
Each document was treatedas a bag of words, and several classes of featureswere extracted: token n-gram frequency, charactern-gram frequency, part of speech n-gram frequency.The feature mix that produced the best score wastoken bigrams and trigrams, character trigrams and2http://lpsolve.sourceforge.net3https://github.com/wellner/jcarafeL1 Mean F Our Best FGER 1 0.776 1 0.921ITA 2 0.757 2 0.88CHI 3 0.723 4 0.85JPN 4 0.708 5 0.837FRE 5 0.701 7 0.818TEL 6 0.667 3 0.802KOR 7 0.665 6 0.827TUR 8 0.656 8 0.81ARA 9 0.65 3 0.872SPA 10 0.631 10 0.768HIN 11 0.606 11 0.762Figure 3: L1s by empirical prediction difficulty.
Mean Fincorporates all submissions by all competition teams.POS trigrams.
A feature frequency threshold of 5was used to curb the number of features.5 ResultsOur best performing ensemble was 82.6% accuratewhen scored on the competition test set, and wascomposed of Carnie, SRILM, and logistic regres-sion, using naive Bayes to combine the subsystemoutputs and confidence scores into a single predic-tion.
The best performing subsystem during systemdevelopment scored 79.3% on the test set in isola-tion, demonstrating once again the value of combin-ing systems that make independent errors.Certain L1s gave our systems more difficulty thanothers.
Our best submitted F-measure scores rangedfrom 0.921 for German to 0.762 for Hindi.
Fig-ure 3 demonstrates that our systems?
scores werehighly correlated with average scores from all sub-missions by all teams (R2 = 0.84).
From this weinfer that our performance differences between L1smay be explained by inherent difficulties in certainlanguages or by the selection of similar L1s as a partof the competition task, rather than quirks of our ap-proach.
Our submissions do appear to have a partic-ular advantage on Arabic and Korean, relative to thefield.Figure 4 shows the overall performance of oursubmissions and subsystems on the developmentand test evaluation sets.Our scores dropped 4 to 5% between developmentand test evaluations, representing significant overfit-106Configuration dev % test %Componentsbase Carnie 82.6+ trigrams 83.1+ POS tags 83.6 79.31v1 voted Carnie 79.4SRILM 77.1MaxEnt 77.7Linear SVM 81.9Logistic Regression 83.4assignment(LR) 82.4Ensemblesbayes(Carnie,SRILM,LR) 87.3 82.6assign(Carnie,SRILM,LR) 86.5 82.0assign(Carnie,SRILM,MaxEnt) 86.4 82.3bayes(Carnie,SRILM) 86.9 81.7Figure 4: Results.ting to the development set.
The development setwas used for model selection, ensemble parameteri-zation, and eventually as additional training data forfinal submissions.
Later tests showed that this fi-nal retraining actually reduced the Carnie score by0.9%.Figure 4 also shows the effect of various efforts toimprove our baseline Carnie system.
Adding part-of-speech n-grams and word trigrams as featuresimproved the score on the development set by 1%in total.
Meanwhile many of our experiments withnew types of features yielded no gains.
Lowercasedcharacter n-grams, skip bigrams and all non-vanillaformulations of part-of-speech tags provided no im-provement and were discarded.It was observed that all of our systems showeda strong preference for binary features overfrequency-weighted inputs.
In the case of theJCarafe classifier, switching to binary featuresyielded a 10% accuracy gain.
Although JCarafedidn?t provide a gain over the ensemble of Carnie,SRILM, and LIBLINEAR logistic regression, de-velopment set results indicated that JCarafe servedcapably as a replacement for LIBLINEAR in someensembles.We also measured the impact of using out-of-domain Japanese and Korean L1 data to train a pair-wise JPN/KOR system.
Only 78.5% of JPN andKOR texts were correctly identified in our eleven-Rank L1 Score Feature14 GER 21.05 (for,example)40 GER 15.95 (have,to)55 HIN 14.80 (as,compared,to)57 ITA 14.60 (I,think,that)58 TEL 14.18 (and,also)60 HIN 13.97 (as,compared)79 TEL 12.82 (the,people)96 TEL 12.14 (for,a)101 ITA 11.83 (that,in)116 ITA 10.94 (think,that)119 GER 10.93 (has,to)120 TEL 10.89 (with,the,statement)Figure 5: Word n-gram features predicting particular L1.way baseline system.
We restricted train and evalu-ation data to only those two L1s and found our base-line technique was 86.5% accurate.
When we addedour out-of-domain data with no domain adaptationtechnique, that score dropped to 82.0%.
Removingfeatures that didn?t appear in our test set only raisedthe score to 82.5%.
However, the EasyAdapt tech-nique (Daum?
and Marcu, 2007) showed promise.By making an additional source-specific copy ofeach feature, we were able to raise the score to88.5%.
While this result was of limited applicabil-ity in our final submission, and was therefore notsubmitted to the open data competition task, we be-lieve that this technique may prove useful in en-abling cross-domain NLI system transfer.Figure 5 provides a small sample of word-levelfeatures discovered by the Winnow classifier.
Thetable shows the rank of each n-gram relative to allfeatures, and the native language that the featurepredicts.
The weight assigned by the Winnow2 al-gorithm is not readily interpretable, although higherweights indicate a stronger association.Similarly, the top character n-grams can be seen inFigure 7, along with manually selected examples ofeach.
These features can be seen to mainly fall intoseveral broad categories.
There are mentions of theauthors?
home countries as in Korean, Italian andTurkey.
There are also characteristic misspellingsand infelicities such as personnaly, perhaps incor-rectly modeled from the French personnellement.It is worth noting that the weights (and thus theranks) for the top character n-gram features are107System Accuracy (%) ErrorsCarnie 80.4 2153SRILM 74.5 2800LIBLINEAR 80.8 2116ensemble-assign 81.9 1990ensemble-Bayes 82.2 1961Figure 6: Training set cross-validation results.higher than for the top word features, indicating thatWinnow found the former to be more informative.Finally, the top part-of-speech n-gram features areshown in Figure 8, again with manually selectedexamples.
These features have similar weightsto the character n-gram features and for the mostpart seem to represent ungrammatical constructions(e.g., the first feature indicates that a personal pro-noun followed by an uninflected verb predicts Chi-nese).
However, there are some perfectly grammat-ical items that are indicative of a particular nativelanguage (e.g., as compared to for Hindi).
One pos-sible explanation might be a dominant L2 pedagogyfor that language.5.1 Cross-validation resultsThe task organizers requested that the participantsrun a ten-fold cross validation on a particular split ofthe union of the training and development sets afterthe evaluation was over.
Results of our leading com-ponent systems and ensemble systems are presentedin Table 6.
These are comparable with the TOEFL-11 column of Figure 3 in Tetreault et al(2012).6 ConclusionIn this paper, we have presented MITRE?s partici-pation in the native language identification task atBEA-8.
Our best system was a naive Bayes ensem-ble combining component systems that used Win-now, language modeling and logistic regression ap-proaches, all using relatively simple character andword n-gram features.
This ensemble performed atan accuracy of 82.6% in the eleven-way NLI task,placing it in a statistical tie with the winning systemssubmitted by 29 teams.
For individual native lan-guages, our submission performed best among theparticipants on Arabic, as ranked by F-measure.In addition to the three base systems in our bestensemble, we experimented with a maximum en-tropy classifier and an assignment-based ensemblemethod.
We described a variety of experiments weperformed to determine the best configurations andsettings for the various systems.
We also coveredexperiments aimed at using out-of-domain data forseveral native languages.
In future work we will ex-pand upon these, with the goal of applying domainadaptation approaches.One concern with NLI as framed in this evalua-tion is the interaction between native language andessay topic.
The distribution of topics was very sim-ilar in the various subcorpora, but in more naturalsettings this is unlikely to be the case, and there isa danger of overtraining on topic, to the detrimentof language identification performance.
This is es-pecially problematic for a highly lexical approachsuch as ours.
In future work, we intend to explorethe extent of this effect, using topic-based splits ofthe corpus.
Our initial experiments to remedy thisproblem are likely to involve domain adaptation ap-proaches, such as Daum?
and Marcu (2007).As described above, we have had success usingthe Winnow-based system Carnie for other latent au-thor attributes, such as gender.
We would like to ex-plore ensembles similar to those described here forthese attributes as well.The techniques described in this paper success-fully identified an author?s native language 82.6% ofthe time using a sample of text averaging less than350 words in length.
Future work could study theinteraction of text length and NLI performance, in-cluding texts shorter than 140 characters in length.AcknowledgmentsThis work was funded under the MITRE InnovationProgram.
Approved for Public Release; DistributionUnlimited: 13-1876.ReferencesAustin Appleby.
2011.
MurmurHash, mur-mur3.
https://sites.google.com/site/murmurhash/.Shlomo Argamon, Moshe Koppel, James W. Pennebaker,and Jonathan Schler.
2007.
Mining the blogosphere:Age, gender, and the varieties of self-expression.
FirstMonday, 12(9), September.108Rank L1 Score Feature Snippet1 KOR 57.34 orea first thing that Korean college students usually buy2 GER 48.68 ,_tha the fact , that people have less moral values3 SPA 23.65 omen consequences related with the enviroment and the atmosphere4 ARA 23.23 _alot becouse you have alot of knowledge6 TUR 22.84 s_abo their searchings about the products11 ITA 21.56 Ital the Italian scholastic system19 TEL 20.19 d_als the whole system and also the concept20 TUR 19.96 urk in Turkey all young people go to the parties21 CHI 19.51 Ta Take school teachers for example23 GER 19.34 _-_ constantly - or as mentioned before even exponentially - breaking27 JPN 17.62 s_,_I For those reasons , I think32 FRE 16.90 ndeed Indeed , facts are just applications of ideas36 JPN 16.57 apan been getting weaker these days in Japan .37 FRE 16.57 onn I personnaly prefer38 GER 16.04 ,_bec would be great , because so everyone41 SPA 15.92 esa its not necesary to ask47 HIN 15.23 in_i the main idea and concept53 ITA 14.93 act_ due to the fact that too much74 ITA 13.00 ,_in academic subjects and , in the mean time81 TEL 12.74 h_ou cannot do with out a tour guideFigure 7: Character n-gram features predicting particular L1.Rank L1 Score Feature Snippet35 CHI 16.58 (PRP,VB) What if he go and see43 CHI 15.85 (NNS,POS) products ?s45 SPA 15.41 (NNS,NNS) companies universities59 TEL 14.05 (RB,IN,VBG) Usually in schooling64 TEL 13.95 (DT,NNS,WDT) the topics which65 TUR 13.71 (IN,DT,IN) after a while66 TEL 13.69 (IN,VBG) in telling69 TUR 13.42 (VBG,DT,NNS) learning the ways70 HIN 13.39 (IN,VBN,TO) as compared to80 HIN 12.81 (FW) [foreign word]Figure 8: Part of Speech n-gram features predicting particular L1.109Douglas Biber and Edward Finegan, editors.
1993.
Soci-olinguistic Perspectives on Register.
Oxford studies insociolinguistics.
Oxford University Press.Daniel Blanchard, Joel Tetreault, Derrick Higgins, AoifeCahill, and Martin Chodorow.
to appear.
TOEFL11:A Corpus of Non-Native English.
Technical report,Educational Testing Service.Julian Brooke and Graeme Hirst.
2012.
Robust, Lexical-ized Native Language Identification.
In Proceedingsof COLING 2012, pages 391?408, Mumbai, India, De-cember.John D. Burger and John C. Henderson.
2006.
An ex-ploration of observable features related to blogger age.In Computational Approaches to Analyzing Weblogs:Papers from the 2006 AAAI Spring Symposium.
AAAIPress.John D. Burger, John Henderson, George Kim, and GuidoZarrella.
2011.
Discriminating gender on twitter.In Proceedings of the 2011 Conference on Empiri-cal Methods in Natural Language Processing, pages1301?1309, Edinburgh, Scotland, UK, July.
Associa-tion for Computational Linguistics.Vitor R. Carvalho and William W. Cohen.
2006.
Single-pass online learning: performance, voting schemesand online feature selection.
In Proceedings ofthe 12th ACM SIGKDD International Conference onKnowledge Discovery and Data Mining, KDD ?06,pages 548?553, New York, NY, USA.
ACM.Hal Daum?
and D Marcu.
2007.
Frustratingly easy do-main adaptation.
In Proceedings of the Association forComputational Linguistics, volume 45, page 256.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
LIBLINEAR: A li-brary for large linear classification.
Journal of Ma-chine Learning Research, 9:1871?1874.John Gibson, Ben Wellner, and Susan Lubar.
2007.Adaptive web-page content identification.
In Proceed-ings of the 9th Annual ACM International Workshopon Web information and Data Management, WIDM?07, pages 105?112, New York, NY, USA.
ACM.William Labov.
1972.
Sociolinguistic Patterns.
Conduct& Communication Series.
University of PennsylvaniaPress.Lillian Lee.
1999.
Measures of distributional similar-ity.
In Proceedings of the 37th Annual Meeting of theAssociation for Computational Linguistics, ACL ?99,pages 25?32, Stroudsburg, PA, USA.
Association forComputational Linguistics.Arjun Mukherjee and Bing Liu.
2010.
Improving gen-der classification of blog authors.
In Proceedings ofthe 2010 Conference on Empirical Methods in Natu-ral Language Processing, Cambridge, MA, October.Association for Computational Linguistics.Delip Rao, David Yarowsky, Abhishek Shreevats, andManaswi Gupta.
2010.
Classifying latent user at-tributes in Twitter.
In 2nd International Workshop onSearch and Mining User-Generated Content.
ACM.Jonathan Schler, Moshe Koppel, Shlomo Argamon, andJames Pennebaker.
2006.
Effects of age and gender onblogging.
In Computational Approaches to AnalyzingWeblogs: Papers from the 2006 AAAI Spring Sympo-sium.
AAAI Press, March.Andreas Stolcke.
2002.
SRILM?an extensible lan-guage modeling toolkit.
In Proceedings of the Inter-national Conference on Spoken Language Processing,volume 2, pages 901?904.Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Mar-tin Chodorow.
2012.
Native tongues, lost andfound: Resources and empirical evaluations in nativelanguage identification.
In Proceedings of COLING2012, pages 2585?2602, Mumbai, India, December.The COLING 2012 Organizing Committee.Kristina Toutanova, Dan Klein, Christopher D. Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In Pro-ceedings of HLT-NAACL, pages 252?259.110
