Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 454?458,Dublin, Ireland, August 23-24, 2014.OPI: Semeval-2014 Task 3 System DescriptionMarek KozlowskiNational Information Processing Institutemkozlowski@opi.org.plAbstractIn this paper, we describe the OPI systemparticipating in the Semeval-2014 task 3Cross-Level Semantic Similarity.
Our ap-proach is knowledge-poor, there is no ex-ploitation of any structured knowledge re-sources as Wikipedia, WordNet or Babel-Net.
The method is also fully unsuper-vised, the training set is only used in orderto tune the system.
System measures thesemantic similarity of texts using corpus-based measures of termsets similarity.1 IntroductionThe task Cross-Level Semantic Similarity ofSemEval-2014 aims at an evaluation for seman-tic similarity across different sizes of text (lexi-cal levels).
Unlike prior SemEval tasks on textualsimilarity that have focused on comparing similar-sized texts, the mentioned task evaluates the casewhere larger text must be compared to smallertext, namely there are covered four semantic sim-ilarity comparisons: paragraph to sentence, sen-tence to phrase, phrase to word and word to sense.We present the method for measuring the se-mantic similarity of texts using a corpus-basedmeasure of termsets (set of words) similarity.
Westart from preprocessing texts, identifying bound-ary values, computing termsets similarities and de-rive from them the final score, which is normal-ized.The input of the task consists of two text seg-ments of different level.
We want to determinea score indicating their semantic similarity of thesmaller item to the larger item.
Similarity is scoredfrom 0 to 4, when 0 means no semantic intersec-This work is licensed under a Creative Commons Attribution4.0 International Licence.
Page numbers and proceedingsfooter are added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/tion, 4 means that two items have very similarmeanings.2 Related WorkThere are lots of papers about measuring thesimilarity between documents and single words.Document-level similarity works are based onVector Space Models (Salton and Lesk, 1971;Salton and McGill, 1983).
A significant effort hasalso been put into measuring similarity at the wordlevel, namely by approaches that use distributionalsemantics (Turney and Pantel, 2010).Related work can be classified into four ma-jor categories: vector-based document mod-els methods, corpus-based methods, knowledge-based methods and hybrid methods (Islam andInkpen, 2008).Vector-based document models represent docu-ment as a vector of words and the similarity eval-uation is based on the number of words that oc-cur in both texts.
Lexical similarity methods haveproblems with different words sharing commonsense.
Next approaches, such as corpus-based andknowledge-based methods, overcome the aboveissues.Corpus based methods apply scores provided byPointwise Mutual Information (PMI) and LatentSemantic Analysis (LSA).The Pointwise Mutual Information (PMI) (Tur-ney, 2001) between two words wiand wjis:PMI(wi, wj) = log2p(wi, wj)p(wi)p(wj)The Latent Semantic Analysis (LSA) (Landauerand Dumais, 1997; Landauer et al., 2007) is amathematical method for modelling of the mean-ing of words and contexts by analysis of represen-tative corpora.
It models the meaning of words andcontexts by projecting them into a vector space ofreduced dimensionality, which is built up by ap-plying singular value decomposition (SVD).454Knowledge based methods apply informationfrom semantic networks as WordNet.
They ex-ploit the structure of WordNet to compare con-cepts.
Leacock and Chodorow (1998) proposedmetric based on the length of the shortest path be-tween two concepts.
Lesk (1986) defined sim-ilarity between concepts as the intersection be-tween the corresponding glosses.
Budanitsky andHirst (2006) conducted the research on variousWordNet-based measures.
Standard thesaurus-based measures of word pair similarity are basedonly on a single path between concepts.
By con-trast Hughes and Ramage (2009) used a seman-tic representation of texts from random walks onWordNet.Hybrid methods use both corpus-based mea-sures and knowledge-based measures of word se-mantic similarity to determine the text similarity(Islam and Inkpen, 2008).
Mihalcea and Corley(2006) suggested a combined method by exploit-ing corpus based measures and knowledge-basedmeasures of words semantic similarity.
Anotherhybrid method was proposed by Li et al.
(2006)that combines semantic and syntactic information.The methods presented above are working atfixed level of textual granularity (documents,phrases, or words).
Pilehvar et al.
(2013) proposeda unified approach to semantic similarity that oper-ates at multiple levels.
The method builds a com-mon probabilistic representation over word sensesin order to compare different types of linguisticdata.
Any lexical item is represented as a distri-bution over a set of word senses (obtained fromWordNet), named as item?s semantic signature.3 Our ApproachOur system is fully unsupervised and knowledge-poor.
It exploits Wikipedia as a raw corpus forwords co-occurrence estimation.
The proposedmethod is not using any kind of textual alignment(e.g.
exploiting PoS tagging or WordNet con-cepts).The method consists of four steps: prepro-cessing, identifying boundary values, termset-to-termset similarity computation, text-to-text sim-ilarity phase, results normalization.
The resultsfrom the text-to-text similarity phase are very of-ten beyond the range 0-4, therefore we must nor-malize them.
We evaluated two normalizationapproaches: linear normalization and non-linearone.
The non-linear normalization is based onbuilt clusters (referring to integer values from 0 to4), which are created using training data set.
Thisstep will be described in details in the section 3.5.3.1 PreprocessingIn the first step the compared texts are retrieved,and then processed into the contexts.
Context isthe preprocessed original text represented as a bagof words.
Texts are processed using a dictionaryof proper names, name entities recognizers, PoS-taggers, providing as a result the required contexts.Contexts contain nouns, adjectives, adverbs andproper names.
The output of this stage is a pairof contexts passed to the next phase.3.2 Identifying Boundary ValuesThis phase is introduced in order to fast detecttexts, which are unrelated (0 score) or very sim-ilar (4 score).
Unrelated ones are identified bas-ing on the lack of any co-occurrences betweenwords from compared texts.
It means that anypair of words from compared contexts do not ap-pear together in any Wikipedia paragraph.
Thevery similar texts are identified in two steps.
Atfirst we check if all words from the shorter textsare contained in the longer one.
If the first checkis not fulfilled we compute: (c1,2) as the num-ber of Wikipedia paragraphs that contain all ofwords from both contexts in the nearest neigh-borhood (20-words window), (c1) and (c2) as thenumbers of Wikipedia paragraphs that containcontexts within 20-words window.
If the ratioc1,2/max(c1, c2) is higher than 50% then the ana-lyzed pair of texts refers to the same concept (verysimilar ones).
Having two texts represented bycontexts we use the proximity Lucene1query inorder to estimate the number of Wikipedia para-graphs, which contain the words from contextswithin the 20-words window.3.3 Termset-to-termset SimilarityTermset-to-termset similarity (t2tSim) is definedby measure similar to PMI.
Given a dictionary Dand two termsets (set of words) Wi?
D andWj?
D then the measure is expressed by the for-mula:t2tSim(Wi,Wj) =c(Wi,Wj)min(c(Wi), c(Wj))Here, c(X1, .., Xn) is a number of Wikipedia para-graphs that contain all terms covered by termsets1http://lucene.apache.org/core/455X1, .., Xn.
Two input termsets are semanticallyclose if the similarity measure t2tSim is higherthan the user-defined threshold (e.g.
10%).
Com-paring to the previous step we use the minimumoperator in the formula?s denominator in order totake into account even one directed relevant asso-ciation.
It was proved experimentally that the pro-posed measure leads to better results than the PMImeasure using NEAR query (co-occurrence withina 10-words window).
Specifically, the followingformula is used to collect the PMI value betweentermsets using the Wikipedia as a background cor-pus:PMI(Wi,Wj) = log2c(Wi,Wj) ?WikiSizec(Wi) ?
c(Wj)In the performed experiments we approximatedthe value of WikiSize to 30 millions (number ofparagraphs of English articles in Wikipedia).
Intable 1 we present results of Spearman correla-tion reported by the System using different mea-sures PMI and t2tSim.
The second measure isslightly better therefore it was chosen as the finalone.
These correlations were computed after lin-ear normalization of the output measures.Level Measure Spearmancorrelationword2sense PMI 19word2sense t2tSim 19phrase2word PMI 29phrase2word t2tSim 29sentence2phrase PMI 45sentence2phrase t2tSim 47paragraph2sentence PMI 48paragraph2sentence t2tSim 49Table 1: Comparison of PMI and t2tSim mea-sures in the semantic similarity task using Spear-man correlation (percentages).3.4 Text-to-text SimilarityGiven two input texts we compute the termset-to-termset similarities in order to derive the final se-mantic score.
We attempt to model the semanticsimilarity of texts as a function of the semanticsimilarities of the component termsets.
We do thisby combining metrics of termset-to-termset simi-larities and weights into a formula that is a poten-tially good indicator of semantic similarity of thetwo input texts.
Weights (wm1> wm2> wm3)are experimentally set with linear scalable valueswm1= 4, wm2= 2, wm3= 1 respectively.
Thepseudo-code of this phase is in Algorithm 1.Algorithm 1 Text-to-text similarityInput: cs, clare contexts representing shorter andlonger texts respectively; wm1, wm2, wm3asweights for different scopes of similarity com-parison;Output: m as a similarity measurem = 0m = m + t2tSim(cs, cl) ?
wm1for term ti?
cldom = m + t2tSim(cs, {ti}) ?
wm2end forfor term tj?
csdom = m + t2tSim(cl, {tj}) ?
wm2end forfor term ti?
csdofor term tj?
cldom = m + t2tSim({ti}, {tj}) ?
wm3end forend forreturn m3.5 Results NormalizationThe crucial part of the method is a process of nor-malization obtained measures into the range (0,4).The values 0 and 4 are covered by the step de-scribed in the section 3.2.
We need to normalizevalues from the text-to-text similarity phase.
Thisstep can be done in two ways: linear normaliza-tion and non-linear one.
The first one is a ca-sual transformation defined as dividing elementsby theirs maximum and scaling to 4.
The sec-ond one is based on clustering training set.
Inother words, using training set we induce ruleshow reported text-to-text similarity values shouldbe transformed into the range (0,4).
We imple-mented hierarchical agglomerative clustering al-gorithm (with average linkage)2in order to clus-ter similarity measures into five distinct groups.Sorted centroids of the above created groups arelabeled with values 0 to 4 respectively.
For eachnew similarity measure (obtained in the testingphase) we measure the distance to the closest clus-ter?s centroids.
The final value is derived linearly2Hierarchical Agglomerative Clustering treats initiallyeach instance as a singleton cluster and then successivelyagglomerate pairs of clusters using the average distance be-tween cluster?s elements until the user defined number ofclusters persist.456from the distance to the centroids (i.e.
if the valueis in the middle between centroids referring to 1and 2, we assign as a final value 1.5).
In the test-ing step we use the non-linear normalization, theevaluations on training set show that clusteringbased approach provides marginal improvementagainst linear normalization (about 1% accordingto Spearman rank, 4-8% according to Pearson cor-relation).4 ResultsIn Task 3, systems were evaluated both withinone of four comparison types and also across allcomparison types.
The system outputs and goldstandard ratings are compared in two ways, us-ing Pearson correlation and Spearman?s rank cor-relation (rho).
Pearson correlation tests the degreeof similarity between the system?s similarity rat-ings and the gold standard ratings.
Spearman?srho tests the degree of similarity between the rank-ings of the items according to similarity.
Rankswere computed by summing the correlation val-ues across all four levels of comparisons.
The sumof the Pearson correlations is used for the officialrank of Task 3.
However, the organizers providea second ranking using the sum of the Spearmancorrelations.Level System Pearson/Spearmanword2sense OPI 15.2/13.1word2sense SimCompass 35.6/34.4word2sense Baseline 10.9/13.0phrase2word OPI 21.3/18.8phrase2word SimCompass 41.5/42.4phrase2word Baseline 16.5/16.2sentence2phrase OPI 43.3/42.4sentence2phrase SimCompass 74.2/72.8sentence2phrase Baseline 56.2/62.6Table 2: Results for Pearson and Spearman corre-lation (percentages) scored by OPI System, Sim-Compass (the best performing one) and the Base-line one.We submitted only one run in three compari-son types.
We avoided the paragraph-to-sentencecomparison.
Evaluations on training set show thatour method reports values below the baseline inboth types: paragraph-to-sentence and sentence-to-phrase.
In the testing phase we decided to per-form only sentence-to-phrase comparison becauseit reports better values than paragraph-to-sentenceaccording to Pearson correlation, which is used forthe official rank.The best results our algorithm scores in the cat-egory phrase-to-word.
In this comparison typeit was ranked at 12th position among 21 partic-ipating systems.
In the word-to-sense it was at14th position among 20 systems.
The word-to-sense comparison is converted into the task sim-ilar to phrase-to-word by using glosses of targetsenses.
Each key of WordNet sense is replacedwith its gloss.
It is the only situation when weuse the external knowledge resources, but it isnot a part of the algorithm.
The last comparison(sentence-to-phrase) was our worst, because wedid not beat the baseline, as we did in the previouscategories.
In the sentence-to-phrase comparisonword alignment or syntax parsing seems to be veryimportant, in our case none of them was applied.The main conclusion is that comparison of largertext units can not be based on bag of words ap-proaches, where order of words is not important.Let us recall that our method is knowledge-poor,what leads to difficulties in evaluating it againstknowledge-rich ones (using sense inventories e.g.WordNet).
Generally, we scored better results us-ing Pearson correlation than Spearman?s one.5 ConclusionsWe presents our cross-level semantic similaritymethod, which is knowledge-poor (not using anykind of structured information from resources likemachine-readable dictionaries, thesaurus, or on-tologies) and fully unsupervised (there is no learn-ing phase leading to models enable to catego-rize compared texts).
The method exploits onlyWikipedia as a raw corpora in order to estimatefrequencies of co-occurrences.
We were aimedto verify how good results can be achieved us-ing only corpus-based approach and not includ-ing algorithms that have embedded deep languageknowledge.
The system scores best in the phrase-to-word (12th rank) and word-to-sense (14th rank)types of comparison with regard to Pearson cor-relation, while performing a little worse with theSpearman?s correlation.
The worst results werereported in the sentence-to-phrase category, whichbrings us the conclusion that larger text units de-mand word alignment, syntax parsing and moresophisticated text-to-text similarity models.457ReferencesAlexander Budanitsky and Graeme Hirst.
2006.
Eval-uating WordNet-based measures of Lexical Seman-tic Relatedness.
Computational Linguistics, 32(1):13?47.Thomas Hughes and Daniel Ramage.
2007.
Lexicalsemantic relatedness with random graph walk.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing, pages 581?589.Aminul Islam and Diana Inkpen.
2008.
Semantic TextSimilarity using Corpus-Based Word Similarity andString Similarity.
ACM Transactions on KnowledgeDiscovery from Data, 2(2): 1?25.Thomas Landauer and Susan Dumais.
1997.
A solu-tion to Platos problem: The latent semantic analysistheory of acquisition, induction, and representationof knowledge.
Psychological Review, 104: 211?240.Thomas Landauer, Danielle McNamara, Simon Den-nis and Walter Kintsch.
2007.
Handbook of LatentSemantic Analysis.
Psychology Press.Claudia Leacock and Martin Chodorow.
1998.
Com-bining local context and WordNet sense similarityfor word sense identification.
In WordNet, An Elec-tronic Lexical Database, pages 265?283.Michael Lesk.
1986.
Automatic sense disambiguationusing machine readable dictionaries: How to tell apine cone from an ice cream cone.
In Proceedingsof the SIGDOC Conference, pages 24?26.Yuhua Li, David McLean, Zuhair Bandar, JamesO?Shea and Keeley Crockett.
2006.
Sentence sim-ilarity based on semantic nets and corpus statistics.IEEE Transactions on Knowledge and Data Engi-neering, 18(8): 1138-1149.Rada Mihalcea, Courtney Corley and Carlo Strappa-rava.
2006.
Corpus-based and Knowledge-basedMeasures of Text Semantic Similarity.
In Proceed-ings of the American Association for Artificial Intel-ligence, pages 775?780.Mohammad Pilehvar, David Jurgens and Roberto Nav-igli.
2013.
Align, Disambiguate and Walk: A Uni-fied Approach for Measuring Semantic Similarity.In Proceedings of the 51st Annual Meeting of theAssociation for Computational Linguistics, pages1341?1351.Gerard Salton and Michael Lesk.
1971.
Computerevaluation of indexing and text processing.
Prentice-Hall, Englewood Cliffs, New Jersey.Gerard Salton and Michael McGill.
1983.
Alterna-tion.
Introduction to modern information retrieval.McGraw-Hill.Peter Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of seman-tics.
Journal of Articial Intelligence Research, 37:141?188.Peter Turney.
2001.
Mining the web for synonyms:PMI-IR versus LSA on TOEFL.
In Proceedingsof the Twelfth European Conference on MachineLearning, pages 491?502.458
