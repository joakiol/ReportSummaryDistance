Proceedings of the Interactive Question Answering Workshop at HLT-NAACL 2006, pages 17?24,New York City, NY, USA.
June 2006. c?2006 Association for Computational LinguisticsModeling Reference Interviews as a Basis for Improving Automatic QASystemsNancy J. McCracken, Anne R. Diekema, Grant Ingersoll, Sarah C.Harwell, Eileen E. Allen, Ozgur Yilmazel, Elizabeth D. LiddyCenter for Natural Language ProcessingSyracuse UniversitySyracuse, NY 13244{ njmccrac, diekemar, gsingers, scharwel, eeallen, oyilmaz, liddy}@syr.eduAbstractThe automatic QA system described inthis paper uses a reference interviewmodel to allow the user to guide andcontribute to the QA process.
A set ofsystem capabilities was designed andimplemented that defines how the user?scontributions can help improve thesystem.
These include tools, called theQuery Template Builder and theKnowledge Base Builder, that tailor thedocument processing and QA system toa particular domain by allowing aSubject Matter Expert to contribute tothe query representation and to thedomain knowledge.
During the QAprocess, the system can interact with theuser to improve query terminology byusing Spell Checking, Answer Typeverification, Expansions and AcronymClarifications.
The system also hascapabilities that depend upon, andexpand the user?s history of interactionwith the system, including a UserProfile, Reference Resolution, andQuestion Similarity modules1  IntroductionReference librarians have successfully fieldedquestions of all types for years using the ReferenceInterview to clarify an unfocused question, narrowa broad question, and suggest further informationthat the user might not have thought to ask for.The reference interview tries to elicit sufficientinformation about the user?s real need to enable alibrarian to understand the question enough tobegin searching.
The question is clarified, mademore specific, and contextualized with relevantdetail.
Real questions from real users are often?ill-formed?
with respect to the informationsystem; that is, they do not match the structure of?expectations?
of the system (Ross et al, 2002).
Areference interview translates the user?s questioninto a representation that the librarian and thelibrary systems can interpret correctly.
The humanreference interview process provides an ideal,well-tested model of how questioner and answererwork together co-operatively and, we believe, canbe successfully applied to the digital environment.The findings of researchers applying this model inonline situations (Bates, 1989, Straw, 2004) haveenabled us to understand how a system might workwith the user to provide accurate and relevantanswers to complex questions.Our long term goal in developing Question-Answering (QA) systems for various user groups isto permit, and encourage users to positivelycontribute to the QA process, to more nearlymirror what occurs in the reference interview, andto develop an automatic QA system that providesfuller, more appropriate, individually tailoredresponses than has been available to date.Building on our Natural LanguageProcessing (NLP) experience in a range ofinformation access applications, we have focusedour QA work in two areas:  1) modeling the subjectdomain of the collections of interest to a set of17users for whom we are developing the QA system,and; 2) modeling the query clarification andnegotiation interaction between the informationseeker and the information provider.
Examples ofthese implementation environments are:1.
Undergraduate aerospace engineering studentsworking in collaborative teams on courseprojects designing reusable launch vehicles,who use a QA system in their course-relatedresearch.2.
Customers of online business sites who use aQA system to learn more about the products orservices provided by the company, or whowish to resolve issues concerning products orservice delivery.In this paper, we describe the capabilities wehave developed for these specific projects in orderto explicate a more general picture of how wemodel and utilize both the domains of inquiry andtypical interaction processes observed in thesediverse user groups.2 Background and related researchOur work in this paper is based on two premises:1) user questions and responsive answers need tobe understood within a larger model of the user?sinformation needs and requirements, and, 2) agood interactive QA system facilitates a dialoguewith its users to ensure it understands and satisfiesthese information needs.
The first premise is basedon the long-tested and successful model of thereference interview (Bates, 1997, Straw, 2004),which was again validated by the findings of anARDA-sponsored workshop to increase theresearch community?s understanding of theinformation seeking needs and cognitive processesof intelligence analysts (Liddy, 2003).
The secondpremise instantiates this model within the digitaland distributed information environment.Interactive QA assumes an interactionbetween the human and the computer, typicallythrough a combination of a clarification dialogueand user modeling to capture previous interactionsof users with the system.
De Boni et al (2005)view the clarification dialogue mainly as thepresence or absence of a relationship between thequestion from the user and the answer provided bythe system.
For example, a user may ask aquestion, receive an answer and ask anotherquestion in order to clarify the meaning, or, theuser may ask an additional question which expandson the previous answer.
In their research De Boniet al (2005) try to determine automaticallywhether or not there exists a relationship between acurrent question and preceding questions, and ifthere is a relationship, they use this additionalinformation in order to determine the correctanswer.We prefer to view the clarification dialogueas more two-sided, where the system and the useractually enter a dialogue, similar to the referenceinterview as carried out by reference librarians(Diekema et al, 2004).
The traditional referenceinterview is a cyclical process in which thequestioner poses their question, the librarian (or thesystem) questions the questioner, then locates theanswer based on information provided by thequestioner, and returns an answer to the user whothen determines whether this has satisfied theirinformation need or whether further clarification orfurther questions are needed.
The HITIQAsystem?s (Small et al, 2004) view of a clarificationsystem is closely related to ours?their dialoguealigns the understanding of the question betweensystem and user.
Their research describes threetypes of dialogue strategies: 1) narrowing thedialogue, 2) broadening the dialogue, and 3) a factseeking dialogue.Similar research was carried out by Hori etal.
(2003), although their system automaticallydetermines whether there is a need for a dialogue,not the user.
The system identifies ambiguousquestions (i.e.
questions to which the system couldnot find an answer).
By gathering additionalinformation, the researchers believe that the systemcan find answers to these questions.
Clarifyingquestions are automatically generated based on theambiguous question to solicit additionalinformation from the user.
This process iscompletely automated and based on templates thatgenerate the questions.
Still, removing thecognitive burden from the user through automationis not easy to implement and can be the cause oferror or misunderstanding.
Increasing userinvolvement may help to reduce this error.As described above, it can be seen thatinteractive QA systems have various levels ofdialogue automation ranging from fully automatic(De Boni et al, 2004, Hori et al, 2004) to a strong18user involvement (Small et al, 2004, Diekema etal., 2004).
Some research suggests thatclarification dialogues in open-domain systems aremore unpredictable than those in restricted domainsystems, the latter lending itself better toautomation (Hori et al, 2003, J?nsson et al, 2004).Incorporating the user?s inherent knowledge of theintention of their query is quite feasible inrestricted domain systems and should improve thequality of answers returned, and make theexperience of the user a less frustrating one.
Whilemany of the systems described above arepromising in terms of IQA, we believe thatincorporating knowledge of the user in thequestion negotiation dialogue is key to developinga more accurate and satisfying QA system.3 System CapabilitiesIn order to increase the contribution of users to ourquestion answering system, we expanded ourtraditional domain independent QA system byadding new capabilities that support system-userinteraction.3.1  Domain Independent QAOur traditional domain-independent QA capabilityfunctions in two stages, the first informationretrieval stage selecting a set of candidatedocuments, the second stage doing the answerfinding within the filtered set.
The answer findingprocess draws on models of question types anddocument-based knowledge to seek answerswithout additional feedback from the user.
Again,drawing on the modeling of questions as theyinteract with the domain representation, the systemreturns answers of variable lengths on the fly inresponse to the nature of the question since factoidquestions may be answered with a short answer,but complex questions often require longeranswers.
In addition, since our QA projects werebased on closed collections, and since closedcollections may not provide enough redundancy toallow for short answers to be returned, the variablelength answer capability assists in finding answersto factoid questions.
The QA system providesanswers in the form of short answers, sentences,and answer-providing passages, as well as links tothe full answer-providing documents.
The user canprovide relevance feedback by selecting the fulldocuments that offer the best information.
Usingthis feedback, the system can reformulate thequestion and look for a better set of documentsfrom which to find an answer to the question.Multiple answers can be returned, giving the user amore complete picture of the information heldwithin the collection.One of our first tactics to assist in bothquestion and domain modeling for specific userneeds was to develop tools for Subject MatterExperts (SMEs) to tailor our QA systems to aparticular domain.
Of particular interest to theinteractive QA community is the Query TemplateBuilder (QTB) and the Knowledge Base Builder(KBB).Both tools allow a priori alterations toquestion and domain modeling for a community,but are not sensitive to particular users.
Then theinteractive QA system permits question- and user-specific tailoring of system behavior simplybecause it allows subject matter experts to changethe way the system understands their need at thetime of the search.Question Template Builder (QTB) allowsa subject matter expert to fine tune a questionrepresentation by adding or removing stopwordson a question-by-question basis, adding or maskingexpansions, or changing the answer focus.
TheQTB displays a list of Question-Answer types,allows the addition of new Answer Types, andallows users to select the expected answer type forspecific questions.
For example, the subject matterexpert may want to adjust particular ?who?questions as to whether the expected answer type is?person?
or ?organization?.
The QTB enablesorganizations to identify questions for which theywant human intervention and to build specializedterm expansion sets for terms in the collection.They can also adjust the stop word list, and refineand build the Frequently or Previously AskedQuestion (FAQ/PAQ) collection.Knowledge Base Builder (KBB) is a suiteof tools developed for both commercial andgovernment customers.
It allows the users to viewand extract terminology that resides in theirdocument collections.
It provides useful statisticsabout the corpus that may indicate portions thatrequire attention in customization.
It collectsfrequent / important terms with categorizations toenable ontology building (semi-automatic,permitting human review), term collocation for use19in identifying which sense of a word is used in thecollection for use in term expansion andcategorization review.
KBB allows companies totailor the QA system to the domain vocabulary andimportant concept types for their market.
Usersare able to customize their QA applicationsthrough human-assisted automatic procedures.The Knowledge Bases built with the tools areIR Answer ProvidersQuestionProcessingSessionTrackingReferenceResolutionUser ProfileQuestionSimilarityUserAnswerSpellcheckingAnswerTypeVerificationExpansionClarificationDomain ModelingQTB KBBFigure 1.
System overviewprimarily lexical semantic taxonomic resources.These are used by the system in creating framerepresentations of the text.
Using automaticallyharvested data, customers can review and altercategorization of names and entities and expandthe underlying category taxonomy to the domain ofinterest.
For example, in the NASA QA system,experts added categories like ?material?, ?fuel?,?spacecraft?
and ?RLV?, (Reusable LaunchVehicles).
They also could specify that ?RLV?
is asubcategory of ?spacecraft?
and that space shuttleslike ?Atlantis?
have category ?RLV?.
The KBBworks in tandem with the QTB, where the user canfind terms in either documents or example queries3.2 Interactive QA DevelopmentIn our current NASA phase, developed forundergraduate aerospace engineering students toquickly find information in the course of theirstudies on reusable launch vehicles, the user canview immediate results, thus bypassing theReference Interviewer, or they may take theopportunity to utilize its increased functionalityand interact with the QA system.
The capabilitieswe have developed, represented by modules addedto the system, fall into two groups.
Group Oneincludes capabilities that draw on direct interactionwith the user to clarify what is being asked and thataddress terminological issues.
It includes SpellChecking, Expansion Clarification, and AnswerType Verification.
Answers change dynamically asthe user provides more input about what wasmeant.
Group Two capabilities are dependentupon, and expand upon the user?s history ofinteraction with the system and include UserProfile, Session Tracking, Reference Resolution,Question Similarity and User FrustrationRecognition modules.
These gather knowledgeabout the user, help provide co-referenceresolution within an extended dialogue, andmonitor the level of frustration a user isexperiencing.20The capabilities are explained in greaterdetail below.
Figure 1 captures the NASA systemprocess and flow.Group One:In this group of interactive capabilities, after theuser asks a query, answers are returned as in atypical system.
If the answers presented aren?tsatisfactory, the system will embark on a series ofinteractive steps (described below) in whichalternative spelling, answertypes, clarifications andexpansions will be suggested.
The user canchoose from the system?s suggestions or type intheir own.
The system will then revise the queryand return a new set of answers.
If those answersaren?t satisfactory, the user can continueinteracting with the system until appropriateanswers are found.Spell checking: Terms not found in theindex of the document collection are displayed aspotentially misspelled words.
In this preliminaryphase, spelling is checked and users have theopportunity to select correct and/or alternativespellings.AnswerType verification: The interactiveQA system displays the type of answer that thesystem is looking for in order to answer thequestion.
For example for the question, Whopiloted the first space shuttle?, the answer type is?person?, and the system will limit the search forcandidate short answers in the collection to thosethat are a person?s name.
The user can eitheraccept the system?s understanding of the questionor reject the type it suggests.
This is particularlyuseful in semantically ambiguous questions such as?Who makes Mountain Dew??
where the systemmight interpret the question as needing a person,but the questioner actually wants the name of acompany.Expansion:  This capability allows users toreview the possible relevant terms (synonyms andgroup members) that could enhance the question-answering process.
The user can either select ordeselect terms of interest which do or do notexpress the intent of the question.
For example, ifthe user asks: How will aerobraking change theorbit size?
then the system can bring back thefollowing expansions for ?aerobraking?
:  Byaerobraking do you mean the following: 1)aeroassist, 2) aerocapture, 3) aeromaneuvering, 4)interplanetary transfer orbits, or 5) transfer orbits.Acronym Clarification: For abbreviationsor acronyms within a query, the full explicationsknown by the system for the term can be displayedback to the user.
The clarifications implementedare a priori limited to those that are relevant to thedomain.
In the aerospace domain for example, ifthe question was What is used for the TPS of theRLV?, the clarifications of TPS would be thermalprotection system, thermal protection subsystem,test preparation sheet, or twisted pair shielded, andthe clarification of RLV would be reusable launchvehicle.
The appropriate clarifications can beselected to assist in improving the search.
For amore generic domain, the system would offerbroader choices.
For example, if the user types inthe question: What educational programs does theAIAA offer?, then the system might return: ByAIAA, do you mean (a) American Institute ofAeronautics and Astronautics (b) AustraliaIndonesia Arts Alliance or (c) Americans forInternational Aid & Adoption?Group Two:User Profile: The User Profile keeps track of morepermanent information about the user.
The profileincludes a small standard set of user attributes,such as the user?s name and / or research interests.In our commercially funded work, selectedinformation gleaned from the question about theuser was also captured in the profile.
For example,if a user asks ?How much protein should myhusband be getting every day?
?, the fact that theuser is married can be added to their profile forfuture marketing, or for a new line of dialogue toask his name or age.
This information is thenmade available as context information for the QAsystem to resolve references that the user makes tothemselves and their own attributes.For the NASA question-answeringcapability, to assist students in organizing theirquestions and results, there is an area for users tosave their searches as standing queries, along withthe results of searching (Davidson, 2006).
Thisinformation, representing topics and areas ofinterest, can help to focus answer finding for newquestions the user asks.Not yet implemented, but of interest, is theability to save information such as a user?s21preferences (format, reliability, sources), that couldbe used as filters in the answer finding process.Reference Resolution:  A basic feature ofan interactive QA system is the requirement tounderstand the user?s questions and responsiveanswers as one session.
The sequence of questionsand answers forms a natural language dialoguebetween the user and the system.
This necessitatesNLP processing at the discourse level, a primarytask of which is to resolve references across thesession.
Building on previous work in this areadone for the Context Track of TREC 2001(Harabagiu et al 2001) and additional work (Chaiand Jin, 2004) suggesting discourse structures areneeded to understand the question/answersequence, we have developed session-basedreference resolution capability.
In a dialogue, theuser naturally includes referring phrases thatrequire several types of resolution.The simplest case is that of referringpronouns, where the user is asking a follow-upquestion, for example:Q1:  When did Madonna enter the music business?A1:  Madonna's first album, Madonna, came out in1983 and since then she's had a string of hits, beena major influence in the music industry andbecome an international icon.Q2:  When did she first move to NYC?In this question sequence, the secondquestion contains a pronoun, ?she?, that refers tothe person ?Madonna?
mentioned both in theprevious question and its answer.
Referenceresolution would transform the question into?When did Madonna first move to NYC?
?Another type of referring phrase is thedefinite common noun phrase, as seen in the nextexample:Q1: If my doctor wants me to take Acyclovir, is itexpensive?A1:  Glaxo-Wellcome, Inc., the company thatmakes Acyclovir, has a program to assistindividuals that have HIV and Herpes.Q2:  Does this company have other assistanceprograms?The second question has a definite nounphrase ?this company?
that refers to ?Glaxo-Wellcome, Inc.?
in the previous answer, thustransforming the question to ?Does Glaxo-Wellcome, Inc. have other assistance programs?
?Currently, we capture a log of thequestion/answer interaction, and the referenceresolution capability will resolve any references inthe current question that it can by using linguistictechniques on the discourse of the current session.This is almost the same as the narrativecoreference resolution used in documents, with theaddition of the need to understand first and secondperson pronouns from the dialogue context.
Thecoreference resolution algorithm is based onstandard linguistic discourse processing techniqueswhere referring phrases and candidate resolventsare analyzed along a set of features that typicallyincludes gender, animacy, number, person and thedistance between the referring phrase and thecandidate resolvent.Question Similarity: Question Similarity isthe task of identifying when two or more questionsare related.
Previous studies (Boydell et al, 2005,Balfe and Smyth, 2005) on information retrievalhave shown that using previously asked questionsto enhance the current question is often useful forimproving results among like-minded users.Identifying related questions is useful for findingmatches to Frequently Asked Questions (FAQs)and Previously Asked Questions (PAQs) as well asdetecting when a user is failing to find adequateanswers and may be getting frustrated.Furthermore, similar questions can be used duringthe reference interview process to presentquestions that other users with similar informationneeds have used and any answers that theyconsidered useful.CNLP?s question similarity capabilitycomprises a suite of algorithms designed toidentify when two or more questions are related.The system works by analyzing each query usingour Language-to-Logic (L2L) module to identifyand weight keywords in the query, provideexpansions and clarifications, as well as determinethe focus of the question and the type of answer theuser is expecting (Liddy et al, 2003).
We thencompute a series of similarity measures on two ormore L2L queries.
Our measures adopt a varietyof approaches, including those that are based onkeywords in the query: cosine similarity, keywordstring matching, expansion analysis, and spellingvariations.
In addition, two measures are based onthe representation of the whole query:answer type22and answer frame analysis.
An answer frame is ourrepresentation of the meaningful extractionscontained in the query, along with metadata aboutwhere they occur and any other extractions thatrelate to in the query.Our system will then combine the weightedscores of two or more of these measures todetermine a composite score for the two queries,giving more weight to a measure that testing hasdetermined to be more useful for a particular task.We have utilized our question similaritymodule for two main tasks.
For FAQ/PAQ (call itXAQ) matching, we use question similarity tocompare the incoming question with our databaseof XAQs.
Through empirical testing, wedetermined a threshold above which we considertwo questions to be similar.Our other use of question similarity is in thearea of frustration detection.
The goal offrustration detection is to identify the signs a usermay be giving that they are not finding relevantanswers so that the system can intervene and offeralternatives before the user leaves the system, suchas similar questions from other users that havebeen successful.4 Implementations:The refinements to our Question Answeringsystem and the addition of interactive elementshave been implemented in three different, butrelated working systems, one of which is strictly anenhanced IR system.
None of the threeincorporates all of these capabilities.
In our workfor MySentient, Ltd, we developed the session-based reference resolution capability, implementedthe variable length and multiple answer capability,modified our processing to facilitate the buildingof a user profile, added FAQ/PAQ capability, andour Question Similarity capability for bothFAQ/PAQ matching and frustration detection.
Arelated project, funded by Syracuse ResearchCorporation, extended the user tools capability toinclude a User Interface for the KBB and basicprocessing technology.
Our NASA project hasseen several phases.
As the project progressed, weadded the relevant developed capabilities forimproved performance.
In the current phase, weare implementing the capabilities which draw onuser choice.5 Conclusions and Future WorkThe reference interview has beenimplemented as an interactive dialogue betweenthe system and the user, and the full system is nearcompletion.
We are currently working on twotypes of evaluation of our interactive QAcapabilities.
One is a system-based evaluation inthe form of unit tests, the other is a user-basedevaluation.
The unit tests are designed to verifywhether each module is working correctly andwhether any changes to the system adversely affectresults or performance.
Crafting unit tests forcomplex questions has proved challenging, as nogold standard for this type of question has yet beencreated.
As the data becomes available, this typeof evaluation will be ongoing and part of regularsystem development.As appropriate for this evolutionary workwithin specific domains for which there are notgold standard test sets, our evaluation of the QAsystems has focused on qualitative assessments.What has been a particularly interesting outcome iswhat we have learned in elicitation from graduatestudents using the NASA QA system, namely thatthey have multiple dimensions on which theyevaluate a QA system, not just traditional recalland precision (Liddy et al 2004).
The high leveldimensions identified include system performance,answers, database content, display, andexpectations.
Therefore the evaluation criteria webelieve appropriate for IQA systems are centeredaround the display (UI) category as described inLiddy et al (2004).
We will evaluate aspects ofthe UI input subcategory, including questionunderstanding, information need understanding,querying style, and question formulationassistance.
Based on this user evaluation thesystem will be improved and retested.ReferencesEvelyn Balfe and Barry Smyth.
2005.
An Analysisof Query Similarity in Collaborative WebSearch.
In Proceedings of the 27th EuropeanConference on Information Retrieval.
Santiagode Compostela, Spain.23Marcia J. Bates.
1989.
The Design of Browsingand Berrypicking Techniques for the OnlineSearch Interface.
Online Review, 13: 407-424.Mary Ellen Bates.
1997.
The Art of the ReferenceInterview.
Online World.
September 15.Ois?n Boydell, Barry Smyth, Cathal Gurrin, andAlan F. Smeaton.
2005.
A Study of SelectionNoise in Collaborative Web Search.
InProceedings of the 19th International JointConference on Artificial Intelligence.
Edinburgh,Scotland.http://www.ijcai.org/papers/post-0214.pdfJoyce Y. Chai, and Rong Jin.
2004.
DiscourseStructure for Context Question Answering.
InProceedings of the Workshp on the Pragmaticsof Quesiton Answering, HST-NAACL, Boston.http://www.cse.msu.edu/~rongjin/publications/HLTQAWorkshop04.pdfBarry D. Davidson.
2006.
An Advanced InteractiveDiscovery Learning Environment forEngineering Education: Final Report.Submitted to R. E. Gillian, National Aeronauticsand Space Administration.Marco De Boni and Suresh Manandhar.
2005.Implementing Clarification Dialogues in OpenDomain Question Answering.
Natural LanguageEngineering 11(4): 343-361.Anne R. Diekema, Ozgur Yilmazel, JiangpingChen, Sarah Harwell, Lan He, and Elizabeth D.Liddy.
2004.
Finding Answers to ComplexQuestions.
In New Directions in QuestionAnswering.
(Ed.)
Mark T. Maybury.
The MITPress, 141-152.Sanda Harabagiu, Dan Moldovan, Marius Pa?ca,Mihai Surdeanu, Rada Mihalcea, Roxana G?rju,Vasile Rus, Finley L?c?tu?u, Paul Mor?rescu,R?zvan Bunescu.
2001.
Answering Complex,List and Context Questions with LCC?sQuestion-Answering Server, TREC 2001.Chiori Hori, Takaaki Hori., Hideki Isozaki, EisakuMaeda, Shigeru Katagiri, and Sadaoki Furui.2003.
Deriving Disambiguous Queries in aSpoken Interactive ODQA System.
In ICASSP.Hongkong, I: 624-627.Arne J?nsson, Frida And?n, Lars Degerstedt,Annika Flycht-Eriksson, Magnus Merkel, andSara Norberg.
2004.
Experiences fromCombining Dialogue System Development WithInformation Extraction Techniques.
In NewDirections in Question Answering.
(Ed.)
Mark T.Maybury.
The MIT Press, 153-164.Elizabeth D. Liddy.
2003.
Question Answering inContexts.
Invited Keynote Speaker.
ARDAAQUAINT Annual Meeting.
Washington, DC.Dec 2-5, 2003.Elizabeth D. Liddy, Anne R. Diekema, JiangpingChen, Sarah Harwell, Ozgur Yilmazel, and LanHe.
2003.
What do You Mean?
Finding Answersto Complex Questions.
Proceedings of NewDirections in Question Answering.
AAAI SpringSymposium, March 24-26.Elizabeth D. Liddy, Anne R. Diekema, and OzgurYilmazel.
2004.
Context-Based Question-Answering Evaluation.
In Proceedings of the 27thAnnual ACM-SIGIR Conference.
Sheffield,EnglandCatherine S. Ross, Kirsti Nilsen, and PatriciaDewdney.
2002.
Conducting the ReferenceInterview.
Neal-Schuman, New York, NY.Sharon Small, Tomek Strzalkowski, Ting Liu,Nobuyuki Shimizu, and Boris Yamrom.
2004.
AData Driven Approach to Interactive QA.
In NewDirections in Question Answering.
(Ed.)
Mark T.Maybury.
The MIT Press, 129-140.Joseph E. Straw.
2004.
Expecting the Stars butGetting the Moon: Negotiating around PatronExpectations in the Digital ReferenceEnvironment.
In The Virtual ReferenceExperience: Integrating Theory into Practice.Eds.
R. David Lankes, Joseph Janes, Linda C.Smith, and Christina M.  Finneran.
Neal-Schuman, New York, NY.24
