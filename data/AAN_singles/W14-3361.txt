Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 477?485,Baltimore, Maryland USA, June 26?27, 2014.c?2014 Association for Computational LinguisticsBayesian Reordering Model with Feature SelectionAbdullah Alrajehaband Mahesan NiranjanbaComputer Research Institute, King Abdulaziz City for Science and Technology (KACST)Riyadh, Saudi Arabia, asrajeh@kacst.edu.sabSchool of Electronics and Computer Science, University of SouthamptonSouthampton, United Kingdom, {asar1a10, mn}@ecs.soton.ac.ukAbstractIn phrase-based statistical machine trans-lation systems, variation in grammaticalstructures between source and target lan-guages can cause large movements ofphrases.
Modeling such movements is cru-cial in achieving translations of long sen-tences that appear natural in the target lan-guage.
We explore generative learningapproach to phrase reordering in Arabicto English.
Formulating the reorderingproblem as a classification problem andusing naive Bayes with feature selection,we achieve an improvement in the BLEUscore over a lexicalized reordering model.The proposed model is compact, fast andscalable to a large corpus.1 IntroductionCurrently, the dominant approach to machinetranslation is statistical, starting from the math-ematical formulations and algorithms for param-eter estimation (Brown et al., 1988), further ex-tended in (Brown et al., 1993).
These early mod-els, widely known as the IBM models, were word-based.
Recent extensions note that a better ap-proach is to group collections of words, or phrases,for translation together, resulting in a significantfocus these days on phrase-based statistical ma-chine translation systems.To deal with the alignment problem of one-to-many word alignments in the IBM modelformulation, whereas phrase-based models mayhave many-to-many translation relationships, IBMmodels are trained in both directions, source to tar-get and target to source, and their word alignmentsare combined (Och and Ney, 2004).While phrase-based systems are a significantimprovement over word-based approaches, a par-ticular issue that emerges is long-range reorder-ings at the phrase level (Galley and Manning,2008).
Analogous to speech recognition systems,translation systems relied on language models toproduce more fluent translation.
While early workpenalized phrase movements without consideringreorderings arising from vastly differing grammat-ical structures across language pairs like Arabic-English, many researchers considered lexical re-ordering models that attempted to learn orienta-tion based on content (Tillmann, 2004; Kumarand Byrne, 2005; Koehn et al., 2005).
Theseapproaches may suffer from the data sparsenessproblem since many phrase pairs occur only once(Nguyen et al., 2009).As an alternative way of exploiting function ap-proximation capabilities offered by machine learn-ing methods, there is recent interest in formulatinga learning problem that aims to predict reorder-ing from linguistic features that capture their con-text.
An example of this is the maximum entropymethod used by (Xiang et al., 2011; Nguyen et al.,2009; Zens and Ney, 2006; Xiong et al., 2006).In this work we apply a naive Bayes classifier,combined with feature selection to address the re-ordering problem.
To the best of our knowledge,this simple model of classification has not beenused in this context previously.
We present em-pirical results comparing our work and previouslyproposed lexicalized reordering model.
We showthat our model is scalable to large corpora.The remainder of this paper is organized as fol-lows.
Section 2 discusses previous work in thefield and how that is related to our paper.
Section 3gives an overview of the baseline translation sys-tem.
Section 4 introduces the Bayesian reorder-ing model and gives details of different inferencemethods, while, Section 5 describes feature selec-tion method.
Section 6 presents the experimentsand reports the results evaluated as classificationand translation problems.
Finally, we end the pa-per with a summary of our conclusions and per-spectives.477Symbol Notationf/e a source / target sentence (string)?f/e?
a source / target phrase sequenceN the number of examplesK the number of classes(?fn, e?n) the n-th phrase pair in (?f , e?
)onthe orientation of (?fn, e?n)?
(?fn, e?n) the feature vector of (?fn, e?n)Table 1: Notation used in this paper.2 Related WorkThe phrase reordering model is a crucial compo-nent of any translation system, particularly be-tween language pairs with different grammaticalstructures (e.g.
Arabic-English).
Adding a lex-icalized reordering model consistently improvedthe translation quality for several language pairs(Koehn et al., 2005).
The model tries to predict theorientation of a phrase pair with respect to the pre-vious adjacent target words.
Ideally, the reorder-ing model would predict the right position in thetarget sentence given a source phrase, which is dif-ficult to achieve.
Therefore, positions are groupedinto limited orientations or classes.The orientationprobability for a phrase pair is simply based on therelative occurrences in the training corpus.The lexicalized reordering model has been ex-tended to tackle long-distance reorderings (Gal-ley and Manning, 2008).
This takes into accountthe hierarchical structure of the sentence whenconsidering such an orientation.
Certain exam-ples are often used to motivate syntax-based sys-tems were handled by this hierarchical model, andthis approach is shown to improve translation per-formance for several translation tasks with smallcomputational cost.Despite the fact that the lexicalized reorderingmodel is always biased towards the most frequentorientation for such a phrase pair, it may sufferfrom a data sparseness problem since many phrasepairs occur only once.
Moreover, the context ofa phrase might affect its orientation, which is notconsidered as well.Adopting the idea of predicting orientationbased on content, it has been proposed to representeach phrase pair by linguistic features as reorder-ing evidence, and then train a classifier for predic-tion.
The maximum entropy classifier is a popu-lar choice among many researchers (Zens and Ney,2006; Xiong et al., 2006; Nguyen et al., 2009; Xi-ang et al., 2011).
Max-margin structure classifierswere also proposed (Ni et al., 2011).
Recently,Cherry (2013) proposed using sparse features op-timize BLEU with the decoder instead of traininga classifier independently.We distinguish our work from the previous onesin the following.
We propose a fast reorderingmodel using a naive Bayes classifier with featureselection.
In this study, we undertake a compari-son between our work and lexicalized reorderingmodel.3 Baseline SystemIn statistical machine translation, the most likelytranslation ebestof an input sentence f can befound by maximizing the probability p(e|f), asfollows:ebest= arg maxep(e|f).
(1)A log-linear combination of different models(features) is used for direct modeling of the poste-rior probability p(e|f) (Papineni et al., 1998; Ochand Ney, 2002):ebest= arg maxen?i=1?ihi(f , e) (2)where the feature hi(f , e) is a score functionover sentence pairs.
The translation model and thelanguage model are the main features in any sys-tem although additional features h(.)
can be inte-grated easily (such as word penalty).
State-of-the-art systems usually have around ten features (i.e.n = 10).In phrase-based systems, the translation modelcan capture the local meaning for each sourcephrase.
However, to capture the whole meaningof a sentence, its translated phrases need to be inthe correct order.
The language model, which en-sures fluent translation, plays an important role inreordering; however, it prefers sentences that aregrammatically correct without considering theiractual meaning.
Besides that, it has a bias towardsshort translations (Koehn, 2010).
Therefore, de-veloping a reordering model will improve the ac-curacy particularly when translating between twogrammatically different languages.3.1 Lexicalized Reordering ModelPhrase reordering modeling involves formulat-ing phrase movements as a classification problem478where each phrase position considered as a class(Tillmann, 2004).
Some researchers classifiedphrase movements into three categories (mono-tone, swap, and discontinuous) but the classes canbe extended to any arbitrary number (Koehn andMonz, 2005).
In general, the distribution of phraseorientation is:p(ok|?fn, e?n) =1Zh(?fn, e?n, ok) .
(3)This lexicalized reordering model is estimatedby relative frequency where each phrase pair(?fn, e?n) with such an orientation (ok) is countedand then normalized to yield the probability as fol-lows:p(ok|?fn, e?n) =count(?fn, e?n, ok)?ocount(?fn, e?n, o).
(4)The orientation class of a current phrase pair isdefined with respect to the previous target wordor phrase (i.e.
word-based classes or phrase-basedclasses).
In the case of three categories (mono-tone, swap, and discontinuous): monotone is theprevious source phrase (or word) that is previ-ously adjacent to the current source phrase, swapis the previous source phrase (or word) that is next-adjacent to the current source phrase, and discon-tinuous is not monotone or swap.Galley and Manning (2008) extended the lex-icalized reordering mode to tackle long-distancephrase reorderings.
Their hierarchical model en-ables phrase movements that are more complexthan swaps between adjacent phrases.4 Bayesian Reordering ModelMany feature-based reordering models have beenproposed to replace the lexicalized reorderingmodel.
The reported results showed consistent im-provement in terms of various translation metrics.Naive Bayes method has been a popular clas-sification model of choice in many natural lan-guage processing problems (e.g.
text classifica-tion).
Naive Bayes is a simple classifier that ig-nores correlation between features, but has the ap-peal of computational simplicity.
It is a generativeprobabilistic model based on Bayes?
theorem asbelow:p(ok|?fn, e?n) =p(?fn, e?n|ok)p(ok)?op(?fn, e?n|o)p(o).
(5)The class prior can be estimated easily as a rel-ative frequency (i.e.
p(ok) =NkN).
The likeli-hood distribution p(?fn, e?n|ok) is defined based onthe type of data.
The classifier will be naive if weassume that feature variables are conditionally in-dependent.
The naive assumption simplifies ourdistribution and hence reduces the parameters thathave to be estimated.
In text processing, multi-nomial is used as a class-conditional distribution(Rogers and Girolami, 2011).
The distribution isdefined as:p(?fn, e?n|q) = C?mq?m(?fn,e?n)m(6)where C is a multinomial coefficient,C =(?m?m(?fn, e?n))!
?m?m(?fn, e?n)!, (7)and q are a set of parameters, each of which is aprobability.
Estimating these parameters for eachclass by maximum likelihood,arg maxqkNk?np(?fn, e?n|qk), (8)will result in (Rogers and Girolami, 2011):qkm=?Nkn?m(?fn, e?n)?Mm??Nkn?m?
(?fn, e?n).
(9)MAP estimate It is clear that qkmmight bezero which means the probability of a new phrasepair with nonzero feature ?m(?fn, e?n) is alwayszero because of the product in (6).
Putting a priorover q is one smoothing technique.
A conjugateprior for the multinomial likelihood is the Dirich-let distribution and the MAP estimate for qkmis(Rogers and Girolami, 2011):qkm=??
1 +?Nkn?m(?fn, e?n)M(??
1) +?Mm??Nkn?m?
(?fn, e?n)(10)where M is the feature vector?s length or thefeature dictionary size and ?
is a Dirichlet param-eter with a value greater than one.
The derivationis in Appendix A.Bayesian inference Instead of using a point es-timate of q as shown previously in equation (10),Bayesian inference is based on the whole param-eter space in order to incorporate uncertainty intoour multinomial model.
This requires a posterior479probability distribution over q as follows:p(?fn, e?n|ok) =?p(?fn, e?n|qk)p(qk|?k) dqk=C?
(?m?km)?m?(?km)?m?
(?km+ ?m(?fn, e?n))?
(?m?km+ ?m(?fn, e?n)).
(11)Here ?kare new hyperparameters of the pos-terior derived by means of Bayes theorem as fol-lows:p(qk|?k) =p(qk|?
)?Nknp(?fn, e?n|qk)?p(qk|?
)?Nknp(?fn, e?n|qk)dqk.
(12)The solution of (11) will result in:?k= ?
+Nk?n?
(?fn, e?n).
(13)For completeness we give a summary of deriva-tions of equations (11) and (13) in Appendix B,more detailed discussions can be found in (Barber,2012).5 Feature SelectionIn several high dimensional pattern classificationproblems, there is increasing evidence that thediscriminant information may be in small sub-spaces, motivating feature selection (Li and Niran-jan, 2013).
Having irrelevant or redundant fea-tures could affect the classification performance(Liu and Motoda, 1998).
They might mislead thelearning algorithms or overfit them to the data andthus have less accuracy.The aim of feature selection is to find the op-timal subset features which maximize the abilityof prediction, which is the main concern, or sim-plify the learned results to be more understand-able.
There are many ways to measure the good-ness of a feature or a subset of features; howeverthe criterion will be discussed is mutual informa-tion.5.1 Mutual InformationInformation criteria are based on the concept ofentropy which is the amount of randomness.
Thedistribution of a fair coin, for example, is com-pletely random so the entropy of the coin is veryhigh.
The following equation calculates the en-tropy of a variable X (MacKay, 2002):H (X) = ?
?xp(x) log p(x).
(14)The mutual information of a feature X can be mea-sured by calculating the difference between theprior uncertainty of the class variable Y and theposterior uncertainty after using the feature as fol-lows (MacKay, 2002):I(X;Y ) = H(Y )?H(Y |X) (15)=?x,yp(x, y) logp(x, y)p(x)p(y).The advantage of mutual Information over othercriteria is the ability to detect nonlinear patterns.The disadvantage is its bias towards higher ar-bitrary features; however this problem can besolved by normalizing the information as follows(Est?evez et al., 2009):Inorm(X;Y ) =I(X;Y )min(H(X), H(Y )).
(16)6 ExperimentsThe corpus used in our experiments is MultiUNwhich is a large-scale parallel corpus extractedfrom the United Nations website1(Eisele andChen, 2010).
We have used Arabic and Englishportion of MultiUN.
Table 2 shows the generalstatistics.Statistics Arabic EnglishSentence Pairs 9.7 MRunning Words 255.5 M 285.7 MWord/Line 22 25Vocabulary Size 677 K 410 KTable 2: General statistics of Arabic-English Mul-tiUN (M: million, K: thousand).We simplify the problem by classifying phrasemovements into three categories (monotone,swap, discontinuous).
To train the reorderingmodels, we used GIZA++ to produce word align-ments (Och and Ney, 2000).
Then, we used theextract tool that comes with the Moses2toolkit(Koehn et al., 2007) in order to extract phrase pairsalong with their orientation classes.Each extracted phrase pair is represented by lin-guistic features as follows:?
Aligned source and target words in a phrasepair.
Each word alignment is a feature.1http://www.ods.un.org/ods/2Moses is an open source toolkit for statistical machinetranslation (www.statmt.org/moses/).480?
Words within a window around the sourcephrase to capture the context.
We choose ad-jacent words of the phrase boundary.Most researchers build one reordering modelfor the whole training set (Zens and Ney, 2006;Xiong et al., 2006; Nguyen et al., 2009; Xianget al., 2011).
Ni et al.
(Ni et al., 2011) simpli-fied the learning problem to have as many sub-models as source phrases.
Training data were di-vided into small independent sets where sampleshaving the same source phrase are considered atraining set.
In our experiments, we have chosenthe first method.We compare lexicalized and Bayesian reorder-ing models in two phases.
In the classificationphase, we see the performance of the models asa classification problem.
In the translation phase,we test the actual impact of these reordering mod-els in a translation system.6.1 ClassificationWe built naive Bayes classifier with both MAP es-timate and Bayesian inference.
We also used mu-tual Information in order to select the most infor-mative features for our classification task.Table 3 reports the error rate of the reorder-ing models compared to the lexicalized reorder-ing model.
All experiments reported here wererepeated three times to evaluate the uncertaintiesin our results.
The results shows that there is noadvantage to using Bayesian inference instead ofMAP estimate.Classifier Error RateLexicalized model 25.2%Bayes-MAP estimate 19.53%Bayes-Bayesian inference 20.13%Table 3: Classification error rate of both lexical-ized and Bayesian models.The feature selection process reveals that manyfeatures have low mutual information.
Hence theyare not related to the classification task and can beexcluded from the model.
Figure 1 shows the nor-malized mutual information for all extracted fea-tures.A ranking threshold for selecting features basedon their mutual information is specified experi-mentally.
In Figure 2, we tried different thresh-olds ranging from 0.001 to 0.05 and measure theerror rate after each reduction.
Although thereis no much gain in terms of performance but theBayesian model maintains low error rate when theproportion of selected features is low.
The modelwith almost half of the feature space is as good asthe one with full feature space.Figure 1: Normalized mutual information for allextracted features (ranked from lowest to highest).0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 11919.52020.52121.52222.5Percentage of Feature ReductuionError RateFigure 2: Classification error rate of the Baysienmodel with different levels of feature reduction.6.2 Translation6.2.1 Experimental DesignWe used the Moses toolkit (Koehn et al., 2007)with its default settings.
The language modelis a 5-gram with interpolation and Kneser-Neysmoothing (Kneser and Ney, 1995).
We tuned thesystem by using MERT technique (Och, 2003).We built four Arabic-English translation sys-tems.
Three systems differ in how their reorderingmodels were estimated and the fourth system is a481baseline system without reordering model.
In allcases, orientation extraction is hierarchical-basedsince it is the best approach while orientations aremonotone, swap and discontinuous.
The model istrained in Moses by specifying the configurationstring hier-msd-backward-fe.As commonly used in statistical machine trans-lation, we evaluated the translation performanceby BLEU score (Papineni et al., 2002).
The testsets are NIST MT06 and NIST MT08.
Table 4shows statistics of development and test sets.
Wealso computed statistical significance for the pro-posed models using the paired bootstrap resam-pling method (Koehn, 2004).Evaluation Set Arabic EnglishDevelopment sentences 696 696words 19 K 21 KNIST MT06 sentences 1797 7188words 49 K 223 KNIST MT08 sentences 813 3252words 25 K 117 KTable 4: Statistics of development and test sets.The English side in NIST is larger because thereare four translations for each Arabic sentence.6.2.2 ResultsWe first demonstrate in Table 5 a general com-parison of the proposed model and the lexicalizedmodel in terms of disc size and average speed in atranslation system.
The size of Bayesian model isfar smaller.
The lexicalized model is slightly fasterthan the Bayesian model because we have over-head computational cost to extract features andcompute the orientation probabilities.
However,the disc size of our model is much smaller whichmakes it more efficient practically for large-scaletasks.Model Size (MB) Speed (s/sent)Lexicalized model 604 2.2Bayesian model 18 2.6Table 5: Disc size and average speed of the re-ordering models in a translation system.Table 6 shows the BLEU scores for the transla-tion systems according to two test sets.
The base-line system has no reordering model.
In the twotest sets, our Bayesian reordering model is betterthan the lexicalized one with at least 95% statis-tical significance.
As we have seen in the clas-sification section, Bayes classifier with Bayesianinference has no advantage over MAP estimate.Translation System MT06 MT08Baseline 28.92 32.13BL+ Lexicalized model 30.86 34.22BL+ Bayes-MAP estimate 31.21* 34.72*BL+ Bayes-Baysien inference 31.20 34.69Table 6: BLEU scores for Arabic-English trans-lation systems (*: better than the baseline with atleast 95% statistical significance).7 ConclusionIn this paper, we have presented generative mod-eling approach to phrase reordering in machinetranslation.
We have experimented with trans-lation from Arabic to English and shown im-provements over the lexicalized model of estimat-ing probabilities as relative frequencies of phrasemovements.
Our proposed Bayesian model withfeature selection is shown to be superior.
Thetraining time of the model is as fast as the lexical-ized model.
Its storage requirement is many timessmaller which makes it more efficient practicallyfor large-scale tasks.The feature selection process reveals that manyfeatures have low mutual information.
Hence theyare not related to the classification task and can beexcluded from the model.
The model with almosthalf of the feature space is as good as the one withfull feature space.Previously proposed discriminative modelsmight achieve higher score than the reported re-sults.
However, our model is scalable to large-scale systems since parameter estimation requireonly one pass over the data with limited memory(i.e.
no iterative learning).
This is a critical advan-tage over discriminative models.Our current work focuses on three issues.
Thefirst is improving the translation speed of the pro-posed model.
The lexicalized model is slightlyfaster.
The second is using more informative fea-tures.
We plan to explore part-of-speech informa-tion, which is more accurate in capturing content.Finally, we will explore different feature selectionmethods.
In our experiments, feature reduction isbased on univariate ranking which is riskier thanmultivariate ranking.
This is because useless fea-ture can be useful with others.482ReferencesD.
Barber.
2012.
Bayesian Reasoning and MachineLearning.
Cambridge University Press.P.
Brown, J. Cocke, S. Della Pietra, V. Della Pietra,F.
Jelinek, R. Mercer, and P. Roossin.
1988.
A sta-tistical approach to language translation.
In 12th In-ternational Conference on Computational Linguis-tics (COLING), pages 71?76.P.
Brown, V. Pietra, S. Pietra, and R. Mercer.
1993.The mathematics of statistical machine translation:Parameter estimation.
Computational Linguistics,19(2):263?311.C.
Cherry.
2013.
Improved reordering for phrase-based translation using sparse features.
In Proceed-ings of the 2013 Conference of the North AmericanChapter of the Association for Computational Lin-guistics: Human Language Technologies, pages 22?31, Atlanta, Georgia, June.
Association for Compu-tational Linguistics.A.
Eisele and Y. Chen.
2010.
Multiun: A multilingualcorpus from united nation documents.
In DanielTapias, Mike Rosner, Stelios Piperidis, Jan Odjik,Joseph Mariani, Bente Maegaard, Khalid Choukri,and Nicoletta Calzolari (Conference Chair), editors,Proceedings of the Seventh conference on Interna-tional Language Resources and Evaluation, pages2868?2872.
European Language Resources Associ-ation (ELRA), 5.P.
Est?evez, M. Tesmer, C. Perez, and J. Zurada.
2009.Normalized mutual information feature selection.Trans.
Neur.
Netw., 20(2):189?201, February.M.
Galley and C. Manning.
2008.
A simple andeffective hierarchical phrase reordering model.
InProceedings of the 2008 Conference on EmpiricalMethods in Natural Language Processing, pages848?856, Hawaii, October.
Association for Compu-tational Linguistics.R.
Kneser and H. Ney.
1995.
Improved backing-offfor m-gram language modeling.
IEEE InternationalConference on Acoustics, Speech and Signal Pro-cessing, pages 181?184.P.
Koehn and C. Monz.
2005.
Shared task: Sta-tistical machine translation between european lan-guages.
In Proceedings of ACL Workshop on Build-ing and Using Parallel Texts, pages 119?124.
Asso-ciation for Computational Linguistics.P.
Koehn, A. Axelrod, A. Mayne, C. Callison-Burch,M.
Osborne, and D. Talbot.
2005.
Edinburgh sys-tem description for the 2005 IWSLT speech trans-lation evaluation.
In Proceedings of InternationalWorkshop on Spoken Language Translation, Pitts-burgh, PA.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,and E. Herbst.
2007.
Moses: Open source toolkitfor statistical machine translation.
In Proceedingsof the ACL 2007 Demo and Poster Sessions, pages177?180.P.
Koehn.
2004.
Statistical significance tests for ma-chine translation evaluation.
In Dekang Lin andDekai Wu, editors, Proceedings of EMNLP 2004,pages 388?395, Barcelona, Spain, July.
Associationfor Computational Linguistics.P.
Koehn.
2010.
Statistical Machine Translation.Cambridge University Press.S.
Kumar and W. Byrne.
2005.
Local phrase reorder-ing models for statistical machine translation.
InProceedings of Human Language Technology Con-ference and Conference on Empirical Methods inNatural Language Processing, pages 161?168, Van-couver, British Columbia, Canada, October.
Associ-ation for Computational Linguistics.Hongyu Li and M. Niranjan.
2013.
Discriminant sub-spaces of some high dimensional pattern classifica-tion problems.
In IEEE International Workshop onMachine Learning for Signal Processing (MLSP),pages 27?32.H.
Liu and H. Motoda.
1998.
Feature Selection forKnowledge Discovery and Data Mining.
KluwerAcademic Publishers, Norwell, MA, USA.D.
MacKay.
2002.
Information Theory, Inference &Learning Algorithms.
Cambridge University Press,New York, NY, USA.V.
Nguyen, A. Shimazu, M. Nguyen, and T. Nguyen.2009.
Improving a lexicalized hierarchical reorder-ing model using maximum entropy.
In Proceed-ings of the Twelfth Machine Translation Summit (MTSummit XII).
International Association for MachineTranslation.Y.
Ni, C. Saunders, S. Szedmak, and M. Niranjan.2011.
Exploitation of machine learning techniquesin modelling phrase movements for machine transla-tion.
Journal of Machine Learning Research, 12:1?30, February.F.
Och and H. Ney.
2000.
Improved statistical align-ment models.
In Proceedings of the 38th AnnualMeeting of the Association of Computational Lin-guistics (ACL).F.
Och and H. Ney.
2002.
Discriminative trainingand maximum entropy models for statistical ma-chine translation.
In Proceedings of the 40th AnnualMeeting of the Association for Computational Lin-guistics (ACL).F.
Och and H. Ney.
2004.
The alignment templateapproach to statistical machine translation.
Compu-tational Linguistics, 30(4):417?449.483F.
Och.
2003.
Minimum error rate training in statisti-cal machine translation.
In Proceedings of the 41stAnnual Meeting on Association for ComputationalLinguistics - Volume 1, ACL ?03, pages 160?167,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.K.
Papineni, S. Roukos, and T. Ward.
1998.
Max-imum likelihood and discriminative training of di-rect translation models.
In Proceedings of ICASSP,pages 189?192.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.Bleu: a method for automatic evaluation of machinetranslation.
In Proceedings of the 40th Annual Meet-ing on Association for Computational Linguistics,pages 311?318, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.S.
Rogers and M. Girolami.
2011.
A First Course inMachine Learning.
Chapman & Hall/CRC, 1st edi-tion.C.
Tillmann.
2004.
A unigram orientation model forstatistical machine translation.
In Proceedings ofHLT-NAACL: Short Papers, pages 101?104.B.
Xiang, N. Ge, and A. Ittycheriah.
2011.
Improvingreordering for statistical machine translation withsmoothed priors and syntactic features.
In Proceed-ings of SSST-5, Fifth Workshop on Syntax, Semanticsand Structure in Statistical Translation, pages 61?69, Portland, Oregon, USA.
Association for Com-putational Linguistics.D.
Xiong, Q. Liu, and S. Lin.
2006.
Maximum en-tropy based phrase reordering model for statisticalmachine translation.
In Proceedings of the 21st In-ternational Conference on Computational Linguis-tics and 44th Annual Meeting of the ACL, pages521?528, Sydney, July.
Association for Computa-tional Linguistics.R.
Zens and H. Ney.
2006.
Discriminative reorder-ing models for statistical machine translation.
InProceedings on the Workshop on Statistical MachineTranslation, pages 55?63, New York City, June.
As-sociation for Computational Linguistics.A MAP Estimate DerivationMultinomial distribution is defined as:p(x|q) = C?mqxmm(17)where C is a multinomial coefficient,C =(?mxm)!
?mxm!, (18)and qmis an event probability (?mqm= 1).A maximum a posteriori probability (MAP) es-timate requires a prior over q. Dirichlet distribu-tion is a conjugate prior and is defined as:p(q|?)
=?
(?m?m)?m?
(?m)?mq?m?1m(19)where ?mis is a parameter with a positive value.Finding the MAP estimate for q given a data isas follows:q?= arg maxqp(q|?,X)= arg maxq{p(q|?
)p(X|q)}= arg maxq{p(q|?
)?np(xn|q)}= arg maxq{?mq?m?1m?n,mqxnmm}= arg maxq{?mlog q?m?1m+?n,mlog qxnmm}.
(20)Since our function is subject to constraints(?mqm= 1), we introduce Lagrange multiplieras follows:f(q) =?mlog q?m?1m+?n,mlog qxnmm??(?mqm?1).
(21)Now we can find q?by taking the partial deriva-tive with respect to one variable qm:?f(q)?qm=?m?
1 +?nxnmqm?
?qm=?m?
1 +?nxnm?.
(22)Finally, we sum both sides over M to find ?
:??mqm=?m(?m?
1 +?nxnm)?
=?m(?m?
1) +?n,mxnm.
(23)The solution can be simplified by choosing thesame value for each ?mwhich will result in:qm=??
1 +?nxnmM(??
1) +?n,m?xnm?.
(24)484B Bayesian Inference DerivationIn Appendix A, the inference is based on a singlepoint estimate of q that has the highest posteriorprobability.
However, it can be based on the wholeparameter space to incorporate uncertainty.
Theprobability of a new data point marginalized overthe posterior as follows:p(x|?,X) =?p(x|q)p(q|?,X) dq, (25)p(q|?,X) =p(q|?)p(X|q)?p(q|?)p(X|q)dq.
(26)Since Dirichlet and Multinomial distributionsare conjugate pairs, they form the same density asthe prior.
Therefore the posterior is also Dirichlet.Now we can expand the posterior expression andre-arrange it to look like a Dirichlet as follows:p(q|?,X) ?
p(q|?)?np(xn|q)??mq?m?1m?n?mqxnmm??mq(?m+?nxnm)?1m.
(27)The new hyperparameters of the posterior is:?
?m= ?m+?nxnm.
(28)Finally, we expand and re-arrange Dirichlet andmultinomial distributions inside the integral in(25) as follows:p(x|?,X) =?C?mqxmm?
(?m??m)?m?(??m)?mq??m?1mdq=C?
(?m??m)?m?(??m)??mq??m+xm?1mdq.
(29)Note that inside the integral looks a Dirichletwithout a normalizing constant.
If we multiplyand divide by its normalizing constant (i.e.
Betafunction), the integral is going to be one becauseit is a density function, resulting in:p(x|?,X) = C?
(?m??m)?m?(??m)B(?
?+ x)?1B(?
?+ x)?mq??m+xm?1mdqc=C?
(?m??m)?m?(??m)B(?
?+ x)=C?
(?m??m)?m?(??m)?m?(?
?m+ xm)?
(?m(?
?m+ xm)).
(30)485
