First Joint Conference on Lexical and Computational Semantics (*SEM), pages 603?607,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsSaarland: Vector-based models of semantic textual similarityGeorgiana DinuCenter of Mind/Brain SciencesUniversity of Trentogeorgiana.dinu@unitn.itStefan ThaterDept.
of Computational LinguisticsUniversita?t des Saarlandesstth@coli.uni-saarland.deAbstractThis paper describes our system for the Se-meval 2012 Sentence Textual Similarity task.The system is based on a combination of fewsimple vector space-based methods for wordmeaning similarity.
Evaluation results showthat a simple combination of these unsuper-vised data-driven methods can be quite suc-cessful.
The simple vector space componentsachieve high performance on short sentences;on longer, more complex sentences, they areoutperformed by a surprisingly competitiveword overlap baseline, but they still bring im-provements over this baseline when incorpo-rated into a mixture model.1 IntroductionVector space models are widely-used methods forword meaning similarity which exploit the so-calleddistributional hypothesis, stating that semanticallysimilar words tend to occur in similar contexts.
Wordmeaning is represented by the contexts in which aword occurs, and similarity is computed by compar-ing these contexts in a high-dimensional vector space(Turney and Pantel, 2010).
Distributional models ofword meaning are attractive because they are sim-ple, have wide coverage, and can be easily acquiredat virtually no cost in an unsupervised way.
Fur-thermore, recent research has shown that, at leastto some extent, these models can be generalized tocapture similarity beyond the (isolated) word level,either as lexical meaning modulated by context, oras vectorial meaning representations for phrases andsentences.
In this paper we evaluate the use of someof these models for the Semantic Textual Similarity(STS) task, which measures the degree of semanticequivalence between two sentences.In recent work Mitchell and Lapata (2008) hasdrawn the attention to the question of building vecto-rial meaning representations for sentences by combin-ing individual word vectors.
They propose a family ofsimple ?compositional?
models that compute a vectorfor a phrase or a sentence by combining vectors ofthe constituent words, using different operations suchas vector addition or component-wise multiplication.More refined models have been proposed recently byBaroni and Zamparelli (2010) and Grefenstette andSadrzadeh (2011).Thater et al (2011) and others take a slightly dif-ferent perspective on the problem: Instead of com-puting a vector representation for a complete phraseor sentence, they focus on the problem of ?disam-biguating?
the vector representation of a target wordbased on distributional information about the wordsin the target?s context.
While this approach is not?compositional?
in the sense described above, it stillcaptures some meaning of the complete phrase inwhich a target word occurs.In this paper, we report on the system we used inthe Semeval 2012 Sentence Textual Similarity sharedtask and describe an approach that uses a combina-tion of few simple vector-based components.
Weextend the model of Thater et al (2011), which hasbeen shown to perform well on a closely related para-phrase ranking task, with an additive composition op-eration along the lines of Mitchell and Lapata (2008),and compare it with a simple alignment-based ap-proach which in turn uses vector-based similarityscores.
Results show that in particular the alignment-based approach can achieve good performance onthe Microsoft Research Video Description dataset.On the other datasets, all vector-based componentsare outperformed by a surprisingly competitive word603overlap baseline, but they still bring improvementsover this baseline when incorporated into a mixturemodel.
On the test dataset, the mixture model ranks10th and 13th on the Microsoft Research Paraphraseand Video Description datasets, respectively, whichwe take this to be a quite promising result given thatwe use only few relatively simple vector based com-ponents to compute similarity scores for sentences.The rest of the paper is structured as follows: Sec-tion 2 presents the individual vector-based compo-nents used by our system.
In Section 3 we presentdetailed evaluation results on the training set, as wellas results for our system on the test set, while Sec-tion 4 concludes the paper.2 Systems for Sentence SimilarityOur system is based on four different components:We use two different vector space models to repre-sent word meaning?a basic bag-of-words modeland a slightly simplified variant of the contextual-ization model of Thater et al (2011)?and two dif-ferent methods to compute similarity scores for sen-tences based on these two vector space models?one?compositional?
method that computes vectors forsentences by summing over the vectors of the con-stituent words, and one alignment-based method thatuses vector-based similarity scores for word pairs tocompute an alignment between the words in the twosentences.2.1 Vector Space ModelsFor the basic vector-space model, we assume a setW of words, and represent the meaning of a wordw ?W by a vector in the vector space V spanned bythe set of basis vectors {~ew?
| w?
?W} as follows:vbasic(w) = ?w?
?Wf (w,w?
)~ew?where f is a function that assigns a co-occurrencevalue to the word pair (w,w?).
In the experimentsreported below, we use pointwise mutual informationestimated on co-occurrence frequencies for wordswithin a 5-word window around the target word oneither side.11We use a 5-word window here as this setting has been shownto give best results on a closely related task in the literature(Mitchell and Lapata, 2008)This basic ?bag of words?
vector space model rep-resents word meaning by summing over all contextsin which the target word occurs.
Since words are of-ten ambiguous, this means that context words pertain-ing to different senses of the target word are mixedwithin a single vector representation, which can leadto ?noisy?
similarity scores.
The vector for the nouncoach, for instance, contains context words like teachand tell (person sense) as well as derail and crash(vehicle sense).To address this problem, Thater et al (2011) pro-pose a ?contextualization?
model in which the indi-vidual components of the target word?s vector are re-weighted, based on distributional information aboutthe words in the target?s context.
Let us assume thatthe context consist of a single word c. The vector fora target w in context c is then defined as:v(w,c) = ?w??W?(c,w?)
f (w,w?
)~ew?where ?
is some similarity score that quantifies towhat extent the vector dimension that correspondsto w?
is compatible with the observed context c. Inthe experiments reported below, we take ?
to be thecosine similarity of c and w?
; see Section 3 for details.In the experiments reported below, we use allwords in the syntactic context of the target word tocontextualize the target:vctx(w) = ?c?C(w)v(w,c)where C(w) is the context in which w occurs, i.e.
allwords related to w by a dependency relation such assubject or object, including inverse relations.Remark.
The contextualization model presentedabove is a slightly simplified version of the originalmodel of Thater et al (2011): it uses standard bag-of-words vectors instead of syntax-based vectors.
Thissimplified version performs better on the trainingdataset.
Furthermore, the simplified model has beenshown to be equivalent to the models of Erk andPado?
(2008) and Thater et al (2010) by Dinu andThater (2012), so the results reported below carryover directly to these other models as well.2.2 Vector Composition and AlignmentThe two vector space models sketched above repre-sent the meaning of words, and thus cannot be applied604directly to model similarity of phrases or sentences.One obvious and straightforward way to extend thesemodels to the sentence level is to follow Mitchell andLapata (2008) and represent sentences by vectorsobtained by summing over the individual vectors ofthe constituent words.
These ?compositional?
mod-els can then be used to compute similarity scoresbetween sentence pairs in a straightforward way, sim-ply by computing the cosine of the angle betweenvectors (or some other similarity score) for the twosentences:simadd(S,S?)
= cos(?w?Sv(w), ?w??S?v(w?
))(1)where v(w) can be instantiated either with basic orwith ctx vectors.In addition to the compositional models, we alsoexperimented with an alignment-based approach: In-stead of computing vectors for complete sentences,we compute an alignment between the words in thetwo sentences.
To be more precise, we computecosine similarity scores between all possible pairsof words (tokens) of the two sentences; based onthese similarity scores, we then compute a one-to-onealignment between the words in the two sentences2,using a greedy search strategy (see Fig.
1).
We assigna weight to each link in the alignment which is simplythe cosine similarity score of the corresponding wordpair and take the sum of the link weights, normalizedby the maximal length of the two sentences to be thecorresponding similarity score for the two sentences.The final score is then:simalign(S,S?)
=?(w,w?)?ALIGN(S,S?)
cos(v(w),v(w?
))max(|S|, |S?|)where v(w) is the vector for w, which again can beeither the basic or the contextualized vector.3 EvaluationIn this section we present our experimental results.In addition to the models described in Section 2, wedefine a baseline model which simply computes theword overlap between two sentences as:simoverlap(S,S?)
=|S?S?||S?S?|(2)2Note that this can result in some words not being alignedfunction ALIGN(S1,S2)alignment?
/0marked?
/0pairs?{?w,w??
| w ?
S1,w?
?
S2}while pairs not empty do?w,w??
?
highest cosine pair in pairsif w /?
marked and w?
/?
marked thenalignment?
?w,w??
?
alignmentmarked?{w,w?}
?markedend ifpairs?
pairs \ {?w,w??
}end whilereturn alignmentend functionFigure 1: The alignment algorithmThe score assigned by this method is simply the num-ber of words that the two sentences have in commondivided by their total number of words.
Finally, wealso propose a straightforward mixture model whichcombines all of the above methods.
We use the train-ing data to fit a degree two polynomial over theseindividual predictors using least squares regression.We report cross-validation scores.3.1 Evaluation setupThe vector space used in all experiments is a bag-of-words space containing word co-occurrence counts.We use the GigaWord (1.7 billion tokens) as inputcorpus and extract word co-occurrences within asymmetric 5-word context window.
Co-occurrencecounts smaller than three are set to 0 and we furtherapply (positive) pmi weighting.3.2 Training resultsThe training data results are shown in Figure 2.
Thebest performance on the video dataset is achievedby the alignment method using a basic vector rep-resentation to compute word-level similarity.
Allvector-space methods perform considerably betterthan the simple word overlap baseline on this dataset,the alignment method achieving almost 20% gainover this baseline.
This indicates that informationabout the meaning of the words is very beneficial forthis type of data, consisting of small, well-structuredsentences.Using the alignment method with contextualized605Component MSRvid MSRpar SMTeurbasic/add 70.9 33.3 31.8ctx/add 65.7 23.0 30.4basic/align 74.6 40.5 32.1overlap 56.8 59.5 50.0mixture 78.1 61.8 54.1Figure 2: Results on the training set.vector representations (omitted in the table) does notbring any improvement and it performs similarly tothe ctx/add method.
This suggests that aligning sim-ilar words in the two sentences does not benefit fromfurther meaning disambiguation through contextual-ized vectors and that some level of disambiguationmay be implicitly performed.On the paraphrase and europarl datasets, the over-lap baseline outperforms, by a large margin, the vec-tor space models.
This is not surprising, as it isknown that word overlap baselines can be very com-petitive on Recognizing Textual Entailment datasets,to which these two datasets bare a large resemblance.In particular this indicates that the methods proposedfor combining vector representations of words donot provide, in the current state, accurate models formodeling the meaning of larger sentences.We also report 10-fold cross-validation scores ob-tained with the mixture model.
On all datasets, thisoutperforms the individual methods, improving bya margin of 2%-4% the best single methods.
In par-ticular, on the paraphrase and europarl datasets, thisshows that despite the considerably inferior perfor-mance of the vector-based methods, these can stillhelp improve the overall performance.This is also reflected in Table 3, where we evaluatethe performance of the mixture method when, inturn, one of the individual components is excluded:with few exceptions, all components contribute to theperformance of the mixtures.3.3 Test resultsWe have submitted as our official runs the best sin-gle vector space model, performing alignment withbasic vector similarity, as well as the mixture meth-ods.
The mixture method uses weights individuallylearned for each of the datasets made available duringComponent MSRvid MSRpar SMTeurbasic/add ?2.1 ?0.1 ?1.5ctx/add ?0.6 +1.3 +0.4basic/align ?4.1 ?1.9 ?2.6overlap ?0.1 ?17.0 ?23.0Figure 3: Results on the training set when removing indi-vidual components from the mixture model.training.
For the two surprise datasets we carry overthe weights of what we have considered to be themost similar training-available sets: video weights ofontonotes and paraphrase weights for news.The test data results are given in 4.
We reportthe results for the individual datasets as well as themean Pearson correlation, weighted by the sizes ofthe datasets.
The table also shows the performanceof the official task baseline as well as the top threeruns accoring to the overall weighted mean score.As expected, the mixture method outperforms bya large margin the alignment model, achieving rank10 and rank 13 on the video and paraphrase datasets.Overall the mixture method ranks 43 according to theweighted mean measure (rank 22 if correcting our of-ficial submission which contained the wrong outputfile for the europarl dataset).
The other more con-troversial measures rank our official, not corrected,submission at position 13 (RankNrm) and 71 (Rank),overall.
This is an encouraging result, as the individ-ual components we have used are all unsupervised,obtained solely from large amounts of unlabeled data,and with no other additional resources.
The trainingdata made available has only been used to learn aset of weights for combining these individual compo-nents.4 ConclusionsThis paper describes an approach that combines fewsimple vector space-based components to model sen-tence similarity.
We have extended the state-of-the-art model for contextualized meaning representationsof Thater et al (2011) with an additive composi-tion operation along the lines of Mitchell and Lap-ata (2008).
We have combined this with a simplealignment-based method and a word overlap baselineinto a mixture model.Our system achieves promising results in particular606Dataset basic/align mixture baseline Run1 Run2 Run3MSRvid 77.1 83.1 30.0 87.3 88.0 85.6MSRpar 40.4 63.1 43.3 68.3 73.4 64.0SMTeur 26.8 13.9 (37.1?)
45.4 52.8 47.7 51.5OnWN 57.2 59.6 58.6 66.4 67.9 71.0SMTnews 35.0 38.0 39.1 49.3 39.8 48.3ALL 49.5 45.4 31.1 82.3 81.3 73.3Rank 65 71 87 1 3 15ALLNrm 78.7 82.5 67.3 85.7 86.3 85.2RankNrm 50 13 85 2 1 5Mean 50.6 56.6 (60.0?)
43.5 67.7 67.5 67.0RankMean 60 43 (22?)
70 1 2 3Figure 4: Results on the test set.
?
?
corrected score (official results score wrong prediction file we have submitted forthe europarl dataset).
Official baseline and top three runs according to the weighted mean measure.on the Microsoft Research Paraphrase and VideoDescription datasets, on which it ranks 13th and 10th,respectively.
We take this to be a promising result,given that our focus has not been the developmentof a highly-competitive complex system, but ratheron investigating what performance can be achievedwhen using only vector space methods.An interesting observation is that the methods forcombining word vector representations (the vectoraddition, or the meaning contextualization) can bebeneficial for modeling the similarity of the small,well-structured sentences of the video dataset, how-ever they do not perform well on comparing longer,more complex sentences.
In future work we plan tofurther investigate methods for composition in vectorspace models using the STS datasets, in addition tothe small, controlled datasets that have been typicallyused in this line of research.Acknowledgments.
This work was supported bythe Cluster of Excellence ?Multimodal Computingand Interaction,?
funded by the German ExcellenceInitiative.ReferencesMarco Baroni and Roberto Zamparelli.
2010.
Nouns arevectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.
In Proceedingsof the 2010 Conference on Empirical Methods in Nat-ural Language Processing, Cambridge, MA, October.Association for Computational Linguistics.Georgiana Dinu and Stefan Thater.
2012.
A comparisonof models of word meaning in context.
In Proceedingsof the 2012 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies.
Short paper, to appear.Katrin Erk and Sebastian Pado?.
2008.
A structured vectorspace model for word meaning in context.
In Proceed-ings of the 2008 Conference on Empirical Methods inNatural Language Processing, Honolulu, HI, USA.Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011.Experimental support for a categorical compositionaldistributional model of meaning.
In Proceedings ofthe 2011 Conference on Empirical Methods in NaturalLanguage Processing, Edinburgh, Scotland, UK., July.Association for Computational Linguistics.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In Proceedings ofACL-08: HLT, Columbus, OH, USA.Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.2010.
Contextualizing semantic representations usingsyntactically enriched vector models.
In Proceedingsof the 48th Annual Meeting of the Association for Com-putational Linguistics, Uppsala, Sweden.Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.2011.
Word meaning in context: A simple and effectivevector model.
In Proceedings of 5th International JointConference on Natural Language Processing, pages1134?1143, Chiang Mai, Thailand, November.
AsianFederation of Natural Language Processing.Peter D. Turney and Patrick Pantel.
2010.
From frequencyto meaning: Vector space modes of semantics.
Journalof Artificial Intelligence Research, 37:141?188.607
