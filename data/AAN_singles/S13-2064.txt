Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 390?394, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsUSNA: A Dual-Classifier Approach to Contextual Sentiment AnalysisGanesh Harihara and Eugene Yang and Nathanael ChambersUnited States Naval AcademyAnnapolis, MD 21401, USAnchamber@usna.eduAbstractThis paper describes a dual-classifier ap-proach to contextual sentiment analysis at theSemEval-2013 Task 2.
Contextual analysis ofpolarity focuses on a word or phrase, ratherthan the broader task of identifying the senti-ment of an entire text.
The Task 2 definitionincludes target word spans that range in sizefrom a single word to entire sentences.
How-ever, the context of a single word is depen-dent on the word?s surrounding syntax, while aphrase contains most of the polarity within it-self.
We thus describe separate treatment withtwo independent classifiers, outperforming theaccuracy of a single classifier.
Our systemranked 6th out of 19 teams on SMS messageclassification, and 8th of 23 on twitter data.We also show a surprising result that a verysmall amount of word context is needed forhigh-performance polarity extraction.1 IntroductionA variety of approaches to sentiment analysis havebeen proposed in the literature.
Early work sought toidentify the general sentiment of entire documents,but a recent shift to social media has provided a largequantity of publicly available data, and private orga-nizations are increasingly interested in how a pop-ulation ?feels?
toward its products.
Identifying thepolarity of language toward a particular topic, how-ever, no longer requires identifying the sentiment ofan entire text, but rather the contextual sentimentsurrounding a target phrase.Identifying the polarity of text toward a phrase issignificantly different from a sentence?s overall po-larity, as seen in this example from the SemEval-2013 Task 2 (Wilson et al 2013) training set:I had a severe nosebleed last night.
I thinkmy iPad caused it as I was browsing for afew hours on it.
Anyhow, its stopped, whichis good.An ideal sentiment classifier would classify thistext as overall positive (the nosebleed stopped!
), butthis short snippet actually contains three types of po-larity (positive, negative, and neutral).
The middlesentence about the iPad is not positive, but neutral.The word ?nosebleed?
has a very negative polarityin this context, and the phrase ?its stopped?
is posi-tive.
Someone interested in specific health concerns,such as nosebleeds, needs a contextual classifier toidentify the desired polarity in this context.This example also illustrates how phrases of dif-ferent sizes require unique handling.
Single tokenphrases, such as ?nosebleed?, are highly dependenton the surrounding context for its polarity.
How-ever, the polarity of the middle iPad sentence is con-tained within the phrase itself.
The surrounding con-text is not as important.
This paper thus proposesa dual-classifier that trains two separate classifiers,one for single words, and another for phrases.
Weempirically show that unique features apply to both,and both benefit from independent training.
In fact,we show a surprising result that a very small win-dow size is needed for the context of single wordphrases.
Our system performs well on the SemEvaltask, placing 8th of 23 systems on twitter text.
It alsoshows strong generalization to SMS text messages,placing 6th of 19.3902 Previous WorkSentiment analysis is a large field applicable tomany genres.
This paper focuses on social media(microblogs) and contextual polarity, so we onlyaddress the closest work in those areas.
For abroader perspective, several survey papers are avail-able (Pang and Lee, 2008; Tang et al 2009; Liu andZhang, 2012; Tsytsarau and Palpanas, 2012).Microblogs serve as a quick way to measure alarge population?s mood and opinion.
Many differ-ent sources have been used.
O?Connor et al(2010)used Twitter data to compute a ratio of positive andnegative words to measure consumer confidence andpresidential approval.
Kramer (2010) counted lex-icon words on Facebook for a general ?happiness?measure, and Thelwall (2011) built a general senti-ment model on MySpace user comments.
These aregeneral sentiment algorithms.Specific work on microblogs has focused on find-ing noisy training data with distant supervision.Many of these algorithms use emoticons as seman-tic indicators of polarity.
For instance, a tweet thatcontains a sad face likely contains a negative polar-ity (Read, 2005; Go et al 2009; Bifet and Frank,2010; Pak and Paroubek, 2010; Davidov et al 2010;Kouloumpis et al 2011).
In a similar vein, hash-tags can also serve as noisy labels (Davidov et al2010; Kouloumpis et al 2011).
Most work on dis-tant supervision relies on a variety of syntactic andword-based features (Marchetti-Bowick and Cham-bers, 2012).
We adopt many of these features.Supervised learning for contextual sentimentanalysis has not been thoroughly investigated.
La-beled data for specific words or queries is expensiveto generate, so Jiang et al(2011) is one of the fewapproaches with labeled training data.
Earlier workon product reviews sought the sentiment toward par-ticular product features.
These systems used rulebased approaches based on parts of speech and othersurface features (Nasukawa and Yi, 2003; Hu andLiu, 2004; Ding and Liu, 2007).Finally, topic identification in microblogs is alsorelated.
The first approaches are somewhat simple,selecting single keywords (e.g., ?Obama?)
to rep-resent the topic (e.g., ?US President?
), and retrievetweets that contain the word (O?Connor et al 2010;Tumasjan et al 2010; Tan et al 2011).
These sys-tems then classify the polarity of the entire tweet,and ignore the question of polarity toward the partic-ular topic.
This paper focuses on the particular key-word or phrase, and identifies the sentiment towardthat phrase, not the overall sentiment of the text.3 DatasetThis paper uses three polarity classes: positive, neg-ative, and neutral.
We developed all algorithms onthe ?Task A?
corpora provided by SemEval-2013Task 2 (Wilson et al 2013).
Both training and de-velopment sets were provided, and an unseen testset was ultimately used to evaluate the final systems.The number of tweets in each set are shown here:positive negative neutraltraining 5348 2817 422development 648 430 57test (tweet) 2734 1541 160test (sms) 1071 1104 1594 Contextual Sentiment AnalysisContextual sentiment analysis focuses on the dispo-sition of a certain word or groups of words.
Mostdata-driven approaches rely on a labeled corpus todrive the learning process, and this paper is no dif-ferent.
However, we propose a novel approach tocontextual analysis that differentiates between sin-gle words and phrases.The semantics of a single word in context fromthat of a phrase are fundamentally different.
Sinceone word will have multiple contexts and is heavilyinfluenced by the surrounding words, more consid-eration is given to adjacent words.
A phrase oftencarries its own semantics, so has less variability inits meaning based on its context.
Context is still im-portant, but we propose separate classifiers in orderto learn weights unique to tokens and phrases.
Thefollowing describes the two unique feature sets.
Wetrained a Maximum Entropy classifier for each set.4.1 Text Pre-ProcessingAll text is lowercased, and twitter usernames (e.g.,@user) and URLs are replaced with placeholder to-kens.
The text is then split on whitespace.
We alsoprepend the occurrence of token ?not?
to the subse-quent token, merging the two (e.g., ?not happy?
be-391comes ?not-happy?).
We also found that removingprefix and affix punctuation from each token, andstoring the punctuation for later use in punctuationfeatures boosts performance.
These cleaned tokensare the input to the features described below.4.2 Single Word Sentiment AnalysisAssigning polarity to a single word mainly requiresfeatures that accurately capture the surrounding con-text.
In fact, many single words do no carry any po-larity in isolation, but solely require context.
Takethe following two examples:Justin LOVE YA so excited for the concert inoctober MEXICO LOVES YOUIm not getting on twitter tomorrow because allmy TL will consist of is a bunch of girls talkingabout Justin BeiberIn these examples, Justin is the name of a singerwho does not carry an initial polarity.
The first tweetis clearly positive toward him, while the second isnot.
Our single-token classifier used the followingset of features to capture these different contexts:Target Token: The first features are the unigramand bigram ending with the target token.
We attach aunique string to each to distinguish it from the text?sother n-grams.
We also include a feature for anypunctuation that was attached to the end of the token(e.g., ?Justin!?
generates ?!?
as a feature).Target Patterns: This feature generalizes the n-grams that include the target word.
It replaces thetarget word with a variable in an effort to capturegeneral patterns that indicate sentiment.
For in-stance, using the first tweet above, we add the tri-gram ?<s> LOVE?
and two bigrams, ?<s> ?and ?
LOVE?.Unigrams, Bigrams, Trigrams: We include allother n-grams in the text within a window of sizen from the target token.Dictionary Matching: We have two binary fea-tures, postivemood and negativemood, that indicateif any word in the text appears in a sentiment lex-icon?s positive or negative list.
We use Bing Liu?sOpinion Lexicon1.1http://www.cs.uic.edu/?liub/FBS/sentiment-analysis.html\#lexiconPunctuation Features: We included a binary fea-ture for the presence or absence of exclamationmarks anywhere in the text.
Further, we generatea feature for punctuation at the end of the text.Emoticons: We included two binary features for thepresence or absence of a smiley face and sad faceemoticon.4.3 Phrasal Sentiment AnalysisWe adopted several single word features for use inphrases, including punctuation, dictionary match-ing, and emoticons.
However, since phrasal analy-sis is often less dependent on context and more de-pendent on the phrase itself, we altered the n-gramfeatures to be unique to the phrase.
The followingfeatures are solely used for target phrases, not singlewords:Unigrams, Bigrams, Trigrams: We include all n-grams in the target phrase only.
This differs fromthe single token features that included n-grams froma surrounding window.Phrasal Punctuation: If the target phrase ends withany type of punctuation, we include it as a feature.5 ExperimentsInitial model design and feature tuning was con-ducted on the SemEval-2013 Task 2 training set fortraining, and its dev set for evaluation.
We split thedata into two parts: tweets with single word targets,and tweets with target phrases.
We trained two Max-Ent classifiers using the Stanford JavaNLP toolkit2.Each datum in the test set is labeled using the appro-priate classifier based on the target phrase?s length.The first experiments are ablation over the fea-tures described in Section 4, separately improvingthe single token and phrasal classifiers.
Results arereported in Table 1 using simple accuracy on the de-velopment set.
We initially do not split off punc-tuation, and use only unigram features for phrases.The window size is initally infinite (i.e., the entiretext is used for n-grams).
Bigrams and trigrams hurtperformance and are not shown.
Reducing the win-dow size to a single token (ignore the entire tweet)increased performance by 1.2%, and stripping punc-tuation off tokens by another 1.9%.
The perfor-2http://nlp.stanford.edu/software/index.shtml392Single Token FeaturesJust Unigrams 70.5+ Target Token Patterns 70.4+ Sentiment Lexicon 71.5+ Target Token N-Grams 73.3+ EOS punctuation 73.2+ Emoticons 73.3Set Window Size = 1 74.5Strip punctuation off tokens 76.4Phrasal FeaturesJust Unigrams 76.4+ Emoticons 76.3+ EOS punctuation 76.6+ Exclamation Marks 76.5+ Sentiment Lexicon 77.7Table 1: Feature ablation in order.
Single token featuresbegin with unigrams only, holding phrasal features con-stant at unigrams only.
The phrasal table picks up wherethe single token table finishes.
Each row uses all featuresadded in previous rows.Dual-Classifier ComparisonSingle Classifier 76.6%Dual-Classifier 77.7%Table 2: Performance increase from splitting into twoclassifiers.
Accuracy reported on the development set.mance increase with phrasal features is 1.3% abso-lute, whereas token features contributed 5.9%.After choosing the optimum set of features basedon ablation, we then retrained the classifiers on boththe training and development sets as one large train-ing corpus.
The SemEval-2013 Task 2 competitionincluded two datasets for testing: tweets and SMSmessages.
Official results for both are given in Ta-ble 3 using the F1 measure.Finally, we compare our dual-classifier to a singlestandard classifier.
We use the same features usedin Table 1, train on the training set, and report accu-racy on the development set.
See Table 2.
Our dualclassifier improves relative accuracy by 1.4%.6 DiscussionOne of the main surprises from our experiments wasthat a large portion of text could be ignored with-out hurting classification performance.
We reducedTwitter DatasetF1 ScoreTop System (1st) 88.9This Paper (8th) 81.3Majority Baseline (20th) 61.6Bottom System (24th) 34.7SMS DatasetF1 ScoreTop System (1st) 88.4This Paper (6th) 79.8Majority Baseline (19th) 47.3Min System (20th) 36.4Table 3: Performance on Twitter and SMS Data.the window size in which n-grams are extracted tosize one, and performance actually increases 1.2%.At least for single word target phrases, including n-grams of the entire tweet/sms is not helpful.
Weonly used n-gram patterns that included the tokenand its two immediate neighbors.
A nice side ben-efit is that the classifier contains fewer features, andtrains faster as a result.The decision to use two separate classifiers helpedperformance, improving by 1.4% relative accuracyon the development set.
The decision was moti-vated by the observation that the polarity of a tokenis dependent on its surrounding context, but a longerphrase is dependent more on its internal syntax.
Thisallowed us to make finer-grained feature decisions,and the feature ablation experiments suggest our ob-servation to be true.
Better feature weights are ulti-mately learned for the unique tasks.Finally, the feature ablation experiments revealeda few key takeaways for feature engineering: bi-grams and trigrams hurt classification, using a win-dow size is better than the entire text, and punctu-ation should always be split off tokens.
Further, asentiment lexicon reliably improves both token andphrasal classification.Opportunities for future work on contextual anal-ysis exist in further analysis of the feature windowsize.
Why doesn?t more context help token classifi-cation?
Do n-grams simply lack the deeper seman-tics needed, or are these supervised algorithms stillsuffering from sparse training data?
Better sentenceand phrase detection may be a fruitful focus.393ReferencesAlbert Bifet and Eibe Frank.
2010.
Sentiment knowl-edge discovery in twitter streaming data.
In LectureNotes in Computer Science, volume 6332, pages 1?15.Dmitry Davidov, Oren Tsur, and Ari Rappoport.
2010.Enhanced sentiment learning using twitter hashtagsand smileys.
In Proceedings of the 23rd InternationalConference on Computational Linguistics (COLING2010).Xiaowen Ding and Bing Liu.
2007.
The utility of lin-guistic rules in opinion mining.
In Proceedings ofSIGIR-2007, pages 23?27.Alec Go, Richa Bhayani, and Lei Huang.
2009.
Twit-ter sentiment classification using distant supervision.Technical report.Minqing Hu and Bing Liu.
2004.
Mining and sum-marizing customer reviews.
In Proceedings of theACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining.Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and TiejunZhao.
2011.
Target-dependent twitter sentiment clas-sification.
In Proceedings of the Association for Com-putational Linguistics (ACL-2011).Efthymios Kouloumpis, Theresa Wilson, and JohannaMoore.
2011.
Twitter sentiment analysis: The goodthe bad and the omg!
In Proceedings of the Fifth In-ternational AAAI Conference on Weblogs and SocialMedia.Adam D. I. Kramer.
2010.
An unobtrusive behavioralmodel of ?gross national happiness?.
In Proceedings ofthe 28th International Conference on Human Factorsin Computing Systems (CHI 2010).Bing Liu and Lei Zhang.
2012.
A survey of opinion min-ing and sentiment analysis.
Mining Text Data, pages415?463.Micol Marchetti-Bowick and Nathanael Chambers.2012.
Learning for microblogs with distant supervi-sion: Political forecasting with twitter.
In Proceedingsof the 13th Conference of the European Chapter of theAssociation for Computational Linguistics.Tetsuya Nasukawa and Jeonghee Yi.
2003.
Sentimentanalysis: capturing favorability using natural languageprocessing.
In Proceedings of K-CAP.Brendan O?Connor, Ramnath Balasubramanyan,Bryan R. Routledge, and Noah A. Smith.
2010.From tweets to polls: Linking text sentiment to publicopinion time series.
In Proceedings of the AAAIConference on Weblogs and Social Media.Alexander Pak and Patrick Paroubek.
2010.
Twitter as acorpus for sentiment analysis and opinion mining.
InProceedings of the Seventh International ConferenceOn Language Resources and Evaluation (LREC).B.
Pang and L. Lee.
2008.
Opinion mining and senti-ment analysis.
Foundations and Trends in InformationRetrieval.Jonathon Read.
2005.
Using emoticons to reduce depen-dency in machine learning techniques for sentimentclassification.
In Proceedings of the ACL Student Re-search Workshop (ACL-2005).Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang, MingZhou, and Ping Li.
2011.
User-level sentiment anal-ysis incorporating social networks.
In Proceedingsof the 17th ACM SIGKDD Conference on KnowledgeDiscovery and Data Mining.H.
Tang, S. Tan, and X. Cheng.
2009.
A survey on senti-ment detection of reviews.
Expert Systems with Appli-cations.Mike Thelwall, Kevan Buckley, and Georgios Paltoglou.2011.
Sentiment in twitter events.
Journal of theAmerican Society for Information Science and Tech-nology, 62(2):406?418.M.
Tsytsarau and T. Palpanas.
2012.
Survey on miningsubjective data on the web.
Data Mining and Knowl-edge Discovery Journal, 24(3):478?514.Andranik Tumasjan, Timm O. Sprenger, Philipp G. Sand-ner, and Isabell M. Welpe.
2010.
Election forecastswith twitter: How 140 characters reflect the politicallandscape.
Social Science Computer Review.Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, AlanRitter, Sara Rosenthal, and Veselin Stoyanov.
2013.Semeval-2013 task 2: Sentiment analysis in twitter.In Proceedings of the 7th International Workshop onSemantic Evaluation.
Association for ComputationalLinguistics.394
