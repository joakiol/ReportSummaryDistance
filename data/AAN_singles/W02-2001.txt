Extracting the Unextractable: A Case Study on Verb-particlesTimothy Baldwin?
and Aline Villavicencio??
CSLI, Ventura Hall, Stanford UniversityStanford, CA 94305-4115 USAtbaldwin@csli.stanford.edu?
University of Cambridge, Computer Laboratory, William Gates BuildingJJ Thomson Avenue, Cambridge CB3 OFD, UKAline.Villavicencio@cl.cam.ac.ukAbstractThis paper proposes a series of techniques for ex-tracting English verb?particle constructions fromraw text corpora.
We initially propose three basicmethods, based on tagger output, chunker outputand a chunk grammar, respectively, with the chunkgrammar method optionally combining with an at-tachment resolution module to determine the syn-tactic structure of verb?preposition pairs in ambigu-ous constructs.
We then combine the three methodstogether into a single classifier, and add in a numberof extra lexical and frequentistic features, producinga final F-score of 0.865 over the WSJ.1 IntroductionThere is growing awareness of the pervasiveness andidiosyncrasy of multiword expressions (MWEs),and the need for a robust, structured handlingthereof (Sag et al, 2002; Calzolari et al, 2002;Copestake et al, 2002).
Examples of MWEs arelexically fixed expressions (e.g.
ad hoc), idioms (e.g.see double), light verb constructions (e.g.
make amistake) and institutionalised phrases (e.g.
kindleexcitement).MWEs pose a challenge to NLP due to their syn-tactic and semantic idiosyncrasies, which are oftenunpredictable from their component parts.
Large-scale manual annotation of MWEs is infeasible dueto their sheer volume (at least equivalent to the num-ber of simplex words (Jackendoff, 1997)), produc-tivity and domain-specificity.
Ideally, therefore, wewould like to have some means of automatically ex-tracting MWEs from a given domain or corpus, al-lowing us to pre-tune our grammar prior to deploy-ment.
It is this task of extraction that we target inthis paper.
This research represents a component ofthe LinGO multiword expression project,1 which istargeted at extracting, adequately handling and rep-resenting MWEs of all types.
As a research testbedand target resource to expand/domain-tune, we usethe LinGO English Resource Grammar (LinGO-ERG), a linguistically-precise HPSG-based gram-mar under development at CSLI (Copestake andFlickinger, 2000; Flickinger, 2000).The particular MWE type we target for extrac-tion is the English verb-particle construction.Verb-particle constructions (?VPCs?)
consist of a1http://lingo.stanford.edu/mwehead verb and one or more obligatory particles,in the form of intransitive prepositions (e.g.
handin), adjectives (e.g.
cut short) or verbs (e.g.
letgo) (Villavicencio and Copestake, 2002a; Villavicen-cio and Copestake, 2002b; Huddleston and Pullum,2002); for the purposes of this paper, we will fo-cus exclusively on prepositional particles?by far themost common and productive of the three types?and further restrict our attention to single-particleVPCs (i.e.
we ignore VPCs such as get alng to-gether).
We define VPCs to optionally select for anNP complement, i.e.
to occur both transitively (e.g.hand in the paper) and intransitively (e.g.
battle on).One aspect of VPCs that makes them difficult toextract (cited in, e.g., Smadja (1993)) is that theverb and particle can be non-contiguous, e.g.
handthe paper in and battle right on.
This sets them apartfrom conventional collocations and terminology (see,e.g., Manning and Schu?tze (1999) and McKeown andRadev (2000)) in that they cannot be captured ef-fectively using N-grams, due to the variability in thenumber and type of words potentially intercedingbetween the verb and particle.We are aiming for an extraction technique whichis applicable to any raw text corpus, allowing us totune grammars to novel domains.
Any linguisticannotation required during the extraction process,therefore, is produced through automatic means,and it is only for reasons of accessibility and compa-rability with other research that we choose to workover the Wall Street Journal section of the PennTreebank (Marcus et al, 1993).
That is, other thanin establishing upper bounds on the performance ofthe different extraction methods, we use only theraw text component of the treebank.In this paper, we first outline distinguishing fea-tures of VPCs relevant to the extraction process(?
2).
We then present and evaluate a number ofsimple methods for extracting VPCs based on, re-spectively, POS tagging (?
3), the output of a fulltext chunk parser (?
4), and a chunk grammar (?
5).Finally, we detail enhancements to the basic meth-ods (?
6) and give a brief description of related re-search (?
7) before concluding the paper (?
8).2 Distinguishing Features of VPCsHere, we review a number of features of VPCs per-tinent to the extraction task.
First, we describe lin-guistic qualities that characterise VPCs, and secondwe analyse the actual occurrence of VPCs in theWSJ.2.1 Linguistic featuresGiven an arbitrary verb?preposition pair, where thepreposition is governed by the verb, a number ofanalyses are possible.
If the preposition is intransi-tive, a VPC (either intransitive or transitive) results.If the preposition is transitive, it must select for anNP, producing either a prepositional verb (e.g.
re-fer to) or a free verb?preposition combination(e.g.
put it on the table, climb up the ladder).A number of diagnostics can be used to distinguishVPCs from both prepositional verbs and free verb?preposition combinations (Huddleston and Pullum,2002):1. transitive VPCs undergo the particle alterna-tion2.
with transitive VPCs, pronominal objects mustbe expressed in the ?split?
configuration3.
manner adverbs cannot occur between the verband particleThe first two diagnostics are restricted to transitiveVPCs, while the third applies to both intransitiveand transitive VPCs.The first diagnostic is the canonical test for par-ticlehood, and states that transitive VPCs take twoword orders: the joined configuration whereby theverb and particle are adjacent and the NP comple-ment follows the particle (e.g.
hand in the paper),and the split configuration whereby the NP com-plement occurs between the verb and particle (e.g.hand the paper in).
Note that prepositional verbsand free verb?preposition combinations can occuronly in the joined configuration (e.g.
refer to the bookvs.
*refer the book to).
Therefore, the existence ofa verb?preposition pair in the split configuration issufficient evidence for a VPC analysis.
It is impor-tant to realise that compatibility with the particlealternation is a sufficient but not necessary condi-tion on verb?particlehood.
That is, a small numberof VPCs do not readily occur in the split configu-ration, including carry out (a threat) (cf.
?carry athreat out).The second diagnostic stipulates that pronominalNPs can occur only in the split configuration (handit in vs. *hand in it).
Note also that heavy NPs tendto occur in the joined configuration, and that variousother factors interact to determine which configura-tion a given VPC in context will occur in (see, e.g.,Gries (2000)).The third diagnostic states that manner adverbscannot intercede between the verb and particle (e.g.
*hand quickly the paper in).
Note that this con-straint is restricted to manner adverbs, and thatthere is a small set of adverbs which can pre-modifyparticles and hence occur between the verb and par-ticle (e.g.
well in jump well up).2.2 Corpus occurrenceIn order to get a feel for the relative frequency ofVPCs in the corpus targeted for extraction, namely05101520253035400  10  20  30  40  50  60  70VPCtypes(%)Corpus frequencyFigure 1: Frequency distribution of VPCs in theWSJTagger correctextracted Prec Rec F?=1Brill 135135 1.000 0.177 0.301Penn 667800 0.834 0.565 0.673Table 1: POS-based extraction resultsthe WSJ section of the Penn Treebank, we took arandom sample of 200 VPCs from the Alvey Natu-ral Language Tools grammar (Grover et al, 1993)and did a manual corpus search for each.
In thecase that a VPC was found attested in the WSJ,we made a note of the frequency of occurrence as:(a) an intransitive VPC, (b) a transitive VPC in thejoined configuration, and (c) a transitive VPC in thesplit configuration.
Of the 200 VPCs, only 62 wereattested in the Wall Street Journal corpus (WSJ),at a mean token frequency of 5.1 and median to-ken frequency of 2 (frequencies totalled over all 3usages).
Figure 1 indicates the relative proportionof the 62 attested VPC types which occur with theindicated frequencies.
From this, it is apparent thattwo-thirds of VPCs occur at most three times in theoverall corpus, meaning that any extraction methodmust be able to handle extremely sparse data.Of the 62 attested VPCs, 29 have intransitive us-ages and 45 have transitive usages.
Of the 45 at-tested transitive VPCs, 12 occur in both the joinedand split configurations and can hence be unambigu-ously identified as VPCs based on the first diagnosticfrom above.
For the remaining 33 transitive VPCs,we have only the joined usage, and must find somealternate means of ruling out a prepositional verbor free verb?preposition combination analysis.
Notethat for the split VPCs, the mean number of wordsoccurring between the verb and particle was 1.6 andthe maximum 3.In the evaluation of the various extraction tech-niques below, recall is determined relative to thislimited set of 62 VPCs attested in the WSJ.
Thatis, recall is an indication of the proportion of the 62VPCs contained within the set of extracted VPCs.3 Method-1: Simple POS-basedExtractionOne obvious method for extracting VPCs is to run asimple regular expression over the output of a part-of-speech (POS) tagger, based on the observationthat the Penn Treebank POS tagset, e.g., contains adedicated particle tag (RP).
Given that all particlesare governed by a verb, extraction consists of simplylocating each particle and searching back (to the leftof the particle, as particles cannot be passivised orotherwise extraposed) for the head verb of the VPC.Here and for the subsequent methods, we assumethat the maximum word length for NP complementsin the split configuration for transitive VPCs is 5,2i.e.
that an NP ?heavier?
than this would occur morenaturally in the joined configuration.
We thus dis-count all particles which are more than 5 words fromtheir governing verb.
Additionally, we extracted aset of 73 canonical particles from the LinGO-ERG,and used this to filter out extraneous particles in thePOS data.In line with our assumption of raw text to extractover, we use the Brill tagger (Brill, 1995) to auto-matically tag the WSJ, rather than making use ofthe manual POS annotation provided in the PennTreebank.
We further lemmatise the data usingmorph (Minnen et al, 2001) and extract VPCs basedon the Brill tags.
This produces a total of 135 VPCs,which we evaluate according to the standard metricsof precision (Prec), recall (Rec) and F-score (F?=1).Note that here and for the remainder of this pa-per, precision is calculated according to the man-ual annotation for the combined total of 4,173 VPCcandidate types extracted by the various methodsdescribed in this paper, whereas recall is relative tothe 62 attested VPCs from the Alvey Tools data asdescribed above.As indicated in the first line of Table 1 (?Brill?
),the simple POS-based method results in a precisionof 1.000, recall of 0.177 and F-score of 0.301.In order to determine the upper bound on per-formance for this method, we ran the extractionmethod over the original tagging from the PennTreebank.
This resulted in an F-score of 0.774(?Penn?
in Table 1).
The primary reason for thelarge disparity between the Brill tagger output andoriginal Penn Treebank annotation is that it is no-toriously difficult to differentiate between particles,prepositions and adverbs (Toutanova and Manning,2000).
Over the WSJ, the Brill tagger achieves amodest tag recall of 0.103 for particles, and tag pre-cision of 0.838.
That is, it is highly conservative inallocating particle tags, to the extent that it recog-nises only two particle types for the whole of theWSJ: out and down.4 Method-2: Simple Chunk-basedExtractionTo overcome the shortcomings of the Brill taggerin identifying particles, we next look to full chunk2Note, this is the same as the maximum span length of 5used by Smadja (1993), and above the maximum attested NPlength of 3 from our corpus study (see Section 2.2).WSJ CoNLLPrec Rec F?=1 Prec Rec F?=10.889 0.911 0.900 0.912 0.925 0.919Table 2: Chunking performanceparsing.
Full chunk parsing involves partitioningup a text into syntactically-cohesive, head-final seg-ments (?chunks?
), without attempting to resolveinter-chunk dependencies.
In the chunk inventorydevised for the CoNLL-2000 test chunking sharedtask (Tjong Kim Sang and Buchholz, 2000), a ded-icated particle chunk type once again exists.
It istherefore possible to adopt an analogous approach tothat from Method-1, in identifying particle chunksthen working back to locate the verb each particlechunk is associated with.4.1 Chunk parsing methodIn order to chunk parse the WSJ, we first taggedthe full WSJ and Brown corpora using the Brill tag-ger, and then converted them into chunks based onthe original Penn Treebank parse trees, with theaid of the conversion script used in preparing theCoNLL-2000 shared task data.3 We next lemma-tised the data using morph (Minnen et al, 2000),and chunk parsed the WSJ with TiMBL 4.1 (Daele-mans et al, 2001) using the Brown corpus as train-ing data.
TiMBL is a memory-based classificationsystem based on the k-nearest neighbour algorithm,which takes as training data a set of fixed-lengthfeature vectors pre-classified according to an infor-mation field.
For each test instance described overthe same feature vector, it then returns the ?neigh-bours?
at the k-nearest distances to the test instanceand classifies the test instance according to the classdistribution over those neighbours.
TiMBL providespowerful functionality for determining the relativedistance between different values of a given featurein the form of MVDM, and also supports weightedvoting between neighbours in classifying inputs, e.g.in the form of inverse distance weighting.We ran TiMBL based on the feature set describedin Veenstra and van den Bosch (2000), that is usingthe 5 word lemmata and POS tags to the left and3 word lemmata and POS tags to the right of eachfocus word, along with the POS tag and lemma forthe focus word.
We set k to 5, ran MVDM over onlythe POS tags4 and used inverse distance weighting,but otherwise ran TiMBL with the default settings.We evaluated the basic TiMBL method over boththe full WSJ data, training on the Brown sectionof the Penn Treebank, and over the original sharedtask data from CoNLL-2000, the results for whichare presented in Table 2.
Note that, similarly tothe CoNLL-2000 shared task, precision, recall and3Note that the gold standard chunk data for the WSJ wasused only in evaluation of chunking performance, and to es-tablish upper bounds on the performance of the various ex-traction methods.4Based on the results of Veenstra and van den Bosch(2000) and the observation that MVDM is temperamentalover sparse data (i.e.
word lemmata).Chunker correctextracted Prec Rec F?=1TiMBL 695854 0.772 0.548 0.641Penn 651760 0.857 0.694 0.766Table 3: Chunk tag-based extraction resultsF-score are all evaluated at the chunk rather thanthe word level.
The F-score of 0.919 for the CoNLL-2000 data is roughly the median score attained bysystems performing in the original task, and slightlyhigher than the F-score of 0.915 reported by Veen-stra and van den Bosch (2000), due to the use ofword lemmata rather than surface forms, and alsoinverse distance weighting.
The reason for the drop-off in performance between the CoNLL data and thefull WSJ is due to the CoNLL training and test datacoming from a homogeneous data source, namely asubsection of the WSJ, but the Brown corpus beingused as the training data in chunking the full extentof the WSJ.4.2 Extraction methodHaving chunk-parsed the WSJ in the manner de-scribed above, we next set about extracting VPCs byidentifying each particle chunk, and searching backfor the governing verb.
As for Method-1, we allow amaximum of 5 words to intercede between a particleand its governing verb, and we apply the additionalstipulation that the only chunks that can occur be-tween the verb and the particle are: (a) noun chunks,(b) preposition chunks adjoining noun chunks, and(c) adverb chunks found in our closed set of particlepre-modifiers (see ?
2.1).
Additionally, we used thegold standard set of 73 particles to filter out extra-neous particle chunks, as for Method-1 above.The results for chunk-based extraction are pre-sented in Table 3, evaluated over the chunk parseroutput (?TiMBL?)
and also the gold-standard chunkdata for the WSJ (?Penn?).
These results are signifi-cantly better than those for Method-1 over the Brilloutput and Penn data, respectively, both in termsof the raw number of VPCs extracted and F-score.One reason for the relative success of extracting overchunker as compared to tagger output is that ourchunker was considerably more successful than theBrill tagger at annotating particles, returning an F-score of 0.737 over particle chunks (precision=0.786,recall=0.693).
The stipulations on particle type andwhat could occur between a verb and particle chunkwere crucial in maintaining a high VPC extractionprecision, relative to both particle chunk precisionand the gold standard extraction precision.
As canbe seen from the upper bound on recall (i.e.
recallover the gold standard chunk data), however, thismethod has limited applicability.5 Method-3: Chunk Grammar-basedExtractionThe principle weakness of Method-2 was recall, lead-ing us to implement a rule-based chunk sequencerwhich searches for particles in prepositional and ad-verbial chunks as well as particle chunks.
In essence,Method correctextracted Prec Rec F?=1Rule?att 6761119 0.604 0.694 0.646Timbl?att 615823 0.747 0.661 0.702Penn?att 694927 0.749 0.823 0.784Rule+att 9513126 0.304 0.823 0.444Timbl+att 7391049 0.704 0.710 0.707Penn+att 7501079 0.695 0.871 0.773Table 4: Chunk grammar-based extraction resultswe take each verb chunk in turn, and search to theright for a single-word particle, prepositional or ad-verbial chunk which is contained in the gold stan-dard set of 73 particles.
For each such chunk pair,it then analyses: (a) the chunks which occur be-tween them to ensure that, maximally, an NP andparticle pre-modifier adverb chunk are found; (b)the chunks that occur immediately after the parti-cle/preposition/adverb chunk to check for a clauseboundary or NP; and (c) the clause context of theverb chunk for possible extraposition of an NP ver-bal complement, through passivisation or relativisa-tion.
The objective of this analysis is to both deter-mine the valence of the VPC candidate (intransitiveor transitive) and identify evidence either support-ing or rejecting a VPC analysis.
Evidence for oragainst a VPC analysis is in the form of congruencewith the known linguistic properties of VPCs, as de-scribed in Section 2.1.
For example, if a pronominalnoun chunk were found to occur immediately afterthe (possibly) particle chunk (e.g.
*see off him), aVPC analysis would not be possible.
Alternatively,if a punctuation mark (e.g.
a full stop) were foundto occur immediately after the ?particle?
chunk andnothing interceded between the verb and particlechunk, then this would be evidence for an intran-sitive VPC analysis.The chunk sequencer is not able to furnish posi-tive or negative evidence for a VPC analysis in allcases.
Indeed, in a high proportion of instances, anoun chunk (=NP) was found to follow the ?parti-cle?
chunk, leading to ambiguity between analysis asa VPC, prepositional verb or free verb?prepositioncombination (see Section 2.1), or in the case thatan NP occurs between the verb and particle, the?particle?
being the head of a PP post-modifyingan NP.
As a case in point, the VP hand the paper inhere could take any of the following structures: (1)hand [the paper] [in] [here] (transitive VPC handin with adjunct NP here), (2) hand [the paper] [inhere] (transitive prepositional verb hand in or sim-ple transitive verb with PP adjunct), and (3) hand[the paper in here] (simple transitive verb).
In suchcases, we can choose to either (a) avoid committingourselves to any one analysis, and ignore all suchambiguous cases, or (b) use some means to resolvethe attachment ambiguity (i.e.
whether the NP isgoverned by the verb, resulting in a VPC, or thepreposition, resulting in a prepositional verb or freeverb?preposition combination).
In the latter case,we use an unsupervised attachment disambiguationmethod, based on the log-likelihood ratio (?LLR?,Dunning (1993)).
That is, we use the chunker outputto enumerate all the verb?preposition, preposition?noun and verb?noun bigrams in theWSJ data, basedon chunk heads rather than strict word bigrams.
Wethen use frequency data to pre-calculate the LLR foreach such type.
In the case that the verb and ?par-ticle?
are joined (i.e.
no NP occurs between them),we simply compare the LLR of the verb?noun andparticle?noun pairs, and assume a VPC analysis inthe case that the former is strictly larger than thelatter.
In the case that the verb and ?particle?
aresplit (i.e.
we have the chunk sequence VC NC1 PCNC2),5 we calculate three scores: (1) the productof the LLR for (the heads of) VC-PC and VC-NC2(analysis as VPC, with NC2 as an NP adjunct ofthe verb); (2) the product of the LLR for NC1-PCand PC-NC2 (transitive verb analysis, with the PPmodifying NC1); and (3) the product of the LLR forVC-PC and PC-NC2 (analysis as prepositional verb orfree verb?preposition combination).
Only in the casethat the first of these scores is strictly greater thanthe other two, do we favour a (transitive) VPC anal-ysis.Based on the positive and negative grammaticalevidence from above, for both intransitive and tran-sitive VPC analyses, we generate four frequency-based features.
The optional advent of data derivedthrough attachment resolution, again for both in-transitive and transitive VPC analyses, provides an-other two features.
These features can be combinedin either of two ways: (1) in a rule-based fashion,where a given verb?preposition pair is extracted outas a VPC only in the case that there is positive andno negative evidence for either an intransitive ortransitive VPC analysis (?Rule?
in Table 4); and(2) according to a classifier, using TiMBL to trainover the auto-chunked Brown data, with the samebasic settings as for chunking (with the exceptionthat each feature is numeric and MVDM is not used?
results presented as ?Timbl?
in Table 4).
We alsopresent upper bound results for the classifier-basedmethod using gold standard chunk data, rather thanthe chunker output (?Penn?).
For each of thesethree basic methods, we present results with andwithout the attachment-resolved data (??att?
).Based on the results in Table 4, the classifier-basedmethod (?Timbl?)
is superior to not only the rule-based method (?Rule?
), but also Method-1 andMethod-2.
While the rule-based method degradessignificantly when the attachment data is factoredin, the classifier-based method remains at the samebasic F-score value, undergoing a drop in precisionbut equivalent gain in recall and gaining more than120 correct VPCs in the process.
Rule+att returnsthe highest recall value of all the automatic meth-ods to date at 0.823, at the cost of low precision at0.304.
This points to the attachment disambigua-tion method having high recall but low precision.Timbl?att and Penn?att are equivalent in terms5Here, VC = verb chunk, NC = noun chunk and PC = (in-transitive or transitive) preposition chunk.Method correctextracted Prec Rec F?=1Combine 719953 0.754 0.710 0.731M?2 686778 0.882 0.677 0.766M3?att?
684788 0.868 0.694 0.771M3+att?
8711020 0.854 0.823 0.838Combine?
10001164 0.859 0.871 0.865Combine?Penn 9311047 0.889 0.903 0.896Table 5: Consolidated extraction resultsof precision, but the Penn data leads to considerablybetter recall.6 Improving on the Basic MethodsComparing the results for the three basic methods,it is apparent that Method-1 and Method-2 offerhigher precision while Method-3 offers higher recall.In order to capitalise on the respective strengths ofthe different methods, in this section, we investigatethe possibility of combining the outputs of the fourmethods into a single consolidated classifier.
Sys-tem combination is achieved by taking the union ofall VPC outputs from all systems, and having a vec-tor of frequency-based features for each, based onthe outputs of the different methods for the VPCin question.
For each of Method-1 and Method-2,a single feature is used describing the total numberof occurrences of the given VPC detected by thatmethod.
For Method-3, we retain the 6 features usedas input to Timbl?att, namely the frequency withwhich positive and negative evidence was detectedand also the frequency of VPCs detected through at-tachment resolution, for both intransitive and tran-sitive VPCs.
Training data comes from the outputof the different methods over the Brown corpus, andthe chunking data for Method-2 and Method-3 wasgenerated using the WSJ gold standard chunk dataas training data, analogously to the method used tochunk parse the WSJ.The result of this simple combination process ispresented in the first line of Table 5 (?Combine?
).Encouragingly, we achieved the exact same recallas the best of the simple methods (Timbl+att) at0.710, and significantly higher F-score than any in-dividual method at 0.731.Steeled by this initial success, we further augmentthe feature space with features describing the fre-quency of occurrence of: (a) the particle in the cor-pus, and (b) deverbal noun and adjective forms ofthe VPC in the corpus (e.g.
turnaround, dried-up),determined through a simple concatenation opera-tion optionally inserting a hyphen.
The first of theseis attempted to reflect the fact that high-frequencyparticles (e.g.
up, over) are more productive (i.e.are found in novel VPCs more readily) than low-frequency particles.6 The deverbal feature is in-tended to reflect the fact that VPCs have the po-6We also experimented with a similar feature describingverb frequency, but found it to either degrade or have noeffect on classifier performance.tential to undergo deverbalisation whereas prepo-sitional verbs and free verb?preposition combina-tions do not.7 We additionally added in featuresdescribing: (a) the number of letters in the verblemma, (b) the verb lemma, and (c) the particlelemma.
The first feature was intended to capturethe informal observation that shorter verbs tendto be more productive than longer verbs (whichoffers one possible explanation for the anomalouscall/ring/phone/*telephone up).
The second andthird features are intended to capture this same pro-ductivity effect, but on a individual word-level.
Notethat as TiMBL treats all features as fully indepen-dent, it is not able to directly pick up on the goldstandard verb?particle pairs in the training data toselect in the test data.The expanded set of features was used to re-evaluate each of: Method-2 (M?2 in Table 5); theclassifier version of Method-3 with and withoutattachment-resolved data (M3?ATT?
); and thesimple system combination method (Combine?
).Additionally, we calculated an upper bound for theexpanded feature set based on the gold standarddata for each of the methods (Combine?Penn in Ta-ble 5).
The results for these five consolidated meth-ods are presented in Table 5.The addition of the 7 new features leads to anappreciable gain in both precision and recall for allmethods, with the system combination method onceagain proving to be the best performer, at an F-scoreof 0.865.
The differential between the system com-bination method when trained over auto-generatedPOS and chunk data (Combine?)
and that trainedover gold standard data (Combine?Penn) is still tan-gible, but considerably less than for any of the in-dividual methods.
Importantly, Combine?
outper-forms the gold standard results for each of the in-dividual methods.
Examples of false positives (i.e.verb?prepositions misclassified as VPCs) returnedby this final system configuration are firm away, baseon and very off.In Section 1, we made the claim that VPCs arehighly productive and domain-specific.
We validatethis claim by comparing the 1000 VPCs correctlyextracted by the Combine?
method against boththe LinGO-ERG and the relatively broad-coverageAlvey Tools VPC inventory.
The 28 March, 2002version of the LinGO-ERG contains a total of 300intransitive and transitive VPC types, of which195 were contained in the 1000 correctly-extractedVPCs.
Feeding the remaining 805 VPCs into thegrammar (with a lexical type describing their tran-sitivity) would therefore result in an almost four-fold increase in the total number of VPCs, and in-crease the chances of the grammar being able toparse WSJ-style text.
The Alvey Tools data con-tains a total of 2254 VPC types.
Of the 1000 ex-tracted VPCs, 284 or slightly over 28%, were notcontained in the Alvey data, with examples includ-ing head down, blend together and bid up.
Combin-ing this result with that for the LinGO-ERG, one can7Note that only a limited number of VPCs can be dever-balised in this manner: of the 62 VPCs attested in the WSJ,only 8 had a deverbal usage.see that we are not simply extracting information al-ready at our fingertips, but are accessing significantnumbers of novel VPC types.7 Related researchThere is a moderate amount of research related tothe extraction of VPCs, or more generally phrasalverbs, which we briefly describe here.One of the earliest attempts at extracting ?in-terrupted collocations?
(i.e.
non-contiguous colloca-tions, including VPCs), was that of Smadja (1993).Smadja based his method on bigrams, but unlikeconventional collocation work, described bigrams byway of the triple of ?word1,word2,posn?, where posnis the number of words occurring between word 1 andword2 (up to 4).
For VPCs, we can reasonably ex-pect from 0 to 4 words to occur between the verband the particle, leading to 5 distinct variants ofthe same VPC and no motivated way of selectingbetween them.
Smadja did not attempt to evalu-ate his method other than anecdotally, making anycomparison with our research impossible.The work of Blaheta and Johnson (2001) is closerin its objectives to our research, in that it takes aparsed corpus and extracts out multiword verbs (i.e.VPCs and prepositional verbs) through the use oflog-linear models.
Once again, direct comparisonwith our results is difficult, as Blaheta and Johnsonoutput a ranked list of all verb?preposition pairs,and subjectively evaluate the quality of different sec-tions of the list.
Additionally, they make no attemptto distinguish VPCs from prepositional verbs.The method which is perhaps closest to ours isthat of Kaalep and Muischnek (2002) in extractingEstonian multiword verbs (which are similar to En-glish VPCs in that the components of the multiwordverb can be separated by other words).
Kaalep andMuischnek apply the ?mutual expectation?
test overa range of ?positioned bigrams?, similar to thoseused by Smadja.
They test their method over threedifferent corpora, with results ranging from a preci-sion of 0.21 and recall of 0.86 (F-score=0.34) for thesmallest corpus, to a precision of 0.03 and recall of0.85 (F-score=0.06) for the largest corpus.
That is,high levels of noise are evident in the system output,and the F-score values are well below those achievedby our method for English VPCs.8 ConclusionIn conclusion, this paper has been concerned withthe extraction of English verb?particle construc-tions from raw text corpora.
Three basic meth-ods were proposed, based on tagger output, chunkeroutput and a chunk grammar; the chunk grammarmethod was optionally combined with attachmentresolution to determine the syntactic structure ofverb?preposition pairs in ambiguous constructs.
Wethen experimented with combining the output of thethree methods together into a single classifier, andfurther complemented the feature space with a num-ber of lexical and frequentistic features, culminatingin an F-score of 0.865 over the WSJ.It is relatively simple to adapt the meth-ods described here to output subcategorisationtypes, rather than a binary judgement on verb?particlehood.
This would allow the extracted out-put to be fed directly into the LinGO-ERG for usein parsing.
We are also interested in extendingthe method to extract prepositional verbs, manyof which appear in the attachment resolution dataand are subsequently filtered out by the consolidatedclassifier.AcknowledgementsThis research was supported in part by NSF grantBCS-0094638 and also the Research Collaborationbetween NTT Communication Science Laboratories,Nippon Telegraph and Telephone Corporation andCSLI, Stanford University.
We would like to thankFrancis Bond, Ann Copestake, Dan Flickinger, Di-ana McCarthy and the three anonymous reviewersfor their valuable input on this research.ReferencesDon Blaheta and Mark Johnson.
2001.
Unsuper-vised learning of multi-word verbs.
In Proc.
ofthe ACL/EACL 2001 Workshop on the Compu-tational Extraction, Analysis and Exploitation ofCollocations, pages 54?60.Eric Brill.
1995.
Transformation-based error-drivenlearning and natural language processing: A casestudy in part-of-speech tagging.
ComputationalLinguistics, 21:543?65.Nicoletta Calzolari, Charles J. Fillmore, Ralph Gr-ishman, Nancy Ide, Alessandro Lenci, CatherineMacLeod, and Antonio Zampolli.
2002.
Towardsbest practice for multiword expressions in compu-tational lexicons.
In Proc.
of the 3rd InternationalConference on Language Resources and Evalua-tion (LREC 2002), pages 1934?40.Ann Copestake and Dan Flickinger.
2000.
Opensource grammar development environment andbroad-coverage English grammar using HPSG.
InProc.
of the 2nd International Conference on Lan-guage Resources and Evaluation (LREC 2000),pages 591?8.Ann Copestake, Fabre Lambeau, Aline Villavicen-cio, Francis Bond, Timothy Baldwin, Ivan Sag,and Dan Flickinger.
2002.
Multiword expres-sions: linguistic precision and reusability.
In Proc.of the 3rd International Conference on LanguageResources and Evaluation (LREC 2002), pages1941?7.Walter Daelemans, Jakub Zavrel, Ko van der Sloot,and Antal van den Bosch.
2001.
TiMBL: TilburgMemory Based Learner, version 4.1, referenceguide.
ILK technical report 01-04.Ted Dunning.
1993.
Accurate methods for thestatistics of surprise and coincidence.
Computa-tional Linguistics, 19(1):61?74.Dan Flickinger.
2000.
On building a more efficientgrammar by exploiting types.
Natural LanguageEngineering, 6(1):15?28.Stefan T. Gries.
2000.
Towards multifactorial anal-yses of syntactic variation: The case of particleplacement.
Ph.D. thesis, University of Hamburg.Claire Grover, John Carroll, and Edward Briscoe.1993.
The Alvey Natural Language Tools gram-mar (4th release).
Technical Report 284, Com-puter Laboratory, Cambridge University, UK.Rodney Huddleston and Geoffrey K. Pullum.
2002.The Cambridge Grammar of the English Lan-guage.
Cambridge: Cambridge University Press.Ray Jackendoff.
1997.
The Architecture of the Lan-guage Faculty.
Cambridge, MA: MIT Press.Heiki-Jaan Kaalep and Kadri Muischnek.
2002.
Us-ing the text corpus to create a comprehensive listof phrasal verbs.
In Proc.
of the 3rd InternationalConference on Language Resources and Evalua-tion (LREC 2002), pages 101?5.Christopher D. Manning and Hinrich Schu?tze.
1999.Foundations of Statistical Natural Language Pro-cessing.
MIT Press.Mitchell P. Marcus, Beatrice Santorini, and MaryAnn Marcinkiewicz.
1993.
Building a large an-notated corpus of English: the Penn Treebank.Computational Linguistics, 19(2):313?30.Kathleen R. McKeown and Dragomir R. Radev.2000.
Collocations.
In Robert Dale, HermannMoisl, and Harold Somers, editors, Handbook ofNatural Language Processing, chapter 21.
MarcelDekker.Guido Minnen, John Carroll, and Darren Pearce.2000.
Robust, applied morphological generation.In Proc.
of the First International Natural Lan-guage Generation Conference (INLG), pages 201?8.Guido Minnen, John Carroll, and Darren Pearce.2001.
Applied morphological processing of En-glish.
7(3).Ivan Sag, Timothy Baldwin, Francis Bond, AnnCopestake, and Dan Flickinger.
2002.
Multiwordexpressions: A pain in the neck for NLP.
InProc.
of the 3rd International Conference on In-telligent Text Processing and Computational Lin-guistics (CICLing-2002), pages 1?15.Frank Smadja.
1993.
Retrieving collocationsfrom text: Xtract.
Computational Linguistics,19(1):143?78.Erik F. Tjong Kim Sang and Sabine Buchholz.2000.
Introduction to the CoNLL-2000 sharedtask: Chunking.
In Proc.
of the 4th Confer-ence on Computational Natural Language Learn-ing (CoNLL-2000), pages 127?132.Kristina Toutanova and Christopher D. Manning.2000.
Enriching the knowledge sources used in amaximum entropy part-of-speech tagger.
In Proc.of the Joint SIGDAT Conference on EmpiricalMethods in Natural Language Processing and VeryLarge Corpora (EMNLP/VLC-2000).Jorn Veenstra and Antal van den Bosch.
2000.Single-classifier memory-based phrase chunking.In Proc.
of the 4th Conference on ComputationalNatural Language Learning (CoNLL-2000), pages157?9.Aline Villavicencio and Ann Copestake.
2002a.Phrasal verbs and the LinGO-ERG.
LinGOWorking Paper No.
2002-01.Aline Villavicencio and Ann Copestake.
2002b.Verb-particle constructions in a computationalgrammar.
In Proc.
of the 9th International Con-ference on Head-Driven Phrase Structure Gram-mar (HPSG-2002).
