Methods and Practical Issues in Evaluating Alignment TechniquesPh i l ippe  Lang la i sCTT/KTH SE-I0044 StockholmCERI-LIA, AGROPARC BP 1228F-84911 Avignon Cedex 9Philippe.Langlais~speech.kth.seMiche l  S imardRALI-DIROUniv.
de MontrdalQudbec, Canada H3C 3J7shnardm~IRO.UMontreal.CAJ ean  Vdron isLPL, Univ.
de Provence29, Av.
R. SchumanF-13621 Aix-en-Provence Cedex 1veronis~univ-aix.frAbstractThis paper describes the work achieved in thefirst half of a 4-year cooperative research project(ARCADE) ,  financed by AUPELF-UREF .
Theproject is devoted to the evaluation of paral-lel text alignment techniques.
In its first periodARCADE ran a competition between six sys-tems on a sentence-to-sentence alignment taskwhich yielded two main types of results.
First,a large reference bilingual corpus comprising oftexts of different genres was created, each pre-senting various degrees of difficulty with respectto the alignment task.Second, significant methodological progresswas made both on the evaluation protocols andmetrics, and the algoritbm.q used by the dif-ferent systems.
For the second phase, which isnow underway, ARCADE has been opened toa larger number of teams who will tackle theproblem of word-level alignment.1 Introduct ionIn the last few years, there has been a growinginterest in parallel text alignment techniques.These techniques attempt to map various tex-tual units to their translation and have provenuseful for a wide range of applicatious and tools.A simple example of such a tool is probablythe TransSearch bilingual concordancing system(Isabelle et al, 1993), which allows a user toquery a large archive of existing translations inorder to find ready-made solutions to specifictranslation problems.
Such a tool has proved ex-tremely useful not only for translators, but alsofor bilingual lexicographers (Langlois, 1996) andterminologists (Dagan and Church, 1994).
Moresophisticated applications based on alignmenttechnology have also been the object of recentwork, such as the automatic building of bilin-gual lexical resources (Melamed, 1996; Klavansand Tzoukermann, 1995), the automatic verifi-cation of translations (Macklovitch, 1995), theautomatic dictation of translations (Brousseauet al, 1995) and even interactive machine trans-lation (Foster et al, 1997).Enthusiasm for this relatively new field wassparked early on by the apparent demonstra-tion that very simple techniques could yield al-most perfect results.
For instance, to producesentence alignments, Brown et al (1991) andGale and Church (1991) both proposed meth-ods that completely ignored the lexical contentof the texts and both reported accuracy lev-els exceeding 98%.
Unfortunately performancetends to deteriorate significantly when alignersare applied to corpora which are widely differ-ent from the training corpus, and/or where thealignments are not straightforward.
For instancegraphics, tables, "floating" notes and missingsegments, which are very common in real texts,all result in a dramatic loss of efficiency.The truth is that, while text alignment ismostly an easy problem, especially when consid-ered at the sentence level, there are situationswhere even humans have a hard time makingthe right decision.
In fact, it could be arguedthat, ultimately, text alignment isno easier thanthe more general problem of natural anguageunderstanding.In addition, most research efforts weredirected towards the easiest problem, that ofsentence-to-sentence alignment (Brown et al,1991; Gale and Church, 1991; Debili, 1992;Kay and l~scheisen, 1993; Simard et al, 1992;Simard and Plamondon, 1996).
Alignment atthe word and term level, which is extremelyuseful for applications uch as lexieal resourceextraction, is still a largely unexplored researcharea(Melamed, 1997).In order to live up to the expectations of the711various application fields, alignment technologywill therefore have to improve substantially.As was the case with several other languageprocessing techniques (such as informationretrieval, document understanding or speechrecognition), it is likely that a systematic evalu-ation will enable such improvements.
However,before the ARCADE project started, no for-real evaluation exercise was underway; andworse still, there was no multilingnal alignedreference corpus to serve as a "gold standard"(as the Brown corpus did, for example, forpart of speech tagging), nor any establishedmethodology for the evaluation of alignmentsystems.2 Organ izat ionARCADE is an evaluation exercise financedby AUPELF-UREF, a network of (at leastpartially) French-speaking universities.
It waslaunched in 1995 to promote research in thefield of multilingual alignment.
The first 2-yearperiod (96-97) was dedicated to two maintasks: 1) producing a reference bilingual corpus(French-English) aligned at sentence level; 2)evaluating several sentence alignment systemsthrough an ARPA-like competition.In the first phase of ARCADE, two types ofteams were involved in the project: the corpusproviders (LPL and RALI) and the (RALI, LO-ILIA, ISSCO, IRMC and LIA).
General coor-dination was handled by J. V~ronis (LPL); adiscussion group was set up and moderated byPh.
Langlais (LIA & KTH).3 Reference  corpusOne of the main results of ARCADE has beento produce an aligned French-English corpus,combining texts of different genres and variousdegrees of difficulty for the alignment ask.
Itis important o mention that until ARCADE,most alignment systems had been tested on ju-dicial and technical texts which present rela-tively few difficulties for a sentence-level align-ment.
Therefore, diversity in the nature of thetexts was preferred to the collection of a largequantity of similar data.3.1 FormatARCADE contributed to the developmentand testing of the Corpus Encoding Standard(CES), which was initiated during the MUL-TEXT project (Ide et al, 1995).
The CES isbased on SGML and it is an extension of thenow internationally-accepted recommendationsof the Text Encoding Initiative (Ide andVdronis, 1995).
Both the JOG and BAF partsof the ARCADE corpus (described below) areencoded in CES format.3:2 JOCThe JOC corpus contains texts which were pub-lished in 1993 as a section of the C Series of theOfficial Journal of the European Community inall of its official languages.
This corpus, whichwas collected and prepared uring the MLCCand MULTEXT projects, contains, in 9 parallelversions, questions asked by members of the Eu-ropean Parliament on a variety of topics and thecorresponding answers from the European Com-mission.
JOC contains approximately 10 millionwords (ca.
1.1 million words per language).
Thepart used for JOC was composed of one fifthof the French and English sections (ca.
200 000words per language).3.3 BAFThe BAF corpus is also a set of parallel French-English texts of about 400 000 words per lan-guage.
It includes four text genres: 1) INST,four institutional texts (including transcriptionof speech from the Hansard corpus) for a total-ing close to 300 000 words per language, 2) SCI-ENCE,  five scientific articles of about 50 000words per language, 3) TECH,  technical doc-umentation of about 40 000 words per languageand 4) VERNE,  the Jules Verne novel: "Dela terre d la lune" (ca.
50 000 words per lan-guage).
This last text is very interesting becausethe translation of literary texts is much freerthan that of other types of tests.
Furthermore,the English version is slightly abridged, whichadds the problem of detecting missing segments.The BAF corpus is described in greater detailin (Simard, 1998).4 Eva luat ion  measuresWe first propose a formal definition of paral-lel text alignment, as defined in (Isabelle andSimard, 1996).
Based on that definition, theusual notions of recall and precision can be usedto evaluate the quality of a given alignment with712respect to a reference.
However, recall and preci-sion can be computed for various levels of gran-ularity: an alignment at a given level (e.g.
sen-tences) can be measured in terms of units of alower level (e.g.
words, characters).
Such a fine-grained measure is less sensitive to segmenta-tion problems, and can be used to weight errorsaccording to the number of sub-units they span.4.1 Formal  def in i t ionIf we consider a text S and its translation T astwo sets of segments S = {Sl, s2, .., Sn} and T ={t l , t2, .
.
.
, tm}, an alignment A between S andT can be defined as a subset of the Cartesianproduct ~(S) x p(T), where p(S) and p(T) arerespectively the set of all subsets of S and T.The triple iS, T, A) will be called a bitext.
Eachof the elements (ordered pairs) of the alignmentwill be called a bisegment.This definition is fairly general.
However, inthe evaluation exercice described here, segmentswere sentences and were supposed to be contigu-ous, yielding monotonic alignments.For instance, let us consider the fol-lowing alignment, which will serve as thereference alignment in the subsequent ex-amples.
The formal representation of it is:Ar = {({Sl}, {tl}), ({s2}, {t2,t3})}.sl Phrase num~ro un.s2 Phrase num~ro deuxqui ressemble h la l~re.tl The first sentence.t2 The 2nd sentence.t3 It looks like the first.4.2 Recall and precisionLet us consider a bitext (S,T,  Ar) and aproposed alignment A.
The alignment recallwith respect to the reference Ar is definedas: recall = IA N Arl/IA~I.
It represents theproportion of bisegments in A that are correctwith respect to the reference At.
The silencecorresponds to 1 -  recall.
The alignmentprecision with respect to the reference Aris defined as: precision = IA N Arl/IAI.
Itrepresents the proportion of bisegments in Athat are right with respect to the number ofbisegment proposed.
The noise corresponds to1 -- precision.We will also use the F-measure (Rijsbergen,1979) which combines recall and precision ina single efficiency measure (harmonic mean ofprecision and recall):(recall x precision) F2" ( recall~ + precision)"Let us assume the following proposed align-ment:sl Phrase num~ro un.
tl The first sentence.t2 The 2nd sentence.s2 Phrase num~ro deux t3 It looks like the first.qui ressemble h la l~re.The formal representation f this alignmentis: A = {({s,}, ({}, {t2}), ({s2}, {t3})}.We note that: A n Ar = {((s,}, {tl})}.
Align-ment recall and precision with respect o Ar are1/2 -- 0.50 and 1/3 -- 0.33 respectively.
The F-measure is 0.40.Improving both recall and precision are an-tagonistic goals : efforts to improve one oftenresult in degrading the other.
Depending on theapplications, different rade-offs can be sought.For example, if the bisegments are used to auto-matically generate a bilingual dictionary, maxi-mizing precision (i.e.
omitting doubtful couples)is likely to be the preferred option.Recall and precision as defined above arerather unforgiving.
They do not take into ac-count the fact that some bisegments could bepartially correct.
In the previous example, thebisegment ({s2}, {t3}) does not belong to thereference, but can be considered as partially cor-rect: t3 does match a part of s2.
To take partialcorrectness into account, we need to compute re-call and precision at the sentence level insteadof the alignment level.Assuming the alignment A = {al, a2, .
.
.
,  am}and the reference Ar = {arl, at2 , .
.
.
,  am}, withai = (asi, ati) and arj = (arsj ,art j ) ,  we canderive the following sentence-to-sentence align-ments:A' = Ui(asi ?
ati)A~r = Uj(arsj  x artj)Sentence-level recall and precision can thusbe defined in the following way:recall = IA' ' ' nAr l / lA r lprecision = IA' n A'rl/IA'IIn the example above: A' = {(sl, tl), (s2, t3)}and A~ = {(sl, tl), (s2, t2), (s2, t3)}.
Sentence-level recall and precision for this example are713therefore 2/3 = 0.66 and 1 respectively, as com-pared to the alignment-level recall and preci-sion, 0.50 and 0.33 respectively.
The F-measurebecomes 0.80 instead of 0.40.4.3 Granu lar i tyIn the definitions above, the sentence is the unitof granularity used for the computation of recalland precision at both levels.
This results in twodifficulties.
First, the measures are very sensi-tive to sentence segmentation errors.
Secondly,they do not reflect the seriousness of misalign-ments.
It seems reasonable that errors involvingshort sentences hould be less penalized thanerrors involving longer ones, at least from theperspective of some applications.These problems can be avoided by taking ad-vantage of the fact that a unit of a given gran-~arity (e.g.
sentence) can always be seen asa (possibly discontinuous) equence of units offiner granularity (e.g.
character).Thus, when an alignment A is compared toa reference alignment Ar using the recall andprecision measures computed at the char-level,the values obtained are inversely proportional tothe quantity of text (i.e.
number of characters)in the misaligned sentences, instead of the num-ber of these misaligned sentences.
For instance,in the example used above, we would have atsentence level:* using word granularity (punctuation marksare considered as words) :IA ' I  = 4*4  + 0*4  + 9*6  = 106IAr ' l  = 4*4  + 9 .10  = 70IAr '  " A' I  = 4*4  + 9*6  = 70reca l l  = 70 /106  = 0 .66prec is ion  = 1F = 0 .80?
using character granularity (excludingspaces):\ [A ' \ [  = 15 .17  + 0 .15  + 36*20  = 975\ [Ar ' \ ]  = 15 .17  + 36*35  = 1515IAr '  " A ' I  = 15 .17  + 36*20  = 975reca l l  = 975/1515 = 0 .64prec i s ion  = 1F=0.785 Sys tems tes tedSix systems were tested, two of which havingbeen submitted by the I:tALI.RAL I / Jaca l  This system uses as a first stepa program that reduces the search space only tothose sentence pairs that are potentially inter-esting (Simard and Plamondon, 1996).
The un-derlying principle is the automatic detection ofisolated cognates (i.e.
for which no other similarword exists in a window of given size).
Once thesearch space is reduced, the system aligns thesentences using the well-known sentence-lengthmodel described in (Gale and Church, 1991).RAL I /Sa l lgn  The second method proposedby RALI is based on a dynamic programmingscheme which uses a score function derived froma translation model similar to that of (Brownet al, 1990).
The search space is reduced to abeam of fixed width around the diagonal (whichwould represent the alignment if the two textswere perfectly synchronized).LORIA  The strategy adopted in this systemdiffers from that of the other systems ince sen-tence alignment is performed after the prelim-inary alignment of larger units (whenever pos-sible, using mark-up), such as paragraphs anddivisions, on the basis of the SGML structure.A dynamic programming scheme is applied toall alignment levels in successive steps.IRMC This system involves a preliminary,rough word alignment step which uses a trans-fer dictionary and a measure of the proximity ofwords (D~bili et al, 1994).
Sentence alignmentis then achieved by an algorithm which opti-mizes several criteria such as word-order con-servation and synchronization between the twotexts.L IA  Like Jacal, the LIA system uses apre-processing step involving cognate recog-nition which restricts the search space, butin a less restrictive way.
Sentence alignmentis then achieved through dynamic program-ming, using a score function which combinessentence length, cognates, transfer dictionaryand frequency of translation schemes (1-1, 1-2,etc.
).ISSCO Like the LORIA system, the ISSCOaligner is sensitive to the macro-structure ofthe document.
It examines the tree structureof an SGML document in a first pass, weightingeach node according to the number of charac-ters contained within the subtree rooted at thatnode.
The second pass descends the tree, first714by depth, then by breath, while aligning sen-tences using a method resembling that of Gale& Church.6 ResultsFour sets of recall/precision measures were com-puted for the alignments achieved by the sixsystems for each text type previously describedabove: Al ign, alignment-level, Sent  sentence-level, Word,  word-level and Char ,  character-level.
The global efficiency of the different sys-tems (average F-values) for each text type isgiven in Figure 1.t.ORJA t !
I " ?
.
W ~  - I I  -I , IA : .
.
\ [ .
J !
i 1 ?
!
, TECJH, .~  i i i d~ i i i .
i _~?~' r  i?
i "?
?-~,  : ~m.
:LIA ~ i ~b-~ ~ i : !
: IX , '~  \] SC~'~ ! '
~ .
!
!
i !
~ !
~ .."" .
SCH~CI!
!
?
~ s.,4~.~ !
!
.
iia~o ~ : T ' ,  , "~?
,,-IH i ii iii i i i ii .
o l l .
l~ l !~ l  i i i i i i i i  !
i i ii i  i i ~ i i i i i i i  i i i i ~,.t- 'I ~ i i i .
.
'~ .
.
.
, ,~ .1~'ii'  !
i i i!
ii , "i iI.., ~ i  i i 1o<r .
, i  I l l  ~ i I s  lFigure h Global efficiency (average F-values forAl ign, Sent,  Word  and Char  measures) of thedifferent systems (Jacal, Salign, LORIA, IRMC,ISSCO, LIA),  by text type (logarithmic scale).First, note than the Char measures are higherthat the Align measures.
This seems to con-firm that systems tend to fail when dealingwith shorter sentences.
In addition, the refer-ence alignment for the BAF corpus combinesseveral 1-1 alignments in a single n-n align-ment, for practical reasons owing to the sen-tence segmentation process.
This results in de-creased Align measures.The corpus on which all systems cored high-est was the JOC.
This corpus is relatively sim-ple to align, since it contains 94% of 1-1 align-ments, reflecting a translation strategy basedon speed and absolute fidelity.
In addition, thiscorpus contains a large amount of data thatremains unchanged uring the translation pro-cess (proper names, dates, etc.)
and which canserve as anchor points by some systems.
Notethat the LORIA system achieves a slightly bet-ter performance than the others on this cor-pus, mainly because it is able to carry out astructure-alignment since paragraphs and divi-sions are explicitly marked.The worst results were achieved on theVERNE corpus.
This is also the corpus forwhich the results showed the most scatteringacross systems (22% to 90% char-precision).These poor results are linked to the literarynature of the corpus, where translation is freerand more interpretative.
In addition, since theEnglish version is slightly abridged, the occa-sional omissions result in de-synchronizationin most systems.
Nevertheless, the LIA sys-tem still achieves a satisfactory performance(90% char-recall and 94% char-precision),which can be explained by the efficiency of itsword-based pre-alignment s ep, as well as thescoring function used to rank the candidatebisegments.Significant discrepancy are also noted be-tween the Align and Char recalls on the TECHcorpus.
This document contained a largeglossary as an appendix, and since the termsare sorted in alphabetic order, they are ordereddifferently in each language.
This portion oftext was not manually aligned in the reference.The size of this bisegment (250-250) drasticallylowers the Char-recall.
Aligning two glossariescan be seen as a document-structure alignmenttask rather than a sentence-alignment task.Since the goal of the evaluation was sentencealignment, the TECH corpus results were nottaken into account in the final grading of thesystems.The overall ranking for all systems (excludingthe TECH corpus results) is given in Figure 2,in terms of the Sent and Char F-measures.
TheLIA system obtains the best average results andshows good stability across texts, which is an715g0L IA  JACALI A l l sn  ~ Chars~8 Sent  ~ Wm-dSAL IGN LORIA  LSSCO \]\[R~lCFigure 2: Final r~nking on the systems (averageF-vaiues).important criterion for many applications.7 Conc lus ion  and  fu ture  workThe ARCADE evaluation exercise has allowedfor significant methodological progress on paral-lel text alignment.
The discussions among par-ticipants on the question of a testing proto-col resulted in the definition of several evalu-ation measures and an assessment of their rela-tive merits.
The comparative study of the sys-tems performance also yielded a better under-standing of the various techniques involved.
Asa significant spin-off, the project has produceda large aligned bilingual corpus, composed ofseveral types of texts, which can be used as agold standard for future evaluation.
Groundedon the experience gained in the first test cam-paign, the second (1998-1999) has been openedto more te~m.q and plans to tackle more difficultproblems, such as word-level alignment.
1AcknowledgmentsThis work has been partially funded byAUPELF-UREF.
We are indebted to LucieLanglois and EUiott Macklovitch for theirfruitful comments on this paper.ReferencesJ.
Brousseau, C. Drouin, G. Foster, P. IsabeUe,R.
Kuhn, Y. Normandin, and P. Platoon-don.
1995.
French Speech Recognition in anAutomatic Dictation System for Translators:the TransTalk Project.
In Proceedings o-f Eu-rospeech 95, Madrid, Spain.1For more information check the Web site athttp: \] \] www.lp l. univ-a~.fr \]pro jects \]arcadeP.
F. Brown, J. Cocke, S. A. Della Pietra,V.
J. Della Pietra, F. Jelinek, J. D. Lafferty,R.
L. Mercer, and P. S. Roosin.
1990.
A Sta-tistical Approach to Machine Translation.
InComputational Linguistics, volume 16, pages79-85, June.P.F.
Brown, J.C. Lai, and R.L.
Mercer.
1991.Aligning Sentences in Parallel Corpora.
In~9th Annual Meeting o-f the Association forComputational Linguistics, pages 169-176,?Berkeley, CA,USA.Ido Dagan and Kenneth W. Church.
1994.
Ter-might: Identifying and Translating Techni-cal Terminology.
InProceedings ofANLP-94,Stuttgart, Germany.?
F. D~bili, E. Sammouda, and A. Zribi.
1994.
Del'appariement des roots ~ la comparaison dephrases.
In 9~me Congr~s de Reconnaissancedes Formes et Intelligence Artificielle, Paris,Janvier.F.
Debili.
1992.
Aligning Sentences in BilingualTexts French - English and French - Arabic.In COLING, pages 517-525, Nantes, 23-28Aout.George Foster, Pierre Isabelle, and Pierre Pla-mondon.
1997.
Target-Text Mediated Inter-active Machine Translation.
Machine Trans-lation, 21(1-2).W.
A. Gale and Kenneth W. Church.
1991.A Program for Aligning Sentences in Bilin-gual Corpora.
In 29th Annual Meeting ofthe Association -for Computational Linguis-tics, Berkeley, CA.N.
Ide and J. V~ronis, 1995.
The Text Encod-ing Initiative: background and context, chap-ter 342p.
Kluwer Academic Publishers, Dor-drecht.N.
Ide, G. Priest-Dorman, and J. V6ronis.1995.
Corpus encoding standard.Report.
Accessible on the WorldWide Web: http://www.lpl, univ-aix.fr/projects/multext/CES/CES 1.html.Pierre IsabeUe and Michel Simard.1996.
Propositions pour larepresentation et l'~valuation desalignements de textes parall~les.http ://www-ral i. iro.
umontreal, ca/arc-a2/-PropEval.Pierre Isabelle, Marc Dymetman, George Fos-ter, Jean-Marc Jutras, Elliott Macklovitch,Franqois Perrault, Xiaobo Ren, and Michel716Simard.
1993.
Translation Analysis andTranslation Automation.
In Proceedings ofTMI-93, Kyoto, Japan.M.
Kay and M. PdSscheisen.
1993.
Text-translation alignment.
Computational Lin-guistics, 19(1):121-142.Judith Klavans and Evelyne Tzoukermama.1995.
Combining Corpus and Machine-readable Dictionary Data for Building Bilin-gual Lexicons.
Machine Translation, 10(3).Lueie Langlois.
1996.
Bilingual Concordances:A New Tool for Bilingual Lexicographers.
InProceedings of AMTA-96, Montreal, Canada.Elliott Maekloviteh.
1995.
TransCheek - -  orthe Automatic Validation of Human Trans-lations.
In Proceedings of the MT Summit V,Luxembourg.I.
Dan Melamed.
1996.
Automatic Con-struetion of Clean Broa~l-eoverage Transla-tion Lexicons.
In Proceedings of AMTA-96,Montreal, Canada.I.
Dan Melamed.
1997.
A portable algorithmfor mapping bitext correspondence.
In 35thConference of the Association for Computa-tional Linguistics, Madrid, Spain.C.J.
Van Rijsbergen.
1979.
Information Re-trieval,2nd edition, London, Butterworths.M.
Simard and P. Plamondon.
1996.
Bilingualsentence alignment: Balancing robustness andaecura~zy.
In Proceedings of the Second Con-ference of the Association for Machine Trans-lation in the Americas (AMTA), Montreal,Quebec.M.
Simard, G.F. Foster, and P. IsabeUe.
1992.Using Cognates to Align Sentences in Bilin-gual Corpora.
In Fourth International Con-ference on Theoretical and Methodological Is-sues in Machine Translation (TM1), pages67-81, Montr6al, Canada.M.
Simard.
1998.
The BAF: A corpus ofEnglish-French Bitext.
In First InternationalConference on Language Resources and Eval-uation, Granada, Spain.717
