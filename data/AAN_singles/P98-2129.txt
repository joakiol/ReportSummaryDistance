Evaluating Response Strategies in a Web-Based Spoken Dialogue AgentDiane J. LitmanAT&T Labs - Research180 Park AvenueFlorham Park, NJ 07932 USAdiane @ research, att.comShimei PanComputer Science DepartmentColumbia UniversityNew York, NY 10027 USApan @ cs.columbia.eduMari lyn A. WalkerAT&T Labs - Research180 Park AvenueFlorham Park, NJ 07932 USAwalker @ research, att.comAbstractWhile the notion of a cooperative r sponse has beenthe focus of considerable research in natural lan-guage dialogue systems, there has been little empir-ical work demonstrating how such responses leadto more efficient, natural, or successful dialogues.This paper presents an experimental evaluation oftwo alternative response strategies in TOOT, a spo-ken dialogue agent hat allows users to access trainschedules stored on the web via a telephone conver-sation.
We compare the performance of two ver-sions of TOOT (literal and cooperative), by hav-ing users carry out a set of tasks with each ver-sion.
By using hypothesis testing methods, we showthat a combination of response strategy, applicationtask, and task/strategy interactions account for var-ious types of performance differences.
By usingthe PARADISE evaluation framework to estimatean overall performance function, we identify inter-dependencies that exist between speech recognitionand response strategy.
Our results elaborate the con-ditions under which TOOT' s cooperative rather thanliteral strategy contributes to greater performance.1 IntroductionThe notion of a cooperative response has been thefocus of considerable research in natural anguageand spoken dialogue systems (Allen and Perrault,1980; Mays, 1980; Kaplan, 1981; Joshi et al, 1984;McCoy, 1989; Pao and Wilpon, 1992; Moore, 1994;Seneff et al, 1995; Goddeau et al, 1996; Pierac-cini et al, 1997).
However, despite the existenceof many algorithms for generating cooperative re-sponses, there has been little empirical work ad-dressing the evaluation of such algorithms in thecontext of real-time natural anguage dialogue sys-tems with human users.
Thus it is unclear un-der what conditions cooperative responses result inmore efficient or efficacious dialogues.This paper presents an empirical evaluationof two alternative algorithms for responding todatabase queries in TOOT, a spoken dialogue agentfor accessing online train schedules via a telephoneconversation.
We conduct an experiment in which12 users carry out 4 tasks of varying difficulty withone of two versions of TOOT (literal and coopera-tive TOOT), resulting in a corpus of 48 dialogues.The values for a wide range of evaluation measuresare then extracted from this corpus.
We analyzeour data using both traditional hypothesis testingmethods and the PARADISE (Walker et al, 1997;Walker et al, 1998) methodology for estimating aperformance function.
Hypothesis testing showsthat while differences among some evaluation mea-sures depend on the response strategy (literal or co-operative), other differences are a function of appli-cation task and task/strategy interactions.
A PAR-ADISE assessment of the contribution of each eval-uation measure to overall performance shows thatstrategy-dependent dialogue phenomena s well asphenomena associated with speech recognition sig-nificantly predict performance.
Our results identifythe conditions under which TOOT' s cooperative r -sponse strategy leads to greater agent performance.2 TOOTTOOT allows users to access online AMTRAK trainschedules via a telephone dialogue, as in Figure 1 .I(All examples are from the experiment in Section 3.
)We have built two versions of TOOT: literal TOOT(LT) and cooperative TOOT (CT).
LT and CT haveequivalent functionality, but use different responsestrategies to present abular esults of web queriesin a displayless environment) LT and CT incorpo-rate many of the types of database responses in the'Our domain was chosen to afford future comparison withsimilar systems, e.g., (Danieli and Gerbino, 1995).2Initiative (Walker et al, 1998) and confirmation (Danieliand Gerbino, 1995) strategies are equivalent in LT and CT, andwere chosen to reduce speech recognition errors.780T: Hi, this is the Amtrak train schedule system.Which city do you want o go to?U: Philadelphia.T: What city do you want to go to Philadelphia from?U: New York City.T: On what day of the week do you want to go toPhiladelphia from New York City?U: On the weekend.T: What time do you want to leave on the weekend?U: 4o'clockpm.T: Do you want me to find the trains going toPhiladelphia from New York City on theweekend around 4 o' clock pm now?.U: Yes.T: I am going to query the Amtrak website for you.This may take some time if the internet traffic isheavy.
Please wait patiently.
\[30 second pause\]There are 7 trains available around 4 o'clockpro.
Here are the first 3 trains.
The 1st trainleaves at 3:00 pm on Saturday and StoMa), andit takes 1 hour 12 rains.
The 2nd train leaves at3:20 pm ever)' da3, and it takes I hour 22 rains.The 3rd train leaves at 4:00 pm on Sunda), andit takes 1 hour 12 rains.
Do you want to hear therest of  the trains?U: No.T: Do you want o continue and find a new set oftrains?U: No.T: Thank you for using the Amtrak schedule system.See you next time.Figure 1 : Example dialogue with (literal) TOOT.literature into relatively literal and cooperative re-sponse strategies.
(More sophisticated cooperativestrategies could be imagined.)
When there is toomuch information to present in a single utterance,LT groups the information into units of 3 trains,then presents each unit, as in the italicized portionof Figure 1.
In contrast, CT summarizes the rangeof trains available, then tells the user to either list thetrains or further constrain the query.
In CT, the ital-icized portion of Figure 1 would be replaced withthe following response:(1) There are 7 trains available around 4 o'clock pro.Here is the earliest train we have.
The f rs t  trainleaves at 3:00 pm on Saturday and Sunday, and ittakes 1 hour 12 rains.
Here is the latest train wehave.
The seventh train leaves at 5:OOpm on Satur-da); and it takes I hour 12 rains.
Please say "list"to hear trains 3 at a time, or say "add constraint"to constrain your departure time or travel day, orsay "continue" if nO' answer was sufficient, or say"repeat" to hear this message agahz.LT's response incrementally presents the set oftrains that match the query, until the user tells LT tostop.
Enumerating large lists, even incrementally,can lead to information overload.
CT's responseis more cooperative because it better espects theresource limitations of the listener.
CT presents asubset of the matching trains using a summary re-sponse (Pao and Wilpon, 1992), followed by an op-tion to reduce the information to be retrieved (Pier-accini et al, 1997; Goddeau et al, 1996; Seneff etal., 1995; Pao and Wilpon, 1992).If there is no information that matches a query,LT reports only the lack of an answer to the query,as in the following dialogue xcerpt:(2) There are no trains going to Chicago fromPhiladelphia on Sunday around 10:30 am.
Do youwant to continue and find a new set of  trains?CT automatically relaxes the user's time constraintand allows the user to perform other elaxations:(3) There are no trains going to Chicago frontPhiladelphia on Sunday around 10:30 ant.
Theclosest earlier train leaves at 9:28 am ever), da3;and it takes I day 3 hours 36 rains.
The closest latertrain leaves at 11:45 ant on Saturday and Sunda3;and it takes 22 hours 5 rains.
Please say "relax"to change your departure time or travel da3; or say"continue" if n O' answer was sufficient, or say "re-peat" to hear this message again.CT's response is more cooperative since identify-ing the source of a query failure can help block in-correct user inferences (Pieraccini et al, 1997; Paoand Wilpon, 1992; Joshi et al, 1984; Kaplan, 1981;Mays, 1980).
LT's response could lead the user tobelieve that there are no trains on Sunday.When there are 1-3 trains that match a query, bothLT and CT list the trains:(4) There are 2 trains available around6 pro.
The firsttrain leaves at 6:05 pm ever), day and it takes 5hours 10 rains.
The second train leaves at 6:30 pmever), da); and it takes 2 days 11 hours 30 rains.
Doyou want to continue and find a new set of  trains?TOOT is implemented using a platform for spo-ken dialogue agents (Kamm et al, 1997) that com-bines automatic speech recognition (ASR), text-to-speech (TTS), a phone interface, and modulesfor specifying a dialogue manager and applicationfunctions.
ASR in our platform supports barge-in,an advanced functionality which allows users to in-terrupt an agent when it is speaking.781The dialogue manager uses a finite state machineto implement dialogue strategies.
Each state spec-ifies 1) an initial prompt (or response) which theagent says upon entering the state (such prompts of-ten elicit parameter values); 2) a helpprompt whichthe agent says if the user says help; 3) rejectionprompts which the agent says if the confidence l velof ASR is too low (rejection prompts typically askthe user to repeat or paraphrase their utterance); and4) timeout prompts which the agent says if the userdoesn't say anything within a specified time frame(timeout prompts are often suggestions about whatto say).
A context-free grammar specifies what ASRcan recognize in each state.
Transitions betweenstates are driven by semantic interpretation.TOOT' s application functions access and processinformation on AMTRAK'S web site.
Given a set ofconstraints, the functions return a table listing allmatching trains in a specified temporal interval, orwithin an hour of a specified timepoint.
This table isconverted to a natural language response which canbe realized by TTS through the use of templates foreither the LT or the CT response type; values in thetable instantiate mplate variables.3 Exper imenta l  DesignThe experimental instructions were given on a webpage, which consisted of a description of TOOT'sfunctionality, hints for talking to TOOT, and linksto 4 task pages.
Each task page contained a taskscenario, the hints, instructions for calling TOOT,anal a web survey designed to ascertain the departand travel times obtained by the user and to measureuser perceptions of task success and agent usability.Users were 12 researchers not involved with the de-sign or implementation f TOOT; 6 users were ran-domly assigned to LT and 6 to CT. Users read the in-structions in their office and then called TOOT fromtheir phone.
Our experiment yielded a corpus of 48dialogues (1344 total tums; 214 minutes of speech).Users were provided with task scenarios for tworeasons.
First, our hypothesis was that performancedepended not only on response strategy, but also ontask difficulty.
To include the task as a factor in ourexperiment, we needed to ensure that users executedthe same tasks and that they varied in difficulty.Figure 2 shows the task scenarios used in our ex-periment.
Our hypotheses about agent performanceare summarized in Table 1.
We predicted that op-timal performance would occur whenever the cor-rect task solution was included in TOOT' s initial re-Task 1 (Exact-Match): Try to find a train going toBoston from New York City on Saturday at 6:00pro.
If you cannot find an exact match, find the onewith the closest departure time.
Write down the ex-act departure time of the train you found as wellas the total travel time.Task2 (No-Match-l): Try to find a train going toChicago from Philadelphia on Sunday at 10:30am.
If you cannot find an exact match, find the onewith the closest departure time.
Write down the ex-act departure time of the train you found as wellas the total travel time.Task3 (No-Match-2): Try to find a train going toBoston from Washington D.C. on Thursday at3:30 pro.
If you cannot find an exact match, findthe one between 12:00 pm and 5:00 pm that hasthe shortest travel time.
Write down the exact de-parture time of the train you found as well as thetotal travel time.Task4 (Too-Much-Info/Early-Answer): Try to find atrain going to Philadelphia from New York Cityon the weekend at 4:00 pro.
If you cannot findan exact match, find the one with the closest de-parture time.
Please write down the exact depar-ture time of the train you found as well as the totaltravel time.
("weekend" means the train departuredate includes either Saturday or Sunday)Figure 2: Task scenarios.sponse to a web query (i.e., when the task was easy).Task 1 (dialogue fragment (4) above) produceda query that resulted in 2 matching trains, one ofwhich was the train requested in the scenario.
Sincethe response strategies of LT and CT were identicalunder this condition, we predicted identical LT andCT performance, asshown in Table 1.3Tasks 2 (dialogue fragments (2) and (3)) and 3 ledto queries that yielded no matching trains.
In Task 2users were told to find the closest rain.
Since onlyCT included this extra information in its response,we predicted that it would perform better than LT.In Task 3 users were told to find the shortesttrain within a new departure interval.
Since neitherLT nor CT provided this information initially, wehypothesized comparable LT and CT performance.However, since CT allowed users to change justtheir departure time while LT required users to con-struct a whole new query, we also thought i possiblethat CT might perform slightly better than LT.Task 4 (Figure 1 and dialogue fragment (1)) led to3Since Task 1 was the easiest, i  was always performed first.The order of the remaining tasks was randomized across users.782Task LT StrategyExact-Match Say itNo-Match-1 Say No MatchNo-Match-2 Say No MatchToo-Much-Info/Early-Answer List 3 thenmore?CT Strategy HypothesisSay it LT equal to CTRelax Time Constraint LT worse than CTRelax Time Constraint LT equal to or worse than CTSummarize; Give Options LT better than CTTable 1: Hypothesized performance of literal TOOT (LT) versus cooperative TOOT (CT).a query where the 3rd of 7 matching trains was thedesired answer.
Since only LT included this train inits initial response (by luck, due to the train's po-sition in the list of matches), we predicted that LTwould perform better than CT.
Note that this pre-diction is highly dependent on the database.
If thedesired train had been last in the list, we would havepredicted that CT would perform better than LT.attribute valuearrival-citydepart-citydepart-daydepart-rangeexact-depart-timetotal-travel-timePhiladelphiaNew York Cityweekend4:00 pm4:00 pm1 hour 12 minsTable 2: Scenario key, Task 4.A second reason for having task scenarioswas that it allowed us to objectively determinewhether users achieved their tasks.
Following PAR-ADISE (Walker et al, 1997), we defined a "key" foreach scenario using an attribute value matrix (AVM)task representation, as in Table 2.
The key indicatesthe attribute values that must be exchanged betweenthe agent and user by the end of the dialogue.
Ifthe task is successfully completed in a scenario ex-ecution (as in Figure 1), the AVM representing thedialogue is identical to the key.4 Measuring Aspects of PerformanceOnce the experiment was completed, values for arange of evaluation measures were extracted fromthe resulting data (dialogue recordings, ystem logs,and web survey responses).
Following PARADISE,we organize our measures along four performancedimensions, as shown in Figure 3.To measure task success, we compared the sce-nario key and scenario execution AVMs for eachdialogue, using the Kappa statistic (Walker et al,1997).
For the scenario execution AVM, the valuesfor arrival-city, depart-city, depart-day, and depart-range were extracted from system logs of ASR re-?
Task Success: Kappa, Completed?
Dialogue Quality: Help Requests, ASR Rejec-tions, Timeouts, Mean Recognition, Barge Ins?
Dialogue Efficiency: System Turns, User Turns,Elapsed Time?
User Satisfaction: User Satisfaction (based onTTS Performance, ASR Performance, Task Ease,Interaction Pace, User Expertise, System Response,Expected Behavior, Future Use)Figure 3: Measures used to evaluate TOOT.suits.
The exact-depart-time and total-travel-timewere extracted from the web survey.
To measureusers' perceptions of task success, the survey alsoasked users whether they had successfully Com-pleted the task.To measure dialogue quali~ or naturalness, welogged the dialogue manager's behavior on enteringand exiting each state in the finite state machine (re-call Section 2).
We then extracted the number ofprompts per dialogue due to Help Requests, ASRRejections, and Timeouts.
Obtaining the valuesfor other quality measures required manual analysis.We listened to the recordings and compared them tothe logged ASR results, to calculate concept accu-racy (intuitively, semantic interpretation accuracy)for each utterance.
This was then used, in com-bination with ASR rejections, to compute a MeanRecognition score per dialogue.
We also listenedto the recordings to determine how many times theuser interrupted the agent (Barge Ins).To measure dialogue efficiency., the number ofSystem Turns and User Turns were extracted fromthe dialogue manager log, and the total ElapsedTime was determined from the recording.To measure user satisfaction 4, users responded tothe web survey in Figure 4, which assessed theirsubjective valuation of the agent's performance.Each question was designed to measure a partic-4Questionnaire-based user satisfaction ratings (Shriberg etal., 1992; Polifroni et al, 1992) have been frequently used inthe literature as an external indicator of agent usability.783?
Was the system easy to understand in this conver-sation?
(TTS Performance)?
In this conversation, did the system understandwhat you said?
(ASR Performance)?
In this conversation, was it easy to find the scheduleyou wanted?
(Task Ease)?
Was the pace of interaction with the system appro-priate in this conversation?
(Interaction Pace)?
In this conversation, did you know what you couldsay at each point of the dialogue?
(User Expertise)?
How often was the system sluggish and slow toreply to you in this conversation?
(System Re-sponse)?
Did the system work the way you expected it to inthis conversation?
(Expected Behavior)?
From your current experience with using our sys-tem, do you think you'd use this regularly to accesstrain schedules when you are away from your desk?
(Future Use)Figure 4: User satisfaction survey and associatedevaluation measures.ular factor, e.g., System Response.
Responsesranged over n pre-defined values (e.g., ahnost never,rarely, sometimes, often, ahnost always), whichwere mapped to an integer in 1 .
.
.n .
CumulativeUser Satisfaction was computed by summing eachquestion' s score.5 Strategy and Task DifferencesTo test the hypotheses in Table 1 we use analysisof variance (ANOVA) (Cohen, 1995) to determinewhether the values of any of the evaluation mea-sures in Figure 3 significantly differ as a functionof response strategy and task scenario.First, for each task scenario (4 sets of 12 dia-logues, 6 per agent and 1 per user), we performan ANOVA for each evaluation measure as a func-tion of response strategy.
For Task 1, there areno significant differences between the 6 LT and 6CT dialogues for any evaluation measure, which isconsistent with Table 1.
For Task 2, mean Com-pleted (perceived task success rate) is 50% for LTand 100% for CT (p < .05).
In addition, the aver-age number of Help Requests per LT dialogue is0, while for CT the average is 2.2 (p < .05).
Thus,for Task 2, CT has a better perceived task successrate than LT, despite the fact that users needed morehelp to use CT. Only the perceived task success dif-ference is consistent with the Task 2 prediction inTable 1.5 For Task 3, there are no significant differ-ences between LT and CT, which again matches ourpredictions.
Finally, for Task 4, mean Kappa (ac-tual task success rate) is 100% for LT but only 65%for CT (p < .01).
6 Like Task 2, this result suggeststhat some type of task success measure is an impor-tant predictor of agent performance.
Surprisingly,we found that LT and CT did not differ with respectto any efficiency measure, in any task.
7Next, we combine all of our data (48 dialogues),and perform a two-way ANOVA for each evaluationmeasure as a function of strategy and task.
An inter-action between response strategy and task scenariois significant for Future Use (p < .03).
For task 1,the likelihood of Future Use is the same for LT andCT; for task 2, the likelihood is higher for CT; fortasks 3 and 4, the likelihood is higher for LT. Thus,the results for tasks 1, 2, and 4, but not for Task 3,are consistent with the predictions in Table 1.
How-ever, Task 3 was the most difficult task (see below),and sometimes led to unexpected user behavior withboth agents.
A strategy/task interaction is also sig-nificant for Help Requests (p < .02).
For tasks 1and 3, the number of requests is higher for LT; fortasks 2 and 4, the number is higher for CT.No evaluation measures ignificantly differ as afunction of response strategy, which is consistentwith Table 1.
Since the task scenarios were con-structed to yield comparable performance in Tasks1 and 3, better CT performance in Task 2, and betterLT performance in Task 4, we expected that overall,LT and CT performance would be comparable.In contrast, many measures (User Satisfaction,Elapsed Time, System Turns, User Turns, ASRPerformance, and Task Ease) differ as a functionof task scenario (p < .03), confirming that our tasksvary with respect o difficulty.
Our results suggestthat the ordering of the tasks from easiest o mostdifficult is 1, 4, 2, and 3, 8 which is consistent withour predictions.
Recal l  that for Task 1, the initialquery was designed to yield the correct train forboth LT and CT. For tasks 4 and 2, the initial querywas designed to yield the correct rain for only oneagent, and to require a follow-up query for the other.SHowever, the analysis in Section 6suggests hat Help Re-quests is not a good predictor of performance.6In our data, actual task success implies perceived task suc-cess, but not vice-versa.7However, our "'difficult" tasks were not that difficult (wewanted to minimize subjects' time commitment).SThis ordering is observed for all the listed measures xceptUser Turns, which reverses tasks 4 and 1.784For Task 3, the initial query was designed to requirea follow-up query for both agents.6 Performance Function EstimationWhile hypothesis testing tells us how each evalua-tion measure differs as a function of strategy and/ortask, it does not tell us how to tradeoff or com-bine results from multiple measures.
Understand-ing such tradeoffs is especially important when dif-ferent measures yield different performance predic-tions (e.g., recall the Task 2 hypothesis testing re-sults for Completed and Help Requests).MAXIMIZE USER SATISFACTION Il MAXIMIZE TASK SUCCESS \[ MINIMIZE COSTS IQUALITATIVI~ EFFICIENCY MEASURES I MEASURESFigure 5: PARADISEs structure of objectives forspoken dialogue performance.?
To assess the relative contribution of each eval-uation measure to performance, we use PAR-ADISE (Walker et al, 1997) to derive a perfo r-mance function from our data.
PARADISE drawson ideas in multi-attribute d cision theory (Keeneyand Raiffa, 1976) to posit the model shown in Fig-ure 5, then uses multivariate linear regression to es-timate a quantitative performance function based onthis model.
Linear regression produces coefficientsdescribing the relative contribution of predictor fac-tors in accounting for the variance in a predicted fac-tor.
In PARADISE, the success and cost measuresare predictors, while user satisfaction is predicted.Figure 3 showed how the measures used to evaluateTOOT instantiate he PARADISE model.The application of PARADISE to the TOOT datashows that the only significant contributors to UserSatisfaction are Completed (Comp), Mean Recog-nition (MR) and Barge Ins (BI), and yields the fol-lowing performance function:Perf = .45jV'( Comp) + .35X(MR) - .42Ar (B I)Completed is significant at p < .0002, MeanRecognition 9 at p < .003, and Barge Ins at p <.0004; these account for 47% of the variance in UserSatisfaction..V is a Z score normalization func-tion (Cohen, 1995) and guarantees that the coeffi-9Since we measure recognition rather than misrecognition,this "cost" factor has a positive coefficient.cients directly indicate the relative contribution ofeach factor to performance.Our performance function demonstrates thatTOOT performance involves task success and di-alogue quality factors.
Analysis of variance sug-gested that task success was a likely performancefactor.
PARADISE confirms this hypothesis, anddemonstrates that perceived rather than actual tasksuccess is the useful predictor.
While 39 dialogueswere perceived to have been successful, only 27were actually successful.Results that were not apparent from the analysisof variance are that Mean Recognition and BargeIns are also predictors of performance.
The meanrecognition for our corpus is 85%.
Apparently,users of both LT and CT are bothered by dialoguephenomena associated with poor recognition.
Forexample, system misunderstandings (which resultfrom ASR misrecognitions) and system requests torepeat what users have said (which result from ASRrejections) both make dialogues eem less natural.While barge-in is usually considered an advanced(and desirable) ASR capability, our performancefunction suggests that in TOOT, allowing users tointerrupt actually degrades performance.
Examina-tion of our transcripts shows that users sometimesuse barge-in to shorten TOOT's prompts.
This oftencircumvents TOOT's confirmation strategy, whichincorporates speech recognition results into promptsto make the user aware of misrecognitions.Surprisingly, no efficiency measures are signif-icant predictors of performance.
This draws intoquestion the frequently made assumption that ef-ficiency is one of the most important measures ofsystem performance, and instead suggests that usersare more attuned to both task success and qualitativeaspects of the dialogue, or that efficiency is highlycorrelated with some of these factors.However, analysis of subsets of our data suggeststhat efficiency measures can become important per-formance predictors when the more primary effectsare factored out.
For example, when a regressionis performed on the 11 TOOT dialogues with per-fect Mean Recognition, the significant contribu-tors to performance become Completed (p < .05),Elapsed time (p < .04), User Turns (p < .03) andBarge Ins (p < 0.0007) (accounting for 87% of thevariance).
Thus, in the presence of perfect ASR,efficiency becomes important.
When a regressionis performed using the 39 dialogues where usersthought hey had successfully completed the task785(perfect Completed), the significant factors becomeElapsed time (p < .002), Timeouts (p < .002), andBarge Ins (p < .02) (58% of the variance).Applying the performance function to each of our48 dialogues yields a performance estimate for eachdialogue.
Analysis with these estimates shows nosignificant differences for mean LT and CT perfor-mance.
This result is consistent with the ANOVAresult, where only one of the three (comparablyweighted) factors in the performance function de-pends on response strategy (Completed).
Note thatfor Tasks 2 and 4, the predictions in Table 1 do nothold for overall performance, despite the ANOVAresults that the predictions do hold for some evalua-tion measures (e.g., Completed in Task 2).7 ConclusionWe have presented an empirical comparison of lit-eral and cooperative query response strategies inTOOT, illustrating the advantages of combining hy-pothesis testing and PARADISE.
By using hypoth-esis testing to examine how a set of evaluation mea-sures differ as a function of response strategy andtask, we show that TOOT's cooperative and literalresponses can both lead to greater task success, like-lihood of future use, and user need for help, de-pending on task.
By using PARADISE to derive aperformance function, we show that a combinationof strategy-dependent (perceived task success) andstrategy-independent (number of barge-ins, meanrecognition score) evaluation measures best predictsoverall TOOT performance.
Our results elaboratethe conditions under which TOOT' s response strate-gies lead to greater performance, and allow us tomake predictions.
For example, our performanceequation predicts that improving mean recognitionand/or judiciously restricting the use of barge-inwill enhance performance.
Our current research isaimed at automatically adapting dialogue behaviorin TOOT, to increase mean recognition and thusoverall agent performance (Walker et al, 1998).Future work utilizing PARADISE will attempt togeneralize our results, to make a more predictivemodel of agent performance.
Performance functionestimation eeds to be done iteratively over differenttasks and dialogue strategies.
We plan to evaluateadditional cooperative r sponse strategies in TOOT(e.g., intensional summaries (Kalita et al, 1986),summarization and constraint elicitation in isola-tion), and to combine TOOT data with data fromother agents (Walker et al, 1998).8 AcknowledgmentsThanks to J. Chu-Carroll, T. Dasu, W. DuMouchel,J.
Fromer, D. Hindle, J. Hirschberg, C. Kamm, J.Kang, A.
Levy, C. Nakatani, S. Whittaker and J.Wilpon for help with this research and/or paper.ReferencesJ.
Allen and C. Perrault.
1980.
Analyzing intention in utter-ances.
Artificial Intelligence, 15.P.
Cohen.
1995.
Empirical Methods for Artificial hltelligence.MIT Press, Boston.M.
Danieli and E. Gerbino.
1995.
Metrics for evaluating dia-logue strategies in a spoken language system.
In Proc.
AAAISpring Symposium on Empirical Methods in Discourse h~-terpretation and Generation.D.
Goddeau, H. Meng, J. Polifroni, S. Seneff, andS.
Busayapongchai.
1996.
A form-based dialogue managerfor spoken language applications.
In Proc.
ICSLP.A.
Joshi, B. Webber, and R. Weischedel.
1984.
Preventingfalse inferences.
In Proc.
COLING.J.
Kalita, M. Jones, and G. McCalla.
1986.
Summarizing nat-ural language database responses.
Computational Lhlguis-tics, 12(2).C.
Kamm, S. Narayanan, D. Dutton, and R. Ritenour.
1997.Evaluating spoken dialog systems for telecommunicationservices.
In Proc.
EUROSPEECH.S.
Kaplan.
1981.
Appropriate r sponses toinappropriate ques..tions.
In A. Joshi, B. Webber, and I.
Sag, editors, Elementsof Discourse Understandh~g.
Cambridge University Press.R.
Keeney and H. Raiffa.
1976.
Decisions with Multiple Ob-jectives: Preferences and Vah~e Tradeoffs.
Wiley.E.
Mays.
1980.
Failures in natural language systems: Applica-tions to data base query systems.
In Proc.
AAALK.
McCoy.
1989.
Generating context-sensitive responses toobject related misconceptions.
Artificial hltelligence, 41 (2).J.
Moore.
1994.
Participating h~ Explanatory Dialogues.
MITPress.C.
Pao and J. Wilpon.
1992.
Spontaneous speech collectionfor the ATIS domain with an aural user feedback paradigm.Technical report, AT&T.R.
Pieraccini, E. Levin, and W. Eckert.
1997.
AMICA: TheAT&T mixed initiative conversational architecture.
In Proc.EUROSPEECH.J.
Polifroni, L. Hirschman, S. Seneff, and V. Zue.
1992.
Exper-iments in evaluating interactive spoken language systems.In Proc.
DARPA Speech and NL Workshop.S.
Seneff, V. Zue, J. Polifroni, C. Pao, L. Hetherington, D. God-deau, and J.
Glass.
1995.
The preliminary development of adisplayless PEGASUS system.
In Proc.
ARPA Spoken Lan-guage Technology Workshop.E.
Shriberg, E. Wade, and P. Price.
1992.
Human-machineproblem solving using spoken language systems (SLS): Fac-tors affecting performance and user satisfaction.
In Proc.DARPA Speech and NL Workshop.M.
Walker, D. Litman, C. Kamm, and A. Abella.
1997.
PAR-ADISE: A general framework for evaluating spoken dia-logue agents.
In Proc.
ACL/EACL.M.
Walker, D. Litman, C. Kamm, and A. Abella.
1998.
Eval-uating spoken dialogue agents with PARADISE: Two casestudies.
Computer Speech and Language.786
