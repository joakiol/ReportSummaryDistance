CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 183?187Manchester, August 2008Dependency-based Syntactic?Semantic Analysis with PropBank andNomBankRichard Johansson and Pierre NuguesLund University, Sweden{richard, pierre}@cs.lth.seAbstractThis paper presents our contribution in theclosed track of the 2008 CoNLL SharedTask (Surdeanu et al, 2008).
To tackle theproblem of joint syntactic?semantic anal-ysis, the system relies on a syntactic anda semantic subcomponent.
The syntacticmodel is a bottom-up projective parser us-ing pseudo-projective transformations, andthe semantic model uses global inferencemechanisms on top of a pipeline of clas-sifiers.
The complete syntactic?semanticoutput is selected from a candidate poolgenerated by the subsystems.The system achieved the top score in theclosed challenge: a labeled syntactic accu-racy of 89.32%, a labeled semantic F1 of81.65, and a labeled macro F1 of 85.49.1 Introduction: Syntactic?SemanticAnalysisIntuitively, semantic interpretation should helpsyntactic disambiguation, and joint syntactic?semantic analysis has a long tradition in linguis-tic theory.
This motivates a statistical modeling ofthe problem of finding a syntactic tree y?synand asemantic graph y?semfor a sentence x as maximiz-ing a function F that scores the joint syntactic?semantic structure:?y?syn, y?sem?
= arg maxysyn,ysemF (x, ysyn, ysem)The dependencies in the feature representationused to compute F determine the tractability of thesearch procedure needed to perform the maximiza-tion.
To be able to use complex syntactic featuresc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.such as paths when predicting semantic structures,exact search is clearly intractable.
This is true evenwith simpler feature representations ?
the problemis a special case of multi-headed dependency anal-ysis, which is NP-hard even if the number of headsis bounded (Chickering et al, 1994).This means that we must resort to a simplifica-tion such as an incremental method or a rerankingapproach.
We chose the latter option and thus cre-ated syntactic and semantic submodels.
The jointsyntactic?semantic prediction is selected from asmall list of candidates generated by the respectivesubsystems.2 Syntactic SubmodelWe model the process of syntactic parsing ofa sentence x as finding the parse tree y?syn=argmaxyF (x, y) that maximizes a scoring func-tion F .
The learning problem consists of fittingthis function so that the cost of the predictions isas low as possible according to a cost function ?.In this work, we consider linear scoring functionsof the following form:F (x, y) = w ??
(x, y)where ?
(x, y) is a numeric feature representationof the pair (x, y) andw a vector of feature weights.We defined the syntactic cost ?
as the sum of linkcosts, where the link cost was 0 for a correct de-pendency link with a correct label, 0.5 for a correctlink with an incorrect label, and 1 for an incorrectlink.A widely used framework for fitting the weightvector is the max-margin model (Taskar et al,2003), which is a generalization of the well-known support vector machines to general cost-based prediction problems.
Since the large num-ber of training examples and features in our casemake an exact solution of the max-margin opti-mization problem impractical, we used the on-line passive?aggressive algorithm (Crammer et al,1832006), which approximates the optimization pro-cess in two ways:?
The weight vector w is updated incremen-tally, one example at a time.?
For each example, only the most violated con-straint is considered.The algorithm is a margin-based variant of the per-ceptron (preliminary experiments show that it out-performs the ordinary perceptron on this task).
Al-gorithm 1 shows pseudocode for the algorithm.Algorithm 1 The Online PA Algorithminput Training set T = {(xt, yt)}Tt=1Number of iterations NRegularization parameter CInitialize w to zerosrepeat N timesfor (xt, yt) in Tlet y?t= argmaxyF (xt, y) + ?
(yt, y)let ?t= min?C,F (xt,y?t)?F (xt,yt)+?(yt,y?t)??(x,yt)??
(x,y?t)?2?w ?
w + ?t(?
(x, yt)??
(x, y?t))returnwaverageWe used a C value of 0.01, and the number ofiterations was 6.2.1 Features and SearchThe feature function ?
is a second-order edge-factored representation (McDonald and Pereira,2006; Carreras, 2007).
The second-order repre-sentation allows us to express features not only ofhead?dependent links, but also of siblings and chil-dren of the dependent.
This feature set forces usto adopt the expensive search procedure by Car-reras (2007), which extends Eisner?s span-baseddynamic programming algorithm (1996) to allowsecond-order feature dependencies.
Since the costfunction ?
is based on the cost of single links, thisprocedure can also be used to find the maximizerof F (xi, yij)+?
(yi, yij), which is needed at train-ing time.
The search was constrained to disallowmultiple root links.2.2 Handling Nonprojective LinksAlthough only 0.4% of the links in the training setare nonprojective, 7.6% of the sentences contain atleast one nonprojective link.
Many of these linksrepresent long-range dependencies ?
such as wh-movement ?
that are valuable for semantic pro-cessing.
Nonprojectivity cannot be handled byspan-based dynamic programming algorithms.
Forparsers that consider features of single links only,the Chu-Liu/Edmonds algorithm can be used in-stead.
However, this algorithm cannot be gen-eralized to the second-order setting ?
McDonaldand Pereira (2006) proved that this problem is NP-hard, and described an approximate greedy searchalgorithm.To simplify implementation, we instead optedfor the pseudo-projective approach (Nivre andNilsson, 2005), in which nonprojective links arelifted upwards in the tree to achieve projectivity,and special trace labels are used to enable recoveryof the nonprojective links at parse time.
The useof trace labels in the pseudo-projective transfor-mation leads to a proliferation of edge label types:from 69 to 234 in the training set, many of whichoccur only once.
Since the running time of ourparser depends on the number of labels, we usedonly the 20 most frequent trace labels.3 Semantic SubmodelOur semantic model consists of three parts:?
A SRL classifier pipeline that generates a listof candidate predicate?argument structures.?
A constraint system that filters the candidatelist to enforce linguistic restrictions on theglobal configuration of arguments.?
A global classifier that rescores the predicate?argument structures in the filtered candidatelist.Rather than training the models on gold-standard syntactic input, we created an automati-cally parsed training set by 5-fold cross-validation.Training on automatic syntax makes the semanticclassifiers more resilient to parsing errors, in par-ticular adjunct labeling errors.3.1 SRL PipelineThe SRL pipeline consists of classifiers for predi-cate identification, predicate disambiguation, sup-port identification (for noun predicates), argumentidentification, and argument classification.
Wetrained one set of classifiers for verb predicatesand another for noun predicates.
For the pred-icate disambiguation classifiers, we trained onesubclassifier for each lemma.
All classifiers in thepipeline were L2-regularized linear logistic regres-sion classifiers, implemented using the efficientLIBLINEAR package (Lin et al, 2008).
For multi-class problems, we used the one-vs-all binarization184method, which makes it easy to prevent outputs notallowed by the PropBank or NomBank frame.Since our classifiers were logistic, their outputvalues could be meaningfully interpreted as prob-abilities.
This allowed us to combine the scoresfrom subclassifiers into a score for the completepredicate?argument structure.
To generate the can-didate lists used by the global SRL models, we ap-plied beam search based on these scores using abeam width of 4.The features used by the classifiers are listed inTables 1 and 2.
In the tables, the features usedby the classifiers for noun and verb predicates areindicated by N and V, respectively.
We selected thefeature sets by greedy forward subset selection.Feature PredId PredDisPREDWORD N,V N,VPREDLEMMA N,V N,VPREDPARENTWORD/POS N,V N,VCHILDDEPSET N,V N,VCHILDWORDSET N,V N,VCHILDWORDDEPSET N,V N,VCHILDPOSSET N,V N,VCHILDPOSDEPSET N,V N,VDEPSUBCAT N,V N,VPREDRELTOPARENT N,V N,VTable 1: Classifier features in predicate identifica-tion and disambiguation.Feature Supp ArgId ArgClPREDPARENTWORD/POS N N,VCHILDDEPSET N N,V N,VPREDLEMMASENSE N N,V N,VVOICE V VPOSITION N N,V N,VARGWORD/POS N N,V N,VLEFTWORD/POS N N,VRIGHTWORD/POS N N,V N,VLEFTSIBLINGWORD/POS N,VRIGHTSIBLINGWORD/POS N NPREDPOS N N,V VRELPATH N N,V N,VPOSPATH NRELPATHTOSUPPORT N NVERBCHAINHASSUBJ V VCONTROLLERHASOBJ V NPREDRELTOPARENT N N,V N,VFUNCTION N,VTable 2: Classifier features in argument identifica-tion and classification and support detection.Features Used in Predicate Identification andDisambiguationPREDWORD, PREDLEMMA.
The lexical formand lemma of the predicate.PREDPARENTWORD and PREDPARENTPOS.Form and part-of-speech tag of the parentnode of the predicate.CHILDDEPSET, CHILDWORDSET, CHILD-WORDDEPSET, CHILDPOSSET, CHILD-POSDEPSET.
These features represent theset of dependents of the predicate usingcombinations of dependency labels, words,and parts of speech.DEPSUBCAT.
Subcategorization frame: the con-catenation of the dependency labels of thepredicate dependents.PREDRELTOPARENT.
Dependency relation be-tween the predicate and its parent.Features Used in Argument Identification andClassificationPREDLEMMASENSE.
The lemma and sensenumber of the predicate, e.g.
give.01.VOICE.
For verbs, this feature is Active or Pas-sive.
For nouns, it is not defined.POSITION.
Position of the argument with respectto the predicate: Before, After, or On.ARGWORD and ARGPOS.
Lexical form andpart-of-speech tag of the argument node.LEFTWORD, LEFTPOS, RIGHTWORD, RIGHT-POS.
Form/part-of-speech tag of the left-most/rightmost dependent of the argument.LEFTSIBLINGWORD, LEFTSIBLINGPOS,RIGHTSIBLINGWORD, RIGHTSIBLING-POS.
Form/part-of-speech tag of theleft/right sibling of the argument.PREDPOS.
Part-of-speech tag of the predicate.RELPATH.
A representation of the complexgrammatical relation between the predicateand the argument.
It consists of the sequenceof dependency relation labels and link direc-tions in the path between predicate and argu-ment, e.g.
IM?OPRD?OBJ?.POSPATH.
An alternative view of the grammat-ical relation, which consists of the POS tagspassed when moving from predicate to argu-ment, e.g.
VB?TO?VBP?PRP.RELPATHTOSUPPORT.
The RELPATH from theargument to a support chain.VERBCHAINHASSUBJ.
Binary feature that is setto true if the predicate verb chain has a sub-ject.
The purpose of this feature is to resolveverb coordination ambiguity as in Figure 1.CONTROLLERHASOBJ.
Binary feature that istrue if the link between the predicate verbchain and its parent is OPRD, and the parenthas an object.
This feature is meant to resolvecontrol ambiguity as in Figure 2.185FUNCTION.
The grammatical function of the ar-gument node.
For direct dependents of thepredicate, this is identical to the RELPATH.ISBJeat drinkyouandCOORD SBJCONJROOTSBJ COORDROOTdrinkandeatICONJFigure 1: Coordination ambiguity: The subject I isin an ambiguous position with respect to drink.I toIMSBJwant sleephimOBJOPRDROOTIMsleepISBJwantROOTtoOPRDFigure 2: Subject/object control ambiguity: I is inan ambiguous position with respect to sleep.3.2 Linguistically Motivated GlobalConstraintsThe following three global constraints were usedto filter the candidates generated by the pipeline.CORE ARGUMENT CONSISTENCY.
Core argu-ment labels must not appear more than once.DISCONTINUITY CONSISTENCY.
If there is a la-bel C-X, it must be preceded by a label X.REFERENCE CONSISTENCY.
If there is a labelR-X and the label is inside a relative clause, itmust be preceded by a label X.3.3 Global SRL ModelToutanova et al (2005) have showed that aglobal model that scores the complete predicate?argument structure can lead to substantial perfor-mance gains.
We therefore created a global SRLclassifier using the following global features in ad-dition to the features from the pipeline:CORE ARGUMENT LABEL SEQUENCE.
Thecomplete sequence of core argument labels.The sequence also includes the predicate andvoice, for instance A0+break.01/Active+A1.MISSING CORE ARGUMENT LABELS.
The setof core argument labels declared in the Prop-Bank/NomBank frame that are not present inthe predicate?argument structure.Similarly to the syntactic submodel, we trainedthe global SRL model using the online passive?aggressive algorithm.
The cost function ?
wasdefined as the number of incorrect links in thepredicate?argument structure.
The number of it-erations was 20 and the regularization parameterC was 0.01.
Interestingly, we noted that the globalSRL model outperformed the pipeline even whenno global features were added.
This shows that theglobal learning model can correct label bias prob-lems introduced by the pipeline architecture.4 Syntactic?Semantic IntegrationOur baseline joint feature representation containedonly three features: the log probability of the syn-tactic tree and the log probability of the semanticstructure according to the pipeline and the globalmodel, respectively.
This model was trained on thecomplete training set using cross-validation.
Theprobabilities were obtained using the multinomiallogistic function (?softmax?
).We carried out an initial experiment with a morecomplex joint feature representation, but failed toimprove over the baseline.
Time prevented us fromexploring this direction conclusively.5 ResultsThe submitted results on the development and testcorpora are presented in the upper part of Table 3.After the submission deadline, we corrected a bugin the predicate identification method.
This re-sulted in improved results shown in the lower part.Corpus Syn acc Sem F1 Macro F1Development 88.47 80.80 84.66Test WSJ 90.13 81.75 85.95Test Brown 82.81 69.06 75.95Test WSJ + Brown 89.32 80.37 84.86Development 88.47 81.86 85.17Test WSJ 90.13 83.75 86.61Test Brown 82.84 69.85 76.34Test WSJ + Brown 89.32 81.65 85.49Table 3: Results.5.1 Syntactic ResultsTable 4 shows the effect of adding second-orderfeatures to the parser in terms of accuracy as wellas training and parsing time on a Mac Pro, 3.2GHz.
The training times were measured on thecomplete training set and the parsing time and ac-curacies on the development set.
Similarly to Car-reras (2007), we see that these features have a verylarge impact on parsing accuracy, but also that theparser pays dearly in terms of efficiency as thesearch complexity increases fromO(n3) toO(n4).186Since the low efficiency of the second-order parserrestricts its use to batch applications, we see an in-teresting research direction to find suitable com-promises between the two approaches, for instanceby sacrificing the exact search procedure.System Training Parse Labeled Unlabeled1st order 65 min 28 sec 85.78 89.512nd order 60 hours 34 min 88.33 91.43Table 4: Impact of second-order features.Table 5 shows the dependency types most af-fected by the addition of second-order features tothe parser when ordered by the increase in F1.
Ascan be seen, they are all verb adjunct categories,which demonstrates the effect of grandchild fea-tures on PP attachment and labeling.Label ?R ?P ?F1TMP 14.7 12.9 13.9DTV 0 19.9 10.5LOC 7.8 12.3 9.9PRP 12.4 6.7 9.6DIR 5.9 7.2 6.5Table 5: Labels affected by second-order features.5.2 Semantic ResultsTo assess the effect of the components in the se-mantic submodel, we tested their performance onthe top-scoring parses from the syntactic model.Table 6 shows the results.
The baseline systemconsists of the SRL pipeline only (P).
Adding lin-guistic constraints (C) results in a more precision-oriented system with slightly lower recall, but sig-nificantly higher F1.
Even higher performance isobtained when adding the global SRL model (G).System P R F1P 80.74 77.98 79.33P+C 82.42 77.66 79.97P+C+G 83.64 78.14 80.40Table 6: SRL results on the top-scoring parse trees.5.3 Syntactic?Semantic IntegrationThe final experiment concerned the integration ofsyntactic and semantic analysis.
In this setting,the system chooses the output that maximizes thejoint syntactic?semantic score, based on the top Nsyntactic trees.
Table 7 shows the results on thedevelopment set.
We see that syntactic?semanticintegration improves both syntactic accuracy andsemantic F1.
This holds for the constraint-basedSRL system as well as for the full system.Sem model N Syn acc Sem F1 Macro F1P+C 1 88.33 79.97 84.17P+C 16 88.42 80.42 84.44P+C+G 1 88.33 80.40 84.39P+C+G 16 88.47 80.80 84.66Table 7: Syntactic?semantic integration.6 ConclusionWe have described a system1 for syntactic and se-mantic dependency analysis based on PropBankand NomBank, and detailed the implementationof its subsystems.
Crucial to our success was thehigh performance of the syntactic parser, whichachieved a high accuracy.
In addition, we recon-firmed the benefits of global inference in semanticanalysis: both constraint-based and learning-basedmethods resulted in improvements over a baseline.Finally, we showed that integration of syntacticand semantic analysis is beneficial for both sub-tasks.
We hope that this shared task will spur fur-ther research that leads to new feature representa-tions and search procedures to handle the problemof joint syntactic and semantic analysis.ReferencesCarreras, Xavier.
2007.
Experiments with a higher-order pro-jective dependency parser.
In Proceedings of CoNLL.Chickering, David M., Dan Geiger, and David Heckerman.1994.
Learning Bayesian networks: The combination ofknowledge and statistical data.
Technical Report MSR-TR-94-09, Microsoft Research.Crammer, Koby, Ofer Dekel, Joseph Keshet, Shai Shalev-Schwartz, and Yoram Singer.
2006.
Online passive-aggressive algorithms.
JMLR, 2006(7):551?585.Eisner, Jason M. 1996.
Three new probabilistic models fordependency parsing: An exploration.
In Proc.
of ICCL.Lin, Chih-Jen, Ruby C. Weng, and S. Sathiya Keerthi.
2008.Trust region Newton method for large-scale logistic regres-sion.
JMLR, 2008(9):627?650.McDonald, Ryan and Fernando Pereira.
2006.
Online learn-ing of approximate dependency parsing algorithms.
InProceedings of EACL-2006.Nivre, Joakim and Jens Nilsson.
2005.
Pseudo-projectivedependency parsing.
In Proceedings of ACL-2005.Surdeanu, Mihai, Richard Johansson, Adam Meyers, Llu?sM?rquez, and Joakim Nivre.
2008.
The CoNLL?2008shared task on joint parsing of syntactic and semantic de-pendencies.
In Proceedings of CoNLL?2008.Taskar, Ben, Carlos Guestrin, and Daphne Koller.
2003.Max-margin Markov networks.
In Proceedings of NIPS.Toutanova, Kristina, Aria Haghighi, and Christopher D. Man-ning.
2005.
Joint learning improves semantic role label-ing.
In Proceedings of ACL-2005.1Our system is freely available for download athttp://nlp.cs.lth.se/lth_srl.187
