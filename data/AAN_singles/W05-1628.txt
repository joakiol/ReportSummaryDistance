Searching for Grammaticality: Propagating Dependencies in the ViterbiAlgorithmStephen Wan12 Robert Dale1 Mark Dras11Centre for Language TechnologyDiv.
of Information Communication SciencesMacquarie UniversitySydney, Australiaswan,rdale,madras@ics.mq.edu.auCe?cile Paris22Information and CommunicationTechnologiesCSIROSydney, AustraliaCecile.Paris@csiro.auAbstractIn many text-to-text generation scenarios (for in-stance, summarisation), we encounter human-authored sentences that could be composed by re-cycling portions of related sentences to form newsentences.
In this paper, we couch the generationof such sentences as a search problem.
We in-vestigate a statistical sentence generation methodwhich recombines words to form new sentences.We propose an extension to the Viterbi algorithmdesigned to improve the grammaticality of gener-ated sentences.
Within a statistical framework, theextension favours those partially generated stringswith a probable dependency tree structure.
Ourpreliminary evaluations show that our approachgenerates less fragmented text than a bigram base-line.1 IntroductionIn abstract-like automatic summary generation, we often re-quire the generation of new and previously unseen summarysentences given some closely related sentences from a sourcetext.
We refer to these as Non-Verbatim Sentences.
Thesesentences are used instead of extracted sentences for a varietyof reasons including improved conciseness and coherence.The need for a mechanism to generate such sentences issupported by recent work showing that sentence extraction isnot sufficient to account for the scope of written human sum-maries.
Jing and McKeown [1999] found that only 42% ofsentences from summaries of news text were extracted sen-tences.
This is also supported by the work of Knight andMarcu [2002] (cited by [Daume?
III and Marcu, 2004]), whichfinds that only 2.7% of human summary sentences are ex-tracts.
In our own work on United Nations HumanitarianAid Proposals, we noticed that only 30% of sentences areextracted from the source document, either verbatim or withtrivial string replacements.While the figures do vary, it shows that additional mecha-nisms beyond sentence extraction are needed.
In response tothis, our general research problem is one in which given a setof related sentences, a single summary sentence is producedby recycling words from the input sentence set.In this paper, we adopt a statistical technique to allow easyportability across domains.
The Viterbi algorithm [Forney,1973] is used to search for the best traversal of a network ofwords, effectively searching through the sea of possible wordsequences.
We modify the algorithm so that it narrows thesearch space not only to those sequences with a semblance ofgrammaticality (via n-grams), but further still to those that aregrammatical sentences preserving the dependency structurefound in the source material.Consider the following ungrammatical word sequence typ-ical of that produced by an n-gram generator, ?The quickbrown fox jumped over the lazy dog slept by the log ?.
Onediagnosis of the problem is that the word dog is also usedas the subject of the second verb slept.
Ideally, we want toavoid such sequences since they introduce text fragments, inthis case ?slept by the log?.
We could, for example, recordthe fact that dog is already governed by the verb jumped, andthus avoid appending a second governing word slept.To do so, our extension propagates dependency featuresduring the search and uses these features to influence thechoice of words suitable for appending to a partially gener-ated sentence.
Dependency relations are derived from shal-low syntactic dependency structure [Kittredge and Mel?cuk,1983].
Specifically, we use representations of relations be-tween a head and modifier, ignoring relationship labels forthe present.Within the search process, we constrain the choice of fu-ture words by preferring words that are likely to attach tothe dependency structure of the partially generated sentence.Thus, sequences with plausible structures are ranked higher.The remainder of the paper is structured as follows.
In Sec-tion 2, we describe the problem in detail and our approach.We outline our use of the Viterbi algorithm in Section 3.
InSection 4, we describe how this is extended to cater for de-pendency features.
We compare related research in Section 5.A preliminary evaluation is presented and discussed in Sec-tion 6.
Finally, we conclude with future work in Section 7.2 Narrowing the Search Space: A Descriptionof the Statistical Sentence GenerationProblemIn this work, sentence generation is essentially a search forthe most probable sequence of words, given some source text.However, this constitutes an enormous space which requiresefficient searching.
Whilst reducing a vocabulary to a suit-able subset narrows this space somewhat, we can use statis-tical models, representing properties of language, to prunethe search space of word sequences further to those stringsthat reflect real language usage.
For example, n-gram modelslimit the word sequences examined to those that seem gram-matically correct, at least for small windows of text.However, n-grams alone often result in sentences that,whilst near-grammatical, are often just gibberish.
When com-bined with a (word) content selection model, we narrow thesearch space even further to those sentences that appear tomake sense.
Accordingly, approaches such as Witbrock andMittal [1999] and Wan et al [2003] have investigated modelsthat improve the choice of words in the sentence.
Witbrockand Mittal?s content model chooses words that make goodheadlines, whilst that of Wan et al attempts to ensure that,given a short document like a news article, only words fromsentences of the same subtopic are combined to form a newsentences.
In this paper, we narrow the search space to thosesequences that conserve dependency structures from withinthe input text.Our algorithm extension essentially passes along the long-distance context of dependency head information of the pre-ceding word sequence, in order to influence the choice of thenext word appended to the sentence.
This dependency struc-ture is constructed statistically by an O(n) algorithm, whichis folded into the Viterbi algorithm.
Thus, the extension isin an O(n4) algorithm.
The use of dependency relations fur-ther constrains the search space.
Competing paths throughthe search space are ranked taking into account the proposeddependency structures of the partially generated word se-quences.
Sentences with probable dependency structures areranked higher.
To model the probability of a dependency re-lation, we use the statistical dependency models inspired bythose described in Collins [1996].3 Using The Viterbi Algorithm for SentenceGenerationWe assume that the reader is familiar with the Viterbi al-gorithm.
The interested reader is referred to Manning andSchutze [1999] for a more complete description.
Here, wesummarise our re-implementation (described in [Wan et al,2003]) of the Viterbi algorithm for summary sentence gener-ation, as first introduced by Witbrock and Mittal [1999].In this work, we begin with a Hidden Markov Model(HMM) where the nodes (ie, states) of the graph are uniquelylabelled with words from a relevant vocabulary.
To obtain asuitable subset of the vocabulary, words are taken from a setof related sentences, such as those that might occur in a newsarticle (as is the case for the original work by Witbrock andMittal).
In this work, we use the clusters of event related sen-tences from the Information Fusion work by Barzilay et al[1999].
The edges between nodes in the HMM are typicallyweighted using bigram probabilities extracted from a relatedcorpus.The three probabilities of the unmodified Viterbi algorithmare defined as follows:Transition Probability (using the Maximum Likelihood Esti-mate to model bigram probabilities)1:ptrngram (wi+1|wi) =count(wi, wi+1)count(wi)Emission Probability: (For the purposes of testing the newtransition probability function described in Section 4, this isset to 1 in this paper):pem(w) = 1Path Probability is defined recursively as:ppath(w0, .
.
.
, wi+1) =ptrngram (wi+1|wi)?
pem(w)?
ppath(w0 .
.
.
wi)The unmodified Viterbi algorithm as outlined here wouldgenerate word sequences just using a bigram model.
As notedabove, such sequences will often be ungrammatical.4 A Mechanism for Propagating DependencyFeatures in the Extended Viterbi AlgorithmIn our extension, we modify the definition of the TransitionProbability such that not only do we consider bigram prob-abilities but also dependency-based transition probabilities.Examining the dependency head of the preceding string thenallows us to consider long-distance context when append-ing a new word.
The algorithm ranks highly those wordswith a plausible dependency relation to the preceding string,with respect to the source text being generated from (or sum-marised).However, instead of considering just the likelihood of adependency relation between adjacent pairs of words, we canconsider the likelihood of a word attaching to the dependencytree structure of the partially generated sentence.
Specifically,it is the rightmost root-to-leaf branch that can still be modifiedor governed by the appending of a new word to the string.This rightmost branch is stored as a stack.
It is updated andpropagated to the end of the path each time we add a word.Thus, our extension has two components: DependencyTransition and Head Stack Reduction.
Aside from these mod-ifications, the Viterbi algorithm remains the same.In the remaining subsections, we describe in detail how thedependency relations are computed and how the stack is re-duced.
In Figure 3, we present pseudo-code for the extendedViterbi algorithm.4.1 Scoring a Dependency TransitionDependency Parse Preprocessing of Source TextThe Dependency Transition is simply an additional weight onthe HMM edge.
The transition probability is the average ofthe two transition weights based on bigrams and dependen-cies:ptr(wi+1|w1) =average(ptrngram (wi+1|w1), ptrdep (wi+1|w1))Before we begin the generation process, we first use a depen-dency parser to parse all the sentences from the source text to1Here the subscripts refer to the fact that this is a transition prob-ability based on n-grams.
We will later propose an alternative usingdependency transitions.obtain dependency trees.
A traversal of each dependency treeyields all parent-child relationships, and we update an adja-cency matrix of connectivity accordingly.
Because the statusof a word as a head or modifier depends on the word order inEnglish, we consider relative word positions to determine ifa relation has a forward or backward2 direction.
Forward andbackward directional relations are stored in separate matri-ces.
The Forward matrix stores relations in which the head isto the right of modifier in the sentence.
Conversely, the Back-ward matrix stores relations in the head to left of the modifier.This distinction is required later in the stack reduction step.As an example, given the two strings (using charactersin lieu of words) ?d b e a c?
and ?b e d c a?
and thecorresponding dependency trees:abd eccdbeawe obtain the following adjacency matrices:Forward (or Right-Direction) Adjacency Matrix???
?0 a b c d ea 0 1 0 0 0b 0 0 0 1 0c 0 0 0 1 0d 0 1 0 0 0e 0 0 0 0 0???
?Backward (or Left-Direction) Adjacency Matrix???
?0 a b c d ea 0 0 1 0 0b 0 0 0 0 2c 1 0 0 0 0d 0 0 0 0 0e 0 0 0 0 0???
?We refer to the matrices as Adjright and Adjleft respectively.The cell value in each matrix indicates the number of timesword i (that is, the row index) governs word j (that is, thecolumn index).Computing the Dependency Transition ProbabilityWe define the Dependency Transition weight as:ptrdep (wi+1|wi) =p(Depsym(wi+1, headStack(wi))where Depsym is the symmetric relation stating that somedependency relation occurs between a word and any of thewords in the stack, irrespective of which is the head.
Intu-itively, the stack is a compressed representation of the depen-dency tree corresponding to the preceding words.
The prob-ability indicates how likely it is that the new word can attachitself to this incrementally built dependency tree, either as amodifier or a governer.
Since the stack is cumulatively passedon at each point, we need only consider the stack stored at thepreceding word.This is estimated as follows:p(Depsym(wi+1, headStack(wi))) =maxh?headStack(wi)p(Depsym(wi+1, h))2These are defined analogously to similar concepts in Combina-tory Categorial Grammar [Steedman, 2000].the quick brown fox jumpsjumps over the lazy dog .
[the][quickthe][brownquickthe][fox][jumps][overjumps][theoverjumps][ lazytheoverjumps][dogoverjumps]Figure 1: A path through a lattice.
Although separated ontwo lines, it represents a single sequence of words.
The stack(oriented upwards) grows and shrinks as we add words.
Notethat the modifiers to dog are popped off before it is pushed on.Note also that modifiers of existing items on the stack, suchas over are merely pushed on.
Words with no connection topreviously seen stack items are also pushed on (eg.
quick) inthe hope that a head will be found later.Here, we assume that a word can only attach to the treeonce at a single node; hence, we find the node that max-imises the probability of node attachment.
The relation-ship Depsym(a, b) is modelled using a simplified version ofCollins?
[1996] dependency model.Because of the status of word as the head relies on thepreservation of word order, we keep track of the direction-ality of a relation.
For two words a and b where a precedes bin the generated string,p(Depsym(a, b)) ?Adjright(a, b) + Adjleft(b, a)cnt(co-occur(a, b))where Adjright and Adjleft are the right and left adjacencymatrices.
Recall that row indices are heads and column in-dices are modifiers.4.2 Head Stack ReductionOnce we decide that a newly considered path is better thanany other previously considered one, we update the headstack to represent the extended path.
At any point in time,the stack represents the rightmost root-to-leaf branch of thedependency tree (for the generated sentence) that can stillbe modified or governed by concatenating new words to thestring.3 Within the stack, older words may be modified bynewer words.
Our rules for modifying the stack are designedto cater for a projective4 dependency grammar.There are three possible alternative outcomes of the reduc-tion.
The first is that the proposed top-of-stack (ToS) hasno dependency relation to any of the existing stack items, inwhich case the stack remains unchanged.
For the second andthird cases, we check each item on the stack and keep a record3Note that we can scan through the stack as well as push ontoand pop from the top; this is thus the same type of stack as used in,for example, Nested Stack Automata.4That is, if wi depends on wj , all words in between wi and wjare also dependent on wj .reduceHeadStack(aNode, aStack) returns aStackNodenew ?aNodeStack ?aStack # duplicateNodemax ?NULLEdgeprob ?0# Find best chunkWhile notEmpty(aStack)Head ?pop(aStack)if p(depsym (Nodenew, Head)) > EdgeprobNodemax ?HeadEdgeprob ?depsym(Nodenew, Head)# Keep only best chunkWhile top(aStack) 6= Nodemaxpop(aStack)# Determine new head of existing stringif isReduced(Nodenew,Nodemax)pop(aStack)elsepush(Nodenew, aStack)Figure 2: Pseudocode for the Head Stack Reduction operationonly of the best probable dependency between the proposedToS and the appropriate stack item.
The second outcome,then, is that the proposed ToS is the head of some item onthe stack.
All items up to and including that stack item arepopped off and the proposed ToS is pushed on.
The third out-come is that it modifies some item on the stack.
All stackitems up to (but not including) the stack item are popped offand the proposed ToS is pushed on.
The pseudocode is pre-sented in Figure 2.
An example of stack manipulation is pre-sented in Figure 1.
We rely on two external functions.
Thefirst function, depsym/2, has already been presented above.The second function, isReduced/2, relies on an auxiliaryfunction returning the probability of one word being governedby the other, given the relative order of the words.
This is inessence our parsing step, determining which word governsthe other.
The function is defined as follows:isReduced(w1, w2) =p(isHeadRight(w1, w2)) > p(isHeadLeft(w1, w2))where w1 precedes w2, and:p(isHeadRight(w1, w2))?
Adjright(w1, w2)cnt(hasRelation(w1, w2, wherei(w1) < i(w2)))and similarly,p(isHeadLeft(w1, w2))?Adjleft(w2, w1)cnt(hasRelation(w1, w2, wherei(w1) < i(w2)))where hasRelation/2 is the number of times we seethe two words in a dependency relation, and where i(wi)returns a word position in the corpus sentence.
Thefunction isReduced/2 makes calls to p(isHeadRight/2)andp(isHeadLeft/2).
It returns true if the first parameterviterbiSearch(maxLength, stateGraph) returns bestPathnumStates ?getNumStates(stateGraph)viterbi ?a matrix[numStates+2,maxLength+2]viterbi[0,0].score ?1.0for each time step t from 0 to maxLength do# Termination Conditionif ((viterbi[endState, t].score 6= 0)AND isAcceptable(endState.headStack))# Backtrace from endState and return path# Continue appending wordsfor each state s from 0 to numStates dofor each transition s?
from snewScore ?viterbi[s,t].score ?
ptr(s?|s) ?
pem(s?
)if ((viterbi[s?,t+1].score = 0) OR(newScore > viterbi[s?, t+1]))viterbi[s?,t+1].score ?newScoreviterbi[s?,t+1].headStack ?reduceHeadStack(s?,viterbi[s,t].headStack)backPointer[s?,t+1] ?sBacktrace from viterbi[endState,t] and return pathFigure 3: Extended Viterbi Algorithmis the head of the second, and false otherwise.
In the com-parison, the denominator is constant.
We thus need only thenumerator in these auxiliary functions.Collins?
distance heuristics [1996] weight the probabilityof a dependency relation between two words based on thedistance between them.
We could implement a similar strat-egy by favouring small reductions in the head stack.
Thus areduction with a more recent stack item which is closer to theproposed ToS would be less penalised than an older one.5 Related WorkThere is a wealth of relevant research related to sentence gen-eration.
We focus here on a discussion of related work fromstatistical sentence generation and from summarisation.In recent years, there has been a steady stream of researchin statistical text generation.
We focus here on work whichgenerates sentences from some sentential semantic represen-tation via a statistical method.
For examples of related sta-tistical sentence generators see Langkilde and Knight [1998]and Bangalore and Rambow [2000].
These approaches be-gin with a representation of sentence semantics that closelyresembles that of a dependency tree.
This semantic represen-tation is turned into a word lattice.
By ranking all traversalsof this lattice using an n-gram model, the best surface realisa-tion of the semantic representation is chosen.
The system thensearches for the best path through this lattice.
Our approachdiffers in that we do not start with a semantic representation.Instead, we paraphrase the original text.
We search for thebest word sequence and dependency tree structure concur-rently.Research in summarisation has also addressed the prob-lem of generating non-verbatim sentences; see [Jing andMcKeown, 1999], [Barzilay et al, 1999] and more recently[Daume?
III and Marcu, 2004].
Jing presented a HMM forlearning alignments between summary and source sentencestrained using examples of summary sentences generated byhumans.
Daume III also provides a mechanism for sub-sentential alignment but allows for alignments between multi-ple sentences.
Both these approaches provide models for laterrecombining sentence fragments.
Our work differs primarilyin granularity.
Using words as a basic unit potentially offersgreater flexibility in pseudo-paraphrase generation since weable to modify the word sequence within the phrase.It should be noted, however, that a successful execution ofour algorithm is likely to conserve constituent structure (ie.
acoarser granularity) via the use of dependencies, whilst stillmaking available a flexibility at the word level.
Addition-ally, our use of dependencies allows us to generate not only astring but a dependency tree for that sentence.6 EvaluationIn this section, we outline our preliminary evaluation of gram-maticality in which we compare our dependency based gener-ation method against a baseline.
To study any improvementsin grammaticality, we compare our dependency based gener-ation method against a baseline consisting of sentences gen-erated using bigram model.In the evaluation, we do not use any smoothing algorithmsfor dependency counts.
For both our approach and the base-line, Katz?s Back-off smoothing algorithm is used for bigramprobabilities.For our evaluation cases, we use the Information Fusiondata collected by [Barzilay et al, 1999].
This data is madeup of news articles that have been first grouped by topic,and then component sentences further clustered by similar-ity of events.
There are 100 sentence clusters and on averagethere are 4 sentences per cluster.
Each sentence in the clusteris initially passed through the Connexor dependency parser(www.connexor.com) to obtain dependency relations.
Eachsentence cluster forms an evaluation case in which we gener-ate a single sentence.
Example output and the original text ofthe cluster is presented in Figure 4.To give both our approach and the baseline the greatestchance of generating a sentence, we obtain our bigrams fromour evaluation cases.5 Aside from this preprocessing to col-lect input sentence bigrams and dependencies, there is notraining as such.
For each evaluation case, both our systemand the baseline method generates a set of answer strings,from 3 to 40 words in length.For each generated output of a given sentence length, wecount the number of times the Connexor parser resorts to re-turning partial parses.
This count, albeit a noisy one, is usedas our measure of ungrammaticality.
We calculate the aver-age ungrammaticality score across evaluation cases for eachsentence length.5Note that this is permissible in this case because we are notmaking any claims about the coverage of our model.Original TextA military transporter was scheduled to take off in the afternoon from Yokota air baseon the outskirts of Tokyo and fly to Osaka with 37,000 blankets .Mondale said the United States, which has been flying in blankets and is sending ateam of quake relief experts, was prepared to do more if Japan requested .United States forces based in Japan will take blankets to help earthquake survivorsThursday, in the U.S. military?s first disaster relief operation in Japan since it set upbases here.Our approach with Dependencies and End of Sentence Check6: united states forces based in blankets8: united states which has been flying in blankets11: a military transporter was prepared to osaka with 37,000 blankets18: mondale said the afternoon from yokota air base on the united states which has beenflying in blankets20: mondale said the outskirts of tokyo and is sending a military transporter was pre-pared to osaka with 37,000 blankets23: united states forces based in the afternoon from yokota air base on the outskirts oftokyo and fly to osaka with 37,000 blankets27: mondale said the afternoon from yokota air base on the outskirts of tokyo and issending a military transporter was prepared to osaka with 37,000 blankets29: united states which has been flying in the afternoon from yokota air base on theoutskirts of tokyo and is sending a team of quake relief operation in blankets31: united states which has been flying in the afternoon from yokota air base on the out-skirts of tokyo and is sending a military transporter was prepared to osaka with 37,000blankets34: mondale said the afternoon from yokota air base on the united states which hasbeen flying in the outskirts of tokyo and is sending a military transporter was preparedto osaka with 37,000 blankets36: united states which has been flying in japan will take off in the afternoon fromyokota air base on the outskirts of tokyo and is sending a military transporter was pre-pared to osaka with 37,000 blanketsFigure 4: A cluster of related sentences and sample generatedoutput from our system.
Leftmost numbers indicate sentencelength.11.522.533.544.553  4  5  6  7  8  9  10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38UngrammaticalityScoreSentence LengthUngrammaticality Errors across Sentence LengthsBaselineSystemFigure 5: Ungrammaticality scores for generated output.Higher scores indicates worse performance.The results are presented in Figure 5.
Our approach almostalways performs better than the baseline, producing less er-rors per sentence length.
Using the Wilcoxon Signed RankText (alpha = 0.5), we found that for sentences of lengthgreater than 12, the differences were usually significant.7 Conclusion and Future WorkIn this paper, we presented an extension to the Viterbi al-gorithm that statistically determines dependency structure ofpartially generated sentences and selects of words that arelikely to attach to this structure.
The resulting sentence ismore grammatical than that generated using a bigram base-line.
In future work, we intend to conduct experiments to seewhether the smoothing approaches chosen are successful inparsing without introducing spurious dependency relations.We would also like to re-integrate the emission probability(that is, the word content selection model).
We are also inthe process of developing a measure of consistency.
Finally,we intend to provide a comparison evaluation with Barzilay?sInformation Fusion work.8 AcknowledgementsThis work was funded by the Centre for Language Technol-ogy at Macquarie University and the CSIRO Information andCommunication Technology Centre.
We would like to thankthe research groups of both organisations for useful com-ments and feedback.References[Bangalore and Rambow, 2000] Srinivas Bangalore andOwen Rambow.
Exploiting a probabilistic hierarchicalmodel for generation.
In Proceedings of the 18th Con-ference on Computational Linguistics (COLING?2000),July 31 - August 4 2000, Universita?t des Saarlandes,Saarbru?cken, Germany, 2000.
[Barzilay et al, 1999] Regina Barzilay, Kathleen R. McKe-own, and Michael Elhadad.
Information fusion in the con-text of multi-document summarization.
In Proceedings ofthe 37th conference on Association for Computational Lin-guistics, pages 550?557, Morristown, NJ, USA, 1999.
As-sociation for Computational Linguistics.
[Collins, 1996] Michael John Collins.
A new statisticalparser based on bigram lexical dependencies.
In ArivindJoshi and Martha Palmer, editors, Proceedings of theThirty-Fourth Annual Meeting of the Association for Com-putational Linguistics, pages 184?191, San Francisco,1996.
Morgan Kaufmann Publishers.[Daume?
III and Marcu, 2004] Hal Daume?
III and DanielMarcu.
A phrase-based hmm approach to docu-ment/abstract alignment.
In Dekang Lin and Dekai Wu,editors, Proceedings of EMNLP 2004, pages 119?126,Barcelona, Spain, July 2004.
Association for Computa-tional Linguistics.
[Forney, 1973] G. David Forney.
The viterbi algorithm.
Pro-ceedings of The IEEE, 61(3):268?278, 1973.
[Jing and McKeown, 1999] Hongyan Jing and KathleenMcKeown.
The decomposition of human-written sum-mary sentences.
In Research and Development in Infor-mation Retrieval, pages 129?136, 1999.
[Kittredge and Mel?cuk, 1983] Richard I. Kittredge and IgorMel?cuk.
Towards a computable model of meaning-textrelations within a natural sublanguage.
In IJCAI, pages657?659, 1983.
[Knight and Marcu, 2002] Kevin Knight and Daniel Marcu.Summarization beyond sentence extraction: a probabilis-tic approach to sentence compression.
Artif.
Intell.,139(1):91?107, 2002.
[Langkilde and Knight, 1998] Irene Langkilde and KevinKnight.
The practical value of N-grams in derivation.
InEduard Hovy, editor, Proceedings of the Ninth Interna-tional Workshop on Natural Language Generation, pages248?255, New Brunswick, New Jersey, 1998.
Associationfor Computational Linguistics.
[Manning and Schu?tze, 1999] Christopher D. Manning andHinrich Schu?tze.
Foundations of Statistical Natural Lan-guage Processing.
The MIT Press, Cambridge, Mas-sachusetts, 1999.
[Steedman, 2000] Mark Steedman.
The syntactic process.MIT Press, Cambridge, MA, USA, 2000.
[Wan et al, 2003] Stephen Wan, Mark Dras, Cecile Paris,and Robert Dale.
Using thematic information in statisticalheadline generation.
In The Proceedings of the Workshopon Multilingual Summarization and Question Answeringat ACL 2003, Sapporo, Japan, July 2003.
[Witbrock and Mittal, 1999] Michael J. Witbrock andVibhu O. Mittal.
Ultra-summarization (poster abstract):a statistical approach to generating highly condensednon-extractive summaries.
In SIGIR ?99: Proceedings ofthe 22nd annual international ACM SIGIR conference onResearch and development in information retrieval, pages315?316, New York, NY, USA, 1999.
ACM Press.
