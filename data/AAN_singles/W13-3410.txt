Proceedings of the Fourth Workshop on Teaching Natural Language Processing, pages 61?65,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsLearning Computational Linguistics through NLP Evaluation Events:the experience of Russian evaluation initiativeAnastasia Bonch-OsmolovskayaNational Research UniversityHigher School of Economics101000, Myasnickaya, 20Moscow, Russiaabonch@gmail.comOlga LyashevskayaNational Research UniversityHigher School of Economics,101000, Myasnickaya, 20Moscow, Russiaolesar@gmail.comSvetlana ToldovaMoscow State University,Faculty of Philology119991, Leninskie Gory,1 Hum.
Bldg., Moscow, Russiatoldova@yandex.ruAbstractWe present in the paper our experience ofinvolving the students of the departmentof theoretical and computational linguis-tics of the Moscow State University intofull-cycle activities of preparing and eval-uating the results of the NLP Evaluationforums, held in 2010 and 2012 in Russia.The forum of 2010 started as a new initia-tive and was the first independent evalua-tion of morphology parsers for Russian inRussia.
At the same time the forum cam-paign has been a source of a successfulacademic course which resulted in a close-knit student team, strong enough to im-plement the two-year research for the sec-ond forum on syntax, held in 2012.
Thenew forum of anaphora (to be held in2014) is now prepared mostly by students.1 IntroductionRussian computational linguistics counts morethan 50 years history, started with the first MTresearch in 1955 (Bar-Hilel 1960).
Still up to thefirst decade of the 21 century all the researchgroups ?
those, inheriting the Soviet tradition, aswell as the new commercial industry labs - exist-ed in a disjoined mode.
The absence of thestate-of-the-art investigation on the performanceof parsers for Russian as well as on the effect ofdifferent computational methods for Russian richmorphology impeded the teaching of computa-tional linguistics, making it dilettantish.
It?s notsurprising that the first initiative of the Evalua-tion forum emerged in the academy.
The aca-demic status of the initiative also guaranteed itsindependence.
The complete cycle of the forumin 2010 on morphology, starting with mark-upscheme of the Gold Standard and ending the finalpaper preparation has served as a basis for acourse in computational linguistics with excel-lent set of tasks for students to carry out.
Theproblem of the first year experience was insuffi-cient communication with all the participantsduring the forum preparation.
This is very im-portant for the pioneer status of the forum andalso the educational perspective of the initiative.That?s why the two year period of forum prepa-ration has been chosen.
The task of the first yearis to prepare and hold a round-table open to allthe potential participants where the basic deci-sions on the test collections, tasks, mark-up andevaluation process are made.
The task of the sec-ond year is the evaluation forum itself and thepreparation of an overview paper.
Below we willfocus on the educational process: we will de-scribe student tasks during the complete cycle ofthe evaluation forum preparation.
The consistentpractical aim of the course distinguishes in frommost of the courses in computational linguistics(Hearst, 2005; Liddy and McCracken, 2005;Baldridge and Erk, 2008).
This is a course inNLP evaluation which, as we believe, gives stu-dents very useful theoretical and practical skillsof making sound and deliberate decisions duringlinguistic data analyses.
The main idea of thecourse is to involve students into solving ?real-life?
expert tasks, and to show them multiple ap-proaches to mark-up and data analysis.
Wewould like to underline that the practical value ofthe course: students not only do the routine as-sessment procedure, but analyze the best practic-es and create the design of the forum.
The courseis organized as follows: students complete tasksat home and discuss the results at class with two61or three instructors.
The experienced studentsmay act as instructors also.
The class ends bycollective presentation at the conference.
Stu-dents work in small teams of 2 or 3 persons, eachteam doing its piece of work.
All the studentshave strong background in theoretical linguisticsand math, some students have good program-ming skills.
The main stages of the first year are:1) getting theoretical background 2) first mark-up experience and proto-gold standard 3) feed-back from the participants 4) round-table prepa-ration.
The second year consists of the followingstage: 5) preparing Gold Standard 6) resultsevaluation 7) final paper preparation.
These stag-es correspond to the four semesters of specialcourses on NLP, home task activities, hands-onstudent activities and practice in academic writ-ing.
Each of the stage will be discussed below.The corresponding teaching methods are de-scribed in a separate section.2 Background taskThe first task students have to complete is tostudy theoretical background which consists of a)actual evaluation practices b) state of art of Rus-sian NLP systems that can potentially participatein the forum.
Primarily students study reports ofthe main evaluation forums that have been heldon the current task.
The topics to be discussed inclass are: the types of system running the compe-titions (statistical, rule-based, hybrid), their theo-retical linguistic basis: for example, HPSGparsers or dependency parsers for syntax; the testcollections, their sources, size and mark-upscheme; the tasks and their metrics; the perfor-mance rate.
The students have to find the an-swers on all this questions making their waythrough exhaustive reports, they have to draw outsome common grounds to be compared and ana-lyzed.
For example for the syntax forum (Gar-eyshina et al 2012) tree-banks of different lan-guages and structure types has been analyzed andcompared.
The very important point of this stageis that it results in collective determining someideal scenario of the future forum which is to beinevitably corrected by performing the secondinvestigation ?
examining all the informationabout the potential participants, such as collect-ing and reading all the related papers, testingdemos or installing the open-source resources.For example, the main problem for the morphol-ogy forum was to determine a mark-up schemethat would be convenient for all the participants(Lyasevskaya et al 2010).
This problem is cru-cial because of Russian rich morphology and thevariety of theoretical traditions different systemsrest upon.
The investigation of syntactic parsing(all the systems that took part in the forum, usedependency parsing) revealed the impossibilityto compare the types of syntactic relations speci-fied by different systems.
The fact is not surpris-ing bearing in mind that there is no open tree-bank such as Penn tree bank to be trained on forRussian.
The workshop devoted to comparingdifferent syntactic parsing outputs has been ex-hausting but fruitful: we arrived to a decision thatthe main task of the forum should include onlyevaluating what syntactic heads were to bemarked by the participants.
Correctness of pars-ing the whole sentence was decided to count asirrelevant.
Only the choice of the head was eval-uated.
We would like to underline that the designand the scenario of the forums are always deter-mined as a result of individual work of studentgroups together with collective analysis andsumming-up conclusions.
Finally the last but notthe least object of this task is to juxtapose theo-retical and computational linguistics: studentshave to analyze the scope of underlining linguis-tic phenomena and to compare them with appliedrealizations in NLP.
The more sophisticated lin-guistic task is in focus, the more interesting top-ics are raised in class.
For example, the examina-tion of different principles of anaphoric resolu-tion this year showed the limits of applied tasksand solutions (particularly in discourse anaphoraresolution and identifying lexical coherence de-termined extralinguistically), and revealed theperspectives of future development in NLP andartificial intelligence.
The analysis is then partlyfulfilled in Gold Standard mark-up.
The schemeis always broader then it has to be for the evalua-tion task.
The important additional outcome ofsuch corpus mark-up is to prepare some newopen resource that can serve also for corpus lin-guistic and theoretical linguistic research.3 First mark-up experience and firstfeedbackAs it has been noted earlier the theoretical stageof the course results in the forum scenario andthe mark-up scheme for the Gold Standard.
Atthe next stage students begin by making mark-upon a few selected texts.
Each text is marked-upwith several students and all the cases of interan-notator discrepancy have to be analyzed and dis-cussed in class.
The discussion leads to formulat-ing more distinct mark-up criteria as well as to62determining the cases which should not be eval-uated.
The mark-up is made by special tool pro-grammed by the students with good program-ming skills.
The specification of requirements forthe tool is also the task to be performed by stu-dents.
The first mark-up staging is all in one test-ing the mark-up scheme, elaboration of the eval-uation framework and metrics as well as tech-nical testing of the tool.
As a result some small(usually 100 sentences) ?pre-gold?
standard ismade.
Then these sentences (both a non-markedand a marked-up variant) are sent to the partici-pants who had by this time made a claim on theirparticipation in the forum.
The idea is to get pre-liminary feedback to control all the previous de-cisions that have been made about the forum dur-ing the theoretical stage of the course.
The par-ticipants have the possibility to estimate themark-up scheme and the assessment scheme andpresent some on-going results of this first smalltest.When we receive the first feedback from the par-ticipants, we turn to the analysis of the systempossible mistakes.
Our aim at this stage is not toevaluate the systems but to exclude all caseswhich are either theoretically unclear (i.e.
thehead of the conjunction group) or cannot be re-solved by the system (a ?boy sees the girl withthe telescope?
problem) or too difficult to unify(i.e.
choice of the basic infinitive for Russianaspectual verbal pairs).All this activities need special clarification: Rus-sian is a so called ?poor resource?
language.
Theforum cannot use existing corpora as a trainingset.
This can violate the independence of evalua-tion results: some of the system had been trainedon these corpora while others had not.
So themain practice of our evaluation forums is to con-duct assessment on a Gold Standard subcorpuswhich normally includes about 800 randomlyselected sentences that have been manuallytagged.
Meanwhile the routine of manual taggingserves as an important practical exercise for stu-dents.4 The round-tableThe closing event of the first year is a round-table, held at the annual conference on computa-tional linguistics ?Dialogue?
(www.dialogue-21.ru).
The presentation is prepared and donemostly by students and contains all the topicsthat had been worked on during the previous pe-riod: all important background, proposals on theforum scenario and the result of the first evalua-tion experiment.
Usually most of the participantstake active part in the round-table.
This is be-sides all an exciting experience for students thathave an opportunity to make acquaintance withresearches from academy and industry, the op-portunity that can have far-reaching effect fortheir future career.
After the round table the workon the second part ?
the evaluation itself begins.5 The Gold Standard mark-up stageThe Gold Standard preparation stage includes:the final version of annotator instruction work-out, the tool for Gold Standard mark-up choiceor creation, Gold Standard annotators disagree-ment cases discussion, the final version of GoldStandard creation.For the Syntax and Anaphora forum the spe-cial tools were created for Gold Standard Mark-up.
These tools are suitable for annotators deci-sion comparison (Gareyshina et al 2012).
Thedesign of the tool was a special issue for discus-sion during the class.The Gold Standard is tagged manually usingthe worked-out tagging tool.
Each item (word,sentence, text (coreference chain)) is inde-pendently tagged by two experts-students, thendivergences are discussed, if any, and the com-mon decision is made.
Each pair of students isresponsible for the common decision in case ofdiscrepancy.
The discrepancies in pairs are writ-ten out in a special document.
The students final-ly work out the list of problematic cases for thecorresponding NLP tasks both from the point ofview of theory and practical decisions, e.g.
thetypical morphological ambiguity cases such asVerbal Adjective vs. Participle for Russian orproblems of Syntactic relation direction in caseof Numeral-Noun syntactic relation, etc.
Thecases are discussed during seminars.
Thus theannotator instruction is improved.
Then the an-notation is checked by the third expert (one ofthe tutors).
Such procedure allowed us to achievethree aims.
It helped to work out the algorithmfor semi-automate annotators?
mistakes detectionprocedure.
Then, we wanted to avoid ?overfit-ting?
: getting the experts used to common errorof the specific system and omitting errors by notnoticing them.
And last, tagging is supposed togive the experts the basic knowledge about diffi-cult cases and to help them form criteria forevaluating mismatches.636 The evaluation procedureThe stage of evaluation includes the creation aspecial tool for systems responses comparisonwith Gold Standard, the comparison of the outputof the parsers to the Gold Standard.The test sets usually are based on a Treebankused for the development of the parsers.
In ourcase there was no Gold Standard Treebank forRussian and there is no Gold Standard Corporawith coreference mark-up.
Moreover each sys-tem has its own theoretical and practical deci-sions due to the final purposes of the system.The students?
activity during this stage in-cludes: the automatic comparison tool creation(this is a task for a ?programming-oriented?
stu-dents), the special editor for system responsescomparison creation, the manual procedure ofsystem mismatches with Gold Standard analysis.The latter is an essential stage for Evaluation.As it was mentioned above there are systems?mismatches that should not be treated as mis-takes.
Thus this procedure includes the collectivedecision for a repertory of marks used by the an-notators for differentiating cases of mismatches,the mismatches discussion during joint seminars,the mismatches manual assessment.
All teams ofassessors (two students and a tutor) have theirown piece of a Gold Standard Corpora to check.Thus every team faces all kinds of difficulties;this principle provides the united consistent ap-proach to all the types of discrepancies.7 Teaching Methods and ScheduleThe Forum cycle takes one and a half of aca-demic years.
Thus we have a series of three Spe-cial seminars in one of the NLP fields.
Studentscould take part in all the stages of a Forum oronly in one of them.
The first part is mainly theo-retical.
They deepen their knowledge in theoreti-cal approaches to linguistic analysis; get ac-quainted with the approaches to the correspond-ing NLP task.
The other useful activities is aNLP software testing, the real systems discrep-ancy analysis.
The course is also good opportuni-ty to train academic reading skill.
The compari-son of systems outputs and the work out of Fo-rum parameters are good hands-on tasks.
Thiscourse is also a challenge for students to learnout how the theoretical principles interact withpractical system requirements.The second course is a practical one.
Its pri-mary aim is to work out and annotate the GoldStandard Corpus.
Thus this activity could betreated as a series of hands-on in classroom to-gether with exhaustive home-tasks.
The course isa project work in a team where IT-oriented stu-dents and linguistically-oriented students worktogether.
The practical result is an opened re-source such as Syntax Treebank consisting of800 sentences manually tagged.
One of the im-portant educational outputs of the seminar is theacquaintance with the repertory of the problemat-ic cases in a certain NLP field of study.The Third course is also practical one.
Besidesthe practical tasks of Systems mismatches evalu-ation this course also allows students to improvetheir Academic writing skills.
The output of thiscourse is not only the Systems evaluation as it isbut a scientific article describing the whole Fo-rum procedure as well.8 ConclusionsThe described above students activity as the or-ganizers of the Evaluation Forum, annotators andassessors has challenges for NLP education theenumerated below.The ?outputs?
for theoretical stage are the fol-lowing:?
the high-targeted, and thus highly moti-vated and deep acquaintance with the ap-proaches to the NLP tasks, existing re-sources in other languages, methods ofevaluation;?
the academic reading skills in NLP re-search field;?
the acquaintance with the different princi-ple of adaptation the linguistic theory tothe NLP task implementation.The practical-skill training output:?
the annotation skill?
the academic reading and writing skill?
the NLP evaluation skill?
the inter-discipline team-working.As it has been mentioned, ironically, the re-source poverty is a challenge for NLP educationwith Russian language in focus.
At start the pro-cedure of a particular NLP evaluation task forRussian is a terra incognita.
Before the Forumstarts the number and entry list of participants(and thus the competing technologies) are notpredictable.
Doing something, that nobody hasdone before, is always a superb motivation forstudent involvement.ReferencesBaldridge, Jason, and Katrin Erk.
2008.
Teachingcomputational linguistics to a large, diverse studentbody: courses, tools, and interdepartmental interac-64tion.
Proceedings of the Third Workshop on Issuesin Teaching Computational Linguistics.P.
1-8.
Association for Computational LinguisticsStroudsburg, PA, USA.Bar-Hillel, Yehoshua.
1960.
The present status ofautomatic translation of languages.
Advances incomputers 1, no.
1 P. 91-163..Hearst, Marti.
2005.
Teaching applied natural lan-guage processing: Triumphs and tribulations.
Pro-ceedings of the Second ACL Workshop on Effec-tive Tools and Methodologies for Teaching NaturalLanguage Processing and Computational Linguis-tics.
P. 1-8.
Ann Arbor, Michigan, June.
Associa-tion for Computational Linguistics.Liddy, Elizabeth D., and Nancy J. McCracken.
2005.Hands-on NLP for an interdisciplinary audi-ence.
Proceedings of the Second ACL Workshop onEffective Tools and Methodologies for TeachingNatural Language Processing and ComputationalLinguistics.
P. 62-28.
Association for Computa-tional Linguistics.Gareyshina Anastasia, Ionov Maxim, Lyash-evskaya Olga,  Privoznov Dmitry, Sokolova Elena,Toldova Svetlana.
2012.
RU-EVAL-2012: Evaluat-ing Dependency Parsers for Russian.
Proceedingsof COLING 2012: Posters.
P. 349-360.URL: http://www.aclweb.org/anthology/C12-2035.Lasevskaya Olga, Astaf'eva Irina, Bonch-Osmolovskaya Anastasia, Gareyshina Anastasia, Grishina Julia,D'jachkov Vadim, Ionov Maxim, Koroleva Anna,Kudrinsky Maxim, Lityagina Anna, Luchina Elena,Sidorova Evgenia, Toldova Svetlana,Savchuk Svetlana., Koval' Sergej.
2010.
Evaluationof the automated text analysis: POS-tagging forRussian.
[Morphological Ananlysis Ocenkametodov avtomaticheskogo analiza teksta: mor-fologicheskije parsery russkogo jazyka.]
Proceed-ings of the International Conference on Computa-tional Linguistics Dialogue-2010.
P. 318-327.65
