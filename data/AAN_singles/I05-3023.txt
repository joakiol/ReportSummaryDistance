Perceptron Learning for Chinese Word SegmentationYaoyong Li?, Chuanjiang Miao?, Kalina Bontcheva?, Hamish Cunningham?
?Department of Computer Science, The University of Sheffield, Sheffield, S1 4DP, UK{yaoyong,kalina,hamish}@dcs.shef.ac.uk?Institute of Chinese Information Processing, Beijing Normal University, Beijing, 100875, Chinamiaochj@bnu.edu.cnAbstractWe explored a simple, fast and effectivelearning algorithm, the uneven marginsPerceptron, for Chinese word segmen-tation.
We adopted the character-basedclassification framework and trans-formed the task into several binary clas-sification problems.
We participatedthe close and open tests for all the fourcorpora.
For the open test we only usedthe utf-8 code knowledge for discrimi-nation among Latin characters, Arabicnumbers and all other characters.
Oursystem performed well on the as, cityuand msr corpora but was clearly worsethan the best result on the pku corpus.1 IntroductionWe participated in the closed and open tests forall the four corpora, referred to as, cityu, msr andpku, respectively.
We adopted the character-basedmethodology for Chinese word segmentation, thatprocessed text character by character.
We ex-plored a simple and effective learning algorithm,the Perceptron with Uneven Margins (PAUM) forChinese word segmentation task.For the open task, we only used the minimal ex-ternal information ?
the utf-8 code knowledge todistinguish Latin characters and Arabic numbersfrom other characters, justified by the fact thatthe English text requires no segmentation sincethey has been segmented already, and another factthat any Arabic number in one particular contextshould have the same segmentation.2 Character Based Chinese WordSegmentationWe adopted the character based methodology forChinese word segmentation, in which every char-acter in a sentence was checked one by one tosee if it was a word on its own or it was begin-ning, middle, or end character of a multi-characterword.
In contrast, another commonly used strat-egy, the word based methodology segments a Chi-nese sentence into the words in a pre-definedword list possibly with probability informationabout each word, according to some maximumprobability criteria ( see e.g.
Chen (2003)).
Theperformance of word based segmentation is de-pendent upon the quality of word list used, whilethe character based method does not need anyword list ?
it segments a sentence only based onthe characters in the sentence.Using character based methodology, we trans-form the word segmentation problem into fourbinary classification problems, corresponding tosingle-character word, the beginning, middle andend character of multi-character word, respec-tively.
For each of the four classes a classifier waslearnt from training set using the one vs. all othersparadigm, in which every character in the train-ing data belonging to the class considered was re-garded as positive example and all other charac-ters were negative examples.After learning, we applied the four classifiers toeach character in test text and assigned the char-acter the class which classifier had the maximaloutput among the four.
This kind of strategy hasbeen widely used in the applications of machinelearning to named entity recognition and has also154been used in Chinese word segmentation (Xueand Shen, 2003).
Finally a word delimiter (often ablank space, depending on particular corpus) wasadded to the right of one character if it was not thelast character of a sentence and it was predictedas end character of word or as a single characterword.3 Learning AlgorithmPerceptron is a simple and effective learning al-gorithm.
For a binary classification problem, itchecks the training examples one by one by pre-dicting their labels.
If the prediction is correct,the example is passed; otherwise, the example isused to correct the model.
The algorithm stopswhen the model classifies all training examplescorrectly.
The margin Perceptron not only classi-fies every training example correctly but also out-puts for every training example a value (beforethresholding) larger than a predefined parameter(margin).
The margin Perceptron has better gen-eralisation capability than the standard Percep-tron.
Li et al (2002) proposed the Perceptron al-gorithm with uneven margins (PAUM) by intro-ducing two margin parameters ?+ and ??
into theupdate rules for the positive and negative exam-ples, respectively.
Two margin parameters allowthe PAUM to handle imbalanced datasets betterthan both the standard Perceptron and the marginPerceptron.
PAUMhas been successfully used fordocument classification and information extrac-tion (Li et al, 2005).We used the PAUM algorithm to train a clas-sifier for each of four classes for Chinese wordsegmentation.
For one test example, the output ofthe Perceptron classifier before thresholding wasused for comparison among the four classifiers.The important parameters of the learning algo-rithm are the uneven margins parameters ?+ and??.
In all our experiments ?+ = 20 and ??
= 1were used.Table 1 presents the results for each of thefour classification problems, obtained from 4-foldcross-validation on training set.
Not surprisingly,the classification for middle character of multi-character word was much harder than other threeclassification problems, since middle character ofChinese word is less characteristic than beginningor end character or single-character word.
On theother hand, improvement on the classification formiddle character, while keeping the performancesof other classification, would improve the overallperformance of segmentation.Table 1: Results for each of the four classifiers:F1 (%) averaged over 4-fold cross-validation ontraining sets of the four corpora.
C1, C2 andC3 refer to the classifier for beginning, middleand end character of multi-character word, re-spectively, and C4 refers to the classifier for singlecharacter word.C1 C2 C3 C4as 95.64 90.07 95.47 95.27cityu 96.64 90.06 96.43 95.14msr 96.36 89.79 96.00 94.99pku 96.09 89.99 96.18 94.12Support vector machines (SVM) is a popularlearning algorithm, which has been successfullyapplied to many classification problems in naturallanguage processing.
Similar to the PAUM, SVMis a maximal margin algorithm.
Table 2 presentsa comparison of performances and computationtimes between the PAUM and the SVM with lin-ear kernel1 on three subsets of cityu corpora withdifferent sizes.
The performance of SVM wasbetter than the PAUM.
However, the larger thetraining data was, the closer the performance ofPAUM to that of SVM.
On the other hand, SVMtook much longer computation time than PAUM.As a matter of fact, we have run the SVM withlinear kernel on the whole cityu training corpususing 4-fold cross-validation for one month and ithas not finished yet.
In contrast, PAUM just tookabout one hour to run the same experiment.4 Features for Each CharacterIn our system every character was regarded asone instance for classification.
The features forone character were the character form itself andthe character forms of the two preceding andthe two following characters of the current one.In other word, the features for one character c0were the character forms from a context win-1The SVMlight package version 5.0, available fromhttp://svmlight.joachims.org/, was used to learn the SVMclassifiers in our experiments.155Table 2: Comparison of the Perceptron with SVMfor Chinese word segmentation: averaged F1 (%)over the 4-fold cross-validation on three subsetsof cityu corpus and the computation time (in sec-ond) for each experiment.
The three subsets have100, 1000 and 5000 sentences, respectively.100 1000 5000PAUM 73.55 78.00 88.084s 14s 92sSVM 75.50 79.15 88.78227s 3977s 49353sdow centering at c0 and containing five char-acters {c?2, c?1, c0, c1, c2} in a sentence.
Ourexperiments on training data showed that co-occurrences of characters in the context win-dow were helpful.
Taking account of all co-occurrences of characters in context window isequivalent to using a quadratic kernel in Percep-tron, while not using any co-occurrence amountsto a linear kernel.
Actually we can only use partof co-occurrences as features, which can be re-garded as some kind of semi-quadratic kernel.Table 3 compares the three types of ker-nel for Perceptron, where for the semi-quadratic kernel we used the co-occurrencesof characters in context window as thoseused in (Xue and Shen, 2003), namely{c?2c?1, c?1c0, c0c1, c1c2, c?1c1}.
It wasshown that the quadratic kernel gave much betterresults than linear kernel and the semi-quadratickernel was slightly better than fully quadratic ker-nel.
Semi-quadratic kernel also led to less featureand less computation time than fully quadratickernel.
Therefore, this kind of semi-quadratickernel was used in our submissions.Table 3: Comparisons between different kernelsfor Perceptron: F1 (%) averaged over 4-foldcross-validation on three training sets.linear quadratic semi-quadraticcityu 81.30 94.78 95.13msr 79.80 94.78 94.93pku 82.33 94.80 95.05Actually it has been noted that quadratic ker-nel for Perceptron, as well as for SVM, per-formed better than linear kernel for informa-tion extraction and other NLP tasks (see e.g.Carreras et al (2003)).
However, quadratic ker-nel was usually implemented in dual form forPerceptron and it took very long time for train-ing.
We implemented the quadratic kernel forPerceptron in primal form by encoding the linearand quadratic features into feature vector explic-itly.
Actually our implementation performed evenslightly better than the Perceptron with quadratickernel as we used only part of quadratic features,and it was still as efficient as the Perceptron withlinear kernel.5 Open TestWhile closed test required the participants only touse the information presented in training material,open test allowed to use any external informationor resources besides the training data.
In our sub-missions for the open test we just used the min-imal external information, namely the utf-8 codeknowledge for identifying a piece of English textor an Arabic number.
and What we did by us-ing this kind of knowledge was to pre-process thetext by replacing each piece of English text witha symbol ?E?
and replacing every Arabic num-ber with another symbol ?N?.
This kind of pre-processing resulted in a smaller training data andless computation time and yet slightly better per-formance on training data, as shown in Table 4which compares the results of collapsing the En-glish text only and collapsing both the Englishtext and Arabic number with those for closed test.Table 4 also presents the 95% confidence intervalsfor the F-measures.6 Results on Test DataTable 5 presents our official results on test corporafor both close and open tests.
First, comparingwith the results in Table 4, the results on test setare significantly different from the result using 4-fold cross validation on training set for all the fourcorpora.
The test result was better than the resultson training set for the msr corpus but was worsefor other three corpora, especially for the pku cor-pora.
We suspected that this may be caused bydifference between training and test data, whichneeds further investigation.156Table 4: Comparisons between the results forclose and open tests: averaged F1 (%) and the95% confidence interval on the 4-fold cross-validation on the training sets of four corpora andthe computation time (in hour) for each experi-ment.
?English?
means only collapsing Englishtexts and ?E & N?
means collapsing both Englishtexts and Arabic numbers.close test English E & Nas 95.53?0.46 95.65?0.47 95.78?0.468.88h 7.66h 7.07hcityu 95.13 ?1.49 95.25 ?1.48 95.25 ?1.481.03h 0.86h 0.82hmsr 94.92 ?0.36 94.98 ?0.40 95.00 ?0.392.62h 1.69h 1.62hpku 95.05 ?0.43 95.08 ?0.36 95.15 ?0.460.70h 0.63h 0.60hSecondly, the test results for close and opentests are close to each other on other three corporaexcept the pku corpora, for which the result foropen test is clearly better than that for close test.This was mainly because of different encodingof Arabic number in training and test sets of thepku corpus.
Since Arabic number was encoded inthree bytes in training set but was encoded in onebyte in test set for the pku corpora, for close testthe trained model for Arabic number was not ap-plicable to the Arabic numbers in test set.
How-ever, for open test, as we replaced Arabic num-ber with one symbol in both training and test sets,the different encoding of Arabic number in train-ing and test sets could not cause any problem atall, which led to better result.
On the other hand,our pre-processing with respect to the English textand Arabic numbers seemed have slightly effecton the F-measure for other three corpora.Finally, comparing with the results of closedtest from other participants, our F1 figures wereno more than 0.008 lower than the best ones onthe as, cityu and msr corpora, but was 0.023 lowerthan the best one on the pku corpus.7 ConclusionWe applied the uneven margins Perceptron to Chi-nese word segmentation.
The learning algorithmis simple, fast and effective.
The results obtainedTable 5: The official results on test set: F-measure(%) for close and open tests, respectively.as cityu msr pkuclose 94.4 93.6 95.6 92.7open 94.8 93.6 95.4 93.8are encouraging.The performance of Perceptron was close tothat of the SVM on Chinese word segmentationfor large training data.
On the other hand, thePerceptron took much less computation time thanSVM.We implemented the Perceptron with semi-quadratic kernel in primal form.
Our implemen-tation was both effective and efficient.Our system performed well for the three of fourcorpora, as, cityu and msr corpora.
But it wassignificantly worse than the best result on the pkucorpora, which needs further investigation.AcknowledgementsThis work is supported by the EU-funded SEKTproject (http://www.sekt-project.org).ReferencesX.
Carreras, L. Ma`rquez, and L. Padro?.
2003.
Learn-ing a perceptron-based named entity chunker viaonline recognition feedback.
In Proceedings ofCoNLL-2003, pages 156?159.
Edmonton, Canada.A.
Chen.
2003.
Chinese Word Segmentation UsingMinimal Linguistic Knowledge.
In Proceedings ofthe 2nd SIGHAN Workshop on Chinese LanguageProcessing.Y.
Li, H. Zaragoza, R. Herbrich, J. Shawe-Taylor, andJ.
Kandola.
2002.
The Perceptron Algorithm withUneven Margins.
In Proceedings of the 9th Inter-national Conference on Machine Learning (ICML-2002), pages 379?386.Y.
Li, K. Bontcheva, and H. Cunningham.
2005.
Us-ing UnevenMargins SVM and Perceptron for Infor-mation Extraction.
In Proceedings of Ninth Confer-ence on ComputationalNatural Language Learning(CoNLL-2005).N.
Xue and L. Shen.
2003.
Chinese Word Segmen-tation as LMR Tagging.
In Proceedings of the 2ndSIGHAN Workshop on Chinese Language Process-ing.157
