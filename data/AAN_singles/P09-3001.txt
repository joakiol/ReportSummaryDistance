Proceedings of the ACL-IJCNLP 2009 Student Research Workshop, pages 1?9,Suntec, Singapore, 4 August 2009.c?2009 ACL and AFNLPSense-based Interpretation of Logical Metonymy Using a StatisticalMethodEkaterina ShutovaComputer LaboratoryUniversity of Cambridge15 JJ Thomson AvenueCambridge CB3 0FD, UKEkaterina.Shutova@cl.cam.ac.ukAbstractThe use of figurative language is ubiqui-tous in natural language texts and it is aserious bottleneck in automatic text un-derstanding.
We address the problem ofinterpretation of logical metonymy, usinga statistical method.
Our approach origi-nates from that of Lapata and Lascarides(2003), which generates a list of non-disambiguated interpretations with theirlikelihood derived from a corpus.
We pro-pose a novel sense-based representationof the interpretation of logical metonymyand a more thorough evaluation methodthan that of Lapata and Lascarides (2003).By carrying out a human experiment weprove that such a representation is intu-itive to human subjects.
We derive a rank-ing scheme for verb senses using an unan-notated corpus, WordNet sense numberingand glosses.
We also provide an accountof the requirements that different aspec-tual verbs impose onto the interpretationof logical metonymy.
We tested our sys-tem on verb-object metonymic phrases.
Itidentifies and ranks metonymic interpreta-tions with the mean average precision of0.83 as compared to the gold standard.1 IntroductionMetonymy is defined as the use of a word or aphrase to stand for a related concept which is notexplicitly mentioned.
Here are some examples ofmetonymic phrases:(1) The pen is mightier than the sword.
(2) He played Bach.
(3) He drank his glass.
(Fass, 1991)(4) He enjoyed the book.
(Lapata and Lascarides,2003)(5) After three martinis John was feeling well.
(Godard and Jayez, 1993)The metonymic adage in (1) is a classical ex-ample.
Here the pen stands for the press and thesword for military power.
In the following exam-ple Bach is used to refer to the composer?s musicand in (3) the glass stands for its content, i.e.
theactual drink (beverage).The sentences (4) and (5) represent a varia-tion of this phenomenon called logical metonymy.Here both the book and three martinis have even-tive interpretations, i.e.
the noun phrases standfor the events of reading the book and drinkingthree martinis respectively.
Such behaviour istriggered by the type requirements the verb (orthe preposition) places onto its argument.
Thisis known in linguistics as a phenomenon of typecoercion.
Many existing approaches to logicalmetonymy explain systematic syntactic ambiguityof metonymic verbs (such as enjoy) or preposi-tions (such as after) by means of type coercion(Pustejovsky, 1991; Pustejovsky, 1995; Briscoeet al, 1990; Verspoor, 1997; Godard and Jayez,1993).Logical metonymy occurs in natural languagetexts relatively frequently.
Therefore, its auto-matic interpretation would significantly facilitatethe task of many NLP applications that requiresemantic processing (e.g., machine translation,information extraction, question answering andmany others).
Utiyama et al (2000) followed byLapata and Lascarides (2003) used text corpora toautomatically derive interpretations of metonymicphrases.1Utiyama et al (2000) used a statistical modelfor the interpretation of general metonymies forJapanese.
Given a verb-object metonymic phrase,such as read Shakespeare, they searched for en-tities the object could stand for, such as plays ofShakespeare.
They considered all the nouns co-occurring with the object noun and the Japaneseequivalent of the preposition of.
Utiyama and hiscolleagues tested their approach on 75 metonymicphrases taken from the literature and reported aprecision of 70.6%, whereby an interpretation wasconsidered correct if it made sense in some imag-inary context.Lapata and Lascarides (2003) extend Utiyama?sapproach to interpretation of logical metonymiescontaining aspectual verbs (e.g.
begin the book)and polysemous adjectives (e.g.
good meal vs.good cook).
Their method generates a list of in-terpretations with their likelihood derived from acorpus.Lapata and Lascarides define an interpretationof logical metonymy as a verb string, which is am-biguous with respect to word sense.
Some of thesestrings indeed correspond to paraphrases that a hu-man would give for the metonymic phrase.
Butthey are not meaningful as such for automatic pro-cessing, since their senses still need to be disam-biguated in order to obtain the actual meaning.
Forexample, compare the grab sense of take vs. itsfilm sense for the metonymic phrase finish video.It is obvious that only the latter sense is a correctinterpretation.We extend the experiment of Lapata and Las-carides by disambiguating the interpretations withrespect to WordNet (Fellbaum, 1998) synsets (forverb-object metonymic phrases).
We propose anovel ranking scheme for the synsets using anon-disambiguated corpus, address the issue ofsense frequency distribution and utilize informa-tion from WordNet glosses to refine the ranking.We conduct and experiment to show that ourrepresentation of a metonymic interpretation as asynset is intuitive to human subjects.
In the dis-cussion section we provide an overview of theconstraints on logical metonymy pointed out inlinguistics literature, as well as proposing someadditional constraints (e.g.
on the type of themetonymic verb, on the type of the reconstructedevent, etc.
)Metonymic Phrase Interpretations Log-probabilityfinish video film -19.65edit -20.37shoot -20.40view -21.19play -21.29stack -21.75make -21.95programme -22.08pack -22.12use -22.23watch -22.36produce -22.37Table 1: Interpretations of Lapata and Lascarides(2003) for finish video2 Lapata and Lascarides?
MethodThe intuition behind the approach of Lapata andLascarides is similar to that of Pustejovsky (1991;1995), namely that there is an event not explic-itly mentioned, but implied by the metonymicphrase (begin to read the book, or the meal thattastes good vs. the cook that cooks well).
Theyused the British National Corpus (BNC)(Burnard,2007) parsed by the Cass parser (Abney, 1996) toextract events (verbs) co-occurring with both themetonymic verb (or adjective) and the noun inde-pendently and ranked them in terms of their like-lihood according to the data.
The likelihood of aparticular interpretation is calculated using the fol-lowing formula:P (e, v, o) =f(v, e) ?
f(o, e)N ?
f(e), (1)where e stands for the eventive interpretation ofthe metonymic phrase, v for the metonymic verband o for its noun complement.
f(e), f(v, e)and f(o, e) are the respective corpus frequencies.N =?if(ei) is the total number of verbs in thecorpus.
The list of interpretations Lapata and Las-carides (2003) report for the phrase finish video isshown in Table 1.Lapata and Lascarides compiled their test set byselecting 12 verbs that allow logical metonymy1from the lexical semantics literature and combin-ing each of them with 5 nouns.
This yields 60phrases, which were then manually filtered, ex-cluding 2 phrases as non-metonymic.They compared their results to paraphrasejudgements elicited from humans.
The subjectswere presented with three interpretations for each1attempt, begin, enjoy, finish, expect, postpone, prefer, re-sist, start, survive, try, want2metonymic phrase (from high, medium and lowprobability ranges) and were asked to associate anumber with each of them reflecting how goodthey found the interpretation.
They report a cor-relation of 0.64, whereby the inter-subject agree-ment was 0.74.
It should be noted, however, thatsuch an evaluation scheme is not very informa-tive as Lapata and Lascarides calculate correlationonly on 3 data points for each phrase out of manymore yielded by the model.
It fails to take intoaccount the quality of the list of top interpreta-tions, although the latter is deemed to be the aim ofsuch applications.
In comparison the fact that La-pata and Lascarides initially select the interpreta-tions from high, medium or low probability rangesmakes the task significantly easier.3 Alternative Interpretation of LogicalMetonymyThe approach of Lapata and Lascarides (2003)produces a list of non-disambiguated verbs, essen-tially just strings, representing possible interpreta-tions of a metonymic phrase.
We propose an alter-native representation of metonymy interpretationconsisting of a list of senses that map to WordNetsynsets.
However, the sense-based representationbuilds on the list of non-disambiguated interpreta-tions similar to the one of Lapata and Lascarides.Our method consists of the following steps:?
Step 1 Use the method of Lapata and Las-carides (2003) to obtain a set of candidate in-terpretations (strings) from a non-annotatedcorpus.
We expect our reimplementation ofthe method to extract data more accurately,since we use a more robust parser (RASP(Briscoe et al, 2006)), take into account moresyntactic structures (coordination, passive),and extract our data from a newer version ofthe BNC.?
Step 2 Map strings to WordNet synsets.
Wenoticed that good interpretations in the listsyielded by Step 1 tend to form coherent se-mantic classes (e.g.
take, shoot [a video] vs.view, watch [a video]).
We search the listfor verbs, whose senses are in hyponymy andsynonymy relations with each other accord-ing to WordNet and store these senses.?
Step 3 Rank the senses, adopting Zipfiansense frequency distribution and using theinitial string likelihood as well as the infor-mation from WordNet glosses.Sense disambiguation is essentially performedin both Step 2 and Step 3.
One of the challengesof our task is that we use a non-disambiguated cor-pus while ranking particular senses.
This is due tothe fact that there is no word sense disambiguatedcorpus available, which would be large enough toreliably extract statistics for metonymic interpre-tations.4 Extracting Ambiguous Interpretations4.1 Parameter EstimationWe used the method developed by Lapata andLascarides (2003) to create the initial list of non-disambiguated interpretations.
The parameters ofthe model were estimated from the British Na-tional Corpus (BNC) (Burnard, 2007) that wasparsed using the RASP parser of Briscoe et al(2006).
We used the grammatical relations (GRs)output of RASP for BNC created by Andersen etal.
(2008).
In particular, we extracted all directand indirect object relations for the nouns fromthe metonymic phrases, i.e.
all the verbs that takethe head noun in the compliment as an object (di-rect or indirect), in order to obtain the counts forf(o, e).
Relations expressed in the passive voiceand with the use of coordination were also ex-tracted.
The verb-object pairs attested in the cor-pus only once were discarded, as well as the verbbe, since it does not add any semantic informa-tion to the metonymic interpretation.
In the caseof indirect object relations, the verb was consid-ered to constitute an interpretation together withthe preposition, e.g.
for the metonymic phrase en-joy the city the correct interpretation is live in asopposed to live.As the next step we need to identify all possibleverb phrase (VP) complements to the metonymicverb (both progressive and infinitive), which rep-resent f(v, e).
This was done by searching forxcomp relations in the GRs output of RASP, inwhich our metonymic verb participates in any ofits inflected forms.
Infinitival and progressivecomplement counts were summed up to obtain thefinal frequency f(v, e).After the frequencies f(v, e) and f(o, e) wereobtained, possible interpretations were ranked ac-cording to the model of Lapata and Lascarides(2003).
The top interpretations for the metonymic3finish video enjoy bookInterpretations Log-prob Interpretations Log-probview -19.68 read -15.68watch -19.84 write -17.47shoot -20.58 work on -18.58edit -20.60 look at -19.09film on -20.69 read in -19.10film -20.87 write in -19.73view on -20.93 browse -19.74make -21.26 get -19.90edit of -21.29 re-read -19.97play -21.31 talk about -20.02direct -21.72 see -20.03sort -21.73 publish -20.06look at -22.23 read through -20.10record on -22.38 recount in -20.13Table 2: Possible Interpretations of MetonymiesRanked by our Systemphrases enjoy book and finish video together withtheir log-probabilities are shown in Table 2.4.2 Comparison with the Results of Lapataand LascaridesWe compared the output of our reimplementationof Lapata and Lascarides?
algorithm with their re-sults, which we obtained from the authors.
Themajor difference between the two systems is thatwe extracted our data from the BNC parsed byRASP, as opposed to the Cass chunk parser (Ab-ney, 1996) utilized by Lapata and Lascarides.
Oursystem finds approximately twice as many in-terpretations as theirs and covers 80% of theirlists (our system does not find some of the low-probability range verbs of Lapata and Lascarides).We compared the rankings of the two implemen-tations in terms of Pearson correlation coefficientand obtained the average correlation of 0.83 (overall metonymic phrases).We also evaluated the performance of our sys-tem against the judgements elicited from humansin the framework of the experiment of Lapata andLascarides (2003) (for a detailed description ofthe human evaluation setup see (Lapata and Las-carides, 2003), pages 12-18).
The Pearson corre-lation coefficient between the ranking of our sys-tem and the human ranking equals to 0.62 (the in-tersubject agreement on this task is 0.74).
Thisis slightly lower than the number achieved by La-pata and Lascarides (0.64).
Such a difference isprobably due to the fact that our system does notfind some of the low-probability range verbs thatLapata and Lascarides included in their test set,and thus those interpretations get assigned a prob-ability of 0.
We conducted a one-tailed t-test todetermine if our counts were significantly differ-ent from those of Lapata and Lascarides.
The dif-ference is statistically insignificant (t=3.6; df=180;p<.0005), and the output of the system is deemedacceptable to be used for further experiments.5 Mapping Interpretations to WordNetSensesThe interpretations at this stage are just stringsrepresenting collectively all senses of the verb.What we aim for is the list of verb senses that arecorrect interpretations for the metonymic phrase.We assume the WordNet synset representation ofa sense.It has been recognized (Pustejovsky, 1991;Pustejovsky, 1995; Godard and Jayez, 1993) andverified by us empirically that correct interpreta-tions tend to form semantic classes, and therefore,correct interpretations should be related to eachother by semantic relations, such as synonymy orhyponymy.
In order to select the right senses ofthe verbs in the context of the metonymic phrasewe did the following.?
We searched the WordNet database for thesenses of the verbs that are in synonymy, hy-pernymy and hyponymy relations.?
We stored the corresponding synsets in a newlist of interpretations.
If one synset was a hy-pernym (or hyponym) of the other, then bothsynsets were stored.For example, for the metonymic phrase finishvideo the interpretations watch, view and seeare synonymous, therefore a synset contain-ing (watch(3) view(3) see(7)) wasstored.
This means that sense 3 of watch, sense3 of view and sense 7 of see would be correctinterpretations of the metonymic expression.The obtained number of synsets ranges from 14(try shampoo) to 1216 (want money) for the wholedataset of Lapata and Lascarides (2003).6 Ranking the SensesA problem that arises with the lists of synsets ob-tained is that they contain different senses of thesame verb.
However, very few verbs have such arange of meanings that their two different sensescould represent two distinct metonymic interpre-tations (e.g., in case of take interpretation of finishvideo shoot sense and look at, consider sense are4both acceptable interpretations, the second obvi-ously being dispreferred).
In the vast majority ofcases the occurrence of the same verb in differentsynsets means that the list still needs filtering.In order to do this we rank the synsets accord-ing to their likelihood of being a metonymic inter-pretation.
The sense ranking is largely based onthe probabilities of the verb strings derived by themodel of Lapata and Lascarides (2003).6.1 Zipfian Sense Frequency DistributionThe probability of each string from our initial listrepresents the sum of probabilities of all senses ofthis verb.
Hence this probability mass needs to bedistributed over senses first.
The sense frequencydistribution for most words tends to be closer toZipfian, rather than uniform or any other distribu-tion (Preiss, 2006).
This is an approximation thatwe rely on, as it has been shown to realisticallydescribe the majority of words.This means that the first senses will be favouredover the others, and the frequency of each sensewill be inversely proportional to its rank in the listof senses (i.e.
sense number, since word senses areordered in WordNet by frequency).Pv,k= Pv?1k(2)where k is the sense number and Pvis the likeli-hood of the verb string being an interpretation ac-cording to the corpus data, i.e.Pv=Nv?k=1Pv,k(3)where Nvis the total number of senses for the verbin question.The problem that arises with (2) is that the in-verse sense numbers (1/k) do not add up to 1.
Inorder to circumvent this, the Zipfian distributionis commonly normalised by the Nth generalisedharmonic number.
Assuming the same notationPv,k= Pv?1/k?Nvn=11/n(4)Once we have obtained the sense probabilitiesPv,k, we can calculate the likelihood of the wholesynsetPs=Is?i=1Pvi,k(5)where viis a verb in the synset s and Isis thetotal number of verbs in the synset s. The verbssuggested by WordNet, but not attested in thecorpus in the required environment, are assignedthe probability of 0.
Some output synsets forthe metonymic phrase finish video and their log-probabilities are demonstrated in Table 3.In our experiment we compare the performanceof the system assuming a Zipfian distribution ofsenses against the baseline using a uniform distri-bution.
We expect the former to yield better re-sults.6.2 Gloss ProcessingThe model in the previous section penalizessynsets that are incorrect interpretations.
How-ever, it can not discriminate well between the onesconsisting of a single verb.
By default it favoursthe sense with a smaller sense number in Word-Net.
This poses a problem for the examples suchas direct for the phrase finish video: our list con-tains several senses of it, as shown in Table 4, andtheir ranking is not satisfactory.
The only correctinterpretation in this case, sense 3, is assigned alower likelihood than the senses 1 and 2.The most relevant synset can be found by us-ing the information from WordNet glosses (theverbal descriptions of concepts, often with ex-amples).
We searched for the glosses contain-ing terms related to the noun in the metonymicphrase, here video.
Such related terms wouldbe its direct synonyms, hyponyms, hypernyms,meronyms or holonyms according to WordNet.We assigned more weight to the synsets whosegloss contained related terms.
In our examplethe synset (direct-v-3), which is the correctmetonymic interpretation, contained the term filmin its gloss and was therefore selected.
Its likeli-hood was multiplied by the factor of 10.It should be noted, however, that the glosses donot always contain the related terms; the expecta-tion is that they will be useful in the majority ofcases, not in all of them.7 Evaluation7.1 The Gold StandardWe selected the most frequent metonymic verbsfor our experiments: begin, enjoy, finish, try, start.We randomly selected 10 metonymic phrases con-taining these verbs.
We split them into the devel-opment set (5 phrases) and the test set (5 phrases)5Synset and its Gloss Log-prob( watch-v-1 ) - look attentively; ?watch a basketball game?
-4.56( view-v-2 consider-v-8 look-at-v-2 ) - look at carefully; study mentally; ?view a problem?
-4.66( watch-v-3 view-v-3 see-v-7 catch-v-15 take-in-v-6 ) - see or watch; ?view a show on television?
; ?This programwill be seen all over the world?
; ?view an exhibition?
; ?Catch a show on Broadway?
; ?see a movie?
-4.68( film-v-1 shoot-v-4 take-v-16 ) - make a film or photograph of something; ?take a scene?
; ?shoot a movie?
-4.91( edit-v-1 redact-v-2 ) - prepare for publication or presentation by correcting, revising, or adapting; ?Edit abook on lexical semantics?
; ?she edited the letters of the politician so as to omit the most personal passages?
-5.11( film-v-2 ) - record in film; ?The coronation was filmed?
-5.74( screen-v-3 screen-out-v-1 sieve-v-1 sort-v-1 ) - examine in order to test suitability; ?screen these samples?
;?screen the job applicants?
-5.91( edit-v-3 cut-v-10 edit-out-v-1 ) - cut and assemble the components of; ?edit film?
; ?cut recording tape?
-6.20Table 3: Metonymy Interpretations as Synsets (for finish video)Synset and its Gloss Log-prob( direct-v-1 ) - command with authority; ?He directed the children to do their homework?
-6.65( target-v-1 aim-v-5 place-v-7 direct-v-2 point-v-11 ) - intend (something) to move towards a certain goal;?He aimed his fists towards his opponent?s face?
; ?criticism directed at her superior?
; ?direct your angertowards others, not towards yourself?
-7.35( direct-v-3 ) - guide the actors in (plays and films) -7.75( direct-v-4 ) - be in charge of -8.04Table 4: Different Senses of direct (for finish video)Development Set Test Setenjoy book enjoy storyfinish video finish projectstart experiment try vegetablefinish novel begin theoryenjoy concert start letterTable 5: Metonymic Phrases in Development andTest Setsgiven in the table 5.The gold standards were created for the top 30synsets of each metonymic phrase after ranking.This threshold was set experimentally: the recallof correct interpretations among the top 30 synsetsis 0.75 (average over metonymic phrases from thedevelopment set).
This threshold allows to filterout a large number of incorrect interpretations.The interpretations that are plausible in someimaginary context are marked as correct in thegold standard.7.2 Evaluation MeasureWe evaluated the performance of the systemagainst the gold standard.
The objective was tofind out if the synsets were distributed in such away that the plausible interpretations appear at thetop of the list and the incorrect ones at the bottom.The evaluation was done in terms of mean averageprecision (MAP) at top 30 synsets.MAP =1MM?j=11NjNj?i=1Pji, (6)where M is the number of metonymic phrases,Njis the number of correct interpretations for themetonymic phrase, Pjiis the precision at each cor-rect interpretation (the number of correct interpre-tations among the top i ranks).
First, the aver-age precision was computed for each metonymicphrase independently.
Then the mean values werecalculated for the development and the test sets.The reasoning behind computing MAP insteadof precision at a fixed number of synsets (e.g.top 30) is that the number of correct interpreta-tions varies dramatically for different metonymicphrases.
MAP essentially evaluates how manygood interpretations appear at the top of the list,which takes this variation into account.7.3 ResultsWe compared the ranking obtained by applyingZipfian sense frequency distribution against thatobtained by distributing probability mass oversenses uniformly (baseline).
We also consideredthe rankings before and after gloss processing.The results are shown in Table 6.
These resultsdemonstrate the positive contribution of both Zip-fian distribution and gloss processing to the rank-ing.7.4 Human ExperimentWe conducted an experiment with humans in orderto prove that this task is intuitive to people, i.e.they agree on the task.We had 8 volunteer subjects altogether.
All of6Dataset Verb Probability Gloss MAPMass Distribution ProcessingDevelopment set Uniform No 0.51Development set Zipfian No 0.65Development set Zipfian Yes 0.73Test set Zipfian Yes 0.83Table 6: Evaluation of the Model RankingGroup 1 Group 2finish video finish projectstart experiment begin theoryenjoy concert start letterTable 7: Metonymic Phrases for Groups 1 and 2them were native speakers of English and non-linguists.
We divided them into 2 groups: 4 and 4.Subjects in each group annotated three metonymicphrases as shown in Table 7.
They received writ-ten guidelines, which were the only source of in-formation on the experiment.For each metonymic phrase they were presentedwith a list of 30 possible interpretations producedby the system.
For each synset in the list they hadto decide whether it was a plausible interpretationof the metonymic phrase in an imaginary context.We evaluated interannotator agreement in termsof Fleiss?
kappa (Fleiss, 1971) and f-measure com-puted pairwise and then averaged across the an-notators.
The agreement in group 1 was 0.76(f-measure) and 0.56 (kappa); in group 2 0.68(f-measure) and 0.51 (kappa).
This yielded theaverage agreement of 0.72 (f-measure) and 0.53(kappa).8 Linguistic Perspective on LogicalMetonymyThere has been debate in linguistics literature aswhether it is the noun or the verb in the metonymicphrase that determines the interpretation.
Some ofthe accounts along with our own analysis are pre-sented below.8.1 The Effect of the Noun ComplementThe interpretation of logical metonymy is oftenexplained by the lexical defaults associated withthe noun complement in the metonymic phrase.Pustejovsky (1991) models these lexical defaultsin the form of the qualia structure of the noun.
Thequalia structure of a noun specifies the followingaspects of its meaning:?
CONSTITUTIVE Role (the relation betweenan object and its constituents)?
FORMAL Role (that which distinguishes theobject within a larger domain)?
TELIC Role (purpose and function of the ob-ject)?
AGENTIVE Role (how the object came intobeing)For the problem of logical metonymy the telic andagentive roles are of particular interest.
For ex-ample, the noun book would have read specifiedas its telic role and write as its agentive role inits qualia structure.
Following Pustejovsky (1991;1995) and others, we take this information fromthe noun qualia to represent the default interpre-tations of metonymic constructions.
Nevertheless,multiple telic and agentive roles can exist and bevalid interpretations, which is supported by the ev-idence derived from the corpus (Verspoor, 1997).Such lexical defaults operate with a lack ofpragmatic information.
In some cases, however,lexical defaults can be overridden by context.Consider the following example taken from Las-carides and Copestake (1995).
(6) My goat eats anything.
He really enjoyedyour book.Here it is clear that the goat enjoyed eating thebook and not reading the book, which is enforcedby the context.
Thus, incorporating the context ofthe metonymic phrase into the model would be an-other interesting extension of our experiment.8.2 The Effect of the Metonymic VerbBy analysing phrases from the dataset of Lap-ata and Lascarides (2003) we found that differentmetonymic verbs have different effect on the inter-pretation of logical metonymy.
In this section weprovide some criteria based on which one couldclassify metonymic verbs:?
Control vs. raising.
Consider the phrase ex-pect poetry taken from the dataset of Lap-ata and Lascarides.
Expect is a typical ob-ject raising verb and, therefore, the most ob-vious interpretation of this phrase would beexpect someone to learn/recite poetry, ratherthan expect to hear poetry or expect to learnpoetry, as suggested by the model of Lapata7and Lascarides.
Their model does not takeinto account raising syntactic frame and assuch its interpretation of raising metonymicphrases will be based on the wrong kindof corpus evidence.
Our expectation, how-ever, is that control verbs tend to form logicalmetonymies more frequently.
By analyzingthe lists of control and raising verbs compiledby Boguraev and Briscoe (1987) we foundevidence supporting this claim.
Only 20% ofraising verbs can form metonymic construc-tions (e.g.
expect, allow, command, request,require etc.
), while others can not (e.g.
ap-pear, seem, consider etc.).
Due to both thisand the fact that we build on the approach ofLapata and Lascarides (2003), we gave pref-erence to control verbs to develop and test oursystem.?
Activity vs. result.
Some metonymic verbsrequire the reconstructed event to be an ac-tivity (e.g.
begin writing the book), while oth-ers require a result (e.g.
attempt to reach thepeak).
This distinction potentially allows torule out some incorrect interpretations, e.g.
aresultative find for enjoy book, as enjoy re-quires an event of the type activity.
Automat-ing this would be an interesting route for ex-tension of our experiment.?
Telic vs. agentive vs. other events.
An-other interesting observation we made cap-tures the constraints that the metonymic verbimposes on the reconstructed event in termsof its function.
While some metonymic verbsrequire rather telic events (e.g., enjoy, want,try), others have strong preference for agen-tive (e.g., start).
However, for some cate-gories of verbs it is hard to define a partic-ular type of the event they require (e.g., at-tempt the peak should be interpreted as at-tempt to reach the peak, which is neither telicnor agentive).9 Conclusions and Future WorkWe presented a system producing disambiguatedinterpretations of logical metonymy with respectto word sense.
Such representation is novel andit is intuitive to humans, as demonstrated by thehuman experiment.
We also proposed a novelscheme for estimating the likelihood of a WordNetsynset as a unit from a non-disambiguated corpus.The obtained results demonstrate the effectivenessof our approach to deriving metonymic interpreta-tions.Along with this we provided criteria for dis-criminating between different metonymic verbswith respect to their effect on the interpretationof logical metonymy.
Our empirical analysis hasshown that control verbs tend to form logicalmetonymy more frequently than raising verbs, aswell as that the former comply with the model ofLapata and Lascarides (2003), whereas the latterform logical metonymies based on a different syn-tactic frame.
Incorporating such linguistic knowl-edge into the model would be an interesting exten-sion of this experiment.One of the motivations of the proposed sense-based representation is the fact that the interpreta-tions of metonymic phrases tend to form coher-ent semantic classes (Pustejovsky, 1991; Puste-jovsky, 1995; Godard and Jayez, 1993).
The au-tomatic discovery of such classes would requireword sense disambiguation as an initial step.
Thisis due to the fact that it is verb senses that form theclasses rather than verb strings.
Comparing the in-terpretations obtained for the phrase finish video,one can clearly distinguish between the meaningpertaining to the creation of the video, e.g., film,shoot, take, and those denoting using the video,e.g., watch, view, see.
Discovering such classesusing the existing verb clustering techniques is ournext experiment.Using sense-based interpretations of logicalmetonymy as opposed to ambiguous verbs couldbenefit other NLP applications that rely on disam-biguated text (e.g.
for the tasks of information re-trieval (Voorhees, 1998) and question answering(Pasca and Harabagiu, 2001)).AcknowledgementsI would like to thank Simone Teufel and Anna Ko-rhonen for their valuable feedback on this projectand my anonymous reviewers whose commentshelped to improve the paper.
I am also very grate-ful to Cambridge Overseas Trust who made thisresearch possible by funding my studies.8ReferencesS.
Abney.
1996.
Partial parsing via finite-state cas-cades.
In J. Carroll, editor, Workshop on RobustParsing, pages 8?15, Prague.O.
E. Andersen, J. Nioche, E. Briscoe, and J. Car-roll.
2008.
The BNC parsed with RASP4UIMA.In Proceedings of the Sixth International LanguageResources and Evaluation Conference (LREC?08),Marrakech, Morocco.B.
Boguraev and E. Briscoe.
1987.
Large lexiconsfor natural language processing: utilising the gram-mar coding system of the Longman Dictionary ofContemporary English.
Computational Linguistics,13(4):219?240.E.
Briscoe, A. Copestake, and B. Boguraev.
1990.Enjoy the paper: lexical semantics via lexicology.In Proceedings of the 13th International Conferenceon Computational Linguistics (COLING-90), pages42?47, Helsinki.E.
Briscoe, J. Carroll, and R. Watson.
2006.
The sec-ond release of the rasp system.
In Proceedings of theCOLING/ACL on Interactive presentation sessions,pages 77?80.L.
Burnard.
2007.
Reference Guide for the British Na-tional Corpus (XML Edition).D.
Fass.
1991. met*: A method for discriminatingmetonymy and metaphor by computer.
Computa-tional Linguistics, 17(1):49?90.C.
Fellbaum, editor.
1998.
WordNet: An ElectronicLexical Database (ISBN: 0-262-06197-X).
MITPress, first edition.J.
L. Fleiss.
1971.
Measuring nominal scale agree-ment among many raters.
Psychological Bulletin,76(5):378?382.D.
Godard and J. Jayez.
1993.
Towards a proper treat-ment of coercion phenomena.
In Sixth Conferenceof the European Chapter of the ACL, pages 168?177,Utrecht.M.
Lapata and A. Lascarides.
2003.
A probabilisticaccount of logical metonymy.
Computational Lin-guistics, 29(2):261?315.A.
Lascarides and A. Copestake.
1995.
The prag-matics of word meaning.
In Journal of Linguistics,pages 387?414.M.
Pasca and S. Harabagiu.
2001.
The informativerole of WordNet in open-domain question answer-ing.
In Proceedings of NAACL-01 Workshop onWordNet and Other Lexical Resources, pages 138?143, Pittsburgh, PA.J.
Preiss.
2006.
Probabilistic word sense disambigua-tion analysis and techniques for combining knowl-edge sources.
Technical report, Computer Labora-tory, University of Cambridge.J.
Pustejovsky.
1991.
The generative lexicon.
Compu-tational Linguistics, 17(4).J.
Pustejovsky.
1995.
The Generative Lexicon.
MITPress, Cambridge, MA.M.
Utiyama, M. Masaki, and I. Hitoshi.
2000.
A sta-tistical approach to the processing of metonymy.
InProceedings of the 18th International Conference onComputational Linguistics, Saarbrucken, Germany.C.
M. Verspoor.
1997.
Conventionality-governed log-ical metonymy.
In Proceedings of the Second In-ternational Workshop on Computational Semantics,pages 300?312, Tilburg.E.
M. Voorhees.
1998.
Using WordNet for text re-trieval.
In C. Fellbaum, editor, WordNet: An Elec-tornic Lexical Database, pages 285?303.
MIT Press.9
