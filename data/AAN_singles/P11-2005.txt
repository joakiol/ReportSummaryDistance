Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 24?29,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsAn Empirical Investigation of Discountingin Cross-Domain Language ModelsGreg Durrett and Dan KleinComputer Science DivisionUniversity of California, Berkeley{gdurrett,klein}@cs.berkeley.eduAbstractWe investigate the empirical behavior of n-gram discounts within and across domains.When a language model is trained and evalu-ated on two corpora from exactly the same do-main, discounts are roughly constant, match-ing the assumptions of modified Kneser-NeyLMs.
However, when training and test corporadiverge, the empirical discount grows essen-tially as a linear function of the n-gram count.We adapt a Kneser-Ney language model toincorporate such growing discounts, result-ing in perplexity improvements over modifiedKneser-Ney and Jelinek-Mercer baselines.1 IntroductionDiscounting, or subtracting from the count of eachn-gram, is one of the core aspects of Kneser-Neylanguage modeling (Kneser and Ney, 1995).
For allbut the smallest n-gram counts, Kneser-Ney uses asingle discount, one that does not grow with the n-gram count, because such constant-discounting wasseen in early experiments on held-out data (Churchand Gale, 1991).
However, due to increasing com-putational power and corpus sizes, language model-ing today presents a different set of challenges thanit did 20 years ago.
In particular, modeling cross-domain effects has become increasingly more im-portant (Klakow, 2000; Moore and Lewis, 2010),and deployed systems must frequently process datathat is out-of-domain from the standpoint of the lan-guage model.In this work, we perform experiments on held-out data to evaluate how discounting behaves in thecross-domain setting.
We find that, when trainingand testing on corpora that are as similar as possi-ble, empirical discounts indeed do not grow with n-gram count, which validates the parametric assump-tion of Kneser-Ney smoothing.
However, when thetrain and evaluation corpora differ, even slightly, dis-counts generally exhibit linear growth in the count ofthe n-gram, with the amount of growth being closelycorrelated with the corpus divergence.
Finally, webuild a language model exploiting a parametric formof the growing discount and show perplexity gains ofup to 5.4% over modified Kneser-Ney.2 Discount AnalysisUnderlying discounting is the idea that n-grams willoccur fewer times in test data than they do in trainingdata.
We investigate this quantitatively by conduct-ing experiments similar in spirit to those of Churchand Gale (1991).
Suppose that we have collectedcounts on two corpora of the same size, which wewill call our train and test corpora.
For an n-gramw = (w1, ..., wn), let ktrain(w) denote the number ofoccurrences of w in the training corpus, and ktest(w)denote the number of occurrences of w in the testcorpus.
We define the empirical discount of w to bed(w) = ktrain(w) ?
ktest(w); this will be negativewhen the n-gram occurs more in the test data thanin the training data.
Let Wi = {w : ktrain(w) = i}be the set of n-grams with count i in the trainingcorpus.
We define the average empirical discountfunction asd?
(i) =1|Wi|?w?Wid(w)24Kneser-Ney implicitly makes two assumptions:first, that discounts do not depend on n-gram count,i.e.
that d?
(i) is constant in i.
Modified Kneser-Neyrelaxes this assumption slightly by having indepen-dent parameters for 1-count, 2-count, and many-count n-grams, but still assumes that d?
(i) is constantfor i greater than two.
Second, by using the samediscount for all n-grams with a given count, Kneser-Ney assumes that the distribution of d(w) for w in aparticular Wi is well-approximated by its mean.
Inthis section, we analyze whether or not the behaviorof the average empirical discount function supportsthese two assumptions.
We perform experiments onvarious subsets of the documents in the English Gi-gaword corpus, chiefly drawn from New York Times(NYT) and Agence France Presse (AFP).12.1 Are Discounts Constant?Similar corpora To begin, we consider the NYTdocuments from Gigaword for the year 1995.
Inorder to create two corpora that are maximallydomain-similar, we randomly assign half of thesedocuments to train and half of them to test, yieldingtrain and test corpora of approximately 50M wordseach, which we denote by NYT95 and NYT95?.
Fig-ure 1 shows the average empirical discounts d?
(i)for trigrams on this pair of corpora.
In this setting,we recover the results of Church and Gale (1991)in that discounts are approximately constant for n-gram counts of two or greater.Divergent corpora In addition to these two cor-pora, which were produced from a single contigu-ous batch of documents, we consider testing on cor-pus pairs with varying degrees of domain difference.We construct additional corpora NYT96, NYT06,AFP95, AFP96, and AFP06, by taking 50M wordsfrom documents in the indicated years of NYTand AFP data.
We then collect training counts onNYT95 and alternately take each of our five new cor-pora as the test data.
Figure 1 also shows the averageempirical discount curves for these train/test pairs.Even within NYT newswire data, we see growingdiscounts when the train and test corpora are drawn1Gigaword is drawn from six newswire sources and containsboth miscellaneous text and complete, contiguous documents,sorted chronologically.
Our experiments deal exclusively withthe document text, which constitutes the majority of Gigawordand is of higher quality than the miscellaneous text.0 1 2 3 4 5 605101520Average empirical discountTrigramcountintrainAFP06AFP96AFP95NYT06NYT96NYT95?Figure 1: Average empirical trigram discounts d?
(i) forsix configurations, training on NYT95 and testing on theindicated corpora.
For each n-gram count k, we computethe average number of occurrences in test for all n-gramsoccurring k times in training data, then report k minusthis quantity as the discount.
Bigrams and bigram typesexhibit similar discount relationships.from different years, and between the NYT and AFPnewswire, discounts grow even more quickly.
Weobserved these trends continuing steadily up into n-gram counts in the hundreds, beyond which point itbecomes difficult to robustly estimate discounts dueto fewer n-gram types in this count range.This result is surprising in light of the constantdiscounts observed for the NYT95/NYT95?
pair.Goodman (2001) proposes that discounts arise fromdocument-level ?burstiness?
in a corpus, becauselanguage often repeats itself locally within a doc-ument, and Moore and Quirk (2009) suggest thatdiscounting also corrects for quantization error dueto estimating a continuous distribution using a dis-crete maximum likelihood estimator (MLE).
Bothof these factors are at play in the NYT95/NYT95?experiment, and yet only a small, constant discountis observed.
Our growing discounts must thereforebe caused by other, larger-scale phenomena, such asshifts in the subjects of news articles over time or inthe style of the writing between newswire sources.The increasing rate of discount growth as the sourcechanges and temporal divergence increases lendscredence to this hypothesis.2.2 Nonuniformity of DiscountsFigure 1 considers discounting in terms of averageddiscounts for each count, which tests one assump-tion of modified Kneser-Ney, that discounts are a2500.10.20.30.405101520Fraction of train-count-10 trigramsTrigramcountintestNYT95/NYT95?NYT95/AFP95Figure 2: Empirical probability mass functions of occur-rences in the test data for trigrams that appeared 10 timesin training data.
Discounting by a single value is plau-sible in the case of similar train and test corpora, wherethe mean of the distribution (8.50) is close to the median(8.0), but not in the case of divergent corpora, where themean (6.04) and median (1.0) are very different.constant function of n-gram counts.
In Figure 2, weinvestigate the second assumption, namely that thedistribution over discounts for a given n-gram countis well-approximated by its mean.
For similar cor-pora, this seems to be true, with a histogram of testcounts for trigrams of count 10 that is nearly sym-metric.
For divergent corpora, the data exhibit highskew: almost 40% of the trigrams simply never ap-pear in the test data, and the distribution has veryhigh standard deviation (17.0) due to a heavy tail(not shown).
Using a discount that depends only onthe n-gram count is less appropriate in this case.In combination with the growing discounts of sec-tion 2.1, these results point to the fact that modifiedKneser-Ney does not faithfully model the discount-ing in even a mildly cross-domain setting.2.3 Correlation of Divergence and DiscountsIntuitively, corpora that are more temporally distantwithin a particular newswire source should perhapsbe slightly more distinct, and still a higher degree ofdivergence should exist between corpora from dif-ferent newswire sources.
From Figure 1, we see thatthis notion agrees with the relative sizes of the ob-served discounts.
We now ask whether growth indiscounts is correlated with train/test dissimilarity ina more quantitative way.
For a given pair of cor-pora, we canonicalize the degree of discounting byselecting the point d?
(30), the average empirical dis-0 5 1015-500-400-300Discount for count-30 trigramsLoglikelihooddifference(inmillions)Figure 3: Log likelihood difference versus average empir-ical discount of trigrams with training count 30 (d?
(30))for the train/test pairs.
More negative values of the loglikelihood indicate more dissimilar corpora, as the trainedmodel is doing less well relative to the jackknife model.count for n-grams occurring 30 times in training.2To measure divergence between the corpus pair, wecompute the difference between the log likelihoodof the test corpus under the train corpus languagemodel (using basic Kneser-Ney) and the likelihoodof the test corpus under a jackknife language modelfrom the test itself, which holds out and scores eachtest n-gram in turn.
This dissimilarity metric resem-bles the cross-entropy difference used by Moore andLewis (2010) to subsample for domain adaptation.We compute this canonicalization for each oftwenty pairs of corpora, with each corpus contain-ing 240M trigram tokens between train and test.
Thecorpus pairs were chosen to span varying numbersof newswire sources and lengths of time in order tocapture a wide range of corpus divergences.
Our re-sults are plotted in Figure 3.
The log likelihood dif-ference and d?
(30) are negatively correlated with acorrelation coefficient value of r = ?0.88, whichstrongly supports our hypothesis that higher diver-gence yields higher discounting.
One explanationfor the remaining variance is that the trigram dis-count curve depends on the difference between thenumber of bigram types in the train and test corpora,which can be as large as 10%: observing more bi-gram contexts in training fragments the token counts2One could also imagine instead canonicalizing the curvesby using either the exponent or slope parameters from a fittedpower law as in section 3.
However, there was sufficient non-linearity in the average empirical discount curves that neither ofthese parameters was an accurate proxy for d?
(i).26and leads to smaller observed discounts.2.4 Related WorkThe results of section 2.1 point to a remarkably per-vasive phenomenon of growing empirical discounts,except in the case of extremely similar corpora.Growing discounts of this sort were previously sug-gested by the model of Teh (2006).
However, weclaim that the discounting phenomenon in our data isfundamentally different from his model?s prediction.In the held-out experiments of section 2.1, growingdiscounts only emerge when one evaluates against adissimilar held-out corpus, whereas his model wouldpredict discount growth even in NYT95/NYT95?,where we do not observe it.Adaptation across corpora has also been ad-dressed before.
Bellegarda (2004) describes a rangeof techniques, from interpolation at either the countlevel or the model level (Bacchiani and Roark, 2003;Bacchiani et al, 2006) to using explicit models ofsyntax or semantics.
Hsu and Glass (2008) employa log-linear model for multiplicatively discountingn-grams in Kneser-Ney; when they include the log-count of an n-gram as the only feature, they achieve75% of their overall word error rate reduction, sug-gesting that predicting discounts based on n-gramcount can substantially improve the model.
Theirwork also improves on the second assumption ofKneser-Ney, that of the inadequacy of the averageempirical discount as a discount constant, by em-ploying various other features in order to provideother criteria on which to discount n-grams.Taking a different approach, both Klakow (2000)and Moore and Lewis (2010) use subsampling toselect the domain-relevant portion of a large, gen-eral corpus given a small in-domain corpus.
Thiscan be interpreted as a form of hard discounting,and implicitly models both growing discounts, sincefrequent n-grams will appear in more of the re-jected sentences, and nonuniform discounting overn-grams of each count, since the sentences are cho-sen according to a likelihood criterion.
Althoughwe do not consider this second point in constructingour language model, an advantage of our approachover subsampling is that we use our entire trainingcorpus, and in so doing compromise between min-imizing errors from data sparsity and accommodat-ing domain shifts to the extent possible.3 A Growing Discount Language ModelWe now implement and evaluate a language modelthat incorporates growing discounts.3.1 MethodsInstead of using a fixed discount for most n-gramcounts, as prescribed by modified Kneser-Ney, wediscount by an increasing parametric function of then-gram count.
We use a tune set to compute an av-erage empirical discount curve d?
(i), and fit a func-tion of the form f(x) = a + bxc to this curve usingweighted least-L1-loss regression, with the weightfor each point proportional to i|Wi|, the total to-ken counts of n-grams occurring that many timesin training.
To improve the fit of the model, weuse dedicated parameters for count-1 and count-2 n-grams as in modified Kneser-Ney, yielding a modelwith five parameters per n-gram order.
We call thismodel GDLM.
We also instantiate this model withc fixed to one, so that the model is strictly linear(GDLM-LIN).As baselines for comparison, we use basic inter-polated Kneser-Ney (KNLM), with one discount pa-rameter per n-gram order, and modified interpolatedKneser-Ney (MKNLM), with three parameters pern-gram order, as described in (Chen and Goodman,1998).
We also compare against Jelinek-Mercersmoothing (JMLM), which interpolates the undis-counted MLEs from every order.
According to Chenand Goodman (1998), it is common to use differentinterpolation weights depending on the history countof an n-gram, since MLEs based on many samplesare presumed to be more accurate than those withfew samples.
We used five history count buckets sothat JMLM would have the same number of param-eters as GDLM.All five models are trigram models with typecounts at the lower orders and independent discountor interpolation parameters for each order.
Param-eters for GDLM, MKNLM, and KNLM are initial-ized based on estimates from d?
(i): the regressionthereof for GDLM, and raw discounts for MKNLMand KNLM.
The parameters of JMLM are initializedto constants independent of the data.
These initial-izations are all heuristic and not guaranteed to beoptimal, so we then iterate through the parametersof each model several times and perform line search27Train NYT00+01 Train AFP02+05+06Voc.
157K 50K 157K 50KGDLM(*) 151 131 258 209GDLM-LIN(*) 151 132 259 210JMLM 165 143 274 221MKNLM 152 132 273 221KNLM 159 138 300 241Table 1: Perplexities of the growing discounts languagemodel (GDLM) and its purely linear variant (GDLM-LIN), which are contributions of this work, versusthe modified Kneser-Ney (MKNLM), basic Kneser-Ney(KNLM), and Jelinek-Mercer (JMLM) baselines.
Wereport results for in-domain (NYT00+01) and out-of-domain (AFP02+05+06) training corpora, for two meth-ods of closing the vocabulary.in each to optimize tune-set perplexity.For evaluation, we train, tune, and test on threedisjoint corpora.
We consider two different train-ing sets: one of 110M words of NYT from 2000and 2001 (NYT00+01), and one of 110M words ofAFP from 2002, 2005, and 2006 (AFP02+05+06).In both cases, we compute d?
(i) and tune parameterson 110M words of NYT from 2002 and 2003, anddo our final perplexity evaluation on 4M words ofNYT from 2004.
This gives us both in-domain andout-of-domain results for our new language model.Our tune set is chosen to be large so that we caninitialize parameters based on the average empiricaldiscount curve; in practice, one could compute em-pirical discounts based on a smaller tune set with thecounts scaled up proportionately, or simply initializeto constant values.We use two different methods to handle out-of-vocabulary (OOV) words: one scheme replaces anyunigram token occurring fewer than five times intraining with an UNK token, yielding a vocabularyof approximately 157K words, and the other schemeonly keeps the top 50K words in the vocabulary.The count truncation method has OOV rates of 0.9%and 1.9% in the NYT/NYT and NYT/AFP settings,respectively, and the constant-size vocabulary hasOOV rates of 2% and 3.6%.3.2 ResultsPerplexity results are given in Table 1.
As expected,for in-domain data, GDLM performs comparably toMKNLM, since the discounts do not grow and sothere is little to be gained by choosing a param-eterization that permits this.
Out-of-domain, ourmodel outperforms MKNLM and JMLM by approx-imately 5% for both vocabulary sizes.
The out-of-domain perplexity values are competitive withthose of Rosenfeld (1996), who trained on New YorkTimes data and tested on AP News data under simi-lar conditions, and even more aggressive closing ofthe vocabulary.
Moore and Lewis (2010) achievelower perplexities, but they use in-domain trainingdata that we do not include in our setting.We briefly highlight some interesting features ofthese results.
In the small vocabulary cross-domainsetting, for GDLM-LIN, we finddtri(i) = 1.31 + 0.27i, dbi(i) = 1.34 + 0.05ias the trigram and bigram discount functions thatminimize tune set perplexity.
For GDLM,dtri(i) = 1.19 + 0.32i0.45, dbi(i) = 0.86 + 0.56i0.86In both cases, a growing discount is indeed learnedfrom the tuning procedure, demonstrating the im-portance of this in our model.
Modeling nonlin-ear discount growth in GDLM yields only a smallmarginal improvement over the linear discountingmodel GDLM-LIN, so we prefer GDLM-LIN for itssimplicity.A somewhat surprising result is the strong per-formance of JMLM relative to MKNLM on the di-vergent corpus pair.
We conjecture that this is be-cause the bucketed parameterization of JMLM givesit the freedom to change interpolation weights withn-gram count, whereas MKNLM has essentially afixed discount.
This suggests that modified Kneser-Ney as it is usually parameterized may be a particu-larly poor choice in cross-domain settings.Overall, these results show that the growing dis-count phenomenon detailed in section 2, beyondsimply being present in out-of-domain held-out data,provides the basis for a new discounting scheme thatallows us to improve perplexity relative to modifiedKneser-Ney and Jelinek-Mercer baselines.AcknowledgmentsThe authors gratefully acknowledge partial supportfrom the GALE program via BBN under DARPAcontract HR0011-06-C-0022, and from an NSF fel-lowship for the first author.
Thanks to the anony-mous reviewers for their insightful comments.28ReferencesMichiel Bacchiani and Brian Roark.
2003.
UnsupervisedLangauge Model Adaptation.
In Proceedings of Inter-national Conference on Acoustics, Speech, and SignalProcessing.Michiel Bacchiani, Michael Riley, Brian Roark, andRichard Sproat.
2006.
MAP adaptation of stochasticgrammars.
Computer Speech & Language, 20(1):41 ?68.Jerome R. Bellegarda.
2004.
Statistical language modeladaptation: review and perspectives.
Speech Commu-nication, 42:93?108.Stanley Chen and Joshua Goodman.
1998.
An EmpiricalStudy of Smoothing Techniques for Language Model-ing.
Technical report, Harvard University, August.Kenneth Church and William Gale.
1991.
A Compari-son of the Enhanced Good-Turing and Deleted Estima-tion Methods for Estimating Probabilities of EnglishBigrams.
Computer Speech & Language, 5(1):19?54.Joshua Goodman.
2001.
A Bit of Progress in LanguageModeling.
Computer Speech & Language, 15(4):403?434.Bo-June (Paul) Hsu and James Glass.
2008.
N-gram Weighting: Reducing Training Data Mismatch inCross-Domain Language Model Estimation.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing, pages 829?838.Dietrich Klakow.
2000.
Selecting articles from the lan-guage model training corpus.
In Proceedings of theIEEE International Conference on Acoustics, Speech,and Signal Processing, volume 3, pages 1695?1698.Reinhard Kneser and Hermann Ney.
1995.
ImprovedBacking-off for M-Gram Language Modeling.
In Pro-ceedings of International Conference on Acoustics,Speech, and Signal Processing.Robert C. Moore and William Lewis.
2010.
Intelligentselection of language model training data.
In Proceed-ings of the ACL 2010 Conference Short Papers, pages220?224, July.Robert C. Moore and Chris Quirk.
2009.
ImprovedSmoothing for N-gram Language Models Based onOrdinary Counts.
In Proceedings of the ACL-IJCNLP2009 Conference Short Papers, pages 349?352.Ronald Rosenfeld.
1996.
A Maximum Entropy Ap-proach to Adaptive Statistical Language Modeling.Computer, Speech & Language, 10:187?228.Yee Whye Teh.
2006.
A Hierarchical Bayesian Lan-guage Model Based On Pitman-Yor Processes.
In Pro-ceedings of ACL, pages 985?992, Sydney, Australia,July.
Association for Computational Linguistics.29
