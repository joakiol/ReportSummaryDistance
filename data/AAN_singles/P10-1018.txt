Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 167?176,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics?Was it good?
It was provocative.
?Learning the meaning of scalar adjectivesMarie-Catherine de Marneffe, Christopher D. Manning and Christopher PottsLinguistics DepartmentStanford UniversityStanford, CA 94305{mcdm,manning,cgpotts}@stanford.eduAbstractTexts and dialogues often express infor-mation indirectly.
For instance, speak-ers?
answers to yes/no questions do notalways straightforwardly convey a ?yes?or ?no?
answer.
The intended reply isclear in some cases (Was it good?
It wasgreat!)
but uncertain in others (Was itacceptable?
It was unprecedented.).
Inthis paper, we present methods for inter-preting the answers to questions like thesewhich involve scalar modifiers.
We showhow to ground scalar modifier meaningbased on data collected from the Web.
Welearn scales between modifiers and inferthe extent to which a given answer conveys?yes?
or ?no?.
To evaluate the methods,we collected examples of question?answerpairs involving scalar modifiers from CNNtranscripts and the Dialog Act corpus anduse response distributions from Mechani-cal Turk workers to assess the degree towhich each answer conveys ?yes?
or ?no?.Our experimental results closely match theTurkers?
response data, demonstrating thatmeanings can be learned from Web dataand that such meanings can drive prag-matic inference.1 IntroductionAn important challenge for natural language pro-cessing is how to learn not only basic linguisticmeanings but also how those meanings are system-atically enriched when expressed in context.
Forinstance, answers to polar (yes/no) questions donot always explicitly contain a ?yes?
or ?no?, butrather give information that the hearer can use toinfer such an answer in a context with some degreeof certainty.
Hockey et al (1997) find that 27% ofanswers to polar questions do not contain a direct?yes?
or ?no?
word, 44% of which they regard asfailing to convey a clear ?yes?
or ?no?
response.
Insome cases, interpreting the answer is straightfor-ward (Was it bad?
It was terrible.
), but in others,what to infer from the answer is unclear (Was itgood?
It was provocative.).
It is even commonfor the speaker to explicitly convey his own uncer-tainty with such answers.In this paper, we focus on the interpretationof answers to a particular class of polar ques-tions: ones in which the main predication in-volves a gradable modifier (e.g., highly unusual,not good, little) and the answer either involves an-other gradable modifier or a numerical expression(e.g., seven years old, twenty acres of land).
Inter-preting such question?answer pairs requires deal-ing with modifier meanings, specifically, learningcontext-dependent scales of expressions (Horn,1972; Fauconnier, 1975) that determine how, andto what extent, the answer as a whole resolves theissue raised by the question.We propose two methods for learning theknowledge necessary for interpreting indirect an-swers to questions involving gradable adjectives,depending on the type of predications in the ques-tion and the answer.
The first technique dealswith pairs of modifiers: we hypothesized that on-line, informal review corpora in which people?scomments have associated ratings would providea general-purpose database for mining scales be-tween modifiers.
We thus use a large collection ofonline reviews to learn orderings between adjec-tives based on contextual entailment (good < ex-cellent), and employ this scalar relationship to in-fer a yes/no answer (subject to negation and otherqualifiers).
The second strategy targets numeri-cal answers.
Since it is unclear what kind of cor-pora would contain the relevant information, weturn to the Web in general: we use distributionalinformation retrieved via Web searches to assesswhether the numerical measure counts as a posi-167tive or negative instance of the adjective in ques-tion.
Both techniques exploit the same approach:using texts (the Web) to learn meanings that candrive pragmatic inference in dialogue.
This paperdemonstrates to some extent that meaning can begrounded from text in this way.2 Related workIndirect speech acts are studied by Clark (1979),Perrault and Allen (1980), Allen and Perrault(1980) and Asher and Lascarides (2003), whoidentify a wide range of factors that govern howspeakers convey their intended messages and howhearers seek to uncover those messages fromuncertain and conflicting signals.
In the com-putational literature, Green and Carberry (1994,1999) provide an extensive model that interpretsand generates indirect answers to polar questions.They propose a logical inference model whichmakes use of discourse plans and coherence rela-tions to infer categorical answers.
However, to ad-equately interpret indirect answers, the uncertaintyinherent in some answers needs to be captured (deMarneffe et al, 2009).
While a straightforward?yes?
or ?no?
response is clear in some indirect an-swers, such as in (1), the intended answer is lesscertain in other cases (2):1(1) A: Do you think that?s a good idea, that wejust begin to ignore these numbers?B: I think it?s an excellent idea.
(2) A: Is he qualified?B: I think he?s young.In (2), it might be that the answerer does notknow about qualifications or does not want to talkabout these directly, and therefore shifts the topicslightly.
As proposed by Zeevat (1994) in his workon partial answers, the speaker?s indirect answermight indicate that he is deliberately leaving theoriginal question only partially addressed, whilegiving a fully resolving answer to another one.The hearer must then interpret the answer to workout the other question.
In (2) in context, we get asense that the speaker would resolve the issue to?no?, but that he is definitely not committed to thatin any strong sense.
Uncertainty can thus resideboth on the speaker and the hearer sides, and thefour following scenarios are attested in conversa-tion:1Here and throughout, the examples come from the corpusdescribed in section 3.a.
The speaker is certain of ?yes?
or ?no?
andconveys that directly and successfully to thehearer.b.
The speaker is certain of ?yes?
or ?no?
butconveys this only partially to the hearer.c.
The speaker is uncertain of ?yes?
or ?no?
andconveys this uncertainty to the hearer.d.
The speaker is uncertain of ?yes?
or ?no?,but the hearer infers one of those with con-fidence.The uncertainty is especially pressing for pred-ications built around scalar modifiers, which areinherently vague and highly context-dependent(Kamp and Partee, 1995; Kennedy and McNally,2005; Kennedy, 2007).
For example, even if wefix the basic sense for little to mean ?young for ahuman?, there is a substantial amount of gray areabetween the clear instances (babies) and the clearnon-instances (adults).
This is the source of un-certainty in (3), in which B?s children fall into thegray area.
(3) A: Are your kids little?B: I have a seven year-old and a tenyear-old.3 Corpus descriptionSince indirect answers are likely to arise in in-terviews, to gather instances of question?answerpairs involving gradable modifiers (which willserve to evaluate the learning techniques), we useonline CNN interview transcripts from five dif-ferent shows aired between 2000 and 2008 (An-derson Cooper, Larry King Live, Late Edition,Lou Dobbs Tonight, The Situation Room).
Wealso searched the Switchboard Dialog Act corpus(Jurafsky et al, 1997).
We used regular expres-sions and manual filtering to find examples of two-utterance dialogues in which the question and thereply contain some kind of gradable modifier.3.1 Types of question?answer pairsIn total, we ended up with 224 question?answerpairs involving gradable adjectives.
Howeverour collection contains different types of answers,which naturally fall into two categories: (I) in205 dialogues, both the question and the answercontain a gradable modifier; (II) in 19 dialogues,the reply contains a numerical measure (as in (3)above and (4)).168Modification in answer Example CountI Other adjective (1), (2) 125Adverb - same adjective (5) 55Negation - same adjective (6), (7) 21Omitted adjective (8) 4II Numerical measure (3), (4) 19Table 1: Types of question?answer pairs, andcounts in the corpus.I Modification in answer Mean SDOther adjective 1.1 0.6Adverb - same adjective 0.8 0.6Negation - same adjective 1.0 0.5Omitted adjective 1.1 0.2II Numerical measure 1.5 0.2Table 2: Mean entropy values and standard devi-ation obtained in the Mechanical Turk experimentfor each question?answer pair category.
(4) A: Have you been living there very long?B: I?m in here right now about twelve anda half years.Category I, which consists of pairs of modifiers,can be further divided.
In most dialogues, the an-swer contains another adjective than the one usedin the question, such as in (1).
In others, the an-swer contains the same adjective as in the ques-tion, but modified by an adverb (e.g., very, basi-cally, quite) as in (5) or a negation as in (6).
(5) A: That seems to be the biggest sign ofprogress there.
Is that accurate?B: That?s absolutely accurate.
(6) A: Are you bitter?B: I?m not bitter because I?m a soldier.The negation can be present in the main clausewhen the adjectival predication is embedded, as inexample (7).
(7) A: [.
.
. ]
Is that fair?B: I don?t think that?s a fair statement.In a few cases, when the question contains an ad-jective modifying a noun, the adjective is omittedin the answer:(8) A: Is that a huge gap in the system?B: It is a gap.Table 1 gives the distribution of the types ap-pearing in the corpus.3.2 Answer assignmentTo assess the degree to which each answer con-veys ?yes?
or ?no?
in context, we use response dis-tributions from Mechanical Turk workers.
Given awritten dialogue between speakers A and B, Turk-ers were asked to judge what B?s answer conveys:?definite yes?, ?probable yes?, ?uncertain?, ?proba-ble no?, ?definite no?.
Within each of the two ?yes?and ?no?
pairs, there is a scalar relationship, butthe pairs themselves are not in a scalar relationshipwith each other, and ?uncertain?
is arguably a sep-arate judgment.
Figure 1 shows the exact formu-lation used in the experiment.
For each dialogue,we got answers from 30 Turkers, and we took thedominant response as the correct one though wemake extensive use of the full response distribu-tions in evaluating our approach.2 We also com-puted entropy values for the distribution of an-swers for each item.
Overall, the agreement wasgood: 21 items have total agreement (entropy of0.0 ?
11 in the ?adjective?
category, 9 in the?adverb-adjective?
category and 1 in the ?nega-tion?
category), and 80 items are such that a singleresponse got chosen 20 or more times (entropy <0.9).
The dialogues in (1) and (9) are examples oftotal agreement.
In contrast, (10) has response en-tropy of 1.1, and item (11) has the highest entropyof 2.2.
(9) A: Advertisements can be good or bad.Was it a good ad?B: It was a great ad.
(10) A: Am I clear?B: I wish you were a little more forthright.
(11) A: 91 percent of the American people stillexpress confidence in the long-termprospect of the U.S. economy; only 8percent are not confident.
Are theyoverly optimistic, in your professionalassessment?2120 Turkers were involved (the median number of itemsdone was 28 and the mean 56.5).
The Fleiss?
Kappa score forthe five response categories is 0.46, though these categoriesare partially ordered.
For the three-category response systemused in section 5, which arguably has no scalar ordering, theFleiss?
Kappa is 0.63.
Despite variant individual judgments,aggregate annotations done with Mechanical Turk have beenshown to be reliable (Snow et al, 2008; Sheng et al, 2008;Munro et al, 2010).
Here, the relatively low Kappa scoresalso reflect the uncertainty inherent in many of our examples,uncertainty that we seek to characterize and come to gripswith computationally.169Indirect Answers to Yes/No QuestionsIn the following dialogue, speaker A asks a simple Yes/Noquestion, but speaker B answers with something more in-direct and complicated.dialogue hereWhich of the following best captures what speaker Bmeant here:?
B definitely meant to convey ?Yes?.?
B probably meant to convey ?Yes?.?
B definitely meant to convey ?No?.?
B probably meant to convey ?No?.?
(I really can?t tell whether B meant to convey ?Yes?or ?No?.
)Figure 1: Design of the Mechanical Turk experi-ment.B: I think it shows how wise the Americanpeople are.Table 2 shows the mean entropy values for thedifferent categories identified in the corpus.
Inter-estingly, the pairs involving an adverbial modifi-cation in the answer all received a positive answer(?yes?
or ?probable yes?)
as dominant response.All 19 dialogues involving a numerical measurehad either ?probable yes?
or ?uncertain?
as domi-nant response.
There is thus a significant bias forpositive answers: 70% of the category I items and74% of the category II items have a positive an-swer as dominant response.
Examining a subsetof the Dialog Act corpus, we found that 38% ofthe yes/no questions receive a direct positive an-swers, whereas 21% have a direct negative answer.This bias probably stems from the fact that peopleare more likely to use an overt denial expressionwhere they need to disagree, whether or not theyare responding indirectly.4 MethodsIn this section, we present the methods we proposefor grounding the meanings of scalar modifiers.4.1 Learning modifier scales and inferringyes/no answersThe first technique targets items such as the onesin category I of our corpus.
Our central hypothesisis that, in polar question dialogues, the semanticrelationship between the main predication PQ inthe question and the main predication PA in the an-swer is the primary factor in determining whether,and to what extent, ?yes?
or ?no?
was intended.
IfPA is at least as strong as PQ, the intended answeris ?yes?
; if PA is weaker than PQ, the intended an-swer is ?no?
; and, where no reliable entailment re-lationship exists between PA and PQ, the result isuncertainty.For example, good is weaker (lower on the rel-evant scale) than excellent, and thus speakers in-fer that the reply in example (1) above is meant toconvey ?yes?.
In contrast, if we reverse the orderof the modifiers ?
roughly, Is it a great idea?
;It?s a good idea ?
then speakers infer that theanswer conveys ?no?.
Had B replied with It?s acomplicated idea in (1), then uncertainty wouldlikely have resulted, since good and complicatedare not in a reliable scalar relationship.
Negationreverses scales (Horn, 1972; Levinson, 2000), so itflips ?yes?
and ?no?
in these cases, leaving ?uncer-tain?
unchanged.
When both the question and theanswer contain a modifier (such as in (9?11)), theyes/no response should correlate with the extent towhich the pair of modifiers can be put into a scalebased on contextual entailment.To ground such scales from text, we collected alarge corpus of online reviews from IMDB.
Eachof the reviews in this collection has an associatedstar rating: one star (most negative) to ten stars(most positive).
Table 3 summarizes the distribu-tion of reviews as well as the number of words andvocabulary across the ten rating categories.As is evident from table 3, there is a signif-icant bias for ten-star reviews.
This is a com-mon feature of such corpora of informal, user-provided reviews (Chevalier and Mayzlin, 2006;Hu et al, 2006; Pang and Lee, 2008).
However,since we do not want to incorporate the linguis-tically uninteresting fact that people tend to writea lot of ten-star reviews, we assume uniform pri-ors for the rating categories.
Let count(w, r) bethe number of tokens of word w in reviews in rat-ing category r, and let count(r) be the total wordcount for all words in rating category r. The prob-ability of w given a rating category r is simplyPr(w|r) = count(w, r)/ count(r).
Then under theassumption of uniform priors, we get Pr(r|w) =Pr(w|r)/?r?
?R Pr(w|r?
).In reasoning about our dialogues, we rescalethe rating categories by subtracting 5.5 from each,to center them at 0.
This yields the scale R =170Rating Reviews Words Vocabulary Average words per review1 124,587 25,389,211 192,348 203.792 51,390 11,750,820 133,283 228.663 58,051 13,990,519 148,530 241.004 59,781 14,958,477 156,564 250.225 80,487 20,382,805 188,461 253.246 106,145 27,408,662 225,165 258.227 157,005 40,176,069 282,530 255.898 195,378 48,706,843 313,046 249.309 170,531 40,264,174 273,266 236.1110 358,441 73,929,298 381,508 206.25Total 1,361,796 316,956,878 1,160,072 206.25Table 3: Numbers of reviews, words and vocabulary size per rating category in the IMDB review corpus,as well as the average number of words per review.enjoyable0.00.10.20.30.4-4.5-3.5-2.5-1.5-0.5 0.5 1.5 2.5 3.5 4.5ER = 0.74best0.00.10.20.30.4-4.5-3.5-2.5-1.5-0.5 0.5 1.5 2.5 3.5 4.5ER = 1.08great0.00.10.20.30.4-4.5-3.5-2.5-1.5-0.5 0.5 1.5 2.5 3.5 4.5ER = 1.1superb0.00.10.20.30.4-4.5-3.5-2.5-1.5-0.5 0.5 1.5 2.5 3.5 4.5ER = 2.18disappointing0.00.10.20.30.4-4.5-3.5-2.5-1.5-0.5 0.5 1.5 2.5 3.5 4.5ER = -1.1bad0.00.10.20.30.4-4.5-3.5-2.5-1.5-0.5 0.5 1.5 2.5 3.5 4.5ER = -1.47awful0.00.10.20.30.4-4.5-3.5-2.5-1.5-0.5 0.5 1.5 2.5 3.5 4.5ER = -2.5worst0.00.10.20.30.4-4.5-3.5-2.5-1.5-0.5 0.5 1.5 2.5 3.5 4.5ER = -2.56Pr(Rating|Word)Rating (centered at 0)Figure 2: The distribution of some scalar modifiers across the ten rating categories.
The vertical linesmark the expected ratings, defined as a weighted sum of the probability values (black dots).?
?4.5,?3.5,?2.5,?1.5,?0.5, 0.5, 1.5, 2.5, 3.5, 4.5?.Our rationale for this is that modifiers at the neg-ative end of the scale (bad, awful, terrible) arenot linguistically comparable to those at thepositive end of the scale (good, excellent, superb).Each group forms its own qualitatively differentscale (Kennedy and McNally, 2005).
Rescalingallows us to make a basic positive vs. negativedistinction.
Once we have done that, an increasein absolute value is an increase in strength.
Inour experiments, we use expected rating valuesto characterize the polarity and strength of mod-ifiers.
The expected rating value for a word wis ER(w) =?r?R r Pr(r|w).
Figure 2 plots thesevalues for a number of scalar terms, both positiveand negative, across the rescaled ratings, withthe vertical lines marking their ER values.
Theweak scalar modifiers all the way on the left aremost common near the middle of the scale, witha slight positive bias in the top row and a slightnegative bias in the bottom row.
As we movefrom left to right, the bias for one end of the scalegrows more extreme, until the words in questionare almost never used outside of the most extremerating category.
The resulting scales correspondwell with linguistic intuitions and thus providean initial indication that the rating categoriesare a reliable guide to strength and polarity forscalar modifiers.
We put this information to usein our dialogue corpus via the decision procedure171Let D be a dialogue consisting of (i) a polar questionwhose main predication is based on scalar predicate PQand (ii) an indirect answer whose main predication isbased on scalar predicate PA. Then:1. if PA or PQ is missing from our data, infer ?Uncer-tain?;2.
else if ER(PQ) and ER(PA) have different signs, in-fer ?No?;3.
else if abs(ER(PQ)) 6 abs(ER(PA)), infer ?Yes?;4.
else infer ?No?.5.
In the presence of negation, map ?Yes?
to ?No?, ?No?to ?Yes?, and ?Uncertain?
to ?Uncertain?.Figure 3: Decision procedure for using the wordfrequencies across rating categories in the reviewcorpus to decide what a given answer conveys.described in figure 3.4.2 Interpreting numerical answersThe second technique aims at determiningwhether a numerical answer counts as a positiveor negative instance of the adjective in the ques-tion (category II in our corpus).Adjectives that can receive a conventional unitof measure, such as little or long, inherently pos-sess a degree of vagueness (Kamp and Partee,1995; Kennedy, 2007): while in the extreme cases,judgments are strong (e.g., a six foot tall womancan clearly be called ?a tall woman?
whereas afive foot tall woman cannot), there are borderlinecases for which it is difficult to say whether theadjectival predication can truthfully be ascribedto them.
A logistic regression model can capturethese facts.
To build this model, we gather distri-butional information from the Web.For instance, in the case of (3), we can retrievefrom the Web positive and negative examples ofage in relation to the adjective and the modified en-tity ?little kids?.
The question contains the adjec-tive and the modified entity.
The reply contains theunit of measure (here ?year-old?)
and the numer-ical answer.
Specifically we query the Web usingYahoo!
BOSS (Academic) for ?little kids?
year-old (positive instances) as well as for ?not littlekids?
year-old (negative instances).
Yahoo!
BOSSis an open search services platform that provides aquery API for Yahoo!
Web search.
We then ex-tract ages from the positive and negative snippetsobtained, and we fit a logistic regression to thesedata.
To remove noise, we discard low counts(positive and negative instances for a given unit< 5).
Also, for some adjectives, such as little oryoung, there is an inherent ambiguity between ab-solute and relative uses.
Ideally, a word sense dis-ambiguation system would be used to filter thesecases.
For now, we extract the largest contiguousrange for which the data counts are over the noisethreshold.3 When not enough data is retrieved forthe negative examples, we expand the query bymoving the negation outside the search phrase.
Wealso replace the negation and the adjective by theantonyms given in WordNet (using the first sense).The logistic regression thus has only one fac-tor ?
the unit of measure (age in the case of lit-tle kids).
For a given answer, the model assigns aprobability indicating the extent to which the ad-jectival property applies to that answer.
If the fac-tor is a significant predictor, we can use the prob-abilities from the model to decide whether the an-swer qualifies as a positive or negative instance ofthe adjective in the question, and thus interpret theindirect response as a ?yes?
or a ?no?.
The prob-abilistic nature of this technique adheres perfectlyto the fact that indirect answers are intimately tiedup with uncertainty.5 Evaluation and resultsOur primary goal is to evaluate how well we canlearn the relevant scalar and entailment relation-ships from the Web.
In the evaluation, we thus ap-plied our techniques to a manually coded corpusversion.
For the adjectival scales, we annotatedeach example for its main predication (modifier, oradverb?modifier bigram), including whether thatpredication was negated.
For the numerical cases,we manually constructed the initial queries: weidentified the adjective and the modified entity inthe question, and the unit of measure in the answer.However, we believe that identifying the requisitepredications and recognizing the presence of nega-tion or embedding could be done automatically us-ing dependency graphs.43Otherwise, our model is ruined by references to ?young80-year olds?, using the relative sense of young, which aremoderately frequent on the Web.4As a test, we transformed our corpus into the Stanforddependency representation (de Marneffe et al, 2006), usingthe Stanford parser (Klein and Manning, 2003) and were ableto automatically retrieve all negated modifier predications,except one (We had a view of it, not a particularly good one),172Modification in answer Precision RecallI Other adjective 60 60Adverb - same adjective 95 95Negation - same adjective 100 100Omitted adjective 100 100II Numerical 89 40Total 75 71Table 4: Summary of precision and recall (%) bytype.Response Precision Recall F1I Yes 87 76 81No 57 71 63II Yes 100 36 53Uncertain 67 40 50Table 5: Precision, recall, and F1 (%) per responsecategory.
In the case of the scalar modifiers exper-iment, there were just two examples whose dom-inant response from the Turkers was ?Uncertain?,so we have left that category out of the results.
Inthe case of the numerical experiment, there werenot any ?No?
answers.To evaluate the techniques, we pool the Me-chanical Turk ?definite yes?
and ?probable yes?categories into a single category ?Yes?, and wedo the same for ?definite no?
and ?probable no?.Together with ?uncertain?, this makes for three-response categories.
We count an inference assuccessful if it matches the dominant Turker re-sponse category.
To use the three-response schemein the numerical experiment, we simply catego-rize the probabilities as follows: 0?0.33 = ?No?,0.33?0.66 = ?Uncertain?, 0.66?1.00 = ?Yes?.Table 4 gives a breakdown of our system?s per-formance on the various category subtypes.
Theoverall accuracy level is 71% (159 out of 224 in-ferences correct).
Table 5 summarizes the resultsper response category, for the examples in whichboth the question and answer contain a gradablemodifier (category I), and for the numerical cases(category II).6 Analysis and discussionPerformance is extremely good on the ?Adverb ?same adjective?
and ?Negation ?
same adjective?cases because the ?Yes?
answer is fairly direct forthem (though adverbs like basically introduce aninteresting level of uncertainty).
The results arebecause of a parse error which led to wrong dependencies.Response Precision Recall F1WordNet-based Yes 82 83 82.5(items I) No 60 56 58Table 6: Precision, recall, and F1 (%) per responsecategory for the WordNet-based approach.somewhat mixed for the ?Other adjective?
cate-gory.Inferring the relation between scalar adjectiveshas some connection with work in sentiment de-tection.
Even though most of the research in thatdomain focuses on the orientation of one term us-ing seed sets, techniques which provide the ori-entation strength could be used to infer a scalarrelation between adjectives.
For instance, Blair-Goldensohn et al (2008) use WordNet to developsentiment lexicons in which each word has a posi-tive or negative value associated with it, represent-ing its strength.
The algorithm begins with seedsets of positive, negative, and neutral terms, andthen uses the synonym and antonym structure ofWordNet to expand those initial sets and refinethe relative strength values.
Using our own seedsets, we built a lexicon using Blair-Goldensohnet al (2008)?s method and applied it as in figure3 (changing the ER values to sentiment scores).Both approaches achieve similar results: for the?Other adjective?
category, the WordNet-basedapproach yields 56% accuracy, which is not signif-icantly different from our performance (60%); forthe other types in category I, there is no differencein results between the two methods.
Table 6 sum-marizes the results per response category for theWordNet-based approach (which can thus be com-pared to the category I results in table 5).
Howeverin contrast to the WordNet-based approach, we re-quire no hand-built resources: the synonym andantonym structures, as well as the strength values,are learned from Web data alone.
In addition, theWordNet-based approach must be supplementedwith a separate method for the numerical cases.In the ?Other adjective?
category, 31 itemsinvolve oppositional terms: canonical antonyms(e.g., right/wrong, good/bad) as well as termsthat are ?statistically oppositional?
(e.g., ready/premature, true/preposterous, confident/nervous).
?Statistically oppositional?
terms are not opposi-tional by definition, but as a matter of contingentfact.
Our technique accurately deals with most1730 10 20 30 40 50 600.00.20.40.60.8little kidsAgeProbabilityofbeing"little"0 10 20 30 40 50 600.20.40.60.8young kidsAgeProbabilityofbeing"young"0 20 40 60 80 100 1200.30.40.50.60.70.8warm weatherDegreeProbabilityofbeing"warm"Figure 4: Probabilities of being appropriately described as ?little?, ?young?
or ?warm?, fitted on dataretrieved when querying the Web for ?little kids?, ?young kids?
and ?warm weather?.of the canonical antonyms, and also finds somecontingent oppositions (qualified/young, wise/neurotic) that are lacking in antonymy resources orautomatically generated antonymy lists (Moham-mad et al, 2008).
Out of these 31 items, our tech-nique correctly marks 18, whereas Mohammad etal.
?s list of antonyms only contains 5 and Blair-Goldensohn et al (2008)?s technique finds 11.
Ourtechnique is solely based on unigrams, and couldbe improved by adding context: making use of de-pendency information, as well as moving beyondunigrams.In the numerical cases, precision is high but re-call is low.
For roughly half of the items, notenough negative instances can be gathered fromthe Web and the model lacks predictive power (asfor items (4) or (12)).
(12) A: Do you happen to be working for alarge firm?B: It?s about three hundred and fiftypeople.Looking at the negative hits for item (12), onesees that few give an indication about the num-ber of people in the firm, but rather qualificationsabout colleagues or employees (great people, peo-ple?s productivity), or the hits are less relevant:?Most of the people I talked to were actually prettyoptimistic.
They were rosy on the job marketand many had jobs, although most were not largefirm jobs?.
The lack of data comes from the factthat the queries are very specific, since the adjec-tive depends on the product (e.g., ?expensive ex-ercise bike?, ?deep pond?).
However when wedo get a predictive model, the probabilities corre-Entropy of response distributionProbabilityof correct inferencebyoursystem0.0 0.5 1.0 1.500.10.20.30.40.50.60.70.80.91Figure 5: Correlation between agreement amongTurkers and whether the system gets the correctanswer.
For each dialogue, we plot a circle atTurker response entropy and either 1 = correctinference or 0 = incorrect inference, except thepoints are jittered a little vertically to show wherethe mass of data lies.
As the entropy rises (i.e., asagreement levels fall), the system?s inferences be-come less accurate.
The fitted logistic regressionmodel (black line) has a statistically significant co-efficient for response entropy (p < 0.001).174late almost perfectly with the Turkers?
responses.This happens for 8 items: ?expensive to call (50cents a minute)?, ?little kids (7 and 10 year-old)?,?long growing season (3 months)?, ?lot of land(80 acres)?, ?warm weather (80 degrees)?, ?youngkids (5 and 2 year-old)?, ?young person (31 year-old)?
and ?large house (2450 square feet)?.
Inthe latter case only, the system output (uncer-tain) doesn?t correlate with the Turkers?
judgment(where the dominant answer is ?probable yes?
with15 responses, and 11 answers are ?uncertain?
).The logistic curves in figure 4 capture nicely theintuitions that people have about the relation be-tween age and ?little kids?
or ?young kids?, aswell as between Fahrenheit degrees and ?warmweather?.
For ?little kids?, the probabilities of be-ing little or not are clear-cut for ages below 7 andabove 15, but there is a region of vagueness in be-tween.
In the case of ?young kids?, the probabil-ities drop less quickly with age increasing (an 18year-old can indeed still be qualified as a ?youngkid?).
In sum, when the data is available, thismethod delivers models which fit humans?
intu-itions about the relation between numerical mea-sure and adjective, and can handle pragmatic in-ference.If we restrict attention to the 66 examples onwhich the Turkers completely agreed about whichof these three categories was intended (again pool-ing ?probable?
and ?definite?
), then the percent-age of correct inferences rises to 89% (59 cor-rect inferences).
Figure 5 plots the relation-ship between the response entropy and the accu-racy of our decision procedure, along with a fit-ted logistic regression model using response en-tropy to predict whether our system?s inferencewas correct.
The handful of empirical points inthe lower left of the figure show cases of highagreement between Turkers but incorrect infer-ence from the system.
The few points in the up-per right indicate low agreement between Turk-ers and correct inference from the system.
Threeof the high-agreement/incorrect-inference casesinvolve the adjectives right?correct.
For low-agreement/correct-inference, the disparity couldtrace to context dependency: the ordering is clearin the context of product reviews, but unclear ina television interview.
The analysis suggests thatoverall agreement is positively correlated with oursystem?s chances of making a correct inference:our system?s accuracy drops as human agreementlevels drop.7 ConclusionWe set out to find techniques for grounding ba-sic meanings from text and enriching those mean-ings based on information from the immediate lin-guistic context.
We focus on gradable modifiers,seeking to learn scalar relationships between theirmeanings and to obtain an empirically grounded,probabilistic understanding of the clear and fuzzycases that they often give rise to (Kamp and Partee,1995).
We show that it is possible to learn the req-uisite scales between modifiers using review cor-pora, and to use that knowledge to drive inferencein indirect responses.
When the relation in ques-tion is not too specific, we show that it is also pos-sible to learn the strength of the relation betweenan adjective and a numerical measure.AcknowledgmentsThis paper is based on work funded in part byONR award N00014-10-1-0109 and ARO MURIaward 548106, as well as by the Air Force Re-search Laboratory (AFRL) under prime contractno.
FA8750-09-C-0181.
Any opinions, findings,and conclusion or recommendations expressed inthis material are those of the authors and do notnecessarily reflect the view of the Air Force Re-search Laboratory (AFRL), ARO or ONR.ReferencesJames F. Allen and C. Raymond Perrault.
1980.
Ana-lyzing intention in utterances.
Artificial Intelligence,15:143?178.Nicholas Asher and Alex Lascarides.
2003.
Logics ofConversation.
Cambridge University Press, Cam-bridge.Sasha Blair-Goldensohn, Kerry Hannan, Ryan McDon-ald, Tyler Neylon, George A. Reis, and Jeff Reynar.2008.
Building a sentiment summarizer for localservice reviews.
In WWW Workshop on NLP in theInformation Explosion Era (NLPIX).Judith A.
Chevalier and Dina Mayzlin.
2006.
Theeffect of word of mouth on sales: Online book re-views.
Journal of Marketing Research, 43(3):345?354.Herbert H. Clark.
1979.
Responding to indirect speechacts.
Cognitive Psychology, 11:430?477.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typed175dependency parses from phrase structure parses.
InProceedings of the 5th International Conference onLanguage Resources and Evaluation (LREC-2006).Marie-Catherine de Marneffe, Scott Grimm, andChristopher Potts.
2009.
Not a simple ?yes?
or?no?
: Uncertainty in indirect answers.
In Proceed-ings of the 10th Annual SIGDIAL Meeting on Dis-course and Dialogue.Gilles Fauconnier.
1975.
Pragmatic scales and logicalstructure.
Linguistic Inquiry, 6(3):353?375.Nancy Green and Sandra Carberry.
1994.
A hybridreasoning model for indirect answers.
In Proceed-ings of the 32nd Annual Meeting of the Associationfor Computational Linguistics, pages 58?65.Nancy Green and Sandra Carberry.
1999.
Interpret-ing and generating indirect answers.
ComputationalLinguistics, 25(3):389?435.Beth Ann Hockey, Deborah Rossen-Knill, BeverlySpejewski, Matthew Stone, and Stephen Isard.1997.
Can you predict answers to Y/N questions?Yes, No and Stuff.
In Proceedings of Eurospeech1997, pages 2267?2270.Laurence R Horn.
1972.
On the Semantic Properties ofLogical Operators in English.
Ph.D. thesis, UCLA,Los Angeles.Nan Hu, Paul A. Pavlou, and Jennifer Zhang.
2006.Can online reviews reveal a product?s true quality?
:Empirical findings and analytical modeling of onlineword-of-mouth communication.
In Proceedings ofElectronic Commerce (EC), pages 324?330.Daniel Jurafsky, Elizabeth Shriberg, and Debra Bi-asca.
1997.
Switchboard SWBD-DAMSL shallow-discourse-function annotation coders manual, draft13.
Technical Report 97-02, University of Colorado,Boulder Institute of Cognitive Science.Hans Kamp and Barbara H. Partee.
1995.
Prototypetheory and compositionality.
Cognition, 57(2):129?191.Christopher Kennedy and Louise McNally.
2005.Scale structure and the semantic typology of grad-able predicates.
Language, 81(2):345?381.Christopher Kennedy.
2007.
Vagueness and grammar:The semantics of relative and absolute gradable ad-jectives.
Linguistics and Philosophy, 30(1):1?45.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Meeting of the Association of ComputationalLinguistics.Stephen C. Levinson.
2000.
Presumptive Meanings:The Theory of Generalized Conversational Implica-ture.
MIT Press, Cambridge, MA.Saif Mohammad, Bonnie Dorr, and Graeme Hirst.2008.
Computing word-pair antonymy.
In Proceed-ings of the Conference on Empirical Methods in Nat-ural Language Processing and Computational Nat-ural Language Learning (EMNLP-2008).Robert Munro, Steven Bethard, Victor Kuperman,Vicky Tzuyin Lai, Robin Melnick, ChristopherPotts, Tyler Schnoebelen, and Harry Tily.
2010.Crowdsourcing and language studies: The new gen-eration of linguistic data.
In NAACL 2010 Workshopon Creating Speech and Language Data With Ama-zon?s Mechanical Turk.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and Trends in In-formation Retrieval, 2(1):1?135.C.
Raymond Perrault and James F. Allen.
1980.
Aplan-based analysis of indirect speech acts.
Amer-ican Journal of Computational Linguistics, 6(3-4):167?182.Victor S. Sheng, Foster Provost, and Panagiotis G.Ipeirotis.
2008.
Get another label?
improving dataquality and data mining using multiple, noisy label-ers.
In Proceedings of KDD-2008.Rion Snow, Brendan O?Connor, Daniel Jurafsky, andAndrew Y. Ng.
2008.
Cheap and fast ?
but is itgood?
evaluating non-expert annotations for naturallanguage tasks.
In Proceedings of the Conference onEmpirical Methods in Natural Language Process-ing and Computational Natural Language Learning(EMNLP-2008).Henk Zeevat.
1994.
Questions and exhaustivity in up-date semantics.
In Harry Bunt, Reinhard Muskens,and Gerrit Rentier, editors, Proceedings of the In-ternational Workshop on Computational Semantics,pages 211?221.176
