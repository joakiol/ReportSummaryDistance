Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 76?81,Baltimore, Maryland USA, June 26 2014.c?2014 Association for Computational LinguisticsTowards README-EVAL : Interpreting README File InstructionsJames Paul WhiteDepartment of LinguisticsUniversity of WashingtonSeattle WA 98195-4340jimwhite@uw.eduAbstractThis abstract describes README-EVAL,a novel measure for semantic parsing eval-uation of interpreters for instructions incomputer program README files.
Thatis enabled by leveraging the tens of thou-sands of Open Source Software programsthat have been annotated by package main-tainers of GNU/Linux operating systems.We plan to make available a public sharedimplementation of this evaluation.1 IntroductionThat natural language is learned by humans inrich grounded perceptual contexts has been rec-ognized by many researchers for quite some time(Regier, 1996) (Silberer and Lapata, 2012).
Butmost efforts at machine learning of natural lan-guage continue to address tasks which are en-tirely divorced from any grounding and/or haveperceptual requirements for which machines areill-suited.
Computers are machines and their nat-ural perceptual context is that of the computingmachine world.
Therefore, to apply the model ofgrounded language learning most effectively, weshould choose tasks in which the relevant perceptsare of those in the computing world (e.g., bits,bytes, characters, files, memory, operations, pro-grams, events, processes, services, devices, pro-cessors, drivers, operating systems, and networks).This abstract describes proposed work aimedat the goal of deep semantic parsing of the web,which for us includes the ability to interpret doc-uments that give instructions for acting on com-puter systems in human natural language.
To facil-itate research in that direction, we plan to evaluatesystems that build software packages by follow-ing the README1file instructions contained in1We use the term README file in a broad sense mean-ing a document that contains instructions to be read by a hu-GNU/Linux distributions like Centos and Debian.Key to this plan is the novel README-EVALscore which we propose as an extrinsic (i.e.
goal-oriented) performance measure for parsing, map-ping/planning, and related linguistics tasks.
Theplanned baseline system is a pipeline using a doc-ument classifier and instruction sequence extractortrained on hand-labeled data followed by a rein-forcement learner for mapping the instructions toa build script (plan of actions) for that softwarepackage (context).2 BackgroundA significant challenge for semantic parsing re-search is finding a method to measure a system?sperformance that will indicate its effectiveness inthe domain of interest.
Traditionally the approachhas been to gather and have human annotatorsmake judgements that are of the same kind thesystem is intended to perform.
That process is rel-atively costly and may result in a corpus which isactually too small considering the amount of varia-tion that occurs when humans perform an activity.Relevant prior work in the computing domain pro-duced the Linux and Monroe plan corpora (Blay-lock and Allen, 2005).
The Linux Plan Corpusconsists of 457 interactive shell script sessions,with an average of 6.1 actions each, captured fromhuman experimental subjects attempting to satisfyone of 19 different goals stated as an English sen-tence.
Although it has been used successfully bythose and other researchers, the natural variationin human behavior means that a corpus of suchrelatively small size appears to be very noisy.
Asa result they have had to rely on artificially gener-ated data such as the Monroe Plan Corpus in orderto get results that are more easily compared acrosssystem evaluations.man that concern performing actions on a computer (whetherat the keyboard or some other input device).
For this taskwe confine ourselves to instructions given for the purpose ofbuilding a software package.76More promising therefore is the way some re-searchers have discovered ways to repurpose dataand/or judgements created for other purposes andturn them into training data and/or evaluations ofNLP systems.
We employ that paradigm here byrepurposing the efforts of Open Source Software(OSS) package maintainers who have created an-notations (aka metadata) including dependency re-lations and scripts that build computer programs.3 GNU/Linux Software Package DataThe advent of the Internet resulted in explosivegrowth for OSS, the premier example of whichis the GNU/Linux operating system family.
Cur-rent distributions contain packages built from over15,000 program source bundles.2The productionof OSS packages for such systems typically in-volves two different types of programmers work-ing independently.
The authors of the sourcecomputer program usually do not produce pack-aging metadata for their work and instead tendto write README files and related documenta-tion explaining how to build and use the software.The package maintainers then work out the spe-cific requirements and scripts necessary to buildthe program as some package(s) using the partic-ular package manager and format of the OS dis-tribution (aka ?distro?)
that they are supporting.Software package metadata contained in bundlessuch as Debian .deb and Fedora RPM .specfiles are rich in annotations.3,4See Figure 1 for excerpts of text describing theBean Scripting Framework (BSF) from its SourceRPM Package Manager (SRPM) package in theFedora Core 17 distribution.5The two kinds ofdata shown are file contents (1a, 1c, 1e), whichusually originate with the ?upstream?
program au-thor(s), and sections from the RPM Spec file (1b,1d, 1f), which are annotations (aka metadata) cu-rated by the package maintainers.
There are other2Debian Wheezy has over 37,000 packages from about17,500 source packages https://www.debian.org/News/2013/20130504 and Fedora 20 has more than15,000 packages https://admin.fedoraproject.org/pkgdb/collections/.3https://www.debian.org/doc/manuals/maint-guide/dreq.en.html4http://www.rpm.org/max-rpm/ch-rpm-inside.html5For more examples, we refer the interested reader theauthor?s web page which includes access to a web linkeddata explorer for the entire corpus.http://students.washington.edu/jimwhite/sp14.htmlsections and fields used in RPM Spec files, butthose tend to more distro-specific and these sufficefor this discussion.Figure 1a shows some BSF package descriptiontext from the source README.txt file and Figure1b shows the version appearing the RPM Spec.That close textual similarity is a common occur-rence in the data and can be used to identify somelikely README files.
Those are only a startingpoint though, because the natural language pro-gram build instructions are often in other files, asin this case.
For many packages those instruc-tions are in a file named INSTALL.
There is anINSTALL.txt file with some instructions for BSFhere (Figure 1e), but they are for a binary instal-lation.
The instructions for building from sourcethat we will primarily concerned with here are inthe file BUILDING.txt (Figure 1c).A potential use for this data that we haven?t ex-plored yet is its use in summarization tasks.
In ad-dition to the text which is usually in the READMEfile and RPM Spec DESCRIPTION section, thereis the ?Summary?
field of the PACKAGE section.Although in Figure 1d the value for the summaryfield appears as just the package?s full name, thisis typically a full sentence that is a good one-linesummary of the multiple line description section.It is worthwhile to notice that thousands ofprograms have been packaged multiple times fordifferent systems (e.g.
Debian, Fedora, Cygwin,NixOS, Homebrew, and others) and many pack-ages have also been internationalized.6Both ofthose aspects point to opportunities for learningfrom parallel data.For the present discussion we focus on two par-ticular elements of package metadata: dependen-cies and build scripts.7The packages in a distribu-tion have dependency relationships which desig-nate which packages must be built and installed forother packages to be built, installed, and/or exe-cuted.
These relationships form a directed acyclicgraph (DAG) in which the nodes are packages andthe edges are dependency relationships.6Debian for example currently lists more than 800k sen-tences in the localization database and about 75 human lan-guages have translations for at least 100k of them with thetop ten languages having over 500k each https://www.debian.org/international/l10n/po/rank.7Packaging systems usually support at least three types ofscripts: build, install, and remove.
The build script usuallyhas more in common with the README instructions thanthe install and remove scripts which are more distro specific.Some packages also have a check script to validate the stateof a build prior to performing the install operation.77(a) README.txt file (d) RPM Spec PACKAGE section (metadata)Bean Scripting Framework (BSF) is a set of Java classeswhich provides an easy to use scripting language supportwithin Java applications.
It also provides access to Javaobjects and methods from supported scripting languages.?
?
?
(b) RPM Spec DESCRIPTION sectionBean Scripting Framework (BSF) is a set of Java classeswhich provides scripting language support within Javaapplications, and access to Java objects and methods fromscripting languages.?
?
?
(c) BUILDING.txt fileFrom the ant "build.xml" file:Master Build file for BSFNotes:This is the build file for use withthe Jakarta Ant build tool.Optional additions:BeanShell -> http://www.beanshell.org/Jython -> http://www.jython.org/JRuby -> http://www.jruby.org/ (3rd ...)Xalan -> http://xml.apache.org/xalan-j.
.
.Build Instructions:To build, runjava org.apache.tools.ant.Main <target>on the directory where this file islocated with the target you want.Most useful targets:- all -> creates the binary and srcdistributions, and builds the site- compile -> creates the "bsf.jar"package in "./build/lib" (default target)- samples -> creates/compiles the samples?
?
?Name: bsfVersion: 2.4.0Release: 12.fc17Summary: Bean Scripting FrameworkLicense: ASL 2.0URL: http://commons.apache.org/bsf/Group: Development/LibrariesBuildRequires: jpackage-utils >= 1.6BuildRequires: ant, xalan-j2, jythonBuildRequires: rhinoBuildRequires: apache-commons-loggingRequires: xalan-j2Requires: apache-commons-loggingRequires: jpackage-utilsBuildArch: noarch?
?
?
(e) INSTALL.txt fileInstalling BSF consists of copyingbsf.jar and .jars for any languagesintended to be supported to a directoryin the execution CLASSPATH of yourapplication, or simply adding themto your CLASSPATH.BSF can be used either as a standalonesystem, as a class library, or as partof an application server.
In order to beused as a class library or as a standalonesystem, one must simply download thebsf.jar file from the BSF web site(http://jakarta.apache.org/bsf/index.html)and include it in their CLASSPATH, alongwith any required classes or jar filesimplementing the desired languages.?
?
?
(f) RPM Spec BUILD section (shell script)[ -z "$JAVA_HOME" ] && export JAVA_HOME=/usr/lib/jvm/javaexport CLASSPATH=$(build-classpath apache-commons-logging jython xalan-j2 rhino)ant jar/usr/bin/rm -rf bsf/src/org/apache/bsf/engines/javaant javadocsFigure 1: Bean Scripting Framework (BSF) excerpts from Fedora Core 17 RPMS.4 From Dependencies to ValidationThe idea that turns the package dependency DAGinto training, test, and evaluation data is to choosedependency targets for test (i.e.
the system buildscript outputs will be used for them in test) anddependency sources (the dependent packages) forvalidation (their package maintainer written buildscripts are used as is to observe whether the depen-dencies are likely to be good).
Validation subsetscan be arranged for both internal validation (tun-ing) and external validation (evaluation).Two kinds of dependency relationships areof special interest here: Requires andBuildRequires.
The former typically meansthe target package (its name appears to the right ofa Requires or BuildRequires in Figure 1d)is required at both build time and execution timeby the source package (identified by the Namefield of Figure 1d) while the latter means it is onlyrequired at build time.
That distinction can beused to guide the selection of which packages tochoose for the validation and test subsets.
Pack-ages that are the target of a BuildRequiresrelationship are more likely to cause their depen-dents?
build scripts to fail when they (the targets)are built incorrectly than targets of a Requiresrelationship.Analysis of the 2,121 packages in Release 17 ofthe Fedora Core SRPM distribution shows 1,673package nodes that have a build script and somedeclared dependency relationship.
Those buildscripts average 6.9 non-blank lines each.
Ofthose nodes, 1,009 are leaves and the 664 inter-78nal nodes are the target of an average of 7 de-pendencies each.
There are 218 internal nodesthat are the direct target of at least one leaf nodevia a BuildRequires relationship and they av-erage 12.4 such dependent leaves each.
We ex-pect to have a larger corpus prepared from a fullGNU/Linux distribution (at least 15,000 sourcepackages) at the time of the workshop.5 Task DescriptionThe top-level README-EVAL task would be togenerate complete packaging metadata given thesource files for a program thus automating thework of a package maintainer.
Since that taskis somewhat complicated, it is useful to breakit down into multiple subtasks which can be ad-dressed and evaluated separately before proceed-ing to combine them.
For the discussion here wewill consider a partial solution using a four stagepipeline: README document classification, in-struction extraction, dependency relation extrac-tion, and build script generation.The corpus?
package metadata can be used todirectly evaluate the results of the last two stagesof that pipeline.
The first two stages, READMEdocument classification and instruction extraction,are well understood tasks for which a moderateamount of manually labelled data can suffice totrain and test effective classifiers.The dependency relation extraction subtask canbe treated as a conventional information extractiontask concerned with named entity recognition forpackages and relation extraction for dependencies.We may regard the dependencies in the corpus aseffectively canonical because the package main-tainers strive to keep those annotations to a rea-sonable minimum.
Therefore computing precisionand recall scores of the dependency DAG edgesand labels of this stage?s output versus the corpus?metadata will be a meaningful metric.Work on instruction and direction following isapplicable to the build script generation subtask.Such systems tend to be somewhat more complexthan shallow extraction systems and may incor-porate further subcomponents including goal de-tectors and/or planners that interact with a seman-tic parser (Branavan et al., 2012).
It is possibleto evaluate the final stage output by comparing itto the build script in the package?s metadata, butthat would suffer from the same sort of evalua-tion problems that other language generation taskshave when we are concerned with semantics ratherthan syntax.
This is where the superiority of anNLP task where the target language is understoodby computers comes in, because we can also eval-uate it using execution.
Which isn?t to say we cansolve the program equivalence problem in general,but README-EVAL does a pragmatic determina-tion of how good a substitute it is based on its us-age by the package?s dependency sources.6 README-EVAL ScoringThe README-EVAL score is a measure of howeffective the system under test (SUT) is at gener-ating software package metadata.
For the compo-nents of the SUT this score can serve as an extrin-sic indication of their effectiveness.Let N be a set of tuples (x, y) representing thecorpus in which x is the package data and relevantmetadata subset minus the labels to be generatedand y is a known good label for x.
To prepare thecorpus for the task, two disjoint subsets C and Tare selected from the set of all package nodes N .C is for the common packages which are availableto the SUT for training, and T is for the test pack-ages that the SUT?s interpretation function will betested on.
A third set V which is disjoint from Tis selected from N for the validation packages.Many partitioning schemes are possible.
A sim-ple method is to choose the leaf nodes (packagesthat are sources but not targets of dependency re-lationships) for V .
The members of T can thenbe chosen as the set of packages which are the di-rect targets of the dependency relationships fromV .
The members of V are expected to be likelyto fail to build correctly if there are errors in thesystem outputs for T .
Note that for the SUT todo tuning it will need some leaf node packages inC.
Therefore if V is made disjoint from C then itshould not actually select all of those leaves.The README-EVAL score R is computed us-ing a suitable loss function L for the SUT?s la-bel predictor function?Y .
?Y is presumed to havebeen trained on C and it yields a set of (x, y?)
tu-ples given a set of x values.
The loss functionL((x, y), D) yields a real number in the range 0to 1 inclusive that indicates what fraction of thecomponents in package (x, y) are incorrect giventhe context D ?
N .
It is required for all v ?
Vthat L(v, (C ?
T ?
V ) \ {v}) = 0.For this exposition, assume y is a build scriptand L yields 0 if it succeeds and 1 if it fails.
Linuxprocesses typically indicate success by returning azero exit code.
Therefore a simple realization of79L is to return 0 if the process executing the buildscript y of (x, y) given D returns zero and 1 oth-erwise.The computation iterates over each memberD ?
partition(T ) and obtains measures of cor-rectness by evaluating B(?Y (X(D))?C ?
T \D)where X is a function that yields the set of x val-ues for a given set of (x, y) tuples.
To keep the taskas easy as possible, the members of partition(T )may be singletons.B(D) = |V | ?
?v?VL(v, (D ?
V ) \ {v})Those values are normalized by a scale factorfor each D determined by the value of B given Dminus B given Z(D).
Z(D) is the set of tuples(x, ?)
for a given set D where ?
is the null label.A null label for a build script is one which has noactions and executes successfully.R(D) =B(?Y (X(D))?C?T\D)B(C?T )?B(Z(D)?C?T\D)The final README-EVAL measureR is the av-erage score over those partitions:R =?D?partition(T )R(D)|partition(T )|6.1 Loss Function VariationsThere are other useful implementation variationsfor the loss function L. In a system where thenumber of components can be determined inde-pendently from whether they are correct or not, apossibly superior alternative is to return the num-ber of incorrect components divided by the totalnumber of components.
To determine loss for abuild script for example, the value may be deter-mined by counting the number of actions that exe-cute successfully and dividing by the total numberof steps.A further consideration in semantic evaluationis parsimony, which is the general expectation thatthe shortest adequate solution is to be preferred(Gagne et al., 2006).
To incorporate parsimonyin the evaluation we can add a measure(s) of thesolution?s cost(s), such as the size of the label yand/or execution resources consumed, to L.7 ConclusionA common objection to tackling this task is thatit seems too hard given the state of our knowl-edge about human language, computer program-ming (as performed by humans), and especiallythe capabilities of current NLP systems.
We con-sider that to be a feature rather than a bug.
Itmay be some time before a state-of-the-art im-plementation of a README interpreter is suffi-ciently capable to be considered comparable toan expert human GNU/Linux package maintainerperformance, but that is perfectly fine because wewould like to have an evaluation that is robust,long-lived, and applicable to many NLP subtasks.We also have the more pragmatic response givenhere which shows that that difficult task can bedecomposed into smaller subtasks like others thathave been addressed in the NLP and computa-tional linguistics communities.To conclude, this proposal recommendsREADME-EVAL as an extrinsic (goal-oriented)evaluation system for semantic parsing that couldprovide a meaningful indication of performancefor a variety of NLP components.Because the evaluation platform may be some-what complicated to set up and run, we wouldlike to make a publicly available shared evalua-tion platform on which it would be a simple matterto submit new systems or components for evalua-tion.
The MLcomp.org system developed by PercyLiang and Jacob Abernethy, a free website for ob-jectively comparing machine learning programs,is an especially relevant precedent (Gollub et al.,2012).
But we notice that the NLP tasks on ML-comp receive little activity (the last new run wasmore than a year ago at this writing) which is instark contrast to the other ML tasks which are veryactive (as they are on sites like Kaggle).
With theREADME-EVAL task available in such an easy-to-use manner could draw significant participationbecause of its interesting and challenging domain,especially from ML and other CS students and re-searchers.Finally we look forward to discussing this pro-posal with the workshop attendees, particularly inworking out the details for manual annotation ofthe README files for the instruction extractor(including whether it is needed), and discussingideas for a baseline implementation.8 AcknowledgementsThank you to my University of Washington col-leagues who reviewed earlier drafts of this abstractand the workshop?s blind reviewers for their help-ful comments.ReferencesNate Blaylock and James Allen.
2005.
RecognizingInstantiated Goals using Statistical Methods.
In IJ-CAI Workshop on Modeling Others from Observa-tions (MOO-2005), page 79.80S.
R. K. Branavan, Nate Kushman, Tao Lei, and ReginaBarzilay.
2012.
Learning High-Level Planning fromText.
In Proceedings of the 50th Annual Meetingof the Association for Computational Linguistics:Long Papers-Volume 1, page 126.
Association forComputational Linguistics.Christian Gagne, Marc Schoenauer, Marc Parizeau,and Marco Tomassini.
2006.
Genetic Programming,Validation Sets, and Parsimony Pressure.
In GeneticProgramming, page 109.
Springer.Tim Gollub, Benno Stein, and Steven Burrows.
2012.Ousting Ivory Tower Research: Towards a WebFramework for Providing Experiments as a Service.In Proceedings of the 35th international ACM SIGIRconference on Research and development in infor-mation retrieval, page 1125.
ACM.Terry Regier.
1996.
The Human Semantic Potential:Spatial Language and Constrained Connectionism.MIT Press.Carina Silberer and Mirella Lapata.
2012.
GroundedModels of Semantic Representation.
In Proceedingsof the 2012 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, page 1423.
Associa-tion for Computational Linguistics.81
