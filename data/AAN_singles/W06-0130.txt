Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 173?176,Sydney, July 2006. c?2006 Association for Computational LinguisticsChinese Named Entity Recognition with Conditional ProbabilisticModelsAitao ChenYahoo701 First AvenueSunnyvale, CA 94089aitao@yahoo-inc.comFuchun PengYahoo701 First AvenueSunnyvale, CA 94089fuchun@yahoo-inc.comRoy ShanYahoo701 First AvenueSunnyvale, CA 94089rshan@yahoo-inc.comGordon SunYahoo701 First AvenueSunnyvale, CA 94089gzsun@yahoo-inc.comAbstractThis paper describes the work on Chinesenamed entity recognition performed byYahoo team at the third InternationalChinese Language Processing Bakeoff.We used two conditional probabilisticmodels for this task, including condi-tional random fields (CRFs) and maxi-mum entropy models.
In particular, wetrained two conditional random field rec-ognizers and one maximum entropy rec-ognizer for identifying names of people,places, and organizations in un-segmented Chinese texts.
Our best per-formance is 86.2% F-score on MSRAdataset, and 88.53% on CITYU dataset.1 IntroductionAt the third International Chinese LanguageProcessing Bakeoff, we participated in the closedtest in the Named Entity Recognition (NER) taskusing the MSRA corpus and the CITYU corpus.The named entity types include person, place,and organization.
The training data consist oftexts that are segmented into words with namesof people, places, and organizations labeled.
Andthe testing data consist of un-segmented Chinesetexts, one sentence per line.There are many well known models for Eng-lish named recognition, among which Condi-tional Random Fields (Lafferty et al 2001) andmaximum entropy models (Berger et al 2001)have achieved good performance in English inCoNLL NER tasks.
To understand the perform-ance of these two models on Chinese, we bothmodels to Chinese NER task on MSRA data andCITYU data.2 Named Entity Recognizer2.1 ModelsWe trained two named entity recognizers basedon conditional random field and one based onmaximum entropy model.
Both conditional ran-dom field and maximum entropy models are ca-pable of modeling arbitrary features of the input,thus are well suit for many language processingtasks.
However, there exist significant differ-ences between these two models.
To apply amaximum entropy model to NER task, we haveto first train a maximum entropy classifier toclassify each individual word and then build adynamic programming for sequence decoding.While in CRFs, these two steps are integratedtogether.
Thus, in theory, CRFs are superior tomaximum entropy models in sequence modelingproblem and this will also confirmed in our Chi-nese NER experiments.
The superiority of CRFson Chinese information processing was alsodemonstrated in word segmentation (Peng et al2004).
However, the training speed of CRFs ismuch slower than that of maximum entropymodels since training CRFs requires expensiveforward-backward algorithm to compute the par-tition function.173We used Taku?s CRF package1  to train the firstCRF recognizer, and the MALLET 2  packagewith BFGS optimization to train the second CRFrecognizer.
We used a C++ implementation3 ofmaximum entropy modeling and wrote our ownsecond order dynamic programming for decod-ing.2.2 FeaturesThe first CRF recognizer used the features C-2, C-1, C0, C-1, C2, C-2C-1, C-1C0, C0C-1, C1C2, and C-1C1, where C0 is the current character, C1 the nextcharacter, C2 the second character after C0, C-1the character preceding C0, and C-2 the secondcharacter before C0.The second CRF recognizer used the same setof basic features but the feature C2.
In addition,the first CRF recognizer used the tag bigram fea-ture, and the second CRF recognizer used wordand character cluster features, obtained automati-cally from the training data only with distribu-tional word clustering (Tishby and Lee, 1993).The maximum entropy recognizer used thefollowing unigram, bigram features, and typefeatures: C-2, C-1, C0, C1, C2, C-4C-3, C-3C-2, C-2C-1,C-1C0, C0C1, C1C2, C2C3, C3C4, and T-2T-1.When using the first CRF package, we foundthe labeling scheme OBIE performs better thanthe OBIE scheme.
In the OBI scheme, the firstcharacter of a named entity is labeled as ?B?, theremaining characters, including the last character,are all labeled as ?I?.
And any character that isnot part of a named entity is labeled as ?O?.
Inthe OBIE scheme, the last character of a namedentity is labeled as ?E?.
The other characters arelabeled in the same way as in OBIE scheme.
Thefirst CRF recognizer used the OBIE labelingscheme, and the second CRF recognizer used theOBI scheme.We tried a window size of seven characters(three characters preceding the current characterand three characters following the current char-acter) with almost no difference in performancefrom using the window size of five characters.When a named entity occurs frequently in thetraining data, there is a very good chance that itwill be recognized when appearing in the testingdata.
However, for entity names of rare occur-rence, they are much harder to recognize in the1 Available from http://chasen.org/~taku/software/CRF++2 Available at http://mallet.cs.umass.edu3 Available athttp://homepages.inf.ed.ac.uk/s0450736/maxent_toolkit.htmtesting data.
Thus it may be beneficial to exam-ine the testing data to identify the named entitiesthat occur in the training data, and assign themthe same label as in the training data.
From thetraining data, we extracted the person names ofat least three characters, the place names of atleast four characters, and the organization namesof at least four characters.
We removed from thedictionary the named entities that are also com-mon words.
We did not include the short namesin the dictionary because they may be part oflong names.
We produced a run first using one ofthe NER recognizers, and then replaced the la-bels of a named entity assigned by a recognizerwith the labels of the same named entity in thetraining data without considering the contexts.3 ResultsRun ID Precision Recall F-Scoremsra_a 91.22% 81.71% 86.20msra_b 88.43% 82.88% 85.56msra_f 88.45% 79.31% 83.63msra_g 86.61% 80.32% 83.35msra_r 87.48% 71.68% 78.80Table 1: Official results in the closed test of theNER task on MSRA corpus.Table 1 presents the official results of five runsin the closed test of the NER task on MSRA cor-pus.
The first two runs, msra_a and msra_b, areproduced using the first CRF recognizer; the nexttwo runs, msra_f and msra_g, are produced usingthe second CRF recognizer which used randomlyselected 90% of the MSRA training data.
Whenwe retrained the second CRF recognizer with thewhole set of the MSRA training data, the overallF-Score is 85.00, precision 90.28%, and recall80.31%.
The last run, msra_r, is produced usingthe MaxEnt recognizer.The msra_a run used the set of basic featureswith a window size of five characters.
Slightlyover eight millions features are generated fromthe MSRA training data, excluding features oc-curred only once.
The training took 321 itera-tions to complete.
The msra_b run is producedfrom the msra_a run by substituting the labelsassigned by the recognizer to a named entity withthe labels of the named entity in the training dataif it occurs in the training data.
For example, inthe MSRA training data, the text?????
inthe sentence ???????????
istagged as a place name.
The same entity also ap-peared in MSRA testing data set.
The first CRFrecognizer failed to mark the text ?????
as174a place name instead it tagged ???
as a per-son name.
In post-processing, the text?????
in the testing data is re-tagged as a place name.As another example, the person name ??
?appears both in the training data and in the test-ing data.
The first CRF recognizer failed to rec-ognize it as a person name.
In post-processingthe text ???
is tagged as a person name be-cause it appears in the training data as a personname.
The text ??????????????????
was correctly tagged as an organizationname.
It is not in the training data, but the texts?????
?, ????????
?, and ???????
are present in the training data and are alllabeled as organization names.
In our post-processing, the correctly tagged organizationname is re-tagged incorrectly as three organiza-tion names.
This is the main reason why the per-formance of the organization name got muchworse than that without post-processing.Precision Recall F-scoreLOC 94.19% 87.14% 90.53ORG 83.59% 80.39% 81.96PER 92.35% 74.66% 82.57Table 2: The performance of the msra_a run bro-ken down by entity type.Precision Recall F-scoreLOC 93.09% 87.35% 90.13ORG 75.51% 78.51 76.98PER 91.52 79.27 84.95Table 3: The performance of the msra_b run bro-ken down by entity type.Table 2 presents the performance of the msra_arun by entity type.
Table 3 shows the perform-ance of the msra_b run by entity type.
While thepost-processing improved the performance ofperson name recognition, but it degraded the per-formance of organization name recognition.Overall the performance was worse than thatwithout post-processing.
In our developmenttesting, we saw large improvement in organiza-tion name recognition with post-processing.Run ID Precision Recall F-Scorecityu_a 92.66% 84.75% 88.53cityu_b 92.42% 84.91% 88.50cityu_f 91.88% 82.31% 86.83cityu_g 91.64% 82.46% 86.81Table 4: Official results in the closed test of theNER task on CITYU corpus.Table 4 presents the official results of four runsin the closed test of the NER task on CITYU cor-pus.
The first two runs, msra_a and msra_b, areproduced using the first CRF recognizer; the nexttwo runs, msra_f and msra_g, are produced usingthe second CRF recognizer.
The system configu-rations are the same as used on the MSRA cor-pus.
The cityu_b run is produced from cityu_arun with post-processing, and the cityu_g runproduced from cityu_f run with post-processing.We used the whole set of CITYU to train the firstCRF model, and 80% of the CITYU training datato train the second CRF model.
No results on fulltraining data are available at the time of submis-sion.All the runs we submitted are based characters.We tried word-based approach but found it wasnot as effective as character-based approach.4 DiscussionsTable 4 is shows the confusion matrix of the la-bels.
The rows are the true labels and the col-umns are the predicated labels.
An entry at row xand column y in the table is the number of char-acters that are predicated as y while the true labelis x.
Ideally, all entries except the diagonalshould be zero.The table was obtained from the result of ourdevelopment dataset for MSRA data, which arethe last 9,364 sentences of the MSRA trainingdata (we used the first 37,000 sentences for train-ing in the model developing phase).
As we cansee, most of the errors lie in the first column, in-dicating many of the entities labels are predi-cated as O.
This resulted low recall for entities.Another major error is on detecting the begin-ning of ORG (B-O).
Many of them are misla-beled as O and beginning of location (B-L), re-sulting low recall and low precision for ORG.O B-L I-L B-O I-O B-P I-PO 406798 86 196 213 973 46 111B-L 463 5185 54 73 29 19 7I-L 852 25 6836 0 197 1 44B-O 464 141 3 2693 62 17 0I-O 1861 28 276 55 12626 2 39B-P 472 16 2 22 3 2998 8I-P 618 0 14 1 49 10 5502Table 4: Confusion matrix of on the MSRA de-velopment datasetA second interesting thing to notice is thenumbers presented in Table 2.
They may suggestthat person name recognition is more difficult175than location name recognition, which is con-trary to what we believe, since Chinese personnames are short and have strict structure and theyshould be easier to recognize than both locationand organization names.
We examined theMSRA testing data and found out that 617 out1,973 person names occur in a single sentence asa list of person names.
In this case, simple rulemay be more effective.
When we excluded thesentence with 617 person names, for personname recognition of our msra_a run, the F-scoreis 90.74, precision 93.44%, and recall 88.20%.Out of the 500 person names that were not rec-ognized in our msra_a run, 340 occurred on thesame line of 617 person names.5 ConclusionsWe applied Conditional Random Fields andmaximum entropy models to Chinese NER tasksand achieved satisfying performance.
Three sys-tems with different implementations and differ-ent features are reported.
Overall, CRFs are su-perior to maximum entropy models in ChineseNER tasks.
Useful features include using BIOEtags instead of BIO tags and word and characterclustering features.ReferencesAdam Berger, Stephen Della Pietra, and VincentDella Pietra, A Maximum Entropy Approach toNatural Language Processing, Computational Lin-guistics, 22 (1)John Lafferty, Andrew McCallum, and FernandoPereira, Conditional random fields: Probabilisticmodels for segmenting and labeling sequence data.In: Proc.
18th International Conf.
on MachineLearning, Morgan Kaufmann, San Francisco, CA(2001) 282?289Fuchun Peng, Fangfang Feng, and AndrewMcCallum, Chinese Segmentation and New WordDetection using Conditional Random Fields, InProceedings of The 20th International Conferenceon Computational Linguistics (COLING 2004) ,pages 562-568, August 23-27, 2004, Geneva, Swit-zerlandNaftali Tishby and Lillian Lee, Distributional Cluster-ing of English Words, In Proceedings of the 31stAnnual Conference of Association for Computa-tional Linguistics, pp 183--190, 1993.176
