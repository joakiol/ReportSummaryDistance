Us ing  Categor ies  in  the  EUTRANS SystemJ.
C. Amengual 1 J .M .
Benedf 2A.
Castellanos 1 D. Llorens ~E.
V idal  2(1) Unidad Predepartamental de Inform?ticaCampus Penyeta RojaUniversitat Jaume I12071 Castelldn de la Pinna (Spain)AbstractThe EUTRANS project, aims at devel-oping Machine Translation systems forlimited domain applications.
These sys-tems accept speech and text input, andare trained using an example based ap-proach.
The translation model used inthis project is the Subsequential Trans-ducer, which is easily integrable in con-ventional speech recognition systems.
Inaddition, Subsequential Transducers canbe automatically learned from corpora.This paper describes the use of categoriesfor improving the EUTRANS translationsystems.
Experimental results with thetask defined in the project show that thisapproach reduces the number of exam-ples required for achieving ood models.1 I n t roduct ionThe EUTRANS project 1(Amengual et al, 1996a),funded by the European Union, aims at develop-ing Machine Translation systems for limited do-main applications.
These systems accept speechand text input, and are trained using an ex-ample based approach.
The translation modelused in this project is the Subsequential Trans-ducer (SST), which is easily integrable in con-ventional speech recognition systems by using itboth as language and translation model (Jimdnezet al, 1995).
In addition, SSTs can be automati-cally learned from sentence aligned bilingual cor-pora (Oncina et ai., 1993).This paper describes the use of categories bothin the training and translation processes for im-proving the EUTRANS translation systems.
The1Example-Based Understanding and TranslationSystems (EuTRANS).
Information Technology, LongTerm Research Domain, Open Scheme, Project Num-ber 20268.F.
Casacuber ta  2 A. Castaf io 1A.
Marzal 1 F. Prat  1J.
M. Vi lar  1(2) Depto.
de Sistemas Inform~ticos yComputacidnUniversidad Politdcnica de Valencia46071 Valencia (Spain)approach presented here improves that in (Vilaret al, 1995), the integration of categories withinthe systems i simpler, and it allows for categoriesgrouping units larger than a word.
Experimentalresults with the Traveler Task, defined in (Amen-gual et al, 1996b), show that this method reducesthe number of examples required for achievinggood models.The rest of the paper is structured as follows.In section 2 some basic concepts and the notationare introduced.
The technique used for integrat-ing categories inthe system is detailed in section 3.Section 4 presents the speech translation system.Both speech and text input experiments are de-scribed in section 5.
Finally, section 6 presentssome conclusions and new directions.2 Basic Concepts  -r id Notat ionGiven an alphabet X, X* is the free monoid ofstrings over X.
The symbol A represents theempty string, first letters (a, b, c, .
.
. )
rep-resent individual symbols of the alphabets andlast letters (z, y, x, .
.
. )
represent strings ofthe free monoids.
We refer to the individual el-ements of the strings by means of subindices, asin x = a l .
.
.an .
Given two strings x ,y  E X ' ,  xydenotes the concatenation f x and y.2.1 Subsequential  TransducersA Subsequential Transducer (Berstel, 1979) is adeterministic f nite state network that accepts en-tences from a given input language and producesassociated sentences of an output language.
ASST is composed of states and arcs.
Each arcconnects two states and it is associated to an in-put symbol and an output substring (that maybe empty).
Translation of an input sentence isobtained starting from the initial state, follow-ing the path corresponding to its symbols throughthe network, and concatenating the correspondingoutput substrings.44Formally, a SST is a tuple r = (X, Y, Q, q0,E, o-) where X and 1," are the input and outputalphabets, Q is a finite set of states, qo E Q isthe initial state, E E Q x X x Y" x ~ is a setof arcs satisfying the determinism condition, anda : Q ~ Y" is a state emission function 2.
Thosestates for which o" is defined are usually called finalstates.
The determinism condition means that, if(p, a. y, q) and (p, a, y', q') belong to E, then y = y'andq=q' .
Given as t r ingx  = a l .
.
.an  E X ' ,  asequence (qo~al,yl,ql) .
.
.
.
, (qn-l,a,~,yn,q,~) is avalid path if (qi-1, ai, Yi, qi) belongs to E for everyi in 1 , .
.
.
,  n, and qn is a final state.
In case thereexists such a valid path for z, the translation ofz by r is y l .
.
.
yna(q~).
Otherwise, the transla-tion is undefined.
Note that due to the conditionof determinism, there can be no more than onevalid path, and hence at most one translation, fora given input string.
Therefore, r defines a func-tion between an input language, Lt C_ X ?, andan output language, Lo C Y*.
Both Lt and Loare regular languages and their corresponding au-tomata re easily obtainable from the SST.
In par-ticular, an automaton for Lt can be obtained byeliminating the output of the arcs and states, andconsidering the final state set of the automatonbeing the same as in the SST.
A state is useless ifit is not contained in any valid path.
Useless tatescan be eliminated from a SST without changingthe function it defines.In section 3, we will relax the model.
Insteadof imposing the determinism conditition, we willonly enforce the existence of at most one validpath in the transducer for each input string (non-ambiguity).
We will call them Unambiguous SSTs(USSTs).
Standard algorithms for finding thepath corresponding to a string in an unambigousfinite state automaton (see for instance (Hopcroftand UNman, 1979)) can be used for finding thetranslation in a USST.
When the problem is thesearch for the best path in the expanded modelduring speech translation (see section 4), the useof the Viterbi algorithm (Forney, 1973) guaranteesthat the most likely path will be found.2.2 In ference  of  Subsequent ia lT ransducersThe use of SSTs to model limited domain trans-lation tasks has the distinctive advantage of al-lowing an automatic and efficient learning of thetranslation models from sets of examples.
An in-ference algorithm known as OSTIA (Onward Sub-21n this paper, the term function refers to partialfunctions.
We will use f(z) = @ to denote that thefunction .f is undefined for ~.sequential Transducer Inference Algorithm) allowsthe obtainment of a SST that correctly models thetranslation of a given task, if the training set isrepresentative (in a formal sense) of the task (On-cina et al, 1993).
Nevertheless, although theSSTs learned by OSTIA are usually good trans-lation models, they are often poor input languagemodels.
In practice, they very accurately trans-late correct input sentences, but also accept andtranslate incorrect sentences producing meaning-less results.
This yields undesirable effects in caseof noisy input, like the one obtained by OCR orspeech recognition.To overcome this problem, the algorithmOSTIA-DR (Oncina and Var6, 1996) uses finitestate domain (input language) and range (out-put language) models, which allow to learn SSTsthat only accept input sentences and only produceoutput sentences compatible with those languagemodels.
OSTIA-DR can make use of any kindof finite state model.
In particular, models canbe n-testable automata, which are equivalent ton-grams (Vidal et al, 1995) and can be also au-tomatically learned from examples.3 Introducing Word Categories inthe Learning and TranslationProcessesAn approach for using categories together withSSTs was presented in (Vilar et al, 1995), provingit to be useful in reducing the number of examplesrequired for learning.
However, the approach pre-sented there was not easily integrable in a speechrecognition system and did not provide for thecase in which the categories included units largerthan a word.For the EUTRANS project, the approach waschanged so that a single USST would compriseall the information for the translation, includingelementary transducers for the categories.
Thesesteps were followed:?
CATEGORY IDENTIFICATION.
The categoriesused in EUTRANS were seven: masculinenames, femenine names, surnames, dates,hours, room numbers, and general numbers.The election of these categories was donewhile keeping with the example based natureof the project.
In particular, the categorieschosen do not need very specific rules forrecognising them, the translation rules theyfollow are quite simple, and the amount ofspecial linguistic knowledge introduced wasvery low.?
CoRPus CATEGORIZATION.
Once the cate-45Original sample:D6me la Ilave de la habitaci6n ciento veintitr~.sGive me the key to room number one two threeEGORI ER ?Categorized sample:D~me la II-',ve de la habltact6n SROOMGive me the key to room number $ROOM( OS' ,A-DR )I( EXPANDER )LEARNING PROCESS\[npu?
sentence:D~me la llave de la habitaci6n quinientos setenta y ochoGive me the key m room SROOM $ROOM=\[five s ven eight( PosT ocv.ssoR )"rrmmlalt4m:Give me the key to room number five seven eightTRANSLATION PROCESSFigure 1: General schema of the treatment ofcategories in the learning and translation processes.gories were defined, simple scripts ubstitutedthe words in the categories by adequate la-bels, so that the pair (ddme la Have de lahabitaci6n ciento veintitrds - give me the keyto room one two three) became (dime Is Uavede la habitaci6n $ROOM - give me the keyto room SROOM), where $ROOM is the cat-egory label for room numbers.?
INITIAL MODEL LEARNING.
The  categorisedcorpus was used for training a model, the ini-tial SST.?
CATEGORY MODELLING.
For each  cate -gory ,  a simple SST was built: its categorySST  (cSST).?
CATEGORY EXPANSION.
The arcs in the ini-tial SST corresponding to the different cate-gories were expanded using their cSSTs.A general view of the process can be seen in Fig-ure 1.
The left part represents he elements in-volved in the learning of the expanded USST, ex-emplified with a single training pair.
The rightpart of the diagram gives a schematic representa-tion of the use of this transducer.The category expansion step is a bit more com-plex than just substituting each category-labeledarc by the corresponding cSST.
The main prob-lems are: (I) how to insert the output of thecSST within the output of the initial transducer;(2) how to deal with more than one final state inthe cSST; (3) how to deal with cycles in the cSSTinvolving its initial state.The problem with the output had certain sub-telities, since the translation of a category label46can appear before or after the label has been seenin the input.
For example, consider the transducerin Figure2(a) and a Spanish sentence categorisedas me voy a $HOUR, which corresponds to thecategorised English one I am leaving at $HOUR.Once me roy a is seen, the continuation can onlybe $HOUR, so the initial SST, before seeing thiscategory label in the input, has already producedthe whole output (including $HOUR).
Taking thisinto account, we decided to keep the output of theinitial SST and to include there the informationnecessary for removing the category labels.
To dothis, the label for the category was considered asa variable that acts as a placeholder in the outputsentence and whose contents are also fixed by anassignment appearing elsewhere within that sen-tence.
In our example, the expected output forme roy alas tres y media could be I am leavingat $HOUR $HOUR = \[half past three\].
This as-sumes that each category appears at most oncewithin each sentence.The expanded model is obtained by an itera-tive procedure which starts with the initial SST.Each time the procedure finds an arc whose in-put symbol is a category label, it expands this arcby the adequate cSST producing a new model.This expansion can introduce non-determinism, sothese new models are now USSTs.
When every arcof this kind has been expanded, we have the ex-panded USST.
The expansion of each arc followsthese steps:?
Eliminate the arc.?
Create a copy of the cSST corresponding tothe category label.?
Add new arcs linking the new cSST with theUSST.
These arcs have to ensure that theoutput produced in the cSST is embraced be-tween c=\[ and \], c being the category label.?
Eliminate useless tates.Formally, we have an USST 7" = (X ,Y ,Q ,qo, E,a) ,  a cSST r~ = (X ,Y ,  Qc, qoe, E~,ac),where we assume that ac(qoc = 0, and an arc(p, c, z, q) e ~ E. We will produce a new USSTr' = (x ,v ,  QuQ~,qo , (E -  (p ,e ,z ,q ) )u  E~,a').The new elements are:?
The set Q~ is disjoint with Q and there existsa bijection ?
: Qc ~ Q~.?
The new set of arcs is:E'~ = {(?(r),a,y,?
(s))lCr, a ~,s) e Ec)}u {(p,a, zc=\[y,?
(s)) l (qoc,a,y,s)  E Ee)}u {(?
(r),a, yac(s)\],q)l(r,a,y,s) ~ Ec)Aa?
(s) # 0}U {(p,a, zc=\[ya~(s)\],q)\[(qo?,a,y,s) E Ec)^so(s) ~ o}Note that this solves the problems derivingfrom the cSST having multiple final states orcycles involving the initial state.
The price topay is the introduction of non-determinism inthe model.?
The new state emission function is:{ a(s) i f sEQFinally, the useless tates that may appear duringthis construction are removed.A simple example of the effects of this procedurecan be seen on Figure 2.
The drawing (a) depictsthe initial SST, (b) is a cSST for the hours betweenone and three (in o'clock and half past forms), andthe expanded USST is in (c).4 Overview of the SpeechTr-an.~lation SystemA possible scheme for speech translation consistsin translating the output of a conventional Contin-uous Speech Recognition (CSR) front-end.
Thisimplies that some restrictions present in the trans-lation and the output language, which could en-hance the acoustic search, are not taken into ac-count.
In this sense, it is preferable to integratethe translation model within a conventional CSRsystem to carry out a simultaneous search for therecognised sentence and its corresponding trans-lation.
This integration can be done by using aSST as language and translation model, since ithas included in the learning process the restric-tions introduced by the translation and the outputlanguage.
Experimental results show that bet-ter performance is achieved (Jimdnez et al, 1994;Jim/mez et al, 1995).Thus, our system can be seen as the result of in-tegrating a series of finite state models at differentlevels:?
ACOUSTIC  LEVEL.
Individual phones are rep-resented by means of Hidden Markov Models(HMMs).?
LEXICAL LEVEL.
Individual words are repre-sented by means of finite state automata witharcs labeled by phones.47her I .~Ja~I =?
SHOUR ?
I ,u SHOU(a) Initial SaT.
(b) A cSST for the category SHOUR.hop /i~ay'una/one I(c) Expanded USST.Figure 2: An example of the expansion procedure.?
SYNTACTIC AND TRANSLATION LEVEL.
Thesyntactic onstrains and translation rules arerepresented by an USST.In our case, the integration means the substitutionof the arcs of the USST by the automata describ-ing the input language words, followed by the sub-stitution of the arcs in this expanded automata bythe corresponding HMMs.
In this way, a conven-tional Viterbi search (Fomey, 1973) for the mostlikely path in the resulting network, given the in-put acoustic observations, can be performed, andboth the recognised sentence and its translationare found by following the optimal path.5 Experiments5.1 The Traveler TaskThe Traveler Task (Amengual et al, 1996b) wasdefined within the EUTRANS project (Amengualet al, 1996a).
It is more realistic that the onein (Castellanos et al, 1994), but, unlike other cor-pora such as the Hansards (Brown et al, 1990), itis not unrestricted.The general framework established for the Trav-eler Task aims at covering usual sentences that canbe needed in typical scenarios by a traveler visitinga foreign country whose language he/she does notspeak.
This framework includes a great varietyof different ranslation scenarios, and thus resultsappropriate for progressive experimentation withincreasing level of complexity.
In a first phase,the scenario has been limited to some human-to-human communication situations in the receptionof a hotel:?
Asking for rooms, wake-up calls, keys, thebill, a taxi and moving the luggage.?
Asking about rooms (availability, features,price).?
Having a look at rooms, complaining aboutand changing them.?
Notifying a previous reservation.?
Signing the registration form.?
Asking and complaining about the bill.?
Notifying the departure.?
Other common expressions.The Traveler Task text corpora re sets of pairs,each pair consisting in a sentence in the input lan-guage and its corresponding translation in the out-put language.
They were automatically built byusing a set of Stochastic, Syntax-directed Trans-lation Schemata (Gonzalez and Thomason, 1978)with the help of a data generation tool, speciallydeveloped for the EUTRANS project.
This soft-ware allows the use of several syntactic extensions48Table 1: Some examples of sentence pairs from the Traveler Task.Spanish: Pot favor, ~quieren pedirnos un taxi para la habitacidn trescientos diez?English: " Will you ask for a taxi for room number three one oh for us, please?Spanish: DeseaHa reservar una habitaciSn tranquiIa con teldfono y teIevisidn hasta pasadomal~ana.German:  Ich mSchte in ruhiges Zimmer mit TeIefon und Fernseher his iibermorgen reservieren.Spanish: zMe pueden dar las llaves de la habitaciSn, por favor?I tal ian: Mi potreste dare le chiavi della stanza, per favore?Table 2: Main features of the Spanish to English, Spanish to German and Spanish to Italian textcorpora.Spanish to English Spanish to German Spanish to ItalianVocabulary size 689 514 691 566 687 585Average sentence length 9.5 9.8 8.9 8.2 12.7 11.8Test set perplexity 13.8 7.0 13.2 9.0 13.6 10.6to these schema specifications in order to expressoptional rules, permutation of phrases, concor-dance (of gender, number and case), etc.
The useof automatic corpora generation was convenientdue to time constrains of the first phase of theEUTRANS project, and cost-effectiveness.
More-over, the complexity of the task can be controlled.The languages considered were Spanish as inputand English, German and Italian as output, givinga total of three independent corpora of 500,000pairs each.
Some examples of sentence pairs areshown in Table I.
Some features of the corporacan be seen in Table 2.
For each language, the testset perplexity has been computed by training atrigram model (with simple fiat smoothing) usinga set of 20,000 random sentences and computingthe probabilities yielded by this model for a set ofi0,000 independent random sentences.
The lowerperplexity of the output languages derives froma design decision: multiple variants of the inputsentences were introduced to account for differentways of expressing the same idea, but they weregiven the same translation.Finally, a multispeaker speech corpus for thetask was acquired.
It consists of 2,000 utterancesin Spanish.
Details can be found in (Amengual etal., 1997a).5.2 Text Input ExperimentsOur approach was tested with the three text cor-pora.
Each one was divided in training and testsets, with 490,000 and 10,000 pairs, respectively.A sequence of models was trained with increasingsubsets of the training set.
Each model was testedusing only those sentences in the test set that werenot seen in training.
This has been done becausea model trained with OST IA -DR is guaranteedto reproduce exactly those sentences it has seenduring learning.
The performance was evaluatedin terms of Word Error Rate (WER), which isthe percentage of output words that has to be in-serted, deleted and substituted for they to exactlymatch the corresponding expected translations.The results for the three corpora can be seenon Table 3.
The columns labeled as "Different"and "Categ.
", refer to the number of different sen-tences in the training set and the number of differ-ent sentences after categorization.
Graphical rep-resentations of the same results are on Figures 3,4 and 5.
As expected, the use of lexical categorieshad a major impact on the learning algorithm.The differences in WER attributable to the use oflexical categories can be as high as about a 40%in the early stages of the learning process and de-crease when the number of examples grows.
Thelarge increase in performance is a natural conse-quence of the fact that the categories help in re-ducing the total variability that can be found inthe corpora (although sentences do exhibit a greatdeal of variability, the underlying syntactic struc-ture is actually much less diverse).
They also havethe advantage of allowing an easier extension inthe vocabulary of the task without having a neg-ative effect on the performance of the models soobtained (Vilar et al, 1995).5.3 Speech Input ExperimentsA set of Spanish to English speaker independenttranslation experiments were performed integrat-ing in our speech input system (as described in49Table 3: Text input results: Translation word error rates (WER) and sizes of the transducers for differentnumber of training pairs.Training pairs Without categories With categoriesGenerated Different Categ.
WER States Arcs WER States ArcsI0,000 6,791 5,964 60.72 3,210 I0,427 30.51 4,50020,000 12,218 9,981 54.86 4,119 15,243 22.46 4,70040,000 21,664 16,207 47.92 5,254 22,001 13.70 4,55180,000 38,438 25,665 38.39 6,494 31,017 7.74 4,256160,000 67,492 39,747 26.00 6,516 36,293 3.71 4,053320,000 119,048 60,401 " 17.38 6,249 41,675 1.42 4,009490,000 168,629 77,499 13.33 5,993 47,151 0.74 3,85432.59935.5853487937.673340453364329394(a) Spanish to English corpus.Training pairs Without categories With categoriesGenerated Different Categ.
WER States Arcs WER States Arcs10,000 6,679 5,746 66.17 3,642 I1,410 35.21 5,256 76,58220,000 11,897 9,535 58.45 4,892 16,956 23.41 8,305 148,88140,000 21,094 15,425 53.87 6,486 25,358 16.06 11,948 245,29380,000 37,452 24,580 48.74 8,611 37,938 9.85 12,530 255,294160,000 66,071 38,656 42.06 11,223 56,432 5.17 11,724 227,667320,000 115,853 59,510 33.93 14,772 82,434 2.55 9,919 174,208490,000 163,505 77,053 29.86 16,914 101,338 1.23 10,055 178,312(b) Spanish to German corpus.Training pairs Without categories With categoriesGenerated Different Categ.
WER States Arcs WER States ArcsI0,000 6,698 5,795 58.29 2,857 9,650 29.8620,000 12,165 9,716 52.96 3,774 14,176 22.2940,000 21,670 15,741 47.39 4,629 19,864 14.3080,000 38,408 25,119 36.40 5,403 26,989 7.66160,000 67,355 39,281 26.98 5,598 32,588 4.68320,000 118,257 60,286 20.72 5,827 40,754 3.06490,000_ 166,897 77,877 17.60 6,399 49,430 2.543 0943 5814 1514 5995 1096 1437 46730,01038,37052,48261,57576,007100,099123,900(c) Spanish to Italian corpus.5070%60%50%40?7o30"20%lO%o%I0,0006,7915,964I I I I IWithout categories 0ties -.4--- -| .
.
.
.
.
.
.
-t-- .
.
.
.
I I I20,000 40,000 80,000 160,000 320,000 490,00012,218 21,664 38 438 67,492 119,048 L68,6299,981 16,207 251665 39,747 60,401 77,499Figure 3: Evolution of translation WER with the size of the training set: Spanish to English text corpus.The sizes in the horizontal axis refer to the first three columns in Table 3(a).Table 4: Speech input results:Translation word er-ror rates (WER)  and real time factor (RTF) forthe best Spanish to English transducer.Number of HMM BeamGaussians Width WER RTF1,663 300 2.3 % 5.91,663 150 6.4 % 2.25,590 300 1.9 % 11.35,590 150 6.3 % 5.6section 4) the following models:?
ACOUSTIC LEVEL.
The phones were rep-resented by context-independent continuous-density HMMs.
Each HMM consisted of sixstates following a left-to-right topology withloops and skips.
The emission distribution ofeach state was modeled by a mixture of Gaus-sians.
Actually, there were only three emi~.sion distributions per HMM since the stateswere tied in pairs (the first with the second,the third with the fourth, and the fifth withthe sixth).
Details about the corpus used intraining these models and its parametrizationcan be found in (Amengnal et al, 1997a).?
LEXICAL LEVEL Spanish Phonetics allowsthe representation f each word as a sequenceof phones that can be derived from standardrules.
This sequence can be represented by asimple chain.
There were a total of 31 phones,including stressed and unstressed vowels plustwo types of silence.SYNTACTIC AND TRANLATION LEVEL.
Weused the best of the transducers obtained inthe Spanish to English text experiments.
Itwas enriched with probabilities estimated byparsing the same training data with the finalmodel and using relative frequencies of use asprobability estimates.The Viterbi search for the most likely path wasspeeded up by using beam search at two levels:independent beam widths were used in the statesof the SST (empirically fixed to 300) and in thestates of the HMMs.
Other details of the experi-ments can be found in (Amengnal et al, 1997a).Table 4 shows that good translation results (aWER of 6.4%) can be achieved with a Real TimeFactor (RTF) of just 2.2.
It is worth noting thatthese results were obtained in a HP-9735 worksta-tion without resorting to any type of specialisedhardware or signal processing device.
When trans-lation accuracy is the main concern, a more de-tailed acoustic model and a wider beam in thesearch can be used to achieve a WER of 1.9%,but with a RTF  of 11.3.6 Conc lus ionsIn the EUTRANS project, Subsequential Tran-sucers are used as the basis of translation systemsthat accept speech and text input.
They can be517O%..6O%50% --4O% --3O%2O% --10% --O%10,0006,6795,746II20,00011,8979,535I I IWithout categories 0With categories -F -  -i. i l .
.
.
.
.40,000 80~0 160,0~ 320,000 49021,094 37'452 66,071 116,853 16315,425 241580 38,656 59,510 77000505053Figure 4: Evolution of translation WER with the size of the training set: Spanish to German textcorpus.
The sizes in the horizontal axis refer to the first three columns in Table 3(b).automatically earned from corpora of examples.This learning process can be improved by meansof categories using the approach detailed in thispaper.Experimental results show that this approachreduces the number of examples required forachieving ood models, with good translation re-sults in acceptable times without using speciaiisedhardware.Our current work concentrates in further reduc-ing the number of examples necessary for trainingthe translation models in order to cope with spon-taneous instead of synthetic sentences.
For this,new approaches are being explored, like reorderingthe words in the translations, the use of new in-ference algorithms, and automatic ategorization.Results obtained with a different enhancementof our text input system, the inclusion of errorcorrecting techniques, can be found in (Amengualet al, 1997b).ReferencesJuan-Carlos Amengual, JosA-Miguel Benedi,Francisco Casacuberta, Asuncidn Castafio, An-tonio Castellanos, Victor M. Jimdnez, DavidLlorens, Andrds MarzM, Federico Prat, HdctorRulot, Enrique Vidal, Juan MiKuel Vilar,Cristina Delogu, Andrea Di Carlo, HermannNey, Stephan Vogel, Josd Manuel Espejo, andJosep Ramdn Freixenet.
1996a.
EUTRANS:Example-based understanding and translationsystems: First-phase project overview.
Techni-cal Report D4, Part 1, EUTRANS (IT-LTR-OS-20268).
(Restricted).Juan-Carlos Amenguai, Jos~-Miguel Benedi,Asuncidn Casta~o, Andrds Marzai, FedericoPrat, Enrique Vidai, Juan Miguel Vilar,Cristina Delogu, Andrea Di Carlo, HermannNey, and Stephan Vogel.
1996b..
Definition ofa machine translation task and generation ofcorpora.
TechnicaJ Report DI, EUTRANS (IT-LTR-OS-20268).
(Restricted).J.
C. AmenguM, J. M. Benedi, K. Beulen,F.
Casacuberta, A. Castafio, A. Castella-nos, V. M. Jimdnez, D. Llorens, A. Marzal,H.
Ney, F. Prat, E. Vidal, and J. M. Vilar.1997a.
Speech translation based on automat-ically trainable finite-state models.
To appearin Proceedings of EUROSPEECH'97 .Juan C. Amengual, Josd M. Benedi, FranciscoCasacuberta, Asuncidn Castafio, Antonio Cas-tellanos, David Llorens, Andrds Marzal, Fed-erico Prat, Enrique Vidal, and Juan M. Vilar.1997b.
Error correcting parsing for text-to-textmachine translation using finite state models.To appear in Proceedings of TMI 97, July.J.
Berstel.
1979.
Transductions and Contezt-FreeLanguages.
Teubner.Peter F. Brown, John Cocke, Stephen A.Della Pietra, Vincent J. Della Pietra, Freder-ick Jelinek, John D. Lafferty, Robert L. Mercer,52.6O%50%40%30%20%10%0%IO,OOO6 6985:795L,.....,.....,...,..........,.....I I I I IWithout categories OI I I20,000 40,000 80,000 160,000 320,000 490,00012 165 21,670 38 408 67 355 118 257 166,8979:716 15,741 25:119 39:281 60:286 77,877Figure 5: Evolution of translation WER with the size of the training set: Spanish to Italian text corpus.The sizes in the horizontal axis refer to the first three columns in Table 3(c).and Paul S. Roossin.
1990.
A statistical ap-proach to machine translation.
ComputationalLinguistics, 16(2):79-85, June.A.
Casteltanos, I. Galiano, and E. Vidal.
1994.Application of OSTIA to machine translationtasks.
In Rafael C. Carrasco and Joss On-cina, editors, Grammatical Inference and Appli-cations, volume 862 of Lecture Notes in Com-puter Science, pages 93-105.
Springer-Verlag,September.G.
D. Forney, Jr. 1973.
The Viterbi algorithm.Proceedings of the IEEE, 61(3):268-278, March.R.
C. Gonzalez and M. G. Thomason.
1978.
Syn-tactic Pattern Recognition: An Introduction.Addison-Wesley, Reading, Massachusetts.J.
E. Hopcroft and J. D. UNman.
1979.
Introduc-tion to Automata Theory, Languages and Com-putation.
Addison-Wesley, Reading, Mass.,USA.V.M.
Jim~nez, E. Vidal, J. Oncina, A. Cas-tellanos, ,H. Rulot, and J.
A. S~inchez.1994.
Spoken-language machine translationin limited-domain tasks.
In H. Niemann,R.
de Mori, and G. Hanrieder, editors, Pro-ceedings of the CRIM/FORWISS Workshop onProgress and Prospects of Speech Research andTechnology, pages 262-265.
Infix.V.
M. Jim~nez, A. Castellanos, and E. Vidal.1995.
Some results with a trainable speechtranslation and understanding system.
In Pro-ceedings of the ICASSP-95, Detroit, MI (USA).Jos60ncina and Miguel Angel Var6.
1996.
Us-ing domain information during the learning ofa subsequential transducer.
In Laurent Micletand Colin de la Higuera, editors, Grammati-cal Inference: Learning Syntax from Sentences,volume 1147 of Lecture Notes in Computer Sci-ence, pages 301-312.
Springer-Verlag.Jose Oncina, Pedro Garcia, and Enrique Vidal.1993.
Learning subsequential transducers forpattern recognition interpretation tasks.
IEEETransactions on Pattern Analysis and MachineIntelligence, 15(5):448-458, may.Enrique Vidal, Francisco Casacuberta, and Pe-dro Garcia.
1995.
Grammatical inference andautomatic speech recognition.
In A. J. Ru-bio and J. M. L6pez, editors, Speech Recogni-tion and Coding, New Advances and Trends,NATO Advanced Study Institute, pages 174-191.
Springer-Verlag, Berlin.Juan Miguel Vilar, AndrOs Marzal, and EnriqueVidal.
1995.
Learning language translation inlimited domains using finite-state models: someextensions and improvements.
In 4th Euro-pean Conference on Speech Communication andTechnology, Madrid (Spain), September.
ESCA.53
