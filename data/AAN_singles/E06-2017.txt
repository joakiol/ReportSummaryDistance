Computing Term Translation Probabilities with Generalized LatentSemantic AnalysisIrina MatveevaDepartment of Computer ScienceUniversity of ChicagoChicago, IL 60637matveeva@cs.uchicago.eduGina-Anne LevowDepartment of Computer ScienceUniversity of ChicagoChicago, IL 60637levow@cs.uchicago.eduAbstractTerm translation probabilities proved aneffective method of semantic smoothing inthe language modelling approach to infor-mation retrieval tasks.
In this paper, weuse Generalized Latent Semantic Analysisto compute semantically motivated termand document vectors.
The normalizedcosine similarity between the term vec-tors is used as term translation probabil-ity in the language modelling framework.Our experiments demonstrate that GLSA-based term translation probabilities cap-ture semantic relations between terms andimprove performance on document classi-fication.1 IntroductionMany recent applications such as document sum-marization, passage retrieval and question answer-ing require a detailed analysis of semantic rela-tions between terms since often there is no largecontext that could disambiguate words?s meaning.Many approaches model the semantic similaritybetween documents using the relations betweensemantic classes of words, such as representingdimensions of the document vectors with distri-butional term clusters (Bekkerman et al, 2003)and expanding the document and query vectorswith synonyms and related terms as discussedin (Levow et al, 2005).
They improve the per-formance on average, but also introduce some in-stability and thus increased variance (Levow et al,2005).The language modelling approach (Ponte andCroft, 1998; Berger and Lafferty, 1999) provedvery effective for the information retrieval task.Berger et.
al (Berger and Lafferty, 1999) usedtranslation probabilities between terms to accountfor synonymy and polysemy.
However, theirmodel of such probabilities was computationallydemanding.Latent Semantic Analysis (LSA) (Deerwester etal., 1990) is one of the best known dimensionalityreduction algorithms.
Using a bag-of-words docu-ment vectors (Salton and McGill, 1983), it com-putes a dual representation for terms and docu-ments in a lower dimensional space.
The resultingdocument vectors reside in the space of latent se-mantic concepts which can be expressed using dif-ferent words.
The statistical analysis of the seman-tic relatedness between terms is performed implic-itly, in the course of a matrix decomposition.In this project, we propose to use a combi-nation of dimensionality reduction and languagemodelling to compute the similarity between doc-uments.
We compute term vectors using the Gen-eralized Latent Semantic Analysis (Matveeva etal., 2005).
This method uses co-occurrence basedmeasures of semantic similarity between termsto compute low dimensional term vectors in thespace of latent semantic concepts.
The normalizedcosine similarity between the term vectors is usedas term translation probability.2 Term Translation Probabilities inLanguage ModellingThe language modelling approach (Ponte andCroft, 1998) proved very effective for the infor-mation retrieval task.
This method assumes thatevery document defines a multinomial probabil-ity distribution p(w|d) over the vocabulary space.Thus, given a query q = (q1, ..., qm), the like-lihood of the query is estimated using the docu-ment?s distribution: p(q|d) = ?m1 p(qi|d), where151qi are query terms.
Relevant documents maximizep(d|q) ?
p(q|d)p(d).Many relevant documents may not contain thesame terms as the query.
However, they maycontain terms that are semantically related to thequery terms and thus have high probability ofbeing ?translations?, i.e.
re-formulations for thequery words.Berger et.
al (Berger and Lafferty, 1999) in-troduced translation probabilities between wordsinto the document-to-query model as a way of se-mantic smoothing of the conditional word proba-bilities.
Thus, they query-document similarity iscomputed asp(q|d) =m?i?w?dt(qi|w)p(w|d).
(1)Each document word w is a translation of a queryterm qi with probability t(qi|w).
This approachshowed improvements over the baseline languagemodelling approach (Berger and Lafferty, 1999).The estimation of the translation probabilities is,however, a difficult task.
Lafferty and Zhai useda Markov chain on words and documents to es-timate the translation probabilities (Lafferty andZhai, 2001).
We use the Generalized Latent Se-mantic Analysis to compute the translation proba-bilities.2.1 Document SimilarityWe propose to use low dimensional term vectorsfor inducing the translation probabilities betweenterms.
We postpone the discussion of how the termvectors are computed to section 2.2.
To evaluatethe validity of this approach, we applied it to doc-ument classification.We used two methods of computing the sim-ilarity between documents.
First, we computedthe language modelling score using term transla-tion probabilities.
Once the term vectors are com-puted, the document vectors are generated as lin-ear combinations of term vectors.
Therefore, wealso used the cosine similarity between the docu-ments to perform classificaiton.We computed the language modelling score ofa test document d relative to a training documentdi asp(d|di) =?v?d?w?dit(v|w)p(w|di).
(2)Appropriately normalized values of the cosinesimilarity measure between pairs of term vectorscos(~v, ~w) are used as the translation probabilitybetween the corresponding terms t(v|w).In addition, we used the cosine similarity be-tween the document vectors?~di, ~dj?
=?w?di?v?dj?diw ?djv ?~w,~v?, (3)where ?diw and ?djv represent the weight of theterms w and v with respect to the documents diand dj , respectively.In this case, the inner products between the termvectors are also used to compute the similarity be-tween the document vectors.
Therefore, the cosinesimilarity between the document vectors also de-pends on the relatedness between pairs of terms.We compare these two document similarityscores to the cosine similarity between bag-of-word document vectors.
Our experiments showthat these two methods offer an advantage for doc-ument classification.2.2 Generalized Latent Semantic AnalysisWe use the Generalized Latent Semantic Analy-sis (GLSA) (Matveeva et al, 2005) to compute se-mantically motivated term vectors.The GLSA algorithm computes the term vectorsfor the vocabulary of the document collection Cwith vocabulary V using a large corpus W .
It hasthe following outline:1.
Construct the weighted term document ma-trix D based on C2.
For the vocabulary words in V , obtain a ma-trix of pair-wise similarities, S, using thelarge corpus W3.
Obtain the matrix UT of low dimensionalvector space representation of terms that pre-serves the similarities in S, UT ?
Rk?|V |4.
Compute document vectors by taking linearcombinations of term vectors D?
= UTDThe columns of D?
are documents in the k-dimensional space.In step 2 we used point-wise mutual informa-tion (PMI) as the co-occurrence based measure ofsemantic associations between pairs of the vocab-ulary terms.
PMI has been successfully applied tosemantic proximity tests for words (Turney, 2001;Terra and Clarke, 2003) and was also success-fully used as a measure of term similarity to com-pute document clusters (Pantel and Lin, 2002).
In152our preliminary experiments, the GLSA with PMIshowed a better performance than with other co-occurrence based measures such as the likelihoodratio, and ?2 test.PMI between random variables representingtwo words, w1 and w2, is computed asPMI(w1, w2) = logP (W1 = 1,W2 = 1)P (W1 = 1)P (W2 = 1).
(4)We used the singular value decomposition(SVD) in step 3 to compute GLSA term vectors.LSA (Deerwester et al, 1990) and some otherrelated dimensionality reduction techniques, e.g.Locality Preserving Projections (He and Niyogi,2003) compute a dual document-term representa-tion.
The main advantage of GLSA is that it fo-cuses on term vectors which allows for a greaterflexibility in the choice of the similarity matrix.3 ExperimentsThe goal of the experiments was to understandwhether the GLSA term vectors can be used tomodel the term translation probabilities.
We useda simple k-NN classifier and a basic baseline toevalute the performance.
We used the GLSA-based term translation probabilities within the lan-guage modelling framework and GLSA documentvectors.We used the 20 news groups data set becauseprevious studies showed that the classification per-formance on this document collection can notice-ably benefit from additional semantic informa-tion (Bekkerman et al, 2003).
For the GLSAcomputations we used the terms that occurred inat least 15 documents, and had a vocabulary of9732 terms.
We removed documents with fewerthan 5 words.
Here we used 2 sets of 6 newsgroups.
Groupd contained documents from dis-similar news groups1, with a total of 5300 docu-ments.
Groups contained documents from moresimilar news groups2 and had 4578 documents.3.1 GLSA ComputationTo collect the co-occurrence statistics for the sim-ilarities matrix S we used the English Gigawordcollection (LDC).
We used 1,119,364 New YorkTimes articles labeled ?story?
with 771,451 terms.1os.ms, sports.baseball, rec.autos, sci.space, misc.forsale,religion-christian2politics.misc, politics.mideast, politics.guns, reli-gion.misc, religion.christian, atheismGroupd Groups#L tf Glsa LM tf Glsa LM100 0.58 0.75 0.69 0.42 0.48 0.48200 0.65 0.78 0.74 0.47 0.52 0.51400 0.69 0.79 0.76 0.51 0.56 0.551000 0.75 0.81 0.80 0.58 0.60 0.592000 0.78 0.83 0.83 0.63 0.64 0.63Table 1: k-NN classification accuracy for 20NG.Figure 1: k-NN with 400 training documents.We used the Lemur toolkit3 to tokenize and in-dex the document; we used stemming and a list ofstop words.
Unless stated otherwise, for the GLSAmethods we report the best performance over dif-ferent numbers of embedding dimensions.The co-occurrence counts can be obtained usingeither term co-occurrence within the same docu-ment or within a sliding window of certain fixedsize.
In our experiments we used the window-based approach which was shown to give betterresults (Terra and Clarke, 2003).
We used the win-dow of size 4.3.2 Classification ExperimentsWe ran the k-NN classifier with k=5 on ten ran-dom splits of training and test sets, with differentnumbers of training documents.
The baseline wasto use the cosine similarity between the bag-of-words document vectors weighted with term fre-quency.
Other weighting schemes such as max-imum likelihood and Laplace smoothing did notimprove results.Table 1 shows the results.
We computed thescore between the training and test documents us-ing two approaches: cosine similarity between theGLSA document vectors according to Equation 3(denoted as GLSA), and the language modellingscore which included the translation probabilitiesbetween the terms as in Equation 2 (denoted as3http://www.lemurproject.org/153LM ).
We used the term frequency as an estimatefor p(w|d).
To compute the matrix of translationprobabilities P , where P [i][j] = t(tj|ti) for theLMCLSA approach, we first obtained the matrixP?
[i][j] = cos(~ti, ~tj).
We set the negative and zeroentries in P?
to a small positive value.
Finally, wenormalized the rows of P?
to sum up to one.Table 1 shows that for both settings GLSA andLM outperform the tf document vectors.
As ex-pected, the classification task was more difficultfor the similar news groups.
However, in thiscase both GLSA-based approaches outperform thebaseline.
In both cases, the advantage is moresignificant with smaller sizes of the training set.GLSA and LM performance usually peaked ataround 300-500 dimensions which is in line withresults for other SVD-based approaches (Deer-wester et al, 1990).
When the highest accuracywas achieved at higher dimensions, the increaseafter 500 dimensions was rather small, as illus-trated in Figure 1.These results illustrate that the pair-wise simi-larities between the GLSA term vectors add im-portant semantic information which helps to gobeyond term matching and deal with synonymyand polysemy.4 Conclusion and Future WorkWe used the GLSA to compute term translationprobabilities as a measure of semantic similaritybetween documents.
We showed that the GLSAterm-based document representation and GLSA-based term translation probabilities improve per-formance on document classification.The GLSA term vectors were computed for allvocabulary terms.
However, different measures ofsimilarity may be required for different groups ofterms such as content bearing general vocabularywords and proper names as well as other namedentities.
Furthermore, different measures of sim-ilarity work best for nouns and verbs.
To extendthis approach, we will use a combination of sim-ilarity measures between terms to model the doc-ument similarity.
We will divide the vocabularyinto general vocabulary terms and named entitiesand compute a separate similarity score for eachof the group of terms.
The overall similarity scoreis a function of these two scores.
In addition, wewill use the GLSA-based score together with syn-tactic similarity to compute the similarity betweenthe general vocabulary terms.ReferencesRon Bekkerman, Ran El-Yaniv, and Naftali Tishby.2003.
Distributional word clusters vs. words for textcategorization.Adam Berger and John Lafferty.
1999.
Information re-trieval as statistical translation.
In Proc.
of the 22rdACM SIGIR.Scott C. Deerwester, Susan T. Dumais, ThomasK.
Lan-dauer, GeorgeW.
Furnas, and Richard A. Harshman.1990.
Indexing by latent semantic analysis.
Jour-nal of the American Society of Information Science,41(6):391?407.Xiaofei He and Partha Niyogi.
2003.
Locality preserv-ing projections.
In Proc.
of NIPS.John Lafferty and Chengxiang Zhai.
2001.
Documentlanguage models, query models, and risk minimiza-tion for information retrieval.
In Proc.
of the 24thACM SIGIR, pages 111?119, New York, NY, USA.ACM Press.Gina-Anne Levow, Douglas W. Oard, and PhilipResnik.
2005.
Dictionary-based techniques forcross-language information retrieval.
InformationProcessing and Management: Special Issue onCross-language Information Retrieval.Irina Matveeva, Gina-Anne Levow, Ayman Farahat,and Christian Royer.
2005.
Generalized latent se-mantic analysis for term representation.
In Proc.
ofRANLP.Patrick Pantel and Dekang Lin.
2002.
Document clus-tering with committees.
In Proc.
of the 25th ACMSIGIR, pages 199?206.
ACM Press.Jay M. Ponte and W. Bruce Croft.
1998.
A languagemodeling approach to information retrieval.
In Proc.of the 21st ACM SIGIR, pages 275?281, New York,NY, USA.
ACM Press.Gerard Salton and Michael J. McGill.
1983.
Intro-duction to Modern Information Retrieval.
McGraw-Hill.Egidio L. Terra and Charles L. A. Clarke.
2003.
Fre-quency estimates for statistical word similarity mea-sures.
In Proc.of HLT-NAACL.Peter D. Turney.
2001.
Mining the web for synonyms:PMI?IR versus LSA on TOEFL.
Lecture Notes inComputer Science, 2167:491?502.154
