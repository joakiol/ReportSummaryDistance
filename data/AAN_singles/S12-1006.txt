First Joint Conference on Lexical and Computational Semantics (*SEM), pages 44?48,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsThe Use of Granularity in Rhetorical Relation PredictionBlake Stephen Howald and Martha AbramsonUltralingua, Inc.1313 SE Fifth Street, Suite 108Minneapolis, MN 55414{howald, abramson}@ultralingua.comAbstractWe present the results of several machinelearning tasks designed to predict rhetori-cal relations that hold between clauses indiscourse.
We demonstrate that organizingrhetorical relations into different granularitycategories (based on relative degree of detail)increases average prediction accuracy from58% to 70%.
Accuracy further increases to80% with the inclusion of clause types.
Theseresults, which are competitive with existingsystems, hold across several modes of writtendiscourse and suggest that features of informa-tion structure are an important considerationin the machine learnability of discourse.1 IntroductionThe rhetorical relations that hold between clausesin discourse index temporal and event informationand contribute to a discourse?s pragmatic coherence(Hobbs, 1985).
For example, in (1) the NARRATIONrelation holds between (1a) and (1b) as (1b) tempo-rally follows (1a) at event time.
(1) a. Pascale closed the toy chest.b.
She walked to the gate.c.
The gate was locked securely.d.
So she couldn?t get into the kitchen.The ELABORATION relation, describing the sur-rounding state of affairs, holds between (1b) and(1c).
(1c) is temporally inclusive (subordinated)with (1b) and there is no temporal progression atevent time.
The RESULT relation holds between (1b-c) and (1d).
(1d) follows (1b) and its subordinatedELABORATION relation (1c) at event time.Additional pragmatic information is encoded inthese relations in terms of granularity.
Granularityrefers to the relative increases or decreases in thelevel of described detail.
For example, moving from(1b) to (1c), we learn more information about thegate via the ELABORATION relation.
Also, movingfrom (1b-c) to (1d) there is a consolidation of infor-mation associated with the RESULT relation.Through several supervised machine learningtasks, we investigate the degree to which granularity(as well as additional elements of discourse struc-ture (e.g.
tense, aspect, event)) serves as a viableorganization and predictor of rhetorical relations ina range of written discourses.
This paper is orga-nized as follows.
Section 2 reviews prior researchon rhetorical relations, discourse structure, granular-ity and prediction.
Section 3 discusses the analyzeddata, the selection and annotation of features, andthe construction of several machine learning tasks.Section 4 provides the results which are then dis-cussed in Section 5.2 BackgroundRhetorical relation prediction has received consid-erable attention and has been shown to be usefulfor text summarization (Marcu, 1998).
Predictiontasks rely on a number of features (discourse con-nectives, part of speech, etc.)
(Marcu and Echihabi,2002; Lapata and Lascarides, 2004).
A wide rangeof accuracies are also reported - 33.96% (Marcu andEchihabi, 2002) to 70.70% (Lapata and Lascarides,2004) for all rhetorical relations and, for individ-ual relations, CONTRAST (43.64%) and CONTINU-ATION (83.35%) (Sporleder and Lascarides, 2005).44We seek to predict the inventory of rhetoricalrelations defined in Segmented Discourse Repre-sentation Theory (?SDRT?)
(Asher and Lascarides,2003).
In addition to the relations illustrated in(1), we consider: BACKGROUND: It was Christ-mas.
Pascale got a new toy.
; EXPLANATION: Theaardvark was dirty.
It fell into a puddle.
; CONSE-QUENCE: If the aardvark fell in the puddle, then itgot dirty.
; ALTERNATION: Pascale got an aardvarkor a stuffed bunny.
; and CONTINUATION: Pascalegot an aardvark.
Grimsby got a rawhide.Discourses were selected based on Smith (2003)who defines five primary discourse modes by: (1)the situations (events and states) they describe; (2)the overarching temporality (tense, aspect); and (3)the type of text progression (temporal - text andevent time progression are similar; atemporal - textand event time progression are not similar).
Thesecontrastive elements inform the features selectedfor the machine learning tasks discussed in Section3.2.
The five modes, narratives, reports (news ar-ticles), description (recipes), information (scientificessays), and argument (editorials) were selected toensure a balanced range of theoretically supporteddiscourse types.2.1 Granularity of InformationGranularity in discourse refers to the relative degreeof detail.
The higher the level of detail, the moreinformative the discourse is.
We assume that therewill be some pragmatic constraints on the informa-tiveness of a discourse (e.g., consistent with Grice?s(1975) Maxim of Quantity).
For our purposes, werely specifically on granularity as defined in Mulkar-Mehta et al (2011) (?MM?)
who characterize gran-ularity in terms of entities and events.To illustrate, consider (2) where the rhetoricalstructure indicates that (2b) is an ELABORATION of(2a), the NARRATION relation holds between (2b)and (2c) and (2c) and (2d), and the RESULT relationbetween (2d) and (2e).
(2) a.
The Pittsburgh Steelers needed to win.b.
Batch took the first snap.c.
Then he threw the ball into the endzone.d.
Ward caught the ball.e.
A touchdown was scored.Entities and events can stand in part-whole andcausality relationships with entities and events insubsequent clauses.
A positive granularity shift in-dicates movement from whole to part (more detail)- e.g., Batch (2b) is a part of the whole PittsburghSteelers (2a).
A negative granularity shift indicatesmovement from part to whole (less detail), or ifone event causes a subsequent event (if an event iscaused by a subsequent event, this is a positive shift)- e.g., Ward?s catching of the ball (2d) caused thescoring of the touchdown (2e).
Maintained granular-ities (not considered by MM) are illustrated in (2b-c)and (2c-d).
Clauses (2b) through (2d) are temporallylinked events, but there is no part-whole shift in, nora causal relationship between, the entities or events;the granularity remains the same.We maintain that there is a close relationship be-tween rhetorical relations and granularity.
Con-sequently, rhetorical relations can be organized asfollows: positive: BACKGROUND, ELABORATION,EXPLANATION; negative: CONSEQUENCE, RE-SULT; and maintained: ALTERNATION, CONTINU-ATION, NARRATION.
The machine learning tasksdiscussed in the remainder of the paper consider thisinformation in the prediction of rhetorical relations.3 Data and MethodsFive written discourses of similar sentence lengthwere selected from each mode for 25 total dis-courses.
The discourses were segmented by inde-pendent or dependent (subordinate) clauses, if theclauses contained discourse markers (but, however),and if the clauses were embedded in the sentenceprovided in the orginal written discourse (e.g., John,who is the director of NASA, gave a speech on Fri-day).
The total number of clauses is 1090, averaging43.6 clauses per discourse (?=7.2).3.1 Feature AnnotationFor prediction, we use a feature set distilled fromSmith?s classification of discourses: TENSE andASPECT; EVENT (from the TimeML annotationscheme (Pustejovksy, et al, 2005), Aspectual, Oc-curence, States, etc.
); SEQUENCE information asthe clause position normalized to the unit interval;and discourse MODE.
We also include CLAUSEtype - independent (IC) or dependent clauses (DC)with the inclusion of a discourse marker (M) or not,45Table 1: Distribution of Relations by Granularity Type.Relation Number (Avg.
)Positive 515 (47%)BACKGROUND 315 (61%)ELABORATION 161 (31%)EXPLANATION 39 (7%)Negative 59 (5%)CONSEQUENCE 16 (26%)RESULT 43 (71%)Maintenance 490 (44%)ALTERNATION 76 (14%)CONTINUATION 30 (6%)NARRATION 384 (78%)embedded (EM) or not - and GRANULARITY shiftcategories which are an organization of the SDRTrhetorical relations (Asher and Lascarides, 2003),summarized in Table 1.All 25 discourses were annotated by one of the au-thors using only a reference sheet.
The other authorindependently coded 80% of the data (20 discourses,four from each mode).
Average agreement and Co-hen?s Kappa (Cohen, 1960) statistics were computedand are within acceptable ranges: TENSE (99.65/ .9945), ASPECT (99.30 / .9937), SDRT (77.42 /.6850), and EVENT (75.88 / .6362).These results are consistent with previously re-ported annotations for rhetorical relations (Sporlederand Lascarides, 2005; Howald and Katz, 2011),event verbs and durations, tense and aspect (Puscasuand Mititelu, 2008; Wiebe et al, 1997).
Positive,negative and maintained granularities were not an-notated, but MM report a Kappa between .8500 and1.
The distribution of these granularities, based onthe organization of the annotated rhetorical relationsis presented in Table 1.3.2 Machine LearningThree supervised machine learning tasks were con-structed to predict SDRT relations.
The first task(Uncollapsed) created a 8-way classifier to predictthe SDRT relations based on the feature set, omit-ting the GRANULARITY feature.
The second task(Collapsed) created a 3-way classifier to predictthe GRANULARITY categories (the SDRT featurewas omitted).
The third task (Combined) includedTable 2: Relation Prediction - Combined Modes.Feature J48 K* NB MCBUncollapsed 58.99 55.41 56.69 35Collapsed 69.90 70.18 69.81 41Combined 78.62 71.92 80.00 35 (70)the GRANULARITY feature back into the Uncol-lapsed 8-way classifier.
We utilized the WEKAtoolkit (Witten and Frank, 2005) and treated eachclause as a vector of information (SDRT, EVENT,TENSE, ASPECT, SEQUENCE, CLAUSE, MODE,GRANULARITY), illustrated in (3)1:(3) a.
The Pittsburgh Steelers needed to win.START, State, Pa., N, .200, IC, NA, startb.
Batch took the first snap.ELAB., Occ., Pa., N, .400, IC, NA, pos.c.
Then he threw the ball into the endzone.NAR., Asp., Pa., N, .600, IC-M, NA, main.d.
Ward caught the ball.NAR., Occ., Pa., N, .800, IC, NA, main.e.
A touchdown was scored.RESULT, Occ., Pa., Perf., 1.00, IC, NA, neg.We report results from the Na?
?ve Bayes (NB), J48(C4.5 decision tree (Quinlan, 1993)) and K* (Clearyand Trigg, 1995) classifiers, run at 10-fold cross-validation.4 ResultsTable 2 indicates that the best average accuracy forthe Uncollapsed task is 58.99 (J48).
The accu-racy increases to 70.18 (K*) for the Collapsed task.The accuracy increases further to 80.00 (NB) for theCombined task.
All accuracies are statistically sig-nificant over majority class baselines (?MCB?
): Un-collapsed (MCB = 35) - ?2 = 15.11, d.f.
= 0, p ?.001; Collapsed (MCB = 41) - ?2 = 20.51, d.f.
=0, p ?
.001; and Combined (treating the best Col-lapsed accuracy as the new baseline (MCB = 70)) -?2 = 1.43, d.f.
= 0, p ?
.001.As shown in Table 3, based on the NB 8-wayCombined classifier, the prediction accuracies of1Note that what is being predicted is the rhetorical relation,or associated granularity, with the second clause in a clause pair.Tasks were performed where clause information was paired, butthis did not translate into improved accuracies.46Table 3: Individual Relation Prediction Accuracies (%).Relation A I D N R TNAR.
73 55 100 100 94 96RES.
75 88 85 100 100 93BACK.
93 92 96 87 94 92ELAB.
57 41 69 21 48 69CONSEQ.
20 0 0 0 0 37ALTER.
50 42 0 0 43 27CONTIN.
8 0 0 0 0 23EXPLAN.
0 20 0 9 0 2Total 68 72 92 74 74 80the individual modes are no more than 12 percent-age points off of the average (80.00).
Accura-cies range from 68% A(rgument) (?=-12) to 92%D(escription) (?=+12) with N(arrative), R(eport),and I(nformation) being closest to average (?=-6-8).
For individual relation predictions, NARRATION,RESULT and BACKGROUND have the highest totalaccuracies followed by ELABORATION and CON-TRAST.
Performing less well is CONSEQUENCE,ALTERNATION and CONTINUATION with EXPLA-NATION performing the worst.
All accuracies arestatistically significant above baseline (?2 = 341.89,d.f.
= 7, p ?
.001).5 Discussion and ConclusionUsing the Collapsed performance as a baseline forthe Combined classifier, we discuss the featurescontributing to the 10 percentage point increase aswell as the optimal (minimal) set of features for pre-diction.
The best accuracies for the Combined ex-periment only require CLAUSE and GRANULAR-ITY information; achieving 79.08% (NB - 44 aboveMCB, f-score=.750).
Both CLAUSE and GRANU-LARITY are necessary.
Relying only on CLAUSEachieves a 48.25% accuracy (J48) and relying onlyon GRANULARITY achieves 70.36% for all clas-sifiers, but this higher accuracy is an artifact of theorganization as evidenced by the f-score (.585).The relationship between CLAUSE and therhetorical relations is straightforward.
For example,the CONSEQUENCE relation is often an ?intersenten-tial?
relation (if the aardvark fell in the puddle, thenit got dirty), each of the 16 CONSEQUENCE relationsare embedded.
Similarly, 93% of all ELABORATIONrelations, which are temporally subordinating, areembedded.
Clause types appear to be a viable sourceof co-varying information in rhetorical relation pre-diction in the tasks under discussion.The aspects of syntactic-semantic form and prag-matic function in the relationship between granular-ity and rhetorical relations is of central interest inthis investigation.
Asher and Lascarides representdiscourses hierarchically through coordination andsubordination of information which corresponds tochanges in granularity.
However, while the notionof granularity enters into the motivation and formu-lation of the SDRT inventory, it is not developed fur-ther.
These results potentailly allow us to say some-thing deeper about the structural organization of dis-course as it relates to granularity.In particualr, while there is some probabilisticleverage in collapsing categories, it is not the casethat arbitrary categorizations will perform similarly.This observation holds true even for theoreticallyinformed categorizations.
For example, organizingthe SDRT inventory into coordinated and subordi-nated relations yields lower performance on relationprediction.
Coordinated and subordinated can bepredicted with 80% accuracy, but the prediction ofthe individual relations given the category performsonly at 70%.
Since the granularity-based organiza-tion presented here performs better, we suggest thatthe pragmatic function of the relation is more sys-tematic than the syntactic-semantic form of the rela-tion.Future research will focus on more data, differ-ent machine learning techniques (e.g.
unsupervisedlearning) and automatization.
Where clause, tense,aspect and event are readily automatable, rhetoricalrelations and granularity are less so.
Automaticallyextracting such information from an annotated cor-pus such as the Penn Discourse Tree Bank is cer-tainly feasible.
However, the distribution of genresin this corpus is somewhat limited (i.e., predomi-nately news text (Webber, 2009)) and calls into ques-tion the generalizeability of results to other modes ofdiscourse.
Overall, we have demonstrated that theinclusion of a granularity-based organization in themachine learning prediction of rhetorical relationsincreases performance by 37%, which is roughly14% above previous reported results for a broaderrange of discourses and relations.47AcknowledgmentsThank you to Jeff Ondich and Ultralingua for facil-itating this research and to four anonymous *SEMreviewers for insightful and constructive comments.ReferencesNicholas Asher and Alex Lascarides.
2003.
Logicsof Conversation.
Cambridge University Press, Cam-bridge, UK.John G. Cleary and Leonard E. Trigg 1995.
K*: AnInstance-based Learner Using an Entropic DistanceMeasure.
In Proceedings of the 12 International Con-ference on Machine Learning, 108?113.Jacob Cohen.
1960.
A Coefficient of Agreement forNominal Scales.
Educational and Psychological Mea-surement, 20(1):37?46.H.
Paul Grice.
1975.
Logic and Conversation.
In Syntaxand Semantics, Vol.
3, Speech Acts, 43?85.
AcademicPress, New York.Jerry R. Hobbs.
1985.
On The Coherence and Structureof Discourse.
CSLI Technical Report, CSLI-85-37.Blake Stephen Howald and Graham Katz.
2011.
TheExploitation of Spatial Information in Narrative Dis-course.
In Proceedings of the Ninth InternationalWorkshop on Computational Semantics, 175?184.Mirella Lapata and Alex Lascarides.
2004.
InferringSentence Internal Temporal Relations.
In Proceedingsof the North American Association of ComputationalLinguistics (NAACL-04) 2004, 153?160.Daniel Marcu.
1998.
Improving SummarizationThrough Rhetorical Parsing Tuning.
In Proceedings ofThe 6th Workshop on Very Large Corpora, 206?215.Daniel Marcu and Abdessamad Echihabi.
2002.
An Un-supervised Approach to Recognizing Discourse Rela-tions.
In Proceedings of the Association of Computa-tional Linguistics (ACL-02) 2002, 368?375.Rutu Mulkar-Mehta, Jerry R. Hobbs and Eduard Hovy.2011.
Granulairty in Natural Language Discourse.In Proceedings of the Ninth International Conferenceon Computational Semantics (IWCS 2011) 2011, 195?204.Georgiana Puscasu and Verginica Mititelu.
2008.
Anno-tation of WordNet Verbs with TimeML Event Classes.Proceedings of the Sixth International Language Re-sources and Evaluation (LREC08)James Pustejovsky, Jose?
Castan?o, Robert Ingria, RoserSaur, Robert Gaizauskas, Andrea Setzer, and GrahamKatz.
2005.
TimeML: Robust Specification of Eventand Temporal Expressions in Text.
In Proceedings ofthe Fith International Conference on ComputationalSemantics (IWCS 2005)Ross Quinlan.
1993 C4.5: Programs for Machine Learn-ing.
Morgan Kaufmann, San Francisco, CA.Carlota Smith.
2003.
Modes of Discourse: The LocalStructure of Texts.
Cambridge University Press, Cam-bridge, UK.Caroline Sporleder and Alex Lascarides.
2005.
Exploit-ing Linguistic Cues to Classify Rhetorical Relations.In Proceedings of Recent Advances in Natural Lan-guage Processing (RANLP-05), 532?539.Caroline Sporleder and Alex Lascarides.
2008.
UsingAutomatically Labelled Examples to Classify Rhetori-cal Relations: An Assessment.
Natural Language En-gineering, 14:369?416.Janyce Wiebe, Thomas O?Hara, Thorsten O?hrstro?m-Sandgren and Kenneth McKeever.
1997.
An Em-pirical Approach to Temporal Reference Resolution.In Proceedings of the 2nd Conference on EmpiricalMethods in Natural Language Processing (EMNLP-97), 174?186.Ian Witten and Eibe Frank.
2005.
Data Mining: Prac-tical Machine Learning Techniques with Java Imple-mentation (2nd Ed.)
Morgan Kaufmann, San Fran-cisco, CA.Bonnie Webber 2009.
Genre Distictions for Discoursein the Penn TreeBank.
In Proceedings of the 47th ACLConference, 674?682.48
