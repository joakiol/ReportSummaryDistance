Two Questions about Data-Oriented Parsing*Rens BodUniversity of AmsterdamDepartment ofComputational LinguisticsSpuistraat 134, NL-1012 VB AmsterdamRens.B od @ let.uva.nlAbstractIn this paper I present ongoing work on the data-oriented parsing (DOP) model.
In previouswork, DOP was tested on a cleaned-up set of analyzed part-of-speech strings from the PennTreebank, achieving excellent est results.
This left, however, two important questionsunanswered: (1) how does DOP perform if tested on unedited ata, and (2) how can DOP beused for parsing word strings that contain unknown words?
This paper addresses thesequestions.
We show that parse results on unedited ata are worse than on cleaned-up data,although still very competitive if compared to other models.
As to the parsing of word strings,we show that the hardness of the problem does not so much depend on unknown words, but onpreviously unseen lexical categories of known words.
We give a novel method for parsingthese words by estimating the probabilities of unknown subtrees.
The method is of generalinterest since it shows that good performance can be obtained without he use of a part-of-speech tagger.
To the best of our knowledge, our method outperforms other statistical parserstested on Penn Treebank word strings.1 IntroductionThe Data-Oriented Parsing (DOP) method suggested in Scha (1990) and developed in Bod (1992-1995) is a probabilistic parsing strategy which does not single out a narrowly predefined set ofstructures as the statistically significant ones.
It accomplishes this by maintaining a large corpus ofanalyses of previously occurring utterances.
New input is parsed by combining tree-fragments from thecorpus; the frequencies of these fragments are used to estimate which analysis is the most probable one.In previous work, we tested the DOP method on a cleaned-up set of analyzed part-of-speech stringsfrom the Penn Treebank (Marcus et al, 1993), achieving excellent test results (Bod, 1993a, b).
This left,however, two important questions unanswered: (1) how does DOP perform if tested on unediteddata,and (2), how can DOP be used for parsing word strings that contain unknown words?This paper addresses these questions.
The rest of it is divided into three parts.
In section 2 we give ashort resume of the DOP method.
In section 3 we address the first question: how does DOP performon unedited ata?
In section 4 we deal with the question how DOP can be used for parsing word stringsthat contain unknown words.
This second question turns out to be the actual focus of the article, whilethe answer to the first question serves as a baseline.
* This work was partially supported by the Netherlands Organization for Scientific Research (NWO).1252 Resume of Data-Oriented ParsingFollowing Bod (1995a), a Data-Oriented Parsing model can be characterized by (1) a definition of aformal representation for utterance analyses, (2) a definition of the fragments of the utterance analysesthat may be used as units in constructing an analysis of a new utterance, (3) a definition of theoperations that may be used in combining fragments, and (4) a definition of the way in which theprobability of an analysis of a new utterance is computed on the basis of occurrence-frequencies of thefragments in the corpus.
In Bod (1992, 1993a), a first instantiation of this model is given, called DOP1,which uses (1) labelled trees for the utterance analyses, (2) subtrees for the fragments, (3) node-substitution for combining subtrees, and (4), the sum of the probabilities of all distinct ways ofgenerating an analysis as a def'mition of the probability of that analysis.An example may illustrate DOP1.
Consider the imaginary, extremely simple corpus in figure 1 whichconsists of only two trees.S SNP VPI shewantedNP PP~ ~ displayed thethe dress P I'~PI /N  on the rackVPvP PPV P NPblock on the tableFigure 1.
Example corpus of two trees.DOP1 uses substitution for the combination of subtrees.
The combination of subtree t and subtree u,written as t o u, yields a copy of t in which its leftmost nonterminal leaf node has been identified withthe root node of u (i.e., u is substituted on the leftmost nonterminal leaf node of t).
For reasons ofsimplicity we will write in the following (t o u) o v as t o u ?
v. Now the (ambiguous) sentence "Shedisplayed the dress on the table" can be parsed in many ways by combining subtrees from the corpus.For instance:S o NP o pp'Ai v /x , , ,  onl .eshe VP PPV NPIdisplayed126NP VPshe VP/Ndisplayed the dressPP/Non the tableFigure 2.
Derivation and parse tree for "She displayed the dress on the table"As the reader may easily ascertain, a different derivation may yield a different parse tree.
However, adifferent derivation may also very well yield the same parse tree; for instance:S o VP o NPNP VP I the dress\[ ~ displayedlshe VP PPp NPI /Non the tableFigure 3.
Different derivation generating the same parse tree for "She displayed the dress on the table"orS o VP  o NPNP VP VP pp the dressshe V NP P ~ Pdisplayed !n /~the  tableFigure 4.
Another derivation generating the same tree for "She displayed the dress on the table"Thus, one parse tree can be generated by many derivations involving different corpus-subtrees.
DOP1estimates the probability of substituting a subtree ti on a specific node as the probability of selecting tiamong all subtrees in the corpus that could be substituted on that node.
This probability can be estimated127as the number of occurrences of a subtree ti, divided by the total number of occurrences of subtrees twith the same root node label as ti: #(ti) / #(t : root(t) = root(t/)).
The probability of a derivation tl o...o tncan be computed as the product of the probabilities of the substitutions that it involves:IFli #(t i ) /#(t  : root(t) = root(t/) .
The probability of a parse tree is equal to the probability that any of itsderivations occurs, which is the sum of the probabilities of all derivations of that parse tree.
If a parsetree has kdefivations: (t11%.
o t l j  o...) ..... (ti l  o.,.
o t~ o...) ..... ( tk l  o... o tkj o...), its probability canbe written as: Zi 1FIj #(tV) I#(t : root(t) = root(t~) ).Bod (1992, 1993a) shows that conventional context-free parsing techniques can be used in creating aparse forest for a sentence in DOP1.
To select he most probable parse from a forest, Bod (1993-95)and Rajman (1995a,b) give Monte Carlo approximation algorithms.
Sima'an (1995) gives an efficientpolynomial algorithm for selecting the parse corresponding to the most probable derivation.
InGoodman (1996), an efficient parsing strategy is given that maximizes the expected number of correctconstituents.
1 The DOP1 model, and some variations of it, have been tested by Bod (1993-1995),Sima'an (1995-1996), Sekine & Grishman (1995), Goodman (1996), and Charniak (1996).3 How does DOP perform on unedited data?Our first question is concerned with the performance of DOP1 on unedited ata.
To deal with thisquestion, we use ATIS p-o-s trees as found in the Penn Treebank (Marcus et al, 1993).
This papercontains the first published results with DOP1 on unedited ATIS data.
2 Although Sima'an (1996) andGoodman (1996) also report experiments on unedited ATIS trees, their results do not refer to the mostprobable parse but to the most probable derivation and the maximum constituents parse respectively.We will not deal here with the algorithms for calculating the most probable parse of a sentence.
Thesehave been extensively described in Bod (1993-1995) and Rajman (1995a,b).3.1 Experiments with the most probable parseIt is one of the most essential features of the DOP approach, that arbitrarily large subtrees are taken intoconsideration to estimate the probability of a parse.
In order to test the usefulness of this feature, weperformed ifferent experiments constraining the depth of the subtrees.
The depth of a tree is defined asthe length of its longest path.
The following table shows the results of seven experiments for differentmaximum depths of the subtrees.
These were obtained by dividing the ATIS trees at random into 500training set trees and 100 test set trees.
The training set trees were converted into subtrees together withtheir substitution probabilities.
The part-of-speech sequences from the test set served as input sentencesthat were parsed and disambiguated using the subtrees from the training set.
The parse accuracy isdefined as the percentage of test sentences for which the most probable parse exactly matches with thetest set parse.1 Goodman's approach is different from Bod (1993-1995) and Sima'an (1995) in that it returns best parsesthat cannot be generated by the DOP1 model (see Bod, 1996 for a reply to Goodman's paper).2 Bod (1995b) also reports on experiments with unedited ata, but the book in which that paper is includedhas not yet appeared.128depth ofsubtrees1_<2_<3_<4_<5_<6unboundedparseaccuracy27 %45 %56 %59 %61%62 %64 %Table 3.1.
Parse accuracies for DOP1 for different maximum subtree depths.The table shows a considerable increase in parse accuracy when enlarging the maximum depth of thesubtrees from 1 to 2.
The accuracy keeps increasing, at a slower ate, when the depth is enlarged further.The highest accuracy of 64% is achieved by using all subtrees from the training set.
If once-occurringsubtrees were ignored, the maximum parse accuracy decreased to 60%.
This shows the importance oftaking all subtrees from the training set.However, the accuracy of 64% is disappointingly bad when compared to exact match results reportedon clean ATIS data: 96% in Bod (1995a), 90% to 95% in Carter & Rayner (1994).
An explanation maybe that the exact match metric is very sensitive to annotation errors.
Since the raw Penn Treebank datacontains many inconsistencies in its annotations (cf.
Ratnaparkhi, 1996), a single inconsistency in a testset tree will very likely yield a zero percent parse accuracy for the particular test set sentence.
Thus, weshould raise the question as to whether the exact match is an interesting metric for parsing withinconsistent dam.
3Accuracy metrics that are less sensitive to annotations errors are the so-called bracketing accuracy (thepercentage of the brackets of the most probable parses that do not cross the brackets in the test setparses), and the sentence accuracy (the percentage of the most probable parses in which no bracketscross the brackets in the test set parses).
We also calculated the accuracies according to these metrics forDOP1.
To increase the reliability of our results, we performed experiments with 8 different randomdivisions of ATIS into training sets of 500 and test sets of 100 trees.
The following table shows themeans of the results for the three accuracy metrics with their standard eviations.Accuracy metricParse accuracySentence accuracyBracketing accuracyMean62.3 %72.8 %94 .1%StdDev3 .4%4.0%1.8%Table 3.2.
Mean accuracies for DOP1 for 8 different raining-test ets from ATIS3 This question only refers to syntactic parsing.
Semantic parsing turns out to be much more robust toannotation errors.
In Bod, Bonnema & Scha (1996), it is shown that on semantically enriched ATIS trees,88% of the test sentences obtain the correct semantic interpretation, while only 62% obtain a fully correctsyntactic structure.1293.2 Comparison with other systemsAbove results now allow us to compare the accuracy of DOP1 with other systems tested on uneditedATIS data.
The system developed by (Pereira & Schabes 1992), where use is made of an iterativereestimation algorithm derived from the well-known inside-outside algorithm (Baker, 1979), obtains90.4% bracketing accuracy.
This is lower than our 94.1%.
Pereira and Schabes do not report thesentence accuracy nor the parse accuracy of their system.
The system developed by (Brill, 1993) uses atransformation-based learning algorithm.
He reports 91.1% bracketing accuracy and 60% sentenceaccuracy on the ATIS.
Although Bfill does not report he parse accuracy of his system, we can derivethat DOP1 does better on the bracketing accuracy and the sentence accuracy.
Finally, the system of (deMarcken, 1995), who incorporates mutual information between phrase heads, obtains maximally 92.0%bracketing accuracy.Thus, we derive that on unedited ATIS data, DOP obtains very competitive r sults, if not better esultsthan other systems.
This is remarkable, since DOP is not trained: it reads the rules (or subtrees) directlyfrom hand-parsed sentences in a treebank, and calculates the probability of a new tree on the basis ofraw subtree-frequencies in the corpus.
Thus, one can seriously put into question the merits ofsophisticated training and learning algorithms.We should of course ask whether the relative succes of the DOP approach only holds for such limiteddomains as ATIS, or whether the approach can be effectively applied to larger corpora such as the WallStreet Journal (WSJ) corpus?
Although we have not yet accomplished experiments on the WSJ, it maybe interesting to report an experiment accomplished by Charniak (1996).
Charniak applies the DOPapproach to p-o-s strings from Penn's WSJ.
Although he only uses corpus-subtrees smaller than depth2 (which in our experience constitutes a less-than-optimal version of the DOP method -- see table 3.1),Charniak applies exactly the same statistics without ignoring low-count events.
He reports that hisprogram "outperforms all other non-word-based statistical parsers/grammars on this corpus".
Weconjecture that his results will be even better if larger subtrees are taken into account.4 Can DOP be used for parsing word strings?The second question mentioned in the introduction concerns the problem of parsing word stnngs.
SinceDOP1 only uses subtrees that are literally found in a training set, it cannot adequately parse ordisambiguate s ntences with unknown words.
In this section, we investigate what is involved inextending DOP1 in order to parse sentences that possibly contain unknown words.
The problem is ofgeneral importance, since practically all methods eem to have accepted the use of a two step approach:first tag the words by a part-of-speech tagger, then parse the tags (with or without the words) by astochastic parser.
We strongly believe that such a two step approach is not optimal (see section 4.3.3),and we therefore want to cope with word parsing by skipping the p-o-s tagging step.4.1 The model DOP2: the partial parse methodIn order to get a feeling of the problems emerging from word parsing, we propose as a very firsttentative solution the model DOP2.
DOP2 is a very simple extension of DOPI: assign all lexicalcategories to each unknown word, and select he most probable parse among the parses of all resulting"sentences" by means of DOP1.
Thus, unknown words are assigned a lexical category such that their130"surrounding" partial parse has maximal probability.
We shall refer to this method as the partial parsemethod.The computational spects of DOP2 are straightforward: we can basically employ the same algorithmsas developed for DOP1 (Bod, 1995a).
We only need to establish the unknown words of an inputsentence and label them with all lexical categories.
Remember that for input sentences without unknownwords, DOP2 is identical to DOP1.4.2 Experiments with DOP2: the problem of unknown-category wordsIn our experiments with DOP2, we used the same initial division of the ATIS corpus as in section 3.1into a training set of 500 trees and a test set of I00 trees, but now the trees were not stripped of theirwords.
For time-cost reasons, no experiments were performed with subtrees larger than depth 3.
Thefollowing table gives the results of these experiments (for subtree-depth _<3).
We representedrespectively the parse accuracy of the test sentences that contained at least one unknown word, the parseaccuracy of the test sentences without unknown words, and finally, the parse accuracy of all testsentences together (we will come back to the other accuracy metrics later).test sentenceswith unknown wordswith only known wordsall test sentencesparse accuracy20%33%24%Table 4.1.
Parse accuracy for word strings from the ATIS corpus by DOP2The table shows that the results of the partial parse method are disappointingly bad.
For the sentenceswith unknown words, only 20% are parsed correctly.
However, if plain DOP1 were used, the accuracywould have been 0% for these sentences.
If we look at the sentences with only known words (whereDOP2 is equivalent to DOP1), we see that the parse accuracy of 33% is higher than for sentences withunknown words.
However, it remains far behind the 56% parse accuracy of DOP1 for part-of-speechstrings (for the same subtree depth; see table 3.1).
Word parsing is obviously amuch more difficult taskthan part-of-speech parsing, even if all words are known.Looking more carefully to the parse results of test sentences with only known words, we discover astriking result: for many of these sentences no parse could be generated at all, not because a word wasunknown, but because an ambiguous word required a lexical category which it didn't have in the trainingset.
We will call these words unknown-category words.One might argue that the problem of unknown-category words is due to the tiny size of the ATIScorpus.
However, no corpus of any size will ever contain all possible uses of all possible words.
Eventhe extension with a dictionary does not solve the problem.
There will be domain-specific words andword senses, abbreviations, proper nouns etc.
that are not found in a dictionary.
It remains thereforeimportant to study how to deal with unknown words and unknown-category words in a statisticallyadequate way.1314.3 The model DOP3: a corpus as a sample of a larger populationWe have seen that the partial parse method, employed by DOP2, yields very poor predictions for thecorrect parse of a sentence with (an) unknown word(s).
For sentences with unknown-category words,the method appeared to be completely inadequate.
A reason for these shortcomings may be thestatistical inadequacy of DOP2: it does not allow for the computation of the probability of a parsecontaining one or more unknown words.
In this section, we study what is involved in creating anextension of DOP1 which can compute probabilities of parses containing unknown (-category) words.In order to create such an extension, we need a method that estimates the probabilities of unknownsubtrees.4.3.1 The problem of unknown subtreesBy an unknown subtree we mean a subtree which does not occur in the training set, but which mayshow up in an additional sample of trees (like the test set).
We will restrict ourselves to subtrees whoseunknownness depends only on unknown terminals.
We assume that there are no unknown subtrees thatdepend on an unknown syntactic structure.
Thus, if there is an unknown subtree in the test set, thenthere is a subtree in the training set which differs from the unknown subtree only in some of itsterminals.
In general, this assumption is wrong, but for the ATIS domain it may not be unreasonable.
Inprinciple, the assumption that only the terminals in unknown subtrees may be unknown can beabolished, but this would lead to a space of possible subtrees which is computationally intractable.Even with the current restriction, the problem is far from trivial.
Two main questions are:1.
How can we derive unknown subtrees?2.
How can we estimate the probabilities of unknown subtrees?As to the first question, we are not able to generate the space of unknown subtrees in advance, as we donot know the unknown terminals in advance.
But since we assume that all syntactic structures have beenseen, we can derive the unknown subtrees that are needed for parsing a certain input sentence, byallowing the unknown words and unknown-category words of the sentence to mismatch with the lexicalterminals of the training set subtrees.
The result of a mismatch between a subtree and one or moreunknown (-category) words is a subtree in which the terminals are replaced by the words with which itmismatched.
In other words, the subtree-terminals re treated as if they are wildcards.
As a result, wemay get subtrees in the parse forest hat do not occur in the training set.The mismatch-method has one bottleneck: the unknown words and unknown-category words of asentence need to be known before parsing can start.
It is easy to establish the unknown words of asentence, but it is unclear how to establish the unknown-category words.
Since every word is a potentialunknown-category word (even closed-class words, if a small corpus is used), we ideally need to treat allwords of a sentence as possible unknown-category words.
Thus, any subtree-terminal is allowed tomismatch with any word of the input sentence.
(In our experiments, however, we need to limit thepotential unknown-category words as much as possible; cf.
?4.5.
)How can we estimate the probability of a subtree which appears as a result of the mismatch-method inthe parse forest, but not in the training set?
It is evident hat we cannot assume, as we did in DOP1, thatthe space of training set subtrees represents he total population of subtrees, since this would lead to azero probability for any unknown subtree.
We therefore pursue an alternative approach, and treat the132space of subtrees as a sample of a larger population.
We believe that such an approach is also reasonablefrom a cognitive point of view.An unknown subtree which has a zero probability in a sample may have a non-zero probability in thetotal population.
Moreover, also known subtrees may have population probabilities that differ from theirsample probabilities.
The problem is how to estimate the population probability of a subtree on the basisof the observed sample.
Much work in statistics is concerned with the fitting of particular distributionsto sample data, with or without motivating why these distributions might be expected to be suitable.
Amethod which is largely independent of the distributions of population probabilities i  the so-calledGood-Turing method (Good, 1953).
It only assumes that the sample is obtained at random from thetotal population.
We will propose this method for estimating the probabilities of unknown subtrees, aswell as of known subtrees.
We briefly describe the method in the following section and then go into theproblem of applying Good-Turing to subtrees (these sections heavily rely on Church & Gale, 1991),Although more sophisticated methods exist as well, we will stick to Good-Turing for the scope of thispaper.4.3.2 Good-Turing: estimating the population probabilities of (un)seen typesThe Good-Turing method, (Good, 1953), estimates the probability of a type t by adjusting its observedsample frequency fit).
To do that, Good-Turing uses an additional notion, represented by Nr, which isdefined as the number of types which are instantiated by r tokens in an observed sample: Nr = #({ t Ifit) = r }).
Thus, Nr is the frequency of frequency r. The Good-Tufing estimator computes for everyfrequency r an adjusted frequency r* asNr+ 1r* = (r+l)NrThe expected probability of a type t with sample frequency fit) = r is estimated by r*/N, where N is thetotal number of observed types.
Good-Tufing obtains good estimates for r*/N if Nr is large.
We willsee that for our applications, Nr tends to be large for small frequencies r, while on the other hand, if Nris small, r is usually large and needs not to be adjusted.For the adjustment of the frequencies of unseen types, where r = 0, r* is equal to N1/No, where NO isthe number of types that we have not seen.
NO is equal to the difference between the total number oftypes and the number of observed types.
Thus, in order to calculate the adjusted frequency of an unseentype, one needs to know the total number of types in the population.
Notice that Good-Turing does notdifferentiate among the types that have not been seen: the adjusted frequencies of all unseen types areidentical.4.3.3 Using Good-Turing to adjust the frequencies of subtreesThe use of the Good-Tufing method in natural language technology is far from new.
It is commonlyapplied in speech recognition and part-of-speech tagging for adjusting the frequencies of (un)seen wordsequences (e.g.
Jelinek, 1985; Katz, 1987; Church & Gale, 1991).
In stochastic parsing, Good-Turinghas to our knowledge never been tried out.
Designers of stochastic parsers eem to have given up on theproblem of creating a statistically adequate theory concerning parsing unknown events.
Stochasticparsing systems either use a closed lexicon, or use a two step approach where first the words are tagged133by a stochastic tagger, after which the p-o-s tags (with or without he words) are parsed by a stochasticparser.
The latter approach as become increasingly popular (e.g.
Schabes et al, 1993; Weischedel t al.,1993; Briscoe, 1994; Magerman, 1995; Collins, 1996).
Notice, however, that the tagger used in this twostep approach often uses Good-Turing (or a similar smoothing method) to adjust the observedfrequencies of n-grams.
So why not apply Good-Turing directly to the structural units of a stochasticgrammar?This lack of interest in using Good-Turing may be due to the fact that many stochastic grammars arestill being constructed within the grammar-building community.
In this community, it is generallyassumed that grammars need to be as succinct as possible.
The existence of unobserved rules isunacceptable from such a competence point of view.
4 But from a performance point of view, it is verywell acceptable that not all statistical units (in our case, subtrees) have been seen; therefore we will putforward the Good-Turing estimator as a statistically and cognitively adequate extension of DOP1.How can Good-Turing be used for adjusting the frequencies of known and unknown subtrees?
It maybe evident hat it is too rough to apply Good-Turing to all subtrees together.
We must distinguishbetween subtrees of different roots, since in DOP, the spaces of subtrees of a certain root constitutedifferent distributions, for each of which the substitution-probabilities sum up to one.
Therefore, Good-Turing is applied to each subtree-class eparately, that is, to the S-subtrees, NP-subtrees, VP-subtrees,N-subtrees, V-subtrees, etc.
As in the previous ection, we will take only the subtrees upto depth three.In order to clarify this, we show in table 4.2 the adjusted frequencies for a class of 118348 NP-subtrees.The first column of the table shows the observed frequencies of NP-subtrees from zero to six.
Thesecond column shows Nr, the number of NP-subtrees that had those frequencies in the training set (theestimation of NO is a special case and will be dealt with shortly).
The third column shows the adjustedfrequencies as calculated by the Good-Tufing formula.Nr110000000060416905741611944773482r ?0.0000550.301.371.861.993.744.37Table 4.2.
Adjusted frequencies for NP-subtreesThe calculations for r = 0 rest on an estimation of NO, the number of NP-subtrees that have not beenseen.
NO is equal to the difference between the total number of distinct NP-subtrees and the number ofdistinct NP-subtrees seen.
Thus, we must estimate the total number of possible NP-subtrees.
To makesuch an estimation feasible, we use the following assumptions:* No subtree is larger than depth 3.
This was already assumed.
* The unknownness of unseen subtrees only depends on the terminals.
Also this was assumed before.It implies that all unlexicalized NP-subtrees (i.e.
all NP-subtrees without words) are known.4 Although the opposite opinion may be heard as well (e.g.
Sampson, 1987).134* The size of the vocabulary is known.
This is common practice in corpus linguistics, where theestimations are usually restricted to the domain under study.
For instance, for the estimation of thepopulation of bigrams the number of distinct unigrams is usually assumed to be known.
In our case, wehappen to know that the whole ATIS domain contains 1508 distinct words.Our calculation of (1) the total number of distinct NP-subtrees, and (2) NO, can now be accomplished asfollows:(1) The total number of NP-subtrees (that can be the result of the mismatch-method) is calculated byattaching in all possible ways 1508 dummies to the tags of the unlexicalized NP-subtrees from thetraining set.
This yields a number of 1.10259 x 10 9 distinct subtrees.
To this number, the number ofdistinct unlexicalized NP-subtrees must be added (12429), yielding 1.10260 x 10 9 types for the totalnumber of distinct NP-subtrees.
(2) The number of unseen types NO is the difference between the total number of distinct NP-subtreesand the observed number of distinct NP-subtrees, Zr>O Nr, which is 1.10260 x 10 9 - 77479 =1.10253 x 10 9.Coming back to our adjustment of the frequency of unseen NP-subtrees, this can now be calculated byGood-Turing as N1/No = 60416/1.1x109 = 0.000055.4.4 The model DOP3DOP3 is very similar to DOP1.
What is different in DOP3 is (1) a much larger space of subtrees,which is extended to include subtrees in which one or more terminals are treated as wildcards, and (2)the frequencies of the subtrees, that are now adjusted by the Good-Turing estimator.
The probabilitydefinitions of derivation and parse in DOP3 are the same as in DOP1.As to the computational spects, we can very easily extend the parsing algorithms designed for DOP1to DOP3, by allowing the terminals of subtrees to mismatch with the words of the input sentence.
Afterassigning the adjusted probabilities to the subtrees in the resulting parse forest, the most probable parsecan be estimated in the same way as in DOP1 by Monte Carlo.In (Bod, 1995a), some interesting properties of DOP3 are derived.
Among others, it is shown thatDOP3 displays a preference for parses constructed by generalizing over a minimal number of words,and that DOP3 prefers parses that generalize over open-class words to parses that generalize overclosed-class words.4.5 Experimental aspects of DOP3Treating all words as potential unknown-category words would certainly lead to an impractically largenumber of subtrees in the parse forest.
As we have seen, the set of possible NP-subtrees (of maximaldepth three) consists of 10 9 types, which is a factor 12000 larger than the set of seen NP-subtrees (8 x104).
It is therefore vident hat we will get impractical processing times with DOP3.If we still want to perform experiments with DOP3, we need to limit the mismatches as much aspossible.
It seems reasonable to allow the mismatches only for unknown words, and for a restricted set135of potential unknown-category words.
From the ATIS training set we derive that only nouns and verbsare actually lexically ambiguous.
In our experiments, we will therefore limit the potential unknown-category words of an input sentence to the nouns and verbs.
This means that only the words which areunknown in the training set and the words of the test sentence which are tagged as a noun or a verb inthe training set are allowed to mismatch with subtree-terminals.We used the initial random division of the ATIS corpus into a training set of 500 trees and a test set of100 trees.
In order to carefully study the experimental merits of DOP3, we distinguished two classes oftest sentences:1. test sentences containing both unknown and unknown-category words2.
test sentences containing only unknown-category wordsNote that all 100 test sentences contained at least one potential unknown-category word (verb or noun).The following table shows the parse accuracy for subtree-depth _< 3.test sentencesw i th  unknown words andunknown-category  wordsw i th  only  unknown-category  wordsal l  test sentencesparse accuracy34%57%41%Table 4.3.
Parse accuracy for word strings from the ATIS corpus by DOP3The table shows that DOP3 has better performance than DOP2 in all respects (cf.
table 4.1).
The parseaccuracy for the sentences with unknown and unknown-category words is with 34% much higher thanthe 20% of DOP2.
As to the sentences with only unknown-category words, the improvement of DOP3with respect o DOP2 is most noticeable: the accuracy increased from 33% to 57%.
However, thecomparison with DOP2 may not be fair, as DOP2 cannot deal with unknown-category words at all.What the parse results of DOP3 do indicate is, that, for sentences without unknown words, the parseaccuracy for word strings is of the same order as the parse accuracy for p-o-s strings (which was 56%at maximum depth 3; see section 4.2).
Nevertheless, the total parse accuracy of 41% is still bad.4.6 Enriching DOP3 with a dictionary: the model DOP4A method which may further improve the accuracy of DOP3 may lie in the use of an externaldictionary.
In the absence of morphological nnotations in ATIS, a dictionary can provide the lexicalcategories of both known and unknown words of an input sentence.
Unfortunately, the ATIS corpuscontains everal abbreviations and proper nouns that are not found in a dictionary, and which thereforestill need to be treated as unknown by means of DOP3.
In the following, we will refer to the extensionof DOP3 with an external dictionary as DOP4.
DOP4 puts all lexical categories (p-o-s tags) of thesentence words, as found in a dictionary, in the chart.
Secondly, the sentence is parsed by DOP3,136provided that subtree-terminalsareallowed to mismatch only with the words that were not found in thedictionary.In our experiments, we used Longman Dictionary (Longman, 1988) to assign lexical categories to thewords of the test sentences.
The lexical categories used in Longman are not equal to the lexicalcategories used in the ATIS corpus, and needed to be converted.
The following table shows the resultsof DOP4 compared with those of DOP3.test sentenceswith unknown words andunknown-category wordswith only unknown-category wordsall test sentencesparse accuracyDOP334%57%41%DOP447%60%51%Table 4.4.
Parse accuracy for word strings from the ATIS corpus by DOP4 against DOP3.The table shows that there is a considerable increase in parse accuracy from 34% to 47% for sentenceswith unknown words, while the accuracy for sentences with only unknown-category words shows aslight increase from 57% to 60%.
The total parse accuracy of DOP4 reaches 51%.
The following tablealso shows the corresponding sentence accuracy and bracketing accuracy of DOP4 (for all testsentences together).Accuracy  metr i cParse  accuracySentence  accuracyBracket ing accuracyDOP451%68%93.2%Table 4.5.
Accuracies for ATIS word strings by DOP4The above accuracies are the best published results on Penn Treebank ATIS word strings, to the best ofour knowledge.
Moreover, our results are still based on a limited version of DOP, since for time costreasons no subtrees larger than depth 3 were used.We must keep in mind that DOP4 is a hybrid model, where frequencies of subtrees are combined witha dictionary look-up.
What we hope to have shown, is, that it is possible to extend a stochastic parsingmodel in a statistically and cognitively adequate way, such that it can directly parse and disambiguateword strings that contain unknown (-category) words without he need of an external part-of-speechtagger.137ConclusionIn this paper we have addressed two, previously neglected questions about he DOP model: how doesDOP perform if tested on unedited Penn Treebank data, and (2), how can DOP be used for directlyparsing word strings that contain unknown words.
We have shown that although parse results areconsiderably ower on unedited ata than on cleaned-up data, they are very competitive, if not better thanother models.
With respect o the parsing of word strings, we have shown that the hardness of theproblem does not lie so much in unknown words, but in previously unseen lexical categories of knownwords.
We have given a novel method for parsing these words by estimating the probabilities ofunknown subtrees.
The method was tested on ATIS trees obtaining results that to the best of ourknowledge are not exceeded by other stochastic parsers.
Moreover, the results of a less-than-optimalversion of DOP on the Wall Street Journal corpus suggest that the approach can be succesfully extendedto larger domains.
As future research, we will apply the full DOP model on WSJ word strings in orderto compare our results with the best known parsers on this domain (Magerman, 1995; Collins, 1996).AcknowledgementsI am grateful to Remko Scha for many useful comments and additions.
I also thank three anonymousreviewers for their comments.ReferencesJ.
Baker, 1979.
"Trainable grammars for speech recognition", In J. Wolf & D. Klatt (eds.)
Speechcommunication papers presented at the 97th Meeting of the Acoustical Society of America, MIT, Cambridge,MA.R.
Bod, 1992.
"Data Oriented Parsing (DOP)", Proceedings COLING"92, Nantes.R.
Bod, 1993a.
"Using an Annotated Corpus as a Stochastic Grammar", Proceedings EACL'93, Utrecht.R.
Bod, 1993b.
"Monte Carlo Parsing", Proceedings Third International Workshop on Parsing Technologies,Tilburg/Durbuy.R.
Bod, 1995a.
Enriching Linguistics with Statistics: Performance Models of Natural Language, ILLCDissertation Series 1995-14, University of Amsterdam (obtainable via anonymous ftp:ftp:l/ftp.fwi.uva.nllpubltheorylillc/dissertationslDS-95-14.text.ps.gz).R.
Bod, 1995b.
"Monte Carlo Parsing", in H. Bunt and M. Tomita (eds.)
Recent Advances in ParsingTechnology, Kluwer Academic Publishers.R.
Bod, 1996.
"Efficient Algorithms for Parsing the DOP Model?
A Reply to Joshua Goodman."
CMP-LG/9605031.R.
Bod, R. Bonnema nd R. Scha, 1996.
"A Data-Oriented Approach to Semantic Interpretation", to appearin Proceedings Workshop on Corpus-Oriented Semantic Analysis, ECAI-96, Budapest.
(also in CMP-LG)T. Briscoe, 1994.
"Prospects forTechniques", N. Oostdijk and P.Amsterdam.Practical Parsing of Unrestricted Text: Robust Statistical Parsingde Haan (eds.
), Corpus-based Research into Language, Rodopi,138E.
Brill, 1993.
"Transformation-Based Error-Driven Parsing", Proceedings Third International Workshop onParsing Technologies, Tilburg/Durbuy.D.
Carter and M. Rayner, 1994.
"The Speech-Language Interface in the Spoken Language Translator", L.Boves & A. Nijholt (eds.)
Speech and Language Engineering, Proceedings of the eighth Twente Workshopon Language Technology, Enschede.M.
Collins, 1996.
"A New Statistical Parser Based on Bigram Lexical Dependencies", to appear inProceedings ACL-96, Santa Cruz (CA).E.
Charniak, 1996.
"Tree-bank Grammars", Technical Report CS-96-02, Department of Computer Science,Brown University.
(obtainable via WWW home page of Eugene Charniak)K. Church, 1988.
"A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text", ProceedingsANLP'88, Austin, Texas.K.
Church and W. Gale, 1991.
"A comparison of the enhanced Good-Turing and deleted estimationmethods for estimating probabilities of English bigrams", Computer Speech and Language 5, 19-54.I.
Good, 1953.
"The Population Frequencies of Species and the Estimation of Population Parameters",Biometrika 40, 237-264.J.
Goodman, 1996.
"Efficient Algorithms for Parsing the DOP Model", Proceedings Empirical Methods inNatural Language Processing, Philadelphia.F.
Jelinek, 1985.
"The Development of an Experimental Discrete Dictation Recognizer", IEEE'85 (InvitedPaper).F.
Jelinek and R. Mercer, 1985.
"Probability Distribution Estimation from Sparse Data", IBM TechnicalDisclosure Bulletin 28, 2591-2594.S.
Katz, 1987.
"Estimation of probabilities from sparse data for the language model component of a speechrecognizer", IEEE Transactions on Acoustics, Speech, and Signal Processing, ASSP-35, 400-401.Longman Dictionary of the English Language, 1988, Longman, London.D.
Magerman, 1995.
"Statistical Decision-Tree Models for Parsing", Proceedings ACL'95, Cambridge,Massachusetts.C.
de Marcken, 1995.
"Lexical Heads, Phrase Structure and the Induction of Grammar", Proceedings of theThird Workshop on Very Large Corpora (WVLC-3), Cambridge (Mass.).M.
Marcus, B. Santorini and M. Marcinkiewicz, 1993.
"Building a Large Annotated Corpus of English: thePenn Treebank", Computational Linguistics 19(2).F.
Pereira and Y. Schabes, 1992.
"Inside-Outside Reestimation from Partially Bracketed Corpora",Proceedings A CL '92, Newark.M.
Rajman, 1995a.
"Approche Probabiliste de l'Analyse Syntaxique", Traitement Automatique d s Langues,vol.
36(1-2).139M.
Rajman, 1995b.
Apports d'une approche a base de corpus aux techniques de traitement automatique dulangage naturel, PhD thesis, Ecole Nationale Superieure des Telecommunications, Paris.A.
Ratnaparkhi, 1996.
"A Maximum Entropy Model for Part-Of-Speech Tagging", Proceedings EmpiricalMethods in Natural Language Processing, Philadelphia.G.
Sampson, 1987.
"Evidence against the 'Grammatical/Ungrammatical' Distinction", W. Meijs (ed.
),Corpus Linguistics and Beyond, Rodopi, Amsterdam.R.
Scha, 1990.
"Taaltheorie n Taaltechnologie; Competence n Performance", in Q.A.M.
de Kort andG.L.J.
Leerdam (eds.
), Computertoepassingen in de Neerlandistiek, Almere: Landelijke Vereniging vanNeerlandici (LVVN-jaarboek).Y.
Schabes, M. Roth and R. Osborne, 1993.
"Parsing the Wall Street Journal with the Inside-OutsideAlgorithm", Proceedings EA CL '93, Utrecht.S.
Sekine and R. Grishman, 1995.
"A Corpus-based Probabilistic Grammar with Only Two Non-terminals",Proceedings Fourth International Workshop on Parsing Technologies, Prague.K.
Sima'an, 1995.
"An optimized algorithm for Data Oriented Parsing", Proceedings InternationalConference on Recent Advances in Natural Language Processing, Tzigov Chark, Bulgaria.K.
Sima'an, 1996.
"An optimized algorithm for Data Oriented Parsing", in R. Mitkov and N. Nicolov (eds.
),Recent Advances in Natural Language Processing 1995, volume 136 of Current Issues in Linguistic Theory.John Benjamins, Amsterdam.R.
Weischedel, M. Meteer, R, Schwarz, L. Ramshaw and J. Palmucci, 1993.
"Coping with Ambiguity andUnknown Words through Probabilistic Models", Computational Linguistics, 19(2), 359-382.140
