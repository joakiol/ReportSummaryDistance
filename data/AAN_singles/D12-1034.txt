Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 368?378, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsWhy Question Answering using Sentiment Analysis and Word ClassesJong-Hoon Oh?
Kentaro Torisawa?
Chikara Hashimoto ?Takuya Kawada?
Stijn De Saeger?
Jun?ichi Kazama?
Yiou Wang?
?Information Analysis LaboratoryUniversal Communication Research InstituteNational Institute of Information and Communications Technology (NICT){?
rovellia,?
torisawa,?
ch,?
tkawada,?stijn,?
kazama,?
?wangyiou}@nict.go.jpAbstractIn this paper we explore the utility of sen-timent analysis and semantic word classesfor improving why-question answering on alarge-scale web corpus.
Our work is moti-vated by the observation that a why-questionand its answer often follow the pattern that ifsomething undesirable happens, the reason isalso often something undesirable, and if some-thing desirable happens, the reason is also of-ten something desirable.
To the best of ourknowledge, this is the first work that intro-duces sentiment analysis to non-factoid ques-tion answering.
We combine this simple ideawith semantic word classes for ranking an-swers to why-questions and show that on a setof 850 why-questions our method gains 15.2%improvement in precision at the top-1 answerover a baseline state-of-the-art QA system thatachieved the best performance in a shared taskof Japanese non-factoid QA in NTCIR-6.1 IntroductionQuestion Answering (QA) research for factoid ques-tions has recently achieved great success as demon-strated by IBM?s Watson at Jeopardy: its accuracyhas been reported to be around 85% on factoid ques-tions (Ferrucci et al2010).
Although recent sharedQA tasks (Voorhees, 2004; Pe?as et al2011; Fuku-moto et al2007) have stimulated the research com-munity to move beyond factoid QA, comparativelylittle attention has been paid to QA for non-factoidquestions such as why questions and how to ques-tions, and the performance of the state-of-art non-factoid QA systems reported in the literature (Mu-rata et al2007; Surdeanu et al2011; Verberne etal., 2010) remains considerably lower than that offactoid QA (i.e., 34% in MRR at top-150 results onwhy-questions (Verberne et al2010)).In this paper we explore the utility of sentimentanalysis (Pang et al2002; Turney, 2002; Nakagawaet al2010) and semantic word classes for improv-ing why-question answering (why-QA) on a large-scale web corpus.
The inspiration behind this workis the observation that why-questions and their an-swers often have the following tendency:?
if something undesirable happens, the reason isoften also something undesirable, and?
if something desirable happens, its reason is of-ten also desirable.Consider the following question Q1, and its an-swer candidates A1-1 and A1-2.?
Q1: Why does cancer occur??
A1-1: Carcinogens such as nitrosamine andbenzopyrene may increase the risk of cancer byaltering DNA in cells.?
A1-2: Maintaining a healthy weight may lowerthe risk of various types of cancer.Here A1-1 describes an undesirable event related tocancer, while A1-2 suggests a desirable action forits prevention.
Our hypothesis suggests that A1-1is more appropriate for answering Q1.
If this hy-pothesis holds, we can obtain a significant improve-ment in performance on why-QA tasks by exploitingthe sentiment orientation1 of expressions obtainable1 In this paper we denote the desirable/undesirable polar-ity of an expression by the term ?sentiment orientation?
insteadof ?semantic orientation?
to avoid confusion with our differentnotion of ?semantic word classes.
?368by automatic sentiment analysis of questions and an-swers.A second observation motivating this work is thatthere are often significant associations between thelexico-semantic classes of words in a question andthose in its answer sentence.
For instance, questionsconcerning diseases like Q1 often have answers thatinclude references to specific semantic word classessuch as chemicals (like A1-1), viruses, body parts,and so on.
Capturing such statistical correlations be-tween diseases and harmful substances may lead tohigher why-QA performance.
For this purpose weuse classes of semantically similar words that wereautomatically acquired from a large web corpus us-ing an EM-based clustering method (Kazama andTorisawa, 2008).Another issue is that simply introducing the sen-timent orientation of words or phrases in questionand answer sentences in a naive way is insufficient,since answer candidate sentences may contain mul-tiple sentiment expressions with different polaritiesin answer candidates (i.e., about 33% of correct an-swers had such multiple sentiment expressions withdifferent polarities in our test set).
For example, ifA1-2 contained a second sentiment expression withnegative polarity like the example below,?Trusting a specific food is not effectivefor preventing cancer, but maintaining ahealthy weight may help lower the risk ofvarious types of cancer.
?both A1-1 and A1-2 would contain sentiment ex-pressions with the same polarity as that of Q1.
Thus,it is difficult to expect that the sentiment orientationalone will work well for recognizing A1-1 as a cor-rect answer to Q1.
To address this problem, we con-sider the combination of sentiment polarity and thecontents of sentiment expressions associated withthe polarity in questions and their answer candidatesas well.
To deal with the data sparseness problemarising in using the content of sentiment expressions,we developed a feature set that combines the polar-ity and the semantic word classes effectively.We exploit these two main ideas (concerned withthe sentiment orientation and the semantic classesdescribed so far) for training a supervised classi-fier to rank answer candidates to why-questions.Through a series of experiments on 850 Japanesewhy-questions, we showed that the proposed seman-tic features were effective in identifying correct an-swers, and our proposed method obtained more than15% improvement in precision of its top answer(P@1) over our baseline, which achieved the bestperformance in the non-factoid QA task in NTCIR-6 (Murata et al2007).
We also show that ourmethod can potentially perform with high precision(64.8% in P@1) when answer candidates containingat least one correct answer are given to our re-ranker.2 ApproachOur proposed method is composed of answer re-trieval and answer re-ranking.
The first step, an-swer retrieval, extracts a set of answer candidates toa why-question from 600 million Japanese Web cor-pus.
The answer retrieval is our implementation ofthe state-of-art method that has shown the best per-formance in the shared task of Japanese non-factoidQA in NTCIR-6 (Murata et al2007; Fukumoto etal., 2007).
The second step, answer re-ranking, isthe focus of this work.2.1 Answer RetrievalWe use Solr2 to retrieve documents from a 600 mil-lion Japanese Web page corpus3for a given why-question.
Let a set of content words in a why-question be T = {t1, ?
?
?
, tn}.
Two boolean queriesfor a why-question, ?t1 AND ?
?
?
AND tn?
and ?t1OR ?
?
?
OR tn,?
are given to Solr and top-300 doc-uments for each query are retrieved.
Note that re-trieved documents by each query have different cov-erage and relevance to a given why-question.
Tokeep balance between the coverage and relevance ofretrieved documents, we use a set of retrieved doc-uments by these two queries for obtaining answercandidates.
Each document in the result of docu-ment retrieval is split into a set of answer candi-dates consisting of five subsequent sentences4.
Sub-sequent answer candidates can share up to two sen-tences to avoid errors due to wrong document seg-mentation.2 http://lucene.apache.org/solr3 To the best of our knowledge, few Japanese non-factoidQA systems in the literature have used such a large-scale cor-pus.4 The length of acceptable answer candidates for why-QA in the literature ranges from one sentence to two para-graphs (Fukumoto et al2007; Murata et al2007; Higashinakaand Isozaki, 2008; Verberne et al2007; Verberne et al2010).369Answer candidate ac for question q is rankedaccording to scoring function S(q, ac) given inEq.
(1) (Murata et al2007).
Murata et al2007)?smethod uses text search to look for answer candi-dates containing terms from the question with ad-ditional clue terms referring to ?reason?
or ?cause.
?Following the original method we used riyuu (rea-son), genin (cause) and youin (cause) as clue terms.The top-20 answer candidates for each question arepassed on to the next step, which is answer re-ranking.
S(q, ac) assigns a score to answer candi-dates like tf -idf , where 1/dist(t1, t2) functions liketf and 1/df(t2) is idf for given terms t1 and t2 thatare shared by q and ac.S(q, ac) = maxt1?T?t2?T??
log(ts(t1, t2)) (1)ts(t1, t2) =N2?
dist(t1, t2)?
df(t2)Here T is a set of terms including nouns, verbs, andadjectives in question q that appear in answer can-didate ac.
Note that the clue terms are added to Tif they exist in ac.
N is the total number of docu-ments (600 million), dist(t1, t2) represents the dis-tance (the number of characters) between t1 and t2in answer candidate ac, df(t) is the document fre-quency of term t, and ?
?
{0, 1} is an indicator,where ?
= 1 if ts(t1, t2) > 1, ?
= 0 otherwise.2.2 Answer Re-rankingOur re-ranker is a supervised classifier (SVMs)(Vapnik, 1995) that uses three types of featuresets: features expressing morphological and syn-tactic analysis of questions and answer candidates,features representing semantic word classes appear-ing in questions and answer candidates, and featuresfrom sentiment analysis.
All answer candidates of aquestion are ranked in a descending order of theirscore given by SVMs.
We trained and tested there-ranker using 10-fold cross validation on a cor-pus composed of 850 why-questions and their top-20 answer candidates provided by the answer re-trieval procedure in Section 2.1.
The answer candi-dates were manually annotated by three human an-notators (not by the authors).
Our corpus construc-tion method is described in more detail in Section 4.3 Features for Answer Re-rankingThis section describes our feature sets for answerre-ranking: features expressing morphological andsyntactic analysis (MSA), features representing se-mantic word class (SWC), and features indicat-ing sentiment analysis (SA).
MSA, which has beenwidely used for re-ranking answers in the literature,is used to identify associations between questionsand answers at the morpheme, word phrase, and syn-tactic dependency levels.
The other two feature setsare proposed in this paper.
SWC is devised for iden-tifying semantic word class associations betweenquestions and answers.
SA is used for identify-ing sentiment orientation associations between ques-tions and answers as well as expressing the combi-nation of each sentiment expression and its polarity.Table 1 summarizes the respective feature sets, eachof which is described in detail below.3.1 Morphological and Syntactic AnalysisMSA including n-grams of morphemes, words, andsyntactic dependencies has been widely used for re-ranking answers in non-factoid QA (Higashinakaand Isozaki, 2008; Surdeanu et al2011; Verberneet al2007; Verberne et al2010).
We use MSA asa baseline feature set in this work.We represent all sentences in a question andits answer candidate in three ways: morphemes,word phrases (bunsetsu5) and syntactic dependencychains.
These are obtained using a morphologicalanalyzer6 and a dependency parser7.
From eachquestion and answer candidate we extract n-gramsof morphemes, word phrases, and syntactic depen-dencies, where n ranges from 1 to 3.
Syntactic de-pendency n-grams are defined as a syntactic depen-dency chain containing n word phrases.
Syntacticdependency 1-grams coincide with word phrase 1-grams, so they are ignored.Table 1 defines four types of MSA (MSA1 toMSA4).
MSA1 is n-gram features from all sen-tences in a question and its answer candidates anddistinguishes an n-gram feature found in a ques-tion from that same feature found in answer candi-dates.
MSA2 contains n-grams found in the answer5 A bunsetsu is a syntactic constituent composed of a contentword and several function words such as post-positions and casemarkers.
It is the smallest unit of syntactic analysis in Japanese.6 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN7 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KNP370MSA1 Morpheme n-grams, word phrase n-grams, and syntactic dependency n-grams in a question and its answer candidate, where n rangesfrom 1 to 3. n-grams in a question and those in an answer candidate are distinguished.MSA2 MSA1?s n-grams in an answer candidate that contain a question term.MSA3 MSA1?s n-grams that contain a clue term including riyuu (reason), genin (cause) and youin (cause).
These n-grams in a question andthose in an answer candidate are distinguished.MSA4 The ratio of the number of question terms in an answer candidate to the total number of question terms.SWC1 Word class n-grams in a question and its answer candidate.
These n-grams in a question and those in an answer candidate are distin-guished.SWC2 SWC1?s n-grams in an answer candidate whose original MSA1?s n-grams contain any question term.SA@W1 Word polarity n-grams in a question and its answer candidate.
These n-grams in a question and those in an answer candidate aredistinguished.SA@W2 SA@W1?s n-grams in an answer candidate whose original MSA1 n-grams contain any question term.SA@W3 Joint class-polarity n-grams in a question and its answer candidate.
These n-grams in a question and those in an answer candidate aredistinguished.SA@W4 SA@W3?s n-grams in an answer candidates whose original MSA1 n-grams contain any question term.SA@P1 The indicator for polarity agreement between sentiment phrases, one in a question and the other in an answer candidate: 1 if any pair ofsuch sentiment phrases has polarity in agreement, 0 otherwise.SA@P2 The phrase-polarity, positive or negative, of a pair of sentiment phrases for which the indicator in SA@P1 is 1.SA@P3 Morpheme n-grams, word phrase n-grams, and syntactic dependency n-grams in sentiment phrases are coupled with their phrase-polarity,where n ranges from 1 to 3.
These n-grams in a question and those in an answer candidate are distinguished.SA@P4 SA@P3?s n-grams in an answer candidates that contain a question term.SA@P5 The ratio of the number of question terms in sentences that have sentiment phrases in answer candidates to the total number of questionterms.SA@P6 Word class n-grams in sentiment phrases are coupled with phrase-polarity.
These n-grams in a question and those in an answer candidateare distinguished.SA@P7 SA@P6?s n-grams in an answer candidates, whose original MSA1?s n-grams include any question term.SA@P8 Joint class-polarity n-grams in sentiment phrases of a question and its answer candidate are coupled with phrase-polarity of the sentimentphrases.
These n-grams in a question and those in an answer candidate are distinguished.SA@P9 SA@P8?s n-grams in an answer candidates, whose original MSA1?s n-grams include any question term.SA@P10 A pair of SA@P6?s n-grams, one from sentiment phrases in a question and the other from those in an answer candidate, where the twosentiment phrases should have the same sentiment orientation.Table 1: Features used in training our re-rankercandidates that themselves contain a term from thequestion (e.g., ?types of cancer?
in example A1-2).MSA3 is the n-gram feature that contains one of theclue terms used for answer retrieval (riyuu (reason),genin (cause) or youin (cause)).
Here too, n-gramsobtained from the questions and answer candidatesare distinguished.
Finally, MSA4 is the percentageof the question terms found in an answer candidate.3.2 Semantic Word ClassSemantic word classes are sets of semantically simi-lar words.
We construct these semantic word classesby using the noun clustering algorithm proposed inKazama and Torisawa (2008).
The algorithm fol-lows the distributional hypothesis, which states thatsemantically similar words tend to appear in simi-lar contexts (Harris, 1954).
By treating syntactic de-pendency relations between words as ?contexts,?
theclustering method defines a probabilistic model ofnoun-verb dependencies with hidden classes as:p(n, v, r) =?cp(n|c)p(?v, r?|c)p(c) (2)Here, n is a noun, v is a verb or noun on which n de-pends via a grammatical relation r (post-positions inJapanese), and c is a hidden class.
Dependency rela-tion frequencies were obtained from our 600-millionpage web corpus, and model parameters p(n|c),p(?v, r?|c) and p(c) were estimated using the EMalgorithm (Hofmann, 1999).
We successfully clus-tered 5.5 million nouns into 500 classes.
For eachnoun n, EM clustering estimates a probability dis-tribution over hidden variables representing seman-tic classes.
From this distribution we obtained dis-crete semantic word classes by assigning each nounn to semantic class c = argmaxc?
p(c?|n).
Theresulting classes actually form clean semantic cat-egories such as chemicals, nutrients, diseases andconditions, in our examples of Q1 and Q2.
The fol-lowing are the top-10 words (English translation) ac-cording to p(c|n) for these classes.chemicals: acetylene, hydrogenation product,phosphoric monoester, methacrylate, levoglu-cosan, ammonium salt, halogenated organiccompound, benzonitrile, alkyne, nitrosamine371nutrients: glucide, carbonhydrate, mineral, salt,sugar, water, fat, vitamin, nutrients, proteindiseases: pneumonia, neuritis, cancer, oral leuko-plakia, pachymeningitis, acidosis, encephalitis,abdominal injury, valvulitis, gingivitisconditions: proficiency, decrepitude, deficiency,impurity, abnormalities, floated, crisis, dis-placement, condition, shortageSemantic word class (SWC) features are used tocapture associations between semantic classes ofwords in the question and those in the answer candi-dates.
For example:?
Q2: Why does rickets (Wdisease) occur in chil-dren??
A2: Deficiency (Wcondition) of vitamin D(Wnutrients) can cause rickets (Wdisease).Wcondition, Wdisease and Wnutrients represent se-mantic word classes of conditions, diseases and nu-trients, respectively.
If this question-answer pair isgiven to the classifier as a positive training sample,we expect it to learn that if a disease name appearsin a question then, everything else being equal, an-swers including nutrient names are more likely to becorrect.
Note that in principle the same associationcould be learned between word pairs, i.e., rickets andvitamin D. However, we found that word level asso-ciations are often too specific, and because of datasparseness this knowledge cannot easily be general-ized to unseen questions.
This is our main motiva-tion for introducing broad coverage semantic wordclasses into the feature set.We call the feature set with the word classes SWCand use two types of SWC, as shown in Table 1.
Toobtain the first type (SWC1), we convert all nounsin the MSA1 n-grams into their respective wordclasses, and keep all n-grams that contain at leastone word class.
We call these features word classn-grams.
Again, word class n-grams obtained fromquestions are distinguished from the ones in answercandidates.
For example, we extract ?Wdisease oc-cur?
as a word class 2-gram from Q2.The second type of SWC, SWC2, represents wordclass n-grams in an answer candidate, in whichquestion terms are replaced by their respective se-mantic word classes.
For example, Wdisease in wordclass 2-gram ?cause Wdisease?
from A2 is the se-mantic word class of rickets, one of the questionterms.
These features capture the correspondencebetween semantic word classes in the question andanswer candidates.3.3 Sentiment AnalysisSentiment analysis (SA) features are classified intoword-polarity and phrase-polarity features.
We useopinion extraction tool8 and sentiment orientationlexicon in the tool for these features.3.3.1 Opinion Extraction ToolOpinion extraction tool is a software, the im-plementation of Nakagawa et al2010).
It ex-tracts linguistic expressions representing opinions(henceforth, we call them sentiment phrases) froma Japanese sentence and then identifies the polarityof these sentiment phrases using machine learningtechniques.
For example, rickets occur in Q2 andDeficiency of vitamin D can cause rickets in A2 canbe identified as sentiment phrases with a negativepolarity.
The tool identifies sentiment phrases andtheir polarity by using polarities of words and de-pendency subtrees as evidence, where these polari-ties are given in a word polarity dictionary.In this paper, we use a trained model and a wordpolarity dictionary (containing about 35,000 entries)distributed via the ALAGIN forum9 for our sen-timent analysis.
Table 2 shows the performanceof opinion extraction tool, precision (P), recall (R)and F-value (F), in this setting (reported in theJapanese homepage of this tool).
In the evaluation ofsentiment-phrase extraction, an extracted sentimentphrase is determined as correct if its head word isthe same as one in the gold standard.
Polarity clas-sification is evaluated under the condition that all ofthe sentiment phrases are correctly extracted.P R FSentiment-phrase extraction 0.602 0.408 0.486Polarity classification (pos.)
0.873 0.893 0.883Polarity classification (neg.)
0.866 0.842 0.854Table 2: The performance of opinion extraction tool3.3.2 Word Polarity (SA@W)Polarities of words are identified by simply look-ing up the word polarity dictionary of opinion ex-8 Available at http://alaginrc.nict.go.jp/opinion/index_e.html9 http://www.alagin.jp/index-e.html.
Only the members ofthe ALAGIN forum can access these resources.372traction tool.
Word polarity features are usedfor identifying associations between the polarity ofwords in a question and that in a correct answer.
Forexample:?
Q2: Why does rickets (W?)
occur in children??
A2: Deficiency (W?)
of vitamin D can causerickets (W?
).Here, W?
represents negative word polarities.
Weexpect our classifier to learn from this question andanswer pair that if a word with negative polarity ap-pears in a question then its correct answer is likelyto contain a negative polarity word as well.SA@W1 and SA@W2 in Table 1 are sentimentanalysis features from word polarity n-grams, whichcontain at least one word that has word polarities.We obtain these n-grams by converting all nouns inMSA1 n-grams into their word polarities throughdictionary lookup.
For example, from Q2 in theabove example we extract ?W?
occur?
as a wordpolarity 2-gram.
SA@W1 is concerned with allword polarity n-grams in questions and answer can-didates.
For SA@W2, we restrict word polarityn-grams from SA@W1 to those whose original n-gram include a question term.Furthermore, word polarities are coupled with se-mantic word classes so that our classifier can iden-tify meaningful combinations of both.
For example,deficiency in A2 can be represented asW?condition byits respective semantic word class and word polar-ity, which allows for the representation of undesir-able conditions.
This in turn lets our system learnmeaningful correlations between words expressingthese kind of negative conditions and their connec-tion to questions asking about diseases.
SA@W3and SA@W4 are features from this combination.They are defined in the same way as SA@W1 andSA@W2 except that word polarities are replacedwith the combination of semantic word classes andword polarities.
We call n-grams in SA@W3 andSA@W4 joint (word) class-polarity n-grams.3.3.3 Phrase Polarity (SA@P)Opinion extraction tool is applied to question andits answer candidate to identify sentiment phrasesand their phrase-polarities.
In preliminary tests wefound that sentiment phrases do not help to iden-tify correct answers if answer sentences includingthe sentiment phrases do not have any term from thequestion.
So we restrict the target sentiment phrasesto those acquired from sentences containing at leastone question term.
From these sentiment phrases weextract three categories of features.First, SA@P1 and SA@P2 are features concernedwith phrase-polarity agreement between sentimentphrases in a question and its answer candidate.
Weconsider all possible pairs of sentiment phrases fromthe question and answer.
If any such pair agreesin phrase-polarity, an indicator for the agreementand its polarity in the agreement become featuresSA@P1 and SA@P2, respectively.Secondly, following the original hypothesis un-derlying this paper, we assume that sentimentphrases often represent the core part of the cor-rect answer (e.g., A2 above) and it is importantto express the content of the sentiment phrases infeatures.
SA@P3 and SA@P4 were devised forthis purpose.
SA@P3 represents this sentimentphrase contents as n-grams of morphemes, words,and syntactic dependencies of sentiment phrases,together with their phrase-polarity.
Furthermore,SA@P4 is the subset of SA@P3 n-grams restrictedto those that include terms found in the question,and SA@P5 indicates the percentage of sentimentn-grams from the question that are found in a givenanswer candidate.Finally, features SA@P6 through SA@P9 use se-mantic word classes to generalize the content fea-tures mentioned above.
These features consist ofword class n-grams and joint class-polarity n-gramstaken from sentiment phrases, together with theirphrase polarity.
Similar to the definition of SA@P4,for SA@P7 and SA@P9 we restrict ourselves to n-grams containing a question term.
SA@P10 repre-sents the semantic content of two sentiment phraseswith the same sentiment orientation (one from aquestion and the other from an answer candidate)using word class n-grams, together with the phrase-polarity in agreement.4 Test SetWe prepared three sets of why-questions (QS1, QS2and QS3) and used these questions to build two testsets for our experiments.Why-questions in QS1 are taken from theJapanese version of Yahoo!
Answers (called Ya-hoo!
Chiebukuro)10.
We automatically extracted10 We used ?Yahoo!
Chiebukuro Data (2nd edition)?
which is373questions consisting of a single sentence and con-taining the interrogative naze (why), and our anno-tators verified that these questions are meaningfulwithout further context.
For example, they discardedquestions like ?Why doesn?t the WBC (world box-ing council) make an objection to the WBC (Worldbaseball classic)??
(the object of the objection isunclear) and ?Why do minors trade at the auctioneven though it is disallowed by the rules?
(informa-tion about which auction is not provided).Because questions in Yahoo!
Answers are aimedat human readers, users often ?set the stage?
by giv-ing lots of background information about their ques-tion.
This often leads to large stylistic differencesbetween the questions in Yahoo!
Answers and thosetypically posed to a QA system.
We therefore cre-ated a second set of why-questions, QS2, whosestyle should be more appropriate for a QA system(examples showing these differences are given in thesupplementary materials of this paper).
Six humanannotators (not the authors) were asked to createwhy-questions in their own words, keeping in mindthat the questions they create are for a QA system.
Inaddition, the annotators were asked to verify on theWeb that the questions they created ask about somereal event or phenomena.
For example, a questionlike ?Why does Mars appear blue??
is disallowed inQS2 because ?Mars appears blue?
is false.
Note thatthe correct answer to these questions does not haveto be either in our target corpus or in real-world Webtexts.
These two sets of why-questions, QS1 andQS2, are used to build a test set for evaluating ourproposed method.Finally, QS3 contains why-questions that have atleast one answer in our target corpus (600 millionJapanese Web page corpus).
For creating such why-questions, four human annotators (not the authors)were given a text passage composed of three contin-uous sentences and asked to locate the reasons forsome event as described in this passage.
Then theycreated a why-question for which the description is acorrect answer.
Because randomly selected passagesfrom our target corpus have little chance of generat-ing good why-questions we extracted passages fromour target corpus that include at least one of the clueterms used in our answer retrieval step (i.e.
riyuu(reason), genin (cause), or youin (cause)).
This set-provided by Yahoo Japan Corporation and contains 16 millionquestions asked from April, 2004 to April 2009.ting may not necessarily reflect a ?real world?
dis-tribution of why-questions, in which ideally a widerange of people ask questions that may or may nothave an answer in our corpus.
However, QS3 al-lows us to evaluate our method under the idealizedconditions where we have a perfect answer retrievalmodule whose answer candidates always contain atleast one correct answer (the source passage usedfor creating the why-question).
This setting allowsus to estimate the ideal-case performance of ourmethod.
Under these circumstances we found thatour method achieves almost 65% precision in P@1,which suggests that it can potentially perform withhigh precision if the answer candidates given by theanswer retrieval module contain at least one correctanswer.
This is the main purpose of QS3.
Addition-ally, we use QS3 for building training data, to checkwhether questions that do not reflect the real-worlddistribution of why-questions are useful for improv-ing the system?s performance on ?real-world?
ques-tions (see Section 5.1).In addition, we checked QS1, QS2 and QS3 forquestions having the same topic, to avoid the pos-sibility that the distribution of questions is biasedtowards certain topics.
We manually extracted thequestions?
topic words and randomly selected a sin-gle representative question from all questions withthe same topic.
For example, ?Why does Twitteronly allow 140 characters??
and ?Why is Twitterso popular??
both have as topic Twitter.
In the endwe obtained 250 questions in QS1, 250 questions inQS2 and 350 questions in QS3.For evaluation we prepared two test sets, Set1 andSet2.
Set1 contains question-answer pairs whosequestions are taken from QS1 and QS2.
In our ex-periment, we evaluate systems with 10-fold crossvalidation on Set1.
Set2 has question-answer pairswhose questions are from QS3.
Set2 is mainly usedfor estimating estimate the ideal-case performanceof our method with a perfect answer retrieval mod-ule.
Furthermore Set2 is used as additional trainingdata in evaluating systems with 10-fold cross vali-dation on Set1.
We used our answer retrieval sys-tem to obtain the top-20 answer candidates for eachquestion, and all question-answer (candidate) pairswere checked by three annotators, where their inter-rater agreement (Fleiss?
kappa) was 0.634, indicat-ing substantial agreement.
Finally, correct answersto each question were determined by majority vote.374Q1:???????????????????????????????????????????
(Why does the increase of greenhouse gases such as carbon dioxide in the atmosphere lead to a rise of ocean level?
)A1: ..
???????????????????????????????????????????????????????????????????????????????????????????????
...
??????????????????????????????????????????
(The burning of fossil fuels contributes to the increase of atmospheric concentrations of greenhouse gases and this makes the atmosphere absorb morethermal radiation.
As a result, Earth?s average surface temperature increases.
This is global warming.
...
There are warnings that the increase of seawater and melting of polar ice due to the global warming may cause sea-surface height to rise by 9?88 cm on average.Q2:?????????????????????????????
(Why does hemoglobin deficiency cause lack of oxygen in the human body?)A2:...
?????????????????????????????????????????????????????????????????????????????????????????????????????????????????..(...
Hemoglobin has an important role in the human body of carrying oxygen to the organs and transferring carbon dioxide back to the lungs, to bedispensed from the organism.
If the amount of hemoglobin produced by the body is insufficient due to iron deficiency, the amount of oxygen deliveredthroughout the body decreases, causing oxygen deficiency.
... )Table 3: Correct question-answer pairs in our test setTable 3 shows a sample of correct question-answerpairs in our test set.
Please see the supplementarymaterials of this paper for more examples.Note that word and phrase polarities are not con-sidered by the annotators in building our test setsand these polarities are automatically identified us-ing a word polarity dictionary and opinion extractiontool.
We confirmed that about 35% of questions and40% of answer candidates had at least one sentimentphrase by opinion extraction tool, and about 45% ofquestions and 85% of answer candidates containedat least one word having polarity by a word polaritydictionary.5 ExperimentsWe use TinySVM11 with a linear kernel for trainingour re-ranker.
Evaluation was done by P@1 (Pre-cision of the top answer) and MAP (Mean AveragePrecision).
P@1 measures how many questions havea correct top answer candidate.
MAP, widely used inevaluation of IR systems, measures the overall qual-ity of the top-n answer candidates (n=20 in this ex-periment) using the formula:MAP =1|Q|?q?Q?nk=1(Prec(k)?
rel(k))|Aq|(3)Here Q is a set of why-questions, Aq is a set of cor-rect answers to why-question q ?
Q, Prec(k) is theprecision at cut-off k in the top-n answer candidates,rel(k) is an indicator, 1 if the item at rank k is a cor-rect answer in Aq, 0 otherwise.We evaluated all systems using 10-fold cross val-idation in two ways.
In the first setting we per-formed 10-fold cross validation on Set1.
Set1 con-11 http://chasen.org/?taku/software/TinySVM/sists of 10,000 question-answer pairs (500 questionswith their 20 answer candidates), and was parti-tioned into 10 subsamples such that the questionsin one subsample do not overlap with those of theother subsamples.
9 subsamples (9,000 question-answer pairs) were used as training data and theremaining subsample (1,000 question-answer pairs)was retained as test data.
This experiment is calledCV(Set1).
It shows the effect of answer re-rankingwhen evaluating our proposed method with train-ing data built with real world why-questions alone.In the second setting, we used the same 10 sub-samples of Set1 as in CV(Set1) and exploited Set2(composed of 7,000 question-answer pairs) as ad-ditional training data for 10-fold cross validation.As a result, in each fold 16,000 question-answerpairs (9,000 from Set1 and 7,000 from Set2) wereused as training data for re-rankers, and all systemswere evaluated on the remaining 1,000 question-answer pair subsample from Set1.
We call this set-ting CV(Set1+Set2).
It verifies whether trainingdata that does not necessarily reflect a real-worlddistribution of why-questions can improve why-QAperformance on real-world questions.5.1 ResultsTable 4 shows the evaluation results of six differentsystems.
For each system, we represent the perfor-mance in P@1 and MAP.
B-QA is a system of ouranswer retrieval and the other five re-rank top-20 an-swer candidates using their own re-ranker.B-QA: our answer retrieval system, our implemen-tation of Murata et al2007).B-Ranker: a system that has a re-ranker trainedwith morphological and syntactic analysis(MSA) features alone.375System CV(Set1) CV(Set1+Set2)P@1 MAP P@1 MAPB-QA 0.222 (0.368) 0.270 (0.447) 0.222 (0.368) 0.270 (0.447)B-Ranker 0.256 (0.424) 0.319 (0.528) 0.274 (0.454) 0.323 (0.535)B-Ranker+CR 0.262 (0.434) 0.319 (0.528) 0.278 (0.460) 0.325 (0.538)B-Ranker+WN 0.257 (0.425) 0.320 (0.530) 0.275 (0.455) 0.325 (0.538)Proposed 0.336 (0.56) 0.377 (0.624) 0.374 (0.619) 0.391 (0.647)UpperBound 0.604 (1) 0.604 (1) 0.604 (1) 0.604 (1)Table 4: Comparison of systemsB-Ranker+CR: a system has a re-ranker trainedwith our MSA features and the causal relation(CR) features used in Higashinaka and Isozaki(2008).
The CR features include binary fea-tures indicating whether an answer candidatecontains a causal relation pattern, which causalrelation pattern the answer candidate has, andwhether the question-answer pair contains acausal relation instance ?
cause in the answer,effect in the question).
We acquired causalrelation instances from our target corpus us-ing the method from (De Saeger et al2009),and exploited the top-100,000 causal relationinstances and the patterns that extracted themfor CR features.
Note that these CR featuresare introduced only for comparing our semanticfeatures with ones in Higashinaka and Isozaki(2008) and they are not a part of our method.B-Ranker+WN: its re-ranker is trained with ourMSA features and the WordNet features in Ver-berne et al2010).
The WordNet features in-clude the percentage of the question terms andtheir synonyms in WordNet synsets found inan answer candidate and the semantic related-ness score between a question and its answercandidate, the average of the concept similar-ity between each question term and all of theanswer terms by WordNet::Similarity (Peder-sen et al2004).
We used the Japanese Word-Net 1.1 (Bond et al2009) for these WordNetfeatures.
Note that the Japanese WordNet 1.1has 93,834 Japanese words linked to 57,238WordNet synsets, while the English WordNet3.0 covers 155,287 words linked to 117,659synsets.
Due to this lower coverage, the Word-Net features in Japanese may have a less powerfor finding a correct answer than those in En-glish used in Verberne et al2010).Proposed: our proposed method.
All of the MSA,SWC and SA features are used for training ourre-ranker.UpperBound: a system that ranks all n correct an-swers as the top n results of the 20 answer can-didates if there are any.
This indicates the per-formance upperbound in this experiment.
Therelative performance of each system comparedto UpperBound is shown in parentheses.The proposed method achieved the best perfor-mance both in CV(Set1) and CV(Set1+Set2).
Ourmethod shows a significant improvement (11.4?15.2% in P@1 and 10.7?12.1% in MAP) over ouranswer retrieval method, B-QA.
Its improvementover B-Ranker, B-Ranker+CR and B-Ranker+WN(7.6?10% in P@1 and 5.7?6.6% in MAP) showsthe effectiveness of our proposed feature set overthe features used in previous works.
Both B-Ranker+CR and B-Ranker+WN did not show signif-icant performance improvement over B-Ranker.
Atleast in our setting, the causal relation and WordNetfeatures did not prove effective.
The performancegap between B-Ranker and B-QA (3.4?5.2% in P@1and 4.9?5.3% in MAP) suggests the effectivenessof re-ranking.
All systems consistently show betterperformance in CV(Set1+Set2) than CV(Set1).
Thissuggests that training data built with why-questionsthat does not reflect real-world distribution of why-questions is useful in training re-rankers.We investigate the contribution of each type offeatures to the performance by removing one fea-ture set from the all feature sets in training our re-ranker.
In this experiment, we split SA into SA@W(features expressing words and their polarity) andSA@P (features expressing phrases and their po-larity) to investigate their contribution either.
Theresults are summarized in Table 5.In Table 5, MSA+SWC+SA represents our pro-posed method using all feature sets.
The perfor-mance gap between MSA+SWC+SA and the othersconfirms that all the features contributed to a higher376System CV(Set1) CV(Set1+Set2)P@1 MAP P@1 MAPSWC+SA 0.302 0.324 0.314 0.332MSA+SWC 0.308 0.349 0.318 0.358MSA+SA 0.300 0.352 0.314 0.364MSA+SWC+SA@W 0.312 0.358 0.325 0.365MSA+SWC+SA@P 0.323 0.369 0.358 0.384MSA+SWC+SA 0.336 0.377 0.374 0.391UpperBound 0.604 0.604 0.604 0.604Table 5: Evaluation with different combination of featuresets used in training our re-rankerperformance.
The significant performance improve-ment by SA (features from sentiment analysis) andSWC (features from semantic word classes) (Thegap between MSA+SWC+SA and MSA+SWC was2.8?6% and that between MSA+SWC+SA andMSA+SA was 3.6%?6% in P@1) supports the hy-pothesis for sentiment analysis and semantic wordclasses in this paper.Though the performance gap betweenMSA+SWC+SA and MSA+SWC+SA@P(1.3%?1.6% in P@1) shows that SA@W isuseful in training our re-ranker, we found thatMSA+SWC+SA@W made only 0.4?0.7% im-provement over MSA+SWC.
We believe that thisis mainly because SA@W and SWC are based onsemantic and sentiment information at the wordlevel, and these often capture a similar type ofinformation.
For instance, disease names that aregrouped together into one class in SWC are typi-cally classified as negative in SA@W. Therefore thesimilarity in the information provided by SA@Wand SWC causes a classifier trained with both ofthese features to obtain only a minor improvementover a classifier using only one of the features.To estimate the ideal-case performance of ourproposed method, we made another experiment byusing Set1 as training data for our re-ranker andSet2 as test data for evaluating our proposed method.Here, we assume a perfect answer retrieval modulethat adds the source passage that was used for gener-ating the original why-question in Set2 as a correctanswer to the set of existing answer candidates, giv-ing 21 answer candidates.
The performance of ourmethod in this setting was 64.8% in P@1 and 66.6%in MAP.
This evaluation result suggests that our re-ranker can potentially perform with high precisionwhen at least one correct answer in answer candi-dates is given by the answer retrieval module.6 Related WorkIn the QA literature, Higashinaka and Isozaki(2008), Verberne et al2010), and Surdeanu et al(2011) are closest to our work.
The first two dealwith why-questions, the last with how-questions.Similar to our method, they use machine learn-ing techniques to re-rank answer candidates to non-factoid questions based on various combinations ofsyntactic, semantic and other statistical features suchas the density and frequency of question terms in theanswer candidates and patterns for causal relationsin the answer candidates.
Especially for why-QA,Higashinaka and Isozaki (2008) used causal relationfeatures and Verberne et al2010) exploited Word-Net features as a kind of semantic features for train-ing their re-ranker, where we used these features, re-spectively, for B-Ranker+CR and B-Ranker+WN inour experiment.Our work differs from the above approaches inthat we propose semantic word classes and senti-ment analysis as a new type of semantic features,and show their usefulness in why-QA.
Sentimentanalysis has been used before on the slightly un-usual task of opinion question answering, where thesystem is asked to answer subjective opinion ques-tions (Stoyanov et al2005; Dang, 2008; Li et al2009).
To the best of our knowledge though, no pre-vious work has systematically explored the use ofsentiment analysis in a general QA setting beyondopinion questions.7 ConclusionIn this paper, we have explored the utility of senti-ment analysis and semantic word classes for rankinganswer candidates to why-questions.
We proposed aset of semantic features that exploit sentiment anal-ysis and semantic word classes obtained from large-scale noun clustering, and used them to train an an-swer candidate re-ranker.
Through a series of exper-iments on 850 why-questions, we showed that theproposed semantic features were effective in identi-fying correct answers, and our proposed method ob-tained more than 15% improvement in precision ofits top answer (P@1) over our baseline, a state-of-the-art IR based QA system.
We plan to use new se-mantic knowledge such as semantic orientation, ex-citatory or inhibitory, proposed in Hashimoto et al(2012) for improving why-QA.377ReferencesFrancis Bond, Hitoshi Isahara, Sanae Fujita, KiyotakaUchimoto, Takayuki Kuribayashi, and Kyoko Kan-zaki.
2009.
Enhancing the japanese wordnet.
In Pro-ceedings of the 7th Workshop on Asian Language Re-sources, pages 1?8.Hoa Tran Dang.
2008.
Overview of the TAC 2008 opin-ion question answering and summarization tasks.
InProc.
TAC 2008.Stijn De Saeger, Kentaro Torisawa, Jun?ichi Kazama,Kow Kuroda, and Masaki Murata.
2009.
Large scalerelation acquisition using class dependent patterns.
InProc.
of ICDM 2009, pages 764?769.David A. Ferrucci, Eric W. Brown, Jennifer Chu-Carroll,James Fan, David Gondek, Aditya Kalyanpur, AdamLally, J. William Murdock, Eric Nyberg, John M.Prager, Nico Schlaefer, and Christopher A. Welty.2010.
Building Watson: An overview of the DeepQAproject.
AI Magazine, 31(3):59?79.Junichi Fukumoto, Tsuneaki Kato, Fumito Masui, andTsunenori Mori.
2007.
An overview of the 4th ques-tion answering challenge (QAC-4) at NTCIR work-shop 6.
In Proc.
of NTCIR-6.Zellig Harris.
1954.
Distributional structure.
Word,10(23):146?162.Chikara Hashimoto, Kentaro Torisawa, Stijn De Saeger,Jong-Hoon Oh, and Jun?ichi Kazama.
2012.
Excita-tory or inhibitory: A new semantic orientation extractscontradiction and causality from the web.
In Proceed-ings of EMNLP-CoNLL 2012.Ryuichiro Higashinaka and Hideki Isozaki.
2008.Corpus-based question answering for why-questions.In Proc.
of IJCNLP, pages 418?425.Thomas Hofmann.
1999.
Probabilistic latent semanticindexing.
In Proc.
of the 22nd annual internationalACM SIGIR conference on Research and developmentin information retrieval, SIGIR ?99, pages 50?57.Jun?ichi Kazama and Kentaro Torisawa.
2008.
Inducinggazetteers for named entity recognition by large-scaleclustering of dependency relations.
In Proc.
of ACL-08: HLT, pages 407?415.Fangtao Li, Yang Tang, Minlie Huang, and XiaoyanZhu.
2009.
Answering opinion questions with ran-dom walks on graphs.
In Proc.
of the Joint Conferenceof the 47th Annual Meeting of the ACL and the 4thInternational Joint Conference on Natural LanguageProcessing of the AFNLP: Volume 2 - Volume 2, pages737?745.Masaki Murata, Sachiyo Tsukawaki, Toshiyuki Kana-maru, Qing Ma, and Hitoshi Isahara.
2007.
A systemfor answering non-factoid Japanese questions by usingpassage retrieval weighted based on type of answer.
InProc.
of NTCIR-6.Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.2010.
Dependency tree-based sentiment classificationusing CRFs with hidden variables.
In Human Lan-guage Technologies: The 2010 Annual Conference ofthe North American Chapter of the Association forComputational Linguistics, pages 786?794, Los An-geles, California, June.
Association for ComputationalLinguistics.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
Sentiment classification using ma-chine learning techniques.
In Proc.
of the 2002 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 79?86.Ted Pedersen, Siddharth Patwardhan, and Jason Miche-lizzi.
2004.
WordNet::Similarity: measuring therelatedness of concepts.
In Demonstration Papersat HLT-NAACL 2004, HLT-NAACL?Demonstrations?04, pages 38?41.Anselmo Pe?as, Eduard H. Hovy, Pamela Forner, ?lvaroRodrigo, Richard F. E. Sutcliffe, Corina Forascu, andCaroline Sporleder.
2011.
Overview of QA4MRE atCLEF 2011: Question answering for machine readingevaluation.
In CLEF.Veselin Stoyanov, Claire Cardie, and Janyce Wiebe.2005.
Multi-perspective question answering using theopqa corpus.
In Proceedings of the conference on Hu-man Language Technology and Empirical Methods inNatural Language Processing, HLT ?05, pages 923?930.Mihai Surdeanu, Massimiliano Ciaramita, and HugoZaragoza.
2011.
Learning to rank answers to non-factoid questions from web collections.
Computa-tional Linguistics, 37(2):351?383.Peter D. Turney.
2002.
Thumbs up or thumbs down?
:semantic orientation applied to unsupervised classifi-cation of reviews.
In Proc.
of the 40th Annual Meetingon Association for Computational Linguistics, ACL?02, pages 417?424.Vladimir N. Vapnik.
1995.
The nature of statisticallearning theory.
Springer-Verlag New York, Inc., NewYork, NY, USA.Suzan Verberne, Lou Boves, Nelleke Oostdijk, and Peter-Arno Coppen.
2007.
Evaluating discourse-based an-swer extraction for why-question answering.
In SIGIR,pages 735?736.Suzan Verberne, Lou Boves, Nelleke Oostdijk, and Peter-Arno Coppen.
2010.
What is not in the bag of wordsfor why-QA?
Computational Linguistics, 36:229?245.Ellen M. Voorhees.
2004.
Overview of the TREC 2004question answering track.
In TREC.378
