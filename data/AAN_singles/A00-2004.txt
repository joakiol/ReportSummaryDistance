Advances in domain independent linear text segmentat ionFreddy Y. Y.  Cho iArtificial Intell igence GroupDepartment  of Computer  ScienceUniversity of ManchesterManchester,  Englandchoif@cs.man.ac.ukAbst rac tThis paper describes a method for linear text seg-mentation which is twice as accurate and over seventimes as fast as the state-of-the-art (Reynar, 1998).Inter-sentence similarity is replaced by rank in thelocal context.
Boundary locations are discovered bydivisive clustering.1 In t roduct ionEven moderately ong documents typically addresssew~ral topics or different aspects of the same topic.The aim of linear text segmentation is to discoverthe topic boundaries.
The uses of this procedureinclude information retrieval (Hearst and Plaunt,1993; Hearst, 1994; Yaari, 1997; Reynar, 1999),summarization (Reynar, 1998), text understanding,anaphora resolution (Kozima, 1993), language mod-elling (Morris and Hirst, 1991; Beeferman et al,199717) and improving document navigation for thevisually disabled (Choi, 2000).This paper focuses on domain independent meth-ods for segmenting written text.
We present a newalgorithm that builds on previous work by Reynar(Reynar, 1998; Reynar, 1994).
The primary distinc-tion of our method is the use of a ranking schemeand the cosine similarity measure (van Rijsbergen,1979) in formulating the similarity matrix.
We pro-pose that the similarity values of short text segmentsis statistically insignificant.
Thus, one can only relyon their order, or rank, for clustering.2 BackgroundExisting work falls into one of two categories, lexicalcohesion methods and multi-source methods (Yaari,1997).
The former stem from the work of Hallidayand Hasan (Halliday and Hasan, 1976).
They pro-posed that text segments with similar vocabularyare likely to be part of a coherent opic segment.hnplementations of this idea use word stem repe-tition (Youmans, 1991; Reynar, 1994; Ponte andCroft, 1997), context vectors (Hearst, 1994; Yaar-i, 1997; Kaufmann, 1999; Eichmann et al, 1999),entity repetition (Kan et al, 1998), semantic simi-larity (Morris and Hirst, 1991; Kozima, 1993), worddistance model (Beeferman et al, 1997a) and wordfrequency model (Reynar, 1999) to detect cohesion.Methods for finding the topic boundaries include s-liding window (Hearst, 1994), lexical chains (Mor-ris, 1988; Kan et al, 1998), dynamic programming(Ponte and Croft, 1997; Heinonen, 1998), agglomer-ative clustering (Yaari, 1997) and divisive clustering(Reynar, 1994).
Lexical cohesion methods are typi-cally used for segmenting written text in a collectionto improve information retrieval (Hearst, 1994; Rey-nat, 1998).Multi-source methods combine lexical cohesionwith other indicators of topic shift such as cue phras-es, prosodic features, reference, syntax and lexicalattraction (Beeferman et al, 1997a) using decisiontrees (Miike et al, 1994; Kurohashi and Nagao,1994; Litman and Passonneau, 1995) and probabilis-tic models (Beeferman et al, 1997b; Hajime et al,1998; Reynar, 1998).
Work in this area is largely mo-tivated by the topic detection and tracking (TDT)initiative (Allan et al, 1998).
The focus is on thesegmentation f transcribed spoken text and broad-cast news stories where the presentation format andregular cues can be exploited to improve accuracy.3 Algor i thmOur segmentation algorithm takes a list of tokenizedsentences as input.
A tokenizer (Grefenstette andTapanainen, 1994) and a sentence boundary disam-biguation algorithm (Palmer and Hearst, 1994; Rey-nar and Ratnaparkhi, 1997) or EAGLE (Reynar etal., 1997) may be used to convert a plain text docu-ment into the acceptable input format.3.1 Similarity measurePunctuation and uninformative words are removedfrom each sentence using a simple regular expressionpattern mateher and a stopword list.
A stemmingalgorithm (Porter, 1980) is then applied to the re-maining tokens to obtain the word stems.
A dic-tionary of word stem frequencies i constructed foreach sentence.
This is represented as a vector offrequency counts.Let fi,j denote the frequency of word j in sentencei.
The similarity between a pair of sentences x,y26is computed using the cosine measure as shown inequation 1.
This is applied to all sentence pairs togenerate a similarity matrix.E:, f.~., x :~.,s im(x ,y )  = ~_~., .~., ,,, f2 .xE j f2 .
(1)Figure 1 shows an example of a similarity matrix ~ .High similarity values are represented by bright pix-els.
The bottom-left and top-right pixel show theself-similarity for the first and last sentence, respec-tively.
Notice the matrix is symmetric and containsbright square r gions along the diagonal.
These re-gions represent cohesive text segments.Each value in the similarity matrix is replaced byits rank in the local region.
The rank is the num-ber of neighbouring elements with a lower similarityvalue.
Figure 2 shows an example of image rankingusing a 3 x 3 rank mask with output range {0, 8}.For segmentation, we used a 11 x 11 rank mask.
Theoutput is expressed as a ratio r (equation 2) to cir-cumvent normalisation problems (consider the caseswhen the rank mask is not contained in the image).Similarity matrix Rank matrixI 5 4 ~7 1~i7  8 ,6  3 i 7Step 1i2.1' ~tLT l~ '6,, ~ 7 ,Step 21214Similarity matrix Rank matrix2 3 2 I - J  ; 4, 7 a ?91t [6  IStep 3Step 4Figure 2: A working example of image ranking.Figure 1: An example similarity matrix.3.2 Rank ingFor short text segments, the absolute value ofsire(x, y) is unreliable.
An additional occurrence ofa common word (reflected in the numerator) causesa disproportionate increase in sim(x,y) unless thedenominator (related to segment length) is large.Thus, in the context of text segmentation where asegment has typically < 100 informative tokens, onecan only use the metric to estimate the order of sim-ilarity between sentences, e.g.
a is more similar to bthan c.Furthermore, language usage varies throughout adocument.
For instance, the introduction section ofa document is less cohesive than a section which isabout a particular topic.
Consequently, it is inap-propriate to directly compare the similarity valuesfrom different regions of the similarity matrix.In non-parametric statistical analysis, one com-pares the rank of data sets when the qualitative be-haviour is similar but the absolute quantities are un-reliable.
We present a ranking scheme which is anadaptation of that described in (O'Neil and Denos,1992).1The contrast of the image has been adjusted to highlightthe image features.# of elements with a lower valuer = (2 )# of elements examinedTo demonstrate the effect of image ranking, theprocess was applied to the matrix shown in figure 1to produce figure 32 .
Notice the contrast has beenimproved significantly.
Figure 4 illustrates the moresubtle effects of our ranking scheme, r(x) is the rank(1 x 11 mask) of f(x) which is a sine wave withdecaying mean, amplitude and frequency (equation3).Figure 3: The matrix in figure 1 after ranking.2The process was applied to the original matrix, prior tocontra.st enhancement.
Theoutput image has not been n-hanced.
:97 27~Tr f(:,:) = ?.q(z) = .~(e -:/'~ + .~- : /2(1 + sin(10z?7)))- 209 '0B07  /~t0605040302010//, // /t(x)fix)xkI100X(3): =  t .50 150 200Figure 4: An illustration of the more subtle effectsof our ranking scheme.3.3 C lus ter ingThe final process determines the location of the topicboundaries.
The method is based on Reynar's max-imisation algorithm (Reynar, 1998; Helfman, 1996;Church, 1993; Church and Helfman, 1993).
A textsegment is defined by two sentences i , j  (inclusive).This is represented as a square region along the di-agonal of the rank matrix.
Let si,j denote the sum ofthe rank values in a segment and a i j  = (j - i + 1) 2be the inside area.
B = {bl,...,bm} is a list of m(:oherent text segments, sk and ak refers to the sumof rank and area of segment k in B.
D is the insidedensity of B (see equation 4).D - (4))-~k~l akTo initialise the process, the entire document isplaced in B as one coherent text segment.
Each stepof the process splits one of the segments in B. Thesplit point is a potential boundary which maximisesD.
Figure 5 shows a working example.The number of segments to generate, m, is deter-mined automatically.
D (n) is the inside density of nsegments and 5D (n) -- D (n) -D  (n-l) is the gradient.For a document with b potential boundaries, b step-s of divisive clustering generates (D (1), ..., D (b+l)}and {SD(2),...,SD (b+l)} (see figure 6 and 7).
Anunusually large reduction in 5D suggests the opti-inal clustering has been obtained 3 (see n = 10 in3In practice, convolution (mask {1,2,4,8,4,2,1}) is firstaI)plied to 5D to smooth out sharp local changesRank matrix Step 2Step 1cutStep 3Figure 5: A working example of the divisive eluster-ing algorithm.figure 7).
Let # and v be the mean and variance of5D(n),n E {2, ..., b + 1}.
m is obtained by applyingthe threshold, p + c ?
v/~, to 5D (c = 1.2 works wellin practice).0.90.80.7~, 0.6g~ o.5~ o.4(1.3(I,2o.1o ,o 2'0 3'0 ,; s; ,'0 ;oNumber of segmentsFigure 6: The inside density of all levels of segmen-tation.3.4 Speed opt imisat ionThe running time of each step is dominated by thecomputation of sk.
Given si,j is constant, our algo-rithm pre-computes all the values to improve speedperformance.
The procedure computes the values a-long diagonals, starting from the main diagonal ands"} O 28.... kl t' 1004 ~003O.02i I I i i | ~ " - " ~o 10 20 30 40 50 60 7Number of segmentsFigure 7: Finding the optimal segmentation usingthe gradient.works towards the corner.
The method has a com-Ln2 Let refer to the rank value plexity of order 12 ?
ri,jin the rank matrix R and S to the sum of rank ma-trix.
Given R of size n ?
n, S is computed in threesteps (see equation 5).
Figure 8 shows the result ofapplying this procedure to the rank matrix in figure5.4 Eva luat ionThe definition of a topic segment ranges from com-plete stories (Allan et al, 1998) to summaries (Ponteand Croft, 1997).
Given the quality of an algorithmis task dependent, he following experiments focuson the relative performance.
Our evaluation strat-egy is a variant of that described in (Reynar, 1998,71-73) and the TDT segmentation task (Allan et al,1998).
We assume a good algorithm is one that findsthe most prominent topic boundaries.4.1 Exper iment  procedureAn artificial test corpus of 700 samples is used toassess the accuracy and speed performance of seg-mentation algorithms.
A sample is a concatenationof ten text segments.
A segment is the first n sen-tences of a randomly selected document from theBrown corpus 4.
A sample is characterised by therange of n. The corpus was generated by an auto-matic procedure 5.
Table 1 presents the corpus s-tatistics.I Range of n - - 11 I 3 -  11400 I 310: I 610: I 9100 I \[ # samplesTable 1: Test corpus statistics.1.
Si, i -~- r i , ifor i E {1,...,n}2.
S i+ l , i  : 2ri+t,i + si , i  + s i+ l , i+ l8i,i+ 1 : 8i+1, ifor i E {1 , .
.
.
,n -  1}3.
8 iT j ,  i -~ 2r i+ j , i  "}- 8iWj-- l , i - l -8 i+j , i+ 1 -- 8 i+ j_ l , i+  1Si, i+ j --~- 8i+j,  ifor jE{2  .... ,n - l}iE  {1 , .
.
.
,n - j}(5)Rank matr ix..... !
..............................................
I ......I i 2 I 2 5 j 4. .
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
a .
.
.
.
.
.
.
.
.
.
ai , i , ,  *i- 15i- i  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
i .
............
I .
.
.
.
.
.
.
.
.
.
.
.
~.
.
.
.
.
.
.
t .
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
4 .
.
.
.
.
.
.~ ,4 5 2 J I i. .
.
.
.
.
!
!
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
; .
.
.
.
.
.
4 .
.
.
.
.
.
,3514 I I i 2 J4 3 i = 411 i1 iSum of rank  matr ix.
.
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
!
.
.
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
.
.91 !67 146 i  33.
19!
430 i 18 / 5 i 13 261 46 i. .
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
~ P i ,  ~ ......15 i 5 18 i 28 i 43 67.......... i ........ !
............ i .
.
.
.
.
.
.
!i i | ,Figure 8: Improving speed performance by pre-computing si,j.p(error\]ref, hyp, k) =p(misslref, hyp, diff, k)p(diffiref, k)+ (6)p(falref, hyp, same, k)p(samelref , k)Speed performance is measured by the averagenumber of CPU seconds required to process a testsample 6.
Segmentation accuracy is measured by th(,.error metric (equation 6, fa --+ false alarms) 1)roposedin (Beeferman et al, 1999).
Low error probabilityindicates high accuracy.
Other performanc(; mea-sures include the popular precision and recall metric(PR) (Hearst, 1994), fuzzy PR (Reynar, 1998) andedit distance (Ponte and Croft, 1997).
The l)rob-lems associated with these metrics are discussed in(Beeferman et al, 1999).4.2  Exper iment  1 - Base l ineFive degenerate algorithms define the baseline forthe experiments.
B,, does not propose any bound-aries.
Ba reports all potential boundaries as realboundaries.
B,.
partitions the sample into regularsegments.
B(r,?)
randomly selects any number of4Only the news articles ca** .pos  and informative textc j ** .pos  were used in the exper iment.5All exper iment  data,  a lgor i thms,  scr ipts att(I detailed re-suits are available from the author .6All exper iments  were conducted on a Pent ium I 1256MllzPC  with 128Mb RAM runn ing  RedHat Linux 6.0 and tileBlackdown Linux port  of .IDK1.1.7 v3.
"lPQ 29boundaries as real boundaries.
B(r,b ) randomly se-lects b boundaries as real boundaries.The accuracy of the last two algorithms are com-puWA analytically.
We consider the status of m po-tential bomldaries as a bit string (1 --+ topic bound-ary).
The terms p(miss) and p(fa) in equation 6 cor-responds to p(samelk ) and p(difflk ) = 1-p(same\[k).Equatioll 7, 8 and 9 gives the general form ofp(samelk ), B(.,.?)
and B(r,b), respectively 7.Table 2 presents the experimental results.
Thevalues in row two and three, four and five are notactually the same.
However, their differences areinsignificant according to the Kolmogorov-Smirnov,or KS-test (Press et al, 1992).# valid segmentations\]) (sanlel  \]~ ) = (7)# possible segmentations2//~- \]?p(samelk, B(~,?))
- 2m - 2 -k(m-k)Cbp(sarne lk ,  m, B(r ,b) )  - -  ~Cb3-11 3-5 6-8 9-11B~ 45% 38% 39% 36%B,, 47% 47% 47% 46%B(,.,~) 47% 47% 47% 46%B,  53% 53% 53% 54%B(~,?)
53% 53% 53% 54%(8)(9)Table 2: The error rate of the baseline algorithms.4.3 Experiment 2 - TextTil ingWe compare three versions of the TextTiling algo-rithm (Hearst, 1994).
H94(c,d) is Hearst's C im-plementation with default parameters.
H94(c,r) us-es the recommended parameters k = 6, w = 20.H94(js) is my implementation of the algorithm.Experimental result (table 3) shows H94(c,a) andH94(~,r) are more accurate than H94(js).
We sus-pect this is due to the use of a different stopword listand stemming algorithm.4.4 Experiment 3 - DotP lo tFive versions of Reynar's optimisation algorithm(Reynar, 1998) were evaluated.
R98 and R98(min)are exact implementations of his maximisation andminimisation algorithm.
R98(~,~o~) is my version ofthe maximisation algorithm which uses the cosinecoefficient instead of dot density for measuring sim-ilarity.
It incorporates the optimisations described7The full derivation of our method is available from theauthor.I 3-11 3-5 6-8 9-11H94(c,~t) 46% 44% 43% 48%H94(c,~) 46% 44% 44% 49%H94(~u. )
54% 45% 52% 53%H94(~,a/ 0.67s 0.52s 0.66s 0.88sH94(c,~) 0.68s 0.52s 0.67s 0.92sH94(j,~) 3.77s 2.21s 3.69s 5.07sTable 3: The error rate and speed performance ofTextTiling.in section 3.4.
R98(m,dot) is the modularised versionof R98 for experimenting with different similaritymeasures.R98(m,s,) uses a variant of Kozima's semantic sim-ilarity measure (Kozima, 1993) to compute blocksimilarity.
Word similarity is a function of word co-occurrence statistics in the given document.
Word-s that belong to the same sentence are consideredto be related.
Given the co-occurrence frequen-cies f(wi, wj), the transition probability matrix t iscomputed by equation 10.
Equation 11 defines ourspread activation scheme, s denotes the word sim-ilarity matrix, x is the number of activation stepsand norm(y) converts a matrix y into a transitionmatrix, x = 5 was used in the experiment.y(w ,wj) (10)t ,j = p(wj  Iw ) = E js=norm(~t ' ) i= l  (11)Experimental result (table 4) shows the cosine co-efficient and our spread activation method improvedsegmentation accuracy.
The speed optimisations sig-nificantly reduced the execution time.3-11 3-5 6-8 9-11R98{,~,8,) 18% 20% 15% 12%R98(8,co~) 21% 18% 19% 18%R98(m,dot) 22% 21% 18% 16%R98 22% 21% 18% 16%_R98(min) n/a 34% 37% 37%R98(s,cos) 4.54s 2.24s 4,36s 6.99sR98 29.58s 9.29s 28.09s 55.03sR98(m,s,) 41.02s 7.34s 40.05s 113.5sR98(m,dot) 46.58s 9.24s 42.72s 115.4s_R98(min) n/a 19.62s 58.77s 122.6sTable 4: The error rate and speed perforinance ofReynar's optimisation algorithm.4.5 Experiment 4 - SegmenterWe compare three versions of Segmenter (Kan et al,1998).
K98(B) is the original Perl implementation f30 30the algoritlun (version 1.6).
K98(j) is my imple-mentation of the algorithm, K98(j,a) is a version ofK98(j) which uses a document specific chain break-ing strategy.
The distribution of link distances areused to identify unusually long links.
The thresholdis a function # + c x vf5 of the mean # and varianceu.
We found c = 1 works well in practice.Table 5 summarises the experimental results.K98(p) performed significantly better than K98g,,).This is due to the use of a different part-of-speechtagger and shallow parser.
The difference in speed islargely due to the programming languages and termclustering strategies.
Our chain breaking strategyimproved accuracy (compare K98(j) with K98(j,~)).3-11 3-5 6-8 9-11K98(p) 36% 23% 33% 43%K98(j,,) n /a  41% 46% 50%K98(i ) n /a  44% 48% 51%K98(p) 4.24s 2.57s 4.21s 6.00sK98(j) n /a  21.43s 65.54s 129.3sK98(L~ ) n/a 21.44s 65.49s 129.7sTable 5: The error rate and speed performance ofSegmenter.4.6 Exper iment  5 - Our  a lgor i thm,  C99Two versions of our algorithm were developed, C99and C99(b).
The former is an exact implementationof the algorithm described in this paper.
The latteris given the expected number of topic segments forfair comparison with R98.
Both algorithms used a11 x 11 ranking mask.The first experiment focuses on the impact of ourautomatic termination strategy on C99(~) (table 6).C99(b) is marginally more accurate than C99.
Thisindicates our automatic termination strategy is effec-tive but not optimal.
The minor reduction in speedperformance is acceptable.3-11 3-5 6-8 9-11C99('b) 12% 12% 9% 9%C99 13% 18% 10% 10%C99(b) 4.00s 1.91s 3.73s 5.99sC99 4.04s 2.12s 4.04s 6.31sTable 6: The error rate and speed performance ofour algorithm, C99.The second experiment investigates the effect ofdifferent ranking mask size on the performance ofC99 (table 7).
Execution time increases with masksize.
A 1 x 1 ranking mask reduces all the elements inthe rank matrix to zero.
Interestingly, the increasein ranking mask size beyond 3 x 3 has insignificanteffect on segmentation accuracy.
This suggests theuse of extrema for clustering has a greater impact onaccuracy than linearising the similarity scores (figure4).I x l3x35x57x79x9ii x 1113 x 1315 x 1517 x 17ix l3x35x57x79x9II x Ii13 x 1315 x 1517 x 173-11 3-5 6-8 9-1148% 48% 50% 49%12% 11% 10% 8%12% 11% 10% 8%12% 11% 10% 8%12% 11% 10% 9%12% 11% 10% 9%12% 11% 10% 9%12% 11% 10% 9%12% 10% 10% 8%3.92s 2.06s 3.84s 5.91s3.83s 2.03s 3i79s 5.85s3.86s 2.04s 3.84s 5.92s3.90s 2.06s 3.88s 6.00s3.96s 2.07s 3.92s 6.12s4.02s 2.09s 3.98s 6.26s4.11s 2,11s 4.07s 6.41s4.20s 2.14s 4.14s 6.60s4.29s 2.17s 4.25s 6.79sTable 7: The impact of mask size oil the performanceof C99.4.7  SummaryExperimental result (table 8) shows our algorith-m C99 is more accurate than existing algorithms.A two-fold increase in accuracy and seven-fold in-crease in speed was achieved (compare C99(b) withR98).
If one disregards egmentation accuracy, H94has the best algorithmic performance (linear).
C99,K98 and R98 are all polynomial time algorithms.The significance of our results has been confirmedby both t-test and KS-test.3-11 3-5 6-8 9-11C99(b) 12% 12% 9% 9%C99 13% 18% 10% 10%R98 22% 21% 18% 16%K98(p) 36% 23% 33% 43%H94(~,d) 46% 44% 43% 48%H94(j,r) 3.77s 2.21s 3.69s 5.07sC99(b) 4,00s 1.91s 3.73s 5.99sC99 4.04s 2.12s 4.04s 6.31sR98 29.58s 9.29s 28.09s 55.03sK98g) n/a 21.43s 65.54s 129.3sTable 8: A summary of our experimental results.5 Conc lus ions  and  fu ture  workA segmentation algorithm has two key elements, aclustering strategy and a similarity me~sure.
Ouro'~4 31results how divisive clustering (R98) is more precisethan sliding window (H94) and lexical chains (K98)for locating topic boundaries.Four similarity measures were examined.
The co-sine coefficient (R98(s,co,)) and dot density measure(R98(m,(lot)) yield similar esults.
Our spread activa-tion based semantic measure (R98( ..... ,)) improveda.ccura(:y.
This confirms that although Kozima's ap-l)roaeh (Kozima, 1993) is computationally expen-sive, it does produce more precise segmentation.Tile most significant improvement was due to ourranking scheme which linearises the cosine coefficien-t. Our exl)eriments demonstrate hat given insuffi-(:lent data, tile qualitative behaviour of the cosinem(,asul'e is indeed more reliable than the actual val-II(~S.Although our evaluation scheme is sufficient forthis (:omparative study, further research requires alarge scale, task independent benchmark.
It wouldbe interesting to corot)are C99 with the multi-sourcemethod escribed in (Beeferman et al, 1999) usingthe TDT corpus.
We would also like to develop alinear time and multi-source version of the algorith-I I l .AcknowledgementsThis paper has benefitted from the comments ofMary McGee Wood and the anonymous reviewer-s.
Thanks are due to my parents and departmenttbr making this work possible; Jeffrey Reynar fordiscussions and guidance on the segmentation prob-lem; Hideki Kozima for help on the spread activationnmasure; Min-Yen Kan and Marti Hearst for theirsegmentation algorithms; Daniel Orain for referencesto image processing techniques; Magnus Rattray andStephen Marsland for help on statistics and mathe-matics.Re ferencesJames Allan, Jaime Carbonell, George Doddington,Jonathan Yamron, and Yiming Yang.
1998.
Topicdetection and tracking pilot study final report.
InProceedings of the DARPA Broadcast News Tran-scription and Understanding Workshop.Doug Beeferman, Adam Berger, and John Lafferty.1997a.
A model of lexical attraction and repul-sion.
In Proceedings of the 35th Annual Meetingof the A CL.Doug Beeferman, Adam Berger, and John Lafferty.1997b.
Text segmentation using exponential mod-els.
In Proceedings of EMNLP-2.Doug Beeferman, Adam Berger, and John Laffer-ty.
1999.
Statistical models for text segmentation.Machine learning, special issue on Natural Lan-guage Processing, 34(1-3):177-210.
C. Cardie andR.
Mooney (editors).Freddy Y. Y. Choi.
2000.
A speech interface forrapid reading.
In Proceedings of lEE coUoquium:Speech and Language Processing for Disabled andElderly People, London, England, April.
IEE.Kenneth W. Church and Jonathan I. Helfman.
1993.Dotplot: A program for exploring self-similarity inmillions of lines of text and code.
The Journal ofComputational nd Graphical Statistics.Kenneth W. Church.
1993.
Char_align: A t)rogramfor aligning parallel texts at the character level.In Proceedings of the 31st Annual Meeting of th.eACL.David Eichmann, Miguel Ruiz, and Padmini S-rinivasan.
1999.
A cluster-based approach totracking, detection and segmentation f broadcastnews.
In Proceedings of the 1999 DARPA Broad-cast News Workshop (TDT-2).Gregory Grefenstette and Pasi Tapanainen.
1994.What is a word, what is a sentence?
problems oftokenization.
In Proceedings of the 3rd Conferenceon Computational Lexicography and Text Research(COMPLEX'94), Budapest, July.Mochizuki Hajime, Honda Takeo, and OkumuraManabu.
1998.
Text segmentation with mul-tiple surface linguistic cues.
In Proceedings ofCOLING-ACL'98, pages 881-885.Michael Halliday and Ruqaiya Hasan.
1976.
Cohe-sion in English.
Longman Group, New York.Marti Hearst and Christian Plaunt.
1993.
Subtopicstructuring for full-length document access.
InProceedings of the 16th Annual InternationalA CM/SIGIR Conference, Pittsburgh, PA.Marti A. Hearst.
1994.
Multi-paragraph segmenta-tion of expository text.
In Proceedings of the A-CL'94.
Las Crees, NM.Oskari Heinonen.
1998.
Optimal multi-paragraphtext segmentation by dynamic programnfing.
InProceedings of COLING-A CL '98.Jonathan I. Helfman.
1996.
Dotplot patterns: A lit-eral look at pattern languages.
Theory and Prac-tice of Object Systems, 2(1):31-41.Min-Yen Kan, Judith L. Klavans, and Kathleen R.McKeown.
1998.
Linear segmentation a d seg-ment significance.
In Proceedings of the 6th.Inter~national Workshop of Very Large Corpora(WVLC-6), pages 197-205, Montreal, Quebec,Canada, August.Stefan Kaufmann.
1999.
Cohesion and collocation:Using context vectors in text segmentation.
InProceedings of the 37th Annual Meeting of the A.s-sociation of for Computational Linguistics (Stu-dent Session), pages 591-595, College Park, USA,June.
ACL.Hideki Kozima.
1993.
Text segmentation based onsimilarity between words.
In Proceedings of A-CL '93, pages 286-288, Ohio.Sadao Kurohashi and Makoto Nagao.
1994.
Auto-~Jp~ 32matic detection of discourse structure by checkingsurface information in sentences.
In Processingsof COLING'94, volume 2, pages 1123-1127.Diane J. Litman and Rebecca J. Passonneau.
1995.Combining multiple knowledge sources for dis-course segmentation.
In Proceedings of the 33rdAnnual Meeting o/the ACL.S.
Miike, E. Itoh, K. Ono, and K. Sumita.
1994.A full text retrieval system.
In Proceedings of SI-GIR '9~, Dublin, Ireland.J.
Morris and G. Hirst.
1991.
Lexical cohesion com-puted by thesaural relations as an indicator ofthe structure of text.
Computational Linguistic-s, (17):21-48.Jane Morris.
1988.
Lexical cohesion, the thesaurus,and the structure of text.
Technical Report CSRI219, Computer Systems Research Institute, Uni-versity of Toronto.M.
A. O'Neil and M. I. Denos.
1992.
Practical ap-proach to the stereo-matching of urban imagery.Image and Vision Computing, 10(2):89-98.David D. Palmer and Marti A. Hearst.
1994.
Adap-tive sentence boundary disambiguation.
In Pro-ceedings of the 4th Conference on Applied NaturalLanguage Processing, Stuttgart, Germany, Octo-ber.
ACL.Jay M. Ponte and Bruce W. Croft.
1997.
Text seg-mentation by topic.
In Proceedings o/the first Eu-ropean Conference on research and advanced tech-nology for digital libraries.
U.Mass.
Computer Sci-ence Technical Report TR97-18.M.
Porter.
1980.
An algorithm for suffix stripping.Program, 14(3):130-137, July.William H. Press, Saul A. Teukolsky, William T.Vettering, and Brian P. Flannery, 1992.
Numeri-cal recipes in C: The Art of Scientific Computing,chapter 14, pages 623-628.
Cambridge UniversityPress, second edition.Jeffrey Reynar and Adwait Ratnaparkhi.
1997.
Amaximum entropy approach to identifying sen-tence boundaries.
In Proceedings o/the fifth con-ference on Applied NLP, Washington D.C.Jeffrey Reynar, Breck Baldwin, Christine Doran,Michael Niv, B. Srinivas, and Mark Wasson.
1997.Eagle: An extensible architecture for general lin-guistic engineering.
In Proceedings o/RIAO '97,Montreal, June.Jeffrey C. Reynar.
1994.
An automatic method offinding topic boundaries.
In Proceedings o/ A-CL '9,~ (Student session).Jeffrey C. Reynar.
1998.
Topic segmentation: Algo-rithms and applications.
Ph.D. thesis, Computerand Information Science, University of Pennsylva-nia.Jeffrey C. Reynar.
1999.
Statistical models for topicsegmentation.
I  Proceedings of the 37th AnnualMeeting o/the A CL, pages 357-364.
20-26th June,Maryland, USA.C.
J. van Rijsbergen.
1979.
Information Retrieval.Buttersworth.Yaakov Yaari.
1997.
Segmentation of expositorytexts by hierarchical gglomerative clustering.
InProceedings o/RANLP'97.
Bulgaria.Gilbert Youmans.
1991.
A new tool for discourseanalysis: The vocabulary-management profile.Language, pages 763-789.33
