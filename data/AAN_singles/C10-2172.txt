Coling 2010: Poster Volume, pages 1507?1514,Beijing, August 2010Predicting Discourse Connectives for Implicit Discourse RelationRecognitionZhi-Min Zhou and Yu XuEast China Normal University51091201052@ecnu.cnZheng-Yu NiuToshiba China R&D Centerzhengyu.niu@gmail.comMan Lan and Jian SuInstitute for Infocomm Researchsujian@i2r.a-star.edu.sgChew Lim TanNational University of Singaporetancl@comp.nus.edu.sgAbstractExisting works indicate that the absenceof explicit discourse connectives makesit difficult to recognize implicit discourserelations.
In this paper we attempt toovercome this difficulty for implicit rela-tion recognition by automatically insert-ing discourse connectives between argu-ments with the use of a language model.Then we propose two algorithms to lever-age the information of these predictedconnectives.
One is to use these pre-dicted implicit connectives as additionalfeatures in a supervised model.
The otheris to perform implicit relation recognitionbased only on these predicted connectives.Results on Penn Discourse Treebank 2.0show that predicted discourse connectiveshelp implicit relation recognition and thefirst algorithm can achieve an absolute av-erage f-score improvement of 3% over astate of the art baseline system.1 IntroductionDiscourse relation analysis is to automaticallyidentify discourse relations (e.g., explanation re-lation) that hold between arbitrary spans of text.This analysis may be a part of many natural lan-guage processing systems, e.g., text summariza-tion system, question answering system.
If thereare discourse connectives between textual unitsto explicitly mark their relations, the recognitiontask on these texts is defined as explicit discourserelation recognition.
Otherwise it is defined as im-plicit discourse relation recognition.Previous study indicates that the presence ofdiscourse connectives between textual units cangreatly help relation recognition.
In Penn Dis-course Treebank (PDTB) corpus (Prasad et al,2008), the most general senses, i.e., Comparison(Comp.
), Contingency (Cont.
), Temporal (Temp.
)and Expansion (Exp.
), can be disambiguated inexplicit relations with more than 90% f-scoresbased only on the discourse connectives explicitlyused to signal the relation (Pitler and Nenkova.,2009b).However, for implicit relations, there are noconnectives to explicitly mark the relations, whichmakes the recognition task quite difficult.
Some ofexisting works attempt to perform relation recog-nition without hand-annotated corpora (Marcuand Echihabi, 2002), (Sporleder and Lascarides,2008) and (Blair-Goldensohn, 2007).
They useunambiguous patterns such as [Arg1, but Arg2]to create synthetic examples of implicit relationsand then use [Arg1, Arg2] as an training exampleof an implicit relation.
Another research line isto exploit various linguistically informed featuresunder the framework of supervised models, (Pitleret al, 2009a) and (Lin et al, 2009), e.g., polarityfeatures, semantic classes, tense, production rulesof parse trees of arguments, etc.Our study on PDTB test data shows that the av-erage f-score for the most general 4 senses canreach 91.8% when we simply mapped the groundtruth implicit connective of each test instance toits most frequent sense.
It indicates the impor-tance of connective information for implicit rela-tion recognition.
However, so far there is no previ-ous study attempting to use such kind of connec-tive information for implicit relation.
One possi-1507ble reason is that implicit connectives do not ex-ist in unannotated real texts.
Another evidenceof the importance of connectives for implicit re-lations is shown in PDTB annotation.
The PDTBannotation consists of inserting a connective ex-pression that best conveys the inferred relation bythe readers.
Connectives inserted in this way toexpress inferred relations are called implicit con-nectives, which do not exist in real texts.
Theseevidences inspire us to consider two interesting re-search questions:(1) Can we automatically predict implicit connec-tives between arguments?
(2) How to use the predicted implicit connectivesto build an automatic discourse relation analysissystem?In this paper we address these two questions asfollows: (1) We insert appropriate discourse con-nectives between two textual units with the use ofa language model.
Here we train the languagemodel on large amount of raw corpora withoutthe use of any hand-annotated data.
(2) Then wepresent two algorithms to use these predicted con-nectives for implicit relation recognition.
One isto use these connectives as additional features in asupervised model.
The other is to perform relationrecognition based only on these connectives.We performed evaluation of the two algorithmsand a baseline system on PDTB 2.0 corpus.
Ex-perimental results showed that using predicteddiscourse connectives as additional features cansignificantly improve the performance of implicitdiscourse relation recognition.
Specifically, thefirst algorithm achieved an absolute average f-score improvement of 3% over a state of the artbaseline system.The rest of this paper is organized as follows.Section 2 describes the two algorithms for implicitdiscourse relation recognition.
Section 3 presentsexperiments and results on PDTB data.
Section4 reviews related work.
Section 5 concludes thiswork.2 Our Algorithms for Implicit DiscourseRelation Recognition2.1 Prediction of implicit connectivesExplicit discourse relations are easily identifiabledue to the presence of discourse connectives be-tween arguments.
(Pitler and Nenkova., 2009b)showed that in PDTB corpus, the most generalsenses, i.e., Comparison (Comp.
), Contingency(Cont.
), Temporal (Temp.)
and Expansion (Exp.
),can be disambiguated in explicit relations withmore than 90% f-scores based only on discourseconnectives.But for implicit relations, there are no connec-tives to explicitly mark the relations, which makesthe recognition task quite difficult.
PDTB dataprovides implicit connectives that are inserted be-tween paragraph-internal adjacent sentence pairsnot marked by any of explicit connectives.
Theavailability of ground-truth implicit connectivesmakes it possible to evaluate the contribution ofthese connectives for implicit relation recognition.Our initial study on PDTB data show that the av-erage f-score for the most general 4 senses canreach 91.8% when we obtained the sense of eachtest example by mapping each ground truth im-plicit connective to its most frequent sense.
Wesee that connective information is an importantknowledge source for implicit relation recogni-tion.
However these implicit connectives do notexist in real texts.
In this paper we overcome thisdifficulty by inserting a connective between twoarguments with the use of a language model.Following the annotation scheme of PDTB, weassume that each implicit connective takes two ar-guments, denoted as Arg1 and Arg2.
Typically,there are two possible positions for most of im-plicit connectives1, i.e., the position before Arg1and the position between Arg1 and Arg2.
Given aset of possible implicit connectives {ci}, we gen-erate two synthetic sentences, ci+Arg1+Arg2 andArg1+ci+Arg2 for each ci, denoted as Sci,1 andSci,2.
Then we calculate the perplexity (an intrin-sic score) of these sentences with the use of a lan-guage model, denoted as PPL(Sci,j).
According1For parallel connectives, e.g., if .
.
.
then.
.
.
, the two con-nectives will take the two arguments together, so there is onlyone possible combination for connectives and arguments.1508to the value of PPL(Sci,j) (the lower the better),we can rank these sentences and select the con-nectives in top N sentences as implicit connec-tives for this argument pair.
The language modelmay be trained on large amount of unannotatedcorpora that can be cheaply acquired, e.g., NorthAmerican News corpus.2.2 Using predicted implicit connectives asadditional featuresWe predict implicit connectives on both trainingset and test set.
Then we can use the predictedimplicit connectives as additional features for su-pervised implicit relation recognition.
Previousworks exploited various linguistically informedfeatures under the framework of supervised mod-els.
In this paper, we include 9 types of featuresin our system due to their superior performancein previous studies, e.g., polarity features, seman-tic classes of verbs, contextual sense, modality,inquirer tags of words, first-last words of argu-ments, cross-argument word pairs, ever used in(Pitler et al, 2009a), production rules of parsetrees of arguments used in (Lin et al, 2009), andintra-argument word pairs inspired by the work of(Saito et al, 2006).Here we provide the details of the 9 features,shown as follows:Verbs: Similar to the work in (Pitler et al,2009a), the verb features consist of the number ofpairs of verbs in Arg1 and Arg2 if they are fromthe same class based on their highest Levin verbclass level (Dorr, 2001).
In addition, the averagelength of verb phrase and the part of speech tagsof main verb are also included as verb features.Context: If the immediately preceding (or fol-lowing) relation is an explicit, its relation andsense are used as features.
Moreover, we use an-other feature to indicate if Arg1 leads a paragraph.Polarity: We use the number of positive,negated positive, negative and neutral words in ar-guments and their cross product as features.
Fornegated positives, we locate the negated words intext span and then define the closely behind posi-tive word as negated positive.Modality: We look for modal words includingtheir various tenses or abbreviation forms in botharguments.
Then we generate a feature to indicatethe presence or absence of modal words in botharguments and their cross product.Inquirer Tags: Inquirer Tags extracted fromGeneral Inquirer lexicon (Stone et al, 1966) con-tains positive or negative classification of words.In fact, its fine-grained categories, such as Fallversus Rise, or Pleasure versus Pain, can indi-cate the relation between two words, especiallyfor verbs.
So we choose the presence or absenceof 21 pair categories with complementary relationin Inquirer Tags as features.
We also include theircross production as features.FirstLastFirst3: We choose the first and lastwords of each argument as features, as well as thepair of first words, the pair of last words, and thefirst 3 words in each argument.
In addition, we ap-ply Porter?s Stemmer (Porter, 1980) to each wordbefore preparation of these features.Production Rule: According to (Lin et al,2009), we extract all the possible production rulesfrom arguments, and check whether the rules ap-pear in Arg1, Arg2 and both arguments.
We re-move the rules occurring less than 5 times in train-ing data.Cross-argument Word Pairs: We perform thePorter?s stemming (Porter, 1980), and then groupall words from Arg1 and Arg2 into two sets W1and W2 respectively.
Then we generate any possi-ble word pair (wi, wj) (wi ?
W1, wj ?
W2).
Weremove the word pairs with less than 5 times.Intra-argument Word Pairs: LetQ1 = (q1, q2, .
.
.
, qn) be the word se-quence of Arg1.
The intra-argument wordpairs for Arg1 is defined as WP1 =((q1, q2), (q1, q3), .
.
.
, (q1, qn), (q2, q3), .
.
.
,(qn?1, qn)).
We extract all the intra-argumentword pairs from Arg1 and Arg2 and remove wordpairs appearing less than 5 times in training data.2.3 Relation recognition based only onpredicted implicit connectivesAfter the prediction of implicit connectives, wecan address the implicit relation recognition taskwith the methods for explicit relation recogni-tion due to the presence of implicit connectives,e.g., sense classification based only on connec-tives (Pitler and Nenkova., 2009b).
The work of(Pitler and Nenkova., 2009b) showed that most1509of connectives are unambiguous and it is possibleto obtain high performance in prediction of dis-course sense due to the simple mapping relationbetween connectives and senses.
Given two ex-amples:(E1) She paid less on her dress, but it is very nice.
(E2) We have to harry up because the raining isgetting heavier and heavier.The two connectives, i.e., but in E1 and becausein E2, convey Comparison and Contingency senserespectively.
In most cases, we can easily recog-nize the relation sense by the appearance of dis-course connective since it can be interpreted inonly one way.
That means, the ambiguity of themapping between sense and connective is quitefew.We count the frequency of sense tags for eachpossible connective on PDTB training data for im-plicit relation.
Then we build a sense recognitionmodel by simply mapping each connective to itsmost frequent sense.
Here we do not perform con-nective prediction on training data.
During test-ing, we use the language model to insert implicitconnectives into each test argument pair.
Then weperform relation recognition by mapping each im-plicit connective to its most frequent sense.3 Experiments and Results3.1 Experiments3.1.1 Data setsIn this work we used the PDTB 2.0 corpus forevaluation of our algorithms.
Following the workof (Pitler et al, 2009a), we used sections 2-20 astraining set, sections 21-22 as test set, and sec-tions 0-1 as development set for parameter opti-mization.
For comparison with the work of (Pitleret al, 2009a), we ran four binary classificationtasks to identify each of the main relations (Cont.,Comp., Exp., and Temp.)
from the rest.
For eachrelation, we used equal numbers of positive andnegative examples as training data2.
The negativeexamples were chosen at random from sections 2-20.
We used all the instances in sections 21 and22 as test set, so the test set is representative of2Here the numbers of training and test instances for Ex-pansion relation are different from those in (Pitler et al,2009a).
The reason is that we do not include instances ofEntRel as positive examples.the natural distribution.
The numbers of positiveand negative instances for each sense in differentdata sets are listed in Table 1.Table 1: Statistics of positive and negative sam-ples in training, development and test sets for eachrelation.Relation Train Dev TestPos/Neg Pos/Neg Pos/NegComp.
1927/1927 191/997 146/912Cont.
3375/3375 292/896 276/782Exp.
6052/6052 651/537 556/502Temp.
730/730 54/1134 67/991In this work we used LibSVM toolkit to con-struct four linear SVM models for a baseline sys-tem and the system in Section 2.2.3.1.2 A baseline systemWe first built a baseline system, which used 9types of features listed in Section 2.2.We tuned the numbers of firstLastFirst3, cross-argument word pair, intra-argument word pair ondevelopment set.
Finally we set the frequencythreshold at 3, 5 and 5 respectively.3.1.3 Prediction of implicit connectivesTo predict implicit connectives, we adopt thefollowing two steps:(1) train a language model;(2) select top N implicit connectives.Step 1: We used SRILM toolkit to train the lan-guage models on three benchmark news corpora,i.e., New York part in the BLLIP North Ameri-can News, Xin and Ltw parts of English Gigaword(4th Edition).
We also tried different values forn in n-gram model.
The parameters were tunedon the development set to optimize the accuracyof prediction.
In this work we chose 3-gram lan-guage model trained on NY corpus.Step 2: We combined each instance?s Arg1 andArg2 with connectives extract from PDTB2 (100in all).
There are two types of connectives, sin-gle connective (e.g.
because and but) and paral-lel connective (such as ?not only .
.
.
, but also?
).Since discourse connectives may appear not onlyahead of the Arg1, but also between Arg1 andArg2, we considered this case.
Given a set of pos-sible implicit connectives {ci}, for single connec-tive {ci}, we constructed two synthetic sentences,ci+Arg1+Arg2 and Arg1+ci+Arg2.
In case of1510parallel connective, we constructed one syntheticsentence like ci1+Arg1+ci2+Arg2.As a result, we can get 198 synthetic sentencesfor each argument pair.
Then we converted allwords to lower cases and used the language modeltrained in the above step to calculate perplexityon sentence level.
The perplexity scores wereranked from low to high.
For example, we got theperplexity (ppl) for two sentences as follows:(1) but this is an old story, we?re talking aboutyears ago before anyone heard of asbestos havingany questionable properties.ppl= 652.837(2) this is an old story, but we?re talking aboutyears ago before anyone heard of asbestos havingany questionable properties.ppl= 583.514We considered the combination of connectivesand their position as final features like mid but,first but, where the features are binary, that is, thepresence and absence of the specific connective.According to the value of PPL(Sci,j) (thelower the better), we selected the connectives intop N sentences as implicit connectives for thisargument pair.
In order to get the optimal N value,we tried various values of N on development setand selected the minimum value of N so that theground-truth connectives appeared in top N con-nectives.
The final N value is set to 60 based onthe trade-off between performance and efficiency.3.1.4 Using predicted connectives asadditional featuresThis system combines the predicted implicitconnectives as additional features and the 9 typesof features in an supervised framework.
The 9types of features are listed as shown in Section 2.2and tuned on development set.We combined predicted connectives with thebest subset features from the development data setwith respect to f-score.
In our experiment of se-lecting best subset features, single features ratherthan the combination of several features achievedmuch higher scores.
So we combine single fea-tures with predicted connectives as final features.3.1.5 Using only predicted connectives forimplicit relation recognitionWe built two variants for the algorithm in Sec-tion 2.3.
One is to use the data for explicit re-lations in PDTB sections 2-20 as training data.The other is to use the data for implicit relationsin PDTB sections 2-20 as training data.
Giventraining data, we obtained the most frequent sensefor each connective appearing in the training data.Then given test data, we recognized the sense ofeach argument pair by mapping each predictedconnective to its most frequent sense.
In thiswork we conducted another experiment to see theupper-bound performance of this algorithm.
Herewe performed recognition based on ground-truthimplicit connectives and used the data for implicitrelations as training data.3.2 Results3.2.1 Result of baseline systemTable 2 summarizes the best performanceachieved by the baseline system in compari-son with previous state-of-the-art performanceachieved in (Pitler et al, 2009a).
The first twolines in the table show their best results using sin-gle feature and using combined feature subset.
Itindicates that the performance of using combinedfeature subset is higher than that using single fea-ture alone.From this table, we can find that our base-line system has a comparable result on Contin-gency and Temporal.
On Comparison, our systemachieved a better performance around 9% f-scorehigher than their best result.
However, for Expan-sion, they expanded both training and testing setsby including EntRel relation as positive examples,which makes it impossible to perform direct com-parison.
Generally, our baseline system is reason-able and thus the consequent experiments on it arereliable.3.2.2 Result of algorithm 1: using predictedconnectives as additional featuresTable 3 summarizes the best performanceachieved by the baseline system and the first al-gorithm (i.e., baseline + Language Model) on testset.
The second and third column show the bestperformance achieved by the baseline system and1511Table 2: Performance comparison of the baseline system with the system of (Pitler et al, 2009a) on testset.System Comp.
vs. Not Cont.
vs. Other Exp.
vs. Other Temp.
vs. OtherF1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc)Using the best single feature (Pitler et al, 2009a) 21.01(52.59) 36.75(62.44) 71.29(59.23) 15.93(61.20)Using the best feature subset (Pitler et al, 2009a) 21.96(56.59) 47.13(67.30) 76.42(63.62) 16.76(63.49)The baseline system 30.72(78.26) 45.38(40.17) 65.95(57.94) 16.46(29.96)the first algorithm using predicted connectives asadditional features.Table 3: Performance comparison of the algo-rithm in Section 2.2 with the baseline system ontest set.Rela- Features Baseline Baseline+LMtion F1 (Acc) F1 (Acc)Comp.
Production Rule 30.72(78.26) 31.08(68.15)Context 24.66(42.25) 27.64(53.97)InquirerTags 23.31(73.25) 27.87(55.48)Polarity 21.11(40.64) 23.64(52.36)Modality 17.25(80.06) 26.17(55.20)Verbs 25.00(53.50) 31.79(58.22)Cont.
Prodcution Rule 45.38(40.17) 47.16(48.96)Context 37.61(44.70) 34.74(48.87)Polarity 35.57(50.00) 43.33(33.74)InquirerTags 38.04(41.49) 42.22(36.11)Modality 32.18(66.54) 35.26(55.58)Verbs 40.44(54.06) 42.04(32.23)Exp.
Context 48.34(54.54) 68.32(53.02)FirstLastFirst3 65.95(57.94) 68.94(53.59)InquirerTags 61.29(52.84) 68.49(53.21)Modality 64.36(56.14) 68.9(52.55)Polarity 49.95(50.38) 68.62(53.40)Verbs 52.95(53.31) 70.11(54.54)Temp.
Context 13.52(64.93) 16.99(79.68)FirstLastFirst3 15.75(66.64) 19.70(64.56)InquirerTags 8.51(83.74) 19.20(56.24)Modality 16.46(29.96) 19.97(54.54)Polarity 16.29(51.42) 20.30(55.48)Verbs 13.88(54.25) 13.53(61.34)From this table, we found that this additionalfeature obtained from language model showedsignificant improvements in almost four relations.Specifically, the top two improvements are on Ex-pansion and Temporal relations, which improved4.16% and 3.84% in f-score respectively.
Al-though on Comparison relation there is only aslight improvement (+1.07%), our two best sys-tems both got around 10% improvements of f-score over a state-of-the-art system in (Pitler et al,2009a).
As a whole, the first algorithm achieved3% improvement of f-score over a state of the artbaseline system.
All these results indicate thatpredicted implicit connectives can help improvethe performance.3.2.3 Result of algorithm 2: using onlypredicted connectives for implicitrelation recognitionTable 4 summarizes the best performanceachieved by the second algorithm in comparisonwith the baseline system on test set.The experiment showed that the baseline sys-tem using just gold-truth implicit connectives canachieve an f-score of 91.8% for implicit relationrecognition.
It once again proved that implicitconnectives make significant contributions for im-plicit relation recognition.
This also encouragesour future work on finding the most suitable con-nectives for implicit relation recognition.From this table, we found that, using only pre-dicted implicit connectives achieved an compara-ble performance to (Pitler et al, 2009a), althoughit was still a bit lower than our best baseline.
Butwe should bear in mind that this algorithm onlyuses 4 features for implicit relation recognitionand these 4 features are easy computable and fastrun, which makes the system more practical in ap-plication.
Furthermore, compared with other al-gorithms which require hand-annotated data fortraining, the performance of this second algorithmis acceptable if we take into account that no la-beled data is used for model training.3.3 AnalysisExperimental results on PDTB showed that usingthe predicted implicit connectives significantlyimproves the performance of implicit discourserelation recognition.
Our first algorithm achievesan average f-score improvement of 3% over astate of the art baseline system.
Specifically, forthe relations: Comp., Cont., Exp., Temp., ourfirst algorithm can achieve 1.07%, 1.78%, 4.16%,3.84% f-score improvements over a state of theart baseline system.
Since (Pitler et al, 2009a)1512Table 4: Performance comparison of the algorithm in Section 2.3 with the baseline system on test set.System Comp.
vs. Other Cont.
vs. Other Exp.
vs. Other Temp.
vs. OtherF1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc)The baseline system 30.72(78.26) 45.38(40.17) 65.95(57.94) 16.46(29.96)Our algorithm with training data for explicit relation 26.02(52.17) 35.72(51.70) 64.94(53.97) 13.76(41.97)Our algorithm with training data for implicit relation 24.55(63.99) 16.26(70.79) 60.70(53.50) 14.75(70.51)Sense recognition using gold-truth implicit connectives 94.08(98.30) 98.19(99.05) 97.79(97.64) 77.04(97.07)used different selection of instances for Expan-sion sense3, we cannot make a direct compari-son.
However, we achieve the best f-score around70%, which provide 5% improvements over ourbaseline system.
On the other hand, the secondproposed algorithm using only predicted connec-tives still achieves promising results for each rela-tion.
Specifically, the model for the Comparisonrelation achieves an f-score of 26.02% (5% overthe previous work in (Pitler et al, 2009a)).
Fur-thermore, the models for Contingency and Tem-poral relation achieve 35.72% and 13.76% f-scorerespectively, which are comparable to the previ-ous work in (Pitler et al, 2009a).
The model forExpansion relation obtains an f-score of 64.95%,which is only 1% less than our baseline systemwhich consists of ten thousands of features.4 Related WorkExisting works on automatic recognition of dis-course relations can be grouped into two cat-egories according to whether they used hand-annotated corpora.One research line is to perform relation recog-nition without hand-annotated corpora.
(Marcu and Echihabi, 2002) used a pattern-based approach to extract instances of discourserelations such as Contrast and Elaboration fromunlabeled corpora.
Then they used word-pairs be-tween two arguments as features for building clas-sification models and tested their model on artifi-cial data for implicit relations.There are other efforts that attempt to extend thework of (Marcu and Echihabi, 2002).
(Saito et al,2006) followed the method of (Marcu and Echi-habi, 2002) and conducted experiments with com-bination of cross-argument word pairs and phrasal3They expanded the Expansion data set by adding ran-domly selected EntRel instances by 50%, which is consid-ered to significantly change data distribution.patterns as features to recognize implicit relationsbetween adjacent sentences in a Japanese corpus.They showed that phrasal patterns extracted froma text span pair provide useful evidence in the re-lation classification.
(Sporleder and Lascarides,2008) discovered that Marcu and Echihabi?s mod-els do not perform as well on implicit relations asone might expect from the test accuracies on syn-thetic data.
(Blair-Goldensohn, 2007) extendedthe work of (Marcu and Echihabi, 2002) by re-fining the training and classification process usingparameter optimization, topic segmentation andsyntactic parsing.
(Lapata and Lascarides, 2004) dealt with tem-poral links between main and subordinate clausesby inferring the temporal markers linking them.They extracted clause pairs with explicit temporalmarkers from BLLIP corpus as training data.Another research line is to use human-annotated corpora as training data, e.g., the RSTBank (Carlson et al, 2001) used by (Soricut andMarcu, 2003), adhoc annotations used by (?
),(Baldridge and Lascarides, 2005), and the Graph-Bank (Wolf et al, 2005) used by (Wellner et al,2006).Recently the release of the Penn DiscourseTreeBank (PDTB) (Prasad et al, 2008) bene-fits the researchers with a large discourse anno-tated corpora, using a comprehensive scheme forboth implicit and explicit relations.
(Pitler et al,2009a) performed implicit relation classificationon the second version of the PDTB.
They usedseveral linguistically informed features, such asword polarity, verb classes, and word pairs, show-ing performance increases over a random classi-fication baseline.
(Lin et al, 2009) presented animplicit discourse relation classifier in PDTB withthe use of contextual relations, constituent ParseFeatures, dependency parse features and cross-argument word pairs.1513In comparison with existing works, we investi-gated a new knowledge source, implicit connec-tives, for implicit relation recognition.
Moreover,our two models can exploit both labeled and un-labeled data by training a language model on un-labeled data and then using this language modelto generate implicit connectives for recognitionmodels trained on labeled data.5 ConclusionsIn this paper we use a language model to auto-matically generate implicit connectives and thenpresent two methods to use these connectives forrecognition of implicit relations.
One method is touse these predicted implicit connectives as addi-tional features in a supervised model and the otheris to perform implicit relation recognition basedonly on these predicted connectives.
Results onPenn Discourse Treebank 2.0 show that predicteddiscourse connectives help implicit relation recog-nition and the first algorithm achieves an absoluteaverage f-score improvement of 3% over a state ofthe art baseline system.AcknowledgmentsThis work is supported by grants from Na-tional Natural Science Foundation of China(No.60903093), Shanghai Pujiang Talent Program(No.09PJ1404500) and Doctoral Fund of Ministryof Education of China (No.20090076120029).ReferencesJ.
Baldridge and A. Lascarides.
2005.
Probabilistichead-driven parsing for discourse structure.
Pro-ceedings of the Ninth Conference on ComputationalNatural Language Learning.L.
Carlson, D. Marcu, and Ma.
E. Okurowski.
2001.Building a discourse-tagged corpus in the frame-work of rhetorical structure theory.
Proceedings ofthe Second SIG dial Workshop on Discourse and Di-alogue.B.
Dorr.
LCS Verb Database.
Technical Report OnlineSoftware Database, University of Maryland, Col-lege Park, MD,2001.R.
Girju.
2003.
Automatic detection of causal rela-tions for question answering.
In ACL 2003 Work-shops.S.
Blair-Goldensohn.
2007.
Long-Answer Ques-tion Answering and Rhetorical-Semantic Relations.Ph.D.
thesis, Columbia Unviersity.M.
Lapata and A. Lascarides.
2004.
InferringSentence-internal Temporal Relations.
Proceedingsof the North American Chapter of the Assocation ofComputational Linguistics.Z.H.
Lin, M.Y.
Kan and H.T.
Ng.
2009.
RecognizingImplicit Discourse Relations in the Penn DiscourseTreebank.
Proceedings of the 2009 Conference onEMNLP.D.
Marcu and A. Echihabi.
2002.
An UnsupervisedApproach to Recognizing Discourse Relations.
Pro-ceedings of the 40th ACL.E.
Pitler, A. Louis, A. Nenkova.
2009.
Automaticsense prediction for implicit discourse relations intext.
Proceedings of the 47th ACL.E.
Pitler and A. Nenkova.
2009.
Using Syntax to Dis-ambiguate Explicit Discourse Connectives in Text.Proceedings of the ACL-IJCNLP 2009 ConferenceShort Papers.M.
Porter.
1980.
An algorithm for suffix stripping.
InProgram, vol.
14, no.
3, pp.130-137.R.
Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L.Robaldo, A. Joshi, B. Webber.
2008.
The Penn Dis-course TreeBank 2.0.
Proceedings of LREC?08.M.
Saito, K.Yamamoto, S.Sekine.
2006.
UsingPhrasal Patterns to Identify Discourse Relations.Proceeding of the HLTCNA Chapter of the ACL.R.
Soricut and D. Marcu.
Sentence Level DiscourseParsing using Syntactic and Lexical Information.Proceedings of HLT/NAACL 2003.C.
Sporleder and A. Lascarides.
2008.
Using automat-ically labelled examples to classify rhetorical rela-tions: an assessment.
Natural Language Engineer-ing, Volume 14, Issue 03.P.J.
Stone, J. Kirsh, and Cambridge Computer Asso-ciates.
1966.
The General Inquirer: A ComputerApproach to Content Analysis.
MIT Press.B.
Wellner , J. Pustejovsky, C. H. R. S., A. Rumshisky.2006.
Classification of discourse coherence rela-tions: An exploratory study using multiple knowl-edge sources.
Proceedings of the 7th SIGDIALWorkshop on Discourse and Dialogue.F.
Wolf, E. Gibson, A. Fisher, M. Knight.
2005.The Discourse GraphBank: A database of texts an-notated with coherence relations.
Linguistic DataConsortium.1514
