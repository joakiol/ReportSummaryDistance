Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 492?502,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsFeature-Rich Part-of-speech Taggingfor Morphologically Complex Languages: Application to BulgarianGeorgi Georgiev and Valentin ZhikovOntotext AD135 Tsarigradsko Sh., Sofia, Bulgaria{georgi.georgiev,valentin.zhikov}@ontotext.comPetya Osenova and Kiril SimovIICT, Bulgarian Academy of Sciences25A Acad.
G. Bonchev, Sofia, Bulgaria{petya,kivs}@bultreebank.orgPreslav NakovQatar Computing Research Institute, Qatar FoundationTornado Tower, floor 10, P.O.
Box 5825, Doha, Qatarpnakov@qf.org.qaAbstractWe present experiments with part-of-speech tagging for Bulgarian, a Slavic lan-guage with rich inflectional and deriva-tional morphology.
Unlike most previouswork, which has used a small number ofgrammatical categories, we work with 680morpho-syntactic tags.
We combine a largemorphological lexicon with prior linguis-tic knowledge and guided learning from aPOS-annotated corpus, achieving accuracyof 97.98%, which is a significant improve-ment over the state-of-the-art for Bulgarian.1 IntroductionPart-of-speech (POS) tagging is the task of as-signing each of the words in a given piece of text acontextually suitable grammatical category.
Thisis not trivial since words can play different syn-tactic roles in different contexts, e.g., can is anoun in ?I opened a can of coke.?
but a verb in?I can write.?
Traditionally, linguists have classi-fied English words into the following eight basicPOS categories: noun, pronoun, adjective, verb,adverb, preposition, conjunction, and interjection;this list is often extended a bit, e.g., with deter-miners, particles, participles, etc., but the numberof categories considered is rarely more than 15.Computational linguistics works with a largerinventory of POS tags, e.g., the Penn Treebank(Marcus et al 1993) uses 48 tags: 36 for part-of-speech, and 12 for punctuation and currencysymbols.
This increase in the number of tagsis partially due to finer granularity, e.g., thereare special tags for determiners, particles, modalverbs, cardinal numbers, foreign words, existen-tial there, etc., but also to the desire to encodemorphological information as part of the tags.For example, there are six tags for verbs in thePenn Treebank: VB (verb, base form; e.g., sing),VBD (verb, past tense; e.g., sang), VBG (verb,gerund or present participle; e.g., singing), VBN(verb, past participle; e.g., sung) VBP (verb, non-3rd person singular present; e.g., sing), and VBZ(verb, 3rd person singular present; e.g., sings);these tags are morpho-syntactic in nature.
Othercorpora have used even larger tagsets, e.g., theBrown corpus (Kuc?era and Francis, 1967) and theLancaster-Oslo/Bergen (LOB) corpus (Johanssonet al 1986) use 87 and 135 tags, respectively.POS tagging poses major challenges for mor-phologically complex languages, whose tagsetsencode a lot of additional morpho-syntactic fea-tures (for most of the basic POS categories), e.g.,gender, number, person, etc.
For example, theBulTreeBank (Simov et al 2004) for Bulgarianuses 680 tags, while the Prague Dependency Tree-bank (Hajic?, 1998) for Czech has over 1,400 tags.Below we present experiments with POS tag-ging for Bulgarian, which is an inflectional lan-guage with rich morphology.
Unlike most previ-ous work, which has used a reduced set of POStags, we use all 680 tags in the BulTreeBank.
Wecombine prior linguistic knowledge and statisticallearning, achieving accuracy comparable to thatreported for state-of-the-art systems for English.The remainder of the paper is organized as fol-lows: Section 2 provides an overview of relatedwork, Section 3 describes Bulgarian morphology,Section 4 introduces our approach, Section 5 de-scribes the datasets, Section 6 presents our exper-iments in detail, Section 7 discusses the results,Section 8 offers application-specific error analy-sis, and Section 9 concludes and points to somepromising directions for future work.4922 Related WorkMost research on part-of-speech tagging has fo-cused on English, and has relied on the Penn Tree-bank (Marcus et al 1993) and its tagset for train-ing and evaluation.
The task is typically addressedas a sequential tagging problem; one notable ex-ception is the work of Brill (1995), who proposednon-sequential transformation-based learning.A number of different sequential learningframeworks have been tried, yielding 96-97%accuracy: Lafferty et al(2001) experimentedwith conditional random fields (CRFs) (95.7%accuracy), Ratnaparkhi (1996) used a maximumentropy sequence classifier (96.6% accuracy),Brants (2000) employed a hidden Markov model(96.6% accuracy), Collins (2002) adopted an av-eraged perception discriminative sequence model(97.1% accuracy).
All these models fix the orderof inference from left to right.Toutanova et al(2003) introduced a cyclic de-pendency network (97.2% accuracy), where thesearch is bi-directional.
Shen et al(2007) havefurther shown that better results (97.3% accu-racy) can be obtained using guided learning, aframework for bidirectional sequence classifica-tion, which integrates token classification and in-ference order selection into a single learning taskand uses a perceptron-like (Collins and Roark,2004) passive-aggressive classifier to make theeasiest decisions first.
Recently, Tsuruoka et al(2011), proposed a simple perceptron-based clas-sifier applied from left to right but augmentedwith a lookahead mechanism that searches thespace of future actions, yielding 97.3% accuracy.For morphologically complex languages, theproblem of POS tagging typically includes mor-phological disambiguation, which yields a muchlarger number of tags.
For example, for Arabic,Habash and Rambow (2005) used support vectormachines (SVM), achieving 97.6% accuracy with139 tags from the Arabic Treebank (Maamouri etal., 2003).
For Czech, Hajic?
et al(2001) com-bined a hidden Markov model (HMM) with lin-guistic rules, which yielded 95.2% accuracy usingan inventory of over 1,400 tags from the PragueDependency Treebank (Hajic?, 1998).
For Ice-landic, Dredze and Wallenberg (2008) reported92.1% accuracy with 639 tags developed for theIcelandic frequency lexicon (Pind et al 1991),they used guided learning and tag decomposition:First, a coarse POS class is assigned (e.g., noun,verb, adjective), then, additional fine-grainedmorphological features like case, number andgender are added, and finally, the proposed tagsare further reconsidered using non-local features.Similarly, Smith et al(2005) decomposed thecomplex tags into factors, where models for pre-dicting part-of-speech, gender, number, case, andlemma are estimated separately, and then com-posed into a single CRF model; this yielded com-petitive results for Arabic, Korean, and Czech.Most previous work on Bulgarian POS tagginghas started with large tagsets, which were thenreduced.
For example, Dojchinova and Mihov(2004) mapped their initial tagset of 946 tags tojust 40, which allowed them to achieve 95.5%accuracy using the transformation-based learningof Brill (1995), and 98.4% accuracy using manu-ally crafted linguistic rules.
Similarly, Georgievet al(2009), who used maximum entropy andthe BulTreeBank (Simov et al 2004), groupedits 680 fine-grained POS tags into 95 coarse-grained ones, and thus improved their accuracyfrom 90.34% to 94.4%.
Simov and Osenova(2001) used a recurrent neural network to predict(a) 160 morpho-syntactic tags (92.9% accuracy)and (b) 15 POS tags (95.2% accuracy).Some researchers did not reduce the tagset:Savkov et al(2011) used 680 tags (94.7% ac-curacy), and Tanev and Mitkov (2002) used 303tags and the BULMORPH morphological ana-lyzer (Krushkov, 1997), achieving P=R=95%.3 Bulgarian MorphologyBulgarian is an Indo-European language from theSlavic language group, written with the Cyrillicalphabet and spoken by about 9-12 million peo-ple.
It is also a member of the Balkan Sprachbundand thus differs frommost other Slavic languages:it has no case declensions, uses a suffixed definitearticle (which has a short and a long form for sin-gular masculine), and lacks verb infinitive forms.It further uses special evidential verb forms to ex-press unwitnessed, retold, and doubtful activities.Bulgarian is an inflective language with veryrich morphology.
For example, Bulgarian verbshave 52 synthetic wordforms on average, whilepronouns have altogether more than ten grammat-ical features (not necessarily shared by all pro-nouns), including case, gender, person, number,definiteness, etc.493This rich morphology inevitably leads to ambi-guity proliferation; our analysis of BulTreeBankshows four major types of ambiguity:1.
Between the wordforms of the same lexeme,i.e., in the paradigm.
For example, divana,an inflected form of divan (?sofa?, mascu-line), can mean (a) ?the sofa?
(definite, singu-lar, short definite article) or (b) a count form,e.g., as in dva divana (?two sofas?).2.
Between two or more lexemes, i.e., conver-sion.
For example, kato can be (a) a subor-dinator meaning ?as, when?, or (b) a preposi-tion meaning ?like, such as?.3.
Between a lexeme and an inflected wordformof another lexeme, i.e., across-paradigms.For example, politika can mean (a) ?thepolitician?
(masculine, singular, definite,short definite article) or (b) ?politics?
(fem-inine, singular, indefinite).4.
Between the wordforms of two or morelexemes, i.e., across-paradigms and quasi-conversion.
For example, vrvi can mean(a) ?walks?
(verb, 2nd or 3rd person, presenttense) or (b) ?strings, laces?
(feminine, plu-ral, indefinite).Some morpho-syntactic ambiguities in Bulgar-ian are occasional, but many are systematic, e.g.,neuter singular adjectives have the same formsas adverbs.
Overall, most ambiguities are local,and thus arguably resolvable using n-grams, e.g.,compare hubavo dete (?beautiful child?
), wherehubavo is a neuter adjective, and ?Pe hubavo.?
(?I sing beautifully.?
), where it is an adverb ofmanner.
Other ambiguities, however, are non-local and may require discourse-level analysis,e.g., ?Vidh go.?
can mean ?I saw him.
?, wherego is a masculine pronoun, or ?I saw it.
?, whereit is a neuter pronoun.
Finally, there are ambi-guities that are very hard or even impossible1 toresolve, e.g., ?Deteto vleze veselo.?
can meanboth ?The child came in happy.?
(veselo is an ad-jective) and ?The child came in happily.?
(it is anadverb); however, the latter is much more likely.1The problem also exists for English, e.g., the annotatorsof the Penn Treebank were allowed to use tag combinationsfor inherently ambiguous cases: JJ|NN (adjective or noun asprenominal modifier), JJ|VBG (adjective or gerund/presentparticiple), JJ|VBN (adjective or past participle), NN|VBG(noun or gerund), and RB|RP (adverb or particle).In many cases, strong domain preferences existabout how various systematic ambiguities shouldbe resolved.
We made a study for the newswiredomain, analyzing a corpus of 546,029 words,and we found that ambiguity type 2 (lexeme-lexeme) prevailed for functional parts-of-speech,while the other types were more frequent for in-flecting parts-of-speech.
Below we show the mostfrequent types of morpho-syntactic ambiguitiesand their frequency in our corpus:?
na: preposition (?of?)
vs. emphatic particle,with a ratio of 28,554 to 38;?
da: auxiliary particle (?to?)
vs. affirmativeparticle, with a ratio of 12,035 to 543;?
e: 3rd person present auxiliary verb (?to be?)vs.
particle (?well?)
vs. interjection (?wow?
),with a ratio of 9,136 to 21 to 5;?
singular masculine noun with a short definitearticle vs. count form of a masculine noun,with a ratio of 6,437 to 1,592;?
adverb vs. neuter singular adjective, with aratio of 3,858 to 1,753.Overall, the following factors should be takeninto account when modeling Bulgarian morpho-syntax: (1) locality vs. non-locality of grammat-ical features, (2) interdependence of grammaticalfeatures, and (3) domain-specific preferences.4 MethodWe used the guided learning framework describedin (Shen et al 2007), which has yielded state-of-the-art results for English and has been success-fully applied to other morphologically complexlanguages such as Icelandic (Dredze and Wallen-berg, 2008); we found it quite suitable for Bul-garian as well.
We used the feature set defined in(Shen et al 2007), which includes the following:1.
The feature set of Ratnaparkhi (1996), in-cluding prefix, suffix and lexical, as well assome bigram and trigram context features;2.
Feature templates as in (Ratnaparkhi, 1996),which have been shown helpful in bidirec-tional search;3.
More bigram and trigram features and bi-lexical features as in (Shen et al 2007).Note that we allowed prefixes and suffixes oflength up to 9, as in (Toutanova et al 2003) and(Tsuruoka and Tsujii, 2005).494We further extended the set of features withthe tags proposed for the current word token by amorphological lexicon, which maps words to pos-sible tags; it is exhaustive, i.e., the correct tag isalways among the suggested ones for each token.We also used 70 linguistically-motivated, high-precision rules in order to further reduce the num-ber of possible tags suggested by the lexicon.The rules are similar to those proposed by Hin-richs and Trushkina (2004) for German; we im-plemented them as constraints in the CLaRK sys-tem (Simov et al 2003).Here is an example of a rule: If a wordformis ambiguous between a masculine count noun(Ncmt) and a singular short definite masculinenoun (Ncmsh), the Ncmt tag should be chosen ifthe previous token is a numeral or a number.The 70 rules were developed by linguists basedon observations over the training dataset only.They target primarily the most frequent cases ofambiguity, and to a lesser extent some infrequentbut very problematic cases.
Some rules operateover classes of words, while other refer to partic-ular wordforms.
The rules were designed to be100% accurate on our training dataset; our exper-iments show that they are also 100% accurate onthe test and on the development dataset.Note that some of the rules are dependent onothers, and thus the order of their cascaded appli-cation is important.
For example, the wordformis ambiguous between an accusative feminine sin-gular short form of a personal pronoun (?her?)
andan interjection (?wow?).
To handle this properly,the rule for interjection, which targets sentenceinitial positions, followed by a comma, needs tobe executed first.
The rule for personal pronounsis only applied afterwards.Word TagsTo$i Ppe-os3mobaqe Cc; Ddnma Afsi; Vnitf-o3s; Vnitf-r3s;Vpitf-o2s; Vpitf-o3s; Vpitf-r3svzmonost Ncfsida Ta;Txsledi Ncfpi; Vpitf-o2s; Vpitf-o3s; Vpitf-r3s;Vpitz?2s.
.
.
.
.
.Table 1: Sample fragment showing the possible tagssuggested by the lexicon.
The tags that are furtherfiltered by the rules are in italic; the correct tag is bold.The rules are quite efficient at reducing the POSambiguity.
On the test dataset, before the rule ap-plication, 34.2% of the tokens (excluding punctu-ation) had more than one tag in our morphologicallexicon.
This number is reduced to 18.5% afterthe cascaded application of the 70 linguistic rules.Table 1 illustrates the effect of the rules on a smallsentence fragment.
In this example, the rules haveleft only one tag (the correct one) for three of theambiguous words.
Since the rules in essence de-crease the average number of tags per token, wecalculated that the lexicon suggests 1.6 tags pertoken on average, and after the application of therules this number decreases to 1.44 per token.5 Datasets5.1 BulTreeBankWe used the latest version of the BulTree-Bank (Simov and Osenova, 2004), which contains20,556 sentences and 321,542 word tokens (fourtimes less than the English Penn Treebank), anno-tated using a total of 680 unique morpho-syntactictags.
See (Simov et al 2004) for a detailed de-scription of the BulTreeBank tagset.We split the data into training/development/testas shown in Table 2.
Note that only 552 of all 680tag types were used in the training dataset, andthe development and the test datasets combinedcontain a total of 128 new tag types that were notseen in the training dataset.
Moreover, 32% of theword types in the development dataset and 31%of those in the testing dataset do not occur in thetraining dataset.
Thus, data sparseness is an issueat two levels: word-level and tag-level.Dataset Sentences Tokens Types TagsTrain 16,532 253,526 38,659 552Dev 2,007 32,995 9,635 425Test 2,017 35,021 9,627 435Table 2: Statistics about our datasets.5.2 Morphological LexiconIn order to alleviate the data sparseness issues,we further used a large morphological lexicon forBulgarian, which is an extended version of thedictionary described in (Popov et al 1998) and(Popov et al 2003).
It contains over 1.5M in-flected wordforms (for 110K lemmata and 40Kproper names), each mapped to a set of possiblemorpho-syntactic tags.4956 Experiments and EvaluationState-of-the-art POS taggers for English typicallybuild a lexicon containing all tags a word type hastaken in the training dataset; this lexicon is thenused to limit the set of possible tags that an inputtoken can be assigned, i.e., it imposes a hard con-straint on the possibilities explored by the POStagger.
For example, if can has only been taggedas a verb and as a noun in the training dataset,it will be only assigned those two tags at testtime; other tags such as adjective, adverb and pro-noun will not be considered.
Out-of-vocabularywords, i.e., those that were not seen in the train-ing dataset, are constrained as well, e.g., to a smallset of frequent open-class tags.In our experiments, we used a morphologicallexicon that is much larger than what could bebuilt from the training corpus only: building alexicon from the training corpus only is of lim-ited utility since one can hardly expect to see inthe training corpus all 52 synthetic forms a verbcan possibly have.
Moreover, we did not use thetags listed in the lexicon as hard constraints (ex-cept in one of our baselines); instead, we experi-mented with a different, non-restrictive approach:we used the lexicon?s predictions as features orsoft constraints, i.e., as suggestions only, thus al-lowing each token to take any possible tag.
Notethat for both known and out-of-vocabulary wordswe used all 680 tags rather than the 552 tags ob-served in the training dataset; we could afford toexplore this huge search space thanks to the effi-ciency of the guided learning framework.
Allow-ing all 680 tags on training helped the model byexposing it to a larger set of negative examples.We combined these lexicon features with stan-dard features extracted from the training corpus.We further experimented with the 70 contextuallinguistic rules, using them (a) as soft and (b) ashard constraints.
Finally, we set four baselines:three that do not use the lexicon and one that does.Accuracy (%)# Baselines (token-level)1 MFT + unknowns are wrong 78.102 MFT + unknowns are Ncmsi 78.523 MFT + guesser for unknowns 79.494 MFT + lexicon tag-classes 94.40Table 3: Most-frequent-tag (MFT) baselines.6.1 BaselinesFirst, we experimented with the most-frequent-tag baseline, which is standard for POS tagging.This baseline ignores context altogether and as-signs each word type the POS tag it was mostfrequently seen with in the training dataset; tiesare broken randomly.
We coped with word typesnot seen in the training dataset using three sim-ple strategies: (a) we considered them all wrong,(b) we assigned them Ncmsi, which is the mostfrequent open-class tag in the training dataset, or(c) we used a very simple guesser, which assignedNcfsi, Ncnsi, Ncfsi, and Ncmsf, if the target wordended by -a, -o, -i, and -t, respectively, other-wise, it assigned Ncmsi.
The results are shownin lines 1-3 of Table 3: we can see that the token-level accuracy ranges in 78-80% for (a)-(c), whichis relatively high, given that we use a large inven-tory of 680 morpho-syntactic tags.We further tried a baseline that uses the above-described morphological lexicon, in addition tothe training dataset.
We first built two frequencylists, containing respectively (1) the most frequenttag in the training dataset for each word type, asbefore, and (2) the most frequent tag in the train-ing dataset for each class of tags that can be as-signed to some word type, according to the lexi-con.
For example, the most frequent tag for poli-tika is Ncfsi, and the most frequent tag for thetag-class {Ncmt;Ncmsi} is Ncmt.Given a target word type, this new baseline firsttries to assign it the most frequent tag from thefirst list.
If this is not possible, which happens(i) in case of ties or (ii) when the word type wasnot seen on training, it extracts the tag-class fromthe lexicon and consults the second list.
If thereis a single most frequent tag in the corpus for thistag-class, it is assigned; otherwise a random tagfrom this tag-class is selected.Line 4 of Table 3 shows that this latter baselineachieves a very high accuracy of 94.40%.
Note,however, that this is over-optimistic: the lexiconcontains a tag-class for each word type in our test-ing dataset, i.e., while there can be word typesnot seen in the training dataset, there are no wordtypes that are not listed in the lexicon.
Thus, thishigh accuracy is probably due to a large extentto the scale and quality of our morphological lexi-con, and it might not be as strong with smaller lex-icons; we plan to investigate this in future work.4966.2 Lexicon Tags as Soft ConstraintsWe experimented with three types of features:1.
Word-related features only;2.
Word-related features + the tags suggestedby the lexicon;3.
Word-related features + the tags suggestedby the lexicon but then further filtered usingthe 70 contextual linguistic rules.Table 4 shows the sentence-level and the token-level accuracy on the test dataset for the threekinds of features: shown on lines 1, 3 and 4, re-spectively.
We can see that using the tags pro-posed by the lexicon as features (lines 3 and 4)has a major positive impact, yielding up to 49%error reduction at the token-level and up to 37%at the sentence-level, as compared to using word-related features alone (line 1).Interestingly, filtering the tags proposed by thelexicon using the 70 contextual linguistic rulesyields a minor decrease in accuracy both at theword token-level and at the sentence-level (com-pare line 4 to line 2).
This is surprising sincethe linguistic rules are extremely reliable: theywere designed to be 100% accurate on the train-ing dataset, and we found them experimentally tobe 100% correct on the development and on thetesting dataset as well.One possible explanation is that by limiting theset of available tags for a given token at trainingtime, we prevent the model from observing somepotentially useful negative examples.
We testedthis hypothesis by using the unfiltered lexiconpredictions at training time but then making useof the filtered ones at testing time; the results areshown on line 5.
We can observe a small increasein accuracy compared to line 4: from 97.80% to97.84% at the token-level, and from 70.30% to70.40% at the sentence-level.
Although these dif-ferences are tiny, they suggest that having morenegative examples at training is helpful.We can conclude that using the lexicon as asource of soft constraints has a major positive im-pact, e.g., because it provides access to impor-tant external knowledge that is complementaryto what can be learned from the training corpusalone; the improvements when using linguisticrules as soft constraints are more limited.6.3 Linguistic Rules as Hard ConstraintsNext, we experimented with using the suggestionsof the linguistic rules as hard constraints.
Table 4shows that this is a very good idea.
Comparingline 1 to line 2, which do not use the morpholog-ical lexicon, we can see very significant improve-ments: from 95.72% to 97.20% at the token-leveland from 52.95% to 64.50% at the sentence-level.The improvements are smaller but still consistentwhen the morphological lexicon is used: compar-ing lines 3 and 4 to lines 6 and 7, respectively, wesee an improvement from 97.83% to 97.91% andfrom 97.80% to 97.93% at the token-level, andabout 1% absolute at the sentence-level.6.4 Increasing the Beam SizeFinally, we increased the beam size of guidedlearning from 1 to 3 as in (Shen et al 2007).Comparing line 7 to line 8 in Table 4, we can seethat this yields further token-level improvement:from 97.93% to 97.98%.7 DiscussionTable 5 compares our results to previously re-ported evaluation results for Bulgarian.
Thefirst four lines show the token-level accuracy forstandard POS tagging tools trained and evalu-ated on the BulTreeBank:2 TreeTagger (Schmid,1994), which uses decision trees, TnT (Brants,2000), which uses a hidden Markov model,SVMtool (Gime?nez and Ma`rquez, 2004), whichis based on support vector machines, andACOPOST (Schro?der, 2002), implementing thememory-based model of Daelemans et al(1996).The following lines report the token-level accu-racy reported in previous work, as compared toour own experiments using guided learning.We can see that we outperform by a very largemargin (92.53% vs. 97.98%, which represents73% error reduction) the systems from the firstfour lines, which are directly comparable to ourexperiments: they are trained and evaluated on theBulTreeBank using the full inventory of 680 tags.We further achieved statistically significant im-provement (p < 0.0001; Pearson?s chi-squaredtest (Plackett, 1983)) over the best pervious resulton 680 tags: from 94.65% to 97.98%, which rep-resents 62.24% error reduction at the token-level.2We used the pre-trained TreeTagger; for the rest, we re-port the accuracy given on the Webpage of the BulTreeBank:www.bultreebank.org/taggers/taggers.html497Lexicon Linguistic Rules (applied to filter): Beam Accuracy (%)# (source of) (a) the lexicon features (b) the output tags size Sentence-level Token-level1 ?
?
?
1 52.95 95.722 ?
?
yes 1 64.50 97.203 features ?
?
1 70.40 97.834 features yes ?
1 70.30 97.805 features yes, for test only ?
1 70.40 97.846 features ?
yes 1 71.34 97.917 features yes yes 1 71.69 97.938 features yes yes 3 71.94 97.98Table 4: Evaluation results on the test dataset.
Line 1 shows the evaluation results when using features derivedfrom the text corpus only; these features are used by all systems in the table.
Line 2 further uses the contextuallinguistic rules to limit the set of possible POS tags that can be predicted.
Note that these rules (1) consult thelexicon, and (2) always predict a single POS tag.
Line 3 uses the POS tags listed in the lexicon as features, i.e.,as soft suggestions only.
Line 4 is like line 3, but the list of feature-tags proposed by the lexicon is filtered bythe contextual linguistic rules.
Line 5 is like line 4, but the linguistic rules filtering is only applied at test time;it is not done on training.
Lines 6 and 7 are similar to lines 3 and 4, respectively, but here the linguistic rulesare further applied to limit the set of possible POS tags that can be predicted, i.e., the rules are used as hardconstraints.
Finally, line 8 is like line 7, but here the beam size is increased to 3.Overall, we improved over almost all previ-ously published results.
Our accuracy is sec-ond only to the manual rules approach of Do-jchinova and Mihov (2004).
Note, however, thatthey used 40 tags only, i.e., their inventory is 17times smaller than ours.
Moreover, they have op-timized their tagset specifically to achieve veryhigh POS tagging accuracy by choosing not to at-tempt to resolve some inherently hard systematicambiguities, e.g., they do not try to choose be-tween second and third person past singular verbs,whose inflected forms are identical in Bulgarianand hard to distinguish when the subject is notpresent (Bulgarian is a pro-drop language).In order to compare our results more closelyto the smaller tagsets in Table 5, we evaluatedour best model with respect to (a) the first letterof the tag only (which is part-of-speech only, nomorphological information; 13 tags), e.g., Ncmsfbecomes N, and (b) the first two letters of thetag (POS + limited morphological information;49 tags), e.g., Ncmsf becomes Nc.
This yielded99.30% accuracy for (a) and 98.85% for (b).The latter improves over (Dojchinova and Mihov,2004), while using a bit larger number of tags.Our best token-level accuracy of 97.98% iscomparable and even slightly better than the state-of-the-art results for English: 97.33% when usingPenn Treebank data only (Shen et al 2007), and97.50% for Penn Treebank plus some additionalunlabeled data (S?gaard, 2011).
Of course, ourresults are only indirectly comparable to English.Still, our performance is impressive because(1) our model is trained on 253,526 tokens onlywhile the standard training sections 0-18 of thePenn Treebank contain a total of 912,344 tokens,i.e., almost four times more, and (2) we predict680 rather than just 48 tags as for the Penn Tree-bank, which is 14 times more.Note, however, that (1) we used a large exter-nal morphological lexicon for Bulgarian, whichyielded about 50% error reduction (without it,our accuracy was 95.72% only), and (2) ourtrain/dev/test sentences are generally shorter, andthus arguably simpler for a POS tagger to analyze:we have 17.4 words per test sentence in the Bul-TreeBank vs. 23.7 in the Penn Treebank.Our results also compare favorably to the state-of-the-art results for other morphologically com-plex languages that use large tagsets, e.g., 95.2%for Czech with 1,400+ tags (Hajic?
et al 2001),92.1% for Icelandic with 639 tags (Dredze andWallenberg, 2008), 97.6% for Arabic with 139tags (Habash and Rambow, 2005).8 Error AnalysisIn this section, we present error analysis with re-spect to the impact of the POS tagger?s perfor-mance on other processing steps in a natural lan-guage processing pipeline, such as lemmatizationand syntactic dependency parsing.First, we explore the most frequently confusedpairs of tags for our best-performing POS taggingsystem; these are shown in Table 6.498AccuracyTool/Authors Method # Tags (token-level, %)*TreeTagger Decision Trees 680 89.21*ACOPOST Memory-based Learning 680 89.91*SVMtool Support Vector Machines 680 92.22*TnT Hidden Markov Model 680 92.53(Georgiev et al 2009) Maximum Entropy 680 90.34(Simov and Osenova, 2001) Recurrent Neural Network 160 92.87(Georgiev et al 2009) Maximum Entropy 95 94.43(Savkov et al 2011) SVM + Lexicon + Rules 680 94.65(Tanev and Mitkov, 2002) Manual Rules 303 95.00(=P=R)(Simov and Osenova, 2001) Recurrent Neural Network 15 95.17(Dojchinova and Mihov, 2004) Transformation-based Learning 40 95.50(Dojchinova and Mihov, 2004) Manual Rules + Lexicon 40 98.40Guided Learning 680 95.72Guided Learning + Lexicon 680 97.83This work Guided Learning + Lexicon + Rules 680 97.98Guided Learning + Lexicon + Rules 49 98.85Guided Learning + Lexicon + Rules 13 99.30Table 5: Comparison to previous work for Bulgarian.
The first four lines report evaluation results for variousstandard POS tagging tools, which were retrained and evaluated on the BulTreeBank.
The following lines reporttoken-level accuracy for previously published work, as compared to our own experiments using guided learning.We can see that most of the wrong tags sharethe same part-of-speech (indicated by the initialuppercase letter), such as V for verb, N for noun,etc.
This means that most errors refer to the mor-phosyntactic features.
For example, personal orimpersonal verb; definite or indefinite femininenoun; singular or plural masculine adjective, etc.At the same time, there are also cases, where theerror has to do with the part-of-speech label itself.For example, between an adjective and an adverb,or between a numeral and an indefinite pronoun.We want to use the above tagger to develop(1) a rule-based lemmatizer, using the morpholog-ical lexicon, e.g., as in (Plisson et al 2004), and(2) a dependency parser like MaltParser (Nivre etal., 2007), trained on the dependency part of theBulTreeBank.
We thus study the potential impactof wrong tags on the performance of these tools.The lemmatizer relies on the lexicon and usesstring transformation functions defined via twooperations ?
remove and concatenate:if tag = Tag then{remove OldEnd; concatenate NewEnd}where Tag is the tag of the wordform, OldEnd isthe string that has to be removed from the end ofthe wordform, and NewEnd is the string that hasto be concatenated to the beginning of the word-form in order to produce the lemma.Here is an example of such a rule:if tag = Vpitf-o1s then{remove oh; concatenate a}The application of the above rule to the pastsimple verb form qetoh (?I read?)
would removeoh, and then concatenate a.
The result would bethe correct lemma qeta (?to read?
).Such rules are generated for each wordform inthe morphological lexicon; the above functionalrepresentation allows for compact representationin a finite state automaton.
Similar rules are ap-plied to the unknown words, where the lemma-tizer tries to guess the correct lemma.Obviously, the applicability of each rule cru-cially depends on the output of the POS tagger.If the tagger suggests the correct tag, then thewordform would be lemmatized correctly.
Notethat, in some cases of wrongly assigned POS tagsin a given context, we might still get the correctlemma.
This is possible in the majority of theerroneous cases in which the part-of-speech hasbeen assigned correctly, but the wrong grammat-ical alternative has been selected.
In such cases,the error does not influence lemmatization.In order to calculate the proportion of suchcases, we divided each tag into two parts:(a) grammatical features that are common for allwordforms of a given lemma, and (b) features thatare specific to the wordform.499Freq.
Gold Tag Proposed Tag43 Ansi Dm23 Vpitf-r3s Vnitf-r3s16 Npmsh Npmsi14 Vpiif-r3s Vniif-r3s13 Npfsd Npfsi12 Dm Ansi12 Vpitcam-smi Vpitcao-smi12 Vpptf-r3p Vpitf-r3p11 Vpptf-r3s Vpptf-o3s10 Mcmsi Pfe-os-mi10 Ppetas3n Ppetas3m10 Ppetds3f Psot?3?f9 Npnsi Npnsd9 Vpptf-o3s Vpptf-r3s8 Dm A-pi8 Ppxts Ppxtd7 Mcfsi Pfe-os-fi7 Npfsi Npfsd7 Ppetas3m Ppetas3n7 Vnitf-r3s Vpitf-r3s7 Vpitcam-p-i Vpitcao-p-iTable 6: Most frequently confused pairs of tags.The part-of-speech features are always deter-mined by the lemma.
For example, Bulgarianverbs have the lemma features aspect and tran-sitivity.
If they are correct, then the lemma is pre-dicted also correctly, regardless of whether cor-rect or wrong on the grammatical features.
Forexample, if the verb participle form (aorist orimperfect) has its correct aspect and transitivity,then it is lemmatized also correctly, regardlessof whether the imperfect or aorist features wereguessed correctly; similarly, for other error types.We evaluated these cases for the 711 errors in ourexperiment, and we found that 206 of them (about29%) were non-problematic for lemmatization.For the MaltParser, we encode most of thegrammatical features of the wordforms as spe-cific features for the parser.
Hence, it is muchharder to evaluate the problematic cases due tothe tagger.
Still, we were able to make an es-timation of some cases.
Our strategy was to ig-nore the grammatical features that do not alwayscontribute to the syntactic behavior of the word-forms.
Such grammatical features for the verbsare aspect and tense.
Thus, proposing perfectiveinstead of imperfective for a verb or present in-stead of past tense would not cause problems forthe MaltParser.
Among our 711 errors, 190 cases(or about 27%) were not problematic for parsing.Finally, we should note that there are two spe-cial classes of tokens for which it is generallyhard to predict some of the grammatical features:(1) abbreviations and (2) numerals written withdigits.
In sentences, they participate in agreementrelations only if they are pronounced as wholephrases; unfortunately, it is very hard for the tag-ger to guess such relations since it does not haveat its disposal enough features, such as the inflec-tion of the numeral form, that might help detectand use the agreement pattern.9 Conclusion and Future WorkWe have presented experiments with part-of-speech tagging for Bulgarian, a Slavic languagewith rich inflectional and derivational morphol-ogy.
Unlike most previous work for this language,which has limited the number of possible tags, weused a very rich tagset of 680 morpho-syntactictags as defined in the BulTreeBank.
By com-bining a large morphological lexicon with priorlinguistic knowledge and guided learning from aPOS-annotated corpus, we achieved accuracy of97.98%, which is a significant improvement overthe state-of-the-art for Bulgarian.
Our token-levelaccuracy is also comparable to the best results re-ported for English.In future work, we want to experiment with aricher set of features, e.g., derived from unlabeleddata (S?gaard, 2011) or from the Web (Umansky-Pesin et al 2010; Bansal and Klein, 2011).
Wefurther plan to explore ways to decompose thecomplex Bulgarian morpho-syntactic tags, e.g., asproposed in (Simov and Osenova, 2001) and(Smith et al 2005).
Modeling long-distancesyntactic dependencies (Dredze and Wallenberg,2008) is another promising direction; we believethis can be implemented efficiently using poste-rior regularization (Graca et al 2009) or expecta-tion constraints (Bellare et al 2009).AcknowledgmentsWe would like to thank the anonymous reviewersfor their useful comments, which have helped usimprove the paper.The research presented above has been par-tially supported by the EU FP7 project 231720EuroMatrixPlus, and by the SmartBook project,funded by the Bulgarian National Science Fundunder grant D002-111/15.12.2008.500ReferencesMohit Bansal and Dan Klein.
2011.
Web-scale fea-tures for full-scale parsing.
In Proceedings of the49th Annual Meeting of the Association for Com-putational Linguistics: Human Language Technolo-gies, ACL-HLT ?10, pages 693?702, Portland, Ore-gon, USA.Kedar Bellare, Gregory Druck, and Andrew McCal-lum.
2009.
Alternating projections for learningwith expectation constraints.
In Proceedings of the25th Conference on Uncertainty in Artificial Intel-ligence, UAI ?09, pages 43?50, Montreal, Quebec,Canada.Thorsten Brants.
2000.
TnT ?
a statistical part-of-speech tagger.
In Proceedings of the Sixth AppliedNatural Language Processing, ANLP ?00, pages224?231, Seattle, Washington, USA.Eric Brill.
1995.
Transformation-based error-drivenlearning and natural language processing: a casestudy in part-of-speech tagging.
Comput.
Linguist.,21:543?565.Michael Collins and Brian Roark.
2004.
Incremen-tal parsing with the perceptron algorithm.
In Pro-ceedings of the 42nd Meeting of the Association forComputational Linguistics, Main Volume, ACL ?04,pages 111?118, Barcelona, Spain.Michael Collins.
2002.
Discriminative training meth-ods for hidden Markov models: theory and experi-ments with perceptron algorithms.
In Proceedingsof the Conference on Empirical Methods in Natu-ral Language Processing, EMNLP ?02, pages 1?8,Philadelphia, PA, USA.Walter Daelemans, Jakub Zavrel, Peter Berck, andSteven Gillis.
1996.
MBT: A memory-based partof speech tagger generator.
In Eva Ejerhed andIdo Dagan, editors, Fourth Workshop on Very LargeCorpora, pages 14?27, Copenhagen, Denmark.Veselka Dojchinova and Stoyan Mihov.
2004.
Highperformance part-of-speech tagging of Bulgarian.In Christoph Bussler and Dieter Fensel, editors,AIMSA, volume 3192 of Lecture Notes in ComputerScience, pages 246?255.
Springer.Mark Dredze and Joel Wallenberg.
2008.
Icelandicdata driven part of speech tagging.
In Proceedingsof the 44th Annual Meeting of the Association ofComputational Linguistics: Short Papers, ACL ?08,pages 33?36, Columbus, Ohio, USA.Georgi Georgiev, Preslav Nakov, Petya Osenova, andKiril Simov.
2009.
Cross-lingual adaptation asa baseline: adapting maximum entropy models toBulgarian.
In Proceedings of the RANLP?09 Work-shop on Adaptation of Language Resources andTechnology to New Domains, AdaptLRTtoND ?09,pages 35?38, Borovets, Bulgaria.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2004.
SVMTool:A general POS tagger generator based on supportvector machines.
In Proceedings of the 4th Inter-national Conference on Language Resources andEvaluation, LREC ?04, Lisbon, Portugal.Joao Graca, Kuzman Ganchev, Ben Taskar, and Fer-nando Pereira.
2009.
Posterior vs parameter spar-sity in latent variable models.
In Yoshua Bengio,Dale Schuurmans, John D. Lafferty, ChristopherK.
I. Williams, and Aron Culotta, editors, Advancesin Neural Information Processing Systems 22, NIPS?09, pages 664?672.
Curran Associates, Inc., Van-couver, British Columbia, Canada.Nizar Habash and Owen Rambow.
2005.
Arabic to-kenization, part-of-speech tagging and morpholog-ical disambiguation in one fell swoop.
In Proceed-ings of the 43rd Annual Meeting of the Associa-tion for Computational Linguistics, ACL ?05, pages573?580, Ann Arbor, Michigan.Jan Hajic?, Pavel Krbec, Pavel Kve?ton?, Karel Oliva,and Vladim?
?r Petkevic?.
2001.
Serial combinationof rules and statistics: A case study in Czech tag-ging.
In Proceedings of the 39th Annual Meetingof the Association for Computational Linguistics,ACL ?01, pages 268?275, Toulouse, France.Jan Hajic?.
1998.
Building a Syntactically AnnotatedCorpus: The Prague Dependency Treebank.
In EvaHajic?ova?, editor, Issues of Valency and Meaning.Studies in Honor of Jarmila Panevova?, pages 12?19.
Prague Karolinum, Charles University Press.Erhard W. Hinrichs and Julia S. Trushkina.
2004.Forging agreement: Morphological disambiguationof noun phrases.
Research on Language & Compu-tation, 2:621?648.Stig Johansson, Eric Atwell, Roger Garside, and Geof-frey Leech, 1986.
The Tagged LOB Corpus: Users?manual.
ICAME, The Norwegian Computing Cen-tre for the Humanities, Bergen University, Norway.Hristo Krushkov.
1997.
Modelling and building ma-chine dictionaries and morphological processors(in Bulgarian).
Ph.D. thesis, University of Plov-div, Faculty of Mathematics and Informatics, Plov-div, Bulgaria.Henry Kuc?era and Winthrop Nelson Francis.
1967.Computational analysis of present-day AmericanEnglish.
Brown University Press, Providence, RI.John D. Lafferty, Andrew McCallum, and FernandoC.
N. Pereira.
2001.
Conditional random fields:Probabilistic models for segmenting and labelingsequence data.
In Proceedings of the 18th Inter-national Conference on Machine Learning, ICML?01, pages 282?289, San Francisco, CA, USA.Mohamed Maamouri, Ann Bies, Hubert Jin, and TimBuckwalter.
2003.
Arabic Treebank: Part 1 v 2.0.LDC2003T06.Mitchell P. Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of English: the Penn Treebank.
Com-put.
Linguist., 19:313?330.501Joakim Nivre, Johan Hall, Jens Nilsson, AtanasChanev, Gu?lsen Eryigit, Sandra Ku?bler, SvetoslavMarinov, and Erwin Marsi.
2007.
MaltParser:A language-independent system for data-driven de-pendency parsing.
Natural Language Engineering,13(2):95?135.Jo?rgen Pind, Fridrik Magnu?sson, and Stefa?n Briem.1991.
The Icelandic frequency dictionary.
Techni-cal report, The Institute of Lexicography, Universityof Iceland, Reykjavik, Iceland.Robin L. Plackett.
1983.
Karl Pearson and the Chi-Squared Test.
International Statistical Review / Re-vue Internationale de Statistique, 51(1):59?72.Joe?l Plisson, Nada Lavrac?, and Dunja Mladenic?.
2004.A rule based approach to word lemmatization.
InProceedings of the 7th International Multiconfer-ence: Information Society, IS ?2004, pages 83?86,Ljubljana, Slovenia.Dimitar Popov, Kiril Simov, and Svetlomira Vidinska.1998.
Dictionary of Writing, Pronunciation andPunctuation of Bulgarian Language (in Bulgarian).Atlantis KL, Sofia, Bulgaria.Dimityr Popov, Kiril Simov, Svetlomira Vidinska, andPetya Osenova.
2003.
Spelling Dictionary of Bul-garian.
Nauka i izkustvo, Sofia, Bulgaria.Adwait Ratnaparkhi.
1996.
A maximum entropymodel for part-of-speech tagging.
In Eva Ejerhedand Ido Dagan, editors, Fourth Workshop on VeryLarge Corpora, pages 133?142, Copenhagen, Den-mark.Aleksandar Savkov, Laska Laskova, Petya Osenova,Kiril Simov, and Stanislava Kancheva.
2011.A web-based morphological tagger for Bulgarian.In Daniela Majchra?kova?
and Radovan Garab?
?k,editors, Slovko 2011.
Sixth International Confer-ence.
Natural Language Processing, Multilingual-ity, pages 126?137, Modra/Bratislava, Slovakia.Helmut Schmid.
1994.
Probabilistic part-of-speechtagging using decision trees.
In International Con-ference on New Methods in Language Processing,pages 44?49, Manchester, UK.Ingo Schro?der.
2002.
A case study in part-of-speech-tagging using the ICOPOST toolkit.
Technical Re-port FBI-HH-M-314/02, Department of ComputerScience, University of Hamburg.Libin Shen, Giorgio Satta, and Aravind Joshi.
2007.Guided learning for bidirectional sequence classi-fication.
In Proceedings of the 45th Annual Meet-ing of the Association of Computational Linguistics,ACL ?07, pages 760?767, Prague, Czech Republic.Kiril Simov and Petya Osenova.
2001.
A hybridsystem for morphosyntactic disambiguation in Bul-garian.
In Proceedings of the EuroConference onRecent Advances in Natural Language Processing,RANLP ?01, pages 5?7, Tzigov chark, Bulgaria.Kiril Simov and Petya Osenova.
2004.
BTB-TR04:BulTreeBank morphosyntactic annotation of Bul-garian texts.
Technical Report BTB-TR04, Bulgar-ian Academy of Sciences.Kiril Ivanov Simov, Alexander Simov, MilenKouylekov, Krasimira Ivanova, Ilko Grigorov, andHristo Ganev.
2003.
Development of corporawithin the CLaRK system: The BulTreeBankproject experience.
In Proceedings of the 10th con-ference of the European chapter of the Associationfor Computational Linguistics, EACL ?03, pages243?246, Budapest, Hungary.Kiril Simov, Petya Osenova, and Milena Slavcheva.2004.
BTB-TR03: BulTreeBank morphosyntac-tic tagset.
Technical Report BTB-TR03, BulgarianAcademy of Sciences.Noah A. Smith, David A. Smith, and Roy W. Tromble.2005.
Context-based morphological disambigua-tion with random fields.
In Proceedings of Hu-man Language Technology Conference and Confer-ence on Empirical Methods in Natural LanguageProcessing, pages 475?482, Vancouver, BritishColumbia, Canada.Anders S?gaard.
2011.
Semi-supervised condensednearest neighbor for part-of-speech tagging.
In Pro-ceedings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics, ACL-HLT ?10,pages 48?52, Portland, Oregon, USA.Hristo Tanev and Ruslan Mitkov.
2002.
Shallowlanguage processing architecture for Bulgarian.
InProceedings of the 19th International Conferenceon Computational Linguistics, COLING ?02, pages1?7, Taipei, Taiwan.Kristina Toutanova, Dan Klein, Christopher D. Man-ning, and Yoram Singer.
2003.
Feature-richpart-of-speech tagging with a cyclic dependencynetwork.
In Proceedings of the Conference ofthe North American Chapter of the Associationfor Computational Linguistics, NAACL ?03, pages173?180, Edmonton, Canada.Yoshimasa Tsuruoka and Jun?ichi Tsujii.
2005.
Bidi-rectional inference with the easiest-first strategyfor tagging sequence data.
In Proceedings of theConference on Human Language Technology andEmpirical Methods in Natural Language Process-ing, HLT-EMNLP ?05, pages 467?474, Vancouver,British Columbia, Canada.Yoshimasa Tsuruoka, Yusuke Miyao, and Jun?ichiKazama.
2011.
Learning with lookahead: Canhistory-based models rival globally optimized mod-els?
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies, ACL-HLT ?10, pages238?246, Portland, Oregon, USA.Shulamit Umansky-Pesin, Roi Reichart, and Ari Rap-poport.
2010.
A multi-domain web-based algo-rithm for POS tagging of unknown words.
In Pro-ceedings of the 23rd International Conference onComputational Linguistics: Posters, COLING ?10,pages 1274?1282, Beijing, China.502
