Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 609?616,Sydney, July 2006. c?2006 Association for Computational LinguisticsTree-to-String Alignment Template for Statistical Machine TranslationYang Liu , Qun Liu , and Shouxun LinInstitute of Computing TechnologyChinese Academy of SciencesNo.6 Kexueyuan South Road, Haidian DistrictP.
O.
Box 2704, Beijing, 100080, China{yliu,liuqun,sxlin}@ict.ac.cnAbstractWe present a novel translation modelbased on tree-to-string alignment template(TAT) which describes the alignment be-tween a source parse tree and a targetstring.
A TAT is capable of generatingboth terminals and non-terminals and per-forming reordering at both low and highlevels.
The model is linguistically syntax-based because TATs are extracted auto-matically from word-aligned, source sideparsed parallel texts.
To translate a sourcesentence, we first employ a parser to pro-duce a source parse tree and then ap-ply TATs to transform the tree into a tar-get string.
Our experiments show thatthe TAT-based model significantly outper-forms Pharaoh, a state-of-the-art decoderfor phrase-based models.1 IntroductionPhrase-based translation models (Marcu andWong, 2002; Koehn et al, 2003; Och and Ney,2004), which go beyond the original IBM trans-lation models (Brown et al, 1993) 1 by model-ing translations of phrases rather than individualwords, have been suggested to be the state-of-the-art in statistical machine translation by empiricalevaluations.In phrase-based models, phrases are usuallystrings of adjacent words instead of syntactic con-stituents, excelling at capturing local reorderingand performing translations that are localized to1The mathematical notation we use in this paper is takenfrom that paper: a source string fJ1 = f1, .
.
.
, fj , .
.
.
, fJ isto be translated into a target string eI1 = e1, .
.
.
, ei, .
.
.
, eI .Here, I is the length of the target string, and J is the lengthof the source string.substrings that are common enough to be observedon training data.
However, a key limitation ofphrase-based models is that they fail to model re-ordering at the phrase level robustly.
Typically,phrase reordering is modeled in terms of offset po-sitions at the word level (Koehn, 2004; Och andNey, 2004), making little or no direct use of syn-tactic information.Recent research on statistical machine transla-tion has lead to the development of syntax-basedmodels.
Wu (1997) proposes Inversion Trans-duction Grammars, treating translation as a pro-cess of parallel parsing of the source and tar-get language via a synchronized grammar.
Al-shawi et al (2000) represent each production inparallel dependency tree as a finite transducer.Melamed (2004) formalizes machine translationproblem as synchronous parsing based on multi-text grammars.
Graehl and Knight (2004) describetraining and decoding algorithms for both gen-eralized tree-to-tree and tree-to-string transduc-ers.
Chiang (2005) presents a hierarchical phrase-based model that uses hierarchical phrase pairs,which are formally productions of a synchronouscontext-free grammar.
Ding and Palmer (2005)propose a syntax-based translation model basedon a probabilistic synchronous dependency in-sert grammar, a version of synchronous gram-mars defined on dependency trees.
All these ap-proaches, though different in formalism, make useof synchronous grammars or tree-based transduc-tion rules to model both source and target lan-guages.Another class of approaches make use of syn-tactic information in the target language alone,treating the translation problem as a parsing prob-lem.
Yamada and Knight (2001) use a parser inthe target language to train probabilities on a set of609operations that transform a target parse tree into asource string.Paying more attention to source language anal-ysis, Quirk et al (2005) employ a source languagedependency parser, a target language word seg-mentation component, and an unsupervised wordalignment component to learn treelet translationsfrom parallel corpus.In this paper, we propose a statistical translationmodel based on tree-to-string alignment templatewhich describes the alignment between a sourceparse tree and a target string.
A TAT is capa-ble of generating both terminals and non-terminalsand performing reordering at both low and highlevels.
The model is linguistically syntax-basedbecause TATs are extracted automatically fromword-aligned, source side parsed parallel texts.To translate a source sentence, we first employ aparser to produce a source parse tree and then ap-ply TATs to transform the tree into a target string.One advantage of our model is that TATs canbe automatically acquired to capture linguisticallymotivated reordering at both low (word) and high(phrase, clause) levels.
In addition, the training ofTAT-based model is less computationally expen-sive than tree-to-tree models.
Similarly to (Galleyet al, 2004), the tree-to-string alignment templatesdiscussed in this paper are actually transformationrules.
The major difference is that we model thesyntax of the source language instead of the targetside.
As a result, the task of our decoder is to findthe best target string while Galley?s is to seek themost likely target tree.2 Tree-to-String Alignment TemplateA tree-to-string alignment template z is a triple?T?
, S?, A?
?, which describes the alignment A?
be-tween a source parse tree T?
= T (F J ?1 ) 2 anda target string S?
= EI?1 .
A source string F J?1 ,which is the sequence of leaf nodes of T (F J ?1 ),consists of both terminals (source words) and non-terminals (phrasal categories).
A target string EI?1is also composed of both terminals (target words)and non-terminals (placeholders).
An alignmentA?
is defined as a subset of the Cartesian productof source and target symbol positions:A?
?
{(j, i) : j = 1, .
.
.
, J ?
; i = 1, .
.
.
, I ?}
(1)2We use T (?)
to denote a parse tree.
To reduce notationaloverhead, we use T (z) to represent the parse tree in z. Simi-larly, S(z) denotes the string in z.Figure 1 shows three TATs automaticallylearned from training data.
Note that whendemonstrating a TAT graphically, we representnon-terminals in the target strings by blanks.NPNR??NN??LCPNPNR?
?CC?NRLC?NPDNPNP DEGNPPresident Bushbetween United States andFigure 1: Examples of tree-to-string alignmenttemplates obtained in trainingIn the following, we formally describe how tointroduce tree-to-string alignment templates intoprobabilistic dependencies to model Pr(eI1|fJ1 ) 3.In a first step, we introduce the hidden variableT (fJ1 ) that denotes a parse tree of the source sen-tence fJ1 :Pr(eI1|fJ1 ) =?T (fJ1 )Pr(eI1, T (fJ1 )|fJ1 ) (2)=?T (fJ1 )Pr(T (fJ1 )|fJ1 )Pr(eI1|T (fJ1 ), fJ1 ) (3)Next, another hidden variable D is introducedto detach the source parse tree T (fJ1 ) into a se-quence of K subtrees T?K1 with a preorder transver-sal.
We assume that each subtree T?k producesa target string S?k.
As a result, the sequenceof subtrees T?K1 produces a sequence of targetstrings S?K1 , which can be combined serially togenerate the target sentence eI1.
We assume thatPr(eI1|D,T (fJ1 ), fJ1 ) ?
Pr(S?K1 |T?K1 ) because eI1is actually generated by the derivation of S?K1 .Note that we omit an explicit dependence on thedetachment D to avoid notational overhead.Pr(eI1|T (fJ1 ), fJ1 ) =?DPr(eI1, D|T (fJ1 ), fJ1 ) (4)=?DPr(D|T (fJ1 ), fJ1 )Pr(eI1|D,T (fJ1 ), fJ1 ) (5)=?DPr(D|T (fJ1 ), fJ1 )Pr(S?K1 |T?K1 ) (6)=?DPr(D|T (fJ1 ), fJ1 )K?k=1Pr(S?k|T?k) (7)3The notational convention will be as follows.
We usethe symbol Pr(?)
to denote general probability distributionwith no specific assumptions.
In contrast, for model-basedprobability distributions, we use generic symbol p(?).610NPDNPNPNR??DEG?NPNN??NN?
?NPDNPNP DEG?NPNPNR?
?NPNN NNNN??NN????
?
??
?
?parsingdetachment productionofChinaeconomic developmentcombinationeconomic development of ChinaFigure 2: Graphic illustration for translation pro-cessTo further decompose Pr(S?|T?
), the tree-to-string alignment template, denoted by the variablez, is introduced as a hidden variable.Pr(S?|T? )
=?zPr(S?, z|T? )
(8)=?zPr(z|T?
)Pr(S?|z, T? )
(9)Therefore, the TAT-based translation model canbe decomposed into four sub-models:1. parse model: Pr(T (fJ1 )|fJ1 )2. detachment model: Pr(D|T (fJ1 ), fJ1 )3.
TAT selection model: Pr(z|T?
)4.
TAT application model: Pr(S?|z, T?
)Figure 2 shows how TATs work to performtranslation.
First, the input source sentence isparsed.
Next, the parse tree is detached into fivesubtrees with a preorder transversal.
For each sub-tree, a TAT is selected and applied to produce astring.
Finally, these strings are combined seriallyto generate the translation (we use X to denote thenon-terminal):X1 ?
X2 of X3?
X2 of China?
X3 X4 of China?
economic X4 of China?
economic development of ChinaFollowing Och and Ney (2002), we base ourmodel on log-linear framework.
Hence, all knowl-edge sources are described as feature functionsthat include the given source string fJ1 , the targetstring eI1, and hidden variables.
The hidden vari-able T (fJ1 ) is omitted because we usually makeuse of only single best output of a parser.
As weassume that all detachment have the same proba-bility, the hidden variable D is also omitted.
Asa result, the model we actually adopt for exper-iments is limited because the parse, detachment,and TAT application sub-models are simplified.Pr(eI1, zK1 |fJ1 )= exp[?Mm=1 ?mhm(eI1, fJ1 , zK1 )]?e?I1,z?K1 exp[?Mm=1 ?mhm(e?I1, fJ1 , z?K1 )]e?I1 = argmaxeI1,zK1{ M?m=1?mhm(eI1, fJ1 , zK1 )}For our experiments we use the following sevenfeature functions 4 that are analogous to defaultfeature set of Pharaoh (Koehn, 2004).
To simplifythe notation, we omit the dependence on the hid-den variables of the model.h1(eI1, fJ1 ) = logK?k=1N(z) ?
?
(T (z), T?k)N(T (z))h2(eI1, fJ1 ) = logK?k=1N(z) ?
?
(T (z), T?k)N(S(z))h3(eI1, fJ1 ) = logK?k=1lex(T (z)|S(z)) ?
?
(T (z), T?k)h4(eI1, fJ1 ) = logK?k=1lex(S(z)|T (z)) ?
?
(T (z), T?k)h5(eI1, fJ1 ) = Kh6(eI1, fJ1 ) = logI?i=1p(ei|ei?2, ei?1)h7(eI1, fJ1 ) = I4When computing lexical weighting features (Koehn etal., 2003), we take only terminals into account.
If there areno terminals, we set the feature value to 1.
We use lex(?
)to denote lexical weighting.
We denote the number of TATsused for decoding by K and the length of target string by I .611Tree String Alignment( NR?? )
Bush 1:1( NN?? )
President 1:1( VV?? )
made 1:1( NN?? )
speech 1:1( NP ( NR ) ( NN ) ) X1 | X2 1:2 2:1( NP ( NR?? )
( NN ) ) X | Bush 1:2 2:1( NP ( NR ) ( NN?? )
) President | X 1:2 2:1( NP ( NR?? )
( NN?? )
) President | Bush 1:2 2:1( VP ( VV ) ( NN ) ) X1 | a | X2 1:1 2:3( VP ( VV?? )
( NN ) ) made | a | X 1:1 2:3( VP ( VV ) ( NN?? )
) X | a | speech 1:1 2:3( VP ( VV?? )
( NN?? )
) made | a | speech 1:1 2:3( IP ( NP ) ( VP ) ) X1 | X2 1:1 2:2Table 1: Examples of TATs extracted from the TSA in Figure 3 with h = 2 and c = 23 TrainingTo extract tree-to-string alignment templates froma word-aligned, source side parsed sentence pair?T (fJ1 ), eI1, A?, we need first identify TSAs (Tree-String-Alignment) using similar criterion as sug-gested in (Och and Ney, 2004).
A TSA is a triple?T (f j2j1 ), ei2i1 , A?)?
that is in accordance with thefollowing constraints:1.
?
(i, j) ?
A : i1 ?
i ?
i2 ?
j1 ?
j ?
j22.
T (f j2j1 ) is a subtree of T (fJ1 )Given a TSA ?T (f j2j1 ), ei2i1 , A?
?, a triple?T (f j4j3 ), ei4i3 , A??
is its sub TSA if and onlyif:1.
T (f j4j3 ), ei4i3 , A??
is a TSA2.
T (f j4j3 ) is rooted at the direct descendant ofthe root node of T (f j1j2 )3. i1 ?
i3 ?
i4 ?
i24.
?
(i, j) ?
A?
: i3 ?
i ?
i4 ?
j3 ?
j ?
j4Basically, we extract TATs from a TSA?T (f j2j1 ), ei2i1 , A??
using the following two rules:1.
If T (f j2j1 ) contains only one node,then ?T (f j2j1 ), ei2i1 , A??
is a TAT2.
If the height of T (f j2j1 ) is greater than one,then build TATs using those extracted fromsub TSAs of ?T (f j2j1 ), ei2i1 , A??.IPNPNR??NN??VPVV??NN?
?President Bush made a speechFigure 3: An example of TSAUsually, we can extract a very large amount ofTATs from training data using the above rules,making both training and decoding very slow.Therefore, we impose three restrictions to reducethe magnitude of extracted TATs:1.
A third constraint is added to the definition ofTSA:?j?, j??
: j1 ?
j?
?
j2 and j1 ?
j??
?
j2and (i1, j?)
?
A?
and (i2, j??)
?
A?This constraint requires that both the firstand last symbols in the target string must bealigned to some source symbols.2.
The height of T (z) is limited to no greaterthan h.3.
The number of direct descendants of a nodeof T (z) is limited to no greater than c.Table 1 shows the TATs extracted from the TSAin Figure 3 with h = 2 and c = 2.As we restrict that T (f j2j1 ) must be a subtree ofT (fJ1 ), TATs may be treated as syntactic hierar-612chical phrase pairs (Chiang, 2005) with tree struc-ture on the source side.
At the same time, we facethe risk of losing some useful non-syntactic phrasepairs.
For example, the phrase pair????????
President Bush madecan never be obtained in form of TAT from theTSA in Figure 3 because there is no subtree forthat source string.4 DecodingWe approach the decoding problem as a bottom-upbeam search.To translate a source sentence, we employ aparser to produce a parse tree.
Moving bottom-up through the source parse tree, we compute alist of candidate translations for the input subtreerooted at each node with a postorder transversal.Candidate translations of subtrees are placed instacks.
Figure 4 shows the organization of can-didate translation stacks.NPDNPNPNR??DEG?NPNN??NN?
?84 72 3 5 61...1...2...3...4...5...6...7...8Figure 4: Candidate translations of subtrees areplaced in stacks according to the root index set bypostorder transversalA candidate translation contains the followinginformation:1. the partial translation2.
the accumulated feature values3.
the accumulated probabilityA TAT z is usable to a parse tree T if and onlyif T (z) is rooted at the root of T and covers partof nodes of T .
Given a parse tree T , we find allusable TATs.
Given a usable TAT z, if T (z) isequal to T , then S(z) is a candidate translation ofT .
If T (z) covers only a portion of T , we haveto compute a list of candidate translations for Tby replacing the non-terminals of S(z) with can-didate translations of the corresponding uncoveredsubtrees.NPDNPNP DEG?NP84 72 3of...1...2...3...4...5...6...7...8Figure 5: Candidate translation constructionFor example, when computing the candidatetranslations for the tree rooted at node 8, the TATused in Figure 5 covers only a portion of the parsetree in Figure 4.
There are two uncovered sub-trees that are rooted at node 2 and node 7 respec-tively.
Hence, we replace the third symbol withthe candidate translations in stack 2 and the firstsymbol with the candidate translations in stack 7.At the same time, the feature values and probabil-ities are also accumulated for the new candidatetranslations.To speed up the decoder, we limit the searchspace by reducing the number of TATs used foreach input node.
There are two ways to limit theTAT table size: by a fixed limit (tatTable-limit) ofhow many TATs are retrieved for each input node,and by a probability threshold (tatTable-threshold)that specify that the TAT probability has to beabove some value.
On the other hand, instead ofkeeping the full list of candidates for a given node,we keep a top-scoring subset of the candidates.This can also be done by a fixed limit (stack-limit)or a threshold (stack-threshold).
To perform re-combination, we combine candidate translationsthat share the same leading and trailing bigramsin each stack.5 ExperimentsOur experiments were on Chinese-to-Englishtranslation.
The training corpus consists of 31, 149sentence pairs with 843, 256 Chinese words and613System Features BLEU4d + ?
(e|f) 0.0573 ?
0.0033Pharaoh d + lm + ?
(e|f) + wp 0.2019 ?
0.0083d + lm + ?
(f |e) + lex(f |e) + ?
(e|f) + lex(e|f) + pp + wp 0.2089 ?
0.0089h1 0.1639 ?
0.0077Lynx h1 + h6 + h7 0.2100 ?
0.0089h1 + h2 + h3 + h4 + h5 + h6 + h7 0.2178 ?
0.0080Table 2: Comparison of Pharaoh and Lynx with different feature settings on the test corpus949, 583 English words.
For the language model,we used SRI Language Modeling Toolkit (Stol-cke, 2002) to train a trigram model with modi-fied Kneser-Ney smoothing (Chen and Goodman,1998) on the 31, 149 English sentences.
We se-lected 571 short sentences from the 2002 NISTMT Evaluation test set as our development cor-pus, and used the 2005 NIST MT Evaluation testset as our test corpus.
We evaluated the transla-tion quality using the BLEU metric (Papineni etal., 2002), as calculated by mteval-v11b.pl with itsdefault setting except that we used case-sensitivematching of n-grams.5.1 PharaohThe baseline system we used for comparison wasPharaoh (Koehn et al, 2003; Koehn, 2004), afreely available decoder for phrase-based transla-tion models:p(e|f) = p?
(f |e)??
?
pLM(e)?LM ?pD(e, f)?D ?
?length(e)?W(e) (10)We ran GIZA++ (Och and Ney, 2000) on thetraining corpus in both directions using its defaultsetting, and then applied the refinement rule ?diag-and?
described in (Koehn et al, 2003) to obtaina single many-to-many word alignment for eachsentence pair.
After that, we used some heuristics,which including rule-based translation of num-bers, dates, and person names, to further improvethe alignment accuracy.Given the word-aligned bilingual corpus, weobtained 1, 231, 959 bilingual phrases (221, 453used on test corpus) using the training toolkitspublicly released by Philipp Koehn with its defaultsetting.To perform minimum error rate training (Och,2003) to tune the feature weights to maximize thesystem?s BLEU score on development set, we usedoptimizeV5IBMBLEU.m (Venugopal and Vogel,2005).
We used default pruning settings forPharaoh except that we set the distortion limit to4.5.2 LynxOn the same word-aligned training data, it tookus about one month to parse all the 31, 149 Chi-nese sentences using a Chinese parser written byDeyi Xiong (Xiong et al, 2005).
The parser wastrained on articles 1 ?
270 of Penn Chinese Tree-bank version 1.0 and achieved 79.4% (F1 mea-sure) as well as a 4.4% relative decrease in er-ror rate.
Then, we performed TAT extraction de-scribed in section 3 with h = 3 and c = 5and obtained 350, 575 TATs (88, 066 used on testcorpus).
To run our decoder Lynx on develop-ment and test corpus, we set tatTable-limit = 20,tatTable-threshold = 0, stack-limit = 100, andstack-threshold = 0.00001.5.3 ResultsTable 2 shows the results on test set using Pharaohand Lynx with different feature settings.
The 95%confidence intervals were computed using Zhang?ssignificance tester (Zhang et al, 2004).
We mod-ified it to conform to NIST?s current definitionof the BLEU brevity penalty.
For Pharaoh, eightfeatures were used: distortion model d, a trigramlanguage model lm, phrase translation probabili-ties ?
(f |e) and ?
(e|f), lexical weightings lex(f |e)and lex(e|f), phrase penalty pp, and word penaltywp.
For Lynx, seven features described in sec-tion 2 were used.
We find that Lynx outperformsPharaoh with all feature settings.
With full fea-tures, Lynx achieves an absolute improvement of0.006 over Pharaoh (3.1% relative).
This differ-ence is statistically significant (p < 0.01).
Notethat Lynx made use of only 88, 066 TATs on testcorpus while 221, 453 bilingual phrases were usedfor Pharaoh.The feature weights obtained by minimum er-614FeaturesSystem d lm ?
(f |e) lex(f |e) ?
(e|f) lex(e|f) pp wpPharaoh 0.0476 0.1386 0.0611 0.0459 0.1723 0.0223 0.3122 -0.2000Lynx - 0.3735 0.0061 0.1081 0.1656 0.0022 0.0824 0.2620Table 3: Feature weights obtained by minimum error rate training on the development corpusBLEU4tat 0.2178 ?
0.0080tat + bp 0.2240 ?
0.0083Table 4: Effect of using bilingual phrases for Lynxror rate training for both Pharaoh and Lynx areshown in Table 3.
We find that ?
(f |e) (i.e.
h2) isnot a helpful feature for Lynx.
The reason is thatwe use only a single non-terminal symbol insteadof assigning phrasal categories to the target string.In addition, we allow the target string consists ofonly non-terminals, making translation decisionsnot always based on lexical evidence.5.4 Using bilingual phrasesIt is interesting to use bilingual phrases tostrengthen the TAT-based model.
As we men-tioned before, some useful non-syntactic phrasepairs can never be obtained in form of TAT be-cause we restrict that there must be a correspond-ing parse tree for the source phrase.
Moreover,it takes more time to obtain TATs than bilingualphrases on the same training data because parsingis usually very time-consuming.Given an input subtree T (F j2j1 ), if Fj2j1 is a stringof terminals, we find all bilingual phrases that thesource phrase is equal to F j2j1 .
Then we build aTAT for each bilingual phrase ?fJ ?1 , eI?1 , A??
: thetree of the TAT is T (F j2j1 ), the string is eI?1 , andthe alignment is A?.
If a TAT built from a bilingualphrase is the same with a TAT in the TAT table, weprefer to the greater translation probabilities.Table 4 shows the effect of using bilingualphrases for Lynx.
Note that these bilingual phrasesare the same with those used for Pharaoh.5.5 Results on large dataWe also conducted an experiment on large data tofurther examine our design philosophy.
The train-ing corpus contains 2.6 million sentence pairs.
Weused all the data to extract bilingual phrases anda portion of 800K pairs to obtain TATs.
Two tri-gram language models were used for Lynx.
Onewas trained on the 2.6 million English sentencesand another was trained on the first 1/3 of the Xin-hua portion of Gigaword corpus.
We also includedrule-based translations of named entities, dates,and numbers.
By making use of these data, Lynxachieves a BLEU score of 0.2830 on the 2005NIST Chinese-to-English MT evaluation test set,which is a very promising result for linguisticallysyntax-based models.6 ConclusionIn this paper, we introduce tree-to-string align-ment templates, which can be automaticallylearned from syntactically-annotated training data.The TAT-based translation model improves trans-lation quality significantly compared with a state-of-the-art phrase-based decoder.
Treated as spe-cial TATs without tree on the source side, bilingualphrases can be utilized for the TAT-based model toget further improvement.It should be emphasized that the restrictionswe impose on TAT extraction limit the expressivepower of TAT.
Preliminary experiments reveal thatremoving these restrictions does improve transla-tion quality, but leads to large memory require-ments.
We feel that both parsing and word align-ment qualities have important effects on the TAT-based model.
We will retrain the Chinese parseron Penn Chinese Treebank version 5.0 and try toimprove word alignment quality using log-linearmodels as suggested in (Liu et al, 2005).AcknowledgementThis work is supported by National High Tech-nology Research and Development Program con-tract ?Generally Technical Research and Ba-sic Database Establishment of Chinese Plat-form?
(Subject No.
2004AA114010).
We aregrateful to Deyi Xiong for providing the parser andHaitao Mi for making the parser more efficient androbust.
Thanks to Dr. Yajuan Lv for many helpfulcomments on an earlier draft of this paper.615ReferencesHiyan Alshawi, Srinivas Bangalore, and Shona Dou-glas.
2000.
Learning dependency translation mod-els as collections of finite-state head transducers.Computational Linguistics, 26(1):45-60.Peter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer.
1993.
Themathematics of statistical machine translation: Pa-rameter estimation.
Computational Linguistics,19(2):263-311.Stanley F. Chen and Joshua Goodman.
1998.
Amempirical study of smoothing techniques for lan-guage modeling.
Technical Report TR-10-98, Har-vard University Center for Research in ComputingTechnology.David Chiang.
2005.
A hierarchical phrase-basedmodel for statistical machine translation.
In Pro-ceedings of 43rd Annual Meeting of the ACL, pages263-270.Yuan Ding and Martha Palmer.
2005.
Machine trans-lation using probabilistic synchronous dependencyinsert grammars.
In Proceedings of 43rd AnnualMeeting of the ACL, pages 541-548.Michel Galley, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a translation rule?In Proceedings of NAACL-HLT 2004, pages 273-280.Jonathan Graehl and Kevin Knight.
2004.
Trainingtree transducers.
In Proceedings of NAACL-HLT2004, pages 105-112.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proceedingsof HLT-NAACL 2003, pages 127-133.Philipp Koehn.
2004.
Pharaoh: a beam search de-coder for phrase-based statistical machine trnasla-tion models.
In Proceedings of the Sixth Confer-ence of the Association for Machine Translation inthe Americas, pages 115-124.Yang Liu, Qun Liu, and Shouxun Lin.
2005.
Log-linear models for word alignment.
In Proceedingsof 43rd Annual Meeting of the ACL, pages 459-466.Daniel Marcu and William Wong.
2002.
A phrase-based, joint probability model for statistical machinetranslation.
In Proceedings of the 2002 Conferenceon Empirical Methods in Natural Language Pro-cessing (EMNLP), pages 133-139.Dan Melamed.
2004.
Statistical machine translationby parsing.
In Proceedings of 42nd Annual Meetingof the ACL, pages 653-660.Franz J. Och and Hermann Ney.
2000.
Improved sta-tistical alignment models.
In Proceedings of 38thAnnual Meeting of the ACL, pages 440-447.Franz J. Och and Hermann Ney.
2002.
Discriminativetraining and maximum entropy models for statisticalmachine translation.
In Proceedings of 40th AnnualMeeting of the ACL, pages 295-302.Franz J. Och and Hermann Ney.
2004.
The alignmenttemplate approach to statistical machine translation.Computational Linguistics, 30(4):417-449.Franz J. Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of41st Annual Meeting of the ACL, pages 160-167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedingsof 40th Annual Meeting of the ACL, pages 311-318.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.Dependency treelet translation: Syntactically in-formed phrasal SMT.
In Proceedings of 43rd An-nual Meeting of the ACL, pages 271-279.Andreas Stolcke.
2002.
SRILM - an extensible lan-guage modeling toolkit.
In Proceedings of Interna-tional Conference on Spoken Language Processing,volume 2, pages 901-904.Ashish Venugopal and Stephan Vogel.
2005.
Consid-erations in maximum mutual information and min-imum classification error training for statistical ma-chine translation.
In Proceedings of the Tenth Con-ference of the European Association for MachineTranslation (EAMT-05).Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377-403.Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin,and Yueliang Qian.
2005.
Parsing the Penn Chinesetreebank with semantic knowledge.
In Proceedingsof IJCNLP 2005, pages 70-81.Kenji Yamada and Kevin Knight.
2001.
A syntax-based statistical translation model.
In Proceedingsof 39th Annual Meeting of the ACL, pages 523-530.Ying Zhang, Stephan Vogel, and Alex Waibel.
2004.Interpreting BLEU/NIST scores: How much im-provement do we need to have a better system?
InProceedings of the Fourth International Conferenceon Language Resources and Evaluation (LREC),pages 2051-2054.616
