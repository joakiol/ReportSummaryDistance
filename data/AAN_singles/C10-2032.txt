Coling 2010: Poster Volume, pages 276?284,Beijing, August 2010A Comparison of Features for Automatic Readability AssessmentLijun FengCity University of New Yorklijun7.feng@gmail.comMartin JanscheGoogle, Inc.jansche@acm.orgMatt HuenerfauthCity University of New Yorkmatt@cs.qc.cuny.eduNoe?mie ElhadadColumbia Universitynoemie@dbmi.columbia.eduAbstractSeveral sets of explanatory variables ?
in-cluding shallow, language modeling, POS,syntactic, and discourse features ?
are com-pared and evaluated in terms of their im-pact on predicting the grade level of read-ing material for primary school students.We find that features based on in-domainlanguage models have the highest predic-tive power.
Entity-density (a discourse fea-ture) and POS-features, in particular nouns,are individually very useful but highly cor-related.
Average sentence length (a shal-low feature) is more useful ?
and less ex-pensive to compute ?
than individual syn-tactic features.
A judicious combinationof features examined here results in a sig-nificant improvement over the state of theart.1 Introduction1.1 Motivation and MethodReadability Assessment quantifies the difficultywith which a reader understands a text.
Automaticreadability assessment enables the selection of ap-propriate reading material for readers of varyingproficiency.
Besides modeling and understandingthe linguistic components involved in readability, areadability-prediction algorithm can be leveragedfor the task of automatic text simplification: as sim-plification operators are applied to a text, the read-ability is assessed to determine whether more sim-plification is needed or a particular reading levelwas reached.Identifying text properties that are strongly cor-related with text complexity is itself complex.
Inthis paper, we explore a broad range of text proper-ties at various linguistic levels, ranging from dis-course features to language modeling features, part-of-speech-based grammatical features, parsed syn-tactic features and well studied shallow features,many of which are inspired by previous work.We use grade levels, which indicate the numberof years of education required to completely under-stand a text, as a proxy for reading difficulty.
Thecorpus in our study consists of texts labeled withgrade levels ranging from grade 2 to 5.
We treatreadability assessment as a classification task andevaluate trained classifiers in terms of their predic-tion accuracy.
To investigate the contributions ofvarious sets of features, we build prediction modelsand examine how the choice of features influencesthe model performance.1.2 Related WorkMany traditional readability metrics are linear mod-els with a few (often two or three) predictor vari-ables based on superficial properties of words, sen-tences, and documents.
These shallow featuresinclude the average number of syllables per word,the number of words per sentence, or binned wordfrequency.
For example, the Flesch-Kincaid GradeLevel formula uses the average number of wordsper sentence and the average number of syllablesper word to predict the grade level (Flesch, 1979).The Gunning FOG index (Gunning, 1952) uses av-erage sentence length and the percentage of wordswith at least three syllables.
These traditional met-rics are easy to compute and use, but they are notreliable, as demonstrated by several recent stud-ies in the field (Si and Callan, 2001; Petersen andOstendorf, 2006; Feng et al, 2009).276With the advancement of natural language pro-cessing tools, a wide range of more complex textproperties have been explored at various linguis-tic levels.
Si and Callan (2001) used unigramlanguage models to capture content informationfrom scientific web pages.
Collins-Thompson andCallan (2004) adopted a similar approach and useda smoothed unigram model to predict the grade lev-els of short passages and web documents.
Heilmanet al (2007) continued using language modelingto predict readability for first and second languagetexts.
Furthermore, they experimented with vari-ous statistical models to test their effectiveness atpredicting reading difficulty (Heilman et al, 2008).Schwarm/Petersen and Ostendorf (Schwarm andOstendorf, 2005; Petersen and Ostendorf, 2006)used support vector machines to combine featuresfrom traditional reading level measures, statisticallanguage models and automatic parsers to assessreading levels.
In addition to lexical and syntacticfeatures, several researchers started to explore dis-course level features and examine their usefulnessin predicting text readability.
Pitler and Nenkova(2008) used the Penn Discourse Treebank (Prasadet al, 2008) to examine discourse relations.
Wepreviously used a lexical-chaining tool to extractentities that are connected by certain semantic re-lations (Feng et al, 2009).In this study, we systematically evaluate allabove-mentioned types of features, as well as afew extensions and variations.
A detailed descrip-tion of the features appears in Section 3.
Section4 discusses results of experiments with classifierstrained on these features.
We begin with a descrip-tion of our data in the following section.2 CorpusWe contacted the Weekly Reader1 corporation, anon-line publisher producing magazines for elemen-tary and high school students, and were grantedaccess in October 2008 to an archive of their ar-ticles.
Among the articles retrieved, only thosefor elementary school students are labeled withgrade levels, which range from 2 to 5.
We selectedonly this portion of articles (1629 in total) for the1http://www.weeklyreader.comTable 1: Statistics for the Weekly Reader CorpusGrade docs.
words/document words/sentencemean std.
dev.
mean std.
dev.2 174 128.27 106.03 9.54 2.323 289 171.96 106.05 11.39 2.424 428 278.03 187.58 13.67 2.655 542 335.56 230.25 15.28 3.21study.2 These articles are intended to build chil-dren?s general knowledge and help them practicereading skills.
While pre-processing the texts, wefound that many articles, especially those for lowergrade levels, consist of only puzzles and quizzes,often in the form of simple multiple-choice ques-tions.
We discarded such texts and kept only 1433full articles.
Some distributional statistics of thefinal corpus are listed in Table 1.3 Features3.1 Discourse FeaturesWe implement four subsets of discourse fea-tures: entity-density features, lexical-chain fea-tures, coreference inference features and entity gridfeatures.
The coreference inference features arenovel and have not been studied before.
We pre-viously studied entity-density features and lexical-chain features for readers with intellectual disabili-ties (Feng et al, 2009).
Entity-grid features havebeen studied by Barzilay and Lapata (2008) in astylistic classification task.
Pitler and Nenkova(2008) used the same features to evaluate how wella text is written.
We replicate this set of featuresfor grade level prediction task.3.1.1 Entity-Density FeaturesConceptual information is often introduced in atext by entities, which consist of general nounsand named entities, e.g.
people?s names, locations,organizations, etc.
These are important in textcomprehension, because established entities formbasic components of concepts and propositions, onwhich higher level discourse processing is based.Our prior work illustrated the importance of en-tities in text comprehension (Feng et al, 2009).2A corpus of Weekly Reader articles was previously usedin work by Schwarm and Ostendorf (2005).
However, the twocorpora are not identical in size nor content.277Table 2: New Entity-Density Features1 percentage of named entities per document2 percentage of named entities per sentences3 percentage of overlapping nouns removed4 average number of remaining nouns per sentence5 percentage of named entities in total entities6 percentage of remaining nouns in total entitiesWe hypothesized that the number of entities in-troduced in a text relates to the working memoryburden on their targeted readers ?
individuals withintellectual disabilities.
We defined entities as aunion of named entities and general nouns (nounsand proper nouns) contained in a text, with over-lapping general nouns removed.
Based on this, weimplemented four kinds of entity-density features:total number of entity mentions per document, totalnumber of unique entity mentions per document,average number of entity mentions per sentence,and average number of unique entity mentions persentence.We believe entity-density features may also re-late to the readability of a text for a general au-dience.
In this paper, we conduct a more re-fined analysis of general nouns and named entities.To collect entities for each document, we usedOpenNLP?s3 name-finding tool to extract namedentities; general nouns are extracted from the out-put of Charniak?s Parser (see Section 3.3).
Basedon the set of entities collected for each document,we implement 12 new features.
We list several ofthese features in in Table 2.3.1.2 Lexical Chain FeaturesDuring reading, a more challenging task with enti-ties is not just to keep track of them, but to resolvethe semantic relations among them, so that infor-mation can be processed, organized and stored ina structured way for comprehension and later re-trieval.
In earlier work (Feng et al, 2009), weused a lexical-chaining tool developed by Galleyand McKeown (2003) to annotate six semantic re-lations among entities, e.g.
synonym, hypernym,hyponym, etc.
Entities that are connected by thesesemantic relations were linked through the text toform lexical chains.
Based on these chains, weimplemented six features, listed in Table 3, which3http://opennlp.sourceforge.net/Table 3: Lexical Chain Features1 total number of lexical chains per document2 avg.
lexical chain length3 avg.
lexical chain span4 num.
of lex.
chains with span ?
half doc.
length5 num.
of active chains per word6 num.
of active chains per entityTable 4: Coreference Chain Features1 total number of coreference chains per document2 avg.
num.
of coreferences per chain3 avg.
chain span4 num.
of coref.
chains with span ?
half doc.
length5 avg.
inference distance per chain6 num.
of active coreference chains per word7 num.
of active coreference chains per entitywe use in our current study.
The length of a chainis the number of entities contained in the chain,the span of chain is the distance between the indexof the first and last entity in a chain.
A chain isdefined to be active for a word or an entity if thischain passes through its current location.3.1.3 Coreference Inference FeaturesRelations among concepts and propositions are of-ten not stated explicitly in a text.
Automatically re-solving implicit discourse relations is a hard prob-lem.
Therefore, we focus on one particular type,referential relations, which are often establishedthrough anaphoric devices, e.g.
pronominal refer-ences.
The ability to resolve referential relations isimportant for text comprehension.We use OpenNLP to resolve coreferences.
En-tities and pronominal references that occur acrossthe text and refer to the same person or objectare extracted and formed into a coreference chain.Based on the chains extracted, we implement sevenfeatures as listed in Table 4.
The chain length,chain span and active chains are defined in a sim-ilar way to the lexical chain features.
Inferencedistance is the difference between the index of thereferent and that of its pronominal reference.
If thesame referent occurs more than once in a chain,the index of the closest occurrence is used whencomputing the inference distance.3.1.4 Entity Grid FeaturesCoherent texts are easier to read.
Several computa-tional models have been developed to represent and278measure discourse coherence (Lapata and Barzilay,2005; Soricut and Marcu, 2006; Elsner et al, 2007;Barzilay and Lapata, 2008) for NLP tasks such astext ordering and text generation.
Although thesemodels are not intended directly for readability re-search, Barzilay and Lapata (2008) have reportedthat distributional properties of local entities gen-erated by their grid models are useful in detectingoriginal texts from their simplified versions whencombined with well studied lexical and syntacticfeatures.
This approach was subsequently pursuedby Pitler and Nenkova (2008) in their readabilitystudy.
Barzilay and Lapata?s entity grid model isbased on the assumption that the distribution ofentities in locally coherent texts exhibits certainregularities.
Each text is abstracted into a gridthat captures the distribution of entity patterns atthe level of sentence-to-sentence transitions.
Theentity grid is a two-dimensional array, with one di-mension corresponding to the salient entities in thetext, and the other corresponding to each sentenceof the text.
Each grid cell contains the grammaticalrole of the specified entity in the specified sentence:whether it is a subject (S), object (O), neither ofthe two (X), or absent from the sentence (-).We use the Brown Coherence Toolkit (v0.2) (El-sner et al, 2007), based on (Lapata and Barzilay,2005), to generate an entity grid for each text inour corpus.
The distribution patterns of entitiesare traced between each pair of adjacent sentences,resulting in 16 entity transition patterns4.
We thencompute the distribution probability of each entitytransition pattern within a text to form 16 entity-grid-based features.3.2 Language Modeling FeaturesOur language-modeling-based features are inspiredby Schwarm and Ostendorf?s (2005) work, a studythat is closely related to ours.
They used datafrom the same data ?
the Weekly Reader ?
fortheir study.
They trained three language mod-els (unigram, bigram and trigram) on two pairedcomplex/simplified corpora (Britannica and Litera-cyNet) using an approach in which words with highinformation gain are kept and the remaining words4These 16 transition patterns are: ?SS?, ?SO?, ?SX?, ?S-?,?OS?, ?OO?, ?OX?, ?O-?, ?XS?, ?XO?, ?XX?, ?X-?, ?-S?,?-O?, ?-X?, ?- -?.are replaced with their parts of speech.
These lan-guage models were then used to score each textin the Weekly Reader corpus by perplexity.
Theyreported that this approach was more successfulthan training LMs on text sequences of word la-bels alone, though without providing supportingstatistics.It?s worth pointing out that their LMs were nottrained on the Weekly Reader data, but rather ontwo unrelated paired corpora (Britannica and Lit-eracyNet).
This seems counter-intuitive, becausetraining LMs directly on the Weekly Reader datawould provide more class-specific information forthe classifiers.
They justified this choice by statingthat splitting limited Weekly Reader data for train-ing and testing purposes resulted in unsuccessfulperformance.We overcome this problem by using a hold-one-out approach to train LMs directly on ourWeekly Reader corpus, which contains texts rang-ing from Grade 2 to 5.
We use grade levels todivide the whole corpus into four smaller subsets.In addition to implementing Schwarm and Osten-dorf?s information-gain approach, we also builtLMs based on three other types of text sequencesfor comparison purposes.
These included: word-token-only sequence (i.e., the original text), POS-only sequence, and paired word-POS sequence.For each grade level, we use the SRI LanguageModeling Toolkit5 (with Good-Turing discountingand Katz backoff for smoothing) to train 5 lan-guage models (1- to 5-gram) using each of the fourtext sequences, resulting in 4?5?4= 80 perplex-ity features for each text tested.3.3 Parsed Syntactic FeaturesSchwarm and Ostendorf (2005) studied four parsetree features (average parse tree height, averagenumber of SBARs, noun phrases, and verb phrasesper sentences).
We implemented these and addi-tional features, using the Charniak parser (Char-niak, 2000).
Our parsed syntactic features focus onclauses (SBAR), noun phrases (NP), verb phrases(VP) and prepositional phrases (PP).
For eachphrase, we implement four features: total num-ber of the phrases per document, average numberof phrases per sentence, and average phrase length5http://www.speech.sri.com/projects/srilm/279measured by number of words and characters re-spectively.
In addition to average tree height, weimplement two non-terminal-node-based features:average number of non-terminal nodes per parsetree, and average number of non-terminal nodesper word (terminal node).3.4 POS-based FeaturesPart-of-speech-based grammatical features wereshown to be useful in readability prediction (Heil-man et al, 2007; Leroy et al, 2008).
To extendprior work, we systematically studied a number ofcommon categories of words and investigated towhat extent they are related to a text?s complex-ity.
We focus primarily on five classes of words(nouns, verbs, adjectives, adverbs, and preposi-tions) and two broad categories (content words,function words).
Content words include nouns,verbs, numerals, adjectives, and adverbs; the re-maining types are function words.
The part ofspeech of each word is obtained from examiningthe leaf node based on the output of Charniak?sparser, where each leaf node consists of a word andits part of speech.
We group words based on theirPOS labels.
For each class of words, we imple-ment five features.
For example, for the adjectiveclass, we implemented the following five features:percent of adjectives (tokens) per document, per-cent of unique adjectives (types) per document,ratio of unique adjectives per total unique wordsin a document, average number of adjectives persentence and average number of unique adjectivesper sentence.3.5 Shallow FeaturesShallow features refer to those used by traditionalreadability metrics, such as Flesch-Kincaid GradeLevel (Flesch, 1979), SMOG (McLaughlin, 1969),Gunning FOG (Gunning, 1952), etc.
Althoughrecent readability studies have strived to take ad-vantage of NLP techniques, little has been revealedabout the predictive power of shallow features.Shallow features, which are limited to superficialtext properties, are computationally much less ex-pensive than syntactic or discourse features.
To en-able a comparison against more advanced features,we implement 8 frequently used shallow featuresas listed in Table 5.Table 5: Shallow Features1 average number of syllables per word2 percentage of poly-syll.
words per doc.3 average number of poly-syll.
words per sent.4 average number of characters per word5 Chall-Dale difficult words rate per doc.6 average number of words per sentence7 Flesch-Kincaid score8 total number of words per document3.6 Other FeaturesFor comparison, we replicated 6 out-of-vocabularyfeatures described in Schwarm and Ostendorf(2005).
For each text in the Weekly Reader corpus,these 6 features are computed using the most com-mon 100, 200 and 500 word tokens and types basedon texts from Grade 2.
We also replicated the 12perplexity features implemented by Schwarm andOstendorf (2005) (see Section 3.2).4 Experiments and DiscussionPrevious studies on reading difficulty explored vari-ous statistical models, e.g.
regression vs. classifica-tion, with varying assumptions about the measure-ment of reading difficulty, e.g.
whether labels areordered or unrelated, to test the predictive powerof models (Heilman et al, 2008; Petersen and Os-tendorf, 2009; Aluisio et al, 2010).
In our re-search, we have used various models, includinglinear regression; standard classification (Logis-tic Regression and SVM), which assumes no rela-tion between grade levels; and ordinal regression/classification (provided by Weka, with LogisticRegression and SMO as base function), which as-sumes that the grade levels are ordered.
Our exper-iments show that, measured by mean squared errorand classification accuracy, linear regression mod-els perform considerably poorer than classificationmodels.
Measured by accuracy and F-measure,ordinal classifiers perform comparable or worsethan standard classifiers.
In this paper, we presentthe best results, which are obtained by standardclassifiers.
We use two machine learning packagesknown for efficient high-quality multi-class classi-fication: LIBSVM (Chang and Lin, 2001) and theWeka machine learning toolkit (Hall et al, 2009),from which we choose Logistic Regression as clas-sifiers.
We train and evaluate various prediction280Table 6: Comparison of discourse featuresFeature Set LIBSVM Logistic Regress.Entity-Density 59.63?0.632 57.59?0.375Lexical Chain 45.86?0.815 42.58?0.241Coref.
Infer.
40.93?0.839 42.19?0.238Entity Grid 45.92?1.155 42.14?0.457all combined 60.50?0.990 58.79?0.703models using the features described in Section 3.We evaluate classification accuracy using repeated10-fold cross-validation on the Weekly Reader cor-pus.
Classification accuracy is defined as the per-centage of texts predicted with correct grade levels.We repeat each experiment 10 times and report themean accuracy and its standard deviation.4.1 Discourse FeaturesWe first discuss the improvement made by extend-ing our earlier entity-density features (Feng et al,2009).
We used LIBSVM to train and test mod-els on the Weekly Reader corpus with our earlierfeatures and our new features respectively.
Withearlier features only, the model achieves 53.66%accuracy.
With our new features added, the modelperformance is 59.63%.Table 6 presents the classification accuracy ofmodels trained with discourse features.
We seethat, among four subsets of discourse features,entity-density features perform significantly betterthan the other three feature sets and generate thehighest classification accuracy (LIBSVM: 59.63%,Logistic Regression: 57.59%).
While Logistic Re-gression results show that there is not much perfor-mance difference among lexical chain, coreferenceinference, and entity grid features, classificationaccuracy of LIBSVM models indicates that lexicalchain features and entity grid features are betterin predicting text readability than coreference in-ference features.
Combining all discourse featurestogether does not significantly improve accuracycompared with models trained only with entity-density features.4.2 Language Modeling FeaturesTable 7 compares the performance of models gen-erated using our approach and our replication ofSchwarm and Ostendorf?s (2005) approach.
In ourapproach, features were obtained from languageTable 7: Comparison of lang.
modeling featuresFeature Set LIBSVM Logistic Regress.IG 62.52?1.202 62.14?0.510Text-only 60.17?1.206 60.31?0.559POS-only 56.21?2.354 57.64?0.391Word/POS pair 60.38?0.820 59.00?0.367all combined 68.38?0.929 66.82?0.448IG by Schwarm 52.21?0.832 51.89?0.405Table 8: Comparison of parsed syntactic featuresFeature Set # Feat.
LIBSVMOriginal features 4 50.68?0.812Expanded features 21 57.79?1.023models trained on the Weekly Reader corpus.
Notsurprisingly, these are more effective than LMstrained on the Britannica and LiteracyNet corpora,in Schwarm and Ostendorf?s approach.
Our resultssupport their claim that LMs trained with infor-mation gain outperform LMs trained with POS la-bels.
However, we also notice that training LMs onword labels alone or paired word/POS sequencesachieved similar classification accuracy to the IGapproach, while avoiding the complicated featureselection of the IG approach.4.3 Parsed Syntactic FeaturesTable 8 compares a classifier trained on the fourparse features of Schwarm and Ostendorf (2005) toa classifier trained on our expanded set of parse fea-tures.
The LIBSVM classifier with the expandedfeature set scored 7 points higher than the onetrained on only the original four features, improv-ing from 50.68% to 57.79%.
Table 9 shows adetailed comparison of particular parsed syntacticfeatures.
The two non-terminal-node-based fea-tures (average number of non-terminal nodes pertree and average number of non-terminal nodesper word) have higher discriminative power thanaverage tree height.
Among SBARs, NPs, VPs andPPs, our experiments show that VPs and NPs arethe best predictors.4.4 POS-based FeaturesThe classification accuracy generated by modelstrained with various POS features is presentedin Table 10.
We find that, among the five wordclasses investigated, noun-based features gener-281Table 9: Detailed comp.
of syntactic featuresFeature Set LIBSVM Logistic Regress.Non-term.-node ratios 53.02?0.571 51.80?0.171Average tree height 44.26?0.914 43.45?0.269SBARs 44.42?1.074 43.50?0.386NPs 51.56?1.054 48.14?0.408VPs 53.07?0.597 48.67?0.484PPs 49.36?1.277 46.47?0.374all combined 57.79?1.023 54.11?0.473Table 10: Comparison of POS featuresFeature Set LIBSVM Logistic Regress.Nouns 58.15?0.862 57.01?0.256Verbs 54.40?1.029 55.10?0.291Adjectives 53.87?1.128 52.75?0.427Adverbs 52.66?0.970 50.54?0.327Prepositions 56.77?1.278 54.13?0.312Content words 56.84?1.072 56.18?0.213Function words 52.19?1.494 50.95?0.298all combined 59.82?1.235 57.86?0.547ate the highest classification accuracy, which isconsistent with what we have observed earlierabout entity-density features.
Another notable ob-servation is that prepositions demonstrate higherdiscriminative power than adjectives and adverbs.Models trained with preposition-based features per-form close to those trained with noun-based fea-tures.
Among the two broader categories, contentwords (which include nouns) demonstrate higherpredictive power than function words (which in-clude prepositions).4.5 Shallow FeaturesWe present some notable findings on shallow fea-tures in Table 11.
Experimental results generatedby models trained with Logistic Regression showthat average sentence length has dominating predic-tive power over all other shallow features.
Featuresbased on syllable counting perform much worse.The Flesch-Kincaid Grade Level score uses a fixedlinear combination of average words per sentenceand average syllables per word.
Combining thosetwo features (without fixed coefficients) results inthe best overall accuracy, while using the Flesch-Kincaid score as a single feature is significantlyworse.Table 11: Comparison of shallow featuresFeature Set Logistic Regress.Avg.
words per sent.
52.17?0.193Avg.
syll.
per word 42.51?0.264above two combined 53.04?0.514Flesch-Kincaid score 50.83?0.144Avg.
poly-syll.
words per sent.
45.70?0.306all 8 features combined 52.34?0.2424.6 Comparison with Previous StudiesA trivial baseline of predicting the most frequentgrade level (grade 5) predicts 542 out of 1433 texts(or 37.8%) correctly.
With this in mind, we firstcompare our study with the widely-used Flesch-Kincaid Grade Level formula, which is a linearfunction of average words per sentence and averagesyllables per word that aims to predict the gradelevel of a text directly.
Since this is a fixed formulawith known coefficients, we evaluated it directlyon our entire Weekly Reader corpus without cross-validation.
We obtain the predicted grade levelof a text by rounding the Flesch-Kincaid scoreto the nearest integer.
For only 20 out of 1433texts the predicted and labeled grade levels agree,resulting in a poor accuracy of 1.4%.
By contrast,using the Flesch-Kincaid score as a feature of asimple logistic regression model achieves above50% accuracy, as discussed in Section 4.5.The most closely related previous study is thework of Schwarm and Ostendorf (2005).
How-ever, because their experiment design (85/15 train-ing/test data split) and machine learning tool(SV Mlight) differ from ours, their results are notdirectly comparable to ours.
To make a compar-ison, we replicated all the features used in theirstudy and then use LIBSVM and Weka?s LogisticRegression to train two models with the replicatedfeatures and evaluate them on our Weekly Readercorpus using 10-fold cross-validation.Using the same experiment design, we train clas-sifiers with three combinations of our features aslisted in Table 12.
?All features?
refers to a naivecombination of all features.
?AddOneBest?
refersto a subset of features selected by a group-wiseadd-one-best greedy feature selection.
?WekaFS?refers to a subset of features chosen by Weka?sfeature selection filter.?WekaFS?
consists of 28 features selected au-282Table 12: Comparison with previous workbaseline accuracy (majority class) 37.8Flesch-Kincaid Grade Level 1.4Feature Set # Feat.
LIBSVM Logistic Reg.Schwarm 25 63.18?1.664 60.50?0.477All features 273 72.21?0.821 63.71?0.576AddOneBest 122 74.01?0.847 69.22?0.411WekaFS 28 70.06?0.777 65.46?0.336tomatically by Weka?s feature selection filter us-ing a best-first search method.
The 28 featuresinclude language modeling features, syntactic fea-tures, POS features, shallow features and out-of-vocabulary features.
Aside from 4 shallow featuresand 5 out-of-vocabulary features, the other 19 fea-tures are novel features we have implemented forthis paper.As Table 12 shows, a naive combination of allfeatures results in classification accuracy of 72%,which is much higher than the current state of theart (63%).
This is not very surprising, since we areconsidering a greater variety of features than anyprevious individual study.
Our WekaFS classifieruses roughly the same number of features as thebest published result, yet it has a higher accuracy(70.06%).
Our best results were obtained by group-wise add-one-best feature selection, resulting in74% classification accuracy, a big improvementover the state of the art.5 ConclusionsWe examined the usefulness of features at variouslinguistic levels for predicting text readability interms of assigning texts to elementary school gradelevels.
We implemented a set of discourse features,enriched previous work by creating several newfeatures, and systematically tested and analyzedthe impact of these features.We observed that POS features, in particularnouns, have significant predictive power.
The highdiscriminative power of nouns in turn explains thegood performance of entity-density features, basedprimarily on nouns.
In general, our selected POSfeatures appear to be more correlated to text com-plexity than syntactic features, shallow featuresand most discourse features.For parsed syntactic features, we found that verbphrases appear to be more closely correlated withtext complexity than other types of phrases.
WhileSBARs are commonly perceived as good predic-tors for syntactic complexity, they did not provevery useful for predicting grade levels of texts inthis study.
In future work, we plan to examine thisresult in more detail.Among the 8 shallow features, which are usedin various traditional readability formulas, we iden-tified that average sentence length has dominatingpredictive power over all other lexical or syllable-based features.Not surprisingly, among language modelingfeatures, combined features obtained from LMstrained directly on the Weekly Reader corpus showhigh discriminative power, compared with featuresfrom LMs trained on unrelated corpora.Discourse features do not seem to be very use-ful in building an accurate readability metric.
Thereason could lie in the fact that the texts in the cor-pus we studied exhibit relatively low complexity,since they are aimed at primary-school students.
Infuture work, we plan to investigate whether thesediscourse features exhibit different discriminativepower for texts at higher grade levels.A judicious combination of features examinedhere results in a significant improvement over thestate of the art.ReferencesSandra Aluisio, Lucia Specia, Caroline Gasperin,and Carolina Scarton.
2010.
Readability assess-ment for text simplification.
In NAACL-HLT2010: The 5th Workshop on Innovative Use ofNLP for Building Educational Applications.Regina Barzilay and Mirella Lapata.
2008.
Model-ing local coherence: An entity-based approach.Computational Linguistics, 34(1):1?34.Chih-Chung Chang and Chih-Jen Lin.
2001.
LIB-SVM: A Library for Support Vector Machines.Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proceedings of the 1st Con-ference of the North American Chapter of theACL, pages 132?139.283Kevyn Collins-Thompson and Jamie Callan.
2004.A language modeling approach to predictingreading difficulty.
In Proceedings of the Hu-man Language Technology Conference of theNorth American Chapter of the Association forComputational Linguistics (HLT-NAACL 2004).Micha Elsner, Joseph Austerweil, and EugeneCharniak.
2007.
A unified local and globalmodel for discourse coherence.
In Proceed-ings of the Conference on Human LanguageTechnology and North American chapter of theAssociation for Computational Linguistics (HLT-NAACL 2007).Lijun Feng, Noe?mie Elhadad, and Matt Huener-fauth.
2009.
Cognitively motivated features forreadability assessment.
In The 12th Conferenceof the European Chapter of the Association forComputational Linguistics (EACL 2009).Rudolf Flesch.
1979.
How to write plain English.Harper and Brothers, New York.Michel Galley and Kathleen McKeown.
2003.
Im-proving word sense disambiguation in lexicalchaining.
In Proceedings of the 18th Inter-national Joint Conference on Artificial Intelli-gence.Robert Gunning.
1952.
The Technique of ClearWriting.
McGraw-Hill.Mark Hall, Eibe Frank, Geoffrey Holmes, Bern-hard Pfahringer, Peter Reutemann, and Ian H.Witten.
2009.
The WEKA data mining software:An update.
SIGKDD Explorations, 11(1):10?18.Michael J. Heilman, Kevyn Collins-Thompson,Jamie Callan, and Maxine Eskenazi.
2007.
Com-bining lexical and grammatical features to im-prove readability measures for first and secondlanguage texts.
In Human Language Technolo-gies 2007: The Conference of the North Amer-ican Chapter of the Association for Computa-tional Linguistics.Michael J. Heilman, Kevyn Collins-Thompson,and Maxine Eskenazi.
2008.
An analysis of sta-tistical models and features for reading difficultyprediction.
In ACL 2008: The 3rd Workshop onInnovative Use of NLP for Building EducationalApplications.Mirella Lapata and Regina Barzilay.
2005.
Auto-matic evaluation of text coherence: Models andrepresentations.
In Proceedings of the Interna-tional Joint Conference on Artificial Intelligence(IJCAI?05), pages 1085?1090.Gondy Leroy, Stephen Helmreich, James R. Cowie,Trudi Miller, and Wei Zheng.
2008.
Evaluatingonline health information: Beyond readabilityformulas.
In AMIA 2008 Symposium Proceed-ings.G.
Harry McLaughlin.
1969.
Smog grading anew readability formula.
Journal of Reading,12(8):639?646.Sarah E. Petersen and Mari Ostendorf.
2006.
Amachine learning approach to reading level as-sessment.
Technical report, University of Wash-ington CSE Technical Report.Sarah E. Petersen and Mari Ostendorf.
2009.
A ma-chine learning approach to reading level assess-ment.
Computer Speech and Language, 23:89?106.Emily Pitler and Ani Nenkova.
2008.
Revisitingreadability: A unified framework for predict-ing text quality.
In Proceedings of the 2008Conference on Empirical Methods in NaturalLanguage Processing.R.
Prasad, N. Dinesh, A. Lee, E. Miltsakaki,L.
Robaldo, A. Joshi, and B. Webber.
2008.
ThePenn discourse treebank.
In The Sixth Interna-tional Conference on Language Resources andEvaluation (LREC?08).Sarah E. Schwarm and Mari Ostendorf.
2005.Reading level assessment using support vectormachines and statistical language models.
InProceedings of the 43rd Annual Meeting of theAssociation for Computational Linguistics.Luo Si and Jamie Callan.
2001.
A statistical modelfor scientific readability.
In Proceedings of theTenth International Conference on Informationand Knowledge Management.Radu Soricut and Daniel Marcu.
2006.
Discoursegeneration using utility-trained coherence mod-els.
In Proceedings of the 21st InternationalConference on Computational Linguistics and44th Annual Meeting of the Association for Com-putational Linguistics.284
