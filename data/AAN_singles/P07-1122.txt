Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 968?975,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsGeneralizing Tree Transformations for Inductive Dependency ParsingJens Nilsson?
Joakim Nivre???Va?xjo?
University, School of Mathematics and Systems Engineering, Sweden?Uppsala University, Dept.
of Linguistics and Philology, Sweden{jni,nivre,jha}@msi.vxu.seJohan Hall?AbstractPrevious studies in data-driven dependencyparsing have shown that tree transformationscan improve parsing accuracy for specificparsers and data sets.
We investigate towhat extent this can be generalized acrosslanguages/treebanks and parsers, focusingon pseudo-projective parsing, as a way ofcapturing non-projective dependencies, andtransformations used to facilitate parsing ofcoordinate structures and verb groups.
Theresults indicate that the beneficial effect ofpseudo-projective parsing is independent ofparsing strategy but sensitive to language ortreebank specific properties.
By contrast, theconstruction specific transformations appearto be more sensitive to parsing strategy buthave a constant positive effect over severallanguages.1 IntroductionTreebank parsers are trained on syntactically anno-tated sentences and a major part of their success canbe attributed to extensive manipulations of the train-ing data as well as the output of the parser, usuallyin the form of various tree transformations.
Thiscan be seen in state-of-the-art constituency-basedparsers such as Collins (1999), Charniak (2000), andPetrov et al (2006), and the effects of different trans-formations have been studied by Johnson (1998),Klein andManning (2003), and Bikel (2004).
Corre-sponding manipulations in the form of tree transfor-mations for dependency-based parsers have recentlygained more interest (Nivre and Nilsson, 2005; Halland Nova?k, 2005; McDonald and Pereira, 2006;Nilsson et al, 2006) but are still less studied, partlybecause constituency-based parsing has dominatedthe field for a long time, and partly because depen-dency structures have less structure to manipulatethan constituent structures.Most of the studies in this tradition focus on a par-ticular parsing model and a particular data set, whichmeans that it is difficult to say whether the effectof a given transformation is dependent on a partic-ular parsing strategy or on properties of a particu-lar language or treebank, or both.
The aim of thisstudy is to further investigate some tree transforma-tion techniques previously proposed for data-drivendependency parsing, with the specific aim of tryingto generalize results across languages/treebanks andparsers.
More precisely, we want to establish, firstof all, whether the transformation as such makesspecific assumptions about the language, treebankor parser and, secondly, whether the improved pars-ing accuracy that is due to a given transformation isconstant across different languages, treebanks, andparsers.The three types of syntactic phenomena that willbe studied here are non-projectivity, coordinationand verb groups, which in different ways pose prob-lems for dependency parsers.
We will focus on treetransformations that combine preprocessing withpost-processing, and where the parser is treated asa black box, such as the pseudo-projective parsingtechnique proposed by Nivre and Nilsson (2005)and the tree transformations investigated in Nils-son et al (2006).
To study the influence of lan-968guage and treebank specific properties we will usedata from Arabic, Czech, Dutch, and Slovene, takenfrom the CoNLL-X shared task on multilingual de-pendency parsing (Buchholz and Marsi, 2006).
Tostudy the influence of parsing methodology, we willcompare two different parsers: MaltParser (Nivre etal., 2004) and MSTParser (McDonald et al, 2005).Note that, while it is possible in principle to distin-guish between syntactic properties of a language assuch and properties of a particular syntactic annota-tion of the language in question, it will be impossi-ble to tease these apart in the experiments reportedhere, since this would require having not only mul-tiple languages but also multiple treebanks for eachlanguage.
In the following, we will therefore speakabout the properties of treebanks (rather than lan-guages), but it should be understood that these prop-erties in general depend both on properties of thelanguage and of the particular syntactic annotationadopted in the treebank.The rest of the paper is structured as follows.
Sec-tion 2 surveys tree transformations used in depen-dency parsing and discusses dependencies betweentransformations, on the one hand, and treebanks andparsers, on the other.
Section 3 introduces the fourtreebanks used in this study, and section 4 brieflydescribes the two parsers.
Experimental results arepresented in section 5 and conclusions in section 6.2 Background2.1 Non-projectivityThe tree transformations that have attracted most in-terest in the literature on dependency parsing arethose concerned with recovering non-projectivity.The definition of non-projectivity can be found inKahane et al (1998).
Informally, an arc is projec-tive if all tokens it covers are descendants of the arc?shead token, and a dependency tree is projective if allits arcs are projective.1The full potential of dependency parsing can onlybe realized if non-projectivity is allowed, whichpose a problem for projective dependency parsers.Direct non-projective parsing can be performed withgood accuracy, e.g., using the Chu-Liu-Edmonds al-1If dependency arcs are drawn above the linearly orderedsequence of tokens, preceded by a special root node, then a non-projective dependency tree always has crossing arcs.gorithm, as proposed byMcDonald et al (2005).
Onthe other hand, non-projective parsers tend, amongother things, to be slower.
In order to maintain thebenefits of projective parsing, tree transformationstechniques to recover non-projectivity while using aprojective parser have been proposed in several stud-ies, some described below.In discussing the recovery of empty categories indata-driven constituency parsing, Campbell (2004)distinguishes between approaches based on purepost-processing and approaches based on a combi-nation of preprocessing and post-processing.
Thesame division can be made for the recovery of non-projective dependencies in data-driven dependencyparsing.Pure Post-processingHall and Nova?k (2005) propose a corrective model-ing approach.
The motivation is that the parsers ofCollins et al (1999) and Charniak (2000) adaptedto Czech are not able to create the non-projectivearcs present in the treebank, which is unsatisfac-tory.
They therefore aim to correct erroneous arcs inthe parser?s output (specifically all those arcs whichshould be non-projective) by training a classifier thatpredicts the most probable head of a token in theneighborhood of the head assigned by the parser.Another example is the second-order approximatespanning tree parser developed by McDonald andPereira (2006).
It starts by producing the highestscoring projective dependency tree using Eisner?s al-gorithm.
In the second phase, tree transformationsare performed, replacing lower scoring projectivearcs with higher scoring non-projective ones.Preprocessing with Post-processingThe training data can also be preprocessed to facili-tate the recovery of non-projective arcs in the outputof a projective parser.
The pseudo-projective trans-formation proposed by Nivre and Nilsson (2005) issuch an approach, which is compatible with differ-ent parser engines.First, the training data is projectivized by makingnon-projective arcs projective using a lifting oper-ation.
This is combined with an augmentation ofthe dependency labels of projectivized arcs (and/orsurrounding arcs) with information that probably re-veals their correct non-projective positions.
The out-969(PS)C1 ?S1?C2 ?
(MS)C1?S1 ?C2 ?
(CS)C1?S1 ?C2 ?Figure 1: Dependency structure for coordinationput of the parser, trained on the projectivized data,is then deprojectivized by a heuristic search usingthe added information in the dependency labels.
Theonly assumption made about the parser is thereforethat it can learn to derive labeled dependency struc-tures with augmented dependency labels.2.2 Coordination and Verb GroupsThe second type of transformation concerns linguis-tic phenomena that are not impossible for a projec-tive parser to process but which may be difficult tolearn, given a certain choice of dependency analy-sis.
This study is concerned with two such phe-nomena, coordination and verb groups, for whichtree transformations have been shown to improveparsing accuracy for MaltParser on Czech (Nils-son et al, 2006).
The general conclusion of thisstudy is that coordination and verb groups in thePrague Dependency Treebank (PDT), based on the-ories of the Prague school (PS), are annotated in away that is difficult for the parser to learn.
By trans-forming coordination and verb groups in the train-ing data to an annotation similar to that advocatedby Mel?c?uk (1988) and then performing an inversetransformation on the parser output, parsing accu-racy can therefore be improved.
This is again aninstance of the black-box idea.Schematically, coordination is annotated in thePrague school as depicted in PS in figure 1, wherethe conjuncts are dependents of the conjunction.
InMel?c?uk style (MS), on the other hand, conjunctsand conjunction(s) form a chain going from left toright.
A third way of treating coordination, not dis-cussed by Nilsson et al (2006), is used by the parserof Collins (1999), which internally represents coor-dination as a direct relation between the conjuncts.This is illustrated in CS in figure 1, where the con-junction depends on one of the conjuncts, in thiscase on the rightmost one.Nilsson et al (2006) also show that the annotationof verb groups is not well-suited for parsing PDTusing MaltParser, and that transforming the depen-dency structure for verb groups has a positive impacton parsing accuracy.
In PDT, auxiliary verbs are de-pendents of the main verb, whereas it according toMel?c?uk is the (finite) auxiliary verb that is the headof the main verb.
Again, the parsing experiments inthis study show that verb groups are more difficultto parse in PS than in MS.2.3 Transformations, Parsers, and TreebanksPseudo-projective parsing and transformations forcoordination and verb groups are instances of thesame general methodology:1.
Apply a tree transformation to the training data.2.
Train a parser on the transformed data.3.
Parse new sentences.4.
Apply an inverse transformation to the outputof the parser.In this scheme, the parser is treated as a blackbox.
All that is assumed is that it is a data-drivenparser designed for (projective) labeled dependencystructures.
In this sense, the tree transformationsare independent of parsing methodology.
Whetherthe beneficial effect of a transformation, if any, isalso independent of parsing methodology is anotherquestion, which will be addressed in the experimen-tal part of this paper.The pseudo-projective transformation is indepen-dent not only of parsing methodology but also oftreebank (and language) specific properties, as longas the target representation is a (potentially non-projective) labeled dependency structure.
By con-trast, the coordination and verb group transforma-tions presuppose not only that the language in ques-tion contains these constructions but also that thetreebank adopts a PS annotation.
In this sense, theyare more limited in their applicability than pseudo-projective parsing.
Again, it is a different questionwhether the transformations have a positive effectfor all treebanks (languages) to which they can beapplied.3 TreebanksThe experiments are mostly conducted using tree-bank data from the CoNLL shared task 2006.
This970Slovene Arabic Dutch CzechSDT PADT Alpino PDT# T 29 54 195 1249# S 1.5 1.5 13.3 72.7%-NPS 22.2 11.2 36.4 23.2%-NPA 1.8 0.4 5.4 1.9%-C 9.3 8.5 4.0 8.5%-A 8.8 - - 1.3Table 1: Overview of the data sets (ordered by size),where # S * 1000 = number of sentences, # T * 1000= number of tokens, %-NPS = percentage of non-projective sentences, %-NPA = percentage of non-projective arcs, %-C = percentage of conjuncts, %-A= percentage of auxiliary verbs.subsection summarizes some of the important char-acteristics of these data sets, with an overview in ta-ble 1.
Any details concerning the conversion fromthe original formats of the various treebanks to theCoNLL format, a pure dependency based format, arefound in documentation referred to in Buchholz andMarsi (2006).PDT (Hajic?
et al, 2001) is the largest manuallyannotated treebank, and as already mentioned, itadopts PS for coordination and verb groups.
Asthe last four rows reveal, PDT contains a quite highproportion of non-projectivity, since almost everyfourth dependency graph contains at least one non-projective arc.
The table also shows that coordina-tion is more common than verb groups in PDT.
Only1.3% of the tokens in the training data are identifiedas auxiliary verbs, whereas 8.5% of the tokens areidentified as conjuncts.Both Slovene Dependency Treebank (Dz?eroski etal., 2006) (SDT) and Prague Arabic DependencyTreebank (Hajic?
et al, 2004) (PADT) annotate co-ordination and verb groups as in PDT, since they tooare influenced by the theories of the Prague school.The proportions of non-projectivity and conjuncts inSDT are in fact quite similar to the proportions inPDT.
The big difference is the proportion of auxil-iary verbs, with many more auxiliary verbs in SDTthan in PDT.
It is therefore plausible that the trans-formations for verb groups will have a larger impacton parser accuracy in SDT.Arabic is not a Slavic languages such as Czechand Slovene, and the annotation in PADT is there-fore more dissimilar to PDT than SDT is.
One suchexample is that Arabic does not have auxiliary verbs.Table 1 thus does not give figures verb groups.
Theamount of coordination is on the other hand compa-rable to both PDT and SDT.
The table also revealsthat the amount of non-projective arcs is about 25%of that in PDT and SDT, although the amount ofnon-projective sentences is still as large as 50% ofthat in PDT and SDT.Alpino (van der Beek et al, 2002) in the CoNLLformat, the second largest treebank in this study,is not as closely tied to the theories of the Pragueschool as the others, but still treats coordination ina way similar to PS.
The table shows that coor-dination is less frequent in the CoNLL version ofAlpino than in the three other treebanks.
The othercharacteristic of Alpino is the high share of non-projectivity, where more than every third sentenceis non-projective.
Finally, the lack of informationabout the share of auxiliary verbs is not due to thenon-existence of such verbs in Dutch but to the factthat Alpino adopts an MS annotation of verb groups(i.e., treating main verbs as dependents of auxiliaryverbs), which means that the verb group transforma-tion of Nilsson et al (2006) is not applicable.4 ParsersThe parsers used in the experiments are Malt-Parser (Nivre et al, 2004) and MSTParser (Mc-Donald et al, 2005).
These parsers are based onvery different parsing strategies, which makes themsuitable in order to test the parser independenceof different transformations.
MaltParser adopts agreedy, deterministic parsing strategy, deriving a la-beled dependency structure in a single left-to-rightpass over the input and uses support vector ma-chines to predict the next parsing action.
MST-Parser instead extracts a maximum spanning treefrom a dense weighted graph containing all possi-ble dependency arcs between tokens (with Eisner?salgorithm for projective dependency structures orthe Chu-Liu-Edmonds algorithm for non-projectivestructures), using a global discriminative model andonline learning to assign weights to individual arcs.22The experiments in this paper are based on the first-orderfactorization described in McDonald et al (2005)9715 ExperimentsThe experiments reported in section 5.1?5.2 beloware based on the training sets from the CoNLL-Xshared task, except where noted.
The results re-ported are obtained by a ten-fold cross-validation(with a pseudo-randomized split) for all treebanksexcept PDT, where 80% of the data was used fortraining and 20% for development testing (againwith a pseudo-randomized split).
In section 5.3, wegive results for the final evaluation on the CoNLL-X test sets using all three transformations togetherwith MaltParser.Parsing accuracy is primarily measured by the un-labeled attachment score (ASU ), i.e., the propor-tion of tokens that are assigned the correct head, ascomputed by the official CoNLL-X evaluation scriptwith default settings (thus excluding all punctuationtokens).
In section 5.3 we also include the labeledattachment score (ASL) (where a token must haveboth the correct head and the correct dependency la-bel to be counted as correct), which was the officialevaluation metric in the CoNLL-X shared task.5.1 Comparing TreebanksWe start by examining the effect of transformationson data from different treebanks (languages), usinga single parser: MaltParser.Non-projectivityThe question in focus here is whether the effect ofthe pseudo-projective transformation for MaltParservaries with the treebank.
Table 2 presents the un-labeled attachment score results (ASU ), compar-ing the pseudo-projective parsing technique (P-Proj)with two baselines, obtained by training the strictlyprojective parser on the original (non-projective)training data (N-Proj) and on projectivized train-ing data with no augmentation of dependency labels(Proj).The first thing to note is that pseudo-projectiveparsing gives a significant improvement for PDT,as previously reported by Nivre and Nilsson (2005),but also for Alpino, where the improvement is evenlarger, presumably because of the higher proportionof non-projective dependencies in the Dutch tree-bank.
By contrast, there is no significant improve-ment for either SDT or PADT, and even a small dropN-Proj Proj P-ProjSDT 77.27 76.63??
77.11PADT 76.96 77.07?
77.07?Alpino 82.75 83.28??
87.08?
?PDT 83.41 83.32??
84.42?
?Table 2: ASU for pseudo-projective parsing withMaltParser.
McNemar?s test: ?
= p < .05 and??
= p < 0.01 compared to N-Proj.1 2 3 >3SDT 88.4 9.1 1.7 0.84PADT 66.5 14.4 5.2 13.9Alpino 84.6 13.8 1.5 0.07PDT 93.8 5.6 0.5 0.1Table 3: The number of lifts for non-projective arcs.in the accuracy figures for SDT.
Finally, in contrastto the results reported by Nivre and Nilsson (2005),simply projectivizing the training data (without us-ing an inverse transformation) is not beneficial at all,except possibly for Alpino.But why does not pseudo-projective parsing im-prove accuracy for SDT and PADT?
One possi-ble factor is the complexity of the non-projectiveconstructions, which can be measured by countingthe number of lifts that are required to make non-projective arcs projective.
The more deeply nesteda non-projective arc is, the more difficult it is to re-cover because of parsing errors as well as search er-rors in the inverse transformation.
The figures in ta-ble 3 shed some interesting light on this factor.For example, whereas 93.8% of all arcs in PDTrequire only one lift before they become projec-tive (88.4% and 84.6% for SDT and Alpino, respec-tively), the corresponding figure for PADT is as lowas 66.5%.
PADT also has a high proportion of verydeeply nested non-projective arcs (>3) in compari-son to the other treebanks, making the inverse trans-formation for PADT more problematic than for theother treebanks.
The absence of a positive effect forPADT is therefore understandable given the deeplynested non-projective constructions in PADT.However, one question that still remains is whySDT and PDT, which are so similar in terms of bothnesting depth and amount of non-projectivity, be-972Figure 2: Learning curves for Alpino measured aserror reduction for ASU .have differently with respect to pseudo-projectiveparsing.
Another factor that may be important hereis the amount of training data available.
As shownin table 1, PDT is more than 40 times larger thanSDT.
To investigate the influence of training setsize, a learning curve experiment has been per-formed.
Alpino is a suitable data set for this dueto its relatively large amount of both data and non-projectivity.Figure 2 shows the learning curve for pseudo-projective parsing (P-Proj), compared to using onlyprojectivized training data (Proj), measured as errorreduction in relation to the original non-projectivetraining data (N-Proj).
The experiment was per-formed by incrementally adding cross-validationfolds 1?8 to the training set, using folds 9?0 as statictest data.One can note that the error reduction for Proj isunaffected by the amount of data.
While the errorreduction varies slightly, it turns out that the errorreduction is virtually the same for 10% of the train-ing data as for 80%.
That is, there is no correla-tion if information concerning the lifts are not addedto the labels.
However, with a pseudo-projectivetransformation, which actively tries to recover non-projectivity, the learning curve clearly indicates thatthe amount of data matters.
Alpino, with 36% non-projective sentences, starts at about 17% and has aclimbing curve up to almost 25%.Although this experiment shows that there is acorrelation between the amount of data and the accu-racy for pseudo-projective parsing, it does probablynot tell the whole story.
If it did, one would expectthat the error reduction for the pseudo-projectivetransformation would be much closer to Proj whenNone Coord VGSDT 77.27 79.33??
77.92?
?PADT 76.96 79.05??
-Alpino 82.75 83.38??
-PDT 83.41 85.51??
83.58?
?Table 4: ASU for coordination and verb group trans-formations with MaltParser (None = N-Proj).
Mc-Nemar?s test: ??
= p < .01 compared to None.the amount of data is low (to the left in the fig-ure) than they apparently are.
Of course, the dif-ference is likely to diminish with even less data, butit should be noted that 10% of Alpino has about halfthe size of PADT, for which the positive impact ofpseudo-projective parsing is absent.
The absenceof increased accuracy for SDT can partially be ex-plained by the higher share of non-projective arcs inAlpino (3 times more).Coordination and Verb GroupsThe corresponding parsing results using MaltParserwith transformations for coordination and verbgroups are shown in table 4.
For SDT, PADT andPDT, the annotation of coordination has been trans-formed from PS to MS, as described in Nilsson etal.
(2006).
For Alpino, the transformation is fromPS to CS (cf.
section 2.2), which was found to giveslightly better performance in preliminary experi-ments.
The baseline with no transformation (None)is the same as N-Proj in table 2.As the figures indicate, transforming coordinationis beneficial not only for PDT, as reported by Nilssonet al (2006), but also for SDT, PADT, and Alpino.
Itis interesting to note that SDT, PADT and PDT, withcomparable amounts of conjuncts, have compara-ble increases in accuracy (about 2 percentage pointseach), despite the large differences in training setsize.
It is therefore not surprising that Alpino, witha much smaller amount of conjuncts, has a lower in-crease in accuracy.
Taken together, these results in-dicate that the frequency of the construction is moreimportant than the size of the training set for thistype of transformation.The same generalization over treebanks holds forverb groups too.
The last column in table 4 showsthat the expected increase in accuracy for PDT is ac-973Algorithm N-Proj Proj P-ProjEisner 81.79 83.23 86.45CLE 86.39Table 5: Pseudo-projective parsing results (ASU ) forAlpino with MSTParser.companied by a even higher increase for SDT.
Thiscan probably be attributed to the higher frequency ofauxiliary verbs in SDT.5.2 Comparing ParsersThe main question in this section is to what extentthe positive effect of different tree transformationsis dependent on parsing strategy, since all previ-ous experiments have been performed with a singleparser (MaltParser).
For comparison we have per-formed two experiments with MSTParser, version0.1, which is based on a very different parsing meth-dology (cf.
section 4).
Due to some technical dif-ficulties (notably the very high memory consump-tion when using MSTParser for labeled dependencyparsing), we have not been able to replicate the ex-periments from the preceding section exactly.
Theresults presented below must therefore be regardedas a preliminary exploration of the dependencies be-tween tree transformations and parsing strategy.Table 5 presents ASU results for MSTParser incombination with pseudo-projective parsing appliedto the Alpino treebank of Dutch.3 The first rowcontains the result for Eisner?s algorithm using notransformation (N-Proj), projectivized training data(Proj), and pseudo-projective parsing (P-Proj).
Thefigures show a pattern very similar to that for Malt-Parser, with a boost in accuracy for Proj comparedto N-Proj, and with a significantly higher accuracyfor P-Proj over Proj.
It is also worth noting that theerror reduction between N-Proj and P-Proj is actu-ally higher for MSTParser here than for MaltParserin table 2.The second row contains the result for the Chu-Liu-Edmonds algorithm (CLE), which constructsnon-projective structures directly and therefore does3The figures are not completely comparable to the previ-ously presented Dutch results for MaltParser, sinceMaltParser?sfeature model has access to all the information in the CoNLLdata format, whereas MSTParser in this experiment only couldhandle word forms and part-of-speech tags.Trans.
None Coord VGASU 84.5 83.5 84.5Table 6: Coordination and verb group transforma-tions for PDT with the CLE algorithm.Dev Eval Niv McDSDT ASU 80.40 82.01 78.72 83.17ASL 71.06 72.44 70.30 73.44PADT ASU 78.97 78.56 77.52 79.34ASL 67.63 67.58 66.71 66.91Alpino ASU 87.63 82.85 81.35 83.57ASL 84.02 79.73 78.59 79.19PDT ASU 85.72 85.98 84.80 87.30ASL 78.56 78.80 78.42 80.18Table 7: Evaluation on CoNLL-X test data; Malt-Parser with all transformations (Dev = development,Eval = CoNLL test set, Niv = Nivre et al (2006),McD = McDonald et al (2006))not require the pseudo-projective transformation.A comparison between Eisner?s algorithm withpseudo-projective transformation and CLE revealsthat pseudo-projective parsing is at least as accurateas non-projective parsing for ASU .
(The small dif-ference is not statistically significant.
)By contrast, no positive effect could be detectedfor the coordination and verb group transformationstogther with MSTParser.
The figures in table 6 arenot based on CoNLL data, but instead on the evalu-ation test set of the original PDT 1.0, which enablesa direct comparison to McDonald et.
al.
(2005) (theNone column).
We see that there is even a negativeeffect for the coordination transformation.
These re-sults clearly indicate that the effect of these transfor-mations is at least partly dependent on parsing strat-egy, in contrast to what was found for the pseudo-projective parsing technique.5.3 Combining TransformationsIn order to assess the combined effect of all threetransformations in relation to the state of the art,we performed a final evaluation using MaltParser onthe dedicated test sets from the CoNLL-X sharedtask.
Table 7 gives the results for both develop-ment (cross-validation for SDT, PADT, and Alpino;974development set for PDT) and final test, comparedto the two top performing systems in the sharedtask, MSTParser with approximate second-ordernon-projective parsing (McDonald et al, 2006) andMaltParser with pseudo-projective parsing (but nocoordination or verb group transformations) (Nivreet al, 2006).
Looking at the labeled attachmentscore (ASL), the official scoring metric of theCoNLL-X shared task, we see that the combined ef-fect of the three transformations boosts the perfor-mance of MaltParser for all treebanks and in twocases out of four outperforms MSTParser (whichwas the top scoring system for all four treebanks).6 ConclusionIn this paper, we have examined the generalityof tree transformations for data-driven dependencyparsing.
The results indicate that the pseudo-projective parsing technique has a positive effecton parsing accuracy that is independent of parsingmethodology but sensitive to the amount of trainingdata as well as to the complexity of non-projectiveconstructions.
By contrast, the construction-specifictransformations targeting coordination and verbgroups appear to have a more language-independenteffect (for languages to which they are applicable)but do not help for all parsers.
More research isneeded in order to know exactly what the dependen-cies are between parsing strategy and tree transfor-mations.
Regardless of this, however, it is safe toconclude that pre-processing and post-processing isimportant not only in constituency-based parsing, aspreviously shown in a number of studies, but also forinductive dependency parsing.ReferencesD.
Bikel.
2004.
Intricacies of Collins?
parsing model.
Compu-tational Linguistics, 30:479?511.S.
Buchholz and E. Marsi.
2006.
CoNLL-X Shared Taskon Multilingual Dependency Parsing.
In Proceedings ofCoNLL, pages 1?17.R.
Campbell.
2004.
Using Linguistic Principles to RecoverEmpty Categories.
In Proceedings of ACL, pages 645?652.E.
Charniak.
2000.
A Maximum-Entropy-Inspired Parser.
InProceedings of NAACL, pages 132?139.M.
Collins, J.
Hajic?, L. Ramshaw, and C. Tillmann.
1999.
Astatistical parser for Czech.
In Proceedings of ACL, pages100?110.M.
Collins.
1999.
Head-Driven Statistical Models for NaturalLanguage Parsing.
Ph.D. thesis, University of Pennsylvania.S.
Dz?eroski, T. Erjavec, N. Ledinek, P. Pajas, Z.
Z?abokrtsky, andA.
Z?ele.
2006.
Towards a Slovene Dependency Treebank.In LREC.J.
Hajic?, B. V. Hladka, J.
Panevova?, Eva Hajic?ova?, Petr Sgall,and Petr Pajas.
2001.
Prague Dependency Treebank 1.0.LDC, 2001T10.J.
Hajic?, O.
Smrz?, P. Zema?nek, J.
S?naidauf, and E. Bes?ka.
2004.Prague Arabic Dependency Treebank: Development in Dataand Tools.
In NEMLAR, pages 110?117.K.
Hall and V. Nova?k.
2005.
Corrective modeling for non-projective dependency parsing.
In Proceedings of IWPT,pages 42?52.M.
Johnson.
1998.
PCFG Models of Linguistic Tree Represen-tations.
Computational Linguistics, 24:613?632.S.
Kahane, A. Nasr, and O. Rambow.
1998.
Pseudo-Projectivity: A Polynomially Parsable Non-Projective De-pendency Grammar.
In Proceedings of COLING/ACL, pages646?652.D.
Klein and C. Manning.
2003.
Accurate unlexicalized pars-ing.
In Proceedings of ACL, pages 423?430.R.
McDonald and F. Pereira.
2006.
Online Learning of Ap-proximate Dependency Parsing Algorithms.
In Proceedingsof EACL, pages 81?88.R.
McDonald, F. Pereira, K. Ribarov, and J. Hajic?.
2005.Non-projective dependency parsing using spanning tree al-gorithms.
In Proceedings of HLT/EMNLP, pages 523?530.R.
McDonald, K. Lerman, and F. Pereira.
2006.
Multilingualdependency analysis with a two-stage discriminative parser.In Proceedings of CoNLL, pages 216?220.I.
Mel?c?uk.
1988.
Dependency Syntax: Theory and Practice.State University of New York Press.J.
Nilsson, J. Nivre, and J.
Hall.
2006.
Graph Transforma-tions in Data-Driven Dependency Parsing.
In Proceedingsof COLING/ACL, pages 257?264.J.
Nivre and J. Nilsson.
2005.
Pseudo-Projective DependencyParsing.
In Proceedings of ACL, pages 99?106.J.
Nivre, J.
Hall, and J. Nilsson.
2004.
Memory-based Depen-dency Parsing.
In H. T. Ng and E. Riloff, editors, Proceed-ings of CoNLL, pages 49?56.J.
Nivre, J.
Hall, J. Nilsson, G. Eryig?it, and S. Marinov.
2006.Labeled Pseudo-Projective Dependency Parsing with Sup-port Vector Machines.
In Proceedings of CoNLL, pages221?225.S.
Petrov, L. Barrett, R. Thibaux, and D. Klein.
2006.
LearningAccurate, Compact, and Interpretable Tree Annotation.
InProceedings of COLING/ACL, pages 433?440.L.
van der Beek, G. Bouma, R. Malouf, and G. van Noord.2002.
The Alpino dependency treebank.
In ComputationalLinguistics in the Netherlands (CLIN).975
