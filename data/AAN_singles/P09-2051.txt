Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 201?204,Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLPMining Association Language Patterns forNegative Life Event ClassificationLiang-Chih Yu1, Chien-Lung Chan1, Chung-Hsien Wu2 and Chao-Cheng Lin31Department of Information Management, Yuan Ze University, Taiwan, R.O.C.2Department of CSIE, National Cheng Kung University, Taiwan, R.O.C.3Department of Psychiatry, National Taiwan University Hospital, Taiwan, R.O.C.
{lcyu, clchan}@saturn.yzu.edu.tw, chwu@csie.ncku.edu.tw, linchri@gmail.comAbstractNegative life events, such as death of a familymember, argument with a spouse and loss of ajob, play an important role in triggering de-pressive episodes.
Therefore, it is worth to de-velop psychiatric services that can automati-cally identify such events.
In this paper, wepropose the use of association language pat-terns, i.e., meaningful combinations of words(e.g., <loss, job>), as features to classify sen-tences with negative life events into prede-fined categories (e.g., Family, Love, Work).The language patterns are discovered using adata mining algorithm, called association pat-tern mining, by incrementally associating fre-quently co-occurred words in the sentencesannotated with negative life events.
The dis-covered patterns are then combined with sin-gle words to train classifiers.
Experimental re-sults show that association language patternsare significant features, thus yielding betterperformance than the baseline system usingsingle words alone.1 IntroductionWith the increased incidence of depressive dis-orders, many psychiatric websites have devel-oped community-based services such as messageboards, web forums and blogs for public access.Through these services, individuals can describetheir stressful or negative life events such asdeath of a family member, argument with aspouse and loss of a job, along with depressivesymptoms, such as depressive mood, suicidaltendencies and anxiety.
Such psychiatric texts(e.g., forum posts) contain large amounts of natu-ral language expressions related to negative lifeevents, making them useful resources for build-ing more effective psychiatric services.
For in-stance, a psychiatric retrieval service can retrieverelevant forum or blog posts according to thenegative life events experienced by users so thatthey can be aware that they are not alone becausemany people have suffered from the same orsimilar problems.
The users can then create acommunity discussion to share their experienceswith each other.
Additionally, a dialog systemcan generate supportive responses like ?Don?tworry?, ?That?s really sad?
and ?Cheer up?
if itcan understand the negative life events embed-ded in the example sentences shown in Table 1.Therefore, this study proposes a framework fornegative life event classification.
We formulatethis problem as a sentence classification task;that is, classify sentences according to the type ofnegative life events within them.
The class labelsused herein are presented in Table 1, which arederived from Brostedt and Pedersen (2003).Traditional approaches to sentence classifica-tion (Khoo et al, 2006; Naughton et al, 2008) ortext categorization (Sebastiani 2002) usuallyadopt bag-of-words as baseline features to trainclassifiers.
Since the bag-of-words approachtreats each word independently without consider-ing the relationships of words in sentences, someresearchers have investigated the use of n-gramsto capture sequential relations between words toboost classification performance (Chitturi andHansen, 2008; Li and Zong, 2008).
The use of n-grams is effective in capturing local dependen-cies of words, but tends to suffer from datasparseness problem in capturing long-distancedependencies since higher-order n-grams requirelarge training data to obtain reliable estimation.For our task, the expressions of negative lifeevents can be characterized by association lan-guage patterns, i.e., meaningful combinations ofwords, such as <worry, children, health>, <breakup, boyfriend>, <argue, friend>, <loss, job>, and201<school, teacher, blame> in the example sen-tences in Table 1.
Such language patterns are notnecessarily composed of continuous words.
In-stead, they are usually composed of the wordswith long-distance dependencies, which cannotbe easily captured by n-grams.Therefore, the aim of this study is two-fold: (1)to automatically discover association languagepatterns from the sentences annotated with nega-tive life events; and (2) to classify sentences withnegative life events using the discovered patterns.To discover association language patterns, weincorporate the measure mutual information (MI)into a data mining algorithm, called associationpattern mining, to incrementally derive fre-quently co-occurred words in sentences (Section2).
The discovered patterns are then combinedwith single words as features to train classifiersfor negative life event classification (Section 3).Experimental results are presented in Section 4.Conclusions are finally drawn in Section 5.2 Association Language Pattern MiningThe problem of language pattern acquisition canbe converted into the problem of association pat-tern mining, where each sales transaction in adatabase can be considered as a sentence in thecorpora, and each item in a transaction denotes aword in a sentence.
An association language pat-tern is defined herein as a combination of multi-ple associated words, denoted by 1,..., kw w< > .Thus, the task of association pattern mining is tomine the language patterns of frequently associ-ated words from the training sentences.
For thispurpose, we adopt the Apriori algorithm(Agrawal and Srikant, 1994) and modified itslightly to fit our application.
Its basic concept isto identify frequent word sets recursively, andthen generate association language patterns fromthe frequent word sets.
For simplicity, only thecombinations of nouns and verbs are considered,and the length is restricted to at most 4 words,i.e., 2-word, 3-word and 4-word combinations.The detailed procedure is described as follows.2.1 Find frequent word setsA word set is frequent if it possesses a minimumsupport.
The support of a word set is defined asthe number of training sentences containing theword set.
For instance, the support of a two-wordset { iw , jw } denotes the number of training sen-tences containing the word pair ( iw , jw ).
Thefrequent k-word sets are discovered from (k-1)-word sets.
First, the support of each word, i.e.,word frequency, in the training corpus is counted.The set of frequent one-word sets, denoted as 1L ,is then generated by choosing the words with aminimum support level.
To calculate kL , the fol-lowing two-step process is performed iterativelyuntil no more frequent k-word sets are found.z Join step: A set of candidate k-word sets,denoted as kC , is first generated by merg-ing frequent word sets of 1kL ?
, in whichonly the word sets whose first (k-2) wordsare identical can be merged.z Prune step: The support of each candidateword set in kC  is then counted to determinewhich candidate word sets are frequent.
Fi-nally, the candidate word sets with a sup-port count greater than or equal to theminimum support are considered to formkL .
The candidate word sets with a subsetthat is not frequent are eliminated.
Figure 1shows an example of generating kL .Label Description Example SentenceFamily  Serious illness of a family member;Son or daughter leaving homeI am very worried about my children?s health.Love Spouse/mate engaged in infidelity;Broke up with a boyfriend or girlfriendI broke up with my dear but cruel boyfriendrecently.School Examination failed or grade dropped;Unable to enter/stay in schoolI hate to go to school because my teacher al-ways blames me.Work Laid off or fired from a job;Demotion and salary reductionI lost my job in this economic recession a fewmonths ago.Social Substantial conflicts with a friend;Difficulties in social activitiesI argued with my best friend and was upset.Table 1.
Classification of negative life events.2022.2 Generate association patterns from fre-quent word setsAssociation language patterns can be generatedvia a confidence measure once the frequent wordsets have been identified.
The confidence of anassociation language pattern of k words is de-fined as the mutual information of the k words,as shown below.1 1111( ,... ) ( ,... )( ,... )( ,... ) log( )k kkk kiiConf w w MI w wP w wP w wP w=< > ==?
(1)where 1( ,... )kP w w  denotes the probability of thek words co-occurring in a sentence in the trainingset, and ( )iP w  denotes the probability of a sin-gle word occurring in the training set.
Accord-ingly, each frequent word set in kL  is assigned amutual information score.
In order to generate aset of association language patterns, all frequentword sets are sorted in the descending order ofthe mutual information scores.
The minimumconfidence (a threshold at percentage) is thenapplied to select top N percent frequent word setsas the resulting language patterns.
This thresholdis determined empirically by maximizing classi-fication performance (Section 4).
Figure 1 (right-hand side) shows an example of generating theassociation language patterns from kL .3 Sentence ClassificationThe classifiers used in this study include SupportVector Machine (SVM), C4.5, and Na?ve Bayes(NB) classifier, which is provided by WekaPackage (Witten and Frank, 2005).
The featureset includes:Bag-of-Words (BOW): Each single word insentences.Association language patterns (ALP): The topN percent association language patterns acquiredin the previous section.Ontology expansion (Onto): The top N percentassociation language patterns are expanded bymapping the constituent words into their syno-nyms.
For example, the pattern <boss, conflict>can be expanded as <chief, conflict> since thewords boss and chief are synonyms.
Here we usethe HowNet (http://www.keenage.com), a Chi-nese lexical ontology, for pattern expansion.4 Experimental ResultsData set: A total of 2,856 sentences were col-lected from the Internet-based Self-assessmentProgram for Depression (ISP-D) database of thePsychPark (http://www.psychpark.org), a virtualpsychiatric clinic, maintained by a group of vol-unteer professionals of Taiwan Association ofMental Health Informatics (Bai et al, 2001).Each sentence was then annotated by trained an-notators with one of the five types of negativelife events.
Table 2 shows the break-down of thedistribution of sentence types.The data set was randomly split into a trainingset, a development set, and a test set with an8:1:1 ratio.
The training set was used for lan-guage pattern generation.
The development setwas used to optimize the threshold (Section 2.2)for the classifiers (SVM, C4.5 and NB).
Eachclassifier was implemented using three differentlevels of features, namely BOW, BOW+ALP,Prune Step(min.
support)SortingandThresholding<Boyfriend, Conflict><Boyfriend, Break up><Boss, Conflict><Conflict, Break up>Find Frequent Word Sets Generate Association Language PatternsJoinStepPrune Step(min.
support)Prune Step(min.
support)<Boyfriend, Conflict, Break up>Join StepFigure 1.
Example of generating association language patterns.Sentence Type % in CorpusFamily 28.8Love 22.8School 13.3Work 14.3Social 20.8Table 2.
Distribution of sentence types.203and BOW+ALP+Onto, to examine the effective-ness of association language patterns.
The classi-fication performance is measured by accuracy,i.e., the number of correctly classified sentencesdivided by the total number of test sentences.4.1 Evaluation on threshold selectionSince not all discovered association languagepatterns contribute to the classification task, thethreshold described in Section 2.2 is used to se-lect top N percent patterns for classification.
Thisexperiment is to determine an optimal thresholdfor each involved classifier by maximizing itsclassification accuracy on the development set.Figure 2 shows the classification accuracy of NBagainst different threshold values.When using association language patterns asfeatures (BOW+ALP), the accuracy increasedwith increasing the threshold value up to 0.6,indicating that the top 60% discovered patternscontained more useful patterns for classification.By contrast, the accuracy decreased when thethreshold value was above 0.6, indicating that theremaining 40% contained more noisy patternsthat may increase the ambiguity in classification.When using the ontology expansion approach(BOW+ALP+Onto), both the number and diver-sity of discovered patterns are increased.
There-fore, the accuracy was improved and the optimalaccuracy was achieved at 0.5.
However, the ac-curacy dropped significantly when the thresholdvalue was above 0.5.
This finding indicates thatexpansion on noisy patterns may produce morenoisy patterns and thus decrease performance.4.2 Results of classification performanceThe results of each classifier were obtained fromthe test set using its own threshold optimized inthe previous section.
Table 3 shows the compara-tive results of different classifiers with differentlevels of features.
The incorporation of associa-tion language patterns improved the accuracy ofNB, C4.5, and SVM by 3.9%, 1.9%, and 2.2%,respectively, and achieved an average improve-ment of 2.7%.
Additionally, the use of ontologyexpansion can further improve the performanceby 1.6% in average.
This finding indicates thatassociation language patterns are significant fea-tures for negative life event classification.5 ConclusionThis work has presented a framework that uses adata mining algorithm and ontology expansionmethod to acquire association language patternsfor negative life event classification.
The asso-ciation language patterns can capture word rela-tionships in sentences, thus yielding higher per-formance than the baseline system using singlewords alone.
Future work will focus on devisinga semi-supervised or unsupervised method forlanguage pattern acquisition from web resourcesso as to reduce reliance on annotated corpora.ReferencesR.
Agrawal and R. Srikant.
1994.
Fast Algorithms for Min-ing Association Rules.
In Proc.
Int?l Conf.
Very LargeData Bases (VLDB), pages 487-499.Y.
M. Bai, C. C. Lin, J. Y. Chen, and W. C. Liu.
2001.
Vir-tual Psychiatric Clinics.
American Journal of Psychiatry,vol.
158, no.
7, pp.
1160-1161.E.
M. Brostedt and N. L. Pedersen.
2003.
Stressful LifeEvents and Affective Illness.
Acta Psychiatrica Scandi-navica, vol.
107, pp.
208-215.R.
Chitturi and J. H.L.
Hansen.
2008.
Dialect Classificationfor online podcasts fusing Acoustic and Language basedStructural and Semantic Information.
In Proc.
of ACL-08,pages 21-24.A.
Khoo, Y. Marom and D. Albrecht.
2006.
Experimentswith Sentence Classification.
In Proc.
of AustralasianLanguage Technology Workshop, pages 18-25.S.
Li and C. Zong.
2008.
Multi-domain Sentiment Classifi-cation.
In Proc.
of ACL-08, pages 257-260.M.
Naughton, N. Stokes, and J. Carthy.
2008.
InvestigatingStatistical Techniques for Sentence-Level Event Classi-fication.
In Proc.
of COLING-08, pages 617-624.F.
Sebastiani.
2002.
Machine Learning in Automated TextCategorization.
ACM Computing Surveys, vol.
34, no.
1,pp.
1-47.I.
H. Witten and E. Frank.
2005.
Data Mining: PracticalMachine Learning Tools and Techniques, 2nd Edition,Morgan Kaufmann, San Francisco.NB C4.5 SVMBOW 0.717 0.741 0.787BOW+ALP 0.745 0.755 0.804BOW+ALP+Onto 0.759 0.766 0.815Table 3.
Accuracy of classifiers on testing data.0.620.640.660.680.700.720.740.760.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0ThresholdAccuracyBOW+ALPBOW+ALP+OntoFigure 2.
Threshold selection.204
