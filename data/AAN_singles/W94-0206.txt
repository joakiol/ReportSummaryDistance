PARSING USING LINEARLY ORDERED PHONOLOGICAL RULESMichael MaxwellSummer Institute of Linguistics7809 Radin RoadWaxhaw, NC 28173 USAIntemet: Mike.Maxwell@sil.orgAbstractA generate and test algorithm isdescribed which parses a surface form into one ormore lexical entries using linearly orderedphonological rules.
This algorithm avoids theexponential expansion of search space which anaive parsing algorithm would face by encodinginto the form being parsed the ambiguities whicharise during parsing.
The algorithm has beenimplemented and tested on real language data,and its speed compares favorably with that of aKIMMO-type parser.I.
INTRODUCTIONA generate and test algorithm isdescribed which uses linearly orderedphonological rules to parse a surface form intoone or more underlying (lexicai) forms.
* Eachstep of the derivation may be rendered visibleduring both generation and test phases.
Thealgorithm avoids an exponential expansion ofsearch space during the generation phase byencoding the ambiguities which arise into theform being parsed.
At the end of the generationphase, lexical lookup matches the ambiguousform against lexical entries.
Because not a l lcombinations of ambiguities in the parsed formare compatible, a test phase is used to filter* 1 have benefited from comments on previousversions of this paper by Alan Busernan, andseveral anonymous referees.
Errors remain myown.forms found at lexical lookup.
In this phase, thephonological rules are applied in forward order,and the derivations of any final forms which donot match the original input word are thrown out.The algorithm has been implemented andtested on real language data; its speed iscomparable tothat of a KIMMO-type parser.2.
THE PROBLEMSince the publication of The 5bundPattern of  English (Chomsky and Halle 1968),most generative linguists have held that thephonological rules of natural languages arelinearly ordered (Bromberger and Halle 1989).That is, when deriving a surface, form from anunderlying (lexical) form, the input of the N+lthrule is the output of the Nth rule.While it is straightforward to derive asurface form from an underlying form withlinearly ordered rules, complications arise insearching for the Icxical form(s) from which agiven surface form may he dcrivcd.
Onedifficulty is that phonological rules are oRenneutralizing, so the result of "unapplying" such arule during parsing is ambiguous.
Consider thefollowing simple rule:\[-continuant\] -->\[-voiced\]/ __ \[-voiced\]Unapplieation of this devoicing rule to anoncontinuant voiceless segment presents adilemma: should the underlying segmcnt bereconstructed as having been \[+voiced\], or wasthe segment originally \[-voiced\] (with the rule59having applied vacuously)7 This dilemma risesunder most theories with linearly ordered rules,whether segmental or autosegmentai.A second difficulty for parsing is that ifrules apply in linear order, later rules canobscure the effects of earlier rules.
In theexample given, a later rule might alter thecnvironment in which the devoicing rule hadapplied, e.g.
by voicing a scgment which servedas the environment for the first rule.This sccond problem arises in anytheoretical framework which allows opaque ruleorderings, that is, rule orders in which a later rulecan opacify (obscure) the effects of earlier rules.Theories which disallow opaque rule orders (suchas Natural Generative Phonology, see Hooper(1975)) have not enjoyed lasting popularityamong linguists.The implication of these two problems isthat parsing would appear to require abifurcation of the search space for each featurevalue assigned in the output of a phonologicalrule.
For instance, consider the above devoicingrule, followed by a voicing rule which opacitiesthe first rule.
Suppose we have a surfacesequence of a voiceless noncontinuant segmentfollowed by a voiced segment.
In parsing thissequence, it would seem that we must exploreseveral paths.
If the surface voiced segment werealso underlyingly voiced (vacuous application ofthe voicing rule), then there is no fimher choice;the surface voiceless noncontinuant could nothave been devoiced by the devoicing rule.
But ifthe surface voiced segment were underlyinglyvoiceless (nonvacuous application of the voicingrule), then the first rule might have applied, eithervacuously or nonvacuously.
Given thatlanguages may have tens of phonological rules,and that each rule may alter multiple features, thesearch space becomes enormous.Anderson (1988:5) summarizes theproblem as follows:...if thc phonology of the languageinvolvcs a non trivial amount ofneutralization.., it is nceessary tocalculate all of the possiblecombinations of alternatives allowed byvarious rules, which may beindividually large when neutralizationsare involved and whose product growsexponentially as the amount ofsignificant rule interaction (ordering)increases.The combinatorial possibilitiesinvolved in undoing the phonology thusget out of hand rather quickly.
Sincethe depth of ordering in a linguisticallymotivated description can easilyapproach 15-20, with many of the rulesinvolved being many-ways ambiguouswhen regarded from the "wrong end,"the approach of simply undoing theeffects of the rules was soon seen to bequite impractical.But in fact this expansion of search space can beavoided by the use of a generate-and-testalgorithm, in which the ambiguity resulting fromthe unapplication of each rule is encoded into theform when the rule is unapplied.
The resultingalgorithm turns out to be tractable for the sorts ofrules and rule ordering which arise in naturallanguages.3.
THE GENERATE-AND-TESTALGORITHMThis section presents an algorithm forparsing with linearly ordered rules.
Thealgorithm is efficient for the sorts of rule sets thathave been proposed by generative phonoiogistsfor natural languages.The algorithm is presented in generalterms, abstracting away from implementationaldetails where possible.
Where a certain degree ofconcreteness i  unavoidable--as in the definitionsof the application or unapplieation of a singlerule--alternative forms of the algorithm arementioned.3.1 DEFINITIONS AND INITIALASSUMPTIONSAn instantiated (phonetic:).feature is afeature-name plus an atomic feature value; anuninstantiated feature is merely the feature-name.6OA segment-specification consists of acharacter representation f some segment (one ormore characters, e.g.
"k" or "oh"), plus a set offeatures, not all of which need be instantiated.An alphabet consists of a set of segment-specifications.
A given language may employmore than one alphabet, distinguishing such as aninput (surface) alphabet and a lexieal(underlying) alphabet.A (phonetic) word consists of a list ofone or more segments, where  each segmentconsists of a set Of features.
Input words (wordsto be parsed) and lexieal words are usuallyrepresented instead in a character-based notation;the translation between this and a segment-basedrepresentation is defined below.A phonological rule consists of an input(left-hand) side, an output (right-hand) side, a left;environment, and a right environment.
The inputand output side each consist of a set of one ormore instantiated features.
(The extension tolists of sets, representing an input or output ofmore than a single segment, is straightforward.Rules in which the input or the output is empty,i.e.
epenthesis or deletion rules, are discussedlater.)
The environments of a rule consist of asequence of zero or more sets of instantiatedfeatures or optional sequences, together with aBoolean specification of whether the environmentmust begin (leR environment) or end (rightenvironment) at a word boundary.
An optionalsequence consists of a sequence of oneor moresets of features, together with a minimum (MIN)and maximum (MAX) number of times theoptional sequence may appear.Finally, the analysis target of a rule isdefined (for a rule with input and output of lengthone) as a set of features, which set consists of thefeatures of the output, together with any non-contradictory features of the input.
(In mostrules, the features of the input and output aredisjoint, so that the target consists of the union ofthe input and output features.
Occasionally arule will speei~ one value of a feature in theinput, and a contrary value in the output.
In thatcase, the analysis target akes the value of thefeature in the output.
)A rule is said to be ~'elJ-'opaquing if itcould be applied nonvacuously to a segment ofits environments, l Such a rule must receivespecial treatment during analysis, because itsapplication may have altered the word so that theoutput no longer meets the structural descriptionof the rule.The list of rules of a language is linearlyordered, and given in synthesis order.
That is,the input of the first rule is a word from thelexicon, the input, of: the second?
ruleisthe.
~ .
.
output.
?
.
:. '
.. ?
.
':"i ..of the first rule; ete.~ and the output of the last ..rule isa surface form.. '3.2 TRANSLATION BETWEENALPHABETIC AND SEGMENTALREPRESENTATIONSA word in a phonetically basedorthography (not, say, English orthography) maybe translated into a segmental representation bythe following algorithm:61IT he precise formulation of "self-opaquing" forthe purposes of the algorithm is somewhat morerestrictive.
Self-opaquing rules cause difficultyfor parsing because such a rule may apply(nonvacuously) to some segment, while in theoutput he rule seems not to .have applied.to thatsegment because the environment for thatsegment has itself been altered by the rule so thatit no longer meets the structural description: aself-counterbleeding rule.
This can only happenif a segment of the environment meets thestructural description of the rule, and thestructural change of the rule assigns a valuecontrary to the value required in the environment.That is, a segment of the rule's environment isunifiable with the structural description of therule but not with the structural change.If the rule applies left-to-right i eratively, only theright environment is relevant, as only thatenvironment can be altered after it has been used.Likewise, if the rule applies right-to-leRiteratively, only the left enviromnent is relevant.If the rule applies simultaneously, bothenvironments are relevant. "
~ .
.
.
: iiiiBeginning at the left end of the word, replacethe longest substring which corresponds tothecharacter representation of some segment-specification i the appropriate alphabet, withits set of features.Continue left to right, replacing substrings ofthe word with their features until the right endof the word is reached.
If the process fails atany point (because no substring correspondsto a segment-specification), fail.This translation algorithm isdeterministic, and would give wrong results for aword like "mishap" (assuming "sh", "s" and "h"to be defined as segment-specifications).
Thealgorithm could easily be made nondeterministic,with the proviso that each translation of an inputword would be subjected to the remainder of theparsing algorithm.
However, how multipletranslations of lexical words would be treated isnot so clear.The translation between alphabetic andsegmental representations could instead be doneby a finite state transducer, with equivalentresults.3.3 UNAPPLICATION OF PHONOLOGICALRULESDuring the analysis phase of thealgorithm, each rule is unapplied byuninstantiating in each segment which matchesthe rule in the correct environment, those featureswhich the right-hand (output) side of the rulesets.
For instance, if a rule assigns the value\[-voiced\] in its output, during parsing the valueof the feature "voiced" in the segments affectedby the rule becomes uninstantiated.More specifically, given an input(surface) .word in its segmental representationand a list of phonological rules, the rules may beunapplied to the word as follows.
(1) Reverse the list of rules to give a list inanalysis order.
(2) Unapply the first rule of the list to theinput word, using the algorithm below.
(3) Unapply each succeeding rule to theoutput of the previous rule.The algorithm for the unapplication of asingle rule in left-to-fight iterative fashion (seeKenstowicz and Kissebeah 1979) is as follows;note that during analysis, a left-to-right iterativerule is applied right-to-left.For each segment S beginning at the right endof the word:If S is unifiable with the analysis target of therule, and the left-hand environment of the rulematches against the word ending with thesegment to the left of S, and the right-handenvironment of the rule matches against hepart of the word beginning with the segmentto the right of S, then uuinstantiate thefeatures of S whose feature-names arecontained in the output of the rule.An environment sequence matches a subsequenceof segments during analysis if:For each member of the environment which isa set of features, that set unifies with thecorresponding segment of the word; else (ifthe member is an optional sequence), theoptional sequence matches against thecorresponding sequence of segments betweenMIN and MAX number of times If theenvironment must mater at the margin of aword, then when the enviromnent sequence isused up, the last segment matched must be thefirst segment of the word for the leftenvironment, or the last segment for the rightenvironment.After a rule has been unapplied to aword, if the rule is self-opaquing and theunapplication was nonvacuous, the rule isunapplied again until its unapplication isvacuous .The unapplication of a rule whichapplies right-to-left iteratively is the obvioustransformation f the above algorithm.The important point in the unapplicationof a single rule to a form is the use of unification,so that a segment in the word matches a featureset in the rule even if the value of one or morerelevant features in the segment has beenuninstantiated by the unapplication of a previous62rule.
Matching against an uninstantiated featurethus represents an assumption, that theunderlying value of that feature was correct.This assumption can only be validated uring thesynthesis phases when a lexical entry from thelexicon will have become available.The unapplication of a rule whichapplies simultaneously to its input may beperformed by either left-to-right or  right-to-leftiterative unapplication, although the un-application may need to be repeated if the rule isself-opaquing.
To see why the self-opaquing testmight be necessary, consider the followinghypothetical rule:\[--sonorant\] --> \[+continuant\]/ __ \[-continuant\]When applied simultaneously to the form apkpa,the result is afxpa.
If the rule were unapplied toafxpa left-to-right i eratively, after the first passwe would have af\[x klpa, where the sequence Ixk\] is intended to represent a voiceless velarobstruent with an uninstantiated value for thefcature \[continuantl (hence ambiguous bctweenthe fricative x and the stop k).
Only after asecond pass would we get a\[.fPllx k\]pa. (In thisexample the rule could have been unappliedright-to-left iteratively in a single pass, but asingle right-to-left iterative application wouldhave given the wrong result with the mirrorimage of the given rule.
)As an alternative to the above algorithm,the unapplication of a single rule could beperformed by a Finite State Transducer (FST)(Johnson 1972, cf.
also Kaplan and Kay, inpress).
It will be more convenient to compare theFST method with the above algorithm when weconsidcr the application of a rule (as opposed toits unapplication).3.4 LEXICAL LOOKUPA word, some of whose segments may bepartially instantiated, matches against a word inthe lexicon if the features of each of its segmentsare unifiable with the corresponding segment ofthe iexical word.
Lexical lookup consists offinding all such matches.The unapplication of the phonologicalroles and the process of lexical lookup constitutethe analysis phase of the algorithm.3.5 APPLICATION OF PHONOLOGICALRULESAs a result of the unapplication of rulesto forms some of whose features may have beenuninstantiated by earlier rules, someovergeneration may result, because a form takenfrom the lexicon may not have the value whichwas assumed during analysis: Thisovergeneration is filtered out by applying therules in a synthesis phase.
Thedegree ofovergeneration is small, for reasons discussed inMaxwell (1991).
The algorithm for applyingrules during synthesis i  straightforward:Given a lexical word and the list of rules, thefirst rule is applied to the lexicai word, thesecond rule is applied to the output of thefirst, etc.The application of a single rule in lefbto-rightiterative fashion is as follows:For each segment S beginning at the left endof the word:If S contains all the features of the left-handside of the rule, and the left and rightenvironments match parts of the wordimmediately tothe left and right of S, then setthe value of each feature in S whose nameappears in the output of the rule tO the valuein that output.An environment sequence matches duringsynthesis if:For each member of the environment which isa set of features, the corresponding segmentof the word contains those same features; else(if the member is an optional sequence), theoptional sequence matches against thecorresponding segments of the word betweenMIN and MAX number of times.
Thecondition on matching a word boundary is thesame as during unapplication.Right-to-left i erative application is againthe obvious transformation of this algorithm.Simultaneous application may be modeled by .
.
'Z  : ..63first collecting the set of all segments whichsatisfy the structural description of the rule, andthen applying the output of the rule to eachsegment in that set.There is no need to check for possiblereapplieation of a rule during synthesis, as therewas during analysis.
This is because if theapplication of a rule creates new environments towhich it might apply, those environments do notserve as fiarther input for the rule apart fromiteration or cyclic application.
Directionaliterative application is handled directly by theabove algorithm, while nondirectional iterativeapplication has generally been rejected byphonologists (cf.
Johnson 1972: 35ff., and for aslightly different form of nondirectional iterativeapplication, Kcnstowicz and Kisseberth, 1979:325).
Cyclic application is not treated under theabove algorithm, but would constitute only arestricted form of reapplication in which theapplication of a set of phonological rules wouldbe sandwiched between each pair of cyclicmorphological rules (as argued originally byPesetsky 1979).
If two or more cyclicmorphological rules applied in a given word, thecyclic phonological rules would also apply atleast twice.
But each such application would beseparated by the application of other rules, bothphonological nd morphological.!
will refer to this algorithm for applyinga single rule as the Target-First ApplicationAlgorithm, or TFAA; it is analogous to thealgorithm given earlier for unapplication of arule.As an alternative to the TFAA, each rulecould instead be applied by an FST.A disadvantage of application of a ruleby the TFAA, compared with its application byFST, is that when checking the left-handenvironment (assuming the rule applies leg-to-right iteratively), the TFAA must retest segmentsit has already considered as possible targetsegments.
In other words, the TFAA backs upthrough the form when checking the lett-handenvironment.
Under those same circumstances,the FST nccd do no backing tip when checkingthe left environment, as the applicability of theleft environment is already determined when theFST arrives at a potential target.
The distancethe TFAA backs up can be considerable, inparticular when the left environment (or the rightenvironment, for a right-to-let~ iterative rule) hasoptional sequences ( o that backtracking must beemployed in case of failure to match theenvironment on the initial check), or when theword being parsed has "optional" segments.
(Optional segments arise in analysis during theunapplication of deletion rules, as discussedlater.
)Both the FST and the TFAA may test thesame segments multiple times when the right-hand environment is nonempty (assuming Ictt-to-right iterative application).
For the FST, this willonly happen if it made an incorrect choice.
Anexample would be the rule:\[-continuant\] -->\[-voiced\]/ __ \[-voicedlwhen applied to the form ba.
After the FST teststhe target, it could attempt to apply the rule byassigning the feature \[-voiced\] to the b (changingit to p).
This would be incorrect, however, as theFST discovers when it processes the \[+voicedlsegment a; it must therefore back up, restore the\[+voicedl value to the b, and move right toprocess the a again.The TFAA, applying the same rule to thesame form, would first notice the potential targetb.
Before altering the value of the feature\[voiced\], however, it would check the rightenvironment: the segment a.
Noticing that it doesnot satisfy the requirement hat the rightenvironment be \[-voicedl, it refrains fromaltering the feature \[voicedl on the b. it thengoes on to check whether the a constitutes apotential target.However, the real question is not theworst case behavior, but the average casebehavior; how many comparisons must be donefor the average word with the average rule?Unfortunately, this is not a straightforwardquestion.
Examples are readily constructed inwhich the FST would do more COluparisous thn.nthe TFAA.
Given that m solnc cases tile TFAA66must back up through segments it has alreadyconsidered while the FST need not, while in other'cases the FST does more comparisons than theTFAA, I leave the question of average easebehavior open.
Note that similar considerationspertain to the behavior of the algorithm givenearlier for the unapplieation f rules.A potential advantage of the TFAA overan FST implementation concerns the debuggingof a single rule.
When scanning a word forpossible rule applications, people often searchfirst for segments matching the input side of therule, then cheek whether the left and fightenvironments of potential targets also match.This is essentially the method employed in theTFAA.
If a rule is at all complicated, trying to.apply it as an FST instead becomes quite difficultfor humans.
By the same token, determining whya parser did or did not apply a rule to a certainsegment of a form should be much easier if theparser presents: a trace of its application in thesame form that the human would do it.
This is ofcourse only an advantage ofthe TFAA if the useris actually tracing a given rule.
Indeed the parserneed not use the same algorithm to apply a rulewhen debugging is turned on as it uses whendebugging is not turned on (although it iscertainly easier on the writer of the parser if itdoes).3.6 COMPARISON WITH INPUT FORMReturning to the overall algorithm,specifically the test phase: the derivation of aword to which all the rules have been applied iscorrect if the derived word matches the originalinput word, that is, if each .segment of the twowords correspond.
A segment corresponds ifeach of its features i identical.During the test phase of the algorithm, aderived word may fail to match against theoriginal (input)' word under two circumstances:either one or more pairs of rules are opaquelyordered (see Maxwell 1991), or one or more rulesare dcpendent on nonphonetic nformation, suchas the location of a morpheme boundary ornonphonetic features.
The resulting (potential)overgeneration is the reason for the test phase ofthe generate-and-test algorithm.This completes the discussion of thegenerate-and-test algo~rithm for feature-changingrules.
The next two sections discuss somerefinements.3.7 EPENTHESIS AND DELETION RULESDuring analysis, a segment which hasbeen inserted by an epenthesis rule 2 must be un-epenthesized, while segments which may havebeen deleted must be re-inserted.
To avoidbifiarcation of the search for each such segment,segments may be assigned an additional featurecalled "optional."
All segments in the input wordare marked \[-optional I.
When an epenthesis ruleis unapplied (using an algorithm similar to thatgiven above for feature-changing rules), thesegments which might be epenthetic are markedas \[+optional\].
Similarly, a deletion rule may beunapplied by inserting a new segment with the setof features pecified on the input side of the rule,and marking that segment as \[+optional\].The unapplication of deletion rules mustbe ~aher constrained toprevent infinite looping.To take a concrete example, consider thefollowing consonant cluster simplification rule:C --> 0 /C  CIf this rule is un-applied to a surfaceform with a two consonant cluster, the result willbe an intermediate form having a three consonantcluster.
But the rule is Self-opaquing, in thesense that it can dclete consonants which formpart Of the environment.
Hence during analysis,it-should be allowed to re-unapply to its ownoutput.
But ifthe rule is allowed to un-apply tothe intermediate form produced by its firstunapplieation, amely a three consonant cluster,it can un-apply in two places to yield a five-consonant cluster; to which the rule can again beunapplied, ad infinitum.2 Pretheoretically, an .epenthesis rule is aphonological rule which inserts a segment into aword.
An example might be the insertion of pinto warm-~th ogive \[warmO\].65The best solution to this problem wouldbe to use reasoning to determine the maximumnumber of contiguous consonants which couldappear in the input to the rule.
But this is by nomeans simple.
It would be straightforward todetermine the maximum number of consonantswhich could appear in underlying forms (basedon the maximum number of consonants whichappear in lexical entries and in affixes, assuminga morphological component), and in fact thelexicon itself is often used for this purpose inKIMMO-based systems.
However, with linearlyordered rules the number of adjacent consonantscould in principle be increased by the applicationof certain rules preceding the deletion rule,including rules epenthesizing consonants, rulesdeleting vowels, and rules changing vowels intoconsonants.
Whether such rules in fact exist, orwhether they exist but would be blocked by otherprinciples from creating inputs to such aconsonant cluster simplification rule is an area ofresearch in phonology.In the absence of a principled way ofdetermining the maximum number of consonantsthat could appear in a cluster (or analogous limitson other deletion rules), an ad hoe limit may beplaced on the application of deletion rules.
Onesuch limit is to unapply a deletion rulesimultaneously, and only once (or only N times).To take a concrete xample, consider theinput abbabba, where a is a vowel and b is aconsonant.
A single simultaneous unapplicationof the above consonant cluster simplification rulewould give abCbabCba, while two un-applications would give abCCCbabCCCa, wherethe first and third Cs in each cluster esult fromthe second unapplication.
Limiting the un-application of deletion rules in this way is ad hoe,but probably sufficient for practical purposes.The presence of l+optional\] segmentsarising from the unapplication of epcnthesis anddeletion rules slightly complicates the algorithmgiven earlier for rule unappl!cation, in that suchsegments may optionally be passed over whenchecking rule environments.During synthesis, epenthesis rules arestraightforwardly applied by inserting a segmentwith the features of the output of the rule, whiledeletion rules are applied by simply deleting therelevant segments.3.8 NONPHONETIC FEATURES,BOUNDARY MARKERS, ALPHAFEATURES ETC.Nonphonetic (diacritic) features andobligatory boundary markers in rules may simplybe ignored during analysis, leading to someovergeneration, I  (manually) checking a numberof such rules against large dictionaries,overgeneration appears to be surprisingly small,in fact virtually nil.Alpha variable features (commonly usedin assimilation rules) may be modeled by the useof variables which become instantiated to thevalue of features in the appropriate segments, othat checking for a match during analysis is amatter of unification.
During synthesis, avariable in the output of a rule results in thefeatures of the corresponding segment of theword being set to the value to which the variablebecomes instantiated in some other part of therule.4.
AN IMPLEMENTATION OF THEALGORITHMThe generate-and-test algorithm has beenimplemented, as a parser which usesphonological rules of classical generativephonology, resembling those of Chomsky andHalle (1968) and much related work.
(A samplerule is shown in the appendix.)
!
call the parser"Hermit Crab."
There is provision for feature-changing rules (including alpha variable rules),epenthesis rules, and deletion rules.
Disjunctiverule ordering may be modeled, as well assimultaneous or directional iterative application.The environments of rules may incorporateoptional sequences ( uch as (CV)~).PC-KIMMO, ml implementation f two-level phonology (Antworth 1990) was used toprovide a comparison between parsing withlinearly ordered generative phonological rules,and with two-level rules.
Both PC-KIMMO andHermit Crab run under MS-DOS.PC-KIMMO comes with exampleanalyses of the phonologies of several languages,including Hebrew, Turkish, Japanese, andFinnish, each analysis containing from 16 to 27two-level rules.
The PC-KIMMO analyses wereconverted into analyses using linearly orderedgenerative rules, which were equivalent in thesense that they derived the surface forms fromthe same underlying forms.
In most cases thelinearly ordered roles were simpler than the two-level rules, in part because rule ordering renderedredundant some of the constraints necessitated bythe two-level formalism.
The number of rules foreach language was reduced to between 7 and 11,as some two-level rules (such as default rules)are unneeded in a generative analysis, whileothers collapse into disjunctively ordered rulesets.
For instance, PC-KIMMO has six rules forvowel harmony in Turkish: two for backnessharmony in low vowels (one to make a low vowelI+back\] in the appropriate environment, and oneto make it i-baekl in the opposite nvironment),and four rules for backncss and roundingharmony in nonlow vowels.
These collapse intotwo generative rules: one for baekness harmony,which affects all vowels, and uses an alphavariable for the two possible values of the featureback; and one rule for rounding harmony, whichaffects nonlow vowels, again using an alphavariable for the two possible values of the featureround.Because the focus here is onphonological parsing, rather than morphologicalparsing, the morphological rules given in PC-KIMMO's sample analyses were ignored, andfully affixed forms were used for underlyingforms, e.g.
:<lex_entry shape "oda+sH"... gloss "room+POSS">In a sample of several hundred words,PC-KIMMO was about three times faster thanthe parser using linearly ordered rules.
Thisdifference is not large, and indeed may beattributed in part to the different programminglanguages used (PC-KIMMO is written in C,while the parser implementing the generate-and-test algorithm is written in Prolog and C).
Theratio of 3:1 is approximately constant among thefour grammars, and independent of word length,indicating that the results hould scale.5.
CONCLUSIONThe algorithm as described andimplemented models segmental phonology.
Anextension to multiple strata of rules, as in lexicalphonology, is trivial, and has also beenimplemented.
Allowing cyclic application ofrules is also simple, although it has not beenimplemented yet (because most phonologistssince Pesetsky 1979 have interpreted cycli cphonology as the interleaving of phonologicalrules and morphological rules, and morphologicalrules have not yet been implemented).The algorithm could be extended toautosegmentai models of phonology byreinterpreting e.g.
feature spreading rules asfeature assignment rules with alpha variablesduring the analysis phase, and reverting to thestandard interpretation of autosegmental rulesduring synthesis.
For instance, an autosegrnentalrule spreading the place of articulation featuresof an obstruent onto a preceding nasal consonantcan be modeled uring analysis by the followingrule:.
_, r-continuent-F+cons \] \[~high 1, \]c~ highpoacK / k+nas J/rcoron J--/pba k- L?,corona 1The modeling of autosegmentalphonology has not been implemented, althoughthe use of alpha variables has.In summary, and contrary to manyearlier claims, it need not be eomputationallyexpensive to parse surface forms into theirunderlying forms using linearly ordered rules.Furthermore, unlike rule compilers (Kaplan andKaye in press), the use of a rule interpretersimplifies grammar debugging, as the input andoutput of each rule can be studied (a sample traceis shown in the appendix).676.
REFERENCESAntworth, Evan L. (1990).
PC-KIMMO: A Two-level Processor jbr Morphological Analyxis.Occasional Publications in Computing,number 16.
Summer Institute of Linguistics,Dallas, Tex.Bromberger, Sylvain, and Halle, Morris.
(1989).
"Why Phonology Is Different."
Linguisticlnquiry 20:51-70.Chomsky, Noarn, and Halle, Morris.
(1968).The Sound Pattern of English.
MIT Press,Cambridge, Mass.Hooper, Joan.
(1976).
An Introduction toNatural Generaave Phonology.
AcademicPress, N.Y.Johnson, C. Douglas.
(1972).
Formal Aspects ofPhonological Description.
Mouton, TheHague.Kaplan, Ronald M. and Kaye, Martin.
(in press).
"Regular Models of Phonological RuleSystems."
To appear in ComputationalLinguistics.Kenstowicz, Michael, and Kisscbcrth, Charles.(1979).
Generative Phonology: Descriptionand lheory.
Academic Press.
N.Y.Maxwell, Michael.
(199 I).
"PhonologicalAnalysis and Opaque Rule Orders."
Pp.
110-ll6 in Proceedings of the SecondInternational Workshop on ParsingTechnologies.
Special Interest Group onParsing of the Association for ComputationalLinguistics, Pittsburgh, Pa.Pesetsky, David.
1979.
"Russian Morphologyand Lexical Theory."
Unpublishedmanuscript, MIT.
Cited by Spencer 1991:109.Spencer, Andrew.
199 I. Morphological Theory.Basil BlackweU, Canlbridge, Mass.7.
APPENDIX: A SAMPLE PARSING RUNThis appendix presents excerpts of the input to and output from the phonological parser.
Someinformation in the original has been omitted here for simplicity.
The language being parsed is Japanese.The data and rules were adapted from one of the sample data sets provided with PC-KIMMO (seeAntworth 1990).
The rule notation is an internal format, not necessarily intended for the end user.Structures are enclosed in "< >".
Added comments are shown in italics.We first load the rules.
The rule shown here (\[+voc -round\] --> 0 / \[+voc\] +__) deletes a non-roundvowel immediately after another vowel, but note the obligatory morpheme boundary ("+ "):(load morpher_rule<prule rname vowel_deletionp_lhs <p_lhs pseq ((+ voc -round))>p_rhs <p_rhs pseq 0 >left environ <ptemp pseq ((+ voc) "+")>>)... remaining rules are not shown here.Turn tracing on ('7').fiw lexical lookup:(trace lexicaLIookup T)Also turn tracing on for the vowel deletion rule, both during analysis (the first '1"9 and synthesis (thesecond 'T g :(trace_morpher rule ('1" T vowel deletion))Finally.
parse the Japanese word "neta ":(morph_and_lookup_word "neta")68the parser ~' trace outputJbllows, l ~rst the trace repeats the input.
"<trace shape "neta"continuations (The "continuation" list is a list of  paths Jollowed by the parser from the current position.
7he parserproceeds by unapplying the rules, the trace shows the unapplication of  the traced rule,"vowel deletion".
The form which is input to this rule is the sequence of segments resulting from theunapplication of  shallower ules, and corresponds to the regular expression "n(\[r y\])et(\[r yJ)a", where"Jr y\]"  signifies a segment ambiguous between "r" and "y" which the parser has undeleted; theparentheses indicate that it is optional (since perhaps the parser shouldn't have undeleted iO.Unapplication of  the traced deletion rule results in the insertion of  more optional segments in two places,whose features Correspond to the Set of  vowels/i e a/:<rule unapp mame voweldeletioninput <lex_entry shape "n(\[r y\])et(\[r y\])a"... >output <lex_entry shape =n(\[r y\])e(\[i e a\])t(\[r y\])a(\[i e a\])"... >cont inuat ions (('ontimting.
the parser unapplies various other rules (not shown here, since they arch 't being traced):qfh'r all the rules have been unapplied, it does lexical lookup.
"<lex lookup virtual- -  m > <lex entry shape =n(\[r yl)e(\[i e a\])t(\[r Yl)a(\[i e al) ...7he above "virtual" (i. e. created uring analysi.~9 lexical entry corresponds to a "real" one fi)und in thelexicon.continuations (<succ lookup real <lex entry shape =ne+ta" gloss =(sleep)+PAST"... >continuations (7he parser continues 39om this lexical lookup by applying the rules to the form in the ~Tnthesis phase;again, only one application is shown, the traced one.
The rule does not apply, since its structuraldescription is not met; hence the output form is the same as the input form:<rule_app rname vowel_deletioninput <lex_entry shape =ne+ta" gloss =(sleep)+PAST" ...>output <lex_entry shape "ne+ta" gloss "(sleep)+PAST"...>continuations (A f ief applying the remaining rules, a surJbce form results whose phonetic shape is identical to the inputfi)rm:<lex_entry shape =neta" gloss "(sleep)+PAST"...>)>)>Now we return to continuations from lexical lookup.
There is a second "real" lexical entry in the user'slexicon which corresponds to the "virtual" lexical entry the parser created:<succ_lookup real<lex entry shape =ne+itai" gloss =(sleep)+VOL"...>continuations (Again, the continuation from the lexical item consists of the application of the rules," only the tracedrule ~" application is shown.
7he deletion rule applies to the first pair of  adjacent vowels, turning/ei/into/eL but does not apply to the second pair of  vowels/aft, although during analysis it was unapplied inboth places.
7he reason is that the rule requires a morpheme boundary to appear between the vowels,and there is only a morpheme boundary between the first pair of  vowels.
(During analysis, the position ofmorpheme boundaries is unknown, hence they are impossible to check for.)
This analysis will be filtered69out; it is an example of overgeneration due to a rule whose application is governed by morphemeboundaries:<rule_app marne voweldeletioninput <lax entry shape "ne+itai" gloss "(sleep)+VOk"...>output <lex entry shape "ne+tai" gloss "(sleep)+VOL"...>The continuation list from this rule application is empty, because the output does not match the inputword:continuations ( )>)>)>)~)>This completes the trace.
Last comes the parser ~ output, the single analysis of the word, depicted as alexical entry at the surface level This wouM have been output even if  tracing had not been turned on:(wordanalysis<lex_entry shape "hera" gloss "(sleep)+PAST"... > )'fr~
