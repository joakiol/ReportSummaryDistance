Proceedings of EACL '99Representing Text ChunksEr ik  F.  T jong  K im SangCenter for Dutch Language and SpeechUniversity of AntwerpUniversiteitsplein 1B-2610 Wilrijk, Belgiumerikt@uia.ac.beJorn Veenst raComputat iona l  LinguisticsTi lburg UniversityP.O.
Box 901535000 LE Tilburg, The Nether landsveenstra@kub.nlAbst rac tDividing sentences in chunks of words isa useful preprocessing step for parsing,information extraction and informationretrieval.
(l~mshaw and Marcus, 1995)have introduced a "convenient" data rep-resentation for chunking by convertingit to a tagging task.
In this paper wewill examine seven different data repre-sentations for the problem of recogniz-ing noun phrase chunks.
We will showthat the the data representation choicehas a minor influence on chunking per-formance.
However, equipped with themost suitable data representation, ourmemory-based l arning chunker was ableto improve the best published chunkingresults for a standard ata set.1 In t roduct ionThe text corpus tasks parsing, information extrac-tion and information retrieval can benefit from di-viding sentences in chunks of words.
(Ramshawand Marcus, 1995) describe an error-driventransformation-based learning (TBL) method forfinding NP chunks in texts.
NP chunks (orbaseNPs) are non-overlapping, non-recursive nounphrases.
In their experiments hey have modeledchunk recognition as a tagging task: words thatare inside a baseNP were marked I, words outsidea baseNP received an 0 tag and a special tag B wasused for the first word inside a baseNP immedi-ately following another baseNP.
A text example:original:In \[N early trading N\] in \[N Hong KongN\] \[N Monday N\], \[N gold N\] was quotedat \[N $ 366.50 N\] \[N an ounce g\] ?tagged:In/O early/I trading/I in/O Hong/IKong/I Monday/B ,/O gold/I was/Oquoted/O at/O $/I 366.50/I an/Bounce/I ./OOther representations for NP chunking can beused as well.
An example is the representationused in (Ratnaparkhi, 1998) where all the chunk-initial words receive the same start tag (analo-gous to the B tag) while the remainder of thewords in the chunk are paired with a different tag.This removes tagging ambiguities.
In the Ratna-parkhi representation equal noun phrases receivethe same tag sequence regardless ofthe context inwhich they appear.The data representation choice might influencethe performance ofchunking systems.
In this pa-per we discuss how large this influence is.
There-fore we will compare seven different data rep-resentation formats for the baseNP recognitiontask.
We are particularly interested in finding outwhether with one of the representation formatsthe best reported results for this task can be im-proved.
The second section of this paper presentsthe general setup of the experiments.
The resultsCan be found in the third section.
In the fourthsection we will describe some related work.2 Methods  and  exper imentsIn this section we present and explain the datarepresentation formats and the machine learningalgorithm that we have used.
In the final partwe describe the feature representation used in ourexperiments.2.1 Data representationWe have compared four complete and three partialdata representation formats for the baseNP recog-nition task presented in (Ramshaw and Marcus,1995).
The four complete formats all use an I tagfor words that are inside a baseNP and an 0 tagfor words that are outside a baseNP.
They differ173Proceedings of EACL '99IOB1 O I I O I I B O I O O O I I B I OIOB2 O B I O B I B O B O O O B I B I OIOE1 O I I O I E I O I O O O I E I I OIOE2 O I E O I E E O E O O O I E I E OIO I O I I O I I I O I O O O I I I I O\[ \[ \[ \[ \[ \[ \[\] \] \] \] \] \] \]Table 1: The chunk tag sequences for the example sentence In early trading in Hong Kong Monday ,gold was quoted at $ 366.50 an ounce .
for seven different agging formats.
The I tag has been usedfor words inside a baseNP, \[:1 for words outside a baseNP, B and \[ for baseNP-initial words and E and \]for baseNP-final words.in their treatment of chunk-initial and chunk-final \[ + \]words:IOB1IOB2IOE1IOE2The first word inside a baseNPimmediately following an-other baseNP receives a Btag (Ramshaw and Marcus,1995).All baseNP-initial words receive aB tag (Ratnaparkhi, 1998).The final word inside a baseNPimmediately preceding anotherbaseNP receives an E tag.All baseNP-final words receive anE tag.We wanted to compare these data representa-tion tbrmats with a standard bracket representa-tion.
We have chosen to divide bracketing exper-iments in two parts: one for recognizing openingbrackets and one for recognizing closing brackets.Additionally we have worked with another partialrepresentation which seemed promising: a tag-ging representation which disregards boundariesbetween adjacent chunks.
These boundaries canbe recovered by combining this format with oneof the bracketing formats.
Our three partial rep-rcsentations are:\[ All baseNP-initial words receive an\[ tag, other words receive a .
tag.\] All t)aseNP-final words receive a \]tag, other words receive a .
tag.IO Words inside a baseNP receive an Itag, others receive an O tag.These partial representations can be combinedill three pairs which encode the complete baseNPstructure, of tile data:\[+IOI0+\]A word sequence is regarded as abaseNP if the first word has re-ceived an \[ tag, the final word hasreceived a \] tag and these are theonly brackets that have been as-signed to words in the sequence.In the IO format, tags of wordsthat have received an I tag and an\[ tag are changed into B tags.
Theresult is interpreted as the IOB2format.In the IO format, tags of wordsthat have received an I tag and a\] tag axe changed into E tags.
Theresult is interpreted as the IOE2format.Examples of the four complete formats and thethree partial formats can be found in table 1.2.2 Memory-Based  Learn ingWe have build a baseNP recognizer by traininga machine learning algorithm with correct taggeddata and testing it with unseen data.
The ma-chine learning algorithm we used was a Memory-Based Learning algorithm (MBL).
During train-ing it stores a symbolic feature representation ofa word in the training data together with its classi-fication (chunk tag).
In the testing phase the algo-rithm compares a feature representation of a testword with every training data item and choosesthe classification of the training item which is clos-est to the test item.In the version of the algorithm that we haveused, IBI-IG, the distances between feature rep-resentations are computed as the weighted sumof distances between individual features (Daele-roans et al, 1998).
Equal features are defined tohave distance 0, while the distance between otherpairs is some feature-dependent value.
This valueis equal to the information gain of the feature, aninformation theoretic measure which contains the174Proceedings of EACL '99word/POS contextIOB1 L=2/R=IIOB2 L=2/R=IIOE1 L=I /R=2IOE2 L=2/R=2\[ + \] L=2/R=I + L=O/R=2\[ + IO L=2/R=O + L=I/R=IIO + \] L=I/R=I + L=O/R=2F~3=l89.1788.7688.6789.0189.3289.4389.42Table 2: Results first experiment series: the best F~=I scores for different left (L) and right (R)word/POS tag pair context sizes for the seven representation formats using 5-fold cross-validation onsection 15 of the WSJ corpus.normalized entropy decrease of the classificationset caused by the presence of the feature.
Detailsof the algorithm can be found in (Daelemans etal., 1998) I.2.3 Represent ing  words with featuresAn important decision in an MBL experiment isthe choice of the features that will be used forrepresenting the data.
IBI-IG is thought to beless sensitive to redundant features because of thedata-dependent feature weighting that is includedin the algorithm.
We have found that the presenceof redundant features has a negative influence onthe performance of the baseNP recognizer.In (Ramshaw and Marcus, 1995) a set of trans-formational rules is used for modifying the clas-sification of words.
The rules use context infor-mation of the words, the part-of-speech tags thathave been assigned to them and the chunk tagsthat are associated with them.
We will use thesame information as in our feature representationfor words.In TBL, rules with different context informationare used successively for solving different prob-lems.
We will use the same context informationfor all data.
The optimal context size will bedetermined by comparing the results of differentcontext sizes on the training data.
Here we willperform four steps.
We will start with testing dif-fhrent context sizes of words with their part-of-speech tag.
After this, we will use the classifica-tion results of the best context size for determiningthe optimal context size for the classification tags.As a third step, we will evaluate combinations ofclassification results and find the best combina-tion.
Finally we will examine the influence of anMBL algorithm parameter: the number of exam-ined nearest neighbors.~lr~l-l(; is a part of the TiMBL software packagewhich is available from http://ilk.kub.nl3 Resu l tsWe have used the baseNP data presented in(Ramshaw and Marcus, 1995) 2.
This data wasdivided in two parts.
The first part was trainingdata and consisted of 211727 words taken fromsections 15, 16, 17 and 18 from the Wall StreetJournal corpus (WSJ).
The second part was testdata and consisted of 47377 words taken fromsection 20 of the same corpus.
The words werepart-of-speech (POS) tagged with the Brill taggerand each word was classified as being inside oroutside a baseNP with the IOB1 representationscheme.
The chunking classification was made by(Ramshaw and Marcus, 1995) based on the pars-ing information in the WSJ corpus.The performance of the baseNP recognizer canbe measured in different ways: by computingthe percentage of correct classification tags (ac-curacy), the percentage of recognized baseNPsthat are correct (precision) and the percentage ofbaseNPs inthe corpus that are found (recall).
Wewill follow (Argamon et al, 1998) and use a com-bination of the precision and recall rates: F~=I =(2" precision*recall) / (precision+recall).In our first experiment series we have tried todiscover the best word/part-of-speech tag contextfor each representation format.
For computationalreasons we have limited ourselves to working withsection 15 of the WSJ corpus.
This section con-tains 50442 words.
We have run 5-fold cross-validation experiments with all combinations ofleft and right contexts of word/POS tag pairs inthe size range 0 to 4.
A summary of the resultscan be found in table 2.The baseNP recognizer performed best with rel-atively small word/POS tag pair contexts.
Differ-ent representation formats required different con-text sizes for optimal performance.
All formats2The data described in (Ramshawand Marcus, 1995) is available fromftp://ftp.cis.upenn.edu/pub/chunker/175Proceedings of EACL '99word/POS context chunk tag contextIOB1 L=2/R=IIOB2 L- -2/R=IIOE1 L=I/R=2IOE2 L=I /R=2\[ +\ ]  L=2/R=I  + L=0/R=2\[ + IO L=2/R=0 + L=I/R=IIO +\ ]  L=I /R=I+L=0/R=2F~=I1/2 90.121/0 89.301/2 89.550/1 89.730/0 + 0/0 89.320/0 + I / I  89.781/1 + 0/0 89.86Table 3: Results second experiment series: the best F~=I scores for different left (L) and right (R)chunk tag context sizes for the seven representation formats using 5-fold cross-validation on section 15of the WSJ corpus.word/POS chunk tag combinationsIOB1 2/1IOB2 2/1IOE1 1/2IOE2 1/2\ [+\]  2/1+0/2\ [+  IO 2/0 + 1/1IO+\] I/1+0/2I / i1/o1/2o/io/o + o/o0/0 -F I / I1/1 -F 0/0F~=I0/0 1/1 2/2 3/3 90.532/1 89.300/0 1/1 2/2 3/3 90.031/2 89.73+ 89.32- + 0/1 1/2 2/3 3/4 89.910/1 1/2 2/3 3/4 +-  90.03Table 4: Results third experiment series: the best F~=I scores for different combinations of chunk tagcontext sizes for the seven representation formats using 5-fold cross-validation  section 15 of the WSJcorpus.with explicit open bracket information preferredlarger left context and most formats with explicitclosing bracket information preferred larger rightcontext size.
The three combinations of partialrepresentations systematically outperformed thefour complete representations.
This is probablycaused by the fact that they are able to use twodifferent context sizes for solving two differentparts of the recognition problem.In a second series of experiments we used a "cas-caded" classifier.
This classifier has two stages(cascades).
The first cascade is similar to the clas-sifter described in the first experiment.
For thesecond cascade we added the classifications of thefirst cascade as extra features.
The extra featuresconsisted of the left and the right context of theclassification tags.
The focus chunk tag (the clas-sification of the current word) accounts for the cor-rect classification in about 95% of the cases.
TheMBL algorithm assigns a large weight to this in-put feature and this makes it harder for the otherfeatures to contribute to a good result.
To avoidthis we have refrained from using this tag.
Ourgoal was to find out the optimal number of ex-tra classification tags in the input.
We performed5-fold cross-validation experiments with all com-binations of left, and right classification tag con-texts in the range 0 tags to 3 tags.
A summary ofthe results can be found in table 33 .
We achievedhigher F~=I for all representations except for thebracket pair representation.The third experiment series was similar to thesecond but instead of adding output of one ex-periment we added classification results of three,four or five experiments of the first series.
By do-ing this we supplied the learning algorithm withinformation about different context sizes.
This in-formation is available to TBL in the rules whichuse different contexts.
We have limited ourselvesto examining all successive combinations of three,four and five experiments of the lists (L=O/R=O,1/1, 2/2, 3/3, 4/4), (0/1, 1/2, 2/3, 3/4) and (1/0,2/1, 3/2, 4/3).
A summary of the results can befound in table 4.
The results for four representa-tion formats improved.In the fourth experiment series we have exper-imented with a different value for the number ofnearest neighbors examined by the IBI-IG algo-rithm (parameter k).
This algorithm standardlyuses the single training item closest to the test3In a number of cases a different base configurationin one experiment series outperformed the best baseconfiguration found in the previous eries.
In the sec-ond series L/R=I/2 outperformed 2/2 for IOE2 whenchunk tags were added and in the third series chunktag context 1/1 outperformed 1/2 for IOB1 when dif-ferent combinations were tested.176Proceedings of EACL '99word/POS chunk tag combinations FB=IIOB1 3/3(k=3)IOB2 3/3(k=3)IOE1 2/3(k=3)IOE2 2/3(k=3)\ [+\ ]  4/3(3) + 4/4(3)\[ + IO 4/3(3) + 3/3(3)IO +\]  3/3(3) + 2/3(3)1/11/o1/2o/1o/o + o/o0/0 + 1/11/1 + OlO0/0(1) 1/1(1) 2/2(3) 3/3(3)3/3(3)0/0(1) 1/1(1) 2/2(3) 3/3(3)2/3(3)- + 0/1(1) 1/2(3) 2/3(3) 3/4(3)0/1(1) 1/2(3) 2/3(3) 3/4(3) +-90.89 + 0.6389.72 4- 0.7990.12 + 0.2790.02 4- 0.4890.08 4- 0.5790.35 4- 0.7590.23 4- 0.73Table 5: Results fourth experiment series: the best FZ=I scores for different combinations of left andright classification tag context sizes for the seven representation formats using 5-fold cross-validationon section 15 of the WSJ corpus obtained with IBI-Ic parameter k=3.
IOB1 is the best representationformat but the differences with the results of the other formats are not significant.item.
However (Daelemans et al, 1999) reportthat for baseNP recognition better results can beobtained by making the algorithm consider theclassification values of the three closest trainingitems.
We have tested this by repeating the firstexperiment series and part of the third experimentseries for k=3.
In this revised version we haverepeated the best experiment of the third serieswith the results for k=l  replaced by the k=3 re-sults whenever the latter outperformed the firstin the revised first experiment series.
The resultscan be found in table 5.
All formats benefitedfrom this step.
In this final experiment series thebest results were obtained with IOB1 but the dif-ferences with the results of the other formats arenot significant.We have used the optimal experiment configura-tions that we had obtained from the fourth experi-ment series for processing the complete (Ramshawand Marcus, 1995) data set.
The results can befound in table 6.
They are better than the resultsfor section 15 because more training data was usedin these experiments.
Again the best result wasobtained with IOB1 (F~=I =92.37) which is an im-I)rovement of the best reported F,~=1 rate for thisdata set ((Ramshaw and Marcus, 1995): 92.03).We would like to apply our learning approachto the large data set mentioned in (Ramshaw andMarcus, 1995): Wall Street Journal corpus sec-tions 2-21 as training material and section 0 astest material.
With our present hardware apply-ing our optimal experiment configuration to thisdata would require several months of computertime.
Therefore we have only used the best stage1 approach with IOB1 tags: a left and right con-t(,.xt of three words and three POS tags combinedwith k=3.
This time the chunker achieved a F~=lscore of 93.81 which is half a point better than theresults obtained by (Ramshaw and Marcus, 1995):93.3 (other chunker ates for this data: accuracy:98.04%; precision: 93.71%; recalh 93.90%).4 Re la ted  workThe concept of chunking was introduced by Ab-ney in (Abney, 1991).
He suggested to developa chunking parser which uses a two-part syntac-tic analysis: creating word chunks (partial trees)and attaching the chunks to create complete syn-tactic trees.
Abney obtained support for such achunking stage from psycholinguistic literature.Ramshaw and Marcus used transformation-based learning (TBL) for developing two chunkers(Ramshaw and Marcus, 1995).
One was trainedto recognize baseNPs and the other was trainedto recognize both NP chunks and VP chunks.Ramshaw and Marcus approached the chunkingtask as a tagging problem.
Their baseNP trainingand test data from the Wall Street Journal corpusare still being used as benchmark data for currentchunking experiments.
(Ramshaw and Marcus,1995) shows that baseNP recognition (Fz=I =92.0)is easier than finding both NP and VP chunks(Fz=1=88.1) and that increasing the size of thetraining data increases the performance on thetest set.The work by Ramshaw and Marcus has inspiredthree other groups to build chunking algorithms.
(Argamon et al, 1998) introduce Memory-BasedSequence Learning and use it for different chunk-ing experiments.
Their algorithm stores equencesof POS tags with chunk brackets and uses this in-formation for recognizing chunks in unseen data.It performed slightly worse on baseNP recognitionthan the (Ramshaw and Marcus, 1995) experi-ments (Fz=1=91.6).
(Cardie and Pierce, 1998)uses a related method but they only store POStag sequences forming complete baseNPs.
Thesesequences were applied to unseen tagged data aI-ter which post-processing repair rules were usedfor fixing some frequent errors.
This approachperforms worse than othe.r reported approaches(Fo=I =90.9).177Proceedings of EACL '99IOB1IOB2IOE1IOE2\[+\]\ [+  IOIO +\](Ramshaw and Marcus, 1995)(Veenstra, 1998)(Argamon et al, 1998)(Cardie and Pierce, 1998)accuracy97.58%96.50%97.58%96.77%97.37%97.2%precision92.50%91.24%92.41%91.93%93.66%91.47%91.25%91.80%89.0%91.6 %90.7%recall F~=I92.25% 92.3792.32% 91.7892.04% 92.2392.46% 92.2090.81% 92.2292.61% 92.0492.54% 91.8992.27% 92.0394.3% 91.691.6% 91.691.1% 90.9Table 6: The F~=I scores for the (Ramshaw and Marcus, 1995) test set after training with theirtraining data set.
The data was processed with the optimal input feature combinations found in thefourth experiment series.
The accuracy rate contains the fraction of chunk tags that was correct.
Theother three rates regard baseNP recognition.
The bottom part of the table shows some other reportedresults with this data set.
With all but two formats IBI-IG achieves better FZ=l rates than the bestpublished result in (Ramshaw and Marcus, 1995).
(Veenstra, 1998) uses cascaded decision treelearning (IGTree) for baseNP recognition.
This al-gorithm stores context information of words, POStags and chunking tags in a decision tree and clas-sifies new items by comparing them to the trainingitems.
The algorithm is very fast and it reachesthe same performance as (Argamon et al, 1998)(F,~=1=91.6).
(Daelemans et al, 1999) uses cas-caded MBL (IBI-IG) in a similar way for severaltasks among which baseNP recognition.
They donot report F~=~ rates but their tag accuracy ratesare a lot better than accuracy rates reported byothers.
However, they use the (Ramshaw andMarcus, 1995) data set in a different raining-testdivision (10-fold cross validation) which makes it(tifficult to compare their results with others.5 Conc lud ing  remarksWe hay('.
(:omI)ared seven (tiffi~rent (tata.
formatsfor the recognition of baseNPs with memory-basedlearning (IBI-IG).
The IOB1 format, introducedin (Ramshaw and Marcus, 1995), consistently(:ame out as the best format.
However, the dif-ferences with other formats were not significant.Some representation formats achieved better pre-(:ision rates, others better recall rates.
This infor-mation is usefifl ibr tasks that require chunkingstructures because some tasks might be more in-terested in high precision rates while others mightbe more interested in high recall rates.The IBI-IG algorithm has been able to im-prove the best reported F2=1 rates for a stan-(lar(l data set (92.37 versus (Ramshaw and Mar-(:us, 1995)'s 92.03).
This result was aided by us-ing non-standard parameter values (k=3) and thealgorithm was sensitive for redundant input fea-tures.
This means that finding an optimal per-formance or this task requires searching a largeparameter/feature configuration space.
An inter-esting topic for future research would be to embedml-IG in a standard search algorithm, like hill-climbing, and explore this parameter space.
Somemore room for improved performance lies in com-puting the POS tags in the data with a bettertagger than presently used.ReferencesSteven Abney.
1991.
Parsing by chunks.In Principle-Based Parsing.
Kluwer AcademicPublishers,.Shlomo Argamon, Ido Dagan, and Yuval Kry-molowski.
1998.
A memory-based approach tolearning shallow natural language patterns.
InProceedings of the 17th International Confer-ence on Computational Linguistics (COLING-ACL '98).Claire Cardie and David Pierce.
1998.
Error-driven pruning of treebank grammars for basenoun phrase identification.
In Proceedings ofthe 17th International Conference on Compu-tational Linguistics (COLING-ACL '98).Walter Daelemans, Jakub Zavrel, Ko van derSloot, and Antal van den  Bosch.
1998.TiMBL: Tilburg Memory Based Learner- version 1.0 - Reference Guide.
ILK,Tilburg University, The Netherlands.http: // i lk.kub.nl/' i lk/papers/i lk9803.ps.gz.178Proceedings-of EACL '99Walter Daelemans, Antal van den Bosch, andJakub Zavrel.
1999.
Forgetting exceptions iharmful in language learning.
Machine Learn-ing, 11.Lance A. Ramshaw and Mitchell P. Marcus.1995.
Text chunking using transformation-based learning.
In Proceedings of the ThirdA CL Workshop on Very Large Corpora.Adwait Ratnaparkhi.
1998.
Maximum EntropyModels for Natural Language Ambiguity Reso-lution.
PhD thesis Computer and InformationScience, University of Pennsylvania.Jorn Veenstra.
1998.
Fast np chunking us-ing memory-based learning techniques.
InBENELEARN-98: Proceedings of the EigthBelgian-Dutch Conference on Machine Learn-ing.
ATO-DLO, Wageningen, report 352.179
