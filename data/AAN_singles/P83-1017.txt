Sentence  D isambiguat ionby a Sh i f t -Reduce  Pars ing  Techn ique*Stuart M. ShieberAbst rac tArtificial Intelligence CenterSRI International333 Ravenswood AvenueMenlo Park, CA 94025Native speakers of English show definite and consistentpreferences for certain readings of syntactically ambiguous en-tences.
A user of a natural-language-processing ystem wouldnaturally expect it to reflect the same preferences.
Thus, suchsystems must model in some way the l inguistic performance aswell as the l inguistic competence of the native speaker.
Wehave developed a parsing algorithm--a variant of the LALR(I}shift.-reduce algorithm--that models the preference behavior ofnative speakers for a range of syntactic preference phenomenareported in the psycholinguistic literature, including the recentdata on lexical preferences.
The algorithm yields the preferredparse deterministically, without building multiple parse treesand choosing among them.
As a side effect, it displays ap-propriate behavior in processing the much discussed garden-pathsentences.
The parsing algorithm has been implemented and hasconfirmed the feasibility of our approach to the modeling of thesephenomena.1.
In t roduct ionFor natural anguage processing systems to be useful, theymust assign the same interpretation to a given sentence that anative speaker would, since that  is precisely the behavior userswill expect..
Consider, for example, the case of ambiguous en-tences.
Native speakers of English show definite and consistentpreferences for certain readings of syntactically ambiguous en-tences \[Kimball, 1973, Frazier and Fodor, 1978, Ford et aL, 1982\].A user of a natural-language-processing system would naturallyexpect, it to reflect the same preferences.
Thus, such systemsmust model in some way the l ineuistie performance as well asthe linguistic competence of the native speaker.This idea is certainly not new in the artificial-intelligenceliterature.
The pioneering work of Marcus \[Marcus, 1980\] is per-haps the best.
known example of linguistic-performance modelingin AI.
Starting from the hypothesis that ~deterministic" parsingof English is possible, he demonstrated that certain performance"This research was supported by the Defense Advanced Research ProiectsAgency under Contract NOOO39-80-C-0575 with the Naval ElectronicSystems Command.
The views and conclusions contained in this documentare those of the author and should not be interpreted a.s representative ofthe oh~cial policies, either expressed or implied, of the Defense AdvancedResearch Projects Agency or the United States government.constraints, e.g., the difl\]culty of parsing garden-path sentences,could be modeled.
His claim about deterministic parsing wasquite strong.
Not only was the behavior of the parser requiredto be deterministic, but, as Marcus claimed,The interpreter cannot use some general rule to takea nondeterministic grammar specification and im-pose arbitrary constraints to convert it to a deter-ministic specification {unless, of course, there is ageneral rule which will always lead to the correctdecision in such a case).
\[Marcus, 1980, p.14\]We have developed and implemented a parsing systemthat.
given a nondeterministic grammar, forces disambiguationin just the manner Marcus rejected (i.e.
t .hrough general rules};it thereby exhibits the same preference behavior that psycbolin-guists have attributed to native speakers of English for a cer-tain range of ambiguities.
These include structural ambiguities\[Frazier and Fodor, 1978, Frazier and Fodor, 1980, Wanner, 1980land lexical preferences \[Ford et aL, 1982l, as well as the garden-path sentences as a side effect.
The parsing system is based onthe shih.-reduee scheduling technique of Pereira \[forthcoming\].Our parsing algorithm is a slight variant of LALR{ 1) pars-ing, and, as such, exhibits the three conditions postulated byMarcus for a deterministic mechanism: it is data-driven, reflectsexpectations, and has look-ahead.
Like Marcus's parser, ourparsing system is deterministic.
Unlike Marcus's parser, thegrammars used by our parser can be ambiguous.2.
The  Phenomena to be Mode ledThe parsing system was designed to manifest preferencesamong ,~tructurally distinct parses of ambiguous entences.
It,does this by building just one parse tree--rather than build-ing multiple parse trees and choosing among them.
Like theMarcus parsing system, ours does not do disambiguation requir-ing "extensive semantic processing," hut, in contrast o Marcus,it does handle such phenomena s PP-attachment insofar asthere exist a priori preferences for one attachment over another.By a priori we mean preferences that are exhibited in contextswhere pragmatic or plausibility considerations do not tend tofavor one reading over the other.
Rather than make such valuejudgments ourselves, we defer to the psycholinguistic literature{specifically \[Frazier and Fodor, 1978\], \[Frazier and Fodor, 1980\]and \[Ford et al, 1982\]) for our examples.113The parsing system models the following phenomena:Right AssociationNative speakers of English tend to prefer readings in whichconstituents are "attached low."
For instance, in the sen-tenceJoe bought the book that I hod been trving to obtain for~usan.the preferred reaL~lng is one in w~lch the prepositionalphrase "for Susan ~ is associated with %o obtain ~ ratherthan %ought.
~Min lmal  A t tachmentOn the other hand, higher attachment in preferred in eer-rain cases such asJoe bought he book \[or Suean.in which "for Susan* modifies %he book" rather than"bought."
Frazier and Fodor \[1978\] note that these arecanes in which the higher attachment includes fewer nodesin the parse tree.
Ore" analysis is somewhat different.Lexical  P re ferenceFord et al \[10821 present evidence that attachmentpreferences depend on lexical choice.
Thus, the preferredreading forThe woman wanted the dresm on that rock.has low attachment of the PP, whereasThe tnoman positioned the dreu on that rack.has high attachment.Garden-Path SentencesGrammatical sentences such asThe horse raced pamt he barn fell.seem actually to receive no parse by the native speakeruntil some sort of "conscioun parsing" is done.
FollowingMarcus \[Marcus, 1980\], we take this to be a hard failureof the human sentence-processing mechanism.It will be seen that all these phenomena axe handled in ouxparser by the same general rules.
The simple context-free gram-mar used t (see Appendix I) allows both parses of the ambiguoussentences as well as one for the garden-path sentences.
The par-ser disambiguates the grammar and yields only the preferredstructure.
The actual output of the parsing system can be foundin Appendix II.3.
The  Pars ing Sys temThe parsing system we use is a shift-reduce purser.
Shift-reduce parsers \[Aho and Johnson, 19741 axe a very general classof bottom-up parsers characterized by the following architecture.They incorporate a stock for holding constituents built up duringIWe make no claims a4 to the accuracy of the sample grammar.
It isobviously a gross simplific~t.ion of English syntax.
Ins role is merely toshow that the parsing system is sble to dis,~mbiguate the sentences underconsideration correctly.the parse and a shift-reduce table for guiding the parse, At eachstep in the parse, the table is used for deciding between two basictypes of operations: the shift operation, which adds the nextword in the sentence (with its pretcrminal category) to the topof the stack, and the reduce operation, which removes severalelements from the top of the stack and replaces them with anew element--for instance, removing an NP  and a VP  from thetop of the stack and replacing them with an S. The state of theparser is also updated in accordance with the shift-reduce tableat each stage.
The combination of the stack, input, and state ofthe parser will be called a configuration and will be notated as,for example,1 NPv IIMar, 110 1where the stack contains the nonterminals NP  and V, the inputcontains the lexical item Mary and the parser is in state 10.By way of example, we demonstrate the operation of theparser (using the grammar of Appendix I) on the oft-cited sen-tence "John loves Mary.
~ Initially the stack is empty and noinput has been consumed.
The parser begins in state 0.I  ahn 10.. Mar, i0 iAs elements are shifted to the stack, they axe replaced by theirpreterminal category."
T.he shiR-reduce table for the grammarof Appendix I states that in state 0, with a proper noun as thenext word in the input, the appropriate action is a shift.
Thenew configuration, therefore, isi PNOUN lo~e8 Mar~l i 4 !The next operation specified is a reduction of the proper nounto a noun phrase yielding, NP iI loves Mary \ [2  iThe verb and second proper noun axe now shifted, in accordancewith the shift-reduce table, exhausting the input, and the propernoun is then reduced to an NP.NP v !l Ma,, !1ov P. ouN il !,NP V NP i\] :14Finally, the verb and noun phrase on the top of the stack arereduced to a VPi NP VP !I !
l II ~6 Iwhich is in turn reduced, together with the subject NP, to an S.i sJl ,'I )This final configuration is an accepting configuration, since all2But see Section 3.'2.
for an exception.114the input has been consumed and an S derived.
Thus the sen-tence is grammatical ia the grammar of Appendix I, as expected.3.1 Di f ferences f rom the  Standard  LR  Techn iquesThe shift-reduce table mentioned above is generatedautomatically from a context-free grammar by the standard al-gorithm \[Aho and Johnson, 1974\].
The parsing alogrithm differs,however, from the standard LALR(1) parsing algorithm in twoways.
First, instead of assigning preterminal symbols to wordsas they are shifted, the algorithm allows the assignment to bedelayed if the word is ambiguous among preterminals.
Whenthe word is used in a reduction, the appropriate preterminal isassigned.Second, and most importantly, since true LR parsers existonly for unambiguous grammars, the normal algorithm for deriv-ing LALR(1) shift-reduce tables yields a table that may specifyconflicting actions under certain configurations.
It is through thechoice made from the options in a conflict that the preferencebehavior we desire is engendered.3.2 P re termina l  De lay ingOne key advantage of shift-reduce parsing that is criticalin our system is the fact that decisions about the structure tobe assigned to a phrase are postponed as long as possible.
Inkeeping with this general principle, we extend the algorithmto allow the ~ssignment of a preterminal category to a lexicalitem to be deferred until a decision is forced upon it, so tospeak, by aa encompassing reduction.
For instance, we would notwant to decide on the preterminal category of the word "that,"which can serve as either a determiner (DET) or complementizer(THAT), until some further information is available.
Considerthe sentencesThat problem i* important.That problema are difficult to naive ia important.Instead of a.~signiag a preterminal to ~that," we leave open thepossibility of assigning either DET or THAT until the first reduc-tion that involves the word.
In the first case, this reductionwill be by the rule NP ~DET NOM, thus forcing, once and forall, the assignment of DET as preterminal.
In the second ease,the DET NOM analysis is disallowed oa the basis of numberagreement, so that the first applicable reduction is the COMPSreduction to S, forcing the assignment of  THAT as preterminal.Of course, the question arises as to what state the par-ser goes into after shitting the lexical item ~that."
The answeris quite straightforward, though its interpretation t,i~ d t,,a thedeterminism hypothesis is subtle.
The simple answer is thatthe parser enters into a state corresponding to the union of thestates entered upon shifting a DET and upon shifting a THATrespectively, in much the same way as the deterministic simula-tion of a nondeterministic finite automaton enters a ~uniou"state when faced with a nondeterministic choice.
Are we thenmerely simulating a aoadeterministic machine here.
~The anss~eris equivocal.
Although the implementation acts as a simulatorfor a nondeterministic machine, the nondeterminism is a prioribounded, given a particular grammar and lexicon.
3 Thus.
thenondeterminism could be traded in for a larger, albeit still finite,set of states, unlike the nondeterminism found in other pars-ing algorithms.
Another way of looking at the situation is tonote that there is no observable property of the algorithm thatwould distinguish the operation of the parser from a determinis-tic one.
In some sense, there is no interesting difference betweenthe limited nondeterminism of this parser, and Marcus's notionof strict determinism.
In fact, the implementation of Marcus'sparser also embodies a bounded nondeterminism in much thesame way this parser does.The differentiating property between this parser and thatof Marcus is a slightly different one, namely, the property ofqaaM-real-time operation.
4By quasi-real-time operation, Marcusmeans that there exists a maximum interval of parser operationfor which no output can be generated.
If the parser operates forlonger than this, it must generate some output.
For instance,the parser might be guaranteed to produce output (i.e., struc-ture) at least every three words.
However, because preterminalassignment can be delayed indefinitely in pathological grammars,there may exist sentences in such grammars for which arbitrarynumbers of words need to be read before output can be produced.It is not clear whether this is a real disadvantage or not, and,if so, whether there are simple adjustments to the algorithmthat would result in quasi-real-time behavior.
In fact, it is aproperty of bottom-up parsing in general that quasi-real-timebehavior is not guaranteed.
Our parser has a less restrictive butsimilar property, fairneaH, that is, our parser generates outputlinear in the input, though there is no constant over which out-put is guaranteed.
For a fuller discussion of these properties, seePereira and Shieber \[forthcoming\].To summarize, preterminal delaying, as an intrinsic partof the algorithm, does not actually change the basic propertiesof the algorithm in any observable way.
Note, however, thatpreterminal assignments, like reductions, are irrevocable oncethey are made {as a byproduct of the determinism of the algo-rithm}.
Such decisions can therefore lead to garden paths, asthey do for the sentences presented in Section 3.6.We now discuss the central feature of the algorithm.namely, the resolution of shift-reduce conflicts.3.3 The  D isambiguat ion  Ru lesConflicts arise in two ways: aM/t-reduce conflicts, in whichthe parser has the option of either shifting a word onto the stackor reducing a set of elements on the stack to a new element;reduce-reduce conflicts, in which reductions by several grammar3The boundedness comes about because only a finite amount or informa-tie, n is kept per state (an integer) and the nondeterrninlsm stops at theprcterminat level, so that, the splitting of states does not.
propogate,41 am indebted to Mitch Marcus for this .bservation and the previouscomparison with his parser.i15rules are possible.
The parser uses two rules to resolve theseconflicts: 5( I )  Resolve shift-reduce conflicts by shifting.
(2) Resolve reduce-reduce onflicts by performingthe longer reduction.These two rules suffice to engender the appropriate be-havior in the parser for cases of right association and minimalattachment.
Though we demonstrate our system primarily withPP-attachment examples, we claim that the rules are generallyvalid for the phenomena being modeled \[Pereira nd Shieber,forthcoming\].3.4 Some ExamplesSome examples demonstrate hese principles.
Consider thesentenceJoe took the book that I bought for Sum,re.After a certain amount of parsing has beta completed eter-ministically, the parser will be in the following coniigttration:I NP v that V Ill?r S, .
.
.
Iwith a shift-reduce confict, since the V can be reduced to aVP/NP ?
or the P can be shifted.
The principle* presented wouldsolve the conflict in favor of the shift, thereby leading to thefollowing derivation:NP V NP that NP V P l\] Su,an 112 )"NPV NP that NPVP NP II 119  INP v NP that NP V PP !l 124 INPVNPthatNPVP/NP  II i 22 INP V NP that S/NP .1O INP v NP II I 7 I,,2Iq'P V NP, 11.
}14 I., NP VP t1 I 8 I.... sll I' Iwhich yields the structure:\[sdoe{vptook{Nl,{xethe book\]\[gthat I bought for Susanl\]\]\]The sentence5The original notion of using a shift-reduce parser and general schedulingprinciples to handle right association and minlmal attachment, togetherwith the following two rules, are due to Fernando Pereira \[Pereira, 1982\[.The formalization f preterminal delaying and the extensions to the Ionic tl-preference ases and garden-path behavior are due to the author.8The "slash-category" analysis of long-distance d pendencies used here isloosely based on the work of Gaadar \[lggl\].
The Appendix 1grammardoes not incorporate he full range of slashed rules, however, but merely arepresentative selection for illustrative purposes.Joe bou?ht he book for Su,an.demonstrates resolution of a reduce-reduce conflict.
At somepoint in the parse, the parser is in the following configuration:\[ NP V NP PP ii 120 Iwith a reduce-reduce onflict.
Either a more complex NP or aVP can be built.
The conflict is resolved in favor of the longerreduction, i.e., the VP reduction.
The derivation continues:I NP VP \[I I 8 !I sll 1!
Iending in an accepting state with the following generated struc-ture:\[sdoe{v~,bought\[Npthe bookl\[Ppfor Susan\]I\]3.5 Lexical PreferenceTo handle the lexical-preferenee xamples, we extend thesecond rule slightly.
Preterminal-word pairs can be stipulated aseither weak or strong.
The second rule becomes(2} Resolve reduce-reduce conflicts by performingthe longest reduction with the stroncest &ftmoststack element.
7Therefore, if it is assumed that the lexicon encodes theinformation that the triadic form of ~ant"  iV2 in the samplegrammar) and the dyadic form of ~position" (V1) are both weak,we can see the operation of the shift-reduce parser on the ~dresson that rack" sentences of Section 2.
Both sentences are similarin form and will thus have a similar configuration when thereduce-reduce onflict arises.
For example, the first sentence willbe in the following configuration:t NP wanted NP PP i\[ 120 iIn this case, the longer eduction would require assignment of thepreterminat category V2 to ~ant,"  which is the weak form: thus,the shorter eduction will be preferred, leading to the derivation:I NP wanted NP \]1 11,1\] NP VP II i 6:,':I sli i land the underlying structure:\[sthe woman\[vpwaated\[Np{Npthe dress\]\[ppoa that r~klll\]7Note that, strength takes precedence over length.116In the ca~e in which the verb is "positioned," however, the longerreduction does not yield the weak form of the verb; it will there-fore be invoked, reslting in the structure:\[sthe woman \[vP positioned \[Npthe dress\]\[ppon that rackl\]\]3.6 Garden-Path  SentencesAs a side effect of these conflict resolution rules, certainsentences in the language of the grammar will receive no parseby the parsing system just discussed.
These sentences are ap-parently the ones classified as "garden-path" sentences, a classthat humans also have great difficulty parsing.
Marcus's conjec-ture that such difficulty stems from a hard failure of the normalsentence-processing mechanism is directly modeled by the pars-ing system presented here.For instance, the sentenceThe horse raced past the barn fellexhibits a reduce-reduce conflict before the last word.
If theparticipial form of "raced" is weak, the finite verb form will bechosen; consequently, "raced pant the barn" will be reduced to aVP rather than a participial phrase.
The parser will fail shortly,since the correct choice of reduction was not made.Similarly, the sentenceThat scaly, deep-sea fish ,hould be underwater i~ impor-tant.will fail.
though grammatical.
Before the word %hould" isshifted, a reduce-reduce conflict arises in forming an NP fromeither "That scaly, deep-sea l~h" or "scaly, deep-sea fish."
Thelonger (incorrect} reduction will be performed and the parser willfail.Other examples, e.g., "the boy got fat melted," or "theprime number few" would be handled similarly by the parser,though the sample grammar of Appendix I does not parse them\[Pcreira nd Shieber, forthcoming\].4.
Conc lus ionTo be useful, aatttral-language systems must model thebehavior, if not the method, of the native speaker.
We havedemonstrated that a parser using simple general rules for disam-biguating sentences can yield appropriate behavior for a largeclass of performance phenomena--right a-~soeiation, minimal at-tachment, lexical preference, and garden-path sentences--andthat, morever, it can do so deterministically wit, hour generatingall the parses and choosing among them.
The parsing systemhas been implemented and has confirmed the feasibility of ottrapproach to the modeling of these phenomena.ReferencesAho, A.V.. and S.C. Johnson, 1974: "LR Parsing," Computi,, 9Sur,,eys.
Volume 6, Number 2, pp.
99-i24 ISpring).Ford, M., J. Bresnan, and R. Kaplan, 1982: "A Competence-Based Theory of Syntactic Closure," in The MentalRepresentation /Grammatical Relations, J. Bresnan, ed.
(Cambridge, Massachusetts: MIT Press).Frazier, L., and J.D.
radar, 1978: ~I'he Sausage Machine: ANew Two-Stage Parsing Model," Cognition, Volume 6, pp.291-325.Frazier, L., and J.D.
Fodor, 1980: "Is the Human SentenceParsing Mechanism aa ATN?"
Cognition, Volume 8, pp.411-459.Gazdar, G., 1981: "Unbounded dependencies and coordinatestructure," Linquistic Inquiry, Volume 12, pp.
105-179.Kimball, d., 1973: "Seven Principles of Surface Structure Parsingin Natural Language," Cognition, Volume 2, Number 1,pp.
15-47.Marcus, M., 1980: A Theory of Syntactic Recognition/or NaturalLanquagc, (Cambridge, Massachusetts: MIT Press).Pereira, F.C.N., forthcoming: "A New Characterization ofAttachment Preferences," to appear in D. Dowry,L.
Karttunen, and A. gwicky (eds.)
NaturalLanguage Prate,int.
Psyeholingui, t c, Computational,and Theoretical Perspective~, Cambridge, England:Cambridge University Press.Pereira, F.C.N., and S.M.
Shieber, forthcoming: "ShiR-ReduceScheduling and Syntactic Closure/ to appear.Wanner, E., 1980: "The ATN and the Sausage Machine: WhichOne is Baloney?"
Caanition, Volume 8, pp.
'209-225.Append ix  I.
The  Test  GrammarThe following is the grammar used to test the parting~ystem descibed in the paper.
Not a robust grammar of Englishby any means, it is presented only for the purpose of establishingthat the preference rules yield the correct, results.S - -  NP VP VP --  V3 INFS - -gVP  VP- -V4  ADJNP - -  DET NOM VP - -  V5 PPNP --  NOM 5-- that SNP --  PNOUN INF - -  to VPNP -- NP S/NP PP --  P NPNP --  NP PARTP PARTP - -  VPART PPNP -- NP PP S/NP - -  that S/NPDET --  NP 's  S/NP --  VPNOM - -  N S/NP - -  NP VP/NPNOM -- ADJ NOM VP/NP --  VlVP - -  AUX VP VP/NP - -  V2 PPVP -- V0 VP/NP -- V3 INF/NPVP -- Vl NP VP/NP - .
AUX VP/NPVP -- V2 NP PP INF/NP --* to VP/NPAppend ix  II.
Sample  Runs>> do*  bought the hook that  I had be ln  t ry in  E to obt .
info r  Susan117Accepted: IsCup Cpnonn Joe))(vpCvl bought)Cap(up (dec the)(uoa (n book)))(sbar/np(that that)Cs/npCup (pnou I))Cvp/up(uuz bud)(vp/np(auz been)(vp/np Cv3 try in l )(t-~/np(~plup(v2 obtain)(pp (p for}(up (pnoun Saul\]sta~e:stack:input:(1)<(0)>(v4 is)\[e \[up (den Thlt)(non (IdJ scaly)Chum (~tJ 4eup-ssl)(mum (u fish\]C,p Can should)(vp (v4 be)(adj uadu~ter \ ](|dj itportut)(end)>> Joe bought the book for SuuuAccepted: \[8 (up (puoun Joe))(vp (v2 boucht)Cup Cdet the)Chum Cn book)))(pp (p for)Cup (puoun Sueua\]>> The vomam vatted the dreou on thnt r~hAccepted: Is Cup Cdut The)Cue= (u vomu)))(Tp (vt v~ted)Cap (up (den the)(no= (n druu) ) )(pp (p on)(rip (det that)Curt (u rack\]>> The youth poeitioued the dreue on that rackAccepted: Is (up (den The)(noa (n vol,~)))(vp (~2 poaitioued)(up (den the)(nee (~ dreJl)))(pp Cp on)(up (den that}Cuom (.
rack\]>> The horse raced put  the barn fellParse failed.
Currant confiEurltlon:8tare: (l)stack: <(0)> Is Cap (4*t me)(not (u horse)))(vp (v6 rncea)(pp (p put )(up (4et the)(aou (u b~rn\]input: (tO fel l)Cend))) That ecal!
~eep-let fish should be undes=l~tur i8 importerParse failed.
Current cou~ilOlrttiou:118
