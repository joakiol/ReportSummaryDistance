Proceedings of the 12th Conference of the European Chapter of the ACL, pages 549?557,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsBilingually Motivated Domain-Adapted Word Segmentationfor Statistical Machine TranslationYanjun Ma Andy WayNational Centre for Language TechnologySchool of ComputingDublin City UniversityDublin 9, Ireland{yma, away}@computing.dcu.ieAbstractWe introduce a word segmentation ap-proach to languages where word bound-aries are not orthographically marked,with application to Phrase-Based Statis-tical Machine Translation (PB-SMT).
In-stead of using manually segmented mono-lingual domain-specific corpora to trainsegmenters, we make use of bilingual cor-pora and statistical word alignment tech-niques.
First of all, our approach isadapted for the specific translation task athand by taking the corresponding source(target) language into account.
Secondly,this approach does not rely on manu-ally segmented training data so that itcan be automatically adapted for differ-ent domains.
We evaluate the perfor-mance of our segmentation approach onPB-SMT tasks from two domains anddemonstrate that our approach scores con-sistently among the best results across dif-ferent data conditions.1 IntroductionState-of-the-art Statistical Machine Translation(SMT) requires a certain amount of bilingual cor-pora as training data in order to achieve compet-itive results.
The only assumption of most cur-rent statistical models (Brown et al, 1993; Vogelet al, 1996; Deng and Byrne, 2005) is that thealigned sentences in such corpora should be seg-mented into sequences of tokens that are meant tobe words.
Therefore, for languages where wordboundaries are not orthographically marked, toolswhich segment a sentence into words are required.However, this segmentation is normally performedas a preprocessing step using various word seg-menters.
Moreover, most of these segmenters areusually trained on a manually segmented domain-specific corpus, which is not adapted for the spe-cific translation task at hand given that the manualsegmentation is performed in a monolingual con-text.
Consequently, such segmenters cannot pro-duce consistently good results when used acrossdifferent domains.A substantial amount of research has been car-ried out to address the problems of word segmen-tation.
However, most research focuses on com-bining various segmenters either in SMT trainingor decoding (Dyer et al, 2008; Zhang et al, 2008).One important yet often neglected fact is that theoptimal segmentation of the source (target) lan-guage is dependent on the target (source) languageitself, its domain and its genre.
Segmentation con-sidered to be ?good?
from a monolingual pointof view may be unadapted for training alignmentmodels or PB-SMT decoding (Ma et al, 2007).The resulting segmentation will consequently in-fluence the performance of an SMT system.In this paper, we propose a bilingually moti-vated automatically domain-adapted approach forSMT.
We utilise a small bilingual corpus withthe relevant language segmented into basic writ-ing units (e.g.
characters for Chinese or kana forJapanese).
Our approach consists of using theoutput from an existing statistical word alignerto obtain a set of candidate ?words?.
We evalu-ate the reliability of these candidates using sim-ple metrics based on co-occurrence frequencies,similar to those used in associative approaches toword alignment (Melamed, 2000).
We then mod-ify the segmentation of the respective sentencesin the parallel corpus according to these candi-date words; these modified sentences are thengiven back to the word aligner, which producesnew alignments.
We evaluate the validity of ourapproach by measuring the influence of the seg-mentation process on Chinese-to-English MachineTranslation (MT) tasks in two different domains.The remainder of this paper is organised as fol-549lows.
In section 2, we study the influence ofword segmentation on PB-SMT across differentdomains.
Section 3 describes the working mecha-nism of our bilingually motivated word segmenta-tion approach.
In section 4, we illustrate the adap-tation of our decoder to this segmentation scheme.The experiments conducted in two different do-mains are reported in Section 5 and 6.
We discussrelated work in section 7.
Section 8 concludes andgives avenues for future work.2 The Influence of Word Segmentationon SMT: A Pilot InvestigationThe monolingual word segmentation step in tra-ditional SMT systems has a substantial impact onthe performance of such systems.
A considerableamount of recent research has focused on the in-fluence of word segmentation on SMT (Ma et al,2007; Chang et al, 2008; Zhang et al, 2008);however, most explorations focused on the impactof various segmentation guidelines and the mech-anisms of the segmenters themselves.
A currentresearch interest concerns consistency of perfor-mance across different domains.
From our ex-periments, we show that monolingual segmenterscannot produce consistently good results when ap-plied to a new domain.Our pilot investigation into the influence ofword segmentation on SMT involves three off-the-shelf Chinese word segmenters includingICTCLAS (ICT) Olympic version1, LDC seg-menter2 and Stanford segmenter version 2006-05-113.
Both ICTCLAS and Stanford segmentersutilise machine learning techniques, with HiddenMarkov Models for ICT (Zhang et al, 2003) andconditional random fields for the Stanford seg-menter (Tseng et al, 2005).
Both segmenta-tion models were trained on news domain datawith named entity recognition functionality.
TheLDC segmenter is dictionary-based with word fre-quency information to help disambiguation, bothof which are collected from data in the news do-main.
We used Chinese character-based and man-ual segmentations as contrastive segmentations.The experiments were carried out on a range ofdata sizes from news and dialogue domains usinga state-of-the-art Phrase-Based SMT (PB-SMT)1http://ictclas.org/index.html2http://www.ldc.upenn.edu/Projects/Chinese3http://nlp.stanford.edu/software/segmenter.shtmlsystem?Moses (Koehn et al, 2007).
The perfor-mance of PB-SMT system is measured with BLEUscore (Papineni et al, 2002).We firstly measure the influence of word seg-mentation on in-domain data with respect to thethree above mentioned segmenters, namely UNdata from the NIST 2006 evaluation campaign.
Ascan be seen from Table 1, using monolingual seg-menters achieves consistently better SMT perfor-mance than character-based segmentation (CS) ondifferent data sizes, which means character-basedsegmentation is not good enough for this domainwhere the vocabulary tends to be large.
We canalso observe that the ICT and Stanford segmenterconsistently outperform the LDC segmenter.
Evenusing 3M sentence pairs for training, the differ-ences between them are still statistically signifi-cant (p < 0.05) using approximate randomisation(Noreen, 1989) for significance testing.40K 160K 640K 3MCS 8.33 12.47 14.40 17.80ICT 10.17 14.85 17.20 20.50LDC 9.37 13.88 15.86 19.59Stanford 10.45 15.26 16.94 20.64Table 1: Word segmentation on NIST data setsHowever, when tested on out-of-domain data,i.e.
IWSLT data in the dialogue domain, the re-sults seem to be more difficult to predict.
Wetrained the system on different sizes of data andevaluated the system on two test sets: IWSLT2006 and 2007.
From Table 2, we can see that onthe IWSLT 2006 test sets, LDC achieves consis-tently good results and the Stanford segmenter isthe worst.4 Furthermore, the character-based seg-mentation also achieves competitive results.
OnIWSLT 2007, all monolingual segmenters outper-form character-based segmentation and the LDCsegmenter is only slightly better than the other seg-menters.From the experiments reported above, wecan reach the following conclusions.
First ofall, character-based segmentation cannot achievestate-of-the-art results in most experimental SMTsettings.
This also motivates the necessity towork on better segmentation strategies.
Second,monolingual segmenters cannot achieve consis-4Interestingly, the developers themselves also note thesensitivity of the Stanford segmenter and incorporate exter-nal lexical information to address such problems (Chang etal., 2008).55040K 160KIWSLT06 CS 19.31 23.06Manual 19.94 -ICT 20.34 23.36LDC 20.37 24.34Stanford 18.25 21.40IWSLT07 CS 29.59 30.25Manual 33.85 -ICT 31.18 33.38LDC 31.74 33.44Stanford 30.97 33.41Table 2: Word segmentation on IWSLT data setstently good results when used in another domain.In the following sections, we propose a bilinguallymotivated segmentation approach which can beautomatically derived from a small representativedata set and the experiments show that we can con-sistently obtain state-of-the-art results in differentdomains.3 Bilingually Motivated WordSegmentation3.1 NotationWhile in this paper, we focus on Chinese?English,the method proposed is applicable to other lan-guage pairs.
The notation, however, assumesChinese?English MT.
Given a Chinese sentencecJ1 consisting of J characters {c1, .
.
.
, cJ} andan English sentence eI1 consisting of I words{e1, .
.
.
, eI}, AC?E will denote a Chinese-to-English word alignment between cJ1 and eI1.
Sincewe are primarily interested in 1-to-n alignments,AC?E can be represented as a set of pairs ai =?Ci, ei?
denoting a link between one single En-glish word ei and a few Chinese characters Ci.Theset Ci is empty if the word ei is not aligned to anycharacter in cJ1 .3.2 Candidate ExtractionIn the following, we assume the availability of anautomatic word aligner that can output alignmentsAC?E for any sentence pair (cJ1 , eI1) in a paral-lel corpus.
We also assume that AC?E contain1-to-n alignments.
Our method for Chinese wordsegmentation is as follows: whenever a single En-glish word is aligned with several consecutive Chi-nese characters, they are considered candidates forgrouping.
Formally, given an alignment AC?Ebetween cJ1 and eI1, if ai = ?Ci, ei?
?
AC?E ,with Ci = {ci1 , .
.
.
, cim} and ?k ?
J1,m ?
1K,ik+1 ?
ik = 1, then the alignment ai between eiand the sequence of words Ci is considered a can-didate word.
Some examples of such 1-to-n align-ments between Chinese and English we can deriveautomatically are displayed in Figure 1.5Figure 1: Example of 1-to-n word alignments be-tween English words and Chinese characters3.3 Candidate Reliability EstimationOf course, the process described above is error-prone, especially on a small amount of trainingdata.
If we want to change the input segmentationto give to the word aligner, we need to make surethat we are not making harmful modifications.
Wethus additionally evaluate the reliability of the can-didates we extract and filter them before inclusionin our bilingual dictionary.
To perform this filter-ing, we use two simple statistical measures.
In thefollowing, ai = ?Ci, ei?
denotes a candidate.The first measure we consider is co-occurrencefrequency (COOC(Ci, ei)), i.e.
the number oftimes Ci and ei co-occur in the bilingual corpus.This very simple measure is frequently used in as-sociative approaches (Melamed, 2000).
The sec-ond measure is the alignment confidence (Ma etal., 2007), defined asAC(ai) =C(ai)COOC(Ci, ei),where C(ai) denotes the number of alignmentsproposed by the word aligner that are identical toai.
In other words, AC(ai) measures how oftenthe aligner aligns Ci and ei when they co-occur.We also impose that |Ci | ?
k, where k is a fixedinteger that may depend on the language pair (be-tween 3 and 5 in practice).
The rationale behindthis is that it is very rare to get reliable alignmentsbetween one word and k consecutive words whenk is high.5While in this paper we are primarily concerned with lan-guages where the word boundaries are not orthographicallymarked, this approach, however, can also be applied to lan-guages marked with word boundaries to construct bilinguallymotivated ?words?.551The candidates are included in our bilingual dic-tionary if and only if their measures are abovesome fixed thresholds tCOOC and tAC , which al-low for the control of the size of the dictionary andthe quality of its contents.
Some other measures(including the Dice coefficient) could be consid-ered; however, it has to be noted that we are moreinterested here in the filtering than in the discov-ery of alignments per se, since our method buildsupon an existing aligner.
Moreover, we will seethat even these simple measures can lead to an im-provement in the alignment process in an MT con-text.3.4 Bootstrapped word segmentationOnce the candidates are extracted, we performword segmentation using the bilingual dictionar-ies constructed using the method described above;this provides us with an updated training corpus,in which some character sequences have been re-placed by a single token.
This update is totallynaive: if an entry ai = ?Ci, ei?
is present in thedictionary and matches one sentence pair (cJ1 , eI1)(i.e.
Ci and ei are respectively contained in cJ1 andeI1), then we replace the sequence of characters Ciwith a single token which becomes a new lexicalunit.6 Note that this replacement occurs even ifno alignment was found between Ci and ei for thepair (cJ1 , eI1).
This is motivated by the fact that thefiltering described above is quite conservative; wetrust the entry ai to be correct.This process can be applied several times: oncewe have grouped some characters together, theybecome the new basic unit to consider, and we canre-run the same method to get additional group-ings.
However, we have not seen in practice muchbenefit from running it more than twice (few newcandidates are extracted after two iterations).4 Word Lattice Decoding4.1 Word LatticesIn the decoding stage, the various segmentationalternatives can be encoded into a compact rep-resentation of word lattices.
A word lattice G =?V,E?
is a directed acyclic graph that formally isa weighted finite state automaton.
In the case ofword segmentation, each edge is a candidate wordassociated with its weights.
A straightforward es-6In case of overlap between several groups of words toreplace, we select the one with the highest confidence (ac-cording to tAC).timation of the weights is to distribute the proba-bility mass for each node uniformly to each out-going edge.
The single node having no outgoingedges is designated the ?end node?.
An exampleof word lattices for a Chinese sentence is shown inFigure 2.4.2 Word Lattice GenerationPrevious research on generating word lattices re-lies on multiple monolingual segmenters (Xu etal., 2005; Dyer et al, 2008).
One advantage ofour approach is that the bilingually motivated seg-mentation process facilitates word lattice genera-tion without relying on other segmenters.
As de-scribed in section 3.4, the update of the trainingcorpus based on the constructed bilingual dictio-nary requires that the sentence pair meets the bilin-gual constraints.
Such a segmentation process inthe training stage facilitates the utilisation of wordlattice decoding.4.3 Phrase-Based Word Lattice DecodingGiven a Chinese input sentence cJ1 consisting of Jcharacters, the traditional approach is to determinethe best word segmentation and perform decodingafterwards.
In such a case, we first seek a singlebest segmentation:f?K1 = arg maxfK1 ,K{Pr(fK1 |cJ1 )}Then in the decoding stage, we seek:e?I1 = arg maxeI1,I{Pr(eI1|f?K1 )}In such a scenario, some segmentations which arepotentially optimal for the translation may be lost.This motivates the need for word lattice decoding.The search process can be rewritten as:e?I1 = arg maxeI1,I{maxfK1 ,KPr(eI1, fK1 |cJ1 )}= arg maxeI1,I{maxfK1 ,KPr(eI1)Pr(fK1 |eI1, cJ1 )}= arg maxeI1,I{maxfK1 ,KPr(eI1)Pr(fK1 |eI1)Pr(fK1 |cJ1 )}Given the fact that the number of segmentationsfK1 grows exponentially with respect to the num-ber of characters K , it is impractical to firstly enu-merate all possible fK1 and then to decode.
How-ever, it is possible to enumerate all the alternativesegmentations for a substring of cJ1 , making theutilisation of word lattices tractable in PB-SMT.552Figure 2: Example of a word lattice5 Experimental Setting5.1 EvaluationThe intrinsic quality of word segmentation is nor-mally evaluated against a manually segmentedgold-standard corpus using F-score.
While thisapproach can give a direct evaluation of the qual-ity of the word segmentation, it is faced with sev-eral limitations.
First of all, it is really difficult tobuild a reliable and objective gold-standard giventhe fact that there is only 70% agreement betweennative speakers on this task (Sproat et al, 1996).Second, an increase in F-score does not necessar-ily imply an improvement in translation quality.
Ithas been shown that F-score has a very weak cor-relation with SMT translation quality in terms ofBLEU score (Zhang et al, 2008).
Consequently,we chose to extrinsically evaluate the performanceof our approach via the Chinese?English transla-tion task, i.e.
we measure the influence of thesegmentation process on the final translation out-put.
The quality of the translation output is mainlyevaluated using BLEU, with NIST (Doddington,2002) and METEOR (Banerjee and Lavie, 2005)as complementary metrics.5.2 DataThe data we used in our experiments are fromtwo different domains, namely news and travel di-alogues.
For the news domain, we trained oursystem using a portion of UN data for NIST2006 evaluation campaign.
The system was de-veloped on LDC Multiple-Translation Chinese(MTC) Corpus and tested on MTC part 2, whichwas also used as a test set for NIST 2002 evalua-tion campaign.For the dialogue data, we used the Chinese?English datasets provided within the IWSLT 2007evaluation campaign.
Specifically, we used thestandard training data, to which we added devset1and devset2.
Devset4 was used to tune the param-eters and the performance of the system was testedon both IWSLT 2006 and 2007 test sets.
We usedboth test sets because they are quite different interms of sentence length and vocabulary size.
Totest the scalability of our approach, we used HITcorpus provided within IWSLT 2008 evaluationcampaign.
The various statistics for the corporaare shown in Table 3.5.3 Baseline SystemWe conducted experiments using different seg-menters with a standard log-linear PB-SMTmodel: GIZA++ implementation of IBM wordalignment model 4 (Och and Ney, 2003), therefinement and phrase-extraction heuristics de-scribed in (Koehn et al, 2003), minimum-error-rate training (Och, 2003), a 5-gram languagemodel with Kneser-Ney smoothing trained withSRILM (Stolcke, 2002) on the English side of thetraining data, and Moses (Koehn et al, 2007; Dyeret al, 2008) to translate both single best segmen-tation and word lattices.6 Experiments6.1 ResultsThe initial word alignments are obtained usingthe baseline configuration described above by seg-menting the Chinese sentences into characters.From these we build a bilingual 1-to-n dictionary,and the training corpus is updated by grouping thecharacters in the dictionaries into a single word,using the method presented in section 3.4.
As pre-viously mentioned, this process can be repeatedseveral times.
We then extract aligned phrases us-ing the same procedure as for the baseline sys-tem; the only difference is the basic unit we areconsidering.
Once the phrases are extracted, weperform the estimation of weights for the fea-tures of the log-linear model.
We then use asimple dictionary-based maximum matching algo-rithm to obtain a single-best segmentation for theChinese sentences in the development set so that553Train Dev.
Eval.Zh En Zh En Zh EnDialogue Sentences 40,958 489 (7 ref.)
489 (6 ref.
)/489 (7 ref.
)Running words 488,303 385,065 8,141 46,904 8,793/4,377 51,500/23,181Vocabulary size 2,742 9,718 835 1,786 936/772 2,016/1,339News Sentences 40,000 993 (9 ref.)
878 (4 ref.
)Running words 1,412,395 956,023 41,466 267,222 38,700 105,530Vocabulary size 6057 20,068 1,983 10,665 1,907 7,388Table 3: Corpus statistics for Chinese (Zh) character segmentation and English (En)minimum-error-rate training can be performed.7Finally, in the decoding stage, we use the samesegmentation algorithm to obtain the single-bestsegmentation on the test set, and word lattices canalso be generated using the bilingual dictionary.The various parameters of the method (k, tCOOC ,tAC , cf.
section 3) were optimised on the develop-ment set.
One iteration of character grouping onthe NIST task was found to be enough; the optimalset of values was found to be k = 3, tAC = 0.0and tCOOC = 0, meaning that all the entries in thebilingually dictionary are kept.
On IWSLT data,we found that two iterations of character groupingwere needed: the optimal set of values was foundto be k = 3, tAC = 0.3, tCOOC = 8 for the firstiteration, and tAC = 0.2, tCOOC = 15 for thesecond.As can be seen from Table 4, our bilinguallymotivated segmenter (BS) achieved statisticallysignificantly better results than character-basedsegmentation when enhanced with word lattice de-coding.8 Compared to the best in-domain seg-menter, namely the Stanford segmenter on thisparticular task, our approach is inferior accord-ing to BLEU and NIST.
We firstly attribute thisto the small amount of training data, from whicha high quality bilingual dictionary cannot be ob-tained due to data sparseness problems.
We alsoattribute this to the vast amount of named entityterms in the test sets, which is extremely difficultfor our approach.9 We expect to see better re-sults when a larger amount of data is used and thesegmenter is enhanced with a named entity recog-niser.
On IWSLT data (cf.
Tables 5 and 6), our7In order to save computational time, we used the sameset of parameters obtained above to decode both the single-best segmentation and the word lattice.8Note the BLEU scores are particularly low due to thenumber of references used (4 references), in addition to thesmall amount of training data available.9As we previously point out, both ICT and Stanford seg-menters are equipped with named entity recognition func-tionality.
This may risk causing data sparseness problems onsmall training data.
However, this is beneficial in the transla-tion process compared to character-based segmentation.approach yielded a consistently good performanceon both translation tasks compared to the best in-domain segmenter?the LDC segmenter.
More-over, the good performance is confirmed by allthree evaluation measures.BLEU NIST METEORCS 8.43 4.6272 0.3778Stanford 10.45 5.0675 0.3699BS-SingleBest 7.98 4.4374 0.3510BS-WordLattice 9.04 4.6667 0.3834Table 4: BS on NIST taskBLEU NIST METEORCS 0.1931 6.1816 0.4998LDC 0.2037 6.2089 0.4984BS-SingleBest 0.1865 5.7816 0.4602BS-WordLattice 0.2041 6.2874 0.5124Table 5: BS on IWSLT 2006 taskBLEU NIST METEORCS 0.2959 6.1216 0.5216LDC 0.3174 6.2464 0.5403BS-SingleBest 0.3023 6.0476 0.5125BS-WordLattice 0.3171 6.3518 0.5603Table 6: BS on IWSLT 2007 task6.2 Parameter Search GraphThe reliability estimation process is computation-ally intensive.
However, this can be easily paral-lelised.
From our experiments, we observed thatthe translation results are very sensitive to the pa-rameters and this search process is essential toachieve good results.
Figure 3 is the search graphon the IWSLT data set in the first iteration step.From this graph, we can see that filtering of thebilingual dictionary is essential in order to achievebetter performance.554Figure 3: The search graph on development set ofIWSLT task6.3 Vocabulary SizeOur bilingually motivated segmentation approachhas to overcome another challenge in order toproduce competitive results, i.e.
data sparseness.Given that our segmentation is based on bilingualdictionaries, the segmentation process can signif-icantly increase the size of the vocabulary, whichcould potentially lead to a data sparseness prob-lem when the size of the training data is small.
Ta-bles 7 and 8 list the statistics of the Chinese sideof the training data, including the total vocabulary(Voc), number of character vocabulary (Char.voc)in Voc, and the running words (Run.words) whendifferent word segmentations were used.
From Ta-ble 7, we can see that our approach suffered fromdata sparseness on the NIST task, i.e.
a largevocabulary was generated, of which a consider-able amount of characters still remain as separatewords.
On the IWSLT task, since the dictionarygeneration process is more conservative, we main-tained a reasonable vocabulary size, which con-tributed to the final good performance.Voc.
Char.voc Run.
WordsCS 6,057 6,057 1,412,395ICT 16,775 1,703 870,181LDC 16,100 2,106 881,861Stanford 22,433 1,701 880,301BS 18,111 2,803 927,182Table 7: Vocabulary size of NIST task (40K)6.4 ScalabilityThe experimental results reported above are basedon a small training corpus containing roughly40,000 sentence pairs.
We are particularly inter-ested in the performance of our segmentation ap-Voc.
Char.voc Run.
WordsCS 2,742 2,742 488,303ICT 11,441 1,629 358,504LDC 9,293 1,963 364,253Stanford 18,676 981 348,251BS 3,828 2,740 402,845Table 8: Vocabulary size of IWSLT task (40K)proach when it is scaled up to larger amounts ofdata.
Given that the optimisation of the bilingualdictionary is computationally intensive, it is im-practical to directly extract candidate words andestimate their reliability.
As an alternative, we canuse the obtained bilingual dictionary optimised onthe small corpus to perform segmentation on thelarger corpus.
We expect competitive results whenthe small corpus is a representative sample of thelarger corpus and large enough to produce reliablebilingual dictionaries without suffering severelyfrom data sparseness.As we can see from Table 9, our segmenta-tion approach achieved consistent results on bothIWSLT 2006 and 2007 test sets.
On the NIST task(cf.
Table 10), our approach outperforms the basiccharacter-based segmentation; however, it is stillinferior compared to the other in-domain mono-lingual segmenters due to the low quality of thebilingual dictionary induced (cf.
section 6.1).IWSLT06 IWSLT07CS 23.06 30.25ICT 23.36 33.38LDC 24.34 33.44Stanford 21.40 33.41BS-SingleBest 22.45 30.76BS-WordLattice 24.18 32.99Table 9: Scale-up to 160K on IWSLT data sets160K 640KCS 12.47 14.40ICT 14.85 17.20LDC 13.88 15.86Stanford 15.26 16.94BS-SingleBest 12.58 14.11BS-WordLattice 13.74 15.33Table 10: Scalability of BS on NIST task5556.5 Using different word alignersThe above experiments rely on GIZA++ to per-form word alignment.
We next show that our ap-proach is not dependent on the word aligner giventhat we have a conservative reliability estimationprocedure.
Table 11 shows the results obtained onthe IWSLT data set using the MTTK alignmenttool (Deng and Byrne, 2005; Deng and Byrne,2006).IWSLT06 IWSLT07CS 21.04 31.41ICT 20.48 31.11LDC 20.79 30.51Stanford 17.84 29.35BS-SingleBest 19.22 29.75BS-WordLattice 21.76 31.75Table 11: BS on IWSLT data sets using MTTK7 Related Work(Xu et al, 2004) were the first to question the useof word segmentation in SMT and showed that thesegmentation proposed by word alignments can beused in SMT to achieve competitive results com-pared to using monolingual segmenters.
Our ap-proach differs from theirs in two aspects.
Firstly,(Xu et al, 2004) use word aligners to reconstructa (monolingual) Chinese dictionary and reuse thisdictionary to segment Chinese sentences as othermonolingual segmenters.
Our approach featuresthe use of a bilingual dictionary and conducts adifferent segmentation.
In addition, we add a pro-cess which optimises the bilingual dictionary ac-cording to translation quality.
(Ma et al, 2007)proposed an approach to improve word alignmentby optimising the segmentation of both source andtarget languages.
However, the reported experi-ments still rely on some monolingual segmentersand the issue of scalability is not addressed.
Ourresearch focuses on avoiding the use of monolin-gual segmenters in order to improve the robustnessof segmenters across different domains.
(Xu et al, 2005) were the first to propose theuse of word lattice decoding in PB-SMT, in orderto address the problems of segmentation.
(Dyeret al, 2008) extended this approach to hierarchi-cal SMT systems and other language pairs.
How-ever, both of these methods require some mono-lingual segmentation in order to generate word lat-tices.
Our approach facilitates word lattice gener-ation given that our segmentation is driven by thebilingual dictionary.8 Conclusions and Future WorkIn this paper, we introduced a bilingually moti-vated word segmentation approach for SMT.
Theassumption behind this motivation is that the lan-guage to be segmented can be tokenised into ba-sic writing units.
Firstly, we extract 1-to-n wordalignments using statistical word aligners to con-struct a bilingual dictionary in which each entryindicates a correspondence between one Englishword and n Chinese characters.
This dictionary isthen filtered using a few simple association mea-sures and the final bilingual dictionary is deployedfor word segmentation.
To overcome the segmen-tation problem in the decoding stage, we deployedword lattice decoding.We evaluated our approach on translation tasksfrom two different domains and demonstrate thatour approach is (i) not as sensitive as monolingualsegmenters, and (ii) that the SMT system usingour word segmentation can achieve state-of-the-artperformance.
Moreover, our approach can easilybe scaled up to larger data sets and achieves com-petitive results if the small data used is a represen-tative sample.As for future work, firstly we plan to integratesome named entity recognisers into our approach.We also plan to try our approach in more do-mains and on other language pairs (e.g.
Japanese?English).
Finally, we intend to explore the corre-lation between vocabulary size and the amount oftraining data needed in order to achieve good re-sults using our approach.AcknowledgmentsThis work is supported by Science Foundation Ire-land (O5/IN/1732) and the Irish Centre for High-End Computing.10 We would like to thank the re-viewers for their insightful comments.ReferencesSatanjeev Banerjee and Alon Lavie.
2005.
METEOR:An automatic metric for MT evaluation with im-proved correlation with human judgments.
In Pro-ceedings of the ACL Workshop on Intrinsic and Ex-trinsic Evaluation Measures for Machine Transla-tion and/or Summarization, pages 65?72, Ann Ar-bor, MI.10http://www.ichec.ie/556Peter F. Brown, Stephen A. Della Pietra, VincentJ.
Della Pietra, and Robert L. Mercer.
1993.The mathematics of statistical machine translation:Parameter estimation.
Computational Linguistics,19(2):263?311.Pi-Chuan Chang, Michel Galley, and Christopher D.Manning.
2008.
Optimizing Chinese word segmen-tation for machine translation performance.
In Pro-ceedings of the Third Workshop on Statistical Ma-chine Translation, pages 224?232, Columbus, OH.Yonggang Deng and William Byrne.
2005.
HMMword and phrase alignment for statistical machinetranslation.
In Proceedings of Human LanguageTechnology Conference and Conference on Empiri-cal Methods in Natural Language Processing, pages169?176, Vancouver, BC, Canada.Yonggang Deng and William Byrne.
2006.
MTTK:An alignment toolkit for statistical machine transla-tion.
In Proceedings of the Human Language Tech-nology Conference of the NAACL, pages 265?268,New York City, NY.George Doddington.
2002.
Automatic evaluationof machine translation quality using n-gram co-occurrence statistics.
In Proceedings of the secondinternational conference on Human Language Tech-nology Research, pages 138?145, San Francisco,CA.Christopher Dyer, Smaranda Muresan, and PhilipResnik.
2008.
Generalizing word lattice translation.In Proceedings of the 46th Annual Meeting of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 1012?1020, Colum-bus, OH.Philipp Koehn, Franz Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proceed-ings of Human Language Technology Conferenceand Conference on Empirical Methods in NaturalLanguage Processing, pages 48?54, Edmonton, AL,Canada.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open source toolkit for statistical machine transla-tion.
In Proceedings of the 45th Annual Meeting ofthe Association for Computational Linguistics Com-panion Volume Proceedings of the Demo and PosterSessions, pages 177?180, Prague, Czech Republic.Yanjun Ma, Nicolas Stroppa, and Andy Way.
2007.Bootstrapping word alignment via word packing.
InProceedings of the 45th Annual Meeting of the As-sociation of Computational Linguistics, pages 304?311, Prague, Czech Republic.I.
Dan Melamed.
2000.
Models of translational equiv-alence among words.
Computational Linguistics,26(2):221?249.Eric W. Noreen.
1989.
Computer-Intensive Methodsfor Testing Hypotheses: An Introduction.
Wiley-Interscience, New York, NY.Franz Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.Computational Linguistics, 29(1):19?51.Franz Och.
2003.
Minimum error rate training in sta-tistical machine translation.
In Proceedings of the41st Annual Meeting of the Association for Com-putational Linguistics, pages 160?167, Sapporo,Japan.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedings ofthe 40th Annual Meeting of the Association for Com-putational Linguistics, pages 311?318, Philadelphia,PA.Richard W Sproat, Chilin Shih, William Gale, andNancy Chang.
1996.
A stochastic finite-state word-segmentation algorithm for Chinese.
ComputationalLinguistics, 22(3):377?404.Andrea Stolcke.
2002.
SRILM ?
An extensible lan-guage modeling toolkit.
In Proceedings of the Inter-national Conference on Spoken Language Process-ing, pages 901?904, Denver, CO.Huihsin Tseng, Pichuan Chang, Galen Andrew, DanielJurafsky, and Christopher Manning.
2005.
A condi-tional random field word segmenter for sighan bake-off 2005.
In Proceedings of Fourth SIGHAN Work-shop on Chinese Language Processing, pages 168?171, Jeju Island, Republic of Korea.Stefan Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM-based word alignment in statisticaltranslation.
In Proceedings of the 16th InternationalConference on Computational Linguistics, pages836?841, Copenhagen, Denmark.Jia Xu, Richard Zens, and Hermann Ney.
2004.
Dowe need Chinese word segmentation for statisticalmachine translation?
In ACL SIGHAN Workshop2004, pages 122?128, Barcelona, Spain.Jia Xu, Evgeny Matusov, Richard Zens, and HermannNey.
2005.
Integrated Chinese word segmentationin statistical machine translation.
In Proceedingsof the International Workshop on Spoken LanguageTranslation, pages 141?147, Pittsburgh, PA.Huaping Zhang, Hongkui Yu, Deyi Xiong, and QunLiu.
2003.
HHMM-based Chinese lexical ana-lyzer ICTCLAS.
In Proceedings of Second SIGHANWorkshop on Chinese Language Processing, pages184?187, Sappora, Japan.Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita.2008.
Improved statistical machine translation bymultiple Chinese word segmentation.
In Proceed-ings of the Third Workshop on Statistical MachineTranslation, pages 216?223, Columbus, OH.557
