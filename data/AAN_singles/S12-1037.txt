First Joint Conference on Lexical and Computational Semantics (*SEM), pages 282?287,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsUCM-I: A Rule-based Syntactic Approach for Resolving the Scope ofNegationJorge Carrillo de Albornoz, Laura Plaza, Alberto D?
?az and Miguel BallesterosUniversidad Complutense de MadridC/ Prof. Jose?
Garc?
?a Santesmases, s/n28040 Madrid (Spain){jcalbornoz,lplazam,albertodiaz,miballes}@fdi.ucm.esAbstractThis paper presents one of the two contribu-tions from the Universidad Complutense deMadrid to the *SEM Shared Task 2012 on Re-solving the Scope and Focus of Negation.
Wedescribe a rule-based system for detecting thepresence of negations and delimitating theirscope.
It was initially intended for process-ing negation in opinionated texts, and has beenadapted to fit the task requirements.
It firstdetects negation cues using a list of explicitnegation markers (such as not or nothing), andinfers other implicit negations (such as affixalnegations, e.g, undeniable or improper) by us-ing semantic information from WordNet con-cepts and relations.
It next uses the informa-tion from the syntax tree of the sentence inwhich the negation arises to get a first approxi-mation to the negation scope, which is later re-fined using a set of post-processing rules thatbound or expand such scope.1 IntroductionDetecting negation is important for many NLP tasks,as it may reverse the meaning of the text affectedby it.
In information extraction, for instance, it isobviously important to distinguish negated informa-tion from affirmative one (Kim and Park, 2006).
Itmay also improve automatic indexing (Mutalik etal., 2001).
In sentiment analysis, detecting and deal-ing with negation is critical, as it may change thepolarity of a text (Wiegand et al, 2010).
How-ever, research on negation has mainly focused on thebiomedical domain, and addressed the problem ofdetecting if a medical term is negated or not (Chap-man et al, 2001), or the scope of different negationsignals (Morante et al, 2008).During the last years, the importance of process-ing negation is gaining recognition by the NLP re-search community, as evidenced by the success ofseveral initiatives such as the Negation and Spec-ulation in Natural Language Processing workshop(NeSp-NLP 2010)1 or the CoNLL-2010 SharedTask2, which aimed at identifying hedges and theirscope in natural language texts.
In spite of this, mostof the approaches proposed so far deal with negationin a superficial manner.This paper describes our contribution to the*SEM Shared Task 2012 on Resolving the Scopeand Focus of Negation.
As its name suggests, thetask aims at detecting the scope and focus of nega-tion, as a means of encouraging research in negationprocessing.
In particular, we participate in Task 1:scope detection.
For each negation in the text, thenegation cue must be detected, and its scope marked.Moreover, the event or property that is negated mustbe recognized.
A comprehensive description of thetask may be found in (Morante and Blanco, 2012).For the sake of clarity, it is important to definewhat the organization of the task understands bynegation cue, scope of negation and negated event.The words that express negation are called negationcues.
Not and no are common examples of suchcues.
Scope is defined as the part of the mean-ing that is negated, and encloses all negated con-cepts.
The negated event is the property that is1http://www.clips.ua.ac.be/NeSpNLP2010/2www.inf.u-szeged.hu/rgai/conll2010st/282negated by the cue.
For instance, in the sentence:[Holmes] did not [say anything], the scope is en-closed in square brackets, the negation cue is under-lined and the negated event is shown in bold.
Moredetails about the annotation of negation cues, scopesand negated events may be found in (Morante andDaelemans, 2012).The system presented to the shared task is anadaptation of the one published in (Carrillo de Al-bornoz et al, 2010), whose aim was to detect andprocess negation in opinionated text in order to im-prove polarity and intensity classification.
Whenclassifying sentiments and opinions it is importantto deal with the presence of negations and their ef-fect on the emotional meaning of the text affected bythem.
Consider the sentence (1) and (2).
Sentence(1) expresses a positive opinion, whereas that in sen-tence (2) the negation word not reverses the polarityof such opinion.
(1) I liked this hotel.
(2) I didn?t like this hotel.Our system has the main advantage of being sim-ple and highly generic.
Even though it was origi-nally conceived for treating negations in opinionatedtexts, a few simple modifications have been suffi-cient to successfully address negation in a very dif-ferent type of texts, such as Conan Doyle stories.
Itis rule-based and does not need to be trained.
It alsouses semantic information in order to automaticallydetect the negation cues.2 MethodologyAs already told, the UCM-I system is a modified ver-sion of the one presented in (Carrillo de Albornozet al, 2010).
Next sections detail the modificationsperformed to undertake the present task.2.1 Detecting negation cuesOur previous work was focused on explicit nega-tions (i.e., those introduced by negation tokens suchas not, never).
In contrast, in the present workwe also consider what we call implicit negations,which includes affixal negation (i.,e., words withprefixes such as dis-, un- or suffixes such as -less;e.g., impatient or careless), inffixal negation (i.e.,pointlessness, where the negation cue less is in themiddle of the noun phrase).
Note that we did notTable 1: Examples of negation cues.Explicit negation cuesno not non nornobody never nowhere ...Words with implicit negation cuesunpleasant unnatural dislike impatientfearless hopeless illegal ...have into account these negation cues when ana-lyzing opinionated texts because these words them-selves usually appear in affective lexicons with theircorresponding polarity values (i.e., impatient, for in-stance, appears in SentiWordNet with a negative po-larity value).In order to detect negation cues, we use a list ofpredefined negation signals, along with an automaticmethod for detecting new ones.
The list has beenextracted from different previous works (Councill etal., 2010; Morante, 2010).
This list also includes themost frequent contracted forms (e.g., don?t, didn?t,etc.).
The automated method, in turn, is intendedfor discovering in text new affixal negation cues.
Tothis end, we first find in the text all words with pre-fixes dis-, a-, un-, in-, im-, non-, il-, ir- and the suf-fix -less that present the appropriate part of speech.Since not all words with such affixes are negationcues, we use semantic information from WordNetconcepts and relations to decide.
In this way, we re-trieve from WordNet the synset that correspond toeach word, using WordNet::SenseRelate (Patward-han et al, 2005) to correctly disambiguate the mean-ing of the word according to its context, along withall its antonym synsets.
We next check if, after re-moving the affix, the word exists in WordNet andbelongs to any of the antonym synsets.
If so, weconsider the original word to be a negation cue (i.e.,the word without the affix has the opposite meaningthan the lexical item with the affix).Table 1 presents some examples of explicit nega-tion cues and words with implicit negation cues.
Forspace reasons, not all cues are shown.
We also con-sider common spelling errors such as the omissionof apostrophes (e.g., isnt or nt).
They are not likelyto be found in literary texts, but are quite frequent inuser-generated content.This general processing is, however, improvedwith two rules:283Table 2: Examples of false negation cues.no doubt without a doubt not merely not justnot even not only no wonder ...1.
False negation cues: Some negation wordsmay be also used in other expressions with-out constituting a negation, as in sentence (3).Therefore, when the negation token belongsto such expressions, this is not processed as anegation.
Examples of false negation cues areshown in Table 2.
(3) ... the evidence may implicate not only yourfriend Mr. Stapleton but his wife as well.2.
Tag questions: Some sentences in the cor-pora present negative tag questions in old En-glish grammatical form, as it may shown insentences (4) and (5).
We have implemented aspecific rule to deal with this type of construc-tions, so that they are not treated as negations.
(4) You could easily recognize it , could you not?.
(5) But your family have been with us for severalgenerations , have they not?2.2 Delimiting the scope of negationThe scope of a negation is determined by using thesyntax tree of the sentence in which the negationarises, as generated by the Stanford Parser.3 To thisend, we find in the syntax tree the first common an-cestor that encloses the negation token and the wordimmediately after it, and assume all descendant leafnodes to the right of the negation token to be af-fected by it.
This process may be seen in Figure1, where the syntax tree for the sentence: [Watsondid] not [solve the case] is shown.
In this sentence,the method identifies the negation token not and as-sumes its scope to be all descendant leaf nodes of thecommon ancestor of the words not and solve (i.e.,solve the case).This modeling has the main advantage of beinghighly generic, as it serves to delimit the scope ofnegation regardless of what the negated event is (i.e.,the verb, the subject, the object of the verb, an ad-jective or an adverb).
As shown in (Carrillo de Al-3http://nlp.stanford.edu/software/lex-parser.shtmlFigure 1: Syntax tree of the sentence: Watson did notsolve the case.bornoz et al, 2010), it behaves well when determin-ing the scope of negation for the purpose of classi-fying product reviews in polarity classes.
However,we have found that this scope is not enough for thepresent task, and thus we have implemented a set ofpost-processing rules to expand and limit the scopeaccording to the task guidelines:1.
Expansion to subject.
This rule expands thenegation scope in order to include the subject ofthe sentence within it.
In this way, in sentence(6) the appropriate rule is fired to include ?Thistheory?
within the negation scope.
(6) [This theory would] not [work].It must be noted that, for polarity classifica-tion purposes, we do not consider the subjectof the sentence to be part of this scope.
Con-sider, for instance, the sentence: The beauti-ful views of the Eiffel Tower are not guaranteedin all rooms.
According to traditional polarityclassification approaches, if the subject is con-sidered as part of the negation scope, the polar-ity of the positive polar expression ?beautiful?should be changed, and considered as negative.2.
Subordinate boundaries.
Our original nega-tion scope detection method works well withcoordinate sentences, in which negation cuesscope only over their clause, as if a ?boundary?exists between the different clauses.
This oc-curs, for instance, in the sentence:284Table 3: List of negation scope delimiters.Tokens POSso, because, if, whileINuntil, since, unlessbefore, than, despite INwhat, whose WPwhy, where WRBhowever RB?,?, - , :, ;, (, ), !, ?, .
-(7) [It may be that you are] not [yourself lumi-nous], but you are a conductor of light.It also works properly in subordinate sentences,when the negation occurs in the subordinateclause, as in: You can imagine my surprisewhen I found that [there was] no [one there].However, it may fail in some types of subor-dinate sentences, where the scope should belimited to the main clause, but our model pre-dict both clauses to be affected by the negation.This is the case for the sentences where the de-pendent clause is introduced by the subordinateconjunctions in Table 3.
An example of suchtype of sentence is (8), where the conjunctiontoken because introduces a subordinate clausewhich is out of the negation scope.
To solve thisproblem, the negation scope detection methodincludes a set of rules to delimit the scope inthose cases, using as delimiters the conjunc-tions in Table 3.
Note that, since some of thesedelimiters are ambiguous, their part of speechtags are used to disambiguate them.
(8) [Her father] refused [to have anything to dowith her] because she had married without hisconsent.3.
Prepositional phrases: Our original methodalso fails to correctly determine the negationscope when the negated event is followed bya prepositional phrase, as it may be seen inFigure 2, where the syntax tree for the sen-tence: [There was] no [attempt at robbery] isshown.
Note that, according to our originalmodel, the phrase ?at robbery?
does not belongto the negation scope.
This is an error that wasnot detected before, but has been fixed for thepresent task.Figure 2: Syntax tree for the sentence: There was no at-tempt at robbery.2.3 Finding negated eventsWe only consider a single type of negated events,so that, when a cue word contains a negative affix,the word after removing the affix is annotated as thenegated event.
In this way, ?doubtedly?
is correctlyannotated as the negated event in sentence (9).
How-ever, the remaining types of negated events are rele-gated to future work.
(9) [The oval seal is] undoubtedly [a plainsleeve-link].3 Evaluation SetupThe data collection consists of a development set, atraining set, and two test sets of 787, 3644, 496 and593 sentences, respectively from different stories byConan Doyle (see (Morante and Blanco, 2012) fordetails).
Performance is measured in terms of recall,precision and F-measure for the following subtasks:?
Predicting negation cues.?
Predicting both the scope and cue.?
Predicting the scope, the cue does not need tobe correct.?
Predicting the scope tokens, where not a fullscope match is required.?
Predicting negated events.?
Full evaluation, which requires all elements tobe correct.285Table 4: Results for the development set.Metric Pr.
Re.
F-1Cues 92.55 86.13 89.22Scope (cue match) 86.05 44.05 58.27Scope (no cue match) 86.05 44.05 58.27Scope tokens (no cue match) 88.05 59.05 70.69Negated (no cue match) 65.00 10.74 18.43Full negation 74.47 20.23 31.824 Evaluation ResultsThe results of our system when evaluated on the de-velopment set and the two test sets (both jointly andseparately), are shown in Tables 4, 5, and 6.It may be seen from these tables that our sys-tem behaves quite well in the prediction of negationcues subtask, achieving around 90% F-measure inall data sets, and the second position in the com-petition.
Performance in the scope prediction task,however, is around 60% F-1, and the same resultsare obtained if the correct prediction of cues is re-quired (Scope (cue match)).
This seems to indicatethat, for all correct scope predictions, our systemhave also predicted the negation cues correctly.
Ob-viously these results improve for the Scope tokensmeasure, achieving more than 77% F-1 for the Card-board data set.
We also got the second position inthe competition for these three subtasks.
Concerningdetection of negated events, our system gets poor re-sults, 22.85% and 19.81% F-1, respectively, in eachtest data set.
These results affect the performanceof the full negation prediction task, where we get32.18% and 32.96% F-1, respectively.
Surprisingly,the result in the test sets are slightly better than thosein the development set, and this is due to a better be-havior of the WordNet-based cue detection methodin the formers than in the later.5 DiscussionWe next discuss and analyze the results above.Firstly, and regarding detection of negation cues, ourinitial list covers all explicit negations in the devel-opment set, while the detection of affixal negationcues using our WordNet-based method presents aprecision of 100% but a recall of 53%.
In particu-lar, our method fails when discovering negation cuessuch as unburned, uncommonly or irreproachable,where the word after removing the affix is a derivedform of a verb or adjective.Secondly, and concerning delimitation of thescope, our method behaves considerably well.
Wehave found that it correctly annotates the negationscope when the negation affects the predicate thatexpresses the event, but sometimes fails to includethe subject of the sentence in such scope, as in:[I know absolutely] nothing [about the fate of thisman], where our method only recognizes as thenegation scope the terms about the fate of this man.The results have also shown that the method fre-quently fails when the subject of the sentence or theobject of an event are negated.
This occurs, forinstance, in sentences: I think, Watson, [a brandyand soda would do him] no [harm] and No [womanwould ever send a reply-paid telegram], where weonly point to ?harm?
and ?woman?
as the scopes.We have found a further category of errors in thescope detection tasks, which concern some typesof complex sentences with subordinate conjunctionswhere our method limits the negation scope to themain clause, as in sentence: [Where they came from,or who they are,] nobody [has an idea] , where ourmethod limits the scope to ?has an idea?.
However,if the negation cue occurs in the subordinate clause,the method behaves correctly.Thirdly, with respect to negated event detection,as already told our method gets quite poor results.This was expected, since our system was not orig-inally designed to face this task and thus it onlycovers one type of negated events.
Specifically,it correctly identifies the negated events for sen-tences with affixal negation cues, as in: It is mostimproper, most outrageous, where the negated eventis ?proper?.
However, it usually fails to identifythese events when the negation affects the subjectof the sentence or the object of an event.6 Conclusions and Future WorkThis paper presents one of the two contributionsfrom the Universidad Complutense de Madrid to the*SEM Shared Task 2012.
The results have shownthat our method successes in identifying negationcues and performs reasonably well when determin-ing the negation scope, which seems to indicate thata simple unsupervised method based on syntactic in-formation and a reduced set of post-processing rules286Table 5: Results for the test sets (jointly).Metric Gold System Tp Fp Fn Precision Recall F-1Cues 264 278 241 29 23 89.26 91.29 90.26Scopes (cue match) 249 254 116 24 133 82.86 46.59 59.64Scopes (no cue match) 249 254 116 24 133 82.86 46.59 59.64Scope tokens (no cue match) 1805 1449 1237 212 568 85.37 68.53 76.03Negated (no cue match) 173 33 22 11 151 66.67 12.72 21.36Full negation 264 278 57 29 207 66.28 21.59 32.57Table 6: Results for the Cardboard and Circle test sets.MetricCardboard set Circle setPr.
Re.
F-1 Pr.
Re.
F-1Cues 90.23 90.23 90.23 88.32 92.37 90.30Scope (cue match) 83.33 46.88 60.00 82.35 46.28 59.26Scope (no cue match) 83.33 46.88 60.00 82.35 46.28 59.26Scope tokens (no cue match) 84.91 72.08 77.97 85.96 64.50 73.70Negated (no cue match) 66.67 13.79 22.85 66.67 11.63 19.81Full negation 68.29 21.05 32.18 64.44 22.14 32.96is a viable approach for dealing with negation.
How-ever, detection of negated events is the main weak-ness of our approach, and this should be tackled infuture work.
We also plan to improve our methodfor detecting affixal negations to increment its recall,by using further WordNet relations such as ?derivedfrom adjective?, and ?pertains to noun?, as well asto extend this method to detect infixal negations.AcknowledgmentsThis research is funded by the Spanish Ministry ofScience and Innovation (TIN2009-14659-C03-01)and the Ministry of Education (FPU program).ReferencesJorge Carrillo de Albornoz, Laura Plaza, and PabloGerva?s.
2010.
A hybrid approach to emotional sen-tence polarity and intensity classification.
In Proceed-ings of the 14th Conference on Computational NaturalLanguage Learning (CoNLL 2010), pages 153?161.W.
W. Chapman, W. Bridewell, P. Hanbury, G. F. Cooper,and B.G.
Buchanan.
2001.
A simple algorithm foridentifying negated findings and diseases in dischargesummaries.
J Biomed Inform, 34:301?310.Isaac Councill, Ryan McDonald, and Leonid Velikovich.2010.
What?s great and what?s not: learning to classifythe scope of negation for improved sentiment analysis.In Proceedings of the Workshop on Negation and Spec-ulation in Natural Language Processing, pages 51?59.Jung-Jae Kim and Jong C. Park.
2006.
Extracting con-trastive information from negation patterns in biomed-ical literature.
ACM Trans.
on Asian Language Infor-mation Processing, 5(1):44?60.Roser Morante and Eduardo Blanco.
2012.
Sem 2012shared task: Resolving the scope and focus of nega-tion.
In Proceedings of the 1st Joint Conference onLexical and Computational Semantics (*SEM 2012).Roser Morante and Walter Daelemans.
2012.Conandoyle-neg: Annotation of negation in conandoyle stories.
In Proceedings of the 8th InternationalConference on Language Resources and Evaluation.Roser Morante, Anthony Liekens, and Walter Daele-mans.
2008.
Learning the scope of negation inbiomedical texts.
In Proceedings of the 2008 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 715?724.Roser Morante.
2010.
Descriptive Analysis of NegationCues in Biomedical Texts.
In Proceedings of the 7thInternational Conference on Language Resources andEvaluation.A.G.
Mutalik, A. Deshpande, and P.M. Nadkarni.
2001.Use of general-purpose negation detection to augmentconcept indexing of medical documents.
A quantita-tive study using the UMLS.
J Am Med Inform Assoc,8(6):598?609.Siddharth Patwardhan, Satanjeev Banerjee, and Ted Ped-ersen.
2005.
SenseRelate::TargetWord: a generalizedframework for word sense disambiguation.
In Pro-ceedings of the ACL 2005 on Interactive poster anddemonstration sessions, pages 73?76.Michael Wiegand, Alexandra Balahur, Benjamin Roth,Dietrich Klakow, and Andre?s Montoyo.
2010.
A sur-vey on the role of negation in sentiment analysis.
InProceedings of the Workshop on Negation and Specu-lation in Natural Language Processing, pages 60?68.287
