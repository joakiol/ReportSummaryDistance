Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 833?838,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsImproving Learning and Inference in a Large Knowledge-baseusing Latent Syntactic CuesMatt Gardner, Partha Pratim Talukdar, Bryan Kisiel, and Tom MitchellCarnegie Mellon University5000 Forbes AvenuePittsburgh, PA 15213, USA{mg1,ppt,bkisiel,tom.mitchell}@cs.cmu.eduAbstractAutomatically constructed Knowledge Bases(KBs) are often incomplete and there is a gen-uine need to improve their coverage.
PathRanking Algorithm (PRA) is a recently pro-posed method which aims to improve KB cov-erage by performing inference directly overthe KB graph.
For the first time, we demon-strate that addition of edges labeled with la-tent features mined from a large dependencyparsed corpus of 500 million Web documentscan significantly outperform previous PRA-based approaches on the KB inference task.We present extensive experimental results val-idating this finding.
The resources presentedin this paper are publicly available.1 IntroductionOver the last few years, several large scale Knowl-edge Bases (KBs) such as Freebase (Bollacker etal., 2008), NELL (Carlson et al 2010), and YAGO(Suchanek et al 2007) have been developed.
Eachsuch KB consists of millions of facts (e.g., (TigerWoods, playsSport, Golf )) spanning over multiplerelations.
Unfortunately, these KBs are often incom-plete and there is a need to increase their coverage offacts to make them useful in practical applications.A strategy to increase coverage might be to per-form inference directly over the KB represented as agraph.
For example, if the KB contained the follow-ing facts, (Tiger Woods, participatesIn, PGA Tour))and (Golf, sportOfTournament, PGA Tour), then byputting these two facts together, we could potentiallyinfer that (Tiger Woods, playsSport, Golf ).
TheFigure 1: Example demonstrating how lexicalized syn-tactic edges can improve connectivity in the KB enablingPRA (Lao and Cohen, 2010) to discover relationships be-tween Alex Rodriguez and World Series.
Edges with la-tent labels can improve inference performance by reduc-ing data sparsity.
See Section 1.1 for details.recently proposed Path Ranking Algorithm (PRA)(Lao and Cohen, 2010) performs such inference byautomatically learning semantic inference rules overthe KB (Lao et al 2011).
PRA uses features basedoff of sequences of edge types, e.g., ?playsSport,sportOfTournament?, to predict missing facts in theKB.PRA was extended by (Lao et al 2012) to per-form inference over a KB augmented with depen-dency parsed sentences.
While this opens up thepossibility of learning syntactic-semantic inferencerules, the set of syntactic edge labels used arejust the unlexicalized dependency role labels (e.g.,nobj, dobj, etc., without the corresponding words),thereby limiting overall expressitivity of the learnedinference rules.
To overcome this limitation, in thispaper we augment the KB graph by adding edgeswith more expressive lexicalized syntactic labels(where the labels are words instead of dependen-833cies).
These additional edges, e.g., (Alex Rodriguez,?plays for?, NY Yankees), are mined by extracting600 million Subject-Verb-Object (SVO) triples froma large corpus of 500m dependency parsed docu-ments, which would have been prohibitively expen-sive to add directly as in (Lao et al 2012).
In orderto overcome the explosion of path features and datasparsity, we derive edge labels by learning latent em-beddings of the lexicalized edges.
Through exten-sive experiments on real world datasets, we demon-strate effectiveness of the proposed approach.1.1 Motivating ExampleIn Figure 1, the KB graph (only solid edges) is dis-connected, thereby making it impossible for PRA todiscover any relationship between Alex Rodriguezand World Series.
However, addition of the twoedges with SVO-based lexicalized syntactic edges(e.g., (Alex Rodriguez, plays for, NY Yankees)) re-stores this inference possibility.
For example, PRAmight use the edge sequence ?
?plays for?, team-PlaysIn?
as evidence for predicting the relation in-stance (Alex Rodriguez, athleteWonChampionship,World Series).
Unfortunately, such na?
?ve additionof lexicalized edges may result in significant datasparsity, which can be overcome by mapping lexi-calized edge labels to some latent embedding (e.g.,(Alex Rodriguez, LatentFeat#5, NY Yankees) andrunning PRA over this augmented graph.
Using la-tent embeddings, PRA could then use the followingedge sequence as a feature in its prediction models:?LatentFeat#5, teamPlaysIn?.
We find this strategyto be very effective as described in Section 4.2 Related WorkThere is a long history of methods using suface-levellexical patterns for extracting relational facts fromtext corpora (Hearst, 1992; Brin, 1999; Agichteinand Gravano, 2000; Ravichandran and Hovy, 2002;Etzioni et al 2004).
Syntactic information in theform of dependency paths have been explored in(Snow et al 2006; Suchanek et al 2006).
Amethod of latent embedding of relation instancesfor sentence-level relation extraction was shown in(Wang et al 2011).
However, none of this priorwork makes explicit use of the background KBs aswe explore in this paper.Path Ranking Algorithm (PRA) (Lao and Cohen,2010) has been used previously to perform inferenceover graph-structured KBs (Lao et al 2011), and tolearn formation of online communities (Settles andDow, 2013).
In (Lao et al 2012), PRA is extendedto perform inference over a KB using syntactic in-formation from parsed text.
In contrast to these pre-vious PRA-based approaches where all edge labelsare either KB labels or at surface-level, in this pa-per we explore using latent edge labels in additionto surface-level labels in the graph over which PRAis applied.
In particular, we focus on the problem ofperforming inference over a large KB and learn la-tent edge labels by mining dependency syntax statis-tics from a large text corpus.Though we use Principal Components Analysis(PCA) for dimensionality reduction for the experi-ments in this paper, this is by no means the onlychoice.
Various other dimensionality reduction tech-niques, and in particular, other verb clustering tech-niques (Korhonen et al 2003), may also be used.OpenIE systems such as Reverb (Etzioni et al2011) also extract verb-anchored dependency triplesfrom large text corpus.
In contrast to such ap-proaches, we focus on how latent embedding ofverbs in such triples can be combined with explicitbackground knowledge to improve coverage of ex-isting KBs.
This has the added capability of infer-ring facts which are not explicitly mentioned in text.The recently proposed Universal Schema (Riedelet al 2013) also demonstrates the benefit of us-ing latent features for increasing coverage of KBs.Key differences between that approach and ours in-clude our use of syntactic information as opposed tosurface-level patterns in theirs, and also the abilityof the proposed PRA-based method to generate use-ful inference rules which is beyond the capability ofthe matrix factorization approach in (Riedel et al2013).3 Method3.1 Path Ranking Algorithm (PRA)In this section, we present a brief overview of thePath Ranking Algorithm (PRA) (Lao and Cohen,2010), building on the notations in (Lao et al 2012).Let G = (V,E, T ) be the graph, where V is the setof vertices, E is the set of edges, and T is the set ofedge types.
For each edge (v1, t, v2) ?
E, we have834v1, v2 ?
V and t ?
T .
LetR ?
T be the set of typespredicted by PRA.
R could in principal equal T , butin this paper we restrict prediction to KB relations,while T also includes types derived from surface textand latent embeddings.
Let pi = ?t1, t2, .
.
.
, tw?
bea path type of length w over graph G, where ti ?
Tis the type of the ith edge in the path.
Each suchpath type is also a feature in the PRA model.
Fora given source and target node pair s, t ?
V , letP (s ?
t;pi) be the value of the feature pi specify-ing the probability of reaching node t starting fromnode s and following a path constrained by path typepi.
We approximate these probabilities using randomwalks.
A value of 0 indicates unreachability from sto t using path type pi.Let B = {pi1, .
.
.
, pim} be the set of all features(path types).
The score that relation r holds betweennode s and node t is given by the following function:ScorePRA(s, t, r) =?pi?BP (s?
t;pi) ?rpiwhere ?rpi is the weight of feature pi in class r ?
R.Feature Selection: The set B of possible pathtypes grows exponentially in the length of the pathsthat are considered.
In order to have a manageableset of features to compute, we first perform a featureselection step.
The goal of this step is to select forcomputation only those path types that commonlyconnect sources and targets of relation r. We per-form this feature selection by doing length-boundedrandom walks from a given list of source and tar-get nodes, keeping track of how frequently each pathtype leads from a source node to a target node.
Themost common m path types are selected for the setB.Training: We perform standard logistic regres-sion with L2 regularization to learn the weights ?rpi.We follow the strategy in (Lao and Cohen, 2010) togenerate positive and negative training instances.3.2 PRAsyntacticIn this section, we shall extend the knowledge graphG = (V,E, T ) from the previous section with anaugmented graph G?= (V,E?, T?
), where E ?
E?and T ?
T?, with the set of vertices unchanged.In order to get the edges in E??
E, we firstcollect a set of Subject-Verb-Object (SVO) triplesD = {(s, v, o, c)} from a large dependency parsedtext corpus, with c ?
R+ denoting the frequencyof this triple in the corpus.
The additional edgeset is then defined as Esyntactic = E??
E ={(s, v, o) | ?
(s, v, o, c) ?
D, s, o ?
V }.
We de-fine S = {v | ?
(s, v, o) ?
Esyntactic} and setT?= T ?
S. In other words, for each pair ofdirectly connected nodes in the KB graph G, we addan additional edge between those two nodes for eachverb which takes the NPs represented by two nodesas subjects and objects (or vice versa) as observed ina text corpus.
In Figure 1, (Alex Rodriguez, ?playsfor?, NY Yankees) is an example of such an edge.PRA is then applied over this augmented graphG?, over the same set of prediction types R as be-fore.
We shall refer to this version of PRA asPRAsyntactic.
For the experiments in this paper, wecollected |D| = 600 million SVO triples1 from theentire ClueWeb corpus (Callan et al 2009), parsedusing the Malt parser (Nivre et al 2007) by theHazy project (Kumar et al 2013).3.3 PRAlatentIn this section we construct G?
?= (V,E?
?, T??
),another syntactic-information-induced extension ofthe knowledge graph G, but instead of using the sur-face forms of verbs in S (see previous section) asedge types, we derive those edges types T?
?basedon latent embeddings of those verbs.
We note thatE ?
E?
?, and T ?
T?
?.In order to learn the latent or low dimensional em-beddings of the verbs in S, we first define QS ={(s, o) | ?
(s, v, o, c) ?
D, v ?
S}, the set ofsubject-object tuples in D which are connected byat least one verb in S. We now construct a matrixX|S|?|QS | whose entry Xv,q = c, where v ?
S, q =(s, o) ?
QS , and (s, v, o, c) ?
D. After row normal-izing and centering matrix X , we apply PCA on thismatrix.
Let A|S|?d with d << |QS | be the low di-mensional embeddings of the verbs in S as inducedby PCA.
We use two strategies to derive mappingsfor verbs from matrix A.?
PRAlatentc : The verb is mapped to concatena-tion of the k2 most positive columns in the rowin A that corresponds to the verb.
Similarly, forthe most negative k2 columns.1This data and other resources from the paper are publiclyavailable at http://rtw.ml.cmu.edu/emnlp2013 pra/.835Precision Recall F1PRA 0.800 0.331 0.468PRAsyntactic 0.804 0.271 0.405PRAlatentc 0.885 0.334 0.485PRAlatentd 0.868 0.424 0.570Table 1: Comparison of performance of different variantsof PRA micro averaged across 15 NELL relations.
Wefind that use of latent edge labels, in particular the pro-posed approach PRAlatentd , significantly outperformsother approaches.
This is our main result.
(See Section 4)?
PRAlatentd : The verb is mapped to disjunctionof top-k most positive and negative columns inthe row in A that corresponds to the verb.4 ExperimentsWe compared the various methods using 15 NELLrelations.
For each relation, we split NELL?s knownrelation instances into 90% training and 10% testing.For each method, we then selected 750 path featuresand trained the model, as described in Section 3, us-ing GraphChi (Kyrola et al 2012) to perform therandom walk graph computations.
To evaluate themodel, we took all source nodes in the testing dataand used the model to predict target nodes.
We re-port the precision and recall (on the set of known tar-get nodes) of the set of predictions for each modelthat are above a certain confidence threshold.
Be-cause we used strong regularization, we picked forour threshold a model score of 0.405, correspond-ing to 60% probability of the relation instance beingtrue; values higher than this left many relations with-out any predictions.
Table 1 contains the results.As can be seen in the table, PRAsyntactic on av-erage performs slightly worse than PRA.
Whilethe extra syntactic features are very informative forsome relations, they also introduce a lot of spar-sity, which makes the model perform worse on otherrelations.
When using latent factorization meth-ods to reduce the sparsity of the syntactic features,we see a significant improvement in performance.PRAlatentc has a 45% reduction in precision er-rors vs. PRA while maintaining the same recall,and PRAlatentd reduces precision errors by 35%while improving recall by 27%.
Section 4.1 con-tains some qualitative analysis of how sparsity is re-duced with the latent methods.
As a piece quanti-0 0.10.2 0.30.4 0.50.6 0.70.8 0.910.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1PRAPRAsyntacticPRAlatentcPRAlatentd0 0.10.2 0.30.4 0.50.6 0.70.8 0.910.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1PRAPRAsyntacticPRAlatentcPRAlatentdFigure 2: Precision (y axis) - Recall (x axis) plots for therelations cityLiesOnRiver (top) and athletePlaysForTeam(bottom).
PRAlatentd (rightmost plot), the proposed ap-proach which exploits latent edge labels, outperformsother alternatives.tative analysis, there were 908 possible path typesfound in the feature selection step with PRA on therelation cityLiesOnRiver (of which we then selected750).
For PRAsyntactic, there were 73,820, whilePRAlatentc had 47,554 and PRAlatentd had 58,414.Table 2 shows F1 scores for each model oneach relation, and Figure 2 shows representativePrecision-Recall plots for two NELL relations.
Inboth cases, we find that PRAlatentd significantlyoutperforms other baselines.4.1 DiscussionWhile examining the model weights for each ofthe methods, we saw a few occasions where sur-face relations and NELL relations combined to forminterpretable path types.
For example, in ath-letePlaysForTeam, some highly weighted featurestook the form of ?athletePlaysSport, ?
(sport) playedby (team)??.
A high weight on this feature wouldbias the prediction towards teams that are known toplay the same sport as the athlete.For PRA, the top features for the best performingrelations are path types that contain a single edge836PRA PRAsyntactic PRAlatentc PRAlatentdanimalIsTypeOfAnimal 0.52 0.50 0.47 0.53athletePlaysForTeam 0.22 0.21 0.56 0.64athletePlaysInLeague 0.81 0.75 0.73 0.74cityLiesOnRiver 0.05 0 0.07 0.31cityLocatedInCountry 0.15 0.20 0.45 0.55companyCeo 0.29 0.18 0.25 0.35countryHasCompanyOffice 0 0 0 0drugHasSideEffect 0.96 0.95 0.94 0.94headquarteredIn 0.31 0.11 0.41 0.64locationLocatedWithinLocation 0.40 0.38 0.38 0.41publicationJournalist 0.10 0.06 0.10 0.16roomCanContainFurniture 0.72 0.70 0.71 0.73stadiumLocatedInCity 0.53 0 0.13 0.67teamPlaysAgainstTeam 0.47 0.24 0.26 0.21writerWroteBook 0.59 0.62 0.73 0.80Table 2: F1 performance of different variants of PRA for all 15 relations tested.which is a supertype or subtype of the relation be-ing predicted.
For instance, for the relation ath-letePlaysForTeam (shown in Figure 2), the highest-weighted features in PRA are athleteLedSport-sTeam (more specific than athletePlaysForTeam)and personBelongsToOrganization (more generalthan athletePlaysForTeam).
For the same rela-tion, PRAsyntactic has features like ?scored for?,?signed?, ?have?, and ?led?.
When using a latentembedding of these verb phrases, ?signed?, ?have?,and ?led?
all have the same representation in the la-tent space, and so it seems clear that PRAlatent gainsa lot by reducing the sparsity inherent in using sur-face verb forms.For cityLiesOnRiver, where PRA does not per-form as well, there is no NELL relation that is an im-mediate supertype or subtype, and so PRA does nothave as much evidence to use.
It finds features that,e.g., are analogous to the statement ?cities in thesame state probably lie on the same river?.
Addinglexical labels gives the model edges to use like ?lieson?, ?runs through?, ?flows through?, ?starts in?and ?reaches?, and these features give a significantboost in performance to PRAsyntactic.
Once again,almost all of those verb phrases share the same latentembedding, and so PRAlatent gains another signifi-cant boost in performance by combining them into asingle feature.5 ConclusionIn this paper, we introduced the use of latent lexi-cal edge labels for PRA-based inference over knowl-edge bases.
We obtained such latent edge labelsby mining a large dependency parsed corpus of500 million web documents and performing PCAon the result.
Through extensive experiments onreal datasets, we demonstrated that the proposed ap-proach significantly outperforms previous state-of-the-art baselines.AcknowledgmentsWe thank William Cohen (CMU) for enlighteningconversations on topics discussed in this paper.
Wethank the ClueWeb project (CMU) and the HazyResearch Group (http://hazy.cs.wisc.edu/hazy/) fortheir generous help with data sets; and to the anony-mous reviewers for their constructive comments.This research has been supported in part by DARPA(under contract number FA8750-13-2-0005), andGoogle.
Any opinions, findings, conclusions andrecommendations expressed in this paper are the au-thors?
and do not necessarily reflect those of thesponsors.837ReferencesEugene Agichtein and Luis Gravano.
2000.
Snowball:Extracting relations from large plain-text collections.In Proceedings of the Fifth ACM conference on Digitallibraries.Kurt Bollacker, Colin Evans, Praveen Paritosh, TimSturge, and Jamie Taylor.
2008.
Freebase: a collabo-ratively created graph database for structuring humanknowledge.
In Proceedings of SIGMOD.Sergey Brin.
1999.
Extracting patterns and relationsfrom the world wide web.J.
Callan, M. Hoy, C. Yoo, and L. Zhao.
2009.Clueweb09 data set.
boston.lti.cs.cmu.edu.Andrew Carlson, Justin Betteridge, Bryan Kisiel, BurrSettles, Estevam R Hruschka Jr, and Tom M Mitchell.2010.
Toward an architecture for never-ending lan-guage learning.
In AAAI.Oren Etzioni, Michael Cafarella, Doug Downey, Stan-ley Kok, Ana-Maria Popescu, Tal Shaked, StephenSoderland, Daniel S Weld, and Alexander Yates.2004.
Web-scale information extraction in know-itall:(preliminary results).
In Proceedings of WWW.Oren Etzioni, Anthony Fader, Janara Christensen,Stephen Soderland, and Mausam Mausam.
2011.Open information extraction: The second generation.In Proceedings of IJCAI.Marti A Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proceedings of the14th conference on Computational Linguistics.Anna Korhonen, Yuval Krymolowski, and Zvika Marx.2003.
Clustering polysemic subcategorization framedistributions semantically.
In Proceedings of ACL.Arun Kumar, Feng Niu, and Christopher Re?.
2013.Hazy: making it easier to build and maintain big-dataanalytics.
Communications of the ACM, 56(3):40?49.Aapo Kyrola, Guy Blelloch, and Carlos Guestrin.
2012.Graphchi: Large-scale graph computation on just a pc.In Proceedings of the 10th USENIX Symposium on Op-erating Systems Design and Implementation (OSDI),pages 31?46.Ni Lao and William W Cohen.
2010.
Relational re-trieval using a combination of path-constrained ran-dom walks.
Machine learning, 81(1):53?67.Ni Lao, Tom Mitchell, and William W Cohen.
2011.Random walk inference and learning in a large scaleknowledge base.
In Proceedings of EMNLP.
Associa-tion for Computational Linguistics.Ni Lao, Amarnag Subramanya, Fernando Pereira, andWilliam W Cohen.
2012.
Reading the web withlearned syntactic-semantic inference rules.
In Pro-ceedings of EMNLP-CoNLL.J.
Nivre, J.
Hall, J. Nilsson, A. Chanev, G. Eryigit,S.
Ku?bler, S. Marinov, and E. Marsi.
2007.
Malt-parser: A language-independent system for data-driven dependency parsing.
Natural Language Engi-neering, 13(02).Deepak Ravichandran and Eduard Hovy.
2002.
Learningsurface text patterns for a question answering system.In Proceedings of ACL.Sebastian Riedel, Limin Yao, Andrew McCallum, andBenjamin M Marlin.
2013.
Relation extraction withmatrix factorization and universal schemas.
In Pro-ceedings of NAACL-HLT.Burr Settles and Steven Dow.
2013.
Let?s get together:the formation and success of online creative collabora-tions.
In Proceedings of CHI.Rion Snow, Daniel Jurafsky, and Andrew Y Ng.
2006.Semantic taxonomy induction from heterogenous evi-dence.
In Proceedings of ACL.Fabian M Suchanek, Georgiana Ifrim, and GerhardWeikum.
2006.
Combining linguistic and statisticalanalysis to extract relations from web documents.
InProceedings of KDD.Fabian M Suchanek, Gjergji Kasneci, and GerhardWeikum.
2007.
Yago: a core of semantic knowledge.In Proceedings of WWW.Chang Wang, James Fan, Aditya Kalyanpur, and DavidGondek.
2011.
Relation extraction with relation top-ics.
In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing, pages1426?1436.
Association for Computational Linguis-tics.838
