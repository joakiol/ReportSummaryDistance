Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 478?487,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsModified Distortion Matricesfor Phrase-Based Statistical Machine TranslationArianna Bisazza and Marcello FedericoFondazione Bruno KesslerTrento, Italy{bisazza,federico}@fbk.euAbstractThis paper presents a novel method to suggestlong word reorderings to a phrase-based SMTdecoder.
We address language pairs wherelong reordering concentrates on few patterns,and use fuzzy chunk-based rules to predictlikely reorderings for these phenomena.
Thenwe use reordered n-gram LMs to rank the re-sulting permutations and select the n-best fortranslation.
Finally we encode these reorder-ings by modifying selected entries of the dis-tortion cost matrix, on a per-sentence basis.In this way, we expand the search space by amuch finer degree than if we simply raised thedistortion limit.
The proposed techniques aretested on Arabic-English and German-Englishusing well-known SMT benchmarks.1 IntroductionDespite the large research effort devoted to the mod-eling of word reordering, this remains one of themain obstacles to the development of accurate SMTsystems for many language pairs.
On one hand, thephrase-based approach (PSMT) (Och, 2002; Zens etal., 2002; Koehn et al, 2003), with its shallow andloose modeling of linguistic equivalences, appearsas the most competitive choice for closely relatedlanguage pairs with similar clause structures, bothin terms of quality and of efficiency.
On the other,tree-based approaches (Wu, 1997; Yamada, 2002;Chiang, 2005) gain advantage, at the cost of highercomplexity and isomorphism assumptions, on lan-guage pairs with radically different word orders.Lying between these two extremes are languagepairs where most of the reordering happens locally,and where long reorderings can be isolated and de-scribed by a handful of linguistic rules.
Notableexamples are the family-unrelated Arabic-Englishand the related German-English language pairs.
In-terestingly, on these pairs, PSMT generally pre-vails over tree-based SMT1, producing overall high-quality outputs and isolated but critical reorderingerrors that undermine the global sentence meaning.Previous works on this type of language pairs havemostly focused on source reordering prior to trans-lation (Xia and McCord, 2004; Collins et al, 2005),or on sophisticated reordering models integrated intodecoding (Koehn et al, 2005; Al-Onaizan and Pap-ineni, 2006), achieving mixed results.
To merge thebest of both approaches ?
namely, access to rich con-text in the former and natural coupling of reorder-ing and translation decisions in the latter ?
we intro-ducemodified distortion matrices: a novel method toseamlessly provide to the decoder a set of likely longreorderings pre-computed for a given input sentence.Added to the usual space of local permutations de-fined by a low distortion limit (DL), this results in alinguistically informed definition of the search spacethat simplifies the task of the in-decoder reorderingmodel, besides decreasing its complexity.The paper is organized as follows.
After review-ing a selection of relevant works, we analyze salientreordering patterns in Arabic-English and German-English, and describe the corresponding chunk-based reordering rule sets.
In the following sectionswe present a reordering selection technique based on1A good comparison of phrase-based and tree-based ap-proaches across language pairs with different reordering levelscan be found in (Zollmann et al, 2008).478reordered n-gram LMs and, finally, explain the no-tion of modified distortion matrices.
In the last partof the paper, we evaluate the proposed techniques ontwo popular MT tasks.2 Previous workPre-processing approaches to word reordering aimat permuting input words in a way that minimizesthe reordering needed for translation: determinis-tic reordering aims at finding a single optimal re-ordering for each input sentence, which is thentranslated monotonically (Xia and McCord, 2004)or with a low DL (Collins et al, 2005; Habash,2007); non-deterministic reordering encodes mul-tiple alternative reorderings into a word lattice andlets a monotonic decoder find the best path accord-ing to its models (Zhang et al, 2007; Crego andHabash, 2008; Elming and Habash, 2009; Niehuesand Kolss, 2009).
The latter approaches are ideallyconceived as alternative to in-decoding reordering,and therefore require an exhaustive reordering ruleset.
Two recent works (Bisazza and Federico, 2010;Andreas et al, 2011) opt instead for a hybrid way:rules are used to generate multiple likely reorder-ings, but only for a specific phenomenon ?
namelyverb-initial clauses in Arabic.
This yields sparse re-ordering lattices that can be translated with a regulardecoder performing additional reordering.Reordering rules for pre-processing are eithermanually written (Collins et al, 2005) or automat-ically learned from syntactic parses (Xia and Mc-Cord, 2004; Habash, 2007; Elming and Habash,2009), shallow syntax chunks (Zhang et al, 2007;Crego and Habash, 2008) or part-of-speech labels(Niehues and Kolss, 2009).
Similarly to hybrid ap-proaches, in this work we use few linguistically in-formed rules to generate multiple reorderings for se-lected phenomena but, as a difference, we do notemploy lattices to represent them.
We also include acompetitive in-decoding reordering model in all thesystems used to evaluate our methods.Another large body of work is devoted to the mod-eling of reordering decisions inside decoding, basedon a decomposition of the problem into a sequenceof basic reordering steps.
Existing approaches rangefrom basic linear distortion to more complex modelsthat are conditioned on the words being translated.The linear distortion model (Koehn et al, 2003)encourages monotonic translations by penalizingsource position jumps proportionally to their length.If used alone, this model is inadequate for languagepairs with different word orders.
Green et al (2010)tried to improve it with a future distortion cost es-timate.
Thus they were able to preserve baselineperformance at a very high DL, but not to improveit.
Lexicalized phrase orientation models (Tillmann,2004; Koehn et al, 2005; Zens and Ney, 2006; Gal-ley and Manning, 2008) predict the orientation of aphrase with respect to the last translated one.
Thesemodels are known to well handle local reorderingand are widely adopted by the PSMT community.However, they are unsuitable to model long reorder-ing as they classify as ?discontinuous?
every phrasethat does not immediately follow or precede the lasttranslated one.
Lexicalized distortion models pre-dict the jump from the last translated word to thenext one, with a class for each possible jump length(Al-Onaizan and Papineni, 2006), or bin of lengths(Green et al, 2010).
These models are conceived todeal with long reordering, but can easily suffer fromdata sparseness, especially for longer jumps occur-ring less frequently.Following a typical sequence modeling approach,Feng et al (2010) train n-gram language models onsource data previously reordered in accordance tothe target language translation.
This method doesnot directly model reordering decisions, but ratherword sequences produced by them.
Despite theirhigh perplexities, reordered LMs yield some im-provements when integrated to a PSMT baseline thatalready includes a discriminative phrase orientationmodel (Zens and Ney, 2006).
In this work we usesimilar models to rank sets of chunk permutations.Attempting to improve the reordering space def-inition, Yahyaei and Monz (2010) train a classifierto guess the most likely jump length at each sourceposition, then use its predictions to dynamically setthe DL.
Translation improvements are obtained on asimple task with mostly short sentences (BTEC).Modifying the distortion function, as proposed inthis paper, makes it possible to expand the pemuta-tion search space by a much finer degree than vary-ing the DL does.4793 Long reordering patternsOur study focuses on Arabic-English and German-English: two language pairs characterized by unevendistributions of word-reordering phenomena, withlong-range movements concentrating on few pat-terns.
In Arabic-English, the internal order of mostnoun phrases needs to be reversed during translation,which is generally well handled by phrase-internalreordering or local distortion.
At the constituentlevel, instead, Arabic admits both SV(O) and VS(O)orders, the latter causing problematic long reorder-ings.
Common errors due to this issue are the ab-sence of main verb in the English translation, or theplacement of the main verb before its own subject.In both cases, adequacy is seriously compromised.In German-English, the noun phrase structure issimilar between source and target languages.
How-ever, at the constituent level, the verb-second orderof German main clauses conflicts with the rigid SVOstructure of English, as does the clause-final verbposition of German subordinate clauses.
As a fur-ther complication, German compound verbs are splitapart so that the non-finite element (main verb) canappear long after the inflected auxiliary or modal.Thanks to sophisticated reordering models, state-of-the-art PSMT systems are generally good at han-dling local reordering phenomena that are not cap-tured by phrase-internal reordering.
However, theytypically fail to predict long reorderings.
We believethis is mainly not the fault of the reordering mod-els, but rather of a too coarse definition of the searchspace.
To have a concrete idea, consider that a smallchange of the DL from 5 to 6 words, in a sentenceof 8, makes the number of explorable permutationsincrease from about 9,000 to 22,000.
Existing mod-els cannot be powerful enough to deal with such arapidly growing search space.As a result, decoding at very high DLs is nota good solution for these language pairs.
Indeed,decent performances are obtained within a low ormediumDL, but this obviously comes at the expenseof long reorderings, which are often crucial to pre-serve the general meaning of a translated sentence.For instance, taking English as the target language,it is precisely the relative positioning of predicate ar-guments that determines their role, in the absence ofcase markers.
Thus, a wrongly reordered verb withminor impact on automatic scores, can be judgedvery badly by a human evaluator.We will now describe two rule sets aimed at cap-turing these reordering phenomena.4 Shallow syntax reordering rulesTo compute the source reorderings, we use chunk-based rules following Bisazza and Federico (2010).Shallow syntax chunking is indeed a lighter andsimpler task compared to full parsing, and it canbe used to constrain the number of reorderings ina softer way.
While rules based on full parsesare generally deterministic, chunk-based rules arenon-deterministic or fuzzy, as they generate sev-eral permutations for each matching sequence2.
Be-sides defining a unique segmentation of the sen-tence, chunk annotation provides other useful infor-mation that can be used by the rules ?
namely chunktype and POS tags3.For Arabic-English we apply the rules proposedby Bisazza and Federico (2010) aimed at transform-ing VS(O) sentences into SV(O).
Reorderings aregenerated by moving each verb chunk (VC), aloneor with its following chunk, by 1 to 6 chunks to theright.
The maximum movement of each VC is lim-ited to the position of the next VC, so that neigh-boring verb-reordering sequences may not overlap.This rule set was shown to cover most (99.5%) ofthe verb reorderings observed in a parallel news cor-pus, including those where the verb must be movedalong with an adverbial or a complement.For German-English we propose a set of threerules4 aimed at arranging the German constituentsin SVO order:?
infinitive: move each infinitive VC right after apreceding punctuation;?
subordinate: if a VC is immediately followedby a punctuation, place it after a preceding sub-ordinating conjunction (KOUS) or substitutiverelative pronoun (PRELS);2Chunk annotation does not identify subject and comple-ment boundaries, nor the relations among constituents that areneeded to deterministically rearrange a sentence in SVO order.3We use AMIRA (Diab et al, 2004) to annotate Arabic andTree Tagger (Schmid, 1994) to annotate German.4A similar rule set was previously used to produce chunkreordering lattices in (Hardmeier et al, 2010).480?????????????????
???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
?????????????????????
??????????????????????????????????????????????????????????????????????????
???????????????????????????????????????????????????????????????????????????????????
??????
???????
?????????????????????????
???????????????????????????????????????????
???
?
???
?
?
?
?
?
??
?
??
???
?
??
??
?
?
?
??
???
?
?
??
???
??
?
?
???
?
?
??
?
?
???
??
?
?
?
?
???
???
???
???
?
??
???
???
???
???
???
???
?
(a) Arabic VS(O) clause: five permutations????
?
???
?????
????
?????
??
?
?
?
?
??
?
?
?
?
??
?
?
?
?
?
???
????
????
????
??
?
????
?
?
?
?
?
???
?
???
?????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
??
???
??
????????????????????
??????????
??????????????????????????????????????????????????????????????????
(b) German broken verb chunk: three permutationsFigure 1: Examples of chunk permutations generated by shallow syntax reordering rules.
Chunk types: CC conjunc-tion, VC verb (auxiliary/past participle), PC preposition, NC noun, Pct punctuation.?
broken verb chunk: join each finite VC (auxil-iary or modal) with the nearest following non-finite VC (infinitive or participle).
Place the re-sulting block in any position between the orig-inal position of the finite verb and that of thenon-finite verb5.The application of chunk reordering rules is illus-trated by Fig.
1: in the Arabic sentence (a), the sub-ject ?dozens of militants?
is preceded by the mainverb ?took part?
and its argument ?to the march?.
Therules generate 5 permutations for one matching se-quence (chunks 2 to 5), out of which the 5th is thebest for translation.
The German sentence (b) con-tains a broken VC with the inflected auxiliary ?has?separated from the past participle ?initiated?.
Here,the rules generate 3 permutations for the chunk se-quence 2 to 5, corresponding to likely locations ofthe merged verb phrase, the 1st being optimal.By construction, both rule sets generate a limitednumber of permutations per matching sequence: in5To bound the number of reorderings, we use the follow-ing heuristics.
In ?infinitive?
at most 3 punctuations precedingthe VC are considered.
In ?subordinate?
1 to 3 chunks are leftbetween the conjunction (or pronoun) and the moved VC to ac-count for the subject.
In ?broken VC?
if the distance between thefinite and non-finite verb is more than 10 chunks, only the first5 and last 5 positions of the verb-to-verb span are considered.Arabic at most 12 for each VC; in German at most 3for each infinitive VC and for each VC-punctuationsequence, at most 10 for each broken VC.
Empiri-cally, this yields on average 22 reorderings per sen-tence in the NIST-MT Arabic benchmark dev06-NWand 3 on the WMT German benchmark test08.
Ara-bic rules are indeed more noisy, which is not surpris-ing as reordering is triggered by any verb chunk.5 Reordering selectionThe number of chunk-based reorderings per sen-tence varies according to the rule set, to the size ofchunks and to the context.
A high degree of fuzzi-ness can complicate the decoding process, leavingtoo much work to the in-decoding reordering model.A solution to this problem is using an external modelto score the rule-generated reorderings and discardthe less probable ones.
In such a way, a further partof reordering complexity is taken out of decoding.At this end, instead of using a Support Vector Ma-chine classifier as was done in (Bisazza et al, 2011),we apply reordered n-gram models that are lighter-weight and more suitable for a ranking task.Differently from Feng et al (2010), we train ourmodels on partially reordered data and at the level ofchunks.
Chunks can be represented simply by their481type label (such as VC or NC), but also by a com-bination of the type and head word, to obtain finerlexicalized distributions.
LMs trained on differentchunk representations can also be applied jointly, bylog-linear combination.We perform reordering selection as follows:1.
Chunk-based reordering rules are applied de-terministically to the source side of the paralleltraining data, using word alignment to choosethe optimal permutation (?oracle reordering?)6.2.
One or several chunk-level 5-gram LMs aretrained on such reordered data, using differentchunk representation modes.3.
Reordering rules are applied to the test sen-tences and the resulting sets of rule-matchingsequence permutations are scored by the LMs.The n-best reorderings of each rule-matchingsequence are selected for translation.In experiments not reported here, we obtainedaccurate rankings by scoring source permutationswith a uniformly weighted combination of two LMstrained on chunk types and on chunk-type+head-word, respectively.
In particular, 3-best reorderingsof each rule-matching sequence yield reordering re-calls of 77.2% in Arabic and 89.3% in German.6 Modified distortion matricesWe present here a novel technique to encode likelylong reorderings of an input sentence, which can beseamlessly integrated into the PSMT framework.During decoding, the distance between source po-sitions is used for two main purposes: (i) generatinga distortion penalty for the current hypothesis and(ii) determining the set of source positions that canbe covered at the next hypothesis expansion.
We canthen tackle the coarseness of both distortion penaltyand reordering constraints, by replacing the distancefunction with a function defined ad hoc for each in-put sentence.Distortion can be thought of as a matrix assigninga positive integer to any ordered pair of source posi-tions (sx, sy).
In the linear distortion model this is6Following Bisazza and Federico (2010), the optimal re-ordering for a source sentence is the one that minimizes dis-tortion in the word alignment to a target translation, measuredby number of swaps and sum of distortion costs.defined as:DL(sx, sy) = |sy ?
sx ?
1|so that moving to the right by 1 position costs 0 andby 2 positions costs 1.
Moving to the left by 1 posi-tion costs 2 and by 2 positions costs 3, and so on.
Atthe level of phrases, distortion is computed betweenthe last word of the last translated phrase and thefirst word of the next phrase.
We retain this equa-tion as the core distortion function for our model.Then, we modify entries in the matrix such that thedistortion cost is minimized for the decoding pathspre-computed with the reordering rules.Given a source sentence and its set of rule-generated permutations, the linear distortion matrixis modified as follows:1. non-monotonic jumps (i.e.
ordered pairs(si, si+1) such that si+1?
si "= 1) are extractedfrom the permutations;2. then, for each extracted pair, the correspondingpoint in the matrix is assigned the lowest possi-ble distortion cost, that is 0 if si < si+1 and 2if si > si+1.
We call these points shortcuts.Although this technique is approximate and canovergenerate minimal-distortion decoding paths7, itpractically works when the number of encoded per-mutations per sequence is limited.
This makes mod-ifed distortion matrices particularly suitable to en-code just those reorderings that are typically missedby phrase-based decoders (see Sect.
3).Since in this work we use chunk-based rules, wealso have to convert chunk-to-chunk jumps intoword-to-word shortcuts.
We propose two ways todo this, given an ordered pair of chunks (cx,cy):mode A?A : create a shortcut from each word ofcx to each word of cy;mode L?F : create only one shortcut from the lastword of cx to the first of cy.The former solution admits more chunk-internal per-mutations with the same minimal distortion cost,whereas the latter implies that the first word of a re-ordered chunk is covered first and the last is coveredlast.7In fact, any decoding path that includes a jump marked asshortcut benefits from the same distortion discount in that point.482?
?
?
?
?
?
?
?
?
?
???
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
???
?
?
?
?
?
?
?
?
???
??
?
?
?
?
?
?
?
?????????????????????????????????????????????????
????
???
?
???
??
???
?
????
????
???????
???
???????
???
???
???????
???
?????
????????????????????
?
???????????????
??
???
????
???
??
???
????
??????
??
???
?????
???
?
???
???
??
?
?Figure 2: Modified distortion matrix (mode A?A) of theGerman sentence given in Fig.
1.
The chunk reorderingshown on top generates three shortcuts corresponding tothe 0?s and 2?s highlighted in the matrix.Fig.
2 shows the distortion matrix of the Germansentence of Fig.
1, with starting positions as columnsand landing positions as rows.
Suppose we want toencode the reordering shown on top of Fig.
2, cor-responding to the merging of the broken VC ?hat ...eingeleitet?.
This permutation contains three jumps:(2,5), (5,3) and (4,6).
Converted to word-level inA?A mode, these yield five word shortcuts8: onefor the onward jump (2,5) assigned 0 distortion; twofor the backward jump (5,3), assigned 2; and two forthe onward jump (4,6), also assigned 0.
The desiredreordering is now attainable within a DL of 2 wordsinstead of 5.
The same process is then applied toother permutations of the sentence.If compared to the word reordering lattices usedby Bisazza and Federico (2010) and Andreas et al(2011), modified distortion matrices provide a morecompact, implicit way to encode likely reorderingsin a sentence-specific fashion.
Matrix representationdoes not require multiplication of nodes for the same8In L?F mode, instead, each chunk-to-chunk jump wouldyield exactly one word shortcut, for a total of three.source word and is naturally compatible with thePSMT decoder?s standard reordering mechanisms.7 EvaluationIn this section we evaluate the impact of modifieddistortion matrices on two news translation tasks.Matrices were integrated into the Mosestoolkit (Koehn et al, 2007) using a sentence-level XML markup.
The list of word shortcutsfor each sentence is provided as an XML tag thatis parsed by the decoder to modify the distortionmatrix just before starting the search.
As usual, thedistortion matrix is queried by the distortion penaltygenerator and by the hypothesis expander9.7.1 Experimental setupFor Arabic-English, we use the union of all in-domain parallel corpora provided for the NIST-MT09evaluation10 for a total of 986K sentences, 31M En-glish words.
The target LM is trained on the Englishside of all available NIST-MT09 parallel data, UN in-cluded (147M words).
For development and test, weuse the newswire sections of the NIST benchmarks,hereby called dev06-NW, eval08-NW and eval09-NW: 1033, 813 and 586 sentences, respectively, eachprovided with four reference translations.The German-English system is instead trainedon WMT10 data: namely Europarl (v.5) plus News-commentary-2010 for a total of 1.6M parallel sen-tences, 43M English words.
The target LM is trainedon the monolingual news data provided for the con-strained track (1133M words).
For development andtest, we use the WMT10 news benchmarks test08,test09 and test10: 2051, 2525 and 2489 sentences,respectively, with one reference translation.Concerning pre-processing, we apply standard to-kenization to the English data, while for Arabic weuse our in-house tokenizer that removes diacriticsand normalizes special characters.
Arabic text isthen segmented with AMIRA (Diab et al, 2004) ac-cording to the ATB scheme11.
German tokenization9Note that lexicalized reordering models use real word dis-tances to compute the orientation class of a new hypothesis, thusthey are not affected by changes in the matrix.10That is everything except the small GALE corpus and theUN corpus.
As reported by Green et al (2010) the removal ofUN data does not affect baseline performances on news test.11The Arabic Treebank tokenization scheme isolates con-483and compound splitting is performed with Tree Tag-ger (Schmid, 1994) and the Gertwol morphologicalanalyser (Koskenniemi and Haapalainen, 1994)12.Using Moses we build competitive baselines onthe training data described above.
Word alignmentis produced by the Berkeley Aligner (Liang et al,2006).
The decoder is based on the log-linear com-bination of a phrase translation model, a lexicalizedreordering model, a 6-gram target language model,distortion cost, word and phrase penalties.
The re-ordering model is a hierarchical phrase orientationmodel (Tillmann, 2004; Koehn et al, 2005; Galleyand Manning, 2008) trained on all the available par-allel data.
We choose the hierarchical variant, as itwas shown by its authors to outperform the defaultword-based on an Arabic-English task.
Finally, forGerman, we enable the Moses option monotone-at-punctuation which forbids reordering across punc-tuation marks.
The DL is initially set to 5 wordsfor Arabic-English and to 10 for German-English.According to our experience, these are the optimalsettings for the evaluated tasks.
Feature weights areoptimized by minimum error training (Och, 2003)on the development sets (dev06-NW and test08).7.2 Translation quality and efficiency resultsWe evaluate translations with BLEU (Papineni et al,2002) and METEOR (Banerjee and Lavie, 2005).As these scores are only indirectly sensitive to wordorder, we also compute KRS or Kendall Reorder-ing Score (Birch et al, 2010; Bisazza et al, 2011)which is a positive score based on the Kendall?sTau distance between the source-output and source-reference permutations.
To isolate the impact of ourtechniques on problematic reordering, we extractfrom each test set the sentences that got permutedby ?oracle reordering?
(see Sect.
5).
These consti-tute about a half of the Arabic sentences, and abouta third of the German.
We refer to the KRS com-puted on these test subsets as KRS(R).
Statisticallysignificant differences are assessed by approximaterandomization as in (Riezler and Maxwell, 2005)13.Tab.
1 reports results obtained by varying the DLjunctions w+ and f+, prepositions l+, k+, b+, future markers+, pronominal suffixes, but not the article Al+.12http://www2.lingsoft.fi/cgi-bin/gertwol13Translation scores and significance tests are computed withthe tools multeval (Clark et al, 2011) and sigf (Pado?, 2006).and modifying the distortion function.
To evalu-ate the reordering selection technique, we also com-pare the encoding of all rule-generated reorderingsagainst only the 3 best per rule-matching sequence,as ranked by our best performing reordered LM (seeend of Sect.
5).
We mark the DL with a ?+?
to denotethat some longer jumps are being allowed by modi-fied distortion.
Run times refer to the translation ofthe first 100 sentences of eval08-NW and test09 bya 4-core processor.Arabic-English.
As anticipated, raising the DLdoes not improve, but rather worsen performances.The decrease in BLEU and METEOR reported withDL=8 is not significant, but the decrease in KRS isboth significant and large.
Efficiency is heavily af-fected, with a 42% increase of the run time.Results in the row ?allReo?
are obtained by encod-ing all the rule-generated reorderings inL?F chunk-to-word conversion mode.
Except for some gains inKRS reported on eval08-NW, most of the scores arelower or equal to the baseline.
Such inconsistent be-haviour is probably due to the low precision of theArabic rule set, pointed out in Sect.
4.Finally, we arrive to the performance of 3-best re-orderings per sequence.
With L?F we obtain sev-eral improvements, but it?s with A?A that we areable to beat the baseline according to all metrics.BLEU andMETEOR improvements are rather smallbut significant and consistent across test sets, thebest gain being reported on eval09-NW (+.9 BLEU).Most importantly, substantial word order improve-ments are achieved on both full test sets (+.7/+.6KRS) and selected subsets (+.7/+.6 KRS(R)).
Ac-cording to these figures, word order is affected onlyin the sentences that contain problematic reordering.This is good evidence, suggesting that the decoderdoes not get ?confused?
by spurious shortcuts.Looking at run times, we can say that modifieddistortion matrices are a very efficient way to ad-dress long reordering.
Even when all the generatedreorderings are encoded, translation time increasesonly by 4%.
Reordering selection naturally helps tofurther reduce decoding overload.
As for conversionmodes, A?A yields slightly higher run times thanL?F because it generates more shortcuts.German-English.
In this task we manage to im-prove translation quality with a setting that is almost484(a) Arabic to Englisheval08-nw eval09-nw runtimeDistortion Function DL bleu met krs krs(R) bleu met krs krs(R) (s)?
plain [baseline] 5 44.5 34.9 81.6 82.9 49.9 38.0 84.1 84.4 1038plain 8 44.2?
34.8 80.7?
82.2?
49.8 37.9 83.3?
83.5?
1470?
modified: allReo, L?F 5+ 44.4 34.9 82.2?
83.7?
49.9 37.8?
84.3 84.4 1078modified: 3bestReo, L?F 5+ 44.5 35.1?
82.3?
83.5?
50.7?
38.1 84.8?
85.0?
1052?
modified: 3bestReo, A?A 5+ 44.8?
35.1?
82.3?
83.6?
50.8?
38.2?
84.7?
85.0?
1072(b) German to Englishtest09 test10 runtimeDistortion Function DL bleu met krs krs(R) bleu met krs krs(R) (s)?
plain [baseline] 10 18.8 27.5 65.8 66.7 20.1 29.4 68.7 68.9 629plain 20 18.4?
27.4?
63.6?
65.2 ?
19.8?
29.3?
66.3?
66.6?
792plain 4 18.4?
27.4?
67.3?
66.9 19.6?
29.1?
70.2?
69.6?
345?
modified: allReo, L?F 4+ 19.1?
27.6?
67.6?
68.1?
20.4?
29.4 70.6?
70.7?
352modified: 3bestReo, L?F 4+ 19.2?
27.7?
67.4?
68.1?
20.4?
29.4 70.4?
70.6?
351?
modified: 3bestReo, A?A 4+ 19.2?
27.7?
67.4?
68.4?
20.6?
29.5?
70.4?
70.7?
357Table 1: Impact of modified distortion matrices on translation quality, measured with BLEU, METEOR and KRS(all in percentage form, higher scores mean higher quality).
The settings used for weight tuning are marked with ?.Statistically significant differences wrt the baseline are marked with ?
at the p ?
.05 level and ?
at the p ?
.10 level.twice as fast as the baseline.
As shown by the firstpart of the table, the best baseline results are ob-tained with a rather high DL, that is 10 (only KRSimproves with a lower DL).
However, with modifieddistortion, the best results according to all metricsare obtained with a DL of 4.Looking at the rest of the table, we see that re-ordering selection is not as crucial as in Arabic-English.
This is in line with the properties of themore precise German reordering rule set (two rulesout of three generate at most 3 reorderings per se-quence).
Considering all scores, the last setting(3-best reordering and A?A) appears as the bestone, achieving the following gains over the base-line: +.4/+.5 BLEU, +.2/+.1 METEOR, +1.6/+1.7KRS and +1.7/+1.8 KRS(R).
The agreement ob-served among such diverse metrics makes us con-fident about the goodness of the approach.8 ConclusionsIn Arabic-English and German-English, long re-ordering concentrates on specific patterns describ-able by a small number of linguistic rules.
Bymeans of non-deterministic chunk reordering rules,we have generated likely permutations of the testsentences and ranked them with n-gram LMs trainedon pre-reordered data.
We have then introduced thenotion of modified distortion matrices to naturallyencode a set of likely reorderings in the decoderinput.
Modified distortion allows for a finer andmore linguistically informed definition of the searchspace, which is reflected in better translation outputsand more efficient decoding.We expect that further improvements may beachieved by refining the Arabic reordering rules withspecific POS tags and lexical cues.
We also planto evaluate modified distortion matrices in conjunc-tion with a different type of in-decoding reorder-ing model such as the one proposed by Green etal.
(2010).
Finally, we may try to exploit not onlythe ranking, but also the scores produced by the re-ordered LMs, as an additional decoding feature.AcknowledgmentsThis work was supported by the T4ME network ofexcellence (IST-249119) funded by the EuropeanCommission DG INFSO through the 7th FrameworkProgramme.
We thank Christian Hardmeier forhelping us define the German reordering rules, andthe anonymous reviewers for valuable suggestions.485ReferencesYaser Al-Onaizan and Kishore Papineni.
2006.
Distor-tion models for statistical machine translation.
In Pro-ceedings of the 21st International Conference on Com-putational Linguistics and 44th Annual Meeting of theAssociation for Computational Linguistics, pages 529?536, Sydney, Australia, July.
Association for Computa-tional Linguistics.Jacob Andreas, Nizar Habash, and Owen Rambow.
2011.Fuzzy syntactic reordering for phrase-based statisticalmachine translation.
In Proceedings of the Sixth Work-shop on Statistical Machine Translation, pages 227?236, Edinburgh, Scotland, July.
Association for Com-putational Linguistics.Satanjeev Banerjee and Alon Lavie.
2005.
METEOR:An automatic metric for MT evaluation with improvedcorrelation with human judgments.
In Proceedings ofthe ACL Workshop on Intrinsic and Extrinsic Evalu-ation Measures for Machine Translation and/or Sum-marization, pages 65?72, Ann Arbor, Michigan, June.Association for Computational Linguistics.Alexandra Birch, Miles Osborne, and Phil Blunsom.2010.
Metrics for MT evaluation: evaluating reorder-ing.
Machine Translation, 24(1):15?26.Arianna Bisazza and Marcello Federico.
2010.
Chunk-based verb reordering in VSO sentences for Arabic-English statistical machine translation.
In Proceed-ings of the Joint Fifth Workshop on Statistical MachineTranslation and Metrics MATR, pages 241?249, Upp-sala, Sweden, July.
Association for Computational Lin-guistics.Arianna Bisazza, Daniele Pighin, and Marcello Federico.2011.
Chunk-lattices for verb reordering in Arabic-English statistical machine translation.
Machine Trans-lation, Published Online.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofthe 43rd Annual Meeting of the Association for Com-putational Linguistics (ACL?05), pages 263?270, AnnArbor, Michigan, June.
Association for ComputationalLinguistics.Jonathan Clark, Chris Dyer, Alon Lavie, and NoahSmith.
2011.
Better hypothesis testing for sta-tistical machine translation: Controlling for opti-mizer instability.
In Proceedings of the Asso-ciation for Computational Lingustics, ACL 2011,Portland, Oregon, USA.
Association for Com-putational Linguistics.
accepted; available athttp://www.cs.cmu.edu/ jhclark/pubs/significance.pdf.Michael Collins, Philipp Koehn, and Ivona Kucerova.2005.
Clause restructuring for statistical machinetranslation.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguistics(ACL?05), pages 531?540, Ann Arbor, Michigan, June.Association for Computational Linguistics.Josep M. Crego and Nizar Habash.
2008.
Using shal-low syntax information to improve word alignment andreordering for smt.
In StatMT ?08: Proceedings ofthe Third Workshop on Statistical Machine Translation,pages 53?61, Morristown, NJ, USA.
Association forComputational Linguistics.Mona Diab, Kadri Hacioglu, and Daniel Jurafsky.
2004.Automatic Tagging of Arabic Text: From Raw Text toBase Phrase Chunks.
In Daniel Marcu Susan Dumaisand Salim Roukos, editors, HLT-NAACL 2004: ShortPapers, pages 149?152, Boston, Massachusetts, USA,May 2 - May 7.
Association for Computational Lin-guistics.Jakob Elming and Nizar Habash.
2009.
Syntacticreordering for English-Arabic phrase-based machinetranslation.
In Proceedings of the EACL 2009 Work-shop on Computational Approaches to Semitic Lan-guages, pages 69?77, Athens, Greece, March.
Asso-ciation for Computational Linguistics.Minwei Feng, Arne Mauser, and Hermann Ney.
2010.A source-side decoding sequence model for statisticalmachine translation.
In Conference of the Associationfor Machine Translation in the Americas (AMTA), Den-ver, Colorado, USA.Michel Galley and Christopher D. Manning.
2008.A simple and effective hierarchical phrase reorderingmodel.
In EMNLP ?08: Proceedings of the Conferenceon Empirical Methods in Natural Language Process-ing, pages 848?856, Morristown, NJ, USA.
Associa-tion for Computational Linguistics.Spence Green, Michel Galley, and Christopher D. Man-ning.
2010.
Improved models of distortion cost for sta-tistical machine translation.
In Human Language Tech-nologies: The 2010 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics (NAACL), pages 867?875, Los An-geles, California.
Association for Computational Lin-guistics.Nizar Habash.
2007.
Syntactic preprocessing for sta-tistical machine translation.
In Bente Maegaard, ed-itor, Proceedings of the Machine Translation SummitXI, pages 215?222, Copenhagen, Denmark.Christian Hardmeier, Arianna Bisazza, and MarcelloFederico.
2010.
FBK at WMT 2010: Word latticesfor morphological reduction and chunk-based reorder-ing.
In Proceedings of the Joint Fifth Workshop on Sta-tistical Machine Translation and Metrics MATR, pages88?92, Uppsala, Sweden, July.
Association for Com-putational Linguistics.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-486ings of HLT-NAACL 2003, pages 127?133, Edmonton,Canada.Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, Chris Callison-Burch, Miles Osborne, andDavid Talbot.
2005.
Edinburgh system descriptionfor the 2005 IWSLT speech translation evaluation.
InProc.
of the International Workshop on Spoken Lan-guage Translation, October.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,and E. Herbst.
2007.
Moses: Open Source Toolkit forStatistical Machine Translation.
In Proceedings of the45th Annual Meeting of the Association for Computa-tional Linguistics Companion Volume Proceedings ofthe Demo and Poster Sessions, pages 177?180, Prague,Czech Republic.Kimmo Koskenniemi and Mariikka Haapalainen, 1994.GERTWOL ?
Lingsoft Oy, chapter 11, pages 121?140.Roland Hausser, Niemeyer, Tu?bingen.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
In Proceedings of the HumanLanguage Technology Conference of the NAACL, MainConference, pages 104?111, New York City, USA,June.
Association for Computational Linguistics.Jan Niehues and Muntsin Kolss.
2009.
A POS-basedmodel for long-range reorderings in SMT.
In Proceed-ings of the Fourth Workshop on Statistical MachineTranslation, pages 206?214, Athens, Greece, March.Association for Computational Linguistics.Franz Josef Och.
2002.
Statistical Machine Trans-lation: From Single-Word Models to Alignment Tem-plates.
Ph.D. thesis, RWTH Aachen, Germany.Franz Josef Och.
2003.
Minimum Error Rate Trainingin Statistical Machine Translation.
In Erhard Hinrichsand Dan Roth, editors, Proceedings of the 41st AnnualMeeting of the Association for Computational Linguis-tics, pages 160?167.Sebastian Pado?, 2006.
User?s guide to sigf: Signifi-cance testing by approximate randomisation.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting of the Association of Computa-tional Linguistics (ACL), pages 311?318, Philadelphia,PA.Stefan Riezler and John T. Maxwell.
2005.
On somepitfalls in automatic evaluation and significance testingfor MT.
In Proceedings of the ACL Workshop on In-trinsic and Extrinsic Evaluation Measures for MachineTranslation and/or Summarization, pages 57?64, AnnArbor, Michigan, June.
Association for ComputationalLinguistics.Helmut Schmid.
1994.
Probabilistic part-of-speech tag-ging using decision trees.
In Proceedings of Interna-tional Conference on New Methods in Language Pro-cessing.Christoph Tillmann.
2004.
A Unigram OrientationModel for Statistical Machine Translation.
In Proceed-ings of the Joint Conference on Human Language Tech-nologies and the Annual Meeting of the North Ameri-can Chapter of the Association of Computational Lin-guistics (HLT-NAACL).Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?403.Fei Xia and Michael McCord.
2004.
Improving a sta-tistical MT system with automatically learned rewritepatterns.
In Proceedings of Coling 2004, pages 508?514, Geneva, Switzerland, Aug 23?Aug 27.
COLING.Sirvan Yahyaei and Christof Monz.
2010.
Dynamic dis-tortion in a discriminative reordering model for statisti-cal machine translation.
In International Workshop onSpoken Language Translation (IWSLT), Paris, France.Kenji Yamada.
2002.
A syntax-based translation model.Ph.D.
thesis, Department of Computer Science, Uni-versity of Southern California, Los Angeles.Richard Zens and Hermann Ney.
2006.
Discriminativereordering models for statistical machine translation.In Proceedings on the Workshop on Statistical MachineTranslation, pages 55?63, New York City, June.
Asso-ciation for Computational Linguistics.R.
Zens, F. J. Och, and H. Ney.
2002.
Phrase-based sta-tistical machine translation.
In 25th German Confer-ence on Artificial Intelligence (KI2002), pages 18?32,Aachen, Germany.
Springer Verlag.Yuqi Zhang, Richard Zens, and Hermann Ney.
2007.Chunk-level reordering of source language sentenceswith automatically learned rules for statistical machinetranslation.
In Proceedings of SSST, NAACL-HLT 2007/ AMTAWorkshop on Syntax and Structure in StatisticalTranslation, pages 1?8, Rochester, New York, April.Association for Computational Linguistics.Andreas Zollmann, Ashish Venugopal, Franz Och, andJay Ponte.
2008.
A systematic comparison of phrase-based, hierarchical and syntax-augmented statisticalMT.
In Proceedings of the 22nd International Con-ference on Computational Linguistics (Coling 2008),pages 1145?1152, Manchester, UK, August.
Coling2008 Organizing Committee.487
