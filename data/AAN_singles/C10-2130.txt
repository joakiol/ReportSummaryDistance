Coling 2010: Poster Volume, pages 1131?1139,Beijing, August 2010Using Clustering to Improve Retrieval Evaluation withoutRelevance JudgmentsZhiwei ShiInstitute of Computing TechnologyChinese Academy of Scienceshizhiwei@ict.ac.cnPeng LiInstitute of Computing TechnologyChinese Academy of Sciencelipeng01@ict.ac.cnBin WangInstitute of Computing TechnologyChinese Academy of Sciencewangbin@ict.ac.cnAbstractRetrieval evaluation without relevancejudgments is a hard but also very mean-ingful work.
In this paper, we use clus-tering technique to improve the per-formance of judgment free retrievalevaluation.
By using one system to rep-resent all the systems that are similar toit, we can largely reduce the negative ef-fect of similar retrieval results in Re-trieval evaluation.
Experimental resultsdemonstrated that our method outper-formed all the previous judgment freeevaluation methods significantly.
Itsoverall average performance outper-formed the best previous result by20.5%.
Besides, our work is a generalframework that can be applied to anyother judgment free evaluation methodfor performance improvement.1 IntroductionGenerally, to compare the effectiveness of in-formation retrieval systems, we need to preparea test collection composed of a set of documents,a set of query topics, and a set of relevancejudgments indicating which documents are rele-vant to which topics.
Among these requirements,relevance judgment is the most human resourceexhausting and time consuming part.
It evenbecomes infeasible when the test collection isextremely large.
To address this problem, theTREC conferences used a pooling technology(Voorhees and Harman, 1999), where the top n(e.g., n=100) documents retrieved by each par-ticipating system are collected into a pool andthen only the documents in the pool are judgedfor system comparison.
Zobel (1998) has shownthat this pooling method leads to reliable resultsin term of determining the effectiveness of re-trieval systems and their relative rankings.
Yet,the relevance determination process is still veryresource intensive especially when the test col-lection reaches or exceeds terabyte, or muchmore queries are included.
More seriously,when we change to a new document collection,we have to redo the entire evaluation process.There are two possible solutions to the prob-lem above, evaluation with incomplete rele-vance judgments and evaluation without rele-vance judgments.
The former is well studied.Many well designed ranking methods with in-complete judgments were carried out.
Two ofthem, Minimal Test Collection (MTC) method(Carterette et al, 2006) and Statistical evalua-tion (statMAP) method (Aslam et al, 2006),even got practical application in the MillionQuery (1MQ) track in TREC 2007 (Allan et al,2007), and achieved satisfactory evaluation per-formance.
The latter is comparatively less stud-ied.
Only a few papers concentrate on the issueof evaluating retrieval systems without rele-vance judgments.
In Section 2 of this paper, wewill briefly review some representative methods.We will see what they are and how they work.1131In this paper, we focus our effort on the re-trieval evaluation without relevance judgments.Although ?blind?
evaluation is really a hardproblem and its evaluation performance is farless than that of methods with incompletejudgments, it is undeniable that non-judgmentevaluation has its own advantages.
In somecases, relevance judgments are non-attainable.For example, when researchers compare theirnovel retrieval algorithms to existing methods,or search for optimal parameters of their algo-rithms, or conduct data fusion in a dynamic en-vironment, relevance judgment usually seemsimpossible.
Besides, to construct a good evalua-tion method without relevance judgments, re-searchers need to mine the retrieval results thor-oughly, and try to find laws that indicate thecorrelation between the effectiveness of a sys-tem and features of its retrieval result.
Theselaws are not only useful for ?blind?
evaluationmethods but also valuable for evaluation meth-ods with incomplete judgments.One of the useful laws for ?blind?
evaluationmethods is Authority Effect (Spoerri, 2005).
Yetit always ruined by multiple similar results.In this work, we use clustering technique tosolve this problem.
By selecting one system torepresent all the systems that are similar to it,we can largely reduce the negative effect ofsimilar retrieval results.
Details of this methodwill be presented Section 3.
Experimental re-sults, which are reported in Section 4, also veri-fied that our idea is feasible and effective.
Ourmethod outperformed all the previous judgmentfree evaluation methods on every test bed.
Theoverall average performance outperformed thebest previous result by 20.5%.
Finally, we con-clude our work in Section 5.2 Related WorkIn 2001, Soboroff et al (2001) firstly proposedthe concept of evaluating retrieval systems inthe absence of relevance judgments.
Theygenerated a set of pseudo-relevance judgmentsby randomly selecting and declaring somedocuments from the pool of top 100 documentsas relevant.
This set of pseudo-relevancejudgments (instead of a set of human relevancejudgments) was then used to determine theeffectiveness of the retrieval systems.
Fourversions of this random pseudo-relevancemethod were designed and tested on data fromthe ad hoc track in TREC 3, 5, 6, 7 and 8.
Theywere simple random pseudo-relevance method,the variant with duplicate documents, thevariant with Shallow pools and the variant withExact-fraction sampling.
All their resultingsystem assessments and rankings were wellcorrelated with actual TREC rankings, and thevariant with duplicate documents in pools gotthe best performance, with an average Kendall?stau value 0.50 over the data of TREC 3, 5, 6, 7and 8.Soboroff et al?s idea came from two resultsin retrieval evaluation.
One is that incompletejudgments do not harm evaluation resultsgreatly.
Zobel?s (1998) research had showedthat the results obtained using pooling technol-ogy were quite reliable given a pool depth of100.
He also found that even though the pooldepth was limited to 10, the relative perform-ance among systems changed little, althoughactual precision scores did change for some sys-tems.
The other is that partially incorrect rele-vance judgments do not harm evaluation resultsgreatly.
Voorhees (1998) ascertained that de-spite a low average overlap between assessmentsets, and wide variation in overlap among par-ticular topics, the relative rankings of systemsremained largely unchanged across the differentsets of relevance judgments.
These two pointsare bases of Soboroff et al?s random pseudo-relevance method, and give explanation to theresult that their rankings were positively relatedto that of the actual TRECs.
As a matter of fact,the two points are bases of all the retrievalevaluation methods without or with incompleterelevance judgments.Aslam and Savell (2003) devised a method tomeasure the relative retrieval effectiveness ofsystems through system similarity computation.In their work, the similarity between two re-trieval systems was the ratio of the number ofdocuments in their intersection and union.
Eachsystem was scored by the average similaritybetween it and all other systems.
This measure-ment produced results that were highly corre-lated with the random pseudo-relevance method.Aslam and Savell hypothesized that this wascaused by ?tyranny of the masses?
effect, andthese two related methods were assessing thesystems based on ?popularity?
instead of ?per-formance?.
The analysis by Spoerri (2005) sug-1132gested that the ?popularity?
effect was caused byconsidering all the runs submitted by a retrievalsystem, instead of only selecting one run persystem.
Our later experimental results will showthat this point of view is partially correct.
The?popularity?
effect could not be avoided com-pletely by only selecting one run per system.This is indeed a hard problem for all the evalua-tion methods without relevance judgments.Wu and Crestani (2003) developed multiple?reference count?
based methods to rank re-trieval systems.
They made the distinction be-tween an ?original?
document and its duplicatesin all other lists, called the ?reference?
docu-ments, when computing a document?s score.
Asystem?s score is the (weighted) sum of thescores of its ?original?
documents.
Several ver-sions of reference count method were carriedout and tested.
The basic method (Basic) scoredeach ?original?
document by the number of its?reference?
documents.
The first variant (V1)assigned different weights to ?reference?
docu-ments based on their ranking positions.
Thesecond variant (V2) assigned different weightsto the ?original?
document based on its rankingposition.
The third variant (V3) assigned differ-ent weights to both the ?original?
documents andthe ?reference?
documents based on their rank-ing positions.
The fourth variant (V4) was simi-lar to V3, except that it normalized the weightsto ?reference?
documents.
Wu and Crestani?smethod output similar evaluation performanceto that of the random pseudo-relevance method.Their work also showed that the similarity be-tween the multiple runs submitted by the sameretrieval system affected the ranking process.
Ifonly one run was selected for any of the partici-pant system for any query, for 3-9 systems, V3outperformed random pseudo-relevance methodby 45.6%; for 10-15 systems, random pseudo-relevance method outperformed V3 by 6.5%.Nuray and Can (2006) introduced a methodto rank retrieval systems automatically usingdata fusion.
Their method consists of two parts.One is selecting systems for data fusion, and theother is selecting documents as pseudo relevantdocuments as the fusion result.
In the formerpart, they hypothesized that systems returningdocuments different from the majority couldprovide better discrimination among the docu-ments and systems.
In return, this could lead toa more accurate pseudo relevant documents andmore accurate rankings.
To find proper systems,they introduced the ?bias?
concept for systemselection.
In their work, bias was 1 minus thesimilarity between a system and the majority,where the similarity is a normalized dot productof two vectors.
In the latter part, Nuray and Cantested three criterions, namely Rank position,Borda count and Condorcet.
Experimental re-sults on data from TREC 3, 5, 6 and 7 showedthat bias plus Condorcet got the best evaluationresults and it outperformed the reference countmethod and random pseudo relevance methodgreatly.More recently, Spoerri (2007) proposed amethod using the structure of overlap betweensearch results to rank retrieval systems.
Thismethod provides us a new view on how to rankretrieval systems without relevance judgments.He used local statistics of retrieval results asindicators of relative effectiveness of retrievalsystems.
Concretely, if there are N systems to beranked, N groups are constructed randomly withthe constraint that each group contains five sys-tems and each system will appear in five groups;then the percentages of a system?s documentsnot found by other systems (Single%) as well asthe difference between the percentages of docu-ments found by a single system and all five sys-tems (Single%-AllFive%) are calculated as in-dicators of relative effectiveness respectively.Spoerri found that these two local statistics werehighly and negatively correlated with the meanaverage precision and precision at 1000 scoresof the systems.
By utilizing the two statistics torank systems from subsets of TREC 3, 6, 7 and8, Spoerri obtained appealing evaluation results.The overlap structure of the top 50 documentswere sufficient to rank retrieval systems andproduced the best results, which outperformedprevious attempts to rank retrieval systemswithout relevance judgments significantly.So far, we have reviewed 5 representatives ofnon-judgment evaluation methods.
All thesemethods faced the same serious problem: simi-lar runs harmed the effectiveness of rankingprocess.
Different methods handled this prob-lem differently.
Aslam and Savell (2003) calledthis the ?tyranny of the masses?
and provided nosolution.
Wu and Crestani (2003) addressed thisproblem by selecting only one run for any of theparticipant system for any query.
Nuray andCan (2006) selected systems that were less simi-1133lar to the majority for data fusion.
Spoerri (2007)performed his method on a selected subset of allthe systems.
All these treatments led to evalua-tion performance improvement.
Yet we will sayit could be improved more.
In the next section,we will present a new solution to this problem.Its performance is examined in Section 4.3 Using Clustering to Improve Re-trieval Evaluation without RelevanceJudgments3.1 ProblemAs we reviewed in Section 2, previous researchhad shown that incomplete relevance judgmentsand partially incorrect relevance judgments donot harm retrieval evaluation greatly.
This iswhy pooling technique can lead to reliableretrieval evaluation results.
It is also thetheoretical foundation of evaluation withoutrelevance judgments.Besides, non-judgments methods armed withmore laws inside retrieval results.
These lawsindicate the correlation between retrieval effec-tiveness of a system and features in its retrievalresults.
One of the most important laws used innon-judgments evaluation is Authority Effect(Spoerri, 2005): document, which is retrieval bymore systems, is more likely being relevant.Unfortunately, similar retrieval results ruinedthis law.
Aslam and Savell (2003) called this the?tyranny of the masses?.
So, how to alleviate thenegative effect of similar retrieval results is abig issue in non-judgments evaluation.3.2 SolutionGenerally, our solution to the ?tyranny of themasses?
is removing similar systems by cluster-ing.
The whole process is as follows:Firstly, all systems to be evaluated are clus-tered into several subsets.Secondly, for each subset, one system is se-lected as a representative.Thirdly, all the information used for systemevaluation comes from these representatives.Finally, score every system according to theinformation collected in the previous step.This is the general framework of our method-ology.
Notice that, in the third step, only se-lected systems contribute to the informationrequired for system evaluation.
So we can elimi-nate the negative effect caused by similar re-trieval results.This solution can be applied to any method ofretrieval evaluation without relevance judg-ments.
To illustrate how to apply it to a retrievalevaluation method, we will describe using clus-tering to improve Average System Similarity,which is proposed by Aslam and Savell (2003),in detail as an example.3.3 Average System Similarity Based onClusteringIn Aslam and Savell?s (2003) method, each sys-tem is evaluated based on a criterion named Av-erage System Similarity.
The average systemsimilarity of a given system S0 is calculated ac-cording to formula (1).
?z0),(11)(AvgSysSim00SSSSSysSimnS(1)where n is the number of systems to be evalu-ated, and similarity between two systems S andS0, SysSim(S, S0), is calculated based on for-mula (2).212121 RetRetRetRet),(SysSim?
?SS (2)where Reti indicates the set of documents re-turned by System i (i = 1, 2).When applying clustering technique to thesystem similarity method, we need to define anequivalence relation first.Definition 1 (System Equivalence): Supposethat all systems are clustered into m clustersnamely C1, C2, ?, Cm.
Two systems S1 and S2are equivalent if and only if there exists k (1 ?
k?
m) so that S1?Ck and S2?Ck.kk CSCSmkkiffSS?
?dd2121,,1,(3)Given the definition of System Equivalence,we get the average system similarity based onclustering as follows:?z0),(11)(AvgSysSim00SRSRSysSimmS(4)where m is the number of clusters and R is therepresentative system of a cluster.1134Replacing formula (1) with formula (4), weget the retrieval evaluation method AverageSystem Similarity Based on Clustering, shortlyASSBC.There are two important issues for ASSBCthat need to be addressed.
Issue 1: How to selectrepresentative system from a cluster?
Issue 2:How to decide the number of clusters we need?Before we address Issue 1, we introduce an-other definition, Cluster Similarity.Definition 2 (Cluster Similarity): for anygiven two clusters C1 and C2, with their respec-tive representative systems S1 and S2, the clustersimilarity between C1 and C2 is the system simi-larity between S1 and S2.
),(SysSim),(ClusterSim 2121 SSCC  (5)Now we come to selecting representative sys-tems for clusters.
Here, we utilize a hierarchicalbottom up clustering technique.
The entire clus-tering process is as follows.Initially, each system forms a cluster.Loop Until the number of clusters is mTwo most similar clusters merge, andone of their representatives with higheraverage system similarity survives asthe representative of the new cluster.End Loop.In the initial step, since every cluster containsonly one system, the representative system isunquestionable.
Within each loop, two represen-tative systems of the old clusters are candidatesof the new cluster, and the one with higher score,which means higher retrieval performance, be-comes the representative of the new cluster.For Issue 2, technically, how to decide thenumber of clusters is always a problem for clus-tering.
Yet, we do not have to rush in the deci-sion.
Let us examine the evaluation perform-ance on different values of m first.4 ExperimentsIn this section, we will illustrate the evaluationperformance of Average System SimilarityBased on Clustering vs. different values of m.Before we come to the experimental results, wewould like to make some details clear first.4.1 Some Clarification4.1.1 DatasetWe perform our experiments on the ad hoc tasksof TREC-3, -5, -6 and -7.
Most existing workson retrieval evaluation without judgments aretested on these tasks.
To make a direct compari-son with these work mentioned in Section 2later, we also choose these tasks as our test bed.4.1.2 Performance MeasurementOne of the measures of retrieval effectivenessused by TREC is mean non-interpolated averageprecision (MAP).
Since average precision isbased on much more information than other ef-fectiveness measures such as R-precision orP(10) and known to be a more powerful andmore stable effectiveness measure (Buckley andVoorhees, 2000), we utilize MAP as the effec-tive measurement of retrieval systems in ourexperiments.The correlation of the ranking with our pro-posed methods, as well as other methods, to theTREC official rankings is measured using theSpearman?s rank correlation coefficient.
Onereason is that it suits better for evaluating corre-lation between ratio sequences, e.g.
MAP, thanKendall?s tau.
The other reason is that we candirectly compare our results with those of pre-vious attempts reviewed in Section 2, sincemost of them provided Spearman?s rank correla-tion coefficient results.4.1.3 Substitute for Number of ClustersTREC Runs3 405 616 747 103Table 1.
Number of TREC runsAs we know, the number of systems (runs) var-ies in different TREC dataset (see Table 1 fordetails).
Instead of examining the evaluationperformance variation when absolute number ofclusters m changes, we illustrate the evaluationperformance vs. the percentage of m. Actually,for the sake of convenience, we will plot thecorrelation of our method to the TREC officialrankings vs. the percentage of systems removedfrom the representative group in the followingsubsection.11354.2 Experimental resultsFigure 1-4 show the plots of the correlationof our method to the TREC official rankings vs.the percentage of systems removed from therepresentative group on TREC-3, -5, -6 and -7respectively.
The percentage of systems re-moved goes from 0 to 85%, where 0 means nosystem removed and represents the original Av-erage System Similarity method, and 85% is anup bound in our experiments.
The horizontalline indicates the original performance.
Thetagged number on the curve says when the per-formance curve reaches its peak and the peakvalue.     5HPRYHG3HUFHQWDJH6SHDUPDQ&RHIILFLHQWFigure 1.
Spearman Coefficient of ASSBC vs. differentpercentage of removed systems on TREC -3.In Figure 1, the Spearman coefficients ofASSBC vs. different percentage of removedsystems on TREC-3 are presented.
Except forthe beginning, almost all the points are abovethe horizontal line.
The curve reaches its top at65%-67%, where the Spearman coefficient is0.8929.     5HPRYHG3HUFHQWDJH6SHDUPDQ&RHIILFLHQWFigure 2.
Spearman Coefficient of ASSBC vs. differentpercentage of removed systems on TREC -5.Figure 2 depicts the evaluation performanceon TREC-5.
From 0 to 63%, the performancecurve fluctuates around the horizontal line.
Thismeans deficient clustering does not bring sub-stantial performance variation.
After 63%, thecurve begins to rise and reaches its peak at 78%,where the performance is 0.8691.
Then it dropsdramatically as more systems removed from therepresentative group.The situation on TREC-6 is plotted in Figure3.
In this case, the curve rises gently in the in-terval between 0 and 70% except for some fluc-tuation.
After 70%, the curve starts to climb andreaches the peak at 75% with the peak value of0.8576.
It remains high performance until 80%,and then decline quickly.     5HPRYHG3HFHQWDJH6SHDUPDQ&RHIILFLHQWFigure 3.
Spearman Coefficient of ASSBC vs. differentpercentage of removed systems on TREC -6.Figure 4 presents the evaluation performanceon TREC-7.
The trend in this figure is prettymuch like that in Figure 2.
The curve fluctuatesfirst, and then climbs the hill, where the peakvalue is 0.6557 and 75% systems are removed.The only difference is in this figure the curve isgentler.
This means on TREC-7 ASSBC doesnot obtain as much improvement as on TREC-5.     5HPRYHG3HUFHQWDJH6SHDUPDQ&RHIILFHQWFigure 4.
Spearman Coefficient of ASSBC vs. differentpercentage of removed systems on TREC -7.According Figure1-4, we can say that cluster-ing systems does bring us evaluation perform-ance improvement.
Generally, obvious im-provement occurs in the interval between 65%and 80%.
TREC-3 is an exception.
The curve onTREC-3 reaches its peak at 65%.
Notice that inTREC-3 there are only 40 systems (runs), and113665% indicates 26 systems removed and 14 sys-tems left as representatives.
Interestingly, forother TRECs, 78% (the biggest peak position)means at least 14 systems left as well.
So, thiscan be interpreted as the minimum number ofclusters.To examine the general effect on evaluationperformance of cluster number, we also plot theaverage performance of TREC -3, -5, -6 and -7vs.
the percentage of systems removed from therepresentative group in Figure 5.
With slightfluctuation, the average performance curveclimbs stably, and reaches its peak 0.7754 at theposition 78%.
Then it drops dramatically.     5HPRYHG3HUFHQWDJH6SHDUPDQ&RHIILFLHQWFigure 5.
Average Spearman Coefficient of ASSBC vs.different percentage of removed systems on TREC -3, -5, -6 and -7.To make the result more intuitive, we presenta comparison of the performance of originalAverage System Similarity (ASS) and the bestperformance of Average System SimilarityBased on Clustering (ASSBC) in Table 2.
Ac-cording to the table, we can see that clusteringsystems improve the evaluation performancesignificantly.ASS ASSBC ImprovementTrec3 0.7086 0.8929 26.0%Trec5 0.5277 0.8691 64.7%Trec6 0.6300 0.8576 36.1%Trec7 0.5855 0.6557 12.0%Avg 0.6129 0.7754 26.5%Table 2.
Spearman coefficients of original AverageSystem Similarity (ASS) and the best performance ofAverage System Similarity Based on Clustering(ASSBC) on TREC -3, -5, -6, -7 and the over all aver-age.4.3 Comparison with All Previous At-temptsMeanwhile, we also provide a comparisonamong the ASSBC method and all the existingnon-judgment evaluation methods mentioned inSection 2.
The result is given in Table 3.  RS RC CB Single% ASS ASSBC optimal(78% Removed)Trec3 0.627  0.587  0.867  0.824  0.709  0.893Trec5 0.429  0.421  0.657*  0.563  0.528  0.869Trec6 0.436  0.384  0.717  0.618  0.630  0.854Trec7 0.411  0.382  0.453  0.550  0.585  0.631Avg 0.476  0.444  0.674  0.639  0.613  0.812Table 3.
Spearman coefficients for best results from different evaluation methodsIn Table 3, RS represents the result of ran-dom pseudo relevance method, where relevanceratio is set to 10% rather than the actual ratio inits original version; RC is the best result pro-duced by reference count method; BC accountsfor the best result of Bias plus Condorcetmethod, a data fusion based method.
Results ofthese three methods are cited from Nuray andCan?s (2006) paper.
For the number with a ?*?
(BC on TREC 5), in their original paper, sameresult in different tables conflict, and we pickthe higher value presenting in Table 3.
Single%is the representative of Spoerri?s overlap struc-ture based method.
Different from its originalversion, the result in Table 3 is gained on all thesystems opposite to on a selected subset, exceptthat runs submitted by the same system arecounted only once.
ASS is short for AverageSystem Similarity.
ASSBC optimal is the bestresult of our method.
Here we utilize both 78%1137as the percentage of removed systems and 14 asthe minimum number of clusters1.
Clearly, ourmethod outperforms all the previous attempts onevery TREC.
The overall average performanceoutperforms the best previous result (from CB)by 20.5%.5 ConclusionRetrieval evaluation without relevance judg-ments is a hard problem.
Meanwhile it is also animportant problem that we can not avoid it inmany research areas and applications.One of the main factors that depress the per-formance of judgments free evaluation is: simi-lar retrieval results ruined the Authority Effect,which is one of the important bases for all thejudgment free evaluation methods.In this paper, we use clustering technique toaddress this problem.
By using one system torepresent all the systems that are similar to it,we can largely reduce the negative effect ofsimilar retrieval results.
Experimental resultsalso verified our idea.
Our method outperformsall the previous judgment free evaluation meth-ods on every test bed.
The overall average per-formance outperforms the best previous resultby 20.5%.Besides, improving judgment free evaluationvia clustering is more than just a method.
It is ageneral framework that can be applied to anyjudgment free evaluation method.
The AverageSystem Similarity Based on Clustering methodis an example.
It works well means that theframework is feasible and successful.
We willapply it to other judgment free evaluation meth-ods in our future work.Acknowledgement This work is supportedby the National Science Foundation of Chinaunder Grant No.
60776797, the Major State Ba-sic Research Project of China (973 Program)under Grant No.
2007CB311103 and the Na-tional High Technology Research and Devel-opment Program of China (863 Program) underGrant No.
2006AA010105.1 Since we add a terminal criterion for clusteringwith 14 as the minimum number of clusters, the av-erage performance in Table 3 gains an improvementcompared to that presented in Figure 5 and Table 2.ReferencesAllan J., Carterette B., Aslam J.
A., Pavlu V.,Dachev B., and Kanoulas E. 2007 Overview ofthe TREC 2007 Million Query Track, Proceedingsof TREC.Aslam J.
A., Pavlu V. and Yilmaz E. 2006 A statisti-cal method for system evaluation using incom-plete judgments, Proceedings of the 29th annualinternational ACM SIGIR conference on Researchand development in information retrieval, August06-11, 2006, Seattle, WashingtonAslam J.
A. and Savell R. 2003 On the effectivenessof evaluating retrieval systems in the absence ofrelevance judgments, Proceedings of the 26th an-nual international ACM SIGIR conference on Re-search and development in informaion retrieval,July 28-August 01, 2003, Toronto, CanadaBuckley, C. and Voorhees, E. M. 2000 Evaluatingevaluation measure stability, Proceedings of the23rd ACMSIGIR conference pp.
33?40Carterette B., Allan J. and Sitaraman R. 2006 Mini-mal test collections for retrieval evaluation, Pro-ceedings of the 29th annual international ACMSIGIR conference on Research and developmentin information retrieval, August 06-11, 2006, Se-attle, Washington, USANuray R. and Can F. 2006 Automatic ranking ofinformation retrieval systems using data fusion,Information Processing and Management: an In-ternational Journal, v.42 n.3, p.595-614, May2006Soboroff I., Nicholas C. and Cahan P. 2001 Rankingretrieval systems without relevance judgments,Proceedings of the 24th annual international ACMSIGIR conference on Research and developmentin information retrieval, p.66-73, September 2001,New Orleans, Louisiana, United StatesSpoerri A.
2005 How the overlap between searchresults correlates with relevance.
In: Proceedingsof the 68th annual meeting of the American Soci-ety for Information Science and Technology(ASIST 2005).Spoerri A.
2007 Using the structure of overlap be-tween search results to rank retrieval systemswithout relevance judgments, Information Proc-essing and Management: an International Journal,v.43 n.4, pp.1059-1070, July, 2007Voorhees E. M. 1998 Variations in relevance judg-ments and the measurement of retrieval effective-ness, Proceedings of the 21st annual internationalACM SIGIR conference on Research and devel-1138opment in information retrieval, p.315-323, Au-gust 24-28, 1998, Melbourne, AustraliaVoorhees E. M. and Harman, D. 1999 Overview ofthe eighth text retrieval conference (TREC-8).The eighth text retrieval conference (TREC-8),Gaithersburg, MD, USA, 1999.
U.S. GovernmentPrinting Office, WashingtonWu S. and Crestani F. 2003 Methods for rankinginformation retrieval systems without relevancejudgments, Proceedings of the 2003 ACM sympo-sium on Applied computing, March 09-12, 2003,Melbourne, FloridaZobel J.
1998 How reliable are the results of large-scale information retrieval experiments?, Proceed-ings of the 21st annual international ACM SIGIRconference on Research and development in in-formation retrieval, p.307-314, August 24-28,1998, Melbourne, Australia1139
