Proceedings of the TextGraphs-7 Workshop at ACL, pages 39?43,Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational LinguisticsCause-Effect Relation LearningZornitsa KozarevaUSC Information Sciences Institute4676 Admiralty WayMarina del Rey, CA 90292-6695kozareva@isi.eduAbstractTo be able to answer the question Whatcauses tumors to shrink?, one would re-quire a large cause-effect relation repos-itory.
Many efforts have been payed onis-a and part-of relation leaning, howeverfew have focused on cause-effect learn-ing.
This paper describes an automatedbootstrapping procedure which can learnand produce with minimal effort a cause-effect term repository.
To filter out theerroneously extracted information, we in-corporate graph-based methods.
To evalu-ate the performance of the acquired cause-effect terms, we conduct three evaluations:(1) human-based, (2) comparison with ex-isting knowledge bases and (3) applica-tion driven (SemEval-1 Task 4) in whichthe goal is to identify the relation betweenpairs of nominals.
The results show thatthe extractions at rank 1500 are 89% ac-curate, they comprise 61% from the termsused in the SemEval-1 Task 4 dataset andcan be used in the future to produce addi-tional training examples for the same task.1 IntroductionOver the years, researchers have successfullyshown how to build ground facts (Etzioni etal., 2005), semantic lexicons (Thelen and Riloff,2002), encyclopedic knowledge (Suchanek et al,2007), and concept lists (Katz et al, 2003).Among the most well developed repositories arethose focusing on is-a (Hearst, 1992) and part-of (Girju et al, 2003; Pennacchiotti and Pantel,2006) relations.
However, to be able to answer thequestion ?What causes tumors to shrink?
?, one re-quires knowledge about cause-effect relation.Other applications that can benefit from cause-effect knowledge are the relational search engineswhich have to retrieve all terms relevant to a querylike: ?find all X such that X causes wrinkles?
(Ca-farella et al, 2006).
Unfortunately to date, thereis no universal repository of cause-effect relationsthat can be used or consulted.
However, one wouldstill like to dispose of an automated procedure thatcan accurately and quickly acquire the terms ex-pressing this relation.Multiple algorithms have been created to learnrelations.
Some like TextRunner (Etzioni et al,2005) rely on labeled data, which is used to traina sequence-labeling graphical model (CRF) andthen the system uses the model to extract termsand relations from unlabeled texts.
Although veryaccurate, such methods require labeled data whichis difficult, expensive and time consuming to cre-ate.
Other more simplistic methods that relyon lexico-syntactic patterns (Hearst, 1992; Riloffand Jones, 1999; Pasca, 2004) have shown to beequally successful at learning relations, temporalverb order (Chklovski and Pantel, 2004) and en-tailment (Zanzotto et al, 2006).
Therefore, in thispaper, we have incorporated an automated boot-strapping procedure, which given a pattern rep-resenting the relation of interest can quickly andeasily learn the terms associated with the relation.In our case, the pattern captures the cause-effectrelation.
After extraction, we apply graph-basedmetrics to rerank the information and filter out theerroneous terms.The contributions of the paper are:?
an automated procedure, which can learnterms expressing cause-effect relation.?
an exhaustive human-based evaluation.?
a comparison of the extracted knowledgewith the terms available in the SemEval-1Task 4 dataset for interpreting the relation be-tween pairs of nominals.The rest of the paper is organized as follows.The next section describes the term extraction pro-cedure.
Section 3 and 4 describe the extracted data39and its characteristics.
Section 5 focuses on theevaluation and finally we conclude in Section 6.2 Cause-Effect Relation Learning2.1 Problem FormulationThe objectives of cause-effect relation learning aresimilar to those of any general open domain rela-tion extraction problem (Etzioni et al, 2005; Pen-nacchiotti and Pantel, 2006).
The task is formu-lated as:Task: Given a cause-effect semantic relation expressedthrough lexico-syntactic pattern and a seed example forwhich the relation is true, the objective is to learn fromlarge unstructured amount of texts terms associated withthe relation.For instance, given the relation cause and theterm virus for which we know that it can causesomething, we express the statement in a recursivepattern1 ?
* and virus cause *?
and use the patternto learn new terms that cause or have been causedby something.
Following our example, the recur-sive pattern learns from the Web on the left sideterms like {bacteria, worms, germs} and on theright side terms like {diseases, damage, contami-nation}.2.2 Knowledge Extraction ProcedureFor our study, we have used the general Web-based class instance and relation extraction frame-work introduced by (Kozareva et al, 2008; Hovyet al, 2009).
The procedure is minimally super-vised and achieves high accuracy of the producedextractions.TermExtraction: To initiate the learning process,the user must provide as input a seed term Y and arecursive pattern ?X?
and Y verb Z??
from whichterms on the X?
and Z?
positions can be learned.The input pattern is submitted to Yahoo!Boss APIas a web query and all snippets matching the queryare retrieved, part-of-speech tagged and used forterm extraction.
Only the previously unexploredterms found on X?
position are used as seedsin the subsequent iteration, while the rest of theterms2 are kept.
The knowledge extraction termi-nates when there are no new extractions.Term Ranking: Despite the specific lexico-syntactic construction of the pattern, erroneous1A recursive pattern is a lexico-syntactic pattern for whichone of the terms is given as input and the other one is anopen slot, allowing the learned terms to replace the initialterm directly.2Including the terms found on Z?
position.extractions are still produced.
To filter outthe information, we incorporate the harvestedterms on X?
and Y ?
positions in a directedgraph G=(V,E), where each vertex v ?
V isa candidate term and each edge (u, v) ?
Eindicates that the term v is generated by the termu.
An edge has weight w corresponding to thenumber of times the term pair (u, v) is extractedfrom different snippets.
A node u is rankedby u=(??
(u,v)?E w(u, v) +??
(v,u)?E w(v, u))which represents the weighted sum of the outgo-ing and incoming edges to a node.
The confidencein a correct argument u increases when the termdiscovers and is discovered by many differentterms.
Similarly, the terms found on Z?
positionare ranked by the total number of incoming edgesfrom the XY pairs z=??(xy,z)?E?
w(xy, z).We assume that in a large corpus as the Web, acorrect term Z?
would be frequently discoveredby various XY term pairs.3 Data CollectionTo learn the terms associated with a cause-effectrelation, the user can use as input any verb ex-pressing causality3.
In our experiment, we usedthe verb cause and the pattern ?
* and <seed>cause *?, which was instantiated with the seedterm virus.
We submitted the pattern to Ya-hoo!Boss API as a search query and collected allsnippets returned during bootstrapping.
The snip-pets were cleaned from the html tags and part-of-speech tagged (Schmid, 1994).
All nouns (propernames) found on the left and right hand side of thepattern were extracted and kept as potential candi-date terms of the cause-effect relation.Table 1 shows the total number of terms foundfor the cause pattern on X?
and Z?
positions in 19bootstrapping iterations.
In the same table, we alsoshow some examples of the obtained extractions.Term Position #Extractions ExamplesX cause 12790 pressure, stress, fire,cholesterol, wars, ice,food, cocaine, injuriesbacteriacause Z 52744 death, pain, diabetes,heart disease, damage,determination, nosebleedschain reactionTable 1: Extracted Terms.3The user can use any pattern from the thesauri ofhttp://demo.patrickpantel.com/demos/lexsem/thesaurus.htm404 Characteristic of Learning TermsAn interesting characteristic of the bootstrappingprocess is the speed of leaning, which can be mea-sured in terms of the number of unique terms ac-quired on each bootstrapping iteration.
Figure 1shows the bootstrapping process for the ?cause?relation.
The term extraction starts of very slowlyand as bootstrapping progresses a rapid growth isobserved until a saturation point is reached.
Thispoint shows that the intensity with which new el-ements are discovered is lower and practically thebootstrapping process can be terminated once theamount of newly discovered information does notexceed a certain threshold.
For instance, insteadof running the algorithm until complete exhaus-tion (19 iterations), the user can terminate it onthe 12th iteration.!
"#$%&'()%'#*(+,-,&(./"*,01'(2"1,3$,4#%1,(./"*,01'(21,3%5/"'(6(/7(#1,$'(&,%3",4(8(1,3$'(9(1,3$'(8:(%"4(;(.%<',(9:(Figure 1: Learning Curve.The speed of leaning depends on the way theX and Y terms relate to each other in the lexico-syntactic pattern.
For instance, the more denselyconnected the graph is, the shorter (i.e., fewer iter-ations) it will take to acquire all terms.5 Evaluation and ResultsIn this section, we evaluate the results of the termextraction procedure.
To the extend to which itis possible, we conduct a human-based evalua-tion, we compare results to knowledge bases thathave been extracted in a similar way (i.e., throughpattern application over unstructured text) and weshow how the extracted knowledge can be usedby NLP applications such as relation identificationbetween nominals.5.1 Human-Based EvaluationFor the human based evaluation, we use two an-notators to judge the correctness of the extractedterms.
We estimate the correctness of the pro-duced extractions by measuring Accuracy as thenumber of correctly tagged examples divided bythe total number of examples.Figure 2, shows the accuracy of the bootstrap-ping algorithm with graph re-ranking in blue andwithout graph re-ranking in red.
The figure showsthat graph re-ranking is effective and can separateout the erroneous extractions.
The overall extrac-tions produced by the algorithm are very precise,at rank 1500 the accuracy is 89%.!"#$%&&'(()#'(*&!"#$!"#%$!"&$!
"&%$'$'$ ($ )$ *$"(!
!$ "%!
!$ '!!
!$ '%!
!$""#%$"'$""&%$""&!$""#!$ "!$Figure 2: Term Extraction Accuracy.Next, in Table 2, we also show a detailed eval-uation of the extracted X and Z terms.
We de-fine five types according to which the humans canclassify the extracted terms.
The types are: Phys-icalObject, NonPhysicalObject, Event, State andOther.
We used Other to indicate erroneous ex-tractions or terms which do not belong to any ofthe previous four types.
The Kappa agreement forthe produced annotations is 0.80.X Cause A1 A2 Cause Z A1 A2PhysicalObj 82 75 PhysicalObj 15 20NonPhysicalObj 69 66 NonPhysicalObj 89 91Event 21 24 Event 72 72State 29 31 State 50 50Other 3 4 Other 5 4Acc.
.99 .98 Acc.
.98 .98Table 2: Term Classification.5.2 Comparison against Existing ResourcesTo compare the performance of our approach withknowledge bases that have been extracted in asimilar way (i.e., through pattern application overunstructured text), we consult the freely avail-able resources NELL (Carlson et al, 2009), Yago41(Suchanek et al, 2007) and TextRunner (Etzioniet al, 2005).
Although these bases contain mil-lions of facts, it turns out that NELL and Yagodo not have information for the cause-effect rela-tion.
While the online demo of TextRunner hasquery limitation, which returns only the top 1000snippets.
Since we do not have the complete andranked output of TextRunner, comparing results interms of relative recall and precision is impossibleand unfair.
Therefore, we decided to conduct anapplication driven evaluation and see whether theextracted knowledge can aid an NLP system.5.3 Application: Identifying SemanticRelations Between NominalsTask Description (Girju et al, 2007) introducedthe SemEval-1 Task 4 on the Classification of Se-mantic Relations between Nominals.
It consistsin given a sentence: ?People in Hawaii might befeeling <e1>aftershocks</e1> from that power-ful <e2>earthquake</e2> for weeks.
?, an NLPsystem should identify that the relationship be-tween the nominals earthquake and aftershocks iscause-effect.Data Set (Girju et al, 2007) created a dataset forseven different semantic relations, one of which iscause-effect.
For each relation, the nominals weremanually selected.
This resulted in the creationof 140 training and 80 testing cause-effect exam-ples.
From the train examples 52.14% were pos-itive (i.e.
correct cause-effect relation) and fromthe test examples 51.25% were positive.Evaluation and Results The objective of our ap-plication driven study is to measure the overlap ofthe cause-effect terms learned by our algorithmand those used by the humans for the creationof the SemEval-1 Task4 dataset.
There are 314unique terms in the train and test dataset for whichthe cause-effect relation must be identified.
Out ofthem 190 were also found by our algorithm.The 61% overlap shows that either our cause-effect extraction procedure can be used to auto-matically identify the relationship of the nominalsor it can be incorporated as an additional featureby a more robust system that relies on semanticand syntactic information.
In the future, the ex-tracted knowledge can be also used to create addi-tional training examples for the machine learningsystems working with this dataset.Table 3 shows some of the overlapping terms inour system and the (Girju et al, 2007) dataset.tremor, depression, anxiety, surgery,exposure, sore throat, fulfillment, yoga,frustration, inhibition, inflammation, fear,exhaustion, happiness, growth, evacuation,earthquake, blockage, zinc, vapour,sleep deprivation, revenue increase, quakeTable 3: Overlapping Terms.6 ConclusionWe have described a simple web based procedurefor learning cause-effect semantic relation.
Wehave shown that graph algorithms can successfullyre-rank and filter out the erroneous information.We have conduced three evaluations using humanannotators, comparing knowledge against existingrepositories and showing how the extracted knowl-edge can be used for the identification of relationsbetween pairs of nominals.The success of the described framework opensup many challenging directions.
We plan to ex-pand the extraction procedure with more lexico-syntactic patterns that express the cause-effect re-lation4 such as trigger, lead to, result among oth-ers and thus enrich the recall of the existing repos-itory.
We also want to develop an algorithm forextracting cause-effect terms from non contigu-ous positions like ?stress is another very impor-tant cause of diabetes?.
We are also interestedin studying how the extracted knowledge can aida commonsense causal reasoner (Gordon et al,2011; Gordon et al, 2012) in understanding thatif a girl wants to wear earrings it is more likely forher to get her ears pierced rather then get a tattoo.This example is taken from the Choice of Plausi-ble Alternatives (COPA) dataset5, which presentsa series of forced-choice questions such that eachquestion provides a premise and two viable causeor effect scenarios.
The goal is to choose a cor-rect answer that is the most plausible cause or ef-fect.
Similarly, the cause-effect repository can beused to support a variety of applications, includ-ing textual entailment, information extraction andquestion answeringAcknowledgmentsWe would like to thank the reviewers for their comments andsuggestions.
The research was supported by DARPA contractnumber FA8750-09-C-3705.4These patterns can be acquired from an existing para-phrase system.5http://people.ict.usc.edu/ gordon/copa.html42ReferencesMichael Cafarella, Michele Banko, and Oren Etzioni.2006.
Relational Web Search.
In World Wide WebConference, WWW 2006.Andrew Carlson, Justin Betteridge, Estevam R. Hr-uschka Jr., and Tom M. Mitchell.
2009.
Couplingsemi-supervised learning of categories and relations.In Proceedings of the NAACL HLT 2009 Workskopon Semi-supervised Learning for Natural LanguageProcessing.Timothy Chklovski and Patrick Pantel.
2004.
Verbo-cean: Mining the web for fine-grained semantic verbrelations.
In Proceedings of EMNLP 2004, pages33?40.Oren Etzioni, Michael Cafarella, Doug Downey, Ana-Maria Popescu, Tal Shaked, Stephen Soderland,Daniel S. Weld, and Alexander Yates.
2005.
Un-supervised named-entity extraction from the web:an experimental study.
Artificial Intelligence,165(1):91?134, June.Roxana Girju, Adriana Badulescu, and Dan Moldovan.2003.
Learning semantic constraints for the auto-matic discovery of part-whole relations.
In Proc.
ofthe 2003 Conference of the North American Chapterof the Association for Computational Linguistics onHuman Language Technology, pages 1?8.Roxana Girju, Preslav Nakov, Vivi Nastaste, Stan Sz-pakowicz, Peter Turney, and Deniz Yuret.
2007.SemEval-2007 task 04: Classification of semanticrelations between nominals.
In SemEval 2007.Andrew Gordon, Cosmin Bejan, and Kenji Sagae.2011.
Commonsense causal reasoning using mil-lions of personal stories.
In Proceedings of theTwenty-Fifth Conference on Artificial Intelligence(AAAI-11).Andrew Gordon, Zornitsa Kozareva, and MelissaRoemmele.
2012.
Semeval-2012 task 7: Choiceof plausible alternatives: An evaluation of common-sense causal reasoning.
In Proceedings of the 6th In-ternational Workshop on Semantic Evaluation (Se-mEval 2012).Marti Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proc.
of the14th conference on Computational linguistics, pages539?545.Eduard Hovy, Zornitsa Kozareva, and Ellen Riloff.2009.
Toward completeness in concept extractionand classification.
In Proceedings of the 2009 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 948?957.Boris Katz, Jimmy Lin, Daniel Loreto, Wesley Hilde-brandt, Matthew Bilotti, Sue Felshin, Aaron Fernan-des, Gregory Marton, and Federico Mora.
2003.Integrating web-based and corpus-based techniquesfor question answering.
In Proceedings of thetwelfth text retrieval conference (TREC), pages 426?435.Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.2008.
Semantic class learning from the web withhyponym pattern linkage graphs.
In Proceedings ofACL-08: HLT, pages 1048?1056.Marius Pasca.
2004.
Acquisition of categorized namedentities for web search.
In Proc.
of the thirteenthACM international conference on Information andknowledge management, pages 137?145.Marco Pennacchiotti and Patrick Pantel.
2006.
On-tologizing semantic relations.
In ACL-44: Proceed-ings of the 21st International Conference on Com-putational Linguistics and the 44th annual meetingof the Association for Computational Linguistics,pages 793?800.Ellen Riloff and Rosie Jones.
1999.
Learning dic-tionaries for information extraction by multi-levelbootstrapping.
In AAAI ?99/IAAI ?99: Proceedingsof the Sixteenth National Conference on Artificial in-telligence.Helmut Schmid.
1994.
Probabilistic part-of-speechtagging using decision trees.Fabian M. Suchanek, Gjergji Kasneci, and GerhardWeikum.
2007.
Yago: a core of semantic knowl-edge.
In WWW ?07: Proceedings of the 16th inter-national conference on World Wide Web, pages 697?706.Michael Thelen and Ellen Riloff.
2002.
A Bootstrap-ping Method for Learning Semantic Lexicons UsingExtraction Pattern Contexts.
In Proc.
of the 2002Conference on Empirical Methods in Natural Lan-guage Processing, pages 214?221.Fabio Massimo Zanzotto, Marco Pennacchiotti, andMaria Teresa Pazienza.
2006.
Discovering asym-metric entailment relations between verbs using se-lectional preferences.
In Proceedings of the 21st In-ternational Conference on Computational Linguis-tics and 44th Annual Meeting of the Association forComputational Linguistics, pages 849?856.43
