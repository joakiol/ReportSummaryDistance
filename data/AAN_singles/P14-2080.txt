Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 488?494,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsLearning Translational and Knowledge-based Similaritiesfrom Relevance Rankings for Cross-Language RetrievalShigehiko Schamoni and Felix Hieber and Artem Sokolov and Stefan RiezlerDepartment of Computational LinguisticsHeidelberg University, 69120 Heidelberg, Germany{schamoni,hieber,sokolov,riezler}@cl.uni-heidelberg.deAbstractWe present an approach to cross-languageretrieval that combines dense knowledge-based features and sparse word transla-tions.
Both feature types are learned di-rectly from relevance rankings of bilin-gual documents in a pairwise rankingframework.
In large-scale experiments forpatent prior art search and cross-lingual re-trieval in Wikipedia, our approach yieldsconsiderable improvements over learning-to-rank with either only dense or onlysparse features, and over very competitivebaselines that combine state-of-the-art ma-chine translation and retrieval.1 IntroductionCross-Language Information Retrieval (CLIR) forthe domain of web search successfully lever-ages state-of-the-art Statistical Machine Transla-tion (SMT) to either produce a single most prob-able translation, or a weighted list of alternatives,that is used as search query to a standard searchengine (Chin et al, 2008; Ture et al, 2012).
Thisapproach is advantageous if large amounts of in-domain sentence-parallel data are available to trainSMT systems, but relevance rankings to train re-trieval models are not.The situation is different for CLIR in specialdomains such as patents or Wikipedia.
Paral-lel data for translation have to be extracted withsome effort from comparable or noisy parallel data(Utiyama and Isahara, 2007; Smith et al, 2010),however, relevance judgments are often straight-forwardly encoded in special domains.
For ex-ample, in patent prior art search, patents grantedat any patent office worldwide are considered rel-evant if they constitute prior art with respect tothe invention claimed in the query patent.
Sincepatent applicants and lawyers are required to listrelevant prior work explicitly in the patent appli-cation, patent citations can be used to automati-cally extract large amounts of relevance judgmentsacross languages (Graf and Azzopardi, 2008).
InWikipedia search, one can imagine a Wikipediaauthor trying to investigate whether a Wikipediaarticle covering the subject the author intends towrite about already exists in another language.Since authors are encouraged to avoid orphan arti-cles and to cite their sources, Wikipedia has a richlinking structure between related articles, whichcan be exploited to create relevance links betweenarticles across languages (Bai et al, 2010).Besides a rich citation structure, patent docu-ments and Wikipedia articles contain a numberof further cues on relatedness that can be ex-ploited as features in learning-to-rank approaches.For monolingual patent retrieval, Guo and Gomes(2009) and Oh et al (2013) advocate the use ofdense features encoding domain knowledge oninventors, assignees, location and date, togetherwith dense similarity scores based on bag-of-wordrepresentations of patents.
Bai et al (2010) showthat for the domain of Wikipedia, learning a sparsematrix of word associations between the query anddocument vocabularies from relevance rankings isuseful in monolingual and cross-lingual retrieval.Sokolov et al (2013) apply the idea of learninga sparse matrix of bilingual phrase associationsfrom relevance rankings to cross-lingual retrievalin the patent domain.
Both show improvementsof learning-to-rank on relevance data over SMT-based approaches on their respective domains.The main contribution of this paper is a thor-ough evaluation of dense and sparse featuresfor learning-to-rank that have so far been usedonly monolingually or only on either patents orWikipedia.
We show that for both domains,patents and Wikipedia, jointly learning bilingualsparse word associations and dense knowledge-based similarities directly on relevance ranked488data improves significantly over approaches thatuse either only sparse or only dense features, andover approaches that combine query translationby SMT with standard retrieval in the target lan-guage.
Furthermore, we show that our approachcan be seen as supervised model combinationthat allows to combine SMT-based and ranking-based approaches for further substantial improve-ments.
We conjecture that the gains are due toorthogonal information contributed by domain-knowledge, ranking-based word associations, andtranslation-based information.2 Related WorkCLIR addresses the problem of translating or pro-jecting a query into the language of the documentrepository across which retrieval is performed.
Ina direct translation approach (DT), a state-of-the-art SMT system is used to produce a single besttranslation that is used as search query in the targetlanguage.
For example, Google?s CLIR approachcombines their state-of-the-art SMT system withtheir proprietary search engine (Chin et al, 2008).Alternative approaches avoid to solve the hardproblem of word reordering, and instead rely ontoken-to-token translations that are used to projectthe query terms into the target language with aprobabilistic weighting of the standard term tf-idf scheme.
Darwish and Oard (2003) termedthis method the probabilistic structured query ap-proach (PSQ).
The advantage of this techniqueis an implicit query expansion effect due to theuse of probability distributions over term trans-lations (Xu et al, 2001).
Ture et al (2012)brought SMT back into this paradigm by pro-jecting terms from n-best translations from syn-chronous context-free grammars.Ranking approaches have been presented byGuo and Gomes (2009) and Oh et al (2013).Their method is a classical learning-to-rank setupwhere pairwise ranking is applied to a few hun-dred dense features.
Methods to learn sparseword-based translation correspondences from su-pervised ranking signals have been presented byBai et al (2010) and Sokolov et al (2013).
Bothapproaches work in a cross-lingual setting, the for-mer on Wikipedia data, the latter on patents.Our approach extends the work of Sokolov etal.
(2013) by presenting an alternative learning-to-rank approach that can be used for supervisedmodel combination to integrate dense and sparsefeatures, and by evaluating both approaches oncross-lingual retrieval for patents and Wikipedia.This relates our work to supervised model merg-ing approaches (Sheldon et al, 2011).3 Translation and Ranking for CLIRSMT-based Models.
We will refer to DT andPSQ as SMT-based models that translate a query,and then perform monolingual retrieval usingBM25.
Translation is agnostic of the retrieval task.Linear Ranking for Word-Based Models.
Letq ?
{0, 1}Qbe a query and d ?
{0, 1}Dbe a doc-ument where the jthvector dimension indicates theoccurrence of the jthword for dictionaries of sizeQ and D. A linear ranking model is defined asf(q,d) = q>Wd =Q?i=1D?j=1qiWijdj,where W ?
IRQ?Dencodes a matrix of ranking-specific word associations (Bai et al, 2010) .
Weoptimize this model by pairwise ranking, whichassumes labeled data in the form of a set R of tu-ples (q,d+,d?
), where d+is a relevant (or higherranked) document and d?an irrelevant (or lowerranked) document for query q.
The goal is tofind a weight matrix W such that an inequalityf(q,d+) > f(q,d?)
is violated for the fewestnumber of tuples from R. We present two meth-ods for optimizing W in the following.Pairwise Ranking using Boosting (BM).
TheBoosting-based Ranking baseline (Freund et al,2003) optimizes an exponential loss:Lexp=?(q,d+,d?)?RD(q,d+,d?)ef(q,d?)?f(q,d+),whereD(q,d+,d?)
is a non-negative importancefunction on tuples.
The algorithm of Sokolov etal.
(2013) combines batch boosting with baggingover a number of independently drawn bootstrapdata samples fromR.
In each step, the single wordpair feature is selected that provides the largest de-crease of Lexp.
The found corresponding modelsare averaged.
To reduce memory requirements weused random feature hashing with the size of thehash of 30 bits (Shi et al, 2009).
For regulariza-tion we rely on early stopping.Pairwise Ranking with SGD (VW).
The sec-ond objective is an `1-regularized hinge loss:Lhng=?(q,d+,d?)?R(f(q,d+)?
f(q,d?
))++ ?||W ||1,489where (x)+= max(0, 1 ?
x) and ?
is the regu-larization parameter.
This newly added model uti-lizes the standard implementation of online SGDfrom the Vowpal Wabbit (VW) toolkit (Goel et al,2008) and was run on a data sample of 5M to 10Mtuples from R. On each step, W is updated witha scaled gradient vector ?WLhngand clipped toaccount for `1-regularization.
Memory usage wasreduced using the same hashing technique as forboosting.Domain Knowledge Models.
Domain knowl-edge features for patents were inspired by Guoand Gomes (2009): a feature fires if two patentsshare similar aspects, e.g.
a common inventor.
Aswe do not have access to address data, we omitgeolocation features and instead add features thatevaluate similarity w.r.t.
patent classes extractedfrom IPC codes.
Documents within a patent sec-tion, i.e.
the topmost hierarchy, are too diverseto provide useful information but more detailedclasses and the count of matching classes do.For Wikipedia, we implemented features thatcompare the relative length of documents, num-ber of links and images, the number of commonlinks and common images, and Wikipedia cat-egories: Given the categories associated with aforeign query, we use the language links on theWikipedia category pages to generate a set of?translated?
English categories S. The English-side category graph is used to construct sets ofsuper- and subcategories related to the candidatedocument?s categories.
This expansion is done inboth directions for two levels resulting in 5 cat-egory sets.
The intersection between target setTnand the source category set S reflects the cat-egory level similarity between query and docu-ment, which we calculate as a mutual containmentscore sn=12(|S ?
Tn|/|S| + |S ?
Tn|/|Tn|) forn ?
{?2,?1, 0,+1,+2} (Broder, 1997).Optimization for these additional models in-cluding domain knowledge features was done byoverloading the vector representation of queries qand documents d in the VW linear learner: Insteadof sparse word-based features, q and d are rep-resented by real-valued vectors of dense domain-knowledge features.
Optimization for the over-loaded vectors is done as described above for VW.4 Model CombinationCombination by Borda Counts.
The baselineconsensus-based voting Borda Count procedureendows each voter with a fixed amount of votingpoints which he is free to distribute among thescored documents (Aslam and Montague, 2001;Sokolov et al, 2013).
The aggregate score fortwo rankings f1(q,d) and f2(q,d) for all (q,d)in the test set is then a simple linear interpolation:fagg(q,d) = ?f1(q,d)?df1(q,d)+(1??)f2(q,d)?df2(q,d).
Pa-rameter ?
was adjusted on the dev set.Combination by Linear Learning.
In order toacquire the best combination of more than twomodels, we created vectors of model scores alongwith domain knowledge features and reused theVW pairwise ranking approach.
This meansthat the vector representation of queries q anddocuments d in the VW linear learner is over-loaded once more: In addition to dense domain-knowledge features, we incorporate arbitraryranking models as dense features whose value isthe score of the ranking model.
Training data wassampled from the dev set and processed with VW.5 DataPatent Prior Art Search (JP-EN).
We useBoostCLIR1, a Japanese-English (JP-EN) corpusof patent abstracts from the MAREC and NTCIRdata (Sokolov et al, 2013).
It contains automati-cally induced relevance judgments for patent ab-stracts (Graf and Azzopardi, 2008): EN patentsare regarded as relevant with level (3) to a JP querypatent, if they are in a family relationship (e.g.,same invention), cited by the patent examiner (2),or cited by the applicant (1).
Statistics on the rank-ing data are given in Table 1.
On average, queriesand documents contain about 5 sentences.Wikipedia Article Retrieval (DE-EN).
The in-tuition behind our Wikipedia retrieval setup is asfollows: Consider the situation where the German(DE) Wikipedia article on geological sea stacksdoes not yet exist.
A native speaker of Ger-man with profound knowledge in geology intendsto write it, naming it ?Brandungspfeiler?, whileseeking to align its structure with the EN counter-part.
The task of a CLIR engine is to return rele-vant EN Wikipedia articles that may describe thevery same concept (Stack (geology)), or relevantinstances of it (Bako National Park, Lange Anna).The information need may be paraphrased as ahigh-level definition of the topic.
Since typicallythe first sentence of any Wikipedia article is such1www.cl.uni-heidelberg.de/boostclir490#q #d #d+/q #words/qPatents (JP-EN)train 107,061 888,127 13.28 178.74dev 2,000 100,000 13.24 181.70test 2,000 100,000 12.59 182.39Wikipedia (DE-EN)train 225,294 1,226,741 13.04 25.80dev 10,000 113,553 12.97 25.75test 10,000 115,131 13.22 25.73Table 1: Ranking data statistics: number of queries and doc-uments, avg.
number of relevant documents per query, avg.number of words per query.a well-formed definition, this allows us to extracta large set of one sentence queries from Wikipediaarticles.
For example: ?Brandungspfeiler sind voreiner Kliffk?uste aufragende Felsent?urme und ver-gleichbare Formationen, die durch Brandungsero-sion gebildet werden.
?2Similar to Bai et al (2010)we induce relevance judgments by aligning DEqueries with their EN counterparts (?mates?)
viathe graph of inter-language links available in arti-cles and Wikidata3.
We assign relevance level (3)to the EN mate and level (2) to all other EN ar-ticles that link to the mate, and are linked by themate.
Instead of using all outgoing links from themate, we only use articles with bidirectional links.To create this data4we downloaded XML andSQL dumps of the DE and EN Wikipedia from,resp., 22ndand 4thof November 2013.
Wikipediamarkup removal and link extraction was carriedout using the Cloud9 toolkit5.
Sentence extrac-tion was done with NLTK6.
Since Wikipedia arti-cles vary greatly in length, we restricted EN doc-uments to the first 200 words after extracting thelink graph to reduce the number of features for BMand VW models.
To avoid rendering the task tooeasy for literal keyword matching of queries aboutnamed entities, we removed title words from theGerman queries.
Statistics are given in Table 1.Preprocessing Ranking Data.
In addition tolowercasing and punctuation removal, we appliedCorrelated Feature Hashing (CFH), that makescollisions more likely for words with close mean-ing (Bai et al, 2010).
For patents, vocabulariescontained 60k and 365k words for JP and EN.Filtering special symbols and stopwords reducedthe JP vocabulary size to 50k (small enough notto resort to CFH).
To reduce the EN vocabulary2de.wikipedia.org/wiki/Brandungspfeiler3www.wikidata.org/4www.cl.uni-heidelberg.de/wikiclir5lintool.github.io/Cloud9/index.html6www.nltk.org/to a comparable size, we applied similar prepro-cessing and CFH with F=30k and k=5.
Since forWikipedia data, the DE and EN vocabularies wereboth large (6.7M and 6M), we used the same filter-ing and preprocessing as for the patent data beforeapplying CFH with F=40k and k=5 on both sides.Parallel Data for SMT-based CLIR.
For bothtasks, DT and PSQ require an SMT baselinesystem trained on parallel corpora that are dis-junct from the ranking data.
A JP-EN sys-tem was trained on data described and prepro-cessed by Sokolov et al (2013), consisting of1.8M parallel sentences from the NTCIR-7 JP-ENPatentMT subtask (Fujii et al, 2008) and 2k par-allel sentences for parameter development fromthe NTCIR-8 test collection.
For Wikipedia, wetrained a DE-EN system on 4.1M parallel sen-tences from Europarl, Common Crawl, and News-Commentary.
Parameter tuning was done on 3kparallel sentences from the WMT?11 test set.6 ExperimentsExperiment Settings.
The SMT-based modelsuse cdec (Dyer et al, 2010).
Word align-ments were created with mgiza (JP-EN) andfast align (Dyer et al, 2013) (DE-EN).
Lan-guage models were trained with the KenLMtoolkit (Heafield, 2011).
The JP-EN system usesa 5-gram language model from the EN side of thetraining data.
For the DE-EN system, a 4-grammodel was built on the EN side of the trainingdata and the EN Wikipedia documents.
Weightsfor the standard feature set were optimized usingcdec?s MERT (JP-EN) and MIRA (DE-EN) im-plementations (Och, 2003; Chiang et al, 2008).PSQ on patents reuses settings found by Sokolovet al (2013); settings for Wikipedia were adjustedon its dev set (n=1000, ?=0.4, L=0, C=1).Patent retrieval for DT was done by sentence-wise translation and subsequent re-joining to formone query per patent, which was ranked against thedocuments using BM25.
For PSQ, BM25 is com-puted on expected term and document frequencies.For ranking-based retrieval, we compare severalcombinations of learners and features (Table 2).VW denotes a sparse model using word-based fea-tures trained with SGD.
BM denotes a similarmodel trained using Boosting.
DK denotes VWtraining of a model that represents queries q anddocuments d by dense domain-knowledge fea-tures instead of by sparse word-based vectors.
In491order to simulate pass-through behavior of out-of-vocabulary terms in SMT systems, additional fea-tures accounting for source and target term iden-tity were added to DK and BM models.
The pa-rameter ?
for VW was found on dev set.
Statis-tical significance testing was performed using thepaired randomization test (Smucker et al, 2007).Borda denotes model combination by BordaCount voting where the linear interpolation pa-rameter is adjusted for MAP on the respective de-velopment sets with grid search.
This type ofmodel combination only allows to combine pairsof rankings.
We present a combination of SMT-based CLIR, DT+PSQ, a combination of denseand sparse features, DK+VW, and a combinationof both combinations, (DT+PSQ)+(DK+VW).LinLearn denotes model combination by over-loading the vector representation of queries q anddocuments d in the VW linear learner by incor-porating arbitrary ranking models as dense fea-tures.
In difference to grid search for Borda, opti-mal weights for the linear combination of incorpo-rated ranking models can be learned automatically.We investigate the same combinations of rank-ing models as described for Borda above.
We donot report combination results including the sparseBM model since they were consistently lower thanthe ones with the sparse VW model.Test Results.
Experimental results on test dataare given in Table 2.
Results are reportedwith respect to MAP (Manning et al, 2008),NDCG (J?arvelin and Kek?al?ainen, 2002), andPRES (Magdy and Jones, 2010).
Scores werecomputed on the top 1,000 retrieved documents.As can be seen from inspecting the two blocksof results, one for patents, one for Wikipedia, wefind the same system rankings on both datasets.
Inboth cases, as standalone systems, DT and PSQare very close and far better than any ranking ap-proach, irrespective of the objective function or thechoice of sparse or dense features.
Model combi-nation of similar models, e.g., DT and PSQ, givesminimal gains, compared to combining orthogo-nal models, e.g.
DK and VW.
The best result isachieved by combining DT and PSQ with DK andVW.
This is due to the already high scores of thecombined models, but also to the combination ofyet other types of orthogonal information.
Bordavoting gives the best result under MAP which isprobably due to the adjustment of the interpola-tion parameter for MAP on the development set.combination models MAP NDCG PRESPatents(JP-EN)standaloneDT 0.2554 0.5397 0.5680PSQ 0.2659 0.5508 0.5851DK 0.2203 0.4874 0.5171VW 0.2205 0.4989 0.4911BM 0.1669 0.4167 0.4665BordaDT+PSQ?0.2747?0.5618?0.5988DK+VW?0.3023?0.5980?0.6137(DT+PSQ)+(DK+VW)?0.3465?0.6420?0.6858LinLearnDT+PSQ??0.2707??0.5578??0.5941DK+VW??0.3283??0.6366??0.7104DT+PSQ+DK+VW??0.3739??0.6755?
?0.7599Wikipedia(DE-EN)standaloneDT 0.3678 0.5691 0.7219PSQ 0.3642 0.5671 0.7165DK 0.2661 0.4584 0.6717VW 0.1249 0.3389 0.6466BM 0.1386 0.3418 0.6145BordaDT+PSQ?0.3742?0.5777?0.7306DK+VW?0.3238?0.5484?0.7736(DT+PSQ)+(DK+VW)?0.4173?0.6333?0.8031LinLearnDT+PSQ??0.3718??0.5751??0.7251DK+VW??0.3436??0.5686??0.7914DT+PSQ+DK+VW?0.4137??0.6435?
?0.8233Table 2: Test results for standalone CLIR models using di-rect translation (DT), probabilistic structured queries (PSQ),sparse model with CFH (VW), sparse boosting model (BM),dense domain knowledge features (DK), and model combi-nations using Borda Count voting (Borda) or linear super-vised model combination (LinLearn).
Significant differences(at p=0.01) between aggregated systems and all its compo-nents are indicated by ?, between LinLearn and the respectiveBorda system by ?.Under NDCG and PRES, LinLearn achieves thebest results, showing the advantage of automati-cally learning combination weights that leads tostable results across various metrics.7 ConclusionSpecial domains such as patents or Wikipedia of-fer the possibility to extract cross-lingual rele-vance data from citation and link graphs.
Thesedata can be used to directly optimizing cross-lingual ranking models.
We showed on two differ-ent large-scale ranking scenarios that a supervisedcombination of orthogonal information sourcessuch as domain-knowledge, translation knowl-edge, and ranking-specific word associations byfar outperforms a pipeline of query translation andretrieval.
We conjecture that if these types of in-formation sources are available, a supervised rank-ing approach will yield superior results in other re-trieval scenarios as well.AcknowledgmentsThis research was supported in part by DFGgrant RI-2221/1-1 ?Cross-language Learning-to-Rank for Patent Retrieval?.492ReferencesJaved A. Aslam and Mark Montague.
2001.
Modelsfor metasearch.
In Proceedings of the ACM SIGIRConference on Research and Development in Infor-mation Retrieval (SIGIR?01), New Orleans, LA.Bing Bai, Jason Weston, David Grangier, Ronan Col-lobert, Kunihiko Sadamasa, Yanjun Qi, OlivierChapelle, and Kilian Weinberger.
2010.
Learningto rank with (a lot of) word features.
InformationRetrieval Journal, 13(3):291?314.Andrei Z. Broder.
1997.
On the resemblance and con-tainment of documents.
In Compression and Com-plexity of Sequences (SEQUENCES?97), pages 21?29.
IEEE Computer Society.David Chiang, Yuval Marton, and Philip Resnik.
2008.Online large-margin training of syntactic and struc-tural translation features.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP?08), Waikiki, Hawaii.Jeffrey Chin, Maureen Heymans, Alexandre Ko-joukhov, Jocelyn Lin, and Hui Tan.
2008.
Cross-language information retrieval.
Patent Application.US 2008/0288474 A1.Kareem Darwish and Douglas W. Oard.
2003.
Proba-bilistic structured query methods.
In Proceedings.of the ACM SIGIR Conference on Research andDevelopment in Information Retrieval (SIGIR?03),Toronto, Canada.Chris Dyer, Adam Lopez, Juri Ganitkevitch, JonathanWeese, Ferhan Ture, Phil Blunsom, Hendra Seti-awan, Vladimir Eidelman, and Philip Resnik.
2010.cdec: A decoder, alignment, and learning frameworkfor finite-state and context-free translation models.In Proceedings of the ACL 2010 System Demonstra-tions, Uppsala, Sweden.Chris Dyer, Victor Chahuneau, and Noah A. Smith.2013.
A simple, fast, and effective reparameteriza-tion of IBM Model 2.
In Proceedings of the Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, Atlanta, GA.Yoav Freund, Ray Iyer, Robert E. Schapire, and YoramSinger.
2003.
An efficient boosting algorithm forcombining preferences.
Journal of Machine Learn-ing Research, 4:933?969.Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, andTakehito Utsuro.
2008.
Overview of the patenttranslation task at the NTCIR-7 workshop.
In Pro-ceedings of NTCIR-7 Workshop Meeting, Tokyo,Japan.Sharad Goel, John Langford, and Alexander L. Strehl.2008.
Predictive indexing for fast search.
In Ad-vances in Neural Information Processing Systems,Vancouver, Canada.Erik Graf and Leif Azzopardi.
2008.
A methodol-ogy for building a patent test collection for priorart search.
In Proceedings of the 2nd Interna-tional Workshop on Evaluating Information Access(EVIA?08), Tokyo, Japan.Yunsong Guo and Carla Gomes.
2009.
Ranking struc-tured documents: A large margin based approach forpatent prior art search.
In Proceedings of the Inter-national Joint Conference on Artificial Intelligence(IJCAI?09), Pasadena, CA.Kenneth Heafield.
2011.
KenLM: faster and smallerlanguage model queries.
In Proceedings of theEMNLP 2011 Sixth Workshop on Statistical Ma-chine Translation (WMT?11), Edinburgh, UK.Kalervo J?arvelin and Jaana Kek?al?ainen.
2002.
Cumu-lated gain-based evaluation of IR techniques.
ACMTransactions in Information Systems, 20(4):422?446.Walid Magdy and Gareth J.F.
Jones.
2010.
PRES:a score metric for evaluating recall-oriented infor-mation retrieval applications.
In Proceedings of theACM SIGIR conference on Research and develop-ment in information retrieval (SIGIR?10), New York,NY.Christopher D. Manning, Prabhakar Raghavan, andHinrich Sch?utze.
2008.
Introduction to InformationRetrieval.
Cambridge University Press.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofthe 41st Meeting on Association for ComputationalLinguistics (ACL?03), Sapporo, Japan.Sooyoung Oh, Zhen Lei, Wang-Chien Lee, PrasenjitMitra, and John Yen.
2013.
CV-PCR: A context-guided value-driven framework for patent citationrecommendation.
In Proceedings of the Interna-tional Conference on Information and KnowledgeManagement (CIKM?13), San Francisco, CA.Daniel Sheldon, Milad Shokouhi, Martin Szummer,and Nick Craswell.
2011.
Lambdamerge: Mergingthe results of query reformulations.
In Proceedingsof WSDM?11, Hong Kong, China.Qinfeng Shi, James Petterson, Gideon Dror, JohnLangford, Alexander J. Smola, Alexander L. Strehl,and Vishy Vishwanathan.
2009.
Hash Kernels.
InProceedings of the 12th Int.
Conference on Artifi-cial Intelligence and Statistics (AISTATS?09), Irvine,CA.Jason R. Smith, Chris Quirk, and Kristina Toutanova.2010.
Extracting parallel sentences from compa-rable corpora using document level alignment.
InProceedings of Human Language Technologies: The11th Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics (NAACL-HLT?10), Los Angeles, CA.493Mark D. Smucker, James Allan, and Ben Carterette.2007.
A comparison of statistical significance testsfor information retrieval evaluation.
In Proceedingsof the 16th ACM conference on Conference on Infor-mation and Knowledge Management (CIKM ?07),New York, NY.Artem Sokolov, Laura Jehl, Felix Hieber, and StefanRiezler.
2013.
Boosting cross-language retrievalby learning bilingual phrase associations from rele-vance rankings.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Pro-cessing (EMNLP?13).Ferhan Ture, Jimmy Lin, and Douglas W. Oard.2012.
Combining statistical translation techniquesfor cross-language information retrieval.
In Pro-ceedings of the International Conference on Compu-tational Linguistics (COLING?12), Bombay, India.Masao Utiyama and Hitoshi Isahara.
2007.
AJapanese-English patent parallel corpus.
In Pro-ceedings of MT Summit XI, Copenhagen, Denmark.Jinxi Xu, Ralph Weischedel, and Chanh Nguyen.
2001.Evaluating a probabilistic model for cross-lingualinformation retrieval.
In Proceedings of the ACMSIGIR Conference on Research and Development inInformation Retrieval (SIGIR?01), New York, NY.494
