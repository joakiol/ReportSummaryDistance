Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics in a World of Social Media, pages 9?10,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsTowards Automatic Question Answering over Social Media by LearningQuestion Equivalence PatternsTianyong Hao1 Wenyin Liu Eugene AgichteinCity University of Hong Kong City University of Hong Kong  Emory University81 Tat Chee Avenue 81 Tat Chee Avenue 201 Dowman DriveKowloon, Hong Kong SAR Kowloon, Hong Kong SAR Atlanta, Georgia 30322 USAhaotianyong@gmail.com csliuwy@cityu.edu.hk eugene@mathcs.emory.eduAbstractMany questions submitted to CollaborativeQuestion Answering (CQA) sites have beenanswered before.
We propose an approach toautomatically generating an answer to suchquestions based on automatically learning toidentify ?equivalent?
questions.
Our maincontribution is an unsupervised method forautomatically learning question equivalencepatterns from CQA archive data.
These pat-terns can be used to match new questions totheir equivalents that have been answered be-fore, and thereby help suggest answers auto-matically.
We experimented with our methodapproach over a large collection of more than200,000 real questions drawn from the Yahoo!Answers archive, automatically acquiringover 300 groups of question equivalence pat-terns.
These patterns allow our method to ob-tain over 66% precision on automaticallysuggesting answers to new questions, signifi-cantly outperforming conventional baselineapproaches to question matching.1 IntroductionSocial media in general exhibit a rich variety ofinformation sources.
Question answering (QA) hasbeen particularly amenable to social media, as itallows a potentially more effective alternative toweb search by directly connecting users with theinformation needs to users willing to share the in-formation directly (Bian, 2008).
One of the usefulby-products of this process is the resulting largearchives of data ?
which in turn could be goodsources of information for automatic question an-swering.
Yahoo!
Answers, as a collaborative QAsystem (CQA), has acquired an archive of morethan 40 Million Questions and 500 Million an-swers, as of 2008 estimates.The main premise of this paper is that there aremany questions that are syntactically differentwhile semantically similar.
The key problem ishow to identify such question groups.
Our methodis based on the key observation that when the bestnon-trivial answers chosen by asker in the samedomain are exactly the same, the correspondingquestions are semantically similar.
Based on thisobservation, we propose answering new methodfor learning question equivalence patterns fromCQA archives.
First, we retrieve ?equivalent?question groups from a large dataset by groupingthem by the text of the best answers (as chosen bythe askers).
The equivalence patterns are then gen-erated by learning common syntactic and lexicalpatterns for each group.
To avoid generating pat-terns from questions that were grouped together bychance, we estimate the group?s topic diversity tofilter the candidate patterns.
These equivalencepatterns are then compared against newly submit-ted questions.
In case of a match, the new questioncan be answered by proposing the ?best?
answerfrom a previously answered equivalent question.We performed large-scale experiments over amore than 200,000 questions from Yahoo!
An-swers.
Our method generated over 900 equivalencepatterns in 339 groups and allows to correctly sug-gest an answer to a new question, roughly 70% ofthe time ?
outperforming conventional similarity-based baselines for answer suggestion.Moreover, for the newly submitted questions,our method can identify equivalent questions andgenerate equivalent patterns incrementally, whichcan greatly improve the feasibility of our method.2 Learning Equivalence PatternsWhile most questions that share exactly the same?best?
answer are indeed semantically equivalent,some may share the same answer by chance.
To???????????????
?1 Work done while visiting Emory University9filter out such cases, we propose an estimate ofTopical Diversity (TD), calculated based on theshared topics for all pairs of questions in the group.If the diversity is larger than a threshold, the ques-tions in this group are considered not equivalent,and no patterns are generated.
To calculate thismeasure, we consider as topics the ?notionalwords?
(NW) in the question, which are the headnouns and the heads of verb phrases recognized bythe OpenNLP parser.
Using these words as ?top-ics?, TD for a group of questions G is calculated as:?
?-= =<-?-=11 2)()1()1(2)(ninj jiji jiQQQQnnGTD UIwhere Qi and Qj are the notional words in eachquestion in within group G with n questions total.Based on the question groups, we can generateequivalence patterns to extend the matching cover-age ?
thus retrieving similar questions with differ-ent syntactic structure.
OpenNLP is used togenerate the basic syntactic structures by phrasechunking.
After that, only the chunks which con-tain NWs are analyzed to acquire the phrase labelsas the syntactic pattern.
Table 1 shows an exampleof a generated pattern.Question: What was the first book you discovered thatmade you think reading wasn't a complete waste of time?Pattern: [NP]-[VP]-[NP]-[NP]-[VP]-[VP]-[NP]-[VP]-?NW: (Disjoint: read waste time) (Shared: book think)Question: What book do you think everyone should haveat home?Pattern: [NP]-[NP]-[VP]-[NP]-[VP]-[PP]-[NP]NW: (Disjoint: do everyone have home) (Shared: bookthink)Table 1.
A group of equivalence patterns3 Experimental EvaluationOur dataset is 216,563 questions and 2,044,296answers crawled from Yahoo!
Answers.
From thiswe acquired 833 groups of similar questions dis-tributed in 65 categories.
After filtering by topicaldiversity, 339 groups remain to generate equiva-lence patterns.
These groups contain 979 questions,with, 2.89 questions per group on average.After that, we split our data into 413 questionsfor training (200 groups) and 566 questions, withrandomly selected an additional 10,000 questions,for testing (the remainder) to compare three vari-ants of our system Equivalence patterns only (EP),Notional words only (NW), and the weighted com-bination (EP+NW).
To match question, bothequivalence patterns and notional words are usedwith different weights.
The weight of pattern, dis-joint NW and shared NW are 0.7, 0.4 and 0.6 afterparameter training.
We then compare the variantsand results are reported in Table 2, showing thatEP+NW achieves the highest performance.Recall Precision F1 scoreEP 0.811 0.385 0.522NW 0.378 0.559 0.451EP+NW 0.726 0.663 0.693Table 2.
Performance comparison of three variantsUsing EP+NW as our best method, we nowcompare it to traditional similarity-based methodson whole question set.
TF*IDF-based vector spacemodel (TFIDF), and a more highly tuned Cosinemodel (that only keeps the same ?notional words?filtered by phrase chunking) are used as baselines.Figure 3 reports the results, which indicate thatEP+NW, outperforms both Cosine and TFIDF me-thods on all metrics.00.10 .20 .30 .40 .50 .60 .70 .8Recall Precisio n F1TFIDF(NW)COSINEEP+NWFigure 3.
Performance of EP+NW vs. baselinesOur work expands on previous significant ef-forts on CQA retrieval (e.g., Bian et al, Jeon et al,Kosseim et al).
Our contribution is a new unsu-pervised and effective method for learning ques-tion equivalence patterns that exploits the structureof the collaborative question answering archives ?an important part of social media.4 ReferencesBian, J., Liu, Y., Agichtein, E., and Zha, H. 2008.
Find-ing the right facts in the crowd: factoid question an-swering over social media.
WWW.Jeon, J., Croft, B.W.
and Lee, J.H.
2005.
Finding simi-lar questions in large question and answer archivesExport Find Similar.
CIKM.Kosseim, L. and Yousefi, J.
2008.Improving the per-formance of question answering with semanticallyequivalent answer patterns, Journal of Data &Knowledge Engineering.10
