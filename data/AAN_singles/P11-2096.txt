Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 546?551,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsAn Empirical Evaluation of Data-Driven Paraphrase Generation TechniquesDonald MetzlerInformation Sciences InstituteUniv.
of Southern CaliforniaMarina del Rey, CA, USAmetzler@isi.eduEduard HovyInformation Sciences InstituteUniv.
of Southern CaliforniaMarina del Rey, CA, USAhovy@isi.eduChunliang ZhangInformation Sciences InstituteUniv.
of Southern CaliforniaMarina del Rey, CA, USAczheng@isi.eduAbstractParaphrase generation is an important taskthat has received a great deal of interest re-cently.
Proposed data-driven solutions to theproblem have ranged from simple approachesthat make minimal use of NLP tools to morecomplex approaches that rely on numerouslanguage-dependent resources.
Despite all ofthe attention, there have been very few directempirical evaluations comparing the merits ofthe different approaches.
This paper empiri-cally examines the tradeoffs between simpleand sophisticated paraphrase harvesting ap-proaches to help shed light on their strengthsand weaknesses.
Our evaluation reveals thatvery simple approaches fare surprisingly welland have a number of distinct advantages, in-cluding strong precision, good coverage, andlow redundancy.1 IntroductionA popular idiom states that ?variety is the spice oflife?.
As with life, variety also adds spice and appealto language.
Paraphrases make it possible to expressthe same meaning in an almost unbounded numberof ways.
While variety prevents language from be-ing overly rigid and boring, it also makes it difficultto algorithmically determine if two phrases or sen-tences express the same meaning.
In an attempt toaddress this problem, a great deal of recent researchhas focused on identifying, generating, and harvest-ing phrase- and sentence-level paraphrases (Barzi-lay and McKeown, 2001; Bhagat and Ravichan-dran, 2008; Barzilay and Lee, 2003; Bannard andCallison-Burch, 2005; Callison-Burch, 2008; Linand Pantel, 2001; Pang et al, 2003; Pasca and Di-enes, 2005)Many data-driven approaches to the paraphraseproblem have been proposed.
The approaches vastlydiffer in their complexity and the amount of NLP re-sources that they rely on.
At one end of the spec-trum are approaches that generate paraphrases froma large monolingual corpus and minimally rely onNLP tools.
Such approaches typically make useof statistical co-occurrences, which act as a rathercrude proxy for semantics.
At the other end ofthe spectrum are more complex approaches that re-quire access to bilingual parallel corpora and mayalso rely on part-of-speech (POS) taggers, chunkers,parsers, and statistical machine translation tools.Constructing large comparable and bilingual cor-pora is expensive and, in some cases, impossible.Despite all of the previous research, there havenot been any evaluations comparing the quality ofsimple and sophisticated data-driven approaches forgenerating paraphrases.
Evaluation is not only im-portant from a practical perspective, but also froma methodological standpoint, as well, since it is of-ten more fruitful to devote attention to building uponthe current state-of-the-art as opposed to improv-ing upon less effective approaches.
Although themore sophisticated approaches have garnered con-siderably more attention from researchers, from apractical perspective, simplicity, quality, and flexi-bility are the most important properties.
But are sim-ple methods adequate enough for the task?The primary goal of this paper is to take a smallstep towards addressing the lack of comparativeevaluations.
To achieve this goal, we empirically546evaluate three previously proposed paraphrase gen-eration techniques, which range from very simpleapproaches that make use of little-to-no NLP orlanguage-dependent resources to more sophisticatedones that heavily rely on such resources.
Our eval-uation helps develop a better understanding of thestrengths and weaknesses of each type of approach.The evaluation also brings to light additional proper-ties, including the number of redundant paraphrasesgenerated, that future approaches and evaluationsmay want to consider more carefully.2 Related WorkInstead of exhaustively covering the entire spectrumof previously proposed paraphrasing techniques, ourevaluation focuses on two families of data-driven ap-proaches that are widely studied and used.
Morecomprehensive surveys of data-driven paraphrasingtechniques can be found in Androutsopoulos andMalakasiotis (2010) and Madnani and Dorr (2010).The first family of approaches that we considerharvests paraphrases from monolingual corpora us-ing distributional similarity.
The DIRT algorithm,proposed by Lin and Pantel (2001), uses parse treepaths as contexts for computing distributional sim-ilarity.
In this way, two phrases were consideredsimilar if they occurred in similar contexts withinmany sentences.
Although parse tree paths serve asrich representations, they are costly to construct andyield sparse representations.
The approach proposedby Pasca and Dienes (2005) avoided the costs asso-ciated with parsing by using n-gram contexts.
Giventhe simplicity of the approach, the authors were ableto harvest paraphrases from a very large collectionof news articles.
Bhagat and Ravichandran (2008)proposed a similar approach that used noun phrasechunks as contexts and locality sensitive hashingto reduce the dimensionality of the context vectors.Despite their simplicity, such techniques are suscep-tible to a number of issues stemming from the distri-butional assumption.
For example, such approacheshave a propensity to assign large scores to antonymsand other semantically irrelevant phrases.The second line of research uses comparable orbilingual corpora as the ?pivot?
that binds para-phrases together (Barzilay and McKeown, 2001;Barzilay and Lee, 2003; Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Pang et al,2003).
Amongst the most effective recent work,Bannard and Callison-Burch (2005) show how dif-ferent English translations of the same entry in astatistically-derived translation table can be viewedas paraphrases.
The recent work by Zhao et al(Zhao et al, 2009) uses a generalization of DIRT-style patterns to generate paraphrases from a bilin-gual parallel corpus.
The primary drawback of thesetype of approaches is that they require a consider-able amount of resource engineering that may not beavailable for all languages, domains, or applications.3 Experimental EvaluationThe goal of our experimental evaluation is to ana-lyze the effectiveness of a variety of paraphrase gen-eration techniques, ranging from simple to sophis-ticated.
Our evaluation focuses on generating para-phrases for verb phrases, which tend to exhibit morevariation than other types of phrases.
Furthermore,our interest in paraphrase generation was initiallyinspired by challenges encountered during researchrelated to machine reading (Barker et al, 2007).
In-formation extraction systems, which are key compo-nent of machine reading systems, can use paraphrasetechnology to automatically expand seed sets of re-lation triggers, which are commonly verb phrases.3.1 SystemsOur evaluation compares the effectiveness of thefollowing paraphrase harvesting approaches:PD: The basic distributional similarity-inspiredapproach proposed by Pasca and Dienes (2005)that uses variable-length n-gram contexts andoverlap-based scoring.
The context of a phraseis defined as the concatenation of the n-gramsimmediately to the left and right of the phrase.
Weset the minimum length of an n-gram context to be2 and the maximum length to be 3.
The maximumlength of a phrase is set to 5.BR: The distributional similarity approach proposedby Bhagat and Ravichandran (2008) that uses nounphrase chunks as contexts and locality sensitivehashing to reduce the dimensionality of the contex-tual vectors.547BCB-S: An extension of the Bannard Callison-Burch (Bannard and Callison-Burch, 2005)approach that constrains the paraphrases to have thesame syntactic type as the original phrase (Callison-Burch, 2008).
We constrained all paraphrases to beverb phrases.We chose these three particular systems becausethey span the spectrum of paraphrase approaches, inthat the PD approach is simple and does not rely onany NLP resources while the BCB-S approach is so-phisticated and makes heavy use of NLP resources.For the two distributional similarity approaches(PD and BR), paraphrases were harvested from theEnglish Gigaword Fourth Edition corpus and scoredusing the cosine similarity between PMI weightedcontextual vectors.
For the BCB-S approach, wemade use of a publicly available implementation1.3.2 Evaluation MethodologyWe randomly sampled 50 verb phrases from 1000news articles about terrorism and another 50 verbphrases from 500 news articles about Americanfootball.
Individual occurrences of verb phraseswere sampled, which means that more common verbphrases were more likely to be selected and that agiven phrase could be selected multiple times.
Thissampling strategy was used to evaluate the systemsacross a realistic sample of phrases.
To obtain aricher class of phrases beyond basic verb groups, wedefined verb phrases to be contiguous sequences oftokens that matched the following POS tag pattern:(TO | IN | RB | MD | VB)+.Following the methodology used in previousparaphrase evaluations (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Kok and Brock-ett, 2010), we presented annotators with two sen-tences.
The first sentence was randomly selectedfrom amongst all of the sentences in the evaluationcorpus that contain the original phrase.
The secondsentence was the same as the first, except the orig-inal phrase is replaced with the system generatedparaphrase.
Annotators were given the followingoptions, which were adopted from those describedby Kok and Brockett (2010), for each sentence pair:0) Different meaning; 1) Same meaning; revised is1Available at http://www.cs.jhu.edu/?ccb/.grammatically incorrect; and 2) Same meaning; re-vised is grammatically correct.
Table 1 shows threeexample sentence pairs and their corresponding an-notations according to the guidelines just described.Amazon?s Mechanical Turk service was used tocollect crowdsourced annotations.
For each para-phrase system, we retrieve (up to) 10 paraphrasesfor each phrase in the evaluation set.
This yieldsa total of 6,465 unique (phrase, paraphrase) pairsafter pooling results from all systems.
Each Me-chanical Turk HIT consisted of 12 sentence pairs.To ensure high quality annotations and help iden-tify spammers, 2 of the 12 sentence pairs per HITwere actually ?hidden tests?
for which the correctanswer was known by us.
We automatically rejectedany HITs where the worker failed either of these hid-den tests.
We also rejected all work from annotatorswho failed at least 25% of their hidden tests.
Wecollected a total of 51,680 annotations.
We rejected65% of the annotations based on the hidden test fil-tering just described, leaving 18,150 annotations forour evaluation.
Each sentence pair received a mini-mum of 1, a median of 3, and maximum of 6 anno-tations.
The raw agreement of the annotators (afterfiltering) was 77% and the Fleiss?
Kappa was 0.43,which signifies moderate agreement (Fleiss, 1971;Landis and Koch, 1977).The systems were evaluated in terms of coverageand expected precision at k. Coverage is definedas the percentage of phrases for which the systemreturned at least one paraphrase.
Expected precisionat k is the expected number of correct paraphrasesamongst the top k returned, and is computed as:E[p@k] =1kk?i=1piwhere pi is the proportion of positive annotationsfor item i.
When computing the mean expectedprecision over a set of input phrases, only thosephrases that generate one or more paraphrases isconsidered in the mean.
Hence, if precision wereto be averaged over all 100 phrases, then systemswith poor coverage would perform significantlyworse.
Thus, one should take a holistic view of theresults, rather than focus on coverage or precisionin isolation, but consider them, and their respectivetradeoffs, together.548Sentence Pair AnnotationA five-man presidential council for the independent state newly proclaimed in south Yemenwas named overnight Saturday, it was officially announced in Aden.0A five-man presidential council for the independent state newly proclaimed in south Yemenwas named overnight Saturday, it was cancelled in Aden.Dozens of Palestinian youths held rally in the Abu Dis Arab village in East Jerusalem toprotest against the killing of Sharif.1Dozens of Palestinian youths held rally in the Abu Dis Arab village in East Jerusalem inprotest of against the killing of Sharif.It says that foreign companies have no greater right to compensation ?
establishing debts at a1/1 ratio of the dollar to the peso ?
than Argentine citizens do.2It says that foreign companies have no greater right to compensation ?
setting debts at a 1/1ratio of the dollar to the peso ?
than Argentine citizens do.Table 1: Example annotated sentence pairs.
In each pair, the first sentence is the original and the second has a system-generated paraphrase filled in (denoted by the bold text).Method CLenient StrictP1 P5 P10 P1 P5 P10PD 86 .48 .42 .36 .25 .22 .19BR 84 .83 .65 .52 .16 .17 .15BCB-S 62 .63 .45 .34 .22 .17 .13Table 2: Coverage (C) and expected precision at k (Pk)under lenient and strict evaluation criteria.Two binarized evaluation criteria are reported.The lenient criterion allows for grammatical er-rors in the paraphrased sentence, while the strictcriterion does not.3.3 Basic ResultsTable 2 summarizes the results of our evaluation.For this evaluation, all 100 verb phrases were runthrough each system.
The paraphrases returned bythe systems were then ranked (ordered) in descend-ing order of their score, thus placing the highestscoring item at rank 1.
Bolded values represent thebest result for a given metric.As expected, the results show that the systemsperform significantly worse under the strict evalu-ation criteria, which requires the paraphrased sen-tences to be grammatically correct.
None of the ap-proaches tested used any information from the eval-uation sentences (other than the fact a verb phrasewas to be filled in).
Recent work showed that us-ing language models and/or syntactic clues from theevaluation sentence can improve the grammatical-ity of the paraphrased sentences (Callison-Burch,MethodLenient StrictP1 P5 P10 P1 P5 P10PD .26 .22 .20 .19 .16 .15BR .05 .10 .11 .04 .05 .05BCB-S .24 .25 .20 .17 .14 .10Table 3: Expected precision at k (Pk) when consideringredundancy under lenient and strict evaluation criteria.2008).
Such approaches could likely be used to im-prove the quality of all of the approaches under thestrict evaluation criteria.In terms of coverage, the distributional similarityapproaches performed the best.
In another set of ex-periments, we used the PD method to harvest para-phrases from a large Web corpus, and found that thecoverage was 98%.
Achieving similar coverage withresource-dependent approaches would likely requiremore human and machine effort.3.4 RedundancyAfter manually inspecting the results returned by thevarious paraphrase systems, we noticed that someapproaches returned highly redundant paraphrasesthat were of limited practical use.
For example,for the phrase ?were losing?, the BR system re-turned ?are losing?, ?have been losing?, ?have lost?,?lose?, ?might lose?, ?had lost?, ?stand to lose?,?who have lost?
and ?would lose?
within the top 10paraphrases.
All of these are simple variants thatcontain different forms of the verb ?lose?.
Underthe lenient evaluation criterion almost all of theseparaphrases would be marked as correct, since the549same verb is being returned with some grammati-cal modifications.
While highly redundant outputof this form may be useful for some tasks, for oth-ers (such as information extraction) is it more usefulto identify paraphrases that contain a diverse, non-redundant set of verbs.Therefore, we carried out another evaluationaimed at penalizing highly redundant outputs.
Foreach approach, we manually identified all of theparaphrases that contained the same verb as themain verb in the original phrase.
During evalua-tion, these ?redundant?
paraphrases were regardedas non-related.The results from this experiment are provided inTable 3.
The results are dramatically different com-pared to those in Table 2, suggesting that evaluationsthat do not consider this type of redundancy mayover-estimate actual system quality.
The percent-age of results marked as redundant for the BCB-S,BR, and PD approaches were 22.6%, 52.5%, and22.9%, respectively.
Thus, the BR system, whichappeared to have excellent (lenient) precision in ourinitial evaluation, returns a very large number of re-dundant paraphrases.
This remarkably reduces thelenient P1 from 0.83 in our initial evaluation to just0.05 in our redundancy-based evaluation.
The BCB-S and PD approaches return a comparable number ofredundant results.
As with our previous evaluation,the BCB-S approach tends to perform better underthe lenient evaluation, while PD is better under thestrict evaluation.
Estimated 95% confidence inter-vals show all differences between BCB-S and PDare statistically significant, except for lenient P10.Of course, existing paraphrasing approaches donot explicitly account for redundancy, and hence thisevaluation is not completely fair.
However, thesefindings suggest that redundancy may be an impor-tant issue to consider when developing and evalu-ating data-driven paraphrase approaches.
There arelikely other characteristics, beyond redundancy, thatmay also be important for developing robust, effec-tive paraphrasing techniques.
Exploring the spaceof such characteristics in a task-dependent manneris an important direction of future work.3.5 DiscussionIn all of our evaluations, we found that the simpleapproaches are surprisingly effective in terms of pre-cision, coverage, and redundancy, making them areasonable choice for an ?out of the box?
approachfor this particular task.
However, additional task-dependent comparative evaluations are necessary todevelop even deeper insights into the pros and consof the different types of approaches.From a high level perspective, it is also importantto note that the precision of these widely used, com-monly studied paraphrase generation approaches isstill extremely poor.
After accounting for redun-dancy, the best approaches achieve a precision at 1of less than 20% using the strict criteria and less than26% when using the lenient criteria.
This suggeststhat there is still substantial work left to be done be-fore the output of these systems can reliably be usedto support other tasks.4 Conclusions and Future WorkThis paper examined the tradeoffs between simpleparaphrasing approaches that do not make use of anyNLP resources and more sophisticated approachesthat use a variety of such resources.
Our evaluationdemonstrated that simple harvesting approaches farewell against more sophisticated approaches, achiev-ing state-of-the-art precision, good coverage, andrelatively low redundancy.In the future, we would like to see more em-pirical evaluations and detailed studies comparingthe practical merits of various paraphrase genera-tion techniques.
As Madnani and Dorr (Madnaniand Dorr, 2010) suggested, it would be beneficialto the research community to develop a standard,shared evaluation that would act to catalyze furtheradvances and encourage more meaningful compara-tive evaluations of such approaches moving forward.AcknowledgmentsThe authors gratefully acknowledge the support ofthe DARPA Machine Reading Program under AFRLprime contract no.
FA8750-09-C-3705.
Any opin-ions, findings, and conclusion or recommendationsexpressed in this material are those of the au-thors and do not necessarily reflect the view of theDARPA, AFRL, or the US government.
We wouldalso like to thank the anonymous reviewers for theirvaluable feedback and the Mechanical Turk workersfor their efforts.550ReferencesI.
Androutsopoulos and P. Malakasiotis.
2010.
A surveyof paraphrasing and textual entailment methods.
Jour-nal of Artificial Intelligence Research, 38:135?187.Colin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In Proceed-ings of the 43rd Annual Meeting on Association forComputational Linguistics, ACL ?05, pages 597?604,Morristown, NJ, USA.
Association for ComputationalLinguistics.Ken Barker, Bhalchandra Agashe, Shaw-Yi Chaw, JamesFan, Noah Friedland, Michael Glass, Jerry Hobbs,Eduard Hovy, David Israel, Doo Soon Kim, RutuMulkar-Mehta, Sourabh Patwardhan, Bruce Porter,Dan Tecuci, and Peter Yeh.
2007.
Learning by read-ing: a prototype system, performance baseline andlessons learned.
In Proceedings of the 22nd nationalconference on Artificial intelligence - Volume 1, pages280?286.
AAAI Press.Regina Barzilay and Lillian Lee.
2003.
Learning toparaphrase: an unsupervised approach using multiple-sequence alignment.
In Proceedings of the 2003 Con-ference of the North American Chapter of the Associ-ation for Computational Linguistics on Human Lan-guage Technology - Volume 1, NAACL ?03, pages 16?23, Morristown, NJ, USA.
Association for Computa-tional Linguistics.Regina Barzilay and Kathleen R. McKeown.
2001.
Ex-tracting paraphrases from a parallel corpus.
In Pro-ceedings of the 39th Annual Meeting on Associationfor Computational Linguistics, ACL ?01, pages 50?57,Morristown, NJ, USA.
Association for ComputationalLinguistics.Rahul Bhagat and Deepak Ravichandran.
2008.
Largescale acquisition of paraphrases for learning surfacepatterns.
In Proceedings of ACL-08: HLT, pages 674?682, Columbus, Ohio, June.
Association for Computa-tional Linguistics.Chris Callison-Burch.
2008.
Syntactic constraints onparaphrases extracted from parallel corpora.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing, EMNLP ?08, pages196?205, Morristown, NJ, USA.
Association for Com-putational Linguistics.Joseph L. Fleiss.
1971.
Measuring Nominal ScaleAgreement Among Many Raters.
Psychological Bul-letin, 76(5):378?382.Stanley Kok and Chris Brockett.
2010.
Hitting the rightparaphrases in good time.
In Human Language Tech-nologies: The 2010 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, HLT ?10, pages 145?153, Morris-town, NJ, USA.
Association for Computational Lin-guistics.J.
R. Landis and G. G. Koch.
1977.
The measurement ofobserver agreement for categorical data.
Biometrics,33(1):159?174, March.Dekang Lin and Patrick Pantel.
2001.
Discovery of in-ference rules for question-answering.
Nat.
Lang.
Eng.,7:343?360, December.Nitin Madnani and Bonnie J. Dorr.
2010.
Generatingphrasal and sentential paraphrases: A survey of data-driven methods.
Comput.
Linguist., 36:341?387.Bo Pang, Kevin Knight, and Daniel Marcu.
2003.Syntax-based alignment of multiple translations: ex-tracting paraphrases and generating new sentences.In Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology -Volume 1, NAACL ?03, pages 102?109, Morristown,NJ, USA.
Association for Computational Linguistics.Marius Pasca and Pter Dienes.
2005.
Aligning needlesin a haystack: Paraphrase acquisition across the web.In Robert Dale, Kam-Fai Wong, Jian Su, and Oi YeeKwong, editors, Natural Language Processing IJC-NLP 2005, volume 3651 of Lecture Notes in ComputerScience, pages 119?130.
Springer Berlin / Heidelberg.Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.2009.
Extracting paraphrase patterns from bilin-gual parallel corpora.
Natural Language Engineering,15(Special Issue 04):503?526.551
