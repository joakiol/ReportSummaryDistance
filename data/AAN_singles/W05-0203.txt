Proceedings of the 2nd Workshop on Building Educational Applications Using NLP,pages 17?20, Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005A real-time multiple-choice question generation for language testing?
a preliminary study?Ayako HoshinoInterfaculty Initiative in Information StudiesUniversity of Tokyo7-3-1 Hongo, Bunkyo, Tokyo,113-0033, JAPANqq36126@iii.u-tokyo.ac.jpHiroshi NakagawaInformation Technology CenterUniversity of Tokyo7-3-1 Hongo, Bunkyo, Tokyo,113-0033, JAPANnakagawa@dl.itc.u-tokyo.ac.jpAbstractAn automatic generation of multiple-choice questions is one of the promisingexamples of educational applications ofNLP techniques.
A machine learning ap-proach seems to be useful for this pur-pose because some of the processes canbe done by classification.
Using basic ma-chine learning algorithms as Naive Bayesand K-Nearest Neighbors, we have devel-oped a real-time system which generatesquestions on English grammar and vocab-ulary from on-line news articles.
This pa-per describes the current version of oursystem and discusses some of the issueson constructing this kind of system.1 IntroductionMultiple-choice question exams are widely used andare effective to assess students?
knowledge, how-ever it is costly to manually produce those questions.Naturally, this kind of task should be done with ahelp of computer.Nevertheless, there have been very few attemptsto generate multiple-choice questions automatically.Mitkov et al(2003) generated questions for a lin-guistics exam in a semi-automatic way and evalu-ated that it exceeds manually made ones in cost andis at least equivalent in quality.
There are someother researches that involve generating questionswith multiple alternatives (Dicheva and Dimitrova,1998).
But to the best of our knowledge, no attempthas been made to generate this kind of questions ina totally automatic way.This paper presents a novel approach to generatemultiple-choice questions using machine learningtechniques.
The questions generated are those of fill-in-the-blank type, so it does not involve transform-ing declarative sentences into question sentences asin Mitkov?s work.
This simplicity makes the methodto be language independent.Although this application can be very versatile, inthat it can be used to test any kind of knowledge as inhistory exams, as a purpose of this research we limitourselves to testing student?s proficiency in a foreignlanguage.
One of the purposes of this research is toautomatically extract important words or phrases ina text for a learner of the language.2 System DesignThe system we have implemented works in a sim-ple pipelined manner; it takes an HTML file andturns it into the one of quiz session.
The processof converting the input to multiple-choice questionsincludes extracting features, deciding the blank po-sitions, and choosing the wrong alternatives (whichare called distractors), which are all done in a mo-ment when the user feeds the input.
When the usersubmits their answer, it shows the text with the cor-rect answers as well as an overall feed back.3 MethodologyThe process of deciding blank positions in a giventext follows a standard machine learning framework,which is first training a classifier on a training data17Table 1: the full list of test instances classified as true in test-on-traincertainty a test instance (sentence with a blank) the answer0.808 Joseph is preparing for tomorrow?s big [ ] to the president.
presentation0.751 Ms. Singh listened [ ] to the president?s announcement.
carefully0.744 The PR person is the one in charge of [ ] meetings and finding accommoda-tions for our associates.scheduling0.73 Ms. Havlat received a memo from the CEO [ ] the employees?
conduct.
regarding0.718 The amount of money in the budget decreased [ ] over the past year.
significantly0.692 Mr. Gomez is [ ] quickly; however it will be a log time before he gets used tothe job.learning0.689 The boss can never get around to [ ] off his desk.
cleaning0.629 The interest rate has been increasingly [ ] higher.
getting0.628 Employees are [ ] to comply with the rules in the handbook.
asked0.62 The lawyer [ ] his case before the court.
presented0.59 The secretary was [ ] to correspond with the client immediately.
supposed0.576 The maintenance worker checked the machine before [ ] it on.
turning0.523 The [ ] manager?s office is across the corridor.
assistant(i.e.
TOEIC questions), then applying it on an un-seen test data, (i.e.
the input text).
In the current sys-tem, the mechanism of choosing distractors is imple-mented with the simplest algorithm, and its investi-gation is left to future work.3.1 Preparing the Training DataThe training data is a collection of fill-in-the-blankquestions from a TOEIC preparation book (Matsunoet al, 2000).
As shown in the box below, a ques-tion consists of a sentence with a missing word (orwords) and four alternatives one of among whichbest fits into the blank.Many people showed up early to [ ] for the posi-tion that was open.1.
apply 2. appliance 3. applies 4. applicationThe training instances are obtained from 100questions by shifting the blank position.
The orig-inal position is labeled as true, while sentences witha blank in a shifted position are at first labeled asfalse.
The instance shown above therefore yields in-stances [ ] people showed up early to apply for theposition that was open., Many [ ] showed up earlyto apply for the position that was open., and so on,all of which are labeled as false except the originalblank position.
1962 (100 true and 1862 false) in-stances were obtained.The label true here is supposed to indicate thatit is possible to make a question with the sentencewith a blank in the specified position, while manyof the shifted positions which are labeled false canalso be good blanks.
A semi-supervised learning(Chakrabarti, 2003) 1 is conducted in the followingmanner to retrieve the instances that are potentiallytrue among the ones initially classified as false.We retrieved the 13 instances (shown in Table 1.
)which had initially been labeled as false and classi-fied as true in a test-on-train result with a certainty 2of more than 0.5 with a Naive Bayes classifier 3.
Thelabels of those instances were changed to true beforere-training the classifier.
In this way, a training setwith 113 true instances was obtained.3.2 Deciding Blank PositionsFor the current system we use news articles fromBBC.com 4, which consist approximately 200-500words.
The test text goes through tagging and fea-ture extraction in the same manner as the training1Semi-supervised learning is a method to identify the classof unclassified instances in the dataset where only some of theinstances are classified.2The result of a classification of a instance is obtained alongwith a certainty value between 0.0 to 1.0 for each class, whichindicates how certain it is that an instance belongs to the class.3Seven features which are word, POS, POS of the previousword, POS of the next word, position in the sentence, sentencelength, word length and were used.4http://news.bbc.co.uk/18data, and the instances are classified into true orfalse.
The positions of the blanks are decided ac-cording to the certainty of the classification so theblanks (i.e.
questions) are generated as many as theuser has specified.3.3 Choosing DistractorsIn the current version of the system, the distractorsare chosen randomly from the same article exclud-ing punctuations and the same word as the other al-ternatives.4 Current systemThe real-time system we are presenting is imple-mented as a Java servlet, whose one of the mainscreens is shown below.
The tagger used here is theTree tagger (Schmid, 1994), which uses the Penn-Treebank tagset.Figure 1: a screen shot of the question session pagewith an enlarged answer selector.The current version of the system is avail-able at http://www.iii.u-tokyo.ac.jp/?qq36126/mcwa1/.
The interface of the systemconsists of three sequenced web pages, namely 1)theparameter selection page, 2)the quiz session pageand 3)the result page.The parameter selection page shows the list of thearticles which are linked from the top page of theBBC website, along with the option selectors fornumber of blanks (5-30) and the classifier (NaiveBayes or Nearest Neighbors).The question session page is shown in Figure 1.
Itdisplays the headline and the image from the chosenarticle under the title and a brief instruction.
Thealternatives are shown on option selectors, which areplaced in the article text.The result page shows the text with the right an-swers shown in green when the user?s choice is cor-rect, red when it is wrong.5 EvaluationTo examine the quality of the questions generatedby the current system, we have evaluated the blankpositions determined by a Naive Bayes classifier anda KNN classifier (K=3) with a certainty of more than50 percent in 10 articles.Among 3138 words in total, 361 blanks weremade and they were manually evaluated accordingto their possibility of being a multiple-choice ques-tion, with an assumption of having alternatives ofthe same part of speech.
The blank positions werecategorized into three groups, which are E (possibleto make a question), and D (difficult, but possible tomake a question), NG (not possible or not suitablee.g.
on a punctuation).
The guideline for decidingE or D was if a question is on a grammar rule, or itrequires more semantic understanding, for instance,a background knowledge 5.Table 2. shows the comparison of the number ofblank positions decided by the two classifiers, eachwith a breakdown for each evaluation.
The num-ber in braces shows the proportion of the blankswith a certain evaluation over the total number ofblanks made by the classifier.
The rightmost columnI shows the number of the same blank positions se-lected by both classifiers.The KNN classifier tends to be more accurate andseems to be more robust, although given the fact thatit produces less blanks.
The fact that an instance-based algorithm exceeds Naive Bayes, whose deci-sion depends on the whole data, can be ascribed toa mixed nature of the training data.
For example,blanks for grammar questions might have differentfeatures from ones for vocabulary questions.The result we sampled has exhibited anotherproblem of Naive Bayes algorithm.
In two articlesamong the data, it has shown the tendency to make ablank on be-verbs.
Naive Bayes tends to choose the5A blank on a verbs or a part of idioms (as [according] to)was evaluated as E, most of the blanks on an adverbs, and (as[now]) were D and a blank on a punctuation or a quotation markwas NG.19Table 2: The evaluation on the blank positions decided by a Naive Bayes (NB) and a KNN classifier.NB KNN Iblanks E(%) D(%) NG(%) blanks E(%) D(%) NG(%) blanksArticle1 69 44(63.8) 21(30.4) 4(5.8) 33 20(60.6) 11(33.3) 2(6.1) 18Article2 22 5(22.7) 3(13.6) 14(63.6) 8 5(62.5) 3(37.5) 0(0.0) 0Article3 38 21(55.3) 15(39.5) 2(5.3) 18 12(66.7) 5(27.8) 1(5.6) 8Article4 19 10(52.6) 9(47.4) 0(0.0) 9 7(77.8) 2(22.2) 0(0.0) 3Article5 28 18(64.3) 10(35.7) 0(0.0) 14 10(71.4) 4(28.6) 0(0.0) 6Article6 26 17(65.4) 8(30.8) 1(3.8) 11 6(54.5) 5(45.5) 0(0.0) 4Article7 18 9(50.0) 5(27.8) 4(22.2) 6 3(50.0) 3(50.0) 0(0.0) 3Article8 24 14(58.3) 9(37.5) 1(4.2) 5 3(60.0) 2(40.0) 0(0.0) 5Article9 20 16(80.0) 4(20.0) 0(0.0) 6 2(33.3) 4(66.7) 0(0.0) 4Article10 30 18(60.0) 12(40.0) 0(0.0) 14 11(78.6) 3(21.4) 0(0.0) 6294 172(58.5) 96(32.7) 26(8.8) 124 79(63.7) 42(33.9) 3(2.4) 57same word as a blank position, therefore generatesmany questions on the same word in one article.Another general problem of these methods wouldbe that the blank positions are decided without con-sideration of one another; the question will be some-times too difficult when another blank is next to orin the vicinity of the blank.6 Discussion and Future workFrom the problems of the current system, we canconclude that the feature set we have used is not suf-ficient.
It is necessary that we use larger numberof features, possibly including semantic ones, so ablank position would not depend on its superficialaspects.
Also, the training data should be examinedin more detail.As it was thought to be a criteria of evaluatinggenerated questions, if a question requires simply agrammatical knowledge or a farther knowledge (i.e.background knowledge) can be a critical property ofa generated question.
We should differentiate thefeatures from the ones which are used to generate,for example, history questions, which require ratherbackground knowledge.
Selecting suitable distrac-tors, which is left to future work, would be a moreimportant process in generating a question.
A se-mantic distance between an alternative and the rightanswer are suggested (Mitkov and Ha, 2003), to bea good measure to evaluate an alternative.
We areinvestigating on a method of measuring those dis-tances and a mechanism to retrieve best alternativesautomatically.7 ConclusionWe have presented a novel application of automat-ically generating fill-in-the-blank, multiple-choicequestions using machine learning techniques, aswell as a real-time system implemented.
Althoughit is required to explore more feature settings for theprocess of determining blank positions, and the pro-cess of choosing distractors needs more elaboration,the system has proved to be feasible.ReferencesSoumen Chakrabarti.
2003.
Mining the Web.
MorganKaufmann Publishers.Darina Dicheva and Vania Dimitrova.
1998.
An ap-proach to representation and extraction of terminolog-ical knowledge in icall.
In Journal of Computing andInformation Technology, pages 39 ?
52.Shuhou Matsuno, Tomoko Miyahara, and Yoshi Aoki.2000.
STEP-UP Bunpo mondai TOEIC TEST.
Kiri-hara Publisher.Ruslan Mitkov and Le An Ha.
2003.
Computer-aidedgeneration of multiple-choice tests.
In Proceedings ofthe HLT-NAACL 2003 Workshop on Building Educa-tional Applications Using Natural Language Process-ing, pages 17 ?
22, Edmonton, Canada, May.Helmut Schmid.
1994.
Probabilistic part-of-speech tag-ging using decision trees.
In Proceedings of Interna-tional Conference on New Methods in Language Pro-cessing, Manchester, UK, September.20
