Stochast i c  HPSGChris BrewLanguage Technology GroupHCRC, University of Edinburgh2 Buccleuch Place?
Edinburgh EH8 9LWScotland, UKemail: Chris.
Brew0edinburgh.
ac.
ukAbstractIn this paper we provide a probabilis-tic interpretation for typed feature struc-tures very similar to those used by Pol-lard and Sag.
We begin with a ver-sion of the interpretation which lacksa treatment of re-entrant feature struc-tures, then provide an extended interpre-tation which allows them.
We sketch al-gorithms allowing the numerical param-eters of our probabilistic interpretationsof HPSG to be estimated from corpora.1 Introduct ionThe purpose of our paper is to develop a princi-pled technique for attaching a probabilistic inter-pretation to feature structures.
Our techniquesapply to the feature structures described by Car-penter (Carpenter, 1992).
Since these structuresare the ones which are used in by Pollard andSag (Pollard and Sag, 1994) their relevance tocomputational grammars is apparent.
On the ba-sis of the usefulness of probabilistic ontext-freegrammars (Charniak, 1993, ch.
5), it is plausibleto assume that that the extension of probabilistictechniques to such structures will allow the ap-plication of known and new techniques of parseranking and grammar induction to more interest-ing grammars than has hitherto been the case.The paper is structured as follows.
We startby reviewing the training and use of probabilis-tic context-free grammars (PCFGs).
We then de:velop a technique to allow analogous probabilisticannotations on type hierarchies.
This gives us aclear account of the relationship between a largeclass of feature structures and their probabilities,but does not treat re-entrancy.
We conclude bysketching a technique which does treat such struc-tures.
While we know of previous work which as-sociates cores with feature structures (Kim, 1994)are not aware of any previous treatment whichmakes explicit the link to classical probability the-ory.We take a slightly unconventional perspectiveon feature structures, because it is easier to castour theory within the more general frameworkof incremental description refinement (Mellish,1988) than to exploit the usual metaphors ofconstraint-based grammar.
In fact we can affordto remain entirely agnostic about the means bywhich the HPSG grammar associates igns withlinguistic strings, because all that we need in or-der to train our stochastic procedures is a corpusof signs which are known to be valid descriptionsof strings.2 Probabil istic interpretation ofPCFGsWe review the standard probabilistic interpreta-tion of PCFGs 1A PCFG is a four-tuple < W,N, N1,R >, where W is a Set of terminal symbols{wl, .
.
.
,  w~}, N is a set of non-terminal symbols{N1, .
.
.
,N~},  N1 is the starting symbol and Ris a set of rules of the form N ~ ~ (J, where (Jis a string of terminals and non-terminals.
Eachrule has a probability P(N i --~ ~J) and the prob-abilities for all the rules that expand a given non-terminal must sum to one.
We associate probabil-ities with partial phrase markers, which are setsof terminal and non-terminal nodes generated bybeginning from the starting node successively ex-panding non-terminal leaves of the partial tree.Phrase markers are those partial phrase markerswhich have no non-terminal leaves.
Probabilitiesare assigned by the following inductive definition:?
P (N1)  = 1.?
If T is a partial phrase marker, and T'  is apartial phrase marker which differs from itonly in that a single non-terminal node N kin T has been expanded to ~'~ in T ', thenP(T') = P(T) ?
P(N~ ~ ~'~).In this definition R acts as a specification ofthe accessibility relationships which can hold be-tween nodes of the trees admitted by the gram-mar.
The rule probabilities pecify the cost of1 Our description is closely based on that given byCharniak(Charniak, 1993, p. 52 if)83making particular choices about the way in whichthe rules develop.
It is going to turn out thatan exactly analogous ystem of accessibility rela-tions is present in the probabilistic type hierar-chies which we define later.L imi ta t ions  of  PCFGs  The definition ofPCFGs implies that the probability of a phrasemarker depends only on the choice of rules usedin expanding non-terminal nodes.
In particular,the probability does not depend on the order inwhich the rules are applied.
This has the ar-guably unwelcome consequence that PCFGs areunable to make certain discriminations betweentrees which differ only in their configuration 2.The models developed in this paper build in simi-lar independence assumptions.
A large part of theart of probabilistic language modelling resides inthe management of the trade-off between descrip-tive power (which has the merit of allowing us tomake the discriminations which we want) and in-dependence assumptions (which have the merit ofmaking training practical by allowing us to treatsimilar situations as equivalent).The crucial advantage of PCFGs over CFGs isthat they can be trained and/or learned from cor-pora.
Readers for whom this fact is unfamiliar arereferred to Charniak's textbook (Charniak, 1993,Chapter 7).
We do not have space to recapitu-late the discussion of training which can be foundthere.
We do however illustrate the outcome oftraining.2.1 App ly ing  a PCFG to  a s imp le  corpusConsider the simple grammar in figure 1 and itstraining against he corpus in figure 2.
Since thereare 3 plural sentences and only 2 singular sen-tences, the optimal set of parameters will reflectthe distribution found in the corpus, as shownin figure 3 One might have hoped that the ra-tio P(np-sing\[np)/P(np-pl\[np) would be 2/3, butit is instead V / -~.
This is a consequence of theassumption of independence.
Effectively the algo-r ithm is ascribing the difference in distribution ofsingular and plural sentences to the joint effect oftwo independent decisions.
What we would reallylike it to do is to recognize that the two apparentlyindependent decisions are (in effect) one and thesame.
Also, because the grammar has no meansof enforcing number agreement, he system sys-tematically prefers plurals to singulars, even whendoing this will lead to agreement clashes.
Thus"buses stop" has estimated 0.55 x 0.55 = 0.3025,"bus stop" and "buses stops" both have proba-bility 0.55 x 0.45 = 0.2475 and "bus stops" hasprobabil ity 0.45 x 0.45 = 0.2025.
This behaviouris clearly unmotivated by the corpus, and arises~The most obvious case is prepositional-phraseattachment.purely because of the inadequacy of the proba-bilistic model.3 Probabilistic type hierarchiesALE s ignatures  Carpenter's ALE (Carpenter,1993) allows the user to define the type hierarchyof a grammar by writing a collection of clauseswhich together denote an inheritance hierarchy, aset of features and a set of appropriateness condi-tions.
An example of such a hierarchy is given inALE syntax in figure 4.What  the  ALE  s ignature  tel ls us The inher-itance information tells us that a sign is a forcedchoice between a sentence and a phrase, that aphrase is a forced choice between a noun-phrase(np) and a verb-phrase (vp) and that number val-ues (num) are partitioned into singular (s ing) andplural (pl).
The features which are defined arele f t , r ight ,  and nura, and the appropriateness in-formation says that the feature num introduces anew instance of the type num on all phrases ,  andthat le f t  and r ight  introduce np and vp respec-tively on sentences .The para l le l  w i th  PCFGs  The parallel whichmakes it possible to apply the PCFG trainingscheme almost unchanged is that the sub-types ofa given super-type partition the feature structuresof that type in just the same way that the differ-ent rules which expand a given non-terminal N ofthe PCFG partition the space of trees whose top-most node is N. Equally, the features defined inthe hierarchy act as an accessibility relation be-tween nodes in a way which is for our purposesentirely equivalent to the way in which the righthand sides of the rules introduce new nodes intopartial phrase markers 3.
The hierarchy in figure 4is related to but not isomorphic with the grammarin figure 1.One difference is that num is explicitly intro-duced as a feature in the hierarchy, where at isonly implicitly present in the original grammar.The other difference is the use of le f t  and r ightas models of the dominance relationships betweennodes.4 A probabilistic interpretation oftyped feature-structuresFor our purposes, a probabilistic type hierarchy(PTH) is a four-tuple< MT, NT, NT1, I >where MT is a set of maximal types 4 {t 1 .
.
.
.
,to~},NT is a set of non-maximal types {T1, .
.
.
,  TV},3Each rule of a PCFG also specifies a total orderingover the nodes which it introduces, but the trainingalgorithm does not rely on this fact4We follow Carpenter's convention for types.
Thebottom node is the one containing no information, andthe maximal nodes are the ones containing the maxi-84bikecarlorrybikescarslorriesstopsstops ---* np vpnp --* np-sing I np-plvp --* vp-sing I vp-plnp-sing bus np-singnp-sing cat np-singnp-singnp-pl buses np-plnp-pl cats np-plnp-plvp-sing crosses vp-singvp-pl cross vp-plFigure 1: A simple grammarcar stopsbikes stopbus stopscats crosslorries stopFigure 2."
A simple corpusP(np vpls ) = 1.0P(np-singlnp ) = 0.45P(np-pl\[np) = 0.55P(vp-sing\[vp) = 0.45P(vp-pllvp ) = 0.55Figure 3: The results of training a PCFGbot sub \ [s ign,num\] .s ign  sub \ [ sentence ,phrase \ ] .sentence sub \[\]intro \[left : np,right : vp\].phrase sub \[np,vp\]intro \[num:num\] .np sub \[\].vp sub \[\].num sub \[sing,pl\].sing sub \[\].pl sub \[\].Figure 4: An ALE signature85NT1 is the starting symbol and I is a set of in-troduction relationships of the form (T ~ ~ TJ)~k, where ~J is a multiset of maximal and non-maximal types.
Each introduction relationshiphas a probabil ity P( (T  i ~ TJ) --+ ~k) and theprobabilities for all the introduction relationshipsthat apply to a given non-maximal type must sumto one.As things stand this definition is nearly isomor-phic to that given for PCFGs, with the major dif-ferences being two changes which move us fromrules to introduction relationships.
Firstly, werelax the stipulation that the items on the righthand side of the rules are strings, allowing theminstead to be multisets.
Secondly, we introduce anadditional term in the head of introduction rulesto signal the fact that when we apply a partic-ular introduction relationship to a node we alsospecialize the type of the node by picking exactlyone of the direct subtypes of its current type.
Fi-nally, we need to deal with the case where TJ isnon-maximal.
This is simply achieved by defin-ing the iterated introduction relationships from T ias being those corresponding to the chains of in-troduction relationships from T i which refine thetype to a maximal type.
In the probabilistic typehierarchy, it is the iterated introduction relation-ships which correspond to the context-free r writerules of a PCFG.
A useful side-effect of this is thatwe can preserve the invariant hat all types exceptthose at the fringe of the structure are maximal.The hierarchy whose ALE syntax is given infigure 4 is captured in the new notation by figure 5We associate probabilities with feature struc-tures, which are sets of maximal and non-maximalnodes generated by beginning from the start-ing node and successively expanding non-maximalleaves of the partial tree.
Maximally specified lea-lure slruclures are those feature structures whichhave only maximal leaves.
Probabilities are as-signed by the following inductive definition:?
P (NT1)= 1.?
If F is a feature structure, and F '  is a partialfeature structure which differs from it onlyin that a single non-maximal node NT k oftype To k in F has been refined to type T1 kexpanded to ~'~ in F ' ,  then P(F ' )  = P (F )  xP((TO :=~ T1) --+ ~'~).Modulo notation, this definition is identical tothe one given earlier for PCFGs.
Given the corre-spondence between the definitions of a PTH anda PCFG it should be apparent hat the trainingmethods which apply to one can equally be usedwith the other.
We will shortly provide an exam-ple.
Because we have not yet treated the crucialmatter of re-entrancy, it would be inappropriateto call what we so far have stochastic HPSG, sowe refer to it as stochastic HPSG- .mum amounts of information possible.4.1 Us ing  s tochast i c  HPSG-  w i th  thecorpusUsing the hierarchy in figure 4 the analyses of thefive sentences from figure 2 are as in figure 6.Training is a matter of counting the transitionswhich are found the observed results, then us-ing counts to refine initial estimates of the prob-abilities of particular transitions.
This is entirelyanalogous to what went on with PCFGs.
The re-sults of training are essentially identical to thosegiven earlier, with the optimal assignment beingas shown in figure 7.
At this point we have pro-vided a system which allows us to use featurestructures instead of PCFGs, but we have notyet dealt with the question of re-entrancy, whichforms a crucial part of the expressive power oftyped feature structures.
We will return to thisshortly, but first we consider the detailed implica-tions of what we have done so far.
The similaritiesbetween these results and those in figure 3?
We still model the distribution observed inthe corpus by assuming two independent de-cisions.?
We still get a strange ranking of the parses,which favours number disagreement,in spiteof the fact that the grammar which generatedthe corpus enforces number agreement.The differences between these results and the ear-lier ones are:?
The hierarchy uses bot  rather than s as itsstart symbol.
The probabilities tell us thatthe corpus contains no free-standing struc-tures of type num.?
The zero probability ofsign ~ phrasecodifies a similar observation that there areno free-standing structures with type phrase.?
Since items of type phrase are never intro-duced at that type, but only in the formof sub-types, there are no transitions fromphrase  in the corpus.
Therefore the initialestimates of the probabilities of such transi-tions are unaffected by training.?
In the PCFG the symmetry between the ex-pansions of np and vp to singular and pluralvariants is implicit, whereas in the PTH thedistribution of singular and plural variants isencoded at a single location, namely that atwhich num is refined.The independence assumption which is builtinto the training algorithm is that types are to berefined according to the same probability distribu-tion irrespective of the context in which they areexpanded.
We have already seen a consequence ofthis: the PTH lumps together all occasions wherenum is expanded, irrespective of whether the en-closing context is np or vp.
For the moment weare prepared to tolerate this because:86MT =NT =NT1 =I ={sentence, np, vp, sing, pl}{bot, sign, phrase, num}bot{(bot :2z sign) --*(bot ::~ num) --+(sign ::V sentence) --+ \[np, vp\](sign =V phrase) --~ \[num\](phrase ::~ np) --~ \[\](phrase ::~ vp) ~ \]\](num =:~ sing) --*(num ::~ pl) --* \[\]}Figure 5: A more formal version of the simple hierarchyLEFTR IGHTvp(2 occurrences)LEFTR IGHTvp(3 occurrences).op\[N M sin \]lvp\[N M si g\]\]op\[N M v \[N M pl\]JFigure 6: Analyses of the corpus using the ALE-hierarchyP(bot  :=~ sign) = 1.0P(bot  =~num) = 0.0P(sign ::~ sentence) = 1.0P(sign =~ phrase) = 0.0P(num==~ sing) = 0.45P(num:=~ pl) = 0.55P(phrase :=~ np) = AP(phrase:=~vp) = 1 -AFigure 7: The results of training the probabilistic type hierarchy87?
C la r i ty :  The decisions which we have madelead to a system with a clear probabilistic se-mantics.?
T ra inab i l i ty :  the number of parameterswhich must be estimated for a grammar is alinear function of the size of the type hierar-chy?
Easy  extens ib i l i ty :  There is a clear routeto a more finely grained account if we allowthe expansion probabilities to be conditionedon surrounding context.
This would increasethe number of parameters to be estimated,which may or may not prove to be a problem.5 Adding re-entranciesWe now turn to an extension of the system whichtakes proper account of re-entrancies in the struc-ture.
The essence of our approach is to definea stochastic procedure which simultaneously ex-pands the nodes of the tree in the way outlinedabove and guesses the pattern of re-entrancieswhich relate them.
It pays to stipulate that thestructures which we build are fully inequated inthe sense defined by Carpenter (Carpenter, 1992,p120).The essential insight is that the choice of afully inequated feature structure involving a setof nodes is the same thing as the choice of anarbitrary equivalence relation over these nodes,and this is in turn equivalent o the choice of apartition of the set of nodes into a set of non-empty sets.
These sets of nodes are equivalenceclasses.
The standard reeursive procedure for gen-erating partitions of k + 1 elements is to non-deterministically add the k + lthq node to eachof the equivalence classes of each of the partitionsof k nodes, and also to nondeterministically con-sider the new node as a singleton set.
The basisof the stochastic procedure for generating fully-inequated feature structures is to interleave thegeneration of equivalence classes with the expan-sion from the initial node as described above.For the purposes of the expansion algorithm, afully inequated feature structure consists of a fea-ture tree (as before) and an equivalence relation 5over all the maximal nodes in that tree.
The taskof the algorithm is to generate all such structuresand to equip them with probabilities.
We proceedas in the case without re-entrancy, except that weonly ever expand sub-trees in the case where thenew node begins a new equivalence class.
Thisavoids the double counting which was a problemearlier.The remaining task is that of assigning scores toequivalence relations.
We do not have a fully sat-5Since maximal types are mutually inconsistent,this equivalence relation can be efficiently representedby a associating a separate partition with each maxi-mal typeisfactory solution to this problem.
The reason forthis is that we would ideally like to assign prob-abilities to intermediate structures in such a waythat the probabilities of fully expanded structuresare independent of the route by which they werearrived at.
This can be done, and the methodwhich we adopt has the merit of simplicity.5.1 Scor ing re -ent ranc iesWe associate a single probabilistic parameterP(T=) with each type T, and derive the probabil-ity of the structure in which a particular pairwiseequation of-nodes in type T have been equatedby multiplying the probability of the structurein which no decision has been made by P(T=).We derive the probability of the corresponding in-equated structure by multiplying by 1 - P(T=) inan entirely analogous way.
This ensures that theprobabilities of the equated and inequated exten-sions of the original structure sum to the origi-nal probability.
The cost is a deficiency in mod-elling, since this takes no account of the fact thattoken identity of nodes is transitive, which aregenerated.
As things stand the stochastic proce-dure is free to generate structures where nl ~ n2,n2 - n3 but nl 7~ n3, which are not in fact legalfeature structures.
This leads to distortions of theprobability estimates ince the training algorithmspends part of its probability mass on impossiblestructures.5.2 Eva luat ionEven a crude account of re-entrancy is better thancompletely ignoring the issue, and the one pro-posed gets the right result for cases of doublecounting such as those discussed above, but itshould be obvious that there is room for improve-ment in the treatment which we provide.
Intu-itively what is required is a parametrisable meansof distributing probability mass among the dis-tinct equivalence relations which extend the cur-rent structure.
One attractive possibility would beto enumerate the relations which can be obtainedby adding the current node to the various differ-ent equivalence classes which are available, applysome scoring function to each class, and then nor-malize such that the total score over all alterna-tives is one.
But this might introduce unpleas-ant dependencies of the probabilities of featurestructures on the order in which the stochasticprocedure chooses to expand nodes, because thenormalisation is carried out before we have fullknowledge of the equivalence classes with whichthe current node might become associated.
It maybe that an appropriate choice of scoring functionwill circumvent this difficulty, but this is left as amatter for further research.886 Conc lus ionsWe have presented two proposals for the associa-tion of probabilities with typed feature-structuresof the form used in HPSG.
As far as we know theseare the most detailed of their type, and the oneswhich are most likely to be able to exploit stan-dard training and parsing algorithms.
For typedfeature structures lacking re-entrancy we believeour proposal to be the simplest and most naturalwhich is available.
The proposal for dealing withre-entrancy is less satisfactory but offers a basisfor empirical exploration, and has definite advan-tages over the straightforward use of PCFGs.
Weplan to follow up the current work by training andtesting a suitable instantiation of our frameworkagainst manually annotated corpora.7 AcknowledgementsI acknowledge the support of the Language Tech-nology Group of the Human Communication Re-search Centre, which is a UK ESRC funded insti-tution.ReferencesBob Carpenter.
1992.
The Logic of Typed Fea-ture Structures.
Cambridge Tracts in Theoreti-cal Computer Science.
CUP.
With Applicationsto Unification Grammars, Logic Programs andConstraint Resolution.Bob Carpenter, 1993.
ALE.
The Atlribule LogicEngine user's guide, version ~.
Carnegie Mel-lon University, Pittsburgh, Pa., Laboratory forComputational Linguistics, MAY.Eugene Charniak.
1993.
Statistical LanguageLearning.
The MIT Press.Albert Kim.
1994.
Graded unification: A frame-work for interactive processing.
In Proceedingsof the 32nd Annual Meeting of the Associationfor Computational Linguistics, pages 313-315,June.C.S.
Mellish.
1988.
Implementing systemic lassi-fication by unification.
Computational Linguis-tics, 14(1):40-51.
Winter.Carl Pollard and Ivan A.
Sag.
1994.
Head-Driven Phrase Structure Grammar.
CSLI andUniversity of Chicago Press, Stanford, Ca.
andChicago, Ill.89
