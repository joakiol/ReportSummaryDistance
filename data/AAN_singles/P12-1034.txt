Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 320?329,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsModeling Review CommentsArjun Mukherjee Bing LiuDepartment of Computer Science Department of Computer ScienceUniversity of Illinois at Chicago University of Illinois at ChicagoChicago, IL 60607, USA Chicago, IL 60607, USAarjun4787@gmail.com liub@cs.uic.eduAbstractWriting comments about news articles,blogs, or reviews have become a popularactivity in social media.
In this paper, weanalyze reader comments about reviews.Analyzing review comments is importantbecause reviews only tell the experiencesand evaluations of reviewers about thereviewed products or services.
Comments,on the other hand, are readers?
evaluationsof reviews, their questions and concerns.Clearly, the information in comments isvaluable for both future readers and brands.This paper proposes two latent variablemodels to simultaneously model andextract these key pieces of information.The results also enable classification ofcomments accurately.
Experiments usingAmazon review comments demonstrate theeffectiveness of the proposed models.1.
IntroductionOnline reviews enable consumers to evaluate theproducts and services that they have used.
Thesereviews are also used by other consumers andbusinesses as a valuable source of opinions.However, reviews only give the evaluations andexperiences of the reviewers.
Often a reviewer maynot be an expert of the product and may misuse theproduct or make other mistakes.
There may also beaspects of the product that the reviewer did notmention but a reader wants to know.
Somereviewers may even write fake reviews to promotesome products, which is called opinion spamming(Jindal and Liu 2008).
To improve the onlinereview system and user experience, some reviewhosting sites allow readers to write commentsabout reviews (apart from just providing afeedback by clicking whether the review is helpfulor not).
Many reviews receive a large number ofcomments.
It is difficult for a reader to read themto get a gist of them.
An automated commentanalysis would be very helpful.
Review commentsmainly contain the following information:Thumbs-up or thumbs-down: Some readers maycomment on whether they find the reviewuseful in helping them make a buying decision.Agreement or disagreement: Some readers whocomment on a review may be users of theproduct themselves.
They often state whetherthey agree or disagree with the review.
Suchcomments are valuable as they provide a secondopinion, which may even identify fake reviewsbecause a genuine user often can easily spotreviewers who have never used the product.Question and answer: A commenter may ask forclarification or about some aspects of theproduct that are not covered in the review.In this paper, we use statistical modeling to modelreview comments.
Two new generative models areproposed.
The first model is called the Topic andMulti-Expression model (TME).
It models topicsand different types of expressions, which representdifferent types of comment posts:1.
Thumbs-up (e.g., ?review helped me?)2.
Thumbs-down (e.g., ?poor review?)3.
Question (e.g., ?how to?)3204.
Answer acknowledgement (e.g., ?thank you forclarifying?).
Note that we have no expressionsfor answers to questions as there are usually nospecific phrases indicating that a post answersa question except starting with the name of theperson who asked the question.
However, thereare typical phrases for acknowledging answers,thus answer acknowledgement expressions.5.
Disagreement (contention) (e.g., ?I disagree?)6.
Agreement (e.g., ?I agree?
).For ease of presentation, we call theseexpressions the comment expressions (or C-expressions).
TME provides a basic model forextracting these pieces of information and topics.Its generative process separates topics and C-expression types using a switch variable and treatsposts as random mixtures over latent topics and C-expression types.
The second model, called ME-TME, improves TME by using Maximum-Entropypriors to guide topic/expression switching.
In short,the two models provide a principled and integratedapproach to simultaneously discover topics and C-expressions, which is the goal of this work.
Notethat topics are usually product aspects in this work.The extracted C-expressions and topics fromreview comments are very useful in practice.
Firstof all, C-expressions enable us to perform moreaccurate classification of comments, which cangive us a good evaluation of the review quality andcredibility.
For example, a review with manyDisagreeing and Thumbs-down comments isdubious.
Second, the extracted C-expressions andtopics help identify the key product aspects thatpeople are troubled with in disagreements and inquestions.
Our experimental results in Section 5will demonstrate these capabilities of our models.With these pieces of information, comments fora review can be summarized.
The summary mayinclude, but not limited to, the following: (1)percent of people who give the review thumbs-upor thumbs-down; (2) percent of people who agreeor disagree (or contend) with the reviewer; (3)contentious (disagreed) aspects (or topics); (4)aspects about which people often have questions.To the best of our knowledge, there is noreported work on such a fine-grained modeling ofreview comments.
The related works are mainly insentiment analysis (Pang and Lee, 2008; Liu2012), e.g., topic and sentiment modeling, reviewquality prediction and review spam detection.However, our work is different from them.
We willcompare with them in detail in Section 2.The proposed models have been evaluated bothqualitatively and quantitatively using a largenumber of review comments from Amazon.com.Experimental results show that both TME and ME-TME are effective in performing their tasks.
ME-TME also outperforms TME significantly.2.
Related WorkWe believe that this work is the first attempt tomodel review comments for fine-grained analysis.There are, however, several general research areasthat are related to our work.Topic models such as LDA (Latent DirichletAllocation) (Blei et al, 2003) have been used tomine topics in large text collections.
There havebeen various extensions to multi-grain (Titov andMcDonald, 2008a), labeled (Ramage et al, 2009),partially-labeled (Ramage et al, 2011), constrained(Andrzejewski et al, 2009) models, etc.
Thesemodels produce only topics but not multiple typesof expressions together with topics.
Note that inlabeled models, each document is labeled with oneor multiple labels.
For our work, there is no labelfor each comment.
Our labeling is on topical termsand C-expressions with the purpose of obtainingsome priors to separate topics and C-expressions.In sentiment analysis, researchers have jointlymodeled topics and sentiment words (Lin and He,2009; Mei et al, 2007; Lu and Zhai, 2008; Titovand McDonald, 2008b; Lu et al, 2009; Brody andElhadad, 2010; Wang et al, 2010; Jo and Oh,2011; Maghaddam and Ester, 2011; Sauper et al,2011; Mukherjee and Liu, 2012a).
Our model ismore related to the ME-LDA model in (Zhao et al,2010), which used a switch variable trained withMaximum-Entropy to separate topic and sentimentwords.
We also use such a variable.
However,unlike sentiments and topics in reviews, which areemitted in the same sentence, C-expressions ofteninterleave with topics across sentences and thesame comment post may also have multiple typesof C-expressions.
Additionally, C-expressions aremostly phrases rather than individual words.
Thus,a different model is required to model them.There have also been works aimed at puttingauthors in debate into support/oppose camps, e.g.,(Galley et al, 2004; Agarwal et al, 2003;Murakami and Raymond, 2010), modeling debatediscussions considering reply relations (Mukherjeeand Liu, 2012b), and identifying stances in debates(Somasundaran and Wiebe, 2009; Thomas et al,3212006; Burfoot et al, 2011).
(Yano and Smith,2010) also modeled the relationship of a blog postand the number of comments it receives.
Theseworks are different as they do not mine C-expressions or discover the points of contentionand questions in comments.In (Kim et al, 2006; Zhang and Varadarajan,2006; Ghose and Ipeirotis, 2007; Liu et al, 2007;Liu et al, 2008; O?Mahony and Smyth, 2009; Tsurand Rappoport 2009), various classification andregression approaches were taken to assess thequality of reviews.
(Jindal and Liu, 2008; Lim etal., 2010; Li et al 2011; Ott et al, 2011;Mukherjee et al, 2012) detect fake reviews andreviewers.
However, all these works are notconcerned with review comments.3.
The Basic TME ModelThis section discusses TME.
The next sectiondiscusses ME-TME, which improves TME.
Thesemodels belong to the family of generative modelsfor text where words and phrases (n-grams) areviewed as random variables, and a document isviewed as a bag of n-grams and each n-gram takesa value from a predefined vocabulary.
In this work,we use up to 4-grams, i.e., n = 1, 2, 3, 4.
Forsimplicity, we use terms to denote both words(unigrams or 1-grams) and phrases (n-grams).
Wedenote the entries in our vocabulary by ????
where ?
is the number of unique terms in the vocabulary.The entire corpus contains ????
documents.
A document (e.g., comment post) ?
is represented asa vector of terms ??
with ??
entries.
?
is the set of all observed terms with cardinality, |?| ?
?
???
.
The TME (Topic and Multi-Expression) model isa hierarchical generative model motivated by thejoint occurrence of various types of expressionsindicating Thumbs-up, Thumbs-down, Question,Answer acknowledgement, Agreement, andDisagreement and topics in comment posts.
Asbefore, these expressions are collectively called C-expressions.
A typical comment post mentions afew topics (using semantically related topicalterms) and expresses some viewpoints with one ormore C-expression types (using semanticallyrelated expressions).
This observation motivatesthe generative process of our model wheredocuments (posts) are represented as randommixtures of latent topics and C-expression types.Each topic or C-expression type is characterized bya distribution over terms (words/phrases).
Assumewe have ????
topics and ????
expression types in our corpus.
Note that in our case of Amazonreview comments, based on reading various posts,we hypothesize that E = 6 as in such reviewdiscussions, we mostly find 6 expression types(more details in Section 5.1).
Let ??
denote the distribution of topics and C-expressions in adocument ?
with ??,?
?
??
?, ???
denoting the binaryindicator variable (topic or C-expression) for the???
term of ?, ??,?.
?
?,?denotes the appropriatetopic or C-expression type index to which ??,?belongs.
We parameterize multinomials over topicsusing a matrix ?????
whose elements ??,??
signify theprobability of document ?
exhibiting topic ?.
Forsimplicity of notation, we will drop the lattersubscript (?
in this case) when convenient and use???
to stand for the ???
row of ??.
Similarly, we define multinomials over C-expression types usinga matrix ?????
.
The multinomials over terms associated with each topic are parameterized by amatrix ?????
, whose elements ??,??
denote theprobability of generating ?
from topic ?.
Likewise,multinomials over terms associated with each C-expression type are parameterized by a matrix?????
.
We now define the generative process of TME (see Figure 1(a)).A.
For each C-expression type ?, draw ???~???????
B.
For each topic t, draw ???
~???????
C. For each comment post ?
?
?1???:i.
Draw ??~????????ii.
Draw ???~???????
iii.
Draw ???~???????
iv.
For each term ?
?,?, ?
?
?1?
???:a.
Draw ??,?~?????????????b.
if (??,?
?
?
??
// ?
?,?is a C-expression termDraw ??,?~?????????
)else  // ??,?
?
?
?
,??
?,?is a topical termDraw ??,?~?????????
)c. Emit ??,?~????????,???,?
)(a) TME Model (b) ME-TME ModelFigure 1: Graphical Models in plate notations.D?E?E??
u?T ?T?T T ?E E ?E?Tzrw Ndx?zrw NdD?E ?E?
?T ?T?T T ?E E ?E?T322To learn the TME model from data, as exactinference is not possible, we resort to approximateinference using collapsed Gibbs sampling(Griffiths and Steyvers, 2004).
Gibbs sampling is aform of Markov Chain Monte Carlo method wherea Markov chain is constructed to have a particularstationary distribution.
In our case, we want toconstruct a Markov chain which converges to theposterior distribution over ?
and ?
conditioned onthe data.
We only need to sample ?
and ?
as we usecollapsed Gibbs sampling and the dependencies of?
and ?
have been integrated out analytically in thejoint.
Denoting the random variables ?
?, ?, ??
bysingular subscripts??
?, ?
?, ??
?, ???
?, where ?
??
???
, a single iteration consists of performing the following sampling:????
?
?, ??
?
?
?| ??
?, ??
?, ???,??
?
??
?????????????????????
????
??,??????????,?????
???????
??,??????????,?????
??????(1)????
?
?, ??
?
?
?| ??
?, ??
?, ??
?, ??
?
??
?????????????????????
????
??,??????????,?????
???????
??,??????????,?????
??????
(2)where ?
?
?
?, ??
denotes the ???
term of document?
and the subscript ??
denotes assignmentsexcluding the term at ?
?, ??.
Counts??,???
and ??,??
?denote the number of times term ?
was assigned totopic ?
and expression type ?
respectively.
??,???
and??,???
denote the number of terms in document ?
thatwere assigned to topic ?
and C-expression type ?respectively.
Lastly, ???
and ???
are the number of terms in ?
that were assigned to topics and C-expression types respectively.
Omission of thelatter index denoted by ???
represents themarginalized sum over the latter index.
We employa blocked sampler jointly sampling ?
and ?
as thisimproves convergence and reduces autocorrelationof the Gibbs sampler (Rosen-Zvi et al, 2004).Asymmetric Beta priors: Based on our initialexperiments with TME, we found that properlysetting the smoothing hyper-parameter ??
iscrucial as it governs the topic/expression switch.According to the generative process, ??
is the (success) probability (of the Bernoulli distribution)of emitting a topical/aspect term in a comment post?
?and1 ?
?
?, the probability of emitting a C-expression term in ?.
Without loss of generality,we draw ??~????????
where ?
is the concentration parameter and ?
?
??
?, ???
is the base measure.
Without any prior belief, one resortsto uniform base measure ??
?
??
?0.5 (i.e., assumes that both topical and C-expression termsare equally likely to be emitted in a comment post).This results in symmetric Beta priors??~??????
?, ???
where ??
?
??
?, ??
?
???
and??
?
??
?
2/?.
However, knowing the fact that topics are more likely to be emitted thanexpressions in a post apriori motivates us to takeguidance from asymmetric priors (i.e., we nowhave a non-uniform base measure?).
Thisasymmetric setting of ?
ensures that samples of ??
are more close to the actual distribution of topicalterms in posts based on some domain knowledge.Symmetric ?
cannot utilize any prior knowledge.
In(Lin and He, 2009), a method was proposed toincorporate domain knowledge during Gibbssampling initialization, but its effect becomes weakas the sampling progresses (Jo and Oh, 2011).For asymmetric priors, we estimate the hyper-parameters from labeled data.
Given a labeled set?
?, where we know the per post probability of C-expression emission (1 ?
??
?, we use the method of moments to estimate ?
?
??
?, ???
as follows:??
?
?
????????
?
1?
, ??
?
??
???
?
1?
; ??
?
????
?, ?
?
???????
(3)4.
ME-TME ModelThe guidance of Beta priors, although helps, is stillrelatively coarse and weak.
We can do better toproduce clearer separation of topical and C-expression terms.
An alternative strategy is toemploy Maximum-Entropy (Max-Ent) priorsinstead of Beta priors.
The Max-Ent parameterscan be learned from a small number of labeledtopical and C-expression terms (words andphrases) which can serve as good priors.
The ideais motivated by the following observation: topicaland C-expression terms typically play differentsyntactic roles in a sentence.
Topical terms (e.g.?ipod?
?cell phone?, ?macro lens?, ?kindle?, etc.
)tend to be noun and noun phrases while expressionterms (?I refute?, ?how can you say?, ?greatreview?)
usually contain pronouns, verbs, wh-determiners, adjectives, and modals.
In order toutilize the part-of-speech (POS) tag information,we move the topic/C-expression distribution ??
(the prior over the indicator variable ??,?)
from thedocument plate to the word plate (see Figure 1 (b))and draw it from a Max-Ent model conditioned onthe observed feature vector ??,????????
associated with??,?
and the learned Max-Ent parameters ??.??,?
can323encode arbitrary contextual features for learning.With Max-Ent priors, we have the new model ME-TME.
In this work, we encode both lexical andPOS features of the previous, current and next POStags/lexemes of the term ??,?.
More specifically,??,????????
?
??????,??
?, ?????,?
, ?????,??
?, ??,?
?
1,?
?,?, ??,?
?
1?For phrasal terms (n-grams), all POS tags andlexemes of ?
?,?are considered as features.Incorporating Max-Ent priors, the Gibbs samplerof ME-TME is given by:????
?
?, ??
?
?
?| ??
?, ??
?, ???,??
?
??
??????
???????,?,???????
??
?????
???????,?,??????
??????,???
???,??????????,?????
???????
??,??????????,?????
??????(4)????
?
?, ??
?
?
?| ??
?, ??
?, ??
?, ??
?
??
??????
???????,?,???????
??
?????
???????,?,??????
??????,???
???,??????????,?????
???????
??,??????????,?????
??????
(5)where ????
are the parameters of the learned Max-Ent model corresponding to the ?
binary featurefunctions ????
from Max-Ent.5.
EvaluationWe now evaluate the proposed TME and ME-TMEmodels.
Specifically, we evaluate the discoveredC-expressions, contentious aspects, and aspectsoften mentioned in questions.5.1 Dataset and Experiment SettingsWe crawledcomments of reviews in Amazon.com for a varietyof products.
For each comment we extracted its id,the comment author id, the review id on which itcommented, and the review author id.
Ourdatabase consisted of 21,316 authors, 37,548reviews, and 88,345 comments with an average of124 words per comment post.For all our experiments, the hyper-parametersfor TME and ME-TME were set to the heuristicvalues ?T = 50/T, ?E = 50/E, ?T = ?E = 0.1 assuggested in (Griffiths and Steyvers, 2004).
For ?,we estimated the asymmetric Beta priors using themethod of moments discussed in Section 3.
Wesampled 1000 random posts and for each post weidentified the C-expressions emitted.
We thuscomputed the per-post probability of C-expressionemission (1 ?
???
and used Eq.
(3) to get the final estimates, ??
= 3.66, ?
?= 1.21.
To learn the Max-Ent parameters ?, we randomly sampled 500 termsfrom our corpus appearing at least 10 times andlabeled them as topical (332) or C-expressions(168) and used the corresponding feature vector ofeach term (in the context of posts where it occurs)to train the Max-Ent model.
We set the number oftopics, T = 100 and the number of C-expressiontypes, E = 6 (Thumbs-up, Thumbs-down, Question,Answer acknowledgement, Agreement andDisagreement) as in review comments, we usuallyfind these six dominant expression types.
Note thatknowing the exact number of topics, T andexpression types, E in a corpus is difficult.
Whilenon-parametric Bayesian approaches (Teh et al,2006) aim to estimate T from the corpus, in thiswork the heuristic values obtained from our initialexperiments produced good results.
We also triedincreasing E to 7, 8, etc.
However, it did notproduce any new dominant expression type.Instead, the expression types became less specificas the expression term space became sparser.5.2 C-Expression EvaluationWe now evaluate the discovered C-expressions.We first evaluate them qualitatively in Tables 1and 2.
Table 1 shows the top terms of allexpression types using the TME model.
We findthat TME can discover and cluster many correct C-expressions, e.g., ?great review?, ?review helpedme?
in Thumbs-up; ?poor review?, ?very unfairreview?
in Thumbs-down; ?how do I?, ?help medecide?
in Question; ?good reply?, ?thank you forclarifying?
in Answer Acknowledgement; ?Idisagree?, ?I refute?
in Disagreement; and ?Iagree?, ?true in fact?
in Agreement.
However, withthe guidance of Max-Ent priors, ME-TME didmuch better (Table 2).
For example, we find ?levelheaded review?, ?review convinced me?
inThumbs-up; ?biased review?, ?is flawed?
inThumbs-down; ?any clues?, ?I was wonderinghow?
in Question; ?clears my?, ?valid answer?
inAnswer-acknowledgement; ?I don?t buy your?,?sheer nonsense?
in Disagreement; ?agreecompletely?, ?well said?
in Agreement.
Thesenewly discovered phrases by ME-TME are markedin blue in Table 3.
ME-TME also has fewer errors.Next, we evaluate them quantitatively using themetric precision @ n, which gives the precision atdifferent rank positions.
This metric is appropriatehere because the C-expressions (according to topterms in ?E) produced by TME and ME-TME arerankings.
Table 3 reports the precisions @ top 25,50, 75, and 100 rank positions for all sixexpression types across both models.
We evaluatedtill top 100 positions because it is usually324important to see whether a model can discover andrank those major expressions of a type at the top.We believe that top 100 are sufficient for mostapplications.
From Table 3, we observe that ME-TME consistently outperforms TME in precisionsacross all expression types and all rank positions.This shows that Max-Ent priors are more effectivein discovering expressions than Beta priors.
Notethat we couldn?t compare with an existing baselinebecause there is no reported study on this problem.5.3 Comment ClassificationHere we show that the discovered C-expressionscan help comment classification.
Note that since acomment can belong to one or more types (e.g., acomment can belong to both Thumbs-up andAgreement types), this task is an instance of multi-label classification, i.e., an instance can have morethan one class label.
In order to evaluate all theexpression types, we follow the binary approachwhich is an extension of one-against-all method formulti-label classification.
Thus, for each label, webuild a binary classification problem.
Instancesassociated with that label are in one class and therest are in the other class.
To perform this task, werandomly sampled 2000 comments, and labeledeach of them into one or more of the following 8labels: Thumbs-up, Thumbs-down, Disagreement,Agreement, Question, Answer-Acknowledgement,Answer, and None, which have 432, 401, 309, 276,305, 201, 228, and 18 comments respectively.
Wedisregard the None category due to its small size.This labeling is a fairly easy task as one can almostcertainly make out to which type a commentbelongs.
Thus we didn?t use multiple labelers.
Thedistribution reveals that the labels are overlapping.For instance, we found many comments belongingto both Thumbs-down and Disagreement, Thumbs-upwith Acknowledgement and with Question.For supervised classification, the choice offeature is a key issue.
While word and POS n-grams are traditional features, such features maynot be the best for our task.
We now compare suchfeatures with the C-expressions discovered by theproposed models.
We used the top 1000 termsfrom each of the 6 C-expression rankings asfeatures.
As comments in Question type mostly usethe punctuation ??
?, we added it in our feature set.We use precision, recall and F1 as our metric tocompare classification performance using a trainedSVM (linear kernel).
All results (Table 4) werecomputed using 10-fold cross-validation (CV).
Wealso tried Na?ve Bayes and Logistic Regressionclassifiers, but they were poorer than SVM.
Hencetheir results are not reported due to spaceconstraints.
As a separate experiment (not shownhere also due to space constraints), we analyzedthe classification performance by varying thenumber of top terms from 200, 400,?, 1000, 1200,etc.
and found that the F1 scores stabilized after topFigure 5: Precision @ top 50,Thumbs-up (e1): review, thanks, great review, nice review, time, best review, appreciate, you, your review helped, nice, terrific,review helped me, good critique, very, assert, wrong, usefulreview, don?t, misleading, thanks a lot, ?Thumbs-down (e2): review, no, poor review, imprecise, you, complaint, very, suspicious, bogus review, absolutely, credible,very unfair review, criticisms, true, disregard this review, disagreewith, judgment, without owning, ?Question (e3): question, my, I, how do I, why isn?t, please explain, good answer, clarify, don?t understand, my doubts, I?m confused,does not, understand, help me decide, how to,  yes, answer, howcan I, can?t explain, ?Answer Acknowledgement (e4): my, informative, answer, good reply, thank you for clarifying, answer doesn?t, good answer,vague, helped me choose, useful suggestion, don?t understand,cannot explain, your answer, doubts, answer isn?t, ?Disagreement (e5): disagree, I, don?t, I disagree, argument claim, I reject, I refute, I refuse, oppose, debate, accept, don?t agree, quote,sense, would disagree, assertions, I doubt, right,  your, really,you, I?d disagree, cannot, nonsense,...Agreement (e6): yes, do, correct, indeed, no, right, I agree, you, agree, I accept, very, yes indeed, true in fact, indeed correct, I?dagree, completely, true, but, doesn?t, don?t, definitely, false,completely agree, agree with your, true, ?Table 1: Top terms (comma delimited) of six expression typese1, e2, e3, e4, e5, e6 (?E) using TME model.
Red (bold) colored terms denote possible errorsThumbs-up (e1): review, you, great review, I'm glad I read, best review, review convinced me, review helped me,  good review, terrificreview, job, thoughtful review, awesome review, level headed review,good critique, good job, video review,...Thumbs-down (e2): review, you, bogus review, con, useless review, ridiculous,  biased review, very unfair review, is flawed, completely,skeptical, badmouth, misleading review, cynical review, wrong,disregard this review, seemingly honest, ?Question (e3): question, I, how do I, why isn?t, please explain, clarify, any clues, answer, please explain, help me decide, vague, how to, howdo I, where can I, how to set, I was wondering how, could you explain,how can I, can I use, ?Answer Acknowledgement (e4): my, good reply, , answer, reply, helped me choose, clears my,  valid answer, answer doesn?t,satisfactory answer, can you clarify, informative answer, usefulsuggestion, perfect answer, thanks for your reply, doubts, ?Disagreement (e5): disagree, I, don?t, I disagree, doesn?t, I don?t buy your, credible, I reject, I doubt, I refuse, I oppose, sheer nonsense,hardly, don?t agree, can you prove, you have no clue, how do you say,sense, you fail, contradiction, ?Agreement (e6): I, do, agree, point, yes, really, would agree, you, agree, I accept, claim, agree completely, personally agree, true in fact,indeed correct, well said, valid point, correct, never meant, might not,definitely agree,?Table 2: Top terms (comma delimited) of six expression typesusing ME-TME model.
Red (bold) terms denote possible errors.Blue (italics) terms denote those newly discovered by the model;rest (black) were used in Max-Ent training.3251000 terms.
From Table 4, we see that F1 scoresdramatically increase with C-expression (??
)features for all expression types.
TME and ME-TME progressively improve the classification.Improvements of TME and ME-TME beingsignificant (p<0.001) using a paired t-test across10-fold cross validations shows that the discoveredC-expressions are of high quality and useful.We note that the annotation resulted in a newlabel ?Answer?
which consists of mostly replies tocomments with questions.
Since an ?answer?
to aquestion usually does not show any specificexpression, it does not attain very good F1 scores.Thus, to improve the performance of the Answertype comments, we added three binary features foreach comment c on top of C-expression features:i) Is the author of c the review author too?
Theidea here is that most of the times the revieweranswers the questions raised in comments.ii) Is there any comment posted before c by someauthor a which has been previously classifiedas a question post?iii) Is there any comment posted after c by authora that replies to c (using @name) and is anAnswer-Acknowledgement comment (whichagain has been previously classified as such)?Using these additional features, we obtained aprecision of 0.78 and a recall of 0.73 yielding an F1C-Expression Type P@25 P@50 P@75 P@100TME ME-TME TME ME-TME TME ME-TME TME ME-TMEThumbs-up 0.60 0.80 0.66 0.78 0.60 0.69 0.55 0.64Thumbs-down 0.68 0.84 0.70 0.80 0.63 0.67 0.60 0.65Question 0.64 0.80 0.68 0.76 0.65 0.72 0.61 0.67Answer-Acknowledgement 0.68 0.76 0.62 0.72 0.57 0.64 0.54 0.58Disagreement 0.76 0.88 0.74 0.80 0.68 0.73 0.65 0.70Agreement 0.72 0.80 0.64 0.74 0.61 0.70 0.60 0.69Table 3: Precision @ top 25, 50, 75, and 100 rank positions for all C-expression types.Features Thumbs-up Thumbs-down Question Answer-Ack.
Disagreement Agreement AnswerP R F1 P R F1 P R F1 P R F1 P R F1 P R F1 P R F1W+POS 1-gram 0.68 0.66 0.67 0.65 0.65 0.65 0.71 0.68 0.69 0.64 0.61 0.62 0.73 0.72 0.72 0.67 0.65 0.66 0.58 0.57 0.57W+POS 1-2 gram 0.72 0.69 0.70 0.68 0.67 0.67 0.74 0.69 0.71 0.69 0.63 0.65 0.76 0.75 0.75 0.71 0.69 0.70 0.60 0.57 0.58W+POS, 1-3 gram 0.73 0.71 0.72 0.69 0.68 0.68 0.75 0.69 0.72 0.70 0.64 0.66 0.76 0.76 0.76 0.72 0.70 0.71 0.61 0.58 0.59W+POS, 1-4 gram 0.74 0.72 0.73 0.71 0.68 0.69 0.75 0.70 0.72 0.70 0.65 0.67 0.77 0.76 0.76 0.73 0.70 0.71 0.61 0.58 0.59C-Expr.
?E, TME 0.82 0.74 0.78 0.77 0.71 0.74 0.83 0.75 0.78 0.75 0.72 0.73 0.83 0.80 0.81 0.78 0.75 0.76 0.66 0.61 0.63C-Expr.
?E, ME-TME 0.87 0.79 0.83 0.80 0.73 0.76 0.87 0.76 0.81 0.77 0.72 0.74 0.86 0.81 0.83 0.81 0.77 0.79 0.67 0.61 0.64Table 4: Precision (P), Recall (R), and F1 scores of binary classification using SVM and different features.
The improvements of our models are significant (p<0.001) over paired t-test across 10-fold cross validation.D?E  + Noun/Noun Phrase TME ME-TMEJ1 J2 J1 J2 J1 J2 P R F1 P R F1 P R F1 P R F1 P R F1 P R F1D1 0.62 0.70 0.66 0.58 0.67 0.62 0.66 0.75 0.70 0.62 0.70 0.66 0.67 0.79 0.73 0.64 0.74 0.69D2 0.61 0.67 0.64 0.57 0.63 0.60 0.66 0.72 0.69 0.62 0.67 0.64 0.68 0.75 0.71 0.64 0.71 0.67D3 0.60 0.69 0.64 0.56 0.64 0.60 0.64 0.73 0.68 0.60 0.67 0.63 0.67 0.76 0.71 0.63 0.72 0.67D4 0.59 0.68 0.63 0.55 0.65 0.60 0.63 0.71 0.67 0.59 0.68 0.63 0.65 0.73 0.69 0.62 0.71 0.66Avg.
0.61 0.69 0.64 0.57 0.65 0.61 0.65 0.73 0.69 0.61 0.68 0.64 0.67 0.76 0.71 0.63 0.72 0.67Table 5 (a)D ?E  + Noun/Noun Phrase TME ME-TMEJ1 J2 J1 J2 J1 J2P R F1 P R F1 P R F1 P R F1 P R F1 P R F1 D1 0.57 0.65 0.61 0.54 0.63 0.58 0.61 0.69 0.65 0.58 0.66 0.62 0.64 0.73 0.68 0.61 0.70 0.65D2 0.61 0.66 0.63 0.58 0.61 0.59 0.64 0.68 0.66 0.60 0.64 0.62 0.68 0.70 0.69 0.65 0.69 0.67D3 0.60 0.68 0.64 0.57 0.64 0.60 0.64 0.71 0.67 0.62 0.68 0.65 0.67 0.72 0.69 0.64 0.69 0.66D4 0.56 0.67 0.61 0.55 0.65 0.60 0.60 0.72 0.65 0.58 0.68 0.63 0.63 0.75 0.68 0.61 0.71 0.66Avg.
0.59 0.67 0.62 0.56 0.63 0.59 0.62 0.70 0.66 0.60 0.67 0.63 0.66 0.73 0.69 0.63 0.70 0.66Table 5 (b)Table 5: Points of Contention (a), Questioned aspects (b).
D1: Ipod, D2: Kindle, D3: Nikon, D4: Garmin.
We report theaverage precision (P), recall (R), and F1 score over 100 comments for each particular domain.Statistical significance: Differences between Nearest Noun Phrase and TME for both judges (J1, J2) across all domains were significant at 97% confidence level (p<0.03).
Differences among TME and ME-TME for both judges (J1, J2) across all domains were significant at 95% confidence level (p<0.05).
A paired t-test was used for testing significance.326score of 0.75 which is a dramatic increase beyond0.64 achieved by ME-TME in Table 4.5.4 Contention Points and Questioned AspectsWe now turn to the task of discovering points ofcontention in disagreement comments and aspects(or topics) raised in questions.
By ?points?, wemean the topical terms on which some contentionsor disagreements have been expressed.
Topicsbeing the product aspects are also indirectlyevaluated in this task.
We employ the TME andME-TME models in the following manner.We only detail the approach for disagreementcomments.
The same method is applied to questioncomments.
Given a disagreement comment post ?,we first select the top k topics that are mentioned ind according to its topic distribution, ???
.
Let ??
be the set of these top ?
topics in ?.
Then, for eachdisagreement expression??
?
?
?
????????????????
,we emit the topical terms (words/phrases) of topicsin ?
?which appear within a word window of ?
from ?
in ?.
More precisely, we emit the set ?
?
??|?
??
?
???
, ?
?
?
?, |???????
?
??????
?| ?
?
?, where posi(?)
returns the position index of the word orphrase in document ?.
To compute the intersection?
?
?
?
???
, we need a threshold.
This is so because the Dirichlet distribution has a smoothingeffect which assigns some non-zero probabilitymass to every term in the vocabulary for each topic?.
So for computing the intersection, we consideredonly terms in ???
which have ???|??
?
???,??
> 0.001as probability masses lower than 0.001 are moredue to the smoothing effect of the Dirichletdistribution than true correlation.
In an actualapplication, the values for ?
and ?
can be setaccording to the user?s need.
In our experiment, weused ?
?= 3 and 5 = ?, which are reasonable becausea post normally does not talk about many topics(?
), and the contention points (aspect terms) appearquite close to the disagreement expressions.For comparison, we also designed a baseline.For each disagreement (or question) expression?
?
?
?
????????????????
(????????????
), we emit thenouns and noun phrases within the same window ?as the points of contention (question) in ?.
Thisbaseline is reasonable because topical terms areusually nouns and noun phrases and are neardisagreement (question) expressions.
We note thatthis baseline cannot stand alone because it has torely on our expression models ??
of ME-TME.Next, to evaluate the performance of thesemethods in discovering points of contention, werandomly selected 100 disagreement (contentious)(and 100 question) comment posts on reviews fromeach of the 4 product domains: Ipod, Kindle,Nikon Cameras, and Garmin GPS in our databaseand employed the aforementioned methods todiscover the points of contention (question) in eachpost.
Then we asked two human judges (graduatestudents fluent in English) to manually judge theresults produced by each method for each post.
Weasked them to report the precision of thediscovered terms for a post by judging them asbeing indeed valid points of contention and reportrecall in a post by judging how many of actuallycontentious points in the post were discovered.
InTable 5 (a), we report the average precision andrecall for 100 posts in each domain by the twojudges J1 and J2 for different methods on the taskof discovering points (aspects) of contention.
InTable 5 (b), similar results are reported for the taskof discovering questioned aspects in 100 questioncomments for each product domain.
Since thisjudging task is subjective, the differences in theresults from the two judges are not surprising.
Ourjudges were made to work in isolation to preventany bias.
We observe that across all domains, ME-TME again performs the best consistently.
Notethat agreement study using Kappa is not used hereas our problem is not to label a fixed set of itemscategorically by the judges.6.
ConclusionThis paper proposed the problem of modelingreview comments, and presented two models TMEand ME-TME to model and to extract topics(aspects) and various comment expressions.
Theseexpressions enable us to classify comments moreaccurately, and to find contentious aspects andquestioned aspects.
These pieces of informationalso allow us to produce a simple summary ofcomments for each review as discussed in Section1.
To our knowledge, this is the first attempt toanalyze comments in such details.
Our experimentsdemonstrated the efficacy of the models.
ME-TMEalso outperformed TME significantly.AcknowledgmentsThis work is supported in part by National ScienceFoundation (NSF) under grant no.
IIS-1111092.327ReferencesAgarwal, R., S. Rajagopalan, R. Srikant, Y. Xu.
2003.Mining newsgroups using networks arising fromsocial behavior.
Proceedings of InternationalConference on World Wide Web 2003.Andrzejewski, D., X. Zhu, M. Craven.
2009.Incorporating domain knowledge into topicmodeling via Dirichlet forest priors.
Proceedings ofInternational Conference on Machine Learning.Blei, D., A. Ng, and M. Jordan.
2003.
Latent DirichletAllocation.
Journal of Machine Learning Research.Brody, S. and S. Elhadad.
2010.
An UnsupervisedAspect-Sentiment Model for Online Reviews.Proceedings of the Annual Conference of the NorthAmerican Chapter of the ACL.Burfoot, C., S. Bird, and T. Baldwin.
2011.
CollectiveClassification of Congressional Floor-DebateTranscripts.
Proceedings of the 49th Annual Meetingof the Association for Computational Linguistics.Galley, M., K. McKeown, J. Hirschberg, E. Shriberg.2004.
Identifying agreement and disagreement inconversational speech: Use of Bayesian networks tomodel pragmatic dependencies.
Proceedings of the42th Annual Meeting of the Association ofComputational Linguistics.Ghose, A. and P. Ipeirotis.
2007.
Designing novelreview ranking systems: predicting the usefulnessand impact of reviews.
Proceedings of InternationalConference on Electronic Commerce.Griffiths, T. and M. Steyvers.
2004.
Finding scientifictopics.
Proceedings of National Academy ofSciences.Kim, S., P. Pantel, T. Chklovski, and M. Pennacchiotti.2006.
Automatically assessing review helpfulness.Proceedings of Empirical Methods in NaturalLanguage Processing.Jindal, N. and B. Liu.
2008.
Opinion spam and analysis.Proceedings of the ACM International Conference onWeb Search and Web Data Mining.Jo, Y. and A. Oh.
2011.
Aspect and sentimentunification model for online review analysis.Proceedings of the ACM International Conferenceon Web Search and Web Data Mining.Li, F., M. Huang, Y. Yang, and X. Zhu.
2011.
Learningto Identify Review Spam.
in Proceedings of theInternational Joint Conference on ArtificialIntelligence.Lim, E., V. Nguyen, N. Jindal, B. Liu, and H. Lauw.2010.
Detecting Product Review Spammers usingRating Behaviors.
Proceedings of the ACMInternational Conference on Information andKnowledge Management.Lin, C. and Y.
He.
2009.
Joint sentiment/topic model forsentiment analysis.
Proceedings of the ACMInternational Conference on Information andKnowledge Management.Liu, J., Y. Cao, C. Lin, Y. Huang, and M. Zhou.
2007.Low-quality product review detection in opinionsummarization.
Proceedings of Empirical Methods inNatural Language Processing.Liu, B.
2012.
Sentiment Analysis and Opinion Mining.Morgan & Claypool publishers (to appear in June2012).Liu, Y., X. Huang, A.
An, and X. Yu.
2008.
Modelingand predicting the helpfulness of online reviews.Proceedings of IEEE International Conference onData Mining.Lu, Y. and C. Zhai.
2008.
Opinion integration throughsemi-supervised topic modeling.
Proceedings ofInternational Conference on World Wide Web.Lu, Y., C. Zhai, and N. Sundaresan.
2009.
Rated aspectsummarization of short comments.
Proceedings ofInternational Conference on World Wide.Mei, Q. X. Ling, M. Wondra, H. Su and C. Zhai.
2007.Topic sentiment mixture: modeling facets andopinions in weblogs.?
Proceedings of InternationalConference on World Wide.Moghaddam, S. and M. Ester.
2011.
ILDA:interdependent LDA model for learning latentaspects and their ratings from online product reviews.Proceedings of Annual ACM SIGIR Conference onResearch and Development in Information Retrieval.Mukherjee, A. and B. Liu.
2012a.
Aspect Extractionthrough Semi-Supervised Modeling.
Proceedings of50th Annual Meeting of Association forComputational Linguistics (to appear in July 2012).Mukherjee, A. and B. Liu.
2012b.
Mining Contentionsfrom Discussions and Debates.
Proceedings of ACMSIGKDD International Conference on KnowledgeDiscovery and Data Mining (to appear in August2012).Mukherjee, A., B. Liu and N. Glance.
2012.
SpottingFake Reviewer Groups in Consumer Reviews.Proceedings of International World Wide WebConference.Murakami A., and R. Raymond, 2010.
Support orOppose?
Classifying Positions in Online Debatesfrom Reply Activities and Opinion Expressions.Proceedings of International Conference on328Computational Linguistics.O'Mahony, M. P. and B. Smyth.
2009.
Learning torecommend helpful hotel reviews.
Proceedings of thethird ACM conference on Recommender systems.Ott, M., Y. Choi, C. Cardie, and J. T. Hancock.
2011.Finding deceptive opinion spam by any stretch of theimagination.
Proceedings of the 49th Annual Meetingof the Association for Computational Linguistics.Pang, B. and L. Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and Trends inInformation Retrieval.Ramage, D., D. Hall, R. Nallapati, and C. Manning.2009.
Labeled LDA: A supervised topic model forcredit attribution in multi-labeled corpora.Proceedings of Empirical Methods in NaturalLanguage Processing.Ramage, D., C. Manning, and S. Dumais.
2011 Partiallylabeled topic models for interpretable text mining.Proceedings of the 17th ACM SIGKDDinternational conference on Knowledge discoveryand data mining.Rosen-Zvi, M., T. Griffiths, M. Steyvers, and P. Smith.2004.
The author-topic model for authors anddocuments.
Uncertainty in Artificial Intelligence.Sauper, C. A. Haghighi and R. Barzilay.
2011.
Contentmodels with attitude.
Proceedings of the 49th AnnualMeeting of the Association for ComputationalLinguistics.Somasundaran, S., J. Wiebe.
2009.
Recognizing stancesin online debates.
Proceedings of the 47th AnnualMeeting of the ACL and the 4th IJCNLP of theAFNLPTeh, Y., M. Jordan, M. Beal and D. Blei.
2006.Hierarchical Dirichlet Processes.
Journal of theAmerican Statistical Association.Thomas, M., B. Pang and L. Lee.
2006.
Get out thevote: Determining support or opposition fromCongressional floor-debate transcripts.
Proceedingsof Empirical Methods in Natural LanguageProcessing.Titov, I. and R. McDonald.
2008a.
Modeling onlinereviews with multi-grain topic models.
Proceedingsof International Conference on World Wide Web.Titov, I. and R. McDonald.
2008b.
A joint model of textand aspect ratings for sentiment summarization.Proceedings of Annual Meeting of the Associationfor Computational Linguistics.Tsur, O. and A. Rappoport.
2009.
Revrank: A fullyunsupervised algorithm for selecting the most helpfulbook reviews.
Proceedings of the International AAAIConference on Weblogs and Social Media.Wang, H., Y. Lu, and C. Zhai.
2010.
Latent aspectrating analysis on review text data: a ratingregression approach.
Proceedings of ACM SIGKDDInternational Conference on Knowledge Discoveryand Data Mining.Yano, T and N. Smith.
2010.
What?s Worthy ofComment?
Content and Comment Volume inPolitical Blogs.
Proceedings of the InternationalAAAI Conference on Weblogs and Social Media.Zhang, Z. and B. Varadarajan.
2006.
Utility scoring ofproduct reviews.
Proceedings of ACM InternationalConference on Information and KnowledgeManagement.Zhao, X., J. Jiang, H. Yan, and X. Li.
2010.
Jointlymodeling aspects and opinions with a MaxEnt-LDAhybrid.
Proceedings of Empirical Methods in NaturalLanguage Processing.329
