Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 148?156,October 25, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsPreference Grammars and Soft Syntactic Constraintsfor GHKM Syntax-based Statistical Machine TranslationMatthias Huck and Hieu Hoang and Philipp KoehnSchool of InformaticsUniversity of Edinburgh10 Crichton StreetEdinburgh EH8 9AB, UK{mhuck,hhoang,pkoehn}@inf.ed.ac.ukAbstractIn this work, we investigate the effec-tiveness of two techniques for a feature-based integration of syntactic informationinto GHKM string-to-tree statistical ma-chine translation (Galley et al., 2004):(1.)
Preference grammars on the tar-get language side promote syntactic well-formedness during decoding while also al-lowing for derivations that are not linguis-tically motivated (as in hierarchical trans-lation).
(2.)
Soft syntactic constraints aug-ment the system with additional source-side syntax features while not modifyingthe set of string-to-tree translation rules orthe baseline feature scores.We conduct experiments with a state-of-the-art setup on an English?Germantranslation task.
Our results suggest thatpreference grammars for GHKM trans-lation are inferior to the plain target-syntactified model, whereas the enhance-ment with soft source syntactic constraintsprovides consistent gains.
By employ-ing soft source syntactic constraints withsparse features, we are able to achieve im-provements of up to 0.7 points BLEU and1.0 points TER.1 IntroductionPrevious research in both formally syntax-based(i.e., hierarchical) and linguistically syntax-basedstatistical machine translation has demonstratedthat significant quality gains can be achieved viaintegration of syntactic information as features ina non-obtrusive manner, rather than as hard con-straints.We implemented two feature-based extensionsfor a GHKM-style string-to-tree translation sys-tem (Galley et al., 2004):?
Preference grammars to soften the hardtarget-side syntactic constraints that are im-posed by the target non-terminal labels.?
Soft source-side syntactic constraints thatenhance the string-to-tree translation modelwith input tree features based on source syn-tax labels.The empirical results on an English?Germantranslation task are twofold.
Target-side prefer-ence grammars do not show an improvement overthe string-to-tree baseline with syntactified trans-lation rules.
Source-side syntactic constraints, onthe other hand, yield consistent moderate gains ifapplied as supplementary features in the string-to-tree setup.2 OutlineThe paper is structured as follows: First we give anoverview of important related publications (Sec-tion 3).
In Section 4, we review the fundamentalsof syntax-based translation in general, and in par-ticular those of GHKM string-to-tree translation.We present preference grammars for GHKMtranslation in Section 5.
Our technique for ap-plying soft source syntactic constraints in GHKMstring-to-tree translation is described in Section 6.Section 7 contains the empirical part of the pa-per.
We first describe our experimental setup (7.1),followed by a presentation and discussion of thetranslation results (7.2).
We conclude the paper inSection 8.3 Related WorkOur syntactic translation model conforms to theGHKM syntax approach as proposed by Galley,Hopkins, Knight, and Marcu (Galley et al., 2004)with composed rules as in (Galley et al., 2006)and (DeNeefe et al., 2007).
Systems based on148this paradigm have recently been among the top-ranked submissions to public evaluation cam-paigns (Williams et al., 2014; Bojar et al., 2014).Our soft source syntactic constraints featuresborrow ideas from Marton and Resnik (2008) whoproposed a comparable approach for hierarchicalmachine translation.
The major difference is thatthe features of Marton and Resnik (2008) are onlybased on the labels from the input trees as seen intuning and decoding.
They penalize violations ofconstituent boundaries but do not employ syntacticparse annotation of the source side of the trainingdata.
We, in contrast, equip the rules with latentsource label properties, allowing for features thatcan check for conformance of input tree labels andsource labels that have been seen in training.Other groups have applied similar techniquesto a string-to-dependency system (Huang et al.,2013) and?like in our work?a GHKM string-to-tree system (Zhang et al., 2011).
Both Huanget al.
(2013) and Zhang et al.
(2011) store sourcelabels as additional information with the rules.They however investigate somewhat different fea-ture functions than we do.Marton and Resnik (2008) evaluated theirmethod on the NIST Chinese?English andArabic?English tasks.
Huang et al.
(2013) andZhang et al.
(2011) present results on the NISTChinese?English task.
We focus our attention ona very different task: English?German.4 Syntax-based TranslationIn syntax-based translation, a probabilistic syn-chronous context-free grammar (SCFG) is in-duced from bilingual training corpora.
The par-allel training data is word-aligned and annotatedwith syntactic parses on either target side (string-to-tree), source side (tree-to-string), or both (tree-to-tree).
A syntactic rule extraction procedure ex-tracts rules which are consistent with the word-alignment and comply with certain syntactic va-lidity constraints.Extracted rules are of the form A,B???,?
,?
?.The right-hand side of the rule ??,?
?
is a bilingualphrase pair that may contain non-terminal sym-bols, i.e.
?
?
(VF?
NF)+and ?
?
(VE?
NE)+,where VFand VEdenote the source and targetterminal vocabulary, and NFand NEdenote thesource and target non-terminal vocabulary, respec-tively.
The non-terminals on the source side andon the target side of rules are linked in a one-to-one correspondence.
The?relation defines thisone-to-one correspondence.
The left-hand sideof the rule is a pair of source and target non-terminals, A ?
NFand B ?
NE.Decoding is typically carried out with a parsing-based algorithm, in our case a customized versionof CYK+(Chappelier and Rajman, 1998).
Theparsing algorithm is extended to handle transla-tion candidates and to incorporate language modelscores via cube pruning (Chiang, 2007).4.1 GHKM String-to-Tree TranslationIn GHKM string-to-tree translation (Galley et al.,2004; Galley et al., 2006; DeNeefe et al., 2007),rules are extracted from training instances whichconsist of a source sentence, a target sentencealong with its constituent parse tree, and a wordalignment matrix.
This tuple is interpreted as adirected graph (the alignment graph), with edgespointing away from the root of the tree, and wordalignment links being edges as well.
A set ofnodes (the frontier set) is determined that con-tains only nodes with non-overlapping closure oftheir spans.1By computing frontier graph frag-ments?fragments of the alignment graph suchthat their root and all sinks are in the frontier set?the GHKM extractor is able to induce a minimalset of rules which explain the training instance.The internal tree structure can be discarded to ob-tain flat SCFG rules.
Minimal rules can be assem-bled to build larger composed rules.Non-terminals on target sides of string-to-treerules are syntactified.
The target non-terminal vo-cabulary of the SCFG contains the set of labels ofthe frontier nodes, which is in turn a subset of (orequal to) the set of constituent labels in the parsetree.
The target non-terminal vocabulary further-more contains an initial non-terminal symbol Q.Source sides of the rules are not decorated withsyntactic annotation.
The source non-terminal vo-cabulary contains a single generic non-terminalsymbol X.In addition to the extracted grammar, the trans-lation system makes use of a special glue grammarwith an initial rule, glue rules, a final rule, and toprules.
The glue rules provide a fall back methodto just monotonically concatenate partial deriva-tions during decoding.
As we add tokens which1The span of a node in the alignment graph is definedas the set of source-side words that are reachable from thisnode.
The closure of a span is the smallest interval of sourcesentence positions that covers the span.149mark the sentence start (?<s>?)
and the sentenceend (?</s>?
), the rules in the glue grammar are ofthe following form:Initial rule:X,Q?
?<s> X?0,<s> Q?0?Glue rules:X,Q?
?X?0X?1,Q?0B?1?for all B ?
NEFinal rule:X,Q?
?X?0</s>,Q?0</s>?Top rules:X,Q?
?<s> X?0</s>,<s> B?0</s>?for all B ?
NE5 Preference GrammarsPreference grammars store a set of implicit labelvectors as additional information with each SCFGrule, along with their relative frequencies giventhe rule.
Venugopal et al.
(2009) have introducedthis technique for hierarchical phrase-based trans-lation.
The implicit label set refines the label setof the underlying synchronous context-free gram-mar.We apply this idea to GHKM translation bynot decorating the target-side non-terminals of theextracted GHKM rules with syntactic labels, butwith a single generic label.
The (explicit) tar-get non-terminal vocabulary NEthus also con-tains only the generic non-terminal symbol X, justlike the source non-terminal vocabulary NF.
Theextraction method remains syntax-directed and isstill guided by the syntactic annotation over thetarget side of the data, but the syntactic labels arestripped off from the SCFG rules.
Rules whichdiffer only with respect to their non-terminal la-bels are collapsed to a single entry in the rule ta-ble, and their rule counts are pooled.
However,the syntactic label vectors that have been seen withthis rule during extraction are stored as implicit la-bel vectors of the rule.5.1 Feature ComputationTwo features are added to the log-linear modelcombination in order to rate the syntactic well-formedness of derivations.
The first feature issimilar to the one suggested by Venugopal et al.
(2009) and computes a score based on the relativefrequencies of implicit label vectors of those ruleswhich are involved in the derivation.
The secondfeature is a simple binary feature which supple-ments the first one by penalizing a rule applicationif none of the implicit label vectors match.We will now formally specify the first feature.2We give a recursive definition of the feature scorehsyn(d) for a derivation d.Let r be the top rule in derivation d, with nright-hand side non-terminals.
Let djdenote thesub-derivation of d at the j-th right-hand side non-terminal of r, 1 ?
j ?
n. hsyn(d) is recursivelydefined ashsyn(d) =?tsyn(d)+n?j=1hsyn(dj) .
(1)In this equation,?tsyn(d) is a simple auxiliaryfunction:?tsyn(d) ={log tsyn(d) if tsyn(d) 6= 00 otherwise(2)Denoting with S the implicit label set of thepreference grammar, we define tsyn(d) as a func-tion that assesses the degree of agreement ofthe preferences of the current rule with the sub-derivations:tsyn(d) =?s?Sn+1(p(s|r) ?n+1?k=2?th(s[k]|dk?1))(3)We use the notation [?]
to address the elements of avector.
The first element of an n+ 1-dimensionalvector s of implicit labels is an implicit label bind-ing of the left-hand side non-terminal of the rule r.p(s|r) is the preference distribution of the rule.Here,?th(Y |d) is another auxiliary function thatrenormalizes the values of th(Y |d):?th(Y |d) =th(Y |d)?Y?
?Sth(Y?|d)(4)It provides us with a probability that the derivationd has the implicit label Y ?
S as its root.
Finally,the function th(Y |d) is defined asth(Y |d) =?s?Sn+1:s[1]=Y(p(s|r) ?n+1?k=2ph(s[k]|dk?1)).
(5)Note that the denominator in Equation (4) thusequals tsyn(d).2Our notational conventions roughly follow the ones byStein et al.
(2010).150This concludes the formal specification of thefirst features.
The second feature hauxSyn(d) penal-izes rule applications in cases where tsyn(d) evalu-ates to 0:hauxSyn(d) ={0 if tsyn(d) 6= 01 otherwise(6)Its intuition is that rule applications that do notcontribute to hsyn(d) should be punished.
Deriva-tions with tsyn(d) = 0 could alternatively bedropped completely, but our approach is to avoidhard constraints.
We will later demonstrate empir-ically that discarding such derivations harms trans-lation quality.6 Soft Source Syntactic ConstraintsSimilar to the implicit target-side label vectorswhich we store in preference grammars, we canlikewise memorize sets of source-side syntactic la-bel vectors with GHKM rules.
In contrast to pref-erence grammars, the rule inventory of the string-to-tree system remains untouched.
The target non-terminals of the SCFG stay syntactified, and thesource non-terminal vocabulary is not extendedbeyond the single generic non-terminal.Source-side syntactic labels are an additional la-tent property of the rules.
We obtain this propertyby parsing the source side of the training data andcollecting the source labels that cover the source-side span of non-terminals during GHKM rule ex-traction.
As the source-side span is frequently notcovered by a constituent in the syntactic parse tree,we employ the composite symbols as suggestedby Zollmann and Venugopal (2006) for the SAMTsystem.3In cases where a span is still not coveredby a symbol, we nevertheless memorize a source-side syntactic label vector but indicate the failurefor the uncovered non-terminal with a special la-bel.
The set of source label vectors that are seenwith a rule during extraction is stored with it in therule table as an additional property.
This informa-tion can be used to implement feature-based softsource syntactic constraints.Table 1 shows an example of a set of sourcelabel vectors stored with a grammar rule.
Thefirst element of each vector is an implicit source-syntactic label for the left-hand side non-terminalof the rule, the remaining elements are implicit3Specifically, we apply relax-parse --SAMT 2 asimplemented in the Moses toolkit (Koehn et al., 2007).source label vector frequency(IN+NP,NN,NN) 7(IN+NP,NNP,NNP) 3(IN++NP,NNS,NNS) 2(IN+NP,NP,NP) 2(PP//SBAR,NP,NP) 1Table 1: The set of source label vec-tors (along with their frequencies in thetraining data) for the rule X,PP-MO ?
?between X?1and X?0,zwischen NN?0und NN?1?.The overall rule frequency is 15.source-syntactic labels for the right-hand sidesource non-terminals.The basic idea for soft source syntactic con-straints features is to also parse the input data ina preprocessing step and try to match input labelsand source label vectors that are associated withSCFG rules.6.1 Feature ComputationUpon application of an SCFG rule, each of thenon-terminals of the rule covers a distinct span ofthe input sentence.
An input label from the inputparse may be available for this span.
We say thata non-terminal has a match in a given source la-bel vector of the rule if its label in the vector is thesame as a corresponding input label over the span.We define three simple features to scorematches and mismatches of the impicit source syn-tactic labels with the labels from the input data:?
A binary feature that fires if a rule is appliedwhich possesses a source syntactic label vec-tor that fully matches the input labels.
Thisfeature rewards exact source label matches ofcomplete rules, i.e., the existance of a vectorin which all non-terminals of the rule havematches.?
A binary feature that fires if a rule is appliedwhich does not possess any source syntacticlabel vector with a match of the label for theleft-hand side non-terminal.
This feature pe-nalizes left-hand side mismatches.?
A count feature that for each rule applicationadds a cost equal to the number of right-handside non-terminals that do not have a matchwith a corresponding input label in any of thesource syntactic label vectors.
This featurepenalizes right-hand side mismatches.151The second and third feature are less strict than thefirst one and give the system a more detailed clueabout the magnitude of mismatch.6.2 Sparse FeaturesWe can optionally add a larger number of sparsefeatures that depend on the identity of the source-side syntactic label:?
Sparse features which fire if a specific inputlabel is matched.
We say that the input la-bel is matched in case the corresponding non-terminal that covers the span has a match inany of the source syntactic label vectors ofthe applied rule.
We distinguish input labelmatches via left-hand side and via right-handside non-terminals.?
Sparse features which fire if the span of a spe-cific input label is covered by a non-terminalof an applied rule, but the input label is notmatched.The first set of sparse features rewards matches,the second set of sparse features penalizes mis-matches.All sparse features have individual scaling fac-tors in the log-linear model combination.
We how-ever implemented a means of restricting the num-ber of sparse features by providing a core set ofsource labels.
If such a core set is specified, thenonly those sparse features are active that dependon the identity of labels within this set.
All sparsefeatures for source labels outside of the core setare inactive.7 ExperimentsWe empirically evaluate the effectiveness ofpreference grammars and soft source syntac-tic constraints for GHKM translation on theEnglish?German language pair using the stan-dard newstest sets of the Workshop on Statisti-cal Machine Translation (WMT) for testing.4Theexperiments are conducted with the open-sourceMoses implementations of GHKM rule extraction(Williams and Koehn, 2012) and decoding withCYK+parsing and cube pruning (Hoang et al.,2009).4http://www.statmt.org/wmt14/translation-task.html7.1 Experimental SetupWe work with an English?German parallel train-ing corpus of around 4.5 M sentence pairs (af-ter corpus cleaning).
The parallel data origi-nates from three different sources which havebeen eligible for the constrained track of theACL 2014 Ninth Workshop on Statistical Ma-chine Translation shared translation task: Europarl(Koehn, 2005), News Commentary, and the Com-mon Crawl corpus as provided on the WMT web-site.
Word alignments are created by aligning thedata in both directions with MGIZA++(Gao andVogel, 2008) and symmetrizing the two trainedalignments (Och and Ney, 2003; Koehn et al.,2003).
The German target side training data isparsed with BitPar (Schmid, 2004).
We removegrammatical case and function information fromthe annotation obtained with BitPar and applyright binarization of the German parse trees priorto rule extraction (Wang et al., 2007; Wang et al.,2010; Nadejde et al., 2013).
For the soft sourcesyntactic constraints, we parse the English sourceside of the parallel data with the English BerkeleyParser (Petrov et al., 2006) and produce compositeSAMT-style labels as discussed in Section 6.When extracting syntactic rules, we impose sev-eral restrictions for composed rules, in particulara maximum number of 100 tree nodes per rule,a maximum depth of seven, and a maximum sizeof seven.
We discard rules with non-terminals ontheir right-hand side if they are singletons in thetraining data.For efficiency reasons, we also enforce a limiton the number of label vectors that are storedas additional properties.
Label vectors are onlystored if they occur at least as often as the 50thmost frequent label vector of the given rule.
Thislimit is applied separately for both source-side la-bel vectors (which are used by the soft syntacticcontraints) and target-side label vectors (which areused by the preference grammar).Only the 200 best translation options per dis-tinct rule source side with respect to the weightedrule-level model scores are loaded by the decoder.Search is carried out with a maximum chart spanof 25, a rule limit of 500, a stack limit of 200, anda k-best limit of 1000 for cube pruning.A standard set of models is used in the base-line, comprising rule translation probabilities andlexical translation probabilities in both directions,word penalty and rule penalty, an n-gram language152system dev newstest2013 newstest2014BLEU TER BLEU TER BLEU TERGHKM string-to-tree baseline 34.7 47.3 20.0 63.3 19.4 65.6+ soft source syntactic constraints 35.1 47.0 20.3 62.7 19.7 64.9+ sparse features 35.8 46.5 20.3 62.8 19.6 65.1+ sparse features (core = non-composite) 35.4 46.8 20.2 62.9 19.6 65.1+ sparse features (core = dev-min-occ100) 35.6 46.7 20.2 62.9 19.6 65.2+ sparse features (core = dev-min-occ1000) 35.4 46.9 20.3 62.8 19.6 65.2+ hard source syntactic constraints 34.6 47.4 19.9 63.4 19.4 65.6string-to-string (GHKM syntax-directed rule extraction) 33.8 48.0 19.3 63.8 18.7 66.2+ preference grammar 33.9 47.7 19.3 63.7 18.8 66.0+ soft source syntactic constraints 34.6 47.0 19.8 62.9 19.5 65.2+ drop derivations with tsyn(d) = 0 34.0 47.5 19.7 63.0 18.8 65.8Table 2: English?German experimental results (truecase).
BLEU scores are given in percentage.A selection of 2000 sentences from the newstest2008-2012 sets is used as development set.model, a rule rareness penalty, and the monolin-gual PCFG probability of the tree fragment fromwhich the rule was extracted (Williams et al.,2014).
Rule translation probabilities are smoothedvia Good-Turing smoothing.The language model (LM) is a large inter-polated 5-gram LM with modified Kneser-Neysmoothing (Kneser and Ney, 1995; Chen andGoodman, 1998).
The target side of the parallelcorpus and the monolingual German News Crawlcorpora are employed as training data.
We usethe SRILM toolkit (Stolcke, 2002) to train the LMand rely on KenLM (Heafield, 2011) for languagemodel scoring during decoding.Model weights are optimized to maximizeBLEU (Papineni et al., 2002) with batch MIRA(Cherry and Foster, 2012) on 1000-best lists.
Weselected 2000 sentences from the newstest2008-2012 sets as a development set.
The selected sen-tences obtained high sentence-level BLEU scoreswhen being translated with a baseline phrase-based system, and do each contain less than30 words for more rapid tuning.
newstest2013 andnewstest2014 are used as unseen test sets.
Trans-lation quality is measured in truecase with BLEUand TER (Snover et al., 2006).57.2 Translation ResultsThe results of the empirical evaluation are given inTable 2.
Our GHKM string-to-tree system attainsstate-of-the-art performance on newstest2013 andnewstest2014.5TER scores are computed with tercom version 0.7.25and parameters -N -s.7.2.1 Soft Source Syntactic ConstraintsAdding the three dense soft source syntactic con-straints features from Section 6.1 improves thebaseline scores by 0.3 points BLEU and 0.6 pointsTER on newstest2013 and by 0.3 points BLEU and0.7 points TER on newstest2014.Somewhat surprisingly, the sparse features fromSection 6.2 do not boost translation quality furtheron any of the two test sets.
We observe a consid-erable improvement on the development set, but itdoes not carry over to the test sets.
We attributedthis to an overfitting effect.
Our source-side softsyntactic label set of composite SAMT-style la-bels comprises 8504 different labels that appear onthe source-side of the parallel training data.
Fourtimes the amount of sparse features are possible(left-hand side/right-hand side matches and mis-matches for each label), though not all of them fireon the development set.
3989 sparse weights aretuned to non-zero values in the experiment.
Due tothe sparse nature of the features, overfitting cannotbe ruled out.We attempted to take measures in order to avoidoverfitting by specifying a core set of source la-bels and deactivating all sparse features for sourcelabels outside of the core set (cf.
Section 6.2).First we specified the core label set as all non-composite labels.
Non-composite labels are theplain constituent labels as given by the syntacticparser.
Complex SAMT-style labels are not in-cluded.
The size of this set is 71 (non-compositelabels that have been observed during rule extrac-tion).
Translation performance on the develop-ment set drops in the sparse features (core = non-153system (tuned on newstest2012) newstest2012 newstest2013 newstest2014BLEU TER BLEU TER BLEU TERGHKM string-to-tree baseline 17.9 65.7 19.9 63.2 19.4 65.3+ soft source syntactic constraints 18.2 65.3 20.3 62.6 19.7 64.7+ sparse features 18.6 64.9 20.4 62.5 19.8 64.7+ sparse features (core = non-composite) 18.4 65.1 20.3 62.7 19.8 64.7+ sparse features (core = dev-min-occ100) 18.4 64.8 20.6 62.2 19.9 64.4Table 3: English?German experimental results (truecase).
BLEU scores are given in percentage.newstest2012 is used as development set.composite) setup, but performance does not in-crease on the test sets.Next we specified the core label set in anotherway: We counted how often each source label oc-curs in the input data on the development set.
Wethen applied a minimum occurrence count thresh-old and added labels to the core set if they did notappear more rarely than the threshold.
We triedvalues of 100 and 1000 for the minimum occur-rence, resulting in 277 and 37 labels being in thecore label set, respectively.
Neither the sparse fea-tures (core = dev-min-occ100) experiment nor thesparse features (core = dev-min-occ1000) experi-ment yields better translation quality than what wesee in the setup without sparse features.We eventually conjectured that the choice of ourdevelopment set might be a reason for the ineffec-tiveness of the sparse features, as on a fine-grainedlevel it could possibly be too different from thetest sets with respect to its syntactic properties.We therefore repeated some of the experimentswith scaling factors optimized on newstest2012(Table 3).
The sparse features (core = dev-min-occ100) setup indeed performs better when tunedon newstest2012, with improvements of 0.7 pointsBLEU and 1.0 points TER on newstest2013 andof 0.5 points BLEU and 0.9 points TER on news-test2014 over the baseline tuned on the same set.Finally, we were interested in demonstratingthat soft source syntactic constraints are superiorto hard source syntactic constraints.
We built asetup that forces the decoder to match source-sidesyntactic label vectors in the rules with input la-bels.6Hard source syntactic constraints are in-deed worse than soft source syntactic constraints(by 0.4 BLEU on newstest2013 and 0.3 BLEU onnewstest2014).
The setup with hard source syntac-tic constraints performs almost exactly at the levelof the baseline.6Glue rules are an exception.
They do not need to matchthe input labels.7.2.2 Preference GrammarIn the series of experiments with a preferencegrammar, we first evaluated a setup with the un-derlying SCFG of the preference grammar sys-tem, but without preference grammar.
We de-note this setup as string-to-string (GHKM syntax-directed rule extraction) in Table 2.
The ex-traction method for this string-to-string system isGHKM syntax-directed with right-binarized syn-tactic target-side parses from BitPar, as in thestring-to-tree setup.
The constituent labels fromthe syntactic parses are however not used to dec-orate non-terminals.
The grammar contains ruleswith a single generic non-terminal instead of syn-tactic ones.
The string-to-string (GHKM syntax-directed rule extraction) setup is on newstest20130.7 BLEU (0.5 TER) worse and on newstest20140.7 BLEU (0.6 TER) worse than the standardGHKM string-to-tree baseline.We then activated the preference grammar asdescribed in Section 5.
GHKM translation with apreference grammar instead of a syntactified targetnon-terminal vocabulary in the SCFG is consider-ably worse than the standard GHKM string-to-treebaseline and barely improves over the string-to-string setup.We added soft source syntactic constraints ontop of the preference grammar system, thus com-bining the two techniques.
Soft source syntacticconstraints give a nice gain over the preferencegrammar system, but the best setup without a pref-erence grammar is not outperformed.
In anotherexperiment, we investigated the effect of droppingderivations with tsyn(d) = 0 (cf.
Section 5.1).
Notethat the second feature hauxSyn(d) is not useful inthis setup, as the system is forced to discard allderivations that would be penalized by that fea-ture.
We deactivated hauxSyn(d) for the experi-ment.
The hard decision of dropping derivationswith tsyn(d) = 0 leads to a performance loss of1540.1 BLEU on newstest2013 and a more severe de-terioration of 0.7 BLEU on newstest2014.8 ConclusionsWe investigated two soft syntactic extensions forGHKM translation: Target-side preference gram-mars and soft source syntactic constraints.Soft source syntactic constraints proved to besuitable for advancing the translation quality overa strong string-to-tree baseline.
Sparse featuresare beneficial beyond just three dense features, butthey require the utilization of an appropriate devel-opment set.
We also showed that the soft integra-tion of source syntactic constraints is crucial: Hardconstraints do not yield gains over the baseline.Preference grammars did not perform well inour experiments, suggesting that translation mod-els with syntactic target non-terminal vocabular-ies are a better choice when building string-to-treesystems.AcknowledgementsThe research leading to these results has re-ceived funding from the European Union Sev-enth Framework Programme (FP7/2007-2013) un-der grant agreements no287658 (EU-BRIDGE)and no288487 (MosesCore).ReferencesOndrej Bojar, Christian Buck, Christian Federmann,Barry Haddow, Philipp Koehn, Johannes Leveling,Christof Monz, Pavel Pecina, Matt Post, HerveSaint-Amand, Radu Soricut, Lucia Specia, and Ale?Tamchyna.
2014.
Findings of the 2014 Work-shop on Statistical Machine Translation.
In Proc.
ofthe Workshop on Statistical Machine Translation(WMT), pages 12?58, Baltimore, MD, USA, June.Jean-C?dric Chappelier and Martin Rajman.
1998.
AGeneralized CYK Algorithm for Parsing Stochas-tic CFG.
In Proc.
of the First Workshop on Tab-ulation in Parsing and Deduction, pages 133?137,Paris, France, April.Stanley F. Chen and Joshua Goodman.
1998.
AnEmpirical Study of Smoothing Techniques for Lan-guage Modeling.
Technical Report TR-10-98, Com-puter Science Group, Harvard University, Cam-bridge, MA, USA, August.Colin Cherry and George Foster.
2012.
Batch Tun-ing Strategies for Statistical Machine Translation.
InProc.
of the Human Language Technology Conf.
/North American Chapter of the Assoc.
for Compu-tational Linguistics (HLT-NAACL), pages 427?436,Montr?al, Canada, June.David Chiang.
2007.
Hierarchical Phrase-BasedTranslation.
Computational Linguistics, 33(2):201?228, June.Steve DeNeefe, Kevin Knight, Wei Wang, and DanielMarcu.
2007.
What Can Syntax-Based MT Learnfrom Phrase-Based MT?
In Proc.
of the 2007Joint Conf.
on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL), pages 755?763,Prague, Czech Republic, June.Michel Galley, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a translation rule?In Proc.
of the Human Language Technology Conf./ North American Chapter of the Assoc.
for Compu-tational Linguistics (HLT-NAACL), pages 273?280,Boston, MA, USA, May.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable Inference and Trainingof Context-Rich Syntactic Translation Models.
InProc.
of the 21st Int.
Conf.
on Computational Lin-guistics and 44th Annual Meeting of the Assoc.
forComputational Linguistics, pages 961?968, Sydney,Australia, July.Qin Gao and Stephan Vogel.
2008.
Parallel Implemen-tations of Word Alignment Tool.
In Software Engi-neering, Testing, and Quality Assurance for NaturalLanguage Processing, SETQA-NLP ?08, pages 49?57, Columbus, OH, USA, June.Kenneth Heafield.
2011.
KenLM: Faster and SmallerLanguage Model Queries.
In Proc.
of the Workshopon Statistical Machine Translation (WMT), pages187?197, Edinburgh, Scotland, UK, July.Hieu Hoang, Philipp Koehn, and Adam Lopez.
2009.A Unified Framework for Phrase-Based, Hierarchi-cal, and Syntax-Based Statistical Machine Transla-tion.
In Proc.
of the Int.
Workshop on Spoken Lan-guage Translation (IWSLT), pages 152?159, Tokyo,Japan, December.Zhongqiang Huang, Jacob Devlin, and Rabih Zbib.2013.
Factored Soft Source Syntactic Constraintsfor Hierarchical Machine Translation.
In Proc.
ofthe Conf.
on Empirical Methods for Natural Lan-guage Processing (EMNLP), pages 556?566, Seat-tle, WA, USA, October.Reinhard Kneser and Hermann Ney.
1995.
ImprovedBacking-Off for M-gram Language Modeling.
InProceedings of the Int.
Conf.
on Acoustics, Speech,and Signal Processing, volume 1, pages 181?184,Detroit, MI, USA, May.Philipp Koehn, Franz Joseph Och, and Daniel Marcu.2003.
Statistical Phrase-Based Translation.
In Proc.of the Human Language Technology Conf.
/ NorthAmerican Chapter of the Assoc.
for ComputationalLinguistics (HLT-NAACL), pages 127?133, Edmon-ton, Canada, May/June.155P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Con-stantin, and E. Herbst.
2007.
Moses: Open SourceToolkit for Statistical Machine Translation.
In Proc.of the Annual Meeting of the Assoc.
for Computa-tional Linguistics (ACL), pages 177?180, Prague,Czech Republic, June.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In Proc.
of the MTSummit X, Phuket, Thailand, September.Yuval Marton and Philip Resnik.
2008.
Soft Syn-tactic Constraints for Hierarchical Phrased-BasedTranslation.
In Proc.
of the Annual Meeting of theAssoc.
for Computational Linguistics (ACL), pages1003?1011, Columbus, OH, USA, June.Maria Nadejde, Philip Williams, and Philipp Koehn.2013.
Edinburgh?s Syntax-Based Machine Transla-tion Systems.
In Proc.
of the Workshop on StatisticalMachine Translation (WMT), pages 170?176, Sofia,Bulgaria, August.Franz Josef Och and Hermann Ney.
2003.
A System-atic Comparison of Various Statistical AlignmentModels.
Computational Linguistics, 29(1):19?51,March.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a Method for AutomaticEvaluation of Machine Translation.
In Proc.
of theAnnual Meeting of the Assoc.
for ComputationalLinguistics (ACL), pages 311?318, Philadelphia, PA,USA, July.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning Accurate, Compact, and In-terpretable Tree Annotation.
In Proc.
of the 21st Int.Conf.
on Computational Linguistics and 44th An-nual Meeting of the Assoc.
for Computational Lin-guistics, pages 433?440, Sydney, Australia, July.Helmut Schmid.
2004.
Efficient Parsing of HighlyAmbiguous Context-Free Grammars with Bit Vec-tors.
In Proc.
of the Int.
Conf.
on ComputationalLinguistics (COLING), Geneva, Switzerland, Au-gust.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A Studyof Translation Edit Rate with Targeted Human An-notation.
In Proc.
of the Conf.
of the Assoc.
forMachine Translation in the Americas (AMTA), pages223?231, Cambridge, MA, USA, August.Daniel Stein, Stephan Peitz, David Vilar, and HermannNey.
2010.
A Cocktail of Deep Syntactic Fea-tures for Hierarchical Machine Translation.
In Proc.of the Conf.
of the Assoc.
for Machine Translationin the Americas (AMTA), Denver, CO, USA, Octo-ber/November.Andreas Stolcke.
2002.
SRILM ?
an Extensible Lan-guage Modeling Toolkit.
In Proc.
of the Int.
Conf.on Spoken Language Processing (ICSLP), volume 3,Denver, CO, USA, September.Ashish Venugopal, Andreas Zollmann, Noah A. Smith,and Stephan Vogel.
2009.
Preference Grammars:Softening Syntactic Constraints to Improve Statis-tical Machine Translation.
In Proc.
of the Hu-man Language Technology Conf.
/ North AmericanChapter of the Assoc.
for Computational Linguistics(HLT-NAACL), pages 236?244, Boulder, CO, USA,June.Wei Wang, Kevin Knight, and Daniel Marcu.
2007.Binarizing Syntax Trees to Improve Syntax-BasedMachine Translation Accuracy.
In Proc.
of the 2007Joint Conf.
on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL), pages 746?754,Prague, Czech Republic, June.Wei Wang, Jonathan May, Kevin Knight, and DanielMarcu.
2010.
Re-structuring, Re-labeling, andRe-aligning for Syntax-based Machine Translation.Computational Linguistics, 36(2):247?277, June.Philip Williams and Philipp Koehn.
2012.
GHKMRule Extraction and Scope-3 Parsing in Moses.
InProc.
of the Workshop on Statistical Machine Trans-lation (WMT), pages 388?394, Montr?al, Canada,June.Philip Williams, Rico Sennrich, Maria Nadejde,Matthias Huck, Eva Hasler, and Philipp Koehn.2014.
Edinburgh?s Syntax-Based Systems atWMT 2014.
In Proc.
of the Workshop on Statis-tical Machine Translation (WMT), pages 207?214,Baltimore, MD, USA, June.Jiajun Zhang, Feifei Zhai, and Chengqing Zong.
2011.Augmenting String-to-Tree Translation Models withFuzzy Use of Source-side Syntax.
In Proc.
of theConf.
on Empirical Methods for Natural LanguageProcessing (EMNLP), pages 204?215, Edinburgh,Scotland, UK, July.Andreas Zollmann and Ashish Venugopal.
2006.
Syn-tax Augmented Machine Translation via Chart Pars-ing.
In Proc.
of the Workshop on Statistical MachineTranslation (WMT), pages 138?141, New York City,NY, USA, June.156
