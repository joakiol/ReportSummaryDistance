Coling 2010: Poster Volume, pages 1077?1085,Beijing, August 2010Log-linear weight optimisation via Bayesian Adaptation in StatisticalMachine TranslationGerma?n Sanchis-Trilles and Francisco CasacubertaDepartamento de Sistemas Informa?ticos y Computacio?nInstituto Tecnolo?gico de Informa?ticaUniversidad Polite?cnica de Valencia{gsanchis,fcn}@dsic.upv.esAbstractWe present an adaptation technique forstatistical machine translation, which ap-plies the well-known Bayesian learningparadigm for adapting the model param-eters.
Since state-of-the-art statistical ma-chine translation systems model the trans-lation process as a log-linear combinationof simpler models, we present the formalderivation of how to apply such paradigmto the weights of the log-linear combina-tion.
We show empirical results in whicha small amount of adaptation data is ableto improve both the non-adapted systemand a system which optimises the above-mentioned weights on the adaptation setonly, while gaining both in reliability andspeed.1 IntroductionThe adaptation problem is a very common issue instatistical machine translation (SMT), where it isfrequent to have very large collections of bilingualdata belonging to e.g.
proceedings from interna-tional entities such as the European Parliament orthe United Nations.
However, if we are currentlyinterested in translating e.g.
printer manuals ornews data, we will need to find a way in which wecan take advantage of such data.The grounds of modern SMT were establishedin (Brown et al, 1993), where the machine trans-lation problem was defined as follows: given asentence f from a certain source language, anequivalent sentence e?
in a given target languagethat maximises the posterior probability is to befound.
According to the Bayes decision rule, suchstatement can be specified as follows:e?
= argmaxePr(e|f) (1)Recently, a direct modelling of the posteriorprobability Pr(e|f) has been widely adopted, and,to this purpose, different authors (Papineni et al,1998; Och and Ney, 2002) proposed the use of theso-called log-linear models, wherep(e|f) = exp?Kk=1 ?khk(f , e)?e?
exp?Kk=1 ?khk(f , e?
)(2)and the decision rule is given by the expressione?
= argmaxeK?k=1?khk(f , e) (3)where hk(f , e) is a score function representing animportant feature for the translation of f into e, asfor example the language model of the target lan-guage, a reordering model or several translationmodels.
K is the number of models (or features)and ?k are the weights of the log-linear combina-tion.
Typically, the weights ?
= [?1, .
.
.
, ?K ]Tare optimised with the use of a development set.The use of log-linear models implied an impor-tant break-through in SMT, allowing for a signifi-cant increase in the quality of the translations pro-duced.
In this work, we present a Bayesian tech-nique for adapting the weights of such log-linearmodels according to a small set of adaptation data.In this paper, we will be focusing on adaptingthe weights vector ?, since appropriate values ofsuch vector for a given domain do not necessarilyimply a good combination in other domains.
Onena?
?ve way in which some sort of adaptation canbe performed on ?
is to re-estimate these weights1077from scratch only on the adaptation data.
How-ever, such re-estimation may not be a good idea,whenever the amount of adaptation data availableis not too big.
On the one hand, because smallamounts of adaptation data may easily yield over-trained values of ?, which may even lead to adegradation of the translation quality.
On the otherhand, because in some scenarios it is not feasibleto re-estimate them because of the time it wouldtake.
Moreover, considering a re-estimation of?
by using both the out-of-domain data and theadaptation set would not be appropriate either.
Forsmall amounts of adaptation data, such data wouldhave no impact on the final value of ?, and thetime required would be even higher.
One suchsituation may be the Interactive Machine Trans-lation (IMT) paradigm (Barrachina et al, 2009),in which a human translator may start translatinga new document, belonging to a specific domain,and the system is required to produce an appro-priate output as soon as possible without any priorre-training.In this paper, a Bayesian adaptation approachsolving both problems is presented.
Nevertheless,adapting ?
constitutes just a first step towardsthe adaptation of all the parameters of the SMTmodel.The rest of this paper is structured as follows.
Innext Section, we perform a brief review of currentapproaches to adaptation and Bayesian learning inSMT.
Section 3 describes the typical frameworkfor phrase-based translation in SMT.
In Section 4,we present the way in which we apply Bayesianadaptation (BA) to log-linear models in SMT.
InSection 5, we describe the practical approxima-tions applied before implementing the BA tech-nique described.
In Section 6, experimental de-sign and results are detailed.
Conclusions and fu-ture work are explained in Section 7.2 Related workAdaptation in SMT is a research field that is re-ceiving an increasing amount of attention.
In(Nepveu et al, 2004), adaptation techniques wereapplied to IMT, following the ideas by (Kuhnand Mori, 1990) and adding cache language mod-els (LM) and TMs to their system.
In (Koehnand Schroeder, 2007), different ways to combineavailable data belonging to two different sourceswas explored; in (Bertoldi and Federico, 2009)similar experiments were performed, but consid-ering only additional source data.
In (Civera andJuan, 2007), alignment model mixtures were ex-plored as a way of performing topic-specific adap-tation.
Other authors (Zhao et al, 2004; Sanchis-Trilles et al, 2009), have proposed the use of clus-tering in order to extract sub-domains of a largeparallel corpus and build more specific LMs andTMs, which are re-combined in test time.With respect to BA in SMT, the authors are notaware of any work up to the date that follows suchparadigm.
Nevertheless, there have been some re-cent approaches towards dealing with SMT fromthe Bayesian learning point of view.
In (Zhanget al, 2008), Bayesian learning was applied forestimating word-alignments within a synchronousgrammar.3 Phrase-based SMTOne of the most popular instantiations of log-linear models in SMT are phrase-based (PB) mod-els (Zens et al, 2002; Koehn et al, 2003).
PBmodels allow to capture contextual information tolearn translations for whole phrases instead of sin-gle words.
The basic idea of PB translation is tosegment the source sentence into phrases, then totranslate each source phrase into a target phrase,and finally reorder the translated target phrases inorder to compose the target sentence.
For thispurpose, phrase-tables are produced, in which asource phrase is listed together with several tar-get phrases and the probability of translating theformer into the latter.
PB models were employedthroughout this work.Typically, the weights of the log-linear com-bination in Equation 3 are optimised by meansof Minimum Error Rate Training (MERT) (Och,2003).
Such algorithm consists of two basic steps.First, n-best hypotheses are extracted for each oneof the sentences of a given development set.
Next,the optimum ?
is computed so that the best hy-potheses in the n-best list, according to a referencetranslation and a given metric, are ranked higherwithin such n-best list.
These two steps are re-peated until convergence.This approach has two main problems.
On the1078one hand, that it heavily relies on having a fairamount of data available as development set.
Onthe other hand, that it only relies on the data inthe development set.
These two problems haveas consequence that, if the development set madeavailable to the system is not big enough, MERTwill most likely become unstable and fail in ob-taining an appropriate weight vector ?.However, it is quite common to have a greatamount of data available in a given domain, butonly a small amount from the specific domain weare interested in translating.
Precisely this sce-nario is appropriate for BA: under this paradigm,the weight vector ?
is biased towards the opti-mal one according to the adaptation set, whileavoiding over-training towards such set by notforgetting the generality provided by the trainingset.
Furthermore, recomputing ?
from scratchby means of MERT may imply a computationaloverhead which may not be acceptable in certainenvironments, such as SMT systems configuredfor online translation, IMT or Computer AssistedTranslation, in which the final human user is wait-ing for the translations to be produced.4 Bayesian adaptation for SMTThe main idea behind Bayesian learning (Duda etal., 2001; Bishop, 2006) is that model parametersare viewed as random variables having some kindof a priori distribution.
Observing these randomvariables leads to a posterior density, which typi-cally peaks at the optimal values of these parame-ters.
Following the notation in Equation 1, previ-ous statement is specified asp(e|f ;T ) =?p(e, ?|f ;T )d?
(4)where T represents the complete training set and?
are the model parameters.However, since we are interested in Bayesianadaptation, we need to consider one training setT and one adaptation set A, leading top(e|f ;T,A) ?
?p(?|T,A)p(e|f , ?)d?
(5)In Equation 5, the integral over the complete para-metric space forces the model to take into accountall possible values of the model parameters, al-though the prior over the parameters implies thatour model will prefer parameter values which arecloser to our prior knowledge.
Two assumptionshave been made: first, that the output sentence eonly depends on the model parameters (and not onthe complete training and adaptation data).
Sec-ond, that the model parameters do not dependon the actual input sentence f .
Such simplifica-tions lead to a decomposition of the integral intwo parts: the first one, p(?|T,A) will assess howgood the current model parameters are, and thesecond one, p(e|f , ?
), will account for the qualityof the translation e given the current model pa-rameters.Then, the decision rule given in Equation 1 isredefined ase?
= argmaxePr(e|f ;T,A) (6)Operating with the probability of ?, we obtain:p(?|T,A) = p(A|?
;T ) p(?|T )?
p(A|?)
p(?|T ) d?
(7)p(A|?
;T ) =??a?Ap(fa|?)
p(ea|fa, ?)
(8)where the probability of the adaptation data hasbeen assumed to be independent of the trainingdata and has been modelled as the probability ofeach bilingual sample (fa, ea) ?
A being gener-ated by our translation model.Assuming that the model parameters depend onthe training data and follow a normal distribution,we obtainp(?|T )= 1(2pi)d/2 exp{?12(???T)T(??
?T)}(9)where ?T is the set of parameters estimated on thetraining set and the variance has been assumed tobe bounded for all parameters.
d is the dimension-ality of ?.Lastly, assuming that our translation model is alog-linear model as described in Equation 3 andthat the only parameters we want to adapt are thelog-linear weights:p(e|f , ?)
= exp?k ?k fk(f , e)?e?
exp?k ?k fk(f , e?
)(10)1079where the model parameters ?
have been instanti-ated to include only the log-linear weights ?.Finally, combining Equations 8, 9 and 10, andconsidering only as model parameters the log-linear weights, we obtain:p(e|f ;T,A)=Z?p(A|?
;T )p(?|T )p(e|f ,?
)d?= Z?
?
?a?Aexp?k ?k fk(fa, ea)?e?
exp?k ?k fk(fa, e?)?exp{?12(??
?T )T (??
?T )}?exp?k ?k fk(f , e)?e?
exp?k ?k fk(f , e?)d?
(11)where Z is the denominator present in the previ-ous equation and may be factored out because itdoes not depend on the integration variable.
It hasalso been assumed that p(fa|?)
is uniform and canalso be factored out.5 Practical approximationsAlthough the integral described in Equation 11 isthe right thing to do from the theoretical point ofview, there are several issues which need to betreated first before implementing it.Since computing the integral over the completeparametric space is computationally impossible inthe case of SMT, we decided to perform a MonteCarlo like sampling of these parameters by assum-ing that the parameters follow a normal distribu-tion centred in ?T , the weight vector obtainedfrom the training data.
This sampling was doneby choosing alternatively only one of the weightsin ?T , modifying it randomly within a given inter-val, and re-normalising accordingly.
Equation 11is approximated in practise asp(e|f ;T,A) =?
?m?MC(?T )p(A|?
;T )p(?|T )p(e|f ,?
)where MC(?T ) is the set of ?m weights gener-ated by the above-mentioned procedure.There is still one issue when trying to imple-ment Equation 11.
The denominator within thecomponents p(A|?
;T ) and p(e|f ,?)
contains asum over all possible sentences of the target lan-guage, which is not computable.
For this reason,?e?
is approximated as the sum over all the hy-pothesis within a given n-best list.
Moreover, in-stead of performing a full search of the best pos-sible translation of a given input sentence, we willperform a rerank of the n-best list provided by thedecoder according to Equation 11.Typical state-of-the-art PB SMT systems do notguarantee complete coverage of all possible sen-tence pairs due to the great number of heuris-tic decisions involved in the estimation of thetranslation models.
Moreover, out-of-vocabularywords may imply that the SMT model is unableto explain a certain bilingual sentence completely.Hence, p(A|?
;T ) is approximated asp(A|?
;T )??
?a?Aexp?k ?k fk(fa, e?a)?e?
exp?k ?kfk(fa, e?
)(12)where e?
represents the best hypothesis the searchalgorithm is able to produce, according to a giventranslation quality measure.
As in Equation 11,p(fa|?)
has been assumed uniform.Once the normalisation factor within Equa-tion 7 has been removed, and the above-mentioned approximations have been introduced,p(e|f ;T,A) is no longer a probability.
This factcannot be underestimated, since it means that theterms p(A|?
;T ) and p(e|f ,?)
on the one hand,and p(?|T ) on the other, may have very differentnumeric ranges.
For this reason, and in order toweaken the influence of this fact, we introduce aleveraging term ?, such thatp(e|f ;T,A) =?
?m?MC(?T )(p(A|?
;T )p(e|f ,?))
1?
p(?|T ) (13)Although there are other, more standard, ways ofadding this leveraging term, we chose this one fornumeric reasons.6 Experiments6.1 Experimental setupTranslation quality will be assessed by meansof BLEU and TER scores.
BLEU measures n-gram precision with a penalty for sentences thatare too short (Papineni et al, 2001), whereasTER (Snover et al, 2006) is an error metric that1080Spanish EnglishTrainingSentences 731KRun.
words 15.7M 15.2MVocabulary 103K 64KDevelopmentSentences 2KRun.
words 61K 59KOoV words 208 127Table 1: Main figures of the Europarl corpus.
OoVstands for Out of Vocabulary.
K/M stands forthousands/millions of elements.Spanish EnglishTest 2008Sentences 2051Run.
words 50K 53KOoV.
words 1247 1201Test 2010Sentences 2489Run.
words 62K 66KOoV.
words 1698 1607Table 2: Main figures of the News-Commentarytest sets.
OoV stands for Out of Vocabulary wordswith respect to the Europarl corpus.computes the minimum number of edits requiredto modify the system hypotheses so that theymatch the references.
Possible edits include in-sertion, deletion, substitution of single words andshifts of word sequences.For computing e?
as described in Equation 12,TER was used, since BLEU implements a geo-metrical average which is zero whenever there isno common 4-gram between reference and hy-pothesis.
Hence, it is not well suited for our pur-poses since the complete set of n-best candidatesprovided by the decoder can score zero.As a first baseline system, we trained a SMTsystem on the Europarl Spanish?English trainingdata, in the partition established in the Workshopon SMT of the NAACL 2006 (Koehn and Monz,2006), using the training and development dataprovided that year.
The Europarl corpus (Koehn,2005) is built from the transcription of EuropeanParliament speeches published on the web.
Statis-tics are provided in Table 1.We used the open-source MT toolkitMoses (Koehn et al, 2007)1 in its defaultmonotonic setup, and estimated the weights ofthe log-linear combination using MERT on theEuroparl development set.
A 5-gram LM withinterpolation and Kneser-Ney smoothing (Kneserand Ney, 1995) was also estimated.Since our purpose is to adapt the initial weight1Available from http://www.statmt.org/moses/vector obtained during the training stage (i.e.
theone obtained after running MERT on the Eu-roparl development set), the tests sets provided forthe 2008 and 2010 evaluation campaigns of theabove-mentioned workshop (Table 2) were alsoused.
These test sets, unlike the one provided in2006, were extracted from a news data corpus, andcan be considered out of domain if the system hasbeen trained on Europarl data.All the experiments displaying BA results werecarried out by sampling a total of 100 randomweights, according to preliminary investigation,following the procedure described in Section 5.For doing this, one single weight was added a ran-dom amount between 0.5 and ?0.5, and then thewhole ?
was re-normalised.With the purpose of providing robustness to theresults, every point in each plot of this paper con-stitutes the average of 10 repetitions, in whichthe adaptation data was randomly drawn from theNews-Commentary test set 2008.6.2 Comparison between BA and MERTThe effect of increasing the number of adaptationsamples made available to the system was inves-tigated.
The adaptation data was used either forestimating ?
using MERT, or as adaptation sam-ple for our BA technique.
Results can be seen inFigure 1.
The ?
scaling factor described in Equa-tion 13 was set to 8.
As it can be seen, the BAadaptation technique is able to improve consis-tently the translation quality obtained by the non-adapted system, both in terms of BLEU and TER.These improvements are quite stable even withas few as 10 adaptation samples.
This result isvery interesting, since re-estimating ?
by meansof MERT is only able to yield improvements whenprovided with at least 100 adaptation samples, dis-playing a very chaotic behaviour until that point.In order to get a bit more insight about thischaotic behaviour, confidence interval sizes areshown in Figure 2, at a 95% confidence level, re-sulting of the repetitions described above.
MERTyields very large confidence intervals (as largeas 10 TER/BLEU points for less than 100 sam-ples), turning a bit more stable from that pointon, where the size of the confidence interval con-verges slowly to 1 TER/BLEU point.
In contrast,10815758596061626310  100  1000TERNumber of adaptation samplesBA ?
=  8mertbaseline18.51919.52020.52121.52222.510  100  1000BLEUNumber of adaptation samplesBA ?
=  8mertbaselineFigure 1: Comparison of translation quality, as measured by BLEU and TER, for baseline system,adapted systems by means of BA and MERT.
Increasing number of samples is considered.0.0010.010.111010010  100  1000TER CDSNumber of adaptation samplesBA ?
=  1BA ?
=  32mert0.0010.010.111010  100  1000BLEUCDSNumber of adaptation samplesBA ?
=  1BA ?
=  32mertFigure 2: Confidence interval sizes (CDS) for MERT and two BA systems, for different number ofadaptation samples.
For visibility purposes, both axes are in logarithmic scale.our BA technique yields very small confidence in-tervals, about half a TER/BLEU point in the worstcase, with only 10 adaptation samples.
This isworth emphasising, since estimating ?
by meansof MERT when very few adaptation data is avail-able may improve the final translation quality, butmay also degrade it to a much larger extent.
Incontrast, our BA technique shows stable and reli-able improvements from the very beginning.
Pre-cisely under such circumstances is an adaptationtechnique useful: when the amount of adaptationdata is small.
In other cases, the best thing onecan do is to re-estimate the model parameters fromscratch.Example translations, extracted from the exper-iments detailed above, are shown in Figure 5.6.3 Varying ?So as to understand the role of scaling factor ?,results obtained varying it are shown in Figure 3.Several things should be noted about these plots:?
Increasing ?
leads to smoother adaptationcurves.
This is coherent with the confidenceinterval sizes shown in Figure 1.?
Smaller values of ?
lead to a slight degrada-tion in translation quality when the amountof adaptation samples becomes larger.
Thereason for this can be explained by look-ing at Equation 13.
Since p(A|?
;T ) isimplemented as a product of probabilities,the more adaptation samples the smaller be-comes p(A|?
;T ), and a higher value of ?
isneeded to compensate this fact.
This sug-gests the need of a ?
which depends on thesize of the adaptation sample.?
Larger values of ?
do not suffer the prob-lem described above, but yield smaller im-provements in terms of translation quality forsmaller amount of samples.108257.857.95858.158.258.310  100  1000TERNumber of adaptation samples?
=  1?
=  2?
=  4?
=  8?
=  16?
=  3221.92222.122.222.322.422.510  100  1000BLEUNumber of adaptation samples?
=  1?
=  2?
=  4?
=  8?
=  16?
=  32Figure 3: Translation quality comparison for different ?
values and number of adaptation samples.It might seem odd that translation quality asmeasured by BLEU drops almost constantly as thenumber of adaptation samples increases.
How-ever, it must be noted that the BA technique im-plemented is set to optimise TER, and not BLEU.Analysing the BLEU scores obtained, we realisedthat the n-gram precision does increase, but thefinal BLEU score drops because of a worseningbrevity penalty, which is not taken into accountwhen optimising the TER score.6.3.1 Increasing the n-best orderThe effect of increasing the order of n-best con-sidered was also analysed.
In order to avoid anoverwhelming amount of results, only those ob-tained when considering 100 adaptation samplesare displayed in Figure 4.
As it can be seen,TER drops monotonically for all ?
values, untilabout 800, where it starts to stabilise.
Similarbehaviour is observed in the case of BLEU, al-though depending on ?
the curve shows an im-provement or a degradation.
Again, this is dueto the brevity penalty, which TER does not imple-ment, and which induces this inverse correlationbetween TER and BLEU when optimising TER.7 Conclusions and future workWe have presented a Bayesian theoretical frame-work for adapting the parameters of a SMT sys-tem.
We have derived the equations needed to im-plement BA of the log-linear weights of a SMTsystem, and present promising results with a state-of-the-art SMT system using standard corpora inSMT.
Such results prove that the BA frameworkcan be very effective when adapting the men-tioned weights.
Consistent improvements are ob-tained over the baseline system with as few as10 adaptation samples.
The BA technique imple-mented is able to yield results comparable witha complete re-estimation of the parameters evenwhen the amount of adaptation data is sufficientfor such re-estimation to be feasible.
Experi-mental results show that our adaptation techniqueproves to be much more stable than MERT, whichrelies very heavily on the amount of adaptationdata and turns very unstable whenever few adap-tation samples are available.
It should be empha-sised that an adaptation technique, by nature, isonly useful whenever few adaptation data is avail-able, and our technique proves to behave well insuch context.Intuitively, the BA technique presented needsfirst to compute a set of random weights, whichare the result of sampling a gaussian distributionwhose mean is the best weight vector obtained intraining.
Then, each hypothesis of a certain testsource sentence is rescored according to the fol-lowing three components:?
The probability of the adaptation corpus un-der each specific random weight?
The probability of such random weight ac-cording to a prior over the weight vector?
The probability of the current hypothesis un-der those weightsConcerning computational time, our adaptationtechnique can easily be implemented within thedecoder itself, without any significant increase incomputational complexity.
We consider this im-108357.657.757.857.95858.1100  200  300  400  500  600  700  800  900  1000TEROrder of n-best considered?
= 1?
= 2?
= 4?
= 8?
= 16?
= 3221.721.821.92222.122.222.322.422.522.6100  200  300  400  500  600  700  800  900  1000BLEUOrder of n-best considered?=1?=2?=4?=8?=16?=32Figure 4: Translation quality for different ?
values and n-best sizes considered in the BA system.source en afganista?n , barack obama espera que se repita el milagro .reference barack obama hopes that , in afghanistan , the miracle will repeat itself .baseline in afghanistan , barack obama waiting to be repeated the miracle .BA s10 in afghanistan , barack obama expected to repeat the miracle .BA s600 in afghanistan , barack obama expected to repeat the miracle .MERT s10 in afghanistan , barack obama expected to repeat of the miracle .MERT s600 in afghanistan , barack obama hopes that a repetition of the miracle .source al final todo fue ma?s rpido de lo que se penso?
.reference it all happened a lot faster than expected .baseline at the end of all was more quickly than we thought .BA s10 ultimately everything was more quickly than we thought .BA s600 ultimately everything was more quickly than we though .MERT s10 the end all was quicker than i thought .MERT s600 ultimately everything was quicker than i thought .Figure 5: Example of translations found in the corpus.
s10 means that only 10 adaptation sampleswere considered, whereas s600 means that 600 were considered.portant, since it implies that rerunning MERT foreach adaptation set is not needed, and this is im-portant whenever the final system is set up in anon-line environment.The derivation presented here can be easily ex-tended in order to adapt the feature functions ofthe log-linear model (i.e.
not the weights).
This isbound to have a more important impact on transla-tion quality, since the amount of parameters to beadapted is much higher.
We plan to address thisissue in future work.In addition, very preliminary experiments showthat, when considering reordering, the advantagesdescribed here are larger.A preliminary version of the present paperwas accepted at the Joint IAPR InternationalWorkshops on Structural and Syntactic PatternRecognition and Statistical Techniques in PatternRecognition 2010.
The main contributions ofthe present paper constitute more extensive ex-periments, which have been conducted on stan-dard SMT corpora.
Furthermore, in this paper wepresent the results of adding the leveraging term?, of applying a random, Monte-Carlo like weightsampling (which was not done previously), and anextensive analysis of the effect of varying the or-der of n-best considered.We also plan to implement Markov ChainMonte Carlo for sampling the parameters, andanalyse the effect of combining the in-domain andout of domain data for MERT.
Such results werenot included here for time constraints.AcknowledgmentsThis paper is based upon work supported bythe EC (FEDER/FSE) and the Spanish MICINNunder the MIPRCV ?Consolider Ingenio 2010?program (CSD2007-00018) and the iTrans2(TIN2009-14511) project.
Also supported bythe Spanish MITyC under the erudito.com (TSI-020110-2009-439) project and by the GeneralitatValenciana under grant Prometeo/2009/014.The authors would like to thank the anonimousreviewers for their constructive comments.1084ReferencesBarrachina, S., O. Bender, F. Casacuberta, J. Civera,E.
Cubel, S. Khadivi, A. Lagarda H. Ney, J. Toma?s,and E. Vidal.
2009.
Statistical approaches tocomputer-assisted translation.
Computational Lin-guistics, 35(1):3?28.Bertoldi, N. and M. Federico.
2009.
Domain adapta-tion in statistical machine translation with monolin-gual resources.
In Proc.
of EACL WMT.Bishop, C. M. 2006.
Pattern Recognition and Ma-chine Learning.
Springer.Brown, P.F., S.A. Della Pietra, V.J.
Della Pietra, andR.L.
Mercer.
1993.
The mathematics of ma-chine translation.
In Computational Linguistics,volume 19, pages 263?311, June.Civera, J. and A. Juan.
2007.
Domain adaptationin statistical machine translation with mixture mod-elling.
In Proc.
of ACL WMT.Duda, R., P. Hart, and D. Stork.
2001.
Pattern Classi-fication.
Wiley-Interscience.Kneser, R. and H. Ney.
1995.
Improved backing-offfor m-gram language modeling.
IEEE Int.
Conf.
onAcoustics, Speech and Signal Processing, II:181?184, May.Koehn, P. and C. Monz, editors.
2006.
Proc.
on theWorkshop on SMT.
Association for ComputationalLinguistics, June.Koehn, P. and J. Schroeder.
2007.
Experiments in do-main adaptation for statistical machine translation.In Proc.
of ACL WMT.Koehn, P., F. J. Och, and D. Marcu.
2003.
Statisticalphrase-based translation.
In Proc.
HLT/NAACL?03,pages 48?54.Koehn et al, P. 2007.
Moses: Open source toolkit forstatistical machine translation.
In Proc.
of the ACLDemo and Poster Sessions, pages 177?180, Prague,Czech Republic.Koehn, P. 2005.
Europarl: A parallel corpus for statis-tical machine translation.
In MT Summit.Kuhn, R. and R. De Mori.
1990.
A cache-based nat-ural language model for speech recognition.
IEEETransactions on PAMI, 12(6):570?583.Nepveu, L., G. Lapalme, P. Langlais, and G. Foster.2004.
Adaptive language and translation models forinteractive machine translation.
In Proc.
of EMNLP.Och, F. and H. Ney.
2002.
Discriminative trainingand maximum entropy models for statistical ma-chine translation.
In Proc.
of the ACL?02, pages295?302.Och, F.J. 2003.
Minimum error rate training for statis-tical machine translation.
In Proc.
of Annual Meet-ing of the ACL, July.Papineni, K., S. Roukos, and T. Ward.
1998.
Max-imum likelihood and discriminative training of di-rect translation models.
In Proc.
of ICASSP, pages189?192.Papineni, K., A. Kishore, S. Roukos, T. Ward, andW.
Jing Zhu.
2001.
Bleu: A method for automaticevaluation of machine translation.
In Technical Re-port RC22176 (W0109-022).Sanchis-Trilles, G., M. Cettolo, N. Bertoldi, andM.
Federico.
2009.
Online Language Model Adap-tation for Spoken Dialog Translation.
In Proc.
ofIWSLT, Tokyo.Snover, Matthew, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proc.
of AMTA?06.Zens, R., F.J. Och, and H. Ney.
2002.
Phrase-basedstatistical machine translation.
In Proc.
of KI?02,pages 18?32.Zhang, Hao, Chris Quirk, Robert C. Moore, andDaniel Gildea.
2008.
Bayesian learning of non-compositional phrases with synchronous parsing.In Proceedings of ACL-08: HLT, pages 97?105,Columbus, Ohio, June.
Association for Computa-tional Linguistics.Zhao, B., M. Eck, and S. Vogel.
2004.
Languagemodel adaptation for statistical machine translationwith structured query models.
In Proc.
of CoLing.1085
