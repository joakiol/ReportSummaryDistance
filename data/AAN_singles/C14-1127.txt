Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1341?1349, Dublin, Ireland, August 23-29 2014.Hybrid Deep Belief Networks forSemi-supervised Sentiment ClassificationShusen Zhou?Qingcai Chen?Xiaolong Wang?Xiaoling Li?
?School of Information and Electrical Engineering, Ludong University, Yantai 264025, China.
?Shenzhen Graduate School, Harbin Institute of Technology, Shenzhen 518055, China.zhoushusen@gmail.com, qingcai.chen@hitsz.edu.cnwangxl@insun.hit.edu.cn, appleling@live.cnAbstractIn this paper, we develop a novel semi-supervised learning algorithm called hybrid deep be-lief networks (HDBN), to address the semi-supervised sentiment classification problem withdeep learning.
First, we construct the previous several hidden layers using restricted Boltzmannmachines (RBM), which can reduce the dimension and abstract the information of the reviewsquickly.
Second, we construct the following hidden layers using convolutional restricted Boltz-mann machines (CRBM), which can abstract the information of reviews effectively.
Third, theconstructed deep architecture is fine-tuned by gradient-descent based supervised learning with anexponential loss function.
We did several experiments on five sentiment classification datasets,and show that HDBN is competitive with previous semi-supervised learning algorithm.
Ex-periments are also conducted to verify the effectiveness of our proposed method with differentnumber of unlabeled reviews.1 IntroductionRecently, more and more people write reviews and share opinions on the World Wide Web, which presenta wealth of information on products and services (Liu et al., 2010).
These reviews will not only help otherusers make better judgements but they are also useful resources for manufacturers of products to keeptrack and manage customer opinions (Wei and Gulla, 2010).
However, there are large amount of reviewsfor every topic, it is difficult for a user to manually learn the opinions of an interesting topic.
Sentimentclassification, which aims to classify a text according to the expressed sentimental polarities of opinionssuch as ?positive?
or ?negtive?, ?thumb up?
or ?thumb down?, ?favorable?
or ?unfavorable?
(Li et al., 2010),can facilitate the investigation of corresponding products or services.In order to learn a good text classifier, a large number of labeled reviews are often needed for training(Zhen and Yeung, 2010).
However, labeling reviews is often difficult, expensive or time consuming(Chapelle et al., 2006).
On the other hand, it is much easier to obtain a large number of unlabeled reviews,such as the growing availability and popularity of online review sites and personal blogs (Pang and Lee,2008).
In recent years, a new approach called semi-supervised learning, which uses large amount ofunlabeled data together with labeled data to build better learners (Zhu, 2007), has been developed in themachine learning community.There are several works have been done in semi-supervised learning for sentiment classification, andget competitive performance (Li et al., 2010; Dasgupta and Ng, 2009; Zhou et al., 2010).
However, mostof the existing semi-supervised learning methods are still far from satisfactory.
As shown by several re-searchers (Salakhutdinov and Hinton, 2007; Hinton et al., 2006), deep architecture, which composed ofmultiple levels of non-linear operations, is expected to perform well in semi-supervised learning becauseof its capability of modeling hard artificial intelligent tasks.
Deep belief networks (DBN) is a represen-tative deep learning algorithm achieving notable success for text classification, which is a directed beliefnets with many hidden layers constructed by restricted Boltzmann machines (RBM), and refined by agradient-descent based supervised learning (Hinton et al., 2006).
Ranzato and Szummer (Ranzato andThis work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footerare added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/1341Szummer, 2008) propose an algorithm to learn text document representations based on semi-supervisedauto-encoders that are combined to form a deep network.
Zhou et al.
(Zhou et al., 2010) propose a nov-el semi-supervised learning algorithm to address the semi-supervised sentiment classification problemwith active learning.
The key issue of traditional DBN is the efficiency of RBM training.
Convolutionalneural networks (CNN), which are specifically designed to deal with the variability of two dimensionalshapes, have had great success in machine learning tasks and represent one of the early successes ofdeep learning (LeCun et al., 1998).
Desjardins and Bengio (Desjardins and Bengio, 2008) adapt RBMto operate in a convolutional manner, and show that the convolutional RBM (CRBM) are more efficientthan standard RBM.CRBM has been applied successfully to a wide range of visual and audio recognition tasks (Lee et al.,2009a; Lee et al., 2009b).
Though the success of CRBM in addressing two dimensional issues, thereis still no published research on the using of CRBM in textual information processing.
In this paper,we propose a novel semi-supervised learning algorithm called hybrid deep belief networks (HDBN), toaddress the semi-supervised sentiment classification problem with deep learning.
HDBN is a hybrid ofRBM and CRBM deep architecture, the bottom layers are constructed by RBM, and the upper layers areconstructed by CRBM, then the whole constructed deep architecture is fine tuned by a gradient-descentbased supervised learning based on an exponential loss function.The remainder of this paper is organized as follows.
In Section 2, we introduce our semi-supervisedlearning method HDBN in details.
Extensive empirical studies conducted on five real-world sentimentdatasets are presented in Section 3.
Section 4 concludes our paper.2 Hybrid deep belief networks2.1 Problem formulationThe sentiment classification dataset composed of many review documents, each review document com-posed of a bag of words.
To classify these review documents using corpus-based approaches, we need topreprocess them in advance.
The preprocess method for these reviews is similar with (Zhou et al., 2010).We tokenize and downcase each review and represent it as a vector of unigrams, using binary weightequal to 1 for terms present in a vector.
Moreover, the punctuations, numbers, and words of length oneare removed from the vector.
Finally, we combine all the words in the dataset, sort the vocabulary bydocument frequency and remove the top 1.5%, because many of these high document frequency wordsare stopwords or domain specific general-purpose words.After preprocess, each review can be represented as a vector of binary weight xi.
If the jthword ofthe vocabulary is in the ithreview, xij= 1; otherwise, xij= 0.
Then the dataset can be represented as amatrix:X =[x1,x2, .
.
.
,xR+T]=????
?x11, x21, .
.
.
, xR+T1x12, x22, .
.
.
, xR+T2... ,... , .
.
.
,...x1D, x2D, .
.
.
, xR+TD?????
(1)where R is the number of training reviews, T is the number of test reviews, D is the number of featurewords in the dataset.
Every column of X corresponds to a sample x, which is a representation of areview.
A sample that has all features is viewed as a vector in RD, where the ithcoordinate correspondsto the ithfeature.The L labeled reviews are chosen randomly from R training reviews, or chosen actively by activelearning, which can be seen as:XL= XR(S) , S = [s1, ..., sL], 1 ?
si?
R (2)where S is the index of selected training reviews to be labeled manually.1342x1 x2 xD?
?
?
?
??
?
?
?RBMh0h1w1?
?
?hM????
?wM+1hM+1CRBM?
?hN??
?f(hN(x), y)y1MinimizeLoss?
?labelsyCy2?
?Figure 1: Architecture of HDBN.The L labels correspond to L labeled training reviews is denoted as:YL=[y1,y2, .
.
.
,yL]=????
?y11, y21, .
.
.
, yL1y12, y22, .
.
.
, yL2... ,... , .
.
.
,...y1C, y2C, .
.
.
, yLC?????
(3)where C is the number of classes.
Every column of Y is a vector in RC, where the jthcoordinatecorresponds to the jthclass.yij={1 if xi?
jthclass?1 if xi/?
jthclass(4)For example, if a review xiis positive, yi= [1,?1]?
; otherwise, yi= [?1, 1]?.We intend to seek the mapping function X?
Y using the L labeled data and all unlabeled data.
Aftertraining, we can determine y using the mapping function when a new sample x comes.2.2 Architecture of HDBNIn this part, we propose a novel semi-supervised learning method HDBN to address the problem for-mulated in Section 2.1.
The sentiment datasets have high dimension (about 10,000), and computationcomplexity of convolutional calculation is relatively high, so we use RBM to reduce the dimension ofreview with normal calculation firstly.
Fig.
1 shows the deep architecture of HDBN, a fully intercon-nected directed belief nets with one input layer h0, N hidden layers h1,h2, ...,hN, and one label layer atthe top.
The input layer h0has D units, equal to the number of features of sample review x.
The hidden1343wkhk-1hk ?
?Group 11 0 ?Group Gk0 1?
?Group 10 1 ?Group Gk-11 0?
?1GkwkGk-11?Figure 2: Architecture of CRBM.layer hasM layers constructed by RBM andN?M layers constructed by CRBM.
The label layer has Cunits, equal to the number of classes of label vector y.
The numbers of hidden layers and the number ofunits for hidden layers, currently, are pre-defined according to the experience or intuition.
The seekingof the mapping function X ?
Y, here, is transformed to the problem of finding the parameter spaceW = {w1,w2, .
.
.
,wN} for the deep architecture.The training of the HDBN can be divided into two stages:1.
HDBN is constructed by greedy layer-wise unsupervised learning using RBMs and CRBMs asbuilding blocks.
L labeled data and all unlabeled data are utilized to find the parameter space Wwith N layers.2.
HDBN is trained according to the exponential loss function using gradient descent based supervisedlearning.
The parameter space W is refined using L labeled data.2.3 Unsupervised learningAs show in Fig.
1, we construct HDBN layer by layer using RBMs and CRBMs, the details of RBM canbe seen in (Hinton et al., 2006), and CRBM is introduced below.The architecture of CRBM can be seen in Fig.
2, which is similar to RBM, a two-layer recurrentneural network in which stochastic binary input groups are connected to stochastic binary output groupsusing symmetrically weighted connections.
The top layer represents a vector of stochastic binary hiddenfeature hkand the bottom layer represents a vector of binary visible data hk?1, k = M + 1, ..., N .
Thekthlayer consists of Gkgroups, where each group consists of Dkunits, resulting in Gk?
Dkhiddenunits.
The layer hMis consist of 1 group andDMunits.
wkis the symmetric interaction term connectingcorresponding groups between data hk?1and feature hk.
However, comparing with RBM, the weightsof CRBM between the hidden and visible groups are shared among all locations (Lee et al., 2009a), andthe calculation is operated in a convolutional manner (Desjardins and Bengio, 2008).We define the energy of the state (hk?1,hk) as:E(hk?1,hk; ?
)= ?Gk?1?s=1Gk?t=1(w?kst?
hk?1s) ?
hkt?Gk?1?s=1bk?1sDk?1?u=1hk?1s?Gk?t=1cktDk?v=1hkt(5)where ?
= (w,b, c) are the model parameters: wkstis a filter between unit s in the layer hk?1and unit tin the layer hk, k = M + 1, ..., N .
The dimension of the filter wkstis equal to Dk?1?Dk+ 1. bk?1sisthe sthbias of layer hk?1and cktis the tthbias of layer hk.
A tilde above an array (w?)
denote flippingthe array, ?
denote valid convolution, and ?
denote element-wise product followed by summation, i.e.,A ?B = trATB (Lee et al., 2009a).Similar to RBM, Gibbs sampler can be performed based on the following conditional distribution.1344The probability of turning on unit v in group t is a logistic function of the states of hk?1and wkst:p(hkt,v= 1|hk?1)= sigm(ckt+ (?sw?kst?
hk?1s)v)(6)The probability of turning on unit u in group s is a logistic function of the states of hkand wkst:p(hk?1s,u= 1|hk)= sigm(bk?1s+ (?twkst?
hkt)u)(7)A star ?
denote full convolution.2.4 Supervised learningIn HDBN, we construct the deep architecture using all labeled reviews with unlabeled reviews by in-putting them one by one from layer h0.
The deep architecture is constructed layer by layer from bottomto top, and each time, the parameter space wkis trained by the calculated data in the k ?
1thlayer.Algorithm 1: Algorithm of HDBNInput: data X, YLnumber of training data R; number of test data T ;number of layers N ; number of epochs Q;number of units in every hidden layer D1...DN;number of groups in every convolutional hidden layer GM...GN;hidden layer h1, .
.
.
,hM;convolutional hidden layer hM+1, .
.
.
,hN?1;parameter space W = {w1, .
.
.
,wN};biases b, c; momentum ?
and learning rate ?
;Output: deep architecture with parameter space W1.
Greedy layer-wise unsupervised learningfor k = 1; k ?
N ?
1 dofor q = 1; q ?
Q dofor r = 1; r ?
R+ T doCalculate the non-linear positive and negative phase:if k ?M thenNormal calculation.elseConvolutional calculation according to Eq.
6 and Eq.
7.endUpdate the weights and biases:wkst= ?wkst+ ?(?hk?1s,rhkt,r?P0??hk?1s,rhkt,r?P1)endendend2.
Supervised learning based on gradient descentarg minWL?i=1C?j=1exp(?hN(xij)yij)According to the wkcalculated by RBM and CRBM, the layer hk, k = 1, .
.
.
,M can be computed asfollowing when a sample x inputs from layer h0:hkt(x) = sigm(ckt+Dk?1?s=1wksthk?1s(x)), t = 1, .
.
.
, Dk(8)1345When k = M + 1, .
.
.
, N ?
1, the layer hkcan be represented as:hkt(x) = sigm??ckt+Gk?1?s=1w?kst?
hk?1s(x)?
?, t = 1, .
.
.
, Gk(9)The parameter space wNis initialized randomly, just as backpropagation algorithm.hNt(x) = cNt+GN?1?DN?1?s=1wNsthN?1s(x), t = 1, .
.
.
, DN(10)After greedy layer-wise unsupervised learning, hN(x) is the representation of x.
Then we useL labeledreviews to refine the parameter space W for better discriminative ability.
This task can be formulated asan optimization problem:arg minWf(hN(XL),YL)(11)wheref(hN(XL),YL)=L?i=1C?j=1T(hNj(xi)yij)(12)and the loss function is defined asT (r) = exp(?r) (13)We use gradient-descent through the whole HDBN to refine the weight space.
In the supervisedlearning stage, the stochastic activities are replaced by deterministic, real valued probabilities.2.5 Classification using HDBNThe training procedure of HDBN is given in Algorithm 1.
For the training of HDBN architecture, theparameters are random initialized with normal distribution.
All the reviews in the dataset are used totrain the HDBN with unsupervised learning.
After training, we can determine the label of the new datathrough:argjmaxhN(x) (14)3 Experiments3.1 Experimental setupWe evaluate the performance of the proposed HDBN method using five sentiment classification datasets.The first dataset is MOV (Pang et al., 2002), which is a classical movie review dataset.
The other fourdatasets contain products reviews come from the multi-domain sentiment classification corpus, includingbooks (BOO), DVDs (DVD), electronics (ELE), and kitchen appliances (KIT) (Blitzer et al., 2007).
Eachdataset contains 1,000 positive and 1,000 negative reviews.The experimental setup is same as (Zhou et al., 2010).
We divide the 2,000 reviews into ten equal-sized folds randomly, maintaining balanced class distributions in each fold.
Half of the reviews in eachfold are random selected as training data and the remaining reviews are used for test.
Only the reviewsin the training data set are used for the selection of labeled reviews by active learning.
All the algorithmsare tested with cross-validation.We compare the classification performance of HDBN with four representative semi-supervised learn-ing methods, i.e., semi-supervised spectral learning (Spectral) (Kamvar et al., 2003), transductive SVM(TSVM) (Collobert et al., 2006), deep belief networks (DBN) (Hinton et al., 2006), and person-al/impersonal views (PIV) (Li et al., 2010).
Spectral learning, TSVM methods are two baseline methodsfor sentiment classification.
DBN (Hinton et al., 2006) is the classical deep learning method proposedrecently.
PIV (Li et al., 2010) is a new sentiment classification method proposed recently.1346Table 1: HDBN structure used in experiment.Dataset StructureMOV 100-100-4-2KIT 50-50-3-2ELE 50-50-3-2BOO 50-50-5-2DVD 50-50-5-2Table 2: Test accuracy with 100 labeled reviews for semi-supervised learning.Type MOV KIT ELE BOO DVDSpectral 67.3 63.7 57.7 55.8 56.2TSVM 68.7 65.5 62.9 58.7 57.3DBN 71.3 72.6 73.6 64.3 66.7PIV - 78.6 70.0 60.1 49.5HDBN 72.2 74.8 73.8 66.0 70.33.2 Performance of HDBNThe HDBN architecture used in all our experiments have 2 normal hidden layer and 1 convolutionalhidden layer, every hidden layer has different number of units for different sentiment datasets.
The deepstructure used in our experiments for different datasets can be seen in Table 1.
For example, the HDBNstructure used in MOV dataset experiment is 100-100-4-2, which represents the number of units in 2normal hidden layers are 100, 100 respectively, and in output layer is 2, the number of groups in 1convolutional hidden layer is 4.
The number of unit in input layer is the same as the dimensions of eachdatasets.
For greedy layer-wise unsupervised learning, we train the weights of each layer independentlywith the fixed number of epochs equal to 30 and the learning rate is set to 0.1.
The initial momentumis 0.5 and after 5 epochs, the momentum is set to 0.9.
For supervised learning, we run 30 epochs, threetimes of linear searches are performed in each epoch.The test accuracies in cross validation for five datasets and five methods with semi-supervised learningare shown in Table 2.
The results of previous two methods are reported by (Dasgupta and Ng, 2009).The results of DBN method are reported by (Zhou et al., 2010).
Li et al.
(Li et al., 2010) reported theresults of PIV method.
The result of PIV on MOV dataset is empty, because (Li et al., 2010) did notreport it.
HDBN is the proposed method.Through Table 2, we can see that HDBN gets most of the best results except on KIT dataset, which isjust slight worse than PIV method.
However, the preprocess of PIV method is much more complicatedthan HDBN, and the PIV results on other datasets are much worse than HDBN method.
HDBN methodis adjusted by DBN, all the experiment results on five datasets for HDBN are better than DBN.
Thiscould be contributed by the convolutional computation in HDBN structure, and proves the effectivenessof our proposed method.3.3 Performance with variance of unlabeled dataTo verify the contribution of unlabeled reviews for our proposed method, we did several experimentswith fewer unlabeled reviews and 100 labeled reviews.The test accuracies of HDBN with different number of unlabeled reviews and 100 labeled reviewson five datasets are shown in Fig.
3.
The architectures for HDBN used in this experiment are sameas Section 3.2 too, which can be seen in Table 1.
We can see that the performance of HDBN is muchworse when just using 400 unlabeled reviews.
However, when using more than 1200 unlabeled reviews,the performance of HDBN is improved obviously.
For most of review datasets, the accuracy of HDBNwith 1200 unlabeled reviews is close to the accuracy with 1600 and 2000 unlabeled reviews.
This provesthat HDBN can get competitive performance with just few labeled reviews and appropriate number of1347400 600 800 1000 1200 1400 1600 1800 20006062646668707274767880Number of unlabeled reviewTest accuracy (%)MOVKITELEBOODVDFigure 3: Test accuracy of HDBN with different number of unlabeled reviews on five datasets.unlabeled reviews.
Considering the much time needed for training with more unlabeled reviews and lessaccuracy improved for HDBN method, we suggest using appropriate number of unlabeled reviews in realapplication.4 ConclusionsIn this paper, we propose a novel semi-supervised learning method, HDBN, to address the sentiment clas-sification problem with a small number of labeled reviews.
HDBN seamlessly incorporate convolutionalcomputation into the DBN architecture, and use CRBM to abstract the review information effectively.To the best of our knowledge, HDBN is the first work that uses convolutional neural network to improvesentiment classification performance.
One promising property of HDBN is that it can effectively use thedistribution of large amount of unlabeled data, together with few label information in a unified frame-work.
In particular, HDBN can greatly reduce the dimension of reviews through RBM and abstract theinformation of reviews through the cooperate of RBM and CRBM.
Experiments conducted on five senti-ment datasets demonstrate that HDBN outperforms state-of-the-art semi-supervised learning algorithms,such as SVM and DBN based methods, using just few labeled reviews, which demonstrate the effectiveof deep architecture for sentiment classification.AcknowledgementsThis work is supported in part by National Natural Science Foundation of China (No.
61300155, No.61100115 and No.
61173075), Natural Science Foundation of Shandong Province (No.
ZR2012FM008),Science and Technology Development Plan of Shandong Province (No.
2013GNC11012), Science andTechnology Research and Development Funds of Shenzhen City (No.
JC201005260118A and No.JC201005260175A), and Scientific Research Fund of Ludong University (LY2013004).ReferencesJohn Blitzer, Mark Dredze, and Fernando Pereira.
2007.
Biographies, bollywood, boom-boxes and blenders:Domain adaptation for sentiment classification.
In Annual Meeting of the Association of Computational Lin-guistics, pages 440?447, Prague, Czech Republic.
Association for Computational Linguistics.Olivier Chapelle, Bernhard Scholkopf, and Alexander Zien.
2006.
Semi-supervised learning.
MIT Press, USA.Ronan Collobert, Fabian Sinz, Jason Weston, and Leon Bottou.
2006.
Large scale transductive svms.
Journal ofMachine Learning Research, 7:1687?1712.Sajib Dasgupta and Vincent Ng.
2009.
Mine the easy, classify the hard: A semi-supervised approach to automaticsentiment classfication.
In Joint Conference of the 47th Annual Meeting of the Association for Computational1348Linguistics and 4th International Joint Conference on Natural Language Processing of the Asian Federation ofNatural Language Processing, pages 701?709, Stroudsburg, PA, USA.
Association for Computational Linguis-tics.Guillaume Desjardins and Yoshua Bengio.
2008.
Empirical evaluation of convolutional rbms for vision.
Technicalreport.Geoffrey E. Hinton, Simon Osindero, and Yee-Whye Teh.
2006.
A fast learning algorithm for deep belief nets.Neural Computation, 18:1527?1554.Sepandar Kamvar, Dan Klein, and Christopher Manning.
2003.
Spectral learning.
In International Joint Confer-ences on Artificial Intelligence, pages 561?566, Catalonia, Spain.
AAAI Press.Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner.
1998.
Gradient-based learning applied to docu-ment recognition.
Proceedings of the IEEE, 86(11):2278?2324.Honglak Lee, Roger Grosse, Rajesh Ranganath, and Andrew Y. Ng.
2009a.
Convolutional deep belief networksfor scalable unsupervised learning of hierarchical representations.
In International Conference on MachineLearning, pages 609?616, Montreal, Canada.
ACM.Honglak Lee, Yan Largman, Peter Pham, and Andrew Y. Ng.
2009b.
Unsupervised feature learning for audioclassification using convolutional deep belief networks.
In Advances in Neural Information Processing Systems,pages 1096?1103, Vancouver, B.C., Canada.
NIPS Foundation.Shoushan Li, Chu-Ren Huang, Guodong Zhou, and Sophia Yat Mei Lee.
2010.
Employing personal/impersonalviews in supervised and semi-supervised sentiment classification.
In Annual Meeting of the Association forComputational Linguistics, pages 414?423, Uppsala, Sweden.
Association for Computational Linguistics.Yang Liu, Xiaohui Yu, Xiangji Huang, and Aijun An.
2010.
S-plasa+: Adaptive sentiment analysis with applica-tion to sales performance prediction.
In International ACM SIGIR Conference on Research and Developmentin Information Retrieval, pages 873?874, New York, NY, USA.
ACM.Bo Pang and Lillian Lee.
2008.
Opinion Mining and Sentiment Analysis, volume 2 of Foundations and Trends inInformation Retrieval.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002.
Thumbs up?
sentiment classification using machinelearning techniques.
In Conference on Empirical Methods in Natural Language Processing, pages 79?86,Stroudsburg, PA, USA.
Association for Computational Linguistics.Marc?Aurelio Ranzato and Martin Szummer.
2008.
Semi-supervised learning of compact document representa-tions with deep networks.
In International Conference on Machine Learning, pages 792?799, Helsinki, Finland.ACM.Ruslan Salakhutdinov and Geoffrey E. Hinton.
2007.
Learning a nonlinear embedding by preserving class neigh-bourhood structure.
Journal of Machine Learning Research, 2:412?419.Wei Wei and Jon Atle Gulla.
2010.
Sentiment learning on product reviews via sentiment ontology tree.
In AnnualMeeting of the Association for Computational Linguistics, pages 404?413, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Yi Zhen and Dit-Yan Yeung.
2010.
Sed: Supervised experimental design and its application to text classification.In International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 299?306, Geneva, Switzerland.
ACM.Shusen Zhou, Qingcai Chen, and Xiaolong Wang.
2010.
Active deep networks for semi-supervised sentimentclassification.
In International Conference on Computational Linguistics, pages 1515?1523, Beijing, China.Coling 2010 Organizing Committee.Xiaojin Zhu.
2007.
Semi-supervised learning literature survey.
Ph.D. thesis.1349
