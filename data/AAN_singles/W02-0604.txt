Unsupervised Learning of Morphology Without MorphemesSylvain NeuvelDept.
of Linguisticssneuvel@uchicago.eduSean A. FulopDepts.
of Linguisticsand Computer Sciencesfulop@uchicago.eduThe University of ChicagoAbstractThe first morphological learner basedupon the theory of Whole Word Mor-phology (Ford et al, 1997) is outlined,and preliminary evaluation results are pre-sented.
The program, Whole Word Mor-phologizer, takes a POS-tagged lexiconas input, induces morphological relation-ships without attempting to discover oridentify morphemes, and is then able togenerate new words beyond the learningsample.
The accuracy (precision) of thegenerated new words is as high as 80% us-ing the pure Whole Word theory, and 92%after a post-hoc adjustment is added to theroutine.The aim of this project is to develop a computa-tional model employing the theory of whole wordmorphology (Ford et al, 1997) capable on the onehand of identifying morphological relations within alist of words from any one of a wide variety of lan-guages and, on the other, of putting that knowledgeto use in creating previously unseen word forms.A small application called Whole Word Morpholo-gizer which does just this is outlined and discussed.In particular, this approach is set against the liter-ature on computational morphology as an entirelydifferent way of doing things which has the potentialto be generalized to all known varieties of morphol-ogy in the world?s languages, a feature not shared byprevious methods.
As it is based on a model of themental lexicon in which all entries are entire, fullyfledged words, this project also serves as an empiri-cal demonstration that a word-based morphologicaltheory that rejects the notion of morpheme as mini-mal unit of form and meaning (and/or grammaticalproperties) is viable from the point of view of acqui-sition as well as generation.1 Morphological learningSince its inception in the mid 1950s, the field ofcomputational morphology has been characterizedby a paucity of procedures for generation.
Notwith-standing the impressive body of literature on theshortcomings of traditional Paninian morphology,most computational research projects also rely on atraditional notion of the morpheme and ignore allnon-compositional aspects of morphology.
Theseobservations are obviously not unrelated and are inpart inherited from the field of computational syntaxwhere applications traditionally were designed to as-sign a syntactic structure to a given string of words,though this is less true today.1.1 Segmentation and morpheme identificationWord formation and the population of the lexicon,while central to morphological theory, are notice-ably absent from the field of computational mor-phology.
Most computational work in the fieldof morphology has focused on the identification ofmorphemes or morphological parsing while payinglittle or no attention to generation.
While these ap-plications find a common goal in the automatic ac-quisition of morphology, it is helpful to distinguishbetween two types of analysis in light of the oftenvery different results sought by various morphologi-cal learners.On the one hand, some applications focus ex-clusively on the segmentation of words or longerstrings into smaller units.
In other words, theirJuly 2002, pp.
31-40.
Association for Computational Linguistics.ACL Special Interest Group in Computational Phonology (SIGPHON), Philadelphia,Morphological and Phonological Learning: Proceedings of the 6th Workshop of thefunction is to identify morpheme boundaries withinwords and, as such, they only indirectly identifymorphemes as linguistic units.
Zellig Harris?s (Har-ris, 1955; Harris, 1967) pioneering work suggeststhat morpheme boundaries can be determined bycounting the number of letters that follow a givensubstring within a corpus (v. (Hafer and Weiss,1974) for a further development of Harris?s ideas).Janssen (1992) and Flenner (1994; 1995) also worktowards segmenting words but use training corporain which morpheme boundaries have been manuallyinserted.
Recent work by Kazakov and Manand-har (1998) combines unsupervised and supervisedlearning techniques to generate a set of segmenta-tion rules that can further be applied to previouslyunseen words.On the other hand, some computational morpho-logical applications are designed solely to identifymorphemes based on a training corpus and not toprovide a morphological analysis for each word ofthat corpus.
Brent (1993), for example, aims at find-ing the right set of suffixes from a corpus, but thealgorithm cannot double as a morphological parser.More recently, efforts have been developingwhich identify morphemes and perform some sort ofanalysis.
Schone and Jurafsky (2001) employ a greatmany sophisticated post-hoc adjustments to obtainthe right conflation sets for words by pure corpusanalysis without annotations.
Their procedure usesa morpheme-based model, provides an analysis ofthe words, and does in a sense discover morphologi-cal relations.
Goldsmith (2001b; 2001a), inspired byde Marcken?s (1995) thesis on minimum descriptionlength, attempts to provide both a list of morphemesand an analysis of each word in a corpus.
Also, Ba-roni (2000) aims at finding a set of prefixes from acorpus, together with an affix-stem parse of each ofthe words.While they might differ in their methods or ob-jectives, all of the above morphological applicationsshare a common characteristic in that they are learn-ers designed exclusively for the acquisition of mor-phological facts from corpora and do not generatenew words based on the information they acquire.1.2 Parsing and generationOnly a handful of programs can both parse and gen-erate words.
Once again, these programs fall intotwo very distinct categories.
In view of the dispar-ity between these programs, it is useful to distin-guish between genuine morphological learners ableto generate from acquired knowledge and genera-tors/parsers that implement a man-made analysis.The latter group is perhaps the most well known, solet us begin with them.Kimmo-type applications of two-level morphol-ogy (Koskenniemi, 1983; Antworth, 1990; Kart-tunen et al, 1992; Karttunen, 1993; Karttunen,1994) can provide a morphological analysis of thewords in a corpus and generate new words based ona set of rules; but these programs must first be pro-vided with that set of rules and a lexicon contain-ing morphemes by the user.
Similar work in one-and two-level morphology has been done using theAttribute-Logic Engine (Carpenter, 1992).
Some ofthese systems (e.g.
(Karttunen et al, 1987)) havea front-end that compiles more traditional linearlyordered morphological rules into the finite-state au-tomata of two-level morphology.
Once again, theseapplications require a set of man-made lexical rulesto function.
While the practical uses of such applica-tions as PC-Kimmo are incontestable, it is clear thatthey are part of a different endeavour, and should notbe confused with genuine morphological learners.The other relevant group of computational appli-cations can, as mentioned, both acquire morpho-logical knowledge from corpora and generate newwords based on that knowledge.
Albright and Hayes(2001a; 2001b) tackle the wider task of acquir-ing morphology and (morpho)phonology based ona small paradigm list and their learner is able to gen-erate particular inflected forms given a related word.Dz?eroski and Erjavec (1997) work towards learningmorphological rules for forming particular inflec-tional forms given a lemma (a set of related words).Their learner produces a set of rules relating all themembers of a paradigm to a base form.
The programcan then produce a member of that paradigm oncommand given the base form.
While the methodsused by Albright and Hayes and Dz?eroski and Er-javec radically differ, both use a form of supervisedlearning which significantly reduces the amount ofinformation their learner has to acquire.
Albrightand Hayes train their program using a paradigm listin which each entry contains, for example, both thepresent and past tense forms of an English verb.Similarly, the training data used by Dz?eroski and Er-javec similarly has a base form, or lexeme, associ-ated to each and every word so that all the wordsof a given paradigm share a common label.
Thedistinctions between the two methods are immate-rial, what matters is that both learners are being toldwhich words are related to which and are left withthe task of describing that relation in the form a rule.In other words, the algorithms they use cannot dis-cover that words are morphologically related.1.3 What?s morphology?In the above algorithms, the task of determiningwhether one word is related to another in a morpho-logical sense is most frequently left to the linguist,as this information has to be encoded in the train-ing data for these algorithms.
(Some of the mostrecent work such as (Schone and Jurafsky, 2001)and (Goldsmith, 2001b) are notable exceptions tothis paradigm.)
This is perhaps not surprising, sinceno serious attempt at defining a morphological rela-tion has been made in the last few decades.
Amer-ican structuralists of the forties and fifties proposedwhat have been referred to as discovery procedures(v. (Nida, 1949), for example) for the identificationof morphemes but since the mid fifties (Chomsky,1955), it has been customary for morphological the-ory to ignore this aspect of morphology and relegateit to studies on language acquisition.
But, since amorphological learner like that presented here is de-signed to model the acquisition of morphology, itseems that it should above all be able to determinefor itself whether two words are morphologically re-lated or not, whether there is anything morphologi-cal to acquire at all.Another important thing to note about the vastmajority of computational morphology learners istheir reliance on a traditional notion of the mor-pheme as a lexical unit and their exclusive fo-cus on concatenative morphology.
There is apanoply of recent publications devoted to the em-pirical shortcomings of traditional so-called ?Item-and-Arrangement?
morphology (Hockett, 1954;Bochner, 1993; Ford and Singh, 1991; Anderson,1992; Ford et al, 1997), and the list of phenomenathat fall out of reach of a compositional approachis rather impressive: zero-morphs, ablaut-like pro-cesses, templatic morphology, class markers, partialsuppletion, etc.
Still, seemingly every documentedmorphological learner relies on a Bloomfieldian no-tion of the morpheme and produces an Item-and-Arrangement analysis; this description applies to allof the computational papers cited above.2 An alternative theoryWhole Word Morphologizer (henceforth WWM) isthe first implementation of the theory of WholeWord Morphology.
The theory, developed by AlanFord and Rajendra Singh at Universite?
de Montre?al,seeks to account for morphological relations in aminimalist fashion.
Ford and Singh published a se-ries of papers dealing with various aspects of the the-ory between 1983 and 1990.
Drawing on these pa-pers, they published a full outline of it in 1991 (Fordand Singh, 1991) and an even fuller defense of itin 1997 (Ford et al, 1997).
Since then, aspects of ithave been taken up in a series of publications by Ag-nihotri, Dasgupta, Ford, Neuvel, Singh, and variouscombinations of these authors.
The central mech-anism of the theory, the Word Formation Strategy(WFS), is a sort of non-decomposable morpholog-ical transformation that relates full words with fullwords (or helps one fashion a full word from an-other full word) and parses any complex word intoa variable and a non-variable component.
Neuveland Singh (In press) offer a strict definition of mor-phological relatedness and, based on this definition,suggest guidelines for the acquisition of Word For-mation Strategies.In Whole-Word Morphology, any morphologicalrelation can be represented by a rule of the followingform:(1) |X |??
|X ?|?in which the following conditions and notations areemployed:1.
|X |?
and |X ?|?
are statements that words of theform X and X ?
are possible in the language,and X and X ?
are abbreviations of the forms ofclasses of words belonging to categories ?
and?
(with which specific words belonging to theright category can be unified in form);2. ?
represents all the form-related differences be-tween X and X ?;3.
?
and ?
are categories that may be representedas feature-bundles;4. ?
represents a bi-directional implication;5.
X ?
and X are semantically related.There are several ramifications of (1).
First, thereis only one morphology; no distinction, other thana functional one, is made between inflection andderivation.
Second, morphology is relational and notcompositional.
The program thus makes no refer-ence to theoretical constructs such as ?root?, ?stem?,and ?morpheme?, or devices such as ?levels?
and?strata?
and relies exclusively on the notion of mor-phological relatedness.
And since its objective isnot to assign a probability to a given word or string,it must rely on a strict formal definition of a mor-phological relation.
Ultimately, the theory takes theSaussurean view that words are defined by the differ-ences amongst them and argues that some of thesedifferences, namely those that are found betweentwo or more pairs of words, constitute the domainof morphology.
In other words, two words of a lexi-con are morphologically related if and only if all thedifferences between them are found in at least oneother pair of words of the same lexicon.3 Overview of the methodUnder the assumption that the morphology of a lan-guage resides exclusively in differences that are ex-ploited in more than one pair of words within its lex-icon, WWM (Algorithm 1 in the next section) com-pares every word of a small lexicon and determinesthe segmental differences found between them.
Theinput to the current version of the program is a smalltext file that contains anywhere from 1000 to 5000words.
Each word appears in orthographic form andis followed by its syntactic and morphological cate-gories, as in the example below:(2) cat, Ns (Noun, singular)catch, Vcatches, V3s (Verb, (pres.)
3rd pers.sing.
)decided, Vp (Verb, past)The algorithm simply compares each letter fromword A to the corresponding one from word B toproduce a comparison record, which can be viewedas a data structure.
Currently, it works on ortho-graphic representations.
This means it would as eas-ily work on phonemic transcriptions, but it will re-quire empirical evaluation to see whether the resultsfrom these can improve upon those obtained usingspellings, and we have not yet gone through such anexercise.
It starts on either the left or right edge ofthe words if the two words share their first (few) seg-ments or their last (few) segments, respectively (theforward version is presented in Algorithm 2 in thenext section).
This is just a simple-minded way ofaligning the similar parts of the words for the com-parison; a more sophisticated implementation in thefuture could use a more general sequence alignmentprocedure.
The segments are placed in one of twolists in the comparison structure (differences or sim-ilarities) based on whether or not they are identical.Each comparison structure also contains the cate-gories of both words, and is kept in a large list of allcomparison structures found from analyzing the en-tire corpus.
The example below shows the informa-tion in the comparison structure produced from theEnglish words receive and reception.
It includes thedifferences and similarities between the two words,from the perspective of each word in turn, as well asthe lexical categories of the words.
(3) DifferencesFirst word Second word####iveV ####ptionNsSimilaritiesFirst Secondrece### rece#####Matching character sequences in the differencesection are replaced with a variable.
The re-sult is then set against comparisons generated byother pairs of words and duplicate differences arerecognized.
In the example below, the compar-isons produced by the pairs receive/reception, con-ceive/conception and deceive/deception are shown.
(4) DifferencesFirst word Second wordX iveV X ptionNsX iveV X ptionNsX iveV X ptionNsSimilaritiesFirst Secondrece### rece#####conce### conce#####dece### dece#####The three comparisons in (4) share the same for-mal and grammatical differences, and so the theoryindicates they should be merged into one morpho-logical strategy.
Since the differences are the same,it is only the similarities that are actually merged.Each new morphological strategy is also restrictedto apply in as narrow an environment as possible.Neuvel and Singh (Neuvel and Singh, In press) sug-gest that any morphological strategy must be maxi-mally restricted at all times; this is accomplished byspecifying as constant all the similarities found, notbetween words, but between the similarities foundbetween words.
In (4), all three sets of similaritiesend with the sequence of letters ?ce.?
These similar-ities between similarities are specified as constant ineach strategy and the length of each word is also fac-tored in.
The merge routine called in Algorithm 2carries out this procedure; we don?t show it becauseit is tedious but not especially interesting.
The re-stricted morphological strategy relating the words in(4) is as follows:(5) DifferencesFirst word Second wordX iveV X ptionNsSimilaritiesFirst Second?##ce### ?##ce#####For the sake of clarity, we can represent the infor-mation contained in (5) in a more familiar fashionusing the formalism described in (1).
The verticalbrackets ?|?|?
are used for orthographic forms so asnot to confuse them with phonemic representations.
(6) |?##ceive|V?
|?##ception|NsThe ?#?
signs in the above representations standfor letters that must be instantiated but are not spec-ified; the ???
symbol stands for a letter that is notspecified and that may or may not be instantiated.Strategy (6) can therefore be interpreted as follows:(6?)
If there is a verb that ends with the sequence?ceive?
preceded by no less than two andno more than three characters, there shouldalso be a singular noun that ends with the se-quence ?ception?
preceded by the same twoor three characters.After performing the comparisons and merging,WWM extracts a list of morphological strategies,which are those comparison structures whose countis more than some fixed threshold.
Table 1 con-tains a few strategies found from the first fewchapters of Moby Dick.
These strategies resultfrom merging comparison structures which have thesame differences?merging the similarities of sev-eral unifiable word pairs, and so many have no spec-ified letters at all.WWM then goes through the lexicon word byword and attempts to unify each word in form andcategory with the left or right side of this strategy.If it succeeds, WWM replaces all the segments fullyspecified on the side of the strategy the word is uni-fied with, with the segments fully specified on theother side.
For example, given the noun perceptionin the corpus and strategy (6), WWM will map theword onto the right hand side of (6), take out the se-quence ?ception?
from the end and replace it withthe sequence ?ceive?
to produce the new word per-ceive.
The category of the word will also be changedfrom singular noun to verb.
New words can thus begenerated in a rather obvious fashion by taking eachword in the original lexicon and applying any strate-gies that can be applied, i.e.
whose orthographicform and part of speech can be unified with the wordat hand.
Algorithm 3 shows the basic generationprocedure; once again the routines called unifyand create which implement the nitty-gritty de-tails of the above description are not given becausethey are more tedious than interesting, and will cer-tainly need to be changed in more general futureversions of WWM.
Table 2 gives some of the newwords WWM creates using text from Le petit princeas its base lexicon.Table 1: Word-formation strategies discovered from Moby DickDifferences Similarities1st word 2nd word 1st word 2nd word ExamplesXdPP XV ???
?####e# ???
?####e baked/bake, charged/chargeXedPP XV ?######## ?###### directed/directXsNp XNs ?????
?##### ?????
?#### helmets/helmet, rabbits/rabbitXingGER XedPP ?????
?####### ?????
?###### walking/walked, talking/talkedXingGER XsV3s ????
?####### ????
?##### walking/walks, talking/talksXnessNs XADJ ???
?######### ???????
?##### short/shortnessXlyADV XADJ ?????
?###### ?????
?#### easy/easily, quick/quicklyXestADJ XADJ ?####### ?#### hardest/hard, shortest/shortXsV3s XV ??
?##### ??
?#### jumps/jump, plays/playXerADJ XADJ ?###### ?#### harder/hard, louder/loudXlessADJ XNs ?######## ?#### painless/pain, childless/childXingGER XyADJ ?####### ?##### raining/rainy, running/runnyXedPP XsV3s ?
?###### ?
?##### played/playsXingsNp XV ??
?######### ??
?##### paintings/paintTable 2: Words generated from Le petit princedrames Np droitement ADVdresse?e PF dro?les AIPdresser INF dro?lement ADVdressa Vp3 dunes Npdressais Vi2 durerait Vc3dresse V3 de?cide?e PFdressent V6 de?cider INFdressez V5 de?cida Vp3dressait Vi3 de?cide V3droits AMP de?coiffe?
AMdroites AFP de?concentre?s AMPThe output from the algorithm is a list of words,1much as in Table 2, which are generated from the in-put corpus using the morphological relations (strate-gies) discovered.
The method described above willclearly force WWM to create words that were al-ready part of its original lexicon; in fact, each andevery word involved in licensing the discovery ofa morphological strategy will be duplicated by theprogram.
Generated words that were not part ofWWM?s original lexicon are then added to a sepa-1By word we mean an orthographic form together with thepart of speech.
Further work in this vein would add meaningsas well.rate word list containing only new words.
If desired,this new word list can be merged with the originallexicon for another round of discovery to formu-late new strategies based on a larger dataset.
Ad-ditionally, each of the new words can simply be putthrough another cycle of word creation by applyingthe same strategies as before a second time.4 ImplementationThis section contains some pseudocode showingseveral basic components of the Whole Word Mor-phologizer.
Algorithm 1 shows the main procedure,which takes a POS-tagged lexicon as input and out-puts a list of all words that are possible given themorphological relations present in the lexicon.The two procedures compforward and comp-backward are symmetrical, so Algorithm 2 showsjust the first of these.
This algorithm provides thedata structure which includes the differences andsimilarities between each pair of words in the lexi-con, in similar fashion to the examples in the preced-ing section.
In practice, only those pairs of wordswhich are by some heuristic sufficiently similar inthe first place are compared.
Additionally, the twosimilarities sequences for each word pair are actu-ally represented as one sequence which encodes theinformation found in the two sequences of the exam-ples in the preceding; this is just for convenience ofAlgorithm 1 WWM(lexicon)Require: lexicon to be a list of POS-taggedwords.Ensure: a list newwords is generatedfor all tagged words wi dofor all tagged words w j doif wi and w j share a beginning sequencethencompforward(wi,w j)else if wi and w j share an ending sequencethencompbackward(wi,w j)end ifend forend forfor all comparison structures in the list doif count(comparison) > Threshold thenappend comparison to the list strate-giesgenerate(lexicon, strategies)end ifend forstorage and computation.Algorithm 3 shows the outline of the final stage,which generates an output list of words from the in-put lexicon and the morphological strategies.
Thestrategy list is simply a list of all comparison struc-tures that occurred more frequently than some arbi-trary threshold number.5 Accomplishments and prospects5.1 Initial resultsWhole Word Morphologizer has been tested on alimited basis using English and French lexicons ofapproximately 3000 entries, garnered from the POS-tagged versions of Le petit prince and Moby Dick.The program initially, without any post-hoc correc-tions, achieved between 70% and 82% accuracy ingeneration; these figures measure the percentage ofthe new words beyond the original lexicon that arepossible words of the language.
The figures thusmeasure a kind of precision value, in terms of theprecision/recall tradeoff, and are fair values in thatthey do not include the generated words that are al-ready in the lexicon.Algorithm 2 compforward(w1,w2)Require: w1 and w2 to be (word, category) pairs.Ensure: a data structure comparison document-ing the different and similar letters between w1and w2 is merged into the global list of com-parisons.
comparison is a structure of 5 listsw1dif, w1cat, w2dif, w2cat, sim.for x = 1 to length(w2) doif characters w1(x) = w2(x) thenappend w1(x) to list simif list w1dif does not end with ?X?
thenappend ?X?
to both lists w1dif and w2difelseappend w1(x) to w1dif,append w2(x) to w2dif, append ?#?
to simend ifend ifend forfor x = length(w2)+1 to length(w1) doappend w1(x) to w1difend forif dif lists and categories match a comparison al-ready in the list comps thenmerge comparisons and incrementcount(comparison)elseappend comparison to compscount(comparison)?
1end ifA satisfactory recall metric seems impossible tothink of in its usual sense here.
First of all, there aregenerally an indefinite number of possible words in alanguage.
One therefore cannot give a precise set ofwords that we wish the system could generate froma specific lexicon, so there seems to be no way tomeasure the percentage of ?desired words?
that arein fact generated.
Even if we were to make such alist by hand from the current small corpora to use asa gold standard (which has been suggested by a ref-eree), it must also be remembered that WWM dis-covers strategies (morphological relations) for cre-ating new words from given ones.
It cannot be ex-pected to discover strategies that are not evident in acorpus.
Indeed, WWM will never discover that, forexample, ?am?
and ?be?
are related, because accord-ing to the theory of morphology being applied theseAlgorithm 3 generate(lexicon, strategies)Ensure: a list newwords is generated using lex-icon and strategiesfor all words in lexicon dofor all strategies doif unify(lexicon[x], strategies[x])says the word and strategy match with eitherleft or right alignment thennewword ?
create(lexicon[x],strategies[x])if newword is not in the lexicon or the listnewwords thenappend newword to newwords listend ifend ifend forend forwords are only related by convention, not by mor-phology.
?Nonproductive morphology?
is not reallymorphology.The real point is that we do not want to holdWWM?s performance up against our own ideasabout morphological relations among words, sinceit would be practically impossible to determine notmerely a large set of possible words that linguiststhink are related to those in the corpus, but rather aset of possible words that WWM ought to generateaccording to its theory.
This would amount to try-ing to beat WWM at its own game in pursuit of agold standard, which could only be obtained using abetter implementation of WWM?s theory.
A perfectimplementation of Whole Word Morphology wouldhave perfect recall, in view of our eventual goal ofusing this theory to inform us about the morphologyof a language?about what ought to be recalled.
Weare not trying to learn something that we feel is al-ready known.5.2 What?s learning?It is worth considering the endeavor of learning mor-phology in terms of formal learning theory, as pre-sented in Osherson et al (1986) or Kanazawa (1998)for example.
In the classical framework, the prob-lem of learning a language from positive exam-ple data is approached by considering the succes-sive guesses at the target language that a purportedlearner makes when presented with some sequen-tially increasing learning sample drawn from thatlanguage.
Considering just morphology, it seemsthat the target language is the set of all possiblewords of the natural language at hand, a possiblyinfinite (or at least indefinite) set.
WWM?s outputis a list of generated words subsuming the corpus,which are supposed to be all the words creatable byapplying its idea of morphology to that corpus.
Itcan thus be viewed as making a guess about the tar-get language, given a certain learning sample.
If thelearning sample is increased, its guess increases insize also.
The errors in precision of course meanthat at the current corpus sizes its guesses are for themoment not even subsets of the target language.According to one classic paradigm, a systemwould be held to be a successful learner if it couldbe proven to home in on the target language as thelearning sample increased in size indefinitely.
Thisis Gold?s (1967) criterion of identification in thelimit.
In this framework, an empirical analysis can-not be used to decide the adequacy of a learner, andwe would like to deemphasize the importance of theempirical results for this purpose.
That said, the em-pirical results are for now all we have to show, buteventually we hope to produce a mathematical proofof just what WWM can learn, and just what kinds oflexicons are learnable in Gold?s sense.To our knowledge, it has never been provenwhether the total lexicon of a natural language isidentifiable in the limit from the sort of data we pro-vide (i.e.
POS-tagged words), using in particular thetheory of Whole Word Morphology in a perfect fash-ion.
Still, it is interesting that nothing about this lan-guage learning paradigm says anything about mor-phological analysis.
The current crop of true mor-phological learners, e.g.
(Goldsmith, 2001b), en-deavor to learn to analyze the morphology of thelanguage at hand in the manner of a linguist.
Gold-smith has even called his Linguistica system a ?lin-guist in a box.?
This is perhaps an interesting andworthwhile endeavor, but it is not one that is un-dertaken here.
WWM is instead attempting to learnthe target language in a more direct way from thedata, without first constructing the intermediary ofa traditional morphological analysis.
We are thusnot learning the linguist?s notion of morphology butrather the result of morphology, i.e.
the word formsof the language together with the other informationthat goes into a word.25.3 Post-hoc fixes and future developmentsA significant proportion of errors in generation re-sult from the application of competing ambiguousmorphological strategies.
For example, when us-ing the (French) text of Le petit prince as its baselexicon, WWM produces two strategies relating 2ndperson verb forms to their infinitives.
Given the verbconjugues ?conjugate,?
pres.
2nd sing., one strategyproduces the correct -er class infinitive conjuguerwhile the other creates the non-word *conjuguere,based on the relation among -re verb forms likefais/faire ?do?
and vends/vendre ?sell.?
This is be-cause of an inherent ambiguity among various wordpairs which do not fully indicate the paradigms ofwhich they are a part.
WWM then adds to its lex-icon, not only the correct form, but all the outputswarranted by its grammar.To try to correct this problem, a form of lexicalblocking has been implemented in the current ver-sion of the program.
WWM creates every possibleword, including different strategies giving the sameone, and lets lexical lookup take precedence overproductive morphology.
The knowledge WWM pos-sesses about its lexicon increases considerably dur-ing the creation of morphological strategies.
Theprogram learns not only which strategies are li-censed by a given lexicon, but also which wordsof its lexicon are related to one another.
WWMcan assign a number to every lexical entry and givethe same ?paradigm?
number to related words.
Be-fore adding a newly created word to its lexicon, theprogram looks for an existing word with the sameparadigm number and category.
For example, ifWWM maps the word decoction, which was as-signed to, say, paradigm 489 onto a strategy creatingplural nouns, it will look for a plural noun belongingto paradigm 489 in its lexicon before it adds decoc-tions to the list of new words.Preliminary results are encouraging, with WWMreaching up to 92% accuracy in generation after2In this theory, a word?s form cannot be usefully divorcedfrom the other information that allows its proper use, and in ourimplementation the POS tags (poor substitutes for what shouldbe a richer database of information) are crucial to the discoveryof the strategies.the blocking modification.
Obviously the programneeds to be systematically tested on multiple lexicafrom different languages, but these results stronglysuggest that it is possible to model the acquisitionof morphology as a component of learning to gen-erate language directly, rather than to treat computa-tional learning as the acquisition of linguistic theoryas several current approaches do, e.g.
(Goldsmith,2001b).Although the principles of whole word morphol-ogy allow one to contemplate versions of WWM thatwould work on templatic morphologies, polysyn-thetic languages, and a host of other recalcitrant phe-nomena, the current instantiation of the program isnot so ambitious.
The comparison algorithm de-tailed in the previous section compares words letterby letter, either from left to right or from right toleft.
No other possible alignments between wordsare considered and WWM is in its current state onlycapable of grasping prefixal and suffixal morphol-ogy.
We are currently developing a more sophis-ticated sequence alignment routine which will al-low the program to handle infixing, circumfixing,and templatic morphologies of the Semitic type, aswell as word-internal changes typified by Germanicstrong verb ablaut.ReferencesAdam Albright and Bruce Hayes.
2001a.
Anautomated learner for phonology and morphol-ogy.
http://www.linguistics.ucla.edu/people/hayes/learning/index.htm.Adam Albright and Bruce Hayes.
2001b.Burnt and splang: Some problemsof generality in phonological learning.http://www.linguistics.ucla.edu/people/hayes/learning/index.htm.Stephen R. Anderson.
1992.
A-Morphous Morphology.Cambridge University Press.Evan L. Antworth.
1990.
PC-KIMMO: a two-level pro-cessor for morphological analysis.
Occasional Publi-cations in Academic Computing 16, Summer Instituteof Linguistics, Dallas, TX.Marco Baroni.
2000.
An automated distribution-drivenprefix learner.
Presented at the 9th International Mor-phology Meeting, Vienna, Austria, February.H.
Bochner.
1993.
Simplicity in Generative Morphology.Mouton de Gruyter, Berlin.Michael Brent.
1993.
Minimal generative models: Amiddle ground between neurons and triggers.
In Pro-ceedings of the 15th Annual Conference of the Cogni-tive Science Society, pages 28?36.
Lawrence ErlbaumAssociates.Bob Carpenter.
1992.
The Logic of Typed Feature Struc-tures, volume 32 of Cambridge Tracts in TheoreticalComputer Science.
Cambridge University Press.Noam Chomsky.
1955.
The logical structure of linguistictheory.
Unpublished manuscript.
Published as a bookwith a new introduction in 1975 by Plenum Press.Carl de Marcken.
1995.
Unsupervised Language Acqui-sition.
Ph.D. thesis, MIT.Sas?o Dz?eroski and Tomaz?
Erjavec.
1997.
Inductionof Slovene nominal paradigms.
In Nada Lavrac andSas?o Dz?eroski, editors, Inductive Logic Programming,7th International Workshop, volume 1297 of LectureNotes in Computer Science.
Springer.Gudrun Flenner.
1994.
Ein quantitatives Morphsegmen-tierungssystem fu?r spanische Wortformen.
In UrsulaKlenk, editor, Computatio Linguae II, pages 31?62.Steiner Verlag, Stuttgart.Gudrun Flenner.
1995.
Quantitative Morphseg-mentierung im Spanischen auf phonologisher Basis.Sprache und Datenverarbeitung, 19(2):63?79.A.
Ford and R. Singh.
1991.
Prope?deutique mor-phologique.
Folia Linguistica, 25(3?4):549?575.A.
Ford, R. Singh, and G. Martohardjono.
1997.
PacePanini.
Peter Lang, New York.E.
M. Gold.
1967.
Language identification in the limit.Information and Control, 10:447?474.John A. Goldsmith.
2001a.
Linguistica: An automaticmorphological analyzer.
In Arika Okrent and JohnBoyle, editors, CLS 36: The Main Session, volume 36-1.
Chicago Linguistic Society, Chicago.John A. Goldsmith.
2001b.
Unsupervised learning ofthe morphology of a natural language.
ComputationalLinguistics, 27(2):153?198.M.
A. Hafer and S. F. Weiss.
1974.
Word segmentationby letter successsor varieties.
Information Storage andRetrieval, 10(371?385).Zellig Harris.
1955.
From phoneme to morpheme.
Lan-guage, 31:190?222.Zellig Harris.
1967.
Morpheme boundaries withinwords: Report on a computer test.
In Transformationsand Discourse Analysis Papers, volume 73.Charles Hockett.
1954.
Two models of grammatical de-scription.
Word, 10:210?231.Axel Janssen.
1992.
Segmentierung franzo?sischer Wort-formen in Morphe ohne Verwendung eines Lexikons.In Ursula Klenk, editor, Computatio Linguae, pages74?95.
Steiner Verlag, Stuttgart.Makoto Kanazawa.
1998.
Learnable Classes of Catego-rial Grammars.
Studies in Logic, Language and Infor-mation.
CSLI Publications and the European Associa-tion for Logic, Language and Information.Lauri Karttunen, Kimmo Koskenniemi, and Ronald M.Kaplan.
1987.
A compiler for two-level phonologicalrules.
Technical Report CSLI-87-108, Center for theStudy of Language and Information, Palo Alto.Lauri Karttunen, Ronald M. Kaplan, and Annie Zaenen.1992.
Two-level morphology with composition.
InProceedings of the 15th International Conference onComputational Linguistics, volume I, pages 141?148,Nantes, France.Lauri Karttunen.
1993.
Finite state constraints.
InJohn A. Goldsmith, editor, The Last PhonologicalRule, pages 173?194.
University of Chicago Press.Lauri Karttunen.
1994.
Constructing lexical transducers.In Proceedings of the 15th International Conferenceon Computational Linguistics, volume I, pages 406?411.Dimitar Kazakov and Suresh Manandhar.
1998.
A hy-brid approach to word segmentation.
In David Page,editor, Proceedings of Inductive Logic Programming-98, volume 1446 of Lecture Notes in Computer Sci-ence.
Springer.Kimmo Koskenniemi.
1983.
Two-level morphology: ageneral computational model for word-form recogni-tion and production.
Technical Report 11, Dept.
ofGeneral Linguistics, University of Helsinki.S.
Neuvel and R. Singh.
In press.
Vive la diffe?rence!What morphology is about.
Folia Linguistica.Eugene Nida.
1949.
Morphology.
The descriptive anal-ysis of words.
University of Michigan Press, Ann Ar-bor, MI.Daniel N. Osherson, Michael Stob, and Scott Weinstein.1986.
Systems that Learn.
The MIT Press, Cam-bridge, MA.Patrick Schone and Daniel Jurafsky.
2001.
Knowledge-free induction of inflectional morphologies.
In 2ndMeeting of the North American Chapter of the ACL,pages 183?191.
Association for Computational Lin-guistics, Morgan Kaufman.
