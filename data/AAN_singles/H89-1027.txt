THE MIT  SUMMIT  SPEECH RECOGNIT ION SYSTEM:  A PROGRESS REPORT*Victor Zue, James Glass, Michael Phillips, and Stephanie SeneffSpoken Language Systems GroupLaboratory for Computer ScienceMassachusetts Institute of TechnologyCambridge, Massachusetts 02139ABSTRACTRecently, we initiated a project to develop a phonetically-based spoken language understanding systemcalled SUMMIT.
In contrast o many of the past efforts that make use of heuristic rules whose developmentrequires intense knowledge ngineering, our approach attempts to express the speech knowledge within aformal framework using well-defined mathematical tools.
In our system, features and decision strategies arediscovered and trained automatically, using a large body of speech data.
This paper describes the system,and documents its current performance.INTRODUCTIONFor slightly over a year, we have focused our research effort on the development of a phonetically-basedspoken language understanding system called SUMMIT.
Our approach is based on the belief that advancedhuman/machine communication systems must build on our understanding of the human communicationprocess.
Despite recent development ofsome speech recognition systems with high accuracy, the performanceof such systems typically falls far short of human capabilities.
We are placing heavy emphasis on designingsystems that can make use of the knowledge gained over the past four decades on human communication,in the hope that such systems will one day have a performance approaching that of humans.We are basing the design of our system on the premise that robust speech recognition is tied to ourability to successfully extract the linguistic information from the speech signal and discard those aspectsthat are extra-linguistic.
Like others before us, we have chosen phonemes and other related descriptors suchas distinctive features and syllables as the units to relate words in the lexicon to the speech signal.
However,there are several aspects that collectively distinguish our approach from those pursued by others.
First,we believe that many of the acoustic cues for phonetic ontrast are encoded at specific times in the speechsignal.
Therefore, one must explicitly establish acoustic landmarks in the speech signal in order to fullyutilize these acoustic attributes.
Second, unlike previous attempts at explicit utilization of speech knowledgeby heuristic means, we seek to make use of the available speech knowledge by embedding such knowledgein a formal framework whereby powerful mathematical tools can be utilized to optimize its use.
Third, thesystem must have a stochastic omponent to deal with the present state of ignorance in our understandingof the human communication process and its inherent variabilities throughout.
It is our belief that speech-specific knowledge will enable us to build more sophisticated stochastic models than what is currently beingattempted, and to reduce the amount of training data necessary for high performance.
Finally, the ultimategoal of our research is the understanding of the spoken message, and the subsequent accomplishment of atask based on this understanding.
To achieve this goal, we must fully integrate the speech recognition partof the problem with natural anguage processing so that higher level linguistic constraints can be utilized.
*This research was supported by DARPA under Contract N00039-85-C-0254, monitored through Naval Electronic SystemsCommand.179This paper describes those parts of our system dealing with acoustic segmentation, phonetic lassification,and lexical access, and documents its current performance on the DARPA Resource Management task \[1\].SYSTEM DESCRIPT IONThere are three major components in the SUMMIT system, as illustrated in Figure 1.
The first componenttransforms the speech signal into an acoustic-phonetic description.
The second expands a set of baseformpronunciations into a lexical network.
The final component provides natural language constraints.
Ourpreliminary efforts in natural angauge are described in a companion paper \[2\].
The acoustic-phonetic andlexical components will be discussed in more detail in the following sections.Signal1SignalRepresentation IAcousticsegmentationFeatureExtraction &PhonemeRecognitionLexiconLexicalExpansionHigher-LevelLinguisticKnowledgeLanguageModeling,1 ,I I oder I1Decoded UtteranceFigure 1: The major components of the SUMMIT system.ACOUSTIC-PHONETIC  REPRESENTATIONThe phonetic recognition subsystem ofSUMMIT takes as input the speech signal and produces as outputa network of phonetic labels with scores indicating the system's confidence in the segments and in theaccuracy of the labels.
The subsystem contains three parts: signal representation, acoustic segmentation,and phonetic lassification.
In this section, we describe ach of these three parts in some detail.Signal Representat ionThe phonetic recognition process tarts by first transforming the speech signal into a representation basedon Seneff's auditory model \[3\].
The model has three stages.
The first stage is a bank of linear filters, equallyspaced on a critical-band scale.
This is followed by a nonlinear stage that models the transduction process ofthe hair cells and the nerve synapses.
The output of the second stage bifurcates, one branch correspondingto the mean firing rate of an auditory nerve fiber, and the other measuring the synchrony of the signal tothe fiber's characteristic frequency.180The outputs from various stages of this model are appropriate for different operations in our subsystem.The nonlinearities of the second stage produce sharper onsets and offsets than are achieved through simplelinear filtering.
In addition, irrelevant acoustic information is often masked or suppressed.
These propertiesmake such a representation well-suited for the detection of acoustic landmarks.
The synchrony response, onthe other hand, provides enhanced spectral peaks.
Since these peaks often correspond to formant frequenciesin vowel and sonorant consonant regions, we surmise that the synchrony representation may be particularlyuseful for performing fine phonetic distinctions.
Advantages of using an auditory model for speech recognitionhave been demonstrated in many contexts, and can be found readily in the literature \[4,5,6\].Acoustic Segmentat ionOutputs of the auditory model are used to perform acoustic segmentation.
The objective of the segmen-tation procedure is to establish explicit acoustic landmarks that will facilitate subsequent feature xtractionand phonetic lassification.
Since there exists no single level of segmental representation that can adequatelydescribe all the acoustic events of interest, we adopted a multi-level representation that enables us to captureboth gradual and abrupt changes in one uniform structure.
Once such a structure has been determined,acoustic-phonetic analysis can then be formulated as a path-finding problem in a highly constrained searchspace.The construction of the multi-level representation has been described elsewhere \[7,8\].
Briefly, the al-gorithm delineates the speech signal into regions that are acoustically homogeneous by associating a givenframe to one of its immediate neighbors.
Acoustic boundaries are marked whenever the association directionswitches from past to future.
The procedure is then repeated by comparing a given acoustic region withits neighboring regions.
When two adjacent regions associate with each other, they are merged together toform a single region.
The process repeats until the entire utterance is described by a single acoustic event.By keeping track of the distance at which two regions merge into one, the multi-level description can bedisplayed in the form of a dendrogram, as is illustrated in Figure 2 for the utterance "Call an ambulance formedical assistance."
From the bottom towards the top of the dendrogram, the acoustic description variesfrom fine to coarse.
The release of the /k / in  "call," for example, may be considered to be a single acousticevent or a combination of two events (release plus aspiration) depending on the level of detail desired.
Bycomparing the dendrogram with the time-aligned phonetic transcription shown below, we see that, for thisexample, most of the acoustic events of interest have been captured.Phonet ic  Recogn i t ionThe multi-level acoustic segmentation provides an acoustic description of the signal.
Before lexical accesscan be performed, the acoustic regions must be converted into a form that reflects the way words arerepresented in the lexicon, which, in our case, is in terms of phonemes.
Since some of the phonemes can havemore than one stable acoustic region, the mapping between phonemes and acoustic region cannot be one-to-one.
Currently, we allow up to two acoustic regions to represent a single phoneme.
This is implemented bycreating an acoustic-phonetic (AP) network from the dendrogram that includes all single and paired regions.We have experimentally found this choice to be a reasonable compromise between a flexible representationand computational tractability.
To account for the fact that certain paths through the AP network are morelikely to occur than others, each segment is assigned a weight.Next, each of the segments in the AP network is described in terms of a set of attributes, which arethen transformed into a set of phoneme hypotheses.
Rather than defining specific algorithms to measurethe acoustic attributes, we define generic property detectors based on our knowledge of acoustic phonetics.These detectors have free parameters that control the details of the measurement.
Their optimal settingsare established by a search procedure using a large body of training data \[11\].This process is illustrated in Figure 3.
In this example, we explore the use of the spectral center of gravityas a generic property detector for distinguishing front from back vowels.
It has two free parameters, the181SUMMIT..... :::.... "%;o*'?o ?o,,;~;... ;8: 6F , s ,  \ [16 .5 ,  15 .2 \ ]  8 .88|abe l  scores.582 p .128.089 g .OG8\[ .851 ~ .843J .838 b .834i .811 y .884.gO3 r .083I abe I ;;%..:I .I 186rl .848 ~ .B~9?
.036 ~ .033E .i~31 U .029.B23 b ,~23# .821 ~r .el9J .el5 t .813?
.012 y ,E l li .808 e ,g~7.905 # ,005=!
.004 o"  .093SX IO3-B-MRTKO - Ca l l  an ambulance  fo r  med ica l  ass i s tance .
J' !?
.il~,(; i;i!
JitW,.
.:,~.
.
.
.
ii i ,,,z o" I f m " ' ?~ " 9' .
.
.
.
.
.
.
.
.
.
.
~ # k = n m m I d" n ~ f m E !
~ k = s l s e ~ ~1 sAlign Words Compare Sequences Netwonk flee Net Spectrogram UtteranceAP Network Oendrogram Parameters Recognition Tlml.g Word LatticeCluste~ Extend Vocabulary $A4ntout Segment Network TranscriptionabCdFigure 2: Acoustic segmentation f the sentence, "Call an ambulance for medical assistance."
The displaypanel on the right contains: a) spectrogram, b) a dendrogram, c) the time-aligned phonetic transcription,and d) an acoustic-phonetic network.lower and upper frequency edges.
An example of this measurement fora vowel token is superimposed onthespectral slice below the spectrogram, with the horizontal line indicating the frequency range?
To determinethe optimal settings for the free parameters, we first compute the classification performance on a large setof training data for all combinations of the parameter settings.
We then search for the maximum on thesurface defined by the classification performance.
The parameter settings that correspond to the maximumare chosen to be the optimal settings.
For this example, the classification performance of this attribute,using the automatically selected parameter settings, is shown at the top right corner.
Note that an attributecan also be used in conjunction with other attributes, or to derive other attributes.We believe that the procedure described above is an example of successful knowledge ngineering inwhich the human provides the knowledge and intuition, and the machine provides the computational power.Frequently, the settings result in a parameter that agrees with our phonetic intuitions.
In this example, theoptimal settings for this property detector result in an attribute that closely follows the second formant,which is known to be important for the front/back distinction.
Our experience with this procedure suggeststhat it is able to discover important acoustic parameters that signify phonetic ontrasts, without resortingto the use of heuristic rules.Once the attributes have been determined, they are selected through another optimization process?Classification is achieved using conventional pattern classification algorithms \[9\].
In our current scheme,we use a double-layered approach, with the first layer distinguishing among a small set of classes, and thesecond layer defining a mapping from these classes to the phone labels used to represent the lexicon.
Thisapproach enables us to build a small number of simple classifiers that distinguish the speech sounds along182GcmHc ProEcz'ty Dcmct~- :  C?~.
?z  o f  Grav i tyFr~ Paran*~=m: Lowcr Et.lgcU ~SAILS37, ly ;key)  \[.
.
.
.
, m. I .I ,2~\[ .
.
.
.
, i l l  .
.
.
.
.
:1111111.?
1t .~ 21 .s,J :~.l,~ ~.
i6?
, .
.
.
.
.
.
.
,2~ .
.
.
.
.
~ ,~ - .
~ j  ~ .
.
.
.Urn ,orSelect Next ValueDURATIONFSN-AREA-AT-TIMEAREA-AT-TIMEFigure 3: An example of interactive discovery of acoustic attributes for phonetic lassification.several phonetic dimensions.
The aggregate of these dimensions describes the contextual variations, whichcan then be captured in the mapping between the classes and the lexicon.
Our experience indicates thatsuch an approach leads to rapid convergence in the models with only a small number of training tokens foreach label.The current scheme for scoring the N classes begins with N(N- 1)/2 pairwise Gaussian classifiers, eachof which uses a subset of the acoustic attributes elected to optimize the discrimination of the pair.
Theprobability of a given class is obtained by summing the probabilities from all the relevant pairwise results.The classes are then mapped to an orthogonal space using principal component analysis.
Finally, the score foreach phoneme label is obtained from a Gaussian model of the distributions of the scores for the transformedclasses.Following phone classification, each segment in the AP network is represented by a list of phone candi-dates, with associated probability, as illustrated in Figure 2.
The network is shown just below the transcrip-tion.
In this display, only the AP segments surrounding the most probable path are displayed.
The networkdisplays only the top-choice label, although additional information can easily be accessed.
For this example,the /k /  in "call" is correctly identified, and its score, in terms of probability, is displayed in the left-handpanel along with several near-miss candidates.
On the other hand, the same panel shows that the correctlabel for the first schwa in "assistance" is the third most likely candidate, beh ind /n /and/ r j / .LEX ICAL  REPRESENTATIONWe are adopting the point of view that it is preferable to offer several alternative pronunciations for eachword in the lexicon, and then to build phoneme models that can be made more specific as a consequence.
Ifaccurate pronunciation probabilities can be acquired for the alternate forms, then this is a viable approachfor capturing inherent variability in the acceptable pronunciations of words.
For example, the last syllablein a word such as 'cushion' could be realized as a single syllabic nasal consonant or as a sequence of a voweland a nasal consonant.
The vowel could be realized as a short schwa, or as a normal lax vowel.
For thesystem to be able to accept all of these alternatives, they must be entered into the lexicon in the form ofa network.
Currently, lexical pronunciations are expanded by rule to incorporate both within-word and183across-word-boundary phonological effects.
1 These rules describes common low-level phonological processessuch as flapping, palatalization, and gemination.We have developed an automatic procedure for establishing probability weighting on all of the arcs inthe word pronunciation etworks.
Currently the weights are entered into the total log probability score andare centered around a score of zero representing no influence.
These weights were generated automaticallyby determining both the recognition path as well as the forced recognition path (i.e., the path obtainedwhen the system is given the correct answer) for a large number of utterances.
From this information, wecomputed: 1) the number of times an arc was used correctly, R, 2) the number of times an arc was missed,M, and 3) the number of times an arc was used incorrectly, W. Once these numbers were tabulated wecould assign a weight to each lexical arc.
Currently, this weight corresponds to the log ratio of R+ M, whichis the total number of times an arc was used in the forced recognition path, to R ?
W, which is the totalnumber of times an arc was used in the normal recognition path.
Thus, if an arc was missed more often thanit was used incorrectly, a positive weight is added to the lexical score, which will make the system prefer touse this arc.
When the arc is more often incorrect, a negative weight is added, penalizing that arc.
Whenthere are the same number of misses as incorrect uses of the arc, or when they form a small fraction of thetotal number of times an arc was used correctly, the weight has little influence.DECODERThe lexical representation described above consists of pronunciation etworks for the words in the vo-cabulary.
These networks may be combined into a single network that represents all possible sentences byconnecting word end nodes with word start nodes that satisfy the inter-word pronunciation constraints.Local grammatical constraints may also be expressed in terms of allowable connections between words.The task of lexical decoding can be expressed as a search for the best match between a path in thislexical network and a path in the AP network.
Currently, we use the Viterbi algorithm to search for thisbest scoring match.
Since we cannot expect he phonetic network to always contain the appropriate phoneticsequence, the search algorithm allows for the insertion and deletion of phonetic segments with penalties thatare based on the performance of the AP network on training data.
The search algorithm is illustrated inFigure 4.The possible alignments of nodes in the lexical network to nodes in the phonetic network are representedby a matrix of node-pairs.
A match between a path in the lexical network and a path in the phoneticnetwork can be represented as a sequence of allowable links between these node-pairs.
The allowable linksfall into four categories: normal matches, insertions, deletions, and interword connections.
Examples of eachare shown in Figure 4.
Link (a) is a normM match between an arc in the lexical network and an arc inthe phonetic network.
Link (b) is an example of an insertion of a phonetic segment (the path advances bya phonetic segment while staying at the same point in the lexical network).
Link (c) is an example of aninterword connection.
Link (d) is an example of a deletion of a phonetic segment (the path contains a lexicalarc without advancing in the phonetic network).The score for a match is the sum of the scores of the links in the match.
This allows the search forthe best path to proceed recursively since the best score to arrive at a given node-pair is the best of thescore of each arriving link plus the best score to arrive at start of the link.
Currently, the scores include aphonetic match component, an existence score based on the probability of the particular segmentation, alexical weight associated with the likelihood of the pronunciation, and a duration score based on the phoneduration statistics.
The best match for the utterance is the best match that ends at terminal nodes of thelexical network and phonetic network.1 Our system currently uses a phonological expansion program, called RULE, developed by researchers at SRI International\[121.184: 0iz ?
:lj?
?
?
?
?
O~ ?
??
?
?
?
?
?
/ (d ) ?
??
?
?
?
O /  ?
??
?
?
?
~ ?
?
??
?
~ ( c )  o ?
?,..
.
?
?
::..@ ?
?
?
?
@ ?
@Phonetic NetworkFigure 4: Illustration of Viterbi search used in SUMMIT.PERFORMANCE EVALUATIONPHONETIC  RECOGNIT IONThe effectiveness of the acoustic-phonetic component has been reported elsewhere \[7,13\].
The performanceof the segmentation algorithm was measured by first finding a path through the dendrogram that correspondsbest to a time-aligned phonetic transcription, as illustrated by the path highlighted in white in Figure 2, andthen tabulating the differences between these two descriptions.
On 500 TIMIT \[10\] sentences spoken by 100speakers, the algorithm deleted about 3.5% of the boundaries along the aligned path, while inserting an extra5%.
Analysis of the time difference between the boundaries found and those provided by the transcriptionshows that more than 70% of the boundaries were within 10 ms of each other, and more than 90% werewithin 20 ms.The phonetic lassification results are evaluated by comparing the labels provided by the classifier to thosein a time-aligned transcription.
We have performed the evaluation on two separate databases, as summarizedin Table 1.
Performance was measured oil a set of 38 context-independent phone labels.
This particularset was selected because it has been used in other recent evaluations within the DARPA community.
Fora single speaker, the top-choice classification accuracy was 77%.
The correct label is within the top threenearly.95% of the time.
For multiple and unknown speakers, the top-choice accuracy is about 70%, and thecorrect choice is within the top three over 90% of the time.
Figure 5 shows the rank order statistics for thespeaker-independent case.185No.
of No.
of No.
of No.
of Top-ChoiceDatabase Training Training Test Test AccuracySentences Speakers Sentences Speakers (%)1 510 1 210 same 772 1500 300 225 45 70Table h Summary of speaker-dependent a d-independent phonetic lassification results.1009080'= 70'860'50 ' '0 1 2 3 4 5 6 7 8 9 10 11 12Rank OrderFigure 5: Rank order statistics for the current phone classifier on a speaker-independent task.
There are 38context-independent phone labels: 14 vowels, 3 semivowels, 3 nasals, 8 fricatives, 2 affricates, 6 stops, 1 flap,and one for silence.WORD RECOGNIT IONThe SUMMIT system was originally developed for the task of recognizing sentences from the TIMITdatabase.
Over the past three, months, we have ported the system to the DARPA 1000-word ResourceManagement (RM) task, and evaluated its recognition performance.
The phoneme models were seeded from1500 TIMIT sentences, and re-tralned on the RM task using 40 sentences each from 72 designated trainingspeakers \[1\].
The system was evaluated on two test sets, and for two conditions.
The first test set, containing10 sentences each from 15 speakers, is known as the '87 Test Set The second test set, called the '89 TestSet, was recently released to the DARPA community, and it contains 30 sentences each from 10 speakers.Each test set was evaluated under both the all-word condition (i.e.
no language model) and the word-pairconditions, in which a designated language model with a perplexity of 60 is used.The results of our evaluation are summarized in Table 2.
~ Note that this result is obtained by using 75phoneme models, 32 of which are used to denote 16 stressed/unstressed vowel pairs.
At the moment, oursystem does not explicitly make use of context-dependent models.2The accuracy is computed from the error rate which includes insertions, deletions, and substitutions186All-Word Word PairTest Set Accuracy Accuracy(%) (%)'87 42.3 87.2'89 46.2 86.4Table 2: Summary of word recognition performance r sults.D ISCUSSIONThis paper summarizes the current status of the SUMMIT system.
We have described the implemen-tation and reported results for phonetic lassification as well as word recognition for the DAR.PA ResourceManagement tasks with and without a language model.Our evaluation results on phonetic classification indicate that performance is much better for a singlespeaker than for multiple, unknown speakers.
This result should not be surprising, since the acousticvariability across speakers is much larger than that within a speaker.
One way to assess these results is tocompare them to human performance on a similar task.
We have conducted some preliminary listening testsin which subjects were asked to identify a phoneme xcised from the same database of multiple speakerswith minimal contextual information.
The results suggest hat human performance may be at the 60 to 70%level.An area where further esearch is definitely needed is the appropriate representation for acoustic-to-lexicalmapping.
This includes a more flexible association of phonemes with acoustic segments than is currentlyallowed, a different choice for the intermediate phonetic representation, and the development of context-dependent models.
Presently the choice of the classes in the first layer of the classifier is somewhat arbitrary.We believe that an inventory of classes that is based on distinctive feature theory may be more appropriate.In the first place, the pairwise discrimination analysis is well-suited to a binary feature representation, wherephonetic units with contrasting feature values logically define the two sets to be discriminated.
Similarly,context-dependent lexical labels in the second stage could also be mapped to a feature-based form to takeinto account allophonic variations.
For example, an /a~/followed by a nasal and an /a~/ followed by analveolar would be marked with the right context \[+ nasalized\] and \[+ alveolar\], respectively.
Thus, the vowelin the word "can" would be pooled with all other instances of/a~/followed by nasals, and, separately, withall other instances of/a~/followed by alveolars (/s,t,d/, etc.
), to form two distinct second-layer mappings.This approach may provide an elegant way to incorporate context dependency into our recognition system.It may also help to overcome the sparse data problems inherent in very specific context-dependent models.The RULE system developed by SRI for phonological expansion has been very useful in providing networksof alternate pronunciations.
Nevertheless, we have recently initiated an effort to develop a pronunciationexpansion program that 'allows researchers to write phonological rules more efficiently and flexibly, and toconform to the architecture of other parts of the SUMMIT system.
We expect that it will be completedwithin the next two months.
The use of lexical weights was found experimentally to improve the performanceof the recognition system by a significant amount.
Note that a similar procedure could be used incrementallyto allow the system to adapt to a particular speaker's pronunciation preferences.One still-unresolved issue has to do with how to combine the scores for the individual matches to forma total score for lexical decoding.
One possibility is to assign equal weight for equal time.
Such a schemeresults in an inordinately large weight for long sustained vowels as compared with rapid nonstatic soundssuch as stop releases.
Within the segmental framework, we have the capability to explore several alternativeapproaches.
With a time-normalization scheme, the system can accept a very short erroneous phoneme witha terrible score, but with a per-phoneme weight a reverse ffect can occur, where a very long badly-matchingphoneme can survive because it gets such little weight.
We have come up with an approach which essentiallyaccumulates a total score without any normalization, but adds to the log-probability estimate with each187update an offset factor that tends to keep the correct answer near zero.
This strategy compared favorablywith others that we tried, and also required less computation.The Viterbi search algorithm is a very efficient mechanism for pruning paths when they merge withbetter-scoring competitors.
However, it loses a great deal of its advantage when a true language modelcapable of natural language understanding is incorporated, because many fewer paths can be collapsed intoa single equivalent class.
Our hope, however, is that a Viterbi-like pruning strategy can be incorporated intoa hierarchical structure representing a syntactic analysis by keeping a record of equivalent subparses locallywith each node in the hierarchy.
We plan to pursue this kind of strategy when we join our recognizer witha natural language component.The word recognition performance of SUMMIT is fairly consistent across test sets.
While it is ahvaysdifficult o compare the performance ofrecognition systems directly, the establishment of standard atasets,language model, and evaluation guidelines has made the task a lot easier.
For example, the SPHINX systemdeveloped at CMU \[14\], when evaluated on the '87 Test Set using the word-pair grammar, achieved a wordrecognition rate of 84% and 93% using 48 and 1000 models.
Our result of 87% on 75 models is quitecompetitive, using a very different approach to speech recognition than hidden Markov modelling.
However,it is sobering to note that these results fall far short of human performance.
For example, we found that thehuman word recognition rate for the '89 Test Set was approximately 99.9% for a single listener.
Clearly westill have a long way to go!Acknowledgment sIn developing the SUMMIT system, we have benefitted from our earlier collaboration with researchersfrom SRI International.
In particular, we would like to thank Mitch Weintraub for providing the RULEprogram, Hy Murveit for writing the first version of the Viterbi decoding algorithm, and Jared Berstein andMike Cohen for many hours of discussion on lexical representation a d expansion.
The development of theSUMMIT system has been helped by the programming expertise of Rob Kassel and David Kaufman.References\[1\] Pallett, D., "Benchmark Tests for DARPA Resource Management Database Performance Evaluations,"Proc.
ICASSP-89, May, 1989.\[2\] Seneff, S., "TINA: A Probabilistic Syntactic Parser for Speech Understanding Systems," These Pro-ceedings, 1989.\[3\] Seneff, S., "A 3oint Synchrony/Mean-Rate Model of Auditory Speech Processing," Proc.
J. of Phonetics,vol.
16, pp.
55-76, January 1988.\[4\] Glass, J. R., and V. W. Zue, "Signal Representation forAcoustic Segmentation," Proc.
First AustralianConference on Speech Science and Technology, pp.
124-129, November 1986\[5\] Hunt, M. J., and C. Lefebvre, "Speaker Dependent and Independent Speech Recognition Experimentswith an Auditory Model," Proc.
ICASSP-88, pp.
215-218, April 1988.\[6\] Cohen, J.R., "Application of an Auditory Model to Speech Recognition," Proceedings, Montreal Sym-posium on Speech Recognition, p. 8, July, 1986.\[7\] Glass, J. R., and V. W. Zue, "Multi-Level Acoustic Segmentation f Continuous Speech," Proc.
ICASSP-88, pp.
429-432, April 1988.\[8\] Glass, J. R., "Finding Acoustic Regularities in Speech: Applications to Phonetic Recognition," Ph.D.thesis, Massachusetts Institute of Technology, May 1988.188\[9\] Duda, R. O., and P. Hart, Pattern Classification and Scene Analysis, John Wiley & Sons, Inc., 1973.\[10\] Lamel, L. F., R. H.Kassel, and S. Seneff, "Speech Database Development: Design and Analysis of theAcoustic-Phonetic Corpus," Proc.
DARPA Speech Recognition Workshop, Report No.
SAIC-86/1546,pp.
100-109, February 1986.\[11\] Phillips, M., "Automatic discovery of acoustic measurements for phonetic lassification," J. Aeoust.
Soc.Am., Vol.
84, $216, 1988.\[12\] Weintraub, M., and J. Bernstein, "RULE: A System for Constructing Recognition Lexicons," Proc.DARPA Speech Recognition Workshop, Report No.
SAIC-87/1644, pp.
44-48, February 1987.\[13\] Zue, V., J.
Glass, M. Pfiillips, and S. Seneff, "Acoustic Segmentation a d Phonetic Classification in theSUMMIT system," Proc.
ICASSP-89, May, 1989.\[14\] Lee, K-F., Automatic Speech Recognition: The Development of the Sphinx System, Kluwer AcademicPublishers, Boston, 1989.189
