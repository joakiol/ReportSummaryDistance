Coling 2010: Poster Volume, pages 1014?1022,Beijing, August 2010Automatic Committed Belief TaggingVinodkumar PrabhakaranColumbia Universityvp2198@columbia.eduOwen RambowColumbia Universityrambow@ccls.columbia.eduMona DiabColumbia Universitymdiab@ccls.columbia.eduAbstractWe go beyond simple propositional mean-ing extraction and present experiments indetermining which propositions in text theauthor believes.
We show that deep syn-tactic parsing helps for this task.
Ourbest feature combination achieves an F-measure of 64%, a relative reduction in F-measure error of 21% over not using syn-tactic features.1 IntroductionRecently, interest has grown in relating text tomore abstract representations of its propositionalmeaning, as witnessed by work on semantic rolelabeling, word sense disambiguation, and textualentailment.
However, there is more to ?meaning?than just propositional content.
Consider the fol-lowing examples, and suppose we find these sen-tences in the New York Times:(1) a. GM will lay off workers.b.
A spokesman for GM said GM will lay offworkers.c.
GM may lay off workers.d.
The politician claimed that GM will layoff workers.e.
Some wish GM would lay of workers.f.
Will GM lay off workers?g.
Many wonder if GM will lay off workers.If we are searching text to find out whetherGM will lay off workers, all of the sen-tences above contain the proposition LAY-OFF(GM,WORKERS).
However, they allow usvery different inferences about whether GM willlay off workers or not.
Supposing we considerthe Times a trustworthy news source, we wouldbe fairly certain if we read (1a) and (1b).
(1c)suggests the Times is not certain about the layoffs,but considers them possible.
When reading (1d),we know that someone else thinks that GM willlay off workers, but that the Times does not nec-essarily share this belief.
(1e), (1f), and (1g) donot tell us anything about whether anyone believeswhether GM will lay off workers.In order to tease apart what is happening, weneed to abandon a simple view of text as a repos-itory of propositions about the world.
We use twoassumptions to aid us.
The first assumption is thatdiscourse participants model each other?s cogni-tive state during discourse (we take the term to in-clude the reading of monologic written text), andthat language provides cues for the discourse par-ticipants to do the modeling.
This assumption iscommonly made, for example by Grice (1975) inhis Maxim of Quantity.
Following the literaturein Artificial Intelligence (Bratman, 1999; Cohenand Levesque, 1990), we model cognitive state asbeliefs, desires, and intentions.
Crucially, thesethree dimensions are orthogonal; for example, wecan desire something but not believe it.
(2) I know John won?t be here, but I wouldn?tmind if he wereHowever, we cannot both believe somethingand not believe it:(3) #John won?t be here, but nevertheless I thinkhe may be hereNote that (2) requires but in order to be felic-itous, but sentence (3) cannot be ?saved?
by anydiscourse markers ?
it is not interpretable.
In thispaper, we are interested in beliefs (and in distin-1014guishing them from desires and intentions).The second assumption is that communicationis intention-driven, and understanding text actu-ally means understanding the communicative in-tention of the writer.
Furthermore, communica-tive intentions are intentions to affect the reader?scognitive state ?
his or her beliefs, desires, and/orintentions.
This view has been adopted in the textgeneration and dialog community more than inthe information extraction and text understandingcommunities (Mann and Thompson, 1987; Hovy,1993; Moore, 1994; Bunt, 2000; Stone, 2004).
Inthis paper we explore the following: we wouldlike to recognize what the writer of the text intendsthe reader to believe about various people?s beliefsabout the world (including the writer?s own).
Inthis view, the result of text processing is not a listof facts about the world, but a list of facts aboutdifferent people?s cognitive states.
In this paper,we limit ourselves to the writer?s beliefs, but wespecifically want to determine which propositionshe or she intends us to believe he or she holds asbeliefs, and with what strength.
The result of suchprocessing will be a much more fine-grained rep-resentation of the information contained in writtentext than has been available so far.2 Belief Annotation and DataWe use a corpus of 10,000 words annotated forspeaker belief of stated propositions (Diab et al,2009).
The corpus is very diverse in terms ofgenre, and it includes newswire text, email, in-structions, and solicitations.
The corpus annotateseach verbal proposition (clause or small clause),by attaching one of the following tags to the headof the proposition (verbs and heads of nominal,adjectival, and prepositional predications).?
Committed belief (CB): the writer indicatesin this utterance that he or she believes the propo-sition.
For example, GM has laid off workers, or,even stronger, We know that GM has laid off work-ers.
Committed belief can also include proposi-tions about the future: people can have equallystrong beliefs about the future as about the past,though in practice probably we have stronger be-liefs about the past than about the future.?
Non-committed belief (NCB): the writeridentifies the proposition as something which heor she could believe, but he or she happens notto have a strong belief in.
There are two sub-cases.
First, the writer makes clear that the be-lief is not strong, for example by using a modalauxiliary epistemically: GM may lay off workers.Second, in reported speech, the writer is not sig-naling to the reader what he or she believes aboutthe reported speech: The politician claimed thatGM will lay off workers.
Again, the issue of tenseis orthogonal.?
Not applicable (NA): for the writer, the propo-sition is not of the type in which he or she is ex-pressing a belief, or could express a belief.
Usu-ally, this is because the proposition does not have atruth value in this world (be it in the past or in thefuture).
This covers expressions of desire (Somewish GM would lay of workers), questions (WillGM lay off workers?
), and expressions of require-ments (GM is required to lay off workers or Layoff workers!
).All propositional heads are classified as one ofthe classes CB, NCB, or NA, and all other tokensare classified as O.
Note that in this corpus, eventnominals (such as the lay-offs by GM were unex-pected) are, unfortunately, not annotated for be-lief and are always marked ?O?.
Note also thatthe syntactic form does not determine the annota-tion, but the perceived writer?s intention ?
a ques-tion will usually be an NA, but sometimes a ques-tion can be used to convey a belief (for example,a rhetorical question), in which case it would belabeled CB.3 Automatic Belief Tagging3.1 ApproachWe applied a supervised learning framework tothe problem of identifying committed belief incontext.
Our task consists of two conceptual sub-tasks: identifying the propositions, and classify-ing each proposition as CB, NCB, or NA.
For thefirst subtask, we could use a system that cuts asentence into propositions, but we are not awareof such a system that performs at an adequatelevel.
Instead, we tag the heads of the proposi-tion, which amounts to the same in the sense thatthere is a bijection between propositions and theirheads.
Practically, we have the choice between1015No Feature Type DescriptionFeatures that performed well1 isNumeric L Word is Alphabet or Numeric?2 POS L Word?s POS tag3 verbType L Modal/Aux/Reg ( = ?nil?
if the word is not a verb)4 whichModalAmI L If I am a modal, what am I?
( = ?nil?
if I am not a modal)3 amVBwithDaughterTo S Am I a VB with a daughter to?4 haveDaughterPerfect S Do I have a daughter which is one of has, have, had?5 haveDaughterShould S Do I have a daughter should?6 haveDaughterWh S Do I have a daughter who is one of where, when, while, who, why?7 haveReportingAncestor S Am I a verb/predicate with an ancestor whose lemma is one of tell, accuse,insist, seem, believe, say, find, conclude, claim, trust, think, suspect, doubt,suppose?8 parentPOS S What is my parent?s POS tag?9 whichAuxIsMyDaughter S If I have a daughter which is an auxiliary, what is it?
( = ?nil?
if I do not havean auxiliary daughter)10 whichModalIsMyDaughter S If I have a daughter which is a modal, what is it?
( = ?nil?
if I do not have amodal daughter)Features that were not useful1 Lemma L Word?s Lemma2 Stem L Word stem (Using Porter Stemmer)3 Drole S Deep role (drole in MICA features)4 isRoot S Is the word the root of the MICA Parse tree?5 parentLemma S Parent word?s Lemma6 parentStem S Parent word stem (Using Porter Stemmer)7 parentSupertag S Parent word?s super tag (from Penn Treebank)8 Pred S Is the word a predicate?
(pred in MICA features)9 wordSupertag S Word?s Super Tag (from Penn Treebank)Table 1: All Features Useda joint model, in which the heads are chosen andclassified simultaneously, and a pipeline model, inwhich heads are chosen first and then classified.In this paper, we consider the joint model in de-tail and in Section 3.5.3, we present results of thepipeline model; they support our choice.In the joint model, we define a four-way clas-sification task where each token is tagged as oneof four classes ?
CB, NCB, NA, or O (nothing)?
as defined in Section 2.
For tagging, we ex-perimented with Support Vector Machines (SVM)and Conditional Random Fields (CRF).
For SVM,we used the YAMCHA(Kudo and Matsumoto,2000) sequence labeling system,1 which uses theTinySVM package for classification.2 For CRF,we used the linear chain CRF implementation of1http://chasen.org/ taku/software/YAMCHA/2http://chasen.org/ taku/software/TinySVM/the MALLET(McCallum, 2002) toolkit.33.2 FeaturesWe divided our features into two types - Lexi-cal and Syntactic.
Lexical features are at the to-ken level and can be extracted without any pars-ing with relatively high accuracy.
We expect thesefeatures to be useful for our task.
For example,isNumeric, which denotes whether the word is anumber or alphabetic, is a lexical feature.
Syn-tactic features of a token access its syntactic con-text in the dependency tree.
For example, par-entPOS, the POS tag of the parent word in thedependency parse tree, is a syntactic feature.
Weused the MICA deep dependency parser (Banga-lore et al, 2009) for parsing in order to derivethe syntactic features.
We use MICA becausewe assume that the relevant information is the3http://MALLET.cs.umass.edu/1016predicate-argument structure of the verbs, whichis explicit in the MICA output.
While it is clearthat having a perfect parse would yield useful fea-tures, current parsers perform at levels of accuracylower than that of part-of-speech taggers, so that itis not a foregone conclusion that using automaticparser output helps in our task.The list of features we used in our experimentsare summarized in Table 1.
The column ?Type?denotes the type of the feature.
?L?
stands for lex-ical features and ?S?
stands for syntactic features.The tree below shows the dependency parsetree output by MICA for the sentence Republicanleader Bill Frist said the Senate was hijacked.saidFristRepublican leader BillhijackedSenatethewasIn the above sentence, said and hijacked arethe propositions that should be tagged.
Let?s lookat hijacked in detail.
The feature haveReportin-gAncestor of hijacked is ?Y?
because it is a verbwith a parent verb said.
Similarly, the featurehaveDaughterAux would also be ?Y?
because ofdaughter was, whereas whichAuxIsMyDaughterwould get the value was.We also considered several other features whichdid not yield good results.
For example, the to-ken?s supertag (Bangalore and Joshi, 1999), theparent token?s supertag, a binary feature isRoot(Is the word the root of the parse tree?)
weredeemed not useful.
We list the features we exper-imented with and decided to discard in Table 1.For finding the best performing features, we didan exhaustive search on the feature space, incre-mentally pruning away features that are not use-ful.3.3 ExperimentsThis section describes different experiments weconducted in detail.
It explains the experimen-tal setup for both learning frameworks we used- YAMCHA and MALLET.
We also explain thepipeline model in detail.Class DescriptionLC Lexical features with ContextLNSN Lexical and Syntactic features with No-contextLCSN Lexical features with Context and Syntacticfeatures with No-contextLCSC Lexical and Syntactic features with ContextTable 2: YAMCHA Experiment Sets3.3.1 YAMCHA ExperimentsWe categorized our YAMCHA experimentsinto different experimental conditions as shown inTable 2.
For each class, we did experiments withdifferent feature sets and (linear) context widths.Here, context width denotes the window of tokenswhose features are considered.
For example, acontext width of 2 means that the feature vectorof any given token includes, in addition to its ownfeatures, those of 2 tokens before and after it aswell as the tag prediction for 2 tokens before it.For LNSN , the context width of all features wasset to 0.
For LCSN , the context width of syntacticfeatures alone was set to 0.
A context width of 0for a feature means that the feature vector includesthat feature of the current token only.
When con-text width was non-zero, we varied it from 1 to 5,and we report the results for the optimal contextwidth.We tuned the SVM parameters, and the bestresults were obtained using the One versus Allmethod for multiclass classification on a quadratickernel with a c value of 0.5.
All results presentedfor YAMCHA here use this setting.3.3.2 MALLET ExperimentsClass DescriptionL Lexical features onlyLS Lexical and Syntactic featuresTable 3: MALLET Experiment SetsWe categorized our MALLET experiments intotwo classes as shown in Table 3.
We computedthe features described in Section 3.2 at the to-ken level and converted them to binary in order touse them for CRF.
We experimented with varyingorders and the best results were obtained for or-1017Class Feature Set Parm P R FYAMCHA - Joint ModelLC POS, whichModalAmI, verbType, isNumeric CW=3 61.9 52.7 56.9LNSN POS, whichModalAmI, parentPOS, haveReportingAncestor, whichModal-IsMyDaughter, haveDaughterPerfect, whichAuxIsMyDaughter, amVBwith-DaughterTo, haveDaughterWh, haveDaughterShouldCW=0 62.5 57.5 59.9LCSN POS, whichModalAmI, parentPOS, haveReportingAncestor, whichModalIs-MyDaughter, whichAuxIsMyDaughter, haveDaughterShouldCW=2 67.4 58.1 62.4LCSC POS, whichModalAmI, parentPOS, haveReportingAncestor, whichModal-IsMyDaughter, haveDaughterPerfect, whichAuxIsMyDaughter, haveDaugh-terWh, haveDaughterShouldCW=2 68.5 60.0 64.0MALLET - Joint ModelL POS, whichModalAmI, verbType GV=1 55.1 45.0 49.6LS POS, whichModalAmI, parentPOS, haveReportingAncestor, whichModal-IsMyDaughter, haveDaughterPerfect, whichAuxIsMyDaughter, haveDaugh-terWh, haveDaughterShouldGV=1 64.5 54.4 59.0Pipeline ModelLCSC POS, whichModalAmI, parentPOS, haveReportingAncestor, whichModal-IsMyDaughter, haveDaughterPerfect, whichAuxIsMyDaughter, haveDaugh-terWh, haveDaughterShouldCW=2 49.8 42.9 46.1Table 4: Overall Results.
CW = Context Width, GV = Gaussian Variance, P = Precision, R = Recall, F= F-Measureder= ?0,1?, which makes the CRF similar to Hid-den Markov Model.
All results reported here usethe order= ?0,1?.
We also conducted experimentsvarying the Gaussian variance parameter from 1.0to 10.0 using the same experimental setup (i.e.we did not have a distinct tuning corpus) and ob-served that best results were obtained with a lowvalue of 1 to 3, instead of MALLET?s defaultvalue of 10.0.3.3.3 Pipeline ModelWe also did experiments to support our choiceof the joint model over the pipeline model.
Wechose the best performing feature configurationof the LCSC class (which is the overall bestperformer as we present in Section 3.5), andset up the pipeline model.
We trained a se-quence classifier using YAMCHA to identify thehead tokens, where tokens are tagged as justpropositional heads without distinguishing be-tween CB/NA/NCB.
The predicted head tokenswere then classified using a 3-Way SVM classi-fier trained on gold data.3.4 EvaluationFor evaluation, we used 4-fold cross validation onthe training data.
The data was divided into 4 foldsof which 3 folds were used to train a model whichwas tested on the 4th fold.
We did this with allfour configurations and all the reported results inthis paper are averaged results across 4 folds.
Wereport Recall and Precision on word tokens in ourcorpus for each of the three tags.
It is worth notingthat the majority of the words in our data will notbe tagged with any of the three classes.
(Recallthat most words have neither of the three tags).We also report F?=1 (F)-measure as the harmonicmean between (P)recision and (R)ecall.3.5 ResultsThis section summarizes the results of variousexperiments we conducted.
The best perform-ing feature configuration and corresponding Pre-cision, Recall and F-measure for each experimen-tal setup discussed in previous section is presentedin Table 4.
The best F-measure for each categoryunder various experimental setups is presented inTable 5.We obtained the best performance using YAM-1018Setup Class CB NCB NAJoint-YAMCHA LC 61.5 15.2 63.2Joint-YAMCHA LNSN 67.0 28.3 59.9Joint-YAMCHA LCSN 67.6 33.2 64.5Joint-YAMCHA LCSC 69.6 34.1 64.5Joint-MALLET L 53.9 7.5 54.1Joint-MALLET LS 65.8 40.6 59.1Pipeline LCSC 55.2 16.5 51.3Table 5: Results per Category (F-Measure)CHA in a joint model.
So, we first analyze thisconfiguration in great detail in Section 3.5.1.
Wediscuss results obtained using MALLET in Sec-tion 3.5.2 and the pipeline model in Section-3.5.3.3.5.1 YAMCHA - ResultsAs described in Section 3.3.1, we divide ourexperiments into 4 classes - LC , LNSN , LCSNand LCSC .
Table 4 presents the best perform-ing feature sets and context width configurationfor each class.
For all experiments with context,the best result was obtained with a context widthof 2, except for LC , where a context width of 3gave the best results.
The results show that syn-tactic features improve the classifier performanceconsiderably.
The best model obtained for LChas an F-measure of 56.9%.
In LNSN it im-proves marginally to 59.9%.
Adding back contextto lexical features improves it to 62.4% in LCSNwhereas addition of context to syntactic featuresfurther improves this to 64.0%.
We observed thatthe feature parentPOS has the most impact on in-creased context widths, among syntactic features.The improvement pattern of Precision and Re-call across the classes is also interesting.
Syntac-tic features with no context improve Recall by 4.8percentage points over only lexical features withcontext, whereas Precision improves only by 0.6points.
However, adding back context to lexicalfeatures further improves Precision by 4.9 pointswhile Recall just improves by 0.6 points.
Finally,adding context of syntactic features improves bothPrecision and Recall moderately.
We infer thatsyntactic features (without context) help identifymore annotatable patterns thereby improving Re-call, whereas linear context helps removing thewrong ones, thereby improving Precision.The per-category F-measure results presentedin Table 5 are also interesting.
The CB F-measureimproves by 8.1 points and NCB improves 18.9points from LC to LCSC .
But, the improvementin NA F-measure is only a marginal 1.3 pointsbetween LC and LCSC .
Furthermore, the F-measure decreases by 3.3 points when syntacticand lexical features with no context are used.
Onanalysis, we found that NAs often occur in syn-tactic structures like want to find or should go (de-ontic should), in which the relevant words occurin a small linear window.
In contrast, NCBs areoften signaled by deeper syntactic structures.
Forexample, in He said that his visit to the US willmainly focus on the humanitarian issues, a simpli-fied sentence from our training set, the verb focusis an NCB because it is in the scope of the report-ing verb said (specifically, it is its daughter).
Thiscould not be captured using the context becausesaid and focus are far apart in the sentence.
Buta correct parse tree gives focus as the daughter ofsaid.
So, a feature like haveReportingAncestorcould easily capture this.
It is also the case that theroot of a dependency parse tree would mostly bea CB.
This is captured by the feature parentPOShaving value ?nil?.
This property also cannot becaptured by lexical features alone.However, NCB performs much worse than theother two categories.
NCB is a class which occursrarely compared to CB and NA in our corpus.
Outof the 1, 357 propositions tagged, only 176 wereNCB.
We assume that this could be a main factorof its poor performance.We analyzed the performance across the folds.Fold-2 contains only 0.03% NCBs compared to1.89% on the rest of the folds.
Similarly, it con-tains 6.43% NAs compared to 3.82% across otherfolds.
However, our best performing model givesa Recall of 59.1% with a Precision of 69.7% (F-measure 64.0%) for Fold-2, which is as good asother folds.
Hence, we observe that our learnedmodel is robust under distributional variations.3.5.2 MALLET ResultsAs explained in Section 3.3.2, we exploredMALLET-CRF using two experimental condi-tions L and LS.
Table 4 presents the best per-forming feature sets for both classes.
These re-1019sults again show that syntactic features improvethe classifier performance considerably.
The bestmodel obtained for L class has an F-measure of49.6%, whereas addition of syntactic features im-proves this to 59.0%.
Both Precision and Recallare improved by 9.4 percentage points as well.However, MALLET-CRF?s performance wascomparatively worse than YAMCHA?s SVM.
Thebest model for MALLET (LS) obtained an F-measure of 59.0% which is 5.0 percentage pointsless than that of the best model for YAMCHA(LCSC).It is interesting to note that MALLET per-formed well on predicting NCB.
The highest NCBF-measure of MALLET - 40.6% is 6.5 percent-age points higher than the highest NCB F-measurefor YAMCHA.
However, corresponding CB andNA F-measures were 61.2% and 56.1% whichare much lower than YAMCHA?s performance forthese categories.Also, MALLET was more time efficient thanYAMCHA.
On an average, for our corpus sizeand feature sets, MALLET ran 3 times as fast asYAMCHA in a cross validation setup (i.e.
trainingand testing together).3.5.3 Joint Model vs Pipeline ModelAs discussed in Section 3.3.3, we set up apipeline model for the best performing configu-ration of LCSC class of YAMCHA experiments.The head prediction step of the pipeline obtainedan F-measure of 83.9% with Precision and Re-call of 86.7% and 81.2%, respectively, across all4 folds.
The 3-way classification step to classifythe belief of the identified head obtained an ac-curacy of 72.7% across all folds.
In the pipelinemodel, false positives and false negatives adds upfrom step 1 and step 2, where as only the truepositives of step 2 is considered as the true pos-itives overall.
In this way, the overall Precisionwas only 49.8% and Recall was 42.9% with an F-measure of 46.1% as shown in Table 4.
The resultsfor CB/NCB/NA separately are given in Table 5.The per-category best F-measure was decreasedby 14.4, 17.6 and 13.2 percentage points from theYAMCHA joint model for CB, NCB and NA, re-spectively.
The performance gap is big enough toconclude that our choice of joint model was right.4 Related WorkOur work falls in the rich tradition of modelingagents in terms of their cognitive states (for ex-ample, (Rao and Georgeff, 1991)) and relatingthis modeling to language use through extensionsto speech act theory (for example, (Perrault andAllen, 1980; Clark, 1996; Bunt, 2000)).
These no-tions have been particularly fruitful in the dialogcommunity, where dialog act tagging is a majortopic of research; to cite just one prominent ex-ample: (Stolcke et al, 2000).
A dialog act repre-sents the communicative intention of the speaker,and its recognition is crucial for the building ofdialog systems.
The specific contribution of thispaper is to investigate exactly how discourse par-ticipants signal their beliefs using language, andthe strength of their beliefs; this latter point is notusually included in dialog act tagging.This paper is not concerned with issues relatingto logics for belief representation or inferencingthat can be done on beliefs (for an overview, see(McArthur, 1988)), nor theories of automatic be-lief ascription (Wilks and Ballim, 1987).
For ex-ample, this paper is not concerned with determin-ing whether a belief in the requirement of p entailsthe belief in p; instead, we are only interested inwhether the writer wants the reader to understandwhether the writer holds a belief in the require-ment that p or in p directly.
This paper is also notconcerned with subjectivity (Wiebe et al, 2004),the nature of the proposition p (statement aboutinterior world or external world) is not of interest,only whether the writer wants the reader to believethe writer believes p. This paper is also not con-cerned with opinion and determining the polarity(or strength) of opinion (for example: (Somasun-daran et al, 2008)), which corresponds to the de-sire dimension.
Thus, this work is orthogonal tothe extensive literature on opinion classification.The work of (Saur??
and Pustejovsky, 2007;Saur??
and Pustejovsky, 2008) is, in many re-spects, very similar to ours.
They propose Fact-bank, which represents the factual interpretationas modality-polarity pairs, extracted from the ba-sic structural elements denoting factuality en-coded by Timebank.
Also, they attribute the factu-ality to specific sources within the text.
Our work1020is more limited in several ways: we currently onlymodel the writer?s beliefs; we do not express po-larity (we believe we can derive it from the syn-tax and lexicon); Saur??
and Pustejovsky (2008)ask their annotators to perform extensive linguis-tic transformations on the text to obtain a ?nor-malized?
representation of propositional content(we simply ask the annotators to make a judg-ment about the writer?s strength of belief withrespect to a given proposition, and expect to beable to extract representations of pure proposi-tional meaning independently); and finally, Saur?
?and Pustejovsky (2008) have a more fine-grainedrepresentation of non-committed belief.
While itis plausible to distinguish between more or lessfirm non-committed belief, we believe the crucialdistinction is between committed belief and non-committed belief.
Furthermore, Saur??
and Puste-jovsky (2008) group reported speech with non-belief statements (our NA), while we group themwith weak belief (our NCB).
The reason for ourdecision is that we wanted to keep NA as a cat-egory which contains no-one?s beliefs, as we as-sumed this is semantically more coherent.
Thecategory NCB thus covers beliefs which the writerdoes not hold firmly or has expressed no opinionon ?
which is different from propositions whichthe writer has clearly attributed to other cognitivestates (such as desire).
In principle, we believea 4-way distinction is the right approach, but ourNCB category is already the least frequent, andsplitting it would have resulted in two very rareclasses.
Another difference include the use of theword ?fact?
in the FactBank manual, which weavoid because we are interested in cognitive mod-eling; however, this is merely a terminological is-sue.Other related works explored belief systems inan inference scenario as opposed to an intentional-ity scenario.
In work by (Krestel et al, 2008), theauthors explore belief in the context of reportedspeech in news media: they track newspaper textlooking for elements indicating evidentiality.
Thisis different from our work, since we seek to makeexplicit the intention of the author or the speaker.5 Future WorkWe are exploring ways to utilize the FactBank an-notated corpus for our purpose, with the goal ofautomatically converting it to our annotation for-mat.
With the added data from FactBank, wehope to be able to split the NCB category intoWB (weak belief) and RS (reported speech).
Wewill also explore learning embedded belief attri-butions, as annotated in FactBank.We found that the per-sentence F-measurehas a small positive correlation with the length-normalized probability of the MICA derivation (ameasure of parse confidence).
In case of a badparse, syntax features add noise which in turn re-duces classifier performance.
We are planningto exploit this correlation in order to choose sen-tences for selective self-training.
Another direc-tion we are looking to extend this work is to em-ploy active learning to overcome the shortcom-ings of a small training set.
Also, we found fre-quent use of epistemic and deontic modals in ourdata.
Both types of modals have identical syntac-tic structure, but they receive very different anno-tations.
This is not easily captured in our system.We are exploring ways to handle this.We will release our Committed Belief Taggingtool as a standalone black-box tool.
We also in-tend to release the annotated corpus.6 AcknowledgmentsThis work is supported, in part, by the Johns Hop-kins Human Language Technology Center of Ex-cellence.
Any opinions, findings, and conclusionsor recommendations expressed in this material arethose of the authors and do not necessarily reflectthe views of the sponsor.
We thank Bonnie Dorr,Lori Levin and our other partners on the TTO8project.
We also thank several anonymous review-ers for their constructive feedback.ReferencesBangalore, Srinivas and Aravind Joshi.
1999.
Su-pertagging: An approach to almost parsing.
Com-putational Linguistics, 25(2):237?266.Bangalore, Srinivas, Pierre Boullier, Alexis Nasr,Owen Rambow, and Beno?
?t Sagot.
2009.
MICA:1021A probabilistic dependency parser based on tree in-sertion grammars.
In NAACL HLT 2009 (Short Pa-pers).Bratman, Michael E. 1999 [1987].
Intention, Plans,and Practical Reason.
CSLI Publications.Bunt, Harry.
2000.
Dialogue pragmatics and contextspecification.
In Bunt, Harry and William J. Black,editors, Abduction, Belief and Context in Dialogue,pages 81?150.Clark, Herbert H. 1996.
Using Language.
cup, Cam-bridge, England.Cohen, Philip R. and Hector J. Levesque.
1990.
Ratio-nal interaction as the basis for communication.
InPhilip Cohen, Jerry Morgan and James Allen, edi-tors, Intentions in Communication.
MIT Press.Diab, Mona T., Lori Levin, Teruko Mitamura, OwenRambow, Vinodkumar Prabhakaran, and WeiweiGuo.
2009.
Committed belief annotation and tag-ging.
In ACL-IJCNLP ?09: Proceedings of theThird Linguistic Annotation Workshop, pages 68?73, Morristown, NJ, USA.
Association for Compu-tational Linguistics.Grice, Herbert Paul.
1975.
Logic and conversation.
InCole, P. and J. Morgan, editors, Syntax and seman-tics, vol 3.
Academic Press, New York.Hovy, Eduard H. 1993.
Automated discourse gener-ation using discourse structure relations.
ArtificialIntelligence, 63:341?385.Krestel, Ralf, Sabine Bergler, and Rene?
Witte.
2008.Minding the Source: Automatic Tagging of Re-ported Speech in Newspaper Articles.
In (ELRA),European Language Resources Association, edi-tor, Proceedings of the Sixth International Lan-guage Resources and Evaluation (LREC 2008),Marrakech, Morocco, May 28?30.Kudo, Taku and Yuji Matsumoto.
2000.
Use of sup-port vector learning for chunk identification.
InProceedings of CoNLL-2000 and LLL-2000, pages142?144.Mann, William C. and Sandra A. Thompson.
1987.Rhetorical Structure Theory: A theory of text orga-nization.
Technical Report ISI/RS-87-190, ISI.McArthur, Gregory L. 1988.
Reasoning about knowl-edge and belief: a survey.
Computational Intelli-gence, 4:223?243.McCallum, Andrew Kachites.
2002.
Mal-let: A machine learning for language toolkit.http://mallet.cs.umass.edu.Moore, Johanna.
1994.
Participating in ExplanatoryDialogues.
MIT Press.Perrault, C. Raymond and James F. Allen.
1980.
Aplan-based analysis of indirect speech acts.
Compu-tational Linguistics, 6(3?4):167?182.Rao, Anand S. and Michael P. Georgeff.
1991.
Mod-eling rational agents within a BDI-architecture.
InAllen, James, Richard Fikes, and Erik Sandewall,editors, Proceedings of the 2nd International Con-ference on Principles of Knowledge Representationand Reasoning, pages 473?484.
Morgan Kaufmannpublishers Inc.: San Mateo, CA, USA.Saur?
?, Roser and James Pustejovsky.
2007.
Determin-ing Modality and Factuality for Textual Entailment.In First IEEE International Conference on SemanticComputing., Irvine, California.Saur?
?, Roser and James Pustejovsky.
2008.
FromStructure to Interpretation: A Double-layered An-notation for Event Factuality.
In Proceedings of the2nd Linguistic Annotation Workshop.
LREC 2008.Somasundaran, Swapna, Janyce Wiebe, and Josef Rup-penhofer.
2008.
Discourse level opinion interpre-tation.
In Proceedings of the 22nd InternationalConference on Computational Linguistics (Coling2008), pages 801?808, Manchester, UK, August.Coling 2008 Organizing Committee.Stolcke, Andreas, Klaus Ries, Noah Coccaro, Eliza-beth Shriberg, Rebecca Bates, Daniel Jurafsky, PaulTaylor, Rachel Martin, Carol Van Ess-Dykema, andMarie Meteer.
2000.
Dialogue act modeling forautomatic tagging and recognition of conversationalspeech.
Computational Linguistics, 26:339?373.Stone, Matthew.
2004.
Intention, interpretation andthe computational structure of language.
CognitiveScience, 24:781?809.Wiebe, Janyce, Theresa Wilson, Rebecca Bruce,Matthew Bell, and Melanie Martin.
2004.
Learningsubjective language.
In Computational Linguistics,Volume 30 (3).Wilks, Yorick and Afzal Ballim.
1987.
Multipleagents and the heuristic ascription of belief.
In Pro-ceedings of the 10th International Joint Conferenceon Artificial Intelligence (IJCAI), pages 118?124.1022
