Robust Parsing: More with LessKilian Foth, Wolfgang MenzelFachbereich Informatik, Universita?t Hamburg, Germanyfoth|menzel@informatik.uni-hamburg.deAbstractCovering as many phenomena as possible is atraditional goal of parser development, but thebroader a grammar is made, the blunter it maybecome, as rare constructions influence the be-haviour on simple sentences that were alreadysolved correctly.
We observe the effects of in-tentionally removing support for specific con-structions from a broad-coverage grammar ofGerman.
We show that accuracy of analysingsentences from the NEGRA corpus can be im-proved not only for sentences that do not needthe extra coverage, but even when includingthose that do.1 IntroductionTraditionally, broad coverage has always been consid-ered to be a desirable property of a grammar: the morelinguistic phenomena are treated properly by the gram-mar, the better results can be expected when applyingit to unrestricted text (c.f.
(Grover et al, 1993; Doranet al, 1994)).
With the advent of empirical methodsand the corresponding evaluationmetrics, however, thisview changed considerably.
(Abney, 1996) was amongthe first who noted that the relationship between cover-age and statistical parsing quality is a more complexone.
Adding new rules to the grammar, i.e.
increas-ing its coverage, does not only allow the parser to dealwith more phenomena, hence more sentences; at thesame time it opens up new possibilities for abusingthe newly introduced rules to mis-analyse constructionswhich were already treated properly before.
As a con-sequence, a net reduction in parsing quality might beobserved for simple statistical reasons, since the gainusually is obtained for relatively rare phenomena,whilethe adverse effects might well affect frequent ones.
(Abney, 1996) uses this observation to argue in favourof stochastic models which attempt to choose the opti-mal structural interpretation instead of only providinga list of equally probable alternatives.
However, usingsuch an optimization procedure is not necessarily a suf-ficient precondition to completely rule out the effect.Compared to traditional handwritten grammars, suc-cessful stochastic models like (Collins, 1999; Charniak,2000) open up an even greater space of alternatives forthe parser and accordingly offer a great deal of oppor-tunities to construct odd structural descriptions fromthem.
Whether the guidance of the stochastic modelcan really prevent the parser from making use of theseunwanted opportunities so far remains unclear.In the following we make a first attempt to quantify theconsequences that different degrees of coverage havefor the output quality of a wide-coverage parser.
Forthis purpose we use a Weighted Constraint DependencyGrammar (WCDG), which covers even relatively raresyntactic phenomena of German and performs reliablyacross a wide variety of different text genres (Foth etal., 2005).
By combining hand-written rules with anoptimization procedure for hypothesis selection, sucha parser makes it possible to successively exclude cer-tain rare phenomena from the coverage of the grammarand to study the impact of these modifications on itsoutput quality2 Some rare phenomena of GermanWhat are good candidates of ?rare?
phenomena thatmight be intentionally removed from the coverage ofour grammar?One possibility is to remove coverage forconstructions that are already slightly dispreferred.
Forinstance, apposition and coordination of noun phrasesoften violate the principle of projectivity:?I got a sled for Christmas, a parrot and a motor-bike.
?This is quite a common construction, but still ?rare?
inthe sense that the great majority of appositions does re-spect projectivity, so that the example seems at leastslightly unusual.
But there are also syntactic relationsthat are quite rare but nevertheless appear perfectly nor-mal when they do occur, such as direct appellations:?James, please open the door.
?This might be because their frequency varies consid-erably between text types; everyone is familiar withpersonal appellation from everyday conversation, butit would be surprising to hear it from the mouth of atelevision news reader.Finally, some constructions form variants e.g.
by omit-ting certain words:?I bought a new broom [in order] to clean the drive-25No.
Phenomenon Example f/10001 Mittelfeld extraposition ?Es strahlt u?ber DVB-T neben dem Fernsehprogramm auch seinen Dig-itext aus, einen Videotext-a?hnlichen Informationsdienst.
?32.52 ethical dative ?Noch erobere sich der PC neue Ka?uferschichten, hei?t es weiter.?
18.53 Nominalization ?Ta?glich kommen rund 1000 neue hinzu.?
13.44 Vocative ?So nicht, ICANN!?
9.75 Parenthetical matrixclause?Bis zum Jahresende 2002, prognostiziert Roland Berger, werdendie am Neuen Markt gelisteten Unternehmen 200.000 Mitarbeiterbescha?ftigen.
?8.86 verb-first subclause ?Erfu?llt ein Mitgliedstaat keines oder nur eines dieser Kriterien, soerstellt die Kommission einen Bericht.
?8.37 Headline phrase ?Lehrer kaum auf Computer vorbereitet?
3.98 coordination cluster ?Auf den Webseiten der Initiative ko?nnen Spender PCs anbieten undSchulen ihren Bedarf anmelden.
?3.19 Adverbial pronoun ?Ihre Sprachen sollen alle gleichberechtigt sein.?
2.610 um omission ?Und Dina ging aus, die To?chter des Landes zu sehen.?
2.111 Metagrammaticalusage?Die Bezugnahmen auf die gemeinsame Agrarpolitik oder auf die Land-wirtschaft und die Verwendung des Wortes ?landwirtschaftlich?
sindin dem Sinne zu verstehen, dass damit unter Beru?cksichtigung derbesonderen Merkmale des Fischereisektors auch die Fischerei gemeintist.
?1.812 Auxiliary flip ?Die Gescha?digten werfen Ricardo nun eine erhebliche Mitschuld vor,da gro?
?erer Schaden ha?tte verhindert werden ko?nnen, wenn der An-bieter sofort gesperrt worden wa?re.
?1.113 Adjectival subclause ?Die Union unterha?lt ferner, soweit zweckdienlich, Beziehungen zu an-deren internationalen Organisationen.
?0.914 Suffix drop ?Ein freundlich Wort, das Maslo intervenieren lie?:?
0.515 Elliptical genitive ?Martins war auch nicht besser.?
0.316 Adverbial noun ?Sie stehen sich Auge in Auge gegenu?ber.?
0.117 Verb/particle mismatch ?Au?er Windows 9x selbst ko?nnen auch andere Hard- und Soft-warekomponenten eines PC mit zu viel Hauptspeicher manchmal nichtzurecht.
?0.118 Vorfeld extraposition ?Der Verdacht liegt nahe, da?
hier Schwarzarbeit betrieben wird.?
0.119 double relative subject ?Ich bin der Herr, der ich dich aus ?Agyptenland herausgefu?hrt habe.?
0.0220 Relative subject clause ?Die dir fluchen, seien verflucht, und die dich segnen, seien gesegnet!?
0.0421 NP extraposition ?Die Verpflichtungen und die Zusammenarbeit in diesem Bereichbleiben im Einklang mit den im Rahmen der Nordatlantikvertrags-Organisation eingegangenen Verpflichtungen, die fu?r die ihrangeho?renden Staaten weiterhin das Fundament ihrer kollektivenVerteidigung und das Instrument fu?r deren Verwirklichung ist.
?0.01Table 1: Some rare phenomena in modern German.way.
?Here the longer variant is unambiguously a subclauseexpressing purpose, while the shorter might be mis-taken for a prepositional phrase, so it could be regardedas misleading for the parser.The selection is necessarily subjective, not only be-cause the delimitation of a phenomenon is subjective(are all kinds of ellipsis fundamentally the same phe-nomenon or not?)
but also because we can remove onlythose phenomena that are already covered in the firstplace.
Therefore we have selected phenomena?
that were explicitly added to the grammar at somepoint in order to deal with actually occurring un-foreseen constructions,?
that can easily be removed from the grammarwithout affecting other phenomena,?
and that are relatively rare in all the texts we haveinvestigated.Table 1 shows the 21 phenomena that we consider inthis paper.
(Note that the three earlier example sen-tences correspond to lines 1, 4, and 10 in this table, butthat not all lines have exact counterparts in English.
)The last column gives the overall frequency per 1,000sentences of each phenomenon when measured acrossall trees in our collection.The collection contains sections of Bible text (Genesis1?50), law text (the constitutions of Federal Germanyand of the EuropeanUnion), online technical newscasts(www.heise.de), novel text, and sentences from theNEGRA corpus of newspaper articles.
Table 2 showsthe sentence counts of the different sections and thefrequency per 1000 of all 21 phenomena in each texttype.
It can be seen that most of the constructions re-main quite rare overall, but often the frequency dependsheavily on the text type, so that a high influence of thecorpus can be expected for our experiments.26f /1000 Bible Law Online Novel News overallPhen.
(2,709) (3,722) (55,327) (20,253) (4,000) (86,011)1 93.6 24.6 29.0 36.7 28.2 32.62 59.6 17.5 12.2 31.3 16.2 18.63 21.0 22.7 12.3 12.4 19.5 13.44 18.4 0.0 0.1 38.2 1.2 9.75 1.1 0.0 5.8 18.2 15.8 8.86 3.4 51.4 7.8 2.6 6.8 8.37 0.7 3.6 4.8 1.3 7.2 3.98 7.1 4.4 3.3 2.4 1.8 3.19 7.1 0.5 1.6 5.0 3.5 2.610 12.7 1.9 1.9 1.2 1.2 2.011 0.4 0.3 2.2 0.5 4.8 1.812 1.5 0.0 0.9 1.8 1.5 1.113 2.2 0.8 1.0 0.5 0.2 0.914 0.7 0.0 0.6 1.2 0.2 0.715 1.9 0.0 0.7 0.0 1.0 0.516 0.4 0.3 0.2 0.0 0.0 0.117 1.1 0.0 0.1 0.0 0.0 0.118 0.0 0.0 0.1 0.0 0.2 0.119 0.7 0.0 0.0 0.0 0.2 0.020 0.7 0.0 0.0 0.0 0.0 0.021 0.0 0.3 0.0 0.0 0.0 0.0Table 2: Frequency of phenomena by text type.3 Weighted Constraint DependencyGrammarIn WCDG (Schro?der, 2002), natural language is mod-elled as labelled dependency trees, in which each wordis assigned exactly one other word as its regent (onlythe root of the syntax tree remains unsubordinated)and a label that describes the nature of their relation.The set of acceptable trees is defined not by way ofgenerative rules, but only through constraints on well-formed structures.
Every possible dependency tree isconsidered correct unless one of its edges or edge pairsviolates a constraint.
This permissiveness extends tomany properties that other grammar formalisms con-sider non-negotiable; for instance, a WCDG can allownon-projective (or, indeed, cyclical) dependencies sim-ply by not forbidding them.
Since the constraints canbe arbitrary logical formulas, a grammar rule can alsoallow some types of non-projective relations and for-bid others, and in fact the grammar in question doesprecisely that.Weighted constraints can be written to express the factthat a construction is considered acceptable but notfully so.
This mechanism is used extensively to achieverobustness against proper errors such as wrong inflec-tion, ellipsis or mis-ordering; all of these are in fact ex-pressed through defeasible constraints.
But it can alsoexpress more subtle dispreferences against a specificphenomenonby writing only a weak constraint that for-bids it; most of the phenomena listed in Table 1 are as-sociated with such constraints to ensure that the parserassumes a rare construction only when this is neces-sary.We employ a previously existing wide-coverageWCDG of modern German (Foth et al, 2005)that covers all of the presented rare phenom-ena.
It comprises about 1,000 constraints, 370of which are hard constraints.
The entire parserand the grammar of German are publicly avail-able at http://nats-www.informatik.uni-hamburg.de/Papa/PapaDownloads.The optimal structure could be defined as the tree thatviolates the least important constraint (as in OptimalityTheory), or the tree that violates the fewest constraints;in fact a multiplicative measure is used that combinesboth aspects byminimizing the collective dispreferencefor all phenomena in a sentence.
Unfortunately, the re-sulting combinatorial problem isNP-complete and ad-mits of no efficient exact solution algorithm.
However,variants of a heuristic local search can be used, whichtry to find the optimal tree by constructing a completetree and then changing it in those places that violate im-portant constraints.
This involves a trade-off betweenparsing accuracy and processing time, because the cor-rect structure is more likely to be found if there is moretime to try out more alternatives.
Given enough time,the method works well enough that the overall systemexhibits a competitive accuracy even though the theo-retical accuracy of the languagemodel may be compro-mised by search errors.As an example of the process, consider the followinganalysis of the German proverb ?Wer anderen eineGrube gra?bt, fa?llt selbst hinein.?
(He who digs a holefor others, will fall into it himself.)
The transformationstarts with the following initial assumptionAVZADVSSOBJADETETHSUBJwer anderen eine Grube gr?bt , f?llt selbst hinein .global score: 0.000001892which, besides producing two isolated fragmentsinstead of a spanning tree, also lacks a subject for thesecond clause.AVZADVSSOBJADETETHSUBJwer anderen eine Grube gr?bt , f?llt selbst hinein .global score: 0.0001888To mend this problem the relative pronoun from thefirst clause has been taken as a subject for the secondone, with the result that the conflict has simply beenmoved to the first part of the sentence.
Nevertheless,the global score improved considerably, since theverb-second condition for German main clauses isviolated less often.AVZADVSSOBJADETSUBJSUBJwer anderen eine Grube gr?bt , f?llt selbst hinein .global score: 0.0004871Here, the indefinite plural pronoun ?anderen?
is takenas the subject for the second clause, creating, however,an agreement error with the finite verb, which issingular.
Both subclauses have still not been integratedinto a single spanning tree.27AVZADVKONSAPPDETOBJASUBJwer anderen eine Grube gr?bt , f?llt selbst hinein .global score: 0.002566The integration is then achieved, but unfortunatelyas a coordination without an appropriate conjunctionbeing available.
Moreover there is a problem with thehypothesized main clause, since it again does not obeythe verb-second condition of German.AVZADVKONRELAPPDETOBJASUBJwer anderen eine Grube gr?bt , f?llt selbst hinein .global score: 0.1026Therefore the interpretation is changed to a relativeclause, which however cannot appear in isolation.The valency requirements of the verb ?gra?bt?
aresatisfied by taking the indefinite pronoun ?anderen?
asa direct object with the true object (?eine Grube?)
as a(mal-formed) apposition.AVZADVSSUBJCAPPDETOBJASUBJwer anderen eine Grube gr?bt , f?llt selbst hinein .global score: 0.5502Finally, the analysis switches to an interpretationwhich accepts the second part of the sentence as themain clause and subordinates the first part as a subjectclause.
The problem with the apposition readingpersists.AVZADVSSUBJCOBJADETETHSUBJwer anderen eine Grube gr?bt , f?llt selbst hinein .global score: 0.7249By interpreting the indefinite pronoun as an ethical da-tive, the direct object valence is freed for the NP ?eineGrube?.
Although this structure still violates some con-straints (e.g.
the ethical dative is slightly penalized forbeing somewhat unusual) a better one cannot be found.Note that the algorithm does not take the shortest pos-sible transformation sequence; in fact, the first analysiscould have been transformed directly into the last byonly one exchange.
Because the algorithm is greedy, itchooses a different repair at that point, but it still findsthe solution in about three seconds on a 3 GHz Pentiummachine.In contrast to stochastic parsing approaches, a WCDGcan be modified in a specifically targeted manner.
Ittherefore provides us with a grammar formalism whichis particularly well suited to precisely measure the con-tributions of different linguistic knowledge sources tothe overall parsing quality.
In particular it allows us to1.
switch off constraints, i.e.
increase the space of ac-ceptable constructions and/or syntactic structures,2.
weaken constraints, by changing the weight in away that it makes the violation of the constraintcondition more easily acceptable,3.
introduce additional dependency labels into themodel,4.
remove existing dependency labels from themodel5.
reinforce constraints, by removing guards for ex-ceptional cases from them,6.
reinforce constraints, by strengthening theirweights or making the constraint non-defeasiblein the extreme case, and7.
introducing new constraints, to prohibit certainconstructions and/or syntactic structures.Since for the purpose of our experiments, we startwith a fairly broad-coverage grammar of German, fromwhich certain rare phenomenawill be removed, options4 to 7 are most important for us.4 Robust behaviour under limitedcoverageIn general, it is not easy to predict the possible outcomeof a parsing run when using a grammar with a reducedcoverage.
Whether a sentence can be analysed at allsolely depends on the available alternatives for struc-turing it.
Which structural description it can receive,however, is influenced by the scores resulting fromrule applications or constraint violations.
Moreover,the transformation-based solution method used for theWCDG-experiments introduces yet another condition:since it is based on a limited heuristics for candidategeneration, the grammar must license not only the fi-nal parsing result for a sentence, but also all the inter-mediate transformation steps with a sufficiently highscore.
This might exclude some structural interpreta-tions from being considered at all if the grammar is nottolerant enough to accommodate highly deviant struc-tures.28Thus, the ability to deal with extragrammatical inputin a robust manner is a crucial property if we are go-ing to use a grammar with coverage limitations.
Un-fortunately, robust behaviour is usually achieved by ex-tending instead of reducing the coverage of the modeland compensating the resulting increase in ambiguityby an appropriately designed scoring scheme togetherwith an optimization procedure.To deal with these opposing tendencies, it is obviouslyimportant to determine which parts of the model needto be relaxed to achieve a sufficient degree of robust-ness, and which ones can be reinforced to limit thespace of alternatives in a sensible way.
Excluding phe-nomena from the grammar which never occur in a cor-pus should always give an advantage, since this reducesthe number of alternatives to consider at each step with-out forbidding any of the correct ones.On the other hand, removing support for a construc-tion that is actually needed forces the parser to choosean incorrect solution for at least some part of a sen-tence, so that a deterioration might occur instead.
Buteven if coverage is reduced below the strictly necessaryamount, a net gain in accuracy could occur for two rea-sons:1.
Leaking: The grammar overgenerates the con-struction in question, so that forbidding it preventserrors occurring on ?normal?
sentences.2.
Focussing: Due to a more restricted search space,the parser is not led astray by rare hypotheses, thussaving processing time which can be used to comecloser to the optimum.4.1 Experiment 1: More with lessIn our first experiment, we analysed 10,000 sentencesof online newscast texts both with the normal grammarand with the 21 rare phenomena explicitly excluded.
Asusual for dependency parsers, we measure the parsingquality by computing the structural accuracy (the ratioof correct subordinations to all subordinations) and la-belled accuracy (the ratio of all correct subordinationsthat also bear the correct label to all subordinations).Note that the WCDG parser always establishes exactlyone subordination for each word of a sentence, so thatno distinction between precision and recall arises.
Also,the grammar is written in such a way that even if anecessary phenomenon is removed, the parser will atleast find some analysis, so that the coverage is always100%.As expected, those ?rare?
sentences in which at leastone of these constructions does actually occur are an-alyzed less accurately than before: structural and la-belled accuracy drop by about 2 percent points (seeTable 3).
However, the other sentences receive slightlybetter analyses, and since they are in the great majority,the overall effect is an increase in parsing quality.
Notealso that the ?rare?
sentences appear to be more difficultto analyze in the first place.Grammar: Normal ReducedOnline newscastsrare (717) 87.6%/85.2% 85.8%/85.8%normal (9,283) 91.0%/89.8% 91.4%/90.4%overall (10,000) 91.0%/89.4% 91.3%/89.7%NEGRA corpusrare (91) 85.5%/83.7% 84.0%/81.4%normal (909) 91.2%/89.3% 91.5%/89.7%overall (1,000) 90.5%/88.6% 90.6%/88.7%Table 3: Structural and labelled accuracy when parsingthe same text with reduced coverage.The net gain in accuracy might be due to plugged leaks(misleading structures that used to be found are rejectedin favor of correct structures) or to focussing (structuresthat were preferred but missed through search errors arenow found).
A point in case of the latter explanationis the fact that the average runtime decreases by 10%with the reduced grammar.
Also, if we consider onlythose sentences on which the local search originallyexceeded the time limit of 500 s and therefore had tobe interrupted, the accuracy rises from 85.2%/83.0% to86.5%/84.4%, i.e.
even more pronounced than overall.4.2 Experiment 2: Stepwise refinementFor comparison with previous work and to investi-gate corpus-specific effects, we repeated the experi-ment with the test set of the NEGRA corpus as definedby (Dubey and Keller, 2003).
For that purpose the NE-GRA annotations were automatically transformed todependency trees with the freely available tool DEPSY(Daum et al, 2004).
Some manual corrections weremade to its output to conform to the annotation guide-lines of the WCDG of German; altogether, 1% of allwords had their regents changed for this purpose.Table 3 shows that the proportion of sentences with rarephenomena is somewhat higher in the NEGRA sen-tences, and consequently the net gain in parsing accu-racy is smaller; apparently the advantage of reducingthe problem size is almost cancelled by the disadvan-tage of losing necessary coverage.To test this theory, we then reduced the coverage of thegrammar in smaller steps.
Since constraints allow us toswitch off each of the 21 rare phenomena individually,we can test whether the effects of reducing coverageare merely due to the smaller number of alternativesto consider or whether some constructions affect theparser more than others, if allowed.We first took the first 3,000 sentences of the NEGRAcorpus as a training set and counted how often eachconstruction actually occurs there and in the test set.Table 4 shows that the two parts of the corpus, whiledifferent, seem similar enough that statistics obtained29Frequency per 1000 onNr Phenomenon training set test set1 Mittelfeld extraposition 33.3 13.02 ethical dative 16.7 15.03 Nominalization 20.3 17.04 Vocative 1.0 2.05 Parenthetical matrix clause 13.3 23.06 verb-first subclause 8.0 3.07 Headline phrase 6.7 9.08 coordination cluster 1.7 2.09 Adverbial pronoun 4.0 2.010 um omission 1.3 1.011 Metagrammatical usage 5.7 2.012 Auxiliary flip 2.0 0.013 Adjectival subclause 0.0 1.014 Suffix drop 1.0 1.015 Elliptical genitive 0.0 1.016 Adverbial noun 0.0 0.017 Verb/particle mismatch 0.0 0.018 Vorfeld extraposition 0.0 1.019 double relative subject 0.0 0.020 Relative subject clause 0.3 0.021 NP extraposition 0.0 0.0Table 4: Comparison of training and test set.on the one could be useful for processing the other.The test set was then parsed again with the coveragesuccessively reduced in several steps: first, all construc-tions were removed that never occur in the training set,then those which occur less than 10 times or 100 timesrespectively were also removed.
We also performedthe opposite experiment, first removing support for theleast rare phenomena and only then for the really rareones.Phenomena structural labelledremoved accuracy accuracynone 90.5% 88.6%= 0 90.5% 88.7%< 10 90.6% 88.8%< 100 90.7% 88.6%>= 100 90.5% 88.6%>= 10 90.4% 88.5%> 0 90.5% 88.6%all 90.6% 88.7%Table 5: Parsing with coverage reduced stepwise.Table 5 shows the results of parsing the test set in thisway (the first and last lines are repetitions from Ta-ble 3).
The resulting effects are very small, but they dosuggest that removing coverage for the very rare con-structions is somewhat more profitable: the first threenew experiments tend to yield better accuracy than theoriginal grammar, while in the last three it tends todrop.4.3 Experiment 3: Plugging known leaksThe previous experiment used only counts from thetreebank annotations to determine how rare a phe-nomenon is supposed to be, but it might also be im-portant how rare the parser actually assumes it to be.The fact that a particular construction never occurs in acorpus does not prevent the parser from using it in itsanalyses, perhaps more often than another constructionthat is much more common in the annotations.
In otherwords, we should measure how much each construc-tion actually leaks.
To this end, we parsed the trainingset with the original grammar and grouped all 21 phe-nomena into three classes:A: Phenomena that are predicted much more oftenthan they are annotatedB: Phenomena that are predicted roughly the rightnumber of timesC: Phenomena that are predicted less often than an-notated (or in fact not at all).
?Much more often?
here means ?by a factor of two ormore?
; constructions which were never predicted or an-notated at all were grouped into class C.There are different reasons why a phenomenon mightleak more or less.
Some constructions depend on par-ticular combinations of word forms in the input; forinstance, an auxiliary flip can only be predicted whenthe finite verb does in fact precede the full verb (phe-nomenon 12 in Table 1), so that covering it should notchange the behaviour of the system much.
But mostsentences contain more than one noun phrase which theparser might possibly misrepresent as a non-projectiveextraposition (phenomenon 1).
Also, some rare phe-nomena are dispreferred more than others even whenthey are allowed.
We did not investigate these reasonsin detail.Phenomena structural labelledremoved accuracy accuracynone 90.5% 88.6%A (1,3,4,6?10,13,16,18?21) 90.9% 89.0%B (2,5,11,12) 90.4% 88.5%C (14,15,17) 90.4% 88.6%1?21 90.6% 88.7%Table 6: Parsing with coverage reduced by increasingleakage.Table 6 shows an interesting asymmetry: of our 21 con-structions, 14 regularly leak into sentences where theyhave no place, while 4 work more or less as designed.Only 3 are predicted too seldom.
This is consistent withour earlier interpretation that most added coverage is infact unhelpful when judging a parser solely by its em-pirical accuracy on a corpus.30Accordingly, it is in fact more helpful to judge con-structions by their observed tendency to leak than justby their annotated frequency: the first experiment (A)yields the highest accuracy for the newspaper text.Conversely, removing those constructions which actu-ally work largely as intended (B) reduces even the over-all accuracy, and not just the accuracy on ?rare?
sen-tences.
The third class contains only three very rarephenomena, and removing them from the grammardoes not influence parsing very much at all.Note that this result was obtained although the distribu-tion of the phenomena differs between parser predic-tions on the training set and the test set; had we clas-sified them according to their behaviour on the test setitself, the class A would have contained only 9 items (ofwhich 7 overlap with the classification actually used).5 Related workThe fact that leaking is an ubiquitous property of natu-ral language grammars has been noted as early as 80years ago by (Sapir, 1921).
Since no precise defini-tion was given, the notion offers room for interpreta-tion.
In general linguistics, leaking is usually under-stood as the underlying reason for the apparent im-possibility to write a grammar which is complete, inthe sense that it covers all sentences of a language,while maintaining a precise distinction between correctan incorrect word form sequences (see e.g.
(Sampson,forthcoming)).
In Computational Linguistics, attentionwas first drawn to the resulting consequences for ob-taining parse trees when it became obvious that all at-tempts to build wide-coverage grammars led to an in-crease in output ambiguity, and that even more fine-grained feature-based descriptions were not able solvethe problem.
Stochastic approaches are usually consid-ered to provide a powerful countermeasure (Manningand Schu?tze, 1999).
However, as (Steedman, 2004) al-ready noted, stochastic models do not address the prob-lem of overgeneration directly.Disregarding rare phenomena is something that can beachieved in a stochastic framework by putting a thresh-old on the minimum number of occurrences to be con-sidered.
Such an approach is mainly used to either ex-clude rare phenomena in grammar induction (c.f.
(Sol-sona et al, 2002)) or to prune the search space by ad-justing a beam width during parsing itself (Goodman,1997).
The direct use of thresholding techniques at thelevel of the stochastic model, however, has not been in-vestigated extensively so far.
Stochastic models of syn-tax suffer to such a degree from data sparseness that ineffect strong efforts in the opposite direction becomenecessary: instead of ignoring rare events in the train-ing data, even unseen events are included by smoothingtechniques.
The only experimental investigation of theimpact of rare events we are aware of is (Bod, 2003),where heuristics are explored to constrain the modelin the DOP framework by ignoring certain tree frag-ments.
Contrary to the results of our experiments, veryfew constraints have been found that do not decreasethe parse accuracy.
In particular, no improvement bydisregarding selected observations was possible.The tradeoff between processing time and output qual-ity which our transformation-based problem solvingstrategy exhibits, is also a fundamental property of allbeam-search procedures.
While a limited beam widthmight cause search errors, widening the beam in or-der to improve the quality requires investing more com-putational resources (see e.g.
(Collins, 1999)).
In con-trast to our transformation-based procedure, however,the commonly used Viterbi search is not interruptibleand therefore not in a position to really profit from thetradeoff.
Thus, focussing as a possibility to increaseoutput quality to our knowledge has never been inves-tigated elsewhere.6 Conclusions and future workWe have investigated the effect of systematically reduc-ing the coverage of a general grammar of German.
Byremoving support for 21 rare phenomena, the overallparsing accuracy could be improved.We confirmed theinitial assumption about the effects that broad cover-age has on the parser: while it allows some special sen-tences to be analysed more accurately, it also causesa slight decrease on the much more numerous normalsentences.This result shows that at least with respect to this par-ticular grammar, more coverage can indeed lead to lessparsing accuracy.
In the first experiment we measuredthe overall loss through adding coverage where it is notneeded as about 0.4% of structural accuracy on news-cast text, and 0.1% on NEGRA sentences.
This fig-ure can be interpreted as the result of overgeneratingor ?leaking?
of rare constructions into sentences wherethey are not wanted.Although we found that it makes little differencewhether to remove support for very rare or for some-what rare phenomena, judging constructions by howmany leaks they actually cause leads to a greater im-provement.
On the NEGRA test set, removing the?known troublemakers?
leads to a greater increase of inaccuracy of 0.4%, reducing the error rate for structuralattachment by 4.2%.Of course, removing rare phenomena is not a viabletechnique to substantially improve parser accuracy, ifonly for the simple fact that it does not scale up.
How-ever, it confirms that as soon as a certain level of cov-erage has been reached, robustness, i.e.
the ability todeal with unexpected data, is more crucial than cover-age itself to achieve high quality results on unrestrictedinput.On the other hand, the improvement we obtained is not31very large, compared to the already rather high over-all performance of the parser.
This may be due to theconsistent use of weighted constraints in the originalgrammar, which slightly disprefer many of the 21 phe-nomena even when they are allowed, and we assumethat the original grammar is already reasonably effec-tive at preventing leaks.
This claim might be confirmedby reversing the experiment: if all phenomena were al-lowed and all dispreferences switched off, we wouldexpect even more leaks to occur.To carry out comparable experiments on generativestochastic models presents us with the difficulty thatit would first be necessary to determine which of itsparameters are responsible for covering a specific phe-nomenon, and whether they can be modified as to re-move the construction from the coverage without af-fecting others as well.
Even in WCDG it is difficultto quantify how much of the observed improvementresults from plugged leaks, and how much from fo-cussing.
This could only be done by observing all in-termediate steps in the solution algorithm, and countinghow many trees that were used as intermediate resultsor considered as alternatives exhibit each phenomenon.The most promising result from the last experiment isthat it is possible to detect particularly detracting phe-nomena, which are prime candidates for exclusion, inone part of a corpus and use them on another.
This sug-gests itself to be exploited as a method to automaticallyadapt a broad-coverage grammar more closely to thecharacteristics of a particular corpus.ReferencesSteven Abney.
1996.
Statistical Methods and Lin-guistics.
In Judith Klavans and Philip Resnik, editors,The Balancing Act: Combining Symbolic and Statis-tical Approaches to Language, pages 1?26.
The MITPress, Cambridge, Massachusetts.Rens Bod.
2003.
Do all fragments count?
NaturalLanguage Engineering, 9(4):307?323.Eugene Charniak.
2000.
A Maximum-Entropy-Inspired Parser.
In Proc.
1st Meeting of the NorthAmerican Chapter of the ACL, NAACL-2000, Seattle,WA.Michael Collins.
1999.
Head-Driven Statistical Mod-els for Natural Language Parsing.
PhD thesis, Univer-sity of Pennsylvania, Philadephia, PA.Michael Daum, Kilian Foth, and Wolfgang Menzel.2004.
Automatic transformation of phrase treebanks todependency trees.
In Proc.
4th Int.
Conf.
on LanguageResources and Evaluation, pages 99?106, Lisbon, Por-tugal.Christy Doran, Dania Egedi, Beth Ann Hockey,B.
Srinivas, and Martin Zaidel.
1994.
XTAG sys-tem - A Wide Coverage Grammar for English.
InProc.
15th Int.
Conf.
on Computational Linguistics,COLING-1994, pages 922 ?
928, Kyoto, Japan.Amit Dubey and Frank Keller.
2003.
ProbabilisticParsing for German using Sister-Head Dependencies.In Proc.
41st Annual Meeting of the Association ofComputational Linguistics, ACL-2003, Sapporo, Japan.Kilian Foth, Michael Daum, and Wolfgang Menzel.2005.
Parsing unrestricted German text with defeasibleconstraints.
In H. Christiansen, P. R. Skadhauge, andJ.
Villadsen, editors, Constraint Solving and LanguageProcessing, volume 3438 of Lecture Notes in ArtificialIntelligence, pages 88?101, Berlin.
Springer-Verlag.Joshua Goodman.
1997.
Global thresholding andmultiple-pass parsing.
In Proc.
2nd Int.
Conf.
on Em-prical Methods in NLP, EMNLP-1997, Boston, MA.C.
Grover, J. Carroll, and E. Briscoe.
1993.
The Alveynatural language tools grammar (4th release).
Tech-nical Report 284, Computer Laboratory, University ofCambridge.Christopher D. Manning and Hinrich Schu?tze.
1999.Foundations of Natural Language Processing.
MITPress, Cambridge etc.Geoffrey Sampson.
forthcoming.
Grammar with-out grammaticality.
Corpus Linguistics and LinguisticTheory.Edward Sapir.
1921.
Language: An Introduction to theStudy of Speech.
Harcourt Brace, New York.Ingo Schro?der.
2002.
Natural Language Parsing withGraded Constraints.
Ph.D. thesis, Department of In-formatics, Hamburg University, Hamburg, Germany.Roger Argiles Solsona, Eric Fosler-Lussier, Hong-Kwang J. Kuo, Alexandros Potamianos, and Imed Zi-touni.
2002.
Adaptive language models for spokenddialogue systems.
In Proc.
Int.
Conf.
on Acoustics,Seech, and Signal Processing, ICASSP-2002, Orlando,FL.Mark Steedman.
2004.
Wide Coverage Pars-ing with Combinatory Grammars.
Slides of aseminar presentation, Melbourne University, Aus-tralia.
http://www.cs.mu.oz.au/research/lt/seminars/steedman.pdf.
Last time visited:2006-01-06.32
