A State-Transition Grammar for Data-Oriented ParsingDav id  Tugwell*Cent re  for Cogn i t ive  Science,  Un ivers i ty  of  Ed inburgh2, Bucc leuch  P lace,  Ed inburgh  EH8 9LW,  Scot landdav id t~cogsc i .ed .ac .ukAbstractThis paper presents a grammar formalism de-signed for use in data-oriented approaches to lan-guage processing.
It goes on to investigate waysin which a corpus pre-parsed with this formalismmay be processed to provide a probabilistic lan-guage model for use in the parsing of fresh texts.Introduct ionRecent years have seen a resurgence of interestin probabilistic techniques for automatic languageanalysis.
In particular, there has arisen a dis-tinct paradigm of processing on the basis of pre-analyzed data which has taken the name Data-Or iented  Pars ing.
"Data Oriented Parsing (DOP) is a modelwhere no abstract rules, but language xperi-ences in the form of an analyzed corpus, con-stitute the basis for language processing."
1There is not space here to present full justification 1.for adopting such an approach or to detail the ad-vantages that it offers.
The main claim it makes isthat effective language processing requires a con-sideration of both the structural and statistical as- 2.pects of language, whereas traditional competencegrammars rely only on the former, and standardstatistical techniques such as n-gram models onlyon the latter.
DOP attempts to combine these twotraditions and produce "performance grammars",which:"... should not only contain informationon the structural possibilities of the general 3.language system, but also on details of actuallanguage use in a language community... ''2*This research was funded by a research studentshipfrom the ESRC.
My thanks also for discussion and com-ments to Matt Crocker, Chris Brew, David Milward andAnna Babarczy.1Bod, 1992.
4.2ibid.This approach entails however that a corpus hasfirst to be pre-analyzed (ie.
hand-parsed), and thequestion immediately arises as to the formalismto be used for this.
There is no lack of compet-ing competence grammars available, but also noreason to expect that such grammars hould besuited to a DOP approach, designed as they wereto characterize the nature of linguistic ompetencerather than performance.The next section sets out some of the propertiesthat we might require from such a "performancegrammar" and offers a formalism which attemptsto satisfy these requirements.A Formalism for DOPGiven that we are attempting to construct a for-malism that will do justice to both the statisticaland structural aspects of language, the featuresthat we would wish to maximize will include thefollowing:The formalism should be easy to use with prob-abilistic processing techniques, ideally having aclose correspondence to a simple probabilisticmodel such as a Markov process.The formalism should be fine-grained, ie.
re-sponsive to the behaviour of individual words(as n-gram models are).
This suggests a radi-cally lexieal ist approach (cf.
Karttunen, 1990)in which all rules are encoded in the lexicon,there being no phrase structure rules which donot introduce lexical items.It should be capable of capturing fully the lin-guistic intuitions of language users.
In otherwords, using the formalism one should be ableto characterize the structural regularities of lan-guage with at least the sophistication ofmoderncompetence grammars.As it is to be used with real data, the formalismshould be able to characterize the wide range272of syntactic structures found in actual anguageuse, including those normally excluded by com-petence grammars as belonging to the "pe-riphery" of the language or as being "ungram-matical".
Ideally every interpretable utteranceshould have one and only one analysis for anyinterpretation of it.Considering the first of these points, namely aclose relation to a simple probabilistic model, agood place to start the search might be with aright-branching f in l te-state grammar .
In thisclass of grammars every rule has the form A -4 aB (A,B E {non-terminals}, a E {terminals}) andall trees have the simple structure :a AA- -  B - -  C- -  D - -  b BI I I I Or: c ca b c d d D(with an equivalent vertical alignment, henceforthto be used in this paper, on the right)In probabilistic terms, a finite-state grammar cor-responds to a first-order Markov process, wheregiven a sequence of states Si, Sj,... drawn froma finite set of possible states {So,..,S=} the prob-ability of a particular state occurring dependssolely on the identity of the previous state.
Inthe finite-state grammar each word is associatedwith a transition between two categories, in thetree above 'a'  with the transition A -4 B and soon.
To calculate the probability that a string ofwords x l ,  x2, x3,.., xn has the parse representedby the string of category-states 81, $2, S3,...S=,we simply take the product of the probability ofeach transition: ie.
l-h~l P(xi : si -4 si+l).In addition to satisfying our first criterion, a finite-state grammar also fulfills the requirement thatthe formalism be radically lexicalist, as by defini-tion every rule introduces a lexical item.Account ing  for L ingu is t i c  S t ructureIf a finite-state grammar is chosen however, thethird criterion, that of linguistic adequacy, seemsto present an insurmountable stumbling block.How can such a simple formalism, in which syntaxis reduced to a string of category-states, hope tocapture even the basic hierarchical structure, thefamiliar "tree structure", of linguistic expressions?Indeed, if the non-terminals are viewed as atomiccategories then there is no way this can be done.
Ifhowever, in line with most current theories, cat-egories are taken to be bundles of features andcrucially if one of these featflres has the value of astack of  categor ies,  then this hierarchical struc-ture can indeed be represented.Using the notation A \[B\] to represent a state ofbasic category A carrying a category B on itsstack, the hierarchical structure of the sentence:(1) The man gave the dog a bone.can be represented as:The S \[\]man N \[VP\]gave VP \[ \](la) the NP \[NP\]dog N \[NP\]a NP \[\]bone N \[\]Intuitively, syntactic links between non-adjacentwords, impossible in a standard finite-state gram-mar, are here established by passing categoriesalong on the stack "through" the state of inter-vening words.
That such a formalism can fullycapture basic linguistic structures i confirmed bythe proof in Aho (1968) that an indexed gram-mar  (ie.
one where categories are supplementedwith a stack of unbounded length, as above), ifrestricted to right linear trees (also as above), isequivalent to a context - f ree  grammar .A perusal of the state transitions associated withindividual words in (la) reveals an obvious re-lationship to the "types" of categorial grammar.Using a to represent a list of categories (possiblynull), we arrive at the following transitions (withtheir corresponding categorial types alongside).The ditransitive verb 'gave' isVP  \[a\]3 _+ NP  \[NP,a\] (VP /NP) /NPDeterminers in complement position are both:NP  \[a\] -4 N \[a\] NP/NDeterminer in subject position is 'type-raised '4 to:S \[a\] -4 N \[VP,a\] (S/VP)/NThe common nouns are all:N \[a\] -4 a NIn fact as no intermediate constituents are formedin the analysis, an even closer parallel is to a de-pendency syntax where only rightward pointingarrows are allowed, of which the formalism as pre-sented above is a notational variant.
This lack of3 "vP" is used here and henceforth as a shorthand foran S with a missing (ie.
"slashed") subject.4The unidirectionality of the formalism results in anautomatic type-raising of all categories appearing beforetheir heads.273intermediate constituents has the added benefitthat no "spurious ambiguities" can arise.Knowing now that the addition of a stack-valuedfeature suffices to capture the basic hierarchi-cal structure of language, additional features canbe used to deal with other syntactic relations.For example, following the example of GPSG,unbounded ependencies can be captured using"slashed" categories.
If we represent a "slashed"category X with the lower case x, and use the no-tation A(b) for a category A carrying a featureb, then the topicalized sentence:(2) This bone the man gave the puppy.will have the analysis:(2a)This S \[1bone N \[S(np)\]the S(np) \[lman N \[VP(np)\]gave VP(np)\[ \]the NP \[\]puppy N \[\]Although there is no space in this paper to gointo greater detail, further constructions involvingunbounded ependency and complement controlphenomena can be captured in similar ways.CoverageThe criterion that remains to be satisfied is thatof width of coverage: can the formalism copewith the many "peripheral" structures found inreal written and spoken texts?
As it stands theformalism is weakly equivalent to a context-freegrammar and as such will have problems dealingwith phenomena like discontinuous constituents,non-constituent coordination and gapping.
For-tunately if extensions are made to the formalism,necessarily taking it outside weak equivalence toa context-free grammar, natural and general anal-yses present themselves for such constructions.Two of these will now be sketched.Discontinuous ConstituentsConsider the pair of sentences (3) and (4), iden-tical in interpretation, but the latter containing adiscontinuous noun phrase and the former not:(3) I saw a dog which had no nose yesterday.
(4) I saw a dog yesterday which had no nose.which have the respective analyses:I S \[\]saw VP \[\]a NP \[NP(t)\]dog N \[NP(t)\](3a) which S(rel) \[NP(t)\]had VP \[NP(t)\]no NP \[NP(t)\]nose N \[NP(t)\]yesterday NP (t) \[ \]i s \[\]saw VP \[\]a NP \[NP(t)\]dog N \[NP(t)\](4a) yesterday NP(t)\[S(rel)\]which S(rel) \[ \]had VP \[\]no NP \[\]nose N \[\]' t '  ~---'time adjunct''rel' ='relative'The only transition in (4a) that differs from thatof the corresponding word in the 'core' variant(3a) is that of 'dog' which has the respective tran-sitions:N \[NP(t)\] ~ S(rel) \[NP(t)\]  (in 3a)N \[NP(t)\] ~ NP( t )  \[S(rel)\] (in 4a)Both nouns introduce a relative clause modifierS(rel), the difference being that in the discon-tinuous variant a category has been taken off thestack at the same time as the modifier has beenplaced on the stack.
It has been assumed so farthat we are using a right-linear indexed grammar,but such a rule is expressly disallowed in an in-dexed grammar and so allowing transitions of thiskind ends the formalism's weak equivalence to thecontext-free grammars.Of course, having allowed such crossed ependen-cies, there is nothing in the formalism itself thatwill disallow a similar analysis for a discontinuityunacceptable in English such as:(5) I saw a yesterday dog.This does not present a problem, however, as inDOP it is information in the parsed corpus whichdetermines the structures that are possible.
Thereis no need to explicitly rule out (5), as the transi-tion NP  \[hi --+ a \[N\] will be vanishingly rare inany corpus of even the most garbled speech, whilethe transition N \[hi --+ a \[S(rel)\] is commonlymet with in both written and spoken English.Non-Const i tuent CoordinationThe analysis of standard coordination is shown in(6):274Fido S \[\]gnawed VP \[\]a NP \[VP(+)\](6) bone N \[VP(+)\]and VP(+)\[ \]barked VP \[ \]Instead of a typical transition for 'gnawed' of VP-+ NP,  we have a transition introducing a coor-dinated VP: VP  -4 NP  \ [VP(+)\]In general for any transition X -4 Y , where Xis a category and Y a list of categories (possiblyempty), there will be a transition introducing co-ordination: X -4 Y IX(+)\]Non-constituent coordinations such as (7) presentserious problems for phrase-structure approaches:(7) Fido had a bone yesterday and biscuit today.However if we generalize the schema lready ob-tained for standard coordination by allowing Xto be not only a single category, but a listof categories ~, it is found to suffice for non-constituent coordination as well.Fido S \[1had VP \[\]a NP \[Ne(t)\](Ta) bone N \[Ne(t)\]yesterday NP(t)\[N(+)\[NP(t)\]\]and N(+) \[NP(t)\]biscuit N \[NP(t)\]today NP(t)\[ \]In this analysis instead of a regular transition for'bone' of: N \[NP(t)\]  -4 NP( t )  \[\]there is instead a transition introducing coordina-tion: N \[NP(t)\] -4 NP( t )  \ [N(+) \[NP(t)\]\]Allowing categories on the stack to themselveshave non-empty stacks moves the formalism onestep further from being an indexed grammar.
Thisis the final incarnation of the formalism, being theState-Trans i t ion Grammar  of the title 6.Similar schemas are being investigated to charac-terize gapping constructions.Centre-EmbeddingIt should be noted that an indefinite amount ofcent re -embedd ing  can be described, but only5There is in general no upper l imit to the length of thislist, eg.
"I gave Fido a biscuit yesterday in the house andRover a bone today in his kennel.
"fiMilward (1990) introduces a formal ism essentiallyidentical to the one presented here, a l though viewed froma very different perspective.
Milward (1994) shows how ithandles a wide range of non-const i tuent co-ordinations.at the expense of unlimited' growth in the lengthof states:The S \[\]ny N \[VP\]the S(np) \[VP\]dog N \[VP(np),VP\](8) the S(np) \[VP(np),VP\]cat N \[VP(np),VP(nD),VP\]scratched VP(np)\[VP(np),VP\]swallowed VP (np) \[VP\]died VP \[\]This contrasts with unlimited right-reeursionwhere there is no growth in state length:i s \[\]saw VP \[\]the NP \[\]cat N \[\](9) that S(rel)\[ \]scratched VP \[\]the NP \[\]dog N \[\]that S(rel)\[ \]As the model is to be trained from real data, tran-sitions involving long states as in (8) will have anever smaller and eventually effectively nil proba-bility.
Therefore, when tuned to any particularlanguage corpus the resulting rammar will be ef-fectively finite-state r.Pars ingAssuming that we now have a corpus parsed withthe state-transition grammar, how can this infor-mation be used to parse fresh text?Firstly, for each word type in the corpus we cancollect the transitions with which it occurs andcalculate its probability distribution over all pos-sible transitions (an infinite number of which willbe zero).
To make this concrete, there are five to-kens of the word 'dog' in the examples thus far,and so 'dog' will have the transition probabilitydistribution:N \[NP\] -4 NP  \[\] 0.2N \[NP(t)\]  -4 S(rel) \[NP(t)\] 0.2N \[NP(t)\] -4 NP( t )  \[S(rel)\] 0.2N \ [VP(np) ,VP\]  -4 S(np) \ [VP(np) ,VP\ ]  0.2rTh is  may be compared to the claim in Krauwer &Des Tombes (1981) that  f inite-state automata  offer a moresatisfactory characterization of language than context-freegrammars .275N \[\] -~ S(rel) \[1 0.2To find the most probable parse for a sentence,we simply find the path from word to word whichmaximizes the product of the state transitions (aswe have a first order Markov process).However this simple-minded approach, althougheasy to implement, in other ways leaves much tobe desired.
The probability distributions are fartoo "gappy" and even if a huge amount of datawere collected, the chances that they would pro-vide the desired path for a sentence of any reason-able length are slim.
The process of generalizingor smoothing the transition probabilities i there-fore seen to be indispensable.Smoothing Probability DistributionsAlthough far from exhausting the possible meth-ods for smoothing, the following three are thoseused in the implementation described at the endof the paper.1.
Factor out elements on the stack which aremerely carried over from state to state (which wasdone earlier in looking at the correspondence ofstate transitions to categorial types).
The previ-ous transitions for 'dog' then become:N \[.\] -~ ~ \[1 0.2N \[a\] --+ a \[S(rel)\] 0.2N \[.\] -~ S(np) \[.\] 0.2N \[a\] --+ S(rel) \[a\] 0.42.
Factor out other features which are merelypassed from state to state.
For instance in theexample sentences, 'the' has the generalized tran-sitions:s \[~\] ~ N \[VP,~\]S(np) \[a\] --4 N \[VP(np),a\]which can be further generalized to the singletransition:S(fl) \[a\] -~ N \[VP(j3),a\] /3 = set of featuresWords hitherto unknown to the system can betreated as being extreme xamples of words lack-ing sufficient transition data and they might thenbe given a transition distribution blended from theopen class word paradigms.Problems Arising from SmoothingAlthough essential for effective processing, thesmoothing operations may give rise to new prob-lems.
For example, factoring out items on thestack, as in (1), removes from the model the dis-inclination for long states inherent in the originalcorpus.
To recapture this discarded aspect of thelanguage, it would be sufficient to introduce intothe model a probabilistic penalty based on statelength.
This penalty may easily be calculated ac-cording to the lengths of states in the parsed cor-pus.Not only would this allow the modelling of the re-striction on centre-embedding, but it would alsoallow many other "processing" phenomena to beaccurately characterized.
Taking as an exam-ple "heavy-NP shift", suppose that the corpuscontained two distinct transitions for the word'threw', with the particle 'out' both before andafter the object.threw VP ~ NP, X(out) prob: plVP --+ X(out), NP prob: p2Even if pl were considerably greater than p2, thecumulative negative ffect of the longer states in(10) would eventually lead to the model givingthe sentence with the shifted NP (11) a higherprobability.3.
Establish word paradigms, ie.
classes of wordswhich occur with similar transitions.
The prob- Iability distribution for individual words can then threw be smoothed by suitably blending in the paradig- outmatic distribution.
These paradigms will corre- the spond to a great extent to the word classes of (11) bacon rule-based grammars.
The advantage would be re-that tained however that the system is still fine-grained Fidoenough to reflect he idiosyncratic patterns of in- had dividual words and could override this paradig- chewedmatic information if sufficient data were available.I S \[\]threw VP \[\]the NP \[X(out)\]bacon N \[X(out)\](10) that S(rel) \[X(out)\]Fido S (np)  \[X(out)\]had VP(np) \[X(out)\]chewed VP(np) \[X(out)\]out X(out) \[\]s \[1vP \[\]X(out) \[NP\]NP \[1N \[1S(rel) \[1S(np) \[1VP(np) \[1VP(np) \[\]276Capturing Lexical PreferencesOne strength of n-gram models is that they cancapture a certain amount of lexical preferenceinformation.
For example, in a bigram modeltrained on sufficient data the probability of thebigram 'dog barked' could be expected to be sig-nificantly higher than 'cat barked', and this slice of"world knowledge" is something our model lacks.It would not be difficult to make a small extensionto the present model to capture such information,namely by introducing an additional feature con-taining the "lexical value" of the head of a phrase.Abandoning the shorthand 'VP'  and representinga subject explicitly as a "slashed" NP, a sentencewith added lexical head features would appear as:Thedogwhich(12) chasedthecatbarkeds \[\]N(dog) \[S(np(dog))\]S(rel,np(dog))\[S (np(dog))\]S(np(dog)) \[S(np(dog))\]NP(cat) \[S(np(dog))\]N(cat) \[S(np(dog))\]S(np(dog)) \[\]In contrast o n-grams, where this sentence wouldcloud somewhat the "world knowledge", contain-ing as it does the bigram 'cat barked', the addedstructure of our model allows the lexical prefer-ence to be captured no matter how far the headnoun is from the head verb.
From (12) the worldknowledge of the system would be reinforced bythe two stereotypical transitions:'chased' S (np(dog) )  -+ NP(cat )'barked' S (np(dog) )  -+ \[\]Present Implementation16,000+ running words from section N of theBrown corpus (texts N01-N08) were hand-parsedusing the state-transition grammar.
The actualformalism used was much fuller than the ratherschematic one given above, including many ad-ditional features such as case, tense, person andnumber.
Transition probabilities were generalizedin the ways discussed in the previous ection.Resu l ts100 sentences of less than 15 words were chosenrandomly from other texts in section N of theBrown corpus (N09-N14) and fed to the parserwithout alteration.
Unknown words in the input,of which there were obviously many, were assignedto one of seven orthographic classes and given ap-propriate transitions calculated from the corpus.
* 27 were parsed correctly, ie.
exactly the sameas the hand parse or differing in only relativelyinsignificant ways which the model could nothope to know s .?
23 were parsed wrongly, ie.
the analysis differedfrom the hand parse in some non-trivial way.
* 50 were not parsed at all, ie.
one or more of thetransitions necessary to find a parse path waslacking, even after generalizing the transitions.Future DevelopmentAlthough the results at present are extremelymodest, it should be borne in mind both that theamount of data the system has to work on is verysmall and that the smoothing of transition prob-abilities is still far from optimal.
The present ar-get is to achieve such a level of performance thatthe corpus can be extended by hand-correction ofthe parser output, rather than hand-parsing fromscratch.
Not only will this hopefully save a cer-tain amount of drudgery, it should also help tominimize .errors and maintain consistency.A more distant goal is to ascertain whether theperformance of the model can improve after pars-ing new texts and processing the data therein evenwithout hand-correction of the parses, and if sowhat the limits are to such "self-improvement".ReferencesAHO A.V.
1968.
Indexed Grammars.
Journal ofthe ACM, 15: 647-671.BOD, RENS 1992.
A Computational Model ofLanguage Performance: Data Oriented Parsing.COLING-gP.KARTTUNEN L. 1990.
Radical Lexicalism.
InBaltin & Kroch (eds), Alternative conceptions ofphrase structure, Univ of Chicago Press, pp 43-65.KRAUWER, STEVEN  DES TOMBES, LOUIS1981.
Transducers and Grammars as Theories ofLanguage.
Theoretical Linguistics, 8, 173-202.MILWARD, DAVID 1990.
Coordination in an Ax-iomatic Grammar.
COLING-90.MILWARD, DAVID 1994.
Non-constituent Coordi-nation: Theory and Practice.
COLING-94.SSuch as the system postulating that "Jess" was a sur-name, as against he hand-parser's guess of a masculinefirst name.277
