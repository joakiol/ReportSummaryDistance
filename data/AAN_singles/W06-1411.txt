Proceedings of the Fourth International Natural Language Generation Conference, pages 73?80,Sydney, July 2006. c?2006 Association for Computational LinguisticsGroup-based Generation of Referring ExpressionsFunakoshi Kotaro ?
Watanabe Satoru ?Department of Computer Science, Tokyo Institute of TechnologyTokyo Meguro O?okayama 2-12-1, 152-8552, Japantake@cl.cs.titech.ac.jpTokunaga TakenobuAbstractPast work of generating referring expres-sions mainly utilized attributes of objectsand binary relations between objects in or-der to distinguish the target object fromothers.
However, such an approach doesnot work well when there is no distinc-tive attribute among objects.
To over-come this limitation, this paper proposesa novel generation method utilizing per-ceptual groups of objects and n-ary re-lations among them.
The evaluation us-ing 18 subjects showed that the proposedmethod could effectively generate properreferring expressions.1 IntroductionIn the last two decades, many researchers havestudied the generation of referring expressions toenable computers to communicate with humansabout objects in the world.In order to refer to an intended object (the tar-get) among others (distractors), most past work(Appelt, 1985; Dale and Haddock, 1991; Dale,1992; Dale and Reiter, 1995; Heeman and Hirst,1995; Horacek, 1997; Krahmer and Theune, 2002;van Deemter, 2002; Krahmer et al, 2003) utilizedattributes of the target and binary relations be-tween the target and distractors.
Therefore, thesemethods cannot generate proper referring expres-sions in situations where there is no significantsurface difference between the target and distrac-tors, and no binary relation is useful to distinguishthe target.
Here, a proper referring expression?Currently at Honda Research Institute Japan Co., Ltd.?Currently at Hitachi, Ltd.means a concise and natural linguistic expressionenabling hearers to identify the target.For example, consider indicating object b to per-son P in the situation of Figure 1.
Note that la-bels a, b and c are assigned for explanation to thereaders, and person P does not share these labelswith the speaker.
Because object b is not distin-guishable from objects a or c by means of theirappearance, one would try to use a binary relationbetween object b and the table, i.e., ?a ball to theright of the table?.
However, ?to the right of?
isnot a discriminatory relation, for objects a and care also located to the right of the table.
Using aand c as a reference object instead of the table doesnot make sense, since a and c cannot be uniquelyidentified because of the same reason that b cannotbe identified.
Such situations have drawn less at-tention (Stone, 2000), but can frequently occur insome domains such as object arrangement (Tanakaet al, 2004).PabcTableFigure 1: An example of problematic situationsIn the situation of Figure 1, the speaker can indi-cate object b to person P with a simple expression?the front ball?.
In order to generate such an ex-pression, one must be able to recognize the salientperceptual group of the objects and use the n-aryrelative relations in the group.73To overcome the problem described above, Fu-nakoshi et al (2004) proposed a method of gen-erating Japanese referring expressions that utilizesn-ary relations among members of a group.
They,however, dealt with the limited situations whereonly homogeneous objects are randomly arranged(see Figure 2).
Thus, their method could han-dle only spatial n-ary relation, and could not han-dle attributes and binary relations between objectswhich have been the main concern of the past re-search.In this paper, we extend the generation methodproposed by (Funakoshi et al, 2004) so as to han-dle object attributes and binary relations betweenobjects as well.
In what follows, Section 2 showsan extension of the SOG representation that wasproposed in (Funakoshi et al, 2004).
Our newmethod will be described in Section 3 and eval-uated in Section 4.
Finally we conclude the paperin Section 5.2 SOG representationFunakoshi et al (2004) proposed an intermedi-ate representation between a referring expressionand the situation that is referred to by the expres-sion.
The intermediate representation representsa course of narrowing down to the target as a se-quence of groups from the group of all objects tothe singleton group of the target object.
Thus it iscalled SOG (Sequence Of Groups).The following example shows an expression de-scribing the target x in Figure 2 with the cor-responding SOG representation below it.
SinceJapanese is a head-final language, the order ofgroups in the SOG representation can be retainedin the linguistic expression.hidari oku ni aru(1)mittu no tama no uti no(2)itiban migi no tama(3)(the rightmost ball(3)among the three balls(2)at the back left(1))SOG:[{a, b, c, d, e, f, x}, {a, b, x}, {x}],where {a, b, c, d, e, f, x} denotes all objects inthe situation, {a, b, x} denotes the three objectsat the back left, and {x} denotes the target.2.1 Extended SOGAs mentioned above, (Funakoshi et al, 2004) sup-posed the limited situations where only homoge-neous objects are randomly arranged, and consid-ered only spatial subsumption relations betweenconsecutive groups.
Therefore, relations betweenPabefc dxFigure 2: An example from (Funakoshi et al,2004)groups are not explicitly denoted in the originalSOGs as shown below.SOG: [G0, G1, .
.
.
, Gn]Gi: a groupIn this paper, however, other types of relationsbetween groups are also considered.
We proposean extended SOG representation where types ofrelations are explicitly denoted as shown below.
Inthe rest of this paper, we will refer to this extendedSOG representation by simply saying ?SOG?.SOG: [G0R0G1R1.
.
.
GiRi.
.
.
Gn]Gi: a groupRi: a relation between Giand Gi+12.2 Relations between groupsRi, a relation between groups Giand Gi+1, de-notes a shift of attention from Gito Gi+1witha certain focused feature.
The feature can be anattribute of objects or a relation between objects.There are two types of relations between groups:intra-group relation and inter-group relation.Intra-group relation When Riis an intra-grouprelation, Gisubsumes Gi+1, that is, Gi?
Gi+1.Intra-group relations are further classified into thefollowing subcategories according to the featureused to narrow down Gito Gi+1.
We denote thesesubcategories with the following symbols.space??
: spatial subsumptiontype??
: the object typeshape??
: the shape of objectscolor??
: the color of objectssize??
: the size of objectsWith respect to this classification, (Funakoshi etal., 2004) dealt with only thespace??
relation.74Inter-group relation When Riis an inter-grouprelation, Giand Gi+1are mutually exclusive, thatis, Gi?
Gi+1= ?.
An inter-group relation is aspatial relation and denoted by symbol ?.Example Rican be one ofspace??
,type??,shape??
,color??,size??
and ?.
We show a referring expres-sion indicating object b1 and the correspondingSOG in the situation of Figure 3.
In the SOG,{all} denotes the total set of objects in the situ-ation.
The indexed underlines denote correspon-dence between SOG and linguistic expressions.As shown in the figure, we allow objects being onthe other objects.marui(1)futatu no tukue no uti no(2)hidari no(3)tukue no(4)ue no(5)tama(6)(the ball(6)on(5)the left(3)table(4)among the two(2)round(1)tables(2))SOG: [{all}type??
{t1, t2, t3}shape??
(1){t1, t2}(2)space??(3){t1}(4)?
(5){b1}(6)]b2b1b5t3t2p1t1b3b4blueblackredFigure 3: An example situation3 GenerationOur generation algorithm proposed in this sectionconsists of four steps: perceptual grouping, SOGgeneration, surface realization and scoring.
In therest of this section, we describe these four steps byusing Figure 3 as an example.3.1 Step 1: Perceptual groupingOur algorithm starts with identifying groups ofobjects that are naturally recognized by humans.We adopt Tho?risson?s perceptual grouping algo-rithm (Tho?risson, 1994) for this purpose.
Per-ceptual grouping is performed with objects in thesituation with respect to each of the followingfeatures: type, shape, color, size, and proxim-ity.
Three special features, total, singleton, andclosure are respectively used to recognize the to-tal set of objects, groups containing each singleobject, and objects bounded in perceptually sig-nificant regions (table tops in the domain of thispaper).
These three features are handled not byTho`risson?s algorithm but by individual proce-dures.Type is the most dominant feature because hu-mans rarely recognize objects of different types asa group.
Thus, first we group objects with respectto types, and then group objects of the same typewith respect to other features (except for total).Although we adopt Tho?risson?s grouping algo-rithm, we use different grouping strategies fromthe original.
Tho?risson (1994) lists the followingthree combinations of features as possible strate-gies of perceptual grouping.?
shape and proximity?
color and proximity?
size and proximityHowever, these strategies are inappropriate to gen-erate referring expressions.
For example, becausetwo blue balls b1 and b2 in Figure 3 are toomuch distant from each other, Tho?risson?s algo-rithm cannot recognize the group consisting of b1and b2 with the original strategies.
However, theexpression like ?the left blue ball?
can naturallyrefer to b1.
When using such an expression, weassume an implicit group consisting of b1 and b2.Hence, we do not combine features but use themseparately.The results of perceptual grouping of the situa-tion in Figure 3 are shown below.
Relation labelsare assigned to recognized groups with respect tofeatures used in perceptual grouping.
We definesix labels: all, type, shape, color, size, andspace.
Features singleton, proximity and closureshare the same label space.
A group may haveseveral labels.feature label recognized groupstotal all {t1, t2, t3, p1, b1, b2, b3, b4, b5}singleton space {t1}, {t2}, {t3}, {p1}, {b1}, {b2},{b3}, {b4}, {b5}type type {t1, t2, t3}, {p1}, {b1, b2, b3, b4, b5}shape shape {t1, t2}, {t3}color color {b1, b2}, {b3}, {b4, b5}size size {b1, b3, b4}, {b2, b5}proximity space {t2, t3}, {b1, b3, b4, b5}, {b3, b4, b5}closure space {b1}, {b3, b4}75Target # target objectAllGroups # all generated groupsSOGList # list of generated SOGs01:makeSOG()02: SOG = []; # list of groups and symbols03: All = getAll(); # total set04: add(All, SOG); # add All to SOG05: TypeList = getAllTypes(All);# list of all object types06: TargetType = getType(Target);# type of the target07: TargetSailency = saliency(TargetType);# saliency of the target type08: for each Type in TypeList do# {Table, Plant, Ball}09: if saliency(Type) ?TargetSaliency then# saliency: Table > Plant > Ball10: Group = getTypeGroup(Type);# get the type group of Type11: extend(SOG, Group);12: end if13: end for14:returnFigure 4: Function makeSOG3.2 Step 2: SOG generationThe next step is generating SOGs.
This is so-called content planning in natural language gen-eration.
Figure 4, Figure 5 and Figure 6 show thealgorithm of making SOGs.Three variables Target, AllGroups, andSOGList defined in Figure 4 are global variables.Target holds the target object which the refer-ring expression refers to.
AllGroups holds theset of all groups recognized in Step 1.
GivenTarget and AllGroups, function makeSOGenumerates possible SOGs in the depth-first man-ner, and stores them in SOGList.makeSOG (Figure 4) makeSOG starts with a list(SOG) that contains the total set of objects in thedomain.
It chooses groups of objects that are moresalient than or equal to the target object and callsfunction extend for each of the groups.extend (Figure 5) Given an SOG and a groupto be added to the SOG, function extend extendsthe SOG with the group for each label attached tothe group.
This extension is done by creating acopy of the given SOG and adding to its end anintra-group relation symbol defined in Section 2.2corresponding to the given label and group.
Fi-nally it calls search with the copy.search (Figure 6) This function takes an SOGas its argument.
According to the last group in01:extend(SOG, Group)02: Labels = getLabels(Group);03: for each Label in Labels do04: SOGcopy = copy(SOG);05: add(Label?
?, SOGcopy);06: add(Group, SOGcopy);07: search(SOGcopy);08: end for09:returnFigure 5: Function extendthe SOG (LastGroup), it extends the SOG asdescribed below.1.
If LastGroup is a singleton of the targetobject, append SOG to SOGList and return.2.
If LastGroup is a singleton of a non-targetobject, find the groups that contain the targetobject and satisfy the following three condi-tions: (a), (b) and (c).
(a) All objects in the group locate inthe same direction from the object ofLastGroup (the reference).
Possi-ble directions are one of ?back?, ?backright?, ?right?, ?front right?, ?front?,?front left?, ?left?, ?left back?
and ?on?.The direction is determined on the basisof coordinate values of the objects, andis assigned to the group for the use ofsurface realization.
(b) There is no same type object located be-tween the group and the reference.
(c) The group is not a total set of a certaintype of object.Then, for each of the groups, make a copyof the SOG, and concatenate ???
and thegroup to the copy, and call search recur-sively with the new SOG.3.
If LastGroup contains the target objecttogether with other objects, let the inter-section of LastGroup and each group inAllGroups be NewG, and copy the labelfrom each group to NewG.
If NewG containsthe target object, call function extend un-less Checked contains NewG.4.
If LastGroup contains only non-target ob-jects, call function extend for each group(Group) in AllGroupswhich is subsumedby LastGroup.Figure 7 shows the SOGs generated to refer toobject b1 in Figure 3.761.
[{all}type??
{t1, t2, t3}space??
{t1} ?{b1}]2.
[{all}type??
{t1, t2, t3}shape??
{t1, t2}space??
{t1} ?{b1}]3.
[{all}type??
{b1, b2, b3, b4, b5}space??
{b1}]4.
[{all}type??
{b1, b2, b3, b4, b5}color??
{b1, b2}space??
{b1}]5.
[{all}type??
{b1, b2, b3, b4, b5}color??
{b1, b2}size??
{b1}]6.
[{all}type??
{b1, b2, b3, b4, b5}size??
{b1, b4, b3}space??
{b1}]7.
[{all}type??
{b1, b2, b3, b4, b5}size??
{b1, b4, b3}color??
{b1}]8.
[{all}type??
{b1, b2, b3, b4, b5}space??
{b1, b3, b4, b5}space??
{b1}]9.
[{all}type??
{b1, b2, b3, b4, b5}space??
{b1, b3, b4, b5}color??
{b1}]10.
[{all}type??
{b1, b2, b3, b4, b5}space??
{b1, b3, b4, b5}size??
{b1, b3, b4}space??
{b1}]11.
[{all}type??
{b1, b2, b3, b4, b5}space??
{b1, b3, b4, b5}size??
{b1, b3, b4}color??
{b1}]Figure 7: Generated SOGs from the situation in Figure 301:search(SOG)02: LastGroup = getLastElement(SOG);# get the rightmost group in SOG03: Card = getCardinality(LastGroup);04: if Card == 1 then05: if containsTarget(LastGroup) then# check if LastGroup contains# the target06: add(SOG, SOGList);07: else08: GroupList =searchTargetGroups(LastGroup);# find groups containing the target09: for each Group in GroupList do10: SOGcopy = copy(SOG);11: add(?, SOGcopy);12: add(Group, SOGcopy);13: search(SOGcopy);14: end for15: end if16: elsif containsTarget(LastGroup) then17: Checked = [ ];18: for each Group in AllGroups do19: NewG = Intersect(Group, LastGroup);# make intersection20: Labels = getLabels(Group);21: setLabels(Labels, NewG);# copy labels from Group to NewG22: if containsTarget(NewG) &!contains(Checked, NewG) then23: add(NewG, Checked);24: extend(SOG, Group);25: end if26: end for27: else28: for each Group of AllGroups do29: if contains(LastGroup, Group) then30: extend(SOG, Group);31: end if32: end for33: end if34:returnFigure 6: Function search3.3 Step 3: Surface realizationA referring expression is generated by determin-istically assigning a linguistic expression to eachelement in an SOG according to Rule 1 and 2.As Japanese is a head-final language, simple con-catenation of element expressions makes a well-formed noun phrase1.
Rule 1 generates expres-sions for groups and Rule 2 does for relations.Each rule consists of several subrules which areapplied in this order.
[Rule 1]: Realization of groupsRule 1.1 The total set ({all}) is not realized.
(Funakoshi et al, 2004) collected referringexpressions from human subjects through ex-periments and found that humans rarely men-tioned the total set.
According to their obser-vation, we do not realize the total set.Rule 1.2 Realize the type name for a singleton.Type is realized as a noun and only for a sin-gleton because the type feature is used first tonarrow down the group, and the succeedinggroups consist of the same type objects untilreaching the singleton.
When the singleton isnot the last element of SOG, particle ?no?
isadded.Rule 1.3 The total set of the same type objects isnot realized.This is because the same reason as Rule 1.1.Rule 1.4 The group followed by the relationspace?
?is realized as ?
[cardinality] [type] no-uti(among)?, e.g., ?futatu-no (two) tukue (desk)no-uti (among)?.
The group followed by1Although different languages require different surfacerealization rules, we presume perceptual grouping and SOGgeneration (Step 1 and 2) are applicable to other languages aswell.77the relation ?
is realized as ?
[cardinality][type] no?.When consecutive groups are connected byother than spatial relations (space??
and ?
),they can be realized as a sequence of relationsahead of the noun (type name).
For example,expression ?the red ball among big balls?
canbe simplified to ?the big red ball?.Rule 1.5 Other groups are not realized.
[Rule 2]: Realization of relationsRule 2.1 Relationtype??
is not realized.See Rule 1.2.Rule 2.2 Relationsshape??
,color??
andsize??
are real-ized as the expressions corresponding to theirattribute values.
Spatial relations (space??
and?)
are realized as follows, where |Gi| de-notes the cardinality of Gi.Intra-group relation (Gispace??
Gi+1)If |Gi| = 2 (i.e., |Gi+1| = 1), based on thegeometric relations among objects, generateone of four directional expressions ?
{migi,hidari, temae, oku} no ({right, left, front,back})?.If |Gi| ?
3 and |Gi+1| = 1, based on thegeometric relations among objects, generateone of eight directional expressions ?itiban{migi, hidari, temae, oku, migi temae, hi-dari temae, migi oku, hidari oku} no ({right,left, front, back, front right, front left, backright, back left}-most)?
if applicable.
If noneof these expressions is applicable, generateexpression ?mannaka no (middle)?
if appli-cable.
Otherwise, generate one of four ex-pressions ?
{hidari, migi, temae, oku} karaj-banme no (j-th from {left, right, front,back})?.If |Gi+1| ?
2, based on the geometric rela-tions among objects, generate one of eight di-rectional expressions ?
{migi, hidari, temae,oku, migi temae, hidari temae, migi oku, hi-dari oku} no ({right, left, front, back, frontright, front left, back right, back left})?.Inter-group relation (Gi?Gi+1)|Gi| = 1 should hold because of searchin Step 2.
According to the direction as-signed by search, generate one of nine ex-pressions : ?
{migi, hidari, temae, oku, migitemae, hidari temaen, migi oku, hidari oku,ue} no ({right, left, front, back, front right,front left, back right, back left, on})?.Figure 8 shows the expressions generated fromthe first three SOGs shown in Figure 7.
The num-bers in the parentheses denote coindexes of frag-ments between the SOGs and the realized expres-sions.3.4 Step 4: ScoringWe assign a score to each expression by taking intoaccount the relations used in the expression, andthe length of the expression.First we assign a cost ranging over [0, 1] to eachrelation in the given SOG.
Costs of relations aredecided as below.
These costs conform to the pri-orities of features described in (Dale and Reiter,1995).type??
: No cost (to be neglected)shape??
: 0.2color??
: 0.4size??
: big(est): 0.6, small(est): 0.8, middle: 1.0space??
,?
: Cost functions are defined according to thepotential functions proposed in (Tokunagaet al, 2005).
The cost for relation ?on?
isfixed to 0.Then, the average cost of the relations is calcu-lated to obtain the relation cost, Crel.
The cost ofsurface length (Clen) is calculated byClen=length(expression)maxilength(expressioni),where the length of an expression is measured bythe number of characters.Using these costs, the score of an expression iscalculated byscore =1??
Crel+ (1?
?)?
Clen.?
was set to 0.5 in the following experiments.4 Evaluation4.1 ExperimentsWe conducted two experiments to evaluate expres-sions generated by the proposed method.Both experiments used the same 18 subjects andthe same 20 object arrangements which were gen-erated automatically.
For each arrangement, allfactors (number of objects, positions of objects, at-tributes of objects, and the target object) were ran-domly decided in advance to conform to the fol-lowing conditions: (1) the proposed method cangenerate more than five expressions for the giventarget and (2) more than two other objects existwhich are the same type as the target.781.
SOG: [{all}type??
{t1, t2, t3}space??(1){t1}(2)?
(3){b1}(4)]itiban hidari no(1)tukue no(2)ue no(3)tama(4)(the ball(4)on(3)the leftmost(1)table(2))2.
SOG: [{all}type??
{t1, t2, t3}shape??
(1){t1, t2}(2)space??(3){t1}(4)?
(5){b1}(6)]marui(1)futatu no tukue no uti(2)hidari no(3)tukue no(4)ue no(5)tama(6)(the ball(6)on(5)the left(3)table(4)among(2)the round(1)two tables(2))3.
SOG: [{all}type??
{b1, b2, b3, b4, b5}space??
(1){b1}(2)]itiban hidari no(1)tama(2)(the leftmost(1)ball(2))Figure 8: Realized expressions?20?20/20???????
?t1b3b1b4t2p1b2Figure 9: An example stimulus of Experiment 1Experiment 1 Experiment 1 was designed toevaluate the ability of expressions to identify thetargets.
The subjects were presented an arrange-ment with a generated referring expression whichgained the highest score at a time, and were in-structed to choose the object referred to by the ex-pression.
Figure 9 is an example of visual stimuliused in Experiment 1.
Each subject responded toall 20 arrangements.Experiment 2 Experiment 2 was designed toevaluate validity of the scoring function describedin Section 3.4.
The subjects were presented anarrangement with a marked target together withthe best five generated expressions referring to thetarget at a time.
Then the subjects were askedto choose the best one from the five expressions.Figure 10 is an example of visual stimuli used inExperiment 2.
Each subject responded to the all20 arrangements.
The expressions used in Experi-ment 2 include those used in Experiment 1.4.2 ResultsTable 1 shows the results of Experiment 1.
Theaverage accuracy of target identification is 95%.Figure 10: An example stimulus of Experiment 2This shows a good performance of the generationalgorithm proposed in this paper.The expression generated for arrangementNo.
20 (shown in Figure 9) resulted in the excep-tionally poor accuracy.
To refer to object b1, ouralgorithm generated expression ?itiban temae notama (the most front ball)?
because b1 is the mostclose object to person P in terms of the verticalaxis.
Humans, however, chose the object that is theclosest to P in terms of Euclidean distance.
Somepsychological investigation is necessary to builda more precise geometrical calculation model tosolve this problem (Landragin et al, 2001).Table 2 shows the results of Experiment 2.
Thefirst row shows the rank of expressions based ontheir score.
The second row shows the count of hu-man votes for the expression.
The third row showsthe ratio of the votes.
The top two expressions oc-cupy 72% of the total.
This concludes that ourscoring function works well.5 ConclusionThis paper extended the SOG representation pro-posed in (Funakoshi et al, 2004) to generate refer-79Table 1: Result of Experiment 1Arrangement No.
1 2 3 4 5 6 7 8 9 10Accuracy 0.89 1.0 1.0 1.0 1.0 1.0 1.0 0.94 1.0 1.011 12 13 14 15 16 17 18 19 20 Ave.1.0 0.94 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.17 0.95Table 2: Result of Experiment 2Rank 1 2 3 4 5 TotalVote 134 125 59 22 20 360Share 0.37 0.35 0.16 0.06 0.06 1ring expressions in more general situations.The proposed method was implemented andevaluated through two psychological experimentsusing 18 subjects.
The experiments showed thatgenerated expressions had enough discriminationability and that the scoring function conforms tohuman preference well.The proposed method would be able to handleother attributes and relations as far as they can berepresented in terms of features as described insection 3.
Corresponding surface realization rulesmight be added in that case.In the implementation, we introduced rather adhoc parameters, particularly in the scoring func-tion.
Although this worked well in our experi-ments, further psychological validation is indis-pensable.This paper assumed a fixed reference frame isshared by all participants in a situation.
How-ever, when we apply our method to conversationalagent systems, e.g., (Tanaka et al, 2004), refer-ence frames change dynamically and they mustbe properly determined each time when generat-ing referring expressions.In this paper, we focused on two dimensionalsituations.
To apply our method to three dimen-sional worlds, more investigation on human per-ception of spatial relations are required.
We ac-knowledge that a simple application of the currentmethod does not work well enough in three dimen-sional worlds.ReferencesDouglas E. Appelt.
1985.
Planning English referring expres-sions.
Artificial Intelligence, 26:1?33.Robert Dale and Nicholas Haddock.
1991.
Generating re-ferring expressions involving relations.
In Proceedings ofthe Fifth Conference of the European Chapter of the As-sociation for Computational Linguistics(EACL?91), pages161?166.Robert Dale and Ehud Reiter.
1995.
Computational interpre-tations of the Gricean maxims in the generation of refer-ring expressions.
Cognitive Science, 19(2):233?263.Robert Dale.
1992.
Generating referring expressions: Con-structing descriptions in a domain of objects and pro-cesses.
MIT Press, Cambridge.Kotaro Funakoshi, Satoru Watanabe, Naoko Kuriyama, andTakenobu Tokunaga.
2004.
Generating referring expres-sions using perceptual groups.
In Proceedings of the 3rdInternational Conference on Natural Language Genera-tion: INLG04, pages 51?60.Peter Heeman and Graeme Hirst.
1995.
Collaborating refer-ring expressions.
Computational Linguistics, 21(3):351?382.Helmut Horacek.
1997.
An algorithm for generating refer-ential descriptions with flexible interfaces.
In Proceedingsof the 35th Annual Meeting of the Association for Compu-tational Linguistics, pages 206?213.Emiel Krahmer and Marie?t Theune.
2002.
Efficient context-sensitive generation of descriptions.
In Kees van Deemterand Rodger Kibble, editors, Information Sharing: Given-ness and Newness in Language Processing.
CSLI Publica-tions, Stanford, California.Emiel Krahmer, Sebastiaan van Erk, and Andre?
Verleg.
2003.Graph-based generation of referring expressions.
Compu-tational Linguistics, 29(1):53?72.Fre?de?ric Landragin, Nadia Bellalem, and Laurent Romary.2001.
Visual salience and perceptual grouping in mul-timodal interactivity.
In Proceedings of InternationalWorkshop on Information Presentation and Natural Mul-timodal Dialogue (IPNMD), pages 151?155.Matthew Stone.
2000.
On identifying sets.
In Proceedingsof the 1st International Conference on Natural LanguageGeneration: INLG00, pages 116?123.Hozumi Tanaka, Takenobu Tokunaga, and Yusuke Shinyama.2004.
Animated agents capable of understanding naturallanguage and performing actions.
In Helmut Prendingerand Mituru Ishizuka, editors, Life-Like Characters, pages429?444.
Springer.Kristinn R. Tho?risson.
1994.
Simulated perceptual grouping:An application to human-computer interaction.
In Pro-ceedings of the Sixteenth Annual Conference of the Cog-nitive Science Society, pages 876?881.Takenobu Tokunaga, Tomofumi Koyama, and Suguru Saito.2005.
Meaning of Japanese spatial nouns.
In Proceedingsof the Second ACL-SIGSEM Workshop on the LinguisticDimentions of Prepositions and their Use in Computa-tional Linguistics: Formalisms and Applications, pages93?100.Kees van Deemter.
2002.
Generating referring expressions:Boolean extensions of the incremental algorithm.
Compu-tational Linguistics, 28(1):37?52.80
