Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 285?290,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsEnhancing Statistical Machine Translation with Character AlignmentNing Xi, Guangchao Tang, Xinyu Dai, Shujian Huang, Jiajun ChenState Key Laboratory for Novel Software Technology,Department of Computer Science and Technology,Nanjing University, Nanjing, 210046, China{xin,tanggc,dxy,huangsj,chenjj}@nlp.nju.edu.cnAbstractThe dominant practice of statistical machinetranslation (SMT) uses the same Chinese wordsegmentation specification in both alignmentand translation rule induction steps in buildingChinese-English SMT system, which may suf-fer from a suboptimal problem that word seg-mentation better for alignment is not necessarilybetter for translation.
To tackle this, we proposea framework that uses two different segmenta-tion specifications for alignment and translationrespectively: we use Chinese character as thebasic unit for alignment, and then convert thisalignment to conventional word alignment fortranslation rule induction.
Experimentally, ourapproach outperformed two baselines: fullyword-based system (using word for bothalignment and translation) and fully charac-ter-based system, in terms of alignment qualityand translation performance.1 IntroductionChinese Word segmentation is a necessary step inChinese-English statistical machine translation(SMT) because Chinese sentences do not delimitwords by spaces.
The key characteristic of a Chi-nese word segmenter is the segmentation specifi-cation1.
As depicted in Figure 1(a), the dominantpractice of SMT uses the same word segmentationfor both word alignment and translation rule induc-tion.
For brevity, we will refer to the word seg-mentation of the bilingual corpus as word segmen-tation for alignment (WSA for short), because itdetermines the basic tokens for alignment; and referto the word segmentation of the aligned corpus asword segmentation for rules (WSR for short), be-cause it determines the basic tokens of translation1 We hereafter use ?word segmentation?
for short.rules2, which also determines how the translationrules would be matched by the source sentences.It is widely accepted that word segmentation witha higher F-score will not necessarily yield bettertranslation performance (Chang et al, 2008; Zhanget al, 2008; Xiao et al, 2010).
Therefore, manyapproaches have been proposed to learn wordsegmentation suitable for SMT.
These approacheswere either complicated (Ma et al, 2007; Chang etal., 2008; Ma and Way, 2009; Paul et al, 2010), orof high computational complexity (Chung andGildea 2009; Duan et al, 2010).
Moreover, theyimplicitly assumed that WSA and WSR should beequal.
This requirement may lead to a suboptimalproblem that word segmentation better for align-ment is not necessarily better for translation.To tackle this, we propose a framework that usesdifferent word segmentation specifications as WSAand WSR respectively, as shown Figure 1(b).
Weinvestigate a solution in this framework: first, weuse Chinese character as the basic unit for align-ment, viz.
character alignment; second, we use asimple method (Elming and Habash, 2007) toconvert the character alignment to conventionalword alignment for translation rule induction.
In the2 Interestingly, word is also a basic token in syntax-based rules.Word alignmentBilingual CorpusAligned CorpusWSATranslation RulesWSAWSRRule inductionDecodingTranslation Results WSRWord alignmentBilingual CorpusAligned CorpusWSATranslation RulesWSAWSRRule inductionDecodingTranslation Results WSRAligned CorpusWSRConversion(b) WSA?WSRFigure 1.
WSA and WSR in SMT pipeline(a)  WSA=WSR285experiment, our approach consistently outper-formed two baselines with three different wordsegmenters: fully word-based system (using wordfor both alignment and translation) and fully char-acter-based system, in terms of alignment qualityand translation performance.The remainder of this paper is structured as fol-lows: Section 2 analyzes the influences of WSA andWSR on SMT respectively; Section 3 discusseshow to convert character alignment to word align-ment; Section 4 presents experimental results, fol-lowed by conclusions and future work in section 5.2 Understanding WSA and WSRWe propose a solution to tackle the suboptimalproblem: using Chinese character for alignmentwhile using Chinese word for translation.
Characteralignment differs from conventional word align-ment in the basic tokens of the Chinese side of thetraining corpus3.
Table 1 compares the token dis-tributions of character-based corpus (CCorpus) andword-based corpus (WCorpus).
We see that theWCorpus has a longer-tailed distribution than theCCorpus.
More than 70% of the unique tokens ap-pear less than 5 times in WCorpus.
However, overhalf of the tokens appear more than or equal to 5times in the CCorpus.
This indicates that modelingword alignment could suffer more from datasparsity than modeling character alignment.Table 2 shows the numbers of the unique tokens(#UT) and unique bilingual token pairs (#UTP) ofthe two corpora.
Consider two extensively features,fertility and translation features, which are exten-sively used by many state-of-the-art word aligners.The number of parameters w.r.t.
fertility featuresgrows linearly with #UT while the number of pa-rameters w.r.t.
translation features grows linearlywith #UTP.
We compare #UT and #UTP of bothcorpora in Table 2.
As can be seen, CCorpus hasless UT and UTP than WCorpus, i.e.
characteralignment model has a compact parameterizationthan word alignment model, where the compactnessof parameterization is shown very important in sta-tistical modeling (Collins, 1999).Another advantage of character alignment is thereduction in alignment errors caused by word seg-3 Several works have proposed to use character (letter) on bothsides of the parallel corpus for SMT between similar (European)languages (Vilar et al, 2007; Tiedemann, 2009), however,Chinese is not similar to English.Frequency Characters (%) Words (%)1 27.22 45.392 11.13 14.613 6.18 6.474 4.26 4.325(+) 50.21 29.21Table 1 Token distribution of CCorpus and WCorpusStats.
Characters Words#UT 9.7K 88.1K#UTP 15.8M 24.2MTable 2 #UT and #UTP in CCorpus and WCorpusmentation errors.
For example, ???
(Cheney)?and ??
(will)?
are wrongly merged into one word???
by the word segmenter, and ??
?wrongly aligns to a comma in English sentence inthe word alignment; However, both ?
and ?
alignto ?Cheney?
correctly in the character alignment.However, this kind of errors cannot be fixed bymethods which learn new words by packing alreadysegmented words, such as word packing (Ma et al,2007) and Pseudo-word (Duan et al, 2010).As character could preserve more meanings thanword in Chinese, it seems that a character can bewrongly aligned to many English words by thealigner.
However, we found this can be avoided to agreat extent by the basic features (co-occurrenceand distortion) used by many alignment models.
Forexample, we observed that the four characters of thenon-compositional word ?????
(Arafat)?
alignto Arafat correctly, although these characters pre-serve different meanings from that of Arafat.
Thiscan be attributed to the frequent co-occurrence (192times) of these characters and Arafat in CCorpus.Moreover,?
usually means France in Chinese,thus it may co-occur very often with France inCCorpus.
If both France and Arafat appear in theEnglish sentence, ?
may wrongly align to France.However, if ?
aligns to Arafat, ?
will probablyalign to Arafat, because aligning ?
to Arafat couldresult in a lower distortion cost than aligning it toFrance.Different from alignment, translation is a patternmatching procedure (Lopez, 2008).
WSR deter-mines how the translation rules would be matchedby the source sentences.
For example, if we usetranslation rules with character as WSR to translatename entities such as the non-compositional word???
?, i.e.
translating literally, we may get awrong translation.
That?s because the linguistic286knowledge that the four characters convey a spe-cific meaning different from the characters has beenlost, which cannot always be totally recovered evenby using phrase in phrase-based SMT systems (seeChang et al (2008) for detail).
Duan et al (2010)and Paul et al, (2010) further pointed out thatcoarser-grained segmentation of the source sen-tence do help capture more contexts in translation.Therefore, rather than using character, usingcoarser-grained, at least as coarser as the conven-tional word, as WSR is quite necessary.3 Converting Character Alignment to WordAlignmentIn order to use word as WSR, we employ the samemethod as Elming and Habash (2007)4 to convertthe character alignment (CA) to its word-basedversion (CA?)
for translation rule induction.
Theconversion is very intuitive: for every Eng-lish-Chinese word pair ?
?, ??
in the sentence pair,we align ?
to ?
as a link in CA?, if and only if thereis at least one Chinese character of ?
aligns to ?
inCA.Given two different segmentations A and B of thesame sentence, it is easy to prove that if every wordin A is finer-grained than the word of B at the cor-responding position, the conversion is unambiguity(we omit the proof due to space limitation).
Ascharacter is a finer-grained than its original word,character alignment can always be converted toalignment based on any word segmentation.Therefore, our approach can be naturally scaled tosyntax-based system by converting characteralignment to word alignment where the word seg-mentation is consistent with the parsers.We compare CA with the conventional wordalignment (WA) as follows: We hand-align somesentence pairs as the evaluation set based on char-acters (ESChar), and converted it to the evaluationset based on word (ESWord) using the above con-version method.
It is worth noting that comparingCA and WA by evaluating CA on ESChar andevaluating WA on ESWord is meaningless, becausethe basic tokens in CA and WA are different.However, based on the conversion method, com-paring CA with WA can be accomplished by evalu-ating both CA?
and WA on ESWord.4 They used this conversion for word alignment combinationonly, no translation results were reported.4 Experiments4.1 SetupFBIS corpus (LDC2003E14) (210K sentence pairs)was used for small-scale task.
A large bilingualcorpus of our lab (1.9M sentence pairs) was used forlarge-scale task.
The NIST?06 and NIST?08 test setswere used as the development set and test set re-spectively.
The Chinese portions of all these datawere preprocessed by character segmenter (CHAR),ICTCLAS word segmenter 5  (ICT) and Stanfordword segmenters with CTB  and PKU specifica-tions6 respectively.
The first 100 sentence pairs ofthe hand-aligned set in Haghighi et al (2009) werehand-aligned as ESChar, which is converted tothree ESWords based on three segmentations re-spectively.
These ESWords were appended totraining corpus with the corresponding word seg-mentation for evaluation purpose.Both character and word alignment were per-formed by GIZA++ (Och and Ney, 2003) enhancedwith gdf heuristics to combine bidirectional align-ments (Koehn et al, 2003).
A 5-gram languagemodel was trained from the Xinhua portion ofGigaword corpus.
A phrase-based MT decodersimilar to (Koehn et al, 2007) was used with thedecoding weights optimized by MERT (Och, 2003).4.2 EvaluationWe first evaluate the alignment quality.
The methoddiscussed in section 3 was used to compare char-acter and word alignment.
As can be seen fromTable 3, the systems using character as WSA out-performed the ones using word as WSA in bothsmall-scale (row 3-5) and large-scale task (row 6-8)with all segmentations.
This gain can be attributedto the small vocabulary size (sparsity) for characteralignment.
The observation is consistent withKoehn (2005) which claimed that there is a negativecorrelation between the vocabulary size and trans-lation performance without explicitly distinguish-ing WSA and WSR.We then evaluated the translation performance.The baselines are fully word-based MT systems(WordSys), i.e.
using word as both WSA and WSR,and fully character-based systems (CharSys).
Table5 http://www.ictclas.org/6 http://nlp.stanford.edu/software/segmenter.shtml287Word alignment Character alignmentP R F P R FSCTB 76.0 81.9 78.9 78.2 85.2 81.8PKU 76.1 82.0 79.0 78.0 86.1 81.9ICT 75.2 80.8 78.0 78.7 86.3 82.3LCTB 79.6 85.6 82.5 82.2 90.6 86.2PKU 80.0 85.4 82.6 81.3 89.5 85.2ICT 80.0 85.0 82.4 81.3 89.7 85.3Table 3 Alignment evaluation.
Precision (P), recall (R),and F-score (F) with ?
?
0.5 (Fraser and Marcu, 2007)WSA WSR CTB PKU ICTS word word 21.52 20.99 20.95char word 22.04 21.98 22.04L word word 22.07 22.86 22.23 char word 23.41 23.51 23.05Table 4 Translation evaluation of WordSys and pro-posed system using BLEU-SBP (Chiang et al, 2008)4 compares WordSys to our proposed system.
Sig-nificant testing was carried out using bootstrapre-sampling method proposed by Koehn (2004)with a 95% confidence level.
We see that our pro-posed systems outperformed WordSys in all seg-mentation specifications settings.
Table 5 lists theresults of CharSys in small-scale task.
In this setting,we gradually set the phrase length and the distortionlimits of the phrase-based decoder (context size) to7, 9, 11 and 13, in order to remove the disadvantageof shorter context size of using character as WSRfor fair comparison with WordSys as suggested byDuan et al (2010).
Comparing Table 4 and 5, wesee that all CharSys underperformed WordSys.
Thisobservation is consistent with Chang et al (2008)which claimed that using characters, even withlarge phrase length (up to 13 in our experiment)cannot always capture everything a Chinese wordsegmenter can do, and using word for translation isquite necessary.
We also see that CharSys under-performed our proposed systems, that?s because theharm of using character as WSR outweighed thebenefit of using character as WSA, which indicatedthat word segmentation better for alignment is notnecessarily better for translation, and vice versa.We finally compared our approaches to Ma et al(2007) and Ma and Way (2009), which proposed?packed word (PW)?
and ?bilingual motivatedword (BS)?
respectively.
Both methods iterativelylearn word segmentation and alignment alterna-tively, with the former starting from word-basedcorpus and the latter starting from characters-basedcorpus.
Therefore, PW can be experimented on allsegmentations.
Table 6 lists their results in small-Context Size 7 9 11 13BLEU 20.90 21.19 20.89 21.09Table 5 Translation evaluation of CharSys.System WSA WSR CTB PKU ICTWordSys word word 21.52 20.99 20.95Proposed char word 22.04 21.98 22.04PW PW PW 21.24 21.24 21.19Char+PW char PW 22.46 21.87 21.97BS BS BS 19.76Char+BS char BS 20.19Table 6 Comparison with other worksscale task, we see that both PW and BS underper-formed our approach.
This may be attributed to thelow recall of the learned BS or PW in their ap-proaches.
BS underperformed both two baselines,one reason is that Ma and Way (2009) also em-ployed word lattice decoding techniques (Dyer et al,2008) to tackle the low recall of BS, which wasremoved from our experiments for fair comparison.Interestingly, we found that using character asWSA and BS as WSR (Char+BS), a moderate gain(+0.43 point) was achieved compared with fullyBS-based system; and using character as WSA andPW as WSR (Char+PW), significant gains wereachieved compared with fully PW-based system,the result of CTB segmentation in this setting evenoutperformed our proposed approach (+0.42 point).This observation indicated that in our framework,better combinations of WSA and WSR can be foundto achieve better translation performance.5 Conclusions and Future WorkWe proposed a SMT framework that uses characterfor alignment and word for translation, which im-proved both alignment quality and translation per-formance.
We believe that in this framework, usingother finer-grained segmentation, with fewer am-biguities than character, would better parameterizethe alignment models, while using other coars-er-grained segmentation as WSR can help capturemore linguistic knowledge than word to get bettertranslation.
We also believe that our approach, ifintegrated with combination techniques (Dyer et al,2008; Xi et al, 2011), can yield better results.AcknowledgmentsWe thank ACL reviewers.
This work is supportedby the National Natural Science Foundation ofChina (No.
61003112), the National FundamentalResearch Program of China (2010CB327903).288ReferencesPeter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPeitra, and Robert L. Mercer.
1993.
The mathematicsof statistical machine translation: parameter estima-tion.
Computational Linguistics, 19(2), pages263-311.Pi-Chuan Chang, Michel Galley, and Christopher D.Manning.
2008.
Optimizing Chinese word segmenta-tion for machine translation performance.
In Pro-ceedings of third workshop on SMT, pages 224-232.David Chiang, Steve DeNeefe, Yee Seng Chan andHwee Tou Ng.
2008.
Decomposability of TranslationMetrics for Improved Evaluation and Efficient Algo-rithms.
In Proceedings of Conference on EmpiricalMethods in Natural Language Processing, pages610-619.Tagyoung Chung and Daniel Gildea.
2009.
Unsuper-vised tokenization for machine translation.
In Pro-ceedings of Conference on Empirical Methods inNatural Language Processing, pages 718-726.Michael Collins.
1999.
Head-driven statistical modelsfor natural language parsing.
Ph.D. thesis, Universityof Pennsylvania.Xiangyu  Duan, Min Zhang,  and  Haizhou Li.
2010.Pseudo-word for phrase-based machine translation.
InProceedings of the Association for ComputationalLinguistics, pages 148-156.Christopher Dyer, Smaranda Muresan, and Philip Resnik.2008.
Generalizing word lattice translation.
In Pro-ceedings of the Association for Computational Lin-guistics, pages 1012-1020.Jakob Elming and Nizar Habash.
2007.
Combination ofstatistical word alignments based on multiple pre-processing schemes.
In Proceedings of the Associa-tion for Computational Linguistics, pages 25-28.Alexander Fraser and Daniel Marcu.
2007.
Squibs andDiscussions: Measuring Word Alignment Quality forStatistical Machine Translation.
In ComputationalLinguistics, 33(3), pages 293-303.Aria Haghighi, John Blitzer, John DeNero, and DanKlein.
2009.
Better word alignments with supervisedITG models.
In Proceedings of the Association forComputational Linguistics, pages 923-931.Phillip Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan,W.
Shen, C.Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, E.Herbst.
2007.
Moses: Open source toolkit for statis-tical machine translation.
In Proceedings of the Asso-ciation for Computational Linguistics, pages 177-180.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proceedings of theConference on Empirical Methods on Natural Lan-guage Processing, pages 388-395.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In Proceedings of theMT Summit.Adam David Lopez.
2008.
Machine translation by pat-tern matching.
Ph.D. thesis, University of Maryland.Yanjun Ma, Nicolas Stroppa, and Andy Way.
2007.Bootstrapping word alignment via word packing.
InProceedings of the Association for ComputationalLinguistics, pages 304-311.Yanjun Ma and Andy Way.
2009.
Bilingually motivateddomain-adapted word segmentation for statisticalmachine translation.
In Proceedings of the Conferenceof the European Chapter of the ACL, pages 549-557.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of theAssociation for Computational Linguistics, pages440-447.Franz Josef Och and Hermann  Ney.
2003.
A systematiccomparison of various statistical alignment models.Computational Linguistics, 29(1), pages 19-51.Michael Paul, Andrew Finch and Eiichiro Sumita.
2010.Integration of multiple bilingually-learned segmenta-tion schemes into statistical machine translation.
InProceedings of the Joint Fifth Workshop on StatisticalMachine Translation and MetricsMATR, pages400-408.J?rg Tiedemann.
2009.
Character-based PSMT forclosely related languages.
In Proceedings of the An-nual Conference of the European Association formachine Translation, pages 12-19.David Vilar, Jan-T. Peter and Hermann Ney.
2007.
Canwe translate letters?
In Proceedings of the SecondWorkshop on Statistical Machine Translation, pages33-39.Xinyan Xiao, Yang Liu, Young-Sook Hwang, Qun Liuand Shouxun Lin.
2010.
Joint tokenization andtranslation.
In Proceedings of the 23rd InternationalConference on Computational Linguistics, pages1200-1208.Ning  Xi, Guangchao  Tang,  Boyuan  Li, and  YinggongZhao.
2011.
Word alignment combination over mul-tiple word segmentation.
In Proceedings of the ACL2011 Student Session, pages 1-5.Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita.2008.
Improved statistical machine translation bymultiple Chinese word segmentation.
In Proceedings289of the Third Workshop on Statistical Machine Trans-lation, pages 216-223.290
