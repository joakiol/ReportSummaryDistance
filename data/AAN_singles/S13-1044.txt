Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conferenceand the Shared Task, pages 317?327, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational LinguisticsBootstrapping Semantic Role Labelers from Parallel DataMikhail Kozhevnikov Ivan TitovSaarland University, Postfach 15 11 5066041 Saarbru?cken, Germany{mkozhevn|titov}@mmci.uni-saarland.deAbstractWe present an approach which uses the sim-ilarity in semantic structure of bilingual par-allel sentences to bootstrap a pair of seman-tic role labeling (SRL) models.
The settingis similar to co-training, except for the inter-mediate model required to convert the SRLstructure between the two annotation schemesused for different languages.
Our approachcan facilitate the construction of SRL modelsfor resource-poor languages, while preservingthe annotation schemes designed for the tar-get language and making use of the limited re-sources available for it.
We evaluate the modelon four language pairs, English vs German,Spanish, Czech and Chinese.
Consistent im-provements are observed over the self-trainingbaseline.1 IntroductionThe success of statistical modeling methods in a va-riety of natural language processing (NLP) tasks inthe last decade depended crucially on the availabilityof annotated resources for their training.
And whilesizable resources for most standard tasks are onlyavailable for a few languages, the human effort re-quired to achieve reasonable performance on suchtasks for other languages may be significantly re-duced by leveraging existing resources and the sim-ilarities between languages.This idea has lead to the development of cross-lingual annotation projection approaches, whichmake use of parallel corpora (Pado?
and Lapata,2009), as well as attempts to adapt models directlyto other languages (McDonald et al 2011).
In thispaper we consider correspondences between SRLstructures in translated sentences from a differentperspective.
Most cross-lingual annotation projec-tion approaches transfer the source language anno-tation scheme to the target language without modifi-cation, which makes it hard to combine their outputwith existing target language resources, as annota-tion schemes may vary significantly.
We instead ad-dress the problem of information transfer betweentwo existing annotation schemes (figure 1) for a pairof languages using an intermediate model of rolecorrespondence (RCM).
An RCM models the prob-ability of a pair of corresponding arguments beingassigned a certain pair of roles.
We then use it toguide a pair of monolingual models toward compat-ible predictions on parallel data in order to extendthe coverage and/or accuracy of one or both models.Romanian is not taught in their schools .Ve ?kol?ch se neu??
rumunsky .A1PATAM-LOCLOCAM-NEGFigure 1: Role correspondence in parallel sentences, anexample.The notion of compatibility here is highly non-trivial, even for sentences translated as close to theoriginal as possible.
Zhuang and Zong (2010), forexample, observe that in the English-Chinese paral-lel PropBank (Palmer et al 2005b) correspondingarguments often bear different labels, even thoughthe same inventory of semantic roles is used for both317languages and the annotation guidelines are similar.When different annotation schemes are considered,the problem is further complicated by the differencein the granularity of semantic roles used and varyingnotions of what is an argument and what is not.Manually annotated training data for such a modelis hard to come by.
Instead, we propose an itera-tive procedure similar to bootstrapping, where theparameters of the RCM are initially estimated froma parallel corpus automatically annotated with se-mantic roles using the monolingual models indepen-dently, and then the RCM is used to refine these an-notations via a joint inference procedure, serving toenforce consistency on the predictions of monolin-gual models on parallel sentences.
The obtained an-notations on the parallel corpus are expected to beof higher quality than the independent predictions ofthe models, so they can be used to improve the SRLmodels?
performance and/or coverage.
We evalu-ate this approach by augmenting the original train-ing data with the annotations obtained on paralleldata and observing the change in the model?s perfor-mance.
This is especially useful if one of the lan-guages is relatively poor in resources, in which casethe proposed procedure will help propagate infor-mation from the stronger model to the weaker one.Even if the two models are comparable in their pre-dictive power, we may be able to benefit from thefact that certain semantic roles are realized less am-biguously in one language than in another.
We willhenceforth refer to these two alternatives as the pro-jection and symmetric setups.The paper is structured as follows.
In the next sec-tion we present our approach and discuss the issuesof role correspondence modeling, then describe theimplementation and datasets used in evaluation insection 3, present the evaluation and results in sec-tion 4 and conclude with the discussion of relatedwork in section 5.2 ApproachWe consider bootstrapping a pair of SRL models ona parallel corpus, using the correspondence betweentheir predictions on parallel sentences to guide thelearning.
The models are forced toward compatiblepredictions, where the notion of compatibility is de-fined by a (statistical) role correspondence model.Let us consider a pair of languages, ?
and ?,and their corresponding datasets T 0?
and T0?
, anno-tated with semantic roles (the upper indices here de-note the iteration number).
We will refer to theseas the initial training sets.
We also assume that aword-aligned parallel corpus is available for the pairof languages, which we denote P , with the pred-icates and their respective arguments identified onboth sides.The procedure is then as follows: we train mono-lingual models M0?
and M0?
on T0?
and T0?
, respec-tively, apply them to the two sides of the parallelcorpus, resulting in a labeling P 0.
We collect the se-mantic role co-occurrence information and train therole correspondence model C0 on it, then proceed tothe joint inference step involving M0?, M0?
and C0,resulting in a refined labeling P 1 of the parallel cor-pus.
The two sides of the P 1 are then used to aug-ment the initial training sets, yielding T 1?
and T1?
,and new models M1?
and M1?
are trained on these.The process can then be repeated using M1?
and M1?instead of the initial models.We report the model?s performance on a held-outtest set, drawn from the same corpus as the corre-sponding initial training set.The procedure can be seen as a form of co-training (Blum and Mitchell, 1998) of a pair ofmonolingual SRL models.
In our case, however, thequestion of the models?
agreement is not as trivial asin most applications of co-training, requiring a sta-tistical model of its own (Ci).In the low-resource (projection) setup our ap-proach is also similar to self-training with weak su-pervision coming from the stronger model.Note that although the approach is iterative, wehave observed no significant improvements from re-peating the procedure, possibly owing to the noiseintroduced by the errors in preprocessing.
In theevaluation we run only one iteration.
In the notationintroduced above, the self-training baseline model(SELF) is trained on P 0?
, the joint model (JOINT) ?on P 1?
and the combined model (COMB) ?
on T1?
.2.1 Modeling Role CorrespondenceIt is necessary to distinguish between semanticroles and their interpretation in a particular con-text.
The former can be defined in a variety of318ways, depending on the formalism used.
In case ofFrameNet (Baker et al 1998), for example, the in-terpretation of a semantic role (frame element) is ex-plicitly provided for each separate frame, so a frameand a frame element label together describe the se-mantics of an argument.
PropBank (Palmer et al2005a) follows a mixed strategy ?
the labels for arelatively small set of core roles are numbered andtheir interpretations are provided separately for eachpredicate (although those of the first two roles, A0and A1, consistently denote what is known as Proto-Agent and Proto-Patient), while modifiers (Merloand Leybold, 2001) bear labels that are interpretedconsistently across all predicates.
Other resources,such as Prague Dependency Treebank (Hajic?
et al2006), use a single set of semantic roles (functors),which are interpretable across different predicates.From the standpoint of defining the semantic sim-ilarity of parallel sentences, the important implica-tion is that we cannot assume that the correspondingarguments should bear the same label, even if theannotation schemes used are compatible (Zhuangand Zong, 2010).
Nor can we write down a singlemapping between the roles that will be valid acrossdifferent predicates (figure 2), which motivates theneed for a statistical model of semantic role corre-spondence.I do not have these concernsYo no tengo tales preocupacionesA0arg1-temA1arg2-atrParliament adopted the resolutionEl Parlamento aprueba la resoluci?nA0arg0-agtA1arg1-patWe would like to know their namesNos gustar?a conocer sus nombresA0arg2-benA1arg1-temFigure 2: Predicate-specific role mapping.
Note that A0corresponds to art0-agt, art1-tem or art2-ben,depending on the predicate.We assume the existence of a one-to-one map-ping between semantic roles for a given predicatepair.
As the mappings are not completely indepen-dent ?
at least some roles have the same interpre-tation across different predicate pairs, ?
we chooseto build a single model, which relies on features de-rived from the pair of predicates in question, ratherthan create a separate model for each predicate pair.The model can then make decisions specific to par-ticular predicates or predicate pairs, where sufficientdata has been observed or back off to a generic map-ping where there is not enough data.For the purpose of this study, we choose to sep-arately model the probability of a target role, giventhe source one and the necessary contextual infor-mation and vice versa.
These two components arereferred to as projection models and realized as apair of linear classifiers.Training such a model in a conventional fash-ion would require a rather specific kind of dataset,namely a parallel corpus annotated with semanticroles, and assuming the availability of such datawould severely limit the applicability of the ap-proach proposed, as, to our knowledge, it is cur-rently only available for two language pairs, namelyEnglish-Chinese (Palmer et al 2005b) and English-Czech (Hajic?
et al 2012).
We instead use the auto-matically produced annotations on a parallel corpus,effectively enforcing consistency on the role corre-spondence in the monolingual models?
predictions.2.2 Joint InferenceThe joint inference would have been simplest if thearguments were classified independently.
This as-sumption is too restrictive, though, since the inter-dependencies between the arguments can be usedto improve the accuracy of semantic role label-ing (Roth and Yih, 2005).2.2.1 Projection SetupIn the projection setup we assume that the modelfor one of the languages, which we will henceforthrefer to as source, is much better informed thanthe one for the other language, referred to as tar-get, so we only have to propagate the informationone way.
The scoring functions of these two mod-els will be denoted fs and ft, respectively, and thatof the projection model from source to target ?
fst.Source and target sentences are denoted Ss and St,319and aligned predicates in these sentences ?
ps andpt.
The task is then to identify the target languagerole assignment rt that would maximize the objec-tive L(rt) = ?tft(rt, St, pt) + ?stfst(rt, rs, ps, pt),where rs = argmaxrfs(rs, Ss, ps) is the role as-signment of the source-side arguments as predictedby the monolingual model and ?
are the weights as-sociated with the models.The exact maximization of this objective is com-putationally expensive, so we resort to an approx-imation.
We chose to use the dual decompositionmethod primarily because it fits the structure of theobjective particularly well (in that it is a sum of theobjectives of two independent models) and since itallows a wide range of monolingual models to beused in this setup.
The only requirement here is thatthe monolingual model must be able to incorporatea bias toward or away from a certain prediction.To apply this approximation, we decouple thert variables into rt and rst and get L1(rt, rst) =?tft(rt, St, pt) + ?stfst(rst, rs, ps, pt) under thecondition that rt = rst.
Applying the Lagrangianrelaxation, we replace the hard equality constrainton rt and rst with a soft one, using slack variables ?,which results in the following objective:min?maxrt,rstL?1(rt, rst, ?)
=?tft(rt, St, pt) + ?stfst(rst, rs, ps, pt)+ (1)?i?r?Rt?i,r(I(rit = r)?
I(rist = r)),where i indexes aligned argument pairs and I is anindicator function.
This is equivalent tomin?maxrt,rstL?1(rt, rst, ?)
=min?
(maxrtgt(rt, St, pt, ?
)+ (2)maxrstgst(rst, rs, ps, pt, ?
)),wheregt(rt, St, pt, ?)
=?tft(rt, St, pt) +?i?r?Rt?i,rI(rit = r)gstp(rst, rs, ps, pt, ?)
= (3)?stfst(rst, rs, ps, pt)?
?i?r?Rt?i,rI(rist = r)are the augmented objectives of the two componentmodels, incorporating bias factors on various possi-ble predictions.The minimization with respect to ?
is per-formed using a subgradient descent algorithm fol-lowing Sontag et al(2011).
Whenever the methodconverges, it converges to the global maximum ofthe sum of the objectives.
We found that in our caseit reaches a solution within the first 1000 iterationsover 99% of the time.2.2.2 Symmetric SetupIf the models have comparable accuracy, theabove inference procedure can be extended to per-form projection both ways.
Formulating this as adual decomposition problem would require usingthree separate components, two for the monolingualmodels and one for the RCM, which would have tomake its own predictions for the semantic roles onboth sides without conditioning on the predictionsof the monolingual models.
This calls for a differentkind of model than the one we use ?
a model thatwill rely on a (possibly simplified) feature represen-tation of the source and target arguments to jointlypredict their labels.
Instead, we perform the pro-jection setup inference procedure in both directionssimultaneously, interleaving gradient descent stepsand allowing the projection models to access the up-dated predictions of the monolingual models.
Thisresults in a block gradient descent algorithm with thefollowing updates:rn+1t = argmaxrtgt(rt, St, pt, ?nt )rn+1s = argmaxrtgs(rs, Ss, ps, ?ns )rn+1st = argmaxrstgst(rst, rns , ps, pt, ?nt )rn+1ts = argmaxrtsgts(rts, rnt , pt, ps, ?ns ) (4)?i?r?Rs?n+1,i,rs = ?n,i,rs +?s(n)(I(rn,its = r)?
I(rn,is = r))?i?r?Rt?n+1,i,rt = ?n,i,rt +?t(n)(I(rn,ist = r)?
I(rn,it = r)),where ?s(n) = ?t(n) =?0n+1 is the update rate func-tion for step n, and gs and gts are defined as in (3),but with the direction reversed.This procedure allows us to use the same RCMimplementation as in the projection setup.
More-over, the inference procedure for projection setup is320a special case of this one with ?s(n) set to 0.
Thealgorithm also demonstrates convergence similar tothat of the projection version, although it lacks theoptimality guarantees.3 Experimental SetupWe evaluate our approach on four language pairs,namely English vs German, Spanish, Czech andChinese, which we will denote en-de, en-es,en-cz and en-zh respectively.3.1 Parallel DataThe parallel data for the first three language pairsis drawn from Europarl v6 (Koehn, 2005) andfrom MultiUN (Eisele and Chen, 2010) for English-Chinese.
We applied Stanford Tokenizer for En-glish, tokenizer scripts (Koehn, 2005) providedwith the Europarl corpus to German, Spanish andCzech, and Stanford Chinese Segmenter (Chang etal., 2008) to Chinese, then performed POS-tagging,morphology tagging (where applicable) and depen-dency parsing using MATE-tools (Bohnet, 2010).Word alignments were acquired usingGIZA++ (Och and Ney, 2003) with its stan-dard settings.
Predicate identification on the paralleldata was done using the supervised classifiers ofthe monolingual SRL systems, except for German,where a simple heuristic had to be used instead,as only some of the predicates are marked inthe training data, which makes it hard to train asupervised classifier.
Following van der Plas et al(2011), we then retain only those sentences whereall identified predicates were aligned.In the experiments we used 50 thousand predicatepairs in each case, as increasing the amount furtherdid not yield noticeable benefits, while increasingthe running time.3.2 Annotated DataThe CoNLL?09 (Hajic?
et al 2009) datasets wereused as a source of annotated data for all languages.Only verbal predicates were considered and pre-dicted syntax was used in evaluation.We consider subsets of the training data in orderto emulate the scenario with a resource-poor lan-guage.
Due to the different sources the datasetswere derived from, sentences contain different pro-portions of annotated predicates depending on thelanguage.
The German corpus, for example, con-tains about 6 times fewer argument labels per sen-tence than the English one.
We will therefore in-dicate the sizes of the datasets used in the numberof argument labels they contain, referred to as in-stances, rather than the number of predicates or sen-tences.
The corpus for English, for example, con-tains 6.2 such instances per sentence on average.We use the 20 thousand instances of the availabledata as the training corpus for each language andsplit the rest equally between the development andthe test set.
The secondary (?out-of-domain?)
testsets are preserved as they are.In dependency-based SRL, only heads of syntac-tic constituents are marked with semantic roles.
Theheads of corresponding arguments may or may notalign, however, even if the arguments are lexicallyvery similar, because their syntactic structure maydiffer.
In general, one would have to identify thewhole phrase for each argument and take into ac-count the links between constituents, rather than sin-gle words (Pado?
and Lapata, 2005).
As reconstruct-ing the constituents from the dependency tree is non-trivial (Hwang et al 2010), we are using a heuristicto address the most common version of this problem,i.e.
a preposition or an auxiliary verb being an argu-ment head.
In such a case we also take into accountany alignment links involving the head?s immediatedescendants.3.3 ImplementationOur system is based on that of Bjo?rkelund et al(2009).
It is a pipeline system comprised of a set ofbinary or multiclass linear classifiers.
Both here andin the projection model, the classifiers are trainedusing Liblinear (Fan et al 2008).We employed a uniqueness constraint on role la-bels (Chang et al 2007), preventing some of themfrom being assigned to more than one argument inthe same predicate, which appears to be more reli-able in a low-resource setting we consider than thereranker the original system employed.
The con-straint is enforced in the monolingual model infer-ence using a beam-search approximation with thebeam size of 10.
The label uniqueness informationwas derived from the training sets.3213.4 The Projection ModelEach projection model is realized by a single lin-ear classifier applied to each argument pair indepen-dently.
It relies on features derived from the sourcesemantic role and source and target predicates, andpredicts the semantic role for the argument in thetarget sentence.The features include the source semantic role andits conjunctions with (lowercased) forms and lem-mata of the source and target predicates.
For ex-ample, assuming the source semantic role is A3 andthe source and target predicates are went and ging(past tense of ?gehen?, German), the features wouldbe as shown in figure 3.FORMPAIR=A3-went-gingLEMMAPAIR=A3-go-gehenFORMSRC=A3-wentFORMTGT=A3-gingLEMMASRC=A3-goLEMMATGT=A3-gehenLABEL=A3Figure 3: Projection model features example.3.5 ParametersIn case of projection there are two parameters, ?stand ?t, ?
the weights of the component models in theobjective.
Only their relative values matter (exceptin the choice of ?0), so we set ?t to 1 and only tunethe weight of the role correspondence model.In the symmetric setup, the objective takesthe form L(rt, rs) = ?tft(rt, St, pt) +?stfst(rt, rs, ps, pt) + ?sfs(rs, Ss, ps) +?tsfts(rs, rt, pt, ps).
Since we assume that thetwo monolingual models here have comparableperformance, we do not tune their relative weights,setting both ?s and ?t to 1.We also use the same weight for both projectionmodels, ?st = ?ts, and this value plays an importantrole ?
it basically indicates how strongly we insiston the role correspondence models?
correctness.
Ifthis weight is set to 0, the RCM will accept the ini-tial predictions the monolingual models make, and ifit is set to a sufficiently large value, the predictionsof the monolingual models will be biased until theymatch the mapping suggested by the RCM.
The op-timal weight will therefore depend on the languagepair, the sizes of the initial training sets and the RCMused.
We use the value of 0.7 in all projection ex-periments and 0.5 in the symmetric setup, however,as excessive tuning may be undesirable in the low-resource setting.3.6 DomainsOne important factor in the understanding of theevaluation figures presented is the fact that sourcesof annotated and parallel data belong to different do-mains.
The former usually contains some sort ofnewswire text ?
Wall Street Journal in case of En-glish, Xinhua newswire, Hong Kong news and Sino-rama news magazine for Chinese, etc.
Parallel data,on the other hand, comes from the proceedings ofEuropean Parliament and United Nations, which arequite different.
For example, the sentences in thelatter domain often start with someone being ad-dressed, either by name or by title, which can hardlybe expected to occur as often in a newspaper or amagazine article.As is well-known, the performance of many sta-tistical tools drops significantly outside the domainthey were trained on (Pradhan et al 2008), and thepreprocessing and SRL models used here are no ex-ception, which results in relatively low quality ofthe initial predictions on the parallel text.
The lowargument identification performance, in particular,is presumably due to inaccurate dependency parses,on which it heavily relies.
Several approached havebeen proposed to improve the accuracy of depen-dency parsers and other tools on out-of-domain data,but this is beyond the scope of this paper.
In somecases (though seldom), sources of parallel data be-longing to the same domain as the annotated trainingdata can be obtained.Another concern is that the performance of amodel trained on automatically labeled parallel dataas measured on a test set we use may not reflect thequality of these annotations.
To assess the resultingmodel?s coverage, it would be interesting to evaluateit on data outside the original domain, so we con-sider the out-of-domain (OOD) test sets as providedfor the CoNLL Shared Task 2009 where available.Perhaps the most interesting one of these is theGerman OOD test set, which is drawn from Europarl(as is the parallel data we use).
It was originallyannotated with syntactic dependency trees and se-322mantic structure in the SALSA format (Burchardtet al 2006) for Pado?
and Lapata (2005), and thenconverted into a PropBank-like form for the CoNLLShared Task 2009 (Hajic?
et al 2009).
The OODtest set for English is drawn from the Brown cor-pus (Francis and Kucera, 1967) and the one forCzech ?
from a Czech translation of Wall StreetJournal articles (Hajic?
et al 2012).4 EvaluationThe first question we are interested in is how thejoint inference affects the quality of the automati-cally obtained annotations on the parallel data.
Toanswer this, we will run the monolingual models in-dependently and jointly, then train models on theoutput of these two procedures and compare theirperformance on a test set.
Note that we do not addthe initial training data at this point, so the initialmodel scores are provided for reference, rather thanas a baseline.4.1 Projection SetupA small initial training set of 600 instances was usedhere for the target language here and the full trainingset (20000 instances) for the source one.
?st was setto 0.7 in all experiments in this section.INIT SELF JOINT ?SELFen-cz* 61.11 60.68 63.01 2.33en-cz 62.45 62.15 63.11 0.96en-de* 66.81 63.96 67.64 3.69en-de 70.40 68.34 70.13 1.79en-es 64.20 64.51 66.01 1.50en-zh 75.80 73.52 74.87 1.35cz-en* 66.82 63.95 64.97 1.02cz-en 74.92 71.60 71.90 0.29de-en* 66.82 63.58 63.21 -0.37de-en 74.93 71.31 70.72 -0.59es-en* 66.82 63.95 64.18 0.23es-en 74.93 71.47 72.09 0.62zh-en* 66.82 64.51 63.67 -0.83zh-en 74.93 72.26 71.24 -1.01Table 1: Projection setup results: self-training baseline,refined model and the difference in their performance.Asterisk indicates out-of-domain test set, statistically sig-nificant improvements are highlighted in bold.In table 1, we present the accuracy of the modeltrained on the output of the joint inference (JOINT)against that of the self-training baseline (SELF).
The?SELF column contains the difference between thetwo.
Note that the SELF model is trained on theparallel data automatically annotated using mono-lingual SRL models (not mixed with the initial train-ing set), since we are interested in the effect of jointinference on the quality of the annotation obtained.Where the improvement is positive and statisticallysignificant with p < 0.005 according to the permuta-tion test (Good, 2000), they are highlighted in bold.We can see that the refined model (JOINT) outper-forms the self-training baseline in most cases by amoderate, but statistically significant margin, whichindicates that the joint inference does improve thequality of annotations on the parallel corpus.The slightly higher improvement on the GermanOOD test set supports our hypothesis that the proce-dure enhances the performance of the model on par-allel data, as the data for this test set is also drawnfrom the Europarl corpus.
The improvement overthe initial model (?INIT) in this case is statisticallysignificant with p < 0.05.
Higher p-value may beattributed to the smaller test set size.Figure 4 shows how the performance of the JOINTmodel changes with the size of the initial trainingset.
The improvements are smaller for en-cz, en-de and en-zh, but they are also statistically signifi-cant for initial training sets of up to 2000 instances.Projection to English from other languages performsworse.Figure 4: Projection setup, English-Spanish, model per-formance as a function of the size of the initial trainingset.3234.2 CombiningIn practice, automatically obtained annotations areusually combined with the existing labeled data.
Forthis purpose, the initial training set is replicated soas to constitute 0.3 (an empirically chosen value thatappears to work well in most experiments) of thesize of the automatically labeled dataset.
We com-pare the performance of the model trained on the re-sulting dataset (COMB) with that of the JOINT modeland the initial models.
The results are presented intable 2.
We omit projection from other languages toEnglish, since the JOINT model there fails to outper-form the initial model and we do not expect to ben-efit from adding the automatically annotated data tothe initial training set in this case.INIT JOINT COMB ?JOINT ?INITen-cz* 61.11 63.01 62.98 -0.03 1.87en-cz 62.45 63.11 63.30 0.19 0.85en-de* 66.81 67.64 67.64 0.00 0.84en-de 70.39 70.19 70.53 0.34 0.15en-es 64.20 66.01 66.01 0.00 1.81en-zh 75.80 74.87 75.03 0.16 -0.77Table 2: The effect of adding automatically obtained an-notation to the initial training set.
Asterisk indicates out-of-domain test set, statistically significant improvementsare highlighted in bold.4.3 Symmetric SetupIn the symmetric setup evaluation, we use a slightlylarger initial training set of 1400 instances for bothsource and target language.
The projection modelweight is set to 0.5.
Table 3 shows the accuracy ofthe JOINT model and the SELF baseline.Note that here, unlike section 4.1, the joint in-ference is run once and then a model is trained foreach language and evaluated on the correspondingtest set(s).The results support our intuition that joint infer-ence helps improve the quality of the resulting an-notations, at least in some cases.4.4 Oracle RCMIt would be useful to know to what extent the per-formance of the role correspondence model affectsthe quality of the output (and thus the performanceof the resulting model).
The RCM we use is ratherINIT SELF JOINT ?SELFen-cz* 67.07 66.15 68.18 2.02en-cz 67.56 66.42 66.72 0.30en-de* 67.64 66.72 68.57 1.84en-de 75.13 71.97 73.57 1.60en-es 68.14 67.80 69.04 1.24en-zh 76.28 72.96 75.22 2.26cz-en* 69.37 66.45 66.22 -0.23cz-en 77.32 74.72 75.02 0.31de-en* 69.37 66.45 66.68 0.23de-en 77.32 73.56 73.72 0.17es-en* 69.37 66.64 66.40 -0.23es-en 77.32 74.05 74.89 0.84zh-en* 69.37 66.08 65.53 -0.56zh-en 77.32 74.48 74.25 -0.24Table 3: Comparing JOINT model against the self-training baseline in symmetric setup.
Asterisk indicatesout-of-domain test set, statistically significant improve-ments are highlighted in bold.simplistic, and we believe it can be substantially im-proved for any given language pair by incorporat-ing prior knowledge and/or using external sourcesof information.
In order to estimate the potentialimpact of such improvements, we simulate a betterinformed projection model, giving it access to thepredictions of more accurate monolingual models onthe parallel data ?
those trained on the full trainingset, rather than the initial training set used in this par-ticular experiment.
We refer to the resulting RCM asoracle and assess the difference it makes, comparedto a regular one (table 4).5 Related WorkThere is a number of approaches to semi-supervisedsemantic role labeling, and most suggest that someexternal supervision is required for such approachesto work (He and Gildea, 2006), such as measures ofsyntactic and semantic similarity (Fu?rstenau and La-pata, 2009) or external confidence measures (Gold-wasser et al 2011).
The alternative we propose isprimarily motivated by the research on annotationprojection (Pado?
and Lapata, 2009; van der Plaset al 2011; Annesi and Basili, 2010; Naseem etal., 2012) and direct transfer (Durrett et al 2012;S?gaard, 2011; Lopez et al 2008; McDonald et al2011).
The key difference of the present approachcompared to annotation projection is that we assume324INIT SELF JOINT ?SELF ?INITen-cz* 61.11 60.68 72.49 11.81 11.38en-cz 62.45 62.15 70.19 8.04 7.74en-de* 66.81 63.96 76.78 12.82 9.97en-de 70.39 68.34 79.22 10.88 8.84en-es 64.20 64.51 75.43 10.92 11.23en-zh 75.80 73.52 76.75 3.22 0.94cz-en* 66.82 63.95 70.75 6.80 3.93cz-en 74.93 71.60 79.70 8.10 4.76de-en* 66.82 63.58 69.46 5.88 2.64de-en 74.93 71.31 77.34 6.03 2.41es-en* 66.82 63.95 69.92 5.97 3.10es-en 74.93 71.47 79.55 8.08 4.62zh-en* 66.82 64.51 67.19 2.68 0.37zh-en 74.93 72.26 76.51 4.26 1.58Table 4: Oracle RCM performance, projection setup: ini-tial model, self-training baseline, refined model and itsimprovement over the other two.
Asterisk indicates out-of-domain test set, statistically significant improvementsare highlighted in bold.the availability of some amount of training data forthe target language, possibly using a different inven-tory of semantic roles.As mentioned previously, from the training pointof view this approach can be seen as similar to co-training (Blum and Mitchell, 1998), other applica-tions of which to NLP are too numerous to list here.Most closely related is the joint inference inZhuang and Zong (2010), the main difference beingthat it relies on a manually annotated parallel corpus,aligned on the argument level, and evaluates only theinference procedure and only on in-domain data.Other related approaches include Kim et al(2010), where a cross-lingual transfer of relationsis performed (which basically represent parts ofthe predicate-argument structure considered by SRLmethods), and Frermann and Bond (2012), wheresemantic structure matching is used to rank HPSGparses for parallel sentences.Unsupervised semantic role labeling meth-ods (Lang and Lapata, 2010; Lang and Lapata,2011; Titov and Klementiev, 2012a; Lorenzo andCerisara, 2012) present an alternative to the cross-lingual information propagation approaches such asours, and at least one the methods in this area alsomakes use of parallel data (Titov and Klementiev,2012b).ConclusionsWe have presented an approach to information trans-fer between SRL systems for different languagepairs using parallel data.
The task proves challeng-ing due to non-trivial mapping between the role la-bels used in different SRL annotation schemes andthe nature of parallel data ?
the difference in do-mains and the limited accuracy of the preprocess-ing tools.
We observe consistent improvements overself-training baseline from using joint inference andthe experiments suggest that improving the role cor-respondence model, for example using language-specific prior knowledge or external data sources,may dramatically increase the performance of the re-sulting system.AcknowledgmentsThe authors acknowledge the support of the MMCICluster of Excellence and thank Alexandre Klemen-tiev and Manfred Pinkal for valuable suggestions.ReferencesPaolo Annesi and Roberto Basili.
2010.
Cross-lingualalignment of framenet annotations through hiddenmarkov models.
In Proceedings of the 11th interna-tional conference on Computational Linguistics andIntelligent Text Processing, CICLing?10, pages 12?25,Berlin, Heidelberg.
Springer-Verlag.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet project.
In Pro-ceedings of the Thirty-Sixth Annual Meeting of theAssociation for Computational Linguistics and Sev-enteenth International Conference on ComputationalLinguistics (ACL-COLING?98), pages 86?90, Mon-treal, Canada.Anders Bjo?rkelund, Love Hafdell, and Pierre Nugues.2009.
Multilingual semantic role labeling.
In Pro-ceedings of the Thirteenth Conference on Computa-tional Natural Language Learning (CoNLL 2009):Shared Task, pages 43?48, Boulder, Colorado, June.Association for Computational Linguistics.Avrim Blum and Tom Mitchell.
1998.
Combining la-beled and unlabeled data with co-training.
In Proceed-ings of the Workshop on Computational Learning The-ory (COLT 98).Bernd Bohnet.
2010.
Top accuracy and fast dependencyparsing is not a contradiction.
In Proceedings of the23rd International Conference on Computational Lin-325guistics (Coling 2010), pages 89?97, Beijing, China,August.
Coling 2010 Organizing Committee.Aljoscha Burchardt, Katrin Erk, Anette Frank, AndreaKowalski, Sebastian Pado, and Manfred Pinkal.
2006.The SALSA corpus: a German corpus resource forlexical semantics.
In Proceedings of LREC 2006,Genoa, Italy.M.W.
Chang, L. Ratinov, and D. Roth.
2007.
Guidingsemi-supervision with constraint-driven learning.
Ur-bana, 51:61801.Pi-Chuan Chang, Michel Galley, and Christopher D.Manning.
2008.
Optimizing chinese word segmen-tation for machine translation performance.
In Pro-ceedings of the Third Workshop on Statistical MachineTranslation, StatMT ?08, pages 224?232, Stroudsburg,PA, USA.
Association for Computational Linguistics.Greg Durrett, Adam Pauls, and Dan Klein.
2012.
Syntac-tic transfer using a bilingual lexicon.
In Proceedingsof the 2012 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, pages 1?11, Jeju Island,Korea, July.
Association for Computational Linguis-tics.Andreas Eisele and Yu Chen.
2010.
MultiUN: A multi-lingual corpus from united nation documents.
In Nico-letta Calzolari (Conference Chair), Khalid Choukri,Bente Maegaard, Joseph Mariani, Jan Odijk, SteliosPiperidis, Mike Rosner, and Daniel Tapias, editors,Proceedings of the Seventh International Conferenceon Language Resources and Evaluation (LREC?10),Valletta, Malta, May.
European Language ResourcesAssociation (ELRA).Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
LIBLINEAR: A li-brary for large linear classification.
Journal of Ma-chine Learning Research, 9:1871?1874.S.
Francis and H. Kucera.
1967.
Computing Analysisof Present-day American English.
Brown UniversityPress, Providence, RI.Lea Frermann and Francis Bond.
2012.
Cross-lingualparse disambiguation based on semantic correspon-dence.
In Proceedings of the 50th Annual Meetingof the Association for Computational Linguistics (Vol-ume 2: Short Papers), pages 125?129, Jeju Island, Ko-rea, July.
Association for Computational Linguistics.Hagen Fu?rstenau and Mirella Lapata.
2009.
Graphalignment for semi-supervised semantic role labeling.In Proceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing, pages 11?20, Singapore.D.
Goldwasser, R. Reichart, J. Clarke, and D. Roth.2011.
Confidence driven unsupervised semantic pars-ing.
In ACL.P.
Good.
2000.
Permutation Tests: A PracticalGuide to Resampling Methods for Testing Hypotheses.Springer.J.
Hajic?, J.
Panevova?, E.
Hajic?ova?, P. Sgall, P. Pajas,J.
S?te?pa?nek, J. Havelka, M.
Mikulova?, Z.
Z?abokrtsky`,and M.
S?evc???kova?-Raz??mova?.
2006.
Prague depen-dency treebank 2.0.
LDC.Jan Hajic?, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Anto`nia Mart?
?, Llu?
?sMa`rquez, Adam Meyers, Joakim Nivre, SebastianPado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,Nianwen Xue, and Yi Zhang.
2009.
The CoNLL-2009 shared task: Syntactic and semantic dependen-cies in multiple languages.
In Proceedings of the Thir-teenth Conference on Computational Natural Lan-guage Learning (CoNLL 2009): Shared Task, pages1?18, Boulder, Colorado.Jan Hajic?, Eva Hajic?ova?, Jarmila Panevova?, Petr Sgall,Ondr?ej Bojar, Silvie Cinkova?, Eva Fuc??
?kova?, MarieMikulova?, Petr Pajas, Jan Popelka, Jir???
Semecky?,Jana S?indlerova?, Jan S?te?pa?nek, Josef Toman, Zden?kaUres?ova?, and Zdene?k Z?abokrtsky?.
2012.
Announc-ing prague czech-english dependency treebank 2.0.In Nicoletta Calzolari (Conference Chair), KhalidChoukri, Thierry Declerck, Mehmet Ug?ur Dog?an,Bente Maegaard, Joseph Mariani, Jan Odijk, and Ste-lios Piperidis, editors, Proceedings of the Eight In-ternational Conference on Language Resources andEvaluation (LREC?12), Istanbul, Turkey, May.
Euro-pean Language Resources Association (ELRA).Shan He and Daniel Gildea.
2006.
Self-training andco-training for semantic role labeling: Primary report.Technical report, University of Rochester.Jena D. Hwang, Rodney D. Nielsen, and Martha Palmer.2010.
Towards a domain independent semantics:Enhancing semantic representation with constructiongrammar.
In Proceedings of the NAACL HLT Work-shop on Extracting and Using Constructions in Com-putational Linguistics, pages 1?8, Los Angeles, Cali-fornia, June.
Association for Computational Linguis-tics.Seokhwan Kim, Minwoo Jeong, Jonghoon Lee, andGary Geunbae Lee.
2010.
A cross-lingual annota-tion projection approach for relation detection.
In Pro-ceedings of the 23rd International Conference on Com-putational Linguistics, COLING ?10, pages 564?571,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Philipp Koehn.
2005.
Europarl: A Parallel Corpusfor Statistical Machine Translation.
In ConferenceProceedings: the tenth Machine Translation Summit,pages 79?86, Phuket, Thailand.
AAMT, AAMT.Joel Lang and Mirella Lapata.
2010.
Unsupervised in-duction of semantic roles.
In Human Language Tech-326nologies: The 2010 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 939?947, Los Angeles, Cal-ifornia, June.
Association for Computational Linguis-tics.Joel Lang and Mirella Lapata.
2011.
Unsupervised se-mantic role induction via split-merge clustering.
InProc.
of Annual Meeting of the Association for Com-putational Linguistics (ACL).Adam Lopez, Daniel Zeman, Michael Nossal, PhilipResnik, and Rebecca Hwa.
2008.
Cross-LanguageParser Adaptation between Related Languages.
InIJCNLP-08 Workshop on NLP for Less PrivilegedLanguages, pages 35?42, Hyderabad, India, January.Alejandra Lorenzo and Christophe Cerisara.
2012.
Un-supervised frame based semantic role induction: ap-plication to french and english.
In Proceedings of theACL 2012 Joint Workshop on Statistical Parsing andSemantic Processing of Morphologically Rich Lan-guages, pages 30?35, Jeju, Republic of Korea, July 12.Association for Computational Linguistics.Ryan McDonald, Slav Petrov, and Keith Hall.
2011.Multi-source transfer of delexicalized dependencyparsers.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,EMNLP ?11, pages 62?72, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Paola Merlo and Matthias Leybold.
2001.
Automaticdistinction of arguments and modifiers: the case ofprepositional phrases.
In Proceedings of the FifthComputational Natural Language Learning Workshop(CoNLL-2001), pages 121?128, Toulouse, France.Tahira Naseem, Regina Barzilay, and Amir Globerson.2012.
Selective sharing for multilingual dependencyparsing.
In Proceedings of the 50th Annual Meetingof the Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 629?637, Jeju Island, Ko-rea, July.
Association for Computational Linguistics.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1).Sebastian Pado?
and Mirella Lapata.
2005.
Cross-linguistic projection of role-semantic information.
InProceedings of Human Language Technology Confer-ence and Conference on Empirical Methods in Natu-ral Language Processing, pages 859?866, Vancouver,British Columbia, Canada.Sebastian Pado?
and Mirella Lapata.
2009.
Cross-lingualannotation projection for semantic roles.
Journal ofArtificial Intelligence Research, 36:307?340.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005a.
The Proposition Bank: An annotated corpusof semantic roles.
Computational Linguistics, 31:71?105.Martha Palmer, Nianwen Xue, Olga Babko-Malaya, Jiny-ing Chen, and Benjamin Snyder.
2005b.
A parallelProposition Bank II for Chinese and English.
In Pro-ceedings of the Workshop on Frontiers in Corpus An-notations II: Pie in the Sky, CorpusAnno ?05, pages61?67, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Sameer S. Pradhan, Wayne Ward, and James H. Martin.2008.
Towards robust semantic role labeling.
Compu-tational Linguistics, 34(2):289?310.Dan Roth and Wen-tau Yih.
2005.
Integer linear pro-gramming inference for conditional random fields.
InICML, pages 736?743.Anders S?gaard.
2011.
Data point selection for cross-language adaptation of dependency parsers.
In Pro-ceedings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics: Human LanguageTechnologies: short papers - Volume 2, HLT ?11,pages 682?686, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.David Sontag, Amir Globerson, and Tommi Jaakkola.2011.
Introduction to dual decomposition for in-ference.
In Suvrit Sra, Sebastian Nowozin, andStephen J. Wright, editors, Optimization for MachineLearning.
MIT Press.Ivan Titov and Alexandre Klementiev.
2012a.
ABayesian approach to unsupervised semantic role in-duction.
In Proc.
of European Chapter of the Associa-tion for Computational Linguistics (EACL).Ivan Titov and Alexandre Klementiev.
2012b.
Crosslin-gual induction of semantic roles.
In Proceedings ofthe 50th Annual Meeting of the Association for Com-putational Linguistics, Jeju Island, South Korea, July.Association for Computational Linguistics.Lonneke van der Plas, Paola Merlo, and James Hender-son.
2011.
Scaling up automatic cross-lingual seman-tic role annotation.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Linguis-tics: Human Language Technologies: short papers -Volume 2, HLT ?11, pages 299?304, Stroudsburg, PA,USA.
Association for Computational Linguistics.Tao Zhuang and Chengqing Zong.
2010.
Joint inferencefor bilingual semantic role labeling.
In Proceedings ofthe 2010 Conference on Empirical Methods in NaturalLanguage Processing, EMNLP ?10, pages 304?314,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.327
