Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 968?978,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsOpen-Domain Semantic Role Labeling by Modeling Word SpansFei HuangTemple University1805 N. Broad St.Wachman Hall 318fei.huang@temple.eduAlexander YatesTemple University1805 N. Broad St.Wachman Hall 303Ayates@temple.eduAbstractMost supervised language processing sys-tems show a significant drop-off in per-formance when they are tested on textthat comes from a domain significantlydifferent from the domain of the trainingdata.
Semantic role labeling techniquesare typically trained on newswire text, andin tests their performance on fiction isas much as 19% worse than their perfor-mance on newswire text.
We investigatetechniques for building open-domain se-mantic role labeling systems that approachthe ideal of a train-once, use-anywheresystem.
We leverage recently-developedtechniques for learning representations oftext using latent-variable language mod-els, and extend these techniques to onesthat provide the kinds of features that areuseful for semantic role labeling.
In exper-iments, our novel system reduces error by16% relative to the previous state of the arton out-of-domain text.1 IntroductionIn recent semantic role labeling (SRL) competi-tions such as the shared tasks of CoNLL 2005 andCoNLL 2008, supervised SRL systems have beentrained on newswire text, and then tested on bothan in-domain test set (Wall Street Journal text)and an out-of-domain test set (fiction).
All sys-tems tested on these datasets to date have exhib-ited a significant drop-off in performance on theout-of-domain tests, often performing 15% worseor more on the fiction test sets.
Yet the baselinefrom CoNLL 2005 suggests that the fiction textsare actually easier than the newswire texts.
Suchobservations expose a weakness of current super-vised natural language processing (NLP) technol-ogy for SRL: systems learn to identify semanticroles for the subset of language contained in thetraining data, but are not yet good at generalizingto language that has not been seen before.We aim to build an open-domain supervisedSRL system; that is, one whose performance onout-of-domain tests approaches the same level ofperformance as that of state-of-the-art systems onin-domain tests.
Importantly, an open-domain sys-tem must not use any new labeled data beyondwhat is included in the original training text whenrunning on a new domain.
This allows the sys-tem to be ported to any new domain without anymanual effort.
In particular, it ought to apply toarbitrary Web documents, which are drawn from ahuge variety of domains.Recent theoretical and empirical evidence sug-gests that the fault for poor performance on out-of-domain tests lies with the representations, or setsof features, traditionally used in supervised NLP.Building on recent efforts in domain adaptation,we develop unsupervised techniques for learningnew representations of text.
Using latent-variablelanguage models, we learn representations of textsthat provide novel kinds of features to our su-pervised learning algorithms.
Similar represen-tations have proven useful in domain-adaptationfor part-of-speech tagging and phrase chunking(Huang and Yates, 2009).
We demonstrate howto learn representations that are effective for SRL.Experiments on out-of-domain test sets show thatour learned representations can dramatically im-prove out-of-domain performance, and narrow thegap between in-domain and out-of-domain perfor-mance by half.The next section provides background informa-tion on learning representations for NLP tasks us-ing latent-variable language models.
Section 3presents our experimental setup for testing open-domain SRL.
Sections 4, 5, 6 describe our SRLsystem: first, how we identify predicates in open-domain text, then how our baseline technique968identifies and classifies arguments, and finally howwe learn representations for improving argumentidentification and classification on out-of-domaintext.
Section 7 presents previous work, and Sec-tion 8 concludes and outlines directions for futurework.2 Open-Domain Representations UsingLatent-Variable Language ModelsLet X be an instance set for a learning problem;for SRL, this is the set of all (sentence,predicate)pairs.
Let Y be the space of possible labels for aninstance, and let f : X ?
Y be the target func-tion to be learned.
A representation is a func-tion R: X ?
Z , for some suitable feature spaceZ (such as Rd).
A domain is defined as a dis-tribution D over the instance set X .
An open-domain system observes a set of training examples(R(x), f(x)), where instances x ?
X are drawnfrom a source domain, to learn a hypothesis forclassifying examples drawn from a separate targetdomain.Previous work by Ben-David et al (2007; 2009)uses Vapnik-Chervonenkis (VC) theory to provetheoretical bounds on an open-domain learningmachine?s performance.
Their analysis shows thatthe choice of representation is crucial to open-domain learning.
As is customary in VC the-ory, a good choice of representation must allowa learning machine to achieve low error rates dur-ing training.
Just as important, however, is thatthe representation must simultaneously make thesource and target domains look as similar to oneanother as possible.For open-domain SRL, then, the traditional rep-resentations are problematic.
Typical represen-tations in SRL and NLP use features of the lo-cal context to produce a representation.
For in-stance, one dimension of a traditional represen-tation R might be +1 if the instance contains theword ?bank?
as the head of a noun-phrase chunkthat occurs before the predicate in the sentence,and 0 otherwise.
Although many previous studieshave shown that these features allow learning sys-tems to achieve impressively low error rates dur-ing training, they also make texts from differentdomains look very dissimilar.
For instance, a fea-ture based on the word ?bank?
or ?CEO?
may becommon in a domain of newswire text, but scarceor nonexistent in, say, biomedical literature.In our recent work (Huang and Yates, 2009) weshow how to build systems that learn new rep-resentations for open-domain NLP using latent-variable language models like Hidden MarkovModels (HMMs).
An HMM is a generative prob-abilistic model that generates each word xi in thecorpus conditioned on a latent variable Yi.
EachYi in the model takes on integral values from 1 toK, and each one is generated by the latent variablefor the preceding word, Yi?1.
The distribution fora corpus x = (x1, .
.
.
, xN ) and a set of state vec-tors s = (s1, .
.
.
, sN ) is given by:P (x, s) =?iP (xi|si)P (si|si?1)Using Expectation-Maximization (Dempster etal., 1977), it is possible to estimate the distribu-tions for P (xi|si) and P (si|si?1) from unlabeleddata.
The Viterbi algorithm (Rabiner, 1989) canthen be used to produce the optimal sequence oflatent states si for a given instance x.
The outputof this process is an integer (ranging from 1 to K)for every word xi in the corpus.
We use the inte-ger value of si as a new feature for every xi in thesentence.In POS-tagging and chunking experiments,these learned representations have proven to meetboth of Ben-David et al?s criteria for open-domainrepresentations: first, they are useful in makingpredictions on the training text because the HMMlatent states categorize tokens according to dis-tributional similarity.
And second, it would bedifficult to tell two domains apart based on theirHMM labels, since the same HMM state can gen-erate similar words from a variety of domains.In what follows, we adapt these representation-learning concepts to open-domain SRL.3 Experimental SetupWe test our open-domain semantic role labelingsystem using data from the CoNLL 2005 sharedtask (Carreras and Ma`rquez, 2005).
We use thestandard training set, consisting of sections 02-21of the Wall Street Journal (WSJ) portion of thePenn Treebank, labeled with PropBank (Palmeret al, 2005) annotations for predicates and argu-ments.
We perform our tests on the Brown corpus(Kucera and Francis, 1967) test data from CoNLL2005, consisting of 3 sections (ck01-ck03) ofpropbanked Brown corpus data.
This test set con-sists of 426 sentences containing 7,159 tokens,804 propositions, and 2,177 arguments.
While the969training data contains newswire text, the test sen-tences are drawn from the domain of ?general fic-tion,?
and contain an entirely different style (orstyles) of English.
The data also includes a sec-ond test set of in-domain text (section 23 of theTreebank), which we refer to as the WSJ test setand use as a reference point.Every sentence in the dataset is automaticallyannotated with a number of NLP pipeline systems,including part-of-speech (POS) tags, phrase chunklabels (Carreras and Ma`rquez, 2003), named-entity tags, and full parse information by multipleparsers.
These pipeline systems are important forgenerating features for SRL, and one key reasonfor the poor performance of SRL systems on theBrown corpus is that the pipeline systems them-selves perform worse.
The Charniak parser, forinstance, drops from an F1 of 88.25 on the WSJtest to a F1 of 80.84 on the Brown corpus.
Forthe chunker and POS tagger, the drop-offs are lesssevere: 94.89 to 91.73, and 97.36 to 94.73.Toutanova et al (2008) currently have the best-performing SRL system on the Brown corpus testset with an F1 score of 68.81 (80.8 for the WSJtest).
They use a discriminative reranking ap-proach to jointly predict the best set of argu-ment boundaries and the best set of argument la-bels for a predicate.
Like the best systems fromthe CoNLL 2005 shared task (Punyakanok et al,2008; Pradhan et al, 2005), they also use featuresfrom multiple parses to remain robust in the faceof parser error.
Owing to the established difficultyof the Brown test set and the different domains ofthe Brown test and WSJ training data, this datasetmakes for an excellent testbed for open-domainsemantic role labeling.4 Predicate IdentificationIn order to perform true open-domain SRL, wemust first consider a task which is not formallypart of the CoNLL shared task: the task of iden-tifying predicates in a given sentence.
While thistask is almost trivial in the WSJ test set, whereall but two out of over 5000 predicates can be ob-served in the training data, it is significantly moredifficult in an open-domain setting.
In the Browntest set, 6.1% of the predicates do not appear in thetraining data, and 11.8% of the predicates appearat most twice in the training data (c.f.
1.5% of theWSJ test predicates that appear at most twice intraining).
In addition, many words which appearBaseline HMMFreq P R F1 P R F10 89.1 80.4 84.5 93.5 84.3 88.70-2 87.4 84.7 86.0 91.6 88.8 90.2all 87.8 92.5 90.1 90.8 96.3 93.5Table 1: Using HMM features in predicate iden-tification reduces error in out-of-domain tests by34.3% overall, and by 27.1% for OOV predicates.?Freq?
refers to frequency in the training data.There were 831 predicates in total; 51 never ap-peared in training and 98 appeared at most twice.as predicates in training may not be predicates inthe test set.
In an open-domain setting, therefore,we cannot rely solely on a catalog of predicatesfrom the training data.To address the task of open-domain predicateidentification, we construct a Conditional RandomField (CRF) (Lafferty et al, 2001) model with tar-get labels of B-Pred, I-Pred, and O-Pred (for thebeginning, interior, and outside of a predicate).We use an open source CRF software package toimplement our CRF models.1 We use words, POStags, chunk labels, and the predicate label at thepreceding and following nodes as features for ourBaseline system.
To learn an open-domain repre-sentation, we then trained an 80 state HMM on theunlabeled texts of the training and Brown test data,and used the Viterbi optimum states of each wordas categorical features.The results of our Baseline and HMM systemsappear in Table 1.
For predicates that never orrarely appear in training, the HMM features in-crease F1 by 4.2, and they increase the overall F1of the system by 3.5 to 93.5, which approachesthe F1 of 94.7 that the Baseline system achieveson the in-domain WSJ test set.
Based on these re-sults, we were satisfied that our system could findpredicates in open-domain text.
In all subsequentexperiments, we fall back on the standard evalua-tion in which it is assumed that the boundaries ofthe predicate are given.
This allows us to comparewith previous work.5 Semantic Role Labeling withHMM-based RepresentationsFollowing standard practice, we divide the SRLtask into two parts: argument identification and1Available from http://sourceforge.net/projects/crf/970argument classification.
We treat both sub-tasksas sequence-labeling problems.
During argumentidentification, the system must label each tokenwith labels that indicate either the beginning or in-terior of an argument (B-Arg or I-Arg), or a labelthat indicates the token is not part of an argument(O-Arg).
During argument classification, the sys-tem labels each token that is part of an argumentwith a class label, such as Arg0 or ArgM.
Follow-ing argument classification, multi-word argumentsmay have different classification labels for each to-ken.
We post-process the labels by changing themto match the label of the first token.
We use CRFsas our models for both tasks (Cohn and Blunsom,2005).Most previous approaches to SRL have reliedheavily on parsers, and especially constituencyparsers.
Indeed, when SRL systems use gold stan-dard parses, they tend to perform extremely well(Toutanova et al, 2008).
However, as several pre-vious studies have noted (Gildea, 2001; Pradhanet al, 2007), using parsers can cause problems foropen-domain SRL.
The parsers themselves maynot port well to new domains, or the features theygenerate for SRL may not be stable across do-mains, and therefore may cause sparse data prob-lems on new domains.
Our first step is thereforeto build an SRL system that relies on partial pars-ing, as was done in CoNLL 2004 (Carreras andMa`rquez, 2004).
We then gradually add in less-sparse alternatives for the syntactic features thatprevious systems derive from parse trees.During argument identification we use the fea-tures below to predict the label Ai for token wi:?
words: wi, wi?1, and wi+1?
parts of speech (POS): POS tags ti, ti?1,and ti+1?
chunk labels: (e.g., B-NP, I-VP, or O)chunk tags ci, ci?1, and ci+1?
combinations: citi, tiwi, citiwi?
NE: the named entity type ni of wi?
position: whether the word occurs beforeor after the predicate?
distance: the number of interveningtokens between wi and the target predicate?
POS before, after predicate: the POS tagof the tokens immediately preceding andfollowing the predicate?
Chunk before, after predicate: the chunktype of the tokens immediately precedingand following the predicate?
Transition: for prediction node Ai, we useAi?1and Ai+1 as featuresFor argument classification, we add the featuresbelow to those listed above:?
arg ID: the labels Ai produced by arg.identification (B-Arg, I-Arg, or O)?
combination: predicate + first argumentword, predicate+ last argument word,predicate + first argument POS, predicate+ last argument POS?
head distance: the number of tokensbetween the first token of the argumentphrase and the target predicate?
neighbors: the words immediately beforeand after the argument.We refer to the CRF model with these features asour Baseline SRL system; in what follows we ex-tend the Baseline model with more sophisticatedfeatures.5.1 Incorporating HMM-basedRepresentationsAs a first step towards an open-domain representa-tion, we use an HMM with 80 latent state values,trained on the unlabeled text of the training andtest sets, to produce Viterbi-optimal state valuessi for every token in the corpus.
We then add thefollowing features to our CRFs for both argumentidentification and classification:?
HMM states: HMM state values si, si?1,and si+1?
HMM states before, after predicate: thestate value of the tokens immediatelypreceding and following the predicateWe call the resulting model our Baseline+HMMsystem.5.2 Path FeaturesDespite all of the features above, the SRL sys-tem has very little information to help it determinethe syntactic relationship between a target predi-cate and a potential argument.
For instance, thesebaseline features provide only crude distance in-formation to distinguish between multiple argu-ments that follow a predicate, and they make itdifficult to correctly identify clause arguments orarguments that appear far from the predicate.
Oursystem needs features that can help distinguishbetween different syntactic relationships, withoutbeing overly sensitive to the domain.As a step in this direction, we introduce pathfeatures: features for the sequence of tokens be-971System P R F1Baseline 63.9 59.7 61.7Baseline+HMM 68.5 62.7 65.5Baseline+HMM+Paths 70.0 65.6 67.7Toutanova et al (2008) NR NR 68.8Table 2: Na?
?ve path features improve our base-line, but not enough to match the state-of-the-art.Toutanova et al do not report (NR) separate val-ues for precision and recall on this dataset.
Dif-ferences in both precision and recall between thebaseline and the other systems are statistically sig-nificant at p < 0.01 using the two-tailed Fisher?sexact test.tween a predicate and a potential argument.
Instandard SRL systems, these path features usuallyconsist of a sequence of constituent parse nodesrepresenting the shortest path through the parsetree between a word and the predicate (Gildea andJurafsky, 2002).
We substitute paths that do notdepend on parse trees.
We use four types of paths:word paths, POS paths, chunk paths, and HMMstate paths.
Given an input sentence labeled withPOS tags, and chunks, we construct path featuresfor a token wi by concatenating words (or tags orchunk labels) between wi and the predicate.
Forexample, in the sentence ?The HIV infection rateis expected to peak in 2010,?
the word path be-tween ?rate?
and predicate ?peak?
would be ?isexpected to?, and the POS path would be ?VBZVBD TO.
?Since word, POS, and chunk paths are all sub-ject to data sparsity for arguments that are far fromthe predicate, we build less-sparse path features byusing paths of HMM states.
If we use a reason-able number of HMM states, each category labelis much more common in the training data thanthe average word, and paths containing the HMMstates should be much less sparse than word paths,and even chunk paths.
In our experiments, we use80-state HMMs.We call the result of adding path features toour feature set the Baseline+HMM+Paths sys-tem((BL).
Table 2 shows the performance of ourthree baseline systems.
In this open-domain SRLexperiment, path features improve over the Base-line?s F1 by 6 points, and by 2.2 points overBaseline+HMM, although the improvement is notenough to match the state-of-the-art system byToutanova et alY1 Y2 Y6The is expected to peak in 2010Y3 Y4 Y5 Y7 Y8HIV infection rateFigure 1: The Span-HMM over the sentence.
Itshows the span of length 3.6 Representations for Word SpansDespite partial success in improving our baselineSRL system with path features, these features stillsuffer from data sparsity ?
many paths in thetest set are never or very rarely observed duringtraining, so the CRF model has little or no datapoints from which to estimate accurate parametersfor these features.
In response, we introduce la-tent variable models of word spans, or sequencesof words.
As with the HMM models above, thelatent states for word spans can be thought of asprobabilistic categories for the spans.
And like theHMM models, we can turn the word span modelsinto representations by using the state value for aspan as a feature in our supervised SRL system.Unlike path features, the features from our modelsof word spans consist of a single latent state valuerather than a concatenation of state values, and asa consequence they tend to be much less sparse inthe training data.6.1 Span-HMM RepresentationsWe build our latent-variable models of word spansusing variations of Hidden Markov Models, whichwe call Span-HMMs.
Figure 1 shows a graphi-cal model of a Span-HMM.
Each Span-HMM be-haves just like a regular HMM, except that it in-cludes one node, called a span node, that can gen-erate an entire span rather than a single word.
Forinstance, in the Span-HMM of Figure 1, node y5 isa span node that generates a span of length 3: ?isexpected to.
?Span-HMMs can be used to provide a singlecategorical value for any span of a sentence us-ing the usual Viterbi algorithm for HMMs.
Thatis, at test time, we generate a Span-HMM featurefor word wj by constructing a Span-HMM that hasa span node for the sequence of words between wjand the predicate.
We determine the Viterbi opti-mal state of this span node, and use that state as thevalue of the new feature.
In our example in Figure1, the value of span node y5 is used as a feature for972the token ?rate?, since y5 generates the sequenceof words between ?rate?
and the predicate ?peak.
?Notice that by using Span-HMMs to providethese features, we have condensed all paths in ourdata into a small number of categorical values.Whereas there are a huge number of variations tothe spans themselves, we can constrain the numberof categories for the Span-HMM states to a rea-sonable number such that each category is likely toappear often in the training data.
The value of eachSpan-HMM state then represents a cluster of spanswith similar delimiting words; some clusters willcorrelate with spans between predicates and argu-ments, and others with spans that do not connectpredicates and arguments.
As a result, Span-HMMfeatures are not sparse, and they correlate with thetarget function, making them useful in learning anSRL model.6.2 Parameter EstimationWe use a variant of the Baum-Welch algorithm totrain our Span-HMMs on unlabeled text.
In orderfor this to work, we need to provide Baum-Welchwith a modified view of the data so that span nodescan generate multiple consecutive words in a sen-tence.
First, we take every sentence S in our train-ing data and generate the set Spans(S) of all validspans in the sentence.
For efficiency?s sake, we useonly spans of length less than 15; approximately95% of the arguments in our dataset were within15 words of the predicate, so even with this re-striction we are able to supply features for nearlyall valid arguments.
The second step of our train-ing procedure is to create a separate data point foreach span of S. For each span t ?
Spans(S), weconstruct a Span-HMM with a regular node gen-erating each element of S, except that a span nodegenerates all of t. Thus, our training data containsmany different copies of each sentence S, with adifferent Span-HMM generating each copy.Intuitively, running Baum-Welch over this datameans that a span node with state k will be likelyto generate two spans t1 and t2 if t1 and t2 tend toappear in similar contexts.
That is, they shouldappear between words that are also likely to begenerated by the same latent state.
Thus, certainvalues of k will tend to appear for spans betweenpredicates and arguments, and others will tendto appear between predicates and non-arguments.This makes the value k informative for both argu-ment identification and argument classification.6.3 Memory ConsiderationsMemory usage is a major issue for our Span-HMM models.
We represent emission distribu-tions as multinomials over discrete observations.Since there are millions of different spans in ourdata, a straightforward implementation would re-quire millions of parameters for each latent stateof the Span-HMM.We use two related techniques to get around thisproblem.
In both cases, we use a second HMMmodel, which we call the base HMM to distin-guish from our Span-HMM, to back-off from theexplicit word sequence.
We use the largest num-ber of states for HMMs that can be fit into mem-ory.
Let S be a sentence, and let s?
be the sequenceof optimal latent state values for S produced byour base HMM.
Our first approach trains the Span-HMM on Spans(s?
), rather than Spans(S).
Ifwe use a small enough number of latent states inthe base HMM (in experiments, we use 10 latentstates), we drastically reduce the number of differ-ent spans in the data set, and therefore the num-ber of parameters required for our model.
We callthis representation Span-HMM-Base10.
As withour other HMM-based models, we use the largestnumber of latent states that will allow the result-ing model to fit in our machine?s memory ?
ourprevious experiments on representations for part-of-speech tagging suggest that more latent statesare usually better.While our first technique solves the memory is-sue, it also loses some of the power of our orig-inal Span-HMM model by using a very coarse-grained base HMM clustering of the text into 10categories.
Our second approach trains a separateSpan-HMM model for spans of different lengths.Since we need only one model in memory at atime, this allows each one to consume more mem-ory.
We therefore use base HMM models withmore latent states (up to 20) to annotate our sen-tences, and then train on the resulting Spans(s?
)as before.
With this technique, we produce fea-tures that are combinations of the state value forspan nodes and the length of the span, in orderto indicate which of our Span-HMM models thestate value came from.
We call this representationSpan-HMM-BaseByLength.6.4 Combining Multiple Span-HMMsSo far, our Span-HMM models produce one newfeature for every token during argument identifi-973System P R F1Baseline+HMM+Paths 70.0 65.6 67.7Toutanova et al NR NR 68.8Span-HMM-Base10 74.5 69.3 71.8Span-HMM-BaseByLength 76.3 70.2 73.1Multi-Span-HMM 77.0 70.9 73.8Table 3: Span-HMM features significantly im-prove over state-of-the-art results in out-of-domain SRL.
Differences in both precision and re-call between the baseline and the Span-HMM sys-tems are statistically significant at p < 0.01 usingthe two-tailed Fisher?s exact test.cation and classification.
While these new fea-tures may be very helpful, ideally we would likeour learned representations to produce multipleuseful features for the CRF model, so that theCRF can combine the signals from each featureto learn a sophisticated model.
Towards this goal,we train N independent versions of our Span-HMM-BaseByLength models, each with a ran-dom initialization for the Baum-Welch algorithm.Since Baum-Welch is a hill-climbing algorithm,it should find local, but not necessarily global,optima for the parameters of each Span-HMM-BaseByLength model.
When we decode each ofthe models on training and test texts, we will ob-tain N different sequences of latent states, onefor each locally-optimized model.
Thus we obtainN different, independent sources of features.
Wecall the CRF model with these N Span-HMM fea-tures the Multi-Span-HMM model(MSH); in ex-periments we use N = 5.6.5 Results and DiscussionResults for the Span-HMM models on the CoNLL2005 Brown corpus are shown in Table 3.
All threeversions of the Span-HMM outperform Toutanovaet al?s system on the Brown corpus, with theMulti-Span-HMM gaining 5 points in F1.
TheMulti-Span-HMM model improves over the Base-line+HMM+Paths model by 7 points in precision,and 5.3 points in recall.
Among the Span-HMMmodels, the use of more states in the Span-HMM-BaseByLength model evidently outweighed thecost of splitting the model into separate versionsfor different length spans.
Using multiple in-dependent copies of the Span-HMMs provides asmall (0.7) gain in precision and recall.
Dif-ferences among the different Span-HMM modelsSystem WSJ Brown DiffMulti-Span-HMM 79.2 73.8 5.4Toutanova et al (2008) 80.8 68.8 12.0Pradhan et al (2005) 78.6 68.4 10.2Punyakanok et al (2008) 79.4 67.8 11.6Table 4: Multi-Span-HMM has a much smallerdrop-off in F1 than comparable systems on out-of-domain test data vs in-domain test data.were not statistically significant, except that thedifference in precision between the Multi-Span-HMM and the Span-HMM-Base10 is significantat p < .1.Table 4 shows the performance drop-off for topSRL systems when applied to WSJ test data andBrown corpus test data.
The Multi-Span-HMMmodel performs near the state-of-the-art on theWSJ test set, and its F1 on out-of-domain datadrops only about half as much as comparable sys-tems.
Note that several of the techniques usedby other systems, such as using features from k-best parses or jointly modeling the dependenciesamong arguments, are complementary to our tech-niques, and may boost the performance of our sys-tem further.Table 5 breaks our results down by argumenttype.
Most of our improvement over the Baselinesystem comes from the core arguments A0 andA1, but also from a few adjunct types like AM-TMP and AM-LOC.
Figure 2 shows that when theargument is close to the predicate, both systemsperform well, but as the distance from the predi-cate grows, our Multi-Span-HMM system is bet-ter able to identify and classify arguments than theBaseline+HMM+Paths system.Table 6 provides results for argument identifi-cation and classification separately.
As Pradhan etal.previously showed (Pradhan et al, 2007), SRLsystems tend to have an easier time with portingargument identification to new domains, but areless strong at argument classification on new do-mains.
Our baseline system decreases in F-scorefrom 81.5 to 78.9 for argument identification, butsuffers a much larger 8% drop in argument classi-fication.
The Multi-Span-HMM model improvesover the Baseline in both tasks and on both testsets, but the largest improvement (6%) is in argu-ment classification on the Brown test set.To help explain the success of the Span-HMMtechniques, we measured the sparsity of our path974Overall A0 A1 A2 A3 A4 ADV DIR DIS LOC MNR MOD NEG PNC TMP R-A0 R-A1Num 2177 566 676 147 12 15 143 53 22 85 110 91 50 17 112 25 21BL 67.7 76.2 70.6 64.8 59.0 71.2 52.7 54.8 71.9 67.5 58.3 90.9 90.0 50.0 76.5 76.5 71.3MSH 73.8 82.5 73.6 63.9 60.3 73.3 50.8 52.9 70.0 70.3 52.7 94.2 92.9 51.6 81.6 84.4 75.7Table 5: SRL results (F1) on the Brown test corpus broken down by role type.
BL is the Base-line+HMM+Paths model, MSH is the Multi-Span-HMM model.
Column 8 to 16 are all adjuncts (AM-).We omit roles with ten or fewer examples.505560657075808590F1scoreWords between predicate and argumentMSHBLFigure 2: The Multi-Span-HMM (MSH) modelis better able to identify and classify argumentsthat are far from the predicate than the Base-line+HMM+Paths (BL) model.Test Id.F1 AccuracyBL WSJ 81.5 93.7Brown 78.9 85.8MSH WSJ 83.9 94.4Brown 80.3 91.9Table 6: Baseline (BL) and Multi-Span-HMM(MSH) performance on argument identification(Id.F1) and argument classification.and Span-HMM features.
Figure 3 shows the per-centage of feature values in the Brown corpus thatappear more than twice, exactly twice, or exactlyonce in the training data.
While word path fea-tures can be highly valuable when there is train-ing data available for them, only about 11% of theword paths in the Brown test set alo appeared atall in the training data.
POS and chunk paths fareda bit better (22% and 33% respectively), but eventhen nearly 70% of all feature values had no avail-able training data.
HMM and Span-HMM-Base10paths achieved far better success in this respect.Importantly, the improvement is mostly due to fea-tures that are seen often in training, rather than fea-tures that were seen just once or twice.
Thus Span-00.10.20.30.40.50.60.70.80.9FractionofFeatureValuesinBrownCorpusOccurs 1xin WSJOccurs 2xin WSJOccurs 3xor more inWSJFractionofFeatureValuesinBrownCorpusFigure 3: HMM path and Span-HMM features arefar more likely to appear often in training data thanthe word, POS, and chunk path features.
Over70% of Span-HMM-Base10 features in the Browncorpus appear at least three times during training;in contrast, fewer than 33% of chunk path featuresin the Brown corpus appear at all during training.HMMs derive their power as representations foropen-domain SRL from the fact that they providefeatures that are mostly the same across domains;80% of the features of our Span-HMM-Base10 inthe Brown corpus were observed at least once inthe training data.Table 7 shows examples of spans that wereclustered into the same Span-HMM state, alongwith word to either side.
All four examplesare cases where the Span-HMM-Base10 modelcorrectly tagged the following argument, but theBaseline+HMM+Paths model did not.
We can seethat the paths of these four examples are com-pletely different, but the words surrounding themare very similar.
The emission from a span nodeare very sparse, so the Span-HMM has unsurpris-ingly learned to cluster spans according to theHMM states that precede and follow the spannode.
This is by design, as this kind of distri-butional clustering is helpful for identifying andclassifying arguments.
One potentially interesting975Predicate Span B-Argpicked the things up frompassed through the barbed wire atcome down from Sundays tosat over his second rock inTable 7: Example spans labeled with the sameSpan-HMM state.
The examples are taken fromsentences where the Span-HMM-Base10 modelcorrectly identified the argument on the right, butthe Baseline+HMM+Paths model did not.question for future work is whether a less sparsemodel of the spans themselves, such as a Na?
?veBayes model for the span node, would yield a bet-ter clustering for producing features for semanticrole labeling.7 Previous WorkDeschact and Moens (2009) use a latent-variablelanguage model to provide features for an SRLsystem, and they show on CoNLL 2008 data thatthey can significantly improve performance whenlittle labeled training data is available.
They donot report on out-of-domain tests.
They use HMMlanguage models trained on unlabeled text, muchlike we use in our baseline systems, but they do notconsider models of word spans, which we found tobe most beneficial.
Downey et al (2007b) also in-corporate HMM-based representations into a sys-tem for the related task of Web information extrac-tion, and are able to show that the system improvesperformance on rare terms.Fu?rstenau and Lapata (2009b; 2009a) use semi-supervised techniques to automatically annotatedata for previously unseen predicates with seman-tic role information.
This task differs from oursin that it focuses on previously unseen predicates,which may or may not be part of text from a newdomain.
Their techniques also result in relativelylower performance (F1 between 15 and 25), al-though their tests are on a more difficult and verydifferent corpus.
Weston et al (2008) use deeplearning techniques based on semi-supervised em-beddings to improve an SRL system, though theirtests are on in-domain data.
Unsupervised SRLsystems (Swier and Stevenson, 2004; Grenagerand Manning, 2006; Abend et al, 2009) can natu-rally be ported to new domains with little trouble,but their accuracy thus far falls short of state-of-the-art supervised and semi-supervised systems.The disparity in performance between in-domain and out-of-domain tests is by no meansrestricted to SRL.
Past research in a variety ofNLP tasks has shown that parsers (Gildea, 2001),chunkers (Huang and Yates, 2009), part-of-speechtaggers (Blitzer et al, 2006), named-entity tag-gers (Downey et al, 2007a), and word sense dis-ambiguation systems (Escudero et al, 2000) allsuffer from a similar drop-off in performance onout-of-domain tests.
Numerous domain adapta-tion techniques have been developed to addressthis problem, including self-training (McClosky etal., 2006) and instance weighting (Bacchiani et al,2006) for parser adaptation and structural corre-spondence learning for POS tagging (Blitzer et al,2006).
Of these techniques, structural correspon-dence learning is closest to our technique in that itis a form of representation learning, but it does notlearn features for word spans.
None of these tech-niques have been successfully applied to SRL.8 Conclusion and Future WorkWe have presented novel representation-learningtechniques for building an open-domain SRL sys-tem.
By incorporating learned features fromHMMs and Span-HMMs trained on unlabeledtext, our SRL system is able to correctly iden-tify predicates in out-of-domain text with an F1of 93.5, and it can identify and classify argu-ments to predicates with an F1 of 73.8, out-performing comparable state-of-the-art systems.Our successes so far on out-of-domain tests bringhope that supervised NLP systems may eventuallyachieve the ideal where they no longer need newmanually-labeled training data for every new do-main.
There are several potential avenues for fur-ther progress towards this goal, including the de-velopment of more portable SRL pipeline systems,and especially parsers.
Developing techniques thatcan incrementally adapt to new domains withoutthe computational expense of retraining the CRFmodel every time would help make open-domainSRL more practical.AcknowledgmentsWe wish to thank the anonymous reviewers fortheir helpful comments and suggestions.976ReferencesOmri Abend, Roi Reichart, and Ari Rappoport.
2009.Unsupervised argument identification for semanticrole labeling.
In Proceedings of the ACL.Michiel Bacchiani, Michael Riley, Brian Roark, andRichard Sproat.
2006.
MAP adaptation of stochas-tic grammars.
Computer Speech and Language,20(1):41?68.Shai Ben-David, John Blitzer, Koby Crammer, and Fer-nando Pereira.
2007.
Analysis of representationsfor domain adaptation.
In Advances in Neural In-formation Processing Systems 20, Cambridge, MA.MIT Press.Shai Ben-David, John Blitzer, Koby Crammer, AlexKulesza, Fernando Pereira, and Jenn Wortman.2009.
A theory of learning from different domains.Machine Learning, (to appear).John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In EMNLP.Xavier Carreras and Llu?
?s Ma`rquez.
2003.
Phraserecognition by filtering and ranking with percep-trons.
In Proceedings of RANLP-2003.Xavier Carreras and Llu?
?s Ma`rquez.
2004.
Introduc-tion to the CoNLL-2004 shared task: Semantic rolelabeling.
In Proceedings of the Conference on Nat-ural Language Learning (CoNLL).Xavier Carreras and Llu?
?s Ma`rquez.
2005.
Introduc-tion to the CoNLL-2005 shared task: Semantic rolelabeling.
In Proceedings of the Conference on Nat-ural Language Learning (CoNLL).Trevor Cohn and Phil Blunsom.
2005.
Semantic rolelabelling with tree conditional random fields.
InProceedings of CoNLL.Arthur Dempster, Nan Laird, and Donald Rubin.
1977.Likelihood from incomplete data via the EM algo-rithm.
Journal of the Royal Statistical Society, Se-ries B, 39(1):1?38.Koen Deschacht and Marie-Francine Moens.
2009.Semi-supervised semantic role labeling using the la-tent words language model.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing (EMNLP).D.
Downey, M. Broadhead, and O. Etzioni.
2007a.
Lo-cating complex named entities in web text.
In Procs.of the 20th International Joint Conference on Artifi-cial Intelligence (IJCAI 2007).Doug Downey, Stefan Schoenmackers, and Oren Et-zioni.
2007b.
Sparse information extraction: Unsu-pervised language models to the rescue.
In ACL.G.
Escudero, L. Ma?rquez, and G. Rigau.
2000.
Anempirical study of the domain dependence of su-pervised word sense disambiguation systems.
InEMNLP/VLC.Hagen Fu?rstenau and Mirella Lapata.
2009a.
Graphalignment for semi-supervised semantic role label-ing.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing(EMNLP), pages 11?20.Hagen Fu?rstenau and Mirella Lapata.
2009b.
Semi-supervised semantic role labeling.
In Proceedingsof the 12th Conference of the European Chapter ofthe ACL, pages 220?228.Daniel Gildea and Daniel Jurafsky.
2002.
Automaticlabeling of semantic roles.
Computational Linguis-tics, 28(3):245?288.Daniel Gildea.
2001.
Corpus Variation and Parser Per-formance.
In Conference on Empirical Methods inNatural Language Processing.Trond Grenager and Christopher D Manning.
2006.Unsupervised discovery of a statistical verb lexi-con.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing.Fei Huang and Alexander Yates.
2009.
Distributionalrepresentations for handling sparsity in supervisedsequence labeling.
In Proceedings of the AnnualMeeting of the Association for Computational Lin-guistics.H.
Kucera and W.N.
Francis.
1967.
ComputationalAnalysis of Present-Day American English.
BrownUniversity Press.J.
Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilisticmodels for segmenting and labeling sequence data.In Proceedings of the International Conference onMachine Learning.David McClosky, Eugene Charniak, and Mark John-son.
2006.
Reranking and self-training for parseradaptation.
In Proceedings of the 21st InternationalConference on Computational Linguistics and 44thAnnual Meeting of the ACL, pages 337?344.Martha Palmer, Dan Gildea, and Paul Kingsbury.
2005.The Proposition Bank: A corpus annotated with se-mantic roles.
Computational Linguistics Journal,31(1).Sameer Pradhan, Kadri Hacioglu, Wayne Ward,James H. Martin, and Daniel Jurafsky.
2005.
Se-mantic role chunking combining complementarysyntactic views.
In Proc.
of the Annual Confer-ence on Computational Natural Language Learning(CoNLL).Sameer Pradhan, Wayne Ward, and James H. Martin.2007.
Towards robust semantic role labeling.
InProceedings of NAACL-HLT, pages 556?563.Vasin Punyakanok, Dan Roth, and Wen-tau Yih.
2008.The importance of syntactic parsing and inference insemantic role labeling.
Computational Linguistics,34(2):257?287.977Lawrence R. Rabiner.
1989.
A tutorial on hiddenMarkov models and selected applications in speechrecognition.
Proceedings of the IEEE, 77(2):257?285.Robert S. Swier and Suzanne Stevenson.
2004.
Unsu-pervised semantic role labelling.
In Proceedings ofthe 2004 Conference on Empirical Methods in Nat-ural Language Processing, pages 95?102.Kristina Toutanova, Aria Haghighi, and Christopher D.Manning.
2008.
A global joint model for se-mantic role labeling.
Computational Linguistics,34(2):161?191.Jason Weston, Frederic Ratle, and Ronan Collobert.2008.
Deep learning via semi-supervised embed-ding.
In Proceedings of the 25th International Con-ference on Machine Learning.978
