ml/////////Extract ing Phoneme Pronunciat ion Information from CorporaIan Thomas, Ingrid ZukermanDepartment ofComputer ScienceMonash UniversityClayton, VICTORIA 3168AUSTRALIA{iant, ingrid}@cs .monash.
edu.
auBhavani RaskuttiArtificial Intelligence SectionTelstra Research LaboratoriesClayton, VICTORIA 3168AUSTRALIAb.
raskut t  i@tr l ,  oz.
auAbstractWe present a procedure that determines a setof phonemes possibly intended by a speakerfrom a recognized or uttered phone.
This in-formation will be used to allow a speech rec-ognizer to take pronunciation i to account orto consider input from a noisy source duringlexical access.
We investigate the hypothesisthat different pronunciations ofa phone occurwithin groups of sounds physically producedthe same way, and use the Minimum MessageLength principle to consider the effect of aphoneme's context on its pronunciation.1 IntroductionWhen trying to match spoken words to dictionaryentries during speech recognition, it is useful to beable to generate alternative versions of the spokensequences of phones to account for the manner inwhich different speakers pronounce a phone.
If weknow the probabilities that the component soundsin a sequence of phones are pronounced like othersounds, then likely alternative pronunciations ofthat sequence of phones can be generated to matchagainst a lexicon of known words.
Furthermore, ifwe also have some idea of how the context withinwhich a phone was uttered affects its pronunciation,we have extra information which can be used to pro-duce more realistic alternative pronunciations.This paper considers the task of automaticallyextracting statistical information about how vari-ous sound sections of words (phonemes) are pro-nounced by speakers (as phones) by matching in-tended phonemes and uttered phones from a tran-scribed speech corpus.
The same approach couldbe used to gather statistics about how phones rec-ogaized (or mis-recognized) by a speech recognizermatch the phonemes intended by a speaker.This information extraction process is part of thetraining phase for the lexical access component of aspeech recognition system, where the pronunciationprobabilities are generated from a training corpus.The study was done on the TIMIT corpus (Fisheret al, , 1986) - -  a collection of American-Englishread sentences with correct time-aligned acoustic-phonetic and orthographic (word-aligned) transcrip-tions.
1 The corpus contains 3696 sentences spokenby 462 speakers from 8 different dialect divisionsacross the United States.Previous work by Riley (1989) and Withgottand Chen (1993) used Classification and RegressionTrees (CART) on a large number of different featuresof the corpus (such as genderi dialect and speakingrate) to obtain pronunciation i formation of inten-ded phonemes.
Our system obtain~ similar resultsusing positional information and context, and us-ing exact matches from uttered phones to intendedphonemes to guide other matches.Work by Cohen et (d., (1987) on pronunciationused a couple of set sentences for multiple speakers,but did not cover a wide range of words (and thusdifferent phone contexts).
Our study considers thepronunciation patterns of a wide range of differentspeakers using a large collection of words.A tree-based system by Luccassen and Mercer(1984) uses an information theoretic approach fordeciding alternative pronunciations based on theclassification of a large context feature vector.
How-ever, when building their decision tree, they do notevaluate the quality of the resulting tree, i.e., theykeep testing attributes until a boundary situationis reached.
In contrast, our system initially usesthe relative positioning of uttered phones and inten-ded phonemes to determine the phonemes possiblyintended by a speaker when uttering a particularphone.
The context of a phone is considered only as1Transcriptions were made by a combination ofhandtranscriptions u ing multiple parametric representationsof sentences as a guide, and automatic alignment (Zueand Seneff, 1988).
The use of different representationsis claimed as a good way of overcoming dialect biasesduring transcription (Withgott and Chen, 1993).Thomas, Zukerman and Raskutti 175 Extracting Phoneme Pronunciation InformationIan Thomas, Ingrid Zukerman and Bhavani Raskurd (1998) Extracting Phoneme Pronunciatim Information from Corpora.In D.M.W.
Powers (ed.)
NeMLaP3/CoNLL98: New Methods in Language Processing and Computational Natural LanguageLearning, ACL, pp 175-183.The Mayan neoclassic scholar disappeared while surveying ancient ruins.the Imayan \[neoclassicDH AX IM AY ax N \[N IY ow K L AE SDH AX IM AY eh N IN IY ix K L AE SC Clih~ k Iix kcl mscholar \[disappearedS K AA L AXR \[D ih S ax P iy rS K AA L AXR \[D ix S ix P ih axr\[while \[d lhh W ay L ldx lax W aa L \[surveying \[ancient \[ruins IS axr V EY ix NG l- EY N SH ix n t \[R uw ih N Z IS er V EY - NG \[q EY N SH - en tcl IR ux ix N Z IFigure 1: A typical alignment of lexicon entry phonemes with input phones.an extra source of information to qualify these pre-dictions.
Further, the method we apply for buildingdecision trees evaluates whether context is meaning-ful in terms of its predictive power.Riley (1991) implements a similar system usinga different method for tree induction, but esti-mates the probability of an uttered phoneme givena phoneme context and a partial phone context,whereas we are inferring an intended phoneme froman uttered phone context.Our specific aim is to test two hypotheses: (1) thatphonemes are pronounced as phones in the samebroad sound category (for example, vowels for vowelsand fricatives for fricatives), and (2) that the contextof a phone, that is, the attributes of the phones im-mediately preceding and following it, influence thepronunciation of this phone.2 Determining AlternativePronunciationsFor each word in each sentence in the corpus, wematch the transcribed phones with the phonemesin that word as recorded in a lexicon, and recordthe frequency of occurrence ofeach phoneme/phonematch over the entire corpus.
The major difficultiesin this process are (1) transcriptions include extraphones that do not appear in the phoneme sequencescorresponding to words in the lexicon, and (2) thereare expected phones that were not pronounced.
Asa result, the phone sequences rarely align exactlywith the phonemes corresponding to the words in thelexicon.
This makes a simple alignment unreliable,and calls for a more flexible method of matching.Our method consists of examining the words ofthe corpus aligned with their lexicon entries, andchoosing phoneme to phone pairs that we are confi-dent are a match.
This information is then used tomake further matches in less certain areas.
This pro-cess has three main steps: (1) aligning each corpusword with its corresponding lexicon entry, (2) build-ing initial data structures, and (3) iteratively makingadditional certain matches.2.1 Aligning Corpus Words with LexiconEntriesWe use the dynamic string alignment algorithm de-scribed by Sandoff and Kruskal (1983) to determinethe minimum number of substitutions, insertionsand deletions needed to turn one string into another.This algorithm can produce several edit sequenceswith the same cost, so the edit sequence with thehighest number of exact matches is selected.
Ta-ble 1 describes elected symbols from the ARPA-bet symbol set (Shoup, 1980) used for representingphonemes and phones in TIMIT.
2 A typical align-ment of words in a sentence is given in Figure 1.
Thefirst row contains phonemes from the lexicon words,and the second row contains phones from the corre-sponding words in a corpus sentence.
Note the useof C to represent the sharing of the N phone betweenmayan and neoclassic due to co-articulation.The TIMIT transcriptions of sentences use somesymbols that are not present in the lexicon entries.For example, stop closures (bcl, dcl, gcl, pcl, tcl,kcl) and releases (b, d, g, p, t, k) are providedin the corpus transcriptions, but only releases areused in the lexicon entries.
In order to prevent hismismatch from causing the surrounding phones to2ARPAbet is a type-written version of the standardInternational Phonetic Alphabet.lI1l/////lmThomas, Zukerman and Raskutti 176 Extracting Phoneme Pronunciation I formation/II//l//ll/I/////I/IITable 1:from the ARFAbet setSelected phoneme and phonetic symbolsSymbol Example Word Possible PhoneticTranscriptiond day DCL D eyp pea PCL P iyt tea TCL T iyk key KCL K iyq bat bcl b ae Qs sea S iysh she SH iyz zone  Z ownv van Vaendh then DH e nm morn Maa Mn noon N uw Nen button bah  q ENl lay L eyr ray R eyw way W eyy yacht Y aa tcl thh hay HH eyhv ahead ax HV eh dcl del bottle bcl baa tcl t ELiy beet bcl b IT tcl tih bit bcl b Itt tcl teh bet bcl b EH tcl tey bait bcl b EY tcl tae bat bcl b AE tcl taa bott bcl b AA tel tay bite bcl b AY  tcl tall but bcl b AH tcl tow boat bcl b OW tcl tuw boot bcl b UW tcl tux toot tel t UX  tcl ter bird bcl b ER  dcl dax about AX bcl b aw tcl tix debit dcl d eh bcl b IX tcl taxr butter bcl b ah dx AXRbe misaligued, we remove stop closures when theyappear before stop releases.
For example, in Figure 1we have removed the kcl closure that preceded thefirst K phone in neoclassic, but the final kcl of thatword was not removed because there was no releasefound for that closure (we don't want to eliminatethe evidence of the k sound completely).2.2 Building Initial Data StructuresWe scan through each aligned word pair in everysentence in the corpus and record certain matchesand uncertain areas, generating frequency counts ofthe certain matches.A certain match is a pairing between aphoneme ina lexicon word and a phone in the same corpus wordwhich we are confident represent the same intendedsound.
Initially, the certain matches are insertions,deletions and substitutions bounded immediately ondid you didd ih d I y uw d ih ddx ix jh I jh ux dx ix dclC C(a) (b)Figure 2: Example of (a) uncertain co-articulation,and (b) multiple choice matchesthe left and right by an exact match or the beginningor end of a word.
Since we know the boundaries ofwords from the transcriptions, we can reliably matchup phonemes on word boundaries provided they arebounded on the other side by an exact match.
Ex-act matches are also recorded as certain matches.
Inthe sentence in Figure 1, ow/ix in neoclassic s acertain match, as is ih/ ix in disappeared, hh/axand ay /aa  in while, and ix/- in surveying.An uncertain area is a group of two or more opera-tions (insert, delete or substitute) bounded by a cer-tain match or the beginning or end of a word.
Exam-pies of uncertain areas in Figure 1 are the last threeoperations in ancient and the second and third op-erations in ruins.
Uncertain areas potentially havematches within them, but we have not committed toan aligmnent at this stage.TIMIT uses a number of phonological rules forsharing and deleting phones on the boundaries ofthe words in the transcriptions ( uch as shown be-tween the second and third word in Figure 1).
Theserules correspond to cases of co-articulation f phones(Giachin et aL,, 1991).
Such co-articulated phonesremove word boundaries, resulting in the concatena-tion of the end of a word with the beginning of thenext word.
For example, Figure 2(a) illustrates howco-articulation ofthe words did you renders the en-tire phone sequence an uncertain area.
In contrast,the co-articulated N/N  phones between mayan andneoclassic n Figure 1 constitutes a certain match.2.3 Making Addit ional Certain MatchesWe use frequency counts of certain matches obtainedin Step 2 to generate additional certain matches fromthe uncertain areas.
To this effect, we consider eachpossible phoneme/phone match in each uncertainarea, and select he phoneme/phone match with thehighest frequency of certain matches.
For instance,if the match n /en  occurs more often than n/tel ,then n /en  will be chosen in the uncertain area be-tween SH and the end of ancient in Figure 1.Whenever there are multiple instances of the samephoneme in an uncertain area, it is matched to thephone that is positionally closest.
For example, bothThomas, Zukerman and Raskutti 177 Extracting Phoneme Pronunciation I formationIIIIIIIIIIIIIIIIIIIIIIId phonemes in the uncertain area in Figure 2(b)match the phones dx and dcl.
In this case, we matchthe first d tO dx  and the second  to del.
During thisprocess, we consider only potential phone/phonemematches, and we ignore any match involving a "-"symbol (which indicates an insertion or a deletion).Insertions and deletions that are certain matches arecollected for statistical purposes but do not influencethe match decision process.After a phoneme/phone match has been deter-mined, the phonemes and phones of the uncertainarea are shifted so that the matched phoneme andphone are lined up.
This new match is then removedfrom the uncertain area and recorded as another cer-tain match.
This process can create other boundedmatches that are also removed from the uncertainareas and added to the certain matches.
For exam-ple, in Figure 1, finding the certain match ih / ix  indisappeared in Step 2 suggests that the match to bemade in the uncertain areas in ruins and neoclas-sic is ih/ ix.
The match in ruins in turn creates auw/ux  pairing on its left-hand side, which is alsorecorded as a certain match, eliminating this uncer-tain area completely.
Similarly, the match in neo-classic yields the k /kc l  match on its right-hand side.This step is repeated until the number of matchesbeing made levels off.2.4 Eva luat ionFor the TIMIT corpus, the number of certainmatches levels off at 123115 from 109898 initialmatches (after six iterations of Step 3), and thenumber of remaining uncertain matches falls from13913 to 908.
Figure 3 shows the percentage ofphones (columns) found for every phoneme in thelexicon words (rows) after six iterations of match-ing attempts in each uncertain area.
For example,between 12-16% of t and d are pronounced as dx.It  is important o note that exact phoneme/phonematches registered in the 1000-2000 range, whilemost of the alternative pronunciations had a fre-quency in the few dozen.Figure 3 suggests that phonemes are often mis-pronounced as phones in the same broad soundgroup, i.e., both the intended phonemes and theuttered phones are generated by the same physicalmethod of production.
This result is most evidentfor vowels.
Some of the other types of sounds, suchas fricatives and nasals, have not been differentiatedso clearly.3 Cons ider ing  ContextTo investigate the effect of the context of a phone(the attributes of the phones before and after thisphone) on its pronunciation, we examined sentencesaligned like the sentence in Figure 1 and recordedphoneme/phone pairs, along with acoustic featuresfor the phones that appear before and after eachphoneme/phone pair.
3 It includes the followingattributes: broad sound category, voiced/voiceless,sibilant/non-sibilant and sonorant/obstruent.
Ta-ble 2 shows how phones are classified according tothese acoustic features, from (Yannakoudakis andHutton, 1987) and (Rabiner and Juang, 1993).The contexts for all the phones were fed into aninductive inference program by Wallace and Patrick(1993) in order to find functions of the context at-tributes (i.e., acoustic features) that are good pre-dictors of the phoneme intended by a speaker whens/he utters a particular phone.
The inference pro-gram uses the Minimum Message Length (MML)principle (Georgeff and Wallace, 1984) to measurethe significance of these functions.
These functionsare realized by a decision tree from the contexts foreach uttered phone, with nodes splitting on the val-ues of particular attributes.
4 Leaf nodes in a deci-sion tree represent a partition of the context sample.Each leaf node contains a collection of contexts inthe sample and the phoneme intended by a speakerfor each context.
An internal node of the tree is spliton an attribute only when doing so creates astatisti-cally significant reduction in the number of differenttypes of intended phonemes in the leaf nodes (bet-ter than that expected from random effects).
Ideally,each leaf node should contain several contexts, all ofwhich have the same intended phoneme.
This meansthat the attributes these contexts have in commonare sufficient o predict his intended phoneme froman uttered phone.The MML principle is based on the followingpremise: if a sender knows both the attribute val-ues and the class of the objects in a set, and wantsto send the class of each object to a receiver (w.hoknows the attribute values but not the classes), thesender aims to send the shortest possible message(in bits).
The MML criterion is used to produce thedecision tree that can be sent by means of the short-est possible message.
A split is made in the decisiontree only if it decreases the message l ngth for trans-mitting the intended information.
The decision treeis then sent in a two-part message: (1) instructionsfor the receiver on how to reconstruct the tree; andSUncertain areas were treated as a singlephoneme/phone pair involving complicated sounds.4Words which are common in the corpus generatecontexts that appear with high frequency.
We assumethat these frequencies are representative of those in En-glish.Thomas.
Zukerman and Raskutti 179 Extracting Phoneme Pronunciation InformationTable 2: Classification of TIMIT phones according to acoustic features.b,d,gp,q,t,k,dxbcl,dcl,gclpcl,tcl,kcljlichz,zliv,dhs,shf,v,tlim,n,nx,ng,em,en,engl,r,y,w,elhhhviy, ih,eh,aeaa,er,ah,ax,aouw,uh,owaxr,ax-hpau,epiwbsbStop Release, voiced, obstruent, non-sibilantStop Release, unvoiced, obstruent, non-sibilantStop Closure, voicedStop Closure, unvoicedAffricate, voiced, obstruent, sibilantAffricate, unvoiced, obstruent, sibilantFricative, voiced, obstruent, sibilantFricative, voiced, obstruent, non-sibilantFricative, unvoiced, obstruent, sibilantFricative, unvoiced, obstruent, non-sibilantNasal, voiced, sonorantSemivowel/Glide, voiced, sonorantSemivowel/Glide, unvoiced, obstruent, non-sibilantSemivowel/Glide, voiced, obstruent, non-sibilantVowel, Front PositionVowel, Mid PositionVowel, Back PositionVowelPauseWord boundarySentence boundaryMissing phoneme(2) the labels for the classes of the objects in theleaves of the tree.
Standard coding techniques showthat the encoded set of labels for the classes in aleaf node will be short if most of the objects in thatleaf node have the same label, and will be longer ifthe labels of the objects are equally likely.
There-fore, if the objects in each leaf node are predomi-nantly of one class, then the message ncoding thedecision tree will be shorter than a message whichsimply encodes the class label for each object in theset (without the tree).Given an uttered phone, the resulting decision treeshows the significant attributes and values for classi-fying the intended phoneme, which is the effect hatthe surrounding sounds have on predicting the in-tended phoneme.3.1 Evaluat ionAs indicated in Figure 3, in the majority of casesthe uttered phone and the intended phoneme arethe same.
Table 3 summarizes the decision trees forsituations where uttered phones are different fromintended phonemes.
This summary shows the effectof contextual phonetic information on the intendedphoneme for each possible uttered phone (one lineper phone).
For example, for the uttered phone d,the intended phoneme was t when the next soundwas" neither a consonant nor a vowel, i.e., a wordboundary, a sentence boundary or missing; this oc-curred in 12 of the samples.
Also, the intendedphoneme was missing (i.e., d was uttered when nophoneme was intended) when the next sound wasan obstruent; this happened in 2 of the samples.
In220 samples, the uttered phone was iy with intendedphoneme ax when iy was the last sound in a word,and the previous ound was a fricative.Some uttered phones found in Figure 3 are missingfrom Table 3 because ither there were not enoughsamples to create a tree (the stops b, g and p, thenasal m, and the pause), or more often because thetrees produced had no discriminatory power.
Thisoccurred when each leaf node in a decision tree hadan evenly spread mixture of intended phonemes, orwhen the same intended phoneme appeared through-out the tree.The decision trees were evaluated using test con-texts from 1344 sentences (out of 5040 sentences)spoken by 26% of the speakers.
No speaker wasin the test and training sets.
Each phone and itscontext was classified into a leaf node using the at-tributes in the context.
A phoneme prediction wasconsidered correct when the intended phoneme wasthe same as the most common phoneme in the leafnode (determined uring training).
75% of the dif-ferent est samples were predicted correctly.4 DiscussionWe have analyzed a large corpus of sentences readby a large number of speakers with a view to deter-mining possible mis-pronunciation f phonemes andthe context in which such mis-pronunciations occur.The results of our analysis support our hypothesesthat phonemes are mis-pronounced asphones in thesame broad sound categories, and that the contextof mis-pronunciation provides valuable informationThomas, Zukerman and Raskutt/ 180 Extracting Phoneme Pronunciation I formationmmm|mmmmm|mm|mmmm|mImmm//mmmut teredphonednumber intended prey next intended prey intended prey nextof  phoneme sound sound phoneme sound phone sound soundrumples (samples) (samples) (samples)2369 t(12) non-cons w (2)non-vowt 3905 -- (41) obst d (8)k 3788 -- (6) nasaldx 1069 t(183) frontVqjhchshfYnemenengrwhheliyiheheyaeaaayahaooyOWuhUWax.,,Lxaxrax-hnextsoundobst"'non-vowg (i) fricatt (171) non-vow voicednon-cons1318 - -  (912) vowel t (154) non-cons wordBvowel987 zh(21) non-vow" d y (2) backV778 sh(5~' nasal jh .
(2) vowel t (2) fricat1276 s(24) obst s ch(3) vowel voiced - -  (3) wordB voicedunvoiced2204 v (23) vowel non-cons p(2) vowel obst1978 b (7) .... vowel - - , (2)  wordB6133 ng.
(26) vowel en ,!2) ,, fricat dh (1) wordB109 ax m(41) stopRe ah m (19) semiG1516 ix n(149) obst frontV ax 'n  (54)' stopR obst ae n d (42 non-cons wordBunvoiced16 ng (3) vowel ix n.g (3) StopR ih n (2) wordB4635 - -  (54) non-cons vowel axr (34) obst2184 - -  (17) non-cons - -  (3) obst vowel uw(2) obst semiGL915 - -  (24!
unvoiced d (2) voiced911 ih l (56) unvoiced ax 1 (33) voiced vowel uh 1 (10) fricatsemiG1 unvoiced4481 ax (220) fricat wordB ix (93) non-vow nasal dh ax (2!)
non-cons wordBunvoiced3838 ax (52) fricat wordB ix (52) non-obst nasal iy (28) semiGl wordBobst unvoiced2920 ae (151) obst sem~Gl .... ae (63) non-obst nasal ax (24) non-obst nasalunvoiced voiced2209 ax (92)" unvoiced wordB ae (6) nasal2273 ih (3) " nasal aa (3) semiGl2137 ao (53) non-obst nasal aw (34)  wordB semiGl ay(ll) nasal wordB1935 aa (4) nasal ax (2) semiGL ax (2) wordB2045 ax (84) unvoiced obst ax (65)  unvoiced non-consvoiced1840 aa (12) voiced semiGl uh ( I i )  unvoiced semiGl ah (2) unvoiced-'" semiGLobst semiGL296 ow ix(3) nasal ao (3) semiGl1643 ax (8) non-cons ao r(4) sonor ao r(3) obst467 ax ( I0)  sonor obst ih (9) sonor sonor uw (I0) obst non-'cons522 ow (3) vowel er (2) wordB uh (2) semiGl3091 uw (59) stopRe wordB ae (55) wordB nasal "unvoiced5958 ax (505) fzicat wordB ih (404) unvoiced nasal uw (186) stopRe " wordBnon-sibi la non-obst unvoiced2181 aor  (130) fricat wordB aa r (63) wordB wordB r (51) vowel non-consunvoiced303 uw (79) stopRe non-cons ix(29) obst ax (26) fricat wordBwordB sibila voiced2136 d (332) nasal unvoiced t (84) fricat wordB hh( l l9 )  wordB .... semiGlnon-obst unvoicedsibflaTab le  3: Summary  of most  signif icant values in the decision tree for each ut tered  phone.Thomas, Zukerman and Raskutti 181 Extracting Phoneme Pronunciation I formationabout the intended phoneme.As indicated in Figure 3, mis-pronunciation faffricates and fricatives is rare (over 84% certainmatches), though when they are mis-pronounced,the uttered phone may be one of the stop conso-nants.
Vowels are often mis-pronounced, but theuttered phone is almost always from the same broadsound category.
The stop consonants, d and t, thenasal en, and the semi-vowel hh are often mis-pronounced.
Analysis of the decision trees gen-erated for these mis-pronunciations i dicates thatthe attributes of the phones urrounding the mis-pronounced phoneme do indeed provide informa-tion about the intended phoneme.
These decisiontrees are particularly informative for vowels owingto the large number of mis-pronunciations as well asthe regularity of these mis-pronunciations.
The at-tributes that are most useful vary for the differentpronunciations of each phoneme.
For instance, ay isthe pronunciation for aa when the previous phoneis nasal, while ao is pronounced for aa when thepreceding phone is a voiced obstruent and the nextphone is a semivowel or glide.The frequency of the matches in Figure 3 com-bined with the decision trees produced using theMML principle may be used to generate alternatepronunciations of phonemes in word models.
Thiswill assist in the recognition of mis-pronouncedwords during automatic speech understanding.
Thedecision tree is weakest for uncommon contexts, be-cause of a lack of training data for constructing thetree (the message length for encoding phonemes insuch contexts i no better than an efficient encodingof the context classes using a Huffman code).
In thiscase, the matrix in Figure 3 should be used to pre-dict alternative pronunciations.
However for morecommon contexts, the decision trees are preferred,as they use more information than the matrix to de-termine the intended phoneme.The system described in this paper investigatesdependencies between an intended phoneme and apronounced phone, but it may be easily adapted todetermine r lationships between an intended soundand a recognized sound, i.e., the output of a speechrecognizer.
Relationships determined in this man-ner may be used during speech recognition, andthus account for mis-recognition as well as mis-pronunciation.AcknowledgmentsThe authors thank Jon Oliver and Chris Wallace fortheir advice on MML encoding.REFERENCESCohen, M., Baldwin, G., Berhnstein, J., Murveit, H.,and Weintraub, M., Studies for an AdaptiveRecognition Lexicon.
In Proceedings of theDARPA Speech Recognition Workshop, ReportNo.
SAIC-87/1644, 1987.Fisher, W.M., Doddington, M., George, R., andGoudie-Marshell, K.M., The DARPA SpeechRecognition Database: Specifications and Status.In Proceedings of the DARPA Speech RecognitionWorkshop, Report No.
SAIC-86/1546, 1986.Georgeff, M.P., and Wallace, C.S., A General Cri-terion for Inductive Inference.
In Proceedings ofthe Sixth European Conference on Artificial Intel-ligence, pp.
473-482, Pisa, Italy, 1984.Giachin, E.P., Rosenberg, A.E., and Lee, C., WordJuncture Modeling using Phonological Rulesfor HMM-based Continuous Speech Recognition,Computer Speech and Language 5:155-168, 1991.Lucassen, J.M., and Mercer, R.L., An InformationTheoretic Approach to the Automatic Determi-nation of Phoneme Baseforms.
In Proceedings ofthe International Conference on Acoustics, Speechand Signal Processing, pp.
42.5.1--42.5.4, 1984.Rabiner, L.R., and Huang, B., Fundamentals ofSpeech Recognition, Prentice Hall, EnglewoodCliffs N J, 1993.Riley, M.D., Some Applications of Tree-based Mod-eling to Speech and Language.
In DARPA Speechand Language Workshop, pp.
339-352, 1989.Riley, M.D., A Statistical Model for Generating Pro-nuncation Networks.
In Proceedings of the Inter-national Conference on Acoustics, Speech and Sig-nal Processing, pp.
737-740, 1991.Sankoff, D. and Kruskal, J.B., Time Warps, StringEdits and Macromolecules: The Theory and Prac-tice of Sequence Comparison, Addison Wesley,London, 1983.Shoup, J.E., Phonological Aspects of SpeechRecognition.
In Trends in Speech Recognition,W.A.
Lea, Ed., Prentice-Hall, Englewood Cliffs,N J, pp.
125-138, 1980.Wallace, C.S., and Patrick, J.D., Coding DecisionTrees, Machine Learning 11:7-22, 1993.Withgott, M.M.
and Chen, F.R.
Computationalmodels of American Speech, CSLI Lecture Notes,No.
32, Stanford, CA, 1993.Thomas, Zukerman and Raskutti 182 Extracting Phoneme Pronunciation I formationIIlIIIlIIlIlIlmmIIIIIIIIIIIIIIIIIIIIIIIIi lII"Yannakoudakis, E.J.
and Hutton, P.J., Speech Syn-thesis and Recognition Systems, Ellis HorwoodLimited, 1987.Zue, V.W.
and Seneff, S., Transcription andAlignment of the Timit Database.
In SecondSymposium on Advanced Man-Machine Interfacethrough Spoken Language, Oahu, Hawaii, 1988.Thomas, Zukerman and Raskutti 183 Extracting Phoneme Pronunciation Informationmmmmmmmmmmmmm
