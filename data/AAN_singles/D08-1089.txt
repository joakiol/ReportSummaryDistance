Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 848?856,Honolulu, October 2008. c?2008 Association for Computational LinguisticsA Simple and Effective Hierarchical Phrase Reordering ModelMichel GalleyComputer Science DepartmentStanford UniversityStanford, CA 94305-9020galley@cs.stanford.eduChristopher D. ManningComputer Science DepartmentStanford UniversityStanford, CA 94305-9010manning@cs.stanford.eduAbstractWhile phrase-based statistical machine trans-lation systems currently deliver state-of-the-art performance, they remain weak on wordorder changes.
Current phrase reorderingmodels can properly handle swaps betweenadjacent phrases, but they typically lack theability to perform the kind of long-distance re-orderings possible with syntax-based systems.In this paper, we present a novel hierarchicalphrase reordering model aimed at improvingnon-local reorderings, which seamlessly in-tegrates with a standard phrase-based systemwith little loss of computational efficiency.
Weshow that this model can successfully han-dle the key examples often used to motivatesyntax-based systems, such as the rotation ofa prepositional phrase around a noun phrase.We contrast our model with reordering modelscommonly used in phrase-based systems, andshow that our approach provides statisticallysignificant BLEU point gains for two languagepairs: Chinese-English (+0.53 on MT05 and+0.71 on MT08) and Arabic-English (+0.55on MT05).1 IntroductionStatistical phrase-based systems (Och and Ney,2004; Koehn et al, 2003) have consistently de-livered state-of-the-art performance in recent ma-chine translation evaluations, yet these systems re-main weak at handling word order changes.
The re-ordering models used in the original phrase-basedsystems penalize phrase displacements proportion-ally to the amount of nonmonotonicity, with no con-sideration of the fact that some words are far moreMMDSD!
"#$%&'()*+,-./euenvironment ministersholdmeetingsin luxemburg.012345678/the developmentandprogressoftheregion.DMDD(b)(a)Figure 1: Phase orientations (monotone, swap, discontin-uous) for Chinese-to-English translation.
While previouswork reasonably models phrase reordering in simple ex-amples (a), it fails to capture more complex reorderings,such as the swapping of ?of the region?
(b).likely to be displaced than others (e.g., in English-to-Japanese translation, a verb should typically move tothe end of the clause).Recent efforts (Tillman, 2004; Och et al, 2004;Koehn et al, 2007) have directly addressed this issueby introducing lexicalized reordering models intophrase-based systems, which condition reorderingprobabilities on the words of each phrase pair.
Thesemodels distinguish three orientations with respect tothe previous phrase?monotone (M), swap (S), anddiscontinuous (D)?and as such are primarily de-signed to handle local re-orderings of neighboringphrases.
Fig.
1(a) is an example where such a modeleffectively swaps the prepositional phrase in Luxem-bourg with a verb phrase, and where the noun min-isters remains in monotone order with respect to theprevious phrase EU environment.While these lexicalized re-ordering models haveshown substantial improvements over unlexicalizedphrase-based systems, these models only have a848limited ability to capture sensible long distance re-orderings, as can be seen in Fig.
1(b).
The phraseof the region should swap with the rest of the nounphrase, yet these previous approaches are unable tomodel this movement, and assume the orientation ofthis phrase is discontinuous (D).
Observe that, ina shortened version of the same sentence (withoutand progress), the phrase orientation would be dif-ferent (S), even though the shortened version has es-sentially the same sentence structure.
Coming fromthe other direction, such observations about phrasereordering between different languages are preciselythe kinds of facts that parsing approaches to machinetranslation are designed to handle and do success-fully handle (Wu, 1997; Melamed, 2003; Chiang,2005).In this paper, we introduce a novel orientationmodel for phrase-based systems that aims to bet-ter capture long distance dependencies, and thatpresents a solution to the problem illustrated inFig.
1(b).
In this example, our reordering modeleffectively treats the adjacent phrases the develop-ment and and progress as one single phrase, and thedisplacement of of the region with respect to thisphrase can be treated as a swap.
To be able iden-tify that adjacent blocks (e.g., the development andand progress) can be merged into larger blocks, ourmodel infers binary (non-linguistic) trees reminis-cent of (Wu, 1997; Chiang, 2005).
Crucially, ourwork distinguishes itself from previous hierarchicalmodels in that it does not rely on any cubic-timeparsing algorithms such as CKY (used in, e.g., (Chi-ang, 2005)) or the Earley algorithm (used in (Watan-abe et al, 2006)).
Since our reordering model doesnot attempt to resolve natural language ambigui-ties, we can effectively rely on (linear-time) shift-reduce parsing, which is done jointly with left-to-right phrase-based beam decoding and thus intro-duces no asymptotic change in running time.
Assuch, the hierarchical model presented in this pa-per maintains all the effectiveness and speed advan-tages of statistical phrase-based systems, while be-ing able to capture some key linguistic phenomena(presented later in this paper) which have motivatedthe development of parsing-based approaches.
Wealso illustrate this with results that are significantlybetter than previous approaches, in particular thelexical reordering models of Moses, a widely usedphrase-based SMT system (Koehn et al, 2007).This paper is organized as follows: the train-ing of lexicalized re-ordering models is describedin Section 3.
In Section 4, we describe how tocombine shift-reduce parsing with left-to-right beamsearch phrase-based decoding with the same asymp-totic running time as the original phrase-based de-coder.
We finally show in Section 6 that our ap-proach yields results that are significantly better thanprevious approaches for two language pairs and dif-ferent test sets.2 Lexicalized Reordering ModelsWe compare our re-ordering model with relatedwork (Tillman, 2004; Koehn et al, 2007) using alog-linear approach common to many state-of-the-art statistical machine translation systems (Och andNey, 2004).
Given an input sentence f, which is tobe translated into a target sentence e, the decodersearches for the most probable translation e?
accord-ing to the following decision rule:e?
= argmaxe{p(e|f)}(1)= argmaxe{ J?j=1?
jh j(f,e)}(2)h j(f,e) are J arbitrary feature functions oversentence pairs.
These features include lexicalizedre-ordering models, which are parameterized asfollows: given an input sentence f, a sequence oftarget-language phrases e = (e1, .
.
.
,en) currentlyhypothesized by the decoder, and a phrase alignmenta = (a1, .
.
.
,an) that defines a source f ai for eachtranslated phrase ei, these models estimate the prob-ability of a sequence of orientations o = (o1, .
.
.
,on)p(o|e, f) =n?i=1p(oi|ei, f ai ,ai?1,ai), (3)where each oi takes values over the set of possi-ble orientations O = {M,S,D}.1 The probability isconditioned on both ai?1 and ai to make sure thatthe label oi is consistent with the phrase alignment.Specifically, probabilities in these models can be1We note here that the parameterization and terminology in(Tillman, 2004) is slightly different.
We purposely ignore thesedifferences in order to enable a direct comparison between Till-man?s, Moses?, and our approach.849.................. ............b i...................................b i..................................................................................................(a)(b)(c)b isuvuvuvssFigure 2: Occurrence of a swap according to the threeorientation models: word-based, phrase-based, and hier-archical.
Black squares represent word alignments, andgray squares represent blocks identified by phrase-extract.In (a), block bi = (ei, fai) is recognized as a swap accord-ing to all three models.
In (b), bi is not recognized as aswap by the word-based model.
In (c), bi is recognizedas a swap only by the hierarchical model.greater than zero only if one of the following con-ditions is true:?
oi = M and ai ?ai?1 = 1?
oi = S and ai ?ai?1 = ?1?
oi = D and |ai ?ai?1| 6= 1At decoding time, rather than using the log-probability of Eq.
3 as single feature function, wefollow the approach of Moses, which is to assignthree distinct parameters (?m,?s,?d) for the threefeature functions:?
fm = ?ni=1 log p(oi = M| .
.
.)?
fs = ?ni=1 log p(oi = S| .
.
.)?
fd = ?ni=1 log p(oi = D| .
.
.
).There are two key differences between this workand previous orientation models (Tillman, 2004;Koehn et al, 2007): (1) the estimation of factors inEq.
3 from data; (2) the segmentation of e and f intophrases, which is static in the case of (Tillman, 2004;Koehn et al, 2007), while it is dynamically updatedwith hierarchical phrases in our case.
These differ-ences are described in the two next sections.3 TrainingWe present here three approaches for computingp(oi|ei, f ai ,ai?1,ai) on word-aligned data using rel-ative frequency estimates.
We assume here thatphrase ei spans the word range s, .
.
.
, t in the targetsentence e and that the phrase f ai spans the rangeORIENTATION MODEL oi = M oi = S oi = Dword-based (Moses) 0.1750 0.0159 0.8092phrase-based 0.3192 0.0704 0.6104hierarchical 0.4878 0.1004 0.4116Table 1: Class distributions of the three orientation mod-els, estimated from 12M words of Chinese-English datausing the grow-diag alignment symmetrization heuristicimplemented in Moses, which is similar to the ?refined?heuristic of (Och and Ney, 2004).u, .
.
.
,v in the source sentence f. All phrase pairs inthis paper are extracted with the phrase-extract algo-rithm (Och and Ney, 2004), with maximum lengthset to 7.Word-based orientation model: This model an-alyzes word alignments at positions (s?1,u?1)and (s?1,v+1) in the alignment grid shown inFig.
2(a).
Specifically, orientation is set to oi =M if (s?
1,u?
1) contains a word alignment and(s?1,v+1) contains no word alignment.
It is set tooi = S if (s?1,u?1) contains no word alignmentand (s?1,v+1) contains a word alignment.
In allother cases, it is set to oi = D. This procedure isexactly the same as the one implemented in Moses.2Phrase-based orientation model: The modelpresented in (Tillman, 2004) is similar to the word-based orientation model presented above, exceptthat it analyzes adjacent phrases rather than specificword alignments to determine orientations.
Specif-ically, orientation is set to oi = M if an adjacentphrase pair lies at (s?1,u?1) in the alignmentgrid.
It is set to S if an adjacent phrase pair cov-ers (s?1,v+1) (as shown in Fig.
2(b)), and is setto D otherwise.Hierarchical orientation model: This model an-alyzes alignments beyond adjacent phrases.
Specif-ically, orientation is set to oi = M if the phrase-extract algorithm is able to extract a phrase pairat (s?1,u?1) given no constraint on maximumphrase length.
Orientation is S if the same is trueat (s?1,v+1), and orientation is D otherwise.Table 1 displays overall class distributions accord-ing to the three models.
It appears clearly that occur-rences of M and S are too sparsely seen in the word-based model, which assigns more than 80% of its2http://www.statmt.org/moses/?n=Moses.AdvancedFeatures850word phrase hier.Monotone with previous p(oi = M|ei, f ai ,ai?1,ai)1 ,4 and is 0.223 0.672 0.9422 , and also 0.201 0.560 0.948Swap with previous p(oi = S|ei, f ai ,ai?1,ai)3 ?
){ of china 0.303 0.617 0.6514 ??
, he said 0.003 0.030 0.395Monotone with next p(oi = M|ei, f ai ,ai+1,ai)5 ???
, he pointed out that 0.601 0.770 0.9916 l, however , 0.517 0.728 0.968Swap with next p(oi = S|ei, f ai ,ai+1,ai)7 {0 the development of 0.145 0.831 0.9008 {?> at the invitation of 0.272 0.834 0.925Table 2: Monotone and swap probabilities for specificphrases according to the three models (word, phrase, andhierarchical).
To ensure probabilities are representative,we only selected phrase pairs that occur at least 100 timesin the training data.probability mass to D. Conversely, the hierarchicalmodel counts considerably less discontinuous cases,and is the only model that accounts for the fact thatreal data is predominantly monotone.Since D is a rather uninformative default cat-egory that gives no clue how a particular phraseshould be displaced, we will also provide MT evalu-ation scores (in Section 6) for a set of classes thatdistinguishes between left and right discontinuity{M,S,Dl,Dr}, a choice that is admittedly more lin-guistically motivated.Table 2 displays orientation probabilities for con-crete examples.
Each example was put under oneof the four categories that linguistically seems thebest match, and we provide probabilities for that cat-egory according to each model.
Note that, whilewe have so far only discussed left-to-right reorder-ing models, it is also possible to build right-to-leftmodels by substituting ai?1 with ai+1 in Eq.
3.
Ex-amples for right-to-left models appear in the secondhalf of the table.
The table strongly suggests thatthe hierarchical model more accurately determinesthe orientation of phrases with respect to large con-textual blocks.
In Examples 1 and 2, the hierarchi-cal model captures the fact that coordinated clausesalmost always remain in the same order, and thatwords should generally be forbidden to move fromone side of ?and?
to the other side, a constraint thatis difficult to enforce with the other two reorder-ing models.
In Example 4, the first two modelscompletely ignore that ?he said?
sometimes rotatesaround its neighbor clause.4 DecodingComputing reordering scores during decoding withword-based3 and phrase-based models (Tillman,2004) is trivial, since they only make use of localinformation to determine the orientation of a new in-coming block bi.
For a left-to-right ordering model,bi is scored based on its orientation with respect tobi?1.
For instance, if bi has a swap orientation withrespect to the previous phrase in the current trans-lation hypothesis, feature p(oi = S| .
.
.)
becomes ac-tive.Computing lexicalized reordering scores withthe hierarchical model is more complex, since themodel must identify contiguous blocks?monotoneor swapping?that can be merged into hierarchicalblocks.
The employed method is an instance of thewell-known shift-reduce parsing algorithm, and re-lies on a stack (S) of foreign substrings that havealready been translated.
Each time the decoder addsa new block to the current translation hypothesis, itshifts the source-language indices of the block ontoS, then repeatedly tries reducing the top two ele-ments of S if they are contiguous.4 This parsingalgorithm was first applied in computational geome-try to identify convex hulls (Graham, 1972), and itsrunning time was shown to be linear in the lengthof the sequence (a proof is presented in (Huang etal., 2008), which applies the same algorithm to thebinarization of SCFG rules).Figure 3 provides an example of the executionof this algorithm for the translation output shownin Figure 4, which was produced by a decoder in-corporating our hierarchical reordering model.
Thedecoder successively pushes source-language spans[1], [2], [3], which are successively merged into[1-3], and all correspond to monotone orientations.3We would like to point out an inconsistency in Moses be-tween training and testing.
Despite the fact that Moses estimatesa word-based orientation model during training (i.e., it analyzesthe orientation of a given phrase with respect to adjacent wordalignments), this model is then treated as a phrase-based orien-tation model during testing (i.e., as a model that orients phraseswith respect to other phrases).4It is not needed to store target-language indices onto thestack, since the decoder proceeds left to right, and thus suc-cessive blocks are always contiguous with respect to the targetlanguage.851Target phrase Source Op.
oi Stackthe russian side [1] S Mhopes [2] R M [1]to [3] R M [1-2]hold [11] S D [1-3]consultations [12] R M [11], [1-3]with iran [9-10] R S [11-12], [1-3]on this [6-7] S D [9-12], [1-3]issue [8] R,R M [6-7], [9-12], [1-3]in the near future [4-5] R,R S [6-12], [1-3].
[13] R,A M [1-12]Figure 3: The application of the shift-reduce parsing al-gorithm for identifying hierarchical blocks.
This execu-tion corresponds to the decoding example of Figure 4.Operations (Op.)
include shift (S), reduce (R), and ac-cept (A).
The source and stack columns contain source-language spans, which is the only information needed todetermine whether two given blocks are contiguous.
oi isthe label predicted by the hierarchical model by compar-ing the current block to the hierarchical phrase that is atthe top of the stack.!
"#$%&'()*+,-./0123456the russianside hopesto hold consultationswith iran on this issue in the near future .................. ................. ................. ................. ................. ................. ................. ................. .................................. ................. ..................................h 1h 2h 3Figure 4: Output of our phrase-based decoder using thehierarchical model on a sentence of MT06.
Hierarchicalphrases h1 and h2 indicate that with Iran and in the nearfuture have a swap orientation.
h3 indicates that ?to?
and?.?
are monotone.
In this particular example, distortionlimit was set to 10.It then encounters a discontinuity that prevents thenext block [11] from being merged with [1-3].
Asthe decoder reaches the last words of the sentence (inthe near future), [4-5] is successively merged with[6-12], then [1-3], yielding a stack that contains only[1-12].A nice property of this parsing algorithm is thatit does not worsen the asymptotic running timeof beam-search decoders such as Moses (Koehn,2004a).
Such decoders run in time O(n2), wheren is the length of the input sentence.
Indeed, eachtime a partial translation hypothesis is expanded intoa longer one, the decoder must perform an O(n) op-eration in order to copy the coverage set (indicatingwhich foreign words have already been translated)into the new hypothesis.
Since this copy operationmust be executed O(n) times, the overall time com-plexity is quadratic.
The incorporation of the shift-reduce parser into such a decoder does not worsenoverall time complexity: whenever the decoder ex-pands a given partial translation into a longer hy-pothesis, it simply copies its stack into the newlycreated hypothesis (similarly to copying the cover-age vector, this is an O(n) operation).
Hence, theincorporation of the hierarchical models describedin the paper into a phrase-based decoder preservesthe O(n2) running time.
In practice, we observebased on a set of experiments for Chinese-Englishand Arabic-English translation that our phrase-baseddecoder is on average only 1.35 times slower when itis running using hierarchical reordering features andthe shift-reduce parser.We finally note that the decoding algorithm pre-sented in this section can only be applied left-to-right if the decoder itself is operating left-to-right.In order to predict orientations relative to the right-to-left hierarchical reordering model, we must re-sort to approximations at decoding time.
We experi-mented with different approximations, and the onethat worked best (in the experiments discussed inSection 6) is described as follows.
First, we note thatan analysis of the alignment grid often reveals thatcertain orientations are impossible.
For instance, theblock issue in Figure 4 can only have discontinuousorientation with respect to what comes next in En-glish, since words surrounding the Chinese phrasehave already been translated.
When several hier-archical orientations are possible according to thealignment grid, we choose according to the follow-ing order of preference: (1) monotone, (2) swap, (3)discontinuous.
For instance, in the case of with iranin Figure 4, only swap and discontinuous orienta-tions are possible (monotone orientation is impossi-ble because of the block hold consultations), hencewe give preference to swap.
This prediction turnsout to be the correct one according to the decoding852steps that complete the alignment grid.5 DiscussionWe now analyze the system output of Figure 4 to fur-ther motivate the hierarchical model, this time fromthe perspective of the decoder.
We first observe thatthe prepositional phrase in the future should rotatearound a relatively large noun phrase headed by con-sultations.
Unfortunately, localized reordering mod-els such as (Tillman, 2004) have no means of identi-fying that such a displacement is a swap (S).
Accord-ing to these models, the orientation of in the futurewith respect to what comes previously is discontin-uous (D), which is an uninformative fall-back cate-gory.
By identifying h2 (hold ... issue) as a hierarchi-cal block, the hierarchical model can properly deter-mine that the block in the near future should have aswap orientation.5 Similar observations can be maderegarding blocks h1 and h3, which leads our modelto predict either monotone orientation (between h3and ?to?
and between h3 and ?.?)
or swap orienta-tion (between h1 and with Iran) while local modelswould predict discontinuous in all cases.Another benefit of the hierarchical model is thatits representation of phrases remains the same dur-ing both training and decoding, which is not the casefor word-based and phrase-based reordering mod-els.
The deficiency of these local models lies in thefact that blocks handled by phrase-based SMT sys-tems tend to be long at training time and short attest time, which has adverse consequences on non-hierarchical reordering models.
For instance, in Fig-ure 4, the phrase-based reordering model categorizesthe block in the near future as discontinuous, thoughif the sentence pair had been a training example,this block would count as a swap because of the ex-tracted phrase on this issue.6 ResultsIn our experiments, we use a re-implementationof the Moses decoder (Koehn et al, 2007).
Ex-cept for lexical reordering models, all other fea-tures are standard features implemented almost5Note that the hierarchical phrase hold ... issue is not a well-formed syntactic phrase ?
i.e., it neither matches the bracketingof the verb phrase hold ... future nor matches the noun phraseconsultations ... issue ?
yet it enables sensible reordering.exactly as in Moses: four translation features(phrase-based translation probabilities and lexically-weighted probabilities), word penalty, phrasepenalty, linear distortion, and language model score.We experiment with two language pairs: Chinese-to-English (C-E) and Arabic-to-English (A-E).
ForC-E, we trained translation models using a subset ofthe Chinese-English parallel data released by LDC(mostly news, in particular FBIS and Xinhua News).This subset comprises 12.2M English words, and11M Chinese words.
Chinese words are segmentedwith a conditional random field (CRF) classifier thatconforms to the Chinese Treebank (CTB) standard.The training set for our A-E systems also includesmostly news parallel data released by LDC, andcontains 19.5M English words, and 18.7M Arabictokens that have been segmented using the ArabicTreebank (ATB) (Maamouri et al, 2004) standard.6For our language model, we trained a 5-grammodel using the Xinhua and AFP sections of theGigaword corpus (LDC2007T40), in addition to thetarget side of the parallel data.
For both C-E andA-E, we manually removed documents of Gigawordthat were released during periods that overlap withthose of our development and test sets.
The languagemodel was smoothed with the modified Kneser-Neyalgorithm, and we kept only trigrams, 4-grams, and5-grams that respectively occurred two, three, andthree times in the training data.Parameters were tuned with minimum error-ratetraining (Och, 2003) on the NIST evaluation set of2006 (MT06) for both C-E and A-E.
Since MERTis prone to search errors, especially with large num-bers of parameters, we ran each tuning experimentfour times with different initial conditions.
This pre-caution turned out to be particularly important in thecase of the combined lexicalized reordering models(the combination of phrase-based and hierarchicaldiscussed later), since MERT must optimize up to26 parameters at once in these cases.7 For testing,6Catalog numbers for C-E: LDC2002E18, LDC2003E07,LDC2003E14, LDC2005E83, LDC2005T06, LDC2006E26,and LDC2006E8.
For A-E: LDC2007E103, LDC2005E83,LDC2006E24, LDC2006E34, LDC2006E85, LDC2006E92,LDC2007E06, LDC2007E101, LDC2007E46, LDC2007E86,and LDC2008E40.7We combine lexicalized reordering models by simply treat-ing them as distinct features, which incidentally increases thenumber of model parameters that must be tuned with MERT.85330.53131.53232.53333.5340  2  4  6  8  10  12  14BLEU[%],Chinese-Englishdistortion limithierarchicalphrase-basedword-basedbaseline4343.54444.54545.50  2  4  6  8  10BLEU[%],Arabic-Englishdistortion limithierarchicalphrase-basedword-basedbaselineFigure 5: Performance on the Chinese-English andArabic-English development sets (MT06) with increas-ing distortion limits for all lexicalized reordering mod-els discussed in the paper.
Our novel hierarchical modelsystematically outperforms all other models for distortionlimit equal to or greater than 4.
The baseline is Moseswith no lexicalized reordering model.we used the NIST evaluation sets of 2005 and 2008(MT05 and MT08) for Chinese-English, and the testset of 2005 (MT05) for Arabic-English.Statistical significance is computed using theapproximate randomization test (Noreen, 1989),whose application to MT evaluation (Riezler andMaxwell, 2005) was shown to be less sensitive totype-I errors (i.e., incorrectly concluding that im-provement is significant) than the perhaps morewidely used bootstrap resampling method (Koehn,2004b).Tuning set performance is shown in Figure 5.Since this paper studies various ordering models,it is interesting to first investigate how the distor-LEXICALIZED REORDERING MT06 MT05 MT08none 31.85 29.75 25.22word-based 32.96 31.45 25.86phrase-based 33.24 31.23 26.01hierarchical 33.80** 32.20** 26.38phrase-based + hierarchical 33.86** 32.85** 26.53*Table 3: BLEU[%] scores (uncased) for Chinese-Englishand the orientation categories {M,S,D}.
Maximum dis-tortion is set to 6 words, which is the default in Moses.The stars at the bottom of the tables indicate when a givenhierarchical model is significantly better than all localmodels for a given development or test set (*: signifi-cance at the .05 level; **: significance at the .01 level).LEXICALIZED REORDERING MT06 MT05 MT08phrase-based 33.79 32.32 26.32hierarchical 34.01 32.35 26.58phrase-based + hierarchical 34.36** 32.33 27.03**Table 4: BLEU[%] scores (uncased) for Chinese-Englishand the orientation categories {M,S,Dl ,Dr}.
Since thedistinction between these four categories is not availablein Moses, hence we have no baseline results for this case.Maximum distortion is set to 6 words.tion limit affects performance.8 As has been shownin previous work in Chinese-English and Arabic-English translation, limiting phrase displacements tosix source-language words is a reasonable choice.For both C-E and A-E, the hierarchical model is sig-nificantly better (p ?
.05) than either other modelsfor distortion limits equal to or greater than 6 (ex-cept for distortion limit 12 in the case of C-E).
Sincea distortion limit of 6 works reasonably well for bothlanguage pairs and is the default in Moses, we usedthis distortion limit value for all test-set experimentspresented in this paper.Our main results for Chinese-English are shownin Table 3.
It appears that hierarchical models pro-vide significant gains over all non-hierarchical mod-els.
Improvements on MT06 and MT05 are very sig-nificant (p ?
.01).
In the case of MT08, significantimprovement is reached through the combination ofboth phrase-based and hierarchical models.
We of-ten observe substantial gains when we combine suchmodels, presumably because we get the benefit ofidentifying both local and long-distance swaps.Since most orientations in the phrase-based modelare discontinuous, it is reasonable to ask whether8Note that we ran MERT separately for each distinct distor-tion limit.854LEXICALIZED REORDERING MT06 MT05none 44.03 54.87word-based 44.64 54.96phrase-based 45.01 55.09hierarchical 45.51* 55.50*phrase-based + hierarchical 45.64** 56.01**Table 5: BLEU[%] scores (uncased) for Arabic-Englishand the reordering categories {M,S,D}.LEXICALIZED REORDERING MT06 MT05phrase-based 44.74 55.52hierarchical 45.53** 56.02**phrase-based + hierarchical 45.63** 56.07**Table 6: BLEU[%] scores (uncased) for Arabic-Englishand the reordering categories {M,S,Dl ,Dr}.the relatively poor performance of the phrase-basedmodel is the consequence of an inadequate set of ori-entation labels.
To try to answer this question, weuse the set of orientation labels {M,S,Dl,Dr} de-scribed in Section 3.
Results for this different set oforientations are shown in Table 4.
While the phrase-based model appears to benefit more from the dis-tinction between left- and right-discontinuous, sys-tems that incorporate hierarchical models remain themost competitive overall: their best performance onMT06, MT05, and MT08 are respectively 34.36,32.85, and 27.03.
The best non-hierarchical modelsachieve only 33.79, 32.32, and 26.32, respectively.All these differences (i.e., .57, .53, and .71) are sta-tistically significant at the .05 level.Our results for Arabic-English are shown in Ta-bles 5 and 6.
Similarly to C-E, we provide results fortwo orientation sets: {M,S,D} and {M,S,Dl,Dr}.We note that the four-class orientation set is overallless effective for A-E than for C-E.
This is probablydue to the fact that there is less probability mass inA-E assigned to the D category, and thus it is lesshelpful to split the discontinuous category into two.For both orientation sets, we observe in A-E thatthe hierarchical model significantly outperforms thelocal ordering models.
Gains provided by the hierar-chical model are no less significant than for Chinese-to-English.
This positive finding is perhaps a bitsurprising, since Arabic-to-English translation gen-erally does not require many word order changescompared to Chinese-to-English translation, and thistranslation task so far has seldom benefited from hi-erarchical approaches to MT.
In our case, one possi-ble explanation is that Arabic-English translation isbenefiting from the fact that orientation predictionsof the hierarchical model are consistent across train-ing and testing, which is not the case for the otherordering models discussed in this paper (see Sec-tion 4).
Overall, hierarchical models are the mosteffective on the two sets: their best performances onMT06 and MT05 are respectively 45.64 and 56.07.The best non-hierarchical models obtain only 45.01and 55.52 respectively for the same sets.
All thesedifferences (i.e., .63 and .55) are statistically signifi-cant at the .05 level.7 Conclusions and Future WorkIn this paper, we presented a lexicalized orientationmodel that enables phrase movements that are morecomplex than swaps between adjacent phrases.
Thismodel relies on a hierarchical structure that is builtas a by-product of left-to-right phrase-based decod-ing without increase of asymptotic running time.
Weshow that this model provides statistically signifi-cant improvements for five NIST evaluation sets andfor two language pairs.
In future work, we planto extend the parameterization of our models to notonly predict phrase orientation, but also the length ofeach displacement as in (Al-Onaizan and Papineni,2006).
We believe such an extension would improvetranslation quality in the case of larger distortionlimits.
We also plan to experiment with discrimi-native approaches to estimating reordering probabil-ities (Zens and Ney, 2006; Xiong et al, 2006), whichcould also be applied to our work.
We think the abil-ity to condition reorderings on any arbitrary featurefunctions is also very effective in the case of our hi-erarchical model, since information encoded in thetrees would seem beneficial to the orientation pre-diction task.8 AcknowledgementsThe authors wish to thank the anonymous reviewersfor their comments on an earlier draft of this paper.This paper is based on work funded by the DefenseAdvanced Research Projects Agency through IBM.The content does not necessarily reflect the views ofthe U.S. Government, and no official endorsementshould be inferred.855ReferencesYaser Al-Onaizan and Kishore Papineni.
2006.
Distor-tion models for statistical machine translation.
In Pro-ceedings of the 21st International Conference on Com-putational Linguistics and the 44th annual meeting ofthe ACL (COLING/ACL), pages 529?536, Morristown,NJ, USA.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofthe 43rd Annual Meeting of the Association for Com-putational Linguistics (ACL), pages 263?270, June.Ronald L. Graham.
1972.
An efficient algorithm for de-termining the convex hull of a finite planar set.
Infor-mation Processing Letters, 1(4):132?133.Liang Huang, Hao Zhang, Daniel Gildea, and KevinKnight.
2008.
Binarization of synchronous context-free grammars.
Technical report, University of Penn-sylvania.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In NAACL?03: Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology.Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,Christopher Callison-Burch, Marcello Federico,Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-tine Moran, Richard Zens, Chris Dyer, Ondrej Bojar,Alexandra Constantin, and Evan Herbst.
2007.Moses: Open source toolkit for statistical machinetranslation.
In Proceedings of the 45th Annual Meet-ing of the Association for Computational Linguistics(ACL), Demonstration Session.Philipp Koehn.
2004a.
Pharaoh: a beam search decoderfor phrase-based statistical machine translation mod-els.
In Proceedings of the Sixth Conference of theAssociation for Machine Translation in the Americas,pages 115?124.Philipp Koehn.
2004b.
Statistical significance tests formachine translation evaluation.
In Proceedings of the2004 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 388?395.M.
Maamouri, A. Bies, T. Buckwalter, and W. Mekki.2004.
The Penn Arabic treebank: Building a large-scale annotated Arabic corpus.I.
Dan Melamed.
2003.
Multitext grammars and syn-chronous parsers.
In NAACL ?03: Proceedings of the2003 Conference of the North American Chapter of theAssociation for Computational Linguistics on HumanLanguage Technology, pages 79?86.Eric W. Noreen.
1989.
Computer Intensive Methodsfor Testing Hypotheses.
An Introduction.
Wiley, NewYork.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine transla-tion.
Computational Linguistics, 30(4):417?449.F.
Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng,V.
Jain, Z. Jin, and D. Radev.
2004.
A smorgasbordof features for statistical machine translation.
In Pro-ceedings of HLT-NAACL.Franz Josef Och.
2003.
Minimum error rate training forstatistical machine translation.
In Proceedings of the41st Annual Meeting of the Association for Computa-tional Linguistics (ACL).Stefan Riezler and John T. Maxwell.
2005.
On somepitfalls in automatic evaluation and significance test-ing for MT.
In Proceedings of the ACL Workshop onIntrinsic and Extrinsic Evaluation Measures for Ma-chine Translation and/or Summarization, pages 57?64, June.Christoph Tillman.
2004.
A unigram orientation modelfor statistical machine translation.
In Proceedings ofHLT-NAACL 2004: Short Papers, pages 101?104.Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.2006.
Left-to-right target generation for hierarchicalphrase-based translation.
In ACL ?06: Proceedings ofthe 21st International Conference on ComputationalLinguistics and the 44th annual meeting of the ACL,pages 777?784.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?404.Deyi Xiong, Qun Liu, and Shouxun Lin.
2006.
Maxi-mum entropy based phrase reordering model for sta-tistical machine translation.
In ACL-44: Proceedingsof the 21st International Conference on ComputationalLinguistics and the 44th annual meeting of the Associ-ation for Computational Linguistics, pages 521?528.Richard Zens and Herman Ney.
2006.
Discriminative re-ordering models for statistical machine translation.
InHuman Language Technology Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics (HLT-NAACL): Proceedings of theWorkshop on Statistical Machine Translation, pages55?63, New York City, NY, June.856
