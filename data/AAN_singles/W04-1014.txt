Evaluation Measures Considering Sentence Concatenation forAutomatic Summarization by Sentence or Word ExtractionChiori Hori, Tsutomu Hirao and Hideki IsozakiNTT Communication Science Laboratories{chiori, hirao, isozaki}@cslab.kecl.ntt.co.jpAbstractAutomatic summaries of text generated throughsentence or word extraction has been evaluated bycomparing them with manual summaries generatedby humans by using numerical evaluation measuresbased on precision or accuracy.
Although sentenceextraction has previously been evaluated based onlyon precision of a single sentence, sentence concate-nations in the summaries should be evaluated aswell.
We have evaluated the appropriateness of sen-tence concatenations in summaries by using eval-uation measures used for evaluating word concate-nations in summaries through word extraction.
Wedetermined that measures considering sentence con-catenation much better reflect the human judgmentrather than those based only on the precision of asingle sentence.1 IntroductionSummarization Target and ApproachThe amount of text is explosively increasing day byday, and it is becoming very difficult to manage in-formation by reading all the text.
To manage infor-mation easily and find target information quickly,we need technologies for summarizing text.
Al-though research into text summarization started inthe 1950?s, it is still largely in the research phase(Mani and Maybury, 1999).
Several projects ontext summarization have been carried out.
1 Inthese project, text summarization has so far focusedon summarizing single documents through sentenceextraction.
Recently, summarizing multiple docu-ments with the same topic has been made a tar-get.
The major approach to extracting sentences thathave significant information is statistical, i.e., su-pervised learning from parallel corpora consistingof original texts and their summarization (Kupiec et1SUMMAC in the Tipster project by DARPA (http://www-nlpir.nist.gov/related projects/tipster summac) and DUC inthe TIDES project (http://duc.nist.gov/) in the U.S. TSC(http://research.nii.ac.jp/ntcir/) in the NTCIR by NII (The Na-tional Institute of Informatica) in Japan.al., 1995) (Aone et al, 1998) (Mani and Bloedorn,1998).Several summarization techniques for multime-dia including image, speech, and text have been re-searched.
Manually transcribed newswire speech(TDT data) and meeting speech (Zechner, 2003)have been set as summarization targets.
The needto automatically generate summaries from speechhas led to research on summarizing transcription re-sults obtained by automatic speech recognition in-stead of manually transcribed speech (Hori and Fu-rui, 2000a).
This summarization approach is wordextraction (sentence compaction) that attempts toextract significant information, exclude acousticallyand linguistically unreliable words, and maintainthe meanings of the original speech.The summarization approaches that have beenmainly researched so far are extracting sentencesor words from original text or transcribed speech.There has also been research on generating an ?ab-stract?
like the much higher level summarizationcomposed freely by human experts (Jing, 2002).This approach includes not only extracting sen-tences but also combining sentences to generate newsentences, replacing words, reconstructing syntacticstructure, and so on.Evaluation Measures for SummarizationMetrics that can be used to accurately evaluatethe various appropriateness to summarization areneeded.The simplest and probably the ideal way ofevaluating automatic summarization is to have hu-man subjects read the summaries and evaluate themin terms of the appropriateness of summarization.However, this type of evaluation is too expensivefor comparing the efficiencies of many different ap-proaches precisely and repeatedly.
We thus need au-tomatic evaluation metrics to numerically validatethe efficiency of various approaches repeatedly andconsistently.Automatic summaries can be evaluated by com-paring them with manual summaries generated byhumans.
The similarities between the targets andthe automatically processed results provide metricsindicating the extent to which the task was accom-plished.
The similarity that can better reflect sub-jective judgments is a better metric.To create correct answers for automatic sum-marization, humans generate manual summariesthrough sentence or word extraction.
However,references consisting of manual summaries varyamong humans.
The problems in validating auto-matic summaries by comparing them with variousreferences are as follows:?
correct answers for automatic results cannot beunified because of subjective variation,?
the coverage of correct answers in the collectedmanual summaries is unknown, and?
the reliability of references in the collectedmanual summaries is not always guaranteed.When the similarity between automatic resultsand references is used for the evaluation metrics,the similarity determination function counts over-lapping of each component or sequence of com-ponents in the automatic results.
If concatenationsbetween components in a summary had no mean-ing, the overlap of a single component between theautomatic results and the references can representthe extent of summarization.
However, concatena-tions between sentences or words have meanings,so some concatenations of sentences or words in theautomatic summaries sometimes generate meaningsdifferent from the original.
The evaluation metricsfor summarization should thus consider each con-catenation between components in the automatic re-sults.To evaluate sentence automatically generatedwith taking consideration word concatenation intoby using references varied among humans, vari-ous metrics using n-gram precision and word ac-curacy have been proposed: word string preci-sion (Hori and Furui, 2000b) for summarizationthrough word extraction, ROUGE (Lin and Hovy,2003) for abstracts, and BLEU (Papineni et al,2002) for machine translation.
Evaluation metricsbased on word accuracy, summarization accuracy(SumACCY), using a word network made by merg-ing manual summaries has been proposed (Hori andFurui, 2001).
In addition, to solve the problems forthe coverage of correct answers and the reliabilityof manual summaries as correct answers, weightedsummarization accuracy (WSumACCY) in whichSumACCY is weighted by the majority of the hu-mans?
selections, has been proposed (Hori and Fu-rui, 2003a).In contrast, summarization through sentence ex-traction has been evaluated using only single sen-tence precision.
Sentence extraction should also beevaluated using measures that take into account sen-tence concatenations, the coverage of correct an-swers, and the reliability of manual summaries.This paper presents evaluation results of auto-matic summarization through sentence or word ex-traction using the above mentioned metrics based onn-gram precision and sentence/word accuracy andexamines how well these measures reflect the judg-ments of humans as well.2 Evaluation Metrics for ExtractionIn summarization through sentence or word extrac-tion under a specific summarization ratio, the orderof the sentences or words and the length of the sum-maries are restricted by the original documents orsentences.
Metrics based on the accuracy of thecomponents in the summary is a straight-forwardapproach to measuring similarities between the tar-get and automatic summaries.2.1 AccuracyIn the field of speech recognition, automatic recog-nition results are compared with manual transcrip-tion results.
The conventional metric for speechrecognition is recognition accuracy calculated basedon word accuracy:ACCY= Len ?
(Sub + Ins + Del)Len ?
100[%], (1)where Sub, Ins, Del, and Len are the numbersof substitutions, insertions, deletions, and words inthe manual transcription, respectively.
Althoughword accuracy cannot be used to directly evaluatethe meanings of sentences, higher accuracy indi-cates that more of the original information has beenpreserved.
Since the meaning of the original doc-uments is generated by combining sentences, thismetric can be applied to the evaluation for sentenceextraction.
Sentence accuracy defined by eq.
(1)with words replaced by sentences represents howmuch the automatic result is similar to the answerand how well it preserves the original meaning.Accuracy is the simplest and most efficient metricwhen the target for the automatic summaries can beset as only one answer.
However, there are usuallymultiple targets for each automatic summary due tothe variation in manual summarization among hu-mans.
Therefore, it is not easy to use accuracy toevaluate automatic summaries.
Subjective variationresults into two problems:?
how to consider all possible correct answers inthe manual summaries, and?
how to measure the similarity between theevaluation sentence and multiple manual sum-maries.If we could collect all possible manual sum-maries, the one most similar to the automatic re-sult could be chosen as the correct answer and usedfor the evaluation.
The sentence or word accuracycompared with the most similar manual summary isdenoted as NrstACCY.
However, in real situations,the number of manual summaries that could be col-lected is limited.
The coverage of correct answers inthe collected manual summaries is unknown.
Whenthe coverage is low, the summaries are comparedwith inappropriate targets, and the NrstACCY ob-tained by such comparison does not provide an effi-cient measure.2.2 N-gram PrecisionOne way to cope with the coverage problem is touse local matching of components or componentstrings with all the manual summaries instead ofusing a measure comparing a word sequence as awhole sentence, such as NrstACCY.
The similar-ity can be measured by counting the precision, i.e.,the number of sentence or word n-gram overlappingbetween the automatic result and all the references.Even if there are multiple targets for an automaticsummary, the precision of components in each orig-inal can be used to evaluate the similarity betweenthe automatic result and the multiple references.Precision is an efficient way of evaluating the sim-ilarity of component occurrence between automaticresults and targets with a different order of compo-nents and different lengths.In the evaluation of summarization through ex-traction, a component occurring in a different loca-tion in the original is considered to be a differentcomponent even if it is the same component as onein the result.
When an answer for the automatic re-sult can be unified and the lengths of the automaticresult and its answer are the same, accuracy countsinsertion errors and deletion errors and thus has boththe precision and recall characteristics.Since meanings are basically conveyed by wordstrings rather than single words, word string preci-sion (Hori and Furui, 2000b) can be used to evalu-ate linguistic precision and the maintenance of theoriginal meanings of an utterance.
In this method,word strings of various lengths, that is n-grams, areused as components for measuring precision.
Theextraction ratio, pn, of each word string consist-ing of n words in a summarized sentence, V =v1, v2, .
.
.
, vM , is given bypn =M?m=n?
(vm?n+1, .
.
.
, vm?1, vm)M ?
n + 1 , (2)where?
(un) ={ 1 if un ?
Un0 if un /?
Un , (3)un: each word string consisting of n wordsUn: a set of word strings consisting of n wordsin all manual summarizations.When n is 1, pn corresponds to the precision ofeach word, and when n is the same length as asummarized sentence (n = M ), pn indicates theprecision of the summarized sentence itself.2.3 Summarization Accuracy: SumACCYSummarization accuracy (SumACCY) was pro-posed to cope with the problem of correct answercoverage and various references among humans(Hori and Furui, 2001).
To cover all possible correctanswers for summarization using a limited numberof manual summaries, all the manual summariesare merged into a word network.
In this evaluationmethod, the word sequence in the network closest tothe evaluation word sequence is considered to be thetarget answer.
The word accuracy of the automaticresult is calculated in comparison with the target an-swer extracted from the network.Since summarization is processed by extractingwords from an original; the words cannot be re-placed by other words, and the order of words can-not be changed.
Multiple manual summaries canbe combined into a network that represents the vari-ations.
Each set of words that could be extractedfrom the network consists of words and word stringsoccurring at least once in all the manual summaries.The network made by the manual summaries canbe considered to represent all possible variations ofcorrect summaries.SUB The beautiful cherry blossoms in Japan bloom in springA The cherry blossoms in JapanB cherry blossoms in Japan bloomC beautiful cherry bloom in springD beautiful cherry blossoms in springE The beautiful cherry blossoms bloomTable 1: Example of manual summarization by sen-tence compaction<s> </s> The beautiful cherry blossoms inJapan bloomin springFigure 1: Word network made by merging manualsummariesThe sentence ?The beautiful cherry blossoms inJapan bloom in spring.?
is assumed to be manuallysummarized as shown in Table 1.
In this example,five words are extracted from the nine words.
There-fore, the summarization ratio is 56%.
The variationsof manual summaries are merged into a word net-work, as shown in Fig.
1.
We use <s> and </s>as the beginning and ending symbols of a sentence.Although ?Cherry blossoms bloom in spring?
is notamong the manual answers in Table 1, this sentence,which could be extracted from the network, is con-sidered a correct answer.When references consisting of manual sum-maries cannot cover all possible answers and lackthe appropriate answer for an automatic summary,SumACCY calculated using such a network is bet-ter than NrstACCY for evaluating the automatic re-sult.
This evaluation method gives a penalty foreach word concatenation in the automatic resultsthat is excluded in the network, so it can be usedto evaluate the sentence-level appropriateness moreprecisely than matching each word in all the refer-ences.2.4 Weighted SumACCY: WSumACCYIn SumACCY, all possible sets of words extractedfrom the network of manually summarized sen-tences are equally used as target answers.
How-ever, the set of words containing word strings se-lected by many humans would presumably be betterand give more reliable answers.
To obtain reliabilitythat reflects the majority of selections by humans,the summarization accuracy is weighted by a pos-terior probability based on the manual summariza-tion network.
The reliability of a sentence extractedfrom the network is defined as the product of theratios of the number of subjects who selected eachword to the total number of subjects.
The weightedsummarization accuracy is given byWSumACCY= P?
(v1 .
.
.
vM |R) ?
SumACCYP?
(v?1 .
.
.
v?M?
|R), (4)where P?
(v1 .
.
.
vM |R) is the reliability score of aset of words v1 .
.
.
vM in the manual summariza-tion network, R, and M represents the total num-ber of words in the target answer.
The set of wordsv?1 .
.
.
v?M?
represents the word sequence that maxi-mizes the reliability score, P?
(?|R), given byP?
(v1 .
.
.
vM |R)=( M?m=2C(vm?1, vm|R)HR)1M?1, (5)where vm is the m-th word in the sentence ex-tracted from the network as the target answer, andC(x, y|R) indicates the number of subjects who se-lected the word connection of x and y.
Here, ?wordconnection?
means an arc in the manual summariza-tion network.
HR is the number of subjects.2.5 Evaluation ExperimentsNewspaper articles and broadcast news speech wereautomatically summarized through sentence extrac-tion and word extraction respectively under thegiven summarization ratio, which is the ratio of thenumbers of sentences or words in the summary tothat in the original.The automatic summarization results were sub-jectively evaluated by ten human subjects.
The sub-jects read these summaries and rated each one from1 (incorrect) to 5 (perfect).
The automatic sum-maries were also evaluated by using the numericalmetrics SumACCY, WSumACCY, NrstACCY,and n-gram precision (1 ?
n ?
5) in compari-son with reference summaries generated by humans.The precisions of 1-gram, .
.
., 5-gram are denotedPREC1, .
.
., PREC5.
The numerical evaluation re-sults were averaged over the number of automaticsummaries.Note that the subjects who judged the automaticsummaries did not include anyone who generatedthe references.
To examine the similarity of the hu-man judgments and that of the manual summaries,the kappa statistics, ?, was calculated using eq.
(A-1) in the Appendix.Finally, to examine how much the evaluationmeasures reflected the human judgment, the correla-tion coefficients between the human judgments andthe numerical evaluation results were calculated.Sentence extractionSixty articles in Japanese newspaper published in94, 95, and 98 were automatically summarized witha 30% summarization ratio.
Half the articles weregeneral news report (NEWS), and other half werecolumns (EDIT).The automatic summarization was performed us-ing a Support Vector Machine (SVM) (Hirao et al,2003), random extraction (RDM), the lead method(LEAD) extracting sentences from the head of ar-ticles.
In comparison with these automatic sum-maries, manual summaries (TSC) was also evalu-ated.These 4 types of summaries, SVM, RDM, LEAD,and TSC were read and rated 1 to 5 by 10 humans.The summaries were evaluated in terms of extrac-tion of significance information (SIG), coherenceof sentences (COH), maintenance of original mean-ings (SEM), and appropriateness of summary as awhole (WHOLE).To numerically evaluate the results using the ob-jective metrics, 20 other human subjects gener-ated manual summaries through sentence extrac-tion.
These manual summaries were set as the targetset for the automatic summaries.Word extractionJapanese TV news broadcasts aired in 1996 wereautomatically recognized and summarized sentenceby sentence (Hori and Furui, 2003b).
They con-sisted of 50 utterances by a female announcer.
Theout-of-vocabulary (OOV) rate for the 20k word vo-cabulary was 2.5%, and the test-set perplexity was54.5.
Fifty utterances with word recognition accu-racy above 90%, which was the average rate over the50 utterances, were selected and used for the evalu-ation.
The summarization ratio was set to 40%.Nine automatic summaries with various summa-rization accuracies from 40% to 70% and a manualsummary (SUB) were selected as a test set.
Theseten summaries for each utterance were judged interms of the appropriateness of the summary as awhole (WHOLE).To numerically evaluate the results using the ob-jective metrics, 25 humans generated manual sum-maries through word extraction.
These manualsummaries were set as a target set for the automaticsummaries, and merged into a network.
Note that aset of 24 manual summaries made by other subjectswas used as the target for SUB.2.6 Evaluation ResultsFigures 2 and 3 show the correlation coefficientsbetween the judgments of the subjects and the nu-merical evaluation results for EDIT and NEWS.They show that the measures based on accuracymuch better reflected human judgments than thoseof the n-gram precisions for evaluating SIG andWHOLE for both EDIT and NEWS.
On the otherhand, PREC2 better reflected the human judgmentsfor evaluating COH and SEM.
These results showthat measures taking into account sentence concate-nations better reflected human judgments than sin-gle component precision.
The precisions of longer00.20.40.60.81.0SIG COH SEM WHOLECorrelationcoefficientEvaluation pointSumACCYWSumACCYNrstACCYPREC1PREC2PREC3PREC4PREC5Figure 2: Correlation coefficients between humanjudgment and numerical evaluation results for EDIT00.20.40.60.81.0SIG COH SEM WHOLECorrelationcoefficientEvaluation pointSumACCYWSumACCYNrstACCYPREC1PREC2PREC3PREC4PREC5Figure 3: Correlation coefficients between hu-man judgment and numerical evaluation results forNEWSsentence strings (PREC3 to PREC5) didn?t reflectthe human judgments for all the conditions.
Theseresults show that meanings of the original article canmaintain by the concatenations of only a few sen-tences in summarization through sentence extrac-tion.Table 2 lists the kappa statistics for the manualsummaries and the human judgments for EDIT andNEWS.
The manual results varied among humansDATA SUMMARIES ?EDIT manual summaries 0.35NEWS manual summaries 0.39Table 2: Kappa statistics for manual summaries andhuman judgments for sentence extraction.and the similarity among humans was low.
Thekappa statistics for NEWS is slightly higher thanthat for EDIT.
The difference of similarities amongmanual summaries is due to the difference in struc-tures of information in each article.
Although thearticles in EDIT had a discourse structure, NEWShad isolated and stereotyped information scatteredthroughout the articles.While the human judgments for NEWS were sim-ilar, those for EDIT varied.The difficulty in evaluat-ing COH and SEM in EDIT is due to the variationin both manual summaries and human judgment.Figure 4 shows the correlation coefficients be-tween the judgments of the subjects and the numer-ical evaluation results for summaries of broadcastnews speech through word extraction.
Table 3 lists00.20.40.60.81WHOLECorrelationcoefficientsEvaluation pointPREC1PREC2PREC3PREC4PREC5SumACCYWSumACCNrstACCYFigure 4: Correlation coefficients between humanjudgment and numerical evaluation results for sum-maries through word extractionthe kappa statistics for the manual summaries andthe human judgments for summaries through wordextraction.
In word extraction, the human judg-DATA SUMMARIES ?Broadcast news manual summaries 0.47Table 3: Kappa statistics for manual summaries andhuman judgments for word extractionments and the manual summaries were very similaramong the subjects.As shown in figure 4, WSumACCY yielded thebest correlation to the human judgments.
Thismeans that the correctness as a sentence and theweight (that is how many subjects support the ex-tracted phrases in summarized sentences) are im-portant in summarization through word extraction.In comparison with the results of sentence extrac-tion in Figures 2 and 3, PREC1 effectively reflectedthe human judgments for word extraction.
Since inthe manual summarized sentences through word ex-traction under the low summarization ratio, the sen-tences were summarized based on significance wordextraction rather than syntactic structure mainte-nance to generate grammatically correct sentences.3 ConclusionWe have presented the results of evaluatingthe appropriateness of the sentence concatena-tions in summaries generated using SumACCY,WSumACCY, NrstACCY and n-gram precision.We found that the measures taking into account sen-tence concatenation much better reflected the judg-ments of humans than did the single sentence pre-cision, so the concatenation of sentences in sum-maries should be evaluated.Although the human judgments and the man-ual summaries for word extraction did not varymuch among the subjects, those for sentence extrac-tion for single article summarization greatly variedamong the subjects.
As a result, it is very difficult toset correct answers for single article summarizationthrough sentence extraction.Future works involves experiments to examinethe efficiency of each numerical measures in re-sponse to the coverage of correct answers.4 AcknowledgmentsWe thank NHK (Japan Broadcasting Corporation)for providing the broadcast news database.
Wealso thank Prof. Sadaoki Furui at Tokyo Instituteof Technology for providing the summaries of thebroadcast news speech.ReferencesC.
Aone, M. Okurowski, and J. Gorlinsky.
1998.Trainable scalable summarization using robustNLP and machine learning.
In Proceedings ACL,pages 62?66.J.
Carletta.
1996.
Assessing agreement on classifi-cation tasks: The kappa statistic.
ComputationalLinguistics, 22(2):249?254.T.
Hirao, K. Takeuchi, H. Isozaki, Y. Sasaki, andE.
Maeda.
2003.
SVM-based multi-documentsummarization integrating sentence extractionwith bunsetsu elimination.
IEICE Trans.
Inf.
&Syst., E86-D(9):1702?1709.C.
Hori and S. Furui.
2000a.
Automatic speechsummarization based on word significance andlinguistic likelihood.
In Proceedings ICASSP,volume 3, pages 1579?1582.C.
Hori and S. Furui.
2000b.
Improvements inautomatic speech summarization and evaluationmethods.
In Proceedings ICSLP, volume 4,pages 326?329.C.
Hori and S. Furui.
2001.
Advances in auto-matic speech summarization.
In Proceedings Eu-rospeech, volume 3, pages 1771?1774.C.
Hori and S. Furui.
2003a.
Evaluation methodsfor automatic speech summarization.
In Proceed-ings Eurospeech, pages 2825?2828.C.
Hori and S. Furui.
2003b.
A new approach toautomatic speech summarization.
IEEE Transac-tions on Multimedia, 3:368?378.H.
Jing.
2002.
Using hidden markov modeling todecompose human-written summaries.
Compu-tational Linguistics, 28(4):527?543.J.
Kupiec, J. Pedersen, and F. Chen.
1995.
A train-able document summarizer.
In Proceedings ofthe 18th ACM-SIGIR, pages 68?73.Chin-Yew Lin and E. H. Hovy.
2003.
Auto-matic evaluation of summaries using n-gramco-occurrence statistics.
In Proceedings HLT-NAACL.I.
Mani and E. Bloedorn.
1998.
Machine learningof general and user-focused summarization.
InProceedings of the 15th National Conference onArtificial Intelligence, pages 821?826.I.
Mani and M. Maybury.
1999.
Advances in Auto-matic Text Summarization.
The MIT Press.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.Bleu: a method for automatic evaluation of ma-chine translation.
In Proceedings ACL.K.
Takeuchi and Y. Matsumoto.
2001.
Relation be-tween text structure and linguistic clues: An in-vestigation on text structure of newspaper arti-cles.
Mathematical Linguistics, 22(8).K.
Zechner.
2003.
Automatic summarization ofopen-domain multiparty dialogues in diversegenres.
Computational Linguistics, 28(4):447?485.Appendix?
is given by?
= P (A) ?
P (E)1 ?
P (E) , (A-1)where P (A) and P (E) are the probabilities of hu-man agreement and chance agreement, respectively,so ?
is adjusted by the possibility of chance agree-ment.
This measure was used to assess agreement ofhuman selections for discourse segmentation (Car-letta, 1996).In this study, kappa was calculated using a tableof objects and categories (Takeuchi and Matsumoto,2001).
P (A) was calculated usingP (A) = 1NN?i=1Si, (A-2)where N is the number of trials to select one classamong all classes, and Si is the probability that twohumans at least agree at the i-th selection:Si =m?j=1nij C2kC2, (A-3)where k and m are the number of subjects andclasses, respectively.
When the task is sentence orword extraction, the number of classes is two, i.e.,extract/not extract.
The numerator of eq.
(A-3)shows the sum of the combinations that two humansat least agree for each class; nij is the number of hu-mans who select the j-th class at the i-th selection.P (E) is the probability of chance agreement byat least two humans:P (E) =m?j=1pj2, (A-4)where pj is the probability of selecting the j-th classgiven byPj =N?i=1nijNk , (A-5)where the total number of humans who select the j-th class for each trial is divided by the total numberof trials performed by all humans.
