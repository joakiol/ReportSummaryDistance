Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 449?459,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsDetecting Highly Confident Word Translations from ComparableCorpora without Any Prior KnowledgeIvan Vulic?
and Marie-Francine MoensDepartment of Computer ScienceKU LeuvenCelestijnenlaan 200ALeuven, Belgium{ivan.vulic,marie-francine.moens}@cs.kuleuven.beAbstractIn this paper, we extend the work on usinglatent cross-language topic models for iden-tifying word translations across compara-ble corpora.
We present a novel precision-oriented algorithm that relies on per-topicword distributions obtained by the bilin-gual LDA (BiLDA) latent topic model.The algorithm aims at harvesting only themost probable word translations across lan-guages in a greedy fashion, without anyprior knowledge about the language pair,relying on a symmetrization process andthe one-to-one constraint.
We report our re-sults for Italian-English and Dutch-Englishlanguage pairs that outperform the currentstate-of-the-art results by a significant mar-gin.
In addition, we show how to use the al-gorithm for the construction of high-qualityinitial seed lexicons of translations.1 IntroductionBilingual lexicons serve as an invaluable resourceof knowledge in various natural language pro-cessing tasks, such as dictionary-based cross-language information retrieval (Carbonell et al1997; Levow et al 2005) and statistical machinetranslation (SMT) (Och and Ney, 2003).
In or-der to construct high quality bilingual lexicons fordifferent domains, one usually needs to possessparallel corpora or build such lexicons by hand.Compiling such lexicons manually is often an ex-pensive and time-consuming task, whereas themethods for mining the lexicons from parallel cor-pora are not applicable for language pairs and do-mains where such corpora is unavailable or miss-ing.
Therefore the focus of researchers turned tocomparable corpora, which consist of documentswith partially overlapping content, usually avail-able in abundance.
Thus, it is much easier to builda high-volume comparable corpus.
A representa-tive example of such a comparable text collectionis Wikipedia, where one may observe articles dis-cussing the similar topic, but strongly varying instyle, length and vocabulary, while still sharing acertain amount of main concepts (or topics).Over the years, several approaches for min-ing translations from non-parallel corpora haveemerged (Rapp, 1995; Fung and Yee, 1998; Rapp,1999; Diab and Finch, 2000; De?jean et al 2002;Chiao and Zweigenbaum, 2002; Gaussier et al2004; Fung and Cheung, 2004; Morin et al 2007;Haghighi et al 2008; Shezaf and Rappoport,2010; Laroche and Langlais, 2010), all sharingthe same Firthian assumption, often called thedistributionial hypothesis (Harris, 1954), whichstates that words with a similar meaning are likelyto appear in similar contexts across languages.All these methods have examined different rep-resentations of word contexts and different meth-ods for matching words across languages, but theyall have in common a need for a seed lexicon oftranslations to efficiently bridge the gap betweenlanguages.
That seed lexicon is usually crawledfrom the Web or obtained from parallel corpora.Recently, Li et al(2011) have proposed an ap-proach that improves precision of the existingmethods for bilingual lexicon extraction, basedon improving the comparability of the corpus un-der consideration, prior to extracting actual bilin-gual lexicons.
Other methods such as (Koehn andKnight, 2002) try to design a bootstrapping algo-rithm based on an initial seed lexicon of transla-tions and various lexical evidences.
However, thequality of their initial seed lexicon is disputable,449since the construction of their lexicon is language-pair biased and cannot be completely employedon distant languages.
It solely relies on unsatis-factory language-pair independent cross-languageclues such as words shared across languages.Recent work from Vulic?
et al2011) utilizedthe distributional hypothesis in a different direc-tion.
It attempts to abrogate the need of a seed lex-icon as a prerequisite for bilingual lexicon extrac-tion.
They train a cross-language topic model ondocument-aligned comparable corpora and intro-duce different methods for identifying word trans-lations across languages, underpinned by per-topic word distributions from the trained topicmodel.
Due to the fact that they deal with compa-rable Wikipedia data, their translation model con-tains a lot of noise, and some words are poorlytranslated simply because there are not enoughoccurrences in the corpus.
The goal of this work isto design an algorithm which will learn to harvestonly the most probable translations from the per-word topic distributions.
The translations learnedby the algorithm then might serve as a highly ac-curate, precision-based initial seed lexicon, whichcan then be used as a tool for translating sourceword vectors into the target language.
The key ad-vantage of such a lexicon lies in the fact that thereis no language-pair dependent prior knowledgeinvolved in its construction (e.g., orthographicfeatures).
Hence, it is completely applicable toany language pair for which there exist sufficientcomparable data for training of the topic model.Since comparable corpora often construct avery noisy environment, it is of the utmost impor-tance for a precision-oriented algorithm to learnwhen to stop the process of matching words, andwhich candidate pairs are surely not translationsof each other.
The method described in this paperfollows this intuition: while extracting a bilinguallexicon, we try to rematch words, keeping onlythe most confident candidate pairs and disregard-ing all the others.
After that step, the most con-fident candidate pairs might be used with someof the existing context-based techniques to findtranslations for the words discarded in the pre-vious step.
The algorithm is based on: (1) theassumption of symmetry, and (2) the one-to-oneconstraint.
The idea of symmetrization has beenborrowed from the symmetrization heuristics in-troduced for word alignments in SMT (Och andNey, 2003), where the intersection heuristics isemployed for a precision-oriented algorithm.
Inour setting, it basically means that we keep atranslation pair (wSi , wTj ) if and only if, after thesymmetrization process, the top translation candi-date for the source word wSi is the target word wTiand vice versa.
The one-to-one constraint aimsat matching the most confident candidates duringthe early stages of the algorithm, and then exclud-ing them from further search.
The utility of theconstraint for parallel corpora has already beenevaluated by Melamed (2000).The remainder of the paper is structured asfollows.
Section 2 gives a brief overview ofthe methods, relying on per-topic word distribu-tions, which serve as the tool for computing cross-language similarity between words.
In Section3, we motivate the main assumptions of the al-gorithm and describe the full algorithm.
Sec-tion 4 justifies the underlying assumptions ofthe algorithm by providing comparisons with acurrent-state-of-the-art system for Italian-Englishand Dutch-English language pairs.
It also con-tains another set of experiments which inves-tigates the potential of the algorithm in build-ing a language-pair unbiased seed lexicon, andcompares the lexicon with other seed lexicons.Finally, Section 5 lists conclusion and possiblepaths of future work.2 Calculating Initial Cross-LanguageWord SimilarityThis section gives a quick overview of the Cuemethod, the TI method, and their combination,described by Vulic?
et al2011), which proved tobe the most efficient and accurate for identify-ing potential word translations once the cross-language BiLDA topic model is trained and theassociated per-topic distributions are obtained forboth source and target corpora.
The BiLDAmodel we use is a natural extension of the stan-dard LDA model and, along with the definition ofper-topic word distributions, has been presentedin (Ni et al 2009; De Smet and Moens, 2009;Mimno et al 2009).
BiLDA takes advantage ofthe document alignment by using a single variablethat contains the topic distribution ?.
This vari-able is language-independent, because it is sharedby each of the paired bilingual comparable doc-uments.
Topics for each document are sampledfrom ?, from which the words are then sampledin conjugation with the vocabulary distribution ?450zSji wSji?
?zTji wTji??
?MSMTDFigure 1: Plate model for bilingual Latent Dirichlet Allocation1Figure 1: The bilingual LDA (BiLDA) model(for language S) and ?
(for language T).2.1 Cue MethodA straightforward approach to express similaritybetween words tries to emphasize the associativerelation in a natural way - modeling the proba-bility P (wT2 |wS1 ), i.e.
the probability that a tar-get word wT2 will be generated as a response to acue source word wS1 , where the link between thewords is established via the shared topic space:P (wT2 |wS1 ) =?Kk=1 P (wT2 |zk)P (zk|wS1 ), whereK denotes the number of cross-language topics.2.2 TI MethodThis approach constructs word vectors over ashared space of cross-language topics, where val-ues within vectors are the TF-ITF scores (termfrequency - inverse topic frequency), computedin a completely analogical manner as the TF-IDF scores for the original word-document space(Manning and Schu?tze, 1999).
Term frequency,given a source word wSi and a topic zk, measuresthe importance of the word wSi within the particu-lar topic zk, while inverse topical frequency (ITF)of the word wSi measures the general importanceof the source word wSi across all topics.
The fi-nal TF-ITF score for the source word wSi and thetopic zk is given by TF ?ITFi,k = TFi,k ?ITFi.The TF-ITF scores for target words associatedwith target topics are calculated in an analogicalmanner and the standard cosine similarity is thenused to find the most similar target word vectorsfor a given source word vector.2.3 Combining the MethodsTopic models have the ability to build clusters ofwords which might not always co-occur togetherin the same textual units and therefore add ex-tra information of potential relatedness.
Thesetwo methods for automatic bilingual lexicon ex-traction interpret and exploit underlying per-topicword distributions in different ways, so combin-ing the two should lead to even better results.
Thetwo methods are linearly combined, with the over-all score given by:SimTI+Cue(wS1 , wT2 ) = ?SimTI(wS1 , wT2 )+ (1?
?
)SimCue(wS1 , wT2 ) (1)Both methods posses several desirable proper-ties.
According to Griffiths et al(2007), the con-ditioning for the Cue method automatically com-promises between word frequency and semanticrelatedness since higher frequency words tend tohave higher probability across all topics, but thedistribution over topics P (zk|wS1 ) ensures that se-mantically related topics dominate the sum.
Thesimilar phenomenon is captured by the TI methodby the usage of TF, which rewards high frequencywords, and ITF, which assigns a higher impor-tance for words semantically more related to aspecific topic.
These properties are incorporatedin the combination of the methods.
As the finalresult, the combined method provides, for eachsource word, a ranked list of target words with as-sociated scores that measure the strength of cross-language similarity.
The higher the score, themore confident a translation pair is.
We will usethis observation in the next section during the al-gorithm construction.The lexicon constructed by solely applying thecombination of these methods without any addi-tional assumptions will serve as a baseline in theresults section.3 Constructing the AlgorithmThis section explains the underlying assumptionsof the algorithm: the assumption of symmetryand the one-to-one assumption.
Finally, it pro-vides the complete outline of the algorithm.3.1 Assumption of SymmetryFirst, we start with the intuition that the assump-tion of symmetry strengthens the confidence of atranslation pair.
In other words, if the most prob-able translation candidate for a source word wS1 isa target word wT2 and, vice versa, the most prob-able translation candidate of the target word wT2451is the source word wS1 , and their TI+Cue scoresare above a certain threshold, we can claim thatthe words wS1 and wT2 are a translation pair.
Thedefinition of the symmetric relation can also berelaxed.
Instead of observing only one top can-didate from the lists, we can observe top N can-didates from both sides and include them in thesearch space, and then re-rank the potential candi-dates taking into account their associated TI+Cuescores and their respective positions in the list.We will call N the search space depth.
Here isthe outline of the re-ranking method if the searchspace consists of the top N candidates on bothsides:1.
Given is a source word wSs , for which we ac-tually want to find the most probable trans-lation candidate.
Initialize an empty listFinals = {} in which target languagecandidates with their recalculated associatedscores will be stored.2.
Obtain TI+Cue scores for all target words.Keep only N best scoring target candidates:{wTs,1, .
.
.
, wTs,N} along with their respectivescores.3.
For each target candidate from{wTs,1, .
.
.
, wTs,N} acquire TI+Cue scoresover the entire source vocabulary.
Keep onlyN best scoring source language candidates.Each word wTs,i ?
{wTs,1, .
.
.
, wTs,N} nowhas a list of N source language candidatesassociated with it: {wSi,1, wSi,2 .
.
.
, wSi,N}.4.
For each target candidate word wTs,i ?
{wTs,1, .
.
.
, wTs,N}, do as follows:(a) If one of the words from the associatedlist is the given source word wSs , re-member: (1) the position m, denotinghow high in the list the word wSs wasfound, and (2) the associated TI+Cuescore SimTI+Cue(wTs,i, wSi,m = wSs ).Calculate:(i) G1,i = SimTI+Cue(wSs , wTs,i)/i(ii) G2,i = SimTI+Cue(wTs,i, wSi,m)/mFollowing that, calculate GMi, the ge-ometric mean of the values G1,i andG2,i1: GMi =?G1,i ?G2,i.
Add a tu-1Scores G1,i and G2,i are structured in such a way tobalance between positions in the ranked lists and the TI+Cuescores, since they reward candidate words which have highTI+Cue scores associated with them, and penalize words ifthey are found lower in the list of potential candidates.ple (wTs,i, GMi) to the list Finals.
(b) If we have reached the end of the listfor the target candidate word wTs,i with-out finding the given source word wSs ,and i < N , continue with the next wordwTs,i+1.
Do not add any tuple to Finalsin this step.5.
If the list Finals is not empty, sort the tuplesin the list in descending order according totheir GMi scores.
The first element of thesorted list contains a word wTs,high, the finaltranslation candidate of the source word wSs .If the list Finals is not empty, the final re-sult of this process will be the cross-languageword translation pair (wSs , wTs,high).We will call this symmetrization process thesymmetrizing re-ranking.
It attempts at push-ing the correct cross-language synonym to the topof the candidates list, taking into account boththe strength of similarities defined through theTI+Cue scores in both directions, and positionsin ranked lists.
A blatant example depicting howthis process helps boost precision is presented inFigure 2.
We can also design a thresholded variantof this procedure by imposing an extra constraint.When calculating target language candidates forthe source word wSs in Step 2, we proceed fur-ther only if the first target candidate scores abovea certain threshold P and, additionally, in Step 3,we keep lists of N source language candidatesfor only those target words for which the firstsource language candidate in their respective listscored above the same threshold P .
We will callthis procedure the thresholded symmetrizing re-ranking, and this version will be employed in thefinal algorithm.3.2 One-to-one AssumptionMelamed (2000) has already established that mostsource words in parallel corpora tend to translateto only one target word.
That tendency is modeledby the one-to-one assumption, which constrainseach source word to have at most one translationon the target side.
Melamed?s paper reports thatthis bias leads to a significant positive impact onprecision and recall of bilingual lexicon extractionfrom parallel corpora.
This assumption shouldalso be reasonable for many types of comparablecorpora such as Wikipedia or news corpora, whichare topically aligned or cover similar themes.
We452abdij monasterymonkabbey kloostermonnikbenedictijnkloostermonnikabdijabdijmonnikklooster0.22370.15860.1155 0.30490.17400.13380.22660.14940.11310.25490.14960.1288Figure 2: An example where the assumption of symmetry and the one-to-one assumption clearly help boostprecision.
If we keep top Nc = 3 candidates from both sides, the algorithm is able to detect that the correctDutch-English translation pair is (abdij, abbey).
The TI+Cue method without any assumptions would result withan indirect association (abdij, monastery).
If only the one-to-one assumption was present, the algorithm wouldgreedily learn the correct direct association (monastery, klooster), remove those words from their respectivevocabularies and then again result with another indirect association (abdij, monk).
By additionally employingthe assumption of symmetry with the re-ranking method from Subsection 3.1, the algorithm correctly learnsthe translation pair (abdij, abbey).
Correct translation pairs (klooster, monastery) and (monnik, monk) are alsoobtained.
Again here, the pair (monnik, monk) would not be obtained without the one-to-one assumption.will prove that the assumption leads to better pre-cision scores even for bilingual lexicon extractionfrom such comparable data.
The intuition be-hind introducing this constraint is fairly simple.Without the assumption, the similarity scores be-tween source and target words are calculated in-dependently of each other.
We will illustrate theproblem arising from the independence assump-tion with an example.Suppose we have an Italian word arcipelago,and we would like to detect its correct Englishtranslation (archipelago).
However, after theTI+Cue method is employed, and even after thesymmetrizing re-ranking process from the previ-ous step is used, we still acquire a wrong transla-tion candidate pair (arcipelago, island).
Why isthat so?
The word (arcipelago) (or its translation)and the acquired translation (island) are semanti-cally very close, and therefore have similar distri-butions over cross-language topics, but island is amuch more frequent term.
The TI+Cue methodconcludes that two words are potential trans-lations whenever their distributions over cross-language topics are much more similar than ex-pected by chance.
Moreover, it gives a preferenceto more frequent candidates, so it will eventuallyend up learning an indirect association2 betweenwords arcipelago and island.
The one-to-one as-sumption should mitigate the problem of such in-direct associations if we design our algorithm insuch a way that it learns the most confident directassociations2 first:2A direct association, as defined in (Melamed, 2000), isan association between two words (in this setting found bythe TI+Cue method) where the two words are indeed mutualtranslations.
Otherwise, it is an indirect association.4531.
Learn the correct direct association pair(isola, island).2.
Remove the words isola and island fromtheir respective vocabularies.3.
Since island is not in the vocabulary, theindirect association between arcipelago andisland is not present any more.
The algo-rithm learns the correct direct association(arcipelago, archipelago).3.3 The Algorithm3.3.1 One-Vocabulary-PassFirst, we will provide a version of the algorithmwith a fixed threshold P which completes onlyone pass through the source vocabulary.
Let V Sdenote a given source vocabulary, and let V T de-note a given target vocabulary.
We need to defineseveral parameters of the algorithm.
Let N0 bethe initial maximum search space depth for thethresholded symmetrizing re-ranking procedure.In Figure 2, the current depth Nc is 3, while themaximum depth might be set to a value higherthan 3.
The algorithm with the fixed threshold Pproceeds as follows:1.
Initialize the maximum search space depthNM = N0.
Initialize an empty lexicon L.2.
For each source word wSs ?
V S do:(a) Set the current search space depthNc =1.3(b) Perform the thresholded symmetrizingre-ranking procedure with the currentsearch space set toNc and the thresholdP .
If a translation pair (wSs , wTs,high) isfound, go to the Sub-step 2(d).
(c) If a translation pair is not found, andNc < NM , increment the currentsearch space Nc = Nc+1 and return tothe previous Sub-step 2(b).
If a trans-lation pair is not found and Nc = NM ,return to Step 2 and proceed with thenext word.
(d) For the found translation pair(wSs , wTs,high), remove words wSsand wTs,high from their respective3The intuition here is simple ?
we are trying to detecta direct association as high as possible in the list.
In otherwords, if the first translation candidate for the source wordisola is the target word island, and, vice versa, the firsttranslation candidate for the target word island is isola, wedo not need to expand our search depth, because these twowords are the most likely translations.vocabularies: V S = V S ?
{wSs } andV T = V T ?
{wTs,high} to satisfy theone-to-one constraint.
Add the pair(wSs , wTs,high) to the lexicon L.We will name this procedure the one-vocabulary-pass and employ it later in an iter-ative algorithm with a varying threshold and avarying maximum search space depth.3.3.2 The Final AlgorithmLet us now define P0 as the initial threshold, letPf be the threshold at which we stop decreas-ing the value for threshold and start expandingour maximum search space depth for the thresh-olded symmetrizing re-ranking, and let decp be avalue for which we decrease the current thresholdin each step.
Finally, let Nf be the limit for themaximum search space depth, andNM denote thecurrent maximum search space depth.
The finalalgorithm is given by:1.
Initialize the maximum search space depthNM = N0 and the starting threshold P =P0.
Initialize an empty lexicon Lfinal.2.
Check the stopping criterion: If NM > Nf ,go to Step 5, otherwise continue with Step 3.3.
Perform the one-vocabulary-pass with thecurrent values of P and NM .
Whenever atranslation pair is found, it is added to thelexicon Lfinal.
Additionally, we can alsosave the threshold and the depth at which thatpair was found.4.
Decrease P : P = P ?
decp, and checkif P < Pf .
If still not P < Pf , go toStep 3 and perform the one-vocabulary-passagain.
Otherwise, if P < Pf and there arestill unmatched words in the source vocab-ulary, reset P : P = P0, increment NM :NM = NM + 1 and go to Step 2.5.
Return Lfinal as the final output of the algo-rithm.The parameters of the algorithm model its be-havior.
Typically, we would like to setP0 to a highvalue, and N0 to a low value, which makes ourconstraints strict and narrows our search space,and consequently, extracts less translation pairsin the first steps of the algorithm, but the setof those translation pairs should be highly accu-rate.
Once it is not possible to extract any morepairs with such strict constraints, the algorithm re-454laxes them by lowering the threshold and expand-ing the search space by incrementing the max-imum search space depth.
The algorithm mayleave some of the source words unmatched, whichis also dependent on the parameters of the algo-rithm, but, due to the one-to-one assumption, thatscenario also occurs whenever a target vocabularycontains more words than a source vocabulary.The number of operations of the algorithm alsodepends on the parameters, but it mostly dependson the sizes of the given vocabularies.
The com-plexity isO(|V S ||V T |), but the algorithm is com-putationally feasible even for large vocabularies.4 Results and Discussion4.1 Training CollectionsThe data used for training of the models is col-lected from various sources and varies strongly intheme, style, length and its comparableness.
Inorder to reduce data sparsity, we keep only lem-matized non-proper noun forms.For Italian-English language pair, we use18, 898 Wikipedia article pairs to train BiLDA,covering different themes with different scopesand subtopics being addressed.
Document align-ment is established via interlingual links from theWikipedia metadata.
Our vocabularies consist of7, 160 Italian nouns and 9, 116 English nouns.For Dutch-English language pair, we use 7, 602Wikipedia article pairs, and 6, 206 Europarl doc-ument pairs, and combine them for training.4 Ourfinal vocabularies consist of 15, 284 Dutch nounsand 12, 715 English nouns.Unlike, for instance, Wikipedia articles, wheredocument alignment is established via interlin-gual links, in some cases it is necessary to performdocument alignment as the initial step.
Since ourwork focuses on Wikipedia data, we will not getinto detail with algorithms for document align-ment.
An IR-based method for document align-ment is given in (Utiyama and Isahara, 2003;Munteanu and Marcu, 2005), and a feature-basedmethod can be found in (Vu et al 2009).4.2 Experimental SetupAll our experiments rely on BiLDA trainingwith comparable data.
Corpora and software for4In case of Europarl, we use only the evidence of docu-ment alignment during the training and do not benefit fromthe parallelness of the sentences in the corpus.BiLDA training are obtained from Vulic?
et al(2011).
We train the BiLDA model with 2000topics using Gibbs sampling, since that numberof topics displays the best performance in theirpaper.
The linear interpolation parameter for thecombined TI+Cue method is set to ?
= 0.1.The parameters of the algorithm, adjusted on aset of 500 randomly sampled Italian words, are setto the following values in all experiments, exceptwhere noted different: P0 = 0.20, Pf = 0.00,decp = 0.01, N0 = 3, and Nf = 10.The initial ground truth for our source vocab-ularies has been constructed by the freely avail-able Google Translate tool.
The final ground truthfor our test sets has been established after wehave manually revised the list of pairs obtained byGoogle Translate, deleting incorrect entries andadding additional correct entries.
All translationcandidates are evaluated against this benchmarklexicon.4.3 Experiment I: Do Our Assumptions HelpLexicon Extraction?With this set of experiments, we wanted to testwhether both the assumption of symmetry andthe one-to-one assumption are useful in improv-ing precision of the initial TI+Cue lexicon extrac-tion method.
We compare three different lexiconextraction algorithms: (1) the basic TI+Cue ex-traction algorithm (LALG-BASIC) which servesas the baseline algorithm5, (2) the algorithm fromSection 3, but without the one-to-one assump-tion (LALG-SYM), meaning that if we find atranslation pair, we still keep words from thetranslation pair in their respective vocabularies,and (3) the complete algorithm from Section 3(LALG-ALL).
In order to evaluate these lexiconextraction algorithms for both Italian-English andDutch-English, we have constructed a test set of650 Italian nouns, and a test set of 1000 Dutchnouns of high and medium frequency.
Precisionscores for both language pairs and for all lexiconextraction algorithms are provided in Table 1.Based on these results, it is clearly visible thatboth assumptions our algorithm makes are valid5We have also tested whether LALG-BASIC outperformsa method modeling direct co-occurrence, that uses cosineto detect similarity between word vectors consisting of TF-IDF scores in the shared document space (Cimiano et al2009).
Precision using that method is significantly lower,e.g.
0.5538 vs. 0.6708 of LALG-BASIC for Italian-English.455LEX Algorithm Italian-English Dutch-EnglishLALG-BASIC 0.6708 0.6560LALG-SYM 0.6862 0.6780LALG-ALL 0.7215 0.7170Table 1: Precision scores on our test sets for the 3 dif-ferent lexicon extraction algorithms.and contribute to better overall scores.
Thereforein all further experiments we will use the LALG-ALL extraction algorithm.4.4 Experiment II: How Does ThresholdingAffect Precision?The next set of experiments aims at exploring howprecision scores change while we gradually de-crease threshold values.
The main goal of theseexperiments is to detect when to stop with the ex-traction of translation candidates in order to pre-serve a lexicon of only highly accurate transla-tions.
We have fixed the maximum search spacedepth N0 = Nf = 3.
We used the same test setsfrom Experiment I.
Figure 3 displays the changeof precision in relation to different threshold val-ues, where we start harvesting translations fromthe threshold P0 = 0.2 down to Pf = 0.0.
Sinceour goal is to extract as many correct translationpairs as possible, but without decreasing the pre-cision scores, we have also examined what impactthis gradual decrease of threshold also has on thenumber of extracted translations.
We have optedfor the F?
measure (van Rijsbergen, 1979):F?
= (1 + ?2)Precision ?Recall?2 ?
Precision+Recall (2)Since our task is precision-oriented, we have set?
= 0.5.
F0.5 measure values precision as twiceas important as recall.
The F0.5 scores are alsoprovided in Figure 3.4.5 Experiment III: Building a Seed LexiconFinally, we wanted to test how many accuratetranslation pairs our best scoring LALG-ALL al-gorithm is able to acquire from the entire sourcevocabulary, with very high precision still remain-ing paramount.
The obtained highly-precise seedlexicon then might be employed for an additionalbootstrapping procedure similar to (Koehn andKnight, 2002; Fung and Cheung, 2004) or sim-ply for translating context vectors as in (Gaussieret al 2004).0.650.70.750.80.850.90.951Precision/F-score00.050.10.150.2ThresholdIT-EN PrecisionIT-EN F-scoreNL-EN PrecisionNL-EN F-scorePrecision/F-scorePrecision/F-scorePrecision/F-scoreFigure 3: Precision and F0.5 scores in relation tothreshold values.
We can observe that the algorithmretrieves only highly accurate translations for both lan-guage pairs while the threshold goes down from value0.2 to 0.1, while precision starts to drop significantlyafter the threshold of 0.1.
F0.5 scores also reach theirpeaks within that threshold region.If we do not know anything about a given lan-guage pair, we can only use words shared acrosslanguages as lexical clues for the construction ofa seed lexicon.
It often leads to a low precisionlexicon, since many false friends are detected.For Italian-English, we have found 431 nounsshared between the two languages, of which 350were correct translations, leading to a precisionof 0.8121.
As an illustration, if we take thefirst 431 translation pairs retrieved by LALG-ALL, there are 427 correct translation pairs, lead-ing to a precision of 0.9907.
Some pairs donot share any orthographic similarities: (uccello,bird), (tastiera, keyboard), (salute, health), (terre-moto, earthquake) etc.Following Koehn and Knight (2002), we havealso employed simple transformation rules for theadoption of words from one language to another.The rules specific to the Italian-English transla-tion process that have been employed are: (R1) ifan Italian noun ends in?ione, but not in?zione,strip the final e to obtain the corresponding En-glish noun.
Otherwise, strip the suffix ?zione,and append ?tion; (R2) if a noun ends in ?ia,but not in ?zia or ?fia, replace the suffix ?iawith ?y.
If a noun ends in ?zia, replace the suf-fix with ?cy and if a noun ends in ?fia, replace456Italian-English Dutch-EnglishLexicon # Correct Precision F0.5 # Correct Precision F0.5LEX-1 350 0.8121 0.1876 898 0.8618 0.2308LEX-2 766 0.8938 0.3473 1376 0.9011 0.3216LEX-LALG 782 0.8958 0.3524 1106 0.9559 0.2778LEX-1+LEX-LALG 1070 0.8785 0.4290 1860 0.9082 0.3961LEX-R+LEX-LALG 1141 0.9239 0.4548 1507 0.9642 0.3500LEX-2+LEX-LALG 1429 0.8926 0.5102 2261 0.9217 0.4505Table 2: A comparison of different lexicons.
For lexicons employing our LALG-ALL algorithm, only translationcandidates that scored above the threshold P = 0.11 have been kept.it with ?phy.
Similar rules have been introducedfor Dutch-English: the suffix ?tie is replaced by?tion, ?sie by ?sion, and ?teit by ?ty.Finally, we have compared the results of thefollowing constructed lexicons:?
A lexicon containing only words sharedacross languages (LEX-1).?
A lexicon containing shared words and trans-lation pairs found by applying the language-specific transformation rules (LEX-2).?
A lexicon containing only translation pairsobtained by the LALG-ALL algorithm thatscore above a certain threshold P (LEX-LALG).?
A combination of the lexicons LEX-1 andLEX-LALG (LEX-1+LEX-LALG).
Non-matching duplicates are resolved by takingthe translation pair from LEX-LALG as thecorrect one.
Note that this lexicon is com-pletely language-pair independent.?
A lexicon combining only translation pairsfound by applying the language-specifictransformation rules and LEX-LALG (LEX-R+LEX-LALG).?
A combination of the lexicons LEX-2 andLEX-LALG, where non-matching dupli-cates are resolved by taking the translationpair from LEX-LALG if it is present inLEX-1, and from LEX-2 otherwise (LEX-2+LEX-LALG).According to the results from Table 2, we canconclude that adding translation pairs extractedby our LALG-ALL algorithm has a major posi-tive impact on both precision and coverage.
Ob-taining results for two different language pairsproves that the approach is generic and appli-cable to any other language pairs.
The previ-ous approach relying on work from Koehn andKnight (2002) has been outperformed in terms ofprecision and coverage.
Additionally, we haveshown that adding simple translation rules for lan-guages sharing same roots might lead to even bet-ter scores (LEX-2+LEX-LALG).
However, it isnot always possible to rely on such knowledge,and the usefulness of the designed LALG-ALLalgorithm really comes to the fore when the algo-rithm is applied on distant language pairs whichdo not share many words and cognates, and wordtranslation rules cannot be easily established.
Insuch cases, without any prior knowledge about thelanguages involved in a translation process, one isleft with the linguistically unbiased LEX-1+LEX-LALG lexicon, which also displays a promisingperformance.5 Conclusions and Future WorkWe have designed an algorithm that focuses on ac-quiring and keeping only highly confident trans-lation candidates from multilingual comparablecorpora.
By employing the algorithm we haveimproved precision scores of the methods rely-ing on per-topic word distributions from a cross-language topic model.
We have shown that the al-gorithm is able to produce a highly reliable bilin-gual seed lexicon even when all other lexical cluesare absent, thus making our algorithm suitableeven for unrelated language pairs.
In future work,we plan to further improve the algorithm and useit as a source of translational evidence for differ-ent alignment tasks in the setting of non-parallelcorpora.AcknowledgmentsThe research has been carried out in the frame-work of the TermWise Knowledge Platform (IOF-KP/09/001) funded by the Industrial ResearchFund K.U.
Leuven, Belgium.457ReferencesJaime G. Carbonell, Jaime G. Yang, Robert E. Fred-erking, Ralf D. Brown, Yibing Geng, Danny Lee,Yiming Frederking, Robert E, Ralf D. Geng, andYiming Yang.
1997.
Translingual information re-trieval: A comparative evaluation.
In Proceedingsof the 15th International Joint Conference on Arti-ficial Intelligence, pages 708?714.Yun-Chuang Chiao and Pierre Zweigenbaum.
2002.Looking for candidate translational equivalents inspecialized, comparable corpora.
In Proceedingsof the 19th International Conference on Computa-tional Linguistics, pages 1?5.Philipp Cimiano, Antje Schultz, Sergej Sizov, PhilippSorg, and Steffen Staab.
2009.
Explicit versuslatent concept models for cross-language informa-tion retrieval.
In Proceedings of the 21st Inter-national Joint Conference on Artifical Intelligence,pages 1513?1518.Wim De Smet and Marie-Francine Moens.
2009.Cross-language linking of news stories on the Webusing interlingual topic modeling.
In Proceedingsof the CIKM 2009 Workshop on Social Web Searchand Mining, pages 57?64.Herve?
De?jean, E?ric Gaussier, and Fatia Sadat.
2002.An approach based on multilingual thesauri andmodel combination for bilingual lexicon extraction.In Proceedings of the 19th International Conferenceon Computational Linguistics, pages 1?7.Mona T. Diab and Steve Finch.
2000.
A statis-tical translation model using comparable corpora.In Proceedings of the 6th Triennial Conference onRecherche d?Information Assiste?e par Ordinateur(RIAO), pages 1500?1508.Pascale Fung and Percy Cheung.
2004.
Mining very-non-parallel corpora: Parallel sentence and lexiconextraction via bootstrapping and EM.
In Proceed-ings of the Conference on Empirical Methods inNatural Language Processing, pages 57?63.Pascale Fung and Lo Yuen Yee.
1998.
An IR ap-proach for translating new words from nonparallel,comparable texts.
In Proceedings of the 17th Inter-national Conference on Computational Linguistics,pages 414?420.Eric Gaussier, Jean-Michel Renders, Irina Matveeva,Cyril Goutte, and Herve?
De?jean.
2004.
A geomet-ric view on bilingual lexicon extraction from com-parable corpora.
In Proceedings of the 42nd AnnualMeeting of the Association for Computational Lin-guistics, pages 526?533.Thomas L. Griffiths, Mark Steyvers, and Joshua B.Tenenbaum.
2007.
Topics in semantic represen-tation.
Psychological Review, 114(2):211?244.Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,and Dan Klein.
2008.
Learning bilingual lexiconsfrom monolingual corpora.
In Proceedings of the46th Annual Meeting of the Association for Compu-tational Linguistics, pages 771?779.Zellig S. Harris.
1954.
Distributional structure.
Word10, (23):146?162.Philipp Koehn and Kevin Knight.
2002.
Learning atranslation lexicon from monolingual corpora.
InProceedings of the ACL-02 Workshop on Unsuper-vised Lexical Acquisition, pages 9?16.Audrey Laroche and Philippe Langlais.
2010.
Re-visiting context-based projection methods for term-translation spotting in comparable corpora.
In Pro-ceedings of the 23rd International Conference onComputational Linguistics, pages 617?625.Gina-Anne Levow, Douglas W. Oard, and PhilipResnik.
2005.
Dictionary-based techniques forcross-language information retrieval.
InformationProcessing and Management, 41:523?547.Bo Li, Eric Gaussier, and Akiko Aizawa.
2011.
Clus-tering comparable corpora for bilingual lexicon ex-traction.
In Proceedings of the 49th Annual Meetingof the Association for Computational Linguistics:Human Language Technologies, pages 473?478.Christopher D. Manning and Hinrich Schu?tze.
1999.Foundations of Statistical Natural Language Pro-cessing.
MIT Press, Cambridge, MA, USA.I.
Dan Melamed.
2000.
Models of translational equiv-alence among words.
Computational Linguistics,26:221?249.David Mimno, Hanna M. Wallach, Jason Naradowsky,David A. Smith, and Andrew McCallum.
2009.Polylingual topic models.
In Proceedings of the2009 Conference on Empirical Methods in NaturalLanguage Processing, pages 880?889.Emmanuel Morin, Be?atrice Daille, Koichi Takeuchi,and Kyo Kageura.
2007.
Bilingual terminologymining - using brain, not brawn comparable cor-pora.
In Proceedings of the 45th Annual Meetingof the Association for Computational Linguistics,pages 664?671.Dragos Stefan Munteanu and Daniel Marcu.
2005.Improving machine translation performance by ex-ploiting non-parallel corpora.
Computational Lin-guistics, 31:477?504.Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and ZhengChen.
2009.
Mining multilingual topics fromWikipedia.
In Proceedings of the 18th InternationalWorld Wide Web Conference, pages 1155?1156.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Reinhard Rapp.
1995.
Identifying word translations innon-parallel texts.
In Proceedings of the 33rd An-nual Meeting of the Association for ComputationalLinguistics, pages 320?322.Reinhard Rapp.
1999.
Automatic identification ofword translations from unrelated English and Ger-man corpora.
In Proceedings of the 37th Annual458Meeting of the Association for Computational Lin-guistics, pages 519?526.Daphna Shezaf and Ari Rappoport.
2010.
Bilinguallexicon generation using non-aligned signatures.
InProceedings of the 48th Annual Meeting of the As-sociation for Computational Linguistics, pages 98?107.Masao Utiyama and Hitoshi Isahara.
2003.
Reliablemeasures for aligning Japanese-English news arti-cles and sentences.
In Proceedings of the 41st An-nual Meeting of the Association for ComputationalLinguistics, pages 72?79.C.
J. van Rijsbergen.
1979.
Information Retrieval.Butterworth.Thuy Vu, Ai Ti Aw, and Min Zhang.
2009.
Feature-based method for document alignment in compara-ble news corpora.
In Proceedings of the 12th Con-ference of the European Chapter of the Associationfor Computational Linguistics, pages 843?851.Ivan Vulic?, Wim De Smet, and Marie-Francine Moens.2011.
Identifying word translations from compara-ble corpora using latent topic models.
In Proceed-ings of the 49th Annual Meeting of the Associationfor Computational Linguistics: Human LanguageTechnologies, pages 479?484.459
