Chinese Chunking with another Type of SpecHongqiao LiBeijing Institute ofTechnologyBeijing 100081 Chinalhqtxm@bit.edu.cnChang-Ning HuangMicrosoft Research AsiaBeijing 100080 Chinacnhuang@msrchina.research.microsoft.comJianfeng GaoMicrosoft Research AsiaBeijing 100080 Chinajfgao@microsoft.comXiaozhong FanBeijing Institute ofTechnologyBeijing 100081 Chinafxz@bit.edu.cnAbstractSpec is a critical issue for automatic chunking.This paper proposes a solution of Chinesechunking with another type of spec, which isnot derived from a complete syntactic tree butonly based on the un-bracketed, POS taggedcorpus.
With this spec, a chunked data is builtand HMM is used to build the chunker.
TBL-based error correction is used to furtherimprove chunking performance.
The averagechunk length is about 1.38 tokens, F measureof chunking achieves 91.13%, labelingaccuracy alone achieves 99.80% and the ratioof crossing brackets is 2.87%.
We also findthat the hardest point of Chinese chunking isto identify the chunking boundary insidenoun-noun sequences1.1 IntroductionAbney (1991) has proposed chunking as a usefuland relative tractable median stage that is to dividesentences into non-overlapping segments onlybased on superficial analysis and local information.
(Ramshaw and Marcus, 1995) represent chunkingas tagging problem and the CoNLL2000 sharedtask (Kim Sang and Buchholz, 2000) is now thestandard evaluation task for chunking English.Their work has inspired many others to studychunking for other human languages.Besides the chunking algorithm, spec (thedetailed definitions of all chunk types) is anothercritical issue for automatic chunking development.The well-defined spec can induce the chunker toperform well.
Currently chunking specs aredefined as some rules or one program to extractphrases from Treebank such as (Li, 2003) and (Li,2004) in order to save the cost of manualannotation.
We name it as Treebank-derived spec.However, we find that it is more valuable tocompile another type of chunking spec accordingto the observation from un-bracketed corpusinstead of Treebank.1This work was done while Hongqiao Li was visitingMicrosoft Research Asia.Based on the problems of chunking Chinese thatare found with our observation, we explain thereason why another type of spec is needed and thenpropose our spec in which the shortening andextending strategies are used to resolve theseproblems.
We also compare our spec with aTreebank-derived spec which is derived fromChinese Treebank (CTB) (Xue and Xia, 2000).
Anannotated chunking corpus is built with the specand then a chunker is also constructed accordingly.For annotation, we adopt a two-stage processing,in which text is first chunked manually and thenthe potential inconsistent annotations are checkedsemi-automatically with a tool.
For the chunker,we use HMM model and TBL (Transform-basedLearning) (Brill, 1995) based error correction tofurther improve chunking performance.
With ourspec the overall average length of chunks arrives1.38 tokens, in open test, the chunking F measureachieves 91.13% and 95.45% if under-combiningerrors are not counted.
We also find the hardestpoint of Chinese chunking is to identify thechunking boundary inside a noun-noun sequence.In the remainder of this paper section 2 describessome problems in chunking Chinese text, section 3discusses the reason why another type of spec isneeded and proposes our chunking spec, section 4discusses the annotation of our chunking corpus,section 5 describes chunking model, section 6gives experiment results, section 7, 8 recall somerelated work and give our conclusions respectively.2 Problems of Chunking Chinese TextThe purpose of Chinese chunking is to dividesentence into syntactically correlated parts ofwords after word segmentation and part-of-speech(POS) tagging.
For example:[NP ??
/ns ?Zhuhai?]
?
/u ?of?
[NP ??
/a?solid?
?
?/n ?traffic?
?
?/n ?frame?]
[VP ?/d?already?
?
??
?
/v ?achieve considerablescale ?
?/u]   ?Zhuhai has achieved considerablescale in solid traffic frame.
?According to Abney?s definition, most chunksare modifier-head structures and non-overlapping.However, some syntactic structures in Chinese arevery hard to be chunked correctly due tocharacteristics of Chinese language, for example,less using of function words and less inflectionformats.
Table 1 shows the most commonstructural ambiguities occurred during Chinesechunking.
Their occurrences and distributions ofeach possible structure are also reported.
As can beseen in Table 1, only 77% neighboring nouns canbe grouped inside one chunk; if the left word is ??/of?
or a verb, this figure will ascend to 80% and94% respectively; but if the left word is anadjective or a numeral, it will descend to 70% and59% respectively; for ?n_c_n?, only 52%  are wordlevel coordination.
In contrast with Englishchunking, several hard problems are described indetail as following.
(1) Noun-noun compoundsCompounds formed by more than twoneighboring nouns are very common in Chineseand not always all the left nouns modify the headof the compound.
Some compounds consist ofseveral shorter sub-compounds.
For example:( ?
?
/younger ???
/volunteer ?
?/science and technology ??
?/service team)?young volunteer service team of science andtechnology????
????
and ???
????
are two sub-compounds and the former modifies the latter.But sometimes it is impossible to distinguish theinner structures, for example:?
?/world ?
?/peace ?
?/careerIt is impossible to distinguish whether it is {{??
??}
??}
or {??
{??
??
}}.English chunking also shows such problem, andthe common solution for English is not to identifytheir inner structure and treat them as a flat nounphrase.
Following is an example in CoNLL2000shared task:[NP employee assistance program directors](2) CoordinationCoordination in all cases can be divided into twotypes: with conjunctions and without conjunctions.The former can be further divided into twosubcategories: word-level and phrase-levelcoordinations.
For example:{ ?
??
/policy ??
/bank ?
/and ?
?/commercial ?
?
/bank} ?
/of { ?
?/relationship ?/and  ?
?/cooperation}    ?therelationship and cooperation between policybanks and commercial banks?.The former coordination is phrase-level and thelatter is word-level.
Unfortunately, sometimes it isdifficult or even impossible to distinguish whetherit is word-level or phrase-level at all, for example:??
/least ??
/salary ?
/and ???
/livingmaintenance ?the least salary and livingmaintenance?It is impossible to distinguish ????
is a sharedmodifier or not.
English chunking also has suchkind of problems.
The solution of CoNLL2000 isto leave the conjunctions outside chunks forphrase-level coordinations and to group theconjunction inside a chunk when it is word-level orimpossibly distinguished phrase-level.
Forexample:[NP enough food and water]In Chinese, some coordinate construction has noconjunction or punctuation inside, and also couldnot be distinguished from a modifier-headconstruction with syntactic knowledge only.
Forexample:?
?/order (?
?/police wagon ?
?/cautionlight ??
?/alarm whistle)   ?Order the policePattern1 No.2 Distributions Examplesn_n 95177% (modifier head)7% (coordination)16% (others)(?
?/society ?
?/phenomenon) ?social phenomena?(?
?/language ?
?/wordage) ?language and wordage?(?
?/capital ?
?/art ?
?/stage) ?
the stage of capital art?v_n_n 154 6% (v_n modify the last noun) 94 % (others)?/enter?/factory??/worker?
?/avoid  ?
?/law ?
?/duty ?avoid legal duties?
?_n_n 98 80% ( n_n is modifier_head) 20% (others)?
?/watch ?/of?
?/traffic ?
?/cop ?a orderly traffic cop???/paralytic?/of??/body?
?/functiona_n_n 27 70% ( a modify the first n) 30% (others)?/high ?
?/technology ?
?/company ?high-tech company?
?/old ?
?/news ??
?/worker ?old news worker?m_n_n 17 41% ( m modify the first n) 59% (others)?/two ?/nation ?
?/people ?our two peoples??
?/some ?
?/country ?
?/area ?some rural areas?n_c_n 88 52%(word level coordination) 48%(others)?
?/economy ?/and ?
?/society ?economy and society??
?/quality  ?/and  ?
?/technology ?
?/requirement1n, v, a, d, m, q,  p, f , c are the POS tags of noun, verb, adjective, adverb, number, measure, preposition, localizer,conjunction respectively, ?_?
means neighboring, ??/of?
is a common auxiliary word in Chinese.2This statistical work is done on our test corpus whose setting is shown in Table 3.Table 1: The observation of several common structural ambiguities during Chinese chunkingwagons, caution lights and alarm whistles?Such problem does not exist in English becausealmost all coordinations have certain conjunctionsor punctuations between words or phrases of thesame syntactic categories in formal English.
(3) Structural ambiguitiesIn Chinese, some structural ambiguities inphrase level are impossible or unnecessary to bedistinguished during chunking.
There is anexample of ?a_n_n?:??
/a ?modern?
??
/n ?industry?
??
/n?system?{??
{??
??}}
or {{??
??}
??}
areidentically acceptable.
English also has suchproblem.
The solution of CoNLL2000 is not todistinguish inner structure and group the givensequence as a single chunk.
For example, the innerstructure of ?
[NP heavy truck production]?
is ?
{{heavytruck} production}?, whereas one reading of ?
[NPheavy quake damage]?
is ?
{heavy {quake damage}}?.Besides, ?a_n_n?, ?m_n_n?
and ?m_q_n_n?
alsohave the similar problem.3 Chinese Chunking SpecAs a kind of shallow parsing, the principles ofchunking are to make chunking much moreefficient and precise than full parsing.
Obviously,one can shorten the length of chunks to leaveambiguities outside of chunks.
For example, if welet noun-noun sequences always chunk into singleword, those ambiguities listed in Table 1 would notbe encountered and the performance would begreatly improved.
In fact, there is an implicitrequirement in chunking, no matter whichlanguage it is, the average length of chunks is aslonger as possible without violating the generalprinciple of chunking.
So a trade-off between theaverage chunk length and the chunkingperformance exists.3.1 Why another type of spec is neededA convenient spec is to extract the lowest non-terminal nodes from a Treebank (e.g.
CTB) asChinese chunked data.
But there are someproblems.
The trees are designed for full parsinginstead of shallow parsing, thus some of theseproblems listed in section 2 could not be resolvedwell in chunking.
Maybe we can compile somerules to prune the tree or break some non-terminalnodes in order to properly resolve these problemsjust like CoNLL2000.
However, just as (Kim Sangand Buchholz, 2000) noted: ?some trees are verycomplex and some annotations are inconsistent?.So these rules are complex, the extracted data areinconsistent and manual check is also needed.
Inaddition, the resource of Chinese Treebank islimited and the extracted data is not enough forchunking.So we compile another type of chunking specaccording to the observation from un-bracketcorpus instead of Treebank.
The only shortcomingis the cost of annotation, but there are someadvantages for us to explore.1) It coincides with auto chunking procedure,and we can select proper solutions to theseproblems without constraints of the exist Treebank.The purpose of drafting another type of chunkingspec is to keep chunking consistency as high aspossible without hurting the performance of auto-chunking in whole.2) Through spec drafting and textannotating most frequent and significant syntacticambiguities could be studied, and thoseobservations are in turn described in the speccarefully.3) With a proper spec and certain mechanicalapproaches, a large-scale chunked data could beproduced without supporting from the Treebank.3.2 Our specOur spec and chunking annotation are based onPK corpus2 (Yu et al 1996).
The PK corpus is un-bracketed, but in which all words are segmentedand only one POS tag is assigned to each word.We define 11 chunk types that are similar withCoNLL2000.
They are NP (noun chunk), VP (verbchunk), ADJP (adjective chunk), ADVP (adverbchunk), PP (prepositional chunk), CONJP(conjunction), MP (numerical chunk), TP(temporal chunk), SP (spatial chunk), INTJP(interjection) and INDP (independent chunk).During spec drafting we try to find a properchunk spec to solve these problems by two ways:either merging neighboring chunks into one chunkor shortening them.
Besides those structuralambiguities, we also extend boundary of thechunks with minor structural ambiguities in orderto make the chunks close to the constituents.3.2.1 ShorteningThe auxiliary ??/of?
is one of the most frequentwords in Chinese and used to connect a pre-modifier with its nominal head.
However the leftboundary of such a ?
-construction is quitecomplicated: almost all kinds of preceding clauses,phrases and words can be combined with it to formsuch a pre-modifier, and even one ?-constructioncan embed into another.
So we definitely leave itoutside any chunk.
Similarly, conjunctions, ?
?/and?, ??
/or?
and ??
/and?
et al, are also leftoutside any chunk no matter they are word-level or2Can be downloaded from www.icl.pku.edu.cnphrase-level coordinations.
For instances, theexamples in Section 2 are chunked as ?
[NP ?????]
?
[NP??
??]
?
[NP ??]
?
[NP??]?
and ?
[ADJP ??]
[NP ??]
?
[NP ???
]?3.2.2 Extending(1) NPSimilar with the shared task of CoNLL2000,we define noun compound that is formed by anoun-sequence: ?a_n_n?, ?m_n_n?
or ?m_q_n_n?,as one chunk, even if there are sub-compounds,sub-phrase or coordination relations inside it.
Forinstances, ?
[NP ??
???
??
???]?,?
[NP ??
??
??
]?, ?[VP??]
[NP??
??
???
]?, ?
[NP ??
??
??]
and ?
[NP ??
??
??]?
are grouped into single chunksrespectively.However, it does not mean that we blindly bindall neighboring nouns into a flat NP.
If thoseneighboring nouns are not in one constituent orcross the phrase boundary, they will be chunkedseparately, such as following two examples inTable 1: ?[VP?]
[NP ?]
[NP??]?
and ?[ADJP??]
?/u [NP ??]
[NP ??]?.
So our solutiondoes not break the grammatical phrase structure ina given sentence.With this chunking strategy, we not onlyproperly resolved these problems, but also getlonger chunks.
Longer chunks can makesuccessive parsing easier based on chunking.
Forexample, if we chunked the sentence as:[NP ??]
?
[NP ??
??]
[NP ??]
[VP ?????
?]
?/wThere would be three possible syntactic treeswhich are difficult to be distinguished:1a) {{ [NP ??]
?
{ [NP ??
??]
[NP ??]}}
[VP ?
????
]}1b) {{{ [NP ??]
?
[NP ??
??]}
[NP ??]}
[VP ?
????
]}1c) {{ [NP ??]
?
[NP ??
??]}
{ [NP ??]
[VP ?
????
]}}Whereas with above chunking strategy of ourspec, there is only one syntactic tree remained:{{[NP ??]
?
[NP ??
??
??]}
[VP ?????
?]}
?/wAnother reason of the chunking strategy is thatfor some NLP applications such as IR, IE or QA, itis unnecessary to analyze these ambiguities at theearly stage of text analysis.
(2) PPMost PP consists of only the preposition itselfbecause the right boundary of a preposition phraseis hard to identify or far from the preposition.
Butcertain prepositional phrases in Chinese are formedwith a frame-like construction, such as [PP ?/p?at?
?
?/f ?middle?
], [PP ?/p ?
?/f ?top?
], etc.Statistics shows that more than 90% of thoseframe-like PPs are un-ambiguous, and otherscommonly have certain formal features such as anauxiliary ?
or a conjunction immediatelyfollowing the localizer.
Table 2 shows the statisticresult.
Thus with those observations, those frame-like constructions could be chunked as PP.
Thelength of such kind of PP frames is restricted to beat most two words inside in order to keep thedistribution of chunk length more even and thechunking annotation more consistent.Pattern1 No.of occurrence Ratio as a chunkp_*_f 45 93.33%P_*_*_f 36 97.22%*_f 40 92.50%*_*_f 9 77.78%1This statistical work is also done on our testcorpus and ?*?
means a wildcard for a POS tag.Table 2: The ration of grouping these patternsas a chunk without any ambiguity(3) SPMost spatial chunks consist of only thelocalizer(with POS tag ?/s?
or ?/f?).
But if thespatial phrase is in the beginning of a sentence, orthere is a punctuation (except ???)
in front of it,then the localizer and its preceding words could bechunked as a SP.
And the number of words in frontof the localizer is also restricted to at most two forthe same reason.
(4) VPCommonly, a verb chunk VP is a pre-modifierverb construction, or a head-verb with its followingverb-particles which form a morphologicallyderived word sequence.
The pre-modifier isformed by adverbial phrases and/or auxiliary verbs.In order to keep the annotation consistent thoseverb particles and auxiliary verbs could be found ina closed list respectively only.
Post-modifiers of averb such as object and complement should beexcluded in a verb chunk.We find that although a head verb groups morethan one preceding adverbial phrases, auxiliaryverbs and following verb-particles into one VP, itschunking performance is still high.
For example:[CONJP ?
?/c ?if?]
[VP ?
?/d ?lately?
?/d ?not?
?/v ?can?
?
?/v ?build?
?/v?up?]
[NP ?
?/n ?diplomat ?
?/n ?relation?
]?If we could not build up the foreign relationssoon?3.3 Spec ComparisonWe compare our spec with the Treebank-derivedspec, named as S1, which is to extract the lowestnon-terminal nodes from CTB as chunks from theaspect of the solutions of these problems in section2.
Noun-noun compound and the coordinationwhich has no conjunction are chunked identicallyin both specs.
But for others, there are different.
InS1, the conjunctions of phrase-level coordinationare outside of chunks and the ones of word-levelare inside a chunk, all adjective or numericalmodifiers are separate from noun head.
Accordingto S1, the example in 3.2.1 should be chunked asfollowing.
[ADJP ???]
[NP ??]
?
[NP??]
[NP ??]
?
[NP ??
?
??
]But these phrases that are impossible todistinguish inner structures during the early stageof text analysis are hard to be chunked and wouldcause some inconsistency.
?
[ADJP ??]
[NP ??]?
[NP ???]?
or ?
[ADJP ??]
[NP ??
?
???
]?, ?
[ADJP ??]
[NP ??]
[NP ??]?
or?
[ADJP ??]
[NP ??
??
]?, are hard to makedecisions with S1.In addition, with our spec outside words are onlypunctuations, structural auxiliary ?
?
/of?, orconjunctions, whereas with S1, outside words aredefined as all left words after lowest non-terminalextraction.4 Chunking AnnotationFour graduate students of linguistics wereassigned to annotate manually the PK corpus withthe proposed chunking spec.
Many discussionsbetween authors and those annotators wereconducted in order to define a better chunking specfor Chinese.
Through the spec drafting and textannotating most significant syntactic ambiguitiesin Chinese, such as those structural ambiguitiesdiscussed in section 2 and 3, have been studied,and those observations are carefully described inthe spec in turn.Consistency control is another important issueduring annotation.
Besides the common methods:manual checking, double annotation, postannotation checking, we explored a newconsistency measure to help us find the potentialinconsistent annotations, which is hinted by(Kenneth and Ryszard.
2000), who definedconsistency gain as a measure of a rule in learningfrom noisy data.The consistency of an annotated corpus in wholecould be divided down into consistency of eachchunk.
If the same chunks appear in the samecontext, they should be identically annotated.
Sowe define the consistency of one special chunk asthe ratio of identical annotation in the same context.corpusin  ))context( ,( of  No.
)context(in  annotation same of No.)
)context( ,cons(PPPPP=(1)?==Niii PcontextPconsN 1))(,(1cons(S)  (2)Where P represents a pattern of the chunk (POSor/and lexical sequence), context(P) represents theneeded context to annotate this chunk, N representsthe number of chunks in the whole corpus S.In order to improve the efficiency we alsodevelop a semi-automatic tool that not only checkmechanical errors but also detect those potentialinconsistent annotations.
For example, one inputs aPOS pattern: ?a_n_n?, and an expected annotationresult: ?B-NP_I-NP_E-NP3?, the tool will list allthe consistent and inconsistent sentences in theannotated text respectively.
Based on the outputone can revise those inconsistent results one by one,and finally the consistency of the chunked text willbe improved step by step.5 Chunking ModelAfter annotating the corpus, we could usevarious learning algorithms to build the chunkingmodel.
In this paper, HMM is selected because notonly its training speed is fast, but also it hascomparable performance (Xun and Huang, 2000).Automatic chunking with HMM should conductthe following two steps.
1) Identify boundaries ofeach chunk.
It is to assign each word a chunk mark,named M, which contains 5 classes: B, I, E, S (asingle word chunk) and O (outside all chunks).
2)Tag the chunk type, named X, which contains 11types defined in Section 3.So each word will be tagged with two tags: Mand X (the words excluding from any chunk onlyhave M).
So the result after chunking is a sequenceof triples (t, m, x), where t, m, x represent POS tag,chunk mark and chunk type respectively.
All thetriples of a chunk are combined as an item ni,which also could be named as a chunk rule.
Let Was the word segmentation result of a given sentence,T as POS tagging result and C (C= n1 n2?nj) as thechunking result.
The statistical chunking modelcould be described as following:),(),|(maxarg),(/),(),|(maxarg),|(maxargTCPTCWPTWPTCPTCWPTWCPCCCC===?
(3)Independent assumption is used to approximateP(W|C,T), that is:3B, E, I represent the left/right boundary of a chunkand inside a chunk respectively, B-NP means this wordis the beginning of NP.
?=?miiiii xmtwPTCWP1),,|(),|(  (4)If the triple is unseen, formula 5 is used.2,)),,((max),,(),,|(kjikjiiiiiiixmtcountxmtcountxmtwP =(5)For P(C, T), tri-grams among chunks and outsidewords are used to approximate, that is:?=??
?kiiii nnnPnnPnPTCP312121 )|()|()(),((6)Smoothing follows the method of (Gao et al,2002).In order to improve the performance we use N-fold error correction (Wu, 2004) technique toreduce the error rate and TBL is used to learn theerror correction rules based on the output of HMM.6 Data and EvaluationThe performance of chunking is commonlymeasured with three figures: precision (P), recall(R) and F measure that are defined in CoNLL2000.Besides these, we also use two other measurementsto evaluate the performance of bracketing andlabeling respectively: RCB(ratio of crossingbrackets), that is the percentage of the foundbrackets which cross the correct brackets;LA(labeling accuracy), that is the percentage of thefound chunks which have the correct labels.datain test  chunks  of  No.boundarieschunk  crossed chunks  theof No.RCB   =boundariescorrect  with chunks   theof  No.chunkscorrect  of No.LA     =The average length (ALen) of chunks for eachtype is the average number of tokens in each chunkof given type.
The overall average length is theaverage number of tokens in each chunk.
To bemore disinterested, outside tokens (includingoutside punctuations) are also concerned and eachof them is counted as one chunk.6.1 Chunking performance with our specTraining and test was done on the PK corpus.Table 3 shows the detail information.
We use theuni-gram of chunk POS rules as the baseline.Data No.
of tokensNo.
ofchunksNo.
ofoutsideALen(include O)Train 444,777 229,989 92,839 1.377Test 28,382 13,879 5,493 1.363Table 3:The information of data setTable 4 shows the chunking performance ofclose test and open test when HMM and ten foldsTBL based error correction (EC) are donerespectively.Close Test (%) Open Test (%)F RCB LA F RCB LABaseline 81.95 6.55 99.46 81.44 6.58 99.47HMM 94.79 2.62 99.78 88.39 3.18 99.65HMM+EC 95.11 2.38 99.91 91.13 2.87 99.80Table 4:The overall performance of chunkingAs can be seen, the performance of open testdoesn?t drop much.
For open test, HMM achieves6.9% F improvement, 3.4% RCB reduction onbaseline; error correction gets another 2.7% Fimprovement, 0.3% RCB reduction.
Labelingaccuracy is so high even with the baseline, whichindicates that the hard point of chunking is toidentify the boundaries of each chunk.Table 5 shows the performance of each type ofchunks respectively.
NP and VP amount toapproximately 76% of all chunks, so theirchunking performance dominates the overallperformance.
Although we extend VP and PP, theirperformances are much better than overall.
Theperformance of INDP can arrive 99% although it ismuch longer than other types.
Because its surfaceevidences are clear and complete owing to itsdefinition: the meta-data of a document, all thedescriptions inside a pair of parenthesis, and alsocertain fixed phrases which do not act as asyntactic constituent in a sentence.
From therelative lower performance of NP, but the mostpart of all chunks, we can conclude that the hardestissue of Chinese chunking is to identify boundariesof NPs.Percentage(%)ALen(tokens)P(%)R(%)F(%)NP 45.94 1.649 88.82 86.25 87.52VP 29.82 1.416 96.60 96.49 96.55PP 6.59 1.221 93.67 93.58 93.63MP 3.69 1.818 89.51 86.33 87.89ADJP 3.77 1.308 86.11 89.43 87.74SP 2.71 1.167 84.70 84.03 84.36TP 2.59 1.251 93.23 94.30 93.76CONJP 2.22 1.000 97.20 98.73 97.96INDP 1.41 4.297 99.06 99.06 99.06ADVP 1.06 1.117 85.48 85.03 85.25INTJP 0.23 1.016 68.75 95.65 80.00ALL 100 1.507 91.70 90.55 91.13Table 5:The result of each type with our specAll the chunking errors could be classified intofour types: wrong labeling, under-combining, over-combining and overlapping.
Table 6 lists thenumber and percentage of each type of errors.Under-combining errors count about a halfnumber of overall chunking errors, however it isnot a problem in certain applications because theydoes not cross the brackets, thus there are stillopportunities to combine them later with additionalknowledge.
If we evaluate the chunking resultwithout counting those under-combining errors, theF score of the proposed chunker achieves 95.45%.Error type No.of the Errors PercentageWrong labeling 22 2.56%Under-combine 418 48.71%Over-combining 339 39.51%Overlapping 59 6.88%Table 6:The distribution of chunking errorsWith comparison we also use some otherlearning methods, MBL(Bosch and Buchholz,2002), SVM(Kudoh and Matsumoto, 2001) andTBL to build the chunker.
The features for MBLand SVM are the POS of current, left two and righttwo words, lexical of current, left one and right oneword.
TiMBL 4  and SVM-light 5  are used as thetools.
For SVM, we convert the chunk marksBIOES to BI and the binary class SVM is used toclassifier the chunk boundary, then some rules areused to identify its label.
For TBL, the ruletemplates are all the possible combinations of thefeatures and the initial state is that each word is achunk.
Table 7 shows the result.
As seen, withouterror correction all these models do not performwell and our HMM gets the best performance.MBL SVM TBL HMMF(%) 85.31 86.25 86.92 88.39Table 7:Comparison with different algorithms6.2 Further applicationsThe length of chunks with our spec (AoL is 1.38)is longer than other Treebank-derived specs (AoLof S1 is 1.239) and closer to the constituents ofsentence.
Thus there are several applicationsbenefit from the fact, such as:1) The longest/full noun phrase identification.According to our statistics, due to including noun-noun compounds, ?a_n_n?
and ?m_n_n?
inside NPs,65% noun chunks are already the longest/full  nounphrases and other 22% could become the longest/full noun phrases by only one next combining step.2) The predicate-verb identification.By extending the average length of VPs, the mainverb (or predicate-verb, also called tensed verb inEnglish) of a given sentence could be identifiedbased on certain surface evidences with a relativelyhigh accuracy.
With certain definition our statisticsbased on our test set show that 84.88% of thosemain verbs are located in the first longest VPsamong all VPs in a sentence.4http://ilk.kub.nl/software.html5http://svmlight.joachims.org/7 Related WorkFor chunking spec, the CoNLL2000 shared taskdefines a program chunklink to extract chunksfrom English Treebank.
(Li, 2003) defines thesimilar Treebank-derived spec for Chinese and shereports manual check is also needed to make dataconsistent.
Part of the Sparkle project hasconcentrates on a spec based on un-bracketedcorpus of English, Italian, French andGerman(Carroll et al, 1997).
(Zhou, 2002) definesbase phrase which is similar as chunk for Chinese,but his annotation and experiment are on his owncorpus.For chunking algorithm, many machine learning(ML) methods have been applied and gotpromising results after chunking is represented astagging problem, such as: SVM (Kudoh andMatsumoto, 2001), Memory-based (Bosch andBuchholz, 2002), SNoW (Li and Roth), et al.Some rule-base chunking (Kinyon, 2003) andcombining rules with learning (Park and Zhang,2003) are also reported.For annotation, (Brants, 2000) reports the inter-annotator agreement of part-of-speech annotationsis 98.57%, the one of structural annotations is92.43% and some consistency measures.
(Xue etal., 2002) also address some issues related tobuilding a large-scale Chinese corpus.8 ConclusionWe propose a solution of Chinese chunking withanother type of spec that is based on un-bracketedcorpus rather than derived from a Treebank.Through spec drafting and annotating, mostsignificant syntactic ambiguous patterns have beenstudied, and those observations in turn have beendescribed in the spec carefully.
The proposedmethod of defining a chunking spec helps us find aproper solution for the hard problems of chunkingChinese.
The experiments show that with our spec,the overall Chinese chunking F-measure achieves91.13% and 95.45% if under-combining errors arenot counted.9 AcknowledgementsWe would like to thank the members of theNatural Language Computing Group at MicrosoftResearch Asia.
Especial acknowledgement is to theanonymous reviewers for their insightfulcomments and suggestions.
Based on that we haverevised the paper accordingly.ReferencesS.
Abney.
1991.
Parsing by chunks.
In Principle-Based Parsing.
Kluwer Academic Publishers,Dordrecht: 257?278.L.A.
Ramshaw and M.P.
Marcus.
1995.
Textchunkingusing transformation-based learning.
InProceedings of the 3rd ACL/SIGDAT Workshop,Cambridge, Massachusetts, USA: 82?94.E.
Tjong Kim Sang and S. Buchholz.
2000.Introduction to the CoNLL-2000 shared task:Chunking.
In Proceedings of CoNLL-2000 andLLL-2000, Lisbon, Portugal: 127?132.Sujian Li, Qun Liu and Zhifeng Yang.
2003.Chunking based on maximum entropy.
ChineseJournal of Computer, 25(12): 1734?1738.Heng Li, Jingbo Zhu and Tianshun Yao.
2004.SVM based Chinese text chunking.
Journal ofChinese Information Processing, 18(2): 1?7.Nianwen Xue and Fei Xia.
2000.
The BracketingGuidelines for the Penn Chinese Treebank(3.0).Technical report ,University of Pennsylvania,URL http:// www.cis.upenn.edu/~chinese/.Eric Brill.
Transformation-based error-drivenlearning and natural language processing: A casestudy in part of speech tagging.
ComputationalLinguistics,21 (4):543?565.Shiwen Yu, Huiming Duan, Xuefeng Zhu, et al2002.
The basic processing of contemporaryChinese corpus at Peking University.
Journal ofChinese Information Processing, 16(6): 58?65.K.A.
Kaufman and R.S.
Michalski.
1999.
Learningfrom inconsistent and noisy data: the AQ18approach, Proceedings of the EleventhInternational Symposium on Methodologies forIntelligent Systems, Warsaw: 411?419.Endong Xun and Changning Huang.
2000.
Aunified statistical model for the identification ofEnglish baseNP, In Proceedings of the 38th ACL:109?117.Jianfeng Gao, Joshua Goodman, Mingjing Li, Kai-Fu Lee.
Toward a unified approach to statisticallanguage modeling for Chinese.
ACMTransactions on Asian Language InformationProcessing, Vol.
1, No.
1, 2002: 3-33.Dekai WU, Grace NGAI, Marine CARPUAT.
N-fold Templated Piped Correction.
Proceedings ofthe First International Joint Conference onNatural Language Processing, SANYA: 632?637.Antal van den Bosch and S. Buchholz.
2002.Shallow parsing on the basis of words only: acase study, In Proceedings of the 40th ACL: 433?440.Taku Kudo and Yuji Matsumoto.
2000.
Use ofsupport vector learning for chunk identification,In Proceedings of the 4th CoNLL: 142?144.J.
Carroll, T. Briscoe, G. Carroll et al 1997.Phrasal parsing software.
Sparkle WorkPackage 3, Deliverable D3.2.Yuqi Zhang and Qiang Zhou.
2002.
Automaticidentification of Chinese base phrases.
Journalof Chinese Information Processing, 16(6):1?8.X.
Li and D. Roth.
2001.
Exploring evidence forshallow parsing.
In Proceedings of the 5thCoNLL.Alexandra Kinyon.
2003.
A language-independentshallow-parser compiler.
In Proceedings of 10thEACL Conference, Toulouse, France: 322-329.S.-B.
Park, B.-T. Zhang.
2003.
Text chunking bycombining hand-crafted rules and memory-based,In Proceedings of the 41th ACL: 497?504.Thorsten Brants.
2000.
Inter-annotator agreementfor a German newspaper corpus, In SecondInternational Conference on LanguageResources and Evaluation LREC-2000, Athens,Greece: 69?76.Nianwen Xue, Fu-Dong Chiou and M. Palmer.2002.
Building a large-scale annotated Chinesecorpus, In Proceedings of COLING.
