Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 62?71,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPCube Pruning as Heuristic SearchMark Hopkins and Greg LangmeadLanguage Weaver, Inc.4640 Admiralty Way, Suite 1210Marina del Rey, CA 90292{mhopkins,glangmead}@languageweaver.comAbstractCube pruning is a fast inexact method forgenerating the items of a beam decoder.In this paper, we show that cube pruningis essentially equivalent to A* search on aspecific search space with specific heuris-tics.
We use this insight to develop fasterand exact variants of cube pruning.1 IntroductionIn recent years, an intense research focus on ma-chine translation (MT) has raised the quality ofMT systems to the degree that they are now viablefor a variety of real-world applications.
Becauseof this, the research community has turned its at-tention to a major drawback of such systems: theyare still quite slow.
Recent years have seen a flurryof innovative techniques designed to tackle thisproblem.
These include cube pruning (Chiang,2007), cube growing (Huang and Chiang, 2007),early pruning (Moore and Quirk, 2007), clos-ing spans (Roark and Hollingshead, 2008; Roarkand Hollingshead, 2009), coarse-to-fine methods(Petrov et al, 2008), pervasive laziness (Pust andKnight, 2009), and many more.This massive interest in speed is bringing rapidprogress to the field, but it comes with a certainamount of baggage.
Each technique brings its ownterminology (from the cubes of (Chiang, 2007)to the lazy lists of (Pust and Knight, 2009)) intothe mix.
Often, it is not entirely clear why theywork.
Many apply only to specialized MT situ-ations.
Without a deeper understanding of thesemethods, it is difficult for the practitioner to com-bine them and adapt them to new use cases.In this paper, we attempt to bring some clarityto the situation by taking a closer look at one ofthese existing methods.
Specifically, we cast thepopular technique of cube pruning (Chiang, 2007)in the well-understood terms of heuristic search(Pearl, 1984).
We show that cube pruning is essen-tially equivalent to A* search on a specific searchspace with specific heuristics.
This simple obser-vation affords a deeper insight into how and whycube pruning works.
We show how this insight en-ables us to easily develop faster and exact variantsof cube pruning for tree-to-string transducer-basedMT (Galley et al, 2004; Galley et al, 2006; DeN-ero et al, 2009).2 Motivating ExampleWe begin by describing the problem that cubepruning addresses.
Consider a synchronouscontext-free grammar (SCFG) that includes thefollowing rules:A ?
?A 0 B 1 , A 0 B 1 ?
(1)B ?
?A 0 B 1 , B 1 A 0 ?
(2)A ?
?B 0 A 1 , c B 0 b A 1 ?
(3)B ?
?B 0 A 1 , B 0 A 1 ?
(4)Figure 1 shows CKY decoding in progress.
CKYis a bottom-up algorithm that works by buildingobjects known as items, over increasingly largerspans of an input sentence (in the context of SCFGdecoding, the items represent partial translationsof the input sentence).
To limit running time, it iscommon practice to keep only the n ?best?
itemsper span (this is known as beam decoding).
Atthis point in Figure 1, every span of size 2 or lesshas already been filled, and now we want to fillspan [2, 5] with the n items of lowest cost.
Cubepruning addresses the problem of how to computethe n-best items efficiently.We can be more precise if we introduce someterminology.
An SCFG rule has the form X ??
?, ?,?
?, where X is a nonterminal (called thepostcondition), ?, ?
are strings that may containterminals and nonterminals, and ?
is a 1-1 corre-spondence between equivalent nonterminals of ?and ?.62Figure 1: CKY decoding in progress.
We want tofill span [2,5] with the lowest cost items.Usually SCFG rules are represented like the ex-ample rules (1)-(4).
The subscripts indicate cor-responding nonterminals (according to ?).
Definethe preconditions of a rule as the ordered sequenceof its nonterminals.
For clarity of presentation, wewill henceforth restrict our focus to binary rules,i.e.
rules of the form: Z ?
?X 0 Y 1 , ??.
Observethat all the rules of our example are binary rules.An item is a triple that contains a span and twostrings.
We refer to these strings as the postcon-dition and the carry, respectively.
The postcon-dition tells us which rules may be applied to theitem.
The carry gives us extra information re-quired to correctly score the item (in SCFG decod-ing, typically it consists of boundary words for ann-gram language model).
1 To flatten the notation,we will generally represent items as a 4-tuple, e.g.
[2, 4,X, a ?
b].In CKY, new items are created by applying rulesto existing items:r : Z ?
?X 0 Y 1 , ??
[?, ?,X, ?1] [?, ?,Y, ?2][?, ?,Z, carry(r, ?1, ?2)](5)In other words, we are allowed to apply arule r to a pair of items ?1, ?2if the itemspans are complementary and preconditions(r) =?postcondition(?1), postcondition(?2)?.
The newitem has the same postcondition as the appliedrule.
We form the carry for the new item throughan application-dependent function carry that com-bines the carries of its subitems (e.g.
if the carry isn-gram boundary words, then carry computes the1Note that the carry is a generic concept that can store anykind of non-local scoring information.new boundary words).
As a shorthand, we intro-duce the notation ?1?
r ?
?2to describe an itemcreated by applying formula (5) to rule r and items?1, ?2.When we create a new item, it is scored usingthe following formula: 2cost(?1?
r ?
?2) , cost(r)+ cost(?1)+ cost(?2)+ interaction(r, ?1, ?2)(6)We assume that each grammar rule r has anassociated cost, denoted cost(r).
The interac-tion cost, denoted interaction(r, ?1, ?2), uses thecarry information to compute cost componentsthat cannot be incorporated offline into the rulecosts (again, for our purposes, this is a languagemodel score).Cube pruning addresses the problem of effi-ciently computing the n items of lowest cost fora given span.3 Item Generation as Heuristic SearchRefer again to the example in Figure 1.
We want tofill span [2,5].
There are 26 distinct ways to applyformula (5), which result in 10 unique items.
Oneapproach to finding the lowest-cost n items: per-form all 26 distinct inferences, compute the cost ofthe 10 unique items created, then choose the low-est n.The 26 different ways to form the items can bestructured as a search tree.
See Figure 2.
Firstwe choose the subspans, then the rule precondi-tions, then the rule, and finally the subitems.
No-tice that this search space is already quite large,even for such a simple example.
In a realistic situ-ation, we are likely to have a search tree with thou-sands (possibly millions) of nodes, and we mayonly want to find the best 100 or so goal nodes.To explore this entire search space seems waste-ful.
Can we do better?Why not perform heuristic search directly onthis search space to find the lowest-cost n items?In order to do this, we just need to add heuristicsto the internal nodes of the space.Before doing so, it will help to elaborate onsome of the details of the search tree.
Letrules(X,Y) be the subset of rules with precondi-tions ?X,Y?, sorted by increasing cost.
Similarly,2Without loss of generality, we assume an additive costfunction.63Figure 2: Item creation, structured as a searchspace.
rule(X,Y, k) denotes the kth lowest-costrule with preconditions ?X,Y?.
item(?, ?,X, k)denotes the kth lowest-cost item of span [?, ?
]with postcondition X.let items(?, ?,X) be the subset of items with span[?, ?]
and postcondition X, also sorted by increas-ing cost.
Finally, let rule(X,Y, k) denote the kthrule of rules(X,Y) and let item(?, ?,X, k) denotethe kth item of items(?, ?,X).A path through the search tree consists of thefollowing sequence of decisions:1.
Set i, j, k to 1.2.
Choose the subspans: [?, ?
], [?, ?].3.
Choose the first precondition X of the rule.4.
Choose the second precondition Y of therule.5.
While rule not yet accepted and i <|rules(X,Y)|:(a) Choose to accept/reject rule(X,Y, i).
Ifreject, then increment i.6.
While item not yet accepted for subspan[?, ?]
and j < |items(?, ?,X)|:(a) Choose to accept/reject item(?, ?,X, j).If reject, then increment j.7.
While item not yet accepted for subspan [?, ?
]and k < |items(?, ?,Y)|:(a) Choose to accept/reject item(?, ?,Y, k).If reject, then increment k.Figure 3: The lookahead heuristic.
We set theheuristics for rule and item nodes by lookingahead at the cost of the greedy solution from thatpoint in the search space.Figure 2 shows two complete search paths forour example, terminated by goal nodes (in black).Notice that the internal nodes of the search spacecan be classified by the type of decision theygovern.
To distinguish between these nodes, wewill refer to them as subspan nodes, preconditionnodes, rule nodes, and item nodes.We can now proceed to attach heuristics to thenodes and run a heuristic search protocol, say A*,on this search space.
For subspan and preconditionnodes, we attach trivial uninformative heuristics,i.e.
h = ??.
For goal nodes, the heuristic is theactual cost of the item they represent.
For rule anditem nodes, we will use a simple type of heuristic,often referred to in the literature as a lookaheadheuristic.
Since the rule nodes and item nodes areordered, respectively, by rule and item cost, it ispossible to ?look ahead?
at a greedy solution fromany of those nodes.
See Figure 3.
This greedy so-lution is reached by choosing to accept every de-cision presented until we hit a goal node.If these heuristics were admissible (i.e.
lowerbounds on the cost of the best reachable goalnode), this would enable us to exactly generate then-best items without exhausting the search space(assuming the heuristics are strong enough for A*to do some pruning).
Here, the lookahead heuris-tics are clearly not admissible, however the hopeis that A* will generate n ?good?
items, and thatthe time savings will be worth sacrificing exact-ness for.644 Cube Pruning as Heuristic SearchIn this section, we will compare cube pruning withour A* search protocol, by tracing through theirrespective behaviors on the simple example of Fig-ure 1.4.1 Phase 1: InitializationTo fill span [?, ?
], cube pruning (CP) begins byconstructing a cube for each tuple of the form:?
[?, ?
], [?, ?
], X , Y?where X and Y are nonterminals.
A cube consistsof three axes: rules(X,Y) and items(?, ?,X) anditems(?, ?,Y).
Figure 4(left) shows the nontrivialcubes for our example scenario.Contrast this with A*, which begins by addingthe root node of our search space to an empty heap(ordered by heuristic cost).
It proceeds to repeat-edly pop the lowest-cost node from the heap, thenadd its children to the heap (we refer to this op-eration as visiting the node).
Note that before A*ever visits a rule node, it will have visited everysubspan and precondition node (because they allhave cost h = ??).
Figure 4(right) shows thestate of A* at this point in the search.
We assumethat we do not generate dead-end nodes (a simplematter of checking that there exist applicable rulesand items for the chosen subspans and precondi-tions).
Observe the correspondence between thecubes and the heap contents at this point in the A*search.4.2 Phase 2: Seeding the HeapCube pruning proceeds by computing the ?best?item of each cube ?
[?, ?
], [?, ?
], X , Y?, i.e.item(?, ?,X, 1)?
rule(X,Y, 1)?
item(?, ?,Y, 1)Because of the interaction cost, there is no guaran-tee that this will really be the best item of the cube,however it is likely to be a good item because thecosts of the individual components are low.
Theseitems are added to a heap (to avoid confusion, wewill henceforth refer to the two heaps as the CPheap and the A* heap), and prioritized by theircosts.Consider again the example.
CP seeds its heapwith the ?best?
items of the 4 cubes.
There is nowa direct correspondence between the CP heap andthe A* heap.
Moreover, the costs associated withthe heap elements also correspond.
See Figure 5.4.3 Phase 3: Finding the First ItemCube pruning now pops the lowest-cost item fromthe CP heap.
This means that CP has decided tokeep the item.
After doing so, it forms the ?one-off?
items and pushes those onto the CP heap.
SeeFigure 5(left).
The popped item is:item (viii) ?
rule (1) ?
item (xii)CP then pushes the following one-off successorsonto the CP heap:item (viii) ?
rule (2) ?
item (xii)item (ix) ?
rule (1) ?
item (xii)item (viii) ?
rule (1) ?
item (xiii)Contrast this with A*, which pops the lowest-cost search node from the A* heap.
Here we needto assume that our A* protocol differs slightlyfrom standard A*.
Specifically, it will practicenode-tying, meaning that when it visits a rule nodeor an item node, then it also (atomically) visits allnodes on the path to its lookahead goal node.
SeeFigure 5(right).
Observe that all of these nodeshave the same heuristic cost, thus standard A* islikely to visit these nodes in succession withoutthe need to enforce node-tying, but it would notbe guaranteed (because the heuristics are not ad-missible).
A* keeps the goal node it finds and addsthe successors to the heap, scored with their looka-head heuristics.
Again, note the direct correspon-dence between what CP and A* keep, and whatthey add to their respective heaps.4.4 Phase 4: Finding Subsequent ItemsCube pruning and A* continue to repeat Phase3 until k unique items have been kept.
Whilewe could continue to trace through the example,by now it should be clear: cube pruning and ourA* protocol with node-tying are doing the samething at each step.
In fact, they are exactly thesame algorithm.
We do not present a formal proofhere; this statement should be regarded as confi-dent conjecture.The node-tying turns out to be an unnecessaryartifact.
In our early experiments, we discoveredthat node-tying has no impact on speed or qual-ity.
Hence, for the remainder of the paper, weview cube pruning in very simple terms: as noth-ing more than standard A* search on the searchspace of Section 3.65Figure 4: (left) Cube formation for our example.
(right) The A* protocol, after all subspan and precon-dition nodes have been visited.
Notice the correspondence between the cubes and the A* heap contents.Figure 5: (left) One step of cube pruning.
(right) One step of the A* protocol.
In this figure,cost(r, ?1, ?2) , cost(?1?
r ?
?2).665 Augmented Cube PruningViewed in this light, the idiosyncracies of cubepruning begin to reveal themselves.
On the onehand, rule and item nodes are associated withstrong but inadmissible heuristics (the short expla-nation for why cube pruning is an inexact algo-rithm).
On the other hand, subspan and precondi-tion nodes are associated with weak trivial heuris-tics.
This should be regarded neither as a surprisenor a criticism, considering cube pruning?s originsin hierarchical phrase-based MT models (Chiang,2007), which have only a small number of distinctnonterminals.But the situation is much different in tree-to-string transducer-based MT (Galley et al,2004; Galley et al, 2006; DeNero et al, 2009).Transducer-based MT relies on SCFGs with largenonterminal sets.
Binarizing the grammars (Zhanget al, 2006) further increases the size of these sets,due to the introduction of virtual nonterminals.A key benefit of the heuristic search viewpointis that it is well positioned to take advantage ofsuch insights into the structure of a particular de-coding problem.
In the case of transducer-basedMT, the large set of preconditions encourages usto introduce a nontrivial heuristic for the precon-dition nodes.
The inclusion of these heuristics intothe CP search will enable A* to eliminate cer-tain preconditions from consideration, giving us aspeedup.
For this reason we call this strategy aug-mented cube pruning.5.1 Heuristics on preconditionsRecall that the total cost of a goal node is given byEquation (6), which has four terms.
We will formthe heuristic for a precondition node by creatinga separate heuristic for each of the four terms andusing the sum as the overall heuristic.To describe these heuristics, we will make intu-itive use of the wildcard operator ?
to extend ourexisting notation.
For instance, items(?, ?, *) willdenote the union of items(?, ?,X) over all possi-ble X, sorted by cost.We associate the heuristic h(?,X,Y) with thesearch node reached by choosing subspans [?, ?
],[?, ?
], precondition X (for span [?, ?
]), and precon-dition Y (for span [?, ?]).
The heuristic is the sumof four terms, mirroring Equation (6):h(?,X,Y) = cost(rule(X,Y, 1))+ cost(item(?, ?,X, 1))+ cost(item(?, ?,Y, 1))+ ih(?,X,Y)The first three terms are admissible becauseeach is simply the minimum possible cost ofsome choice remaining to be made.
To con-struct the interaction heuristic ih(?,X,Y), con-sider that in a translation model with an inte-grated n-gram language model, the interactioncost interaction(r, ?1, ?2) is computed by addingthe language model costs of any new complete n-grams that are created by combining the carries(boundary words) with each other and with thelexical items on the rule?s target side, taking intoaccount any reordering that the rule may perform.We construct a backoff-style estimate of thesenew n-grams by looking at item(?, ?,X, 1) =[?, ?,X, ?1], item(?, ?,Y, 1) = [?, ?,Y, ?2], andrule(X,Y, 1).
We set ih(?,X,Y) to be a linearcombination of the backoff n-grams of the carries?1and ?2, as well as any n-grams introduced bythe rule.
For instance, if?1= a b ?
c d?2= e f ?
g hrule(X,Y, 1) = Z ?
?X 0 Y 1 , X 0 g h i Y 1 ?thenih(?,X,Y) = ?1?
LM(a) + ?2?
LM(a b)+ ?1?
LM(e) + ?2?
LM(e f)+ ?1?
LM(g) + ?2?
LM(g h)+ ?3?
LM(g h i)The coefficients of the combination are free pa-rameters that we can tune to trade off betweenmore pruning and more admissability.
Setting thecoefficients to zero gives perfect admissibility butis also weak.The heuristic for the first precondition node iscomputed similarly:h(?,X, ?)
= cost(rule(X, ?, 1))+ cost(item(?, ?,X, 1))+ cost(item(?, ?, ?, 1))+ ih(?,X, ?
)67Standard CP Augmented CPnodes (k) BLEU time nodes (k) BLEU time80 34.9 2.5 52 34.7 1.9148 36.1 3.9 92 35.9 2.4345 37.2 7.9 200 37.3 5.4520 37.7 13.4 302 37.7 8.5725 38.2 17.1 407 38.0 10.71092 38.3 27.1 619 38.2 16.31812 38.6 45.9 1064 38.5 27.7Table 1: Results of standard and augmented cubepruning.
The number of (thousands of) searchnodes visited is given along with BLEU and av-erage time to decode one sentence, in seconds.0 500000 1x1061.5x106 2x106Search nodes visited35363738BLEUStandard CPAugmented CPFigure 6: Nodes visited by standard and aug-mented cube pruning.We also apply analogous heuristics to the subspannodes.5.2 Experimental setupWe evaluated all of the algorithms in this paper ona syntax-based Arabic-English translation systembased on (Galley et al, 2006), with rules extractedfrom 200 million words of parallel data from NIST2008 and GALE data collections, and with a 4-gram language model trained on 1 billion wordsof monolingual English data from the LDC Giga-word corpus.
We evaluated the system?s perfor-mance on the NIST 2008 test corpus, which con-sists of 1357 Arabic sentences from a mixture ofnewswire and web domains, with four English ref-erence translations.
We report BLEU scores (Pa-pineni et al, 2002) on untokenized, recapitalizedoutput.5.3 Results for Augmented Cube PruningThe results for augmented cube pruning are com-pared against cube pruning in Table 1.
The data0 10 20 30 40 50Average time per sentence (s)35363738BLEUStandard CPAugmented CPFigure 7: Time spent by standard and augmentedcube pruning, average seconds per sentence.Standard CP Augmented CPsubspan 12936 12792precondition 851458 379954rule 33734 33331item 119703 118889goal 74618 74159TOTAL 1092449 619125BLEU 38.33 38.22Table 2: Breakdown of visited search nodes bytype (for a fixed beam size).from that table are also plotted in Figure 6 andFigure 7.
Each line gives the number of nodesvisited by the heuristic search, the average timeto decode one sentence, and the BLEU of the out-put.
The number of items kept by each span (thebeam) is increased in each subsequent line of thetable to indicate how the two algorithms differ atvarious beam sizes.
This also gives a more com-plete picture of the speed/BLEU tradeoff offeredby each algorithm.
Because the two algorithmsmake the same sorts of lookahead computationswith the same implementation, they can be mostdirectly compared by examining the number ofvisited nodes.
Augmenting cube pruning with ad-missible heuristics on the precondition nodes leadsto a substantial decrease in visited nodes, by 35-44%.
The reduction in nodes converges to a con-sistent 40% as the beam increases.
The BLEUwith augmented cube pruning drops by an averageof 0.1 compared to standard cube pruning.
This isdue to the additional inadmissibility of the interac-tion heuristic.To see in more detail how the heuristics affectthe search, we give in Table 2 the number of nodesof each type visited by both variants for one beam68size.
The precondition heuristic enables A* toprune more than half the precondition nodes.6 Exact Cube PruningCommon wisdom is that the speed of cube prun-ing more than compensates for its inexactness (re-call that this inexactness is due to the fact that ituses A* search with inadmissible heuristics).
Es-pecially when we move into transducer-based MT,the search space becomes so large that brute-forceitem generation is much too slow to be practi-cal.
Still, within the heuristic search frameworkwe may ask the question: is it possible to applystrictly admissible heuristics to the cube pruningsearch space, and in so doing, create a version ofcube pruning that is both fast and exact, one thatfinds the n best items for each span and not justn good items?
One might not expect such a tech-nique to outperform cube pruning in practice, butfor a given use case, it would give us a relativelyfast way of assessing the BLEU drop incurred bythe inexactness of cube pruning.Recall again that the total cost of a goal nodeis given by Equation (6), which has four terms.
Itis easy enough to devise strong lower bounds forthe first three of these terms by extending the rea-soning of Section 5.
Table 3 shows these heuris-tics.
The major challenge is to devise an effectivelower bound on the fourth term of the cost func-tion, the interaction heuristic, which in our case isthe incremental language model cost.We take advantage of the following observa-tions:1.
In a given span, many boundary word pat-terns are repeated.
In other words, for a par-ticular span [?, ?]
and carry ?, we often seemany items of the form [?, ?,X, ?
], wherethe only difference is the postcondition X.2.
Most rules do not introduce lexical items.
Inother words, most of the grammar rules havethe form Z ?
?X0Y1, X0Y1?
(concatena-tion rules) or Z ?
?X0Y1, Y1X0?
(inver-sion rules).The idea is simple.
We split the search into threesearches: one for concatenation rules, one for in-version rules, and one for lexical rules.
Eachsearch finds the n?best items that can be createdusing its respective set of rules.
We then take these3n items and keep the best n.10 20 30 40 50 60 70Average time per sentence (s)35363738BLEUStandard CPExact CPFigure 8: Time spent by standard and exact cubepruning, average seconds per sentence.Doing this split enables us to precompute astrong and admissible heuristic on the interactioncost.
Namely, for a given span [?, ?
], we pre-compute ihadm(?,X,Y), which is the best LMcost of combining carries from items(?, ?,X)and items(?, ?,Y).
Notice that this statistic isonly straightforward to compute once we can as-sume that the rules are concatenation rules orinversion rules.
For the lexical rules, we setihadm(?,X,Y) = 0, an admissible but weakheuristic that we can fortunately get away with be-cause of the small number of lexical rules.6.1 Results for Exact Cube PruningComputing the ihadm(?,X,Y) heuristic is notcheap.
To be fair, we first compare exact CP tostandard CP in terms of overall running time, in-cluding the computational cost of this overhead.We plot this comparison in Figure 8.
Surprisingly,the time/quality tradeoff of exact CP is extremelysimilar to standard CP, suggesting that exact cubepruning is actually a practical alternative to stan-dard CP, and not just of theoretical value.
Wefound that the BLEU loss of standard cube prun-ing at moderate beam sizes was between 0.4 and0.6.Another surprise comes when we contrast thenumber of visited search nodes of exact CP andstandard CP.
See Figure 9.
While we initially ex-pected that exact CP must visit fewer nodes tomake up for the computational overhead of its ex-pensive heuristics, this did not turn out to be thecase, suggesting that the computational cost ofstandard CP?s lookahead heuristics is just as ex-pensive as the precomputation of ihadm(?,X,Y).69heuristic componentssubspan precondition1 precondition2 rule item1 item2h(?)
h(?,X) h(?,X,Y) h(?,X,Y, i) h(?,X,Y, i, j) h(?,X,Y, i, j, k)r rule(?, ?, 1) rule(X, ?, 1) rule(X,Y, 1) rule(X,Y, i) rule(X,Y, i) rule(X,Y, i)?1item(?, ?, ?, 1) item(?, ?,X, 1) item(?, ?,X, 1) item(?, ?,X, 1) item(?, ?,X, j) item(?, ?,X, j)?2item(?, ?, ?, 1) item(?, ?, ?, 1) item(?, ?,Y, 1) item(?, ?,Y, 1) item(?, ?,Y, 1) item(?, ?,Y, k)ih ihadm(?, ?, ?)
ihadm(?,X, ?)
ihadm(?,X,Y) ihadm(?,X,Y) ihadm(?,X,Y) ihadm(?,X,Y)Table 3: Admissible heuristics for exact CP.
We attach heuristic h(?,X,Y, i, j, k) to the search nodereached by choosing subspans [?, ?
], [?, ?
], preconditions X and Y, the ith rule of rules(X,Y), the jthitem of item(?, ?,X), and the kth item of item(?, ?,Y).
To form the heuristic for a particular type ofsearch node (column), compute the following: cost(r) + cost(?1) + cost(?2) + ih500000 1x1061.5x106 2x106Search nodes visited35363738BLEUStandard CPExact CPFigure 9: Nodes visited by standard and exactcube pruning.7 ImplicationsThis paper?s core idea is the utility of framingCKY item generation as a heuristic search prob-lem.
Once we recognize cube pruning as noth-ing more than A* on a particular search spacewith particular heuristics, this deeper understand-ing makes it easy to create faster and exact vari-ants for other use cases (in this paper, we focuson tree-to-string transducer-based MT).
Depend-ing on one?s own particular use case, a variety ofpossibilities may present themselves:1.
What if we try different heuristics?
In this pa-per, we do some preliminary inquiry into thisquestion, but it should be clear that our minorchanges are just the tip of the iceberg.
Onecan easily imagine clever and creative heuris-tics that outperform the simple ones we haveproposed here.2.
What if we try a different search space?
Whyare we using this particular search space?Perhaps a different one, one that makes de-cisions in a different order, would be moreeffective.3.
What if we try a different search algorithm?A* has nice guarantees (Dechter and Pearl,1985), but it is space-consumptive and it isnot anytime.
For a use case where we wouldlike a finer-grained speed/quality tradeoff, itmight be useful to consider an anytime searchalgorithm, like depth-first branch-and-bound(Zhang and Korf, 1995).By working towards a deeper and unifying under-standing of the smorgasbord of current MT speed-up techniques, our hope is to facilitate the task ofimplementing such methods, combining them ef-fectively, and adapting them to new use cases.AcknowledgmentsWe would like to thank Abdessamad Echihabi,Kevin Knight, Daniel Marcu, Dragos Munteanu,Ion Muslea, Radu Soricut, Wei Wang, and theanonymous reviewers for helpful comments andadvice.
Thanks also to David Chiang for the useof his LaTeX macros.
This work was supported inpart by CCS grant 2008-1245117-000.ReferencesDavid Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228.Rina Dechter and Judea Pearl.
1985.
Generalized best-first search strategies and the optimality of a*.
Jour-nal of the ACM, 32(3):505?536.John DeNero, Mohit Bansal, Adam Pauls, and DanKlein.
2009.
Efficient parsing for transducer gram-mars.
In Proceedings of the Human Language Tech-nology Conference of the NAACL, Main Conference.70Michel Galley, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a translation rule?In Proceedings of HLT/NAACL.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic models.
In Proceedings ofACL-COLING.Liang Huang and David Chiang.
2007.
Forest rescor-ing: Faster decoding with integrated language mod-els.
In Proceedings of ACL.Robert C. Moore and Chris Quirk.
2007.
Fasterbeam-search decoding for phrasal statistical ma-chine translation.
In Proceedings of MT Summit XI.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automaticevaluation of machine translation.
In Proceedingsof 40th Annual Meeting of the Association for Com-putational Linguistics, pages 311?318.Judea Pearl.
1984.
Heuristics.
Addison-Wesley.Slav Petrov, Aria Haghighi, and Dan Klein.
2008.Coarse-to-fine syntactic machine translation usinglanguage projections.
In Proceedings of EMNLP.Michael Pust and Kevin Knight.
2009.
Faster mt de-coding through pervasive laziness.
In Proceedingsof NAACL.Brian Roark and Kristy Hollingshead.
2008.
Classi-fying chart cells for quadratic complexity context-free inference.
In Proceedings of the 22nd Inter-national Conference on Computational Linguistics(Coling 2008), pages 745?752.Brian Roark and Kristy Hollingshead.
2009.
Lin-ear complexity context-free parsing pipelines viachart constraints.
In Proceedings of Human Lan-guage Technologies: The 2009 Annual Conferenceof the North American Chapter of the Associa-tion for Computational Linguistics, pages 647?655,Boulder, Colorado, June.
Association for Computa-tional Linguistics.Weixiong Zhang and Richard E. Korf.
1995.
Perfor-mance of linear-space search algorithms.
ArtificialIntelligence, 79(2):241?292.Hao Zhang, Liang Huang, Daniel Gildea, and KevinKnight.
2006.
Synchronous binarization for ma-chine translation.
In Proceedings of the Human Lan-guage Technology Conference of the NAACL, MainConference, pages 256?263.71
