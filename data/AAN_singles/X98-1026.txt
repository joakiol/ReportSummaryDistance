AUTOMATED TEXT SUMMARIZATIONAND THE SUMMARIST SYSTEMEduard Hovy and Chin-Yew LinI n fo rmat ion  Sc iences Inst ituteo f  the Un ivers i ty  o f  Southern Ca l i fo rn ia4676 Admira l ty  WayMar ina  del Rey,  CA  90292-6695emai l :  { hovy ,cy l  } @isi .edutel: 310-822-1511AbstractThis paper consists of three parts: a preliminary typologyof summaries in general; a description of the current andplanned modules and performance of the SUMMARISTautomated multilingual text summarization system beingbuilt sat ISI, and a discussion of three methods to evaluatesummaries.1.
THE NATURE OF  SUMMARIESEarly experimentation in the late 1950's and early 60'ssuggested that text summarization by computer wasfeasible though not straightforward (Luhn, 59;Edmundson, 68).
The methods developed then werefairly unsophisticated, relying primarily on surface levelphenomena such as sentence position and word frequencycounts, and focused on producing extracts (passagesselected from the text, reproduced verbatim) rather thanabstracts (interpreted portions of the text, newlygenerated).After a hiatus of some decades, the growing presence oflarge amounts of online text--in corpora and especiallyon the Web--renewed the interest in automated textsummarization.
During these intervening decades,progress in Natural Language Processing (NLP), coupledwith great increases of computer memory and speed,made possible more sophisticated techniques, with veryencouraging results.
In the late 1990's, some relativelysmall research investments in the US (not more than 10projects, including commercial efforts at Microsoft,Lexis-Nexis, Oracle, SRA, and TextWise, and universityefforts at CMU, NMSU, UPenn, and USC/ISI) over threeor four years have produced several systems that exhibitpotential marketability, as well as several innovations thatpromise continued improvement.
In addition, severalrecent workshops, a book collection, and several tutorialstestify that automated text summarization has become ahot area.However, when one takes a moment to study the varioussystems and to consider what has really been achieved,one cannot help being struck by their underlyingsimilarity, by the narrowness of their focus, and by thelarge numbers of unknown factors that surround theproblem.
For example, what precisely is a summary?No-one seems to know exactly.
In our work, we usesummary as the generic term and define it as follows:A summary is a text that is produced out of one ormore (possibly multimedia) texts, that contains(some of) the same information of the originaltext(s), and that is no longer than half of theoriginal text(s).To clarify the picture a little, we follow and extend(Sp~irck Jones, 97) by identifying the following aspects ofvariation.
Any summary can be characterized by (at least)three major classes of characteristics:Invut: characteristics of the source text(s)Source size: single-document vs .
multi-docu-ment: A single-document summary derives from asingle input text (though the summarization processitself may employ information compiled earlier fromother texts).
A multi-document summary is one textthat covers the content of more than one input text,and is usually used only when the input texts arethematically related.Specificity: domain-specific vs. general: Whenthe input texts all pertain to a single domain, it maybe appropriate to apply domain-speci f icsummarization techniques, focus on specific content,and output specific formats, compared to the generalcase.
A domain-specific summary derives from inputtext(s) whose theme(s) pertain to a single restricteddomain.
As such, it can assume less term ambiguity,idiosyncratic word and grammar usage, specializedformatting, etc., and can reflect hem in the summary.197A general-domain summary derives from inputtext(s) in any domain, and can make no suchassumptions.Genre and scale: Typical input genres includenewspaper articles, newspaper editorials or opinionpieces, novels, short stories, non-fiction books,progress reports, business reports, and so on.
Thescale may vary from book-length to paragraph-length.
Different summarization techniques mayapply to some genres and scales and not others.Output: characteristics of the summary as a textDerivation: Extract vs .
abstract: An extract is acollection of passages (ranging from single words towhole paragraphs) extracted from the input text(s)and produced verbatim as the summary.
An abstractis a newly generated text, produced from somecomputer-internal representation that results afteranalysis of the input.Coherence: f luent vs .
disfluent: A fluentsummary is written in full, grammatical sentences,and the sentences are related and follow one anotheraccording to the rules of coherent discourse structure.A disfluent summary is fragmented, consisting ofindividual words or text portions that are either notcomposed into grammatical sentences or notcomposed into coherent paragraphs.Partiality: neutral vs .
evaluative: Thischaracteristic applies principally when the inputmaterial is subject to opinion or bias.
A neutralsummary reflects the content of the input text(s),partial or impartial as it may be.
An evaluativesummary includes some of the system's own bias,whether explicitly (using statements of opinion) orimplicitly (through inclusion of material with onebias and omission of material with another).Conventionality: fixed vs .
floating: A fixed-situation summary is created for a specific use, reader(or class of reader), and situation.
As such, it canconform to appropriate in-house conventions ofhighlighting, formatting, and so on.
A floating-situation summary cannot assume fixed conventions,but is created and displayed in a variety of settings toa variety of readers for a variety of purposes.Purpose: characteristics of the summary usageAudience: Generic vs .
query-oriented: A genericsummary provides the author's point of view of theinput text(s), giving equal import to all major themesin it.
A query-oriented (or user-oriented) summaryfavors specific themes or aspect(s) of the text, inresponse to a user's desire to learn about just thosethemes in particular.
It may do so explicitly, byhighlighting pertinent themes, or implicitly, byomitting themes that do not match the user'sinterests.Usage: Indicative vs .
informative: An indicativesummary provides merely an indication of theprincipal subject matter or domain of the input text(s)without including its contents.
After reading aninformative summary, one can explain what the inputtext was about, but not necessarily what wascontained in it.
An informative summary reflects(some of) the content, and allows one to describe(parts of) what was in the input text.Expansiveness: Background vs .
just-the-news: Abackground summary assumes the reader's priorknowledge of the general setting of the input text(s)content is poor, and hence includes explanatorymaterial, such as circumstances of place, time, andactors.
A just-the-news summary contains just thenew or principal themes, assuming that the readerknows enough background to interpret them incontext.At this time, apart from early work by Sp~irck Jones andstudents, such as (Tait and Sp~irck Jones, 83), we know offew linguistic or computational studies of these and otheraspects of summaries; the work by (Van Dijk andKintsch, 83) and (Endres-Niggemeyer, 97) focus on thepsycholinguistic aspects of humans when they createsummaries.
We believe that the typology of summaries ia fruitful area for further study, both by linguistsperforming text analysis and by computational linguiststrying to create techniques to create summariesconforming to one or more of the characteristics listedabove.
A better understanding of the types of summarywill facilitate the construction of techniques and systemsthat better serve the various purposes of summarization igeneral.Our own work is computational.
Over the past twoyears, under the TIPSTER program, we have beendeveloping the text summarization system SUMMARIST(Hovy and Lin, 98; Lin, 98).
Our goal is to investigatethe nature of text summarization, using SUMMARISTboth as a research tool and as an engine to producesummaries for people upon demand.In this paper, we first describe the architecture ofSUMMARIST and provide details on the evaluatedresults of several of its modules in Sections 3, 4, and 5.Finally, since the evaluation of summaries (and ofsummarization) is a little-understood business, wedescribe some preliminary experiments in this regard inSection 6.1982.
SUMMARISTThe goal of SUMMARIST is to create summaries ofarbitrary text in English and selected other languages(Hovy and Lin, 98).
By eschewing language-specificmethods for the relatively surface-level processing, it ispossible to create a multi-lingual summarizer fairly easily.Eventually, however, SUMMARIST will includelanguage-specific techniques of parsing and semanticanalysis, and will combine robust NLP processing (usingInformation Retrieval and statistical techniques) withsymbolic world knowledge (embodied in the conceptthesaurus SENSUS (Knight and Luk, 94; Hovy, 98),derived from WordNet (Miller et al, 90) and augmentedby dictionaries and similar resources) to overcome theproblems endemic to either approach alone.
Theseproblems arise because xisting robust NLP methods tendto operate at the word level, and hence miss concept-levelgeneralizations (which are provided by symbolic worldknowledge), while on the other hand symbolic knowledgeis too difficult to acquire in large enough scale to provideadequate coverage and robustness.
For high-quality etrobust summarization, both aspects are needed.In order to maintain functionality while we experimentwith new aspects, and since not all kinds of summaryrequire the same processing steps, we have adopted a veryopen, modular design.
Since it is still under development,not all the modules of SUMMARIST are equally mature.To create extracts, one needs procedures to identify themost important passages in the input text.
To createabstracts, the core procedure is a process of interpretation.In this step, two or more topics are fused together to forma third, more succinctly stated, one.
(We define topic as aparticular subject that we write about or discuss.)
Thisstep must occur after the identification step.
Finally, toproduce the summary, a concluding step of sentencegeneration is needed.
Thus SUMMARIST is structuredaccording to the following 'equation':summarization =topic identification + interpretation + generationFor identification, the goal is to filter the input to retainonly the most important, central, topics.
Once they havebeen identified, they can simply be output, to form anextract.
Typically, topic identification can be achievedusing various complementary techniques.
This stage ofSUMMARIST is by far the most developed; making it, atpresent, an extract-only summarizer.
See Section 3.For interpretation, the goal is to perform compactionthrough re-interpreting and fusing the extracted topicsinto more succinct ones.
This is necessary becauseabstracts are usually much shorter than their equivalentextracts.
All the variations of fusion are yet to bediscovered, but they include at least simple conceptgeneralization (he ate pears, apples, and bananas---+ heate fruit) and script identification (he sat down, read themenu, ordered, ate, and left----~ he visited the restaurant).We discuss interpretation i  Section 4.For generation, the goal is to produce the outputsummary.
In the case of extracts, generation is a nullstep, but in the case of abstracts, the generator has toreformulate the extracted and fused material into acoherent, densely phrased, new text.
The modulesplanned for SUMMARIST are described in Section 5.Prior to topic identification, the system preprocessesthe input text.
This stage converts all inputs into astandard format we call SNF (Summarist Normal Form).Preprocessing includes tokenizing (to read non-Englishtexts and output word-segmented tokens); part-of-speechtagging (the tagger is based on Brill's (1992) part-of-speech tagger); demorphing (to find root forms of eachinput token, using a modification of WordNet's (Miller etal., 90) demorphing program); phrasing (to findcollocations and multi-word phrases, as recorded inWordNet); token frequency counting; tf.idf weightcalculation (to calculate the tf.idf weight (Salton, 88) foreach input token, and rank the tokens accordingly); andquery relevance calculation (to record with each sentencethe number of demorphed content words in the user'squery that also appear in that sentence).An example text in Indonesian, preprocessed into SNF,is shown in Figures l(a) and l(b).
Figure l(a) indicatesthat the text contained 1618 characters, and that it hadbeen processed by the following modules: tokenizationand part of speech tagging, title treatment, demorphing,WordNet categorization and common word identification,tf.idf computation, and OPP processing (see Section 3.1).It also records the top-scoring words in the text, togetherwith their scores, as given by the modules computing termfrequency (tf_keywords), tf.idf, and the OPP.
The fieldopp_rule shows the most important sentence positions as0 (the title); sentence 1; sentences 2 and 3 (tied), in thatorder.
Figure l(b) contains the processed text itself, oneword per line, with the features added to each word byvarious modules.
The features include paragraph andsentence number (pno and sno), part of speech (pos,empty for Indonesian), common word indicator (cwd),presence of word in title (ttl), morphology (mph),WordNet count (wnc), word frequency in text (frq), andtfidfand OPP scores (see Sections 3.3 and 3.1 resp.).3.
Phase 1: Topic IdentificationSummarization systems that perform topic identificationonly produce xtract summaries; these include the current199operational version of SUMMARIST, as well as the systems of (Aone et al, 98; Strzalkowski et al, 98; Baggaand Baldwin, 98; and Mitra et al, 97).<*top ic=#??
?><*docno=HTML-DOC><*t i t le="Senator  Dar i  Demokrat  Sarankan  C l in tonMember iKesaks ian  Di DepanKongres  "><*token_t i t le="sarankan{demokrat l c l in tonmember i l kongres lkesaks ian lsenator"><*pos t i t le=" - "><*char_count=2300><*modu le=PRE{TTL IMPHICAT{TF IDF IOPP><*t f _keywords=kesaks ian ,6 .000{kongres ,6 .00011ewinsky ,  5 .0001demokrat ,3 .000  {hadapan 3 .
    v ide  3 .
   {  saya 2 .
    agung 2 .
    d igambarkan 2 .
   { jawaban 2 .
  ><*t f id f _keywords=lewinsky ,  35 .0881kesaks ian ,  32 .448{kongres ,27 .7181v ideo ,16 .8941demkrat  3 .859 hadapan  2 .2 9 d igambarkan    .838 senat  r    .
262{pembaruan    .45 ><*opp ru le=p:0 ,111 ,212 ,413 ,4  s : - , -><*opp keywords=kongres ,26 .9171kesaks ian ,  25 .6671demokrat ,16 .33311ewinsky ,  14.167 1senat  r   2 .667{sar nkan    .667 v id    9 .333{screen 9 .
   { the 9 .
   {had pan 8 .9 7>Figure 1 (a).
Indonesian text: preamble, after preprocessing.Dari <pno=2 sno=3 pos=NA cwd=l ttl=0 mph=- wnc=- frq=0 tfidf=0.000 opp=-,->Demokrat <pno=2 sno=3 pos=NA cwd=0 ttl=l mph=- wnc=- frq=3 tfidf=13.859 opp=16.333,3.667>Sarankan <pno=2 sno=3 pos=NA cwd=0 ttl=l mph=- wnc=- f rq=l  tfidf=5.631 opp=i0.667,3.667>Clinton <pno=2 sno=3 pos=NA cwd=l ttl=0 mph=- wnc=- frq=0 tfidf=0.000 opp=-,->Memberi <pno=3 sno=l pos=NA cwd=l ttl=0 mph=- wnc=- frq=0 tfidf=0.000 opp=-,->Kesaksian <pno=3 sno=l pos=NA cwd=0 ttl=l mph=- wnc=- frq=6 tfidf=32.448 opp=25.667,3.250>Di <pno=3 sno=l pos=NA cwd=l ttl=0 mph=- wnc=- frq=0 tfidf=0.000 opp=-,->Depan <pno=3 sno=l pos=NA cwd=l ttl=0 mph=- wnc=- frq=0 tfidf=0.000 opp=-,->Kongres <pno=3 sno=l pos=NA cwd=0 ttl=l mph=- wnc=- frq=6 tfidf=27.718 opp=26.917,3.250>Washington <pno=4 sno=l pos=NA cwd=0 ttl=0 mph=- wnc=- frq=l tfidf=3.434 opp=2.833,2.833><pno=4 sno=l pos=PUN cwd=l ttl=0 mph=- wnc=- frq=0 tfidf=0 opp=-,->Pembaruan <pno=4 sno=l pos=NA cwd=0 ttl=0 mph=- wnc=- frq=2 tfidf=10.451 opp=6.500,2.833>Seorang <pno=5 sno=l pos=NA cwd=l ttl=0 mph=- wnc=- frq=0 tfidf=0.000 opp=-,->senator <pno=5 sno=l pos=NA cwd=0 ttl=l mph=- wnc=- frq=2 tfidf=ll.262 opp=12.667,2.833>dari <pno=5 sno=l pos=NA cwd=l ttl=0 mph=- wnc=- frq=0 tfidf=0.000 opp=-,->Partai <pno=5 sno=l pos=NA cwd=l ttl=0 mph=- wnc=- frq=0 tfidf=0.000 opp=-,->Demokrat <pno=5 sno=l pos=NA cwd=0 ttl=l mph=- wnc=- frq=3 tfidf=13.859 opp=16.333,2.833><pno=5 sno=l pos=PUN cwd=l ttl=0 mph=- wnc=- frq=0 tfidf=0 opp=-,->hari <pno=5 sno=l pos=NA cwd=l ttl=0 mph=- wnc=- frq=0 tfidf=0.000 opp=-,->Minggu <pno=5 sno=l pos=NA cwd=l ttl=0 mph=- wnc=- frq=0 tfidf=0.000 opp=-,->( <pno=5 sno=l pos=PUN cwd=l ttl=0 mph=- wnc=- frq=0 tfidf=0 opp=-,->20 <pno=5 sno=l pos=CD cwd=l ttl=0 mph=- wnc=- frq=0 tfidf=0 opp=-,->/ <pno=5 sno=l pos=PUN cwd=l ttl=0 mph=- wnc=- frq=0 tfidf=0 opp=-,->9 <pno=5 sno=l pos=NA cwd=l ttl=0 mph=- wnc=- frq=0 tfidf=0 opp=-,->) <pno=5 sno=l pos=PUN cwd=l ttl=0 mph=- wnc=- frq=0 tfidf=0 opp=-,-><pno=5 sno=l pos=PUN cwd=l ttl=0 mph=- wnc=- frq=0 tfidf=0 opp=-,->menyarankan <pno=5 sno=l pos=NA cwd=0 ttl=0 mph=- wnc=- frq=l tfidf=3.759 opp=2.833,2.833>agar <pno=5 sno=l pos=NA cwd=l ttl=0 mph=- wnc=- frq=0 tfidf=0.000 opp=-,->Presiden <pno=5 sno=l pos=NA cwd=l ttl=0 mph=- wnc=- frq=0 tfidf=0.000 opp=-,->Clinton <pno=5 sno=l pos=NA cwd=l ttl=0 mph=- wnc=- frq=0 tfidf=0.000 opp=-,->secara <pno=5 sno=l pos=NA cwd=l ttl=0 mph=- wnc=- frq=0 tfidf=0.000 opp=-,->...continued...Figure 1 (b).
Indonesian text: words plus their attributes, after preprocessing.200We assume that a text can have many (sub)-topics, andthat the topic extraction process can be parameterized inat least two ways: first, to include more or fewer topics toproduce longer or shorter summaries, and second, toinclude only topics relating to the user's expressedinterests.Typically, topic identification can be achieved usingvarious complementary techniques, including those basedon stereotypical text structure, cue words, high-frequencyindicator phrases, and discourse structure.
Modules for allof these have been completed or are under constructionfor SUMMARIST.
In processing, each module assigns anumeric score to each sentence.
When all modules aredone, the Topic Id Integration Module combines theirscores to produce the overall ranking.
The final result isthe top-ranked n% of sentences as its final result, where nis specified by the user.3.1 Position ModuleThe Position Module is based on the well-known factthat certain genres exhibit such structural and expositionalregularity that one can reliably locate important sentencesin certain fixed positions in the text.
In early studies,Luhn (1959) and Edmundson (1968) identified severalprivileged positions, such as first and last sentences.We generalized their results (Lin and Hovy, 97),developing a method for automatically identifying thesentence positions most likely to yield good summarysentences.
The training phase of this method calculatesthe yield of each sentence position by comparing thesimilarity between human-created abstracts and thecontents of the sentence in each ordinal position in thecorresponding texts.
By summing over a large collectionof text-abstract pairs from the same corpus andappropriately normalizing, we create the Optimal PositionPolicy (OPP), a ranked list that indicates in what ordinalpositions in the text the high-topic-bearing sentences tendto occur.
We tested this method on two corpora: the Ziff-Davis texts (13,000 newspaper articles announcingcomputer products) and a set of several thousand.
WallStreet Journal newspaper articles.
For the Ziff-Daviscorpus we found the OPP to be\[T1, P2S1, P3S1, P4S1, PIS1, P2S2,{P3S2, P4S2, P5S1, P1S2}, P6S1 ....
\]i.e., the title (T1) is the most likely to bear topics,followed by the first sentence of paragraph 2, the firstsentence of paragraph 3, etc.
(Paragraph 1 is invariably ateaser sentence in this corpus.)
In contrast, for the WallStreet Journal, we found the OPP to be\[T1,P1S1,P1S2 ....\]We evaluated the OPP method in various ways.
Onemeasured coverage, the fraction of the (human-supplied)keywords that are included verbatim in the sentencesselected under the policy.
(A random selection policywould extract sentences with a random distribution oftopics; a good position policy would extract rich topic-bearing sentences.)
We measured the effectiveness of anOPP by taking cumulatively more of its sentences: firstjust the title, then the title plus P2S1, and so on.Summing together the multi-word contributions in the topten sentence positions, 10-sentence extracts (approx.
15%of a typical Ziff-Davis text) intersected with 95% of thecorresponding human abstracts.In addition to the OPP itself, we created OPP keywords,by counting the number of times each open-class wordappeared in an OPP-selected sentence, and sorting themby frequency.
Any other sentence with a high number ofthese keywords can also be rewarded with an appropriatescore.In operation, the Position Module simply selects anappropriate OPP for the input text, assigns a score to eachsentence in order of the OPP, and then computes the OPPkeyword list for the text.
It then assigns additional scoresto sentences according to how many OPP keywords theycontain.
These scores can be seen in Figure l(b), in theitem opp=x,y on each line.
The first number provides theglobal OPP score (the score of this word, summed overthe whole text) and the second score the local OPP (thescore of this sentence in the OPP).3.2 Cue Phrase ModuleIn pioneering work, Baxendale (1958) identified twosets of phrases--bonus phrases and stigma phrases--thattend to signal when a sentence is a likely candidate forinclusion in a summary and when it is definitely not acandidate, respectively.
Bonus phrases such as "insummary", in conclusion", and superlatives such as "thebest", "the most important" can be good indicators ofimportant content.
During processing, the Cue PhraseModule simply rewards each sentence containing a cuephrase with an appropriate score (constant per cue pfirase)and penalizes those containing stigma phrases.Unfortunately, cue phrases are genre dependent.
Forexample, "Abstract" and "in conclusion" are more likelyto occur in scientific literature than in newspaper articles.Given this genre-dependence, th  major problem with cuephrases is identifying them.
A natural method is toidentify high-yield sentences in texts (compared to theirhuman-made abstracts) and then to identify commonphrases in those sentences.
A careful study on theautomated collection of cue phrases is reported in (Teufeland Moens, 98).201In the context of SUMMARIST, we have tried severalmethods of acquiring cue phrases.
In one experiment, wemanually compiled a list of cue phrases from a trainingcorpus of paragraphs that themselves were summaries oftexts.
In this corpus, sentences containing phrases uch as"this paper", "this article", "this document", and "weconclude" fairly reliably reflected the major content of theparagraphs.
This indicated to us the possibility ofsummarizing a summary.In another experiment, we examined methods toautomatically generate cue phrases (Liu and Hovy, inprep.).
We examined various counting methods, all ofthem comparing the ratios of occurrence densities ofwords in summaries and in the corresponding texts invarious ways, and then extracted the words showing thehighest increase in occurrence density between text andassociated abstract.
Finally, we searched for frequentconcatenations of such privileged words into phrases.While we found no useful phrases in a corpus of 1,000newspaper articles, we found the following in 87 articleson Computational Linguistics:Method 1 Method 2$1 phrase $2 phrase11.50 multiling, natural ang.
3.304 in this paper8.500 paper presents the 2.907 this paper we7.500 paper gives 2.723 base on the6.000 paper presents 2.221 a set of6.000 now present 2.192 the result of5.199 this paper presents 2.000 the number of4.555 paper describes 1.896 in order toIn method 1, S1 = wc~, the total number of wordsoccurring in the summary that co-occur with the word win any sentence, normalized by the total number of words.In method 2, $2 = w c~ * d f /D ,  where D is the totalnumber of training documents and df is the number ofdocuments in which the word being counted appears.The Cue Phrase Module was not applied in the examplein Figure l(b), since we have not trained cue phrases forIndonesian.3.3 Topic Signature ModuleIn a straightforward application of word counting, onemight surmise that words that occur most frequently inthe text may possibly indicate important material.Naturally, one has to rule out closed-class words such as"the" and "in".
A common method is to create a list ofwords using ~.idf, a measure that rewards words for beingrelatively frequent--much more frequent in one text thanon average, across the corpus.
This method, pioneered adecade ago (Salton, 88), is used in IR systems to achievequery term expansion.The same idea can be used in topic identification.
Onthe assumption that semantically related words tend to co-occur, one can construct word families and then count thefrequency of word families instead of individual words.A frequent word family will indicate the importance of itscommon semantic notion(s) in the text.
To implementthis idea, we define a Topic Signature as a topic word (thehead) together with a list of associated (keyword weight)pairs.
Each topic signature represents a semantic onceptusing word co-occurrence patterns.
We describe inSection 4.2 how we automatically build Topic Signaturesand plan to use them for topic interpretation.For use in topic identification, we created a TopicSignature for each of five groups of 200 documents,drawn from five domains.
When performing topicidentification for a document, the Topic Id SignatureModule scanned each sentence, assigning to each wordthat occurred in a signature the weight of that keyword inthe signature.
Each sentence then received a signaturescore equal to the total of all signature word scores itcontained, normalized by sentence length.
This scoreindicated the relevance of the sentence to the signaturetopic.Since we have no signatures for Indonesian wordfamilies, no signature score appears in Figure l(b).However, the tf.idf score of each word (comparing thefrequencies of each term in the text and across acollection of Indonesian texts) appears in the item tfidf=x.3.4 Discourse Structure ModuleA new module that uses discourse structure is underconstruction for SUMMARIST.
This module, being builtby Daniel Marcu, is an extension of his Ph.D. work(Marcu, 97).
It is based on the fact that texts are notsimply flat lists of sentences; they have a hierarchicalstructure, one in which certain clauses are more importantthan others.
After parsing the hierarchical structure of aninput text and then identifying the important clauses inthis structure, Marcu discards unimportant clauses andretains only the most important ones, still bound togetherwithin the discourse structure, and hence still forming acoherent text.To produce the text's discourse structure, Marcuadapted Rhetorical Structure Theory (Mann andThompson, 88), which postulates approximately 25relations that bind clauses (or groups of clauses) togetherif they exhibit certain semantic and pragmatic properties.These relations are signaled by so-called cue phrases uchas "but" and "however" (for the relation Contrast), "inorder to" and "because" (for the relation Cause), "then"202and "next" (for Sequence), and so on.
Most relations arebinary, having a principal component ( he Nucleus) and asubsidiary one (the Satellite).
Relations can be nestedrecursively; a text is only coherent if all its clauses can belinked together, first in local subtrees and then inprogressively larger ones, under a single overarchingrelation.
Marcu uses a constraint satisfaction algorithm toassemble all the trees that legally organize the input text,and then employs everal heuristics to prefer one tree overthe others.To produce an extract summary of the input text, Marcusimply discards the least salient material, in order, bytraversing the discourse structure top-down, followingSatellite links only.Marcu's subsequent this work combines the discoursestructure paradigm with several other surface-basedmethods, including cue phrases, the discourse tree shape,title words, position, and so on (Marcu, 98a).
Using anautomated coefficient learning method, he finds that thebest linear combination of values for all these methods.Evaluation shows that the resulting extracts approximatehuman performance on both newspaper articles andScientific American texts:Extraet F-score10% (clauses) news Sei.
Am.Human 79.41% 71.27%System 68.86% 70.42%3.5 Topic Identification Integration ModuleAfter SUMMARIST applies some or all the abovemodules, each sentence has been assigned severaldifferent scores.
Some method is required to combinethese scores into a single score, so that the most importanttopic-bearing sentence can be ranked first.
However, it isnot immediately clear how the various scores should becombined for the best result.
Various approaches havebeen described in the literature.
Most of them employsome sort of combination function, in which coefficientsassign various weights to the individual scores, which arethen summed.
(Kupiec et al, 95) and (Aone et al, 97)employ the Expectation Maximization algorithm to derivecoefficients for their systems.Initially, we implemented for SUMMARIST astraightforward linear combination function, in which wespecified the coefficients manually, by experimentation.This hand tuning method is good for getting a feeling ofhow various modules can affect the SUMMARISToutput, but it does not guarantee consistent performanceover a large collection.
As we found in the formalT IPSTER-SUMMAC evaluat ion of varioussummarization systems (Firmin Hand and Sundheim, 98),the results of this function were decidedly non-optimal,and did not show the potential and power of the system!Since consistent performance and graceful degradationare very important for SUMMARIST, alternativecombination functions were needed.In subsequent work, we tested two automated methodsof creating better combination functions.
These methodsassumed that a set of texts with their ideal summaries areavailable, and that information contained in the idealsummaries can be reliably recovered from theircorresponding original texts.
The sentences in the originaltexts that were also included in the summaries we calledtarget sentences.Unfortunately, we know of no large corpus of texts withtruly ideal summaries.
Therefore, as training data, weused a portion of the results of the TIPSTER-SUMMACsummarization evaluation dry run, annotated to indicatethe relevance and popularity of each sentence, asaggregated over the ratings of several systems (Baldwin,98).
This collection contains 403 summaries containing4,830 training instances/sentences, which are judged asrelevant o TREC topics 110, 132, 138, 141, and 151.
(Note that contributions from topics are not uniform.Specific (topic/contribution) umbers are 110/226,132/122, 138/50, 141/42, and 151/63.)
Since the sentencescores are based on the consensus votes of the summariesresulting from six different experimental summarizationsystems participating in the dry run, no single sentencerelevance judgement is available to construct a truly idealsummary set.
Thus the consensus selected targetsentences hould be called 'pseudo-ideal sentences'.Please refer to (Firmin Hand and Sundheim, 98) for amore detail description of the TIPSTER-SUMMAC dryrun evaluation procedure and setup.We then employed two methods to automatically earnthe combination function(s) that identified in eachtraining text the most target sentences.The first method is a decision tree learning algorithmbased on C4.5 (Quinlan, 86).
Each module's outcome isused as a feature in the learning space.
The normalizedscore '(from 0 to 1 inclusive) of each module for eachsentence is used as its feature value.
A feature vector is asix-tuple: (TTL: vl, TF: v2, TFIDF: v3, SIG: v4, OPP:v5, QRY: v6).
TTL indicates the score from the titlemodule; TF, the term frequency module; TF1DF, the ~.idfmodule; SIG, the topic signature module; OPP, theposition module; and QRY, the query signature module.All sentences included in the ideal summary of a text arepositive xamples, all others are negative xamples.To fully utilize limited training data, we followed thestandard ecision tree training and validation procedure203and conducted a 5-way cross-validation test.
Thealgorithm generated a tree of 1,611 nodes, of which thetop (most informative) questions pertain to the querysignature, term frequency, overlap with title, and OPP.Compared with the manually built function, the decisiontree is considerably better.
With the linear combinationfunction, SUMMARIST used to score 33.02% (Recalland Precision) on an unseen test set of 82 dry-run texts.On the same data, SUMMARIST now scores 58.07%(Recall and Precision) in the 5-way cross-validation test.This represents an improvement of 25%.
It is important tounderstand that this 58.07% score should not beinterpreted as how frequently SUMMARIST producesand recovers relevant summaries.
This figure is obtainedby evaluating against every sentence contained in thepseudo-ideal summaries.
Thus it is simply a measure ofhow well SUMMARIST correctly recovers the pseudo-ideal sentences.
The final judgement has to be made byhuman analysts who judge the sentences extracted bySUMMARIST as a whole, a judgement that unfortunatelywe cannot carry out on a large scale with our limited staff.However, the figure is still a valuable performanceindicator.
No summaries can be called good summaries ifthey do not contain any summary-worthy sentences.For the second method, we followed the same setup asthe decision tree training method mentioned above butimplemented a 6-node perceptron as the learning engine.Training it on the same data produced results within 1%of the decision tree.When measuring overall summarization performance,we conjecture that the performance of SUMMARISTwith the decision tree, tested in a relevance judgementsetting such as the TIPSTER-SUMMAC evaluation,should lie somewhere in the 70% range.
As explainedabove, using the decision tree, SUMMARIST'sperformance increased by 25% (= 57% of its initialscore).
Adding this improvement to its score of 49% inthe Ad Hoc normalized best summary category (FirminHand and Sundheim, 98) places it with that range.
Welook forward to new evaluations like SUMMAC.We have recently trained a new decision tree, usingdifferent data.
The new training data derives from theQuestion and Answer summary evaluation data providedby TIPSTER-SUMMAC.
The principal differencebetween the (Baldwin, 98) data and the new Q&A data isthat the latter contains essential text fragments (phrases,clauses, and sentences) which must be included insummaries to answer some TREC topics.
These fragmentsare judged by two human judges and are thus much moreaccurate training data.
SUMMARIST trained on the Q&Adata should therefore perform better than the versiontrained on the older data.An example xtract summary of the Indonesian text inFigure 1, using the latest decision tree as combinationfunction, appears in Figure 2.
A detailed description ofthe aforementioned training experiments and theimproved combination function appears in (Lin, in prep.
).<DOC><SUMMARIZER>ISI</SUMMARIZER><TASKTYPE>qanda</TASKTYPE><SUMMARYTYPE>I5%</SUMMARYTYPE><QNUM> ??
?</QNUM><DOCNO>HTML-DOC</DOCNO><TITLE> Senator  Dar i  Demokrat  Sarankan  C l in ton  Member i  Kesaks ian  Di DepanKongres</TITLE><TEXT>Seorang senator  dari  Parta i  Demokrat  , har i  M inggu ( 20 / 9 ) , menyarankan  agarP res iden  C l in ton  secara sukare la  member ikan  kesaks ian  di hadapan Kongres  gunamenghent ikan  " s iksaan  po l i t i k  " yang memanas  Sen in  pagi  ket ika  rakyat  ASmenyaks ikan  tes t imon i  C l in ton  d i  hadapan ju r i  agung menyangkut  Mon ica  Lewinsky  .Senator  John Ker ry  , anggota  Demokrat  dar i  Massachuset ts  mengusu lkan  C l in tonbersaks i  men jawab per tanyaan Komi te  Hukum Kongres  Dengan meny iarkan  v ideokesaks ian  C l in ton  , Kongres  sebetu lnya  menghadap i  puku lan  ba l i k  ..</TEXT></DOC>Figure 2.
Summary of Indonesian text in Figure 1.2044.
Phase 2: Topic InterpretationThe second phase of summarization--going fromextract o abstract--is considerably more complex thanthe first, for it requires that the system have recourse toworld knowledge.
Without knowledge of the world, nosystem can fuse together the topics extracted to produce asmaller number of topics to form an abstract.
Yet onecannot simply ignore abstraction: extracts are notadequate for many tasks.
In one study, (Marcu, 98b)counted how many clauses had to be extracted from a textin order to fully contain all the material included in ahuman abstract of that text.
Working with a newspapercorpus of 10 texts and 14 judges, he found a compressionfactor of 2.76--in that genre, extracts are almost threetimes as long (counting words) as their correspondingabstracts!
Results of this kind indicate the need forsummarization systems to further process extractedmaterial: to remove redundancies, rephrase sentences topack material more densely, and, importantly, to merge orfuse related topics into more 'general' ones using worldknowledge.The major problem encountered in the abstractionprocess is the acquisition of such world knowledge.
Inthis section we describe two experiments performed in thecontext of SUMMARIST that investigate topicinterpretation.4.1.
Concept Counting and the WavefrontOne of the most straightforward examples of topicfusion is concept generalization:John bought some apples, pears, and oranges.---y John bought some fruit.Using a concept generalization taxonomy calledWordNet (Miller et al, 90), we have developed a methodto recognize that apple, pear, etc., can be summarized asfruit.
The idea is simple.
We first identify the WordNetequivalent of each topic word in the text, and then locatean appropriate generalization concept.To identify an appropriate generalization concept, weneed to count frequencies of occurrence of concepts (afterall, apple and pear can equally well be generalized asplant-product or physical-object).
Our algorithm (Lin 95)first counts the number of occurrences of each contentword in the text, and assigns that number to the word'sassociated concept in WordNet.
It then propagates allthese weights upward, assigning to each node the sum ofits weight plus all its childrens' weights.
Next, itproceeds back down, deciding at each node whether tostop or to continue downward.
The algorithm stops whenthe node is an appropriate generalization of its children;that is, when its weight derives so equally from two ormore of its children that no child is the clear majoritycontributor to its weight.
To find the wavefront, we definea concept's weight to be the sum of the frequency ofoccurrence of the concept C plus the weights of all itssubconcepts.
We then define the concept frequency ratiobetween aconcept and its subconcepts:R ~ MAX(sum of all children of C)SUM(sum of all children of C)This criterion selects the most specific generalization of aset of concepts as their fuser.This algorithm can be extended to identify successivelayers of fuser concepts.
As described in (Lin 95), thefirst set of fuser concepts the algorithm locates need notbe the last; one can continue downward, in order to locatemore specific generalizations.
Each stopping frontier wecall an interesting wavefront.
By repeating the wavefrontlocation process until it reaches the leaf concepts of thehierarchy, the algorithm derives a set of interestingwavefronts.
From all the interesting wavefronts, one canchoose the most general one below a certain depth D toensure a good balance of generality and specificity.
ForWordNet, we found D=6, by experimentation.To evaluate the results of this type of fusion, weselected 26 articles about new computer products fromBusinessWeek (1993-94) of average 750 words each.
Foreach text we extracted the eight sentences containing themost interesting concepts using the wavefront technique,and compared them to the contents of a professional'sabstracts of these 26 texts from an online service.
Wedeveloped several weighting and scoring variations andtried various ratio and depth parameter settings for thealgorithm.
We also implemented a random sentenceselection algorithm as a baseline comparison.The results were promising, though not startling.Average recall (R) and precision (P) values over the threescoring variations were R=0.32 and P=0.35, when thesystem produces extracts of 8 sentences.
In comparison,the random selection method scored R=O.18 and P=0.22in the same experimental setting.
These values show thatsemantic knowledge can help enable improvements overtraditional IR word-based techniques.
However, thelimitations of WordNet are serious drawbacks: it containsalmost no domain-specific knowledge.4.2 Interpretation using Topic SignaturesBefore addressing the problem of world knowledgeacquisition head-on, we decided to investigate what typeof knowledge would be useful for topic interpretation.After all, one can spend a lifetime acquiring knowledge in205just a small domain.
How little knowledge does one needto enable ffective concept fusion?Our idea, again, was simple.
We would collect a set ofwords that were typically associated with a target word,and then, during interpretation, replace the occurrence ofthe related words by the target word.
For example, wewould replace joint instances of table, menu, waiter,order, eat, pay, tip, and so on, by the single phraserestaurant-visit, n producing an indicative summary.
Wethus defined a topic signature as a family of relatedwords, as follows:TS = {head, (wl sl), (w2 s2), (w3 s~) .... }where head is the target word and each w~ is an relatedword with association strength s~.As described in (Lin 97), we constructed signaturesautomatically from a set of 30,000 texts from the 1987Wall Street Journal (WSJ) corpus.
The paper's editorshave classified each text into one of 32 classes.
Withinthe texts of each class, we counted the occurrences ofeach content word (demorphed to remove plurals, etc.
),relative to the number of times they occur in the wholecorpus, using the standard oSidfmethod.
We then selectedthe top-scoring 300 terms for each category and created asignature with the category name as its head.
The topterms of four example signatures are shown in Figure 3.It is quite easy to determine the identity of the signaturehead just by inspecting the top few signature words.In order to evaluate the quality of the signatures formedby the algorithm, we evaluated the effectiveness of eachsignature by seeing how well it served as a selectioncriterion on texts.
While this is not our intended use ofsignatures, document categorization is a well-known taskwith enough results in the literature to give us a sense ofthe performance of our methods.
As data we used a set of2,204 previously unseen WSJ news articles from 1988.For each test text, we created a single-text 'documentsignature', again using tf.idf, and then matched thisdocument signature against he category signatures.
Theclosest match provided the class into which the text wascategorized.
We tested several matching functions,including a simple binary match (count 1 if a term matchoccurs; 0 otherwise); curve-fit match (minimize thedifference in occurrence frequency of each term betweendocument and concept signatures), and cosine match(minimize the cosine angle in the hyperspace formedwhen each signature is viewed as a vector and each wordfrequency specifies the distance along the dimension forthat word).
These matching functions all providedapproximately the same results.
The values for Recalland Precision (R=0.7566 and P=0.6931) are encouragingand compare well with recent IR results (TREC, 95).Current experiments are investigating the use of contextssmaller than a full text to create more accurate signatures.RANK Aerospace  Bank ing  Env i ronment  Te lecomm.1 contract  bank  epa at&t2 a i r _ force  thr i f t  waste  network3 a i rcra f t  bank ing  env i ronmenta l  fcc4 navy loan water  cbs5 army mr, ozone cable6 space depos i t  state bel l7 miss i le  board  inc inerator  long-d is tance8 equ ipment  fs l ic  agency  te lephone9 mcdonne l l  fed c lean telecomm.i0 nor throp  ins t i tu t ion  landf i l l  mciii nasa federal  hazardous  mr.12 pentagon fdic ac id_ ra in  doct r ine13 defense vo lcker  s tandard  serv ice14 rece ive henkel  federal  news15 boe ing  banker  lake turnerFigure 3.
Portions of the topic signatures of several concepts.206These results are encouraging enough to allowus to continue with topic signatures as thevehicle for a first approximation to worldknowledge, as useful for topic interpretation.Considerable subsequent experimentation (Hovyand Junk, in prep.)
with a variety of methods,including Latent Semantic Analysis, pairwisesignatures, etc., indicates that the most promisingmethod of creating signatures i Z 2.
We are nowbusy creating a large number of signatures in anattempt to overcome the world knowledgeacquisition problem.5.
Phase 3: Summary GenerationWe have devoted no effort to adapting anexisting language generator or developing a newone for SUMMARIST.
It has scarcely beennecessary, since SUMMARIST is principally anextract-only system at present.
However, weenvisage the language generation needs ofsummarization systems in general, andSUMMARIST in particular, to require two majorsteps.The mieroplanner: The task of amicroplanner, in general, is to convert adiscourse-level specification of a sequence oftopics into a list of specifications at the level ofone or a few clauses at a time.
This involvesmaking choices for several semi-independentaspects, including sentence length, internalsentence organization (order of prepositionphrases, active or passive mood, etc.
),identification of the principal theme and focusunits, selection of the main verb and otherimportant words, and so on.
In the context ofsummarization, the microplanner's task is toensure that the information selected by the topicidentification module and (possibly) fused by .thetopic interpretation module is phrased compactlyand as briefly as possible though still in agrammatical sentence.The microplanner can be built to perform itswork at two levels: the textual evel, in which itsinput is a list of sentences or sentence fragments,and its output is a compacted list of sentences,and the representational level, in which its inputis couched in an abstract notation (whether moreor less explicitly syntactic depends on theimplementation), and its output is a fairlysyntactic abstract specification of each sentence.In the former case, the output is more or lessdirectly readable by a human, while in the latter,the output has to be converted into grammaticalsentences by the sentence generator.Microplanning is an area still largelyunexplored by computational linguists.
The workthat has been done, including some of the majorsystems (Nirenburg et al, 89; Rambow andKorelsky, 92; Hovy and Wanner, 96), is notreally appropriate to the specific compaction-related needs of summarization.
The mostrelevant study on microplanning forsummarization is (McKeown and Radev, 95).The sentence generator: The task of asentence generator (often called realizer) is toconvert a fairly detailed specification of one or afew clause-sized units into a grammaticalsentence.
A number of relatively easy to usesentence generators i available to the researchcommunity, including Penman (Penman, 88),FUF/SURGE (Elhadad, 92), RealPro (Lavoieand Rambow, 97), and NITROGEN (Langkildeand Knight, 98).
We plan to employ one or moreof these in SUMMARIST.6.
Summary Evaluation6.1 Two Basic MeasuresHow can you evaluate the quality of asummary?
We have found no literature on thisfascinating question.
Indeed, many anecdotesand experiences lead one to believe that the usesof summaries are so task-specific and user-oriented that no objective measurement ispossible.
When the inter-judge scoringvariability is higher than half the average score,as small tests relating to summaries occasionallyhave suggested, then perhaps there is no hope.However, it is possible to develop somegeneral guidelines and approaches, and fromthem to develop some approximations tosummarization evaluation.
We give a very roughsketch here of some work performed in thecontext of SUMMARIST in early 1997; moredetails are in (Hovy, in prep.
).It is obvious that to be a summary, the summarymust obey two requirements:?
it must be shorter than the original inputtext;?
it must contain (some of) the sameinformation as the original, and not other,new, information.207One can then define two measures to capture theextent to which a summary S conforms to theserequirements with regard to a text T:Compression Ratio:CR = (length S) / (length T)Retention Ratio:RR = (info in S) / (info in T)However we choose to measure the length andthe information content, we can say that a goodsummary is one in which CR is small (tending tozero) while RR is large (tending to unity).
Wecan characterize summarization systems and/ortext types by plotting the ratios of the summariesproduced under varying conditions.
ForRRexample, Figure 4(a) shows a fairly normalgrowth curve: as the summary gets longer(grows along the x axis, which measures CR), itcontains more information (grows also along they axis, which measures RR), until it is just aslong at the original text and contains the sameinformation.
In contrast, Figure 4(b) shows acurve with a very desirable bend: at some specialpoint, the addition of just a little more material tothe summary adds a disproportionately argeamount more of information.
Figure 4(c) showsanother desirable behavior: initially, all theimportant material is included in the summary;as it grows, the new material is less interesting.CRFigure 4(a).
Compression Ratio (CR) vs.
Retention Ratio (RR) for normal summary growth.RR _ jfCRFigure 4(b).
Compression Ratio (CR) vs.
Retention Ratio (RR) for desirable summary growth.RRCRFigure 4(c).
Compression Ratio (CR) vs.
Retention Ratio (RR) for desirable summary growth.208Measur ing length: Measur ing length isrelatively straightforward; one can choose asmetric the number of words, of letters, ofsentences, and so on.
For a given genre andregister level, there is a fairly fixed correlationbetween these metrics, in most cases.Measuring information content: Ideally, onewants to measure not information content, butinteresting information content only.
Theproblem is that it is very hard to measureinformation content and almost impossible todefine even what interesting information contentmay be.
However, it is possible to approximatemeasures of information content in several ways.We describe three here.The Shannon Game: This measure derivesfrom a variant of a parlor game invented byClaude Shannon, when he was developingInformation Theory (Shannon, 51).
In thistheory, the amount of information contained in amessage is measured by -p log p, where p is,roughly speaking, the probability of the readerguessing the message (or each piece thereof,individually).
To test his theory, Shannon askedhis wife and several helpers to receive and writedown a message, by guessing one letter at a time,and being answered simply yes or no.
Naturally,their knowledge of the typical letter frequenciesin English (e being most frequent, then t, a, o, i,n, and so forth), coupled with their knowledge ofEnglish words and word endings (after satisfac-the chances for -tion are very high) helped themmake informed guesses.
And as soon as theystarted to make out the meaning of thetransmitted message, their guesses got evenbetter.
Shannon's argument went as follows: themore the receiver could guess the messagewithout any help, the less novel (and hence theless informative) it was to the receiver; andcontrarily, the more guesses the receiver needed,the more informative the message was.One can use this technique to measure theinformation content of a summary S relative tothat of its corresponding text T as follows.Assemble three sets of testers.
One set firstreads T, slowly, and must then try to re-create it,letter by letter, without any longer seeing T.When they guess a wrong letter, they must guessagain; when they guess a correct one, theysimply proceed to the next letter.
The number ofwrong guesses g,rong and the total number ofguesses g,,,,,~ they make are recorded, and theirscore is computed as the ratio R r = gwrong / g,o,al.The second set of testers first reads S, slowly,and then, without any further recourse to S,proceeds to try to create T, letter by letter.
Theyknow that S is only a summary of their goal.
Aswith the first set, the number of guesses, wrongand total, are recorded, and the ratio Rsiscomputed.
The third set of testers reads nothingfirst, but starts immediately to create T, withoutknowing what it might be about.
Also for them,the ratio RN is computed.The quality of S can be computed bycomparing the three ratios, where RN and RTdefine the end points of a scale; the formerquantifies how much a tester could guess fromdefault world knowledge (and should hence notbe attributed to the summary), and the latterquantifies how much a tester still has to guess,even with "perfect' prior knowledge.
The closerRs is to Rr, the better the summary.Section 6.2 describes a small experiment.The Quest ion  Game:  This measureapproximates the information content of S bydetermining how many questions drawn up aboutT can be answered.
Before starting, one or morepeople create a set of questions based on whatthey consider the principal content of T. Thesequestions can be neutral (author's point of view)or query-oriented, as required.
Then the testersare brought in, and asked to answer thesequestions three times in succession.
In the firstround, the testers are simply asked the questionswithout having read either S or T, and theiranswers are scored.
In the second round, theyare given S to read, and are asked to answer thesame questions again.
In the third round, theyare given T, and asked to answer the questions athird time.
After each round, the number ofquestions answered correctly is tallied.The quality of S can be computed bycomparing the three tallies, where the first andlast tallies provide baselines--respectively, howmany questions the testers would be able toanswer from default world knowledge, and howmany they would not be able to answer evenwhen given the whole text T. The closer thetesters' answer tally for the summary to the tallythe full text, the better the summary.The formal SUMMAC summarizat ionevaluation (Firmin Hand and Sundheim, 98)contained a pilot test of the Question Game.
Asmall experiment is described in Section 6.2.209The Classification Game: This measureapproximates information content by testing howwell people can perform a classification task on acollection of summaries Si and on full texts Tj.
Aset of texts is drawn from a few different opics,several texts per topic.
For each text, a summaryis created.
Some testers are then asked toclassify either a text or its correspondingsummary (but not both) into one of the topicclasses; other testers are given the converse sets(a summary or its corresponding text) to classify.After classification, the correspondence b tweenthe classifications of full texts and theircorresponding summaries is measured; thegreater the agreement, he better the summary isat capturing that which causes the full text to beclassified as it is.Many variants of this game are possible.
Onemay ask the testers to classify all summaries firstand then all the full texts, as long as one takescare to test for prior classification distortion.
Or,instead of asking testers to classify texts, onemay ask them to rate the summaries and texts forrelevance to the topic classes.
In this case, thegame reduces to classification into two piles:Relevant and Not Relevant.The formal SUMMAC summarizat ionevaluation (Firmin Hand and Sundheim, 98)contained tests for two variants of  theClassification Game.6.2 Some Evaluation ExperimentsExperiment 1: In a small pilot test at ISI, weperformed a test of the Shannon Game.
Wemanual ly  created two paragraph- lengthsummaries of two 300-word newspaper articlesfor the LA Times, and had 3 groups of graduatestudents (native or near-native speakers ofEnglish) recreate the full text, as outlined above.The results were astounding: the students whohad seen the full text required on average 11additional letter guesses (i.e., they made onaverage only I 1 errors); those who had seen thesummaries required approx imate ly  150additional guesses each; and those who had seenno text at all required over 1,100 guesses eachand in some cases did not complete the test, after3 hours?
It is seldom that one finds aphenomenon yielding an order of magnitudedifference between contrast sets; we were quitepleased at the result.Experiment 2: Taking advantage of thepresence of a larger number of enthusiastic andable participants at the 1997 AAAI FallSymposium on Text Summarization, we did asecond evaluation study, focusing on thefollowing questions:1.
How do different evaluation methodscompare for each type of summary?2.
How do different summary types fareunder different evaluation methods?3.
How much does the evaluator affectscores?4.
Is there a preferred evaluation method?Preparation: We selected two newspaperarticles, one about an art theft and one about ahousing development, and created the followingtypes of summary for each domain:?
Human abstract, full background?
Human abstract, just-the-news?
SUMMARIST extract?
SUMMARIST keywords-only?
Random sentence xtractWe then created the materials for threeevaluations: a Shannon Game, a Question Game,and a Classification Game.
The Shannon andQuestion Games followed quite closely the setupoutlined above.
The Classification Game askedfor an initial classification into some verydistinct possible categories and then a sub-sequent one into some much closer categories.Execution: At the Symposium, we had theparticipants double up into pairs.
We thenconducted each evaluation with at least fourpairs.
Each pair of people performed twodifferent evaluations, seeing different exts eachtime.
The whole exercise required one session ofapprox-imately three hours.Results: Since the number of subjects was toosmall to support statistically meaningful scores,we provide in Figure 5 the relative order ofmagnitude results instead of the actual scores.The percentage difference between scorecategories is listed in the columns below thescores (for example, scores marked "1" in theShannon Game column were 50% better thanscores marked "2" in that column).Most surprising was the lack of difference inthe Classification Game: despite the difference indistinctiveness of the classification categories,the two categories were different enough to pose210no challenges.
Surprising in the Shannon Gamewas the result that the Full Text, Abstract, andSUMMARIST Extract were all equally good inproviding information content.
Finally, therelative ranking of results in the Question Gamedeserves ome discussion.
As expected, the FullText provides the most information and No Textthe least.
However, the SUMMARIST Extractprovided approx.
30% higher scores on averagethan either type of (human-made) Abstract, andthe Random sentence selection was just as goodas the Abstracts!
A larger study is required todetermine whether this result is simply a result ofthe texts selected or whether something moreunsuspected is going on.While not large enough to warrant anyconclusive statements, these evaluation trialsmake clear that different evaluation methods ratesummaries di f ferently (compare theClassification Game to the others, even thoughsome correlations can be seen between scores inthe Shannon and Question Games).
It is veryclear that different summary types give differentevaluation results, especially within the class ofExtracts.
More studies of this type will help uslearn which kinds of evaluations are good forwhich kinds of text and summary types,domains, and uses.Shannon Questions ClassificationFull Text 1 1 1AbstractExtractNo TextBackground 1 3 1Just-the-News * 3 1Regular 1 2 1Keywords 2 4 1Random * 3 I3 51-2: 50% diff.2-3: 50% diff.
*: not performed1-2: 30% diff.2-3: 20% diff.3-4: 20% diff.4-5: 100% diff.Figure 5.
Results of evaluation experiments atAAAI Spring Symposium.7.
Conc lus ionThe study of automated text summarizationstill has a long way to go before we can reallyclaim to understand the nature of summaries.That does not mean, however, that the resultsobtained over the past few years are not useful.Even fairly surface-oriented systems such asSUMMARIST and the other ones built under theTIPSTER program already exhibit somecommercial potential.
The kinds of techniquesoutlined in Sections 4 and 5, to move fromextracts to abstracts and all the other types ofsummary, will require some careful and large-scale work over the next years.
The potentialpayoff is enormous, however, and growing, asthe amount of text available in corpora and onthe web keeps growing.Several bottlenecks impede currentdevelopment of summarization systems.
Theprimary bottleneck is the shortage of goodtraining data in a variety of domains and genres,and for a variety of summary types.
What isrequired is triples--full text, human abstract, andcorresponding extract--from which it will bepossible to empirically determine answers toopen questions in all three phases ofsummarization.
A second bottleneck is theshortage of world knowledge, as required toperform topic interpretation.
A third bottleneck211is the lack of resources and funding to focus onmicroplanning in summary generation.In some sense, one might say that textsummarization is one of the most difficult tasksof natural anguage processing.
True abstractingrequires true text understanding and truegeneration; it may quite possibly prove moredifficult to find effective shortcuts than inmachine translation or information extraction.But given how directly system improvementstranslate into useful results, text summarizationis an exciting and very rewarding area in whichto work.AcknowledgementsWe thank Louke van Wensveen for very usefulinitial discussions on evaluation, Daniel Marcufor the discourse work, Hao Liu and Mike Junkfor their tireless experiments on topic signatures,Th6r~se Firmin Hand, Sara Shelton, and BethSundheim for discussions about evaluation, andespecial ly Sara Shelton for continuedencouragement.ReferencesAone, C., M.E.
Okurowski, J. Gorlinsky, B.Larsen.
1998.
A Scalable SummarizationSystem using Robust NLP.
In I. Mani and M.Maybury (eds), Advances in Automated TextSummarization.
MIT Press.Bagga, A. and B. Baldwin.
1998.
Entity-BasedCross-Document Cross-Referencing using theVector Space Model.
In Proceedings ofCOLING/ACL, 79-86.
Montreal, Canada.Baldwin, B.
1998.
Rework ing ofTIPSTER/SUMMAC Dry Run EvaluationResults.
University of Pennsylvania Report.Baxendale, P.B.
1958.
Machine-Made Index forTechnical Literature--An Experiment.
IBMJournal (October) 354-361.Brill, E. 1992.
A Corpus-Based Approach toLanguage Learning.
Ph.D. dissertation,University of Pennsylvania.Edmundson, H.P.
1968.
New Methods inAutomatic Extraction.
Journal of the ACM16(2), 264-285.Elhadad, M. 1992.
Using Argumentation toControl Lexical Choice: A FunctionalUnification-Based Approach.
Ph.D.dissertation, Columbia University.Endres-Niggemeyer, B.
1997.
SimSum:Simulation of Summatizing.
In Proceedingsof the Workshop on Intelligent ScalableSummarization at the ACL/EACL Conference,89-96.
Madrid, Spain.Firmin Hand, T. and B. Sundheim.
1998.TIPSTER-SUMMAC S u m m a r i z a t i o nEvaluation.
Proceedings of the TIPSTER TextPhase 111 Workshop.
Washington.Hovy, E.H. and U Wanner.
1996.
ManagingSentence Planning Requirements.
InProceedings of the Workshop on Gaps andBridges in NL Planning and Generation,53-58.
ECAI Conference.
Budapest,Hungary.Hovy, E.H. 1998.
Combining and StandardizingLarge-Scale, Practical Ontologies for MachineTranslation and Other Uses.
In Proceedingsof the First International Conference onLanguage Resources and Evaluation (LREC),535-542.
Granada, Spain.Hovy, E.H. and M. Junk.
In prep.Hovy, E.H. and C-Y.
Lin.
1998.
AutomatingText Summarization i SUMMARIST.
In I.Mani and M. Maybury (eds), Advances inAutomated Text Summarization.
MIT Press.Hovy, E.H.
The Evaluation of Summaries.
Inprep.Knight, K. and S.K.
Luk.
1994.
Building aLarge-Scale Knowledge Base for MachineTranslation.
Proceedings of the Conference ofthe American Association of ArtificialIntelligence (AAAI-94), 773-778.
Seattle,WA.Kupiec, J., J. Pedersen, and F. Chen.
1995.
ATrainable Document Summarizer.
InProceedings of the Eighteenth AnnualInternational ACM Conference on Researchand Development in Information Retrieval(SIGIR), 68-73.
Seattle, WA.Langkilde, I. and K. Knight.
1998.
Generationthat Exploits Corpus Knowledge.
InProceedings of the COLING/A CL Conference,704-709.
Montreal, Canada.Lavoie, B. and O. Rambow.
1997.
A Fast andPortable Realizer for Text Generation212Systems.
In Proceedings of the AppliedNatural Language Processing Conference(ANLP-97), 265-270.
Washington, DC.Lin, C-Y.
1995.
Topic Identification by ConceptGeneralization.
In Proceedings of the Thirty-third Conference of the Association ofComputational Linguistics (ACL-95),308-310.
Boston, MA.Lin, C-Y.
1997.
Robust Automated TopicIdentification.
Ph.D. dissertation, Universityof Southern California.Lin, C-Y.
and E.H. Hovy.
1997.
IdentifyingTopics by Position.
In Proceedings of theApplied Natural Language ProcessingConference (ANLP-97), 283-290.Washington, DC.Lin, C-Y.
and E.H. Hovy.
1998.
Automatic TextCategorization: A Concept-Based Approach.In prep.Lin, C-Y.
Training a Selection Function forExtraction in SUMMARIST.
In prep.Liu, H. and E.H. Hovy.
Automated Learning ofCue Phrases for Text Summarization.
I  prep.Luhn, H.P.
1959.
The Automatic Creation ofLiterature Abstracts.
IBM Journal of Researchand Development, 159-165.Mann, W.C. and S.A. Thompson.
1988.Rhetorical Structure Theory: Toward aFunctional Theory of Text Organization.
Text8(3) (243-281).Marcu, D. 1997.
The Rhetorical Parsing,Summarization, and Generation of NaturalLanguage Texts.
Ph.D. dissertation,University of Toronto.Marcu, D. 1998.
The Automatic Construction ofLarge-scale Corpora for SummarizationResearch.
Forthcoming.Marcu, D. 1998a.
Improving Summarizationthrough Rhetorical Parsing Tuning.Proceedings of the COLING-ACL Workshopon Very Large Corpora.
Montreal, Canada.Marcu, D. 1998b.
Automated Creation ofExtracts from Texts and Abstracts.
In prep.McKeown, K.R.
and D.R.
Radev.
1995.Generating Summaries of Multiple NewsArticles.
In Proceedings of the EighteenthAnnual International ACM Conference onResearch and Development in InformationRetrieval (SIGIR), 74-82.
Seattle, WA.Miller, G., R. Beckwith, C. Fellbaum, D. Gross,and K. Miller.
1990.
Five papers on WordNet.CSL Report 43, Cognitive ScienceLaboratory, Princeton University.Mitra, M., A. Singhal, and C. Buckley.
1997.Automatic Text Summarization by ParagraphExtraction.
In Proceedings of the Workshopon Intelligent Scalable Summarization at theACL/EACL Conference, 39-46.
Madrid,Spain.Nirenburg, S., V. Lesser, and E. Nyberg.
1989.Controlling a Language Generation Planner.In Proceedings of IJCAI, 1524-1530.
Detroit,MI.The Penman Primer, User Guide, and ReferenceManual.
1988.
Unpublished ocumentation,Information Sciences Institute, University ofSouthern California.Quinlan, J.R. 1986.
Induction of Decision Trees.Machine Learning 81-106.Rambow, O. and T. Korelsky.
1992.
AppliedText Generation.
In Proceedings of theApplied Natural Language ProcessingConference (ANLP).
Trento, ItalySalton, G. 1988.
Automatic Text Processing.Reading, MA: Addison-Wesley.Shannon, C. 1951.
Prediction and Entropy ofPrinted English.
Bell System TechnicalJournal, January 1951.Sp/irck Jones, K. 1998.
Introduction to TextSummarisation.
In I. Mani and M. Maybury(eds), Advances in Automated TextSummarization.
MIT Press.Strzalkowski, T. et al, 1998. ?
In I. Mani andM.
Maybury (eds), Advances in AutomatedText Summarization.
MIT Press.Tait, J.I.
and K. Sparck Jones.
1982.
AutomaticSummarising of English Texts.
Ph.D.dissertation, Cambridge University.Teufel, S. and M. Moens.
1998.
SentenceExtraction as a Classification Task.
In I. Maniand M. Maybury (eds), Advances inAutomated Text Summarization.
MIT Press.TREC.
Harman, D. (ed).
1995.
Proceedings ofthe TREC Conference.213Van Dijk, T.A.
and W. Kintsch.
1983.
Strategiesof Discourse Comprehension.
New York:Academic Press.214
