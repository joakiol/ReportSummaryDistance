Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2319?2324,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsWord Ordering Without SyntaxAllen Schmaltz and Alexander M. Rush and Stuart M. ShieberHarvard University{schmaltz@fas,srush@seas,shieber@seas}.harvard.eduAbstractRecent work on word ordering has argued thatsyntactic structure is important, or even re-quired, for effectively recovering the order ofa sentence.
We find that, in fact, an n-gramlanguage model with a simple heuristic givesstrong results on this task.
Furthermore, weshow that a long short-term memory (LSTM)language model is even more effective at re-covering order, with our basic model outper-forming a state-of-the-art syntactic model by11.5 BLEU points.
Additional data and largerbeams yield further gains, at the expense oftraining and search time.1 IntroductionWe address the task of recovering the original wordorder of a shuffled sentence, referred to as bag gen-eration (Brown et al, 1990), shake-and-bake genera-tion (Brew, 1992), or more recently, linearization, asstandardized in a recent line of research as a methoduseful for isolating the performance of text-to-textgeneration models (Zhang and Clark, 2011; Liu etal., 2015; Liu and Zhang, 2015; Zhang and Clark,2015).
The predominant argument of the more re-cent works is that jointly recovering explicit syn-tactic structure is crucial for determining the correctword order of the original sentence.
As such, thesemethods either generate or rely on given parse struc-ture to reproduce the order.Independently, Elman (1990) explored lineariza-tion in his seminal work on recurrent neural net-works.
Elman judged the capacity of early recurrentneural networks via, in part, the network?s ability topredict word order in simple sentences.
He notes,The order of words in sentences reflects a num-ber of constraints.
.
.
Syntactic structure, selectiverestrictions, subcategorization, and discourse con-siderations are among the many factors whichjoin together to fix the order in which words oc-cur.
.
.
[T]here is an abstract structure which un-derlies the surface strings and it is this structurewhich provides a more insightful basis for under-standing the constraints on word order.
.
.
.
It is,therefore, an interesting question to ask whether anetwork can learn any aspects of that underlyingabstract structure (Elman, 1990).Recently, recurrent neural networks havereemerged as a powerful tool for learning the latentstructure of language.
In particular, work on longshort-term memory (LSTM) networks for languagemodeling has provided improvements in perplexity.We revisit Elman?s question by applying LSTMsto the word-ordering task, without any explicit syn-tactic modeling.
We find that language models arein general effective for linearization relative to ex-isting syntactic approaches, with LSTMs in particu-lar outperforming the state-of-the-art by 11.5 BLEUpoints, with further gains observed when trainingwith additional text and decoding with larger beams.2 Background: LinearizationThe task of linearization is to recover the original or-der of a shuffled sentence.
We assume a vocabularyV and are given a sequence of out-of-order phrasesx1, .
.
.
, xN , with xn ?
V+ for 1 ?
n ?
N .
DefineM as the total number of tokens (i.e., the sum of thelengths of the phrases).
We consider two varieties ofthe task: (1) WORDS, where each xn consists of asingle word and M = N , and (2) WORDS+BNPS,2319where base noun phrases (noun phrases not con-taining inner noun phrases) are also provided andM ?
N .
The second has become a standard for-mulation in recent literature.Given input x, we define the output set Y to beall possible permutations over the N elements of x,where y?
?
Y is the permutation generating the trueorder.
We aim to find y?, or a permutation close toit.
We produce a linearization by (approximately)optimizing a learned scoring function f over the setof permutations, y?
= arg maxy?Y f(x, y).3 Related Work: Syntactic LinearizationRecent approaches to linearization have been basedon reconstructing the syntactic structure to producethe word order.
Let Z represent all projective de-pendency parse trees over M words.
The objectiveis to find y?, z?
= arg maxy?Y,z?Z f(x, y, z) wheref is now over both the syntactic structure and the lin-earization.
The current state of the art on the PennTreebank (PTB) (Marcus et al, 1993), without ex-ternal data, of Liu et al (2015) uses a transition-based parser with beam search to construct a sen-tence and a parse tree.
The scoring function is alinear model f(x, y) = ?>?
(x, y, z) and is trainedwith an early update structured perceptron to matchboth a given order and syntactic tree.
The featurefunction ?
includes features on the syntactic tree.This work improves upon past work which usedbest-first search over a similar objective (Zhang andClark, 2011).In follow-up work, Liu and Zhang (2015) arguethat syntactic models yield improvements over puresurface n-gram models for the WORDS+BNPS case.This result holds particularly on longer sentencesand even when the syntactic trees used in trainingare of low quality.
The n-gram decoder of this workutilizes a single beam, discarding the probabilitiesof internal, non-boundary words in the BNPs whencomparing hypotheses.
We revisit this comparisonbetween syntactic models and surface-level models,utilizing a surface-level decoder with heuristic fu-ture costs and an alternative approach for scoringpartial hypotheses for the WORDS+BNPS case.Additional previous work has also explored n-gram models for the word ordering task.
The workof de Gispert et al (2014) demonstrates improve-ments over the earlier syntactic model of Zhang et al(2012) by applying an n-gram language model overthe space of word permutations restricted to concate-nations of phrases seen in a large corpus.
Horvat andByrne (2014) models the search for the highest prob-ability permutation of words under an n-gram modelas a Travelling Salesman Problem; however, directcomparisons to existing works are not provided.4 LM-Based LinearizationIn contrast to the recent syntax-based approaches,we use an LM directly for word ordering.
Weconsider two types of language models: an n-gram model and a long short-term memory network(Hochreiter and Schmidhuber, 1997).
For the pur-pose of this work, we define a common abstractionfor both models.
Let h ?
H be the current state ofthe model, with h0 as the initial state.
Upon seeinga word wi ?
V , the LM advances to a new statehi = ?(wi,hi?1).
At any time, the LM can bequeried to produce an estimate of the probability ofthe next word q(wi,hi?1) ?
p(wi | w1, .
.
.
, wi?1).For n-gram language models, H, ?, and q can natu-rally be defined respectively as the state space, tran-sition model, and edge costs of a finite-state ma-chine.LSTMs are a type of recurrent neural network(RNN) that are conducive to learning long-distancedependencies through the use of an internal memorycell.
Existing work with LSTMs has generated state-of-the-art results in language modeling (Zaremba etal., 2014), along with a variety of other NLP tasks.In our notation we define H as the hidden statesand cell states of a multi-layer LSTM, ?
as theLSTM update function, and q as a final affine trans-formation and softmax given as q(?,hi?1; ?q) =softmax(Wh(L)i?1 + b) where h(L)i?1 is the top hid-den layer and ?q = (W , b) are parameters.
We di-rect readers to the work of Graves (2013) for a fulldescription of the LSTM update.For both models, we simply define the scoringfunction asf(x, y) =N?n=1log p(xy(n) | xy(1), .
.
.
, xy(n?1))where the phrase probabilities are calculated word-by-word by our language model.2320Algorithm 1 LM beam-search word ordering1: procedure ORDER(x1 .
.
.
xN , K, g)2: B0 ?
?(?
?, {1, .
.
.
, N}, 0,h0)?3: form = 0, .
.
.
,M ?
1 do4: for k = 1, .
.
.
, |Bm| do5: (y,R, s,h)?
B(k)m6: for i ?
R do7: (s?,h?)?
(s,h)8: for word w in phrase xi do9: s?
?
s?
+ log q(w,h?
)10: h?
?
?(w,h?
)11: j ?
m+ |xi|12: Bj ?
Bj + (y + xi,R?
i, s?,h?
)13: keep top-K of Bj by f(x, y) + g(R)14: return BMSearching over all permutations Y is intractable,so we instead follow past work on linearization (Liuet al, 2015) and LSTM generation (Sutskever etal., 2014) in adapting beam search for our genera-tion step.
Our work differs from the beam searchapproach for the WORDS+BNPS case of previ-ous work in that we maintain multiple beams, asin stack decoding for phrase-based machine trans-lation (Koehn, 2010), allowing us to incorporatethe probabilities of internal, non-boundary wordsin the BNPs.
Additionally, for both WORDS andWORDS+BNPS, we also include an estimate of fu-ture cost in order to improve search accuracy.Beam search maintains M + 1 beams,B0, .
.
.
, BM , each containing at most the top-K partial hypotheses of that length.
A partialhypothesis is a 4-tuple (y,R, s,h), where y is apartial ordering,R is the set of remaining indices tobe ordered, s is the score of the partial linearizationf(x, y), and h is the current LM state.
Each stepconsists of expanding all next possible phrases andadding the next hypothesis to a later beam.
The fullbeam search is given in Algorithm 1.As part of the beam search scoring function wealso include a future cost g, an estimate of the scorecontribution of the remaining elements in R. To-gether, f(x, y) + g(R) gives a noisy estimate of thetotal score, which is used to determine the K bestelements in the beam.
In our experiments we use avery simple unigram future cost estimate, g(R) =?i?R?w?xi log p(w).Model WORDS WORDS+BNPSZGEN-64 30.9 49.4ZGEN-64+POS ?
50.8NGRAM-64 (NO g) 32.0 51.3NGRAM-64 37.0 54.3NGRAM-512 38.6 55.6LSTM-64 40.5 60.9LSTM-512 42.7 63.2ZGEN-64+LM+GW+POS ?
52.4LSTM-64+GW 41.1 63.1LSTM-512+GW 44.5 65.8Table 1: BLEU score comparison on the PTB testset.
Results from previous works (for ZGEN) arethose provided by the respective authors, except forthe WORDS task.
The final number in the modelidentifier is the beam size, +GW indicates additionalGigaword data.
Models marked with +POS are pro-vided with a POS dictionary derived from the PTBtraining set.5 ExperimentsSetup Experiments are on PTB with sections 2-21 as training, 22 as validation, and 23 as test1.We utilize two UNK types, one for initial upper-case tokens and one for all other low-frequency to-kens; end sentence tokens; and start/end tokens,which are treated as words, to mark BNPs for theWORDS+BNPS task.
We also use a special symbolto replace tokens that contain at least one numericcharacter.
We otherwise train with punctuation andthe original case of each token, resulting in a vocab-ulary containing around 16K types from around 1Mtraining tokens.For experiments marked GW we augment thePTB with a subset of the Annotated Gigaword cor-pus (Napoles et al, 2012).
We follow Liu andZhang (2015) and train on a sample of 900k AgenceFrance-Presse sentences combined with the full PTBtraining set.
The GW models benefit from both ad-ditional data and a larger vocabulary of around 25Ktypes, which reduces unknowns in the validation andtest sets.We compare the models of Liu et al (2015)1In practice, the results in Liu et al (2015) and Liu andZhang (2015) use section 0 instead of 22 for validation (authorcorrespondence).2321BNP g GW 1 10 64 128 256 512LSTM?
41.7 53.6 58.0 59.1 60.0 60.6?
?
47.6 59.4 62.2 62.9 63.6 64.3?
?
?
48.4 60.1 64.2 64.9 65.6 66.215.4 26.8 33.8 35.3 36.5 38.0?
25.0 36.8 40.7 41.7 42.0 42.5?
?
23.8 35.5 40.7 41.7 42.9 43.7NGRAM?
40.6 49.7 52.6 53.2 54.0 54.7?
?
45.7 53.6 55.6 56.2 56.6 56.614.6 27.1 32.6 33.8 35.1 35.8?
27.1 34.6 37.5 38.1 38.4 38.7Table 2: BLEU results on the validation set forthe LSTM and NGRAM model with varying beamsizes, future costs, additional data, and use of basenoun phrases.
(known as ZGEN), a 5-gram LM using Kneser-Neysmoothing (NGRAM)2, and an LSTM.
We experi-ment on the WORDS and WORDS+BNPS tasks, andwe also experiment with including future costs (g),the Gigaword data (GW), and varying beam size.We retrain ZGEN using publicly available code3 toreplicate published results.The LSTM model is similar in size and architec-ture to the medium LSTM setup of Zaremba et al(2014)4.
Our implementation uses the Torch5 frame-work and is publicly available6.We compare the performance of the models usingthe BLEU metric (Papineni et al, 2002).
In gener-ation if there are multiple tokens of identical UNKtype, we randomly replace each with possible un-used tokens in the original source before calculatingBLEU.
For comparison purposes, we use the BLEUscript distributed with the publicly available ZGENcode.Results Our main results are shown in Table 1.On the WORDS+BNPS task the NGRAM-64 modelscores nearly 5 points higher than the syntax-basedmodel ZGEN-64.
The LSTM-64 then surpasses2We use the KenLM Language Model Toolkit (https://kheafield.com/code/kenlm/).3https://github.com/SUTDNLP/ZGen4We hypothesize that additional gains are possible via alarger model and model averaging, ceteris paribus.5http://torch.ch6https://github.com/allenschmaltz/word_ordering5 10 15 20 25 30 35 40Sentence length30507090BLEU(%)LSTM-512LSTM-64ZGen-64LSTM-10.0 0.2 0.4 0.6 0.8 1.0Distortion rate04000800012000TokensZGen-64LSTM-64Figure 1: Experiments on the PTB validation on theWORDS+BNPS models: (a) Performance on the setby length on sentences from length 2 to length 40.
(b) The distribution of token distortion, binned at 0.1intervals.NGRAM-64 by more than 5 BLEU points.
Differ-ences on the WORDS task are smaller, but show asimilar pattern.
Incorporating Gigaword further in-creases the result another 2 points.
Notably, theNGRAM model outperforms the combined resultof ZGEN-64+LM+GW+POS from Liu and Zhang(2015), which uses a 4-gram model trained on Gi-gaword.
We believe this is because the combinedZGEN model incorporates the n-gram scores as dis-cretized indicator features instead of using the prob-ability directly.7 A beam of 512 yields a further im-provement at the cost of search time.To further explore the impact of search accuracy,Table 2 shows the results of various models withbeam widths ranging from 1 (greedy search) to 512,and also with and without future costs g. We see thatfor the better models there is a steady increase in ac-curacy even with large beams, indicating that searcherrors are made even with relatively large beams.7In work of Liu and Zhang (2015), with the given decoder,N-grams only yielded a small further improvement over the syn-tactic models when discretized versions of the LM probabilitieswere incorporated as indicator features in the syntactic models.2322Model WORDS WORDS+BNPSZGEN-64(z?)
39.7 64.9ZGEN-64 40.8 65.2NGRAM-64 46.1 67.0NGRAM-512 47.2 67.8LSTM-64 51.3 71.9LSTM-512 52.8 73.1Table 3: Unlabeled attachment scores (UAS) onthe PTB validation set after parsing and aligningthe output.
For ZGEN we also include a result us-ing the tree z?
produced directly by the system.For WORDS+BNPS, internal BNP arcs are alwayscounted as correct.One proposed advantage of syntax in lineariza-tion models is that it can better capture long-distancerelationships.
Figure 1 shows results by sentencelength and distortion, which is defined as the abso-lute difference between a token?s index position iny?
and y?, normalized by M .
The LSTM model ex-hibits consistently better performance than existingsyntax models across sentence lengths and generatesfewer long-range distortions than the ZGEN model.Finally, Table 3 compares the syntactic fluencyof the output.
As a lightweight test, we parse theoutput with the Yara Parser (Rasooli and Tetreault,2015) and compare the unlabeled attachment scores(UAS) to the trees produced by the syntactic system.We first align the gold head to each output token.
(Incases where the alignment is not one-to-one, we ran-domly sample among the possibilities.)
The modelswith no knowledge of syntax are able to recover ahigher proportion of gold arcs.6 ConclusionStrong surface-level language models recover wordorder more accurately than the models trained withexplicit syntactic annotations appearing in a recentseries of papers.
This has implications for the utilityof costly syntactic annotations in generation mod-els, for both high- and low- resource languages anddomains.AcknowledgmentsWe thank Yue Zhang and Jiangming Liu for assis-tance in using ZGen, as well as verification of thetask setup for a valid comparison.
Jiangming Liualso assisted in pointing out a discrepancy in the im-plementation of an earlier version of our NGRAMdecoder, the resolution of which improved BLEUperformance.References[Brew1992] Chris Brew.
1992.
Letting the cat out of thebag: Generation for shake-and-bake MT.
In Proceed-ings of the 14th Conference on Computational Lin-guistics - Volume 2, COLING ?92, pages 610?616,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.
[Brown et al1990] Peter F Brown, John Cocke, StephenA Della Pietra, Vincent J Della Pietra, Fredrick Je-linek, John D Lafferty, Robert L Mercer, and Paul SRoossin.
1990.
A statistical approach to machinetranslation.
Computational linguistics, 16(2):79?85.
[de Gispert et al2014] Adria` de Gispert, Marcus Tomalin,and Bill Byrne.
2014.
Word ordering with phrase-based grammars.
In Proceedings of the 14th Confer-ence of the European Chapter of the Association forComputational Linguistics, pages 259?268, Gothen-burg, Sweden, April.
Association for ComputationalLinguistics.
[Elman1990] Jeffrey L. Elman.
1990.
Finding structurein time.
Cognitive Science, 14(2):179 ?
211.
[Graves2013] Alex Graves.
2013.
Generating se-quences with recurrent neural networks.
CoRR,abs/1308.0850.
[Hochreiter and Schmidhuber1997] Sepp Hochreiter andJu?rgen Schmidhuber.
1997.
Long short-term memory.Neural Comput., 9(8):1735?1780, November.
[Horvat and Byrne2014] Matic Horvat and WilliamByrne.
2014.
A graph-based approach to stringregeneration.
In Proceedings of the Student ResearchWorkshop at the 14th Conference of the EuropeanChapter of the Association for Computational Lin-guistics, pages 85?95, Gothenburg, Sweden, April.Association for Computational Linguistics.
[Koehn2010] Philipp Koehn.
2010.
Statistical MachineTranslation.
Cambridge University Press, New York,NY, USA, 1st edition.
[Liu and Zhang2015] Jiangming Liu and Yue Zhang.2015.
An empirical comparison between n-gram andsyntactic language models for word ordering.
In Pro-ceedings of the 2015 Conference on Empirical Meth-ods in Natural Language Processing, pages 369?378,Lisbon, Portugal, September.
Association for Compu-tational Linguistics.2323[Liu et al2015] Yijia Liu, Yue Zhang, Wanxiang Che, andBing Qin.
2015.
Transition-based syntactic lineariza-tion.
In Proceedings of the 2015 Conference of theNorth American Chapter of the Association for Com-putational Linguistics: Human Language Technolo-gies, pages 113?122, Denver, Colorado, May?June.Association for Computational Linguistics.
[Marcus et al1993] Mitchell P. Marcus, Beatrice San-torini, and Mary Ann Marcinkiewicz.
1993.
Buildinga large annotated corpus of english: The penn tree-bank.
Computational Linguistics, 19(2):313?330.
[Napoles et al2012] Courtney Napoles, Matthew Gorm-ley, and Benjamin Van Durme.
2012.
Annotated gi-gaword.
In Proceedings of the Joint Workshop on Au-tomatic Knowledge Base Construction and Web-scaleKnowledge Extraction, AKBC-WEKEX ?12, pages95?100, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.
[Papineni et al2002] Kishore Papineni, Salim Roukos,Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: Amethod for automatic evaluation of machine transla-tion.
In Proceedings of the 40th Annual Meeting onAssociation for Computational Linguistics, ACL ?02,pages 311?318, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.
[Rasooli and Tetreault2015] Mohammad Sadegh Rasooliand Joel R. Tetreault.
2015.
Yara parser: A fast andaccurate dependency parser.
CoRR, abs/1503.06733.
[Sutskever et al2014] Ilya Sutskever, Oriol Vinyals, andQuoc VV Le.
2014.
Sequence to sequence learningwith neural networks.
In Advances in Neural Informa-tion Processing Systems, pages 3104?3112.
[Zaremba et al2014] Wojciech Zaremba, Ilya Sutskever,and Oriol Vinyals.
2014.
Recurrent neural networkregularization.
CoRR, abs/1409.2329.
[Zhang and Clark2011] Yue Zhang and Stephen Clark.2011.
Syntax-based grammaticality improvement us-ing ccg and guided search.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, EMNLP ?11, pages 1147?1157, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.
[Zhang and Clark2015] Yue Zhang and Stephen Clark.2015.
Discriminative syntax-based word ordering fortext generation.
Comput.
Linguist., 41(3):503?538,September.
[Zhang et al2012] Yue Zhang, Graeme Blackwood, andStephen Clark.
2012.
Syntax-based word ordering in-corporating a large-scale language model.
In Proceed-ings of the 13th Conference of the European Chap-ter of the Association for Computational Linguistics,EACL ?12, pages 736?746, Stroudsburg, PA, USA.Association for Computational Linguistics.2324
