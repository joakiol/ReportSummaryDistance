Workshop on TextGraphs, at HLT-NAACL 2006, pages 33?36,New York City, June 2006. c?2006 Association for Computational LinguisticsSimilarity between Pairs of Co-indexed Treesfor Textual Entailment RecognitionFabio Massimo ZanzottoDISCoUniversity Of Milan-BicoccaMilano, Italyzanzotto@disco.unimib.itAlessandro MoschittiDISPUniversity Of Rome ?Tor Vergata?Roma, Italymoschitti@info.uniroma2.itAbstractIn this paper we present a novel similaritybetween pairs of co-indexed trees to auto-matically learn textual entailment classi-fiers.
We defined a kernel function basedon this similarity along with a more clas-sical intra-pair similarity.
Experimentsshow an improvement of 4.4 absolute per-cent points over state-of-the-art methods.1 IntroductionRecently, a remarkable interest has been devoted totextual entailment recognition (Dagan et al, 2005).The task requires to determine whether or not a textT entails a hypothesis H .
As it is a binary classifica-tion task, it could seem simple to use machine learn-ing algorithms to learn an entailment classifier fromtraining examples.
Unfortunately, this is not.
Thelearner should capture the similarities between dif-ferent pairs, (T ?,H ?)
and (T ?
?,H ??
), taking into ac-count the relations between sentences within a pair.For example, having these two learning pairs:T1 ?
H1T1 ?At the end of the year, all solid compa-nies pay dividends?H1 ?At the end of the year, all solidinsurance companies pay dividends.
?T1 ; H2T1 ?At the end of the year, all solid compa-nies pay dividends?H2 ?At the end of the year, all solid compa-nies pay cash dividends.
?determining whether or not the following implica-tion holds:T3 ?
H3?T3 ?All wild animals eat plants that havescientifically proven medicinal proper-ties.
?H3 ?All wild mountain animals eat plantsthat have scientifically proven medici-nal properties.
?requires to detect that:1.
T3 is structurally (and somehow lexically) sim-ilar to T1 and H3 is more similar to H1 than toH2;2. relations between the sentences in the pairs(T3,H3) (e.g., T3 and H3 have the same noungoverning the subject of the main sentence) aresimilar to the relations between sentences in thepairs (T1,H1) and (T1,H2).Given this analysis we may derive that T3 ?
H3.The example suggests that graph matching tec-niques are not sufficient as these may only detectthe structural similarity between sentences of textualentailment pairs.
An extension is needed to consideralso if two pairs show compatible relations betweentheir sentences.In this paper, we propose to observe textual entail-ment pairs as pairs of syntactic trees with co-indexednodes.
This shuold help to cosider both the struc-tural similarity between syntactic tree pairs and thesimilarity between relations among sentences withina pair.
Then, we use this cross-pair similarity withmore traditional intra-pair similarities (e.g., (Corleyand Mihalcea, 2005)) to define a novel kernel func-tion.
We experimented with such kernel using Sup-port Vector Machines on the Recognizing TextualEntailment (RTE) challenge test-beds.
The compar-ative results show that (a) we have designed an ef-fective way to automatically learn entailment rules33from examples and (b) our approach is highly accu-rate and exceeds the accuracy of the current state-of-the-art models.In the remainder of this paper, Sec.
2 introducesthe cross-pair similarity and Sec.
3 shows the exper-imental results.2 Learning Textual Entailment fromexamplesTo carry out automatic learning from exam-ples, we need to define a cross-pair similarityK((T ?,H ?
), (T ?
?,H ??)).
This function should con-sider pairs similar when: (1) texts and hypothesesare structurally and lexically similar (structural sim-ilarity); (2) the relations between the sentences inthe pair (T ?,H ?)
are compatible with the relationsin (T ?
?,H ??)
(intra-pair word movement compatibil-ity).
We argue that such requirements could be metby augmenting syntactic trees with placeholders thatco-index related words within pairs.
We will thendefine a cross-pair similarity over these pairs of co-indexed trees.2.1 Training examples as pairs of co-indexedtreesSentence pairs selected as possible sentences in en-tailment are naturally co-indexed.
Many words (orexpressions) wh in H have a referent wt in T .
Thesepairs (wt, wh) are called anchors.
Possibly, it ismore important that the two words in an anchor arerelated than the actual two words.
The entailmentcould hold even if the two words are substitued withtwo other related words.
To indicate this we co-index words associating placeholders with anchors.For example, in Fig.
1, 2?
indicates the (compa-nies,companies) anchor between T1 and H1.
Theseplaceholders are then used to augment tree nodes.
Tobetter take into account argument movements, place-holders are propagated in the syntactic trees follow-ing constituent heads (see Fig.
1).In line with many other researches (e.g., (Cor-ley and Mihalcea, 2005)), we determine these an-chors using different similarity or relatedness dec-tors: the exact matching between tokens or lemmas,a similarity between tokens based on their edit dis-tance, the derivationally related form relation andthe verb entailment relation in WordNet, and, fi-nally, a WordNet-based similarity (Jiang and Con-rath, 1997).
Each of these detectors gives a differentweight to the anchor: the actual computed similarityfor the last and 1 for all the others.
These weightswill be used in the final kernel.2.2 Similarity between pairs of co-indexedtreesPairs of syntactic trees where nodes are co-indexedwith placeholders allow the design a cross-pair simi-larity that considers both the structural similarity andthe intra-pair word movement compatibility.Syntactic trees of texts and hypotheses permit toverify the structural similarity between pairs of sen-tences.
Texts should have similar structures as wellas hypotheses.
In Fig.
1, the overlapping subtreesare in bold.
For example, T1 and T3 share the sub-tree starting with S ?
NP VP.
Although the lexicalsin T3 and H3 are quite different from those T1 andH1, their bold subtrees are more similar to those ofT1 and H1 than to T1 and H2, respectively.
H1 andH3 share the production NP ?
DT JJ NN NNS whileH2 and H3 do not.
To decide on the entailment for(T3,H3), we can use the value of (T1,H1).Anchors and placeholders are useful to verify iftwo pairs can be aligned as showing compatibleintra-pair word movement.
For example, (T1,H1)and (T3,H3) show compatible constituent move-ments given that the dashed lines connecting place-holders of the two pairs indicates structurally equiv-alent nodes both in the texts and the hypotheses.
Thedashed line between 3 and b links the main verbsboth in the texts T1 and T3 and in the hypotheses H1and H3.
After substituting 3 to b and 2 to a , T1and T3 share the subtree S ?
NP 2 VP 3 .
The samesubtree is shared between H1 and H3.
This impliesthat words in the pair (T1,H1) are correlated likewords in (T3,H3).
Any different mapping betweenthe two anchor sets would not have this property.Using the structural similarity, the placeholders,and the connection between placeholders, the over-all similarity is then defined as follows.
Let A?
andA??
be the placeholders of (T ?,H ?)
and (T ?
?,H ??),respectively.
The similarity between two co-indexedsyntactic tree pairs Ks((T ?,H ?
), (T ?
?,H ??))
is de-fined using a classical similarity between two treesKT (t1, t2) when the best alignment between the A?and A??
is given.
Let C be the set of all bijective34T1 T3SPPINAtNP 0NP 0DTtheNN 0end0PPINofNP 1DTtheNN 1year1,,NP 2DTallJJ 2solid2?NNS 2companies2?VP 3VBP 3pay3NP 4NNS 4dividends4SNP aDTAllJJ awilda?NNS aanimalsa?VP bVBP beatbNP cplantsc ... propertiesH1 H3SPPINAtNP 0NP 0DTtheNN 0end0PPINofNP 1DTtheNN 1year1,,NP 2DTallJJ 2solid2?NNinsuranceNNS 2companies2?VP 3VBP 3pay3NP 4NNS 4dividends4SNP aDTAllJJ awilda?NNmountainNNS aanimalsa?VP bVBP beatbNP cplantsc ... propertiesH2 H3SPPAt ... yearNP 2DTallJJ 2solid2?NNS 2companies2?VP 3VBP 3pay3NP 4NNcashNNS 4dividends4SNP aDTAllJJ awilda?NNmountainNNS aanimalsa?VP bVBP beatbNP cplantsc ... propertiesFigure 1: Relations between (T1,H1), (T1,H2), and (T3,H3).mappings from a?
?
A?
: |a?| = |A?
?| to A?
?, anelement c ?
C is a substitution function.
The co-indexed tree pair similarity is then defined as:Ks((T ?, H ?
), (T ?
?,H ??))
=maxc?C(KT (t(H ?, c), t(H ?
?, i)) +KT (t(T ?, c), t(T ?
?, i))where (1) t(S, c) returns the syntactic tree of thehypothesis (text) S with placeholders replaced bymeans of the substitution c, (2) i is the identity sub-stitution and (3) KT (t1, t2) is a function that mea-sures the similarity between the two trees t1 and t2.2.3 Enhancing cross-pair syntactic similarityAs the computation cost of the similarity measuredepends on the number of the possible sets of corre-spondences C and this depends on the size of theanchor sets, we reduce the number of placehold-ers used to represent the anchors.
Placeholders willhave the same name if these are in the same chunkboth in the text and the hypothesis, e.g., the place-holders 2?
and 2?
are collapsed to 2 .3 Experimental investigationThe aim of the experiments is twofold: we show that(a) entailments can be learned from examples and(b) our kernel function over syntactic structures iseffective to derive syntactic properties.
The abovegoals can be achieved by comparing our cross-pairsimilarity kernel against (and in combination with)other methods.3.1 Experimented kernelsWe compared three different kernels: (1) the ker-nel Kl((T ?,H ?
), (T ?
?,H ??))
based on the intra-pair35Datasets Kl Kl +Kt Kl +KsTrain:D1 Test:T1 0.5888 0.6213 0.6300Train:T1 Test:D1 0.5644 0.5732 0.5838Train:D2(50%)?
Test:D2(50%)??
0.6083 0.6156 0.6350Train:D2(50%)??
Test:D2(50%)?
0.6272 0.5861 0.6607Train:D2 Test:T2 0.6038 0.6238 0.6388Mean 0.5985 0.6040 0.6297(?
0.0235 ) (?
0.0229 ) (?
0.0282 )Table 1: Experimental resultslexical similarity siml(T,H) as defined in (Cor-ley and Mihalcea, 2005).
This kernel is de-fined as Kl((T ?,H ?
), (T ?
?,H ??))
= siml(T ?,H ?)
?siml(T ?
?,H ??).
(2) the kernel Kl+Ks that combinesour kernel with the lexical-similarity-based kernel;(3) the kernel Kl + Kt that combines the lexical-similarity-based kernel with a basic tree kernel.This latter is defined as Kt((T ?,H ?
), (T ?
?,H ??))
=KT (T ?, T ??
)+KT (H ?,H ??).
We implemented thesekernels within SVM-light (Joachims, 1999).3.2 Experimental settingsFor the experiments, we used the Recognizing Tex-tual Entailment (RTE) Challenge data sets, whichwe name as D1, T1 and D2, T2, are the develop-ment and the test sets of the first and second RTEchallenges, respectively.
D1 contains 567 exampleswhereas T1, D2 and T2 have all the same size, i.e.800 instances.
The positive examples are the 50%of the data.
We produced also a random split of D2.The two folds are D2(50%)?
and D2(50%)?
?.We also used the following resources: the Char-niak parser (Charniak, 2000) to carry out the syntac-tic analysis; the wn::similarity package (Ped-ersen et al, 2004) to compute the Jiang&Conrath(J&C) distance (Jiang and Conrath, 1997) needed toimplement the lexical similarity siml(T,H) as de-fined in (Corley and Mihalcea, 2005); SVM-light-TK (Moschitti, 2004) to encode the basic tree kernelfunction, KT , in SVM-light (Joachims, 1999).3.3 Results and analysisTable 1 reports the accuracy of different similar-ity kernels on the different training and test split de-scribed in the previous section.
The table showssome important result.First, as observed in (Corley and Mihalcea, 2005)the lexical-based distance kernel Kl shows an accu-racy significantly higher than the random baseline,i.e.
50%.
This accuracy (second line) is comparablewith the best systems in the first RTE challenge (Da-gan et al, 2005).
The accuracy reported for the bestsystems, i.e.
58.6% (Glickman et al, 2005; Bayeret al, 2005), is not significantly far from the resultobtained with Kl, i.e.
58.88%.Second, our approach (last column) is signifi-cantly better than all the other methods as it pro-vides the best result for each combination of train-ing and test sets.
On the ?Train:D1-Test:T1?
test-bed, it exceeds the accuracy of the current state-of-the-art models (Glickman et al, 2005; Bayer et al,2005) by about 4.4 absolute percent points (63% vs.58.6%) and 4% over our best lexical similarity mea-sure.
By comparing the average on all datasets, oursystem improves on all the methods by at least 3 ab-solute percent points.Finally, the accuracy produced by our kernelbased on co-indexed trees Kl + Ks is higher thanthe one obtained with the plain syntactic tree ker-nel Kl + Kt.
Thus, the use of placeholders and co-indexing is fundamental to automatically learn en-tailments from examples.ReferencesSamuel Bayer, John Burger, Lisa Ferro, John Henderson, andAlexander Yeh.
2005.
MITRE?s submissions to the eu pas-cal rte challenge.
In Proceedings of the 1st Pascal ChallengeWorkshop, Southampton, UK.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.In Proc.
of the 1st NAACL, pages 132?139, Seattle, Wash-ington.Courtney Corley and Rada Mihalcea.
2005.
Measuring the se-mantic similarity of texts.
In Proc.
of the ACL Workshopon Empirical Modeling of Semantic Equivalence and Entail-ment, pages 13?18, Ann Arbor, Michigan, June.
Associationfor Computational Linguistics.Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005.
ThePASCAL RTE challenge.
In PASCAL Challenges Workshop,Southampton, U.K.Oren Glickman, Ido Dagan, and Moshe Koppel.
2005.
Webbased probabilistic textual entailment.
In Proceedings of the1st Pascal Challenge Workshop, Southampton, UK.Jay J. Jiang and David W. Conrath.
1997.
Semantic similaritybased on corpus statistics and lexical taxonomy.
In Proc.
ofthe 10th ROCLING, pages 132?139, Tapei, Taiwan.Thorsten Joachims.
1999.
Making large-scale svm learningpractical.
In B. Schlkopf, C. Burges, and A. Smola, editors,Advances in Kernel Methods-Support Vector Learning.
MITPress.Alessandro Moschitti.
2004.
A study on convolution kernelsfor shallow semantic parsing.
In proceedings of the ACL,Barcelona, Spain.Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi.2004.
Wordnet::similarity - measuring the relatedness ofconcepts.
In Proc.
of 5th NAACL, Boston, MA.36
