Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 851?856,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsA Corpus Level MIRA Tuning Strategy for Machine TranslationMing Tan, Tian Xia, Shaojun WangWright State University3640 Colonel Glenn Hwy,Dayton, OH 45435 USA{tan.6, xia.7, shaojun.wang}@wright.eduBowen ZhouIBM T.J. Watson Research Center1101 Kitchawan Rd,Yorktown Heights, NY 10598 USAzhou@us.ibm.comAbstractMIRA based tuning methods have beenwidely used in statistical machine translation(SMT) system with a large number of fea-tures.
Since the corpus-level BLEU is not de-composable, these MIRA approaches usuallydefine a variety of heuristic-driven sentence-level BLEUs in their model losses.
Instead,we present a new MIRA method, which em-ploys an exact corpus-level BLEU to com-pute the model loss.
Our method is simpler inimplementation.
Experiments on Chinese-to-English translation show its effectiveness overtwo state-of-the-art MIRA implementations.1 IntroductionMargin infused relaxed algorithm (MIRA) has beenwidely adopted for the parameter optimization inSMT with a large feature size (Watanabe et al 2007;Chiang et al 2008; Chiang et al 2009; Chiang,2012; Eidelman, 2012; Cherry and Foster, 2012).Since BLEU is defined on the corpus, and not de-composed into sentences, most MIRA approachesconsider a variety of sentence-level BLEUs for themodel losses, many of which are heuristic-driven(Watanabe et al 2007; Chiang et al 2008; Chi-ang et al 2009; Chiang, 2012; Cherry and Foster,2012).
The sentence-level BLEU appearing in theobjective is generally based on a pseudo-document,which may not precisely reflect the corpus-levelBLEU.
We believe that this mismatch could poten-tially harm the performance.
To avoid the sentenceBLEU, the work in (Haddow et al 2011) proposedto process sentences in small batches.
The authorsadopted a Gibbs sampling (Arun et al 2009) tech-nique to search the hope and fear hypotheses, andthey did not compare with MIRA.
Watanabe (2012)also tuned the parameters with small batches of sen-tences and optimized a hinge loss not explicitly re-lated to BLEU using stochastic gradient descent.Both approaches introduced additional complexitiesover baseline MIRA approaches.In contrast, we propose a remarkably simple butefficient batch MIRA approach which exploits theexact corpus-level BLEU to compute model losses.We search for a hope and a fear hypotheses for thecorpus with a straightforward approach and mini-mize the structured hinge loss defined on them.
Theexperiments show that our method consistently out-performs two state-of-the-art MIRAs in Chinese-to-English translation tasks with a moderate margin.2 Margin Infused Relaxed AlgorithmWe optimize the model parameters based on N-bestlists.
Our development (dev) set is a set of triples{(fi, ei , ri)}Mi=1, where fi is a source-language sen-tence, corresponded by a list of target-language hy-potheses ei = {eij}N(fi)j=1 , with a number of refer-ences ri.
h(e ij ) is a feature vector.
Generally, mostdecoders return a top-1 candidate as the transla-tion result, such that e?i(w) = arg maxj w ?
h(eij ),where w are the model parameters.
In this paper, weaim at optimizing the BLEU score (Papineni et al2002).MIRA is an instance of online learning which as-sumes an overlap of the decoding procedure and theparameter optimization procedure.
For example in(Crammer et al 2006; Chiang et al 2008), MIRA851is performed after an input sentence are decoded,and the next sentence is decoded with the updatedparameters.
The objective for each sentence i is,minw12||w ?w?||2 + C ?
li(w) (1)li(w) = maxeij{b(e?i )?
b(eij)?w ?
[h(e?i )?
h(eij )]} (2)where e?i ?
ei is a hope candidate, w?
is the pa-rameter vector from the last sentence.
Since MIRAdefines its objective only based on the current sen-tence, b(?)
is a sentence-level BLEU.Most MIRA algorithms need a deliberate defini-tion of b(?
), since BLEU cannot be decomposed intosentences.
The types of the sentence BLEU calcula-tion includes: (a) a smoothed version of BLEU foreij (Liang et al 2006), (b) fit eij into a pseudo-document considering the history (Chiang et al2008; Chiang, 2012), (c) use eij to replace the corre-sponding hypothesis in the oracles (Watanabe et al2007).
The sentence-level BLEU sometimes per-plexes the algorithms and results in a mismatch withthe corpus-level BLEU.3 Corpus-level MIRA3.1 AlgorithmWe propose a batch tuning strategy, corpus-levelMIRA (c-MIRA), in which an objective is not builtupon a hinge loss of a single sentence, but upon thatof the entire corpus.The online MIRAs are difficult to parallelize.Therefore, similar to the batch MIRA in (Cherry andFoster, 2012), we conduct the batch tuning by re-peating the following steps: (a) Decode source sen-tences (in parallel) and obtain {ei}Mi=1, (b) Merge{ei}Mi=1 with the one from the previous iteration, (c)Invoke Algorithm 1.We define E = (eE,1 , eE,2 , ..., eE,M ) as a corpushypothesis, with H (E) =1MM?i=1h(eE,i).
eE,i is thehypothesis of the source sentence fi covered by E .E is corresponded to a corpus-level BLEU, whichwe ultimately want to optimize.
Following MIRAformulated in (Crammer et al 2006; Chiang et al2008), c-MIRA repeatedly optimizes,minw12||w ?w?||2 + C ?
lcorpus(w) (3)lcorpus(w) = maxE{B(E?)?
B(E)?w ?
[H(E?
)?H(E)]} (4)where B(?)
is a corpus-level BLEU.
E?
is a hopehypothesis.
E ?
L, where L is the hypothesis spaceof the entire corpus, and |L| = |e1| ?
?
?
|eM|.Algorithm 1 Corpus-Level MIRARequire: {(fi, ei , ri)}Mi=1, w0, C1: for t = 1 ?
?
?T do2: E?
= {} ,E ?
= {} .
Initialize the hope and fear3: for i = 1 ?
?
?M do4: eE?,i = arg maxeij[wt?1 ?
h(eij) + b?
(eij )]5: eE?,i = arg maxeij[wt?1 ?
h(eij)?
b?
(eij )]6: E?
?
E?
+ {eE?,i} .
Build the hope7: E ?
?
E ?
+ {eE?,i} .
Build the fear8: end for9: 4B = B(E?)?
B(E ?)
.
the BLEU difference10: 4H = H(E ?)?H(E?)
.
the feature difference11: ?
= min[C, 4B+wt?1?4H||4H||2]12: wt = wt?1 ?
?
?
4H13: w?t =1t+ 1t?t=0wt14: end for15: return w?t with the optimal BLEU on the dev set.c-MIRA can be regarded as a standard MIRA,in which there is only one single triple (F ,L,R),where F and R are the source and reference ofthe corpus respectively.
Eq.
3 is equivalent to aquadratic programming with |L| constraints.
Cram-mer et al(2006) show that a single constraint withone hope E?
and one fear E ?
admits a closed-formupdate and performs well.
We denote one executionof the outer loop as an epoch.
The hope and fearare updated in each epoch.
Similar to (Chiang et al2008), the hope and fear hypotheses are defined asfollowing,E?
= maxE[w ?H(E) + B(E)] (5)E ?
= maxE[w ?H(E)?
B(E)] (6)Eq.
5 and 6 find the hypotheses with the best andworse BLEU that the decoder can easily achieve.
Itis unnecessary to search the entire space of L forprecise solution E?and E ?, because MIRA only at-852tempts to separate the hope from the fear by a mar-gin proportional to their BLEU differentials (Cherryand Foster, 2012).
We just construct E?and E ?
re-spectively by,eE?,i = maxei,j[w ?
h(ei,j) + b?
(ei,j)]eE ?,i = maxei,j[w ?
h(ei,j)?
b?
(ei,j)]where b?
is simply a BLEU with add one smoothing(Lin and Och, 2004).
A smoothed BLEU is goodenough to pick up a ?satisfying?
pair of hope andfear.
However, the updating step (Line 11) uses thecorpus-level BLEU.3.2 Justificationc-MIRA treats a corpus as one sentence for decod-ing, while conventional decoders process sentencesone by one.
We show the optimal solutions from thetwo methods are equivalent theoretically.We follow the notations in (Och and Ney,2002).
We search a hypothesis on corpus E ={e1 ,k1 , e2 ,k2 , ..., eM ,kM } with the highest probabil-ity given the source corpus F = {f1, f2, ..., fM},E = arg maxElogP (E|F)= arg maxE(w ?M?i=1h(ei,ki )?M?i=1log(Zi))(7)= {arg maxei,kiw ?
h(ei,ki )}Mi=1 (8)where Zi =?N(fi)j=1 exp(w ?
h(ei ,j )), which is aconstant with respective to E .
Eq.
7 shows thatthe feature vector of E is determined by the sum ofeach candidate?s feature vectors.
Also, the modelscore can be decomposed into each sentence in Eq.8, which shows that decoding all sentences togetherequals to decoding one by one.We also show that if the metric is decomposable,the loss in c-MIRA is actually the sum of the hingeloss li(w) in structural SVM (Tsochantaridis et al2004; Cherry and Foster, 2012).
We assume B(eij)to be the metric of a sentence hypothesis, then theloss of c-MIRA in Eq.
4 is,lcorpus(w) ?
maxE?M?i=1[B(ei,kE?
)?B(ei,kE?
)?w?h(ei,kE? )
+ w ?
h(ei,kE?
)]=M?i=1maxeij[B(ei,kE?
)?B(eij)?w?h(ei,kE? )
+ w ?
h(eij)] =M?i=1li(w)Instead of adopting a cutting-plane algorithm(Tsochantaridis et al 2004), we optimize the sameloss with a MIRA pattern in a simpler way.
How-ever, since BLEU is not decomposable, the struc-tural SVM (Cherry and Foster, 2012) uses an inter-polated sentence BLEU (Liang et al 2006).
Al-though Algorithm 1 has an outlook similar to thebatch-MIRA algorithm in (Cherry and Foster, 2012),their loss definitions differ fundamentally.
BatchMIRA basically uses a sentence-level loss, and theyalso follow the sentence-by-sentence tuning pattern.In the future work, we will compare structural SVMand c-MIRA under decomposable metrics like WERor SSER (Och and Ney, 2002).4 Experiments and AnalysisWe first evaluate c-MIRA in a iterative batch tuningprocedure in a Chinese-to-English machine transla-tion system with 228 features.
Second, we show c-MIRA is also effective in the re-ranking task withmore than 50,000 features.In both experiments, we compare c-MIRA andthree baselines: (1) MERT (Och, 2003), (2) Chianget als MIRA (MIRA1) in (Chiang et al 2008).
(3)batch-MIRA (MIRA2) in (Cherry and Foster, 2012).Here, we roughly choose C with the best BLEU ondev set, from {0.1, 0.01, 0.001, 0.0001, 0.00001}.We convert Chiang et als MIRA to the batch modedescribed in section 3.1.
So the only difference be-tween MIRA1 and MIRA2 is: MIRA1 obtains mul-tiple constraints before optimization, while MIRA2only uses one constraint.
We implement MERTand MIRA1, and directly use MIRA2 from Moses(Koehn et al 2007).
We conduct experiments in aserver of 8-cores with 2.5GHz Opteron.
We set themaximum number of epochs as we generally do notobserve an obvious increase on the dev set BLEU.853MERT MIRA1 MIRA2 c-MIRAC 0.0001 0.001 0.00018 dev 34.80 34.70 34.73 34.70feat.
04 31.92 31.81 31.73 31.8305 28.85 28.94 28.71 28.92C 0.001 0.001 0.001all dev 34.61 35.24 35.14 35.56feat.
04 31.76 32.25 32.04 32.57+05 28.85 29.43 29.37 29.4106news 30.91 31.43 31.24 31.82+06others 27.43 28.01 28.13 28.4508news 25.62 26.11 26.03 26.4008others 16.22 16.66 16.46 17.10+Table 1: BLEUs (%) on the dev and test sets with 8 densefeatures only and all features.
The significant symbols (+at 0.05 level) are compared with MIRA2The epoch size for MIRA1 and MIRA2 is 40, whilethe one for c-MIRA is 400. c-MIRA runs moreepochs, because we update the parameters by muchfewer times.
However, we can implement Line 3?8in Algorithm 1 in multi-thread (we use eight threadsin the following experiments), which makes our al-gorithm much faster.
Also, we increase the epochsizes of MIRA1 and MIRA2 to 400, and find thereis no improvement on their performance.4.1 Iterative Batch TrainingIn this experiment, we conduct the batch tuning pro-cedure shown in section 3.
We align the FBIS dataincluding about 230K sentence pairs with GIZA++for extracting grammar, and train a 4-gram languagemodel on the Xinhua portion of Gigaword corpus.
Ahierarchical phrase-based model (Chiang, 2007) istuned on NIST MT 2002, which has 878 sentences,and tested on MT 2004, 2005, 2006, and 2008.
Allfeatures used here, besides eight basic ones in (Chi-ang, 2007), consists of an extra 220 group features.We design such feature templates to group gram-mar by the length of source side and target side,(feat type, a ?
src side ?
b, c ?
tgt side ?
d) ,where feat type denotes any of relative frequency,reversed relative frequency, lexical probability andreversed lexical probability, and [a, b], [c, d] enumer-ate all possible subranges of [1, 10], as the maximumMERT MIRA1 MIRA2 c-MIRAR.
T. 25.8min 16.0min 7.3min 7.8minTable 2: Running time.length on each side of a hierarchical grammar is lim-ited to 10.
There are 4?
55 extra group features.
Wealso set the size of N-best list per sentence beforemerge as 200.All methods use 30 decoding iterations.
We se-lect the iteration with the best BLEU of the dev setfor testing.
We present the BLEU scores in Table 1on two feature settings: (1) 8 basic features only, and(2) all 228 features.
In the first case, due to the smallfeature size, MERT can get a better BLEU of thedev set, and all MIRA algorithms fails to generallybeat MERT on the test set.
However, as the featuresize increase to 228, MERT degrades on the dev-setBLEU, and also become worse on test sets, whileMIRA algorithms improve on the dev set expect-edly.
MIRA1 performs better than MIRA2, proba-bly because of more constraints.
c-MIRA can mod-erately improve BLEU by 0.2?0.4 from MIRA1and 0.2?0.6 from MIRA2.
This might indicate thata loss defined on corpus is more accurate than theone defined on sentence.
Table 2 lists the runningtime.
Only MIRA2 is fairly faster than c-MIRA be-cause of more epochs in c-MIRA.4.2 Re-ranking ExperimentsThe baseline system is a state-of-the-art hierarchi-cal phrase-based system, and trained on six millionparallel sentences corpora available to the DARPABOLT Chinese-English task.
This system includes51 dense features (including translation probabili-ties, provenance features, etc.)
and about 50k sparsefeatures (mostly lexical and fertility-based).
Thelanguage model is a six-gram model trained on a10 billion words monolingual corpus, including theEnglish side of our parallel corpora plus other cor-pora such as Gigaword (LDC2011T07) and GoogleNews.
We use 1275 sentences for tuning and 1239sentences for testing from the LDC2010E30 corpusrespectively.
There are four reference translationsfor each input sentence in both tuning and testingdatasets.We use a N-best list which is an intermediate out-854MIRA1 MIRA2 c-MIRAdense dev 31.90 31.78 32.00only test 30.89 30.89 31.07dense dev 32.29 32.20 32.49+sparse test 31.12 31.00 31.39Table 3: BLEUs (%) on re-ranking experiments.MIRA1 MIRA2 c-MIRAabout 1,966,720 35,120 400Table 4: Times of updating model parameters.put of the baseline system optimized on TER-BLEUinstead of BLEU.
Before the re-ranking task, the ini-tial BLEUs of the top-1 hypotheses on the tuningand testing set are 31.45 and 30.56.
The averagenumbers of hypotheses per sentence are about 200and 500, respectively for the tuning and testing sets.Again, we use the best epoch on the tuning set fortesting.
The BLEUs on dev and test sets are reportedin Table 3.
We observe that the effectiveness of c-MIRA is not harmed as the feature size is scaled up.4.3 AnalysisTo examine the simple search for hopes and fears(Line 3?8 in Alg.
1), we use two hope/fear buildingstrategies to get E?
and E ?
: (1) simply connect eache?i and e?i in Line 4?5 of Algorithm 1, (2) conduct aslow beam search among the N-best lists of all for-eign sentences from e1 to eM and use Eq.
5 and6 to prune the stack.
The stack size is 10.
We ob-serve that there is no significant difference betweenthe two strategies on the BLEU of the dev set.
Butthe second strategy is about 10 times slower.We also consider more constraints in Eq.
3.
Bybeam search, we obtain one corpus-level oracle and29 other hypotheses similar to (Chiang et al 2008),and optimize with SMO (Platt, 1998).
Unfortu-nately, experiments show that more constraints leadto an overfitting and no improved performance.As shown in Table 4, in one execution, ourmethod updates the parameters by only 400 times;MIRA2 updates by 40 ?
878 = 35120 times; andMIRA1 updates much more (about 1,966,720 times)due to the SMO procedure.
We are surprised to findc-MIRA gets a higher training BLEU with such fewparameter updates.
This probably suggests that thereis a gap between sentence-level BLEU and corpus-level BLEU, so standard MIRAs need to update theparameters more often.Regarding simplicity, MIRA1 uses a strongly-heuristic definition of a sentence BLEU, andMIRA2 needs a pseudo-document with a decay rateof ?
= 0.9.
In comparison, c-MIRA avoids boththe sentence level BLEU and the pseudo-document,thus needs fewer variables.5 ConclusionWe present a simple and effective MIRA batch tun-ing algorithm without the heuristic-driven calcula-tion of sentence-level BLEU, due to the indecom-posability of a corpus-level BLEU.
Our optimiza-tion objective is directly defined on the corpus-levelhypotheses.
This work simplifies the tuning pro-cess, and avoid the mismatch between the sentence-level BLEU and the corpus-level BLEU.
This strat-egy can be potentially applied to other optimiza-tion paradigms, such as the structural SVM (Cherryand Foster, 2012), SGD and AROW (Chiang, 2012),and other forms of samples, such as forests (Chiang,2012) and lattice (Cherry and Foster, 2012).6 AcknowledgmentsThe key idea and a part of the experimental workof this paper were developed in collaboration withthe IBM researcher when the first author was an in-tern at IBM T.J. Watson Research Center.
This re-search is partially supported by Air Force Office ofScientific Research under grant FA9550-10-1-0335,the National Science Foundation under grant IIS RI-small 1218863 and a Google research award.ReferencesA.
Arun, C. Dyer, B. Haddow, P. Blunsom, A. Lopez,and P. Koehn.
2009.
Monte Carlo inference and maxi-mization for phrase-based translation.
In Proceedingsof the Thirteenth Conference on Computational Natu-ral Language Learning (CoNLL), 102-110.C.
Cherry and G. Foster.
2012.
Batch tuning strategiesfor statistical machine translation.
Conference of theNorth American Chapter of the Association for Com-putational Linguistics: Human Language Technolo-gies (NAACL-HLT), 427-436.855D.
Chiang.
2012.
Hope and fear for discriminative train-ing of statistical translation models.
Journal of Ma-chine Learning Research (JMLR), 1159-1187.D.
Chiang, K. Knight, and W. Wang.
2009.
11,001 newfeatures for statistical machine translation.
Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics: Human LanguageTechnologies (NAACL-HLT), 218-226.D.
Chiang, Y. Marton, and P. Resnik.
2008.
Online large-margin training of syntactic and structural translationfeatures.
In Proc.
of Conference on Empirical Meth-ods in Natural Language Processing (EMNLP), 224-233.D.
Chiang.
2007.
Hierarchical phrase-based translation.Computational Linguistics, 33(2):201-228.K.
Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,and Y.
Singer.
2006.
Online passive-aggressive al-gorithms.
Journal of Machine Learning Research(JMLR), 7:551-585.V.
Eidelman.
2012.
Optimization strategies for onlinelarge-margin learning in machine translation.
Pro-ceedings of the Seventh Workshop on Statistical Ma-chine Translation, 480-489.B.
Haddow, A. Arun, and P. Koehn.
2011.
SampleRanktraining for phrase-based machine translation.
Pro-ceedings of the Sixth Workshop on Statistical MachineTranslation.
Association for Computational Linguis-tics, 261-271.P.
Koehn, H. Hoang, A. Birch, C. Burch, M. Federico, N.Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C.Dyer, O. Bojar, A. Constantin, and E. Herbst.
2007.Moses: Open source toolkit for statistical machinetranslation.
Proceedings of the Annual Meeting ofthe Association for Computational Linguistics (ACL),177-180.P.
Liang, A. Bouchard-Cote, D. Klein, and B. Taskar.2006.
An end-to-end discriminative approach to ma-chine translation.
In Proceedings of the 21st Interna-tional Conference on Computational Linguistics andthe 44th annual meeting of the Association for Com-putational Linguistics, 761-768.C.
Lin and F. Och.
2004.
Orange: a method for evaluat-ing automatic evaluation metrics for machine transla-tion.
In Proc.
of International Conference on Compu-tational Linguistics (COLING), No.
501.F.
Och.
2003.
Minimum error rate training in statisticalmachine translation.
Proceedings of the 41st AnnualMeeting on Association for Computational Linguistics(ACL), 160-167.F.
Och and H. Ney.
2002.
Discriminative trainingand maximum entropy models for statistical machinetranslation.
Proceedings of the 40th Annual Meetingon Association for Computational Linguistics (ACL),295-302.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.BLEU: a method for automatic evaluation of machinetranslation.
Proceedings of the 40th annual meeting onassociation for computational linguistics.
Associationfor Computational Linguistics (ACL), 311-318.J.
Platt.
1998.
Sequetial minimal optimization: A fast al-gorithm for training support vector machines.
In Tech-nical Report MST-TR-98-14.
Microsoft Research.I.
Tsochantaridis, T. Hofman, T. Joachims, and Y. Altun.2004.
Support vector machine learning for interde-pendent and structured output spaces.
InternationalConference on Machine Learning (ICML), 823-830.T.
Watanabe.
2012.
Optimized online rank learning formachine translation.
Proceedings of Conference of theNorth American Chapter of the Association for Com-putational Linguistics: Human Language Technolo-gies (NAACL-HLT), 253-262.T.
Watanabe, J. Suzuki, H. Tsukada, and H. Isozaki.2007.
Online large-margin training for statistical ma-chine translation.
Proceedings of Conference on Em-pirical Methods in Natural Language Processing andComputational Natural Language Learning (EMNLP-CoNLL), 764-773.856
