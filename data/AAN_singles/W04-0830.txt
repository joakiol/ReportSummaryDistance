The University of Ja?n Word Sense Disambiguation System*Manuel Garc?a-VegaUniversidad de Ja?nAv.
Madrid 35Ja?n, Spain, 23071mgarcia@ujaen.esMiguel A. Garc?a-CumbrerasUniversidad de Ja?nJa?n, Spain, 23071Av.
Madrid 35, 23071magc@ujaen.esM.
Teresa Mart?n-ValdiviaUniversidad de Ja?nAv.
Madrid 35Ja?n, Spain, 23071maite@ujaen.esL.
Alfonso Ure?a-L?pezUniversidad de Ja?nAv.
Madrid 35Ja?n, Spain, 23071laurena@ujaen.es*This paper has been partially supported by the Spanish Government (MCYT) Project number TIC2003-07158-C04-04AbstractThis paper describes the architecture and re-sults of the University of Ja?n system pre-sented at the SENSEVAL-3 for the English-lexical-sample and English-All-Words tasks.The system is based on a neural network ap-proach.
We have used the Learning VectorQuantization, which is a supervised learningalgorithm based on the Kohonen neural model.1 IntroductionOur system for SENSEVAL-3 uses a supervisedlearning algorithm for word sense disambiguation.The method suggested trains a neural network us-ing the Learning Vector Quantization (LVQ) algo-rithm, integrating several semantic relations ofWordNet (Fellbaum, 1998) and SemCor corpus(Miller et al, 1993).
The University of Ja?n systemhas been used in English-lexical-sample and Eng-lish-All-Words tasks.2 Experimental EnvironmentThe presented disambiguator uses the VectorSpace Model (VSM) as an information representa-tion model.
Each sense of a word is represented asa vector in an n-dimensional space where n is thenumber of words in all its contexts.The accuracy of the disambiguator depends es-sentially on the word weights.
We use the LVQalgorithm to adjust them.
The input vector weightsare calculated as shown by (Salton and McGill,1983) with the standard tf?idf, where the documentsare the paragraphs.
They are presented to the LVQnetwork and, after training, the output vectors(called prototype or codebook vectors) are ob-tained, containing the adjusted weights for allsenses of each word.Any word to disambiguate is represented with avector in the same way.
This representation mustbe compared with all the trained word sense vec-tors by applying the cosine similarity rule:ikikiksimxwxwxw??
),( =  [1]The sense corresponding to the vector of highestsimilarity is selected as the disambiguated sense.To train the neural network we have integratedsemantic information from two linguistic re-sources: SemCor corpus and WordNet lexical da-tabase.2.1 SemCorFirstly, the SemCor (the Brown Corpus labeledwith the WordNet senses) was fully used (theBrown-1, Brown-2 and Brown-v partitions).
Weused the paragraph as a contextual semantic unitAssociation for Computational Linguisticsfor the Semantic Analysis of Text, Barcelona, Spain, July 2004SENSEVAL-3: Third International Workshop on the Evaluation of Systemsand each context was included in the training vec-tor set.The SENSEVAL-3 English tasks have used theWordNet 1.7.1 sense inventory, but the SemCor istagged with an earlier version of WordNet (spe-cifically WordNet version 1.6).Figure 1.
SemCor context for ?climb?.Therefore it was necessary to update the SemCorword senses.
We have used the automaticallymapped version of Semcor with the WordNet 1.7.1senses found in WordNet site1.Figure 2.
WordNet artificial paragraph1http://www.cogsci.princeton.edu/~wn/Figure 1 shows the common format for the allthe resource input paragraphs.
For each word, thepos and sense are described, e.g.
?climb\2#1?
is theverb ?climb?
with sense 1.
In addition, it has 158different words in its context and all of them areshown like the pair word-frequency.2.2 WordNet'Semantic relations from WordNet 1.7.1 wereconsidered, in particular synonymy, antonymy,hyponymy, homonymy, hyperonymy, meronymy,and coordinate terms to generate artificial para-graphs with words along each relation.For example, for a word with 7 senses, 7 artifi-cial paragraphs with the synonyms of the 7 senseswere added, 7 more with all its hyponyms, and soon.Figure 2 shows these artificial paragraphs for the?climb?
verb.3 Learning Vector QuantizationThe LVQ algorithm (Kohonen, 1995) performssupervised learning, which uses a set of inputs withtheir correctly annotated outputs adjusting themodel when an error is committed between themodel outputs and the known outputs.The LVQ algorithm is a classification methodbased on neural competitive learning, which allowsthe definition of a group of categories on the inputdata space by reinforced learning, either positive(reward) or negative (punishment).
In competitivelearning, the output neurons compete to becomeactive.
Only a single output neuron is active at anyone time.The general application of LVQ is to adjust theweights of labels to high dimensional input vec-tors, which is technically done by representing thelabels as regions of the data space, associated withadjustable prototype or codebook vectors.
Thus, acodebook vector, wk, is associated for each class,k.
This is particularly useful for pattern classifica-tion problems.The learning algorithm is very simple.
First, thelearning rate and the codebook vectors are initial-ised.
Then, the following procedure is repeated forall the training input vectors until a stopping crite-rion is satisfied:- Select a training input pattern, x, with classd, and present it to the networkclimb\2#1 158 a_hundred\5 1 ab-sorb\2 1 advance\2 1 ... walk\2 1want\1 1 warn\2 1 warped\5 1 way\12 west\3 1 whip\1 2 whir\1 1wraithlike\5 1climb\2#1 45 abruptly\4 1 absence\11 ... stop\2 1 switch_off\2 1there\4 1 tube\1 1 two\5 1 unex-pectedly\4 1 water\1 1...climb\2#2 33 adjust\2 1 almost\4 1arrange\2 1 ... procedure\1 1 re-vetment\1 1 run\2 1 sky\1 1snatch\2 1 spread\2 1 stand\2 1truck\1 1 various\5 1 wait\2 1wing\1 1...climb\2#3 3 average\2 1 feel\2 1report\2 1climb\2#1 10 arise\2 1 come_up\2 1go\2 1 go_up\2 1 lift\2 1 locomote\21 move\2 1 move_up\2 1 rise\2 1travel\2 1climb\2#1 3 climb_up\2 1 go_up\2 1mount\2 1climb\2#1 5 mountaineer\2 1 ramp\2 1ride\2 1 scale\2 1 twine\2 1climb\2#2 7 clamber\2 1 scramble\2 1shin\2 1 shinny\2 1 skin\2 1 sput-ter\2 1 struggle\2 1...climb\2#5 2 go up\2 1 rise\2 1- Calculate the Euclidean distance betweenthe input vector and each codebook vector||x-wk||- Select the codebook vector, wc, that isclosest to the input vector, x.
{ }kcwxwx ?=?kmin[2]This codebook vector is the winner neu-ron and only this neuron updates itsweights according the learning equation(equation 3).
If the class of the input pat-tern, x, matches the class of the winnercodebook vector, wc(the classification hasbeen correct), then the codebook vector ismoved closer to the pattern (reward), oth-erwise it is moved further away.Let x(t) be a input vector at time t, and wk(t) thecodebook vector for the class k at time t. The fol-lowing equation defines the basic learning processfor the LVQ algorithm.
[ ]  )()()()()1( tttsttcccwxww ??
?+=+ ?
[3]Figure 3.
Codebook vectors for ?climb?
domainwhere s = 0, if k ?
c; s = 1, if x(t) and wc(t) be-long to the same class (c = d); and s = -1, if they donot (c ?
d).
?
(t) is the learning rate, and 0<?
(t)<1is a monotically decreasing function of time.
It isrecommended that ?
(t) should initially be rathersmall, say, smaller than 0.1 (Kohonen, 1995) and?
(t) continues decreasing to a given threshold, u,very close to 0.The codebook vectors for the LVQ were initial-ized to zero and every training vector was intro-duced into the neural network, modifying theprototype vector weights depending on the correct-ness in the winner election.All training vectors were introduced severaltimes, updating the weights according to learningequation.
?
(t) is a monotonically decreasing func-tion and it represents the learning rate factor, be-ginning with 0.1 and decreasing lineally:( ) ( ) ( )Ptt01???
?=+  [4]where P is the number of iterations performed inthe training.
The number of iterations has beenfixed at 25 because at this point the network isstabilized.The LVQ must find the winner sense by calcu-lating the Euclidean distances between the code-book vectors and input vector.
The shortestdistance points to the winner and its weights mustbe updated.4 English TasksThe training corpus generated from SemCor andWordNet has been used to train the neural net-works.
All contexts of every word to disambiguateconstitute a domain.
Each domain represents aword and its senses.
Figure 3 shows the codebookvectors generated after training process for ?climb?domain.We have generated one network per domain andafter the training process, we have as many do-mains as there are words to disambiguate adjusted.The network architecture per domain is shown inFigure 4.
The number of input units is the numberof different terms in all contexts of the given do-main and the number of output units is the numberof different senses.The disambiguator system has been used in Eng-lish lexical sample and English all words tasks.For the English lexical sample task, we haveused the available SENSEVAL-3 corpus to trainthe neural networks.
We have also used the con-texts generated using SemCor and WordNet foreach word in SENSEVAL-3 corpus.
For the Eng-climb\2#1 1921 a\1#0 0.01883aarseth\1#0 0.03259 abelard\1#0 ...yorkshire\1#0 0.03950 young\3#00.00380 zero\1#0 0.01449climb\2#2 235 act\1#0 -0.11558alone\4#0 -0.07754 ... windy\3#0 -0.00922 worker\1#0 -0.02738 year\1#0-0.03715 zacchaeus\1#0 -0.02344climb\2#3 1148 abchasicus\1#00.04127 able\3#0 -0.00945 ...young\3#0 -0.00275 zero\1#0 -0.00010climb\2#4 258 age\1#0 -0.04180 air-space\1#0 -0.02862 alone\4#0 -0.01920 apple\1#0 -0.04242 ...world\1#0 -0.14184 year\1#0 -0.04113young\3#0 -0.04831 zero\1#0 -0.06230...lish all word task, we have only used the completecontexts of both SemCor and WordNet resources.The corpus has been tagged and lemmatized usingthe Tree-tagger (Schmid, 1994).Figure 4.
The network architectureOnce the training has finished, the testing be-gins.
The test is very simple.
We establish thesimilarity between a given vector of the corpusevaluation with all the codebook vectors of its do-main, and the highest similarity value correspondsto the disambiguated sense (winner sense).
If it isnot possible to  find a sense (it is impossible to ob-tain  the cosine similarity value), we assign by de-fault the most frequent sense (e.g.
the first sense inWordNet).The official results achieved by the University ofJa?n system are presented in Table 1 for Englishlexical sample task, and in Table 2 for English allwords.ELS Precision Recall CoverageFine-grained 0.613 0.613 99.95%Coarse-grained 0.695 0.695 99.95%Table 1.
Official results for ELS.EAW Precision Recall CoverageWith U 0.590 0.590 100%Without U 0.601 0.588 97.795%Table 2.
Official results for EAW.5 ConclusionThis paper presents a new approach based onneural networks to disambiguate the word senses.We have used the LVQ algorithm to train a neuralnetwork to carry out the English lexical sample andEnglish all words tasks.
We have integrated twolinguistic resources in the corpus provided by theorganization: WordNet and SemCor.ReferencesFellbaum, C. 1998.
WordNet: An Electronic Lexi-cal Database.
The MIT PressKohonen, T. 1995.
Self-Organization and Associa-tive Memory.
2nd Ed, Springer.Verlag, Berl?n.Kohonen, T., J. Hynninen, J. Kangas, J. Laak-sonen, K. Torkkola.
1996.
Technical Report,LVQ_PAK: The Learning Vector QuantizationProgram Package.
Helsinki University of Tech-nology, Laboratory of Computer and InformationScience, FIN-02150 Espoo, Finland.Miller G., C. Leacock, T. Randee, R. Bunker.1993.
A Semantic Concordance.
Proc.
of the 3rdDARPA Workshop on Human Language Tech-nology.Salton, G. & McGill, M.J. 1983.
Introduction toModern Information Retrieval.
McGraw-Hill,New York.Schmid, H., 1994.
Probabilistic Part-of-SpeechTagging Using Decision Trees.
In Proceedingsof International Conference on New Methods inLanguage Processing.T1T2T3TM...Sense1Sense1SenseN...
