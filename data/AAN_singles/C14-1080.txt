Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 837?847, Dublin, Ireland, August 23-29 2014.Identifying Emotion Labels from Psychiatric Social Texts UsingIndependent Component AnalysisLiang-Chih Yu1,2  and  Chun-Yuan Ho11Department of Information Management2Innovation Center for Big Data and Digital ConvergenceYuan Ze University, Chung-Li, Taiwanlcyu@saturn.yzu.edu.tw, s986304@mail.yzu.edu.tw?AbstractAccessing the web has been an efficient and effective means to acquire self-help knowledge when suf-fering from depressive problems.
Many mental health websites have developed community-based ser-vices such as web forums and blogs for Internet users to share their depressive problems with other us-ers and health professionals.
Other users or health professionals can then make recommendations in re-sponse to these problems.
Such communications produce a large number of documents called psychiat-ric social texts containing rich emotion labels representing different depressive problems.
Automaticallyidentify such emotion labels can make online psychiatric services more effective.
This study proposes aframework combining latent semantic analysis (LSA) and independent component analysis (ICA) to ex-tract concept-level features for emotion label identification.
LSA is used to discover latent concepts thatdo not frequently occur in psychiatric social texts, and ICA is used to extract independent componentsby minimizing the term dependence among the concepts.
By combining LSA and ICA, more useful la-tent concepts can be discovered for different emotion labels, and the dependence between them can alsobe minimized.
The discriminant power of classifiers can thus be improved by training them on the inde-pendent components with minimized term overlap.
Experimental results show that the use of concept-level features yielded better performance than the use of word-level features.
Additionally, combiningLSA and ICA improved the performance of using each LSA and ICA alone.1 IntroductionSentiment analysis has been successfully applied for many applications (Picard, 1997; Pang and Lee,2008; Calvo and D'Mello, 2010; Liu, 2012; Johansson and Moschitti, 2013; Balahur et al., 2014).Analysis of online psychiatric or mental health texts (Wu et al., 2005; Yu et al., 2009) is also anemerging field that could benefit from sentiment analysis techniques because more and more peoplesearch for help from the web when they suffered from depressive problems, which boost the develop-ment of online community-based services for Internet users to share their depressive problems withother users and health professionals.
Through these services, individuals can describe their depressivesymptoms via web forums and blogs.
Other users or health professionals can then make recommenda-tions in response to these problems.
Figure 1 shows an example psychiatric social text collected fromPsychPark (http://www.psychpark.org), a virtual psychiatric clinic, maintained by a group of volunteerprofessionals belonging to the Taiwan Association of Mental Health Informatics (Bai et al., 2001; Linet al., 2003).This example shows a subject?s depressive problems and the responses recommended by the experts.Some meaningful tags called emotion labels herein are also annotated by the experts to indicate whichcategories the text belongs to.
These emotion labels are useful information and can make online psy-chiatric services more effective.
For instance, psychiatric retrieval systems are able to retrieve relevantdocuments according to the depressive problems (emotion labels) described in user queries so that theThis work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footerare added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/837users can learn self-help knowledge from the responses.
Therefore, this study aims to identify emotionlabels from psychiatric social texts.
We cast this problem into a multi-label text classification task be-cause a psychiatric social text may contain multiple emotion labels.
Additionally, we propose the useof concept-level features to build classifiers instead of using surface-level features such as words, n-grams and dependency structure commonly used in the previous studies (Naughton et al., 2008; Chit-turi and Hansen, 2008; Li and Zong, 2008; Kessler and Sch?tze, 2012; Post and Bergsma, 2013; Yu etal., 2011).In extraction of concept-level features, latent semantic analysis (LSA) (Landauer et al., 1998) hasbeen demonstrated its effectiveness in exploring the latent structure from a collection of documents.
Ituses singular value decomposition (SVD) (Golub and Van Loan, 1996) to discover latent features thatdo not frequently occur in the documents through the indirect associations between words and docu-ments.
Figure 2 shows an example.
The original matrix, as shown in Figure 2(a), is built using fivedocuments with two different emotion labels Ei and Ej.
Suppose that the words w1, w2 are the usefulfeatures for Ei, and w3, w4 are useful for Ej, but w4 is a latent feature because it does not frequently oc-cur in the documents of Ej.
After applying SVD, the latent features can be identified by replacing thezero entries in the original matrix with non-zero real values through the indirect associations betweenwords and documents.
For instance, w4 originally does not occur in d3 and d4, but it does co-occur withw3 in the matrix (e.g., in d5), which means that w4 might also occur in the documents where w3 occursUser Problem:I broke up with my dear but cruel boyfriend recently.Since then, I have often felt like crying out of nowhere, and I feel pain every day.
<Depression>Also, it takes me a long time to fall asleep at night.
<Insomnia>So, I think that continuing to live like this is meaningless                                              <Suicide>Recommendation:Feeling this way is normal when going through these kinds of struggles, but over time your emo-tions should level out.
Suicide doesn't solve anything; think about how it would affect your family.It's only when we learn to face our despair that we can learn the value of life, and also how to helpother people.
There are a few things you can try to help you get to sleep at night, like doing somelight exercise in the evening, drinking warm milk, and listening to relaxing music; all of these canbe conducive to sleep.
If you still have trouble dealing with the pain, and you feel as if your moodis getting worse, it wouldn't hurt to get seek help from a healthcare professional, who can help youwork through your emotions.Emotion Label: <Depression>, <Insomnia>, <Suicide>Figure 1.
Example of a psychiatric social text.1w2w3w3 4 5jEd d d????
?1 2iEd d??
?4w3 4 5jEd d d????
?1 2iEd d??
?Figure 2.
Comparison of LSA and ICA for feature representation.838(e.g., d3 and d4).
Therefore, the zero entries (w4, d3) and (w4, d4) are replaced with a non-zero valuethrough the indirect associations between of w3 and w4 in d5, as shown in Figure 2(b).
This helps iden-tify a useful latent feature w4 for Ej.
However, identifying latent features through the indirect associa-tions cannot avoid feature overlap when different emotion labels share common words.
For instance,in Figure 2(a), w1, which is useful for Ei, still occurs in the document of Ej (e.g., d4).
Through the indi-rect associations between of w1 and w3 in d4, the frequency of w1 increases in the document of Ej be-cause it may also occur in the documents where w3 occurs (e.g., d3 and d5), as shown in Figure 2(b).Therefore, when all word features are to be accommodated in a low-dimensional space reduced bySVD, term overlap may occur between the latent concepts.
As indicated in Figure 2(c), the two samplelatent concepts which contribute to two different emotion labels share a common feature w1.
Classifi-ers trained on such latent vectors with term overlap may decrease classification performance.To reduce the term overlap among concepts, we used the independent component analysis (ICA)(Lee, 1998; Hyv?rinen et al., 2001; Naik and Kumar, 2011) because it can extract independent com-ponents from a mixture of signals and has been used in various text applications (Kolenda and Hansen,2000; Rapp, 2004; Honkela et al., 2010; Yu and Chien, 2013).
For our task, the psychiatric social textsare a mixture of emotion labels, which can be separated by ICA to obtain a set of independent compo-nents (concepts) with minimized term dependency for different emotion labels.
Instead of using ICAalone, we propose a framework combining LSA and ICA for emotion label identification.
The LSA isused to discover latent features that do not frequently occur in psychiatric texts, and ICA is used tofurther minimize the dependence of the latent features such that overlapped features can be removed,as presented in Figure 2(d).
Based on this combination, the proposed framework can discover moreuseful latent features for different emotion labels, and the dependence between them can also be min-imized.
The discriminant power of classifiers can thus be improved by training them on the independ-ent components with minimized term overlap.
In experiments, we evaluate the proposed method todetermine whether the use of concept-level features could improve the classification performance, anddetermine whether the combination method could improve the performance of using each LSA andICA alone.The rest of this paper is organized as follows.
Section 2 describes the overall framework includingLSA and ICA for emotion label identification.
Section 3 summarizes comparative results.
Conclusionsare finally drawn in Section 4.LSA ICAPsychiatric social textsConcept Analysistopic 1topic 2topic 3...topic n...demixing matrix WXSVM testingWXWdtest document ddemxingdemixingtopic 1topic 2SVM trainingmodelFigure 3.
Framework of emotion label identification.8392 Framework of Emotion Label IdentificationFigure 3 shows the overall framework for emotion label identification.
A corpus of psychiatric socialtexts with annotation of emotion labels are first collected from the web.
This corpus which is a mixtureof different emotion labels is then sequentially analyzed by LSA and ICA to generate a demixing ma-trix composed of a set of concepts with minimized term dependency for different emotion labels.
Thedemixing matrix is used to separate the psychiatric social texts with mixed emotion labels into inde-pendent components for building a support vector machine (SVM) classifier.
The classifier can thenbe benefit from the independent components to identity multiple emotion labels contained in each testexample.2.1 Latent Semantic Analysis (LSA)LSA is a technique for analyzing the relationships between words and documents.
For our task,LSA is used to identify useful latent concepts for emotion labels through indirect associations betweenwords and documents.
The first step in LSA is to build a word-by-document matrix from a corpus ofpsychiatric texts with different emotion labels, as shown in the sample matrix X in Figure 4.The columns in Q D?X  represent D psychiatric texts in the corpus, and the rows represent Q distinctwords occurring in the corpus.
Singular value decomposition (SVD) is then used to decompose thematrix Q D?X  into three matrices as follows:,TQ D Q n n n n D?
?
?
??
?X U V             (1)where U and V respectively consist of a set of latent vectors of words and documents, ?
is a diagonalmatrix of singular values, and min( , )n Q D?
denotes the dimensionality of the latent semantic space.Each element in U represents the weight of a word, and the higher-weighted words are the useful fea-tures for the emotion labels.
By selecting the largest k1 ( n? )
singular values together with the first k1columns of U and V, the word and documents can be represented in a low-dimensional latent semanticspace.
The matrix Tn D?V  can then represented with the reduced dimensions, as shown in Eq.
(2).1 1 1 11 ,T Tk D k k k Q Q D??
?
?
??
?V U X                         (2)In SVM training and testing, each input psychiatric text first transformed into the latent semantic rep-resentation as follows:1 1 11111 ,?
??
?
??
?
?
Tk k k Q Qkt U t                          (3)XQ D?
?
UQ n?
?
n n??
?
VTn D?1( )Q k?1 1( )k k?
( )n D?1d Dd1wQwwords ?
( )n n?
( )Q n?
( )Q D?1( )k D?Figure 4.
Illustrative example of singular value decomposition for latent semantic analysis.840where 1Q?t  denotes the vector representation of an input instance, and 1 1k?
?t  denotes the transformedvector in the latent semantic space.
An SVM classifier is then trained with the transformed trainingvectors.2.2 Independent Component Analysis (ICA)ICA is a technique for extracting independent components from a mixture of signals and has been suc-cessfully applied to solve the blind source separation problem (Saruwatari et al., 2006; Chien andHsieh, 2012).
The ICA model can be formally described asX AS?
(4)where X denotes the observed mixture signals, A denotes a mixing matrix, and S denotes the inde-pendent components.
The goal of ICA is to estimate both A and S. Once the mixing matrix A is esti-mated, the demixing matrix can be obtained by 1W A??
, and Eq.
(4) can be re-written asS WX?
(5)That is, the observed mixture signals can be separated into independent components using the demix-ing matrix.
For our problem, psychiatric texts can be considered as a mixture of signals because eachof them may contain multiple emotion labels.
Therefore, ICA used herein is to estimate the demixingmatrix so that it can separate the psychiatric texts with mixed emotion labels to derive the independentcomponents for each emotion label.
Figure 5 shows the diagram of the proposed method.2.1.1  LSA decomposition and transformationIn the training phase, the original matrix Q D?X  is first processed by SVD using Eq.
(1) and (2) Usefullatent features that do not frequently occur in the original matrix can thus be discovered in this step.2.1.2  ICA decomposition and demixingThe matrix 1Tk D?V  decomposed by SVD is then passed to ICA to estimate the demixing matrix.
ICAaccomplishes this by decomposing 1Tk D?V  using Eq.
(6).
Figure 6 shows an example of the decomposi-tion.LSATQ D Q n n n n D1) decomposition: X U V?
?
?
??
?1 1 1 11T Tk D k k k Q Q D2) transformation: V U X??
?
?
??
?TV1 1 2 2Tk D k k k D1) decomposition: V A S?
?
?
?2 2 1 1Tk D k k k D2) demixing: S W V?
?
?
?SSVM training1 TU?
?transformation matrixWdemixing matrixtraining1 1 111Tk k k Q QU t?
?
?
?
?Q DX ?testing1Qt ?2 1 1 1 111Tk k k k k Q QW U t??
?
?
?
?ICALSA transformationICA demixingModelemotion labels  Figure 5.
ICA-based method for emotion label identification.8411 1 2 2 .Tk D k k k D?
?
?
?V A S                           (6)Based on this decomposition, the demixing matrix can be obtained by 2 1 1 21k k k kW A??
??
, where k2( 1k? )
is the number of independent components.
The demixing matrix is then used to separate 1Tk D?Vto derive the independent components as follows:2 2 1 1 ,Tk D k k k D?
?
?
?S W V                      (7)An SVM classifier is then trained with the independent components 2k D?S , as shown in Figure 5.
Intesting, each test instance 1Q?t  is transformed using both LSA and ICA, and then predicted with thetrained SVM model.3 Experimental Results3.1 Experiment Setup3.1.1  DataThe data set used for experiments included 1,711 Chinese psychiatric social texts collected from thePsychPark.
Each psychiatric social text was manually annotated with an emotion label by a group ofvolunteer mental health professionals.
Table 1 shows the proportions of the emotion labels in the cor-pus.
In calculating the proportion of each emotion label, a psychiatric social text was counted for mul-tiple emotion labels depending on the number of emotion labels contained in it.
In evaluation, 20% ofpsychiatric social texts in the corpus were randomly selected as a test set, and the remaining 80% wereused for training.No.
Emotion Label Proportion1 Depression 35.26%2 Drug 13.38%3 Insomnia 5.79%4 Mood 30.04%5 OCD (Obsessive compulsive disorder) 4.51%6 Schizophrenia 5.36%7 Social Anxiety 5.65%Table 1.
Distribution of emotion labels in experimental data?
1 2Ak k?
?
2Sk D?1 2( )k k?
( )n D?1( )k n?2( )k D?1VTk D?1( )k D?1d Ddconcepts ?Figure 6.
Illustrative example of ICA decomposition.8423.1.2 ClassifiersThe classifiers involved in this experiment included PureSVM, LSA, ICA, and LSA+ICA.
ThePureSVM was trained on word-level features, and the others were trained on concept-level featuresderived using LSA, ICA, and combination of them, respectively.
The implementation details for eachclassifier are as follows:?
PureSVM: An SVM classifier trained with bag-of-words features.?
LSA: An SVM classifier trained with the latent vectors obtained from the word-by-documentmatrix built from the training corpus.?
ICA: An SVM classifier trained with the independent components obtained by demixing theword-by-document matrix built from the training corpus.?
LSA+ICA: An SVM classifier trained with the independent components obtained by demixingthe word-by-document matrix produced by LSA.To identify multiple emotion labels contained in test examples, each emotion label presented in Ta-ble 1 was trained a binary classifier in the training phase.
That is, for each method presented above, webuilt seven binary classifiers so that they can output multiple positive results to indicate that a test ex-ample contained multiple emotion labels.3.1.3 Evaluation MetricsThe metrics used for performance evaluation included recall, precision, and F-measure, respectively.Recall was defined as the number of emotion labels correctly identified by the method divided by thetotal number of emotion labels in the test set.
Precision was defined as the number of emotion labelscorrectly identified by the method divided by the number of emotion labels identified by the method.The F-measure (F1) was defined as 2 recall precision?
?
/ (recall + precision).3.2 Evaluation of LSA and ICAThis experiment compared the performance of LSA and ICA using different settings for the parame-ters k1 and k2, which respectively represent the dimensionality of the latent semantic space and the200 400 600 800100 300 500 700 900k0.40.450.50.550.60.65F-measureLSA+ICAICALSAFigure 7.
Performance of the LSA, ICA and LSA+ICA, as a function of k.843number of independent components.
Figure 7 shows the F-measure of LSA, ICA, and combination ofthem with the setting 1 2k k k?
?
.
The F-measure is the average F-measure over the seven emotion la-bels.
The results show that the optimal settings of LSA was k=200.
The performance of LSA droppeddramatically as k>200, indicating that most useful latent features were discovered within the first 200concepts and the remaining concepts may contain noisy features, thus reducing performance.
The re-spective optimal settings for ICA and LSA+ICA were k=900 and k=800.
In addition, both ICA andLSA+ICA outperformed LSA for most settings of k. The best settings of the parameters were used inthe following experiments.3.3 Comparative ResultsThis section reports the classification performance of PureSVM, LSA, ICA, and LSA+ICA.
Table 2shows the comparative results.
Compared to the use of word-level features (i.e., PureSVM), LSA, ICA,and LSA+ICA achieved a higher F-measure.
Additionally, LSA yielded a much greater recall than didPureSVM, whereas ICA yielded much greater precision.
These findings indicate that the concept-levelfeatures are useful for emotion label identification.
Among the three concept-based methods, LSA candiscover latent concepts for emotion labels, whereas ICA can extract independent components that canminimize the term dependence within them.
The results show that ICA yielded higher recall and F-measure but lower precision than did LSA.
By combining LSA and ICA, the performance was im-proved on all measures because LSA+ICA can not only discover latent concepts but also minimizeterm overlap among the concepts.Another observation is that the emotion label Depression yielded the highest F-measure while bothOCD and Schizophrenia yielded the lowest.
One possible reason for these results is the distribution ofemotion labels in the test set (e.g., Depression and Mood are the major classes).
However, the skeweddistribution was just a minor factor.
For example, the test set included four small classes (Insomnia,OCD, Schizophrenia and Social Anxiety) with similar proportions (5.79%, 4.51%, 5.36% and 5.65%),but their F-measures were quite different (70%, 57%, 57% and 64%).
Terms overlap emotion labelscould have a significant impact on classification performance.
For example, Insomnia had a muchhigher classification performance than the other three minor classes because the words used in thisclass were quite distinct from those used for other classes.
Conversely, the words used for OCD andSocial Anxiety overlapped significantly, thus yielding lower performance.
Table 3 shows some repre-sentative words (with higher weights) in the independent components for the emotion labels.Class PureSVM LSA ICA LSA+ICA R P F R P F R P F R P FDepression 58 59 59 68 74 71 72 75 73 73 78 75Drug 60 38 47 57 71 63 51 69 59 55 72 62Insomnia 53 66 59 49 76 60 65 76 70 66 75 70Mood 63 48 54 61 56 58 65 59 62 67 61 64OCD 58 39 47 53 53 53 53 53 53 56 59 57Schizophrenia 63 23 34 56 64 60 56 47 51 58 57 57Social Anxiety 34 40 37 24 78 37 52 71 60 56 74 64Avg.
56 45 48 53 67 57 59 64 61 62 68 64Table 2.
Performance for different classifiers.
The columns R, P, and F represent recall, precision, andf-measure, respectively.
(in %)8443.4 Term Overlap AnalysisIn order to investigate the term overlap in LSA and LSA+ICA, we analyze their respective corre-sponding matrices Q kU ?
and TQ kW ?
where TQ kW ?
is the transpose of the demixing matrix obtained withthe input of Q D?X  reconstructed using LSA.
Each column of Q kU ?
and TQ kW ?
represents a latent vec-tor/independent component of Q words, and each element in the vector is a word weight representingits relevance to the corresponding latent vector/independent component.
Figure 8 shows two samplelatent vectors for LSA and two independent components for LSA+ICA, where the weights shown inthis figure are the absolute values.The upper part of Fig.
8 shows parts of the words and their weights in the two latent vectors, wherelatent vector #1 can be characterized by depressed, depression, and sad which are the useful featuresfor identifying the emotion label <Depression>, and latent vector #2 can be characterized by depressed,sad, and cry which are useful for identifying <Mood>.
Although the two latent vectors contained use-ful features for the respective emotion labels, these features still had some overlap between the latentvectors, as marked by the dashed rectangles.
The overlapped features, especially those with higherweights, may reduce the classifier?s ability to distinguish between the emotion labels.
The lower partof Fig.
8 also shows two independent components for the emotion labels <Depression> and<Mood>.As indicated, the term overlap between the two independent components was relatively low.
Table 3shows some representative words (with higher weights) in the independent components for the emo-tion labels.Figure 8.
Examples of latent vectors, selected from Q kU ?
, and independent components, selectedfrom TQ kW ?
, for the emotion labels <Depression> and <Mood>.845Emotion Label Representative WordsDepression depression, depressed, sad, down, suicideDrug Medication, drug, dose, sedative, withdrawal,Insomnia sleep, insomnia, dream, nightmare, awakeMood cry, upset, anxious, energy, obstacleOCD OCD, compulsion, weight, overeating, behaviorSchizophrenia paranoia, fantasy, memory, split, geneticSocial Anxiety crowd, tense, friend, stiffness, ridicule, shiveringTable 3.
Representative words for the emotion labels.4 ConclusionsThis work has presented a framework combining LSA and ICA for emotion label identification.
BothLSA and ICA are used to analyze concept-level features, where LSA is used to discover latent con-cepts that do not frequently occur in psychiatric texts, and ICA is used to further minimize the termdependence among the concepts.
The experimental results show that the use of concept-level featuresyielded better performance than the use of word-level features.
Additionally, ICA can reduce the de-gree of term overlap of LSA so that combining LSA and ICA can discover more useful latent conceptswith minimized term dependence for different emotion labels, thus improving classification perfor-mance.
Future work will focus on investigating the use of the machine-labeled emotion labels as meta-information to improve online psychiatric services such as information retrieval for self-helpknowledge recommendation.AcknowledgmentsThis work was supported by the Ministry of Science and Technology, Taiwan, ROC, under Grant No.NSC102-2221-E-155-029-MY3.ReferenceY.
M. Bai, C. C. Lin, J. Y. Chen, and W.C. Liu.
2001.
Virtual psychiatric clinics.
American Journal of Psychia-try, 158(7): 1160-1161.A.
Balahur, R. Mihalcea, and A. Montoyo.
2014.
Computational approaches to subjectivity and sentiment analy-sis: Present and envisaged methods and applications.
Computer Speech & Language 28(1): 1-6.R.
A. Calvo and S. D'Mello.
2010.
Affect Detection: An interdisciplinary review of models, methods, and theirapplications.
IEEE Trans.
Affective Computing, 1(1): 18-37.J.
T. Chien and H. L.Hsieh.
2012.
Convex divergence ICA for blind source separation.
IEEE Trans.
Audio,Speech, and Language Processing, 20(1): 302-313.R.
Chitturi and J. H. L. Hansen.
2008.
Dialect classification for online podcasts fusing acoustic and languagebased structural and semantic information.
Proceedings of the 46th Annual Meeting of the Association forComputational Linguistics (ACL-08), pages 21-24.G.
H. Golub and C. F. Van Loan.
1996.
Matrix Computations, Third Edition, Johns Hopkins University Press,Baltimore, MD.T.
Honkela, A. Hyv?rinen, and J. J. V?yrynen.
2010.
WordICA ?
emergence of linguistic representations forwords by independent component analysis.
Natural Language Engineering, 16(3): 277-308.846A.
Hyv?rinen, J. Karhunen, and E. Oja.
2001.
Independent Component Analysis.
Wiley, New York.R.
Johansson and A. Moschitti.
2013.
Relational features in fine-grained opinion analysis.
Computational Lin-guistics, 39(3): 473-509.W.
Kessler and H. Sch?tze.
2012.
Classification of inconsistent sentiment words using syntactic constructions.Proceedings of the 24th International Conference on Computational Linguistics (COLING-12), pages 569-578.T.
Kolenda and L. K. Hansen.
2000.
Independent components in text.
Advances in Neural Information Pro-cessing Systems 13: 235-256.T.
K. Landauer, P. W. Foltz, and D. Laham.
1998.
An introduction to latent semantic analysis.
Discourse Pro-cesses, 25(2&3): 259-284.T.
W. Lee.
1998.
Independent Component Analysis?Theory and Applications.
Kluwer, Norwell, MA.S.
Li and C. Zong.
2008.
Multi-domain Sentiment Classification.
Proceedings of the 46th Annual Meeting of theAssociation for Computational Linguistics (ACL-08), pages 257-260.C.
C. Lin, Y. M. Bai, and J. Y. Chen.
2003.
Reliability of information provided by patients of a virtual psychiat-ric clinic, Psychiatric Services, 54(8): 1167-1168.B.
Liu.
2012.
Sentiment Analysis and Opinion Mining.
Morgan & Claypool, Chicago, IL.G.
R. Naik and D. K. Kumar.
2011.
An overview of independent component analysis and its applications.
Infor-matica 35(1): 63-81.M.
Naughton, N. Stokes, and J. Carthy.
2008.
Investigating statistical techniques for sentence-level event classi-fication.
Proceedings of the 22nd International Conference on Computational Linguistics (COLING-08), pag-es 617-624.B.
Pang and L. Lee.
2008.
Opinion mining and sentiment analysis.
Foundations and Trends in Information Re-trieval, 2: 1-135.R.
W. Picard.
1997.
Affective Computing, MIT Press, Cambridge, MA.M.
Post and S. Bergsma.
2013.
Explicit and implicit syntactic features for text classification.
Proceedings of the51st Annual Meeting of the Association for Computational Linguistics (ACL-13), pages 866?872.R.
Rapp.
2004.
Mining text for word senses using independent component analysis.
Proceedings of the 4th SIAMInternational Conference on Data Mining (SDM-04), pages 422-426.H.
Saruwatari, T. Kawamura, T. Nishikawa, A. Lee, and K. Shikano.
2006.
Blind source separation based on afast-convergence algorithm combining ICA and beamforming.
IEEE Trans.
Audio, Speech, and LanguageProcessing, 14(2): 666-678.C.
H. Wu, L. C. Yu, and F. L. Jang.
2005.
Using semantic dependencies to mine depressive symptoms from con-sultation records.
IEEE Intelligent System, 20(6): 50-58.L.
C. Yu, C. L. Chan, C. C. Lin, and I. C. Lin.
2011.
Mining association language patterns using a distributionalsemantic model for negative life event classification.
Journal of Biomedical Informatics, 44(4): 509-518.L.
C. Yu and W N. Chien.
2013.
Independent component analysis for near-synonym choice.
Decision SupportSystems, 55(1): 146-155.L.
C. Yu, C. H. Wu, and F. L. Jang.
2009.
Psychiatric document retrieval using a discourse-aware model.
Artifi-cial Intelligence, 173(7-8): 817-829.847
