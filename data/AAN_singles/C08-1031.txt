Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 241?248Manchester, August 2008Mining Opinions in Comparative SentencesMurthy GanapathibhotlaDepartment of Computer ScienceUniversity of Illinois at Chicago851 South Morgan StreetChicago, IL 60607-7053sganapat@cs.uic.eduBing LiuDepartment of Computer ScienceUniversity of Illinois at Chicago851 South Morgan StreetChicago, IL 60607-7053liub@cs.uic.eduAbstractThis paper studies sentiment analysisfrom the user-generated content on theWeb.
In particular, it focuses on miningopinions from comparative sentences, i.e.,to determine which entities in a compari-son are preferred by its author.
A typicalcomparative sentence compares two ormore entities.
For example, the sentence,?the picture quality of Camera X is betterthan that of Camera Y?, compares twoentities ?Camera X?
and ?Camera Y?with regard to their picture quality.
Clear-ly, ?Camera X?
is the preferred entity.Existing research has studied the problemof extracting some key elements in acomparative sentence.
However, there isstill no study of mining opinions fromcomparative sentences, i.e., identifyingpreferred entities of the author.
This pa-per studies this problem, and proposes atechnique to solve the problem.
Our ex-periments using comparative sentencesfrom product reviews and forum postsshow that the approach is effective.1 IntroductionIn the past few years, there was a growing inter-est in mining opinions in the user-generated con-tent (UGC) on the Web, e.g., customer reviews,forum posts, and blogs.
One major focus is sen-timent classification and opinion mining (e.g.,Pang et al2002; Turney 2002; Hu and Liu 2004;Wilson et al2004; Kim and Hovy 2004; Popescuand Etzioni 2005)?
2008.
Licensed under the Creative Commons Attri-bution-Noncommercial-Share Alike 3.0 Unportedlicense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.However, these studies mainly center on directopinions or sentiments expressed on entities.
Lit-tle study has been done on comparisons, whichrepresent another type of opinion-bearing text.Comparisons are related to but are also quite dif-ferent from direct opinions.
For example, a typi-cal direct opinion sentence is ?the picture qualityof Camera X is great?, while a typical compara-tive sentence is ?the picture quality of Camera Xis better than that of Camera Y.?
We can see thatcomparisons use different language constructsfrom direct opinions.
A comparison typicallyexpresses a comparative opinion on two or moreentities with regard to their shared features orattributes, e.g., ?picture quality?.
Although directopinions are most common in UGC, comparisonsare also widely used (about 10% of the sen-tences), especially in forum discussions whereusers often ask questions such as ?X vs. Y?
(Xand Y are competing products).
Discussions arethen centered on comparisons.Jindal and Liu (2006) proposed a technique toidentify comparative sentences from reviews andforum posts, and to extract entities, comparativewords, and entity features that are being com-pared.
For example, in the sentence, ?Camera Xhas longer battery life than Camera Y?, thetechnique extracts ?Camera X?
and ?Camera Y?as entities, and ?longer?
as the comparativeword and ?battery life?
as the attribute of thecameras being compared.
However, the tech-nique does not find which entity is preferred bythe author.
For this example, clearly ?Camera Y?is the preferred camera with respect to the ?bat-tery life?
of the cameras.
This paper aims tosolve this problem, which is useful in many ap-plications because the preferred entity is the keypiece of information in a comparative opinion.For example, a potential customer clearly wantsto buy the product that is better or preferred.In this work, we treat a sentence as the basic241information unit.
Our objective is thus to identifythe preferred entity in each comparative sentence.A useful observation about comparative sen-tences is that in each such sentence there isusually a comparative word (e.g., ?better?,?worse?
and ?er word) or a superlative word(e.g., ?best?, ?worst?
and ?est word).
The entitiesbeing compared often appear on the two sides ofthe comparative word.
A superlative sentencemay only have one entity, e.g., ?Camera X is thebest?.
For simplicity, we use comparative words(sentences) to mean both comparative words(sentences) and superlative words (sentences).Clearly, the preferred entity in a comparativesentence is mainly determined by the compara-tive word in the sentence.
Some comparativewords explicitly indicate user preferences, e.g.,?better?, ?worse?, and ?best?.
We call suchwords opinionated comparative words.
For ex-ample, in the sentence, ?the picture quality ofCamera X is better than that of Camera Y?,Camera X is preferred due to the opinionatedcomparative word ?better?.However, many comparative words are notopinionated, or their opinion orientations (i.e.,positive or negative) depend on the contextand/or the application domain.
For instance, theword ?longer?
is not opinionated as it is normal-ly used to express that the length of some featureof an entity is greater than the length of the samefeature of another entity.
However, in a particularcontext, it can express a desired (or positive) orundesired (or negative) state.
For example, in thesentence, ?the battery life of Camera X is longerthan Camera Y?, ?longer?
clearly expresses adesired state for ?battery life?
(although this is anobjective sentence with no explicit opinion).
?Camera X?
is thus preferred with regard to?battery life?
of the cameras.
The opinion in thissentence is called an implicit opinion.
We alsosay that ?longer?
is positive in this context.
Weknow this because of our existing domain know-ledge.
However, ?longer?
may also be used toexpress an undesirable state in a different context,e.g., ?Program X?s execution time is longer thanProgram Y?.
longer?
is clearly negative here.
?Program Y?
is thus preferred.
We call compara-tive words such as ?longer?
and ?smaller?
con-text-dependent opinion comparatives.Sentences with opinionated words (e.g., ?bet-ter?, and ?worse?)
are usually easy to handle.Then the key to solve our problem is to identifythe opinion orientations (positive or negative) ofcontext-dependent comparative words.
To thisend, two questions need to be answered: (1) whatis a context and (2) how to use the context tohelp determine the opinion orientation of a com-parative word?The simple answer to question (1) is the wholesentence.
However, a whole sentence as contextis too complex because it may contain too muchirrelevant information, which can confuse thesystem.
Intuitively, we want to use the smallestcontext that can determine the orientation of thecomparative word.
Obviously, the comparativeword itself must be involved.
We thus conjecturethat the context should consist of the entity fea-ture being compared and the comparative word.Our experimental results show that this contextdefinition works quite well.To answer the second question, we need ex-ternal information or knowledge because there isno way that a computer program can solve theproblem by analyzing the sentence itself.
In thispaper, we propose to use the external informationin customer reviews on the Web to help solve theproblem.
There are a large number of such re-views on almost any product or service.
Thesereviews can be readily downloaded from manysites.
In our work, we use reviews from epi-nions.com.
Each review in epinions.com has sep-arate Pros and Cons (which is also the case inmost other review sites).
Thus, positive andnegative opinions are known as they are sepa-rated by reviewers.
However, they cannot beused directly because Pros and Cons seldom con-tain comparative words.
We need to deal withthis problem.
Essentially, the proposed methodcomputes whether the comparative word and thefeature are more associated in Pros or in Cons.
Ifthey are more associated in Pros (or Cons) thanCons (or Pros), then the comparative word islikely to be positive (or negative) for the feature.A new association measure is also proposed tosuit our purpose.
Our experiment results showthat it can achieve high precision and recall.2 Related WorkSentiment analysis has been studied by manyresearchers recently.
Two main directions aresentiment classification at the document and sen-tence levels, and feature-based opinion mining.Sentiment classification at the document levelinvestigates ways to classify each evaluativedocument (e.g., product review) as positive ornegative (Pang et al2002; Turney 2002).
Senti-ment classification at the sentence-level has alsobeen studied (e.g., Riloff and Wiebe 2003; Kimand Hovy 2004; Wilson et al2004; Gamon et al2422005; Stoyanov and Cardie 2006).
These worksare different from ours as we study comparatives.The works in (Hu and Liu 2004; Liu et al2005;Popescu and Etzioni 2005; Mei et al2007) per-form opinion mining at the feature level.
Thetask involves (1) extracting entity features (e.g.,?picture quality?
and ?battery life?
in a camerareview) and (2) finding orientations (positive,negative or neutral) of opinions expressed on thefeatures by reviewers.
Again, our work is differ-ent because we deal with comparisons.Discovering orientations of context dependentopinion comparative words is related to identify-ing domain opinion words (Hatzivassiloglou andMcKeown 1997; Kanayama and Nasukawa2006).
Both works use conjunction rules to findsuch words from large domain corpora.
One con-junction rule states that when two opinion wordsare linked by ?and?, their opinions are the same.Our method is different in three aspects.
First, weargue that finding domain opinion words is prob-lematic because in the same domain the sameword may indicate different opinions dependingon what features it is applied to.
For example, inthe camera domain, ?long?
is positive in ?thebattery life is very long?
but negative in ?it takesa long time to focus?.
Thus, we should considerboth the feature and the opinion word rather thanonly the opinion word.
Second, we focus onstudying opinionated comparative words.
Third,our technique is quite different as we utilize rea-dily available external opinion sources.As discussed in the introduction, a closely re-lated work to ours is (Jindal and Liu 2006).However, it does not find which entities are pre-ferred by authors.
Bos and Nissim (2006) pro-poses a method to extract some useful items fromsuperlative sentences.
Fiszman et al(2007) stu-died the problem of identifying which entity hasmore of certain features in comparative sen-tences.
It does not find which entity is preferred.3 Problem StatementDefinition (entity and feature): An entity is thename of a person, a product, a company, a lo-cation, etc, under comparison in a compara-tive sentence.
A feature is a part or attributeof the entity that is being compared.For example, in the sentence, ?Camera X?s bat-tery life is longer than that of Camera Y?, ?Cam-era X?
and ?Camera Y?
are entities and ?batterylife?
is the camera feature.Types of Comparatives1)  Non-equal gradable: Relations of the typegreater or less than that express a total order-ing of some entities with regard to theirshared features.
For example, the sentence,?Camera X?s battery life is longer than that ofCamera Y?, orders ?Camera X?
and ?CameraY?
based on their shared feature ?battery life?.2)  Equative: Relations of the type equal to thatstate two objects as equal with respect tosome features, e.g., ?Camera X and Camera Yare about the same size?.3)  Superlative: Relations of the type greater orless than all others that rank one object overall others, ?Camera X?s battery life is thelongest?.4)  Non-gradable: Sentences which compare fea-tures of two or more entities, but do not expli-citly grade them, e.g., ?Camera X and Cam-era Y have different features?The first three types are called gradable compar-atives.
This paper focuses on the first and thethird types as they express ordering relationshipsof entities.
Equative and non-gradable sentencesusually do not express preferences.Definition (comparative relation): A compara-tive relation is the following:<ComparativeWord, Features, EntityS1, EntityS2, Type>ComparativeWord is the keyword used to ex-press a comparative relation in the sentence.
Fea-tures is a set of features being compared.
En-tityS1 and EntityS2 are sets of entities beingcompared.
Entities in EntityS1 appear on the leftof the comparative word and entities in EntityS2appear on the right.
Type is non-equal gradable,equative or superlative.
Let us see an example.For the sentence ?Camera X has longer batterylife than Camera Y,?
the extracted relation is:<longer, {battery life}, {Camera X}, {Camera Y},non-equal gradable>.We assume that the work in (Jindal and Liu 2006)has extracted the above relation from a compara-tive sentence.
In this work, we aim to identify thepreferred entity of the author, which is not stu-died in (Jindal and Liu 2006).Our objective: Given the extracted comparativerelation from a comparative sentence, we wantto identify whether the entities in EntityS1 orin EntityS2 are preferred by the author.4 Proposed TechniqueWe now present the proposed technique.
As dis-cussed above, the primary determining factors ofthe preferred entity in a comparative sentence are243the feature being compared and the comparativeword, which we conjecture, form the context foropinions (or preferred entities).
We develop ourideas from here.4.1 Comparatives and superlativesIn English, comparatives and superlatives arespecial forms of adjectives and adverbs.
In gen-eral, comparatives are formed by adding the suf-fix ?-er?
and superlatives are formed by addingthe suffix ??est?
to the base adjectives and ad-verbs.
We call this type of comparatives and su-perlatives Type 1 comparatives and superlatives.For simplicity, we will use Type 1 comparativesto represent both from now on.Adjectives and adverbs with two syllables ormore and not ending in y do not form compara-tives or superlatives by adding ??er?
or ?
?est?.Instead, ?more?, ?most?, ?less?
and ?least?
areused before such words, e.g., ?more beautiful?.We call this type of comparatives and superla-tives Type 2 comparatives and Type 2 superla-tives.
These two types are called regular com-paratives and superlatives respectively.In English, there are also some irregular com-paratives and superlatives, which do not followthe above rules, i.e., ?more?, ?most?, ?less?,?least?, ?better?, ?best?, ?worse?, ?worst?, ?fur-ther/farther?
and ?furthest/farthest?.
They be-have similarly to Type 1 comparatives and super-latives and thus are grouped under Type 1.Apart from these comparatives and superla-tives, there are non-standard words that expressgradable comparisons, e.g., ?prefer?, and ?supe-rior?.
For example, the sentence, ?in term of bat-tery life, Camera X is superior to Camera Y?,says that ?Camera X?
is preferred.
We obtained alist of 27 such words from (Jindal and Liu 2006)(which used more words, but most of them arenot used to express gradable comparisons).
Sincethese words behave similarly to Type 1 compara-tives, they are thus grouped under Type 1.Further analysis also shows that we can groupcomparatives into two categories according towhether they express increased or decreased val-ues:Increasing comparatives: Such a comparativeexpresses an increased value of a quantity, e.g.,?more?, and ?longer?.Decreasing comparatives: Such a comparativeexpresses a decreased value of a quantity, e.g.,?less?, and ?fewer?.As we will see later, this categorization is veryuseful in identifying the preferred entity.Since comparatives originate from adjectivesand adverbs, they may carry positive or negativesentiments/opinions.
Along this dimension, wecan divide them into two categories.1.
Opinionated comparatives: For Type 1 com-paratives, this category contains words suchas "better", "worse", etc, which has explicitopinions.
In sentences involving such words,it is normally easy to determine which entityis the preferred one of the sentence author.In the case of Type 2 comparatives, formedby adding ?more?, ?less?, ?most?, and ?least?before adjectives or adverbs, the opinion (orpreferred entity) is determined by both words.The following rules apply:?increasing comparative?
Negative  ?
Negative Opinion?increasing comparative?
Positive   ?
Positive Opinion?decreasing comparative?
Negative ?
Positive Opinion?decreasing comparative?
Positive  ?
Negative OpinionThe first rule says that the combination of anincreasing comparative word (e.g., ?more?
)and a negative opinion adjective/adverb (e.g.,?awful?)
implies a negative Type 2 compara-tive.
The other rules are similar.
These rulesare intuitive and will not be discussed further.2.
Comparatives with context-dependent opi-nions: These comparatives are used to com-pare gradable quantities of entities.
In the caseof Type 1 comparatives, such words include?higher?, ?lower?, etc.
Although they do notexplicitly describe the opinion of the author,they often carry implicit sentiments or prefe-rences based on contexts.
For example, in?Car X has higher mileage per gallon thanCar Y?, it is hard to know whether ?higher?
ispositive or negative without domain know-ledge.
It is only when the two words, ?higher?and ?mileage?, are combined we know that?higher?
is desirable for ?mileage?
from ourdomain knowledge.In the case of Type 2 comparatives, the sit-uation is similar.
However, the comparativeword (?more?, ?most?, ?less?
or ?least?
), theadjective/adverb and the feature are all impor-tant in determining the opinion or the prefe-rence.
If we know whether the comparativeword is increasing or decreasing (which iseasy since there are only four such words),then the opinion can be determined by apply-ing the four rules above in (1).For this work, we used the opinion word listfrom (Hu and Liu 2004), which was compiledusing a bootstrapping approach based on Word-Net.
For opinionated comparatives, due to theobservation below we simply convert the opinion244adjectives/adverbs to their comparative forms,which is done automatically based on grammar(comparative formation) rules described aboveand WordNet.Observation: If a word is positive (or negative),then its comparative or superlative form is al-so positive (or negative), e.g., ?good?, ?bet-ter?
and ?best?.After the conversion, these words are manuallycategorized into increasing and decreasing com-paratives.
Although this consumes some time, itis only a one-time effort.4.2 ContextsTo deal with comparatives with context depen-dent opinions, we need contexts.
It is conjecturedthat the comparative and the feature in the sen-tence form the context.
This works very well.
Fora Type 2 comparative, we only need the featureand the adjective/adverb to form a context.
Forexample, in the sentence, ?Program X runs morequickly than Program Y?, the context is the pair,(?run?, ?quickly?
), where ?run?
is a verb feature.If we find out that (?run?, ?quickly?)
is positivebased on some external information, we can con-clude that ?Program X?
is preferred using one ofthe four rules above since ?more?
is an increas-ing comparative.We will use such contexts to find opinionorientations of comparatives with regard to somefeatures from the external information, i.e., Prosand Cons in online reviews.4.3 Pros and Cons in ReviewsFigure 1 shows a popular review format.
Thereviewer first describes Pros and Cons briefly,and then writes a full review.Pros and Cons are used in our work for twomain reasons.
First, the brief information in Prosand Cons contains the essential information re-lated to opinions.
Each phrase or sentence seg-ment usually contains an entity feature and anopinion word.
Second, depending on whether itis in Pros or in Cons, the user opinion on theproduct feature is clear.To use the Pros and Cons phrases, we separatethem use punctuations and words, i.e., ?,?, ?.
?,?and?, and ?but?.
Pros in Figure 1 can be sepa-rated into 5 phrases or segments,great photos  <photo>easy to use    <use>good manual  <manual>many options <option>takes videos <video>We can see that each segment describes an entityfeature on which the reviewer has expressed anopinion.
The entity feature for each segment islisted within <>.4.4 Identifying Preferred Entities: The Al-gorithmSince we use Pros and Cons as the external in-formation source to help determine whether thecombination of a comparative and an entity fea-ture is positive or negative, we need to find com-parative and entity features words in Pros andCons.
However, in Pros and Cons, comparativesare seldom used (entity features are alwaysthere).
Thus we need to first convert compara-tives to their base forms.
This can be done auto-matically using WordNet and grammar rules de-scribed in Section 4.1.
We will not discuss theprocess here as it is fairly straightforward.We now put everything together to identify thepreferred entity in a comparative sentence.
Foreasy reference, we denote the comparative wordas C and the feature being compared as F. Afterobtaining the base forms of C, we work on twomain cases for the two types of comparatives:Case 1.
Type 1 Comparative or Superlative:There are four sub-cases.1.A.
C is opinionated: If the comparative or su-perlative C has a positive orientation (e.g.,?better?
), EntityS1 (which appears before Cin the sentence) is temporarily assigned as thepreferred entity.
Otherwise, EntityS2 is as-signed as the preferred entity.
The reason forthe temporary assignment is that the sentencemay contain negations, e.g., ?not?, which isdiscussed below.1.B.
C is not opinionated but F is opinionated:An example is, ?Car X generates more noisethan Car Y?, which has the feature F ?noise?,a negative noun.
If the orientation of F ispositive and C is an increasing comparativeword, we assign EntityS1 as the preferred ent-ity.
Otherwise, we assign EntityS2 as the pre-ferred entity.
The possibilities are listed asfour rules below, which are derived from the4 rules earlier:?increasing C?
+ Positive ?
EntityS1 preferred?decreasing C?
+ Positive ?
EntityS2 preferredFigure 1: An example review245?increasing C?
+ Negative ?
EntityS2 preferred?decreasing C?
+ Negative ?
EntityS1 preferred?Positive?
and ?Negative?
stand for the orien-tation of feature F being positive and negativerespectively.1.C.
Both C and F are not opinionated: In thiscase, we need external information to identifythe preferred entity.
We use phrases in Prosand Cons from reviews.In this case, we look for the feature F andcomparative word C, (i.e., the context) in thelist of phrases in Pros and Cons.
In order tofind whether the combination of C and F indi-cates a positive or negative opinion, we com-pute their associations in Pros and in Cons.
Ifthey are more associated in Pros than in Cons,we conclude that the combination indicates apositive sentiment, and otherwise a negativesentiment.
The result decides the preferredentity.
Point-wise mutual information (PMI)is commonly used for computing the associa-tion of two terms (e.g., Turney 2002), whichis defined as:????
?, ??
?
??????
?, ???????????
?.However, we argue that PMI is not a suitablemeasure for our purpose.
The reason is thatPMI is symmetric in the sense that PMI(F, C)is the same as PMI(C, F).
However, in ourcase, the feature F and comparative word Cassociation is not symmetric because althougha feature is usually modified by a particularadjective word, the adjective word can modifymany other features.
For example, ?long?
canbe used in ?long lag?, but it can also be usedin ?long battery life?, ?long execution time?and many others.
Thus, this association isasymmetric.
We are more interested in theconditional probability of C (including itssynonyms) given F, which is essentially theconfidence measure in traditional data mining.However, confidence does not handle well thesituation where C occurs frequently but F ap-pears rarely.
In such cases a high conditionalprobability Pr(C|F) may just represent somepure chance, and consequently the resultingassociation may be spurious.
We propose thefollowing measure, which we call one-sideassociation (OSA), and it works quite well:????
?, ??
?
??????
?, ??????|???????????
?The difference between OSA and PMI is theconditional probability Pr(C|F) used in OSA,which biases the mutual association of F andC to one side.Given the comparative word C and the fea-ture F, we first compute an OSA value forpositive, denoted by OSAP(F, C), and thencompute an OSA value for negative, denotedby OSAN(F, C).
The decision rule is simplythe following:If OSAP(F, C) ?
OSAN(F, C) ?
0 thenEntityS1 is preferredOtherwise,  EntityS2 is preferredComputing OSAP(F, C): We need to computePrP(F, C), for which we need to count thenumber of times that comparative word C andthe feature F co-occur.
Instead of using Calone, we also use its base forms and syn-onyms and antonyms.
Similarly, for F, we al-so use its synonyms.
If C (or a synonym of C)and F (or a synonym) co-occur in a Prosphrase, we count 1.
If an antonym of C and F(or a synonym) co-occur in a Cons phrase, wealso count 1.
Thus, although we only evaluatefor positive, we actually use both Pros andCons.
This is important because it allows usto find more occurrences to produce more re-liable results.
Synonyms and antonyms areobtained from WordNet.
Currently, synonymsand antonyms are only found for single wordfeatures.We then count the number of occurrences ofthe comparative word C and the feature Fseparately in both Pros and Cons to computePrP(F) and PrP(C).
In counting the number ofoccurrences of C, we consider both its syn-onyms in Pros and antonyms in Cons.
Incounting the number of occurrences of F, weconsider its synonyms in both Pros and Cons.Computing OSAN(F, C): To compute PrN(F,C), we use a similar strategy as for computingPrP(F, C).
In this case, we start with Cons.1.D.
C is a feature indicator: An example sen-tence is ?Camera X is smaller than CameraY?, where ?smaller?
is the feature indicatorfor feature ?size?.
In this case, we simplycount the number of times (denoted by n+)that C appears in Pros and the number oftimes (denoted by n-) that C appears in Cons.If n+ ?
n-, we temporarily assign EntityS1 asthe preferred entity.
Otherwise, we assign En-tityS2 as the preferred entity.
Note that insome sentences, the entity features do not ap-pear explicitly in the sentences but are im-plied.
The words that imply the features arecalled feature indicators.246Case 2: Type 2 Comparative or Superlative:There are two sub-cases:2.A.
Adjective/adverb in the comparison is opi-nionated: In this case, the feature F is not im-portant.
An example sentence is:?Car X has more beautiful interior than Car Y?,?more?
is an increasing comparative, and?beautiful?
is the adjective with a positiveorientation (the feature F is ?interior?).
?CarX?
is clearly preferred in this case.Another example is: ?Car X is more beautifulthan Car Y?.
In this case, ?beautiful?
is a fea-ture indicator for the feature ?appearance?.Again, ?Car X?
is preferred.
This sub-casecan be handled similarly as case 1.B.2.B.
adjective/adverb in the comparison is notopinionated: If the adjective/adverb in com-parison is a feature indicator, we can use themethod in 1.D.
Otherwise, we form a contextusing the feature and adjective/adverb, andapply the method in 1.C.
We then combinethe result with the comparative word beforethe adjective/adverb to decide based on therules in 1.B.Negations: The steps above temporarily deter-mine which entity is the preferred entity.
How-ever, a comparative sentence may contain a ne-gation word or phrase (we have compiled 26 ofthem), e.g., ?Camera X?s battery life is not long-er than that of Camera Y.?
Without considering?not?, ?Camera X?
is preferred.
After consider-ing ?not?, we assign the preferred entity to?Camera Y?.
This decision may be problematicbecause ?not longer?
does not mean ?shorter?
(thus it can also be seen to have no preference).5 EvaluationA system, called PCS (Preferred entities inComparative Sentences), has been implementedbased the proposed method.
Since there is noexisting system that can perform the task, wecould not compare with an existing approach.Below, we first describe the evaluation datasetsand then present the results.5.1 Evaluation DatasetsOur comparative sentence dataset consists of twosubsets.
The first subset is from (Jindal and Liu2006), which are product review and forum dis-cussion sentences on digital cameras, DVD play-ers, MP3 players, Intel vs AMD, Coke vs Pepsi,and Microsoft vs Google.
The original datasetused in (Jindal and Liu 2006) also contains manynon-gradable comparative sentences, which arenot used here as most such sentences do not ex-press any preferences.To make the data more diverse, we collectedmore forum discussion data about mobile phonesfrom http://www.howardforums.com/, and re-views from amazon.com and cnet.com on prod-ucts such as laptops, cameras and mobile phones.Table 1 gives the number of sentences from thesetwo sources.
Although we only have 837 com-parative sentences, they were collected fromthousands of sentences in reviews and forums.About 10% of the sentences from them are com-parative sentences.Skewed Distribution: An interesting observa-tion about comparative sentences is that a largeproportion (based on our data) of them (84%) hasEntityS1 as the preferred entity.
This means thatwhen people make comparisons, they tend to putthe preferred entities first.Pros and Cons corpus: The Pros and Conscorpus was crawled from reviews of epi-nions.com.
It has 15162 Pros and 15162 Consextracted from 15162 reviews of three types ofproducts, i.e., digital cameras (8479), and prin-ters (5778), and Strollers (905).Table 1.
Sentences from different sourcesData Sources No.
of Comparative Sentences(Jindal and Liu 2006) 418Reviews and forum posts 419Total 8375.2 ResultsThe results on the whole dataset are given in Ta-ble 2.
Note that 84% of the sentences have En-tityS1 as the preferred entity.
If a system doesnothing but simply announces that EntityS1 ispreferred, we will have the accuracy of 84%.However, PCS using the OSA measure achievesthe accuracy of 94.4%, which is much better thanthe baseline of taking the majority.
Since inskewed datasets accuracy does not reflect theprediction well, we will mainly use precision(Prec.
), recall (Rec.)
and F-score (F) in evalua-tion.
For the case that EntityS1 is preferred, thealgorithm does extremely well.
For the case thatEntityS2 is preferred, the algorithm also doeswell although not as well as for the EntityS1 case.Based on our observation, we found that in suchcases, the sentences are usually more complex.Next, we compare with the case that the sys-tem does not use Pros and Cons (then OSA orPMI is not needed) (row 2).
When a sentencerequires context dependency handling, the sys-tem simply takes the majority as the default, i.e.,247assigning EntityS1 as the preferred entity.
Fromthe results in Table 2, we can see that F-scoresare all worse.
In the case that EntityS1 is the pre-ferred entity, taking defaults is not so bad, whichis not surprising because of the skewed data dis-tribution.
Even in this case, the precision im-provement of PCS(OSA) is statistically signifi-cant at the 95% confidence level.
The recall isslight less but their difference is not statisticallysignificant.
When EntityS2 is the preferred entity,its F-score (row 2) is much worse, which showsthat our technique is effective.
The recall im-provement of PCS (OSA) is dramatic (statistical-ly significant at the 95% confidence level).
Thetwo precisions are not statistically different.
ForOSA vs. PMI, see below.Table 2: Preferred entity identification: whole dataEntityS1 Preferred EntityS2 PreferredPrec.
Rec.
F Prec.
Rec.
FPCS (OSA) 0.967 0.966 0.966 0.822 0.828 0.825PCS: No Pros &Cons 0.925 0.980 0.952 0.848 0.582 0.690PCS (PMI) 0.967 0.961 0.964 0.804 0.828 0.816Now let us look at only the 187 sentences thatneed context dependency handling.
The data isstill skewed.
72.2% of the sentences have En-tityS1 as the preferred entities.
Table 3 shows theresults of PCS with and without using Pros andCons.
The results of PCS without Pros and Cons(OSA or PMI is not needed) are based on assign-ing EntityS1 as preferred for every sentence (tak-ing the majority).
Again, we can see that usingexternal Pros and Cons (PCS(OSA)) helps dra-matically.
Not surprisingly, the improvementsare statistically significant except the recall whenEntityS1 is preferred.Table 3: Preferred entity identification with 187sentences that need context dependency handlingEntityS1 Preferred EntityS2 PreferredPrec.
Rec.
F Prec.
Rec.
FPCS (OSA) 0.896 0.877 0.886 0.696 0.736 0.716PCS: No Pros &Cons 0.722 1.000 0.839 0.000 0.000 0.000PCS (PMI) 0.894 0.855 0.874 0.661 0.736 0.696OSA vs. PMI: Comparing PCS(OSA) with PCS(PMI) (Table 3), OSA is better in F-score whenEntityS1 is preferred by 1.2%, and better in F-score when EntityS2 is preferred by 2%.
Al-though OSA?s improvements over PMI are notlarge, we believe that in principle OSA is a moresuitable measure.
Comparing with PMI when thewhole dataset is used (Table 2), OSA?s gains areless because the number of sentences requiringcontext dependency handling is small (22%).6 ConclusionsThis paper studied sentiments expressed in com-parative sentences.
To our knowledge, no workhas been reported on this topic.
This paper pro-posed an effective method to solve the problem,which also deals with context based sentimentsby exploiting external information available onthe Web.
To use the external information, weneeded a measure of association of the compara-tive word and the entity feature.
A new measure,called one-side association (OSA), was then pro-posed.
Experimental results show that the tech-nique produces accurate results.ReferencesBos, J. and Nissim, M. An Empirical Approach to theInterpretation of Superlatives.
EMNLP-06, 2006.Esuli, A and Sebastiani, F. Determining Term Subjec-tivity and Term Orientation for Opinion Mining,EACL?06, 2006.Fiszman, M., Demner-Fushman, D., Lang, F., Goetz,P., and Rindflesch, T. Interpreting ComparativeConstructions in Biomedical Text.
BioNLP, 2007.Gamon, M., Aue, A., Corston-Oliver, S. and Ringger,E.K.
Pulse: Mining customer opinions from freetext.
IDA?2005.Hatzivassiloglou, V. and McKeown, K. Predicting theSemantic Orientation of Adjectives.
ACL-EACL?97.Hu, M and Liu, B.
Mining and summarizing customerreviews.
KDD?04, 2004.Jindal, N. and Liu, B.
Mining Comparative Sentencesand Relations.
AAAI?06, 2006.Kanayama, H and Nasukawa, T. Fully automatic lex-icon expansion for domain-oriented sentiment anal-ysis.
EMNLP?06.Kim, S. and Hovy, E. Determining the Sentiment ofOpinions.
COLING?04, 2004.Liu, B, Hu, M. and Cheng, J.
Opinion Observer: Ana-lyzing and Comparing Opinions on the Web.WWW?05, 2005.Mei, Q., Ling, X., Wondra, W., Su, H. and Zhai, C.Topic Sentiment Mixture: Modeling Facets andOpinions in Weblogs.
WWW?07, 2007.Pang, B., Lee, L. and Vaithyanathan, S. Thumbs up?Sentiment Classification Using Machine LearningTechniques.
EMNLP?02, 2002.Popescu, A.-M. and Etzioni, O.
Extracting ProductFeatures and Opinions from Reviews.
EMNLP?05.Riloff, E & Wiebe, J.
Learning extraction patterns forsubjective expressions.
EMNLP?03, 2003.Stoyanov, V. and Cardie, C. Toward opinion summa-rization: Linking the sources.
In Proc.
of the Work-shop on Sentiment and Subjectivity in Text, 2006.Turney, P. Thumbs Up or Thumbs Down?
SemanticOrientation Applied to Unsupervised Classificationof Reviews.ACL-2002.Wiebe, J. and Mihalcea, R. Word Sense and Subjec-tivity.
ACL?06, 2006.Wilson, T., Wiebe, J. and Hwa, R. Just how mad areyou?
Finding strong and weak opinion clauses.AAAI?04, 2004.248
