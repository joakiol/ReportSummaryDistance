The Structure of Shared Forestsin Ambiguous ParsingSylvie Billot t" Bernard Lang*INRIArand Universit~ d'Orl~ansbillotGinria.inria.fr langGinria.inria.frAbst rac tThe Context-Free backbone of some natural anguage ana-lyzers produces all possible CF parses as some kind of sharedforest, from which a single tree is to be chosen by a disam-biguation process that may be based on the finer features ofthe language.
We study the structure of these forests withrespect o optimality of sharing, and in relation with theparsing schema used to produce them.
In addition to a theo-retical and experimental framework for studying these issues,the main results presented are:- sophistication i chart parsing schemata (e.g.
use oflook-ahead) may reduce time and space efficiency instead ofimproving it,- there is a shared forest structure with at most cubic sizefor any CF grammar,- when O(n 3) complexity is required, the shape of a sharedforest is dependent on the parsing schema used.Though analyzed on CF grammars for simplicity, these re-sults extend to more complex formalisms such as unificationbased grammars.Key words: Context-Free Parsing, Ambiguity, DynamicProgramming, Earley Parsing, Chart Parsing, ParsingStrategies, Parsing Schemata, Parse Tree, Parse Forest.1 Introduct ionSeveral natural language parser start with & pure Conte~zt.Free (CF) backbone that makes a first sketch of the struc-ture of the analyzed sentence, before it is handed to a moreelaborate analyzer (possibly a coroutine), that takes into ac-count the finer grammatical structure to filter out undesir-able parses (see for example \[24,28\]).
In \[28\], Shieber sur-veys existing variants to this approach before giving his owntunable approach based on restrictions that ~ split up theinfinite nonterminal domain into a finite set of equivalenceclasses that can be used for parsing".
The basic motivationfor this approach is to benefit from the CF parsing technol-ogy whose development over 30 years has lead to powerfuland ei~cient parsers \[I,7\].A parser that takes into account only an approximation fthe grammatical features will often find ambiguities it can-not resolve in the analyzed sentences I.
A natural solution*Address: INRIA, B.P.
105, 78153 Le Chesn~y, France.The work reported here was partially supported by the EurekaSoftware Factory project.1 Ambiguity may also have a semantical origin.
"is then to produce all possible parses, according to the CFbackbone, and then select among them on the basis of thecomplete features information.
One hitch is that the num-ber of parses may be exponential in the size of the inputsentence, or even infuite for cyclic grammars or incompletesentences \[16\].
However chart parsing techniques have beendeveloped that produce an encoding of all possible parses asa data structure with a size polynomial in the length of theinput sentence.
These techniques are all based on a dynamicprogramming paradigm.The kind of structure they produce to represent all parsesof the analyzed sentence is an essential characteristic of thesealgorithm.
Some of the published algorithms produce onlya chart as described by Kay in \[14\], which only associatesnonterminal categories to segments of the analyzed sentence\[11,39,13,3,9\], and which thus still requires non-trivial pro-ceasing to extract parse-trees \[26\].
The worst size complex-ity of such a chart is only a square function of the size of theinput 2.However, practical parsing algorithms will often produce amore complex structure that explicitly relates the instancesof nonterminals a sociated with sentence fragments to theirconstituents, possibly in several ways in case of ambiguity,with a sharing of some common subtrees between the distinctambiguous parses \[7,4,24,31,25\] ~One advantage of this structure is that the chart retainsonly these constituents hat can actually participate in aparse.
Furthermore it makes the extraction of parse-treesa trivial matter.
A drawback is that this structure may becubic in the length of the parsed sentence, and more gener-ally polynomial' for some proposed algorithms \[31\].
How-ever, these algorithms are rather well behaved in practice,and this complexity is not a problem.In this paper we shall call shared forests uch data struc-2 We do not consider CF reco~zers that have asymptoticallythe lowest complexity, but are only of theoretical interest here\[~S,5\].3 There are several other published implementation f chartparsers \[23,20,33\], hut they often do not give much detail on theoutput of the parsing process, or even side-step the problem ~1.together \[33\].
We do not consider here the well .formed s~bs~ringfablea of Shell \[26\] which falls somewhere inbetween i  our classi-ficgtlon.
They do not use pointers and parse-trees are only "indi-rectly" visible, but may be extracted rather simply in linear time.?
The table may contain useless constituents.4 Space cubic algorithms often require the lan~tage grammar tobe in Chomsky Normal Form, and some authors have incorrectlyconjectured tha~ cubic complexity cannot he obtained otherwise.143tures used to represent simultaneously all parse trees for agiven sentence.Several question?
may be asked in relation with sharedforests:?
How to construct hem during the parsing process??
Can the cubic complexity be attained without modify-ing the grammar (e.g.
into Chomsky Normal Form)?s What is the appropriate data structure to improvesharing and reduce time and space complexity??
How good is the sharing of tree fragments betweenambiguous parses, and how can it be improved??
Is there a relation between the coding of parse-trees inthe shared forest and the parsing schema used??
How well formalized is their definition snd construc-tion?These questions are of importance in practical systemsbecause the answers impact both the performance and theimplementation techniques.
For example good sharing mayallow a better factorization of the computation that filtersparse trees with the secondary features of the language.
Therepresentation needed for good sharing or low space com-plexity may be incompatible with the needs of other com-ponents of the system.
These components may also makeassumptions about this representation that are incompatiblewith some parsing schemata.
The issue of formalization is ofcourse related to the formal tractability of correctness prooffor algorithms using shared forests.In section 2 we describe a uniform theoretical framework inwhich various parsing strategies are expressed and comparedwith respect o the above questions.
This approach as beenimplemented into a system intended for the experimentalstudy and comparison of parsing strategies.
This system isdescribed in section 3.
Section 4 contain~ a detailed exampleproduced with our implementation which illustrates both theworking of the system and the underlying theory.2 A Un i fo rm FrameworkTo discus?
the above issue?
in a uniform way, we need a gen-era\] framework that encompasses all forms of chart parsingand shared forest building in a unique formalism.
We shalltake a?
a l~sk a formalism developed by the second authorin previous papers \[15,16\].
The idea of this approach is toseparate the dynamic programming construct?
needed for ef-ficient chart parsing from the chosen parsing schema.
Com-parison between the classifications ofKay \[14\] and Gritfith &Petrick \[10\] shows that a parsing schema (or parsing strat-egy) may be expressed in the construction of a Push-DownTransducer (PDT), a well studied formalization of left-to-right CF parsers 5.
These PDTs are usually non-deterministicand cannot be used as produced for actual parsing.
Theirbacktrack simulation does not alway?
terminate, and is oftentime-exponential when it does, while breadth-first simula-tion is usually exponential for both time and space.
However,by extending Earley's dynamic programming construction toPDTs, Long provided in\[15\] a way of simulating all possiblecomputations of any PDT in cubic time and space complex-s Grifllth & Petrick actually use Turing ma,'hines for pedagog-ical reasons.ity.
This approach may thus be used as a uniform frameworkfor comparing chart parsers s.2 .1  The  a lgor i thmThe following is a formal overview of parsing by dynamicprogramming interpretation of PDT?.Our ahn is to parse sentences in the language ?
(G)  gen-erated by a CF phrase structure grammar G -- (V, ~,  H, N)according to its syntax.
The notation used is V for the setof nontermln~l, ~ for the set of terminals, H for the rules,for the initial nonterminal, and e for the empty string.We assume that, by some appropriate parser constructiontechnique (e.g.
\[12,6,1\]) we mechanically produce from thegrammar G a parser for the language ?
(G)  in the form ofa (possibly non-deterministic) push.down transducer (PDT)T G. The output of each possible computation of the parseris a sequence of rules in r l  ~ to be used in a left-to-rightreduction of the input sentence (this is obviously equivalentto producing a parse-tree).We assume for the PDT T G a very general formal defini-tion that can fit most usual PDT construction techniques.
Itis defined as an 8-tuple T G -- (Q, \]~, A ,  H, 6, ~, ;,  F) where:Q is the set of states, ~ is the set of input word symbols, Ais the set of stack symbols, H is the set of output symbols s(i.e.
rule?
of G), q is the initial state, $ is the initial stacksymbol, F is the set of final states, 6 is a fnite set of tran-sitions of the form: (p A a ~-* q B u) with p, q E Q,x,s ?
A u {e}, a E ~: u {~}, and .
~ H* .Let the PDT be in a configuration p -- (p Aa az u) wherep is the  current state, Aa is the ?tack contents with A onthe top, az is the remaining input where the symbol a is thenext to be shifted and z E ~*,  and u is the already producedoutput.
The application of a transition r = (p A a ~-* qB v)result?
in a new configuration p' ---- (q Bot z uv) where theterminal symbol a has been scanned (i.e.
shifted), A has beenpopped and B has been pushed, and t, has been concatenatedto the existing output ,~ If the terminal symbol a is replacedby e in the transition, no input symbol is scanned.
If A (reap.B) is replaced by ?
then no stack symbol is popped from (resp.pushed on) the ?tack.Our algorithm consist?
in an Earley-like 9 simulation of thePDT T G.  Using the terminology of \[1\], the algorithm buildsan item set ,~ successively for each word symbol z~ holdingposition i in the input sentence z.
An item is constitutedof two modes of the form (p A i) where p is a PDT state,A is a stack symbol, and i.is the index of an input symbol.The item set & contains items of the form ((p A i) (q B j)) .These item?
are used as nontermineds of an output grammarS The original intent of \[15\] was to show how one can generateefficient general CF chart parsers, by first producing the PDT withthe efllcient echniques for deterministic parsing developed for thecompiler technology \[6,12,1\].
This idea was later successfu/ly usedby Tomits \[31\] who applied it to LR(1) parsers \[6,1\], and later toother puelulown based parsers \[32\].7 Implomczxtations u ually dc~ote these rules by their index inthe set r l .s Actual implementations use output symbols from r Iu~,  sincerules alone do not distinguish words in the same lexical category.s We asmune the reader to be familiar with some variation ofEarley's algorithm.
Earley's original paper uses the word stere(from dynamic programming terminology) instead of item.144= (8, l'I, ~,  U~), where 8 is the set of all items (i.e.
theunion of &), and the rules in ~ are constructed togetherwith their left-hand-side item by the algorithm.
The initialnonterminal Ut of ~ derives on the last items produced by asuccessful computation.Appendix A gives the details of the construction of itemsand rules in G by interpretation f the transitions of the PDT.More details may be found in \[15,16\].2.2 The shared forestAn apparently major difference between the above algorithmand other parsers is that it represents a parse as the string ofthe grammar rules used in a leftmost reduction of the parsedsentence, rather than as a parse tree (cf.
section 4).
Whenthe sentence has several distinct paxses, the set of all possi-ble parse strings is represented in finite shared form by a CFgrammar that generates that possibly infinite set.
Otherpublished algorithms produce instead a graph structure rep-resenting all paxse-trees with sharing of common subpaxts,which corresponds well to the intuitive notion of a sharedforest.This difference is only appearance.
We show here in sec-tion 4 that the CF grammar of all leftmost parses is just atheoretical formalization of the shared.forest graph.
Context-Free grammars can be represented by AND-OR graphs thatare closely related to the syntax diagrams often used to de-scribe the syntax of programming languages \[37\], and to thetransition etworks of Woods \[22\].
In the case of our gram-mar of leftmost parses, this AND-OR graph (which is acyclicwhen there is only finite ambiguity) is precisely the shaxed-forest graph.
In this graph, AND-nodes correspond to theusual parse-tree nodes, whil~ OR-nodes correspond to xmbi-guities, i.e.
distinct possible subtrees occurring in the samecontext.
Sharing ofsubtrees in represented by nodes accessedby more than one other node.The grammar viewpoint is the following (cf.
the examplein section 4).
Non-terminal (reap.
terminal) symbols corre-spond to nodes with (reap.
without) outgoing arcs.
AND-nodes correspond to right-hand sides of grammar ules, andOR-nodes (i.e.
ambiguities) correspond to non-terminals de-fined by several rules.
Subtree sharing is represented by seVoeral uses of the same symbol in rule right-hand sides.To our knowledge, this representation f parse-forests agrammars i the simplest and most tractable theoretical for-malization proposed so far, and the parser presented here isthe only one for which the correctness of the output gram-mar - -  i.e.
of the shared-forest - - has ever been proved.Though in the examples we use graph(ical) representationsfor intuitive understanding (grammars axe also sometimesrepresented as graphs \[37\]), they are not the proper formaltool for manipulating shared forests, and developing formal-ized (proved) algorithms that use them.
Graph formalizationis considerably more complex and awkward to manipulatethan the well understood, specialized and few concepts ofCF grammars.
Furthermore, unlike graphs, this grammarformalization of the shared forest may be tractably extendedto other grammatical formalisms (ct: section 5).More importantly, our work on the parsing of incompletesentences \[16\] has exhibited the fundamental character ofour grammatical view of shared forests: when parsing thecompletely unknown sentence, the shared forest obtained isprecisely the complete grammar of the analyzed language.This also leads to connections with the work on partial eval-nation \[8\].2.3 The shape o f  the  fo res tFor our shared-forest, x cubic space complexity (in the worstcase - -  space complexity is often linear in practice) isachieved, without requiring that the language grammar be inChonmky Normal Form, by producing a grammar of parsesthat has at most two symbols on the right-hand side of itsrules.
This amounts to representing the list of sons of a parsetree node as a Lisp-like list built with binary nodes (see fig-ures 1 L- 2), and it allows partial sharing of the sons i0The structure of the parse grammar, i.e.
the shape of theparse forest, is tightly related to the parsing schema used,hence to the structure of the possible computation of thenon-deterministic PDT from which the parser is constructed.First we need a precise characterization f parsing strategies,whose distinction is often blurred by superimposed optimiza-tions.
We call bottom-up a strategy in which the PDTdecides on the nature of a constituent (i.e.
on the grammarrule that structures it), after having made this decision firston its subconstituents.
It corresponds to a postfix left-to-right walk of the parse tree.
Top-Down parsing recognizesa constituent before recognition of its subconstituents, andcorresponds to a prefix walk.
Intermediate strategies are alsopossible.The sequence of operations of a bottom-up arser is basi-cally of the following form (up to possible simplifying oi>.timizations): To parse a constituent A, the parser firstparses and pushes on the stack each sub-constituent B~; atsome point, it decides that it has all the constituents ofA on the stack and it pops them all, and then it pushesA and outputs the (rule number ~- of the) recognized rulef : A -* Bl .
.
.
Bn,.
Dynamic programming interpretationof such a sequence results in a shared forest containing parse-trees with the shape described in figure 1, i.e.
where eachnode of the forest points to the beginning of the llst of itssons.A top-down PDT uses a different sequence of operations,detailed in appendix B, resulting in the shape of figure 2where a forest node points to the end of the list of sons, whichis itself chained backward.
These two figures are only simpleexamples.
Many variations on the shape of parse trees andforests may be obtained by changing the parsing schema.Sharing in the shared forest may correspond to sharing ofa complete subtree, but also to sharing of a tail of a llst ofsons: this is what allows the cubic complezity.
Thus bottom-up parsing may share only the rightmost subconstituents of aconstituent, while top-down parsing may share only the left-most subconstituents.
This relation between parsing schemaand shape of the shared forest (and type of sharing) is a con-sequence of intrinsic properties of chart parsing, and not ofour specific implementation.It is for example to be expected that the bidirectional na-ture of island parsing leads to irregular structure in sharedforests, when optimal sharing is sought for.3 Implementation and ExperimentalResultsThe ideas presented above have been implemented in an ex-perimental system called Tin (after the woodman of OZ).10 This was noted by Shell \[26\] and is implicit in his use of "2-form ~ grammars.145A AFigure 1: Bottom-up arse-tree'Figure 2: Top-down parse-treeThe intent is to provide a uniform f~amework for the con-struction and experimentation f chart parsers, somewhatas systems like MCHART \[29\], but with a more systematictheoretical foundation.
The kernel of the system is a virtualparsing machine with a stack and a set of primitive com-mands corresponding essentially to the operation of a practi-cal Push-Down Transducer.
These commands include for ex-ample: push (resp.
pop) to push a symbol on the stack (reap.pop one), check~indow tocompare the look-ahead symbol(s)to some given symbol, chsckstack to branch depending onthe top of the sta~k, scan to read an input word, outpu$ tooutput a rule number (or a terminal symbol), goto for uncon-ditional jumps, and a few others.
However theae commandsare never used directly to program parsers.
They are used asmachine instructions for compilers that compile grammaticaldefinitions into Tin code according to some parsing schema.A characteristic of these commands i  that they may all bemarked as non-determlnistic.
The intuitive interpretation isthat there is a non-deterministic choice between a commandthus marked and another command whose address in thevirtual machine code is then specified.
However execution ofthe virtual machine code is done by an all-paths interpreterthat follows the dynamic programming strategy described insection 2.1 and appendix A.The Tin interpreter is used in two different ways:1. to study the effectiveness for chart parsing of knownparsing schemata designed for deterministic parsing.We have only considered formally defined parsingschemata, corresponding to established PDA construc-tion techniques that we use to mechanically translateCF grammars into Tin code.
(e.g.
LALR(1) andLALR(2) \[6\], weak precedence \[12\], LL(0) top-down(recursive descent), LR(0), LR(1) \[1\] ...).2. to study the computational behavior of the generatedcode, and the optimization techniques that could beused on the Tin code - -  and more generally chartparser code - -  with respect to code size, executionspeed and better sharing in the parse forest.Experimenting with several compilation schemata hasshown that sophistication may have a negative effect on theej~iciency of all-path parsin911 .
Sophisticated PDT construc-tion techniques tend to multiply the number of special cases,thereby increasing the code size of the chart parser.
Some-times it also prevents haring of locally identical subcom-putations because of differences in context analysis.
Thisin turn may result in lesser sharing in the parse forest andsometimes longer computation, as in example $BBL in ap-pendix C, but of course it does not change the set of parse-trees encoded in the forest 12.
Experimentally, weak prece-dence gives slightly better sharing than LALR(1) parsing.The latter is often v/ewed as more efficient, whereas it onlyhas a larger deterministic domain.One essential guideline to achieve better sharing (and oftenalso reduced computation time) is to try to recognize verygrammar rule in only one place of the generated chart parsercode, even at the cost of increasing non-determinism.Thus simpler schemata such as precedence, LL(0) (andprobably LR(0) I~) produce the best sharing.
However, sincethey correspond to a smaller deterministic domain within theCF grammar realm, they may sometimes be computationallyless efficient because they produce a larger number of uselessitems (Le.
edges) that correspond to dead-end computationalpaths.Slight sophistication (e.g.
LALR(1) used by Tomita in\[31\], or LR(1) ) may slightly improve computational per-formance by detecting earlier dead-end computations.
Thismay however be at the expense of the forest sharing quality.More sophistication (say LR(2)) is usually losing on bothaccounts as explained earlier.
The duplication of computa-tional pgths due to distinct context analysis overweights he11 We mean here the sophistication f the CF parser construc-tion technique rather than the sophistication f the language f a-tures chopin to be used by this parser.l~ This negative behavior of some techniques originally intendedto preserve determlni~n had beam remarked and analyzed in aspecial case by Bouckaert, Pirotte and Shelling \[3\].
However webelieve their result o be weaker than ours, since it seems to relyon the fact that they directly interpret ~'anuuars rather than firstcompile them.
Hence each interpretive step include in some sensecompilation steps, which are more expensive when look-ahead isincreased.
Their paper presents several examples that run less ef-ficiently when look-ahead isincreased.
For all these examples, thisbehavior disappears in our compiled setting.
However the gram-mar SBBL in appendix C shows a loss of eltlciency with increasedlook-ahead that is due exclusively to loss of sharing caused by ir-relevant contextual distinctions.
This effect is particularly visiblewhen parsing incomplete s ntences \[16\].Eiticiency loss with increased look-ahead ismainly due to statesplitting \[6\].
This should favor LALR techniques ova- LR ones.is Our resnlts do not take into account anewly found optimiza-tion of PDT interpretation that applies to all and only to bottom-up PDTs.
This should make simple bottom-up schemes compet-itive for sharing quality, and even increase their computationalei~ciency.
However it should not change qualitatively the rela-tive performances ofbottom-up arsers, and n~y emphasize evenmore the phenomenon that reduces efficiency when look-ahead in-146benefits of early elimination of dead-end paths.
But therecan be no absolute rule: ff a grammar is aclose" to the LR(2)domain, an LR(2) schema is likely to give the best result formost parsed sentences.Sophisticated schemata correspond also to larger parsers,which may be critical in some natural language applicationswith very large grammars.The choice of a parsing schema depends in fine on thegrammar used, on the corpus (or kind) of sentences tobe an-alyzed, and on a balance between computational and sharingefficiency.
It is best decided on an experimental basis witha system such as ours.
Furthermore, we do not believe thatany firm conclusion limited to CF grammars would be ofreal practical usefulness.
The real purpose of the work pre-sented is to get a qualitative insight in phenomena whichare best exhibited in the simpler framework of CF parsing.This insight should help us with more complex formalisms(cf.
section 5) for which the phenomena might be less easilyevidenced.Note that the evidence gained contradicts he common be-l id that parsing schemata with a large deterministic domain(see for example the remarks on LR parsing in \[31\]) are moreeffective than simpler ones.
Most experiments in this areawere based on incomparable implementations, while our uni-form framework gives us a common theoretical yardstick.4 A Simple Bottom-Up ExampleThe following is a simple example based on a bottom-upPDT generated by our LALR(1) compiler from the followinggrammar taken from \[31\]:I (0) '$ax ::= $ 's $ (1) 's ::= 'up 'vp (2) 'e ::- 's  'pp (3) 'up ::= n (4) 'up : : -  det n (5) 'up : : -  'up 'pp (6) 'pp : : -  prep 'up (7) 'vp : := v 'upNonterminals are prefixed with a quote symbol The firstrule is used for initialization and handlhg of the delimitersymbol 8.
The $ delimiters are implicit in the actual inputsentence.The sample input is a(n v det n prep n) ~.
It figures(for example) the sentence: aT see a man at home ~.4.1 Output  grammar  produced by  the  parserThe grammar of parses of the input sentence is given in fig-ure 3.The initial nonterminal is the left-hand side of the firstrule.
For readability, the nonterminals have been given com-puter generated names of the form at2, where z is an integer.All other symbols are terminal.
Integer terminals correspondto rule numbers of the input language grammar given above,and the other terminals are symbols of the parsed language,except fo r the special terminal %i1" which indicates the endof the list of subconstituents of a sentence constituent, andmay also be read as the empty string ~.
Note the ambiguityfor nontermlnal t4.It is possible to simplify this grammar to 7 rules withoutlosing the sharing of common subparses.
However it wouldno longer exhibit the structure that makes it readable as ashared-forest (though this structure could be retrieved).nt0 : := nt l  0 n t l9  : := nt20 n i ln t l  : := nt2 nt3 nt20 : := nnt2 : : -  $ at21 : : -  nt22 n i lnt3 : := nt4 nt37 nt22 : := nt23 6at4 : := at5 2 at23 : := at24 nt25nt4 : := nt29 1 nt24 : := prepnt5 : := nt6 nt21 nt25 : := nt26 n i lnt6 : := nt7 1 nt26 : := nt27 3nit ::= nt8 ntll nt27 ::= nt28 nilat8 ::- at9 3 nt28 ::= nnt9 : :=nt lO nil at29 ::- nt8 nt30nil0 ::- n at30 ::= nt31 nilntll ::- nil2 nil at31 ::= at32 7nil2 ::= nil3 7 at32 ::= nil4 at33n~13 ::= nil4 nil5 nt33 ::= nt34 nilnil4 ::- v at34 ::= nt35 5nt15 ::= nil6 nil nt35 ::= nil6 nt36nil6 ::= at17 4 nt36 ::= nt22 nilnil7 ::= ntl8 ntl9 nt37 ::= nt38 nilnt18 : := det nt38 : := $Figure 3: Grammar  of parses of the input sentenceThe two parses of the input sentence defined by this gram-mar are: $ n 3 v det n 4 7 1 prep n 3 6 2 $$ n 3 vdet  n 4 prepn  3 6 5 7 1 $Here again the two $ symbols must be read as delimiters.The ~1"  symbols, no longer useful, have been omitted inthese two parses.4.2 Parse  shared- fo res t  const ructed  f i 'om thatgrnnalxlarTo explain the structure of the shared forest, we first build agraph from the grammar, as shown in figure 4.
Each nodecorresponds to one terminal or nonterminal of the grammarin figure 3, and is labelled by it.
The labels at the rightof small dashes are rule numbers from the parsed languagegrammar (see beginning of section 4).
The basic structure isthat of figure 1.From this first graph, we can trivially derive the more tra-ditional shared forest given in figure 5.
Note that this simpli-fied representation is not always adequate since it does notallow partial sharing of their sons between two nodes.
Eachnode includes a label which is a non-terminal of the parsedlanguage grammar, and for each possible derivation (severalin case of ambiguity) there is the number of the grammarrule used for that derivation.
Though this simplified versionis more readable, the representation f figure 5 is not ade-quate to represent partial sharing of the subconstituents ofa constituent.Of course, the ~constructions ~ given in this section arepurely virtual.
In an implementation, the data-structure rep-resenting the grammar of figure 3 may be directly interpretedand used as a shared-forest.A similar construction for top-down parsing is sketched inappendix B.1470--@I1~ 3 37"~ I"2 38$ $5 21 .--\]l.
29 ~ 30- -~""-r'i .
?
"-') ? '
f - '9~.
13--f-~ ~ -~," z.t - I ,  3s 3e--l,"10 14 16B4 28rt V \] fl EI17 ~ 19"-~'I I18 20det nFigure 4: Graph of the output grammarNP4n v det  nPP6prep  n .
,Figure 5: The shared forest5 ExtensionsAs indicated earlier, our intent is mostly to understand phe-nomena that would be harder to evidence in more complexgrammatical formalisms.This statement implies that our approach can be extended.This is indeed the case.
It is known that many simple parsingschemata can be expressed with stack based machines \[32\]?This is certainly the case for M!
left-to-right CF chart parsingschemata.We have formally extended the concept of PDA into thatof Logical PDA which is an operational push-down stack de-vice for parsing unification based grammars \[17,18\] or othernon-CF grammars uch as Tree Adjoining Grammars \[19\].Hence we axe reusing and developing our theoretical \[18\] andexperimental \[38\] approach in this much more general set-ting which is more likely to be effectively usable for naturallanguage parsing.Furthermore, these extensions can also express, within thePDA model, non-left-to-fight behavior such as is used in is-land parsing \[38\] or in Shei\]'s approach \[26\]?
More generallythey allow the formal analysis of agenda strategies, whichwe have not considered here.
In these extensions, the coun-terpart of parse forests are proof forests of definite clauseprograms.6 ConclusionAnMysis of Ml-path parsing schemata within a commonframework exhibits in comparable terms the properties ofthese schemata, and gives objective criteria for chosing agiven schema when implementing a language analyzer.
Theapproach taken here supports both theoreticM analysis andactuM experimentation, both for the computational behaviorof pLmers and for the structure of the resulting shared forest.Many experiments and extensions till remain t 9 be made:improved dynamic programming interpretation of bottom-up parsers, more extensive xperimental measurements witha variety of languages and parsing schemata, or generaliza-tion of this approach to more complex situations, such asword lattice parsing \[21,30\], or even handling of "secondary"language features.
Early research in that latter direction ispromising: our framework and the corresponding paradigmfor parser construction have been extended to full first-orderHorn clauses \[17,18\], and are hence applicable to unificationbased grammatical formalisms \[27\].
Shared forest construc-tion and analysis can be generalized in the same way to thesemore advanced formalisms.Acknowledgements :  We are grateful to V~roniqueDonzeau-Gouge for many fruitful discussions.This work has been partially supported by the EurekaSoftware Factory (ESF) project.References\[1\] Aho, A.V.
; and Ullman, J.D?
1972 The Theoryof Parsing, Trar~lation and Compiling.
Prentice-Hall, Englewood Cliffs, New Jersey.\[2\] Billot~ S. 1988 Analyseurs Syntaxiques et Non.D6terminigme.
Th~se de Doctorat, Universit~d'Of l~ns la Source, Orleans (France).148\[3\] Bouckaert, M.; Pirotte, A~; and Sn~lllng, M. 1975Efficient Parsing Algorithms for General Context-Free Grammars.
Information Sciences 8(1): 1-26\[4\] Cooke, J.; ~nd Schwartz, J.T.
1970 ProgrammingLanguages and Their Compilers.
Courant Insti-tute of Mathematical Sciences, New York Univer-sity, New York.\[5\] Coppersmith, D.; and Winograd, S. 1982 On theAsymptotic Complexity of Matrix Multiplication.SIAM Journal on Computing, 11(3): 472-492.\[6\] DeRemer, F.L.
1971 Simple LR(k) Grammars.Communications A CM 14(7): 453-460.\[7\] Earley, J.
1970 An Efficient Context-Free ParsingAlgorithm.
Communications ACM 13(2): 94-102.\[8\] Fntamura, Y.
(ed.)
1988 Proceedings ofthe Work-shop on Paxtial Evaluation and Mixed Computa-tion.
New Generation Computing 6(2,3).\[9\] Graham, S.L.
; Harrison, M.A.
; and Ruzzo W.L.1980 An Improved Context-Free Recognizer.
A CMTransactions on Programming Languages and Sys-tems 2(3): 415-462.\[10\] Griffiths, L; and Petrick, S. 1965 On the RelativeEfficiencies of Context-Free Grammar Recogniz-ers.
Communications A CM 8(5): 289-300.\[11\] Hays, D.G.
1962 Automatic Language-Data Pro-ceesing.
In Computer Applications in the Behav-ioral Sciences, (H. Borko ed.
), Prentice-Hall, pp.394-423.\[12\] Ichbiah, J.D.
; and Morse, S.P.
1970 A Techniquefor Generating Almost Optimal Floyd-Evans Pro-ductions for Precedence Grammars.
Communica-tions ACM 13(8): 501-508.\[13\] Kuami, J.
1965 An E~icient Recognition andSlmtax Analysis Algorithm .for Context-Free Lan.geages.
Report of Univ.
of Hawaii, also AFCRL-65-758, Air Force Cambridge Research Labor~-tory, Bedford (Massachusetts), also 1968, Univer-sity of Illinois Coordinated Science Lab.
Report,No.
R-257.\[14\] Kay, M. 1980 Algorithm Schemata nd DataStructures in Syntactic Processing.
Proceedings oythe Nobel Symposium on Text Processing, Gothen-burg.\[15\] Lung, B.
1974 Deterministic Techniques for Effi-cient Non-deterministic Parsers.
Proc.
oy the 2 "~Colloquium on Automata, Languages and Pro-gramming, J. Loeckx (ed.
), Saarbrflcken, SpringerLecture Notes in Computer Science 14: 255-269.Also: Rapport de Recherche 72, IRIA-Laboris,Rocquencourt (France).\[16\] Lung, B.
1988 Parsing Incomplete Sentences.
Proc.of the 12 en Internat.
Cony.
on Computational Lin-guistics (COLING'88) "CoL 1:365-371, D.
Vargha(ed.
), Budapest (Hungary).\[17\] Lung, B.
1988 Datalog Automata.
Proc.
of therd 3 Internat.
Cony.
on Data and Knowledge Bases,C.
Beeri, J.W.
Schmidt, U. Dayal (eds.
), MorganKanfmann Pub., pp.
389-404, Jerusalem (Israel).\[18\] Lung, B.
1988 Complete Evaluation of HornClauses, an Automata Theoretic Approach.
INRIAResearch Report 913.\[19\] LanK, B.
1988 The Systematic Construction ofEadey Parsers: Application to the Production o/O(n 6) Earle~ Parsers for Tree Adjoining Gram-mars.
In preparation.\[20\] Li, T.; and Chun, H.W.
1987 A Massively Psral-lel Network-Based Natural Language Parsing Sys-tem.
Proc.
ol ?nd Int.
Cony.
on Computers andApplications Beijing (Peking), : 401-408.\[21\] Nakagawa, S. 1987 Spoken Sentence Recogni-tion by Time-Synchronous Parsing Algorithm ofContext-Free Grammar.
Proc.
ICASSP 87, Dallas(Texas), Vol.
2 : 829-832.\[22\] Pereira, F.C.N.
; and Warren, D.H.D.
1980 Deft-uite Clause Grammars for Language Analysis - -Asurvey of the Formalism and a Comparison withAugmented Transition Networks.
Artificial Intel.ligence 13: 231-278.\[23\] Phillips, J.D.
1986 A Simple Efficient Parser forPhrase-Structure Grammars.
Quarterly Newslet-ter of the Soc.
for the Study of Artificial Intelli-gence (AISBQ) 59: 14-19.\[24\] Pratt, V.R.
1975 LINGOL - -  A Progress Report.In Proceedings of the Jth IJCAI: 422-428.\[25\] Rekers, J.
1987 A Parser Generator for FinitelyAmbiguous Context-Free Grammars.
Report CS-R8712, Computer Science/Dpt.
of Software Tech-nology, Centrum voor Wiskunde en Informatica,Amsterdam (The Netherlands).\[26\] Sheil, B.A.
1976 Observations on Context FreeParsing.
in Statistical Methods in Linguistics:.
71-109, Stockholm (Sweden), Pros.
of Internat.
Conf.on Computational Linguistics (COLING-76), Or-taw'4 (Canada).Also: Techuical Report TR  12-76, Center for Re-search in Computing Technology, Alken Computa-tion Laboratory, Harvard Univ., Cambridge (Mas-sachusetts).\[27\] Shieber, S.M.
1984 The Design of a ComputerLanguage for Linguistic Information.
Proc.
of the10 'h Internat.
Cony.
on Computational Linguistics--  COLING'84: 362-366, Stanford (California).\[28\] Shieber, S.M.
1985 Using Restriction to ExtendParsing Algorithms for Complex-Feature-BasedFormalisms.
Proceedings oy the ~3rd Annual Meet-ing of the Association for Computational Linguis-tics: 145-152.\[29\] Thompson, H. 1983 MCHART: A Flexible, Mod-ular Chart Parsing System.
Proc.
of the NationalConf.
on Artificial Intelligence (AAAI-83), Wash-ington (D.C.), pp.
408-410.\[30\] Tomita, M. 1986 An Efficient Word Lattice Pars-ing Algorithm for Continuous Speech Recognition.In Proceedings oy IEEE-IECE-ASJ InternationalConference on Acoustics, Speech, and Signal Pro-?essing (ICASSP 86), Vol.
3: 1569-1572.\[31\] Tomita, M. 1987 An Efficient Augmented-Context-Free Parsing Algorithm.
ComputationalLinguistics 13(1-2): 31-46.\[32\] Tomita, M. 1988 Graph-structured Stack and Nat-ural Language Parsing.
Proceedings oy the 26 thAnnual Meeting Of the Association for Computa.tional Linguistics: 249-257.149\[33\] Uehaxa, K.; Ochitani, R.; Kaknsho, 0.; Toyoda,J.
1984 A Bottom-UpParser based on PredicateLogic: A Survey of the Formalism and its Im-plementation Technique.
198~ In?ernst.
Syrup.
onLogic P~mming,  Atlantic City (New Jersey), :220-227.\[34\] U.S. Department of Defense 1983 ReferenceManual for the Ada Programming Language.ANSI/MIL-STD-1815 A.I35\] Valiant, L.G.
1975 General Context-Free Recog-nition in Less than Cubic Time.
Journal of Com-puter and System Sciences, 10: 308-315.\[36\] Villemonte de la Clergerie, E.; and Zanchetta, A.1988 Eealuateur de Clauaes de Horn.
Rapport deStage d'Option, Ecole Polytechulque, Palaise&u(n'auce).\[37\] Wirth, N. 1971 The Programming Language Pas-cal.
Acta Informatica, 1(1).\[38\] Ward, W.H.
; Hauptmann, A.G.; Stern, R.M.
; andChanak, T. 1988 Parsing Spoken Phrases DespiteMissing Words.
In Proceedings of the 1988 In-ternational Conference on Acot~tics, Speech, andSignal Processing (ICASSP 88), Vol.
1: 275-278.\[39\] Younger, D.H. 1967 Recognition and Parsing ofContext-Free Languages in Time n 3.
Informationand Control, 10(2): 189-208A The a lgor i thmThis is the formal description of a minimal dynamic pro-gramming PDT interpreter.
The actual Tin interpreter hasa larger instruction set.
Comments are prefixed with ~.- -  Begin parse with input sentence x of length nstep-A: - -  Initialization:=So :=  {~};7, := {~};i := 0;- -  initial item- -  first rule of output grammar- -  initialize item.set So- -  ru les  of  output grammar- -  input-scanner indez is set- -  before the first input symbolstep-B: - -  Iterationwhile i < n loopfor  every i res  U f ( (pA i ) (qB j ) )  in S, dofor  every ~zanai t ion  r in  6 dome consider four kinds of transitions, correspondingto the instructions of a minimal PDT interpreter.i~ r f (p ?e  ~-* fez) then ~ O U T P U T zY :---- ( ( rA i )  CqBj)) ;& := 8, u {v};7' :=  P u {(v - u~)};if r - - - - (pe ?
~-, roe)  then - -PUSHCV :---- ((r O i) (p A i)) ;s, :=&u(V};:= 7' u {(v - ?
)};i f  r=(pAe ~ tee)  then - -PAPAfo r  every il;en Y = ((q B j)  (S D k)) in Sj doV := ((r B i) (s V k)) ;s, := & u {v};7' := 7, u ( (v  - Yu)};i f  r = (p ?a  ~-~ r ?
?)
thenV := ( ( rA i+ l ) (qe j ) ) ;S,+x :=  &+x u {V} ;:= 7, u ( (v  - .
u )} ;i := i+1;.nd loop;step-C: --  Termination:Jar every item O = ( ( f ;  n) (~;  0))such that  fEF  do:=  ~' u (U~ -.
U);- -  Uf is the initial nonterminal of ~.- -  End  o f  parse-- SHIFT ain S.B I n te rpreta t ion  of a top -down PDTTo illustrate the creation of the shared forest, we presenthere informally a simplified sequence of transitions in theirorder of execution by a top-down parser.
We indicate thetransitions as Tin instructions on the left, as defined in ap-pendix A.
On the right we indicate the item and the ruleproduced by execution of each instruction: the item is theleft-hand-side of the rule.The pseudo-instruction scan is given in italics becauseit does not exist, and stands for the parsing of a sub-constituent: either several transitions for a complex con-stituent or a single sh i f t  instruction for a lexical constituent.The global behavior of scan is the same as that of ehif% andit may be understood as a shift on the whole sub-constituent.Items axe represented by a pair of integer.
Hence we giveno details about states or input, but keep just enough infor-mation to see how items axe inter-related when applying apop transition: it must use two items of the form (a,b)  and(b, c) as indicated by the algorithm.The symbol r stands for the rule used to recognize a con-stituent s, and ~ri stands for the rule used to recognize its i 'hsub-constituent ei.
The whole sequence, minus the first andthe last two instructions, would be equivalent to "scan s'.?
.. (6 ,6 )push r (7,6) -> epush r l  (8,7) -> ?scan 81 (9,7) -> (8,7) slout r l  (10,7) -> (9,7) r lpop (11,6) -> (7,6) (10,7)ptmh f2 (12,11) -> ?scan s~ (13,11) -> (12,11) ssout r2 (14,11) -> (13,11) r2pop (15,6) -> (11,6) (14,11)push r~ (16,15) -> ?scan sa (17,15) -> (16,15) s3out r3 (18,15) -> (17,15) ~3pop (19,6) -> (15,6) (18,15)out f (20 ,6 )  -> (19 ,6 )pop (21,5) -> (6 ,5) (20,6), , .This grammar may be simplified by eliminating uselessnon-terminals, deriving on the empty string e or on a singleother non-terminal.
As in section 4, the simplified grammarmay then be represented as a graph which is similar, withmore details (the rules used for the subconstituents), to thegraph given in figure 2.150C Exper imenta l  Compar i sonsThis appc~dlx gives some of the experimental data gathered toc~npa~ compilation achemata~For each grammar, the first table gives the size of the PDTs oh-t~dned by compiling it accordlnZ to several compilation schematLThis size corresponds to the number of instructions genca'ated forthe PDT, which is roughly the n,mher of possible PDT states.The second table gives two figures far each schema nd forsevm-al input sentences.
The first figure is the number of itemscomputed to parse that sentence with the given schema: it maybe read as the number of computation steps and is thus ?
measureof computational ei~ciency.
The second figure is the n,,ml~er ofitems r~n~in;ng after simp/ification of the output grarnm~, it isthus an indicator of shsx~g quality.
Sharing is better when thissecond figure is low.In these tables, columns beaded with LR/LALR stands for theLR(0), LR(1), LALR(1) and LALR(2) cases (which often give thesame results), unlesa one of these cases has its own expl;clt column.Tests were run on the GRE, NSE, UBDA and RR grammanof \[3\]: they did not exhibit the loss of eRiciency with incre~mdlook-ahead that was reported for the bottom-up look-ahead of \[3\].We believe the results presented here axe consistent and givean accurate comparison ofperformances of the parsers considered,despite some implementation departure from the strict theoreticalmodel required by performance onsiderations.
A t int version ofour LL(0) compiler &,ave results that were inconsistent with theresults of the bottom-up arsers.
This was ,, due to & weakness inthat LL(0) compiler which was then corrected.
We consider thisexperience to be a conflrm~ion of the nsefu ln~ of our uniformframework.It must be stressed that these ~re prellmi~L~-y experiments.
Onthe basis of thdr.
~,dysis, we intend a new set of experimentsthat will better exhibit he phenomena discussed in the paper.
Inparticular we wish to study variants of the schen~ta and dynamicprogr~nming interpretation that give the best p,~dble sharing.C.
I  Gr-mmar UBDAit : : 'A ?
J ?LR(0) \[ LR(1) LALR(1) LALR(2)38 60  41 41input stringma&aaaamLR/LALR14-  923-  15249-  156prece(L15-929-  15226 - 124preced.
LL(0)36 46LL(0)41 - 975 - 15391 - 112C.2 Gr-mmar RR?
: : -x ?
l ?gramm~ is LALR(1) but not LR(0), which explains thelower performance of the LR(O) parser.LB.
(0) LR(1) LALR(1) LALR(2) preced.
LL(0)34 37 37 37 48 46input string LR(0) LR/LALR preced.?
14 -9  14-9  15-9xx  23- 13 20- 13 25 - 13xxzxxz 99-  29 44 - 29 56 - 29C.3  Picogrsmmar of EnglishS : :8  EP VP \[ S PPI P  : : "  n J dec  a \[ lip PPVP ::= v irPPP : : ?
prep lipLL(O)28- 943-  13123- 29LR(0) LR(1) LALR(1) LALR(2) preced.
LL(0)110 341 104 104 90 116input string LFt/LALR preced.
LL(0)n ?
n p rep  n 71.47 72 - 47  169  - 43n ?
n (prep n) 2 146 - 97 141 - 93 260 - 77n ?
u (Fep  n) 3 260 - 172 245 - 161 371 - 122n ?
n (prep n) s 854 - 541 775 - 491 844 - 317C .4  Grammar  o f  Ada  express ionsThis grimm&r, too long for inclusion h~e, is the grammar of ex-pressions of the \]an~cru~e AdsN as given in the reference man-ual \[3@ This grammar isambiguous.In these examples, the use of look-ahead give approximately a25% gain in speed elliciency over LR(0) parsing, with the samefo~t shadng.However the use of look-ahead rn~y increase the LR(1) parsersize quadratically with the granunar size.
Still, a better engineeredLR(1) construction should not usually increase that size as dra-nmticaily as indicated by our experimental figure.LR(0) LR(1) LALR(1) preced.587 32210 534 323input string LIt(0) LR(1) LALR(1)a*3 7'4 - 39 59 - 39 59 - 39(ae3)+b 137- 75 113 - 75 113- 75&*3"I-b**4 169- 81 122 - 81 122 - 81C.5  Grnmmar PBE : :=  a A d \[ ?
Be  \[ b a ?
\[ b B d?
: :meB : :mepreced.80- 39293- 75227 - 81LR(0) LR(1) LALR(1) ~ (2) p~'cd.
LL(0)76 i00 80 84 122Thin ~p-ammar is LR(1) but is not LALR.
For each compilationscb,'ma it gives the same result on all possible inputs: aed, ae?,bec and bed.LR(0) LR(1) LALR(1) & (2) preced.
LL(0)26-15  23-  15 26-  15 29-15 47- 15C.6  Grammar SBBLE : :8  X ?
d J I B c \[ Y ?
c \[ Y B dX : :mrY : :mrA : :=e i \ [  gB : :=eAJ  sLR(0) LR(1) LALR(1) LALR(2) preced.159 294 158 158 104input string LR(0) LR(1) LALR(1) & (2) preced.fegd 50- 21 57 o 37 50 - 21 84 - 36feee~Fl 62 - 29 7'5 - 49 62 - 29 II0 - 44The termln,d f may be ambiguously parsed as X or as Y. Thisambiguous left context increases uselessly the complexity of theLR(1) ~ during recognition ofthe A and B constituents.
HenceLR(0) performs better in this case since it ignores the context.151
