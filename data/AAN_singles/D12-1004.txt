Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 37?47, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsLexical Differences in Autobiographical Narratives from SchizophrenicPatients and Healthy ControlsKai Hong1, Christian G. Kohler2, Mary E. March2, Amber A. Parker3, Ani Nenkova1University of PennsylvaniaPhiladelphia, PA, 19104, USA1{hongkai1,nenkova}@seas.upenn.edu2{kohler,memarch}@mail.med.upenn.edu3{parker}@sas.upenn.eduAbstractWe present a system for automaticidentification of schizophrenic patientsand healthy controls based on narrativesthe subjects recounted about emotionalexperiences in their own life.
The focus of thestudy is to identify the lexical features thatdistinguish the two populations.
We report theresults of feature selection experiments thatdemonstrate that the classifier can achieveaccuracy on patient level prediction as high as76.9% with only a small set of features.
Weprovide an in-depth discussion of the lexicalfeatures that distinguish the two groups andthe unexpected relationship between emotiontypes of the narratives and the accuracy ofpatient status prediction.1 IntroductionRecent studies have shown that automatic languageanalysis can be successfully applied to detectcognitive impairment and language disorders.
Ourwork further extends this line of investigation withanalysis of the lexical differences between patientssuffering from schizophrenia and healthy controls.Prior work has reported on characteristiclanguage peculiarities exhibited by schizophreniapatients.
There are more repetitions in speechof patients compared to controls (Manschreck etal., 1985).
Patients also tend to repeatedly referback to themselves (Andreasen., 1986).
Deviationsfrom normal language use in patients on differentlevels, including phonetics and syntax, have beendocumented (Covington et al2005), howeverlexical differences have not been investigated indetail.In this paper we introduce a dataset ofautobiographical narratives told by schizophrenicpatients and by healthy controls.
The narrativesare related to emotional personal experiences of thesubjects for five basic emotions: ANGER, SAD,HAPPY, DISGUST, FEAR.
We train an SVMclassifier to predict subject status.
Our good resultson the relatively small dataset indicate the potentialof the approach.
An automatic system for predictingpatient status from autobiographical narratives canaid psychiatrists in tracking patients over time andcan serve as an easy way to administer largescale screening.
The detailed feature analysis weperformed also pinpoints key differences betweenthe two populations.We study a range of lexical features includingindividual words, repetitions as well as classesof words defined in specialized dictionariescompiled by psychologists (Section 4).
We useseveral approaches for feature analysis to identifystatistically significant differences in the twopopulations.
There are 169 significant featuresamong all of the 6057 features we examined.Through feature selection we are able to obtain asmall set of 25 highly predictive features whichlead to status classification accuracy significantlybetter than chance (Section 6.3).
We also showthat differences between patients and controls arerevealed best in stories related to SAD and ANGRYnarratives, they are decent in HAPPY stories, andthat distinctions are poor for DISGUST and FEAR(Section 6.5).372 Related WorkResearch in psychometrics has studied patternsof lexical usage in a large variety of scenarios.A popular tool used for psychometric analysisis Linguistic Inquiry and Word Count (LIWC)(Pennebaker et al2007).
One of the mostinteresting discoveries in that line of research is thatpeople with physical or emotional pain are likely touse first-person singular pronouns more often thanthe general population (Rude et al2004).
In theview of therapy, Pennebaker discovered that writingemotional experiences can be helpful in therapeuticprocess (Pennebaker, 1997).
It has also been shownthat the usage of pronouns and function words canbe indicators of writing styles, physical health andother distinctions (Tausczik and Pennebaker, 2010).The combination of natural language processing(NLP) and machine learning (ML) has beenexplored in many psychology related projects,and is gaining popularity.
It has been shownthat features from language models (LMs) canbe used to detect impairment in monolingualand bilingual children (Gabani et al2009).Even better results are achieved when featuresderived from LMs are combined with other surfacefeatures to predict language impairment.
Similarly,studies on child language development and autismhave shown that n-gram cross-entropy from LMsrepresentative of healthy and impaired subjects isa highly significant feature predictive of languageimpairment (Prud?hommeaux et al2011).
Thefeasibility of making use of lexical featuresto analyze language dominance among bilingualchildren has also been confirmed (Solorio et al2011).In non-medically related research, LIWC andlexical features have been used to recognizedifferent personalities such as introvert vs extrovert,openness vs experience, conscientiousness vsunconscientiousness, etc.
(Mairesse et al2007).Similar features have been applied to differentiateauthor personality of e-mails (Gill et al2006),blogs (Gill et al2009) and other documents.Speech-related features and interactional aspectsof dialog behavior such as pauses, fillers, etc,have also been found helpful in identifying autisticpatients (Heeman et al2010).Variables Schizophrenia Control(# Subjects) (n=23) (n=16)Mean age (SD) 33.81 (9.65) 32.29 (6.59)Mean number ofwords per story (SD) 192.22 (122.4) 180.79 (95.87)Table 1: Basic demographic informationSyntax features have been used in approachesof automatic detection of neurological problems.Parsing texts produced by subjects and usingbag of rules as features have been applied inanalyzing language dominance (Solorio et al2011).
Methods that quantify syntactic complexitylike Yngve score and Fraizer score have been usedto analyze autism (Prud?hommeaux et al2011).Moreover, there has been research on detecting mildcognitive impairment, which could be an earlierstate of Alzheimer?s disease: five different waysof evaluating syntactic complexity measures wereintroduced in their paper (Roark et al2011).In our own work, we focus our analysisexclusively on lexical features.
Similarly to priorwork, we present the most significant featuresrelated to differences between schizophrenicpatients and healthy controls.
Unlike prior work,instead of doing class ablation studies we performfeature selection from the full set of availablefeatures and identify a small set of highly predictivefeatures which are sufficient to achieve the topperformance we report.
Such targeted analysisis more helpful for medical professionals as theysearch to develop new therapies and ways to trackpatient status between visits.3 DataFor our experiments we collected autobiographicalnarratives from 39 speakers.
The speakers areasked to tell their experience involving the followingemotions: HAPPY, ANGER, SAD, FEAR andDISGUST, which comprise the set of the five basicemotions (Cowie, 2000).
Most subjects told a singlestory for each of the emotions, some told two.
Thetotal number of stories in the dataset is 201.The stories were narrated in the doctor?s office.The recordings of the narratives were manuallytranscribed in plain text format.
We show age andlength in words of the told stories for the two groups38in Table 1.
There are 23 patients with schizophreniaand 16 healthy controls, telling 120 and 81 storiesrespectively.4 FeaturesHere we introduce the large set of lexical featuresthat we group in three classes: a large class offeatures computed for individual lexical items, basicfeatures, features derived on the basis of pre-existingdictionaries and language model features.
We alsodetail the way we performed feature normalizationand feature selection.4.1 Surface Features4.1.1 Basic FeaturesBasic features include token to type ratio tocapture vocabulary diversity, letters per word, wordsper sentence, sentences per document and wordsper document.
These features describe the generalproperties of the language used by the subject,without focus on specific words.Repetitions, revisions, large amount of fillersor disfluencies can be indicators for languageimpairment.
In our basic features we detect thenumber of repetitions in words, punctuations andsentences for each transcript.
Then these threemeasures are normalized by total number of wordsor sentences.We define repetitions as the occurrence of thesame token in a sliding window of five itemswithin the same sentence.
We count repetitions ofwords and punctuation separately.
The repetitionof punctuation, mostly commas and full-stops, areindicative of phrasing in speech which has beenindirectly captured in the transcript.
Repetition ofany word is counted, regardless of which specificword was repeated.
For example, for the sentence Iam, am, afraid, that something bad would happen.am is counted as repeated once, and comma iscounted as repeated twice.
Finally, sentencerepetition captures the amount of overlapping at thebeginning of two adjacent sentences, defined as thenumber of tokens from the beginning of the sentenceuntil the first token where the two sentences differ.4.1.2 Lexical FeaturesFor words in the vocabulary: we use a realvalue feature equal to the word frequency for eachdocument.
Of particular interest we track the useof pronouns because early research has reported thatpeople with cognitive impairment have a tendencyto use subjective words or referring to themselves(Rude et al2004).In addition, for each word in the vocabulary,we apply the presence of the repetition about oneparticular word.4.1.3 Perplexity from Language ModelsInspired by the predictive power of languagemodel reported in prior work, we also includeseveral language model features.
We build languagemodels on words as well as part-of-speech (POS)tags from Stanford POS-tagger (Toutanova et al2003).
We tried unigram, bigram and trigramlanguage models by word and POS tag.
Experimentsshowed that bigram performed better than random,and the other two performed below random.
Thusin the experiments we report later we train onemodel for patients and one for controls and use theperplexity of a given text according to the bigramlanguage models on word and POS as features inprediction.4.2 Dictionaries: LIWC and DictionText analysis packages have been widely used inresearch related to personality analysis, sentimentalanalysis and psychometric studies.
We use twodictionary-based systems, LIWC (Pennebaker et al2007)1 and Diction2, which both give scores totranscripts based on broad categories.4.2.1 Linguistic Inquiry&Word Count(LIWC)LIWC calculates the degree to which people usedifferent categories of words.
Several manuallycompiled dictionaries are at the heart of theapplication.
Each word or word stem could be inone or more word categories or sub-dictionaries.For instance, the word ?cried?
is part of thefollowing categories: sadness, negative emotion,overall affect, verb, and past tense verb.
Whena narrative contains the word ?cried?, the scalescores corresponding to these five subcategories areincremented.
The final output for each narrative is areal value score for each of the 69 categories.1See http://www.liwc.net2See http://www.dictionsoftware.com39Because of the elaborate development ofdictionaries and categories, LIWC has been usedfor predicting emotional and cognitive problemsfrom subject?s spoken and written samples.Representative applications include studyingattention focus through personal pronouns, studyinghonesty and deception by emotion words andexclusive words and identifying thinking styles(Tausczik and Pennebaker, 2010).
Thus it isreasonable to expect that LIWC derived featureswould be helpful in identifying schizophreniapatients.
In Section 6.4 we discuss in more detailthe features which turned out to be significantlydifferent between patients and controls withinLIWC.4.2.2 DictionWe also use Diction to analyze the lexicalcharacteristics of the transcripts.
Similar toLIWC, Diction scores are computed with referenceto manually compiled dictionaries.
The mastervariable scores in Diction include activity, certainty,commonality, optimism and realism.
These fivemain scores are computed with 33 dictionaries thatdefine pertinent subcategories.
The master variablescores are constructed as follows: Sm =?ni=1 ai ?
?mj=1 sj , where ai are additive traits, sj aresubtractive traits (giving positive/negative evidencefor the presence of the feature, respectively).For example, Certainty and Realism scores arecalculated as follows:Realism = [Familiarity + Spatial Awareness +Temporal Awareness + Present Concern + HumanInterest + Concreteness] - [Past Concern +Complexity]Certainty = [Tenacity + Leveling + Collectives +Insistence] - [Numerical Terms + Ambivalence +Self Reference + Variety]We also give definitions for some importantcategories.
The complete description of categoriesis available in the Diction manual (Hart, 2000).Cognition: Words referring to cerebral processes,both functional and imaginative.Satisfaction: Terms associated with positiveaffective states.Insistence: A measure of code-restriction andcontentedness, with the assumption that therepetition of key terms indicates a preference for alimited, ordered world.Diversity: Words describing individuals or groupsof individuals differing from the norm.Familiarity: Consisted of the most common wordsin English.Certainty: Language indicating resoluteness,inflexibility, and completeness and a tendency tospeak ex cathedra.Realism: Language describing tangible, immediate,recognizable matters that affect people?s everydaylives.4.3 Feature normalizationWe use two feature normalization approaches:projection normalization and binary normalization.Both of the two approaches are applied to basicfeatures, dictionary features and word features.
Asfor repetition, we don?t use normalization, becauseit is in itself binary.
For transcript i, we denotethe value of the jth feature as vij .
We denoteminj , maxj , averagej as the minimum, maximumand average value for each feature in the trainingcorpus, respectively.
Thus for each feature j,we have: averagej = 1n?ni=1 vij minj =mini{vij},maxj = maxi{vij}.4.3.1 Projection NormalizationHere we simply normalize all feature values to arange of [0, 1], where 0 corresponds to the smallestobserved value and 1 to the largest observed valueacross all transcripts.
Then we could have pij =vij?minjmaxj?minj , where pij is the feature value afternormalization.4.3.2 Binary normalizationHere all features are converted to binary values,reflecting whether the value falls below or above theaverage value for that feature observed in training.The value pij of j-th feature for the i-th instance isas below:pij ={0 vij < 1n?ni=1 vij1 otherwise4.3.3 Prediction on the Test SetAll of the previous values, averagej , maxj andminj are derived from the training set.
Whiledoing classification, for a new testing instance, wedenote the feature vector as f = (f1, f2, .
.
.
fn).40fj is then compared with averagej to do binarynormalization.
We also use pj = fj?minjmaxj?minj to doprojection normalization.
If pj < 0, we change pjinto 0; if pj > 1, we change pj into 1.
For thewords or features that are not seen in training, wejust ignore this dimension.4.4 Feature selectionAll lexically based analysis is plagued by datasparsity problems.
In the medical domain thisproblem is even more acute because collectingpatient data is difficult.
The number of featureswe defined outnumbers our samples by ordersof magnitude.
Therefore, in our classificationprocedure, we perform feature selection by doingtwo-sided T-test to compare the values of featuresin the patient and control groups.
The features withp-value ?
0.05 are considered as indicative and areselected for later machine learning experiments, inwhich 169 out of 6057 features have been selected.We discuss the significant features in the full set inSection 6.4 .Note however that we don?t use the featuresselected on the full dataset for machine learningexperiments because when T-tests are appliedon the full dataset feature selection decisionswould include information about the test set aswell.
Therefore, we adopt a leave-one-subject-out(LOSO) evaluation approach instead.
In eachiteration, we set aside one subject as test set.
Thedata from the remaining subjects form the trainingset.
Feature selection is done on the training set onlyand a model is trained.
The predictions are tested onthe held out subject.
The procedure is repeated forevery subject as test set.The choice of p-value cut-off allows us to relaxand tighten the requirement on significance of thefeatures and thus the size of the feature set.
Wereport results with different p-values in Table 3.We also explore alternative feature ranking andfeature selection procedures in Section 6.3.
Ineach fold different features may be selected.
Forease of discussing feature differences we presenta discussion of the 169 significant features on theentire dataset.5 Our approachThe goal of our system is to classify the person whotold a story in one of two categories: Schizophreniagroup (SC) and Control group (CO).
In order todo this, we give labels to the stories told by eachsubject.
Therefore we could use our model toidentify the status of the person who told eachindividual story, the task is to answer the question?Was the subject who told this story a patient orcontrol??.
Then we combine the predictions forstories to predict status of each subject, and thetask becomes answering the question ?Is this subjecta patient or control given that they told these fivestories??.
Thus in story level prediction we use noinformation about the fact that subjects told morethan one story, while in subject-level prediction wedo use this information.First we present an experiment that relies onlyon language models for the prediction.
Then wepresent the complete learning-based system thatuses the full set of features.
Finally, we describethe decision making approach to combine the storylevel predictions to derive a subject-level prediction.5.1 Language ModelLanguage models have been used previously forlanguage impairment on children (Gabani et al2009) and language dominance prediction (Solorioet al2011).
Patients with speaking disorderor cognitive impairment express themselves inatypical ways.
Language models (LMs) give astraightforward way of estimating the probabilityof the productions of a given subject.
We expectthat the approach would be useful for the study ofschizophrenia as well and so start with a descriptionof the LM experiments.We use LMs on words to recognize the differencebetween patients and controls in vocabulary use.We also trained a LM on POS tags becauseit could reduce sparsity and focus more ongrammatical patterns.
Two separate LMs aretrained on transcripts of schizophrenia and controlsrespectively, using leave-one-subject-out protocol.Story-level decisions are made by assigning theclass whose language model yields lower perplexity:s(t) ={SC PERSC(t) ?
PERCO(t)CO otherwise41by Story (%) SC-F CO-F Accuracy Macro-FRandom 54.4 44.6 50.0 49.5Majority 74.8 0.0 59.7 37.42-gram 62.5 44.4 55.2 53.52-gram-Pos 62.2 53.3 58.2 57.8by Subject (%) SC-F CO-F Accuracy Macro-FRandom 54.1 45.1 50.0 49.6Majority 74.2 0.0 59.1 37.12-gram 65.2 50.0 58.9 57.62-gram-Pos 66.7 54.5 61.5 60.6Table 2: Language model performanceHere t means a transcript from a subject, whilePERSC and PERCO are perplexities for patientsand controls, respectively.
We experimented withunigram, bigram and trigram LMs on words andPOS tags.
Laplace smoothing is used whengenerating word probabilities.5.2 Classification PhaseLanguage models are convenient because theysummarize information from patterns in lexical andPOS use into a single number.
However, most of thesuccessful applications of LMs require large amountof training data while our dataset is relatively small.Moreover, we would like to analyze more specificdifferences between the patient and control groupand this would be more appropriately done using alarger set of features.We have described our features and featureselection process in Section 4.
We use SVM-light(Joachims, 1999) for our machine learningalgorithm, as its effectiveness has been proved invarious learning-based clinical tasks compared toother classifiers (Gabani et al2009) .5.3 Status DecisionStory level predictions are made for each transcripteither based on LM perplexity or SVM prediction.The most intuitive way to obtain a subject-levelprediction is by voting from story-level predictionsbetween the stories told by the particular subject.The subject-level prediction is simply set to equalthe majority prediction from individual stories.
Onthe few occasions where there are equal votes forschizophrenia and control, the system makes apreference towards schizophrenia, because it is moreP-value cut-off by Story by Subject # Features0.15 59.0 58.9 4500.10 61.7 64.1 3410.05 62.7 64.1 1690.01 57.7 65.4 440.005 64.2 71.6 320.001 65.7 75.6 180.0005 61.7 66.7 14Table 3: Performance by subject after T-test featureselection in different confidence levels.dangerous to omit a potential patient.6 Experiments and ResultsWe perform our experiments on the 201 transcriptsof the 39 speakers.
The two baselines wecompare with are doing random assignments andmajority class, which for our datasets correspond topredicting all subjects into the Schizophrenia group.We report precision, recall and F-measure forboth patient and control groups, as well as overallaccuracy and Macro-F value.
We get predictionsin leave-one-subject-out fashion and compute theresults over the complete set of predictions.6.1 Language Model PerformanceOur first experiment relies only on the perplexityfrom language models to make the prediction.We use the 1,2,3-gram models on word and POSsequences.
From the result in Table 2 we cansee bigram LM performed better than randombaseline for both story and subject level prediction.3-gram and 1-gram LM did not give a credibleperformance, with results worse than that of thebaselines.
Because of space constraints we do notreport the specific numbers.6.2 Classification Result after Feature SelectionNext we evaluate the performance of classificationwith different number of features from the classeswe define in Section 4.
As discussed above, weperformed feature selection by choosing differentlevels of significance for the p-value cut-off.
Featureselection is performed 39 times for each LOSOtraining fold.
On the standard cut-off p-value ?0.05, our system could achieve 62.7% accuracy onstory and 64.1% on patient level prediction.
The bestperformance is achieved when the cut-off p-value is42Schizophrenia Control GeneralMeasurement P (%) R (%) F (%) P (%) R (%) F (%) Accuracy (%) Macro-F (%)Story Random 59.7 50.0 54.4 40.5 50.0 44.6 50.0 49.5Majority 59.7 100.0 74.8 NA 0.0 0.0 (NA) 59.7 37.425-Features 68.7 75.0 71.7 57.1 49.4 52.9 64.7 62.3Subject Random 59.0 50.0 54.1 41.0 50.0 45.0 50.0 49.6Majority 59.0 100.0 74.2 NA 0.0 0.0 (NA) 59.0 37.125-Features 75.0 91.3 82.4 81.8 56.3 66.7 76.9 74.6Table 4: Performance on best feature-set by feature ranking using signal to noisestricter, 0.001, where an accuracy of 75.6% can bereached.
In this case only about 18 features are usedfor the classification.
Detailed results are shown inTable 3.6.3 Performance with Different Feature SizeNext we investigate the relationship between featureset size and accuracy of prediction.
We areinterested in identifying the smallest possible setof features which gives performance close to theone reported on the full set of significant features.Narrowing the feature set as much as possible willbe most useful for clinicians as they understandthe differences between the groups and look forindicators of the illness they need to track duringregular patient visits.
Physicians and psychologistsare also interested to know the most significantlexical differences revealed by the stories.As an alternative to ranking features by p-value,we use the Challenge Learning Object Package(CLOP) 3 (Guyon et al2006) .
It is a toolkitwith a combination of preprocessing and featureselection.
We experiment with signal-to-noise (s2n),Gram-Schmidt orthogonalization and RecursiveFeature Elimination for finding a subset of indicativefeatures (Guyon and Elisseeff, 2003).
Thesignal-to-noise method gives better results than theother two by at least 6% for the top performancefeature set.
Thus we pick the best k featuresaccording to the s2n result and use only those kfeatures for classification.Figure 1 shows how prediction accuracy changeswith feature sets of different sizes.
From the plotwe clearly see that our top performance is achievedwith 25 to 40 features, after which performancedrops.
The peak performance is achieved when3See http://clopinet.com/CLOP/Figure 1: Story and Subject prediction accuracythere are 25 features, where we could reach 75.0%precision, 91.3% recall, 82.4% F-measure forpatient, and 76.9% accuracy for overall, as shownin Table 4.
Detailed information about the top30 features can be found in Table 5.
?+?
and ?-?means more prevalent for patient and control, while?prj?
and ?01?
correspond to the two normalizationapproaches in Section 4.3, projection and binaryrespectively.6.4 Analysis of Significant FeaturesIn this section we discuss the specific features thatwere revealed as most predictive by the featureselection methods that we employed.
We have seenthat it only requires about 25-40 features to obtainpeak performance.First we briefly review the features that turnedout to be statistically significant (for 0.05 p-valuecut-off).
Table 7 provides a list of the featureswith higher values for Schizophrenia and Controlrespectively.
4 We group the significant featuresaccording to the feature classes we introduced in4LM1 is defined as the ratio of CO perplexity andSC perplexity from LMs, LM7 comes from projectionnormalization of LM1.
If LM perplexity for CO is smaller thanthat of SC, then we set LM3 as 1; otherwise we set LM4 as 1.43Rank Feature Category P-value1 Prj-Self + Diction 5.33E-062 01-Self + Diction 7.34E-063 Prj-punctuation - Basic 1.33E-054 01-I + LIWC 2.73E-055 01-sorry - Lexical 0.0076 01-money + Lexical 6.95E-057 01-punctuation - Basic 4.88E-058 prj-I + LIWC 5.12E-059 01-extremely + Lexical 5.10E-0510 prj-mildly + Lexical 0.000611 prj-sorry - Lexical 0.01112 prj-I + Lexical 0.000213 LM1 + LM 0.000214 LM7 + LM 0.000215 I + Repeat 0.0003Rank Feature Category P-value16 and + Repeat 0.000217 01-mildly + Lexical 0.000418 prj-adverb - LIWC 0.000619 01-relationship - Lexical 0.02420 01-late - Lexical 0.02421 prj-comma - Lexical 0.00122 Repeat word - Basic 0.00123 prj-late - Lexical 0.03424 prj-very - Lexical 0.00725 prj-extremely + Lexical 0.00126 01-couldn?t + Lexical 0.00127 prj-relationship - Lexical 0.03728 very - Repeat 0.00729 prj-?
+ Lexical 0.00230 prj-moderately + Lexical 0.006Table 5: Table of the top 30 features by signal-to-noise rankingSection 4.
Of the 169 significant features, 111 aremore prevalent in patients, 58 are more prevalentamong the controls.
If a feature was significant withboth normalizations we use, we list it only once inTable 7.Among the words indicative of schizophrenia,subjective words such as I and LIWC categoryself are among the most significant.
This findingconforms with prior research that patients withmental disorders refer to themselves more often thanregular people.
Patients produce more questions (asindicated by the significance of the question markas a feature).
It is possible that this indicates adisruption in their thought process and they forgetwhat they are talking about.
Further work will beneeded to understand this difference better.In terms of words, patients talked more aboutmoney, trouble, and used adverbs like moderatelyand basically.
Repetition in language is also arevealing characteristic of the patient narratives.There is a substantial difference in the appearanceof repetitions between the two groups, as well asrepetition of specific words: I, and, and repetitionof filled pauses um.
As patients focus more on theirown feelings, they talked a lot about their family,using words such as son, grandfather and even dogs.Diction features revealed some unexpecteddifferences.
The schizophrenia group scoreshigher in the Self, Cognition, Past, Insistence andSatisfaction categories.
This indicates that they aremore likely to talk about past experience, usingcognitive terms and having a repetition of keyterms.
We were particularly curious to understandwhy patients score higher on Satisfaction ratings.On closer inspection we discovered that patients?stories were rated higher in Satisfaction whenthey were telling SAD stories.
This finding hasimportant clinical implications because one of thediagnostic elements for the disease is inappropriateemotion expression.
Our study is the first to applyan automatic measure to detect such anomaly inpatients?
emotional narratives.
Prompted by thisdiscovery, we take a closer look at the interactionbetween the emotion expressed in a story and theaccuracy of status prediction in the next section.The control group exhibited more wordcomplexity, sentence complexity and thoughtfulnessin their stories.
They use more adverbs and exclusivewords (e.g.
but, without, exclude) on general trend.They use the word sorry significantly more oftenthan patients.6.5 Status Prediction by EmotionWe also investigate if classification accuracy differsdepending on the type of conveyed emotion.Accuracy per emotion with three feature selectionmethods is shown in Table 6.
When usingsignal-to-noise, we can see that on SAD stories thetwo groups can be distinguished better.
Story-levelaccuracies on HAPPY stories reach 72.5%, andthat the accuracy on HAPPY stories is the nexthighest one.
When applying the 0.05 p-valuecut-off to select significant features, ANGER storiesbecome the ones for which the status of a subject44Accuracy (%) s2n (25) T-test (0.05) T-test (0.001)Happy 66.7 59.0 71.8Disgust 63.4 61.0 51.2Anger 61.0 70.7 70.7Fear 60.0 55.0 67.5Sad 72.5 60.0 67.5Story 64.7 62.9 65.7Patient 76.9 64.1 74.4Majority 59.0 59.0 59.0Table 6: Accuracy per emotion by different feature-setscan be predicted most accurately.
Using thethreshold of 0.001 for selection gives the best overallprediction.
In that case, HAPPY and ANGER arethe emotions for which recognition is best.
Thechanges in the recognition accuracy depending onfeature selection suggests that in future studies itmay be more beneficial to perform feature selectiononly on stories from a given type because obviouslyindicative features exist at least for the SAD, ANGERand HAPPY stories.Regardless of the feature selection approach, itis more difficult to tell the two groups apart whenthey tell DISGUST and FEAR stories.
These resultsseem to indicate that when talking about certainemotions patients and controls look much more alikethan when other emotions are concerned.
Futuredata acquisition efforts can focus only on collectingautobiographical narratives relevant to the emotionsfor which patients and controls differ most.Figure 2: Number of significant features by P-valueselection on different thresholds (per emotion)In future work we would like to use only storiesfrom a given emotion to classify between patientsTypes Significant features more common in SCHBasic repeat-word, sentence/documentLIWC I, insight, personal-pronounDiction self, cognition, past, insistence, satisfactionLexical ?, ain?t, alone, at, aw, become, before, behindcare, chance, confused, couldn?t, December, dogdogs, extreme, extremely, feeling, forty, friendsgod, got, grandfather, guess, guy, hand, hanginghearing, hundred, increased, looking, lovedmental, met, mild, mildly, moderate, moderatelymoney, my, myself, outside, paper, passed, pieceremember, sister, son, stand, step, story, taketaken, throwing, took, trouble, use, wakewanna, wayRepeat a, and, I, um, wasLM LM1, LM4, LM7Types Significant features more common in COBasic length/word, words/sentenceLIWC ?6-letters, adverb, exclusive words, inhibitiveDiction certainty, cooperation, diversityfamiliarity, realismLexical ?,?, able, actually, are, basically, be, being, get?sin, late, not, really, relationship, result, she?ssleep, sorry, tell, their, there?s, very, weeksRepeat very, ?,?LM LM3Table 7: Significant features (p-value ?
0.05)and controls.
Doing this with our current datasetis not feasible because there are only about 40transcripts per emotion.
Therefore, we use ourdata to identify significant features that distinguishpatients from controls only on narratives from aparticular emotion.
For example, we compare thedifferences of SAD stories told by patients andcontrols.
We count the number of significantfeatures between patients and controls with 11different p-value cut-offs, and provide a plot thatvisualizes the results in Figure 2.
From the graph,it is clear that there are many more differencesbetween the two groups in ANGER and SADnarratives.
HAPPY comes next, then DISGUST andFEAR.
However, at lower confidence levels, HAPPYhas equal number of significant features as ANGERand SAD, which is in line with the result in Table 6.The feature analysis performed by emotionreveals more differences between patients andcontrols, beyond common features such as self,I, etc.
For HAPPY stories, patients talk moreabout their friends and relatives; they also have a45higher tendency of being ambivalent.
For DISGUSTstories, patients are more disgusted with dogs, andthey talk more about health.
The control groupshows a higher communication score, referring toa better social interaction.
ANGER is one of theemotions that best reveals the differences betweengroups, and schizophrenia patients show moreaggression and cognition while talking, accordingto features derived from Diction.
The controlgroup sometimes talks more about praise.
In FEARstories patients talk about money more often thancontrols.
Meanwhile, the control group uses moreinhibition words, for instance: block, constrain andstop.
An interesting phenomenon happens in SADnarratives.
When talking about sad experiences,patients sometimes show satisfaction and insistence,while the controls talked more about workingexperiences.7 ConclusionIn this paper, we analyzed the predictive powerof different kinds of features for distinguishingschizophrenia patients from healthy controls.
Weprovided an in-depth analysis of features thatdistinguish patients from controls and showed thatthe type of emotion conveyed by the personalnarratives is important for the distinction and thatstories for different emotions give different setsindicators for subject status.
We report classificationresults as high as 76.9% on the subject level,with 75.0% precision and 91.3% on recall forschizophrenia patients.We consider the results presented here to bea pilot study.
We are currently collecting andtranscribing additional stories from the two groupswhich we would like to use as a definitive testset to verify the stability of our findings.
Weplan to explore syntactic and coherence models toanalyze the stories, as well as emotion analysis ofthe narratives.ReferencesNancy C. Andreasen.
1986.
Scale for the assessmentof thought, language, and communication (TLC).Schizophrenia Bulletin, 12:473 ?
482.Michael A. Covington, Congzhou He, Cati Brown,Lorina Naci, Jonathan T. McClain, Bess SirmonFjordbak, James Semple, and John Brown.
2005.Schizophrenia and the structure of language: Thelinguist?s view.
Schizophrenia Research, 77(1):85 ?98.Roddy Cowie.
2000.
Describing the emotional statesexpressed in speech.
In Proceedings of the ISCAWorkshop on Speech and Emotion.Keyur Gabani, Melissa Sherman, Thamar Solorio, YangLiu, Lisa Bedore, and Elizabeth Pen?a.
2009.A corpus-based approach for the prediction oflanguage impairment in monolingual english andspanish-english bilingual children.
In Proceedings ofHLT-NAACL, pages 46?55.Alastair J. Gill, Jon Oberlander, and Elizabeth Austin.2006.
Rating e-mail personality at zero acquaintance.Personality and Individual Differences, 40(3):497 ?507.Alastair J. Gill, Scott Nowson, and Jon Oberlander.
2009.What are they blogging about?
personality, topic andmotivation in blogs.
In Proceedings of the AAAIICWSM?09.Isabelle Guyon and Andre?
Elisseeff.
2003.
Anintroduction to variable and feature selection.
J. Mach.Learn.
Res., 3:1157?1182, March.Isabelle Guyon, Jiwen Li, Theodor Mader, Patrick A.Pletscher, Georg Schneider, and Markus Uhr.
2006.Feature selection with the CLOP package.
Technicalreport, http://clopinet.com/isabelle/Projects/ETH/TM-fextract-class.pdf.Rodrick Hart.
2000.
Diction 5.0, the text-analysisprogram user?s manual, Scolari Software, Sage Press.http://www.dictionsoftware.com/.Peter A. Heeman, Rebecca Lunsford, Ethan Selfridge,Lois M. Black, and Jan P. H. van Santen.
2010.Autism and interactional aspects of dialogue.
InProceedings of the SIGDIAL 2010 Conference, pages249?252.T.
Joachims.
1999.
Making large?scale SVM learningpractical.
In B. Scho?lkopf, C. J. C. Burges, andA.
J. Smola, editors, Advances in Kernel Methods ?Support Vector Learning, pages 169?184, Cambridge,MA.
MIT Press.F.
Mairesse, M. A. Walker, M. R. Mehl, and R. K.Moore.
2007.
Using Linguistic Cues for theAutomatic Recognition of Personality in Conversationand Text.
Journal of Artificial Intelligence Research,30:457?500.Theo C. Manschreck, Brendan A. Maher, Toni M.Hoover, and Donna Ames.
1985.
Repetition inschizophrenic speech.
Language & Speech, 28(3):255?
268.J.W.
Pennebaker, R.J. Booth, and Francis.
2007.Linguistic inquiry and word count (LIWC462007): A text analysis program.
Austin, Texas.http://www.liwc.net/.James W. Pennebaker.
1997.
Writing about EmotionalExperiences as a Therapeutic Process.
PsychologicalScience, 8(3):162?166.Emily T. Prud?hommeaux, Brian Roark, Lois M. Black,and Jan van Santen.
2011.
Classification of atypicallanguage in autism.
In Proceedings of the 2ndWorkshop on Cognitive Modeling and ComputationalLinguistics, CMCL?11, pages 88?96.Brian Roark, Margaret Mitchell, John-Paul Hosom,Kristy Hollingshead, and Jeffrey Kaye.
2011.Spoken language derived measures for detecting mildcognitive impairment.
IEEE Transactions on Audio,Speech & Language Processing, 19(7):2081?2090.Stephanie Rude, Eva-Maria Gortner, and JamesPennebaker.
2004.
Language use of depressed anddepression-vulnerable college students.
Cognition &Emotion, 18(8):1121?1133.Thamar Solorio, Melissa Sherman, Y. Liu, Lisa Bedore,Elizabeth Pen?a, and A. Iglesias.
2011.
Analyzinglanguage samples of spanish-english bilingual childrenfor the automated prediction of language dominance.Natural Language Engineering, 17(3):367?395.Yla R. Tausczik and James W. Pennebaker.
2010.The Psychological Meaning of Words: LIWC andComputerized Text Analysis Methods.
Journalof Language and Social Psychology, 29(1):24?54,March.Kristina Toutanova, Dan Klein, and Christopher D.Manning.
2003.
Feature-rich part-of-speech taggingwith a cyclic dependency network.
In Proceedings ofHLT-NAACL 03.47
