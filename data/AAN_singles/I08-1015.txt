Lexical Chains as Document FeaturesDinakar Jayarajan, Dipti DeodhareCentre for Artificial Intelligence and Robotics,Defence R & D Organisation,Bangalore, INDIA.
[dinakarj, dipti]@cair.drdo.inB RavindranDept.
of CSE,IIT Madras,Chennai, INDIA.ravi@cse.iitm.ac.inAbstractDocument clustering and classification isusually done by representing the documentsusing a bag of words scheme.
This schemeignores many of the linguistic and semanticfeatures contained in text documents.
Wepropose here an alternative representationfor documents using Lexical Chains.
Wecompare the performance of the new repre-sentation against the old one on a cluster-ing task.
We show that Lexical Chain basedfeatures give better results than the Bag ofWords based features, while achieving al-most 30% reduction in the dimensionality ofthe feature vectors resulting in faster execu-tion of the algorithms.1 IntroductionText data usually contains complex semantic infor-mation which is communicated using a combinationof words.
Ideally, the representation used shouldcapture and reflect this fact in order to semanticallydrive the clustering algorithm and obtain better re-sults.The Bag of Words (BoW) (Salton et al, 1975)scheme is a very popular scheme which has beenused for representing documents.
But, this schemeignores many of the linguistic and semantic featurescontained in text documents.
This paper exploresan alternative representation for documents, usinglexical chains, which encodes some of the semanticinformation contained in the document.
This rep-resentation results in improved performance on theclustering tasks and achieves a drastic reduction inthe size of the feature space as well.The BoW scheme was originally designed for theInformation Retrieval domain (Salton, 1989) wherethe aim was to ?index?
the document and not nec-essarily to model the topic distribution.
This rep-resentation has since been adopted as the defactodocument representation scheme for supervised andunsupervised learning on documents.
The BoWscheme represents features as an unordered set ofwords contained in the document, along with theirfrequency count.The BoW scheme assumes that the distributionof words in a document reflect the underlying dis-tribution of topics and hence if the documents aregrouped on the basis of the similarity of the wordscontained in them, it will implicitly result in a clus-tering based on topics.
This representation, usinga simple frequency count alone, does not captureall the underlying information present in the doc-uments.
Moreover, it ignores information such asposition, relations and co-occurrences among thewords.
In addition, the feature space formed willbe very huge and sparse resulting in time and spacecosts as well.Lexical Chaining is a technique which seeks toidentify and exploit the semantic relatedness ofwords in a document.
It is based on the phe-nomenon of lexical cohesion (Halliday and Hasan,1976) and works on the premise that semanticallyrelated words co-occur close together in a passagemore than ?just by chance?.
Lexical chaining is theprocess of identifying and grouping such words to-gether to form chains which in turn will help in iden-tifying and representing the topic and content of thedocument.Lexical chains have been used as an intermediaterepresentation of text for various tasks such as au-111tomatic text summarisation (Barzilay and Elhadad,1997; Silber and McCoy, 2002), malapropism de-tection and correction (Hirst and St-Onge, 1997),and hypertext construction (Green, 1998).
An al-gorithm for computing lexical chains was first givenby (Morris and Hirst, 1991) using the Roget?s The-saurus (Kirkpatrick, 1998).
Since an electronic ver-sion of the Roget?s Thesaurus was not available then,later algorithms were based on the WordNet lexicaldatabase (Fellbaum, 1998).We present here a two pass algorithm to com-pute a representation of documents using lexicalchains and use these lexical chains to derive fea-ture vectors.
These lexical chain based feature vec-tors are used to cluster the documents using two dif-ferent algorithms - k-Means and Co-clustering.
k-Means is a well studied clustering algorithm widelyused in the text domain.
Co-clustering, also knownas bi-clustering (Madeira and Oliveira, 2004), isa clustering approach which was developed in thebioinformatics domain for clustering gene expres-sions.
Since the text domain shares a lot of char-acteristics (high dimensionality, sparsity, etc.)
ofgene expression data, a lot of interest has beengenerated recently in applying the co-clustering ap-proaches (Dhillon et al, 2003) to the text domainwith promising results.
Co-clustering (Dhillon et al,2003; Sra et al, 2004) exploits the duality betweenrows and columns of the document-term matrix usedto represent the features, by simultaneously cluster-ing both the rows and columns.We compare the clustering results obtained fromdocument features extracted using lexical chainsagainst those obtained by using the traditionalmethod of bag of words.2 Lexical ChainsLexical chains are groups of words which exhibitlexical cohesion.
Cohesion as given by (Hallidayand Hasan, 1976) is a way of getting text to ?hangtogether as a whole?.
Lexical cohesion is exhib-ited through cohesive relations.
They (Halliday andHasan, 1976) have classified these relations as:1.
Reiteration with identity of reference2.
Reiteration without identity of reference3.
Reiteration by means of super ordinate4.
Systematic semantic relation5.
Non systematic semantic relationThe first three relations involve reiteration whichincludes repetition of the same word in the samesense (e.g., car and car), the use of a synonym for aword (e.g., car and automobile) and the use of hyper-nyms (or hyponyms) for a word (e.g., car and vehi-cle) respectively.
The last two relations involve col-locations i.e, semantic relationships between wordsthat often co-occur (e.g., football and foul).
Lexi-cal chains in a text are identified by the presence ofstrong semantic relations between the words in thetext.Algorithms for building lexical chains work byconsidering candidate words for inclusion in thechains constructed so far.
Usually these candidatewords are nouns and compound nouns.
LexicalChains can be computed at various granularities -across sentences, paragraphs or documents.
In gen-eral, to compute lexical chains, each candidate wordin the sentence/paragraph/document is compared,with each lexical chain identified so far.
If a candi-date word has a ?cohesive relation?
with the words inthe chain it is added to the chain.
On the other hand,if a candidate word is not related to any of the chains,a new chain is created for the candidate word.
Thusa lexical chain is made up of a set of semanticallyrelated words.
The lexical chains obtained are thenevaluated based on a suitable criteria and the betterchains are selected and used to further processing.Naturally, the computation of lexical chains is predi-cated on the availability of a suitable database whichmaps relations between words.Several algorithms have been proposed for com-puting lexical chains.
Prominent among them arethose by (Hirst and St-Onge, 1997; Barzilay and El-hadad, 1997; Silber and McCoy, 2002; Jarmasz andSzpakowicz, 2003).
Except for the one by Jarmaszand Szpakowicz, all others use WordNet (Fellbaum,1998) to identify relations among words.
A briefoverview of these algorithms is given in (Jayarajanet al, 2007).WordNet is a lexical database which organiseswords into synonym sets or synsets.
Each synsetcontains one or more words that have the samemeaning.
A word may appear in many synsets, de-pending on the number of senses that it has.
The112synsets are connected by links that indicate differ-ent semantic relations such as generalisation (hy-pernyms), specialisation (hyponyms), part relations(holonyms1 and meronyms2), etc.Our approach to computing lexical chains differsfrom those listed above and is described in the nextsection.3 Lexical Chains based Feature VectorsAll the algorithms mentioned in the previous sec-tion, try to disambiguate the sense of the word aspart of the chaining process.
Both Word Sense Dis-ambiguation (WSD) and lexical chaining are veryprofound processes.
The aim of computing the lex-ical chains here is to try and identify the topics ina document.
If WSD has be performed as an im-plicit step in the lexical chain computing algorithm,it tends to deteriorate the outcome of both.
We feelthat the words should be disambiguated by lookingat their context in a sentence/paragraph as a whole.As such, we propose to perform WSD as a prepro-cessing step, before the word is considered for lex-ical chaining.
We use an algorithm by (Patwardhanet al, 2003) to disambiguate the senses of the wordsin reference to Wordnet.
We then filter out all non-noun words identified in the WSD stage.
This isbased on the assumption that nouns are better at re-flecting the topics contained in a document than theother parts of speech.
The result is a set of nounswhich appear in the text along with its sense.
Werefer to these as ?candidate words?.Our algorithm is based on the WordNet LexicalDatabase.
WordNet is used to identify the relationsamong the words.
We use only the identity and syn-onymy relations to compute the chains.
A word hasa identity or synonymy relation with another word,only if both the words occur in the same synset inWordnet.
Empirically, we found that usage of onlythese two relations, resulted in chains representingcrisp topics.A lexical chain contains a list of words which arerelated to each other and is identified using a uniquenumeric identifier.
Each word in turn is representedas a 4-tuple   term, pos, sense, rel , where ?pos?
is1part of, member of, substance of relations, e.g., ?wheel?
ispart of a ?vehicle?2has part, has member, has substance relations, e.g., ?wheel?has part ?rim?the part-of-speech of the term, ?sense?
is the Word-net sense number and ?rel?
is the relation of this wordto the chain.
In this case, we treat the two rela-tions - identity and synonymy, as a single relationand hence this is uniformly ?IS?
for all the words.Definition 1 Length of a lexical chain  is definedas the number of words in the chain.                ff  fi (1)The length of a lexical chain is an indicator of thestrength of the chain in representing a topic.
Domi-nant topics/information will have long chains, whilestray information will form extremely short chains.Each occurrence of a word in a document, will in-crease the length of the chain by one.
Thus, thelength of a chain gives a composite measure of thenumber of documents in which the chain occurs andthe number of occurences of words in the chain inthese documents.3.1 Feature Vector ComputationWe use a two pass algorithm to generate feature vec-tors based on lexical chains.
Our algorithm worksby maintaining a global set of lexical chains, eachof which represents a topic.
Initially, the global listis empty.
In the first pass we identify all possiblelexical chains for that document.
This is achievedby comparing the candidate words of each docu-ment with the global list to identify those chains withwhich it has a identity or synonymy relation.
If nochains are identified, a new chain is created and putin the global list.
The candidate word is then addedto the chain.
At the end of this pass, we obtain aglobal set which lists all the chains contained in allthe documents.
The algorithm is presented in Algo-rithm 1.In the second pass we select a subset of chainsfrom the global set, which can be used to representthe document.
We define and use a measure to eval-uate and select the chains as follows:Definition 2 The significance of a lexical chain L ina Global set G is defined as113Algorithm 1 Identify Chains1: Maintain a global set of lexical chains, ini-tialised to a Null set2: for each document do3: for each candidate word in document do4: Identify lexical chains in global set withwhich the word has a identity/synonym re-lation5: if No chain is identified then6: Create a new chain for this word and in-sert in global set7: end if8: Add word to the identified/created chainsin Global Set9: end for10: end forAlgorithm 2 Select Chains and Generate FVfor each document do2: Initialise feature vector to zerofor each candidate word in document do4: Identify lexical chains in global set withwhich the word has a identity/synonym re-lationend for6: Compute threshold for documentfor each identified chain in global set do8: if utility of chain greater than thresholdthenSet component corresponding to chainin feature vector to 110: end ifend for12: end forflffi   ! "
#   	$% & '  ! "
#   	(2)The significance of a chain L measures howrandomly the chain appears in the global set G.This measure helps in identifying good chains fromweak, random ones in the global set.
In effect,flffiwill select those chains which are not abnor-mally long or short with respect to the distributionsof chains in the global set.Definition 3 A candidate word W is related to a lex-ical chain L if W has an identity or synonym relationwith L.( )  * 	+ ,-, + ) *)(( )  *., /  ( 0 ffi fl(3)Definition 4 The utility of a lexical chain L to a doc-ument D is defined as1ffi, 2flffi$34!!
5 " 6( )  *0,(4)The utility of a chain L is a measure of how goodL will be in representing the document.
This is basedon the observation that long chains are better thanshort ones.
This measure will prefer ?good?
chainsfrom the global set, which are related to a large num-ber of candidate words in the document.We select and assign to the document all thosechains which cross a threshold on the utility of thechain.
Empirically, we found that using a thresh-old of ?half the average?
utility for a document gavegood results.
For a document D, let the set of all lex-ical chains assignable to D be 7 8 9 :% ff%;< : .The threshold for D is computed as (fl / * 	2! "
#=1ffi 	 , 2>$ ?78?
(5)The lexical chains in the global list form the com-ponents of the feature vectors.
We use a binary val-ued scheme, where in we put a - corresponding toa chain if the chain is assigned to the document and.
otherwise.
Essentially, what we obtain here is afeature vector of size equal to the number of lexi-cal chains in the global list.
The second pass of thealgorithm is listed in Algorithm 2.114Cluster Document Idscollege atheists 53675, 53357, 53540amusing atheists and anarchists 53402, 53351islam & dress code for women 51212, 51216, 51318Table 2: Example of the classes obtained from grouping the documents using the subject line4 ExperimentsWe use the 20 Newsgroups (Rennie, 1995) datasetto evaluate the utility of representing documentsusing the lexical chains (lexchains) scheme.
The20 Newsgroups (20NG) corpus is a collection ofusenet messages spread across 20 Usenet groups.These messages are written by a wide populationof net users and represent a good variation in writ-ing styles, choice of words and grammar.
Thus, wefeel that the 20NG is a representative corpus for thepurpose.
We derive three datasets from three dis-tinct groups of the 20NG corpus - comp.windows.x(cwx), rec.motorcycles (rm) and alt.atheism (aa).The statistics of the datasets is given in Table 1.The documents in each dataset are furthergrouped on the basis of their subject lines.
Thisgrouping into classes is used as the gold standardfor evaluating the clustering algorithms.
An exam-ple of the groups formed for the aa dataset is shownin Table 2.We prepared the dataset for feature extraction byremoving the complete header including the subjectline and used only the body portion of the mes-sages to compute the features.
We extracted fea-tures on this cleaned data using both the BoW andlexchains scheme.
For the BoW scheme, we firsttokenised the document, filtered out the stopwordsusing the list obtained from (Fox, 1992) and fur-ther stemmed them using a Porter Stemmer (Porter,1980).
The feature vectors were then computed us-ing the tf.idf scheme.
We refer to the feature vectorsthus obtained as cwx-BoW, rm-BoW and aa-BoWCollection # Classes # Documentscomp.windows.x 649 980alt.atheism 196 799rec.motorcycles 340 994Table 1: Dataset Statisticsfor the cwx, rm and aa datasets respectively.
Lex-chains based features were derived as described inSection 3.1 and are analogously referred to here ascwx-lc, rm-lc and aa-lc.
This results in a totalof six datasets.
The dimensions of the feature vec-tors obtained are summarised in Table 3.
It can benoted that the size of the feature vectors are reducedby more than 30% with the lexchains based features.These six datasets were clustered using the k-Means and Co-clustering algorithms.
The k-Meansimplementation in Matlab was used and k was setto 649, 340 and 196 for cwx, rm and aa respec-tively and reflects the number of classes identified inthe gold standard (ref.
Table.
1).
The co-clusteringexperiments were done using the Minimum Sum-Squared Residue Co-clustering algorithm (Sra et al,2004) with the number of row clusters set to thesame values as given to the k-Means algorithm.We use a normalised edit distance based measureto evaluate the goodness of the clusters.
This mea-sure is a variant of the one used by (Pantel and Lin,2002), which defines an edit distance as the num-ber of merge, move and copy operations requiredto transform the resulting clusters to the gold stan-dard.
Initially, if there are @ classes in the gold stan-dard, we create @ empty clusters.
The measure thenmerges each resulting cluster to the cluster in thegold standard with which it has maximum overlap,breaking ties randomly.
Thus, the merge operationattempts to bring the obtained clusters as close aspossible to the gold standard as a whole.
Subse-quently, the move and copy operations are used toBoW lexchain Reductioncwx 12767 4569 64%aa 8881 5980 32%rm 8675 5288 39%Table 3: Dimensionality of the Feature Vectors115k-Means Co-cluster Time(secs)cwx-BoW 203 (0.21) 140 (0.14) 1529cwx-lc 179 (0.18) 158 (0.16) 201aa-BoW 85 (0.11) 110 (0.13) 869aa-lc 60 (0.08) 82 (0.10) 221rm-BoW 113 (0.11) 208 (0.21) 1177rm-lc 127 (0.12) 144 (0.14) 229Table 4: Edit distance between obtained clusters andgold standard.
Normalised edit distances are givenin parenthesis.
The fourth column gives runtime forthe co-clustering algorithm, averaged over four runs.
(For all cases, lower is better.
)move (copy) the documents around so that they fi-nally match the gold standard.We observed that the merge operation would in-evitably add as many clusters as there are in thegold standard to the final count, skewing the results.Hence, we define the edit distance as only the num-ber of move and copy3 operations required to con-vert the obtained clusters to that of the gold stan-dard.
In effect, it measures the number of docu-ments which are misplaced with respect to the goldstandard.
The obtained edit distance is normalisedby dividing it with the number of documents in thedataset.
This will normalise the value of the mea-sure to range between 0 and 1.
The lower the valueof this measure, the closer the obtained clustering isto the gold standard.The results are enumerated in Table 4.
The lex-chains based document feature gives an improve-ment of upto 33% over the BoW representationwhile achieving a reduction in dimensions of the fea-ture vectors by more than 30% (ref.
Table 3).
Weperformed run time studies on the dataset using theco-clustering algorithm.
The runtimes are averagedover four runs.
It can be seen that a speedup of morethan 74% is achieved with the lexchain based fea-tures4.Thus, the results show that the running time3The copy count will be included only in the case of over-lapping clusters, which happens if a document is in more thanone cluster.4It was observed empirically that the time required to com-pute both the BoW and lexchain features are nearly the sameand hence can be ignored.of the clustering algorithms is drastically reducedwhile maintaining or improving the clustering per-formance through the use of lexchain based features.4.1 DiscussionA document is not just a bunch of loose words.
Eachword in a document contributes to some aspect ofthe overall semantics of the document.
Classifica-tion and clustering algorithms seek to group the doc-uments based on its semantics.
The BoW schemeinherently throws away a lot of information, whichwould have otherwise been useful in discerning thesemantics of the document.
The BoW representationfails to capture and represent these semantics result-ing in a less accurate representation for the docu-ments.
This fact is reflected by higher edit distancein the case of BoW based clustering in Table 4.Earlier, Hatzivassiloglou, et.
al.
(Hatzivassiloglouet al, 2000) had studied the effects of linguisticallymotivated features on clustering algorithms.
Theyhad explored two linguistically motivated features -noun phrase heads and proper names and comparedthese against the bag of words representation.
Theyhad reported that the BoW representation was betterthan linguistically motivated features.
We believethat noun phrase heads and proper names are inade-quate representations of the semantics of a documentand a more composite representation is required toobtain better results on semantically oriented tasks.Lexical chains appear to be capable of doing thisto a certain extent.
During the process of com-puting and selecting the lexical chains, we are im-plicitly trying to decode the semantics of the doc-uments.
Lexical chains work on the basic premisethat a document describes topics through a combi-nation of words and these words will exhibit a co-hesion among them.
This cohesion can be identifiedusing a resource such as WordNet.
In the process,lexical chains capture some amount of the seman-tics contained in the documents, resulting in a betterperformance in subsequent processing of the docu-ments.5 ConclusionWe have shown that semantically motived features,such as lexical chains, provide a better representa-tion for the documents, resulting in comparable or116better performance on clustering tasks while effect-ing a drastic reduction in time and space complexity.Even though the lexical chains manage to repre-sent the semantics to a certain extent, we feel it canbe further enhanced by more involved processing.
Acomparision of lexical chains based representationwith other document representation schemes such asLSA also warrants investigation.6 AcknowledgementThe authors would like to thank Director, CAIR forthe encouragement and support given for this work.The authors would also like to thank the three anony-mous reviewers, whose detailed comments helpedimprove this work.ReferencesRegina Barzilay and M. Elhadad.
1997.
Using lexicalchains for text summarization.
In In Proceedings ofthe Intelligent Scalable Text Summarization Workshop(ISTS?97), ACL, Madrid, Spain.Inderjit S. Dhillon, Subramanyam Mallela, and Dhar-mendra S. Modha.
2003.
Information-theoreticco-clustering.
In Proceedings of The Ninth ACMSIGKDD International Conference on KnowledgeDiscovery and Data Mining(KDD-2003), pages 89?98.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press.Christopher Fox, 1992.
Information retrieval: datastructures and algorithms, chapter Lexical Analysisand Stoplists.
Prentice-Hall, Inc., Upper Saddle River,NJ, USA.Stephen J Green.
1998.
Automatically generating hy-pertext in newspaper articles by computing seman-tic relatedness.
In D.M.W.
Powers, editor, NeM-LaP3/CoNLL98: New Methods in Language Process-ing and Computational Natural Language.M.
A. K. Halliday and Ruqaiya Hasan.
1976.
Cohesionin English.
Longman.Vasileios Hatzivassiloglou, Luis Gravano, and AnkineeduMaganti.
2000.
An investigation of linguistic featuresand clustering algorithms for topical document clus-tering.
In SIGIR 2000, pages 224?231.Graeme Hirst and David St-Onge.
1997.
Lexical chainsas representation of context for the detection and cor-rection of malapropisms.
In C. Fellbaum, editor,WordNet:An electronic lexical database and some ofits applications.
The MIT Press, Cambrige, MA.Mario Jarmasz and Stan Szpakowicz.
2003.
Not as easyas it seems: Automating the construction of lexicalchains using roget?s thesaurus.
In Proceedings of the16th Canadian Conference on Artificial Intelligence.Dinakar Jayarajan, Dipti Deodhare, B Ravindran, andSandipan Sarkar.
2007.
Document clustering usinglexical chains.
In Proceedings of the Workshop on TextMining & Link Analysis (TextLink 2007), Hyderbad,INDIA, January.B Kirkpatrick.
1998.
Roget?s Thesaurus of EnglishWords and Phrases.
Penguin.Sara C. Madeira and Arlindo L. Oliveira.
2004.
Biclus-tering algorithms for biological data analysis: a sur-vey.
IEEE/ACM Transactions on Computational Biol-ogy and Bioinformatics, 1(1):24?45.Jane Morris and Graeme Hirst.
1991.
Lexical cohe-sion computed by thesaural relations as an indicatorof the structure of text.
Computational Linguistics,17(1):21?48.Patrick Pantel and Dekang Lin.
2002.
Document clus-tering with committees.
In SIGIR ?02: Proceedings ofthe 25th annual international ACM SIGIR conferenceon Research and development in information retrieval,pages 199?206, New York, NY, USA.
ACM Press.Siddharth Patwardhan, Satanjeev Banerjee, and Ted Ped-ersen.
2003.
Using semantic relatedness for wordsense disambiguation.
In Proceedings of the FourthInternational Conference on Intelligent Text Process-ing and Computational Linguistics (CiCLING-03),Mexico City, Mexico.Martin F. Porter.
1980.
An algorithm for suffix stripping.Program.Jason Rennie.
1995.
20 Newsgroups dataset.
Online:http://people.csail.mit.edu/jrennie/20Newsgroups/.Gerard Salton, A. Wong, and C. S. Yang.
1975.
A vectorspace model for automatic indexing.
Commun.
ACM,18(11):613?620.Gerard Salton.
1989.
Automatic Text Processing ?
TheTransformation, Analysis, and Retrieval of Informa-tion by Computer.
Addison?Wesley.H.
Gregory Silber and Kathleen F. McCoy.
2002.
Effi-ciently computed lexical chains as an intermediate rep-resentation for automatic text summarization.
Compu-tational Linguistics, 28(4):487?496.Suvrit Sra, Hyuk Cho, Inderjit S. Dhillon, and YuqiangGuan.
2004.
Minimum sum-squared residue co-clustering of gene expression data.
In Proceedingsof the Fourth SIAM International Conference on DataMining, Lake Buena Vista, Florida, USA, April 22-24,2004.117
