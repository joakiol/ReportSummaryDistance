Virtual Modality: a Framework for Testing and BuildingMultimodal ApplicationsP?ter P?l Boda1Audio-Visual Systems LaboratoryNokia Research CenterHelsinki, Finlandboda@mit.eduEdward FiliskoSpoken Language Systems GroupCSAIL, MITCambridge, Massachusetts, USAfilisko@csail.mit.edu1Currently a Visiting Scientist with the Spoken LanguageSystems Group, CSAIL, MIT.AbstractThis paper introduces a method that generatessimulated multimodal input to be used in test-ing multimodal system implementations, aswell as to build statistically motivated multi-modal integration modules.
The generation ofsuch data is inspired by the fact that true mul-timodal data, recorded from real usage scenar-ios, is difficult and costly to obtain in largeamounts.
On the other hand, thanks to opera-tional speech-only dialogue system applica-tions, a wide selection of speech/text data (inthe form of transcriptions, recognizer outputs,parse results, etc.)
is available.
Taking the tex-tual transcriptions and converting them intomultimodal inputs in order to assist multimo-dal system development is the underlying ideaof the paper.
A conceptual framework is es-tablished which utilizes two input channels:the original speech channel and an additionalchannel called Virtual Modality.
This addi-tional channel provides a certain level of ab-straction to represent non-speech user inputs(e.g., gestures or sketches).
From the tran-scriptions of the speech modality, pre-definedsemantic items (e.g., nominal location refer-ences) are identified, removed, and replacedwith deictic references (e.g., here, there).
Thedeleted semantic items are then placed into theVirtual Modality channel and, according toexternal parameters (such as a pre-defineduser population with various deviations), tem-poral shifts relative to the instant of each cor-responding deictic reference are issued.
Thepaper explains the procedure followed to cre-ate Virtual Modality data, the details of thespeech-only database, and results based on amultimodal city information and navigationapplication.1 IntroductionMultimodal systems have recently drawn significantattention from researchers, and the reasons for such aninterest are many.
First, speech recognition based appli-cations and systems have become mature enough forlarger-scale deployment.
The underlying technologiesare gradually exhibiting increased robustness and per-formance, and from the usability point of view, userscan see some clear benefits from speech-driven applica-tions.
The next evolutionary step is the extension of the"one dimensional" (i.e., speech-only) interface capabili-ties to include other modalities, such as gesture, sketch,gaze, and text.
This will lead to a better and more com-prehensive user experience.A second reason is the widely accepted, and ex-pected, mobility and pervasiveness of computers.
De-vices are getting more and more powerful and versatile;they can be connected anywhere and anytime to net-works, as well as to each other.
This poses new de-mands for the user interface.
It is no longer sufficient tosupport only a single input modality.
Depending on thespecific application, the given usage scenario, and thecontext, for example, users should be offered a varietyof options by which to interact with the system in anappropriate and efficient way.Third, as the output capabilities of devices provideever-increasing multimedia experiences, it is naturalthat the input mechanism must also deal with variousmodalities in an intuitive and comprehensive manner.
Ifa map is displayed to the user, it is natural to expect thatthe user may want to relate to this physical entity, forinstance, via gestures, pointing, gazing or by other, notnecessarily speech-based, communicative means.Multimodal interfaces give the user alternatives andflexibility in terms of the interaction; they are enablingrather than restricting.
The primary goal is to fully un-derstand the user's intention, and this can only be real-ized if all intentional user inputs, as well as anyavailable contextual information (e.g., location, prag-matics, sensory data, user preferences, current and pre-vious interaction histories) are taken into account.This paper is organized as follows.
Section 2 intro-duces the concept of Virtual Modality and how the mul-timodal data are generated.
Section 3 explains theunderlying Galaxy environment and briefly summarizesthe operation of the Context Resolution module respon-sible for, among other tasks, resolving deictic refer-ences.
The data generation as well as statistics iscovered in Section 4.
The experimental methodology isdescribed in Section 5.
Finally, the results are summa-rized and directions for future work are outlined.2 Virtual ModalityThis section explains the underlying concept of VirtualModality, as well as the motivation for the work pre-sented here.2.1 MotivationMultiple modalities and multimedia are an essential partof our daily lives.
Human-human communication relieson a full range of input and output modalities, for in-stance, speech, gesture, vision, gaze, and paralinguistic,emotional and sensory information.
In order to conductseamless communication between humans and ma-chines, as many such modalities as possible need to beconsidered.Intelligent devices, wearable terminals, and mobilehandsets will accept multiple inputs from different mo-dalities (e.g., voice, text, handwriting), they will rendervarious media from various sources, and in an intelli-gent manner they will also be capable of providing addi-tional, contextual information about the environment,the interaction history, or even the actual state of theuser.
Information such as the emotional, affective stateof the user, the proximity of physical entities, the dia-logue history, and biometric data from the user could beused to facilitate a more accommodating, and conciseinteraction with a system.
Once contextual informationis fully utilized and multimodal input is supported, thenthe load on the user can be considerably reduced.
This isespecially important for users with severe disabilities.Implementing complex multimodal applicationsrepresents the chicken-and-egg problem.
A significantamount of data is required in order to build and tune asystem; on the other hand, without an operational sys-tem, no real data can be collected.
Incremental and rule-based implementations, as well as quick mock-ups andWizard-of-Oz setups (Lemmel?
and Boda, 2002), aim toaddress the application development process from bothends; we follow an intermediate approach.The work presented here is performed under the as-sumption that testing and building multimodal systemscan benefit from a vast amount of multimodal data, evenif the data is only a result of simulation.
Furthermore,generating simulated multimodal data from textual datais justified by the fact that a multimodal system shouldalso operate in speech-only mode.2.2 The conceptMost current multimodal systems are developed withparticular input modalities in mind.
In the majority ofcases, the primary modality is speech and the additionalmodality is typically gesture, gaze, sketch, or any com-bination thereof.
Once the actual usage scenario is fixedin terms of the available input modalities, subsequentwork focuses only on these input channels.
This is ad-vantageous on the one hand; however, on the otherhand, there is a good chance that system developmentwill focus on tiny details related to the modality-dependent nature of the recognizers and their particularinteraction in the given domain and application sce-nario.Virtual Modality represents an abstraction in thissense.
The focus is on what semantic units (i.e., mean-ingful information from the application point of view)are delivered in this channel and how this channel alignswith the speech channel.
Note that the speech channelhas no exceptional role; it is equal in every sense withthe Virtual Modality.
There is only one specific consid-eration regarding the speech channel, namely, it con-veys deictic references that establish connections withthe semantic units delivered by the Virtual Modalitychannel.The abstraction provided by the Virtual Modalityenables the developer to focus on the interrelation of thespeech and the additional modalities, in terms of theirtemporal correlation, in order to study and experimentwith various usage scenarios and usability issues.
It alsomeans that we do not care how the information deliv-ered by the Virtual Modality arose, what (ac-tual/physical) recognition process produced them, norhow the recognition processes can influence eachother?s performance via cross-interaction using earlyevidence available in one channel or in the other - al-though we acknowledge that this aspect is important anddesired, as pointed out by Coen (2001) and Haikonen(2003), this has not yet been addressed in the first im-plementation of the model.The term ?virtual modality?
is not used in the multimo-dal research community, as far as we know.
The onlyoccurrence we found is by Marsic and Dorohonceanu(2003), however, with ?virtual modality system?
theyrefer to a multimodal management module that managesand controls various applications sharing common mo-dalities in the context of telecollaboration user inter-faces.2.3 OperationThe idea behind Virtual Modality is explained with thehelp of Figure 1.
The upper portion describes how theoutput of a speech recognizer (or direct natural languageinput from keyboard) and a sequence of words, {w1 ?.wN}, is transformed into a corresponding sequence ofconcepts, {C1 ?.
CM}.
The module responsible for thisoperation, for the sake of simplicity and generality, iscalled a classifier (CL).
In real-life implementations,this module can be a sophisticated natural language un-derstanding (NLU) unit, a simple semantic grammar, ora hybrid of several approaches.The middle part of Figure 1 exhibits how the VirtualModality is plugged into the classifier.
The Virtual Mo-dality channel (VM) is parallel to the speech channel(Sp) and it delivers certain semantic units to the classi-fier.
These semantic units correspond to a portion of theword sequence in the speech channel.
For instance, theoriginal incoming sentence might be, ?From HarvardUniversity to MIT.?
In the case of a multimodal setupthe semantic unit, originally represented by a set ofwords in the speech channel (e.g., ?to MIT?
), will bedelivered by the Virtual Modality as mi.The tier between the two modality channels is adeictic reference in the speech channel (di in the bottomof Figure 1).
There are various realizations of a deicticreference, in this example it can be, for example, ?here?,?over there?, or ?this university?.
Nevertheless, in allcases for these input combinations (i.e., speech only,speech and some other modality) the requirement is thatthe very same sequence of semantic concepts is to beproduced by the classifier.There is one more criterion: there must be a tempo-ral correspondence between the input channels.
Thedeictic reference can only be resolved if the input deliv-ered by the other modality channel is within certain timeframes.
This is indicated tentatively in the figure as mioccurring either in synchrony with, prior to, or follow-ing the deictic reference in time (see Section 4.3).CLw1 w2 w3 ?.
wN C1 C2 ?.
CMSpw1 ?
di ?
wNCLC1 C2 ?.
CMmimimiSpVMCLw1 w2 w3 ?.
wNC1 C2 ?.
CMw2 w3SpVMFigure 1.
The concept of Virtual Modality (Sp and VM stand for the Speech and VirtualModality channels, respectively.
CL is a classifier and integrator that transforms and fuses asequence of words, wi, and Virtual Modality inputs, mi, into a corresponding sequence ofconcepts Ck.
).In the above described model the speech channel willalways have a deictic replacement when a semantic unitis moved to the Virtual Modality channel, althoughOviatt, DeAngeli and Kuhn (1997) reported their find-ings that in a given application domain users are noteven using spoken deictic references in more than halfof the multimodal input cases.
Therefore, to conform tothis, we keep in mind that di can have a void value, aswell.2.4 The use of Virtual ModalityThe framework described above enables two steps in thedevelopment of multimodal systems.
First, with the in-troduction of the Virtual Modality, modules designed toresolve inputs from multimodal scenarios can be tested.Quite often, these inputs alone represent ambiguity andthe combination of two or more input channels areneeded to resolve them.On the other hand, with the removal of pre-definedsemantic units to the Virtual Modality channel, amultimodal database can be generated from the speech-only data.
For instance, in a given application domain,all location references can be moved to the Virtual Mo-dality channel and replaced by randomly chosen deicticreferences.
Furthermore, the temporal relation betweenthe deictic reference and the corresponding semanticunit in the Virtual Modality can be governed by externalparameters.
This method facilitates the generation of alarge amount of ?multimodal?
data from only a limitedamount of textual data.
This new database can then beused for the first task, as described above, and equallyimportantly, it can be used to train statistically moti-vated multimodal integrator/fusion modules.As it was pointed out by Oviatt et al (2003), predic-tive and adaptive integration of multimodal input is nec-essary in order to provide robust performance formultimodal systems.
Availability of data, even if it isgenerated artificially, can and will help in the develop-ment process.2.5 Further considerationsThe primary goal of an interactive system is the fullunderstanding of the user?s intention in the given con-text of an application.
Processing all active inputs fromthe user can only attain this task: recognizing and inter-preting them accurately.
Additionally, by consideringall passively and implicitly available information (e.g.,location, sensory data, dialogue history, user prefer-ences, pragmatics), the system can achieve an evenfuller understanding of the user?s intention.The Virtual Modality can be used to simulate the de-livery of all the previously described information.
Froma semantic interpretation point of view, an implicitlyavailable piece of information, i.e., the physical locationof the user (detectable by a mobile device, for instance),is equal to an active user input generated in a given mo-dality channel.
The only difference might be the tempo-ral availability of the data: a location informationderived from a mobile device is continuously availableover a longer period of time, while a user gesture over amap specifying, for example the value for a ?from here?deictic reference, is present only for a relatively shorttime.3 System ArchitectureResearchers in the Spoken Language Systems group atMIT have been developing human-computer dialoguesystems for nearly two decades.
These systems are im-plemented within the Galaxy Communicator architec-ture, which is a multimodal conversational systemframework (Seneff et al, 1998).
As shown in Figure 2,a Galaxy system is configured around a central pro-grammable hub, which handles the communicationsamong various human language technology servers,including those that handle speech recognition and syn-thesis, language understanding and generation, contextresolution, and dialogue management.Figure 2.
The Galaxy Communicator archi-tecture.Several Galaxy domains are currently under develop-ment at MIT (Zue et al, 1994; Seneff et al, 2000; Zueet al, 2000; Seneff, 2002), but the research effort pre-sented here concerns only Voyager, the traffic and cityguide domain (Glass et al, 1995; Wang, 2003) - al-though the Virtual Modality concept is applicable forother domains as well.
Voyager?s map-based interfaceprovides opportune conditions for the use of multimodalinput and deictic references.
For example, a typical userinput may be, ?How do I get from here to there??
whichis spoken while the user clicks on two different loca-tions on a graphical map.After the utterance has been recognized and parsed,the semantic frame representation of the utterance issent to the Context Resolution (CR) server (Filisko andSeneff, 2003).
It is the CR server?s duty to interpret theuser?s utterance in the context of the dialogue history,the user?s physical environment, and limited worldknowledge, via a resolution algorithm.
This protocolincludes a step to resolve any deictic references the userhas made.In addition to the user?s utterance and dialogue his-tory, all gestures for the current turn are sent to the CRserver.
All of this contextual information can then beutilized to make the most appropriate resolutions of allthe deictic references.
The context-resolved semanticframe is finally sent to the dialogue manager, where anappropriate reply to the user is formulated.The simulation of such an interaction cycle has beenfacilitated by the use of a Batchmode server, developedby Polifroni and Seneff (2000).
The server receives aninput (e.g.
the text representation of the spoken utter-ance) from a file of logged or pre-formatted data.
Afterthe input has been processed, the next input is obtainedfrom the input file, and the cycle continues (more detailsin Section 5).4 Data Generation4.1 Application domainThe original data used for generating multimodal simu-lated inputs are taken from the log files of the Voyagerapplication.
The Voyager application provides informa-tion about city landmarks (e.g.
universities, museums,sport arenas, subway stops), gives navigation guidanceand up-to-date traffic information over the phone andvia a graphical interface.
Geographically it covers thearea of Boston and Cambridge in Massachusetts.
Userscan use natural language in the queries and dialoguemanagement takes care of user-friendly disambiguation,error recovery and history handling.
A typical dialoguebetween Voyager (V) and a user (U) is given below:U: Can you show me the universities in Boston?V: Here is a map and list of universities in Boston?U: What about Cambridge?V: Here is a map and list of universities in Cambridge?U: How do I get there <click Harvard> from here <click MIT>?V: Here are directions to Harvard University from MIT?4.2 Defining a user populationAs mentioned earlier, the data to be generated can beused both for testing and for system development.
Inboth scenarios, real dialogues should be simulated asclosely as possible.
Therefore a virtual user populationwas defined for each experiment.First, the distribution of various user types was de-fined.
A user type is specified in terms of the delay auser exhibits with the Virtual Modality data delivery,relative to the speech channel.
The following six usertypes were defined: outspoken, precise, too-fast,quickie, slowhand and everlate.
Outspoken is an imagi-nary user who never uses the Virtual Modality, andcommunicates with the system using only the speechmodality.
Precise always issues the Virtual Modalityinput in synchrony with the spoken deictic reference.Too-fast always issues the Virtual Modality input sig-nificantly earlier than the corresponding deictic refer-ence in the speech channel, while Quickie issues theVirtual Modality input only slightly earlier than thedeictic reference.
Similar rules apply for Slowhand andEverlate, except that they issue the Virtual Modalityinput slightly later or much later, respectively, than thedeictic reference.Once the composition of the user population hasbeen determined, the corresponding temporal deviationsmust be specified.
In a real system the exact instancesare typically given as elapsed time from a referencepoint specified by a universal time value (with differentdevices synchronized using the Network Time Proto-col).
However, such accuracy is not necessary for theexperiments.
Rather, a simplified measurement is intro-duced in order to describe intuitively how the VirtualModality input deviates from the instant when the corre-sponding deictic reference was issued.
The unit usedhere is a word distance, more precisely the averagelength of a word (how many words are between thedeictic reference and the input in the Virtual Modalitychannel).
A 0 means that the deictic reference and theVirtual Modality event are in synchrony, while a ?1(+1) means that the Virtual Modality input was issuedone word earlier (later) than the corresponding deicticreference.Using this formalism, the following deviation pat-tern for the five user types is defined as a starting pointfor the experiments:Too-fast -2Quickie -1Precise 0Slowhand +1Everlate +2Table 1.
Temporal deviation parameters forthe user types that use the Virtual Modality.4.3 Generation of the multimodal dataGenerating multimodal data is, in a sense, the reverseprocess of the multimodal integration step.
Since it isknown how the deictic references are realized in a givendomain, generating sentences with deictic referencesonce the actual definite phrases are found, seemsstraightforward.The idea is simple: find all instances of a given typeof semantic unit (e.g., location reference) in the inputsentences, move them to the Virtual Modality channelwith timing information and, as a replacement, put sen-sible deictic references back into the original sentences.The implementation, however, reveals several prob-lems.
First, identification of the location references isnot necessarily an easy task.
It may require a complexparsing or keyword-spotting algorithm, depending onthe application in question.
In our case, the log filesinclude the output of the TINA Natural Language Un-derstanding module, meaning that all semantically rele-vant units present in an input sentence are markedexplicitly in the output parse frame (Seneff, 1992).Figure 3 gives an example of the parse frame.
{c directions:subject 1:domain "Voyager":input_string " give me directions from harvard to mit ":utterance_id 6:pred {p from:topic {q university:name "Harvard University" } }:pred {p to:topic {q university:name "Massachusetts Institute of Technology" } } }Figure 3.
Parse frame for the input sentence?give me directions from harvard to mit?.The movement and time marker placement step repre-sents no problem.The third step, namely the replacement of the re-moved semantic units with sensible deictic references,requires certain manipulation.
Performing the replace-ment using only deictic references, such as, ?here?,?over here?, ?there?, and ?over there?, would result in arather biased data set.
Instead, depending on the topic ofthe location reference (e.g., city, road, university), defi-nite noun phrases like ?this city?
and ?that university?were also used.
Eventually, a look-up table was definedwhich included the above general expressions, as wellas patterns such as ?this $?
and ?that $?
in which thevariable part (i.e., $) was replaced with the actual topic.The selection for a sentence was randomly chosen, re-sulting in good coverage of various deictic referencesfor the input sentences.
For the example depicted inFigure 3, the following sentence is generated:?give me directions from there to this university?The following is a summary of the overall multimodaldata generation process:1.
Define the distribution of the user population (e.g.,outspoken 20%, precise 40%, quickie, 20%, slow-hand 15%, everlate 5%);2.
Define the corresponding deviations (see Table 1);3.
Randomly allocate turns (sentences) to the pre-defined user types (e.g.
40% of all data will go forthe precise user type with deviation 0);4.
Identify all location references in the input sen-tence based on the parse frame;5.
Remove all or a pre-defined quantity of locationexpressions from the original sentence and replacethem with deictic markers;6.
Place the removed location phrases into the VirtualModality channel;7.
Place time markers to the Virtual Modality channelreferring to the original position of the locationphrases in the input sentence;8.
Issue the pre-determined time shift, if needed, inthe Virtual Modality channel;9.
Randomly select an acceptable deictic referenceand insert it into the original sentence in place ofthe deictic marker;10.
Repeat 4-9 until all data has been processed.An example of the generated Virtual Modality data andthe corresponding sentence is shown in Figure 4.4.4 StatisticsTable 2 below details some statistics on data obtainedfrom Voyager?s original log files.Number of sessions 1099Number of turns 6077Average number of turnsper session 5.53All location references 6982Average number of refer-ences per turn 1.14Number of different loca-tion reference patterns 203The five most frequent loca-tion reference patterns:in <?.>of <?.>show_me <?.>on <?.>from <?.> to <?.>9.95%8.15%6.39%5.69%3.95%Table 2.
Overview of the original Voyagerdata (turn = sentence).Although the above table covers the original data, thenewly generated Virtual Modality database has the samecharacteristics since the location references there be-come deictic references.5 ExperimentsThe experimental setup is depicted in Figure 4.
The coreof the system is the Galaxy Communicator architectureextended with the Batchmode server (as explained inSection 3 and shown in more details in Figure 2).
Itmust be noted that although the sentences are takenfrom dialogues, each sentence is processed independ-ently so the focus of attention is the new aspect intro-duced by the Virtual Modality.There are two runs for each experiment.
First, theoriginal sentences are input to the Batchmode server andthen passed to the Voyager application via the Galaxyarchitecture.
The outcomes are the correspondingframes from the Language Understanding server (theContext Resolution server is not invoked due to the ab-sence of context in this case).
The second run takes theVirtual Modality data, namely the new sentences withthe deictic references and the accompanying data for theVirtual Modality channel (semantic value, begin-endmarkers).
The output frames are produced by the Lan-guage Understanding module and further processed bythe Context Resolution server to resolve deictic refer-ences.The last step of the execution is the comparison ofthe frame pairs: one frame for the original sentence andthe other for the Virtual Modality data.The results presented below are from the very initialtests; clearly more work is needed to justify the conceptof Virtual Modality, as well as to fully investigate theutilization of the generated data in testing.The initial experiments were run on 436 sentences,which represent a small portion of the entire database.The results indicate that if only one deictic reference persentence is used with zero deviation, the generated out-put frames are identical to the original sentence outputframes in 82.03% of the cases.
The erroneous resultsoccurred when the preposition and a chosen deictic formtogether formed an ungrammatical expression (e.g.
?how about on over there??).
The data generation proc-ess requires further refinements to decide whether apreposition can be used with a randomly selected deicticexpression.In sentences with two deictic references only78.26% agreement was achieved.
The major reason forthis is the incorrect replacement of highways and high-way numbers with deictic references by the data genera-tion process.
Also, awkward combinations of deicticreferences result in incorrect resolution.
All these prob-lems will be addressed in future work.Additionally, since the current version of the Con-text Resolution server has no built-in time limits forresolving deictic references, future work will aim toincorporate some kind of temporal considerations andadaptivity.
The Virtual Modality data creation processsupports the generation of a large amount of time-shifted versions of the original data, which can be usedfor further testing of the system?s temporal robustness.GalaxyCommunicatorBatchmodeserverVM data MM eventdatabaseOriginalsentences{c directions:subject 1:domain "Voyager":utterance_id 6:pred {p from:topic {q university:name "Harvard University" } }:pred {p to:topic {q university:name ?MIT" } }Original frame{c directions:subject 1:domain "Voyager":utterance_id 6:pred {p from:topic {q university:name "Harvard University" } }:pred {p to:topic {q pronoun:name ?here" } }VM frame?
?From Harvard University to MIT?
?From Harvard University to here?D1 = ?MIT?D1_begin = 4D1_end    = 5{c directions:subject 1:domain "Voyager":utterance_id 6:pred {p from:topic {q university:name "Harvard University" } }:pred {p to:topic {q university:name ?MIT" } }After Context ResolutionFigure 4.
The evaluation procedure6 Summary and Future WorkAn experimental framework has been introduced, calledVirtual Modality, which aims to assist in the develop-ment and testing of multimodal systems.
The paper ex-plained the motivation behind generating (simulated)multimodal data from available textual representationsof logged user inputs.
The procedure of replacing loca-tion references with deictic expressions, as well as theplacement of the referenced semantic value along withtemporal information to the Virtual Modality channelwere explained.
Lastly, the evaluation scenario and pre-liminary test results were presented.Future work will investigate the following topics:?
how the generated Virtual Modality data can beutilized to train a statistical (or hybrid) multimo-dal integration module;?
how adaptivity in the integration module can beachieved using a vast amount of training data;?
how N-best choices in the Virtual Modality inputcan be utilized;?
whether disambiguation can be achieved withtraining examples covering all temporal cases;?
how evidence in one modality can help to resolvesome ambiguity in the other modality, and ulti-mately, how to provide an accurate interpretationof the overall user intention.7 AcknowledgementsThanks to Stephanie Seneff for her comments, toMitchell Peabody for setting up the database server andto D. Scott Cyphers for his always-available help withthe Galaxy system.The first author acknowledges the financial supportfrom the Nokia Foundation, Suomen Akatemia and theElla and Georg Ehrnrooth Foundation.ReferencesMichael H. Coen.
2001.
?Multimodal Integration ABiological View.?
17th International Joint Confer-ence on Artificial Intelligence, IJCAI 2001, Seattle,Washington, USA, August 4?10, pp.
1417?1424.Edward Filisko and Stephanie Seneff.
2003.
?A Con-text Resolution Server for the Galaxy ConversationalSystems.?
Eurospeech?2003.
Geneva, Switzerland,September 1?4, pp.
197?200.J.
Glass, G. Flammia, D. Goodine, M. Phillips, J. Poli-froni, S. Sakai, S. Seneff, and V. Zue.
1995.
?Multi-lingual Spoken-Language Understanding in the MITVoyager System,?
Speech Communication, 17(1-2):1?18.Pentti O. Haikonen.
2003.
The Cognitive Approach toConscious Machines.
Imprint Academic, Exeter, UK.Saija-Maaria Lemmel?
and P?ter P?l Boda.
2002.
?Ef-ficient Combination of Type-In and Wizard-of-OzTests in Speech Interface Development Process.
?ICSLP'2002, Denver, CO, September 16?20, pp.1477?1480.Ivan Marsic and Bogdan Dorohonceanu.
2003.
?Flexi-ble User Interfaces for Group Collaboration?.
Inter-national Journal of Human-Computer Interaction,Vol.15, No.3, pp.
337-360.Sharon Oviatt, Antonella DeAngeli and Karen Kuhn.1997.
?Integration and Synchronization of InputModes During Multimodal Human-Computer Inter-action.?
Conference on Human Factors in Comput-ing Systems, CHI '97.
ACM Press, New York.Sharon Oviatt, Rachel Coulston, Stefanie Tomko, Ben-fang Xiao, Rebecca Lunsford, Matt Wesson, andLesley Carmichael.
2003.
?Toward a Theory of Or-ganized Multimodal Integration Patterns DuringHuman-Computer Interaction.?
5th InternationalConference on Multimodal Interfaces, ICMI?2003,Vancouver, British Columbia, Canada, pp.
44?51.Joseph Polifroni and Stephanie Seneff.
2000.
?Galaxy-II as an Architecture for Spoken Dialogue Evalua-tion.?
2nd International Conference on Language Re-sources and Evaluation (LREC).
Athens, Greece.Stephanie Seneff.
1992.
?TINA: A Natural LanguageSystem for Spoken Language Applications,?
Com-putational Linguistics, 18(1):61?86.Stephanie Seneff.
2002.
?Response Planning and Gen-eration in the MERCURY Flight Reservation System,?Computer Speech and Language, 16:283?312.Stephanie Seneff, Chian Chuu, and D. Scott Cyphers.2000.
?ORION:  From On-line Interaction to Off-lineDelegation.?
ICSLP?2000, Beijing, China, October,pp.
142?145.Stephanie Seneff, Ed Hurley, Raymond Lau, ChristinePao, P. Schmid, and Victor Zue.
1998.
?Galaxy-II:A Reference Architecture for Conversational SystemDevelopment.?
ICSLP?1998, pp.
931?934.Sy Bor Wang.
2003.
A Multimodal Galaxy-basedGeographic System.
S.M.
thesis, MIT Department ofElectrical Engineering and Computer Science.Victor Zue, Stephanie Seneff, Joseph Polifroni, MichaelPhillips, Christine Pao, D. Goodine, David Goddeau,and James Glass.
1994.
?PEGASUS: A Spoken Dia-logue Interface for On-line Air Travel Planning,?Speech Communication, 15(3?4):331?340.Victor Zue, Stephanie Seneff, James Glass, Joseph Poli-froni, Christine Pao, Timothy J. Hazen, and I. LeeHetherington.
2000.
?JUPITER:  A Telephone-basedConversational Interface for Weather Information,?IEEE Transactions on Speech and Audio Processing,8(1):85?96.
