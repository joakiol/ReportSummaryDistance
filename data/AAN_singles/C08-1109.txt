Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 865?872Manchester, August 2008The Ups and Downs of Preposition Error Detection in ESL WritingJoel R. TetreaultEducational Testing Service660 Rosedale RoadPrinceton, NJ, USAJTetreault@ets.orgMartin ChodorowHunter College of CUNY695 Park AvenueNew York, NY, USAmartin.chodorow@hunter.cuny.eduAbstractIn this paper we describe a methodologyfor detecting preposition errors in the writ-ing of non-native English speakers.
Oursystem performs at 84% precision andclose to 19% recall on a large set of stu-dent essays.
In addition, we address theproblem of annotation and evaluation inthis domain by showing how current ap-proaches of using only one rater can skewsystem evaluation.
We present a samplingapproach to circumvent some of the issuesthat complicate evaluation of error detec-tion systems.1 IntroductionThe long-term goal of our work is to develop asystem which detects errors in grammar and us-age so that appropriate feedback can be given tonon-native English writers, a large and growingsegment of the world?s population.
Estimates arethat in China alone as many as 300 million peo-ple are currently studying English as a second lan-guage (ESL).
Usage errors involving prepositionsare among the most common types seen in thewriting of non-native English speakers.
For ex-ample, (Izumi et al, 2003) reported error rates forEnglish prepositions that were as high as 10% ina Japanese learner corpus.
Errors can involve in-correct selection (?we arrived to the station?
), ex-traneous use (?he went to outside?
), and omission(?we are fond null beer?).
What is responsiblefor making preposition usage so difficult for non-native speakers?c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.At least part of the difficulty seems to be due tothe great variety of linguistic functions that prepo-sitions serve.
When a preposition marks the ar-gument of a predicate, such as a verb, an ad-jective, or a noun, preposition selection is con-strained by the argument role that it marks, thenoun which fills that role, and the particular predi-cate.
Many English verbs also display alternations(Levin, 1993) in which an argument is sometimesmarked by a preposition and sometimes not (e.g.,?They loaded the wagon with hay?
/ ?They loadedhay on the wagon?).
When prepositions introduceadjuncts, such as those of time or manner, selec-tion is constrained by the object of the preposition(?at length?, ?in time?, ?with haste?).
Finally, theselection of a preposition for a given context alsodepends upon the intended meaning of the writer(?we sat at the beach?, ?on the beach?, ?near thebeach?, ?by the beach?
).With so many sources of variation in Englishpreposition usage, we wondered if the task of se-lecting a preposition for a given context mightprove challenging even for native speakers.
Toinvestigate this possibility, we randomly selected200 sentences from Microsoft?s Encarta Encyclo-pedia, and, in each sentence, we replaced a ran-domly selected preposition with a blank line.
Wethen asked two native English speakers to performa cloze task by filling in the blank with the bestpreposition, given the context provided by the restof the sentence.
Our results showed only about75% agreement between the two raters, and be-tween each of our raters and Encarta.The presence of so much variability in prepo-sition function and usage makes the task of thelearner a daunting one.
It also poses special chal-lenges for developing and evaluating an NLP errordetection system.
This paper addresses both the865development and evaluation of such a system.First, we describe a machine learning systemthat detects preposition errors in essays of ESLwriters.
To date there have been relatively fewattempts to address preposition error detection,though the sister task of detecting determiner er-rors has been the focus of more research.
Our sys-tem performs comparably with other leading sys-tems.
We extend our previous work (Chodorow etal., 2007) by experimenting with combination fea-tures, as well as features derived from the GoogleN-Gram corpus and Comlex (Grishman et al,1994).Second, we discuss drawbacks in current meth-ods of annotating ESL data and evaluating errordetection systems, which are not limited to prepo-sition errors.
While the need for annotation bymultiple raters has been well established in NLPtasks (Carletta, 1996), most previous work in errordetection has surprisingly relied on only one raterto either create an annotated corpus of learner er-rors, or to check the system?s output.
Some gram-matical errors, such as number disagreement be-tween subject and verb, no doubt show very highreliability, but others, such as usage errors involv-ing prepositions or determiners are likely to bemuch less reliable.
Our results show that relyingon one rater for system evaluation can be problem-atic, and we provide a sampling approach whichcan facilitate using multiple raters for this task.In the next section, we describe a system thatautomatically detects errors involving incorrectpreposition selection (?We arrived to the station?
)and extraneous preposition usage (?He went tooutside?).
In sections 3 and 4, we discuss theproblem of relying on only one rater for exhaus-tive annotation and show how multiple raters canbe used more efficiently with a sampling approach.Finally, in section 5 we present an analysis of com-mon preposition errors that non-native speakersmake.2 System2.1 ModelWe have used a Maximum Entropy (ME) classi-fier (Ratnaparkhi, 1998) to build a model of correctpreposition usage for 34 common English prepo-sitions.
The classifier was trained on 7 millionpreposition contexts extracted from parts of theMetaMetrics Lexile corpus that contain textbooksand other materials for high school students.
Eachcontext was represented by 25 features consistingof the words and part-of-speech (POS) tags foundin a local window of +/- two positions around thepreposition, plus the head verb of the precedingverb phrase (PV), the head noun of the precedingnoun phrase (PN), and the head noun of the follow-ing noun phrase (FH), among others.
In analyz-ing the contexts, we used only tagging and heuris-tic phrase-chunking, rather than parsing, so as toavoid problems that a parser might encounter withill-formed non-native text1.
In test mode, the clas-sifier was given the context in which a prepositionoccurred, and it returned a probability for each ofthe 34 prepositions.2.2 Other ComponentsWhile the ME classifier constitutes the core of thesystem, it is only one of several processing com-ponents that refines or blocks the system?s output.Since the goal of an error detection system is toprovide diagnostic feedback to a student, typicallya system?s output is heavily constrained so that itminimizes false positives (i.e., the system tries toavoid saying a writer?s preposition is used incor-rectly when it is actually right), and thus does notmislead the writer.Pre-Processing Filter: A pre-processing pro-gram skips over preposition contexts that containspelling errors.
Classifier performance is poor insuch cases because the classifier was trained onwell-edited text, i.e., without misspelled words.
Inthe context of a diagnostic feedback and assess-ment tool for writers, a spell checker would firsthighlight the spelling errors and ask the writer tocorrect them before the system analyzed the prepo-sitions.Post-Processing Filter: After the ME clas-sifier has output a probability for each of the 34prepositions but before the system has made its fi-nal decision, a series of rule-based post-processingfilters block what would otherwise be false posi-tives that occur in specific contexts.
The first filterprevents the classifier from marking as an error acase where the classifier?s most probable preposi-tion is an antonym of what the writer wrote, suchas ?with/without?
and ?from/to?.
In these cases,resolution is dependent on the intent of the writerand thus is outside the scope of information cap-1For an example of a common ungrammatical sentencefrom our corpus, consider: ?In consion, for some reasons,museums, particuraly known travel place, get on many peo-ple.
?866tured by the current feature set.
Another problemfor the classifier involves differentiating betweencertain adjuncts and arguments.
For example, inthe sentence ?They described a part for a kid?, thesystem?s top choices were of and to.
The benefac-tive adjunct introduced by for is difficult for theclassifier to learn, perhaps because it so freely oc-curs in many locations within a sentence.
A post-processing filter prevents the system from markingas an error a prepositional phrase that begins withfor and has an object headed by a human noun (aWordNet hyponym of person or group).Extraneous Use Filter: To cover extraneoususe errors, we developed two rule-based filters:1) Plural Quantifier Constructions, to handle casessuch as ?some of people?
and 2) Repeated Prepo-sitions, where the writer accidentally repeated thesame preposition two or more times, such as ?canfind friends with with?.
We found that extrane-ous use errors usually constituted up to 18% of allpreposition errors, and our extraneous use filtershandle a quarter of that 18%.Thresholding: The final step for the preposi-tion error detection system is a set of thresholdsthat allows the system to skip cases that are likelyto result in false positives.
One of these is wherethe top-ranked preposition and the writer?s prepo-sition differ by less than a pre-specified amount.This was also meant to avoid flagging cases wherethe system?s preposition has a score only slightlyhigher than the writer?s preposition score, such as:?My sister usually gets home around 3:00?
(writer:around = 0.49, system: by = 0.51).
In these cases,the system?s and the writer?s prepositions both fitthe context, and it would be inappropriate to claimthe writer?s preposition was used incorrectly.
An-other system threshold requires that the probabil-ity of the writer?s preposition be lower than a pre-specified value in order for it to be flagged as anerror.
The thresholds were set so as to strongly fa-vor precision over recall due to the high number offalse positives that may arise if there is no thresh-olding.
This is a tactic also used for determinerselection in (Nagata et al, 2006) and (Han et al,2006).
Both thresholds were empirically set on adevelopment corpus.2.3 Combination FeaturesME is an attractive choice of machine learning al-gorithm for a problem as complex as prepositionerror detection, in no small part because of theavailability of ME implementations that can han-dle many millions of training events and features.However, one disadvantage of ME is that it doesnot automatically model the interactions amongfeatures as some other approaches do, such as sup-port vector machines (Jurafsky and Martin, 2008).To overcome this, we have experimented with aug-menting our original feature set with ?combinationfeatures?
which represent richer contextual struc-ture in the form of syntactic patterns.Table 1 (first column) illustrates the four com-bination features used for the example context?take our place in the line?.
The p denotes apreposition, so N-p-N denotes a syntactic contextwhere the preposition is preceded and followedby a noun phrase.
We use the preceding nounphrase (PN) and following head (FH) from theoriginal feature set for the N-p-N feature.
Column3 shows one instantiation of combination features:Combo:word.
For the N-p-N feature, the cor-responding Combo:word instantiation is ?place-line?
since ?place?
is the PN and ?line?
is theFH.
We also experimented with using combina-tions of POS tags (Combo:tag) and word+tag com-binations (Combo:word+tag).
So for the example,the Combo:tag N-p-N feature would be ?NN-NN?,and the Combo:word+tag N-p-N feature would beplace NN+line NN (see the fourth column of Ta-ble 1).
The intuition with the Combo:tag featuresis that the Combo:word features have the potentialto be sparse, and these capture more general pat-terns of usage.We also experimented with other features suchas augmenting the model with verb-prepositionpreferences derived from Comlex (Grishman et al,1994), and querying the Google Terabyte N-gramcorpus with the same patterns used in the combina-tion features.
The Comlex-based features did notimprove the model, and though the Google N-gramcorpus represents much more information than our7 million event model, its inclusion improved per-formance only marginally.2.4 EvaluationIn our initial evaluation of the system we col-lected a corpus of 8,269 preposition contexts,error-annotated by two raters using the scheme de-scribed in Section 3 to serve as a gold standard.
Inthis study, we focus on two of the three types ofpreposition errors: using the incorrect prepositionand using an extraneous preposition.
We compared867Class Components Combo:word Features Combo:tag Featuresp-N FH line NNN-p-N PN-FH place-line NN-NNV-p-N PV-PN take-line VB-NNV-N-p-N PV-PN-FH take-place-line VB-NN-NNTable 1: Feature Examples for take our place in the linedifferent models: the baseline model of 25 featuresand baseline with combination features added.
Theprecision and recall for the top performing mod-els are shown in Table 2.
These results do not in-clude the extraneous use filter; this filter generallyincreased precision by as much as 2% and recallby as much as 5%.Evaluation Metrics In the tasks of determinerand preposition selection in well-formed, nativetexts (such as (Knight and Chander, 1994), (Min-nen et al, 2000), (Turner and Charniak, 2007) and(Gamon et al, 2008)), the evaluation metric mostcommonly used is accuracy.
In these tasks, onecompares the system?s output on a determiner orpreposition to the gold standard of what the writeroriginally wrote.
However, in the tasks of deter-miner and preposition error detection, precisionand recall are better metrics to use because oneis only concerned with a subset of the preposi-tions (or determiners), those used incorrectly, asopposed to all of them in the selection task.
Inessence, accuracy has the problem of distortingsystem performance.Results The baseline system (described in(Chodorow et al, 2007)) performed at 79.8% pre-cision and 11.7% recall.
Next we tested the differ-ent combination models: word, tag, word+tag, andall three.
Surprisingly, three of the four combina-tion models: tag, word+tag, all, did not improveperformance of the system when added to themodel, but using just the +Combo:word featuresimproved recall by 1%.
We use the +Combo:wordmodel to test our sampling approach in section 4.As a final test, we tuned our training corpus of7 million events by removing any contexts withunknown or misspelled words, and then retrainedthe model.
This ?purge?
resulted in a removalof nearly 200,000 training events.
With this newtraining corpus, the +Combo:tag feature showedthe biggest improvement over the baseline, withan improvement in both precision (+2.3%) and re-call (+2.4%) to 82.1% and 14.1% respectively (lastline of Table 2.
While this improvement may seemsmall, it is in part due to the difficulty of the prob-lem, but also the high baseline system score thatwas established in our prior work (Chodorow etal., 2007).It should be noted that with the inclusionof the extraneous use filter, performance of the+Combo:tag rose to 84% precision and close to19% recall.Model Precision RecallBaseline 79.8% 11.7%+Combo:word 79.8% 12.8%+Combo:tag (with purge) 82.1% 14.1%Table 2: Best System Results on Incorrect Selec-tion Task2.5 Related WorkCurrently there are only a handful of approachesthat tackle the problem of preposition error detec-tion in English learner texts.
(Gamon et al, 2008)used a language model and decision trees to de-tect preposition and determiner errors in the CLECcorpus of learner essays.
Their system performs at79% precision (which is on par with our system),however recall figures are not presented thus mak-ing comparison difficult.
In addition, their eval-uation differs from ours in that they also includeerrors of omission, and their work focuses on thetop twelve most frequent prepositions, while ourshas greater coverage with the top 34.
(Izumi etal., 2003) and (Izumi et al, 2004) used an ME ap-proach to classify different grammatical errors intranscripts of Japanese interviews.
They do notpresent performance of prepositions specifically,but overall performance for the 13 error typesthey target reached 25% precision and 7% recall.
(Eeg-Olofsson and Knuttson, 2003) created a rule-based approach to detecting preposition errors inSwedish language learners (unlike the approachespresented here, which focus on English languagelearners), and their system performed at 25% ac-curacy.
(Lee and Seneff, 2006) used a languagemodel to tackle the novel problem of prepositionselection in a dialogue corpus.
While their perfor-mance results are quite high, 88% precision and86878% recall, it should be noted that their evaluationwas on a small corpus with a highly constraineddomain, and focused on a limited number of prepo-sitions, thus making direct comparison with ourapproach difficult.Although our recall figures may seem low, es-pecially when compared to other NLP tasks suchas parsing and anaphora resolution, this is really areflection of how difficult the task is.
For example,in the problem of preposition selection in nativetext, a baseline using the most frequent preposition(of) results in precision and recall of 26%.
In addi-tion, the cloze tests presented earlier indicate thateven in well-formed text, agreement between na-tive speakers on preposition selection is only 75%.In texts written by non-native speakers, rater dis-agreement increases, as will be shown in the nextsection.3 Experiments with Multiple RatersWhile developing an error detection system forprepositions is certainly challenging, given the re-sults from our work and others, evaluation alsoposes a major challenge.
To date, single humanannotation has typically been the gold standard forgrammatical error detection, such as in the workof (Izumi et al, 2004), (Han et al, 2006), (Nagataet al, 2006), (Eeg-Olofsson and Knuttson, 2003)2.Another method for evaluation is verification ((Ga-mon et al, 2008), where a human rater checks overa system?s output.
The drawbacks of this approachare: 1. every time the system is changed, a rateris needed to re-check the output, and 2. it is veryhard to estimate recall.
What these two evaluationmethods have in common is that they side-step theissue of annotator reliability.In this section, we show how relying on only onerater can be problematic for difficult error detec-tion tasks, and in section 4, we propose a method(?the sampling approach?)
for efficiently evaluat-ing a system that does not require the amount ofeffort needed in the standard approach to annota-tion.3.1 AnnotationTo create a gold-standard corpus of error anno-tations for system evaluation, and also to deter-mine whether multiple raters are better than one,2(Eeg-Olofsson and Knuttson, 2003) had a small evalua-tion on 40 preposition contexts and it is unclear whether mul-tiple annotators were used.we trained two native English speakers with priorNLP annotation experience to annotate prepositionerrors in ESL text.
The training was very exten-sive: both raters were trained on 2000 preposi-tion contexts and the annotation manual was it-eratively refined as necessary.
To summarize theprocedure, the two raters were shown sentencesrandomly selected from student essays with eachpreposition highlighted in the sentence.
Theymarked each context (?2-word window around thepreposition, plus the commanding verb) for gram-mar and spelling errors, and then judged whetherthe writer used an incorrect preposition, a correctpreposition, or an extraneous preposition.
Finally,the raters suggested prepositions that would bestfit the context, even if there were no error (somecontexts can license multiple prepositions).3.2 ReliabilityEach rater judged approximately 18,000 prepo-sitions contexts, with 18 sets of 100 contextsjudged by both raters for purposes of comput-ing kappa.
Despite the rigorous training regimen,kappa ranged from 0.411 to 0.786, with an overallcombined value of 0.630.
Of the prepositions thatRater 1 judged to be errors, Rater 2 judged 30.2%to be acceptable.
Conversely, of the prepositionsRater 2 judged to be erroneous, Rater 1 found38.1% acceptable.
The kappa of 0.630 shows thedifficulty of this task and also shows how twohighly trained raters can produce very differentjudgments.
Details on our annotation and humanjudgment experiments can be found in (Tetreaultand Chodorow, 2008).Variability in raters?
judgments translates tovariability of system evaluation.
For instance, inour previous work (Chodorow et al, 2007), wefound that when our system?s output was com-pared to judgments of two different raters, therewas a 10% difference in precision and a 5% differ-ence in recall.
These differences are problematicwhen evaluating a system, as they highlight the po-tential to substantially over- or under-estimate per-formance.4 Sampling ApproachThe results from the previous section motivate theneed for a more refined evaluation.
They sug-gest that for certain error annotation tasks, such aspreposition usage, it may not be appropriate to useonly one rater and that if one uses multiple raters869for error annotation, there is the possibility of cre-ating an adjudicated set, or at least calculating thevariability of the system?s performance.
However,annotation with multiple raters has its own disad-vantages as it is much more expensive and time-consuming.
Even using one rater to produce asizeable evaluation corpus of preposition errors isextremely costly.
For example, if we assume that500 prepositions can be annotated in 4 hours us-ing our annotation scheme, and that the base ratefor preposition errors is 10%, then it would take atleast 80 hours for a rater to find and mark 1000 er-rors.
In this section, we propose a more efficientannotation approach to circumvent this problem.4.1 MethodologyFigure 1: Sampling Approach ExampleThe sampling procedure outlined here is in-spired by the one described in (Chodorow and Lea-cock, 2000) for the task of evaluating the usage ofnouns, verbs and adjectives.
The central idea isto skew the annotation corpus so that it contains agreater proportion of errors.Here are the steps in the procedure:1.
Process a test corpus of sentences so that eachpreposition in the corpus is labeled ?OK?
or?Error?
by the system.2.
Divide the processed corpus into two sub-corpora, one consisting of the system?s ?OK?prepositions and the other of the system?s?Error?
prepositions.
For the hypotheticaldata in Figure 1, the ?OK?
sub-corpus con-tains 90% of the prepositions, and the ?Error?sub-corpus contains the remaining 10%.3.
Randomly sample cases from each sub-corpus and combine the samples into an an-notation set that is given to a ?blind?
humanrater.
We generally use a higher samplingrate for the ?Error?
sub-corpus because wewant to ?enrich?
the annotation set with alarger proportion of errors than is found in thetest corpus as a whole.
In Figure 1, 75% ofthe ?Error?
sub-corpus is sampled while only16% of the ?OK?
sub-corpus is sampled.4.
For each case that the human rater judges tobe an error, check to see which sub-corpus itcame from.
If it came from the ?OK?
sub-corpus, then the case is a Miss (an error thatthe system failed to detect).
If it came fromthe ?Error?
sub-corpus, then the case is a Hit(an error that the system detected).
If the raterjudges a case to be a correct usage and it camefrom the ?Error?
sub-corpus, then it is a FalsePositive (FP).5.
Calculate the proportions of Hits and FPs inthe sample from the ?Error?
sub-corpus.
Forthe hypothetical data in Figure 1, these val-ues are 600/750 = 0.80 for Hits, and 150/750= 0.20 for FPs.
Calculate the proportion ofMisses in the sample from the ?OK?
sub-corpus.
For the hypothetical data, this is450/1500 = 0.30 for Misses.6.
The values computed in step 5 are conditionalproportions based on the sub-corpora.
To cal-culate the overall proportions in the test cor-pus, it is necessary to multiply each valueby the relative size of its sub-corpus.
Thisis shown in Table 3, where the proportion ofHits in the ?Error?
sub-corpus (0.80) is mul-tiplied by the relative size of the ?Error?
sub-corpus (0.10) to produce an overall Hit rate(0.08).
Overall rates for FPs and Misses arecalculated in a similar manner.7.
Using the values from step 6, calculate Preci-sion (Hits/(Hits + FP)) and Recall (Hits/(Hits+ Misses)).
These are shown in the last tworows of Table 3.Estimated Overall RatesSample Proportion * Sub-Corpus ProportionHits 0.80 * 0.10 = 0.08FP 0.20 * 0.10 = 0.02Misses 0.30 * 0.90 = 0.27Precision 0.08/(0.08 + 0.02) = 0.80Recall 0.08/(0.08 + 0.27) = 0.23Table 3: Sampling Calculations (Hypothetical)870This method is similar in spirit to active learning((Dagan and Engelson, 1995) and (Engelson andDagan, 1996)), which has been used to iterativelybuild up an annotated corpus, but it differs fromactive learning applications in that there are no it-erative loops between the system and the humanannotator(s).
In addition, while our methodologyis used for evaluating a system, active learning iscommonly used for training a system.4.2 ApplicationNext, we tested whether our proposed sam-pling approach provides good estimates of a sys-tem?s performance.
For this task, we used the+Combo:word model to separate a large corpusof student essays into the ?Error?
and ?OK?
sub-corpora.
The original corpus totaled over 22,000prepositions which would normally take severalweeks for two raters to double annotate and thenadjudicate.
After the two sub-corpora were propor-tionally sampled, this resulted in an annotation setof 752 preposition contexts (requiring roughly 6hours for annotation), which is substantially moremanageable than the full corpus.
We had bothraters work together to make judgments for eachpreposition.It is important to note that while these are notthe exact same essays used in the previous evalua-tion of 8,269 preposition contexts, they come fromthe same pool of student essays and were on thesame topics.
Given these strong similarities, wefeel that one can compare scores between the twoapproaches.
The precision and recall scores forboth approaches are shown in Table 4 and are ex-tremely similar, thus suggesting that the samplingapproach can be used as an alternative to exhaus-tive annotation.Precision RecallStandard Approach 80% 12%Sampling Approach 79% 14%Table 4: Sampling ResultsIt is important with the sampling approach to useappropriate sample sizes when drawing from thesub-corpora, because the accuracy of the estimatesof hits and misses will depend upon the propor-tion of errors in each sub-corpus as well as on thesample sizes.
The OK sub-corpus is expected tohave even fewer errors than the overall base rate,so it is especially important to have a relativelylarge sample from this sub-corpus.
The compari-son study described above used an OK sub-corpussample that was twice as large as the Error sub-corpus sample (about 500 contexts vs. 250 con-texts).In short, the sampling approach is intended toalleviate the burden on annotators when faced withthe task of having to rate several thousand errorsof a particular type in order to produce a sizeableerror corpus.
On the other hand, one advantagethat exhaustive annotation has over the samplingmethod is that it makes possible the comparisonof multiple systems.
With the sampling approach,one would have to resample and annotate for eachsystem, thus multiplying the work needed.5 Analysis of Learner ErrorsOne aspect of automatic error detection that usu-ally is under-reported is an analysis of the errorsthat learners typically make.
The obvious benefitof this analysis is that it can focus development ofthe system.From our annotated set of preposition errors,we found that the most common prepositionsthat learners used incorrectly were in (21.4%), to(20.8%) and of (16.6%).
The top ten prepositionsaccounted for 93.8% of all preposition errors in ourlearner corpus.Next, we ranked the common preposition ?con-fusions?, the common mistakes made for eachpreposition.
The top ten most common confusionsare listed in Table 5, where null refers to caseswhere no preposition is licensed (the writer usedan extraneous preposition).
The most common of-fenses were actually extraneous errors (see Table5): using to and of when no preposition was li-censed accounted for 16.8% of all errors.It is interesting to note that the most commonusage errors by learners overwhelmingly involvedthe ten most frequently occurring prepositions innative text.
This suggests that our effort to handlethe 34 most frequently occurring prepositions maybe overextended and that a system that is specif-ically trained and refined on the top ten preposi-tions may provide better diagnostic feedback to alearner.6 ConclusionsThis paper has two contributions to the field oferror detection in non-native writing.
First, wediscussed a system that detects preposition errorswith high precison (up to 84%) and is competitive871Writer?s Prep.
Rater?s Prep.
Frequencyto null 9.5%of null 7.3%in at 7.1%to for 4.6%in null 3.2%of for 3.1%in on 3.1%of in 2.9%at in 2.7%for to 2.5%Table 5: Common Preposition Confusionswith other leading methods.
We used an MEapproach augmented with combination featuresand a series of thresholds.
This system is currentlyincorporated in the Criterion writing evaluationservice.
Second, we showed that the standard ap-proach to evaluating NLP error detection systems(comparing a system?s output with a gold-standardannotation) can greatly skew system results whenthe annotation is done by only one rater.
However,one reason why a single rater is commonly usedis that building a corpus of learner errors can beextremely costly and time consuming.
To addressthis efficiency issue, we presented a samplingapproach that produces results comparable toexhaustive annotation.
This makes using multipleraters possible since less time is required toassess the system?s performance.
While the workpresented here has focused on prepositions, thearguments against using only one rater, and forusing a sampling approach generalize to othererror types, such as determiners and collocations.Acknowledgements We would first like tothank our two annotators Sarah Ohls and WaverlyVanWinkle for their hours of hard work.
We wouldalso like to acknowledge the three anonymous re-viewers and Derrick Higgins for their helpful com-ments and feedback.ReferencesCarletta, J.
1996.
Assessing agreement on classifica-tion tasks: The kappa statistic.
Computational Lin-guistics, pages 249?254.Chodorow, M. and C. Leacock.
2000.
An unsupervisedmethod for detecting grammatical errors.
In NAACL.Chodorow, M., J. Tetreault, and N-R. Han.
2007.
De-tection of grammatical errors involving prepositions.In Proceedings of the Fourth ACL-SIGSEM Work-shop on Prepositions.Dagan, I. and S. Engelson.
1995.
Committee-basedsampling for training probabilistic classifiers.
InProceedings of ICML, pages 150?157.Eeg-Olofsson, J. and O. Knuttson.
2003.
Automaticgrammar checking for second language learners - theuse of prepositions.
In Nodalida.Engelson, S. and I. Dagan.
1996.
Minimizing manualannotation cost in supervised training from corpora.In Proceedings of ACL, pages 319?326.Gamon, M., J. Gao, C. Brockett, A. Klementiev, W. B.Dolan, D. Belenko, and L. Vanderwende.
2008.
Us-ing contextual speller techniques and language mod-eling for esl error correction.
In IJCNLP.Grishman, R., C. Macleod, and A. Meyers.
1994.Comlex syntax: Building a computational lexicon.In COLING.Han, N-R., M. Chodorow, and C. Leacock.
2006.
De-tecting errors in english article usage by non-nativespeakers.
Natural Language Engineering, 12:115?129.Izumi, E., K. Uchimoto, T. Saiga, T. Supnithi, andH.
Isahara.
2003.
Automatic error detection in theJapanese leaners?
English spoken data.
In ACL.Izumi, E., K. Uchimoto, and H. Isahara.
2004.
Theoverview of the sst speech corpus of Japanese learnerEnglish and evaluation through the experiment onautomatic detection of learners?
errors.
In LREC.Jurafsky, D. and J. Martin.
2008.
Speech and LanguageProcessing (2nd Edition).
Prentice Hall.
To Appear.Knight, K. and I. Chander.
1994.
Automated postedit-ing of documents.
In Conference on Artificial Intel-ligence.Lee, J. and S. Seneff.
2006.
Automatic grammar cor-rection for second-language learners.
In Interspeech.Levin, B.
1993.
English verb classes and alternations:a preliminary investigation.
Univ.
of Chicago Press.Minnen, G., F. Bond, and A. Copestake.
2000.Memory-based learning for article generation.
InCoNLL.Nagata, R., A. Kawai, K. Morihiro, and N. Isu.
2006.A feedback-augmented method for detecting errorsin the writing of learners of English.
In Proceedingsof the ACL/COLING.Ratnaparkhi, A.
1998.
Maximum Entropy Models fornatural language ambiguity resolution.
Ph.D. thesis,University of Pennsylvania.Tetreault, J. and M. Chodorow.
2008.
Native Judg-ments of non-native usage: Experiments in preposi-tion error detection.
In COLING Workshop on Hu-man Judgments in Computational Linguistics.Turner, J. and E. Charniak.
2007.
Language modelingfor determiner selection.
In HLT/NAACL.872
