Coling 2008: Companion volume ?
Posters and Demonstrations, pages 39?42Manchester, August 2008The Impact of Reference Quality on Automatic MT EvaluationOlivier Hamon1,2 and Djamel Mostefa1(1) Evaluation and Language Resources Distribution Agency (ELDA)55-57 rue Brillat-Savarin, 75013 Paris, France(2) LIPN (UMR 7030) ?
Universit?
Paris 13 & CNRS99 av.
J.-B.
Cl?ment, 93430 Villetaneuse, France{hamon,mostefa}@elda.orgAbstractLanguage resource quality is crucial inNLP.
Many of the resources used are de-rived from data created by human beingsout of an NLP context, especially regard-ing MT and reference translations.
In-deed, automatic evaluations need high-quality data that allow the comparison ofboth automatic and human translations.The validation of these resources iswidely recommended before being used.This paper describes the impact of usingdifferent-quality references on evalua-tion.
Surprisingly enough, similar scoresare obtained in many cases regardless ofthe quality.
Thus, the limitations of theautomatic metrics used within MT arealso discussed in this regard.1 IntroductionLanguage resources (LRs) are essential compo-nents in research and development of NLP sys-tems.
However, the production of most LRs isdone by human beings and is therefore subject toerrors or imperfections.
The creation of LRs re-quires a quality assurance procedure that helpscontrol their quality and make sure that theycomply with the specifications.The importance of validation criteria is evenhigher when it comes to evaluation, as referenceLRs are used to measure system performance and,thus, quality.
An evaluation must be done in asuitable qualitative framework and data usedshould be as good-quality as possible.
Bearing?
2008.
Licensed under the Creative Commons At-tribution-Noncommercial-Share Alike 3.0 Unportedlicense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.that in mind, validation standards have been de-fined (Van den Heuvel et al, 2003) and re-sources should follow the specifications as closeas possible for that purpose.The problem also applies to reference transla-tions in MT.
Most of the automatic metrics usedcompare human reference translations to the out-put obtained from MT systems.
Generally, morethan one reference is used to get multiple transla-tion possibilities (Papineni et al, 2001), but theevaluation of sentences depends highly on thehuman reference(s) translation(s) used.
However,only a few studies have gone deeper into a defi-nition of quality and have tried to detail how toevaluate it (Van den Heuvel & Sanders, 2006).N-gram metrics give scores that strongly dependon the reference and, thus, we wonder how muchscores computed with a poor reference transla-tion diverge from the ones computed with a highquality reference translation.
This paper focuseson two issues: 1) how to validate the quality of ahuman translation; 2) study of the impact of thequality of reference translations on MT evalua-tions.
The final objective behind this work is tofind out to what an extent a validation is usefulwithin the evaluation protocol.
The building ofreference translations is very time and moneyconsuming, but the cost of validation should notbe underestimated either (Fers?e et al, 2006).2 ContextIn our experiments, we used the material fromthe TC-STAR 1  second evaluation campaign(Mostefa et al, 2006) and the third one (Mostefaet al, 2007).
For both campaigns, three languagedirections were used: English-to-Spanish (EnEs),Spanish-to-English (EsEn) and Chinese-to-English (ZhEn).
Data came from European Par-liament Plenary Sessions (EPPS) for EnES and1http://www.tc-star.org39EsEn, Spanish Parliament Sessions (Cortes) forEsEn, and Voice of America (VoA) for ZhEn.Three kinds of input were considered: automatictranscriptions from Automatic Speech Recogni-tion (ASR) systems, manual transcriptions (Ver-batim) and Final Text Editions (FTE) providedby the European Parliament.
This represents 14sets consisting of source documents, referencetranslations translated twice by two differentagencies, and translations obtained from MT sys-tems.
Each set contains around 25,000 words.Therefore, we had an overall set of 28 referencetranslations on the evaluations, directions andinputs from both years.
During the campaigns,MT systems have been evaluated with automaticmetrics such as BLEU (Papineni et al, 2001).3 Validation3.1 GuidelinesThe quality of reference translations is consid-ered in two ways.
First, translation guidelines aregiven to the translators.
Then, translated files aresent to a validation agency in order to check theirquality according to the defined criteria.Guidelines were produced within the TC-STAR project.
They were discussed internallybut also with the Linguistic Data Consortium(LDC) who has had the experience of producingmany reference translations.Translation agencies are informed about thequality control and extra attention is paid to therecommendations given for translation quality:meaning and style should remain as close to theoriginal source documents as possible; no addi-tional annotations should be added to the transla-tion; capitalization has to be carefully respected;the translation of neologisms and unknownwords should take into account the speaker?s in-tention; date format should also follow the estab-lished conventions, etc.3.2 Criteria and ProcedureFor each reference translation of the threelanguage directions, the Speech ProcessingEXpertise centre (SPEX)2  validated 600 wordsfrom contiguous segments randomly selected.Translations were checked by professionaltranslators, who classified errors into categories.Points are given to references, according to thepenalty scheme presented in Table 1.2http://www.spex.nlPenalty points Error categoryYear 2 Year 3Syntactical 4 3Deviation from guidelines 3 -Lexical 2 3Poor usage 1 1Capitalization - 1Punctuation / spelling (max.10) 0.5 0.5Table 1.
Translation errors penalties.In order to be considered valid, a referencetranslation must have less than 40 penalty points.A non-valid reference translation is sent back tothe translation agency/ies to be proofread andimproved with the help of an errors report.3.3 Typical ErrorsMost errors are lexical ones, followed by poorusage of the language.
Syntactic and spellingcategory errors are considerably fewer.
In termson input type, the number of lexical, spelling andsyntactic errors is higher for FTE than Verbatim.On the other hand, the number of errors for usageand deviation from guidelines (including globaltranslation quality) is higher for Verbatim.Likewise, general errors are more frequent forEnglish-to-Spanish than for Spanish-to-English.Chinese-to-English produces many more errors,in particular lexical ones.
Syntactic errors couldbe wrong placement of adjective, wrong choiceof person for pronouns, wrong use of verb tenseor use of adjective as noun.
Deviations fromguidelines do not offer a wide variety: word/partof sentence omission, proper nouns mistransla-tion or translation quality/odd sentence problems.Thus, they have been regrouped under the othersfor the 3rd year evaluation.
Lexical errors showthe widest variety, probably due to the specificityof the vocabulary: mistranslation of acronyms,wrong word order, missing plural, literal transla-tion, bad terminology or approximation, wrongpreposition or translation inconsistencies.
Othererrors are wrong punctuation or spelling errors.All these errors will lower the quality of thereference translations, which would imply a bi-ased evaluation.
The aim of the validation is thento reduce, as much as possible, the impact ofmistranslation in order to improve a priori theassessment of the automatic translations.4 ResultsThe following format is adopted for each set:?Year/Data-Input_Direction?, e.g.
?3/EPPS-FTE_EsEn?
refers to the third-year set on Spanish-to-English using the FTE input on EPPS data.404.1 Results of the ValidationOn the overall 14 sets, 10 sets have been trans-lated again and revalidated at least once (for ei-ther one or the two reference translations).
Table2 gives the scores of validation for each of thesesets together with the Word Error Rate with itsrespective validated reference.
The left-hand sidenumber gives the result for the first reference andthe right-hand side one gives the result for thesecond reference.
The different lines for each setgive results for the different types of reference(from intermediate to final), thus showing theevolution of their validation.Validation score WER / finalreference SetRef 1 Ref 2 Ref 1 Ref 23/EPPS-FTE_EsEn59.51818104.573.53812.5-8.21.33/Cortes-FTE_EsEn43.53434120.570.5356.2-5.11.23/Cortes-Verb_EsEn5426.56722.50.5 0.33/VoA-Verb_ZhEn13053.527129373724.26.315.3-2/EPPS-FTE_EnEs23233123.5- 0.92/EPPS-FTE_EsEn18.518.53317- 2.62/EPPS-Verb_EnEs59.511.517170.7 -2/Cortes-FTE_EsEn42661.590.2 5.32/Cortes-Verb_EsEn6918.554020.7 4.72/VoA-Verb_ZhEn8439.5383817.0 -Table 2.
Validation scores of reference translationsand WER between intermediate (upper line) andfinal references (bottom line) for the first reference(Ref 1) and the second one (Ref 2).The mean score for the reference translationsbefore any correction takes place is around 71,while after correction this is around 23.
Thus,final translations are not perfect but their qualityis sufficient to serve as reference.
However, amaximum of 130 is obtained from the translationfor Chinese-to-English, which seems to be moredifficult than the other directions.
WER was alsocomputed between the non-validated translationsand their corresponding validated versions.
As itcan be observed, are not necessarily very highand many WER values are below 1%.4.2 Intermediate vs.
Final ReferenceWhen comparing the differences between thevalidation scores and WER for each translation,no correlation is found.
The correlation coeffi-cient between the score differences and the WERis around 58%.
For instance, a score differenceof 36 between non-validated and validated refer-ences corresponds to a WER of 0.2, while an-other difference of 26.5 corresponds to a WER of6.3.
There is no direct correlation between thequality of the references and the WER scores.Thus, a priori, the quality of reference transla-tions has no impact on the WER, which could beextended to the scoring of MT systems.
Indeed,if WER does not reflect in a precise manner thequality increase of a human translation, how canit be useful/reliable for scoring MT systems?Figure 1 presents the correlation between thesescore differences and WER.
It shows that qualityis not necessarily well correlated with WER.
Theexplanation is twofold: firstly, the improvementof a human translation does not necessarily implymany changes; secondly, WER does not reflectthe quality of a translation accurately, as it doesnot seem to focus on essential language issues.05101520250 20 40 60 80 100 120Score differenceWERFigure 1.
Correlation of score difference  and WERbetween non-validated and validated references.4.3 BLEU Results of MT SystemsBLEU scores have been computed for MT sys-tem output, using each set and each reference(whether validated or not).
Then, either scoresare quite identical or scores are slightly diver-gent.
With the aim of studying this in detail, weassembled together the mean difference BLEUscores and the WER (against the final reference)for all the intermediate reference translations, asshown in Table 3.The correlation coefficient between the abso-lute value of the mean difference score and meanof the WER is around 80%.
Thus, the changesmade into the references seem to have an impacton BLEU scores.
However, given that quality isnot correlated with WER, the absolute variationof the BLEU scores cannot be interpreted as a41difference in MT system quality.
It rather showsthat comparing systems is the only plausiblething with BLEU, instead of evaluating systemsin an absolute way.Set Mean diff.
BLEU scoreWER / finalreference3/EPPS-FTE_EsEn 1.18-0.0812.5 / 8.2- / 1.33/Cortes-FTE_EsEn 0.67 0.0356.2 / 5.1- / 1.23/Cortes-Verb_EsEn 0.021 0.5 / 0.33/VoA-Verb_ZhEn -1.71 0.00124.2 / 15.36.3 / -2/EPPS-FTE_EnEs 0.02 - / 0.92/EPPS-FTE_EsEn 0.24 - / 2.62/EPPS-Verb_EnEs -0.05 0.7 / -2/Cortes-FTE_EsEn 1.152 0.2 / 5.32/Cortes-Verb_EsEn -2.21 20.7 / 4.72/VoA-Verb_ZhEn 0.08 17.0 / -Table 3.
Mean difference BLEU scores for eachreference translation and WER between interme-diate and final references.4.4 Correlations of systems?
evaluationsCorrelations for BLEU scores were computedbetween 2 different-quality references.
This al-lowed us to obtain 2 correlation coefficients for 2non-validated references.
For the correlation onscores, all coefficients are over 99%, so that evenif scores increase or decrease, the distance be-tween systems does not change.
This is con-firmed by the correlation on ranks, since the co-efficients are between 96% and 100%.
Thus, bet-ter reference translations could hardly distinguishMT systems in an easier way during evaluation.5 Discussion and ConclusionsThis work has used the BLEU metric to scoreMT system output and has demonstrated that thequality of reference translations does not have aclear impact on WER, also using n-grams.
Evenwhen using lower-quality translations, scoresremain similar from one reference to another andimportant modifications of the human translationdo not affect strongly the scores of the MT sys-tems.
This behaviour concerns all the languagestested, and remains the same regardless of theinput or language used.
However, we should notforget that the context of this experiment con-cerns actual automatic metrics.
When referencetranslations have been modified, the impact onscores is not that clear, and even worse, this im-pact could be argued, to a certain extent, whenthe aim is to compare systems.
Indeed, we alsoobserved changes into scores when referenceswere modified.
Moreover, the quality of MT sys-tems should not be ignored: if the overall qualityof a system output is low, changes in referencetranslation will certainly have a lower impact ontheir scores.Over the modification of the scores, the vali-dation of the reference translation leads up to thevalidation criteria (although they are rigorouslydefined they are sometimes not very easy to ap-ply by the validation team), the consistenciesbetween agencies and translators (differencesbetween reference translations show how thehuman translation quality may vary according tothe translator) and some errors made by agencies(could be argued and validation can be difficultdepending on the context, input, etc).
Thosepoints have to be carefully checked during avalidation procedure and scores given by auto-matic metrics should be studied in agreementwith the variation of the quality and validation.AcknowledgementsThis work was supported by the TC-STAR pro-ject (grant number IST-506738).
We would liketo thank SPEX, especially Henk Van den Heuveland Eric Sanders, for the validation work.
Ourthanks also to Victoria Arranz for her help.ReferencesFers?e H., Van den Heuvel H., Olsen S. 2006.
Valida-tion of third party Spoken and Written LanguageResources Methods for performing Quick QualityChecks.
Proceedings of Workshop ?Quality assur-ance and quality measurement for language andspeech resources?, LREC 2006, Genoa, Italy.Mostefa D., Garcia M-N., Hamon O. and Moreau N.2006.
Evaluation report, Technology and Corporafor Speech to Speech Translation (TC-STAR) pro-ject.
Deliverable D16.Mostefa D., Hamon O., Moreau N. and Choukri K.2007.
Evaluation report, Technology and Corporafor Speech to Speech Translation (TC-STAR) pro-ject.
Deliverable D30.Papineni K, Roukos S, Ward T, and Wei-Jing Z.2001.
BLEU: A Method for Automatic Evaluationof Machine Translation.
IBM Research Division,Thomas J. Watson Research Center.Van den Heuvel H., Choukri K., H?ge H., MaegaardB., Odijk J., Mapelli V. 2003.
Quality Control ofLanguage Resources at ELRA.
Proceedings of Eu-rospeech, Geneva, Switzerland, pp.
1541-1544.42
