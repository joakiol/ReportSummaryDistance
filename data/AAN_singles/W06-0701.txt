Proceedings of the Workshop on Task-Focused Summarization and Question Answering, pages 1?7,Sydney, July 2006. c?2006 Association for Computational LinguisticsDimensionality Reduction Aids Term Co-Occurrence BasedMulti-Document SummarizationBen Hachey, Gabriel Murray & David ReitterSchool of InformaticsUniversity of Edinburgh2 Buccleuch Place, Edinburgh EH8 9LWbhachey@inf.ed.ac.uk, gabriel.murray@ed.ac.uk, dreitter@inf.ed.ac.ukAbstractA key task in an extraction system forquery-oriented multi-document summari-sation, necessary for computing relevanceand redundancy, is modelling text seman-tics.
In the Embra system, we use a repre-sentation derived from the singular valuedecomposition of a term co-occurrencematrix.
We present methods to show thereliability of performance improvements.We find that Embra performs better withdimensionality reduction.1 IntroductionWe present experiments on the task of query-oriented multi-document summarisation as ex-plored in the DUC 2005 and DUC 2006 sharedtasks, which aim to model real-world complexquestion-answering.
Input consists of a detailedquery1 and a set of 25 to 50 relevant docu-ments.
We implement an extractive approachwhere pieces of the original texts are selected toform a summary and then smoothing is performedto create a discursively coherent summary text.The key modelling task in the extraction phaseof such a system consists of estimating responsive-ness to the query and avoiding redundancy.
Bothof these are often approached through some tex-tual measure of semantic similarity.
In the Embra2system, we follow this approach in a sentence ex-traction framework.
However, we model the se-mantics of a sentence using a very large distri-butional semantics (i.e.
term co-occurrence) spacereduced by singular value decomposition.
Our hy-1On average, queries contain approximately 34 wordswords and three sentences.2Edinburgh Multi-document Breviloquence Assaypothesis is that this dimensionality reduction us-ing a large corpus can outperform a simple termco-occurrence model.A number of papers in the literature look at sin-gular value decomposition and compare it to unre-duced term ?
document or term co-occurrencematrix representations.
These explore varied tasksand obtain mixed results.
For example, Peder-sen et al (2005) find that SVD does not improveperformance in a name discrimination task whileMatveeva et al (2005) and Rohde et al (In prep)find that dimensionality reduction with SVD doeshelp on word similarity tasks.The experiments contained herein investigatethe contribution of singular value decompositionon the query-oriented multi-document summarisa-tion task.
We compare the singular value decom-position of a term co-occurrence matrix derivedfrom a corpus of approximately 100 million words(DS+SVD) to an unreduced version of the matrix(DS).
These representations are described in Sec-tion 2.
Next, Section 3 contains a discussion ofrelated work using SVD for summarisation and adescription of the sentence selection component inthe Embra system.
The paper goes on to give anoverview of the experimental design and results inSection 4.
This includes a detailed analysis of thestatistical significance of the results.2 Representing Sentence SemanticsThe following three subsections discuss variousways of representing sentence meaning for infor-mation extraction purposes.
While the first ap-proach relies solely on weighted term frequenciesin a vector space, the subsequent methods attemptto use term context information to better representthe meanings of sentences.12.1 Terms and Term Weighting (TF.IDF)The traditional model for measuring semantic sim-ilarity in information retrieval and text mining isbased on a vector representation of the distributionof terms in documents.
Within the vector spacemodel, each term is assigned a weight which sig-nifies the semantic importance of the term.
Often,tf.idf is used for this weight, which is a schemethat combines the importance of a term within thecurrent document3 and the distribution of the termacross the text collection.
The former is oftenrepresented by the term frequency and the latterby the inverse document frequency (idfi = Ndfi ),where N is the number of documents and dfi isthe number of documents containing term ti.2.2 Term Co-occurrence (DS)Another approach eschews the traditional vectorspace model in favour of the distributional seman-tics approach.
The DS model is based on the in-tuition that two words are semantically similar ifthey appear in a similar set of contexts.
We canobtain a representation of a document?s semanticsby averaging the context vectors of the documentterms.
(See Besanc?on et al (1999), where the DSmodel is contrasted with a term ?
document vec-tor space representation.
)2.3 Singular Value Decomposition(DS+SVD)Our third approach uses dimensionality reduction.Singular value decomposition is a technique fordimensionality reduction that has been used ex-tensively for the analysis of lexical semantics un-der the name of latent semantic analysis (Landaueret al, 1998).
Here, a rectangular (e.g., term ?document) matrix is decomposed into the productof three matrices (Xw?p = Ww?nSn?n(Pp?n)T )with n ?latent semantic?
dimensions.
W and Prepresent terms and documents in the new space.And S is a diagonal matrix of singular values indecreasing order.Taking the product Ww?kSk?k(Pp?k)T overthe first k columns gives the best least square ap-proximation of the original matrix X by a matrixof rank k, i.e.
a reduction of the original matrix tok dimensions.
Similarity between documents canthen be computed in the space obtained by takingthe rank k product of S and P .3The local importance of a term can also be computedover other textual units, e.g.
sentence in extractive summari-sation or the context of an entity pair in relation discovery.This decomposition abstracts away from termsand can be used to model a semantic similaritythat is more linguistic in nature.
Furthermore, ithas been successfully used to model human intu-itions about meaning.
For example, Landauer etal.
(1998) show that latent semantic analysis cor-relates well with human judgements of word sim-ilarity and Foltz (1998) shows that it is a good es-timator for textual coherence.It is hoped that these latter two techniques (di-mensionality reduction and the DS model) willprovide for a more robust representation of termcontexts and therefore better representation of sen-tence meaning, enabling us to achieve more reli-able sentence similarity measurements for extrac-tive summarisation.3 SVD in SummarisationThis section describes ways in which SVD hasbeen used for summarisation and details the im-plementation in the Embra system.3.1 Related WorkIn seminal work by Gong and Liu (2001), the au-thors proposed that the rows of P T may be re-garded as defining topics, with the columns rep-resenting sentences from the document.
In theirSVD method, summarisation proceeds by choos-ing, for each row in P T , the sentence with thehighest value.
This process continues until the de-sired summary length is reached.Steinberger and Jez?ek (2004) have offered twocriticisms of the Gong and Liu approach.
Firstly,the method described above ties the dimension-ality reduction to the desired summary length.Secondly, a sentence may score highly but never?win?
in any dimension, and thus will not be ex-tracted despite being a good candidate.
Their solu-tion is to assign each sentence an SVD-based scoreusing:ScSV Di =???
?n?i=1v(i, k)2 ?
?
(k)2 ,where v(i, k) is the kth element of the ith sen-tence vector and ?
(k) is the corresponding singu-lar value.Murray et al (2005a) address the same concernsbut retain the Gong and Liu framework.
Ratherthan extracting the best sentence for each topic,the n best sentences are extracted, with n deter-mined by the corresponding singular values from2matrix S. Thus, dimensionality reduction is nolonger tied to summary length and more than onesentence per topic can be chosen.A similar approach in DUC 2005 using termco-occurrence models and SVD was presented byJagarlamudi et al (2005).
Their system performsSVD over a term ?
sentence matrix and combinesa relevance measurement based on this representa-tion with relevance based on a term co-occurrencemodel by a weighted linear combination.3.2 Sentence Selection in EmbraThe Embra system developed for DUC 2005 at-tempts to derive more robust representations ofsentences by building a large semantic space us-ing SVD on a very large corpus.
While researchershave used such large semantic spaces to aid in au-tomatically judging the coherence of documents(Foltz et al, 1998; Barzilay and Lapata, 2005), toour knowledge this is a novel technique in sum-marisation.Using a concatenation of Aquaint and DUC2005 data (100+ million words), we utilised theInfomap tool4 to build a semantic model based onsingular value decomposition (SVD).
The decom-position and projection of the matrix to a lower-dimensionality space results in a semantic modelbased on underlying term relations.
In the currentexperiments, we set dimension of the reduced rep-resentation to 100.
This is a reduction of 90% fromthe full dimensionality of 1000 content-bearingterms in the original DS matrix.
This was foundto perform better than 25, 50, 250 and 500 dur-ing parameter optimisation.
A given sentence isrepresented as a vector which is the average of itsconstituent word vectors.
This sentence represen-tation is then fed into an MMR-style algorithm.MMR (Maximal Marginal Relevance) is a com-mon approach for determining relevance and re-dundancy in multi-document summarisation, inwhich candidate sentences are represented asweighted term-frequency vectors which can thusbe compared to query vectors to gauge similarityand already-extracted sentence vectors to gaugeredundancy, via the cosine of the vector pairs(Carbonell and Goldstein, 1998).
While this hasproved successful to a degree, the sentences arerepresented merely according to weighted termfrequency in the document, and so two similar sen-tences stand a chance of not being considered sim-4http://infomap.stanford.edu/for each sentence in document:for each word in sentence:get word vector from semantic modelaverage word vectors to form sentence vectorsim1 = cossim(sentence vector, query vector)sim2 = highest(cossim(sentence vector, all extracted vectors))score = ?
*sim1 - (1-?
)*sim2extract sentence with highest scorerepeat until desired lengthFigure 1: Sentence extraction algorithmilar if they do not share the same terms.Our implementation of MMR (Figure 1) uses ?annealing following (Murray et al, 2005a).
?
de-creases as the summary length increases, therebyemphasising relevance at the outset but increas-ingly prioritising redundancy removal as the pro-cess continues.4 ExperimentThe experimental setup uses the DUC 2005 data(Dang, 2005) and the Rouge evaluation met-ric to explore the hypothesis that query-orientedmulti-document summarisation using a term co-occurrence representation can be improved usingSVD.
We frame the research question as follows:Does SVD dimensionality reductionlead to an increase in Rouge score com-pared to the DS representation?4.1 MaterialsThe DUC 2005 task5 was motivated by Amigo etal.
?s (2004) suggestion of evaluations that modelreal-world complex question answering.
The goalis to synthesise a well-organised, fluent answer ofno more than 250 words to a complex questionfrom a set of 25 to 50 relevant documents.
Thedata includes a detailed query, a document set, andat least 4 human summaries for each of 50 topics.The preprocessing was largely based on LT TTTand LT XML tools (Grover et al, 2000; Thomp-son et al, 1997).
First, we perform tokenisationand sentence identification.
This is followed bylemmatisation.At the core of preprocessing is the LT TTTprogram fsgmatch, a general purpose transducerwhich processes an input stream and adds annota-tions using rules provided in a hand-written gram-mar file.
We also use the statistical combined part-of-speech (POS) tagger and sentence boundarydisambiguation module from LT TTT (Mikheev,5http://www-nlpir.nist.gov/projects/duc/duc2005/tasks.html31997).
Using these tools, we produce an XMLmarkup with sentence and word elements.
Furtherlinguistic markup is added using the morpha lem-matiser (Minnen et al, 2000) and the C&C namedentity tagger (Curran and Clark, 2003) trained onthe data from MUC-7.4.2 MethodsThe different system configurations (DS,DS+SVD, TF.IDF) were evaluated againstthe human upper bound and a baseline usingRouge-2 and Rouge-SU4.
Rouge estimates thecoverage of appropriate concepts (Lin and Hovy,2003) in a summary by comparing it severalhuman-created reference summaries.
Rouge-2does so by computing precision and recall basedon macro-averaged bigram overlap.
Rouge-SU4allows bigrams to be composed of non-contiguouswords, with as many as four words intervening.We use the same configuration as the official DUC2005 evaluation,6 which is based on word stems(rather than full forms) and uses jackknifing (k?1cross-evaluation) so that human gold-standard andautomatic system summaries can be compared.The independent variable in the experiment isthe model of sentence semantics used by the sen-tence selection algorithm.
We are primarily inter-ested in the relative performance of the DS andDS+SVD representations.
As well as this, weinclude the DUC 2005 baseline, which is a leadsummary created by taking the first 250 words ofthe most recent document for each topic.
We alsoinclude a tf.idf -weighted term ?
sentence repre-sentation (TF.IDF) for comparison with a conven-tional MMR approach.7 Finally, we include an up-per bound calculated using the DUC 2005 humanreference summaries.
Preprocessing and all otheraspects of the sentence selection algorithm remainconstant over all systems.In general, Rouge shows a large variance acrossdata sets (and so does system performance).
It isimportant to test whether obtained nominal differ-ences are due to chance or are actually statisticallysignificant.To test whether the Rouge metric showed a re-liably different performance for the systems, the6i.e.
ROUGE-1.5.5.pl -n 2 -x -m -2 4 -u-c 95 -r 1000 -f A -p 0.5 -t 0 d7Specifically, we use tfi,j ?
log( Ndfi ) for term weightingwhere tfi,j is the number of times term i occurs in sentencej, N is the number of sentences, and dfi is the number ofsentences containing term i.p Metric hypothesis0.000262 Rouge-2 base<TF.IDF ***0.021640 Rouge-2 base<DS *0.000508 Rouge-2 base<DS+SVD ***0.014845 Rouge-2 DS<TF.IDF *0.507702 Rouge-2 TF.IDF<DS+SVD0.047016 Rouge-2 DS<DS+SVD *0.000080 Rouge-SU4 base<TF.IDF ***0.006803 Rouge-SU4 base<DS **0.000006 Rouge-SU4 base<DS+SVD ***0.012815 Rouge-SU4 DS<TF.IDF *0.320083 Rouge-SU4 TF.IDF<DS+SVD0.001053 Rouge-SU4 DS<DS+SVD **Table 1: Holm-corrected Wilcoxon hypothesis testresults.Friedman rank sum test (Friedman, 1940; Dems?ar,2006) can be used.
This is a hypothesis test notunlike an ANOVA, however, it is non-parametric,i.e.
it does not assume a normal distribution ofthe measures (i.e.
precision, recall and F-score).More importantly, it does not require homogene-ity of variances.To (partially) rank the systems against eachother, we used a cascade of Wilcoxon signed rankstests.
These tests are again non-parametric (as theyrank the differences between the system results forthe datasets).
As discussed by Dems?ar (2006), weused Holm?s procedure for multiple tests to correctour error estimates (p).4.3 ResultsFriedman tests for each Rouge metric (withF-score, precision and recall included as ob-servations, with the dataset as group) showeda reliable effect of the system configuration(?2F,SU4 = 106.6, ?2P,SU4 = 96.1,?2R,SU4 = 105.5, all p < 0.00001).Post-hoc analysis (Wilcoxon) showed (see Ta-ble 1) that all three systems performed reliablybetter than the baseline.
TF.IDF performed bet-ter than simple DS in Rouge-2 and Rouge-SU4.DS+SVD performed better than DS (p2 < 0.05,pSU4 < 0.005).
There is no evidence to supporta claim that DS+SVD performed differently fromTF.IDF.However, when we specifically compared theperformance of TF.IDF and DS+SVD with theRouge-SU4 F score for only the specific (asopposed to general) summaries, we found thatDS+SVD scored reliably, but only slightly better4baselineTF.IDF MMRDS MMRDS+SVD MMRHumanMean Rouge F?Scores0.00 0.05 0.10 0.15Figure 2: Mean system performance over 50datasets (F-scores).
Precision and Recall lookqualitatively similar.
(Wilcoxon, p<0.05).
This result is unadjusted,and post-hoc comparisons with other scores or forthe general summaries did not show reliable dif-ferences.Having established the reliable performance im-provement of DS+SVD over DS, it it importantto take the effect size into consideration (withenough data, small effects may be statistically sig-nificant, but practically unimportant).
Figure 2 il-lustrates that the gain in mean performance is sub-stantial.
If the mean Rouge-SU4 score for humanperformance is seen as upper bound, the DS+SVDsystem showed a 25.4 percent reduction in errorcompared to the DS system.8A similar analysis for precision and recall givesqualitatively comparable results.5 Discussion and Future WorkThe positive message from the experimental re-sults is that SVD dimensionality reduction im-proves performance over a term co-occurrencemodel for computing relevance and redundancy ina MMR framework.
We note that we cannot con-clude that the DS or DS+SVD systems outper-form a conventional tf.idf -weighted term ?
sen-tence representation on this task.
However, resultsfrom Jagarlamudi et al (2005) suggest that the DSand term ?
sentence representations may be com-plementary in which case we would expect a fur-ther improvement through an ensemble technique.Previous results comparing SVD with unre-duced representations show mixed results.
Forexample, Pedersen et al (2005) experiment withterm co-occurrence representations with and with-out SVD on a name discrimination task and find8Pairwise effect size estimates over datasets aren?t sensi-ble.
Averaging of differences between pairs was affected byoutliers, presumably caused by Rouge?s error distribution.that the unreduced representation tends to performbetter.
Rohde et al (In prep), on the other hand,find that a reduced matrix does perform better onword pair similarity and multiple-choice vocabu-lary tests.
One crucial factor here may be the sizeof the corpus.
SVD may not offer any reliable ?la-tent semantic?
advantage when the corpus is small,in which case the efficiency gain from dimension-ality reduction is less of a motivation anyway.We plan to address the question of corpus sizein future work by comparing DS and DS+SVDderived from corpora of varying size.
We hypoth-esise that the larger the corpus used to compilethe term co-occurrence information, the larger thepotential contribution from dimensionality reduc-tion.
This will be explored by running the experi-ment described in this paper a number of times us-ing corpora of different sizes (e.g.
0.5m, 1m, 10mand 100m words).Unlike official DUC evaluations, which rely onhuman judgements of readability and informative-ness, our experiments rely solely on Rouge n-gram evaluation metrics.
It has been shown inDUC 2005 and in work by Murray et al (2005b;2006) that Rouge does not always correlate wellwith human evaluations, though there is more sta-bility when examining the correlations of macro-averaged scores.
Rouge suffers from a lack ofpower to discriminate between systems whose per-formance is judged to differ by human annotators.Thus, it is likely that future human evaluationswould be more informative.
Another way that theevaluation issue might be addressed is by using anannotated sentence extraction corpus.
This couldproceed by comparing gold standard alignmentsbetween abstract and full document sentences withpredicted alignments using correlation analysis.6 ConclusionsWe have presented experiments with query-oriented multi-document summarisation.
The ex-periments explore the question of whether SVDdimensionality reduction offers any improvementover a term co-occurrence representation for sen-tence semantics for measuring relevance and re-dundancy.
While the experiments show thatour system does not outperform a term ?
sen-tence tf.idf system, we have shown that the SVDreduced representation of a term co-occurrencespace built from a large corpora performs betterthan the unreduced representation.
This contra-5dicts related work where SVD did not providean improvement over unreduced representationson the name discrimination task (Pedersen et al,2005).
However, it is compatible with other workwhere SVD has been shown to help on the taskof estimating human notions of word similarity(Matveeva et al, 2005; Rohde et al, In prep).A detailed analysis using the Friedman test anda cascade of Wilcoxon signed ranks tests suggestthat our results are statistically valid despite theunreliability of the Rouge evaluation metric due toits low variance across systems.AcknowledgementsThis work was supported in part by Scottish Enter-prise Edinburgh-Stanford Link grant R36410 and,as part of the EASIE project, grant R37588.
Itwas also supported in part by the European Union6th FWP IST Integrated Project AMI (AugmentedMultiparty Interaction, FP6-506811, publication).We would like to thank James Clarke for de-tailed comments and discussion.
We would alsolike to thank the anonymous reviewers for theircomments.ReferencesEnrique Amigo, Julio Gonzalo, Victor Peinado,Anselmo Penas, and Felisa Verdejo.
2004.
Anempirical study of information synthesis tasks.
InProceedings of the 42nd Annual Meeting of the As-sociation for Computational Linguistics, Barcelona,Spain.Regina Barzilay and Mirella Lapata.
2005.
Modelinglocal coherence: an entity-based approach.
In Pro-ceedings of the 43rd Annual Meeting of the Associa-tion for Computational Linguistics, Ann Arbor, MI,USA.Romaric Besanc?on, Martin Rajman, and Jean-Ce?dricChappelier.
1999.
Textual similarities based ona distributional approach.
In Proceedings of the10th International Workshop on Database And Ex-pert Systems Applications, Firenze, Italy.Jaime G. Carbonell and Jade Goldstein.
1998.
Theuse of mmr, diversity-based reranking for reorderingdocuments and producing summaries.
In Proceed-ings of the 21st Annual International ACM SIGIRConference on Research and Development in Infor-mation Retrieval, Melbourne, Australia.James R. Curran and Stephen Clark.
2003.
Languageindependent NER using a maximum entropy tag-ger.
In Proceedings of the 2003 Conference on Com-putational Natural Language Learning, Edmonton,Canada.Hoa T. Dang.
2005.
Overview of DUC 2005.
InProceedings of the Document Understanding Con-ference, Vancouver, B.C., Canada.Janez Dems?ar.
2006.
Statistical comparisons of clas-sifiers over multiple data sets.
Journal of MachineLearning Research, 7:1?30, Jan.Peter W. Foltz, Walter Kintsch, and Thomas K. Lan-dauer.
1998.
The measurement of textual coherencewith latent semantic analysis.
Discourse Processes,25.Milton Friedman.
1940.
A comparison of alternativetests of significance for the problem of m rankings.The Annals of Mathematical Statistics, 11:86?92.Yihon Gong and Xin Liu.
2001.
Generic text summa-rization using relevance measure and latent semanticanalysis.
In Proceedings of the 24th Annual Interna-tional ACM SIGIR Conference on Research and De-velopment in Information Retrieval, New Orleans,LA, USA.Claire Grover, Colin Matheson, Andrei Mikheev, andMarc Moens.
2000.
LT TTT?a flexible tokeni-sation tool.
In Proceedings of the 2nd InternationalConference on Language Resources and Evaluation,Athens, Greece.Ben Hachey and Claire Grover.
2004.
A rhetoricalstatus classifier for legal text summarisation.
InProceedings of the ACL-2004 Text SummarizationBranches Out Workshop, Barcelona, Spain.Jagadeesh Jagarlamudi, Prasad Pingali, and VasudevaVarma.
2005.
A relevance-based language mod-eling approach to DUC 2005.
In Proceedings ofthe Document Understanding Conference, Vancou-ver, B.C., Canada.Thomas K. Landauer, Peter W. Foltz, and Darrell La-ham.
1998.
Introduction to latent semantic analysis.Discourse Processes, 25.Chin-Yew Lin and Eduard H. Hovy.
2003.
Au-tomatic evaluation of summaries using n-gramco-occurrence statistics.
In Proceedings of theJoint Human Language Technology Conference andNorth American Chapter of the Association forComputational Linguistics Annual Meeting, Edmon-ton, Alberta, Canada.Irina Matveeva, Gina-Anne Levow, Ayman Farahat,and Christiaan Royer.
2005.
Term represetationwith generalized latent semantic analysis.
In Pro-ceedings of the 2005 Conference on Recent Ad-vances in Natural Language Processing, Borovets,Bulgaria.Andrei Mikheev.
1997.
Automatic rule induction forunknown word guessing.
Computational Linguis-tics, 23(3).6Guido Minnen, John Carroll, and Darren Pearce.
2000.Robust, applied morphological generation.
In Pro-ceedings of the 1st International Natural LanguageGeneration Conference, Mitzpe Ramon, Israel.Gabriel Murray, Steve Renals, and Jean Carletta.2005a.
Extractive summarization of meeting record-ings.
In Proceedings of the 9th European Con-ference on Speech Communication and Technology,Lisbon, Portugal.Gabriel Murray, Steve Renals, Jean Carletta, and Jo-hanna Moore.
2005b.
Evaluating automatic sum-maries of meeting recordings.
In Proceedings of the43rd Annual Meeting of the Association for Compu-tational Linguistics, Ann Arbor, MI, USA.Gabriel Murray, Steve Renals, Jean Carletta, and Jo-hanna Moore.
2006.
Incorporating speaker anddiscourse features into speech summarization.
InProceedings of the Joint Human Language Technol-ogy Conference and North American Chapter of theAssociation for Computational Linguistics AnnualMeeting, New York City, NY, USA.Ted Pedersen, Amruta Purandare, and Anagha Kulka-rni.
2005.
Name discrimination by clustering simi-lar contexts.
In Proceedings of the 6th InternationalConference on Intelligent Text Processing and Com-putational Linguistics, Mexico City, Mexico.Douglas L. T. Rohde, Laur M. Gonnerman, andDavid C. Plaut.
In prep.
An improvedmethod for deriving word meaning from lexicalco-occurrence.
http://dlt4.mit.edu/?dr/COALS/Coals.pdf (1 May 2006).Josef Steinberger and Karel Jez?ek.
2004.
Using latentsemantic analysis in text summarization and sum-mary evaluation.
In Proceedings of the 5th Inter-national Conference on Information Systems Imple-mentation and Modelling, Ostrava, Czech Republic.Henry Thompson, Richard Tobin, David McK-elvie, and Chris Brew.
1997.
LT XML:Software API and toolkit for XML processing.http://www.ltg.ed.ac.uk/software/.7
