Finite-State Approximations of GrammarsFernando PereiraAT&T Bell Laboratories600 Mountain Ave., Murray Hill, NJ 07974Motivat ionGrammars for spoken language systems are subject tothe conflicting requirements of language modeling forrecognition and of language analysis for sentence inter-pretation.
Current recognition algorithms can most di-rectly use finite-state acceptor (FSA) language models.However, these models are inadequate for language inter-pretation, since they cannot express the relevant syntac-tic and semantic regularities.
Augmented phrase struc-ture grammar (APSG) formalisms, such as unificationgrammars, can express many of those regularities, butthey are computationally less suitable for language mod-eling, because of the inherent cost of computing statetransitions in APSG parsers.The above problems might be circumvented by us-ing separate grammars for language modeling and lan-guage interpretation.
Ideally, the recognition grammarshould not reject sentences acceptable by the interpreta-tion grammar, but it should contain as much as possibleof the constraints built into the interpretation grammar.However, if the two grammars are built independently,those constraints are difficult to maintain.
For this rea-son, we have developed a method for constructing au-tomatically a finite-state approximation for an 'APSG.Since the purpose of the approximation is to serve as afilter in a speech-recognition front-end to the real parser,the approximation language is a superset of the languageaccepted by the APSG.
The term "approximation" willalways be used in this sense in what follows.If no further constraints were placed on the close-ness of the approximation, the trivial algorithm thatassigns to any APSG over alphabet ~ the regular lan-guage ~* would do.
Clearly, this is not what is required.One possible criterion for "goodness" of approximationarises from the observation that many interesting phrase-structure grammars have substantial parts that acceptregular languages.
That does not mean that the gram-mar rules are in the standard forms for defining :regularlanguages (left-linear or right-linear), because syntacticand semantic onsiderations often require that strings ina regular set be assigned structural descriptions not de-finable by regular productions.
A useful criterion is thusthat if a grammar generates a regular language, the ap-proximation algorithm yields an acceptor for that :regularlanguage.
In other words, one would like the algorithmto be exact for APSGs yielding regular languages.We have not yet proved that our method satisfies theabove exactness criterion, but some experiments haveshown that the method is exact for a variety of interest-ing grammars.The Algor i thmOur approximation method applies to any context-freegrammar (CFG), or any unification grammar that canbe fully expanded into a context-free grammar)  The re-sulting FSA accepts a superset of the sentences acceptedby the input grammar.The current implementation accepts as input a formof unification grammar in which features can take onlyatomic values drawn from a specified finite set.
It isclear that such grammars can only generate context-freelanguages, since an equivalent CFG can be obtained byinstantiating features in rules in all possible ways.The heart of our approximation method is an al-gorithm to convert the LR(0) characteristic machineAA(G) (Aho and Ullman, 1977; Uackhouse, 1979) of aCFG G into an FSA for a superset of the language L(G)defined by G. The characteristic machine for a CFG Gis an FSA for the viable prefixes of G, which are just thepossible stacks built by the standard shift-reduce recog-nizer for G when recognizing strings in L(G).This is not the place to review the characteristic ma-chine construction in detail.
However, to explain theapproximation algorithm we will need to recall the mainaspects of the construction.
The states of .hA(G) are setsof dotted rules A ---+ or.
fl where A ---+ aft is some rule ofG..A4(G) is the determinization by the standard subsetconstruction (Aho and Ullman, 1977) of the FSA definedas follows:?
The initial state is the dotted rule S' ~ S. where Sis the start symbol of G and S' is a new auxiliarystart symbol.?
The final state is S' ---, S..1 Unification grammars  not in this class must  first be weakenedusing techniques such as Shieber's restrictor (Shieber, 1985).201S'->.S ~, ,  S'->S.S ->.
Ab 3A->.Aa (4  S'>Ab" A ->.
A~2S -> A. bA -> A.  aa A->Aa.Figure 1: Characteristic Machine for G1?
The other states are all the possible dotted rules ofG.
* There is a transition labeled X, where X is aterminal or nonterminal symbol, from dotted ruleA ~ a .X /3  to A ~ aX ./3.?
There is an e-transition from A -+ a.  B/3 to B ~ "7,where B is a nonterminal symbol and B ~ "7 a rulein G..M(G) can be seen as a finite state control for a non-deterministic shift-reduce pushdown recognizer for G. Astate transition labeled by a terminal symbol z fromstate s to state s' licenses a shift move, pushing ontothe stack of the recognizer the pair (s, Ix).
Arrival at astate containing a completed otted rule A ~ a. licensesa reduction move.
This pops from the stack as manypairs as the symbols in a, checking that the symbols inthe pairs match the corresponding elements of a, andthen takes the transition out of the last state popped slabeled by A, pushing (s, A) onto the stack.The basic ingredient of our approximation algorithmis the flattening of a shift-reduce recognizer for a gram-mar G into an FSA by eliminating the stack and turningreduce moves into e-transitions.
However, as we will seebelow, flattening the characteristic machine recognizerdirectly will lead to poor approximations in many inter-esting cases.
Instead, the characteristic machine must beunfolded into a larger machine whose states carry infor-mation about the possible shift-reduce stacks at statesof the characteristic machine.
The quality of the ap-proximation is crucially influenced by how much stackinformation is encoded in the states of the unfolded ma-chine: too little leads to coarse approximations, whiletoo much leads to redundant automata needing very ex-pensive optimization.The algorithm is best understood with a simple exam-ple.
Consider the left-linear grammar G1S-- - tAbA---+ Aa le.M(G1) is shown on Figure 1.
Unfolding is not requiredfor this simple example, so the approximating FSA is ob-tained from .A//(G1) by the flattening method outlinedabove.
The reducing states in .h4(G1), those contain-ing completed otted rules, are states 0, 3 and 4.
Forinstance, the reduction at state 4 would lead to a tran-sition on nonterminal A, to state 2, from the state thatFigure 2: Flattened FSAactivated the rule being reduced.
Thus the correspond-ing e-transition goes from state 4 to state 2.
Adding allthe transitions that arise in this way we obtain the FSAin Figure 2.
From this point on, the arcs labeled withnonterminals can be deleted.
Doing that and simplify-ing, we get finally the FSA in Figure 3. which is theminimal FSA for the input left-linear grammar.If flattening were applied to the LR(0) characteristicmachine as in the example above, even simple grammarsdefining regular languages might be inexactly approxi-mated by the algorithm.
The reason for this is that ingeneral the reduction at a given reducing state in thecharacteristic machine transfers to different states de-pending on stack contents.
In other words, the reducingstate might be reached by different routes which use theresult of the reduction in different ways.
Consider forexample the grammar G2S --~ aXa  \[ bXbX ---~ cwhich accepts the two strings aca and bcb.
Flattening.A4(G2) will produce an FSA that will also accept acband bca, clearly an undesirable outcome.
The reason forthis is that the e-transitions leaving the reducing statecontaining X ~ c. do not distinguish between the dif-ferent ways of reaching that state, which are encoded inthe stack of the characteristic recognizer.One way of solving the above problem is to unfold eachstate of the characteristic machine into a set of states cor-responding to different stacks at that state, and flatten-ing the unfolded acceptor ather than the original one.However, the set of possible stacks at a state is in generalinfinite.
Therefore, it is necessary to do the unfolding notwith respect o stacks, but with respect o a finite par-?ab-?Figure 3: Minimal Acceptor21tition of the set of stacks possible at the state, inducedby an appropriate quivalence relation.
The relation weuse currently makes two stacks equivalent if they canbe made identical by collapsing loops, that is, remov-ing portions of stack pushed between two arrivals at thesame state in the finite-state control of the shift-reducerecognizer.
The purpose of collapsing loops is to "for-get" stack segments that may be arbitrarily repeated3Clearly, each equivalence class is uniquely defined by theshortest stack in the class, and the classes can be con-structed without having to consider all the (infinitely)many possible stacks.Soundness  o f  the  A lgor i thmWe will show here that the approximation method de-scribed informally in the previous ection is sound, in thesense that the approximating FSA will always accept asuperset of the language accepted by the input CFG.In what follows, G is a fixed CFG with terminal vo-cabulary ~, nonterminal vocabulary N and start symbolS .
.M  is the characteristic machine for G, with state setQ, start state so, final states F, and transition function6 : S x (E U N) --~ S. As usual, transition functions uchas 6 are extended from input symbols to input stringsby defining 6(s, e) = s and 6(s, aft) = 6(6(s, a), fl).The shift-reduce recognizer T?
associated to .M has thesame states, start state and final states.
Its configura-tions are triples i s, 05 w) of a state, a stack and an inputstring.
The stack is a sequence of pairs (s, X) of a stateand a terminal or nonterminal symbol.
The transitionsof the shift-reduce recognizer are given as follows:Shift :  i s, o', xw) ~- (s', o'is, x), w) if 6(s, x) = s'Reduce:  (s, ar, w) ~- (s', o'is" ,A), w} if 6(s", A) = s'and either (1) A + ?
is a completed ottedrule in s, s" = s and r is empty, or (2) AX1 .
.
.Xn-  is a completed otted rule in s,r = (sx ,X i ) - - .
( s .
,X .}
and s" = sl.The initial configurations of ~ are is0, e, w) for someinput string w, and the final configurations are(s, (s0,S},e) for some state s E F.  A derivation of astring w is a sequence of configurations co, .
.
.
,  cm suchthat co = (so, e, w}, cm = i s, (So, S}, e} for some finalstate p, and ci-1 ~- ci for 1 < i < n.Let s be a state.
The set Stacks(s) containseach sequence ( so ,Xo} .
.
.
i sk ,Xk}  such that  si =6(s~_i,X~_~), 1 < i < k and s = 6(sk,Xk).
In addition,Stacks(s0) contains the empty sequence .
By construc-tion, it is clear that if i s, o', w} is reachable from an initialconfiguration in 7~, then (r E Stacks(s).A stack congruence on T?
is a family of equivalencerelations _---~ on Stacks(s) for each state s E S such thatif o" =--, tr' and 6(s, X )  = s' then ~(s, X )  _---,, ais, X}.
A2Since possible stacks can be easily shown to form a regularlanguage, loop collapsing has a direct connection to the pumpinglemma for regular languages.stack congruence ~ partitions each Stacks(s) into equiv-alence classes \[a\], of the stacks in Stacks(s) equivalentto a under ~, .Each stack congruence ~ on T~ induces a correspond-ing unfolded recognizer ~_ .
The states of the unfoldedrecognizer are pairs of a state and stack equivalence classat that state.
The initial state is (so, \[e\]so), and the fi-nal states are all (s, \[a\]s) with s E F.  The transitionfunction 5__- of the unfolded recognizer is defined byM,), x) = (6(s, x), X)l,(,,x))That this is well-defined follows immediately from thedefinition of stack congruence.The definitions of dotted rules in states, configura-tions, shift and reduce transitions given above carry overimmediately to unfolded recognizers.
Also, the charac-teristic recognizer can also be seen as an unfolded recog-nizer for the trivial coarsest congruence.For any unfolded state p, let Pop(p) be the set of statesreachable from p by a reduce transition.
More precisely,Pop(p) contains any state p' such that there is a com-pleted dotted rule A --~ c~.
in p and a state p" such that6___(p",c~) = p and 6-(p" ,A)  = p'.
Then the flatteningU-  of 7~_- is a nodeterministie FSA with the same stateset, start state and final states as 7~- and nondetermin-isti?
transition function ?
-  defined as follows:?
If 6~_(p,x) = p' for some x E E, then p' E ?_=(p,x)?
If p' E Pop(p) then p' E ?= (p, e).Let co, .
.
.
,  cm be a derivation of string w in TO, andput ci = (qi, o'i, wi}, and Pi = (qi, \[o'i\]p,}.
By construc-tion, if ci-1 F- ci is a shift move on x (wi-1 = zwi),then 6_--(pi-1, x) = Pi, and thus P ie  ?_=(Pi-1, x).
Alter-natively, assume the transition is a reduce move associ-ated to the completed otted rule A ~ a.. We considerfirst the case a # e. Put a = X1 .
.
.X , .
By definitionof reduce move, there is a sequence of states ra , .
.
.
,  rnand a stack a such that o'i-1 = a( r l ,X1) .
.
.
( rn ,X ,} ,ai = o' i r l ,A ), 6 i r l ,A)  = qi, and 6(r j ,X j )  = rj+l for1 < j < n. By definition of stack congruence, we willthen havewhere n = and = {r,, X -i) for j > 1.Furthermore, again by definition of stack congruence wehave 6_=((rl, \[o'\]r,),A) = Pi.
Therefore, Pi E Pop(pi-1)and thus Pi E ?_=(Pi-1, e).
A similar but simpler argu-ment allows us to reach the same conclusion for the casea = e. Finally, the definition of final state for TO__- and.T~ makes Pm a final state.We have thus shown how to construct from a deriva-tion of a string in T?
an accepting path for the samestring in .T=-.
This proves that every string in L(G) isaccepted by .T~, that is, that our construction is sound.Finally, we should show that the stack collaps-ing equivalence informally described earlier is indeeda stack congruence.
A stack r is a loop if r =22Symbol Category Featuressnpvpargsdetnpronvsentencenoun phraseverb phraseverb argumentsdeterminernounpronounverbn (number), p (person)n, p, c (case)n, p, t (verb type)tnnn, p, cn, p, tTable 1: Categories of Example Grammar(s l ,X1) .
.
.
( sk ,Xk)  and 6(sk,Xk) = sx.
A stack a col-lapses to a stack a' if o" = pry, ~' = pv and r is a loop.Two stacks are equivalent if they can be collapsed to thesame stack.
Clearly, this equivalence relation is closedunder suffixing, therefore it is a stack congruence.A Complete ExampleThe example grammar in the appendix shows an APSGfor a small fragment of English, written in the notationaccepted by the curent version of our grammar compiler.The categories and features used in the grammar aredescribed in Tables 1 and 2 (categories without featuresare omitted).
The example grammar accepts entencessuch asI give a cake to TomTom sleepsI eat every nice cakebut rejects ill-formed inputs such asI sleepsI eats a cakeI giveTom eatThe current grammar compiler factors out terminalproductions to make the approximation algorithm inde-pendent of vocabulary size; transitions are labeled byautomatically generated preterminal symbols instead ofterminal symbols.
After this factoring, the full instan-tiation of the example grammar has 181 rules, its char-acteristic machine 222 states and 922 transitions, theFeature Valuesn (number)p (person)?
(case)t (verb type)s (singular), p (plural)1 (first), 2 (second), 3 (third)s (subject), o (nonsubject)i (intransitive), t (transitive), d(ditransitive)Table 2: Features of Example Grammarunfolded and flattened FSA 3417 states and 4255 tran-sitions, and the determinized and minimized final DFA18 states and 67 transitions.
The compilation time is123.19 seconds on a Sun SparcStation 1, with the deter-minizer and minimizer written in C and the rest of thecompiler in Quintus Prolog.
Most of the time is spent inthe unfolding and flattening phases (90.62 seconds un-folding and 17.33 flattening).
It is hoped that recodingthese  phases in C using carefully tuned data structureswill speed them up by between one and two orders ofmagnitude.Substantially larger grammars, with thousands of in-stantiated rules, have been developed for a speech-to-speech translation project.
Compilation times rangefrom the very reasonable (around 10 minutes) to the veryhigh (10 hours).
Very long compilations are caused by acombinatorial explosion in the unfolding of right recur-sions that will be discussed further in the next section.Informal AnalysisThe present algorithm has not yet been analyzed suffi-ciently to determine the class of context-free grammarsgenerating regular languages for which it is exact.
IIow-ever, it is exact for in a variety of interesting cases, in-cluding the examples of Church and Patil (Church andPatil, 1982), which illustrate how typical attachment am-biguities arise as structural ambiguities on regular stringsets.For example, the left-linear grammarS- - tAbA - - ,  Aa leand the right-linear grammarS---~aS \[ bboth of which generate the regular set a'b, are mappedby the algorithm into the FSA in Figure 3.The algorithm is also exact for some self-embeddinggrammars s of regular languages, such asS---+ aS \[ Sb l cdefining the regular language a*cb*.A more interesting example is the following simplifiedgrammar for the structure of English noun phrases:NP ~ Det Nom I PNDet ~ Art \] NP 'sNom ~ N I Nom PP I Adj NomPP ~ P NPThe symbols Art, N, PN and P correspond to the parts ofspeech article, noun, proper noun and preposition.
Fromthis grammar, the algorithm derives the FSA in Figure4.3A grammar isself-embeddlng if and only if licenses the deriva-tion X ~ c~X~ for nonempty a and/3.
A language isregular ifandonly if it can be described by some non-self-embedding grammar.23Figure 4: Acceptor for Noun PhrasesAs an example of inexact approximation, consider thethe self-embedding CFGS --~ aSb \] efor the nonregular language anb n, n _> O.
This grammaris mapped by the algorithm into an FSA accepting e Ia+b +.
The effect of the algorithm is thus to "forget" thepairing between a's and b's mediated by the stack in apushdown acceptor for the CFG.As noted earlier, right recursion is rather bad for thepresent unfolding scheme.
It is easy to see that the num-ber of unfolded states for a grammar of the formS--~ X1S \] ...
I XnS I Yis exponential in n. However, the problem can be cir-cumvented by left factoring the grammar as follows:S~ZSiYz xl I...Ix.This kind of situation often arises indirectly in the expan-sion of an APSG when some features in the right-handside of a rule are unconstrained and thus lead to manydifferent instantiated rules.Related Work and ConclusionsOur work can be seen as an algorithmic realization ofsuggestions of Church and Patil (Church, 1980; Churchand Patil, 1982) on algebraic simplifications of CFGs ofregular languages.
Other work on finite state approxi-mations of phrase structure grammars has typically re-lied on arbitrary depth cutoffs in rule application.
Whilethis is reasonable for psycholinguistic modeling of perfor-mance restrictions on center embedding (Pulman, 1986),it does not seem appropriate for speeech recognitionwhere the approximating FSA is intended to work asa filter and not reject inputs acceptable by the givengrammar.
For instance, depth cutoffs in the methoddescribed by Black (1989) lead to approximating FSAswhose language is neither a subset nor a superset of thelanguage of the given phrase-structure grammar, In con-trast, our method will produce an exact FSA for manyinteresting grammars generating regular languages, uchas those arising from systematic attachment ambiguities(Church and Patil, 1982).
It important to note, however,that even when the result FSA accepts the same lan-guage, the original grammar isstill necessary because in-terpretation algorithms are generally expressed in termsof phrase structures described by that grammar, not interms of the states of the FSA.The current algorithm can be combinatorially explo-sive in two places: the instantiation ofunification gram-mar rules to derive an equivalent CFG, and the unfold-ing of the characteristic machine, in particular for right-recursive rules.
The former problem can be alleviated byavoiding full instantiation of unification grammar ruleswith respect o "don't care" features, that is, featuresthat are not constrained by the rule.
This can alsohelp decrease the right-recursion u folding explosion dis-cussed earlier.
As for the cost of unfolding, preliminaryexperiments suggest that dividing the grammar into non-mutually-recursive components and applying the LR(0)construction and unfolding separately to those compo-nents could lead to much smaller unfolded automata.AcknowledgmentsThanks are due to Mark Liberman for suggesting thatfinite-state approximations might be worth investigatingto David Roe and Pedro Moreno for using the gram-mar compiler prototype and patiently putting up withits bugs and inefficiencies.ReferencesAlfred V. Aho and Jeffrey D. Ullman.
1977.
Principlesof Compiler Design.
Addison-Wesley, Reading, Mas-sachusetts.Roland C. Backhouse.
1979.
Syntax of ProgrammingLanguages--Theory and Practice.
Series in ComputerScience.
Prentice-Hall, Englewood Cliffs, New Jersey.Alan W. Black.
1989.
Finite state machines from fea-ture grammars.
In Masaru Tomita, editor, Interna-tional Workshop on Parsing Technologies, pages 277-285, Pittsburgh, Pennsylvania.
Carnegie Mellon Uni-versity.Kenneth W. Church and Ramesh Patil.
1982.
Copingwith syntactic ambiguity or how to put the block inthe box on the table.
Computational Linguistics, 8(3-4):139-149.Kenneth W. Church.
1980.
On memory limitations innatural anguage processing.
Master's thesis, M.I.T.Published as Report MIT/LCS/TR-245.Steven G. Pulman.
1986.
Grammars, parsers, and mem-ory limitations.
Language and Cognitive Processes,1(3):197-225.Stuart M. Shieber.
1985.
Using restriction to ex-tend parsing algorithms for complex-feature-based for-malisms.
In 23rd Annual Meeting of the Association24for Computational Linguistics, pages 145-152, Mor-ristown, New Jersey.
Association for ComputationalLinguistics.AppendixNonterminal symbols (syntactic categories) may havefeatures that specify variants of the category (eg.
sin-gular or plural noun phrases, intransitive or transitiveverbs).
A category cat with feature constraints is writ-tenCarE\[el,... , Crn\] .Feature constraints for feature f have the formf=vfor a single value v orf = ( ,1 , .
.
.
,  v,)for several alternative values.
The symbol ")" appear-ing as the value of a feature in the right-hand side ofa rule indicates that that feature must have the samevalue as the feature of the same name of the category inthe left-hand side of the rule.
This can be used to en-force feature agreement, for instance, number agreementbetween subject and verb.It is convenient to declare the features and possiblevalues of categories with category declarations appearingbefore the grammar ules.
Category declarations havethe formcat  cat#\ [  f l   (V l l , .
.
.
,V lk l ) , .
.
.
,/m=(Vml .
.
.
.
,Vmk ) 1.giving all the possible values of all the features for thecategory.The declarations tar t  cat.declares cat as the start symbol of the grammar.In the grammar rules, the symbol " ' "  prefixes terminalsymbols, commas are used for sequencing and "l" foralternation.start s.cat s#\[n=(s,p),p=(l,2,3)\].cat np#\[n=(s,p) ,p=(1,2,3) ,c=(s,o)\] .cat vp#\[n=(s,p) ,p=(1,2,3) ,type=(i,t,d)\].cat args# \[type=(i,t,d)\] .cat det#\ [n=(s ,p ) \ ] .cat  n#\ [n=(s ,p ) \ ] .cat  p ron#\ [n=(s ,p ) ,p=(1 ,2 ,3 ) ,c=(s ,o ) \ ] .cat  v#\ [n=(s ,p ) ,p=(1 ,2 ,3 ) , type=( i , t ,d ) \ ] .s => np#\ [n=!
,p=!
,c=s \ ] ,  vp#\ [n=!
,p=!
\ ] .np#\[p=3\] => det#\[n=!\], adjs, n#\[n=!\].np#\[n=s,p=3\] => pn.np => pron#\[n=!, p=!, c=!\].pron#\[n=s,p=l,c=s\] => 'i.pron#\[p=2\] => 'you.pron#\[n=s,p=3,c=s\] => 'he I 'she.pron#\[n=s,p=3\] => 'it.pron#\[n=p,p=l,c=s\] => ',e.pron#\[n=p,p=3,c=s\] => 'they.pron#\[n=s,p=l,c=o\] => 'me.pron#\[n=s,p=3,c=o\] => 'him I 'her.pron#\[n=p,p=l,c=o\] => 'us.pron#\[n=p,p=3,c=o\] => 'them.vp => v#\ [n=!
,p=!
, type=!
\ ] ,  a rgs#\ [ type=!
\ ] .adjs => \[1.adjs => adj, adjs.args#\[type=i\] => \[\].args#\[type=t\] => np#\[c=o\].args#\[type=d\] => np#\[c=o\], ' to ,  np#\[c=o\].pn => 'tom \] 'dick I 'harry.det => 'somel 'the.det#\[n=s\] => 'every I 'a.det#\[n=p\] => 'all I 'most.n#\[n=s\] => 'child \[ 'cake.n#\[n=p\] => 'children \[ 'cakes.adj => 'nice \[ 'sweet.v#\[n=s,p=3,type=i\] => 'sleeps.v#\[n=p,type=i\] => 'sleep.v#\[n=s,p=(l,2),type=i\] => 'sleep.v#\[n=s,p=3, type=t\ ]  > 'eats .v#\[n=p,type=t\ ]  => 'eat .v#\ [n=s ,p=(1 ,2) , type=t \ ]  => 'eat.v#\[n=s,p=3,type=d\] => 'gives.v#\[n=p,type=d\] => 'give.v#\[n=e,p=(1,2),type=d\] => 'give.25
