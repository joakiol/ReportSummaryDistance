Lexicase Parsing: A Lexicon-driven App.roach.
to Syntactic AnalysisStanley STAROSTAUniversity of lIawaii Social Science Research Institute andPacific international Center for lIigh Technology ResearchHoaohdu, Hawaii 96822, U.S.A.Abst rac tThis paper presents a lexicon-based approach to syntacticanalysis, l,exicase, and applies it to a lexicon driven computatlolmlparsing system.
The basic descriptive mechanism in a l,exicasegrammar is lexieal features.
The properties of lexieal items arerepresented by contextual and non-contextual features, andgeneralizations are expressed as relationships among sets of thesefeatures and among sets of lexieal entries.
Syntactic tree strue{,urosare representaed as networks of pairwise dependency relationshipsamong the words in a sentence.
Possible dependencies are marked ascontextual features on individual exical items, and Lexicase parsingix a process of picking out words in a str ing and attaching dependentsto them in accordance with their contextual features.
Lexiease is anappropriate vehicle for parsing because I,exicase analyses aremonostratal, fiat, and relatively non-abstract, and it is well suited tomachine translation because grammatical  rel)resentations forcorresponding smrtences in two languages will Im very similar to eachother in structure and inter-constituent relations, and teas far easierto interconvert.1.
In t roduct ionThere are a number of current frameworks of syntactic analysiswhich have been used as the basis for natural language processing.Many suffer from serious metatheoretical or pra(:tieal defects,especially in the areas of power and descriptive adequacy.
Severalmore recent syntactic fl'ameworks, including I,exical-FunctionalGrammar \[1\], Generalized Phrase Structure Grammm" \[2\], andLexiease \[3\] have begun to take these problems seriously, and toconsider al)plications to natural anguage processi~g.
This paper willbe concerned with the application of lexiease grammatical  theory tocomputer parsing of natural anguage texts.The point of view which we will adopt here is a very simple one:sentences are hierarchical ly structured strings of words, andgrammar is a statement about the internal composition and externaldistributions of words.
Proceeding from this basis, it is possible toconstruct a fornml and explicit grannnatical f 'amework of l imitedgenerative power which is capable of stating language-specific anduniversal generalizations in a natural way, unhh}dered bypretheoretieal  priori assumptions about VP's, etc.
The fl'ameworkso constructed, lexiease \[13\], \[4\], \[5\], turns out to have a significant1)otential for application in the processing of natural anguage \[6\],The basic descriptive mechanism in a lexiease grammar is lexiealfeatures.
The properties of lexieal items are represented byeantextual and non-.contexttlal features, aud generalizations areexpressed as relationships among sets of these features.
The ways inwhich words can combine together are strongly restricted by theSisterhead Constraint \[3\], which states that a word can contract agrammatleal  relationship only with the head of a dependent sistereonstruetlon, and tbe One-bar Constraint lop.
ell,l, which requiresevery construction to have at least one lexical head.
The result issyntaetie tree representations which are flatter, since there are nointermediate nodes between lexical entries and their maximalprojections, and more universal, since there are only a very l imitednumber of ways in which languages can differ in their grammars.These properties turn out to make lexicase especially well suited tomachine translation, since the grammatical  representations forH i rosato NOMURANT'T Basic Research LaboratoriesMusashino.shi, Tokyo, 180, Japancorresponding sentences in two languages will be very similar to eachother in structm'e and inter constituent relations, and thus far' easlerto intcrconvert.This paper begins with a briefdescriptiml of the basic structure ofa lexieasc grammar, and then describes an algorithm which applieslexlease principles to sentence parsing.
Because of space limitations,we will not provide a full explication of the whole theory here.Instead, we will place the primary focus on the ways in whiehparticular lexicase principles aid in the straightforward and efficlcntconstruction of syntactic tree representations for input sentences.Section 2 describes the way in which grammatical  information can bepresented as a ~;et of generalizations about classes of lexical itemsrepresented in a dependency-type tr e format.
Section 3 describes tirevarious types of lexicase feahn'es and their respective roles in agramamr.
Section 4 discusses tim representation of structural in ferelation about individual sentences in terms of a tree representation,and sections 5 and 6 present an algorithm showing how theinibrmation provided by a lexicase grammar may be used in parsing.2.
Rules  and representat ions  in Iexiease theoryl,exicase is part of the generative grammar tradition, with itsname derived from Chomsky's lexiealist hypothesis \[7\] and Filhnore'sCase (_h'annnar 18\].
It has also been strongly influenced by l!
',uropeangrammatical  theory, especially the localistie case grammar anddependency approaches of Jolm Anderson \[91 and his recent andclassical predecessors.
I,ike Chomskyan generative grammar, it is anattempt o provide a psychologically wdid description of tile l inguisticcompetence of a native speaker, but it differs fl'om Chomsky'sgrammatical  f 'amework in power, since it has no transformationalrules, and in generativity, since it requires grammatical  z'ules andrepresentations to be expressed formally and explicitly and not justtalked about.
The rules of lexlcase grammm" proper are lexlcal rules,rules that express relations among lexicaI items and among featureswithin \]exieal entries.
There are no rules for constructing ormodifying trees, and trees are generated by the lexicon rather than byrules: the structural representation of a sentence is aay sequence ofwords connected by lines in a way which satisfies the contextualfeatures of all the words and does not violate the Sisterhead or One-bar Constraints or the eouventim~s for constructing well-formed trees.A lexiease parsing algorithm, aeeordingly, is just a mechanism forl inldng pairs of words together in a dependency relationship whichsatisfies these contextual features and tree-forming conventions.
(\[1l\], \[12\], and \[131 rm.
a very similar but independently developedapproach which evolved fi'om the computational rather than thel inguistic direction.
)Figure 1 lists the rule types in a lexiease grammar and theirinterrelationships.
Redundaney rules supply all predictable featuresto lexieal entries, which are stored in their maximally reduced tbrms,with all predictable features extracted.
For example, all pronotms arenecessarily members of the class of nouns, and since the feature \[ q N\]is thus predictable fl'om the \[ q prnnl (pronoun) feature, \[-I-NI can beomitted from pronoun entries in the lexicon and supplied to the entryby a demon, a lexical Igedtmdan%~ Rule, daring processing.Subcategorization rules characterize choices that are availablewithin a particular category.
These rules are of two subtypes,inflectional and lexicah Fer example, one in\[leetioualt27Suhcategorization Rule states that English count nouns may bemarked as either singular or plural.
The other type ofSubcategorization Rule does not allow an actual choice, but rathercharacterizes binary subcategories of a lexical categery.
Forexample, there is a non-inflectional Subcategorization Rule whichstates that English non-pronouns are either proper or common.Inflectional Redundancy Rules state the contextual consequencesof a particular choice of inflectional feature.
Thus the choice of thefeature 'plural' on a head noun triggers the addition of a contextualfeature to its matrix stating that none of its dependent sisters may besingular.Derivation Rules characterize relations between distinct butrelated lexical entries.
For example, they provide a means ofassociating 'quality' adjectives with corresponding -ly manneradverbs.
Due to the non-productivity of ahnost all derivationalrelations, both derived and underivcd lexical items must be storedand accessed separately in the lexicon, so these rules play only aminor role in parsing.
(They are however the major loxicasemechanism for stating the interrelationships of sentenceconstructions such as active and passive clauses.
)Phrase-level phonological rules and anaphorie rules are the onlynon-lexieal rules in the lexicase system.
The latter mark pronouns,'gaps' or qmles', and other anaphorie devices as coreforential or non-eoreferential, and so are a very important component of an adequateparsing system, tlowever, a discussion of this question would go wellbeyond the intended boundaries of this paper.With the rules and constraints outlined in this section, it ispossible to radically simplify a grammar and the associated lexicon inways which facilitate parsing, as detailed below.3.
Features in lexieaseAs mentioned above, lexical features in a lexiease grammar are oftwo types: contextual and non-contextual.
Contextual featuresspecify ordering and dependency relationships among major syntacticcategories ('parts of speech'), agreement and governmentrequirements, and 'selection', semantic implications imposed by headitems on their dependents.
Non-contextual features characterizeclass memberships, including membership in major syntacticcategories, subcategory features, inflectional features (includingperson, number, gender, and tense features as well as localistic aseform and case relation features, which will not be discussed in thispaper; but see \[3\]), and the minimum number of semantic featuresneeded to distinguish non-synonyms from each other.Lexical entries.~ rivation ,'tiles \]r Redundancy rulesW\[ Subcategorlzation rules }----n,"~ I Morphological rules-\]InflectionalRedundancy rules l~ ' J4.Fully specified lexical entries4.Phrases\[ Phrase- i~  Phrase-levelphonological rules J ?
anaphorlc ru les \ ]Fully speci~ed phrases\[ Disconrsecontext \]4.Interpreted phrasesPARSING PRODUCTIONFig.
1 Lexicase theory construction128(1) Case relationsLexlcase assumes only five 'deep' case relations, with inner andouter functions distinguished for three of them \[5\], as shown in Figm'e2.
The inventory of case relations is as short as it is because lexicaseestablishes a more efficient division of labor: much of the semanticinformation formerly carried by case relation differences inFillmorean-type case ,'elations is now carried by the semanticsubcategory features of classes of verbs, and by the semantic featuresof the case markers themselves.
The resulting reduced non-redundant case relation inventory improves the efficiency of case-related parsing procedures, and makes it possible to capturesignificant generalizations about case marking that are not possiblewith the usual extended inventories used in other case grammar andnatural anguage processing systems.
It is necessary to refer to caserelations in parsing structures containing multi-argumentpredicates, in accounting for anaphora and semantic scopephenomena and ~ext coherence, and of course in translation.
Again,however, a discussion here of this aspect of lexicase parsing would gobeyond the scope of this paper.
(2) (3ase formsUnlike case relations, syntactic-semantic categories whosepresence is inferred indirectly in order to account for lexicalderivation and scope and anaphora phenomena, case forms areconfigurations o?
surface case markers such as word order,prepositions, postpositlons, case inflections, or relator nouns whichfunction to mark the presence of case relations.
They are groupedtogether into equivalence classes functionally in terms of which caserelations they identify, and semantically on the basis of sharedlocalistic features as established by means of componential nalysis.Case forms in a lexicase grammar are thus composite rather thanatomic.
Each is composed of one.
or nmre features, either purelygramrnatical ones such as :t2 Nominative (q~ Nom), whichcharacterizes the grammatical subject of a sentence, or localistic onessuch as source, goal, terminus, surface, association, etc.Semantically, case forms carry most of the relational informationin a sentence, and are used by the parser in recognizing the presenceof particular ease relations.
For example, it is necessary to refer tothem in for example identifying subjects in order to check for subject-verb agreement.
Since so much 'case relation'-type information hasbeen found to be present lexieally in the case markers themselves,they bear much of the semantic load in the semantic analysis ofrelationships among lexieal items, so that this information eed notbe duplicated by proliferating parallel ease relations.
This meansthat in parsing, such information is obtainable directly by simplyaccessing the lexical entries of the case-markers rather than by morecmnplex inference procedures needed to identify the presence of themore usual Filhnore-type case relations.Patient (PAT):the perceived central participant in a state or eventAgent (AGT):the perceived external instigator, initiator, controller, orexperienccr ofthe action, event, or stateLocus (LOC):inner: the perceived concrete or abstract source, goal, orlocation of the Patientouter: the perceived concrete or abstract source, goal, orlocation of the action,event, or stateCorrespondent (CAR):inner: the entity perceived as being in correspondence withthe Patientouter: the perceived external frame or point of reference forthe action, event, or state as a wholeMeans (MNS):inner: the perceived immediate affeetor or effeetor of thePatientouter: the means by which the action, state, or event as awhole is perceived as being realizeFig.
2 Case relations in lexicase(3) Syntactic category featuresA small inventory of major atomic syntactic ategory features isassmncd by lexicase, current ly l imited to the following seven: noun(N), verb (V), adverb (Adv), preposit ion or postposition (P), sentenceparticle (SPort), adjective (Adj), and determiner (Det).Major syntactic categories are divided into syntacticsubcatego,' ies based on differences in distribution.
Thus nouns aredivided into pronouns (no modifiers allowed), proper nouns (noadjectives and typical ly no determiners allowed), mass nouns (notpluralizable), etc., and simi lar ly for the other syntactic lasses.
Thecontextual features associated with the words in these variousdistr ibutional classes determine which words are dependent on whichother words, and thus are very important  in assigning correct b'ees toparsed sentences.
(4) Inflectional featuresTradit ional  inflectional categories such as person, number,gender, case, tense, etc., are b'eated in lexicase as fl-eely variablefeatures which are not stored in their lexical entries (except in thecases of unpredictable forms), but are rather  added as needed by aSubeategorizat ien Rule in the course of processing.
Inflection istypically involved in agreement,  and agreement  relat ionships (inconjm~ction with the Sisterhood Constrahrt) are important  in locatingand l inking together those words bear ing a head dependentrelat ionship to each other.
(5) Semant ic  featuresLexicase assumes that  there must  be enough semantic featm'esmarked on lexical items so that  every lexieal item is differentiatedfrom every other (non-synonymous) i tem by at least one distinctivesemantic feature.
These features are not directly involved in parsing,but may f igure in the identif ication of metaphors in sentences whichdo not have any other well-formed parsings.
(6) Contextua l  featuresContextual  features are the part  of the lexical representat ionwhich makes phrase structure rules unnecessary.
A contextualfeature is a Idnd of atmnic valence, stat ing which other words mayattach to a giwm word as dependents to form the molecules called'sentences'.
Contextual features may function syntactical ly,morphological ly, or semantieal ly.
For example, tile feature\ [ - \ [ -F  Det\]\] on Engl ish nouns states that  English determiners maynot follow their nouns; another feature, \ [+\ [+I )e t \ ] \ ] ,  is marked ondefinite common nouns to show that  they must reoccur withdeterminers,  and a third, \[-\[-plrl\]\], marks  plural nouns as notal lowing non-plural attr ibutes.
The feature \ [+( \ [+Adj \ ] ) \ ]  on commonnouns states that  they may have adjectival attr ibutes,  a possibil itywhich would otherwise be excluded by the Omega-rule (see below).Contextual features may refer to dependents occm'ring on the leftor on the r ight,  or they may be non directional,  referr ing to sisterdependents on either' side when the presence of some category isimportant  but the order varies (as in topicalization and Fmglishsubject-auxi l iary inversion) or is i rrelevant (as in fl'ce word-orderlanguages).Selcetional features are also contextual, but they differ infunction from grammat ica l  contextual features.
Thus a verb likethe there my(a) Noun phrase (b) SentenceFig.
3 l,exicase tree representat ions'love' may impose an an imate interpretat ion on its subject by meansof the following selcctional feature: \ [D\ [+AGT,  -anmt\] \ ] .
A l thoughthe violation of a selectional feature does not result  inungrammat lca l i ty ,  solectionaI features are usefal in pars ing to pickthe most promis ing branch in pars ing a sentence when two or morediffm'ent l inks are possible for a given word, or in identifyingmetaphors when no well-formed parse of a sentence is otherwisepossible.Since the 'range' of contextual features is sharply lhnited by theSisterhead Constraint,  only certain kir~ds of l inks between words arepossible, and only those words directly connected by a single l ink needbe checked for the satisfaction ef grammat ica l  requirements such ascase fl'ames, agreement  featm'es, etc.
This great ly l imits the numberof places a parser  has to cheek in determining the well-formedness ofa given sentence, and so facil itates parsing.Contextual fcatm'es may be positive, negative or optional.Positive contextual features state the presence of a requireddependent, and are used in pars ing to establish initial links betweenpairs of words.
Negative features klentify classes of words which arenot allowed to occur as dependent sisters, and serve in pars ing toreject some of the l inks nmde in accordance with positive features.Optional featm'es do not require or reject any links, but rather  serveto keep open tire possiblity of l inking pairs of words by a generalprocedure applying near the end of the algoritt~m (see 6.3 below).
Alll inks which are not marked as permissible in this way are ruled outby the 'Omega Rule', a lexical Redundancy Rule which states thedefimlt value for tire ' l inkabil lty' of given pairs of wm'ds: all l iukingswhich are not explicitly allowed for are disallowed.The most iml)ortant charactm'ist ic for all contextual features forthe purposes of pars ing is tile Sisterhcad Constraint: in (lctenninh~gwhether a contextual feature is satisfied for a given item, the parserneed look only at the head words of its sister eategm'ies.4.
Lex iease  tree representat ionIn lexicase, tree d iagrams arc graphic representat ions ofdependency and constituency relat ionships holding among pairs ofwords in a sentence, and thus indirectly of relations among theconstructions of which these words are the heads.
Two types ofconstructions are recognized: endocentric and exoeentrle.
These twoconstruction types can be identified and their internal  and externaldependency relations determined irectly from the kinds of lines bywhich they are connected in a Iexiease tree representat ion (or,equlwdently,  by  their bracket ing in a LISP-type parenthesisnotation):i) vertical lines l ink a phrasal  node with its head: a un i t lengthline indicates a lexical head, and a two-unit- length lineidentifies a phrasal  head of an exoeentrie construction;it) s lant ing lines l ink an endoeentrie phrasal  node with itsdependents; andiii) horizontal lines l ink the vertical lines above the lexieal orphrasal  heads of an cxocentrie construction.An endoeentrie construction is any syntactic construction whichhas only one obligatory member, i.e.
one head, which in accordancewith the lexiease One-Bar Constra int  must  be a single lexicai item.The other constituents of such constructions are phrases which aresyntact ical ly optional dependents of the head word.
Noun Phrasesand Sentences for example arc endocentric onstructions, headed by,, J J a ( |  l i t  iI.lS c ........ \] \[ ES",~;,\] E"-',,','~myFig.
4 The domain of the verb 'saw'saw ITom / I and IP~?q / ..,1 C+~cni m,r.,L ' "  g \] \[-+?AT\] \] F .
.
.
.
.
1my L ~ N j L ~.
N jE * l)a \] ?cnj = coordinating conjunctionFig.
5 Tree representat ion with category features129nouns and verbs respectively.
In a tree, the head word of anendocentric construction has a vertical ine of unit- length above it.An exoccntric onstruction on the other hand has more than oneobligatory constituent.
Again, the One-Bar Constraint requires thatat least one of the constituents must be a single word, the lexical headof the construction.
The other obligatory head (or heads) may be aword or a phrase.
Examples of exocentric constructions areprepositional phrases and coordinate constructions.
In a tree, each ofthe co-heads of an exocentric onstruction has a vertical ine above it,of unit- length above lexical co-heads and two-unit-length above thelexical heads of phrasal co-heads.
The apexes of the vertical ines arejoined by a horizontal ine, in effect an elongated node.
Examples ofboth types of phrases appear in Figure 3.The gramatical ly relevant relationships between pairs of nodes ina tree are expressed in lexicase in terms of the notions 'command' and'cap-command' (from Latin caput, capitis 'head'):i) a wm'd cap-commands the lexlcal heads of its dependent sisters;thus in the two trees in Figure 3,a) 'boy' cap-commands 'that', 'on', and 'bus', since 'boy' hastwo dependent sister constituents (indicated by s lant ing lines),'that' and 'on the bus there'.
The lexical head of the construction'that' (stlown by a vertical ine) is the word 'that'.
However 'on thebus there' is an exocentric onstruction (shown by a horizontalllne) which has two heads (shown by vertical ines), 'on' and 'thebus there'.
The lexical head of 'on' is 'on', and the lexical head of'the bus there (vertical line) is 'bus'.b) 'on' cap-commands 'bus', since 'on' has a single dependentsister (the phrasal co&cad of the exocentric onstruction 'on thebus there'), 'the bus there', and the lexical head of 'the bus there'is 'bus'.
Finally,c) 'bus' cap-commands 'the' and 'there', since 'bus' has twodependent sisters, 'the' mad 'there', and the respective heads ofthese two constructions are the words 'the' and 'there'.ii) a word X commands a word Y if elthera) X cap-commands Y, orb) X cap-commands Z and Z commands Y.Thus for example 'boy' commands 'there' because 'boy' cap-commands 'bus' and 'bus' cap-connnands 'there'; however 'that' doesnot command 'there' because 'that' has no dependent sisters at all,and so does not cap-command anything.The notion 'cap-command' plays a crucial role in defining thedomain of subcategorization.
To determine which constituents arerelevant in subcategorization, lexicase appeals to the SisterheadConstraint, which maintains that 'contextual features are marked onthe lexical heads of constructions, and refer only to lexlcal heads ofsister constructions' \[3\].
That is, a word is subcategorized only by thewords which it cap-commands.
For example, a verb may besubcategorized by the heads of the noun phrases which are its sisters,but not by the other constituents which are inside the NP's.Conversely, a noun may not be subcategorized by any constituentoutside the NP.
However, in the case of exocentric onstructions suchas prepositional phrases, the head words of botl~'all obligatory co-head constituents are accessible for subcategorization, since they areall cap-commanded by the higher head item.To i l lustrate, in the Noun Phrase in Figm'e 3 (a), the lexical headof the construction is the noun 'boy'.
Following the SisterheadConstraint, the contextual features marked on 'boy' can refer only tofeatures of the words it cap-commands, in this case 'that' and theheads of the exocentric PP, 'on' and 'bus', but not to 'the' or 'there'.The features of both the preposition and the head of its sister NP fallwithin the domain of subcategorization of the cap-commandinglexical item and jointly subcategorize it.
Their features takentogether are said to form a 'virtual matrix', i.e.
a matrix which is notthe lexical specification of any single lexical item, but which is rathera composite of the (non-contextual) features of all of the lexical heads130of the construction \[3\].
In the lexicase parsing algorithm discussed inthis paper, the effect of a virtual matr ix has been achieved by copyingthe features of the phrasal head (the lexical head of the phrasal co-.head, e.g.
'bus' in 'on the bus') into the matrix of the lexicaI head (e.g.
'on' in 'on the bus' in Figure 3).
The matrix of the preposition 'on' thenbecomes in effect the virtual matrix of the exocentric onstruction,representing the grammatical ly signif icant features for the whole PP.The Sisterhead Constraint makes it possible to define the notionof syntactic domain as all those constituents whose heads are referredto by the contextual features of a particular lexieal item.
Forexample, the domain of the verb 'saw' in the example of Figure 3 isindicated in Figure 4 with ease relations.
Thus the domain of the verb'saw' in this sentence consists of the arguments marked \[ + PAT\] and\[+AGT\].
The determiner 'my', on the other hand, is not in thedomain of the verb; rather, it is in the dmnain of its own dominatinglloon~ 'Dad'.There are a number of other constraints in lexiease which apply tosyntactic trees \[3\].
The effect of these constraints i to l imit the classof possible trees and, consequently, the class of possible analyses.
Oneconstraint is that all terminal nodes are words, not morphemes orempty categories.
A related constraint states that syntactic featuresare marked only on lexieal items, not on nodes or on ad hoe abstractlexieal categories.
Finally, lexicase requires that every constructionhave at least one immediate lexieal head; that is, there can be nointervening non-Iexical node between the phrasal node and thelcxlcal head of the phrase.
In X-bar terminology, lexiease allowsphrasal nodes with a maxinmm of one bar, where an S is equivalent toV-bar.The interaction of the tree-drawing conventions, the One-barl imitation, and the Sisterhead Constraint makes it possible toel iminate both phrasal and major category labels from syntactic treeswithout any loss of information \[3\].
The matr ix of an individuallexieal item contains information about  its syntactic category,making a category node label redundant.
With the One-BarConstraint, the nature of the phrasal construction can be determinedwith reference to tbe lexieal category of the head of the construction,which is identifiable by the unit- length vertical line above it.
Thusany node directly attached to a lower \ [+N\]  item by a vertical ine ofunit-length is an NP, so it is redundant o mark such a node by thelabel 'NP'.
As a consequence, the tree representatiml in Figure 5which has no node labels overtly marked is adequate for therepresentation f all constituency and dependency information.
Notethat the CCJN ('conjunction-bar') 'my Dad and Rufus' in Figure 5 isstil l an NP in function, because a coordinate construction isexocentric, and so the virtual matrix associated with 'my Dad andRufus' contains the feature \ [+N\]  as well as \[+cejnl, making it an NPfor external subcategorizing purposes.The single-level lexicase tree notation incorporates theinformation carried by the three different kinds of tree structurecontrasted by Winograd \[10\], dependency (head and modifier), phrasestructure (immediate constituents), and role structure (slot andfiller).
Because it allows no VP constituent, it can equate constituentstructure with dependency structure.
The case role of a constituent isthe case role of its lexical head.
Thus semantic information is readilyextracted from the syntactic representation, because therepresentation links together those words which are semantical ly aswell as syntactically related.5.
The pars ing  a lgor i thmFigure 6 shows the fundamental components of the Iexieaseparser.
The function of these components in brief is as follows:(1) P re -proeessorThis procedure replaces the word forms in the input sentence byhmnographic fully specified lexicaI entries, that is, entries withidentical spelling, specified for all contextual and non-contextualsyntactic features as well as contextual and non contextual semanticfeatures ('selection'd restrictions').
If an input form matches morethan one lexical entry, replace the form by a 'cluster', a list of all thelcxical entries whose forms match tim input form.
The output is astr ing composed of lexieal entries and clusters of lexical entries whichis isomorphous with the input str ing of word forms.
(2) Morpho log ica l  ana lyzerIf an input fin'm is not matched by any item listed in the lexicon,the morphological analyzer checks to see if the form matches anystored stem-affix pattern.
If it does, the form is divided into stem plusinflectional affix and the stem is markcd with the syntactic classfeatures associated with tile pattern.
Using inflectionalSubcategorization Rules, the stem is expanded into its fullinflectional paradigm, and the original input word form is replaced bya 'cluster' composed of those (ffdly ,~;pecifled) members of tileinflectional paradigm which are homographic with the original wordlbrm.
(3) l ' l aeeho lder  subst i tu t ionEach cluster of homographic lexical entries in the substitutionstring is temporari ly replaced by a 'placeholder' entry composed of theintersection of the form and features of all the entries in tile cluster.If the entries have nothing in common hut the form itself, then theplaceholder will be the form alone, with no associated feature matrix.If the lcxical entries in a cluster have enough featnres ill con'nnonto be equivalent in terms of l in ldag potential, they are l inked into thetree structure as a group during the parsing process.
When thestructures containing clusters of entries are subsequently resolvedinto lexically unambiguous structures during placeholder expansion,many of the necessary l inks will have ah'eady been nmde, and willnet have to be repeated for each separate but syntactically equivalenthomographic enlry.
(d) P laeeho lder  expans ionEach substitution string containing plaeeholder clusters isexpanded into separate structm'es by replacing the clusters withsubclustcrs of items sharing nlore features in comn'lon, andult imately with their original constituent hldividual entries.
Aftereach cluster is resolved into subclusters or individual entries, theresultant substitution strings are passed through the parser again toadd l inks that become possible as the new clusters and entries becomeaccessible.As with the previous parsing phase, this phase establishes linksthat work for clusters of honmgraphie tems, so that these l inks do nothave to be nmde separately and repeatedly for each substitutionInputorphological_ _ ~  - - - '9  .
\[ AnalyzerPlaceholdcr \] \] PlaceholderSubstitution J \[ ExpansionParserP'sV'sN'sI)et'sAdj'sAdv'sConjunctionsOrphans - -OutputFig.
6 Fundanmntalcomponentsofthelex icaseparserstr ing containing a different homographic item.
In this way, nosequence of words ever has to be rcparsed.
(5) ParserBased on the positive contextual syntactic features of head lexicalitems, the beads are linked to eligible and accessible dependent i ems.As each link is established, the negative contextual ?eatm'es arechecked.
If there is a violation, that track is immediately abandoned.Note that exactly the same negative contextual feature mechanismtakes carc of two distinct contextual dependency phenomena:i) general cooccurrencc properties, such as the fact that Englishnouns may not have following Determiners, andii) grammatical  agreement; thus for example subjce.t-verbagreement is stated as a negative contextual feature: a finiteverb marked for plural nlay not have a dependent Nominativesister marked singular.
(Ar.tually the matter is somewhat morecomplex than this, but a fidl discussion would go beyond thescope of this paper.
)After each pair of words has been linked in accordance withpositive and negative grammatical  contextual features, implicationalsemantic contextual features C,;eleetienal restrictions') are checkedfor compatibility.
If a violation is found, that str ing is semantical lyallonlalous.Lexicase theory is designed such that only the heads of sistercategories need to be considered in determining whether there is aninconsistency in a structure being parsed.
That is, only words directlyconnected by a single line need to be checked for the satisfaction orviolation of any grammatical  or selectional contextual requirement,and this checking can be done immediately afte.r each l ink is firstmade.
If a violation is found, the structure can be shunted off on asiding immediately without wasting time examining surroundingmaterial.
The parsing procedm'e will be considered in somewhatmore detail in the section 6.
(6) OutputThe outptlt of tile algorithm is zero or nmre syntactic analyses ofthe input sentcnce, but at the same time it can be considered anintensional semantic representation: it presents all the sernanticdistinctive features for each word, and specifies the head-modifier andsemantic implication relations between each linked pair of wm'ds.The 'extensional' meaning of the sentence then is just tile range ofexternal situations which are compatible with the intension, thelexical meanings and interrelationships characterized by thisstructure.
I,exicase is very well suited to characterizing thisintcasional semantic representation bccausc it formally defines therange of possible loxical l inkages.
The structure is simple yet richenough to in principle carry enough information to serve as the inputto a know\]edge extraction or machine translation system.6.
The parsing procedure6.1 Words(1) Prepos i t ions :  IAnk each preposition by contextualfeatures with an accessible N, V, or P. Prepositions are linked firstbecause they l ink with N's, V's, or other P's to form PP's wbich delimitclosed domains whose internal non-head constituents are theninaccessible to connections with external elements.
Subsequentparsing stages then search inside of or outside of these dmnains, butdo not need to consider l inks between PP-internal not>heads and PP-external lexical items.
(2) Verbs:  Verbs are l inked with their attr ibutes to formclauses or sentences.
Note that in the lexicase framework, 'sentence'refers to any verb-headed construction, regardless of the finiteness ofits verbal head or its position in the tree.
The searching proceedes131from left to right in English, but would scan fi'om right to left in averb-final left-branching language such as Japanese.
In adependency grammar f amework such as lexicase, a (verbal) sentenceis defined as a verb together with its syntactic dependents.
Asentence is the basic unit of syntax because it is the maximumdomain of dependencies.
Once a sentence unit has been establishedin this way, subsequent parsing stages can ignore links betweensentence-internal and sentence-external items.
(3)Nouns: Nouns are linked with their dependents to formNoun Phrases.
Noun Phrases and Sentences ('verb phrases') are thesyntactically and semantically basic sentence constituents.
Likeother head items, nouns establish domains whose non-headconstituents are inaccessible to external links, so that cross-domainIinkages can be ignored on subsequent passes, thereby radicallylimiting the number of pairs of items that have to be considered oneach subsequent pass and again cutting down on computation time.
(4) Determiners: Link each Determiner with an accessibleNoun.
In English, the Determiner marks the left boundary of a NounPhrase.
Linking the N and its Det establishes one boundary of theNP, and subsequent parsing can ignore links between elementsinside this domain and elements outside it.
(5) Adjectives Link each Adjective with an adjacent noun.Because previous passes will have already delimited majorconstituent boundaries and radically narrowed the set of possibleconnections, very little checking will need to be done to link anAdjective with the correct head Noun.
(6) Adverbs: Link each Adverb with a head Verb or Adjective.Structural ambiguity is most likely to appear in connection withalternate attachments of PP's and Adverbs with other words in asentence.
By saving Adverb linking until near the end of the parsingsequence, we establish domains of inaccessibility which greatlyreduce the number of possible Adverb attachment points which needto be considered.6.2 CoordinationLink each conjunction with one or more major constituents (S,NP, PP, AdjP, or AdvP) on each side.
At this point, all the majorconstituents have already been established, so the conjunctionlinking procedure needs to consider only the head word of each majorconstituent.
Since every conjunction will at this time be either at thehighest level, that is, linkable only to the immediate constituents ofthe sentence, or inside the domain of some other construction, thcnumber of linking choices will be extremely limited.6.3 OrphanageLink all remaining upwardly unlinked Nouns, Determiners,Adjectives, Adverbs, Prepositions, and Verbs with an accessible ' ldersister' (or 'regent' \[12\]).
At this point unattached lexical items will befound only embedded inside of other constructions, with very fewaccessible attachment possibilities to consider (usually only one).Thus there will generally be no backtracking and stacking required.The exception will be Adverbs and PP's, which account for most of thestructural ambiguity likely to be encountered.
By saving thesealternative connection possiblities until near the end of the parsingprocess, we minimize the amount of computation that has to be done'on top of' the alternative structures produced at this stage.7.
Overal l  assessment  and conclusionThe parsing approach we advocate here is in principle very simplebecause lexicase requires no rules for normal parsing situations atall, and is based on linguistic principles designed to maximize thegenerality and simplicity of descriptions.
It has no deep structure or132transformations; instead, 'transformed' and 'untransformed' lexicalentries are listed separately in the lexicon, thereby placing theparsing burden on memory rather than processing.
Since Iexicaseautomatically determines which items are relevant to the satisfactionof particular contextual requirements, no feature percolation orfeature copying mechanism is needed to move features around in atree to get them into a position where they are accessible to relateditems.Lexicase parsing is bottom-up in the sense that it begins withindividual words rather than some 'root node' S. It scans from left toright or vice versa, depending on whether the language is verb-initial, verb-medial, or verb-final, but in fact it is a mechanism whichworks from head to dependent rather than primarily from one end orthe other.
Since it forms constituents from heads and dependents atall levels simultaneously, it thus incorporates virtues of both top-down and bottom-up arsers.
Lexicase accomplishes this by onlymaking links allowed or required by contextual features of headlexical items, and since the 'overall structure of the sentence' isdetermined by just these features, it is not possible to make linkswhich are not compatible with this overall structure.Since lexicase has no Phrase Structure rules, a lexicase parsercannot blunder into the loops caused by left-recursive rules.
Lexicasegenerates linguistically correct structures: they directly representhead-attribute relationships, they characterize the concept ofgrammatical relatedness, they allow various other importantgeneralizations to be captured, and they account adequately forspeakers' intuitions.References\[11 Kaplan, It., and Bresnan, J., Lexical-Functional Grammar: AFormal System for Grammatical Representation.
In J. Bresnan(ed), The Mental Representation of Grammatical Relations.Cambridge University Press.
1982.\[2\] Gazdar, G., Klein.
E., Pullmn, G., and Sag, I., Generalized PhraseStructure Grammar.
Harvard University Press.
1985.\[31 Staresta, S., The End of Phrase Structure as We Know it.Linguistic Agency - University of Duisburg (Trier) Series A,Paper no.
147.
1985.\[41 Starosta, S., Case in the Lexicon.
Proceedings of the EleventhInternational Congress of Linguists.
1975.\[51 Starosta, S., Patient Centrality and English Verbal Derivation.Proceedings of the thirteenth International Congress ofLinguists.
1983.\[6\] Starosta, S., and Nomura, II., Lexiease and Japanese LanguageProcessing.
Musashino Electrical Communication LaboratoryTechnical Report.
1984.\[71 Chomsky, N., Remarks on Nominalization.
In Jacobs, R. A., andRosenbaum, P. S. (eds), Readings in English TransformationalGrammar.
Ginn and Company.
1970.\[81 Fillmore, C. J., The Case for Case.
in Bach, E., and Harms, R. T.(eds), Readings in English Transformational Grammar.
Ginn andCompany.
1970.\[91 Anderson, J., The Grammar of Case: Towards a Localistic Theory.Cambridge Studies in Linguistics 4.
Cambridge UniversityPress.
1971.\[101Winog~rad, T., Language as a Cognitive Process, Volume I:Syntax.
Addison-Wesley Publishing Company.
1983.\[lllLehtola, A., Jifppinen, H., and Nelimarkka, E. Language-basedenvironment for natural language parsing.
Precedings of theSecond European Conference of the Association forComputational Linguistics.
1985.\[12\]Nelimarkka, E., Jfippinen, H., and Lehtola, A., A computationalmodel of Finnish sentence structure.
In Ann S~ggcall Hein (ed),Fhredrag rid De nordiska datatingvistik dagarna.
1983.\[13\]Nelimarkka, E. J~ippinen, H., and Lehtola, A., Parsing aninflectional free word order language with two-way finiteautomata.
In T. O'shea (ed), ECAI-'84: Advances in artificialintelligence.
Elsevier Science Publishers B.V. 1984.
