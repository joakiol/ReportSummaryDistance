IMPROVING ENGLISH AND CHINESE AD-HOC RETRIEVAL:T IPSTER TEXT PHASE 3 F INAL REPORTKui-Lam KwokComputer  Science Department,  Queens Col lege, CUNY65-30 Kissena Boulevard, F lushing, NY  11367?
email: kwok@ir .cs .qc.eduphone: (718) 997 3482/3500ABSTRACTWe investigated both English and Chinese ad-hocinformation retrieval (IR).
Part of our objectives is tostudy the use of term, phrasal and topical concept levelevidence, either individually or in combination, toimprove retrieval accuracy.
For short queries, westudied five term level techniques that together lead toimprovements over standard ad-hoc 2-stage retrievalsome 20% to 40% for TREC5 & 6 experiments.For long queries, we studied linguistic phrases asevidence to re-rank outputs of term level retrieval.
Itbrings small improvements in both TREC5 & 6experiments, but needs further confirmation.
We alsoinvestigated clustering of output documents from termlevel retrieval.
Our aim is to separate relevant andirrelevant documents into different clusters, and to re-rank the output list by groups based on query andcluster-profile matching.
Investigation is still on-going.For Chinese IR, many results were confirmed ordiscovered.
For example, accurate word segmentationis not as important as first thought, but short-wordsegmentation is preferable to long-word (phrase).Simple bigram representation can give very goodretrieval.
A stopword list is not necessary; andpresence of non-content terms does not hurt evaluationresults much.
One only needs screening out statisticalstopwords of high frequency.
Character indexing byitself is not competitive, but is useful for augmentingshort-words or bigrams.
Best results were obtained bycombining retrievals of bigram and short-word withcharacter epresentation.
Chinese IR retums betterprecision than English, and it is not clear if this is alanguage-related, or collection-related phenomenon.1.
INTRODUCTIONAs increasing amounts of computer-readable textsare becoming available on the web or on CDROMs,text searching and detection has become anindispensable tool for information users and analysts ofall walks of life.
Up till the late 1980's, research intext retrieval has been mainly with small collections ofa few thousand items.
Since 1990, with the foresightof the TIPSTER and TREC programs, substantialprogress has been made to advance the state-of-the-artin text detection and ad-hoc information retrieval (IR)methodologies.
Examples include: availability,experimentation a d uniform evaluation of gigabyte-size collections, term weighting improvements, 2-stage'pseudo-feedback' retrieval strategy, recognition ofdifficulties of short queries versus long, use of phrases,treatment of foreign languages for multilingualretrieval, among others.
This investigation builds uponprevious findings to bring further advances in this fieldusing our PIRCS system.We have participated in all past TREC experimentswith consistently superior results.
Since 1996, we havealso participated in the TIPSTER Text Phase 3program.
This report serves to summarize work thathas been done, and some of the important findings forboth English and Chinese IR.
Section 2 and 3 gives anoverview of our PIRCS system and the 2-stageretrieval strategy.
Section 4 presents our work forEnglish ad-hoc retrieval employing term, phrasal andtopical concept levels of evidence.
Section 5 describesvarious Chinese retrieval experiments.
Section 6 hasthe conclusions.2.
P IRCS RETRIEVAL  SYSTEMThe software program we use for our Tipster 3investigations i PIRCS (acronym for ProbabilisticIndexing and Retrieval - Components - System).
It is adocument retrieval system that has been developed in-house since the mid 1980s.
It is based on theprobabilistic indexing and retrieval approach,conceptualized asa three layer network with adaptivecapability to support feedback and query expansion,129q=QTD?loiQ '~oDTQ\[~,.0 ~ WkiDTdl qa w=Qtk..v-O .~0o t~DTQl~earning &ExpansionWkiFig.la PIRCS's 3 Layer Network for Retrievaland operates via activation spreading.
The networkwith three levels of query Q, term T and document Dnodes are connected with bi-directional weighted edgesas shown in Fig.la for retrieval.
Fig.lb shows thenetwork for performing learning where both the edgeweights and the architecture can adapt?
Learning takesplace when some relevant documents are known for aquery.
The basic model evaluates a retrieval statusvalue (RSV) for each query document pair (qa di) as acombination of a document-focused QTD process thatspreads activation from query to document hroughcommon terms k, and an analogous query-focusedDTQ process operating vice versa, as follows:RSV = cx*Z k Wik *S(qak/La).-{- (1-(X)*ZkWak * S(dik/Li)where 0_<ct_<l is a combination parameter for the twoprocesses, qak and d~k are the frequency of term k in aquery or document respectively, La, Li are the query ordocument lengths, and S(.)
is a sigmoid-like functionto suppress outlying values?
A major difference of ourmodel from other probabilistic approaches i to treat adocument or query as non-monolithic, but constitutedof conceptual components (which we approximate asterms).
This leads us to formulate in a collection ofcomponents rather than documents, and allows us toaccount for the non-binary occurrence of terms initems in a natural way.
For example, in the usualdiscriminatory weighting formula for query term k: wak= log \[p*(1-q)/(1-p)/q\], p = Pr(term k present \[relevant) is set to a query 'self-learn' value of qak /Labased on the assumption that a query is relevant oitself, and q = Pr(term k present I -relevant) is set toFk/M, the collection term frequency of k, Fk, divided bythe total number of terms M used in the collection?This we call the inverse collection term frequencyICTF.
It differs from the usual inverse documentfrequency IDF in that the latter counts only theFig.lb Query-Focused Learning & Expansionpresence and absence of terms in a document, ignoringthe within-document term frequency.
Moreover, as thesystem learns from relevant documents, p can betrained to a value intermediate between the basic self-learn value and that given by the known relevantsaccording to a learning procedure \[1\].
Our systemalso uses two-word adjacency phrases as terms toimprove on the basic single word representation.Documents of many thousands or more words long canhave adverse ffect on retrieval.
PIRCS deals with theproblem by simply segmenting long documents intoapproximately equal sub-documents of 550-word sizeand ending on a paragraph boundary.
For the finalretrieval ist, retrieval status values (RSV) of the topthree sub-documents of the same document arecombined with decreasing weights to return a finalRSV.
This in effect favors retrieval of longerdocuments that contain positive evidence in differentsub-parts of it.
PIRCS has participated in all previousTREC 1-6 blind retrieval experiments and consistentlyreturned some of the best results, see for example \[2\].3.
TWO-STAGE AD-HOC STRATEGYAutomatic ad-hoc retrieval refers to theenvironment where a user attempts to retrieve relevantdocuments from an existing collection by issuing 'any'query.
We have experimented only with naturallanguage queries that are derived from TREC topics.
Itis a difficult problem because the query wordings areunknown beforehand, and its topical content isunpredictable?
Moreover, there will not be anyexample relevant documents that a system can rely onfor training purposes like in a routing situation.To improve the accuracy of ad-hoc retrieval, it isnow a common practice to adopt a 2-stage retrievalstrategy.
Under the right circumstances this can givesubstantial improvements over single stage.
In a 1-130final retrievaldefine docu m ent dom ain docu m ent'3' collection collection_enrichment r~ '5'\[ 1st ~_~ 2nd local statistics collection ~ret r ieva l  retrievastatistics ~2~,  I ,~  ~ I feedback ocs'1', ' ' 4~raw expanded query querylistI~ - - -~~Fig.2 Two-Stage Retrieval and Methods of Improvementsstage retrieval, the raw query which is a user-provideddescription of information eeds is directly employedby the retrieval algorithm to assign a retrieval statusvalue (RSV) to each document in a collection, and theranked list of documents is interpreted as the f'malretrieval result.
In a 2-stage strategy, this initial rankedlist is interpreted as but an intermediate step.
The setof n top-ranked ocuments of the initial retrieval isassumed relevant, even though the user has not madeany judgment.
These 'pseudo-relevant' documents arethen used to modify the weight of the initial queryaccording to some learning procedure, as well as toexpand the query with terms from these documentsbased on some selection criteria like frequency ofoccurrence.
The modified query is then used to do asecond retrieval, and the resultant ranked list becomesthe final result.
This helps because if the raw query isreasonable and the retrieval engine is any good, theinitial top n documents can be considered as definingthe topical domain of the user need and should have areasonable density of relevant or highly relateddocuments, and the procedure simulates real relevancefeedback.Traditionally, real relevance feedback can give verylarge improvements in average precision, like 50 toover 100%.
Experiments with our PIRCS system haveshown that this 2-stage of ad-hoc method works moreoften than not, about 2 out 3 times (35 queries inTREC-5 and 32 in TREC-6 out of 50 queries each),and the average precision for a set of queries canimprove a few to over 20%.
The process of a 2-stageretrieval is depicted in Fig.2.In all of our work, this 2-stage approach is used inour retrieval experiments.
Some tables below showinitial 1-stage results for comparison purposes.4.
ENGL ISH AD-HOC RETRIEVALAn important finding in the TREC experiments ithat short queries have substantially different retrievalproperties from long ones.
We consider short queriesas those with a few content erms and are popular incasual environments such as web searching.
Serioususers wanting more exhaustive and accurate searchingshould issue longer paragraph-size queries with somerelated conceptual terms.
They usually return bettereffectiveness because longer exposition of needs canreduce the ambiguity problem due to homographs andthe descriptive deficiency due to synonyms.
The 2-stage retrieval approach has been shown in severalyears of TREC experiments o improve over 1-stagefor both query types.
Our work has investigatedadditional methods to enhance retrieval accuracy forthis strategy.4.1 Term Leve l  EvidenceWe studied several methods for improving ourapproach of 2-stage pseudo-relevance feedbackretrieval for short queries \[3\].
These are related tousing single term statistics and evidence, and include(see Fig.2): 1) avtf query term weighting, 2) variable131high frequency Zipfian threshold, 3) collectionenrichment, 4) enhancing term variety in raw queries,and 5) using retrieved ocument local term statistics.Avtf employs collection statistics to weight terms inshort queries \[4\] where term importance indication isgenerally not available.
Variable high frequencythreshold efines statistical stopwords based on querylength.
Collection enrichment adds externalcollections to the target collection under investigationso as to improve the chance of ranking more relevantdocuments in the top n for the pseudo-feedbackprocess.
Adding term variety to raw queries meansadding highly associated terms from the domain-related top n documents based on mutual informationvalues.
Making the query longer may improve 1 st stageretrieval.
And retrieved ocument local statistics re-weight terms in the 2 nd stage using the set of domain-related documents rather than the whole collection asused during the initial stage.
Results using thesemethods are tabulated in Table 1 where we show someof the popular evaluation measures: RR - the numberof relevant documents returned after retrieving 1000documents; AvPre -  the non-interpolated averageprecision; P@10 - the precision at 10 documentsretrieved, and R.Pre - the recall precision at the pointwhere the number retrieved is exactly equal to thenumber of relevant documentsIt can be seen that standard 2-stage strategyperforms about 9% to 15% better than initial retrievalusing the AvPre measure as reference (TREC5.161 vs..140, TREC6 .240 vs..220).
The other techniquessuccessively bring further improvements, accumulatingto about 20 to 40% over the standard 2nd stage retrievalresults (TREC5.239 vs.. 161, TREC6.289 vs..240).It is found that collection enrichment also works forlong queries.
It is an attractive technique sincesearchable t xts are increasingly available nowadays.RRAvPreP@IOR.Pre1 st 2 nd Avtf Var.
Coll.
M.I.Retr Retr Thld Enrich Terms~-TREC5 50 Short Queries .-91763 2279 2335 2635 2732 2787.140 .161 .181 .214 .234 .239.290 .284 .326 .372 .382 .404.179 .191 .210 .249 .270 .271RRAvPreP@IOR.Pre(-TREC6 50 Short Queries-92188 2272 2384 2517 2656 2738.220 .240 .258 .258 .284 .289.334 .372 .402 .388 .444 .442.262 .264 .291 .287 .312 .311Tablel :  Term Level Retrieval EnhancementWe envisage that so long as the external text fallswithin similar topical domain of the query, it could behelpful as an enrichment tool.
It goes quite a way toimprove the accuracy of retrieval, especially in thedifficul t ad-hoc, short query situations.4.2 Phrase  Leve l  Ev idenceInvestigators in IR are aware of the simplistic andinadequate representation of document content basedon a bag of single word stems or some 2-wordadjacency phrases.
To a certain extent his is dictatedby the requirements hat text retrieval systems have tosupport 'large scale environments as well asunpredictable, diverse needs.
Many previous attempts,including Tipster contractors (e.g.
\[5\]), have beenmade to include more sophisticated phrasalrepresentation i order to improve retrieval results.They have not worked as well as content terms orgenerally been inconclusive.We also investigated phrasal evidence for retrieval,but only to the extent hat it is used to refine results thathave been obtained via term level retrieval.
Only longqueries are considered since queries with too fewphrases would not provide sufficient evidence to workwith.
Specifically, we use phrasal evidence to re-ranka retrieved document list so as to promote morerelevant documents earlier in the list.
This could leadto higher density of true relevant documents in the 1 ststage retrieval, thereby improving 'pseudo-feedback'for the 2 nd stage downstream.
The 2 "d stage retrievallist could similarly be re-ranked to return bettereffectiveness a well.A query is processed into variable length nounphrases using a POS-tagger from Mitre and simplebracketing.
(We have also experimented with the BBNtagger before).
Given a retrieved ocument, each nounphrase concept of the query is then matched within upto a 3-sentence context anywhere in the document.When there are matches of two or more terms,appropriate weights are noted for this phrase and thesentence counted.
In addition, the amount of coverageof all the query phrases by the document is also afactor by which the original RSV of a document isboosted.
However, not all documents have their RSVmodified.
They need to pass a threshold for coverage.After many experiments for the TREC 5 and 6 longquery environments, the attempt was moderatelysuccessful as shown in Table 2.
For TREC5, animprovement in AvPre of 4% (.273 vs. .262) wasobtained, but in TREC6 only about 1% (.308 vs..305).132RRAvPreP@lOR.PreRRAvPreP@IOR.Pre(- Phrase -->1 st 2 nd 2 nd Re- Rerank Re-Retr Retr Retr rank A; then rankEnrich C 2 nd C'Retr(A) (B) (C) (O) (C') (D')~TREC5 50 Long Queries -->2463 3077 3034 3049 3052 3072.220 .253 .262 .265 .270 .273.404 .414 .438 .440 .446 .444.258 .277 .292 .292 .295 .296~TREC6 50 Long Queries2537 2947 3043 3064 3074 3088.237 .264 .305 .310 .304 .308.402 .452 .492 .498 .488 .490.278 .296 .326 .332 .327 .331Table2: Phrase Level Re-ranking ResultsMore studies need to be done to confirm its utility.Also shown in Table 2 is 2 na stage retrieval without andwith collection enrichment (columns B and C).
It isseen that this strategy works for long queries too.4.3 Top ica l  Concept  Leve l  Ev idenceWe have also investigated re-ranking of term levelresults based on clustering of the retrieval output.
Theidea is that it is often the case documents are rankedhigh by matching a query with terms that are related todifferent unwanted sub-topics or have different sensesfrom those used in the query.
Examples of the latterare 'bank', 'deposit' in the money sense, or their riversense.
Other terms may disambiguate he true sense ina document, but they may not be present or sufficientlymatched to the query.
Assuming there are sufficientnumber of retrieved ocuments using the terms in theirdifferent senses or for different sub-topics, one couldseparate them into groups by clustering the list.
Eachgroup will be characterized by a profile consisting ofterms with the highest occurrence frequency withineach group.
The query can now be matched with theprofiles as if they were documents, and the highestranked profile group would be promoted in ranking.Because cluster profiles would be important for aquery to pick the groups correctly, we haveimplemented a clustering algorithm that emphasizes onprofile forming rather than the more commonsimilarity-matrix based methods uch as the single-linkor average-link.
It is based on the iterative clusteringapproach of \[6,7\].
Each sub-document of a (100) top-ranked retrieval list, if not too long or too short, is usedas a seed to form a cluster by picking highly similardocuments that are not yet clustered.
The profile fromthe resulting group is further iterated until there is noor little change in the profile.
Each unclustered sub-document is tested as a seed to form a group, but manyfailed because fairly stringent conditions need to besatisfied.
After the process, there often would be leftwith sub-documents that belong to no clusters.
Theyare lumped together as 'miscellaneous' and has itsprofile formed.
In a number of queries, this'miscellaneous' cluster actually contain the mostrelevant documents.
This is the case because there isnot sufficient relevant documents to satisfy the groupforming criteria, or that their usage of terms are toodiverse and non-overlapping.So far the attempt has not been successful.
Severaldifficulties are noted: the clustering algorithmsometimes does not work well in separating relevantand irrelevant documents into different clusters; oftenthe query may not pick the right cluster to re-rank; andeven if the right cluster has been picked, the relevantdocuments may not rank sufficiently high within thecluster so that a lower AvPre measure may result.
Theinvestigation is still ongoing.5.
CHINESE AD-HOC RETRIEVALOur research continues the work of otherinvestigators on Chinese IR during Tipster l&2 (e.g.\[8\]).
We have augmented our PIRCS system to handlethe 2-byte encoding of Chinese characters according tothe GB2312 convention.
During processing, oursystem can handle both English and Chinese presentsimultaneously in documents and queries.5.1 Word  Segmentat ionA major difference of Chinese writing from Englishis that a Chinese sentence (which can usually berecognized by a punctuation ending) consists of acontinuous tring of characters and there is no white-space to delimit words.
Words can be one, two or morecharacters long.
At the time, we believed that wordsegmentation is important for effective Chinese IR.Since efficient word segmentation software for largecollections were not available, we relied on anapproximate short-word segmenter that was developedby ourselves in house (Queens segmenter \[9\]).Because the segmenter may not be sufficientlyaccurate, we-actually use characters in addition toshort-words for both query and documentrepresentation.
Earlier work has used wordsegmentation on queries only and rely on characterrepresentation for documents with operators to133combine characters for matching query words \[8\].The blind Chinese retrieval results in both TREC 5and 6 showed that our short-word plus characterindexing method works very well, since we havereturned the best automatic retrieval evaluations forboth years \[10,11\].
It also demonstrates that thePIRCS retrieval model can handle both English andChinese languages equally good.
After the blindTREC5 experiment, we further optimize parameters inPIRCS such as sub-document size, number ofdocuments and number of terms to use for 2 nd stageretrieval to obtain better esults \[12\] as shown in Table3.It can be seen that two-stage retrieval is good forboth English and Chinese, leading to improvements inAvPre of some 15% to 31% (.452 vs..392 and .384 vs..293) over initial 1 st stage retrieval.
Moreover, longqueries perform better than short ones as in English,between 17% and 22% (.452 vs. .384 and .603 vs..476).
These Chinese queries return surprisingly goodresults even though the segmentation is approximate.
Itis not clear if the language characteristics it elf may bea factor contributing to this.5.2 Compar ing  SegmentersWord segmentation is a big issue for Chinese sincelinguistics-strong applications uch as POS tagging,sentence parsing, machine translation, text to voice,etc.
are all dependent on words being accuratelyidentified to do well.
It would therefore be interestingto see if better word segmentation could lead to moreaccurate retrieval.RRAvPreP@10R.PreTREC5<- 28 Long Queries @~ 28 Short Queries "->1st Stage 2 nd 1 st Stage 2 ndStage Stage1944 2015 1615 1707.392 .452 .293 .384.546 .600 .389 .511.403 .452 .316 .389RRAvPreP@10R.PreTREC626 Long Queries @~26 Short Queries@1st Stage 2 nd I st Stage 2 ndStage Stage2738 2791 2277 2547.551 .603 .376 .476.808 .869 .615 .712.532 .567 .401 .463Table3:1 st and 2 "a Stage Chinese Retrieval ResultsWe have done manual analysis of our approximatesegmenter for correctness using the 54 TREC 5 & 6topics and concluded that its recall and precisionmeasures for segmenting sentences into short-wordsare about mid to high 80%.
These figures areapproximate because ven native speakers ometimesdisagree on the correct segmentation.
We have alsoanalyzed a segmenter f om UMASS \[13\] that is basedon a unigram model.
It can be trained from acollection that has been segmented based on a lexiconlist.
It segments a sentence by evaluating possiblechoices and selecting the one with the highestprobability of the trained model.
Our opinion is that itsrecall and precision values vary between about 90% tolow-90%, approximately 5% better than ours.
We usedboth segmenters to investigate the Chinese collectionand did retrieval using our PIRCS system under thesame parameter settings.
The result is presented inTable 4 below.
In this table, TREC5 precision valuestook account of larger lexicons (Section 5.3) and arebetter than those in Table 3.It is a bit surprising to see that results using the twosegmenters are very similar.
It appears that bettersegmentation may not mean better retrieval.
It ispossible that these two segmenters are not sufficientlydifferent o reflect any significant changes in results.
Avery high quality segmenter of 95% or higheraccuracy may tell a different story.5.3 Lex icon  S ize  E f fec tsWe made further studies of retrieval using ourapproximate segmenter to see how it might depend onthe lexicon used.
Our segmentation procedure dependson some simple, approximate language usage rules aswell as an initial lexicon list.
I f  a string of Chinesecharacters i not found on the lexicon, the rules operateTREC5<- 28 Long Queries @~- 28 Short Queries --->Queens UMASS Queens UMASSRR 2059 2070 1972 1991AvPre .467 .460 .417 .414P@10 .625 .589 .554 .561R.Pre .471 .453 .413 .412RRAvPreP@IOR.PreTREC6~.- 26 Long Queries @~-26 Short Queries--~Queens UMASS Queens UMASS2791 2761 2547 2488.603 .587 .476 .491.869 .850 .712 .750.567 .557 .463 .476Table4: Comparing Queens & UMASS Segmenters134to segment he string into short-words, thereby alsodiscovering unknown words.
Our initial lexicon L0 ismanually prepared and about 2K in size, minusculecompared to lists used by.
other investigators forsegmentation purposes.
By bootstrapping, a largerlexicon list L01 (about 15K) was derivedautomatically, and it can be used in place of the initiallexicon list for a more refined segmentation.If a larger initial lexicon list is used, there should bemore matching between a document string and thelexicon entries, the approximate rules would be usedless often and the resultant segmentation could be moreaccurate.
This would also be true for the derivedlexicon.
Better segmentation might also affect retrievalfavorably.We have additionally prepared a much larger initiallexicon list L1 (-27K) based on the association list inthe Cxterm software.
Together with the derivedlexicon L l l  (43K), we have studied the effects ofusing these four lexicons for segmentation andretrieval.
The results are shown in Table 5.
Weobserve that larger lexicon list can lead toincrementally better AvPre values (.463 vs .455 forlong queries and .409 vs .398 for short), but the rate ofincrease is very slow.
The initial 2K lexicon givessurprisingly good results.TREC5<- 28 Long Queries @~- 28 Short QueriesL0 L 11 L0 L 11RR 2059 2061 1958 1975AvPre .455 .463 .398 .409P@10 .596 .604 .534 .564R.Pre .455 .461 .403 .398Table5: Lexicon Size Effects on Chinese Retrieval5.4 S topword  EffectsStopwords are function words that do not carrymuch content by themselves, and are usually removedbased on a compiled stopword list to improve precisionand efficiency.
In addition, high frequency terms in acollection, which we call statistical stopwords, are alsoremoved because they are too widespread.
On theother hand, stopword removal always carry the riskthat one might delete some words that might be crucialfor particular queries or documents but in general notvery useful.
Examples (in English) are words like'hope' in 'Hope Project' \[9\], or 'begin' in 'PrimeMinister Begin'.
They can normally be regarded as notcontent-bearing, but in the examples given theybecome crucial.
Removing them will adversely affectresults.
Experiments with and without stopwordremoval (from a list) however shows that retrievalresults are minimally affected.
Chinese IR seems totolerate noisy indexing well.
The lesson is not to useany stopword list at all else one might run into perils asdiscussed.
Statistical stopwords are still removed.5.5 B igram Representat ionWe have further experimented with using simplerrepresentation methods uch as single characters andbigrams (consecutive overlapping two character) forretrieval.
Bigram representation does not need anysegmentation or linguistic rules, but often over-generates a large number of indexing terms that are notmeaningful to humans.
Character indexing is evensimpler, but they are highly ambiguous ince there areonly 6763 distinct characters in the GB2312 scheme.Surprisingly results with single characters are good,though not competitive; but bigram results can rivalthose of short-words when the queries are long.
Thishas important ramifications ince it means .that foreffective Chinese IR, one need not worry about whichsegmentation method to use.
(More intensive linguisticprocessing of course still requires accuratesegmentation.)
For large-scale collections, bigrramsegmentation is also more efficient ime-wise, althoughit is more expensive space-wise.
Table 6 showsexamples of retrieval measures using character andbigram representation.TREC528 Long Queries @~ 28 Short QueriesChar Bigram Char BigramRR 2007 2128 1757 1971AvPre .381 .457 .318 .372P@10 .539 .618 .421 .479R.Pre .403 .459 .351 .382TREC6~- 26 Long Queries -.->~- 26 Short QueriesChar Bigram Char BigramRR 2612 2735 2304 2342AvPre .512 .574 .432 .459P@I0 .785 .827 .723 .658R.Pre .507 .547 .433 .460Table6: Character and Bigram Retrieval Results5.6 Combin ing  Representat ions135Since short-word with character and bigramrepresentations separately returns comparable goodresults, this leads us to investigate whether they canperhaps reinforce each other.
Short-words provideeffective term matching between a query and adocument, but one might have wrong segmentations.Bigrams however are exhaustive and can remedy thesituation.
Given a collection, we index it both ways.For each query we also index it both ways andperform separate retrievals.
Their retrieval ists arethen combined based on the RSV of each document ias follows (with ct=l/2):RSVi = tx*RSVil + (1- ot)*RSVi2The result, shown in Table 7 as 'sw.c+bi' column, wasa further improvement of about 2 to 4% compared withthe best of the two base precision without combinationfor both short and long queries.
The price to pay is thedoubling of time and space.
If  for some applicationsthe last bit of effectiveness is important, his is a viableapproach.
Moreover, this strategy could be realized byhaving both retrievals performed in parallel on separatehardware, thus without affecting the time of retrievaltoo much.Included in Table 7 as the 'bi.c' column is the resultof adding characters to bigram indexing, just likeadding characters to short-words.
Compared to Table6, it is seen that this is also useful in 3 out of 4 cases,varying from -0.7% (.454 vs.0.457) to +13% (0.489vs.
.432) changes in AvPre for bigram results.Characters are highly ambiguous as indexing terms butthere are also Chinese words that are truly singlecharacter, and using bigrams only would not lead tocorrect erm matching.RRAvPreP@10R.PreTREC5(- 28 Long Queries -'-~-28 Short Queries ?--)bi.c sw.c+bi bi.c sw.c+bi2126 2111 1981 1985.454 .471 .387 .425.600 .621 .511 .539.456 .468 .405 .423RRAvPreP@IOR.PreTREC6<- 26 Long Queries ---~- 26 Short Queries --)bi.c sw.c+bi bi.c sw.c+bi2806 2784 2521 2611.627 .633 .489 .514.858 .869 .739 .750.575 .582 .482 .496Table7: Combining Representations for Retrieval5.7 Co l lec t ion  Enr ichment  fo r  Ch inese  IRIn Section 4.1, we observed that collectionenrichment is an effective strategy to improve Englishad-hoc retrieval, especially for short queries.
Here, westudy if this is also true for Chinese.The TREC5 Chinese collection came from twosources: 24,988 documents from XinHua NewsAgency (xh) and 139,801 from Peoples' Dailynewspaper (pd).
In PIRCS, they were segmented intosub-documents of 38,287 and 193,240 itemsrespectively.
We use the combined TREC5 and 6queries numbering 54, and do retrieval with the xhcollection as the target but enriched with pd, and viceversa.
Some queries do not have any relevants in oneof the sub-collections and the actual number of queriesfor evaluation is less.
This is done for the both longand short (title only) versions of the queries.
Resultsare tabulated in Table 8.It is seen that, except for long queries retrieving onpd and enriched with xh where the AvPre practicallyremains unchanged (.499 vs..500), the other caseshave improvements of between 3 to 4% over thestandard 2 nd retrieval without enrichment.
The latteralready has quite high effectiveness in these cases.Thus, we may say that collection enrichment alsoworks in Chinese.RRAvPreP@10R.Pre<-Target: xh (enriched by pd)52 Long Queries "-->~ 52 Short Queries1st 2nd 2nd 1st 2nd 2ndRetr Retr Retr Retr Retr Retrenrich enrich1685 1704 1706 1497 1586 1592.472 .533 .550 .384 .445 .462.498 .575 .585 .423 .494 .504.468 .508 .515 .399 .430 .443RRAvPreP@10R.PreTarget: pd (enriched by xh)~'- 53 Long Queries @~- 53 Short Queries --)1st 2nd 2nd 1st 2nd 2ndRetr Retr Retr Retr Retr Retrenrich enrich3174 3264 3269 2763 3066 3052.443 .500 .499 .325 .404 .416.615 .664 .677 .468 .542 .581.447 .485 .486 .345 .413 .420Table 8: Chinese Collection Enrichment Results1366.
CONCLUSIONA 2-stage retrieval strategy with pseudo-feedbackoften returns better ad-hoc results than 1-stage alone.We have further investigated term, phrasal and topicalconcept level evidence methods for improving retrievalaccuracy in this situation.
We showed that five termlevel methods together are effective for enhancing ad-hoc short query results some 20 to 40% for TREC5 &6 experiments.
A particularly useful technique iscollection enrichment, which simply adds domain-related external collections to a target collection to helpimprove 2 ?d stage retrieval downstream.
It bringssubstantial improvements in many cases and does nothurt much in others.
It works for long and shortqueries in both English and Chinese IR.With long queries we showed that using linguisticphrases to match within document windows as furtherevidence to re-rank retrieval output can lead to somesmall improvements.
We also studied re-ranking ofoutput documents based on topical concept levelevidence using document clustering, but the effort hasso far not been successful.Contrary to expectations, word segmentation is notcrucial for Chinese IR.
Simple bigrams or short-wordwith character indexing can produce very good results.A manual stoplist is also unnecessary; one only needsto screen out high frequency statistical stopwords.Best results are obtained by combining retrievals usingmultiple representations.For the future, it will be interesting to see if phrasalevidence can be employed for Chinese IR, and to studyhow to improve its usefulness.
Topical clustering forenhancing retrieval, display and for data reduction ingeneral are also important issues for large scale IR.ACKNOWLEDGMENTSThis research is partially supported by a contract fromthe U.S. Department of Defense MDA904-96-C-1481.I like to express my appreciation toR.
Weischedel foruse of the BBN POS tagger; L. Hirschman for theMitre POS tagger and W.B.
Croft for the UMASSChinese segmenter.REFERENCES\[1\] Kwok, K.L.
"A network approach to probabilisticinformation retrieval".
ACM Transactions on OfficeInformation System, 13:324-353, July 1995.\[2\] Voorhees, E. & Harman, D. "Overview of the SixthText REtrieval Conference (TREC 6).
In: The SixthText REtrieval Conference (TREC-6), E. Voorhees& D.K.
Harman (eds.)
NIST Special Publication500-240, Gaithersburg, MD 20899. pp.l-24, 1998.\[3\] Kwok, K.L.
& M. Chan.
"Improving two-stage ad-hoc retrieval for short queries."
in Proc.
21st Ann.Intl.
ACM SIGIR Conf.
on R&D in IR.
pp.250-6,1998.\[4\] Kwok, K.L.
"A new method of weighting queryterms for ad-hoc retrieval".
Proc.
19th Annual Intl.ACM SIGIR Conf.
on R&D in IR.
ETH, Zurich,Aug.
18-22, 96. pp.187-195, 1996.\[5\] Strzalkowski, T. "Natural language informationretrieval: Tipster-2 final report".
Proc.
of TipsterText Program (Phase 2).
pp.
143-8, Sept., 1996.\[6\] Rocchio, J.J. Jr. "Document retrieval systems -optimization and evaluation" Ph.D. thesis, HarvardUniversity \[1966\].\[7\] Schiminovich, S. "Automatic classification andretrieval of documents by means of a bibliographicpattern discovery algorithm".
Info.
Stor.
& Retr.6:417-435, 1971.\[8\] Boisen, S., Crystal, M., Petersen, E., Weischedel,R., Broglio, J., Callan, J., Croft, B., Hand, T.,Keenan, T., Okurowski, M. "Chinese informationextraction & retrieval".
Proc.
of Tipster TextProgram (Phase 2).
pp.
109-119, Sept., 1996.\[9\] Kwok, K.L.
"Lexicon effects on Chinese informationretrieval".
Proc.
of 2nd Conf.
on EmpiricalMethods in NLP.
Cardie, C. & Weischedel, R.(eds).
Brown Univ., Aug. 1-2, 1997. pp.
141-148.\[10\] Kwok, K.L.
& Grunfeld, L. "TREC-5 English andChinese retrieval experiments using PIRCS".
In:Information Technology: The Fifth Text REtrievalConference (TREC-5), E.M. Voorhees & D.K.Harman, eds.
NIST Special Publication 500-238,Gaithersburg, MD 20899. pp.
133-142, 1997.\[11\] Kwok, K.L., Grunfeld, L. & Xu, J.H.
"TREC-6English and Chinese retrieval experiments usingPIRCS".
In: The Sixth Text REtrieval Conference(TREC-6), E. Voorhees & D.K.
Harman, eds.NIST Special Publication 500-240, Gaithersburg,MD 20899. pp.207-214 1998.\[12\] Kwok, K.L.
"Comparing representations forChinese information retrieval".
Proc.
20th AnnualIntl.
ACM SIGIR Conf.
on R&D in IR.Philadelphia, Ju127-31, 1997. pp.34-41.\[13\] Ponte, J.
& Croft, W.B.
"Useg: a retargetableword segmentation procedure for informationretrieval".
In: Symposium on Document Analysis& Information Retrieval (SDAIR 1996)137
