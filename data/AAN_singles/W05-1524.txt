Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 194?195,Vancouver, October 2005. c?2005 Association for Computational LinguisticsTFLEX: Speeding up Deep Parsing with Strategic PruningMyroslava O. DzikovskaHuman Communication Research CentreUniversity of EdinburghEdinburgh, EH8 9LW, UKmdzikovs@inf.ed.ac.ukCarolyn P. RoseCarnegie Mellon UniversityLanguage Technologies InstitutePittsburgh PA 15213, USAcprose@cs.cmu.edu1 IntroductionThis paper presents a method for speeding up adeep parser through backbone extraction and prun-ing based on CFG ambiguity packing.1 The TRIPSgrammar is a wide-coverage grammar for deep nat-ural language understanding in dialogue, utilized in6 different application domains, and with high cov-erage and sentence-level accuracy on human-humantask-oriented dialogue corpora (Dzikovska, 2004).The TRIPS parser uses a best-first beam search al-gorithm and a chart size limit, both of which are aform of pruning focused on finding an n-best list ofinterpretations.
However, for longer sentences lim-iting the chart size results in failed parses, while in-creasing the chart size limits significantly impactsthe parsing speed.It is possible to speed up parsing by implement-ing faster unification algorithms, but this requiresconsiderable implementation effort.
Instead, we de-veloped a new parser, TFLEX, which uses a sim-pler technique to address efficiency issues.
TFLEXcombines the TRIPS grammar with the fast parsingtechnologies implemented in the LCFLEX parser(Rose?
and Lavie, 2001).
LCFLEX is an all-pathsparser which uses left-corner prediction and ambi-guity packing, and which was shown to be efficienton other unification augmented context-free gram-mars.
We describe a way to transfer the TRIPSgrammar to LCFLEX, and a pruning method whichachieves significant improvements in both speed andcoverage compared to the original TRIPS parser.1This material is based on work supported by grants fromthe Office of Naval Research under numbers N000140510048and N000140510043.2 TFLEXTo use the TRIPS grammar in LCFLEX we first ex-tracted a CFG backbone from the TRIPS grammar,with CFG non-terminals corresponding directly toTRIPS constituent categories.
To each CFG rulewe attach a corresponding TRIPS rule.
Whenevera CFG rule completes, a TRIPS unification functionis called to do all the unification operations associ-ated with the TRIPS rule.
If the unification fails, theconstituent built by the CFG is cancelled.The TFLEX pruning algorithm uses ambiguitypacking to provide good pruning points.
For exam-ple, in the sentence ?we have a heart attack victimat marketplace mall?
the phrase ?a heart attack vic-tim?
has two interpretations depending on whether?heart?
modifies ?attack?
or ?attack victim?.
Theseinterpretations will be ambiguity packed in the CFGstructure, which offers an opportunity to make prun-ing more strategic by focusing specifically on com-peting interpretations for the same utterance span.For any constituent where ambiguity-packed non-head daughters differ only in local features, weprune the interpretations coming from them to aspecified prune beam width based on their TRIPSscores.
In the example above, pruning will happenat the point of making a VP ?have a heart attack vic-tim?.
The NP will be ambiguity packed, and we willprune alternative VP interpretations resulting fromcombining the same sense of the verb ?have?
anddifferent interpretations of the NP.This approach works better than the originalTRIPS best-first algorithm, because for long sen-tence the TRIPS chart contains a large number194of similar constituents, and the parser frequentlyreaches the chart size limit before finding the correctconstituent to use.
Ambiguity packing in TFLEXhelps chose the best constituents to prune by prun-ing competing interpretations which cover the samespan and have the same non-local features, thusmaking it less likely that a constituent essential forbuilding a parse will be pruned.3 EvaluationOur evaluation data is an excerpt from the Monroecorpus that has been used in previous TRIPS re-search on parsing speed and accuracy (Swift et al,2004).
The test contained 1042 utterances, from 1to 45 words in length (mean 5.38 words/utt, st. dev.5.7 words/utt).
Using a hold-out set, we determinedthat a beam width of 3 was an optimal setting forTFLEX.
We then compared TFLEX at beam width3 to the TRIPS parser with chart size limits of 1500,5000, and 10000.
As our evaluation metrics we re-port are average parse time per sentence and proba-bility of finding at least one parse, the latter being ameasure approximating parsing accuracy.The results are presented in Figure 1.
We groupedsentences into equivalence classes based on lengthwith a 5-word increment.
On sentences greaterthan 10 words long, TFLEX is significantly morelikely to produce a parse than any of the TRIPSparsers (evaluated using a binary logistic regression,p < .001).
Moreover, for sentences greater than20 words long, no form of TRIPS parser returneda complete parse.
TFLEX is significantly fasterthan TRIPS-10000, statistically indistinguishable interms of parse time from TRIPS-5000, and signifi-cantly slower than TRIPS-1500 (p < .001).Thus, TFLEX presents a superior balance of cov-erage and efficiency especially for long sentences(10 words or more) since for these sentences it issignificantly more likely to find a parse than any ver-sion of TRIPS, even a version where the chart size isexpanded to an extent that it becomes significantlyslower (i.e., TRIPS-10000).4 ConclusionsIn this paper, we described a combination of effi-cient parsing techniques to improve parsing speedand coverage with the TRIPS deep parsing grammar.Figure 1: Parse times and probability of getting aparse depending on (aggregated) sentence lengths.5 denotes sentences with 5 or fewer words, 25 sen-tences with more than 20 words.The TFLEX system uses an all-paths left-cornerparsing from the LCFLEX parser, made tractableby a pruning algorithm based on ambiguity packingand local features, generalizable to other unificationgrammars.
Our pruning algorithm provides a bet-ter efficiency-coverage balance than best-first pars-ing with chart limits as utilised by the TRIPS parser.ReferencesM.
O. Dzikovska.
2004.
A Practical Semantic Represen-tation For Natural Language Parsing.
Ph.D. thesis,University of Rochester.C.
P. Rose?
and A. Lavie.
2001.
Balancing robustnessand efficiency in unification-augmented context-freeparsers for large practical applications.
In J.C. Junquaand G Van Noord, editors, Robustness in Languageand Speech Technology.
Kluwer Academic Press.M.
Swift, J. Allen, and D. Gildea.
2004.
Skeletons inthe parser: Using a shallow parser to improve deepparsing.
In Proceedings of COLING-04.J.
Tetreault, M. Swift, P. Prithviraj, M. Dzikovska, and J.Allen.
2004.
Discourse annotation in the monroe cor-pus.
In ACL-04 workshop on Discourse Annotation.195
