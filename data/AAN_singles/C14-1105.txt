Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1114?1123, Dublin, Ireland, August 23-29 2014.Fast Domain Adaptation of SMT models without in-Domain Parallel DataPrashant Mathur?Fondazione Bruno KeselerPovo - 38100 (IT)first@fbk.euSriram VenkatapathyXerox Research Center EuropeMeylan (FR)first.last@xrce.xerox.comNicola Cancedda?Microsoft, London (UK)first.last@gmail.comAbstractWe address a challenging problem frequently faced by MT service providers: creating a domain-specific system based on a purely source-monolingual sample of text from the domain.
We solvethis problem by introducing methods for domain adaptation requiring no in-domain parallel data.Our approach yields results comparable to state-of-the-art systems optimized on an in-domainparallel set with a drop of as little as 0.5 BLEU points across 4 domains.1 IntroductionWe consider the problem of creating the best possible statistical machine translation (SMT) system fora specific domain when no parallel sample or training data from such domain is available.
We assumethat we have access to a collection of phrase tables (PT) and other models independently created fromnow unavailable corpora, and we receive a monolingual source language sample from a text source wewould like to optimize for.For a MT provider to deliver a SMT system tailored to a customer?s domain, a sample dataset isrequested.
In most cases, the customer is able to provide an in-domain mono-lingual sample from hisoperations.
However, it is generally not feasible for the customer to provide the translations as wellbecause the customer has to hire professional translators to do that.
In such a scenario, the translations hasto be generated by MT service provider itself by hiring human translators thus requiring an investmentupfront.
The methods proposed in this paper aim to avoid that by building a good quality pilot SMTsystem leveraging only sample mono-lingual source corpus, and previously trained library of models.This in turn postpones the task of generating in-domain parallel data to a later date when there is acommitment by the customer.Unavailability of the raw parallel data could derive from a trading model where data owners shareintermediate-level resources like PTs, Reordering Models (RM) and Language Models (LM), but cannot, or do not want to, share the textual data such resources were derived from.
This particular scenariohas been explained in (Cancedda, 2012).This scenario is similar to the multi-model framework studied in (Sennrich et al., 2013), with theadditional challenge that no parallel development set is available.
We build on the linear mixture modelcombination of the cited work, extending it to our more challenging environment:1.
We propose a new measure derived from the popular BLEU score (Papineni et al., 2002) to assessthe fitness of a PT to cope with a given monolingual sample S. This measure is computed fromn-gram statistics that can be easily extracted from a PT.2.
We propose a new method for tuning the parameters of a log linear model that does not requirean in-domain parallel development set, and yet achieves results very close to traditional tuning onparallel in-domain data.We present our proposed metric BLEU-PT and computation of multi-model in Section 2.
The pa-rameter estimation of log-linear parameters of the SMT system is described in Section 3.
We presentexperiments and results in Sections 4 and 6 respectively.
?Major part of the work was performed when the authors were in Xerox Research Center Europe.This work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/11142 Building Multi-ModelGiven a library of phrase tables, the goal of this step is to generate a domain adapted multi-model.
Thechallenging aspect in our scenario is the lack of in-domain parallel data, as well as absence of originalparallel corpora corresponding to the library of models.
This rules out the possibility of using metricssuch as cross-entropy (Sennrich, 2012b) or LM-perplexity for computing the mixing coefficients.
Wepresent our proposed metric in section 2.1, and interpolation of the phrase tables in section 2.2.2.1 BLEU-PTGiven a source corpus s, and a set of phrase tables {pt1, pt2,.
.
.
,ptn}, the goal is to measure the similarityof each of these tables with s. For measuring the similarity, we use BLEU-PT which is an adaptation ofthe popular BLEU score for measuring the similarity between a corpus and a phrase table.
The metricBLEU-PT is measured as described in Equation 1.BLEU-PT(PT, S) =(4?n=1match(n|pt, s)total(n|s))1/4(1)where match(n|pt, s) is the count of n-grams of order n in the source corpus s that exist in the sourceside of the phrase table pt.
total(n|s) is the number of n-grams of order n in the source corpus.2.2 Interpolating ModelsA state-of-the-art approach for building multi-models is through linear interpolation of component mod-els, exemplified in Equation 2 for the case of the forward conditional phrase translation model.hphr(s, t) = logN?j=1?jPphr,j(t|s) (2)Various approaches have been suggested for computing the coefficients ?
of the interpolated model, themost recent being perplexity minimization described in (Sennrich, 2012b), where each translation modelfeature is optimized separately on the parallel development set.
Our work is set in a scenario where noparallel development set is available for optimizing the interpolation coefficients.
We have also observedthat perplexity minimization is computationally intensive, requires aligned parallel development set, andthe optimization time increases rapidly with increasing number of component models (for details, seeSection 4.2).We propose a simple approach for computation of the mixing coefficients that relies on the similarityof each model with respect to the test set.
The mixing coefficients are obtained by normalizing similarityvalues.
The similarity between a model (phrase table) and a corpus is computed using the BLEU-PTmetric proposed in the previous section.
Another similarity metric that can be used is LM Perplexity.However, in the current scenario we do not have resources (training data) to build a source side LM forcomputing the perplexity.We empirically compare our method for computing mixing coefficients with the the perplexity min-imization method.
We also experiment with applying the mixing coefficients obtained by using ourmethod for mixing features of a reordering and language model.3 Parameter EstimationThe overall quality of translation is strongly impacted by how optimized the weights of the log-linearcombination of various translation features are for a domain of interest.
MERT (Och, 2003) and MIRA(Watanabe et al., 2007) are popular solution to compute an optimal weight vector by minimizing the erroron a held-out parallel development set.
BLEU and its approximations are commonly used error metrics.In this paper we assume lack of a parallel development set, therefore the above methods cannot be used.Pecina et.
al.
(2012) showed that the optimized log-linear weight vector1of a SMT system does notdepend as much on the actual domain of the development set (on which the system was optimized), as1Not to be confused with the mixing coefficients in a linear combination of model components.1115on how ?distant?
the relevant domain is from the domain of the training corpus used to build the SMTmodels.
This is an important finding.
It means that the weight vector can be modeled as a function of thedistance/similarity between the in-domain development set and the model built from the training set.
Inthis work, we learn this function from examples of previous parameter optimizations, using our BLEU-PT as a similarity metric.
Once we have retrieved the most relevant PTs (translation and reorderingmodels) from our library, and we have linearly interpolated them using normalized BLEU-PT, we usethe learned model to estimate the optimal value of the log-linear weights, instead of optimizing them.In order to learn this mapping, we create a dataset of examples (pairs of the form <BLEU-PT, log-linear weight vector>, where weight vectors are normalized to ensure comparability across models) byperforming repeated optimizations for out-domain models on a number of parallel development sets (seesection 4 for more details of this data) using a traditional optimization method (MIRA in this work).Based on this dataset, the function of our interest can therefore be learnt using a supervised approach.We explore two parametric methods and a non-parametric method.
We present these in Section 3.1, and3.2 respectively.
For a mono-lingual source in a new domain, the BLEU-PT can be computed, and thenmapped to the appropriate weight vector using the methods presented below.3.1 Parametric MethodsWe considered two distinct parametric methods for estimating the mapping from model/corpus similarityinto weight vectors.
The first one makes the assumption that parameters can be estimated independentlyof one another, given the similarity, whereas the second tries to leverage known covariance betweendistinct parameters in the vector.3.1.1 Linear RegressionMotivated by initial experiments highlighting strong correlation between BLEU-PT and optimal featureweights (see Section 5.1 below), we assumed here a simple linear relation of the form:?
?i= WiX + bi(3)where ?
?iis the optimal log-linear weight for feature i, X is the feature vector (BLEU-PT vector), Wiand biare coefficients to be estimated.
While a drastic assumption, this has the advantage of limitingthe risk of overfitting in a situation like ours where there is only relatively few datapoints to learn from.We estimate aiand biby simple least squares regression.
Once these are available for all features, wecan predict the log linear weights of any model given its BLEU-PT similarity to a monolingual sourcesample using Eq.
3.3.1.2 Multi-Task learningOptimal log-linear parameters might not be fully independent given BLEU-PT, especially since it isknown that model features can be highly correlated.
To account for correlation between parameterweights, we explore the use of multi-task lasso2(Caruana, 1997) where several functions correspondingto each parameter are learned jointly considering the correlation between their values observed in thetraining data.
Multi-task lasso consists of a least square loss model trained along with a regularizer andthe objective is to minimize the following:argminw12N||X ?W ?
?||22+ ?||W ||21where; ||W ||21=M?j?
?iw2ij(4)Here, N is the number of training samples, X is the feature vector(BLEU-PT score vector) ?
is the labelvector(log linear weights).
||W ||21is the l21regularizer (Yang et al., 2011).
The problem of predictionof log linear weights is reduced to prediction of i interlinked tasks where each task has M features3.Coefficients are calculated using coordinate descent algorithm in Multi-Task lasso.
Once the coefficientsare calculated we use Eq.
3 to predict the log linear weights.2http://scikit-learn.org/3In our case we only have 1 feature i.e.
BLEU-PT score.11163.2 Non Parametric: Nearest NeighborFinally, instead of building a parametric predictor for log linear weights, we experimented with a simplenearest-neighbor approach:?
?i= ?i(Mj?)
(5)whereMjranges over the linearly interpolated phrase tables, and ?i(M) returns the stored optimal valuefor the ithlog-linear weight, and:j?= argminjmins?
(|BLEU-PT(M, s)?
BLEU-PT?
(Mj, s?
)|) (6)where s is the monolingual sample on which we want to calculate the BLEU-PT and s?ranges overthe source sides of our available parallel development sets.
In other words, a BLEU-PT of a model iscalculated on the source sample to be translated and the log-linear weight is chosen which correspondsto BLEU-PT?, where BLEU-PT?is a training data point closest to BLEU-PT.
This approach is close tothe cross-domain tuning of Pecina et.
al.
(2012).4 Experimental ProgramWe conducted a number of experiments for English-French language pair, comparing the methods pro-posed in the previous sections among one another and against state-of-the-art baselines and oracles.4.1 DatasetsIn this section, we present the datasets (EN-FR) that we have used for our experiments and the trainingdata that was created for the purpose of supervised learning.
We collected a set of 12 publicly availablecorpora and 1 proprietary corpus, statistics of datasets are provided in Table 1.Corpus Train Development TestCommoncrawl 78M 12.4K 12.6KECB 4.7M 13.9K 14KEMEA 13.8M 14K 15.7KEUconst 133K 8K 8.4KEuroparl 52.8M 13.5K 13.5KP1 5M 35K 14.5KKDE4 1.5M 12.8K 5.8KNews Comm.
4M 12.7K 65KOpenOffice 400K 5.4K 5.6KOpenSubs 156M 16K 15.7KPHP 314K 3.5K 4KTED 2.65M 21K 14.6KUN 1.92M 21K 21KTable 1: Statistics of parallel sets (# of source tokens)500 1000 1500 2000 2500 3000 3500 4000 4500Phrase Table size (MB)05001000150020002500SearchTime(seconds)Calculating time of Metrics v/s Size of Phrase Tablebleu-ptcross entropyFigure 1: BLEU-PT v/s Cross-EntropyCommoncrawl (CC) (Smith et al., 2013) and News Commentary (Bojar et al., 2013) corpora wereprovided in the 2013 shared translation task organized with workshop on machine translation.
TED talksdata was released as a part of IWSLT evaluation task (Cettolo et al., 2012).
ECB, EMEA, EUconst,OpenOffice, OpenSubs 2011, PHP and UN corpora are provided as a part of OPUS parallel corpora(Tiedemann, 2012).
The parallel corpora from OPUS were randomly split into training, developmentand testsets.
Commoncrawl, News Commentary and TED datasets were used as they were provided inthe evaluation task.Out of 13 different domain datasets we selected 4 datasets randomly: Commoncrawl, KDE4, TED andUN (in bold in Table 1), to test our methods.4.2 BLEU-PT v/s Cross-EntropyWe compared the overheads of calculating BLEU-PT and Cross-Entropy4.
We are interested in estimat-ing whether with increasing number of phrase tables the computation of both measures becomes slow ormemory intensive.4We used tmcombine.py script that comes along with the moses package to calculate the mixing coefficients.1117Another advantage of using BLEU-PT apart from fast retrieval is that we can index the phrase tablesusing wFSA based indexing (explanation of indexing the phrase tables is not in the scope of this paper)and store the FSTs in binarised format on disk.
When a source sample comes, we just load the indexedbinaries and calculate the BLEU-PT while this cannot be achieved when we want to calculate crossentropy because we have to do one pass over all the phrase tables in question.Experimental results depicted in Figure 1 shows that computation of BLEU-PT is fast (160 seconds)while computation of cross-entropy is slow (42 minutes) when we combine 12 phrase tables with totalsize of 4.2GB.4.3 Training data for supervised learning and testingAs mentioned earlier, for estimating the parameters we require a training data containing the tuples of<BLEU-PT, log-linear-weight>.
We perform parameter estimation on four of our datasets: Common-crawl, KDE4, TED and UN.
So, for obtaining evaluation results on say, UN, the rest of the resourcesare used for generating the training data.
Our experimental setup can be explained well using the Venndiagram shown in Figure 2.We set one of four domains as the test domain (in this case, UN) whose parallel set is not available to usand call it setup-UN.
The training data tuples obtained from the rest of the 12 datasets are used to estimateparameters for the UN domain.
From these 12 datasets we perform a round-robin experiment where oneby one each dataset is considered as in-domain and the rest as out-domain.
In-domain dataset provides thedevelopment set and the rest 11 out-domain models are linearly combined to build translation models.In figure 2, for example, the development set from the TED domain is taken as the development setof the multi-model build using the rest (i.e.
excluding TED and UN).
This multi-model is built by aweighted linear combination of the out-domain models (11 models).
The parameters of this multi-modelare tuned on the in-domain development set using MIRA.
Simultaneously, we also calculate the BLEU-PT of the linear interpolated model on the source side of the in-domain development set (i.e.
TED).This provides us the tuples of BLEU-PT and the log linear weights, which is our training data.
So, foursets of experiments are conducted (one each for four datasets considered for testing), and for each setof experiments, there are 12 training data points.
The final evaluation is done by measuring the BLEUscore obtained on each test set using the predicted parameter estimates.Reiterating, our optimizing method is fast, and hence, we are not not looking to learn the parametersapriori for all the domains based on a source side of the development set.
The goal is to do a fastadaptation by predicting the parameters using statistical models for every new test in a particular domaineven in the absence of a parallel development set.4.4 PredictionFor prediction of parameters for a new domain, the BLEU-PT of the sample source corpus (UN in ourexample) is measured with the multi-model built on all the models (all the rest of 12 datasets includingthe TED model) and then the supervised predictor is applied.
In our experiments, we test both parametricand non-parametric methods to estimate the parameters based on the training data obtained using the 12domains.TESTIn-domainUN......EMEAECBKDEPHPDEVIn-domainTEDFigure 2: Cross domain tuning setupFigure 3: Correlation of log linear weights with BLEU-PTwhen indomain sets set to UN and TED1118Domain Linear InterpolationSystem Train Dev Param.
Est.
TM(coeff.)
RM(coeff.)
LM(coeff.
)in-dom-train In In mira N.A N.A N.Amira-bleupt-tm-rm Out In mira 3 3 7mira-perp-tm-bleupt-rm Out In mira 3(Perp.
Min) 3 7mira-bleupt-tm-rm-perp-lm Out In mira 3 3 3(LM Perp.
Min.
)mira-bleupt-all Out In mira 3 3 3def-bleupt-all Out 7 def 3 3 3gen-reg-bleupt-all Out 7 regression 3 3 3gen-mtl-bleupt-all Out 7 multi-task 3 3 3gen-nn-bleupt-all Out 7 Near.Neigh.
3 3 3top5-reg-bleupt-all Out 7 regression 3 3 3top5-mtl-bleupt-all Out 7 multi-task 3 3 3top5-nn-bleupt-all Out 7 Near.Neigh.
3 3 3Table 2: System Description: Each system?s training domain and development set domain along with the optimizer/predictoris mentioned.
def-bleupt-all uses default weights from Moses decoder.
Near.Neigh.
shows that we used Nearest Neighborpredictor for optimizing weights.
7 represent log linear interpolation of models while 3 represents linear interpolation.
Themixing coefficients for linear interpolation are calculated by normalizing bleu-pt scores unless mentioned otherwise.5 Experiments and Results5.1 Correlation analysisBefore embarking in the actual regression task, we examined the correlation between the similarity values(BLEU-PT) and the various weights in the training data.
If there is good correlation between BLEU-PTand a particular parameter, then the linear regressor is expected to fit well and then predict an accurateparameter value for a new domain.
For computing the correlation, we use Pearson correlation coefficient(PCC).
Figure 3 shows the PCC between the feature weights and the BLEU-PT scores.
The tm?s are thetranslation model features, and rm?s are the reordering model features.We see that there is either a strong positive correlation or a strong negative correlation for most fea-tures in both the experimental setups shown in the figure 3.
This validates our hypothesis that optimalparameters for a new test domain can indeed be estimated with good reliability.
One can also observethat the correlation level also varies based on the mixture of training models.
For example, the correla-tion is much higher in the training data that excluded UN (setup-UN) than the one that excluded TED(setup-TED).In figure 3, one can also see that tm0 (forward phrase conditional probability) and tm2 (backwardphrase conditional probability) which are shown in previous work to be the two most important featuresamongst all SMT features (Lopez and Resnik, 2006) in terms of their impact on translation quality, havea high correlation in setup-UN.5.2 SystemsAll SMT systems were built using the Moses toolkit (Koehn et al., 2007).
To automatically align theparallel corpora we used MGIZA (Gao and Vogel, 2008).
Aligned training data in each domain wasthen used to create the corresponding component translation models and lexical reordering models.
Wecreated 5-gram language models for every domain using SRILM (Stolcke, 2002) with improved Kneser-Ney smoothing (Chen and Goodman, 1999) on the target side of the training parallel corpora.
Log linearweights for the systems were optimized using MIRA (Watanabe et al., 2007; Hasler et al., 2011) whichis provided in the Moses toolkit.
Performance of the systems are measured in terms of BLEU computedusing the MultEval script (mteval-v13.pl).We built one in-dom-train system where only in-domain training data is taken into account.
Thissystem shows the importance of in-domain training data in SMT (Haddow and Koehn, 2012).
Threeoracle systems are trained on out-domain training corpus and tuned on in-domain development data (inthis case there are four domains we chose to test on: UN, TED, CommonCrawl and KDE4), thus 4systems for each of the in-domain test sets.We build another set of SMT systems in which language models are combined by linear interpolation5.5Linear interpolation of 12 LMs result in one single large LM, thus, one weight.
So, a total of 14 weights have to beoptimized or predicted1119The systems using linear interpolated LM (mixing coefficients are normalized BLEU-PT scores) are def-bleupt-all, mira-bleupt-all, gen-reg-bleupt-all, gen-mtl-bleupt-all and gen-nn-bleupt-all.
We comparemira-bleupt-all with mira-bleupt-tm-rm-perp-lm where mixing coefficients for LM interpolation are cal-culated by standard LM perplexity minimization method over target side of development set.As mentioned earlier, ideally only a subset of all the models closer to the source sample should betaken into account for quick adaptation, so we select the top five domains related to the source sampleand interpolate the respective models and address them as top5-* systems.
Adding more domains wouldunnecesary increase the size of the model and add more noise.
Table 2 shows the configuration ofdifferent systems.
In the next section we compare the performances of these systems and report thefindings.6 Results and DiscussionTable 3 presents results of the systems that use an in-domain parallel data.
As expected, when an in-domain corpus is used both for training as well as for optimizing the log-linear parameters, the pefor-mance is much higher than those systems that do not use in-domain parallel corpus for training (Koehnand Schroeder, 2007).
We also observe that the use of normalized BLEU-PT for computing mixingcoefficients gives comparable performance to using Cross-Entropy.
The primary advantage in usingBLEU-PT is that it can be compute much faster than Cross-Entropy (as shown in Figure 1).
Evidently,normalized BLEU-PT scores as mixing coefficients performs at par with mixing coefficients retrieved bystandard perplexity minimization method (Bertoldi and Federico, 2009).
One can also use BLEU-PT forLM interpolation in cases where target side in-domain text is not available.System UN TED CC KDEin-dom-train 67.87 29.98 26.62 35.82mira-bleupt-tm-rm 44.14 31.20 17.43 24.25mira-perp-tm-bleupt-rm 43.56 31.36 17.54 24.72mira-bleupt-tm-rm-perp-lm 43.96 31.85 18.45 23.39mira-bleupt-all 43.66 32.04 18.44 23.09Table 3: Comparison of In-Domain system versus the estab-lished Oracles in different setups.System UN TED CC KDEgen-reg-bleupt-all 43.27 32.18 17.95 21.05gen-mtl-bleupt-all 43.35 32.61 18.26 20.67gen-nn-bleupt-all 42.73 31.04 18.24 21.85Table 4: Performance of generic systems (gen-*) in all se-tups.Table 4 illustrates the impact of phrase table retrieval on the performance of multi-model.
All thesystems presented in this table use BLEU-PT for computing mixing coefficients, while the weightsare computed using the three techniques that we explored in this paper.
We see that in case of re-gression, the phrase table retrieval also results in a better MT performance.
In the other two cases,the results are comparable.
It shows that retrieval helps in building smaller sized multi-models whilebeing more accurate on an average.
Phrase table retrieval, thus, becomes particularly useful when amulti-model needs to be built from a library of dozens of pre-trained phrase tables of various domains.15.51616.51717.51818.52  4  6  8  10  12BLEUscoreNumber of ModelsNearest NeighborRegressionMulti-Task LearningFigure 4: BLEU scores when top k models wereused to evaluate commoncrawl test set wherek ?
1..12.System UN TED CC KDEdef-bleupt-all 42.03 30.82 17.97 19.66mira-bleupt-all 43.66 32.04 18.44 23.09top5-reg-bleupt-all 43.39N32.31N18.10 21.54Ntop5-mtl-bleupt-all 43.56N32.60N18.14 20.91Ntop5-nn-bleupt-all 42.96N30.89M17.79 22.24NTable 5: Comparing the baseline system (def-bleupt-all)and Oracle (mira-bleupt-all) with domain specific multi-modelsystems trained on top5 domains.NandMdenotes significantlybetter results in comparison with def-bleupt-all system withp-value < 0.0001 and < 0.05 respectively.Table 5 compares our approach of computing log-linear weights (in the absence of in-domain develop-ment set) to the state-of-art weight optimization technique MIRA (which requires an in-domain devel-opment set).
As a baseline, we set default weights to all the parameters, which was shown to a strong1120baseline in (Pecina et al., 2012).
We see that the methods proposed by us perform significantly bet-ter than the default weights baseline (improvement of more than 1.5 BLEU score on an average across 4domains).
Among the three approaches for computing weights, the method that uses multi-task lasso per-forms best (except in setup-KDE where the non-parametric method performs best), along the expectedlines as multi-task lasso considers the correlation between various features.
In comparison to MIRA, ourmethods result in an average drop of as little as 0.5 BLEU points across 4 domains (see Table 5).Figure 4 shows BLEU score curve when we vary the k in top-k systems.
BLEU score curve is almosttangential zero when k is between 5 and 6 which essentially means that selection of k = 5 is a goodchoice.
For CommonCrawl test set, the top five domains used were Europarl, OpenSubs, NewsCom-mentary, TED and ECB.
This is a significant result which indicates that one can build a good system fora domain even in the absence of the parallel data in the domain of interest.7 Related WorkDomain adaptation in statistical machine translation has been widely studied and leveraged throughadding more training data (Koehn and Knight, 2001), filtering of out of domain training data (Axelrodet al., 2011; Koehn and Haddow, 2012), fillup technique (Bisazza et al., 2011), language model adap-tation by perplexity minimization over in-domain data (Bertoldi and Federico, 2009) and various otherapproaches.
However, all the above adaptation approaches require either parallel in-domain corpus ormonolingual in-domain target side corpus, thus, not applicable in our scenario.In this paper we studied mixture modelling of heterogeneous translation models which was first pro-posed in Foster et.
al.
(2007).
They showed various ways of computing mixing coefficients for linearinterpolation using several distance based metrics borrowed from information theory.
However, to cal-culate any such metrics it was required that one has an access to the source/target training corpus andsource/target development corpus.
Other noteable works in mixture modelling in SMT are (Civera andJuan, 2007; Razmara et al., 2012; Duan et al., 2010).More recently, Sennrich (2012b) designed an approach to calculate mixing coefficients by minimizingthe perplexity of translation models over an aligned development set for mixture modelling via linearinterpolation or by weighting the corpora.
Sennrich et.
al.
(2012a) clustered of a large heterogeneousdevelopment corpus and tuned a translation system on different clusters.
In the decoding phase eachsentence was assigned to a cluster and the translation system tuned on that cluster was used to translatethat sentence.
(Banerjee et al., 2010) build several domain specific translation systems, and trained a classifier toassign each incoming sentence to a domain and use the domain specific system to translate the corre-sponding sentence.
They assume that each sentence in test set belongs to one of the already existingdomains which means it would fail in the case where the sentence doesn?t belong to any of the existingdomains.
In our case we do not make any such assumptions.Academically, above approaches are well suited for solving the problem of domain adaptation, butduring the deployment of SMT systems in industrial scenario where the client is unable to deliver theparallel in-domain data these approaches fail to provide a quick solution.8 ConclusionWe present an approach to multi-model domain adaptation in a particularly challenging setting wherethere is no parallel in-domain data.
Parameter estimation without in-domain development set is a problemthat, to the best of our knowledge, has not been addressed before.
We designed a method for tuning modelparameters without parallel development set and validated it through an experimental program for whichwe compared performances against an array of Oracles and Baselines.
The effectiveness of the proposedmethod empirically supports the findings of (Pecina et al., 2012), who discovered that the log linearweights largely depend on the distance of training domain from the domain on which the models arebeing optimized on.
As a side result, we designed in the process a novel similarity metric between aphrase table and a source sample and implemented it effectively using wFSAs.
We empirically showedthe excellent computation speed of BLEU-PT scores as compared to standard Cross-Entropy measureusing standard toolkits.1121AcknowledgementThe authors thank the three anonymous reviewers for their comments and suggestions.ReferencesAmittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011.
Domain adaptation via pseudo in-domain data selection.In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ?11, pages355?362, Stroudsburg, PA, USA.
Association for Computational Linguistics.Pratyush Banerjee, Jinhua Du, Baoli Li, Sudip Kumar Naskar, Andy Way, and Josef Van Genabith.
2010.
Com-bining multi-domain statistical machine translation models using automatic classifiers.
In Proceedings of 9thConference of the Association for Machine Translation in the Americas.Nicola Bertoldi and Marcello Federico.
2009.
Domain adaptation for statistical machine translation with monolin-gual resources.
In Proceedings of the Fourth Workshop on Statistical Machine Translation, StatMT ?09, pages182?189, Stroudsburg, PA, USA.
Association for Computational Linguistics.Arianna Bisazza, Nick Ruiz, and Marcello Federico.
2011.
Fill-up versus Interpolation Methods for Phrase-basedSMT Adaptation.
In International Workshop on Spoken Language Translation (IWSLT), pages 136?143, SanFrancisco, CA.Ond?rej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, ChristofMonz, Matt Post, Radu Soricut, and Lucia Specia.
2013.
Findings of the 2013 Workshop on Statistical MachineTranslation.
In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 1?44, Sofia,Bulgaria, August.
Association for Computational Linguistics.Nicola Cancedda.
2012.
Private access to phrase tables for statistical machine translation.
In ACL (2), pages23?27.Rich Caruana.
1997.
Multitask learning.
Mach.
Learn., 28(1):41?75, July.Mauro Cettolo, Christian Girardi, and Marcello Federico.
2012.
Wit3: Web inventory of transcribed and translatedtalks.
In Proceedings of the 16thConference of the European Association for Machine Translation (EAMT),pages 261?268, Trento, Italy, May.Stanley F. Chen and Joshua Goodman.
1999.
An empirical study of smoothing techniques for language modeling.Computer Speech and Language, 4(13):359?393.Jorge Civera and Alfons Juan.
2007.
Domain adaptation in statistical machine translation with mixture modelling.In Proceedings of the Second Workshop on Statistical Machine Translation, pages 177?180, Prague, CzechRepublic, June.
Association for Computational Linguistics.Nan Duan, Mu Li, Dongdong Zhang, and Ming Zhou.
2010.
Mixture model-based minimum bayes risk de-coding using multiple machine translation systems.
In Proceedings of the 23rd International Conference onComputational Linguistics, pages 313?321.
Association for Computational Linguistics.George Foster and Roland Kuhn.
2007.
Mixture-model adaptation for smt.
In Proceedings of the Second Work-shop on Statistical Machine Translation, StatMT ?07, pages 128?135, Stroudsburg, PA, USA.
Association forComputational Linguistics.Qin Gao and Stephan Vogel.
2008.
Parallel implementations of word alignment tool.
In Software Engineering,Testing, and Quality Assurance for Natural Language Processing, SETQA-NLP ?08, pages 49?57, Stroudsburg,PA, USA.
Association for Computational Linguistics.Barry Haddow and Philipp Koehn.
2012.
Analysing the effect of out-of-domain data on smt systems.
In Pro-ceedings of the Seventh Workshop on Statistical Machine Translation, pages 422?432, Montreal, Canada, June.Association for Computational Linguistics.Eva Hasler, Barry Haddow, and Philipp Koehn.
2011.
Margin Infused Relaxed Algorithm for Moses.
The PragueBulletin of Mathematical Linguistics, 96:69?78.Philipp Koehn and Barry Haddow.
2012.
Towards effective use of training data in statistical machine transla-tion.
In Proceedings of the Seventh Workshop on Statistical Machine Translation, WMT ?12, pages 317?321,Stroudsburg, PA, USA.
Association for Computational Linguistics.1122Philipp Koehn and Kevin Knight.
2001.
Knowledge sources for word-level translation models.
In In Proceedingsof the 2001 Conference on Empirical Methods in Natural Language Processing, pages 27?35.Philipp Koehn and Josh Schroeder.
2007.
Experiments in domain adaptation for statistical machine translation.
InProceedings of the Second Workshop on Statistical Machine Translation, StatMT ?07, pages 224?227, Strouds-burg, PA, USA.
Association for Computational Linguistics.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,R.
Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst.
2007.
Moses: Open Source Toolkit for Statisti-cal Machine Translation.
In Proceedings of the 45th Annual Meeting of the Association for ComputationalLinguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177?180, Prague, CzechRepublic.Adam Lopez and Philipp Resnik.
2006.
Word-based alignment, phrase-based translation: What?s the link?
In InProceedings of AMTA, pages 90?99.Franz Josef Och.
2003.
Minimum Error Rate Training in Statistical Machine Translation.
In Erhard Hinrichs andDan Roth, editors, Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-jing Zhu.
2002.
BLEU : a Method for Automatic Evalua-tion of Machine Translation.
In Computational Linguistics, volume pages, pages 311?318.Pavel Pecina, Antonio Toral, and Josef van Genabith.
2012.
Simple and effective parameter tuning for domainadaptation of statistical machine translation.
In COLING, pages 2209?2224.Majid Razmara, George Foster, Baskaran Sankaran, and Anoop Sarkar.
2012.
Mixing multiple translation modelsin statistical machine translation.
In Proceedings of the 50th Annual Meeting of the Association for Computa-tional Linguistics: Long Papers-Volume 1, pages 940?949.
Association for Computational Linguistics.Rico Sennrich, Holger Schwenk, and Walid Aransa.
2013.
A multi-domain translation model framework forstatistical machine translation.
In Proceedings of the 51st Annual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 832?840, Sofia, Bulgaria, August.
Association for ComputationalLinguistics.Rico Sennrich.
2012a.
Mixture-modeling with unsupervised clusters for domain adaptation in statistical machinetranslation.
In Proceedings of the 16th Annual Conference of the European Association of Machine Translation(EAMT).Rico Sennrich.
2012b.
Perplexity minimization for translation model domain adaptation in statistical machinetranslation.
In Proceedings of the 13th Conference of the European Chapter of the Association for Computa-tional Linguistics, EACL ?12, pages 539?549, Stroudsburg, PA, USA.
Association for Computational Linguis-tics.Jason R. Smith, Herve Saint-Amand, Magdalena Plamada, Philipp Koehn, Chris Callison-Burch, and Adam Lopez.2013.
Dirt cheap web-scale parallel text from the common crawl.
In Proceedings of the 51st Annual Meetingof the Association for Computational Linguistics (Volume 1: Long Papers), pages 1374?1383, Sofia, Bulgaria,August.
Association for Computational Linguistics.Andreas Stolcke.
2002.
Srilm - an extensible language modeling toolkit.
In Proceedings of ICSLP, Denver,Colorado.J?org Tiedemann.
2012.
Parallel data, tools and interfaces in opus.
In Nicoletta Calzolari (Conference Chair),Khalid Choukri, Thierry Declerck, Mehmet Uur Doan, Bente Maegaard, Joseph Mariani, Jan Odijk, and SteliosPiperidis, editors, Proceedings of the Eight International Conference on Language Resources and Evaluation(LREC?12), Istanbul, Turkey, may.
European Language Resources Association (ELRA).Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki Isozaki.
2007.
Online large-margin training for statisticalmachine translation.
In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural LanguageProcessing and Computational Natural Language Learning (EMNLP-CoNLL), pages 764?773, Prague, CzechRepublic, June.
Association for Computational Linguistics.Yi Yang, Heng Tao Shen, Zhigang Ma, Zi Huang, and Xiaofang Zhou.
2011. l 2, 1-norm regularized discriminativefeature selection for unsupervised learning.
In Proceedings of the Twenty-Second international joint conferenceon Artificial Intelligence-Volume Volume Two, pages 1589?1594.
AAAI Press.1123
