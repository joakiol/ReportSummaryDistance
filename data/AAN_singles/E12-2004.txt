Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 16?19,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsTransAhead: A Writing Assistant for CAT and CALL*Chung-chi Huang  ++Ping-che Yang  *Mei-hua Chen *Hung-ting Hsieh  +Ting-hui Kao+Jason S. Chang*ISA, NTHU, HsinChu, Taiwan, R.O.C.++III, Taipei, Taiwan, R.O.C.
+CS, NTHU, HsinChu, Taiwan, R.O.C.
{u901571,maciaclark,chen.meihua,vincent732,maxis1718,jason.jschang}gmail.comAbstractWe introduce a method for learning topredict the following grammar and textof the ongoing translation given a sourcetext.
In our approach, predictions areoffered aimed at reducing users?
burdenon lexical and grammar choices, andimproving productivity.
The methodinvolves learning syntactic phraseologyand translation equivalents.
At run-time,the source and its translation prefix aresliced into ngrams to generate subsequentgrammar and translation predictions.
Wepresent a prototype writing assistant,TransAhead1, that applies the method towhere computer-assisted translation andlanguage learning meet.
The preliminaryresults show that the method has greatpotentials in CAT and CALL (significantboost in translation quality is observed).1.
IntroductionMore and more language learners use the MTsystems on the Web for language understandingor learning.
However, web translation systemstypically suggest a, usually far from perfect, one-best translation and hardly interact with the user.Language learning/sentence translation couldbe achieved more interactively and appropriatelyif a system recognized translation as acollaborative sequence of the user?s learning andchoosing from the machine-generated predictionsof the next-in-line grammar and text and themachine?s adapting to the user?s accepting/overriding the suggestions.Consider the source sentence ??????????????????
(We play an important rolein closing this deal).
The best learningenvironment is probably not the one solely1Available at http://140.114.214.80/theSite/TransAhead/which, for the time being, only supports Chrome browsers.providing the automated translation.
A goodlearning environment might comprise a writingassistant that gives the user direct control overthe target text and offers text and grammarpredictions following the ongoing translations.We present a new system, TransAhead, thatautomatically learns to predict/suggest thegrammatical constructs and lexical translationsexpected to immediately follow the currenttranslation given a source text, and adapts to theuser?s choices.
Example TransAhead responsesto the source ?????????????????
?and the ongoing translation ?we?
and ?we playan important role?
are shown in Figure 12(a) and(b) respectively.
TransAhead has determined theprobable subsequent grammatical constructionswith constituents lexically translated, shown inpop-up menus (e.g., Figure 1(b) shows aprediction ?IN[in] VBG[close, end, ?]?
due tothe history ?play role?
where lexical items insquare brackets are lemmas of potentialtranslations).
TransAhead learns these constructsand translations during training.At run-time, TransAhead starts with a sourcesentence, and iteratively collaborates with theuser: by making predictions on the successivegrammar patterns and lexical translations, and byadapting to the user?s translation choices toreduce source ambiguities (e.g., wordsegmentation and senses).
In our prototype,TransAhead mediates between users andautomatic modules to boost users?
writing/translation performance (e.g., productivity).2.
Related WorkCAT has been an area of active research.
Ourwork addresses an aspect of CAT focusing onlanguage learning.
Specifically, our goal is tobuild a human-computer collaborative writingassistant: helping the language learner with in-text  grammar  and  translation  and  at  the  same2Note that grammatical constituents (in all-capitalizedwords) are represented using Penn parts-of-speech and thehistory based on the user input is shown in shades.16Figure 1.
Example TransAhead responses to a source text under the translation (a) ?we?
and (b) ?we play an important role?.
Notethat the grammar/text predictions of (a) and (b) are not placed directly under the current input focus for space limit.
(c) and (d)depict predominant grammar constructs which follow and (e) summarizes the translations for the source?s character-based ngrams.time updating the system?s segmentation/translation options through the user?s wordchoices.
Our intended users are different fromthose of the previous research focusing on whatprofessional translator can bring for MT systems(e.g., Brown and Nirenburg, 1990).More recently, interactive MT (IMT) systemshave begun to shift the user?s role from analysesof the source text to the formation of the targettranslation.
TransType project (Foster et al 2002)describes such pioneering system that supportsnext word predictions.
Koehn (2009) developscaitra which displays one phrase translation at atime and offers alternative translation options.Both systems are similar in spirit to our work.The main difference is that we do not expect theuser to be a professional translator and weprovide translation hints along with grammarpredictions to avoid the generalization issuefacing phrase-based system.Recent work has been done on using fully-fledged statistical MT systems to produce targethypotheses completing user-validated translationprefix in IMT paradigm.
Barrachina et al(2008)investigate the applicability of different MTkernels within IMT framework.
Nepveu et al(2004) and Ortiz-Martinez et al(2011) furtherexploit user feedbacks for better IMT systemsand user experience.
Instead of trigged by usercorrection, our method is triggered by worddelimiter and assists in target language learning.In contrast to the previous CAT research, wepresent a writing assistant that suggestssubsequent grammar constructs with translationsand interactively collaborates with learners, inview of reducing users?
burden on grammar andword choice and enhancing their writing quality.3.
The TransAhead System3.1 Problem StatementFor CAT and CALL, we focus on predicting aset of grammar patterns with lexical translationslikely to follow the current target translationgiven a source text.
The predictions will beexamined by a human user directly.
Not tooverwhelm the user, our goal is to return areasonable-sized set of predictions that containsuitable word choices and correct grammar tochoose and learn from.
Formally speaking,Problem Statement: We are given a target-language reference corpus Ct, a parallel corpusCst, a source-language text S, and its targettranslation prefix Tp.
Our goal is to provide a setof predictions based on Ct and Cst likely tofurther translate S in terms of grammar and text.For this, we transform S and Tp into sets ofngrams such that the predominant grammarconstructs with suitable translation optionsfollowing Tp are likely to be acquired.3.2  Learning to Find Pattern and TranslationWe attempt to find syntax-based phraseology andtranslation equivalents beforehand (four-staged)so that a real-time system is achievable.Firstly, we syntactically analyze the corpus Ct.In light of the phrases in grammar book (e.g.,one?s in ?make up one?s mind?
), we resort toparts-of-speech for syntactic generalization.Secondly, we build up inverted files of the wordsin Ct for the next stage (i.e., pattern grammargeneration).
Apart from sentence and positioninformation, a word?s lemma and part-of-speech(POS) are also recorded.
(b)Source text:????????????????
(a)Pop-up predictions/suggestions:we MD VB[play, act, ..] , ?we VBP[play, act, ..] DT , ?we VBD[play, act, ..] DT , ?Pop-up predictions/suggestions:play role IN[in] VBG[close, end, ..] , ?important role IN[in] VBG[close, end, ..] , ?role IN[in] VBG[close, end, ..] , ?
(c)(d)(e)Patterns for ?we?
:we MD VB , ?,we VBP DT , ?,we VBD DT , ?Patterns for ?we play an important role?
:play role IN[in] DT ,play role IN[in] VBG , ?,important role IN[in] VBG , ?,role IN[in] VBG , ?Translations for the source text:????
: we, ?
; ????
: close, end, ?
;  ?
; ????
:play, ?
; ????
: critical, ?
; ?
; ???
: act, ?
; ?;???
: heavy, ?
; ???
: will, wish, ?
; ???
: cents, ?;???
: outstanding, ?Input your source text and start to interact with TransAhead!17We then leverage the procedure in Figure 2 togenerate grammar patterns for any givensequence of words (e.g., contiguous or not).Figure 2.
Automatically generating pattern grammar.The algorithm first identifies the sentencescontaining the given sequence of words, query.Iteratively, Step (3) performs an AND operationon the inverted file, InvList, of the current wordwi and interInvList, a previous intersected results.Afterwards, we analyze query?s syntax-basedphraseology (Step (5)).
For each element of theform ([wordPosi(w1),?,wordPosi(wn)], sentencenumber) denoting the positions of query?s wordsin the sentence, we generate grammar patterninvolving replacing words with POS tags andwords in wordPosi(wi) with lemmas, andextracting fixed-window3  segments surroundingquery from the transformed sentence.
The resultis a set of grammatical, contextual patterns.The procedure finally returns top Npredominant syntactic patterns associated withthe query.
Such patterns characterizing thequery?s word usages follow the notion of patterngrammar in (Hunston and Francis, 2000) and arecollected across the target language.In the fourth and final stage, we exploit Cst forbilingual phrase acquisition, rather than a manualdictionary, to achieve better translation coverageand variety.
We obtain phrase pairs throughleveraging IBM models to word-align the bitexts,?smoothing?
the directional word alignments viagrow-diagonal-final, and extracting translationequivalents using (Koehn et al 2003).3.3  Run-Time Grammar and Text PredictionOnce translation equivalents and phraseologicaltendencies are learned, TransAhead thenpredicts/suggests the following grammar and textof a translation prefix given the source text usingthe procedure in Figure 3.We first slice the source text S and itstranslation prefix Tp into character-level and3Inspired by (Gamon and Leacock, 2010).word-level ngrams respectively.
Step (3) and (4)retrieve the translations and patterns learnedfrom Section 3.2.
Step (3) acquires the activetarget-language vocabulary that may be used totranslate the source text.
To alleviate the wordboundary issue in MT raised by Ma et al(2007),TransAhead non-deterministically segments thesource text using character ngrams and proceedswith collaborations with the user to obtain thesegmentation for MT and to complete thetranslation.
Note that a user vocabulary ofpreference (due to users?
domain of knowledgeor errors of the system) may be exploited forbetter system performance.
On the other hand,Step (4) extracts patterns preceding with thehistory ngrams of {tj}.Figure 3.
Predicting pattern grammar and translations.In Step (5), we first evaluate and rank thetranslation candidates using linear combination:( ) ( )( ) ( )1 1 1 2 2   i i pP t s P s t P t T?
??
+ + ?where ?i is combination weight, P1 and P2 aretranslation and language model respectively, andt is one of the translation candidates under S andTp.
Subsequently, we incorporate the lemmatizedtranslation candidates into grammar constituentsin GramOptions.
For example, we would include?close?
in pattern ?play role IN[in] VBG?
as?play role IN[in] VBG[close]?.At last, the algorithm returns therepresentative grammar patterns with confidenttranslations expected to follow the ongoingtranslation and further translate the source.
Thisalgorithm will be triggered by word delimiter toprovide an interactive environment where CATand CALL meet.4.
Preliminary ResultsTo train TransAhead, we used British NationalCorpus and Hong Kong Parallel Text anddeployed GENIA tagger for POS analyses.To evaluate TransAhead in CAT and CALL,we introduced it to a class of 34 (Chinese) first-year college students learning English as foreignlanguage.
Designed to be intuitive to the generalpublic, esp.
language learners, presentationaltutorial lasted only for a minute.
After the tutorial,the participants were asked to translate 15procedure PatternFinding(query,N,Ct) (1)  interInvList=findInvertedFile(w1 of query)for each word wi in query except for w1 (2)     InvList=findInvertedFile(wi) (3a)   newInterInvList= ?
; i=1; j=1(3b)   while i<=length(interInvList) and j<=lengh(InvList)(3c)      if interInvList[i].SentNo==InvList[j].SentNo(3d)         Insert(newInterInvList, interInvList[i],InvList[j])else(3e)         Move i,j accordingly(3f)    interInvList=newInterInvList(4) Usage= ?for each element in interInvList(5)     Usage+={PatternGrammarGeneration(element,Ct)} (6) Sort patterns in Usage in descending order of frequency (7) return the N patterns in Usage with highest frequencyprocedure MakePrediction(S,Tp)(1) Assign sliceNgram(S) to {si} (2) Assign sliceNgram(Tp) to {tj} (3) TransOptions=findTranslation({si},Tp) (4) GramOptions=findPattern({tj}) (5) Evaluate translation options in TransOptionsand incorporate them into GramOptions (6) Return GramOptions18Chinese texts from (Huang et al 2011a) one byone (half with TransAhead assistance, and theother without).
Encouragingly, the experimentalgroup (i.e., with the help of our system) achievedmuch better translation quality than the controlgroup in BLEU (Papineni et al 2002) (i.e.,35.49 vs. 26.46) and significantly reduced theperformance gap between language learners andautomatic decoder of Google Translate (44.82).We noticed that, for the source ?????????????????
?, 90% of the participants in theexperimental group finished with moregrammatical and fluent translations (see Figure 4)than (less interactive) Google Translate (?Weconclude this transaction plays an importantrole?).
In comparison, 50% of the translations ofthe source from the control group were erroneous.Figure 4.
Example translations with TransAhead assistance.Post-experiment surveys indicate that a) theparticipants found TransAhead intuitive enoughto collaborate with in writing/translation; b) theparticipants found TransAhead suggestionssatisfying, accepted, and learned from them; c)interactivity made translation and languagelearning more fun and the participants foundTransAhead very recommendable and would liketo use the system again in future translation tasks.5.
Future Work and SummaryMany avenues exist for future research andimprovement.
For example, in the linearcombination, the patterns?
frequencies could beconsidered and the feature weight could be bettertuned.
Furthermore, interesting directions toexplore include leveraging user input such as(Nepveu et al 2004) and (Ortiz-Martinez et al2010) and serially combining a grammar checker(Huang et al 2011b).
Yet another directionwould be to investigate the possibility of usinghuman-computer collaborated translation pairs tore-train word boundaries suitable for MT.In summary, we have introduced a method forlearning to offer grammar and text predictionsexpected to assist the user in translation andwriting (or even language learning).
We haveimplemented and evaluated the method.
Thepreliminary results are encouragingly promising,prompting us to further qualitatively andquantitatively evaluate our system in the nearfuture (i.e., learners?
productivity, typing speedand keystroke ratios of ?del?
and ?backspace?
(possibly hesitating on the grammar and lexicalchoices), and human-computer interaction,among others).AcknowledgementThis study is conducted under the ?ProjectDigital Convergence Service Open Platform?
ofthe Institute for Information Industry which issubsidized by the Ministry of Economy Affairsof the Republic of China.ReferencesS.
Barrachina, O. Bender, F. Casacuberta, J. Civera, E.Cubel, S. Khadivi, A. Lagarda, H. Ney, J. Tomas, E.Vidal, and J.-M. Vilar.
2008.
Statistical approaches tocomputer-assisted translation.
Computer Linguistics,35(1): 3-28.R.
D. Brown and S. Nirenburg.
1990.
Human-computerinteraction for semantic disambiguation.
In Proceedingsof COLING, pages 42-47.G.
Foster, P. Langlais, E. Macklovitch, and G. Lapalme.2002.
TransType: text prediction for translators.
InProceedings of ACL Demonstrations, pages 93-94.M.
Gamon and C. Leacock.
2010.
Search right and thoushalt find ?
using web queries for learner errordetection.
In Proceedings of the NAACL Workshop onInnovative Use of NLP for Building EducationalApplications, pages 37-44.C.-C. Huang, M.-H. Chen, S.-T. Huang, H.-C. Liou, and J.S.
Chang.
2011a.
GRASP: grammar- and syntax-basedpattern-finder in CALL.
In Proceedings of ACL.C.-C. Huang, M.-H. Chen, S.-T. Huang, and J. S. Chang.2011b.
EdIt: a broad-coverage grammar checker usingpattern grammar.
In Proceedings of ACL.S.
Hunston and G. Francis.
2000.
Pattern Grammar: ACorpus-Driven Approach to the Lexical Grammar ofEnglish.
Amsterdam: John Benjamins.P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statistical phrase-based translation.
In Proceedings of NAACL.P.
Koehn.
2009.
A web-based interactive computer aidedtranslation tool.
In Proceedings of ACL.Y.
Ma, N. Stroppa, and A.
Way.
2007.
Bootstrapping wordalignment via word packing.
In Proceedings of ACL.L.
Nepveu, G. Lapalme, P. Langlais, and G. Foster.
2004.Adaptive language and translation models for interactivemachine translation.
In Proceedings of EMNLP.Franz Josef Och and Hermann Ney.
2003.
A systematicComparison of Various Statistical Alignment Models.Computational Linguistics, 29(1):19-51.D.
Ortiz-Martinez, L. A. Leiva, V. Alabau, I. Garcia-Varea,and F. Casacuberta.
2011.
An interactive machinetranslation system with online learning.
In Proceedingsof ACL System Demonstrations, pages 68-73.K.
Papineni, S. Roukos, T. Ward, W.-J.
Zhu.
2002.
Bleu: amethod for automatic evaluation of machine translation.In Proceedings of ACL, pages 311-318.1. we play(ed) a critical role in closing/sealing this/the deal.2.
we play(ed) an important role in ending/closing this/the deal.19
