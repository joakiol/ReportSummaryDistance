Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2122?2132,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsHow NOT To Evaluate Your Dialogue System: An Empirical Study ofUnsupervised Evaluation Metrics for Dialogue Response GenerationChia-Wei Liu1?, Ryan Lowe1?, Iulian V.
Serban2?, Michael Noseworthy1?,Laurent Charlin1, Joelle Pineau11 School of Computer Science, McGill University{chia-wei.liu,ryan.lowe,michael.noseworthy}@mail.mcgill.ca{lcharlin, jpineau}@cs.mcgill.ca2 DIRO, Universite?
de Montre?aliulian.vlad.serban@umontreal.caAbstractWe investigate evaluation metrics for dialogueresponse generation systems where supervisedlabels, such as task completion, are not avail-able.
Recent works in response generationhave adopted metrics from machine transla-tion to compare a model?s generated responseto a single target response.
We show thatthese metrics correlate very weakly with hu-man judgements in the non-technical Twitterdomain, and not at all in the technical Ubuntudomain.
We provide quantitative and quali-tative results highlighting specific weaknessesin existing metrics, and provide recommenda-tions for future development of better auto-matic evaluation metrics for dialogue systems.1 IntroductionAn important aspect of dialogue response generationsystems, which are trained to produce a reasonableutterance given a conversational context, is how toevaluate the quality of the generated response.
Typi-cally, evaluation is done using human-generated su-pervised signals, such as a task completion test or auser satisfaction score (Walker et al, 1997; Mo?lleret al, 2006; Kamm, 1995), which are relevant whenthe dialogue is task-focused.
We call models opti-mized for such supervised objectives supervised di-alogue models, while those that do not are unsuper-vised dialogue models.This paper focuses on unsupervised dialogue re-sponse generation models, such as chatbots.
These?Denotes equal contribution.models are receiving increased attention, partic-ularly using end-to-end training with neural net-works (Serban et al, 2016; Sordoni et al, 2015;Vinyals and Le, 2015).
This avoids the need to col-lect supervised labels on a large scale, which canbe prohibitively expensive.
However, automaticallyevaluating the quality of these models remains anopen question.
Automatic evaluation metrics wouldhelp accelerate the deployment of unsupervised re-sponse generation systems.Faced with similar challenges, other natural lan-guage tasks have successfully developed automaticevaluation metrics.
For example, BLEU (Papineniet al, 2002a) and METEOR (Banerjee and Lavie,2005) are now standard for evaluating machinetranslation models, and ROUGE (Lin, 2004) is oftenused for automatic summarization.
These metricshave recently been adopted by dialogue researchers(Ritter et al, 2011; Sordoni et al, 2015; Li et al,2015; Galley et al, 2015b; Wen et al, 2015; Liet al, 2016).
However these metrics assume thatvalid responses have significant word overlap withthe ground truth responses.
This is a strong assump-tion for dialogue systems, where there is significantdiversity in the space of valid responses to a givencontext.
This is illustrated in Table 1, where two rea-sonable responses are proposed to the context, butthese responses do not share any words in commonand do not have the same semantic meaning.In this paper, we investigate the correlation be-tween the scores from several automatic evaluationmetrics and human judgements of dialogue responsequality, for a variety of response generation models.We consider both statistical word-overlap similar-2122Context of ConversationSpeaker A: Hey John, what do you want to do tonight?Speaker B: Why don?t we go see a movie?Ground-Truth ResponseNah, I hate that stuff, let?s do something active.Model ResponseOh sure!
Heard the film about Turing is out!Table 1: Example showing the intrinsic diversityof valid responses in a dialogue.
The (reasonable)model response would receive a BLEU score of 0.ity metrics such as BLEU, METEOR, and ROUGE,and word embedding metrics derived from wordembedding models such as Word2Vec (Mikolov etal., 2013).
We find that all metrics show eitherweak or no correlation with human judgements, de-spite the fact that word overlap metrics have beenused extensively in the literature for evaluating dia-logue response models (see above, and Lasguido etal.
(2014)).
In particular, we show that these metricshave only a small positive correlation on the chitchatoriented Twitter dataset, and no correlation at all onthe technical Ubuntu Dialogue Corpus.
For the wordembedding metrics, we show that this is true eventhough all metrics are able to significantly distin-guish between baseline and state-of-the-art modelsacross multiple datasets.
We further highlight theshortcomings of these metrics using: a) a statisti-cal analysis of our survey?s results; b) a qualitativeanalysis of examples from our data; and c) an explo-ration of the sensitivity of the metrics.Our results indicate that a shift must be made inthe research community away from these metrics,and highlight the need for a new metric that corre-lates more strongly with human judgement.2 Related WorkWe focus on metrics that are model-independent,i.e.
where the model generating the response doesnot also evaluate its quality; thus, we do not con-sider word perplexity, although it has been used toevaluate unsupervised dialogue models (Serban etal., 2015).
This is because it is not computed ona per-response basis, and cannot be computed forretrieval models.
Further, we only consider met-rics that can be used to evaluate proposed responsesagainst ground-truth responses, so we do not con-sider retrieval-based metrics such as recall, whichhas been used to evaluate dialogue models (Schatz-mann et al, 2005; Lowe et al, 2015).
We also do notconsider evaluation methods for supervised evalua-tion methods.1Several recent works on unsupervised dialoguesystems adopt the BLEU score for evaluation.
Rit-ter et al (2011) formulate the unsupervised learningproblem as one of translating a context into a candi-date response.
They use a statistical machine trans-lation (SMT) model to generate responses to variouscontexts using Twitter data, and show that it outper-forms information retrieval baselines according toboth BLEU and human evaluations.
Sordoni et al(2015) extend this idea using a recurrent languagemodel to generate responses in a context-sensitivemanner.
They also evaluate using BLEU, howeverthey produce multiple ground truth responses by re-trieving 15 responses from elsewhere in the corpus,using a simple bag-of-words model.
Li et al (2015)evaluate their proposed diversity-promoting objec-tive function for neural network models using BLEUscore with only a single ground truth response.
Amodified version of BLEU, deltaBLEU (Galley etal., 2015b), which takes into account several human-evaluated ground truth responses, is shown to have aweak to moderate correlation to human judgementsusing Twitter dialogues.
However, such human an-notation is often infeasible to obtain in practice.
Gal-ley et al (2015b) also show that, even with sev-eral ground truth responses available, the standardBLEU metric does not correlate strongly with hu-man judgements.There has been significant previous work thatevaluates how well automatic metrics correlate withhuman judgements in in both machine translation(Callison-Burch et al, 2010; Callison-Burch et al,2011; Bojar et al, 2014; Graham et al, 2015)and natural language generation (NLG) (Stent etal., 2005; Cahill, 2009; Reiter and Belz, 2009; Es-pinosa et al, 2010).
There has also been workcriticizing the usefulness of BLEU in particular formachine translation (Callison-Burch et al, 2006).While many of the criticisms in these works applyto dialogue generation, we note that generating di-alogue responses conditioned on the conversational1Evaluation methods in the supervised setting have beenwell studied, see (Walker et al, 1997; Mo?ller et al, 2006; Joki-nen and McTear, 2009).2123context is in fact a more difficult problem.
This isbecause most of the difficulty in automatically eval-uating language generation models lies in the largeset of correct answers.
Dialogue response genera-tion given solely the context intuitively has a higherdiversity (or entropy) than translation given text ina source language, or surface realization given someintermediate form (Artstein et al, 2009).3 Evaluation MetricsGiven a dialogue context and a proposed response,our goal is to automatically evaluate how appropri-ate the proposed response is to the conversation.
Wefocus on metrics that compare it to the ground truthresponse of the conversation.
In particular, we inves-tigate two approaches: word based similarity met-rics and word-embedding based similarity metrics.3.1 Word Overlap-based MetricsWe first consider metrics that evaluate the amountof word-overlap between the proposed response andthe ground-truth response.
We examine the BLEUand METEOR scores that have been used for ma-chine translation, and the ROUGE score that hasbeen used for automatic summarization.
While thesemetrics have been shown to correlate with humanjudgements in their target domains (Papineni et al,2002a; Lin, 2004), they have not been thoroughlyinvestigated for dialogue systems.2We denote the ground truth response as r (thus weassume that there is a single candidate ground truthresponse), and the proposed response as r?.
The j?thtoken in the ground truth response r is denoted bywj , with w?j denoting the j?th token in the proposedresponse r?.BLEU.
BLEU (Papineni et al, 2002a) analyzesthe co-occurrences of n-grams in the ground truthand the proposed responses.
It first computes ann-gram precision for the whole dataset (we assumethat there is a single candidate ground truth response2To the best of our knowledge, only BLEU has been eval-uated in the dialogue system setting quantitatively by Galleyet al (2015a) on the Twitter domain.
However, they carriedout their experiments in a very different setting with multipleground truth responses, which are rarely available in practice,and without providing any qualitative analysis of their results.per context):Pn(r, r?)
=?k min(h(k, r), h(k, r?i))?k h(k, ri)where k indexes all possible n-grams of length n andh(k, r) is the number of n-grams k in r.3 To avoidthe drawbacks of using a precision score, namelythat it favours shorter (candidate) sentences, the au-thors introduce a brevity penalty.
BLEU-N, whereN is the maximum length of n-grams considered, isdefined as:BLEU-N := b(r, r?)
exp(N?n=1?n logPn(r, r?
))?n is a weighting that is usually uniform, and b(?)
isthe brevity penalty.
The most commonly used ver-sion of BLEU uses N = 4.
Modern versions ofBLEU also use sentence-level smoothing, as the ge-ometric mean often results in scores of 0 if there isno 4-gram overlap (Chen and Cherry, 2014).
Notethat BLEU is usually calculated at the corpus-level,and was originally designed for use with multiplereference sentences.METEOR.
The METEOR metric (Banerjee andLavie, 2005) was introduced to address severalweaknesses in BLEU.
It creates an explicit align-ment between the candidate and target responses.The alignment is based on exact token matching,followed by WordNet synonyms, stemmed tokens,and then paraphrases.
Given a set of alignments, theMETEOR score is the harmonic mean of precisionand recall between the proposed and ground truthsentence.ROUGE.
ROUGE (Lin, 2004) is a set of evalua-tion metrics used for automatic summarization.
Weconsider ROUGE-L, which is a F-measure based onthe Longest Common Subsequence (LCS) betweena candidate and target sentence.
The LCS is a set ofwords which occur in two sentences in the same or-der; however, unlike n-grams the words do not haveto be contiguous, i.e.
there can be other words in be-tween the words of the LCS.3Note that the min in this equation is calculating the num-ber of co-occurrences of n-gram k between the ground truth re-sponse r and the proposed response r?, as it computes the fewestappearances of k in either response.21243.2 Embedding-based MetricsAn alternative to using word-overlap based metricsis to consider the meaning of each word as definedby a word embedding, which assigns a vector toeach word.
Methods such as Word2Vec (Mikolov etal., 2013) calculate these embeddings using distribu-tional semantics; that is, they approximate the mean-ing of a word by considering how often it co-occurswith other words in the corpus.4 These embedding-based metrics usually approximate sentence-levelembeddings using some heuristic to combine thevectors of the individual words in the sentence.
Thesentence-level embeddings between the candidateand target response are compared using a measuresuch as cosine distance.Greedy Matching.
Greedy matching is the oneembedding-based metric that does not computesentence-level embeddings.
Instead, given two se-quences r and r?, each token w ?
r is greedilymatched with a token w?
?
r?
based on the cosinesimilarity of their word embeddings (ew), and thetotal score is then averaged across all words:G(r, r?)
=?w?r; maxw??r?
cos sim(ew, ew?
)|r|GM(r, r?)
= G(r, r?)
+G(r?, r)2This formula is asymmetric, thus we must averagethe greedy matching scores G in each direction.This was originally introduced for intelligent tutor-ing systems (Rus and Lintean, 2012).
The greedyapproach favours responses with key words that aresemantically similar to those in the ground truth re-sponse.Embedding Average.
The embedding averagemetric calculates sentence-level embeddings usingadditive composition, a method for computing themeanings of phrases by averaging the vector repre-sentations of their constituent words (Foltz et al,1998; Landauer and Dumais, 1997; Mitchell andLapata, 2008).
This method has been widely usedin other domains, for example in textual similarity4To maintain statistical independence between the task andeach performance metric, it is important that the word embed-dings used are trained on corpora which do not overlap with thetask corpus.tasks (Wieting et al, 2015).
The embedding aver-age, e?, is defined as the mean of the word embed-dings of each token in a sentence r:e?r =?w?r ew|?w?
?r ew?
|.To compare a ground truth response r and retrievedresponse r?, we compute the cosine similarity be-tween their respective sentence level embeddings:EA := cos(e?r, e?r?
).Vector Extrema.
Another way to calculatesentence-level embeddings is using vector ex-trema (Forgues et al, 2014).
For each dimensionof the word vectors, take the most extreme valueamongst all word vectors in the sentence, and usethat value in the sentence-level embedding:erd ={maxw?r ewd if ewd > |minw?
?r ew?d|minw?r ewd otherwisewhere d indexes the dimensions of a vector; ewd isthe d?th dimensions of ew (w?s embedding).
Themin in this equation refers to the selection of thelargest negative value, if it has a greater magnitudethan the largest positive value.Similarity between response vectors is again com-puted using cosine distance.
Intuitively, this ap-proach prioritizes informative words over commonones; words that appear in similar contexts will beclose together in the vector space.
Thus, commonwords are pulled towards the origin because theyoccur in various contexts, while words carrying im-portant semantic information will lie further away.By taking the extrema along each dimension, we arethus more likely to ignore common words.4 Dialogue Response Generation ModelsIn order to determine the correlation between au-tomatic metrics and human judgements of responsequality, we obtain response from a diverse range ofresponse generation models in the recent literature,including both retrieval and generative models.4.1 Retrieval ModelsRanking or retrieval models for dialogue systemsare typically evaluated based on whether they canretrieve the correct response from a corpus of pre-defined responses, which includes the ground truth2125Ubuntu Dialogue Corpus Twitter CorpusEmbedding Greedy Vector Embedding Greedy VectorAveraging Matching Extrema Averaging Matching ExtremaR-TFIDF 0.536 ?
0.003 0.370 ?
0.002 0.342 ?
0.002 0.483 ?
0.002 0.356 ?
0.001 0.340 ?
0.001C-TFIDF 0.571 ?
0.003 0.373 ?
0.002 0.353 ?
0.002 0.531 ?
0.002 0.362 ?
0.001 0.353 ?
0.001DE 0.650 ?
0.003 0.413 ?
0.002 0.376 ?
0.001 0.597 ?
0.002 0.384 ?
0.001 0.365 ?
0.001LSTM 0.130 ?
0.003 0.097 ?
0.003 0.089 ?
0.002 0.593 ?
0.002 0.439 ?
0.002 0.420 ?
0.002HRED 0.580 ?
0.003 0.418 ?
0.003 0.384 ?
0.002 0.599 ?
0.002 0.439 ?
0.002 0.422 ?
0.002Table 2: Models evaluated using the vector-based evaluation metrics, with 95% confidence intervals.response to the conversation (Schatzmann et al,2005).
Such systems can be evaluated using recall orprecision metrics.
However, when deployed in a realsetting these models will not have access to the cor-rect response given an unseen conversation.
Thus,in the results presented below we remove one occur-rence of the ground-truth response from the corpusand ask the model to retrieve the most appropriateresponse from the remaining utterances.
Note thatthis does not mean the correct response will not ap-pear in the corpus at all; in particular, if there ex-ists another context in the dataset with an identicalground-truth response, this will be available for se-lection by the model.We then evaluate each model by comparing theretrieved response to the ground truth response ofthe conversation.
This closely imitates real-life de-ployment of these models, as it tests the ability ofthe model to generalize to unseen contexts.TF-IDF.
We consider a simple Term Frequency- Inverse Document Frequency (TF-IDF) retrievalmodel (Lowe et al, 2015).
TF-IDF is a statis-tic that intends to capture how important a givenword is to some document, which is calculated as:tfidf(w, c, C) = f(w, c)?
log N|{c?C:w?c}| , where Cis the set of all contexts in the corpus, f(w, c) indi-cates the number of times word w appeared in con-text c, N is the total number of dialogues, and thedenominator represents the number of dialogues inwhich the word w appears.In order to apply TF-IDF as a retrieval model fordialogue, we first compute the TF-IDF vectors foreach context and response in the corpus.
We thenreturn the response with the largest cosine similar-ity in the corpus, either between the input contextand corpus contexts (C-TFIDF), or between the in-put context and corpus responses (R-TFIDF).Dual Encoder.
Next we consider the recurrentneural network (RNN) based architecture called theDual Encoder (DE) model (Lowe et al, 2015).
TheDE model consists of two RNNs which respectivelycompute the vector representation of an input con-text and response, c, r ?
Rn.
The model then cal-culates the probability that the given response is theground truth response given the context, by takinga weighted dot product: p(r is correct|c, r,M) =?
(cTMr + b) where M is a matrix of learned pa-rameters and b is a bias.
The model is trained usingnegative sampling to minimize the cross-entropy er-ror of all (context, response) pairs.
To our knowl-edge, our application of neural network models tolarge-scale retrieval in dialogue systems is novel.4.2 Generative ModelsIn addition to retrieval models, we also consider gen-erative models.
In this context, we refer to a modelas generative if it is able to generate entirely newsentences that are unseen in the training set.LSTM language model.
The baseline model is anLSTM language model (Hochreiter and Schmidhu-ber, 1997) trained to predict the next word in the(context, response) pair.
During test time, the modelis given a context, encodes it with the LSTM andgenerates a response using a greedy beam searchprocedure (Graves, 2013).HRED.
Finally we consider the Hierarchical Re-current Encoder-Decoder (HRED) (Serban et al,2015).
In the traditional Encoder-Decoder frame-work, all utterances in the context are concatenatedtogether before encoding.
Thus, information fromprevious utterances is far outweighed by the mostrecent utterance.
The HRED model uses a hier-archy of encoders; each utterance in the contextpasses through an ?utterance-level?
encoder, and the2126Twitter UbuntuMetric Spearman p-value Pearson p-value Spearman p-value Pearson p-valueGreedy 0.2119 0.034 0.1994 0.047 0.05276 0.6 0.02049 0.84Average 0.2259 0.024 0.1971 0.049 -0.1387 0.17 -0.1631 0.10Extrema 0.2103 0.036 0.1842 0.067 0.09243 0.36 -0.002903 0.98METEOR 0.1887 0.06 0.1927 0.055 0.06314 0.53 0.1419 0.16BLEU-1 0.1665 0.098 0.1288 0.2 -0.02552 0.8 0.01929 0.85BLEU-2 0.3576 < 0.01 0.3874 < 0.01 0.03819 0.71 0.0586 0.56BLEU-3 0.3423 < 0.01 0.1443 0.15 0.0878 0.38 0.1116 0.27BLEU-4 0.3417 < 0.01 0.1392 0.17 0.1218 0.23 0.1132 0.26ROUGE 0.1235 0.22 0.09714 0.34 0.05405 0.5933 0.06401 0.53Human 0.9476 < 0.01 1.0 0.0 0.9550 < 0.01 1.0 0.0Table 3: Correlation between each metric and human judgements for each response.
Correlations shown inthe human row result from randomly dividing human judges into two groups.Spearman p-value Pearson p-valueBLEU-1 0.1580 0.12 0.2074 0.038BLEU-2 0.2030 0.043 0.1300 0.20Table 4: Correlation between BLEU metric andhuman judgements after removing stopwords andpunctuation for the Twitter dataset.Mean score?w <= 6 ?w >= 6 p-value(n=47) (n=53)BLEU-1 0.1724 0.1009 < 0.01BLEU-2 0.0744 0.04176 < 0.01Average 0.6587 0.6246 0.25METEOR 0.2386 0.2073 < 0.01Human 2.66 2.57 0.73Table 5: Effect of differences in response lengthfor the Twitter dataset, ?w = absolute difference in#words between a ground truth response and pro-posed responseoutput of these encoders is passed through another?context-level?
encoder, which enables the handlingof longer-term dependencies.4.3 Conclusions from an Incomplete AnalysisWhen evaluation metrics are not explicitly corre-lated to human judgement, it is possible to drawmisleading conclusions by examining how the met-rics rate different models.
To illustrate this point,we compare the performance of selected models ac-cording to the embedding metrics on two differentdomains: the Ubuntu Dialogue Corpus (Lowe etal., 2015), which contains technical vocabulary andwhere conversations are often oriented towards solv-ing a particular problem, and a non-technical Twittercorpus collected following the procedure of Ritteret al (2010).
We consider these two datasets sincethey cover contrasting dialogue domains, i.e.
tech-nical help vs casual chit-chat, and because they areamongst the largest publicly available corpora, mak-ing them good candidates for building data-drivendialogue systems.Results on the proposed embedding metrics areshown in Table 2.
For the retrieval models, we ob-serve that the DE model significantly outperformsboth TFIDF baselines on all metrics across bothdatasets.
Further, the HRED model significantlyoutperforms the basic LSTM generative model inboth domains, and appears to be of similar strengthas the DE model.
Based on these results, one mightbe tempted to conclude that there is some infor-mation being captured by these metrics, that sig-nificantly differentiates models of different qual-ity.
However, as we show in the next section,the embedding-based metrics correlate only weaklywith human judgements on the Twitter corpus, andnot at all on the Ubuntu Dialogue Corpus.
Thisdemonstrates that metrics that have not been specif-ically correlated with human judgements on a newtask should not be used to evaluate that task.5 Human Correlation AnalysisData Collection.
We conducted a human surveyto determine the correlation between human judge-ments on the quality of responses, and the score as-signed by each metric.
We aimed to follow the pro-cedure for the evaluation of BLEU (Papineni et al,2127(a) Twitter(b) UbuntuFigure 1: Scatter plots showing the correlation between metrics and human judgements on the Twittercorpus (a) and Ubuntu Dialogue Corpus (b).
The plots represent BLEU-2 (left), embedding average (center),and correlation between two randomly selected halves of human respondents (right).2002a).
25 volunteers from the Computer Sciencedepartment at the author?s institution were given acontext and one proposed response, and were askedto judge the response quality on a scale of 1 to 5.5;a 1 indicates that the response is not appropriate orsensible given the context, and a 5 indicates that theresponse is very reasonable.
Out of the 25 respon-dents, 23 had Cohen?s kappa scores ?
> 0.2 w.r.t.the other respondents, which is a standard measurefor inter-rater agreement (Cohen, 1968).
The 2 re-spondents with ?
< 0.2, indicating slight agree-ment, were excluded from the analysis below.
Themedian ?
score was approximately 0.55, roughly in-dicating moderate to strong annotator agreement.Each volunteer was given 100 questions perdataset.
These questions correspond to 20 uniquecontexts, with 5 different responses: one utterance5Studies asking humans to evaluate text often rate differentaspects separately, such as ?adequacy?, ?fluency?
and ?informa-tiveness?
of the text (Hovy, 1999; Papineni et al, 2002b) Ourevaluation focuses on adequacy.
We did not consider fluencybecause 4 out of the 5 proposed responses to each context weregenerated by a human.
We did not consider informativeness be-cause in the domains considered, it is not necessarily important(in Twitter), or else it seems to correlate highly with adequacy(in Ubuntu).randomly drawn from elsewhere in the test set, theresponse selected from each of the TF-IDF, DE, andHRED models, and a response written by a humanannotator.
These were chosen as they cover therange of qualities almost uniformly (see Figure 1).Survey Results.
We present correlation results be-tween the human judgements and each metric in Ta-ble 3.
We compute the Pearson correlation, whichestimates linear correlation, and Spearman correla-tion, which estimates any monotonic correlation.The first observation is that in both domains theBLEU-4 score, which has previously been used toevaluate unsupervised dialogue systems, shows veryweak if any correlation with human judgement.
Infact we found that the BLEU-3 and BLEU-4 scoreswere near-zero for a majority of response pairs; forBLEU-4, only four examples had a score > 10?9.Despite this, they still correlate with human judge-ments on the Twitter Corpus at a rate similar toBLEU-2.
This is because of the smoothing con-stant, which gives a tiny weight to unigrams and bi-grams despite the absence of higher-order n-grams.BLEU-3 and BLEU-4 behave as a scaled, noisy ver-sion of BLEU-2; thus, if one is to evaluate dialogue2128Context of ConversationA: dearest!
question.
how many thousands of peoplecan panaad occupy?B: @user panaad has <number> k seat capacity while rizalhas <number> k thats why they choose rizal i think .Ground Truth ResponseA: now i know about the siting capacity .
thanks for theinfo @user great evening.Proposed ResponseA: @user makes sense.
thanks!Context of ConversationA: never felt more sad than i am nowB: @user aww why ?A: @user @user its a long story !
sure you wanna knowit ?
bahaha and thanks for caring btw <heart>Ground Truth ResponseA: @user i don ?t mind to hear it i ?ve got all day andyoure welcome <number>Proposed ResponseA: @user i know , i ?m just so happy for you !
!
!
!
!
!
!
!!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!Figure 2: Examples where the metrics rated the response poorly and humans rated it highly (left), and theconverse (right).
Both responses are given near-zero score by BLEU-N for N> 1.
While no metric willperform perfectly on all examples, we present these examples to provide intuition on how example-levelerrors become aggregated into poor correlation to human judgements at the corpus-level.responses with BLEU, we recommend the choice ofN = 2 over N = 3 or 4.
Note that using a test cor-pus larger than the size reported in this paper maylead to stronger correlations for BLEU-3 and BLEU-4, due to a higher number of non-zero scores.It is interesting to note that, while some of the em-bedding metrics and BLEU show small positive cor-relation in the non-technical Twitter domain, thereis no metric that significantly correlates with hu-mans on the Ubuntu Dialogue Corpus.
This is likelybecause the correct Ubuntu responses contain spe-cific technical words that are less likely to be pro-duced by our models.
Further, it is possible that re-sponses in the Ubuntu Dialogue Corpus have intrin-sically higher variability (or entropy) than Twitterwhen conditioned on the context, making the eval-uation problem significantly more difficult.Figure 1 illustrates the relationship between met-rics and human judgements.
We include only thebest performing metric using word-overlaps, i.e.
theBLEU-2 score (left), and the best performing met-ric using word embeddings, i.e.
the vector average(center).
These plots show how weak the correlationis: in both cases, they appear to be random noise.It seems as though the BLEU score obtains a pos-itive correlation because of the large number of re-sponses that are given a score of 0 (bottom left cor-ner of the first plot).
This is in stark contrast to theinter-rater agreement, which is plotted between tworandomly sampled halves of the raters (right-mostplots).
We also calculated the BLEU scores afterremoving stopwords and punctuation from the re-sponses.
As shown in Table 4, this weakens the cor-relation with human judgements for BLEU-2 com-pared to the values in Table 3, and suggests thatBLEU is sensitive to factors that do not change thesemantics of the response.Finally, we examined the effect of response lengthon the metrics, by considering changes in scoreswhen the ground truth and proposed response hada large difference in word counts.
Table 4 showsthat BLEU and METEOR are particularly sensitiveto this aspect, compared to the Embedding Averagemetric and human judgement.Qualitative Analysis.
In order to determinespecifically why the metrics fail, we examine qual-itative samples where there is a disagreement be-tween the metrics and human rating.
Although theseonly show inconsistencies at the example-level, theyprovide some intuition as to why the metrics don?tcorrelate with human judgements at the corpus-level.
We present in Figure 2 two examples where allof the embedding-based metrics and BLEU-1 scorethe proposed response significantly differently thanthe humans.The left of Figure 2 shows an example wherethe embedding-based metrics score the proposed re-sponse lowly, while humans rate it highly.
It isclear from the context that the proposed responseis reasonable ?
indeed both responses intend to ex-press gratitude.
However, the proposed responsehas a different wording than the ground truth re-sponse, and therefore the metrics are unable to sep-arate the salient words from the rest.
This sug-gests that the embedding-based metrics would ben-2129efit from a weighting of word saliency.The right of the figure shows the reverse scenario:the embedding-based metrics score the proposed re-sponse highly, while humans do not.
This is mostlikely due to the frequently occurring ?i?
token, andthe fact that ?happy?
and ?welcome?
may be closetogether in the embedding space.
However, froma human perspective there is a significant semanticdifference between the responses as they pertain tothe context.
Metrics that take into account the con-text may be required in order to differentiate theseresponses.
Note that in both responses in Figure 2,there are no overlapping n-grams greater than un-igrams between the ground truth and proposed re-sponses; thus, all of BLEU-2,3,4 would assign ascore near 0 to the response.6 DiscussionWe have shown that many metrics commonly usedin the literature for evaluating unsupervised dialoguesystems do not correlate strongly with human judge-ment.
Here we elaborate on important issues arisingfrom our analysis.Constrained tasks.
Our analysis focuses on rela-tively unconstrained domains.
Other work, whichseparates the dialogue system into a dialogue plan-ner and a natural language generation componentfor applications in constrained domains, may findstronger correlations with the BLEU metric.
For ex-ample, Wen et al (2015) propose a model to mapfrom dialogue acts to natural language sentences anduse BLEU to evaluate the quality of the generatedsentences.
Since the mapping from dialogue acts tonatural language sentences has lower diversity andis more similar to the machine translation task, itseems likely that BLEU will correlate better withhuman judgements.
However, an empirical inves-tigation is still necessary to justify this.Incorporating multiple responses.
Our correla-tion results assume that only one ground truth re-sponse is available given each context.
Indeed, thisis the common setting in most of the recent literatureon training end-to-end conversation models.
Therehas been some work on using a larger set of auto-matically retrieved plausible responses when evalu-ating with BLEU (Galley et al, 2015b).
However,there is no standard method for doing this in the lit-erature.
Future work should examine how retriev-ing additional responses affects the correlation withword-overlap metrics.Searching for suitable metrics.
While we pro-vide evidence against existing metrics, we do notyet provide good alternatives for unsupervised eval-uation.
Despite the poor performance of the wordembedding-based metrics in this survey, we believethat metrics based on distributed sentence represen-tations hold the most promise for the future.
Thisis because word-overlap metrics will simply requiretoo many ground-truth responses to find a significantmatch for a reasonable response, due to the high di-versity of dialogue responses.
As a simple example,the skip-thought vectors of Kiros et al (2015) couldbe considered.
Since the embedding-based metricsin this paper only consist of basic averages of vectorsobtained through distributional semantics, they areinsufficiently complex for modeling sentence-levelcompositionality in dialogue.
Instead, these metricscan be interpreted as calculating the topicality of aproposed response (i.e.
how on-topic the proposedresponse is, compared to the ground-truth).All of the metrics considered in this paper directlycompare a proposed response to the ground-truth,without considering the context of the conversation.However, metrics that take into account the contextcould also be considered.
Such metrics could comein the form of an evaluation model that is learnedfrom data.
This model could be either a discrim-inative model that attempts to distinguish betweenmodel and human responses, or a model that usesdata collected from the human survey in order toprovide human-like scores to proposed responses.Finally, we must consider the hypothesis that learn-ing such models from data is no easier than solvingthe problem of dialogue response generation.
If thishypothesis is true, we must concede and always usehuman evaluations together with metrics that onlyroughly approximate human judgements.ReferencesR.
Artstein, S. Gandhe, J. Gerten, A. Leuski, and D.Traum.
2009.
Semi-formal evaluation of conversa-tional characters.
In Languages: From Formal to Nat-ural, pages 22?35.
Springer.2130S.
Banerjee and A. Lavie.
2005.
METEOR: An auto-matic metric for mt evaluation with improved corre-lation with human judgments.
In Proceedings of theACL workshop on intrinsic and extrinsic evaluationmeasures for machine translation and/or summariza-tion.O.
Bojar, C. Buck, C. Federmann, B. Haddow, P. Koehn,J.
Leveling, C. Monz, P. Pecina, M. Post, H. Saint-Amand, et al 2014.
Findings of the 2014 workshopon statistical machine translation.
In Proceedings ofthe Ninth Workshop on Statistical Machine Transla-tion, pages 12?58.
Association for Computational Lin-guistics Baltimore, MD, USA.A.
Cahill.
2009.
Correlating human and automaticevaluation of a german surface realiser.
In Proceed-ings of the ACL-IJCNLP 2009 Conference Short Pa-pers, pages 97?100.
Association for ComputationalLinguistics.C.
Callison-Burch, M. Osborne, and P. Koehn.
2006.Re-evaluation the role of bleu in machine translationresearch.
In EACL, volume 6, pages 249?256.C.
Callison-Burch, P. Koehn, C. Monz, K. Peterson, M.Przybocki, and O. F. Zaidan.
2010.
Findings of the2010 joint workshop on statistical machine translationand metrics for machine translation.
In Proceedings ofthe Joint Fifth Workshop on Statistical Machine Trans-lation and MetricsMATR, pages 17?53.
Associationfor Computational Linguistics.C.
Callison-Burch, P. Koehn, C. Monz, and O. F. Zaidan.2011.
Findings of the 2011 workshop on statisticalmachine translation.
In Proceedings of the Sixth Work-shop on Statistical Machine Translation, pages 22?64.Association for Computational Linguistics.B.
Chen and C. Cherry.
2014.
A systematic comparisonof smoothing techniques for sentence-level bleu.
ACL2014, page 362.J.
Cohen.
1968.
Weighted kappa: Nominal scaleagreement provision for scaled disagreement or partialcredit.
Psychological bulletin, 70(4):213.D.
Espinosa, R. Rajkumar, M. White, and S. Berleant.2010.
Further meta-evaluation of broad-coverage sur-face realization.
In Proceedings of the 2010 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 564?574.
Association for Compu-tational Linguistics.P.
W. Foltz, W. Kintsch, and T. K. Landauer.
1998.
Themeasurement of textual coherence with latent semanticanalysis.
Discourse processes, 25(2-3):285?307.G.
Forgues, J. Pineau, J.-M. Larcheveque, and R. Trem-blay.
2014.
Bootstrapping dialog systems with wordembeddings.M.
Galley, C. Brockett, A. Sordoni, Y. Ji, M. Auli, C.Quirk, M. l, J. Gao, and B. Dolan.
2015a.
deltaBLEU:A discriminative metric for generation tasks with in-trinsically diverse targets.
In Proceedings of the An-nual Meeting of the Association for ComputationalLinguistics and the International Joint Conference onNatural Language Processing (Short Papers).M.
Galley, C. Brockett, A. Sordoni, Y. Ji, M. Auli, C.Quirk, M. Mitchell, J. Gao, and B. Dolan.
2015b.deltableu: A discriminative metric for generationtasks with intrinsically diverse targets.
arXiv preprintarXiv:1506.06863.Y.
Graham, N. Mathur, and T. Baldwin.
2015.
Accurateevaluation of segment-level machine translation met-rics.
In Proc.
of NAACL-HLT, pages 1183?1191.
Cite-seer.A.
Graves.
2013.
Generating sequences with recurrentneural networks.
arXiv preprint arXiv:1308.0850.S.
Hochreiter and J. Schmidhuber.
1997.
Long short-term memory.
Neural Computation, 9(8):1735?1780.E.
Hovy.
1999.
Toward finely differentiated evaluationmetrics for machine translation.
In Proceedings of theEagles Workshop on Standards and Evaluation.K.
Jokinen and M. McTear.
2009.
Spoken Dialogue Sys-tems.
Morgan Claypool.C.
Kamm.
1995.
User interfaces for voice applica-tions.
Proceedings of the National Academy of Sci-ences, 92(22):10031?10037.R.
Kiros, Y. Zhu, R. R. Salakhutdinov, R. Zemel, R. Ur-tasun, A. Torralba, and S. Fidler.
2015.
Skip-thoughtvectors.
In Advances in Neural Information Process-ing Systems, pages 3276?3284.T.
K. Landauer and S. T. Dumais.
1997.
A solution toplato?s problem: The latent semantic analysis theoryof acquisition, induction, and representation of knowl-edge.
Psychological review, 104(2):211.N.
Lasguido, S. Sakti, G. Neubig, T. Tomoki, and S.Nakamura.
2014.
Utilizing human-to-human conver-sation examples for a multi domain chat-oriented di-alog system.
IEICE TRANSACTIONS on Informationand Systems, 97(6):1497?1505.J.
Li, M. Galley, C. Brockett, J. Gao, and B. Dolan.2015.
A diversity-promoting objective functionfor neural conversation models.
arXiv preprintarXiv:1510.03055.J.
Li, M. Galley, C. Brockett, J. Gao, and B. Dolan.
2016.A persona-based neural conversation model.
arXivpreprint arXiv:1603.06155.C.-Y.
Lin.
2004.
Rouge: A package for automatic eval-uation of summaries.
In Text summarization branchesout: Proceedings of the ACL-04 workshop, volume 8.R.
Lowe, N. Pow, I. V. Serban, and J. Pineau.
2015.
Theubuntu dialogue corpus: A large dataset for researchin unstructured multi-turn dialogue systems.
In SIG-DIAL.2131T.
Mikolov, I. Sutskever, K. Chen, G. S. Corrado, andJ.
Dean.
2013.
Distributed representations of wordsand phrases and their compositionality.
In Advancesin neural information processing systems, pages 3111?3119.J.
Mitchell and M. Lapata.
2008.
Vector-based modelsof semantic composition.
In ACL, pages 236?244.S.
Mo?ller, R. Englert, K. Engelbrecht, V. Hafner, A.Jameson, A. Oulasvirta, A. Raake, and N. Reithinger.2006.
MeMo: towards automatic usability evaluationof spoken dialogue services by user error simulations.In INTERSPEECH.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002a.BLEU: a method for automatic evaluation of machinetranslation.
In Proceedings of the 40th annual meetingon Association for Computational Linguistics (ACL).K.
Papineni, S. Roukos, T. Ward, J. Henderson, and F.Reeder.
2002b.
Corpus-based comprehensive anddiagnostic MT evaluation: Initial Arabic, Chinese,French, and Spanish results.
In Proceedings of thesecond international conference on Human LanguageTechnology Research, pages 132?137.E.
Reiter and A. Belz.
2009.
An investigation into thevalidity of some metrics for automatically evaluatingnatural language generation systems.
ComputationalLinguistics, 35(4):529?558.A.
Ritter, C. Cherry, and B. Dolan.
2010.
Unsupervisedmodeling of twitter conversations.
In North AmericanChapter of the Association for Computational Linguis-tics (NAACL).A.
Ritter, C. Cherry, and W. B. Dolan.
2011.
Data-driven response generation in social media.
In Pro-ceedings of the conference on empirical methods innatural language processing, pages 583?593.
Associ-ation for Computational Linguistics.V.
Rus and M. Lintean.
2012.
A comparison of greedyand optimal assessment of natural language studentinput using word-to-word similarity metrics.
In Pro-ceedings of the Seventh Workshop on Building Ed-ucational Applications Using NLP, pages 157?162,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.J.
Schatzmann, K. Georgila, and S. Young.
2005.
Quan-titative evaluation of user simulation techniques forspoken dialogue systems.
In 6th Special InterestGroup on Discourse and Dialogue (SIGDIAL).I.
V. Serban, A. Sordoni, Y. Bengio, A. Courville, and J.Pineau.
2015.
Building End-To-End Dialogue Sys-tems Using Generative Hierarchical Neural Networks.In AAAI Conference on Artificial Intelligence.I.
V. Serban, A. Sordoni, R. Lowe, L. Charlin, J. Pineau,A.
Courville, and Y. Bengio.
2016.
A hierarchicallatent variable encoder-decoder model for generatingdialogues.
arXiv preprint arXiv:1605.06069.A.
Sordoni, M. Galley, M. Auli, C. Brockett, Y. Ji, M.Mitchell, J. Nie, J. Gao, and B. Dolan.
2015.
Aneural network approach to context-sensitive genera-tion of conversational responses.
In Conference of theNorth American Chapter of the Association for Com-putational Linguistics (NAACL-HLT 2015).A.
Stent, M. Marge, and M. Singhai.
2005.
Evaluatingevaluation methods for generation in the presence ofvariation.
In International Conference on IntelligentText Processing and Computational Linguistics, pages341?351.
Springer.O.
Vinyals and Q.
Le.
2015.
A neural conversationalmodel.
arXiv preprint arXiv:1506.05869.M.
Walker, D. Litman, C. Kamm, and A. Abella.
1997.Paradise: A framework for evaluating spoken dialogueagents.
In Proceedings of the eighth conference on Eu-ropean chapter of the Association for ComputationalLinguistics, pages 271?280.
ACL.T.-H. Wen, M. Gasic, N. Mrksic, P.-H. Su, D. Vandyke,and S. Young.
2015.
Semantically conditioned lstm-based natural language generation for spoken dialoguesystems.
arXiv preprint arXiv:1508.01745.J.
Wieting, M. Bansal, K. Gimpel, and K. Livescu.
2015.Towards universal paraphrastic sentence embeddings.CoRR, abs/1511.08198.2132
