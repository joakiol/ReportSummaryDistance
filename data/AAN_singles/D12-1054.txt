Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 590?599, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsLyrics, Music, and EmotionsRada MihalceaUniversity of North Texasrada@cs.unt.eduCarlo StrapparavaFBK-irststrappa@fbk.euAbstractIn this paper, we explore the classification ofemotions in songs, using the music and thelyrics representation of the songs.
We intro-duce a novel corpus of music and lyrics, con-sisting of 100 songs annotated for emotions.We show that textual and musical features canboth be successfully used for emotion recog-nition in songs.
Moreover, through compar-ative experiments, we show that the joint useof lyrics and music brings significant improve-ments over each of the individual textual andmusical classifiers, with error rate reductionsof up to 31%.1 IntroductionLanguage and music are peculiar characteristics ofhuman beings.
The capability of producing andenjoying language and music appears in every hu-man society, regardless of the richness of its culture(Nettl, 2000).Importantly, language and music complementeach other in many different ways.
For instance,looking at music and language in terms of fea-tures, we can observe that music organizes pitchand rhythm in ways that language does not, and itlacks the specificity of language in terms of seman-tic meaning.
On the other hand, language is builtfrom categories that are absent in music (e.g., nounsand verbs), whereas music seems to have a deeperpower over our emotions than does ordinary speech.Composers, musicians, and researchers in poetryand literature alike have been long fascinated by thecombination of language and music, even since thetime of the earliest written records of music encoun-tered in musical settings for poetry.
Despite this in-terest, and despite the long history of the interactionbetween music and lyrics, there is only little workthat explicitly focuses on the connection betweenmusic and lyrics.In this paper, we focus on the connection betweenthe musical and linguistic representations in popu-lar songs, and their role in the expression of affect.We introduce a novel corpus of lyrics and music, an-notated for emotions at line level, and explore theautomatic recognition of emotions using both tex-tual and musical features.
Through comparative ex-periments, we show that emotion recognition can beperformed using either textual or musical features,and that the joint use of lyrics and music can im-prove significantly over classifiers that use only onedimension at a time.
We believe our results demon-strate the promise of using joint music-lyric modelsfor song processing.2 Related WorkThe literature on music analysis is noticeably large,and there are several studies concerning the music?spower over emotions (Juslin and Sloboda, 2001),thinking (Rauscher et al1993), or physical effort(Karageorghis and Priest, 2008).In particular, there has been significant researchin music and psychology focusing on the idea of aparallel between affective cues in music and speech(Sundberg, 1982; Scherer, 1995).
For instance,(Scherer, 2004) investigated the types of emotionsthat can be induced by music, their mechanisms, andhow they can be empirically measured.
(Juslin and590Laukka, 2003) conducted a comprehensive reviewof vocal expressions and music performance, find-ing substantial overlap in the cues used to conveybasic emotions in speech and music.The work most closely related to ours is the com-bination of audio and lyrics for emotion classifica-tion in songs, as thoroughly surveyed in (Kim et al2010).
Although several methods have been pro-posed, including a combination of textual featuresand beats per minute and MPEG descriptors (Yangand Lee, 2004); individual audio and text classifiersfor arousal and valence, followed by a combinationthrough meta-learning (Yang et al2008); and theuse of crowdsourcing labeling from Last.fm to col-lect large datasets of songs annotated for emotions(Laurier et al2008; Hu et al2009), all this pre-vious work was done at song level, and most ofit focused on valence-arousal classifications.
Noneof the previous methods considered the fine-grainedclassification of emotions at line level, as we do, andnone of them considered the six Ekman emotionsused in our work.Other related work consists of the developmentof tools for music accessing, filtering, classification,and retrieval, focusing primarily on music in digitalformat such as MIDI.
For instance, the task of musicretrieval and music recommendation has received alot of attention from both the arts and the computerscience communities (see for instance (Orio, 2006)for an introduction to this task).
There are also sev-eral works on MIDI analysis.
Among them, partic-ularly relevant to our research is the work by (Daset al2000), who described an analysis of predom-inant up-down motion types within music, throughextraction of the kinematic variables of music veloc-ity and acceleration from MIDI data streams.
(Catal-tepe et al2007) addressed music genre classifica-tion (e.g., classic, jazz, pop) using MIDI and au-dio features, while (Wang et al2004) automati-cally aligned acoustic musical signals with their cor-responding textual lyrics.
MIDI files are typicallyorganized into one or more parallel ?tracks?
for in-dependent recording and editing.
A reliable systemto identify the MIDI track containing the melody1is very relevant for music information retrieval, and1A melody can be defined as a ?cantabile?
sequence ofnotes, usually the sequence that a listener can remember afterhearing a song.there are several approaches that have been proposedto address this issue (Rizo et al2006; Velusamy etal., 2007).Another related study concerned with the interac-tion of lyrics and music using an annotated corpus isfound in (O?Hara, 2011), who presented preliminaryresearch that checks whether the expressive meaningof a particular harmony or harmonic sequence couldbe deduced from the lyrics it accompanies, by us-ing harmonically annotated chords from the Usenetgroup alt.guitar.tab.Finally, in natural language processing, there area few studies that mainly exploited the lyrics com-ponent of the songs, while generally ignoring themusical component.
For instance, (Mahedero et al2005) dealt with language identification, structureextraction, and thematic categorization for lyrics.
(Xia et al2008) addressed the task of sentimentclassification in lyrics, recognizing positive and neg-ative moods in a large dataset of Chinese pop songs,while (Yang and Lee, 2009) approached the problemof emotion identification in lyrics, classifying songsfrom allmusic.com using a set of 23 emotions.3 A Corpus of Music and LyricsAnnotated for EmotionsTo enable our exploration of emotions in songs, wecompiled a corpus of 100 popular songs (e.g., Danc-ing Queen by ABBA, Hotel California by Eagles,Let it Be by The Beatles).
Popular songs exert alot of power on people, both at an individual levelas well as on groups, mainly because of the mes-sage and emotions they convey.
Songs can lift ourmoods, make us dance, or move us to tears.
Songsare able to embody deep feelings, usually through acombined effect of both music and lyrics.The corpus is built starting with the MIDI tracksof each song, by extracting the parallel alignmentof melody and lyrics.
Given the non-homogeneousquality of the MIDI files available on the Web, weasked a professional MIDI provider for high qualityMIDI files produced for singers and musicians.
TheMIDI files, which were purchased from the provider,contain also lyrics that are synchronized with thenotes.
In these MIDI files, the melody channel is un-equivocally decided by the provider, making it easierto extract the music and the corresponding lyrics.591MIDI format.
MIDI is an industry-standard pro-tocol that enables electronic musical instruments,computers, and other electronic equipment to com-municate and synchronize with each other.
Unlikeanalog devices, MIDI does not transmit an audiosignal: it sends event messages about musical no-tation, pitch, and intensity, control signals for pa-rameters such as volume, vibrato, and panning, andcues and clock signals to set the tempo.
As an elec-tronic protocol, it is notable for its widespread adop-tion throughout the music industry.MIDI files are typically created using computer-based sequencing software that organizes MIDImessages into one or more parallel ?tracks?
for in-dependent recording, editing, and playback.
In mostsequencers, each track is assigned to a specific MIDIchannel, which can be then associated to specific in-strument patches.
MIDI files can also contain lyrics,which can be displayed in synchrony with the music.Starting with the MIDI tracks of a song, we ex-tract and explicitly encode the following features.At the song level, the key of the song (e.g., G ma-jor, C minor).
At the line level, we represent theraising, which is the musical interval (in half-steps)between the first note in the line and the most impor-tant note (i.e., the note in the line with the longestduration).
Finally, at the note level, we encode thetime code of the note with respect to the beginningof the song; the note aligned with the correspondingsyllable; the degree of the note with relation to thekey of the song; and the duration of the note.Table 1 shows statistics on the corpus.
An exam-ple from the corpus, consisting of the first two linesfrom the Beatles?
song A hard day?s night, is illus-trated in Figure 3.SONGS 100SONGS IN ?MAJOR?
KEY 59SONGS IN ?MINOR?
KEY 41LINES 4,976ALIGNED SYLLABLES / NOTES 34,045Table 1: Some statistics of the corpusEmotion Annotations with Mechanical Turk.
Inorder to explore the classification of emotions insongs, we needed a gold standard consisting of man-ual emotion annotations of the songs.
Followingprevious work on emotion annotation of text (Almet al2005; Strapparava and Mihalcea, 2007), toannotate the emotions in songs we use the six ba-sic emotions proposed by (Ekman, 1993): ANGER,DISGUST, FEAR, JOY, SADNESS, SURPRISE.
To col-lect the annotations, we use the Amazon MechanicalTurk service, which was previously found to pro-duce reliable annotations with a quality comparableto those generated by experts (Snow et al2008).The annotations are collected at line level, with aseparate annotation for each of the six emotions.
Wecollect numerical annotations using a scale between0 and 10, with 0 corresponding to the absence of anemotion, and 10 corresponding to the highest inten-sity.
Each HIT (i.e., annotation session) contains anentire song, with a number of lines ranging from 14to 110, for an average of 50 lines per song.The annotators were instructed to: (1) Score theemotions from the writer perspective, not their ownperspective; (2) Read and interpret each line in con-text; i.e., they were asked to read and understandthe entire song before producing any annotations;(3) Produce the six emotion annotations independentfrom each other, accounting for the fact that a linecould contain none, one, or multiple emotions.
Inaddition to the lyrics, the song was also availableonline, so they could listen to it in case they werenot familiar with it.
The annotators were also giventhree different examples to illustrate the annotation.While the use of crowdsourcing for data annota-tion can result in a large number of annotations ina very short amount of time, it also has the draw-back of potential spamming that can interfere withthe quality of the annotations.
To address this aspect,we used two different techniques to prevent spam.First, in each song we inserted a ?checkpoint?
at arandom position in the song ?
a fake line that reads?Please enter 7 for each of the six emotions.?
Thoseannotators who did not follow this concrete instruc-tion were deemed as spammers who produce anno-tations without reading the content of the song, andthus removed.
Second, for each remaining annota-tor, we calculated the Pearson correlation betweenher emotion scores and the average emotion scoresof all the other annotators.
Those annotators with acorrelation with the average of the other annotatorsbelow 0.4 were also removed, thus leaving only thereliable annotators in the pool.592<token time=5760 orig?note=D?
degree=5 duration=810>HARD </token><song filename=AHARDDAY.m2a><key time=0>G major</key><line pvers=1 raising=3 anger=1.5 disgust=0.7 sadness=2.5 surprise=0.8 ></line><line pvers=2 raising=5 anger=3.5 disgust=2 sadness=1.2 surprise=0.2 ></line><token time=5040 orig?note=B degree=3 duration=210>IT</token><token time=5050 orig?note=B degree=3 duration=210>?S </token><token time=5280 orig?note=C?
degree=4 duration=210>BEEN </token><token time=5520 orig?note=B degree=3 duration=210>A </token><token time=6720 orig?note=D?
degree=5 duration=570>DAY</token><token time=6730 orig?note=D?
degree=5 duration=570>?S </token><token time=7440 orig?note=D?
degree=5 duration=690>NIGHT</token><token time=8880 orig?note=C?
degree=4 duration=212>AND </token><token time=9120 orig?note=D?
degree=5 duration=210>I</token><token time=9130 orig?note=D?
degree=5 duration=210>?VE </token><token time=9600 orig?note=D?
degree=5 duration=210>WOR</token><token time=9840 orig?note=F?
degree=7?
duration=930>KING </token><token time=10800 orig?note=D?
degree=5 duration=210>LI</token><token time=11040 orig?note=C?
degree=4 duration=210>KE </token><token time=11050 orig?note=C?
degree=4 duration=210>A </token><token time=11280 orig?note=D?
degree=5 duration=330>D</token><token time=11640 orig?note=C?
degree=4 duration=90>O</token><token time=11760 orig?note=B degree=3 duration=330>G</token><token time=9360 orig?note=C?
degree=4 duration=210>BEEN </token>Figure 1: Two lines of a song in the corpus: It-?s been a hard day-?s night, And I-?ve been wor-king li-ke a d-o-gFor each song, we start by asking for ten annota-tions.
After spam removal, we were left with abouttwo-five annotations per song.
The final annotationsare produced by averaging the emotions scores pro-duced by the reliable annotators.
Figure 3 showsan example of the emotion scores produced for twolines.
The overall correlation between the remain-ing reliable annotators was calculated as 0.73, whichrepresents a strong correlation.For each of the six emotions, Table 2 shows thenumber of lines that had that emotion present (i.e.,the score of the emotion was different from 0), aswell as the average score for that emotion over all4,976 lines in the corpus.
Perhaps not surprisingly,the emotions that are dominant in the corpus are JOYand SADNESS ?
which are the emotions that are of-ten invoked by people as the reason behind a song.Note that the emotions do not exclude each other:i.e., a line that is labeled as containing JOY may alsocontain a certain amount of SADNESS, which is thereason for the high percentage of songs containingboth JOY and SADNESS.
The emotional load forthe overlapping emotions is however very different.For instance, the lines that have a JOY score of 5or higher have an average SADNESS score of 0.34.Conversely, the lines with a SADNESS score of 5 orNumberEmotion lines AverageANGER 2,516 0.95DISGUST 2,461 0.71FEAR 2,719 0.77JOY 3,890 3.24SADNESS 3,840 2.27SURPRISE 2,982 0.83Table 2: Emotions in the corpus of 100 songs: numberof lines including a certain emotion, and average emotionscore computed over all the 4,976 lines.higher have a JOY score of 0.22.4 Experiments and EvaluationsThrough our experiments, we seek to determine theextent to which we can automatically determine theemotional load of each line in a song, for each of thesix emotion dimensions.We use two main classes of features: textual fea-tures, which build upon the textual representation ofthe lyrics; and musical features, which rely on themusical notation associated with the songs.
We runthree sets of experiments.
The first one is intended todetermine the usefulness of the textual features for593emotion classification.
The second set specificallyfocuses on the musical features.
Finally, the last setof experiments makes joint use of textual and musi-cal features.The experiments are run using linear regression,2and the results are evaluated by measuring the Pear-son correlation between the classifier predictionsand the gold standard.
For each experiment, a ten-fold cross validation is run on the entire dataset.34.1 Textual FeaturesFirst, we attempt to identify the emotions in a lineby relying exclusively on the features that can be de-rived from the lyrics of the song.
We decided to fo-cus on those features that were successfully used inthe past for emotion classification (Strapparava andMihalcea, 2008).
Specifically, we use: (1) unigramfeatures obtained from a bag-of-words representa-tion, which are the features typically used by corpus-based methods; and (2) lexicon features, indicatingthe appartenance of a word to a semantic class de-fined in manually crafted lexicons, which are oftenused by knowledge-based methods.Unigrams.
We use a bag-of-words representationof the lyrics to derive unigram counts, which arethen used as input features.
First, we build a vo-cabulary consisting of all the words, including stop-words, occurring in the lyrics of the training set.
Wethen remove those words that have a frequency be-low 10 (value determined empirically on a small de-velopment set).
The remaining words represent theunigram features, which are then associated with avalue corresponding to the frequency of the unigraminside each line.
Note that we also attempted to usehigher order n-grams (bigrams and trigrams), butevaluations on a small development dataset did notshow any improvements over the unigram model,and thus all the experiments are run using unigrams.Semantic Classes.
We also derive and use coarsetextual features, by using mappings between wordsand semantic classes.
Specifically, we use the Lin-2We use the Weka machine learning toolkit.3There is no clear way to determine a baseline for theseexperiments.
A simple baseline that we calculated, which as-sumed by default an emotional score equal to the average of thescores on the training data, and measured the correlation be-tween these default scores and the gold standard, consistentlyled to correlations close to 0 (0.0081-0.0221).guistic Inquiry and Word Count (LIWC) and Word-Net Affect (WA) to derive coarse textual features.LIWC was developed as a resource for psycholin-guistic analysis (Pennebaker and Francis, 1999;Pennebaker and King, 1999).
The 2001 version ofLIWC includes about 2,200 words and word stemsgrouped into about 70 broad categories relevant topsychological processes (e.g., emotion, cognition).WA (Strapparava and Valitutti, 2004) is a resourcethat was created starting with WordNet, by annotat-ing synsets with several emotions.
It uses several re-sources for affective information, including the emo-tion classification of Ortony (Ortony et al1987).From WA, we extract the words corresponding tothe six basic emotions used in our experiments.
Foreach semantic class, we infer a feature indicating thenumber of words in a line belonging to that class.Table 3 shows the Pearson correlations obtainedfor each of the six emotions, when using only uni-grams, only semantic classes, or both.Semantic AllEmotion Unigrams Classes TextualANGER 0.5525 0.3044 0.5658DISGUST 0.4246 0.2394 0.4322FEAR 0.3744 0.2443 0.4041JOY 0.5636 0.3659 0.5769SADNESS 0.5291 0.3006 0.5418SURPRISE 0.3214 0.2153 0.3392AVERAGE 0.4609 0.2783 0.4766Table 3: Evaluations using textual features: unigrams,semantic classes, and all the textual features.4.2 Musical Features.In a second set of experiments, we explore the roleplayed by the musical features.
While the musicalnotation of a song offers several characteristics thatcould be potentially useful for our classification ex-periments (e.g., notes, measures, dynamics, tempo),in these initial experiments we decided to focus ontwo main features, namely the notes and the key.Notes.
A note is a sign used in the musical nota-tion associated with a song, to represent the relativeduration and pitch of a sound.
In traditional mu-sic theory, the notes are represented using the firstseven letters of the alphabet (C-D-E-F-G-A-B), al-594though other notations can also be used.
Notes canbe modified by ?accidentals?
?
a sharp or a flat sym-bol that can change the note by half a tone.
A writtennote can also have associated a value, which refersto its duration (e.g., whole note; eighth note).
Simi-lar to the unigram features, for each note, we recorda feature indicating the frequency of that note insidea line.Key.
The key of a song refers to the harmony or?pitch class?
used for a song, e.g., C major, or F#.Sometime the term minor or major can be appendedto a key, to indicate a minor or a major scale.
Forinstance, a song in ?the key of C minor?
means thatthe song is harmonically centered on the note C, andit makes use of the minor scale whose first note is C.The key system is the structural foundation of mostof the Western music.
We use a simple feature thatreflects the key of the song.
Note that with a few ex-ceptions, when more than one key is used in a song,all the lines in a song will have the same key.Table 4 shows the results obtained in these clas-sification experiments, when using only the notes asfeatures, only the key, or both.AllEmotion Notes Key MusicalANGER 0.2453 0.4083 0.4405DISGUST 0.1485 0.2922 0.3199FEAR 0.1361 0.2203 0.2450JOY 0.1533 0.3835 0.4001SADNESS 0.1738 0.3502 0.3762SURPRISE 0.0983 0.2241 0.2412AVERAGE 0.1592 0.3131 0.3371Table 4: Evaluations using musical features: notes, key,and all the musical features.4.3 Joint Textual and Musical Features.To explore the usefulness of the joint lyrics and mu-sic representation, we also run a set of experimentsthat use all the textual and musical features.
Table 5shows the Pearson correlations obtained when us-ing all the features.
To facilitate the comparison,the table also includes the results obtained with thetextual-only and musical-only features (reported inTables 3 and 4).All All Textual &Emotion Textual Musical MusicalANGER 0.5658 0.4405 0.6679DISGUST 0.4322 0.3199 0.5068FEAR 0.4041 0.2450 0.4384JOY 0.5769 0.4001 0.6456SADNESS 0.5418 0.3762 0.6193SURPRISE 0.3392 0.2412 0.3855AVERAGE 0.4766 0.3371 0.5439Table 5: Evaluations using both textual and musical fea-tures.5 DiscussionOne clear conclusion can be drawn from these ex-periments: the textual and musical features are bothuseful for the classification of emotions in songs,and, more importantly, their joint use leads to thehighest classification results.
Specifically, the jointmodel gives an error rate reduction of 12.9% withrespect to the classifier that uses only textual fea-tures, and 31.2% with respect to the classifier thatuses only musical features.
This supports the ideathat lyrics and music represent orthogonal dimen-sions for the classification of emotions in songs.Among the six emotions considered, the largestimprovements are observed for JOY, SADNESS, andANGER.
This was somehow expected for the firsttwo emotions, since they appear to be dominant inthe corpus (see Table 2), but comes as a surprisefor ANGER, which is less dominant.
Further explo-rations are needed to determine the reason for thiseffect.Looking at the features considered, textual fea-tures appear to be the most useful.
Nonetheless,the addition of the musical features brings clear im-provements, as shown in the last column from thesame table.Additionally, we made several further analyses ofthe results, as described below.Feature ablation.
To determine the role played byeach of the feature groups we consider, we run anablation study where we remove one feature groupat a time from the complete set of features and mea-sure the accuracy of the resulting classifier.
Table 6shows the feature ablation results.
Note that featureablation can also be done in the reverse direction, by595All features, excludingAll Semantic Semantic ClassesEmotion Features Unigrams Classes Notes Key and NotesANGER 0.6679 0.4996 0.5525 0.6573 0.6068 0.6542DISGUST 0.5068 0.3831 0.4246 0.5013 0.4439 0.4814FEAR 0.4384 0.3130 0.3744 0.4313 0.4150 0.4114JOY 0.6456 0.5141 0.5636 0.6432 0.5829 0.6274SADNESS 0.6193 0.4586 0.5291 0.6176 0.5540 0.6029SURPRISE 0.3855 0.3083 0.3214 0.3824 0.3421 0.3721AVERAGE 0.5439 0.4127 0.4609 0.5388 0.4908 0.5249Table 6: Ablation studies excluding one feature group at a time.Textual andEmotion Baseline Textual Musical MusicalANGER 89.27% 91.14% 89.63% 92.40%DISGUST 93.85% 94.67% 93.85% 94.77%FEAR 93.58% 93.87% 93.58% 93.87%JOY 50.26% 70.92% 61.95% 75.64%SADNESS 67.40% 75.84% 70.65% 79.42%SURPRISE 94.83% 94.83% 94.83% 94.83%AVERAGE 81.53% 86.87% 84.08% 88.49%Table 7: Evaluations using a coarse-grained binary classification.keeping only one group of features at a time; the re-sults obtained with the individual feature groups arealready reported in Tables 3 and 4.The ablation studies confirm the findings from ourearlier experiments: while the unigrams and the keysare the most predictive features, the semantic classesand the notes are also contributing to the final clas-sification even if to a lesser extent.
To measure theeffect of these groups of somehow weaker features(semantic classes and notes), we also perform an ab-lation experiment where we remove both these fea-ture groups from the feature set.
The results are re-ported in the last column of Table 6.Coarse-grained classification.
As an additionalevaluation, we transform the task into a binary clas-sification by using a threshold empirically set at 3.Thus, to generate the coarse binary annotations, ifthe score of an emotion is below 3, we record it as?negative?
(i.e., the emotion is absent), whereas ifthe score is equal to or above 3, we record it as ?pos-itive?
(i.e., the emotion is present).For the classification, we use Support Vector Ma-chines (SVM), which are binary classifiers that seekto find the hyperplane that best separates a set of pos-itive examples from a set of negative examples, withmaximum margin (Vapnik, 1995).
Applications ofSVM classifiers to text categorization led to some ofthe best results reported in the literature (Joachims,1998).Table 7 shows the results obtained for each of thesix emotions, and for the three major settings thatwe considered: textual features only, musical fea-tures only, and a classifier that jointly uses the tex-tual and the musical features.
As before, the classi-fication accuracy for each experiment is reported asthe average of the accuracies obtained during a ten-fold cross-validation on the corpus.
The table alsoshows a baseline, computed as the average of theaccuracies obtained when using the most frequentclass observed on the training data for each fold.As seen from the table, on average, the joint use oftextual and musical features is also beneficial for thisbinary coarser-grained classification.
Perhaps notsurprisingly, the effect of the classifier is stronger for5961,000 news headlines 4,976 song linesBest result (Strapparava Joint TextEmotion SEMEVAL?07 and Mihalcea, 08) and MusicANGER 0.3233 0.1978 0.6679DISGUST 0.1855 0.1354 0.5068FEAR 0.4492 0.2956 0.4384JOY 0.2611 0.1381 0.6456SADNESS 0.4098 0.1601 0.6193SURPRISE 0.1671 0.1235 0.3855AVERAGE 0.2993 0.1750 0.5439Table 8: Results obtained in previous work on emotion classification.those emotions that are dominant in the corpus, i.e.,JOY and SADNESS (see Table 2).
The improvementobtained with the classifiers is much smaller for theother emotions (or even absent, e.g., for SURPRISE),which is also explained by their high baseline of over90%.Comparison to previous work.
There is no pre-vious research that has considered the joint use oflyrics and songs representations for emotion classifi-cation at line level, and thus we cannot draw a directcomparison with other work on emotion classifica-tion in songs.Nonetheless, as a point of reference, we considerthe previous work done on emotion classification oftexts.
Table 8 shows the results obtained in previ-ous work for the recognition of emotions in a corpusconsisting of 1,000 news headlines (Strapparava andMihalcea, 2007) annotated for the same six emo-tions.
Specifically, the table shows the best over-all correlation results obtained by the three emotionrecognition systems in the SEMEVAL task on Affec-tive Text (Strapparava and Mihalcea, 2007): (Chau-martin, 2007; Kozareva et al2007; Katz et al2007).
The table also shows the best results obtainedin follow up work carried out on the same dataset(Strapparava and Mihalcea, 2008).Except for one emotion (FEAR), the correlationfigures we obtain are significantly higher than thosereported in previous work.
As mentioned before,however, a direct comparison cannot be made, sincethe earlier work used a different, smaller dataset.Moreover, our corpus of songs is likely to be moreemotionally loaded than the news titles used in pre-vious work.6 ConclusionsPopular songs express universally understood mean-ings and embody experiences and feelings shared bymany, usually through a combined effect of both mu-sic and lyrics.
In this paper, we introduced a novelcorpus of music and lyrics, annotated for emotions atline level, and we used this corpus to explore the au-tomatic recognition of emotions in songs.
Throughexperiments carried out on the dataset of 100 songs,we showed that emotion recognition can be per-formed using either textual or musical features, andthat the joint use of lyrics and music can improvesignificantly over classifiers that use only one di-mension at a time.The dataset introduced in this paper is availableby request from the authors of the paper.AcknowledgmentsThe authors are grateful to Rajitha Schellenbergfor her help with collecting the emotion annota-tions.
Carlo Strapparava was partially supported bya Google Research Award.
Rada Mihalcea?s workwas in part supported by the National Science Foun-dation award #0917170.
Any opinions, findings, andconclusions or recommendations expressed in thismaterial are those of the authors and do not neces-sarily reflect the views of the National Science Foun-dation.ReferencesC.
Alm, D. Roth, and R. Sproat.
2005.
Emotions fromtext: Machine learning for text-based emotion predic-tion.
In Proceedings of the Conference on Empirical597Methods in Natural Language Processing, pages 347?354, Vancouver, Canada.Z.
Cataltepe, Y. Yaslan, and A. Sonmez.
2007.
Mu-sic genre classification using MIDI and audio features.Journal on Advances in Signal Processing.F.R.
Chaumartin.
2007.
Upar7: A knowledge-based sys-tem for headline sentiment tagging.
In Proceedings ofthe Fourth International Workshop on Semantic Evalu-ations (SemEval-2007), Prague, Czech Republic, June.M.
Das, D. Howard, and S. Smith.
2000.
The kinematicanalysis of motion curves through MIDI data analysis.Organised Sound, 5(1):137?145.P.
Ekman.
1993.
Facial expression of emotion.
Ameri-can Psychologist, 48:384?392.X.
Hu, J. S. Downie, and A. F. Ehmann.
2009.
Lyrictext mining in music mood classification.
In Proceed-ings of the International Society for Music InformationRetrieval Conference, Kobe, Japan.T.
Joachims.
1998.
Text categorization with SupportVector Machines: learning with mny relevant features.In Proceedings of the European Conference on Ma-chine Learning, pages 137?142, Chemnitz, Germany.P.
Juslin and P. Laukka.
2003.
Communication of emo-tion in vocal expression and music performance: Dif-ferent channels, same code?
Psychological Bulletin,129:770?814.P.
N. Juslin and J.
A. Sloboda, editors.
2001.
Music andEmotion: Theory and Research.
Oxford UniversityPress.C.
Karageorghis and D. Priest.
2008.
Music in sport andexercise : An update on research and application.
TheSport Journal, 11(3).P.
Katz, M. Singleton, and R. Wicentowski.
2007.Swat-mp:the semeval-2007 systems for task 5 andtask 14.
In Proceedings of the Fourth InternationalWorkshop on Semantic Evaluations (SemEval-2007),Prague, Czech Republic, June.Y.
Kim, E. Schmidt, R. Migneco, B. Morton, P. Richard-son, J. Scott, J. Speck, and D. Turnbull.
2010.
Mu-sic emotion recognition: A state of the art review.
InInternational Symposium on Music Information Re-trieval.Z.
Kozareva, B. Navarro, S. Vazquez, and A. Mon-toyo.
2007.
Ua-zbsa: A headline emotion classifi-cation through web information.
In Proceedings of theFourth International Workshop on Semantic Evalua-tions (SemEval-2007), Prague, Czech Republic, June.C.
Laurier, J. Grivolla, and P. Herrera.
2008.
Multimodalmusic mood classification using audio and lyrics.
InProceedings of the International Conference on Ma-chine Learning and Applications, Barcelona, Spain.J.
Mahedero, A. Martinez, and P. Cano.
2005.
Natu-ral language processing of lyrics.
In Proceedings ofMM?05, Singapore, November.B.
Nettl.
2000.
An ethnomusicologist contemplatesuniversals in musical sound and musical culture.
InN.
Wallin, B. Merker, and S. Brown, editors, Theorigins of music, pages 463?472.
MIT Press, Cam-bridge, MA.T.
O?Hara.
2011.
Inferring the meaning of chord se-quences via lyrics.
In Proceedings of 2nd Workshopon Music Recommendation and Discovery (WOMRAD2011), Chicago, IL, October.N.
Orio.
2006.
Music retrieval: A tutorial and re-view.
Foundations and Trends in Information Re-trieval, 1(1):1?90, November.A.
Ortony, G. L. Clore, and M. A. Foss.
1987.
The ref-erential structure of the affective lexicon.
CognitiveScience, (11).J.
Pennebaker and M. Francis.
1999.
Linguistic inquiryand word count: LIWC.
Erlbaum Publishers.J.
Pennebaker and L. King.
1999.
Linguistic styles: Lan-guage use as an individual difference.
Journal of Per-sonality and Social Psychology, (77).F.
Rauscher, G. Shaw, and K. Ky. 1993.
Music and spa-tial task performance.
Nature, 365.D.
Rizo, P. Ponce de Leon, C. Perez-Sancho, A. Pertusa,and J. Inesta.
2006.
A pattern recognition approachfor melody track selection in MIDI files.
In Proceed-ings of 7th International Symposyum on Music Infor-mation Retrieval (ISMIR-06), pages 61?66, Victoria,Canada, October.K.
Scherer.
1995.
Expression of emotion in voice andmusic.
Journal of Voice, 9:235?248.K.
Scherer.
2004.
Which emotions can be induced bymusic?
what are the underlying mechanisms?
andhow can we measure them ?
Journal of New MusicResearch, 33:239?251.R.
Snow, B. O?Connor, D. Jurafsky, and A. Ng.
2008.Cheap and fast ?
but is it good?
evaluating non-expertannotations for natural language tasks.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing, Honolulu, Hawaii.C.
Strapparava and R. Mihalcea.
2007.
Semeval-2007task 14: Affective text.
In Proceedings of the 4thInternational Workshop on the Semantic Evaluations(SemEval 2007), Prague, Czech Republic.C.
Strapparava and R. Mihalcea.
2008.
Learning to iden-tify emotions in text.
In Proceedings of the ACM Con-ference on Applied Computing ACM-SAC 2008, Fort-aleza, Brazile.C.
Strapparava and A. Valitutti.
2004.
Wordnet-affect:an affective extension of wordnet.
In Proceedingsof the 4th International Conference on Language Re-sources and Evaluation, Lisbon.J.
Sundberg.
1982.
Speech, song, and emotions.
InM.
Clynes, editor, Music, Mind and Brain: The Neu-ropsychology of Music.
Plenum Press, New York.598V.
Vapnik.
1995.
The Nature of Statistical Learning The-ory.
Springer, New York.S.
Velusamy, B. Thoshkahna, and K. Ramakrishnan.2007.
Novel melody line identification algorithm forpolyphonic MIDI music.
In Proceedings of 13th In-ternational Multimedia Modeling Conference (MMM2007), Singapore, January.Y.
Wang, M. Kan, T. Nwe, A. Shenoy, and J. Yin.
2004.LyricAlly: Automatic synchronization of acoustic mu-sical signals and textual lyrics.
In Proceedings ofMM?04, New York, October.Y.
Xia, L. Wang, K.F.
Wong, and M. Xu.
2008.
Lyric-based song sentiment classification with sentimentvector space model.
In Proceedings of the Associationfor Computational Linguistics, Columbus, Ohio.D.
Yang and W. Lee.
2004.
Disambiguating musicemotion using software agents.
In Proceedings of theInternational Conference on Music Information Re-trieval, Barcelona, Spain.D.
Yang and W. Lee.
2009.
Music emotion identificationfrom lyrics.
In Proceedings of 11th IEEE Symposiumon Multimedia.Y.-H. Yang, Y.-C. Lin, H.-T. Cheng, I.-B.
Liao, Y.-C. Ho,and H. Chen.
2008.
Toward multi-modal music emo-tion classification.
In Proceedings of the 9th PacificRim Conference on Multimedia: Advances in Multi-media Information Processing.599
