Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 69?77,Portland, OR, USA, 24 June 2011. c?2011 Association for Computational LinguisticsCrowdsourcing syntactic relatedness judgements for opinion mining in thestudy of information technology adoptionAsad B. Sayeed, Bryan Rusk, Martin Petrov,Hieu C. Nguyen, Timothy J. MeyerDepartment of Computer ScienceUniversity of MarylandCollege Park, MD 20742 USAasayeed@cs.umd.edu,brusk@umd.edu,martin@martinpetrov.com,{hcnguyen88,tmeyer88}@gmail.comAmy WeinbergCenter for the AdvancedStudy of Languageand Department of LinguisticsUniversity of MarylandCollege Park, MD 20742 USAaweinberg@casl.umd.eduAbstractWe present an end-to-end pipeline includinga user interface for the production of word-level annotations for an opinion-mining taskin the information technology (IT) domain.Our pre-annotation pipeline selects candidatesentences for annotation using results from asmall amount of trained annotation to bias therandom selection over a large corpus.
Ouruser interface reduces the need for the user tounderstand the ?meaning?
of opinion in ourdomain context, which is related to commu-nity reaction.
It acts as a preliminary bufferagainst low-quality annotators.
Finally, ourpost-annotation pipeline aggregates responsesand applies a more aggressive quality filter.We present positive results using two differ-ent evaluation philosophies and discuss howour design decisions enabled the collection ofhigh-quality annotations under subjective andfine-grained conditions.1 IntroductionCrowdsourcing permits us to use a bank of anony-mous workers with unknown skill levels to performcomplex tasks given a simple breakdown of thesetasks with user interface design that hides the fulltask complexity.
Use of these techniques is growingin the areas of computational linguistics and infor-mation retrieval, particularly since these fields nowrely on the collection of large datasets for use in ma-chine learning.
Considering the variety of applica-tions, a variety of datasets is needed, but trained,known workers are an expense in principle that mustbe furnished for each one.
Consequently, crowd-sourcing offers a way to collect this data cheaply andquickly (Snow et al, 2008; Sayeed et al, 2010a).We applied crowdsourcing to perform the fine-grained annotation of a domain-specific corpus.
Ouruser interface design and our annotator quality con-trol process allows these anonymous workers to per-form a highly subjective task in a manner that cor-relates their collective understanding of the task toour own expert judgements about it.
The path tosuccess provides some illustration of the pitfalls in-herent in opinion annotation.
Our task is: domainand application-specific sentiment classification atthe sub-sentence level?at the word level.1.1 OpinionsFor our purposes, we define opinion mining (some-times known as sentiment analysis) to be the re-trieval of a triple {source, target, opinion} (Sayeedet al, 2010b; Pang and Lee, 2008; Kim and Hovy,2006) in which the source is the entity that origi-nated the opinionated language, the target is a men-tion of the entity or concept that is the opinion?stopic, and the opinion is a value (possibly a struc-ture) that reflects some kind of emotional orientationexpressed by the source towards the target.In much of the recent literature on automaticopinion mining, opinion is at best a gradient be-tween positive and negative or a binary classifica-tion thereof; further complexity affects the reliabilityof machine-learning techniques (Koppel and Schler,2006).We call opinion mining ?fine-grained?
when weare attempting to retrieve potentially many different69{source, target, opinion} triples per document.
Thisis particularly challenging when there are multipletriples even at a sentence level.1.2 Corpus-based social scienceOur work is part of a larger collaboration with socialscientists to study the diffusion of information tech-nology (IT) innovations through society by identify-ing opinion leaders and IT-relevant opinionated lan-guage (Rogers, 2003).
A key hypothesis is that thelanguage used by opinion leaders causes groups ofothers to encourage the spread of the given IT con-cept in the market.Since the goal of our exercise is to ascertain thecorrelation between the source?s behaviour and thatof others, then it may be more appropriate to lookat opinion analysis with the view that what we areattempting to discover are the views of an aggregatereader who may otherwise have an interest in the ITconcept in question.
We thus define an expression ofopinion in the following manner:A expresses opinion about B if an in-terested third party C?s actions towards Bmay be affected by A?s textually recordedactions, in a context where actions havepositive or negative weight.This perspective runs counter to a widespread view(Ruppenhofer et al, 2008) which has assumed atreatment of opinionated language as an observationof a latent ?private state?
held by the source.
Thisdefinition reflects the relationship of sentiment andopinion with the study of social impact and marketprediction.
We return to the question of how to de-fine opinion in section 6.2.1.3 Crowdsourcing in sentiment analysisPaid crowdsourcing is a relatively new trend in com-putational linguistics.
Work exists at the paragraphand document level, and it exists for the Twitter andblog genres (Hsueh et al, 2009).A key problem in crowdsourcing sentiment analy-sis is the matter of quality control.
A crowdsourcedopinion mining task is an attempt to use untrainedannotators over a task that is inherently very subjec-tive.
It is doubly difficult for specialized domains,since crowdsourcing platforms have no way of di-rectly recruiting domain experts.Hsueh et al (2009) present results in quality con-trol over snippets of political blog posts in a taskclassifying them by sentiment and political align-ment.
They find that they can use a measurement ofannotator noise to eliminate low-quality annotationsat this coarse level by reweighting snippet ambigu-ity scores with noise scores.
We demonstrate that wecan use a similar annotator quality measure alone toeliminate low-quality annotations on a much finer-grained task.1.4 Syntactic relatednessWe have a downstream application for this annota-tion task which involves acquiring patterns in thedistribution of opinion-bearing words and targets us-ing machine learning (ML) techniques.
In partic-ular, we want to acquire the syntactic relationshipsbetween opinion-bearing words and within-sentencetargets.
Supervised ML techniques require goldstandard data annotated in advance.The Multi-Perspective Question-Answering(MPQA) newswire corpus (Wilson and Wiebe,2005) and the J. D. Power & Associates (JDPA)automotive review blog post (Kessler et al, 2010)corpus are appropriate because both contain sub-sentence annotations of sentiment-bearing languageas text spans.
In some cases, they also include linksto within-sentence targets.
This is an example of anMPQA annotation:That was the moment at which the fabricof compassion tore, and worlds crackedapart; when the contrast and conflict ofcivilisational values became so great asto remove any sense of common ground -even on which to do battle.The italicized portion is intended to reflect a negativesentiment about the bolded portion.
However, whileit is the case that the whole italicized phrase repre-sents a negative sentiment, ?remove?
appears to rep-resent far more of the negativity than ?common?
and?ground?.
While there are techniques that dependon access to entire phrases, our project is to identifysentiment spans at the length of a single word.2 Data sourceOur corpus for this task is a collection of arti-cles from the IT professional magazine, Information70Week, from the years 1991 to 2008.
This consistsof 33K articles of varying lengths including newsbulletins, full-length magazine features, and opin-ion columns.
We obtained the articles via an institu-tional subscription, and reformatted them in XML1.Certain IT concepts are particularly significant inthe context of the social science application.
Our tar-get list consists of 59 IT innovations and concepts.The list includes plurals, common variations, andabbreviations.
Examples of IT concepts include ?en-terprise resource planning?
and ?customer relation-ship management?.
To avoid introducing confound-ing factors into our results, we only include explicitmentions and omit pronominal coreference.3 User interfaceOur user interface (figure 1) uses a drag-and-dropprocess through which workers make decisionsabout whether particular highlighted words withina given sentence reflect an opinion about a particu-lar mentioned IT concept or innovation.
The useris presented with a sentence from the corpus sur-rounded by some before and after context.
Under-neath the text are four boxes: ?No effect on opin-ion?
(none), ?Affects opinion positively?
(postive),?Affects opinion negatively?
(negative), and ?Can?ttell?
(ambiguous).The worker must drag each highlighted word inthe sentence into one of the boxes, as appropriate.
Ifthe worker cannot determine the appropriate box fora particular word, she is expected to drag this to theambiguous box.
The worker is presented with de-tailed instructions which also remind her that mostof words in the sentence are not actually likely to beinvolved in the expression of an opinion about therelevant IT concept2.
The worker is not permittedto submit the task without dragging all of the high-lighted words to one of the boxes.
When a wordis dragged to a box, the word in context changescolour; the worker can change her mind by clickingan X next to the word in the box.1We will likely be able to provide a sample of sentence dataannotated by our process as a resource once we work out docu-mentation and distribution issues.2We discovered when testing the interface that workers canfeel obliged to find a opinion about the selected IT concept.
Wereduced it by explicitly reminding them that most words do notexpress a relevant opinion and by placing the none box first.We used CrowdFlower to manage the task withAmazon Mechanical Turk as its distribution chan-nel.
We set CrowdFlower to present three sentencesat a time to users.
Only users with USA-based IPaddresses were permitted to perform the final task.4 ProcedureIn this section, we discuss the data processingpipeline (figure 3) through which we select candi-dates for annotations and the crowdsourcing inter-face we present to the end user for classifying indi-vidual words into categories that reflect the effect ofthe word on the worker.4.1 Data preparation4.1.1 Initial annotationTwo social science undergraduate students werehired to do annotations on Information Week withthe original intention of doing all the annotationsthis way.
There was a training period where they an-notated about 60 documents in sets of 20 in iterativeconsultation with one of the authors.
Then they weregiven 142 documents to annotate simultaneously inorder to assess their agreement after training.Annotation was performed in Atlas.ti, an anno-tation tool popular with social science researchers.It was chosen for its familiarity to the social sci-entists involved in our project and because of theirstated preference for using tools that would allowthem to share annotations with colleagues.
Atlas.tihas limitations, including the inability to create hier-archical annotations.
We overcame these limitationsusing a special notation to connect related annota-tions.
An annotator highlights a sentence that shebelieves contains an opinion about a mentioned tar-get on one of the lists.
She then highlights the men-tion of the target and, furthermore, highlights the in-dividual words that express the opinion about the tar-get, using the notation to connect related highlights.4.1.2 Candidate selectionWhile the use of trained annotators did not pro-duce reliable results (section 6.2) in acceptable timeframes, we decided to use the annotations in a pro-cess for selecting candidate sentences for crowd-sourcing.
All 219 sentences that the annotators se-lected as having opinions about within-sentence IT71Figure 1: A work unit presented in grayscale.
?E-business?
is the IT concept and would be highlighted in blue.
Thewords in question are highlighted in gray background and turn red after they are dragged to the boxes.concepts were concatenated into a single string andconverted into a TFIDF unit vector.We then selected all the sentences that containIT concept mentions from the entire InformationWeek corpus using an OpenNLP 1.4.3 model asour sentence-splitter.
This produced approximately77K sentences.
Every sentence was converted into aTFIDF unit vector, and we took the cosine similar-ity of each sentence with the TFIDF vector.
We thenranked the sentences by cosine similarity.4.1.3 Selecting highlighted wordsWe ran every sentence through the Stanfordpart-of-speech tagger.
Words that belonged toopen classes such as adjectives and verbs were se-lected along with certain closed-class words such asmodals and negation words.
These candidate wordswere highlighted in the worker interface.We did not want to force workers to classify everysingle word in a sentence, because this would be tootedious.
So we instead randomly grouped the high-lighted words into non-overlapping sets of six.
(Re-mainders less than five were dropped from the task.
)We call these combinations of sentence, six words,and target IT concept a ?highlight group?
(figure 2).Each highlight group represents a task unit whichwe present to the worker in our crowdsourcing ap-plication.
We generated 1000 highlight groups fromThe amount of industry attention paid to thisnew class of integration software speaks volumesabout the need to extend the reach of ERP systems.The amount of industry attention paid to thisnew class of integration software speaks volumesabout the need to extend the reach of ERP systems.Figure 2: Two highlight groups consisting of thesame sentence and concept (ERP) but different non-overlapping sets of candidate words.the top-ranked sentences.4.2 Crowdsourced annotation4.2.1 Training goldWe used CrowdFlower partly because of its au-tomated quality control process.
The bedrock ofthis process is the annotation of a small amount ofgold standard data by the task designers.
Crowd-Flower randomly selects gold-annotated tasks andpresents them to workers amidst other unannotatedtasks.
Workers are evaluated by the percentage ofgold-annotated tasks they perform correctly.
The re-sult of a worker performing a task unit is called a?judgement.
?Workers are initially presented their gold-annotated tasks without knowing that they are an-swering a test question.
If they get the questionwrong, CrowdFlower presents the correct answer to72them along with a reason why their answer was anerror.
They are permitted to write back to the taskdesigner if they disagree with the gold judgement.This process functions in a manner analogous tothe training of a machine-learning system.
Further-more, it permits CrowdFlower to exclude or rejectlow-quality results.
Judgements from a worker whoslips below 65% correctness are rated as untrustwor-thy and not included in the CrowdFlower?s results.We created training gold in the manner recom-mended by CrowdFlower.
We randomly selected50 highlight groups from the 1000 mentioned in theprevious section.
We ran these examples throughCrowdFlower using the interface we discuss in thenext section.
Then we used the CrowdFlower goldeditor to select 30 highlight groups that containedclear classification decisions where it appeared thatthe workers were in relative consensus and where weagreed with their decision.
Of these, we designatedonly the clearest-cut classifications as gold, leav-ing more ambiguous-seeming ones up to the users.For example, in the second highlight group in 2, wewould designate software and systems as none andextend as positive in the training gold and the re-mainder as up to the workers.
That would be a ?min-imum effort?
to indicate that the worker understandsthe task the way we do.Unfortunately, CrowdFlower has some limita-tions in the way it processes the responses to gold?it is not possible to define a minimum effort pre-cisely.
CrowdFlower?s setting either allow us to passworkers based on getting at least one item in eachclass correct or by placing all items in their correctclasses.
The latter is too strict a criterion for an in-herently subjective task.
So we accepted the former.We instead applied our minimum effort criterion insome of our experiments as described in section 4.3.4.2.2 Full runWe randomly selected another 200 highlightgroups and posted them at 12 US cents for each setof three highlight groups, with at least three Me-chanical Turk workers seeing each highlight group.The 30 training gold highlight groups were postedalong with them.
Including CrowdFlower and Ama-zon fees, the total cost was approximately 60 USD.We permitted only USA-based workers to access thetask.
Once initiated, the entire task took approxi-Figure 3: Schematic view of pipeline.mately 24 hours to complete.4.3 Post-processing4.3.1 AggregationEach individual worker?s ambiguous annotationsare converted to none annotations, as the ambigu-ous box is intended as an outlet for a worker?s un-certainty, but we choose to interpret anything thata worker considers too uncertain to be classifiedas positive or negative as something that is notstrongly opinionated under our definitions.Aggregation is performed by majority vote of theannotators on each word in each highlight group.
Ifno classification obtains more than 50% for a givenword, the word is dropped as too ambiguous to beaccepted either way as a result.
This aggregationhas the effect of smoothing out individual annotatordifferences.4.3.2 Extended quality controlWhile CrowdFlower provides a first-pass qualitycontrol system for selecting annotators who are do-ing the task in good faith and with some understand-ing of the instructions, we wanted particularly toselect annotators who would be more likely to beconsistent on the most obvious cases without overlyconstraining them.
Even with the same general ideaof our intentions, some amount of variation amongthe annotators is unavoidable; how do we then rejectannotations from those workers who pass Crowd-Flower?s liberal criteria but still do not have an ideaof annotation close enough to ours?73Our solution was to score the annotators post hocby their accuracy on our minimum-effort traininggold data.
Then we progressively dropped the worstn annotators starting from n = 0 and measured thequality of the aggregated annotations as per the fol-lowing section.5 ResultsThis task can be interpreted in two different ways:as an annotation task and as a retrieval system.
An-notator reliability is an issue insofar as it is impor-tant that the annotations themselves conform to apredetermined standard.
However, for the machinelearning task that is downstream in our processingpipeline, obtaining a consistent pattern is more im-portant than conformance to an explicit definition.We can thus interpret the results as being the out-put of a system whose computational hardware hap-pens to be a crowd of humans rather than silicon,considering that the time of the ?run?
is compara-ble to many automated systems; Amazon Mechani-cal Turk?s slogan is ?artificial artificial intelligence?for a reason.Nevertheless, we evaluated our procedure underboth interpretations by comparing against our ownannotations in order to assess the quality of our col-lection, aggregation, and filtering process:1.
As an annotation task: we use Cohen?s ?between the aggregated and filtered data vs.our annotations in the belief that higher above-chance agreement would imply that the aggre-gate annotation reflected collective understand-ing of our definition of sentiment.
Consider-ing the inherently subjective nature of this taskand the interdependencies inherent in within-sentence judgements, Cohen?s ?
is not a defini-tive proof of success or failure.2.
As a retrieval task: Relative to our own an-notations, we use the standard information re-trieval measures of precision, recall, and F-measure (harmonic mean) as well as accuracy.We merge positive and negative annotationsinto a single opinion-bearing class and measurewhether we can retrieve opinion-bearing wordswhile minimizing words that are, in context,not opinion-bearing relative to the given target.
(We do not merge the classes for agreement-based evaluation as there was not much over-lap between positive and negative classifica-tions.)
The particular relative difference be-tween precision and recall will suggest whetherthe workers had a consistent collective under-standing of the task.It should be noted that the MPQA and the JDPA donot report Cohen?s ?
for subjective text spans partlyfor the reason we suggest above: the difficulty of as-sessing objective agreement on a task in which sub-jectivity is inherent and desirable.
There is also alarge class imbalance problem.
Both these effortssubstitute retrieval-based measures into their assess-ment of agreement.We annotated a randomly-selected 30 of the 200highlight groups on our own.
Those 30 had 169annotated words of which 117 were annotated asnone, 35 as positive, and 17 as negative.
The re-sults of our process are summarized in table 1.In the 30 highlight groups, there were 155 totalwords for which a majority consensus (>50%) wasreached.
48 words were determined by us in ourown annotation to have opinion weight (positive ornegative).
There are only 22 annotators who passedCrowdFlower?s quality control.The stringent filter on workers based on their ac-curacy on our minimum-effort gold annotations hasa remarkable effect on the results.
As we excludeworkers, the F-measure and the Cohen?s ?
appearto rise, up to a point.
By definition, each exclu-sion raises the threshold score for acceptance.
Aswe cross the 80% threshold, the performance of thesystem drops noticeably, as the smoothing effect ofvoting is lost.
Opinion-bearing words also reducein number as the threshold rises as some highlightgroups simply have no one voting for them.
Weachieve our best result in terms of Cohen?s ?
ondropping the 7 lowest workers.
We achieve our high-est precision and accuracy after dropping the 10 low-est workers.Between the 7th and 10th underperforming an-notator, we find that precision starts to exceed re-call, possibly due to the loss of retrievable words assome highlight groups lose all their annotators.
Lostwords can be recovered in another round of annota-tion.74Workers excluded No.
of words lost (of 48) Prec/Rec/F Acc Cohen?s ?
Score threshold(prior polarity) N/A 0.87 / 0.38 / 0.53 0.79 -0.26 N/A0 0 0.64 / 0.71 / 0.67 0.79 0.48 0.3331 0 0.64 / 0.71 / 0.67 0.79 0.48 0.4763 0 0.66 / 0.73 / 0.69 0.80 0.51 0.5605 0 0.69 / 0.73 / 0.71 0.81 0.53 0.6747 2 0.81 / 0.76 / 0.79 0.86 0.65 0.71410 9 0.85 / 0.74 / 0.79 0.88 0.54 0.77612 11 0.68 / 0.68 / 0.68 0.82 0.20 0.820Table 1: Results by number of workers excluded from the task.
The prior polarity baseline comes from a lexicon byWilson et al (2005) that is not specific to the IT domain.6 DiscussionWe have been able to show that crowdsourcing avery fine-grained, domain-specific sentiment analy-sis task with a nonstandard, application-specific def-inition of sentiment is possible with careful user in-terface design and mutliple layers of quality control.Our techniques succeed on two different interpreta-tions of the evaluation measure, and we can reclaimany lost words by re-running the task.
We used anelaborate processing pipeline before and after anno-tation in order to accomplish this.
In this section, wediscuss some aspects of the pipeline that led to thesuccess of this technique.6.1 QualityThere are three major aspects of our procedure thatdirectly affect the quality of our results: the first-pass quality control in CrowdFlower, the majority-vote aggregation, and the stringent post hoc filteringof workers.
These interact in particular ways.The first-pass quality control interacts with thestringent filter in that even if it were possible tohave run the stringent filter on CrowdFlower itself,it would probably not have been a good idea.
Al-though we intended the stringent filter to be a min-imum effort, it would have rejected workers tooquickly.
It is technically possible to implement thestringent filtering directly without the CrowdFlowerbuilt-in control, but that would have entailed spend-ing an unpredictable amount more money paying foradditional unwanted annotations from workers.Furthermore, the majority-vote aggregation re-quires that there not be too few annotators; our re-sults show that filtering the workers too aggressivelyharms the aggregation?s smoothing effect.
The les-son we take from this is that it can be beneficial toaccept some amount of ?bad?
with the ?good?
in im-plementing a very subjective crowdsourcing task.6.2 Design decisionsOur successful technique for identifying opinionatedwords was developed after multiple iterations usingother approaches which did not succeed in them-selves but produced outputs that were amenable torefinement, and so these techniques became part ofa larger pipeline.
However, the reasons why they didnot succeed on their own are illustrative of some ofthe challenges in both fine-grained domain-specificopinion annotation and in annotation via crowd-sourcing under highly subjective conditions.6.2.1 Direct annotationWe originally intended to stop with the trained an-notation we described in 4.1.1, but collecting opin-ionated sentences in this corpus turned out to be veryslow.
Despite repeated training rounds, the annota-tors had a tendency to miss a large number of sen-tences that the authors found to be relevant.
On dis-cussion with the annotators, it turned out that thevariable length of the articles made it easy to missrelevant sentences, particularly in the long featurearticles likely to contain opinionated language?akind of ?needle-in-a-haystack?
problem.Even worse, however, the annotators were vari-ably conservative about what constituted an opinion.One annotator produced far fewer annotations thanthe other one?but the majority of her annotationswere also annotated by the other one.
Discussionwith the annotators revealed that one of them simplyhad a tighter definition of what constituted an opin-ion.
Attempts to define opinion explicitly for themstill led to a situations in which one was far moreconservative than the other.756.2.2 Cascaded crowdsourcing techniqueInsofar as we were looking for training data foruse in downstream machine learning techniques,getting uniform sentence-by-sentence coverage ofthe corpus was not necessary.
There are 77K sen-tences in this corpus which mention the relevant ITconcepts; even if only a fraction of them mention theIT concepts with opinionated language, we wouldstill have a potentially rich source of training data.Nevertheless the direct annotation with trainedannotators provided data for selecting candidate sen-tences for a more rapid annotation.
We used theprocess in section 4.1.2 and chose the top-rankedsentences.
Then we constructed a task design thatdivided the annotation into two phases.
In the firstphase, for each candidate sentence, we ask the anno-tator whether or not the sentence contains opinion-ated language about the mentioned IT concept.
(Wepermit ?unsure?
answers.
)In the second phase, for each candidate sentencefor which a majority vote of annotators decided thatthe sentence contained a relevant opinion, we runa second task asking whether particular words (se-lected as per section 4.1.3) were words directly in-volved in the expression of the opinion.We tested this process with the 90 top-rankedsentences.
Four individuals in our laboratory an-swered the ?yes/no/unsure?
question of the firstphase.
However, when we took their pairwise Co-hen?s ?
score, no two got more than approximately0.4.
We also took majority votes of each subset ofthree annotators and found the Cohen?s ?
betweenthem and the fourth.
The highest score was 0.7, butthe score was not stable, and we could not trust theresults enough to move onto the second phase.We also ran this first phase through Amazon Me-chanical Turk.
It turned out that it was far too easyto cheat on this yes/no question, and some workerssimply answered ?yes?
or ?no?
all the time.
Agree-ment scores of a Turker majority vote vs. one of theauthors turned out to yield a Cohen?s ?
of 0.05?completely unacceptable.Discussion with the in-laboratory annotators sug-gested the roots of the problem: it was the sameproblem as with the direct Atlas.ti annotation we re-ported in the previous section.
It was very difficultfor them to agree on what it meant for a sentence tocontain an opinion expressed about a particular con-cept.
Opinions about the nature of opinion rangedfrom very ?conservative?
to very ?liberal.?
Evenexplicit definition with examples led annotators toreach very different conclusions.
Furthermore, thelonger the annotators thought about it, the more con-fused and uncertain they were about the criterion.What is an opinion can itself be a matter of opin-ion.
It became clear that without very tight reviewof annotation and careful task design, asking usersan explicit yes/no question about whether a particu-lar concept has a particular opinion mentioned in aparticular sentence has the potential to induce over-thinking by annotators, despite our variations on thetask.
The difficulty may also lead to a tendency tocheat.
Crowdsourcing allows us to make use of non-expert labour on difficult tasks if we can break thetasks down into simple questions and aggregate non-expert responses, but we needed a somewhat morecomplex task design in order to eliminate the diffi-culty of the task and the tendency to cheat.7 Future workForemost among the avenues for future work is ex-perimentation with other vote aggregration and posthoc filtering schemes.
For example, one type of ex-periment could be the reweighting of votes by an-notator quality rather than the wholesale droppingof annotators.
Another could involve the use ofgeneral-purpose sentiment analysis lexica to bias thevote aggregation in the manner of work in sentimentdomain transfer (Tan et al, 2007).This work also points to the potential for crowd-sourcing in computational linguistics applicationsbeyond opinion mining.
Our task is a sentiment-specific instance of a large class of syntactic relat-edness problems that may suitable for crowdsourc-ing.
One practical application would be in obtainingtraining data for coreference detection.
Another onemay be in the establishment of empirical support fortheories about syntactic structure.AcknowledgementsThis paper is based on work supported by the Na-tional Science Foundation under grant IIS-0729459.76ReferencesPei-Yun Hsueh, Prem Melville, and Vikas Sindhwani.2009.
Data quality from crowdsourcing: a study ofannotation selection criteria.
In Proceedings of theNAACL HLT 2009 Workshop on Active Learning forNatural Language Processing, HLT ?09, Stroudsburg,PA, USA.
Association for Computational Linguistics.Jason S. Kessler, Miriam Eckert, Lyndsay Clark, andNicolas Nicolov.
2010.
The 2010 ICWSM JDPA sent-ment corpus for the automotive domain.
In 4th Int?lAAAI Conference on Weblogs and Social Media DataWorkshop Challenge (ICWSM-DWC 2010).Soo-Min Kim and Eduard Hovy.
2006.
Extracting opin-ions, opinion holders, and topics expressed in onlinenews media text.
In SST ?06: Proceedings of the Work-shop on Sentiment and Subjectivity in Text, pages 1?8,Morristown, NJ, USA.
Association for ComputationalLinguistics.Moshe Koppel and Jonathan Schler.
2006.
The im-portance of neutral examples for learning sentiment.Computational Intelligence, 22(2).Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Found.
Trends Inf.
Retr., 2(1-2).Everett M. Rogers.
2003.
Diffusion of Innovations, 5thEdition.
Free Press.Josef Ruppenhofer, Swapna Somasundaran, and JanyceWiebe.
2008.
Finding the sources and targets ofsubjective expressions.
In Nicoletta Calzolari, KhalidChoukri, Bente Maegaard, Joseph Mariani, Jan Odjik,Stelios Piperidis, and Daniel Tapias, editors, Proceed-ings of the Sixth International Language Resourcesand Evaluation (LREC?08), Marrakech, Morocco.
Eu-ropean Language Resources Association (ELRA).Asad B. Sayeed, Timothy J. Meyer, Hieu C. Nguyen,Olivia Buzek, and Amy Weinberg.
2010a.
Crowd-sourcing the evaluation of a domain-adapted namedentity recognition system.
In Human Language Tech-nologies: The 2010 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics.
Association for Computational Lin-guistics.Asad B. Sayeed, Hieu C. Nguyen, Timothy J. Meyer, andAmy Weinberg.
2010b.
Expresses-an-opinion-about:using corpus statistics in an information extraction ap-proach to opinion mining.
In Proceedings of the 23rdInternational Conference on Computational Linguis-tics, COLING ?10.Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-drew Y. Ng.
2008.
Cheap and fast?but is it good?
:evaluating non-expert annotations for natural languagetasks.
In EMNLP 2008.Songbo Tan, Gaowei Wu, Huifeng Tang, and XueqiCheng.
2007.
A novel scheme for domain-transferproblem in the context of sentiment analysis.
In Pro-ceedings of the sixteenth ACM conference on Con-ference on information and knowledge management,CIKM ?07, New York, NY, USA.Theresa Wilson and Janyce Wiebe.
2005.
Annotatingattributions and private states.
In CorpusAnno ?05:Proceedings of the Workshop on Frontiers in CorpusAnnotations II, Morristown, NJ, USA.
Association forComputational Linguistics.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-levelsentiment analysis.
In HLT/EMNLP.77
