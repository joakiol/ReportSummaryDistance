Coling 2010: Poster Volume, pages 418?426,Beijing, August 2010Word Sense Disambiguation-based Sentence SimilarityChukfong Ho1, Masrah AzrifahAzmi Murad2Department of Information SystemUniversity Putra Malaysiahochukfong@yahoo.com1,masrah@fsktm.upm.edu.my2Rabiah Abdul Kadir, ShyamalaC.
DoraisamyDepartment of MultimediaUniversity Putra Malaysia{rabiah, shya-mala}@fsktm.upm.edu.myAbstractPrevious works tend to compute thesimilarity between two sentences basedon the comparison of their nearestmeanings.
However, the nearestmeanings do not always represent theiractual meanings.
This paper presents amethod which computes the similaritybetween two sentences based on a com-parison of their actual meanings.
This isachieved by transforming an existingmost-outstanding corpus-based measureinto a knowledge-based measure, whichis then integrated with word sense dis-ambiguation.
The experimental resultson a standard data set show that the pro-posed method outperforms the baselineand the improvement achieved is statisti-cally significant at 0.025 levels.1 IntroductionAlthough measuring sentence similarity is acomplicated task, it plays an important role innatural language processing applications.
In textcategorization (Yang and Wen, 2007), docu-ments are retrieved based on similar or relatedfeatures.
In text summarization (Zhou et al,2006) and machine translation (Kauchak andBarzilay, 2006), summaries comparison basedon sentence similarity has been applied forautomatic evaluation.
In text coherence (Lapataand Barzilay, 2005), different sentences arelinked together based on the sequence of similaror related words.Two main issues are investigated in this paper:1) the performance between corpus-based meas-ure and knowledge-based measure, and 2) theinfluence of word sense disambiguation (WSD)on measuring sentence similarity.
WSD is thetask of determining the sense of a polysemousword within a specific context (Wang et al,2006).
Corpus-based methods typically computesentence similarity based on the frequency of aword?s occurrence or the co-occurrence betweencollocated words.
Although these methods bene-fit from the statistical information derived fromthe corpus, this statistical information is closer tosyntactic representation than to semantic repre-sentation.
In comparison, knowledge-basedmethods compute the similarity between twosentences based on the semantic informationcollected from knowledge bases.
However, thissemantic information is applied in a way that,for any two sentences, the comparison of theirnearest meanings is taken into consideration in-stead of the comparison of their actual meanings.More importantly, the nearest meaning does notalways represent the actual meaning.
In this pa-per, a solution is proposed that seeks to addressthese two issues.
Firstly, the most outstandingexisting corpus-based sentence similarity meas-ure is transformed into a knowledge-basedmeasure.
Then, its underlying concept, which isthe comparison of the nearest meanings, is re-placed by another underlying concept, the com-parison of the actual meanings.The rest of this paper is organized into fivesections.
Section 2 presents an overview of therelated works.
Section 3 details the problem ofthe existing method and the improvement of theproposed method.
Section 4 describes the ex-perimental design.
In Section 5, the experimentalresults are discussed.
Finally, the implicationsand contributions are addressed in Section 6.4182 Related WorkIn general, related works can be categorized intocorpus-based, knowledge-based and hybrid-based methods.
Islam and Inkpen (2008) pro-posed a corpus-based sentence similarity meas-ure as a function of string similarity, word simi-larity and common word order similarity (CWO).They claimed that a corpus-based measure hasthe advantage of large coverage when comparedto a knowledge-based measure.
However, thejudgment of similarity is situational and timedependent (Feng et al, 2008).
This suggests thatthe statistical information collected from the pastcorpus may not be relevant to sentences presentin the current corpus.
Apart from that, the role ofstring similarity is to identify any misspelledword.
A malfunction may occur whenever stringsimilarity deals with any error-free sentencesbecause the purpose for its existence is no longervalid.For knowledge-based methods, Li et al (2009)adopted an existing word similarity measure todeal with the similarities of verbs and nounswhile the similarities of adjectives and adverbswere measured only based on simple word over-laps.
However, Achananuparp et al (2008) pre-viously showed that the word overlap-basedmethod performed badly in measuring text simi-larity.
Liu et al (2007) integrated the DynamicTime Warping (DTW) technique into the simi-larity measure to identify the distance betweenwords.
The main drawback of DTW is that thecomputational cost and time will increase pro-portionately with the sentence?s length.
Wee andHassan (2008) proposed a method that takes intoaccount the directionality of similarity in whichthe similarity of any two words is treated asasymmetric.
The asymmetric issue between apair of words was resolved by considering boththe similarity of the first word to the secondword, and vice versa.Corley and Mihalcea (2005) proposed a hy-brid method by combining six existing knowl-edge-based methods.
Mihalcea et al (2006) fur-ther combined those six knowledge-based meth-ods with two corpus-based methods and claimedthat they usually achieved better performance interms of precision and recall respectively.
How-ever, those methods were only combined by us-ing simple average calculation.Perhaps the most closely related work is a re-cently proposed query extension technique.Perez-Ag?era and Zaragoza (2008) made use ofWSD information to map the original querywords and the expansion words to WordNetsenses.
However, without the presence of orconsidering the surrounding words, the meaningof the expansion words alone tend to be repre-sented by their most general meanings instead ofthe disambiguated meanings, which results inthe possibility of WSD information not beinguseful for word expansions.
In contrast to theirwork, which is more suitable to be applied onword-to-word similarity task, the method pro-posed in this paper is more suitable for applica-tion on sentence-to-sentence similarity tasks.Overall, the above-mentioned related workscompute similarity based either on statisticalinformation or on a comparison of the nearestmeanings in terms of words.
None of them com-pute sentence similarity based on the comparisonof actual meanings.
Our proposed method,which is a solution to this issue, will be ex-plained in detail in the next section.3 Sentence SimilarityFigure 1.
The proposed methodOur proposed method shown in Figure 1, is theoutcome of some modifications on an existingmethod, which is also the most outstandingmethod, the Semantic Text Similarity (STS)model (Islam and Inkpen, 2008).
First of all,CWO is removed from STS as the previousworks (Islam and Inkpen, 2007; Islam and Ink-pen, 2008) have shown that the presence ofCWO has no influence on the outcome.
Then,419the corpus-based word similarity function ofSTS is replaced by an existing knowledge-basedword similarity measure called YP (Yang andPowers, 2005).
Finally, the underlying conceptof YP is modified by the integration of WSDand is based on the assumption that any disam-biguated sense of a word represents its actualmeaning.
Thus, the proposed method is alsocalled WSD-STS.3.1 String similarity measureThe string similarity between two words ismeasured by using the following equations:)()()),((),(21jibjaibjaiwlwlwwLCSlwwNLCSv?==(1))()()),((),(2112jibjaibjaiwlwlwwMCLCSlwwNMCLCSv?==(2))()()),((),(23jibjainbjainwlwlwwMCLCSlwwNMCLCSv?==(3)321 33.033.033.0),( vvvYXSimstring ++=        (4)where l(x) represents the length of x; a and brepresent the lengths of sentences X and Y re-spectively after removing stop words; wi repre-sents the i-th word in sequence a; wj representsthe j-th word in sequence b; and Simstring(X,Y)represents the overall string similarity.
The un-derlying concept of string similarity is based oncharacter matching.
NLCS represents the nor-malized version of the traditional longest com-mon subsequence (LCS) technique in which thelengths of the two words are taken into consid-eration.
MCLCS1 represents the modified versionof the traditional LCS in which the string match-ing must start from the first character whileMCLCSn represents the modified version of thetraditional LCS in which the string matchingmay start from any character.
NMCLCS1 andNMCLCSn represent the normalized versions ofMCLCS1 and MCLCSn respectively.
More de-tailed information regarding string similaritymeasure can be found in the original paper (Is-lam and Inkpen, 2008).3.2 Adopted word similarity measureYang and Powers (2005) proposed YP based onthe assumptions that every single path in the hi-erarchical structure of WordNet 1) is identical;and 2) represents the shortest distance betweenany two connected words.
The similarity be-tween two words in sequence a and sequence bcan be represented by the following equation:???????????<?=?=???
?llwwSim itlitbjaiword,0,),(11   (5)where 0 ?
),( bjaiword wwSim ?
1; d is the depth ofLCS; l is the length of path between disambigu-ated aiw  and bjw ; t represents the type of path(hypernyms/hyponym, synonym or holo-nym/meronym) which connects them; ?t repre-sents their path type factor; ?t represents theirpath distance factor; and ?
represents an arbitrarythreshold on the distance introduced for effi-ciency, representing human cognitive limitations.The values of ?t, ?t and ?
have already been em-pirically tuned as 0.9, 0.85 and 12 respectively.More detailed information regarding YP can befound in the original paper (Yang and Powers,2005).In order to adapt a different underlying concept,which is the comparison of actual meanings, lhas to be redefined as the path distance betweendisambiguated words, aiw  and bjw .
Since YPonly differs from the modified version of YP(MYP) in terms of the definition of l, MYP canalso be represented by equation (5).3.3 The proposed measureThe gapGenerally, all the related works in Section 2 canbe abstracted as a function of word similarity.This reflects the importance of a word similaritymeasure in measuring sentence similarity.
How-ever, measuring sentence similarity is always amore complicated task than measuring wordsimilarity.
The reason is that while a word simi-larity measure only involves a single pair ofwords, a sentence similarity measure has to dealwith multiple pairs of words.
In addition, due tothe presence of the surrounding words in a sen-tence, the possible meaning of a word is alwaysbeing restricted (Kolte and Bhirud, 2008).
Thus,without some modifications, the traditional wordsimilarity measures, which are based on the con-cept of a comparison of the nearest meanings,are inapplicable in the context of sentence simi-larity measures.The importance of WSD in reducing the gap420Before performing the comparison of actualmeanings, WSD has to be integrated so that themost suitable sense can be assigned to anypolysemous word.
The importance of WSD canbe investigated by using a simple example.
Con-sider a pair of sentences, collected from Word-Net 2.1, which use two words, ?dog?
and ?cat?
:X: The dog barked all night.Y: What a cat she is!Based on the definition in WordNet 2.1, theword ?dog?
in X is annotated as the first sensewhich means ?a member of the genus Canis(probably descended from the common wolf)that has been domesticated by man since prehis-toric times?.
Meanwhile, the word ?cat?
in Y isannotated as the third sense with the definitionof ?a spiteful woman?s gossip?.
The path dis-tance between ?cat?
and ?dog?
based on theiractual senses is equal to 7.
However, their short-est path distance (SPD), which is based on theirnearest senses, is equal to 4.
SPD is the leastnumber of edges connecting two words in thehierarchical structure of WordNet.
In otherwords, ?cat?
and ?dog?
in X and Y respectively,are not as similar as the one measured by usingSPD.
The presence of the additional path dis-tances is significant as it is almost double theactual path distance between ?cat?
and ?dog?.WSD-STSThe adopted sentence similarity measure, STS,can be represented by the following equations:abbaYXSimci isemantic 2)()(),( 1?
+?+= = ??
(6)2),(),(),( YXSimYXSimYXSIM stringsmeantic += (7)where for equation (6): ?
represents the numberof overlapped words between the words in se-quence a and sequence b; c represents the num-ber of semantically matched words between thewords in sequence a and sequence b, in which c= a if a < b or c = b if b < a, ?i represents thehighest matching similarity score of i-th word inthe shorter sequence with respect to one of thewords in the longer sequence; and ??
representsthe sum of the highest matching similarity scorebetween the words in sequence a and sequenceb.For STS, the similarity between two words ismeasured by using a corpus-based measure.
ForWSD-STS, this corpus-based measure is re-placed by MYP.
Finally, the overall sentencesimilarity is represented by equation (7).4 Experimental Design4.1 Data setLi et al, (2006) constructed a data set whichconsists of 65 pairs of human-rated sentences byapplying the similar experimental design for cre-ating the standard data set for the word similaritytask (Rubenstein and Goodenough, 1965).
These65 sentence pairs were the definitions collectedfrom the Collin Cobuild Dictionary.
Out ofthese, 30 sentence pairs with rated similarityscores that ranged from 0.01 to 0.96 were se-lected as test data set.
The corresponding 30word pairs for these 30 sentence pairs are shownin the second column of Table 1.
A further set of66 sentence pairs is still under development andit will be combined with the existing data set inthe future (O?Shea et al, 2008b).4.2 ProcedureFirstly, Stanford parser 1  is used to parse eachsentence and to tag each word with a part ofspeech (POS).
Secondly, Structural SemanticInterconnections2 (SSI), which is an online WSDsystem, is used to disambiguate and to assign asense for each word in the 30 sentences based onthe assigned POS.
SSI is applied based on theassumption that it is able to perform WSD cor-rectly.
The main reason for choosing SSI to per-form WSD is its promising results reported in astudy by Navigli and Verladi (2006).
Thirdly, allthe stop words which exist in these 30 pairs ofsentences are removed.
It is important to notethat the 100 most frequent words collected fromBritish National Corpus (BNC) were applied asthe stop words list on the baseline, STS.
How-ever, due to the limited accessibility to BNC, adifferent stop words list 3 , which is availableonline, is applied in this paper.1http://nlp.stanford.edu/software/lex-parser.shtml2http://lcl.uniroma1.it/ssi3http://www.translatum.gr/forum/index.php?topic=2476.0421Table 1.
Data Set ResultsFinally, the remaining content words arelemmatized by using Natural Language Toolkit4(NLTK).
Nevertheless, those words which canbe found in WordNet and which have differentdefinitions from their lemmatized form will beexcluded from lemmatization.
For instance,Cooking[NN] can be a great art.The word in the bracket represents the taggedPOS for its corresponding word.
Since based onthe definitions provided by WordNet, ?cooking?,which is tagged as a noun, has a different mean-ing from its lemmatized form ?cook?, which isalso tagged as a noun.
Therefore, ?cooking?
isexcluded from lemmatization.4.3 Experimental conditionsSentence similarity is measured under the fol-lowing three conditions:4http://www.nltk.org/?
OLP-STS: A modified version of thebaseline, STS (Islam and Inkpen, 2008),in which it only relies on the presence ofoverlapped words.
This means that thecomponent ?
= ici ?1 , which represents theword similarity, is removed from equa-tion (6).?
SPD-STS: The corpus-based word simi-larity measure of the baseline, STS,which is represented by ?
= ici ?1  in equa-tion (6), is replaced by a knowledge-based word similarity measure, YP.?
WSD-STS: A modified version of SPD-STS in which the knowledge-basedmeasure, YP, is replaced by MYP.As mentioned in Section 4.2, different stopwords lists were applied between the baselineand the proposed methods under different ex-422perimental conditions in this paper.
Since thisissue may be questioned due to the unfair com-parison, the performance of WSD-STS is evalu-ated on top of a number of different stop wordslists which are available online in order to inves-tigate any influence which may be caused bystop words list.5 Results and DiscussionTable 1 presents the similarity scores obtainedfrom the mean of human ratings, the benchmarks,and different experimental conditions of the pro-posed methods.
Figure 2 presents the corre-sponding Pearson correlation coefficients ofvarious measures as listed in Table 1.Figure 2.
Pearson Correlation CoefficientFigure 2 shows that STS appears to be themost outstanding measure among the existingworks with a correlation coefficient of 0.853.However, Figure 2 also shows that both the pro-posed methods in this paper, WSD-STS andSPD-STS, outperform STS.
This result indicatesthat knowledge-based method tends to performbetter than a corpus-based method.
The reason isthat a knowledge base is much closer to humanrepresentation of knowledge (WordNet is theknowledge base applied in this paper) than acorpus.
A corpus only reflects the usage of lan-guages and words while WordNet is a model ofhuman knowledge constructed by many expertlexicographers (Li et al, 2006).
In other words, acorpus is more likely to provide unprocessedraw data while a knowledge base tends to pro-vide ready-to-use information.The results of the performance of the twoproposed methods are as expected.
SPD-STSachieved a bigger but statistically insignificantimprovement while WSD-STS achieved asmaller but statistically significant improvementat 0.01 levels.
The significance of a correlationis calculated by using an online calculator, Vas-sarStats5.
The reason for the variance in the out-comes between SPD-STS and WSD-STS is ob-vious; it is the difference in terms of their under-lying concepts.
In other words, sentence similar-ity computation, which is based on a comparisonof the nearest meanings, results in insignificantimprovement while sentence similarity computa-tion, which is based on a comparison of actualmeanings, achieves statistically significant im-provement.
These explanations indicate thatWSD is essential in confirming the validity ofthe task of measuring sentence similarity.Figure 2 also reveals that a relatively low cor-relation is achieved by OLP-STS.
This is not atall surprising since Achananuparp et al (2008)has already demonstrated that the overlappedword-based method tends to perform badly inmeasuring sentence similarity.
However, it isinteresting to find that the difference in perform-ance between STS and OLP-STS is very small.This indirectly suggests that the presence of thestring similarity measure and the corpus-basedword similarity measure has only a slight im-provement on the performance of OLP-STS.Figure 3.
The performance of the WSD-SPDversus different stop words listsNext, in order to address the issue of unfaircomparison due to the usage of different stopwords lists, the performance of WSD-SPD hasbeen evaluated on top of a number of different5http://faculty.vassar.edu/lowry/rdiff.html?423stop words lists.
A total of five stop words listswith different lengths (896, 2237, 319, 5718 and6599) of stop words were applied.
The perform-ances of WSD-SPD with respect to these stopwords lists are portrayed in Figure 3.
They arefound to be in a comparable condition.
This re-sult connotes that the influence caused by theusage of different stop words lists is small andcan be ignored.
Hence, the unfair comparisonbetween our proposed method and the baselineshould not be treated as an issue for the bench-marking purpose of this paper.On the other hand, although an assumption ismade that SSI performs WSD correctly, we no-ticed that not all the words were disambiguatedconfidently.
The confident scores which wereassigned to the disambiguated words by SSIrange between 30% and 100%.
These confidentscores reflect the confidence of SSI in perform-ing WSD.
Thus, it is possible that some of thosewords which were assigned with low confidentscores were disambiguated incorrectly.
Conse-quently, the final sentence similarity score islikely to be affected negatively.
In order to re-duce the negative effect which may be caused byincorrect WSD, any words pair which is not con-fidently disambiguated is assigned the similarityscore based on the concept of comparing thenearest meanings instead of comparing the ac-tual meanings.
In other words, WSD-STS andSPD-STS are combined and results in WSD-SPD.
The performance of WSD-SPD across arange of confident scores is essential in reveal-ing the impact of WSD and SPD on the task ofmeasuring sentence similarity.Figure 4 outlines the performance achieved byWSD-SPD across different confident scores as-signed by SSI.
The confident score of at least 0.7is identified as the threshold in which SSI opti-mizes its performance.
The performance ofWSD-SPD is found to be statistically insignifi-cant for those confident scores above the thresh-old.
The explanation for this phenomenon can be6http://msdn.microsoft.com/en-us/library/bb164590.aspx7http://snowball.tartarus.org/algorithms/english/stop.txt8http://truereader.com/manuals/onix/stopwords2.html9http://www.link-assistant.com/seo-stop-words.htmlfound in Figure 5.
Figure 5 illustrates the per-centage of the composition between WSD andSPD in WSD-SPD.
It is obvious that once theportion of WSD exceeds the portion of SPD, theperformance of WSD-SPD is found to be statis-tically insignificant.
This finding suggests thatSPD, which reflects the application of the con-cept of nearest meaning comparison, is likely todecrease the validity of sentence similaritymeasurement while WSD, which reflects theapplication of the concept of actual meaningcomparison, is essential in confirming the valid-ity of sentence similarity measurement.Figure 4.
The performance of WSD-SPD versusconfident scoresFigure 5.
The percentage of WSD/SPD versusconfident scoreThe trend of the performance of string simi-larity measure and word similarity measure withrespect to different weight assignments is de-lineated in Figure 6.
The lowest correlation of0.856 is obtained when only the string similarityfunction is considered while the word similarity424function is excluded.
A better performance isachieved by taking the two measures into con-sideration where more weight is given to themeasure of word similarity.
This trend intimatesthat the string similarity measure offers a smallercontribution in measuring sentence similaritythan word similarity measure.
In contrast to aword similarity measure, a string similaritymeasure is purposely proposed to address theissue of misspelled words.
Since the data set ap-plied in this experiment does not contain anymisspelled words, it is obvious that a string simi-larity measure performs badly.
In addition, theunderlying concept of string similarity is ques-tionable.
Does it make sense to determine thesimilarity of two words based on the matchingbetween their characters or the matching of thesequence of characters?
Consider four pairs ofwords: ?play?
versus ?pray?, ?plant?
versus?plane?, ?plane?
versus ?plan?
and ?stationary?versus ?stationery?.
These word pairs are highlysimilar in terms of characters but they are se-mantically dissimilar or unrelated.Figure 6.
The performance of the differentmeasures versus the weight between string simi-larity and word similarityFigure 6 also depicts that the combination ofword similarity measure (70%) and string simi-larity measure (30%) performs better than themeasure which is solely based on word similar-ity function.
It is obvious that the difference iscaused by the presence of string similaritymeasure.
The combination assigns similarityscores to all word pairs while the word similaritymeasure only assigns similarity scores to thoseword pairs which fulfill two requirements: 1)any two words which share an identical POS,and 2) any two words which must either be apair of nouns or a pair of verbs.
In fact, adjec-tives and adverbs do contribute to representingthe meaning of a sentence although their contri-bution is relatively smaller than the contributionof nouns and verbs (Liu et al, 2007; Li et al,2009).
Therefore, by ignoring the presence ofadjectives and adverbs, the performance willdefinitely be affected negatively.6 ConclusionThis paper has presented a knowledge-basedmethod which measures the similarity betweentwo sentences based on their actual meaningcomparison.
The result shows that the proposedmethod, which is a knowledge-based measure,performs better than the baseline, which is acorpus-based measure.
The improvement ob-tained is statistically significant at 0.025 levels.This result also shows that the validity of theoutput of measuring the similarity of two sen-tences can be improved by comparing their ac-tual meanings instead of their nearest meanings.These are achieved by transforming the baselineinto a knowledge-based method and then by in-tegrating WSD into the adopted knowledge-based measure.Although the proposed method significantlyimproves the quality of measuring sentencesimilarity, it has a limitation.
The proposedmethod only measures the similarity betweentwo words with an identical part of speech(POS) and these two words must either be a pairof nouns or a pair of verbs.
By ignoring the im-portance of adjectives and adverbs, and the rela-tionship between any two words with differentPOS, a slight decline is observed in the obtainedresult.
In future research, these two issues willbe addressed by taking into account the related-ness between two words instead of only consid-ering their similarity.ReferencesAchananuparp, Palakorn, Xiao-Hua Hu, and Xiao-Jiong Shen.
2008.
The Evaluation of SentenceSimilarity Measures.
In Proceedings of the 10thInternational Conference on Data Warehousing425and Knowledge Discovery (DaWak), pages 305-316, Turin, Italy.Corley, Courtney, and Rada Mihalcea.
2005.
Measur-ing the Semantic Similarity of Texts.
In Proceed-ings of the ACL Workshop on Empirical Modelingof Semantic Equivalence and Entailment, pages48-55, Ann Arbor.Feng, Jin, Yi-Ming Zhou, and Trevor Martin.
2008.Sentence Similarity based on Relevance.
In Pro-ceedings of IPMU, pages 832-839.Islam, Aminul, and Diana Inkpen.
2007.
SemanticSimilarity of Short Texts.
In Proceedings ofRANLP, pages 291-297.Islam, Aminul, and Diana Inkpen.
2008.
SemanticText Similarity Using Corpus-Based Word Simi-larity and String Similarity.
ACM Transactions onKnowledge Discovery from Data, 2(2):10.Kauchak, David, and Regina Barzilay.
2006.
Para-phrasing for Automatic Evaluation.
In Proceedingsof HLT-NAACL, pages 455-462, New York.Kolte, Sopan Govind, and Sunil G. Bhirud.
2008.Word Sense Disambiguation using WordNet Do-mains.
In The First International Conference onEmerging Trends in Engineering and Technology,pages 1187-1191.Lapata, Mirella, and Regina Barzilay.
2005.
Auto-matic Evaluation of Text Coherence: Models andRepresentations.
In Proceedings of the 19th Inter-national Joint Conference on Artificial Intelli-gence.Li, Lin, Xia Hu, Bi-Yun Hu, Jun Wang, and Yi-MingZhou.
2009.
Measuring Sentence Similarity fromDifferent Aspects.
In Proceedings of the Eighth In-ternational Conference on Machine Learning andCybernetics, pages 2244-2249.Li, Yu-Hua, David McLean, Zuhair A. Bandar, JamesD.O'Shea, and Keeley Crockett.
2006.
SentenceSimilarity Based on Semantic Nets and CorpusStatistics.
IEEE Transactions on Knowledge andData Engineering, 18(8):1138-50.Liu, Xiao-Ying, Yi-Ming Zhou, and Ruo-Shi Zheng.2007.
Sentence Similarity based on Dynamic TimeWarping.
In The International Conference on Se-mantic Computing, pages 250-256.Mihalcea, Rada, Courtney Corley, and Carlo Strap-parava.
2006.
Corpus-based and Knowledge-basedMeasures of Text Semantic Similarity.
In Proceed-ings of the American Association for Artificial In-telligence.Navigli, Roberto, and Paola Velardi.
2005.
StructuralSemantic Interconnections: A Knowledge-BasedApproach to Word Sense Disambiguation.
IEEETransactions on Pattern Analysis and Machine In-telligence 27(7):1075-86.O'Shea, James, Zuhair Bandar, Keeley Crockett, andDavid McLean.
2008a.
A Comparative Study ofTwo Short Text Semantic Similarity Measures.
InKES-AMSTA, LNAI: Springer Berlin / Heidelberg.O'Shea, James, Zuhair Bandar, Keeley Crockett, andDavid McLean.
2008b.
Pilot Short Text SemanticSimilarity Benchmark Data Set: Full Listing andDescription.Perez-Aguera, Jose R., and Hugo Zaragoza.
2008.UCM-Y!R at Clef 2008 Robust and WSD Tasks.In Working Notes for CLEF Workshop.Rubenstein, Herbert, and John B. Goodenough.
1965.Contextual Correlates of Synonymy.
Communica-tions of the ACM, pages 627-633.Wang, Yao-Feng, Yue-Jie Zhang, Zhi-Ting Xu, andTao Zhang.
2006.
Research on Dual Pattern of Un-supervised and Supervised Word Sense Disam-biguation.
In Proceedings of the Fifth Interna-tional Conference on Machine Learning and Cy-bernetics, pages 2665-2669.Wee, Leong Chee, and Samer Hassan.
2008.
Exploit-ing Wikipedia for Directional Inferential TextSimilarity.
In Proceedings of Fifth InternationalConference on Information Technology: NewGenerations, pages 686-691.Yang, Cha, and Jun Wen.
2007.
Text CategorizationBased on Similarity Approach.
In Proceedings ofInternational Conference on Intelligence Systemsand Knowledge Engineering (ISKE).Yang, Dong-Qiang, and David M.W.
Powers.
2005.Measuring Semantic Similarity in the Taxonomyof WordNet.
In Proceedings of the 28th Austral-asian Computer Science Conference, pages 315-332, Australia.Zhou, Liang, Chin-Yew Lin, Dragos StefanMunteanu, and Eduard Hovy.
2006.
ParaEval: Us-ing Paraphrases to Evaluate Summaries Automati-cally.
In Proceedings of Human Language Tech-nology Conference of the North American Chapterof the ACL, pages 447-454, New York.426
