Proceedings of ACL-08: HLT, pages 174?182,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsCorrecting Misuse of Verb FormsJohn Lee and Stephanie SeneffSpoken Language SystemsMIT Computer Science and Artificial Intelligence LaboratoryCambridge, MA 02139, USA{jsylee,seneff}@csail.mit.eduAbstractThis paper proposes a method to correct En-glish verb form errors made by non-nativespeakers.
A basic approach is template match-ing on parse trees.
The proposed method im-proves on this approach in two ways.
Toimprove recall, irregularities in parse treescaused by verb form errors are taken into ac-count; to improve precision, n-gram countsare utilized to filter proposed corrections.Evaluation on non-native corpora, represent-ing two genres and mother tongues, showspromising results.1 IntroductionIn order to describe the nuances of an action, a verbmay be associated with various concepts such astense, aspect, voice, mood, person and number.
Insome languages, such as Chinese, the verb itself isnot inflected, and these concepts are expressed viaother words in the sentence.
In highly inflected lan-guages, such as Turkish, many of these concepts areencoded in the inflection of the verb.
In betweenthese extremes, English uses a combination of in-flections (see Table 1) and ?helping words?, or aux-iliaries, to form complex verb phrases.It should come as no surprise, then, that the mis-use of verb forms is a common error category forsome non-native speakers of English.
For example,in the Japanese Learners of English corpus (Izumi etal., 2003), errors related to verbs are among the mostfrequent categories.
Table 2 shows some sentenceswith these errors.Form Examplebase (bare) speakbase (infinitive) to speakthird person singular speakspast spoke-ing participle speaking-ed participle spokenTable 1: Five forms of inflections of English verbs (Quirket al, 1985), illustrated with the verb ?speak?.
The baseform is also used to construct the infinitive with ?to?.
Anexception is the verb ?to be?, which has more forms.A system that automatically detects and correctsmisused verb forms would be both an educationaland practical tool for students of English.
It mayalso potentially improve the performance of ma-chine translation and natural language generationsystems, especially when the source and target lan-guages employ very different verb systems.Research on automatic grammar correction hasbeen conducted on a number of different parts-of-speech, such as articles (Knight and Chander, 1994)and prepositions (Chodorow et al, 2007).
Errors inverb forms have been covered as part of larger sys-tems such as (Heidorn, 2000), but we believe thattheir specific research challenges warrant more de-tailed examination.We build on the basic approach of template-matching on parse trees in two ways.
To improve re-call, irregularities in parse trees caused by verb formerrors are considered; to improve precision, n-gramcounts are utilized to filter proposed corrections.We start with a discussion on the scope of our174task in the next section.
We then analyze the spe-cific research issues in ?3 and survey previous workin ?4.
A description of our data follows.
Finally, wepresent experimental results and conclude.2 BackgroundAn English verb can be inflected in five forms (seeTable 1).
Our goal is to correct confusions amongthese five forms, as well as the infinitive.
Theseconfusions can be viewed as symptoms of one oftwo main underlying categories of errors; roughlyspeaking, one category is semantic in nature, and theother, syntactic.2.1 Semantic ErrorsThe first type of error is concerned with inappropri-ate choices of tense, aspect, voice, or mood.
Thesemay be considered errors in semantics.
In the sen-tence below, the verb ?live?
is expressed in the sim-ple present tense, rather than the perfect progressive:He *lives there since June.
(1)Either ?has been living?
or ?had been living?
maybe the valid correction, depending on the context.
Ifthere is no temporal expression, correction of tenseand aspect would be even more challenging.Similarly, correcting voice and mood often re-quires real-world knowledge.
Suppose one wantsto say ?I am prepared for the exam?, but writes ?Iam preparing for the exam?.
Semantic analysis ofthe context would be required to correct this kind oferror, which will not be tackled in this paper1.1If the input is ?I am *prepare for the exam?, however, wewill attempt to choose between the two possibilities.Example UsageI take a bath and *reading books.
FINITEI can?t *skiing well , but ... BASEmdWhy did this *happened?
BASEdoBut I haven?t *decide where to go.
EDperfI don?t want *have a baby.
INFverbI have to save my money for *ski.
INGprepMy son was very *satisfy with ... EDpassI am always *talk to my father.
INGprogTable 2: Sentences with verb form errors.
The intendedusages, shown on the right column, are defined in Table 3.2.2 Syntactic ErrorsThe second type of error is the misuse of verb forms.Even if the intended tense, aspect, voice and moodare correct, the verb phrase may still be constructederroneously.
This type of error may be further sub-divided as follows:Subject-Verb Agreement The verb is not correctlyinflected in number and person with respect tothe subject.
A common error is the confusionbetween the base form and the third person sin-gular form, e.g.,He *have been living there since June.
(2)Auxiliary Agreement In addition to the modal aux-iliaries, other auxiliaries must be used whenspecifying the perfective or progressive aspect,or the passive voice.
Their use results in a com-plex verb phrase, i.e., one that consists of twoor more verb constituents.
Mistakes arise whenthe main verb does not ?agree?
with the aux-iliary.
In the sentence below, the present per-fect progressive tense (?has been living?)
is in-tended, but the main verb ?live?
is mistakenlyleft in the base form:He has been *live there since June.
(3)In general, the auxiliaries can serve as a hint tothe intended verb form, even as the auxiliaries?has been?
in the above case suggest that theprogressive aspect was intended.Complementation A nonfinite clause can serve ascomplementation to a verb or to a preposition.In the former case, the verb form in the clauseis typically an infinitive or an -ing participle; inthe latter, it is usually an -ing participle.
Hereis an example of a wrong choice of verb formin complementation to a verb:He wants *live there.
(4)In this sentence, ?live?, in its base form, shouldbe modified to its infinitive form as a comple-mentation to the verb ?wants?.This paper focuses on correcting the above threeerror types: subject-verb agreement, auxiliary agree-ment, and complementation.
Table 3 gives a com-plete list of verb form usages which will be covered.175Form Usage Description ExampleBase Form as BASEmd After modals He may call.
May he call?Bare Infinitive BASEdo ?Do?-support/-periphrasis; He did not call.
Did he call?emphatic positive I did call.Base or 3rd person FINITE Simple present or past tense He calls.Base Form as INFverb Verb complementation He wants her to call.to-Infinitive-ing INGprog Progressive aspect He was calling.
Was he calling?participle INGverb Verb complementation He hated calling.INGprep Prepositional complementation The device is designed for calling-ed EDperf Perfect aspect He has called.
Has he called?participle EDpass Passive voice He was called.
Was he called?Table 3: Usage of various verb forms.
In the examples, the italized verbs are the ?targets?
for correction.
In comple-mentations, the main verbs or prepositions are bolded; in all other cases, the auxiliaries are bolded.3 Research IssuesOne strategy for correcting verb form errors is toidentify the intended syntactic relationships betweenthe verb in question and its neighbors.
For subject-verb agreement, the subject of the verb is obviouslycrucial (e.g., ?he?
in (2)); the auxiliary is relevantfor resolving auxiliary agreement (e.g., ?has been?in (3)); determining the verb that receives the com-plementation is necessary for detecting any comple-mentation errors (e.g., ?wants?
in (4)).
Once theseitems are identified, most verb form errors may becorrected in a rather straightforward manner.The success of this strategy, then, hinges on accu-rate identification of these items, for example, fromparse trees.
Ambiguities will need to be resolved,leading to two research issues (?3.2 and ?3.3).3.1 AmbiguitiesThe three so-called primary verbs, ?have?, ?do?
and?be?, can serve as either main or auxiliary verbs.The verb ?be?
can be utilized as a main verb, but alsoas an auxiliary in the progressive aspect (INGprog inTable 3) or the passive voice (EDpass).
The three ex-amples below illustrate these possibilities:This is work not play.
(main verb)My father is working in the lab.
(INGprog)A solution is worked out.
(EDpass)These different roles clearly affect the forms re-quired for the verbs (if any) that follow.
Dis-ambiguation among these roles is usually straight-forward because of the different verb forms (e.g.,?working?
vs.
?worked?).
If the verb forms are in-correct, disambiguation is made more difficult:This is work not play.My father is *work in the lab.A solution is *work out.Similar ambiguities are introduced by the other pri-mary verbs2.
The verb ?have?
can function as anauxiliary in the perfect aspect (EDperf ) as well asa main verb.
The versatile ?do?
can serve as ?do?-support or add emphasis (BASEdo), or simply act asa main verb.3.2 Automatic ParsingThe ambiguities discussed above may be expectedto cause degradation in automatic parsing perfor-mance.
In other words, sentences containing verbform errors are more likely to yield an ?incorrect?parse tree, sometimes with significant differences.For example, the sentence ?My father is *work inthe laboratory?
is parsed (Collins, 1997) as:(S (NP My father)(VP is (NP work))(PP in the laboratory))2The abbreviations ?s (is or has) and ?d (would or had) com-pound the ambiguities.176The progressive form ?working?
is substituted withits bare form, which happens to be also a noun.The parser, not unreasonably, identifies ?work?
asa noun.
Correcting the verb form error in this sen-tence, then, necessitates considering the noun that isapparently a copular complementation.Anecdotal observations like this suggest that onecannot use parser output naively3.
We will show thatsome of the irregularities caused by verb form errorsare consistent and can be taken into account.One goal of this paper is to recognize irregular-ities in parse trees caused by verb form errors, inorder to increase recall.3.3 OvergeneralizationOne potential consequence of allowing for irregu-larities in parse tree patterns is overgeneralization.For example, to allow for the ?parse error?
in ?3.2and to retrieve the word ?work?, every determiner-less noun would potentially be turned into an -ingparticiple.
This would clearly result in many invalidcorrections.
We propose using n-gram counts as afilter to counter this kind of overgeneralization.A second goal is to show that n-gram counts caneffectively serve as a filter, in order to increase pre-cision.4 Previous ResearchThis section discusses previous research on process-ing verb form errors, and contrasts verb form errorswith those of the other parts-of-speech.4.1 Verb FormsDetection and correction of grammatical errors, in-cluding verb forms, have been explored in variousapplications.
Hand-crafted error production rules(or ?mal-rules?
), augmenting a context-free gram-mar, are designed for a writing tutor aimed at deafstudents (Michaud et al, 2000).
Similar strategieswith parse trees are pursued in (Bender et al, 2004),and error templates are utilized in (Heidorn, 2000)for a word processor.
Carefully hand-crafted rules,when used alone, tend to yield high precision; they3According to a study on parsing ungrammatical sen-tences (Foster, 2007), subject-verb and determiner-noun agree-ment errors can lower the F-score of a state-of-the-art prob-abilistic parser by 1.4%, and context-sensitive spelling errors(not verbs specifically), by 6%.may, however, be less equipped to detect verb formerrors within a perfectly grammatical sentence, suchas the example given in ?3.2.An approach combining a hand-crafted context-free grammar and stochastic probabilities is pursuedin (Lee and Seneff, 2006), but it is designed for arestricted domain only.
A maximum entropy model,using lexical and POS features, is trained in (Izumiet al, 2003) to recognize a variety of errors.
Itachieves 55% precision and 23% recall overall, onevaluation data that partially overlap with those ofthe present paper.
Unfortunately, results on verbform errors are not reported separately, and compar-ison with our approach is therefore impossible.4.2 Other Parts-of-speechAutomatic error detection has been performed onother parts-of-speech, e.g., articles (Knight andChander, 1994) and prepositions (Chodorow et al,2007).
The research issues with these parts-of-speech, however, are quite distinct.
Relative to verbforms, errors in these categories do not ?disturb?
theparse tree as much.
The process of feature extractionis thus relatively simple.5 Data5.1 Development DataTo investigate irregularities in parse tree patterns(see ?3.2), we utilized the AQUAINT Corpus of En-glish News Text.
After parsing the corpus (Collins,1997), we artificially introduced verb form errorsinto these sentences, and observed the resulting ?dis-turbances?
to the parse trees.For disambiguation with n-grams (see ?3.3), wemade use of the WEB 1T 5-GRAM corpus.
Preparedby Google Inc., it contains English n-grams, up to5-grams, with their observed frequency counts froma large number of web pages.5.2 Evaluation DataTwo corpora were used for evaluation.
They wereselected to represent two different genres, and twodifferent mother tongues.JLE (Japanese Learners of English corpus) Thiscorpus is based on interviews for the Stan-dard Speaking Test, an English-language pro-ficiency test conducted in Japan (Izumi et al,177Input Hypothesized CorrectionNone Valid Invalidw/ errors false neg true pos inv posw/o errors true neg false posTable 4: Possible outcomes of a hypothesized correction.2003).
For 167 of the transcribed interviews,totalling 15,637 sentences4, grammatical errorswere annotated and their corrections provided.By retaining the verb form errors5, but correct-ing all other error types, we generated a test setin which 477 sentences (3.1%) contain subject-verb agreement errors, and 238 (1.5%) containauxiliary agreement and complementation er-rors.HKUST This corpus6 of short essays was col-lected from students, all native Chinese speak-ers, at the Hong Kong University of Scienceand Technology.
It contains a total of 2556 sen-tences.
They tend to be longer and have morecomplex structures than their counterparts inthe JLE.
Corrections are not provided; how-ever, part-of-speech tags are given for the orig-inal words, and for the intended (but unwrit-ten) corrections.
Implications on our evaluationprocedure are discussed in ?5.4.5.3 Evaluation MetricFor each verb in the input sentence, a change in verbform may be hypothesized.
There are five possibleoutcomes for this hypothesis, as enumerated in Ta-ble 4.
To penalize ?false alarms?, a strict definitionis used for false positives ?
even when the hypoth-esized correction yields a good sentence, it is stillconsidered a false positive so long as the originalsentence is acceptable.It can sometimes be difficult to determine whichwords should be considered verbs, as they are not4Obtained by segmenting (Reynar and Ratnaparkhi, 1997)the interviewee turns, and discarding sentences with only oneword.
The HKUST corpus was processed likewise.5Specifically, those tagged with the ?v fml?, ?v fin?
(cov-ering auxiliary agreement and complementation) and ?v agr?
(subject-verb agreement) types; those with semantic errors (see?2.1), i.e.
?v tns?
(tense), are excluded.6Provided by Prof. John Milton, personal communication.clearly demarcated in our evaluation corpora.
Wewill thus apply the outcomes in Table 4 at the sen-tence level; that is, the output sentence is considereda true positive only if the original sentence containserrors, and only if valid corrections are offered forall errors.The following statistics are computed:Accuracy The proportion of sentences which, afterbeing treated by the system, have correct verbforms.
That is, (true neg + true pos) dividedby the total number of sentences.Recall Out of all sentences with verb form errors,the percentage whose errors have been success-fully corrected by the system.
That is, true posdivided by (true pos+ false neg + inv pos).Detection Precision This is the first of two typesof precision to be reported, and is defined asfollows: Out of all sentences for which thesystem has hypothesized corrections, the per-centage that actually contain errors, without re-gard to the validity of the corrections.
That is,(true pos + inv pos) divided by (true pos +inv pos + false pos).Correction Precision This is the more stringenttype of precision.
In addition to successfullydetermining that a correction is needed, the sys-tem must offer a valid correction.
Formally, it istrue pos divided by (true pos + false pos +inv pos).5.4 Evaluation ProcedureFor the JLE corpus, all figures above will be re-ported.
The HKUST corpus, however, will not beevaluated on subject-verb agreement, since a sizablenumber of these errors are induced by other changesin the sentence7.Furthermore, the HKUST corpus will requiremanual evaluation, since the corrections are not an-notated.
Two native speakers of English were giventhe edited sentences, as well as the original input.For each pair, they were asked to select one of fourstatements: one of the two is better, or both areequally correct, or both are equally incorrect.
The7e.g., the subject of the verb needs to be changed from sin-gular to plural.178Expected Tree {?usage?,...}
Tree disturbed by substitution [?crr?
?
?err?
]{INGprog,EDpass} A dog is [sleeping?sleep].
I?m [living?live] in XXX city.VPbe VPcrr/{VBG,VBN}VPbe NPerr/NNVPbe ADJPerr/JJ{INGverb,INFverb} I like [skiing?ski] very much; She likes to [go?going] aroundVP*/V SGVPcrr/{VBG,TO} ...VP*/V NPerr/NNVP*/V PPto/TO SGVPerr/VBGINGprep I lived in France for [studying?study] French language.PP*/IN SGVPcrr/VBG ...PP*/IN NPerr/NNTable 5: Effects of incorrect verb forms on parse trees.
The left column shows trees normally expected for the indicatedusages (see Table 3).
The right column shows the resulting trees when the correct verb form ?crr?
is replaced by ?err?.Detailed comments are provided in ?6.1.correction precision is thus the proportion of pairswhere the edited sentence is deemed better.
Accu-racy and recall cannot be computed, since it was im-possible to distinguish syntactic errors from seman-tic ones (see ?2).5.5 BaselinesSince the vast majority of verbs are in their cor-rect forms, the majority baseline is to propose nocorrection.
Although trivial, it is a surprisinglystrong baseline, achieving more than 98% for aux-iliary agreement and complementation in JLE, andjust shy of 97% for subject-verb agreement.For auxiliary agreement and complementation,the verb-only baseline is also reported.
It attemptscorrections only when the word in question is actu-ally tagged as a verb.
That is, it ignores the spuriousnoun- and adjectival phrases in the parse tree dis-cussed in ?3.2, and relies only on the output of thepart-of-speech tagger.6 ExperimentsCorresponding to the issues discussed in ?3.2 and?3.3, our experiment consists of two main steps.6.1 Derivation of Tree PatternsBased on (Quirk et al, 1985), we observed tree pat-terns for a set of verb form usages, as summarizedin Table 3.
Using these patterns, we introduced verbform errors into AQUAINT, then re-parsed the cor-pus (Collins, 1997), and compiled the changes in the?disturbed?
trees into a catalog.179N -gram Examplebe {INGprog, The dog is sleeping.EDpass} ?
The door is open.verb {INGverb, I need to do this.INFverb} ?
I need beef for the curry.verb1 *ing enjoy reading andand {INGverb, going to pachinkoINFverb} go shopping and have dinnerprep for studying French language{INGprep} ?
a class for sign languagehave I have rented a video{EDperf} * I have lunch in GinzaTable 6: The n-grams used for filtering, with examplesof sentences which they are intended to differentiate.
Thehypothesized usages (shown in the curly brackets) as wellas the original verb form, are considered.
For example,the first sentence is originally ?The dog is *sleep.?
Thethree trigrams ?is sleeping .
?, ?is slept .?
and ?is sleep .
?are compared; the first trigram has the highest count, andthe correction ?sleeping?
is therefore applied.A portion of this catalog8 is shown in Table 5.Comments on {INGprog,EDpass} can be found in?3.2.
Two cases are shown for {INGverb,INFverb}.In the first case, an -ing participle in verb comple-mentation is reduced to its base form, resulting ina noun phrase.
In the second, an infinitive is con-structed with the -ing participle rather than the baseform, causing ?to?
to be misconstrued as a preposi-tion.
Finally, in INGprep, an -ing participle in prepo-sition complementation is reduced to its base form,and is subsumed in a noun phrase.6.2 Disambiguation with N-gramsThe tree patterns derived from the previous stepmay be considered as the ?necessary?
conditions forproposing a change in verb forms.
They are not ?suf-ficient?, however, since they tend to be overly gen-eral.
Indiscriminate application of these patterns onAQUAINT would result in false positives for 46.4%of the sentences.For those categories with a high rate of false posi-tives (all except BASEmd, BASEdo and FINITE), weutilized n-grams as filters, allowing a correctiononly when its n-gram count in the WEB 1T 5-GRAM8Due to space constraints, only those trees with significantchanges above the leaf level are shown.Hyp.
False Hypothesized FalseUsage Pos.
Usage Pos.BASEmd 16.2% {INGverb,INFverb} 33.9%BASEdo 0.9% {INGprog,EDpass} 21.0%FINITE 12.8% INGprep 13.7%EDperf 1.4%Table 7: The distribution of false positives in AQUAINT.The total number of false positives is 994, represents lessthan 1% of the 100,000 sentences drawn from the corpus.corpus is greater than that of the original.
The filter-ing step reduced false positives from 46.4% to lessthan 1%.
Table 6 shows the n-grams, and Table 7provides a breakdown of false positives in AQUAINTafter n-gram filtering.6.3 Results for Subject-Verb AgreementIn JLE, the accuracy of subject-verb agreement er-ror correction is 98.93%.
Compared to the majoritybaseline of 96.95%, the improvement is statisticallysignificant9.
Recall is 80.92%; detection precision is83.93%, and correction precision is 81.61%.Most mistakes are caused by misidentified sub-jects.
Some wh-questions prove to be especially dif-ficult, perhaps due to their relative infrequency innewswire texts, on which the parser is trained.
Oneexample is the question ?How much extra time doesthe local train *takes??.
The word ?does?
is notrecognized as a ?do?-support, and so the verb ?take?was mistakenly turned into a third person form toagree with ?train?.6.4 Results for Auxiliary Agreement &ComplementationTable 8 summarizes the results for auxiliary agree-ment and complementation, and Table 2 shows someexamples of real sentences corrected by the system.Our proposed method yields 98.94% accuracy.
Itis a statistically significant improvement over themajority baseline (98.47%), although not significantover the verb-only baseline10 (98.85%), perhaps areflection of the small number of test sentences withverb form errors.
The Kappa statistic for the man-9p < 0.005 according to McNemar?s test.10With p = 1?10?10 and p = 0.038, respectively, accordingto McNemar?s test180Corpus Method Accuracy Precision Precision Recall(correction) (detection)JLE verb-only 98.85% 71.43% 84.75% 31.51%all 98.94% 68.00% 80.67% 42.86%HKUST all not available 71.71% not availableTable 8: Results on the JLE and HKUST corpora for auxiliary agreement and complementation.
The majority baselineaccuracy is 98.47% for JLE.
The verb-only baseline accuracy is 98.85%, as indicated on the second row.
?All?
denotesthe complete proposed method.
See ?6.4 for detailed comments.Usage JLE HKUSTCount (Prec.)
Count (Prec.
)BASEmd 13 (92.3%) 25 (80.0%)BASEdo 5 (100%) 0FINITE 9 (55.6%) 0EDperf 11 (90.9%) 3 (66.7%){INGprog,EDpass} 54 (58.6%) 30 (70.0%){INGverb,INFverb} 45 (60.0%) 16 (59.4%)INGprep 10 (60.0%) 2 (100%)Table 9: Correction precision of individual correctionpatterns (see Table 5) on the JLE and HKUST corpus.ual evaluation of HKUST is 0.76, correspondingto ?substantial agreement?
between the two evalu-ators (Landis and Koch, 1977).
The correction pre-cisions for the JLE and HKUST corpora are compa-rable.Our analysis will focus on {INGprog,EDpass} and{INGverb,INFverb}, two categories with relativelynumerous correction attempts and low precisions,as shown in Table 9.
For {INGprog,EDpass}, manyinvalid corrections are due to wrong predictions ofvoice, which involve semantic choices (see ?2.1).For example, the sentence ?...
the main duty is studywell?
is edited to ?...
the main duty is studied well?,a grammatical sentence but semantically unlikely.For {INGverb,INFverb}, a substantial portion of thefalse positives are valid, but unnecessary, correc-tions.
For example, there is no need to turn ?I likecooking?
into ?I like to cook?, as the original is per-fectly acceptable.
Some kind of confidence measureon the n-gram counts might be appropriate for re-ducing such false alarms.Characteristics of speech transcripts pose somefurther problems.
First, colloquial expressions, suchas the word ?like?, can be tricky to process.
In thequestion ?Can you like give me the money back?,?like?
is misconstrued to be the main verb, and?give?
is turned into an infinitive, resulting in ?Canyou like *to give me the money back?.
Second, thereare quite a few incomplete sentences that lack sub-jects for the verbs.
No correction is attempted onthem.Also left uncorrected are misused forms in non-finite clauses that describe a noun.
These are typ-ically base forms that should be replaced with -ingparticiples, as in ?The girl *wear a purple skiwearis a student of this ski school?.
Efforts to detect thiskind of error had resulted in a large number of falsealarms.Recall is further affected by cases where a verb isseparated from its auxiliary or main verb by manywords, often with conjunctions and other verbs inbetween.
One example is the sentence ?I used toclimb up the orange trees and *catching insects?.The word ?catching?
should be an infinitive comple-menting ?used?, but is placed within a noun phrasetogether with ?trees?
and ?insects?.7 ConclusionWe have presented a method for correcting verbform errors.
We investigated the ways in which verbform errors affect parse trees.
When allowed for,these unusual tree patterns can expand correctioncoverage, but also tend to result in overgenerationof hypothesized corrections.
N -grams have beenshown to be an effective filter for this problem.8 AcknowledgmentsWe thank Prof. John Milton for the HKUST cor-pus, Tom Lee and Ken Schutte for their assistancewith the evaluation, and the anonymous reviewersfor their helpful feedback.181ReferencesE.
Bender, D. Flickinger, S. Oepen, A. Walsh, and T.Baldwin.
2004.
Arboretum: Using a Precision Gram-mar for Grammar Checking in CALL.
Proc.
In-STIL/ICALL Symposium on Computer Assisted Learn-ing.M.
Chodorow, J. R. Tetreault, and N.-R. Han.
2007.Detection of Grammatical Errors Involving Preposi-tions.
In Proc.
ACL-SIGSEM Workshop on Preposi-tions.
Prague, Czech Republic.M.
Collins.
1997.
Three Generative, Lexicalised Modelsfor Statistical Parsing.
Proc.
ACL.J.
Foster.
2007.
Treebanks Gone Bad: Generating a Tree-bank of Ungrammatical English.
In Proc.
IJCAI Work-shop on Analytics for Noisy Unstructured Data.
Hy-derabad, India.G.
Heidorn.
2000.
Intelligent Writing Assistance.Handbook of Natural Language Processing.
RobertDale, Hermann Moisi and Harold Somers (ed.).
Mar-cel Dekker, Inc.E.
Izumi, K. Uchimoto, T. Saiga, T. Supnithi, and H.Isahara.
2003.
Automatic Error Detection in theJapanese Learner?s English Spoken Data.
In Compan-ion Volume to Proc.
ACL.
Sapporo, Japan.K.
Knight and I. Chander.
1994.
Automated Posteditingof Documents.
In Proc.
AAAI.
Seattle, WA.J.
R. Landis and G. G. Koch.
1977.
The Measurement ofObserver Agreement for Categorical Data.
Biometrics33(1):159?174.L.
Michaud, K. McCoy and C. Pennington.
2000.
An In-telligent Tutoring System for Deaf Learners of WrittenEnglish.
Proc.
4th International ACM Conference onAssistive Technologies.J.
Lee and S. Seneff.
2006.
Automatic Grammar Cor-rection for Second-Language Learners.
In Proc.
Inter-speech.
Pittsburgh, PA.J.
C. Reynar and A. Ratnaparkhi.
1997.
A Maximum En-tropy Approach to Identifying Sentence Boundaries.In Proc.
5th Conference on Applied Natural LanguageProcessing.
Washington, D.C.R.
Quirk, S. Greenbaum, G. Leech, and J. Svartvik.
1985.A Comprehensive Grammar of the English Language.Longman, New York.182
