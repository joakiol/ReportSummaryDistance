Proceedings of the 2nd Workshop on Speech and Language Processing for Assistive Technologies, pages 73?83,Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational LinguisticsREAD?IT:Assessing Readability of Italian Texts with a View to Text SimplificationFelice Dell?Orletta, Simonetta Montemagni and Giulia VenturiIstituto di Linguistica Computazionale ?Antonio Zampolli?
(ILC?CNR)via G. Moruzzi, 1 ?
Pisa (Italy){felice.dellorletta,simonetta.montemagni,giulia.venturi}@ilc.cnr.itAbstractIn this paper, we propose a new approach toreadability assessment with a specific view tothe task of text simplification: the intendedaudience includes people with low literacyskills and/or with mild cognitive impairment.READ?IT represents the first advanced read-ability assessment tool for what concerns Ital-ian, which combines traditional raw text fea-tures with lexical, morpho-syntactic and syn-tactic information.
In READ?IT readabilityassessment is carried out with respect to bothdocuments and sentences where the latter rep-resents an important novelty of the proposedapproach creating the prerequisites for align-ing the readability assessment step with thetext simplification process.
READ?IT showsa high accuracy in the document classificationtask and promising results in the sentence clas-sification scenario.1 IntroductionRecently, there has been increasing interest in theexploitation of results from Natural Language Pro-cessing (NLP) for the development of assistive tech-nologies.
Here, we address this topic by reportingthe first but promising results in the developmentof a software architecture for the Italian languageaimed at assisting people with low literacy skills(both native and foreign speakers) or who have lan-guage disabilities in reading texts.Within an information society, where everyoneshould be able to access all available information,improving access to written language is becomingmore and more a central issue.
This is the case, forinstance, of administrative and governmental infor-mation which should be accessible to all members ofthe society, including people who have reading dif-ficulties for different reasons: because of a low edu-cation level or because of the fact that the languagein question is not their mother tongue, or becauseof language disabilities.
Health related informationrepresents another crucial domain which should beaccessible to a large and heterogenous target group.Understandability in general and readability in par-ticular is also an important issue for accessing infor-mation over the web as stated in the Web ContentAccessibility Guidelines (WCAG) proposed by theWeb Accessibility Initiative of the W3C.In this paper, we describe the approach we devel-oped for automatically assessing the readability ofnewspaper texts with a view to the specific task oftext simplification.
The paper is organized as fol-lows: Section 2 describes the background literatureon the topic; Section 3 introduces the main featuresof our approach to readability assessment, with Sec-tion 4 illustrating its implementation in the READ-IT prototype; Sections 5 and 6 describe the experi-mental setting and discuss achieved results.2 BackgroundReadability assessment has been a central researchtopic for the past 80 years which is still attractingconsiderable interest nowadays.
Over the last tenyears, within the NLP community the automatic as-sessment of readability has received increasing at-tention: if on the one hand the availability of sophis-ticated NLP technologies makes it possible to moni-tor a wide variety of factors affecting the readability73of a text, on the other hand there is a wide rangeof both human- and machine-oriented applicationswhich can benefit from it.Traditional readability formulas focus on a lim-ited set of superficial text features which are takenas rough approximations of the linguistic factors atplay in readability assessment.
For example, theFlesch-Kincaid measure (the most common readingdifficulty measure still in use, Kincaid (1975)) is alinear function of the average number of syllablesper word and of the average number of words persentence, where the former and latter are used assimple proxies for lexical and syntactic complexityrespectively.
For Italian, there are two readabilityformulas: an adaptation of the Flesh-Kincaid for En-glish to Italian known as the Flesch-Vacca formula(Franchina and Vacca, 1986); the GulpEase index(Lucisano and Piemontese, 1988), assessing read-ability on the basis of the average number of char-acters per word and the average number of wordsper sentence.A widely acknowledged fact is that all traditionalreadability metrics are quick and easy to calculatebut have drawbacks.
For example, the use of sen-tence length as a measure of syntactic complexity as-sumes that a longer sentence is more grammaticallycomplex than a shorter one, which is often but notalways the case.
Word syllable count is used start-ing from the assumption that more frequent wordsare more likely to have fewer syllables than less fre-quent ones (an association that is related to Zipf?sLaw, Zipf (1935)); yet, similarly to the previouscase, word length does not necessarily reflects itsdifficulty.
The unreliability of these metrics has beenexperimentally demonstrated by several recent stud-ies in the field: to mention only a few Si and Callan(2001), Petersen and Ostendorf (2006), Feng (2009).On the front of the assessment of the lexical dif-ficulty of a given text, a first step forward is rep-resented by vocabulary-based formulas such as theDale-Chall formula (Chall and Dale, 1995), usinga combination of average sentence length and wordfrequency counts.
In particular, for what concernsthe latter it reconstructs the percentage of wordsnot on a list of 3000 ?easy?
words by matching itsown list to the words in the material being evalu-ated, to determine the appropriate reading level.
Ifvocabulary-based measures represent an improve-ment in assessing the readability of texts which waspossible due to the availability of frequency dictio-naries and reference corpora, they are still unsatis-factory for what concerns sentence structure.Over the last ten years, work on readability de-ployed sophisticated NLP techniques, such as syn-tactic parsing and statistical language modeling, tocapture more complex linguistic features and usedstatistical machine learning to build readability as-sessment tools.
A variety of different NLP-basedapproaches to the automatic readability assessmenthas been proposed so far, differing with respect to:a) the typology of features taken into account (e.g.lexical, syntactic, semantic, discourse), and, for eachtype, at the level of the inventory of used individualfeatures; b) the intended audience of the texts underevaluation, which strongly influences the readabilityassessment, and last but not least c) the applicationwithin which readability assessment is carried out.Interesting alternatives to static vocabulary-basedmeasures have been put forward by Si and Callan(2001) who used unigram language models com-bined with sentence length to capture content in-formation from scientific web pages, or by Collins-Thompson and Callan (2004) who adopted a sim-ilar language modeling approach (Smoothed Uni-gram model) to predict reading difficulty of shortpassages and web documents.
These approaches canbe seen as a generalization of the vocabulary-basedapproach, aimed at capturing finer-grained and moreflexible information about vocabulary usage.
If un-igram language models help capturing importantcontent information and variation of word usage,they do not cover other types of features which arereported to play a significant role in the assessmentof readability.
More recently, the role of syntac-tic features started being investigated (Schwarm andOstendorf, 2005; Heilman et al, 2007; Petersen andOstendorf, 2009): in these studies syntactic structureis tracked through a combination of features from n-gram (trigram, bigram and unigram) language mod-els and parse trees (parse tree height, number ofnoun phrases, verb phrases and subordinated clausesor SBARs) with more traditional features.Yet, besides lexical and syntactic complexity fea-tures there are other important factors, such as thestructure of the text, the definition of discourse topic,discourse cohesion and coherence and so on, play-74ing a central role in determining the reading diffi-culty of a text.
More recent approaches esploredthe role of these features in readability assessment:this is the case, for instance, of Barzilay and Lap-ata (2008) or Feng (2010).
The last few years havebeen characterised by approaches based on the com-bination of features ranging over different linguisticlevels, namely lexical, syntactic and discourse (seee.g.
Pitler and Nenkova (2008), Kate (2010)).Another important factor determining the typol-ogy of features to be considered for assessing read-ability has to do with the intended audience ofreaders: it is commonly agreed that reading easedoes not follow from intrinsic text properties alone,but it is also affected by the expected audience.Among the studies addressing readability with re-spect to specific audiences, it is worth mentioninghere: Schwarm and Ostendorf (2005) and Heilmanet al (2007) dealing with language learners, or Feng(2009) focussing on people with mild intellectualdisabilities.
Interestingly,Heilman et al (2007) dif-ferentiate the typology of used features when ad-dressing first (L1) or second (L2) language learn-ers: they argue that grammatical features are morerelevant for L2 than for L1 learners.
Feng (2009)propose a set of cognitively motivated features op-erating at the discourse level specifically addressingthe cognitive characteristics of the expected users.When readability is targeted towards adult compe-tent language users a more prominent role is playedby discourse features (Pitler and Nenkova, 2008).Applications which can benefit from an automaticreadability assessment range from the selection ofreading material tailored to varying literacy levels(e.g.
for L1/L2 students or low literacy people)and the ranking of documents by reading difficulty(e.g.
in returning the results of web queries) to NLPtasks such as automatic document summarization,machine translation as well as text simplification.Again, also the application making use of the read-ability assessment, which is in turn strictly related tothe intended audience of readers, strongly influencesthe typology of features to be taken into account.Advanced NLP?based readability metrics devel-oped so far typically deal with English, with a fewattempts devoted to other languages, namely French(Collins-Thompson and Callan, 2004), Portuguese(Aluisio et al, 2010) and German (Bru?ck, 2008).3 Our ApproachOur approach to readability assessment was devel-oped with a specific application in mind, i.e.
textsimplification, and addresses a specific target audi-ence of readers, namely people characterised by lowliteracy skills and/or by mild cognitive impairment.Following the most recent approaches, we treat read-ability assessment as a classification task: in partic-ular, given the available corpora for the Italian lan-guage as well as the type of target audience, we re-sorted to a binary classification aimed at discerningeasy?to?read textual objects from difficult?to?readones.
The language dealt with is Italian: to ourknowledge, this is the first attempt of an advancedmethodology for readability assessment for this lan-guage.
Our approach focuses on lexical and syntac-tic features, whose selection was influenced by theapplication, the intended audience and the languagedealt with (both for its intrinsic linguistic featuresand for the fact of being a less resourced language).Following Roark (2007), in the features selectionprocess we preferred easy-to-identify features whichcould be reliably identified within the output of NLPtools.
Last but not least, as already done by Aluisioet al (2010) the set of selected syntactic features alsoincludes simplification oriented ones, with the finalaim of aligning the readability assessment step withthe text simplification process.Another qualifying feature of our approach toreadability assessment consists in the fact that weare dealing with two types of textual objects: docu-ments and sentences.
The latter represents an impor-tant novelty of our work since so far most researchfocused on readability classification at the documentlevel (Skory and Eskenazi, 2010).
When the tar-get application is text simplification, we strongly be-lieve that also assessing readability at the sentencelevel could be very useful.
We know that methodsdeveloped so far perform well to characterize thelevel of an entire document, but they are unreliablefor short texts and thus also for single sentences.Sentence-based readability assessment thus repre-sents a further challenge we decided to tackle: infact, if all sentences occurring in simplified texts canbe assumed to be easy-to-read sentences, the reversedoes not necessarily hold since not all sentences oc-curring in complex texts are difficult-to-read sen-75tences.
Since there are no training data at the sen-tence level, it becomes difficult ?
if not impossible?
to evaluate the effectiveness of our approach, i.e.erroneous readability assessments within the classof difficult-to-read texts may either correspond tothose easy?to?read sentences occurring within com-plex texts or represent real classification errors.
Inorder to overcome this problem in the readability as-sessment of individual sentences, we introduced anotion of distance with respect to easy-to-read sen-tences.
In this way, the prerequisites are created forthe integration of the two processes of readability as-sessment and text simplification.
Before, text read-ability was assessed with respect to the entire doc-ument and text simplification was carried out at thesentence level: due to the decoupling of the two pro-cesses, the impact of simplification operations on theoverall readability level of the text was not alwaysimmediately clear.
With sentence-based readabilityassessment, this should be no longer a problem.4 READ?ITOur approach to readability assessment has been im-plemented in a software prototype, henceforth re-ferred to as READ?IT.
READ?IT operates on syn-tactically (i.e.
dependency) parsed texts and it as-signs to each considered reading object - either adocument or a sentence - a score quantifying itsreadability.
READ?IT is a classifier based on Sup-port Vector Machines using LIBSVM (Chang andLin, 2001) that, given a set of features and a trainingcorpus, creates a statistical model using the featurestatistics extracted from the training corpus.
Sucha model is used in the assessment of readability ofunseen documents and sentences.The set of features used to build the statisticalmodel can be parameterized through a configura-tion file: as we will see, the set of relevant fea-tures used for readability assessment at the docu-ment level differs from the those used at the sen-tence level.
This also creates the prerequisites forspecialising the readability assessment measure withrespect to more specific target audiences: as pointedout in Heilman et al (2007) different types of fea-tures come into play e.g.
when addressing L1 or L2language learners.
Here follows the complete list offeatures used in the reported experiments.4.1 FeaturesThe features used for predicting readability are or-ganised into four main categories: namely, raw textfeatures, lexical features as well as morpho-syntacticand syntactic features.
This proposed four?fold par-tition closely follows the different levels of linguis-tic analysis automatically carried out on the text be-ing evaluated, i.e.
tokenization, lemmatization, PoStagging and dependency parsing.
Such a partitionwas meant to identify those easy to extract featureswith high discriminative power in order to reducethe linguistic pre-processing of texts guaranteeing atthe same time a reliable readability assessment.Raw Text FeaturesThey refer to those features typically used within tra-ditional readability metrics.
They include SentenceLength, calculated as the average number of wordsper sentence, and Word Length, calculated as the av-erage number of characters per words.Lexical FeaturesBasic Italian Vocabulary rate features: these fea-tures refer to the internal composition of the vocab-ulary of the text.
To this end, we took as a refer-ence resource the Basic Italian Vocabulary by DeMauro (2000), including a list of 7000 words highlyfamiliar to native speakers of Italian.
In particular,we calculated two different features correspondingto: i) the percentage of all unique words (types)on this reference list (calculated on a per?lemmabasis); ii) the internal distribution of the occurringbasic Italian vocabulary words into the usage clas-sification classes of ?fundamental words?
(very fre-quent words), ?high usage words?
(frequent words)and ?high availability words?
(relatively lower fre-quency words referring to everyday objects or ac-tions and thus well known to speakers).
Whereasthe latter represents a novel feature in the readabilityassessment literature, the former originates from theDale-Chall formula (Chall and Dale, 1995) and, asimplemented here, it can be seen as the complementof the type out-of-vocabulary rate features used byPetersen and Ostendorf (2009).Type/Token Ratio: this feature refers to the ratiobetween the number of lexical types and the num-ber of tokens.
This feature, which can be consid-ered as an indicator of expressive language delay or76disorder as shown in Wright (2003) for adults andin Retherford (2003) for children, has already beenused for readability assessment purposes by Aluisioet al (2010).
Due to its sensitivity to sample size,this feature has been computed for text samples ofequivalent length.Morpho?syntactic FeaturesLanguage Model probability of Part-Of-Speechunigrams: this feature is based on a unigram lan-guage model assuming that the probability of a tokenis independent of its context.
The model is simplydefined by a list of types (POS) and their individualprobabilities.
This feature has already been shownto be a reliable indicator for automatic readabilityassessment (see, for example, Pitler and Nenkova(2008) and Aluisio et al (2010)).Lexical density: this feature refers to the ratio ofcontent words (verbs, nouns, adjectives and adverbs)to the total number of lexical tokens in a text.
Con-tent words have already been used for readability as-sessment by Aluisio et al (2010) and Feng (2010).Verbal mood: this feature refers to the distributionof verbs according to their mood.
It is a novel andlanguage?specific feature exploiting the predictivepower of the Italian rich verbal morphology.Syntactic FeaturesUnconditional probability of dependency types:this feature refers to the unconditional probability ofdifferent types of syntactic dependencies (e.g.
sub-ject, direct object, modifier, etc.)
and can be seenas the dependency-based counterpart of the ?phrasetype rate?
feature used by Nenkova (2010).Parse tree depth features: parse tree depth can beindicative of increased sentence complexity as statedby, to mention only a few, Yngve (1960), Frazier(1985) and Gibson (1998).
This set of features ismeant to capture different aspects of the parse treedepth and includes the following measures: a) thedepth of the whole parse tree, calculated in terms ofthe longest path from the root of the dependency treeto some leaf; b) the average depth of embedded com-plement ?chains?
governed by a nominal head andincluding either prepositional complements or nomi-nal and adjectival modifiers; c) the probability distri-bution of embedded complement ?chains?
by depth.The first feature has already been used in syntax-based readability assessment studies (Schwarm andOstendorf, 2005; Heilman et al, 2007; Nenkova,2010); the latter two are reminiscent of the ?headnoun modifiers?
feature used by Nenkova (2010).Verbal predicates features: this set of features cap-tures different aspects of the behaviour of verbalpredicates.
They range from the number of verbalroots with respect to number of all sentence rootsoccurring in a text to their arity.
The arity of verbalpredicates is calculated as the number of instanti-ated dependency links sharing the same verbal head(covering both arguments and modifiers).
Althoughthere is no obvious relation between the number ofverb dependents and sentence complexity, we be-lieve that both a low and a high number of depen-dents can make sentence readability quite complex,although for different reasons (elliptical construc-tions in the former case, a high number of modifiersin the latter).
Within this feature set we also con-sidered the distribution of verbal predicates by arity.To our knowledge, this set of features has never beenused so far for readability assessment purposes.Subordination features: subordination is widelyacknowledged to be an index of structural complex-ity in language.
As in Aluisio et al (2010), this setof features has been introduced here with a specificview to the text simplification task.
A first featurewas meant to measure the distribution of subordi-nate vs main clauses.
For subordinates, we alsoconsidered their relative ordering with respect tothe main clause: according to Miller and Weinert(1998), sentences containing subordinate clauses inpost?verbal rather than in pre?verbal position areeasier to read.
Two further features were intro-duced to capture the depth of embedded subordinateclauses since it is a widely acknowledged fact thathighly complex sentences contain deeply embeddedsubordinate clauses: in particular, a) the averagedepth of ?chains?
of embedded subordinate clausesand b) the probability distribution of embedded sub-ordinate clauses ?chains?
by depth.Length of dependency links feature: both Lin(1996) and Gibson (1998) showed that the syntacticcomplexity of sentences can be predicted with mea-sures based on the length of dependency links.
Thisis also demonstrated in McDonald and Nivre (2007)who claim that statistical parsers have a drop in ac-curacy when analysing long dependencies.
Here, the77dependency length is measured in terms of the wordsoccurring between the syntactic head and the depen-dent.
This feature is the dependency-based counter-part of the ?phrase length?
feature used for readabil-ity assessment by Nenkova (2010) and Feng (2010).5 The CorporaOne challenge in this work was finding an appropri-ate corpus.
Although a possibly large collection oftexts labelled with their target grade level (such asthe Weekly Reader for English) would be ideal, weare not aware of any such collection that exists forItalian in electronic form.
Instead, to test our ap-proach to automatically identify the readability of agiven text, we used two different corpora: a news-paper corpus, La Repubblica (henceforth, ?Rep?
),and an easy?to?read newspaper, Due Parole (hence-forth, ?2Par?)
which was specifically written for anaudience of adults with a rudimentary literacy levelor with mild intellectual disabilities.
The articles in2Par were written by Italian linguists expert in textsimplification using a controlled language both atthe lexicon and sentence structure levels (Piemon-tese, 1996) .There are different motivations underlying the se-lection of these two corpora for our study.
On thepractical side, to our knowledge 2Par is the onlyavailable corpus of simplified texts addressing awide audience characterised by a low literacy level.So, the use of 2Par represented the only possible op-tion on the front of simplified texts.
For the selectionof the second corpus we opted for texts belongingto the same class, i.e.
newspapers: this was aimedat avoiding interferences due to textual genre varia-tion in the measure of text readability.
This is con-firmed by the fact that the two corpora show a sim-ilar behaviour with respect to a number of differentparameters, which according to the literature on reg-ister variation (Biber, 2009) are indicative of textualgenre differences: e.g.
lexical density, the noun/verbratio, the percentage of verbal roots, etc.
On theother hand, the two corpora differ significantly withrespect to the distribution of features typically cor-related with text complexity, e.g.
the composition ofthe used vocabulary (e.g.
the percentage of wordsbelonging to the Basic Italian Vocabulary in Rep is4.14% and in 2Par is 48.04%) or, from the syntacticpoint of view, the average parse tree height (which inRep is 5.71 and in 2Par 3.67), the average number ofverb phrases per sentence (which in Rep is 2.40 andin 2Par 1.25), the depth of nested structures (e.g.
theaverage depth of embedded complement ?chains?
inRep is 1.44 and in 2Par is 1.30), the proportion ofmain vs subordinate clauses (in Rep main and sub-ordinate clauses represent respectively 65.11% and34.88% of the cases; in 2Par there is 79.66% of mainclauses and 20.33% of subordinate clauses).The Rep/2Par pair of corpora is somehow remi-niscent of corpora used in other readability studies,such as Encyclopedia Britannica and Britannica El-ementary, but with a main difference: whereas theEnglish corpora consist of paired original/simplifiedtexts, which we might define as ?parallel monolin-gual corpora?, the selected Italian corpora ratherpresent themselves as ?comparable monolingualcorpora?, without any pairing of the full?simplifiedversions of the same article.
Comparability is guar-anteed here by the inclusion of texts belonging to thesame textual genre: we expect such comparable cor-pora to be usefully exploited for readability assess-ment because of the emphasis on style over topic.Although these corpora do not provide an ex-plicit grade?level ranking for each article, broadcategories are distinguished, namely easy?to?readvs difficult?to?read texts.
The two paired com-plex/simplified corpora were used to train and testdifferent language models described in Section 6.As already pointed out, such a distinction is reliablein a document classification scenario, while at thesentence classification level it poses the remarkableissue of discerning easy?to?read sentences withindifficult?to?read documents (i.e.
Rep).6 Experiments and ResultsREAD?IT was tested on the 2Par and Rep cor-pora automatically POS tagged by the Part?Of?Speech tagger described in Dell?Orletta (2009) anddependency?parsed by the DeSR parser (Attardi,2006) using Support Vector Machine as learning al-gorithm.
Three different sets of experiments weredevised to test the performance of READ-IT in thefollowing subtasks: i) document readability classifi-cation, ii) sentence readability classification and iii)detection of easy?to?read sentences within difficult?78to?read texts.For what concerns the document classificationsubtask, we used a corpus made up of 638 docu-ments of which 319 were extracted from 2Par (takenas representative of the class easy?to?read texts) and319 from Rep (representing the class of difficult?to?read texts).
We have followed a 5?fold cross?validation process: the corpus was randomly splitinto 5 training and test sets.
The test sets consistedof 20% of the individual documents belonging to thetwo considered readability levels, with each docu-ment being included in one test set only.
With re-gard to the sentence classification subtask, we used atraining set of about 3,000 sentences extracted from2Par and of about 3,000 sentences from Rep and atest corpus of 1,000 sentences of which 500 were ex-tracted from 2Par (hereafter, 2Par test set) and 500from Rep (hereafter, Rep test set).
In the third ex-periment, readability assessment was carried out byREAD?IT with respect to a much bigger corpus of2,5 milion of words extracted from the newspaperLa Repubblica (hereafter, Rep 2.5), for a total of123,171 sentences, with the final aim of detectingeasy?to?read sentences.All the experiments were carried out using fourdifferent readability models, described as follows:1.
Base Model, using raw text features only;2.
Lexical Model, using a combination of rawtext and lexical features;3.
MorphoS Model: using raw text, lexical andmorpho?syntactic features;4.
Syntax Model: combining all feature types,namely raw text, lexical, morpho?syntactic andsyntactic features.Note that in the Lexical and Syntax Models, dif-ferent sets of features were selected for the subtasksof document and sentence classification.
In particu-lar, for sentence?based readability assesment we didnot take into account the Type/Token Ratio feature,all features concerning the distribution of ?chains?of embedded complements and subordinate clausesand the distribution of verbal predicates by arity.Since, to our knowledge, a machine learning read-ability classifier does not exist for the Italian lan-guage we consider the Base Model as our baseline:this can be seen as an approximation of the Gul-pEase index, which is based on the same raw textfeatures (i.e.
sentence and word length).6.1 Evaluation MethodologyDifferent evaluation methods have been defined inorder to assess achieved results in the three afore-mentioned experiment sets.
The performance ofboth document and sentence classification experi-ments have been evaluated in terms of i) overall Ac-curacy of the system and ii) Precision and Recall.In particular, Accuracy is a global score referringto the percentage of documents or sentences cor-rectly classified, either as easy?to?read or difficult?to?read objects.
Precision and Recall have beencomputed with respect to two the target reading lev-els: in particular, Precision is the ratio of the numberof correctly classified documents or sentences overthe total number of documents and sentences classi-fied by READ?IT as belonging to the easy?to?read(i.e.
2Par) or difficult?to?read (i.e.
Rep) classes; Re-call has been computed as the ratio of the number ofcorrectly classified documents or sentences over thetotal number of documents or sentences belongingto each reading level in the test sets.
For each set ofexperiments, evaluation was carried out with respectto the four models of the classifier.Following from the assumption that 2Par con-tains only easy?to?read sentences while Rep doesnot necessarily contain only difficult?to?read ones,we consider READ?IT errors in the classification of2Par sentences as erroneously classified sentences.On the other hand, classification errors within theset of Rep sentences deserve an in?depth error anal-ysis, since we need to discern real errors from mis-classifications due to the fact that we are in front ofeasy?to?read sentences occurring in a difficult?to?read context.
In order discern errors from ?correct?misclassifications, we introduced a new evaluationmethodology, based on the notion of Euclidean dis-tance between feature vectors.
Each feature vec-tor is a n?dimensional vector of linguistic features(see Section 4.1) that represents a set of sentences.Two vectors with 0 distance represent the same setof sentences, i.e.
those sentences sharing the samevalues for the monitored linguistic features.
Con-versely, the bigger the distance between two vectorsis, the more distant are the two represented sets of79sentences with respect to the monitored features.The same notion of distance has also been usedto test which model was more effective in predictingthe readability of n?word long sentences.6.2 ResultsIn Table 1, the Accuracy, Precision and Recall scoresachieved with the different READ?IT models in thedocument classification subtask are reported.
It canbe noticed that the Base Model shows the lowest per-formance, while the MorphoS Model outperformsall other models.
Interestingly, the Lexical Modelshows a high accuracy for what concerns the doc-ument classification subtask (95.45%), by signifi-cantly improving the accuracy score of the BaseModel (about +19%).
This result demonstrates thatfor assessing the readability of documents a combi-nation of raw and lexical features provides reliableresults which can be further improved (about +3%)by also taking into account morpho-syntax.2Par RepModel Accuracy Prec Rec Prec RecBase 76.65 74.71 80.56 78.91 72.73Lexical 95.45 95.60 95.30 95.31 95.61MorphoS 98.12 98.12 98.12 98.12 98.12Syntax 97.02 97.17 96.87 96.88 97.18Table 1: Document classification resultsConsider now the sentence classification subtask.Table 2 shows that in this case the most reliable re-sults are achieved with the Syntax Model.
It is inter-esting to note that the morpho?syntactic and syntac-tic features allow a much higher increment in termsof Accuracy, Precision and Recall scores than in thedocument classification scenario: i.e.
the differencebetween the performance of the Lexical Model andthe best one in the document classification experi-ment (i.e.
the MorphoS Model) is equal to 2.6%,while in the sentence classification case (i.e.
SyntaxModel) is much higher, namely 17% .In Table 3, we detail the performance of the bestREAD?IT model (i.e.
the Syntax Model) on theRep test set.
In order to evaluate those sentenceswhich were erroneously classified as belonging to2Par, we calculated the distance between 2Par and i)these sentences (140 sentences referred to as wrongin the Table), ii) the correctly classified sentences2Par RepModel Accuracy Prec Rec Prec RecBase 59.6 55.6 95.0 82.9 24.2Lexical 61.6 57.3 91.0 78.1 32.2MorphoS 76.1 72.8 83.4 80.6 68.8Syntax 78.2 75.1 84.4 82.2 72.0Table 2: Sentence classification results(360 sentences, referred to as correct in the Table),iii) the whole Rep test set.
As we can see, the dis-tance between the wrong sentences and 2Par is muchlower than the distance holding between 2Par andthe correcly classified sentences (correct).
This en-tails that the sentences which were erroneously clas-sified as easy?to?read sentences (i.e.
belonging to2Par) are in fact more readable than the correctlyclassified ones (as belonging to Rep).It is obviousthat the Rep test set, which contains both correct andwrong sentences, has an intermediate distance valuewith respect to 2Par.DistanceCorrect 52.072Rep test set 45.361Wrong 37.843Table 3: Distances between 2Par and Rep on the basis ofthe Syntax ModelIn Table 4, the percentage of Rep 2.5 sentencesclassified as difficult?to?read is reported.
The re-sults show that the Syntax Model classifies the highernumber of sentences as difficult?to?read, but fromthese results we cannot say whether this model is thebest one or not since Rep 2.5 sentences are not anno-tated with readability information.
Therefore, in or-der to compare the performance of the four READ?IT models and to identify which is the best one, wecomputed the distance between the sentences clas-sified as easy?to?read and 2Par, which is reported,for each model, in Table 5.
It can be noticed thatthe Syntax Model appears to be the best one since itshows the lowest distance with respect to 2Par; onthe other hand, the whole Rep 2.5 corpus shows ahigher distance since it contains both difficult?
andeasy?to?read sentences.
Obviously, the sentencesclassified as difficult?to?read by the Syntax Model(Diff Syntax in the Table) show the broader distance.80AccuracyBase 0.234Lexical 0.387MorphoS 0.705Syntax 0.755Table 4: Accuracy in sentence classification of Rep 2.5.DistanceDiff Syntax 66.526Rep 2.5 64.040Base 61.135Lexical 60.529MorphoS 55.535Syntax 51.408Table 5: Distance between 2Par and i) difficult?to?readsentences according to the Syntax Model, ii) Rep 2.5, iii)easy?to?read sentences by the four models.In order to gain an in?depth insight into thedifferent behaviour of the four READ?IT models,we evaluated their performances for sentences of afixed length.
We considered sentences whose lengthranges between 8 and 30.
For every set of sentencesof the same legth, we compared the easy?to-readsentences of Rep 2.5 classified by the four modelswith respect to 2Par.
In Figure 1, each point rep-resents the distance between a set of sentences ofthe same length and the same n?word long set ofsentences in the 2Par corpus.
As it can be seen, thebottom line which represents the sentences classifiedas easy?to?read by the Syntax Model is the closestto the 2Par sentences of the same length.
On thecontrary, the line representing the sentences classi-fied by the Base Model is the most distant amongstthe four READ?IT models.
Interestingly, it over-laps with the line representing the Rep 2.5 sentences:this suggests that a classification model based onlyon raw text features (i.e.
sentence and word length)is not able to identify easy?to?read sentences if weconsider sets of sentences of a fixed length.
Obvi-ously, the line representing the sentences classifiedas difficult?to?read by the Syntax Model shows thebroadest distance.
This experiment has shown thatlinguistically motivated features (and in particularsyntactic ones) have a fundamental role in the sen-tence readability assessment subtask.Figure 1: Distance between 2Par and i) difficult?to?readsentences according to the Syntax Model, ii) Rep 2.5, iii)easy?to?read sentences by the four models for sets of sen-tences of fixed length7 ConclusionIn this paper, we proposed a new approach to read-ability assessment with a specific view to the task oftext simplification: the intended audience includespeople with low literacy skills and/or with mild cog-nitive impairment.
The main contributions of thiswork can be summarised as follows: i) READ?IT represents the first advanced readability assess-ment tool for what concerns Italian; ii) it combinestraditional raw text features with lexical, morpho-syntactic and syntactic information; iii) readabilityassessment is carried out with respect to both doc-uments and sentences.
Sentence?based readabilityassessment is an important novelty of our approachwhich creates the prerequisites for aligning readabil-ity assessment with text simplification.
READ?ITshows a high accuracy in the document classificationtask and promising results in the sentence classifica-tion scenario.
The two different tasks appear to en-force different requirements at the level of the under-lying linguistic features.
To overcome the lack of anItalian reference resource annotated with readabilityinformation at the sentence level we introduced thenotion of distance to assess READ?IT performance.81AcknowledgementsThe research reported in the paper has been partlysupported by the national project ?Migrations?
ofthe National Council of Research (CNR) in theframework of the line of research Advanced tech-nologies and linguistic and cultural integration inthe school.
In particular the authors would liketo thank Eva Maria Vecchi who contributed to theprerequisites of the proposed readability assessmentmethodology reported in the paper.ReferencesSandra Aluisio, Lucia Specia, Caroline Gasperin andCarolina Scarton.
2010.
Readability assessment fortext simplification.
In Proceedings of the NAACL HLT2010 Fifth Workshop on Innovative Use of NLP forBuilding Educational Applications, pp.
1?9.Giuseppe Attardi.
2006.
Experiments with a multilan-guage non-projective dependency parser.
In Proceed-ings of the Tenth Conference on Computational Nat-ural Language Learning (CoNLL-X ?06), New YorkCity, New York, pp.
166?170.Regina Barzilay and Mirella Lapata.
2008.
Modelinglocal coherence: An entity-based approach.
In Com-putational Linguistics, 34(1), pp.
1?34.Douglas Biber and Susan Conrad.
2009.
Register, genre,and style.
Cambridge, Cambridge University Press.Tim vor der Bru?ck, Sven Hartrumpf, Hermann Helbig.2008.
A Readability Checker with Supervised Learn-ing using Deep Syntactic and Semantic Indicators.
InProceedings of the 11th International Multiconference:Information Society - IS 2008 - Language Technolo-gies, Ljubljana, Slovenia, pp.
92?97.Jeanne S. Chall and Edgar Dale.
1995.
ReadabilityRevisited: The New Dale?Chall Readability Formula.Brookline Books, Cambridge, MA.Chih-Chung Chang and Chih-Jen Lin.
2001.
LIB-SVM: a library for support vector machines.
Soft-ware available at http://www.csie.ntu.edu.tw/?cjlin/libsvmKevyn Collins-Thompson and Jamie Callan.
2004.
Alanguage modeling approach to predicting reading dif-ficulty.
In Proceedings of the HLT / NAACL, pp.
193?200.Felice Dell?Orletta.
2009.
Ensemble system for Part-of-Speech tagging.
In Proceedings of Evalita?09, Eval-uation of NLP and Speech Tools for Italian, ReggioEmilia, December.Tullio De Mauro.
2000.
Il dizionario della lingua ital-iana.
Torino, Paravia.Lijun Feng, Martin Jansche, Matt Huenerfauth andNoe?mie Elhadad.
2010.
A comparison of features forautomatic readability assessment.
In Proceedings ofthe 23rd International Conference on ComputationalLinguistics (COLING 2010), pp.
276?284.Lijun Feng, Noe?mie Elhadad and Matt Huenerfauth.2009.
Cognitively motivated features for readabilityassessment.
In Proceedings of the 12th Conference ofthe European Chapter of the Association for Compu-tational Linguistics (EACL ?09), pp.
229?237.V.
Franchina and Roberto Vacca.
1986.
Adaptation ofFlesh readability index on a bilingual text written bythe same author both in Italian and English languages.In Linguaggi (3), pp.
47?49.Lyn Frazier.
1985.
Syntactic complexity.
In D.R.Dowty, L. Karttunen and A.M. Zwicky (eds.
), NaturalLanguage Parsing, Cambridge University Press, Cam-bridge, UK.Edward Gibson.
1998.
Linguistic complexity: Localityof syntactic dependencies.
In Cognition, 68(1), pp.
1-76.Michael J. Heilman, Kevyn Collins and Jamie Callan.2007.
Combining Lexical and Grammatical Featuresto Improve Readability Measures for First and SecondLanguage Texts.
In Proceedings of the Human Lan-guage Technology Conference, pp.
460?467.Michael J. Heilman, Kevyn Collins and Maxine Eske-nazi.
2008.
An analysis of statistical models and fea-tures for reading difficulty prediction.
In Proceedingsof the Third Workshop on Innovative Use of NLP forBuilding Educational Applications (EANL ?08), pp.71?79.Rohit J. Kate, Xiaoqiang Luo, Siddharth Patwardhan,Martin Franz, Radu Florian, Raymond J. Mooney,Salim Roukos, Chris Welty.
2010.
Learning to Pre-dict Readability using Diverse Linguistic Features.
InProceedings of the 23rd International Conference onComputational Linguistics (COLING 2010), pp.
546?554.J.
Peter Kincaid, Lieutenant Robert P. Fishburne, RichardL.
Rogers and Brad S. Chissom.
1975.
Derivationof new readability formulas for Navy enlisted person-nel.
Research Branch Report, Millington, TN: Chiefof Naval Training, pp.
8?75.George Kingsley Zipf.
1988.
The Psychobiology of Lan-guage.
Houghton?Miflin, Boston.Dekan Lin.
1996.
On the structural complexity of nat-ural language sentences.
In Proceedings of COLING1996, pp.
729?733.Pietro Lucisano and Maria Emanuela Piemontese.
1988.Gulpease.
Una formula per la predizione della diffi-colta` dei testi in lingua italiana.
In Scuola e Citta` (3),pp.
57?68.82Ryan McDonald and Joakim Nivre.
2007.
Character-izing the Errors of Data-Driven Dependency ParsingModels.
In Proceedings of EMNLP-CoNLL, 2007, pp.122-131.Jim Miller and Regina Weinert.
1998.
Spontaneous spo-ken language.
Syntax and discourse.
Oxford, Claren-don Press.Ani Nenkova, Jieun Chae, Annie Louis, and Emily Pitler.2010.
Structural Features for Predicting the LinguisticQuality of Text Applications to Machine Translation,Automatic Summarization and Human?Authored Text.In E. Krahmer, M. Theune (eds.
), Empirical Methodsin NLG, LNAI 5790, Springer-Verlag Berlin Heidel-berg, pp.
222?241.Sarah E. Petersen and Mari Ostendorf.
2006.
A machinelearning approach to reading level assessment.
Uni-versity of Washington CSE Technical Report.Sarah E. Petersen and Mari Ostendorf.
2009.
A ma-chine learning approach to reading level assessment.In Computer Speech and Language (23), pp.
89?106.Maria Emanuela Piemontese.
1996.
Capire e farsicapire.
Teorie e tecniche della scrittura controllataNapoli, Tecnodid.Emily Pitler and Ani Nenkova.
2008.
Revisiting read-ability: A unified framework for predicting text qual-ity.
In Proceedings of the 2008 Conference on Em-pirical Methods in Natural Language Processing, pp.186?195.Kristine Retherford.
2003.
Normal development: adatabase of communication and related behaviors.Eau Claire, WI: Thinking Publications.Brian Roark, Margaret Mitchell and Kristy Hollingshead.2007.
Syntactic complexity measures for detectingmild cognitive impairment.
In Proceedings of theWorkshop on BioNLP 2007: Biological, Translational,and Clinical Language Processing, pp.
1?8.Sarah E. Schwarm and Mari Ostendorf.
2005.
Read-ing level assessment using support vector machinesand statistical language models.
In Proceedings ofthe 43rd Annual Meeting on Association for Compu-tational Linguistics (ACL 05), pp.
523?530.Luo Si and Jamie Callan.
2001.
A statistical model forscientific readability.
In Proceedings of the tenth in-ternational conference on Information and knowledgemanagement, pp.
574?576.Adam Skory and Maxine Eskenazi.
2010.
Predictingcloze task quality for vocabulary training.
In Proceed-ings of the NAACL HLT 2010 Fifth Workshop on In-novative Use of NLP for Building Educational Appli-cations, pp.
49?56.Victor H.A.
Yngve.
1960.
A model and an hypothesis forlanguage structure.
In Proceedings of the AmericanPhilosophical Society, pp.
444-466.Heather Harris Wright, Stacy W. Silverman and MarilynNewhoff.
2003.
Measures of lexical diversity in apha-sia.
In Aphasiology, 17(5), pp.
443-452.83
