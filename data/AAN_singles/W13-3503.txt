Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 20?28,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsImproving Pointwise Mutual Information (PMI) by IncorporatingSignificant Co-occurrenceOm P. DamaniIIT Bombaydamani@cse.iitb.ac.inAbstractWe design a new co-occurrence basedword association measure by incorpo-rating the concept of significant co-occurrence in the popular word associ-ation measure Pointwise Mutual Infor-mation (PMI).
By extensive experimentswith a large number of publicly availabledatasets we show that the newly intro-duced measure performs better than otherco-occurrence based measures and de-spite being resource-light, compares wellwith the best known resource-heavy dis-tributional similarity and knowledge basedword association measures.
We investi-gate the source of this performance im-provement and find that of the two typesof significant co-occurrence - corpus-leveland document-level, the concept of cor-pus level significance combined with theuse of document counts in place of wordcounts is responsible for all the perfor-mance gains observed.
The concept ofdocument level significance is not helpfulfor PMI adaptation.1 IntroductionCo-occurrence based word association measureslike PMI, LLR, and Dice are popular since theyare easy to understand and computationally effi-cient.
They measure the strength of associationbetween two words by comparing the word pair?scorpus-level bigram frequency to some function ofthe unigram frequencies of the individual words.Recently a new measure called Co-occurrenceSignificance Ratio (CSR) was introducedin (Chaudhari et al 2011) based on the no-tion of significant co-occurrence.
Since CSR wasfound to perform better than other co-occurrencemeasures, in this work, our goal was to incorpo-rate the concept of significant co-occurrence intraditional word-association measures to designnew measures that may perform better than bothCSR and the traditional measures.Two different notions of significant co-occurrence are employed in CSR:?
Corpus-level significant co-occurrence de-termines whether the ratio of observed bi-gram occurrences to their expected occur-rences across the corpus can be explained asa pure chance phenomenon, and,?
Document-level significant co-occurrencedetermines whether a large fraction of aword-pair?s occurrences within a given doc-ument have smaller spans than that under anull model where the words in the documentare permuted randomly.While both these notions are employed in anintegrated fashion in CSR, on analyzing CSR de-tails, we realized that these two concepts are in-dependent and can be applied separately to anyword association measure which is a ratio ofsome variable?s observed frequency to its ex-pected frequency.
We incorporate the conceptsof corpus-level and document-level significant co-occurrence in PMI to design a new measure thatperforms better than both PMI and CSR, as well asother co-occurrence based word association mea-sures.
To incorporate document level significance,we need to use document level counts instead ofword level counts (this distinction is explainedin detail in Section 4.3).
To investigate whetherthe performance gains observed are because ofthe concept of significant co-occurrence or sim-ply because of the fact that we are using docu-ment counts instead of the word counts, we alsodesign document count based baseline version ofPMI called PMId, and several intermediate vari-ants whose definitions are given in Table 1.To our surprise, we discover that the conceptof document level significant co-occurrence does20without corpus with corpuslevel significance level significanceword-based PMI: log f(x,y)f(x)?f(y)/W cPMI: log f(x,y)f(x)?f(y)/W+?f(x)?
?ln ?/(?2)document-based PMId: log d(x,y)d(x)?d(y)/D cPMId: log d(x,y)d(x)?d(y)/D+?d(x)?
?ln ?/(?2)with document levelsignificancePMIz: log Zd(x)?d(y)/D cPMIz: log Zd(x)?d(y)/D+?d(x)?
?ln ?/(?2)CSR: ZE(Z)+?K?
?ln ?/(?2)f(x, y) Span-constrained (x, y) word pair frequency in the corpusf(x), f(y) unigram frequencies of x, y respectively in the corpusW Total number of words in the corpusd(x, y) Total number of documents in the corpus having at-leastone span-constrained occurrence of the word pair (x, y)d(x), d(y) Total number of documents in the corpus containingat least one occurrence of x and y respectivelyD Total number of documents in the corpus?
a parameter varying between 0 and 1Z as per Definition 4.3E(Z) Expected value of Z as given in Section 2.2 of (Chaudhari et al 2011)K Total number of documents in the corpus having at-leastone occurrence of the word pair (x, y) regardless of the spanTable 1: Definitions0 of PMI, CSR, and various measures developed in this work.not contribute to the PMI performance improve-ment.
Two newly designed, best-performing mea-sures cPMId and cPMIz have almost identical per-formance.
As the definitions in Table 1 show,cPMId incorporates corpus level significance in adocument count based version of PMI but doesnot employ the concept of document level signif-icance, whereas cPMIz employs both corpus anddocument level significance.
This demonstratesthat the concept of corpus level significance com-bined with document counts is responsible for allthe performance gains observed.To summarize, we make the following contribu-tions in this work:?
We incorporate the notion of significant co-occurrence in PMI to design a new measurecPMId that performs better than PMI as wellas other popular co-occurrence based word-association measures on both free associationand semantic relatedness tasks.
In addition,despite being resource-light, cPMId performsas well as the best known distributional sim-ilarity and knowledge based measures whichare resource-intensive.?
We investigate the source of this performanceimprovement and find that of the two notions0We consider only those word-pair occurrences whereinter-word distance between x and y is atmost s, the spanthreshold.
For a particular occurrence of x, we get a windowof size s on either side within which y can occur.
Strictlyspeaking, there should be a factor 2s in the denominator ofthe formula for PMI.
Since we are only interested in the rel-ative rankings of word-pairs, we follow the standard practiceof ignoring the 2s factor, as its removal affects only the abso-lute PMI values but not the relative rankings.of significance - corpus-level and document-level significant co-occurrence, the conceptof document level significant co-occurrenceis not helpful for PMI adaptation.
The con-cept of corpus level significance combinedwith document counts is responsible for allthe performance gains observed.2 Related WorkWord association measures can be divided intothree broad categories: knowledge based, dis-tributional similarity based, and co-occurrencebased measures.
Knowledge-based measuresare based on thesauri, semantic networks, tax-onomies, or other knowledge sources (Libermanand Markovitch, 2009; Yeh et al 2009; Milneand Witten, 2008; Hughes and Ramage, 2007).Distributional similarity-based measures comparetwo words by comparing distributional similar-ity of other words around them (Agirre et al2009; Wandmacher et al 2008; Bollegala et al2007).
In this work, our focus is on Co-occurrencebased measures and hence we do not discussKnowledge-based and Distributional similarity-based measures further.Co-occurrence based measures estimate asso-ciation between two words by computing somefunction of the words unigram and bigram fre-quencies.
Table 2 contains definitions of popu-lar co-occurrence measures.
The concept of docu-ment and corpus level significance can be appliedto any word association measure which is definedas the ratio of a variable?s observed frequency toits expected frequency.
While Chi-Square (?2),21Measure DefinitionChi-Square(?2)?x?
?
{x,?x}y?
?
{y,?y}(f(x?,y?)?Ef(x?,y?))2Ef(x?,y?
)Dice (Dice,1945)2f(x,y)f(x)+f(y)Jaccard (Jac-card, 1912)f(x,y)f(x)+f(y)?f(x,y)Log Like-lihood Ra-tio(LLR) (Dun-ning, 1993)?x?
?
{x,?x}y?
?
{y,?y}p(x?, y?
)log p(x?,y?)p(x?)p(y?
)Pointwise Mu-tual Informa-tion(PMI) (Churchand Hanks,1989)log f(x,y)f(x)?f(y)/WT-test f(x,y)?Ef(x,y)?f(x,y)(1?
f(x,y)W)W Total number of tokens in the corpusf(x), f(y) unigram frequencies of x, y in the corpusp(x), p(y) f(x)/W, f(y)/Wf(x, y) Span-constrained (x, y) word pair frequency in corpusp(x, y) f(x, y)/WTable 2: Definition of popular co-occurrence based wordassociation measures.LLR, and T-test already incorporate some notionof statistical significance, among Dice, Jaccard,and PMI, only the PMI meets this requirement.Hence our focus in this work is on designing newmeasures by incorporating the notion of signifi-cant co-occurrence in PMI.3 Incorporating Corpus LevelSignificanceIn (Chaudhari et al 2011), the concept of corpuslevel significance was introduced by bounding theprobability of observing a given corpus level phe-nomenon under a particular null model.
In the for-mula for PMI, the observed frequency of a wordpair?s occurrences is compared with its expectedfrequency under a null model which assumes in-dependent unigram occurrences.
Near a given oc-currence of the word x in the corpus, the word ycan be observed with probability f(y)/W .
Hencethe expected value of f(x, y) is f(x) ?
f(y)/W .Adapting from (Chaudhari et al 2011) and usingHoeffding?s Inequality, the probability of observ-ing a given deviation between f(x, y) and its ex-pected value f(x) ?
f(y)/W can be bounded.
Forany t > 0:P [f(x, y) ?
f(x) ?
f(y)/W + f(x) ?
t]?
exp(?2 ?
f(x) ?
t2)= ?The upper-bound ?
(= exp(?2?f(x)?t2)) denotesthe probability of observing more than f(x) ?f(y)/W + f(x)?
t bigram occurrences in the cor-pus, just by chance, under the given independentunigram occurrence null model.
With ?
as a pa-rameter (0 < ?
< 1) and t = ?ln ?/(?2 ?
f(x)),we can define a new word association measurecalled Corpus Level Significant PMI(cPMI) as:cPMI(x, y) = log f(x, y)f(x) ?
f(y)/W + f(x) ?
t= log f(x, y)f(x) ?
f(y)/W +?f(x) ?
?ln ?/(?2)where t = ?ln ?/(?2 ?
f(x)).By taking the probability of observing a givendeviation between f(x, y) and its expected valuef(x) ?
f(y)/W in account, cPMI addresses oneof the main weakness of PMI of working onlywith probabilities and completely ignoring the ab-solute amount of evidence.
In two scenarios whereall frequency ratios (that of f(x), f(y), f(x, y),and W ) are equal, PMI values will be same whilecPMI value will be higher for the case where ab-solute number of occurrences are higher.
This canbe seen easily by multiplying all of f(x), f(y),f(x, y), and W with some constant n:log n ?
f(x, y)n ?
f(x) ?
n ?
f(y)/n ?W +?n ?
f(x) ?
?ln ?/(?2)= log f(x, y)f(x) ?
f(y)/W +?1/n ?
?f(x) ?
?ln ?/(?2)> log f(x, y)f(x) ?
f(y)/W +?f(x) ?
?ln ?/(?2)= cPMI(x, y)4 Incorporating Document LevelSignificant Co-occurrenceTraditional measures like PMI can be viewed asworking with a null hypothesis where each wordin a document is generated completely indepen-dently of the other words in that document.
Witheach word, a global unigram generation probabil-ity is associated and all documents are assumedto be generated as per a multinomial distribution.Such a null model generates different expectedspan (inter-word gap) for high frequency words vs.low frequency words.
In reality, if strongly associ-ated words co-occur in a document then they do sowith low span, i.e., they occur close to each-otherregardless of the underlying unigram frequencies.224.1 Determining Document LevelSignificanceTo correct this span bias of traditional measures,a new null model is employed in (Chaudhari etal., 2011).
A bag of word is associated with eachdocument.
The null model assumes that the ob-served document is a random permutation of theassociated bag of words.
Given the occurrences ofa word-pair in the document, if the number of oc-currences with span less than a given threshold canbe explained by this null model then the word pairis assumed to be unassociated in the document.Else, some form of association is assumed.
Fol-lowing definitions are introduced in (Chaudhari etal., 2011) to formalize this concept.Definition 1 (span-constrained frequency) Letf be the maximum number of non-overlappedoccurrences of a word-pair ?
in a document.
Letf?s(0 ?
f?s ?
f) be the maximum number ofnon-overlapped occurrences of ?
with span lessthan a given threshold s. We refer to f?s as thespan-constrained frequency of ?
in the document.For a given document of length ` and a word-pair with f occurrences in it, as we vary the spanthreshold s, the number of occurrences of theword-pair with span less than s, i.e.
its span-constrained frequency f?s varies.
For a given sand the f?s resulting from it, we can ask, what isthe probability that f?
s out of f occurrences of aword-pair in a document of length `will have spanless than s, if the words in the document were tobe permuted randomly.
If this probability is lessthan some threshold , then we can assume thatthe words in the pair have some tendency of co-occurring in the document.
Formally,Definition 2 (-significant co-occurrence) Let `be the length of a document and let f be the fre-quency of a word-pair ?
in it.
For a given a spanthreshold s, define pis(f?s, f, `) as the probabilityunder the null that ?
will appear in the documentwith a span-constrained frequency of at least f?s.Given a probability threshold  (0 <  < 1) anda span threshold s, the document is said to supportthe hypothesis ??
is an -significant word-pairwithin the document?
if we have [pis(f?
s, f, `) <].The key idea is that we should concentrate onthose documents where a word pair has an -significant occurrence and ignore its occurrencesin non -significant documents.
This point is moresubtle than it appears.
Earlier, if the span of an oc-currence was less than a threshold, it was counted,else it was ignored.
In the new null model, in-stead of an individual occurrence, all occurrencesof the word-pair in the document are considered asa single unit.
Either all occurrences confirm to thenull model or they do not.
Of course, some occur-rences will have span less than the threshold whileothers will have higher span, but when consider-ing significance, all occurrences in the documentare considered significant or insignificant as a unit.This point is discussed further in Section 4.3.4.2 pis[] Computation OverheadThe detailed discussion of the computation of pis[]table can be found in (Chaudhari et al 2011).
Forour work, it suffices to know that pis[] table needsto be computed only once and hence it can be doneoffline.
We use the pis[] table made publicly avail-able1 by CSR researchers.
The use of pis[] tablesimply entails a memory lookup and does not in-crease the computation cost of a measure.4.3 Adapting PMI for Document LevelSignificanceConsider the cPMI definition given earlier.
Oneway to adapt it for document significance is to alterthe numerator such that only the span-constrainedbigram occurrences in -significant documents areconsidered in computing f(x, y).However, this simple adaptation is problem-atic.
Consider a document with f occurrences ofa word-pair of which span of f?s occurrences is at-most s, the given span threshold.
In the definitionof cPMI, the numerator takes in account only thoseoccurrences whose span is less than s, i.e., only thef?s occurrences from a document.
As discussedearlier, the -significance of a document is deter-mined by looking at all f occurrences as a whole.In the null model, whether a particular occurrencehas span less than or greater than s is not so impor-tant, what matters is that span of f?
s occurrencesout of f is at most s. The word-pair is consideredan -significant pair within the document if the ob-served span of all f occurrences of the pair can beexplained by the null model.
Hence, when adapt-ing for -significance, it is improper to count onlyf?s occurrences out of f .The way out of this difficulty is to count the doc-uments and not the words.
We do this adaptation1http://www.cse.iitb.ac.in/ damani/papers/EMNLP11/resources.html23in two steps.
First, we replace the word countswith document counts in the definition of cPMI,giving a new measure called Corpus Level Signifi-cant PMI based on Document count (cPMId):cPMId(x, y) = log d(x, y)d(x) ?
d(y)/D +?d(x) ?
?ln ?/(?2)where d(x, y) indicates the number of documentscontaining at least one span constrained occur-rence of (x, y), and d(x) and d(y) indicate thenumber of document containing x and y, D indi-cates the total number of documents in the corpus,and as before, ?
is a parameter varying between 0and 1.Having replaced the word counts with docu-ment counts, we now incorporate the concept ofdocument level significant co-occurrence (as dis-cussed in Section 4.1) in cPMId by replacingd(x, y) in numerator with Z which is defined as:Definition 3 (Z) Let Z be the number of doc-uments that support the hypothesis ?the givenword-pair is an -significant word-pair?, i.e., Z isthe number of documents for which pis(f?
s, f, `) <.The new measure is called Document and CorpusLevel Significant PMI (cPMIz) and is defined as:cPMIz(x, y) = log Zd(x) ?
d(y)/D +?d(x) ?
?ln ?/(?2)Note that cPMIz has three parameters: spanthreshold s, the corpus level significant parame-ter ?
(0 < ?
< 1) and the document level signif-icant parameter  (0 <  < 1).
In comparison,cPMI/cPMId have s and ?
as parameters whilePMI has only s as the parameter.
The three pa-rameters of cPMId are similar to those of CSR.cPMIz and cPMId differ in the fact that cP-MId does not incorporate the document level sig-nificance.
Similarly, we can design another mea-sure that differs from cPMIz in that it does not in-corporate corpus level significance.
This measureis called Document Level Significant PMI (PMIz)and is defined as:PMIz(x, y) = log Zd(x) ?
d(y)/DBaseline Measure: Suppose cPMIz were to dobetter than the PMI.
One could ask whether theimprovement achieved is due to the concept of sig-nificant co-occurrence or is it simply a result ofthe fact that we are counting documents instead ofwords.
To answer this, we design a baseline ver-sion of PMI where we simply replace word countswith document counts.
The new baseline measureis called PMI based on Document count (PMId)and is defined as:PMId(x, y) = log d(x, y)d(x) ?
d(y)/D5 Performance EvaluationHaving introduced various measures, we wish todetermine whether the incorporation of corpus anddocument level significance improves the perfor-mance of PMI.
Also, if the adapted versions per-form better than PMI, what are the sources of theimprovements.
Is it the concept of corpus level ordocument level significance or both, or is the per-formance gain simply a result of the fact that weare counting documents instead of words?
Sincethe newly introduced measures have multiple pa-rameters, how sensitive is their performance to theparameter values.To answer these questions, we repeat the exper-iments performed in (Chaudhari et al 2011), us-ing the exact same dataset, resources, and method-ology - the same 1.24 Gigawords Wikipedia cor-pus and the same eight publicly available datasets- Edinburgh (Kiss et al 1973), Florida (Nelsonet al 1980), Kent (Kent and Rosanoff, 1910),Minnesota (Russell and Jenkins, 1954), White-Abrams (White and Abrams, 2004), Goldfarb-Halpern (Goldfarb and Halpern, 1984), Word-sim (Finkelstein et al 2002), and Esslli (ESSLLI,2008).
Of these, Wordsim measures semantic re-latedness which encompasses relations like syn-onymy, meronymy, antonymy, and functional as-sociation (Budanitsky and Hirst, 2006).
All otherdatasets measure free association which refers tothe first response given by a subject on being givena stimulus word (ESSLLI, 2008).5.1 Evaluation MethodologyEach measure is evaluated by the correlation be-tween the ranking of word-associations producedby the measure and the gold-standard human rank-ing for that dataset.
Since all methods have at leastone parameter, we perform five-fold cross valida-tion.
The span parameter s is varied between 5 and50 words, and  and ?
are varied between 0 and 1.Each dataset is partitioned into five folds - four for24Edinburgh(83,713)Florida(59,852)Kent(14,086)Minnesota(9,649)White-Abrams(652)Goldfarb-Halpern(384)Wordsim(351)Esslli(272)PMI 0.22 0.25 0.35 0.25 0.27 0.16 0.69 0.38cPMI 0.23 0.28 0.40 0.29 0.29 0.17 0.70 0.46PMId 0.22 0.26 0.37 0.26 0.28 0.17 0.71 0.42cPMId 0.27 0.32 0.44 0.33 0.36 0.16 0.72 0.54PMIz 0.24 0.26 0.38 0.26 0.28 0.18 0.71 0.39cPMIz 0.27 0.32 0.44 0.34 0.35 0.18 0.71 0.53CSR 0.25 0.30 0.42 0.31 0.34 0.10 0.63 0.43Table 3: 5-fold cross validation comparison of rank coefficients for different measures.
The number of word-pairs in eachdataset is shown against its name.
The best performing measures for each dataset are shown in bold.without corpus with corpuslevel significance level signifi-canceword-based PMI: 0.075 cPMI: 0.044document-based PMId: 0.060 cPMId: 0.004with documentlevel significancePMIz: 0.059 cPMIz: 0.004CSR: 0.049Table 4: Average deviation of various measures from the best performing measure for each dataset.training and one for testing.
For each associationmeasure, the parameter values that perform beston four training folds is used for the remainingone testing fold.
The performance of a measureon a dataset is its average Spearman rank correla-tion over 5 runs with 5 different test folds.5.2 Experimental ResultsResults of the 5-fold cross validation are shownin Table 3.
From the results we conclude that theconcept of significant co-occurrence improves theperformance of PMI.
The newly designed mea-sures cPMId and cPMIz perform better than bothPMI and CSR on all eight datasets.5.3 Performance Improvement AnalysisWe can infer from Table 3 that the concept of cor-pus level significant co-occurrence and not thatof document level significant co-occurrence is re-sponsible for the PMI performance improvement.The Spearman rank correlation for cPMIz and cP-MId are almost identical.
cPMId incorporates cor-pus level significance in a document count basedversion of PMI but unlike cPMIz, it does not em-ploy the concept of document level significance.To underscore this point, we also compute thedifference between the correlation of each mea-sure from the correlation of the best measure foreach data set.
For each measure we can then com-pute the average deviation of the measure from thebest performing measure across datasets.
In Ta-ble 4 we present these average deviations.
We ob-serve that:?
Average deviation reduces as we move hor-izontally across a row - from PMI to cPMI,from PMId to cPMId, and from PMIz to cP-MIz.
This shows that the incorporation ofcorpus level significance helps improve theperformance.?
The average deviation reduces as we movevertically from the first row to the second -from PMI to PMId, and from cPMI to cP-MId.
This shows that the performance gainachieved is also due to the fact that we arecounting documents instead of words.?
Finally, the average deviation remains practi-cally unchanged as we move vertically fromthe second row to the third - from PMId toPMIz, from cPMId to cPMIz.
This showsthat the incorporation of document level sig-nificance does not help improve the perfor-mance.5.4 Parameter Sensitivity AnalysisTo find out the sensitivity of cPMId performanceto the parameter values, we evaluate it for differentparameter combinations and present the results inTable 5.
To save space, we show some of the com-binations only, though one can see the continuityof performance with gradually changing parame-ter values.25Parameters(s,?
)Edinburgh(83,713)Florida(59,852)Kent(14,086)Minnesota(9,649)White-Abrams(652)Goldfarb-Halpern(384)Wordsim(351)Esslli(272)*, 0.1 0.27 0.32 0.43 0.33 0.35 0.12 0.65 0.55*, 0.3 0.27 0.32 0.44 0.33 0.36 0.14 0.67 0.55*, 0.5 0.27 0.32 0.43 0.33 0.36 0.15 0.68 0.54*, 0.7 0.27 0.32 0.43 0.33 0.36 0.14 0.70 0.54*, 0.9 0.27 0.31 0.43 0.32 0.35 0.16 0.72 0.535w, * 0.27 0.31 0.43 0.33 0.35 0.18 0.66 0.4910w, * 0.27 0.32 0.43 0.33 0.36 0.18 0.70 0.5220w, * 0.27 0.32 0.43 0.33 0.36 0.18 0.71 0.5430w, * 0.27 0.32 0.42 0.32 0.36 0.18 0.71 0.5440w, * 0.27 0.31 0.42 0.32 0.35 0.17 0.71 0.5450w, * 0.27 0.31 0.42 0.31 0.36 0.17 0.72 0.53*, * 0.27 0.32 0.44 0.33 0.36 0.16 0.72 0.5420w,0.7 0.27 0.32 0.43 0.33 0.36 0.16 0.70 0.5450w,0.9 0.27 0.31 0.41 0.31 0.35 0.17 0.72 0.53Table 5: 5-fold cross validation performance of cPMId for various parameter combinations.
* indicates a varying parameter.Edinburgh(83,713)Florida(59,852)Kent(14,086)Minnesota(9,649)White-Abrams(652)Goldfarb-Halpern(384)Wordsim(351)Esslli(272)PMI 0.22 0.25 0.35 0.25 0.27 0.16 0.69 0.38PMId 0.22 0.26 0.37 0.26 0.28 0.17 0.71 0.42PMI2 0.24 0.30 0.43 0.31 0.29 0.08 0.62 0.44PMI2d 0.23 0.29 0.42 0.31 0.30 0.06 0.61 0.43nPMI 0.25 0.30 0.41 0.30 0.31 0.13 0.72 0.47nPMId 0.23 0.26 0.28 0.24 0.30 0.15 0.71 0.46cPMId(?
: 0.9) 0.27 0.31 0.43 0.32 0.35 0.16 0.72 0.53Table 6: 5-fold cross validation comparison of cPMId with other PMI variants.From the results we conclude that the per-formance of cPMId is reasonably insensitive tothe actual parameter values.
For a large rangeof parameter combinations, cPMId?s performancevaries marginally and most of the parameter com-binations perform close to the best.
If one does nothave a training corpus then one can chose the bestperforming (20w, 0.7) as default parameter values.As an aside, introducing extra tunable pa-rameter occasionally reduces performance, as isthe case for Goldfarb-Halpern and Esslli datasetswhere (*,*) is not the best performing cobination.This happens when the parameters combinationthat performs best on the four training fold turnsout particularly bad for the testing fold.5.5 Comparison with other measuresBefore comparing cPMId with other measures, wenote that while all co-occurrence measures beingcompared have span threshold s as a parameter,cPMId has an extra tunable parameter ?.
Whilewe would like to argue that part of power of cP-MId comes from this extra tunable parameter, foran arguably fairer comparison, we would like tofix the ?
value and then compare so that all meth-ods have only one tunable parameter s. In Table 5we find that ?
= 0.9 performs best on the fewestnumber of datasets and hence we select this fixedvalue for comparison.
However most of the con-clusions that follow do not change if we were tofix some other ?
value, or keep it variable.5.5.1 Comparison with other PMI variantsIn Section 3 we pointed out the PMI only workswith probabilities and ignores the absolute amountof evidence.
Another side-effect of this phe-nomenon is that PMI over-values sparseness.
Allfrequency ratios (that of f(x), f(y), and f(x, y))being equal, bigrams composed of low frequencywords get higher score than those composed ofhigh frequency words.
In particular, in case ofperfect dependence, i.e.
f(x) = f(y) = f(x, y),PMI(x, y) = log Wf(x,y) .
cPMId addresses thisweakness by explicitly bounding the probabilityof observing a given deviation between f(x, y)and its expected value f(x) ?
f(y)/W .
Other re-26Edinburgh(83,713)Florida(59,852)Kent(14,086)Minnesota(9,649)White-Abrams(652)Goldfarb-Halpern(384)Wordsim(351)Esslli(272)Dice 0.20 0.27 0.43 0.32 0.21 0.09 0.59 0.36Jaccard 0.20 0.27 0.43 0.32 0.21 0.09 0.59 0.36?2 0.24 0.30 0.43 0.31 0.29 0.08 0.62 0.44LLR 0.20 0.26 0.40 0.29 0.18 0.03 0.51 0.38TTest 0.17 0.23 0.37 0.26 0.17 -0.02 0.45 0.33cPMId(?
: 0.9) 0.27 0.31 0.43 0.32 0.35 0.16 0.72 0.53Table 7: 5-fold cross validation comparison of cPMId with other co-occurrence based measures.Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) 0.74(reimplemented in (Yeh et al 2009)) 0.71Compact Hierarchical ESA (Liberman and Markovitch, 2009) 0.71Hyperlink Graph (Milne and Witten, 2008) 0.69Graph Traversal (Agirre et al 2009)) 0.66Distributional Similarity (Agirre et al 2009)) 0.65Latent Semantic Analysis (Finkelstein et al 2002) 0.56Random Graph Walk (Hughes and Ramage, 2007) 0.55Normalized Path-length (lch) (Strube and Ponzetto, 2006) 0.55cPMId(?
: 0.9) 0.72Table 8: Comparison of cPMId with knowledge-based and distributional similarity based measures for the Wordsim dataset.searchers have addressed this issue by modifyingPMI such that its upper value gets bounded.Since the maximum value of f(x,y)f(x)?f(y)/W is1f(x,y)/W , one way to bound the former is to di-vide it by later.
(Daille, 1994) defined PMI2 as:PMI2(x, y) = logf(x,y)f(x)?f(y)/W1f(x,y)/W= log f(x, y)2f(x) ?
f(y)In (Bouma, 2009), it was noted that max.
and min.value of PMI2 are 0,?
?, whereas one can get1,-1 as the bounds if one normalize PMI as nPMI:nPMI(x, y) =log f(x,y)f(x)?f(y)/Wlog 1f(x,y)/WIn Table 6, we compare the performance ofword and document count based variants of PMI2and nPMI with PMI and cPMId.
We find thatwhile both nPMI and PMI2 perform better thanPMI,cPMId performs better than both variants ofnPMI and PMI2 on almost all datasets.5.5.2 Comparison with other co-occurrencebased measuresIn Table 7, we compare cPMId with other co-occurrence based measures defined in Table 2.We find that cPMId performs better than all otherco-occurrence based measures.
Note that perfor-mance of Jaccard and Dice measure is identical tothe second decimal place.
This is because for ourdatasets f(x, y)  f(x) and f(x, y)  f(y) formost word-pairs under consideration.5.5.3 Comparison with non co-occurrencebased measuresFor completeness of comparison, we also comparethe performance of cPMId with distributional sim-ilarity and knowledge based measures discussed inSection 2.
Of the datasets discussed here, thesemeasures have only been tested on the Wordsimdataset.
In Table 8, we compare the performanceof cPMId with these other measures on the Word-sim dataset.
We can see that cPMId compares wellwith the best non co-occurrence based measures.6 Conclusions and Future WorkBy incorporating the concept of significant co-occurrence in PMI, we get a new measure whichperforms better than other co-occurrence basedmeasures.
We investigate the source of the perfor-mance improvement and find that of the two no-tions of significance: corpus-level and document-level significant co-occurrence, the concept of cor-pus level significance combined with use docu-ment counts in place of word counts is responsi-ble for all the performance gains observed.
Wealso find that the performance of the newly intro-duced measure cPMId is reasonably insensitive tothe values of its tunable parameters.AcknowledgementsWe thank Dipak Chaudhari and Shweta Ghonghefor their help with the implementation.27ReferencesEneko Agirre, Enrique Alfonseca, Keith Hall, JanaKravalova, Marius Pasca, and Aitor Soroa.
2009.A study on similarity and relatedness using distribu-tional and wordnet-based approaches.
In NAACL-HLT.Danushka Bollegala, Yutaka Matsuo, and MitsuruIshizuka.
2007.
Measuring semantic similarity be-tween words using web search engines.
In WWW,pages 757?766.Gerlof Bouma.
2009.
Normalized (pointwise) mutualinformation in collocation extraction, from form tomeaning: Processing texts automatically.
In Pro-ceedings of the Biennial GSCL Conference.Alexander Budanitsky and Graeme Hirst.
2006.
Eval-uating wordnet-based measures of lexical semanticrelatedness.
Computational Linguists, 32(1):13?47.Dipak L. Chaudhari, Om P. Damani, and Srivatsan Lax-man.
2011.
Lexical co-occurrence, statistical sig-nificance, and word association.
In EMNLP.Kenneth Ward Church and Patrick Hanks.
1989.
Wordassociation norms, mutual information and lexicog-raphy.
In ACL, pages 76?83.B.
Daille.
1994.
Approche mixte pour l?extraction au-tomatique de terminologie: statistiques lexicales etl-tres linguistiques.
Ph.D. thesis, Universitie Paris 7.L.
R. Dice.
1945.
Measures of the amount of ecolog-ical association between species.
Ecology, 26:297?302.Ted Dunning.
1993.
Accurate methods for the statis-tics of surprise and coincidence.
ComputationalLinguistics, 19(1):61?74.ESSLLI.
2008.
Free association task atlexical semantics workshop esslli 2008.http://wordspace.collocations.de/doku.php/workshop:esslli:task.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-tan Ruppin.
2002.
Placing search in context: theconcept revisited.
ACM Trans.
Inf.
Syst., 20(1):116?131.Evgeniy Gabrilovich and Shaul Markovitch.
2007.Computing semantic relatedness using wikipedia-based explicit semantic analysis.
In IJCAI.Robert Goldfarb and Harvey Halpern.
1984.
Word as-sociation responses in normal adult subjects.
Jour-nal of Psycholinguistic Research, 13(1):37?55.T Hughes and D Ramage.
2007.
Lexical semantic re-latedness with random graph walks.
In EMNLP.P.
Jaccard.
1912.
The distribution of the flora of thealpine zone.
New Phytologist, 11:37?50.G.
Kent and A. Rosanoff.
1910.
A study of associa-tion in insanity.
American Journal of Insanity, pages317?390.G.
Kiss, C. Armstrong, R. Milroy, and J. Piper.
1973.An associative thesaurus of english and its computeranalysis.
In The Computer and Literary Studies,pages 379?382.
Edinburgh University Press.Sonya Liberman and Shaul Markovitch.
2009.
Com-pact hierarchical explicit semantic representation.
InProceedings of the IJCAI 2009 Workshop on User-Contributed Knowledge and Artificial Intelligence:An Evolving Synergy (WikiAI09), Pasadena, CA,July.David Milne and Ian H. Witten.
2008.
An effective,low-cost measure of semantic relatedness obtainedfrom wikipedia links.
In ACL.D.
Nelson, C. McEvoy, J. Walling, and J. Wheeler.1980.
The university of south florida homographnorms.
Behaviour Research Methods and Instru-mentation, 12:16?37.W.A.
Russell and J.J. Jenkins.
1954.
The completeminnesota norms for responses to 100 words fromthe kent-rosanoff word association test.
Technicalreport, Office of Naval Research and University ofMinnesota.Michael Strube and Simone Paolo Ponzetto.
2006.Wikirelate!
computing semantic relatedness usingwikipedia.
In AAAI, pages 1419?1424.T.
Wandmacher, E. Ovchinnikova, and T. Alexandrov.2008.
Does latent semantic analysis reflect humanassociations?
In European Summer School in Logic,Language and Information (ESSLLI?08).Katherine K. White and Lise Abrams.
2004.
Free as-sociations and dominance ratings of homophones foryoung and older adults.
Behavior Research Meth-ods, Instruments, & Computers, 36(3):408?420.Eric Yeh, Daniel Ramage, Chris Manning, EnekoAgirre, and Aitor Soroa.
2009.
Wikiwalk: Ran-dom walks on wikipedia for semantic relatedness.
InACL workshop ?TextGraphs-4: Graph-based Meth-ods for Natural Language Processing?.28
