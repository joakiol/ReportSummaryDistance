Parsing Strategies with 'Lexicalized' Grammars:Appl icat ion to Tree Adjoining Grammars *Yves SCHABES,  Anne  ABE ILLE**and  Arav ind  K. JOSHIDepartment of Computer and Information ScienceUniversity of PennsylvaniaPhiladelphia PA 19104-6389 USAschabes~linc.cis.upenn.edu abeille~cis.upenn.edu joshi~eis.upenn.eduABSTRACTIn this paper we present a general parsing strategy thatarose from the development of an Earley-type parsing al-gorithm for TAGs (Schabes and Joshi 1988) and from re-cent linguistic work in TAGs (Abeille 1988).In our approach elementary structures are associatedwith their lexical heads.
These structures specify extendeddomains of locality (as compared to a context-free gram-mar) over which constraints can be stated.
These con-straints either hold within the elementary structure itselfor specify what other structures can be composed with agiven elementary structure.We state the conditions under which context-free basedgrammars can be 'lexicalized' without changing the lin-guistic structures originally produced.
We argue that evenif one extends the domain of locality of CFGs to trees, us-ing only substitution does not givo the freedom to choosethe head of each structure.
We show how adjunction al-lows us to 'lexicalize' a CFG freely.We then show how a 'lexicalized' grammar naturallyfollows from the extended omain of locality of TAGs andpresent some of the linguistic advantages ofour approach.A novel general parsing strategy for 'lexicalized' gram-mars is discussed.
In a first stage, the parser builds a setstructures corresponding to the input sentence and in asecond stage, the sentence is parsed with respect o thisset.
The strategy is independent of the linguistic theoryadopted and of the underlying rammar formalism.
How-ever, we focus our attention on TAGs.
Since the set oftrees needed to parse an input sentence is supposed to befinite, the parser can use in principle any search strategy.Thus, in particular, a top-down strategy can be used sinceproblems due to recursive structures are eliminated.
Theparser is also able to use non-local information to guidethe search.We then explain how the Earley-type parser for TAGscan be modified to take advantage of this approach.
*This work is partially supported by ARO grant DAA29-84-9-007, DARPA grant N0014-85-K0018, NSF grants MCS-82-191169and DGR-84-10413.
The second author is also partially supported byJ.W.
Zellldja grant.
The authors would llke to thank Mitch Marcusfor his helpful conunents about this work.
Thanks are also due toEllen Hays.
**Visiting from University of Paris VII.1 'Lexicalization' of grammar for-malismsMost of the current linguistics theories tend to give lexicalaccounts of several phenomena that used to be consid-ered purely syntactic.
The information put in the lexi-con is therefore increased and complexified (e.g.
lexicalrules in LFG, used also by HPSG, or Gross 1984's lexicon-grammar).
But the question of what it means to 'lexical-ize' a grammar is seldom addressed.
The possible conse-quences of this question for parsing are not fully investi-gated.
We present how to 'lexicalize' grammars uch asCFGs in a radical way, while possibly keeping the rules intheir full generality.
If one assumes that the input sentenceis finite and that it cannot be syntactically infinitely am-biguous, the 'lexicalization' simplifies the task of a parser.We say that a grammar formalism is 'lexicalized' if itconsists of:?
a finite set of structures to be associated with lexicalitems, which usually will be heads of these structures,?
an operation or operations for composing thestructures.
1 The finite set of structures define thedomain of locality over which constraints are speci-fied and these are local with respect o their lexicalheads.Not every grammar formalism in a given form is in a'lexicalized' form.
For example, a CFG, in general, willnot be in a 'lexicalized' form.
However, by extending itsdomain of locality, it can be 'lexicalized'.
We require thatthe 'lexicalized' grammar produces not only the same lan-guage as the original grammar, but also the same struc-tures (or tree set)?We propose to study the conditions under which sucha 'lexicalization' is possible for CFGs and TAGs.
Thedomain of locality of a CFG can be extended by usinga tree rewriting system that only uses substitution.
Westate the conditions under which CFGs can be 'lexlcalized'without changing the structures originally produced.
Weargue that even if one extends the domain of locality ofCFGs to trees, using only substitution does not give thefreedom to  choose the head of each structure.
We then1 By 'lexicalization' we mean tlmt in each structure there is a lex-ical item that is realized.
We do not mean just adding features (suchas head) and unification equations to the rules of the formalism.Categorlal grammars are 'lexicaUzed' according to our definition,However, they do not correspond in a simple way to a rtde-basedsystem that could be used for top-down recognition.57,~show how adjunction enables one to freely 'lexicalize' aCFG.2 'Lexical izat ion'  of  CFGsThe domain of locality of CFGs can be easily extendedby using a tree rewriting grammar.
This tree rewritinggrammar consists of a set of trees that are not restricted tobe of depth one (as in CFGs).
It uses only substitution asa combining operation.
Substitution can take place onlyon non-terminal nodes of the frontier of each tree.
Thelanguage is defined to be the set of strings on the frontiersof trees whose roots are labeled by a distinguished symbolS.
It is easy to see that the set of languages generatedby this tree rewriting grammar is exactly the same set ascontext-free languages.If no recarsive chain rules exist, it is formally possibleto 'lexicalize' a CFG with this tree rewriting grammar, aRecursive chain rules are disallowed since they introduceunbounded structures with no lexical items attached tothem.Although a CFG can be 'lexicalized' by using trees, it isnot possible to choose freely the lexical item that plays therole of the head for each structure.
Consider the followingexample:S ~ NP  VPVP  "-~ adv VPVP  --+ vNP  --+ nThe grammar can be qexicalized' as follows:SNP VP/ \adv VPVP VP NPA I Iadv VP v nHowever, in this 'lexiealization' one is forced to chooseadv as the head of the structure given in the first tree.
It isnot possible to choose the verb v as the head of this struc-ture.
If one tried to do so, recursion on the substitutionof the VP node would be inhibited.2"his example shows that although it is possible to 'lexi-calize' CFGs, substitution alone does not allow us to freelychoose the lexical heads.
Substitution alone forces us tomake choices that might not be syntactically and seman-tically justified.Tree adjoining grammars (TAGs) are also a tree-basedsystem, ltowever, the major composition operation inTAGs is adjoining or adjunct ion.
I t  builds a new treefrom an auxiliary tree # and a tree c~ (a is any tree, initial,auxiliary or derived by adjunction).
The resulting tree iscalled a derived tree.
Let t~ be atree containing a noden labeled by X and let # be an auxiliary tree whose rootnode is also labeled by X.
Then the adjunction of fl to aat node n results a tree 7 as shown in Figure 1.
Adjunetionenables to factor recursion from local dependencies.aNote that  a CFG in Greibach normal  form can be 'lexicallzed'trivially.
But since G~eihach normal  form of a given CFG might notgenerate the same tree set as  the  original grammar,  it cannot beused as a 8.eneral method for 'lexicaUzation'.
(a) (g)AFigure 1: The mechanism of adjunctionThe previous CFG can be 'lexicalized' by using adjunc-tion as follows: 4sNP VP NP VPI I Av n adv VPThe auxiliary tree rooted by VP can be inserted in theS tree on tile VP node by adjunction.
Using adjunctionone is thus able to choose the appropriate l xical item ashead.
This example illustrates the fact that a CFG withno recursive chain rules can be 'lexicalized' in TAGs, andthat if that is done the head can be freely chosen.3 TAGs  and ' lexical izat ion'TAGs are 'naturally' lexicalized because they used an ex-tended domain of locality.
TAGs were first introducedby Joshi, Levy and Takabashi (1975) and Joshi (1985).For more details on the original definition of TAGs, werefer the reader to Joshi (1985), Kroch and Joshi (1985)or Vijay-Shanker (1987).
It is known that Tree Adjoin-ing Languages (TALs) are mildly context-sensitive.
TALsproperly contain context-free languages.
It is also possi-ble to encode a context-free grammar with auxiliary treesusing adjunction only.
However, although the languagescorrespond, the possible ncoding does not directly reflectthe original context-free grammar since this encoding usesadjunction.Although adjunction is more powerful than substitutionand could be used to simulate it, in recent linguistic workin TAG (Abeill~ 1988) substitution has been used in ad-dition to adjunction in order to obtain appropriate struc-tural descriptions in certain cases, such as verbs takingtwo sentential arguments (e.g.
"John equates olving thisproblem with doing the impossible").
Adding substitutiondoes not change the mathematical properties of TAGs.We describe very briefly the Tree Adjoining Grammarformalism with adjunction and substitution.A Tree Adjoining Grammar  is a tree-based systemthat consists of.three finite sets of trees: I, A and L. Thetrees in I O A tJ L are called e lementary trees.The trees in I are called initial trees.
Initial trees rep-resent basic sententiai structures.
They are usually con-sidered as projections of the verb and they take nominal4We chose v as lexical head of the S tree but  we could have chosenn instead (although it is not motivated).579complements.
Initial trees (see the left tree in Figure 2)are rooted in S and their frontier consists of terminal sym-bols (including the empty string) and non-terminal nodesto be substituted for.The trees in A are called auxi l iary trees.
They canrepresent constituents which ar e adjuncts to basic struc-tures (adverbial).
They can also represent basic senten-tial structures corresponding to verbs or predicates takingsentential complements.
Auxiliary trees (see right tree inFigure 2) are characterized as follows:?
internal nodes are labeled by non-terminals;?
leaf nodes are labeled by terminals or by non-terminalnodes to be substituted except for exactly one node(called the foot node) labeled by a non-terminal onwhich only adjunction can apply; furthermore the la-bel of the foot node is the same as the label of theroot node.Initial ~:  Auxiliary ~c~:$; ;substitution nodesFigure 2: Schematic initial and auxiliaxy treesThe trees in L are called lexical trees.
They repre-sent basic categories or constituents which serve as argu-ments, to initial or auxiliary trees.
They are reduced to apre-terminal node in the case of simple categories or areexpanded into tree structures in the case 0f compounds.Structurally they are characterized the same way as initialtrees except hat they are not necessary rooted by S.As noted in Section 2, the major composition operationin TAGs is adjunct ion.We define subst i tut ion in TAGs to take place on spec-ified nodes on the frontiers of elementary trees.
When anode is marked to be substituted, no adjunction can takeplace on that node.
Furthermore , substitution is alwaysmandatory.
In case of substitution on a node labeled byS (sentential complement), only trees derived from initialtrees (therefore rooted by S) can be substituted.
In allother cases, any tree derived from a lexlcal tree rootedby the same label as the given node can be substituted.The resulting tree is obtained by replacing the node bythe derived tree.
Substitution is illustrated in Figure 3.We conventionally mark substitution odes by a downar row (~).We define the tree set  of a TAG G, T(G) to be the setof all derived trees starting from initial trees in I. Further-more, the str ing language generated by a TAG, ?
:(G),is defined to be the set of all terminal strings of the treesin T(G).Grammar ules defined by the linguistic theory are notthe same as the rules used by the parser--let us refer tothem as parser  rules.
A parser rule is defined to bea structure encoding a rule of the grammar (or a set ofrules) instantiated bythe parser when it comes to alex-580/\Figure 3: Mechanism of substitutionical item (considered to 'yield' the rule(s)).
It is thus aunique object.
It is individualized by the lexical item,which is itself individualized by its position in the inputstring.
The lexical item is directly inserted into the struc-ture corresponding to the parser ule, and such a rule canonly occur once.
Lexleal i tems are differentiated by theirrealization in the input sentence and also their position inthe sentence.
Therefore a given rule corresponds to ex-actly one lexical item in the input sentence.The structures are produced by lexical items which serveas heads.
If a structure has only one terminal, the terminalis the head of the structure; if there are several terminals,the choice of the head is linguistically motivated, e.g.
bythe principles of X theory.
S also has to be considered asthe projection of a lexical head, usually V. Each lexicalitem corresponds to as many entries as there are possiblecategory or argument structures.The category structure is a lexical tree that is notnecessarily reduced to a single category.
It corresponds tothe maximal projection of a category in the case of simplephrases, to the entire compound, in the case of compoundcategories.Category structures can be of two different kinds:?
lexical trees reduced to a single category: ~DETNP PPDET N of N$ DET, I I t hell) JR bunch(i )?
lexical trees that consist of a phrase:NP NPA ID~ N NI Iboy (I) Ma r y (I)The argument  st ructure is not reduced to a list ofarguments as the usual subcategorization frames.
It is thesyntactic structure constructed with the lexlcal value ofthe predicate and with all the nodes for its arguments.
Theargument structure for a predicate is its maximal struc-ture.
An argument is present in the argumefit structureeven if it is optional and its optionality is stated in thestructure.SThe index in "parentheses on a lexical item that produces thestructure ncodes the position of the lexical item in the string.A simple case of a argument structure is a verb withits subcategorized arguments.
For example, the verb saw(at position i) generates the following structures (amongothers): 6S 8NPo$ VPV NPI$ V $I Isaw(l) eaw(i)The left structure corresponds to:0 Jolt.
1 aaw 2 llary a ( i=  2)and the other to:0 John  1 saw 2 that  3 Mary  4 le f t  5.
( i - - - -2 )An argument structure can correspond to either one ora set of syntactic surface structures.
The lexical headwill then produce a set of possible trees, one for NP0 sawNP1 and another for whol did  NP 0 see e i ?, for exam-ple.
If one defines principles for building such sets oftrees, these principles will correspond to syntactic rulesin a derivation-based theory of grammar.Category and argument structures thus instantiated asthe parser scans the input string are combined together ina sentence t~tructure by adjoining or substituting.As Gross (1984), we consider verbs, nouns, and adjec-tives as predicates yielding sentences.
They can take nomi-nal or sentential arguments.
If the predicate takes nominalarguments it produces an initial tree.
If it takes a senten-tial argument then it produces an auxiliary tree.
Puttingarguments into predicates i done by substituting nomi-nal arguments or by adjoining a predicate structure to itssentential argument.Adjuncts are represented as auxiliary trees rooted bythe category of the node they are adjoined to.
They arealso produced by a head.
They can be reduced to a basiccategory or take nominal or sentential arguments intro-duced by substitution.Example,~ of Adjuncts:S vpA AS VP PP S SA A AS ADV P NP$ SC S~l I I probablYll) during(i) while(i)4 Parsing 'lexicalized' grammarsIf we have a 'lexicalized' grammar, the grammar of theparser can be reduced to a set of structures whose naturedepends on the input string and whose size is proportionalto the length of the sentence (if we suppose that the num-ber of structures associated with a lexical item is finite).Since each structure' ( rule') corresponds to a token in the?We put indices on categories to express syntactic roles (0 forsubject, 1 for object).sentence, it can be used only once.
Rules are now differen-tiated by their realization in the sentence.
The number ofrules that can be used for a given sentence is bounded andis proportional to the length of the sentence.
Since eachrule can be used once, recursion does not lead to the usualnon-termination problem.
Once a structure has been cho~sen for a given token, the other possible structures for thesame token do not participate in the parse.
Of course, ifthe sentence is ambiguous, there may be more than onechoice.If one adopts an off-line parsing algorithm, the parsingproblem is reduced to the following two steps:t First produce the set of structures corresponding toeach word in the sentence.
This step performs the roleof an expanded morphological nalysis (or tagging).?
Then put the argument structures into the predicatestructures.
This step performs a modified syntacticanalysis.In principle any parsing strategy can be applied to executethe second step, since the number of structures producedjs finite and since each of them corresponds to a token inthe input string, the search space is finite and terminationis guaranteed.
In principle, one can proceed inside out, leftto right or in any other way.
Of course, standard parsingalgorithm can be used too.
In particular, we can use thetop-down parsing strategy without encountering the usualproblems due to recursion.
Problems in the prediction stepof the Earley parser used for unification-based formalismsno longer exist.
The use of restrictors as proposed byShieber (1985) is no longer necessary and the difficultiescaused by treating subcategorization as a feature is nolonger a problem.By assuming that the number of structures associatedwith a lexical item is finite, since each structure has a lexi-cal item attached to it, we implicitly make the assumptionthat an input string of finite length cannot be syntacticallyinfinitely ambiguous.Since the trees are produced by the input string, theparser can use information that might be non-local toguide the search.
For example, consider the language gen-erated by the following CFG (example due to Mitch Mar-cus):S ~ A IBA ~ aAlaxB -* aBlayThis grammar generates the language:{a*x} U {a'y}.In a standard CFG parsing algorithm, A's and B's will bebuilt until the last token in the input (x or y) is recog-nized.
It would require unbounded look-ahead to decidewhich rule (3 -+ A or S ~ B) to choose.
One can encodethe grammar in TAG as follows:S S A BA AA Aa x a ySuppose that the heads of the initial trees are respec-tively x and y and that a is the head of both auxiliary581trees.
Then, if the elementary trees are built according tothe input string, and if a top-down strategy is used, onlyA or B trees will be built.An application concerns the parsing of discontinuousconstituents.
They are recognized even if there are un-bounded insertions between their components and even iftheir 'head' is the last element of the string.In the two-step strategy described here, before the firststep is taken, there is no grammar.
After the first step, wehave a grammar whose size is proportional to the lengthof the input string.
The size of the grammar to be takeninto consideration i  the analysis of the parsing complexityof grammar formalisms has been reduced to an amountproportional to the length of the input.
Although we havenot yet investigated the implication of this approach onsome complexity results, we feel that some of them mightbe improved.It is possible to express the parsing problem in a de-cidable deduction system on trees (similar to Lambek'sdeduction system on categories (1958 and 1961)).
Thegrammar can be thought as a five-tuple (VN, ~, O, S, Lex)where:?
VN is a finite set of non-terminal symbols,?
~ is a finite set of alphabet symbols,?
O is the set of trees constructed with P,* and VN (theelements of Z* having ranked 0).?
Lex is the lexicon , i.e.
a function from lexical itemsto finite subsets of O: P?'
--+ 2?
(finite).A sequent is defined to be of the form:Vl,.. -, rn ~ A, where ri E O and A E VNTwo inference rules combine two trees of the left handside to form a new one.
One inference rule correspondsto adjunction of two trees, and the other to substitution ofa node in one tree by the other tree.
Once two trees arecombined, they are replaced by the resulting tree in theleft hand side of the seouent.
This facts takes into accountthat each tree corresponds to a single lexical item in theinput string.
Therefore each tree can be used only once.Axioms of the system are of the form:v ---+ Awhere r is a completed tree rooted by A.The sequentT1," ? "
,Tn "----+ Ais said to be provable if the sequent can be reduced (bythe inference rules) to an axiom; we write:~- r l , .
.
.
, r ,  --+ A.Since there are finitely many ways to combine a finite num-ber of trees with each other, the system is decidable.The language generated by such system is defined to be:= {a i , ' .
.
,anl3rl e Lex(al) s. t. ~- r l , ' " , rn  ----+ S}Also, one can state a necessary condition on the correct-ness of a sentence similar to the category count theoremof van Benthem (1985 and 1986).5 Extending the Earley-typeparser for TAGsAn Earley-type parser for TAGs has been proposed bySchabes and Joshi (1988a).
It takes as input a TAG anda sentence to be parsed.
It places no restrictions on thegrammar.
The algorithm is a bottom-up arser that usestop-down filtering.
It is able to parse constraints on ad-junction, substitution and feature structures for TAGs asdefined by Vijay-Shanker (1987) and Vijay-Shanker andJoshi (1988).
It is able to parse directly CFGs and TAGs.Thus it embeds the essential aspects of PATR-II as definedby Shieber (1984 and 1986).
Its correctness was proven inSehabes and Joshi (1988b).
The concepts of dotted ruleand states have been extended to TAG trees.
The algo-rithm as described by Schabes and Joshi (1988a) manip-ulates states of the form:s = \[a, dot, side, pos, l, fl, fi, star, t\[, b\[, snbst?\]where a is a tree, dot is the address of the dot in the tree,side is the side of the symbol the dot is on (left or right),pos is the position of the dot (above or below), star is anaddress in a and l, f~, fr, star, t~, b~ are indices of positionsin the input string.
The variable subst?
is a boolean thatindicates whether the tree has been predicted for substi-tution.The algorithm uses nine processes:?
The Scanner  allows lexical items to be recognized.?
Move dot  down and Move dot  up perform a treetraversal that allow the parser to scan the input fromleft to right.?
The Left P red ic tor  predicts an adjunetion if it ispossible.?
Suppose that the auxiliary tree that we left-predictedhas been recognized as far as its foot, then the LeftCompletor  tries to recognize what was pushed underthe foot.?
Once the subtree pushed under the foot has been rec-ognized, the R ight  P red ic tor  tries to recognize theother half of the auxiliary tree.?
If the auxiliary tree has been totally recognized, theRight  Completor  tries to recognize the rest of thetree in which the auxiliary tree has been adjoined.?
The Subst i tu t ion  Pred ic tor  performs the same op-erations as Earley's original predictor.
It predicts forsubstitution (when appropriate) all lexical trees or ini-tial trees that could be substituted.?
If the tree that we predicted for substitution hasbeen totally recognized, the Subst i tu t ion  Comple-tor  tries to recognize the rest of the tree in which wepredicted a substitution.The Earley-type parser can be extended to take advan-tage of  the lexicon-based strategy proposed earlier.
Oncethe input string has been scanned and the correspondingelementary trees have been built, the parser will proceedbottom-up using the top-down filtering from the initialtrees that have been built.
In order to take into accountthat each tree is unique and therefore can be used onlyonce, a new component r is added to the states.
A stateis now defined to be:s = \[a, dot, side, pos, l, fl, fr, star, t~, b~, subst?, r\]r encodes the trees corresponding to the input string thathave not yet been used:r ~--- {{"/11, """ , '~'lk},"" : ,  { '~ml,"""  , '~mk}}where {7i l , ' " ,7~j} is the set of trees generated by thelexical item a~.582The left predictor must be modified so that it predictsonly trees that are in the set F of the given state.
As soonas one tree (say 7in) is used, the entire set of trees cor-responding to the same token ({711," ' ,7i j})  cannot beused later on.
Of course, all competitive paths are takenin parallel as in the usual Earley parser.
The way thatF is modified by the Left Predictor is illustrated in thefollowing figure:A addedtoSir=((~ ,...a } ..... {~ ..... ~ ), ..., (v ..... v 1} r=({~ ,...,~ } ........ (~ ..... r }1n lr il is ml mt 11 lr ml ratFigure 4: Update of F in the Left PredictorThe tree 71u is predicted and therefore the trees corre-sponding to the token ai ({ 'Y / l , - ' " ,  "/is}) are removed from1 a"The scanner must also be slightly modified since thehead of the structure is differentiated not only by its lexicalvalue but al,'~o by its position in the string.6 ConclusionIn this paper we presented a general parsing strategy basedon 'lexicalized' grammar.
We defined the notion of lexi-calization of a grammar.
We showed how a CFG can be'lexicalized' by using only substitution.
But the use ofadjunction permits 'lexicalization' with linguistically mo-tivated structures.
TAGs have been shown to be naturally'lexicalized'.
Then we gave an overview of the specific lex-icon of TAGs.
The %xicalization ~ of grammar lead us tointroduce a two step parsing strategy.
The first step picksup the set of structures corresponding to each word in thesentence.
The second step puts the argument structuresinto predicate structures.
Therefore, the relationship be-tween the morphological nd syntactic analyses has beenmodified.
In the first step, structures instead of categoriesare associated with lexical items.
The strategy has beenshown to be able to use non-local information in the in-put string.
Also problems due to recursion are eliminated.The grammar of the parser has been reduced to a set ofstructures whose size is proportional to the length Of theinput sentence.
Furthermore, the parsing strategy appliesto any parsing algorithm; in particular top-down.
It canbe formalized into a decidable deduction system that hasfinite search space for a sentence of finite length.
TheEarley-type parser for TAGs has been easily extended totake advantage of this strategy.R e f e r e n c e sAbeilld, Anne, 1988.
Parsing French with Tree AdjoiningGrammar: some Linguistic Accounts.
In Proceeding of the 12 ?
'International Conference on Computational Linguistics.van Benthem, Johns, 1985.
Lambek Calculus.
Manuscript,Filosofisch Instituut, Rijks Universiteit, Groningen.van Benthem, Johan, 1986.
Essays on Logical Semantics,Chapter 7, pages 123-150.
D. Reidel Publishing Company.Gross, Manriee, 1984.
Lexicon-Grammar nd the SyntacticAnalysis of French.
In Proceeding of the 10 th InternationalConference on Computational Linguistics.Joshi, Aravind K., 1985.
How Much Context-Sensitivlty is Nec-essary for Characterizing Structural Descriptions--Tree Ad-joining Grammars.
In Dowry, D.; Karttunen, L.; and Zwicky,A.
(editors), Natural Language Processing--Theoretical, Com-putational and Psychological Perspectives.
Cambridge Univer-sity Press, New York.
Originally presented in 1983.Joshi, A. K.; Levy, L. S.; and Ta~ahashi, M, 1975.
Tree Ad-junct Grammars.
J Comput.
Syst.
Sci.
10(1).Kroch, A. and Joshi, A. K., 1985.
Linguistic Relevance ofTree Adjoining Grammars.
Technical Report MS-CIS-85-18,Department ofComputer and Information Science, Universityof Pennsylvania.Lambek, Joachim, 1958.
The Mathematics of Sentence Struc-ture.
American Mathematical Monthly 65:154-170.Lambek, Joachim, 1961.
On the Calculus of Syntactic Types.In Proceedings of the Symposium on Applied Mathematics,pages 166-178.Schabes, Yves and Joshi, Aravind K., 1988 (a).
An Earley-Type Parsing Algorithm for Tree Adjoining Grammars.
In 26 thMeeting of the Association for Computational Linguistics.Schabes, Yves and Joshi, Aravind K., \]988 (b).
An Earley-type Parser for Tree Adjoining Grammars.
Technical Report,Department of Computer and Information Science, Universityof Pennsylvania.Shieber, Stuart M., 1984.
The Design of a Computer Languagefor Linguistic Information.
In 22 ~d Meeting of the Associationfor Computational Linguistics, pages 362-366.Shieber, Stuart M., 1985.
Using Restriction to Extend Pars-ing Algorithms for Complex-feature-based Formalisms.
In23 rd Meeting of the Association for Computational Linguistics,pages 82-93.Shieber, Stuart M., 1986.
An Introduction to Unification-BasedApproaches to Grammar.
Center for the Study of Languageand Information, Stanford, CA.Vijay-Shanker, K., 1987.
A Study of Tree Adjoining Gram-mars.
PhD thesis, Department of Computer and InformationScience, University of Pennsylvania.Vijay-Shanker, K. and Joshi, A.K., 1988.
Feature StructureBaaed Tree Adjoining Grammars.
In Proceedings of the 12 thInternational Conference on Computational Linguistics.583
