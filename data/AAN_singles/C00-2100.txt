Automatic Extraction of Subcategorization Frames for Czech*Anoop SarkarCIS Dept, Univ of Pennsylvania200 South 33rd Street,Philadelphia, PA 19104 USAanoop@linc, cis.
upenn, eduDaniel Zeman0stay formflnf a aplikovand lingvistikyUniverzita KarlovaPraha, Czechiazeman@ufa l  .mf f .
cun i .
czAbstractWe present some novel nmchine learning techniquesfor the identilication of subcategorization infornm-tion for verbs in Czech.
We compare three differentstatistical techniques applied to this problem.
Weshow how the learning algorithm can be used to dis-cover previously unknown subcategorization framesfrom the Czech Prague 1)ependency Treebank.
Thealgorithm can then be used to label dependents ofa verb in the Czech treebank as either argumentsor adjuncts.
Using our techniques, we are able toachieve 88% precision on unseen parsed text.1 IntroductionTl-te subcategorization f verbs is an essential is-sue in parsing, because it helps disambiguate theattachment of arguments and recover the correctpredicate-argument relations by a parser.
(CmTolland Minnen, 1998; CmToll and Rooth, 1998) giveseveral reasons why subcategorization informationis important for a natural anguage parser.
Machine-readable dictionaries are not comprehensive enoughto provide this lexical infornaation (Manning, 1993;Briscoe and Carroll, 1997).
Furthermore, such dic-tionaries are available only for very few languages.We need some general method for the automatic ex-traction of subcategorization information from textcorpora.Several techniques and results have been reportedon learning subcategorization frames (SFs) fromtext corpora (Webster and Marcus, 1989; Brent,1991; Brent, 1993; Brent, 1994; Ushioda et al,1993; Manning, 1993; Ersan and Charniak, 1996;Briscoe and Carroll, 1997; Carroll and Minnen,1998; Carroll and Rooth, 1998).
All of this work" Tiffs work was done during the second author's visit to tl~eUniversity of Pennsylvania.
We would like to thank Prof. Ar-avind Joshi, l)avid Chiang, Mark l)ras and the anonymous re-viewers for their comments.
The first at,thor's work is partiallysupported by NS F Grant S BR 8920230.
Many tools used in thiswork are the resuhs of project No.
VS96151 of the Ministry ofEducation of the Czech Republic.
The data (PDT) is thanksto grant No.
405/96/K214 of the Grant Agency of the CzechRepublic.
Both grants were given to the Institute of Fornmland Applied linguistics, Faculty of Mathenmtics and Physics,Charles University, Prague.deals with English.
In this paper we report ontechniques that automatically extract SFs for Czech,which is a flee word-order language, where verbcomplements have visible case marking.IApart from the choice of target language, thiswork also differs from previous work in other ways.Unlike all other previous work in this area, we donot assume that the set of SFs is known to us in ad-vance.
Also in contrast, we work with syntacticallyannotated ata (the Prague Dependency Treebank,PDT (HajiC 1998)) where the subcategorization in-formation is not  given; although this might be con?sidered a simpler problem as compared to using rawtext, we have discovered interesting problems that auser of a raw or tagged corpus is unlikely to face.We first give a detailed description of the taskof uncovering SFs and also point out those prop-erties of Czech that have to be taken into accountwhen searching lbr SFs.
Then we discuss some dif-.ferences fl'Oln the other research efforts.
We thenpresent he three techniques that we use to learn SFsfrom the input data.In the input data, many observed ependents ofthe verb are adjuncts.
To treat this problem effec-tively, we describe a novel addition to the hypoth-esis testing technique that uses subset of observedfl'ames to permit he learning algorithm to better dis-tinguish arguments fl-om adjtmcts.Using our techniques, we arc able to achieve 88%precision in distinguishing argunaents from adjunctson unseen parsed text.2 lhsk DescriptionIn this section we describe precisely the proposedtask.
We also describe the input training materialand the output produced by our algorithms.2.1 Identifying subeategorization framesIll general, the problem of identifying subcatego-rization fi-ames is to distinguish between argumentsand adjuncts among the constituents modifying aIOI/c of the ammymous rcviewcrs pointed out that (Basiliand Vindigni.
1908) presents a corpus-driven acquisition ofsubcategorization frames for Italian.691f N4 R2(od) {2}N4 R2(od) R2(do) _ R2(od) R2(do) {0~j~/{~}~.
.~ N4 R2(do){0} q~"--_//...~N4 R6(v) R6(na) { 11 ~ N4 R6(v) l I} ~/ / f f _N4 R6(po) {1 } /-----------~"R2(od) {0}R2(do) {0}R6(v) {0}R6(na) {0}R6(po) {0 }N4 {2+1+1}empty 10}Figure 2: Computing the subsets of observed frames for tile verb absoh,ovat.
The counts for each frame aregiven within braces {}.
In this example, the frames N4 R2(od), N4 R6(v) and N4 R6(po) have been observedwith other verbs in the corpus.
Note that the counts in this figure do not correspond to the real counts for theverb absoh,ovat in the training corpus.where c(.)
are counts in the training data.
Usingthe values computed above:Pl --7tlk2P2 --= - -77, 2k l  +k2  p --7z 1 .-\]- 'It 2Taking these probabilities to be binomially dis-tributed, the log likelihood statistic (Dunning, 1993)is given by:- 2 log A =2\[log L(pt, k:l, rtl) @ log L(p2, k2, rl,2) --log L(p, kl, n2) - log L(p, k2, n2)\]where,log L(p, n, k) = k logp + (,z -- k)log(1 - p)According to this statistic, tile greater the value of-2  log A for a particular pair of observed frame andverb, the more likely that frame is to be valid SF ofthe verb.3.2 T-scoresAnother statistic that has been used for hypothesistesting is the t-score.
Using tile definitions fromSection 3.1 we can compute t-scores using the equa-tion below and use its value to measure the associa-tion between a verb and a frame observed with it.T = Pl - P2where,p)  = , p(1 -In particular, the hypothesis being tested usingthe t-score is whether the distributions Pi and P2are not independent.
If the value of T is greaterthan some threshold then the verb v should take theframe f as a SF.3.3 B inomia l  Mode ls  o f  M iscue  Probabi l i t iesOnce again assuming that the data is binomially dis-tributed, we can look for fiames that co-occur with averb by exploiting the miscue probability: the prob-ability of a frame co-occuring with a verb when itis not a valid SF.
This is the method used by severalearlier papers on SF extraction starting with (Brent,1991; Brent, 1993; Brent, 1994).Let us consider probability PU which is the prob-ability that a given verb is observed with a fiame butthis frame is not a valid SF for this verb.
p!f is theerror probability oil identifying a SF for a verb.
Letus consider a verb v which does not have as one ofits valid SFs the frame f .
How likely is it that v willbe seen 'm, or more times in the training data withfi'ame f?
If v has been seen a total of n times ill thedata, then H*(p!f; m, 7z) gives us this likelihood./ xH'(p,f~,n,)'L) = ~__,pif(1 t" )n - i (  ~" )- " f  ii=rn.
X /If H*(p; rn, n) is less than or equal to some smallthreshold value then it is extremely unlikely that thehypothesis is tree, and hence the frame f must bea SF of tile verb v. Setting the threshold value to0.0,5 gives us a 95% or better contidence value thatthe verb v has been observed often enough with aflame f for it to be a valid SEInitially, we consider only the observed fnnnes(OFs) from the treebank.
There is a chance thatsome are subsets of some others but now we countonly tile cases when the OFs were seen themselves.Let's assume the test statistic reiected the flame.Then it is not a real SF but there probably is a sub-set of it that is a real SE So we select exactly one of694tile subsets whose length is one member less: thisis the successor of the rejected flame and inheritsits frequency.
Of course one frame may be suc-cessor of several onger frames and it can have itsown count as OF.
This is how frequencies accumu-late and frames become more likely to survive.
Theexalnple shown in Figure 2 illustrates how the sub-sets and successors are selected.An important point is the selection of the succes-sor.
We have to select only one of the ~t possiblesuccessors of a flame of length 7z, otherwise wewould break tile total frequency of the verb.
Sup-pose there is m rejected flames of length 7z.
"Ellisyields m * n possible modifications to consider be-fore selection of the successor.
We implementedtwo methods for choosing a single successor flame:1.
Choose the one that results in the strongestpreference for some frame (that is, the succes-sor flmne results in the lowest entropy acrossthe corpus).
This measure is sensitive to thefrequency of this flame in the rest of corpus.2?
Random selection of the successor frame fromthe alternatives.Random selection resulted in better precision(88% instead of 86%).
It is not clear wily a methodthat is sensitive to the frequency of each proposedsuccessor frame does not perform better than ran-dom selection.The technique described here may sometimes re-sult in subset of a correct SF, discarding one or moreof its members.
Such frame can still hel ) parsers be-cause they can at least look for the dependents thathave survived.4 EvaluationFor the evalnation of the methods described abovewe used the Prague l)ependency Treebank (PI)T).We used 19,126 sentences of training data from tilePDT (about 300K words).
In this training set, therewere 33,641 verb tokens with 2,993 verb types.There were a total of 28,765 observed fiames (seeSection 2.1 for exphmation of these terms).
Therewere 914 verb types seen 5 or more times.Since there is no electronic valence dictionary forCzech, we evaluated our tiltering technique on a setof 500 test sentences which were unseen and sep-arate flom the training data.
These test sentenceswere used as a gold standard by distinguishing thearguments and adjuncts manually.
We then com-pared the accuracy of our output set of items markedas either arguments or adjuncts against this goldstandard.First we describe the baseline methods.
Base-line method 1: consider each dependent of a verban adjunct.
Baseline method 2: use just the longestknown observed frame matching the test pattern.
Ifno matching OF is known, lind the longest partialmatch in the OFs seen in the training data.
We ex-ploit the functional and morphological tags whilematching.
No statistical filtering is applied in eitherbaseline method.A comparison between all three methods thatwere proposed in this paper is shown in Table 1.The experiments howed that the method im-proved precision of this distinction flom 57% to88%.
We were able to classify as many as 914 verbswhich is a number outperlormed only by Manning,with 10x more data (note that our results arc for adifferent language).Also, our method discovered 137 subcategoriza-tion frames from the data.
The known upper boundof frames that the algorithm could have found (thetotal number of the obsem, edframe types) was 450.5 Comparison with related workPreliminary work on SF extraction from coq~orawas done by (Brent, 1991; Brunt, 1993; Brent,1994) and (Webster and Marcus, 1989; Ushioda etal., 1993).
Brent (Brent, 1993; Brent, 1994) uses thestandard method of testing miscue probabilities forfiltering frames observed with a verb.
(Brent, 1994)presents a method lbr estimating 1)7.
Brent appliedhis method to a small number of verbs and asso-ciated SF types.
(Manning, 1993) applies Brent'smethod to parsed data and obtains a subcategoriza-tion dictionary for a larger set of verbs.
(Briscoeand Carroll, 1997; Carroll and Minnen, 1998) dif-fers from earlier work in that a substantially largerset of SF types are considered; (Canoll and Rooth,1998) use an EM algorithm to learn subcategoriza-tion as a result of learning rule probabilities, and, intnrn, to improve parsing accuracy by applying theverb SFs obtained.
(Basili and Vindigni, 1998) usea conceptual clustering algorithm for acquiring sub-categorization fl'ames for Italian.
They establish apartial order on partially overlapping OFs (similarto our Ot: subsets) which is then used to suggest apotential SF.
A complete comparison of all the pre-vious approaches with tile current work is given inTable 2.While these approaches differ in size and qualityof training data, number of SF types (e.g.
intran-sitive verbs, transitive verbs) and number of verbsprocessed, there are properties that all have in con>mon.
They all assume that they know tile set of pos-sible SF types in advance.
Their task can be viewedas assigning one or more of the (known) SF typesto a given verb.
In addition, except for (Briscoe andCarroll, 1997; Carroll and Minnen, 1998), only asmall number of SF types is considered.695Baseline Lik.
Ratio q-scores Hyp.
TestingPrecision 55% 82% 82% 88%Recall: 55% 77% 77% 74%_h'f~: l 55% 79% 79% 80%% unknown 0% 6% 6% 16%Total verb nodesTotal complementsNodes with known verbsComplements of known verbsCorrect SuggestionsTrue ArgumentsSuggested ArgumentsIncorrect arg suggestionsIncorrect adj suggestions10272144102721441187.5956.500956.51 Baseline 278%73%75%6%1027214498120101573.5910.51122324112.51027214498120101642.5910.5974215.51521027214498120101652.9910.51026236.3120.81027214490718121596.5834.567427.5188Table 1: Comparison between the baseline methods and the three methods proposed in this paper.
Some ofthe values are not integers ince for some difficult cases in the test data, the value for each argument/adjunctdecision was set to a value between \[0, 1\].
Recall is computed as the number of known verb complementsdivided by the total number of complements.
Precision is computed as the number of correct suggestionsdivided by the number of known verb complements.
Ffl=l = (2 x p x r)/(p + r).
% unknown representsthe percent of test data not considered by a particular method.Using a dependency treebank as input to ourlearning algorithm has both advantages and draw-backs.
There are two main advantages of using atreebank:?
Access to more accurate data.
Data is lessnoisy when compared with tagged or parsed in-put data.
We can expect correct identificationof verbs and their dependents.?
We can explore techniques (as we have done inthis paper) that try and learn the set of SFs fromthe data itself, unlike other approaches wherethe set of SFs have to be set in advance.Also, by using a treebank we can use verbs in dif-ferent contexts which are problematic for previousapproaches, e.g.
we can use verbs that appear inrelative clauses.
However, there are two main draw-backs:Treebanks are expensive to build and so thetechniques presented here have to work withless data.All the dependents of each verb are visible tothe learning algorithm.
This is contrasted withprevious techniques that rely on linite-state x=traction rules which ignore many dependentsof the verb.
Thus our technique has to dealwith a different kind of data as compared toprevious approaches.We tackle the second problem by using themethod of observed frame subsets described in Sec-tion 3.3.6 Conclus ionWe arc currently incorporating the SF informationproduced by the methods described in this paperinto a parser for Czech.
We hope to duplicate theincrease in performance shown by treebank-basedparsers for English when they use SF information.Our methods can also be applied to improve theannotations in the original treebank that we use astraining data.
The automatic addition of subcate-gorization to the treebank can be exploited to addpredicate-argument i formation to the treebank.Also, techniques for extracting SF informationfiom data can be used along with other researchwhich aims to discover elationships between dif-ferent SFs of a verb (Stevenson and Merlo, t999;Lapata and Brew, 1999; Lapata, 1999; Stevenson etal., 1999).The statistical models in this paper were based onthe assumption that given a verb, different SFs oc-cur independently.
This assumption is used to jus-tify the use of the binomial.
Future work perhapsshould look towards removing this assumption bymodeling the dependence between different SFs forthe same verb using a multinomial distribution.To summarize: we have presented techniques thatcan be used to learn subcategorization informationfor verbs.
We exploit a dependency treebank tolearn this information, and moreover we discoverthe final set of valid subcategorization frames fromthe training data.
We achieve upto 88% precision onunseen data.We have also tried our methods on data whichwas automatically morphologically tagged which696Previouswork(Ushioda et al, 1993)(Brent, 1993)(Mmming, 1993)(Brent, 1994)(Ersan and Charniak, 1996)(Briscoe and Carroll, 1997)(CatToll and Rooth, 1998)DataPOS +FS ntlesraw +FS rulesPOS +FS rulesraw +heurist icsFullparsingFullparsingUnlabeled#SFsCurrent Work Ful ly  LearnedParsed 1376 336 19319 310412 12616 30160 149+ 3914MethodheuristicsHypothesistestingMiscuerateNAiterativeestimationCorpusWNJ (300K)Brown ( 1.
IM)Hypothesis  hand NYT  (4.1 M)testingHypothesis  non-iter CHIL I )ES  (32K)testing est imationHypothesis  hand WSJ  (36M)testingHypothesis  Dict ionary various (7OK)testing est imationInside- NA BNC (5-30M)outsideSubsets+ Est imate PDT (300K)Hyp.
testingTable 2: Comparison with previous work on automatic SF extraction from corporaal lowed us to use more data (82K sentences insteadof  19K).
The performance went up to 89% (a 1%improvement) .Re ferencesRoberto Basili and Michele Vmdigni.
1998.
Adapting a sub-categorization lexicon to a domain.
In I'roceedings ofthe ECML'98 Workshop TANLPS: Towards adaptive NLP-d,iven systems: lingui'stic information, learning methodsand applications, Chemnitz, Germany, Apr 24.Peter Bickel and Kjell l)oksum.
1977.
Mathematical Statis-tics.
Holden-Day Inc.Michael Brent.
1991.
Automatic acquisition of subcategoriza-tion flames from untagged text.
In Proceedings of the 29thMeeting of the AUL.
pages 209-214, Berkeley, CA.Michael Brent.
1993.
From grammar to lexicon: unsuper-vised learning of lexical syntax.
('Omlmtational Linguistics,19(3):243-262.Michael Brent.
1994.
Acquisition of subcategorization framesusing aggregated evidence fiom local syntactic ues.
Lin-gmt, 92:433-470.
Reprinted in Acqttisition of the Lexicon,L.
Gleinnan and B. Landau (Eds.).
MIT Press, Cambridge,MA.Ted Briscoe and John Carroll.
1997.
Automatic extraction ofsubcategorization from corpora.
In Proceedings of the 5thANI, P Conference, pages 356-363.
Washington.
D.C. ACI,.John Carroll and Guido Minnen.
1998.
Can subcategorisa-tion probabilities help a statistical parser.
In Proceedingsof the 6th AClJSIGDAT Workshop on Very lztrge ('orpora(WVLC-6), Montreal, Canada.Glenn Carroll and Mats Rooth.
1998.
Valence induction witha head-lcxicalized PCFG.
In Proceedings of the 3rd Confer-ence on Empirical Methods in Natural Language Processing(EMNLI' 3), Granada, Spain.Ted Dunning.
1993.
Accurate methods for the statisticsof surprise and coincidence.
Computational Ling,istics.19( 1):61-74, March.Murat Ersan and Eugene Chamiak.
1996.
A statistical syn-tactic disambiguation program and what it learns.
InS.
Wcrmter, E. Riloff, and G. Scheler.
editors, Comwc-tionist, Statistical and Symbolic Approaches in Learning.fi~r Natural xmguage I'rocessing, volume 1040 of LectureNotes in ArtiJical Intelligence, pages 146-159.
Springer-Verlag, Berlin.Jan ttaji,.?
and Barbora ttladkfi.
1998.
"Fagging inllective lan-guages: Prediction of morphological categories for a rich,structured tagset.
In Proceedings of COLING-ACI, 98, Uni-versitd e Montrdal, Montreal, pages 483-490.Jan Itaji,L 1998.
Building a syntactically annotated corpus:The prague dependency treebank.
In Issues off Valency andMeaning, pages 106-132.
Karolinum, Praha.Maria Lapata and Chris Brew.
1999.
Using subcategorizationto resolve verb class ambiguity.
In Pascale Furtg and JoeZhou, editors, Proceedings o1' WVL(TEMNI,I ~, pages 266--274, 21-22 June.Maria Lapata.
1999.
Acquiring lexical generalizations fromcorpora: A case study for diathesis alternations.
In Proceed-ings q/37th Meeting olA( :L, pages 397-404.Christopher I).
Manning.
1993.
Automatic acquisition of alarge subcategorization dictionary from corpora.
In Pro-ceedil~gs of the 31st Meeting q/' the ACI,, pagcs 235-242,Columbus, Ohio.Suzanne Stevenson and Paola Merlo.
1999.
Automatic verbclassilication using distributions of grammatical features.
InProceedings of I'JACL '99, pages 45-52, Bergen, Norway,8-12 J une.Suzanne Stevenson, Paoht Merlo, Natalia Kariaeva, and KaminWhitehouse.
1999.
Supervised learning of lexical semanticclasses using frequency distributions.
In SIGLEX-99.Akira Ushioda, David A. Evans, Ted Gibson, and Alex Waibel.1993.
The autonaatic acquisition of frequencies ofverb st, b-categorization frames from tagged corpora.
In B. Boguraevand J. Pustejovsky, editors, Proceedings of the Workshop onAcquisition of Lexical Knowledge fi'om 7kvt, pages 95-106,Columbus, Otl, 21 June.Mort Webster and Mitchell Marcus.
1989.
Automatic acquisi-tion of the lexical frames of verbs from sentence frames.
InProceedings of the 27th Meeting of the ACL, pages 177-184.697
