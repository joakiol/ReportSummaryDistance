Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 268?276,Beijing, August 2010Resolving Object and Attribute Coreference in Opinion MiningXiaowen DingDepartment of Computer ScienceUniversity of Illinois at Chicagodingxwsimon@gmail.comBing LiuDepartment of Computer ScienceUniversity of Illinois at Chicagoliub@cs.uic.eduAbstractCoreference resolution is a classic NLPproblem and has been studied extensively bymany researchers.
Most existing studies,however, are generic in the sense that theyare not focused on any specific text.
In thepast few years, opinion mining became apopular topic of research because of a widerange of applications.
However, limitedwork has been done on coreference resolu-tion in opinionated text.
In this paper, wedeal with object and attribute coreferenceresolution.
Such coreference resolutions areimportant because without solving it a greatdeal of opinion information will be lost, andopinions may be assigned to wrong entities.We show that some important features re-lated to opinions can be exploited to performthe task more accurately.
Experimental re-sults using blog posts demonstrate the effec-tiveness of the technique.1 IntroductionOpinion mining has been actively researched inrecent years.
Researchers have studied the prob-lem at the document level (e.g., Pang et al,2002; Tuney, 2002; Gamon et al, 2005) sen-tence and clause level (Wilson et al, 2004; Kimand Hovy, 2004), word level (e.g., Andreevs-kaia and Bergler, 2006; Hatzivassiloglou andMcKeown, 1997; Esuli and Sebastiani, 2006;Kanayama and Nasukawa, 2006; Qiu et al,2009), and attribute level (Hu and Liu 2004;Popescu and Etzioni, 2005; Ku et al, 2006; Meiet al, 2007; Titov and McDonald 2008).
Hereattributes mean different aspects of an objectthat has been commented on.
Let us use the fol-lowing example blog to illustrate the problem:?I bought a Canon S500 camera yesterday.
Itlooked beautiful.
I took a few photos last night.They were amazing?.
?It?
in the second sen-tence refers to ?Canon S500 camera?, which iscalled an object.
?They?
in the fourth sentencerefers to ?photos?, which is called an attributeof the object ?Canon S500 camera?.
The use-fulness of coreference resolution in this case isclear.
Without resolving them, we lose opinions.That is, although we know that the second andfourth sentences express opinions, we do notknow on what.
Without knowing the opiniontarget, the opinion is of limited use.
In (Nicolovet al, 2008), it was shown based on manuallyannotated data that opinion mining results canbe improved by 10% if coreference resolution isused (the paper did not provide an algorithm).In this paper, we propose the problem of ob-ject and attribute coreference resolution ?
thetask of determining which mentions of objectsand attributes refer to the same entities.
Notethat here entities refer to both objects andattributes, not the traditional named entities.
Toour knowledge, limited work has been done onthis problem in the opinion mining context apartfrom a prior study on resolving opinion sources(or holders) (Stoyanov and Cardie 2006).
Opi-nion sources or holders are the persons or or-ganizations that hold some opinions on objectsand attributes.
In this paper, we do not deal withsource resolution as we are mainly interested inopinion texts on the web, e.g., reviews, discus-sions and blogs.
In such environments opinionsources are usually the authors of the posts,which are displayed in Web pages.This work follows the attribute-based opi-nion mining model in (Hu and Liu 2004; Popes-cu and Etzioni, 2005).
In their work, attributesare called features.
We do not use the term ?fea-ture?
in this paper to avoid confusion with theterm ?feature?
used in machine learning.Our primary interests in this paper are opi-268nions expressed on products and services, whichare called objects.
Each object is described byits parts/components and attributes, which areall called attributes for simplicity.This paper takes the supervised learning ap-proach to solving the problem.
The key contri-bution of this paper is the design and testing oftwo novel opinion related features for learning.The first feature is based on sentiment analysisof normal sentences (non-comparative sen-tences), comparative sentences, and the idea ofsentiment consistency.
For example, we havethe sentences, ?The Sony camera is better thanthe Canon camera.
It is cheap too.?
It is clearthat ?It?
means ?Sony?
because in the first sen-tence, the opinion on ?Sony?
is positive (com-parative positive), but negative (comparativenegative) on ?Canon?, and the second sentenceis positive.
Thus, we can conclude that ?It?
re-fers to ?Sony?
because people usually expresssentiments in a consistent way.
It is unlikelythat ?It?
refers to ?Canon?.
This is the idea ofsentiment consistency.
As we can see, this fea-ture requires the system to have the ability todetermine positive and negative opinions ex-pressed in normal and comparative sentences.The second feature considers what objectsand attributes are modified by what opinionwords.
Opinion words are words that are com-monly used to express positive or negative opi-nions, e.g., good, best, bad, and poor.
Considerthe sentences, ?The picture quality of the Canoncamera is very good.
It is not expensive either.
?The question is what ?It?
refers to, ?Canoncamera?
or ?picture quality?.
Clearly, we knowthat ?It?
refers to ?Canon camera?
because ?pic-ture quality?
cannot be expensive.
To make thisfeature work, we need to identify what opinionwords are usually associated with what objectsor attributes, which means that the system needsto discover such relationships from the corpus.These two features give significant boost tothe coreference resolution accuracy.
Experimen-tal results based on three corpora demonstratethe effectiveness of the proposed features.2 Related WorkCoreference resolution is an extensively studiedNLP problem (e.g., Morton, 2000; Ng and Car-die, 2002; Gasperin and Briscoe, 2008).
Earlyknowledge-based approaches were domain andlinguistic dependent (Carbonell and Brown1988), where researchers focused on diverselexical and grammatical properties of referringexpressions (Soon et al, 2001; Ng and Cardie,2002; Zhou et al, 2004).
Recent research reliedmore on exploiting semantic information.
Forexample, Yang et al (2005) used the semanticcompatibility information, and Yang and Su(2007) used automatically discovered patternsintegrated with semantic relatedness informa-tion, while Ng (2007) employed semantic classknowledge acquired from the Penn Treebank.Versley et al (2008) used several kernel func-tions in learning.Perhaps, the most popular approach is basedon supervised learning.
In this approach, thesystem learns a pairwise function to predictwhether a pair of noun phrases is coreferent.Subsequently, when making coreference resolu-tion decisions on unseen documents, the learntpairwise noun phrase coreference classifier isrun, followed by a clustering step to produce thefinal clusters (coreference chains) of coreferentnoun phrases.
For both training and testing, co-reference resolution algorithms rely on featurevectors for pairs of noun phrases that encodelexical, grammatical, and semantic informationabout the noun phrases and their local context.Soon et al (2001), for example, built a nounphrase coreference system based on decisiontrees and it was tested on two standard corefe-rence resolution data sets (MUC-6, 1995; MUC-7, 1998), achieving performance comparable tothe best-performing knowledge based corefe-rence engines at that time.
The learning algo-rithm used 12 surface-level features.
Our pro-posed method builds on this system with addi-tional sentiment related features.
The featuresinherit from this paper includes:Distance Feature: Its possible values are 0,1, 2, 3 and so on which captures the sentencedistance between two entities.Antecedent-pronoun feature, anaphor-pronoun feature: If the candidate antecedent oranaphor is a pronoun, it is true; false otherwise.Definite noun phrase feature: The value istrue if the noun phrase starts with ?the?
; falseotherwise.Demonstrative noun phrase feature: Thevalue is true if the noun phrase starts with theword ?this?, ?that?, ?these?, or ?those?
; falseotherwise.269Number agreement feature: If the candidateantecedent and anaphor are both singular orboth plural, the value is true; otherwise false.Both-proper-name feature: If both the can-didates are proper nouns, which are determinedby capitalization, return true; otherwise false.Alias feature: It is true if one candidate is analias of the other or vice versa; false otherwise.Ng and Cardie (2002) expanded the featureset of Soon et al (2001) from 12 to 53 features.The system was further improved by Stoyanovand Cardie (2006) who gave a partially super-vised clustering algorithm and tackled the prob-lem of opinion source coreference resolution.Centering theory is a linguistic approach triedto model the variation or shift of the main sub-ject of the discourse in focus.
In (Grosz et al,1995; Tetreault, 2001), centering theory wasapplied to sort the antecedent candidates basedon the ranking of the forward-looking centers,which consist of those discourse entities thatcan be interpreted by linguistic expressions inthe sentences.
Fang et al (2009) employed thecentering theory to replace the grammatical rolefeatures with semantic role information andshowed superior accuracy performances.Ding et al (2009) studied the entity assign-ment problem.
They tried to discover the prod-uct names discussed in forum posts and assignthe product entities to each sentence.
The workdid not deal with product attributes.Unsupervised approaches were also applieddue to the cost of annotating large corpora.
Ng(2008) used an Expectation-Maximization (EM)algorithm, and Poon and Domingos (2008) ap-plied Markov Logic Network (MLN).Another related work is the indirect anapho-ra, known as bridging reference.
It arises whenan entity is part of an earlier mention.
Resolvingindirect anaphora requires background know-ledge (e.g.
Fan et al, 2005), and it is thus not inthe scope of this paper.Our work differs from these existing studiesas we work in the context of opinion mining,which gives us extra features to enable us toperform the task more effectively.3 Problem of Object and Attribute Co-reference ResolutionIn general, opinions can be expressed on any-thing, e.g., a product, an individual, an organi-zation, an event, a topic, etc.
Following (Liu,2006), we also use the term object to denote annamed entity that has been commented on.
Theobject has a set of components (or parts) andalso a set of attributes.
For simplicity, attributeis used to denote both component and attributein this paper.
Thus, we have the two concepts,object and attribute.3.1 ObjectiveTask objective: To carry out coreference reso-lution on objects and attributes in opinion text.As we discussed in the introduction section,coreference resolution on objects and attributesis important because they are the core entitieson which people express opinions.
Due to ourobjective, we do not evaluate other types of co-references.
We assume that objects and entitieshave been discovered by an existing system(e.g., Hu and Liu 2004, Popescu and Etzioni2005).
Recall that a coreference relation holdsbetween two noun phrases if they refer to thesame entity.
For example, we have the follow-ing three consecutive sentences:s1: I love the nokia n95 but not sure how goodthe flash would be?s2: and also it is quite expensive so anyone gotany ideas?s3: I will be going on contract so as long as i canget a good deal of it.?it?
in s2 refers to the entity ?the nokia n95?in s1.
In this case, we call ?the nokia n95?
theantecedent and pronoun ?it?
in s2 the anaphor.The referent of ?it?
in s3 is also ?the nokia n95?,so the ?it?
in s3 is coreferent with the ?it?
in s2.Our task is thus to decide which mentions ofobjects and attributes refer to the same entities.3.2 Overview of Our ApproachLike traditional conference resolution, we em-ploy the supervised learning approach by in-cluding additional new features.
The main stepsof our approach are as follows:Preprocessing: We first preprocess the cor-pus by running a POS tagger 1 , and a NounPhrase finder2.
We then produce the set O-NPwhich includes both possible objects, attributesand other noun phrases.
The noun phrases are1 http://nlp.stanford.edu/software/tagger.shtml2 http://crfchunker.sourceforge.net/270found using the Noun Phrase finder and the ob-ject names are consecutive NNPs.
O-NP thuscontains everything that needs to be resolved.Feature vector construction: To performmachine learning, we need a set of features.Similar to previous supervised learning ap-proaches (Soon et al, 2001), a feature vector isformed for every pair of phrases in O-NP ex-tracted in the preprocessing step.
We use someof the features introduced by Soon et al (2001)together with some novel new features that wepropose in this work.
Since our focus is onproducts and attributes in opinionated docu-ments, we do not use personal pronouns, thegender agreement feature, and the appositivefeature, as they are not essential in blogs andforum posts discussing products.Classifier construction: Using the featurevectors obtained from the previous step, weconstruct the training data, which includes allpairs of manually tagged phrases that are eitherobject names or attributes.
More precisely, eachpair contains at least one object or one attribute.Using the training data, a decision tree is con-structed using WEKA3.Testing: The testing phase employs the samepreprocessing and feature vector constructionsteps as described above, followed by the appli-cation of the learnt classifier on all candidatecoreference pairs (which are represented as fea-ture vectors).
Since we are only interested incoreference information for objects and attributenoun phrases, we discard non-object and non-attribute noun phrases.4 The Proposed New FeaturesOn surface, object and attribute coreference res-olution seems to be the same as the traditionalnoun phrase coreference resolution.
We can ap-ply an existing coreference resolution technique.However, as we mentioned earlier, in the opi-nion mining context, we can have a better solu-tion by integrating opinion information into thetraditional lexical and grammatical features.Below are several novel features that we haveproposed.
We use ?i to denote an antecedentcandidate and ?j an anaphor candidate.
Note thatwe will not repeat the features used in previoussystems, but only focus on the new features.3 http://www.cs.waikato.ac.nz/ml/weka/4.1 Sentiment ConsistencyIntuitively, in a post, if the author starts express-ing opinions on an object, he/she will continueto have the same opinion on that object or itsattributes unless there are contrary words suchas ?but?
and ?however?.
For example, we havethe following blog (an id is added before eachsentence to facilitate later discussion):?
(1) I bought Camera-A yesterday.
(2) Itook a few pictures in the evening in my livingroom.
(3) The images were very clear.
(4)They were definitely better than those frommy old Camera-B.
(5a) It is cheap too.
(5b)The pictures of that camera were blurring fornight shots, but for day shots it was ok?The comparative sentence (4) says that Cam-era-A is superior to Camera-B.
If the next sen-tence is (5a) ((5a) and (5b) are alternative sen-tences), ?it?
should refer to the superior prod-uct/object (Camera-A) because sentence (5a)expresses a positive opinion.
Similarly, if thenext sentence is sentence (5b) which expresses anegative opinion in its first clause, ?that cam-era?
should refer to the inferior product (Cam-era-B).
We call this phenomenon sentiment con-sistency (SC), which says that consecutive sen-timent expressions should be consistent witheach other unless there are contrary words suchas ?but?
and ?however?.
It would be ambiguousif such consistency is not observed.Following the above observation, we furtherobserve that if the author wants to introduce anew object o, he/she has to state the name of theobject explicitly in a sentence si-1.
The questionis what happens to the next sentence si if weneed to resolve the pronouns in si.We consider several cases:1. si-1 is a normal sentence (not a comparativesentence).
If si expresses a consistent senti-ment with si-1, it should refer to the same ob-ject as si-1.
For example, we havesi-1: The N73 is my favorite.si: It can produce great pictures.Here ?It?
in si clearly refers to ?The N73?
inthe first sentence si-1.2.
si-1 is a normal sentence and si does not ex-press a consistent sentiment, then ?i and ?jintroduced in these two sentences may not becoreferenced.
For example, we havesi-1:  The K800 is awesome.si: That phone has short battery life.271Here ?The K800?
and ?That phone?
may notbe a coreference pair according to sentimentconsistency.
?That phone?
should refer to anobject appeared in an earlier sentence.3.
si-1 is a comparative sentence.
If si expressesa positive (respectively negative) sentiment,the pronoun in si should refer to the superior(or inferior) entity in si-1 to satisfy sentimentconsistency.
This situation is depicted in theearlier example blog.
For completeness, wegive another example.si-1: The XBR4 is brighter than the 5080.si: Overall, it is a great choice.Here ?it?
in si should refer to ?The XBR4?
insi-1 since they both have positive sentimentsexpressed on them.Opinion Mining of Comparative Sentences:To deal with case (3), we need to identify supe-rior entities from comparative sentences.
In fact,we first need to find such comparative sen-tences.
There is a prior work on identifyingcomparative sentences (Jindal and Liu.
2006).Since our focus is not to identify such sen-tences, we used several heuristic rules based onsome comparative keywords, e.g.
than, win,superior, etc.
They achieve the F-score of 0.9.We then followed the opinion mining methodintroduced in (Ding et al 2009) to find superiorentities.
Since a comparative sentence typicallyhas entities on the two sides of a comparativekeyword, i.e., ?Camera-X is better than Cam-era-Y?, based on opinion mining, if the sentenceis positive, then the entities before the compara-tive keyword is superior and otherwise they areinferior (with the negation considered).SC Feature: The possible value for this fea-ture is 0, 1, or 2.
If ?i and ?j have the same opi-nion, return 1; different opinions, return 0; andif the opinions cannot be identified for one orboth of them, return 2.
Here is an example ex-plaining how the feature is used in our system:?My wife has currently got a Nokia 7390,which is terrible.
My 6233 would always getgreat reception, hers would get no signal.
?Using our algorithm for opinion mining, ?hers?gets a negative opinion in the second sentence.So the value for this feature for the pair, ?hers?and ?a Nokia 7390?, is 1.
The feature value forthe pair ?hers?
and ?My 6233?
is 0.
The idea isthat because the first sentence expresses a nega-tive sentiment on ?a Nokia 7390?, and there isno discourse connective (such as ?but?
and?however?)
between these two sentences.?Hers?
should be talking about ?a Nokia 7390?so as to satisfy sentiment consistency.4.2 Entity and Opinion Word AssociationOne of the most important factors determiningthe orientation of opinions is the opinion wordsthat opinion holders use to express their opi-nions.
Different entities may be modified bydifferent opinion words.
We can use their asso-ciation information with entities (both objectsand attributes) to identify their coreferences.Opinion Words: In most cases, opinions insentences are expressed using opinion words.For example, the sentence, ?The picture qualityis amazing?, expresses a positive opinion on the?picture quality?
attribute because of the posi-tive opinion word ?amazing?.Researchers have compiled sets of suchwords for adjectives, adverbs, verbs, and nounsrespectively.
Such lists are collectively calledthe opinion lexicon.
We obtained an opinionlexicon from the authors of (Ding et al 2009).It is useful to note that opinion words used toexpress opinions on different entities are usuallydifferent apart from some general opinion wordssuch as good, great, bad, etc, which can expressopinions on almost anything.
For example, wehave the following passage:?i love the nokia n95 but not sure howstrong the flash would be?
And also it is quiteexpensive, so anyone got any ideas?
?Here ?strong?
is an opinion word that expressesa positive opinion on ?the flash?, but is seldomused to describe ?the nokia n95?.
?expensive?,on the other hand, should not be associated with?the flash?, but is an opinion word that indicatesa negative opinion on ?the nokia n95?.
So ?thenokia n95?
is more likely to be the antecedentof ?it?
in the second sentence.The question is how to find such associationsof entities and opinion words.
We use their co-occurrence information to measure, i.e., thepointwise mutual information of the two terms.First, we estimate the probability of P(NP),P(OW) and P(NP&OW).
Here NP means a nounphrase, e.g., an object (attribute) after removingdeterminers, and OW means an opinion word.To compute the probability, we first count theoccurrences of the words.
Then the probabilityis computed as follow:272?????
???
?
??????????????????????????????
?where NumofS is a function that gives the num-ber of sentences that contain the particular wordstring.
P(NP, OW) is computed in the sameway.
Let us use the previous example again.
Wecompute P(?nokia n95?,?expensive?)
as thenumber of sentences containing both ?nokian95?
and ?expensive?
divided by the total num-ber of sentences in the whole corpus.Then we use the pointwise mutual informa-tion between a noun phrase and an opinion wordto measure the association.???????
???
?
????????
????????????
?However, this PMI value cannot be encodeddirectly as a feature as it only captures the localinformation between antecedent candidates andopinion words.
That is, it cannot be used as aglobal feature in the classifier.
We thus rank allpossible antecedents of anaphor ?j based ontheir PMI values and use the ranking as the fea-ture value.
The highest ranked antecedent ?i hasvalue 1; the second one has value 2 and so on.The candidates ranked below the fourth placeall have the value 5.
In the example above, ifPMI(?nokia n95?, ?expensive?)
is greater thanPMI(?flash?, ?expensive?
), the feature for ?no-kia n95?
and ?it?
pair will have a smaller valuethan the feature for the ?flash?
and ?it?
pair.One may ask if we can use all adjectives andadverbs to associate with objects and attributesrather than just opinion words since most opi-nion words are adjectives and adverbs.
Wetested that, but the results were poor.
We be-lieve the reason is that there are many adjectivesand adverbs which are used for all kinds of pur-poses and may not be meaningful for our task.4.3 String Similarity FeatureSoon et al (2001) has a string match feature(SOON STR), which tests whether the two nounphrases are the same string after removing de-terminers from each.
Ng and Cardie (2002) splitthis feature into several primitive features, de-pending on the type of noun phrases.
They re-place the SOON STR feature with three features?
PRO STR, PN STR, and WORDS STR ?which restrict the application of string matchingto pronouns, proper names, and non-pronominalnoun phrases, respectively.In the user generated opinion data, these maynot be sufficient.
For a certain product, peoplecan have a large number of ways to express it.For example, we have?Panasonic TH50PZ700U VS TH50PZ77U,Which Plasma tv should I go for.
The TH77Uis about $500.00 more than the 700U.
?Here ?TH77U?
is the same entity as ?PanasonicTH50PZ77U?, and ?TH50PZ700U?
is the sameas ?700U?.
But they cannot be easily identifiedby ?same string?
features mentioned above.
Al-though ?700U?
can be solved using substringfeatures, ?TH77U?
is difficult to deal with.We employ a modified edit distance to com-puting a similarity score between different men-tions and use that as a feature in our system.When one candidate is a substring of another,return 1; otherwise, 1 plus the edit distance.4.4 Other Useful FeaturesIn the machine learning approach introduced bySoon et al (2001), they had several general fea-tures that can deal with various kinds of entities,e.g., semantic class agreement features dealingwith different semantic classes like date, loca-tion, etc., and the gender agreement feature re-lated to personal entities.
However, these fea-tures are not so useful for our task because thesemantic class of a product in one domain isusually consistent, and dates and locations areunlikely to be of any products that people willexpress their opinions.
Moreover, we do notstudy opinion holders (as they are known in theWeb environment), so personal entities are notthe aspect that we concentrate on.
Thus we didnot use the following features: semantic classagreement features, the gender agreement fea-ture, and appositive feature.However, we added some specific features,which are based on two extracted entities, ?i and?j, where ?i is the potential antecedent and ?j isthe potential anaphor:Is-between feature: Its possible values aretrue and false.
If the words between ?i and ?jhave an is-like verb (i.e., is, are, was, were, andbe) between them and there is no comparativeindicators, this feature has the value of true,e.g., ?The nokia e65 is a good handset.
?In sentences similar to this example, the enti-ties before and after ?is?
usually refer to thesame object or attribute by a definition relation.273And the value of this feature will be true.If ?is?
appears together with a comparativeword, it is probably an indication that the twoentities are different, and the value for this fea-ture will be false, e.g., ?Overall the K800 is farsuperior to the W810.
?Has-between feature: Its possible values arealso true and false.
If the words between ?i and?j have a has-like verb (i.e., has, have, and had),the value is true, and otherwise false, e.g., ?Thek800 has a 3.2 megapixel camera.
?This feature usually indicates a ?part-of?
rela-tion if ?has?
appears between two entities.
Theydo not refer to the same entity.
Table 1 gives asummary of all the features used in our system.5 Experiments and Discussions5.1 DatasetsFor evaluation, we used forum discussions fromthree domains, mobile phones, plasma and LCDTVs, and cars.
Table 2 shows the characteristicsof the three data sets.
Altogether, we down-loaded 64 discussion threads, which contain 453individual posts with a total of 3939 sentences.All the sentences and product names were anno-tated strictly following the MUC-7 coreferencetask annotation standard4.
Here is an example:?Phil had <COREF ID = "6" TYPE ="OBJ">a z610</COREF> which has <COREFID = "7" TYPE = "ATTR">a 2MP cema-ra</COREF>, and he never had a problemwith <COREF ID = "8" TYPE = "OBJ" REF ="6">it</COREF>.
?ID and REF features are used to indicate thatthere is a coreference link between two strings.ID is arbitrary but uniquely assigned to eachnoun phrase.
REF uses the ID to indicate a core-ference link.
?TYPE?
can be ?OBJ?
(an objector a product), or ?ATTR?
(an attribute of anobject).
The annotation was done by the firstauthor and another student before the algorithmconstruction, and the annotated data sets will bemade public for other researchers to use.For our experiments, we used the J48-decision tree builder in WEKA, a popularof machine learning suite developed at the  Uni-versity of Waikato.
We conducted 10-fold crossvalidation on each dataset.4 http://www-nlpir.nist.gov/related_projects/muc/procee-dings/co_task.htmlThe performances are measured using thestandard evaluation measures of precision (p),recall (r) and F-score (F), F = 2pr/(p+r).
As westated in Section 3, we are only interested inobject and attributes noun phrases.
So in thetesting phrases, we only compute the precisionand recall based on those pairs of candidatesthat contain at least one object or attribute nounphrase in each pair.
If both of the candidates arenot an object or an attribute, we ignore them.5.2 BaselineAs the baseline systems, we duplicated two rep-resentative systems.
Baseline1 is the decisiontree system in Soon et al (2001).
We do not usethe semantic class agreement feature, genderagreement feature and appositive feature in theoriginal 12 features for the reason discussed inSection 4.4.
Thus, the total number of featuresin Baseline1 is 9.
The second baseline (base-line2) is based on the centering theory from thesemantic perspective introduced by Fang et al(2009).
Centering theory is a theory about thelocal discourse structure that models the interac-tion of referential continuity and the salience ofdiscourse entities in the internal organization ofa text.
Fang et al (2009) extended the centeringtheory from the grammar level to the semanticlevel in tracking the local discourse focus.5.3 Results AnalysisTable 3 gives the experimental results of thetwo baseline systems and our system with dif-ferent features included.
From Table 3, we canmake several observations.
(1) Comparing the results of Baseline1 and oursystem with all features (Our System (All)),the new features introduced in this paperimproves Baseline1 on average by morethan 9% in F-score.
(2) Comparing the results of Baseline2 and oursystem with all features (Our System (All)),our system performs better than Baseline2by about 3 - 5%.
We also observe that cen-tering theory (Baseline2) is indeed betterthan the traditional decision tree.
(3) Our system with sentiment consistency (SC)makes a major difference.
It improves Base-line1 (our method is based on Baseline1) by5-6% in F-score.
(4) With the additional feature of entity andopinion association (EOA), the results are274improved further by another 2-4%.
(5)  Our system with all features (row 5) per-forms the best.Paired t-tests were performed on the threesystems, i.e., baseline1, baseline2, and our sys-tem (row 5).
The tests show that the improve-ments of our method over both Baseline1 andBaseline2 are significant at the confidence levelof 95% for the first two datasets.
For the thirddataset, the improvement over Baseline1 is alsosignificant at the confidence level of 95%, whilethe improvement over Baseline2 is significant atthe confidence level of 90%.In summary, we can conclude that the newtechnique is effective and is markedly betterthan the existing methods.
It is clear that thenew features made a major difference.6 ConclusionThis paper investigated the coreference resolu-tion problem in the opinion mining context.
Inparticular, it studied object and attribute resolu-tions which are crucial for improving opinionmining results.
Although we still took the su-pervised learning approach, we proposed sev-eral novel features in the opinion mining con-text, e.g., sentiment consistency, and ob-ject/attribute and opinion word associations.Experimental results using forum posts demon-strated the effectiveness of the proposed tech-nique.
In our future work, we plan to furtherimprove the method and discover some otheropinion related features that can be exploited toproduce more accurate results.Feature category Feature RemarkOpinion miningbased featuresOpinion consistency 1, if the opinion orientation of ?i is the same as ?j, 0 ifthe opinions are different, else 2Entity and opinion wordsassociation1, 2, 3, 4, 5 which indicate the rank positive based on thePMI value introduced in Section 4.2grammatical i-Pronoun feature 1, if ?i is a pronoun, else 0j-Pronoun feature 1, if ?j is a pronoun, else 0Number agreement feature 1, if both of the noun phrases agree in numbers, else 0Definite feature 1, if ?j starts with the word ?the?, else 0Demonstrative feature 1, if ?j starts with the word ?this?, ?that?, ?those?, or?these?, else 0Both proper-name feature 1, if ?i and ?j are both proper names, else 0lexical String similarity The string similarity score between ?i and ?jAlias feature  1, If ?i is an alias of ?j or vice versa, else 0Others Distance feature The sentence distance between the pair of noun phrases,0 if they are in the same sentenceKeywords between features 1, if some keywords exist between ?i and ?j, else 0.
De-tails are discussed in Section 4.5Table 1: Feature list: ?i denotes the antecedent candidate and ?j the anaphor candidatePosts SentencesPhone 168 1498TVs 173 1376Cars 112 1065Total 453 3939Table 2: Characteristics of the datasetsCellphone TVs Carsp r F p r F p r F1 Baseline1 0.66 0.57 0.61 0.67 0.61 0.64 0.70 0.63 0.662 Baseline2 0.70 0.64 0.67 0.72 0.65 0.68 0.76 0.70 0.733 Our System (SC) 0.71 0.64 0.67 0.73 0.66 0.69 0.74 0.69 0.724 Our System (SC+EOA) 0.74 0.68 0.71 0.74 0.68 0.71 0.77 0.71 0.745 Our System (All) 0.75 0.70 0.72 0.76 0.70 0.73 0.78 0.73 0.75Table 3: Results of object and attribute coreference resolution275ReferencesA.
Andreevskaia and S. Bergler.
2006.
MiningWordNet for Fuzzy Sentiment: Sentiment Tag Ex-traction from WordNet Glosses.
EACL?06.J.
Carbonell and R. Brown.
1988.
Anaphora resolu-tion: a multi-strategy approach.
COLING?1988.G.
Carenini, R. Ng, and A. Pauls.
2006.
Interactivemultimedia summaries of evaluative text.
IUI?06.X.
Ding, B. Liu and L. Zhang.
2009.
Entity Discov-ery and Assignment for Opinion Mining Applica-tion.
KDD?09.A.
Esuli and F. Sebastiani.
2006.
Determining TermSubjectivity and Term Orientation for OpinionMining, EACL?06.J.
Fan, K. Barker and B. Porter.
2005.
Indirect ana-phora resolution as semantic path search.
K-CAP?05.C.
Gasperin and T. Briscoe.
2008.
Statistical ana-phora resolution in biomedical texts.
COLING'08B.
J. Grosz, A. K. Joshi and S. Weinstein.
1995.Centering: a framework for modeling the localcoherence of discourse.
Computational Linguis-tics, 21(2).V.
Hatzivassiloglou and K. McKeown.
1997.
Pre-dicting the Semantic Orientation of Adjectives.ACL-EACL?97.M.
Hu and B. Liu.
2004.
Mining and summarizingcustomer reviews.
KDD?04.N.
Jindal, and B. Liu.
2006.
Mining ComparativeSentences and Relations.
AAAI?06.H.
Kanayama and T. Nasukawa.
2006.
Fully Auto-matic Lexicon Expansion for Domain-OrientedSentiment Analysis.
EMNLP?06.S.
Kim and E. Hovy.
2004.
Determining the Senti-ment of Opinions.
COLING?04.N.
Kobayashi, K. Inui and Y. Matsumoto.
2007.
Ex-tracting Aspect-Evaluation and Aspect-of Rela-tions in Opinion Mining.
EMNLP-CoNLL?07.F.
Kong, G. Zhou, Q. Zhu and P. Qian.
2009.
Em-ploying the Centering Theory in Pronoun Resolu-tion from the Semantic Perspective.
EMNLP?09.L.-W. Ku, Y.-T. Liang and H.-H. Chen.
2006.
Opi-nion Extraction, Summarization and Tracking inNews and Blog Corpora.
CAAW'06.B.
Liu.
2006.
Web Data Mining, Springer.R.
McDonald, K. Hannan, T Neylon, M. Wells, andJ.Reynar.
2007.
Structured Models for Fine-to-Coarse Sentiment Analysis.
ACL-07.Q.
Mei, X. Ling, M. Wondra, H. Su, and C. Zhai.2007.
Topic Sentiment Mixture: Modeling Facetsand Opinions in Weblogs.
WWW?07.T.
S. Morton.
2000.
Coreference for NLP applica-tions.
ACL?00.V.
Ng and C. Cardie.
2002.
Improving machinelearning approaches to coreference resolution.ACL?02.V.
Ng.
2007.
Semantic Class Induction and Corefe-rence Resolution.
ACL?07.V.
Ng.
2008.
Unsupervised Models for CoreferenceResolution.
EMNLP?08.N.
Nicolov, F. Salvetti and S. Ivanova, Sentimentanalysis: Does coreference matter?
AISB'2008.B.
Pang, L. Lee, and S. Vaithyanathan.
2002.Thumbs up?
Sentiment Classification Using Ma-chine Learning Techniques.
EMNLP?02.H.
Poon and P. Domingos.
2008.
Joint UnsupervisedCoreference Resolution with Markov Logic.EMNLP?08, 650?659.A-M. Popescu and O. Etzioni.
2005.
Extractingproduct features and opinions from reviews.EMNLP?05.Qiu, Guang, B. Liu, J. Bu and C. Chen.
2009 Ex-panding Domain Sentiment Lexicon throughDouble Propagation.
IJCAI 2009.W.
M. Soon, H. T. Ng and D. Lim.
2001.
A machinelearning approach to coreference resolution ofnoun phrase.
Computational Linguistics, 27(4).V.
Stoyanov, C. Cardie.
2006.
Partially supervisedcoreference resolution for opinion summarizationthrough structured rule learning.
EMNLP?06.I.
Titov and R. McDonald.
2008, A joint model oftext and aspect ratings for sentiment summariza-tion, ACL?08.J.
Tetreault.
2001.
A corpus-based evaluation of cen-tering and pronoun resolution.
ComputationalLinguistics.
27(4):507-520.P.
Turney.
2002.
Thumbs Up or Thumbs Down?
Se-mantic Orientation Applied to Unsupervised Clas-sification of Reviews.
ACL?02.Y.
Versley, A. Moschitti, M. Poesio and X. Yang.2008.
Coreference systems based on kernels me-thods.
COLING?08.T.
Wilson, J. Wiebe, and R. Hwa.
2004.
Just howmad are you?
Finding strong and weak opinionclauses.
AAAI?04.X.
F. Yang?
J. Su and C. L. Tan.
2005.
ImprovingPronoun Resolution Using Statistics - Based Se-mantic Compatibility Information.
ACL?05.X.
F. Yang and J. Su.
2007.
Coreference ResolutionUsing Semantic Relatedness Information from Au-tomatically Discovered Patterns.
ACL?07.G.
D. Zhou and J. Su.
2004.
A high-performancecoreference resolution system using a multi-agentstrategy.
COLING?04.276
