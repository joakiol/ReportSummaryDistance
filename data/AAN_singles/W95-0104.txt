A Bayesian hybrid method for context-sensitive spelling correctionAndrew It.
GoldingMitsubishi Electric Research Labs201 Broadway, 8th FloorCambridge, MA 02139golding?merl, tomAbstractTwo classes of methods have been shown to be useful for resolving lexical ambiguity.
Thefirst relies on tile presence of particular words within some distance of tile ambiguous targetword; the second uses the pattern of words and part-of-speech tags around the target word.These methods have complementary coverage: the former captures the lexical "atmosphere"(discourse topic, tense, etc.
), while tile latter captures local syntax.
Yarowsky has exploitedthis complementarity by combining the two methods using decision lists.
The idea is topool the evidence provided by the component methods, and to then solve a target problemby applying the single strongest piece of evidence, whatever type it happens to be.
Thispaper takes Yarowsky's work as a starting point, applying decision lists to the problem ofcontext-sensitive spelling correction.
Decision lists are found, by and large, to outperformeither component method.
However, it is found that further improvements can be obtainedby taking into account not just the single strongest piece of evidence, but all the availableevidence.
A new hybrid method, based on Bayesian classifiers, is presented for doing this,and its performance improvements are demonstrated.1 Introduct ionTwo classes of methods have been shown useful for resolving lexical ambiguity.
The first testsfor the presence of particular context  words within a certain distance of the ambiguous targetword.
The second tests for col locat ions - -  patterns of words and part-of-speech tags around thetarget word.
The context-word and collocation methods have complementary coverage: the formercaptures the lexical "atmosphere" (discourse topic, tense, etc.
), while the latter captures localsyntax.
Yarowsky \[1994\] has exploited this complementarity by combining the two methods usingdecision lists.
The idea is to pool the evidence provided by the component methods, and to thensolve a target problem by applying the single strongest piece of evidence, whatever type it happensto be.
Yarowsky applied his method to the task of restoring missing accents in Spanish and French,and found that it outperformed both the method based on context words, and one based on localsyntax.
This paper takes Yarowsky's method as a starting point, and hypothesizes that furtherimprovements can be obtained by taking into account not only the single strongest piece of evidence,but all the available vidence.
A method is presented for doing this, based on Bayesian classifiers.The work reported here was applied not to accent restoration, but to a related lexical disam-biguation task: context-sensitive spelling correction.
The task is to fix spelling errors that happento result in valid words in the lexicon; for example:I'd like the chocolate cake for ,desert.where desser t  was misspelled as desert.
This goes beyond the capabilities of conventional spellcheckers, which can only detect errors that result in non-words.39We start by applying a very simple method to the task, to serve as a baseline for comparisonwith the other methods.
We then al)ply each of the two component methods mentioned above - -context words and collocations.
We try two ways of combining these components: decision lists,and Bayesian classifiers.
We evaluate the above methods by comparing them with an alternativeapproach to spelling correction based on part-of-speech trigrams.The sections below discuss the task of context-sensitive spelling correction, the five methods wetried for the task (baseline, two component methods, and two hybrid methods), and the evaluation.The final section draws some conclusions.2 Context-sensit ive spelling correctionContext-sensitive spelling correction is the problem of correcting spelling errors that result in validwords in the lexicon.
Such errors can arise for a. variety of reasons, including typos (e.g., out forour), homonym confusions (there for their), and usage errors (between for among).
These errorsare not detected by collventional spell checkers, as they only notice errors resulting in non-words.We treat context-sensitive spelling correction as a task of word disambiguation.
The ambiguityamong words is modelled by eonfusio~ sets.
A confilsion set C = {wl , .
.
.
,  Wn} means that eachword wi in the set is ambiguous with each other word in the set.
Thus if C = {desert, dessert},then when the spelling-correction program sees an occurrence of either desert or dessert in thetarget document, it takes it to be a.mbiguous between desert and dessert, and tries to infer fi'omthe context which of the two it should be.This treatment requires a collection of confusion sets to start with.
There are several waysto  obtain such a collection.
One is based on finding words in the dictionary that are one typoaway from each other \[Mays et al, 1991\].
1 Another finds words that have the same or similarpronunciations.
Since this was not the focus of the work reported here, we simply took (most of)our confusion sets fl'om the list of "Words Commonly Confused" in the back of the Random Houseunabridged ictionary \[Flexner, 1983\].A final point concerns the two types of errors a spelling-correction program can make: falsenegatives (complaining about a correct word), and false positives (failing to notice an error).
We willmake the simplifying assumption that both kinds of errors are equally bad.
In practice, however,false negatives are much worse, as users get irritated by programs that badger them with boguscomplaints.
However, given the probabilistic nature of the methods that will be presented below,it would not be hard to modify them to take this into account.
We would merely set a confidencethreshold, and report a suggested correction only if the probability of the suggested word exceedsthe probability of the user's original spelling by at least the threshold amount.
The reason thiswas not done in the work reported here is that setting this confidence threshold involves a certainsubjective factor (which depends on the user's "irritability threshold").
Our simpli~,ing assumptionallows us to measure performance objectively, by the single parameter of prediction accuracy.1Constructing confllsion sets in this way requires assigning each word in the lexicon its own confusion set.
Forinstance, cat might have the confusion set {hat, car,...}, hat might have {cat, had .... }, and so on.
We cannot usethe  symmetric conflmion sets that we have adopted -- where very word in the set is confusable with every other  one- -  because  the  "confusable" relation is no longer transitive.403 Five methods  for spel l ing correct ionThis section presents a progression of five methods for context-sensitive spelling correction:Base l ine  An indicator of "minimal competency" for comparison with the other methodsContext  words  Tests for particular words within ::t=k words of the ambiguous target wordCo l locat ions  Tests for syntactic patterns around the ambiguous target wordDec is ion  l ists Combines context words and collocations via decision listsBayes ian  classif iers Combines context words and collocations via Bayesian classifiers.Each method will be described in terms of its operation on a single confusion set C = (Wl , .
.
.
,  w~};that is, we will say how the method disambiguates occurrences of words wl through wn from thecontext.
The methods handle multiple confusion sets by applying the same technique to eachconfusion set independently.Each method involves a training phase and a test phase.
The performance figures given beloware based on training each method on the 1-million-word Brown corpus \[Ku~:era and Francis, 1967\]and testing it on a 3/4-million-word corpus of Wall Street Journal text \[Marcus et al, 1993\].3.1 Base l ine  methodThe baseline method disambiguates words wl through wn by simply ignoring the context, andalways guessing that the word should be whichever wi occurred most often in the training corpus.For instance, if C -- (desert, dessert}, and desert occurred more often than dessert in the trainingcorpus, then the method will predict that every occurrence of desert or dessert in the test corpusshould be changed to (dr left as) desert.Table 1 shows the performance of the baseline method for 18 confusion sets.
This collection ofconfusion sets will be used for evaluating the methods throughout the paper.
Each line of the tablegives the results for one confusion set: the words in the confusion set; the number of instances of anyword in the confusion set in the training corpus and in the test corpus; the word in the confusion setthat occurred most often in the training corpus; and the prediction accuracy of the baseline methodfor the test corpus.
Prediction accuracy is the number of times the correct word was predicted,divided by the total number of test cases.
For example, the members of the confusion set {I, me}occurred 840 times in the test corpus, the breakdown being 744 I and 96 me.
The baseline methodpredicted I every time, a.nd thus was right 744 times, for a score of 744/840 = 0.886.Essentially the baseline method measures how accurately one can predict words using just theirprior probabilities.
This provides a lower bound on the performance we would expect from theother methods, which use more than just the priors.3.2 Component  method 1: Context  wordsOne clue about the identity of an ambiguous target word comes from the words around it.
Forinstance, if the target word is ambiguous between desert and dessert, and we see words like arid,sand, and sun nearby, this suggests that the target word should be desert.
On the other hand,words such as chocolate and delicious ill the context imply dessert.
This observation is the basisfor the method of context words.
The idea is that each word wi in the confusion set will have acharacteristic distribution of words that occur in its context; thus to classify an ambiguous targetword, we look at the set of words around it and see which wi's distribution they most closely follow.41Confusion set No.
oftrainingcasesNo.
oftestcaseswhether, weather 331 245I, me 6125 840its, it's 1951 3575past, passed 385 397than, then 2949 1659being, begin 727 449effect, affect 228 162your, you're 1047 212number, amount 588 429council, counsel 82 83rise, raise 139 301between, among 1003 730led, lead 226 219except, accept 232 95peace, piece 310 61there, their, they're 5026 2187principle, principal 184 69sight, site, cite 149 44MostfrequentwordBaselinewhether 0.922I 0.886its 0.863past 0.861than 0.807being 0.780effect 0.741your 0.726number 0.627council 0.614rise 0.575between 0.538led 0.530except 0.442peace 0.393there 0.306principle 0.290sight 0.114Table 1: Performance of the baseline method for 18 confusion sets.
The "Most frequent word"column gives the word in the confusion set that occurred most frequently in the training corpus.
(Insubsequent tables, confusion sets will be referred to by their most frequent word.)
The "Baseline"column gives the prediction a,ccuracy of the baseline system on the test corpus.Following previous work \[Gale et al, 1994\], we formulate tile method in a Bayesian framework.The task is to pick the word wi that is most probable, given the context words cj observed withina =t:k-word window of the target word.
The probability for each wi is calculated using Bayes' rule:p(wilc-k, .
.
.
,  C-1, e l , .
.
.
,  Ck) = P (C-k ' ' ' "  C-l '  C l ' ' " '  CklWi) p(wi)p (C -k , .
.
.
,C - l ,C l , .
.
.
,Ck)As it stands, the likelihood term, p(c -k  .
.
.
.
, C-l, Cl , .
.
.
, CklWi) , is difficult to estimate from trainingdata - -  we would have to count situations in which the entire context was previously observedaround word wi, which raises a severe sparse-data problem.
Instead, therefore, we assume that thepresence of one word in the context is independent of the presence of any other word.
This lets usdecompose the likelihood into a product:p(c_k,...,c_1,cl,.. ,cklw ) = lII p(cjlw )j 6 -k  .
.
.
.
.
-1 ,1  .
.
.
.
.
kGale et al \[1994\] provide evidence that this is in fact a reasonable approximation.We still have the problem, however, of estimating the individual p(cj lwi)  probabilities from ourtraining corpus.
The straightforward way would be to use a. maximum likelihood estimate - -  we42would count Mi, the total number of occurrences of wi in the training corpus, and mi, the number ofsuch occurrences for which cj occurred within ?k words, and we would then take the ratio mi/~4i.2Unfortunately, we may not have enough training data to get an accurate stimate this way.
Galeet al \[1994\] address this problem by interpolating between two maximum-likefihood estimates: oneof p(cjlwi), and one of p(cj).
The former measures the desired quantity, but is subject to inaccuracydue to sparse data; the latter provides a robust estimate, but of a potentially irrelevant quantity.Gale et al interpolate between the two so as to minimize the overall inaccuracy.We have pursued an alternative approach to the problem of estimating the likelihood terms.We start with the observation that there is no need to use every word in the ?k-word window todiscriminate among the words in the confusion set.
If we do not have enough training data for agiven word c to accurately estimate p(clwi ) for all w/, then we simply disregard e, and base ourdiscrimination on other, more reliable evidence.
We implement his by introducing a "minimumoccurrences" threshold, Train.
It is currently set to 10.
We then ignore a context word c if:l< i<n l< i<nwhere mi and Mi are defined as above.
In other words, e is ignored if it practically never occurswithin the context of any wi, or if it practically always occurs within the context of every wi.
Inthe former case, we have insufficient data to measure its presence; in the latter, its absence.Besides the reason of insufficient data, a second reason to ignore a context word is if it does nothelp discriminate among the words in the confusion set.
For instance, if we are trying to decidebetween I and me, then the presence of the in the context probably does not help.
By ignoring suchwords, we eliminate a source of noise in our discrimination procedure, as well as reducing storagerequirements and run time.
To determine whether a context word e is a useful discriminator, werun a chi-squa.re test \[Fleiss, 1981\] to check for an association between the presence of c and thechoice of word in the confusion set.
If the observed association is not judged to be significant, athen c is discarded.
The significance level is currently set to 0.05.Figure 1 pulls together the points of the preceding discussion into an outline of the methodof context words.
In the training phase, it identifies a list of context words that are useful fordiscriminating among the words in the confusion set.
At run time, it estimates the probability ofeach word in the confusion set.
It starts with the prior probabilities, and multiplies them by thelikelihood of each context word fl'om its list that appears in the ?k-word window of the targetword.
Finally, it selects the word in the confusion set with the greatest probability.The main parameter to tune for the method of context words is k, the half-width of the contextwindow.
Previous work \[Yarowsky, 1994\] shows that smaller values of k (3 or 4) work well forresolving local syntactic ambiguities, while larger values (20 to 50) are suitable for resolving semanticambiguities.
We tried the values 3, 6, 12, and 24 on some practice confusion sets (not shown here),and found that k = 3 generally did best, indicating that most of the action, for our task andconfusion sets, comes fl'om local syntax.
In the rest of this paper, this value of k will be used.=We are interpret ing the condit ion "cj occurs within a =l=k-word window of wi" as a binary feature - -  either ithappens,  or it does not.
This allows us to handle context words in the same Bayesian framework as will be usedlater for other binary features (see Section 3.3).
A more conventional interpretat ion is to take into account thenumber of occurrences of each cj within the ::l=k-word window, and to est imate p(cjlwi ) accordingly.
However, eitherinterpretat ion is valid, as long as it is applied consistently - -  that  is, both when est imat ing the likelihoods fromtra in ing data,  and when classifying test.
cases.3An association is significant if the probabi l i ty that  it occurred by chance is low.
This is not a s tatement  aboutthe strength of the association.
Even a weak association may be judged significant if there are enough data  to supportit.
Measures of the strength of association will be discussed in Section 3.4.43Training phase(1) Propose all words as candidate context words.
(2) Count occurrences of each candidate context word in the training corpus.
(3) Prune context words that have insufficient data or are uninformative discriminators.
(4) Store the remaining context words (and their associated statistics) for use at run time.Run time(1) Initialize the probability for each word in the confusion set to its prior probability.
(2) Go through the list of context words that was saved during training.
For each context wordthat appears in the context of the ambiguous target word, update the probabilities.
(3) Choose the word in the confusion set with the highest probability.Figure 1: Outline of the method of context words.Table 2 shows the effect of varying k for our usual collection of confusion sets.
It can be seenthat performance generally degrades as k increases.
The reason is that the method starts pickingup spurious correlations in the training corpus.
Table 4 gives some examples of the context wordslearned for the confusion set {peace, piece}, with k = 24.
The context words coTTs, united, nations,etc., all imply peace, and appear to be plausible (although united and nations are a counterexampleto our earlier assumption of independence).
On the other hand, consider the context word how,which allegedly also implies peace.
If we look back at the training corpus for the supporting datafor this word, we find excerpts uch as:But oh, how I do sometimes need just a moment of rest, and peace .
.
.No matter how earnest is our quest for guaranteed peace .
.
.How best to destroy your peace ?There does not seem to be a necessary connection here between how and peace; the correlation isprobably spurious.
Although we are using a chi-square test expressly to filter out such spuriouscorrelations, we can only expect the test to catch 95% of them (given that the significance levelwas set to 0.05).
As mentioned above, most of the legitimate context words show up for small k;thus as k gets large, the limited number of legitimate context words gets overwhelmed by the 5%of the spurious correlations that make it through our filter.3.3 Component  method 2: Co l locat ionsThe method of context words is good at capturing generalities that depend on the presence ofnearby words, but not their order.
When order matters, other more syntax-based methods, such ascollocations and trigrams, are appropriate.
In the work reported here, the method of collocationswas used to capture order dependencies.
A collocation expresses a pattern of syntactic elementsaround the target word.
We allow two types of syntactic elements: words, and part-of-speech tags.Going back to the {desert, dessert} example, a collocation that would imply desert might be:PREP the44ConfusionsetwhetherIitspastthanbeingeffectyournumbercouncilri sebetweenledexceptpeacethereprinciplesightBaseline0.9220.8860.8630.8610.8070.7800.7410.7260.6270.6140.5750.5380.5300.4420.3930.3060.2900.114Avg no.
of context wordsCwords Cwords Cwords Cwords:1=3 =t=6 =i:12 =t=240.902 0.922 0.927 0.9220.914 0.893 0.883 0.8510.862 0.795 0.743 0.7020.861 0.849 0.801 0.7430.931 0.901 0.896 0.8550.791 0.795 0.793 0.7550.747 0.741 0.759 0.7160.816 0.783 0.774 0.7360.646 0.622 0.636 0.6390.639 .
0.614 0.602 0.6140.575 0.575 0.585 0.4980.759 0.697 0.671 0.5860.530 0.530 0.521 0.5570.695 0.526 0.516 0.5580.754 0.705 0.574 0.5740.726 0.623 0.557 0.4660.290 0.290 0.290 0.4350.455 0.250 0.364 0.31827.9 36.9 55.9 92.9Table 2: Performance of the method of context words as a function of k, the half-width of the contextwindow.
The bottom line of the table shows the number of context words learned, averaged overall confusion sets, also as a function of k.This collocation would match the sentences:Travelers entering from the desert  were confounded.... .
.
along with some guerrilla fighting in the desert.. .
.
two  ladies who lay pinkly nude beside him in the desert  .
.
.Matching part-of-speech tags (here, PREP) against the sentence is done by first tagging each wordin the sentence with its set of possible part-of-speech tags, obtained from a dictionary.
For instance,walk has the tag set {NS, V}, corresponding to its use as a singular noun and as a verb.
4 For a tagto match a word, the tag must be a member of the word's tag set.
The reason we use tag sets,instead of running a tagger on the sentence to produce unique tags, is that taggers need to look atall words in the sentence, which is impossible when the target word is taken to be ambiguous (butsee the trigram method in Section 4).The method of collocations was implemented in much the same way as the method of contextwords.
The idea.
is to discriminate among the words wi in the confusion set by identifying thecollocations that tend to occur around each wi.
An ambiguous target word is then classified byfinding all collocations that match its context.
Each collocation provides some degree of evidence4Our tag inventory contains 40 tags, and includes the usual categories for determiners, nouns, verbs, modals, etc.,a few specialized tags (for be, have, and do), and a dozen compound tags (such as V+PRO for let's).45for each word in the confusion set.
This evidence is combined using Bayes' rule.
In the end, the wiwith the highest probability, given the evidence, is selected.A new complication arises for collocations, however, in that collocations, unlike context words,cannot be assumed independent.
Consider, for example, the following collocations for desert:PREP thein thethe __These collocations are highly interdependent - -  we will say they conflict.
To deal with this problem,we invoke our earlier observation that there is no need to use all the evidence.
If two pieces ofevidence conflict, we simply eliminate one of them, and base our decision on the rest of the evidence.We identify conflicts by the heuristic that two collocations conflict iff they overlap.
The overlappingportion is the factor they have in common, and thus represents their lack of independence.
Thisis only a heuristic because we could imagine collocations that do not overlap, but still conflict.Note, incidentally, that there can be at most two non-conflicting collocations for any decision - -one matching on the left-hand side of the target word, and one on the right.Having said that we resolve conflicts between two collocations by eliminating one of them, westill need to specify which one.
Our approach is to assign each one a strength, just as Yarowsky\[1994\] does in his hybrid method, and to eliminate the one with the lower strength.
This preservesthe strongest non-conflicting evidence as the basis for our answer.
The strength of a collocationreflects its reliability for decision-making; a further discussion of strength is deferred to Section 3.4.Figure 2 ties together the preceding discussion into an outline of the method of collocations.
Themethod is described in terms of "features" rather than "collocations" to reflect its full generality;the features could be context words as well a.s collocations.
In fact, the method subsumes themethod of context words - -  it does everything that method does, and resolves conflicts among itsfeatures as well.
To facilitate the conflict resolution, it sorts the features by decreasing strength.Like the method of context words, the method of collocations has one main parameter to tune:e, the maximum number of syntactic elements in a collocation.
Since the number of collocationsgrows exponentially with e, it was only practical to vary g from 1 to 3.
We tried this on somepractice confusion sets, and found that all values of g gave roughly comparable performance.
Weselected g = 2 to use from here on, as a compromise between reducing the expressive power ofcollocations (with g = 1) and incurring a high computational cost (with g = 3).Table 3 shows the results of varying f for the usual confusion sets.
There is no clear winner; eachvalue of g did best for certain confusion sets.
Table 5 gives examples of the collocations learned for{peace, piece} with g = 2.
A good deal of redundancy can be seen among the collocations.
There isalso some redundancy between the collocations and the context words of the previous section (e.g.,for corps).
Many of the collocations a.t the end of the list appear to be overgeneral and irrelevant.3.4 Hybr id  method  1: Dec is ion  l istsYarowsky \[1994\] pointed out the complementarity between context words and collocations: contextwords pick up those generalities that are best expressed in an order-independent way, while collo-cations capture order-dependent generalities.
Yarowsky proposed decision lists as a way to get thebest of both methods.
The idea is to make one big list of all features - -  in this case, context wordsand collocations.
The features are sorted in order of decreasing strength, where the strength of afeature reflects its reliability for decision-making.
An ambiguous target word is then classified byrunning down the list and matching each feature against the target context.
The first feature that46Training phase(1)(2)(3)(3.5)(4)Propose all possible features as candidate features.Count occurrences of each candidate feature in the training corpus.Prune features that have insufficient data or are uninformative discriminators.Sor t  the  remain ing  features  in o rder  o f  decreas ing  s t rength .Store the list of features (and their associated statistics) for use at run time.Run time(1)(2)(3)Initialize the probability for each word in the confusion set to its prior probability.Go through the sorted list of features that was saved during training.
For each featurethat matches the context of the ambiguous target word, and does  not conf l ict  w i tha feature  accepted  prev ious ly ,  update the probabilities.Choose the word in the confiision set with the highest probability.Figure 2: Outline of the method of collocations.
Differences from the method of context words arehighlighted in boldface.
The method is described in terms of "features" rather than "collocations"to reflect its full generality.matches is used to classify the target word.
Yarowsky \[1994\] describes further refinements, uch asdetecting and pruning features that make a zero or negative contribution to overall performance.The method of decision lists, as just described, is almost the same as the method for collocationsin Figure 2, where we take "features" in that figure to include both context words and collocations.The main difference is that during evidence gathering (step (2) at run time), decision lists terminateafter matching the first feature.
This obviates the need for resolving conflicts between features.Given that decision lists base their answer for a problem on the single strongest feature, theirperformance rests heavily on how the strength of a feature is defined.
Yarowsky \[1994\] used thefollowing metric to calculate the strength of a feature f :abs / log  (p (w l l f ) )  reliability(f) \ \p(w21f) \]\]This is for the case of a confusion set of two words, wl and w2.
It can be shown that this metricproduces the identical ranking of features as the following somewhat simpler metric, providedp(wi\]f)  > 0 for all i: sreliability'(f) = m.ax p(wi l f )As an example of using tile metric, suppose f is the context word arid, and suppose that arid co-occurs 10 times with desert and 1 time with dessert in the training corpus.
Then reliability~(f) =max(10/11, 1/11) = 10/11 = 0.909.
This value measures the extent to which the presence of thefeature is unambiguously correlated with one particular wi.
It can be thought of as the feature'sreliability at picking out that wi fi'om the others in the confusion set.Sin fact, we guarantee that this inequality holds by performing smoothing before calculating strength.
We smooththe data by adding 1 to the count of how many times each feature was observed for each wi.47ConfllsionsetwhetherIitspastthanbeingeffectyournumbercouncilrisebetweenledexceptpeacethereprinciplesightBaseline0.9220.8860.8630.8610.8070.7800.7410.7260.6270.6140.5750.5380.5300.4420.3930.3060.2900.114Collocs Collocs Collocs<1 _<2 _<30.939 0.931 0.9310.979 0.981 0.9800.943 0.945 0.9500.919 0.909 0.9090.966 0.965 0.9660.853 0.853 0.8420.821 0.821 0.8210.877 0.887 0.8870.646 0.646 0.6810.663 0.639 0.6390.807 0.807 0.8070.699 0.730 0.7330.849 0.840 0.8630.800 0.789 0.7890.869 0.869 0.8520.911 0.932 0.9320.841 0.812 0.8120.341 0.318 0.318Avg no.
of collocations 33.9 263.1 985.4Table 3: Performance of the method of collocations as a function of g, the maximum length of acollocation.
The bottom line of the table shows the number of collocations learned, averaged overall confusion sets, also as a function of e.One peculiar property of the reliability metric is that it ignores the prior probabilities of thewords in the confusion set.
For instance, in the arid example, it would award the same high scoreeven if the total number of occurrences of desert and dessert in the training corpus were 50 and5, respectively - -  in which case arid's 1)erformance of 10/11 would be exactly what one wouldexpect by chance, and therefore hardly impressive.
Besides the reliability metric, therefore, wealso considered an alternative metric: the uncertainty coefficient of x, denoted U(xIy ) \[Press et al,1988, p.501\].
U(xly ) measures how much additional information we get about the presence of thefeature by knowing the choice of word in the confusion set.
6 U(xly ) is calculated as follows:H(x) -  H(xly )v(x ly )  = H(x)H(x) = -p(f)  lnp( f ) -  p(~f)lnp(-~/)H(xly ) = - ~ p(wl) (p(flwi) lnp(flwl) + p(~flwi) ln p(~flwi) )iThe probM)ilities are calculated for the population consisting of all occurrences in the trainingcorpus of any wi.
For instance, p(f) is the probability of feature f being present within this6This definition may seem backwards, but is appropriate for use on the right-hand side of Bayes' rule, where thechoice of word in the confusion set is the "given".48Context  word peace piececorpspeaceunitednat ionsourheartjust icestateameri  canaidinternat ionalwomenwarworldpieceovermustgreatunderhowtwoforabouteveryl itt lelongonetheso49 141 120 015 027 112 012 012 011 011 011 010 020 140 31 151 1411 111 110 110 15 1283 384 94 95 106 1114 23179 1139 1416 22Total  occurrences 184 126Col locat ionw corpsDET w corpsADV __  corpsthe __ corps__ and__  of NSthe __ NSa __  PREPPREP  __  o fa __  o ffo rand NSDET __  NPNS __ of__ corps NSPREP __  CONJthe __ NPV CONJ- -  NS PUNCo f vC, ONJ  ADJthe NS __NS ADJADV NS __PREP  N SADV __  PREPADJ  ADJN SADJNS N Speace4732282722237111161632214142713131piece000006013534340014500100254 94 913 2612 2317 3112 229 1462 7946 5429 32Total  occurrences 184 126Table 4: Excerpts  from the list of 43 contextwords learned for {peace, piece} with k = 24.Each line gives a context word, and the num-ber of peace and piece occurrences for whichthat  context word occurred within ?k  words.The last line of the table gives the total  num-ber of occurrences of peace and piece in thetra in ing corpus.Table 5: Excerpts  f rom the sorted list of98 col locations learned for {peace, piece} with= 2.
Each line gives a col locat ion,  andthe number  of peace and piece occurrences itmatched.
The last line of the table gives thetotal  number  of occurrences of peace and piecein the tra in ing corpus.49population.
Applying tim U(x\]y) metric to the arid example, the value returned now depends onthe number of occurrences of desert and dessert in the training corpus.
If these numbers are 50and 5, then U(xly ) = 0.0, reflecting the mfinformativeness of the arid feature in this situation.If instead the numbers are 50 and 500, then U(xly ) = 0.402, indicating arid's better-than-chanceability to pick out desert (10 out of 50 occurrences) over dessert (1 out of 500 occurrences).To compare the two strength metrics, we tried both on some practice confusion sets.
Sometimesone metric did sul)stantially better, sometimes the other.
In the balance, the reliability metricseemed to give higher performance.
This metric is therefore the one that will be used from hereon.
It was also used for all experiments involving the method of collocations.Table 6 shows the performance of decision lists with each metric for the usual confusion sets.As with the practice confusion sets, we see sometimes dramatic performance differences betweenthe two metrics, and no clear winner.
For instance, for {I, me}, the reliability metric did betterthan U(xly) (0.980 versus 0.808); whereas for {between, among}, it did worse (0.659 versus 0.800).Further research is needed to understand the circumstances under which each metric performs best.Focusing for now on the reliability metric, Table 6 shows that the method of decision lists does,by and large, accomplish what it set out to do - -  namely, outperform either component methodalone.
There axe, however, a few cases where it falls short; for instance, for {between, among},decision lists score only 0.659, compared with 0.759 for context words and 0.730 for collocations.
7We believe that the problem lies in the strength metric: because decision lists make their judgementsbased on a single piece of evidence, their performance is very sensitive to the metric used to selectthat piece of evidence.
But as the relial)ility and U(x\[y) metrics indicate, it is not completely clearhow the metric should be defined.
This problem is addressed in the next section.3.5 Hybr id  method 2: Bayes ian  c lass i f ie rsThe previous section confirmed that decision lists are effective at combining two complementarymethods - -  context words and collocations.
In doing the combination, however, decision lists lookonly at the single strongest piece of evidence for a given problem.
We hypothesize that even betterperformance can be obtained by ta.king into account all available vidence.
This section presents amethod of doing this based on Bayesian classifiers.Like decision lists, the Bayesian method starts with a list of all features, sorted by decreasingstrength.
It classifies a.n ambiguous target word by matching each feature in the list in turn againstthe target context.
Instead of stopping at the first matching feature, however, it traverses the entirelist, combining evidence fi'om all matching features, and resolving conflicts where necessary.This method is essentially the same as the one for collocations (see Figure 2), except that ituses context words as well as collocations for the features.
The only new wrinkle is in checkingfor conflicts between features (in step (2) a.t run tilne), as there are now two kinds of features toconsider.
If both features are context words, we say the features never conflict (as in the methodof context words).
If both features are collocations, we say they conflict iff they overlap (as inthe method of collocations).
The new case is if one feature is a context word, and the other is acollocation.
Consider, for example, the context word walk, and the following collocations:(1) __ walk(2 )  _ v(3 )  CONJ __  PREP7If we use the U(x\[y) metr ic instead, then decision lists fall down on different examples; e.g., {its, it's}.50ConfusionsetwhetherIitspastthanbeingeffectyournumbercouncilrisebetweenledexceptpeacethereprinciplesightBaseline Cwords Collocs:53 < 20.922 0.902 0.9310.886 0.914 0.9810.863 0.862 0.9450.861 0.861 0.9090.807 0.931 0.9650.780 0.791 0.8530.741 0.747 0.8210.726 0.816 0.8870.627 0.646 0.6460.614 0.639 0.6390.575 0.575 0.8070.538 0.759 0.7300.530 0.530 0.8400.442 0.695 0.7890.393 0.754 0.8690.306 0.726 0.9320.290 0.290 0.8120.114 0.455 0.318Dlist DlistRely U(xly )0.935 0.8290.980 0.8080.931 0.8050.932 0.8920.967 0.9610.842 0.9330.821 0.6540.868 0.8960.629 0.6670.627 0.6510.804 0.8270.659 0.8000.840 0.8400.789 0.7260.852 0.8360.914 0.9060.812 0.8410.432 0.568Table 6: Performance of decision lists with the reliability and U(xly ) strength metrics.To some extent, all of these collocations conflict with walk.
Collocation (1) is the most blatantcase; if it matches the target context, this logically implies that the context word walk will match.If collocation (2) matches, this guarantees that one of the possible tags of walk will be presentnearby the target word, thereby elevating the probability that walk will match within :5k words.If collocation (3) matches, this guarantees that there are two positions nearby the target word thatare incompatible with walk, thereby reducing the probability that walk will match.
If we wereto treat all of these cases as conflicts, we would end up losing a great deal of (potentially useful)evidence.
Instead, we adopt the more relaxed policy of only flagging the most egregious conflicts- -  here, the one between collocation (1) and walk.
In general, we will say that a collocation and acontext word conflict iff the collocation contains an explicit test for the context word.Table 7 compares all methods covered so far - -  baseline, two component methods, and twohybrid methods.
(A sixth method, trigrams, is included as well - -  it will be discussed in Section 4.
)The table shows that the Bayesian hyt)rid method does at least as well as the previous four methodsfor almost every confusion set.
Occasionally it scores slightly less than collocations; this appearsto be due to some averaging effect where noisy context words are dragging it down.
Occasionallytoo it scores less than decision lists, 1)ut never by much; on the whole, it yields a modest butconsistent improvement, and in the case of {between, among}, a sizable improvement.
We believethe improvement is due to considering all of the evidence, rather than just the single strongest piece,which makes the method more robust to inaccurate judgements about which piece of evidence is"strongest".51ConfusionsetwhetherIitspastthanbeingeffectyournumbercouncilrisebetweenledexceptpeacethereprinciplesightBaseline0.9220.8860.8630.8610.8070.7800.7410.7260.6270.614O.5750.5380.5300.4420.3930.3060.2900.114Cwords Collocs Dlist Bayes:t:3 _< 2 Rely Rely0.902 0.931 0.935 0.9350.914 0.981 0.980 0.9850.862 0.945 0.931 0.9420.861 0.909 0.932 0.9240.931 0.965 0.967 0.9730.791 0.853 0.842 0.8690.747 0.821 0.821 0.8270.816 0.887 0.868 0.9010.646 0.646 0.629 0.6620.639 0.639 0.627 0.6390.575 0.807 0.804 0.8070.759 0.730 0.659 0.7860.530 0.840 0.840 0.8400.695 0.789 0.789 0.8110.754 0.869 0.852 0.8520.726 0.932 0.914 0.9160.290 0.812 0.812 0.8120.455 0.318 0.432 0.455Trigrams0.8730.9850.9650.9550.7800.9780.9750.9580.6360.6510.5740.5380.9090.6950.3930.9610.6090.250Table 7: Performance of six methods for context-sensitive spelling correction.4 Eva luat ionWhile the previous section demonstrated that the Bayesian hybrid method does better than itscomponents, we would still like to know how it compares with alternative methods.
We looked ata method based on part-of-speech trigrams, developed and implemented by Schabes \[1995\].Schabes's method can be viewed as performing an abductive inference: given a sentence con-taining an ambiguous word, it asks which choice wi for that word would best explain the observedsequence of words in the sentence.
It answers this question by substituting each wi in turn into thesentence.
The wi that produces the highest-probability sentence is selected.
Sentence probabilitiesare calculated using a part-of-speech trigram model.We tried Schabes's method on the usual confusion sets; the results are in the last column ofTable 7.
It can be seen that trigrams and the Bayesian hybrid method each have their bettermoments.
Trigrams are at their worst when the words in the confusion set have the same part ofspeech.
In this case, trigrams can distinguish between the words only by their prior probabilities - -this follows from the way the method calculates entence probabilities.
Thus, for {between, among},for example, where both words are prepositions, trigrams score the same as the baseline method.In such cases, the Bayesian hybrid method is clearly better.
On the other hand, when the wordsin the confusion set have different parts of speech - -  as in, for example, {there, their, they%e} - -trigrams are often better than the Bayesian method.
We believe this is because trigrams look notjust at a few words on either side of the target word, but at the part-of-speech sequence of the wholesentence.
This analysis indicates a complementarity between trigrams and Bayes, and suggests a52combination ill which trigrams would be applied first, but if trigrams determine that the words inthe confusion set have the same part of speech for the sentence at issue, then the sentence wouldbe passed to the Bayesian method.
This is a research direction we plan to pursue.5 ConclusionThe work reported here builds on Yarowsky's use of decision lists to combine two componentmethods - -  context words and collocations.
Decision lists pool the evidence fl'om the two methods,and solve a target problem by applying the single strongest piece of evidence, whichever typethat happens to be.
This paper investigated the hypothesis that even better performance can beobtained by basing decisions on not just the single strongest piece of evidence, but on M1 availableevidence.
A method for doing this, based on Bayesian classifiers, was presented.
It was appliedto the task of context-sensitive spelling correction, and was found to outperform the componentmethods as well as decision lists.
A comparison of the Bayesian hybrid method with Schabes'strigram-based method suggested a further combination i which trigrams would be used when thewords in the confusion set had different parts of speech, and the Bayesian method would be usedotherwise.
This is a direction we plan to pursue in future research.AcknowledgementsWe would like to thank Bill Freeman, Yves Schabes, Emmanuel Roche, and Jacki Golding forhelpful and enjoyable discussions on the work reported here.ReferencesJoseph L. Fleiss.
Statistical Methods for Rates and Proportions.
John Wiley and Sons, 1981.Stuart Berg Flexner, editor.
Random House Unabridged Dictionary.
Random House, New York,1983.
Second edition.William A. Gale, Kenneth W. Church, and David Yarowsky.
Discrimination decisions for 100,000-dimensional spaces.
In Current Issues in Computational Linguistics: In Honour off Don Walker,pages 429-450.
Kluwer Academic Publishers, 1994.H.
Ku~era and W. N. Francis.
Computational Analysis of Present-Day American English.
BrownUniversity Press, Providence, RI, 1967.Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz.
Building a large annotatedcorpus of English: The Penn Treebank.
Computational Linguistics, 19(2):313-330, June 1993.Eric Mays, Fred J. Damerau, and Robert L. Mercer.
Context based spelling correction.
InformationProcessing ~4 Management, 27(5):517-522, 1991.William Press, Brian Flannery, Saul Teukolsky, and William Vetterling.
Numerical Recipes in C:The Art of Scientific Computing.
Cambridge University Press, New York, 1988.
Reprinted twice.Yves Schabes.
Technical report, Mitsubishi Electric Research Laboratories, 1995.
Forthcoming.David Yarowsky.
A comparison of corpus-based techniques for restoring accents in Spanish andFrench text.
In Proceedings of the 2nd Annual Workshop on Very Large Corpora, Kyoto, 1994.53
