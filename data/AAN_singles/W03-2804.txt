A Quantitative Method for Machine Translation EvaluationJes?s Tom?sEscola Polit?cnica Superior deGandiaUniversitat Polit?cnica deVal?nciajtomas@upv.esJosep ?ngel MasDepartament d?IdiomesUniversitat Polit?cnica deVal?nciajamas@idm.upv.esFrancisco CasacubertaInstitut Tecnol?gicd?Inform?ticaUniversitat Polit?cnica deVal?nciafcn@iti.upv.esAbstractAccurate evaluation of machinetranslation (MT) is an open problem.
Abrief survey of the current approach totackle this problem is presented and anew proposal is introduced.
This proposalattempts to measure the percentage ofwords, which should be modified at theoutput of an automatic translator in orderto obtain a correct translation.
To showthe feasibility of the method we haveassessed the most important Spanish-Catalan translators in comparing theresults obtained by the various methods.1 IntroductionResearch in automatic translation lacks anappropriate, consistent and easy to use criterionfor evaluating the results (White et al, 1994;Niessen et al, 2000).
However, it turns out to beindispensable to have some tool that may allowus to compare two translation systems or to elicithow any variation of our system may affect thequality of the translations.
This is important inthe field of research as well as when a user has tochoose between two or more translators.The evaluation of a translation system showsa number of inherent difficulties.
First of all weare dealing with a subjective process, which iseven difficult to define.This paper is circumscribed to the projectSISHITRA (SIStemas H?bridos para laTRAducci?n valenciano-castellano supported bythe Spanish Government), whose aim is theconstruction of an automatic translator betweenSpanish and Catalan texts using hybrid methods(both deductive and inductive).In the following section we discuss some ofthe most important translation quality metrics.After that, we introduce a semiautomaticmethodology for MT evaluation and we show atool to facilitate this kind of evaluation.
Finally,we present the results obtained on the evaluationof several Spanish-Catalan translators.2 Metrics in MT Evaluation2.1    Automatic Evaluation CriteriaWithin the scope of inductive translation, the useof objective metrics, which can be evaluatedautomatically, is quite frequent.
These metricstake as their starting point a possible referencetranslation for each of the sentences we want totranslate.
This reference will be compared withthe proposed sentences by the translation system.The most important metric systems are:Word Error Rate (WER):WER is the percentage of words, which are to beinserted, deleted or replaced in the translation inorder to obtain the sentence of reference (Vidal,1997; Tillmann et al, 1997).
WER can beobtained automatically by using the editingdistance between both sentences.
This metric iscomputed efficiently and is reproducible(successive applications to the same data producethe same results).
However, the main drawback isits dependency on the sentences of reference.There is an almost unlimited number of correcttranslations for one and the same sentence and,however, this metric considers only one to becorrect.Sentence Error Rate (SER):SER indicates the percentage of sentences, whosetranslations have not matched in an exact mannerthose of reference.
It shows similar advantagesand shortcomings as WER.Some variations on WER have been defined,which can also be obtained automatically:Multi reference WER (mWER):Identical approach to WER, but it considersseveral references for each sentence to betranslated, i.e., for each sentence the editingdistance will be calculated with regard to thevarious references and the smallest one is chosen(Niessen et al, 2000).
It presents the drawback ofrequiring a great human effort before actuallybeing able to use it.
However, the effort isworthwhile, if it can be later used for hundreds ofevaluations.BLEU Score:BLEU is an automatic metric designed by IBM,which uses several references (Papineni et al,2002).
The main problem of mWER is that allpossible reference translations cannot beintroduced.
The BLEU score try to solve thisproblem by combining the available references.In a simplified manner we could say that itmeasures how many word sequences in thesentence under evaluation match the wordsequences of some reference sentence.
TheBLEU score also includes a penalty fortranslations whose length differs significantlyfrom that of the reference translation.2.2     Subjective Evaluation CriteriaOther kinds of metrics have been developed,which require human intervention in order toobtain an evaluation.
Among the most widelyused we could stand out:Subjective Sentence Error Rate (SSER)Each sentence is scored from 0 to 10, accordingto its translation quality (Niessen et al, 2000).An example of these categories is:0 ?
nonsensical...1 ?
some aspects of the content are conveyed...5 ?
comprehensible, but with importantsyntactic errors...9 ?
OK. Only slight style errors.10 ?
perfect.The biggest problem shown by this technique isits subjective nature.
Two people who mayevaluate the same experiment could obtain quitedifferent results.
To solve this problem severalevaluations can be performed.
Another drawbackis that the different sentence lengths have notbeen taken into account.
The score of a 100word-long sentence has the same impact on thetotal score as that of a word-long sentence.Information Item Error Rate (IER)An unclear question is how to evaluate longsentences consisting of correct and wrong parts.IER attempts to find a solution to this question.In order to solve the problem the concept of?information items?
is introduced.
The sentencesare divided into word segments.
Each item of thesentence is marked with ?OK?, ?error?, ?syntax?,?meaning?
or ?others?, as shown in thetranslation.
The metric IER (Information ItemError Rate) can then be calculated as thepercentage of badly translated items (not markedas ?OK?)
(Niessen et al, 2000).2.3 New Evaluation CriteriaAutomatic metrics are especially useful, sincetheir cost is practically null.
However, they arevery dependent on the used references.
In somecases they can yield misleading results, forinstance, if we want to compare an inductivetranslation system with some deductive onewhich, in principle, should produce translationsof a similar quality.
If we extract the referencesfrom the same source as the training material ofthe inductive translator, the inductive translatorwill have an advantage over the deductivetranslator, since it has learned to translate byusing a vocabulary and structures that are similarto those appearing in the references.The non-automatic evaluation metricsdescribed above presents various constraints:When an SSER is used, it may be very difficultto decide the score to be assigned to onesentence.
For example, if in one sentence a smallsyntactic error appears, we can assign an 8.
If inthe following sentence two similar errors appear,what score should we assign?
The same or halfthe score?
To solve these kinds of matters, IERintroduces the concept of ?information item?.This proposal has the drawback of being quitecostly, both during the initial stage of decidingthe word segments which form each item as wellas when classifying the correction for each item.After having seen the previous drawbacks thefollowing metric has been introduced:All references WER (aWER):It measures the number of words, which are to beinserted, deleted or replaced in the sentenceunder evaluation in order to obtain a correcttranslation.
It can also be seen as a particular caseof the mWER, but taking for granted that all thepossible references are at our disposal.
Since it isimpossible to have a priori all possiblereferences, the evaluator will be able to proposenew references, if needed.
The evaluation processcan be carried out very quickly, if one takes asthe starting point the result obtained by the WERor the mWER.
The idea consists of visualisingthe incorrect words detected by one of thesemethods (editing operations).
The evaluator justneeds to indicate whether each of the markeditems is an actual error or whether it can rather beconsidered as an alternative translationThis metric resembles very much the oneproposed in (Brown et al 1990).
That worksuggested for measuring the translation qualitycounting the number of times an evaluator wouldhave to press the keyboard keys in order to makethe proposed sentence correct.All references Sentence Error Rate (aSER):The SER metric presents the drawback ofworking with only one reference.
Therefore, itdoes not really measure the number of wrongsentences, but rather those that do not matchexactly the reference.
For this reason we thoughtit would be interesting to introduce a metric thatcould indicate the percentage of sentences whoseacronym name  on references descriptionWER Word Error Rate word 1% of words which are to be inserted,deleted or replaced in order to obtain thereference.SER Sentence Error Rate sent.
1 % of sentences different from reference.mWER Multi reference WER word various The same as WER, but with several reference sentences.BLEU Bilingual Evaluation Understudyobjectivesent.
various The number of word groups that match the reference groups.SSER Subjective Sentence Error Rate sent.
-To each sentence a score from 0 to 10 isassigned.
Later on, it is converted into %.IER Information Item Error Rate item -The sentence is segmented intoinformation items.
IER = % of badlytranslated items.aWER All references WER word -% of words to be inserted, deleted orreplaced in order to obtain a correcttranslation.aSER All references SERsubjectivesent.
- % of incorrect sentences.Table 1.
Some metrics in MT evaluationtranslations are incorrect.
This metric can beobtained as a by-product of the aWER.3 Evaluation Tool for MTIn order to facilitate the evaluation of automatictranslators a graphic user interface has beenimplemented.
The metrics provided by theprogram are: WER, mWER, aWER, SER, SSERand aSER.
Figure 1 shows how it is displayed.Next, the way the program works is described:On the editing window from top to bottom thefollowing items are displayed: the sourcesentence, the sentence to be evaluated, the newsentences proposed by the user, the four mostsimilar references to the sentence underevaluation (according to editing distance).
Thenew sentence proposed by the user will be inprinciple the same as that of the most similarreference.
In the sentence being evaluated usingdifferent colours, depending on whether they areconsidered insertions, replacements or deletions,the words that may be wrong are highlighted.The user can click with the mouse on thosewords that may be considered correct.
As aresult, this action will modify the new reference.In the example (figure 1), if the user clicks on thehighlighted words ?-?, ?Diagram?
and ?locate?,he will obtain the new reference ?Diagramshows the scan procedure to locate thearchives.?.
This new reference reduces theediting distance from 5 to 2.
The user will also beable to click directly on some word of newreference to modify it.
The aim of this is to allowthe evaluator the introduction of any newreference which may be a correct translation ofthe source sentence and which, furthermore, mayresemble most closely the sentence beingevaluated.This tool can be obtained for free on(http://ttt.gan.upv.es/~jtomas/eval), both in theLinux version as wells as in Windows.3.1 Evaluation Database FormatA format in XML has been defined to store thereference files.
For each evaluation sentence westore: the source sentence, the target referencesentences and the target sentences proposed bythe different MT with their subjectiveFigure 1.
The Graphic User Interface.
The system highlights the non-matching wordsbetween the evaluation sentence and the nearest reference.evaluations.
Should during an aWER evaluationa new reference be proposed, this one is alsostored.
An example of a file with a sentenceunder evaluation is shown as follows:<evalTrans><sentence><source>La figura muestra el m?todo.</source><eval translator="first reference"><target>This figure shows the procedure.</target></eval><eval translator="multi reference"><target>This figure shows the method.</target></eval><eval translator="Statistical"evaluator="JM" sser="8" awer="1/5"><target>Chart represent the method.</target><newRef>Chart represents the method.</newRef></eval></sentence>...</evalTrans>4 Example of Evaluation4.1 Spanish-Catalan TranslatorsThe tool described in the previous section hasbeen applied to the most important Spanish-Catalan translators.The Catalan language receives more or lessintense institutional support in all territories ofthe Spanish state, where it is co-official withSpanish (Balearic Islands, Catalonia andValencian Community).
This makes itcompulsory from an administrative standpoint topublish a bilingual edition of all officialdocuments.
For that purpose the use of aMachine Translator becomes almostindispensable.But the official scope is not the only onewhere we can find the need to write bilingualdocuments in a short period of time.
The mostobvious example can be the bilingual edition ofsome newspapers, such as El Pa?s or ElPeri?dico de Catalunya, both in their editions forthe autonomous community of Catalonia.In the following section there is a briefdescription of each of the programs we havereviewed:Salt: an automatic translation program of theValencian local government, which also includesa text corrector.
It can be downloaded for freefrom http://www.cultgva.es.
It has an interactiveoption for solving doubts (subjective ambiguityresolution) and is executed with the OSMicrosoft Windows.Incyta: the translation business web-siteIncyta (http://www.incyta.com) was adding at thetime of this evaluation example review a free on-line automatic translator for short texts.Internostrum: an on-line automatictranslation program, available athttp://www.torsimany.ua.es, designed by theLanguage and Computational SystemsDepartment of the University of Alicante.
Itmarks the doubtful words or segments as areview helping aid.
It uses finite-state technology(Canals et al, 2001).Statistical: An experimental translatordeveloped at the Computer Technology Instituteof the Polytechnic University of Valencia.
Allcomponents have been inferred automaticallyfrom training pairs using statistical methods(Tom?s & Casacuberta, 2001).
It is accessible athttp://ttt.gan.upv.es/~jtomas/trad.4.2 Setting up the evaluation experimentIn order to carry out our evaluation, we havetranslated 120 sentences (2456 words) with thedifferent MT.
These sentences have been takenfrom different media: a newspaper, a technicalmanual, legal text...
The references used by theWER were also taken from the Catalan version ofthe same documents.
In mWER and in BLEU weused three additional references.
These newreferences have been introduced by a humantranslator modifying the initial reference.Before applying the metrics shown in point 2,a human expert carries out a detailed analysis inorder to establish the quality of the translations.The experiment consists of sorting out the fouroutputs obtained by each translator for each testsentence, according to its quality.
If the expertdoes no find any quality difference between thesentences proposed by two translators, he assignsthe same rank to them.
Table 2 shows the resultsobtained.
After this sentence by sentenceanalysis, the expert concludes that Salt is thebetter translator, followed closely by Incyta.Statistical is in an intermediate position and theworst is Internostrum.4.3 ResultsThe results of our experiment can be observed inFigure 2.
Table 3 shows the evaluation time forthe 120 sentences.
The first thing we can pointout is that the Salt translator obtains the bestresults from all used metrics and Internostrum isthe worst of all metrics.
The other two translatorsobtain different results depending on the usedmethod.
Next we will discuss the results obtainedby the different methods:The WER metric shows a strong dependenceon the used reference.
If the translator employs asimilar style or vocabulary with regard to thoseof the reference, it clearly achieves better results.This fact determines that the obtained results donot show faithfully the quality of the translations.Specifically, for Incyta it obtains bad results,although that does not coincide with theconclusions of the expert.The main advantage of this method is that it isa totally automatic measurement without anyevaluation cost.
These conclusions can also beextended to the SER.mWER solves in part the problem posed bythe WER.
To attempt to introduce a priori allpossible translations turns out to be impossible,so that it has to choose a subset of these givingthus the method a certain subjective nature.
In thecase of our evaluation, the references wereintroduced by using certain dialectal variants.That worked slightly against some automatictranslator, which preferred some other dialectalvariants.The BLEU metric tries to combine theavailable references in order to improve themWER metric.
In our experiment the use ofseveral references, in mWER and BLEU, doesnot solve the deficiency of WER.
It continuesbeing most detrimental to Incyta.The use of the mWER and BLEU required agreat initial effort, when the references werewritten, by even choosing only three newreferences for each translation.
However, thesemethods had a big advantage: each evaluation isdone without any additional cost.When we applied the SSER, we faced thefollowing dilemma: Which criteria should we usefor applying the scoring scale?
We decided thatthe latter had to be related with the globalunderstanding of the sentence and the number oferrors in correspondence with the sentencelength.
Since this criterion is not made explicit inthe method the choice of a different criterionwould have produced very diverse results.Regarding the evaluation effort, it was themost costly method.
In order to evaluate eachsentence it was necessary to read and understandboth the source sentence and the target sentenceto try to score at the end the translation.The aWER metric breaks with thedependence on the used references, whichdisplayed the WER, mWER and BLEU.Moreover, it turned out to be much moreobjective and clearer to apply than the SSER.The metric achieved by this method provides uswith clear and intuitive information.
If we use theSalt translator we will have to correct 3% of thewords in order to obtain a correct translation.Interpret the metrics supplied by the othermethods it becomes unavoidable to know theconditions under which the evaluation has beencarried out (references used, criteria ...).The evaluation effort for the aWER issignificantly less than the mWER and the SSER.Translator first second thrid fourthSalt 69% 13% 13% 4%Incyta 63% 11% 13% 13%Statistical 60% 13% 7% 20%Internostrum 48% 12% 20% 20%Table 2.
Comparative classification sentence by sentence.The discussion on the aWER method can beextended to the aSER.Considering the expert evaluation, thesubjective metrics reflect better the quality of theevaluated translations than the automatic ones.The Incyta translator works quite appropriately,but it proposes translations that deviate from thereferences.
Thus, the automatic measures (WER,mWER and BLEU), based on these references,do not evaluate correctly this translator.
On theother hand, the Statistical Translator worksworse, even though its translations are moresimilar to the references.
It is an example-basedtranslator, and the training and test sentenceshave been obtained from the same sources.
Thiscan benefit the evaluation of the Statisticaltranslator using automatic measures.5 ConclusionsIn this paper we present a criterion (aWER) forthe evaluation of translation systems.
Theevaluation of the translations can be carried outquickly thanks to the use of a computer tooldeveloped for this purpose.We have compared this criterion with othercriteria (WER, mWER, SER, BLEU and SSER)using the translations obtained by severalSpanish-Catalan translators.
It is ourunderstanding that automatic measures (WER,mWER and BLEU) do not evaluate correctly thetranslators (specifically, they affect Incytanegatively).Translator WER mWER aWER SER SSER BLEU aSERSalt   9.9 1 6.6 1 3.0 1 68.3 1 10.3 1 0.866 1 40.0 1Incyta 10.9 2 7.6 2 3.1 1 74.2 2 11.2 1 0.855 2 41.7 1Statistical 10.7   2 7.8 2 3.8 2 70.8 2 12.8 2 0.857 2 45.8 2Internostrum 11.9  3 8.5 3 4.9 3 80.0 3 15.8 3 0.837 3 58.3 399,51010,51111,5WER5,66,16,67,17,68,1mWER22,533,544,5aWER62656871747780SER7,5910,51213,515SSER1828384858BLEU303540455055aSERFigure 2.
Comparative evaluation results using 7 different metrics for the 4 Spanish-Catalantranslators.
In order to interpret quickly the results obtained in each metric, we have classifiedeach translator using the following ranking: 1- better 2- intermediate 3- worse.mWER / BLEU SSER aWER / aSERSet-up time*  210 0 0Internostrum 0 70 40Salt 0 60 25Incyta 0 55 30Statistical 0 60 25Total: 210 245 120Table 3.
Comparative evaluation time (minutes) of the 120 sentences usingthe different metrics.
*Time spent to introduce the proposed references.0.840.850.860.870.8The scores produced by human experts (SSERand aWER) are the metrics that best capture thetranslation quality among the different systems.As its most important aWER feature we wouldstand out that, in spite of being a subjectivemethod which requires the intervention of ahuman evaluator, the latter will not have to taketoo subjective decisions.We believe that the aWER tool could be usedin another domain, for the evaluation of othernatural language processing systems, e.g.summarizing systems.In a future our aim is to add to thiscomparative study other score methods, inaddition to comparing the variability introducedby different human evaluators in each of themethods.AcknowledgementThis work was partially funded by the SpanishCICYT under grant TIC2000-1599-C02 and theIST Programme of the European Union undergrant IST-2001-32091.
The authors wish to thankthe anonymous reviewers for their criticism andsuggestions.ReferencesBrown, P. F., J. Cocke, S. Della Pietra, V. DellaPietra, F. Jelinek, R. Mercer, & P. Roossin.
1990.
AStatistical Approach to Machine Translation.Computational Linguistics 16(2).Canals-Marote, R., A. Esteve-Guill?n, A. Garrido-Alenda, M.I.
Guardiola-Savall, A. Iturraspe-Bellver, S. Montserrat-Buendia, S. Ortiz-Rojas, H.Pastor-Pina, P.M. P?rez-Ant?n, M.L.
Forcada.2001.
The Spanish-Catalan machine translationsystem interNOSTRUM.
In Proceedings of theMachine Translation Summit VIII.
Santiago deCompostela, Spain.Niessen, S., F.J. Och, G. Leusch, and H. Ney.
2000.An Evaluation Tool for Machine Translation: FastEvaluation for MT Research.
In Proceedings of the2nd International Conference on LanguageResources and Evaluation, Athens, Greece.Papineni, K.A., S. Roukos, T. Ward, W.J.
Zhu.
2002.Bleu: a method for automatic evaluation ofmachine translation.
In Proceedings of the 40thAnnual Meeting of the Association forComputational Linguistics (ACL), Philadelphia.Tom?s, J., F. Casacuberta.
2001.
Monotone StatisticalTranslation using Word Groups.
In Proceedings ofthe Machine Translation Summit VIII.
Santiago deCompostela, Spain.Tillmann, C., S. Vogel, H. Ney, H. Sawaf, and A.Zubiaga.
1997.
Accelerated DP based Search forStatistical Translation.
In Proceedings of the 5thEuropean Conference on Speech Communicationand Technology, Rhodes, Greece.Vidal, E. 1997.
Finite-State Speech-to-SpeechTranslation.
In Proceedings of the InternationalConference on Acoustics, Speech and SignalProcessing, Munich, Germany.White, J., T. O?Connell, F. O?Mara.
1994.
TheDARPA Machine Translation EvaluationMethodologies: Evolution, Lessons and FutureApproaches.
In Proceedings of the first Conferenceof the Association for Machine Translation in theAmericas.
Columbia, USA.
