Proceedings of the 43rd Annual Meeting of the ACL, pages 338?345,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsA Dynamic Bayesian Framework to Model Context and Memory in EditDistance Learning: An Application to Pronunciation ClassificationKarim Filali and Jeff Bilmes?Departments of Computer Science & Engineering and Electrical EngineeringUniversity of WashingtonSeattle, WA 98195, USA{karim@cs,bilmes@ee}.washington.eduAbstractSitting at the intersection between statis-tics and machine learning, DynamicBayesian Networks have been appliedwith much success in many domains, suchas speech recognition, vision, and compu-tational biology.
While Natural LanguageProcessing increasingly relies on statisti-cal methods, we think they have yet touse Graphical Models to their full poten-tial.
In this paper, we report on experi-ments in learning edit distance costs usingDynamic Bayesian Networks and presentresults on a pronunciation classificationtask.
By exploiting the ability within theDBN framework to rapidly explore a largemodel space, we obtain a 40% reduc-tion in error rate compared to a previoustransducer-based method of learning editdistance.1 IntroductionEdit distance (ED) is a common measure of the sim-ilarity between two strings.
It has a wide rangeof applications in classification, natural languageprocessing, computational biology, and many otherfields.
It has been extended in various ways; forexample, to handle simple (Lowrance and Wagner,1975) or (constrained) block transpositions (Leuschet al, 2003), and other types of block opera-tions (Shapira and Storer, 2003); and to measuresimilarity between graphs (Myers et al, 2000; Klein,1998) or automata (Mohri, 2002).
?This material was supported by NSF under Grant No.
ISS-0326276.Another important development has been the useof data-driven methods for the automatic learning ofedit costs, such as in (Ristad and Yianilos, 1998) inthe case of string edit distance and in (Neuhaus andBunke, 2004) for graph edit distance.In this paper we revisit the problem of learn-ing string edit distance costs within the Graphi-cal Models framework.
We apply our method toa pronunciation classification task and show sig-nificant improvements over the standard Leven-shtein distance (Levenshtein, 1966) and a previoustransducer-based learning algorithm.In section 2, we review a stochastic extension ofthe classic string edit distance.
We present our DBN-based edit distance models in section 3 and show re-sults on a pronunciation classification task in sec-tion 4.
In section 5, we discuss the computationalaspects of using our models.
We end with our con-clusions and future work in section 6.2 Stochastic Models of Edit DistanceLet sm1 = s1s2...sm be a source string over a sourcealphabet A, and m the length of the string.
sji is thesubstring si...sj and sji is equal to the empty string,?, when i > j.
Likewise, tn1 denotes a target stringover a target alhabet B, and n the length of tn1 .A source string can be transformed into a targetstring through a sequence of edit operations.
Wewrite ?s, t?
((s, t) 6= (?, ?))
to denote an edit opera-tion in which the symbol s is replaced by t. If s=?and t 6=?, ?s, t?
is an insertion.
If s 6=?
and t=?, ?s, t?is a deletion.
When s 6= ?, t 6= ?
and s 6= t, ?s, t?
is asubstitution.
In all other cases, ?s, t?
is an identity.The string edit distance, d(sm1 , tn1 ) between sm1and tn1 is defined as the minimum weighted sum of338the number of deletions, insertions, and substitutionsrequired to transform sm1 into tn1 (Wagner and Fis-cher, 1974).
A O(m ?
n) Dynamic Programming(DP) algorithm exists to compute the ED betweentwo strings.
The algorithm is based on the followingrecursion:d(si1, tj1) = min??
?d(si?11 , tj1) + ?
(?si, ??
),d(si1, tj?11 ) + ?(?
?, tj?
),d(si?11 , tj?11 ) + ?
(?si, tj?)??
?with d(?, ?
)=0 and ?
: {?s, t?|(s, t) 6= (?, ?
)}?<+a cost function.
When ?
maps non-identity edit op-erations to unity and identities to zero, string ED isoften referred to as the Levenshtein distance.To learn the edit distance costs from data, Ristadand Yianilos (1998) use a generative model (hence-forth referred to as the RY model) based on a mem-oryless transducer of string pairs.
Below we sum-marize their main idea and introduce our notation,which will be useful later on.We are interested in modeling the joint probabilityP (Sm1=sm1 , Tn1=tn1 | ?)
of observing the source/targetstring pair (sm1 , tn1 ) given model parameters ?.
Si(resp.
Ti), 1?i?m, is a random variable (RV) as-sociated with the event of observing a source (resp.target) symbol at position i.1To model the edit operations, we introduce a hid-den RV, Z, that takes values in (A ?
?
?
B ?
?)
\{(?, ?)}.
Z can be thought of as a random vectorwith two components, Z(s) and Z(t).We can then write the joint probabilityP (sm1 , tn1 | ?)
asP (sm1 , tn1 | ?)
=??
{z`1:v(z`1)=<sm1 ,tn1>, max(m,n)?`?m+n}P (Z`1=z`1, sm1 , tn1 | ?)
(1)where v(z`1) is the yield of the sequence z`1: thestring pair output by the transducer.Equation 1 says that the probability of a par-ticular pair of strings is equal to the sum of theprobabilities of all possible ways to generate thepair by concatenating the edit operations z1...z`.
Ifwe make the assumption that there is no depen-dence between edit operations, we call our modelmemoryless.
P (Z`1, sm1 , tn1 | ?)
can then be factoredas ?iP (Zi, sm1 , tn1 | ?).
In addition, we call themodel context-independent if we can write Q(zi) =1We follow the convention of using capital letters for ran-dom variables and lowercase letters for instantiations of randomvariables.P (Zi=zi, sm1 , tn1 | ?
), 1<i<`, where zi=?z(s)i , z(t)i ?,in the formQ(zi) ?????????
?f ins(tbi) for z(s)i = ?
; z(t)i = tbifdel(sai) for z(s)i = sai ; z(t)i = ?fsub(sai , tbi) for (z(s)i , z(t)i )= (sai , tbi)0 otherwise(2)where?z Q(z) = 1; ai =?i?1j=1 1{z(s)j 6=?}(resp.
bi)is the index of the source (resp.
target) string gen-erated up to the ith edit operation; and f ins,fdel,andfsub are functions mapping to [0, 1].2 Context in-dependence is not to be taken here to mean Zidoes not depend on sai or tbi .
It depends on themthrough the global context which forces Z`1 to gen-erate (sm1 , tn1 ).
The RY model is memoryless andcontext-independent (MCI).Equation 2, also implicitly enforces the consis-tency constraint that the pair of symbols output,(z(s)i , z(t)i ), agrees with the actual pair of symbols,(sai , tbi), that needs to be generated at step i in or-der for the total yield, v(z`1), to equal the string pair.The RY stochastic model is similar to the one in-troduced earlier by Bahl and Jelinek (1975).
Thedifference is that the Bahl model is memorylessand context-dependent (MCD); the f functions arenow indexed by sai (or tai , or both) such that?z Qsai (z) = 1 ?sai .
In general, context depen-dence can be extended to include up to the wholesource (and/or target) string, sai?11 , sai , smai+1.
Sev-eral other types of dependence can be exploited aswill be discussed in section 3.Both the Ristad and the Bahl transducer mod-els give exponentially smaller probability to longerstrings and edit sequences.
Ristad presents an al-ternate explicit model of the joint probability ofthe length of the source and target strings.
In thisparametrization the probability of the length of anedit sequence does not necessarily decrease geomet-rically.
A similar effect can be achieved by modelingthe length of the hidden edit sequence explicitly (seesection 3).3 DBNs for Learning Edit DistanceDynamic Bayesian Networks (DBNs), of whichHidden Markov Models (HMMs) are the most fa-2By convention, sai = ?
for ai > m. Likewise, tbi = ?
ifbi > n. f ins(?)
= fdel(?)
= fsub(?, ?)
= 0.
This takes careof the case when we are past the end of a string.339mous representative, are well suited for modelingstochastic temporal processes such as speech andneural signals.
DBNs belong to the larger family ofGraphical Models (GMs).
In this paper, we restrictourselves to the class of DBNs and use the termsDBN and GM interchangeably.
For an example inwhich Markov Random Fields are used to computea context-sensitive edit distance see (Wei, 2004).3There is a large body of literature on DBNs andalgorithms associated with them.
To briefly de-fine a graphical model, it is a way of representinga (factored) probability distribution using a graph.Nodes of the graph correspond to random variables;and edges to dependence relations between the vari-ables.4 To do inference or parameter learning us-ing DBNs, various generic exact or approximatealgorithms exist (Lauritzen, 1996; Murphy, 2002;Bilmes and Bartels, 2003).
In this section we startby introducing a graphical model for the MCI trans-ducer then present four additional classes of DBNmodels: context-dependent, memory (where an editoperation can depend on past operations), direct(HMM-like), and length models (in which we ex-plicitly model the length of the sequence of editsto avoid the exponential decrease in likelihood oflonger sequences).
A few other models are dis-cussed in section 4.2.3.1 Memoryless Context-independent ModelFig.
1 shows a DBN representation of the memo-ryless context-independent transducer model (sec-tion 2).
The graph represents a template which con-sists, in general, of three parts: a prologue, a chunk,and an epilogue.
The chunk is repeated as manytimes as necessary to model sequences of arbitrarylength.
The product of unrolling the template is aBayesian Network organized into a given number offrames.
The prologue and the epilogue often differfrom the chunk because they model boundary con-ditions, such as ensuring that the end of both stringsis reached at or before the last frame.Associated with each node is a probability func-tion that maps the node?s parent values to the valuesthe node can take.
We will refer to that function as a3While the Markov Edit Distance introduced in the papertakes local statistical dependencies into account, the edit costsare still fixed and not corpus-driven.4The concept of d-separation is useful to read independencerelations encoded by the graph (Lauritzen, 1996).Figure 1: DBN for the memory-less transducermodel.
Unshaded nodes are hidden nodes with prob-abilistic dependencies with respect to their parents.Nodes with stripes are deterministic hidden nodes,i.e., they take a unique value for each configurationof their parents.
Filled nodes are observed (they canbe either stochastic or deterministic).
The graphtemplate is divided into three frames.
The centerframe is repeated m+n?
2 times to yield a graphwith a total of m+n frames, the maximum numberof edit operations needed to transform sm1 into tn1 .Outgoing light edges mean the parent is a switch-ing variable with respect to the child: depending onthe value of the switching RV, the child uses differentCPTs and/or a different parent set.conditional probability table (CPT).Common to all the frames in fig.
1, are positionRVs, a and b, which encode the current positions inthe source and target strings resp.
; source and targetsymbols, s and t; the hidden edit operation, Z; andconsistency nodes sc and tc, which enforce the con-sistency constraint discussed in section 2.
Becauseof symmetry we will explain the upper half of thegraph involving the source string unless the targethalf is different.
We drop subscripts when the framenumber is clear from the context.In the first frame, a and b are observed to havevalue 1, the first position in both strings.
a and bdetermine the value of the symbols s and t. Z takesa random value ?z(s), z(t)?.
sc has the fixed observedvalue 1.
The only configurations of its parents, Zand s, that satisfy P (sc=1|s, z) > 0 are such that(Z(s) = s) or (Z(s) = ?
and Z 6= ?
?, ??).
This is theconsistency constraint in equation 2.In the following frame, the position RV a2 de-pends on a1 and Z1.
If Z1 is an insertion (i.e.Z(s)1 = ?
: the source symbol in the first frame is340not output), then a2 retains the same value as a1;otherwise a2 is incremented by 1 to point to the nextsymbol in the source string.The end RV is an indicator of when we are pastthe end of both source and target strings (a>m andb > n).
end is also a switching parent of Z; whenend = 0, the CPT of Z is the same as describedabove: a distribution over edit operations.
Whenend = 1, Z takes, with probability 1, a fixed valueoutside the range of edit operations but consistentwith s and t. This ensures 1) no ?null?
state (?
?, ??
)is required to fill in the value of Z until the endof the graph is reached; our likelihoods and modelparameters therefore do not become dependent onthe amount of ?null?
padding; and 2) no probabilitymass is taken from the other states of Z as is the casewith the special termination symbol # in the originalRY model.
We found empirically that the use of ei-ther a null or an end state hurts performance to asmall but significant degree.In the last frame, two new nodes make their ap-pearance.
send and tend ensure we are at or pastthe end of the two strings (the RV end only checksthat we are past the end).
That is why send dependson both a and Z.
If a>m, send (observed to be 1) is1 with probability 1.
If a<m, then P (send=1)=0and the whole sequence Z`1 has zero probability.
Ifa=m, then send only gets probability greater thanzero if Z is not an insertion.
This ensures the lastsource symbol is indeed consumed.Note that we can obtain the equivalent of the to-tal edit distance cost by using Viterbi inference andadding a costi variable as a deterministic child of therandom variable Zi : in each frame the cost is equalto costi?1 plus 0 when Zi is an identity, or plus 1otherwise.3.2 Context-dependent ModelAdding context dependence in the DBN frameworkis quite natural.
In fig.
2, we add edges from si,sprevi, and snexti to Zi.
The sc node is no longerrequired because we can enforce the consistencyconstraint via the CPT of Z given its parents.
snextiis an RV whose value is set to the symbol at the ai+1position of the string, i.e., snexti=sai+1.
Likewise,sprevi = sai?1.
The Bahl model (1975) uses a de-pendency on si only.
Note that si?1 is not necessar-ily equal to sai?1.
Conditioning on si?1 induces anFigure 2: Context-dependent model.indirect dependence on whether there was an inser-tion in the previous step because si?1 = si might becorrelated with the event Z(s)i?1 = ?.3.3 Memory ModelMemory models are another easy extension of thebasic model as fig.
3 shows.
Depending on whetherthe variable Hi?1 linking Zi?1 to Zi is stochasticor deterministic, there are several models that canbe implemented; for example, a latent factor mem-ory model when H is stochastic.
The cardinality ofH determines how much the information from oneframe to the other is ?summarized.?
With a deter-ministic implementation, we can, for example, spec-ify the usual P (Zi|Zi?1) memory model when H isa simple copy of Z or have Zi depend on the type ofedit operation in the previous frame.Figure 3: Memory model.
Depending on the type ofdependency between Zi and Hi, the model can belatent variable based or it can implement a deter-ministic dependency on a function of Zi3.4 Direct ModelThe direct model in fig.
4 is patterned on the clas-sic HMM, where the unrolled length of graph is thesame as the length of the sequence of observations.The key feature of this model is that we are required341to consume a target symbol per frame.
To achievethat, we introduce two RVs, ins, with cardinality2, and del, with cardinality at most m. The depen-dency of del on ins is to ensure the two events neverhappen concomitantly.
At each frame, a is incre-mented either by the value of del in the case of a(possibly block) deletion or by zero or one depend-ing on whether there was an insertion in the previousframe.
An insertion also forces s to take value ?.Figure 4: Direct model.In essence the direct model is not very differ-ent from the context-dependent model in that heretoo we learn the conditional probabilities P (ti|si)(which are implicit in the CD model).3.5 Length ModelWhile this model (fig.
5) is more complex thanthe previous ones, much of the network structureis ?control logic?
necessary to simulate variablelength-unrolling of the graph template.
The key ideais that we have a new stochastic hidden RV, inclen,whose value added to that of the RV inilen deter-mines the number of edit operations we are allowed.A counter variable, counter is used to keep trackof the frame number and when the required num-ber is reached, the RV atReqLen is triggered.
If atthat point we have just reached the end of one of thestrings while the end of the other one is reached inthis frame or a previous one, then the variable endis explained (it has positive probability).
Otherwise,the entire sequence of edit operations up to that pointhas zero probability.4 Pronunciation ClassificationIn pronunciation classification we are given a lexi-con, which consists of words and their correspond-ing canonical pronunciations.
We are also providedwith surface pronunciations and asked to find themost likely corresponding words.
Formally, for eachFigure 5: Length unrolling model.surface form, tn1 , we need to find the set of wordsW?
s.t.
W?
= argmaxwP (w|tn1 ).
There are severalways we could model the probability P (w|tn1 ).
Oneway is to assume a generative model whereby a wordw and a surface pronunciation tn1 are related via anunderlying canonical pronunciation sm1 of w and astochastic process that explains the transformationfrom sm1 to tn1 .
This is summarized in equation 3.C(w) denotes the set of canonical pronunciations ofw.W?
= argmaxw?sm1 ?C(w)P (w|sm1 )P (sm1 , tn1 ) (3)If we assume uniform probabilities P (w|sm1 )(sm1 ?C(w)) and use the max approximation in placeof the sum in eq.
3 our classification rule becomesW?
= {w|S?
?C(w) 6=?, S?=argmaxsm1P (sm1 , tn1 )} (4)It is straightforward to create a DBN to model thejoint probability P (w, sm1 , tn1 ) by adding a word RVand a canonical pronunciation RV on top of any ofthe previous models.There are other pronunciation classification ap-proaches with various emphases.
For example,Rentzepopoulos and Kokkinakis (1996) use HMMsto convert phoneme sequences to their most likelyorthographic forms in the absence of a lexicon.4.1 DataWe use Switchboard data (Godfrey et al, 1992) thathas been hand annotated in the context of the SpeechTranscription Project (STP) described in (Green-berg et al, 1996).
Switchboard consists of spon-taneous informal conversations recorded over the342phone.
Because of the informal non-scripted natureof the speech and the variety of speakers, the cor-pus presents much variety in word pronunciations,which can significantly deviate from the prototypicalpronunciations found in a lexicon.
Another sourceof pronunciation variability is the noise introducedduring the annotation of speech segments.
Evenwhen the phone labels are mostly accurate, the startand end time information is not as precise and it af-fects how boundary phones get algned to the wordsequence.
As a reference pronunciation dictionarywe use a lexicon of the 2002 Switchboard speechrecognition evaluation.
The lexicon contains 40000entries, but we report results on a reduced dictio-nary5 with 5000 entries corresponding to only thosewords that appear in our train and test sets.
Ristadand Yianilos use a few additional lexicons, some ofwhich are corpus-derived.
We did reproduce theirresults on the different types of lexicons.For testing we randomly divided STP data into9495 training words (corresponding to 9545 pronun-ciations) and 912 test words (901 pronunciations).For the Levenshtein and MCI results only, we per-formed ten-fold cross validation to verify we did notpick a non-representative test set.
Our models areimplemented using GMTK, a general-purpose DBNtool originally created to explore different speechrecognition models (Bilmes and Zweig, 2002).
Asa sanity check, we also implemented the MCI modelin C following RY?s algorithm.The error rate is computed by calculating, for eachpronunciation form, the fraction of words that arecorrectly hypothesized and averaging over the testset.
For example if the classifier returns five wordsfor a given pronunciation, and two of the words arecorrect, the error rate is 3/5*100%.Three EM iterations are used for training.
Addi-tional iterations overtrained our models.4.2 ResultsTable 1 summarizes our results using DBN basedmodels.
The basic MCI model does marginally bet-ter than the Levenshtein edit distance.
This is con-sistent with the finding in RY: their gains come fromthe joint learning of the probabilities P (w|sm1 ) andP (sm1 , tn1 ).
Specifically, the word model accountsfor much of their gains over the Levenshtein dis-5Equivalent to the E2 lexicon in RY.tance.
We use uniform priors and the simple classi-fication rule in eq.
4.
We feel it is more compellingthat we are able to significantly improve upon stan-dard edit distance and the MCI model without usingany lexicon or word model.Memory Models Performance improves with theaddition of a direct dependence of Zi on Zi?1.
Thebiggest improvement (27.65% ER) however comesfrom conditioning on Z(t)i?1, the target symbol thatis hypothesized in the previous step.
There was nogain when conditioning on the type of edit operationin the previous frame.Context Models Interestingly, the exact oppositefrom the memory models is happening here whenwe condition on the source context (versus condi-tioning on the target context).
Conditioning on sigets us to 21.70%.
With si, si?1 we can further re-duce the error rate to 20.26%.
However, when weadd a third dependency, the error rate worsens to29.32%, which indicates a number of parameters toohigh for the given amount of training data.
Backoff,interpolation, or state clustering might all be appro-priate strategies here.Position Models Because in the previous mod-els, when conditioning on the past, boundary condi-tions dictate that we use a different CPT in the firstframe, it is fair to wonder whether part of the gainwe witness is due to the implicit dependence on thesource-target string position.
The (small) improve-ment due to conditioning on bi indicates there is suchdependence.
Also, the fact that the target position ismore informative than the source one is likely due tothe misalignments we observed in the phoneticallytranscribed corpus, whereby the first or last phoneswould incorrectly be aligned with the previous ornext word resp.
I.e., the model might be learningto not put much faith in the start and end positionsof the target string, and thus it boosts deletion andinsertion probabilities at those positions.
We havealso conditioned on coarser-grained positions (be-ginning, middle, and end of string) but obtained thesame results as with the fine-grained dependency.Length Models Modeling length helps to a smallextent when it is added to the MCI and MCD mod-els.
Belying the assumption motivating this model,we found that the distribution over the RV inclen(which controls how much the edit sequence extends343beyond the length of the source string) is skewed to-wards small values of inclen.
This indicates on thatinsertions are rare when the source string is longerthan the target one and vice-versa for deletions.Direct Model The low error rate obtained by thismodel reflects its similarity to the context-dependentmodel.
From the two sets of results, it is clear thatsource string context plays a crucial role in predict-ing canonical pronunciations from corpus ones.
Wewould expect additional gains from modeling con-text dependencies across time here as well.Model Zi Dependencies % Err rateLev none 35.97Baseline none 35.55MemoryZi?1 30.05editOperationType(Zi?1) 36.16stochastic binary Hi?1 33.87Z(s)i?1 29.62Z(t)i?1 27.65Contextsi 21.70ti 32.06si, si?1 20.26ti, ti?1 28.21si, si?1, sai+1 29.32si, sai+1 (sai?1 in last frame) 23.14si, sai?1 (sai+1 in first frame) 23.15Positionai 33.80bi 31.06ai, bi 34.17Mixed bi,si 22.22Z(t)i?1,si 24.26Length none 33.56si 20.03Direct none 23.70Table 1: DBN based model results summary.When we combine the best position-dependentor memory models with the context-dependent one,the error rate decreases (from 31.31% to 25.25%when conditioning on bi and si; and from 28.28%to 25.75% when conditioning on z(t)i?1 and si) but notto the extent conditioning on si alone decreases errorrate.
Not shown in table 1, we also tried several othermodels, which although they are able to producereasonable alignments (in the sense that the Leven-shtein distance would result in similar alignments)between two given strings, they have extremely poordiscriminative ability and result in error rates higherthan 90%.
One such example is a model in whichZi depends on both si and ti.
It is easy to see wherethe problem lies with this model once one considersthat two very different strings might still get a higherlikelihood than more similar pair because, given sand t s.t.
s 6= t, the probability of identity is obvi-ously zero and that of insertion or deletion can bequite high; and when s = t, the probability of in-sertion (or deletion) is still positive.
We observe thesame non-discriminative behavior when we replace,in the MCI model, Zi with a hidden RV Xi, whereXi takes as values one of the four edit operations.5 Computational ConsiderationsThe computational complexity of inference in agraphical model is related to the state space of thelargest clique (maximal complete subgraph) in thegraph.
In general, finding the smallest such clique isNP-complete (Arnborg et al, 1987).In the case of the MCI model, however, it is notdifficult to show that the smallest such clique con-tains all the RVs within a frame and the complex-ity of doing inference is order O(mn ?max(m,n)).The reason there is a complexity gap is that thesource and target position variables are indexed bythe frame number and we do not exploit the factthat even though we arrive at a given source-targetposition pair along different edit sequence paths atdifferent frames, the position pair is really the sameregardless of its frame index.
We are investigatinggeneric ways of exploiting this constraint.In practice, however, state space pruning can sig-nificantly reduce the running time of DBN infer-ence.
Ukkonen (1985) reduces the complexity of theclassic edit distance to O(d?max(m,n)), where d isthe edit distance.
The intuition there is that, assum-ing a small edit distance, the most likely alignmentsare such that the source position does not diverge toomuch from the target position.
The same intuitionholds in our case: if the source and the target posi-tion do not get too far out of sync, then at each step,only a small fraction of the m ?
n possible source-target position configurations need be considered.The direct model, for example, is quite fast inpractice because we can restrict the cardinality of thedel RV to a constant c (i.e.
we disallow long-spandeletions, which for certain applications is a reason-able restriction) and make inference linear in n witha running time constant proportional to c2.3446 ConclusionWe have shown how the problem of learning editdistance costs from data can be modeled quitenaturally using Dynamic Bayesian Networks eventhough the problem lacks the temporal or order con-straints that other problems such as speech recog-nition exhibit.
This gives us confidence that otherimportant problems such as machine translation canbenefit from a Graphical Models perspective.
Ma-chine translation presents a fresh set of challengesbecause of the large combinatorial space of possiblealignments between the source string and the target.There are several extensions to this work that weintend to implement or have already obtained pre-liminary results on.
One is simple and block trans-position.
Another natural extension is modeling editdistance of multiple strings.It is also evident from the large number of depen-dency structures that were explored that our learn-ing algorithm would benefit from a structure learn-ing procedure.
Maximum likelihood optimizationmight, however, not be appropriate in this case, asexemplified by the failure of some models to dis-criminate between different pronunciations.
Dis-criminative methods have been used with significantsuccess in training HMMs.
Edit distance learningcould benefit from similar methods.ReferencesS.
Arnborg, D. G. Corneil, and A. Proskurowski.
1987.Complexity of finding embeddings in a k-tree.
SIAMJ.
Algebraic Discrete Methods, 8(2):277?284.L.
R. Bahl and F. Jelinek.
1975.
Decoding for channelswith insertions, deletions, and substitutions with appli-cations to speech recognition.
Trans.
on InformationTheory, 21:404?411.J.
Bilmes and C. Bartels.
2003.
On triangulating dy-namic graphical models.
In Uncertainty in Artifi-cial Intelligence: Proceedings of the 19th Conference,pages 47?56.
Morgan Kaufmann.J.
Bilmes and G. Zweig.
2002.
The Graphical ModelsToolkit: An open source software system for speechand time-series processing.
Proc.
IEEE Intl.
Conf.
onAcoustics, Speech, and Signal Processing.J.
J. Godfrey, E. C. Holliman, and J. McDaniel.
1992.SWITCHBOARD: Telephone speech corpus for re-search and development.
In ICASSP, volume 1, pages517?520.S.
Greenberg, J. Hollenback, and D. Ellis.
1996.
Insightsinto spoken language gleaned from phonetic transcrip-tion of the switchboard corpus.
In ICSLP, pages S24?27.P.
N. Klein.
1998.
Computing the edit-distance betweenunrooted ordered trees.
In Proceedings of 6th AnnualEuropean Symposium, number 1461, pages 91?102.S.L.
Lauritzen.
1996.
Graphical Models.
Oxford Sci-ence Publications.G.
Leusch, N. Ueffing, and H. Ney.
2003.
A novelstring-to-string distance measure with applications tomachine translation evaluation.
In Machine Transla-tion Summit IX, pages 240?247.V.
Levenshtein.
1966.
Binary codes capable of cor-recting deletions, insertions and reversals.
Sov.
Phys.Dokl., 10:707?710.R.
Lowrance and R. A. Wagner.
1975.
An extensionto the string-to-string correction problem.
J. ACM,22(2):177?183.M.
Mohri.
2002.
Edit-distance of weighted automata.In CIAA, volume 2608 of Lecture Notes in ComputerScience, pages 1?23.
Springer.K.
Murphy.
2002.
Dynamic Bayesian Networks: Repre-sentation, Inference and Learning.
Ph.D. thesis, U.C.Berkeley, Dept.
of EECS, CS Division.R.
Myers, R.C.
Wison, and E.R.
Hancock.
2000.Bayesian graph edit distance.
IEEE Trans.
on PatternAnalysis and Machine Intelligence, 22:628?635.M.
Neuhaus and H. Bunke.
2004.
A probabilistic ap-proach to learning costs for graph edit distance.
InICPR, volume 3, pages 389?393.P.
A. Rentzepopoulos and G. K. Kokkinakis.
1996.
Ef-ficient multilingual phoneme-to-grapheme conversionbased on hmm.
Comput.
Linguist., 22(3):351?376.E.
S. Ristad and P. N. Yianilos.
1998.
Learning stringedit distance.
Trans.
on Pattern Recognition and Ma-chine Intelligence, 20(5):522?532.D.
Shapira and J.
A. Storer.
2003.
Large edit distancewith multiple block operations.
In SPIRE, volume2857 of Lecture Notes in Computer Science, pages369?377.
Springer.E.
Ukkonen.
1985.
Algorithms for approximate stringmatching.
Inf.
Control, 64(1-3):100?118.R.
A. Wagner and M. J. Fischer.
1974.
The string-to-string correction problem.
J. ACM, 21(1):168?173.J.
Wei.
2004.
Markov edit distance.
Trans.
on PatternAnalysis and Machine Intelligence, 26(3):311?321.345
