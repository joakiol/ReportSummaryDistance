Classifying Semantic Relationsin Bioscience TextsBarbara RosarioSIMSUC BerkeleyBerkeley, CA 94720rosario@sims.berkeley.eduMarti A. HearstSIMSUC BerkeleyBerkeley, CA 94720hearst@sims.berkeley.eduAbstractA crucial step toward the goal of au-tomatic extraction of propositional in-formation from natural language text isthe identification of semantic relationsbetween constituents in sentences.
Weexamine the problem of distinguishingamong seven relation types that can oc-cur between the entities ?treatment?
and?disease?
in bioscience text, and theproblem of identifying such entities.
Wecompare five generative graphical mod-els and a neural network, using lexical,syntactic, and semantic features, findingthat the latter help achieve high classifi-cation accuracy.1 IntroductionThe biosciences literature is rich, complex andcontinually growing.
The National Library ofMedicine?s MEDLINE database1 contains bibli-ographic citations and abstracts from more than4,600 biomedical journals, and an estimated half amillion new articles are added every year.
Muchof the important, late-breaking bioscience infor-mation is found only in textual form, and so meth-ods are needed to automatically extract semanticentities and the relations between them from thistext.
For example, in the following sentences, hep-atitis and its variants, which are DISEASES, arefound in different semantic relationships with var-ious TREATMENTs:1http://www.nlm.nih.gov/pubs/factsheets/medline.html(1) Effect of interferon on hepatitis B(2) A two-dose combined hepatitis A and B vac-cine would facilitate immunization programs(3) These results suggest that con A-induced hep-atitis was ameliorated by pretreatment withTJ-135.In (1) there is an unspecified effect of the treat-ment interferon on hepatitis B.
In (2) the vaccineprevents hepatitis A and B while in (3) hepatitisis cured by the treatment TJ-135.We refer to this problem as Relation Classifi-cation.
A related task is Role Extraction (alsocalled, in the literature, ?information extraction?or ?named entity recognition?
), defined as: givena sentence such as ?The fluoroquinolones for uri-nary tract infections: a review?, extract all andonly the strings of text that correspond to the rolesTREATMENT (fluoroquinolones) and DISEASE(urinary tract infections).
To make inferencesabout the facts in the text we need a system thataccomplishes both these tasks: the extraction ofthe semantic roles and the recognition of the rela-tionship that holds between them.In this paper we compare five generative graph-ical models and a discriminative model (a multi-layer neural network) on these tasks.
Recogniz-ing subtle differences among relations is a diffi-cult task; nevertheless the results achieved by ourmodels are quite promising: when the roles are notgiven, the neural network achieves 79.6% accu-racy and the best graphical model achieves 74.9%.When the roles are given, the neural net reaches96.9% accuracy while the best graphical modelgets 91.6% accuracy.
Part of the reason for theRelationship Definition and ExampleCure TREAT cures DIS810 (648, 162) Intravenous immune globulin forrecurrent spontaneous abortionOnly DIS TREAT not mentioned616 (492, 124) Social ties and susceptibility to thecommon coldOnly TREAT DIS not mentioned166 (132, 34) Flucticasone propionate is safe inrecommended dosesPrevent TREAT prevents the DIS63 (50, 13) Statins for prevention of strokeVague Very unclear relationship36 (28, 8) Phenylbutazone and leukemiaSide Effect DIS is a result of a TREAT29 (24, 5) Malignant mesodermal mixed tu-mor of the uterus following irradi-ationNO Cure TREAT does not cure DIS4 (3, 1) Evidence for double resistance topermethrin and malathion in headliceTotal relevant: 1724 (1377, 347)Irrelevant TREAT and DIS not present1771 (1416, 355) Patients were followed up for 6monthsTotal: 3495 (2793, 702)Table 1: Candidate semantic relationships be-tween treatments and diseases.
In parentheses areshown the numbers of sentences used for trainingand testing, respectively.success of the algorithms is the use of a largedomain-specific lexical hierarchy for generaliza-tion across classes of nouns.In the remainder of this paper we discuss relatedwork, describe the annotated dataset, describe themodels, present and discuss the results of runningthe models on the relation classification and en-tity extraction tasks and analyze the relative im-portance of the features used.2 Related workWhile there is much work on role extraction, verylittle work has been done for relationship recogni-tion.
Moreover, many papers that claim to be do-ing relationship recognition in reality address thetask of role extraction: (usually two) entities areextracted and the relationship is implied by the co-occurrence of these entities or by the presence ofsome linguistic expression.
These linguistic pat-terns could in principle distinguish between differ-ent relations, but instead are usually used to iden-tify examples of one relation.
In the related workfor statistical models there has been, to the best ofour knowledge, no attempt to distinguish betweendifferent relations that can occur between the samesemantic entities.In Agichtein and Gravano (2000) the goal is toextract pairs such as (Microsoft, Redmond), whereRedmond is the location of the organization Mi-crosoft.
Their technique generates and evaluateslexical patterns that are indicative of the relation.Only the relation location of is tackled and the en-tities are assumed given.In Zelenko et al (2002), the task is to ex-tract the relationships person-affiliation andorganization-location.
The classification (donewith Support Vector Machine and Voted Percep-tron algorithms) is between positive and negativesentences, where the positive sentences containthe two entities.In the bioscience NLP literature there arealso efforts to extract entities and relations.
InRay and Craven (2001), Hidden Markov Modelsare applied to MEDLINE text to extract the enti-ties PROTEINS and LOCATIONS in the relation-ship subcellular-location and the entities GENEand DISORDER in the relationship disorder-association.
The authors acknowledge that thetask of extracting relations is different from thetask of extracting entities.
Nevertheless, they con-sider positive examples to be all the sentencesthat simply contain the entities, rather than an-alyzing which relations hold between these enti-ties.
In Craven (1999), the problem tackled is re-lationship extraction from MEDLINE for the re-lation subcellular-location.
The authors treat itas a text classification problem and propose andcompare two classifiers: a Naive Bayes classi-fier and a relational learning algorithm.
Thisis a two-way classification, and again there isno mention of whether the co-occurrence of theentities actually represents the target relation.Pustejovsky et al (2002) use a rule-based systemto extract entities in the inhibit-relation.
Their ex-periments use sentences that contain verbal andnominal forms of the stem inhibit.
Thus the ac-tual task performed is the extraction of entitiesthat are connected by some form of the stem in-hibit, which by requiring occurrence of this wordexplicitly, is not the same as finding all sen-tences that talk about inhibiting actions.
Similarly,Rindflesch et al (1999) identify noun phrases sur-rounding forms of the stem bind which signifyentities that can enter into molecular binding re-lationships.
In Srinivasan and Rindflesch (2002)MeSH term co-occurrences within MEDLINE ar-ticles are used to attempt to infer relationships be-tween different concepts, including diseases anddrugs.In the bioscience domain the work on relationclassification is primary done through hand-builtrules.
Feldman et al (2002) use hand-built rulesthat make use of syntactic and lexical featuresand semantic constraints to find relations betweengenes, proteins, drugs and diseases.
The GENIESsystem (Friedman et al, 2001) uses a hand-builtsemantic grammar along with hand-derived syn-tactic and semantic constraints, and recognizesa wide range of relationships between biologicalmolecules.3 Data and FeaturesFor our experiments, the text was obtained fromMEDLINE 20012.
An annotator with biology ex-pertise considered the titles and abstracts sepa-rately and labeled the sentences (both roles andrelations) based solely on the content of the indi-vidual sentences.
Seven possible types of relation-ships between TREATMENT and DISEASE wereidentified.
Table 1 shows, for each relation, its def-inition, one example sentence and the number ofsentences found containing it.We used a large domain-specific lexical hi-erarchy (MeSH, Medical Subject Headings3) tomap words into semantic categories.
There areabout 19,000 unique terms in MeSH and 15 mainsub-hierarchies, each corresponding to a majorbranch of medical ontology; e.g., tree A corre-sponds to Anatomy, tree C to Disease, and so on.As an example, the word migraine maps to theterm C10.228, that is, C (a disease), C10 (Ner-vous System Diseases), C10.228 (Central Ner-2We used the first 100 titles and the first 40 abstracts fromeach of the 59 files medline01n*.xml in Medline 2001; thelabeled data is available at biotext.berkeley.edu3http://www.nlm.nih.gov/mesh/meshhome.htmlvous System Diseases).
When there are multi-ple MeSH terms for one word, we simply choosethe first one.
These semantic features are shownto be very useful for our tasks (see Section 4.3).Rosario et al (2002) demonstrate the usefulnessof MeSH for the classification of the semantic re-lationships between nouns in noun compounds.The results reported in this paper were obtainedwith the following features: the word itself, its partof speech from the Brill tagger (Brill, 1995), thephrase constituent the word belongs to, obtainedby flattening the output of a parser (Collins, 1996),and the word?s MeSH ID (if available).
In addi-tion, we identified the sub-hierarchies of MeSHthat tend to correspond to treatments and diseases,and convert these into a tri-valued attribute indi-cating one of: disease, treatment or neither.
Fi-nally, we included orthographic features such as?is the word a number?, ?only part of the word is anumber?, ?first letter is capitalized?, ?all letters arecapitalized?.
In Section 4.3 we analyze the impactof these features.4 Models and ResultsThis section describes the models and their perfor-mance on both entity extraction and relation clas-sification.
Generative models learn the prior prob-ability of the class and the probability of the fea-tures given the class; they are the natural choicein cases with hidden variables (partially observedor missing data).
Since labeled data is expensiveto collect, these models may be useful when nolabels are available.
However, in this paper wetest the generative models on fully observed dataand show that, although not as accurate as the dis-criminative model, their performance is promisingenough to encourage their use for the case of par-tially observed data.Discriminative models learn the probability ofthe class given the features.
When we have fullyobserved data and we just need to learn the map-ping from features to classes (classification), a dis-criminative approach may be more appropriate,as shown in Ng and Jordan (2002), but has othershortcomings as discussed below.For the evaluation of the role extraction task, wecalculate the usual metrics of precision, recall andF-measure.
Precision is a measure of how many ofthe roles extracted by the system are correct andrecall is the measure of how many of the true roleswere extracted by the system.
The F-measure isa weighted combination of precision and recall4.Our role evaluation is very strict: every token is as-sessed and we do not assign partial credit for con-stituents for which only some of the words are cor-rectly labeled.
We report results for two cases: (i)considering only the relevant sentences and (ii) in-cluding also irrelevant sentences.
For the relationclassification task, we report results in terms ofclassification accuracy, choosing one out of sevenchoices for (i) and one out of eight choices for (ii).
(Most papers report the results for only the rele-vant sentences, while some papers assign credit totheir algorithms if their system extracts only oneinstance of a given relation from the collection.
Bycontrast, in our experiments we expect the systemto extract all instances of every relation type.)
Forboth tasks, 75% of the data were used for trainingand the rest for testing.4.1 Generative ModelsIn Figure 1 we show two static and three dynamicmodels.
The nodes labeled ?Role?
represent theentities (in this case the choices are DISEASE,TREATMENT and NULL) and the node labeled?Relation?
represents the relationship present inthe sentence.
We assume here that there is a singlerelation for each sentence between the entities5.The children of the role nodes are the words andtheir features, thus there are as many role states asthere are words in the sentence; for the static mod-els, this is depicted by the box (or ?plate?)
whichis the standard graphical model notation for repli-cation.
For each state, the features are thosementioned in Section 3.The simpler static models S1 and S2 do notassume an ordering in the role sequence.
Thedynamic models were inspired by prior work onHMM-like graphical models for role extraction(Bikel et al, 1999; Freitag and McCallum, 2000;Ray and Craven, 2001).
These models consist of a4In this paper, precision and recall are given equal weight,that is, F-measure = 	 .5We found 75 sentences which contain more than one re-lationship, often with multiple entities or the same entitiestaking part in several interconnected relationships; we did notinclude these in the study.f1Rolef2fn.
.
.RelationTf1Rolef2fn.
.
.RelationTffstatic model (S1) static model (S2)f1Rolef2fn.
.
.f1Rolef2fn.
.
.f1Rolef2fn.
.
.Relationdynamic model (D1)f1Rolef2fn.
.
.f1Rolef2fn.
.
.f1Rolef2fn.
.
.Relationdynamic model (D2)f1Rolef2fn.
.
.f1Rolef2fn.
.
.f1Rolef2fn.
.
.Relationdynamic model (D3)Figure 1: Models for role and relation extraction.Markov sequence of states (usually correspondingto semantic roles) where each state generates oneor multiple observations.
Model D1 in Figure 1 istypical of these models, but we have augmented itwith the Relation node.The task is to recover the sequence of Rolestates, given the observed features.
These mod-els assume that there is an ordering in the seman-tic roles that can be captured with the Markov as-sumption and that the role generates the observa-tions (the words, for example).
All our modelsmake the additional assumption that there is a re-lation that generates the role sequence; thus, theseSentences Static DynamicS1 S2 D1 D2 D3No SmoothingOnly rel.
0.67 0.68 0.71 0.52 0.55Rel.
+ irrel.
0.61 0.62 0.66 0.35 0.37Absolute discountingOnly rel.
0.67 0.68 0.72 0.73 0.73Rel.
+ irrel.
0.60 0.62 0.67 0.71 0.69Table 2: F-measures for the models of Figure 1 forrole extraction.models have the appealing property that they cansimultaneously perform role extraction and rela-tionship recognition, given the sequence of obser-vations.
In S1 and D1 the observations are inde-pendent from the relation (given the roles).
In S2and D2, the observations are dependent on boththe relation and the role (or in other words, the re-lation generates not only the sequence of roles butalso the observations).
D2 encodes the fact thateven when the roles are given, the observations de-pend on the relation.
For example, sentences con-taining the word prevent are more likely to repre-sent a ?prevent?
kind of relationship.
Finally, inD3 only one observation per state is dependent onboth the relation and the role, the motivation beingthat some observations (such as the words) dependon the relation while others might not (like for ex-ample, the parts of speech).
In the experimentsreported here, the observations which have edgesfrom both the role and the relation nodes are thewords.
(We ran an experiment in which this obser-vation node was the MeSH term, obtaining similarresults.
)Model D1 defines the following joint probabil-ity distribution over relations, roles, words andword features, assuming the leftmost Role node isfiffifl "!$#, and % is the number of words in the sen-tence:&')(*+,(-'/.*10202*+,$('/34*6587+.9*9020:*65);<.
)*602020:*6587=3>*60202*65);<3	?&')(&,('.A@)'9(;BCED7&5C.@),('.$ (1)3BFD7&A,$('F@)A,$('F+G71*+'9(;BCHD7&5CF@)A,$('FModel D1 is similar to the modelin Thompson et al (2003) for the extractionof roles, using a different domain.
Structurally,the differences are (i) Thompson et al (2003) hasonly one observation node per role and (ii) it hasan additional node ?on top?, with an edge to therelation node, to represent a predicator ?triggerword?
which is always observed; the predicatorwords are taken from a fixed list and one must bepresent in order for a sentence to be analyzed.The joint probability distributions for D2and D3 are similar to Equation (1) wherewe substitute the term IJ<KMLONP JHQSRfiTfl U!Q9Vwith IJ<KML NP JHQSRfiTflW"!Q9XfiT!YV for D2 andNP L"Q/Rfiffifl "!Q6Xfiffi!YVIJ<K	ZNP JHQ/Rfiffifl "!Q6V for D3.The parameters NP JHQ Rfiffifl "!Q V and NP J#Rfiffifl "!
#Vof Equation (1) are constrained to be equal.The parameters were estimated using maximumlikelihood on the training set; we also imple-mented a simple absolute discounting smoothingmethod (Zhai and Lafferty, 2001) that improvesthe results for both tasks.Table 2 shows the results (F-measures) for theproblem of finding the most likely sequence ofroles given the features observed.
In this case, therelation is hidden and we marginalize over it6.
Weexperimented with different values for the smooth-ing factor ranging from a minimum of 0.0000005to a maximum of 10; the results shown fix thesmoothing factor at its minimum value.
We foundthat for the dynamic models, for a wide rangeof smoothing factors, we achieved almost identi-cal results; nevertheless, in future work, we planto implement cross-validation to find the optimalsmoothing factor.
By contrast, the static modelswere more sensitive to the value of the smoothingfactor.Using maximum likelihood with no smoothing,model D1 performs better than D2 and D3.
Thiswas expected, since the parameters for models D2and D3 are more sparse than D1.
However, whensmoothing is applied, the three dynamic modelsachieve similar results.
Although the additionaledges in models D2 and D3 did not help muchfor the task of role extraction, they did help forrelation classification, discussed next.
Model D26To perform inference for the dynamic model, weused the junction tree algorithm.
We used Kevin Mur-phy?s BNT package, found at http://www.ai.mit.edu/ mur-phyk/Bayes/bnintro.html.achieves the best F-measures: 0.73 for ?only rele-vant?
and 0.71 for ?rel.
+ irrel.
?.It is difficult to compare results with the relatedwork since the data, the semantic roles and theevaluation are different; in Ray and Craven (2001)however, the role extraction task is quite similar toours and the text is also from MEDLINE.
They re-port approximately an F-measure of 32% for theextraction of the entities PROTEINS and LOCA-TIONS, and an F-measure of 50% for GENE andDISORDER.The second target task is to find the most likelyrelation, i.e., to classify a sentence into one of thepossible relations.
Two types of experiments wereconducted.
In the first, the true roles are hiddenand we classify the relations given only the ob-servable features, marginalizing over the hiddenroles.
In the second, the roles are given and onlythe relations need to be inferred.
Table 3 reportsthe results for both conditions, both with absolutediscounting smoothing and without.Again model D1 outperforms the other dy-namic models when no smoothing is applied; withsmoothing and when the true roles are hidden, D2achieves the best classification accuracies.
Whenthe roles are given D1 is the best model; D1 doeswell in the cases when both roles are not present.By contrast, D2 does better than D1 when the pres-ence of specific words strongly determines the out-come (e.g., the presence ?prevention?
or ?prevent?helps identify the Prevent relation).The percentage improvements of D2 and D3versus D1 are, respectively, 10% and 6.5% for re-lation classification and 1.4% for role extraction(in the ?only relevant?, ?only features?
case).
Thissuggests that there is a dependency between theobservations and the relation that is captured bythe additional edges in D2 and D3, but that thisdependency is more helpful in relation classifica-tion than in role extraction.For relation classification the static models per-form worse than for role extraction; the decreasesin performance from D1 to S1 and from D2 to S2are, respectively (in the ?only relevant?, ?only fea-tures?
case), 7.4% and 7.3% for role extraction and27.1% and 44% for relation classification.
Thissuggests the importance of modeling the sequenceof roles for relation classification.To provide an idea of where the errors occur,Table 4 shows the confusion matrix for model D2for the most realistic and difficult case of ?rel + ir-rel.
?, ?only features?.
This indicates that the algo-rithm performs poorly primarily for the cases forwhich there is little training data, with the excep-tion of the ONLY DISEASE case, which is oftenmistaken for CURE.4.2 Neural NetworkTo compare the results of the generative models ofthe previous section with a discriminative method,we use a neural network, using the Matlab pack-age to train a feed-forward network with conjugategradient descent.The features are the same as those used for themodels in Section 4.1, but are represented with in-dicator variables.
That is, for each feature we cal-culated the number of possible values [ and thenrepresented an observation of the feature as a se-quence of [ binary values in which one value is setto \ and the remaining [^]_\ values are set to ` .The input layer of the NN is the concatenationof this representation for all features.
The net-work has one hidden layer, with a hyperbolic tan-gent function.
The output layer uses a logistic sig-moid function.
The number of units of the outputlayer is fixed to be the number of relations (sevenor eight) for the relation classification task andthe number of roles (three) for the role extractiontask.
The network was trained for several choicesof numbers of hidden units; we chose the best-performing networks based on training set error.We then tested these networks on held-out testingdata.The results for the neural network are reportedin Table 3 in the column labeled NN.
These re-sults are quite strong, achieving 79.6% accuracyin the relation classification task when the entitiesare hidden and 96.9% when the entities are given,outperforming the graphical models.
Two possiblereasons for this are: as already mentioned, the dis-criminative approach may be the most appropriatefor fully labeled data; or the graphical models weproposed may not be the right ones, i.e., the inde-pendence assumptions they make may misrepre-sent underlying dependencies.It must be pointed out that the neural networkSentences Input B Static Dynamic NNS1 S2 D1 D2 D3No SmoothingOnly rel.
only feat.
46.7 51.9 50.4 65.4 58.2 61.4 79.8roles given 51.3 52.9 66.6 43.8 49.3 92.5Rel.
+ irrel.
only feat.
50.6 51.2 50.2 68.9 58.7 61.4 79.6roles given 55.7 54.4 82.3 55.2 58.8 96.6Absolute discountingOnly rel.
only feat.
46.7 51.9 50.4 66.0 72.6 70.3roles given 51.9 53.6 83.0 76.6 76.6Rel.
+ irrel.
only feat.
50.6 51.1 50.2 68.9 74.9 74.6roles given 56.1 54.8 91.6 82.0 82.3Table 3: Accuracies of relationship classification for the models in Figure 1 and for the neural network(NN).
For absolute discounting, the smoothing factor was fixed at the minimum value.
B is the baselineof always choosing the most frequent relation.
The best results are indicated in boldface.is much slower than the graphical models, and re-quires a great deal of memory; we were not able torun the neural network package on our machinesfor the role extraction task, when the feature vec-tors are very large.
The graphical models canperform both tasks simultaneously; the percent-age decrease in relation classification of model D2with respect to the NN is of 8.9% for ?only rele-vant?
and 5.8% for ?relevant + irrelevant?.4.3 FeaturesIn order to analyze the relative importance of thedifferent features, we performed both tasks usingthe dynamic model D1 of Figure 1, leaving outsingle features and sets of features (grouping all ofthe features related to the MeSH hierarchy, mean-ing both the classification of words into MeSHIDs and the domain knowledge as defined in Sec-tion 3).
The results reported here were found withmaximum likelihood (no smoothing) and are forthe ?relevant only?
case; results for ?relevant + ir-relevant?
were similar.For the role extraction task, the most impor-tant feature was the word: not using it, theGM achieved only 0.65 F-measure (a decrease of9.7% from 0.72 F-measure using all the features).Leaving out the features related to MeSH the F-measure obtained was 0.69% (a 4.1% decrease)and the next most important feature was the part-of-speech (0.70 F-measure not using this feature).For all the other features, the F-measure rangedbetween 0.71 and 0.73.For the task of relation classification, theMeSH-based features seem to be the most im-portant.
Leaving out the word again lead to thebiggest decrease in the classification accuracy fora single feature but not so dramatically as in therole extraction task (62.2% accuracy, for a de-crease of 4% from the original value), but leavingout all the MeSH features caused the accuracy todecrease the most (a decrease of 13.2% for 56.2%accuracy).
For both tasks, the impact of the do-main knowledge alone was negligible.As described in Section 3, words can be mappedto different levels of the MeSH hierarchy.
Cur-rently, we use the ?second?
level, so that, for ex-ample, surgery is mapped to G02.403 (when thewhole MeSH ID is G02.403.810.762).
This issomewhat arbitrary (and mainly chosen with thesparsity issue in mind), but in light of the impor-tance of the MeSH features it may be worthwhileinvestigating the issue of finding the optimal levelof description.
(This can be seen as another formof smoothing.
)5 ConclusionsWe have addressed the problem of distinguishingbetween several different relations that can holdbetween two semantic entities, a difficult and im-portant task in natural language understanding.We have presented five graphical models and aneural network for the tasks of semantic relationclassification and role extraction from biosciencetext.
The methods proposed yield quite promis-ing results.
We also discussed the strengths andweaknesses of the discriminative and generativePrediction Num.
Sent.
RelationTruth Vague OD NC Cure Prev.
OT SE Irr.
(Train, Test) accuracyVague 0 3 0 4 0 0 0 1 28, 8 0Only DIS (OD) 2 69 0 27 1 1 0 24 492, 124 55.6No Cure (NC) 0 0 0 1 0 0 0 0 3, 1 0Cure 2 5 0 150 1 1 0 3 648, 162 92.6Prevent 0 1 0 2 5 0 0 5 50, 13 38.5Only TREAT (OT) 0 0 0 16 0 6 1 11 132, 34 17.6Side effect (SE) 0 0 0 3 1 0 0 1 24, 5 20Irrelevant 1 32 1 16 2 7 0 296 1416, 355 83.4Table 4: Confusion matrix for the dynamic model D2 for ?rel + irrel.
?, ?only features?.
In column ?Num.Sent.?
the numbers of sentences used for training and testing and in the last column the classificationaccuracies for each relation.
The total accuracy for this case is 74.9%.approaches and the use of a lexical hierarchy.Because there is no existing gold-standard forthis problem, we have developed the relation def-initions of Table 1; this however may not be anexhaustive list.
In the future we plan to assess ad-ditional relation types.
It is unclear at this time ifthis approach will work on other types of text; thetechnical nature of bioscience text may lend itselfwell to this type of analysis.Acknowledgements We thank Kaichi Sung forher work on the relation labeling and Chris Man-ning for helpful suggestions.
This research wassupported by a grant from the ARDA AQUAINTprogram, NSF DBI-0317510, and a gift fromGenentech.ReferencesE.
Agichtein and L. Gravano.
2000.
Snowball: Ex-tracting relations from large plain-text collections.Proceedings of DL ?00.D.
Bikel, R. Schwartz, and R. Weischedel.
1999.
Analgorithm that learns what?s in a name.
MachineLearning, 34(1-3):211?231.E.
Brill.
1995.
Transformation-based error-drivenlearning and natural language processing: A casestudy in part-of-speech tagging.
Computational Lin-guistics, 21(4):543?565.M.
Collins.
1996.
A new statistical parser based onbigram lexical dependencies.
Proc.
of ACL ?96.M.
Craven.
1999.
Learning to extract relations fromMedline.
AAAI-99 Workshop on Machine Learningfor Information Extraction.R.
Feldman, Y. Regev, M. Finkelstein-Landau,E.
Hurvitz, and B. Kogan.
2002.
Mining biomed-ical literature using information extraction.
CurrentDrug Discovery, Oct.D.
Freitag and A. McCallum.
2000.
Information ex-traction with HMM structures learned by stochasticoptimization.
AAAI/IAAI, pages 584?589.C.
Friedman, P. Kra, H. Yu, M. Krauthammer, andA.
Rzhetzky.
2001.
Genies: a natural-language pro-cessing system for the extraction of molecular path-ways from journal articles.
Bioinformatics, 17(1).A.
Ng and M. Jordan.
2002.
On discriminative vs.generative classifiers: A comparison of logistic re-gression and Naive Bayes.
NIPS 14.J.
Pustejovsky, J. Castano, and J. Zhang.
2002.
Robustrelational parsing over biomedical literature: Ex-tracting inhibit relations.
PSB 2002.S.
Ray and M. Craven.
2001.
Representing sentencestructure in Hidden Markov Models for informationextraction.
Proceedings of IJCAI-2001.T.
Rindflesch, L. Hunter, and L. Aronson.
1999.
Min-ing molecular binding terminology from biomedicaltext.
Proceedings of the AMIA Symposium.B.
Rosario, M. Hearst, and C. Fillmore.
2002.
Thedescent of hierarchy, and selection in relational se-mantics.
Proceedings of ACL-02.P.
Srinivasan and T. Rindflesch.
2002.
Exploring textmining from Medline.
Proceedings of the AMIASymposium.C.
Thompson, R. Levy, and C. Manning.
2003.
A gen-erative model for semantic role labeling.
Proceed-ings of EMCL ?03.D.
Zelenko, C. Aone, and A. Richardella.
2002.
Ker-nel methods for relation extraction.
Proceedings ofEMNLP 2002.C.
Zhai and J. Lafferty.
2001.
A study of smoothingmethods for language models applied to ad hoc in-formation retrieval.
In Proceedings of SIGIR ?01.
