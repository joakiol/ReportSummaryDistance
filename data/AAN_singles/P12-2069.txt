Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 354?358,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsFully Abstractive Approach to Guided SummarizationPierre-Etienne Genest, Guy LapalmeRALI-DIROUniversite?
de Montre?alP.O.
Box 6128, Succ.
Centre-VilleMontre?al, Que?becCanada, H3C 3J7{genestpe,lapalme}@iro.umontreal.caAbstractThis paper shows that full abstraction can beaccomplished in the context of guided sum-marization.
We describe a work in progressthat relies on Information Extraction, statis-tical content selection and Natural LanguageGeneration.
Early results already demonstratethe effectiveness of the approach.1 IntroductionIn the last decade, automatic text summarization hasbeen dominated by extractive approaches that relypurely on shallow statistics.
In the latest evalu-ation campaign of the Text Analysis Conference1(TAC), the top systems were considered only ?barelyacceptable?
by human assessment (Owczarzak andDang, 2011).
The field is also getting saturated nearwhat appears to be a ceiling in performance.
Sys-tems that claim to be very different from one an-other have all become statistically indistinguishablein evaluation results.
An experiment (Genest et al,2009) found a performance ceiling to pure sentenceextraction that is very low compared to regular (ab-stractive) human summaries, but not that much bet-ter than the current best automatic systems.Abstractive summarization has been explored tosome extent in recent years: sentence compression(Knight and Marcu, 2000) (Cohn and Lapata, 2009),sentence fusion (Barzilay and McKeown, 2005) orrevision (Tanaka et al, 2009), and a generation-based approach that could be called sentence split-ting (Genest and Lapalme, 2011).
They are all1www.nist.gov/tacrewriting techniques based on syntactical analysis,offering little improvement over extractive methodsin the content selection process.We believe that a fully abstractive approach with aseparate process for the analysis of the text, the con-tent selection, and the generation of the summaryhas the most potential for generating summaries at alevel comparable to human.
For the foreseeable fu-ture, we think that such a process for full abstractionis impossible in the general case, since it is almostequivalent to perfect text understanding.
In specificdomains, however, an approximation of full abstrac-tion is possible.This paper shows that full abstraction can be ac-complished in the context of guided summarization.We propose a methodology that relies on Informa-tion Extraction and Natural Language Generation,and discuss our early results.2 Guided SummarizationThe stated goal of the guided summarization taskat TAC is to motivate a move towards abstractiveapproaches.
It is an oriented multidocument sum-marization task in which a category is attributedto a cluster of 10 source documents to be summa-rized in 100 words or less.
There are five cate-gories: Accidents and Natural Disasters, Attacks,Health and Safety, Endangered Resources, and In-vestigations/Trials.
Each category is associated witha list of aspects to address in the summary.
Figure 1shows the aspects for the Attacks category.
We usethis specification of categories and aspects to accom-plish domain-specific summarization.3542.1 WHAT: what happened2.2 WHEN: date, time, other temporal placement markers2.3 WHERE: physical location2.4 PERPETRATORS: individuals or groups responsible for the attack2.5 WHY: reasons for the attack2.6 WHO AFFECTED: casualties (death, injury), or individuals otherwise negatively affected2.7 DAMAGES: damages caused by the attack2.8 COUNTERMEASURES: countermeasures, rescue efforts, prevention efforts, other reactionsFigure 1: Aspects for TAC?s guided summarization task, category 2: Attacks3 Fully Abstractive ApproachGuided summarization categories and aspects definean information need, and using Information Extrac-tion (IE) seems appropriate to address it.
The ideato use an IE system for summarization can be tracedback to the FRUMP system (DeJong, 1982), whichgenerates brief summaries about various kinds ofstories; (White et al, 2001) also wrote abstractivesummaries using the output of an IE system appliedto events such as natural disasters.
In both cases, theend result is a generated summary from the informa-tion available.
A lot of other work has instead usedIE to improve the performance of extraction-basedsystems, like (Barzilay and Lee, 2004) and (Ji et al,2010).What is common to all these approaches is thatthe IE system is designed for a specific purpose, sep-arate from summarization.
However, to properly ad-dress each aspect requires a system designed specifi-cally for that task.
To our knowledge, tailoring IE tothe needs of abstractive summarization has not beendone before.
Our methodology uses a rule-based,custom-designed IE module, integrated with Con-tent Selection and Generation in order to write short,well-written abstractive summaries.Before tackling these, we perform some prepro-cessing on the cluster of documents.
It includes:cleaning up and normalization of the input using reg-ular expressions, sentence segmentation, tokeniza-tion and lemmatization using GATE (Cunninghamet al, 2002), syntactical parsing and dependencyparsing (collapsed) using the Stanford Parser (deMarneffe et al, 2006), and Named Entity Recogni-tion using Stanford NER (Finkel et al, 2005).
Wehave also developed a date resolution engine that fo-cuses on days of the week and relative terms.3.1 Information ExtractionOur architecture is based on Abstraction Schemes.An abstraction scheme consists of IE rules, con-tent selection heuristics and one or more genera-tion patterns, all created by hand.
Each abstrac-tion scheme is designed to address a theme or sub-category.
Thus, rules that extract information forthe same aspect within the same scheme will share asimilar meaning.
An abstraction scheme aims to an-swer one or more aspects of its category, and morethan one scheme can be linked to the same aspect.Figure 2 shows two of the schemes that we havecreated.
For the scheme killing, the IE rules wouldmatch X as the perpetrator and Y as a victim forall of the following phrases: X killed Y, Y wasassassinated by X, and the murder of Xby Y.
Other schemes have similar structure and pur-pose, such as wounding, abducting, damagingand destroying.
To create extraction rules for ascheme, we must find several verbs and nouns shar-ing a similar meaning and identify the syntacticalposition of the roles we are interested in.
Three re-sources have helped us in designing extraction rules:a thesaurus to find semantically related nouns andverbs; VerbNet (Kipper et al, 2006), which providesamongst other things the semantic roles of the syn-tactical dependents of verbs; and a hand-crafted listof aspect-relevant word stems provided by the teamthat made CLASSY (Conroy et al, 2010).Schemes and their extraction rules can also bequite different from this first example, as shown withthe scheme event.
This scheme gathers the basic in-formation about the attack event: WHAT category ofattack, WHEN and WHERE it occurred.
A list of keywords is used to identify words that imply an attackevent, while a list of EVENT NOUNs is used to iden-tify specifically words that refer to a type of attack.355Scheme: killingInformation ExtractionSUBJ(kill, X) ?
WHO(X)OBJ(kill, Y) ?
WHO AFFECTED(Y)SUBJ(assassinate, X) ?
WHO(X)OBJ(assassinate, Y) ?
WHO AFFECTED(Y)...PREP OF(murder, Y) ?
WHO AFFECTED(Y)PREP BY(murder, X) ?
WHO(X)...Content Selection Select best candidates for kill verb, WHO(X) and WHO AFFECTED(Y)Generation X kill verb YScheme: eventInformation ExtractionPREP IN(key word, X), LOCATION(X) ?
WHERE(X)PREP IN(key word, X), ORGANIZATION(X) ?
WHERE(X)PREP AT(key word, X), LOCATION(X) ?
WHERE(X)PREP AT(key word, X), ORGANIZATION(X) ?
WHERE(X)DEP(key word, Y), DATE(Y) ?
WHEN(Y)EVENT NOUN(Z) ?
WHAT(Z)Content Selection Select best candidates for at or in, WHERE(X), WHEN(Y) and WHAT(Z)Generation On Y, Z occurred at/in XFigure 2: Abstraction schemes killing and event.
The information extraction rules translate preprocessing annota-tions into candidate answers for a specific aspect.
Content selection determines which candidate will be included in thegenerated sentence for each aspect.
Finally, a pattern is used to determine the structure of the generated sentence.
No-tation: word or lemma, variable, group of words, PREDICATE OR ASPECT.
Note that the predicate DEP matchesany syntactical dependency and that key words refer to a premade list of category-relevant verbs and nouns.3.2 Content SelectionA large number of candidates are found by the IErules for each aspect.
The content selection moduleselects the best ones and sends them to the genera-tion module.
The basic heuristic is to select the can-didate most often mentioned for an aspect, and simi-larly for the choice of a preposition or a verb for gen-eration.
More than one candidate may be selectedfor the aspect WHO AFFECTED, the victims ofthe attack.
Several heuristics are used to avoid re-dundancies and uninformative answers.News articles may contain references to morethan one event of a given category, but our sum-maries describe only one.
To avoid mixing candi-dates from two different event instances that mightappear in the same cluster of documents, we rely ondates.
The ancestors of a date in the dependencytree are associated with that date, and excluded fromthe summary if the main event occurs on a differentdate.3.3 GenerationThe text of a summary must be fluid and feel natu-ral, while being straightforward and concise.
Fromour observation of human-written summaries, it alsodoes not require a great deal of originality to beconsidered excellent by human standards.
Thus,we have designed straightforward generation pat-terns for each scheme.
They are implemented us-ing the SimpleNLG realizer (Gatt and Reiter, 2009),which takes a sentence structure and words in theirroot form as input and gives a sentence with re-solved agreements and sentence markers as output.The greatest difficulty in the structure is in realizingnoun phrases.
The content selection module selectsa lemma that should serve as noun phrase head, andits number, modifiers and specifier must be deter-mined during generation.
Frequencies and heuristicsare again used to identify appropriate modifiers, thistime from all those used with that head within thesource documents.
We apply the constraint that the356On April 20, 1999, a massacre occurred at Columbine High School.Two student gunmen killed 12 students, a teacher and themselves.On November 2, 2004, a brutal murder occurred in Amsterdam.A gunman stabbed and shot Dutch filmmaker Theo van Gogh.A policeman and the suspect were wounded.On February 14, 2005, a suicide car bombing occurred in Beirut.Former Lebanese Prime Minister Rafik Hariri and 14 others were killed.Figure 3: Brief fully abstractive summaries on clusters D1001A-A, D1039G-A and D1043H-A, respectively on theColumbine massacre, the murder of Theo van Gogh and the assassination of Rafik Hariri.combination of number and modifiers chosen mustappear at least once as an IE rule match.As for any generated text, a good summary alsorequires a text plan (Hovy, 1988) (McKeown, 1985).Ours consists of an ordering of the schemes.
For ex-ample, an Attack summary begins with the schemeevent.
This ordering also determines which schemeto favor in the case of redundancy, e.g.
given that abuilding was both damaged and destroyed, only thefact that is was destroyed will be mentioned.4 Results and DiscussionWe have implemented this fully abstractive summa-rization methodology.
The abstraction schemes andtext plan for the Attack category are written in anXML document, designed to easily allow the addi-tion of more schemes and the design of new cate-gories.
The language processing of the source docu-ments and the domain-specific knowledge are com-pletely separate in the program.Our system, which is meant as a proof of concept,can generate useful summaries for the Attack cate-gory, as can be seen in Figure 3.
The key elementsof information are present in each case, stated in away that is easy to understand.These short summaries have a high density of in-formation, in terms of how much content from thesource documents they cover for a given number ofwords.
For example, using the most widely usedcontent metric, Pyramid (Nenkova et al, 2007), thetwo sentences generated for the cluster D1001A-A contain 8 Semantic Content Units (SCU) for aweighted total of 30 out of a maximum of 56, fora raw Pyramid score of 0.54.
Only 3 of the 43 auto-matic summaries beat this score on this cluster thatyear (the average was 0.31).
Note that the sum-maries that we compare against contain up to 100words, whereas ours is only 21 words long.
We con-clude that our method has the potential for creatingsummaries with much greater information densitythan the current state of the art.In fact, our approach does not only have the po-tential to increase a summary?s coverage, but also itslinguistic quality and the reader satisfaction as well,since the most relevant information now appears atthe beginning of the summary.5 Conclusion and Future WorkWe have developed and implemented a fully abstrac-tive summarization methodology in the context ofguided summarization.
The higher density of infor-mation in our short summaries is one key to addressthe performance ceiling of extractive summarizationmethods.
Although fully abstractive summarizationis a daunting challenge, our work shows the feasibil-ity and usefulness of this new direction for summa-rization research.We are now expanding the variety and complexityof the abstraction schemes and generation patternsto deal with more aspects and other categories.
Weshould then be able to compare on a greater scalethe output of our system with the ones produced byother automatic systems and by humans on all theclusters used at TAC 2010 and 2011.6 AcknowledgementsThe authors want to thank Dr. Eduard Hovy, of ISI,and Prof. Kathy McKeown, of Columbia Univer-sity, for fruitful discussions on abstractive summa-rization, and Dr. Judith Schlesinger and Dr. JohnConroy, both of the IDA / Center for Computing Sci-ences, for providing us with their hand-crafted list ofcategory- and aspect-relevant keywords.357ReferencesR.
Barzilay and L. Lee.
2004.
Catching the Drift: Prob-abilistic Content Models, with Applications to Gen-eration and Summarization.
eprint arXiv:cs/0405039,May.Regina Barzilay and Kathleen R. McKeown.
2005.
Sen-tence fusion for multidocument news summarization.Computational Linguistics, 31(3):297?328.Trevor Cohn and Mirella Lapata.
2009.
Sentencecompression as tree transduction.
J. Artif.
Int.
Res.,34(1):637?674.John M. Conroy, Judith D. Schlesinger, Peter A. Rankel,and Dianne P. O?Leary.
2010.
CLASSY 2010: Sum-marization and metrics.
In Proceedings of the ThirdText Analysis Conference, Gaithersburg, Maryland,USA.
National Institute of Standards and Technology.Hamish Cunningham, Diana Maynard, KalinaBontcheva, and Valentin Tablan.
2002.
GATE:A framework and graphical development environmentfor robust NLP tools and applications.
In Proceedingsof the 40th Annual Meeting of the Association forComputational Linguistics, Philadelphia, PA, USA.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating TypedDependency Parses from Phrase Structure Parses.
InProceedings of the IEEE / ACL 2006 Workshop onSpoken Language Technology.
The Stanford NaturalLanguage Processing Group.Gerald DeJong, 1982.
An Overview of the FRUMP Sys-tem, pages 149?176.
Lawrence Erlbaum.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by Gibbs sam-pling.
In Proceedings of the 43rd Annual Meeting onAssociation for Computational Linguistics, ACL ?05,pages 363?370, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Albert Gatt and Ehud Reiter.
2009.
SimpleNLG: a Re-alisation Engine for Practical Applications.
In ENLG?09: Proceedings of the 12th European Workshop onNatural Language Generation, pages 90?93, Morris-town, NJ, USA.
Association for Computational Lin-guistics.Pierre-Etienne Genest and Guy Lapalme.
2011.
Frame-work for Abstractive Summarization using Text-to-Text Generation.
In Proceedings of the Workshop onMonolingual Text-To-Text Generation, pages 64?73,Portland, Oregon, USA, June.
Association for Com-putational Linguistics.Pierre-Etienne Genest, Guy Lapalme, and Mehdi Yousfi-Monod.
2009.
HexTac: the Creation of a Manual Ex-tractive Run.
In Proceedings of the Second Text Anal-ysis Conference, Gaithersburg, Maryland, USA.
Na-tional Institute of Standards and Technology.Eduard H. Hovy.
1988.
Planning coherent multisenten-tial text.
In Proceedings of the 26th annual meetingon Association for Computational Linguistics, pages163?169, Morristown, NJ, USA.
Association for Com-putational Linguistics.Heng Ji, Juan Liu, Benoit Favre, Dan Gillick, and DilekHakkani-Tur.
2010.
Re-ranking summaries basedon cross-document information extraction.
In Pu-JenCheng, Min-Yen Kan, Wai Lam, and Preslav Nakov,editors, Information Retrieval Technology, volume6458 of Lecture Notes in Computer Science, pages432?442.
Springer Berlin / Heidelberg.
10.1007/978-3-642-17187-1 42.Karen Kipper, Anna Korhonen, Neville Ryant, andMartha Palmer.
2006.
Extending VerbNet with NovelVerb Classes.
In LREC 2006.Kevin Knight and Daniel Marcu.
2000.
Statistics-based summarization - step one: Sentence compres-sion.
In Proceedings of the Seventeenth National Con-ference on Artificial Intelligence and Twelfth Confer-ence on Innovative Applications of Artificial Intelli-gence, pages 703?710.
AAAI Press.Kathleen R. McKeown.
1985.
Discourse strategies forgenerating natural-language text.
Artif.
Intell., 27:1?41, September.Ani Nenkova, Rebecca Passonneau, and Kathleen McK-eown.
2007.
The pyramid method: Incorporating hu-man content selection variation in summarization eval-uation.
ACM Trans.
Speech Lang.
Process., 4, May.Karolina Owczarzak and Hoa Trang Dang.
2011.Overview of the TAC 2011 summarization track:Guided task and aesop task.
In Proceedings of theFourth Text Analysis Conference, Gaithersburg, Mary-land, USA.
National Institute of Standards and Tech-nology.
http://www.nist.gov/tac/publications/.Hideki Tanaka, Akinori Kinoshita, Takeshi Kobayakawa,Tadashi Kumano, and Naoto Kato.
2009.
Syntax-driven sentence revision for broadcast news summa-rization.
In Proceedings of the 2009 Workshop on Lan-guage Generation and Summarisation, UCNLG+Sum?09, pages 39?47, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Michael White, Tanya Korelsky, Claire Cardie, VincentNg, David Pierce, and Kiri Wagstaff.
2001.
Multi-document summarization via information extraction.In Proceedings of the first international conference onHuman language technology research, HLT ?01, pages1?7, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.358
