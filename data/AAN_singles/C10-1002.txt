Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 10?18,Beijing, August 2010Identifying Multi-word Expressions byLeveraging Morphological and Syntactic IdiosyncrasyHassan Al-HajLanguage Technologies InstituteCarnegie Mellon Universityhhaj@cs.cmu.eduShuly WintnerDepartment of Computer ScienceUniversity of Haifashuly@cs.haifa.ac.ilAbstractMulti-word expressions constitute a sig-nificant portion of the lexicon of everynatural language, and handling them cor-rectly is mandatory for various NLP appli-cations.
Yet such entities are notoriouslyhard to define, and are consequently miss-ing from standard lexicons and dictionar-ies.
Multi-word expressions exhibit id-iosyncratic behavior on various levels: or-thographic, morphological, syntactic andsemantic.
In this work we take advan-tage of the morphological and syntacticidiosyncrasy of Hebrew noun compoundsand employ it to extract such expressionsfrom text corpora.
We show that relyingon linguistic information dramatically im-proves the accuracy of compound extrac-tion, reducing over one third of the errorscompared with the best baseline.1 IntroductionMulti-word expressions (MWEs) are notoriouslyhard to define.
They span a range of constructions,from completely frozen, semantically opaque id-iomatic expressions, to frequent but morpholog-ically productive and semantically compositionalcollocations.
Various linguistic processes (ortho-graphic, morphological, syntactic, semantic, andcognitive) apply to MWEs in idiosyncratic ways.Notably, MWEs blur the distinction between thelexicon and the grammar, since they often havesome properties of words and some of phrases.In this work we define MWEs as expressionswhose linguistic properties (morphological, syn-tactic or semantic) are not directly derived fromthe properties of their word constituents.
This isa functional definition, driven by a practical mo-tivation: any natural language processing (NLP)application that cares about morphology, syntaxor semantics must consequently store MWEs inthe lexicon.MWEs are numerous and constitute a signif-icant portion of the lexicon of any natural lan-guage.
They are a heterogeneous class of con-structions with diverse sets of characteristics.Morphologically, some MWEs allow some oftheir constituents to freely inflect while restricting(or even preventing) the inflection of other con-stituents.
MWEs may allow constituents to un-dergo non-standard morphological inflections thatthey would not undergo in isolation.
Some MWEscontain words that never occur outside the contextof the MWE.
Syntactically, some MWEs appearin one rigid pattern (and a fixed order), while oth-ers permit various syntactic transformations.
Se-mantically, the compositionality of MWEs (i.e.,the degree to which the meaning of the whole ex-pression results from combining the meanings ofits individual words when they occur in isolation)is gradual.These morphological, syntactic and semanticidiosyncrasies make MWEs a challenge for NLPapplications (Sag et al, 2002).
They are evenmore challenging in languages with complex mor-phology, because of the unique interaction of mor-phological and orthographic processes with thelexical specification of MWEs (Oflazer et al,2004; Alegria et al, 2004).Because the idiosyncratic features of MWEscannot be predicted on the basis of their com-ponent words, they must be stored in the lexi-con of NLP applications.
Handling MWEs cor-rectly is beneficial for a variety of applications,including information retrieval, building ontolo-gies, text alignment, and machine translation.
Au-tomatic identification and corpus-based extractionof MWEs is thus crucial for such (and severalother) applications.10In this work we describe an approach that lever-ages the morphological and syntactic idiosyncrasyof a certain class of Hebrew1 MWEs, namelynoun compounds, to help identify such expres-sions in texts.
While the main contribution ofthis work is a system that can distinguish be-tween MWE and non-MWE instances of a partic-ular construction in Hebrew, thereby facilitatingfaster and more accurate integration of MWEs ina large-coverage lexicon of the language, we be-lieve that it carries added value to anyone inter-ested in MWEs.
The technique that we proposehere should be applicable in principle to any lan-guage in which MWEs exhibit linguistically id-iosyncratic behavior.We describe the properties of Hebrew noun-noun constructions in Section 2, and specify theirregularities exhibited by compounds.
Section 3presents the experimental setup and the main re-sults.
Compared with the best (collocation-based)baseline, our approach reduces over 30% of theerrors, yielding accuracy of over 80%.
We dis-cuss related work in Section 4 and conclude withsuggestions for future research.2 Hebrew noun-noun constructionsWe focus on Hebrew noun-noun constructions;these are extremely frequent constructions, andwhile many of them are fully compositional, oth-ers, called noun compounds (or just compounds)here, are clearly MWEs.
We first discuss the gen-eral construction and then describe the peculiar,idiosyncratic properties of compounds.2.1 The general caseHebrew nouns inflect for number (singular andplural) and, when the noun denotes an animate en-tity, for gender (masculine and feminine).
In ad-dition, nouns come in three states: indefinite, def-inite and a construct state that is used in genitiveconstructions.
Table 1 demonstrates the paradigm.A noun-noun construction (henceforth NNC)consists of a construct-state noun, called headhere, followed by a noun phrase, the modi-fier (Borer, 1988; Borer, 1996; Glinert, 1989).1To facilitate readability we use a transliteration of He-brew using Roman characters; the letters used, in Hebrewlexicographic order, are abgdhwzxTiklmns?pcqrs?t.State M/Sg F/Sg M/Pl F/Plindefinite ild ildh ildim ildwtdefinite hild hildh hildim hildwtconstruct ild ildt ildi ildwtTable 1: The noun paradigm, demonstrated on ild?child?The semantic relation between the two is usually,but not always, related to possession (Levi, 1976).Construct-state nouns only occur in the context ofNNC, and can never occur in isolation.
When aNNC is definite, the definite article is expressedon its modifier (Wintner, 2000).In the examples below, we explicitly indicateconstruct-state nouns by the morpheme ?.CONST?in the gloss; and definite nouns are indicated bythe morpheme ?the-?.
We provide both a literaland a non-literal meaning of the MWE examples.Expressions that have a literal, but not the ex-pected MWE meaning, are preceded by ?#?.Example 1 (Noun-noun constructions)hxlTt hw?dhdecision.CONST the-committee?the committee decision?
?wrk h?itwneditor.CONST the-journal?the journal editor?
?wrk dineditor.CONST law?law editor?
=?
lawyerbti xwlimhouses.CONST patients?patient houses?
=?
hospitals2.2 Noun compounds: Linguistic propertiesWhile many of the NNCs are free, compositionalcombinations of words, some are not; we use theterm noun compounds for the latter group.
Com-pounds typically (but not necessarily) have non-compositional meaning; presumably due to theiropaque, more lexical meaning, they also differfrom other NNCs in their morphological and syn-tactic behavior.
Some of these distinctive prop-erties are listed below, to motivate the methodol-ogy that we propose in Section 3 to distinguishbetween compounds and non-MWE NNCs.112.2.1 Limited inflectionWhen a NNC consists of two nouns, the sec-ond can typically occur in either singular or pluralform.
Compounds often limit the possibilities toonly one of those.Example 2 (No plural form of the modifier)?wrki h?itwnimeditors-.CONST the-journals?the journals?
editors?
?wrki hdineditors.CONST the-law?the law editors?
=?
the lawyers#wrki hdinimeditors.CONST the-lawsExample 3 (No singular form of the modifier)kiwwn hrwxdirection.CONST the-wind?the wind?s direction?kiwwn hrwxwtdirection.CONST the-winds?the winds?
direction?s?ws?nt h-rwxwtlily.CONST the-winds?lily of the winds?
=?
compass rose#s?ws?nt h-rwxlily.CONST the-wind2.2.2 Limited syntactic variationSince NNCs typically denote genitive (posses-sive) constructions, they can be paraphrased by aconstruction that uses the genitive preposition s?l?of?
(or, in some cases, other prepositions).
Thesesyntactic variants are often restricted in the case ofcompounds.Example 4 (Limited paraphrasing)h?wrk s?l h?itwnthe-editor of the-journal?the journal editor?#h?wrk s?l hdinthe-editor of the-lawExample 5 (Limited paraphrasing)m?il cmrcoat.CONST wool?wool coat?m?il mcmrcoat from-wool?wool coat?cmr pldhwool.CONST steel?steel wool?
=?
steel wool#cmr mpldhwool from-steel2.2.3 Limited syntactic modificationNNCs typically allow adjectival modificationof either of their constituents.
Since compoundstend to be more semantically opaque, it is of-ten only possible to modify the entire compound,but not any of the constituents.
In the follow-ing example, note that ?wrkt ?editor?
is feminine,whereas ?itwn ?journal?
is masculine; adjectivesmust agree on gender with the noun they modify.Example 6 (Limited adjectival modification)?wrkt h?itwneditor-f.CONST the-journal-m?the journal editor?
?wrkt h?itwn hxds?heditor-f.CONST the-journal-m the-new-f?the new editor of the journal?
?wrkt h?itwn hxds?editor-f.CONST the-journal-m the-new-m?the editor of the new journal?
?wrkt hdin hxds?heditor-f.CONST the-law-m the-new-f?the new law editor?
=?
the new lawyer#?wrkt hdin hxds?editor-f.CONST the-law-m the-new-m2.2.4 Limited coordinationTwo NNCs that share a common head can beconjoined using the coordinating conjunction w?and?.
This possibility is often blocked in the caseof compounds.Example 7 (Limited coordination)mwsdwt xinwk wbriawtinstitutions.CONST education and-health?education and health institutions?bti sprhouses.CONST book?book houses?
=?
schools12bti xwlimhouses.CONST patients?patient houses?
=?
hospitals#bti spr wxwlimhouses.CONST book and-patients3 Identification of noun compoundsIn this section we describe a system that identi-fies noun compounds in Hebrew text, and extractsthem in order to extend the lexicon.
We capitalizeon the morphological and syntactic irregularitiesof noun compounds described in Section 2.2.Given a large monolingual corpus, the textis first morphologically analyzed and disam-biguated.
Then, all NNCs (candidate noun com-pounds) are extracted from the morphologicallydisambiguated text.
For each candidate nouncompound we define a set of features (Section 3.3)based on the idiosyncratic morphological and syn-tactic properties defined in Section 2.2.
Thesefeatures inform a support vector machine classi-fier which is then used to identify the noun com-pounds in the set of NNCs with high accuracy(Section 3.5).3.1 ResourcesWe use (a subset of) the Corpus of Contempo-rary Hebrew (Itai and Wintner, 2008) which con-sists of four sub-corpora: The Knesset corpuscontains the Israeli parliament proceedings from2004-2005; the Haaretz corpus contains articlesfrom the Haaretz newspaper from 1991; The-Marker corpus contains financial articles from theTheMarker newspaper from 2002; and the Arutz7 corpus contains newswire articles from 2001-2006.
Corpora sizes are listed in Table 2.Corpus Number of tokensKnesset 12,742,879Harretz 463,085The Marker 684,801Arutz 7 7,714,309Total 21,605,074Table 2: Corpus dataThe entire corpus was morphologically ana-lyzed (Yona and Wintner, 2008; Itai and Wintner,2008) and POS-tagged (Bar-haim et al, 2008);note that no syntactic parser is available for He-brew.
From the morphologically disambiguatedcorpus, we extract all bi-grams in which the firsttoken is a noun in the construct state and the sec-ond token is a noun that is not in the constructstate, i.e., all two-word NNC candidates.3.2 AnnotationFor training and evaluation, we select the NNCsthat occur at least 100 times in the corpus, yield-ing 1060 NNCs.
These NNCs were annotatedby three annotators, who were asked to classifythem to the following four groups: compounds(+); non-compounds (?
); unsure (0); and errors ofthe morphological processor (i.e., the candidate isnot a NNC at all).
Table 3 lists the number of can-didates in each class.Annotator + ?
0 err1 314 332 238 1762 335 403 179 1433 400 630 16 14Table 3: NNC classification by annotatorWe adopt a conservative approach in combin-ing the three annotations.
First, we eliminate 204NNCs that were tagged as errors by at least oneannotator.
For the remaining NNCs, a candidate isconsidered a compound or a non-compound onlyif all three annotators agree on its classification.This reduces the annotated data to 463 instances,of which 205 are compounds and 258 are clearcases of non-compound NNCs.23.3 Linguistically-motivated featuresWe define a set of features based on the idiosyn-cratic properties of noun compounds defined inSection 2.2.
For each candidate NNC, we com-pute counts which reflect the likelihood of it ex-hibiting one of the linguistic properties.Refer back to Section 2.2.
We focus on theproperty of limited inflection (Section 2.2.1), anddefine features 1?8 to reflect it.
To reflect limitedsyntactic variation (Section 2.2.2) we define fea-tures 9?10.
Feature 11 addresses the phenomenon2This annotated corpus is freely available for download.13of limited coordination (Section 2.2.4).
To reflectlimited syntactic modification (Section 2.2.3) wedefine feature 12.
.For each NNC candidate N1 N2, the followingfeatures are defined:1.
The number of occurrences of the NNC inwhich both constituents are in singular.2.
The number of occurrences of the NNC inwhich N1 is in singular and N2 is in plural.3.
The number of occurrences of the NNC inwhich N1 is in plural and N2 is in singular.4.
The number of occurrences of the NNC inwhich both constituents are in plural.5.
The number of occurrences of N1 in pluraloutside the expression.6.
The number of occurrences of N1 in singularoutside the expression.7.
The number of occurrences of N2 in pluraloutside the expression.8.
The number of occurrences of N2 in singularoutside the expression.9.
The number of occurrences of N1 s?l N2 ?N1of N2?
in the corpus.10.
The number of occurrences of N1 m N2 ?N1from N2?
in the corpus.11.
The number of occurrences of N1 N2 w N3?N1 N2 and N3?
in the corpus, where N3 isan indefinite, non-construct-state noun.12.
The number of occurrences of N1 N2 Adj inthe corpus, where the adjective Adj agreeswith N2 on both gender and number, whiledisagreeing with N1 on at least one of theseattributes.We also define four features that represent knowncollocation measures (Evert and Krenn, 2001):Point-wise mutual information (PMI); T-Score;log-likelihood; and the raw frequency of N1 N2in the corpus.33A detailed description of these measures is given byManning and Schu?tze (1999, Chapter 5); see also http://www.collocations.de/, where several other asso-ciation measures are discussed as well.3.4 Training and evaluationFor each NNC in the annotated set of Section 3.2we create a vector of the 16 features described inSection 3.3 (12 linguistically-motivated featuresplus four collocation measures).
We obtain a listof 463 instances, of which 205 are positive ex-amples (noun compounds) and 258 are negative.We use this set for training and evaluation of atwo class soft margin SVM classifier (Chang andLin, 2001) with a radial basis function kernel.
Weexperiment below with different combinations offeatures, where for each combination we use 10-fold cross-validation over the 463 NNcs to evalu-ate the classifier.
We report Precision, Recall, F-score and Accuracy (averaged over the 10 folds).3.5 ResultsThe results of the different classifiers that wetrained are given in Table 4.
The first four rowsof the table show the performance of classifierstrained using each of the four different colloca-tion measure features alone.
Both PMI and Log-likelihood outperform the other collocation mea-sures, with an F-score of 60, which we considerour baseline.
We also report the performance oftwo combinations of collocation measures, whichyield small improvement.
The best combinationsprovide accuracy of about 70% and F-score of 63.The remaining rows report results using thelinguistically-motivated features (LMF) of Sec-tion 3.3.
These features alone yield accuracy of77.75% and an F-score of 76.
Adding also Log-likelihood improves F-score by 1.16 and accuracyby 1.29%.
Finally, using Log-likelihood with asubset of the LMF consisting of features 1-2, 4-6, 9-10 and 12 (see below) yields the best re-sults, namely accuracy of over 80% and F-scoreof 78.85, reflecting a reduction of over one thirdin classification error rate compared with the base-line.3.6 Optimizing feature combinationWe search for the combination of linguistically-motivated features that would yield the best per-formance.
Training a classifier on all possiblefeature combinations is clearly infeasible.
In-stead, we follow a more efficient greedy approach,whereby we start with the best collocation mea-14Features Accuracy Precision Recall F-scorePMI 67.17 64.97 56.09 60.20Frequency 60.47 60.00 32.19 41.90T-Score 61.98 59.86 42.92 50.00Log-likelihood 69.33 71.42 51.21 59.65T-score+Log-likelihood 70.62 71.42 56.09 62.84PMI+Log-likelihood 69.97 68.96 58.53 63.32LMF 77.75 71.98 81.46 76.43LMF+PMI 77.32 71.18 81.95 76.19LMF+Log-likelihood 79.04 73.68 81.95 77.59Log-likelihood+LMF[1-2,4-6,9-10,12] 80.77 76.85 80.97 78.85Table 4: Results: 10-Fold accuracy, precision, recall, and F-score for classifiers trained using differentcombinations of features.
LMF stands for linguistically-motivated featuressure, Log-likelihood, and add other features one ata time, in the order in which they are listed in Sec-tion 3.3.
After adding each feature the classifier isretrained; the feature is retained in the feature setonly if adding it improves the 10-fold F-score ofthe current feature set.Table 5 lists the results of this experiment.
Foreach feature set the difference in the 10-fold F-score compared to the previous feature set is listedin parentheses.
The results show that the best fea-ture combination improves the F-score by 1.26,compared with using all features.
This experi-ments shows that features 3, 7, 8 and 11 turn outnot to be useful, and the classifier is more accuratewithout them.
We also tried this approach withPMI as the starting feature, with very similar re-sults.Feature set F-scoreLog-likelihood 59.65Log-likelihood,1 60.34 (+0.68)Log-likelihood,1-2 65.42 (+5.08)Log-likelihood,1-3 64.87 (-0.54)Log-likelihood,1-2,4 66.66 (+1.78)Log-likelihood,1-2,4-5 70.00 (+3.33)Log-likelihood,1-2,4-6 74.37 (+4.37)Log-likelihood,1-2,4-7 73.78 (?0.58)Log-likelihood,1-2,4-6,8 73.58 (?0.79)Log-likelihood,1-2,4-6,9 78.72 (+4.35)Log-likelihood,1-2,4-6,9-10 78.83 (+0.10)Log-likelihood,1-2,4-6,9-11 77.37 (?1.46)Log-likelihood,1-2,4-6,9-10,12 78.85 (+0.02)Table 5: Optimizing the set of linguistically-motivated features4 Related workThere has been a growing awareness in the re-search community of the problems that MWEspose, both in linguistics and in NLP (Villavicencioet al, 2005).
Recent works address the definition,lexical representation and computational process-ing of MWEs, as well as algorithms for extractingthem from data.Focusing on acquisition of MWEs, early ap-proaches concentrated on their collocational be-havior (Church and Hanks, 1989).
Pecina (2008)compares 55 different association measures inranking German Adj-N and PP-Verb colloca-tion candidates.
This work shows that combin-ing different collocation measures using standardstatistical-classification methods (such as LinearLogistic Regression and Neural Networks) givesa significant improvement over using a single col-location measure.
Our results show that this isindeed the case, but the contribution of colloca-tion methods is limited, and more information isneeded in order to distinguish frequent colloca-tions from bona fide MWEs.Other works show that adding linguistic infor-mation to collocation measures can improve iden-tification accuracy.
Several approaches rely on thesemantic opacity of MWEs; but very few seman-tic resources are available for Hebrew (the He-brew WordNet (Ordan and Wintner, 2007), theonly lexical semantic resource for this language,is small and too limited).
Instead, we capital-15ize on the morphological and syntactic irregular-ities that MWEs exhibit, using computational re-sources that are more readily-available.Ramisch et al (2008) evaluate a number ofassociation measures on the task of identifyingEnglish Verb-Particle Constructions and GermanAdjective-Noun pairs.
They show that addinglinguistic information (mostly POS and POS-sequence patterns) to the association measureyields a significant improvement in performanceover using pure frequency.
We follow this lineof research by defining a number of syntactic pat-terns as a source of linguistic information.
In ad-dition, our linguistic features are much more spe-cific to the phenomenon we are interested in, andthe syntactic patterns are enriched by morpholog-ical information pertaining to the idiosyncrasy ofMWEs; we believe that this explains the improvedperformance compared to the baseline.Several works address the lexical fixedness orsyntactic fixedness of (certain types of) MWEs inorder to extract them from texts.
An expressionis considered lexically fixed if replacing any of itsconstituents by a semantically (and syntactically)similar word generally results in an invalid or lit-eral expression.
Syntactically fixed expressionsprohibit (or restrict) syntactic variation.For example, Van de Cruys and Villada Moiro?n(2007) use lexical fixedness to extract Dutch Verb-Noun idiomatic combinations (VNICs).
Bannard(2007) uses syntactic fixedness to identify En-glish VNICs.
Another work uses both the syn-tactic and the lexical fixedness of VNICs in or-der to distinguish them from non-idiomatic ones,and eventually to extract them from corpora (Fa-zly and Stevenson, 2006).
While these approachesare in line with ours, they require lexical seman-tic resources (e.g., a database that determines se-mantic similarity among words) and syntactic re-sources (parsers) that are unavailable for Hebrew(and many other languages).
Our approach onlyrequires morphological processing, which is morereadily-available for several languages.Another unique feature of our work is thatit computationally addresses Hebrew (and, moregenerally, Semitic) MWEs for the first time.Berman and Ravid (1986) define the dictionarydegree of noun compounds in Hebrew as theircloseness to a single word from a grammaticalpoint of view, as judged by the manner in whichthey are grasped by language speakers.
A groupof 120 Hebrew speakers were asked to assign adictionary degree (from 1 to 5) to a list of 30noun compounds.
An analysis of the question-naire results revealed that language speaker sharea common dictionary, where the highest degree ofagreement was achieved on the ends of the dictio-nary degree spectrum.
Another conclusion is thatboth the pragmatic uses of the noun compoundand the semantic relation between its constituentsdefine the dictionary degree of the compound.
Nothaving access to semantic and pragmatic knowl-edge, we are trying to approximate it using mor-phology.Attia (2005) proposes methods to processfixed, semi-fixed, and syntactically-flexible Ara-bic MWEs (adopting the classification and the ter-minology of Sag et al (2002)).
Fabri (2009) pro-vides an overview of the different types of com-pounds (14 in total) in present-day Maltese, fo-cusing on one type of compounds consisting of anadjective followed by a noun.
He also providesmorphological, syntactic, and semantic propertiesof this group which distinguishes them from othernon-compound constructions.
Automatic identifi-cation of MWEs is not addressed in either of theseworks.5 Conclusions and future workWe described a system that can identify Hebrewnoun compounds with high accuracy, distinguish-ing them from non-idiomatic noun-noun construc-tions.
The methodology we advocate is based oncareful examination of the linguistic peculiaritiesof the construction, followed by corpus-based ap-proximation of these properties via a general ma-chine learning algorithm that is fed with featuresbased on the linguistic properties.
While our ap-plication is limited to a particular construction ina particular language, we are confident that it canbe equally well applied to other constructions andother languages, as long as the targeted MWEsexhibit a consistent set of irregular features (es-pecially in the morphology).This work can be extended in various direc-tions.
Addressing other constructions is relatively16easy, and requires only a theoretical linguistic in-vestigation of the construction.
We are currentlyinterested in extending the system to cope alsowith Adjective-Noun, Noun-Adjective and Verb-Preposition constructions in Hebrew.The accuracy of MWE acquisition systems canbe further improved by combining our morpho-logical and syntactic features with semanticallyinformed features such as translational entropycomputed from a parallel corpus (Villada Moiro?nand Tiedemann, 2006), or features that can cap-ture the local linguistic context of the expressionusing latent semantic analysis (Katz and Gies-brecht, 2006).
We are currently working on theformer direction (Tsvetkov and Wintner, 2010b),utilizing a small Hebrew-English parallel corpus(Tsvetkov and Wintner, 2010a).Finally, we are interested in evaluating themethodology proposed in this paper to other lan-guages with complex morphology, in particular toArabic.
We leave this direction to future research.AcknowledgmentsThis research was supported by THE IS-RAEL SCIENCE FOUNDATION (grantsNo.
137/06, 1269/07).
We are grateful to AlonItai for his continuous help and advice throughoutthe course of this project, and to Bracha Nir forvery useful comments.
We also wish to thankYulia Tsvetkov and Gily Chen for their annotationwork.ReferencesAlegria, In?aki, Olatz Ansa, Xabier Artola, NereaEzeiza, Koldo Gojenola, and Ruben Urizar.
2004.Representation and treatment of multiword expres-sions in Basque.
In Tanaka, Takaaki, Aline Villavi-cencio, Francis Bond, and Anna Korhonen, editors,Second ACL Workshop on Multiword Expressions:Integrating Processing, pages 48?55, Barcelona,Spain, July.
Association for Computational Linguis-tics.Attia, Mohammed A.
2005.
Accommodat-ing multiword expressions in an lfg grammar.The ParGram Meeting, Japan September 2005,September.
Mohammed A. Attia The Univer-sity of Manchester School of Informatics mo-hammed.attia@postgrad.manchester.ac.uk.Bannard, Colin.
2007.
A measure of syntactic flexibil-ity for automatically identifying multiword expres-sions in corpora.
In Proceedings of the Workshop onA Broader Perspective on Multiword Expressions,pages 1?8.
Association for Computational Linguis-tics.Bar-haim, Roy, Khalil Sima?an, and Yoad Winter.2008.
Part-of-speech tagging of Modern Hebrewtext.
Natural Language Engineering, 14(2):223?251.Berman, Ruth A. and Dorit Ravid.
1986.
Lexicaliza-tion of noun compounds.
Hebrew Linguistics, 24:5?22.
In Hebrew.Borer, Hagit.
1988.
On the morphological parallelismbetween compounds and constructs.
In Booij, Geertand Jaap van Marle, editors, Yearbook of Morphol-ogy 1, pages 45?65.
Foris publications, Dordrecht,Holland.Borer, Hagit.
1996.
The construct in review.In Lecarme, Jacqueline, Jean Lowenstamm, andUr Shlonsky, editors, Studies in Afroasiatic Gram-mar, pages 30?61.
Holland Academic Graphics,The Hague.Chang, Chih-Chung and Chih-Jen Lin, 2001.
LIB-SVM: a library for support vector machines.Software available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.Church, Kenneth.
W. and Patrick Hanks.
1989.
Wordassociation norms, mutual information and lexicog-raphy (rev).
Computational Linguistics, 19(1):22?29.Evert, Stefan and Brigitte Krenn.
2001.
Methods forthe qualitative evaluation of lexical association mea-sures.
In Proceedings of the 39th Annual Meetingof the Association for Computational Linguistics,pages 188?195, Morristown, NJ, USA.
Associationfor Computational Linguistics.Fabri, Ray.
2009.
Compounding and adjective-nouncompounds in Maltese.
In Comrie, Bernard, RayFabri, Elizabeth Hume, Manwel Mifsud, ThomasStolz, and Martine Vanhove, editors, IntroducingMaltese Linguistics, volume 113 of Studies in Lan-guage Companion Series.
John Benjamins.Fazly, Afsaneh and Suzanne Stevenson.
2006.
Auto-matically constructing a lexicon of verb phrase id-iomatic combinations.
In Proceedings of the 11thConference of the European Chapter of the Associ-ation for Computational Linguistics (EACL), pages337?344.Glinert, Lewis.
1989.
The Grammar of Modern He-brew.
Cambridge University Press, Cambridge.17Itai, Alon and Shuly Wintner.
2008.
Language re-sources for Hebrew.
Language Resources and Eval-uation, 42:75?98, March.Katz, Graham and Eugenie Giesbrecht.
2006.
Au-tomatic identification of non-compositional multi-word expressions using latent semantic analysis.
InProceedings of the Workshop on Multiword Expres-sions: Identifying and Exploiting Underlying Prop-erties, pages 12?19, Sydney, Australia, July.
Asso-ciation for Computational Linguistics.Levi, Judith N. 1976.
A semantic analysis of Hebrewcompound nominals.
In Cole, Peter, editor, Stud-ies in Modern Hebrew Syntax and Semantics, num-ber 32 in North-Holland Linguistic Series, pages 9?55.
North-Holland, Amsterdam.Manning, Christopher D. and Hinrich Schu?tze.
1999.Foundations of statistical natural language process-ing.
The MIT Press, Cambridge, Mass.Oflazer, Kemal, O?zlem C?etinog?lu, and Bilge Say.2004.
Integrating morphology with multi-word ex-pression processing in Turkish.
In Tanaka, Takaaki,Aline Villavicencio, Francis Bond, and Anna Ko-rhonen, editors, Second ACL Workshop on Multi-word Expressions: Integrating Processing, pages64?71, Barcelona, Spain, July.
Association forComputational Linguistics.Ordan, Noam and Shuly Wintner.
2007.
HebrewWordNet: a test case of aligning lexical databasesacross languages.
International Journal of Transla-tion, special issue on Lexical Resources for MachineTranslation, 19(1).Pecina, Pavel.
2008.
A machine learning approachto multiword expression extraction.
In Proceedingsof the LREC Workshop Towards a Shared Task forMultiword Expressions.Ramisch, Carlos, Paulo Schreiner, Marco Idiart, andAlline Villavicencio.
2008.
An evaluation of meth-ods for the extraction of multiword expressions.In Proceedings of the LREC Workshop Towards aShared Task for Multiword Expressions.Sag, Ivan, Timothy Baldwin, Francis Bond, AnnCopestake, and Dan Flickinger.
2002.
Multiwordexpressions: A pain in the neck for NLP.
In Pro-ceedings of the Third International Conference onIntelligent Text Processing and Computational Lin-guistics (CICLING 2002), pages 1?15, Mexico City,Mexico.Tsvetkov, Yulia and Shuly Wintner.
2010a.
Automaticacquisition of parallel corpora from websites withdynamic content.
In Proceedings of the Seventhconference on International Language Resourcesand Evaluation (LREC?10), pages 3389?3392.
Eu-ropean Language Resources Association (ELRA),May.Tsvetkov, Yulia and Shuly Wintner.
2010b.
Ex-traction of multi-word expressions from small par-allel corpora.
In Proceedings of the 23rd Inter-national Conference on Computational Linguistics(COLING 2010), August.Van de Cruys, Tim and Begon?a Villada Moiro?n.
2007.Semantics-based multiword expression extraction.In Proceedings of the Workshop on A BroaderPerspective on Multiword Expressions, pages 25?32, Prague, Czech Republic, June.
Association forComputational Linguistics.Villada Moiro?n, Begon?a and Jo?rg Tiedemann.
2006.Identifying idiomatic expressions using automaticword alignment.
In Proceedings of the EACL 2006Workshop on Multi-word-expressions in a multilin-gual context.
Association for Computational Lin-guistics.Villavicencio, Aline, Francis Bond, Anna Korhonen,and Diana McCarthy.
2005.
Introduction to thespecial issue on multiword expressions: Having acrack at a hard nut.
Computer Speech & Language,19(4):365?377.Wintner, Shuly.
2000.
Definiteness in the Hebrewnoun phrase.
Journal of Linguistics, 36:319?363.Yona, Shlomo and Shuly Wintner.
2008.
A finite-statemorphological grammar of Hebrew.
Natural Lan-guage Engineering, 14(2):173?190, April.18
