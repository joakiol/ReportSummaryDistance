Experiments with Learning Parsing HeuristicsSylvain DELISLEDrpartement de mathrmatiques et d'informatiqueUniversit6 du Qurbec ~ Trois-Rivi~resTrois-Rivi~res, Qurbec, Canada, GgA 5H7Sylvain_Delisle @uqtr.uquebec.caSylvain LI~TOURNEAU, Stan MATWlNSchool of Information Technology andEngineering, University of OttawaOttawa, Ontario, Canada, KIN 6N5sletour@ ai.iit.nrc.ca, stan @site.uottawa.caAbstractAny large language processing softwarerelies in its operation on heuristic decisionsconcerning the strategy of processing.These decisions are usually "hard-wired"into the software in the form of hand-crafted heuristic rules, independent of thenature of the processed texts.
We proposean alternative, adaptive approach in whichmachine learning techniques learn the rulesfrom examples of sentences in each class.We have experimented with a variety oflearning techniques on a representative in-stance of this problem within the realm ofparsing.
Our approach lead to the discoveryof new heuristics that perform significantlybetter than the current hand-crafted heuris-tic.
We discuss the entire cycle of applica-tion of machine learning and suggest amethodology for the use of machine learn-ing as a technique for the adaptive optimi-sation of language-processing software.1 IntroductionAny language processing program---in our case, atop-down parser which outputs only the first treeit could find--must make decisions as to whatprocessing strategy, or rule ordering, is most ap-propriate for the problem (i.e.
string) at hand.Given the size and the intricacy of the rule-baseand the goal (to optimise a parser's precision, orrecall, or even its speed), this becomes a complexdecision problem.
Without precise knowledge ofthe kinds of texts that will be processed, these de-cisions can at best be educated guesses.
In theparser we used, they were performed with thehelp of hand-crafted heuristic rules, which arebriefly presented in section 2.Even when the texts are available to fine-tunethe parser, it is not obvious how these decisions areto be made from texts alone.
Indeed, the decisionsmay often be expressed as rules whoserepresentation is in terms which are not directly oreasily available from the text (e.g.
non-terminals ofthe grammar of the language in which the texts arewritten).
Hence, any technique that mayautomatically or semi-automatically adapt suchrules to the corpus at hand will be valuable.
As it isoften the case, there may be a linguistic shift in thekinds of texts that are processed, especially if thelinguistic task is as general as parsing.
It is theninteresting to adapt the "version" of the parser tothe corpus at hand.We report on an experiment that targets thiskind of adaptability.
We use machine learning asan artificial intelligence technique that achievesadaptability.
We cast the task described above as aclassification task: which, among the parser's top-level rules, is most appropriate to launch theparsing of the current input string?
Although werestricted ourselves to a subset of a parser, ourobjective is broader than just applying an existinglearning system on this problem.
What is interes-ting is: a) definition of the attributes in whichexamples are given, so that the attributes are bothobtainable automatically from the text and lead togood rules--this is called "feature ngineering"; b)selection of the most interesting learned rules; c)incorporation of the learned rules in the parser; d)evaluation of the performance of the learned rulesafter they have been incorporated in the parser.
It isthe lessons from the whole cycle that we followedin the work that we report here, and we suggest itas a methodology for an adaptive optimisation oflanguage processing programs.3072 The existing hand-crafted heuristicsThe rule-based parser we used was DIPETT\[Delisle 1994\]: it is a top-down, depth-firstparser, augmented with a few look-ahead mecha-nisms, which returns the first analysis (parsetree).
The fact that our parser produces only asingle analysis, the "best" one according to itshand-crafted heuristics, is part of the motivationfor this work.
When DIPETT is given an inputstring, it first selects the top-level rules it is to at-tempt, as well as their ordering in this process.Ideally, the parser would find an optimal orderthat minimises parsing time and maximises par-sing accuracy by first selecting the most promi-sing rules.
For example, there is no need to treat asentence as multiply coordinated or compoundwhen the data contains only one verb.
DIPETThas three top-level rules for declarative state-ments: i) MULT_COOR for multiple (normally,three or more) coordinated sentences; ii)COMPOUND for compound sentences, that is, cor-relative and simple coordination (of, normally,two sentences); iii) NONCOMPOUND for simpleand complex sentences, that is, a single mainclause with zero or more subordinate clauses(\[Quirk et el .
1985\]).
To illustrate the data thatwe worked with and the classes for which weneeded the rules, here are two sentences (from theBrown corpus) used in our experiments: "And know,while all this went on, that there was no real reason to suppo-se that the murderer had been a guest in either hotel."
is anon-compound sentence, and =Even I can remembernothing but ruined cellars and tumbled pillars, and nobody haslived there in the memory of any living man."
is a com-pound sentence.The current hand-crafted heuristic (\[Delisle1994\]) is based on three parameters, obtained af-ter (non-disambiguating) lexical analysis and be-fore parsing: 1) the number of potential verbs' inthe data, 2) the presence of potential coordinatorsin the data, and 3) verb density (roughly spea-king, it indicates how potential verbs are distri-buted).
For instance, low density means thatverbs are scattered throughout the input string;high density means that the verbs appear close toeach other in the input string, as in a conjunctioni A "potential" verb may actually turn out to be, say, anoun, but only parsing can tell us how such a lexicalambiguity has been resolved.
If the input were pre-processed by a tagger, the ambiguity might disappear.of verbs such as "Verbl and Verb2 and Verb3".Given the input string's features we have just dis-cussed, DIPETT's algorithm for top-level ruleselection returns an ordered list of up to 3 of therules COMPOUND, NONCOMPOUND, andMULT_COOR tO be attempted when parsing thisstring.
For the purposes of our experiment, we sim-plified the situation by neglecting the MULT_COORrule since it was rarely needed when parsing real-life text.
Thus, the original problem went from a 3-class to a 2-class classification problem:COMPOUND or NON_COMPOUND.3 Learning rules from sentencesAs any heuristic, the top-level rule selectionmechanism just described is not perfect.
Amongthe principal difficulties, the most important are: i)the accuracy of the heuristic is limited and ii) theinternal choices are relatively complex andsomewhat obscure from a linguist's viewpoint.
Theaim of this research was to use classificationsystems as a tool to help developing new know-ledge for improving the parsing process.
To pre-serve the broad applicability of DIPETT, we haveemphasised the generality of the results and did notuse any kind of domain knowledge.
The sentencesused to build the classifiers and evaluate theperformance have been randomly selected fromfive unrelated real corpora.Typical classification systems (e.g.
decisiontrees, neural networks, instance based learning)require the data to be represented by feature vec-tors.
Developing such a representation for the taskconsidered here is difficult.
Since the top-level ruleselection heuristic is one of the first steps in theparsing process, very little information for makingthis decision is available at the early stage ofparsing.
All the information available at this phaseis provided by the (non-disambiguating) lexicalanalysis that is performed before parsing.
Thispreliminary analysis provides four features: 1)number of potential verbs in the sentence, 2)presence of potential coordinators, 3) verb density,and 4) number of potential auxiliaries.
Asmentioned above, only the first three features areactually used by the current hand-crafted heuristic.However, preliminary experiments have shownthat no interesting knowledge can be inferred byusing only these four features.
We then decided toimprove our representation by the use of DIPETT's308fragmentary parser: an optional parsing mode inwhich DIPETT does not attempt o produce asingle structure for the current input string but,rather, analyses a string as a sequence of majorconstituents (i.e.
noun, verb, prepositional andadverbial phrases).
The new features obtainedfrom fragmentary parsing are: the number offragments, the number of "verbal" fragments(fragments hat contain at least one verb), numberof tokens skipped, and the total percentage of theinput recognised by the fragmentary parser.
Thefragmentary parser is a cost-effective solution toobtain a better representation of sentencesbecause it is very fast---on average, less than onesecond of CPU time for any sentence--incomparison to full parsing.Moreover, the information obtained from thefragmentary parser is adequate for the task athand because it represents well the complexity ofthe sentence to be parsed.
In addition to the featu-res obtained from the lexical analysis and thoseobtained from the fragmentary parser, we use thestring length (number of tokens in the sentence)to describe each sentence.
The attribute used toclassify the sentences, provided by a human ex-pert, is called rule-to-attempt and it can take twovalues: compound or non-compound, accordingto the type of the sentence.
To summarise, weused the ten following features to represent eachsentence: l) string-length: number of tokens(integer); 2) num-potential-verbs: number ofpotential verbs (integer); 3) num-potential-auxiliary:number of potential auxiliaries (integer); 4) verb-density: a flag that indicates if all potential verbs areseparated by coordinators (boolean); 5) nbr-potential,coordinators: number of potential coordinators(integer); 6) num-fragments: number of fragmentsused by the fragmentary parser (integer); 7) num-verbal-fragments: number of fragments that containat least one potential verb (integer); 8) num-tokens-skip: number of tokens not considered by thefragmentary parser (integer); 9) %.input.recognized:percentage ofthe sentence r cognized, i.e.
not skipped(real); 10) rule-to-attempt: type of the sentence(COMPOUND or NON-COMPOUND).We built the first data set by randomlyselecting 300 sentences from four real texts: asoftware user manual, a tax guide, a juniorscience textbook on weather phenomena, nd theBrown corpus.
Each sentence was described interms of the above features, which are of courseacquired automatically by the lexical analyser andthe fragmentary parser, except for rule-to-attemptas mentioned above.
After a preliminary analysisof these 300 sentences, we realised that we had un-balanced numbers of examples of compound andnon-compound sentences: non-compounds areapproximately five times more frequent thancompounds.
However, it is a well-known fact inmachine learning that such unbalanced training setsare not suitable for inductive learning.
For thisreason, we have re-sampled our texts to obtainroughly an equal number of non-compound andcompound sentences (55 compounds and 56 non-compounds).Our experiment consisted in running a varietyof attribute classification systems: IMAFO (\[Famili& Tumey 1991\]), C4.5 (\[Quinlan 1993\]), anddifferent learning algorithms from MLC++(\[Kohavi et al 1994\]).
IMAFO includes an en-hanced version of ID3 and an interface to C4.5 (weused both engines in our experimentation).
MLC++is a machine learning library developed in C++.We experimented with many algorithms includedin MLC++.We concentrated mainly on learning algorithmsthat generate results in the form of rules.
For thisproject, rules are more interesting than other formof results because they are relatively easy tointegrate in a rule-based parser and because theycan be evaluated by experts in the domain.However, for accuracy comparison, we have alsoused learning systems that do not generate rules interms of the initial representation: neural networksand instance-based systems.
We randomly dividedour data set into the training set (2/3 of theexamples, or 74 instances) and the testing set (1/3of the examples, or 37 instances).
Table 1summarises the results obtained from differentsystems in terms of the error rates on the testingset.
All systems gave results with an error ratebelow 20%.SYSTEM Type of systemdecision rulesError rateID3 16.2%C4.5 decision rules 18.9%IMAFO decision rules 16.5%decision rule (one)instance-basedoneR 15.6%IB 10.8%aha-ib instance-based 18.9%belief networks naive-bayesperceptron16.2%neural networks 13.5%Table 1.
Global results from learning.309The error rates presented in Table I for thefirst four systems (decision rules systems) repre-sent the average rates for all rules generated bythese systems.
However, not all rules were parti-cularly interesting.
We kept only some of themfor further evaluation and integration in theparser.
Our selection criteria were: 1) the esti-mated error rate, 2) the "reasonability" (onlyrules that made sense for a computational linguistwere kept), 3) the readability (simple rules arepreferred), and 4) the novelty (we discarded rulesthat are already in the parser).
Tables 2 and 3 pre-sent rules that satisfy all the above the criteria:Table 2 focuses on rules to identify compoundsentences while Table 3 presents rules to identifynon-compound sentences.
The error rate for eachrule is also given.
These error rates were obtainedby a 10 fold cross-validation test.Rules to identify COMPOUND sen-tencesnum-potential-verbs <=3 ANDnum-potential-coordinators > 0 ANDnum-verbal-fra?ments > 1num-fragments > 7num-fragments > 5 ANDnum-verbal-fragments <= 2string-length <= 17 ANDnum-potential-coordinators > 0 ANDnum-verbal-fra?ments > 1num-potential-verbs > 1 ANDnum-potential-verbs <=3 ANDnum-potential-coordinators > 0 ANDnum-fra~ments > 4Errorrate (%)10.59.423.95.44.2num-potential-coordinators > 0 AND 4.3num-fragrnents >= 7num-potential-coordinators > 0 AND 16.8num-verbal-fragments > 1num-potential-coordinators > 0 ANDnum-fragments < 7 AND 4.7string-length <18Table 2.
Rules to identify COMPOUND sentencesThe error rates that we have obtained are quiterespectable for a two-class learning problemgiven the volume of available examples.
More-over, the rules are justified and make sense.
Theyare also very compact in comparison with theoriginal hand-crafted heuristics.
We will see insection 4 how these rules behave on unseen datafrom a totally different ext.Rules to identify NON-COMPOUND sentencesnum-potential-verbs <=3 ANDnum-verbal-fragments <= 1string-length > 10 ANDnum-potential-verbs <=3 ANDnum-fra~ments <= 4string-length <= 21 ANDnum-potential-coordinators = 0Errorrate (%)8.36.75.6num-potential-coordinators = 0 AND 9.7num-fragments <= 7Table 3.
Rules to identify NON-COMPOUND sen-tencesAttribute classification systems uch as those usedduring the experiment reported here are highlysensitive to the adequacy of the features used torepresent he instances.
For our task (parsing),these features were difficult to find and we hadonly a rough idea about their appropriateness.
Forthis reason, we felt that better results could beobtained by transforming the original instancespace into a more adequate space by creating newattributes.
In machine learning research, thisprocess is referred as constructive learning, orconstructive induction (\[Wnek & Michalski 1994\]).We even attempted to use principal componentanalysis (PCA) (\[Johnson & Wichern 1992\]) as atechnique of choice for simple constructivelearning but we did not get very impressive results.We see two reasons for this.
The primary reason isthat the ratio between the number of examples andthe number of attributes is not high enough forPCA to derive high-quality new attributes.
The se-cond reason is that the original attributes are al-ready highly non-redundant.
I  is important o notethat these rules do not satisfy the reasonabilitycriteria applied to the original representation.
Infact, losing the understandability of the attributes ithe usual consequence of almost all approaches thatchange the representation f instances.4 Eva luat ion  o f  the  new ru lesWe explained in section 3 how we derived newparsing heuristics with the help of machinelearning techniques.
The next step was to evaluatehow well would the new rules perform if wereplaced the parser's current hand-crafted heuris-tics with the new ones.
In particular, we wanted toevaluate the accuracy of the heuristics in correctlyidentifying the appropriate rule, COMPOUND orNON COMPOUND, that should first be attempted by310the parser.
This goal was prompted by an earlierevaluation of DIPETT in which it was noted thata good proportion of questionable parses (i.e.either bad parses or correct but too time-consuming parses) were caused by a bad firstattempt, such as attempting COMPOUND instead ofNON_COMPOUND.4.1 From new rules to new parsersOur machine learning experiments lead us to twoclasses of rules obtained from a variety of classi-fiers and concerned only with the notion of com-poundness: 1) those predicting a COMPOUNDsentence, and 2) those predicting aNON_COMPOUND.
The problem was then to de-cide what should be done with the set of newrules.
More precisely, before actually imple-menting the new rules and including them in theparser, we first had to decide on an appropriatestrategy for exploiting the set of new rules.
Wenow describe the three implementations that werealised and evaluated.The first implements only the rules for theCOMPOUND class---one big rule which is a dis-junct of all the learned rules for that class.
Andsince there are only two alternatives, eitherCOMPOUND or NON_COMPOUND, if none of theCOMPOUND rules applies, the NON_COMPOUNDclass is predicted.
This first implementation is re-ferred to as C-Imp.
The second implementation,referred to as NC-Imp, does exactly the opposite:i.e.
it implements only the rules predicting theNON_COMPOUND class.The third implementation, referred to asNC_C-Imp, benefits from the first two imple-mentations.
The class of a new sentence is deter-mined by combining the output from C-Imp andNC-Imp.
The combination of the output is doneaccording to the following decision table in Table4.C-Imp NC-Imp .\] Output ofI NC_C-ImpC C CNC NC NCNC C NCC NC NCTable 4.
Decision table used in the NC_C imple-mentation.The first two lines of this decision table are ob-vious since the outputs from both implementationsare consistent.
When the two implementationsdisagree, the NC_C-Imp implementation predictsthe non-compound.
This prediction is justified by abayesian argumentation.
In the absence of anyadditional knowledge, we are forced to assign anequal probability of success to each of the two setsof rules and the most probable class becomes theone with the highest frequency.
Thus, in general,non-compound sentences are more frequent hencompound ones.
One obvious way to improve thisthird implementation would be to preciselyevaluate the accuracies of the two sets of rules andthen incorporate these accuracies in the decisionprocess.4.2 The resultsTo perform the evaluation, we randomly sampled200 sentences from a new corpus on mechanics(\[Atkinson 1990\]): note that this text had not beenused to sample the sentences used for learning.
Outof these 200 sentences, 10 were discarded sincethey were not representative (e.g.
one-word"sentences").
We ran the original implementationof DIPETT plus the three new implementationsdescribed in the previous ection on the remaining190 test sentences.
Table 5 presents the results.
Theerror-rate, the standard eviation of the error-rateand the p-value are listed for each implementation.The p-value gives the probability that DIPETT'soriginal hand-crafted heuristics are better than thenew heuristics.
In other words, a small p-valuemeans an increase in performance with a highprobability.ImplementationOriginal heur.C-ImpNC-ImpNC_C-ImpErr- Std.
p-valuerate dev.
(%)25.268 ?3.220.526 ?2.9 0.12622.105 ?3.0 0.22916.316 ?2.7 0.009Table 5.
Performances of the new implementationsversus DIPETT's original heuristics.We observe that all new automatically-derivedheuristics did beat DIPETT's hand-crafted heu-ristics and quite clearly.
The results from the thirdimplementation (i.e.
NC_C-Imp) are especiallyremarkable: with a confidence of over 99%, we can311affirm that the NC_C-lmplementation willoutperform DIPETT's original heuristic.
We alsonote that the error rate drops by 35% of its valuefor the original heuristic.
Similarly, with a confi-dence of 87.4%, we can affirm that the imple-mentation that uses only the C-rules (i.e.
C-Imp)will perform better then DIPETT's current heu-ristics.These very good results are also amplified bythe fact that the testing described in this evalua-tion was done on sentences totally independentfrom the ones used for training.
Usually, in ma-chine learning research, the training and the tes-ting sets are sampled from the same original dataset, and the kind of "out-of-sample" testing thatwe perform here has only recently come to theattention of the learning community (\[Ezawa etal.
1996\]).
Our experiments have shown that it ispossible to infer rules that perform very well andare highly meaningful in the eyes of an experteven if the training set is relatively small.
Thisindicates that the representation f sentences thatwe chose for the problem was adequate.
Finally,an other important output of our research is theidentification of the most significant attributes todistinguish non-compound sentences from com-pound ones.
This alone is valuable information toa computational linguist.
Only five out of tenoriginal attributes are used by the learned rules,and all of them are cheap to compute: two attri-butes are derived by fragmentary parsing (num-ber of verbal fragments and number of frag-ments), and three are lexical (number of potentialverbs, length of the input string, and presence ofpotential coordinators).5 Related WorkThere have been successful attempts at using ma-chine learning in search of a solution for linguis-tic tasks, e.g.
discriminating between discourseand sentential senses of cues (\[Litman 1996\]) orresolution of coreferences in texts (\[McCarthy &Lehnert 1995\]).
Like our work, these problemsare cast as classification problems, and then ma-chine learning (mainly C4.5) techniques are usedto induce classifiers for each class.
What makes"these applications different from ours is that theyhave worked on surface linguistic or mixed surfa-ce linguistic and intonational representation, andthat the classes are relatively balanced, while inour case the class of compound sentences i muchless numerous than the class of non-compositesentences.
Such unbalanced classes create prob-lems for the majority of inductive learning systems.A distinctive feature of our work is the fact thatwe used machine learning techniques to improvean existing rule-based natural language processorfrom the inside.
This contrasts with approacheswhere there are essentially no explicit rules, suchas neural networks (e.g.
\[Buo 1996\]), orapproaches where the machine learning algorithmsattempt o infer--via deduction (e.g.
\[Samuelsson1994\]), induction (e.g.
\[Theeramunkong et al1997\]; \[Zelle & Mooney 1994\]) under user coope-ration (e.g.
\[Simmons & Yu 1992\]; \[Hermjakob &Mooney 1997\]), transformation-based error-drivenlearning (e.g.
\[Brill 1993\]), or even decision trees(e.g.
\[Magerman 1995\])--a grammar from raw orpreprocessed data.
In our work, we do not wish toacquire a grammar: we have one and want to de-vise a mechanism to make some of its partsadaptable to the corpus at hand or, to improvesome aspect of its performance.
Other researchers,such as \[Lawrence t al.
1996\], have comparedneural networks and machine learning methods atthe task of sentence classification.
In this task, thesystem must classify a string as either grammaticalor not.
We do not content ourselves with resultsbased on a grammatical/ungrammatical d hotomy.We are looking for heuristics, using relevantfeatures, that will do better than the current onesand improve the overall performance of a naturallanguage processor: this is a very difficult problem(see, e.g., \[Huyck & Lytinen 1993\]).
One couldalso look at this problem as one of optimisation ofa rule-based system.Work somewhat related to ours was conductedby \[Samuelsson 1994\] who used explanation-basedgeneralisation to extract a subset of a grammar thatwould parse a given corpus faster than the original,larger grammar \[Neumann 1997\] also used EBLbut for a generation task.
In our case, we are notlooking for a subset of the existing rules but, rather,we are looking for brand new rules that wouldreplace and outperform the existing rules.
Weshould also mention the work of \[Soderland 1997\]who also worked on the comparison ofautomatically learned and hand-crafted rules fortext analysis.3126 ConclusionWe have presented an experiment which demon-strates that machine learning may be used as atechnique to optimise in an adaptive manner thehigh-level decisions that any parser must make inthe presence of incomplete information about theproperties of the text it analyses.
The results howclearly that simple and understandable ruleslearned by machine learning techniques can sur-pass the performance of heuristics upplied by anexperienced computational linguist.
Moreover,these very encouraging results indicate that therepresentation that we chose and discuss was anadequate one for this problem.
We feel that amethodology is at hand to extend and deepen thisapproach to language processing programs ingeneral.
The methodology consists of three mainsteps: I) feature engineering, 2) learning, usingseveral different available learners, 3) evaluation,with the recommendation f using the "out-of-sample" approach to testing.
Future work will fo-cus on improvements to constructive l arning; onnew ways of integrating the rules acquired by dif-ferent learners in the parser; and on the identifi-cation of criteria for selecting parser rules thathave the best potential to benefit from the gene-ralisation of our results.AcknowledgementsThe work described here was supported by the NaturalSciences and Engineering Research Council of Canada.ReferencesAtkinson, H.F. (1990) Mechanics of Small Engines.
NewYork: Gregg Division, McGraw-Hill.Brill E. (1993) "Automatic Grammar Induction and ParsingFree Text: A Transformation-Based Approach", Proc.
ofthe 31st Annual Meeting of the ACL, pp.259-265.Buo F.D.
(1996) "FeasPar--A Feature Structure ParserLearning to Parse Spontaneous Speech", Ph.D. Thesis,Fakultiit ftir Informatik, Univ.
Karlsruhe, Germany.Delisle S. (1994) "Text Processing without a priori DomainKnowledge: Semi-Automatic Linguistic for IncrementalKnowledge Acquisition", Ph.D. Thesis, Dept.
of Compu-ter Science, Univ.
of Ottawa.
Published as technical reportTR-94-02.Ezawa K., Singh M. & Norton S. (1996) "Learning GoalOriented Bayesian Networks for TelecommunicationsRisk Management", Proc.
of the 13th International Conf.on Machine Learning, pp.
139-147.Famili A.
& Turney P. (1991) "Intelligently Helping theHuman Planner in Industrial Process Planing", AI EDAM -AI for Engineering Design Analysis and Manufacturing,5 (2), pp.
109-124.Hermjakob U.
& Mooney R.J. (1997) "Learning Parse andTranslation Decisions From Examples With Rich Context",Proc.
of ACL-EACL Conf., pp.482-489.Huyck C.R.
& Lytinen S.L.
(1993) "Efficient HeuristicNatural Language Parsing", Proc.
of the llth NationalConf.
on AI, pp.386-391.Johnson R.A. & Wichern D.W. (1992) Applied MultivariateStatistical Analysis, Prentice Hall.Kohavi R., John G., Long R., Manley D. & Pleger K. (1994)"MLC++: A machine learning library in C++", Tools withAI, IEEE Computer Society Press, pp.740-743.Lawrence S., Fong S. & Lee Giles C. (1996) "Natural Lan-guage Grammatical Inference: A Comparison of RecurrentNeural Networks and Machine Learning Methods", in S.Wermter, E. Riloff and G. Scheler (eds.
), Symbolic,Connectionnist, and Statistical Approaches to Learning forNatural Language Processing, Lectures Notes in AI,Springer-Verlag, pp.33-47.Litman D. (1996) "Cue Phrase Classification Using MachineLearning', Journal of Al Research, 5, pp.53-95.Magerman D. (1995) "Statistical Decision-Tree Models forParsing", Proc.
of the 33rd Annual Meeting of the ACL,276-283.McCarthy J.
& Lehnert W.G.
(1995) "Using Decision Treesfor Coreference Resolution", Proc.
of IJCAI-95, pp.1050-1055.Neumann G. (1997) "Applying Explanation-based Learning toControl and Speeding-up Natural Language Generation",Proc.
of ACL-EACL Conf., pp.214-221.Quinlan J.R. (1993) C4.5: Programs for Machine Learning,Morgan Kaufmann.Quirk R., Greenbaum S., Leech G. & Svartvik J.
0985) AComprehensive Grammar of the English Language,Longman.Samuelsson C. (1994) "Grammar Specialization ThroughEntropy Thresholds", Proc.
of the 32nd Annual Meeting ofthe ACL, pp.188-195.Simmons F.S.
& Yu Y.H.
(1992) "The Acquisition and Use ofContext-dependent Grammars for English", ComputationalLinguistics, 18(4), pp.392-418.Soderland S.G. (1997) "Learning Text Analysis Rules forDomain-Specific Natural Language Processing", Ph.D.Thesis, Dept.
of Computer Science, Univ.
of Massachusetts.Theeramunkong T., Kawaguchi Y.
& Okumura (1997)"Exploiting Contextual Information i  Hypothesis Selectionfor Grammar Refinement", Proc.
of the CEGDLE Workshopat ACL-EACL'97, pp.78-83.Wnek J.
& Michalski R.S.
(1994) "Hypothesis-driven cons-tructive induction in AQ17-HCI: a method and experi-ments", Machine Learning, 14(2), pp.
139-168.Zelle J.M.
& Mooney R.J. (1994) "Inducing DeterministicProlog Parsers from Treebanks: A Machine Learning Ap-proach", Proc.
of the 12th National Conf.
on AI, pp.748-753.313
