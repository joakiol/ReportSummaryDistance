Discriminative Training and Maximum Entropy Models for StatisticalMachine TranslationFranz Josef Och and Hermann NeyLehrstuhl fu?r Informatik VI, Computer Science DepartmentRWTH Aachen - University of TechnologyD-52056 Aachen, Germany{och,ney}@informatik.rwth-aachen.deAbstractWe present a framework for statisticalmachine translation of natural languagesbased on direct maximum entropy mod-els, which contains the widely used sour-ce-channel approach as a special case.
Allknowledge sources are treated as featurefunctions, which depend on the sourcelanguage sentence, the target languagesentence and possible hidden variables.This approach allows a baseline machinetranslation system to be extended easily byadding new feature functions.
We showthat a baseline statistical machine transla-tion system is significantly improved us-ing this approach.1 IntroductionWe are given a source (?French?)
sentence fJ1 =f1, .
.
.
, fj , .
.
.
, fJ , which is to be translated into atarget (?English?)
sentence eI1 = e1, .
.
.
, ei, .
.
.
, eI .Among all possible target sentences, we will choosethe sentence with the highest probability:1e?I1 = argmaxeI1{Pr(eI1|fJ1 )} (1)The argmax operation denotes the search problem,i.e.
the generation of the output sentence in the targetlanguage.1The notational convention will be as follows.
We use thesymbol Pr(?)
to denote general probability distributions with(nearly) no specific assumptions.
In contrast, for model-basedprobability distributions, we use the generic symbol p(?
).1.1 Source-Channel ModelAccording to Bayes?
decision rule, we can equiva-lently to Eq.
1 perform the following maximization:e?I1 = argmaxeI1{Pr(eI1) ?
Pr(fJ1 |eI1)} (2)This approach is referred to as source-channel ap-proach to statistical MT.
Sometimes, it is also re-ferred to as the ?fundamental equation of statisti-cal MT?
(Brown et al, 1993).
Here, Pr(eI1) isthe language model of the target language, whereasPr(fJ1 |eI1) is the translation model.
Typically, Eq.
2is favored over the direct translation model of Eq.
1with the argument that it yields a modular approach.Instead of modeling one probability distribution,we obtain two different knowledge sources that aretrained independently.The overall architecture of the source-channel ap-proach is summarized in Figure 1.
In general, asshown in this figure, there may be additional trans-formations to make the translation task simpler forthe algorithm.
Typically, training is performed byapplying a maximum likelihood approach.
If thelanguage model Pr(eI1) = p?
(eI1) depends on pa-rameters ?
and the translation model Pr(fJ1 |eI1) =p?
(fJ1 |eI1) depends on parameters ?, then the opti-mal parameter values are obtained by maximizingthe likelihood on a parallel training corpus fS1 , eS1(Brown et al, 1993):??
= argmax?S?s=1p?
(fs|es) (3)??
= argmax?S?s=1p?
(es) (4)Computational Linguistics (ACL), Philadelphia, July 2002, pp.
295-302.Proceedings of the 40th Annual Meeting of the Association forSourceLanguage Text?
?PreprocessingPr(eI1): Language ModelooGlobal Searche?I1 = argmaxeI1{Pr(eI1) ?
Pr(fJ1 |eI1)}???
?Pr(fJ1 |eI1): Translation ModelooPostprocessing?
?TargetLanguage TextFigure 1: Architecture of the translation approach based on source-channel models.We obtain the following decision rule:e?I1 = argmaxeI1{p??
(eI1) ?
p??
(fJ1 |eI1)} (5)State-of-the-art statistical MT systems are based onthis approach.
Yet, the use of this decision rule hasvarious problems:1.
The combination of the language model p??
(eI1)and the translation model p??
(fJ1 |eI1) as shownin Eq.
5 can only be shown to be optimal if thetrue probability distributions p??
(eI1) = Pr(eI1)and p??
(fJ1 |eI1) = Pr(fJ1 |eI1) are used.
Yet,we know that the used models and trainingmethods provide only poor approximations ofthe true probability distributions.
Therefore, adifferent combination of language model andtranslation model might yield better results.2.
There is no straightforward way to extend abaseline statistical MT model by including ad-ditional dependencies.3.
Often, we observe that comparable results areobtained by using the following decision ruleinstead of Eq.
5 (Och et al, 1999):e?I1 = argmaxeI1{p??
(eI1) ?
p??
(eI1|fJ1 )} (6)Here, we replaced p??
(fJ1 |eI1) by p??
(eI1|fJ1 ).From a theoretical framework of the source-channel approach, this approach is hard to jus-tify.
Yet, if both decision rules yield the sametranslation quality, we can use that decisionrule which is better suited for efficient search.1.2 Direct Maximum Entropy TranslationModelAs alternative to the source-channel approach, wedirectly model the posterior probability Pr(eI1|fJ1 ).An especially well-founded framework for doingthis is maximum entropy (Berger et al, 1996).
Inthis framework, we have a set of M feature func-tions hm(eI1, fJ1 ),m = 1, .
.
.
,M .
For each featurefunction, there exists a model parameter ?m,m =1, .
.
.
,M .
The direct translation probability is givenSourceLanguage Text??Preprocessing??
?1 ?
h1(eI1, fJ1 )ooGlobal SearchargmaxeI1{ M?m=1?mhm(eI1, fJ1 )}??
?2 ?
h2(eI1, fJ1 )oo.
.
.ooPostprocessing?
?TargetLanguage TextFigure 2: Architecture of the translation approach based on direct maximum entropy models.by:Pr(eI1|fJ1 ) = p?M1 (eI1|fJ1 ) (7)= exp[?Mm=1 ?mhm(eI1, fJ1 )]?e?I1 exp[?Mm=1 ?mhm(e?I1, fJ1 )](8)This approach has been suggested by (Papineni etal., 1997; Papineni et al, 1998) for a natural lan-guage understanding task.We obtain the following decision rule:e?I1 = argmaxeI1{Pr(eI1|fJ1 )}= argmaxeI1{ M?m=1?mhm(eI1, fJ1 )}Hence, the time-consuming renormalization in Eq.
8is not needed in search.
The overall architecture ofthe direct maximum entropy models is summarizedin Figure 2.Interestingly, this framework contains as specialcase the source channel approach (Eq.
5) if we usethe following two feature functions:h1(eI1, fJ1 ) = log p??
(eI1) (9)h2(eI1, fJ1 ) = log p??
(fJ1 |eI1) (10)and set ?1 = ?2 = 1.
Optimizing the correspondingparameters ?1 and ?2 of the model in Eq.
8 is equiv-alent to the optimization of model scaling factors,which is a standard approach in other areas such asspeech recognition or pattern recognition.The use of an ?inverted?
translation model in theunconventional decision rule of Eq.
6 results if weuse the feature function logPr(eI1|fJ1 ) instead oflogPr(fJ1 |eI1).
In this framework, this feature canbe as good as logPr(fJ1 |eI1).
It has to be empiricallyverified, which of the two features yields better re-sults.
We even can use both features logPr(eI1|fJ1 )and logPr(fJ1 |eI1), obtaining a more symmetrictranslation model.As training criterion, we use the maximum classposterior probability criterion:?
?M1 = argmax?M1{ S?s=1log p?M1 (es|fs)}(11)This corresponds to maximizing the equivocationor maximizing the likelihood of the direct transla-tion model.
This direct optimization of the poste-rior probability in Bayes decision rule is referred toas discriminative training (Ney, 1995) because wedirectly take into account the overlap in the proba-bility distributions.
The optimization problem hasone global optimum and the optimization criterionis convex.1.3 Alignment Models and MaximumApproximationTypically, the probability Pr(fJ1 |eI1) is decomposedvia additional hidden variables.
In statistical align-ment models Pr(fJ1 , aJ1 |eI1), the alignment aJ1 is in-troduced as a hidden variable:Pr(fJ1 |eI1) =?aJ1Pr(fJ1 , aJ1 |eI1)The alignment mapping is j ?
i = aj from sourceposition j to target position i = aj .Search is performed using the so-called maximumapproximation:e?I1 = argmaxeI1??
?Pr(eI1) ?
?aJ1Pr(fJ1 , aJ1 |eI1)????
argmaxeI1{Pr(eI1) ?maxaJ1Pr(fJ1 , aJ1 |eI1)}Hence, the search space consists of the set of all pos-sible target language sentences eI1 and all possiblealignments aJ1 .Generalizing this approach to direct translationmodels, we extend the feature functions to in-clude the dependence on the additional hidden vari-able.
Using M feature functions of the formhm(eI1, fJ1 , aJ1 ),m = 1, .
.
.
,M , we obtain the fol-lowing model:Pr(eI1, aJ1 |fJ1 ) ==exp(?Mm=1 ?mhm(eI1, fJ1 , aJ1 ))?e?I1,a?J1 exp(?Mm=1 ?mhm(e?I1, fJ1 , a?J1 ))Obviously, we can perform the same step for transla-tion models with an even richer structure of hiddenvariables than only the alignment aJ1 .
To simplifythe notation, we shall omit in the following the de-pendence on the hidden variables of the model.2 Alignment TemplatesAs specific MT method, we use the alignment tem-plate approach (Och et al, 1999).
The key elementsof this approach are the alignment templates, whichare pairs of source and target language phrases to-gether with an alignment between the words withinthe phrases.
The advantage of the alignment tem-plate approach compared to single word-based sta-tistical translation models is that word context andlocal changes in word order are explicitly consid-ered.The alignment template model refines the transla-tion probability Pr(fJ1 |eI1) by introducing two hid-den variables zK1 and aK1 for the K alignment tem-plates and the alignment of the alignment templates:Pr(fJ1 |eI1) =?zK1 ,aK1Pr(aK1 |eI1) ?Pr(zK1 |aK1 , eI1) ?
Pr(fJ1 |zK1 , aK1 , eI1)Hence, we obtain three different probabilitydistributions: Pr(aK1 |eI1), Pr(zK1 |aK1 , eI1) andPr(fJ1 |zK1 , aK1 , eI1).
Here, we omit a detailed de-scription of modeling, training and search, as this isnot relevant for the subsequent exposition.
For fur-ther details, see (Och et al, 1999).To use these three component models in a directmaximum entropy approach, we define three dif-ferent feature functions for each component of thetranslation model instead of one feature function forthe whole translation model p(fJ1 |eI1).
The featurefunctions have then not only a dependence on fJ1and eI1 but also on zK1 , aK1 .3 Feature functionsSo far, we use the logarithm of the components ofa translation model as feature functions.
This is avery convenient approach to improve the quality ofa baseline system.
Yet, we are not limited to trainonly model scaling factors, but we have many possi-bilities:?
We could add a sentence length feature:h(fJ1 , eI1) = IThis corresponds to a word penalty for eachproduced target word.?
We could use additional language models byusing features of the following form:h(fJ1 , eI1) = h(eI1)?
We could use a feature that counts how manyentries of a conventional lexicon co-occur inthe given sentence pair.
Therefore, the weightfor the provided conventional dictionary can belearned.
The intuition is that the conventionaldictionary is expected to be more reliable thanthe automatically trained lexicon and thereforeshould get a larger weight.?
We could use lexical features, which fire if acertain lexical relationship (f, e) occurs:h(fJ1 , eI1) =??J?j=1?
(f, fj)??
?
( I?i=1?
(e, ei))?
We could use grammatical features that relatecertain grammatical dependencies of sourceand target language.
For example, using a func-tion k(?)
that counts how many verb groups ex-ist in the source or the target sentence, we candefine the following feature, which is 1 if eachof the two sentences contains the same numberof verb groups:h(fJ1 , eI1) = ?
(k(fJ1 ), k(eI1)) (12)In the same way, we can introduce semanticfeatures or pragmatic features such as the di-alogue act classification.We can use numerous additional features that dealwith specific problems of the baseline statistical MTsystem.
In this paper, we shall use the first three ofthese features.
As additional language model, weuse a class-based five-gram language model.
Thisfeature and the word penalty feature allow a straight-forward integration into the used dynamic program-ming search algorithm (Och et al, 1999).
As this isnot possible for the conventional dictionary feature,we use n-best rescoring for this feature.4 TrainingTo train the model parameters ?M1 of the direct trans-lation model according to Eq.
11, we use the GIS(Generalized Iterative Scaling) algorithm (Darrochand Ratcliff, 1972).
It should be noted that, aswas already shown by (Darroch and Ratcliff, 1972),by applying suitable transformations, the GIS algo-rithm is able to handle any type of real-valued fea-tures.
To apply this algorithm, we have to solve var-ious practical problems.The renormalization needed in Eq.
8 requires asum over a large number of possible sentences,for which we do not know an efficient algorithm.Hence, we approximate this sum by sampling thespace of all possible sentences by a large set ofhighly probable sentences.
The set of consideredsentences is computed by an appropriately extendedversion of the used search algorithm (Och et al,1999) computing an approximate n-best list of trans-lations.Unlike automatic speech recognition, we do nothave one reference sentence, but there exists a num-ber of reference sentences.
Yet, the criterion as itis described in Eq.
11 allows for only one referencetranslation.
Hence, we change the criterion to al-low Rs reference translations es,1, .
.
.
, es,Rs for thesentence es:?
?M1 = argmax?M1{ S?s=11RsRs?r=1log p?M1 (es,r|fs)}We use this optimization criterion instead of the op-timization criterion shown in Eq.
11.In addition, we might have the problem that nosingle of the reference translations is part of the n-best list because the search algorithm performs prun-ing, which in principle limits the possible transla-tions that can be produced given a certain input sen-tence.
To solve this problem, we define for max-imum entropy training each sentence as referencetranslation that has the minimal number of word er-rors with respect to any of the reference translations.5 ResultsWe present results on the VERBMOBIL task, whichis a speech translation task in the domain of appoint-ment scheduling, travel planning, and hotel reser-vation (Wahlster, 1993).
Table 1 shows the cor-pus statistics of this task.
We use a training cor-pus, which is used to train the alignment templatemodel and the language models, a development cor-pus, which is used to estimate the model scaling fac-tors, and a test corpus.Table 1: Characteristics of training corpus (Train),manual lexicon (Lex), development corpus (Dev),test corpus (Test).German EnglishTrain Sentences 58 073Words 519 523 549 921Singletons 3 453 1 698Vocabulary 7 939 4 672Lex Entries 12 779Ext.
Vocab.
11 501 6 867Dev Sentences 276Words 3 159 3 438PP (trigr.
LM) - 28.1Test Sentences 251Words 2 628 2 871PP (trigr.
LM) - 30.5So far, in machine translation research does notexist one generally accepted criterion for the evalu-ation of the experimental results.
Therefore, we usea large variety of different criteria and show that theobtained results improve on most or all of these cri-teria.
In all experiments, we use the following sixerror criteria:?
SER (sentence error rate): The SER is com-puted as the number of times that the generatedsentence corresponds exactly to one of the ref-erence translations used for the maximum en-tropy training.?
WER (word error rate): The WER is computedas the minimum number of substitution, inser-tion and deletion operations that have to be per-formed to convert the generated sentence intothe target sentence.?
PER (position-independent WER): A short-coming of the WER is the fact that it requiresa perfect word order.
The word order of anacceptable sentence can be different from thatof the target sentence, so that the WER mea-sure alone could be misleading.
To overcomethis problem, we introduce as additional mea-sure the position-independent word error rate(PER).
This measure compares the words in thetwo sentences ignoring the word order.?
mWER (multi-reference word error rate): Foreach test sentence, there is not only used a sin-gle reference translation, as for the WER, buta whole set of reference translations.
For eachtranslation hypothesis, the edit distance to themost similar sentence is calculated (Nie?en etal., 2000).?
BLEU score: This score measures the precisionof unigrams, bigrams, trigrams and fourgramswith respect to a whole set of reference trans-lations with a penalty for too short sentences(Papineni et al, 2001).
Unlike all other eval-uation criteria used here, BLEU measures ac-curacy, i.e.
the opposite of error rate.
Hence,large BLEU scores are better.?
SSER (subjective sentence error rate): For amore detailed analysis, subjective judgmentsby test persons are necessary.
Each trans-lated sentence was judged by a human exam-iner according to an error scale from 0.0 to 1.0(Nie?en et al, 2000).?
IER (information item error rate): The test sen-tences are segmented into information items.For each of them, if the intended informationis conveyed and there are no syntactic errors,the sentence is counted as correct (Nie?en etal., 2000).In the following, we present the results of this ap-proach.
Table 2 shows the results if we use a directtranslation model (Eq.
6).As baseline features, we use a normal word tri-gram language model and the three component mod-els of the alignment templates.
The first row showsthe results using only the four baseline features with?1 = ?
?
?
= ?4 = 1.
The second row shows theresult if we train the model scaling factors.
We see asystematic improvement on all error rates.
The fol-lowing three rows show the results if we add theword penalty, an additional class-based five-gramTable 2: Effect of maximum entropy training for alignment template approach (WP: word penalty feature,CLM: class-based language model (five-gram), MX: conventional dictionary).objective criteria [%] subjective criteria [%]SER WER PER mWER BLEU SSER IERBaseline(?m = 1) 86.9 42.8 33.0 37.7 43.9 35.9 39.0ME 81.7 40.2 28.7 34.6 49.7 32.5 34.8ME+WP 80.5 38.6 26.9 32.4 54.1 29.9 32.2ME+WP+CLM 78.1 38.3 26.9 32.1 55.0 29.1 30.9ME+WP+CLM+MX 77.8 38.4 26.8 31.9 55.2 28.8 30.90.740.760.780.80.820.840.860.880.90 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000sentence error rate(SER) number of iterationsMEME+WPME+WP+CLMME+WP+CLM+MXFigure 3: Test error rate over the iterations of theGIS algorithm for maximum entropy training ofalignment templates.language model and the conventional dictionary fea-tures.
We observe improved error rates for using theword penalty and the class-based language model asadditional features.Figure 3 show how the sentence error rate (SER)on the test corpus improves during the iterations ofthe GIS algorithm.
We see that the sentence errorrates converges after about 4000 iterations.
We donot observe significant overfitting.Table 3 shows the resulting normalized modelscaling factors.
Multiplying each model scaling fac-tor by a constant positive value does not affect thedecision rule.
We see that adding new features alsohas an effect on the other model scaling factors.6 Related WorkThe use of direct maximum entropy translation mod-els for statistical machine translation has been sug-Table 3: Resulting model scaling factors of maxi-mum entropy training for alignment templates; ?1:trigram language model; ?2: alignment templatemodel, ?3: lexicon model, ?4: alignment model(normalized such that ?4m=1 ?m = 4).ME +WP +CLM +MX?1 0.86 0.98 0.75 0.77?2 2.33 2.05 2.24 2.24?3 0.58 0.72 0.79 0.75?4 0.22 0.25 0.23 0.24WP ?
2.6 3.03 2.78CLM ?
?
0.33 0.34MX ?
?
?
2.92gested by (Papineni et al, 1997; Papineni et al,1998).
They train models for natural language un-derstanding rather than natural language translation.In contrast to their approach, we include a depen-dence on the hidden variable of the translation modelin the direct translation model.
Therefore, we areable to use statistical alignment models, which havebeen shown to be a very powerful component forstatistical machine translation systems.In speech recognition, training the parameters ofthe acoustic model by optimizing the (average) mu-tual information and conditional entropy as they aredefined in information theory is a standard approach(Bahl et al, 1986; Ney, 1995).
Combining variousprobabilistic models for speech and language mod-eling has been suggested in (Beyerlein, 1997; Petersand Klakow, 1999).7 ConclusionsWe have presented a framework for statistical MTfor natural languages, which is more general than thewidely used source-channel approach.
It allows abaseline MT system to be extended easily by addingnew feature functions.
We have shown that a base-line statistical MT system can be significantly im-proved using this framework.There are two possible interpretations for a statis-tical MT system structured according to the source-channel approach, hence including a model forPr(eI1) and a model for Pr(fJ1 |eI1).
We can inter-pret it as an approximation to the Bayes decision rulein Eq.
2 or as an instance of a direct maximum en-tropy model with feature functions logPr(eI1) andlogPr(fJ1 |eI1).
As soon as we want to use modelscaling factors, we can only do this in a theoreticallyjustified way using the second interpretation.
Yet,the main advantage comes from the large number ofadditional possibilities that we obtain by using thesecond interpretation.An important open problem of this approach isthe handling of complex features in search.
An in-teresting question is to come up with features thatallow an efficient handling using conventional dy-namic programming search algorithms.In addition, it might be promising to optimize theparameters directly with respect to the error rate ofthe MT system as is suggested in the field of patternand speech recognition (Juang et al, 1995; Schlu?terand Ney, 2001).ReferencesL.
R. Bahl, P. F. Brown, P. V. de Souza, and R. L. Mer-cer.
1986.
Maximum mutual information estimationof hidden markov model parameters.
In Proc.
Int.Conf.
on Acoustics, Speech, and Signal Processing,pages 49?52, Tokyo, Japan, April.A.
L. Berger, S. A. Della Pietra, and V. J. DellaPietra.
1996.
A maximum entropy approach to nat-ural language processing.
Computational Linguistics,22(1):39?72, March.P.
Beyerlein.
1997.
Discriminative model combina-tion.
In Proc.
of the IEEE Workshop on AutomaticSpeech Recognition and Understanding, pages 238?245, Santa Barbara, CA, December.P.
F. Brown, S. A. Della Pietra, V. J. Della Pietra, andR.
L. Mercer.
1993.
The mathematics of statisticalmachine translation: Parameter estimation.
Computa-tional Linguistics, 19(2):263?311.J.
N. Darroch and D. Ratcliff.
1972.
Generalized itera-tive scaling for log-linear models.
Annals of Mathe-matical Statistics, 43:1470?1480.B.
H. Juang, W. Chou, and C. H. Lee.
1995.
Statisti-cal and discriminative methods for speech recognition.In A. J. R. Ayuso and J. M. L. Soler, editors, SpeechRecognition and Coding - New Advances and Trends.Springer Verlag, Berlin, Germany.H.
Ney.
1995.
On the probabilistic-interpretation ofneural-network classifiers and discriminative trainingcriteria.
IEEE Trans.
on Pattern Analysis and MachineIntelligence, 17(2):107?119, February.S.
Nie?en, F. J. Och, G. Leusch, and H. Ney.
2000.An evaluation tool for machine translation: Fast eval-uation for MT research.
In Proc.
of the Second Int.Conf.
on Language Resources and Evaluation (LREC),pages 39?45, Athens, Greece, May.F.
J. Och, C. Tillmann, and H. Ney.
1999.
Improvedalignment models for statistical machine translation.In Proc.
of the Joint SIGDAT Conf.
on Empirical Meth-ods in Natural Language Processing and Very LargeCorpora, pages 20?28, University of Maryland, Col-lege Park, MD, June.K.
A. Papineni, S. Roukos, and R. T. Ward.
1997.Feature-based language understanding.
In EuropeanConf.
on Speech Communication and Technology,pages 1435?1438, Rhodes, Greece, September.K.
A. Papineni, S. Roukos, and R. T. Ward.
1998.
Max-imum likelihood and discriminative training of directtranslation models.
In Proc.
Int.
Conf.
on Acoustics,Speech, and Signal Processing, pages 189?192, Seat-tle, WA, May.K.
A. Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2001.Bleu: a method for automatic evaluation of machinetranslation.
Technical Report RC22176 (W0109-022),IBM Research Division, Thomas J. Watson ResearchCenter, Yorktown Heights, NY, September.J.
Peters and D. Klakow.
1999.
Compact maximum en-tropy language models.
In Proc.
of the IEEE Workshopon Automatic Speech Recognition and Understanding,Keystone, CO, December.R.
Schlu?ter and H. Ney.
2001.
Model-based MCE boundto the true Bayes?
error.
IEEE Signal Processing Let-ters, 8(5):131?133, May.W.
Wahlster.
1993.
Verbmobil: Translation of face-to-face dialogs.
In Proc.
of MT Summit IV, pages 127?135, Kobe, Japan, July.
