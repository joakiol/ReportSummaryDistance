Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora 2006, pages 29?37,Sydney, July 2006. c?2006 Association for Computational LinguisticsFrontiers in Linguistic Annotation for Lower-Density LanguagesMike MaxwellCenter for Advanced Study of LanguageUniversity of Marylandmmaxwell@casl.umd.eduBaden HughesDepartment of Computer ScienceThe University of Melbournebadenh@csse.unimelb.edu.auAbstractThe languages that are most commonlysubject to linguistic annotation on a largescale tend to be those with the largest pop-ulations or with recent histories of lin-guistic scholarship.
In this paper we dis-cuss the problems associated with lower-density languages in the context of the de-velopment of linguistically annotated re-sources.
We frame our work with threekey questions regarding the definition oflower-density languages; increasing avail-able resources and reducing data require-ments.
A number of steps forward areidentified for increasing the number lower-density language corpora with linguisticannotations.1 IntroductionThe process for selecting a target language for re-search activity in corpus linguistics, natural lan-guage processing or computational linguistics islargely arbitrary.
To some extent, the motivationfor a specific choice is based on one or more of arange of factors: the number of speakers of a givenlanguage; the economic and social dominance ofthe speakers; the extent to which computationaland/or lexical resources already exist; the avail-ability of these resources in a manner conducive toresearch activity; the level of geopolitical supportfor language-specific activity, or the sensitivity ofthe language in the political arena; the degree towhich the researchers are likely to be appreciatedby the speakers of the language simply becauseof engagement; and the potential scientific returnsfrom working on the language in question (includ-ing the likelihood that the language exhibits inter-esting or unique phenomena).
Notably, these fac-tors are also significant in determining whether alanguage is worked on for documentary and de-scriptive purposes, although an additional factorin this particular area is also the degree of endan-germent (which can perhaps be contrasted with thelikelihood of economic returns for computationalendeavour).As a result of these influencing factors, it isclear that languages which exhibit positive effectsin one or more of these areas are likely to be thetarget of computational research.
If we considerthe availability of computationally tractable lan-guage resources, we find, unsuprisingly that majorlanguages such as English, German, French andJapanese are dominant; and research on computa-tional approaches to linguistic analysis tends to befarthest advanced in these languages.However, renewed interest in the annotation oflower-density languages has arisen for a numberof reasons, both theoretical and practical.
In thispaper we discuss the problems associated withlower-density languages in the context of the de-velopment of linguistically annotated resources.The structure of this paper is as follows.
Firstwe define the lower-density languages and lin-guistically annotated resources, thus defining thescope of our interest.
We review some relatedwork in the area of linguistically annotated cor-pora for lower-density languages.
Next we posethree questions which frame the body of this pa-per: What is the current status of in terms of lower-density languages which have linguistically anno-tated corpora?
How can we more efficiently createthis particular type of data for lower-density lan-guages?
Can existing analytical methods methodsperform reliably with less data?
A number of stepsare identified for advancing the agenda of linguis-29tically annotated resources for lower-density lan-guages, and finally we draw conclusions.2 Lower-Density LanguagesIt should be noted from the outset that in this pa-per we interpret ?density?
to refer to the amountof computational resources available, rather thanthe number of speakers any given language mighthave.The fundamental problem for annotation oflower-density languages is that they are lower-density.
While on the surface, this is a tautol-ogy, it in fact is the problem.
For a few lan-guages of the world (such as English, Chineseand Modern Standard Arabic, and a few West-ern European languages), resources are abundant;these are the high-density Languages.
For a fewmore languages (other European languages, forthe most part), resources are, if not exactly abun-dant, at least existent, and growing; these may beconsidered medium-density languages.
Together,high-density and medium-density languages ac-count for perhaps 20 or 30 languages, although ofcourse the boundaries are arbitrary.
For all otherlanguages, resources are scarce and hence they fallinto our specific area of interest.3 Linguistically Annotated ResourcesWhile the scarcity of language resources forlower-density languages is apparent for all re-source types (with the possible exception of mono-lingual text ), it is particularly true of linguisticallyannotated texts.
By annotated texts, we includethe following sorts of computational linguistic re-sources:?
Parallel text aligned with another language atthe sentence level (and/or at finer levels ofparallelism, including morpheme-level gloss-ing)?
Text annotated for named entities at variouslevels of granularity?
Morphologically analyzed text (for non-isolating languages; at issue here is particu-larly inflectional morphology, and to a lesserdegree of importance for most computationalpurposes, derivational morphology); also amorphological tag schema appropriate to theparticular language?
Text marked for word boundaries (for thosescripts which, like Thai, do not mark mostword boundaries)?
POS tagged text, and a POS tag schema ap-propriate to the particular language?
Treebanked (syntactically annotated andparsed) text?
Semantically tagged text (semantic roles) cf.Propbank (Palmer et al, 2005), or frames cf.Framenet1?
Electronic dictionaries and other lexical re-sources, such as Wordnet2There are numerous dimensions for linguisti-cally annotated resources, and a range of researchprojects have attempted to identify the core prop-erties of interest.
While concepts such as the Ba-sic Language Resource Kit (BLARK; (Krauwer,2003; Mapelli and Choukri, 2003)) have gainedconsiderable currency in higher-density languageresource creation projects, it is clear that the base-line requirements of such schemes are signifi-cantly more advanced than we can hope for forlower-density languages in the short to mediumterm.
Notably, the concept of a reduced BLARK(?BLARKette?)
has recently gained some currencyin various forums.4 Key QuestionsGiven that the vast majority of the more than seventhousand languages documented in the Ethno-logue (Gordon, 2005) fall into the class of lower-density languages, what should we do?
Equallyimportant, what can we realistically do?
We posethree questions by which to frame the remainderof this paper.1.
Status Indicators: How do we know wherewe are?
How do we keep track of what lan-guages are high-density or medium-density,and which are lower-density?2.
Increasing Available Resources: How (orcan) we encourage the movement of lan-guages up the scale from lower-density tomedium-density or high-density?1http://framenet.icsi.berkeley.edu/2http://wordnet.princeton.edu303.
Reducing Data Requirements: Given thatsome languages will always be relativelylower-density, can language processing ap-plications be made smarter, so that they don?trequire largely unattainable resources in or-der to perform adequately?5 Status IndicatorsWe have been deliberately vague up to this pointabout how many lower-density languages thereare, or the simpler question, how my high andmedium density languages there are.
Of courseone reason for this is that the boundary betweenlow density and medium or high density is inher-ently vague.
Another reason is that the situationis constantly changing; many Central and East-ern European languages which were lower-densitylanguages a decade or so ago are now arguablymedium density, if not high density.
(The stan-dard for high vs. low density changes, too; the baris considerably higher now than it was ten yearsago.
)But the primary reason for being vague abouthow many ?
and which ?
languages are low den-sity today is that no is keeping track of what re-sources are available for most languages.
So wesimply have no idea which languages are low den-sity, and more importantly (since we can guess thatin the absence of evidence to the contrary, a lan-guage is likely to be low density), we don?t knowwhich resource types most languages do or do nothave.This lack of knowledge is not for lack of trying,although perhaps we have not been trying hardenough.
The following are a few of the catalogsof information about languages and their resourcesthat are available:?
The Ethnologue3: This is the standard list-ing of the living languages of the world, butcontains little or no information about whatresources exist for each language.?
LDC catalog4 and ELDA catalog5: TheLinguistic Data Consortium (LDC) and theEuropean Language Resources DistributionAgency (ELDA) have been among the largestdistributors of annotated language data.
Theircatalogs, naturally, cover only those corpora3http://www.ethnologue.org4http://www.ldc.upenn.edu/Catalog/5http://www.elda.org/rubrique6.htmldistributed by each organization, and theseinclude only a small number of languages.Naturally, the economically important lan-guages constitute the majority of the holdingsof the LDC and ELDA.?
AILLA (Archive of the Indigenous Lan-guages of Latin America6), and numerousother language archiving sites: Such sitesmaintain archives of linguistic data for lan-guages, often with a specialization, such asindigenous languages of a country or region.The linguistic data ranges from unannotatedspeech recordings to morphologically ana-lyzed texts glossed at the morpheme level.?
OLAC (Open Archives Language Commu-nity7): Given that many of the above re-sources (particularly those of the many lan-guage archives) are hard to find, OLAC isan attempt to be a meta-catalog (or aggre-gator)of such resources.
It allows lookup ofdata by type, language etc.
for all data repos-itories that ?belong to?
OLAC.
In fact, all theabove resources are listed in the OLAC unioncatalogue.?
Web-based catalogs of additional resources:There is a huge number of additional web-sites which catalog information about lan-guages, ranging from electronic and printdictionaries (e.g.
yourDictionary8), to dis-cussion groups about particular languages9.Most such sites do little vetting of the re-sources, and dead links abound.
Neverthe-less, such sites (or a simple search with anInternet search engine) can often turn up use-ful information (such as grammatical descrip-tions of minority languages).
Very few ofthese web sites are cataloged in OLAC, al-though recent efforts (Hughes et al, 2006a)are slowly addressing the inclusion of web-based low density language resources in suchindexes.None of the above catalogs is in any sense com-plete, and indeed the very notion of completenessis moot when it comes to cataloging Internet re-sources.
But more to the point of this paper, it6http://www.ailla.utexas.org7http://www.language-archives.org8http://www.yourdictionary.com9http://dir.groups.yahoo.com/dir/Cultures Community/By Language31is difficult, if not impossible, to get a picture ofthe state of language resources in general.
Howmany languages have sufficient bitext (and in whatgenre), for example, that one could put together astatistical machine translation system?
What lan-guages have morphological parsers (and for whatlanguages is such a parser more or less irrele-vant, because the language is relatively isolating)?Where can one find character encoding convertersfor the Ge?ez family of fonts for languages writtenin Ethiopic script?The answer to such questions is important forseveral reasons:1.
If there were a crisis that involved an arbitrarylanguage of the world, what resources couldbe deployed?
An example of such a situa-tion might be another tsunami near Indone-sia, which could affect dozens, if not hun-dreds of minority languages.
(The Decem-ber 26, 2004 tsunami was particularly felt inthe Aceh province of Indonesia, where one ofthe main languages is Aceh, spoken by threemillion people.
Aceh is a lower-density lan-guage.)2.
Which languages could, with a relativelysmall amount of effort, move from lower-density status to medium-density or high-density status?
For example, where paralleltext is harvestable, a relatively small amountof work might suffice to produce many appli-cations, or other resources (e.g.
by projectingsyntactic annotation across languages).
Onthe other hand, where the writing system ofa language is in flux, or the language is po-litically oppressed, a great deal more effortmight be necessary.3.
For which low density languages might re-lated languages provide the leverage neededto build at least first draft resources?
For ex-ample, one might think of using Turkish (ar-guably at least a medium-density language)as a sort of pivot language to build lexiconsand morphological parsers for such low den-sity Turkic languages as Uzbek or Uyghur.4.
For which low density languages are thereextensive communities of speakers living inother countries, who might be better able tobuild language resources than speakers livingin the perhaps less economically developedhome countries?
(Expatriate communitiesmay also be motivated by a desire to main-tain their language among younger speakers,born abroad.)5.
Which languages would require more work(and funding) to build resources, but are stillplausible candidates for short term efforts?To our knowledge, there is no general, on-goingeffort to collect the sort of data that would makeanswers to these questions possible.
A survey wasdone at the Linguistic Data Consortium severalyears ago (Strassel et al, 2003) , for text-based re-sources for the three hundred or so languages hav-ing at least a million speakers (an arbitrary cutoff,to be sure, but necessary for the survey to have hadat least some chance of success).
It was remark-ably successful, considering that it was done bytwo linguists who did not know the vast majorityof the languages surveyed.
The survey was fundedlong enough to ?finish?
about 150 languages, butno subsequent update was ever done.A better model for such a survey might be anedited book: one or more computational linguistswould serve as ?editors?, responsible for the over-all framework, and training of other participants.Section ?editors?
would be responsible for a lan-guage family, or for the languages of a geographicregion or country.
Individual language expertswould receive a small amount of training to enablethem to answer the survey questions for their lan-guage, and then paid to do the initial survey, plusperiodic updates.
The model provided by the Eth-nologue (Gordon, 2005) may serve as a startingpoint, although for the level of detail that wouldbe useful in assessing language resource availabil-ity will make wholesale adoption unsuitable.6 Increasing Available ResourcesGiven that a language significantly lacks compu-tational linguistic resources (and in the context ofthis paper and the associated workshop, annotatedtext resources), so that it falls into the class oflower-density languages (however that might bedefined), what then?Most large-scale collections of computationallinguistics resources have been funded by govern-ment agencies, either the US government (typi-cally the Department of Defense) or by govern-ments of countries where the languages in ques-tion are spoken (primarily European, but also a32few other financially well-off countries).
In somecases, governments have sponsored collections forlanguages which are not indigenous to the coun-try in question (e.g.
the EMILLE project10, see(McEnery et al, 2000)).In most such projects, production of resourcesfor lower-density languages have been the work ofa very small team which oversees the effort, to-gether with paid annotators and translators.
Morespecifically, collection and processing of monolin-gual text can be done by a linguist who need notknow the language (although it helps to have aspeaker of the language who can be called on todo language identification, etc.).
Dictionary col-lection from on-line dictionaries can also be doneby a linguist; but if it takes much more effort thanthat ?
for example, if the dictionary needs to beconverted from print format to electronic format ?it is again preferable to have a language speakeravailable.Annotating text (e.g.
for named entities) is dif-ferent: it can only be done by a speaker of the lan-guage (more accurately, a reader: for Punjabi, forinstance, it can be difficult to find fluent readers ofthe Gurmukhi script).
Preferably the annotator isfamiliar enough with current events in the countrywhere the language is spoken that they can inter-pret cross-references in the text.
If two or more an-notators are available, the work can be done some-what more quickly.
More importantly, there can besome checking for inter-annotator agreement (andrevision taking into account such differences as arefound).Earlier work on corpus collection from the web(e.g.
(Resnik and Smith, 2003)) gave some hopethat reasonably large quantities of parallel textcould be found on the web, so that a bitext collec-tion could be built for interesting language pairs(with one member of the pair usually being En-glish) relatively cheaply.
Subsequent experiencewith lower-density languages has not born thathope out; parallel text on the web seems rela-tively rare for most languages.
It is unclear whythis should be.
Certainly in countries like India,there are large amounts of news text in English andmany of the target languages (such as Hindi).
Nev-ertheless, very little of that text seems to be gen-uinely parallel, although recent work (Munteanuand Marcu, 2005) indicates that true parallelismmay not be required for some tasks, eg machine10http://bowland-files.lancs.ac.uk/corplang/emille/translation, in order to gain acceptable results.Because bitext was so difficult to find for lower-density languages, corpus creation efforts relylargely, if not exclusively, on contracting out textfor translation.
In most cases, source text is har-vested from news sites in the target language, andthen translated into English by commercial trans-lation agencies, at a rate usually in the neighbor-hood of US$0.25 per word.
In theory, one couldreduce this cost by dealing directly with trans-lators, avoiding the middleman agencies.
Sincemany translators are in the Third World, this mightresult in considerable cost savings.
Nevertheless,quality control issues loom large.
The more pro-fessional agencies do quality control of their trans-lations; even so, one may need to reject transla-tions in some cases (and the agencies themselvesmay have difficulty in dealing with translators forlanguages for which there is comparatively littledemand).
Obviously this overall cost is high; itmeans that a 100k word quantity of parallel textwill cost in the neighborhood of US$25K.Other sources of parallel text might includegovernment archives (but apart from parliamen-tary proceedings where these are published bilin-gually, such as the Hansards, these are usually notopen), and the archives of translation companies(but again, these are seldom if ever open, becausethe agencies must guard the privacy of those whocontracted the translations).Finally, there is the possibility that parallel text?
and indeed, other forms of annotation ?
could beproduced in an open source fashion.
Wikipedia11is perhaps the most obvious instance of this, asthere are parallel articles in English and other lan-guages.
Unfortunately, the quantity of such par-allel text at the Wikipedia is very small for allbut a few languages.
At present (May 2006),there are over 100,000 articles in German, Span-ish, French, Italian, Japanese, Dutch, Polish, Por-tuguese and Swedish.12 Languages with over10,000 articles include Arabic, Bulgarian, Cata-lan, Czech, Danish, Estonian, Esperanto and Ido(both constructed languages), Persian, Galician,Hebrew, Croatian), Bahasa Indonesian, Korean,Lithuanian, Hungarian, Bahasa Malay, Norwegian11http://en.wikipedia.org12Probably some of these articles are non-parallel.
Indeed,a random check of Cebuano articles in Wikipedia revealedthat many were stubs (a term used in the Wikipedia to refer to?a short article in need of expansion?
), or were simply links toInternet blogs, many of which were monolingual in English.33(Bokma?l and Nynorsk), Romanian, Russian, Slo-vak, Slovenian, Serbian, Finnish, Thai, Turkish,Ukrainian, and Chinese.
The dominance of Euro-pean languages in these lists is obvious.During a TIDES exercise in 2003, researchersat Johns Hopkins University explored an innova-tive approach to the creation of bitext (parallel En-glish and Hindi text, aligned at the sentence level):they elicited translations into English of Hindi sen-tences they posted on an Internet web page (Oard,2003; Yarowsky, 2003).
Participants were paid forthe best translations in Amazon.com gift certifi-cates, with the quality of a twenty percent subsetof the translations automatically evaluated usingBLEU scores against highly scored translations ofthe same sentences from previous rounds.
Thispool of high-quality translations was initialized toa set of known quality translations.
A valuableside effect of the use of previously translated textsfor evaluation is that this created a pool of multiplytranslated texts.The TIDES translation exercise quickly pro-duced a large body of translated text: 300K words,in five days, at a cost of about two cents per word.This approach to resource creation is similar tonumerous open source projects, in the sense thatthe work is being done by the public.
It differedin that the results of this work were not madepublicly available; the use of an explicit qual-ity control method; and of course the paymentsto (some) participants.
While the quality controlaspect may be essential to producing useful lan-guage resources, hiding those resources not cur-rently being used for evaluation is not essential tothe methodology.Open source resource creation efforts are ofcourse common, with the Wikipedia13 being thebest known.
Other such projects include Ama-zon.com?s Mechanical Turk14, LiTgloss15, TheESP Game16, and the Wiktionary17.
Clearly someforms of annotation will be easier to do usingan open source methodology than others will.For example, translation and possibly named en-tity annotation might be fairly straightforward,while morphological analysis is probably moredifficult, particularly for morphologically complexlanguages.13http://www.wikipedia.org14http://www.mturk.com/mturk/15http://litgloss.buffalo.edu/16http://www.espgame.org/17http://wiktionary.org/Other researchers have experimented with theautomatic creation of corpora using web data(Ghani et al, 2001) Some of these corpora havegrown to reasonable sizes; (Scannell, 2003; Scan-nell, 2006) has corpora derived from web crawlingwhich are measured in tens of millions of wordsfor a variety of lower-density languages.
Howeverit should be noted that in these cases, the type oflinguistic resource created is often not linguisti-cally annotated, but rather a lexicon or collectionof primary texts in a given language.Finally, we may mention efforts to create cer-tain kinds of resources by computer-directed elic-itation.
Examples of projects sharing this focusinclude BOAS (Nirenburg and Raskin, 1998), andthe AVENUE project (Probst et al, 2002), (Lavieet al, 2003).7 Reducing Data RequirementsCreating more annotated resources is the obviousway to approach the problem of the lack of re-sources for lower-density languages.
A comple-mentary approach is to improve the way the infor-mation in smaller resources is used, for exampleby developing machine translation systems that re-quire less parallel text.How much reduction in the required amount ofresources might be enough?
An interesting ex-periment, which to our knowledge has never beentried, would be for a linguist to attempt as a testcase what we hope that computers can do.
Thatis, a linguist could take a ?small?
quantity of paral-lel text, and extract as much lexical and grammat-ical information from that as possible.
The lin-guist might then take a previously unseen text inthe target language and translate it into English,or perform some other useful task on target lan-guage texts.
One might argue over whether thisexperiment would constitute an upper bound onhow much information could be extracted, but itwould probably be more information than currentcomputational approaches extract.Naturally, this approach partially shifts theproblem from the research community interestedin linguistically annotated corpora to the researchcommunity interested in algorithms.
Much ef-fort has been invested in scaling algorithmic ap-proaches upwards, that is, leveraging every lastavailable data point in pursuit of small perfor-mance improvements.
We argue that scaling down(ie using less training data) poses an equally sig-34nificant challenge.
The basic question of whethermethods which are data-rich can scale down to im-poverished data has been the focus of a number ofrecent papers in areas such as machine translation(Somers, 1997; Somers, 1998), language identifi-cation (Hughes et al, 2006b) etc.
However, taskswhich have lower-density language at their corehave yet to become mainstream in shared evalua-tion tasks which drive much of the algorithmic im-provements in computational linguistics and natu-ral language processing.Another approach to data reduction is to changethe type of data required for a given task.
Formany lower-density languages a significant vol-ume of linguistically annotated data exists, but notin the form of the curated, standardised corporato which language technologists are accustomed.Neverthless for extremely low density languages,a degree of standardisation is apparent by virtue ofdocumentary linguistic practice.
Consider for ex-ample, the number of Shoebox lexicons and cor-responding interlinear texts which are potentiallyavailable from documentary sources: while notbeing the traditional resource types on which sys-tems are trained, they are reasonably accessible,and cover a larger number of languages.
Bibletranslations are another form of parallel text avail-able in nearly every written language (see (Resniket al, 1999)).
There are of course issues of quality,not to mention vocabulary, that arise from usingthe Bible as a source of parallel text, but for somepurposes ?
such as morphology learning ?
Bibletranslations might be a very good source of data.Similarly, a different compromise may be foundin the ratio of the number of words in a corpusto the richness of linguistic annotation.
In manyhigh-density corpora development projects, an ar-bitrary (and high) target for the number of words isoften set in advance, and subsequent linguistic an-notation is layered over this base corpus in a pro-gressively more granular fashion.
It may be thatthis corpus development model could be modifiedfor lower-density language resource development:we argue that in many cases, the richness of lin-guistic annotation over a given set of data is moreimportant than the raw quantity of the data set.A related issue is different standards for an-notating linguistic concepts We already see thisin larger languages (consider the difference inmorpho-syntactic tagging between the Penn Tree-bank and other corpora), but has there is a higherdiversity of standards in lower-density languages.Solutions may include ontologies for linguisticconcepts e.g.
General Ontology for LinguisticDescription18 and the ISO Data Category Reg-istry (Ide and Romary, 2004), which allow cross-resource navigation based on common semantics.Of course, cross-language and cross-cultural se-mantics is a notoriously difficult subject.Finally, it may be that development of webbased corpora can act as the middle ground: thereare plenty of documents on the web in lower-density languages, and efforts such as projects byScannell19 and Lewis20 indicate these can be cu-rated reasonably efficiently, even though the out-comes may be slightly different to that which weare accustomed.
Is it possible to make use of XMLor HTML markup directly in these cases?
Some-day, the semantic web may help us with this typeof approach.8 Moving ForwardHaving considered the status of linguistically-annotated resources for lower-density languages,and two broad strategies for improving this situ-ation (innovative approaches to data creation, andscaling down of resource requirements for existingtechniques), we now turn to the question of whereto go from here.
We believe that there are a num-ber of practical steps which can be taken in orderto increase the number of linguistically-annotatedlower-density language resources available to theresearch community:?
Encouraging the publication of electroniccorpora of lower-density languages: mosteconomic incentives for corpus creation onlyexhibit return on investment because of thefocus on higher-density languages; new mod-els of funding and commercializing corporafor lower-density languages are required.?
Engaging in research on bootstrapping fromhigher density language resources to lower-density surrogates: it seems obvious thatat least for related languages adopting aderivational approach to the generation oflinguistically annotated corpora for lower-density languages by using automated an-notation tools trained on higher-density lan-18http://www.linguistics-ontology.org19http://borel.slu.edu/crubadan/stadas.html20http://www.csufresno.edu/odin35guages may at least reduce the human effortrequired.?
Scaling down (through data requirement re-duction) of state of the art algorithms: therehas been little work in downscaling state ofthe art algorithms for tasks such as namedentity recognition, POS tagging and syntac-tic parsing, yet (considerably) reducing thetraining data requirement seems like one ofthe few ways that existing analysis technolo-gies can be applied to lower-density lan-guages.?
Shared evaluation tasks which include lower-density languages or smaller amounts of data:most shared evaluation tasks are construedas exercises in cross-linguistic scalability (egCLEF) or data intensivity (eg TREC) or both(eg NTCIR).
Within these constructs thereis certainly room for the inclusion of lower-density languages as targets, although no-tably the overhead here is not in the provi-sion of the language data, but the derivatives(eg query topics) on which these exercises arebased.?
Promotion of multilingual corpora which in-clude lower-density languages: as multi-lingual corpora emerge, there is opportu-nity to include lower-density languages atminimal opportunity cost e.g.
EuroGOV(Sigurbjo?nsson et al, 2005) or JRC-Acquis(Steinberger et al, 2006), which are based onweb data from the EU, includes a number oflower-density languages by virtue of the cor-pus creation mechanism not being language-specific.?
Language specific strategies: collectively wehave done well at developing formal strate-gies for high density languages e.g.
in EUroadmaps, but not so well at strategies formedium-density or lower-density languages.The models for medium to long term strate-gies of language resource development maybe adopted for lower density languages.
Re-cently this has been evidenced through eventssuch as the LREC 2006 workshop on Africanlanguage resources and the development of acorresponding roadmap.?
Moving towards interoperability between an-notation schemes which dominate the higher-density languages (eg Penn Treebank tag-ging conventions) and the relatively ad-hocschemes often exhibited by lower-densitylanguages, through means such as markupontologies like the General Ontology for Lin-guistic Description or the ISO Data CategoryRegistry.Many of these steps are not about to be realisedin the short term.
However, developing a cohesivestrategy for addressing the need for linguisticallyannotated corpora is a first step in ensuring com-mittment from interested researchers to a commonroadmap.9 ConclusionIt is clear that the number of linguistically-annotated resources for any language will in-evitably be less than optimal.
Regardless of thedensity of the language under consideration, thecost of producing linguistically annotated corporaof a substantial size is significant, Inevitably, lan-guages which do not have a strong political, eco-nomic or social status will be less well resourced.Certain avenues of investigation e.g.
collect-ing language specific web content, or building ap-proximate bitexts web data are being explored, butother areas (such as rich morphosyntactic annota-tion) are not particularly evidenced.However, there is considerable research inter-est in the development of linguistically annotatedresources for languages of lower density.
We areencouraged by the steady rate at which academicpapers emerge reporting the development of re-sources for lower-density language targets.
Wehave proposed a number of steps by which the is-sue of language resources for lower-density lan-guages may be more efficiently created and lookforward with anticipation as to how these ideasmotivate future work.ReferencesRayid Ghani, Rosie Jones, and Dunja Mladenic.
2001.Mining the web to create minority language corpora.In Proceedings of 2001 ACM International Con-ference on Knowledge Management (CIKM2001),pages 279?286.
Association for ComputingMachin-ery.Raymond G. Gordon.
2005.
Ethnologue: Languagesof the World (15th Edition).
SIL International: Dal-las.36Baden Hughes, Timothy Baldwin, and Steven Bird.2006a.
Collecting low-density language data on theweb.
In Proceedings of the 12th Australasian WebConference (AusWeb06).
Southern Cross University.Baden Hughes, Timothy Baldwin, Steven Bird, JeremyNicholson, and Andrew MacKinlay.
2006b.
Recon-sidering language identification for written languageresources.
In Proceedings of the 5th InternationalConference on Language Resources and Evaluation(LREC2006).
European Language Resources Asso-ciation: Paris.Nancy Ide and Laurent Romary.
2004.
A registry ofstandard data categories for linguistic annotation.
InProceedings of the 4th International Conference onLanguage Resources and Evaluation (LREC2004,pages 135?139.
European Language Resources As-sociation: Paris.Steven Krauwer.
2003.
The basic language resourcekit (BLARK) as the first milestone for the lan-guage resources roadmap.
In Proceedings of 2ndInternational Conference on Speech and Computer(SPECOM2003).A.
Lavie, S. Vogel, L. Levin, E. Peterson, K. Probst,A.
Font Llitjos, R. Reynolds, J. Carbonell, andR.
Cohen.
2003.
Experiments with a hindi-to-english transfer-based mt system under a miserlydata scenario.
ACM Transactions on Asian Lan-guage Information Processing (TALIP), 2(2).Valerie Mapelli and Khalid Choukri.
2003.
Reporton a monimal set of language resources to be madeavailable for as many languages as possible, and amap of the actual gaps.
ENABLER internal projectreport (Deliverable 5.1).Tony McEnery, Paul Baker, and Lou Burnard.
2000.Corpus resources and minority language engineer-ing.
In Proceedings of the 2nd International Con-ference on Language Resources and Evaluation(LREC2002).
European Language Resources Asso-ciation: Paris.Dragos Stefan Munteanu and Daniel Marcu.
2005.
Im-proving machine translation performance by exploit-ing non-parallel corpora.
Computational Linguis-tics, 31(4):477?504.Sergei Nirenburg and Victor Raskin.
1998.
Univer-sal grammar and lexis for quick ramp-up of mt.
InProceedings of 36th Annual Meeting of the Associ-ation for Computational Linguistics and 17th Inter-national Conference on Computational Linguistics,pages 975?979.
Association for Computational Lin-guistics.Douglas W. Oard.
2003.
The surprise language exer-cises.
ACM Transactions on Asian Language Infor-mation Processing (TALIP), 2(2):79?84.Martha Palmer, Dan Gildea, and Paul Kingsbury.
2005.The proposition bank: A corpus annotated with se-mantic roles.
Computational Linguistics, 31(1).K.
Probst, L. Levin, E. Peterson, A. Lavie, and J. Car-bonell.
2002.
Mt for minority languages usingelicitation-based learning of syntactic transfer rules.Machine Translation, 17(4):245?270.Philip Resnik and Noah A. Smith.
2003.
The webas a parallel corpus.
Computational Linguistics,29(3):349?380.Philip Resnik, Mari Broman Olsen, and Mona Diab.1999.
The Bible as a parallel corpus: Annotatingthe ?Book of 2000 Tongues?.
Computers and theHumanities, 33(1-2):129?153.Kevin Scannell.
2003.
Automatic thesaurus genera-tion for minority languages: an irish example.
InActes des Traitement Automatique des Langues Mi-noritaires et des Petites Langues, volume 2, pages203?212.Kevin Scannell.
2006.
Machine translation forclosely related language pairs.
In Proceedings of theLREC2006 Workshop on Strategies for developingmachine translation for minority languages.
Euro-pean Language Resources Association: Paris.B.
Sigurbjo?nsson, J. Kamps, and M. de Rijke.
2005.Blueprint of a cross-lingual web collection.
Journalof Digital Information Management, 3(1):9?13.Harold Somers.
1997.
Machine translation and minor-ity languages.
Translating and the Computer, 19:1?13.Harold Somers.
1998.
Language resources and minor-ity languages.
Language Today, 5:20?24.R.
Steinberger, B. Pouliquen, A. Widger, C. Ignat,T.
Erjavec, D. Tufis, and D. Varga.
2006.
The jrc-acquis: A multilingual aligned parallel corpus with20+ languages.
In Proceedings of the 5th Interna-tional Conference on Language Resources and Eval-uation (LREC2006).
European Language ResourcesAssociation: Paris.Stephanie Strassel, Mike Maxwell, and ChristopherCieri.
2003.
Linguistic resource creation for re-search and technology development: A recent exper-iment.
ACM Transactions on Asian Language Infor-mation Processing (TALIP), 2(2):101?117.David Yarowsky.
2003.
Scalable elicitation of trainingdata for machine translation.
Team Tides, 4.10 AcknowledgementsThe authors are grateful to Kathryn L. Baker forher comments on earlier drafts of this paper.Portions of the research in this paper were sup-ported by the Australian Research Council Spe-cial Research Initiative (E-Research) grant num-ber SR0567353 ?An Intelligent Search Infrastruc-ture for Language Resources on the Web.
?37
