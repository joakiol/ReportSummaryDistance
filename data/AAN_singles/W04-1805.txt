Discovering Specific Semantic Relationships between Nouns andVerbs in a Specialized French CorpusVincent CLAVEAU and Marie-Claude L'HOMMEOLST - University of MontrealC.P.
6128, succ.
Centre-VilleMontr?al, QC, H3C 3J7Canada{Vincent.Claveau,Marie-Claude.L'Homme}@umontreal.caAbstractRecent literature in computational termi-nology has shown an increasing interest inidentifying various semantic relationshipsbetween terms.
In this paper, we pro-pose an original strategy to find specificnoun-verb combinations in a specialized cor-pus.
We focus on verbs that convey ameaning of realization.
To acquire thesenoun-verb pairs, we use asares, a machinelearning technique that automatically in-fers extraction patterns from examples andcounter-examples of realization noun-verbpairs.
The patterns are then applied to thecorpus to retrieve new pairs.
Results, mea-sured with a large test set, show that ouracquisition technique outperforms classicalstatistical methods used for collocation ac-quisition.
Moreover, the inferred patternsyield interesting clues on which structuresare more likely to convey the target seman-tic link.1 IntroductionRecent literature in computational terminologyhas shown an increasing interest in identifyingvarious semantic relationships between terms.Different strategies have been developed in or-der to identify pairs of terms that share a spe-cific semantic relationship (such as hyperonymyor meronymy) or to build classes of terms.However, most strategies are based on in-ternal or external methods (Grabar andZweigenbaum, 2002), i.e.
methods that rely onthe form of terms or on the information gatheredfrom contexts.
(In some cases, an additionalresource, such as a dictionary or a thesaurus,is used during the identification process.)
Thework reported here infers specific semantic rela-tionships based on sets of examples and counter-examples.In this paper, the method is applied to aFrench corpus on computing to find noun-verbcombinations in which verbs convey a meaningof realization.
The work is carried out in orderto assist terminographers in the enrichment of adictionary on computing that includes colloca-tional information (L'Homme, 2004).Even though this work is carried out for ter-minographical and lexicographical purposes, itcan certainly be of use in other applications,namely information retrieval.
Indeed, such richsemantic links can be used to extend indicesor reformulate queries (similar to the work byVoorhees (1994) with WordNet relations).2 ObjectivesThe noun-verb combinations we aim to identifyhave the following characteristics.
They share:?
A syntactic relationship : nouns can besubjects (e.g., ordinateur tourne; com-puter runs); direct objects (e.g., configurerl'application; configure the application); orsecond complement (e.g., charger x dans lam?moire;  load x into memory);?
A valid semantic relationship.
The follow-ing semantic relationships are sought:1. verbs that refer to activities carried outby the nouns (e.g., serveur d?marre;server starts);2. verbs that refer to uses of the nouns(e.g., activer une option; activate anoption or naviguer sur Internet; surfthe Internet);3. verbs that refer to activities carried outby means of the nouns (e.g., activerl'option au moyen de cette commande;activate this option with this com-mand ); and, finally,4.
verbs that refer to the processes bywhich nouns are prepared fur use (e.g., installer un logiciel; install an applica-tion).These noun-verb combinations will hereafter becalled valid N-V pairs.CompuTerm 2004  -  3rd International Workshop on Computational Terminology 39The semantic relationships listed above cor-respond to a set of lexical functions (LFs) de-fined in (Mel'?uk et al, 1984 1999), i.e.
LFsused to represent realization (e.g., Facti, Reali,Labrealij) and to represent the process by whichsomething is prepared (Prepar)1.
(Both ofthese types of LFs can be combined with oth-ers [to create complex lexical functions].)
TheseLFs are opposed to support verbs representedby the LFs Funci, Operi, Laborij (e.g., cr?er unfichier; create a file; definir une variable; de-fine a variable).
).Realization verbs (and verbs denoting thepreparation of nouns) were chosen since theyare believed to be frequent in technical corpora,such as the corpus of computing used in this ex-periment.
However, this is seen as a first stepin order to validate an acquisition process forsemantically-related V-N pairs.
Other semanticrelationships could be sought in the future.3 Related workA number of applications have relied on distri-butional analysis (Harris, 1971) in order to buildclasses of semantically related terms.
This ap-proach, which uses words that appear in the con-text of terms to formulate hypotheses on theirsemantic relatedness (Habert et al, 1996, for ex-ample), does not specify the relationship itself.Hence, synonyms, co-hyponyms, hyperonyms,etc.
are not differentiated.More recent work on terminology structuringhas focussed on formal similarity to develop hy-potheses on the semantic relationships betweenterms: Daille (2003) uses derivational morphol-ogy; Grabar and Zweigenbaum (2002) use, as astarting point, a number of identical characters.Up to now, the focus has been on nounsand adjectives, since these structuring methodshave been applied to lists of extracted candidateterms (Habert et al, 1996; Daille, 2003) or tolists of admitted terms (Grabar and Zweigen-baum, 2002).
As a consequence, relationshipsconsidered have been mostly synonymic or tax-onomic, or defined as term variations.On the other hand, other work has been car-ried out in order to acquire collocations.
Most ofthese endeavours have focused on purely statis-tical acquisition techniques (Church and Hanks,1However, our interpretation of LFs in this work ismuch looser, since we admitted verbs that would not beconsidered to be members of true collocations as Mel'?uket al (1984 1999) define them, i.e.
groups of lexical unitsthat share a restricted cooccurrence relationship.1990), on linguisitic acquisition (by the use ofPart-of-Speech filters hand-crafted by a linguist)(Oueslati, 1999) or, more frequently, on a combi-nation of the two (Smadja, 1993; Kilgarriff andTugwell, 2001, for example).
It is worth notingthat although these techniques are able to iden-tify N-V pairs, they do not specify the relation-ship between N and V, nor are they capable offocusing on a subset of N-V pairs.
The originalacquisition methodology we present in the nextsection will allow us to overcome this limitation.4 Methodology for finding validnoun-verb pairsThis section is devoted to the description of themethodology and the data we use to acquiresemantically related noun-verb pairs.
We firstdescribe the specialized corpus used in this ex-periment.
Then, we briefly present asares, apattern inference tool, on which our acquisitionstrategy relies.
Finally, we explain the differentsteps of the acquisition process.4.1 Computer science corpusThe French corpus used in our experiments iscomposed of more than 50 articles from booksor web sites specialized in computer science; allof them were published between 1988 and 2003.It covers different computer science sub-domains(networking, managing Unix computers, web-cams...) and comprises 600,000 words.Segmentation, morpho-syntactic tagging andlemmatization have been carried out using thetool cordial2.
Each word is accompanied byits lemma and Part-of-Speech tag (noun, verb,adjective).
Also, the tool indicates inflection(gender and number for nouns, tense and per-son for verbs) and gives syntactic information(head-modifier) for noun phrases.4.2 Overview of asaresThe method used for the acquisition of N-Vpairs relies mainly on asares, a pattern in-ference tool.
Asares is presented in detail in(Claveau et al, 2003).
We simply give a shortaccount of its basic principles herein.Asares is based on a Machine Learningtechnique, Inductive Logic Programming (ILP)(Muggleton and De-Raedt, 1994), which infersgeneral morpho-syntactic patterns from a setof examples (this set is noted E+ hereafter)and counter-examples (E?)
of the elements one2Cordial is a commercial product of Synapse-D?veloppement.CompuTerm 2004  -  3rd International Workshop on Computational Terminology40wants to acquire and their context.
The con-textual patterns produced can then be appliedto the corpus in order to retrieve new elements.The acquisition process can be summarized in 3steps:1. construction of the sets of examples (andcounter-examples);2. inference of extraction patterns withasares; and3.
extraction of N-V pairs from the corpuswith the inferred patterns.Asares has been previously applied to the ac-quisition of word pairs sharing semantic rela-tions defined in the Generative Lexicon frame-work (Pustejovsky, 1995) and called qualia rela-tions (Bouillon et al, 2001).
Here, we proposeto use asares in a quite similar way to retrieveour valid N-V pairs.
However, the N-V combi-nations sought are more specific than those thatwere identified in these previous experiments.Formally, ILP aims at inferring logic pro-grams (sets of Horn clauses, notedH) from a setof facts (examples and counter-examples of theconcept to be learnt) and background knowledge(B), such that the program H logically entailsthe examples with respect to the backgroundknowledge and rejects (most of) the counter-examples.
This is transcribed by the two logicalformulae B ?H |= E+, B ?H 6|= E?, which setthe aim of an ILP algorithm.In this framework, asares infers clauses ex-pressing morpho-syntactic patterns that gener-alize the structures of sentences containing thetarget element (examples) but not the structuresof the sentences containing counter-examples.The background knowledge encodes informationabout each word occurring in the example orcounter-example, namely the meaning of its tag(e.g., adjective in plural form, infinitive verb).The main benefits of this acquisition tech-nique lie in the inferred patterns.
Indeed, con-trary to the more classical statistical methods(Mutual Information, Loglike..., see below) usedfor collocation acquisition (see (Pearce, 2002)for a review), these patterns allow:1. understanding of the results, that is, why aspecific element has been retrieved or not;2. highlighting of the corpus-specific struc-tures conveying the target element.In addition to its explanatory capacity, this sym-bolic acquisition technique has obtained goodresults for other acquisition tasks when com-pared to existing statistical techniques (Bouillonet al, 2002).4.3 Acquisition processTo infer extraction patterns, asares needs a setof examples (E+) and a set of counter-examples(E?)
of the elements we want to retrieve.
Inour case, E+ must thus be composed of (POS-tagged) sentences containing valid N-V pairs;conversely, E?
must be composed of sentencescontaining non-valid N-V pairs.
While this stepis tedious and usually carried out manually, theoriginality of our work lies in the fact that E+and E?
are obtained automatically.To produce the positive examples, we use theexisting entries of a terminological database weare currently developing.
These entries are thusa kind of bootstrap in our acquisition process.More precisely, every N-V pair in which V is de-fined in the database as a realization verb for Nis included.
Then, all sentences in our corpuscontaining this N-V pair are considered as ex-amples and added to E+.
Note that we do notcheck if each occurrence of the N-V pair actuallyshares the target semantic link or even a syn-tactic link in the sentences that are extracted.Some of the examples in E+ might be incorrect,but asares tolerates a certain amount of noise.A totally different technique is needed to pro-duce the E?
set, since no information concern-ing verbs that are not semantically related isavailable in the terminological database.
To ob-tain a list of invalid N-V pairs, we acquire themfrom our corpus using a statistical technique.This produces a list of all N-V pairs that ap-pear in the same sentence, and assigns each ascore.
Many statistical coefficients exist (Man-ning and Sch?tze, 1999); most of them can beeasily expressed with the help of a contingencytable similar to that reproduced in Table 1 andby noting S = a + b + c + d. For example, theVj Vk, k 6= jNi a bNl, l 6= i c dTable 1: Contingency table for the pair Ni-VjMutual Information coefficient is defined as:MI = log2 a(a+ b)(a+ c)and the loglike coefficient (Dunning, 1993) as:Log = a log a + b log b + c log c + d log d ?
(a +CompuTerm 2004  -  3rd International Workshop on Computational Terminology 41b) log(a+ b)?
(a+ c) log(a+ c)?
(b+ d) log(b+d)?
(c+ d) log(c+ d) + S log S.In the work presented here, we have adoptedthe Loglike coefficient.
From the N-V pair listproduced with this method, we have chosen thepairs that obtained the lower Loglike scores.
Asfor the positive examples, we consider that eachsentence containing one of the pairs is a counter-example and is added to E?.Finally, asares is launched with these E+and E?
sets; each containing about 2600 sen-tences.
About 80 patterns are then produced;some of them are presented and discussed in sec-tion 5.1.
These patterns can now be applied tothe corpus in order to retrieve valid N-V pairs.5 Performance evaluation5.1 Inferred patternsThe inferred patterns give some interesting in-formation about the way the target semantic re-lationships are expressed in our corpus.
Whilesome structures are general, others seem veryspecific to our corpus.First of all, proximity is an important factorin valid relationships between nouns and verbs;it can be observed in numerous patterns.
Forexample, the inferred Horn clause:realization(N,V) :- common_noun(N), contigu-ous(N,V), N 6=V.
transcribes the fact that a nounand a verb may be a valid N-V pair if N isa common noun (common_noun(N)) and V iscontiguous to N (contiguous(N,V).
(Determinersare not taken into account.)
This patterncovers the case in which N precedes V, such as les utilisateurs lancent tour-?-tour leurs programmes(users launch in turn their programs) orin which V precedes N, such as  il est doncn?cessaire d'ex?cuter la commande depmod  (thus,it is necessary to run the depmod command .A quite similar clue is given in the followingpattern:realization(N,V) :- near_verb(N,V), suc(V,C),suc(C,N), noun(N), V 6=C, N 6=C, N 6=V.
Thenear_verb(N,V) predicate means that no verboccurs between N and V, and suc(X,Y) thatthe word X is followed by Y.
This clause canbe expressed in a more classical way as V +(anything but a verb) + N and retrieves pairs likeC'est un service qui vous permet de vous connecter?
Internet (This is a service that allows you toconnect to the Internet).Another frequent clue in the patterns pro-duced is, unsurprisingly, that N must be thehead of a noun phrase.
For example, realiza-tion(N,V) :- near_word(N,V), near_verb(N,V), pre-cedes(V,N), noun_ph_head(N), pred(N,C), preposi-tion(C), V 6=C, N 6=C, N 6=V.
This pattern meansV + (anything but a verb)?
+ (preposition) + Nhead of a noun phrase and retrieves pairs such as: les dispositifs d'impression n'?taient pas contr?l?s parordinateur (the printing devices were not con-trolled by computer ).Prepositions also play an important partand appear frequently in the patterns: re-alization(N,V) :- near_verb(N,V), pred(N,C),lemma(C,sur), V 6=C, N 6=C, N 6=V.
(that is V +(anything but a verb)* + sur + N) covers forexample un terminal (...) permet de travailler surl'ordinateur (a terminal (...) allows [one/theuser] to work on the computer ).The pattern realization realization(N,V):- near_verb(N,V), precedes(V,N), pred(N,C),lemma(C,?), V 6=C, N 6=C, N 6=V., that is V +(anything but a verb)* + ?
+ N, covers commentvous connecter ?
Internet (How to connect to theInternet).realization(N,V) :- near_verb(N,V), precedes(N,V),pred(V,C), lemma(C,?), common_noun(N), V 6=C,N 6=C, N 6=V., that is N + (anything but a verb)* +?
+ V , covers (...) mode de traitement des don-n?es suivant lequel les programmes ?
ex?cuter (...)(...mode of data processing in which the pro-grams to execute...).A certain number of patterns express struc-tures more specific to our corpus.
For example,the clause:realization(N,V) :- near_verb(N,V), precedes(V,N),suc(N,C), proper_noun(C), common_noun(N), V 6=C,N 6=C, N 6=V.
(that is, V + (anything but a verb)* +common noun N + (proper noun)) is very specific tostructures including a proper noun, such as thesentence: (...) Internet utilise le protocole TCP/IP(...) (the Internet uses the TCP/IP protocol).5.2 Methodology for evaluationIn order to evaluate the quality of the extractedN-V pairs, we are interested in two differentmeasures.
The first one expresses the complete-ness of the set of retrieved N-V pairs, that is,how many valid pairs are found with respectto the total number of pairs which should havebeen found; this is the recall rate.
The secondmeasure indicates the reliability of the set of re-trieved N-V pairs, that is, how many valid pairsare found with respect to the total number of re-trieved pairs; this is the precision rate (definedbelow).
These two rates were evaluated using atest sample containing all this information.CompuTerm 2004  -  3rd International Workshop on Computational Terminology42To construct this test set, we have focusedour attention on ten domain-specific terms: com-mande (command), configuration, fichier (file), In-ternet, logiciel (software), option, ordinateur (com-puter), serveur (server), syst?me (system), utilisa-teur (user).
The terms have been identified asthe most specific to our corpus by a programdeveloped by Drouin (2003) and called Termo-Stat.
The ten most specific nouns have beenproduced by comparing our corpus of comput-ing to the French corpus Le Monde, composedof newspaper articles (Lemay et al, 2004).
Notethat to prevent any bias in the results, none ofthese terms were used as positive examples dur-ing the pattern inference step.
(They were re-moved from the example set.
)For each of these 10 nouns, a manual identifi-cation of valid and invalid pairs was carried out.Linguists were asked to analyze the sentencesand decide whether the highlighted pairs werevalid N-V pairs.
Examples of the sentences pro-duced are given in Table 2 for the term utilisateur(user).
A pair is considered as valid if at leastone of its occurrences has the desired semantic(and syntactic) relationship (cf.
section 2).Finally, 603 of the N-V pairs examined arevalid and 4446 are considered not to be valid.The results for each noun are detailed in Table 3.valid non-valid totalcommande 114 346 460configuration 4 361 365fichier 21 604 625option 69 324 393syst?me 82 605 687Internet 9 535 544ordinateur 85 432 517utilisateur 96 435 531logiciel 64 440 504serveur 59 364 423Total 603 4446 5049Table 3: Summary of the test set5.3 ResultsTo compare the results obtained by our tech-nique with the analysis carried out manually,we use the traditional precision/recall approach.Thus we applied the patterns to the corpus andkept all the pairs retrieved when N is one of theten specific nouns.
The results of the compari-son are summarized with the help of confusionmatrices like the one presented in Table 43.actual actualvalid non-valid Totalpredicatedvalid TP FP PrPpredicatednon-valid FN TN PrNTotal AP AN STable 4: Confusion matrixIt is important to note here that the values inthis confusion matrix depend on a parameter: adetection threshold.
Indeed, a single occurrenceof a N-V pair with the patterns is not sufficientfor a pair to be considered as valid.
The thresh-old, called s, represents the minimal number ofoccurrences to be detected to consider a pair asvalid.
The recall and precision rates (respec-tively R and P ), measured on our test set, arethus defined according to s.In order to represent every possible value of Rand P according to s, we draw a recall-precisiongraph in which each value of P is related toits corresponding value of R. Figure 1 givesthe graph obtained when applying the inferredpatterns to the test set.
For comparison pur-poses, Figure 1 also indicates the recall-precisiongraphs obtained by two common statistical tech-niques for collocation acquisition (the Loglikeet Mutual Information coefficients presented insection 4.3).
As a baseline, this graph also givesthe density, computed as AP/S, which repre-sents the precision that would be obtained bya system deciding randomly if a pair is valid ornot.The recall-precision graph shows that oursymbolic technique outperforms the two statis-tical ones; for a fixed recall, the precision gainis up to 45% with respect to the Loglike resultsand is even higher with respect the MI coeffi-cient.
Thus, our acquisition technique meets theobjective of offering assistance to the terminog-rapher, since it provides many reliable N-V paircandidates.
However, results show that invalidpairs are also retrieved; thus a manual analysisremains unavoidable.5.4 Discussion of the resultsWhen examining the retrieved pairs, it appearsthat most invalid N-V pairs can be classified in3The meaning of the variables is given by the combi-nation of the letters: A means actual, Pr predicated, Ttrue, F false, P positive and N negative.CompuTerm 2004  -  3rd International Workshop on Computational Terminology 43Examples Commentutilisateur___ex?cuter 162.823 13.907792621292* MemCheckBoxInRunDlg Autorise les utilisateurs#N# ?
ex?cuter#V# unprogramme 16 bits dans un processus VDM ( Virtual DOS Machine ) d?di?
( non partag? )
.Valid syntactic and seman-tic relationship: "user runsa program"utilisateur___formater 162.823 0.27168791461736* AllocateDasd D?termine quels utilisateurs#N# peuvent formater#V# et?jecter les disques_durs amovibles .Valid syntactic and seman-tic relationship: "user for-mats a hard disk"utilisateur___lancer 162.823 17.9010990569368* Il utilise donc le num?ro de l_ utilisateur#N# r?el qui a lanc?#V#la commande pour savoir si c_ est bien l_ utilisateur#N# root qui l_a lanc?#V# .Valid syntactic and se-mantic relationship: "userlaunches a command"Counter-examples Commentutilisateur___accepter 162.823 0.0059043737128377* Ces commandes sont absolument essentielles pour pouvoirutiliser le syst?me , mais elles sont assez r?barbatives et peu_d_utilisateurs#N# acceptent#V# de s_ en contenter .Syntactic relationship isvalid; Semantic relation-ship is not validutilisateur___entourer 162.823 0.41335402801632* Mais , c?t?
utilisateur#N# , plus on a entour?#V# ou remplison Macintosh de p?riph?riques , plus grands sont les risques derencontrer des blocages ou des sautes_d_humeur .Syntactic relationship isnot validTable 2: Positive and negative examples with utilisateur (Engl.
user)00.20.40.60.810  0.2  0.4  0.6  0.8  1PrecisionRecallASARESDensityMutual InformationLoglikeFigure 1: Recall-Precision graphone of the four following categories.First, some errors are due to tagging mistakes.For example, in the sentence  la premi?re solutionest d'utiliser la commande date  (the first solutionis to use the date command ) the noun datewas incorrectly tagged as a verb, and the N-Vpair commande-dater was retrieved by one of theinferred patterns.
Even if this kind of error doesnot come directly from our acquisition techniqueand does not call into question our approach, itis still a factor that should be taken into ac-count, especially when considering the choice ofCompuTerm 2004  -  3rd International Workshop on Computational Terminology44the tagger and the quality of the texts compos-ing our corpus.Secondly, in a few cases, there is no syn-tactic link between N and V, as is thecase with logiciel-garantir (software-guarantee) incette phase garantit la v?rification du logiciel (thisstep guarantees the software verification).
Evenif these errors are rare in the retrieved pairs ex-amined, our symbolic extraction system couldcertainly be enhanced with information aboutthe syntactic function of nouns (subject, directobject...).
The learning algorithm could thenincorporate this information and produce morerelevant patterns.Thirdly, some N-V pairs are retrieved, al-though there is no semantic link between N andV, or at least, not a semantic link that wouldbe encoded by a terminographer in a dictionary.This is the case for ordinateur-savoir (computer-know) in  l'ordinateur ne sait pas o?
chercher (thecomputer does not know where to look ).
Again,morpho-syntactic information is not always suf-ficient to distinguish between semantically re-lated pairs and non-semantically (but syntacti-cally) related ones.Finally, other very frequent errors are causedby the fact that there actually is an interestingsemantic link between N and V in a retrievedpair, but not the realization link we are lookingfor.
Indeed, some nouns belonging to specificsemantic classes will often cooccur with verbsexpressing realization meanings (and thus oftenappear in valid N-V pairs) while others do not.For example, nouns like ordinateur (computer)and utilisateur (user) often appear in valid N-Vpairs.
On the other hand, other nouns clearlydo not appear in combinations with realizationverbs (e.g., configuration; Internet, refer to Table3).These last two kinds of error clearly illustratethe limitations of our symbolic approach (butare also very frequent errors in statistical ap-proaches since cooccurrence is not enough tocapture subtle semantic distinctions).
In fact,they tend to show that our method could beenhanced if it could incorporate richer linguis-tic information.
Indeed, morpho-syntactic infor-mation is not always sufficient to operate fine-grained sense distinctions.
For example, thetwo sentences below have the same combina-tion of Part-of-Speech tags: vous pouvez utiliser lacommande exit (you can use the exit command)and vous devez choisir l'option 64MB (you mustchoose the 64MB option); in the first one theunderlined N-V pair is valid whereas this is notthe case in the second one.
Realization verbsfor option would be valider (validate) and activer(activate), for example.
Here again, these subtledistinctions could be handled by our symbolicmethod provided that some semantic informa-tion is given on nouns This could be suppliedby a semantic tagger.6 Concluding remarks and futureworkWe have presented an original acquisition pro-cess for noun-verb pairs in which verbs convey arealization meaning.
These noun-verb pairs areacquired from a domain-specific corpus of com-puting.
Our acquisition method, which relieson asares, a extraction pattern inference tech-nique, produces results that are quite good andcould improve manual terminographical work.In particular, our method outperforms classicalstatistical techniques often used for collocationacquisition.
Moreover, the inferred patterns giveinteresting clues about the structures that arelikely to convey the target semantic link.Many possibilities for future work are sug-gested by this experiment.
Concerning our ac-quisition process, some adaptations could cer-tainly improve the results, currently limitedby the sole use of Part-of-Speech tags andnoun-phrase information.
As was previouslymentioned, syntactic and semantic informationcould be added to the corpus through a tag-ging and parsing process.
These two enrich-ments could help to overcome some limitationsof our symbolic approach to capture the natureof N-V relationships.
In terms of applications, itwould be interesting to use a similar techniquefor the acquisition of other more specific seman-tic links between nouns and verbs and even be-tween nouns and nouns or other categories ofwords.
These semantic relationships would al-low us to complete the description of the ter-minological units contained in our dictionary ofcomputing.
The comparison of the acquisitionresults and of the inferred patterns could leadto interesting insights.AknowlegementsThe authors would like to thank Sahara IvethCarre?o Cruz and L?onie Demers-Dion for theirhelp in analyzing the data and Elizabeth Marsh-man for her comments on a previous version ofthis paper.CompuTerm 2004  -  3rd International Workshop on Computational Terminology 45ReferencesPierrette Bouillon, Vincent Claveau, C?cileFabre, and Pascale S?billot.
2001.
UsingPart-of-Speech and Semantic Tagging for theCorpus-Based Learning of Qualia StructureElements.
In First International Workshopon Generative Approaches to the Lexicon,GL'2001, Geneva, Switzerland.Pierrette Bouillon, Vincent Claveau, C?cileFabre, and Pascale S?billot.
2002.
Acqui-sition of Qualia Elements from Corpora Evaluation of a Symbolic Learning Method.In 3rd International Conference on LanguageResources and Evaluation, LREC 02, Las Pal-mas de Gran Canaria, Spain.Kenneth W. Church and Patrick Hanks.
1990.Word Association Norms, Mutual Informa-tion, and Lexicography.
Computational Lin-guistics, 16(1):2229.Vincent Claveau, Pascale S?billot, C?cile Fabre,and Pierrette Bouillon.
2003.
Learning Se-mantic Lexicons from a Part-of-Speech andSemantically Tagged Corpus using Induc-tive Logic Programming.
Journal of Ma-chine Learning Research, special issue on ILP,4:493525.B?atrice Daille.
2003.
Conceptual structuringthrough term variation.
InWorkshop on Mul-tiword Expressions.
Analysis, Acquisition andTreatment.
Proceedings of the ACL'03, Sap-poro, Japan.Patrick Drouin.
2003.
Term-extraction usingnon-technical corpora as a point of leverage.Terminology, 9(1):99115.Ted E. Dunning.
1993.
Accurate methods forthe statistics of surprise and coincidence.Computational Linguistics, 19(1):6174.Natalia Grabar and Pierre Zweigenbaum.2002.
Lexically-based terminology structur-ing.
some inherent limits.
In Second Work-shop on Computational Terminology, Com-puterm 2002.
Coling 2002, Taipei, Taiwan.Beno?t Habert, Ellie Naulleau, and AdelineNazarenko.
1996.
Symbolic word clusteringfor medium-sized corpora.
In Proceedings ofthe 16th Conference on Computational Lin-guistics, Coling'96, Copenhagen, Denmark.Zellig Harris.
1971.
Structures math?matiquesdu langage.
Paris: Dunod.Adam Kilgarriff and David Tugwell.
2001.Word-Sketch: Extraction and Display ofSignificant Collocations for Lexicography.
InWorkshop on Collocation: ComputationalExtraction, Analysis and Exploitation, 39thACL and 10th EACL Conference, Toulouse,France.Chantal Lemay, Marie-Claude L'Homme, andPatrick Drouin.
2004.
Two methods for ex-tracting "specific" single-word terms fromspecialized corpora.
Forthcoming.Marie-Claude L'Homme.
2004.
S?lection de ter-mes dans un dictionnaire d'informatique :Comparaison de corpus et crit?res lexico-s?mantiques.
In Euralex 2004.
Proceedings,Lorient, France.
Forthcoming.Christopher D. Manning and Hinrich Sch?tze.1999.
Foundations of Statistical Natural Lan-guage Processing.
The MIT Press, Cam-bridge, MA, USA.Igor Mel'?uk, Nadia Arbatchewsky-Jumarie,L?o Elnitsky, Lidija Iordanskaja, Ad?le Les-sard, Louise Dagenais, Marie-No?lle Lefeb-vre, Suzanne Mantha, and Alain Polgu?re.19841999.
Dictionnaire explicatif et combi-natoire du fran?ais contemporain, Rechercheslexico-s?mantiques, volumes I-IV.
Les Pressesde l'Universit?
de Montr?al, Montr?al, QC,Canada.Stephen Muggleton and Luc De-Raedt.
1994.Inductive Logic Programming: Theory andMethods.
Journal of Logic Programming, 19-20:629679.Rochdi Oueslati.
1999.
Aide ?
l'acquisitionde connaissances ?
partir de corpus.
Ph.D.thesis, Univesit?
Louis Pasteur, Strasbourg,France.Darren Pearce.
2002.
A Comparative Evalu-ation of Collocation Extraction Techniques.In 3rd International Conference on LanguageResources and Evaluation, LREC 02, Las Pal-mas de Gran Canaria, Spain.James Pustejovsky.
1995.
The Generative Lexi-con.
The MIT Press, Cambridge, MA, USA.Frank Smadja.
1993.
Retrieving Collocationsfrom Text: Xtract.
Computational Linguis-tics, 19(1):143178.Ellen M. Voorhees.
1994.
Query Expansion Us-ing Lexical-Semantic Relations.
In Proceed-ings of ACM SIGIR'94, Dublin, Ireland.CompuTerm 2004  -  3rd International Workshop on Computational Terminology46
