Improving Chinese Word Segmentation by AdoptingSelf-Organized Maps of Character N-gramChongyang ZhangiFLYTEK Researchcyzhang@iflytek.comZhigang CheniFLYTEK Research?zgchen@iflytek.comGuoping Hu?iFLYTEK Researchgphu@iflytek.com??
?AbstractCharacter-based tagging method hasachieved great success in Chinese WordSegmentation (CWS).
This paperproposes a new approach to improve theCWS tagging accuracy by combiningSelf-Organizing Map (SOM) withstructured support vector machine(SVM) for utilization of enormousunlabeled text corpus.
First, characterN-grams are clustered and mapped intoa low-dimensional space by adoptingSOM algorithm.
Two different maps arebuilt based on the N-gram?s precedingand succeeding context respectively.Then new features are extracted fromthese maps and integrated into thestructured SVM methods for CWS.Experimental results on Bakeoff-2005database show that SOM-based featurescan contribute more than 7% relativeerror reduction, and the structured SVMmethod for CWS proposed in this paperalso outperforms traditional conditionalrandom field (CRF) method.1 IntroductionIt is well known that there is no space or anyother separators to indicate the word boundaryin Chinese.
But word is the basic unit for mostof Chinese natural language process tasks, suchas Machine Translation, Information Extraction,Text Categorization and so on.
As a result,Chinese word segmentation (CWS) becomesone of the most fundamental technologies inChinese natural language process.In the last decade, many statistics-basedmethods for automatic CWS have beenproposed with development of machine learningand statistical method (Huang and Zhao, 2007).Especially, the character-based tagging methodwhich was proposed by Nianwen Xue (2003)achieves great success in the secondInternational Chinese word segmentationBakeoff in 2005 (Low et al, 2005).
Thecharacter-based tagging method formulates theCWS problem as a task of predicting a tag foreach character in the sentence, i.e.
everycharacter is considered as one of four differenttypes in 4-tag set: B (begin of word), M (middleof word), E (end of word), and S (single-character word).Most of these works train tagging modelsonly on limited labeled training sets, withoutusing any unsupervised learning outcomes frominnumerous unlabeled text.
But in recent years,researchers begin to exploit the value ofenormous unlabeled corpus for CWS.
Somestatistics information on co-occurrence of sub-sequences in the whole text has been extractedfrom unlabeled data and been employed as inputfeatures for tagging model training (Zhao andKit , 2007).Word clustering is a common method toutilize unlabeled corpus in language processingresearch to enhance the generalization ability,such as part-of-speech clustering and semanticclustering (Lee et al, 1999 and B Wang and HWang 2006).
Character-based tagging methodusually employs N-gram features, where an N-gram is an N-character segment of a string.
Webelieve that there are also semantic orgrammatical relationships between most of N-grams and these relationships will be useful inCWS.
Intuitively, assuming the training datacontains the bigram ??
/?
?
(The last twocharacters of the word ?Israel?
in Chinese), notcontain the bigram ??
/?
?
(The last twocharacters of the word ?Turkey?
in Chinese), ifwe could cluster the two bigrams togetheraccording to unlabeled corpus and employ it asa feature for supervised training of taggingmodel, maybe we will know that there shouldbe a word boundary after ??/??
though weonly find the existence of word boundary after??/??
in the training data.
So we investigatehow to apply clustering method onto unlabeleddata for the purpose of improving CWSaccuracy in this paper.This paper proposes a novel method of usingunlabeled data for CWS, which employs Self-Organizing Map (SOM) (Kohonen 1982) toorganize Chinese character N-grams on a two-dimensional array, named as ?N-gram clustermap?
(NGCM), in which the character N-gramssimilar in grammatical structure and semanticmeaning are organized in the same or adjacentposition.
Two different arrays are built based onthe N-gram?s preceding context and succeedingcontext respectively because sometimes N-gramis just a part of Chinese word and does not sharesimilar preceding and succeeding context in thesame time.
Then NGCM-based features areextracted and applied to tagging model of CWS.Two tagging models are investigated, which arestructured support vector machine (SVM)(Tsochantaridis et al, 2005) model andConfidential Random Fields (CRF) (Lafferty etal., 2001).
The experimental results show thatNGCM is really helpful to CWS.
In addition,we find that the structured SVM achieves betterperformance than CRF.The rest of this paper is organized as follows:Section 2 presents self-organizing map and theidea of N-gram cluster maps.
Section 3describes structured SVM and how to use theNGCMs based features in CWS.
Section 4shows experimental results on Bakeoff-2005database and Section 5 gives our conclusion.2 N-gram cluster mapsSupervised learning method for CWS needsenough pre-labeled corpus with word boundaryinformation for training.
The final CWSperformance relies heavily on the quality of thetraining data.
The training data is limited andcannot cover completely the linguisticphenomenon.
But unlabeled corpus can beobtained easily from internet.
One intuitivemethod is to extract information fromunsupervised learning results from enormousunlabeled data to enhance supervised learning.2.1 Self-Organizing MapThe Self-Organizing Map (SOM) (Kohonen1982), sometimes called Kohonen map, wasdeveloped by Teuvo Kohonen in the early1980s.
Different from other clustering method,SOM is a type of artificial neural network onthe basis of competitive learning to visualizehigher dimensional data in a low-dimensionalspace (usually 1D or 2D) while preserving thetopological properties of the input space.
Figure1 displays a 2D SOM.Best matching unitInputTwo-dimensionalarray of neurousFigure 1: SOM modelIn SOM, the input is a lot of data samples,and each sample is represented as a vector, 1,2,...,ix i M= , where M is the number of theinput vectors.
SOM will cluster all thesesamples into L neurons, and each neuron isassociated with a weight vector , 1,2,...,iw i L= ,where L  is the total number of the neurons.?
jw ?is?
of the same dimensions as the input datavectors ix .
The learning algorithm of SOM isas follows:1.
Randomize every neuron?s weight vectoriw  ;2.
Randomly select an input vector tx  ;3.
Find the winning neuron j ?, whoseassociate weight vector jw  has  theminimal distance to tx  ;4.
Update the weight vector of all the neuronsaccording to the following formula:( , )( )i i t iw w i j x w???
+ ?Where ?
is the learning-rate and ( , )i j?is the neighborhood function.
A simplechoice defines ( , ) 1i j?
=  for all neuron iin a neighborhood of radius r of neuronj and ( , ) 0i j?
=  for all other neurons.
?and ( , )i j?
usually varied dynamicallyduring learning for best results;5.
Continue step 2 until maximum number ofiterations has been reached or no noticeablechanges are observed.2.2 SOM-based N-gram cluster mapsSelf-organizing semantic maps (Ritter andKohonen 1989, 1990) are SOMs that have beenorganized according to word similarities,measured by the similarity of the short contextsof the words.
Our algorithm of building N-gramcluster maps is similar to self-organizingsemantic maps.
Because sometimes N-gram isjust part of Chinese word and do not sharesimilar preceding and succeeding context in thesame time, so we build two different mapsaccording to the preceding context and thesucceeding context of N-gram individually.
Inthe end we build two NGCMs: NGCMP(NGCM according to preceding context) andNGCMS (NGCM according to succeedingcontext).In this paper we only consider bigram clustermaps.
So our purpose is to acquire a 2GCMPand a 2GCMS.
The large-scale unlabeled corpuswe used for training NGCMs is about 3.5G insize.
It was obtained easily from websites likeSohu, Netease, Sina and People Daily.
Whenthe cut-off threshold is set to 5, we got about 9Kdifferent characters and 380K different bigramsby counting the corpus.
For each bigram, a 9K-dimensional sparse vector can be derived fromthe preceding character of the bigram.
Thereforea collection of 380K vector samples aregenerated, which is denoted as P. Anothervector collection S which considers succeedingcharacter was obtained using the same method.Our implementation used SOM-PAKpackage Version 1.0 (Kohonen et al, 1996).
Weset the topology type to rectangular and the mapsize to15 15?
.
In the training process, we usedP and S as input data respectively.
After thetraining we acquired a 2GCMP and a 2GCMS,meanwhile each bigram was mapped to oneneuron.
Because the number of neurons is muchsmaller than the number of bigrams, eachneuron in the map was labeled with multiplebigrams.
The 2GCMP and 2GCMS are shownin Figure 2 and Figure 3 respectively.
Thecomment boxes in the figures show somesamples of bigrams mapped in the same neuron.Figure 2: 2GCMP0,00,1 1,1 2,10,2 1,2 2,21,0 2,014,1414,214,00,14 1,14 2,1414,1?/??/??/??/??/???/??/??/??/???/??/??/??/?...?/??/??/?
?/?...Figure 3: 2GCMSAfter checking the results, we find that mostof the meaningless bigrams that containcharacters from more than one word, such as thebigram "?? "
in "...????
..." , areorganized into the same neurons in the map, andmost of the first or last bigrams of the countrynames are organized into a few adjacentneurons, such as ??/?
?, ??/?
?, ??/??
and??/?
?in 2GCMS , ??/?
?, ??/?
?, ??/??,??/??
, and ??/??
in 2GCMP.
We also triedto use the preceding and the succeeding contexttogether in NGCM training just like the methodused in the self-organizing semantic maps.
Wefound that the bigrams of ??/?
?, ??/??
and??
/??
will never be assigned to the sameneuron again, which indicates that we need tobuild two NGCMs according to preceding andsucceeding context separately.3 Integrate NGCM into StructuredSVM for CWS3.1 Structured support vector machineThe structured support vector machine can learnto predict structured y , such as trees sequencesor sets, from x  based on large-margin approach.We employ a structured SVM that can predict asequence of labels 1( ,..., )Ty y y=  for a givenobservation sequence 1( ,..., )Tx x x= , wherety ??
,?
is the label set for y.There are two types of features in thestructured SVM: transition features (interactionsbetween neighboring labels along the chain),emission features (interactions betweenattributes of the observation vectors and aspecific label).we can represent the input-outputpairs via joint feature map (JFM)1111( ) ( )( , )( ) ( )Tt c ttTc t c ttx yx yy y???=?
+=?
????
??
?= ?
??
???
??
??
?where{1 21 2,1,0,( ) ( ( , ), ( , ),..., ( , )) '{0,1} ,  { , ,..., }Kronecker delta  ,cKKKi ji ji jy y y y y y yy y y y?
?
??
?
=???
??
?
==( )x?
denotes an arbitrary feature representationof the inputs.
The sign " "?
expresses tensorproduct defined as ?
: d kR R?
dkR?
,[ ] ( 1)i j da b + ??
[ ] [ ]i ja b= .T  is the length of anobservation sequence.
0?
?
is a scaling factorwhich balances the two types of contributions.Note that both transition features andemission features can be extended by includinghigher-order interdependencies of labels (e.g.1 2( ) ( ) ( )c t c t c ty y y+ +?
??
??
),by includinginput features from a window centered at thecurrent position (e.g.
replacing ( )tx?
with( ,..., ,... )t r t t rx x x?
?
+ )or by combining higher-order output features with input features (e.g.1( ) ( ) ( )t c t c ttx y y?
+??
???
)The w-parametrized discriminant function:F X Y R?
?
interpreted as measuring thecompatibility of x and y is defined as:( , ; ) , ( , )F x y w w x y?=So we can maximize this function over theresponse variable to make a prediction( ) arg max ( , , )y Yf x F x y w?=Training the parameters can be formulated asthe following optimization problem.,11min ,2. .
, :, ( , ) ( , ) ( , )niwii i i i i i iCw wns t i y Yw x y x y y y?
??
?
?=+?
?
??
?
?
?
?where n  is the number of the training samples,i?
is a slack variable , 0C ?
is a constantcontrolling the tradeoff between training errorminimization and margin maximization,1( , )y y?
is the loss function ,usually thenumber of misclassified tags in the sentence.3.2 Features set for tagging modelFor a training sample denoted as1( ,..., )Tx x x=  and 1( ,..., )Ty y y= .
We chosefirst-order interdependencies of labels to betransition features, and dependencies betweenlabels and N-grams (n=1, 2, 3) at currentposition in observed input sequence to beemission features.So our JFM is the concatenation of the followvectors111( ) ( )Tc t c tty y?
+=?
???
,1( ) ( ), { 1,0,1}Tt m c ttx y m?
+=??
?
?
?11( , ) ( ), { 2, 1,0,1}Tt m t m c ttx x y m?
+ + +=??
?
?
?
?1 11( , , ) ( ), { 1,0,1}Tt m t m t m c ttx x x y m?
+ ?
+ + +=??
?
?
?Figure 4 shows the transition features and theemission features of N-grams (n=1, 2) at 3y .The emission features of 3-grams are not shownhere because of the large number of theinteractions.1y 2y 3y 4y1x 2x 3x 4x5y5xFigure 4: the transition features and theemission features at 3y  for structured SVM3.3 Using NGCM in CWSTwo methods can be used for extracting thefeatures from NGCMs to expend featuresdefinition in section 3.2.One method is to treat NGCM just as aclustering tool and do not take into account thesimilarity between adjacent neurons.
So a newfeature with L dimensions can be generated,where L is the number of the neurons or classes.Only one value of the L dimension equals to 1and others equal to 0.
We call it NGCMclustering feature.Another way of using the NGCM is to adoptthe position of the neurons which current N-gram mapped in the NGCM as a new feature.So every feature has D dimensions (D equals tothe dimension of the NGCM, every dimensionis corresponding to the coordinate value in theNGCM).
In this way, N-gram which isoriginally represented as a high dimensionalvector based on its context is mapped into avery low-dimensional space.
We call it NGCMmapping feature.In this paper, we only consider the NGCMclustering or mapping features related to thecurrent label iy .
We also extract features fromthe quantization error of current N-grambecause the result of the NGCM is very noisy.Then our previous JFM in section 3.2 isconcatenated with the following features:2GCMS 11( , ) ( ), { 2, 1}Tt m t m c ttx x y m?
+ + +=??
?
?
?
?2GCMP 11( , ) ( ), {0,1}Tt m t m c ttx x y m?
+ + +=??
?
?2GCMS 11( , ) ( ), { 2, 1}Tt m t m c ttx x y m?
+ + +=??
?
?
?
?2GCMP 11( ) ( ), {0,1}Tt m t m c ttx x y m?
+ + +=??
??
?where 2GCMS( )x?
denotes the NGCM featurefrom 2GCMS, 2GCMP ( )x?
denotes the NGCMfeature from 2GCMP.?
NGCM ( )x?
denotes thequantization error  of  current N-gram x on itsNGCM.
?In 15 15?
size NGCM, when we use theNGCM clustering feature 2GCMS( )x?
and2GCMP ( )x?
15 15{0,1} ??
.
When we use theNGCM mapping feature 2GCMS( )x?
and2GCMP ( )x?
2{0,1,...,14}?
.?
Notice that thedimension of the NGCM clustering feature ismuch higher than the NGCM mapping feature.As an example, the process of import featuresfrom NGCMs at 3y  is presented in Figure 5.1y 2y 3y 4y1x 2x 3x 4x5y5x2GCMS 2GCMPFigure 5: Using 2GCMS and 2GCMP as inputto structured SVM4 Applications and Experiments4.1 CorpusWe use the data adopted by the secondInternational Chinese Word SegmentationBakeoff (Bakeoff-2005).
The corpus sizeinformation is listed in Table 1.Corpus As CityU MSRA PKUTraining(M) 5.45 1.46 2.37 1.1Test(K) 122 41 107 104Table 1: Corpus size of Bakeoff-2005 innumber of words4.2 Text PreprocessingText is usually mixed up with numerical oralphabetic characters in Chinese naturallanguage, such as ???
office ?????
9??.
These numerical or alphabetic charactersare barely segmented in CWS.
Hence, we treatthese symbols as a whole ?character?
accordingto the following two preprocessing steps.
Firstreplace one alphabetic character to fourcontinuous alphabetic characters with E1 to E4respectively, five or more alphabetic characterswith E5.
Then replace one numerical number tofour numerical numbers with N1 to N4 and fiveor more numerical numbers with N5.
After textpreprocessing, the above examples will be ???
E5?????
N1?
?.4.3 Character-based tagging methodfor CWSPrevious works show that 6-tag set achieveda better CWS performance (Zhao et al,2006).
Thus, we opt for this tag set.
This 6-tag set adds ?B2?
and ?B3?
to 4-tag setwhich stand for the type of the second andthe third character in a Chinese wordrespectively.
For example, the tag sequencefor the sentence ??????/?/??/??
(Shanghai World Expo / will / last / sixmonths)?
will be ?B B2 B3 M E S B E B E?.4.4 ExperimentsThe F-measure is employed for evaluation,which is defined as follows:num of correctly segmented wordsPrecision: Pnum of the system output words=num of correctly segmented wordsRecall: Rnum of total words in test data=2 P RF-measure: FP R?
?= +To compare with other discriminativelearning methods we first developed a baselinesystem using conditional random field (CRF)without using NGCM feature and then wedeveloped another CRF system: CFCRF (usingNGCM clustering features).
In the end wedeveloped three structured SVM CWS systems:SVM (without using NGCM features), CFSVM(using NGCM clustering features), andMFSVM (using NGCM mapping features).
Thefeatures for the baseline CRF system are thesame with the SVM system.
The features forCFCRF are the same with CFSVM.
The resultof the CRF system using NGCM mappingfeatures cannot be given here, because it isdifficult to support continuous-value featuresfor CRF method which is based on theMaximum Entropy Model.We use CRF++ version 0.5 (Kudu, 2009) tobuild our CRF models.
The cut-off threshold isset to 2(using the features that occurs no lessthan 2 times in the given training data) and thehyper-parameter is set to 4.5.
We use hmmsvmversion 3.1 to build our structured SVM models.The cut-off threshold is set to 2.
The precisionparameter is set to 0.1.
The tradeoff betweentraining error minimization and marginmaximization is set to 1000.The comparisons between CRF, CFCRF, SVM,CFSVM and MFSVM are shown in Table 2.Corpus  As CityU MSRA PKUCRFbaselineP 0.945 0.943 0.971 0.953R 0.955 0.942 0.970 0.946F 0.950 0.942 0.971 0.950CFCRF P 0.948 0.956 0.973 0.959R 0.959 0.961 0.972 0.952F 0.953 0.958 0.973 0.955SVM P 0.949 0.957 0.972 0.953R 0.959 0.959 0.972 0.946F 0.954 0.958 0.972 0.950CFSVM P 0.952 0.959 0.974 0.958R 0.960 0.964 0.974 0.952F 0.956 0.961 0.974 0.955MFSVMP 0.950 0.957 0.974 0.958R 0.961 0.963 0.974 0.951F 0.956 0.960 0.974 0.954Table 2: The results of our systems4.5 DiscussionFrom Table 2, we can see that:1) The NGCM feature is useful for CWS.
Thefeature achieves 13.9% relative errorreduction on CRF method and 7.2% relativeerror reduction on structured SVM method;2) CFSVM and MFSVM achieve similarperformance, differ from the expectation ofMFSVM should be better than CFSVM.
Wethink that this is because the size of 2GCMsis too small.
Due to the limitation of ourcomputer and time we only get two 15 15?size 2GCMs, similarity between adjacentneurons on the two small 2GCMs is veryweek, NGCM cluster feature performs asgood as NGCM mapping feature on CWS.But due to the dimensions of the NGCMcluster feature is much larger than theNGCM mapping feature, the training timeof the CFSVM is much longer than theMFSVM;3) It is obvious that structured SVM performsbetter than CRF, demonstrating the benefitof large margin approach.5 ConclusionThis paper proposes an approach to improveCWS tagging accuracy by combining SOM withstructured SVM.
We use SOM to organizeChinese character N-grams on a two-dimensional array, so that the N-grams similarin grammatical structure and semantic meaningare organized in the same or adjacent position.Two different maps are built based on the N-gram?s preceding and succeeding contextrespectively.
Then new features are extractedfrom these maps and integrated into thestructured SVM methods for CWS.Experimental results on Bakeoff-2005 databaseshow that SOM-based features can contributemore than 7% relative error reduction, and thestructured SVM method for CWS,?
to ourknowledge, first proposed in this paper alsooutperforms traditional CRF method.In future work, we will try to organizing allthe N-grams on a much larger array, so thatevery neuron will be labeled by a single N-gram.Our ultimate objective is to reduce thedimension of input features for supervised CWSlearning , such as structured SVM , by replacingN-gram features with two-dimensional NGCMmapping features in most of Chinese naturallanguage process tasks.ReferencesB.Wang, H.Wang 2006.A Comparative Study onChinese Word Clustering.
Computer Processingof Oriental Languages.
Beyond the Orient: TheResearch Challenges Ahead, pages 157-164Chang-Ning Huang and Hai Zhao.
2007.
Chineseword segmentation: A decade review.
Journal ofChinese Information Processing, 21(3):8?20.Chung-Hong Lee & Hsin-Chang Yang.1999, A WebText Mining Approach Based on Self-OrganizingMap, ACM-libraryG.Bakir, T.Hofmann, B.Scholkopf, A.Smola, B.Taskar, and S. V. N. Vishwanathan, editors.
2007Predicting Structured Data.
MIT Press,Cambridge, Massachusetts.Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang Lu.
2006.
Effective tag set selectioninChinese word segmentation via conditionalrandom field modeling.
In Proceedings ofPACLIC-20, pages 87?94.
Wuhan, China.Hai Zhao, Chang-Ning Huang, and Mu Li.
2006.Animproved Chinese word segmentation system withconditional random field.
In SIGHAN-5, pages162?165, Sydney, Australia, July 22-23.Hai Zhao and Chunyu Kit.
2007.
Incorporatingglobal information into supervised learning forChinese word segmentation.
In PACLING-2007,pages 66?74, Melbourne,Australia, September 19-21.H.Ritter, and T.Kohonen, 1989.
Self-organizingsemantic maps.
Biological Cybernetics, vol.
61,no.
4, pp.
241-254.I.Tsochantaridis,T.Joachims,T.Hofmann,and Y.Altun.2005.
Large Margin Methods for Structured andInterdependent Output Variables, Journal ofMachine Learning Research (JMLR),6(Sep):1453-1484.Jin Kiat Low, Hwee Tou Ng, and WenyuanGuo.2005.
A maximum entropy approach toChinese word segmentation.
In Proceedings of theFourth SIGHAN Workshop on Chinese LanguageProcessing, pages 161?164.
Jeju Island,Korea.J.Lafferty,A.McCallum, F.Pereira.
2001.
Conditionalrandom fields: Probabilistic models forsegmenting and labeling sequence data.
InProceedings of the International Conference onMachine Learning (ICML).
San Francisco:Morgan Kaufmann Publishers, 282?289.Nianwen Xue and Susan P.
Converse., 2002,Combining Classifiers for Chinese WordSegmentation, In Proceedings of First SIGHANWorkshop on Chinese Language Processing.Nianwen Xue.
2003.
Chinese word segmentation ascharacter tagging.
Computational Linguistics andChinese Language Processing, 8(1):29?48.R.Sproat and T.Emerson.
2003.The firstinternational Chinese word segmentation bakeoff.In The Second SIGHAN Workshop on ChineseLanguage Processing, pages 133?143.Sapporo,Japan.S.Haykin, 1994.
Neural Networks: A ComprehensiveFoundation.
NewYork: MacMillan.T.Joachims, T.Finley, Chun-Nam Yu.
2009, Cutting-Plane Training of Structural SVMs, MachineLearning Journal,77(1):27-59.T.Joachims.
2008 .hmmsvm  Sequence Tagging withStructural Support Vector Machines,http://www.cs.cornell.edu/People/tj/svm_light/svm_hmm.htmlT.Honkela, 1997.
Self-Organizing Maps in NaturalLanguage Processing.
PhD thesis, HelsinkiUniversity of Technology, Department ofComputer Science and Engineering, Laboratory ofComputer and Information Science.T.Kohonen.
1982.Self-organized formation oftopologically correct feature maps.
BiologicalCybernetics, 43, pp.
59-69.T.Kohonen., J.Hynninen, J.Kangas, J.Laaksonen,1996 ,SOM_PAK: The Self-Organizing MapProgram Package,Technical Report A31,Helsinki University of Technology ,http://www.cis.hut.fi/nnrc/nnrc-programs.htmlT.Kudu.2009.
CRF++: Yet another CRFtoolkit.
:http://crfpp.sourceforge.net/.Y.Altun, I.Tsochantaridis, T.Hofmann.
2003.
HiddenMarkov Support Vector Machines.
In Proceedingsof International Conference on Machine Learning(ICML).
