Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1252?1261,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsOpinion Mining on YouTubeAliaksei Severyn1, Alessandro Moschitti3,1,Olga Uryupina1, Barbara Plank2, Katja Filippova41DISI - University of Trento,2CLT - University of Copenhagen,3Qatar Computing Research Institute,4Google Inc.severyn@disi.unitn.it, amoschitti@qf.org.qa,uryupina@gmail.com, bplank@cst.dk, katjaf@google.comAbstractThis paper defines a systematic approachto Opinion Mining (OM) on YouTubecomments by (i) modeling classifiers forpredicting the opinion polarity and thetype of comment and (ii) proposing ro-bust shallow syntactic structures for im-proving model adaptability.
We rely on thetree kernel technology to automatically ex-tract and learn features with better gener-alization power than bag-of-words.
An ex-tensive empirical evaluation on our manu-ally annotated YouTube comments corpusshows a high classification accuracy andhighlights the benefits of structural mod-els in a cross-domain setting.1 IntroductionSocial media such as Twitter, Facebook orYouTube contain rapidly changing informationgenerated by millions of users that can dramati-cally affect the reputation of a person or an orga-nization.
This raises the importance of automaticextraction of sentiments and opinions expressed insocial media.YouTube is a unique environment, just likeTwitter, but probably even richer: multi-modal,with a social graph, and discussions between peo-ple sharing an interest.
Hence, doing sentimentresearch in such an environment is highly relevantfor the community.
While the linguistic conven-tions used on Twitter and YouTube indeed showsimilarities (Baldwin et al, 2013), focusing onYouTube allows to exploit context information,possibly also multi-modal information, not avail-able in isolated tweets, thus rendering it a valuableresource for the future research.Nevertheless, there is almost no work showingeffective OM on YouTube comments.
To the bestof our knowledge, the only exception is given bythe classification system of YouTube commentsproposed by Siersdorfer et al (2010).While previous state-of-the-art models for opin-ion classification have been successfully appliedto traditional corpora (Pang and Lee, 2008),YouTube comments pose additional challenges:(i) polarity words can refer to either video or prod-uct while expressing contrasting sentiments; (ii)many comments are unrelated or contain spam;and (iii) learning supervised models requires train-ing data for each different YouTube domain, e.g.,tablets, automobiles, etc.
For example, consider atypical comment on a YouTube review video abouta Motorola Xoom tablet:this guy really puts a negative spin onthis , and I ?m not sure why , this seemscrazy fast , and I ?m not entirely surewhy his pinch to zoom his laggy all theother xoom reviewsThe comment contains a product name xoom andsome negative expressions, thus, a bag-of-wordsmodel would derive a negative polarity for thisproduct.
In contrast, the opinion towards the prod-uct is neutral as the negative sentiment is ex-pressed towards the video.
Similarly, the follow-ing comment:iPad 2 is better.
the superior apps justdestroy the xoom.contains two positive and one negative word, yetthe sentiment towards the product is negative (thenegative word destroy refers to Xoom).
Clearly,the bag-of-words lacks the structural informationlinking the sentiment with the target product.In this paper, we carry out a systematic study onOM targeting YouTube comments; its contributionis three-fold: firstly, to solve the problems outlinedabove, we define a classification schema, whichseparates spam and not related comments from theinformative ones, which are, in turn, further cate-gorized into video- or product-related comments1252(type classification).
At the final stage, differ-ent classifiers assign polarity (positive, negative orneutral) to each type of a meaningful comment.This allows us to filter out irrelevant comments,providing accurate OM distinguishing commentsabout the video and the target product.The second contribution of the paper is the cre-ation and annotation (by an expert coder) of acomment corpus containing 35k manually labeledcomments for two product YouTube domains:tablets and automobiles.1It is the first manu-ally annotated corpus that enables researchers touse supervised methods on YouTube for commentclassification and opinion analysis.
The commentsfrom different product domains exhibit differentproperties (cf.
Sec.
5.2), which give the possibilityto study the domain adaptability of the supervisedmodels by training on one category and testing onthe other (and vice versa).The third contribution of the paper is a novelstructural representation, based on shallow syn-tactic trees enriched with conceptual information,i.e., tags generalizing the specific topic of thevideo, e.g., iPad, Kindle, Toyota Camry.
Given thecomplexity and the novelty of the task, we exploitstructural kernels to automatically engineer novelfeatures.
In particular, we define an efficient treekernel derived from the Partial Tree Kernel, (Mos-chitti, 2006a), suitable for encoding structural rep-resentation of comments into Support Vector Ma-chines (SVMs).
Finally, our results show that ourmodels are adaptable, especially when the struc-tural information is used.
Structural models gen-erally improve on both tasks ?
polarity and typeclassification ?
yielding up to 30% of relative im-provement, when little data is available.
Hence,the impractical task of annotating data for eachYouTube category can be mitigated by the use ofmodels that adapt better across domains.2 Related workMost prior work on more general OM has beencarried out on more standardized forms of text,such as consumer reviews or newswire.
The mostcommonly used datasets include: the MPQA cor-pus of news documents (Wilson et al, 2005), webcustomer review data (Hu and Liu, 2004), Ama-zon review data (Blitzer et al, 2007), the JDPA1The corpus and the annotation guidelines are pub-licly available at: http://projects.disi.unitn.it/iKernels/projects/sentube/corpus of blogs (Kessler et al, 2010), etc.
Theaforementioned corpora are, however, only par-tially suitable for developing models on socialmedia, since the informal text poses additionalchallenges for Information Extraction and Natu-ral Language Processing.
Similar to Twitter, mostYouTube comments are very short, the languageis informal with numerous accidental and deliber-ate errors and grammatical inconsistencies, whichmakes previous corpora less suitable to train mod-els for OM on YouTube.
A recent study focuses onsentiment analysis for Twitter (Pak and Paroubek,2010), however, their corpus was compiled auto-matically by searching for emoticons expressingpositive and negative sentiment only.Siersdorfer et al (2010) focus on exploiting userratings (counts of ?thumbs up/down?
as flagged byother users) of YouTube video comments to trainclassifiers to predict the community acceptance ofnew comments.
Hence, their goal is different: pre-dicting comment ratings, rather than predicting thesentiment expressed in a YouTube comment or itsinformation content.
Exploiting the informationfrom user ratings is a feature that we have not ex-ploited thus far, but we believe that it is a valuablefeature to use in future work.Most of the previous work on supervised senti-ment analysis use feature vectors to encode doc-uments.
While a few successful attempts havebeen made to use more involved linguistic anal-ysis for opinion mining, such as dependencytrees with latent nodes (T?ackstr?om and McDonald,2011) and syntactic parse trees with vectorizednodes (Socher et al, 2011), recently, a comprehen-sive study by Wang and Manning (2012) showedthat a simple model using bigrams and SVMs per-forms on par with more complex models.In contrast, we show that adding structural fea-tures from syntactic trees is particularly useful forthe cross-domain setting.
They help to build a sys-tem that is more robust across domains.
Therefore,rather than trying to build a specialized systemfor every new target domain, as it has been donein most prior work on domain adaptation (Blitzeret al, 2007; Daum?e, 2007), the domain adapta-tion problem boils down to finding a more robustsystem (S?gaard and Johannsen, 2012; Plank andMoschitti, 2013).
This is in line with recent ad-vances in parsing the web (Petrov and McDonald,2012), where participants where asked to build asingle system able to cope with different yet re-1253lated domains.Our approach relies on robust syntactic struc-tures to automatically generate patterns that adaptbetter.
These representations have been inspiredby the semantic models developed for Ques-tion Answering (Moschitti, 2008; Severyn andMoschitti, 2012; Severyn and Moschitti, 2013)and Semantic Textual Similarity (Severyn et al,2013).
Moreover, we introduce additional tags,e.g., video concepts, polarity and negation words,to achieve better generalization across differentdomains where the word distribution and vocab-ulary changes.3 Representations and modelsOur approach to OM on YouTube relies on thedesign of classifiers to predict comment type andopinion polarity.
Such classifiers are traditionallybased on bag-of-words and more advanced fea-tures.
In the next sections, we define a baselinefeature vector model and a novel structural modelbased on kernel methods.3.1 Feature SetWe enrich the traditional bag-of-word representa-tion with features from a sentiment lexicon andfeatures quantifying the negation present in thecomment.
Our model (FVEC) encodes each docu-ment using the following feature groups:- word n-grams: we compute unigrams andbigrams over lower-cased word lemmas wherebinary values are used to indicate the pres-ence/absence of a given item.- lexicon: a sentiment lexicon is a collection ofwords associated with a positive or negative senti-ment.
We use two manually constructed sentimentlexicons that are freely available: the MPQA Lex-icon (Wilson et al, 2005) and the lexicon of Huand Liu (2004).
For each of the lexicons, we usethe number of words found in the comment thathave positive and negative sentiment as a feature.- negation: the count of negation words, e.g.,{don?t, never, not, etc.
}, found in a comment.2Our structural representation (defined next) en-ables a more involved treatment of negation.- video concept: cosine similarity between a com-ment and the title/description of the video.
Mostof the videos come with a title and a short descrip-tion, which can be used to encode the topicality of2The list of negation words is adopted fromhttp://sentiment.christopherpotts.net/lingstruc.htmleach comment by looking at their overlap.3.2 Structural modelWe go beyond traditional feature vectors by em-ploying structural models (STRUCT), which en-code each comment into a shallow syntactic tree.These trees are input to tree kernel functionsfor generating structural features.
Our struc-tures are specifically adapted to the noisy user-generated texts and encode important aspects ofthe comments, e.g., words from the sentiment lexi-cons, product concepts and negation words, whichspecifically targets the sentiment and commenttype classification tasks.In particular, our shallow tree structure is atwo-level syntactic hierarchy built from word lem-mas (leaves) and part-of-speech tags that are fur-ther grouped into chunks (Fig.
1).
As full syn-tactic parsers such as constituency or dependencytree parsers would significantly degrade in perfor-mance on noisy texts, e.g., Twitter or YouTubecomments, we opted for shallow structures, whichrely on simpler and more robust components: apart-of-speech tagger and a chunker.
Moreover,such taggers have been recently updated withmodels (Ritter et al, 2011; Gimpel et al, 2011)trained specifically to process noisy texts show-ing significant reductions in the error rate on user-generated texts, e.g., Twitter.
Hence, we use theCMU Twitter pos-tagger (Gimpel et al, 2011;Owoputi et al, 2013) to obtain the part-of-speechtags.
Our second component ?
chunker ?
is takenfrom (Ritter et al, 2011), which also comes with amodel trained on Twitter data3and shown to per-form better on noisy data such as user comments.To address the specifics of OM tasks onYouTube comments, we enrich syntactic treeswith semantic tags to encode: (i) central con-cepts of the video, (ii) sentiment-bearing wordsexpressing positive or negative sentiment and (iii)negation words.
To automatically identify con-cept words of the video we use context words (to-kens detected as nouns by the part-of-speech tag-ger) from the video title and video description andmatch them in the tree.
For the matched words,we enrich labels of their parent nodes (part-of-speech and chunk) with the PRODUCT tag.
Sim-ilarly, the nodes associated with words found in3The chunker from (Ritter et al, 2011) relies on its ownPOS tagger, however, in our structural representations we fa-vor the POS tags from the CMU Twitter tagger and take onlythe chunk tags from the chunker.1254Figure 1: Shallow tree representation of the example comment (labeled with product type andnegative sentiment): ?iPad 2 is better.
the superior apps just destroy the xoom.?
(lemmas are replacedwith words for readability) taken from the video ?Motorola Xoom Review?.
We introduce additional tagsin the tree nodes to encode the central concept of the video (motorola xoom) and sentiment-bearing words(better, superior, destroy) directly in the tree nodes.
For the former we add a PRODUCT tag on the chunkand part-of-speech nodes of the word xoom) and polarity tags (positive and negative) for the latter.
Twosentences are split into separate root nodes S.the sentiment lexicon are enriched with a polar-ity tag (either positive or negative), while nega-tion words are labeled with the NEG tag.
It shouldbe noted that vector-based (FVEC) model reliesonly on feature counts whereas the proposed treeencodes powerful contextual syntactic features interms of tree fragments.
The latter are automati-cally generated and learned by SVMs with expres-sive tree kernels.For example, the comment in Figure 1 showstwo positive and one negative word from the senti-ment lexicon.
This would strongly bias the FVECsentiment classifier to assign a positive labelto the comment.
In contrast, the STRUCT modelrelies on the fact that the negative word, destroy,refers to the PRODUCT (xoom) since they form averbal phase (VP).
In other words, the tree frag-ment: [S [negative-VP [negative-V[destroy]] [PRODUCT-NP [PRODUCT-N[xoom]]]] is a strong feature (inducedby tree kernels) to help the classifier to dis-criminate such hard cases.
Moreover, treekernels generate all possible subtrees, thusproducing generalized (back-off) features,e.g., [S [negative-VP [negative-V[destroy]] [PRODUCT-NP]]]] or [S[negative-VP [PRODUCT-NP]]]].3.3 LearningWe perform OM on YouTube using supervisedmethods, e.g., SVM.
Our goal is to learn a modelto automatically detect the sentiment and type ofeach comment.
For this purpose, we build a multi-class classifier using the one-vs-all scheme.
A bi-nary classifier is trained for each of the classesand the predicted class is obtained by taking aclass from the classifier with a maximum predic-tion score.
Our back-end binary classifier is SVM-light-TK4, which encodes structural kernels in theSVM-light (Joachims, 2002) solver.
We define anovel and efficient tree kernel function, namely,Shallow syntactic Tree Kernel (SHTK), which isas expressive as the Partial Tree Kernel (PTK)(Moschitti, 2006a) to handle feature engineeringover the structural representations of the STRUCTmodel.
A polynomial kernel of degree 3 is appliedto feature vectors (FVEC).Combining structural and vector models.
Atypical kernel machine, e.g., SVM, classifies atest input x using the following prediction func-tion: h(x) =?i?iyiK(x,xi), where ?iarethe model parameters estimated from the trainingdata, yiare target variables, xiare support vec-tors, and K(?, ?)
is a kernel function.
The lattercomputes the similarity between two comments.The STRUCT model treats each comment as a tu-ple x = ?T ,v?
composed of a shallow syntactictree T and a feature vector v .
Hence, for each pairof comments x1and x2, we define the followingcomment similarity kernel:K(x1,x2) = KTK(T1,T2) +Kv(v1, v2), (1)where KTKcomputes SHTK (defined next), andKvis a kernel over feature vectors, e.g., linear,polynomial, Gaussian, etc.Shallow syntactic tree kernel.
Following theconvolution kernel framework, we define the new4http://disi.unitn.it/moschitti/Tree-Kernel.htm1255SHTK function from Eq.
1 to compute the similar-ity between tree structures.
It counts the number ofcommon substructures between two trees T1andT2without explicitly considering the whole frag-ment space.
The general equations for Convolu-tion Tree Kernels is:TK(T1, T2) =?n1?NT1?n2?NT2?
(n1, n2), (2)where NT1and NT2are the sets of the T1?s andT2?s nodes, respectively and ?
(n1, n2) is equal tothe number of common fragments rooted in the n1and n2nodes, according to several possible defini-tion of the atomic fragments.To improve the speed computation of TK, weconsider pairs of nodes (n1, n2) belonging to thesame tree level.
Thus, given H , the height of theSTRUCT trees, where each level h contains nodesof the same type, i.e., chunk, POS, and lexicalnodes, we define SHTK as the following5:SHTK(T1, T2) =H?h=1?n1?NhT1?n2?NhT2?
(n1, n2), (3)where NhT1and NhT2are sets of nodes at height h.The above equation can be applied with any ?function.
To have a more general and expressivekernel, we use ?
previously defined for PTK.More formally: if n1and n2are leaves then?
(n1, n2) = ??
(n1, n2); else ?
(n1, n2) =?(?2+?~I1,~I2,|~I1|=|~I2|?d(~I1)+d(~I2)|~I1|?j=1?
(cn1(~I1j), cn2(~I2j))),where ?, ?
?
[0, 1] are decay factors; the largesum is adopted from a definition of the sub-sequence kernel (Shawe-Taylor and Cristianini,2004) to generate children subsets with gaps,which are then used in a recursive call to ?.
Here,cn1(i) is the ithchild of the node n1;~I1and~I2aretwo sequences of indexes that enumerate subsetsof children with gaps, i.e.,~I = (i1, i2, .., |I|), with1 ?
i1< i2< .. < i|I|; and d(~I1) =~I1l(~I1)?~I11+ 1and d(~I2) =~I2l(~I2)?~I21+ 1, which penalizessubsequences with larger gaps.It should be noted that: firstly, the use of asubsequence kernel makes it possible to generatechild subsets of the two nodes, i.e., it allows forgaps, which makes matching of syntactic patterns5To have a similarity score between 0 and 1, a normaliza-tion in the kernel space, i.e.SHTK(T1,T2)?SHTK(T1,T1)?SHTK(T2,T2)isapplied.less rigid.
Secondly, the resulting SHTK is essen-tially a special case of PTK (Moschitti, 2006a),adapted to the shallow structural representationSTRUCT (see Sec.
3.2).
When applied to STRUCTtrees, SHTK exactly computes the same featurespace as PTK, but in faster time (on average).
In-deed, SHTK required to be only applied to nodepairs from the same level (see Eq.
3), where thenode labels can match ?
chunk, POS or lexicals.This reduces the time for selecting the matching-node pairs carried out in PTK (Moschitti, 2006a;Moschitti, 2006b).
The fragment space is obvi-ously the same, as the node labels of differentlevels in STRUCT are different and will not bematched by PTK either.Finally, given its recursive definition in Eq.
3and the use of subsequence (with gaps), SHTK canderive useful dependencies between its elements.For example, it will generate the following subtreefragments: [positive-NP [positive-AN]], [S [negative-VP [negative-V[destroy]] [PRODUCT-NP]]]] and so on.4 YouTube comments corpusTo build a corpus of YouTube comments, we fo-cus on a particular set of videos (technical reviewsand advertisings) featuring commercial products.In particular, we chose two product categories:automobiles (AUTO) and tablets (TABLETS).
Tocollect the videos, we compiled a list of prod-ucts and queried the YouTube gData API6to re-trieve the videos.
We then manually excludedirrelevant videos.
For each video, we extractedall available comments (limited to maximum 1kcomments per video) and manually annotated eachcomment with its type and polarity.
We distin-guish between the following types:product: discuss the topic product in general orsome features of the product;video: discuss the video or some of its details;spam: provide advertising and malicious links; andoff-topic: comments that have almost no content(?lmao?)
or content that is not related to the video(?Thank you!?
).Regarding the polarity, we distinguish between{positive, negative, neutral} sentiments with re-spect to the product and the video.
If the commentcontains several statements of different polarities,it is annotated as both positive and negative: ?Lovethe video but waiting for iPad 4?.
In total we have6https://developers.google.com/youtube/v3/1256annotated 208 videos with around 35k comments(128 videos TABLETS and 80 for AUTO).To evaluate the quality of the produced labels,we asked 5 annotators to label a sample set of onehundred comments and measured the agreement.The resulting annotator agreement ?
value (Krip-pendorf, 2004; Artstein and Poesio, 2008) scoresare 60.6 (AUTO), 72.1 (TABLETS) for the senti-ment task and 64.1 (AUTO), 79.3 (TABLETS) forthe type classification task.
For the rest of thecomments, we assigned the entire annotation taskto a single coder.
Further details on the corpus canbe found in Uryupina et al (2014).5 ExperimentsThis section reports: (i) experiments on individ-ual subtasks of opinion and type classification; (ii)the full task of predicting type and sentiment; (iii)study on the adaptability of our system by learn-ing on one domain and testing on the other; (iv)learning curves that provide an indication on therequired amount and type of data and the scalabil-ity to other domains.5.1 Task descriptionSentiment classification.
We treat each com-ment as expressing positive, negative orneutral sentiment.
Hence, the task is a three-way classification.Type classification.
One of the challenging as-pects of sentiment analysis of YouTube data is thatthe comments may express the sentiment not onlytowards the product shown in the video, butalso the video itself, i.e., users may post posi-tive comments to the video while being generallynegative about the product and vice versa.
Hence,it is of crucial importance to distinguish betweenthese two types of comments.
Additionally, manycomments are irrelevant for both the product andthe video (off-topic) or may even containspam.
Given that the main goal of sentimentanalysis is to select sentiment-bearing commentsand identify their polarity, distinguishing betweenoff-topic and spam categories is not critical.Thus, we merge the spam and off-topic intoa single uninformative category.
Similar tothe opinion classification task, comment type clas-sification is a multi-class classification with threeclasses: video, product and uninform.Full task.
While the previously discussed sen-timent and type identification tasks are useful toTask classAUTO TABLETSTRAIN TEST TRAIN TESTSentimentpositive 2005 (36%) 807 (27%) 2393 (27%) 1872 (27%)neutral 2649 (48%) 1413 (47%) 4683 (53%) 3617 (52%)negative 878 (16%) 760 (26%) 1698 (19%) 1471 (21%)total 5532 2980 8774 6960Typeproduct 2733 (33%) 1761 (34%) 7180 (59%) 5731 (61%)video 3008 (36%) 1369 (26%) 2088 (17%) 1674 (18%)off-topic 2638 (31%) 2045 (39%) 2334 (19%) 1606 (17%)spam 26 (>1%) 17 (>1%) 658 (5%) 361 (4%)total 8405 5192 12260 9372Fullproduct-pos.
1096 (13%) 517 (10%) 1648 (14%) 1278 (14%)product-neu.
908 (11%) 729 (14%) 3681 (31%) 2844 (32%)product-neg.
554 (7%) 370 (7%) 1404 (12%) 1209 (14%)video-pos.
909 (11%) 290 (6%) 745 (6%) 594 (7%)video-neu.
1741 (21%) 683 (14%) 1002 (9%) 773 (9%)video-neg.
324 (4%) 390 (8%) 294 (2%) 262 (3%)off-topic 2638 (32%) 2045 (41%) 2334 (20%) 1606 (18%)spam 26 (>1%) 17 (>1%) 658 (6%) 361 (4%)total 8196 5041 11766 8927Table 1: Summary of YouTube comments dataused in the sentiment, type and full classificationtasks.
The comments come from two product cate-gories: AUTO and TABLETS.
Numbers in paren-thesis show proportion w.r.t.
to the total number ofcomments used in a task.model and study in their own right, our end goal is:given a stream of comments, to jointly predict boththe type and the sentiment of each comment.
Wecast this problem as a single multi-class classifica-tion task with seven classes: the Cartesian productbetween {product, video} type labels and{positive, neutral, negative} senti-ment labels plus the uninformative category(spam and off-topic).
Considering a real-life ap-plication, it is important not only to detect the po-larity of the comment, but to also identify if it isexpressed towards the product or the video.75.2 DataWe split all the videos 50% between trainingset (TRAIN) and test set (TEST), where eachvideo contains all its comments.
This ensuresthat all comments from the same video appeareither in TRAIN or in TEST.
Since the numberof comments per video varies, the resulting sizesof each set are different (we use the larger splitfor TRAIN).
Table 1 shows the data distributionacross the task-specific classes ?
sentiment andtype classification.
For the sentiment task we ex-clude off-topic and spam comments as wellas comments with ambiguous sentiment, i.e., an-7We exclude comments annotated as both video andproduct.
This enables the use of a simple flat multi-classifiers with seven categories for the full task, instead ofa hierarchical multi-label classifiers (i.e., type classificationfirst and then opinion polarity).
The number of comments as-signed to both product and video is relatively small (8%for TABLETS and 4% for AUTO).1257notated as both positive and negative.For the sentiment task about 50% of thecomments have neutral polarity, while thenegative class is much less frequent.
Inter-estingly, the ratios between polarities expressedin comments from AUTO and TABLETS are verysimilar across both TRAIN and TEST.
Conversely,for the type task, we observe that comments fromAUTO are uniformly distributed among the threeclasses, while for the TABLETS the majority ofcomments are product related.
It is likely dueto the nature of the TABLETS videos, that aremore geek-oriented, where users are more proneto share their opinions and enter involved discus-sions about a product.
Additionally, videos fromthe AUTO category (both commercials and userreviews) are more visually captivating and, be-ing generally oriented towards a larger audience,generate more video-related comments.
Regard-ing the full setting, where the goal is to havea joint prediction of the comment sentiment andtype, we observe that video-negative andvideo-positive are the most scarce classes,which makes them the most difficult to predict.5.3 ResultsWe start off by presenting the results for the tradi-tional in-domain setting, where both TRAIN andTEST come from the same domain, e.g., AUTO orTABLETS.
Next, we show the learning curves toanalyze the behavior of FVEC and STRUCT mod-els according to the training size.
Finally, we per-form a set of cross-domain experiments that de-scribe the enhanced adaptability of the patternsgenerated by the STRUCT model.5.3.1 In-domain experimentsWe compare FVEC and STRUCT models on threetasks described in Sec.
5.1: sentiment, type andfull.
Table 2 reports the per-class performanceand the overall accuracy of the multi-class clas-sifier.
Firstly, we note that the performance onTABLETS is much higher than on AUTO acrossall tasks.
This can be explained by the follow-ing: (i) TABLETS contains more training data and(ii) videos from AUTO and TABLETS categoriesdraw different types of audiences ?
well-informedusers and geeks expressing better-motivated opin-ions about a product for the former vs. more gen-eral audience for the latter.
This results in thedifferent quality of comments with the AUTO be-ing more challenging to analyze.
Secondly, weobserve that the STRUCT model provides 1-3%of absolute improvement in accuracy over FVECfor every task.
For individual categories the F1scores are also improved by the STRUCT model(except for the negative classes for AUTO, wherewe see a small drop).
We conjecture that sentimentprediction for AUTO category is largely drivenby one-shot phrases and statements where it ishard to improve upon the bag-of-words and senti-ment lexicon features.
In contrast, comments fromTABLETS category tend to be more elaboratedand well-argumented, thus, benefiting from the ex-pressiveness of the structural representations.Considering per-class performance, correctlypredicting negative sentiment is most difficultfor both AUTO and TABLETS, which is proba-bly caused by the smaller proportion of the neg-ative comments in the training set.
For the typetask, video-related class is substantially more dif-ficult than product-related for both categories.
Forthe full task, the class video-negative ac-counts for the largest error.
This is confirmed bythe results from the previous sentiment and typetasks, where we saw that handling negative sen-timent and detecting video-related comments aremost difficult.5.3.2 Learning curvesThe learning curves depict the behavior of FVECand STRUCT models as we increase the size ofthe training set.
Intuitively, the STRUCT modelrelies on more general syntactic patterns and mayovercome the sparseness problems incurred by theFVEC model when little training data is available.Nevertheless, as we see in Figure 2, the learningcurves for sentiment and type classification tasksacross both product categories do not confirm thisintuition.
The STRUCT model consistently outper-forms the FVEC across all training sizes, but thegap in the performance does not increase when wemove to smaller training sets.
As we will see next,this picture changes when we perform the cross-domain study.5.3.3 Cross-domain experimentsTo understand the performance of our classifierson other YouTube domains, we perform a set ofcross-domain experiments by training on the datafrom one product category and testing on the other.Table 3 reports the accuracy for three taskswhen we use all comments (TRAIN + TEST) fromAUTO to predict on the TEST from TABLETS1258Task classAUTO TABLETSFVEC STRUCT FVEC STRUCTP R F1 P R F1 P R F1 P R F1Sentpositive 49.1 72.1 58.4 50.1 73.9 59.0 67.5 70.3 69.9 71.2 71.3 71.3neutral 68.2 55.0 61.4 70.1 57.6 63.1 81.3 71.4 76.9 81.1 73.1 77.8negative 42.0 36.9 39.6 41.3 35.8 38.8 48.3 60.0 54.8 50.2 62.6 56.5Acc 54.7 55.7 68.6 70.5Typeproduct 66.8 73.3 69.4 68.8 75.5 71.7 78.2 95.3 86.4 80.1 95.5 87.6video 45.0 52.8 48.2 47.8 49.9 48.7 83.6 45.7 58.9 83.5 46.7 59.4uninform 59.3 48.2 53.1 60.6 53.0 56.4 70.2 52.5 60.7 72.9 58.6 65.0Acc 57.4 59.4 77.2 78.6Fullproduct-pos 34.0 49.6 39.2 36.5 51.2 43.0 48.4 56.8 52.0 52.4 59.3 56.4product-neu 43.4 31.1 36.1 41.4 36.1 38.4 68.0 67.5 68.1 59.7 83.4 70.0product-neg 26.3 29.5 28.8 26.3 25.3 25.6 43.0 49.9 45.4 44.7 53.7 48.4video-pos 23.2 47.1 31.9 26.1 54.5 35.5 69.1 60.0 64.7 64.9 68.8 66.4video-neu 26.1 30.0 29.0 26.5 31.6 28.8 56.4 32.1 40.0 55.1 35.7 43.3video-neg 21.9 3.7 6.0 17.7 2.3 4.8 39.0 17.5 23.9 39.5 6.1 11.5uninform 56.5 52.4 54.9 60.0 53.3 56.3 60.0 65.5 62.2 63.3 68.4 66.9Acc 40.0 41.5 57.6 60.3Table 2: In-domain experiments on AUTO and TABLETS using two models: FVEC and STRUCT.
Theresults are reported for sentiment, type and full classification tasks.
The metrics used are precision (P),recall (R) and F1 for each individual class and the general accuracy of the multi-class classifier (Acc).AUTOSTRUCTAUTOFVECTABLETSSTRUCTTABLETSFVECAccuracy55606570training size1k 2k 3k 4k 5k ALL(a) Sentiment classificationAUTOSTRUCTAUTOFVECTABLETSSTRUCTTABLETSFVECAccuracy404550556065707580training size1k 2k 3k 4k 5k ALL(b) Type classificationFigure 2: In-domain learning curves.
ALL refersto the entire TRAIN set for a given product cate-gory, i.e., AUTO and TABLETS (see Table 1)and in the opposite direction (TABLETS?AUTO).When using AUTO as a source domain, STRUCTmodel provides additional 1-3% of absolute im-Source Target Task FVEC STRUCTAUTO TABLETSSent 66.1 66.6Type 59.9 64.1?Full 35.6 38.3?TABLETS AUTOSent 60.4 61.9?Type 54.2 55.6?Full 43.4 44.7?Table 3: Cross-domain experiment.
Ac-curacy using FVEC and STRUCT modelswhen trained/tested in both directions, i.e.AUTO?TABLETS and TABLETS?AUTO.
?de-notes results statistically significant at 95% level(via pairwise t-test).provement, except for the sentiment task.Similar to the in-domain experiments, we stud-ied the effect of the source domain size on the tar-get test performance.
This is useful to assess theadaptability of features exploited by the FVEC andSTRUCT models with the change in the numberof labeled examples available for training.
Addi-tionally, we considered a setting including a smallamount of training data from the target data (i.e.,supervised domain adaptation).For this purpose, we drew the learning curves ofthe FVEC and STRUCT models applied to the sen-timent and type tasks (Figure 3): AUTO is usedas the source domain to train models, which aretested on TABLETS.8The plot shows that when8The results for the other direction (TABLETS?AUTO)show similar behavior.1259STRUCTFVECSource +TargetAccuracy62636465666768training size1k 2k 3k 4k 5k 8.5k(ALL) 100 500 1k(a) Sentiment classificationSTRUCTFVECSource +TargetAccuracy303540455055606570training size1k 2k 3k 4k 5k 8.5k(TRAIN) 13k(ALL) 100 500 1k(b) Type classificationFigure 3: Learning curves for the cross-domainsetting (AUTO?TABLETS).
Shaded area refers toadding a small portion of comments from the samedomain as the target test data to the training.little training data is available, the features gener-ated by the STRUCT model exhibit better adapt-ability (up to 10% of improvement over FVEC).The bag-of-words model seems to be affected bythe data sparsity problem which becomes a crucialissue when only a small training set is available.This difference becomes smaller as we add datafrom the same domain.
This is an important ad-vantage of our structural approach, since we can-not realistically expect to obtain manual annota-tions for 10k+ comments for each (of many thou-sands) product domains present on YouTube.5.4 DiscussionOur STRUCT model is more accurate since it isable to induce structural patterns of sentiment.Consider the following comment: optimus padis better.
this xoom is just to bulky but optimuspad offers better functionality.
The FVEC bag-of-words model misclassifies it to be positive,since it contains two positive expressions (better,better functionality) that outweigh a single nega-tive expression (bulky).
The structural model, incontrast, is able to identify the product of interest(xoom) and associate it with the negative expres-sion through a structural feature and thus correctlyclassify the comment as negative.Some issues remain problematic even for thestructural model.
The largest group of errors areimplicit sentiments.
Thus, some comments do notcontain any explicit positive or negative opinions,but provide detailed and well-argumented criti-cism, for example, this phone is heavy.
Such com-ments might also include irony.
To account forthese cases, a deep understanding of the productdomain is necessary.6 Conclusions and Future WorkWe carried out a systematic study on OM fromYouTube comments by training a set of su-pervised multi-class classifiers distinguishing be-tween video and product related opinions.
Weuse standard feature vectors augmented by shallowsyntactic trees enriched with additional conceptualinformation.This paper makes several contributions: (i) itshows that effective OM can be carried out withsupervised models trained on high quality annota-tions; (ii) it introduces a novel annotated corpusof YouTube comments, which we make availablefor the research community; (iii) it defines novelstructural models and kernels, which can improveon feature vectors, e.g., up to 30% of relative im-provement in type classification, when little datais available, and demonstrates that the structuralmodel scales well to other domains.In the future, we plan to work on a joint modelto classify all the comments of a given video, s.t.
itis possible to exploit latent dependencies betweenentities and the sentiments of the comment thread.Additionally, we plan to experiment with hierar-chical multi-label classifiers for the full task (inplace of a flat multi-class learner).AcknowledgmentsThe authors are supported by a Google Fac-ulty Award 2011, the Google Europe FellowshipAward 2013 and the European Community?s Sev-enth Framework Programme (FP7/2007-2013) un-der the grant #288024: LIMOSINE.1260ReferencesRon Artstein and Massimo Poesio.
2008.
Inter-coderagreement for computational linguistics.
Computa-tional Linguistics, 34(4):555?596, December.Timothy Baldwin, Paul Cook, Marco Lui, AndrewMacKinlay, and Li Wang.
2013.
How noisy socialmedia text, how diffrnt social media sources?
InIJCNLP.John Blitzer, Mark Dredze, and Fernando Pereira.2007.
Biographies, bollywood, boom-boxes andblenders: Domain adaptation for sentiment classi-fication.
In ACL.Hal Daum?e, III.
2007.
Frustratingly easy domainadaptation.
ACL.Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein,Michael Heilman, Dani Yogatama, Jeffrey Flanigan,and Noah A. Smith.
2011.
Part-of-speech taggingfor Twitter: annotation, features, and experiments.In ACL.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In KDD.Thorsten Joachims.
2002.
Optimizing search enginesusing clickthrough data.
In KDD.Jason S. Kessler, Miriam Eckert, Lyndsie Clark, andNicolas Nicolov.
2010.
The 2010 ICWSM JDPAsentiment corpus for the automotive domain.
InICWSM-DWC.Klaus Krippendorf, 2004.
Content Analysis: An In-troduction to Its Methodology, second edition, chap-ter 11.
Sage, Thousand Oaks, CA.Alessandro Moschitti.
2006a.
Efficient convolutionkernels for dependency and constituent syntactictrees.
In ECML.Alessandro Moschitti.
2006b.
Making tree kernelspractical for natural language learning.
In EACL,pages 113?120.Alessandro Moschitti.
2008.
Kernel methods, syntaxand semantics for relational text categorization.
InCIKM.Olutobi Owoputi, Brendan OConnor, Chris Dyer,Kevin Gimpel, Nathan Schneider, and Noah ASmith.
2013.
Improved part-of-speech tagging foronline conversational text with word clusters.
InProceedings of NAACL-HLT.Alexander Pak and Patrick Paroubek.
2010.
Twitter asa corpus for sentiment analysis and opinion mining.In LREC.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and trends in infor-mation retrieval, 2(1-2):1?135.Slav Petrov and Ryan McDonald.
2012.
Overview ofthe 2012 shared task on parsing the web.
In Notesof the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL).Barbara Plank and Alessandro Moschitti.
2013.
Em-bedding semantic similarity in tree kernels for do-main adaptation of relation extraction.
In ACL.Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.2011.
Named entity recognition in tweets: an ex-perimental study.
In ACL.Aliaksei Severyn and Alessandro Moschitti.
2012.Structural relationships for large-scale learning ofanswer re-ranking.
In SIGIR.Aliaksei Severyn and Alessandro Moschitti.
2013.
Au-tomatic feature engineering for answer selection andextraction.
In EMNLP.Aliaksei Severyn, Massimo Nicosia, and AlessandroMoschitti.
2013.
Learning semantic textual simi-larity with structural representations.
In ACL.John Shawe-Taylor and Nello Cristianini.
2004.
Ker-nel Methods for Pattern Analysis.
Cambridge Uni-versity Press.Stefan Siersdorfer, Sergiu Chelaru, Wolfgang Nejdl,and Jose San Pedro.
2010.
How useful are yourcomments?
: Analyzing and predicting YouTubecomments and comment ratings.
In WWW.Richard Socher, Jeffrey Pennington, Eric H Huang,Andrew Y Ng, and Christopher D Manning.
2011.Semi-supervised recursive autoencoders for predict-ing sentiment distributions.
In EMNLP.Anders S?gaard and Anders Johannsen.
2012.
Ro-bust learning in random subspaces: Equipping nlpfor oov effects.
In COLING.Oscar T?ackstr?om and Ryan McDonald.
2011.
Semi-supervised latent variable models for sentence-levelsentiment analysis.
In ACL.Olga Uryupina, Barbara Plank, Aliaksei Severyn,Agata Rotondi, and Alessandro Moschitti.
2014.SenTube: A corpus for sentiment analysis onYouTube social media.
In LREC.Sida Wang and Christopher Manning.
2012.
Baselinesand bigrams: Simple, good sentiment and topic clas-sification.
In ACL.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In EMNLP.1261
