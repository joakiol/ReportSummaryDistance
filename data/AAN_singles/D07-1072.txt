Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
688?697, Prague, June 2007. c?2007 Association for Computational LinguisticsThe Infinite PCFG using Hierarchical Dirichlet ProcessesPercy Liang Slav Petrov Michael I. Jordan Dan KleinComputer Science Division, EECS DepartmentUniversity of California at BerkeleyBerkeley, CA 94720{pliang, petrov, jordan, klein}@cs.berkeley.eduAbstractWe present a nonparametric Bayesian modelof tree structures based on the hierarchicalDirichlet process (HDP).
Our HDP-PCFGmodel allows the complexity of the grammarto grow as more training data is available.In addition to presenting a fully Bayesianmodel for the PCFG, we also develop an ef-ficient variational inference procedure.
Onsynthetic data, we recover the correct gram-mar without having to specify its complex-ity in advance.
We also show that our tech-niques can be applied to full-scale parsingapplications by demonstrating its effective-ness in learning state-split grammars.1 IntroductionProbabilistic context-free grammars (PCFGs) havebeen a core modeling technique for many as-pects of linguistic structure, particularly syntac-tic phrase structure in treebank parsing (Charniak,1996; Collins, 1999).
An important question whenlearning PCFGs is how many grammar symbolsto allocate to the learning algorithm based on theamount of available data.The question of ?how many clusters (symbols)?
?has been tackled in the Bayesian nonparametricsliterature via Dirichlet process (DP) mixture mod-els (Antoniak, 1974).
DP mixture models have sincebeen extended to hierarchical Dirichlet processes(HDPs) and HDP-HMMs (Teh et al, 2006; Beal etal., 2002) and applied to many different types ofclustering/induction problems in NLP (Johnson etal., 2006; Goldwater et al, 2006).In this paper, we present the hierarchical Dirich-let process PCFG (HDP-PCFG).
a nonparametricBayesian model of syntactic tree structures basedon Dirichlet processes.
Specifically, an HDP-PCFGis defined to have an infinite number of symbols;the Dirichlet process (DP) prior penalizes the useof more symbols than are supported by the trainingdata.
Note that ?nonparametric?
does not mean ?noparameters?
; rather, it means that the effective num-ber of parameters can grow adaptively as the amountof data increases, which is a desirable property of alearning algorithm.As models increase in complexity, so does the un-certainty over parameter estimates.
In this regime,point estimates are unreliable since they do not takeinto account the fact that there are different amountsof uncertainty in the various components of the pa-rameters.
The HDP-PCFG is a Bayesian modelwhich naturally handles this uncertainty.
We presentan efficient variational inference algorithm for theHDP-PCFG based on a structured mean-field ap-proximation of the true posterior over parameters.The algorithm is similar in form to EM and thus in-herits its simplicity, modularity, and efficiency.
Un-like EM, however, the algorithm is able to take theuncertainty of parameters into account and thus in-corporate the DP prior.Finally, we develop an extension of the HDP-PCFG for grammar refinement (HDP-PCFG-GR).Since treebanks generally consist of coarsely-labeled context-free tree structures, the maximum-likelihood treebank grammar is typically a poormodel as it makes overly strong independence as-sumptions.
As a result, many generative approachesto parsing construct refinements of the treebankgrammar which are more suitable for the model-ing task.
Lexical methods split each pre-terminalsymbol into many subsymbols, one for each word,and then focus on smoothing sparse lexical statis-688tics (Collins, 1999; Charniak, 2000).
Unlexicalizedmethods refine the grammar in a more conservativefashion, splitting each non-terminal or pre-terminalsymbol into a much smaller number of subsymbols(Klein and Manning, 2003; Matsuzaki et al, 2005;Petrov et al, 2006).
We apply our HDP-PCFG-GRmodel to automatically learn the number of subsym-bols for each symbol.2 Models based on Dirichlet processesAt the heart of the HDP-PCFG is the Dirichlet pro-cess (DP) mixture model (Antoniak, 1974), which isthe nonparametric Bayesian counterpart to the clas-sical finite mixture model.
In order to build up anunderstanding of the HDP-PCFG, we first reviewthe Bayesian treatment of the finite mixture model(Section 2.1).
We then consider the DP mixturemodel (Section 2.2) and use it as a building blockfor developing nonparametric structured versions ofthe HMM (Section 2.3) and PCFG (Section 2.4).Our presentation highlights the similarities betweenthese models so that each step along this progressionreflects only the key differences.2.1 Bayesian finite mixture modelWe begin by describing the Bayesian finite mixturemodel to establish basic notation that will carry overthe more complex models we consider later.Bayesian finite mixture model?
?
Dirichlet(?, .
.
.
, ?)
[draw component probabilities]For each component z ?
{1, .
.
.
,K}:?
?z ?
G0 [draw component parameters]For each data point i ?
{1, .
.
.
, n}:?zi ?
Multinomial(?)
[choose component]?xi ?
F (?
;?zi) [generate data point]The model has K components whose prior dis-tribution is specified by ?
= (?1, .
.
.
, ?K).
TheDirichlet hyperparameter ?
controls how uniformthis distribution is: as ?
increases, it becomes in-creasingly likely that the components have equalprobability.
For each mixture component z ?
{1, .
.
.
,K}, the parameters of the component ?z aredrawn from some prior G0.
Given the model param-eters (?,?
), the data points are generated i.i.d.
byfirst choosing a component and then generating froma data model F parameterized by that component.In document clustering, for example, each datapoint xi is a document represented by its term-frequency vector.
Each component (cluster) zhas multinomial parameters ?z which specifies adistribution F (?
;?z) over words.
It is custom-ary to use a conjugate Dirichlet prior G0 =Dirichlet(?
?, .
.
.
, ??)
over the multinomial parame-ters, which can be interpreted as adding ??
?1 pseu-docounts for each word.2.2 DP mixture modelWe now consider the extension of the Bayesian finitemixture model to a nonparametric Bayesian mixturemodel based on the Dirichlet process.
We focuson the stick-breaking representation (Sethuraman,1994) of the Dirichlet process instead of the stochas-tic process definition (Ferguson, 1973) or the Chi-nese restaurant process (Pitman, 2002).
The stick-breaking representation captures the DP prior mostexplicitly and allows us to extend the finite mixturemodel with minimal changes.
Later, it will enable usto readily define structured models in a form similarto their classical versions.
Furthermore, an efficientvariational inference algorithm can be developed inthis representation (Section 2.6).The key difference between the Bayesian finitemixture model and the DP mixture model is thatthe latter has a countably infinite number of mixturecomponents while the former has a predefined K.Note that if we have an infinite number of mixturecomponents, it no longer makes sense to considera symmetric prior over the component probabilities;the prior over component probabilities must decay insome way.
The stick-breaking distribution achievesthis as follows.
We write ?
?
GEM(?)
to meanthat ?
= (?1, ?2, .
.
. )
is distributed according to thestick-breaking distribution.
Here, the concentrationparameter ?
controls the number of effective com-ponents.
To draw ?
?
GEM(?
), we first generatea countably infinite collection of stick-breaking pro-portions u1, u2, .
.
.
, where each uz ?
Beta(1, ?
).The stick-breaking weights ?
are then defined interms of the stick proportions:?z = uz?z?<z(1 ?
uz?).
(1)The procedure for generating ?
can be viewed asiteratively breaking off remaining portions of a unit-6890 1?1 ?2 ?3 ...Figure 1: A sample ?
?
GEM(1).length stick (Figure 1).
The component probabilities{?z} will decay exponentially in expectation, butthere is always some probability of getting a smallercomponent before a larger one.
The parameter ?
de-termines the decay of these probabilities: a larger ?implies a slower decay and thus more components.Given the component probabilities, the rest of theDP mixture model is identical to the finite mixturemodel:DP mixture model?
?
GEM(?)
[draw component probabilities]For each component z ?
{1, 2, .
.
.
}:?
?z ?
G0 [draw component parameters]For each data point i ?
{1, .
.
.
, n}:?zi ?
Multinomial(?)
[choose component]?xi ?
F (?
;?zi) [generate data point xn]2.3 HDP-HMMThe next stop on the way to the HDP-PCFG is theHDP hidden Markov model (HDP-HMM) (Beal etal., 2002; Teh et al, 2006).
An HMM consists of aset of hidden states, where each state can be thoughtof as a mixture component.
The parameters of themixture component are the emission and transitionparameters.
The main aspect that distinguishes itfrom a flat finite mixture model is that the transi-tion parameters themselves must specify a distribu-tion over next states.
Hence, we have not just onetop-level mixture model over states, but also a col-lection of mixture models, one for each state.In developing a nonparametric version of theHMM in which the number of states is infinite, weneed to ensure that the transition mixture modelsof each state share a common inventory of possiblenext states.
We can achieve this by tying these mix-ture models together using the hierarchical Dirichletprocess (HDP) (Teh et al, 2006).
The stick-breakingrepresentation of an HDP is defined as follows: first,the top-level stick-breaking weights ?
are drawn ac-cording to the stick-breaking prior as before.
Then,a new set of stick-breaking weights ??
are generatedaccording based on ?:??
?
DP(??,?
), (2)where the distribution of DP can be characterizedin terms of the following finite partition property:for all partitions of the positive integers into setsA1, .
.
.
, Am,(??
(A1), .
.
.
,??
(Am)) (3)?
Dirichlet(???
(A1), .
.
.
, ???
(Am)),where ?
(A) =?k?A ?k.1 The resulting ??
is an-other distribution over the positive integers whosesimilarity to ?
is controlled by a concentration pa-rameter ??.HDP-HMM?
?
GEM(?)
[draw top-level state weights]For each state z ?
{1, 2, .
.
.
}:?
?Ez ?
Dirichlet(?)
[draw emission parameters]?
?Tz ?
DP(?
?, ?)
[draw transition parameters]For each time step i ?
{1, .
.
.
, n}:?xi ?
F (?
;?Ezi) [emit current observation]?zi+1 ?
Multinomial(?Tzi) [choose next state]Each state z is associated with emission param-eters ?Ez .
In addition, each z is also associatedwith transition parameters ?Tz , which specify a dis-tribution over next states.
These transition parame-ters are drawn from a DP centered on the top-levelstick-breaking weights ?
according to Equations (2)and (3).
Assume that z1 is always fixed to a specialSTART state, so we do not need to generate it.2.4 HDP-PCFGWe now present the HDP-PCFG, which is the focusof this paper.
For simplicity, we consider Chomskynormal form (CNF) grammars, which has two typesof rules: emissions and binary productions.
We con-sider each grammar symbol as a mixture componentwhose parameters are the rule probabilities for thatsymbol.
In general, we do not know the appropriatenumber of grammar symbols, so our strategy is tolet the number of grammar symbols be infinite andplace a DP prior over grammar symbols.1Note that this property is a specific instance of the generalstochastic process definition of Dirichlet processes.690HDP-PCFG?
?
GEM(?)
[draw top-level symbol weights]For each grammar symbol z ?
{1, 2, .
.
.
}:?
?Tz ?
Dirichlet(?T ) [draw rule type parameters]?
?Ez ?
Dirichlet(?E) [draw emission parameters]?
?Bz ?
DP(?B ,?
?T ) [draw binary production parameters]For each node i in the parse tree:?ti ?
Multinomial(?Tzi) [choose rule type]?If ti = EMISSION:?
?xi ?
Multinomial(?Ezi) [emit terminal symbol]?If ti = BINARY-PRODUCTION:??
(zL(i), zR(i)) ?
Multinomial(?Bzi) [generate children symbols]?
?Bz?Tz?Ezz ?z1z2x2z3x3TParameters TreesFigure 2: The definition and graphical model of the HDP-PCFG.
Since parse trees have unknown structure,there is no convenient way of representing them in the visual language of traditional graphical models.Instead, we show a simple fixed example tree.
Node 1 has two children, 2 and 3, each of which has oneobserved terminal child.
We use L(i) and R(i) to denote the left and right children of node i.In the HMM, the transition parameters of a statespecify a distribution over single next states; simi-larly, the binary production parameters of a gram-mar symbol must specify a distribution over pairsof grammar symbols for its children.
We adapt theHDP machinery to tie these binary production distri-butions together.
The key difference is that now wemust tie distributions over pairs of grammar sym-bols together via distributions over single grammarsymbols.Another difference is that in the HMM, at eachtime step, both a transition and a emission are made,whereas in the PCFG either a binary production oran emission is chosen.
Therefore, each grammarsymbol must also have a distribution over the typeof rule to apply.
In a CNF PCFG, there are onlytwo types of rules, but this can be easily generalizedto include unary productions, which we use for ourparsing experiments.To summarize, the parameters of each grammarsymbol z consists of (1) a distribution over a finitenumber of rule types ?Tz , (2) an emission distribu-tion ?Ez over terminal symbols, and (3) a binary pro-duction distribution ?Bz over pairs of children gram-mar symbols.
Figure 2 describes the model in detail.Figure 3 shows the generation of the binary pro-duction distributions ?Bz .
We draw ?Bz from a DPcentered on ?
?T , which is the product distributionover pairs of symbols.
The result is a doubly-infinitematrix where most of the probability mass is con-stateright child stateleft child stateright child stateleft child state?
?
GEM(?)?
?T?Bz ?
DP(?
?T )Figure 3: The generation of binary production prob-abilities given the top-level symbol probabilities ?.First, ?
is drawn from the stick-breaking prior, asin any DP-based model (a).
Next, the outer-product?
?T is formed, resulting in a doubly-infinite matrixmatrix (b).
We use this as the base distribution forgenerating the binary production distribution from aDP centered on ?
?T (c).centrated in the upper left, just like the top-level dis-tribution ?
?T .Note that we have replaced the general691G0 and F (?Ezi) pair with Dirichlet(?E) andMultinomial(?Ezi) to specialize to natural language,but there is no difficulty in working with parsetrees with arbitrary non-multinomial observationsor more sophisticated word models.In many natural language applications, there isa hard distinction between pre-terminal symbols(those that only emit a word) and non-terminal sym-bols (those that only rewrite as two non-terminal orpre-terminal symbols).
This can be accomplishedby letting ?T = (0, 0), which forces a draw ?Tz toassign probability 1 to one rule type.An alternative definition of an HDP-PCFG wouldbe as follows: for each symbol z, draw a distributionover left child symbols lz ?
DP(?)
and an inde-pendent distribution over right child symbols rz ?DP(?).
Then define the binary production distribu-tion as their cross-product ?Bz = lzrTz .
This alsoyields a distribution over symbol pairs and hence de-fines a different type of nonparametric PCFG.
Thismodel is simpler and does not require any additionalmachinery beyond the HDP-HMM.
However, themodeling assumptions imposed by this alternativeare unappealing as they assume the left child andright child are independent given the parent, whichis certainly not the case in natural language.2.5 HDP-PCFG for grammar refinementAn important motivation for the HDP-PCFG is thatof refining an existing treebank grammar to alle-viate unrealistic independence assumptions and toimprove parsing accuracy.
In this scenario, the setof symbols is known, but we do not know howmany subsymbols to allocate per symbol.
We in-troduce the HDP-PCFG for grammar refinement(HDP-PCFG-GR), an extension of the HDP-PCFG,for this task.The essential difference is that now we have acollection of HDP-PCFG models for each symbols ?
S, each one operating at the subsymbol level.While these HDP-PCFGs are independent in theprior, they are coupled through their interactions inthe parse trees.
For completeness, we have also in-cluded unary productions, which are essentially thePCFG counterpart of transitions in HMMs.
Finally,since each node i in the parse tree involves a symbol-subsymbol pair (si, zi), each subsymbol needs tospecify a distribution over both child symbols andsubsymbols.
The former can be handled througha finite Dirichlet distribution since all symbols areknown and observed, but the latter must be handledwith the Dirichlet process machinery, since the num-ber of subsymbols is unknown.HDP-PCFG for grammar refinement (HDP-PCFG-GR)For each symbol s ?
S:?
?s ?
GEM(?)
[draw subsymbol weights]?For each subsymbol z ?
{1, 2, .
.
.
}:??
?Tsz ?
Dirichlet(?T ) [draw rule type parameters]??
?Esz ?
Dirichlet(?E(s)) [draw emission parameters]??
?usz ?
Dirichlet(?u) [unary symbol productions]??
?bsz ?
Dirichlet(?b) [binary symbol productions]?
?For each child symbol s?
?
S:????Uszs?
?
DP(?U ,?s?)
[unary subsymbol prod.]?
?For each pair of children symbols (s?, s??)
?
S ?
S:????Bszs?s??
?
DP(?B ,?s??Ts??)
[binary subsymbol]For each node i in the parse tree:?ti ?
Multinomial(?Tsizi) [choose rule type]?If ti = EMISSION:?
?xi ?
Multinomial(?Esizi) [emit terminal symbol]?If ti = UNARY-PRODUCTION:?
?sL(i) ?
Multinomial(?usizi) [generate child symbol]?
?zL(i) ?
Multinomial(?UsizisL(i)) [child subsymbol]?If ti = BINARY-PRODUCTION:??
(sL(i), sR(i)) ?
Mult(?sizi) [children symbols]??
(zL(i), zR(i)) ?
Mult(?BsizisL(i)sR(i)) [subsymbols]2.6 Variational inferenceWe present an inference algorithm for the HDP-PCFG model described in Section 2.4, which canalso be adapted to the HDP-PCFG-GR model witha bit more bookkeeping.
Most previous inferencealgorithms for DP-based models involve sampling(Escobar and West, 1995; Teh et al, 2006).
How-ever, we chose to use variational inference (Bleiand Jordan, 2005), which provides a fast determin-istic alternative to sampling, hence avoiding issuesof diagnosing convergence and aggregating samples.Furthermore, our variational inference algorithm es-tablishes a strong link with past work on PCFG re-finement and induction, which has traditionally em-ployed the EM algorithm.In EM, the E-step involves a dynamic programthat exploits the Markov structure of the parse tree,and the M-step involves computing ratios based onexpected counts extracted from the E-step.
Our vari-ational algorithm resembles the EM algorithm inform, but the ratios in the M-step are replaced withweights that reflect the uncertainty in parameter es-692?
?Bz?Tz?Ezz ?z1z2 z3TParameters TreesFigure 4: We approximate the true posterior p overparameters ?
and latent parse trees z using a struc-tured mean-field distribution q, in which the distri-bution over parameters are completely factorized butthe distribution over parse trees is unconstrained.timates.
Because of this procedural similarity, ourmethod is able to exploit the desirable properties ofEM such as simplicity, modularity, and efficiency.2.7 Structured mean-field approximationWe denote parameters of the HDP-PCFG as ?
=(?,?
), where ?
denotes the top-level symbol prob-abilities and ?
denotes the rule probabilities.
Thehidden variables of the model are the training parsetrees z.
We denote the observed sentences as x.The goal of Bayesian inference is to compute theposterior distribution p(?, z | x).
The central ideabehind variational inference is to approximate thisintractable posterior with a tractable approximation.In particular, we want to find the best distribution q?as defined byq?def= argminq?QKL(q(?, z)||p(?, z | x)), (4)where Q is a tractable subset of distributions.
Weuse a structured mean-field approximation, meaningthat we only consider distributions that factorize asfollows (Figure 4):Qdef={q(z)q(?
)K?z=1q(?Tz )q(?Ez )q(?Bz )}.
(5)We further restrict q(?Tz ), q(?Ez ), q(?Bz ) to beDirichlet distributions, but allow q(z) to be anymultinomial distribution.
We constrain q(?)
to be adegenerate distribution truncated at K; i.e., ?z = 0for z > K. While the posterior grammar does havean infinite number of symbols, the exponential de-cay of the DP prior ensures that most of the proba-bility mass is contained in the first few symbols (Ish-waran and James, 2001).2 While our variational ap-proximation q is truncated, the actual PCFG modelis not.
AsK increases, our approximation improves.2.8 Coordinate-wise ascentThe optimization problem defined by Equation (4)is intractable and nonconvex, but we can use a sim-ple coordinate-ascent algorithm that iteratively op-timizes each factor of q in turn while holding theothers fixed.
The algorithm turns out to be similar inform to EM for an ordinary PCFG: optimizing q(z)is the analogue of the E-step, and optimizing q(?
)is the analogue of the M-step; however, optimizingq(?)
has no analogue in EM.
We summarize eachof these updates below (see (Liang et al, 2007) forcomplete derivations).Parse trees q(z): The distribution over parse treesq(z) can be summarized by the expected suffi-cient statistics (rule counts), which we denote asC(z ?
zl zr) for binary productions and C(z ?x) for emissions.
We can compute these expectedcounts using dynamic programming as in the E-stepof EM.While the classical E-step uses the current ruleprobabilities ?, our mean-field approximation in-volves an entire distribution q(?).
Fortunately, wecan still handle this case by replacing each rule prob-ability with a weight that summarizes the uncer-tainty over the rule probability as represented by q.We define this weight in the sequel.It is a common perception that Bayesian inferenceis slow because one needs to compute integrals.
Ourmean-field inference algorithm is a counterexample:because we can represent uncertainty over rule prob-abilities with single numbers, much of the existingPCFG machinery based on EM can be modularlyimported into the Bayesian framework.Rule probabilities q(?
): For an ordinary PCFG,the M-step simply involves taking ratios of expected2In particular, the variational distance between the stick-breaking distribution and the truncated version decreases expo-nentially as the truncation level K increases.693counts:?Bz (zl, zr) =C(z ?
zl zr)C(z ?
??).
(6)For the variational HDP-PCFG, the optimal q(?)
isgiven by the standard posterior update for Dirichletdistributions:3q(?Bz ) = Dirichlet(?Bz ;?B?
?T + ~C(z)), (7)where ~C(z) is the matrix of counts of rules with left-hand side z.
These distributions can then be summa-rized with multinomial weights which are the onlynecessary quantities for updating q(z) in the next it-eration:WBz (zl, zr)def= expEq[log?Bz (zl, zr)] (8)=e?
(C(z?zl zr)+?B?zl?zr )e?(C(z???
)+?B), (9)where ?(?)
is the digamma function.
The emissionparameters can be defined similarly.
Inspection ofEquations (6) and (9) reveals that the only differencebetween the maximum likelihood and the mean-fieldupdate is that the latter applies the exp(?(?))
func-tion to the counts (Figure 5).When the truncation K is large, ?B?zl?zr is near0 for most right-hand sides (zl, zr), so exp(?(?))
hasthe effect of downweighting counts.
Since this sub-traction affects large counts more than small counts,there is a rich-get-richer effect: rules that have al-ready have large counts will be preferred.Specifically, consider a set of rules with the sameleft-hand side.
The weights for all these rules onlydiffer in the numerator (Equation (9)), so applyingexp(?(?))
creates a local preference for right-handsides with larger counts.
Also note that the ruleweights are not normalized; they always sum to atmost one and are equal to one exactly when q(?)
isdegenerate.
This lack of normalization gives an ex-tra degree of freedom not present in maximum like-lihood estimation: it creates a global preference forleft-hand sides that have larger total counts.Top-level symbol probabilities q(?
): Recall thatwe restrict q(?)
= ???(?
), so optimizing ?
isequivalent to finding a single best ??.
Unlike q(?
)3Because we have truncated the top-level symbol weights,the DP prior on ?Bz reduces to a finite Dirichlet distribution.00.5 11.5 2  00.511.52xexp(?
(x)) xFigure 5: The exp(?(?))
function, which is used incomputing the multinomial weights for mean-fieldinference.
It has the effect of reducing a larger frac-tion of small counts than large counts.and q(z), there is no closed form expression forthe optimal ?
?, and the objective function (Equa-tion (4)) is not convex in ??.
Nonetheless, we canapply a standard gradient projection method (Bert-sekas, 1999) to improve ??
to a local maxima.The part of the objective function in Equation (4)that depends on ??
is as follows:L(??)
= logGEM(??;?
)+ (10)K?z=1Eq[logDirichlet(?Bz ;?B???
?T )]See Liang et al (2007) for the derivation of the gra-dient.
In practice, this optimization has very little ef-fect on performance.
We suspect that this is becausethe objective function is dominated by p(x | z) andp(z | ?
), while the contribution of p(?
| ?)
is mi-nor.3 ExperimentsWe now present an empirical evaluation of the HDP-PCFG(-GR) model and variational inference tech-niques.
We first give an illustrative example of theability of the HDP-PCFG to recover a known gram-mar and then present the results of experiments onlarge-scale treebank parsing.3.1 Recovering a synthetic grammarIn this section, we show that the HDP-PCFG-GRcan recover a simple grammar while a standard694S ?
X1X1 | X2X2 | X3X3 | X4X4X1 ?
a1 | b1 | c1 | d1X2 ?
a2 | b2 | c2 | d2X3 ?
a3 | b3 | c3 | d3X4 ?
a4 | b4 | c4 | d4SXi Xi{ai, bi, ci, di} {ai, bi, ci, di}(a) (b)Figure 6: (a) A synthetic grammar with a uniformdistribution over rules.
(b) The grammar generatestrees of the form shown on the right.PCFG fails to do so because it has no built-in con-trol over grammar complexity.
From the grammar inFigure 6, we generated 2000 trees.
The two terminalsymbols always have the same subscript, but we col-lapsed Xi to X in the training data.
We trained theHDP-PCFG-GR, with truncation K = 20, for bothS and X for 100 iterations.
We set al hyperparame-ters to 1.Figure 7 shows that the HDP-PCFG-GR recoversthe original grammar, which contains only 4 sub-symbols, leaving the other 16 subsymbols unused.The standard PCFG allocates all the subsymbols tofit the exact co-occurrence statistics of left and rightterminals.Recall that a rule weight, as defined in Equa-tion (9), is analogous to a rule probability for stan-dard PCFGs.
We say a rule is effective if its weightis at least 10?6 and its left hand-side has posterioris also at least 10?6.
In general, rules with weightsmaller than 10?6 can be safely pruned without af-fect parsing accuracy.
The standard PCFG uses all20 subsymbols of both S and X to explain the data,resulting in 8320 effective rules; in contrast, theHDP-PCFG uses only 4 subsymbols for X and 1 forS, resulting in only 68 effective rules.
If the thresh-old is relaxed from 10?6 to 10?3, then only 20 rulesare effective, which corresponds exactly to the truegrammar.3.2 Parsing the Penn TreebankIn this section, we show that our variational HDP-PCFG can scale up to real-world data sets.
We ranexperiments on the Wall Street Journal (WSJ) por-tion of the Penn Treebank.
We trained on sections2?21, used section 24 for tuning hyperparameters,and tested on section 22.We binarize the trees in the treebank as follows:for each non-terminal node with symbol X , we in-1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 200.25subsymbolposterior1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 200.25subsymbolposteriorstandard PCFG HDP-PCFGFigure 7: The posteriors over the subsymbols of thestandard PCFG is roughly uniform, whereas the pos-teriors of the HDP-PCFG is concentrated on foursubsymbols, which is the true number of symbolsin the grammar.troduce a right-branching cascade of new nodes withsymbol X .
The end result is that each node has atmost two children.
To cope with unknown words,we replace any word appearing fewer than 5 timesin the training set with one of 50 unknown word to-kens derived from 10 word-form features.Our goal is to learn a refined grammar, where eachsymbol in the training set is split into K subsym-bols.
We compare an ordinary PCFG estimated withmaximum likelihood (Matsuzaki et al, 2005) andthe HDP-PCFG estimated using the variational in-ference algorithm described in Section 2.6.To parse new sentences with a grammar, we com-pute the posterior distribution over rules at each spanand extract the tree with the maximum expected cor-rect number of rules (Petrov and Klein, 2007).3.2.1 HyperparametersThere are six hyperparameters in the HDP-PCFG-GR model, which we set in the following manner:?
= 1, ?T = 1 (uniform distribution over unar-ies versus binaries), ?E = 1 (uniform distributionover terminal words), ?u(s) = ?b(s) = 1N(s) , whereN(s) is the number of different unary (binary) right-hand sides of rules with left-hand side s in the tree-bank grammar.
The two most important hyperpa-rameters are ?U and ?B , which govern the sparsityof the right-hand side for unary and binary rules.We set ?U = ?B although more performance couldprobably be gained by tuning these individually.
Itturns out that there is not a single ?B that works forall truncation levels, as shown in Table 1.If the top-level distribution ?
is uniform, the valueof ?B corresponding to a uniform prior over pairs ofchildren subsymbols is K2.
Interestingly, the opti-mal ?B appears to be superlinear but subquadratic695truncation K 2 4 8 12 16 20best ?B 16 12 20 28 48 80uniform ?B 4 16 64 144 256 400Table 1: For each truncation level, we report the ?Bthat yielded the highest F1 score on the developmentset.K PCFG PCFG (smoothed) HDP-PCFGF1 Size F1 Size F1 Size1 60.47 2558 60.36 2597 60.5 25572 69.53 3788 69.38 4614 71.08 42644 75.98 3141 77.11 12436 77.17 97108 74.32 4262 79.26 120598 79.15 5062912 70.99 7297 78.8 160403 78.94 8638616 66.99 19616 79.2 261444 78.24 13137720 64.44 27593 79.27 369699 77.81 202767Table 2: Shows development F1 and grammar sizes(the number of effective rules) as we increase thetruncation K.in K. We used these values of ?B in the followingexperiments.3.2.2 ResultsThe regime in which Bayesian inference is mostimportant is when training data is scarce relative tothe complexity of the model.
We train on just sec-tion 2 of the Penn Treebank.
Table 2 shows howthe HDP-PCFG-GR can produce compact grammarsthat guard against overfitting.
Without smoothing,ordinary PCFGs trained using EM improve as K in-creases but start to overfit around K = 4.
Simpleadd-1.01 smoothing prevents overfitting but at thecost of a sharp increase in grammar sizes.
The HDP-PCFG obtains comparable performance with a muchsmaller number of rules.We also trained on sections 2?21 to demon-strate that our methods can scale up and achievebroadly comparable results to existing state-of-the-art parsers.
When using a truncation level of K =16, the standard PCFG with smoothing obtains anF1 score of 88.36 using 706157 effective rules whilethe HDP-PCFG-GR obtains an F1 score of 87.08 us-ing 428375 effective rules.
We expect to see greaterbenefits from the HDP-PCFG with a larger trunca-tion level.4 Related workThe question of how to select the appropriate gram-mar complexity has been studied in earlier work.It is well known that more complex models nec-essarily have higher likelihood and thus a penaltymust be imposed for more complex grammars.
Ex-amples of such penalized likelihood procedures in-clude Stolcke and Omohundro (1994), which usedan asymptotic Bayesian model selection criterionand Petrov et al (2006), which used a split-mergealgorithm which procedurally determines when toswitch between grammars of various complexities.These techniques are model selection techniquesthat use heuristics to choose among competing sta-tistical models; in contrast, the HDP-PCFG relies onthe Bayesian formalism to provide implicit controlover model complexity within the framework of asingle probabilistic model.Johnson et al (2006) also explored nonparamet-ric grammars, but they do not give an inference al-gorithm for recursive grammars, e.g., grammars in-cluding rules of the form A ?
BC and B ?
DA.Recursion is a crucial aspect of PCFGs and ourinference algorithm does handle it.
Finkel et al(2007) independently developed another nonpara-metric model of grammars.
Though their model isalso based on hierarchical Dirichlet processes and issimilar to ours, they present a different inference al-gorithm which is based on sampling.
Kurihara andSato (2004) and Kurihara and Sato (2006) appliedvariational inference to PCFGs.
Their algorithm issimilar to ours, but they did not consider nonpara-metric models.5 ConclusionWe have presented the HDP-PCFG, a nonparametricBayesian model for PCFGs, along with an efficientvariational inference algorithm.
While our primarycontribution is the elucidation of the model and algo-rithm, we have also explored some important empir-ical properties of the HDP-PCFG and also demon-strated the potential of variational HDP-PCFGs on afull-scale parsing task.696ReferencesC.
E. Antoniak.
1974.
Mixtures of Dirichlet processeswith applications to Bayesian nonparametric prob-lems.
Annals of Statistics, 2:1152?1174.M.
Beal, Z. Ghahramani, and C. Rasmussen.
2002.
Theinfinite hidden Markov model.
In Advances in NeuralInformation Processing Systems (NIPS), pages 577?584.D.
Bertsekas.
1999.
Nonlinear programming.D.
Blei and M. I. Jordan.
2005.
Variational inference forDirichlet process mixtures.
Bayesian Analysis, 1:121?144.E.
Charniak.
1996.
Tree-bank grammars.
In Associationfor the Advancement of Artificial Intelligence (AAAI).E.
Charniak.
2000.
A maximum-entropy-inspired parser.In North American Association for ComputationalLinguistics (NAACL), pages 132?139.M.
Collins.
1999.
Head-Driven Statistical Models forNatural Language Parsing.
Ph.D. thesis, Universityof Pennsylvania.M.
D. Escobar and M. West.
1995.
Bayesian densityestimation and inference using mixtures.
Journal ofthe American Statistical Association, 90:577?588.T.
S. Ferguson.
1973.
A Bayesian analysis of some non-parametric problems.
Annals of Statistics, 1:209?230.J.
R. Finkel, T. Grenager, and C. Manning.
2007.
Theinfinite tree.
In Association for Computational Lin-guistics (ACL).S.
Goldwater, T. Griffiths, and M. Johnson.
2006.
Con-textual dependencies in unsupervised word segmenta-tion.
In International Conference on ComputationalLinguistics and Association for Computational Lin-guistics (COLING/ACL).H.
Ishwaran and L. F. James.
2001.
Gibbs samplingmethods for stick-breaking priors.
Journal of theAmerican Statistical Association, 96:161?173.M.
Johnson, T. Griffiths, and S. Goldwater.
2006.
Adap-tor grammars: A framework for specifying composi-tional nonparametric Bayesian models.
In Advancesin Neural Information Processing Systems (NIPS).D.
Klein and C. Manning.
2003.
Accurate unlexicalizedparsing.
In Association for Computational Linguistics(ACL), pages 423?430.K.
Kurihara and T. Sato.
2004.
An application of thevariational Bayesian approach to probabilistic context-free grammars.
In International Joint Conference onNatural Language Processing Workshop Beyond Shal-low Analyses.K.
Kurihara and T. Sato.
2006.
Variational Bayesiangrammar induction for natural language.
In Interna-tional Colloquium on Grammatical Inference.P.
Liang, S. Petrov, M. I. Jordan, and D. Klein.2007.
Nonparametric PCFGs using Dirichlet pro-cesses.
Technical report, Department of Statistics,University of California at Berkeley.T.
Matsuzaki, Y. Miyao, and J. Tsujii.
2005.
Probabilis-tic CFG with latent annotations.
In Association forComputational Linguistics (ACL).S.
Petrov and D. Klein.
2007.
Learning and inferencefor hierarchically split PCFGs.
In Human LanguageTechnology and North American Association for Com-putational Linguistics (HLT/NAACL).S.
Petrov, L. Barrett, R. Thibaux, and D. Klein.
2006.Learning accurate, compact, and interpretable tree an-notation.
In International Conference on Computa-tional Linguistics and Association for ComputationalLinguistics (COLING/ACL).J.
Pitman.
2002.
Combinatorial stochastic processes.Technical Report 621, Department of Statistics, Uni-versity of California at Berkeley.J.
Sethuraman.
1994.
A constructive definition of Dirich-let priors.
Statistica Sinica, 4:639?650.A.
Stolcke and S. Omohundro.
1994.
Inducing prob-abilistic grammars by Bayesian model merging.
InGrammatical Inference and Applications.Y.
W. Teh, M. I. Jordan, M. Beal, and D. Blei.
2006.Hierarchical Dirichlet processes.
Journal of the Amer-ican Statistical Association, 101:1566?1581.697
