Structuring Knowledge for Reference Generation:A Clustering AlgorithmAlbert GattDepartment of Computing ScienceUniversity of AberdeenScotland, United Kingdomagatt@csd.abdn.ac.ukAbstractThis paper discusses two problems that arisein the Generation of Referring Expressions:(a) numeric-valued attributes, such as size orlocation; (b) perspective-taking in reference.Both problems, it is argued, can be resolvedif some structure is imposed on the availableknowledge prior to content determination.
Wedescribe a clustering algorithm which is suffi-ciently general to be applied to these diverseproblems, discuss its application, and evaluateits performance.1 IntroductionThe problem of Generating Referring Expressions(GRE) can be summed up as a search for the prop-erties in a knowledge base (KB) whose combinationuniquely distinguishes a set of referents from their dis-tractors.
The content determination strategy adoptedin such algorithms is usually based on the assump-tion (made explicit in Reiter (1990)) that the space ofpossible descriptions is partially ordered with respectto some principle(s) which determine their adequacy.Traditionally, these principles have been defined viaan interpretation of the Gricean maxims (Dale, 1989;Reiter, 1990; Dale and Reiter, 1995; van Deemter,2002)1.
However, little attention has been paid to con-textual or intentional influences on attribute selection(but cf.
Jordan and Walker (2000); Krahmer and The-une (2002)).
Furthermore, it is often assumed thatall relevant knowledge about domain objects is repre-sented in the database in a format (e.g.
attribute-valuepairs) that requires no further processing.This paper is concerned with two scenarios whichraise problems for such an approach to GRE:1.
Real-valued attributes, e.g.
size or spatial coor-dinates, which represent continuous dimensions.The utility of such attributes depends on whethera set of referents have values that are ?sufficiently1For example, the Gricean Brevity maxim (Grice, 1975)has been interpreted as a directive to find the shortest possibledescription for a given referentclose?
on the given dimension, and ?sufficientlydistant?
from those of their distractors.
We dis-cuss this problem in ?2.2.
Perspective-taking The contextual appropriate-ness of a description depends on the perspectivebeing taken in context.
For instance, if it is knownof a referent that it is a teacher, and a sportsman, itis better to talk of the teacher in a context whereanother referent has been introduced as the stu-dent.
This is discussed further in ?3.Our aim is to motivate an approach to GRE wherethese problems are solved by pre-processing the infor-mation in the knowledge base, prior to content deter-mination.
To this end, ?4 describes a clustering algo-rithm and shows how it can be applied to these differentproblems to structure the KB prior to GRE.2 Numeric values: The case of locationSeveral types of information about domain entities,such as gradable properties (van Deemter, 2000) andphysical location, are best captured by real-valued at-tributes.
Here, we focus on the example of location asan attribute taking a tuple of values which jointly de-termine the position of an entity.The ability to distinguish groups is a well-established feature of the human perceptual appara-tus (Wertheimer, 1938; Treisman, 1982).
Representingsalient groups can facilitate the task of excluding dis-tractors in the search for a referent.
For instance, theset of referents marked as the intended referential tar-get in Figure 1 is easily distinguishable as a group andwarrants the use of a spatial description such as the ob-jects in the top left corner, possibly with a collectivepredicate, such as clustered or gathered.
In case ofreference to a subset of the marked set, although loca-tion would be insufficient to distinguish the targets, itwould reduce the distractor set and facilitate referenceresolution2.In GRE, an approach to spatial reference basedon grouping has been proposed by Funakoshi et al2Location has been found to significantly facilitate reso-lution, even when it is logically redundant (Arts, 2004)321e1e4 e3e2e8e9e13e12e10e11e6 e7e5Figure 1: Spatial Example(2004).
Given a domain and a target referent, a se-quence of groups is constructed, starting from thelargest group containing the referent, and recursivelynarrowing down the group until only the referent isidentified.
The entire sequence is then rendered lin-guistically.
The algorithm used for identifying percep-tual groups is the one proposed by Thorisson (1994),the core of which is a procedure which takes as inputa list of pairs of objects, ordered by the distance be-tween the entities in the pairs.
The procedure loopsthrough the list, finding the greatest difference in dis-tance between two adjacent pairs.
This is determinedas a cutoff point for group formation.
Two problemsare raised by this approach:P1 Ambiguous clusters A domain entity can beplaced in more than one group.
If, say, the in-put list is?
{a, b}, {c, e}, {a, f}?and the great-est difference after the first iteration is between{c, e} and {a, f}, then the first group to be formedwill be {a, b, c, e} with {a, f} likely to be placedin a different group after further iterations.
Thismay be confusing from a referential point of view.The problem arises because grouping or cluster-ing takes place on the basis of pairwise proxim-ity or distance.
This problem can be partially cir-cumvented by identifying groups on several per-ceptual dimensions (e.g.
spatial distance, colour,and shape) and then seeking to merge identicalgroups determined on the basis of these differ-ent qualities (see Thorisson (1994)).
However, thegrouping strategy can still return groups which donot conform to human perceptual principles.
Abetter strategy is to base clustering on the Near-est Neighbour Principle, familiar from computa-tional geometry (Prepaarata and Shamos, 1985),whereby elements are clustered with their nearestneighbours, given a distance function.
The solu-tion offered below is based on this principle.P2 Perceptual proximity Absolute distance is notsufficient for cluster identification.
In Figure 1,for example, the pairs {e1, e2} and {e5, e6} couldeasily be consecutively ranked, since the distancebetween e1 and e2 is roughly equal to that be-tween e5 and e6.
However, they would not natu-rally be clustered together by a human observer,because grouping of objects also needs to takeinto account the position of the surrounding ele-ments.
Thus, while e1 is as far away from e2 ase5 is from e6, there are elements which are closerto {e1, e2} than to {e5, e6}.The proposal in ?4 represents a way of gettingaround these problems, which are expected to arise inany kind of domain where the information given is thepairwise distance between elements.
Before turning tothe framework, we consider another situation in GREwhere the need for clustering could arise.3 Perspectives and semantic similarityIn real-world discourse, entities can often be talkedabout from different points of view, with speakersbringing to bear world and domain-specific knowledgeto select information that is relevant to the currenttopic.
In order to generate coherent discourse, a gener-ator should ideally keep track of how entities have beenreferred to, and maintain consistency as far as possible.type profession nationalitye1 man student englishmane2 woman teacher italiane3 man chef greekTable 1: Semantic ExampleSuppose e1 in Table 1 has been introduced into thediscourse via the description the student and the nextutterance requires a reference to e2.
Any one of thethree available attributes would suffice to distinguishthe latter.
However, a description such as the womanor the italianwould describe this entity from a differentpoint of view relative to e1.
By hypothesis, the teacheris more appropriate, because the property ascribed toe2 is more similar to that ascribed to e1.A similar case arises with plural disjunctive descrip-tions of the form ?x[p(x)?q(x)], which are usually re-alised as coordinate constructions of the form the N?1and the N?2.
For instance a reference to {e1, e2} suchas the woman and the student, or the englishman andthe teacher, would be odd, compared to the alterna-tive the student and the teacher.
The latter describesthese entities under the same perspective.
Note that?consistency?
or ?similarity?
is not guaranteed simplyby attempting to use values of the same attribute(s) fora given set of referents.
The description the student322and the chef for {e1, e3} is relatively odd compared tothe alternative the englishman and the greek.
In bothkinds of scenarios, a GRE algorithm that relied on arigid preference order could not guarantee that a coher-ent description would be generated every time it wasavailable.The issues raised here have never been systemati-cally addressed in the GRE literature, although supportfor the underlying intuitions can be found in variousquarters.
Kronfeld (1989) distinguishes between func-tionally and conversationally relevant descriptions.
Adescription is functionally relevant if it succeeds in dis-tinguishing the intended referent(s), but conversationalrelevance arises in part from implicatures carried bythe use of attributes in context.
For example, describ-ing e1 as the student carries the (Gricean) implicaturethat the entity?s academic role or profession is some-how relevant to the current discourse.
When two enti-ties are described using contrasting properties, say thestudent and the italian, the listener may find it harderto work out the relevance of the contrast.
In a relatedvein, Aloni (2002) formalises the appropriateness of ananswer to a question of the form Wh x?
with referenceto the ?conceptual covers?
or perspectives under whichx can be conceptualised, not all of which are equallyrelevant given the hearer?s information state and thediscourse context.With respect to plurals, Eschenbach et al (1989) ar-gue that the generation of a plural anaphor with a splitantecedent is more felicitous when the antecedentshave something in common, such as their ontologicalcategory.
This constraint has been shown to hold psy-cholinguistically (Kaup et al, 2002; Koh and Clifton,2002; Moxey et al, 2004).
Gatt and van Deemter(2005a) have shown that people?s perception of the ad-equacy of plural descriptions of the form, the N1 and(the) N2 is significantly correlated with the seman-tic similarity of N1 and N2, while singular descrip-tions are more likely to be aggregated into a plural ifsemantically similar attributes are available (Gatt andVan Deemter, 2005b).The two kinds of problems discussed here could beresolved by pre-processing the KB in order to iden-tify available perspectives.
One way of doing this isto group available properties into clusters of seman-tically similar ones.
This requires a well-defined no-tion of ?similarity?
which determines the ?distance?
be-tween properties in semantic space.
As with spatialclustering, the problem is then of how to get frompairwise distance to well-formed clusters or groups,while respecting the principles underlying human per-ceptual/conceptual organisation.
The next section de-scribes an algorithm that aims to achieve this.4 A framework for clusteringIn what follows, we assume the existence of a set ofclusters C in a domain S of objects (entities or proper-ties), to be ?discovered?
by the algorithm.
We furtherassume the existence of a dimension, which is char-acterised by a function ?
that returns the pairwise dis-tance ?
(a, b), where ?a, b?
?
S?S.
In case an attributeis characterised bymore than one dimension, say ?x, y?coordinates in a 2D plane, as in Figure 1, then ?
is de-fined as the Euclidean distance between pairs:?
=???x,y?
?D|xab ?
yab|2 (1)where D is a tuple of dimensions, xab = ?
(a, b) on di-mension x. ?
satisfies the axioms of minimality (2a),symmetry (2b), and the triangle inequality (2c), bywhich it determines a metric space on S:?
(a, b) ?
0 ?(?
(a, b) = 0?
a = b) (2a)?
(a, b) = ?
(b, a) (2b)?
(a, b) + ?
(b, c) ?
?
(a, c) (2c)We now turn to the problems raised in ?2.
P1 wouldbe avoided by a clustering algorithm that satisfies (3).
?Ci?C= ?
(3)It was also suggested above that a potential solutionto P1 is to cluster using the Nearest Neighbour Princi-ple.
Before considering a solution to P2, i.e.
the prob-lem of discovering clusters that approximate humanintuitions, it is useful to recapitulate the classic prin-ciples of perceptual grouping proposed by Wertheimer(1938), of which the following two are the most rele-vant:1.
Proximity The smaller the distance between ob-jects in the cluster, the more easily perceived it is.2.
Similarity Similar entities will tend to be moreeasily perceived as a coherent group.Arguably, once a numeric definition of (semantic)similarity is available, the Similarity Principle boilsdown to the Proximity principle, where proximity isdefined via a semantic distance function.
This viewis adopted here.
How well our interpretation of theseprinciples can be ported to the semantic clusteringproblem of ?3 will be seen in the following subsec-tions.To resolve P2, we will propose an algorithm thatuses a context-sensitive definition of ?nearest neigh-bour?.
Recall that P2 arises because, while ?
is a mea-sure of ?objective?
distance on some scale, perceived323proximity (resp.
distance) of a pair ?a, b?
is contingentnot only on ?
(a, b), but also on the distance of a andb from all other elements in S. A first step towardsmeeting this requirement is to consider, for a givenpair of objects, not only the absolute distance (prox-imity) between them, but also the extent to which theyare equidistant from other objects in S. Formally, ameasure of perceived proximity prox(a, b) can be ap-proximated by the following function.
Let the two setsPab, Dab be defined as follows:Pab ={x|x ?
S ?
?
(x, a) ?
?
(x, b)}Dab ={y|y ?
S ?
?
(y, a) 6?
?
(y, b)}Then:prox(a, b) = F (?
(a, b), |Pab|, |Dab|) (4)that is, prox(a, b) is a function of the absolute dis-tance ?
(a, b), the number of elements in S ?
{a, b}which are roughly equidistant from a and b, and thenumber of elements which are not equidistant.
Oneway of conceptualising this is to consider, for a givenobject a, the list of all other elements of S, ranked bytheir distance (proximity) to a.
Suppose there exists anobject b whose ranked list is similar to that of a, whileanother object c?s list is very different.
Then, all otherthings being equal (in particular, the pairwise absolutedistance), a clusters closer to b than does c.This takes us from a metric, distance-based concep-tion, to a broader notion of the ?similarity?
between twoobjects in a metric space.
Our definition is inspiredby Tversky?s feature-based Contrast Model (1977), inwhich the similarity of a, b with feature sets A,B isa linear function of the features they have in com-mon and the features that pertain only to A or B, i.e.
:sim(a, b) = f(A ?
B) ?
f(A ?B).
In (4), the dis-tance of a and b from every other object is the relevantfeature.4.1 Computing perceived proximityThe computation of pairwise perceived proximityprox(a, b), shown in Algorithm 1, is the first step to-wards finding clusters in the domain.Following Thorisson (1994), the procedure usesthe absolute distance ?
to calculate ?absolute proxim-ity?
(1.7), a value in (0, 1), with 1 corresponding to?
(a, b) = 0, i.e.
identity (cf.
axiom (2a) ).
The proce-dure then visits each element of the domain, and com-pares its rank with respect to a and b (1.9?1.13)3, in-crementing a proximity score s (1.10) if the ranks are3We simplify the presentation by assuming the functionrank(x, a) that returns the rank of x with respect to a. Inpractice, this is achieved by creating, for each element of theinput pair, a totally ordered list La such that La[r] holds theset of elements ranked at r with respect to ?
(x, a)Algorithm 1 prox(a,b)Require: ?
(a, b)Require: k (a constant)1: maxD ?
max?x,y?
?S?S ?
(x, y)2: if a = b then3: return 14: end if5: s?
06: d?
07: p(a, b)?
1?
?
(a,b)maxD8: for all x ?
S ?
{a, b} do9: if |rank(x, a)?
rank(x, b)| ?
k then10: s?
s + 111: else12: d?
d + 113: end if14: end for15: return p(a, b)?
sdapproximately equal, or a distance score d otherwise(1.12).
Approximate equality is determined via a con-stant k (1.1), which, based on our experiments is set toa tenth the size of S. The procedure returns the ratio ofproximity and distance scores, weighted by the abso-lute proximity p(a, b) (1.15).
Algorithm 1 is called forall pairs in S ?
S yielding, for each element a ?
S, alist of elements ordered by their perceived proximity toa.
The entity with the highest proximity to a is calledits anchor.
Note that any domain object has one, andonly one anchor.4.2 Creating clustersThe procedure makeClusters(S,Anchors),shown in its basic form in Algorithm 2, uses thenotion of an anchor introduced above.
The rationalebehind the algorithm is captured by the followingdeclarative principle, where C ?
C is any cluster, andanchor(a, b) means ?b is the anchor of a?
:a ?
C ?
anchor(a, b)?
b ?
C (5)A cluster is defined as the transitive closure of theanchor relation, that is, if it holds that anchor(a, b)and anchor(b, c), then {a, b, c} will be clustered to-gether.
Apart from satisfying (5), the procedure also in-duces a partition on S, satisfying (3).
Given these pri-mary aims, no attempt is made, once clusters are gen-erated, to further sub-divide them, although we brieflyreturn to this issue in ?5.
The algorithm initialises aset Clusters to empty (2.1), and iterates through thelist of objects S (2.5).
For each object a and its anchorb (2.6), it first checks whether they have already beenclustered (e.g.
if either of them was the anchor of anobject visited earlier) (2.7, 2.12).
If this is not the case,then a provisional cluster is initialised for each element324Algorithm 2 makeClusters(S, Anchors)Ensure: S 6= ?1: Clusters?
?2: if |S| = 1 then3: return S4: end if5: for all a ?
S do6: b?
Anchors[a]7: if ?C ?
Clusters : a ?
C then8: Ca ?
C9: else10: Ca ?
{a}11: end if12: if ?C ?
Clusters : b ?
C then13: Cb ?
C14: Clusters?
Clusters?
{Cb}15: else16: Cb ?
{b}17: end if18: Ca ?
Ca ?
Cb19: Clusters?
Clusters ?
{Ca}20: end for21: return Clusters(2.10, 2.16).
The procedure simply merges the clustercontaining a with that of its b (2.18), having removedthe latter from the cluster set (2.14).This algorithm is guaranteed to induce a partition,since no element will end up in more than one group.It does not depend on an ordering of pairs a` la Tho-risson.
However, problems arise when elements andanchors are clustered na?ively.
For instance, if an el-ement is very distant from every other element in thedomain, prox(a, b) will still find an anchor for it, andmakeClusters(S,Anchors) will place it in the samecluster as its anchor, although it is an outlier.
Beforedescribing how this problem is rectified, we introducethe notion of a family (F ) of elements.
Informally, thisis a set of elements of S that have the same anchor, thatis:?a, b ?
F : anchor(a, x) ?
anchor(b, y)?
x = y(6)The solution to the outlier problem is to calculate acentroid value for each family found after prox(a, b).This is the average proximity between the common an-chor and all members of its family, minus one stan-dard deviation.
Prior to merging, at line (2.18), thealgorithm now checks whether the proximity value be-tween an element and its anchor falls below the cen-troid value.
If it does, the the cluster containing anobject and that containing its anchor are not merged.4.3 Two applicationsThe algorithm was applied to the two scenarios de-scribed in ?2 and ?3.
In the spatial domain, the al-gorithm returns groups or clusters of entities, based ontheir spatial proximity.
This was tested on domains likeFigure 1 in which the input is a set of entities whoseposition is defined as a pair of x/y coordinates.
Fig-ure 1 illustrates a potential problem with the proce-dure.
In that figure, it holds that anchor(e8, e9) andanchor(e9, e8), making e8 and e9 a reciprocal pair.In such cases, the algorithm inevitably groups the twoelements, whatever their proximity/distance.
This maybe problematic when elements of a reciprocal pair arevery distant from eachother, in which case they are un-likely to be perceived as a group.
We return to thisproblem briefly in ?5.The second domain of application is the cluster-ing of properties into ?perspectives?.
Here, we usethe information-theoretic definition of similarity de-veloped by Lin (1998) and applied to corpus data byKilgarriff and Tugwell (Kilgarriff and Tugwell, 2001).This measure defines the similarity of two words as afunction of the likelihood of their occurring in the samegrammatical environments in a corpus.
This measurewas shown experimentally to correlate highly with hu-man acceptability judgments of disjunctive plural de-scriptions (Gatt and van Deemter, 2005a), when com-pared with a number of measures that calculate thesimilarity of word senses in WordNet.
Using this asthe measure of semantic distance between words, thealgorithm returns clusters such as those in Figure 2.input: { waiter, essay, footballer, article, servant,cricketer, novel, cook, book, maid,player, striker, goalkeeper }output:1 { essay, article, novel, book }2 { footballer, cricketer }3 { waiter, cook, servant, maid }4 { player, goalkeeper, striker }Figure 2: Output on a Semantic DomainIf the words in Figure 2 represented properties ofdifferent entities in the domain of discourse, thenthe clusters would represent perspectives or ?covers?,whose extension is a set of entities that can be talkedabout from the same point of view.
For example, ifsome entity were specified as having the property foot-baller, and the property striker, while another entityhad the property cricketer, then according to the outputof the algorithm, the description the footballer and thecricketer is the most conceptually coherent one avail-able.
It could be argued that the units of representation325spatial semantic1 0.94 0.582 0.86 0.363 0.62 0.764 0.93 0.52mean 0.84 0.64Table 2: Proportion of agreement among participantsin GRE are not words but ?properties?
(e.g.
values ofattributes) which can be realised in a number of differ-ent ways (if, for instance, there are a number of syn-onyms corresponding roughly to the same intension).This could be remedied by defining similarity as ?dis-tance in an ontology?
; conversely, properties could beviewed as a set of potential (word) realisations.5 EvaluationThe evaluation of the algorithm was based on a com-parison of its output against the output of human beingsin a similar task.Thirteen native or fluent speakers of English volun-teered to participate in the study.
The materials con-sisted of 8 domains, 4 of which were graphical repre-sentations of a 2D spatial layout containing 13 points.The pictures were generated by plotting numerical x/ycoordinates (the same values are used as input to thealgorithm).
The other four domains consisted of aset of 13 arbitrarily chosen nouns.
Participants werepresented with an eight-page booklet with spatial andsemantic domains on alternate pages.
They were in-structed to draw circles around the best clusters in thepictures, or write down the words in groups that wererelated according to their intuitions.
Clusters could beof arbitrary size, but each element had to be placed inexactly one cluster.5.1 Participant agreementParticipant agreement on each domain was measuredusing kappa.
Since the task did not involve predefinedclusters, the set of unique groups (denoted G) gener-ated by participants in every domain was identified,representing the set of ?categories?
available post hoc.For each domain element, the number of times it oc-curred in each group served as the basis to calculatethe proportion of agreement among participants for theelement.
The total agreement P (A) and the agreementexpected by chance, P (E) were then used in the stan-dard formulak = P (A)?
P (E)1?
P (E)Table 2 shows a remarkable difference between thetwo domain types, with very high agreement on spa-tial domains and lower values on the semantic task.The difference was significant (t = 2.54, p < 0.05).Disagreement on spatial domains was mostly due tothe problem of reciprocal pairs, where participants dis-agreed on whether entities such as e8 and e9 in Figure 1gave rise to a well-formed cluster or not.
However, allthe participants were consistent with the version of theNearest Neighbour Principle given in (5).
If an elementwas grouped, it was always grouped with its anchor.The disagreement in the semantic domains seemedto turn on two cases4:1.
Sub-clusters Whereas some proposals includedclusters such as { man, woman, boy, girl, infant,toddler, baby, child } , others chose to group {infant, toddler, baby,child } separately.2.
Polysemy For example, liver was in some casesclustered with { steak, pizza } , while othersgrouped it with items like { heart, lung } .Insofar as an algorithm should capture the whole rangeof phenomena observed, (1) above could be accountedfor by making repeated calls to the Algorithm to sub-divide clusters.
One problem is that, in case only onecluster is found in the original domain, the same clusterwill be returned after further attempts at sub-clustering.A possible solution to this is to redefine the parameterk in Algorithm (1), making the condition for proximitymore strict.
As for the second observation, the desider-atum expressed in (3) may be too strong in the semanticdomain, since words can be polysemous.
As suggestedabove, one way to resolve this would be to measuredistance between word senses, as opposed to words.5.2 Algorithm performanceThe performance of the algorithm (hereafter the target)against the human output was compared to two base-line algorithms.
In the spatial domains, we used animplementation of the Thorisson algorithm (Thorisson,1994) described in ?2.
In our implementation, the pro-cedure was called iteratively until all domain objectshad been clustered in at least one group.For the semantic domains, the baseline was a simpleprocedure which calculated the powerset of each do-main S. For each subset in pow(S) ?
{?, S}, the pro-cedure calculates the mean pairwise similarity betweenwords, returning an ordered list of subsets.
This partialorder is then traversed, choosing subsets until all ele-ments had been grouped.
This seemed to be a reason-able baseline, because it corresponds to the intuitionthat the ?best cluster?
from a semantic point of view isthe one with the highest pairwise similarity among itselements.4The conservative strategy used here probably amplifiesdisagreements; disregarding clusters which are subsumed byother clusters would control at least for case (1)326The output of the target and baseline algorithms wascompared to human output in the following ways:1.
By item In each of the eight test domains, anagreement score was calculated for each domainelement e (i.e.
13 scores in each domain).
LetUs be the set of distinct groups containing e pro-posed by the experimental participants, and let Uabe the set of unique groups containing e proposedby the algorithm (|Ua| = 1 in case of the targetalgorithm, but not necessarily for the baselines,since they do not impose a partition).
For eachpair ?Uai , Usj ?
of algorithm-human clusters, theagreement score was defined as|Uai ?
Usj ||Uai ?
Usj |+ |Uai ?
Usi |,i.e.
the ratio of the number of elements on whichthe human/algorithm agree, and the number of el-ements on which they do not agree.
This returns anumber in (0, 1) with 1 indicating perfect agree-ment.
The maximal such score for each entity wasselected.
This controlled for the possible advan-tage that the target alorithm might have, giventhat it, like the human participants, partitions thedomain.2.
By participant An overall mean agreement scorewas computed for each participant using theabove formula for the target and baseline algo-rithms in each domain.Results by item Table 3 shows the mean and modalagreement scores obtained for both target and base-line in each domain type.
At a glance, the target alo-rithm performed better than the baseline on the spatialdomains, with a modal score of 1, indicating perfectagreement on 60% of the objects.
The situation is dif-ferent in the semantic domains, where target and base-line performed roughly equally well; in fact, the modalscore of 1 accounts for 75% baseline scores.target baselinespatial mean 0.84 0.72mode 1 (60%) 0.67 (40%)semantic mean 0.86 0.86mode 1 (65%) 1 (75%)Table 3: Mean and modal agreement scoresUnsurprisingly, the difference between target andbaseline algorithmswas reliable on the spatial domains(t = 2.865, p < .01), but not on the semantic domains(t < 1, ns).
This was confirmed by a one-way Analysisof Variance (ANOVA), testing the effect of algorithm(target/baseline) and domain type (spatial/semantic) onagreement results.
There was a significant main ef-fect of domain type (F = 6.399, p = .01), whilethe main effect of algorithm was marginally significant(F = 3.542, p = .06).
However, there was a reliabletype ?
algorithm interaction (F = 3.624, p = .05),confirming the finding that the agreement between tar-get and human output differed between domain types.Given the relative lack of agreement between partic-ipants in the semantic clustering task, this is unsur-prising.
Although the analysis focused on maximalscores obtained per entity, if participants do not agreeon groupings, then the means which are statisticallycompared are likely to mask a significant amount ofvariance.
We now turn to the analysis by participants.Results by participant The difference between tar-get and baselines in agreement across participants wassignificant both for spatial (t = 16.6, p < .01)and semantic (t = 5.759, t < .01) domain types.This corroborates the earlier conclusion: once par-ticipant variation is controlled for by including it inthe statistical model, the differences between targetand baseline show up as reliable across the board.
Aunivariate ANOVA corroborates the results, showingno significant main effect of domain type (F < 1,ns), but a highly significant main effect of algorithm(F = 233.5, p < .01) and a significant interaction(F = 44.3, p < .01).Summary The results of the evaluation are encour-aging, showing high agreement between the output ofthe algorithm and the output that was judged by hu-mans as most appropriate.
They also suggest frame-work of ?4 corresponds to human intuitions better thanthe baselines tested here.
However, these results shouldbe interpretedwith caution in the case of semantic clus-tering, where there was significant variability in humanagreement.
With respect to spatial clustering, one out-standing problem is that of reciprocal pairs which aretoo distant from eachother to form a perceptually well-formed cluster.
We are extending the empirical studyto new domains involving such cases, in order to inferfrom the human data a threshold on pairwise distancebetween entities, beyond which they are not clustered.6 Conclusions and future workThis paper attempted to achieve a dual goal.
First, wehighlighted a number of scenarios in which the perfor-mance of a GRE algorithm can be enhanced by an ini-tial step which identifies clusters of entities or proper-ties.
Second, we described an algorithm which takes asinput a set of objects and returns a set of clusters basedon a calculation of their perceived proximity.
The def-inition of perceived proximity seeks to take into ac-count some of the principles of human perceptual andconceptual organisation.In current work, the algorithm is being applied to327two problems in GRE, namely, the generation of spatialreferences involving collective predicates (e.g.
gath-ered), and the identification of the available perspec-tives or conceptual covers, under which referents maybe described.ReferencesM.
Aloni.
2002.
Questions under cover.
In D. Barker-Plummer, D. Beaver, J. van Benthem, and P. Scottode Luzio, editors, Words, Proofs, and Diagrams.CSLI.Anja Arts.
2004.
Overspecification in InstructiveTexts.
Ph.D. thesis, Univiersity of Tilburg.Robert Dale and Ehud Reiter.
1995.
Computationalinterpretation of the Gricean maxims in the gener-ation of referring expressions.
Cognitive Science,19(8):233?263.Robert Dale.
1989.
Cooking up referring expressions.In Proceedings of the 27th Annual Meeting of theAssociation for Computational Linguistics, ACL-89.C.
Eschenbach, C. Habel, M. Herweg, and K. Rehkam-per.
1989.
Remarks on plural anaphora.
In Pro-ceedings of the 4th Conference of the EuropeanChapter of the Association for Computational Lin-guistics, EACL-89.K.
Funakoshi, S. Watanabe, N. Kuriyama, and T. Toku-naga.
2004.
Generating referring expressions usingperceptual groups.
In Proceedings of the 3rd Inter-national Conference on Natural Language Genera-tion, INLG-04.A.
Gatt and K. van Deemter.
2005a.
Semantic simi-larity and the generation of referring expressions: Afirst report.
In Proceedings of the 6th InternationalWorkshop on Computational Semantics, IWCS-6.A.
Gatt and K. Van Deemter.
2005b.
Towards apsycholinguistically-motivated algorithm for refer-ring to sets: The role of semantic similarity.
Techni-cal report, TUNA Project, University of Aberdeen.H.P.
Grice.
1975.
Logic and conversation.
In P. Coleand J.L.
Morgan, editors, Syntax and Semantics:Speech Acts., volume III.
Academic Press.P.
Jordan and M. Walker.
2000.
Learning attributeselections for non-pronominal expressions.
In Pro-ceedings of the 38th Annual Meeting of the Associa-tion for Computational Linguistics, ACL-00.B.
Kaup, S. Kelter, and C. Habel.
2002.
Represent-ing referents of plural expressions and resolving plu-ral anaphors.
Language and Cognitive Processes,17(4):405?450.A.
Kilgarriff and D. Tugwell.
2001.
Word sketch: Ex-traction and display of significant collocations forlexicography.
In Proceedings of the CollocationsWorkshop in Association with ACL-2001.S.
Koh and C. Clifton.
2002.
Resolution of the an-tecedent of a plural pronoun: Ontological categoriesand predicate symmetry.
Journal of Memory andLanguage, 46:830?844.E.
Krahmer and M. Theune.
2002.
Efficient context-sensitive generation of referring expressions.
InKees van Deemter and Rodger Kibble, editors, In-formation Sharing: Reference and Presupposition inLanguage Generation and Interpretation.
Stanford:CSLI.A.
Kronfeld.
1989.
Conversationally relevant descrip-tions.
In Proceedings of the 27th Annual Meetingof the Association for Computational Linguistics,ACL89.D.
Lin.
1998.
An information-theoretic definition ofsimilarity.
In Proceedings of the International Con-ference on Machine Learning.L.
Moxey, A. J. Sanford, P. Sturt, and L. I Morrow.2004.
Constraints on the formation of plural refer-ence objects: The influence of role, conjunction andtype of description.
Journal of Memory and Lan-guage, 51:346?364.F.
P. Prepaarata and M. A. Shamos.
1985.
Computa-tional Geometry.
Springer.E.
Reiter.
1990.
The computational complexity ofavoiding conversational implicatures.
In Proceed-ings of the 28th Annual Meeting of the Associationfor Computational Linguistics, ACL-90.K.
R. Thorisson.
1994.
Simulated perceptual group-ing: An application to human-computer interaction.In Proceedings of the 16th Annual Conference of theCognitive Science Society.A.
Treisman.
1982.
Perceptual grouping and attentionin visual search for features and objects.
Journal ofExperimental Psychology: Human Perception andPerformance, 8(2):194?214.A.
Tversky.
1977.
Features of similarity.
Psychologi-cal Review, 84(4):327?352.K.
van Deemter.
2000.
Generating vague descriptions.In Proceedings of the First International Conferenceon Natural Language Generation, INLG-00.Kees van Deemter.
2002.
Generating referring expres-sions: Boolean extensions of the incremental algo-rithm.
Computational Linguistics, 28(1):37?52.M.
Wertheimer.
1938.
Laws of organization in per-ceptual forms.
In W. Ellis, editor, A Source Book ofGestalt Psychology.
Routledge & Kegan Paul.328
