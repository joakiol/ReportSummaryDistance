Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 259?270,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsLearning Word Sense Distributions, Detecting Unattested Senses andIdentifying Novel Senses Using Topic ModelsJey Han Lau,?Paul Cook,?Diana McCarthy,?Spandana Gella,?and Timothy Baldwin??
Dept of Philosophy, King?s College London?
Dept of Computing and Information Systems, The University of Melbourne?
University of Cambridgejeyhan.lau@gmail.com, paulcook@unimelb.edu.au,diana@dianamccarthy.co.uk, spandanagella@gmail.com, tb@ldwin.netAbstractUnsupervised word sense disambiguation(WSD) methods are an attractive approachto all-words WSD due to their non-relianceon expensive annotated data.
Unsuper-vised estimates of sense frequency havebeen shown to be very useful for WSD dueto the skewed nature of word sense distri-butions.
This paper presents a fully unsu-pervised topic modelling-based approachto sense frequency estimation, which ishighly portable to different corpora andsense inventories, in being applicable toany part of speech, and not requiring a hi-erarchical sense inventory, parsing or par-allel text.
We demonstrate the effective-ness of the method over the tasks of pre-dominant sense learning and sense distri-bution acquisition, and also the novel tasksof detecting senses which aren?t attestedin the corpus, and identifying novel sensesin the corpus which aren?t captured in thesense inventory.1 IntroductionThe automatic determination of word sense infor-mation has been a long-term pursuit of the NLPcommunity (Agirre and Edmonds, 2006; Navigli,2009).
Word sense distributions tend to be Zip-fian, and as such, a simple but surprisingly high-accuracy back-off heuristic for word sense dis-ambiguation (WSD) is to tag each instance of agiven word with its predominant sense (McCarthyet al, 2007).
Such an approach requires knowl-edge of predominant senses; however, word sensedistributions ?
and predominant senses too ?vary from corpus to corpus.
Therefore, meth-ods for automatically learning predominant sensesand sense distributions for specific corpora are re-quired (Koeling et al, 2005; Lapata and Brew,2004).In this paper, we propose a method which usestopic models to estimate word sense distributions.This method is in principle applicable to all partsof speech, and moreover does not require a parser,a hierarchical sense representation or parallel text.Topic models have been used for WSD in a num-ber of studies (Boyd-Graber et al, 2007; Li etal., 2010; Lau et al, 2012; Preiss and Stevenson,2013; Cai et al, 2007; Knopp et al, 2013), butour work extends significantly on this earlier workin focusing on the acquisition of prior word sensedistributions (and predominant senses).Because of domain differences and the skewednature of word sense distributions, it is often thecase that some senses in a sense inventory willnot be attested in a given corpus.
A system ca-pable of automatically finding such senses couldreduce ambiguity, particularly in domain adapta-tion settings, while retaining rare but neverthelessviable senses.
We further propose a method for ap-plying our sense distribution acquisition system tothe task of finding unattested senses ?
i.e., sensesthat are in the sense inventory but not attested ina given corpus.
In contrast to the previous workof McCarthy et al (2004a) on this topic whichuses the sense ranking score from McCarthy etal.
(2004b) to remove low-frequency senses fromWordNet, we focus on finding senses that are unat-tested in the corpus on the premise that, given ac-curate disambiguation, rare senses in a corpus con-tribute to correct interpretation.Corpus instances of a word can also correspondto senses that are not present in a given sense in-ventory.
This can be due to, for example, wordstaking on new meanings over time (e.g.
the rela-259tively recent senses of tablet and swipe related totouchscreen computers) or domain-specific termsnot being included in a more general-purposesense inventory.
A system for automatically iden-tifying such novel senses ?
i.e.
senses that areattested in the corpus but not in the sense inven-tory ?
would be a very valuable lexicographi-cal tool for keeping sense inventories up-to-date(Cook et al, 2013).
We further propose an appli-cation of our proposed method to the identificationof such novel senses.
In contrast to McCarthy et al(2004b), the use of topic models makes this possi-ble, using topics as a proxy for sense (Brody andLapata, 2009; Yao and Durme, 2011; Lau et al,2012).
Earlier work on identifying novel sensesfocused on individual tokens (Erk, 2006), whereasour approach goes further in identifying groups oftokens exhibiting the same novel sense.2 Background and Related WorkThere has been a considerable amount of researchon representing word senses and disambiguatingusages of words in context (WSD) as, in orderto produce computational systems that understandand produce natural language, it is essential tohave a means of representing and disambiguat-ing word sense.
WSD algorithms require wordsense information to disambiguate token instancesof a given ambiguous word, e.g.
in the form ofsense definitions (Lesk, 1986), semantic relation-ships (Navigli and Velardi, 2005) or annotateddata (Zhong and Ng, 2010).
One extremely use-ful piece of information is the word sense prioror expected word sense frequency distribution.This is important because word sense distributionsare typically skewed (Kilgarriff, 2004), and sys-tems do far better when they take bias into ac-count (Agirre and Martinez, 2004).Typically, word frequency distributions are esti-mated with respect to a sense-tagged corpus suchas SemCor (Miller et al, 1993), a 220,000 wordcorpus tagged with WordNet (Fellbaum, 1998)senses.
Due to the expense of hand tagging, andsense distributions being sensitive to domain andgenre, there has been some work on trying toestimate sense frequency information automati-cally (McCarthy et al, 2004b; Chan and Ng, 2005;Mohammad and Hirst, 2006; Chan and Ng, 2006).Much of this work has been focused on rankingword senses to find the predominant sense in agiven corpus (McCarthy et al, 2004b; Mohammadand Hirst, 2006), which is a very powerful heuris-tic approach to WSD.
Most WSD systems rely uponthis heuristic for back-off in the absence of strongcontextual evidence (McCarthy et al, 2007).
Mc-Carthy et al (2004b) proposed a method whichrelies on distributionally similar words (nearestneighbours) associated with the target word inan automatically acquired thesaurus (Lin, 1998).The distributional similarity scores of the nearestneighbours are associated with the respective tar-get word senses using a WordNet similarity mea-sure, such as those proposed by Jiang and Conrath(1997) and Banerjee and Pedersen (2002).
Theword senses are ranked based on these similar-ity scores, and the most frequent sense is selectedfor the corpus that the distributional similarity the-saurus was trained over.As well as sense ranking for predominant senseacquisition, automatic estimates of sense fre-quency distribution can be very useful for WSDfor training data sampling purposes (Agirre andMartinez, 2004), entropy estimation (Jin et al,2009), and prior probability estimates, all of whichcan be integrated within a WSD system (Chan andNg, 2005; Chan and Ng, 2006; Lapata and Brew,2004).
Various approaches have been adopted,such as normalizing sense ranking scores to ob-tain a probability distribution (Jin et al, 2009), us-ing subcategorisation information as an indicationof verb sense (Lapata and Brew, 2004) or alter-natively using parallel text (Chan and Ng, 2005;Chan and Ng, 2006; Agirre and Martinez, 2004).The work of Boyd-Graber and Blei (2007) ishighly related in that it extends the method of Mc-Carthy et al (2004b) to provide a generative modelwhich assumes the words in a given document aregenerated according to the topic distribution ap-propriate for that document.
They then predict themost likely sense for each word in the documentbased on the topic distribution and the words incontext (?corroborators?
), each of which, in turn,depends on the document?s topic distribution.
Us-ing this approach, they get comparable results toMcCarthy et al when context is ignored (i.e.
us-ing a model with one topic), and at most a 1% im-provement on SemCor when they use more topicsin order to take context into account.
Since theresults do not improve on McCarthy et al as re-gards sense distribution acquisition irrespective ofcontext, we will compare our model with that pro-posed by McCarthy et al260Recent work on finding novel senses has tendedto focus on comparing diachronic corpora (Sagiet al, 2009; Cook and Stevenson, 2010; Gulor-dava and Baroni, 2011) and has also consideredtopic models (Lau et al, 2012).
In a similar vein,Peirsman et al (2010) considered the identifica-tion of words having a sense particular to onelanguage variety with respect to another (specif-ically Belgian and Netherlandic Dutch).
In con-trast to these studies, we propose a model for com-paring a corpus with a sense inventory.
Carpuatet al (2013) exploit parallel corpora to identifywords in domain-specific monolingual corporawith previously-unseen translations; the methodwe propose does not require parallel data.3 MethodologyOur methodology is based on the WSI systemdescribed in Lau et al (2012),1which has beenshown (Lau et al, 2012; Lau et al, 2013a; Lau etal., 2013b) to achieve state-of-the-art results overthe WSI tasks from SemEval-2007 (Agirre andSoroa, 2007), SemEval-2010 (Manandhar et al,2010) and SemEval-2013 (Navigli and Vannella,2013; Jurgens and Klapaftis, 2013).
The systemis built around a Hierarchical Dirichlet Process(HDP: Teh et al (2006)), a non-parametric variantof a Latent Dirichlet Allocation topic model (Bleiet al, 2003) where the model automatically opti-mises the number of topics in a fully-unsupervisedfashion over the training data.To learn the senses of a target lemma, we traina single topic model per target lemma.
The sys-tem reads in a collection of usages of that lemma,and automatically induces topics (= senses) in theform of a multinomial distribution over words, andper-usage topic assignments (= probabilistic senseassignments) in the form of a multinomial distri-bution over topics.
Following Lau et al (2012),we assign one topic to each usage by selecting thetopic that has the highest cumulative probabilitydensity, based on the topic allocations of all wordsin the context window for that usage.2Note that intheir original work, Lau et al (2012) experimentedwith the use of features extracted from a depen-dency parser.
Due to the computational overheadassociated with these features, and the fact that theempirical impact of the features was found to be1Based on the implementation available at: https://github.com/jhlau/hdp-wsi2This includes all words in the usage sentence exceptstopwords, which were filtered in the preprocessing step.marginal, we make no use of parser-based featuresin this paper.3The induced topics take the form of word multi-nomials, and are often represented by the top-Nwords in descending order of conditional probabil-ity.
We interpret each topic as a sense of the targetlemma.4To illustrate this, we give the example oftopics induced by the HDP model for network inTable 1.We refer to this method as HDP-WSI hence-forth.5In predominant sense acquisition, the task is tolearn, for each target lemma, the most frequentlyoccurring word sense in a particular domain orcorpus, relative to a predefined sense inventory.The WSI system provides us with a topic alloca-tion per usage of a given word, from which we canderive a distribution of topics over usages and apredominant topic.
In order to map this onto thepredominant sense, we need to have some way ofaligning a topic with a sense.
We design our topic?sense alignment methodology with portability inmind ?
it should be applicable to any sense in-ventory.
As such, our alignment methodology as-sumes only that we have access to a conventionalsense gloss or definition for each sense, and doesnot rely on ontological/structural knowledge (e.g.the WordNet hierarchy).To compute the similarity between a senseand a topic, we first convert the words in thegloss/definition into a multinomial distributionover words, based on simple maximum likeli-hood estimation.6We then calculate the Jensen?Shannon divergence between the multinomial dis-tribution (over words) of the gloss and that of thetopic, and convert the divergence value into a sim-ilarity score by subtracting it from 1.
Formally, thesimilarity sense siand topic tjis:sim(si, tj) = 1?
JS(S?T ) (1)where S and T are the multinomial distributions3For hyper-parameters ?
and ?, we used 0.1 for both.
Wedid not tune the parameters, and opted to use the default pa-rameters introduced in Teh et al (2006).4To avoid confusion, we will refer to the HDP-inducedtopics as topics, and reserve the term sense to denote sensesin a sense inventory.5The code used to learn predominant sense and run allexperiments described in this paper is available at: https://github.com/jhlau/predom_sense.6Words are tokenised using OpenNLP and lemmatisedwith Morpha (Minnen et al, 2001).
We additionally removethe target lemma, stopwords and words that are less than 3characters in length.261Topic Num Top-10 Terms1 network support @card@ information research service group development community member2 service @card@ road company transport rail area government network public3 network social model system family structure analysis form relationship neural4 network @card@ computer system service user access internet datum server5 system network management software support corp company service application product6 @card@ radio news television show bbc programme call think film7 police drug criminal terrorist intelligence network vodafone iraq attack cell8 network atm manager performance craigavon group conference working modelling assistant9 root panos comenius etd unipalm lse brazil telephone xxx discussTable 1: An example to illustrate the topics induced for network by the HDP model.
The top-10 highestprobability terms are displayed to represent each topic (@card@ denotes a tokenised cardinal number).over words for sense siand topic tj, respectively,and JS(X?Y ) is the Jensen?Shannon divergencefor distribution X and Y .To learn the predominant sense, we compute theprevalence score of each sense and take the sensewith the highest prevalence score as the predom-inant sense.
The prevalence score for a sense iscomputed by summing the product of its similar-ity scores with each topic (i.e.
sim(si, tj)) and theprior probability of the topic in question (basedon maximum likelihood estimation).
Formally, theprevalence score of sense siis given as follows:prevalence(si) =T?j(sim(si, tj)?
P (tj)) (2)=T?j(sim(si, tj)?f(tj)?Tkf(tk))where f(tj) is the frequency of topic tj(i.e.
thenumber of usages assigned to topic tj), and T isthe number of topics.The intuition behind the approach is that thepredominant sense should be the sense that has rel-atively high similarity (in terms of lexical overlap)with high-probability topic(s).4 WordNet ExperimentsWe first test the proposed method over the tasksof predominant sense learning and sense distribu-tion induction, using the WordNet-tagged datasetof Koeling et al (2005), which is made up of3 collections of documents: a domain-neutralcorpus (BNC), and two domain-specific corpora(SPORTS and FINANCE).
For each domain,annotators were asked to sense-annotate a ran-dom selection of sentences for each of 40 targetnouns, based on WordNet v1.7.
The predominantsense and distribution across senses for each targetlemma was obtained by aggregating over the senseannotations.
The authors evaluated their method interms of WSD accuracy over a given corpus, basedon assigning all instances of a target word with thepredominant sense learned from that corpus.
Forthe remainder of the paper, we denote their systemas MKWC.To compare our system (HDP-WSI) withMKWC, we apply it to the three datasets of Koel-ing et al (2005).
For each dataset, we use HDPto induce topics for each target lemma, computethe similarity between the topics and the WordNetsenses (Equation (1)), and rank the senses basedon the prevalence scores (Equation (2)).
In addi-tion to the WSD accuracy based on the predomi-nant sense inferred from a particular corpus, weadditionally compute: (1) AccUB, the upper boundfor the first sense-based WSD accuracy (using thegold standard predominant sense for disambigua-tion);7and (2) ERR, the error rate reduction be-tween the accuracy for a given system (Acc) andthe upper bound (AccUB), calculated as follows:ERR = 1?AccUB?
AccAccUBLooking at the results in Table 2, we see lit-tle difference in the results for the two methods,with MKWC performing better over two of thedatasets (BNC and SPORTS) and HDP-WSI per-forming better over the third (FINANCE), but alldifferences are small.
Based on the McNemar?sTest with Yates correction for continuity, MKWCis significantly better over BNC and HDP-WSI issignificantly better over FINANCE (p < 0.0001in both cases), but the difference over SPORTSis not statistically significance (p > 0.1).
Notethat there is still much room for improvement with7The upper bound for a WSD approach which tags all to-ken occurrences of a given word with the same sense, as afirst step towards context-sensitive unsupervised WSD.262DatasetFSCORPUSMKWC HDP-WSIAccUBAcc ERR Acc ERRBNC 0.524 0.407 (0.777) 0.376 (0.718)FINANCE 0.801 0.499 (0.623) 0.555 (0.693)SPORTS 0.774 0.437 (0.565) 0.422 (0.545)Table 2: WSD accuracy for MKWC and HDP-WSIon the WordNet-annotated datasets, as comparedto the upper-bound based on actual first sense inthe corpus (higher values indicate better perfor-mance; the best system in each row [other than theFSCORPUSupper bound] is indicated in boldface).Dataset MKWC HDP-WSIBNC 0.226 0.214FINANCE 0.426 0.375SPORTS 0.420 0.363Table 3: Sense distribution evaluation of MKWCand HDP-WSI on the WordNet-annotated datasets,evaluated using JS divergence (lower values indi-cate better performance; the best system in eachrow is indicated in boldface).both systems, as we see in the gap between the up-per bound (based on perfect determination of thefirst sense) and the respective system accuracies.Given that both systems compute a continuous-valued prevalence score for each sense of a tar-get lemma, a distribution of senses can be ob-tained by normalising the prevalence scores acrossall senses.
The predominant sense learning taskof McCarthy et al (2007) evaluates the ability ofa method to identify only the head of this dis-tribution, but it is also important to evaluate thefull sense distribution (Jin et al, 2009).
To thisend, we introduce a second evaluation metric:the Jensen?Shannon (JS) divergence between theinferred sense distribution and the gold-standardsense distribution, noting that smaller values arebetter in this case, and that it is now theoreticallypossible to obtain a JS divergence of 0 in the caseof a perfect estimate of the sense distribution.
Re-sults are presented in Table 3.HDP-WSI consistently achieves lower JS diver-gence, indicating that the distribution of sensesthat it finds is closer to the gold standard distri-bution.
Testing for statistical significance over thepaired JS divergence values for each lemma usingthe Wilcoxon signed-rank test, the result for FI-NANCE is significant (p < 0.05) but the resultsfor the other two datasets are not (p > 0.1 in eachcase).DatasetFSCORPUSFSDICTHDP-WSIAccUBAcc ERR Acc ERRUKWAC 0.574 0.387 (0.674) 0.514 (0.895)TWITTER 0.468 0.297 (0.635) 0.335 (0.716)Table 4: WSD accuracy for HDP-WSI on theMacmillan-annotated datasets, as compared to theupper-bound based on actual first sense in the cor-pus (higher values indicate better performance; thebest system in each row [other than the FSCORPUSupper bound] is indicated in boldface).Dataset FSCORPUSFSDICTHDP-WSIUKWAC 0.210 0.393 0.156TWITTER 0.259 0.472 0.171Table 5: Sense distribution evaluation of HDP-WSI on the Macmillan-annotated datasets as com-pared to corpus- and dictionary-based first sensemethods, evaluated using JS divergence (lowervalues indicate better performance; the best sys-tem in each row is indicated in boldface).To summarise, the results for MKWC and HDP-WSI are fairly even for predominant sense learn-ing (each outperforms the other at a level of statis-tical significance over one dataset), but HDP-WSIis better at inducing the overall sense distribution.It is important to bear in mind that MKWC inthese experiments makes use of full-text parsing incalculating the distributional similarity thesaurus,and the WordNet graph structure in calculating thesimilarity between associated words and differentsenses.
Our method, on the other hand, uses noparsing, and only the synset definitions (and notthe graph structure) of WordNet.8The non-relianceon parsing is significant in terms of portability totext sources which are less amenable to parsing(such as Twitter: (Baldwin et al, 2013)), and thenon-reliance on the graph structure of WordNet issignificant in terms of portability to conventional?flat?
sense inventories.
While comparable resultson a different dataset have been achieved with aproximity thesaurus (McCarthy et al, 2007) com-pared to a dependency one,9it is not stated how8McCarthy et al (2004b) obtained good results with def-inition overlap, but their implementation uses the relationstructure alongside the definitions (Banerjee and Pedersen,2002).
Iida et al (2008) demonstrate that further exten-sions using distributional data are required when applying themethod to resources without hierarchical relations.9The thesauri used in the reimplementation of MKWCin this paper were obtained from http://webdocs.cs.ualberta.ca/?lindek/downloads.htm.263wide a window is needed for the proximity the-saurus.
This could be a significant issue with Twit-ter data, where context tends to be limited.
In thenext section, we demonstrate the robustness of themethod in experimenting with two new datasets,based on Twitter and a web corpus, and the Macmil-lan English Dictionary.5 Macmillan ExperimentsIn our second set of experiments, we move to anew dataset (Gella et al, to appear) based on textfrom ukWaC (Ferraresi et al, 2008) and Twit-ter, and annotated using the Macmillan English Dic-tionary10(henceforth ?Macmillan?).
For the pur-poses of this research, the choice of Macmillan issignificant in that it is a conventional dictionarywith sense definitions and examples, but no link-ing between senses.11In terms of the original re-search which gave rise to the sense-tagged dataset,Macmillan was chosen over WordNet for reasons in-cluding: (1) the well-documented difficulties ofsense tagging with fine-grained WordNet senses(Palmer et al, 2004; Navigli et al, 2007); (2) theregular update cycle of Macmillan (meaning it con-tains many recently-emerged senses); and (3) thefinding in a preliminary sense-tagging task that itbetter captured Twitter usages than WordNet (andalso OntoNotes: Hovy et al (2006)).The dataset is made up of 20 target nouns whichwere selected to span the high- to mid-frequencyrange in both Twitter and the ukWaC corpus, andhave at least 3 Macmillan senses.
The average senseambiguity of the 20 target nouns in Macmillan is 5.6(but 12.3 in WordNet).
100 usages of each targetnoun were sampled from each of Twitter (from acrawl over the time period Jan 3?Feb 28, 2013 us-ing the Twitter Streaming API) and ukWaC, afterlanguage identification using langid.py (Luiand Baldwin, 2012) and POS tagging (based onthe CMU ARK Twitter POS tagger v2.0 (Owoputiet al, 2012) for Twitter, and the POS tags providedwith the corpus for ukWaC).
Amazon Mechani-cal Turk (AMT) was then used to 5-way sense-tageach usage relative to Macmillan, including allow-ing the annotators the option to label a usage as?Other?
in instances where the usage was not cap-tured by any of the Macmillan senses.
After qual-ity control over the annotators/annotations (see10http://www.macmillandictionary.com/11Strictly speaking, there is limited linking in the form ofsets of synonyms in Macmillan, but we choose to not use thisinformation in our research.Gella et al (to appear) for details), and aggregationof the annotations into a single sense per usage(possibly ?Other?
), there were 2000 sense-taggedukWaC sentences and Twitter messages over the20 target nouns.
We refer to these two datasets asUKWAC and TWITTER henceforth.To apply our method to the two datasets, we useHDP-WSI to train a model for each target noun,based on the combined set of usages of that lemmain each of the two background corpora, namely theoriginal Twitter crawl that gave rise to the TWIT-TER dataset, and all of ukWaC.5.1 Learning Sense DistributionsAs in Section 4, we evaluate in terms of WSDaccuracy (Table 4) and JS divergence over thegold-standard sense distribution (Table 5).
Wealso present the results for: (a) a supervised base-line (?FSCORPUS?
), based on the most frequentsense in the corpus; and (b) an unsupervised base-line (?FSDICT?
), based on the first-listed sense inMacmillan.
In each case, the sense distribution isbased on allocating all probability mass for a givenword to the single sense identified by the respec-tive method.We first notice that, despite the coarser-grainedsenses of Macmillan as compared to WordNet, theupper bound WSD accuracy using Macmillan iscomparable to that of the WordNet-based datasetsover the balanced BNC, and quite a bit lower thanthat of the two domain corpora of Koeling et al(2005).
This suggests that both datasets are di-verse in domain and content.In terms of WSD accuracy, the results overUKWAC (ERR = 0.895) are substantially higherthan those for BNC, while those over TWITTER(ERR = 0.716) are comparable.
The accuracy issignificantly higher than the dictionary-based firstsense baseline (FSDICT) over both datasets (McNe-mar?s test; p < 0.0001), and the ERR is also con-siderably higher than for the two domain datasetsin Section 4 (FINANCE and SPORTS).
Onecause of difficulty in sense-modelling TWITTERis large numbers of missing senses, with 12.3%of usages in TWITTER and 6.6% in UKWAC hav-ing no corresponding Macmillan sense.12This chal-lenges the assumption built into the sense preva-lence calculation that all topics will align to a pre-existing sense, a point we return to in Section 5.2.12The relative occurrence of unlisted/unclear senses in thedatasets of Koeling et al (2005) is comparable to UKWAC.264Dataset P R FUKWAC 0.73 0.85 0.74TWITTER 0.56 0.88 0.65Table 6: Evaluation of our method for identify-ing unattested senses, averaged over 10 runs of 10-fold cross validationThe JS divergence results for both datasets arewell below (= better than) the results for all threeWordNet-based datasets, and also superior to boththe supervised and unsupervised first-sense base-lines.
Part of the reason for this improvement issimply that the average polysemy in Macmillan (5.6senses per target lemma) is slightly less than inWordNet (6.7 senses per target lemma),13makingthe task slightly easier in the Macmillan case.5.2 Identification of Unattested SensesWe observed in Section 5.1 that there are rela-tively frequent occurrences of usages (e.g.
12.3%for TWITTER) which aren?t captured by Macmil-lan.
Conversely, there are also senses in Macmillanwhich aren?t attested in the annotated sample ofusages.
Specifically, of the 112 senses defined forthe 20 target lemmas, 25 (= 22.3%) of the sensesare not attested in the 2000 usages in either cor-pora.
Given that our methodology computes aprevalence score for each sense, it can equally beapplied to the detection of these unattested senses,and it is this task that we address in this section:the identification of senses that are defined in thesense inventory but not attested in a given corpus.Intuitively, an unused sense should have lowsimilarity with the HDP induced topics.
As such,we introduce sense-to-topic affinity, a measurethat estimates how likely a sense is not attested inthe corpus:st-affinity(si) =?Tjsim(si, tj)?Sk?Tlsim(sk, tl)(3)where sim(si, tj) is carried over from Equa-tion (1), and T and S represent the number of top-ics and senses, respectively.We treat the task of identification of unusedsenses as a binary classification problem, wherethe goal is to find a sense-to-topic affinity thresh-old below which a sense will be considered to13Note that the set of lemmas differs between the respec-tive datasets, so this isn?t an accurate reflection of the relativegranularity of the two dictionaries.be unused.
We pool together all the senses andrun 10-fold cross validation to learn the thresholdfor identifying unused senses,14evaluated usingsense-level precision (P ), recall (R) and F-score(F ) at detecting unattested senses.
We repeat theexperiment 10 times (partitioning the items ran-domly into folds) and collect the mean precision,recall and F-scores across the 10 runs.
We foundencouraging results for the task, as detailed in Ta-ble 6.
For the threshold, the average value withstandard deviation is 0.092?
0.044 over UKWACand 0.125?0.052 over TWITTER, indicating rela-tive stability in the value of the threshold both in-ternally within a dataset, and also across datasets.5.3 Identification of Novel SensesIn both TWITTER and UKWAC, we observed fre-quent occurrences of usages of our target nounswhich didn?t map onto a pre-existing Macmillansense.
A natural question to ask is whether ourmethod can be used to predict word senses that aremissing from our sense inventory, and identify us-ages associated with each such missing sense.
Wewill term these ?novel senses?, and define ?novelsense identification?
to be the task of identifyingnew senses that are not recorded in the inventorybut are seen in the corpus.An immediate complication in evaluating novelsense identification is that we are attempting toidentify senses which explicitly aren?t in our senseinventory.
This contrasts with the identification ofunattested senses, e.g., where we were attemptingto identify which of the known senses wasn?t ob-served in the corpus.
Also, while we have annota-tions of ?Other?
usages in TWITTER and UKWAC,there is no real expectation that all such usageswill correspond to the same sense: in practice,they are attributable to a myriad of effects such asincorporation in a non-compositional multiwordexpression, and errors in POS tagging (i.e.
the us-age not being nominal).
As such, we can?t use the?Other?
annotations to evaluate novel sense iden-tification.
The evaluation of systems for this taskis a known challenge, which we address similarlyto Erk (2006) by artificially synthesising novelsenses through removal of senses from the senseinventory.
In this way, even if we remove multi-ple senses for a given word, we still have accessto information about which usages correspond to14We used a fixed step and increment at steps of 0.001, upto the max value of st-affinity when optimising the threshold.265No.
Lemmas with Relative Freq ThresholdP R Fa Removed Sense of Removed Sense Mean?stdev20 0.0?0.2 0.052?0.009 0.35 0.42 0.369 0.2?0.4 0.089?0.024 0.24 0.59 0.296 0.4?0.6 0.061?0.004 0.63 0.64 0.63Table 7: Classification of usages with novel sense for all target lemmas.No.
Lemmas with Relative Freq ThresholdP R Fa Removed Sense of Removed Sense Mean?stdev9 0.2?0.4 0.093?0.023 0.50 0.66 0.526 0.4?0.6 0.099?0.018 0.73 0.90 0.80Table 8: Classification of usages with novel sense for target lemmas with a removed sense.which novel sense.
An additional advantage ofthis procedure is that it allows us to control an im-portant property of novel senses: their frequencyof occurrence.In the experiments that follow, we randomlyselect senses for removal from three frequencybands: low, medium and high frequency senses.Frequency is defined by relative occurrence in theannotated usages: low = 0.0?0.2; medium = 0.2?0.4; and high = 0.4?0.6.
Note that we do not con-sider high-frequency senses with frequency higherthan 0.6, as it is rare for a medium- to high-frequency word to take on a novel sense whichis then the predominant sense in a given corpus.Note also that not all target lemmas will have anovel sense through synthesis, as they may haveno senses that fall within the indicated bounds ofrelative occurrence (e.g.
if > 60% of usages are asingle sense).
For example, only 6 of our 20 targetnouns have senses which are candidates for high-frequency novel senses.As before, we treat the novel sense identifica-tion task as a classification problem, although witha significantly different formulation: we are nolonger attempting to identify pre-existing senses,as novel senses are by definition not included inthe sense inventory.
Instead, we are seeking toidentify clusters of usages which are instances ofa novel sense, e.g.
for presentation to a lexicogra-pher as part of a dictionary update process (Run-dell and Kilgarriff, 2011; Cook et al, 2013).
Thatis, for each usage, we want to classify whether itis an instance of a given novel sense.A usage that corresponds to a novel senseshould have a topic that does not align well withany of the pre-existing senses in the sense inven-tory.
Based on this intuition, we introduce topic-to-sense affinity to estimate the similarity of atopic to the set of senses, as follows:ts-affinity(tj) =?Sisim(si, tj)?Tl?Sksim(sk, tl)(4)where, once again, sim(si, tj) is defined as inEquation (1), and T and S represent the numberof topics and senses, respectively.Using topic-to-sense affinity as the sole fea-ture, we pool together all instances and optimisethe affinity feature to classify instances that havenovel senses.
Evaluation is done by computing themean precision, recall and F-score across 10 sepa-rate runs; results are summarised in Table 7.
Notethat we evaluate only over UKWAC in this section,for ease of presentation.The results show that instances with high-frequency novel senses are more easily identifi-able than instances with medium/low-frequencynovel senses.
This is unsurprising given that high-frequency senses have a higher probability of gen-erating related topics (sense-related words are ob-served more frequently in the corpus), and as suchare more easily identifiable.We are interested in understanding whetherpooling all instances ?
instances from target lem-mas that have a sense artificially removed andthose that do not ?
impacted the results (re-call that not all target lemmas have a removedsense).
To that end, we chose to include onlyinstances from lemmas with a removed sense,and repeated the experiment for the medium- andhigh-frequency novel sense condition (for the low-frequency condition, all target lemmas have anovel sense).
In other words, we are assumingknowledge of which words have novel sense, andthe task is to identify specifically what the novelsense is, as represented by novel usages.
Resultsare presented in Table 8.266No.
of Lemmas with No.
of Lemmas without Relative Freq Wilcoxon Rank Suma Removed Sense a Removed Sense of Removed Sense p-value10 0 0.0?0.2 0.45439 11 0.2?0.4 0.03916 14 0.4?0.6 0.0247Table 9: Wilcoxon Rank Sum p-value results for testing target lemmas with removed sense vs. targetlemmas without removed sense using novelty.From the results, we see that the F-scores im-proved notably.
This reveals that an additional stepis necessary to determine whether a target lemmahas a potential novel sense before feeding its in-stances to learn which of them contains the usageof the novel sense.In the last experiment, we propose a new mea-sure to tackle this: the identification of target lem-mas that have a novel sense.
We introduce novelty,a measure of the likelihood of a target lemma whaving a novel sense:novelty(w) = mintj(maxsisim(si, tj)f(tj))(5)where f(tj) is the frequency of topic tjin thecorpus.
The intuition behind novelty is that atarget lemma with a novel sense should have a(somewhat-)frequent topic that has low associa-tion with any sense.
That we use the frequencyrather than the probability of the topic here is de-liberate, as topics with a higher raw number of oc-currences (whether as a low-probability topic fora high-frequency word, or a high-probability topicfor a low-frequency word) are indicative of a novelword sense.For each of our three datasets (with low-,medium- and high-frequency novel senses, respec-tively), we compute the novelty of the target lem-mas and the p-value of a one-tailed Wilcoxon ranksum test to test if the two groups of lemmas (i.e.lemmas with a novel sense vs. lemmas without anovel sense) are statistically different.15Resultsare presented in Table 9.
We see that the nov-elty measure can readily identify target lemmaswith high- and medium-frequency novel senses(p < 0.05), but the results are less promising forthe low-frequency novel senses.6 DiscussionOur methodologies for the two proposed tasks ofidentifying unused and novel senses are simple15Note that the number of words with low-frequency novelsenses here is restricted to 10 (cf.
20 in Table 7) to ensure wehave both positive and negative lemmas in the dataset.extensions to demonstrate the flexibility and ro-bustness of our methodology.
Future work couldpursue a more sophisticated methodology, usingnon-linear combinations of sim(si, tj) for com-puting the affinity measures or multiple featuresin a supervised context.
We contend, however,that these extensions are ultimately a preliminarydemonstration to the flexibility and robustness ofour methodology.A natural next step for this research would be tocouple sense distribution estimation and the detec-tion of unattested senses with evidence from thecontext, using topics or other information aboutthe local context (e.g.
Agirre and Soroa (2009))to carry out unsupervised WSD of individual tokenoccurrences of a given word.In summary, we have proposed a topicmodelling-based method for estimating wordsense distributions, based on Hierarchical Dirich-let Processes and the earlier work of Lau et al(2012) on word sense induction, in probabilisti-cally mapping the automatically-learned topics tosenses in a sense inventory.
We evaluated the abil-ity of the method to learn predominant senses andinduce word sense distributions, based on a broadrange of datasets and two separate sense invento-ries.
In doing so, we established that our methodis comparable to the approach of McCarthy et al(2007) at predominant sense learning, and supe-rior at inducing word sense distributions.
We fur-ther demonstrated the applicability of the methodto the novel tasks of detecting word senses whichare unattested in a corpus, and identifying novelsenses which are found in a corpus but not cap-tured in a word sense inventory.AcknowledgementsWe wish to thank the anonymous reviewers fortheir valuable comments.
This research was sup-ported in part by funding from the Australian Re-search Council.267ReferencesEneko Agirre and Philip Edmonds, editors.
2006.Word Sense Disambiguation: Algorithms and Appli-cations.
Springer, Dordrecht, Netherlands.Eneko Agirre and David Martinez.
2004.
Unsuper-vised WSD based on automatically retrieved exam-ples: The importance of bias.
In Proceedings ofEMNLP 2004, pages 25?32, Barcelona, Spain.Eneko Agirre and Aitor Soroa.
2007.
SemEval-2007task 02: Evaluating word sense induction and dis-crimination systems.
In Proceedings of the 4thInternational Workshop on Semantic Evaluations,pages 7?12, Prague, Czech Republic.Eneko Agirre and Aitor Soroa.
2009.
PersonalizingPageRank for word sense disambiguation.
In Pro-ceedings of the 12th Conference of the EACL (EACL2009), pages 33?41, Athens, Greece.Timothy Baldwin, Paul Cook, Marco Lui, AndrewMacKinlay, and Li Wang.
2013.
How noisy so-cial media text, how diffrnt social media sources?In Proceedings of the 6th International Joint Con-ference on Natural Language Processing (IJCNLP2013), pages 356?364, Nagoya, Japan.Satanjeev Banerjee and Ted Pedersen.
2002.
Anadapted Lesk algorithm for word sense disambigua-tion using WordNet.
In Proceedings of the 3rd In-ternational Conference on Intelligent Text Process-ing and Computational Linguistics (CICLing-2002),pages 136?145, Mexico City, Mexico.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet alocation.
Journal of Ma-chine Learning Research, 3:993?1022.Jordan Boyd-Graber and David Blei.
2007.
Putop:Turning predominant senses into a topic model forword sense disambiguation.
In Proc.
of the FourthInternational Workshop on Semantic Evaluations(SemEval-2007), pages 277?281, Prague, Czech Re-public.Jordan Boyd-Graber, David Blei, and Xiaojin Zhu.2007.
A topic model for word sense disambigua-tion.
In Proc.
of the 2007 Joint Conference onEmpirical Methods in Natural Language Process-ing and Computational Natural Language Learning(EMNLP-CoNLL), pages 1024?1033, Prague, CzechRepublic.Samuel Brody and Mirella Lapata.
2009.
Bayesianword sense induction.
In Proceedings of the 12thConference of the EACL (EACL 2009), pages 103?111, Athens, Greece.Jun Fu Cai, Wee Sun Lee, and Yee Whye Teh.2007.
NUS-ML: Improving word sense disam-biguation using topic features.
In Proc.
of the FourthInternational Workshop on Semantic Evaluations(SemEval-2007), pages 249?252, Prague, Czech Re-public.Marine Carpuat, Hal Daum?e III, Katharine Henry,Ann Irvine, Jagadeesh Jagarlamudi, and RachelRudinger.
2013.
SenseSpotting: Never let your par-allel data tie you to an old domain.
In Proc.
of the51st Annual Meeting of the Association for Compu-tational Linguistics (ACL 2013), pages 1435?1445,Sofia, Bulgaria.Yee Seng Chan and Hwee Tou Ng.
2005.
Wordsense disambiguation with distribution estimation.In Proc.
of the 19th International Joint Conferenceon Artificial Intelligence (IJCAI 2005), pages 1010?1015, Edinburgh, UK.Yee Seng Chan and Hwee Tou Ng.
2006.
Estimatingclass priors in domain adaptation for word sense dis-ambiguation.
In Proc.
of the 21st International Con-ference on Computational Linguistics and 44th An-nual Meeting of the Association for ComputationalLinguistics, pages 89?96, Sydney, Australia.Paul Cook and Suzanne Stevenson.
2010.
Automati-cally identifying changes in the semantic orientationof words.
In Proceedings of the 7th InternationalConference on Language Resources and Evaluation(LREC 2010), pages 28?34, Valletta, Malta.Paul Cook, Jey Han Lau, Michael Rundell, Diana Mc-Carthy, and Timothy Baldwin.
2013.
A lexico-graphic appraisal of an automatic approach for de-tecting new word senses.
In Proceedings of eLex2013, pages 49?65, Tallinn, Estonia.Katrin Erk.
2006.
Unknown word sense detection asoutlier detection.
In Proc.
of the Main Conferenceon Human Language Technology Conference of theNorth American Chapter of the Association of Com-putational Linguistics, pages 128?135, New YorkCity, USA.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press, Cambridge,USA.Adriano Ferraresi, Eros Zanchetta, Marco Baroni, andSilvia Bernardini.
2008.
Introducing and evaluatingukWaC, a very large web-derived corpus of English.In Proc.
of the 4th Web as Corpus Workshop: Canwe beat Google, pages 47?54, Marrakech, Morocco.Spandana Gella, Paul Cook, and Timothy Baldwin.
toappear.
One sense per tweeter ... and other lexicalsemantic tales of Twitter.
In Proceedings of the 14thConference of the EACL (EACL 2014), Gothenburg,Sweden.Kristina Gulordava and Marco Baroni.
2011.
A distri-butional similarity approach to the detection of se-mantic change in the Google Books Ngram corpus.In Proceedings of the GEMS 2011 Workshop on GE-ometrical Models of Natural Language Semantics,pages 67?71, Edinburgh, UK.Eduard Hovy, Mitchell Marcus, Martha Palmer,Lance Ramshaw, and Ralph Weischedel.
2006.OntoNotes: The 90% solution.
In Proceedings of268the Main Conference on Human Language Technol-ogy Conference of the North American Chapter ofthe Association of Computational Linguistics, pages57?60, New York City, USA.Ryu Iida, Diana McCarthy, and Rob Koeling.
2008.Gloss-based semantic similarity metrics for predom-inant sense acquisition.
In Proc.
of the Third In-ternational Joint Conference on Natural LanguageProcessing, pages 561?568.Jay Jiang and David Conrath.
1997.
Semantic similar-ity based on corpus statistics and lexical taxonomy.In Proceedings on International Conference on Re-search in Computational Linguistics, pages 19?33,Taipei, Taiwan.Peng Jin, Diana McCarthy, Rob Koeling, and John Car-roll.
2009.
Estimating and exploiting the entropyof sense distributions.
In Proceedings of the NorthAmerican Chapter of the Association for Computa-tional Linguistics ?
Human Language Technologies2009 (NAACL HLT 2009): Short Papers, pages 233?236, Boulder, USA.David Jurgens and Ioannis Klapaftis.
2013.
Semeval-2013 task 13: Word sense induction for graded andnon-graded senses.
In Proceedings of the 7th In-ternational Workshop on Semantic Evaluation (Se-mEval 2013), pages 290?299, Atlanta, USA.Adam Kilgarriff.
2004.
How dominant is the common-est sense of a word?
Technical Report ITRI-04-10,Information Technology Research Institute, Univer-sity of Brighton.Johannes Knopp, Johanna V?olker, and Simone PaoloPonzetto.
2013.
Topic modeling for word sense in-duction.
In Proc.
of the International Conference ofthe German Society for Computational Linguisticsand Language Technology, pages 97?103, Darm-stadt, Germany.Rob Koeling, Diana McCarthy, and John Carroll.2005.
Domain-specific sense distributions and pre-dominant sense acquisition.
In Proceedings of the2005 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP 2005), pages 419?426, Vancouver, Canada.Mirella Lapata and Chris Brew.
2004.
Verb classdisambiguation using informative priors.
Computa-tional Linguistics, 30(1):45?75.Jey Han Lau, Paul Cook, Diana McCarthy, David New-man, and Timothy Baldwin.
2012.
Word sense in-duction for novel sense detection.
In Proceedingsof the 13th Conference of the EACL (EACL 2012),pages 591?601, Avignon, France.Jey Han Lau, Paul Cook, and Timothy Baldwin.
2013a.unimelb: Topic modelling-based word sense induc-tion.
In Proceedings of the 7th International Work-shop on Semantic Evaluation (SemEval 2013), pages307?311, Atlanta, USA.Jey Han Lau, Paul Cook, and Timothy Baldwin.
2013b.unimelb: Topic modelling-based word sense induc-tion for web snippet clustering.
In Proceedings ofthe 7th International Workshop on Semantic Evalua-tion (SemEval 2013), pages 217?221, Atlanta, USA.Michael Lesk.
1986.
Automatic sense disambiguationusing machine readable dictionaries: How to tell apine cone from an ice cream cone.
In Proceedingsof the 1986 SIGDOC Conference, pages 24?26, On-tario, Canada.Linlin Li, Benjamin Roth, and Caroline Sporleder.2010.
Topic models for word sense disambiguationand token-based idiom detection.
In Proc.
of the48th Annual Meeting of the Association for Com-putational Linguistics, pages 1138?1147, Uppsala,Sweden.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of the 36th AnnualMeeting of the ACL and 17th International Confer-ence on Computational Linguistics (COLING/ACL-98), pages 768?774, Montreal, Canada.Marco Lui and Timothy Baldwin.
2012. langid.py: Anoff-the-shelf language identification tool.
In Pro-ceedings of the 50th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL 2012)Demo Session, pages 25?30, Jeju, Republic of Ko-rea.Suresh Manandhar, Ioannis Klapaftis, Dmitriy Dligach,and Sameer Pradhan.
2010.
SemEval-2010 Task14: Word sense induction & disambiguation.
InProceedings of the 5th International Workshop onSemantic Evaluation, pages 63?68, Uppsala, Swe-den.Diana McCarthy, Rob Koeling, Julie Weeds, and JohnCarroll.
2004a.
Automatic identification of infre-quent word senses.
In Proc.
of the 20th InternationalConference of Computational Linguistics, COLING-2004, pages 1220?1226, Geneva, Switzerland.Diana McCarthy, Rob Koeling, Julie Weeds, and JohnCarroll.
2004b.
Finding predominant senses inuntagged text.
In Proceedings of the 42nd An-nual Meeting of the Association for ComputationalLinguistics (ACL 2004), pages 280?287, Barcelona,Spain.Diana McCarthy, Rob Koeling, Julie Weeds, and JohnCarroll.
2007.
Unsupervised acquisition of pre-dominant word senses.
Computational Linguistics,4(33):553?590.George A. Miller, Claudia Leacock, Randee Tengi, andRoss T. Bunker.
1993.
A semantic concordance.
InProc.
of the ARPA Workshop on Human LanguageTechnology, pages 303?308.Guido Minnen, John Carroll, and Darren Pearce.
2001.Applied morphological processing of English.
Nat-ural Language Engineering, 7(3):207?223.269Saif Mohammad and Graeme Hirst.
2006.
Determin-ing word sense dominance using a thesaurus.
InProc.
of EACL-2006, pages 121?128, Trento, Italy.Roberto Navigli and Daniele Vannella.
2013.SemEval-2013 task 11: Word sense induction anddisambiguation within an end-user application.
InProceedings of the 7th International Workshop onSemantic Evaluation (SemEval 2013), pages 193?201, Atlanta, USA.Roberto Navigli and Paola Velardi.
2005.
Structuralsemantic interconnections: a knowledge-based ap-proach to word sense disambiguation.
IEEE Trans-actions on Pattern Analysis and Machine Intelli-gence, 27(7):1075?1088.Roberto Navigli, Kenneth C. Litkowski, and Orin Har-graves.
2007.
SemEval-2007 task 07: Coarse-grained English all-words task.
In Proceedings ofthe 4th International Workshop on Semantic Evalu-ations, pages 30?35, Prague, Czech Republic.Roberto Navigli.
2009.
Word sense disambiguation: Asurvey.
ACM Computing Surveys, 41(2).Olutobi Owoputi, Brendan O?Connor, Chris Dyer,Kevin Gimpel, and Nathan Schneider.
2012.
Part-of-speech tagging for Twitter: Word clusters andother advances.
Technical Report CMU-ML-12-107, Machine Learning Department, Carnegie Mel-lon University.Martha Palmer, Olga Babko-Malaya, and Hoa TrangDang.
2004.
Different sense granularities for differ-ent applications.
In Proceedings of the HLT-NAACL2004 Workshop: 2nd Workshop on Scalable Natu-ral Language Understanding, pages 49?56, Boston,USA.Yves Peirsman, Dirk Geeraerts, and Dirk Speelman.2010.
The automatic identification of lexical varia-tion between language varieties.
Natural LanguageEngineering, 16(4):469?491.Judita Preiss and Mark Stevenson.
2013.
Unsuper-vised domain tuning to improve word sense dis-ambiguation.
In Proc.
of the 2013 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 680?684, Atlanta, USA.Michael Rundell and Adam Kilgarriff.
2011.
Au-tomating the creation of dictionaries: where willit all end?
In Fanny Meunier, Sylvie DeCock, Ga?etanelle Gilquin, and Magali Paquot, ed-itors, A Taste for Corpora.
In honour of SylvianeGranger, pages 257?282.
John Benjamins, Amster-dam, Netherlands.Eyal Sagi, Stefan Kaufmann, and Brady Clark.
2009.Semantic density analysis: Comparing word mean-ing across time and space.
In Proceedings ofthe EACL 2009 Workshop on GEMS: GEometricalModels of Natural Language Semantics, pages 104?111, Athens, Greece.Yee Whye Teh, Michael I. Jordan, Matthew J. Beal,and David M. Blei.
2006.
Hierarchical Dirichletprocesses.
Journal of the American Statistical Asso-ciation, 101:1566?1581.Xuchen Yao and Benjamin Van Durme.
2011.
Non-parametric Bayesian word sense induction.
In Pro-ceedings of TextGraphs-6: Graph-based Methodsfor Natural Language Processing, pages 10?14,Portland, USA.Zhi Zhong and Hwee Tou Ng.
2010.
It makes sense:A wide-coverage word sense disambiguation sys-tem for free text.
In Proc.
of the ACL 2010 SystemDemonstrations, pages 78?83, Uppsala, Sweden.270
