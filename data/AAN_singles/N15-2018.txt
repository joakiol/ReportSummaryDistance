Proceedings of NAACL-HLT 2015 Student Research Workshop (SRW), pages 133?139,Denver, Colorado, June 1, 2015.c?2015 Association for Computational LinguisticsLifelong Machine Learning for Topic Modeling and BeyondZhiyuan ChenDepartment of Computer ScienceUniversity of Illinois at Chicagoczyuanacm@gmail.comAbstractMachine learning has been popularly used innumerous natural language processing tasks.However, most machine learning models arebuilt using a single dataset.
This is often re-ferred to as one-shot learning.
Although thisone-shot learning paradigm is very useful, itwill never make an NLP system understandthe natural language because it does not ac-cumulate knowledge learned in the past andmake use of the knowledge in future learn-ing and problem solving.
In this thesis pro-posal, I first present a survey of lifelong ma-chine learning (LML).
I then narrow down toone specific NLP task, i.e., topic modeling.
Ipropose several approaches to apply lifelonglearning idea in topic modeling.
Such capabil-ity is essential to make an NLP system versa-tile and holistic.1 IntroductionMachine learning serves as a prevalent approachfor research in many natural language processingtasks.
However, most of existing machine learningapproaches are built using a single dataset, whichis often referred to as one-shot learning.
This kindof one-shot approach is useful but it does not usu-ally perform well to various datasets or tasks.
Themain shortcoming of such one-short approach is thelack of continuous learning ability, i.e., learning andaccumulating knowledge from past tasks and lever-aging the knowledge for future tasks and problemsolving in a lifelong manner.To overcome the above shortcoming, lifelong ma-chine learning (LML) has attracted researchers?attention.
The term was initially introduced in1990s (Thrun, 1995, Caruana, 1997).
LML aims todesign and develop computational systems and algo-rithms that learn as humans do, i.e., retaining the re-sults learned in the past, abstracting knowledge fromthem, and using the knowledge to help future learn-ing.
The motivation is that when faced with a newsituation, we humans always use our previous expe-rience and learned knowledge to help deal with andlearn from the new situation, i.e., we learn and ac-cumulate knowledge continuously.
The same ratio-nale can be applied to computational models.
Whena model is built using a single dataset for a task,its performance is limited.
However, if the modelsees more datasets from the same or similar tasks,it should be able to adjust its learning algorithm forbetter performance.
There are four components in aLML framework: knowledge representation, knowl-edge extraction, knowledge transfer, and knowledgeretention and maintenance.
These components areclosely connected.
I will illustrate each componentusing examples from topic modeling in Section 3.Compared to the significant progress of machinelearning theory and algorithm, there is relativelylittle study on lifelong machine learning.
One ofthe most notable works is Never-Ending LanguageLearner (NELL) (Carlson et al, 2010) which wasproposed to extract or read information from theweb to expand the knowledge base in an endlessmanner, aiming to achieve better performance ineach day than the previous day.
Recently, we pro-posed lifelong Topic Modeling (LTM) that extractsknowledge from topic modeling results of many do-mains and utilizes the knowledge to generate co-herent topics in the new domains (Chen and Liu,2014b).
In (Ruvolo and Eaton, 2013), the authorsproposed a method that tackles online multi-tasklearning in the lifelong learning setting.
Some otherLML related works include (Silver, 2013, Raina etal., 2007, Pentina and Lampert, 2014, Kamar et al,2013, Kapoor and Horvitz, 2009).
Note that LML isdifferent from transfer learning which usually con-siders one single source domain where the knowl-edge is coming from and one target domain wherethe knowledge is applied on (Pan and Yang, 2010).133In this thesis proposal, I narrow down the scopeand focus on LML in topic modeling.
Topic model-ing has been successfully applied to extract semantictopics from text data.
However, the majority of ex-isting topic models (one exception is the LTM modelmentioned before) belong to the one-shot approach,i.e., they are proposed to address a specific problemwithout any knowledge accumulation.
To leveragethe idea of LML, I propose several new approachesto advance topic modeling.
I believe that the pro-posed approaches can significantly advance LML intopic modeling.
More broadly, this thesis proposalaims to encourage the community to apply LML ina variety of NLP tasks.This thesis proposal makes the following threecontributions:1.
It studies and discusses lifelong machine learn-ing (LML) in natural language processing.
Itidentifies several important components in LML:knowledge representation, knowledge extraction,knowledge transfer, knowledge retention andmaintenance.
As there is relatively little studyon LML compared to classic machine learning,I believe this thesis proposal will shed some lighton the area and encourage the NLP community toadvance the area of LML.2.
It reviews the LTM model and discusses themodel in terms of LML components.
In eachcomponent, the model mechanism as well as theshortcomings are discussed.3.
It proposes several new approaches to improveLML in the context of topic modeling.
It pro-poses to enrich the knowledge representation, ad-dress knowledge conflicts, select domains andmake the algorithm scalable.
It further proposesnew evaluation frameworks for LTM.2 Background of Topic ModelingTopic modeling, such as LDA (Blei et al, 2003) andpLSA (Hofmann, 1999), have been popularly usedin many NLP tasks such as opinion mining (Chenet al, 2014), machine translation (Eidelman et al,2012), word sense disambiguation (Boyd-Graber etal., 2007), phrase extraction (Fei et al, 2014) andinformation retrieval (Wei and Croft, 2006).
In gen-eral, topic models assume that each document isa multinomial distribution over topics, where eachtopic is a multinomial distribution over words.
Thetwo types of distributions in topic modeling aredocument-topic distributions and topic-word distri-butions respectively.
The intuition is that words aremore or less likely to be present given the topics ofa document.
For example, ?sport?
and ?player?
willappear more often in documents about sports, ?rain?and ?cloud?
will appear more frequently in docu-ments about weather.My work is mainly related to knowledge-basedtopic models (Chen and Liu, 2014a, Andrzejewski etal., 2009) which incorporate different types of priorknowledge into topic models.
Supervised label in-formation was considered in (Blei and McAuliffe,2010, Ramage et al, 2009).
Some works also en-able the user to specify prior knowledge as seedwords/terms for some topics (Mukherjee and Liu,2012).
Interactive topic modeling was proposedin (Hu et al, 2011) to improve topics with the in-teractive help from the user.
However, these worksrequire labeled data or user manual guidance whilemy proposed approaches do not.3 Lifelong Topic ModelingThis section introduces the LTM model (Chen andLiu, 2014b).
It first presents the overall algorithmof LTM.
Then it reviews the model using the fourcomponents in the LML framework: knowledge rep-resentation, knowledge extraction, knowledge trans-fer, and knowledge retention and maintenance.3.1 Overall AlgorithmThe basic idea of LTM is that it extracts knowl-edge from the topic results obtained by topic modelsin the previous domains or tasks.
The knowledgeshould reflect the correct semantic relationship byinvestigating different topic model results.
By ex-ploiting such knowledge, the LTM model can gener-ate more coherent topics.
It consists of 3 main steps:1.
Given a set of document corpora D ={D1, .
.
.
, Dn} from n domains, LTM runs a topicmodel (e.g., LDA) on each Di?
D to produce aset of topics Si.
Such topics are called the priortopics (or p-topics for short), forming the topicbase in LTM.2.
A set of pk-sets (prior knowledge sets) K aremined from all the p-topics S = ?iSiin the topic134base.
The knowledge base in LTM is composedof such pk-sets.3.
The knowledge, i.e., pk-sets K, is used in LTMto generate topics for a test document collectionDt(Dtmay or may not be from D).3.2 Knowledge RepresentationThe prior knowledge set (pk-sets)K for LTM is rep-resented by must-links, i.e., if a pair of words forma must-link, they are more likely to belong to thesame topic.
For example, words ?price?
and ?expen-sive?
can form a must-link.
Such knowledge rep-resentation is also used in other topic models suchas (Andrzejewski et al, 2009).
However, they didnot model in the lifelong setting.
The must-links in-dicate a positive semantic relationship while someother existing models (Chen and Liu, 2014a, An-drzejewski et al, 2009) also used the negative re-lationship called cannot-links.
Cannot-links expressthat two words do not share the semantic meaning,e.g., words ?price?
and ?beauty?.
Note that for topicmodeling, semantics related knowledge is mostlybeneficial as topic modeling tries to group wordsinto topics with different semantics.3.3 Knowledge ExtractionTo extract pk-sets from all the prior topics (Step 2in Section 3.1, LTM utilizes frequent itemset min-ing (FIM) (Agrawal and Srikant, 1994).
The goalof FIM is to identify all itemsets (an itemset is a setof items) that satisfy some user-specified frequencythreshold (also called minimum support) in a set oftransactions.
The identified itemsets are called fre-quent itemsets.
In the context of LTM, an item is aword and an itemset is a set of words.
Each transac-tion consists of the top words in a past topic.
Notethat top words ranked by the topic-word distributionfrom topic modeling are more likely to represent thetrue semantics embedded in the latent topic.
The fre-quent itemsets of length 2 are used as pk-sets.
Therationale for using frequency-based approach is thata piece of knowledge is more reliable when it ap-pears frequent in the prior topics.3.4 Knowledge TransferFor topic modeling, Gibbs sampling is a popularinference technique (Griffiths and Steyvers, 2004).The Gibbs sampler for LDA corresponds to the sim-ple P?olya urn (SPU) model (Mimno et al, 2011).In SPU, a ball of a color (each color denotes eachword) is randomly drawn from an urn (each urn cor-responds to each topic) and then two balls of thesame color are put back into the urn.
It increasesthe probability of seeing a ball of the drawn color inthe future, which is known as ?the rich get richer?.LTM instead uses the generalized P?olya urn(GPU) model (Mahmoud, 2008).
The difference isthat after sampling the ball of a certain color, twoballs of that color are put back along with a certainnumber of balls of some other colors.
This flexibilityis able to change the probability of multiple colors ineach sampling step.
Based on the GPU model, LTMincreases the probabilities of both words in a pk-set when seeing either of them.
For example, giventhe pk-set {price, expensive}, seeing word ?price?under topic t will increase the probability of see-ing word ?expensive?
under topic t; and vice versa.In other words, word ?price?
promotes word ?ex-pensive?
under topic t. The extent of promotion ofwords is determined by the promotion scale param-eter ?.
This mechanism can transfer the informationfrom the knowledge to the topics generated by LTM.Since the knowledge is automatically extracted,to ensure the knowledge quality, LTM proposes twoadditional mechanisms.
First, for each topic in thecurrent domain, it uses KL-Divergence to find thematched topics from the topic base.
Note that intopic modeling, a topic is a distribution over words.In addition, LTM proposes to use Pointwise MutualInformation (PMI) to estimate the correctness of theknowledge towards the current task/domain.
The in-tuition is that if a piece of knowledge, i.e., must-link,is appropriate, both words in the must-link shouldhave reasonable occurrences in the corpus of the cur-rent domain, which means the PMI value of bothwords is positive.
On the other hand, a non-positivePMI value indicates little or no semantic correlation,and thus making the knowledge unreliable.3.5 Knowledge Retention and MaintenanceLTM simply retains knowledge by adding the topicsof a new domain into the topic base which containsall prior topics (Step 1 in Section 3.1).
Then, theknowledge is extracted from the new topic base byusing FIM mentioned in Section 3.3.
There is no135knowledge maintenance.4 Shortcomings of LTMThis section presents the shortcomings of LTM thatcorresponds to each of the four LML components.4.1 Knowledge RepresentationThere are two shortcomings in terms of knowledgerepresentations in LTM:1.
Since must-links only contain two words, the in-formation contained is limited.
The knowledge inthe form of sets (containing multiple words) maybe more informative.2.
The knowledge does not have a confidence value.The prior knowledge is represented and treatedequally.
Due to the different frequency of eachpiece of knowledge (i.e., each pk-set), thereshould be an additional value indicating confi-dence attached to each pk-set.4.2 Knowledge ExtractionKnowledge extraction in LTM also has two mainshortcomings:1.
The frequent itemset mining (FIM) used in LTMonly extracts frequent itemsets that appear morethan a uniformed support threshold.
However,due to the power law distribution of natural lan-guage (Zipf, 1932), only a small portion of wordsin the vocabulary appears very frequently whilethe majority of words are relatively rare.
Sincethe frequencies of words are different, the cor-responding support threshold should also be dis-tinct.2.
FIM cannot easily produce knowledge of richerforms.
For example, as mentioned above, eachpiece of knowledge should contain an additionalvalue, e.g., confidence.
It is unclear how FIMgenerates such value, especially if the valueneeds to be a probability.4.3 Knowledge TransferThe shortcoming here is that depending on the pro-motion scale parameter ?
set by the user (Sec-tion 3.4), the GPU model may over-promote orunder-promote the words in the pk-sets.
That meansthat if the promotion scale parameter ?
is set too low,the knowledge may not influence the topics much.
Incontrast, if this parameter is set too high, the wordsin the knowledge may dominate the topics resultinginscrutable topics.
So the manual setting of this pa-rameter requires expertise from the user.4.4 Knowledge Retention and MaintenanceSince LTM does not focus on this component, it hasthree main issues:1.
It is unclear how to retrieve knowledge efficientlywhen the number of prior topics is huge.
Thisissue is ignored in the LTM model.2.
How a user interacts with the knowledge base(i.e., pk-sets) to improve the quality of knowl-edge base is also unknown.
Since the knowledgeis automatically extracted in LTM, the assistancefrom human beings should contribute to improv-ing the quality of the knowledge base.3.
If the time factor is considered, the new addedtopics in the topic base may better representemerging topics while old prior topics may notfit the new tendency anymore.
In that case, theknowledge base should weight the new topicsmore than old topics.5 Proposed ApproachesThe previous section pointed out the shortcomingsof LTM.
In this section, I propose several approachesto address some of them.
Additional strategies areproposed to deal with issues beyond the knowledgecomponents.5.1 Expanding Knowledge BaseAs mentioned above, each piece of knowledge in theknowledge base (i.e., pk-set) is stored and treatedequally.
However, a piece of knowledge may bemore reliable if it gets supports from a large num-ber of domains or it is extracted from the domainsor data of higher quality with less noise.
In suchcase, it is more informative to assign a value to theknowledge to indicate its confidence.
I propose toadd this additional value to each piece of knowl-edge in the knowledge base.
The value is obtainedfrom the normalized support of the knowledge, i.e.,the normalized frequency of the knowledge in mul-tiple domains.
This expansion can also benefit theknowledge estimation part because the confidencefield can provide the prior information to the modelfor knowledge filtering and estimation.136Another useful expansion is to consider cannot-links with confidence value (Chen and Liu, 2014a,Andrzejewski et al, 2009).
Cannot-links expressthe negative semantic relationship between words,which can lead the model to separate them in differ-ent topics.
Same as for must-links, cannot-links canalso be attached with a confidence value, indicatingits prior reliability.5.2 Knowledge ConflictAfter expanding the knowledge base, knowledge re-tention and maintenance needs additional attention.As we know, must-links express positive semanticcorrelations while cannot-links express the negativecorrelations, which means must-links and cannot-links are completely exclusive.
Apparently, twowords can form a must-link or cannot-link, but notboth.
The extracted knowledge can contain noisedue to 3 reasons below:1.
The corpora which topic models are built on con-tain noise.
This becomes a more serious problemif the corpora are coming from social media withinformal languages.2.
Topic modeling is an unsupervised learningmethod and thus it can generate illogical top-ics containing words without any semantic cor-relation.
Such topics will then produce incorrectknowledge.3.
The knowledge extraction step is not perfect ei-ther.
The knowledge extracted using frequency-based FIM approach may include noisy must-links as some words are very frequent that theirpairs can also pass the support threshold and formmust-links.The noise in knowledge base means that thenewly extracted knowledge may have conflict withthe ones in knowledge base.
For example, theknowledge base contains the must-link {A, B}.However, the new knowledge contains cannot-link{A, B}.
In such a case, we should not simply mergesuch knowledge into the knowledge base as it willmake the knowledge base nonsensical.
It requires usto propose a new strategy when such conflict hap-pens.
I propose two approaches to deal with theabove situations:1.
Leverage the confidence assigned to each pieceof knowledge.
Intuitively, when a must-link and acannot-link forms a conflict, the knowledge baseshould remain the type of knowledge (must-linkor cannot-link) if its confidence is significantlyhigher than the conflicted one.
By doing so,I make sure that the knowledge base does notcontain conflicted knowledge and the knowledgepiece in the knowledge base has the highest con-fidence among its conflicted ones.2.
If the confidence is same or similar between twotypes of knowledge having conflicts, I use thewords that share must-links to make the decision.Let us say the must-link is {A, B}, I denote theset of words in which each word shares a must-link with A (or B) as SA(or SB).
Then I use theoverlapping percentage of SAand SBas estima-tion that how likely wordsA andB share the pos-itive semantic correlation.
This is intuitive sinceif words A and B are truly semantically corre-lated, they should share a lot of words in theirmust-links.
For instance, words ?price?
and ?ex-pensive?
can form must-links with words such as?cheap?, ?cost?, ?pricy?, etc.5.3 Domain SelectionI also notice an important issue that LTM strug-gles with, i.e., LTM uses all the domains as thesource from which the knowledge is extracted.
Inother words, LTM assumes all the domains are rel-evant and helpful to the current domain.
However,this assumption may not always hold.
For example,the topics from the domain ?Politics?
may not con-tribute much to the domain ?Laundry?
as they arevery different in terms of both word usage and wordsemantics.
Simply using all the domains as LTM hastwo major drawbacks:1.
The knowledge extracted from all the domainsmay contain some inappropriate knowledge to-wards a particular domain.
Although LTM has amechanism to estimate and filter knowledge, it isstill not perfect.
For a more effective knowledgetransfer, a domain selection step is indispensableto make sure the knowledge is more relevant andbeneficial.2.
Extracting knowledge from all the domains canbe time-consuming given a huge number of do-mains.
Many of the extracted knowledge is use-less as a particular domain only contains a lim-ited set of words.
So domain selection can alsoimprove the knowledge extraction efficiency.137To select domains, I propose to measure the do-main distance by utilizing JS-Divergence.
Giventwo distributions P and Q, JS-Divergence betweenthem is defined as below:JS(P,Q) =12KL(P,M) +12KL(Q,M) (1)M =12(P +Q) (2)KL(P,Q) =?iln(P (i)Q(i))P (i) (3)Since each topic produced by topic models is adistribution over words, I can use JS-Divergence tomeasure the distance between topics.
The problemis defined as given two domains D1and D2, thegoal is to estimate the domain distance by estimat-ing their corresponding topic distance.
I propose thefollowing algorithm: for each topic t in domain D1,I find the most similar topic (say t?)
in domain D2that has the smallest JS-Divergence with t. I denotethis smallest JS-Divergence by e(t).
Then, the dis-tance between domainD1and domainD2is definedas below:DIST (D1, D2) =?t?D1e(t) +?t??D2e(t?)
(4)Note that to make the distance symmetric, I cal-culate function e() for each topic in domain D1aswell as domain D2.
After the domain distance iscalculated, given a new domain D?, I can rank allexisting domains by Equation 4 and pick up top Kmost relevant domains.5.4 ScalabilityIn this sub-section, I also consider the scalability is-sue.
There are generally 2 bottlenecks in LTM.The first one is frequent itemset mining (FIM).There are some proposed scalable versions of FIMsuch as (Chester et al, 2009, Moens et al, 2013).The second one is Gibbs sampling in topic mod-els.
Gibbs sampling (Griffiths and Steyvers, 2004)is a popular inference technique for topic model-ing.
However, it is not scalable to large datasetsas it needs to make pass over the corpus manytimes.
Some promising frameworks have been pro-posed (Yao et al, 2009, Zhai et al, 2012, Hu et al,2014) to solve this issue.
Since the GPU model usedin LTM is a natural extension to that in LDA, theseproposed methods are also applicable to LTM.6 EvaluationThis section proposes a new evaluation frameworkthat suits our proposed approaches.
In (Chen andLiu, 2014b), the evaluation measurements are TopicCoherence (Mimno et al, 2011) and Precision@nwhich asks annotators to label both topics andwords.
A more comprehensive evaluation frame-work can contain the following two measurements:1.
Knowledge Evaluation.
In order to evaluate eachpiece of knowledge (must-link or cannot-link) inthe knowledge base, PMI score of both words us-ing a large standard text corpus (Newman et al,2010) can be applied.
Human annotation can alsobe used to label the correctness of each piece ofknowledge.
This is to evaluate the effectivenessof knowledge handling in the model.2.
Domain Evaluation.
As mentioned in 5.3, notall the prior domains are suitable to a new do-main.
It is important to evaluate the model per-formance by providing different sets of prior do-mains.
There could be three main sets of priordomains for an extensive evaluation: 1) all rele-vant; 2) all irrelevant; 3) a combination of both.The relevance of domains should be defined byexperts that are familiar with these domains.7 ConclusionsThis thesis proposal studied lifelong machine learn-ing in topic modeling.
It first introduced lifelongmachine learning and its important components.Then, it reviewed the LTM model and pointed outits drawbacks.
The corresponding approaches wereproposed to address the issues and further advancethe problem.
For future direction, I would like to fur-ther integrate lifelong machine learning in the con-text of other NLP tasks, such as word sense disam-biguation.
I believe that the lifelong machine learn-ing capacity is essential to a robust NLP system toovercome the dynamics and complexity of naturallanguage, and for the purpose of a deeper under-standing of natural language.AcknowledgmentsThis work was supported in part by grants from Na-tional Science Foundation (NSF) under grant no.IIS-1111092 and IIS-1407927.138ReferencesRakesh Agrawal and Ramakrishnan Srikant.
1994.
FastAlgorithms for Mining Association Rules.
In VLDB,pages 487?499.David Andrzejewski, Xiaojin Zhu, and Mark Craven.2009.
Incorporating Domain Knowledge into TopicModeling via Dirichlet Forest Priors.
In ICML, pages25?32.David M. Blei and Jon D. McAuliffe.
2010.
SupervisedTopic Models.
In NIPS, pages 121?128.David M Blei, Andrew Y Ng, and Michael I Jordan.2003.
Latent Dirichlet Allocation.
Journal of Ma-chine Learning Research, 3:993?1022.Jordan L Boyd-Graber, David M. Blei, and Xiaojin Zhu.2007.
A Topic Model for Word Sense Disambigua-tion.
In EMNLP-CoNLL, pages 1024?1033.Andrew Carlson, Justin Betteridge, and Bryan Kisiel.2010.
Toward an Architecture for Never-Ending Lan-guage Learning.
In AAAI, pages 1306?1313.Rich Caruana.
1997.
Multitask Learning.
Machinelearning, 28(1):41?75.Zhiyuan Chen and Bing Liu.
2014a.
Mining Topics inDocuments : Standing on the Shoulders of Big Data.In KDD, pages 1116?1125.Zhiyuan Chen and Bing Liu.
2014b.
Topic Modelingusing Topics from Many Domains, Lifelong Learningand Big Data.
In ICML, pages 703?711.Zhiyuan Chen, Arjun Mukherjee, and Bing Liu.
2014.Aspect Extraction with Automated Prior KnowledgeLearning.
In ACL, pages 347?358.Sean Chester, Ian Sandler, and Alex Thomo.
2009.
Scal-able apriori-based frequent pattern discovery.
In CSE,pages 48?55.Vladimir Eidelman, Jordan Boyd-Graber, and PhilipResnik.
2012.
Topic Models for Dynamic TranslationModel Adaptation.
In ACL, pages 115?119.Geli Fei, Zhiyuan Chen, and Bing Liu.
2014.
ReviewTopic Discovery with Phrases using the P?olya UrnModel.
In COLING, pages 667?676.Thomas L Griffiths and Mark Steyvers.
2004.
Findingscientific topics.
PNAS, 101 Suppl:5228?5235.Thomas Hofmann.
1999.
Probabilistic Latent SemanticAnalysis.
In UAI, pages 289?296.Yuening Hu, Jordan Boyd-Graber, and Brianna Satinoff.2011.
Interactive Topic Modeling.
In ACL, pages248?257.Yuening Hu, Ke Zhai, Vladimir Edelman, and JordanBoyd-Graber.
2014.
Polylingual Tree-Based TopicModels for Translation Domain Adaptation.
In ACL,pages 1166?1176.Ece Kamar, Ashish Kapoor, and Eric Horvitz.
2013.Lifelong Learning for Acquiring the Wisdom of theCrowd.
In IJCAI, pages 2313?2320.Ashish Kapoor and Eric Horvitz.
2009.
Principles oflifelong learning for predictive user modeling.
In UserModeling, pages 37?46.Hosam Mahmoud.
2008.
Polya Urn Models.
Chapman& Hall/CRC Texts in Statistical Science.David Mimno, Hanna M. Wallach, Edmund Talley,Miriam Leenders, and Andrew McCallum.
2011.Optimizing semantic coherence in topic models.
InEMNLP, pages 262?272.Sandy Moens, Emin Aksehirli, and Bart Goethals.
2013.Frequent Itemset Mining for Big Data.
In IEEE Inter-national Conference on Big Data, pages 111?118.Arjun Mukherjee and Bing Liu.
2012.
Aspect Extractionthrough Semi-Supervised Modeling.
In ACL, pages339?348.David Newman, Jey Han Lau, Karl Grieser, and TimothyBaldwin.
2010.
Automatic evaluation of topic coher-ence.
In HLT-NAACL, pages 100?108.Sinno Jialin Pan and Qiang Yang.
2010.
A Survey onTransfer Learning.
IEEE Trans.
Knowl.
Data Eng.,22(10):1345?1359.Anastasia Pentina and Christoph H Lampert.
2014.A PAC-Bayesian Bound for Lifelong Learning.
InICML, pages 991?999.Rajat Raina, Alexis Battle, Honglak Lee, BenjaminPacker, and Andrew Y Ng.
2007.
Self-taught Learn-ing : Transfer Learning from Unlabeled Data.
InICML, pages 759?766.Daniel Ramage, David Hall, Ramesh Nallapati, andChristopher D Manning.
2009.
Labeled LDA : Asupervised topic model for credit attribution in multi-labeled corpora.
In EMNLP, pages 248?256.Paul Ruvolo and Eric Eaton.
2013.
ELLA: An efficientlifelong learning algorithm.
In ICML, pages 507?515.Daniel L Silver.
2013.
On Common Ground: Neural-Symbolic Integration and Lifelong Machine Learning.In 9th International Workshop on Neural-SymbolicLearning and Reasoning NeSy13, pages 41?46.Sebastian Thrun.
1995.
Lifelong Learning: A CaseStudy.
Technical report.Xing Wei and W Bruce Croft.
2006.
LDA-based doc-ument models for ad-hoc retrieval.
In SIGIR, pages178?185.Limin Yao, David Mimno, and Andrew McCallum.2009.
Efficient methods for topic model inference onstreaming document collections.
In KDD, pages 937?946.Ke Zhai, Jordan Boyd-Graber, Nima Asadi, and Mo-hamad L. Alkhouja.
2012.
Mr. LDA: a Flexible LargeScale Topic Modeling Package using Variational Infer-ence in MapReduce.
In WWW, pages 879?888.George Kingsley Zipf.
1932.
Selected Papers of thePrinciple of Relative Frequency in Language.
HarvardUniversity Press.139
