A New Quantitative Quality Measurefor Machine 'lYanslation SystemsKeh-Yih Su t, Ming-Wen Wu ~" and J ing -Sh in  Chang t~Department  of  Electr ical  Eng ineer ingNat ional  Ts ing-Hua Univers i tyHs inchu,  Ta iwan 300, R.O.C.emai l :kysu@ee.nthu .edu .
tw,  sh in@ee.nthu .edu.
twABSTRACTIn this paper, an objective qtumtitative quality mea-sure is proposed to evaluate tile performance of machiuetranslation systems.
The proposed method is to comparethe raw translation output of an MT system with the fi-nal revised version lor the customers, and then computethe editing efforts required to convert he raw translationto the final version.
In contrast o the other prolx)sals,the evaluatiral process can he (lone quickly and auto-matically.
Itence, it can provide a quick response onany system change.
A system designer can thus quicklylind the advantages or faults of a particular performance-improving strategy aml improve system performance dy-namieally.
Application of such a measure to improvethe system performance on-line on a parameterized andfeedback-controlled system will be demonstrated.
Fur-thermore, because the revised versiou is used directlyas a reference, tile perfoInunice lneasnre can reflect tilereal quality gap between the system performance andcustomer expectation.
A system designer can thus con-centrate on practically impo~ult opics rather than mltheoretically interesting issues.1.
IntroductionThere are several reasons performance measure isrequired while building machine translation systems.
(1).Potential customers need to be able to compare the per-formance of different systems.
(2).
System designerswould like to keep abreast of the current system perfor-mauce, and make sure the system keeps on improving,not subject o tbeflip-flop problem.
(The flip-flop prob-lem is caused when system designers try to fix sooreproblem of the system without a thorough test.
Whilethat problem is solved, other problems may pop up.
Thiskind of problem becomes more serious when the systemscales up.)
(3).
The measure feedback will highlight cor-rect research direction.
(4).
In a parameterized system,:~Behavior Des ign Corporat ion2F, 28, R&D Road I1Sc ience-based Industr ial  Parkl l s inchu,  "l~tiwan 3(10, R.O.C.if a quantified cost function is provided, it can be useddirectly for parameter tuning.
Thus.
a systematic andstandardized approach for performance evaluation andthe establishment of a common l~3Stillg I~lse are urgeudyrequired.Most conventional pproaches evaluate system lx~r-fonnance by human inspection and subjective measures.While post-editiug, the post-editors ean provide I~_xlbackon the quality of machine translafioo, which then is usedfor dictionary update and linguistic analyses of errors\[Ross 82\].
Also, feedback can be obtained from pro-fessional translators who annotate carefully on the print-outs of raw translation output \[Pigo 82\].
From humanfeedback, system designers tend to overtune the system.Another approach, plvposed in \[King 901, is Ix) collectthe test suites and divide them into two sections: one tolook at source language coverage, the other to extanlaetranslatioual problems.
Such an approach can avuid theover-taning problem caused by hnmau feedback.
The ad-vantage of human inspection is tbat humans can tlinl)ointthe real linguistic pp.thlenls and make corrections, tlow-ever, there are several disadvantages: (1).
It is too costlyfor human inslw.ction of the translation output quality.
Toget significant statistics on the real system performance,a large volume of text must be provided.
The cost forhuman inspection is thus extremely high.
(2).
It will taketoo Ioug for the results to come out.
For this reason andthe cost consideration, it can uot be repeated frequently.Therefore, it can not provide a quick suggestion to a sys-tem designer when the system is changed or when thedomain is 'alerted.
For a system that must handle a widevariety of types of text, it fails IX) provide immediate helpto adapt o the particular domain or field.
(3).
It is noteasy to achieve consistency and objectiveness.
Eveu forthe same person, it is very likely that he/she would judgea translation result differently at different time, especiallywhen the evaluation criteria are loosely defined.Based on the above problems with human inspec-tion, some automatic approaches were proposed to eval-AcrEs DE COLING-92, NAI~S.
23-28 AO~r 1992 4 3 3 PREC.
OF COLING-92, NAICrUS, AUG. 23-2g, 1992uate translation output quality.
In \[Yu 91\], for example,a corpus of 3,200 sentences were collected.
Then, sometest points are selected by linguists based on the sen-tences in the corpus.
The test points are what linguiststhink the most impo~mt features for the sentences in thecorpus.
Each test point is assigned a weight according toits importance in translation.
The test points are codedin programs, therefore the testing can be done automati-cally.
The advantage of this approach is that since theircriteria are purely linguistic, they can do a very delicateevaluation and find the real linguistics problems involved.However, to acquire significant statistics on the perfor-mance, a large corpus is required.
Corpus collecting andtest points selecting are very time-consuming.
Further-more, to achieve high grade in quality with respect othese test points, the system might be over-tuned to theset of particular test points such that they fail to revealtheir real performance on a broader domain.
The systemdesigner might thus be misled by such a close-test ortraining-set performance and have an over-optimisticallyevaluated figure of performance.
(See \[Devi 82, Chapter10\] for detailed comments on performance evaluation.
)We propose a new quantitative quality measure toevaluate the performance of machine translation sys-tems.
The method is to compare the raw translationoutput of an MT system with the final revised versionfor the customers, and then the editing efforts requiredto convert the raw translation to the final version is com-puted.
Compared with the above proposals, the eval-uation process can be done quickly and automatically.Moreover, application of such a measure to improvethe system performance on-line on a parumeterized andfeedhack-controlled system is easy.
Finally, since the re-vised version is used directly as a reference, the perfor-mance measure can reflect the real quality gap betweenthe system performance and customer expectation.2.
Performance Evaluation UsingBi-Text Corpus2.1.
Criteria for a Good MeasureFrom the above discussion, it is desirable to havea performance measure and a performance evaluationprocess with the following properties:\[1\] low cost: minimal human interference is involvedand can be done automatically.\[2\] high speed: it can give system designers quickresponse and immediate help (even on-line, for aparameterized system); it can also provide positivestimulation to the system designer psychologically\[3\] exacmess: the difference between customer expec-tation and real system performance an be reflected.Because the design goal of a system is to optimizesome gain or minimiz~e some cost, a good performancemeasure is definitely an important factor on the improve-ment of the system.2.2.
A Distance Measure ApproachTo achieve the goals outlined in the previous ection,a quantitative measure is proposed.
In our approach,we first establish a bi-text corpus composed of sourcelanguage sentences and the corresponding target languagesentences.
The target sentences are the revised versionof the raw translation which were post-edited to tailorto the customers' need (for publication).
Therefore, thetarget sentences are what customers really want.
Then,we employ a distance measure method to evaluate theminimum distance between the raw translation output andthe target sentences in the bi-text corpus.
By distance, wemean the editing efforts needed to edit the raw translationoutput into the revised version.
In other words, we wouldlike to know the number of key strokes required to beperformed for such editing.
The smaller the distance is,the higher the translation output quality is.The sentence pairs in the bi-text corpus is the sourcesentence and the target sentence post-edited for the cus-tomers.
The reason for adopting the revised version textas the measure reference is that even the machine trans-lated texts are error-free judged by system designers, itmay not be the final version customers really wanL Ingeneral, the system designers, who are aware of the lim-itation and restriction of an MT system, tend to giveloose quality criteria, and thus an overoptimistic evalua-tion.
Human inspection can only achieve correctness andreadability, but the acceptability ocustomers i usuallylow.
We try to offer customers the solution they reallyneed.
Thus, every trial to fine-tune the output qualityshould be directed to fit customers' needs \[Chert 91, Wu91\].This approach has several advantages over othermethods: (1) Since the final revised version is used forcomparison, it will reflect the real quality gap betweenthe capability of the system and the expectation of thecustomers.
According to oar experience on providingtranslation services with the ArehTran MTS, for mosttranslation materials, even for manuals or announcement,the final versions are intended for publication, not justfor information retrieval.
Therefore, traditional qualitymeasures which are graded loosely like 'correct', 'un-derstandable', ... and so on, provides little informationon how the system should be tuned.
Thus, it's reasonableto adopt the final revised version as the measure refer-ence.
(2) Human power is more expansive than computerpower.
Since this approach involves no human interfer-ence, the evaluation cost is fairly low.
(3) The currentsystem performance an be reported very soon becauseof high computer speed.
With the quick feedback, moreperformance improving strategies can be tried out, andthus research efficiency is improved.
(4) We can showimprovement to raise research morale and excite enthu-siasm, for a clear indicator of performance improvementAcrEs DE COLING-92, NA)cr~, 23-28 AOt3"r 1992 4 3 4 PROC.
OF COLING-92, NANTES, AUG. 23-28, 1992is the strongest incentive R)r R&D engineers.
Systemproblems can be located and solved quickly, thus a lotof work is saved.
(5) Because the final revised versionis used as the measure reference, the text can' be clas-sifted into different domains and styles.
With the quickfeedback, it can help adapt system to different donlainsand styles.2.3.
Distance Measure and WeightAssignmentFour primitive editing operations are defined,namely insertion, deletion, replacement and swap \[Wagn74, Lowr 75\].
Since each operation requires differentediting el-forts, different weights must be assigned.
Weassigql the weight according to tile number of key strokesneeded for each operation under the most popular editor.For Chinese editing, tile weights we ttssign tot insertionis 5, deletion 1, replacement 5 and swap 6.
The deletionoperation is the least costly operation for its simplicity.The insertion aurl replacement operations take more ef-forts including cursor addressing, entering and leavingediting mode.
The swap operation eeds a little more ef-tort than insertion and deletion.
(The swap cost is definedhere to be the cost of one insertion plus one deletion.
Fora post-editing facility with a special swap editing func-tkm, the swap cost should be a function of the distancebetween the characters to be swapped.
This cost mightbe less tiran the cost of one insertion plus one deletionfor adjacent words.
For tile present, the cost is used forsimplicity.)2.4.
AlignmentEndC 2n - -<//C21 ,~/sLar t  C i  I C12 e l  mFigure 1, Alignment of raw outputsentence  wi th  rev ised  vers ion  sentenceThe evaluation of the distance between the rawoutput sentence and the final revised sentence can beformulated as a "shortest path" searching problem.
Theproblem can be solved with the well-known dynamicprogramming technique \[Bell 57\].
Figure I shows adiagram tt)r the dynmnic programming problem.
Assanrethat R : {ell, c12, ..., el, ,  } is the raw output sentencecomposed of m characters ell through c1,,,, and Q ={ c21, c~2, ..., c2,, } is the final version sentence composedof n characters c~1 through c2,,.Ill Figure 1, the big square Ires m x n grids withweight (cost on" distmlce) associated with eacln edge anddiagonal.
Many l~tths call be picked from the Start tothe End.
Any path along tile edge or diagonal fromthe Start to tim End represents a sequence of editingoperations that changes the raw output sentence Io thefinal revised sentence, qhe cost incurred for each path isthe accumulative weights along the path travelled.
Thecost/distance of a path stands for the editing eflbrts tomake tbe clmnges.
Therefore, the minimmn distance pathstands ibr the least cost.
"lhe goal is to pick the path withthe minimum cost, or shortest distance.There are three directions to go at each position:right, up or up-right.
We can make an analogy between\[inding the shortest path and lrerlbrnling the fewest edit-ing operations to convert he raw output sentence into thefinal version sentence.
When we are at the Start i)omt, wehave the raw output sentence.
If we go right, a deletionoperation is performed.
If we go upward, an inseltioniS performed.
If we go along the diagonal, citlter oneof two cases will happen.
When ~:rli and c:~j on the twoedges of file diagonal arc the same, no operation is per*formed, and no cost is izlctlncd.
If, however, they aredifferent, a replacement is performed.
When we evanm..ally reach the End point, we have edited the raw outputsentence into the final versiou sentence.
The second |lathtraversal is required to compute the nmnber of swap op-eration.
If deletion of one character is to be performed,and that character will have to Ire inserted in the follow-ing operations, then one deletion and one insertion arerephtced by onc swap operation.
By the same token, Iftile iuserted character will Ire deleted later, the insertionand deletion are saved by performing one swap.
If theshortest path is picked, then we have exfited the sentencewith least effort.Tile dislar|cc betwcell the raw output seul~uce audthe revised seatcoce can be formulated as follows:\]) :~ lt) i x ~Ii .4, lt; d x 914 ~ l()r ?
~tr ~ ItY~ X rt swhere hi, ha, )tr and ns are tile numbers of operationfor insertion, deletion, replacement and swap performedrespectively; wl ,  wd, w~ and w,  are the weights for theseoperations.
D is the total distance for one specific editingsequence; that is to say, D is the number of key strokesrequired to lXlSt-edit the raw translation sentence into filefinal version sentence.2.5.
An ExampleThe solid path in figure 2 gives an example toshow the steps performed using dynamic programmingto find the least cost for editing one sentence.
Theraw output sentence is "This is my two computer" illfile X axis, and the revised version sentence is '"ll;isAcrEs DE COLING-92, NANTES, 23-28 AO~" 1992 4 3 5 PROC.
OF COLING-92, NANTES, AUG. 23-28, 1992mir~is:o~npTiffscomputer is mine" in the Y axis.
One insertion anddeletion along the path are marked an "X" because theword "computer" appears in different locations in twosentences.
Therefore, a swap operation is performed tosave one insertion and one deletion.
Totally, there areone replacement ("my" to "mine"), one deletion ("own")and one swap ("computer").
Table 1 shows the path withthe least cost.
The first row in Table 1 is the raw outputsentence, the second row the revised sentence, and diethird the editing operations performed.
The least cost is12 (i.e., 1 x 5 + 1 x 1 + 1 x 6 : 12).
And, the averagecost is 2.4 per word.
Note the difference between sucha measure with a conventional subjective approach.
Thetwo sentences might be judged as equally readable andunderstandable by human inspection.
However, to tailorto the final output for publication, we still need 2.4 unitsof cost per word.
If we follow the dotted path in Figure2, we will get the path not of the least cost.
Table 2shows this path.
In this case, there are onc deletionand three replacements.
The cost incurred is 16 (i.e.,1 x 1+3x5 : 16).Del DelNonS"This is my own compIns : InsertionDel : DeletionRep : ReplacementNop : No OperationFigure 2.
An example to showthe steps in dynamic  programmingRaw This is my own compRev This comp is mine XEdit NOP SWAF NOP REP DELOpTable 1.
A path of  the least cost 12Raw This is my own compRev This comp is mine XEdit NOP REP REP REP DELOpTable 2.
A path of cost 163.
Application to Performance Evaluationand ImprovementAs discussed above, the most direct application ofthe preference measure is, of course, to show the currentstatus of the system peffomaance.
This function directlyserves several purposes.\[1\] With this performance measure applied to a large bi-text corpus, one can show to the potential customersthe current system performance in terms of the edit-ing efforts required to get high quality translation.Furthermore, because the performance measure isan objective measure, it can be used to compare thesystem performance with other systems bused onthe same testing base.\[2\] The quick response makes it possible for the systemdesigners and the linguists to get a clear idea aboutthe advantages or faults of a particular strategy orformalism.
From the quick feedback of the mea-sarement, one can try different approaches in rathershort time.
Hence, the research pace will be accel-erated rapidly.
And the system designers can makesure the system is on the right track.\[3\] Psychologically, a clear indicator of performanceimprovement is the strongest incentive for R&Dteams.
According to our working experience, theresearch team members tend to become upset whentheir ideas can not be fully implemented and jus-tiffed in a reasonable time.
With a clear perfor-mance indicator and quick response, the team mem-bers usually get excited and their morale is raisedsubstantially.The following case study shows how the quick andautomatic performance evaluation method help make de-cision on some designing issues and highlight researchdirections.
In a recent evaluation run, a bi-text corpus,containing 6,110 English-Chinese ntence pairs are usedto evaluate a particular version of the ArehTran English-Chinese MT system.
The Chinese sentences are the re-vised version of the corresponding English sentences,which are to be published us a Chinese technical manual.The revised Chinese sentences are used as the referencefor comparison with the unrevised version.
The editingeffort required to post-edit the unrevised version is thenevaluated using the proposed istance measure.
It takesonly about 30 seconds to get the required measure.
Theexperimental results are shown in Table 3.At first, we think the editing cost might be too highto get the required high quality, and we suspect that theprobabilistic disambiguation mechanism for the analysismodule \[Chan 92a, Liu 90, Su 88, 89, 91b\] might not beproperly tuned.
So we use an adaptive l arning algorithm\[Amar 67, Kata 90, Su 91a\] to adjust the probabilisticdisambiguation modules of the system.
Table 4 showsthe comparison of the original status of the MTS and itsbest-tuned case.
In the best case, the translation with theleast cost is selected.AcrEs DE COL1NG-92, NANTEs, 23-28 AO't~T 1992 4 3 6 PROC.
OF COLING-92, NANTES, AUG. 23-28, 1992tolzd number of sentences 6,110total number of Chinese characters 135,318total number of insertions 23,541total number of deletions 21,238total number of replacements 22,721total number of swaps 9,953number of insertions per sentence 3.85number of deletions per sentence 3.48number of replacements per sentence 3.72number of swaps per sentence 1.63total cost for 6,110 sentences 312,266cost per sentence 5 l. 1 lcost per character 2.31Table 3.
Statistics summaa-y of distancemeasure (Edit ing cost scheme: Insertion 5,Deletion 1, Rep lacement  5 and swap 6)Origi~ad status Best caseCost per wordTotal t 312,266 289,290Cost per sentence 51.11 47.352.31 2.14Table 4.
Per fommnce of theor iginal  status and of the best caseTable 4 does show some improvement after ttmingthe disambiguation module.
However, the improvementis not apparent.
This implies that the disambiguation partis not the major bottleneck for the quality gap.
In fact,most translations are readable and understandable underhuman's judgement.
So we examined the other parts ofthe system.
We found that the biggest problem is thatthe translation style does not lit customers' need.
Wethus conclude that more efforts should be concentratedon the transfer and generation phases, and a transfer andgeneration model that is capable of adaptiug the system todifferent domains and styles {Chan 92bl is required.
Thiscase study shows that a quick performance evaluationdoes play an important role in directing the researchdirection.4.
Parameterized Feedback Control SystemBased on the Performance MeasureThrough the quick and automatic quantitative dis-lar~ce ntcasure, the system performance can be on-linereported in terms of an objective cost function.
There-fore, it can be applied in the guided searching in a Ira -rameteriTgd, feedback controlled system.
"lhe followingsections how how the quick performance measure helpsto construct such a feedback system.
Without a quickperformance evaluator, these models will not be madepossible.4.1.
Ambigu i ty  Reso lu t ion  and  LexiconSelect ion in a Feedback  Systemaeferen~e..... i l~exical 1_ ,.~Syntacfic \[ d Semantic \ ]~Tr rmls fe r& i~/AnalY si~ / \] Analysis \] l Analysis | IGen~rat ion l /Figure 3.
Parameter tuning from feedbackA parameterized feexlback-cont~olled MT systemcan be modeled as in Figure 3.
The control of the systemis governed by its static knowledge and a dynanfically ad-justable parameter set which are used to select he bestinterpretation among the various ambiguities, or to selectthe most preferred style in the transfer and generationphases.
The probabilistic translation model proposed in\[Chert 91\] is one such example.
In this model, the bestanalysis is to be selected ~ maximize an analysis coreor score function \[Chart 92a, Liu 90, Su 88, 89, 91b\] ofthe 1011owing form:Score ~ F' (Se*,t,, Sy,tj, Lexk \[ Words)whel~ ,?
'etlti, Sylty, Le~:k is a particu'lar set of semanticannotation, syntactic structure and lexical category cos-responding to some ambiguous construct of the sentenceWords.
Furthermore, the best transfer and generation isto be selected to maximize file following transfer scoreStx/ and generation score Sa~,,:s ,~/= P(T~ 17;) ~ I' (7 ~, 17/';)where 7',, 7; are the target and source of intemtediaterepresentations i  the form of an annotated syntax tree(AST); qL "/; are the normalized version of the AST's,called the normal forms (NF) of the AST's, which areAca~s DE COLING-92, Nnl, rrl's.
23-28 noel' 1992 4 3 7 PROC.
OF COLING-92, NANTI'.S, AU6.
23-28, 1992used particularly for transfer and generation, and t is thegenerated target sentence \[Chan 92b\].In such a system, we are to formulate a model asclosely to the unknown real language model as possi-ble.
Thus, the main task is to estimate the parameterswhich characterize the model.
Because it is not alwayspossible to acquire sufficient data, particularly for com-plex language models, the estimated parameters mightnot be able to characterize the real model Under suchcircumstances, we can adaptively adjust the estimatedparameters according to the error feedback.In Figure 3, the analysis phase (lexical, syntacticand semantic analyses) of the system is characterized bya set of selection parameters.
The input is fed into thelexical analysis phase.
The output is generated and actsas the input of transfer phase.
In the feedback controlledscheme, a set of revised text, such as the 6,110 Chi-nese sentences in the previous section, can be used asthe reference, and be compared with the translation out-put of 6,110 sentences.
Under the scoring mechanism,the preferred analysis elected may not correspond to thetranslation with the shortest distance from the reference.Under this circumstance, we can adjust he selection pa-rameters according to the error ~ (the difference betweenthe reference and the system outpu0 iteratively.
Throughthis adaptive l arning procedure \[Amar 67, Chia 92, Kata90\], the estimated parameters will approach the real pa-rameters very closely.
In this way, it will help in am-bignity resolution and lexicon selection.
Such a systemis made possible to automatically fine-tune the systembecause the performance measure proposed in this paperprovides an on-line response to the itemtively changedparameters.4.2.
B i - l ingua l  T rans fer  Mode lSOUVCO TargetNormal l za t lon  ~'rln~\[er Generat ion  at~t e rence~ Source Targel Targe~ParametersA~I" : Annotated S~a~lt Tre~NF : Natural V~mFigure 4.
An  adapt ive learn ing conceptualmodel  for  the transfer  and  generat ion phasesAs another application of the quick performancemeasure, we can construct a feedback controlled transferand generation model.
Figure 4 shows such a concep-tual model for the parameterized transfer and generationphases \[Chan 92b\], where AST \[Chan 92a\] is a syntaxtree whose nodes are annotated with syntactic and se-mantic features and NF is a normalized version of AST,which consists of only atomic transfer units.
(See alsothe previous ection for the transfer score and generationscore).
The Source NF and Target NF are characterizedby a set of selection parameters.
By jointly consideringthe parameters characterizing the Source and Target NF,we can adaptively adjust he parameters from the feed-back of both directions just like in the previous ection.Also the feedback control will make the parameters betuned to fit the stylistic haracteristics of the revised tar-get sentences.
Hence, more natural sentences could begenerated and less editing effort could be expected in or-der to get high quality translation.
Again, only a quickperformance evaluator can make such feedback systempractical.5.
Conc lus ionThe need for performance evaluation is rising, forboth customers and system designers.
We proposed aperformance evaluation method with which system per-formance can be evaluated automatically and quickly.The approach to improve system performance and feed-back controlled MT system is proposed based on suchmeasure.
Because the revised text is used directly as ref-erence, the performance measure can indicate real qualitygap between users' expectation and system capability.Though we can not measure the very fine detailedfeatures because there is not very much linguistic knowl-edge incorporated, our approach as many advantagesover conventional pproaches.
There is no need for hu-man interference.
The criteria are consistent and objec-tive.
And, we are trying to offer the solutions what usersreally need.
Most important of all, from the feedback ofmeasurement, it is fairly easy for system fine-tuning.References\[Amar 67\] Amari S., 1967.
"A Theory of Adaptive Pat-tern Classifiers," IEEE Trans.
on Electronics Com-puters, vol.
EC-16, pp.
299-307, June 1967.\[Bell 57\] Bellman R.-E., "Dynamic Programming,"Princeton University Pres.L 1957, Princeton, NJ,USA.\[Chan 92a\] Chang, J.-S., Y.-F. Luo and K.-Y.
Su,1992.
"GPSM: A Generalized Probabilistic Seman-tic Model for Ambiguity Resolution," to appear inProceedings ofACL-92, 30th Annual Meeting of theAssociation for Computational Linguistics, Univer-sity of Delaware, Newark, DE, USA, 28 June-2 July,1992.\[Chart 92b\] Chang, J.-S. and K.-Y.
Su, 1992.
"A Corpus-Based Statistics-Oriented Transfer and GenerationModel for Machine Translation," to submiL\[Chen 91\] Cheu, S.42., J.-S. Chang, J.-N. Wang and K.-Y.
Su, 1991.
"ArchTran: A Corpus-based Statistics-Acr~s DE COLING-92, NANTES, 23-28 AO~r 1992 4 3 8 PROC, OF COLING-92, NANTES, AUG. 23-28, 1992oriented English-Chinese Machine Translation Sys-tem," Proceedings of Machine Translation Summit111, pp.
33~10, Washington D.C., USA, Jnly 1~1,1991.\[Chia 92\] Chiang, T.-H., Y.-C. Lin and K.-Y.
Su, "Syn-tactic Ambiguity Resolution Using A Discrimina-tion and Robustness Oriented Adaptive LearningAlgorithm," to appear in Proceedings of COLING-92, 14th International Conference on ComputatioualLinguistics, Nantes, France, 20-28 July, 1992.\[Devi 82\] Devijver, P.A.
and J. Kitties.
Pattern Recogni-tion : A Statistical Approach, Prentice-Hall, London,1982.\[Kata 90\] Katagiri, S. and C.-H. Lee, 1990.
"A Gener-alized Probabilistic Decent Method," Proc.
Acous.Set.
of Japan, pp.
141-142, Nagoya, Sep. 1990.\[King 90\] King, M. and K. Falkedal, 1990.
"Us-ing Test Suites in Evaluation of Machine Transla-tion Systems," Proceedings of COLING-90, vol.
2.pp.
211-216, the 13th Internathmal Conference onComputational Linguistics, Helsinki, Finland, 20-25Aug.
1991.\[Liu 90\] Liu, C.-L., J.-S. Chang and K.-Y.
Su, 1990.
"The Semantic Score Approach to the I)isambigua-lion of PP Attachment Problem," Proceedings ofROCLING-III, pp.
253-270, Taipei, ROC.\[Lowr 75\] Lowrence, R. and R.-A.
Wagner, 1975.
"AnExtension of the String-to-String Correction Prob-lem,"Journal A.C.M., vol.
22, no.
2, pp.
177-83,1975.\[Pigo 82\] Pigott, I.-M., 1982.
"The Importance of Feed-back from Translators in the Development of High-quality Machine Translation," Practical Experienceof Machine Translation, pp.
61-71.\[Ross 82\] Rossi F., 1982.
"The Impact of Posteditors'Feedback on the Quality of MT," Practical Experi-ence of Machine l~'anslation, pp.
113-117.ISu 88\] Su, K.-Y.
and J.-S. Chang, 1988.
"'Semantic andSyntactic Aspects of Score Function," Proceedingsof COLING..88, vol.
2, pp.
642~44, 12th Inter-national Conference on Computational Linguistics,Budapest, Hungary, August 22-27, 1988.\[Su 89\] Su, K.-Y., J.-N., Wang, M.-H. Su, and J.-S.Chang, 1989.
"A Sequential Truncation ParsingAlgorithm Based on the Score Function," Proceed-ings of International Workshop on Parsing Technolo-gies(IWPT 89), pp.
95-104, Pittsburgh, PA, USA.\[Su 9(1\] Su, K.-Y., and J.-S. Chang, 1990.
"Some KeyIssues in L,N?.signing MT Systems," Machine Trans-lation, vol.
5, no.
4, pp.
265-30(I, 1990.\[Su 91a\] Su, K.-Y.
and C.-It.
Lee., 1991.
"Robust-uess and Discrimirtation Oriented Speech Recogni-tion Using Weighting HMM and Subspace Projec-tion Approaches," Proceedings oflEEE ICASSP-91,Toronto, Ontario, Canada.\[Su 91hi Su, K.-Y., J:N. Wang, M.-H. Su, and' J.-S.Chaog, 1991.
"GLR Parsing with Scoring," M.Tomita (cal.
), Generalized LR Parsing, Kluwer Aca-demic Publishers.\[Wagn 74\] Wagner, C.-K. and M.-J.
Fischer, 1974.
"The String-to-String Correction Problem," JournalA.C.M., w)l. 21, no.
l, pp.
168-173.IWu 911 Wu., M.-W., J.-S. Chang and K.-Y.
Su, 1991.
"The Current Status of ArchTran: A Corpus-BasedStatistics-Oriented English-Chinese Machine Traos-latiou System," Proceedings of the 1991 Workshopon Machine 7?anslation, pp.
123-138, Nantou, qhi-wan, ROC, 24-26 June, 1991.\[Yu 91\] Yu, S., 1991.
"'Aulomatic Evaluation of OutputQuality for Machine Translation System," Interna-tional Symposium on Multilingual Machine Transla-tion, Beijing, China, pp.
57-58, 19-21 Aug. 1991.ACI'ES DE COLING-92, NANTES, 23-28 AO~r 1992 4 3 9 I)ROC.
OJ: COLING-92, NANTES, Alto.
23-28, 1992
