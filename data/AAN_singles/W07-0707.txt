Proceedings of the Second Workshop on Statistical Machine Translation, pages 48?55,Prague, June 2007. c?2007 Association for Computational LinguisticsWord Error Rates: Decomposition over POS Classes and Applications forError AnalysisMaja Popovic?Lehrstuhl fu?r Informatik 6RWTH Aachen UniversityAachen, Germanypopovic@cs.rwth-aachen.deHermann NeyLehrstuhl fu?r Informatik 6RWTH Aachen UniversityAachen, Germanyney@cs.rwth-aachen.deAbstractEvaluation and error analysis of machinetranslation output are important but difficulttasks.
In this work, we propose a novelmethod for obtaining more details about ac-tual translation errors in the generated outputby introducing the decomposition of WordError Rate (WER) and Position independentword Error Rate (PER) over different Part-of-Speech (POS) classes.
Furthermore, weinvestigate two possible aspects of the useof these decompositions for automatic er-ror analysis: estimation of inflectional errorsand distribution of missing words over POSclasses.
The obtained results are shown tocorrespond to the results of a human erroranalysis.
The results obtained on the Euro-pean Parliament Plenary Session corpus inSpanish and English give a better overviewof the nature of translation errors as well asideas of where to put efforts for possible im-provements of the translation system.1 IntroductionEvaluation of machine translation output is a veryimportant but difficult task.
Human evaluation isexpensive and time consuming.
Therefore a varietyof automatic evaluation measures have been studiedover the last years.
The most widely used are WordError Rate (WER), Position independent word ErrorRate (PER), the BLEU score (Papineni et al, 2002)and the NIST score (Doddington, 2002).
These mea-sures have shown to be valuable tools for comparingdifferent systems as well as for evaluating improve-ments within one system.
However, these measuresdo not give any details about the nature of translationerrors.
Therefore some more detailed analysis of thegenerated output is needed in order to identify themain problems and to focus the research efforts.
Aframework for human error analysis has been pro-posed in (Vilar et al, 2006), but as every humanevaluation, this is also a time consuming task.This article presents a framework for calculatingthe decomposition of WER and PER over differentPOS classes, i.e.
for estimating the contribution ofeach POS class to the overall word error rate.
Al-though this work focuses on POS classes, the methodcan be easily extended to other types of linguis-tic information.
In addition, two methods for erroranalysis using the WER and PER decompositons to-gether with base forms are proposed: estimation ofinflectional errors and distribution of missing wordsover POS classes.
The translation corpus used forour error analysis is built in the framework of theTC-STAR project (tcs, 2005) and contains the tran-scriptions of the European Parliament Plenary Ses-sions (EPPS) in Spanish and English.
The translationsystem used is the phrase-based statistical machinetranslation system described in (Vilar et al, 2005;Matusov et al, 2006).2 Related WorkAutomatic evaluation measures for machine trans-lation output are receiving more and more atten-tion in the last years.
The BLEU metric (Pap-ineni et al, 2002) and the closely related NIST met-ric (Doddington, 2002) along with WER and PER48have been widely used by many machine translationresearchers.
An extended version of BLEU whichuses n-grams weighted according to their frequencyestimated from a monolingual corpus is proposedin (Babych and Hartley, 2004).
(Leusch et al, 2005)investigate preprocessing and normalisation meth-ods for improving the evaluation using the standardmeasures WER, PER, BLEU and NIST.
The same setof measures is examined in (Matusov et al, 2005)in combination with automatic sentence segmenta-tion in order to enable evaluation of translation out-put without sentence boundaries (e.g.
translation ofspeech recognition output).
A new automatic met-ric METEOR (Banerjee and Lavie, 2005) uses stemsand synonyms of the words.
This measure countsthe number of exact word matches between the out-put and the reference.
In a second step, unmatchedwords are converted into stems or synonyms andthen matched.
The TER metric (Snover et al, 2006)measures the amount of editing that a human wouldhave to perform to change the system output so thatit exactly matches the reference.
The CDER mea-sure (Leusch et al, 2006) is based on edit distance,such as the well-known WER, but allows reorderingof blocks.
Nevertheless, none of these measures orextensions takes into account linguistic knowledgeabout actual translation errors, for example what isthe contribution of verbs in the overall error rate,how many full forms are wrong whereas their baseforms are correct, etc.
A framework for human erroranalysis has been proposed in (Vilar et al, 2006)and a detailed analysis of the obtained results hasbeen carried out.
However, human error analysis,like any human evaluation, is a time consuming task.Whereas the use of linguistic knowledge for im-proving the performance of a statistical machinetranslation system is investigated in many publi-cations for various language pairs (like for exam-ple (Nie?en and Ney, 2000), (Goldwater and Mc-Closky, 2005)), its use for the analysis of translationerrors is still a rather unexplored area.
Some auto-matic methods for error analysis using base formsand POS tags are proposed in (Popovic?
et al, 2006;Popovic?
and Ney, 2006).
These measures are basedon differences between WER and PER which are cal-culated separately for each POS class using subsetsextracted from the original texts.
Standard overallWER and PER of the original texts are not at alltaken into account.
In this work, the standard WERand PER are decomposed and analysed.3 Decomposition of WER and PER overPOS classesThe standard procedure for evaluating machinetranslation output is done by comparing the hypoth-esis document hyp with given reference translationsref , each one consisting of K sentences (or seg-ments).
The reference document ref consists ofR reference translations for each sentence.
Let thelength of the hypothesis sentence hypk be denotedas Nhypk , and the reference lengths of each sentenceNref k,r .
Then, the total hypothesis length of the doc-ument is Nhyp =?k Nhypk , and the total referencelength is Nref =?k N?ref k where N?ref k is definedas the length of the reference sentence with the low-est sentence-level error rate as shown to be optimalin (Leusch et al, 2005).3.1 Standard word error rates (overview)The word error rate (WER) is based on the Lev-enshtein distance (Levenshtein, 1966) - the mini-mum number of substitutions, deletions and inser-tions that have to be performed to convert the gen-erated text hyp into the reference text ref .
A short-coming of the WER is the fact that it does not allowreorderings of words, whereas the word order of thehypothesis can be different from word order of thereference even though it is correct translation.
Inorder to overcome this problem, the position inde-pendent word error rate (PER) compares the wordsin the two sentences without taking the word orderinto account.
The PER is always lower than or equalto the WER.
On the other hand, shortcoming of thePER is the fact that the word order can be impor-tant in some cases.
Therefore the best solution is tocalculate both word error rates.Calculation of WER: The WER of the hypothe-sis hyp with respect to the reference ref is calculatedas:WER = 1N?refK?k=1minrdL(ref k,r, hypk)where dL(ref k,r, hypk) is the Levenshtein dis-tance between the reference sentence ref k,r and thehypothesis sentence hypk.
The calculation of WER49is performed using a dynamic programming algo-rithm.Calculation of PER: The PER can be calcu-lated using the counts n(e, hypk) and n(e, ref k,r)of a word e in the hypothesis sentence hypk and thereference sentence ref k,r respectively:PER = 1N?refK?k=1minrdPER(ref k,r, hypk)wheredPER(ref k,r, hypk) =12(|Nref k,r ?
Nhypk |+?e|n(e, ref k,r) ?
n(e, hypk)|)3.2 WER decomposition over POS classesThe dynamic programming algorithm for WER en-ables a simple and straightforward identification ofeach erroneous word which actually contributes toWER.
Let errk denote the set of erroneous wordsin sentence k with respect to the best reference andp be a POS class.
Then n(p, errk) is the number oferrors in errk produced by words with POS class p.It should be noted that for the substitution errors, thePOS class of the involved reference word is takeninto account.
POS tags of the reference words arealso used for the deletion errors, and for the inser-tion errors the POS class of the hypothesis word istaken.
The WER for the word class p can be calcu-lated as:WER(p) = 1N?refK?k=1n(p, errk)The sum over all classes is equal to the standardoverall WER.An example of a reference sentence and hypothe-sis sentence along with the corresponding POS tagsis shown in Table 1.
The WER errors, i.e.
actualwords participating in WER together with their POSclasses can be seen in Table 2.
The reference wordsinvolved in WER are denoted as reference errors,and hypothesis errors refer to the hypothesis wordsparticipating in WER.Standard WER of the whole sentence is equalto 4/12 = 33.3%.
The contribution of nouns isreference:Mister#N Commissioner#N ,#PUNtwenty-four#NUM hours#Nsometimes#ADV can#V be#V too#ADVmuch#PRON time#N .#PUNhypothesis:Mrs#N Commissioner#N ,#PUNtwenty-four#NUM hours#N is#Vsometimes#ADV too#ADVmuch#PRON time#N .#PUNTable 1: Example for illustration of actual errors: aPOS tagged reference sentence and a correspondinghypothesis sentencereference errors hypothesis errors error typeMister#N Mrs#N substitutionsometimes#ADV is#V substitutioncan#V deletionbe#V sometimes#ADV substitutionTable 2: WER errors: actual words which are partici-pating in the word error rate and their correspondingPOS classesWER(N) = 1/12 = 8.3%, of verbs WER(V) =2/12 = 16.7% and of adverbs WER(ADV) =1/12 = 8.3%3.3 PER decomposition over POS classesIn contrast to WER, standard efficient algorithms forthe calculation of PER do not give precise informa-tion about contributing words.
However, it is pos-sible to identify all words in the hypothesis whichdo not have a counterpart in the reference, and viceversa.
These words will be referred to as PER errors.reference errors hypothesis errorsMister#N Mrs#Nbe#V is#Vcan#VTable 3: PER errors: actual words which are partic-ipating in the position independent word error rateand their corresponding POS classesAn illustration of PER errors is given in Table 3.50The number of errors contributing to the standardPER according to the algorithm described in 3.1 is 3- there are two substitutions and one deletion.
Theproblem with standard PER is that it is not possibleto detect which words are the deletion errors, whichare the insertion errors, and which words are the sub-stitution errors.
Therefore we introduce an alterna-tive PER based measure which corresponds to theF-measure.
Let herrk refer to the set of words in thehypothesis sentence k which do not appear in thereference sentence k (referred to as hypothesis er-rors).
Analogously, let rerrk denote the set of wordsin the reference sentence k which do not appear inthe hypothesis sentence k (referred to as referenceerrors).
Then the following measures can be calcu-lated:?
reference PER (RPER) (similar to recall):RPER(p) = 1N?refK?k=1n(p, rerrk)?
hypothesis PER (HPER) (similar to precision):HPER(p) = 1NhypK?k=1n(p, herrk)?
F-based PER (FPER):FPER(p) = 1N?ref + Nhyp?
?K?k=1(n(p, rerrk) + n(p, herrk))Since we are basically interested in all words with-out a counterpart, both in the reference and in thehypothesis, this work will be focused on FPER.
Thesum of FPER over all POS classes is equal to theoverall FPER, and the latter is always less or equalto the standard PER.For the example sentence presented in Table 1, thenumber of hypothesis errors n(e, herrk) is 2 and thenumber of reference errors n(e, rerrk) is 3 where edenotes the word.
The number of errors contributingto the standard PER is 3, since |Nref ?
Nhyp | = 1and?e |n(e, ref k) ?
n(e, hypk)| = 5.
The stan-dard PER is normalised over the reference lengthNref = 12 thus being equal to 25%.
The FPER is thesum of hypothesis and reference errors divided bythe sum of hypothesis and reference length: FPER =(2 + 3)/(11 + 12) = 5/23 = 21.7%.
The contribu-tion of nouns is FPER(N) = 2/23 = 8.7% and thecontribution of verbs is FPER(V) = 3/23 = 13%.4 Applications for error analysisThe decomposed error rates described in Section 3.2and Section 3.3 contain more details than the stan-dard error rates.
However, for more precise informa-tion about certain phenomena some kind of furtheranalysis is required.
In this work, we investigate twopossible aspects for error analysis:?
estimation of inflectional errors by the use ofFPER errors and base forms?
extracting the distribution of missing wordsover POS classes using WER errors, FPER er-rors and base forms.4.1 Inflectional errorsInflectional errors can be estimated using FPERerrors and base forms.
From each reference-hypothesis sentence pair, only erroneous wordswhich have the common base forms are takeninto account.
The inflectional error rate of each POSclass is then calculated in the same way as FPER.For example, from the PER errors presented in Ta-ble 3, the words ?is?
and ?be?
are candidates for aninflectional error because they are sharing the samebase form ?be?.
Inflectional error rate in this exam-ple is present only for the verbs, and is calculated inthe same way as FPER, i.e.
IFPER(V) = 2/23 =8.7%.4.2 Missing wordsDistribution of missing words over POS classes canbe extracted from the WER and FPER errors in thefollowing way: the words considered as missing arethose which occur as deletions in WER errors andat the same time occur only as reference PER errorswithout sharing the base form with any hypothesiserror.
The use of both WER and PER errors is muchmore reliable than using only the WER deletion er-ros because not all deletion errors are produced bymissing words: a number of WER deletions appears51due to reordering errors.
The information about thebase form is used in order to eliminate inflectionalerrors.
The number of missing words is extracted foreach word class and then normalised over the sum ofall classes.
For the example sentence pair presentedin Table 1, from the WER errors in Table 2 and thePER errors in Table 3 the word ?can?
will be identi-fied as missing.5 Experimental settings5.1 Translation SystemThe machine translation system used in this workis based on the statistical aproach.
It is built asa log-linear combination of seven different statisti-cal models: phrase based models in both directions,IBM1 models at the phrase level in both directions,as well as target language model, phrase penalty andlength penalty are used.
A detailed description of thesystem can be found in (Vilar et al, 2005; Matusovet al, 2006).5.2 Task and corpusThe corpus analysed in this work is built in theframework of the TC-STAR project.
The trainingcorpus contains more than one million sentences andabout 35 million running words of the European Par-liament Plenary Sessions (EPPS) in Spanish and En-glish.
The test corpus contains about 1 000 sentencesand 28 000 running words.
The OOV rates are low,about 0.5% of the running words for Spanish and0.2% for English.
The corpus statistics can be seenin Table 4.
More details about the EPPS data can befound in (Vilar et al, 2005).TRAIN Spanish EnglishSentences 1 167 627Running words 35 320 646 33 945 468Vocabulary 159 080 110 636TESTSentences 894 1 117Running words 28 591 28 492OOVs 0.52% 0.25%Table 4: Statistics of the training and test corporaof the TC-STAR EPPS Spanish-English task.
Testcorpus is provided with two references.6 Error analysisThe translation is performed in both directions(Spanish to English and English to Spanish) and theerror analysis is done on both the English and theSpanish output.
Morpho-syntactic annotation of theEnglish references and hypotheses is performed us-ing the constraint grammar parser ENGCG (Vouti-lainen, 1995), and the Spanish texts are annotatedusing the FreeLing analyser (Carreras et al, 2004).In this way, all references and hypotheses are pro-vided with POS tags and base forms.
The decom-position of WER and FPER is done over the tenmain POS classes: nouns (N), verbs (V), adjectives(A), adverbs (ADV), pronouns (PRON), determiners(DET), prepositions (PREP), conjunctions (CON),numerals (NUM) and punctuation marks (PUN).
In-flectional error rates are also estimated for each POSclass using FPER counts and base forms.
Addition-ally, details about the verb tense and person inflec-tions for both languages as well as about the adjec-tive gender and person inflections for the Spanishoutput are extracted.
Apart from that, the distribu-tion of missing words over the ten POS classes isestimated using the WER and FPER errors.6.1 WER and PER (FPER) decompositionsFigure 1 presents the decompositions of WER andFPER over the ten basic POS classes for both lan-guages.
The largest part of both word error ratescomes from the two most important word classes,namely nouns and verbs, and that the least criticalclasses are punctuations, conjunctions and numbers.Adjectives, determiners and prepositions are sig-nificantly worse in the Spanish output.
This is partlydue to the richer morphology of the Spanish lan-guage.
Furthermore, the histograms indicate that thenumber of erroneus nouns and pronouns is higherin the English output.
As for verbs, WER is higherfor English and FPER for Spanish.
This indicatesthat there are more problems with word order in theEnglish output, and more problems with the correctverb or verb form in the Spanish output.In addition, the decomposed error rates give anidea of where to put efforts for possible improve-ments of the system.
For example, working on im-provements of verb translations could reduce up toabout 10% WER and 7% FPER, working on nouns5201234567891011PUNNUMPREP CONDETPRONADVAVNWER over POS classes [%]EnglishSpanish0123456789PUNNUMPREP CONDETPRONADVAVNFPER over POS classes [%]EnglishSpanishFigure 1: Decomposition of WER and FPER [%]over the ten basic POS classes for English and Span-ish outputup to 8% WER and 5% FPER, whereas there is noreason to put too much efforts on e.g.
adverbs sincethis could lead only to about 2% of WER and FPERreduction.
16.2 Inflectional errorsInflectional error rates for the ten POS classes arepresented in Figure 2.
For the English language,these errors are significant only for two POS classes:nouns and verbs.
The verbs are the most problem-atic category in both languages, for Spanish havingalmost two times higher error rate than for English.This is due to the very rich morphology of Spanishverbs - one base form might have up to about fourtydifferent inflections.1Reduction of FPER leads to a similar reduction of PER.00.511.522.5PUNNUMPREP CONDETPRONADVAVNinflectional errors [%]EnglishSpanishFigure 2: Inflectional error rates [%] for English andSpanish outputNouns have a higher error rate for English thanfor Spanish.
The reason for this difference is notclear, since the noun morphology of neither of thelanguages is particularly rich - there is only distinc-tion between singular and plural.
One possible ex-planation might be the numerous occurences of dif-ferent variants of the same word, like for example?Mr?
and ?Mister?.In the Spanish output, two additional POS classesare showing significant error rate: determiners andadjectives.
This is due to the gender and number in-flections of those classes which do not exist in theEnglish language - for each determiner or adjective,there are four variants in Spanish and only one in En-glish.
Working on inflections of Spanish verbs mightreduce approximately 2% of FPER, on English verbsabout 1%.
Improvements of Spanish determinerscould lead up to about 2% of improvements.6.2.1 Comparison with human error analysisThe results obtained for inflectional errors arecomparable with the results of a human error anal-ysis carried out in (Vilar et al, 2006).
Although itis difficult to compare all the numbers directly, theoverall tendencies are the same: the largest num-ber of translation errors are caused by Spanish verbs,and much less but still a large number of errors byEnglish verbs.
A much smaller but still significantnumber of errors is due to Spanish adjectives, andonly a few errors of English adjectives are present.Human analysis was done also for the tense and53person of verbs, as well as for the number and gen-der of adjectives.
We use more detailed POS tags inorder to extract this additional information and cal-culate inflectional error rates for such tags.
It shouldbe noted that in contrast to all previous error rates,these error rates are not disjunct but overlapping:many words are contributing to both.The results are shown in Figure 3, and the tenden-cies are again the same as those reported in (Vilaret al, 2006).
As for verbs, tense errors are muchmore frequent than person errors for both languages.Adjective inflections cause certain amount of errorsonly in the Spanish output.
Contributions of genderand of number are aproximately equal.00.511.52A numberA genderV personV tenseinflectional errors of verbs and adjectives [%]EnglishSpanishFigure 3: More details about inflections: verb tenseand person error rates and adjective gender and num-ber error rates [%]6.3 Missing wordsFigure 4 presents the distribution of missing wordsover POS classes.
This distribution has a same be-haviour as the one obtained by human error analysis.Most missing words for both languages are verbs.For English, the percentage of missing verbs is sig-nificantly higher than for Spanish.
The same thinghappens for pronouns.
The probable reason for thisis the nature of Spanish verbs.
Since person andtense are contained in the suffix, Spanish pronounsare often omitted, and auxiliary verbs do not existfor all tenses.
This could be problematic for a trans-lation system, because it processes only one Spanishword which actually contains two (or more) Englishwords.024681012141618202224262830PUNNUMPREP CONDETPRONADVAVNmissing words [%]engespFigure 4: Distribution of missing words over POSclasses [%] for English and Spanish outputPrepositions are more often missing in Spanishthan in English, as well as determiners.
A probablereason is the disproportion of the number of occur-rences for those classes between two languages.7 ConclusionsThis work presents a framework for extraction of lin-guistic details from standard word error rates WERand PER and their use for an automatic error analy-sis.
We presented a method for the decomposition ofstandard word error rates WER and PER over ten ba-sic POS classes.
We also carried out a detailed anal-ysis of inflectional errors which has shown that theresults obtained by our method correspond to thoseobtained by a human error analysis.
In addition, weproposed a method for analysing missing word er-rors.We plan to extend the proposed methods in orderto carry out a more detailed error analysis, for ex-ample examining different types of verb inflections.We also plan to examine other types of translationerrors like for example errors caused by word order.AcknowledgementsThis work was partly funded by the European Unionunder the integrated project TC-STAR?
Technologyand Corpora for Speech to Speech Translation (IST-2002-FP6-506738).54ReferencesBogdan Babych and Anthony Hartley.
2004.
Extend-ing BLEU MT Evaluation Method with FrequencyWeighting.
In Proc.
of the 42nd Annual Meeting ofthe Association for Computational Linguistics (ACL),Barcelona, Spain, July.Satanjeev Banerjee and Alon Lavie.
2005.
METEOR:An Automatic Metric for MT Evaluation with Im-proved Correlation with Human Judgements.
In 43rdAnnual Meeting of the Assoc.
for Computational Lin-guistics: Proc.
Workshop on Intrinsic and ExtrinsicEvaluation Measures for MT and/or Summarization,pages 65?72, Ann Arbor, MI, June.Xavier Carreras, Isaac Chao, Llu?
?s Padro?, and MuntsaPadro?.
2004.
FreeLing: An Open-Source Suite ofLanguage Analyzers.
In Proc.
4th Int.
Conf.
on Lan-guage Resources and Evaluation (LREC), pages 239?242, Lisbon, Portugal, May.George Doddington.
2002.
Automatic evaluation of ma-chine translation quality using n-gram co-occurrencestatistics.
In Proc.
ARPA Workshop on Human Lan-guage Technology, pages 128?132, San Diego.Sharon Goldwater and David McClosky.
2005.
Improv-ing stastistical machine translation through morpho-logical analysis.
In Proc.
of the Conf.
on EmpiricalMethods for Natural Language Processing (EMNLP),Vancouver, Canada, October.Gregor Leusch, Nicola Ueffing, David Vilar, and Her-mann Ney.
2005.
Preprocessing and Normalizationfor Automatic Evaluation of Machine Translation.
In43rd Annual Meeting of the Assoc.
for ComputationalLinguistics: Proc.
Workshop on Intrinsic and Extrin-sic Evaluation Measures for MT and/or Summariza-tion, pages 17?24, Ann Arbor, MI, June.
Associationfor Computational Linguistics.Gregor Leusch, Nicola Ueffing, and Hermann Ney.
2006.CDER: Efficient MT Evaluation Using Block Move-ments.
In EACL06, pages 241?248, Trento, Italy,April.Vladimir Iosifovich Levenshtein.
1966.
Binary CodesCapable of Correcting Deletions, Insertions and Re-versals.
Soviet Physics Doklady, 10(8):707?710,February.Evgeny Matusov, Gregor Leusch, Oliver Bender, andHermann Ney.
2005.
Evaluating Machine Transla-tion Output with Automatic Sentence Segmentation.In Proceedings of the International Workshop on Spo-ken Language Translation (IWSLT), pages 148?154,Pittsburgh, PA, October.Evgeny Matusov, Richard Zens, David Vilar, ArneMauser, Maja Popovic?, and Hermann Ney.
2006.The RWTH Machine Translation System.
In TC-StarWorkshop on Speech-to-Speech Translation, pages 31?36, Barcelona, Spain, June.Sonja Nie?en and Hermann Ney.
2000.
Improving SMTquality with morpho-syntactic analysis.
In COLING?00: The 18th Int.
Conf.
on Computational Linguistics,pages 1081?1085, Saarbru?cken, Germany, July.Kishore Papineni, Salim Roukos, Todd Ward, and Wie-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proc.
of the 40thAnnual Meeting of the Association for ComputationalLinguistics (ACL), pages 311?318, Philadelphia, PA,July.Maja Popovic?
and Hermann Ney.
2006.
Error Analysisof Verb Inflections in Spanish Translation Output.
InTC-Star Workshop on Speech-to-Speech Translation,pages 99?103, Barcelona, Spain, June.Maja Popovic?, Adria` de Gispert, Deepa Gupta, PatrikLambert, Hermann Ney, Jose?
B. Marin?o, MarcelloFederico, and Rafael Banchs.
2006.
Morpho-syntacticInformation for Automatic Error Analysis of Statisti-cal Machine Translation Output.
In Proc.
of the HLT-NAACL Workshop on Statistical Machine Translation,pages 1?6, New York, NY, June.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A Studyof Translation Error Rate with Targeted Human An-notation.
In Proc.
of the 7th Conf.
of the Associationfor Machine Translation in the Americas (AMTA 06),pages 223?231, Boston, MA.2005.
TC-STAR - technology and corpora for speech tospeech translation.
Integrated project TCSTAR (IST-2002-FP6-506738) funded by the European Commis-sion.
http://www.tc-star.org/.David Vilar, Evgeny Matusov, Sas?a Hasan, Richard Zens,and Hermann Ney.
2005.
Statistical Machine Transla-tion of European Parliamentary Speeches.
In Proc.
MTSummit X, pages 259?266, Phuket, Thailand, Septem-ber.David Vilar, Jia Xu, Luis Fernando D?Haro, and Her-mann Ney.
2006.
Error Analysis of Statistical Ma-chine Translation Output.
In Proc.
of the Fifth Int.Conf.
on Language Resources and Evaluation (LREC),pages 697?702, Genoa, Italy, May.Atro Voutilainen.
1995.
ENGCG -Constraint Grammar Parser of English.http://www2.lingsoft.fi/doc/engcg/intro/.55
