Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 773?782,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsExtracting Social Power Relationships from Natural LanguagePhilip BramsenLouisville, KYbramsen@alum.mit.edu*Ami PatelMassachusetts Institute of TechnologyCambridge, MAampatel@mit.edu*Martha Escobar-MolanoSan Diego, CAmescobar@asgard.com*Rafael AlonsoSET CorporationArlington, VAralonso@setcorp.comAbstractSociolinguists have long argued that socialcontext influences language use in all mannerof ways, resulting in lects 1 .
This paper ex-plores a text classification problem we willcall lect modeling, an example of what hasbeen termed computational sociolinguistics.
Inparticular, we use machine learning techniquesto identify social power relationships betweenmembers of a social network, based purely onthe content of their interpersonal communica-tion.
We rely on statistical methods, as op-posed to language-specific engineering, toextract features which represent vocabularyand grammar usage indicative of social powerlect.
We then apply support vector machines tomodel the social power lects representing su-perior-subordinate communication in the En-ron email corpus.
Our results validate thetreatment of lect modeling as a text classifica-tion problem ?
albeit a hard one ?
and consti-tute a case for future research in computationalsociolinguistics.1 IntroductionLinguists in sociolinguistics, pragmatics and re-lated fields have analyzed the influence of socialcontext on language and have catalogued countlessphenomena that are influenced by it, confirmingmany with qualitative and quantitative studies.
In-* This work was done while these authors were at SET Corpo-ration, an SAIC Company.1 Fields that deal with society and language have inconsistentterminology; ?lect?
is chosen here because ?lect?
has no otherEnglish definitions and the etymology of the word gives it thesense we consider most relevant.deed, social context and function influence lan-guage at every level ?
morphologically, lexically,syntactically, and semantically, through discoursestructure, and through higher-level abstractionssuch as pragmatics.Considered together, the extent to which speak-ers modify their language for a social contextamounts to an identifiable variation on language,which we call a lect.
Lect is a backformation fromwords such as dialect (geographically defined lan-guage) and ethnolect (language defined by ethniccontext).In this paper, we describe lect classifiers for so-cial power relationships.
We refer to these lects as:?
UpSpeak: Communication directed tosomeone with greater social authority.?
DownSpeak: Communication directed tosomeone with less social authority.?
PeerSpeak: Communication to someone ofequal social authority.We call the problem of modeling these lects SocialPower Modeling (SPM).
The experiments reportedin this paper focused primarily on modeling Up-Speak and DownSpeak.Manually constructing tools that effectivelymodel specific linguistic phenomena suggested bysociolinguistics would be a Herculean effort.Moreover, it would be necessary to repeat the ef-fort in every language!
Our approach first identi-fies statistically salient phrases of words and partsof speech ?
known as n-grams ?
in training textsgenerated in conditions where the social power773relationship is known.
Then, we apply machinelearning to train classifiers with groups of these n-grams as features.
The classifiers assign the Up-Speak and DownSpeak labels to unseen text.
Thismethodology is a cost-effective approach to model-ing social information and requires no language- orculture-specific feature engineering, although webelieve sociolinguistics-inspired features holdpromise.When applied to the corpus of emails sent andreceived by Enron employees (CALO Project2009), this approach produced solid results, despitea limited number of training and test instances.This has many implications.
Since manually de-termining the power structure of social networks isa time-consuming process, even for an expert, ef-fective SPM could support data driven socio-cultural research and greatly aid analysts doingnational intelligence work.
Social network analysis(SNA) presupposes a collection of individuals,whereas a social power lect classifier, once trained,would provide useful information about individualauthor-recipient links.
On networks where SNAalready has traction, SPM could provide comple-mentary information based on the content of com-munications.If SPM were yoked with sentiment analysis, wemight identify which opinions belong to respectedmembers of online communities or lay thegroundwork for understanding how respect isearned in social networks.More broadly, computational sociolinguistics isa nascent field with significant potential to aid inmodeling and understanding human relationships.The results in this paper suggest that successes todate modeling authorship, sentiment, emotion, andpersonality extend to social power modeling, andour approach may well be applicable to other di-mensions of social meaning.In the coming sections, we first establish theRelated Work, primarily from Statistical NLP.We then cover our Approach, the Evaluation,and, finally, the Conclusions and Future Re-search.2 Related WorkThe feasibility of Social Power Modeling is sup-ported by sociolinguistic research identifying spe-cific ways in which a person?s language reflects hisrelative power over others.
Fairclough's classicwork Language and Power explores how"sociolinguistic conventions .
.
.
arise out of -- andgive rise to ?
particular relations of power" (Fair-clough, 1989).
Brown and Levinson created a the-ory of politeness, articulating a set of strategieswhich people employ to demonstrate different lev-els of politeness (Brown & Levinson, 1987).
Mo-rand drew upon this theory in his analysis ofemails sent within a corporate hierarchy; in it, hequantitatively showed that emails from subordi-nates to superiors are, in fact, perceived as morepolite, and that this perceived politeness is corre-lated with specific linguistic tactics, including onesset out by Brown and Levinson (Morand, 2000).Similarly, Erikson et alidentified measurable char-acteristics of the speech of witnesses in a court-room setting which were directly associated withthe witness?s level of social power (Erikson, 1978).Given, then, that there are distinct differencesamong what we term UpSpeak and DownSpeak,we treat Social Power Modeling as an instance oftext classification (or categorization): we seek toassign a class (UpSpeak or DownSpeak) to a textsample.
Closely related natural language process-ing problems are authorship attribution, sentimentanalysis, emotion detection, and personality classi-fication: all aim to extract higher-level informationfrom language.Authorship attribution in computational linguis-tics is the task of identifying the author of a text.The earliest modern authorship attribution workwas (Mosteller & Wallace, 1964), although foren-sic authorship analysis has been around muchlonger.
Mosteller and Wallace used statistical lan-guage-modeling techniques to measure the similar-ity of disputed Federalist Papers to samples ofknown authorship.
Since then, authorship identifi-cation has become a mature area productively ex-ploring a broad spectrum of features (stylistic,lexical, syntactic, and semantic) and many genera-tive and discriminative modeling approaches (Sta-matatos, 2009).
The generative models ofauthorship identification motivated our statisticallyextracted lexical and grammatical features, andfuture work should consider these language model-ing (a.k.a.
compression) approaches.Sentiment analysis, which strives to determinethe attitude of an author from text, has recentlygarnered much attention (e.g.
Pang, Lee, & Vai-thyanathan, 2002; Kim & Hovy, 2004; Breck, Choi774& Cardie, 2007).
For example, one problem isclassifying user reviews as positive, negative orneutral.
Typically, polarity lexicons (each term islabeled as positive, negative or neutral) help de-termine attitudes in text (Hiroya & Takamura,2005, Ravichandran 2009, Choi & Cardie 2009).The polarity of an expression can be determinedbased on the polarity of its component lexicalitems (Choi & Cardie 2008).
For example, the po-larity of the expression is determined by the major-ity polarity of its lexical items or by rules appliedto syntactic patterns of expressions on how to de-termine the polarity from its lexical components.McDonald et alstudied models that classify senti-ment on multiple levels of granularity: sentenceand document-level (McDonald, 2007).
Their workjointly classifies sentiment at both levels instead ofusing independent classifiers for each level or cas-caded classifiers.
Similar to our techniques, thesestudies determine the polarity of text based on itscomponent lexical and grammatical sequences.Unlike their works, our text classification tech-niques take into account the frequency of occur-rence of word n-grams and part-of-speech (POS)tag sequences, and other measures of statisticalsalience in training data.Text-based emotion prediction is another in-stance of text classification, where the goal is todetect the emotion appropriate to a text (Alm, Roth& Sproat, 2005) or provoked by an author, for ex-ample (Strapparava & Mihalcea, 2008).
Alm, Roth,and Sproat explored a broad array of lexical andsyntactic features, reminiscent of those of author-ship attribution, as well as features related to storystructure.
A Winnow-based learning algorithmtrained on these features convincingly predicted anappropriate emotion for individual sentences ofnarrative text.
Strapparava and Mihalcea try topredict the emotion the author of a headline intendsto provoke by leveraging words with known affec-tive sense and by expanding those words?
syno-nyms.
They used a Na?ve Bayes classifier trainedon short blogposts of known emotive sense.
Theknowledge engineering approaches were generallysuperior to the Na?ve Bayes approach.
Our ap-proach is corpus-driven like the Na?ve Bayes ap-proach, but we interject statistically driven featureselection between the corpus and the machinelearning classifiers.In personality classification, a person?s lan-guage is used to classify him on different personal-ity dimensions, such as extraversion or neuroticism(Oberlander & Nowson, 2006; Mairesse & Walker;2006).
The goal is to recover the more permanenttraits of a person, rather than fleeting characteris-tics such as sentiment or emotion.
Oberlander andNowson explore using a Na?ve Bayes and an SVMclassifier to perform binary classification of text oneach personality dimension.
For example, one clas-sifier might determine if a person displays a highor low level of extraversion.
Their attempt to clas-sify each personality trait as either ?high?
or ?low?echoes early sentiment analysis work that reducedsentiments to either positive or negative (Pang,Lee, & Vaithyanathan, 2002), and supports ini-tially treating Social Power Modeling as a binaryclassification task.
Personality classification seemsto be the application of text classification which isthe most relevant to Social Power Modeling.
AsMairesse and Walker note, certain personalitytraits are indicative of leaders.
Thus, the ability tomodel personality suggests an ability to model so-cial power lects as well.Apart from text classification, work from thetopic modeling community is also closely relatedto Social Power Modeling.
Andrew McCallum ex-tended Latent Dirichlet Allocation to model theauthor and recipient dependencies of per-messagetopic distributions with an Author-Recipient-Topic(ART) model (McCallum, Wang, & Corrada-Emmanuel, 2007).
This was the first significantwork to model the content and relationships ofcommunication in a social network.
McCallum etal applied ART to the Enron email corpus to showthat the resulting topics are strongly tied to role.They suggest that clustering these topic distribu-tions would yield roles and argue that the person-to-person similarity matrix yielded by this ap-proach has advantages over those of canonical so-cial network analysis.
The same authors proposedseveral Role-Author-Recipient-Topic (RART)models to model authors, roles and words simulta-neously.
With a RART modeling roles-per-word,they produced per-author distributions of generatedroles that appeared reasonable (e.g.
they labeledRole 10 as ?grant issues?
and Role 2 as ?naturallanguage researcher?
).We have a similar emphasis on statisticallymodeling language and interpersonal communica-775tion.
However, we model social power relation-ships, not roles or topics, and our approach pro-duces discriminative classifiers, not generativemodels, which enables more concrete evaluation.Namata, Getoor, and Diehl effectively appliedrole modeling to the Enron email corpus, allowingthem to infer the social hierarchy structure of En-ron (Namata et al, 2006).
They applied machinelearning classifiers to map individuals to their rolesin the hierarchy based on features related to emailtraffic patterns.
They also attempt to identify casesof manager-subordinate relationships within theemail domain by ranking emails using traffic-basedand content-based features (Diehl et al, 2007).While their task is similar to ours, our goal is toclassify any case in which one person has moresocial power than the other, not just identify in-stances of direct reporting.3 Approach3.1 Feature Set-UpPrevious work in traditional text classification andits variants ?
such as sentiment analysis ?
hasachieved successful results by using the bag-of-words representation; that is, by treating text as acollection of words with no interdependencies,training a classifier on a large feature set of wordunigrams which appear in the corpus.
However,our hypothesis was that this approach would not bethe best for SPM.
Morand?s study, for instance,identified specific features that correlate with thedirection of communication within a social hierar-chy (Morand, 2000).
Few of these tactics would beeffectively encapsulated by word unigrams.
Manywould be better modeled by POS tag unigrams(with no word information) or by longer n-gramsconsisting of either words, POS tags, or a combina-tion of the two.
?Uses subjunctive?
and ?Uses pasttense?
are examples.
Because considering suchfeatures would increase the size of the featurespace, we suspected that including these featureswould also benefit from algorithmic means of se-lecting n-grams that are indicative of particularlects, and even from binning these relevant n-grams into sets to be used as features.Therefore, we focused on an approach whereeach feature is associated with a set of one or moren-grams.
Each n-gram is a sequence of words, POStags or a combination of words and POS tags(?mixed?
n-grams).
Let S represent a set {n1, ?,nk} of n-grams.
The feature associated with S ontext T would be:1( , ) ( , )kiif S T freq n T==?where ( , )ifreq n T is the relative frequency (de-fined later) of in  in text T. Let in  represent thesequence 1 ms s?
where js  specifies either a wordor a POS tag.
Let T represent the text consisting ofthe sequence of tagged-word tokens 1 lt t?
.
( , )ifreq n T is then defined as follows:1( , ) ( , )i mfreq n T freq s s T= ?
{ }1 1: ( )1b b m p m b p pt t t sl m+ + ?
?
+?
== ?
+?where:( )( )i j ji ji j jword t s if s is a wordt stag t s if s is a tag=?
?= ?
?
=?
?To illustrate, consider the following feature set, abigram and a trigram (each term in the n-gram ei-ther has the form word or ^tag):{please ^VB, please ^?comma?
^VB}2The tag ?VB?
denotes a verb.
Suppose T consistsof the following tokenized and tagged text (sen-tence initial and final tokens are not shown):please^RB bring^VB the^DET report^NNto^TO our^PRP$ next^JJ weekly^JJ meet-ing^NN .^.The first n-gram of the set, please ^VB, wouldmatch please^RB bring^VB from the text.
The fre-quency of this n-gram in T would then be 1/9,where 1 is the number of substrings in T that match2 To distinguish a comma separating elements of a set with acomma as part of an ngram, we use ?comma?
to denote thepunctuation mark ?,?
as part of the ngram.776please ^VB and 9 is the number of bigrams in T,excluding sentence initial and final markers.
Theother n-gram, the trigram please ^?comma?
^VB,does not have any match, so the final value of thefeature is 1/9.Defining features in this manner allows us toboth explore the bag-of-words representation aswell as use groups of n-grams as features, whichwe believed would be a better fit for this problem.3.2 N-Gram SelectionTo identify n-grams which would be useful fea-tures, frequencies of n-grams in only the trainingset are considered.
Different types of frequencymeasures were explored to capture different typesof information about an n-gram?s usage.
These are:?
Absolute frequency: The total number oftimes a particular n-gram occurs in the textof a given class (social power lect).?
Relative frequency: The total number oftimes a particular n-gram occurs in a givenclass, divided by the total number of n-grams in that class.
Normalization by thesize of the class makes relative frequency abetter metric for comparing n-gram usageacross classes.We then used the following frequency-based met-rics to select n-grams:?
We set a minimum threshold for the abso-lute frequency of the n-gram in a class.This helps weed out extremely infrequentwords and spelling errors.?
We require that the ratio of the relativefrequency of the n-gram in one class to itsrelative frequency in the other class is alsogreater than a threshold.
This is a simplemeans of selecting n-grams indicative oflect.In experiments based on the bag-of-words model,we only consider an absolute frequency threshold,whereas in later experiments, we also take into ac-count the relative frequency ratio threshold.3.3 N-gram BinningIn experiments in which we bin n-grams, selectedn-grams are assigned to the class in which theirrelative frequency is highest.
For example, an n-gram whose relative frequency in UpSpeak text istwice that in DownSpeak text would be assigned tothe class UpSpeak.N-grams assigned to a class are then partitionedinto sets of n-grams.
Each of these sets of n-gramsis associated with a feature.
This partition is basedon the n-gram type, the length of n-grams and therelative frequency ratio of the n-grams.
While then-grams composing a set may themselves be in-dicative of social power lects, this method ofgrouping them makes no guarantees as to how in-dicative the overall set is.
Therefore, we experi-mented with filtering out sets which had anegligible information gain.
Information gain is aninformation theoretic concept measuring howmuch the probability distributions for a feature dif-fer among the different classes.
A small informa-tion gain suggests that a feature may not beeffective at discriminating between classes.Although this approach to partitioning is simpleand worthy of improvement, it effectively reducedthe dimensionality of the feature space.3.4 ClassificationOnce features are selected, a classifier is trained onthese features.
Many features are weak on theirown; they either occur rarely or occur frequentlybut only hint weakly at social information.
There-fore, we experimented with classifiers friendly toweak features, such as Adaboost and Logistic Re-gression (MaxEnt).
However, we generallyachieved the best results using support vector ma-chines, a machine learning classifier which hasbeen successfully applied to many previous textclassification problems.
We used Weka?s opti-mized SVMs (SMO) (Witten 2005, Platt 1998) anddefault parameters, except where noted.4 Evaluation4.1 DataTo validate our supervised learning approach, wesought an adequately large English corpus of per-son-to-person communication labeled with theground truth.
For this, we used the publicly avail-777able Enron corpus.
After filtering for duplicatesand removing empty or otherwise unusable emails,the total number of emails is 245K, containingroughly 90 million words.
However, this total in-cludes emails to non-Enron employees, such asfamily members and employees of other corpora-tions, emails to multiple people, and emails re-ceived from Enron employees without a knowncorporate role.
Because the author-recipient rela-tionships of these emails could not be established,they were not included in our experiments.Building upon previous annotation done on thecorpus, we were able to ascertain the corporate role(CEO, Manager, Employee, etc.)
of many emailauthors and recipients.
From this information, wedetermined the author-recipient relationship byapplying general rules about the structure of a cor-porate hierarchy (an email from an Employee to aCEO, for instance, is UpSpeak).
This annotationmethod does not take into account promotions overtime, secretaries speaking on behalf of their super-visors, or other causes of relationship irregularities.However, this misinformation would, if anything,generally hurt our classifiers.The emails were pre-processed to eliminate textnot written by the author, such as forwarded textand email headers.
As our approach requires text tobe POS-tagged, we employed Stanford?s POS tag-ger (http://nlp.stanford.edu/software/tagger.shtml).In addition, text was regularized by conversion tolower case and tokenized to improve counts.To create training and test sets, we partitionedthe authors of text from the corpus into two sets: Aand B.
Then, we used text authored by individualsin A as a training set and text authored by indi-viduals in B as a test set.
The training set is used todetermine discriminating features upon which clas-sifiers are built and applied to the test set.
WeTable 1.
Author-based Training and Test partitions.
Thenumber of author-recipient pairs (links) and the numberof words in text labeled as UpSpeak and DownSpeakare shown.found that partitioning by authors was necessary toavoid artificially inflated scores, because the clas-sifiers pick up aspects of particular authors?
lan-guage (idiolect) in addition to social power lectinformation.
It was not necessary to account forrecipients because the emails did not contain textfrom the recipients.
Table 1 summarizes the textpartitions.Because preliminary experiments suggested thatsmaller text samples were harder to classify, theclassifiers we describe in this paper were bothtrained and tested on a subset of the Enron corpuswhere at least 500 words of text was communi-cated from a specific author to a specific recipient.This subset contained 142 links, 40% of whichwere used as the test set.Weighting for Cost-Sensitive Learning: Theoriginal corpus was not balanced: the number ofUpSpeak links was greater than the number ofDownSpeak links.
Varying the weight given totraining instances is a technique for creating a clas-sifier that is cost-sensitive, since a classifier builton an unbalanced training set can be biased to-wards avoiding errors on the overrepresented class(Witten, 2005).
We wanted misclassifying Up-Speak as DownSpeak to have the same cost as mis-classifying DownSpeak as UpSpeak.
To do this,we assigned weights to each instance in the train-ing set.
UpSpeak instances were weighted less thanDownSpeak instances, creating a training set thatwas balanced between UpSpeak and DownSpeak.Balancing the training set generally improved re-sults.Weighting the test set in the same manner al-lowed us to evaluate the performance of the classi-fier in a situation in which the numbers ofUpSpeak and DownSpeak instances were equal.
Abaseline classifier that always predicted the major-ity class would, on its own, achieve an accuracy of74% on UpSpeak/DownSpeak classification ofunweighted test set instances with a minimumlength of 500 words.
However, results on theweighted test set are properly compared to a base-line of 50%.
We include both approaches to scor-ing in this paper.4.2 UpSpeak/DownSpeak ClassifiersIn this section, we describe experiments on classi-fication of interpersonal email communication intoUpSpeak and DownSpeak.
For these experiments,only emails exchanged between two people relatedby a superior/subordinate power relationship wereUpSpeak DownSpeakLinks Words Links WordsTraining 431 136K 328 63KTest 232 74K 148 27K778Table 2.
Experiment Results.
Accuracies/F-Scores with an SVM classifier for 10-fold cross validation on theweighted training set and evaluation against the weighted and unweighted test sets.
Note that the baseline accu-racy against the unweighted test set is 74%, but 50% for the weighted test set and cross-validation.Human-Engineered Features: Before examin-ing the data itself, we identified some featureswhich we thought would be predictive of UpSpeakor DownSpeak, and which could be fairly accu-rately modeled by mixed n-grams.
These featuresincluded the use of different types of imperatives.We also thought that the type of greeting or sig-nature used in the email might be reflective offormality, and therefore of UpSpeak and Down-Speak.
For example, subordinates might be morelikely to use an honorific when addressing a supe-rior, or to sign an email with ?Thanks.?
We pre-formed some preliminary experiments using thesefeatures.
While the feature set was too small toproduce notable results, we identified which fea-tures actually were indicative of lect.
One suchfeature was polite imperatives (imperatives pre-ceded by the word ?please?).
The polite imperativefeature was represented by the n-gram set:{please ^VB, please ^?comma?
^VB}.Unigrams and Bigrams: As a different sort ofbaseline, we considered the results of a bag-of-words based classifier.
Features used in these ex-periments consist of single words which occurred aminimum of four times in the relevant lects (Up-Speak and DownSpeak) of the training set.
Theresults of the SVM classifier, shown in line (1) ofTable 2, were fairly poor.
We then performed ex-periments with word bigrams, selecting as featuresthose which occurred at least seven times in therelevant lects of the training set.
This threshold forbigram frequency minimized the difference in thenumber of features between the unigram and bi-gram experiments.
While the bigrams on their ownwere less successful than the unigrams, as seen inline (2), adding them to the unigram features im-proved accuracy against the test set, shown in line(3).As we had speculated that including surface-level grammar information in the form of tag n-grams would be beneficial to our problem, we per-formed experiments using all tag unigrams and alltag bigrams occurring in the training set as fea-tures.
The results are shown in line (4) of Table 2.The results of these experiments were not particu-larly strong, likely owing to the increased sparsityof the feature vectors.Binning: Next, we wished to explore longer n-grams of words or POS tags and to reduce thesparsity of the feature vectors.
We therefore ex-perimented with our method of binning the indi-vidual n-grams to be used as features.
We binnedfeatures by their relative frequency ratios.
In addi-tion to binning, we also reduced the total numberof n-grams by setting higher frequency thresholdsand relative frequency ratio thresholds.When selecting n-grams for this experiment, weconsidered only word n-grams and tag n-grams ?not mixed n-grams, which are a combination ofwords and tags.
These mixed n-grams, while usefulfor specifying human-defined features, largely in-creased the dimensionality of the feature searchspace and did not provide significant benefit inpreliminary experiments.
For the word sequences,Cross-Validation Test Set(weighted)Test Set(unweighted)Features # offeatures# ofn-gramsAcc (%) F-score Acc (%) F-score Acc (%) F-score(1) Word unigrams 3899 3899 55.4 .481 62.1 .567 78.9 .748(2) Word bigrams 3740 3740 54.5 .457 56.4 .498 73.7 .693(3) Word unigrams +word bigrams7639 7639 51.8 .398 63.3 .576 80.7 .762(4) (3) + tag unigrams+ tag bigrams9014 9014 51.8 .398 58.8 .515 77.2 .719(5) Binned n-grams 8 106 83.0 .830 78.1 .781 77.2 .783(6) N-grams from (5),separated106 106 83.0 .828 60.5 .587 70.2 .698(7) (5) + politeimperatives9 108 83.9 .839 77.1 .771 78.9 .797779we set an absolute frequency threshold that de-pended on class.
The frequency of a word n-gramin a particular class was required to be 0.18 *nrlinks / n, where nrlinks is the number of links ineach class (431 for UpSpeak and 328 for Down-Speak), and n is the number of words in the class.The relative frequency ratio was required to be atleast 1.5.
The tag sequences were required to meetan absolute frequency threshold of 20, but thesame relative frequency ratio of 1.5.Binning the n-grams into features was donebased on both the length of the n-gram and the rel-ative frequency ratio.
For example, one featuremight represent the set of all word unigrams whichhave a relative frequency ratio between 1.5 and1.6.We explored possible feature sets with cross va-lidation.
Before filtering for low information gain,we used six word n-gram bins per class (relativefrequency ratios of 1.5, 1.6 ..., 1.9 and 2.0+), onetag n-gram bin for UpSpeak (2.0+), and three tagn-gram bins for DownSpeak (2.0+, 5.0+, 10.0+).Even with the weighted training set, DownSpeakinstances were generally harder to identify andlikely benefited from additional representation.Grouping features by length was a simple but arbi-trary method for reducing dimensionality, yetsometimes produced small bins of otherwise goodfeatures.
Therefore, as we explored the featurespace, small bins of different n-gram lengths weremerged.
We then employed Weka?s InfoGain fea-ture selection tool to remove those features with alow information gain3, which removed all but eightfeatures.
The results of this experiment are shownin line (5) of Table 2.
It far outperforms the bag-of-words baselines, despite significantly fewer fea-tures.To ascertain which feature reduction method hadthe greatest effect on performance ?
binning orsetting a relative frequency ratio threshold ?
weperformed an experiment in which all the n-gramsthat we used in the previous experiment were theirown features.
Line (6) of Table 2 shows that whilethis approach is an improvement over the basicbag-of-words method, grouping features still im-proves results.3 In Weka, features (?attributes?)
with a sufficiently low in-formation gain have this value rounded down to ?0?
; these arethe features we removed.Our goal was to have successful results usingonly statistically extracted features; however, weexamined the effect of augmenting this feature setwith the most indicative of the human-identifiedfeature ?
polite imperatives.
The results, in line (7),show a slight improvement in both the cross vali-dation accuracy, and the accuracy against the un-weighted test set increases to 78.9%4.
However,among the weighted test sets, the highest accuracywas 78.1%, with the features in line (5).We report the scores for cross-validation on thetraining set for these features; however, becausethe features were selected with knowledge of theirper-class distribution in the training set, thesecross-validation scores should not be seen as theclassifier?s true accuracy.Self-Training: Besides sparse feature vectors,another factor likely to be hurting our classifierwas the limited amount of training data.
We at-tempted to increase the training set size by per-forming exploratory experiments with self-training, an iterative semi-supervised learning me-thod (Zhu, 2005) with the feature set from (7).
Onthe first iteration, we trained the classifier on thelabeled training set, classified the instances of theunlabeled test set, and then added the instances ofthe test set alng with their predicted class to thetraining set to be used for the next iteration.
Afterthree iterations, the accuracy of the classifier whenevaluated on the weighted test set improved to82%, suggesting that our classifiers would benefitfrom more data.Impact of Cost-Sensitive Learning: Withoutcost-sensitive learning, the classifiers were heavilybiased towards UpSpeak, tending to classify bothDownSpeak and UpSpeak test instances as Up-Speak.
With cost-sensitive training, overall per-formance improved and classifier performance onDownSpeak instances improved dramatically.
In(5) of Table 2, DownSpeak  classifier accuracyeven edged out the accuracy for UpSpeak.
Weexpect that on a larger dataset behavior with un-weighted training and test data would improve.5 Conclusions and Future ResearchWe presented a corpus-based statistical learningapproach to modeling social power relationshipsand experimental results for our methods.
To our4 The associated p-value is 6.56E-6.780knowledge, this is the first corpus-based approachto learning social power lects beyond those in di-rect reporting relationships.Our work strongly suggests that statistically ex-tracted features are an efficient and effective ap-proach to modeling social information.
Ourmethods exploit many aspects of language use andeffectively model social power information whileusing statistical methods at every stage to tease outthe information we seek, significantly reducinglanguage-, culture-, and lect-specific engineeringneeds.
Our feature selection method picks up onindicators suggested by sociolinguistics, and it alsoallows for the identification of features that are notobviously characteristic of UpSpeak or Down-Speak.
Some easily recognizable features include:Lect Ngram ExampleUpSpeak if you ?Let me know if you need any-thing.?
?Please call me if you have anyquestions.
?Down-Speakgive me ?Read this over and give me acall.?
?Please give me your commentsnext week.
?On the other hand, other features are less intuitive:Lect Ngram ExampleUpSpeak I?ll, we?ll ?I?ll let you know the final re-sults soon?
?Everyone is very excited [?
]and we?re confident we?ll besuccessful?DownSpeak that is,this is?Neither does any other groupbut that is not my problem?
?I think this is an excellent let-ter?We hope to improve our methods for selectingand binning features with information theoreticselection metrics and clustering algorithms.We also have begun work on 3-way, UpSpeak/DownSpeak/PeerSpeak classification.
Training amulticlass SVM on the binned n-gram featuresfrom (5) produces 51.6% cross-validation accu-racy on training data and 44.4% accuracy on theweighted test set (both numbers should be com-pared to a 33% baseline).
That classifier containedno n-gram features selected from the PeerSpeakclass.
Preliminary experiments incorporatingPeerSpeak n-grams yield slightly better numbers.However, early results also suggest that the three-way classification problem is made more tractablewith cascaded two-way classifiers; feature selec-tion was more manageable with binary problems.For example, one classifier determines whether aninstance is UpSpeak; if it is not, a second classifierdistinguishes between DownSpeak and PeerSpeak.Our text classification problem is similar to senti-ment analysis in that there are class dependencies;for example, DownSpeak is more closely related toPeerSpeak than to UpSpeak.
We might attempt toexploit these dependencies in a manner similar toPang and Lee (2005) to improve three-way classi-fication.In addition, we had promising early results forclassification of author-recipient links with 200 to500 words, so we plan to explore performance im-provements for links of few words.In early, unpublished work, we had promisingresults with generative model-based approach toSPM, and we plan to revisit it; language modelsare a natural fit for lect modeling.
Finally, we hopeto investigate how SPM and SNA can enhance oneanother, and explore other lect classification prob-lems for which the ground truth can be found.AcknowledgmentsDr.
Richard Sproat contributed time, valuable in-sights, and wise counsel on several occasions dur-ing the course of the research.
Dr. Lillian Lee andher students in Natural Language Processing andSocial Interaction reviewed the paper, offeringvaluable feedback and helpful leads.Our colleague, Diane Bramsen, created an ex-cellent graphical interface for probing and under-standing the results.
Jeff Lau guided and advisedthroughout the project.We thank our anonymous reviewers for prudentadvice.This work was funded by the Army StudiesBoard and sponsored by Col. Timothy Hill of theUnited Stated Army Intelligence and SecurityCommand (INSCOM) Futures Directorate undercontract W911W4-08-D-0011.ReferencesCecilia Ovesdotter Alm, Dan Roth and Richard Sproat.2005.
Emotions from text: machine learning for text-based emotion prediction.
HLT/EMNLP 2005.
Octo-ber 6-8, 2005, Vancouver.781Penelope Brown and Stephen C. Levinson.
1987.
Po-liteness: Some universals in language usage.
Cam-bridge: Cambridge University Press.Eric Breck, Yejin Choi and Claire Cardie.
2007.
Identi-fying expressions of opinion in context.In Proceedings of the Twentieth International JointConference on Artificial Intelligence (IJCAI-2007)CALO Project.
2009.
Enron E-Mail Dataset.http://www.cs.cmu.edu/~enron/.Yejin Choi and Claire Cardie.
2008.
Learning withcompositional semantics as structural inference forsubsentential sentiment analysis.
Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing.
Honolulu, Hawaii: ACM.
793-801.Yejin Choi and Claire Cardie.
2009.
Adapting a polaritylexicon using integer linear programming for domain-specific sentiment classification.
Empirical Methodsin Natural Language Processing (EMNLP).Christopher P. Diehl, Galileo Namata, and Lise Getoor.2007.
Relationship identification for social networkdiscovery.
AAAI '07: Proceedings of the 22nd Na-tional Conference on Artificial Intelligence.Bonnie Erickson, et al 1978.
Speech style and impres-sion formation in a court setting: The effects of 'pow-erful?
and 'powerless' speech.
Journal of ExperimentalSocial Psychology 14: 266-79.Norman Fairclough.
1989.
Language and power.
Lon-don: Longman.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA data mining software: An update.SIGKDD Exploration (1): Issue 1.JHU Center for Imaging Science.
2005.
Scan Statisticson Enron Graphs.
http://cis.jhu.edu/~parky/Enron/Soo-min Kim and Eduard Hovy.
2004.
Determining theSentiment of Opinions.
Proceedings of the COLINGConference.
Geneva, Switzerland.Francois Mairesse and Marilyn Walker.
2006.
Auto-matic recognition of personality in conversation.
Pro-ceedings of HLT-NAACL.
New York City, New York.Galileo Mark S. Namata Jr., Lise Getoor, and Christo-pher P. Diehl.
2006.
Inferring organizational titles inonline communication.
ICML 2006, 179-181.Andrew McCallum, Xuerui Wang, and Andres Corrada-Emmanuel.
2007.
Topic and role discovery in socialnetworks with experiments on Enron and academic e-Mail.
Journal of Artificial Intelligence Research 29.Ryan McDonald, Kerry Hannan, Tyler Neylon, MikeWells, and Jeff Reynar.
2007.
Structured models forfine-to-coarse sentiment analysis.
Proceedings of theACL.David Morand.
2000.
Language and power: An empiri-cal analysis of linguistic strategies used in supe-rior/subordinate communication.
Journal ofOrganizational Behavior, 21:235-248.Frederick Mosteller and David L. Wallace.
1964.
Infer-ence and disputed authorship: The Federalist.
Addi-son-Wesley, Reading, Mass.Jon Oberlander and Scott Nowson.
2006.
Whose thumbis it anyway?
Classifying author personality from we-blog text.
Proceedings of CoLing/ACL.
Sydney, Aus-tralia.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
Sentiment classification using ma-chine learning techniques.
Proceedings of EMNLP,79?86.Bo Pang and Lillian Lee.
2005.
Seeing stars: Exploitingclass relationships for sentiment categorization withrespect to rating scales.
Proceedings of the ACL.John Platt.
1998.
Sequential minimal optimization: Afast algorithm for training support vector machines.
InTechnical Report MST-TR-98-14.
Microsoft Re-search.Delip Rao and Deepak Ravichandran.
2009.
Semi-supervised polarity lexicon induction.
EuropeanChapter of the Association for Computational Lin-guistics.Efstathios Stamatatos.
2009.
A survey of modern au-thorship attribution methods.
JASIST 60(3): 538-556.Carol Strapparava and Rada Mihalcea.
2008.
Learningto identify emotions in text.
SAC 2008: 1556-1560Hiroya Takamura, Takashi Inui, and Manabu Okumura.2005.
Semantic Orientations of Words using SpinModel.
Annual Meeting of the Association for Com-putational Linguistics.Ian H. Witten and Eibe Frank.
2005.
Data Mining:Practical Machine Learning Tools and Techniques.Morgan Kauffman.Xiaojin Zhu.
2005.
Semi-supervised learning literaturesurvey.
Technical Report 1530, Department of Com-puter Sciences, University of Wisconsin, Madison.782
