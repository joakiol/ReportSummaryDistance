Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 962?973, Dublin, Ireland, August 23-29 2014.Improving Cloze Test Performance of Language Learners Using Web N-GramsMartin Potthast Matthias Hagen Anna Beyer Benno SteinBauhaus-Universit?t Weimar, Germany<first name>.<last name>@uni-weimar.deAbstractWe study the effectiveness of search engines for common usage, a new category of search enginesthat exploit n-gram frequencies on the web to measure the commonness of a formulation, and thatallow their users to submit wildcard queries about formulation uncertainties often encountered inthe process of writing.
These search engines help to resolve questions on common prepositionsfollowing verbs, common synonyms in given contexts, and word order difficulties, to name onlya few.
Until now, however, it has never been shown that search engines for common usage havea positive impact on writing performance.Our contribution is a large-scale user study with 121 participants using the Netspeak searchengine to shed light on this issue for the first time.
Via carefully designed cloze tests we showthat second language learners who have access to a search engine for common usage significantlyand effectively improve their test performance as opposed to not using them.1 IntroductionWhen writing texts in a second language, uncertainties on specific formulations regularly come up.
Evenexperienced second language writers may sometimes be in doubt about the preposition following a verbor what word order to choose.
In this paper, we study search engines for common usage (usage searchengines, for short) that aim at assisting second language writers to cope with their uncertainties.
Thesesearch engines allow for phrasal queries that include wildcards at positions where a user is not sure whatto write.
The search results typically consist of a list of phrases matching the query?s expression?thewildcards filled with formulations.
The returned phrases are ranked by their commonness of being used ineveryday writing, where a phrase?s commonness is estimated by its occurrence frequency in a collectionof web n-grams.
The occurrence frequencies are usually not hidden from the user but displayed alongsideeach phrase, either implicitly or explicitly.
This way, the users of usage search engines have a way ofjudging whether a phrase is commonly used by others.
Figure 1 (left) shows an example search result.Target audience of usage search engines is language learners who have mastered basic vocabularyand grammar but whose language proficiency in terms of their feeling for language usage is still worsethan that of a native speaker.
Until recently, there has been hardly any technological support for them,so they could only resort to studying abstract style guides, consuming foreign language media, andlanguage study travels in order to improve their usage skills.
Today, three public usage search enginesare available.
The first one, called Netspeak (Stein et al., 2010), is developed at our group since 2008.It was followed by PhraseUp and Linggle (Boisson et al., 2013), which have been released in 2011and 2013.1Moreover, there is Google?s N-Gram Viewer prototype (Michel et al., 2011), which has adifferent purpose and target audience but visualizes n-gram usage over time.All of these search engines provide a way to quantify the commonness of a phrase and thus have thepotential to become important tools for second language learners.
That is, if they work as advertised.Until now, it has not at all been clear whether writers can actually benefit from the information distilledfrom analyzing n-gram occurrence frequencies, or whether they are easily misled, for example, by noisyThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedingsfooter are added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1Netspeak is freely available at www.netspeak.org, PhraseUp at www.phraseup.com, and Linggle at www.linggle.com.962Figure 1: Netspeak?s two alternative interfaces: search results can either be displayed as textual rankedlist of phrases alongside frequencies (left), or as WordGraph visualization (right) (Riehmann et al., 2011),where the frequencies determine various aspects of the visualization.
The WordGraph is particularlysuited to handling multiple wildcards per query.
The participants of our user study used primarily thetextual interface, since they did not require more than one or two wildcards for solving the cloze tests.data.
Our contribution is to shed light on this issue for the first time and to conduct a large-scale user studywith 121 language learners aged 14?18, measuring their performance when using our Netspeak searchengine to solve cloze tests.
The study ascertains the positive impact of Netspeak and by extension, usagesearch engines in general; moreover, it shows the low barrier to entry of Netspeak?s user interface.The paper is organized as follows: after a detailed discussion of related work in Section 2, Netspeak?sretrieval engine is formally described in Section 3 as background for the design of our user study and asan example of how such search engines work internally.
Section 4 reports on our user study and providesa statistical analysis of our findings.
The paper closes with a conclusion and an outlook into future work.2 Related WorkCarrying out research and development on usage search engines is an interdisciplinary effort that requiresexpertise from information retrieval, information visualization and interface design, as well as domainknowledge from computer linguistics.
Therefore, we divide our review of related work into four parts:(1) existing search engines and web services, (2) retrieval engines and wildcard search from the perspec-tive of information retrieval, (3) search result visualization, and, (4) writing support systems dedicatedto second language writers.2.1 Public Search Engines and Web ServicesThere are currently three public search engines and one public prototype that fall into the category ofsearch engines for common usage, namely Netspeak (Stein et al., 2010), PhraseUp, Linggle (Boissonet al., 2013), and the Google N-Gram Viewer (Michel et al., 2011).
All of them index large n-gramcorpora, and their search interfaces are primarily dedicated to returning results that allow their users tojudge the commonness of a phrase compared to alternative phrases.
We distinguish the former threesearch engines from the latter mainly by its target audience.
While the former target average web users,the latter targets professional linguists and humanities researchers.
To the best of our knowledge, ourpaper is the first to investigate the effectiveness of such search engines for the use case of assistingwriters, thereby underpinning these efforts.Moreover, a number of other linguistic search engines are available, such as WebCorp Live (Kehoeand Renouf, 2002), WebAsCorpus (Fletcher, 2007), and the Linguist?s Search Engine (Resnik and Elkiss,2005).
These search engines cannot be readily used for usage search as defined above, since they workmore like concordancers in that they only retrieve usage examples and present them in context, disregard-ing usage commonness.
Again, their target audience is professional linguists rather than laymen users,let alone second language learners.
While they may still be applied in the context of language learning,the search interfaces of these search engines are not sufficiently tailored to this domain.963Another category of related web services that are readily available to second language learners includestyle and grammar checkers, such as Grammarly, PaperRater, SlickWrite, AfterTheDeadline (Mudge,2010), the Hemingway App, GrammarBase, etc.
From what can be said by analyzing their features, allof these services are based on a collection of basic style and grammar rules that can be checked automati-cally with some degree of confidence in their recommendations.
However, none of the services we foundmake any recommendations with regard to usage commonness, i.e., they do not identify uncommonformulations or make recommendations for more common ones.2.2 Information Retrieval Models and Indexes for Wildcard SearchThe retrieval models employed by usage search engines are hardly ever discussed in the literature citedabove.
One of the few exceptions is Netspeak (Stein et al., 2010), where the retrieval model has been acontribution in itself since it is tailored specifically to its application domain.
For the lack of discussionof the finer details of how the above search engines work, it can be assumed that they do not employ aspecifically tailored retrieval approach.
Nevertheless, when reviewing the information retrieval literaturefor retrieval models that support linguistic queries or wildcard queries, a number of sources can be found.Cafarella et al.
(2005, 2007) study indexing methods that are particularly suited to support queriescomprising parts-of-speech as wildcards.
They introduce so-called neighborhood indexes whose diskaccesses required to answer a query are on the order of the number of non-wildcard terms in a query.Rafiei and Li (2009) develop a wildcard search engine that supports linguistically rich wildcards inorder to support information extraction from the web, which employs a preprocessor for queries, and apostprocessor for search results on top of a traditional web search engine.
The approach does not createa tailored index but translates the wildcard queries into flat queries that can be answered by traditionalsearch engines.
Sekine (2008) explores the trie data structure as an alternative to inverted indexes whenindexing large-scale n-gram corpora.
The approach is limited to short n-grams (n < 10) to be feasible,which can be a strong point in terms of retrieval speed.
Netspeak?s retrieval engine is also intentionallyrestricted to small values of n, but uses minimal perfect hash functions instead of tries to maximizeretrieval performance.While all of the aforementioned approaches support shallow linguistic wildcards, or only basic wild-cards, Tsang and Chawla (2011) propose a method to support regular expressions.
Doing so involvesvarious trade-offs between retrieval performance and index size.
Further, a search engine like this maybe only useful to experts, but not second language learners.
Again, all of the aforementioned contri-butions target either professional linguists or they are meant to facilitate automatic usage, instead ofsupporting average writers.2.3 Visualization of Usage Search ResultsAn important part of every search engine is its user interface.
Since usage search engines are still in theirinfancy, their user interfaces have not been studied in-depth, so far.
As a first attempt to close this gap,we developed and analyzed two alternative user interfaces for Netspeak in a previous work, one textualinterface and one using a tailored visualization that was specifically developed for usage search engines,the so-called WordGraph (Riehmann et al., 2011).
Figure 1 shows them side-by-side.
The textual inter-face displays search results in the form of a tabular list, where each row lists an n-gram matching thewildcard query alongside its absolute and relative occurrence frequency.
If a query comprises more thanone wildcard, situations arise where this linear ranking of n-grams is insufficient to grasp the true distri-bution of formulations that may be used instead of the wildcards.
The WordGraph therefore visualizesthe search results as a horizontal graph, so that the i-th word of an n-gram is displayed as a node onthe i-th level of the graph.
Paths from left to right through the graph correspond to n-grams found inthe result set returned by Netspeak.
A user study that investigated the fitness of the WordGraph to serveas a user interface for specific search tasks found that study participants prefer the WordGraph over thetextual user interface when the number of wildcards increases (Riehmann et al., 2012).
The user studywe report on in this paper is based solely on the textual user interface, since most of our cloze tests canbe solved by using one wildcard.9642.4 Writing Support for Second Language Learners?For writers of English as a Second Language (ESL), useful editorial assistance geared to their needs issurprisingly hard to come by,?
and ?[...]
there has been remarkably little progress in this area over the lastdecade,?
observe Brockett et al.
(2006) about the state of the art.
This is despite the fact that English isthe second language of most people who speak English today.2A recent overview of technology to detectgrammatical errors of language learners is given by Leacock et al.
(2010), whereas computer feedback forsecond language learners is mostly studied within pedagogical research under the label of computer-aidedlanguage learning (CALL).
There, classroom systems are being deployed on a small scale to measuretheir effects on student learning performance.
The development of usage search engines in general, ourNetspeak engine in particular, and the user study contributed in this paper may be considered first stepstoward the development of new, better technologies that specifically target the needs of second languagelearners and writers.3 Netspeak: A Search Engine for Common Usage Based on Web N-GramsAs a background for our user study and as an example of how usage search engines work internally, thissection briefly describes Netspeak and its retrieval engine.3The main building block of Netspeak is aquery processor tailored to the following task: given a wildcard query q and a set D of n-grams, retrievethose n-grams Dq?
D that match the pattern defined by q.
To solve this task, we have developedan index-based wildcard query processor addressing the three steps indexing, retrieval, and filtering, asillustrated in Figure 2 (middle).3.1 Query LanguageNetspeak utilizes a query language defined by the EBNF grammar shown in Figure 2 (left).
A query is asequence of literal words and wildcard operators, wherein the literal words must occur in the expressionsought after, while the wildcard operators allow to specify uncertainties.
Currently five operators aresupported:?
the question mark (?
), which matches exactly one word;?
the asterisk (*), which matches any sequence of words;?
the tilde sign in front of a word (?<word>), which matches any of the word?s synonyms;?
the multiset operator ({<words>}), which matches any ordering of the enumerated words; and,?
the optionset operator ([<words>]), which matches any one word from a list of options.The textual interface displays the search results for the given query as a ranked list of phrases, orderedby decreasing absolute and relative occurrence frequencies.
This way, the user can find confidence inchoosing a particular phrase by judging both its absolute and relative frequencies.
For example, a phrasemay have a low relative frequency but a high absolute frequency, or vice versa, which in both casesindicates that the phrase is not the worst of all choices.
Furthermore, the textual web interface offersexample sentences for each phrase, which are retrieved on demand when clicking on a plus sign next toa phrase.
This allows users who are still in doubt to get an idea of the larger context of a phrase.3.2 Retrieval EngineThe indexing step is done offline.
Let V denote the set of all words found in the n-grams D, andlet D?denote the set of integer references to the storage positions of the n-grams in D on hard disk.During indexing, an inverted index ?
: V ?
P(D )?
is built that maps each word w ?
V to a sortedlist ?
(w) ?
D ,?
where ?
(w) is comprised of exactly all references to the n-grams in D that contain w.2http://en.wikipedia.org/wiki/English language#Geographical distribution3Extended versions of this section can be found in previous publications on Netspeak?s WordGraph visualization (Riehmannet al., 2011; Riehmann et al., 2012).965EBNF grammar of Netspeak?s query languagequery = { word | wildcard }51word = ( [apostrophe] ( letter { alpha } ) ) | ?
, ?letter = ?
a ?
| ... | ?
z ?
| ?
A ?
| ... | ?
Z ?alpha = letter | ?
0 ?
| ... | ?
9 ?apostrophe = ?
?
?wildcard = ?
?
?
| ?
* ?
| synonyms | multiset | optionsetsynonyms = ?
~ ?
wordmultiset = ?
{ ?
word { word } ?}
?optionset = ?
[ ?
word { word } ? ]
?Netspeak's retrieval engineRetrieval FilteringInvertedindex ?Webn-grams D?w?q  ?
(w) = ?q DqDq'qSequentialaccessRandomaccessIndexingonlineofflinerotate aboutaroundonceontheaxisythe zon itstheitsanits own<empty>a verticalFrequency128,176      63.7%36,615      18.2%10,390        5.2%4,091        2.0%3,941        2.0%3,323        1.7%3,110        1.5%2,574        1.3%Phrasei am waiting fori am waiting toi am waiting oni am waiting.i am waiting,i am waiting impatientlyi am waiting uri am waiting untilFigure 2: Netspeak at a glance (Riehmann et al., 2012): the left table shows Netspeak?s query languageas an EBNF grammar, the middle figure overviews its retrieval engine, and the right figure shows anexample of search results as shown to its users.
Given a query q, the intersection of relevant postlistsyields a tentative postlist ?q, which then is filtered and presented as a ranked list.
The index ?
exploitsessential characteristics that are known a-priori about possible queries and the n-gram set D.The list ?
(w) is referred to as posting list or postlist.
Since D is invariant, ?
can be implemented asan external hash table with O(1)-access to ?(w).
For ?
being space-optimal, a minimal perfect hashfunction based on the CHD algorithm is employed (Belazzougui et al., 2009).The two online steps, retrieval and filtering, are taken successively when answering a query q. Withinthe retrieval step, a tentative postlist ?q=?w?q?
(w) is constructed; ?qis the complete set of referencesto n-grams in D that contain all words in q.
The computation of ?qis done in increasing order of postlistlength, whereas each ?
(w) is read sequentially from hard disk.
Within the filtering step, a pattern matcheris compiled from q, and Dqis constructed as the set of those n-grams referenced in ?qthat are acceptedby the pattern matcher.
Constructing Dqrequires random hard disk access.
Basically, this approachcorresponds to how web search engines retrieve documents for a given keyword query before rankingthem.
In what follows, we briefly outline how the search in D is significantly narrowed down.With an inverted index that also stores specific n-gram information along with the keywords, thefiltering of ?qcan be avoided.
In this regard, we distinguish the queries that can be formulated withNetspeak?s query language into two classes: fixed-length queries and variable-length queries.
A fixed-length query contains only wildcard operators that represent an a-priori known number of words, whilea variable-length query contains at least one wildcard operator that expands to a variable number ofwords.
For example, the query fine ?
me is a fixed-length query since only 3-grams in D match thispattern, while the query fine*me is a variable-length query since n-grams of length 2, 3, 4, .
.
.
match.Obviously, fixed-length queries can be answered with less filtering effort than variable-length queries:simply checking an n-gram?s length suffices to discard many non-matching queries.
The query processorfirst reformulates a variable-length query into a set of fixed-length queries, which then are processed inparallel, merging the results.Moreover, the retrieval engine employs pruning strategies so that only relevant parts of a postlistare read during retrieval, presuming sorted postlists.
Head pruning means to start reading a postlist atsome entry within, without compromising recall.
Given a query q, let ?
denote an upper bound for thefrequencies of the n-grams in q?s result set Dq, i.e., d ?
Dqimplies f(d) ?
?
.
Obviously, in all postliststhat are involved within the construction of Dq, all entries whose n-gram frequencies are above ?
cansafely be skipped, whereas ?
is determined in a preprocessing step as the lowest occurrence frequency ofa sub-sequence of q that does not include wildcards.
Up to this point, the retrieval of n-grams matchinga query q is exact?but, not all n-grams that match a query are of equal importance.
We consider thisfact by applying tail pruning for postlists that are too long to be read at once into main memory.
As aconsequence, less frequent n-grams that might match a given query can be missed.3.3 The Web n-Gram CollectionTo provide relevant suggestions, a wide cross-section of written text on the web is required.
Currently,Netspeak indexes the Google n-gram corpus ?Web 1T 5-gram Version 1?
(Brants and Franz, 2006),966which consists of 42 GB of phrases up to a length of n = 5 words along with their occurrence frequencieson the web in 2006.
This corpus has been compiled from approximately 1 trillion words extracted fromthe English portion of the web, totaling more than 3 billion n-grams.
Two post-processing steps wereapplied: case reduction and vocabulary filtering.
For the latter, a white list vocabulary V was compiledand only n-grams whose words appear in V were retained.
The vocabulary V consists of the wordsfound in the Wiktionary and various other dictionaries, complemented by words from the 1-gram portionof the Google corpus whose occurrence frequency exceeds 10 000.
After post-processing, the size of thecorpus has been reduced by about 54%.3.4 Retrieval Performance in Practice and Public AvailabilityIn practice, the described techniques enable Netspeak to provide search results at a speed similar tomodern web search engines.
Results are usually returned within a couple of milliseconds.
Whenever auser stops typing for more than 300 milliseconds, the current input is submitted as an ?instant?
querywithout need for a click.
That way, the ?search experience?
with Netspeak is similar to what users expectfrom web search engines.Netspeak is freely available online and has about 300 distinct users on a working day who submit about2500 queries (half the workload on weekends).
Most of its users are returning users.
From their feedbackand from our own experience, we know that Netspeak helps to resolve uncertainties on formulations inthe daily process of writing papers, proposals, etc.
However, in the following section we attempt tocapture Netspeak?s effectiveness in a controlled user study.4 User Study on the Effectiveness of Usage Search EnginesIt is generally assumed that usage search engines are useful, say, that they provide valuable feedback thatleads to improved writing.
To empirically confirm this ?usefulness?
assumption, we conduct systematictests with experienced language learners and analyze whether a usage search engine enables them toimprove their writing.
We choose Netspeak as a representative of usage search engines for our study.Our study?s underlying rationale is to model the use case of usage search engines by solving clozetests.
In a cloze test, a word or a phrase is removed from a sentence and the participant has to replacethe missing words.
Although we followed standard procedures on constructing cloze tests (Sachs et al.,1997), it should be noted that our usage of cloze tests is not as originally intended (Taylor, 1953).
Wedo not assess a language learner?s reading skills, but use the cloze test to model word choice, whichresembles the use case of usage search engines very well.
For each participant, we provide two differentcloze test questionnaires.
The first has to be solved without any help, whereas for the second, participantsare allowed to use the search engine.
Besides evaluating the answers, we also analyze the submittedsearch queries.4.1 Experiment 1: General Usage, Average LearnersIn the first experiment, we examine whether the search engine in general can support users in resolvinguncertainties on formulations modeled by cloze tests.
Our hypothesis is that using a usage search enginehelps to improve a human?s performance in such tests.Experimental Design To test our hypothesis, we conduct an empirical study with a within-subjectsdesign (Lazar et al., 2010).
This means that our participants are exposed to a cloze test without the helpof a search engine and then to another cloze test where our chosen usage search engine is allowed.The to-be-solved cloze tests are carefully constructed under the guidance of a university-level Englishteacher who is a native English speaker.
From several language learner textbooks, we selected questionsin order to have an equal mix of two easy, four medium, and three hard questions for two different clozetest questionnaires A and B (see Appendix A and B).In order to have objectively comparable test cases, the English teacher provided four possible answersfor each of the nine questions from test A and B, from which participants had to choose one in each case.This way, the participants do not have to rely on their subjective own vocabulary knowledge.967Table 1: Results of our user study on the impact of usage search engines on language learners.Experiment Question Questions answereddifficulty manually with search engine availablebut not used and usedX ?
?
sum X ?
?
X ?
?
sumeasy 17 41 0 58 7 2 1 42 6 0 58Average medium 61 100 3 164 25 16 1 88 34 0 164Learners hard 37 72 2 111 4 22 2 18 62 1 111all 115 213 5 333 36 40 4 149 102 1 333Highly easy 11 5 0 16 10 1 0 4 1 0 16Experienced medium 27 17 0 44 24 2 0 14 3 1 44Learners hard 18 12 0 30 8 8 0 4 10 0 30all 56 34 0 90 42 11 0 22 14 1 90easy 147 29 1 177 28 2 1 135 11 0 177Specific medium 117 57 3 177 20 6 1 123 24 3 177Operators hard 135 40 2 177 31 5 2 130 18 1 177all 399 126 6 531 79 13 4 378 53 4 531Search engine not used Search engine usedExperiment Search engine used vs not usedp-value effect sizeAverage Learners 0.0000 0.73 largeHighly Exp.
Learners 0.7030 0.12 smallSpecific Operators 0.0000 0.58 largeIn the left table,Xdenotes correct answers,?
denotes wrong answers, and ?
denotesunanswered questions.To evaluate the statistical significance and theeffect size, we distinguished cloze test answersfor the conditions ?Search engine not used?
and?Search engine used?
in the left table.The brackets below the bottom row of the lefttable indicate which cases fall under whatcondition.The English teacher first chose the questions independent of knowing the indexed n-grams of thesearch engine.
In a ?postprocessing?
step, the chosen answers for the questions are checked for existencein the n-gram vocabulary of the search engine.
This always was the case, although sometimes the queriesrequired to retrieve them were different from the exact context around the cloze test?s missing word.
Thischeck ensured that there was a chance of answering each individual question in the cloze tests with thesearch engine.During the experiment, the use or non-use of the search engine is the independent variable.
Thedependent variable is the number of correct answers per questionnaire.
There also are confoundingvariables like whether our engine really was used when it was allowed, the time needed to type queries,or the different numbers of answered questions with and without the search engine.
We will furtherelaborate on how we deal with these variables in the following description of the experimental process.Experimental Process From three different local high schools, 43 German pupils (23 female, 20 male;mean age 16.2, SD = 1.2) with five or more years of English courses participated in six groups.
Noneof the participants had any previous experience with any usage search engine.When a group arrived in our lab, they were randomly assigned to a lab seat; questionnaire A or Bwere distributed ensuring that neighboring participants had a different question set.
This way, the testdistribution was random and the participants could not collaborate (which was also ensured by theiraccompanying ?watchdog?
teachers).
After seven minutes, the first questionnaires were collected anda short five minute introduction to the search engine and its operator set was given.
To ensure thatthe pupils really followed the introduction, we provided the chance of winning small prices based oncorrectly answering a question on the underlying technique of usage search engines?the index?in anexit questionnaire.
After that, each participant had to solve the opposite questionnaire (A when the firstwas B, and vice versa) but was allowed to use the search engine this time.
In pilot studies, we noticedthat pupils of that age often need a lot of time for typing their search queries on a standard keyboard.Thus, we allowed 10 minutes for the second questionnaire.
This confounding variable of different timingfor the questionnaires could not be avoided.
Otherwise, most participants would not have had the chanceto complete all questions.
In order to check whether our participants actually used the search engine, welogged their querying behavior and manually identified the questions which they had answered withoutusing the search engine.Results and Discussion Since not all participants answered all questions for both cloze tests, we ex-cluded the six participants from the following analyses, who had a difference of more than one betweenthe number of answered questions for either test.The aggregated numbers on questionnaire performance for the remaining 37 individuals are given inthe first block of rows of Table 1 (?Average Learners?).
Note that the ratio of correct vs. incorrect answersgoes up when the search engine was used: on average, an individual answered two more questionscorrectly.
Especially interesting is that the short five minute introduction was sufficient for that effect968which shows the strength of the textual interface.
To statistically estimate the per-individual effect, wecompare the ratio of correct answers among all answers when the search engine was used to the ratiowhen it was not used (note that this includes the questions where the engine was allowed but was not used;i.e., columns ?manually?
and ?but not used?
in Table 1).
According to the Shapiro-Wilk test (Razali andWah, 2011), the individual participants?
ratios are not normally distributed for either condition (engineused vs. not used) such that we choose a non-parametric significance test (Lazar et al., 2010).
For ourwithin-subjects design with ratio data and two to-be-compared samples, the Wilcoxon signed rank testis known as a suitable significance test (Lazar et al., 2010).
For the 37 participants?
ratios we get a p-value below 0.001 and thus can reject the null hypothesis that the ratios?
distributions are equal.
Furtherestimating the effect size for the Wilcoxon signed rank test, we obtain a value of 0.73 which correspondsto a large effect (Cohen, 1988; Fritz et al., 2012).
This result supports our prediction that the searchengine can help resolve writing uncertainties.We also studied the query logs of our participants.
Per cloze test question, they submitted 4?5 querieswith 2?3 terms on average (a wildcard is counted as a term).
The last query in each such ?search session?for a single question typically was 3?4 terms long.
Almost all participants only used the ?-operator andmost participants chose the strategy of querying with context before and after the operator.
Having onlycontext before or only after the operator are less successful strategies with higher error ratios.4.2 Experiment 2: General Usage, Highly Experienced LearnersIn our neighborhood, there also is an international high school, where German pupils have all theirclasses taught in English.
Obviously, such pupils have a much higher experience speaking and writingEnglish than our participants from Experiment 1.
For a second experiment, we invited pupils from theinternational school to our lab.
Our hypothesis is that the pupils from the international school will haveto use the search engine less frequently but still can benefit from it for individual questions.Experimental Design and Process We used the same questionnaires, time constraints, and loggingstrategies as in Experiment 1.
From the international school, 12 German pupils (7 female, 5 male; meanage 16.5, SD = 0.7) participated in two groups.
These pupils are taught all their courses in Englishfor five and more years.
None of them had any previous experience with usage search engines.
Theexperimental process was as in Experiment 1.Results and Discussion Again, not all participants answered all questions for both cloze tests; we ex-cluded the two participants from the following analyses, who had a difference of more than one betweenthe number of answered questions for either test.The aggregated numbers on questionnaire performance for the remaining 10 individuals are given inthe second block of rows of Table 1 (?Highly Experienced Learners?).
As expected, the highly experi-enced pupils used the search engine very rarely.
This is not too surprising since our questionnaires weredesigned with an average German pupil in mind; many questions seemed too easy to the internationalswhich they also indicated in their exit questionnaires.
Still, on a per-question basis, for the medium anddifficult questions where the pupils used the search engine, they slightly improved their performance.However, the sample and the effect size are too small to draw any reliable conclusions.The experiment shows that the highly experienced pupils indeed did not use our engine often.
How-ever, the predicted benefit for them cannot be confirmed from our small sample.
It is thus an interestingopen task to conduct a larger study with highly experienced users and more difficult questions.4.3 Experiment 3: Specific Operators, Average LearnersOur first experiment revealed that most participants used the ?-operator to solve the tasks.
We thusdesigned a third experiment specifically targeted at the options, synonyms, and word-order operators ofour Netspeak search engine.
Our hypothesis is that each individual operator helps improve a human?sperformance in cloze tests targeted at the individual operator.Experimental Design As in Experiment 1, we asked the university-level English teacher to designtwo cloze test questionnaires (see Appendix C and D); for each operator with an easy, a medium, and969a hard question.
Here, the questions for the option operator are of a similar kind as the questions fromExperiment 1.
Four alternatives are given, but the participants are asked to use the option operator [] andnot the ?-operator.
For synonyms, a complete sentence is given and for a specified word, the best amongfour given potential synonyms is requested.
As for the word order operator, a two-word phrase is missingfrom the sentence and the two different word orders are provided as options.
Like in Experiment 1 and 2,the explicit answer options ensure that the test is objective and not subjective.
In a second developmentstep, the questions were checked for solvability using the search engine just like in Experiment 1.Experimental Process From three different local schools, 66 pupils (45 female, 21 male; meanage 15.9, SD = 1.4) participated in six groups.
None of the pupils participated in Experiment 1 or 2 norhad they any previous experience with usage search engines.
These pupils have learned English in theirschools for at least five years.
The schedule was similar to Experiment 1 with an emphasis on the threetested operators in the introductory explanations on Netspeak.
In the questionnaires, the pupils wereasked to use only the specific operator for the respective queries.
Logging their queries, we are able toexclude solutions obtained by using a not-allowed operator.Results and Discussion Again, not all participants answered all questions for both cloze tests; weexcluded the seven participants from the following analyses, who had a difference of more than onebetween the number of answered questions for either test.The aggregated numbers on questionnaire performance for the remaining 59 individuals are given inthe third block of rows of Table 1 (?Specific Operators?).
Note that the ratio of correct vs. incorrectanswers goes up when the search engine was used: one to two more questions correctly answered onaverage.
As in Experiment 1, the short five minute introduction is sufficient for that effect which showsthe strength of our interface.
To statistically estimate the per-individual effect, we compare the ratioof correct answers among all answers when the search engine was used to the ratio when it was notused (note that this includes the questions where the engine was allowed but was not used; i.e., columns?manually?
and ?but not used?
in Table 1).
For the 59 participants?
ratios, we get a p-value below 0.001and thus can reject the null hypothesis that the ratios?
distributions are equal.
Further, estimating theeffect size for the Wilcoxon signed rank test, we obtain a value of 0.58 which corresponds to a largeeffect (Cohen, 1988; Fritz et al., 2012).
Again, the result supports our prediction that usage searchengines can help resolve writing uncertainties.However, a deeper analysis reveals that the large effect is due to the synonym operator.
Only forthat operator, a statistically significant performance difference and a large effect size can be shown.
Forthe other two operators, the null hypothesis of no performance difference cannot be rejected.
This isin line with the exit questionnaire findings, where the pupils reported the synonym operator to be veryhelpful while the other questions were perceived as rather easy.
In the query log analyses, we found thatcontext before and after the wildcard had a similarly positive effect as before and was generally betterthan adding context only before the wildcard.5 Conclusion and Future WorkSearch engines for common usage have the potential to become an important tool for second languagewriters and learners.
The possibility to check one?s language against what is commonly written forms aunique opportunity to improve one?s writing on-the-fly.
Such information has not been available at scaleso far.
Our user study shows that usage search engines can indeed help second language writers solveuncertainties about formulations.
Modeling writing uncertainties by carefully designed cloze tests, weare able to show a significant improvement when experienced language learners use the search engine.Highly experienced language learners represented by our study participants from an internationalschool, however, did not use the search engine often enough to draw meaningful conclusions.
Thiscan probably be attributed to the fact that the cloze tests were not tailored to their level of language pro-ficiency.
Therefore, the question of whether also highly experienced writers and learners, or even nativespeakers, can benefit from such search engines remains open and is left for future work.Another missing piece in determining the effectiveness of usage search engines is whether their users970actually learn something while using them, or whether users frequently submit the same or similar queriesagain and again.
Our user study was not designed to answer this question, since our participants wereonly around for about 30 minutes for organizational reasons.
Even measuring effects on short-termmemory is rendered infeasible in this time frame.
A longitudinal study would be ideal, in this case, butwe also see an exciting, data-driven way to approach this.
By analyzing the query logs of Netspeak,which is currently being used hundreds of times per day, we can track returning users.
We can then studytheir online search behavior to determine if and how often they return to submit similar queries, whichallows us to draw conclusions about their learning success.
More generally, the query logs of usagesearch engines may form a unique opportunity to observe language learners ?in the wild?
as opposed tothe laboratory.Finally, regarding the user interface of usage search engines, our user study has revealed ways toimprove them.
For example, the interface must be optimized for faster typing (especially on mobile de-vices) as we observed that the pupils were not adept to entering special characters on standard keyboards,which resulted in slow typing speed.
Besides this, our user study also showed that the current state ofNetspeak?s textual user interface as well as the simplified wildcard query language is easy enough tobe understood in less than a minute by any newcomer, which demonstrates the low barrier to entry thatsearch engines for common usage have right now.AcknowledgementsWe thank the anonymous participants of our user study as well as Tim Gollub, Martin Trenkmann,Michael V?lske, Howard Atkinson, Johannes Kiesel, Matthias Busse, and Alexander Herr for their helpin organizing the user study.ReferencesDjamal Belazzougui, Fabiano C. Botelho, and Martin Dietzfelbinger.
2009.
Hash, Displace, and Compress.
InProceedings of ESA 2009, pages 682?693.Joanne Boisson, Ting-Hui Kao, Jian-Cheng Wu, Tzu-Hsi Yen, and Jason S. Chang.
2013.
Linggle: A Web-scaleLinguistic Search Engine for Words in Context.
In Proceedings of ACL 2013 (Demos), pages 139?144.Thorsten Brants and Alex Franz.
2006.
Web 1T 5-gram Version 1.
Linguistic Data Consortium LDC2006T13.Chris Brockett, William B. Dolan, and Michael Gamon.
2006.
Correcting ESL Errors Using Phrasal SMT Tech-niques.
In Proceedings of ACL 2006, pages 249?256.Michael J. Cafarella and Oren Etzioni.
2005.
A Search Engine for Natural Language Applications.
In Proceedingsof WWW 2005, pages 442?452.Michael J. Cafarella, Christopher Re, Dan Suciu, and Oren Etzioni.
2007.
Structured Querying of Web Text Data:A Technical Challenge.
In Proceedings of CIDR 2007, pages 225?234.Jacob Cohen.
1988.
Statistical Power Analysis for the Behavioral Sciences.
Psychology Press.William H. Fletcher.
2007.
Implementing a BNC-Compare-able Web Corpus.
In Proceedings of the 3rd Web asCorpus Workshop, pages 43?56.Catherine O. Fritz, Peter E. Morris, and Jennifer J. Richler.
2012.
Effect Size Estimates: Current Use, Calculations,and Interpretation.
Journal of Experimental Psychology: General, 141(1):2.Andrew Kehoe and Antoinette Renouf.
2002.
WebCorp: Applying the Web to Linguistics and Linguistics to theWeb.
In Proceedings of WWW 2002 (Posters).Jonathan Lazar, Jinjuan Heidi Feng, and Harry Hochheiser.
2010.
Research Methods in Human-Computer Inter-action.
Wiley Publishing.Claudia Leacock, Martin Chodorow, Michael Gamon, and Joel Tetreault.
2010.
Automated Grammatical ErrorDetection for Language Learners.
Morgan and Claypool Publishers.Jean-Baptiste Michel, Yuan K. Shen, Aviva P. Aiden, Adrian Veres, Matthew K. Gray, The Google Books Team,Joseph P. Pickett, Dale Hoiberg, Dan Clancy, Peter Norvig, Jon Orwant, Steven Pinker, Martin A. Nowak,and Erez L. Aiden.
2011.
Quantitative Analysis of Culture Using Millions of Digitized Books.
Science,331(6014):176?182.971Raphael Mudge.
2010.
The Design of a Proofreading Software Service.
In Proceedings of HLT 2010 Workshopon Computational Linguistics and Writing, pages 24?32.Davood Rafiei and Haobin Li.
2009.
Data Extraction from the Web Using Wild Card Queries.
In Proceedings ofCIKM 2009, pages 1939?1942.Nornadiah Mohd Razali and Yap Bee Wah.
2011.
Power Comparisons of Shapiro-Wilk, Kolmogorov-Smirnov,Lilliefors and Anderson-Darling Tests.
Journal of Statistical Modeling and Analytics, 2(1):21?33.Philip Resnik and Aaron Elkiss.
2005.
The Linguist?s Search Engine: An Overview.
In Proceedings of ACL 2005(Posters and Demos), pages 33?36.Patrick Riehmann, Henning Gruendl, Bernd Froehlich, Martin Potthast, Martin Trenkmann, and Benno Stein.2011.
The NETSPEAK WORDGRAPH: Visualizing Keywords in Context.
In Proceedings of PacificVis 2011,pages 123?130.Patrick Riehmann, Henning Gruendl, Martin Potthast, Martin Trenkmann, Benno Stein, and Bernd Froehlich.2012.
WORDGRAPH: Keyword-in-Context Visualization for NETSPEAK?s Wildcard Search.
IEEE Transac-tions on Visualization and Computer Graphics, 18(9):1411?1423.J.
Sachs, P. Tung, and R.Y.H.
Lam.
1997.
How to Construct a Cloze Test: Lessons from Testing MeasurementTheory Models.
Perspectives, 9:145?160.Satoshi Sekine.
2008.
A Linguistic Knowledge Discovery Tool: Very Large N -gram Database Search withArbitrary Wildcards.
In Proceedings of COLING 2008 (Demos), pages 181?184.Benno Stein, Martin Potthast, and Martin Trenkmann.
2010.
Retrieving Customary Web Language to AssistWriters.
In Proceedings of ECIR 2010, pages 631?635.W.
L. Taylor.
1953.
Cloze Procedure: A New Tool for Measuring Readability.
Journalism Quarterly, 30:415?433.Dominic Tsang and Sanjay Chawla.
2011.
A Robust Index for Regular Expression Queries.
In Proceedings ofCIKM 2011, pages 2365?2368.AppendixA Questionnaire A from Experiments 1 and 21.
I really prefer just anything watching television.?
against X to ?
about ?
on2.
Has Tony?s new book yet?X come out ?
published ?
developed ?
drawn up3.
If this plan off, I promise you you?ll get the credit for it.?
lets ?
goes ?
gets X comes4.
Helen had great admiration her history teacher.?
in ?
to X for ?
on5.
I just couldn?t over how well the team played!X get ?
turn ?
make ?
put6.
The problem stems the government?s lack of action.?
out X from ?
under ?
for7.
It?s too late to phone Jill at work, at any .?
case ?
time ?
situation X rate8.
I?m afraid I?m not very good children.?
about ?
for X with ?
at9.
We are no obligation to change goods which were not purchased here.?
with X under ?
to ?
atB Questionnaire B from Experiments 1 and 21.
Don?t worry about the lunch.
I?ll to it.?
look ?
prepare ?
care X see2.
I am afraid that these regulations have to be with.?
provided X complied ?
faced ?
met3.
Our thoughts on our four missing colleagues.?
based X centred ?
laid ?
depended4.
Carol doesn?t have a very good relationship her mother.X with ?
at ?
for ?
to9725.
It seems to be your boss who is fault in this case.?
under ?
with X at ?
for6.
Being rich doesn?t count much on a desert island.?
on ?
to ?
of X for7.
The policeman me off with a warning as it was Christmas.?
sent ?
gave X let ?
set8.
Tina is an authority Byzantine architecture.X on ?
for ?
with ?
in9.
I was the impression that you liked Indian food.?
at ?
with ?
of X underC Questionnaire A from Experiment 3Choose the word which fits best using the options operator [<words>].1.
If you spend so much money every day, you will out of money before the end of the month.?
pay ?
use X run ?
take2.
You need to take all your other clothes before you put on your swimming costume.?
down ?
away ?
out X off3.
I?m afraid I?m not very good history.?
about ?
for X at ?
withChoose the best synonym for the underlined word using the synonym operator ?<word>.4.
I love studying geometry the most.?
hate ?
absent X enjoy ?
difficult5.
My ambition is to become a computer scientist.?
thought ?
reward ?
study X dream6.
Your action will have serious consequences.X effects ?
events ?
reasons ?
affectsChoose the correct word order using the word order operator {<words>}.7.
The bird!
I?m going to help it!X poor little ?
little poor8.
She was wearing a dress.?
green beautiful X beautiful green9.
I plan on wearing my coat.X long black ?
black longD Questionnaire B from Experiment 3Choose the word which fits best using the options operator [<words>].1.
Sometimes Julia speaks very quickly so the other students have to ask her to slow .X down ?
up ?
out ?
off2.
The missing plane has apparently disappeared without a .?
sign ?
news ?
word X trace3.
When Gabriel?s credit card stopped, he cut it many small pieces.?
out X into ?
apart ?
inChoose the best synonym for the underlined word using the synonym operator ?<word>.4.
I choose to study the differences between alligators and crocodiles.?
make ?
buy X prefer ?
wash5.
I cannot find my money.
Can you get me my billfold?X wallet ?
pocket ?
watch ?
bag6.
This is a very rough environment for elephants to live in.X harsh ?
abrasive ?
coarse ?
beneficialChoose the correct word order using the word order operator {<words>}.7.
She sold the chairs at a yard sale.?
wooden old X old wooden8.
The years were fantastic.?
two first X first two9.
It?s close to the building.X big blue ?
blue big973
