Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1104?1112,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsActive Learning with Efficient Feature Weighting Methodsfor Improving Data Quality and Classification AccuracyJustin Martineau1, Lu Chen2?, Doreen Cheng3, and Amit Sheth41,3Samsung Research America, Silicon Valley1,375 W Plumeria Dr. San Jose, CA 95134 USA2,4Kno.e.sis Center, Wright State University2,43640 Colonel Glenn Hwy.
Fairborn, OH 45435 USA1,3{justin.m, doreen.c}@samsung.com2,4{chen, amit}@knoesis.orgAbstractMany machine learning datasets are noisywith a substantial number of mislabeledinstances.
This noise yields sub-optimalclassification performance.
In this paperwe study a large, low quality annotateddataset, created quickly and cheaply us-ing Amazon Mechanical Turk to crowd-source annotations.
We describe compu-tationally cheap feature weighting tech-niques and a novel non-linear distributionspreading algorithm that can be used to it-eratively and interactively correcting mis-labeled instances to significantly improveannotation quality at low cost.
Eight dif-ferent emotion extraction experiments onTwitter data demonstrate that our approachis just as effective as more computation-ally expensive techniques.
Our techniquessave a considerable amount of time.1 IntroductionSupervised classification algorithms require anno-tated data to teach the machine, by example, howto perform a specific task.
There are generally twoways to collect annotations of a dataset: througha few expert annotators, or through crowdsourc-ing services (e.g., Amazon?s Mechanical Turk).High-quality annotations can be produced by ex-pert annotators, but the process is usually slowand costly.
The latter option is appealing since itcreates a large annotated dataset at low cost.
Inrecent years, there have been an increasing num-ber of studies (Su et al, 2007; Kittur et al, 2008;Sheng et al, 2008; Snow et al, 2008; Callison-Burch, 2009) using crowdsourcing for data anno-tation.
However, because annotators that are re-cruited this way may lack expertise and motiva-tion, the annotations tend to be more noisy and?This author?s research was done during an internshipwith Samsung Research America.unreliable, which significantly reduces the perfor-mance of the classification model.
This is a chal-lenge faced by many real world applications ?given a large, quickly and cheaply created, lowquality annotated dataset, how can one improveits quality and learn an accurate classifier fromit?Re-annotating the whole dataset is too expen-sive.
To reduce the annotation effort, it is desirableto have an algorithm that selects the most likelymislabeled examples first for re-labeling.
The pro-cess of selecting and re-labeling data points can beconducted with multiple rounds to iteratively im-prove the data quality.
This is similar to the strat-egy of active learning.
The basic idea of activelearning is to learn an accurate classifier using lesstraining data.
An active learner uses a small set oflabeled data to iteratively select the most informa-tive instances from a large pool of unlabeled datafor human annotators to label (Settles, 2010).
Inthis work, we borrow the idea of active learning tointeractively and iteratively correct labeling errors.The crucial step is to effectively and efficientlyselect the most likely mislabeled instances.
An in-tuitive idea is to design algorithms that classifythe data points and rank them according to thedecreasing confidence scores of their labels.
Thedata points with the highest confidence scores butconflicting preliminary labels are most likely mis-labeled.
The algorithm should be computationallycheap as well as accurate, so it fits well with ac-tive learning and other problems that require fre-quent iterations on large datasets.
Specifically,we propose a novel non-linear distribution spread-ing algorithm, which first uses Delta IDF tech-nique (Martineau and Finin, 2009) to weight fea-tures, and then leverages the distribution of DeltaIDF scores of a feature across different classesto efficiently recognize discriminative features forthe classification task in the presence of misla-beled data.
The idea is that some effective fea-1104tures may be subdued due to label noise, and theproposed techniques are capable of counteractingsuch effect, so that the performance of classifica-tion algorithms could be less affected by the noise.With the proposed algorithm, the active learner be-comes more accurate and resistant to label noise,thus the mislabeled data points can be more easilyand accurately identified.We consider emotion analysis as an interest-ing and challenging problem domain of this study,and conduct comprehensive experiments on Twit-ter data.
We employ Amazon?s Mechanical Turk(AMT) to label the emotions of Twitter data, andapply the proposed methods to the AMT datasetwith the goals of improving the annotation qualityat low cost, as well as learning accurate emotionclassifiers.
Extensive experiments show that, theproposed techniques are as effective as more com-putational expensive techniques (e.g, Support Vec-tor Machines) but require significantly less timefor training/running, which makes it well-suitedfor active learning.2 Related WorkResearch on handling noisy dataset of mislabeledinstances has focused on three major groups oftechniques: (1) noise tolerance, (2) noise elimi-nation, and (3) noise correction.Noise tolerance techniques aim to improvethe learning algorithm itself to avoid over-fittingcaused by mislabeled instances in the trainingphase, so that the constructed classifier becomesmore noise-tolerant.
Decision tree (Mingers,1989; Vannoorenberghe and Denoeux, 2002) andboosting (Jiang, 2001; Kalaia and Servediob,2005; Karmaker and Kwek, 2006) are two learn-ing algorithms that have been investigated in manystudies.
Mingers (1989) explores pruning methodsfor identifying and removing unreliable branchesfrom a decision tree to reduce the influence ofnoise.
Vannoorenberghe and Denoeux (2002) pro-pose a method based on belief decision trees tohandle uncertain labels in the training set.
Jiang(2001) studies some theoretical aspects of regres-sion and classification boosting algorithms in deal-ing with noisy data.
Kalaia and Servediob (2005)present a boosting algorithm which can achievearbitrarily high accuracy in the presence of datanoise.
Karmaker and Kwek (2006) propose a mod-ified AdaBoost algorithm ?
ORBoost, which min-imizes the impact of outliers and becomes moretolerant to class label noise.
One of the main dis-advantages of noise tolerance techniques is thatthey are learning algorithm-dependent.
In con-trast, noise elimination/correction approaches aremore generic and can be more easily applied tovarious problems.A large number of studies have explored noiseelimination techniques (Brodley and Friedl, 1999;Verbaeten and Van Assche, 2003; Zhu et al, 2003;Muhlenbach et al, 2004; Guan et al, 2011), whichidentifies and removes mislabeled examples fromthe dataset as a pre-processing step before build-ing classifiers.
One widely used approach (Brod-ley and Friedl, 1999; Verbaeten and Van Assche,2003) is to create an ensemble classifier that com-bines the outputs of multiple classifiers by eithermajority vote or consensus, and an instance istagged as mislabeled and removed from the train-ing set if it is classified into a different class thanits training label by the ensemble classifier.
Thesimilar approach is adopted by Guan et al (2011)and they further demonstrate that its performancecan be significantly improved by utilizing unla-beled data.
To deal with the noise in large ordistributed datasets, Zhu et al (2003) propose apartition-based approach, which constructs clas-sification rules from each subset of the dataset,and then evaluates each instance using these rules.Two noise identification schemes, majority andnon-objection, are used to combine the decisionfrom each set of rules to decide whether an in-stance is mislabeled.
Muhlenbach et al (2004)propose a different approach, which representsthe proximity between instances in a geometricalneighborhood graph, and an instance is consid-ered suspect if in its neighborhood the proportionof examples of the same class is not significantlygreater than in the dataset itself.Removing mislabeled instances has beendemonstrated to be effective in increasing theclassification accuracy in prior studies, but thereare also some major drawbacks.
For example,useful information can be removed with noiseelimination, since annotation errors are likely tooccur on ambiguous instances that are potentiallyvaluable for learning algorithms.
In addition,when the noise ratio is high, there may not beadequate amount of data remaining for buildingan accurate classifier.
The proposed approachdoes not suffer these limitations.Instead of eliminating the mislabeled examples1105from training data, some researchers (Zeng andMartinez, 2001; Rebbapragada et al, 2012; Lax-man et al, 2013) propose to correct labeling er-rors either with or without consulting human ex-perts.
Zeng and Martinez (2001) present an ap-proach based on backpropagation neural networksto automatically correct the mislabeled data.
Lax-man et al (2012) propose an algorithm which firsttrains individual SVM classifiers on several small,class-balanced, random subsets of the dataset, andthen reclassifies each training instance using a ma-jority vote of these individual classifiers.
How-ever, the automatic correction may introduce newnoise to the dataset by mistakenly changing a cor-rect label to a wrong one.In many scenarios, it is worth the effort andcost to fix the labeling errors by human experts,in order to obtain a high quality dataset that canbe reused by the community.
Rebbapragada et al(2012) propose a solution called Active Label Cor-rection (ALC) which iteratively presents the ex-perts with small sets of suspected mislabeled in-stances at each round.
Our work employs a sim-ilar framework that uses active learning for datacleaning.In Active Learning (Settles, 2010) a small set oflabeled data is used to find documents that shouldbe annotated from a large pool of unlabeled doc-uments.
Many different strategies have been usedto select the best points to annotate.
These strate-gies can be generally divided into two groups: (1)selecting points in poorly sampled regions, and (2)selecting points that will have the greatest impacton models that were constructed using the dataset.Active learning for data cleaning differs fromtraditional active learning because the data alreadyhas low quality labels.
It uses the difference be-tween the low quality label for each data point anda prediction of the label using supervised machinelearning models built upon the low quality labels.Unlike the work in (Rebbapragada et al, 2012),this paper focuses on developing algorithms thatcan enhance the ability of active learner on identi-fying labeling errors, which we consider as a keychallenge of this approach but ALC has not ad-dressed.3 An Active Learning Framework forLabel CorrectionLet?D = {(x1, y1), ..., (xn, yn)} be a dataset ofbinary labeled instances, where the instance xibe-longs to domain X , and its label yi?
{?1,+1}.
?D contains an unknown number of mislabeleddata points.
The problem is to obtain a high-quality dataset D by fixing labeling errors in?D,and learn an accurate classifier C from it.Algorithm 1 illustrates an active learning ap-proach to the problem.
This algorithm takes thenoisy dataset?D as input.
The training set T isinitialized with the data in?D and then updatedeach round with new labels generated during re-annotation.
Data sets Srand S are used to main-tain the instances that have been selected for re-annotation in the whole process and in the currentiteration, respectively.Data: noisy data?DResult: cleaned data D, classifier CInitialize training set T =?D ;Initialize re-annotated data sets Sr= ?
;S = ?
;repeatTrain classifier C using T ;Use C to select a set S of m suspectedmislabeled instances from T ;Experts re-annotate the instances inS ?
(Sr?
S) ;Update T with the new labels in S ;Sr= Sr?
S; S = ?
;until for I iterations;D = T ;Algorithm 1: Active Learning Approach for La-bel CorrectionIn each iteration, the algorithm trains classifiersusing the training data in T .
In practice, we ap-ply k-fold cross-validation.
We partition T into ksubsets, and each time we keep a different subsetas testing data and train a classifier using the otherk ?
1 subsets of data.
This process is repeated ktimes so that we get a classifier for each of the ksubsets.
The goal is to use the classifiers to ef-ficiently and accurately seek out the most likelymislabeled instances from T for expert annotatorsto examine and re-annotate.
When applying a clas-sifier to classify the instances in the correspond-ing data subset, we get the probability about howlikely one instance belongs to a class.
The top minstances with the highest probabilities belongingto some class but conflicting preliminary labels areselected as the most likely errors for annotators tofix.
During the re-annotation process we keep theold labels hidden to prevent that information from1106biasing annotators?
decisions.
Similarly, we keepthe probability scores hidden while annotating.This process is done with multiple iterations oftraining, sampling, and re-annotating.
We main-tain the re-annotated instances in Srto avoid an-notating the same instance multiple times.
Aftereach round of annotation, we compare the old la-bels to the new labels to measure the degree of im-pact this process is having on the dataset.
We stopre-annotating on the Ith round after we decide thatthe reward for an additional round of annotation istoo low to justify.4 Feature Weighting MethodsBuilding the classifier C that allows the mostlikely mislabeled instances to be selected and an-notated is the essence of the active learning ap-proach.
There are two main goals of developingthis classifier: (1) accurately predicting the labelsof data points and ranking them based on predic-tion confidence, so that the most likely errors canbe effectively identified; (2) requiring less time ontraining, so that the saved time can be spent on cor-recting more labeling errors.
Thus we aim to builda classifier that is both accurate and time efficient.Labeling noise affects the classification accu-racy.
One possible reason is that some effectivefeatures that should be given high weights are in-hibited in the training phase due to the labelingerrors.
For example, emoticon ?:D?
is a good in-dicator for emotion happy, however, if by mis-take many instances containing this emoticon arenot correctly labeled as happy, this class-specificfeature would be underestimated during training.Following this idea, we develop computationallycheap feature weighting techniques to counteractsuch effect by boosting the weight of discrimina-tive features, so that they would not be subduedand the instances with such features would havehigher chance to be correctly classified.Specifically, we propose a non-linear distribu-tion spreading algorithm for feature weighting.This algorithm first utilizes Delta IDF to weigh thefeatures, and then non-linearly spreads out the dis-tribution of features?
Delta IDF scores to exagger-ate the weight of discriminative features.
We firstintroduce Delta-IDF technique, and then describeour algorithm of distribution spreading.
Since wefocus on n-gram features, we use the words featureand term interchangeably in this paper.4.1 Delta IDF Weighting SchemeDifferent from the commonly used TF (term fre-quency) or TF.IDF (term frequency.inverse doc-ument frequency) weighting schemes, Delta IDFtreats the positive and negative training instancesas two separate corpora, and weighs the terms byhow biased they are to one corpus.
The more bi-ased a term is to one class, the higher (absolutevalue of) weight it will get.
Delta IDF boosts theimportance of terms that tend to be class-specificin the dataset, since they are usually effective fea-tures in distinguishing one class from another.Each training instance (e.g., a document)is represented as a feature vector: xi=(w1,i, ..., w|V |,i), where each dimension in the vec-tor corresponds to a n-gram term in vocabularyV = {t1, ..., t|V |}, |V | is the number of uniqueterms, and wj,i(1 ?
j ?
|V |) is the weight ofterm tjin instance xi.
Delta IDF (Martineau andFinin, 2009) assigns score ?
idfjto term tjin Vas:?
idfj= log(N + 1)(Pj+ 1)(Nj+ 1)(P + 1)(1)where P (or N ) is the number of positively (ornegatively) labeled training instances, Pj(or Nj)is the number of positively (or negatively) labeledtraining instances with term tj.
Simple add-onesmoothing is used to smooth low frequency termsand prevent dividing by zero when a term appearsin only one corpus.
We calculate the Delta IDFscore of every term in V , and get the Delta IDFweight vector ?
= (?
idf1, ...,?
idf|V |) for allterms.When the dataset is imblanced, to avoid build-ing a biased model, we down sample the majorityclass before calculating the Delta IDF score andthen use the a bias balancing procedure to balancethe Delta IDF weight vector.
This procedure firstdivides the Delta IDF weight vector to two vec-tors, one of which contains all the features withpositive scores, and the other of which contains allthe features with negative scores.
It then appliesL2 normalization to each of the two vectors, andadd them together to create the final vector.For each instance, we can calculate theTF.Delta-IDF score as its weight:wj,i= tfj,i??
idfj(2)where tfj,iis the number of times term tjoccursin document xi, and ?
idfjis the Delta IDF scoreof tj.11074.2 A Non-linear Distribution SpreadingAlgorithmDelta IDF technique boosts the weight of featureswith strong discriminative power.
The model?sability to discriminate at the feature level can befurther enhanced by leveraging the distribution offeature weights across multiple classes, e.g., mul-tiple emotion categories funny, happy, sad, ex-citing, boring, etc..
The distinction of multipleclasses can be used to further force feature biasscores apart to improve the identification of class-specific features in the presence of labeling errors.Let L be a set of target classes, and |L| be thenumber of classes in L. For each class l ?
L,we create a binary labeled dataset?Dl.
Let Vlbe the vocabulary of dataset?Dl, V be the vo-cabulary of all datasets, and |V | is the number ofunique terms in V .
Using Formula (1) and dataset?Dl, we get the Delta IDF weight vector for eachclass l: ?l= (?
idfl1, ...,?
idfl|V |).
Note that?
idflj= 0 for any term tj?
V ?
Vl.
For aclass u, we calculate the spreading score spreadujof each feature tj?
V using a non-linear distri-bution spreading formula as following (where s isthe configurable spread parameter):spreaduj= ?
idfuj?
(3)?l?L?u|?
idfuj??
idflj|s|L| ?
1For any term tj?
V , we can get its Delta IDFscore on a class l. The distribution of Delta IDFscores of tjon all classes in L is represented as?j= {?
idf1j, ...,?
idf|L|j}.The mechanism of Formula (3) is to non-linearly spread out the distribution, so that theimportance of class-specific features can be fur-ther boosted to counteract the effect of noisy la-bels.
Specifically, according to Formula (3), ahigh (absolute value of) spread score indicates thatthe Delta IDF score of that term on that class ishigh and deviates greatly from the scores on otherclasses.
In other words, our algorithm assigns highspread score (absolute value) to a term on a classfor which the term has strong discriminative powerand very specific to that class compared with toother classes.
When the dataset is imbalanced, weapply the similar bias balancing procedure as de-scribed in Section 4.1 to the spreading model.While these feature weighting models can beused to score and rank instances for data clean-ing, better classification and regression models canbe built by using the feature weights generated bythese models as a pre-weight on the data points forother machine learning algorithms.5 ExperimentsWe conduct experiments on a Twitter dataset thatcontains tweets about TV shows and movies.
Thegoal is to extract consumers?
emotional reactionsto multimedia content, which has broad commer-cial applications including targeted advertising,intelligent search, and recommendation.
To createthe dataset, we collected 2 billion unique tweetsusing Twitter API queries for a list of known TVshows and movies on IMDB.
Spam tweets werefiltered out using a set of heuristics and manuallycrafted rules.
From the set of 2 billion tweets werandomly selected a small subset of 100K tweetsabout the 60 most highly mentioned TV showsand movies in the dataset.
Tweets were randomlysampled for each show using the round robin algo-rithm.
Duplicates were not allowed.
This samplesan equal number of tweets for each show.
We thensent these tweets to Amazon Mechanical Turk forannotation.We defined our own set of emotions to anno-tate.
The widely accepted emotion taxonomies, in-cluding Ekmans Basic Emotions (Ekman, 1999),Russells Circumplex model (Russell and Barrett,1999), and Plutchiks emotion wheel (Plutchik,2001), did not fit well for TV shows and Movies.For example, the emotion expressed by laughteris a very important emotion for TV shows andmovies, but this emotion is not covered by the tax-onomies listed above.
After browsing through theraw dataset, reviewing the literature on emotionanalysis, and considering the TV and movie prob-lem domain, we decided to focus on eight emo-tions: funny, happy, sad, exciting, boring, angry,fear, and heartwarming.Emotion annotation is a non-trivial task thatis typically time-consuming, expensive and error-prone.
This task is difficult because: (1) There aremultiple emotions to annotate.
In this work, weannotate eight different emotions.
(2) Emotion ex-pressions could be subtle and ambiguous and thusare easy to miss when labeling quickly.
(3) Thedataset is very imbalanced, which increases theproblem of confirmation bias.
As minority classes,emotional tweets can be easily missed because thelast X tweets are all not emotional, and the annota-1108Funny Happy Sad Exciting Boring Angry Fear Heartwarming# Pos.
1,324 405 618 313 209 92 164 24# Neg.
88,782 95,639 84,212 79,902 82,443 57,326 46,746 15,857# Total 90,106 96,044 84,830 80,215 82,652 57,418 46,910 15,881Table 1: Amazon Mechanical Turk annotation label counts.Funny Happy Sad Exciting Boring Angry Fear Heartwarming# Pos.
1,781 4,847 788 1,613 216 763 285 326# Neg.
88,277 91,075 84,031 78,573 82,416 56,584 46,622 15,542# Total190,058 95,922 84,819 80,186 82,632 57,347 46,907 15,868Table 2: Ground truth annotation label counts for each emotion.2tors do not expect the next one to be either.
Due tothese reasons, there is a lack of sufficient and highquality labeled data for emotion research.
Someresearchers have studied harnessing Twitter hash-tags to automatically create an emotion annotateddataset (Wang et al, 2012).In order to evaluate our approach in real worldscenarios, instead of creating a high quality anno-tated dataset and then introducing artificial noise,we followed the common practice of crowdsouc-ing, and collected emotion annotations throughAmazon Mechanical Turk (AMT).
This AMT an-notated dataset was used as the low quality dataset?D in our evaluation.
After that, the same datasetwas annotated independently by a group of expertannotators to create the ground truth.
We evaluatethe proposed approach on two factors, the effec-tiveness of the models for emotion classification,and the improvement of annotation quality pro-vided by the active learning procedure.
We firstdescribe the AMT annotation and ground truth an-notation, and then discuss the baselines and exper-imental results.Amazon Mechanical Turk Annotation: weposted the set of 100K tweets to the workers onAMT for emotion annotation.
We defined a setof annotation guidelines, which specified rules andexamples to help annotators determine when to taga tweet with an emotion.
We applied substantialquality control to our AMT workers to improve theinitial quality of annotation following the commonpractice of crowdsourcing.
Each tweet was anno-tated by at least two workers.
We used a series oftests to identify bad workers.
These tests include(1) identifying workers with poor pairwise agree-ment, (2) identifying workers with poor perfor-mance on English language annotation, (3) iden-tifying workers that were annotating at unrealis-tic speeds, (4) identifying workers with near ran-dom annotation distributions, and (5) identifyingworkers that annotate each tweet for a given TVshow the same (or nearly the same) way.
We man-ually inspected any worker with low performanceon any of these tests before we made a final deci-sion about using any of their annotations.For further quality control, we also gathered ad-ditional annotations from additional workers fortweets where only one out of two workers iden-tified an emotion.
After these quality control stepswe defined minimum emotion annotation thresh-olds to determine and assign preliminary emo-tion labels to tweets.
Note that some tweets werediscarded as mixed examples for each emotionbased upon thresholds for how many times theywere tagged, and it resulted in different number oftweets in each emotion dataset.
See Table 1 for thestatistics of the annotations collected from AMT.Ground Truth Annotation: After we obtainedthe annotated dataset from AMT, we posted thesame dataset (without the labels) to a group of ex-pert annotators.
The experts followed the same an-notation guidelines, and each tweet was labeled byat least two experts.
When there was a disagree-ment between two experts, they discussed to reachan agreement or gathered additional opinion fromanother expert to decide the label of a tweet.
Weused this annotated dataset as ground truth.
SeeTable 2 for the statistics of the ground truth an-notations.
Compared with the ground truth, manyemotion bearing tweets were missed by the AMTannotators, despite the quality control we applied.It demonstrates the challenge of annotation bycrowdsourcing.
The imbalanced class distribution1The total number of tweets is lower than the AMT datasetbecause the experts removed some off-topic tweets.2Expert annotators had a Kappa agreement score of 0.639before meeting to resolve their differences.1109lll ll ll lll0.360.400.440.480.520.560.600.640 4500 9000 13500 18000 22500 27000Number of Instances Re?annotatedMacro?averaged MAPMethod l Spread SVM?TF SVM?Delta?IDF(a) Macro-Averaged MAPlll ll ll lll0.280.330.380.430.480.530.580 4500 9000 13500 18000 22500 27000Number of Instances Re?annotatedMacro?averaged F1ScoreMethod l Spread SVM?TF SVM?Delta?IDF(b) Macro-Averaged F1 ScoreFigure 1: Performance comparison of mislabeled instance selection methods.
Classifiers become more accurate as more in-stances are re-annotated.
Spread achieves comparable performance with SVMs in terms of both MAP and F1 Score.aggravates the confirmation bias ?
the minorityclass examples are especially easy to miss whenlabeling quickly due to their rare presence in thedataset.Evaluation Metric: We evaluated the resultswith both Mean Average Precision (MAP) and F1Score.
Average Precision (AP) is the average ofthe algorithm?s precision at every position in theconfidence ranked list of results where a true emo-tional document has been identified.
Thus, APplaces extra emphasis on getting the front of thelist correct.
MAP is the mean of the average pre-cision scores for each ranked list.
This is highlydesirable for many practical application such asintelligent search, recommendation, and target ad-vertising where users almost never see results thatare not at the top of the list.
F1 is a widely-usedmeasure of classification accuracy.Methods: We evaluated the overall perfor-mance relative to the common SVM bag of wordsapproach that can be ubiquitously found in textmining literature.
We implemented the followingfour classification methods:?
Delta-IDF: Takes the dot product of theDelta IDF weight vector (Formula 1) with thedocument?s term frequency vector.?
Spread: Takes the dot product of the distri-bution spread weight vector (Formula 3) withthe document?s term frequency vector.
Forall the experiments, we used spread parame-ter s = 2.?
SVM-TF: Uses a bag of words SVM withterm frequency weights.?
SVM-Delta-IDF: Uses a bag of words SVMclassification with TF.Delta-IDF weights(Formula 2) in the feature vectors beforetraining or testing an SVM.We employed each method to build the activelearner C described in Algorithm 1.
We usedstandard bag of unigram and bigram words rep-resentation and topic-based fold cross validation.Since in real world applications people are primar-ily concerned with how well the algorithm willwork for new TV shows or movies that may notbe included in the training data, we defined a testfold for each TV show or movie in our labeled dataset.
Each test fold corresponded to a training foldcontaining all the labeled data from all the otherTV shows and movies.
We call it topic-based foldcross validation.We built the SVM classifiers using LIB-LINEAR (Fan et al, 2008) and applied itsL2-regularized support vector regression model.Based on the dot product or SVM regressionscores, we ranked the tweets by how strongly theyexpress the emotion.
We selected the topm tweetswith the highest dot product or regression scoresbut conflicting preliminary AMT labels as the sus-pected mislabeled instances for re-annotation, justas described in Algorithm 1.
For the experimentalpurpose, the re-annotation was done by assigningthe ground truth labels to the selected instances.Since the dataset is highly imbalanced, we ap-plied the under-sampling strategy when trainingthe classifiers.Figure 1 compares the performance of differ-ent approaches in each iteration after a certainnumber of potentially mislabeled instances are re-1110annotated.
The X axis shows the total numberof data points that have been examined for eachemotion so far till the current iteration (i.e., 300,900, 1800, 3000, 4500, 6900, 10500, 16500, and26100).
We reported both the macro-averagedMAP (Figure 1a) and the macro-averaged F1Score (Figure 1b) on eight emotions as the over-all performance of three competitive methods ?Spread, SVM-Delta-IDF and SVM-TF.
We havealso conducted experiments using Delta-IDF, butits performance is low and not comparable withthe other three methods.Generally, Figure 1 shows consistent perfor-mance gains as more labels are corrected duringactive learning.
In comparison, SVM-Delta-IDFsignificantly outperforms SVM-TF with respectto both MAP and F1 Score.
SVM-TF achieveshigher MAP and F1 Score than Spread at the firstfew iterations, but then it is beat by Spread after16,500 tweets had been selected and re-annotatedtill the eighth iteration.
Overall, at the end of theactive learning process, Spread outperforms SVM-TF by 3.03% the MAP score (and by 4.29% the F1score), and SVM-Delta-IDF outperforms SVM-TF by 8.59% the MAP score (and by 5.26% theF1 score).
Spread achieves a F1 Score of 58.84%,which is quite competitive compared to 59.82%achieved by SVM-Delta-IDF, though SVM-Delta-IDF outperforms Spread with respect to MAP.Spread and Delta-IDF are superior with respectto the time efficiency.
Figure 2 shows the averagetraining time of the four methods on eight emo-tions.
The time spent training SVM-TF classi-fiers is twice that of SVM-Delta-IDF classifiers,12 times that of Spread classifiers, and 31 timesthat of Delta-IDF classifiers.
In our experiments,on average, it took 258.8 seconds to train a SVM-TF classifier for one emotion.
In comparison, theaverage training time of a Spread classifier wasonly 21.4 seconds, and it required almost no pa-rameter tuning.
In total, our method Spread savedup to (258.8 ?
21.4) ?
9 ?
8 = 17092.8 seconds(4.75 hours) over nine iterations of active learningfor all the eight emotions.
This is enough time tore-annotate thousands of data points.The other important quantity to measure is an-notation quality.
One measure of improvement forannotation quality is the number of mislabeled in-stances that can be fixed after a certain number ofactive learning iterations.
Better methods can fixmore labels with fewer iterations.0100200Delta?IDF Spread SVM?Delta?IDF SVM?TFMethodAverageTraming Time(s)Figure 2: Average training time on eight emotions.
Spread re-quires only one-twelfth of the time spent to training an SVM-TF classifier.
Note that the time spent tuning the SVM?s pa-rameters has not been included, but is considerable.
Com-pared with such computationally expensive methods, Spreadis more appropriate for use with active learning.lllll ll lll0%10%20%30%40%50%60%70%80%90%100%0 4500 9000 13500 18000 22500 27000Number of Instances Re?annotatedPercentageof FixedLabelsMethod l Spread SVM?TF SVM?Delta?IDFDelta?IDF RandomFigure 3: Accumulated average percentage of fixed labels oneight emotions.
Spreading the feature weights reduces thenumber of data points that must be examined in order to cor-rect the mislabeled instances.
SVMs require slightly fewerpoints but take far longer to build.Besides the four methods, we also implementeda random baseline (Random) which randomly se-lected the specified number of instances for re-annotation in each round.
We compared the im-proved dataset with the final ground truth at theend of each round to monitor the progress.
Figure3 reports the accumulated average percentage ofcorrected labels on all emotions in each iterationof the active learning process.According to the figure, SVM-Delta-IDF andSVM-TF are the most advantageous methods, fol-lowed by Spread and Delta-IDF.
After the lastiteration, SVM-Delta-IDF, SVM-TF, Spread andDelta-IDF has fixed 85.23%, 85.85%, 81.05%and 58.66% of the labels, respectively, all ofwhich significantly outperform the Random base-line (29.74%).11116 ConclusionIn this paper, we explored an active learning ap-proach to improve data annotation quality forclassification tasks.
Instead of training the ac-tive learner using computationally expensive tech-niques (e.g., SVM-TF), we used a novel non-lineardistribution spreading algorithm.
This algorithmfirst weighs the features using the Delta-IDF tech-nique, and then non-linearly spreads out the distri-bution of the feature scores to enhance the model?sability to discriminate at the feature level.
Theevaluation shows that our algorithm has the fol-lowing advantages: (1) It intelligently ordered thedata points for annotators to annotate the mostlikely errors first.
The accuracy was at least com-parable with computationally expensive baselines(e.g.
SVM-TF).
(2) The algorithm trained and ranmuch faster than SVM-TF, allowing annotators tofinish more annotations than competitors.
(3) Theannotation process improved the dataset qualityby positively impacting the accuracy of classifiersthat were built upon it.ReferencesCarla E Brodley and Mark A Friedl.
1999.
Identifyingmislabeled training data.
Journal of Artificial Intel-ligence Research, 11:131?167.Chris Callison-Burch.
2009.
Fast, cheap, and cre-ative: evaluating translation quality using amazon?smechanical turk.
In Proceedings of EMNLP, pages286?295.
ACL.Paul Ekman.
1999.
Basic emotions.
Handbook of cog-nition and emotion, 4:5?60.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
Liblinear: Alibrary for large linear classification.
The Journal ofMachine Learning Research, 9:1871?1874.Donghai Guan, Weiwei Yuan, Young-Koo Lee, andSungyoung Lee.
2011.
Identifying mislabeled train-ing data with the aid of unlabeled data.
Applied In-telligence, 35(3):345?358.Wenxin Jiang.
2001.
Some theoretical aspects ofboosting in the presence of noisy data.
In Proceed-ings of ICML.
Citeseer.Adam Tauman Kalaia and Rocco A Servediob.
2005.Boosting in the presence of noise.
Journal of Com-puter and System Sciences, 71:266?290.Amitava Karmaker and Stephen Kwek.
2006.
Aboosting approach to remove class label noise.
In-ternational Journal of Hybrid Intelligent Systems,3(3):169?177.Aniket Kittur, Ed H Chi, and Bongwon Suh.
2008.Crowdsourcing user studies with mechanical turk.In Proceedings of CHI, pages 453?456.
ACM.Srivatsan Laxman, Sushil Mittal, and RamarathnamVenkatesan.
2013.
Error correction in learning us-ing svms.
arXiv preprint arXiv:1301.2012.Justin Martineau and Tim Finin.
2009.
Delta tfidf: Animproved feature space for sentiment analysis.
InProceedings of ICWSM.John Mingers.
1989.
An empirical comparison ofpruning methods for decision tree induction.
Ma-chine learning, 4(2):227?243.Fabrice Muhlenbach, St?ephane Lallich, and Djamel AZighed.
2004.
Identifying and handling mislabelledinstances.
Journal of Intelligent Information Sys-tems, 22(1):89?109.Robert Plutchik.
2001.
The nature of emotions.
Amer-ican Scientist, 89(4):344?350.Umaa Rebbapragada, Carla E Brodley, Damien Sulla-Menashe, and Mark A Friedl.
2012.
Active labelcorrection.
In Proceedings of ICDM, pages 1080?1085.
IEEE.James A Russell and Lisa Feldman Barrett.
1999.
Coreaffect, prototypical emotional episodes, and otherthings called emotion: dissecting the elephant.
Jour-nal of personality and social psychology, 76(5):805.Burr Settles.
2010.
Active learning literature sur-vey.
Technical Report 1648, University of Wiscon-sin, Madison.Victor S Sheng, Foster Provost, and Panagiotis GIpeirotis.
2008.
Get another label?
improving dataquality and data mining using multiple, noisy label-ers.
In Proceedings of KDD, pages 614?622.
ACM.Rion Snow, Brendan O?Connor, Daniel Jurafsky, andAndrew Y Ng.
2008.
Cheap and fast?but is itgood?
: evaluating non-expert annotations for natu-ral language tasks.
In Proceedings of EMNLP, pages254?263.Qi Su, Dmitry Pavlov, Jyh-Herng Chow, and Wendell CBaker.
2007.
Internet-scale collection of human-reviewed data.
In Proceedings of WWW, pages 231?240.
ACM.P Vannoorenberghe and T Denoeux.
2002.
Handlinguncertain labels in multiclass problems using beliefdecision trees.
In Proceedings of IPMU, volume 3,pages 1919?1926.Sofie Verbaeten and Anneleen Van Assche.
2003.
En-semble methods for noise elimination in classifica-tion problems.
In Multiple classifier systems, pages317?325.
Springer.Wenbo Wang, Lu Chen, Krishnaprasad Thirunarayan,and Amit P Sheth.
2012.
Harnessing twitter?
bigdata?
for automatic emotion identification.
In Pro-ceedings of SocialCom, pages 587?592.
IEEE.Xinchuan Zeng and Tony R Martinez.
2001.
An al-gorithm for correcting mislabeled data.
Intelligentdata analysis, 5(6):491?502.Xingquan Zhu, Xindong Wu, and Qijun Chen.
2003.Eliminating class noise in large datasets.
In Pro-ceedings of ICML, pages 920?927.1112
