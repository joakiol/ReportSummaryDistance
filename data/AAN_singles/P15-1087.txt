Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 899?908,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsThe NL2KR Platform for building Natural Language Translation SystemsNguyen H. Vo, Arindam Mitra and Chitta BaralSchool of Computing, Informatics and Decision Systems EngineeringArizona State University{nguyen.h.vo, amitra7, chitta }@asu.eduAbstractThis paper presents the NL2KR platformto build systems that can translate text todifferent formal languages.
It is freely-available1, customizable, and comes withan Interactive GUI support that is use-ful in the development of a translationsystem.
Our key contribution is a user-friendly system based on an interactivemultistage learning algorithm.
This effec-tive algorithm employs Inverse-?, Gener-alization and user provided dictionary tolearn new meanings of words from sen-tences and their representations.
Usingthe learned meanings, and the Generaliza-tion approach, it is able to translate newsentences.
NL2KR is evaluated on twostandard corpora, Jobs and GeoQuery andit exhibits state-of-the-art performance onboth of them.1 Introduction and Related WorkFor natural language interaction with systems oneneeds to translate natural language text to the inputlanguage of that system.
Since different systems(such as a robot or database system) may have dif-ferent input language, we need a way to translatenatural language to different formal languages asneeded by the application.
We have developed auser friendly platform, NL2KR, that takes exam-ples of sentences and their translations (in a de-sired target language that varies with the applica-tion), and some bootstrap information (an initiallexicon), and constructs a translation system fromtext to that desired target language.1http://nl2kr.engineering.asu.edu/Our approach to translate natural language textto formal representation is inspired by Montague?swork (Montague, 1974) where the meanings ofwords and phrases are expressed as ?-calculus ex-pressions and the meaning of a sentence is builtfrom semantics of constituent words through ap-propriate ?-calculus (Church, 1936) applications.A major challenge in using this approach has beenthe difficulty of coming up with the ?-calculusrepresentation of words.Montague?s approach has been widely used in(Zettlemoyer and Collins, 2005; Kwiatkowski etal., 2010) to translate natural language to formallanguages.
In ZC05 (Zettlemoyer and Collins,2005) the learning algorithm requires the user toprovide the semantic templates for all words.
Asemantic template is a ?-expression (e.g.
?x.p(x)for an arity one predicate), which describes a par-ticular pattern of representation in that formal lan-guage.
With all these possible templates, thelearning algorithm extracts the semantic represen-tation of the words from the formal representa-tion of a sentence.
It then associates the extractedmeanings to the words of the sentence in all possi-ble ways and ranks the associations according tosome goodness measure.
While manually com-ing up with semantic templates for one target lan-guage is perhaps reasonable, manually doing it fordifferent target languages corresponding to differ-ent applications may not be a good idea as manualcreation of semantic templates requires deep un-derstanding of translation to the target language.This calls for automating this process.
In UBL(Kwiatkowski et al, 2010) this process is auto-mated by restricting the choices of formal rep-resentation and learning the meanings in a bruteforce manner.
Given, a sentence S and its rep-resentation M in the restricted formal language,899it breaks the sentence into two smaller substringsS1, S2 and uses higher-order unification to com-pute two ?-termsM1,M2 which combines to pro-duce M .
It then recursively learns the meaningsof the words, from the sub-instance < S1,M1 >and < S2,M2 >.
Since, there are many waysto split the input sentence S and the choice ofM1,M2 can be numerous, it needs to consider allpossible splittings and their combinations; whichproduces many spurious meanings.
Most impor-tantly, their higher-order unification algorithm im-poses various restrictions (such as limited num-ber of conjunctions in a sentence, limited forms offunctional application) on the meaning representa-tion language which severely limits its applicabil-ity to new applications.
Another common draw-back of these two algorithms is that they both suf-fer when the test sentence contains words that arenot part of the training corpus.Our platform NL2KR uses a different auto-mated approach based on Inverse-?
(section 2.1)and Generalization (section 2.2) which does notimpose such restrictions enforced by their higher-order unification algorithm.
Also, Generaliza-tion algorithm along with Combinatory Categor-ical Grammar (Steedman, 2000) parser, allowsNL2KR to go beyond the training dictionary andtranslate sentences which contain previously un-seen words.
The main aspect of our approach is asfollows: given a sentence, its semantic representa-tion and an initial dictionary containing the mean-ing of some words, NL2KR first obtains severalderivation of the input sentence in CombinatoryCategorical Grammar (CCG).
Each CCG deriva-tion tree describes the rules of functional appli-cation through which constituents combine witheach other.
With the user provided initial dictio-nary, NL2KR then traverses the tree in a bottom-up fashion to compute the semantic expressionsof intermediate nodes.
It then traverses the aug-mented tree in a top-down manner to learn themeaning of missing words using Inverse-?
(sec-tion 2.1).
If Inverse-?
is not sufficient to learn themeaning of all unknown words, it employs Gen-eralization (section 2.2) to guess the meanings ofunknown words with the meaning of known sim-ilar words.
It then restarts the learning processwith the updated knowledge.
The learning pro-cess stops if it learns the meanings of all words orfails to learn any new meaning in an iteration.
Inthe latter case, it shows the augmented tree to theuser.
The user can then provide meanings of someunknown words and resumes the learning process.Another distinguishing feature of NL2KR is itsuser-friendly interface that helps users in creatingtheir own translation system.
The closest systemto NL2KR is the UW Semantic Parsing Frame-work (UW SPF) (Artzi and Zettlemoyer, 2013)which incorporates the algorithms in (Zettlemoyerand Collins, 2005; Kwiatkowski et al, 2010) .However, to use UW SPF for the development ofa new system, the user needs to learn their codingguidelines and needs to write new code in theirsystem.
NL2KR does not require the users towrite new code and guides the development pro-cess with its rich user interface.We have evaluated NL2KR on two standarddatasets: GeoQuery (Tang and Mooney, 2001) andJobs (Tang and Mooney, 2001).
GeoQuery is adatabase of geographical questions and Jobs con-tains sentences with job related query.
Experi-ments demonstrate that NL2KR can exhibit state-of-the-art performance with fairly small initial dic-tionary.
The rest of the paper is organized as fol-lows: we first present the algorithms and archi-tecture of the NL2KR platform in section 2; wediscuss about the experiments in section 3; and fi-nally, we conclude in section 4.2 Algorithms and ArchitectureThe NL2KR architecture (Figure 1) has two sub-parts which depend on each other (1) NL2KR-L for learning and (2) NL2KR-T for translation.The NL2KR-L sub-part takes the following as in-put: (1) a set of training sentences and their tar-get formal representations, and (2) an initial lexi-con or dictionary consisting of some words, theirCCG categories, and their meanings in terms of ?-calculus expressions.
It then constructs the CCGparse trees and uses them for learning of wordmeanings.Learning of word meanings is done by usingInverse-?
and Generalization (Baral et al, 2012;Baral et al, 2011) and ambiguity is addressedby a Parameter Learning module that learns theweights of the meanings.
The learned meaningsupdate the lexicon.
The translation sub-part usesthis updated lexicon to get the meaning of all thewords in a new sentence, and combines them to getthe meaning of the new sentence.
Details of eachmodule will be presented in the following subsec-tions.900Figure 1: Architecture of NL2KRThe NL2KR platform provides a GUI (Figure 2)with six features: ?-application, Inverse-?, Gen-eralization, CCG-Parser, NL2KR-L and NL2KR-T.
The fourth feature is a stand-alone CCG parserand the first four features can help on user withconstructing the initial lexicon.
The user can thenuse NL2KR-L to update the lexicon using train-ing data and the NL2KR-T button then works as atranslation system.2.1 Inverse-?Inverse-?
plays a key role in the learning pro-cess.
Formally, given two ?-expressions H andG with H = F@G or H = G@F , theInverse-?
operation computes the ?
expressionF .
For example, given the meaning of ?is texas?as ?x2.x2@stateid(texas) and the meaning of?texas?
as stateid(texas), with the additionalinformation that ?is?
acts as the function while?texas?
is the argument, the Inverse-?
algorithmcomputes the meaning of ?is?
as ?x3.
?x2.x2@x3(Figure 4).
NL2KR implements the Inverse-?
al-gorithm specified in (Baral et al, 2012).
TheInverse-?
module is separately accessible throughthe main GUI (Figure 2).2.2 GeneralizationGeneralization (Baral et al, 2012; Baral et al,2011) is used when Inverse-?
is not sufficient tolearn new semantic representation of words.
Incontrast to Inverse-?
which learns the exact mean-ing of a word in a particular context, General-ization learns the meanings of a word from sim-ilar words with existing representations.
Thus,Generalization helps NL2KR to learn meaningsof words that are not even present in the train-ing data set.
In the current implementation, twowords are considered as similar if they have theexact same CCG category.
As an example, ifwe want to generalize the meaning of the word?plays?
with CCG category (S\NP )/NP ) andthe lexicon already contains an entry for ?eats?with the same CCG category, and the mean-ing ?y.
?x.eats(x, y), the algorithm will ex-tract the template ?y.
?x.WORD(x, y) and ap-ply the template to plays to get the meaning?y.
?x.plays(x, y).2.3 Combinatory Categorial GrammarDerivation of a sentence in Combinatory Catego-rial Grammar (CCG) determines the way the con-stituents combine together to establish the mean-ing of the whole.
CCG is a type of phrase struc-ture grammar and clearly describes the predicate-argument structure of constituents.Figure 3 shows an example output of NL2KR?sCCG parser.
In the figure, ?John?
and ?home?have the category [N] (means noun) and canchange to [NP] (means noun phrase).
Thephrase?walk home?
has the category [S\NP],which means that it can combine with a con-stituent with category [NP] (?John?
in this case)from left with the backward application to formcategory [S] (sentence).
The word ?walk?
hasthe category [(S\NP)/NP], which means it cancombine with a constituent with category [NP](?home?)
from right through the forward appli-cation combinator to form category [S\NP] (of?walk home?
).A detailed description on CCG goes beyond thescope of this paper (see (Steedman, 2000) for moredetails).
Since, natural language sentences canhave various CCG parse trees, each expressing adifferent meaning of the sentence, a key challenge901Figure 2: NL2KR?s main GUI, Version 1.7.0001Figure 3: CCG parse tree of ?John walked home?.in the learning and the translation process is to finda suitable CCG parse tree for a sentence in natu-ral language.
We overcome this impediment byallowing our learning and translation subsystemto work with multiple weighted parse trees for agiven sentence and determining on the fly, the onethat is most suitable.
We discuss more on this insections 2.4-2.6.Existing CCG parsers (Curran et al, 2007; Lier-ler and Sch?uller, 2012) either return a single bestparse tree for a given sentence or parse it in allpossible ways with no preferential ordering amongthem.
In order to overcome this shortcoming andgenerate more than one weighted candidate parsetrees, we have developed a new parser using beamsearch with Cocke-Younger-Kasami(CYK) algo-rithm.
NL2KRs CCG parser uses the C&C model(Curran et al, 2007) and constraints from the Stan-ford parser (Socher et al, 2013; Toutanova et al,2003) to guide the derivation of a sentence.
Theoutput of the CCG parser is a set of k weightedparse trees, where the parameter k is provided bythe user.NL2KR system allows one to use the CCGparser independently through the interactive GUI.The output graphs look like the one in Figure 3.
Itcan be zoomed in/out and its nodes can be movedaround, making it easier to work with complexsentences.2.4 Multistage learning approachLearning meanings of words is the major com-ponent of our system.
The inputs to the learningmodule are a list of training sentences, their targetformal representations and an initial lexicon con-sisting of triplets of the form <word, CCG cate-gory, meaning>, where meanings are representedin terms of ?-calculus expressions.
The outputof the algorithm is a final dictionary containinga set of 4-tuples (word, CCG category, meaning,weight).Interactive Multistage Learning Algorithm(IMLA) NL2KR employs an Interactive Multi-stage Learning Algorithm (Algorithm 1) that runsmany iterations on the input sentences.
In eachiteration, it goes through one or more of the fol-lowing stages:Stage 1 In Stage 1, it gets all the unfinishedsentences.
It then employs Bottom Up-Top Downalgorithm (Algorithm 2) to learn new meanings(by Inverse-?).
For a sentence, if it has com-puted the meanings of all its constituents, whichcan be combined to produce the given representa-tion, that sentence is considered as learned.
Each902Algorithm 1 IMLA algorithm1: function IMLA(initLexicon,sentences,sentsMeanings)2: regWords?
?3: generalize?
false4: lexicon?
initLexicon5: repeat6: repeat7: repeat8: for all s ?
sentences do9: newMeanings ?BT(s,lexicon,sentsMeanings)10: lexicon?
lexicon ?
newMeanings11: for all n ?
newMeanings do12: ms?
GENERALIZE(regWords, n)13: lexicon?
lexicon ?ms14: end for15: end for16: until newMeanings = ?17: if generalize=false then18: generalize?
true19: for all t ?
unfinishedSents do20: words?
GETALLWORDS(t)21: ms?
GENERALIZE(words)22: lexicon?
lexicon ?ms23: regWords?
regWords ?
words24: end for25: end if26: until newMeanings = ?27: INTERATIVELEARNING28: until unfinishedSents = ?
OR userBreak29: lexicon ?
PARAMETERESTIMA-TION(lexicon,sentences)30: return lexicon31: end functionnew meaning learned by this process is used togeneralize the words in a waiting list.
Initially,this waiting list is empty and is updated in stage2.
When no more new meaning can be learnedby Bottom Up-Top Down algorithm, IMLA (Algo-rithm 1) enters stage 2.Stage 2 In this stage, it takes all the sentencesfor which the learning is not yet finished and ap-plies Generalization process on all the words ofthose sentences.
At the same time, it populatesthose words into the waiting list, so that from nowon, Bottom Up-Top Down will try to generalizenew meanings for them when it learns some newmeanings.
It then goes back to stage 1.
Next time,after exiting stage 1, it directly goes to stage 3.Stage 3 When both aforementioned stagescan not learn all the sentences, the InteractiveLearning process is invoked and all the unfinishedsentences are shown on the interactive GUI (Fig-ure 4).
Users can either skip or provide more in-formation on the GUI and the learning process iscontinued.After finishing all stages, IMLA (Algorithm 1)calls Parameter Estimation (section 2.5) algorithmto compute the weight of each lexicon tuple.Bottom Up-Top Down learning For a givensentence, the CCG parser is used for the CCGparse trees like the one of how big is texas in Fig-ure 4.
For each parse tree, two main processesare called, namely ?bottom up?
and ?top down?.In the first process, all the meanings of the wordsin the sentences are retrieved from the lexicon.These meanings are populated in the leaf nodesof a parse tree (see Figure 4), which are combinedin a bottom-up manner to compute the meaningsof phrases and full sentences.
We call these mean-ings, the current meanings.In the ?top down?
process, using Inverse-?
al-gorithm, the given meaning of the whole sentence(called the expected meaning of the sentence) andthe current meanings of the phrases, we calcu-late the expected meanings of each of the phrasesfrom the root of the tree to the leaves.
For ex-ample, given the expected meaning of how big istexas and the current meaning of how big, we useInverse-?
algorithm to get the meaning (expected)of is texas.
This expected meaning is used togetherwith current meanings of is (texas) to calculatethe expected meanings of texas (is).
The expectedmeanings of the leaf nodes we have just learnedwill be saved to the lexicon and will be used in theother sentences and in subsequent learning itera-tion.
The ?top down?
process is stopped when theexpected meanings are same as the current mean-ings.
And in both ?bottom up?
and ?top-down?processes, the beam search algorithm is used tospeed-up the learning process.Interactive learning In the interactive learningprocess it opens a GUI which shows the unfinishedsentences.
Users can see the current and expectedmeanings for the unfinished sentences.
When theuser gives additional meanings of word(s), the ?-application or Inverse-?
operation is automaticallyperformed to update the new meaning(s) to related903Figure 4: Interactive learning GUI.
The box under each node show: the corresponding phrases [CCG category], the expectedmeanings and the current meanings.
Click on the red node will show the window to change the current meaning (CLE)Algorithm 2 BottomUp-TopDown (BT) algo-rithm1: function BT(sentence, lexicon, sentsMeanings)2: parseTrees?
CCGPARSER(sentence)3: for all tree ?
parseTrees do4: t?
BOTTOMUP(tree,lexicon)5: TOPDOWN(t,sentsMeanings)6: end for7: end functionword(s).
Once satisfied, the user can switch backto the automated learning mode.Example Let us consider the ques-tion ?How big is texas??
with meaninganswer(size(stateid(texas))) (see Figure4).Let us assume that the initial dictionary hasthe following entries: how := NP/(N/N) :?x.
?y.answer(x@y), big := N/N : ?x.size(x)and texas :=NP : stateid(texas).
The algorithmthen proceeds as follows.First, the meanings of ?how?
and ?big?
are com-bined to compute the current meaning of ?howbig?
:= NP : ?x.answer(size(x)) in the ?bot-tom up?
process.
Since the meaning of ?is?
is un-known, the current meaning of ?is texas?
still re-mains unknown.It then starts the ?top down?
process whereit knows the expected meaning of ?How big istexas?
:= S : answer(size(stateid(texas)))and the current meaning of ?how big?.
Usingthem in the Inverse-?
algorithm, it then com-pute the meaning of ?is texas?
:= S\NP :?x1.x1@stateid(texas).
Using this expectedmeaning and current meaning of ?texas?
:= NP :stateid(texas), it then calculates the expectedmeaning of ?is?
as ?is?
:= (S\NP )/NP :?x2.?x1.x1@x2.
This newly learned expectedmeaning is then saved into the lexicon.Since the meaning of all the words in the ques-tion are known, the learning algorithm stops hereand the Interactive Learning is never called.If initially, the dictionary contains only twomeanings: ?big?
:= N/N : ?x.size(x) and?texas?
:= NP : stateid(texas), NL2KR triesto first learn the sentence but fails to learnthe complete sentence and switches to Inter-active Learning which shows the interactiveGUI (see Figure 4).
If the user specifiesthat ?how?
means ?x.
?y.answer(x@y), NL2KRcombines its meaning with the meaning of ?big?to get the meaning ?how big?
:= NP :?x.answer(size(x)).
It will then use Inverse-?
to figure out the meaning of ?is texas?
andthen the meaning of ?is?.
Now all the mean-ings are combined to compute the current mean-ing answer(size(stateid(texas))) of ?How bigis texas?.
This meaning is same as the expected904meaning, so we know that the sentence is suc-cessfully learned.
Now, the user can press RetryLearning to switch back to automated learning.2.5 Parameter EstimationThe Parameter Estimation module estimates aweight for each word-meaning pair such that thejoint probability of the training sentences gettingtranslated to their given representation is maxi-mized.
It implements the algorithm described inZettlemoyer et.
al.
(2005).2.6 TranslationThe goal of this module is to convert input sen-tences into the target formalism using the lexi-con previously learned.
The algorithm used inTranslation module (Algorithm 3) is similar to thebottom-up process in the learning algorithm.
Itfirst obtains several weighted CCG parse trees ofthe input sentence.
It then computes a formal rep-resentation for each of the parse trees using thelearned dictionary.
Finally, it ranks the transla-tions according to the weights of word-meaningpairs and the weights of the CCG parse trees.However, test sentences may contain words whichwere not present in the training set.
In such cases,Generalization is used to guess the meanings ofthose unknown words from the meanings of thesimilar words present in the dictionary.Algorithm 3 Translation algorithm1: function TRANSLATE(sentence, lexicon)2: candidates?
?3: parseTrees?
CCGPARSER(sentence)4: for all tree ?
parseTrees do5: GENERALIZE(tree);6: t?
BOTTOMUP(tree)7: candidates?
candidates ?
t8: end for9: output?
VERIFY-RANK(candidates)10: return output11: end function3 Experimental EvaluationWe have evaluated NL2KR on two standard cor-pora: GeoQuery and Jobs.
For both the corpus, theoutput generated by the learned system has beenconsidered correct if it is an exact replica of thelogical formula described in the corpus.We report the performance in terms of precision(percentage of returned logical-forms that are cor-rect), recall (percentage of sentences for which thecorrect logical-form was returned), F1-measure(harmonic mean of precision and recall) and thesize of the initial dictionary.We compare the performance of our sys-tem with recently published, directly-comparableworks, namely, FUBL (Kwiatkowski et al,2011), UBL (Kwiatkowski et al, 2010), ?-WASP(Wong and Mooney, 2007), ZC07 (Zettlemoyerand Collins, 2007) and ZC05 (Zettlemoyer andCollins, 2005) systems.3.1 CorporaGeoQuery GeoQuery (Tang and Mooney, 2001)is a corpus containing questions on geographicalfacts about the United States.
It contains a total of880 sentences written in natural language, pairedwith their meanings in a formal query language,which can be executed against a database of thegeographical information of the United States.We follow the standard training/testing split of600/280.
An example sentence meaning pair isshown below.Sentence: How long is the Colorado river?Meaning: answer(A,(len(B,A),const(B,riverid(colorado)), river(B)))Jobs The Jobs (Tang and Mooney, 2001) datasetcontains a total of 640 job related queries writtenin natural language.
The Prolog programminglanguage has been used to represent the meaningof a query.
Each query specifies a list of jobcriteria and can be directly executed against adatabase of job listings.
An example sentencemeaning pair from the corpus is shown below.Question: What jobs are there for program-mers that know assembly?Meaning: answer(J,(job(J),title(J,T),const(T,?Programmer?),language(J,L),const(L,?assembly?
))))The dataset contains a training split of 500 sen-tences and a test split of 140 sentences.3.2 Initial Dictionary FormulationGeoQuery For GeoQuery corpus, we manuallyselected a set of 100 structurally different sen-tences from the training set and initiated the learn-ing process with a dictionary containing the repre-905GUI Driven Initial Dictionary Learned Dictionary] <word, category > 31 118 401] <word, category, meaning> 36 127 1572] meaning 30 89 819Table 1: Comparison of Initial and Learned dictionary for GeoQuery corpus on the basis of the number of entries in thedictionary, number of unique <word, CCG category> pairs and the number of unique meanings across all the entries.
?GUIDriven?
denotes the amount of the total meanings given through interactive GUI and is a subset of the Initial dictionary.GUI Driven Initial Dictionary Learned Dictionary] <word, category> 58 103 226] <word, category, meaning> 74 119 1793] meaning 57 71 940Table 2: Comparison of Initial and Learned dictionary for Jobs corpus.sentation of the nouns and question words.
Thesemeanings were easy to obtain as they follow sim-ple patterns.
We then trained the translation sys-tem on those selected sentences.
The output ofthis process was used as the initial dictionary fortraining step.
Further meanings were provided ondemand through interactive learning.
A total of119 word meanings tuples (Table 1, ] <word, cat-egory, meaning >) were provided from which theNL2KR system learned 1793 tuples.
45 of the 119were representation of nouns and question wordsthat were obtained using simple patterns.
The re-maining 74 were obtained by a human using theNL2KR GUI.
These numbers illustrate the useful-ness of the NL2KR GUI as well as the NL2KRlearning component.
One of our future goals is tofurther automate the process and reduce the GUIinteraction part.Table 1 compares the initial and learned dic-tionary for GeoQuery on the basis of numberof unique <word, category, meaning> entries indictionary, number of unique <word, category>pairs and the number of unique meanings acrossall the entries in the dictionary.
Since each unique<word, CCG category> pair must have at leastone meaning, the total number of unique <word,category> pairs in the training corpus provides alower bound on the size of the ideal output dictio-nary.
However, one <word, category> pair mayhave multiple meanings, so the ideal dictionarycan be much bigger than the number of unique<word, category> pairs.
Indeed, there were manywords such as ?of?, ?in?
that had multiple mean-ings for the same CCG category.
Table 1 clearlydescribes that the amount of initial effort is sub-stantially less compared to the return.Jobs For the Jobs dataset, we followed a similarprocess as in the GeoQuery dataset.
A set of 120structurally different sentences were selected and adictionary was created which contained the repre-sentation of the nouns and the question words fromthe training corpus.
A total of 127 word meaningswere provided in the process.
Table 2 comparesthe initial and learned dictionary for Jobs.
Again,we can see that the amount of initial effort is sub-stantially less in comparison to the return.3.3 Precision, Recall and F1-measureFigure 5: Comparison of Precision, Recall and F1-measureon GeoQuery and Jobs dataset.Table 3, Table 4 and Figure 5 present the com-parison of the performance of NL2KR on the Geo-Query and Jobs domain with other recent works.NL2KR obtained 91.1% precision value, 92.1%906System Precision Recall F1ZC05 0.963 0.793 0.87ZC07 0.916 0.861 0.888?-WASP 0.9195 0.8659 0.8919UBL 0.885 0.879 0.882FUBL 0.886 0.886 0.886NL2KR 0.911 0.921 0.916Table 3: Comparison of Precision, Recall and F1-measure onGeoQuery dataset.recall value and a F1-measure of 91.6% on Geo-Query (Figure 5, Geo880) dataset.
For Jobs cor-pus, the precision, recall and F1-measure were95.43%, 94.03% and 94.72% respectively.
Inall cases, NL2KR achieved state-of-the-art recalland F1 measures and it significantly outperformedFUBL (the latest work on translation systems) onGeoQuery.For both GeoQuery and Jobs corpus, our recallis significantly higher than existing systems be-cause meanings discovered by NL2KRs learningalgorithm is more general and reusable.
In otherwords, meanings learned from a particular sen-tence are highly likely to be applied again in thecontext of other sentences.
It may be noted that,larger lexicons do not necessarily imply higher re-call as lambda expressions for two phrases maynot be suitable for functional application, thusfailing to generate any translation for the whole.Moreover, the use of a CCG parser maximizes therecall by exhibiting consistency and providing aset of weighted parse trees.
By consistency, wemean that the order of the weighted parse tree re-mains same over multiple parses of the same sen-tence and the sentences having similar syntacticstructures have identical ordering of the deriva-tions, thus making Generalization to be more ef-fective in the process of translation.The sentences for which NL2KR did not havea translation are the ones having structural dif-ference with the sentences present in the train-ing dataset.
More precisely, their structure wasnot identical with any of the sentences present inthe training dataset or could not be constructed bycombining the structures observed in the trainingsentences.We analyzed the sentences for which the trans-lated meaning did not match the correct one andobserved that the translation algorithm selectedthe wrong meaning, even though it discovered thecorrect one as one of the possible meanings theSystem Precision Recall F1ZC05 0.9736 0.7929 0.8740COCKTAIL 0.9325 0.7984 0.8603NL2KR 0.9543 0.9403 0.9472Table 4: Comparison of Precision, Recall and F1-measure onJobs dataset.sentence could have had in the target formal lan-guage.
Among the sentences for which NL2KRreturned a translation, there were very few in-stances where it did not discover the correct mean-ing in the set of possible meanings.It may be noted that even though our preci-sion is lower than ZC05 and very close to ZC07and WASP; we have achieved significantly higherF1 measure than all the related systems.
Infact, ZC05, which achieves the best precision forboth the datasets, is better by a margin of only0.019 on the Jobs dataset and 0.052 on the Geo-Query dataset.
We think one of the main rea-sons is that it uses manually predefined lambda-templates, which we try to automate as much aspossible.4 ConclusionNL2KR is a freely available2, user friendly, richgraphical platform for building translation systemsto convert sentences from natural language to theirequivalent formal representations in a wide vari-ety of domains.
We have described the system al-gorithms and architecture and its performance onthe GeoQuery and Jobs datasets.
As mentionedearlier, the NL2KR GUI and the NL2KR learningmodule help in starting from a small initial lex-icon (for example, 119 in Table 2) and learninga much larger lexicon (1793 in Table 2).
One ofour future goals is to reduce the initial lexicon tobe even smaller by further automating the NL2KRGUI interaction component .AcknowledgementsWe thank NSF for the DataNet Federation Consor-tium grant OCI-0940841 and ONR for their grantN00014-13-1-0334 for partially supporting this re-search.2More examples and a tutorial to use NL2KR are availablein the download package.907ReferencesYoav Artzi and Luke Zettlemoyer.
2013.
UW SPF:The University of Washington Semantic ParsingFramework.
arXiv preprint arXiv:1311.3011.Chitta Baral, Juraj Dzifcak, Marcos Alvarez Gonzalez,and Jiayu Zhou.
2011.
Using inverse ?
and gener-alization to translate english to formal languages.
InProceedings of the Ninth International Conferenceon Computational Semantics, pages 35?44.
Associ-ation for Computational Linguistics.Chitta Baral, Juraj Dzifcak, Marcos Alvarez Gonzalez,and Aaron Gottesman.
2012.
Typed answer set pro-gramming lambda calculus theories and correctnessof inverse lambda algorithms with respect to them.TPLP, 12(4-5):775?791.Alonzo Church.
1936.
An Unsolvable Problem ofElementary Number Theory.
American Journal ofMathematics, 58(2):345?363, April.James Curran, Stephen Clark, and Johan Bos.
2007.Linguistically Motivated Large-Scale NLP withC&C and Boxer.
In Proceedings of the 45th An-nual Meeting of the Association for ComputationalLinguistics Companion Volume Proceedings of theDemo and Poster Sessions, pages 33?36, Prague,Czech Republic, June.
Association for Computa-tional Linguistics.Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-ter, and Mark Steedman.
2010.
Inducing probabilis-tic CCG grammars from logical form with higher-order unification.
In Proceedings of the 2010 con-ference on empirical methods in natural languageprocessing, pages 1223?1233.
Association for Com-putational Linguistics.Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-ter, and Mark Steedman.
2011.
Lexical general-ization in ccg grammar induction for semantic pars-ing.
In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing, pages1512?1523.
Association for Computational Linguis-tics.Yuliya Lierler and Peter Sch?uller.
2012.
Parsing com-binatory categorial grammar via planning in answerset programming.
In Correct Reasoning, pages 436?453.
Springer.Richard Montague.
1974.
English as a Formal Lan-guage.
In Richmond H. Thomason, editor, FormalPhilosophy: Selected Papers of Richard Montague,pages 188?222.
Yale University Press, New Haven,London.Richard Socher, John Bauer, Christopher D. Manning,and Andrew Y. Ng.
2013.
Parsing with Composi-tional Vector Grammars.
In ACL (1), pages 455?465.Mark Steedman.
2000.
The syntactic process, vol-ume 35.
MIT Press.Lappoon R Tang and Raymond J Mooney.
2001.
Us-ing multiple clause constructors in inductive logicprogramming for semantic parsing.
In MachineLearning: ECML 2001, pages 466?477.
Springer.Kristina Toutanova, Dan Klein, Christopher D. Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology- Volume 1.Yuk Wah Wong and Raymond J Mooney.
2007.Learning synchronous grammars for semantic pars-ing with lambda calculus.
In Annual Meeting-Association for computational Linguistics, vol-ume 45, page 960.
Citeseer.Luke S. Zettlemoyer and Michael Collins.
2005.Learning to Map Sentences to Logical Form: Struc-tured Classification with Probabilistic CategorialGrammars.
In UAI, pages 658?666.
AUAI Press.Luke S Zettlemoyer and Michael Collins.
2007.
On-line learning of relaxed CCG grammars for parsingto logical form.
In In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL-2007).908
