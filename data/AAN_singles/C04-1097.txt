Linguistically Informed Statistical Models of Constituent Structure forOrdering in Sentence RealizationEric RINGGER1, Michael GAMON1, Robert C. MOORE1,David ROJAS2, Martine SMETS1, Simon CORSTON-OLIVER11Microsoft ResearchOne Microsoft WayRedmond, Washington 98052, USA{ringger, mgamon, bobmoore, msmets,simonco}@microsoft.com2Butler Hill Group, LLC& Indiana University Linguistics Dept.1021 East 3rd Street, MM 322Bloomington, Indiana 47405, USAdrojas@indiana.eduAbstractWe present several statistical models of syntacticconstituent order for sentence realization.
Wecompare several models, including simple jointmodels inspired by existing statistical parsingmodels, and several novel conditional models.
Theconditional models leverage a large set of linguisticfeatures without manual feature selection.
We applyand evaluate the models in sentence realization forFrench and German and find that a particularconditional model outperforms all others.
Weemploy a version of that model in an evaluation onunordered trees from the Penn TreeBank.
We offerthis result on standard data as a reference-point forevaluations of ordering in sentence realization.1 IntroductionWord and constituent order play a crucial role inestablishing the fluency and intelligibility of asentence.
In some systems, establishing orderduring the sentence realization stage of naturallanguage generation has been accomplished byhand-crafted generation grammars in the past (seefor example, Aikawa et al (2001) and Reiter andDale (2000)).
In contrast, the Nitrogen (Langkildeand Knight, 1998a, 1998b) system employs a wordn-gram language model to choose among a largeset of word sequence candidates which vary inconstituent order, word order, lexical choice, andmorphological inflection.
Nitrogen?s model doesnot take into consideration any non-surfacelinguistic features available during realization.The Fergus system (Bangalore and Rambow,2000) employs a statistical tree model to selectprobable trees and a word n-gram model to rankthe string candidates generated from the best trees.Like Nitrogen, the HALogen system (Langkilde,2000; Langkilde-Geary, 2002a, 2002b) uses wordn-grams, but it extracts the best-scoring surfacerealizations efficiently from a packed forest byconstraining the search first within the scope ofeach constituent.Our research is carried out within the Amalgambroad coverage sentence realization system.Amalgam generates sentence strings from abstractpredicate-argument structures (Figure 1), using apipeline of stages, many of which employmachine-learned models to predict where toperform specific linguistic operations based on thelinguistic context (Corston-Oliver et al, 2002;Gamon et al, 2002a, 2002b; Smets et al, 2003).Amalgam has an explicit ordering stage thatdetermines the order of constituents and theirdaughters.
The input for this stage is an unorderedtree of constituents; the output is an ordered tree ofconstituents or a ranked list of such trees.
Forordering, Amalgam leverages tree constituentstructure and, importantly, features of thoseconstituents and the surrounding context.
Byseparately establishing order within constituents,Amalgam heavily constrains the possiblealternatives in later stages of the realizationprocess.
The design allows for interaction betweenordering choices and other realization decisions,such as lexical choice (not considered in thepresent work), through score combinations fromdistinct Amalgam pipeline stages.Most previous research into the problem ofestablishing order for sentence realization hasfocused on English, a language with fairly strictword and constituent order.
In the experimentsdescribed here we first focus on German andFrench which present novel challenges.1 We alsodescribe an English experiment involving datafrom the Penn Treebank.
Our ultimate goal is todevelop a model that handles all orderingphenomena in a unified and elegant way acrosstypologically diverse languages.
In the presentpaper, we explore the space of possible models andexamine some of these closely.1For an overview of some of the issues indetermining word and constituent order in German andFrench, see (Ringger et al, 2003).Figure 1: Abstract predicate-argument structure (NLPWin logical form) for the German sentence:In der folgenden Tabelle werden die Optionen sowie deren Funktionen aufgelistet.
(The options and their functions are listed in the following table.
)2 Models of Constituent OrderIn order to develop a model of constituentstructure that captures important order phenomena,we will consider the space of possible joint andconditional models in increasing complexity.
Foreach of the models, we will survey theindependence assumptions and the feature set usedin the models.Our models differ from the previous statisticalapproaches in the range of input features.
Like theknowledge-engineered approaches, the modelspresented here incorporate lexical features, parts-of-speech, constituent-types, constituentboundaries, long-distance dependencies, andsemantic relations between heads and theirmodifiers.Our experiments do not cover the entire space ofpossible models, but we have chosen significantpoints in the space for evaluation and comparison.2.1 Joint ModelsWe begin by considering joint models ofconstituent structure of the form ( ),P ?
?
overordered syntax trees ?
and unordered syntax trees?
.
An ordered tree ?
contains non-terminalconstituents C, each of which is the parent of anordered sequence of daughters ( 1,..., nD D ), one ofwhich is the head constituent H.2 Given an orderedtree ?
, the value of the function_ ( )unordered tree ?
is an unordered tree ?corresponding to ?
that contains a constituent Bfor each C in ?
, such that( ) ( )1_ ( ){ ,..., }nunordered set daughters Cdaughters BD D==again with iH D=  for some i in ( )1..n .
Thehierarchical structure of ?
is identical to that of?
.We employ joint models for scoring alternativeordered trees as follows: given an unorderedsyntax tree ?
, we want the ordered syntax tree ?
?that maximizes the joint probability:2 All capital Latin letters denote constituents, andcorresponding lower-case Latin letters denote theirlabels (syntactic categories).
( ) ( ): _ ( )?
arg max , arg maxunordered treeP P?
?
?
??
?
?
?== =    (1)As equation (1) indicates, we can limit our searchto those trees ?
which are alternative orderings ofthe given tree ?
.Inter-dependencies among ordering decisionswithin different constituents (e.g., for achievingparallelism) make the global sentence orderingproblem challenging and are certainly worthinvestigating in future work.
For the present, weconstrain the possible model types considered hereby assuming that the ordering of any constituent isindependent of the ordering within otherconstituents in the tree, including its daughters;consequently,( ) ( )( )C constitsP P C??
?= ?Given this independence assumption, the onlypossible ordered trees are trees built with non-terminal constituents computed as follows: foreach ( )B constits ??
,( ): _ ( )* arg maxC B unordered set CC P C==In fact, we can further constrain our search for thebest ordering of each unordered constituent B,since C?s head must match B?s head:( ): _ ( )& ( ) ( )* arg maxC B unordered set Chead B head CC P C===Thus, we have reduced the problem to finding thebest ordering of each constituent of the unorderedtree.Now if we wish to condition on some feature ( )x f ?= , then we must first predict it as follows:( ) ( ): _ ( )& ( ) ( )* arg maxC B unordered set Chead B head CC P x P C x===If x is truly a feature of ?
and does not depend onany particular ordering of any constituent in ?
,then ( )P x  is constant, and we do not need tocompute it in practice.
In other words,( ): _ ( )& ( ) ( )* arg maxC B unordered set Chead B head CC P C x===       (2)Hence, even for a joint model ( )P C , we cancondition on features that are fixed in the givenunordered tree ?
without first predicting them.The joint models described here are of this form.For this reason, when we describe a distribution ( )P C x , unless we explicitly state otherwise, weare actually describing the part of the joint modelthat is of interest.
As justified above, we do notneed to compute ( )P x  and will simply presentalternative forms of ( )P C x .We can factor a distribution ( )P C x  in manydifferent ways using the chain rule.
As our startingpoint we adopt the class of models called Markovgrammars.3 We first consider a left-to-rightMarkov grammar of order j that expands C bypredicting its daughters 1,..., nD D  from left-to-right, one at a time, as shown in Figure 2: in thefigure.
iD  depends only on ( i jD ?
, ?, 1iD ?
), andthe parent category C ., according to thedistribution in equation (3).i?Figure 2: Left-to-right Markov grammar.
( ) ( )11,..., , ,ni i i jiP C h P d d d c h?
?== ?
(3)In order to condition on another feature of eachordered daughter iD , such as its semantic relationi?
to the head constituent H, we also first predictit, according to the chain rule.
The result is thesemantic Markov grammar in equation (4):( ) ( )( )1 11 1 1, ,..., , , ,, , ,..., , , ,n i i i i j i ji i i i i i j i jP d d c hP C hP d d d c h?
?
??
?
??
?
?
?=?
?
?
??
??
?=?
???
??
??
(4)Thus, the model predicts semantic relation i?
andthen the label id  in the context of that semanticrelation.
We will refer to this model as Type 1(T1).As an extension to model Type 1, we includefeatures computed by the following functions onthe set i?
of daughters of C already ordered (seeFigure 2):?
Number of daughters already ordered (sizeof i?
)?
Number of daughters in i?
having aparticular label for each of the possibleconstituent labels {NP, AUXP, VP, etc.
}(24 for German, 23 for French)We denote that set of features in shorthand as ( )if ?
.
With this extension, a model of Markov3A ?Markov grammar?
is a model of constituentstructure that starts at the root of the tree and assignsprobability to the expansion of a non-terminal onedaughter at a time, rather than as entire productions(Charniak, 1997 & 2000).order j can potentially have an actual Markov ordergreater than j.
Equation (5) is the extended model,which we will refer to as Type 2 (T2):( ) ( )( )( )( )1 11 1 1, ,..., , , , ,, , ,..., , , , ,n i i i i j i j ii i i i i i j i j iP d d c h fP C hP d d d c h f?
?
?
??
?
?
??
?
?
?=?
?
?
??
??
?=?
???
??
??
(5)As an alternative to a left-to-right expansion, wecan also expand a constituent in a head-drivenfashion.
We refer the reader to (Ringger et al,2003) for details and evaluations of several head-driven models (the missing ?T3?, ?T4?, and ?T6?in this discussion).2.2 Conditional ModelsWe now consider more complex models that useadditional features.
We define a function ( )g X onconstituents, where the value of ( )g X represents aset of many lexical, syntactic, and semanticfeatures of X (see section 5.2 for more details).
Nodiscourse features are included for the presentwork.
We condition on?
( )g B , where B is the unordered constituentbeing ordered?
( )g H , where H is the head of B?
( )Bg P , where BP  is the parent of B, and?
( )Bg G , where BG  is the grandparent of B.These features are fixed in the given unordered tree?
, as in the discussion of equation (2), hence theresulting complex model is still a joint model.Up until this point, we have been describing jointgenerative models that describe how to generate anordered tree from an unordered tree.
These modelsrequire extra effort and capacity to accuratelymodel the inter-relations among all features.
Nowwe move on to truly conditional models byincluding features that are functions on the set i?of daughters of C yet to be ordered.
In theconditional models we do not need to model theinterdependencies among all features.
We includethe following:?
Number of daughters remaining to beordered (size of i?
)?
Number of daughters in i?
having aparticular labelAs before, we denote these feature sets inshorthand as ( )if ?
.
The resulting distribution isrepresented in equation (6), which we will refer toas Type 5 (T5):( )( )( )1 11 1 1( ), ( ), ( ), ( ), ,..., , , , ,( ), ( ), ( ), ( ), , ( ), , ,..., , , , ,( ), ( ), ( ), ( ), , ( )B Bi i i j i jin B B i ii i i i i j i jiB B i iP C g H g B g P g Gd d c hPg H g B g P g G f fd d c hP dg H g B g P g G f f?
???
??
?
??
??
?
?
?=?
?
?
??
??
??
??
??
??
??
?=?
??
??
???
??
??
??
??
??
??
(6)All models in this paper are nominally Markovorder 2, although those models incorporating theadditional feature functions ( )if ?
and ( )if ?defined in Section 2.2 can be said to have higherorder.2.3 Binary conditional modelWe introduce one more model type called thebinary conditional model.
It estimates a muchsimpler distribution over the binary variable ?called ?sort-next?
with values in {yes, no}representing the event that an as-yet unorderedmember D of i?
(the set of as-yet unordereddaughters of parent C, as defined above) should be?sorted?
next, as illustrated in Figure 3.i?i?
?Figure 3: Binary conditional model.The conditioning features are almost identical tothose used in the left-to-right conditional modelsrepresented in equation (6) above, except that idand i?
(the semantic relation of D with head H)appear in the conditional context and need not firstbe predicted.
In its simple form, the modelestimates the following distribution:( )1 1, , , ,..., , , , ,( ), ( ), ( ), ( ), , ( )i i i i i j i jiB B i id d d c hPg H g B g P g G f f?
?
???
??
?
?
??
??
??
??
?
(7)In our shorthand, we will call this Type 7 (T7).
Wedescribe how to apply this model directly in a left-to-right ?sorting?
search later in the section onsearch.3 EstimationWe estimate a model?s distributions withprobabilistic decision trees (DTs).4 We builddecision trees using the WinMine toolkit(Chickering, 2002).
WinMine-learned decisiontrees are not just classifiers; each leaf is aconditional probability distribution over the targetrandom variable, given all features available intraining; hence the tree as a whole is an estimate ofthe conditional distribution of interest.
The primaryadvantage of using decision trees, is the automaticfeature selection and induction from a large pool offeatures.We train four models for German and Frencheach.
One model is joint (T1); one is joint withadditional features on the set of daughters alreadyordered (T2); one is conditional (T5).
In addition,we employ one binary conditional DT model (T7),both with and without normalization (see equation(8)).4Other approaches to feature selection, featureinduction, and distribution estimation are certainlypossible, but they are beyond the scope of this paper.One experiment using interpolated language modelingtechniques is described in (Ringger et al, 2003)4 Search4.1 Exhaustive searchGiven an unordered tree ?
and a model ofconstituent structure O of any of the types alreadypresented, we search for the best ordered tree ?that maximizes ( )OP ?
or ( )OP ?
?
, asappropriate, with the context varying according tothe complexity of the model.
Each of our models(except the binary conditional model) estimates theprobability of an ordering of any given constituentC in ?
, independently of the ordering inside otherconstituents in ?
.
The complete search is adynamic programming (DP) algorithm, either left-to-right in the daughters of C (or head-driven,depending on the model type).
The search canoptionally maintain one non-statistical constraintwe call Input-Output Coordination Consistency(IOCC), so that the order of coordinatedconstituents is preserved as they were specified inthe given unordered tree.
For these experiments,we employ the constraint.4.2 Greedy search for binary conditionalmodelThe binary conditional model can be applied in aleft-to-right ?sorting?
mode (Figure 3).
At stage i,for each unordered daughter jD , in i?
, the modelis consulted for the probability of j yes?
= ,namely the probability that jD  should be placed tothe right of the already ordered sister constituentsi?
.
The daughter in i?
with the highestprobability is removed from i?
to produce 1i?
+and added to the right of i?
to produce 1i?
+ .
Thesearch proceeds through the remaining unorderedconstituents until all constituents have beenordered in this greedy fashion.4.3 Exhaustive search for binary conditionalmodelIn order to apply the binary conditional model inthe exhaustive DP search, we normalize the modelat every stage of the search and thereby coerce itinto a probability distribution over the remainingdaughters in i?
.
We represent the distribution in?equation?
(7) in short-hand as ( ), , iP d?
?
?
,with i?
representing the contextual features for thegiven search hypothesis at search stage i.
Thus, ournormalized distribution for stage i is given byequation (8).
Free variable j represents an index onunordered daughters in i?
, as does k.( ) ( )( )1, ,, ,, ,ij j j ij j j ik k k ikP yes dP D dP yes d??
???
?== ??
== ??
(8)This turns out to be the decision tree analogue of aMaximum Entropy Markov Model (MEMM)(McCallum et al, 2000), which we can refer to as aDTMM.5 Experiments5.1 TrainingWe use a training set of 20,000 sentences, bothfor French and German.
The data come fromtechnical manuals in the computer domain.
For agiven sentence in our training set, we begin byanalyzing the sentence as a surface syntax tree andan abstract predicate argument structure using theNLPWin system (Heidorn, 2000).
By consultingthese two linked structures, we produce a tree withall of the characteristics of trees seen by theAmalgam ordering stage at generation run-timewith one exception: these training trees areproperly ordered.
The training trees include allfeatures of interest, including the semanticrelations among a syntactic head and its modifiers.We train our order models from the constituents ofthese trees.
NLPWin parser output naturallycontains errors; hence, the Amalgam training datais imperfect.5.2 Selected FeaturesA wide range of linguistic features is extractedfor the different decision tree models.
The numberof selected features for German reaches 280 (out of651 possible features) in the binary conditionalmodel T7.
For the French binary conditionalmodel, the number of selected features is 218 (outof 550).
The binary conditional models draw fromthe full set of available features, including:?
lexical sub-categorization features such astransitivity and compatibility with clausalcomplements?
lemmas (word-stems)?
semantic features such as the semanticrelation and the presence ofquantificational operators?
length of constituent in words?
syntactic information such as the label andthe presence of syntactic modifiers5.3 EvaluationTo evaluate the constituent order models inisolation, we designed our experiments to beindependent of the rest of the Amalgam sentencerealization process.
We use test sets of 1,000sentences, also from technical manuals, for eachlanguage.
To isolate ordering, for a given testsentence, we process the sentence as in training toproduce an ordered tree ?
(the reference forevaluation) and from it an unordered tree ?
.Given ?
, we then search for the best ordered treehypothesis ??
using the model in question.We then compare ?
and ??
.
Because we areonly ordering constituents, we can compare ?
and??
by comparing their respective constituents.
Foreach C in ?
, we measure the per-constituent editdistance D, between C and its counterpart C?
in ?
?as  follows:1.
Let d be the edit distance between theordered set of daughters in each, with theonly possible edit operators being insert anddelete2.
Let the number of moves / 2m d= , sinceinsertions and deletions can be paireduniquely3.
Divide by the total number ofdaughters: ( )/D m daughters C=This metric is like the ?Generation Tree Accuracy?metric of Bangalore & Rambow (2000), exceptthat there is no need to consider cross-constituentmoves.
The total score for the hypothesis tree ??
isthe mean of the per-constituent edit distances.For each of the models under consideration andeach language, we report in Table 1 the averagescore across the test set for the given language.
Thefirst row is a baseline computed from randomlyscrambling constituents (mean over fouriterations).Model German FrenchBaseline (random) 35.14 % 34.36 %T1: DT joint 5.948% 3.828%T2: DT jointwith ( )if ?
5.852% 4.008%T5: DT conditional 6.053% 4.271%T7: DT binary cond.,greedy search 3.516% 1.986%T7: DT normalizedbinary conditional,exhaustive search3.400% 1.810%Table 1: Mean per-constituent edit distances forGerman & French.5.4 DiscussionFor both German and French, the binaryconditional DT model outperforms all othermodels.
Normalizing the binary conditional modeland applying it in an exhaustive search performsbetter than a greedy search.
All score differencesare statistically significant; moreover, manualinspection of the differences for the various modelsalso substantiates the better quality of those modelswith lower scores.With regard to the question of conditional versusjoint models, the joint models (T1, T2) outperformtheir conditional counterparts (T5).
This may bedue to a lack of sufficient training data for theconditional models.
At this time, the training timeof the conditional models is the limiting factor.There is a clear disparity between theperformance of the German models and theperformance of the French models.
The bestGerman model is twice as bad as the best Frenchmodel.
(For a discussion of the impact ofmodeling German verb position, please consult(Ringger et al, 2003).
)Baseline(random)Greedy,IOCC GreedyDP,IOCC DPTotal Sentences 2416 2416 2416 2416 2416Mean Tokens/Sentence 23.59 23.59 23.59 23.59 23.59Time/Input (sec.)
n/a 0.01 0.01 0.39 0.43Exact Match 0.424% 33.14% 27.53% 33.53% 35.72%Coverage 100% 100% 100% 100% 100%Mean Per-Const.
Edit Dist.
38.3% 6.02% 6.84% 5.25% 4.98%Mean NIST SSA -16.75 74.98 67.19 74.65 73.24Mean IBM Bleu Score 0.136 0.828 0.785 0.817 0.836Table 2: DSIF-Amalgam ordering performance on WSJ section 23.6 Evaluation on the Penn TreeBankOur goal in evaluating on Penn Tree Bank (PTB)data is two-fold: (1) to enable a comparison ofAmalgam?s performance with other systemsoperating on similar input, and (2) to measureAmalgam?s capabilities on less domain-specificdata than technical software manuals.
We derivefrom the bracketed tree structures in the PTB usinga deterministic procedure an abstractrepresentation we refer to as a DependencyStructure Input Format (DSIF), which is onlyloosely related to NLPWin?s abstract predicate-argument structures.The PTB to DSIF transformation pipelineincludes the following stages, inspired byLangkilde-Geary?s (2002b) description:A. Deserialize the treeB.
Label heads, according to Charniak?s headlabeling rules (Charniak, 2000)C. Remove empty nodes and flatten anyremaining empty non-terminalsD.
Relabel heads to conform more closely to thehead conventions of NLPWinE.
Label with logical roles, inferred from PTBfunctional rolesF.
Flatten to maximal projections of heads(MPH), except in the case of conjunctionsG.
Flatten non-branching non-terminalsH.
Perform dictionary look-up andmorphological analysisI.
Introduce structure for material betweenpaired delimiters and for any coordinationnot already represented in the PTBJ.
Remove punctuationK.
Remove function wordsL.
Map the head of each maximal projection toa dependency node, and map the head?smodifiers to the first node?s dependents,thereby forming a complete dependency tree.To evaluate ordering performance alone, our dataare obtained by performing all of the steps aboveexcept for (J) and (K).
We employ only a binaryconditional ordering model, found in the previoussection to be the best of the models considered.
Totrain the order models, we use a set of 10,000sentences drawn from the standard PTB trainingset, namely sections 02?21 from the Wall StreetJournal portion of the PTB (the full set containsapprox.
40,000 sentences).
For development andparameter tuning we used a separate set of 2000sentences drawn from sections 02?21.Decision trees are trained for each of fiveconstituent types characterized by their headlabels: adjectival, nominal, verbal, conjunctions(coordinated material), and other constituents notalready covered.
The split DTs can be thought ofas a single DT with a five-way split at the topnode.Our DSIF test set consists of the blind test set(section 23) of the WSJ portion of the PTB.
Atrun-time, for each converted tree in the test set, alldaughters of a given constituent are first permutedrandomly with one another (scrambled), with theoption for coordinated constituents to remainunscrambled, according to the Input-OutputCoordination Consistency (IOCC) option.
For agiven unordered (scrambled) constituent, theappropriate order model (noun-head, verb-head,etc.)
is used in the ordering search to order thedaughters.
Note that for the greedy search, theinput order can influence the final result; therefore,we repeat this process for multiple randomscramblings and average the results.We use the evaluation metrics employed inpublished evaluations of HALogen, FUF/SURGE,and FERGUS (e.g., Calloway, 2003), although ourresults are for ordering only.
Coverage, or thepercentage of inputs for which a system canproduce a corresponding output, is uninformativefor the Amalgam system, since in all cases, it cangenerate an output for any given DSIF.
In additionto processing time per input, we apply four othermetrics: exact match, NIST simple string accuracy(the complement of the familiar word error rate),the IBM Bleu score (Papineni et al, 2001), and theintra-constituent edit distance metric introducedearlier.We evaluate against ideal trees, directlycomputed from PTB bracketed tree structures.
Theresults in Table 2 show the effects of varying theIOCC parameter.
For both trials involving a greedysearch, the results were averaged across 25iterations.
As should be expected, turning on theinput-output faithfulness option (IOCC) improvesthe performance of the greedy search.
Keepingcoordinated material in the same relative orderwould only be called for in applications that plandiscourse structure before or during generation.7 Conclusions and Future WorkThe experiments presented here provideconclusive reasons to favor the binary conditionalmodel as a model of constituent order.
Theinclusion of linguistic features is of great value tothe modeling of order, specifically in verbalconstituents for both French and German.Unfortunately space did not permit a thoroughdiscussion of the linguistic features used.
Judgingfrom the high number of features that wereselected during training for participation in theconditional and binary conditional models, theavailability of automatic feature selection iscritical.Our conditional and binary conditional modelsare currently lexicalized only for function words;the joint models not at all.
Experiments by Daum?et al(2002) and the parsing work of Charniak(2000) and others indicate that furtherlexicalization may yield some additionalimprovements for ordering.
However, the parsingresults of Klein & Manning (2003) involvingunlexicalized grammars suggest that gains may belimited.For comparison, we encourage implementers ofother sentence realization systems to conductorder-only evaluations using PTB data.AcknowledgementsWe wish to thank Irene Langkilde-Geary andmembers of the MSR NLP group for helpfuldiscussions.
Thanks also go to the anonymousreviewers for helpful feedback.ReferencesAikawa T., Melero M., Schwartz L. Wu A.
2001.Multilingual sentence generation.
In Proc.
of 8thEuropean Workshop on NLG.
pp.
57-63.Bangalore S. Rambow O.
2000.
Exploiting aprobabilistic hierarchical model for generation.In Proc.
of COLING.
pp.
42-48.Calloway, C. 2003.
Evaluating Coverage for LargeSymbolic NLG Grammars.
In Proc.
of IJCAI2003.
pp 811-817.Charniak E. 1997.
Statistical Techniques forNatural Language Parsing, In AI Magazine.Charniak E. 2000.
A Maximum-Entropy-InspiredParser.
In Proc.
of ACL.
pp.132-139.Chickering D. M. 2002.
The WinMine Toolkit.Microsoft Technical Report 2002-103.Corston-Oliver S., Gamon M., Ringger E., MooreR.
2002.
An overview of Amalgam: a machine-learned generation module.
In Proc.
of INLG.pp.33-40.Daum?
III H., Knight K., Langkilde-Geary I.,Marcu D., Yamada K. 2002.
The Importance ofLexicalized Syntax Models for NaturalLanguage Generation Tasks.
In Proc.
of INLG.pp.
9-16.Gamon M., Ringger E., Corston-Oliver S. 2002a.Amalgam: A machine-learned generationmodule.
Microsoft Technical Report 2002-57.Gamon M., Ringger E., Corston-Oliver S., MooreR.
2002b.
Machine-learned contexts forlinguistic operations in German sentencerealization.
In Proc.
of ACL.
pp.
25-32.Heidorn G. 2000.
Intelligent Writing Assistance.
InA Handbook of Natural Language Processing,,R.
Dale, H. Moisl, H. Somers (eds.).
MarcelDekker, NY.Klein D., Manning C. 2003.
"AccurateUnlexicalized Parsing."
In Proceedings of ACL-03.Langkilde I.
2000.
Forest-Based StatisticalSentence generation.
In Proc.
of NAACL.
pp.170-177.Langkilde-Geary I.
2002a.
An EmpiricalVerification of Coverage and Correctness for aGeneral-Purpose Sentence Generator.
In Proc.
ofINLG.
pp.17-24.Langkilde-Geary, I.
2002b.
A Foundation forGeneral-purpose Natural Language Generation:Sentence Realization Using Probabilistic Modelsof Language.
PhD Thesis, University ofSouthern California.Langkilde I., Knight K. 1998a.
The practical valueof n-grams in generation.
In Proc.
of 9thInternational Workshop on NLG.
pp.
248-255.Langkilde I., Knight K. 1998b.
Generation thatexploits corpus-based statistical knowledge.
InProc.
of ACL and COLING.
pp.
704-710.McCallum A., Freitag D., & Pereira F.
2000.?Maximum Entropy Markov Models forInformation Extraction and Segmentation.?
InProc.
Of ICML-2000.Papineni, K.A., Roukos, S., Ward, T., and Zhu,W.J.
2001.
Bleu: a method for automaticevaluation of machine translation.
IBMTechnical Report RC22176 (W0109-022).Reiter E. and Dale R. 2000.
Building naturallanguage generation systems.
CambridgeUniversity Press, Cambridge.Ringger E., Gamon M., Smets M., Corston-OliverS.
and Moore R. 2003 Linguistically informedmodels of constituent structure for ordering insentence realization.
Microsoft Researchtechnical report MSR-TR-2003-54.Smets M., Gamon M., Corston-Oliver S. andRingger E. (2003) The adaptation of a machine-learned sentence realization system to French.In Proceedings of EACL.
