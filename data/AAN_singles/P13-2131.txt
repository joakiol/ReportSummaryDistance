Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 748?752,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsSmatch: an Evaluation Metric for Semantic Feature StructuresShu CaiUSC Information Sciences Institute4676 Admiralty Way, Suite 1001Marina del Rey, CA 90292shucai@isi.eduKevin KnightUSC Information Sciences Institute4676 Admiralty Way, Suite 1001Marina del Rey, CA 90292knight@isi.eduAbstractThe evaluation of whole-sentence seman-tic structures plays an important role insemantic parsing and large-scale seman-tic structure annotation.
However, there isno widely-used metric to evaluate whole-sentence semantic structures.
In this pa-per, we present smatch, a metric that cal-culates the degree of overlap between twosemantic feature structures.
We give anefficient algorithm to compute the metricand show the results of an inter-annotatoragreement study.1 IntroductionThe goal of semantic parsing is to generate all se-mantic relationships in a text.
Its output is of-ten represented by whole-sentence semantic struc-tures.
Evaluating such structures is necessary forsemantic parsing tasks, as well as semantic anno-tation tasks which create linguistic resources forsemantic parsing.However, there is no widely-used evalua-tion method for whole-sentence semantic struc-tures.
Current whole-sentence semantic parsingis mainly evaluated in two ways: 1. task cor-rectness (Tang and Mooney, 2001), which eval-uates on an NLP task that uses the parsing re-sults; 2. whole-sentence accuracy (Zettlemoyerand Collins, 2005), which counts the number ofsentences parsed completely correctly.Nevertheless, it is worthwhile to explore evalua-tion methods that use scores which range from 0 to1 (?partial credit?)
to measure whole-sentence se-mantic structures.
By using such methods, we areable to differentiate between two similar whole-sentence semantic structures regardless of specifictasks or domains.
In this work, we provide an eval-uation metric that uses the degree of overlap be-tween two whole-sentence semantic structures asthe partial credit.In this paper, we observe that the difficultyof computing the degree of overlap between twowhole-sentence semantic feature structures comesfrom determining an optimal variable alignmentbetween them, and further prove that finding suchalignment is NP-complete.
We investigate how tocompute this metric and provide several practicaland replicable computing methods by using Inte-ger Linear Programming (ILP) and hill-climbingmethod.
We show that our metric can be usedfor measuring the annotator agreement in large-scale linguistic annotation, and evaluating seman-tic parsing.2 Semantic OverlapWe work on a semantic feature structure represen-tation in a standard neo-Davidsonian (Davidson,1969; Parsons, 1990) framework.
For example,semantics of the sentence ?the boy wants to go?
isrepresented by the following directed graph:In this graph, there are three concepts: want-01, boy, and go-01.
Both want-01 and go-01 areframes from PropBank framesets (Kingsbury andPalmer, 2002).
The frame want-01 has two argu-ments connected with ARG0 and ARG1, and go-01 has an argument (which is also the same boyinstance) connected with ARG0.748Following (Langkilde and Knight, 1998) and(Langkilde-Geary, 2002), we refer to this semanticrepresentation as AMR (Abstract Meaning Repre-sentation).Semantic relationships encoded in the AMRgraph can also be viewed as a conjunction of logi-cal propositions, or triples:instance(a, want-01) ?instance(b, boy) ?instance(c, go-01) ?ARG0(a, b) ?ARG1(a, c) ?ARG0(c, b)Each AMR triple takes one of these forms:relation(variable, concept) (the first three triplesabove), or relation(variable1, variable2) (the lastthree triples above).Suppose we take a second AMR (for ?the boywants the football?)
and its associated proposi-tional triples:instance(x, want-01) ?instance(y, boy) ?instance(z, football) ?ARG0(x, y) ?ARG1(x, z)Our evaluation metric measures precision, re-call, and f-score of the triples in the second AMRagainst the triples in the first AMR, i.e., theamount of propositional overlap.The difficulty is that variable names are notshared between the two AMRs, so there are mul-tiple ways to compute the propositional overlapbased on different variable mappings.
We there-fore define the smatch score (for semantic match)as the maximum f-score obtainable via a one-to-one matching of variables between the two AMRs.In the example above, there are six ways tomatch up variables between the two AMRs:M P R Fx=a, y=b, z=c: 4 4/5 4/6 0.73x=a, y=c, z=b: 1 1/5 1/6 0.18x=b, y=a, z=c: 0 0/5 0/6 0.00x=b, y=c, z=a: 0 0/5 0/6 0.00x=c, y=a, z=b: 0 0/5 0/6 0.00x=c, y=b, z=a: 2 2/5 2/6 0.36----------------------------------smatch score: 0.73Here, M is the number of propositional triples thatagree given a variable mapping, P is the precisionof the second AMR against the first, R is its re-call, and F is its f-score.
The smatch score is themaximum of the f-scores.However, for AMRs that contain large numberof variables, it is not efficient to get the f-score bysimply using the method above.
Exhaustively enu-merating all variable mappings requires comput-ing the f-score for n!/(n?m)!
variable mappings(assuming one AMR has n variables and the otherhas m variables, and m ?
n).
This algorithm istoo slow for all but the shortest AMR pairs.3 Computing the MetricThis section describes how to compute the smatchscore.
As input, we are given AMR1 (with m vari-ables) and AMR2 (with n variables).
Without lossof generality, m ?
n.Baseline.
Our baseline first matches variablesthat share concepts.
For example, it would matcha in the first AMR example with x in the secondAMR example of Section 2, because both are in-stances of want-01.
If there are two or more vari-ables to choose from, we pick the first availableone.
The rest of the variables are mapped ran-domly.ILP method.
We can get an optimal solutionusing integer linear programming (ILP).
We createtwo types of variables:?
(Variable mapping) vij = 1 iff the ith vari-able in AMR1 is mapped to the jth variablein AMR2 (otherwise vij = 0)?
(Triple match) tkl = 1 iff AMR1 triplek matches AMR2 triple l, otherwise tkl= 0.
A triple relation1(xy) matchesrelation2(wz) iff relation1 = relation2, vxw= 1, and vyz = 1 or y and z are the same con-cept.Our constraints ensure a one-to-one mapping ofvariables, and they ensure that the chosen t valuesare consistent with the chosen v values:For all i,?jvij ?
1For all j,?ivij ?
1For all triple pairs r(xy)r(wz) (r for relation),tr(xy)r(wz) ?
vxw749tr(xy)r(wz) ?
vyzwhen y and z are variables.Finally, we ask the ILP solver to maximize:?kltklwhich denotes the maximum number of matchingtriples which lead to the smatch score.Hill-climbing method.
Finally, we develop aportable heuristic algorithm that does not requirean ILP solver1.
This method works in a greedystyle.
We begin with m random one-to-one map-pings between the m variables of AMR1 and then variables of AMR2.
Each variable mapping isa pair (i,map(i)) with 1 ?
i ?
m and 1 ?map(i) ?
n. We refer to the m mappings as avariable mapping state.We first generate a random initial variable map-ping state, compute its triple match number, thenhill-climb via two types of small changes:1.
Move one of the m mappings to a currently-unmapped variable from the n.2.
Swap two of the m mappings.Any variable mapping state has m(n ?
m) +m(m ?
1) = m(n ?
1) neighbors during thehill-climbing search.
We greedily choose the bestneighbor, repeating until no neighbor improves thenumber of triple matches.We experiment with two modifications to thegreedy search: (1) executing multiple randomrestarts to avoid local optima, and (2) using ourBaseline concept matching (?smart initialization?
)instead of random initialization.NP-completeness.
There is unlikely to bean exact polynomial-time algorithm for comput-ing smatch.
We can reduce the 0-1 MaximumQuadratic Assignment Problem (0-1-Max-QAP)(Nagarajan and Sviridenko, 2009) and the sub-graph isomorphism problem directly to the fullsmatch problem on graphs.2We note that other widely-used metrics, such asTER (Snover et al 2006), are also NP-complete.Fortunately, the next section shows that the smatchmethods above are efficient and effective.1The tool can be downloaded athttp://amr.isi.edu/evaluation.html.2Thanks to David Chiang for observing the subgraph iso-morphism reduction.4 Using SmatchWe report an AMR inter-annotator agreementstudy using smatch.1.
Our study has 4 annotators (A, B, C, D), whothen converge on a consensus annotation E.We thus have 10 pairs of annotations: A-B,A-C, .
.
.
, D-E.2.
The study is carried out 5 times.
Eachtime annotators build AMRs for 4 sentencesfrom the Wall Street Journal corpus.
Sen-tence lengths range from 12 to 54 words, andAMRs range from 6 to 29 variables.3.
We use 7 smatch calculation methods in ourexperiments:?
Base: Baseline matching method?
ILP: Integer Linear Programming?
R: Hill-climbing with random initializa-tion?
10R: Hill-climbing with random initial-ization plus 9 random restarts?
S: Hill-climbing with smart initializa-tion?
S+4R: Hill-climbing with smart initial-ization plus 4 random restarts?
S+9R: Hill-climbing with smart initial-ization plus 9 random restartsTable 1 shows smatch scores provided by themethods.
Columns labeled 1-5 indicate sen-tence groups.
Each individual smatch score isa document-level score of 4 AMR pairs.3 ILPscores are optimal, so lower scores (in bold) in-dicate search errors.Table 2 summarizes search accuracy as a per-centage of smatch scores that equal that of ILP.Results show that the restarts are essential for hill-climbing, and that 9 restarts are sufficient to obtaingood quality.
The table also shows total runtimesover 200 AMR pairs (10 annotator pairs, 5 sen-tence groups, 4 AMR pairs per group).
Heuris-tic search with smart initialization and 4 restarts(S+4R) gives the best trade-off between accuracyand speed, so this is the setting we use in practice.Figure 1 shows smatch scores of each annotator(A-D) against the consensus annotation (E).
The3For documents containing multiple AMRs, we use thesum of matched triples over all AMR pairs to compute pre-cision, recall, and f-score, much like corpus-level Bleu (Pap-ineni et al 2002).750B C D E1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5Base 0.68 0.74 0.84 0.71 0.83 0.69 0.70 0.80 0.69 0.78 0.77 0.72 0.75 0.68 0.63 0.79 0.86 0.92 0.85 0.89ILP 0.74 0.80 0.84 0.76 0.88 0.75 0.78 0.80 0.77 0.88 0.83 0.77 0.75 0.72 0.76 0.85 0.92 0.92 0.89 0.92R 0.74 0.79 0.84 0.75 0.86 0.74 0.75 0.80 0.77 0.88 0.83 0.76 0.75 0.72 0.75 0.85 0.92 0.92 0.89 0.89A 10R 0.74 0.80 0.84 0.76 0.88 0.75 0.78 0.80 0.77 0.88 0.83 0.77 0.75 0.72 0.76 0.85 0.92 0.92 0.89 0.92S 0.74 0.80 0.84 0.75 0.88 0.75 0.78 0.80 0.76 0.88 0.83 0.77 0.75 0.72 0.76 0.85 0.92 0.92 0.89 0.92S+4R 0.74 0.80 0.84 0.76 0.88 0.75 0.78 0.80 0.77 0.88 0.83 0.77 0.75 0.72 0.76 0.85 0.92 0.92 0.89 0.92S+9R 0.74 0.80 0.84 0.76 0.88 0.75 0.78 0.80 0.77 0.88 0.83 0.77 0.75 0.72 0.76 0.85 0.92 0.92 0.89 0.92Base - - - - - 0.72 0.68 0.74 0.69 0.79 0.71 0.72 0.76 0.65 0.57 0.68 0.71 0.83 0.79 0.86ILP - - - - - 0.74 0.83 0.74 0.75 0.85 0.78 0.83 0.76 0.68 0.73 0.76 0.81 0.83 0.83 0.89R - - - - - 0.74 0.83 0.72 0.72 0.83 0.78 0.83 0.76 0.68 0.68 0.74 0.81 0.83 0.83 0.89B 10R - - - - - 0.74 0.83 0.74 0.75 0.85 0.78 0.83 0.76 0.68 0.73 0.76 0.81 0.83 0.83 0.89S - - - - - 0.73 0.83 0.74 0.75 0.85 0.78 0.83 0.76 0.68 0.73 0.76 0.81 0.83 0.83 0.89S+4R - - - - - 0.74 0.83 0.74 0.75 0.85 0.78 0.83 0.76 0.68 0.73 0.76 0.81 0.83 0.83 0.89S+9R - - - - - 0.74 0.83 0.74 0.75 0.85 0.78 0.83 0.76 0.68 0.73 0.76 0.81 0.83 0.83 0.89Base - - - - - - - - - - 0.68 0.68 0.74 0.69 0.65 0.64 0.64 0.87 0.79 0.83ILP - - - - - - - - - - 0.74 0.79 0.74 0.78 0.81 0.74 0.76 0.87 0.85 0.89R - - - - - - - - - - 0.74 0.79 0.74 0.75 0.78 0.71 0.76 0.87 0.85 0.89C 10R - - - - - - - - - - 0.74 0.79 0.74 0.78 0.81 0.74 0.76 0.87 0.85 0.89S - - - - - - - - - - 0.74 0.79 0.74 0.77 0.81 0.74 0.76 0.87 0.85 0.89S+4R - - - - - - - - - - 0.74 0.79 0.74 0.78 0.81 0.74 0.76 0.87 0.85 0.89S+9R - - - - - - - - - - 0.74 0.79 0.74 0.78 0.81 0.74 0.76 0.87 0.85 0.89Base - - - - - - - - - - - - - - - 0.68 0.69 0.81 0.74 0.64ILP - - - - - - - - - - - - - - - 0.77 0.78 0.81 0.78 0.79R - - - - - - - - - - - - - - - 0.77 0.73 0.81 0.78 0.79D 10R - - - - - - - - - - - - - - - 0.77 0.78 0.81 0.78 0.79S - - - - - - - - - - - - - - - 0.77 0.78 0.81 0.78 0.79S+4R - - - - - - - - - - - - - - - 0.77 0.78 0.81 0.78 0.79S+9R - - - - - - - - - - - - - - - 0.77 0.78 0.81 0.78 0.79Table 1: Inter-annotator smatch agreement for 5 groups of sentences, as computed with seven differentmethods (Base, ILP, R, 10R, S, S+4R, S+9R).
The number 1-5 indicate the sentence group number.
Boldscores are search errors.Base ILP R 10R S S+4R S+9RAccuracy 20% 100% 66.5% 100% 92% 100% 100%Time (sec) 0.86 49.67 5.85 64.78 2.31 28.36 59.69Table 2: Accuracy and running time (seconds) ofvarious computing methods of smatch over 200AMR pairs.plot demonstrates that, as time goes by, annotatorsreach better agreement with the consensus.We also note that smatch is used to measurethe accuracy of machine-generated AMRs.
(Joneset al 2012) use it to evaluate automatic seman-tic parsing in a narrow domain, while Ulf Her-mjakob4 has developed a heuristic algorithm thatexploits and supplements Ontonotes annotations(Pradhan et al 2007) in order to automaticallycreate AMRs for Ontonotes sentences, with asmatch score of 0.74 against human consensusAMRs.5 Related WorkRelated work on directly measuring the seman-tic representation includes the method in (Dri-dan and Oepen, 2011), which evaluates semanticparser output directly by comparing semantic sub-structures, though they require an alignment be-tween sentence spans and semantic sub-structures.In contrast, our metric does not require the align-4personal communication0.70.750.80.850.90.9511  2  3  4  5Smatch scores of each annotatoragainstconsensusTimeAnnotator AAnnotator BAnnotator CAnnotator DFigure 1: Smatch scores of annotators (A-D)against the consensus annotation (E) over time.ment between an input sentence and its semanticanalysis.
(Allen et al 2008) propose a metricwhich computes the maximum score by any align-ment between LF graphs, but they do not addresshow to determine the alignments.6 Conclusion and Future WorkWe present an evaluation metric for whole-sentence semantic analysis, and show that it canbe computed efficiently.
We use the metric tomeasure semantic annotation agreement rates andparsing accuracy.
In the future, we plan to investi-gate how to adapt smatch to other semantic repre-sentations.7517 AcknowledgementsWe would like to thank David Chiang, Hui Zhang,other ISI colleagues and our anonymous review-ers for their thoughtful comments.
This work wassupported by NSF grant IIS-0908532.ReferencesJ.F.
Allen, M. Swift, and W. Beaumont.
2008.
DeepSemantic Analysis of Text.
In Proceedings of the2008 Conference on Semantics in Text Processing.D.
Davidson.
1969.
The Individuation of Events.
InNicholas Rescher (ed.)
Essays in Honor of Carl G.HempeL Dordrecht: D. Reidel.R.
Dridan and S. Oepen.
2011.
Parser Evaluation us-ing Elementary Dependency Matching.
In Proceed-ings of the 12th International Conference on ParsingTechnologies.B.
Jones, J. Andreas, D. Bauer, K. M. Hermann, andK.
Knight.
2012.
Semantics-Based Machine Trans-lation with Hyperedge Replacement Grammars.
InProceedings of COLING.P.
Kingsbury and M. Palmer.
2002.
From Treebank toPropbank.
In Proceedings of LREC.I.
Langkilde and K. Knight.
1998.
Generation that Ex-ploits Corpus-based Statistical Knowledge.
In Pro-ceedings of COLING-ACL.I.
Langkilde-Geary.
2002.
An Empirical Verifica-tion of Coverage and Correctness for a General-Purpose Sentence Generator.
In Proceedings of In-ternational Natural Language Generation Confer-ence (INLG?02).V.
Nagarajan and M. Sviridenko.
2009.
On the Maxi-mum Quadratic Assignment Problem.
Mathematicsof Operations Research, 34.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.BLEU: a Method for Automatic Evaluation of Ma-chine Translation.
In Proceedings of the 40th An-nual Meeting on Association for Computational Lin-guistics.T.
Parsons.
1990.
Events in the Semantics of English.The MIT Press.S.
S. Pradhan, E. Hovy, M. Marcus, M. Palmer,L.
Ramshaw, and R. Weischedel.
2007.
Ontonotes:A Unified Relational Semantic Representation.
InProceedings of the International Conference on Se-mantic Computing (ICSC ?07).M.
Snover, B. Dorr, R. Schwartz, L. Micciulla, andJ.
Makhoul.
2006.
A Study of Translation Edit Ratewith Targeted Human Annotation.
In Proceedingsof the 7th Conference of the Association for MachineTranslation in the Americas (AMTA-2006).L.
R. Tang and R. J. Mooney.
2001.
Using MultipleClause Constructors in Inductive Logic Program-ming for Semantic Parsing.
In Proceedings of the12th European Conference on Machine Learning.L.
S. Zettlemoyer and M. Collins.
2005.
Learning toMap Sentences to Logical Form: Structured Classi-fication with Probabilistic Categorial Grammars.
InProceedings of the 21st Conference in Uncertaintyin Artificial Intelligence.752
