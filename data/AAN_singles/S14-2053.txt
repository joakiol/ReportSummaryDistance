Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 319?323,Dublin, Ireland, August 23-24, 2014.IITP:Supervised Machine Learning for Aspect based Sentiment AnalysisDeepak Kumar GuptaIndian Institute of Technology PatnaPatna, Indiadeepak.mtmc13@iitp.ac.inAsif EkbalIndian Institute of Technology PatnaPatna, Indiaasif@iitp.ac.inAbstractThe shared task on Aspect based Senti-ment Analysis primarily focuses on miningrelevant information from the thousandsof online reviews available for a popularproduct or service.
In this paper we re-port our works on aspect term extractionand sentiment classification with respectto our participation in the SemEval-2014shared task.
The aspect term extractionmethod is based on supervised learningalgorithm, where we use different classi-fiers, and finally combine their outputs us-ing a majority voting technique.
For senti-ment classification we use Random Forestclassifier.
Our system for aspect term ex-traction shows the F-scores of 72.13% and62.84% for the restaurants and laptops re-views, respectively.
Due to some techni-cal problems our submission on sentimentclassification was not evaluated.
Howeverwe evaluate the submitted system with thesame evaluation metrics, and it shows theaccuracies of 67.37% and 67.07% for therestaurants and laptops reviews, respec-tively.1 IntroductionNowadays user review is one of the means to drivethe sales of products or services.
There is a grow-ing trend among the customers who look at the on-line reviews of products or services before takinga final decision.
In sentiment analysis and opinionmining, aspect extraction aims to extract entity as-pects or features on which opinions have been ex-pressed (Hu and Liu, 2004; Liu, 2012).
An aspectis an attribute or component of the product thatThis work is licensed under a Creative Commons At-tribution 4.0 International License.
Page numbers and pro-ceedings footer are added by the organizers.
License details:http://creativecommons.org/licenses/by/4.0/has been commented on in a review.
For exam-ple:?Dell Laptop has very good battery life andclick pads?.
Here aspect terms are battery life andclick pads.
Sentiment analysis is the task of iden-tifying the polarity (positive, negative or neutral)of review.
Aspect terms can influence sentimentpolarity within a single domain.
As an example,for the restaurant domain cheap is usually posi-tive with respect to food, but it denotes a negativepolarity when discussing the decor or ambiance(Brody and Elhadad, 2010).A key task of aspect based sentiment analysisis to extract aspects of entities and determine thesentiment corresponding to aspect terms that havebeen commented in review document.
In recenttimes there has been huge interest to identify as-pects and sentiments simultaneously.
The methodproposed in (Hu and Liu, 2004) is based on infor-mation extraction (IE) approach that identifies fre-quently occurring noun phrases using associationmining.
Some other works include the methods,viz those that define aspect terms using a manuallyspecified subset of the Wikipedia category (Fahrniand Klenner, 2008) hierarchy, unsupervised clus-tering technique (Popescu and Etzionir, 2005) andsemantically motivated technique (Turney, 2002)etc.
Our proposed approach for aspect term ex-traction is based on supervised machine learning,where we build many models based on differentclassifiers, and finally combine their outputs us-ing majority voting.
Before combining, the out-put of each classifier is post-processed with a setof heuristics.
Each of these classifiers is trainedwith a moderate set of features, which are gen-erated without using any domain-specific knowl-edge and/or resources.
Our submitted systemfor the second task is based on Random Forest(Breiman, 2001).3192 TasksThe SemEval-2014 shared task on Aspect basedSentiment Analysis1focuses on identifying theaspects of a given target entities and the senti-ment expressed towards each aspect.
A bench-mark setup was provided with the datasets con-sisting of customer reviews with human-annotatedannotations of the aspects and their polarity infor-mation.
There were four subtasks, and we partic-ipated in the first two of them.
These are definedas follows:Subtask-1: The first task is related to aspectterm extraction.
Given a set of sentences withpre-identified entities, identify the aspect termspresent in the sentence and return a list containingall the distinct aspect terms.Substask-2: The second task addresses the as-pect term polarity.
For a given set of aspect termswithin a sentence, determine whether the polarityof each aspect term is positive, negative, neutral orconflict (i.e.
both positive and negative).3 Methods3.1 Pre-processingEach review is in the XML form.
At first we ex-tract the reviews along with their identifiers.
Eachreview is tokenized using the Stanford parser2andPart-of-Speech tagged using the Stanford PoS tag-ger3.
At the various levels we need the chunk-level information.
We extract these informationusing the OpenNLP chunker available at4.3.2 Aspect Term ExtractionThe approach we adopted for aspect term extrac-tion is based on the supervised machine learn-ing algorithm.
An aspect can be expressed bya noun, adjective, verb or adverb.
But the re-cent research in (Liu, 2007) shows that 60-70%of the aspect terms are explicit nouns.
The aspectterms could also consist of multiword entities suchas ?battery life?
and ?spicy tuna rolls?
etc.
Asthe classification algorithms we make use of Se-quential minimal optimization (SMO), Multiclassclassifier, Random forest and Random tree.
Forfaster computation of Support Vector Machine,SMO (Platt, 1998) was proposed.
Random tree(Breiman, 2001) is basically a decision tree, and1http://alt.qcri.org/semeval2014/task4/2http://nlp.stanford.edu/software/lexparser.shtml3http://nlp.stanford.edu/software/tagger.shtml4http://opennlp.sourceforge.net/models-1.5/in general used as a weak learner to be included insome ensemble learning method.
Multiclass clas-sifier is a meta learner based on binary SMO.
Thishas been converted to multiclass classifier usingthe pairwise method.
In order to reduce the errorscaused by the incorrect boundary identification wedefine a set of heuristics, and apply on each output.At the end these models are combined together us-ing a simple majority voting.We implement the following set of features foraspect terms extraction.?
Local context: Local contexts that span thepreceding and following few tokens of thecurrent word are used as the features.
Herewe use the previous two and next two tokensas the features.?
Part-of-Speech information: Part-of-Speech(PoS)information plays an importantrole in identifying the aspect terms.
We usethe PoS information of the current token asthe feature.?
Chunk Information: Chunk informationhelps in identifying the boundaries of aspectterms.
This is particularly more helpful torecognize multiword aspect terms.?
Root word: Roots of the surface forms areused as the features.
We use the Porter Stem-mer algorithm5to extract the root forms.?
Stop word: We use the list of stop wordsavailable at6.
A feature is defined that takesthe value equal to 1 or 0 depending uponwhether it appears in the training/test set ornot.?
Length: Length of token plays an importantrole in identifying the aspect terms.
We as-sume an entity as the candidate aspect termif its length exceeds a predefined thresholdvalue equal to five.?
Prefix and Suffix: Prefix and suffix of fixedlength character sequences are stripped fromeach token and used as the features of classi-fier.
Here we use the prefixes and suffixes oflength upto three characters as the features.5http://tartarus.org/martin/PorterStemmer/java.txt6http://ir.dcs.gla.ac.uk/resources/linguistic utils/stop words320?
Frequent aspect term: We extract the the as-pect terms from the training data, and preparea list by considering the most frequently oc-curring terms.
We consider an aspect term tobe frequent if it appears at least five times inthe training data.
A feature is then definedthat fires if and only if the current token ap-pears in this list.The output of each classifiers is post-processedwith a set of hand-crafted rules, defined as below:Rule 1: If the PoS tag of the target token is noun,chunk tag is I-NP (denoting the intermediate to-ken of a noun phrase) and the observed class of theprevious token is O (other than aspect terms) thenthe current token should be assigned the class B-Aspect (denotes the beginning of an aspect term).Rule 2: If the current token has PoS tag noun,chunk tag I-NP and the observed class of the im-mediately preceding token is B-Aspect then thecurrent token should be assigned the class I-Aspect(denoting the intermediate token).3.3 Polarity IdentificationPolarity classification of aspect terms is the classi-cal problem in sentiment analysis.
The task is toclassify the sentiments or opinions into semanticclasses such as positive, negative, and neutral.
Wedevelop a Random Forest classifier for this task.In this particular task one more class conflict is in-troduced.
It is assigned if the sentiment can eitherbe positive or negative.
For classification we makeuse of some of the features such as local context,PoS, Chunk, prefix and suffix etc., as defined in theprevious Subsection.
Some other problem-specificfeatures that we implement for sentiment classifi-cation are defined as below:?
MPQA feature: We make use of MPQAsubjectivity lexicon (Wiebe and Mihalcea,2006) that contains sentiment bearing wordsas feature in our classifier.
This list was pre-pared semi-automatically from the corpora ofMPQA7and Movie Review dataset8.
A fea-ture is defined that takes the values as fol-lows: 1 for positive; -1 for negative; 0 forneutral and 2 for those words that do not ap-pear in the list.?
Function words: A list of function words is7http://cs.pitt.edu/mpqa/8http://cs.cornell.edu/People/pabo/movie-review-data/compiled from the web9.
A binary-valuedfeature is defined that fires for those wordsthat appear in this list.4 Experiments and AnalysisWe use the datasets and the evaluation scripts asprovided by the SemEval-2014 shared task orga-nizer.4.1 DatasetsThe datasets comprise of the domains of restau-rants and laptop reviews.
The training sets con-sist of 3,044 and 3,045 reviews.
There are 3,699and 2,358 aspect terms, respectively.
The test setcontains 800 reviews for each domain.
There are1,134 and 654 test instances in the respective do-mains.4.2 Results and AnalysisAt first we develop several machine learning mod-els based on the different classification algorithms.All these classifiers were trained using the sameset of features as mentioned in Section 3.
Weuse the default implementations of these classi-fiers in Weka10.
We post-process the outputs ofall the models using some heuristics.
Finally, allthese classifiers are combined together using ma-jority voting.
It is to be noted that we determinethe best configuration by carrying out different ex-periments on the development set, which is con-structed by taking a part of the training set, and fi-nally blind evaluation is performed on the respec-tive test set.
We use the evaluation script providedwith the SemEval-2014 shared task.
The trainingsets contain multiword aspect terms, and so we usethe standard BIO notation11for proper boundarymarking.Experiments show the precision, recall and F-score values 77.97%, 72.13% and 74.94%, respec-tively for the restaurant dataset.
This is approxi-mately 10 points below compared to the best sys-tem.
But it shows the increments of 4.16 and27.79 points over the average and baseline mod-els, respectively.
For the laptop dataset we ob-tain the precision, recall and F-score values of70.74%, 62.84% and 66.55%, respectively.
Thisis 8 points below the best one and 10.35 points9http://www2.fs.u-bunkyo.ac.jp/ gilner/wordlists.html10www.cs.waikato.ac.nz/ml/weka/11B, I and O denote the beginning, intermediate and out-side tokens321Model precision recall F-scoreRandom Tree 65.21 59.63 62.29Random Forest 70.93 62.69 66.55SMO 71.18 64.22 67.52Multiclass 73.44 68.50 70.88Ensemble 77.97 72.13 74.94Best system 85.35 82.71 84.01Average 76.74 67.26 70.78Baseline - - 47.15Table 1: Result of Task-A for restaurants datasetwith different classifiers (in %).Model precision recall F-scoreRandom Tree 56.52 56.17 56.34Random Forest 58.38 58.02 58.19SMO 63.62 63.22 63.39Multiclass 65.30 64.90 65.09Ensemble 70.74 62.84 66.55Best system 84.80 66.51 74.55Average 68.97 50.45 56.20Baseline - - 35.64Table 2: Results of aspect term extraction for lap-tops dataset with different classifiers (in %).above the average system.
Compared to the base-line it achieves more than 20 point increment.
De-tailed evaluation results for all the classifiers arereported in Table 1 and Table 2 for restaurant andlaptop datasets, respectively.
Results show thatmulticlass classifier achieves the highest perfor-mance with precision, recall and F-score valuesof 73.44%, 68.50% and 70.88%, respectively forthe restaurant dataset.
The same model shows thehighest performance with precision, recall and F-score values of 65.30%, 64.90% and 65.09%, re-spectively for the laptop dataset.
Because of ma-jority ensemble we observe increments of 4.06%and 1.46% F-score points over the best individualmodel, respectively.We also perform error analysis to understandthe possible sources of errors.
We show only theconfusion matrix for Task-A in Table 3.
It showsthat in most cases I-ASP is misclassified as B-ASP.System also suffers because of the misclassifica-tion of aspect terms to others.Experiments for classification are reported inTable 4.
Evaluation shows that the systemachieves the accuracies of 67.37% and 67.07% forB-ASP I-ASP OtherB-ASP 853 15 269I-ASP 114 213 142Other 123 35 11431Table 3: Confusion matrix for Task-A on restau-rants dataset.Datasets#AspectTerms#CorrectIdentificationAccuracy(in %)Restaurants 1134 764 67.37Laptops 654 438 67.07Table 4: Results of aspect terms polarity (in %).the restaurants and laptops datasets, respectively.Please note that our system for the second taskwas not officially evaluated because of the techni-cal problems of the submitted zipped folder.
How-ever we evaluated the same system with the of-ficial evaluation script, and it shows the accura-cies as reported in Table 4.
We observe that theclassifier performs reasonably well for the posi-tive and negative classes, and suffers most for theconflict classes.
This may be due to the numberof instances present in the respective training set.Results show that our system achieves much lowerclassification accuracy (13.58 points below) com-pared to the best system for the restaurant datasets.However, for the laptop datasets the classificationaccuracy is quite encouraging (just 3.42 points be-low the best system).
It is also to be noted that ourclassifier achieves quite comparable performancefor both the datasets.
Therefore it is more generaland not biased to any particular domain.5 ConclusionIn this paper we report our works on aspect termextraction and sentiment classification as part ofour participation in the SemEval-2014 shared task.For aspect term extraction we develop an ensem-ble system.
Our aspect term classification model isbased on Random Forest classifier.
Runs for bothof our systems were constrained in nature, i.e.
wedid not make use of any external resources.
Evalu-ation on the shared task dataset shows encouragingresults that need further investigation.Our analysis suggests that there are many waysto improve the performance of the system.
In fu-ture we will identify more features to improve theperformance of each of the tasks.322ReferencesLeo Breiman.
2001.
Random forests.
45(1):5?32.S.
Brody and N. Elhadad.
2010.
An unsupervisedaspect-sentiment model for online reviews.
In Pro-ceedings of NAACL, pages 804?812, Los Angeles,CA.Angela Fahrni and Manfred Klenner.
2008.
Old wineor warm beer: Target-specic sentiment analysis ofadjectives.
In Symsposium on Affective Language inHuman and Machine, pages 60?63.
The Society forthe Study of Artificial Intelligence and Simulation ofBehavior (AISB).M.
Hu and B. Liu.
2004.
Mining and summarizingcustomer reviews.
In Proceedings of the 10th KDD,pages 168?177, Seattle, WAs.B.
Liu.
2007.
Exploring Hyperlinks, Contents, andUsage Data.
Springer.Bing Liu.
2012.
Sentiment Analysis and Opinion Min-ing.
Synthesis Lectures on Human Language Tech-nologies.
Morgan & Claypool Publishers.John C. Platt.
1998.
Sequential minimal optimiza-tion: A fast algorithm for training support vectorma-chines.
Technical report, ADVANCES IN KERNELMETHODS - SUPPORT VECTOR LEARNING.Ana-Maria Popescu and Oren Etzionir.
2005.
Ex-tracting product features and opinions from reviews.In Proceedings of the Conference on HLT/EMNLP,pages 339?346.P.
D. Turney.
2002.
Thumbs up or thumbs down?
:Semantic orientation applied to unsupervised classi-fication of reviews.
In Proceedings of the 40th ACL,pages 417?424.Janyce Wiebe and Rada Mihalcea.
2006.
Wordsense and subjectivity.
In Proceedings of the COL-ING/ACL, pages 065?1072, Australia.323
