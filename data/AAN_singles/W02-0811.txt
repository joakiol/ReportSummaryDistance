Combining Heterogeneous Classifiers for Word-Sense DisambiguationDan Klein, Kristina Toutanova, H. Tolga Ilhan,Sepandar D. Kamvar and Christopher D. ManningComputer Science DepartmentStanford UniversityStanford, CA 94305-9040, USAAbstractThis paper discusses ensembles of simple but het-erogeneous classifiers for word-sense disambigua-tion, examining the Stanford-CS224N system en-tered in the SENSEVAL-2 English lexical sampletask.
First-order classifiers are combined by asecond-order classifier, which variously uses ma-jority voting, weighted voting, or a maximum en-tropy model.
While individual first-order classifiersperform comparably to middle-scoring teams?
sys-tems, the combination achieves high performance.We discuss trade-offs and empirical performance.Finally, we present an analysis of the combination,examining how ensemble performance depends onerror independence and task difficulty.1 IntroductionThe problem of supervised word sense disambigua-tion (WSD) has been approached using many differ-ent classification algorithms, including naive-Bayes,decision trees, decision lists, and memory-basedlearners.
While it is unquestionable that certain al-gorithms are better suited to the WSD problem thanothers (for a comparison, see Mooney (1996)), itseems that, given similar input features, various al-gorithms exhibit roughly similar accuracies.1 Thiswas supported by the SENSEVAL-2 results, where aThis paper is based on work supported in part by the Na-tional Science Foundation under Grants IIS-0085896 and IIS-9982226, by an NSF Graduate Fellowship, and by the ResearchCollaboration between NTT Communication Science Labora-tories, Nippon Telegraph and Telephone Corporation and CSLI,Stanford University.1In fact, we have observed that differences between imple-mentations of a single classifier type, such as smoothing or win-dow size, impacted accuracy far more than the choice of classi-fication algorithm.large fraction of systems had scores clustered in afairly narrow region (Senseval-2, 2001).We began building our system with 23 supervisedWSD systems, each submitted by a student takingthe natural language processing course (CS224N) atStanford University in Spring 2000.
Students werefree to implement whatever WSD method they chose.While most implemented variants of naive-Bayes,others implemented a range of other methods, in-cluding n-gram models, vector space models, andmemory-based learners.
Taken individually, the bestof these systems would have turned in an accuracyof 61.2% in the SENSEVAL-2 English lexical sam-ple task (which would have given it 6th place), whileothers would have produced middling to low perfor-mance.
In this paper, we investigate how these clas-sifiers behave in combination.In section 2, we discuss the first-order classifiersand describe our methods of combination.
In sec-tion 3, we discuss performance, analyzing what ben-efit was found from combination, and when.
We alsodiscuss aspects of the component systems whichsubstantially influenced overall performance.2 The System2.1 Training ProcedureFigure 1 shows the high-level organization of oursystem.
Individual first-order classifiers each maplists of context word tokens to word-sense predic-tions, and are self-contained WSD systems.
The first-order classifiers are combined in a variety of wayswith second-order classifiers.
Second-order classi-fiers are selectors, taking a list of first-order out-July 2002, pp.
74-80.
Association for Computational Linguistics.Disambiguation: Recent Successes and Future Directions, Philadelphia,Proceedings of the SIGLEX/SENSEVAL Workshop on Word Senserankingorder2nd.EntropyValidationVoting1 2 3 4 5 6 7 8MaximumWeightedVotingMajorityclassifiersorder1st.2nd.
classifiersorderChosenClassifierFinal classifierCrossrankingorder1st.Figure 1: High-level system organization.1 Split data into multiple training and held-out parts.2 Rank first-order classifiers globally (across all words).3 Rank first-order classifiers locally (per word),breaking ties with global ranks.4 For each word w5 For each size k6 Choose the ensemble Ew,k to be the top k classifiers7 For each voting method m8 Train the (k, m) second-order classifier with Ew,k9 Rank the second-order classifier types (k, m) globally.10 Rank the second-order classifier instances locally.11 Choose the top-ranked second-order classifier for each word.12 Retrain chosen per-word classifiers on entire training data.13 Run these classifiers on test data, and evaluate results.Table 1: The classifier construction process.puts and choosing from among them.
An outlineof the classifier construction process is given in ta-ble 1.
First, the training data was split into trainingand held-out sets for each word.
This was done us-ing 5 random bootstrap splits.
Each split allocated75% of the examples to training and 25% to held-out testing.2 Held-out data was used both to selectthe subsets of first-order classifiers to be combined,and to select the combination methods.For each word and each training split, the 23 first-order classifiers were (independently) trained andtested on held-out data.
For each word, the first-order classifiers were ranked by their average per-formance on the held-out data, with the most accu-rate classifiers at the top of the rankings.
Ties werebroken by the classifiers?
(weighted) average perfo-mance across all words.For each word, we then constructed a set of can-2Bootstrap splits were used rather than standard n-foldcross-validation for two reasons.
First, it allowed us to gen-erate an arbitrary number of training/held-out pairs while stillleaving substantial held-out data set sizes.
Second, this ap-proach is commonly used in the literature on ensembles.
Itswell-foundedness and theoretical properties are discussed inBreiman (1996).
In retrospect, since we did not take proper ad-vantage of the ability to generate numerous splits, it might havebeen just as well to use cross-validation.didate second-order classifiers.
Second-order clas-sifier types were identified by an ensemble size kand a combination method m. One instance of eachsecond-order type was constructed for each word.We originally considered ensemble sizes k in therange {1, 3, 5, 7, 9, 11, 13, 15}.
For a second-orderclassifier with ensemble size k, the ensemble mem-bers were the top k first-order classifiers accordingto the local rank described above.We combined first-order ensembles using one ofthree methods m:?
Majority voting: The sense output by the mostfirst-order classifiers in the ensemble was chosen.Ties were broken by sense frequency, in favor ofmore frequent senses.?
Weighted voting: Each first-order classifier wasassigned a voting weight (see below).
The sensereceiving the greatest total weighted vote waschosen.?
Maximum entropy: A maximum entropy classifierwas trained (see below) and run on the outputs ofthe first-order classifiers.We considered all pairs of k and m, and so foreach word there were 24 possible second-order clas-sifiers, though for k = 1 all three values of m areequivalent and were merged.
The k = 1 ensemble,as well as the larger ensembles (k ?
{9, 11, 13, 15}),did not help performance once we had good first-order classifier rankings (see section 3.4).For m = Majority, there are no parameters to set.For the other two methods, we set the parameters ofthe (k, m) second-order classifier for a word w usingthe bootstrap splits of the training data for w.In the same manner as for the first-order classi-fiers, we then ranked the second-order classifiers.For each word, there was the local ranking of thesecond-order classifiers, given by their (average) ac-curacy on held-out data.
Ties in these rankings werebroken by the average performance of the classifiertype across all words.
The top second-order classi-fier for each word was selected from these tie-brokenrankings.At this point, all first-order ensemble membersand chosen second-order combination methods wereretrained on the unsplit training data and run on thefinal test data.It is important to stress that each target word wasconsidered an entirely separate task, and differentfirst- and second-order choices could be, and were,made for each word (see the discussion of table 2below).
Aggregate performance across words wasonly used for tie-breaking.2.2 Combination MethodsOur second-order classifiers take training instancesof the form s?
= (s, s1, .
.
.
, sk) where s is the correctsense and each si is the sense chosen by classifier i .All three of the combination schemes which we usedcan be seen as weighted voting, with different waysof estimating the voting weights ?i of the first-ordervoters.
In the simplest case, majority voting, we skipany attempt at statistical estimation and simply seteach ?i to be 1/k.For the method we actually call ?weighted vot-ing,?
we view the combination output as a mixturemodel in which each first-order system is a mixturecomponent:P(s|s1, .
.
.
, sk) =?i?i P(s|si )The conditional probabilties P(s|si ) assign massone to the sense si chosen by classifier i .
The mix-ture weights ?i were estimated using EM to max-imize the likelihood of the second-order traininginstances.
In testing, the sense with the highestweighted vote, and hence highest posterior likeli-hood, is the selected sense.For the maximum entropy classifier, we have adifferent model for the chosen sense s. In this case,it is an exponential model of the form:P(s|s1, .
.
.
, sk) =exp?x ?x fx(s, s1, .
.
.
, sk)?t exp?x ?x fx(t, s1, .
.
.
, sk)The features fx are functions which are true oversome subset of vectors s?.
The original intent was todesign features to recognize and exploit ?sense ex-pertise?
in the individual classifiers.
For example,one classifier might be trustworthy when reportinga certain sense but less so for other senses.
How-ever, there was not enough data to accurately esti-mate parameters for such models.3 In fact, we no-3The number of features was not large: only one for each(classifier, chosen sense, correct sense) triple.
However, mostsenses are rarely chosen and rarely correct, so most featureshad zero or singleton support.ticed that, for certain words, simple majority votingperformed better than the maximum entropy model.It also turned out that the most complex features wecould get value from were features of the form:fi(s, s1, .
.
.
, sk) = 1 ??
s = siThat is, for each first-order classifier, there is a sin-gle feature which is true exactly when that classi-fier is correct.
With only these features, the maxi-mum entropy approach also reduces to a weightedvote; the s which maximizes the posterior probabil-ity P(s|s1, .
.
.
, sk) also maximizes the vote:v(s) =?i ?i?
(si = s)The indicators ?
are true for exactly one sense, andcorrespond to the simple f i defined above.4 Thesense with the largest vote v(s) will be the sensewith the highest posterior probability P(s|s1, .
.
.
sk)and will be chosen.For the maximum entropy classifier, we estimatethe weights by maximizing the likelihood of a held-out set, using the standard IIS algorithm (Berger etal., 1996).
For both weighted schemes, we foundthat stopping the iterative procedures before conver-gence gave better results.
IIS was halted after 50rounds, while EM was halted after a single round.Both methods were initialized to uniform startingweights.More importantly than changing the exact weightestimates, moving from method to method triggersbroad qualitative changes in what kind of weightsare allowed.
With majority voting, classifiers allhave equal, positive weights.
With weighted vot-ing, the weights are no longer required to be equal,but are still non-negative.
With maximum entropyweighting, this non-negativity constraint is also re-laxed, allowing classifiers?
votes to actually reducethe score for the sense that classifier has chosen.Negative weights are in fact assigned quite fre-quently, and often seem to have the effect of usingpoor classifiers as ?error masks?
to cancel out com-mon errors.As we move from majority voting to weightedvoting to maximum entropy, the estimation becomes4If the i th classifier returns the correct sense s, then?
(si = s) is 1, otherwise it is zero.more sophisticated, but also more prone to overfit-ting.
Since solving the overfitting problem is hard,while choosing between classifiers based on held-out data is relatively easy, this spectrum gives us away to gracefully handle the range of sparsities inthe training corpora for different words.2.3 Individual ClassifiersWhile our first-order classifiers implemented a va-riety of classification algorithms, the differences intheir individual accuracies did not primarily stemfrom the algorithm chosen.
Rather, implementationdetails led to the largest differences.
For example,naive-Bayes classifiers which chose sensible win-dow sizes, or dynamically chose between windowsizes, tended to outperform those which chose poorsizes.
Generally, the optimal windows were eitherof size one (for words with strong local syntactic orcollocational cues) or of very large size (which de-tected more topical cues).
Programs with hard-wiredwindow sizes of, say, 5, performed poorly.
Iron-ically, such middle-size windows were commonlychosen by students, but rarely useful; either extremewas a better design.5Another implementation choice dramatically af-fecting performance of naive-Bayes systems was theamount and type of smoothing.
Heavy smoothingand smoothing which backed off conditional dis-tributions P(w j |si) to the relevant marginal P(w j)gave good results, while insufficient smoothing orbacking off to uniform marginals gave substantiallydegraded results.6There is one significant way in which our first-order classifiers were likely different from otherteams?
systems.
In the original class project, stu-dents were guaranteed that the ambiguous wordwould appear only in a single orthographic form,and many of the systems depended on the input sat-isfying this guarantee.
Since this was not true ofthe SENSEVAL-2 data, we mapped the ambiguous5Such window sizes were also apparently chosen by otherSENSEVAL-2 systems, which commonly used ?long distance?and ?local?
features, but defined local as a window size of 3?5words on each side of the ambiguous word.6In particular, there is a defective behavior with naive-Bayeswhere, when one smooths far too little, the chosen sense is theone which has occurred with the most words in the contextwindow.
For small training sets of skewed-prior data like theSENSEVAL-2 sets, this is invariably the common sense, regard-less of the context words.words (but not context words) to a citation form.We suspect that this lost quite a bit of informationand negatively affected the system?s overall perfor-mance, since there is considerable correlation be-tween form and sense, especially for verbs.
Nev-ertheless, we have made no attempt to re-engineerthe student systems, and have not thoroughly inves-tigated how big a difference this stemming made.3 Results and Discussion3.1 ResultsTable 2 shows the results per word, and table 3shows results by part-of-speech and overall, for theSENSEVAL-2 English lexical sample task.
It alsoshows what second-order classifiers were selectedfor each word.
54.2% of the time, we made an opti-mal second-order classifier choice.
When we chosewrong, we usually made a mistake in either ensem-ble size or method, rarely both.
A wide range ofsecond-order classifier types were chosen.
As anoverview of the benefit of combination, the globallybest single classifier scored 61.2%, the locally bestsingle classifier (best on test data) scored 62.2%, theglobally best second order classifier (ME-7, best ontest data) scored 63.2%, and our dynamic selectionmethod scored 63.9%.
Section 3.3 examines combi-nation effectiveness more closely.3.2 Changes from SENSEVAL-2The system we originally submitted to theSENSEVAL-2 competition had an overall accu-racy of 61.7%, putting it in 4th place in the revisedrankings (among 21 supervised and 28 total sys-tems).
Assuming that our first-order classifierswere fixed black-boxes, we wanted an idea of howgood our combination and selection methods were.To isolate the effectiveness of our second-orderclassifier choices, we compared our system to anoracle method (OR-BEST) which chose a word?ssecond-order classifier based on test data (ratherthan held-out data).
The overall accuracy of thisoracle method was 65.4% at the time, a jump of3.7%.7 This gap was larger than the gap betweenthe various top-scoring teams?
systems.
Therefore,while the test-set performance of the second-orderclassifiers is obviously not available, it was clear7With other changes, OR-BEST rose to 66.1%.LB Baselines Combination OR UB SystemWord ALL MFS SNG MJ-7 WT-7 ME-7 BEST SOME ACC CLart-n 28.6 41.8 50.6 52.0 54.1 52.0 58.2 69.4 58.2 WT-5authority-n 45.7 33.7 61.3 69.6 69.6 65.2 69.6 78.3 66.3 WT-3bar-n 31.1 39.7 63.7 61.6 69.5 72.2 72.2 81.5 72.2 ME-7begin-v 50.0 58.6 70.0 83.6 84.3 88.2 88.2 94.6 83.6 MJ-7blind-a 65.5 83.6 77.8 83.6 83.6 85.5 85.5 90.9 83.6 WT-7bum-n 71.1 75.6 71.3 75.6 75.6 77.8 77.8 82.2 77.8 ME-7call-v 1.5 25.8 33.3 25.8 30.3 27.3 34.8 62.1 30.3 WT-7carry-v 9.1 22.7 27.8 34.8 33.3 33.3 37.9 62.1 33.3 MJ-5chair-n 76.8 79.7 84.2 82.6 82.6 82.6 82.6 84.1 81.2 ME-3channel-n 46.6 27.4 61.1 60.3 60.3 65.8 67.1 78.1 67.1 ME-3child-n 34.4 54.7 57.9 67.2 70.3 70.3 75.0 90.6 71.9 WT-5church-n 56.2 53.1 63.1 73.4 73.4 75.0 75.0 85.9 73.4 WT-7circuit-n 52.9 27.1 70.9 65.9 65.9 78.8 78.8 80.0 78.8 ME-5collaborate-v 90.0 90.0 92.9 90.0 90.0 90.0 90.0 90.0 90.0 WT-5colorless-a 48.6 65.7 80.0 68.6 68.6 68.6 68.6 82.9 68.6 ME-5cool-a 15.4 46.2 65.0 57.7 55.8 59.6 59.6 80.8 59.6 ME-5day-n 36.6 59.3 58.4 69.0 68.3 66.2 69.0 82.8 63.4 WT-3develop-v 11.6 29.0 35.2 42.0 43.5 42.0 43.5 68.1 42.0 MJ-3draw-v 4.9 9.8 23.4 29.3 26.8 24.4 29.3 41.5 26.8 WT-5dress-v 25.4 42.4 49.9 52.5 52.5 55.9 59.3 72.9 55.9 ME-7drift-v 3.1 25.0 31.7 37.5 37.5 34.4 37.5 65.6 37.5 WT-5drive-v 16.7 28.6 40.0 45.2 45.2 40.5 45.2 61.9 42.9 MJ-3dyke-n 85.7 89.3 86.5 89.3 89.3 89.3 92.9 96.4 92.9 WT-3face-v 82.8 83.9 80.9 83.9 83.9 82.8 83.9 84.9 83.9 WT-5facility-n 36.2 48.3 70.5 67.2 70.7 65.5 74.1 86.2 70.7 WT-7faithful-a 56.5 78.3 65.0 78.3 78.3 78.3 82.6 100.0 78.3 MJ-3fatigue-n 67.4 76.7 83.9 88.4 90.7 90.7 90.7 93.0 90.7 MJ-5feeling-n 29.4 56.9 76.7 62.7 70.6 72.5 74.5 86.3 72.5 WT-3find-v 7.4 14.7 37.6 30.9 27.9 30.9 32.4 48.5 32.4 WT-3fine-a 32.9 38.6 46.9 51.4 57.1 54.3 57.1 67.1 52.9 MJ-3fit-a 51.7 51.7 87.7 89.7 89.7 86.2 93.1 96.6 93.1 MJ-5free-a 26.8 39.0 58.2 65.9 65.9 61.0 65.9 74.4 64.6 ME-3graceful-a 62.1 75.9 81.4 79.3 79.3 79.3 79.3 82.8 79.3 WT-5green-a 69.1 78.7 80.0 83.0 83.0 83.0 85.1 88.3 83.0 MJ-3grip-n 25.5 54.9 49.2 60.8 60.8 58.8 74.5 84.3 60.8 MJ-7hearth-n 46.9 75.0 56.3 75.0 71.9 65.6 75.0 84.4 62.5 WT-3holiday-n 77.4 83.9 89.7 83.9 83.9 80.6 83.9 87.1 83.9 WT-5keep-v 19.4 37.3 36.1 38.8 49.3 52.2 52.2 65.7 52.2 WT-5lady-n 60.4 69.8 67.7 75.5 75.5 77.4 77.4 81.1 75.5 WT-3leave-v 21.2 31.8 29.1 43.9 53.0 50.0 54.5 68.2 54.5 WT-5live-v 20.9 50.7 54.6 53.7 59.7 65.7 71.6 77.6 71.6 MJ-3local-a 15.8 57.9 76.8 71.1 68.4 68.4 71.1 92.1 71.1 MJ-7match-v 11.9 35.7 30.4 52.4 52.4 57.1 57.1 78.6 47.6 WT-3material-n 39.1 42.0 56.0 55.1 55.1 50.7 66.7 73.9 66.7 WT-3mouth-n 15.0 45.0 40.5 53.3 53.3 45.0 56.7 78.3 53.3 MJ-5nation-n 70.3 70.3 71.1 70.3 70.3 70.3 70.3 70.3 70.3 WT-5natural-a 18.4 27.2 50.4 49.5 50.5 58.3 58.3 76.7 55.3 WT-3nature-n 23.9 45.7 51.3 63.0 67.4 65.2 67.4 82.6 60.9 MJ-5oblique-a 51.7 69.0 73.7 82.8 82.8 82.8 86.2 89.7 79.3 WT-5play-v 12.1 19.7 35.6 40.9 51.5 50.0 51.5 62.1 51.5 WT-5post-n 26.6 31.6 66.5 49.4 57.0 65.8 67.1 73.4 67.1 ME-3pull-v 1.7 21.7 27.7 21.7 21.7 28.3 28.3 46.7 23.3 WT-3replace-v 28.9 53.3 49.0 57.8 53.3 60.0 60.0 77.8 57.8 MJ-7restraint-n 35.6 31.1 53.9 71.1 68.9 71.1 71.1 82.2 66.7 ME-5see-v 29.0 31.9 40.0 42.0 42.0 42.0 42.0 55.1 42.0 MJ-5sense-n 18.9 22.6 46.3 64.2 60.4 50.9 64.2 79.2 64.2 MJ-7serve-v 35.3 29.4 54.4 60.8 64.7 66.7 66.7 74.5 62.7 WT-5simple-a 51.5 51.5 43.0 51.5 51.5 51.5 51.5 54.5 51.5 ME-3solemn-a 96.0 96.0 89.2 96.0 96.0 96.0 96.0 96.0 96.0 WT-3spade-n 66.7 63.6 81.8 75.8 75.8 78.8 78.8 81.8 78.8 WT-3stress-n 7.7 46.2 47.0 43.6 43.6 35.9 51.3 82.1 48.7 WT-5strike-v 5.6 16.7 32.3 31.5 29.6 29.6 40.7 55.6 31.5 MJ-5train-v 22.2 30.2 48.3 57.1 57.1 54.0 57.1 76.2 57.1 WT-7treat-v 36.4 38.6 51.8 54.5 54.5 52.3 54.5 70.5 52.3 WT-3turn-v 1.5 14.9 38.8 32.8 29.9 32.8 35.8 52.2 31.3 MJ-5use-v 61.8 65.8 69.6 65.8 65.8 72.4 72.4 75.0 72.4 ME-3vital-a 84.2 92.1 91.5 92.1 92.1 92.1 92.1 92.1 92.1 WT-5wander-v 70.0 80.0 83.2 80.0 82.0 82.0 82.0 84.0 80.0 ME-3wash-v 16.7 25.0 40.0 58.3 58.3 25.0 58.3 83.3 58.3 MJ-7work-v 10.0 26.7 28.1 43.3 43.3 41.7 45.0 63.3 45.0 WT-3yew-n 75.0 78.6 81.4 78.6 78.6 78.6 78.6 82.1 78.6 WT-5Table 2: Results by word for the SENSEVAL-2 English lexi-cal sample task.
Lower bound (LB): ALL is how often all ofthe first-orders chose correctly.
Baselines (BL): MFS is themost-frequent-sense baseline, SNG is the best single first-orderclassifier as chosen on held-out data for that word.
Fixed com-binations: majority vote (MJ), weighted vote (WT), maximumentropy (ME).
Oracle bound (OR): BEST is the best second-order classifier as measured on the test data.
Upper bound (UB):SOME is how often at least one first-order classifier producedthe correct answer.
Methods which are ensemble-size depen-dent are shown for k = 7.
System choices: ACC is the accuracyof the selection the system makes based on held-out data.
CL isthe 2nd-order classifier selected.that a more sophisticated or better-tuned methodof selecting combination models could lead tosignificant improvement.
In fact, changing onlyranking methods, which are discussed further in thenext section, resulted in an increase in final accu-racy for our system to the current score of 63.9%,which would have placed it 1st in the SENSEVAL-2preliminary results or 2nd in the revised results.
OurLB Baselines Combination OR UB SystemALL MFS SNG MJ-7 WT-7 ME-7 BEST SOME ACCnoun 42.5 50.5 63.8 66.4 67.9 67.8 71.9 81.2 69.7adj.
45.1 57.8 66.7 69.0 69.4 69.9 71.6 81.0 69.9verb 28.8 40.2 48.7 53.4 54.7 55.8 58.2 71.2 55.7avg.
46.5 47.5 62.2 61.5 62.7 63.2 68.9 72.0 63.9Table 3: Results by part-of-speech, and overall.58596061626364651 3 5 7 9 11 13 15 	 	   	   fffffiflffiChosenCombinationMaximumEntropy!
#" $&% '  (Vote)+* , -&. "
' /Vote021 -&34*#1 5	 6 'SingleFigure 2: Accuracy of the various combination methods asthe ensemble size varies.
The three combination methods areshown.
In addition, the globally best single classifier is the sin-gle first-order classifier with the highest overall accuracy on thetest data.
Chosen combination is our final system?s score.
Thesetwo are both independent of k in this graph.final accuracy is thus higher than the first draft ofthe system, and, in particular, the classifier selectiongap between actual performance and the OR-BESToracle has been substantially decreased.In addition, since the top first-order classifierswere more reliably identified, larger ensembles wereno longer beneficial in the revised system, for an in-teresting reason.
When the first-order rankings werepoorly estimated, large ensembles and weightedmethods were important for achieving good accu-racy, because the weighting scheme could ?rescue?good classifiers which had been incorrectly rankedlow.
In our current system, however, first-order clas-sifiers were ranked reliably enough that we could re-strict our ensemble sizes to k ?
{1, 3, 5, 7}.
Further-more, since k = 1 was only chosen a few times,usually among ties, we removed that option as well.3.3 Combination Methods and Ensemble SizeOur system differs from the typical ensemble ofclassifiers in that the first-order classifiers are notmerely perturbations of each other, but are highlyvaried in both quality and character.
This scenariohas been investigated before, e.g.
(Zhang et al,1992), but is not the common case.
With such het-erogeneity, having more classifiers is not always bet-ter.
Figure 2 shows how the three combination meth-ods?
average scores varied with the number of com-ponent classifiers used.
Initially, accuracy increasesas added classifiers bring value to the ensemble.However, as lower-quality classifiers are added in,the better classifiers are steadily drowned out.
Theweighted vote and maximum entropy combinationsare much less affected by low-quality classifiers thanthe majority vote, being able to suppress them withlow weights.
Still, majority vote over small ensem-bles was effective for some words where weightscould not be usefully set by the other methods.3.4 Ranking MethodsBecause of the effects described above, it was nec-essary to identify which classifiers were worth in-cluding for a given word.
A global ranking of first-order classifiers, averaged over all words, was noteffective because the strengths of the classifiers wereso different.
In fact, every single first-order clas-sifier was a top-5 performer on at least one word.On the other hand, SENSEVAL-2 training sets wereoften very small, and very skewed towards a fre-quent most-frequent-sense.
As a result, accuracy es-timates based on single words?
held-out data pro-duced frequent ties.
The average size of the per-word largest set of tied first-order classifiers was3.6 (with a maximum of 23 on the word collabo-rate where all tied).
The second-order local rank-ings also produced many ties.
For the top position(the most important for second-order ranks) 43.1%of the words had local ties.In our submitted entry, all ties were broken unin-telligently (in an arbitrary manner based on the orderin which systems were listed in a file).
The approachof local ranking with global tie-breaking presentedin this paper was much more successful accord-ing to two distinct measures.
First, it predicted thetrue ranks more accurately, (measured by the Spear-man rank correlation: 0.08 for global ranks, 0.63for globally-broken local ranks) and gave better fi-nal accuracy scores (63.5% with global, 63.9% withglobally-broken local ?
significant only at p=0.1 bya sign test at the word type level).The other ranking that our system attempts to es-timate is the per-word ranking of the second-orderclassifiers.
In this case, however, we are onlyever concerned with which classifier ends up be-ing ranked first, as only that classifier is chosen.Again, globally-broken local ranks were the mosteffective, choosing a second-order classifier whichwas actually top-performing on test data for 54% ofthe words, as opposed to 50% for global selection(and increasing the overall accuracy from 62.8% to63.9% ?
significant at p=0.01, sign test).These results stress that ranking, and effective tie-breaking, are important for a system such as ourswhere the classifiers are so divergent in behavior.3.5 CombinationWhen combining classifiers, one would like to knowwhen and how the combination will outperform theindividuals.
One factor (Tumer and Ghosh, 1996)is how complementary the mistakes of the individ-ual classifiers are.
If all make the same mistakes,combination can do no good.
We can measure thiscomplementarity by averaging, over all pairs of first-order classifiers, the fraction of errors that pair has incommon.
This gives average pairwise error indepen-dence.
Another factor is the difficulty of the wordbeing disambiguated.
A high most-frequent-sensebaseline (BL-MFS) means that there is little roomfor improvement by combining classifiers.
Figure 3shows, for the global top 7 first-order classifiers, theabsolute gain between their average accuracy (BL-AVG-7) and the accuracy of their majority combina-tion (MJ-7).
The quantity on the x-axis is the dif-ference between the pairwise independence and thebaseline accuracy.
The pattern is loose, but clear.When either independence increases or the word?sdifficulty (as indicated by the BL-MFS baseline) in-creases, the combination tends to win by a greateramount.Figure 4 shows how the average pairwise inde-pendent error fraction (api) varies as we add classi-fiers.
Here classifiers are added in an order based ontheir accuracy on the entire test set.
For each k, theaverage is over all pairs of classifiers in the top k andall samples of all words.
This graph should be com-pared to figure 2.
After the third classifier, addingclassifiers reduces the api, and the performance ofthe majority vote begins to drop at exactly this point.However, the weighted methods continue to gain inaccuracy since they have the capacity to downweightclassifiers which hurt held-out accuracy.The drop in api reflects that the newly added sys-tems are no longer bringing many new correct an-swers to the collection.
However, they can still add-50510152025-100 -80 -60 -40 -20 0 20 40    		 	    fffi flffi!
"#fl!$ %%&'"%()" !
* +,'%,!#-Figure 3: Gain in accuracy of majority vote over the averagecomponent performance as (pairwise independence ?
baselineaccuracy) grows.0.350.370.390.410.430.452 4 6 8 10 12 14 16 18 20 22 24./10321457618:9; <==> 8 > 45 =?
@@A@B CD EFECDECGEHFE@GECIJFigure 4: The average pairwise error independence of classifiersas their number is increased.deciding votes in areas where the ensemble had theright answer, but did not choose it.
The final gradualrise in api reflects the somewhat patternless new er-rors that substantially lower-performing systems un-fortunately bring to the ensemble.4 ConclusionsIn this paper, we have explored ensemble sizes, com-bination methods, bounds for what can be expectedfrom combinations, factors in the performance of in-dividual classifiers, and methods of improving per-formance by effective tie-breaking.
In accord withmuch recent work on classifier combination, e.g.
(Breiman, 1996; Bauer and Kohavi, 1999), we havedemonstrated that the combination of classifiers canlead to a substantial performance increase over theindividual classifiers within the domain of WSD.
Inaddition, we have shown that highly varying com-ponent systems augment each other well and thatadding lower-scoring systems can still improve en-semble performance, at least to a certain point.
Aparticular emphasis of our research has been how tomake the combination robust to both the wide rangeof first-order classifier accuracies and to the sparsityof the available training data.
Careful but greedy de-termination of rankings proved to be effective, cap-turing the highly word-dependent strengths of ourclassifiers.
The resulting system?s overall accuracyis very high, despite the medium level of accuracyof the component systems.5 AcknowledgmentsWe would like to thank the following people forcontributing their classifiers to the Stanford CS224Nsystem: Zoe Abrams, Jenny Berglund, Dmitri Bo-brovnikoff, Chris Callison-Burch, Marcos Chavira,Shipra Dingare, Elizabeth Douglas, Sarah Har-ris, Ido Milstein, Jyotirmoy Paul, Soumya Ray-chaudhuri, Paul Ruhlen, Magnus Sandberg, AdilSherwani, Philip Shilane, Joshua Solomin, PatrickSutphin, Yuliya Tarnikova, Ben Taskar, KristinaToutanova, Christopher Unkel, and Vincent Van-houcke.ReferencesAlan Agresti.
1990.
Categorical Data Analysis.
John Wiley &Sons.Eric Bauer and Ron Kohavi.
1999.
An empirical comparison ofvoting classication algorithms: Bagging, boosting and vari-ants.
Machine Learning, 36:105?139.Adam L. Berger, Stephen A. Della Pietra, and Vincent J. DellaPietra.
1996.
A maximum entropy approach to natural lan-guage processing.
Computational Linguistics, 22(1):39?71.Leo Breiman.
1996.
Bagging predictors.
Machine Learning,24:123?140.Raymond J. Mooney.
1996.
Comparative experiments on dis-ambiguating word senses: An illustration of the role of biasin machine learning.
In EMNLP 1.Senseval-2.
2001.
Senseval-2 proceedings, in publication.Kagan Tumer and Joydeep Ghosh.
1996.
Error correlation anderror reduction in ensemble classifiers.
Connection Science,8:385?404.Xiru Zhang, Jill Mesirov, and David L. Waltz.
1992.
Hybridsystem for protein structure prediction.
Journal of MolecularBiology, 225:1049?1063.
