Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 61?65,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsApplying mpaligner to Machine Transliteration with Japanese-SpecificHeuristicsYoh OkunoJob Hunternokuno@nokuno.jpAbstractWe developed a machine transliteration sys-tem combining mpaligner (an improvement ofm2m-aligner), DirecTL+, and some Japanese-specific heuristics for the purpose of NEWS2012.
Our results show that mpaligneris greatly better than m2m-aligner, and theJapanese-specific heuristics are effective forJnJk and EnJa tasks.
While m2m-aligner isnot good at long alignment, mpaligner per-forms well at longer alignment without anylength limit.
In JnJk and EnJa tasks, it is cru-cial to handle long alignment.
An experimen-tal result revealed that de-romanization, whichis reverse operation of romanization, is crucialfor JnJk task.
In EnJa task, it is shown thatmora is the best alignment unit for Japaneselanguage.1 IntroductionNEWS 2012 shared task regards transliteration asphonetic translation of proper nouns across differentlanguages (Zhang et al, 2012).
The most commonapproach for automatic transliteration is to followthe manner of statistical machine translation (Finchand Sumita, 2008).
This approach mainly consistsof 3 steps below.1.
Align training data monotonically2.
Train discriminative model given aligned data3.
Decode input characters to n-best candidateOne of the most popular alignment tools is m2m-aligner (Jiampojamarn et al, 2007), which is re-leased as an open source software 1.
DirecTL+ (Ji-ampojamarn et al, 2008) is a decoding and trainingtool 2 and can be used with m2m-aligner for translit-eration generation task.However, m2m-aligner is not good at long align-ment with no length limit.
It tends to overfit for longalignment since its training is based on maximumlikelihood estimation.
Finch and Sumita (2010)proposed non-parametric Bayesian co-segmentationand applied it to machine transliteration (Finch etal., 2011).
They penalized long alignment adoptingPoisson distribution as prior of word length in theBayesian model.
Another method to penalize longalignment is proposed by Kubo et al (2011) and re-leased as mpaligner 3, originally developed for thepurpose of Japanese pronunciation prediction.
Justfor its availability, we used mpaligner as an alterna-tive of m2m-aligner.Since m2m-aligner and mpaligner are bothcharacter-based alignment, there is a problem toproduce phonetically invalid alignment.
That is,character-based alignment may divide atomic unitsof characters, called mora, into meaningless pieces.Ideally, mora-to-mora alignment should be used forthis task while no training data is provided for suchpurpose.
In this paper, we propose Japanese-specificheuristics to cope with this problem depending onlanguage-specific knowledge.1http://code.google.com/p/m2m-aligner/2http://code.google.com/p/directl-p/3http://sourceforge.jp/projects/mpaligner/612 Related WorksBeside general researches for machine translitera-tion, there are other researches related to Japaneselanguage.
Cherry and Suzuki (2009) applied dis-criminative training to English-name-to-Japanese-Katakana transliteration.
Hatori and Suzuki (2011)proposed a statistical machine translation approachfor Japanese pronunciation prediction task.
Hagi-wara and Sekine (2011) used latent class model fortransliteration including English-to-Japanese.3 mpaligner: Minimum Pattern Alignermpaligner (Kubo et al, 2011) is an improvementof m2m-aligner.
Their idea is simple; to penalizelong alignment by scaling its probability using sumof their length.
More formally, mpaligner uses amodel;P (x, y) = px,y |x|+|y| (1)when deletion and insertion are not allowed.Here, x and y are source and target strings, P (x, y)is probability of string pair (x, y), px,y is a parameterwhich is estimated by previous iteration, and |x|+|y|is sum of length of strings x and y.
Though thescaled probability is no longer normalized, M-stepof EM algorithm performs a kind of normalization.4 Japanese-Specific HeuristicsSince mpaligner is a general-purpose alignment tool,we developed Japanese-specific heuristics as pre-processing for training data.
That is, our systemregards combined characters as one character, andapplies mpaligner to them.4.1 Romanized Japanese Name to JapaneseKanji Back-Transliteration Task (JnJk)The most important heuristic for JnJk task is de-romanization, which is the reverse operation of ro-manization.
In Japanese language, consonants andvowels are coupled and expressed as Kana charac-ters.
Since Kana characters should not be divided,de-romanization converts romanized Japanese toKana characters.
This enables the system to alignKana character as minimal unit.
For this conver-sion, a common romanization table for Japanese in-put method is used 4.
Moreover, a silent charactercalled Sokuon is combined with its previous charac-ter since it can not be aligned alone.Table 1 shows basic conversion table.
We adoptlongest-match algorithm to replace sequence of Ro-man characters to Kana characters.
Without theseoperations, characters like ?KA?
may wrongly di-vided into ?K?
and ?A?
and aligned to differentKanji characters.
More detailed examples are de-scribed in table 2.
The bold rows are correctalignemnts performed by deromanization.4.2 English to Japanese Katakana Task (EnJa)In EnJa task, the alignment unit of target side shouldbe mora, not character.
For this purpose, our sys-tem combines lower case characters with their pre-vious characters.
Moreover, Japanese hyphen is alsocombined with the previous one since they form onemora.As a result, ??
?, ??
?, ??
?, ??
?, ??
?, ??
?, ???,??
?, ??
?, ??
?, ??
?, ???
are combined with theirprevious characters and treated as one mora.
Table3 shows alignment examples with and without thisheuristics.5 ExperimentsIn this section, we show the official scores for 8 lan-guage pairs and further investigation for JnJk andEnJa tasks.5.1 Official Scores for 8 Language PairsTable 4 shows the official scores for 8 languagepairs.
In the official submits, we used mpaligner foralignment and DirecTL+ for training and decoding.We tried two version of mpaligner, 0.9 and 0.97, andchose better one as the primary submission.
Theversion of DirecTL+ is 1.1, and the iteration num-ber is selected automatically by the development set.For JnJk and EnJa tasks, we used our heuristics de-scribed above.
For other language pairs, we justapplied mpaligner and DirecTL+ using their defaultsettings.The results seem good, and we can find that ChEn,EnCh, EnHe and JnJk are difficult tasks in both mea-sures ACC and F-Score.4http://www.social-ime.com/romaji-table.html62Table 1: Basic De-romanization TableBasic RomajiRoman A I U E OKana ?
?
?
?
?Roman KA KI KU KE KOKana ?
?
?
?
?Roman SA SI SU SE SOKana ?
?
?
?
?Roman TA TI TU TE TOKana ?
?
?
?
?Roman NA NI NU NE NOKana ?
?
?
?
?Roman HA HI HU HE HOKana ?
?
?
?
?Roman MA MI MU ME MOKana ?
?
?
?
?Roman YA YU YE YOKana ?
?
??
?Roman RA RI RU RE ROKana ?
?
?
?
?Roman WA WI WU WE WOKana ?
??
?
??
?Voiced Consonants (Dakuon)Roman GA GI GU GE GOKana ?
?
?
?
?Roman ZA ZI ZU ZE ZOKana ?
?
?
?
?Roman DA DI DU DE DOKana ?
?
?
?
?Roman BA BI BU BE BOKana ?
?
?
?
?Unvoiced Consonants (Han-Dakuon)Roman PA PI PU PE POKana ?
?
?
?
?Unvoiced Consonants (Yo-on)Roman FA FI FU FE FOKana ??
??
?
??
?
?Roman SHA SHI SHU SHE SHOKana ??
?
??
??
?
?Roman CHA CHI CHU CHE CHOKana ??
?
??
??
?
?Table 2: Alignment Exapmles for JnJk TaskUnit Source TargetRoman SUZ:UKI ?
:?Kana SUZU:KI ?
:?Roman HIR:OMI ?
:?Kana HIRO:MI ?
:?Roman OK:UNO ?
:?Kana OKU:NO ?
:?Roman JU:NYA ?
:?Kana JUN:YA ?
:?Table 3: Alignment Exapmles for EnJa TaskUnit Source TargetChar J:u:s:mi:ne ?:?:?:?
:?Mora Ju:s:mi:ne ??:?:?
:?Char C:h:a:p:li:n ?:?:?:?:?
:?Mora Cha:p:li:n ???:?:?
:?Char A:r:th:ur ?:?:?
:?Mora Ar:thur ??:?
?Table 4: Official Scores for 8 Language PairsTask ACC F-Score MRR MAPChEn 0.013 0.259 0.017 0.013EnBa 0.404 0.882 0.515 0.403EnCh 0.301 0.655 0.376 0.292EnHe 0.191 0.808 0.254 0.190EnJa 0.362 0.803 0.469 0.359EnKo 0.334 0.688 0.411 0.334EnPe 0.658 0.941 0.761 0.640JnJk 0.512 0.693 0.582 0.401635.2 Investigation for JnJk TaskWe further investigated the results for JnJk task tocompare baseline and proposed system.Table 5 shows the results of JnJk task for devel-opment set.
The settings of tools are determinedby preliminary experiments.
We used m2m-alignerwith length limit of maxX == 6 and maxY == 1,mpaligner with no length limit, and DirecTL+ withcontext size 7 and n-gram order 1.
Proposed sys-tem is combined with Japanese-specific heuristicsincluding de-romanization.The results show two facts; mpaligner greatlybeats m2m-aligner, and proposed de-romanizationimproves more both baseline systems.Table 5: Results on JnJk TaskMethod ACC F-Score MRR MAPm2m-aligner 0.113 0.389 0.182 0.114mpaligner 0.121 0.391 0.197 0.122Proposed 0.199 0.494 0.300 0.2005.3 Investigation for EnJa TaskIn this subsection, we show the results for EnJa taskto compare baseline and proposed system.Table 6 shows the results of EnJa task for devel-opment set.
All of the settings of tools are set defaultin this investigation.Again, mpaligner beats m2m-aligner and ourmora-based alignment improves scores of baselinesystems in this system.Table 6: Results on EnJa TaskMethod ACC F-Score MRR MAPm2m-aligner 0.280 0.737 0.359 0.280mpaligner 0.326 0.761 0.431 0.326Proposed 0.358 0.774 0.469 0.3586 DisccussionWe compared mpaligner and m2m-aligner in theframework of statistical machine transliteration.
InJapanese language, mpaligner performs better thanm2m-aligner.
This fact shows that maximum likeli-hood estimation approach adopted by m2m-aligneris not suitable for the purpose of machine translit-eration.
More importantly in practice, mpaligner isfree from hand-tuning for length limits.We proposed two Japanese-specific heuristics, de-romanization for JnJk task and mora-based align-ment for EnJa task.
They are implemented as pre-processing for training data, and improved the re-sults of transliteration by eliminating linguisticallyinvalid alignments.
This shows the possibility thatcharacter-based alignment may not be the best solu-tion for machine transliteration.Beside Japanese, there can be efficient heuristicsfor other languages.
But, more interesting issue iswhether we can find such heuristics automaticallyor not.7 ConclusionWe applied mpaligner to machine transliteration taskfor the first time and we proposed Japanese-specificheuristics for JnJk and EnJa tasks.We confirmed that the maximum likelihood esti-mation approach adopted by m2m-aligner performspoor for the purpose of machine transliteration.
Oneof methods to cope with this issue is to penalize longalignment using mpaligner.We proposed de-romanization for JnJk task, andmora-based alignment for EnJa task.
In the experi-ments, they demonstrated their capability to improveaccuracy greatly.Our proposed heuristics are language-dependentwhile they can be combined with any otherlanguage-independent methods including (Finch etal., 2011) or (Hagiwara and Sekine, 2011).For future work, language-dependent heuristicsbeside Japanese or methods to find such heuristicsautomatically should be developed.AcknowledgmentsReferencesColin Cherry and Hisami Suzuki.
2009.
Discriminativesubstring decoding for transliteration.
In Proceedingsof the 2009 Conference on Empirical Methods in Nat-ural Language Processing, pages 1066?1075, Singa-pore, August.
Association for Computational Linguis-tics.Andrew Finch and Eiichiro Sumita.
2008.
Phrase-basedmachine transliteration.
In Proceedings of the Work-shop on Technologies and Corpora for Asia-Pacific64Speech Translation (TCAST), pages 13?18, Hyder-abad, India, January.Andrew Finch and Eiichiro Sumita.
2010.
A BayesianModel of Bilingual Segmentation for Transliteration.In Marcello Federico, Ian Lane, Michael Paul, andFranc?ois Yvon, editors, Proceedings of the seventh In-ternational Workshop on Spoken Language Transla-tion (IWSLT), pages 259?266.Andrew Finch, Paul Dixon, and Eiichiro Sumita.2011.
Integrating models derived from non-parametricbayesian co-segmentation into a statistical machinetransliteration system.
In Proceedings of the 3rdNamed Entities Workshop (NEWS 2011), pages 23?27,Chiang Mai, Thailand, November.
Asian Federation ofNatural Language Processing.Masato Hagiwara and Satoshi Sekine.
2011.
Latentclass transliteration based on source language origin.In Proceedings of the 49th Annual Meeting of the As-sociation for Computational Linguistics: Human Lan-guage Technologies, pages 53?57, Portland, Oregon,USA, June.
Association for Computational Linguis-tics.Jun Hatori and Hisami Suzuki.
2011.
Japanese pronun-ciation prediction as phrasal statistical machine trans-lation.
In Proceedings of 5th International Joint Con-ference on Natural Language Processing, pages 120?128, Chiang Mai, Thailand, November.
Asian Federa-tion of Natural Language Processing.Sittichai Jiampojamarn, Grzegorz Kondrak, and TarekSherif.
2007.
Applying many-to-many alignmentsand hidden markov models to letter-to-phoneme con-version.
In Human Language Technologies 2007: TheConference of the North American Chapter of the As-sociation for Computational Linguistics; Proceedingsof the Main Conference, pages 372?379, Rochester,New York, April.
Association for Computational Lin-guistics.Sittichai Jiampojamarn, Colin Cherry, and GrzegorzKondrak.
2008.
Joint processing and discrimina-tive training for letter-to-phoneme conversion.
In Pro-ceedings of ACL-08: HLT, pages 905?913, Columbus,Ohio, June.
Association for Computational Linguis-tics.Keigo Kubo, Hiromichi Kawanami, Hiroshi Saruwatari,and Kiyohiro Shikano.
2011.
Unconstrained many-to-many alignment for automatic pronunciation anno-tation.
In Proceedings of Asia-Pacific Signal and In-formation Processing Association Annual Summit andConference 2011 (APSIPA2011), Xi?an, China, Octo-ber.Min Zhang, A Kumaran, and Haizhou Li.
2012.Whitepaper of news 2012 shared task on machinetransliteration.
In Proceedings of the 4th Named En-tities Workshop (NEWS 2012), Jeju, Korea, July.
TheAssociation of Computational Linguistics.65
