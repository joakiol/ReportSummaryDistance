Proceedings of the Second Workshop on Computational Linguistics for Literature, pages 52?60,Atlanta, Georgia, June 14, 2013. c?2013 Association for Computational LinguisticsGroundhog DAG: Representing Semantic Repetition in Literary Narratives?Greg LessardFrench StudiesQueen?s UniversityCanadagreg.lessard@queensu.caMichael LevisonSchool of ComputingQueen?s UniversityCanadalevison@cs.queensu.caAbstractThis paper discusses the concept of semanticrepetition in literary texts, that is, the recur-rence of elements of meaning, possibly in theabsence of repeated formal elements.
A ty-pology of semantic repetition is presented, aswell as a framework for analysis based on theuse of threaded Directed Acyclic Graphs.
Thismodel is applied to the script for the movieGroundhog Day.
It is shown first that seman-tic repetition presents a number of traits notfound in the case of the repetition of formal el-ements (letters, words, etc.).
Consideration ofthe threaded DAG also brings to light severalclasses of semantic repetition, between individ-ual nodes of a DAG, between subDAGs withina larger DAG, and between structures of sub-DAGs, both within and across texts.
The modelpresented here provides a basis for the detailedstudy of additional literary texts at the seman-tic level and illustrates the tractability of theformalism used for analysis of texts of someconsiderable length and complexity.1 BackgroundRepetition, that is, the reuse of a finite number ofelements, is a fundamental characteristic of naturallanguage.
Thus, the words of a language are com-posed of a small number of phonemes or letters, sen-tences are constructed from repeated words, as wellas larger collocational or syntactic chunks, and so on.Most work on repetition has been concerned with thestudy of such recurring formal elements, typically?This research was supported in part by the Social Sciencesand Humanities Research Council of Canada.from the perspective of their frequency.
However,it is important to recognize that a text can presentnot only cases in which some form recurs, as in 1(a)below, but also instances where meaning recurs, with-out any formal element being necessarily repeated,as in 1(b).
(1) (a) Brutus has killed Caesar!
He has killedhim!
(b) Brutus has killed Caesar!
He plunged hisknife into him and our beloved leader isdead!It is such semantic repetition that concerns us here:that is, the repetition of some semantic content withina text, without there being necessarily a formal ele-ment which recurs.
In particular, we wish to studysemantic repetition in literary texts.
This is important,since literature often brings with it the expectationthat repetition is significant.
To put this another way,repetition tends to be semanticized: its very existence?means?
something.
Consider this first at the formallevel.
It is well-known that human language process-ing tends to extract meaning from sequences of formsand retain the forms themselves for only a limitedtime.
Literary texts counteract this fading effect byseveral linguistic means, including physical proxim-ity of repeated items, stress, and syntactic position.Devices such as these often carry additional informa-tion on importance or some other factor, as when anorator repeats the same word or sequence.
This hasbeen much discussed.
To mention several examplesamong many, Jakobson (1960) refers to this as thepoetic function of language, Genette (1972) providesa typology of narrative repetition, Tsur (2008) argues52that repetition is one of the devices which ?slowsdown?
processing of text and contributes to poeticeffects, Tannen (1989) gives examples of the usage ofrepetition in oral discourse, Okpewho (1992) showsits importance in folk literature, and Johnstone (1991)examines the role of repetition in Arabic discourse.As we will see below, semantic repetition in lit-erature also lends itself to semanticization.
In otherwords, the fact that events are repeated in a narrativecan be, and often is, seen not as the product of chancebut rather as part of a larger pattern.
The potentialfor this is supported by several features of meaning.First, as Stanhope et al(1993) have shown in theirwork on the long-term retention of a novel, unlikeformal elements, at least some semantic elementsare extremely resistant to decay and can be recalledweeks and even many months later.
As a result, thefading effects observed earlier for formal repetitioncannot be assumed to apply in exactly the same fash-ion to semantic repetition: items remain accessibleacross entire texts and even across different texts.Second, there is in principle no upper limit on thesize of semantic elements which may be repeated.At one extreme, a single character from a novel mayremain in memory, along with some of the items as-sociated with him or her.
If one hears the single wordHamlet, what comes to mind?
At the other, entireplots may be recalled.
If asked to summarize theplot of A Christmas Carol in 100 words, most nativespeakers would have no difficulty in doing this.
Andthird, by their tendency to exploit and semanticizerepetition, literary texts differ from other genres, suchas expository texts, whose goal is typically to presentsome set of information in a coherent fashion suchthat the same element not be repeated.In light of this, our goal here is threefold: to give asense of the diversity of semantic repetition in literarytexts, including its various granularities; to propose aformal model capable of dealing with these variousdimensions of semantic repetition; and to test thismodel against an actual text of some considerablelength.2 Events and repetitionLet us assume for the moment that semantic repeti-tion is limited to repeated events, leaving aside issuesof repeated qualities, entities and so on.
A numberof formal and semantic tools suggest themselves fordealing with this case.
Within a single utterance, aneo-Davidsonian event semantics might be used, asshown in (2), where e represents the ?glue?
which tiestogether the action and the agent.
?e[speak(e) ?
agent(e) = fred(e)] (2)This places the event at the centre of focus.
Thelogical machinery behind this has been extended invarious ways.
For example, Hewitt (2012) proposesthe use of serial logic to capture ordered sets ofevents.
In addition, since events are also repeatedacross utterances and related to other events, as inconversations, Asher and Lascarides (2003) providesan extended logical formalism to begin to deal withthis and Helbig (2006) proposes several specificfunctions for linking propositions, including CAUS(causality), CONTR (contrast), and CONF (conformitywith an abstract frame).
However, both approachesare applied to short spans of text and neither dealsexplicitly with repetition.At a slightly higher level of granularity, Rhetori-cal Structure Theory (Mann and Thompson, 1988)provides a set of frameworks to describe relation-ships among elements of a paragraph, some of which,Restatement and Elaboration in particular,have the potential to deal with elements of repeti-tion.1 Work in Natural Language Generation, whichhas often focused on the production of longer exposi-tory texts, has also typically paid more attention tothe reduction of repetition than to its production.2Even work on narrative generation has tended to con-centrate mostly on reduction of repetition (Callawayand Lester, 2001; Callaway and Lester, 2002).Several attempts have been made to deal withlonger spans of texts, typically based on the markupof elements within a text.
Most recently, Mani (2012)proposes a Narrative Markup Language capable ofdealing with elements of repetition, but this markupis anchored to the text itself and it is unclear how suchan approach could capture more abstract elements ofsemantic repetition.
In fact, the fundamental issue1For details, see http://www.sfu.ca/rst/01intro/definitions.html.2See, however, de Rosis and Grasso (2000) who argue forthe role of what they call redundancy.53is that semantic repetition exists across a wide rangeof spans, from the very smallest (both across differ-ent events and within elements of some inherentlyrepeated activity (Tovena and Donazzan, 2008)), tothe very largest, spanning multiple texts.
To illustratethis, consider the following cases.
(a) A single event and the memory of the event inthe mind of the perpetrator.
For example, Brutusstabs Caesar, and then the next day replays thestabbing in his memory.
(b) A single event seen from the point of view oftwo different characters.
For example, Liviasees Brutus stab Caesar, and so does Cassius.
(c) A single, perhaps complex, event, whose dif-ferent facets are represented, perhaps in an in-terspersed fashion.
Good examples of this arefound in cinema, such as Eisenstein?s famousbridge scene in October, or the Odessa stepsscene in Battleship Potemkin, where the sameimages recur (such as the opening bridge, thedead horse, or the baby carriage tipping on theend of a stairway).Examples such as these illustrate what might becalled representational repetition, in which the same(perhaps complex) event is shown from differentpoints of view.
However, we also find examples ofwhat might be called class-based repetition, in whichvarious simple examples share a common abstractstructure, as the following examples illustrate.
(d) Two sets of events in the same text representinstantiations of the same topos, or recurringnarrative structure.
For example, the HebrewBible contains multiple instances in which a par-ent favours a younger sibling over an older one.Thus, the Deity favours Abel over Cain, Abra-ham favours Isaac over Ishmael, Isaac favoursJacob over Esau, and so on.
In these cases, weare actually faced with an abstract frameworkwhich is instantiated with different actual pa-rameters.
(e) Two different texts represent the same abstractplot.
Thus, Pyramus and Thisbe and Romeoand Juliet may both be represented by the sameabstract formula, which we captures the story ofstar-crossed lovers whose feuding families leadto their demise.Examples such as (d) and (e) show that at leastsome elements of literary repetition may only be cap-tured by some device which permits a greater degreeof abstraction than is provided by traditional deviceslike predicate calculus or instance-based markup.From the literary perspective, they are sometimesreferred to as topoi, that is, recurring narrative se-quences.3 However, as formulated in most literaryanalyses, the notion of topos has several shortcom-ings.
First, definitions tend to be informal.4 Second,the granularity of topoi is unclear.
One might ex-press a given topos in very general terms or quitespecifically.Our goal here is to build on the insights of liter-ary theory regarding the meaning of literary texts,while retaining some level of formalism.
To do this,we need first to respect the empirical richness of lit-erary texts.
As the examples above show, simpletwo-line examples are not sufficient to show the truecomplexity of semantic repetition.
Accordingly, wehave chosen as our corpus an entire movie script,described below.
Second, in the case of semanticrepetition, we need a formalism capable of capture-ing various levels of granularity, from quite fine tovery general, and which shows not just differencesof point of view, but elements of class inclusion.
Todo accomplish this, we have adopted the formalismdescribed in Levison et al(2012), based on a func-tional representation of meaning elements by meansof semantic expressions.5 When combined with theuse of threaded Directed Acyclic Graphs, discussedbelow, this formalism permits the representation ofelements of meaning at various levels of granularity,3 Groundhog DayTo illustrate the phenomenon of semantic repetition,we have created a formal analysis of the screenplayfor the popular movie Groundhog Day (henceforth,3A detailed list of topoi, together with examples, may befound in http://satorbase.org.4See Lessard et al(2004) for one attempt at formalization.Note also that the concept of topos shares features with theconcept of scripts (Schank and Abelson, 1977), which has beenformalized to some degree.5The formalism is inspired by the Haskell programming lan-guage (Bird, 1998).54GH).6 Because of its plot structure, discussed below,the script represents arguably an extreme case of se-mantic repetition and thus a good test of the proposedmodel of semantic repetition.GH recounts the story of Phil Connors, an ego-centric weatherman, who has been sent with his pro-ducer, Rita, and cameraman Larry, to cover the an-nual February 2 event at Punxsutawney, Pennsylva-nia, where a groundhog (Punxsutawney Phil), byseeing or not seeing his shadow, provides a predic-tion on the number of weeks remaining in winter.Displeased at such a lowly assignment, Connors be-haves badly to all.
However, on waking up the nextday, he discovers that it is still February 2, and theday unfolds as it had previously.
In the many sub-sequent iterations of the day, Connors discovers thepossibilities inherent in there being no consequencesto his acts, the advantages of being able to perfectthe elements of a seduction by repeated trials, andfinally, the value of altruism and love.
At this point,after many iterations, the cycle is broken, and Philand Rita, now in love, greet February 3.74 Directed Acyclic GraphsTo capture the various elements of granularity in theGH script, we make use of the well-known distinc-tion in literary theory between two perspectives ona narrative.
The fabula or histoire is the informationon which the narrative is based; the sjuzhet or re?citis a particular telling (Bal, 1985; Genette, 1983).
Inour model, we represent the former, which we shallterm a story, by a Directed Acyclic Graph, hence-forth DAG.
A directed graph is a collection of nodeslinked by unidirectional paths.
In an acyclic graph,no sequence of paths may link back to a node alreadyvisited.
In technical terms, the dependency relationportrayed by the graph is transitive, irreflexive andantisymmetric.
Within the DAG, nodes denote piecesof the meaning, perhaps at different levels of granu-larity, and directed paths which indicate the depen-dence of one node upon another.
By dependence,6It should be noted that this screenplay, which may be foundonline at http://www.dailyscript.com/scripts/groundhogday.pdf, diverges in some respects from the film.It contains some scenes which do not appear in the film, and itdoes not contain some others which do appear in the film.7A fuller synopsis can be found at http://www.imdb.com/title/tt0107048/synopsis.we mean that subsequent nodes in the DAG makeuse of information present on previous nodes.
In afiner analysis, the nature of the various dependenciesmight be sub-divided into subclasses like logical de-pendency, temporal dependency, and so on, but wewill not do that here.As noted earlier, we represent the meanings car-ried by the nodes of a DAG by means of semanticexpressions.
So, for example, given the semantic en-tities phil and rita, and the action meet, the ex-pression meet(phil, rita) represents a meet-ing between the two characters in the film.
This ex-pression represents what is called, in the frameworkused, a completion.
Although the functional repre-sentation used permits the representation of semanticniceties like temporal relations and definiteness, themodel used here does not include them.
In the anal-ysis here, each semantic expression corresponds toone node of the DAG.
Of course, such a model mayvary in granularity.
At one extreme, the entire scriptcould be represented by a single expression (as inimprove(phil).
At the other, each small eventcould form the basis of a semantic expression.
Forthe purposes of the present analysis, we have adoptedan intermediate granularity.8Each element of the functional representation isdrawn from a semantic lexicon composed of a formalspecification and an informal one, which provides abasic-level textual output, as shown by the followingexamples:meet :: (entity, entity)-> completionmeet(x,y) ="[x] meets [y]"where the first line shows the formal specificationand the second line the informal one.
The sequenceof semantic expressions, when used to call the in-formal representations, thus provides the gist of thescript, or alternatively, can be used to drive a naturallanguage generation environment.
In addition, sincethe elements of the DAG are formally specified in thesemantic lexicon, they may be analyzed or furthermanipulated by graph manipulation software.
To takea trivial case, the transitive closure of a DAG mightbe calculated.8A fuller discussion of these issues may be found in Levisonand Lessard (2012).555 Threads and threading of a DAGA particular telling of a story, which we call herethe narrative, may be conceived of as a particulartraversal of the DAG.
To designate this, we makeuse of the concept of threading.
Threads are simplysequences of nodes and we often display them in thediagram of a DAG by a dotted line through the nodes.A thread need not follow the edges of the DAG, norneed it be acyclic.
In other words, the same threadmay traverse the same node more than once.
Theordering of the threads of a narrative is assumed tocorrespond to narrative time.
The various segmentsin our diagrams are numbered.
Threads may traversesome but not necessarily all nodes of the DAG.It should be noted that a particular DAG may giverise to numerous possible threadings.
So, for exam-ple, a story may be told in chronological order (?Onceupon a time, there was a beautiful princess ... she waskidnapped by an evil wizard ... a handsome princerescued her ... they lived happily ever after.?
), or inreverse (?The prince and the princess were prepar-ing for their wedding ... this was the outcome of hisrescue of her ... she had been kidnapped...?).
Fur-thermore, a DAG may be threaded to capture not justsome telling of the narrative, but also in terms of thepoint of view of some character, the states of someobject in the narrative, or the representation of spaceor description of places or characters.We will apply this conceptual machinery to theanalysis semantic repetition in the GH script.6 AnalysisAt an abstract level, the relationships behind GH(that is, the story) may be represented by three nodesjoined by solid edges, which show the semantic de-pendencies among the nodes, as shown in Figure1.
The first sets the scene by placing Phil in Punx-sutawney, the second represents Phil?s recursive ac-tions during his endless series of February 2?s, andthe third represents his escape from recursion.At the opposite extreme of granularity, it is pos-sible to show the GH DAG with a thread travers-ing fine-grained nodes, each represented by a se-mantic expression.
This representation, which con-tains 172 nodes and 171 edges, is far too large tofit onto a page.
It may be viewed in its entiretyat http://tinyurl.com/awsb4x6.
As notedFigure 1: The most abstract DAG for GHabove, the segments of the thread are numbered anddotted.
Following them in order thus recounts thesemantic representation of the GH narrative at a rel-atively fine level of granularity.
Between these twoextremes of the abstract DAG and the linear thread-ing, we will now examine several issues of semanticrepetition.6.1 Repetition as return of threads to a nodeThe simplest form of semantic repetition takes theform of a thread passing through some node morethan once.
Figure 2 provides a simple case of this.Figure 2: A thread passing multiple times through thesame nodeThus, Phil meets a beggar at several points in thenarrative (threads 9, 53, 146), with various outcomes,including ignoring the beggar (threads 10, 26, 54)and helping him (thread 147).
Despite this help, thebeggar dies (thread 148), but Phil is given the oppor-tunity to replay this sequence (thread 149), choosingthen to feed the beggar (thread 150).566.2 DAGs and subDAGsConsideration of the entire GH threading shows notjust return of the thread to a single node, but also con-stellations of nodes which ?hang together?.
In somecases, this is based on common membership of thenodes in some class of events.
One example of this isprovided by Phil?s various attempts at suicide.
SincePhil returns to life after each suicide, each suicideattempt (a toaster dropped into a bathtub, leapingfrom a tall building, walking in front of a bus, andso on) shares with the others only membership in theclass of suicide events.
This state of affairs may becaptured by including each of these nodes within alocal subDAG, which itself represents a subnode ofthe larger DAG.
So, for example, we could representthe local subDAG here by means of the semanticexpression attempt(phil, suicide).
SuchsubDAGs may be further refined or combined, sim-ilar to the concept of stepwise refinement found incomputer programming.In the case of the various suicide attempts, it isimportant to note that the various attempts show nodependency among themselves, and no order amongthem is required, beyond that imposed by a particularthreading.
This may be represented as follows:kill(phil, phil, with(electricity))kill(phil, phil, with(jump))and so on.
A similar example is found in Phil?sattempts to improve himself, which involve learningItalian, music, sculpture and medicine, among otherthings.However, we also find instances in which severalnodes within a subDAG do show dependency rela-tions within a common subDAG.
So, for example,when Phil meets Rita at a bar, the same sequence isfollowed: he buys her a drink, they make a toast, andthey discuss Rita?s previous education, as can be seenin Figure 3.Note that both temporal and logical dependence ex-ists between two of the nodes (Phil must buy thedrink in order for them to make a toast).
There isno dependence between these two and the discussionof Rita?s education, but the threading may indicate atemporal order.Mixed models are also possible, in which someelements of a subDAG show dependency while othersdo not, as in the case where Phil awakens to the factFigure 3: The subDAG for Phil and Rita at the barthat his acts have no long-term consequences.
Inone reaction to this, he robs a bank, buys a car andtours the town.
Each of these steps depends on theprevious one.
However, he also gets a tattoo andthrows a party, both of which are independent ofeach other and of the others.
However, together, allthese elements constitute the subDAG of exploringthe absence of consequences.6.3 Parametrized subDAGsIn the presentation so far, we have treated the seman-tic expressions within nodes as constants.
However,examination of the GH DAG brings to light severalinstances in which some part of the DAG is repeatedwith one or more elements replaced systematicallywith different ones.
One illustration of this may befound in Phil?s various reportings of the events atGobbler?s Knob, when the groundhog appears.
Overthe course of the narrative, he is first glib and sarcas-tic, then confused, then professional, then learned,poetic, and finally profound.
This might be repre-sented by five distinct copies of the part.describe(phil, groundhog, ironic)describe(phil, groundhog, confused)and so on.
However, given the similarity betweenthe five nodes, it would be more efficient to create asingle, separated, copy containing parameters, whichcould be instantiated in each of the five places withthe parameters replaced by the appropriate variants.57A similar series of parameters may be found else-where in GH, for example, when Phil greets the manon the stairs of the B&B first ironically, then angrily,and finally with good humour, in Italian.
Or again, ata more complex level, we find a series of instanceswhere Phil learns some new skill (French poetry, pi-ano, Italian, sculpture,...) and subsequently applies it.This is illustrated by two typical subDAGs in Figure4.learn(phil, music)play(phil, piano)learn(phil, sculpture)sculpt(phil, rita)Figure 4: Learning and implementationEach of these series forms a sequence such as:improve(phil, altruism, 0)improve(phil, altruism, 1)and so on, where the third parameter indicates Phil?sprogression along the scale of character development.This particular series provides a means of capturingeach particular state in Phil?s evolution from egotistto altruist.Note however that Phil?s moral development doesnot progress through different areas of his life, oneseries at a time.
In other words, he does not firstchange from a sarcastic to a poetic reporter, thengrow from an egotist to an altruist in the community,then make the transformation from a seducer to anattentive lover, and so on.
Rather, his personal im-provement happens more or less at the same paceacross different facets of his life, reflecting his over-all personal growth, although evidence of this mightbe drawn first from one and then from another of hisactivities.6.4 Parallel DAGsIn the discussion to this point, we have been con-cerned with repetition within a single subDAG.
How-ever, in GH, we also find instances where one sub-DAG shows an architectural similarity to another.This similarity can be construed as a sort of high-levelrepetition.
For example, while on location in Punx-sutawney, Phil meets and seduces Nancy, a womanfrom Punxsutawney.
At the same time, he attemptsto seduce Rita, his producer.In both cases, Phil makes an initial attempt andis rebuffed, by both Nancy and Rita.
Undaunted, hethen seeks more information about both, determiningNancy?s name and obtaining enough information topass as a former high school classmate, and deter-mining that Rita drinks tequila with lime, that shetoasts world peace, and that she likes French poetry.He then uses the information about Nancy to seduceher, but the same tactic is unsuccessful with Rita.The two parallel subDAGs may be represented bya higher-level subDAG where almost all the individ-ual elements change from case to case, with onlythe general framework remaining.
This might beexpressed schematically as follows:experiment(x,y) =slist(meet(x,y)learn(x, of(y, characteristics)))and so on.Applied within a single narrative, such an approachdeals with the sort of parallel cases seen here.
Ap-plied across narratives, it gives rise to texts seen as?telling the same story?, like Romeo and Juliet men-tioned earlier.
At an even more abstract level, it pro-vides a means of modelling parodies, or works basedon some previous model.
Think of Joyce?s Ullysses,in which Stephen Daedalus?
peregrinations aroundDublin represent a parellel to Homer?s Odyssey.6.5 Connections between subDAGsWe now have a means of representing semantic repe-tition at both the low level, of individual nodes of aDAG, as well as within and across DAGs.
However,we have left unspecified an important point, to whichwe now return.
Earlier, we showed that individualnodes may contain subDAGs of interior nodes, upto some indefinite level of complexity.
This varyinggranularity provides a model for different degreesof detail in the recounting of a story, between thehighest-level and coarsest summary, to the finest de-tail.
Consider now the following case from GH.
Eachday, Phil wakes up, hears the same song on the radio,followed by inane banter from two disc jockeys.
At58the level of the DAG, this may be represented as anoverarching node which contains two interior nodes,as shown formulaically here:wakeup(phil) = slist(hear(phil, song)hear(phil, dj_banter))and graphically in Figure 5.Figure 5: Part of the DAG for Phil?s waking upHowever, the actual threading of this higher-levelnodes and its interior nodes in the narrative variesover the course of the narration, as shown in Figure6.Figure 6: The threading of Phil?s waking upThus, in threads 5, 22, 50, 103, 119, 122 and 135,Phil?s waking up is followed by his hearing of thesong, but in thread 36, Phil?s waking up is followedimmediately by the DJ banter.
Similarly, threads 6,23, 51 and 104 join the hearing of the song withthe hearing of the banter, but in the case of threads120 and 123, the recounting of Phil?s hearing of thesong is followed directly by suicide attempts, withno mention of the banter.
In both these cases, wecan presume that the DAG remains constant, but thethreading represents either a complete traversal ofall the interior nodes, or, typically later in the narra-tive, narrative ?shortcuts?
which indicate the entirewakeup DAG by explicitly mentioning only someelements.
Such shortcuts may be found in most narra-tives.
For example, subsequent references to a knowncharacter or event may be reduced to the minimum,since a simple mention reactivates the entire refer-ence.
Conversely, the exploration of interior nodesrather than higher-level ones (in other words, provid-ing more detail) may produce an effect of slowdown(Bal, 1985).In the case of semantic repetition, shortcuts likethose just described demonstrate that not only canrepetition occur in the absence of repeated formal el-ements, but even in the absence of explicitly repeatedsemantic elements.
At the extreme, the activation ofa higher-level node by reference to an interior nodeprovides a model for literary allusions, perhaps themost subtle type of repetition, where some elementin one text activates a reference to another.7 Conclusions and next stepsThe series of examples presented here provide evi-dence for the existence of semantic repetition at boththe atomic and structural levels.
They also showthat these can be captured by a model which permitsvarious levels of granularity, from atomic semanticexpressions to higher-level subDAGs and DAGs.
Itmust be admitted however that, at this stage of theresearch, only human intelligence has permitted theidentification of semantic repetition in its variousforms.
In an ideal world, a computer program mightbe capable of arriving at the same judgments.
Worksuch as Chambers and Jurafsky (2008) or Hobbs et al(1993) might provide a good starting point for this.In the meantime, we believe that there is value in con-tinuing the meaning-first perspective illustrated here,as a complement to the more usual text-first analyses.When combined with a user-friendly formalism, thisapproach would go some way to bridging the dividebetween computer scientists and literary specialistsin their analysis of literary texts.59ReferencesNicholas Asher and Alex Lascarides.
2003.
Logics ofconversation.
Cambridge University Press, Cambridge.Mieke Bal.
1985.
Narratology: introduction to the theoryof narrative.
University of Toronto Press, Toronto.Richard Bird.
1998.
Introduction to functional program-ming using Haskell.
Prentice-Hall, London, 2nd edi-tion.Charles Callaway and James Lester.
2001.
Evaluating theeffects of natural language generation techniques onreader satisfaction.
In Proceedings of the Twenty-ThirdAnnual Conference of the Cognitive Science Society,pages 164?169.Charles Callaway and James Lester.
2002.
Narrativeprose generation.
Artificial Intelligence, 139(2):213?252.Nathanael Chambers and Dan Jurafsky.
2008.
Unsuper-vised learning of narrative event chains.
In Proceedingsof ACL-08: HLT, pages 789?797.Fiorella de Rosis and Floriana Grasso.
2000.
Affectivenatural language generation.
In A.M. Paiva, editor,Affective instructions, pages 204?218.
Springer, Berlin.Ge?rard Genette.
1972.
Figures III.
Editions du Seuil,Paris.Ge?rard Genette.
1983.
Nouveau discours du re?cit.
Edi-tions du Seuil, Paris.Hermann Helbig.
2006.
Knowledge representation andthe semantics of natural language.
Springer, Berlin.Simon Hewitt.
2012.
The logic of finite order.
NotreDame Journal of Formal Logic, 53(3):297?318.Jerry R. Hobbs, Mark E. Stickel, Douglas E. Appelt, andPaul Martin.
1993.
Interpretation as abduction.
Artifi-cial Intelligence, 63:69?142.Roman Jakobson.
1960.
Linguistics and poetics.
InThomas A Sebeok, editor, Style in language, pages350?377.
MIT, Cambridge, Mass.Barbara Johnstone.
1991.
Repetition in Arabic discourse:paradigms, syntagms, and the ecology of language.
J.Benjamins, Amsterdam.Greg Lessard, Ste?fan Sinclair, Max Vernet, Franc?oisRouget, Elisabeth Zawisza, Louis-E?mile Fromet deRosnay, and E?lise Blumet.
2004.
Pour une recherchesemi-automatise?e des topo??
narratifs.
In P. Enjalbertand M. Gaio, editors, Approches se?mantiques du docu-ment e?lectronique, pages 113?130.
Europia, Paris.Michael Levison and Greg Lessard.
2012.
Is this a DAGthat I see before me?
An onomasiological approach tonarrative analysis and generation.
In Mark Finlayson,editor, The Third Workshop on Computational Mod-els of Narrative, pages 134?141, LREC Conference,Istanbul.Michael Levison, Greg Lessard, Craig Thomas, andMatthew Donald.
2012.
The Semantic Representationof Natural Language.
Bloomsbury, London.Inderjeet Mani.
2012.
Computational Modelling of Nar-rative.
Synthesis Lectures on Human Language Tech-nologies.
Morgan and Claypool.William C. Mann and Sandra Thompson.
1988.
Rhetori-cal Structure Theory: toward a functional theory of textorganization.
Text, 8(3):243?281.Isidore Okpewho.
1992.
African oral literature: back-grounds, character, and continuity.
Indiana UniversityPress, Bloomington.Roger C. Schank and Robert P. Abelson.
1977.
Scripts,Plans, Goals and Understanding: An Inquiry into Hu-man Knowledge Structures.
Lawerence Erlbaum Asso-ciates, Hillsdale, NJ.Nicola Stanhope, Gillian Cohen, and Martin Conway.1993.
Very long-term retention of a novel.
AppliedCognitive Psychology, 7:239?256.Deborah Tannen.
1989.
Talking voices: repetition, dia-logue, and imagery in conversational discourse.
Cam-bridge University Press, Cambridge.Lucia M. Tovena and Marta Donazzan.
2008.
On waysof repeating.
Recherches linguistiques de Vincennes,37:85?112.Reuven Tsur.
2008.
Toward a theory of cognitive poetics.Sussex Academic Press, Brighton, 2nd edition.60
