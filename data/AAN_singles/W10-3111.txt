Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, pages 60?68,Uppsala, July 2010.A Survey on the Role of Negation in Sentiment AnalysisMichael WiegandSaarland UniversitySaarbru?cken, Germanymichael.wiegand@lsv.uni-saarland.deAlexandra BalahurUniversity of AlicanteAlicante, Spainabalahur@dlsi.ua.esBenjamin Roth and Dietrich KlakowSaarland UniversitySaarbru?cken, Germanybenjamin.roth@lsv.uni-saarland.dedietrich.klakow@lsv.uni-saarland.deAndre?s MontoyoUniversity of AlicanteAlicante, Spainmontoyo@dlsi.ua.esAbstractThis paper presents a survey on the role ofnegation in sentiment analysis.
Negationis a very common linguistic constructionthat affects polarity and, therefore, needsto be taken into consideration in sentimentanalysis.We will present various computational ap-proaches modeling negation in sentimentanalysis.
We will, in particular, focuson aspects, such as level of representationused for sentiment analysis, negation worddetection and scope of negation.
We willalso discuss limits and challenges of nega-tion modeling on that task.1 IntroductionSentiment analysis is the task dealing with theautomatic detection and classification of opinionsexpressed in text written in natural language.Subjectivity is defined as the linguistic expressionof somebody?s opinions, sentiments, emotions,evaluations, beliefs and speculations (Wiebe,1994).
Subjectivity is opposed to objectivity,which is the expression of facts.
It is important tomake the distinction between subjectivity detec-tion and sentiment analysis, as they are two sep-arate tasks in natural language processing.
Sen-timent analysis can be dependently or indepen-dently done from subjectivity detection, althoughPang and Lee (2004) state that subjectivity de-tection performed prior to the sentiment analysisleads to better results in the latter.Although research in this area has started only re-cently, the substantial growth in subjective infor-mation on the world wide web in the past yearshas made sentiment analysis a task on which con-stantly growing efforts have been concentrated.The body of research published on sentiment anal-ysis has shown that the task is difficult, not onlydue to the syntactic and semantic variability oflanguage, but also because it involves the extrac-tion of indirect or implicit assessments of objects,by means of emotions or attitudes.
Being a partof subjective language, the expression of opinionsinvolves the use of nuances and intricate surfacerealizations.
That is why the automatic study ofopinions requires fine-grained linguistic analysistechniques and substantial efforts to extract fea-tures for machine learning or rule-based systems,in which subtle phenomena as negation can be ap-propriately incorporated.Sentiment analysis is considered as a subsequenttask to subjectivity detection, which should ideallybe performed to extract content that is not factualin nature.
Subsequently, sentiment analysis aimsat classifying the sentiment of the opinions intopolarity types (the common types are positive andnegative).
This text classification task is also re-ferred to as polarity classification.This paper presents a survey on the role of nega-tion in sentiment analysis.
Negation is a very com-mon linguistic construction that affects polarityand, therefore, needs to be taken into considera-tion in sentiment analysis.
Before we describe thecomputational approaches that have been devisedto account for this phenomenon in sentiment anal-ysis, we will motivate the problem.2 MotivationSince subjectivity and sentiment are related to ex-pressions of personal attitudes, the way in whichthis is realized at the surface level influences themanner in which an opinion is extracted and itspolarity is computed.
As we have seen, sentimentanalysis goes a step beyond subjectivity detection,60including polarity classification.
So, in this task,correctly determining the valence of a text span(whether it conveys a positive or negative opinion)is equivalent to the success or failure of the auto-matic processing.It is easy to see that Sentence 1 expresses a posi-tive opinion.1.
I like+ this new Nokia model.The polarity is conveyed by like which is a polarexpression.
Polar expressions, such as like or hor-rible, are words containing a prior polarity.
Thenegation of Sentence 1, i.e.
Sentence 2, using thenegation word not, expresses a negative opinion.2.
I do [not like+]?
this new Nokia model.In this example, it is straightforward to notice theimpact of negation on the polarity of the opinionexpressed.
However, it is not always that easyto spot positive and negative opinions in text.
Anegation word can also be used in other expres-sions without constituting a negation of the propo-sition expressed as exemplified in Sentence 3.3.
Not only is this phone expensive but it is also heavy anddifficult to use.In this context, not does not invert the polarity ofthe opinion expressed which remains negative.Moreover, the presence of an actual negation wordin a sentence does not mean that all its polar opin-ions are inverted.
In Sentence 4, for example, thenegation does not modify the second polar expres-sion intriguing since the negation and intriguingare in separate clauses.4.
[I do [not like+]?
the design of new Nokia model] but[it contains some intriguing+ new functions].Therefore, when treating negation, one must beable to correctly determine the scope that it has(i.e.
determine what part of the meaning expressedis modified by the presence of the negation).Finally, the surface realization of a negation ishighly variable, depending on various factors,such as the impact the author wants to make onthe general text meaning, the context, the textualgenre etc.
Most of the times, its expression is farfrom being simple (as in the first two examples),and does not only contain obvious negation words,such as not, neither or nor.
Research in the fieldhas shown that there are many other words that in-vert the polarity of an opinion expressed, such asdiminishers/valence shifters (Sentence 5), connec-tives (Sentence 6), or even modals (Sentence 7).5.
I find the functionality of the new phone less practical.6.
Perhaps it is a great phone, but I fail to see why.7.
In theory, the phone should have worked even underwater.As can be seen from these examples, modelingnegation is a difficult yet important aspect of sen-timent analysis.3 The SurveyIn this survey, we focus on work that has presentednovel aspects for negation modeling in sentimentanalysis and we describe them chronologically.3.1 Negation and Bag of Words in SupervisedMachine LearningSeveral research efforts in polarity classificationemploy supervised machine-learning algorithms,like Support Vector Machines, Na?
?ve Bayes Clas-sifiers or Maximum Entropy Classifiers.
For thesealgorithms, already a low-level representation us-ing bag of words is fairly effective (Pang et al,2002).
Using a bag-of-words representation, thesupervised classifier has to figure out by itselfwhich words in the dataset, or more precisely fea-ture set, are polar and which are not.
One eitherconsiders all words occurring in a dataset or, asin the case of Pang et al (2002), one carries outa simple feature selection, such as removing infre-quent words.
Thus, the standard bag-of-words rep-resentation does not contain any explicit knowl-edge of polar expressions.
As a consequence ofthis simple level of representation, the reversalof the polarity type of polar expressions as it iscaused by a negation cannot be explicitly modeled.The usual way to incorporate negation modelinginto this representation is to add artificial words:i.e.
if a word x is preceded by a negation word,then rather than considering this as an occurrenceof the feature x, a new feature NOT x is created.The scope of negation cannot be properly modeledwith this representation either.
Pang et al (2002),for example, consider every word until the nextpunctuation mark.
Sentence 2 would, therefore,result in the following representation:8.
I do not NOT like NOT this NOT new NOT NokiaNOT model.The advantage of this feature design is that a plainoccurrence and a negated occurrence of a word are61reflected by two separate features.
The disadvan-tage, however, is that these two contexts treat thesame word as two completely different entities.Since the words to be considered are unrestricted,any word ?
no matter whether it is an actual po-lar expression or not ?
is subjected to this nega-tion modification.
This is not only linguisticallyinaccurate but also increases the feature space withmore sparse features (since the majority of wordswill only be negated once or twice in a corpus).Considering these shortcomings, it comes to nosurprise that the impact of negation modeling onthis level of representation is limited.
Pang et al(2002) report only a negligible improvement byadding the artificial features compared to plain bagof words in which negation is not considered.Despite the lack of linguistic plausibility, super-vised polarity classifiers using bag of words (inparticular, if training and testing are done on thesame domain) offer fairly good performance.
Thisis, in particular, the case on coarse-grained clas-sification, such as on document level.
The suc-cess of these methods can be explained by thefact that larger texts contain redundant informa-tion, e.g.
it does not matter whether a classifiercannot model a negation if the text to be classi-fied contains twenty polar opinions and only oneor two contain a negation.
Another advantageof these machine learning approaches on coarse-grained classification is their usage of higher ordern-grams.
Imagine a labeled training set of docu-ments contains frequent bigrams, such as not ap-pealing or less entertaining.
Then a feature set us-ing higher order n-grams implicitly contains nega-tion modeling.
This also partially explains the ef-fectiveness of bigrams and trigrams for this task asstated in (Ng et al, 2006).The dataset used for the experiments in (Pang etal., 2002; Ng et al, 2006) has been established asa popular benchmark dataset for sentiment analy-sis and is publicly available1.3.2 Incorporating Negation in Models thatInclude Knowledge of Polar Expressions- Early WorksThe previous subsection suggested that appropri-ate negation modeling for sentiment analysis re-quires the awareness of polar expressions.
Oneway of obtaining such expressions is by using a1http://www.cs.cornell.edu/people/pabo/movie-review-datapolarity lexicon which contains a list of polar ex-pressions and for each expression the correspond-ing polarity type.
A simple rule-based polarityclassifier derived from this knowledge typicallycounts the number of positive and negative polarexpressions in a text and assigns it the polaritytype with the majority of polar expressions.
Thecounts of polar expressions can also be used asfeatures in a supervised classifier.
Negation is typ-ically incorporated in those features, e.g.
by con-sidering negated polar expressions as unnegatedpolar expressions with the opposite polarity type.3.2.1 Contextual Valence ShiftersThe first computational model that accounts fornegation in a model that includes knowledge ofpolar expressions is (Polanyi and Zaenen, 2004).The different types of negations are modeled viacontextual valence shifting.
The model assignsscores to polar expressions, i.e.
positive scores topositive polar expressions and negative scores tonegative polar expressions, respectively.
If a polarexpression is negated, its polarity score is simplyinverted (see Example 1).clever (+2) ?
not clever (?2) (1)In a similar fashion, diminishers are taken intoconsideration.
The difference is, however, thatthe score is only reduced rather than shifted to theother polarity type (see Example 2).efficient (+2)?
rather efficient (+1) (2)Beyond that the model also accounts for modals,presuppositional items and even discourse-basedvalence shifting.
Unfortunately, this model isnot implemented and, therefore, one can onlyspeculate about its real effectiveness.Kennedy and Inkpen (2005) evaluate a nega-tion model which is fairly identical to the one pro-posed by Polanyi and Zaenen (2004) (as far assimple negation words and diminishers are con-cerned) in document-level polarity classification.A simple scope for negation is chosen.
A polarexpression is thought to be negated if the negationword immediately precedes it.
In an extension ofthis work (Kennedy and Inkpen, 2006) a parser isconsidered for scope computation.
Unfortunately,no precise description of how the parse is usedfor scope modeling is given in that work.
Neitheris there a comparison of these two scope modelsmeasuring their respective impacts.62Final results show that modeling negation is im-portant and relevant, even in the case of such sim-ple methods.
The consideration of negation wordsis more important than that of diminishers.3.2.2 Features for Negation ModelingWilson et al (2005) carry out more advancednegation modeling on expression-level polarityclassification.
The work uses supervised machinelearning where negation modeling is mostly en-coded as features using polar expressions.
Thefeatures for negation modeling are organized inthree groups:?
negation features?
shifter features?
polarity modification featuresNegation features directly relate to negation ex-pressions negating a polar expression.
One featurechecks whether a negation expression occurs in afixed window of four words preceding the polarexpression.
The other feature accounts for a polarpredicate having a negated subject.
This frequentlong-range relationship is illustrated in Sentence 9.9.
[No politically prudent Israeli]subjectcouldsupportpolar predeither of them.All negation expressions are additionally disam-biguated as some negation words do not functionas a negation word in certain contexts, e.g.
not tomention or not just.Shifter features are binary features checking thepresence of different types of polarity shifters.
Po-larity shifters, such as little, are weaker than ordi-nary negation expressions.
They can be groupedinto three categories, general polarity shifters,positive polarity shifters, and negative polarityshifters.
General polarity shifters reverse polaritylike negations.
The latter two types only reversea particular polarity type, e.g.
the positive shifterabate only modifies negative polar expressions asin abate the damage.
Thus, the presence of a pos-itive shifter may indicate positive polarity.
The setof words that are denoted by these three featurescan be approximately equated with diminishers.Finally, polarity modification features describepolar expressions of a particular type modify-ing or being modified by other polar expressions.Though these features do not explicitly containnegations, language constructions which are sim-ilar to negation may be captured.
In the phrase[disappointed?
hope+]?, for instance, a negativepolar expression modifies a positive polar expres-sion which results in an overall negative phrase.Adding these three feature groups to a featureset comprising bag of words and features count-ing polar expressions results in a significant im-provement.
In (Wilson et al, 2009), the experi-ments of Wilson et al (2005) are extended by adetailed analysis on the individual effectiveness ofthe three feature groups mentioned above.
The re-sults averaged over four different supervised learn-ing algorithms suggest that the actual negation fea-tures are most effective whereas the binary polar-ity shifters have the smallest impact.
This is con-sistent with Kennedy and Inkpen (2005) given thesimilarity of polarity shifters and diminishers.Considering the amount of improvement that isachieved by negation modeling, the improvementseems to be larger in (Wilson et al, 2005).
Theremight be two explanations for this.
Firstly, thenegation modeling in (Wilson et al, 2005) is con-siderably more complex and, secondly, Wilson etal.
(2005) evaluate on a more fine-grained level(i.e.
expression level) than Kennedy and Inkpen(2005) (they evaluate on document level).
As al-ready pointed out in ?3.1, document-level polar-ity classification contains more redundant infor-mation than sentence-level or expression-level po-larity classification, therefore complex negationmodeling on these levels might be more effectivesince the correct contextual interpretation of an in-dividual polar expression is far more important2.The fine-grained opinion corpus used in (Wilsonet al, 2005; Wilson et al, 2009) and all the re-sources necessary to replicate the features used inthese experiments are also publicly available3.3.3 Other ApproachesThe approaches presented in the previous sec-tion (Polanyi and Zaenen, 2004; Kennedy andInkpen, 2005; Wilson et al, 2005) can be consid-ered as the works pioneering negation modelingin sentiment analysis.
We now present some morerecent work on that topic.
All these approaches,however, are heavily related to these early works.2This should also explain why most subsequent works(see ?3.3) have been evaluated on fine-grained levels.3The corpus is available under:http://www.cs.pitt.edu/mpqa/databaserelease and the resourcesfor the features are part of OpinionFinder:http://www.cs.pitt.edu/mpqa/opinionfinderrelease633.3.1 Semantic CompositionIn (Moilanen and Pulman, 2007), a method tocompute the polarity of headlines and complexnoun phrases using compositional semantics ispresented.
The paper argues that the principles ofthis linguistic modeling paradigm can be success-fully applied to determine the subsentential polar-ity of the sentiment expressed, demonstrating itthrough its application to contexts involving senti-ment propagation, polarity reversal (e.g.
throughthe use of negation following Polanyi and Zae-nen (2004) and Kennedy and Inkpen (2005)) orpolarity conflict resolution.
The goal is achievedthrough the use of syntactic representations of sen-tences, on which rules for composition are de-fined, accounting for negation (incrementally ap-plied to constituents depending on the scope) us-ing negation words, shifters and negative polar ex-pressions.
The latter are subdivided into differ-ent categories, such that special words are defined,whose negative intensity is strong enough that theyhave the power to change the polarity of the entiretext spans or constituents they are part of.A similar approach is presented by Shaikh et al(2007).
The main difference to Moilanen andPulman (2007) lies in the representation formaton which the compositional model is applied.While Moilanen and Pulman (2007) use syntac-tic phrase structure trees, Shaikh et al (2007) con-sider a more abstract level of representation be-ing verb frames.
The advantage of a more abstractlevel of representation is that it more accuratelyrepresents the meaning of the text it describes.Apart from that, Shaikh et al (2007) design amodel for sentence-level classification rather thanfor headlines or complex noun phrases.The approach by Moilanen and Pulman (2007) isnot compared against another established classifi-cation method whereas the approach by Shaikh etal.
(2007) is evaluated against a non-compositionalrule-based system which it outperforms.3.3.2 Shallow Semantic CompositionChoi and Cardie (2008) present a more lightweightapproach using compositional semantics towardsclassifying the polarity of expressions.
Theirworking assumption is that the polarity of a phrasecan be computed in two steps:?
the assessment of polarity of the constituents?
the subsequent application of a set of previously-defined inference rulesAn example rule, such as:Polarity([NP1]?
[IN] [NP2]?)
= + (3)may be applied to expressions, such as[lack]?NP1 [of]IN [crime]?NP2 in rural areas.The advantage of these rules is that they restrictthe scope of negation to specific constituentsrather than using the scope of the entire targetexpression.Such inference rules are very reminiscent ofpolarity modification features (Wilson et al,2005), as a negative polar expression is modifiedby positive polar expression.
The rules presentedby Choi and Cardie (2008) are, however, muchmore specific, as they define syntactic contexts ofthe polar expressions.
Moreover, from each con-text a direct polarity for the entire expression canbe derived.
In (Wilson et al, 2005), this decisionis left to the classifier.
The rules are also similarto the syntactic rules from Moilanen and Pulman(2007).
However, they involve less linguisticprocessing and are easier to comprehend4 .
Theeffectiveness of these rules are both evaluated inrule-based methods and a machine learning basedmethod where they are anchored as constraintsin the objective function.
The results of theirevaluation show that the compositional methodsoutperform methods using simpler scopes fornegation, such as considering the scope of theentire target expression.
The learning methodincorporating the rules also slightly outperformsthe (plain) rule-based method.3.3.3 Scope ModelingIn sentiment analysis, the most prominent workexamining the impact of different scope modelsfor negation is (Jia et al, 2009).
The scope de-tection method that is proposed considers:?
static delimiters?
dynamic delimiters?
heuristic rules focused on polar expressionsStatic delimiters are unambiguous words, such asbecause or unless marking the beginning of an-other clause.
Dynamic delimiters are, however,4It is probably due to the latter, that these rules havebeen successfully re-used in subsequent works, most promi-nently Klenner et al (2009).64ambiguous, e.g.
like and for, and require disam-biguation rules, using contextual information suchas their pertaining part-of-speech tag.
These de-limiters suitably account for various complex sen-tence types so that only the clause containing thenegation is considered.The heuristic rules focus on cases in which po-lar expressions in specific syntactic configurationsare directly preceded by negation words which re-sults in the polar expression becoming a delimiteritself.
Unlike Choi and Cardie (2008), these rulesrequire a proper parse and reflect grammatical re-lationships between different constituents.The complexity of the scope model proposedby Jia et al (2009) is similar to the ones ofthe compositional models (Moilanen and Pulman,2007; Shaikh et al, 2007; Choi and Cardie, 2008)where scope modeling is exclusively incorporatedin the compositional rules.Apart from scope modeling, Jia et al (2009) alsoemploy a complex negation term disambiguationconsidering not only phrases in which potentialnegation expressions do not have an actual negat-ing function (as already used in (Wilson et al,2005)), but also negative rhetorical questions andrestricted comparative sentences.On sentence-level polarity classification, theirscope model is compared with?
a simple negation scope using a fixed window size(similar to the negation feature in (Wilson et al, 2005))?
the text span until the first occurrence of a polar expres-sion following the negation word?
the entire sentenceThe proposed method consistently outperformsthe simpler methods proving that the incorpora-tion of linguistic insights into negation modelingis meaningful.
Even on polarity document re-trieval, i.e.
a more coarse-grained classificationtask where contextual disambiguation usuallyresults in a less significant improvement, theproposed method also outperforms the otherscopes examined.There have only been few research efforts insentiment analysis examining the impact of scopemodeling for negation in contrast to other researchareas, such as the biomedical domain (Huang andLowe, 2007; Morante et al, 2008; Morante andDaelemans, 2009).
This is presumably due to thefact that only for the biomedical domain, publiclyavailable corpora containing annotation for thescope of negation exist (Szarvas et al, 2008).
Theusability of those corpora for sentiment analysishas not been tested.3.4 Negation within WordsSo far, negation has only be considered as a phe-nomenon that affects entire words or phrases.The word expressing a negation and the wordsor phrases being negated are disjoint.
There are,however, cases in which both negation and thenegated content which can also be opinionatedare part of the same word.
In case, these wordsare lexicalized, such as flaw-less, and are conse-quently to be found a polarity lexicon, this phe-nomenon does not need to be accounted for in sen-timent analysis.
However, since this process is (atleast theoretically) productive, fairly uncommonwords, such as not-so-nice, anti-war or offensive-less which are not necessarily contained in lexicalresources, may emerge as a result of this process.Therefore, a polarity classifier should also be ableto decompose words and carry out negation mod-eling within words.There are only few works addressing this particu-lar aspect (Moilanen and Pulman, 2008; Ku et al,2009) so it is not clear how much impact this typeof negation has on an overall polarity classificationand what complexity of morphological analysis isreally necessary.
We argue, however, that in syn-thetic languages where negation may regularly berealized as an affix rather than an individual word,such an analysis is much more important.3.5 Negation in Various LanguagesCurrent research in sentiment analysis mainly fo-cuses on English texts.
Since there are signifi-cant structural differences among the different lan-guages, some particular methods may only cap-ture the idiosyncratic properties of the English lan-guage.
This may also affect negation modeling.The previous section already stated that the needfor morphological analyses may differ across thedifferent languages.Moreover, the complexity of scope modeling mayalso be language dependent.
In English, for ex-ample, modeling the scope of a negation as afixed window size of words following the oc-currence of a negation expression already yieldsa reasonable performance (Kennedy and Inkpen,2005).
However, in other languages, for exampleGerman, more complex processing is required asthe negated expression may either precede (Sen-65tence 10) or follow (Sentence 11) the negation ex-pression.
Syntactic properties of the negated nounphrase (i.e.
the fact whether the negated polar ex-pression is a verb or an adjective) determine theparticular negation construction.10.
Peter mag den Kuchen nicht.Peter likes the cake not.
?Peter does not like the cake.?11.
Der Kuchen ist nicht ko?stlich.The cake is not delicious.
?The cake is not delicious.
?These items show that, clearly, some more ex-tensive cross-lingual examination is required in or-der to be able to make statements of the generalapplicability of specific negation models.3.6 Bad and Not Good are Not the SameThe standard approach of negation modeling sug-gests to consider a negated polar expression, suchas not bad, as an unnegated polar expression withthe opposite polarity, such as good.
Liu and Seneff(2009) claim, however, that this is an oversimpli-fication of language.
Not bad and good may havethe same polarity but they differ in their respec-tive polar strength, i.e.
not bad is less positivethan good.
That is why, Liu and Seneff (2009)suggest a compositional model in which for indi-vidual adjectives and adverbs (the latter includenegations) a prior rating score encoding their in-tensity and polarity is estimated from pros andcons of on-line reviews.
Moreover, compositionalrules for polar phrases, such as adverb-adjective ornegation-adverb-adjective are defined exclusivelyusing the scores of the individual words.
Thus,adverbs function like universal quantifiers scalingeither up or down the polar strength of the specificpolar adjectives they modify.
The model indepen-dently learns what negations are, i.e.
a subset ofadverbs having stronger negative scores than otheradverbs.
In short, the proposed model providesa unifying account for intensifiers (e.g.
very), di-minishers, polarity shifters and negation words.
Itsadvantage is that polarity is treated composition-ally and is interpreted as a continuum rather thana binary classification.
This approach reflects itsmeaning in a more suitable manner.3.7 Using Negations in Lexicon InductionMany classification approaches illustrated abovedepend on the knowledge of which natural lan-guage expressions are polar.
The process of ac-quiring such lexical resources is called lexicon in-duction.
The observation that negations co-occurwith polar expressions has been used for inducingpolarity lexicons on Chinese in an unsupervisedmanner (Zagibalov and Carroll, 2008).
One ad-vantage of negation is that though the inductionstarts with just positive polar seeds, the methodalso accomplishes to extract negative polar expres-sions since negated mentions of the positive po-lar seeds co-occur with negative polar expressions.Moreover, and more importantly, the distributionof the co-occurrence between polar expressionsand negations can be exploited for the selection ofthose seed lexical items.
The model presented byZagibalov and Carroll (2008) relies on the obser-vation that a polar expression can be negated but itoccurs more frequently without the negation.
Thedistributional behaviour of an expression, i.e.
sig-nificantly often co-occurring with a negation wordbut significantly more often occurring without anegation word makes up a property of a polar ex-pression.
The data used for these experiments arepublicly available5 .3.8 Irony ?
The Big ChallengeIrony is a rhetorical process of intentionally usingwords or expressions for uttering meaning that isdifferent from the one they have when used liter-ally (Carvalho et al, 2009).
Thus, we considerthat the use of irony can reflect an implicit nega-tion of what is conveyed through the literal use ofthe words.
Moreover, due to its nature irony ismostly used to express a polar opinion.Carvalho et al (2009) confirm the relevance of(verbal) irony for sentiment analysis by an erroranalysis of their present classifier stating that alarge proportion of misclassifications derive fromtheir system?s inability to account for irony.They present predictive features for detectingirony in positive sentences (which are actuallymeant to have a negative meaning).
Their find-ings are that the use of emoticons or expressionsof gestures and the use of quotation marks withina context in which no reported speech is includedare a good signal of irony in written text.
Althoughthe use of these clues in the defined patterns helpsto detect some situations in which irony is present,they do not fully represent the phenomenon.5http://www.informatics.sussex.ac.uk/users/tz21/coling08.zip66A data-driven approach for irony detection onproduct-reviews is presented in (Tsur et al, 2010).In the first stage, a considerably large list of simplesurface patterns of ironic expressions are inducedfrom a small set of labeled seed sentences.
A pat-tern is a generalized word sequence in which con-tent words are replaced by a generic CW symbol.In the second stage, the seed sentences are used tocollect more examples from the web, relying onthe assumption that sentences next to ironic onesare also ironic.
In addition to these patterns, somepunctuation-based features are derived from thelabeled sentences.
The acquired patterns are usedas features along the punctuation-based featureswithin a k nearest neighbour classifier.
On an in-domain test set the classifier achieves a reasonableperformance.
Unfortunately, these experimentsonly elicit few additional insights into the generalnature of irony.
As there is no cross-domain eval-uation of the system, it is unclear in how far thisapproach generalizes to other domains.4 Limits of Negation Modeling inSentiment AnalysisSo far, this paper has not only outlined the impor-tance of negation modeling in sentiment analysisbut it has also shown different ways to account forthis linguistic phenomenon.
In this section, wepresent the limits of negation modeling in senti-ment analysis.Earlier in this paper, we stated that negation mod-eling depends on the knowledge of polar expres-sions.
However, the recognition of genuine polarexpressions is still fairly brittle.
Many polar ex-pressions, such as disease are ambiguous, i.e.
theyhave a polar meaning in one context (Sentence 12)but do not have one in another (Sentence 13).12.
He is a disease to every team he has gone to.13.
Early symptoms of the disease are headaches, fevers,cold chills and body pain.In a pilot study (Akkaya et al, 2009), it has al-ready been shown that applying subjectivity wordsense disambiguation in addition to the feature-based negation modeling approach of Wilson et al(2005) results in an improvement of performancein polarity classification.Another problem is that some polar opinions arenot lexicalized.
Sentence 14 is a negative prag-matic opinion (Somasundaran and Wiebe, 2009)which can only be detected with the help of exter-nal world knowledge.14.
The next time I hear this song on the radio, I?ll throwmy radio out of the window.Moreover, the effectiveness of specific negationmodels can only be proven with the help of cor-pora containing those constructions or the type oflanguage behaviour that is reflected in the mod-els to be evaluated.
This presumably explains whyrare constructions, such as negations using con-nectives (Sentence 6 in ?2), modals (Sentence 7in ?2) or other phenomena presented in the con-ceptual model of Polanyi and Zaenen (2004), havenot yet been dealt with.5 ConclusionIn this paper, we have presented a survey onthe role of negation in sentiment analysis.
Theplethora of work presented on the topic proves thatthis common linguistic construction is highly rel-evant for sentiment analysis.An effective negation model for sentiment analy-sis usually requires the knowledge of polar expres-sions.
Negation is not only conveyed by commonnegation words but also other lexical units, such asdiminishers.
Negation expressions are ambiguous,i.e.
in some contexts do not function as a nega-tion and, therefore, need to be disambiguated.
Anegation does not negate every word in a sentence,therefore, using syntactic knowledge to model thescope of negation expressions is useful.Despite the existence of several approaches tonegation modeling for sentiment analysis, in or-der to make general statements about the effective-ness of specific methods systematic comparativeanalyses examining the impact of different nega-tion models (varying in complexity) with regard toclassification type, text granularity, target domain,language etc.
still need to be carried out.Finally, negation modeling is only one aspect thatneeds to be taken into consideration in sentimentanalysis.
In order to fully master this task, otheraspects, such as a more reliable identification ofgenuine polar expressions in specific contexts, areat least as important as negation modeling.AcknowledgementsMichael Wiegand was funded by the BMBF project NL-Search under contract number 01IS08020B.
Alexandra Bal-ahur was funded by Ministerio de Ciencia e Innovacio?n -Spanish Government (grant no.
TIN2009-13391-C04-01),and Conselleria d?Educacio?n-Generalitat Valenciana (grantno.
PROMETEO/2009/119 and ACOMP/2010/286).67ReferencesC.
Akkaya, J. Wiebe, and R. Mihalcea.
2009.
Subjec-tivity Word Sense Disambiguation.
In Proceedingsof EMNLP.P.
Carvalho, L. Sarmento, M. J. Silva, andE.
de Oliveira.
2009.
Clues for DetectingIrony in User-Generated Contents: Oh...!!
It?s ?soeasy?
;-).
In Proceedings of CIKM-Workshop TSA.Y.
Choi and C. Cardie.
2008.
Learning with Compo-sitional Semantics as Structural Inference for Sub-sentential Sentiment Analysis.
In Proceedings ofEMNLP.Y.
Huang and H. J. Lowe.
2007.
A Novel Hybrid Ap-proach to Automated Negation Detection in ClinicalRadiology Reports.
JAMIA, 14.L.
Jia, C. Yu, and W. Meng.
2009.
The Effect of Nega-tion on Sentiment Analysis and Retrieval Effective-ness.
In Proceedings of CIKM.A.
Kennedy and D. Inkpen.
2005.
Sentiment Classifi-cation of Movie Reviews Using Contextual ValenceShifters.
In Proceedings of FINEXIN.A.
Kennedy and D. Inkpen.
2006.
Sentiment Classifi-cation of Movie Reviews Using Contextual ValenceShifters.
Computational Intelligence, 22.M.
Klenner, S. Petrakis, and A. Fahrni.
2009.
RobustCompositional Polarity Classification.
In Proceed-ings of RANLP.L.
Ku, T. Huang, and H. Chen.
2009.
Using Morpho-logical and Syntactic Structures for Chinese OpinionAnalysis.
In Proceedings ACL/IJCNLP.J.
Liu and S. Seneff.
2009. Review Sentiment Scoringvia a Parse-and-Paraphrase Paradigm.
In Proceed-ings of EMNLP.K.
Moilanen and S. Pulman.
2007.
Sentiment Con-struction.
In Proceedings of RANLP.K.
Moilanen and S. Pulman.
2008.
The Good, the Bad,and the Unknown.
In Proceedings of ACL/HLT.R.
Morante and W. Daelemans.
2009.
A MetalearningApproach to Processing the Scope of Negation.
InProceedings of CoNLL.R.
Morante, A. Liekens, and W. Daelemans.
2008.Learning the Scope of Negation in BiomedicalTexts.
In Proceedings of EMNLP.V.
Ng, S. Dasgupta, and S. M. Niaz Arifin.
2006.
Ex-amining the Role of Linguistic Knowledge Sourcesin the Automatic Identification and Classification ofReviews.
In Proceedings of COLING/ACL.B.
Pang and L. Lee.
2004.
A Sentimental Education:Sentiment Analysis Using Subjectivity Summariza-tion Based on Minimum Cuts.
In Proceedings ofACL.B.
Pang, L. Lee, and S. Vaithyanathan.
2002.
Thumbsup?
Sentiment Classification Using Machine Learn-ing Techniques.
In Proceedings of EMNLP.L.
Polanyi and A. Zaenen.
2004.
Context ValenceShifters.
In Proceedings of the AAAI Spring Sym-posium on Exploring Attitude and Affect in Text.M.
A. M. Shaikh, H. Prendinger, and M. Ishizuka.2007.
Assessing Sentiment of Text by Semantic De-pendency and Contextual Valence Analysis.
In Pro-ceedings of ACII.S.
Somasundaran and J. Wiebe.
2009.
Recogniz-ing Stances in Online Debates.
In Proceedings ofACL/IJCNLP.G.
Szarvas, V. Vincze, R. Farkas, and J. Csirik.
2008.The BioScope Corpus: Annotation for Negation,Uncertainty and Their Scope in Biomedical Texts.In Proceedings of BioNLP.O.
Tsur, D. Davidov, and A. Rappoport.
2010.ICWSM - A Great Catchy Name: Semi-SupervisedRecognition of Sarcastic Sentences in Online Prod-uct Reviews.
In Proceeding of ICWSM.J.
Wiebe.
1994.
Tracking Point of View in Narrative.Computational Linguistics, 20.T.
Wilson, J. Wiebe, and P. Hoffmann.
2005.
Recog-nizing Contextual Polarity in Phrase-level SentimentAnalysis.
In Proceedings of HLT/EMNLP.T.
Wilson, J. Wiebe, and P. Hoffmann.
2009.
Rec-ognizing Contextual Polarity: An Exploration forPhrase-level Analysis.
Computational Linguistics,35:3.T.
Zagibalov and J. Carroll.
2008.
Automatic SeedWord Selection for Unsupervised Sentiment Classi-fication of Chinese Text.
In Proceedings of COL-ING.68
