Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 716?724,Beijing, August 2010Semantic Role Features for Machine TranslationDing LiuDepartment of Computer ScienceUniversity of RochesterDaniel GildeaDepartment of Computer ScienceUniversity of RochesterAbstractWe propose semantic role features for aTree-to-String transducer to model the re-ordering/deletion of source-side semanticroles.
These semantic features, as well asthe Tree-to-String templates, are trainedbased on a conditional log-linear modeland are shown to significantly outperformsystems trained based on Max-Likelihoodand EM.
We also show significant im-provement in sentence fluency by usingthe semantic role features in the log-linearmodel, based on manual evaluation.1 IntroductionSyntax-based statistical machine translation(SSMT) has achieved significant progress duringrecent years (Galley et al, 2006; May andKnight, 2007; Liu et al, 2006; Huang et al,2006), showing that deep linguistic knowledge,if used properly, can improve MT performance.Semantics-based SMT, as a natural extensionto SSMT, has begun to receive more attentionfrom researchers (Liu and Gildea, 2008; Wuand Fung, 2009).
Semantic structures have twomajor advantages over syntactic structures interms of helping machine translation.
First of all,semantic roles tend to agree better between twolanguages than syntactic constituents (Fung et al,2006).
This property motivates the approach ofusing the consistency of semantic roles to selectMT outputs (Wu and Fung, 2009).
Secondly,the set of semantic roles of a predicate modelsthe skeleton of a sentence, which is crucial tothe readability of MT output.
By skeleton, wemean the main structure of a sentence includingthe verbs and their arguments.
In spite of thetheoretical potential of the semantic roles, therehas not been much success in using them toimprove SMT systems.Liu and Gildea (2008) proposed a semantic rolebased Tree-to-String (TTS) transducer by addingsemantic roles to the TTS templates.
Their ap-proach did not differentiate the semantic roles ofdifferent predicates, and did not always improvethe TTS transducer?s performance.
Wu and Fung(2009) took the output of a phrase-based SMT sys-tem Moses (Koehn et al, 2007), and kept permut-ing the semantic roles of the MT output until theybest matched the semantic roles in the source sen-tence.
This approach shows the positive effect ofapplying semantic role constraints, but it requiresre-tagging semantic roles for every permuted MToutput and does not scale well to longer sentences.This paper explores ways of tightly integratingsemantic role features (SRFs) into an MT system,rather than using them in post-processing or n-best re-ranking.
Semantic role labeling (SRL) sys-tems usually use sentence-wide features (Xue andPalmer, 2004; Pradhan et al, 2004; Toutanova etal., 2005); thus it is difficult to compute target-side semantic roles incrementally during decoding.Noticing that the source side semantic roles areeasy to compute, we apply a compromise approach,where the target side semantic roles are generatedby projecting the source side semantic roles us-ing the word alignments between the source andtarget sentences.
Since this approach does not per-form true SRL on the target string, it cannot fullyevaluate whether the source and target semanticstructures are consistent.
However, the approachdoes capture the semantic-level re-ordering of thesentences.
We assume here that the MT system iscapable of providing word alignment (or equiva-lent) information during decoding, which is gener-ally true for current statistical MT systems.Specifically, two types of semantic role featuresare proposed in this paper: a semantic role re-ordering feature designed to capture the skeleton-level permutation, and a semantic role deletion fea-716ture designed to penalize missing semantic roles inthe target sentence.
To use these features during de-coding, we need to keep track of the semantic rolesequences (SRS) for partial translations, which canbe generated based on the source-side semanticrole sequence and the corresponding word align-ments.
Since the SRL system and the MT sys-tem are separate, a translation rule (e.g., a phrasepair in phrase-based SMT) could cover two partialsource-side semantic roles.
In such cases partialSRSs must be recorded in such a way that they canbe combined later with other partial SRSs.
Deal-ing with this problem will increase the complexityof the decoding algorithm.
Fortunately, Tree-to-String transducer based MT systems (Liu et al,2006; Huang et al, 2006) can avoid this problemby using the same syntax tree for both SRL andMT.
Such an arrangement guarantees that a TTStemplate either covers parts of one source-side se-mantic role, or a few complete semantic roles.
Thisadvantage motivates us to use a TTS transducer asthe MT system with which to demonstrate the useof the proposed semantic role features.
Since it ishard to design a generative model to combine boththe semantic role features and the TTS templates,we use a log-linear model to estimate the featureweights, by maximizing the conditional probabil-ities of the target strings given the source syntaxtrees.
The log-linear model with latent variableshas been discussed by Blunsom et al (2008); weapply this technique to combine the TTS templatesand the semantic role features.The remainder of the paper is organized as fol-lows: Section 2 describes the semantic role fea-tures proposed for machine translation; Section 3describes how semantic role features are used andtrained in a TTS transducer; Section 4 presentsthe experimental results; and Section 5 gives theconclusion.2 Semantic Role Features for MachineTranslation2.1 Defining Semantic RolesThere are two semantic standards with publiclyavailable training data: PropBank (Palmer et al,2005) and FrameNet (Johnson et al, 2002).
Prop-Bank defines a set of semantic roles for the verbsin the Penn TreeBank using numbered roles.
Theseroles are defined individually for each verb.
Forexample, for the verb disappoint, the role namearg1 means experiencer, but for the verb wonder,role name arg1 means cause.
FrameNet is moti-vated by the idea that a certain type of verbs canbe gathered together to form a frame, and in thesame frame, a set of semantic roles is defined andshared among the verbs.
For example, the verbsboil, bake, and steam will be in frame apply heat,and they have the semantic roles of cook, food, andheating instrument.
Of these two semantic stan-dards, we choose PropBank over FrameNet for thefollowing reasons:1.
PropBank has a simpler semantic definitionthan FrameNet and thus is easier for auto-matic labeling.2.
PropBank is built upon the Penn TreeBankand is more consistent with statistical parsers,most of which are trained on the Penn Tree-Bank.3.
PropBank is a larger corpus than FrameNet.Note that the semantic standard/corpus is not cru-cial in this paper.
Any training corpus that can beused to automatically obtain the set of semanticroles of a verb could be used in our approach.2.2 Semantic Role FeaturesIdeally, we want to use features based on the truesemantic roles of the MT candidates.
Consider-ing there is no efficient way of integrating SRLand MT, accurate target-side semantic roles canonly be used in post-processing and re-rankingthe MT outputs, where a limited number of MTcandidates are considered.
On the other hand, itis much easier to obtain reliable semantic rolesfor the source sentences.
This paper uses a com-promise approach, where the target-side semanticroles are projected from the source-side semanticroles using the word alignment derived from thetranslation process.
More specifically, we definetwo types of semantic role features:1.
Semantic Role Re-ordering (SRR) This fea-ture describes re-ordering of the source-side717semantic roles (including the predicate) in thetarget side.
It takes the following form:SrcPred : SrcRole1, ..., SrcRolen?
TarRole1, ..., TarRolenwhere SrcPred and SrcRole denotes thecentral verb and semantic roles in the sourceside, and TarRole denotes the target-sideroles.
The source/target SRSs do not need becontinuous, but there should be a one-to-onealignment between the roles in the two sides.Compared to the general re-ordering modelsused in statistical MT systems, this type offeature is capable of modeling skeleton-levelre-ordering, which is crucial to the fluencyof MT output.
Because a predicate can havedifferent semantic role sequences in differentvoices, passive/active are tagged for each oc-currence of the verbs based on their POS andpreceding words.
Figure 1 shows examplesof the feature SRR.2.
Deleted Roles (DR) are the individual source-side semantic roles which are deleted in theMT outputs, taking the form of:SrcPred : SrcRole ?
deletedDR is meant to penalize the deletion of thesemantic roles.
Though most statistical MTsystems have penalties for word deletion, itis still useful to make separate features forthe deletion of semantic roles, which is con-sidered more harmful than the deletion ofnon-core components (e.g., modifiers) anddeserves more serious penalty.
Examples ofthe deletion features can be found in Figure 1.Both types of features can be made non-lexicalizedby removing the actual verb but retaining its voiceinformation in the features.
Non-lexicalized fea-tures are used in the system to alleviate the problemof sparse verbs.3 Using Semantic Role Features inMachine TranslationThis section describes how to use the proposed se-mantic role features in a Tree-to-String transducer,I??did??not??see??the??b???
?arg0arg?negarg1SRR: see?active:?arg?negverbborrowed?active:?arg1?aborrowed?active:?arg1?v eborrowed?active:?arg0?veborrowed?active:?arg1?aDR:see?active:?arg0??deletbook??you??borrowed????arg1arg0??arg?negverbrg0??arg0?arg1erb??verb?arg1erb??arg0?verbrg0?verb?
?arg0?verb?arg1ted?Figure 1: Examples of the semantic role featuresassuming that the semantic roles have been taggedfor the source sentences.
We first briefly describethe basic Tree-to-String translation model used inour experiments, and then describe how to modifyit to incorporate the semantic role features.3.1 Basic Tree-to-String TransducerA Tree-to-String transducer receives a syntax treeas its input and, by recursively applying TTS tem-plates, generates the target string.
A TTS tem-plate is composed of a left-hand side (LHS) anda right-hand side (RHS), where the LHS is a sub-tree pattern and the RHS is a sequence of variablesand translated words.
The variables in the RHSof a template correspond to the bottom level non-terminals in the LHS?s subtree pattern, and theirrelative order indicates the permutation desired atthe point where the template is applied to translateone language to another.
The variables are furthertransformed, and the recursive process goes on un-til there are no variables left.
The formal descrip-tion of a TTS transducer is given by Graehl andKnight (2004), and our baseline approach followsthe Extended Tree-to-String Transducer defined byHuang et al (2006).
For a given derivation (de-composition into templates) of a syntax tree, thetranslation probability is computed as the productof the templates which generate both the sourcesyntax trees and the target translations.Pr(S | T,D?)
=?t?D?Pr(t)Here, S denotes the target sentence, T denotes thesource syntax tree, and D?
denotes the derivationof T .
In addition to the translation model, the718function DECODE(T )for tree node v of T in bottom-up order dofor template t applicable at v do{c1, c2}=match(v, t);s.leftw = c1.leftw;s.rightw = c2.rightw;s.val = c1.val ?
c2.val;s.val ?= Pr(t);s.val ?= Pr(c2.leftw|c1.rightw);add s to v?s beam;Figure 2: Decoding algorithm for the standard Tree-to-Stringtransducer.
leftw/rightw denote the left/right boundaryword of s. c1, c2 denote the descendants of v, ordered basedon RHS of t.TTS system includes a trigram language model,a deletion penalty, and an insertion bonus.
Thebottom-up decoding algorithm for the TTS trans-ducer is sketched in Figure 2.
To incorporate then-gram language model, states in the algorithmdenote a tree node?s best translations with differentleft and right boundary words.
We use standardbeam-pruning to narrow the search space.
To sim-plify the description, we assume in Figure 2 thata bigram language model is used and all the TTStemplates are binarized.
It is straightforward togeneralize the algorithm for larger n-gram modelsand TTS templates with any number of children inthe bottom using target-side binarized combination(Huang et al, 2006).3.2 Modified Tree-to-String Transducer withSemantic Role FeaturesSemantic role features can be used as an auxiliarytranslation model in the TTS transducer, whichfocuses more on the skeleton-level permutation.The model score, depending on not only the in-put source tree and the derivation of the tree, butalso the semantic roles of the source tree, can beformulated as:Pr(S | T,D?)
=?f?F (S,T.role,D?
)Pr(f)where T denotes the source syntax tree withsemantic roles, T.role denotes the seman-tic role sequence in the source side andF (S.role, T.role,D?)
denotes the set of definedsemantic role features over T.role and the targetside semantic role sequence S.role.
Note thatgiven T.role and the derivation D?, S.role canVP NP[giving:?VBG[giving:?verb]givingVP[giving:?ar gTTS?template:(VP?(VBG?givinTriggered??SRR:??giving?active:?aTriggered?DR:?????giving?active:?vNP[giving:?VBG[giving:?verb]givingarg2]NP[giving:?arg1]g2?arg1]g?)??NP#1?NP#2?)???NP#1?NP#2arg2?arg1??arg2?arg1verb?
?deletedarg2]NP[giving:?arg1]Figure 3: An example showing the combination of the se-mantic role sequences of the states.
Above/middle is the stateinformation before/after applying the TTS template, and bot-tom is the used TTS template and the triggered SRFs duringthe combination.be easily derived.
Now we show how to in-corporate the two types of semantic role featuresinto a TTS transducer.
To use the semantic rolere-ordering feature SRR, the states in the decod-ing algorithm need to be expanded to encode thetarget-side SRSs.
The SRSs are initially attachedto the translation states of the source tree con-?
PP VBZ ?
?
?VPVBZ[bring: verb]NP[bring: arg1]PP[bring: arg3]NNP NNnew test0 3 4^Combined SRS arg3 verb arg1Median = 3 arg1Figure 4: An example showing how to compute the target sideposition of a semantic role by using the median of its aligningpoints.719stituents which are labeled as semantic roles forsome predicate.
These semantic roles are thenaccumulated with re-ordering and deletion oper-ations specified by the TTS templates as the de-coding process goes bottom-up.
Figure 5 showsthe decoding algorithm incorporating the SRR fea-tures.
The model component corresponding to thefeature SRR is computed when combining twotranslation states.
I.e., the probabilities of the SRRfeatures composed based on the semantic roles ofthe two combining states will be added into thecombined state.
See Figure 3 for examples.
Thetheoretical upper bound of the decoding complex-ity is O(NM4(n?1)R(?Ci=0 C!i!
)V ), where N isthe number of nodes in the source syntax tree, Mis the vocabulary size of the target language, n isthe order of the n-gram language model, R is themaximum number of TTS templates which can bematched at a tree node, C is the maximum numberof roles of a verb, and V is the maximum numberof verbs in a sentence.
In this formula, ?Ci=0 C!i!is the number of role sequences obtained by firstchoosing i out of C possible roles and then per-muting the i roles.
This theoretical upper boundis not reached in practice, because the number ofpossible TTS templates applicable at a tree nodeis very limited.
Furthermore, since we apply beampruning at each tree node, the running time is con-trolled by the beam size, and is linear in the size ofthe tree.The re-ordering of the semantic roles fromsource to target is computed for each TTS templateas part of the template extraction process, usingthe word-level alignments between the LHS/RHSof the TTS template (e.g., Figure 3).
This is usu-ally straightforward, with the exception of the casewhere the words that are aligned to a particularrole?s span in the source side are not continuousin the target side, as shown in Figure 4.
Sincewe are primarily interested in the relative order ofthe semantic roles, we approximate each seman-tic role?s target side position by the median of theword positions that is aligned to.
If more than onesemantic role is mapped to the same position inthe target side, their source side order will be usedas their target side order, i.e., monotonic transla-tion is assumed for those semantic roles.
Figure 4shows an example of calculating the target sidefunction DECODE(T )for tree node v of T in bottom-up order dofor template t applicable at v do{c1, c2}=match(v, t);s.leftw = c1.leftw;s.rightw = c2.rightw;s.role = concatenate(c1.role, c2.role);if v is a semantic role thenset s.role to v.role;s.val = c1.val ?
c2.val;s.val ?= Pr(t);s.val ?= Pr(c2.leftw|c1.rightw);.
Compute the probabilities associated with semantic roless.val ?= Qf?Sema(c1.role,c2.role,t) Pr(f);add s to v?s beam;Figure 5: Decoding algorithm using semantic role features.Sema(c1.role, c2.role, t) denotes the triggered semanticrole features when combining two children states, and ex-amples can be found in Figure 3.SRS based on a complicated TTS template.
Theword alignments in the TTS templates are also usedto compute the deletion feature DR.
Whenever asemantic role is deleted in a TTS template?s RHS,the corresponding deletion penalty will be applied.3.3 TrainingWe describe two alternative methods for trainingthe weights for the model?s features, including boththe individual TTS templates and the semanticrole features.
The first method maximizes datalikelihood as is standard in EM, while the secondmethod maximizes conditional likelihood for a log-linear model following Blunsom et al (2008).3.3.1 Maximizing Data LikelihoodThe standard way to train a TTS translationmodel is to extract the minimum TTS templates us-ing GHKM (Galley et al, 2004), and then normal-ize the frequency of the extracted TTS templates(Galley et al, 2004; Galley et al, 2006; Liu et al,2006; Huang et al, 2006).
The probability of thesemantic features SRR and DR can be computedsimilarly, given that SRR and DR can be derivedfrom the paired source/target sentences and theword alignments between them.
We refer to thismodel as max-likelihood training and normalizethe counts of TTS templates and semantic featuresbased on their roots and predicates respectively.We wish to overcome noisy alignments fromGIZA++ and learn better TTS rule probabilitiesby re-aligning the data using EM within the TTS720E-step:for all pair of syntax tree T and target string S dofor all TTS Template t, semantic features f doEC(t) +=PD:t?D Pr(S,T,D)PD?
Pr(S,T,D?
);EC(f) +=PD:f?D Pr(S,T,D)PD?
Pr(S,T,D?
);M-step:for all TTS Template t, semantic features f doPr(t) = EC(t)Pt?
:t?.root=t.root EC(t?
);Pr(f) = EC(f)Pf?
:f?.predicate=t.predicate EC(f ?
);Figure 6: EM Algorithm For Estimating TTS Templates andSemantic Featuresframework (May and Knight, 2007).
We can es-timate the expected counts of the TTS templatesand the semantic features by formulating the prob-ability of a pair of source tree and target stringas:XDPr(S, T,D) =XD0@Yt?DPr(t)Yf?F (S,T.role,D)Pr(f)1AThough the above formulation, which makes thetotal probability of all the pairs of trees and stringsless than 1, is not a strict generative model, we canstill use the EM algorithm (Dempster et al, 1977)to estimate the probability of the TTS templatesand the semantic features, as shown in Figure 6.The difficult part of the EM algorithm is the E-step, which computes the expected counts of theTTS templates and the semantic features by sum-ming over all possible derivations of the sourcetrees and target strings.
The standard inside-outside algorithm (Graehl and Knight, 2004) canbe used to compute the expected counts of the TTStemplates.
Similar to the modification made in theTTS decoder, we can add the target-side semanticrole sequence to the dynamic programming statesof the inside-outside algorithm to compute the ex-pected counts of the semantic features.
This wayeach state (associated with a source tree node) rep-resents a target side span and the partial SRSs.
Tospeed up the training, a beam is created for eachtarget span and only the top rated SRSs in the beamare kept.3.3.2 Maximizing Conditional LikelihoodA log-linear model is another way to combinethe TTS templates and the semantic features to-gether.
Considering that the way the semanticfunction COMPUTE PARTITION(T )for tree node v of T in bottom-up order dofor template t applicable at v dofor {s1, s2}=Match(v, t) dos.sum += s1.sum?
s2.sum?exp(?t +Pf?Sema(s1,s2,t) ?f );s.role = concatenate(s1.role, s2.role);add s to v;for state s in root do res += s.sum;return res;Figure 7: Computing the partition function of the conditionalprobability Pr(S|T ).
Sema(s1, s2, t) denotes all the seman-tic role features generated by combining s1 and s2 using t.role features are defined makes it impossible todesign a sound generative model to incorporatethese features, a log-linear model is also a theoreti-cally better choice than the EM algorithm.
If wedirectly translate the EM algorithm into the log-linear model, the problem becomes maximizingthe data likelihood represented by feature weightsinstead of feature probabilities:Pr(S, T ) =PD expPi ?ifi(S, T,D)PS?,T ?PD?
expPi ?ifi(S?, T ?, D?
)where the features f include both the TTS tem-plates and the semantic role features.
The numer-ator in the formula above can be computed usingthe same dynamic programming algorithm used tocompute the expected counts in the EM algorithm.However, the partition function (denominator) re-quires summing over all possible source trees andtarget strings, and is infeasible to compute.
In-stead of approximating the partition function usingmethods such as sampling, we change the objectivefunction from the data likelihood to the conditionallikelihood:Pr(S | T ) =PD expPi ?ifi(S, T,D)PS?
?all(T )PD?
expPi ?ifi(S?, T,D?
)where all(T ) denotes all the possible target stringswhich can be generated from the source tree T .Given a set of TTS templates, the new partitionfunction can be efficiently computed using the dy-namic programming algorithm shown in Figure 7.Again, to simplify the illustration, only binary TTStemplates are used.
Using the conditional proba-bility as the objective function not only reducesthe computational cost, but also corresponds betterto the TTS decoder, where the best MT output is721selected only among the possible candidates whichcan be generated from the input source tree usingTTS templates.The derivative of the logarithm of the objectivefunction (over the entire training corpus) w.r.t.
afeature weight can be computed as:?
logQS,T Pr(S | T )?
?i=XS,T{ECD|S,T (fi)?
ECS?|T (fi)}where ECD|S,T (fi), the expected count of a fea-ture over all derivations given a pair of tree andstring, can be computed using the modified inside-outside algorithm described in Section 3.2, andECS?|T (fi), the expected count of a feature overall possible target strings given the source tree,can be computed in a similar way to the partitionfunction described in Figure 7.
With the objectivefunction and its derivatives, a variety of optimiza-tion methods can be used to obtain the best featureweights; we use LBFGS (Zhu et al, 1994) in ourexperiments.
To prevent the model from overfittingthe training data, a weighted Gaussian prior is usedwith the objective function.
The variance of theGaussian prior is tuned based on the developmentset.4 ExperimentsWe train an English-to-Chinese translation systemusing the FBIS corpus, where 73,597 sentencepairs are selected as the training data, and 500sentence pairs with no more than 25 words on theChinese side are selected for both the developmentand test data.1 Charniak (2000)?s parser, trained onthe Penn Treebank, is used to generate the Englishsyntax trees.
To compute the semantic roles for thesource trees, we use an in-house max-ent classifierwith features following Xue and Palmer (2004) andPradhan et al (2004).
The semantic role labeleris trained and tuned based on sections 2?21 andsection 24 of PropBank respectively.
The standardrole-based F-score of our semantic role labeler is88.70%.
Modified Kneser-Ney trigram modelsare trained using SRILM (Stolcke, 2002) on theChinese portion of the training data.
The model1The total 74,597 sentence pairs used in experiments arethose in the FBIS corpus whose English part can be parsedusing Charniak (2000)?s parser.
(n-gram language model, TTS templates, SRR,DR) weights of the transducer are tuned based onthe development set using a grid-based line search,and the translation results are evaluated based on asingle Chinese reference using BLEU-4 (Papineniet al, 2002).
Huang et al (2006) used character-based BLEU as a way of normalizing inconsistentChinese word segmentation, but we avoid this prob-lem as the training, development, and test data arefrom the same source.The baseline system in our experiments usesthe TTS templates generated by using GHKMand the union of the two single-direction align-ments generated by GIZA++.
Unioning the twosingle-direction alignments yields better perfor-mance for the SSMT systems using TTS templates(Fossum et al, 2008) than the two single-directionalignments and the heuristic diagonal combination(Koehn et al, 2003).
The two single-directionword alignments as well as the union are used togenerate the initial TTS template set for both theEM algorithm and the log-linear model.
The ini-tial TTS templates?
probabilities/weights are set totheir normalized counts based on the root of theTTS template (Galley et al, 2006).
To test seman-tic role features, their initial weights are set to theirnormalized counts for the EM algorithm and to 0for the log-linear model.
The performance of thesesystems is shown in Table 1.
We can see that theEM algorithm, based only on TTS templates, isslightly better than the baseline system.
Addingsemantic role features to the EM algorithm actu-ally hurts the performance, which is not surprisingsince the combination of the TTS templates andsemantic role features does not yield a sound gen-erative model.
The log-linear model based on TTStemplates achieves significantly better results thanboth the baseline system and the EM algorithm.Both improvements are significant at p < 0.05based on 2000 iterations of paired bootstrap re-sampling of the test set (Koehn, 2004).Adding semantic role features to the log-linearmodel further improves the BLEU score.
One prob-lem in our approach is the sparseness of the verbs,which makes it difficult for the log-linear modelto tune the lexicalized semantic role features.
Oneway to alleviate this problem is to make featuresbased on verb classes.
We first tried using the verb722TTS Templates + SRF + Verb ClassUnion 15.6 ?
?EM 15.9 15.5 15.6Log-linear 17.1 17.4 17.6Table 1: BLEU-4 scores of different systemsequal better worseWith SRF vs. W/O SRF 72% 20.2% 7.8%Table 2: Distribution of the sentences where the semanticrole features give no/positive/negative impact to the sentencefluency in terms of the completeness and ordering of thesemantic roles.classes in VerbNet (Dang et al, 1998).
Unfortu-nately, VerbNet only covers about 34% of the verbtokens in our training corpus, and does not im-prove the system?s performance.
We then resortedto automatic clustering based on the aspect model(Hofmann, 1999; Rooth et al, 1999).
The trainingcorpus used in clustering is the English portion ofthe selected FBIS corpus.
Though automaticallyobtained verb clusters lead to further improvementin BLEU score, the total improvement from the se-mantic role features is not statistically significant.Because BLEU-4 is biased towards the adequacyof the MT outputs and may not effectively evaluatetheir fluency, it is desirable to give a more accurateevaluation of the sentence?s fluency, which is theproperty that semantic role features are supposedto improve.
To do this, we manually comparethe outputs of the two log-linear models with andwithout the semantic role features.
Our evaluationfocuses on the completeness and ordering of thesemantic roles, and better, equal, worse are taggedfor each pair of MT outputs indicating the impactof the semantic role features.
Table 2 shows themanual evaluation results based on the entire testset, and the improvement from SRF is significantat p < 0.005 based on a t-test.
To illustrate howSRF impacts the translation results, Figure 8 gives3 examples of the MT outputs with and withoutthe SRFs.5 ConclusionThis paper proposes two types of semantic rolefeatures for a Tree-to-String transducer: one mod-els the reordering of the source-side semantic rolesequence, and the other penalizes the deletion of asource-side semantic role.
These semantic featuresSourceLaunching1New2DipSRF?On??1??
2??3??
4SRF?Off??2??
3?
?4SourceIt 1is 2therefore3netransformation9of 10high 14technologies 15SRF?On??123?4??
6,7??SRF?Off??123?4??
?14,15SourceA 1gratifying2chanstructure8of 9ethnicSRF?On????10,11??8?4SRF?Off??1???2??
3,?plomatic 3Offensive 44 ecessary 4to 5speed6up7the8traditional 11industries12with 135 ???
14,15??9????
11,12,??
6,7????
11,12?
?9nge3also 4occurred5in 6the710minority11cadres12??5???2??
3??4????
10,11????
?8Figure 8: Examples of the MT outputs with and without SRFs.The first and second example shows that SRFs improve thecompleteness and the ordering of the MT outputs respectively,the third example shows that SRFs improve both properties.The subscripts of each Chinese phrase show their alignedwords in English.and the Tree-to-String templates, trained based ona conditional log-linear model, are shown to sig-nificantly improve a basic TTS transducer?s per-formance in terms of BLEU-4.
To avoid BLEU?sbias towards the adequacy of the MT outputs, man-ual evaluation is conducted for sentence fluencyand significant improvement is shown by usingthe semantic role features in the log-linear model.Considering our semantic features are the most ba-sic ones, using more sophisticated features (e.g.,the head words and their translations of the source-side semantic roles) provides a possible directionfor further experimentation.Acknowledgments This work was funded byNSF IIS-0546554 and IIS-0910611.ReferencesBlunsom, Phil, Trevor Cohn, and Miles Osborne.
2008.A discriminative latent variable model for statisticalmachine translation.
In Proceedings of the 46th An-nual Meeting of the Association for ComputationalLinguistics (ACL-08), Columbus, Ohio.Charniak, Eugene.
2000.
A maximum-entropy-inspired parser.
In Proceedings of NAACL-01, pages132?139.Dang, Hoa Trang, Karin Kipper, Martha Palmer, and723Joseph Rosenzweig.
1998.
Investigating regu-lar sense extensions based on intersective Levinclasses.
In COLING/ACL-98, pages 293?299, Mon-treal.
ACL.Dempster, A. P., N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via theEM algorithm.
Journal of the Royal Statistical Soci-ety, 39(1):1?21.Fossum, Victoria, Kevin Knight, and Steven Abney.2008.
Using syntax to improveword alignment pre-cision for syntax-based machine translation.
In Pro-ceedings of the Third Workshop on Statistical Ma-chine Translation, Columbus, Ohio.
ACL.Fung, Pascale, Zhaojun Wu, Yongsheng Yang, andDekai Wu.
2006.
Learning of Chinese/English se-mantic structure mapping.
In IEEE/ACL 2006 Work-shop on Spoken Language Technology, Aruba.Galley, Michel, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a translation rule?In Proceedings of NAACL-04, pages 273?280.Galley, Michel, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Pro-ceedings of COLING/ACL-06, pages 961?968, July.Graehl, Jonathan and Kevin Knight.
2004.
Trainingtree transducers.
In Proceedings of NAACL-04.Hofmann, Thomas.
1999.
Probabilistic latent semanticanalysis.
In Uncertainity in Artificial Intelligence,UAI?99, Stockholm.Huang, Liang, Kevin Knight, and Aravind Joshi.
2006.Statistical syntax-directed translation with extendeddomain of locality.
In Proceedings of the 7th Bi-ennial Conference of the Association for MachineTranslation in the Americas (AMTA), Boston, MA.Johnson, Christopher R., Charles J. Fillmore, MiriamR.
L. Petruck, Collin F. Baker, Michael Ellsworth,Josef Ruppenhofer, and Esther J.
Wood.
2002.FrameNet: Theory and practice.
Version 1.0,http://www.icsi.berkeley.edu/framenet/.Koehn, Philipp, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of NAACL-03, Edmonton, Alberta.Koehn, Philipp, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProceedings of ACL, Demonstration Session, pages177?180.Koehn, Philipp.
2004.
Statistical significance tests formachine translation evaluation.
In Proceedings ofEMNLP, pages 388?395, Barcelona, Spain, July.Liu, Ding and Daniel Gildea.
2008.
Improved tree-to-string transducers for machine translation.
InACL Workshop on Statistical Machine Translation(ACL08-SMT), pages 62?69, Columbus, Ohio.Liu, Yang, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machinetranslation.
In Proceedings of COLING/ACL-06,Sydney, Australia, July.May, Jonathan and Kevin Knight.
2007.
Syntacticre-alignment models for machine translation.
In Pro-ceedings of EMNLP.Palmer, Martha, Daniel Gildea, and Paul Kingsbury.2005.
The Proposition Bank: An annotated corpus ofsemantic roles.
Computational Linguistics, 31(1):71?106.Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automaticevaluation of machine translation.
In Proceedings ofACL-02.Pradhan, Sameer, Wayne Ward, Kadri Hacioglu, JamesMartin, , and Dan Jurafsky.
2004.
Shallow semanticparsing using support vector machines.
In Proceed-ings of NAACL-04.Rooth, Mats, Stefan Riezler, Detlef Prescher, GlennCarroll, and Franz Beil.
1999.
Inducing a semanti-cally annotated lexicon via EM-based clustering.
InProceedings of the 37th Annual Meeting of the ACL,pages 104?111, College Park, Maryland.Stolcke, Andreas.
2002.
SRILM - an extensible lan-guage modeling toolkit.
In International Conferenceon Spoken Language Processing, volume 2, pages901?904.Toutanova, Kristina, Aria Haghighi, and ChristopherManning.
2005.
Joint learning improves semanticrole labeling.
In Proceedings of ACL-05, pages 589?596.Wu, Dekai and Pascale Fung.
2009.
Semantic rolesfor smt: A hybrid two-pass model.
In Proceedingsof the HLT-NAACL 2009: Short Papers, Boulder,Colorado.Xue, Nianwen and Martha Palmer.
2004.
Calibratingfeatures for semantic role labeling.
In Proceedingsof EMNLP.Zhu, Ciyou, Richard H. Byrd, Peihuang Lu, and JorgeNocedal.
1994.
L-BFGS-B: Fortran subroutines forlarge-scale bound constrained optimization.
Techni-cal report, ACM Trans.
Math.
Software.724
