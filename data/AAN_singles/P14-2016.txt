Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 93?98,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsAutomatic Detection of Multilingual Dictionaries on the WebGintar?e Grigonyt?e?Timothy Baldwin??
Department of Linguistics, Stockholm University?
Department of Computing and Information Systems, The University of Melbournegintare@ling.su.se tb@ldwin.netAbstractThis paper presents an approach to queryconstruction to detect multilingual dictio-naries for predetermined language combi-nations on the web, based on the identifi-cation of terms which are likely to occurin bilingual dictionaries but not in generalweb documents.
We use eight target lan-guages for our case study, and train ourmethod on pre-identified multilingual dic-tionaries and the Wikipedia dump for eachof our languages.1 MotivationTranslation dictionaries and other multilinguallexical resources are valuable in a myriad ofcontexts, from language preservation (Thiebergerand Berez, 2012) to language learning (Lauferand Hadar, 1997), cross-language informationretrieval (Nie, 2010) and machine translation(Munteanu and Marcu, 2005; Soderland et al,2009).
While there are syndicated effortsto produce multilingual dictionaries for differ-ent pairings of the world?s languages such asfreedict.org, more commonly, multilingualdictionaries are developed in isolation for a spe-cific set of languages, with ad hoc formatting,great variability in lexical coverage, and no cen-tral indexing of the content or existence of thatdictionary (Baldwin et al, 2010).
Projects suchas panlex.org aspire to aggregate these dic-tionaries into a single lexical database, but arehampered by the need to identify individual multi-lingual dictionaries, especially for language pairswhere there is a sparsity of data from existing dic-tionaries (Baldwin et al, 2010; Kamholz and Pool,to appear).
This paper is an attempt to automatethe detection of multilingual dictionaries on theweb, through query construction for an arbitrarylanguage pair.
Note that for the method to work,we require that the dictionary occurs in ?list form?,that is it takes the form of a single document (orat least, a significant number of dictionary entrieson a single page), and is not split across multiplesmall-scale sub-documents.2 Related WorkThis research seeks to identify documents of aparticular type on the web, namely multilingualdictionaries.
Related work broadly falls intofour categories: (1) mining of parallel corpora;(2) automatic construction of bilingual dictionar-ies/thesauri; (3) automatic detection of multilin-gual documents; and (4) classification of docu-ment genre.Parallel corpus construction is the task of au-tomatically detecting document sets that containthe same content in different languages, com-monly based on a combination of site-structuraland content-based features (Chen and Nie, 2000;Resnik and Smith, 2003).
Such methods couldpotentially identify parallel word lists from whichto construct a bilingual dictionary, although morerealistically, bilingual dictionaries exist as singledocuments and are not well suited to this style ofanalysis.Methods have also been proposed to automat-ically construct bilingual dictionaries or thesauri,e.g.
based on crosslingual glossing in predictablepatterns such as a technical term being immedi-ately proceeded by that term in a lingua francasource language such as English (Nagata et al,2001; Yu and Tsujii, 2009).
Alternatively, com-parable or parallel corpora can be used to extractbilingual dictionaries based on crosslingual distri-butional similarity (Melamed, 1996; Fung, 1998).While the precision of these methods is generallyrelatively high, the recall is often very low, as thereis a strong bias towards novel technical terms be-ing glossed but more conventional terms not.Also relevant to this work is research on lan-93guage identification, and specifically the detectionof multilingual documents (Prager, 1999; Yam-aguchi and Tanaka-Ishii, 2012; Lui et al, 2014).Here, multi-label document classification meth-ods have been adapted to identify what mix oflanguages is present in a given document, whichcould be used as a pre-filter to locate documentscontaining a given mixture of languages, althoughthere is, of course, no guarantee that a multilingualdocument is a dictionary.Finally, document genre classification is rele-vant in that it is theoretically possible to developa document categorisation method which classi-fies documents as multilingual dictionaries or not,with the obvious downside that it would need to beapplied exhaustively to all documents on the web.The general assumption in genre classification isthat the type of a document should be judged notby its content but rather by its form.
A varietyof document genre methods have been proposed,generally based on a mixture of structural andcontent-based features (Matsuda and Fukushima,1999; Finn et al, 2002; zu Eissen and Stein, 2005).While all of these lines of research are relevantto this work, as far as we are aware, there has notbeen work which has proposed a direct methodfor identifying pre-existing multilingual dictionar-ies in document collections.3 MethodologyOur method is based on a query formulation ap-proach, and querying against a pre-existing indexof a document collection (e.g.
the web) via an in-formation retrieval system.The first intuition underlying our approach isthat certain words are a priori more ?language-discriminating?
than others, and should be pre-ferred in query construction (e.g.
sushi occurs asa [transliterated] word in a wide variety of lan-guages, whereas anti-discriminatory is found pre-dominantly in English documents).
As such, weprefer search terms wiwith a higher value formaxlP (l|wi), where l is the language of interest.The second intuition is that the lexical cover-age of dictionaries varies considerably, especiallywith multilingual lexicons, which are often com-piled by a single developer or small communityof developers, with little systematicity in what isincluding or not included in the dictionary.
Assuch, if we are to follow a query construction ap-proach to lexicon discovery, we need to be ableto predict the likelihood of a given word wibe-ing included in an arbitrarily-selected dictionaryDlincorporating language l (i.e.
P (wi|Dl)).
Fac-tors which impact on this include the lexical priorof the word in the language (e.g.
P (paper|en) >P (papyrus|en)), whether they are lemmas or not(noting that multilingual dictionaries tend not tocontain inflected word forms), and their word class(e.g.
multilingual dictionaries tend to contain morenouns and verbs than function words).The third intuition is that certain word combi-nations are more selective of multilingual dictio-naries than others, i.e.
if certain words are foundtogether (e.g.
cruiser, gospel and noodle), the con-taining document is highly likely to be a dictionaryof some description rather than a ?conventional?document.Below, we describe our methodology for queryconstruction based on these elements in greater de-tail.
The only assumption on the method is thatwe have access to a selection of dictionaries D(mono- or multilingual) and a corpus of conven-tional (non-dictionary) documents C, and knowl-edge of the language(s) contained in each dictio-nary and document.Given a set of dictionaries Dlfor a language land the complement set Dl= D\Dl, we first con-struct the lexicon Llfor that language as follows:Ll={wi|wi?
Dl?
wi/?
Dl}(1)This creates a language-discriminating lexicon foreach language, satisfying the first criterion.Lexical resources differ in size, scope and cov-erage.
For instance, a well-developed, maturemultilingual dictionary may contain over 100,000multilingual lexical records, while a specialised 5-way multilingual domain dictionary may containas few as 100 multilingual lexical records.
In linewith our second criterion, we want to select wordswhich have a higher likelihood of occurrence ina multilingual dictionary involving that language.To this end, we calculate the weight sdict(wi,l) foreach word wi,l?
Ll:sdict(wi,l) =?d?Dl{|Ll|?|d||Ll|if wi,l?
d?|d||Ll|otherwise(2)where |d| is the size of dictionary d in terms of thenumber of lexemes it contains.The final step is to weight words by their typ-icality in a given language, as calculated by their94likelihood of occurrence in a random document inthat language.
This is estimated by the proportionof Wikipedia documents in that language whichcontain the word in question:Score(wi,l) =df(wi,l)Nlsdict(wi,l) (3)where df(wi,l) is the count of Wikipedia docu-ments of language l which contain wi, and Nlisthe total number of Wikipedia documents in lan-guage l.In all experiments in this paper, we assume thatwe have access to at least one multilingual dictio-nary containing each of our target languages, butin absence of such a dictionary, sdict(wi,l) couldbe set to 1 for all words wi,lin the language.The result of this term weighing is a ranked listof words for each language.
The next step is toidentify combinations of words that are likely tobe found in multilingual dictionaries and not stan-dard documents for a given language, in accor-dance with our third criterion.3.1 Apriori-based query generationWe perform query construction for each languagebased on frequent item set mining, using the Apri-ori algorithm (Agrawal et al, 1993).
For a givencombination of languages (e.g.
English and Swa-heli), queries are then formed simply by combin-ing monolingual queries for the component lan-guages.The basic approach is to use a modified supportformulation within the Apriori algorithm to preferword combinations that do not cooccur in regulardocuments.
Based on the assumption that query-ing a (pre-indexed) document collection is rela-tively simple, we generate a range of queries of de-creasing length and increasing likelihood of termco-occurrence in standard documents, and queryuntil a non-empty set of results is returned.The modified support formulation is as follows:cscore(w1, ..., wn) ={0 if ?d,wi, wj: cod(wi, wj)?iScore(wi) otherwisewhere cod(wi, wj) is a Boolean function whichevaluates to true iff wiand wjco-occur in doc-ument d. That is, we reject any combinations ofwords which are found to co-occur in Wikipediadocuments for that language.
Note that the actualcalculation of this co-occurrence can be performedFigure 1: Examples of learned queries for differentlanguagesefficiently, as: (a) for a given iteration of Apri-ori, it only needs to be performed between the newword that we are adding to the query (?item set?
inthe terminology of Apriori) and each of the otherwords in a non-zero support itemset from the pre-vious iteration of the algorithm (which are guaran-teed to not co-occur with each other); and (b) thedetermination of whether two terms collocate canbe performed efficiently using an inverted index ofWikipedia for that language.In our experiments, we apply the Apriori al-gorithm exhaustively for a given language with asupport threshold of 0.5, and return the resultantitem sets in ranked order of combined score forthe component words.A random selection of queries learned for eachof the 8 languages targeted in this research is pre-sented in Figure 1.4 Experimental methodologyWe evaluate our proposed methodology in twoways:1. against a synthetic dataset, whereby we in-jected bilingual dictionaries into a collectionof web documents, and evaluated the abilityof the method to return multilingual dictio-naries for individual languages; in this, wenaively assume that all web documents in thebackground collection are not multilingualdictionaries, and as such, the results are po-tentially an underestimate of the true retrievaleffectiveness.2.
against the open web via the Google searchAPI for a given combination of languages,and hand evaluation of the returned docu-ments95Lang Wikipedia articles (M) Dictionaries Queries learned Avg.
query lengthen 3.1 26 2546 3.2zh 0.3 0 5034 3.6es 0.5 2 356 2.9ja 0.6 0 1532 3.3de 1.0 13 634 2.7fr 0.9 5 4126 3.0it 0.6 4 1955 3.0ar 0.1 2 9004 3.2Table 1: Details of the training data and queries learned for each languageNote that the first evaluation with the syntheticdataset is based on monolingual dictionary re-trieval effectiveness because we have very few(and often no) multilingual dictionaries for a givenpairing of our target languages.
For a given lan-guage, we are thus evaluating the ability of ourmethod to retrieve multilingual dictionaries con-taining that language (and other indeterminate lan-guages).For both the synthetic dataset and open web ex-periments, we evaluate our method based on meanaverage precision (MAP), that is the mean of theaverage precision scores for each query which re-turns a non-empty result set.To train our method, we use 52 bilingual Free-dict (Freedict, 2011) dictionaries and Wikipedia1documents for each of our target languages.
Asthere are no bilingual dictionaries in Freedict forChinese and Japanese, the training of Score valuesis based on the Wikipedia documents only.
Mor-phological segmentation for these two languageswas carried out using MeCab (MeCab, 2011) andthe Stanford Word Segmenter (Tseng et al, 2005),respectively.
See Table 1 for details of the num-ber of Wikipedia articles and dictionaries for eachlanguage.Below, we detail the construction of the syn-thetic dataset.4.1 Synthetic datasetThe synthetic dataset was constructed using a sub-set of ClueWeb09 (ClueWeb09, 2009) as the back-ground web document collection.
The originalClueWeb09 dataset consists of around 1 billionweb pages in ten languages that were collected inJanuary and February 2009.
The relative propor-tions of documents in the different languages inthe original dataset are as detailed in Table 2.We randomly downsampled ClueWeb09 to 101Based on 2009 dumps.Language Proportionen (English) 48.41%zh (Chinese) 17.05%es (Spanish) 7.62%ja (Japanese) 6.47%de (German) 4.89%fr (French) 4.79%ko (Korean) 3.61%it (Italian) 2.8%pt (Portuguese) 2.62%ar (Arabic) 1.74%Table 2: Language proportions in ClueWeb09.million documents for the 8 languages targetedin this research (the original 10 ClueWeb09 lan-guages minus Korean and Portuguese).
We thensourced a random set of 246 multilingual dic-tionaries that were used in the construction ofpanlex.org, and injected them into the docu-ment collection.
Each of these dictionaries con-tains at least one of our 8 target languages, withthe second language potentially being outside the8.
A total of 49 languages are contained in thedictionaries.We indexed the synthetic dataset using Indri (In-dri, 2009).5 ResultsFirst, we present results over the synthetic datasetin Table 3.
As our baseline, we simply query forthe language name and the term dictionary in thelocal language (e.g.
English dictionary, for En-glish) in the given language.For languages that had bilingual dictionaries fortraining, the best results were obtained for Span-ish, German, Italian and Arabic.
Encouragingly,the results for languages with only Wikipedia doc-uments (and no dictionaries) were largely com-parable to those for languages with dictionaries,with Japanese achieving a MAP score compara-ble to the best results for languages with dictio-nary training data.
The comparably low result for96Lang Dicts MAP Baselineen 92 0.77 0.00zh 7 0.75 0.00es 34 0.98 0.04ja 5 0.94 0.00de 75 0.97 0.08fr 34 0.84 0.03it 8 0.95 0.01ar 3 0.92 0.00AVERAGE: 32.2 0.88 0.04Table 3: Dictionary retrieval results over the syn-thetic dataset (?Dicts?
= the number of dictionariesin the document collection for that language.English is potentially affected by its prevalenceboth in the bilingual dictionaries in training (re-stricting the effective vocabulary size due to ourLlfiltering), and in the document collection.
Re-call also that our MAP scores are an underestimateof the true results, and some of the ClueWeb09documents returned for our queries are potentiallyrelevant documents (i.e.
multilingual dictionariesincluding the language of interest).
For all lan-guages, the baseline results were below 0.1, andsubstantially lower than the results for our method.Looking next to the open web, we present in Ta-ble 4 results based on querying the Google searchAPI with the 1000 longest queries for Englishpaired with each of the other 7 target languages.Most queries returned no results; indeed, for theen-ar language pair, only 49/1000 queries returneddocuments.
The results in Table 4 are based onmanual evaluation of all documents returned forthe first 50 queries, and determination of whetherthey were multilingual dictionaries containing theindicated languages.The baseline results are substantially higherthan those for the synthetic dataset, almost cer-tainly a direct result of the greater sophisticationand optimisation of the Google search engine (in-cluding query log analysis, and link and anchortext analysis).
Despite this, the results for ourmethod are lower than those over the syntheticdataset, we suspect largely as a result of the styleof queries we issue being so far removed fromstandard Google query patterns.
Having said this,MAP scores of 0.32?0.92 suggest that the methodis highly usable (i.e.
at any given cutoff in the doc-ument ranking, an average of at least one in threedocuments is a genuine multilingual dictionary),and any non-dictionary documents returned by themethod could easily be pruned by a lexicographer.Lang Dicts MAP Baselinezh 16 0.55 0.19es 17 0.92 0.13ja 13 0.32 0.04de 34 0.77 0.09fr 36 0.77 0.08it 23 0.69 0.11ar 8 0.39 0.17AVERAGE: 21.0 0.63 0.12Table 4: Dictionary retrieval results over the openweb for dictionaries containing English and eachof the indicated languages (?Dicts?
= the numberof unique multilingual dictionaries retrieved forthat language).Among the 7 language pairs, en-es, en-de, en-frand en-it achieved the highest MAP scores.
Interms of unique lexical resources found with 50queries, the most successful language pairs wereen-fr, en-de and en-it.6 ConclusionsWe have described initial results for a method de-signed to automatically detect multilingual dictio-naries on the web, and attained highly credible re-sults over both a synthetic dataset and an exper-iment over the open web using a web search en-gine.In future work, we hope to explore the abilityof the method to detect domain-specific dictionar-ies (e.g.
training over domain-specific dictionar-ies from other language pairs), and low-densitylanguages where there are few dictionaries andWikipedia articles to train the method on.AcknowledgementsWe wish to thank the anonymous reviewers fortheir valuable comments, and the Panlex devel-opers for assistance with the dictionaries and ex-perimental design.
This research was supportedby funding from the Group of Eight and the Aus-tralian Research Council.ReferencesRakesh Agrawal, Tomasz Imieli?nski, and Arun Swami.1993.
Mining association rules between sets ofitems in large databases.
ACM SIGMOD Record,22(2):207?216.Timothy Baldwin, Jonathan Pool, and Susan M.Colowick.
2010.
PanLex and LEXTRACT: Trans-lating all words of all languages of the world.
In97Proceedings of the 23rd International Conference onComputational Linguistics (COLING 2010), DemoVolume, pages 37?40, Beijing, China.Jiang Chen and Jian-Yun Nie.
2000.
Parallel web textmining for cross-language IR.
In Proceedings ofRecherche d?Informations Assistee par Ordinateur2000 (RIAO?2000), pages 62?77, College de France,France.ClueWeb09.
2009.
The ClueWeb09 dataset.
http://lemurproject.org/clueweb09/.Aidan Finn, Nicholas Kushmerick, and Barry Smyth.2002.
Genre classification and domain transfer forinformation filtering.
In Proceedings of the 24th Eu-ropean Conference on Information Retrieval (ECIR2002), pages 353?362, Glasgow, UK.Freedict.
2011.
Freedict dictionaries.
http://www.freedict.com.Pascale Fung.
1998.
A statistical view on bilin-gual lexicon extraction: From parallel corpora tonon-parallel corpora.
In Proceedings of Associa-tion for Machine Translation in the Americas (AMTA1998): Machine Translation and the InformationSoup, pages 1?17, Langhorne, USA.Indri.
2009.
Indri search engine.
http://www.lemurproject.org/indri/.David Kamholz and Jonathan Pool.
to appear.
PanLex:Building a resource for panlingual lexical transla-tion.
In Proceedings of the 9th International Confer-ence on Language Resources and Evaluation (LREC2014), Reykjavik, Iceland.Batia Laufer and Linor Hadar.
1997.
Assessing theeffectiveness of monolingual, bilingual, and ?bilin-gualised?
dictionaries in the comprehension and pro-duction of new words.
The Modern Language Jour-nal, 81(2):189?196.Marco Lui, Jey Han Lau, and Timothy Baldwin.
2014.Automatic detection and language identification ofmultilingual documents.
Transactions of the Associ-ation for Computational Linguistics, 2(Feb):27?40.Katsushi Matsuda and Toshikazu Fukushima.
1999.Task-oriented world wide web retrieval by documenttype classification.
In Proceedings of the 1999 ACMConference on Information and Knowledge Man-agement (CIKM 1999), pages 109?113, Kansas City,USA.MeCab.
2011. http://mecab.googlecode.com.I.
Dan Melamed.
1996.
Automatic construction ofclean broad-coverage translation lexicons.
In Pro-ceedings of the 2nd Conference of the Associationfor Machine Translation in the Americas (AMTA1996), Montreal, Canada.Dragos Stefan Munteanu and Daniel Marcu.
2005.
Im-proving machine translation performance by exploit-ing non-parallel corpora.
Computational Linguis-tics, 31(4):477?504.Masaaki Nagata, Teruka Saito, and Kenji Suzuki.2001.
Using the web as a bilingual dictionary.
InProceedings of the ACL 2001 Workshop on Data-driven Methods in Machine Translation, pages 1?8,Toulouse, France.Jian-Yun Nie.
2010.
Cross-language informationretrieval.
Morgan and Claypool Publishers, SanRafael, USA.John M. Prager.
1999.
Linguini: language identifi-cation for multilingual documents.
In Proceedingsthe 32nd Annual Hawaii International Conferenceon Systems Sciences (HICSS-32), Maui, USA.Philip Resnik and Noah A. Smith.
2003.
The webas a parallel corpus.
Computational Linguistics,29(3):349?380.Stephen Soderland, Christopher Lim, Mausam,Bo Qin, Oren Etzioni, and Jonathan Pool.
2009.Lemmatic machine translation.
In Proceedingsof the Twelfth Machine Translation Summit (MTSummit XII), Ottawa, Canada.Nicholas Thieberger and Andrea L. Berez.
2012.
Lin-guistic data management.
In Nicholas Thieberger,editor, The Oxford Handbook of Linguistic Field-work.
Oxford University Press, Oxford, UK.Huihsin Tseng, Pichuan Chang, Galen Andrew, DanielJurafsky, and Christopher Manning.
2005.
A condi-tional random field word segmenter for sighan bake-off 2005.
In Proceedings of the Fourth SIGHANWorkshop on Chinese Language Processing, volume171.Hiroshi Yamaguchi and Kumiko Tanaka-Ishii.
2012.Text segmentation by language using minimum de-scription length.
In Proceedings the 50th AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 969?978,Jeju Island, Korea.Kun Yu and Junichi Tsujii.
2009.
Bilingual dictio-nary extraction from Wikipedia.
In Proceedings ofthe Twelfth Machine Translation Summit (MT Sum-mit XII), pages 379?386, Ottawa, Canada.Sven Meyer zu Eissen and Benno Stein.
2005.
Genreclassification of web pages.
In Proceedings of the27th Annual German Conference in AI (KI 2005),pages 256?269, Ulm, Germany.98
