Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 468?478,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsImproving Web Search Ranking by Incorporating StructuredAnnotation of Queries*Xiao Ding1, Zhicheng Dou2, Bing Qin1, Ting Liu1, Ji-Rong Wen31Research Center for Social Computing and Information RetrievalHarbin Institute of Technology, China2Microsoft Research Asia, Beijing 100190, China3Renmin University of China, Beijing, China1{xding, qinb, tliu}@ir.hit.edu.cn;2zhichdou@microsoft.com; 3jirong.wen@gmail.comAbstract?Web users are increasingly looking forstructured data, such as lyrics, job, or recipes,using unstructured queries on the web.However, retrieving relevant results from suchdata is a challenging problem due to theunstructured language of the web queries.
Inthis paper, we propose a method to improveweb search ranking by detecting StructuredAnnotation of queries based on top searchresults.
In a structured annotation, the originalquery is split into different units that areassociated with semantic attributes in thecorresponding domain.
We evaluate ourtechniques using real world queries and achievesignificant improvement.1 IntroductionSearch engines are getting more sophisticated byutilizing information from multiple diverse sources.One such valuable source of information isstructured and semi-structured data, which is notvery difficult to access, owing to informationextraction (Wong et al 2009; Etzioni et al 2008;Zhai and Liu 2006) and semantic web efforts.?
*Work was done when the first author was visiting MicrosoftResearch AsiaDriving the web search evolution are the userneeds.
Users usually have a template in mind whenformulating queries to search for information.Agarwal et al (2010) surveyed a search log of 15million queries from a commercial search engine.They found that 90% of queries follow certaintemplates.
For example, by issuing the query?taylor swift lyrics falling in love?, the users areactually seeking for the lyrics of the song ?Mary'sSong (oh my my my)?
by artist Taylor Swift.
Thewords ?falling in love?
are actually part of thelyrics they are searching for.
However, some topsearch results are irrelevant to the query, althoughthey contain all the query terms.
For example, thefirst top search result shown in Figure 1(a) doesnot contain the required lyrics.
It just contains thelyrics of another song of Taylor Swift, rather thanthe song that users are seeking.A possible way to solve the above rankingproblem is to understand the underlying querystructure.
For example, after recognizing that?taylor swift?
is an artist name and ?falling in love?are part of the lyrics, we can improve the rankingby comparing the structured query with thecorresponding structured data in documents(shown in Figure 1(b)).
Some previous studiesinvestigated how to extract structured informationfrom user queries, such as query segmentation(Bergsma and Wang, 2007).
The task of querysegmentation is to separate the query words into468disjointed segments so that each segment maps to asemantic unit (Li et al 2011).
For example, thesegmentation of the query ?taylor swift lyricsfalling in love?
can be ?taylor swift | lyrics | fallingin love?.
Since query segmentation cannot tell?talylor swift?
is an artist name and ?falling in love?are part of lyrics, it is still difficult for us to judgewhether each part of the query segmentationsmatches the right field of the documents or not(such as judge whether ?talylor swift?
matches theartist name in the document).
Recently, a lot ofwork (Sarkas et al 2010; Li et al 2009) proposedthe task of structured annotation of queries whichaims to detect the structure of the query and assigna specific label to it.
However, to our knowledge,the previous methods do not exploit an effectiveapproach for improving web search ranking byincorporating structured annotation of queries.In this paper, we investigate the possibility ofusing structured annotation of queries to improveweb search ranking.
Specifically, we propose agreedy algorithm which uses the structured data(named annotated tokens in Figure 1(b)) extractedfrom the top search results to annotate the latentstructured semantics in web queries.
We thencompute matching scores between the annotatedquery and the corresponding structuredinformation contained in documents.
The topsearch results can be re-ranked according to thematching scores.
However, it is very difficult toextract structured data from all of the search results.Hence, we propose a relevance feedback based re-ranking model.
We use these structured documentswhose matching scores are greater than a thresholdas feedback documents, to effectively re-rank othersearch results to bring more relevant and novelinformation to the user.Experiments on a large web search dataset froma major commercial search engine show that the F-Measure of structured annotation generated by ourapproach is as high as 91%.
On this dataset, our re-ranking model using the structured annotationssignificantly outperforms two baselines.The main contributions of our work include:1.
We propose a novel approach to generatestructured annotation of queries based on topsearch results.2.
Although structured annotation of queries hasbeen studied previously, to the best of ourknowledge this is the first paper that attemptsto improve web search ranking byincorporating structured annotation of queries.The rest of this paper is organized as follows.We briefly introduce related work in Section 2.Section 3 presents our method for generatingstructured annotation of queries.
We then proposetwo novel re-ranking models based on structuredannotation in Section 4.
Section 5 introduces thedata used in this paper.
We report experimentalresults in Section 6.
Finally we conclude the workin Section 7.Figure 1.
Search results of query ?taylor swift lyrics falling in love?
and processing pipeline[Taylor Swift, #artist_name, 0.34]...[Mary?s Song (oh my my my), #song_name, 0.16][Crazier, #song_name, 0.1][Jump Then Fall, #song_name, 0.08]...[Growing up and falling in love?, #lyrics, 0.16][Feel like I?m falling and ?, #lyrics, 0.1][I realize your love is the best ?, #lyrics, 0.08]d1 [Taylor Swift, #artist_name][Crazier, #song_name][Feel like I?m falling and ?, #lyrics]d2 [Taylor Swift, #artist_name][Mary?s Song (oh my my my), #song_name][Growing up and falling in love?, #lyrics]d3 [Taylor Swift, #artist_name][Jump Then Fall, #song_name][I realize your love is the best ?, #lyrics]d4 [Taylor Swift, #artist_name][Mary?s Song (oh my my my), #song_name][Growing up and falling in love?, #lyrics]Search Results (a)Weighted Annotated Tokens (c)Query Structured Annotation Generation (d)Top Results Re-ranking (e)Annotated Tokens (b)1.2.3.4.Query: taylor swift lyrics falling in love<[taylor swift, #artist_name] lyrics[falling in love, #lyrics]>1.2.3.4.1.2.3.4.4692 Related WorkThere is a great deal of prior research thatidentifies query structured information.
Wesummarize this research according to theirdifferent approaches.2.1 Structured Annotation of QueriesRecently, a lot of work has been done onunderstanding query structure (Sarkas et al 2010;Li et al 2009; Bendersky et al 2010).
Oneimportant method is structured annotation ofqueries which aims to detect the structure of thequery and assign a specific label to it.
Li et al(2009) proposed web query tagging and its goal isto assign to each query term a specified category,roughly corresponding to a list of attributes.
Asemi-supervised Conditional Random Field (CRF)is used to capture dependencies between querywords and to identify the most likely jointassignment of words to ?categories.?
Comparingwith previous work, the advantages of ourapproach are on the following aspects.
First, wegenerate structured annotation of queries based ontop search results, not some global knowledge baseor query logs.
Second, they mainly focus on themethod of generating structured annotation ofqueries, rather than leverage the generated querystructures to improve web search rankings.
In thispaper, we not only offer a novel solution forgenerating structured annotation of queries, butalso propose a re-ranking approach to improveWeb search based on structured annotation ofqueries.
Bendersky et al (2011) also used topsearch results to generate structured annotation ofqueries.
However, the annotations in theirdefinition are capitalization, POS tags, andsegmentation indicators, which are different fromours.2.2 Query Template GenerationThe concept of query template has been discussedin a few recent papers (Agarwal et al 2010; Pasca2011; Liu et al 2011; Szpektor et al 2011).
Aquery template is a sequence of terms, where eachterm could be a word or an attribute.
For example,<#artist_name lyrics #lyrics> is a query template,?#artist_name?
and ?#lyrics?
are attributes, and?lyrics?
is a word.
Structured annotation of queriesis different from query template, as a querytemplate can instantiate multiple queries while astructured annotation only serves for a specificquery.
Unlike query template, our work is ranking-oriented.
We aim to automatically annotate querystructure based on top search results, and furtheruse these structured annotations to re-rank topsearch results for improving search performance.2.3 Query SegmentationThe task of query segmentation is to separate thequery words into disjointed segments so that eachsegment maps to a semantic unit (Li et al 2011).Query segmentation techniques have been wellstudied in recent literature (Tan and Peng, 2008;Yu and Shi, 2009).
However, structured annotationof queries cannot only separate the query wordsinto disjoint segments but can also assign eachsegment a semantic label which can help the searchengine to judge whether each part of querysegmentation matches the right field of thedocuments or not.2.4 Entity SearchThe problem of entity search has received a greatdeal of attention in recent years (Guo et al 2009;Bron et al 2010; Cheng et al 2007).
Its goal is toanswer information needs that focus on entities.The problem of structured annotation of queries isrelated to entity search because for some queries,structured annotation items are entities or attributes.Some existing entity search approaches alsoexploit knowledge from the structure of webpages(Zhao et al 2005).
Annotating query structuredinformation differs from entity search in thefollowing aspects.
First, structured annotationbased ranking is applicable for all queries, ratherthan just entity related queries.
Second, the resultof an entity search is usually a list of entities, theirattributes, and associated homepages, whereas ourwork uses the structured information fromwebpages to annotate query structured informationand further leverage structured annotation ofqueries to re-rank top search results.Table 1.
Example domain schemasDomain Schema Example structured annotationslyrics #artist_name#song_name#lyrics<lyrics of [hey jude, #song_name] [beatles,#artist_name]>job #category#location<[teacher, #category] job in [America,#location]>recipe  #directions#ingredients<[baking, # directions] [bread, #ingredients] recipe>4703 Structured Annotation of Queries3.1 Problem DefinitionWe start our discussion by defining some basicconcepts.
A token is defined as a sequence ofwords including space, i.e., one or more words.
Forexample, the bigram ?taylor swift?
can be a singletoken.
As our objective is to find structuredannotation of queries in a specific domain, webegin with a definition of domain schema.Definition 1 (Domain Schema): For a givendomain of interest, the domain schema is the set ofattributes.
We denote the domain schema as ?
={?1, ?2,?
, ??
}, where each ??
is the name of anattribute of the domain.
Sample domain schemasare shown in Table 1.
In contrast to previousmethods (Agarwal et al 2010), our definition ofdomain schema does not need attribute values.
Forthe sake of simplicity, this paper assumes thatattributes in domain schema are available.However, it is not difficult to pre-specify attributesin a specific domain.Definition 2 (Annotated Token): An annotatedtoken in a specific domain is a pair [?, ?
], where vis a token and a is a corresponding attribute for vin this domain.
[hey jude, #song_name] is anexample of an annotated token for the ?lyrics?domain shown in Table 1.
The words ?hey jude?comprise a token, and its corresponding attributename is #song_name.
If a token does not have anycorresponding attributes, we denote it as free token.Definition 3 (Structured Annotation): Astructured annotation p is a sequence of terms <?1,?2,?,??
>, where each ??
could be a free token oran annotated token, and at least one of the terms isan annotated token, i.e., ??
?
[1, ?]
for which ??
isan annotated token.Given the schema for the domain ?lyrics?,<[taylor swift, #artist_name] lyrics [falling in love,#lyrics]> is a possible structured annotation for thequery ?taylor swift lyrics falling in love?.
In thisannotation, [taylor swift, #artist_name] and[falling in love, #lyrics] are two annotated tokens.The word ?lyrics?
is a free token.Intuitively, a structured annotation correspondsto an interpretation of the query as a request forsome structured information from documents.
Theset of annotated tokens expresses the informationneed of the documents that have been requested.The free tokens may provide more diverseinformation.
Annotated tokens and free tokenstogether cover all query terms, reflecting thecomplete user intent of the query.3.2 Generating Structured AnnotationIn this paper, given a domain schema A, wegenerate structured annotation for a query q basedon the top search results of q.
We propose usingtop search results, rather than some globalknowledge base or query logs, because:(1) Top search results have been proven to bea successful technique for query explanation(Bendersky et al 2010).
(2) We have observed that in most cases, areasonable percentage of the top search results arerelevant to the query.
By aggregating structuredinformation from the top search results, we can getmore query-dependent annotated tokens than usingglobal data sources which may contain more noiseand outdated.
(3) Our goal for generating structuredannotation is to improve the ranking quality ofqueries.
Using top search results enablessimultaneous and consistent detection of structuredinformation from documents and queries.As mentioned in Section 3.1, we generatestructured annotation of queries based on annotatedtokens, which are actually structured data (shownin Figure 1(b)) embedded in web documents.
Inthis paper, we assume that the annotated tokens areAlgorithm 1: Query Structured Annotation GenerationInput: a list of weighted annotated tokens T = {t1, ?
, tm} ;a query q = ?w1, ?
, wn?
where wi ?
W;a pre-defined threshold score ?.Output: a query structured annotation p = <s1, ?
, sk>.1: Set p = q = {s1, ?, sn}, where si = wi2: for u = 1 to T.size do3:       compute ?????
(?, ??
)= ?????
(?, ??.
?
)= ??.
?
?
???0??<??????(???
, ??.
?
),where pij = si,?,sj, s.t.
sl ?
W for l ?
[i, j].
//pij is justin the remaining query words4: end for5: find the maximum matching tu with????
= ??????1?????????
(?, ??
)6: if ?????
(?, ????)
> ?
then7:      replace si,?,sj in p with [si,?,sj, tmax.a ]8:      remove tmax from T9:      n ?
n ?
(j - i)10:      go to step 211: else12:      return p13: end if471available and we mainly focus on how to use theseannotated tokens from top search results togenerate structured annotation of queries.
Theapproach is comprised of two parts, one forweighting annotated tokens and the other forgenerating structured annotation of queries basedon the weighted annotated tokens.Weighting: As shown in Figure 1, annotatedtokens extracted from top results may beinconsistent, and hence some of the extractedannotated tokens are less useful or even useless forgenerating structured annotation.We assume that a better annotated token shouldbe supported by more top results; while a worseannotated token may appear in fewer results.Hence we aggregate all the annotated tokensextracted from top search results, and evaluate theimportance of each unique one by a ranking-awarevoting model as follows.
For an annotated token [v,a], its weight w is defined as:?
=1??
??1????
(1)where wj is a voting from document dj, and??
= {?
?
?
+ 1?,             if [?, ?]
?
?
?0,                      elseHere, N is the number of top search results and jis the ranking position of document dj.
We thengenerate a weighted annotated token [v, a, w] foreach original unique token [v, a].Generating: The process by which we map aquery q to Structured Annotation is shown inAlgorithm 1.
The algorithm takes as input a list ofweighted annotated tokens and the query q, andoutputs the structured annotation of the query q.The algorithm first partitions the query q bycomparing each sub-sequence of the query with allthe weighted annotated tokens, and find themaximum matching annotated token (line 1 to line5).
Then, if the degree of match is greater than thethreshold ?
which is a pre-defined threshold scorefor fuzzy string matching, the query substring willbe assigned the attribute label of the maximummatching annotated token (line 6 to line 8).
Thealgorithm stops when all the weighted annotatedtokens have been scanned, and outputs thestructured annotation of the query.Note that in some cases, the query may fail toexactly match with the annotated tokens, due tospelling errors, acronyms or abbreviations in users?queries.
For example, in the query ?broken andbeatuful lyrics?, ?broken and beatuful?
is amisspelling of ?broken and beautiful.?
We adopt afuzzy string matching function for comparing asub-sequence string s with a token v:???
(?, ?)
= 1 ?????????????(?,?
)max (|?|,|?|)(2)where EditDistance(s, v) measures the editdistances of two strings, |s| is the length of string sand |v| is the length of string v.4 Ranking with Structured AnnotationGiven a domain schema ?
= {?1, ?2, ?
, ??
}, and aquery q, suppose that ?
= < ?1,?2,?,??
>  is thestructured annotation for query q obtained usingthe method introduced in the above sections.
p canbetter reflects the user?s real search intent than theoriginal q, as it presents the structured semanticinformation needed instead of a simple word string.Therefore, a document di can better satisfy a user?sinformation need if it contains correspondingstructured semantic information in p. Suppose thatTi is the set of annotated tokens extracted fromdocument di, we compute a re-ranking score,denoted by RScore, for document di as follows:RScore(q, di) = ?????
(?, ??
)= ?????
(?, ??
)= ?
?
?????(??
, ?)????1????where?????(??
, ?
)= {???(??
.
??
, ?.
?
),        if ??
.
??
= ?.
?0,                                else(3)where ??
is an annotated token in p and t is anannotated token in di.
We use Equation (2) tocompute the similarity between values in queryannotated tokens and values in document annotatedtokens.
We propose two re-ranking models,namely the conservative re-ranking model, to re-rank top results based on RScore and relevancefeedback based re-ranking model.4.1 Conservative Re-ranking ModelA nature way to re-rank top search results isaccording to their RScore.
However, we fail toobtain annotated tokens from some retrieveddocuments, and hence the RScore of thesedocuments are not available.
In the conservativere-ranking model, we only re-rank search resultsthat have an RScore.
For example, suppose thereare five retrieved documents {d1, d2, d3, d4, d5} forquery q, we can extract structured informationfrom document d3 and d4 and RScore(q, d4) >RScore(q, d3).
Note that we cannot obtain472structured information from d1, d2, and d5.
In theconservative re-ranking method, d1, d2, and d5retain their original positions; while d3 and d4 willbe re-ranked according to their RScore.
Therefore,the final ranking generated by our conservative re-ranking model should be {d1, d2, d4, d3, d5}, inwhich the documents are re-ranked among theaffected positions.There is also useful information in thedocuments without structured data, such ascommunity question answering websites.
However,in the conservative re-ranking model they will notbe re-ranked.
This may hurt the performance of ourre-ranking model.
One reasonable solution isrelevance feedback model.4.2 Relevance Feedback based Re-rankingModelThe disadvantage of the conservative re-rankingmodel is that it only can re-rank those top searchresults with structured data.
To make up itslimitation, we propose a relevance feedback basedre-ranking model.
The key idea of this model isbased on the observation that the search resultswith the corrected annotated tokens could giveimplicit feedback information.
Hence, we use thesestructured documents whose RScore are greaterthan a threshold ?
(empirically set it as 0.6) asfeedback documents, to effectively re-rank othersearch results to bring more relevant and novelinformation to the user.Formally, given a query Q and a documentcollection C, a retrieval system returns a ranked listof documents D. Let di denote the i-th rankeddocument in the ranked list.
Our goal is to studyhow to use these feedback documents, J ?
{d1,?,dk}, to effectively re-rank the other r search results:U ?
{dk+1,?, dk+r}.
A general formula of relevancefeedback model (Salton et al1990) R is as follows:?(??)
= (1 ?
?)??
(Q) + ???
(J)             (4)where ?
?
[0, 1] is the feedback coefficient, and ?
?and ??
are two models that map a query and a setof relevant documents, respectively, into somecomparable representations.
For example, they canbe represented as vectors of weighted terms orlanguage models.In this paper, we explore the problem in thelanguage model framework, particularly the KL-divergence retrieval model and mixture-modelfeedback method (Zhai and Lafferty, 2001), mainlybecause language models deliver state-of-the-artretrieval performance and the mixture-model basedfeedback is one of the most effective feedbacktechniques which outperforms Rocchio feedback.4.2.1 The KL-Divergence Retrieval ModelThe KL-divergence retrieval model was introducedin Lafferty and Zhai, (2001) as a special case of therisk minimization retrieval framework and cansupport feedback more naturally.
In this model,queries and documents are represented by unigramlanguage models.
Assuming that these languagemodels can be appropriately estimated, KL-divergence retrieval model measures the relevancevalue of a document D with respect to a query Qby computing the negative Kullback-Leiblerdivergence between the query language model ?
?and the document language model ??
as follows:?
(?, ?)
= ??(??||??)
= ??
?(?|??)????(?|??)?(?|??)???
(5)where V is the set of words in our vocabulary.Intuitively, the retrieval performance of the KL-divergence relies on the estimation of thedocument model ??
and the query model ?
?.For the set of k relevant documents, thedocument model ??
is estimated as ?(w|??)
=1???(?,??)|??|?
?=1 , where ?
(?, ??)
is the count of wordw in the i-th relevant document, and |?
?| is the totalnumber of words in that document.
The documentmodel ??
needs to be smoothed and an effectivemethod is Dirichlet smoothing (Zhai et al 2001).The query model intuitively captures what theuser is interested in, and thus would affect retrievalperformance.
With feedback documents, ??
isestimated by the mixture-model feedback method.4.2.2 The Mixture Model Feedback MethodAs the problem definition in Equation (4), thequery model can be estimated by the original querymodel ?(?|??)
=?(?,?
)|?|(where c(w,Q) is the countof word w in the query Q, and |Q| is the totalnumber of words in the query) and the feedbackdocument model.
Zhai and Lafferty, (2001)proposed a mixture model feedback method toestimate the feedback document model.
Morespecifically, the model assumes that the feedbackdocuments can be generated by a backgroundlanguage model ?(?|?)
estimated using the wholecollection and an unknown topic language model473??
to be estimated.
Formally, let F ?
C be a set offeedback documents.
In this paper, F is comprisedof documents that RScore are greater than?.
Thelog-likelihood function of the mixture model is:???(?|??)
=?
?
?
(?, ?)???
log [(1 ?
?)?(?|??)
+ ??(?|?)]???
(6)where ?
?
[0,1)  is a mixture noise parameterwhich controls the weight of the backgroundmodel.
Given a fixed ?, a standard EM algorithmcan then be used to estimate ?(?|??
), which isthen interpolated with the original query model?
(?|Q) to obtain an improved estimation of thequery model:?(?|??)
= (1 ?
?)?(?|?)
+ ??(?|??)
(7)where ?
is the feedback coefficient.5 DataWe used a dataset composed of 12,396 queriesrandomly sampled from query logs of a searchengine.
For each query, we retrieved its top 100results from a commercial search engine.
Thedocuments were judged by human editors.
A five-grade (from 0 to 4 meaning from bad to perfect)relevance rating was assigned for each document.We used a proprietary query domain classifier toidentify queries in three domains, namely ?lyrics,??recipe,?
and ?job,?
from the dataset.
The statisticsabout these domains are shown in Table 2.
Toinvestigate how many queries may potentially havestructured annotations, we manually createdstructured annotations for these queries.
The lastcolumn of Table 2 shows the percentage of queriesthat have structured annotations created byannotators.
We found that for each domain, therewas on average more than 90% of queriesidentified by us that had a certain structuredannotation.
This indicates that a large percentageof these queries contain structured information, aswe expected.6 Experimental ResultsIn this section, we present the structured annotationof queries and further re-rank the top search resultsfor the three domains introduced in Section 5.
Weused the ranking returned by a commercial searchengine as our one of the Baselines.
Note that as thebaseline already uses a large number of rankingsignals, it is very difficult to improve it any further.We evaluate the ranking quality using the widelyused Normalized Discounted Cumulative Gainmeasure (NDCG) (Javelin and Kekalainen., 2000).We use the same configuration for NDCG as(Burges et al2005).
More specifically, for a givenquery q, the NDCG@K is computed as:??
=1???
(2?(?)?1)?
?=1log (1 + ?
)(4)Mq is a normalization constant (the ideal NDCG)so that a perfect ordering would obtain an NDCGof 1; and r(j) is the rating score of the j-thdocument in the ranking list.6.1 Overall Results6.1.1 Quality of Structured Annotation ofQueriesWe generated the structured annotation of queriesbased on the top 10 search results and used ?
=0.04  for Algorithm 1.
We used several existingmetrics, P (Precision), R (Recall), and F-Measureto evaluate the quality of the structured annotation.As a query structured annotation may contain morethan one annotated token, we concluded that theFigure 2.
Ranking Quality (* indicates significantimprovement)0.540.550.560.570.580.590.60.610.62NDCG@1 NDCG@3 NDCG@5ValueofmeasurementMeasurementSeg-Ranker Ori-Ranker Con-Ranker FB-Ranker******Table 3.
Quality of Structured Annotation.
All theimprovements are significant (p < 0.05)Domain Method Precision Recall F-Measurelyrics BaselineOur90.06%95.45%84.92%89.83%87.41%92.55%job BaselineOur89.62%95.31%80.14%84.93%84.62%89.82%recipe BaselineOur83.96%89.68%84.23%88.44%84.09%89.06%All BaselineOur87.88%93.61%83.10%88.45%85.42%90.96%Table 2.
Domain queries used in our experimentDomain ContainingKeywordQueriesStructuredAnnotation%lyrics ?lyrics?
196 95%job ?job?
124 92%recipe ?recipe?
76   93%474annotation was correct only if the entire annotationwas completely the same as the annotation labeledby annotators.
Otherwise we treated the structuredannotation as incorrect.
Experimental results forthe three domains are shown in Table 3.
Wecompare our approach with Xiao Li, (2010)(denoted as baseline), on the dataset described inSection 5.
They labeled the semantic structure ofnoun phrase queries based on semi-Markov CRFs.Our approach achieves better performance than thebaseline (about 5.5% significant improvement onF-Measure).
This indicates that the approach ofgenerating structured annotation based on the topsearch results is more effective.
With the high-quality structured annotation of queries in hand, itmay be possible to obtain better ranking resultsusing our proposed re-ranking models.6.1.2 Re-ranking ResultWe used the models introduced in Section 4 to re-rank the top 10 search results, based on structuredannotation of queries and annotated tokens.Recall that our goal is to quantify theeffectiveness of structured annotation of queriesfor real web search.
One dimension is to comparewith the original search results of a commercialsearch engine (denoted as Ori-Ranker).
The otheris to compare with the query segmentation basedre-ranking model (denoted as Seg-Ranker; Li etal., 2011) which tries to improve web searchranking by incorporating query segmentation.
Li etal., (2011) incorporated query segmentation in theBM25, unigram language model and bigramlanguage model retrieval framework, and bigramlanguage model achieved the best performance.
Inthis paper, Seg-Ranker integrates bigram languagemodel with query segmentation.The ranking results of these models are shownin Figure 2.
This figure shows that all our tworankers significantly outperform the Ori-Ranker?the original search results of a commercial searchengine.
This means that using high-qualitystructured annotation does help betterunderstanding of user intent.
By comparing thesestructured annotations and the annotated tokens indocuments, we can re-rank the more relevantresults higher and yield better ranking quality.Figure 2 also suggests that structured annotationbased re-ranking models outperform querysegmentation based re-ranking model.
This ismainly because structured annotation can not onlyseparate the query words into disjoint segments butcan also assign each segment a semantic label.Taking full advantage of the semantic label canlead to better ranking performance.Furthermore, Figure 2 shows that FB-Rankeroutperforms Con-Ranker.
The main reason is thatin Con-Ranker, we can only reasonably re-rank thesearch results with structured data.
However, inFB-Ranker we can not only re-rank the structuredsearch results but also can re-rank other documentsby incorporating implicit information from thosestructured documents.On average, FB-Ranker achieves the bestranking performance.
Table 4 shows more detailedTable 4.
Detailed ranking results on three domains.All the improvements are significant (p < 0.05)Domain Ranking Method NDCG@1 NDCG@3 NDCG@5lyrics Seg-Ranker 0.572 0.574 0.575Ori-RankerFB-Ranker0.6210.6370.6280.6390.6360.647recipe Seg-Ranker 0.629 0.631 0.634Ori-RankerFB-Ranker0.6780.7070.6870.7040.6960.709job Seg-Ranker 0.438 0.413 0.408Ori-RankerFB-Ranker0.4700.5040.4530.4740.4420.45900.10.20.30.40.50.60.70.80.910 0.02 0.04 0.06 0.08 0.1 0.3 0.5 0.7 0.9ValueofmeasurementQuery structured annotation generation threshold ?PrecisionRecallF-Measure0.540.550.560.570.580.590.60.610 0.02 0.
4 .06 0.08 .1 0.3 0.5 0.7 0.9 perfectNDCG@3Que tr ctured annotation generation threshold ?Seg-Ranker Ori-Ranker FB-Ranker(a) Quality of re-ranking                                    (b) Quality of query structured annotationFigure 3.
Quality of re-ranking and quality of query structured annotation with different number of search results475results for the three selected domains.
This tableshows that FB-Ranker consistently outperforms thetwo baseline rankers on these domains.
In theremaining part of this paper, we will only reportthe results for this ranker, due to space limitations.Table 4 also indicates that we can get robustranking improvement in different domains, and wewill consider applying it to more domains.6.2 Experiment with Different Thresholds ofQuery Structured Annotation AlgorithmAs introduced in Algorithm 1, we pre-defined athreshold ?
for fuzzy string matching.
Weevaluated the quality of re-ranking and querystructured annotation with different settings for ?.The results are shown in Figure 3.
We found that:(1) When we use ?
= 0, which means that thestructured annotations can be generated no matterhow small the similarity between the query stringand a weighted annotated token is, we can get asignificant NDCG@3 gain of 2.15%.
Figure 3(b)shows that the precision of the structuredannotation is lowest when ?
= 0 .
However, theprecision is still as high as 0.7375, and the highestrecall is obtained in this case.
This means that thequality of the generated structured annotations isstill reasonable, and hence we can get a rankingimprovement when ?
= 0, as shown in Figure 3(a).
(2) Figure 3(a) suggests that the quality of re-ranking increases when the threshold ?
increasesfrom 0 to 0.05.
It then decreases when ?
increasesfrom 0.06 to 0.5.
Comparing these two figuresshows that the trend of re-ranking performanceadheres to the quality of the structured annotation.The settings for ?
dramatically affect the recall andprecision of the structured annotation; and hencethe ranking quality is impacted.
The larger ?
is, thelower the recall of the structured annotation is.
(3) Since the re-ranking performancedramatically changes along with the quality of thestructured annotation, we conducted a re-rankingexperiment with perfect structured annotations (F-Measure equal to 1.0).
Perfect structuredannotations mean the annotations created byannotators as introduced in Section 5.
The resultsare shown in the last bar of Figure 3(a).
We did notfind a large space for ranking improvement.
TheNDCG@3 when using perfect structuredannotations was 0.606, which is just slightly betterthan our best result (yield when ?=0.05).
Itindicates that our structured annotation generationalgorithm is already quite effective.
(4) Figure 3(a) shows that our approachoutperforms the two baseline approaches with mostsettings for ?.
This indicates that our approach isrelatively stable with different settings for ?.6.3 Experiment with Number of Top SearchResultsThe above experiments are conducted based on thetop 10 search results.
In this section, by adjustingthe number of top search results, ranging from 2 to100, we investigate whether the quality ofstructured annotation of queries and theperformance of re-ranking are affected by thequantity of search results.
The results shown inFigure 4 indicate that the number of search resultsdoes affect the quality of structured annotation ofqueries and the performance of re-ranking.Structured annotations of queries become betterwhen more search results are used from 2 to 20.This is because more search results cover morewebsites in our domain list, and hence can generatemore annotated tokens.
More results also providemore evidence for voting the importance of00.10.20.30.40.50.60.70.80.912 3 4 5 6 7 8 9 10 20 30 40 50 60 70 80 90 100VaulueofmeasurementNumber of search resultsPrecisionRecallF-Measure0.540.550.560.570.580.590.60.612 3 4 5 6 7 8 9 10 0 30 4 50 60 70 80 90 100NDCG@3Number of search resultSeg-Ranker Ori-Ranker FB-Ranker(a) Quality of re-ranking                                   (b) Quality of query structured annotationFigure 4.
Quality of re-ranking and quality of query structured annotation with different number of search results476annotated tokens, and hence can improve thequality of structured annotation of queries.In addition, we also found that structuredannotation of queries become worse when toomany lower ranked results are used (e.g, usingresults ranked lower than 20).
This is because thelower ranked results are less relevant than thehigher ranked results.
They may contain moreirrelevant or noisy annotated tokens than higherranked documents; and hence using them mayharm the precision of the structured annotations.Figure 4 also indicates that the quality of rankingand the accuracy of structured annotations arecorrelated.7 ConclusionsIn this paper, we studied the problem of improvingweb search ranking by incorporating structuredannotation of queries.
We proposed a systematicsolution, first to generate structured annotation ofqueries based on top search results, and thenlaunching two structured annotation based re-ranking models.
We performed a large-scaleevaluation over 12,396 queries from a major searchengine.
The experiment results show that the F-Measure of query structured annotation generatedby our approach is as high as 91%.
In the samedataset, our structured annotation based re-rankingmodel significantly outperforms the original ranker?
the ranking of a major search engine, withimprovements 5.2%.AcknowledgmentsThis work was supported by National Natural ScienceFoundation of China (NSFC) via grant 61273321,61133012 and the Nation-al 863 Leading TechnologyResearch Project via grant 2012AA011102.ReferencesG.
Agarwal, G. Kabra, and K. C.-C. Chang.
Towardsrich query interpretation: walking back and forth formining query templates.
In Proc.
of WWW '10.M.
Bendersky, W. Bruce Croft and D. A. Smith.
JointAnnotation of Search Queries, In Proc.
of ACL-HLT2011.M.
Bendersky, W. Bruce Croft and D. A. Smith.Structural Annotation of Search Queries UsingPseudo-Relevance Feedback, In Proc.
Of CIKM 2010.S.
Bergsma and Q. I. Wang.
Learning noun phrasequery segmentation.
In Proceedings of EMNLP-CoNLL'07.M.
Bron, K. Balog, and M. de Rijke.
Ranking relatedentities: components and analyses.
In Proc.
ofCIKM ?10.C.
Buckley.
Automatic query expansion using SMART.InProc.
of TREC-3, pages 69?80, 1995.C.
Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds,N.
Hamilton, and G. Hullender.
Learning to rankusinggradient descent.
In Proceedings of ICML '05.T.
Cheng, X. Yan, and K. C.-C. Chang.
Supportingentity search: a large-scale prototype search engine.In Proc.
of SIGMOD ?07.O.
Etzioni, M. Banko, S. Soderland, and D.S.
Weld,(2008).
Open Information Extraction from the Web,Communications of the ACM, 51(12): 68-74.J.
Guo, G. Xu, X. Cheng, and H. Li.
Named entityrecognition in query.
In Proc.
Of SIGIR?
2009.K.
Jarvelin and J. Kekalainen.
Ir evaluation methods forretrieving highly relevant documents.
In SIGIR '00.J.
Lafferty and C. Zhai, Document language models,query models, and risk minimization for informationretrieval, In Proceedings of SIGIR'01, pages 111-119,2001.V.
Lavrenko and W. B. Croft.
Relevance basedlanguage models.
In Proc.
of SIGIR, pages 120?127,2001.Y.
Li, BJP.
Hsu, CX.
Zhai and K. Wang.
UnsupervisedQuery Segmentation Using Clickthrough forInformation Retrieval.
In Proc.
of SIGIR'11.X.
Li, Y.-Y.
Wang, and A. Acero.
Extracting structuredinformation from user queries with semi-supervisedconditional random fields.
In Proc.
of SIGIR'09.Y.
Liu, X. Ni, J-T. Sun, Z. Chen.
UnsupervisedTransactional Query Classification Based onWebpage Form Understanding.
In Proc.
of CIKM '11.Y.
Liu, M. Zhang, L. Ru, and S. Ma.
Automatic querytype identification based on click-throughinformation.
In LNCS, 2006.M.
Pasca.
Asking What No One Has Asked Before:Using Phrase Similarities To Generate SyntheticWeb Search Queries.
In Proc.
of CIKM '11.G.
Salton and C. Buckley.
Improving retrievalperformance by relevance feedback.
Journal of theAmerican Society for Information Science,41(4):288-297, 1990.477N.
Sarkas, S. Paparizos, and P. Tsaparas.
Structuredannotations of web queries.
In Proc.
of SIGMOD'10.I.
Szpektor, A. Gionis, and Y. Maarek.
Improvingrecommendation for long-tail queries via templates.In Proc.
of WWW '11B.
Tan and F. Peng.
Unsupervised query segmentationusing generative language models and wikipedia.
InWWW?08.T.-L. Wong, W. Lam, and B. Chen.
Miningemployment market via text block detection andadaptive cross-domain information extraction.
InProc.
SIGIR, pages 283?290, 2009.X.
Yu and H. Shi.
Query segmentation usingconditional random fields.
In Proceedings of KEYS'09.C.
Zhai and J. Lafferty, Model-based feedback in thelanguage modeling approach to informationretrieval , In Proceedings of CIKM'01, pages 403-410,2001.C.
Zhai and J. Lafferty, A study of smoothing methodsfor language models applied to ad hoc informationretrieval, In Proceedings of SIGIR'01, pages 334-342,2001.Y.
Zhai and B. Liu.
Structured data extraction from theWeb based on partial tree alignment.
IEEE Trans.Knowl.
Data Eng., 18(12):1614?1628, Dec. 2006.H.
Zhao, W. Meng, Z. Wu, V. Raghavan, and C. Yu.Fully automatic wrapper generation for searchengines.
In Proceedings of WWW ?05.S.
Zheng, R. Song, J.-R. Wen, and D. Wu.
Jointoptimization of wrapper generation and templatedetection.
In Proc.
of SIGKDD'07.478
