-q~NIIIIl1IIIIIIIII!I!ABSTRACTION IS HARMFUL IN LANGUAGE LEARNINGWalter DaelemansDILK (Induction of Linguistic Knowledge)Computational LinguisticsTilburg University, The NetherlandsandCNTS (Center for Dutch Language and Speech)LinguisticsUniversity of Antwerp, BelgiumWalter.
Dael emans@kub, nl1.
AbstractThe usual approach to learning language processingtasks such as tagging, parsing, grapheme-to-phonemeconversion, pp-attachrnent, e c., is to extract regularitiesfrom training data in the form of decision trees, rules,probabilities or other abstractions.
These representationsof regularities are then used to solve new cases of thetask.
The individual training examples on which theabstractions were based are discarded (forgotten).
Whilethis approach seems to work well for other applicationareas of Machine Learning, I will show that here is evi-dence that it is not the best way to learn language pro-cessing tasks.I will briefly review empirical work in our groups inAntwerp and Tilburg on lazy language l arning.
In thisapproach (also called, instance-based, case-based, mem-ory-based, and example-based l arning), generalizationhappens at processing time by means of extrapolationfrom the most similar items in memory to the new itembeing processed.
Lazy Learning with a simple similaritymetric based on information entropy (IB I-IG, Daele-marts & van den Bosch, 1992, 1997) consistently out-performs abstracting ( reedy) learning techniques suchas C5.0 or backprop learning on a broad selection of nat-ural language processing tasks ranging from phonologyto semantics.
Our intuitive xplanation for this result isthat lazy learning techniques keep all training items,whereas greedy approaches lose useful information byforgetting low-frequency orexceptional instances of thetask, not covered by the extracted rules or models(Daelemans, 1996).
Apart from the empirical work inTilburg and Antwerp, a number of recent studies on sta-tistical natural language processing (e.g.
Dagan & Lee,1997; Collins & Brooks, 1995) also suggest that, con-trary to common wisdom, forgetting specific trainingitems, even when they represent extremely low-fre-quency events, is harmful to generalization accuracy.After reviewing this empirical work briefly, I willreport on new results (work in progress in collaborationwith van den Bosch and Zavrel), systematically compar-ing greedy and lazy learning techniques on a number ofbenehrnark natural language processing tasks: tagging,grapheme-to-phoneme conversion, and pp-attachment.The results how that forgetting individual trainingitems, however "improbable' they may be, is indeedharmful.
Furthermore, they show that combining lazylearning with training set editing techniques (based ontypicality and other egularity criteria) also leads toworse generalization results.I will conclude that forgetting, either by abstractingfrom the training data or by editing exceptional trainingitems in lazy learning is ha_rm~ to generalization accu-racy, and will attempt to provide an explanation forthese unexpected results.2.
ReferencesCollins, M. and J. Brooks.
"Prepositional Phrase Attach-ment hrough aBacked-off Model.
Proceedings ThirdWorkshop on Very Large Corpora, MIT, 1995.Daelemans, W. and A. van den Bosch.
'GeneralizationPerformance of Backpropagation Learning on a Syl-labification Task.'
In: M.EJ.
Drossaers and A.
Nijholt(eds.)
Cormectionism and Natural Language Process-ing.
Proceedings Third Twente Workshop on Lan-guage Technology, 27-38, 1992.Daelemans, W., Van den Bosch, A., & Weijters, A.
"IGTree: Using Trees for Compression a d Classifica-tion in Lazy Learning Algorithms.'
Artificial Intelli-gence Review 11,407-423, 1997.Daelemans, W. 'Abstraction Considered Harmful: LazyLearning of Language Processing.'
In: van den Herik,J.
and T. Weijters (eds.)
Beneleam-96.
Proceedings ofthe 6th Belgian-Dutch Conference on Machine Learn-ing.
MATRIKS: Maastricht, The Netherlands, 3-12,1996.Dagan, I., L. Lee, F. Pereira. '
Similarity-Based methodsfor Word Sense Disarnbiguation.'
Proceedings 35thACL - 8th EACL, Madrid, 1997.Daelemans 1 Abstraction Harmful in Language LearningWalter Daelemans (1998) Abstraction isHarmful in Language Learning.
In D.M.W.
Powers (ed.)
NeMLaP3/CoNLL98: NewMethods in Language Processing and Computational Natural Language Learning, ACL, pp 1-1.mmmmmmmmmmmmmmm
