Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 154?157,Sydney, July 2006. c?2006 Association for Computational LinguisticsChinese word segmentation and named entity recognition basedon a context-dependent Mutual Information Independence ModelZhang Min    Zhou GuoDong    Yang LingPeng    Ji DongHongInstitute for Infocomm Research21 Heng Mui Keng TerraceSingapore, 119613Email: (mzhang, zhougd, lpyang, dhji)@i2r.a-star.edu.sgAbstractThis paper briefly describes our system in thethird SIGHAN bakeoff on Chinese wordsegmentation and named entity recognition.This is done via a word chunking strategyusing a context-dependent MutualInformation Independence Model.Evaluation shows that our system performswell on all the word segmentation closedtracks and achieves very good scalabilityacross different corpora.
It also shows thatthe use of the same strategy in named entityrecognition shows promising performancegiven the fact that we only spend less thanthree days in total on extending the system inword segmentation to incorporate namedentity recognition, including training andformal testing.1  IntroductionWord segmentation and named entity recognitionaim at recognizing the implicit word boundariesand proper nouns, such as names of persons,locations and organizations, respectively in plainChinese text, and are critical in Chineseinformation processing.
However, there exist twoproblems when developing a practical wordsegmentation or named entity recognition systemfor large open applications, i.e.
the resolution ofambiguous segmentations and the identificationof OOV words or OOV entity names.In order to resolve above problems, wedeveloped a purely statistical Chinese wordsegmentation system and a named entityrecognition system using a three-stage strategyunder an unified framework.The first stage is called known wordsegmentation, which aims to segment an inputsequence of Chinese characters into a sequence ofknown words (called word atoms in this paper).
Inthis paper, all Chinese characters are regarded asknown words and a word unigram model isapplied to perform this task for efficiency.
Also,for convenience, all the English characters aretransformed into the Chinese counterparts inpreprocessing, which will be recovered justbefore outputting results.The second stage is the word and/or namedentity identification and classification on thesequence of atomic words in the first step.
Here, aword chunking strategy is applied to detect wordsand/or entity names by chunking one or moreatomic words together according to the wordformation patterns of the word atoms and optionalentity name formation patterns for named entityrecognition.
The problem of word segmentationand/or entity name recognition are re-cast aschunking one or more word atoms together toform a new word and/or entity name, and adiscriminative Markov model, named MutualInformation Independence Model (MIIM), isadopted in chunking.
Besides, a SVM plussigmoid model is applied to integrate varioustypes of contexts and implement thediscriminative modeling in MIIM.The third step is post processing, which triesto further resolve ambiguous segmentations andunknown word segmentation.
Due to time limit,this is only done in Chinese word segmentation.No post processing is done on Chinese namedentity recognition.The rest of this paper is as follows: Section 2describes the context-dependent MutualInformation Independence Model in details whilepurely statistical post-processing in Chinese wordsegmentation is presented in Section 3.
Finally,we report the results of our system in Chineseword segmentation and named entity recognitionin Section 4 and conclude our work in Section 5.1542 Mutual Information IndependenceModelIn this paper, we use a discriminative Markovmodel, called Mutual Information IndependenceModel (MIIM) as proposed by Zhou et al(2002),for Chinese word segmentation and named entityrecognition.
MIIM is derived from a conditionalprobability model.
Given an observation sequencenn oooO L211 = , MIIM finds a stochastic optimalstate(tag) sequence nn sssS L211 =  thatmaximizes:??==?
+=nininiiinn OsPSsPMIOSP1121111 )|(log),()|(logWe call the above model the MutualInformation Independence Model due to itsPair-wise Mutual Information (PMI) assumption(Zhou et al2002).
The above model consists oftwo sub-models: the state transition model?=?niii SsPMI211 ),( , which can be computed byapplying ngram modeling, and the output model?=nini OsP11 )|(log , which can be estimated by anyprobability-based classifier, such as a maximumentropy classifier or a SVM plus sigmoidclassifier (Zhou et al2006).
In this competition,the SVM plus sigmoid classifier is used inChinese word segmentation while a simplebackoff  approach as described in Zhou et al(2002) is used in named entity recognition.Here, a variant of the Viterbi algorithm(Viterbi 1967) in decoding the standard HiddenMarkov Model (HMM) (Rabiner 1989) isimplemented to find the most likely statesequence by replacing the state transition modeland the output model of the standard HMM withthe state transition model and the output model ofthe MIIM, respectively.
The above MIIM hasbeen successfully applied in many applications,such as text chunking (Zhou 2004), Chinese wordsegmentation ( Zhou 2005), English named entityrecognition in the newswire domain (Zhou et al2002) and the biomedical domain (Zhou et al2004; Zhou et al2006).For Chinese word segmentation and namedentity recognition by chunking, a word or a entityname is regarded as a chunk of one or more wordatoms and we have:?
>=< iii wpo , ; iw is the thi ?
word atom inthe sequence of word atoms nn wwwW L211 = ;ip  is the word formation pattern of the wordatom iw .
Here ip  measures the wordformation power of the word atom iw  andconsists of:o The percentage of iw  occurring as a wholeword (round to 10%)o The percentage of iw  occurring at thebeginning of other words (round to 10%)o The percentage of iw  occurring at the endof other words (round to 10%)o The length of iwo Especially for named entity recognition,the percentages of a word occurring indifferent entity types (round to 10%).?
is : the states are used to bracket anddifferentiate various types of words andoptional entity types for named entityrecognition.
In this way, Chinese wordsegmentation and named entity recognitioncan be regarded as a bracketing andclassification process.
is  is structural andconsists of two parts:o Boundary category (B): it includes fourvalues: {O, B, M, E}, where O means thatcurrent word atom is a whOle word orentity name and B/M/E means that currentword atom is at the Beginning/in theMiddle/at the End of a word or entity name.o Unit category (W): It is used to denote thetype of the word or entity name.Because of the limited number of boundaryand unit categories, the current word atomformation pattern ip  described above is addedinto the state transition model in MIIM.
Thismakes the above MIIM context dependent asfollows:??==??
+=nininiiiiinnOsPppSsPMIOSP11211111)|(log)|,()|(log3 Post Processing in WordSegmentationThe third step is post processing, which tries toresolve ambiguous segmentations and falseunknown word generation raised in the secondstep.
Due to time limit, this is only done inChinese word segmentation, i.e.
no postprocessing is done on Chinese named entityrecognition.155A simple pattern-based method is employed tocapture context information to correct thesegmentation errors generated in the second steps.The pattern is designed as follows:<Ambiguous Entry (AE)> | <Left Context,Right Context> => <Proper Segmentation>The ambiguity entry (AE) means ambiguoussegmentations or forced-generated unknownwords.
We use the 1st and 2nd words before AE asthe left context and the 1st and 2nd words after AEas the right context.
To reduce sparseness, we alsoonly use the 1st left and right words as context.This means that there are two patterns generatedfor the same context.
All the patterns areautomatically learned from training corpus usingthe following algorithm.LearningPatterns()// Input: training corpus// Output: patternsBEGIN(1) Training a MIIM model using trainingcorpus(2) Using the MIIM model to segment trainingcorpus(3) Aligning the training corpus with thesegmented training corpus(4) Extracting error segmentations(5) Generating disambiguation patterns usingthe left and right context(6) Removing the conflicting entries if twopatterns have the same left hand side butdifferent right hand side.END4 EvaluationWe first develop our system using the PKU datareleased in the Second SIGHAN Bakeoff lastyear.
Then, we train and evaluate it on the ThirdSIGHAN Bakeoff corpora without anyfine-tuning.
We only carry out our evaluation onthe closed tracks.
It means that we do not use anyadditional knowledge beyond the training corpus.Precision (P), Recall (R), F-measure (F), OOVRecall and IV Recall are adopted to measure theperformance of word segmentation.
Accuracy(A), Precision (P), Recall (R) and F-measure (F)are adopted to measure the performance of NER.Tables 1, 2 and 3 in the next page report theperformance of our algorithm on different corpusin the SIGHAN Bakeoff 02 and Bakeoff 03,respectively.
For the performance of othersystems, please refer tohttp://sighan.cs.uchicago.edu/bakeoff2005/data/results.php.htm for the Chinese bakeoff 2005 andhttp://sighan.cs.uchicago.edu/bakeoff2006/longstats.html for the Chinese bakeoff 2006.Comparison against other systems shows thatour system achieves the state-of-the-artperformance on all Chinese word segmentationclosed tracks and shows good scalability acrossdifferent corpora.
The small performance gapshould be able to overcome by replacing the wordunigram model with the more powerful wordbigram model.
Due to very limited time of lessthan three days, although our NER system underthe unified framework as Chinese wordsegmentation does not achieve thestate-of-the-art, its performance in NER is quitepromising and provides a good platform forfurther improvement.
Error analysis reveals thatOOV is still an open problem that is far from toresolve.
In addition, different corpus definesdifferent segmentation principles.
This will stressOOV handling in the extreme.
Therefore a systemtrained on one genre usually performances worsewhen faced with text from a different register.5 ConclusionThis paper proposes a purely unified statisticalthree-stage strategy in Chinese wordsegmentation and named entity recognition,which are based on a context-dependent MutualInformation Independence Model.
Evaluationshows that our system achieves thestates-of-the-art segmentation performance andprovides a good platform for further performanceimprovement of Chinese NER.ReferencesRabiner L. 1989.
A Tutorial on Hidden MarkovModels and Selected Applications in SpeechRecognition.
IEEE 77(2), pages257-285.Viterbi A.J.
1967.
Error Bounds forConvolutional Codes and an AsymptoticallyOptimum Decoding Algorithm.
IEEETransactions on Information Theory, IT 13(2),260-269.Zhou GuoDong and Su Jain.
2002.
Named EntityRecognition Using a HMM-based ChunkTagger, Proceedings of the 40th Annual Meetingof the Association for ComputationalLinguistics  (ACL?2002).
Philadelphia.
July2002.
pp473-480.156Zhou GuoDong, Zhang Jie, Su Jian, Shen Dan andTan ChewLim.
2004.
Recognizing Names inBiomedical Texts: a Machine LearningApproach.
Bioinformatics.
20(7): 1178-1190.DOI: 10.1093/bioinformatics/bth060.
2004.ISSN: 1460-2059Zhou GuoDong.
2004.
Discriminative hiddenMarkov modeling with long state dependenceusing a kNN ensemble.
Proceedings of 20thInternational Conference on ComputationalLinguistics (COLING?2004).
23-27 Aug, 2004,Geneva, Switzerland.Zhou GuoDong.
2005.
A chunking strategytowards unknown word detection in Chineseword segmentation.
Proceedings of 2ndInternational Joint Conference on NaturalLanguage Processing (IJCNLP?2005), LectureNotes in Computer Science (LNCS 3651)Zhou GuoDong.
2006.
Recognizing names inbiomedical texts using Mutual InformationIndependence Model and SVM plus Sigmod.International Journal of Medical Informatics(Article in Press).
ISSN 1386-5056TablesTask P R F OOV Recall IV RecallCityU 0.9 38 0.952 94.5 0.578 0.967MSRA 0.952 0.962 95.7 0.51 0.98CKIP 0.94 0.957 94.8 0.502 0.976PKU 0.952 0.952 95.2 0.71 0.967Table 1: Performance of Word Segmentation on Closed Tracks in the SIGHAN Bakeoff 02Task P R F OOV Recall IV RecallCityU 0.968 0.961 96.5 0.633 0.983MSRA 0.961 0.953 95.7 0.499 0.977CKIP 0.958 0.941 94.9 0.554 0.976UPUC 0.936 0.917 92.6 0.617 0.966Table 2: Performance of Word Segmentation on Closed Tracks in the SIGHAN Bakeoff 03Task A P R FMSRA 0.9743 0.8150 0.7882 79.92CityU 0.9725 0.8466 0.8061 82.59Table 3: Performance of NER on Closed Tracks in the SIGHAN Bakeoff 03157
