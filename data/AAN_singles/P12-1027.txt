Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 253?262,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsFast Online Training with Frequency-Adaptive Learning Rates for ChineseWord Segmentation and New Word DetectionXu Sun?, Houfeng Wang?, Wenjie Li?
?Department of Computing, The Hong Kong Polytechnic University?Key Laboratory of Computational Linguistics (Peking University), Ministry of Education, China{csxsun, cswjli}@comp.polyu.edu.hk wanghf@pku.edu.cnAbstractWe present a joint model for Chineseword segmentation and new word detection.We present high dimensional new features,including word-based features and enrichededge (label-transition) features, for the jointmodeling.
As we know, training a wordsegmentation system on large-scale datasetsis already costly.
In our case, adding highdimensional new features will further slowdown the training speed.
To solve thisproblem, we propose a new training method,adaptive online gradient descent based onfeature frequency information, for very fastonline training of the parameters, even givenlarge-scale datasets with high dimensionalfeatures.
Compared with existing trainingmethods, our training method is an ordermagnitude faster in terms of training time, andcan achieve equal or even higher accuracies.The proposed fast training method is a generalpurpose optimization method, and it is notlimited in the specific task discussed in thispaper.1 IntroductionSince Chinese sentences are written as continuoussequences of characters, segmenting a charactersequence into words is normally the first stepin the pipeline of Chinese text processing.
Themajor problem of Chinese word segmentationis the ambiguity.
Chinese character sequencesare normally ambiguous, and new words (out-of-vocabulary words) are a major source of theambiguity.
A typical category of new wordsis named entities, including organization names,person names, location names, and so on.In this paper, we present high dimensionalnew features, including word-based features andenriched edge (label-transition) features, for thejoint modeling of Chinese word segmentation(CWS) and new word detection (NWD).
While mostof the state-of-the-art CWS systems used semi-Markov conditional random fields or latent variableconditional random fields, we simply use a singlefirst-order conditional random fields (CRFs) forthe joint modeling.
The semi-Markov CRFs andlatent variable CRFs relax the Markov assumptionof CRFs to express more complicated dependencies,and therefore to achieve higher disambiguationpower.
Alternatively, our plan is not to relaxMarkov assumption of CRFs, but to exploit morecomplicated dependencies via using refined high-dimensional features.
The advantage of our choiceis the simplicity of our model.
As a result, ourCWS model can be more efficient compared withthe heavier systems, and with similar or even higheraccuracy because of using refined features.As we know, training a word segmentation systemon large-scale datasets is already costly.
In ourcase, adding high dimensional new features willfurther slow down the training speed.
To solve thischallenging problem, we propose a new trainingmethod, adaptive online gradient descent based onfeature frequency information (ADF), for very fastword segmentation with new word detection, evengiven large-scale datasets with high dimensionalfeatures.
In the proposed training method, we tryto use more refined learning rates.
Instead of usinga single learning rate (a scalar) for all weights,we extend the learning rate scalar to a learningrate vector based on feature frequency informationin the updating.
By doing so, each weight has253its own learning rate adapted on feature frequencyinformation.
We will show that this can significantlyimprove the convergence speed of online learning.We approximate the learning rate vector basedon feature frequency information in the updatingprocess.
Our proposal is based on the intuitionthat a feature with higher frequency in the trainingprocess should be with a learning rate that is decayedfaster.
Based on this intuition, we will show theformalized training algorithm later.
We will show inexperiments that our solution is an order magnitudefaster compared with exiting learning methods, andcan achieve equal or even higher accuracies.The contribution of this work is as follows:?
We propose a general purpose fast onlinetraining method, ADF.
The proposed trainingmethod requires only a few passes to completethe training.?
We propose a joint model for Chinese wordsegmentation and new word detection.?
Compared with prior work, our systemachieves better accuracies on both wordsegmentation and new word detection.2 Related WorkFirst, we review related work on word segmentationand new word detection.
Then, we review popularonline training methods, in particular stochasticgradient descent (SGD).2.1 Word Segmentation and New WordDetectionConventional approaches to Chinese wordsegmentation treat the problem as a sequentiallabeling task (Xue, 2003; Peng et al, 2004; Tsenget al, 2005; Asahara et al, 2005; Zhao et al,2010).
To achieve high accuracy, most of the state-of-the-art systems are heavy probabilistic systemsusing semi-Markov assumptions or latent variables(Andrew, 2006; Sun et al, 2009b).
For example,one of the state-of-the-art CWS system is the latentvariable conditional random field (Sun et al, 2008;Sun and Tsujii, 2009) system presented in Sun et al(2009b).
It is a heavy probabilistic model and it isslow in training.
A few other state-of-the-art CWSsystems are using semi-Markov perceptron methodsor voting systems based on multiple semi-Markovperceptron segmenters (Zhang and Clark, 2007;Sun, 2010).
Those semi-Markov perceptron systemsare moderately faster than the heavy probabilisticsystems using semi-Markov conditional randomfields or latent variable conditional random fields.However, a disadvantage of the perceptron stylesystems is that they can not provide probabilisticinformation.On the other hand, new word detection is also oneof the important problems in Chinese informationprocessing.
Many statistical approaches have beenproposed (J. Nie and Jin, 1995; Chen and Bai, 1998;Wu and Jiang, 2000; Peng et al, 2004; Chen andMa, 2002; Zhou, 2005; Goh et al, 2003; Fu andLuke, 2004; Wu et al, 2011).
New word detectionis normally considered as a separate process fromsegmentation.
There were studies trying to solve thisproblem jointly with CWS.
However, the currentstudies are limited.
Integrating the two tasks wouldbenefit both segmentation and new word detection.Our method provides a convenient framework fordoing this.
Our new word detection is not a stand-alone process, but an integral part of segmentation.2.2 Online TrainingThe most representative online training methodis the SGD method.
The SGD uses a smallrandomly-selected subset of the training samples toapproximate the gradient of an objective function.The number of training samples used for thisapproximation is called the batch size.
By using asmaller batch size, one can update the parametersmore frequently and speed up the convergence.
Theextreme case is a batch size of 1, and it gives themaximum frequency of updates, which we adopt inthis work.
Then, the model parameters are updatedin such a way:wt+1 = wt + ?t?wtLstoch(z i,wt), (1)where t is the update counter, ?t is the learning rate,and Lstoch(z i,wt) is the stochastic loss functionbased on a training sample z i.There were accelerated versions of SGD,including stochastic meta descent (Vishwanathanet al, 2006) and periodic step-size adaptationonline learning (Hsu et al, 2009).
Compared withthose two methods, our proposal is fundamentally254different.
Those two methods are using 2nd-ordergradient (Hessian) information for acceleratedtraining, while our accelerated training methoddoes not need such 2nd-order gradient information,which is costly and complicated.
Our ADF trainingmethod is based on feature frequency adaptation,and there is no prior work on using feature frequencyinformation for accelerating online training.Other online training methods includes averagedSGD with feedback (Sun et al, 2010; Sun et al,2011), latent variable perceptron training (Sun et al,2009a), and so on.
Those methods are less related tothis paper.3 System Architecture3.1 A Joint Model Based on CRFsFirst, we briefly review CRFs.
CRFs are proposedas a method for structured classification by solving?the label bias problem?
(Lafferty et al, 2001).Assuming a feature function that maps a pair ofobservation sequence x and label sequence y to aglobal feature vector f , the probability of a labelsequence y conditioned on the observation sequencex is modeled as follows (Lafferty et al, 2001):P (y|x,w) =exp{w?f (y,x)}??y?
exp{w?f (y?
,x)} , (2)wherew is a parameter vector.Given a training set consisting of n labeledsequences, z i = (xi, y i), for i = 1 .
.
.
n, parameterestimation is performed by maximizing the objectivefunction,L(w) =n?i=1logP (y i|xi,w)?R(w).
(3)The first term of this equation represents aconditional log-likelihood of a training data.
Thesecond term is a regularizer for reducing overfitting.We employed an L2 prior, R(w) = ||w||22?2 .
In whatfollows, we denote the conditional log-likelihood ofeach sample logP (y i|xi,w) as ?
(z i,w).
The finalobjective function is as follows:L(w) =n?i=1?
(z i,w)?||w||22?2 .
(4)Since no word list can be complete, new wordidentification is an important task in Chinese NLP.New words in input text are often incorrectlysegmented into single-character or other very shortwords (Chen and Bai, 1998).
This phenomenonwill also undermine the performance of Chineseword segmentation.
We consider here new worddetection as an integral part of segmentation,aiming to improve both segmentation and new worddetection: detected new words are added to theword list lexicon in order to improve segmentation.Based on our CRF word segmentation system,we can compute a probability for each segment.When we find some word segments are of reliableprobabilities yet they are not in the existing wordlist, we then treat those ?confident?
word segmentsas new words and add them into the existing wordlist.
Based on preliminary experiments, we treata word segment as a new word if its probabilityis larger than 0.5.
Newly detected words are re-incorporated into word segmentation for improvingsegmentation accuracies.3.2 New FeaturesHere, we will describe high dimensional newfeatures for the system.3.2.1 Word-based FeaturesThere are two ideas in deriving the refinedfeatures.
The first idea is to exploit word featuresfor node features of CRFs.
Note that, although ourmodel is a Markov CRFmodel, we can still use wordfeatures to learn word information in the trainingdata.
To derive word features, first of all, our systemautomatically collect a list of word unigrams andbigrams from the training data.
To avoid overfitting,we only collect the word unigrams and bigramswhose frequency is larger than 2 in the training set.This list of word unigrams and bigrams are then usedas a unigram-dictionary and a bigram-dictionary togenerate word-based unigram and bigram features.The word-based features are indicator functions thatfire when the local character sequence matches aword unigram or bigram occurred in the trainingdata.
The word-based feature templates derived forthe label yi are as follows:?
unigram1(x, yi) ?
[xj,i, yi], if thecharacter sequence xj,i matches a word w ?
U,255with the constraint i ?
6 < j < i.
The itemxj,i represents the character sequence xj .
.
.
xi.U represents the unigram-dictionary collectedfrom the training data.?
unigram2(x, yi) ?
[xi,k, yi], if thecharacter sequence xi,k matches a wordw ?
U,with the constraint i < k < i + 6.?
bigram1(x, yi) ?
[xj,i?1, xi,k, yi], ifthe word bigram candidate [xj,i?1, xi,k] hitsa word bigram [wi, wj ] ?
B, and satisfiesthe aforementioned constraints on j and k. Brepresents the word bigram dictionary collectedfrom the training data.?
bigram2(x, yi) ?
[xj,i, xi+1,k, yi], ifthe word bigram candidate [xj,i, xi+1,k] hits aword bigram [wi, wj ] ?
B, and satisfies theaforementioned constraints on j and k.We also employ the traditional character-basedfeatures.
For each label yi, we use the featuretemplates as follows:?
Character unigrams locating at positions i?
2,i?
1, i, i + 1 and i + 2?
Character bigrams locating at positions i ?2, i?
1, i and i + 1?
Whether xj and xj+1 are identical, for j = i?2, .
.
.
, i + 1?
Whether xj and xj+2 are identical, for j = i?3, .
.
.
, i + 1The latter two feature templates are designedto detect character or word reduplication, amorphological phenomenon that can influence wordsegmentation in Chinese.3.2.2 High Dimensional Edge FeaturesThe node features discussed above are based ona single label yi.
CRFs also have edge featuresthat are based on label transitions.
The second ideais to incorporate local observation information ofx in edge features.
For traditional implementationof CRF systems (e.g., the HCRF package), usuallythe edges features contain only the informationof yi?1 and yi, and without the information ofthe observation sequence (i.e., x).
The majorreason for this simple realization of edge featuresin traditional CRF implementation is for reducingthe dimension of features.
Otherwise, there canbe an explosion of edge features in some tasks.For example, in part-of-speech tagging tasks, therecan be more than 40 labels and more than 1,600types of label transitions.
Therefore, incorporatinglocal observation information into the edge featurewill result in an explosion of edge features, whichis 1,600 times larger than the number of featuretemplates.Fortunately, for our task, the label set is quitesmall, Y = {B,I,E}1.
There are only nine possiblelabel transitions: T = Y ?
Y and |T| = 9.2 Asa result, the feature dimension will have nine timesincrease over the feature templates, if we incorporatelocal observation information of x into the edgefeatures.
In this way, we can effectively combineobservation information of x with label transitionsyi?1yi.
We simply used the same templates ofnode features for deriving the new edge features.We found adding new edge features significantlyimproves the disambiguation power of our model.4 Adaptive Online Gradient Descent basedon Feature Frequency InformationAs we will show in experiments, the training of theCRF model with high-dimensional new features isquite expensive, and the existing training method isnot good enough.
To solve this issue, we propose afast online training method: adaptive online gradientdescent based on feature frequency information(ADF).
The proposed method is easy to implement.For high convergence speed of online learning, wetry to use more refined learning rates than the SGDtraining.
Instead of using a single learning rate (ascalar) for all weights, we extend the learning ratescalar to a learning rate vector, which has the samedimension of the weight vector w. The learningrate vector is automatically adapted based on featurefrequency information.
By doing so, each weight1B means beginning of a word, I means inside a word, andE means end of a word.
The B,I,E labels have been widelyused in previous work of Chinese word segmentation (Sun etal., 2009b).2The operator ?
means a Cartesian product between twosets.256ADF learning algorithm1: procedure ADF(q, c, ?, ?
)2: w ?
0, t?
0, v ?
0, ?
?
c3: repeat until convergence4: .
Draw a sample z i at random5: .
v ?
UPDATE(v , z i)6: .
if t > 0 and t mod q = 07: .
.
?
?
UPDATE(?
, v)8: .
.
v ?
09: .
g ?
?wLstoch(z i,w)10: .
w ?
w + ?
?
g11: .
t?
t + 112: returnw13:14: procedure UPDATE(v , z i)15: for k ?
features used in sample z i16: .
vk ?
vk + 117: return v18:19: procedure UPDATE(?
, v)20: for k ?
all features21: .
u?
vk/q22: .
?
?
??
u(??
?
)23: .
?k ?
?
?k24: return ?Figure 1: The proposed ADF online learning algorithm.q, c, ?, and ?
are hyper-parameters.
q is an integerrepresenting window size.
c is for initializing the learningrates.
?
and ?
are the upper and lower bounds of a scalar,with 0 < ?
< ?
< 1.has its own learning rate, and we will show that thiscan significantly improve the convergence speed ofonline learning.In our proposed online learning method, theupdate formula is as follows:wt+1 = wt + ?
t ?
gt.
(5)The update term gt is the gradient term of arandomly sampled instance:gt = ?wtLstoch(z i,wt) = ?wt{?
(z i,wt)?||wt||22n?2}.In addition, ?
t ?
Rf+ is a positive vector-valued learning rate and ?
denotes component-wise(Hadamard) product of two vectors.We learn the learning rate vector ?
t basedon feature frequency information in the updatingprocess.
Our proposal is based on the intuition that afeature with higher frequency in the training processshould be with a learning rate that decays faster.
Inother words, we assume a high frequency featureobserved in the training process should have a smalllearning rate, and a low frequency feature shouldhave a relatively larger learning rate in the training.Our assumption is based on the intuition that aweight with higher frequency is more adequatelytrained, hence smaller learning rate is preferable forfast convergence.Given a window size q (number of samples in awindow), we use a vector v to record the featurefrequency.
The k?th entry vk corresponds to thefrequency of the feature k in this window.
Givena feature k, we use u to record the normalizedfrequency:u = vk/q.For each feature, an adaptation factor ?
is calculatedbased on the normalized frequency information, asfollows:?
= ??
u(??
?
),where ?
and ?
are the upper and lower bounds ofa scalar, with 0 < ?
< ?
< 1.
As we can see,a feature with higher frequency corresponds to asmaller scalar via linear approximation.
Finally, thelearning rate is updated as follows:?k ?
?
?k.With this setting, different features will correspondto different adaptation factors based on featurefrequency information.
Our ADF algorithm issummarized in Figure 1.The ADF training method is efficient, becausethe additional computation (compared with SGD) isonly the derivation of the learning rates, which issimple and efficient.
As we know, the regularizationof SGD can perform efficiently via the optimizationbased on sparse features (Shalev-Shwartz et al,2007).
Similarly, the derivation of ?
t can alsoperform efficiently via the optimization based onsparse features.4.1 Convergence AnalysisPrior work on convergence analysis of existingonline learning algorithms (Murata, 1998; Hsu et257Data Method Passes Train-Time (sec) NWD Rec Pre Rec CWS F-scoreMSR Baseline 50 4.7e3 72.6 96.3 95.9 96.1+ New features 50 1.2e4 75.3 97.2 97.0 97.1+ New word detection 50 1.2e4 78.2 97.5 96.9 97.2+ ADF training 10 2.3e3 77.5 97.6 97.2 97.4CU Baseline 50 2.9e3 68.5 94.0 93.9 93.9+ New features 50 7.5e3 68.0 94.4 94.5 94.4+ New word detection 50 7.5e3 68.8 94.8 94.5 94.7+ ADF training 10 1.5e3 68.8 94.8 94.7 94.8PKU Baseline 50 2.2e3 77.2 95.0 94.0 94.5+ New features 50 5.2e3 78.4 95.5 94.9 95.2+ New word detection 50 5.2e3 79.1 95.8 94.9 95.3+ ADF training 10 1.2e3 78.4 95.8 94.9 95.4Table 2: Incremental evaluations, by incrementally adding new features (word features and high dimensional edgefeatures), new word detection, and ADF training (replacing SGD training with ADF training).
Number of passes isdecided by empirical convergence of the training methods.#W.T.
#Word #C.T.
#CharMSR 8.8?
104 2.4?
106 5?
103 4.1?
106CU 6.9?
104 1.5?
106 5?
103 2.4?
106PKU 5.5?
104 1.1?
106 5?
103 1.8?
106Table 1: Details of the datasets.
W.T.
represents wordtypes; C.T.
represents character types.al., 2009) can be extended to the proposed ADFtraining method.
We can show that the proposedADF learning algorithm has reasonable convergenceproperties.When we have the smallest learning rate ?
t+1 =??
t, the expectation of the obtainedwt isE(wt) = w?
+t?m=1(I ?
?0?mH (w?
))(w0 ?w?),wherew?
is the optimal weight vector, andH is theHessian matrix of the objective function.
The rate ofconvergence is governed by the largest eigenvalue ofthe functionC t =?tm=1(I ?
?0?mH (w?)).
Then,we can derive a bound of rate of convergence.Theorem 1 Assume ?
is the largest eigenvalue ofthe function C t =?tm=1(I ?
?0?mH (w?)).
Forthe proposed ADF training, its convergence rate isbounded by ?, and we have?
?
exp{?0???
?
1},where ?
is the minimum eigenvalue ofH (w?
).5 Experiments5.1 Data and MetricsWe used benchmark datasets provided by the secondInternational Chinese Word Segmentation Bakeoffto test our proposals.
The datasets are fromMicrosoft Research Asia (MSR), City Universityof Hongkong (CU), and Peking University (PKU).Details of the corpora are listed in Table 1.
Wedid not use any extra resources such as commonsurnames, parts-of-speech, and semantics.Four metrics were used to evaluate segmentationresults: recall (R, the percentage of gold standardoutput words that are correctly segmented by thedecoder), precision (P , the percentage of words inthe decoder output that are segmented correctly),balanced F-score defined by 2PR/(P + R), andrecall of new word detection (NWD recall).
Formore detailed information on the corpora, refer toEmerson (2005).5.2 Features, Training, and TuningWe employed the feature templates defined inSection 3.2.
The feature sets are huge.
There are2.4 ?
107 features for the MSR data, 4.1 ?
107features for the CU data, and 4.7 ?
107 features forthe PKU data.
To generate word-based features, weextracted high-frequency word-based unigram andbigram lists from the training data.As for training, we performed gradient descent2580 10 20 30 40 509595.59696.59797.5MSRNumber of PassesF?scoreADFSGDLBFGS (batch)0 10 20 30 40 509292.59393.59494.595CUNumber of PassesF?score0 10 20 30 40 509494.59595.5PKUNumber of PassesF?score0 2000 4000 60009595.59696.59797.5MSRTraining time (sec)F?scoreADFSGDLBFGS (batch)0 1000 2000 3000 40009292.59393.59494.595CUTraining time (sec)F?score0 1000 2000 3000 40009494.59595.5PKUTraining time (sec)F?scoreFigure 2: F-score curves on the MSR, CU, and PKU datasets: ADF learning vs. SGD and LBFGS training methods.with our proposed training method.
To comparewith existing methods, we chose two populartraining methods, a batch training one and anonline training one.
The batch training methodis the Limited-Memory BFGS (LBFGS) method(Nocedal and Wright, 1999).
The online baselinetraining method is the SGD method, which we haveintroduced in Section 2.2.For the ADF training method, we need to tune thehyper-parameters q, c, ?, and ?.
Based on automatictuning within the training data (validation in thetraining data), we found it is proper to set q = n/10(n is the number of training samples), c = 0.1,?
= 0.995, and ?
= 0.6.
To reduce overfitting,we employed an L2 Gaussian weight prior (Chenand Rosenfeld, 1999) for all training methods.
Wevaried the ?
with different values (e.g., 1.0, 2.0, and5.0), and finally set the value to 1.0 for all trainingmethods.5.3 Results and DiscussionFirst, we performed incremental evaluation in thisorder: Baseline (word segmentation model withSGD training); Baseline + New features; Baseline+ New features + New word detection; Baseline +New features + New word detection + ADF training(replacing SGD training).
The results are shown inTable 2.As we can see, the new features improvedperformance on both word segmentation and newword detection.
However, we also noticed thatthe training cost became more expensive viaadding high dimensional new features.
Addingnew word detection function further improved thesegmentation quality and the new word recognitionrecall.
Finally, by using the ADF training method,the training speed is much faster than the SGDtraining method.
The ADF method can achieveempirical optimum in only a few passes, yetwith better segmentation accuracies than the SGDtraining with 50 passes.To get more details of the proposed trainingmethod, we compared it with SGD and LBFGStraining methods based on an identical platform,by varying the number of passes.
The comparisonwas based on the same platform: Baseline + Newfeatures + New word detection.
The F-score curvesof the training methods are shown in Figure 2.Impressively, the ADF training method reachedempirical convergence in only a few passes, whilethe SGD and LBFGS training converged muchslower, requiring more than 50 passes.
The ADFtraining is about an order magnitude faster thanthe SGD online training and more than an ordermagnitude faster than the LBFGS batch training.Finally, we compared our method with the state-259Data Method Prob.
Pre Rec F-scoreMSR Best05 (Tseng et al, 2005)?96.2 96.6 96.4CRF + rule-system (Zhang et al, 2006)?97.2 96.9 97.1Semi-Markov perceptron (Zhang and Clark, 2007) ?
N/A N/A 97.2Semi-Markov CRF (Gao et al, 2007)?N/A N/A 97.2Latent-variable CRF (Sun et al, 2009b)?97.3 97.3 97.3Our method (A Single CRF)?97.6 97.2 97.4CU Best05 (Tseng et al, 2005)?94.1 94.6 94.3CRF + rule-system (Zhang et al, 2006)?95.2 94.9 95.1Semi-perceptron (Zhang and Clark, 2007) ?
N/A N/A 95.1Latent-variable CRF (Sun et al, 2009b)?94.7 94.4 94.6Our method (A Single CRF)?94.8 94.7 94.8PKU Best05 (Chen et al, 2005) N/A 95.3 94.6 95.0CRF + rule-system (Zhang et al, 2006)?94.7 95.5 95.1semi-perceptron (Zhang and Clark, 2007) ?
N/A N/A 94.5Latent-variable CRF (Sun et al, 2009b)?95.6 94.8 95.2Our method (A Single CRF)?95.8 94.9 95.4Table 3: Comparing our method with the state-of-the-art CWS systems.of-the-art systems reported in the previous papers.The statistics are listed in Table 3.
Best05 representsthe best system of the Second International ChineseWord Segmentation Bakeoff on the correspondingdata; CRF + rule-system represents confidence-based combination of CRF and rule-based models,presented in Zhang et al (2006).
Prob.
indicateswhether or not the system can provide probabilisticinformation.
As we can see, our method achievedsimilar or even higher F-scores, compared with thebest systems reported in previous papers.
Note that,our system is a single Markov model, while most ofthe state-of-the-art systems are complicated heavysystems, with model-combinations (e.g., voting ofmultiple segmenters), semi-Markov relaxations, orlatent-variables.6 Conclusions and Future WorkIn this paper, we presented a joint model forChinese word segmentation and newword detection.We presented new features, including word-basedfeatures and enriched edge features, for the jointmodeling.
We showed that the new features canimprove the performance on the two tasks.On the other hand, the training of the model,especially with high-dimensional new features,became quite expensive.
To solve this problem,we proposed a new training method, ADF training,for very fast training of CRFs, even given large-scale datasets with high dimensional features.
Weperformed experiments and showed that our newtraining method is an order magnitude faster thanexisting optimization methods.
Our final system canlearn highly accurate models with only a few passesin training.
The proposed fast learning methodis a general algorithm that is not limited in thisspecific task.
As future work, we plan to applythis fast learning method on other large-scale naturallanguage processing tasks.AcknowledgmentsWe thank Yaozhong Zhang and Weiwei Sunfor helpful discussions on word segmentationtechniques.
The work described in this paper wassupported by a Hong Kong RGC Project (No.
PolyU5230/08E), National High Technology Research andDevelopment Program of China (863 Program) (No.2012AA011101), and National Natural ScienceFoundation of China (No.91024009, No.60973053).ReferencesGalen Andrew.
2006.
A hybrid markov/semi-markovconditional random field for sequence segmentation.260In Proceedings of EMNLP?06, pages 465?472.Masayuki Asahara, Kenta Fukuoka, Ai Azuma, Chooi-Ling Goh, Yotaro Watanabe, Yuji Matsumoto, andTakahashi Tsuzuki.
2005.
Combination ofmachine learning methods for optimum chinese wordsegmentation.
In Proceedings of The Fourth SIGHANWorkshop, pages 134?137.K.J.
Chen and M.H.
Bai.
1998.
Unknown worddetection for chinese by a corpus-based learningmethod.
Computational Linguistics and ChineseLanguage Processing, 3(1):27?44.Keh-Jiann Chen and Wei-Yun Ma.
2002.
Unknown wordextraction for chinese documents.
In Proceedings ofCOLING?02.Stanley F. Chen and Ronald Rosenfeld.
1999.
Agaussian prior for smoothing maximum entropymodels.
Technical Report CMU-CS-99-108, CMU.Aitao Chen, Yiping Zhou, Anne Zhang, and Gordon Sun.2005.
Unigram language model for chinese wordsegmentation.
In Proceedings of the fourth SIGHANworkshop, pages 138?141.Thomas Emerson.
2005.
The second internationalchinese word segmentation bakeoff.
In Proceedingsof the fourth SIGHAN workshop, pages 123?133.Guohong Fu and Kang-Kwong Luke.
2004.
Chineseunknown word identification using class-based lm.
InProceedings of IJCNLP?04, volume 3248 of LectureNotes in Computer Science, pages 704?713.
Springer.Jianfeng Gao, Galen Andrew, Mark Johnson, andKristina Toutanova.
2007.
A comparative study ofparameter estimation methods for statistical naturallanguage processing.
In Proceedings of the 45thAnnual Meeting of the Association of ComputationalLinguistics (ACL?07), pages 824?831.Chooi-Ling Goh, Masayuki Asahara, and YujiMatsumoto.
2003.
Chinese unknown wordidentification using character-based tagging andchunking.
In Kotaro Funakoshi, Sandra Kbler, andJahna Otterbacher, editors, Proceedings of ACL(Companion)?03, pages 197?200.Chun-Nan Hsu, Han-Shen Huang, Yu-Ming Chang, andYuh-Jye Lee.
2009.
Periodic step-size adaptation insecond-order gradient descent for single-pass on-linestructured learning.
Machine Learning, 77(2-3):195?224.M.
Hannan J. Nie and W. Jin.
1995.
Unknownword detection and segmentation of chinese usingstatistical and heuristic knowledge.
Communicationsof the Chinese and Oriental Languages InformationProcessing Society, 5:47C57.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic modelsfor segmenting and labeling sequence data.
InProceedings of the 18th International Conference onMachine Learning (ICML?01), pages 282?289.Noboru Murata.
1998.
A statistical study of on-linelearning.
In On-line learning in neural networks,Cambridge University Press, pages 63?92.Jorge Nocedal and Stephen J. Wright.
1999.
Numericaloptimization.
Springer.Fuchun Peng, Fangfang Feng, and Andrew McCallum.2004.
Chinese segmentation and new word detectionusing conditional random fields.
In Proceedings ofColing 2004, pages 562?568, Geneva, Switzerland,Aug 23?Aug 27.
COLING.Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro.2007.
Pegasos: Primal estimated sub-gradient solverfor svm.
In Proceedings of ICML?07.Xu Sun and Jun?ichi Tsujii.
2009.
Sequential labelingwith latent variables: An exact inference algorithmand its efficient approximation.
In Proceedings ofEACL?09, pages 772?780, Athens, Greece, March.Xu Sun, Louis-Philippe Morency, Daisuke Okanohara,and Jun?ichi Tsujii.
2008.
Modeling latent-dynamicin shallow parsing: A latent conditional model withimproved inference.
In Proceedings of COLING?08,pages 841?848, Manchester, UK.Xu Sun, Takuya Matsuzaki, Daisuke Okanohara, andJun?ichi Tsujii.
2009a.
Latent variable perceptronalgorithm for structured classification.
In Proceedingsof the 21st International Joint Conference on ArtificialIntelligence (IJCAI 2009), pages 1236?1242.Xu Sun, Yaozhong Zhang, TakuyaMatsuzaki, YoshimasaTsuruoka, and Jun?ichi Tsujii.
2009b.
Adiscriminative latent variable chinese segmenter withhybrid word/character information.
In Proceedingsof NAACL-HLT?09, pages 56?64, Boulder, Colorado,June.Xu Sun, Hisashi Kashima, Takuya Matsuzaki, andNaonori Ueda.
2010.
Averaged stochastic gradientdescent with feedback: An accurate, robust, andfast training method.
In Proceedings of the 10thInternational Conference on Data Mining (ICDM?10),pages 1067?1072.Xu Sun, Hisashi Kashima, Ryota Tomioka, and NaonoriUeda.
2011.
Large scale real-life action recognitionusing conditional random fields with stochastictraining.
In Proceedings of the 15th Pacific-AsiaConf.
on Knowledge Discovery and Data Mining(PAKDD?11).Weiwei Sun.
2010.
Word-based and character-based word segmentation models: Comparison andcombination.
In Chu-Ren Huang and Dan Jurafsky,editors, COLING?10 (Posters), pages 1211?1219.Chinese Information Processing Society of China.Huihsin Tseng, Pichuan Chang, Galen Andrew, DanielJurafsky, and Christopher Manning.
2005.
A261conditional random field word segmenter for sighanbakeoff 2005.
In Proceedings of The Fourth SIGHANWorkshop, pages 168?171.S.V.N.
Vishwanathan, Nicol N. Schraudolph, Mark W.Schmidt, and Kevin P. Murphy.
2006.
Acceleratedtraining of conditional random fields with stochasticmeta-descent.
In Proceedings of ICML?06, pages 969?976.A.
Wu and Z. Jiang.
2000.
Statistically-enhanced newword identification in a rule-based chinese system.In Proceedings of the Second Chinese LanguageProcessing Workshop, page 46C51, Hong Kong,China.Yi-Lun Wu, Chaio-Wen Hsieh, Wei-Hsuan Lin, Chun-Yi Liu, and Liang-Chih Yu.
2011.
Unknownword extraction from multilingual code-switchingsentences (in chinese).
In Proceedings of ROCLING(Posters)?11, pages 349?360.Nianwen Xue.
2003.
Chinese word segmentationas character tagging.
International Journal ofComputational Linguistics and Chinese LanguageProcessing, 8(1):29?48.Yue Zhang and Stephen Clark.
2007.
Chinesesegmentation with a word-based perceptron algorithm.In Proceedings of the 45th Annual Meeting of theAssociation of Computational Linguistics, pages 840?847, Prague, Czech Republic, June.
Association forComputational Linguistics.Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumita.2006.
Subword-based tagging by conditional randomfields for chinese word segmentation.
In Proceedingsof the Human Language Technology Conference ofthe NAACL, Companion Volume: Short Papers, pages193?196, New York City, USA, June.
Association forComputational Linguistics.Hai Zhao, Changning Huang, Mu Li, and Bao-Liang Lu.2010.
A unified character-based tagging frameworkfor chinese word segmentation.
ACM Trans.
AsianLang.
Inf.
Process., 9(2).Guodong Zhou.
2005.
A chunking strategytowards unknown word detection in chinese wordsegmentation.
In Robert Dale, Kam-Fai Wong,Jian Su, and Oi Yee Kwong, editors, Proceedingsof IJCNLP?05, volume 3651 of Lecture Notes inComputer Science, pages 530?541.
Springer.262
