Integration of Speech and Vision in a small mobile robotDominique ESTIVALDepartment of Linguistics and Applied LinguisticsUniversity of MelbourneParkville VIC 3052, AustraliaD.Estival @linguistics.unimelb.edu.auAbstractThis paper reports on the integration of aspeech recognition component into a smallrobot, J. Edgar, which was developed in theAI Vision Lab at the University ofMelbourne.
While the use of voicecommands was fairly easy to implement,the interaction of the voice commands withthe existing navigation system of the robotturned out to pose a number of problems.IntroductionJ.
Edgar is a small autonomous mobile robotdeveloped in the AI Vision Lab at theUniversity of Melbourne, which is primarilyused as a platform for research in vision andnavigation.
The project which we describe inthis paper consists in the addition of somelanguage capabilities to the existing system, inparticular the recognition of voice commandsand the integration of the speech recognitioncomponent with the navigation system.While the vision and navigation work is mainlycarried out by Ph.D. students in ComputerScience, adding speech and languagecapabilities to the J.Edgar robot has been acollaborative project between the twoDepartments of Computer Science and ofLinguistics and Applied Linguistics, and thework has been performed by severai linguisticsstudents hosted by the Computer Sciencedepartment and working in tandem with CSstudents.The paper is organized as follows: section1describes the capabilities and restrictions ofthe robot J. Edgar, section 2 is an overview ofthe speech recognition and languageunderstanding system we have added to therobot, section 3 goes through the differentstages of the integration and section 4 brieflydescribes the generation component.1 Description of J. Edgar1.1 Moving aroundThe J.Edgar robot is rather limited in the typesof movement i can perform.
Its twin wheelsallow it to move forward in a straight line, andto turn around, either right or left, up to 360 ?
,but it cannot move backwards.
Its speed can bevaried, but is usually kept very low to avoidaccidents.1 .2  Vision and Navigation1.2.1 VisionThe vision system of J.Edgar consists in a one-eye monochrome camera mounted on a smailframe with two independent drive wheels and apan head.
Its spatial representation is two-dimensional and relies on edge detection.More specifically, it interprets discontinuitiesas boundaries between surfaces, whichconstitute obstacles.1.2.2 NavigationThe J.Edgar robot uses MYNORCA, a vision-based navigation system developed in theUniversity Melbourne AI Vision Lab (Howardand Kitchen, 1997a, 1997b).
This navigationsystem is divided into two levels:?
The local navigation system uses visualclues for obstacle detection and to formlocal maps.
It allows the robot to navigatein its immediate nvironment and to reachlocal goals without colliding withobstacles.
Most solid objects arerecognized as obstacles, but obstacles canalso be recognized as walls, corners ordoorways (see section 3.3).?
The global navigation system detectssignificant landmarks and uses a globalmap to determine its location in theenvironment.
It allows the robot to reachdistant goals specified according to theglobal map.
The detection of landmarksalso requires a level of object recognition109and the interpretation of visual cuesneeded at the local level.Figure 1 shows a series of snapshots for thelocal and global navigation systems during agiven time period.
Both systems are based onthe production of occupancy maps generatedby a visual mapping system based on thedetection of boundaries.This project has so far been able to interfaceonly with the vision-based navigation system atthe local level, but we hope we will soon beable to extend it to the object recognitionaspect and interact with the global level.\]\] .............~'  ~i-:" ~, ,iTi~i~,il~!
:4::~ ,~, <7 .~ :ili liI!li :.i~!~...:::(~:?
..:.Sl ~!
:::: ::i~ ~~ ~ ' t ~ i l  :::~:::: ' ....":"::~::::'i~' ~ ::::: ?
:~:::: ~:~: 'i~, .1~ , :~:~: ,~:~!:!
: :::.
:::.~ ~~"~'~"~'~ '  ~ i  f~ii ~ i!
:':S'.:.
: ~ .:"~..~.
~1~ ~ ~ ~:::::%::::~,~+:~ ..'.:.
~--"~:l~'~ :_-~:~,~ :::..~!~.
.
. '
: .
?.............
~z .
.
:~  ~C\]~:,:~.,.~..:~:,~ ~ ~ a: ;~ :.s !
:~ :.:.
'.~ ,%:.siI.":?~.
~.'?.,~':.~,(.:':'~.
'\] \ ] .
.
: "  \]~'::"~:" ":'::"~l'<~-~.,.'.:...~.'.::~,~..
'~:i:i~ I~4? '
~ '~ :::::" .
.
.
.
?
...::::: i i i : : r .
: .
:~ .
,  :: '~:.~.
~~.
.
.~  : , ~.:t.
:'.~:~'":'< ' ~ \ ]  IZ:I~:::I ::~ i".."~:,~i~r,., .~:: ~.
:~...~,:..~| ::~ i~.::~..:.
:%~\]I~"~i l~i!I't ~ ~"..'~.:~I~.
::::::::::::::::::::::::::::::::::::::::::::::::::: .
::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::.~ .i-<".~..<:i~!~i~i~i~:.
:.-.~i~i~i~ !i~i~<..:~.:.
:~7"" ,-: ~iiliii~ i.
..... ,...>, :"~!
:ii~i~i~.~  ",~,>" ..::.~.
:i!~!~i~i~ii~ r" ~.
i  i.... :..~!~i~!~:'..':'~.~.
.,.
....,; :::::::::::::::::::::::: ,..... , ::::::::::::::::::::::::~.
:: ::::::::::::::::::::::::::::::::::::::::::::::::::::  ====================================~::~:!,::..::W:~i~iL~~!~ii~.:Z'~iiD:i~%!~i~!iii!
I: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :?,e,a~ : ....~4 :~:f~i!~:~:: ~: :'-~ ~'~ ~:'-:..S~!:~:~:!
:"E f~~i !
~.
.
.
.
.
<~ ...... ~il ~ ~.
t;~1 ~; i:, i;.
LL,:t:.'%~.::1~7+P':":':':!
................ "" .
.
.
.
.
.
> "~ ~ '...'w>:'<~ : ?
?
.
.
: : i' 1 ~"~ii~ '~ 4 ,~ ~ ~:.~ ~{ ~:~.t ~'.it i t  ~l./>" '~ " i { < ,~' .-.,i "~ ~..,.
E l ,i~.
:,~, i~; ':.,<~.~ ,~ ~ i ' .
- .
.
.
t ~'~ i ~,.~.~S.~.~,..I,:,.~,~:...:.
:.>:;: .
: xx.~.l.b2~.J2~,~d..~,.~.L.,,d .
... ~i~.~.
7::~,..~,,.~:~.
i ~.
'~..~:~.~:;~..~.~.Figure 1: The upper set of images is a series of snapshots of the local occupancymap indicating the robot's current location and path.
The lower set of images is aseries of snapshots howing the evolution of the estimated global position(global pose estimate).
The cross-hatched region indicates possible robotlocations in the global model.
\[from Howard & Kitchen 1997a\].i i 0The vision and navigation systems are installedon a base-station which communicates via aUHF data-link with the on-board computer.The on-board computer performs the low-level hardware functions and supports theobstacle detection agent (see below).2 Speech and LanguageThe first step towards integrating some sort ofNatural Language capabilities into the robotwas to install a speech recognition component.The second step was to develop a grammar toanalyze voice commands and to map thosecommands onto the actual actions which therobot can perform.In the next stage of the project, we are nowworking towards the development of adialogue system, with which J.Edgar canrespond according to its internal status andmake appropriate answers to the voicecommands it recognizes.
Until the speechsynthesizer component is fully incorporatedinto the system, we are using canned speechfor the answers.The speech recognition system is installed onthe base-station and communicates with therobot via the UHF modem.2.1 Speech Recogn i t ionThe main factor taken into consideration inchoosing an off-the-shelf speech recognitionsystem was the possibility of building anapplication on top of it, and the IBMVoiceType system was first chosen because ofthe availability of development tools.
Despitesome initial problems, these tools have provenuseful and have allowed us to develop our owngrammar and interface with the robot.
Wehave now migrated to the IBM ViaVoice Goldsystem, which provides better speechrecognition performance and the samedevelopment tools as VoiceType.
In addition,ViaVoice includes a speech synthesizer, whichwe are currently incorporating in our system.In the remainder of this paper, I will describethe work that has been carried out using theIBM VoiceType system and ported to theViaVoice system.The system is speaker-independent a dso farhas been trained with more than 15 people.Care has been taken not to overtrain it withany one particular person in order to maintainspeaker-independence.In general terms, the lexicon used in thesystem maps onto the actions which the robotcan perform and the entities it can recognize.The lexicon is thus as limited as the world ofthe robot, but it includes as many variantlexical items as might be plausibly used (e.g.turn, rotate, spin etc.
for TURN).
Theseactions and entities are described in section 3.The IBM VoiceType or ViaVoice system canbe used either as a dictation system withdiscrete words, or in continuous peech mode.Taking advantage of the grammardevelopment ools, we are using it incontinuous mode, and the voice commands areparsed by the grammar described in section2.2.2.2.
Commands  GrammarIn addition to the baseline word recognitioncapability, the development tools in the IBMVoiceType or ViaVoice systems all thedeveloper to write a BNF grammar for parsinginput strings of recognized words.
We havethus developed a grammar mapping voicecommands to the actions J. Edgar is capable ofperforming.2.2.1.
SemanticsEach item in the lexicon is annotated with an"annodata", which can be thought of as itssemantic interpretation for this domain.Recognized input strings are thus transformedinto strings of "annodata", which are furtherparsed and sent to the communicationprotocol.
A command such as (1) will berecognized as (2) and the string of annodata(3) will be then parsed to produce thesequence of commands (4).
(1) J.Edgar before turning left andmoving forward please turn around(2) J.Edgar:"INITIALIZE'" before:"INIT2"turning:"TURN" left:"LEFT"and:"SEQUENCE" moving :"MOVE"forward :"FORWARD" please :"INITI"turn:"TURN" around:"B ACKW ARDS"(3) INITIALIZE INIT2 TURN LEFTSEQUENCE MOVE FORWARDINIT1 TURN BACKWARDS(4) INITIALIZE INIT1 TURNBACKWARDS INIT2 TURN LEFTSEQUENCE MOVE FORWARDI i i2.2.2.
Syntactic analysisAll commands to the robot are in theimperative.
However, some structures forcomplex commands have been implemented.These concern mainly the coordination ofcommands and temporal sequence.
As shownin the example above, conjunctions uch asbefore and after will trigger the recognition ofa temporal sequence and the possiblereordering of the commands.
Otherrecognized constructions include:(5) IF .... COMMANDIf there is a wall to your left, turnright and move forward.
(6) WHEN .... COMMANDWhen you get m a all, go along it.3.
Integration3.1.
Movements  onlyIn the first stage of this project, the naturallanguage system was only interfacing with themovement commands of the robot, and notwith the navigation system (either locai orglobal).
That is, the robot was eitherperforming in the voice command modality,or in the navigation modality.
The marereason for this limitation was that thenavigation system was still under developmentand not robust enough to ensure safemanoeuvering in case of voice commandsleading to potentially damaging situations.As a result, only commands relating tomovements (MOVE or TURN), and theirspecifications (FORWARD, LEFT, RIGHT, andspecific distances) were understood and therewas no need for representing objects orentities.3.2.
Low- leve l  visionIn the second stage of the project, we onlyintegrated the language capabilities with thelow-level vision system of the local navigationsystem.
In practical terms this means that whilethe robot can both accept spoken commandsand scan its environment, i  can only recognizelocal movement commands and will only obeythem if they do not lead to a collision.
'naus, this stage also did not require theaddition of any semantic representation forobjects.
However, to avoid a collision with anobstacle, we need the local vision system forobstacle recognition.
We use the"careForward" function, which overrides thedefault distance of 1 meter if there is anobstacle in the path of the robot and ensuresthat the robot will only move to a safedistance from it.3.3.
Loca l  navigationFurther integration consists in issuingcommands that involve locations and objectsthe robot knows about, as in (7):(7) Go down the corridor and go throughthe first doorway on the right.This stage involves referring to objects andentities recognized by the robot.There are five types of primitive objects in theworld which the robot can identify:- WALLa straight line;- DOORWAYa gap between two walls;- INSIDE CORNER ("in the corner")two lines meeting at an angle andenclosing the robot;- OUTSIDE CORNER ("around the corner")two lines meeting at an angle andgoing away from the robot;- LUMPa bounded solid object.From combining these primitive objects, therobot can also create representations forcomplex objects:- INTERSECTION:two outside comers that form anopening;- CORRIDOR:two parallel walls.Both types of objects can be used as referentsin commands and can be queried.It is worth emphasising that obstacles are notrecognized as a separate categorie, but areeither walls, lumps, corners, or doorways whichare not wide enough for the robot to passthrough.For instance, in Figure 2, the robot recognizesan opening in the wall on its right and mightlater recognize an outside corner to its left.i12The white area corresponds to the area therobot has already recognized as being emptyand the black areas to recognized walls.Figure 2: Obstacle detection3.4.
Global navigationThe next stage of the project is the integrationwith the whole navigation system, includingthe recognition of objects and locations.
Inthis mode, the robot will not only stop whenthere is an obstacle, but will be able to decidewhether to try to go around it.
The objects tobe used as referents will include locations uchas Office 214, Andrew's office, or Corridor A,which have specific coordinates on the robot'sglobal map.
This is on-going work and wehope to have achieved this level of integrationin the next few months.4.
GenerationIn the meantime, theinformation about itsenvironment, includingrobot can returnperception of thethe obstacles whichwere recognized, and can ask for furtherinstructions.
We have identified four situationsfor the generation of questions by the robot:1. when a command is not recognized,2.
when a command is incomplete,3.
when a command cannot be completed,4.
when an object referred to in a commandcannot be located.The first and second situations only requireinput from the speech recognition system,including the mapping to robot commands.However, the third situation requires access tothe local navigation system, or at least toobstacle detection, and the fourth situationrequires access to either the local or globalnavigation system, depending on whether theobject is a primitive object or whether itrequires coordinates on the global map.
Inthese last two situations, the generation ofquestions by the robot involves a mappingbetween the robot's internal representations ofthe recognized environment and the actualexpressions used both in the commands and inreturning answers.ConclusionWhile this project has been a successfulcollaboration between vision-based navigationand natural language processing, the J.Edgarrobot is still far from having achieved aconvincing level of speech understanding.Some of the challenges of such a projectreside in the successful communicationbetween the speech recognition system and therobot, but the more interesting aspect is that ofthe correspondence b tween the entities usedby the navigation system and the phrasesrecognized by the speech system.Since the speech system is independent of thephysical robot, it can be interfaced with anumber of robots.
One of the extensions ofthis project is to install a natural languageinterface for some of the other robots beingbuilt in the AI lab and eventually to use thesame natural language interface with morethan one robot at a time.AcknowledgmentsWe thank Leon Sterling and Liz Sonnenbergfor the support of the Computer ScienceDepartment for this project, Andrew Howardfor letting us use J.Edgar and for his help andadvice throughout, Elise Dettman, MeladelMistica and John Moore for their enthusiasmand dedication, and all the people in the AIVision Lab for their help.ReferencesColleen Crangle and Patrick Suppes (1994).Language and learning for robots.
CSLI lecturenotes 41.
Stanford: CSLI.Andrew Howard and Les Kitchen (1997a).
Vision-Based navigation Using Natural Landmarks,FSR'97 International Conference on Field andService Robotics.
Canberra, Australia.Andrew Howarcl and Les Kitchen (1997b).
Fast Visualmapping for Mobile Robot Navigation, ICIPS'97IEEE International Conference on IntelligentProcessing Systems, Beijing.113
