Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 326?332,Queen Mary University of London, September 2009. c?2009 Association for Computational LinguisticsA Comparison between Dialog Corpora Acquiredwith Real and Simulated UsersDavid GriolDepartamento de Informa?ticaUniversidad Carlos III de Madriddgriol@inf.uc3m.esZoraida Callejas, Ramo?n Lo?pez-Co?zarDpto.
Lenguajes y Sistemas Informa?ticosUniversidad de Granada{zoraida, rlopezc}@ugr.esAbstractIn this paper, we test the applicabilityof a stochastic user simulation techniqueto generate dialogs which are similar toreal human-machine spoken interactions.To do so, we present a comparison be-tween two corpora employing a compre-hensive set of evaluation measures.
Thefirst corpus was acquired from real inter-actions of users with a spoken dialog sys-tem, whereas the second was generated bymeans of the simulation technique, whichdecides the next user answer taking intoaccount the previous user turns, the lastsystem answer and the objective of the di-alog.1 IntroductionDuring the last decade, there has been a grow-ing interest in learning corpus-based approachesfor the different components of spoken dialog sys-tems (Minker, 1999), (Young, 2002), (Esteve et al,2003), (He and Young, 2003), (Torres et al, 2005),(Georgila et al, 2006), (Williams and Young,2007).
One of the most relevant areas of studyhas been the automatic generation of dialogs be-tween the dialog manager and an additional mod-ule, called the user simulator, which generates au-tomatic interactions with the dialog system.A considerable effort is necessary to acquireand label a corpus with the data necessary to traingood models.
User simulators make it possible togenerate a large number of dialogs in a very simpleway, reducing the time and effort needed for theevaluation of a dialog system each time the sys-tem is modified.The construction of user models based on sta-tistical methods has provided interesting and well-founded results in recent years and is currently agrowing research area.
A probabilistic user modelcan be trained from a corpus of human-computerdialogs to simulate user answers.
Therefore, it canbe used to learn a dialog strategy by means of itsinteraction with the dialog manager.
In the liter-ature, there are several corpus-based approachesfor developing user simulators, learning optimalmanagement strategies, and evaluating the dialogsystem (Scheffler and Young, 2001) (Pietquin andDutoit, 2005) (Georgila et al, 2006) (Cuaya?huitlet al, 2006) (Lo?pez-Co?zar et al, 2006).
A sum-mary of user simulation techniques for reinforce-ment learning of the dialog strategy can be foundin (Schatzmann et al, 2006).
In this paper, wepropose a statistical approach to acquire a labeleddialog corpus from the interaction of a user simu-lator and a dialog manager.
In our methodology,the new user turn is selected using the probabil-ity distribution provided by a neural network.
Bymeans of the interaction of the dialog manager andthe user simulator, an initial dialog corpus can beextended by increasing its variability and detect-ing dialog situations in which the dialog managerdoes not provide an appropriate answer.
We pro-pose the use of this corpus for evaluating both ouruser simulation technique and our dialog systemperformance.Different studies have been carried out to com-pare corpora acquired by means of different tech-niques and to define the most suitable measures tocarry out this evaluation (Schatzmann et al, 2005),(Turunen et al, 2006), (Ai et al, 2007b), (Ai andLitman, 2006), (Ai and Litman, 2007), (Ai et al,2007a).
In this work, we have applied our dia-log simulation technique to acquire a corpus in theacademic domain, and compared it with a corpusrecorded from real users interactions with a spo-326ken dialog systemThe results of this comparison show that thesimulated corpus obtained is very similar to thecorpus recorded from real user interactions interms of number of turns, confirmations and dia-log acts among other evaluation measures.The rest of the paper is organized as follows.Section 2 summarizes the main characteristics ofthe UAH system.
Section 3 describes our statis-tical methodology for user simulation.
Section 4describes the set of measures used to compare thecorpus acquired with real users and the simulatedcorpus.
Section 5 presents the results of this eval-uation, and finally, the conclusions are presentedin Section 6.2 The UAH SystemUniversidad Al Habla (UAH - University on theLine) is a spoken dialog system developed to pro-vide spoken access to academic information aboutour department (Callejas and Lo?pez-Co?zar, 2005)(Callejas and Lo?pez-Co?zar, 2008).
The system iscomprised of the five typical modules of currentspoken dialog systems, concerned with automaticspeech recognition (ASR), dialog management(DM), database access (DB Access), data storage(DB) and oral response generation (RG).
In ad-dition, we implemented a module called GAG toautomatically create ASR grammars (Callejas andLo?pez-Co?zar, 2007).The dialog manager of the UAH system was im-plemented using VoiceXML documents that aredynamically created using PHP.
This way, it canadapt the system responses to the context of theconversation and the dialog state, which improvesthe naturalness of the interaction.
For example, thehelp messages provided by the system take intoaccount the topic that the user and the system areaddressing at a particular moment.
The context isused as well to decide the confirmation strategy touse.The information that the system provides can beclassified in four main groups: subjects, profes-sors, doctoral studies and registration, as shown inTable 1.
As can be observed, the system must askthe user for different pieces of information beforeproducing a response.A set of 100 dialogs was acquired with the UAHsystem by means of its interaction with real users.The acquisition process resulted in a spontaneousSpanish speech dialog corpus with 60 differentspeakers.
The total number of user turns was 422and the recorded material has a duration of 150minutes.
Figure 1 shows an example of a dialogextracted from this corpus.
Turns tagged with an?S?
refer to system turns, and turns tagged with ?U?refer to user turns.S1: Welcome to the UAH system.
How can Ihelp you?U1: I want to know information about sub-jects in Computer ScienceS2: Which subject do you want to consult?U2: Language Processors IS3: What would you like to know?U3: Can you inform me about the schedulesof Group D?S4: The schedule of Group D is: Mondaysfrom 9 to 10 and Wednesdays from 9 to 11and from 11 to 13.
Anything else?U4: Can you tell me the name of the lecturer?S5: The name of the lecturer of the GroupD of Language Processors I is David Griol.Anything else?U5: No, thank you.S6: Thank you for using the UAH system.Figure 1: An example of a dialog acquired withthe interaction of the UAH system with real users3 The Statistical User SimulationTechniqueIn the UAH spoken dialogue system, the user sim-ulator developed replaces the functions performedby the ASR and the NLU modules.The methodology that we have developed foruser simulation extends our work for developinga statistical methodology for dialog management(Griol et al, 2008).
The user answers are gener-ated taking into account the information providedby the simulator throughout the history of the dia-log, the last system turn, and the objective(s) pre-defined for the dialog.
A labeled corpus of dialogsis used to estimate the user model.
The formal de-scription of the proposed model is as follows:Let Ai be the output of the dialog system (thesystem answer) at time i, expressed in terms of di-alog acts.
Let Ui be the semantic representation ofthe user turn.
We represent a dialog as a sequenceof pairs (system-turn, user-turn):327Category Information provided by the user (including examples) Information provided by thesystemSubjectName Compilers Degree, lecturers, responsiblelecturer, semester, credits, webpageDegree, in case that there areseveral subjects with the samenameComputer scienceGroup name and optionally type,in case he asks for informationabout a specific groupATheory ATimetable, lecturerLecturers Any combination of name andsurnamesZoraidaZoraida CallejasMs.
CallejasOffice location, contact infor-mation (phone, fax, email),groups and subjects, doctoralcoursesOptionally semester, in case heasks for the tutoring hoursFirst semesterSecond semesterTutoring timetableDoctoral studies Name of a doctoral program Software development Department, responsibleName of a course if he asksfor information about a specificcourseObject-oriented program-mingType, creditsRegistration Name of the deadline Provisional registrationconfirmationInitial time, final time, de-scriptionTable 1: Information provided by the UAH system(A1, U1), ?
?
?
, (Ai, Ui), ?
?
?
, (An, Un)where A1 is the greeting turn of the system (thefirst turn of the dialog), and Un is the last user turn.We refer to a pair (Ai, Ui) as Si, the state of thedialog sequence at time i.Given this representation, the objective of theuser simulator at time i is to find an appropriateuser answer Ui.
This selection, which is a localprocess for each time i, takes into account the se-quence of dialog states that precede time i, the sys-tem answer at time i, and the objective of the di-alog O.
If the most probable user answer Ui isselected at each time i, the selection is made usingthe following maximization:U?i = argmaxUi?UP (Ui|S1, ?
?
?
, Si?1, Ai,O)where set U contains all the possible user answers.As the number of possible sequences of statesis very large, we establish a partition in this space(i.e., in the history of the dialog preceding time i).Let URi be the user register at time i.
The userregister is defined as a data structure that containsthe information provided by the user throughoutthe previous history of the dialog.The partitionthat we establish in this space is based on the as-sumption that two different sequences of states areequivalent if they lead to the same UR.
After ap-plying the above considerations and establishingthe equivalence relations in the histories of the di-alogs, the selection of the best Ui is given by:U?i = argmaxUi?UP (Ui|URi?1, Ai,O) (1)We propose the use of a multilayer percep-tron (MLP) to make the assignation of a userturn.
The input layer receives the current situa-tion of the dialog, which is represented by the term(URi?1, Ai,O) in Equation 1.
The values of theoutput layer can be viewed as the a posteriori prob-ability of selecting the different user answers de-fined for the simulator given the current situationof the dialog.
The choice of the most probableuser answer of this probability distribution leadsto Equation 1.
In this case, the user simulator willalways generate the same answer for the same sit-uation of the dialog.
Since we want to provide theuser simulator with a richer variability of behav-iors, we base our choice on the probability distri-bution supplied by the MLP on all the feasible useranswers.For the UAH task, the variable O is modeledtaking into account the different types of scenariosdefined for the acquisition of the original corpuswith real users (33).The corpus acquired with real users includes in-formation about the errors that were introduced by328the ASR and the NLU modules during this acqui-sition.
This information also includes confidencemeasures, which are used by the DM to evaluatethe reliability of the concepts and attributes gener-ated by the NLU module.An error simulator module has been designedto perform error generation.
The error simulatormodifies the frames generated by the user simula-tor once the UR is updated.
In addition, the errorsimulator adds a confidence score to each conceptand attribute in the frames.
Experimentally, wehave detected 2.3 errors per dialog in our initialcorpus.
This value can be modified to adapt the er-ror simulator module to the operation of any ASRand NLU modules.A maximum number of twelve user turns per di-alog was defined for acquiring a corpus using ouruser simulator.
A user request for closing the di-alog is selected once the system has provided theinformation defined in the objective(s) of the dia-log.
The dialogs that fulfill this condition beforethe maximum number of turns are considered suc-cessful.
The dialog manager considers that the di-alog is unsuccessful and decides to abort it whenthe following conditions take place:?
The dialog exceeds the maximum number ofuser turns.?
The answer selected by the dialog managercorresponds with a query not required by theuser simulator.?
The database query module provides an errorwarning because the user simulator has notprovided the mandatory information neededto carry out the query.?
The answer generator provides a error warn-ing when the selected answer involves the useof a data not provided by the user simulator.The semantics selected for the dialog manageris represented through the 27 possible system an-swers defined for the UAH task.
The selection ofthe possible user answers is carried out using thesemantics defined for the user in the NLU module.A dialog corpus of 3000 successful dialogs wasacquired using the user simulation technique fol-lowing a set of scenarios covering all the possibletypes of queries regarding the information sought.Figure 2 shows an example of a dialog cor-responding to a scenario with two objectives (toknow timetables of a given lecturer and subject,and information about the registration in its spe-cific degree).
The confidence scores generated bythe error simulator are shown between brackets.Sentences in natural language, which are equiv-alent to the labeled system and user turns, havebeen incorporated to clarify the explanation.S1: OpeningWelcome to the UAH system.
How may I helpyou?U1: Lecturers [0.1]I want to know information about lecturers.S2: (Confirmation:Lecturers)Do you want to know information about lec-turers?U2: Affirmation [0.9]Yes.S3: (Question:Lecturer Name)Tell me the name of the lecturer.U3: Lecturer Name: Ramo?n Lo?pez-Co?zar[0.8]Ramo?n Lo?pez-Co?zar.S4: (Question:Lecturer Information)What would you like to know about Mr.Lo?pez-Co?zar?U4: (Question:Office) [0.9]The location of his office please.S5: (Answer:Office) (New-Query){The office of Ramo?n Lo?pez-Co?zar is locatedin the third floor of ...} Anything else?U5: (Question:Registration) [0.9]Degree: Computer Science[0.9]How to register in Computer Science.S6: (Answer:Registration) (New-Query){Registration information} Anything else?U6: Negation [0.9] No.S7: (Closing)Thank you for using the UAH system.Figure 2: An example of a dialog acquired bymeans of the simulation technique4 Evaluation of the CorporaWe used a set of measures to carry out the evalu-ation of the acquired corpora based on prior workin the dialog literature.
(Schatzmann et al, 2005)proposed a comprehensive set of quantitative eval-uation measures to compare two dialog corpora.These measures were adapted for our purpose andcan be divided into three types:329High-level dialog featuresAverage number of turns per dialogPercentage of different dialogsNumber of repetitions of the most seen dialogNumber of turns of the most seen dialogNumber of turns of the shortest dialogNumber of turns of the longest dialogDialog style/cooperativeness measuresSystem dialog acts: Confirmation of concepts and attributes, Questions to require information, andAnswers generated after a database query.User dialog acts: Request to the system, Provide information, Confirmation, Yes/No answers, andOther answers.Figure 3: Evaluation measures used to compare the acquired corpora?
High-level dialog features: These featuresevaluate the duration of the dialogs, theamount of information transmitted in the in-dividual turns, and how active the dialog par-ticipants are.?
Dialog style/cooperativeness measures:These measures analyze the frequency ofthe different speech acts and study, forexample, the proportion of actions which aregoal-directed vs. dialog formalities.?
Task success/efficiency measures: These arecomputations of the goal achievement ratesand goal completion times.We have defined six high-level dialog featuresfor the evaluation of the dialogs: the average num-ber of turns per dialog, the percentage of differ-ent dialogs without considering the attribute val-ues, the number of repetitions of the most seen di-alog, the number of turns of the most seen dialog,the number of turns of the shortest dialog, and thenumber of turns of the longest dialog.
Using thesemeasures, we tried to evaluate the success of thesimulated dialogs as well as their efficiency andvariability with regard to the different objectives.For dialog style features, we have defined a setof system/user dialog acts.
On the system side,we have measured the frequency of confirmations,questions that require information, and system an-swers generated after a database query.
We havenot taken into account the opening and closing sys-tem turns.
On the user side, we have measured thepercentage of turns in which the user carries outa request to the system, provide information, con-firms a concept or attribute, Yes/No answers, andother answers not included in the previous cate-gories.We have not considered task success/efficiencymeasures in our evaluation, since only the dialogsthat fulfill the objectives predefined in the scenar-ios have been incorporated into our corpora.
Wehave considered successful dialogs those that ful-fill the complete list of objectives defined in thecorresponding scenario.
Figure 3 summarizes thecomplete set of measures used in the evaluation.5 Evaluation ResultsTo compare the two corpora, we have computedthe mean value for each corpus with respect toeach of the evaluation measures shown in the pre-vious section.
Then two-tailed t-tests have beenemployed to compare the means across the twocorpora as described in (Ai et al, 2007a).
All dif-ferences reported as statistically significant havep-values less than 0.05 after Bonferroni correc-tions.5.1 High-level Dialog FeaturesAs stated in the previous section, the first group ofexperiments covers the following statistical prop-erties: i) Dialog length in terms of the averagenumber of turns per dialog, number of turns of theshortest dialog, number of turns of the longest di-alog, and number of turns of the most seen dialog;ii) Number of different dialogs in each corpus interms of the percentage of different dialogs and thenumber of repetitions of the most seen dialog; iii)Turn length in terms of actions per turn; iv) Partic-ipant activity as a ratio of system and user actionsper dialog.330Initial Corpus Simulated CorpusAverage number of user turns per dialog 4.99 3.75Percentage of different dialogs 85.71% 77.42%Number of repetitions of the most seen dialog 5 27Number of turns of the most seen dialog 2 2Number of turns of the shortest dialog 2 2Number of turns of the longest dialog 14 12Table 2: Results of the high-level dialog features defined for the comparison of the three corporaTable 2 shows the results of the comparison ofthe high-level dialog features.
It can be observedthat all measures have similar values in both cor-pora.
The more significant difference is the aver-age number of user turns.
In the four types of sce-narios, the dialogs acquired using the simulationtechnique were shorter than the dialogs acquiredwith real users.
This can be explained by the factthat there was a number of dialogs acquired withreal users in which the user asked for additionalinformation not included in the definition of thecorresponding scenario once the dialog objectiveshad been achieved.5.2 Dialog Style and CooperativenessTables 3 and 4 respectively show the frequency ofthe most dominant user and system dialog acts.Table 3 shows the results of this comparison forthe system dialog acts.
It can be observed thatthere are also only slight differences between thevalues obtained for both corpora.
There is a higherpercentage of confirmations and questions in thecorpus acquired with real users due to its higheraverage number of turns per dialog.Table 4 shows the results of this comparison forthe user dialog acts.
The most significant differ-ence between both corpora is the percentage ofturns in which the user makes a request to the sys-tem, which is lower in the corpus acquired withreal users.
This is possibly because it is less prob-able that simulated users provide useless informa-tion, as it is shown in the lower percentage of theusers turns classified as Other answers.6 ConclusionsIn this paper, we have presented a comparison be-tween two corpora acquired using two differenttechniques.
Firstly, we gathered an initial dialogcorpus from real user-system interactions.
Sec-ondly, we have employed a statistical user simu-lation technique based on a classification processto automatically obtain a corpus of simulated di-alogs.
Our results show that it is feasible to acquirea realistic corpus by means of the simulation tech-nique.
The experimental results reported indicatethat the simulated and real interactions corpora arevery similar in terms of number of user turns, userand system dialog style and cooperativeness, andmost frequent dialogs statistics.
As future work,we plan to employ the simulated dialogs for eval-uation purposes and for extracting valuable infor-mation to optimize the current dialog strategy.ReferencesH.
Ai and D. Litman.
2006.
Comparing Real-Real,Simulated-Simulated, and Simulated-Real SpokenDialogue Corpora.
In Procs.
of AAAI Workshop Sta-tistical and Empirical Approaches for Spoken Dia-logue Systems, Boston, USA.H.
Ai and D.J.
Litman.
2007.
Knowledge ConsistentUser Simulations for Dialog Systems.
In Proc.
of In-terspeech?07, pages 2697?2700, Antwerp, Belgium.H.
Ai, A. Raux, D. Bohus, M. Eskenazi, and D. Litman.2007a.
Comparing Spoken Dialog Corpora Col-lected with Recruited Subjects versus Real Users.
InProc.
of the SIGdial?07, pages 124?131, Antwerp,Belgium.H.
Ai, J.R. Tetreault, and D.J.
Litman.
2007b.
Com-paring User Simulation Models For Dialog StrategyLearning.
In Proc.
of NAACL HLT?07, pages 1?4,Rochester, NY, USA.Z.
Callejas and R. Lo?pez-Co?zar.
2005.
Implementingmodular dialogue systems: a case study.
In Proc.
ofApplied Spoken Language Interaction in DistributedEnvironments (ASIDE?05), Aalborg, Denmark.Z.
Callejas and R. Lo?pez-Co?zar.
2007.
Automaticcreation of ASR grammar rules for unknown vo-cabulary applications.
In Proc.
of the 8th Interna-tional workshop on Electronics, Control, Modelling,Measurement and Signals (ECMS?07), pages 50?55,Liberec, Czech Republic.Z.
Callejas and R. Lo?pez-Co?zar.
2008.
Relations be-tween de-facto criteria in the evaluation of a spoken331Initial Corpus Simulated CorpusConfirmation of concepts and attributes 13.51% 12.23%Questions to require information 18.44% 16.57%Answers generated after a database query 68.05% 71.20%Table 3: Percentages of the different types of system dialog acts in both corporaInitial Corpus Simulated CorpusRequest to the system 31.74% 35.43%Provide information 21.72% 20.98%Confirmation 10.81% 9.34%Yes/No answers 33.47% 32.77%Other answers 2.26% 1.48%Table 4: Percentages of the different types of user dialog acts in both corporadialogue system.
Speech Communication, 50(8?9):646?665.H.
Cuaya?huitl, S. Renals, O.
Lemon, and H. Shi-modaira.
2006.
Learning Multi-Goal DialogueStrategies Using Reinforcement Learning with Re-duced State-Action Spaces.
In Proc.
of the 9th Inter-national Conference on Spoken Language Process-ing (Interspeech/ICSLP), pages 469?472, Pittsburgh(USA).Y.
Esteve, C. Raymond, F. Bechet, and R. De Mori.2003.
Conceptual Decoding for Spoken Dialog sys-tems.
In Proc.
of European Conference on SpeechCommunications and Technology (Eurospeech?03),volume 1, pages 617?620, Geneva (Switzerland).K.
Georgila, J. Henderson, and O.
Lemon.
2006.
UserSimulation for Spoken Dialogue Systems: Learn-ing and Evaluation.
In Proc.
of the 9th Interna-tional Conference on Spoken Language Processing(Interspeech/ICSLP), pages 1065?1068, Pittsburgh(USA).D.
Griol, L.F. Hurtado, E. Segarra, and E. Sanchis.2008.
A Statistical Approach to Spoken Dialog Sys-tems Design and Evaluation.
Speech Communica-tion, 50(8?9):666?682.Y.
He and S. Young.
2003.
A data-driven spoken lan-guage understanding system.
In Proc.
of IEEE Auto-matic Speech Recognition and Understanding Work-shop (ASRU?03), pages 583?588, St. Thomas (U.S.Virgin Islands).R.
Lo?pez-Co?zar, Z. Callejas, and M. McTear.
2006.Testing the performance of spoken dialogue systemsby means of an artificially simulated user.
ArtificialIntelligence Review, 26:291?323.W.
Minker.
1999.
Stocastically-based semantic analy-sis.
In Kluwer Academic Publishers, Boston (USA).O.
Pietquin and T. Dutoit.
2005.
A probabilisticframework for dialog simulation and optimal strat-egy learning.
In IEEE Transactions on Speech andAudio Processing, Special Issue on Data Mining ofSpeech, Audio and Dialog, volume 14, pages 589?599.J.
Schatzmann, K. Georgila, and S. Young.
2005.Quantitative Evaluation of User Simulation Tech-niques for Spoken Dialogue Systems.
In Proc.
ofSIGdial?05, pages 45?54, Lisbon (Portugal).J.
Schatzmann, K. Weilhammer, M. Stuttle, andS.
Young.
2006.
A Survey of Statistical User Sim-ulation Techniques for Reinforcement-Learning ofDialogue Management Strategies.
In KnowledgeEngineering Review, volume 21(2), pages 97?126.K.
Scheffler and S. Young.
2001.
Automatic learningof dialogue strategy using dialogue simulation andreinforcement learning.
In Proc.
of HLT?02, pages12?18, San Diego (USA).F.
Torres, L.F. Hurtado, F.
Garc?
?a, E. Sanchis, andE.
Segarra.
2005.
Error handling in a stochastic dia-log system through confidence measures.
In SpeechCommunication, pages (45):211?229.M.
Turunen, J. Hakulinen, and A. Kainulainen.
2006.Evaluation of a Spoken Dialogue System with Us-ability Tests and Long-term Pilot Studies: Similar-ities and Differences.
In Proc.
of the 9th Interna-tional Conference on Spoken Language Processing(Interspeech/ICSLP), pages 1057?1060, Pittsburgh,USA.J.
Williams and S. Young.
2007.
Partially ObservableMarkov Decision Processes for Spoken Dialog Sys-tems.
In Computer Speech and Language, volume21(2), pages 393?422.S.
Young.
2002.
The Statistical Approach to the De-sign of Spoken Dialogue Systems.
Technical re-port, CUED/F-INFENG/TR.433, Cambridge Uni-versity Engineering Department, Cambridge (UK).332
