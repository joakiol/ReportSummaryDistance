Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1489?1501,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsDiachronic Word Embeddings Reveal Statistical Laws ofSemantic ChangeWilliam L. Hamilton, Jure Leskovec, Dan JurafskyDepartment of Computer Science, Stanford University, Stanford CA, 94305wleif,jure,jurafsky@stanford.eduAbstractUnderstanding how words change theirmeanings over time is key to models oflanguage and cultural evolution, but his-torical data on meaning is scarce, mak-ing theories hard to develop and test.Word embeddings show promise as a di-achronic tool, but have not been carefullyevaluated.
We develop a robust method-ology for quantifying semantic changeby evaluating word embeddings (PPMI,SVD, word2vec) against known historicalchanges.
We then use this methodologyto reveal statistical laws of semantic evo-lution.
Using six historical corpora span-ning four languages and two centuries, wepropose two quantitative laws of seman-tic change: (i) the law of conformity?therate of semantic change scales with an in-verse power-law of word frequency; (ii)the law of innovation?independent of fre-quency, words that are more polysemoushave higher rates of semantic change.1 IntroductionShifts in word meaning exhibit systematic regu-larities (Br?eal, 1897; Ullmann, 1962).
The rateof semantic change, for example, is higher insome words than others (Blank, 1999) ?
com-pare the stable semantic history of cat (from Proto-Germanic kattuz, ?cat?)
to the varied meanings ofEnglish cast: ?to mould?, ?a collection of actors?,?a hardened bandage?, etc.
(all from Old Norsekasta, ?to throw?, Simpson et al, 1989).Various hypotheses have been offered aboutsuch regularities in semantic change, such as an in-creasing subjectification of meaning, or the gram-maticalization of inferences (e.g., Geeraerts, 1997;Blank, 1999; Traugott and Dasher, 2001).But many core questions about semantic changeremain unanswered.
One is the role of fre-quency.
Frequency plays a key role in other lin-guistic changes, associated sometimes with fasterchange?sound changes like lenition occur inmore frequent words?and sometimes with slowerchange?high frequency words are more resistantto morphological regularization (Bybee, 2007;Pagel et al, 2007; Lieberman et al, 2007).
Whatis the role of word frequency in meaning change?Another unanswered question is the relationshipbetween semantic change and polysemy.
Wordsgain senses over time as they semantically drift(Br?eal, 1897; Wilkins, 1993; Hopper and Trau-gott, 2003), and polysemous words1occur inmore diverse contexts, affecting lexical accessspeed (Adelman et al, 2006) and rates of L2learning (Crossley et al, 2010).
But we don?tknow whether the diverse contextual use of pol-ysemous words makes them more or less likelyto undergo change (Geeraerts, 1997; Winter etal., 2014; Xu et al, 2015).
Furthermore, poly-semy is strongly correlated with frequency?highfrequency words have more senses (Zipf, 1945;?Ilgen and Karaoglan, 2007)?so understandinghow polysemy relates to semantic change requirescontroling for word frequency.Answering these questions requires new meth-ods that can go beyond the case-studies of a fewwords (often followed over widely different time-periods) that are our most common diachronicdata (Br?eal, 1897; Ullmann, 1962; Blank, 1999;Hopper and Traugott, 2003; Traugott and Dasher,2001).
One promising avenue is the use of distri-butional semantics, in which words are embeddedin vector spaces according to their co-occurrencerelationships (Bullinaria and Levy, 2007; Turneyand Pantel, 2010), and the embeddings of words1We use ?polysemy?
here to refer to related senses as wellas rarer cases of accidental homonymy.1489Figure 1: Two-dimensional visualization of semantic change in English using SGNS vectors.2a, The word gay shifted frommeaning ?cheerful?
or ?frolicsome?
to referring to homosexuality.
b, In the early 20th century broadcast referred to ?castingout seeds?
; with the rise of television and radio its meaning shifted to ?transmitting signals?.
c, Awful underwent a process ofpejoration, as it shifted from meaning ?full of awe?
to meaning ?terrible or appalling?
(Simpson et al, 1989).are then compared across time-periods.
This newdirection has been effectively demonstrated in anumber of case-studies (Sagi et al, 2011; Wijayaand Yeniterzi, 2011; Gulordava and Baroni, 2011;Jatowt and Duh, 2014) and used to perform large-scale linguistic change-point detection (Kulkarniet al, 2014) as well as to test a few specific hy-potheses, such as whether English synonyms tendto change meaning in similar ways (Xu and Kemp,2015).
However, these works employ widely dif-ferent embedding approaches and test their ap-proaches only on English.In this work, we develop a robust methodol-ogy for quantifying semantic change using embed-dings by comparing state-of-the-art approaches(PPMI, SVD, word2vec) on novel benchmarks.We then apply this methodology in a large-scalecross-linguistic analysis using 6 corpora spanning200 years and 4 languages (English, German,French, and Chinese).
Based on this analysis, wepropose two statistical laws relating frequency andpolysemy to semantic change:?
The law of conformity: Rates of semanticchange scale with a negative power of wordfrequency.?
The law of innovation: After controlling forfrequency, polysemous words have signifi-cantly higher rates of semantic change.2 Diachronic embedding methodsThe following sections outline how we constructdiachronic (historical) word embeddings, by firstconstructing embeddings in each time-period andthen aligning them over time, and the metrics that2Appendix B details the visualization method.we use to quantify semantic change.
All of thelearned embeddings and the code we used to ana-lyze them are made publicly available.32.1 Embedding algorithmsWe use three methods to construct word em-beddings within each time-period: PPMI, SVD,and SGNS (i.e., word2vec).4These distributionalmethods represent each word wiby a vector withat captures information about its co-occurrencestatistics.
These methods operationalize the ?dis-tributional hypothesis?
that word semantics are im-plicit in co-occurrence relationships (Harris, 1954;Firth, 1957).
The semantic similarity/distance be-tween two words is approximated by the cosinesimilarity/distance between their vectors (Turneyand Pantel, 2010).2.1.1 PPMIIn the PPMI representations, the vector embeddingfor word wi?
V contains the positive point-wisemutual information (PPMI) values betweenwianda large set of pre-specified ?context?
words.
Theword vectors correspond to the rows of the matrixMPPMI?
R|V|?|VC|with entries given byMPPMIi,j= max{log(p?
(wi, cj)p?(w)p?(cj))?
?, 0},(1)where cj?
VCis a context word and ?
> 0is a negative prior, which provides a smooth-ing bias (Levy et al, 2015).
The p?
correspondto the smoothed empirical probabilities of word3http://nlp.stanford.edu/projects/histwords4Synchronic applications of these three methods are re-viewed in detail in Levy et al (2015).1490Name Language Description Tokens Years POS SourceENGALL English Google books (all genres) 8.5?
10111800-1999 (Davies, 2010)ENGFIC English Fiction from Google books 7.5?
10101800-1999 (Davies, 2010)COHA English Genre-balanced sample 4.1?
1081810-2009 (Davies, 2010)FREALL French Google books (all genres) 1.9?
10111800-1999 (Sagot et al, 2006)GERALL German Google books (all genres) 4.3?
10101800-1999 (Schneider and Volk, 1998)CHIALL Chinese Google books (all genres) 6.0?
10101950-1999 (Xue et al, 2005)Table 1: Six large historical datasets from various languages and sources are used.
(co-)occurrences within fixed-size sliding win-dows of text.
Clipping the PPMI values above zeroensures they remain finite and has been shown todramatically improve results (Bullinaria and Levy,2007; Levy et al, 2015); intuitively, this clippingensures that the representations emphasize posi-tive word-word correlations over negative ones.2.1.2 SVDSVD embeddings correspond to low-dimensionalapproximations of the PPMI embeddings learnedvia singular value decomposition (Levy et al,2015).
The vector embedding for word wiis givenbywSVDi= (U??
)i, (2)where MPPMI= U?V>is the truncated singularvalue decomposition of MPPMIand ?
?
[0, 1] isan eigenvalue weighting parameter.
Setting ?
< 1has been shown to dramatically improve embed-ding qualities (Turney and Pantel, 2010; Bulli-naria and Levy, 2012).
This SVD approach canbe viewed as a generalization of Latent Seman-tic Analysis (Landauer and Dumais, 1997), wherethe term-document matrix is replaced with MPPMI.Compared to PPMI, SVD representations can bemore robust, as the dimensionality reduction actsas a form of regularization.2.1.3 Skip-gram with negative samplingSGNS ?neural?
embeddings are optimized to pre-dict co-occurrence relationships using an approx-imate objective known as ?skip-gram with nega-tive sampling?
(Mikolov et al, 2013).
In SGNS,each word wiis represented by two dense, low-dimensional vectors: a word vector (wSGNSi) andcontext vector (cSGNSi).
These embeddings are op-timized via stochastic gradient descent so thatp?
(ci|wi) ?
exp(wSGNSi?
cSGNSj), (3)where p(ci|wi) is the empirical probability of see-ing context word ciwithin a fixed-length windowof text, given that this window contains wi.
TheSGNS optimization avoids computing the normal-izing constant in (3) by randomly drawing ?neg-ative?
context words, cn, for each target word andensuring that exp(wSGNSi?cSGNSn) is small for theseexamples.SGNS has the benefit of allowing incrementalinitialization during learning, where the embed-dings for time t are initialized with the embed-dings from time t ?
?
(Kim et al, 2014).
Weemploy this trick here, though we found that it hada negligible impact on our results.2.2 Datasets, pre-processing, andhyperparametersWe trained models on the 6 datasets describedin Table 1, taken from Google N-Grams (Lin etal., 2012) and the COHA corpus (Davies, 2010).The Google N-Gram datasets are extremely large(comprising?6% of all books ever published), butthey also contain many corpus artifacts due, e.g.,to shifting sampling biases over time (Pechenicket al, 2015).
In contrast, the COHA corpus wascarefully selected to be genre-balanced and rep-resentative of American English over the last 200years, though as a result it is two orders of mag-nitude smaller.
The COHA corpus also containspre-extracted word lemmas, which we used to val-idate that our results hold at both the lemma andraw token levels.
All the datasets were aggregatedto the granularity of decades.5We follow the recommendations of Levy et al(2015) in setting the hyperparameters for the em-bedding methods, though preliminary experimentswere used to tune key settings.
For all methods,we used symmetric context windows of size 4 (oneach side).
For SGNS and SVD, we use embed-dings of size 300.
See Appendix A for further im-plementation and pre-processing details.5The 2000s decade of the Google data was discarded dueto shifts in the sampling methodology (Michel et al, 2011).14912.3 Aligning historical embeddingsIn order to compare word vectors from differ-ent time-periods we must ensure that the vectorsare aligned to the same coordinate axes.
Ex-plicit PPMI vectors are naturally aligned, as eachcolumn simply corresponds to a context word.Low-dimensional embeddings will not be natu-rally aligned due to the non-unique nature of theSVD and the stochastic nature of SGNS.
In par-ticular, both these methods may result in arbi-trary orthogonal transformations, which do not af-fect pairwise cosine-similarities within-years butwill preclude comparison of the same word acrosstime.
Previous work circumvented this problemby either avoiding low-dimensional embeddings(e.g., Gulordava and Baroni, 2011; Jatowt andDuh, 2014) or by performing heuristic local align-ments per word (Kulkarni et al, 2014).We use orthogonal Procrustes to align thelearned low-dimensional embeddings.
DefiningW(t)?
Rd?|V|as the matrix of word embeddingslearned at year t, we align across time-periodswhile preserving cosine similarities by optimizing:R(t)= arg minQ>Q=I?W(t)Q?W(t+1)?F, (4)with R(t)?
Rd?d.
The solution correspondsto the best rotational alignment and can be ob-tained efficiently using an application of SVD(Sch?onemann, 1966).2.4 Time-series from historical embeddingsDiachronic word embeddings can be used in twoways to quantify semantic change: (i) we can mea-sure changes in pair-wise word similarities overtime, or (ii) we can measure how an individualword?s embedding shifts over time.Pair-wise similarity time-series Measuringhow the cosine-similarity between pairs of wordschanges over time allows us to test hypothesesabout specific linguistic or cultural shifts in a con-trolled manner.
We quantify shifts by computingthe similarity time-seriess(t)(wi, wj) = cos-sim(w(t)i,w(t)j) (5)between two words wiand wjover a time-period(t, ..., t + ?).
We then measure the Spearmancorrelation (?)
of this series against time, whichallows us to assess the magnitude and signifi-cance of pairwise similarity shifts; since the Spear-man correlation is non-parametric, this measureessentially detects whether the similarity series in-creased/decreased over time in a significant man-ner, regardless of the ?shape?
of this curve.6Measuring semantic displacement Afteraligning the embeddings for individual time-periods, we can use the aligned word vectors tocompute the semantic displacement that a wordhas undergone during a certain time-period.
Inparticular, we can directly compute the cosine-distance between a word?s representation fordifferent time-periods, i.e.
cos-dist(wt,wt+?
),as a measure of semantic change.
We can alsouse this measure to quantify ?rates?
of semanticchange for different words by looking at thedisplacement between consecutive time-points.3 Comparison of different approachesWe compare the different distributional ap-proaches on a set of benchmarks designed to testtheir scientific utility.
We evaluate both their syn-chronic accuracy (i.e., ability to capture word sim-ilarity within individual time-periods) and their di-achronic validity (i.e., ability to quantify semanticchanges over time).3.1 Synchronic AccuracyWe evaluated the synchronic (within-time-period)accuracy of the methods using a standard modernbenchmark and the 1990s portion of the ENGALLdata.
On Bruni et al (2012)?s MEN similarity taskof matching human judgments of word similari-ties, SVD performed best (?
= 0.739), followedby PPMI (?
= 0.687) and SGNS (?
= 0.649).These results echo the findings of Levy et al(2015), who found SVD to perform best on sim-ilarity tasks while SGNS performed best on anal-ogy tasks (which are not the focus of this work).3.2 Diachronic ValidityWe evaluate the diachronic validity of the methodson two historical semantic tasks: detecting knownshifts and discovering shifts from data.
For boththese tasks, we performed detailed evaluations ona small set of examples (28 known shifts and thetop-10 ?discovered?
shifts by each method).
Us-ing these reasonably-sized evaluation sets allowedthe authors to evaluate each case rigorously usingexisting literature and historical corpora.6Other metrics or change-point detection approaches, e.g.mean shifts (Kulkarni et al, 2014) could also be used.1492Word Moving towards Moving away Shift start Sourcegay homosexual, lesbian happy, showy ca 1920 (Kulkarni et al, 2014)fatal illness, lethal fate, inevitable <1800 (Jatowt and Duh, 2014)awful disgusting, mess impressive, majestic <1800 (Simpson et al, 1989)nice pleasant, lovely refined, dainty ca 1900 (Wijaya and Yeniterzi, 2011)broadcast transmit, radio scatter, seed ca 1920 (Jeffers and Lehiste, 1979)monitor display, screen ?
ca 1930 (Simpson et al, 1989)record tape, album ?
ca 1920 (Kulkarni et al, 2014)guy fellow, man ?
ca 1850 (Wijaya and Yeniterzi, 2011)call phone, message ?
ca 1890 (Simpson et al, 1989)Table 2: Set of attested historical shifts used to evaluate the methods.
The examples are taken from previous works on semanticchange and from the Oxford English Dictionary (OED), e.g.
using ?obsolete?
tags.
The shift start points were estimated usingattestation dates in the OED.
The first six examples are words that shifted dramatically in meaning while the remaining four arewords that acquired new meanings (while potentially also keeping their old ones).Method Corpus % Correct %Sig.PPMIENGALL 96.9 84.4COHA 100.0 88.0SVDENGALL 100.0 90.6COHA 100.0 96.0SGNSENGALL 100.0 93.8COHA 100.0 72.0Table 3: Performance on detection task, i.e.
ability to cap-ture the attested shifts from Table 2.
SGNS and SVD capturethe correct directionality of the shifts in all cases (%Correct),e.g., gay becomes more similar to homosexual, but there aredifferences in whether the methods deem the shifts to be sta-tistically significant at the p < 0.05 level (%Sig).Detecting known shifts.
First, we testedwhether the methods capture known historicalshifts in meaning.
The goal in this task is forthe methods to correctly capture whether pairs ofwords moved closer or further apart in semanticspace during a pre-determined time-period.
Weuse a set of independently attested shifts as anevaluation set (Table 2).
For comparison, we eval-uated the methods on both the large (but messy)ENGALL data and the smaller (but clean) COHAdata.
On this task, all the methods performedalmost perfectly in terms of capturing the correctdirectionality of the shifts (i.e., the pairwisesimilarity series have the correct sign on theirSpearman correlation with time), but there weresome differences in whether the methods deemedthe shifts statistically significant at the p < 0.05level.7Overall, SGNS performed the best on thefull English data, but its performance droppedsignificantly on the smaller COHA dataset, whereSVD performed best.
PPMI was noticeably worsethan the other two approaches (Table 3).Discovering shifts from data.
We testedwhether the methods discover reasonable shifts7All subsequent significance tests are at p < 0.05.by examining the top-10 words that changed themost from the 1900s to the 1990s according tothe semantic displacement metric introduced inSection 2.4 (limiting our analysis to words withrelative frequencies above 10?5in both decades).We used the ENGFIC data as the most-changedlist for ENGALL was dominated by scientificterms due to changes in the corpus sample.Table 4 shows the top-10 words discovered byeach method.
These shifts were judged by the au-thors as being either clearly genuine, borderline,or clearly corpus artifacts.
SGNS performed byfar the best on this task, with 70% of its top-10list corresponding to genuine semantic shifts, fol-lowed by 40% for SVD, and 10% for PPMI.
How-ever, a large portion of the discovered words forPPMI (and less so SVD) correspond to borderlinecases, e.g.
know, that have not necessarily shiftedsignificantly in meaning but that occur in differ-ent contexts due to global genre/discourse shifts.The poor quality of the nearest neighbors gener-ated by the PPMI algorithm?which are skewedby PPMI?s sensitivity to rare events?also madeit difficult to assess the quality of its discoveredshifts.
SVD was the most sensitive to corpus arti-facts (e.g., co-occurrences due to cover pages andadvertisements), but it still captured a number ofgenuine semantic shifts.We opted for this small evaluation set and re-lied on detailed expert judgments to minimize am-biguity; each potential shift was analyzed in detailby consulting consulting existing literature (espe-cially the OED; Simpson et al, 1989) and all dis-agreements were discussed.Table 5 details representative example shifts inEnglish, French, and German.
Chinese lacks suf-ficient historical data for this task, as only years1950-1999 are usable; however, we do still see1493Method Top-10 words that changed from 1900s to 1990sPPMI know, got, would, decided, think, stop, remember, started, must, wantedSVD harry, headed, calls, gay, wherever, male, actually, special, cover, naturallySGNS wanting, gay, check, starting, major, actually, touching, harry, headed, romanceTable 4: Top-10 English words with the highest semantic displacement values between the 1900s and 1990s.
Bolded entriescorrespond to real semantic shifts, as deemed by examining the literature and their nearest neighbors; for example, headedshifted from primarily referring to the ?top of a body/entity?
to referring to ?a direction of travel.?
Underlined entries areborderline cases that are largely due to global genre/discourse shifts; for example, male has not changed in meaning, but itsusage in discussions of ?gender equality?
is relatively new.
Finally, unmarked entries are clear corpus artifacts; for example,special, cover, and romance are artifacts from the covers of fiction books occasionally including advertisements etc.Word Language Nearest-neighbors in 1900s Nearest-neighbors in 1990swanting English lacking, deficient, lacked, lack, needed wanted, something, wishing, anything,anybodyasile French refuge, asiles, hospice, vieillards, in-firmeriedemandeurs, refuge, hospice, visas, ad-missionwiderstand German scheiterte, volt, stromst?arke, leisten,brechenopposition, verfolgung, nationalsozialis-tische, nationalsozialismus, kollaborationTable 5: Example words that changed dramatically in meaning in three languages, discovered using SGNS embeddings.
Theexamples were selected from the top-10 most-changed lists between 1900s and 1990s as in Table 4.
In English, wantingunderwent subjectification and shifted from meaning ?lacking?
to referring to subjective ?desire?, as in ?the education systemis wanting?
(1900s) vs. ?I?ve been wanting to tell you?
(1990s).
In French asile (?asylum?)
shifted from primarily referringto ?hospitals, or infirmaries?
to also referring to ?asylum seekers, or refugees?.
Finally, in German Widerstand (?resistance?
)gained a formal meaning as referring to the local German resistance to Nazism during World War II.some significant changes for Chinese in this shorttime-period, such as ??
(?virus?)
moving closerto??
(?computer?, ?
= 0.89).3.3 Methodological recommendationsPPMI is clearly worse than the other two meth-ods; it performs poorly on all the benchmark tasks,is extremely sensitive to rare events, and is proneto false discoveries from global genre shifts.
Be-tween SVD and SGNS the results are somewhatequivocal, as both perform best on two out of thefour tasks (synchronic accuracy, ENGALL detec-tion, COHA detection, discovery).
Overall, SVDperforms best on the synchronic accuracy task andhas higher average accuracy on the ?detection?task, while SGNS performs best on the ?discov-ery?
task.
These results suggest that both thesemethods are reasonable choices for studies of se-mantic change but that they each have their owntradeoffs: SVD is more sensitive, as it performswell on detection tasks even when using a smalldataset, but this sensitivity also results in false dis-coveries due to corpus artifacts.
In contrast, SGNSis robust to corpus artifacts in the discovery task,but it is not sensitive enough to perform well on thedetection task with a small dataset.
Qualitatively,we found SGNS to be most useful for discoveringnew shifts and visualizing changes (e.g., Figure 1),while SVD was most effective for detecting subtleshifts in usage.4 Statistical laws of semantic changeWe now show how diachronic embeddings can beused in a large-scale cross-linguistic analysis to re-veal statistical laws that relate frequency and pol-ysemy to semantic change.
In particular, we ana-lyze how a word?s rate of semantic change,?
(t)(wi) = cos-dist(w(t)i,w(t+1)i) (6)depends on its frequency, f(t)(wi) and a measureof its polysemy, d(t)(wi) (defined in Section 4.4).4.1 SetupWe present results using SVD embeddings(though analogous results were found to hold withSGNS).
Using all four languages and all fourconditions for English (ENGALL, ENGFIC, andCOHA with and without lemmatization), we per-formed regression analysis on rates of seman-tic change, ?
(t)(wi); thus, we examined onedata-point per word for each pair of consecutivedecades and analyzed how a word?s frequencyand polysemy at time t correlate with its degreeof semantic displacement over the next decade.To ensure the robustness of our results, we ana-lyzed only the top-10000 non?stop words by aver-1494Top-10 most polysemous yet, always, even, little, called, also, sometimes, great, still, quiteTop-10 least polysemous photocopying, retrieval, thirties, mom, sweater, forties, seventeenth,fifteenth, holster, postageTable 6: The top-10 most and least polysemous words in the ENGFIC data.
Words like yet, even, and still are used in manydiverse ways and are highly polysemous.
In contrast, words like photocopying, postage, and holster tend to be used in veryspecific well-clustered contexts, corresponding to a single sense; for example, mail and letter are both very likely to occur inthe context of postage and are also likely to co-occur with each other, independent of postage.a bFigure 2: Higher frequency words have lower rates of change (a), while polysemous words have higher rates of change (b).The negative curvature for polysemy?which is significant only at high d(wi)?varies across datasets and was not present withSGNS, so it is not as robust as the clear linear trend that was seen with all methods and across all datasets.
The trendlines show95% CIs from bootstrapped kernel regressions on the ENGALL data (Li and Racine, 2007).age historical frequency (lower-frequency wordstend to lack sufficient co-occurrence data acrossyears) and we discarded proper nouns (changes inproper noun usage are primarily driven by non-linguistic factors, e.g.
historical events, Traugottand Dasher, 2001).
We also log-transformed thesemantic displacement scores and normalized thescores to have zero mean and unit variance; wedenote these normalized scores by??
(t)(wi).We performed our analysis using a linear mixedmodel with random intercepts per word and fixedeffects per decade; i.e., we fit ?f, ?d, and ?ts.t.??
(t)(wi) = ?flog(f(t)(wi))+?dlog(d(t)(wi))+ ?t+ zwi+ (t)wi?wi?
V, t ?
{t0, ..., tn}, (7)where zwi?
N (0, ?wi) is the random interceptfor word wiand (t)wi?
N (0, ?)
is an error term.
?f, ?dand ?tcorrespond to the fixed effects forfrequency, polysemy and the decade t, respec-tively8.
Intuitively, this model estimates the effectsof frequency and polysemy on semantic change,while controlling for temporal trends and correct-ing for the fact that measurements on same wordwill be correlated across time.
We fit (7) using thestandard restricted maximum likelihood algorithm(McCulloch and Neuhaus, 2001; Appendix C).8Note that time is treated as a categorical variable, as eachdecade has its own fixed effect.4.2 Overview of resultsWe find that, across languages, rates of semanticchange obey a scaling relation of the form?
(wi) ?
f(wi)?f?
d(wi)?d, (8)with ?f< 0 and ?d> 0.
This finding implies thatfrequent words change at slower rates while pol-ysemous words change faster, and that both theserelations scale as power laws.4.3 Law of conformity: Frequently usedwords change at slower ratesUsing the model in equation (7), we found thatthe logarithm of a word?s frequency, log(f(wi)),has a significant and substantial negative effect onrates of semantic change in all settings (Figures 2aand 3a).
Given the use of log-transforms in pre-processing the data this implies rates of semanticchange are proportional to a negative power (?f)of frequency, i.e.?
(wi) ?
f(wi)?f, (9)with ?f?
[?1.26,?0.27] across lan-guages/datasets.
The relatively large rangeof values for ?fis due to the fact that the COHAdatasets are outliers due to their substantiallysmaller sample sizes (Figure 3; the range is?f?
[?0.66,?0.27] with COHA excluded).1495Figure 3: a, The estimated linear effect of log-frequency (?
?f) is significantly negative across all languages.
The effect issignificantly stronger in the COHA data, but this is likely due to its small sample size (?100?
smaller than the other datasets);the small sample size introduces random variance that may artificially inflate the effect of frequency.
From the COHA data,we also see that the result holds regardless of whether lemmatization is used.
b, Analogous trends hold for the linear effect ofthe polysemy score (?
?d), which is strong and significantly positive across all conditions.
Again, we see that the smaller COHAdatasets are mild outliers.995% CIs are shown.4.4 Law of innovation: Polysemous wordschange at faster ratesThere is a common hypothesis in the linguistic lit-erature that ?words become semantically extendedby being used in diverse contexts?
(Winter et al,2014), an idea that dates back to the writings ofBr?eal (1897).
We tested this notion by examiningthe relationship between polysemy and semanticchange in our data.Quantifying polysemyMeasuring word polysemy is a difficult andfraught task, as even ?ground truth?
dictionariesdiffer in the number of senses they assign to words(Simpson et al, 1989; Fellbaum, 1998).
We cir-cumvent this issue by measuring a word?s contex-tual diversity as a proxy for its polysemousness.The intuition behind our measure is that wordsthat occur in many distinct, unrelated contexts willtend to be highly polysemous.
This view of pol-ysemy also fits with previous work on semanticchange, which emphasizes the role of contextualdiversity (Br?eal, 1897; Winter et al, 2014).We measure a word?s contextual diversity, andthus polysemy, by examining its neighborhood inan empirical co-occurrence network.
We con-struct empirical co-occurrence networks using thePPMI measure defined in Section 2.
In these net-works words are connected to each other if theyco-occur more than one would expect by chance(after smoothing).
The polysemy of a word is thenmeasured as its local clustering coefficient within9The COHA data is ?100?
smaller, which has a globaleffect on the construction of the co-occurrence network (e.g.,lower average degree) used to compute polysemy scores.this network (Watts and Strogatz, 1998):d(wi) = ?
?ci,cj?NPPMI(wi)I {PPMI(ci, cj) > 0}|NPPMI(wi)|(|NPPMI(wi)| ?
1),(10)where NPPMI(wi) = {wj: PPMI(wi, wj) > 0}.This measure counts the proportion of wi?s neigh-bors that are also neighbors of each other.
Accord-ing to this measure, a word will have a high clus-tering coefficient (and thus a low polysemy score)if the words that it co-occurs with also tend to co-occur with each other.
Polysemous words that arecontextually diverse will have low clustering co-efficients, since they appear in disjointed or unre-lated contexts.Variants of this measure are often used in word-sense discrimination and correlate with, e.g., num-ber of senses in WordNet (Dorow and Widdows,2003; Ferret, 2004).
However, we found thatit was slightly biased towards rating contextuallydiverse discourse function words (e.g., also) ashighly polysemous, which needs to be taken intoaccount when interpreting our results.
We opted touse this measure, despite this bias, because it hasthe strong benefit of being clearly interpretable: itsimply measures the extent to which a word ap-pears in diverse textual contexts.
Table 6 gives ex-amples of the least and most polysemous words inthe ENGFIC data, according to this score.As expected, this measure has significant intrin-sic positive correlation with frequency.
Acrossdatasets, we found Pearson correlations in therange 0.45 < r < 0.8 (all p < 0.05), confirm-ing frequent words tend to be used in a greater di-versity of contexts.
As a consequence of this highcorrelation, we interpret the effect of this measureonly after controlling for frequency (this control isnaturally captured in equation (7)).1496Polysemy and semantic changeAfter fitting the model in equation (7), we foundthat the logarithm of the polysemy score exhibits astrong positive effect on rates of semantic change,throughout all four languages (Figure 3b).
As withfrequency, the relation takes the form of a powerlaw?
(wi) ?
d(wi)?d, (11)with a language/corpus dependent scaling constantin ?d?
[0.37, 0.77].
Note that this relation-ship is a complete reversal from what one wouldexpect according to d(wi)?s positive correlationwith frequency; i.e., since frequency and poly-semy are highly positively correlated, one wouldexpect them to have similar effects on seman-tic change, but we found that the effect of poly-semy completely reversed after controlling for fre-quency.
Figure 2b shows the relationship of pol-ysemy with rates of semantic change in the EN-GALL data after regressing out effect of frequency(using the method of Graham, 2003).5 DiscussionWe show how distributional methods can revealstatistical laws of semantic change and offer a ro-bust methodology for future work in this area.Our work builds upon a wealth of previousresearch on quantitative approaches to semanticchange, including prior work with distributionalmethods (Sagi et al, 2011; Wijaya and Yeniterzi,2011; Gulordava and Baroni, 2011; Jatowt andDuh, 2014; Kulkarni et al, 2014; Xu and Kemp,2015), as well as recent work on detecting theemergence of novel word senses (Lau et al, 2012;Mitra et al, 2014; Cook et al, 2014; Mitra et al,2015; Frermann and Lapata, 2016).
We extendthese lines of work by rigorously comparing dif-ferent approaches to quantifying semantic changeand by using these methods to propose new statis-tical laws of semantic change.The two statistical laws we propose have strongimplications for future work in historical seman-tics.
The law of conformity?frequent wordschange more slowly?clarifies frequency?s rolein semantic change.
Future studies of semanticchange must account for frequency?s conformingeffect: when examining the interaction betweensome linguistic process and semantic change, thelaw of conformity should serve as a null model inwhich the interaction is driven primarily by under-lying frequency effects.The law of innovation?polysemous wordschange more quickly?quantifies the central rolepolysemy plays in semantic change, an issue thathas concerned linguists for more than 100 years(Br?eal, 1897).
Previous works argued that seman-tic change leads to polysemy (Wilkins, 1993; Hop-per and Traugott, 2003).
However, our resultsshow that polysemous words change faster, whichsuggests that polysemy may actually lead to se-mantic change.Overall, these two factors?frequency andpolysemy?explain between 48% and 88% of thevariance10in rates of semantic change (across con-ditions).
This remarkable degree of explanatorypower indicates that frequency and polysemy areperhaps the two most crucial linguistic factors thatexplain rates of semantic change over time.These empirical statistical laws also lend them-selves to various causal mechanisms.
The lawof conformity might be a consequence of learn-ing: perhaps people are more likely to use rarewords mistakenly in novel ways, a mechanism for-malizable by Bayesian models of word learningand corresponding to the biological notion of ge-netic drift (Reali and Griffiths, 2010).
Or per-haps a sociocultural conformity bias makes peopleless likely to accept novel innovations of commonwords, a mechanism analogous to the biologicalprocess of purifying selection (Boyd and Richer-son, 1988; Pagel et al, 2007).
Moreover, suchmechanisms may also be partially responsible forthe law of innovation.
Highly polysemous wordstend to have more rare senses (Kilgarriff, 2004),and rare senses may be unstable by the law of con-formity.
While our results cannot confirm suchcausal links, they nonetheless highlight a new rolefor frequency and polysemy in language changeand the importance of distributional models in his-torical research.AcknowledgmentsThe authors thank D. Friedman, R. Sosic, C. Man-ning, V. Prabhakaran, and S. Todd for their helpfulcomments and discussions.
We are also indebtedto our anonymous reviewers.
W.H.
was supportedby an NSERC PGS-D grant and the SAP StanfordGraduate Fellowship.
W.H., D.J., and J.L.
weresupported by the Stanford Data Science Initiative,and NSF Awards IIS-1514268, IIS-1149837, andIIS-1159679.10Marginal R2(Nakagawa and Schielzeth, 2013).1497ReferencesJames S. Adelman, Gordon D. A.
Brown, and Jos?e F.Quesada.
2006.
Contextual diversity, not word fre-quency, determines word-naming and lexical deci-sion times.
Psychol.
Sci., 17(9):814?823.Steven Bird, Ewan Klein, and Edward Loper.
2009.Natural language processing with Python.
O?ReillyMedia, Inc.Andreas Blank.
1999.
Why do new meanings occur?A cognitive typology of the motivations for lexicalsemantic change.
In Peter Koch and Andreas Blank,editors, Historical Semantics and Cognition.
Walterde Gruyter, Berlin, Germany.Robert Boyd and Peter J Richerson.
1988.
Culture andthe Evolutionary Process.
University of ChicagoPress, Chicago, IL.Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-Khanh Tran.
2012.
Distributional semantics in tech-nicolor.
In Proc.
ACL, pages 136?145.Michel Br?eal.
1897.
Essai de S?emantique: Science dessignifications.
Hachette, Paris, France.John A. Bullinaria and Joseph P. Levy.
2007.
Ex-tracting semantic representations from word co-occurrence statistics: A computational study.
Behav.Res.
Methods, 39(3):510?526.John A. Bullinaria and Joseph P. Levy.
2012.
Ex-tracting semantic representations from word co-occurrence statistics: stop-lists, stemming, andSVD.
Behav.
Res.
Methods, 44(3):890?907.J.L.
Bybee.
2007.
Frequency of Use And the Organi-zation of Language.
Oxford University Press, NewYork City, NY.Paul Cook, Jey Han Lau, Diana McCarthy, and Timo-thy Baldwin.
2014.
Novel Word-sense Identifica-tion.
In Proc.
COLING, pages 1624?1635.Scott Crossley, Tom Salsbury, and Danielle McNa-mara.
2010.
The development of polysemy andfrequency use in english second language speakers.Language Learning, 60(3):573?605.Mark Davies.
2010.
The Corpus of HistoricalAmerican English: 400 million words, 1810-2009.http://corpus.byu.edu/coha/.Beate Dorow and Dominic Widdows.
2003.
Discov-ering corpus-specific word senses.
In Proc.
EACL,pages 79?82.Christiane Fellbaum.
1998.
WordNet.
Wiley OnlineLibrary.Olivier Ferret.
2004.
Discovering word senses froma network of lexical cooccurrences.
In Proc.
COL-ING, page 1326.J.R.
Firth.
1957.
A Synopsis of Linguistic Theory,1930-1955.
In Studies in Linguistic Analysis.
Spe-cial volume of the Philological Society.
Basil Black-well, Oxford, UK.Lea Frermann and Mirella Lapata.
2016.
A BayesianModel of Diachronic Meaning Change.
Trans.
ACL,4:31?45.Dirk Geeraerts.
1997.
Diachronic Prototype Se-mantics: A Contribution to Historical Lexicology.Clarendon Press, Oxford, UK.Michael H. Graham.
2003.
Confronting multi-collinearity in ecological multiple regression.
Ecol-ogy, 84(11):2809?2815.Kristina Gulordava and Marco Baroni.
2011.
A dis-tributional similarity approach to the detection ofsemantic change in the Google Books Ngram cor-pus.
In Proc.
GEMS 2011 Workshop on Geometri-cal Models of Natural Language Semantics, pages67?71.
Association for Computational Linguistics.Zellig S. Harris.
1954.
Distributional structure.
Word,10:146?162.Paul J. Hopper and Elizabeth Closs Traugott.
2003.Grammaticalization.
Cambridge University Press,Cambridge, UK.Adam Jatowt and Kevin Duh.
2014.
A frameworkfor analyzing semantic change of words across time.In Proc.
ACM/IEEE-CS Conf.
on Digital Libraries,pages 229?238.
IEEE Press.R.
Jeffers and Ilse Lehiste.
1979.
Principles and Meth-ods for Historical Linguistics.
MIT Press, Cam-bridge, MA.Adam Kilgarriff.
2004.
How dominant is the common-est sense of a word?
In Text, Speech and Dialogue,pages 103?111.
Springer.Yoon Kim, Yi-I.
Chiu, Kentaro Hanaki, DarshanHegde, and Slav Petrov.
2014.
Temporal analysisof language through neural language models.
arXivpreprint arXiv:1405.3515.Vivek Kulkarni, Rami Al-Rfou, Bryan Perozzi, andSteven Skiena.
2014.
Statistically significant de-tection of linguistic change.
In Proc.
WWW, pages625?635.Thomas K. Landauer and Susan T. Dumais.
1997.A solution to Plato?s problem: The latent semanticanalysis theory of acquisition, induction, and repre-sentation of knowledge.
Psychol.
Rev., 104(2):211.Jey Han Lau, Paul Cook, Diana McCarthy, David New-man, and Timothy Baldwin.
2012.
Word sense in-duction for novel sense detection.
In Proc.
EACL,pages 591?601.Omer Levy, Yoav Goldberg, and Ido Dagan.
2015.
Im-proving distributional similarity with lessons learnedfrom word embeddings.
Trans.
ACL, 3.1498Qi Li and Jeffrey Scott Racine.
2007.
Nonparametriceconometrics: theory and practice.
Princeton Uni-versity Press, Princeton, NJ.Erez Lieberman, Jean-Baptiste Michel, Joe Jackson,Tina Tang, and Martin A. Nowak.
2007.
Quantify-ing the evolutionary dynamics of language.
Nature,449(7163):713?716.Yuri Lin, Jean-Baptiste Michel, Erez Lieberman Aiden,Jon Orwant, Will Brockman, and Slav Petrov.
2012.Syntactic annotations for the google books ngramcorpus.
In Proc.
ACL, System Demonstrations,pages 169?174.Charles E McCulloch and John M Neuhaus.
2001.Generalized linear mixed models.
Wiley-Interscience, Hoboken, NJ.Jean-Baptiste Michel, Yuan Kui Shen, Aviva PresserAiden, Adrian Veres, Matthew K. Gray, Joseph P.Pickett, Dale Hoiberg, Dan Clancy, Peter Norvig,Jon Orwant, and others.
2011.
Quantitative analysisof culture using millions of digitized books.
Sci-ence, 331(6014):176?182.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-rado, and Jeff Dean.
2013.
Distributed representa-tions of words and phrases and their compositional-ity.
In Advances in Neural Information ProcessingSystems, pages 3111?3119.Sunny Mitra, Ritwik Mitra, Martin Riedl, Chris Bie-mann, Animesh Mukherjee, and Pawan Goyal.2014.
That?s sick dude!
: Automatic identificationof word sense change across different timescales.
InProc.
ACL.Sunny Mitra, Ritwik Mitra, Suman Kalyan Maity,Martin Riedl, Chris Biemann, Pawan Goyal, andAnimesh Mukherjee.
2015.
An automatic ap-proach to identify word sense changes in text mediaacross timescales.
Natural Language Engineering,21(05):773?798.Shinichi Nakagawa and Holger Schielzeth.
2013.
Ageneral and simple method for obtaining R2fromgeneralized linear mixed-effects models.
MethodsEcol.
Evol., 4(2):133?142.Mark Pagel, Quentin D. Atkinson, and Andrew Meade.2007.
Frequency of word-use predicts rates oflexical evolution throughout Indo-European history.Nature, 449(7163):717?720.Eitan Adam Pechenick, Christopher M. Danforth, andPeter Sheridan Dodds.
2015.
Characterizing theGoogle Books corpus: Strong limits to inferences ofsocio-cultural and linguistic evolution.
PLoS ONE,10(10).F.
Reali and T. L. Griffiths.
2010.
Words as alle-les: connecting language evolution with Bayesianlearners to models of genetic drift.
Proc.
R. Soc.B, 277(1680):429?436.Eyal Sagi, Stefan Kaufmann, and Brady Clark.
2011.Tracing semantic change with latent semantic analy-sis.
In Kathryn Allan and Justyna A. Robinson, edi-tors, Current Methods in Historical Semantics, page161.
De Gruyter Mouton, Berlin, Germany.Ben?oit Sagot, Lionel Cl?ement, Eric de La Clergerie,and Pierre Boullier.
2006.
The Lefff 2 syntacticlexicon for French: architecture, acquisition, use.
InProc.
LREC, pages 1?4.Gerold Schneider and Martin Volk.
1998.
Addingmanual constraints and lexical look-up to a Brill-tagger for German.
In Proceedings of the ESSLLI-98 Workshop on Recent Advances in Corpus Anno-tation, Saarbr?ucken.Peter H Sch?onemann.
1966.
A generalized solution ofthe orthogonal Procrustes problem.
Psychometrika,31(1):1?10.J.S.
Seabold and J. Perktold.
2010.
Statsmodels:Econometric and statistical modeling with python.In Proc.
9th Python in Science Conference.John Andrew Simpson, Edmund SC Weiner, et al1989.
The Oxford English Dictionary, volume 2.Clarendon Press Oxford, Oxford, UK.Elizabeth Closs Traugott and Richard B Dasher.
2001.Regularity in Semantic Change.
Cambridge Univer-sity Press, Cambridge, UK.Peter D. Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of seman-tics.
J. Artif.
Intell.
Res., 37(1):141?188.S.
Ullmann.
1962.
Semantics: An Introduction to theScience of Meaning.
Barnes & Noble, New YorkCity, NY.Laurens Van der Maaten and Geoffrey Hinton.
2008.Visualizing data using t-SNE.
Journal of MachineLearning Research, 9(2579-2605):85.Duncan J Watts and Steven H Strogatz.
1998.
Col-lective dynamics of ?small-world?networks.
Nature,393(6684):440?442.Derry Tanti Wijaya and Reyyan Yeniterzi.
2011.
Un-derstanding semantic change of words over cen-turies.
In Proc.
Workshop on Detecting and Exploit-ing Cultural Diversity on the Social Web, pages 35?40.
ACM.David P Wilkins.
1993.
From part to person: Natu-ral tendencies of semantic change and the search forcognates.
Cognitive Anthropology Research Groupat the Max Planck Institute for Psycholinguistics.B.
Winter, Graham Thompson, and Matthias Urban.2014.
Cognitive Factors Motivating The EvolutionOf Word Meanings: Evidence From Corpora, Be-havioral Data And Encyclopedic Network Structure.In Proc.
EVOLANG, pages 353?360.1499Yang Xu and Charles Kemp.
2015.
A computationalevaluation of two laws of semantic change.
In Proc.Annual Conf.
of the Cognitive Science Society.Yang Xu, Terry Regier, and Barbara C. Malt.
2015.Historical Semantic Chaining and Efficient Commu-nication: The Case of Container Names.
CognitiveScience.Naiwen Xue, Fei Xia, Fu-Dong Chiou, and MartaPalmer.
2005.
The Penn Chinese TreeBank: Phrasestructure annotation of a large corpus.
Natural lan-guage engineering, 11(02):207?238.George Kingsley Zipf.
1945.
The meaning-frequencyrelationship of words.
J. Gen.
Psychol., 33(2):251?256.Bahar?Ilgen and Bahar Karaoglan.
2007.
Investiga-tion of Zipf?s ?law-of-meaning?on Turkish corpora.In International Symposium on Computer and Infor-mation Sciences, pages 1?6.
IEEE.A Hyperparameter and pre-processingdetailsFor all datasets, words were lowercased andstripped of punctuation.
For the Google datasetswe built models using the top-100000 words bytheir average frequency over the entire histori-cal time-periods, and we used the top-50000 forCOHA.
During model learning we also discardedall words within a year that occurred below a cer-tain threshold (500 for the Google data, 100 for theCOHA data).For all methods, we used the hyperparametersrecommended in Levy et al (2015).
For the con-text word distributions in all methods, we usedcontext distribution smoothing with a smoothingparameter of 0.75.
Note that for SGNS this cor-responds to smoothing the unigram negative sam-pling distribution.
For both, SGNS and PPMI, weset the negative sample prior ?
= log(5), while weset this value to ?
= 0 for SVD, as this improvedresults.
When using SGNS on the Google data,we also subsampled, with words being random re-moved with probability pr(wi) = 1 ?
?10?5f(wi), asrecommended by Levy et al (2015) and Mikolovet al (2013).
Furthermore, to improve the com-putational efficiency of SGNS (which works withtext streams and not co-occurrence counts), wedownsampled the larger years in the Google N-Gram data to have at most 109tokens.
No suchsubsampling was performed on the COHA data.For all methods, we defined the context set tosimply be the same vocabulary as the target words,as is standard in most word vector applications(Levy et al, 2015).
However, we found that thePPMI method benefited substantially from largercontexts (similar results were found in Bullinariaand Levy, 2007), so we did not remove any low-frequency words per year from the context for thatmethod.
The other embedding approaches did notappear to benefit from the inclusion of these low-frequency terms, so they were dropped for compu-tational efficiency.For SGNS, we used the implementation pro-vided in Levy et al (2015).
The implementationsfor PPMI and SVD are released with the codepackage associated with this work.B Visualization algorithmTo visualize semantic change for a word wiin twodimensions we employed the following procedure,which relies on the t-SNE embedding method(Van der Maaten and Hinton, 2008) as a subrou-tine:1.
Find the union of the word wi?s k nearestneighbors over all necessary time-points.2.
Compute the t-SNE embedding of thesewords on the most recent (i.e., the modern)time-point.3.
For each of the previous time-points, holdall embeddings fixed, except for the targetword?s (i.e., the embedding for wi), and op-timize a new t-SNE embedding only for thetarget word.
We found that initializing theembedding for the target word to be the cen-troid of its k?-nearest neighbors in a time-point was highly effective.Thus, in this procedure the background words arealways shown in their ?modern?
positions, whichmakes sense given that these are the current mean-ings of these words.
This approximation is neces-sary, since in reality all words are moving.C Regression analysis detailsIn addition to the pre-processing mentioned in themain text, we also normalized the contextual di-versity scores d(wi) within years by subtractingthe yearly median.
This was necessary becausethere was substantial changes in the median con-textual diversity scores over years due to changesin corpus sample sizes etc.
Data points corre-sponding to words that occurred less than 500times during a time-period were also discarded, as1500these points lack sufficient data to robustly esti-mate change rates (this threshold only came intoeffect on the COHA data, however).
We removedstop words and proper nouns by (i) removing allstop-words from the available lists in Python?sNLTK package (Bird et al, 2009) and (ii) re-stricting our analysis to words with part-of-speech(POS) tags corresponding to four main linguisticcategories (common nouns, verbs, adverbs, andadjectives), using the POS sources in Table 1.When analyzing the effects of frequency andcontextual diversity, the model contained fixed ef-fects for these features and for time along withrandom effects for word identity.
We opted notto control for POS tags in the presented results,as contextual diversity is co-linear with these tags(e.g., adverbs are more contextual diverse thannouns), and the goal was to demonstrate the maineffect of contextual diversity across all word types.That said, the effect of contextual diversity re-mained strong and significantly positive in alldatasets even after controlling for POS tags.To fit the linear mixed models, we usedthe Python statsmodels package with re-stricted maximum likelihood estimation (REML)(Seabold and Perktold, 2010).
All mentionedsignificance scores were computed according toWald?s z-tests, though these results agreed withBonferroni corrected likelihood ratio tests on theeng-all data.The visualizations in Figure 2 were computedon the eng-all data and correspond to boot-strapped locally-linear kernel regressions withbandwidths selected via the AIC Hurvitch criteria(Li and Racine, 2007).1501
