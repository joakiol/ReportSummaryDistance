Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1312?1320,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsIdentification of Speakers in NovelsHua He?
Denilson Barbosa ?
Grzegorz Kondrak?
?Department of Computer Science ?Department of Computing ScienceUniversity of Maryland University of Albertahuah@cs.umd.edu {denilson,gkondrak}@ualberta.caAbstractSpeaker identification is the task of at-tributing utterances to characters in a lit-erary narrative.
It is challenging to auto-mate because the speakers of the majorityof utterances are not explicitly identified innovels.
In this paper, we present a super-vised machine learning approach for thetask that incorporates several novel fea-tures.
The experimental results show thatour method is more accurate and generalthan previous approaches to the problem.1 IntroductionNovels are important as social communicationdocuments, in which novelists develop the plotby means of discourse between various charac-ters.
In spite of a frequently expressed opinionthat all novels are simply variations of a certainnumber of basic plots (Tobias, 2012), every novelhas a unique plot (or several plots) and a differentset of characters.
The interactions among charac-ters, especially in the form of conversations, helpthe readers construct a mental model of the plotand the changing relationships between charac-ters.
Many of the complexities of interpersonal re-lationships, such as romantic interests, family ties,and rivalries, are conveyed by utterances.A precondition for understanding the relation-ship between characters and plot development ina novel is the identification of speakers behind allutterances.
However, the majority of utterancesare not explicitly tagged with speaker names, asis the case in stage plays and film scripts.
In mostcases, authors rely instead on the readers?
compre-hension of the story and of the differences betweencharacters.Since manual annotation of novels is costly, asystem for automatically determining speakers ofutterances would facilitate other tasks related tothe processing of literary texts.
Speaker identifica-tion could also be applied on its own, for instancein generating high quality audio books without hu-man lectors, where each character would be iden-tifiable by a distinct way of speaking.
In addi-tion, research on spoken language processing forbroadcast and multi-party meetings (Salamin etal., 2010; Favre et al, 2009) has demonstrated thatthe analysis of dialogues is useful for the study ofsocial interactions.In this paper, we investigate the task of speakeridentification in novels.
Departing from previousapproaches, we develop a general system that canbe trained on relatively small annotated data sets,and subsequently applied to other novels for whichno annotation is available.
Since every novel hasits own set of characters, speaker identificationcannot be formulated as a straightforward taggingproblem with a universal set of fixed tags.
Instead,we adopt a ranking approach, which enables ourmodel to be applied to literary texts that are differ-ent from the ones it has been trained on.Our approach is grounded in a variety of fea-tures that are easily generalizable across differ-ent novels.
Rather than attempt to construct com-plete semantic models of the interactions, we ex-ploit lexical and syntactic clues in the text itself.We propose several novel features, including thespeaker alternation pattern, the presence of voca-tives in utterances, and unsupervised actor-topicfeatures that associate speakers with utterances onthe basis of their content.
Experimental evaluationshows that our approach not only outperforms thebaseline, but also compares favorably to previousapproaches in terms of accuracy and generality,even when tested on novels and authors that aredifferent from those used for training.The paper is organized as follows.
After dis-cussing previous work, and defining the terminol-ogy, we present our approach and the features thatit is based on.
Next, we describe the data, the an-1312notation details, and the results of our experimen-tal evaluation.
At the end, we discuss an applica-tion to extracting a set of family relationships froma novel.2 Related WorkPrevious work on speaker identification includesboth rule-based and machine-learning approaches.Glass and Bangay (2007) propose a rule gener-alization method with a scoring scheme that fo-cuses on the speech verbs.
The verbs, such assaid and cried, are extracted from the communi-cation category of WordNet (Miller, 1995).
Thespeech-verb-actor pattern is applied to the utter-ance, and the speaker is chosen from the avail-able candidates on the basis of a scoring scheme.Sarmento and Nunes (2009) present a similar ap-proach for extracting speech quotes from onlinenews texts.
They manually define 19 variations offrequent speaker patterns, and identify a total of35 candidate speech verbs.
The rule-based meth-ods are typically characterized by low coverage,and are too brittle to be reliably applied to differ-ent domains and changing styles.Elson and McKeown (2010) (henceforth re-ferred to as EM2010) apply the supervised ma-chine learning paradigm to a corpus of utterancesextracted from novels.
They construct a singlefeature vector for each pair of an utterance anda speaker candidate, and experiment with variousWEKA classifiers and score-combination meth-ods.
To identify the speaker of a given utterance,they assume that all previous utterances are al-ready correctly assigned to their speakers.
Ourapproach differs in considering the utterances ina sequence, rather than independently from eachother, and in removing the unrealistic assumptionthat the previous utterances are correctly identi-fied.The speaker identification task has also been in-vestigated in other domains.
Bethard et al (2004)identify opinion holders by using semantic pars-ing techniques with additional linguistic features.Pouliquen et al (2007) aim at detecting directspeech quotations in multilingual news.
Krestelet al (2008) automatically tag speech sentencesin newspaper articles.
Finally, Ruppenhofer et al(2010) implement a rule-based system to enrichGerman cabinet protocols with automatic speakerattribution.3 Definitions and ConventionsIn this section, we introduce the terminology usedin the remainder of the paper.
Our definitions aredifferent from those of EM2010 partly because wedeveloped our method independently, and partlybecause we disagree with some of their choices.The examples are from Jane Austen?s Pride andPrejudice, which was the source of our develop-ment set.An utterance is a connected text that can be at-tributed to a single speaker.
Our task is to associateeach utterance with a single speaker.
Utterancesthat are attributable to more than one speaker arerare; in such cases, we accept correctly identifyingone of the speakers as sufficient.
In some cases, anutterance may include more than one quotation-delimited sequence of words, as in the followingexample.
?Miss Bingley told me,?
said Jane, ?thathe never speaks much.
?In this case, the words said Jane are simply aspeaker tag inserted into the middle of the quotedsentence.
Unlike EM2010, we consider this a sin-gle utterance, rather than two separate ones.We assume that all utterances within a para-graph can be attributed to a single speaker.
This?one speaker per paragraph?
property is rarely vi-olated in novels ?
we identified only five suchcases in Pride & Prejudice, usually involving onecharacter citing another, or characters reading let-ters containing quotations.
We consider this anacceptable simplification, much like assigning asingle part of speech to each word in a corpus.We further assume that each utterance is containedwithin a single paragraph.
Exceptions to this rulecan be easily identified and resolved by detectingquotation marks and other typographical conven-tions.The paragraphs without any quotations are re-ferred to as narratives.
The term dialogue denotesa series of utterances together with related narra-tives, which provide the context of conversations.We define a dialogue as a series of utterances andintervening narratives, with no more than threecontinuous narratives.
The rationale here is thatmore than three narratives without any utterancesare likely to signal the end of a particular dialogue.We distinguish three types of utterances, whichare listed with examples in Table 1: explicitspeaker (identified by name within the paragraph),1313Category ExampleImplicitspeaker?Don?t keep coughing so, Kitty,for heaven?s sake!
?Explicitspeaker?I do not cough for my ownamusement,?
replied Kitty.Anaphoricspeaker?Kitty has no discretion in hercoughs,?
said her father.Table 1: Three types of utterances.anaphoric speaker (identified by an anaphoric ex-pression), and implicit speaker (no speaker infor-mation within the paragraph).
Typically, the ma-jority of utterances belong to the implicit-speakercategory.
In Pride & Prejudice only roughly 25%of the utterances have explicit speakers, and aneven smaller 15% belong to the anaphoric-speakercategory.
In modern fiction, the percentage of ex-plicit attributions is even lower.4 Speaker IdentificationIn this section, we describe our method of extract-ing explicit speakers, and our ranking approach,which is designed to capture the speaker alterna-tion pattern.4.1 Extracting SpeakersWe extract explicit speakers by focusing on thespeech verbs that appear before, after, or betweenquotations.
The following verbs cover most casesin our development data: say, speak, talk, ask, re-ply, answer, add, continue, go on, cry, sigh, andthink.
If a verb from the above short list cannot befound, any verb that is preceded by a name or apersonal pronoun in the vicinity of the utterance isselected as the speech verb.In order to locate the speaker?s name oranaphoric expression, we apply a deterministicmethod based on syntactic rules.
First, all para-graphs that include narrations are parsed with adependency parser.
For example, consider the fol-lowing paragraph:As they went downstairs together, Char-lotte said, ?I shall depend on hearingfrom you very often, Eliza.
?The parser identifies a number of dependency rela-tions in the text, such as dobj(went-3, downstairs-4) and advmod(went-3, together-5).
Our methodextracts the speaker?s name from the dependencyrelation nsubj(said-8, Charlotte-7), which links aspeech verb with a noun phrase that is the syntac-tic subject of a clause.Once an explicit speaker?s name or an anaphoricexpression is located, we determine the corre-sponding gender information by referring to thecharacter list or by following straightforward rulesto handle the anaphora.
For example, if the utter-ance is followed by the phrase she said, we inferthat the gender of the speaker is female.4.2 Ranking ModelIn spite of the highly sequential nature of thechains of utterances, the speaker identification taskis difficult to model as sequential prediction.
Theprincipal problem is that, unlike in many NLPproblems, a general fixed tag set cannot be de-fined beyond the level of an individual novel.Since we aim at a system that could be applied toany novel with minimal pre-processing, sequentialprediction algorithms such as Conditional Ran-dom Fields are not directly applicable.We propose a more flexible approach that as-signs scores to candidate speakers for each utter-ance.
Although the sequential information is notdirectly modeled with tags, our system is ableto indirectly utilize the speaker alternation pat-tern using the method described in the followingsection.
We implement our approach with SVM-rank (Joachims, 2006).4.3 Speaker Alternation PatternThe speaker alternation pattern is often employedby authors in dialogues between two charac-ters.
After the speakers are identified explicitly atthe beginning of a dialogue, the remaining odd-numbered and even-numbered utterances are at-tributable to the first and second speaker, respec-tively.
If one of the speakers ?misses their turn?, aclue is provided in the text to reset the pattern.Based on the speaker alternation pattern, wemake the following two observations:1.
The speakers of consecutive utterances areusually different.2.
The speaker of the n-th utterance in a dia-logue is likely to be the same as the speakerof the (n?
2)-th utterance.Our ranking model incorporates the speaker al-ternation pattern by utilizing a feature expansionscheme.
For each utterance n, we first gener-ate its own features (described in Section 5), and1314Features NoveltyDistance to Utterance NoSpeaker Appearance Count NoSpeaker Name in Utterance NoUnsupervised Actor-Topic Model YesVocative Speaker Name YesNeighboring Utterances YesGender Matching YesPresence Matching YesTable 2: Principal feature sets.subsequently we add three more feature sets thatrepresent the following neighboring utterances:n?
2, n?
1 and n+1.
Informally, the features ofthe utterances n?
1 and n+1 encode the first ob-servation, while the features representing the utter-ance n ?
2 encode the second observation.
In ad-dition, we include a set of four binary features thatare set for the utterances in the range [n?2, n+1]if the corresponding explicit speaker matches thecandidate speaker of the current utterance.5 FeaturesIn this section, we describe the set of features usedin our ranking approach.
The principal feature setsare listed in Table 2, together with an indicationwhether they are novel or have been used in previ-ous work.5.1 Basic FeaturesA subset of our features correspond to the featuresthat were proposed by EM2010.
These are mostlyfeatures related to speaker names.
For example,since names of speakers are often mentioned inthe vicinity of their utterances, we count the num-ber of words separating the utterance and a namemention.
However, unlike EM2010, we consideronly the two nearest characters in each direction,to reflect the observation that speakers tend to bementioned by name immediately before or aftertheir corresponding utterances.
Another feature isused to represent the number of appearances forspeaker candidates.
This feature reflects the rela-tive importance of a given character in the novel.Finally, we use a feature to indicate the presenceor absence of a candidate speaker?s name withinthe utterance.
The intuition is that speakers areunlikely to mention their own name.Feature Examplestart of utterance ?Kitty .
.
.before period .
.
.
Jane.between commas .
.
.
, Elizabeth, .
.
.between comma & period .
.
.
, Mrs. Hurst.before exclamation mark .
.
.
Mrs. Bennet!before question mark .
.
.
Lizzy?.
.
.vocative phrase Dear .
.
.after vocative phrase Oh!
Lydia .
.
.2nd person pronoun .
.
.
you .
.
.Table 3: Features for the vocative identification.5.2 VocativesWe propose a novel vocative feature, which en-codes the character that is explicitly addressed inan utterance.
For example, consider the followingutterance:?I hope Mr. Bingley will like it, Lizzy.
?Intuitively, the speaker of the utterance is neitherMr.
Bingley nor Lizzy; however, the speaker of thenext utterance is likely to be Lizzy.
We aim at cap-turing this intuition by identifying the addressee ofthe utterance.We manually annotated vocatives in about 900utterances from the training set.
About 25% ofthe names within utterance were tagged as voca-tives.
A Logistic Regression classifier (Agresti,2006) was trained to identify the vocatives.
Theclassifier features are shown in Table 3.
The fea-tures are designed to capture punctuation context,as well as the presence of typical phrases that ac-company vocatives.
We also incorporate interjec-tions like ?oh!?
and fixed phrases like ?my dear?,which are strong indicators of vocatives.
Under10-fold cross validation, the model achieved an F-measure of 93.5% on the training set.We incorporate vocatives in our speaker identi-fication system by means of three binary featuresthat correspond to the utterances n?
1, n?
2, andn ?
3.
The features are set if the detected voca-tive matches the candidate speaker of the currentutterance n.5.3 Matching FeaturesWe incorporate two binary features for indicatingthe gender and the presence of a candidate speaker.The gender matching feature encodes the genderagreement between a speaker candidate and thespeaker of the current utterance.
The gender in-formation extraction is applied to two utterance1315groups: the anaphoric-speaker utterances, and theexplicit-speaker utterances.
We use the techniquedescribed in Section 4.1 to determine the genderof a speaker of the current utterance.
In contrastwith EM2010, this is not a hard constraint.The presence matching feature indicateswhether a speaker candidate is a likely partic-ipant in a dialogue.
Each dialogue consists ofcontinuous utterance paragraphs together withneighboring narration paragraphs as defined inSection 3.
The feature is set for a given characterif its name or alias appears within the dialogue.5.4 Unsupervised Actor-Topic FeaturesThe final set of features is generated by the unsu-pervised actor-topic model (ACTM) (Celikyilmazet al, 2010), which requires no annotated train-ing data.
The ACTM, as shown in Figure 1, ex-tends the work of author-topic model in (Rosen-Zvi et al, 2010).
It can model dialogues in a lit-erary text, which take place between two or morespeakers conversing on different topics, as distri-butions over topics, which are also mixtures of theterm distributions associated with multiple speak-ers.
This follows the linguistic intuition that richcontextual information can be useful in under-standing dialogues.Figure 1: Graphical Representation of ACTM.The ACTM predicts the most likely speakers ofa given utterance by considering the content of anutterance and its surrounding contexts.
The Actor-Topic-Term probabilities are calculated by usingboth the relationship of utterances and the sur-rounding textual clues.
In our system, we utilizefour binary features that correspond to the four topranking positions from the ACTM model.Figure 2: Annotation Tool GUI.6 DataOur principal data set is derived from the textof Pride and Prejudice, with chapters 19?26 asthe test set, chapters 27?33 as the developmentset, and the remaining 46 chapters as the trainingset.
In order to ensure high-quality speaker anno-tations, we developed a graphical interface (Fig-ure 2), which displays the current utterance in con-text, and a list of characters in the novel.
After thespeaker is selected by clicking a button, the textis scrolled automatically, with the next utterancehighlighted in yellow.
The complete novel wasannotated by a student of English literature.
Theannotations are publicly available1.For the purpose of a generalization experiment,we also utilize a corpus of utterances from the19th and 20th century English novels compiled byEM2010.
The corpus differs from our data set inthree aspects.
First, as discussed in Section 3, wetreat all quoted text within a single paragraph asa single utterance, which reduces the total num-ber of utterances, and results in a more realisticreporting of accuracy.
Second, our data set in-cludes annotations for all utterances in the novel,as opposed to only a subset of utterances from sev-eral novels, which are not necessarily contiguous.Lastly, our annotations come from a single expert,while the annotations in the EM2010 corpus werecollected through Amazon?s Mechanical Turk, andfiltered by voting.
For example, out of 308 utter-ances from The Steppe, 244 are in fact annotated,which raises the question whether the discardedutterances tend to be more difficult to annotate.Table 4 shows the number of utterances in all1www.cs.ualberta.ca/?kondrak/austen1316IS AS ES TotalPride & P. (all) 663 292 305 1260Pride & P. (test) 65 29 32 126Emma 236 55 106 397The Steppe 93 39 112 244Table 4: The number of utterances in variousdata sets by the type (IS - Implicit Speaker; AS- Anaphoric Speaker; ES - Explicit Speaker).data sets.
We selected Jane Austen?s Emma asa different novel by the same author, and AntonChekhov?s The Steppe as a novel by a different au-thor for our generalization experiments.Since our goal is to match utterances to charac-ters rather than to name mentions, a preprocess-ing step is performed to produce a list of char-acters in the novel and their aliases.
For exam-ple, Elizabeth Bennet may be referred to as Liz,Lizzy, Miss Lizzy, Miss Bennet, Miss Eliza, andMiss Elizabeth Bennet.
We apply a name entitytagger, and then group the names into sets of char-acter aliases, together with their gender informa-tion.
The sets of aliases are typically small, exceptfor major characters, and can be compiled withthe help of web resources, such as Wikipedia, orstudy guides, such as CliffsNotesTM .
This pre-processing step could also be performed automati-cally using a canonicalization method (Andrews etal., 2012); however, since our focus is on speakeridentification, we decided to avoid introducing an-notation errors at this stage.Other preprocessing steps that are required forprocessing a new novel include standarizing thetypographical conventions, and performing POStagging, NER tagging, and dependency parsing.We utilize the Stanford tools (Toutanova et al,2003; Finkel et al, 2005; Marneffe et al, 2006).7 EvaluationIn this section, we describe experiments conductedto evaluate our speaker identification approach.We refer to our main model as NEIGHBORS, be-cause it incorporates features from the neighbor-ing utterances, as described in Section 4.3.
Incontrast, the INDIVIDUAL model relies only onfeatures from the current utterance.
In an at-tempt to reproduce the evaluation methodology ofEM2010, we also test the ORACLE model, whichhas access to the gold-standard information aboutthe speakers of eight neighboring utterances in thePride & P. Emma SteppeBASELINE 42.0 44.1 66.8INDIVIDUAL 77.8 67.3 74.2NEIGHBORS 82.5 74.8 80.3ORACLE 86.5 80.1 83.6Table 5: Speaker identification accuracy (in %) onPride & Prejudice, Emma, and The Steppe.range [n ?
4, n + 4].
Lastly, the BASELINE ap-proach selects the name that is the closest in thenarration, which is more accurate than the ?mostrecent name?
baseline.7.1 ResultsTable 5 shows the results of the models trained onannotated utterances from Pride & Prejudice onthree test sets.
As expected, the accuracy of alllearning models on the test set that comes fromthe same novel is higher than on unseen novels.However, in both cases, the drop in accuracy forthe NEIGHBORS model is less than 10%.Surprisingly, the accuracy is higher on TheSteppe than on Emma, even though the differ-ent writing style of Chekhov should make thetask more difficult for models trained on Austen?sprose.
The protagonists of The Steppe are mostlymale, and the few female characters rarely speakin the novel.
This renders our gender featurevirtually useless, and results in lower accuracyon anaphoric speakers than on explicit speakers.On the other hand, Chekhov prefers to mentionspeaker names in the dialogues (46% of utterancesare in the explicit-speaker category), which makeshis prose slightly easier in terms of speaker identi-fication.The relative order of the models is the sameon all three test sets, with the NEIGHBORSmodel consistently outperforming the INDIVID-UAL model, which indicates the importance ofcapturing the speaker alternation pattern.
The per-formance of the NEIGHBORS model is actuallycloser to the ORACLE model than to the INDIVID-UAL model.Table 6 shows the results on Emma brokendown according to the type of the utterance.
Un-surprisingly, the explicit speaker is the easiest cat-egory, with nearly perfect accuracy.
Both the IN-DIVIDUAL and the NEIGHBORS models do betteron anaphoric speakers than on implicit speakers,which is also expected.
However, it is not the1317IS AS ES TotalINDIVIDUAL 52.5 67.3 100.0 67.3NEIGHBORS 63.1 76.4 100.0 74.8ORACLE 74.2 69.1 99.1 80.1Table 6: Speaker identification accuracy (in %) onAusten?s Emma by the type of utterance.case for the ORACLE model.
We conjecture thatthe ORACLE model relies heavily on the neighbor-hood features (which are rarely wrong), and con-sequently tends to downplay the gender informa-tion, which is the only information extracted fromthe anaphora.
In addition, anaphoric speaker is theleast frequent of the three categories.Table 7 shows the results of an ablation studyperformed to investigate the relative importance offeatures.
The INDIVIDUAL model serves as thebase model from which we remove specific fea-tures.
All tested features appear to contribute tothe overall performance, with the distance featuresand the unsupervised actor-topic features havingthe most pronounced impact.
We conclude that theincorporation of the neighboring features, whichis responsible for the difference between the IN-DIVIDUAL and NEIGHBORS models, is similar interms of importance to our strongest textual fea-tures.Feature ImpactClosest Mention -6.3Unsupervised ACTM -5.6Name within Utterance -4.8Vocative -2.4Table 7: Results of feature ablation (in % accu-racy) on Pride & Prejudice.7.2 Comparison to EM2010In this section we analyze in more detail our re-sults on Emma and The Steppe against the pub-lished results of the state-of-the-art EM2010 sys-tem.
Recall that both novels form a part of thecorpus that was created by EM2010 for the devel-opment of their system.Direct comparison to EM2010 is difficult be-cause they compute the accuracy separately forseven different categories of utterances.
For eachcategory, they experiment with all combinationsof three different classifiers and four score com-bination methods, and report only the accuracyCharacterid name gender.
.
.9 Mr. Collins m10 Charlotte f11 Jane Bennet f12 Elizabeth Bennet f. .
.Relationfrom to type mode.
.
.10 9 husband explicit9 10 wife derived10 12 friend explicit12 10 friend derived11 12 sister explicit.
.
.Figure 3: Relational database with extracted socialnetwork.achieved by the best performing combination onthat category.
In addition, they utilize the groundtruth speaker information of the preceding utter-ances.
Therefore, their results are best comparedagainst our ORACLE approach.Unfortunately, EM2010 do not break down theirresults by novel.
They report the overall ac-curacy of 63% on both ?anaphora trigram?
(ouranaphoric speaker), and ?quote alone?
(similar toour implicit speaker).
If we combine the two cate-gories, the numbers corresponding to our NEIGH-BORS model are 65.6% on Emma and 64.4% onThe Steppe, while ORACLE achieves 73.2% and70.5%, respectively.
Even though a direct com-parison is not feasible, the numbers are remarkableconsidering the context of the experiment, whichstrongly favors the EM2010 system.8 Extracting Family RelationshipsIn this section, we describe an application ofthe speaker identification system to the extractionof family relationships.
Elson et al (2010) ex-tract unlabeled networks where the nodes repre-sent characters and edges indicate their proxim-ity, as indicated by their interactions.
Our goalis to construct networks in which edges are la-beled by the mutual relationships between charac-ters in a novel.
We focus on family relationships,but also include social relationships, such as friend1318INSERT INTO Relation (id1, id2, t, m)SELECT r.to AS id1, r.from AS id2 , ?wife?
AS t, ?derived?
AS mFROM Relation rWHERE r.type=?husband?
AND r.mode=?explicit?
ANDNOT EXISTS(SELECT * FROM Relation r2WHERE r2.from=r.to AND r2.to=r.from AND r2.type=t)Figure 4: An example inference rule.and attracted-to.Our approach to building a social network fromthe novel is to build an active database of relation-ships explicitly mentioned in the text, which is ex-panded by triggering the execution of queries thatdeduce implicit relations.
This inference processis repeated for every discovered relationship untilno new knowledge can be inferred.The following example illustrates how speakeridentification helps in the extraction of social re-lations among characters.
Consider, the followingconversation:?How so?
how can it affect them??
?My dear Mr. Bennet,?
replied his wife,?how can you be so tiresome!
?If the speakers are correctly identified, the utter-ances are attributed to Mr. Bennet and Mrs. Ben-net, respectively.
Furthermore, the second utter-ance implies that its speaker is the wife of the pre-ceding speaker.
This is an example of an explicitrelationship which is included in our database.Several similar extraction rules are used to extractexplicit mentions indicating family and affectiverelations, including mother, nephew, and fiancee.We can also derive relationships that are not ex-plicitly mentioned in the text; for example, thatMr.
Bennet is the husband of Mrs. Bennet.Figure 3 shows a snippet of the relationaldatabase of the network extracted from Pride &Prejudice.
Table Character contains all charactersin the book, each with a unique identifier and gen-der information, while Table Relation contains allrelationships that are explicitly mentioned in thetext or derived through reasoning.Figure 4 shows an example of an inference ruleused in our system.
The rule derives a new re-lationship indicating that character c1 is the wifeof character c2 if it is known (through an explicitmention in the text) that c2 is the husband of c1.One condition for the rule to be applied is that thedatabase must not already contain a record indi-cating the wife relationship.
This inference rulewould derive the tuple in Figure 3 indicating thatthe wife or Mr. Collins is Charlotte.In our experiment with Pride & Prejudice, a to-tal of 55 explicitly indicated relationships were au-tomatically identified once the utterances were at-tributed to the characters.
From those, another 57implicit relationships were derived through infer-ence.
A preliminary manual inspection of the setof relations extracted by this method (Makazhanovet al, 2012) indicates that all of them are correct,and include about 40% all personal relations thatcan be inferred by a human reader from the text ofthe novel.9 Conclusion and Future WorkWe have presented a novel approach to identifyingspeakers of utterances in novels.
Our system in-corporates a variety of novel features which utilizevocatives, unsupervised actor-topic models, andthe speaker alternation pattern.
The results of ourevaluation experiments indicate a substantial im-provement over the current state of the art.There are several interesting directions for thefuture work.
Although the approach introducedin this paper appears to be sufficiently general tohandle novels written in a different style and pe-riod, more sophisticated statistical graphical mod-els may achieve higher accuracy on this task.
A re-liable automatic generation of characters and theiraliases would remove the need for the preprocess-ing step outlined in Section 6.
The extraction ofsocial networks in novels that we discussed in Sec-tion 8 would benefit from the introduction of ad-ditional inference rules, and could be extended tocapture more subtle notions of sentiment or rela-tionship among characters, as well as their devel-opment over time.We have demonstrated that speaker identifica-tion can help extract family relationships, but theconverse is also true.
Consider the following utter-ance:?Lizzy,?
said her father, ?I have givenhim my consent.
?1319In order to deduce the speaker of the utterance,we need to combine the three pieces of informa-tion: (a) the utterance is addressed to Lizzy (voca-tive prediction), (b) the utterance is produced byLizzy?s father (pronoun resolution), and (c) Mr.Bennet is the father of Lizzy (relationship ex-traction).
Similarly, in the task of compiling alist of characters, which involves resolving aliasessuch as Caroline, Caroline Bingley, and Miss Bin-gley, simultaneous extraction of family relation-ships would help detect the ambiguity of MissBenett, which can refer to any of several sis-ters.
A joint approach to resolving speaker attri-bution, relationship extraction, co-reference reso-lution, and alias-to-character mapping would notonly improve the accuracy on all these tasks, butalso represent a step towards deeper understandingof complex plots and stories.AcknowledgmentsWe would like to thank Asli Celikyilmaz for col-laboration in the early stages of this project, Su-san Brown and Michelle Di Cintio for help withdata annotation, and David Elson for the attemptto compute the accuracy of the EM2010 systemon Pride & Prejudice.
This research was partiallysupported by the Natural Sciences and Engineer-ing Research Council of Canada.ReferencesAlan Agresti.
2006.
Building and applying logistic re-gression models.
In An Introduction to CategoricalData Analysis.
John Wiley & Sons, Inc.Nicholas Andrews, Jason Eisner, and Mark Dredze.2012.
Name phylogeny: A generative model ofstring variation.
In EMNLP-CoNLL.Steven Bethard, Hong Yu, Ashley Thornton, VasileiosHatzivassiloglou, and Dan Jurafsky.
2004.
Auto-matic extraction of opinion propositions and theirholders.
In AAAI Spring Symposium on ExploringAttitude and Affect in Text.Asli Celikyilmaz, Dilek Hakkani-Tur, Hua He, Grze-gorz Kondrak, and Denilson Barbosa.
2010.
Theactor-topic model for extracting social networks inliterary narrative.
In Proceedings of the NIPS 2010Workshop - Machine Learning for Social Comput-ing.David K. Elson and Kathleen McKeown.
2010.
Auto-matic attribution of quoted speech in literary narra-tive.
In AAAI.David K. Elson, Nicholas Dames, and Kathleen McKe-own.
2010.
Extracting social networks from literaryfiction.
In ACL.Sarah Favre, Alfred Dielmann, and Alessandro Vincia-relli.
2009.
Automatic role recognition in multi-party recordings using social networks and proba-bilistic sequential models.
In ACM Multimedia.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by gibbssampling.
In ACL.Kevin Glass and Shaun Bangay.
2007.
A naivesalience-based method for speaker identification infiction books.
In Proceedings of the 18th AnnualSymposium of the Pattern Recognition.Thorsten Joachims.
2006.
Training linear SVMs inlinear time.
In KDD.Ralf Krestel, Sabine Bergler, and Rene?
Witte.
2008.Minding the source: Automatic tagging of reportedspeech in newspaper articles.
In LREC.Aibek Makazhanov, Denilson Barbosa, and GrzegorzKondrak.
2012.
Extracting family relations fromliterary fiction.
Unpublished manuscript.Marie Catherine De Marneffe, Bill Maccartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InLREC.George A. Miller.
1995.
Wordnet: A lexical databasefor english.
Communications of the ACM, 38:39?41.Bruno Pouliquen, Ralf Steinberger, and Clive Best.2007.
Automatic detection of quotations in multi-lingual news.
In RANLP.Michal Rosen-Zvi, Chaitanya Chemudugunta,Thomas L. Griffiths, Padhraic Smyth, and MarkSteyvers.
2010.
Learning author-topic models fromtext corpora.
ACM Trans.
Inf.
Syst., 28(1).Josef Ruppenhofer, Caroline Sporleder, and FabianShirokov.
2010.
Speaker attribution in cabinet pro-tocols.
In LREC.Hugues Salamin, Alessandro Vinciarelli, Khiet Truong,and Gelareh Mohammadi.
2010.
Automatic rolerecognition based on conversational and prosodicbehaviour.
In ACM Multimedia.Luis Sarmento and Sergio Nunes.
2009.
Automatic ex-traction of quotes and topics from news feeds.
In 4thDoctoral Symposium on Informatics Engineering.Ronald B. Tobias.
2012.
20 Master Plots: And How toBuild Them.
Writer?s Digest Books, 3rd edition.Kristina Toutanova, Dan Klein, Christopher D. Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In NAACL-HLT.1320
