Proceedings of the 6th Workshop on Statistical Machine Translation, pages 250?260,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsProductive Generation of Compound Words in Statistical MachineTranslationSara StymneLinko?ping UniversityLinko?ping, Swedensara.stymne@liu.seNicola CanceddaXerox Research Centre EuropeMeylan, Francenicola.cancedda@xrce.xerox.comAbstractIn many languages the use of compoundwords is very productive.
A common practiceto reduce sparsity consists in splitting com-pounds in the training data.
When this is done,the system incurs the risk of translating com-ponents in non-consecutive positions, or in thewrong order.
Furthermore, a post-processingstep of compound merging is required to re-construct compound words in the output.
Wepresent a method for increasing the chancesthat components that should be merged aretranslated into contiguous positions and in theright order.
We also propose new heuristicmethods for merging components that outper-form all known methods, and a learning-basedmethod that has similar accuracy as the heuris-tic method, is better at producing novel com-pounds, and can operate with no backgroundlinguistic resources.1 IntroductionIn many languages including most of the Germanic(German, Swedish etc.)
and Uralic (Finnish, Hun-garian etc.)
language families so-called closed com-pounds are used productively.
Closed compoundsare written as single words without spaces or otherword boundaries, as the Swedish:gatstenshuggare gata + sten + huggarepaving stone cutter street stone cutterTo cope with the productivity of the phenomenon,any effective strategy should be able to correctlyprocess compounds that have never been seen in thetraining data as such, although possibly their com-ponents have, either in isolation or within a differentcompound.The extended use of compounds make them prob-lematic for machine translation.
For translation intoa compounding language, often fewer compoundsthan in normal texts are produced.
This can be dueto the fact that the desired compounds are missing inthe training data, or that they have not been alignedcorrectly.
When a compound is the idiomatic wordchoice in the translation, a MT system can oftenproduce separate words, genitive or other alternativeconstructions, or translate only one part of the com-pound.Most research on compound translation in com-bination with SMT has been focused on transla-tion from a compounding language, into a non-compounding one, typically into English.
A com-mon strategy then consists in splitting compoundsinto their components prior to training and transla-tion.Only few have investigated translation into a com-pounding language.
For translation into a com-pounding language, the process becomes:?
Splitting compounds on the target (compound-ing language) side of the training corpus;?
Learn a translation model from this split train-ing corpus from source (e.g.
English) intodecomposed-target (e.g.
decomposed-German)?
At translation time, translate using the learnedmodel from source into decomposed-target.?
Apply a post-processing ?merge?
step to recon-struct compounds.The merging step must solve two problems: identifywhich words should be merged into compounds, andchoose the correct form of the compound parts.250The former problem can become hopelessly diffi-cult if the translation did not put components nicelyside by side and in the correct order.
Preliminaryto merging, then, the problem of promoting transla-tions where compound elements are correctly posi-tioned needs to be addressed.
We call this promotingcompound coalescence.2 Related workThe first suggestion of a compound merging methodfor MT that we are aware of was described byPopovic?
et al (2006).
Each word in the translationoutput is looked up in a list of compound parts, andmerged with the next word if it results in a knowncompound.
This method led to improved overalltranslation results from English to German.
Stymne(2008) suggested a merging method based on part-of-speech matching, in a factored translation system,where compound parts had a special part-of-speechtag, and compound parts are only merged with thenext word if the part-of-speech tags match.
This re-sulted in improved translation quality from Englishto German, and from English to Swedish (Stymneand Holmqvist, 2008).
Another method, based onseveral decoding runs, was investigated by Fraser(2009).Stymne (2009a) investigated and compared merg-ing methods inspired by Popovic?
et al (2006),Stymne (2008) and a method inspired by morphol-ogy merging (El-Kahlout and Oflazer, 2006; Virpi-oja et al, 2007), where compound parts were anno-tated with symbols, and parts with symbols in thetranslation output were merged with the next word.3 Promoting coalescence of compoundsIf compounds are split in the training set, then thereis no guarantee that translations of components willend up in contiguous positions and in the correct or-der.
This is primarily a language model problem,and we will model it as such by applying POS lan-guage models on specially designed part-of-speechsets, and by applying language model inspired countfeatures.The approach proposed in Stymne (2008) consistsin running a POS tagger on the target side of the cor-pus, decompose only tokens with some predefinedPOS (e.g.
Nouns), and then marking with specialPOS-tags whether an element is a head or a modi-fier.
As an example, the German compound ?Fremd-sprachenkenntnisse?, originally tagged as N(oun),would be decomposed and re-tagged before trainingas:fremd sprachen kenntnisseN-Modif N-Modif NA POS n-gram language model using these extendedtagset, then, naturally steers the decoder towardstranslations with good relative placement of thesecomponentsWe modify this approach by blurring distinctionsamong POS not relevant to the formation of com-pounds, thus further reducing the tagset to only threetags:?
N-p ?
all parts of a split compound except thelast?
N ?
the last part of the compound (its head) andall other nouns?
X ?
all other tokensThe above scheme assumes that only noun com-pounds are treated but it could easily be extended toother types of compounds.
Alternatively, splittingcan be attempted irrespective of POS on all tokenslonger than a fixed threshold, removing the need ofa POS tagger.3.1 Sequence models as count featuresWe expect a POS-based n-gram language model onour reduced tagset to learn to discourage sequencesunseen in the training data, such as the sequenceof compound parts not followed by a suitable head.Such a generative LM, however, might also have atendency to bias lexical selection towards transla-tions with fewer compounds, since the correspond-ing tag sequences might be more common in text.To compensate for this bias, we experiment with in-jecting a little dose of a-priori knowledge, and add acount feature, which explicitly counts the number ofoccurrences of POS-sequences which we deem goodand bad in the translation output.Table 1 gives an overview of the possible bigramcombinations, using the three symbol tagset, plussentence beginning and end markers, and their judg-ment as good, bad or neutral.251Combination JudgmentN-p N-p GoodN-p N GoodN-p < \s > BadN-p X Badall other combinations NeutralTable 1: Tag combinations in the translation outputWe define two new feature functions: one count-ing the number of occurrences of Good sequences(the Boost model) and the other counting the occur-rences of Bad sequences (the Punish model).
Thetwo models can be used either in isolation or com-bined, with or without a further POS n-gram lan-guage model.4 Merging compoundsOnce a translation is generated using a systemtrained on split compounds, a post-processing stepis required to merge components back into com-pounds.
For all pairs of consecutive tokens we haveto decide whether to combine them or not.
Depend-ing on the language and on preprocessing choices,we might also have to decide whether to apply anyboundary transformation like e.g.
inserting an ?s?
be-tween components.The method proposed in Popovic?
et al (2006)maintains a list of known compounds and compoundmodifiers.
For any pair of consecutive tokens, if thefirst is in the list of known modifiers and the com-bination of the two is in the list of compounds, thanthe two tokens are merged.A somewhat orthogonal approach is the one pro-posed in Stymne (2008): tokens are labeled withPOS-tags; compound modifiers are marked withspecial POS-tags based on the POS of the head.
Ifa word with a modifier POS-tag is followed by ei-ther another modifier POS-tag of the same type, orthe corresponding head POS-tag, then the two to-kens are merged.In the following sections we describe how wemodify and combine these two heuristics, and howwe alternatively formulate the problem as a se-quence labelling problem suitable for a machinelearning approach.4.1 Improving and combining heuristicsWe empirically verified that the simple heuristics inPopovic?
et al (2006) tends to misfire quite often,leading to too many compounds.
We modify it byadding an additional check: tokens are merged ifthey appear combined in the list of compounds, butonly if their observed frequency as a compound islarger than their frequency as a bigram.
This blocksthe merging of many consecutive words, which justhappen to form a, often unrelated, compound whenmerged, such as fo?r sma?
(too small) into fo?rsma?
(spurn) in Swedish.
Compound and bigram frequen-cies can be computed on any available monolingualcorpus in the domain of interest.We furthermore observed that the (improved) list-based heuristic and the method based on POS pat-terns lead to complementary sets of false negatives.We thus propose to combine the two heuristics inthis way: we merge two consecutive tokens if theywould be combined by either the list-based heuris-tic or the POS-based heuristic.
We empirically veri-fied improved performance when combining heuris-tics in this way (Section 5.2).4.2 Compound merging as sequence labellingBesides extending and combining existing heuris-tics, we propose a novel formulation of compoundmerging as a sequence labelling problem.
The oppo-site problem, compound splitting, has successfullybeen cast as a sequence labelling problem before(Dyer, 2010), but here we apply this formulation inthe opposite direction.Depending on choices made at compound split-ting time, this task can be either a binary or mul-ticlass classification task.
If compound parts werekept as-is, the merging task is a simple concatena-tion of two words, and each separation point mustreceive a binary label encoding whether the two to-kens should be merged.
An option at splitting timeis to normalize compound parts, which often havea morphological form specific to compounds, to acanonical form (Stymne, 2009b).
In this case thecompound form has to be restored before concate-nating the parts.
This can be modeled as a multi-class classifier that have the possible boundary trans-formations as its classes.Consider for instance translating into German the252English sentence:Europe should promote the knowledge offoreign languagesAssuming that the training corpus did not con-tain occurrences of the pair (?knowledge of foreignlanguages?,?fremdsprachenkenntnisse?)
but con-tained occurrences of (?knowledge?,?kenntnisse?),(?foreign?,?fremd?)
and (?languages?,?sprachen?
),then the translation model from English intodecomposed-German could be able to produce:Europa sollte fremd sprachen kenntnissefo?rdernWe cast the problem of merging compounds as oneof making a series of correlated binary decisions,one for each pair of consecutive words, each decid-ing whether the whitespace between the two wordsshould be suppressed (label ?1?)
or not (label ?0?
).In the case above, the correct labelling for the sen-tence would be {0,0,1,1,0}, reconstructing the cor-rect German:Europa sollte fremdsprachenkenntnissefo?rdern1If conversely, components are normalized uponsplitting, then labels are no longer binary, but comefrom a set describing all local orthographic transfor-mations possible for the language under considera-tion.
In this work we limited our attention to the casewhen compounds are not normalized upon splitting,and labels are hence binary.While in principle one could address each atomicmerging decision independently, it seems intuitivethat a decision taken at one point should influencemerging decisions in neighboring separation points.For this reason, instead of a simple (binary or n-ary) classification problem, we prefer a sequence la-belling formulation.The array of sequence labelling algorithms po-tentially suitable to our problem is fairly broad, in-cluding Hidden Markov Models (HMMs) (Rabiner,1989), Conditional Random Fields (CRFs) (Laffertyet al, 2001), structured perceptrons (Collins, 2002),1Nouns in German are capitalized.
This is normally dealtas a further ?truecasing?
postprocessing, and is an orthogonalproblem from the one we deal with here.and more.
Since the focus of this work is on theapplication rather than on a comparison among al-ternative structured learning approaches, we limitedourselves to a single implementation.
Consideringits good scaling capabilities, appropriateness in pres-ence of strongly redundant and overlapping features,and widespread recognition in the NLP community,we chose to use Conditional Random Fields.4.2.1 FeaturesEach sequence item (i.e.
each separation point be-tween words) is represented by means of a sparsevector of features.
We used:?
Surface words: word-1, word+1?
Part-of-speech: POS-1, POS+1?
Character n-grams around the merge point?
3 character suffix of word-1?
3 character prefix of word+1?
Combinations crossing the merge points:1+3, 3+1, 3+3 characters?
Normalized character n-grams around themerge point, where characters are replaced byphonetic approximations, and grouped accord-ing to phonetic distribution, see Figure 1 (onlyfor Swedish)?
Frequencies from the training corpus, binnedby the following method:f?
={10blog10(f)c if f > 1f otherwisefor the following items:?
bigram, word-1,word+1?
Compound resulting from merging word-1,word+1?
Word-1 as a true prefix of words in the cor-pus?
Word+1 as a true suffix of words in thecorpus?
Frequency comparisons of two different fre-quencies in the training corpus, classified intofour categories: freq1 = freq2 = 0, freq1 <freq2, freq1 = freq2, freq1 > freq2253# vowels (soft versus hard)$word = s/[aoua?
]/a/g;$word = s/[eiya?o?e?
]/e/g;# consonant combinations and# spelling alternations$word = s/ng/N/g;$word = s/gn/G/g;$word = s/ck/K/g;$word = s/[lhgd]j/J/g;$word = s/?ge/Je/g;$word = s/?ske/Se/g;$word = s/?s[kt]?j/S/g;$word = s/?s?ch/S/g;$word = s/?tj/T/g;$word = s/?ke/Te/g;#consonants grouping$word = s/[ptk]/p/g;$word = s/[bdg]/b/g;$word = s/[lvw]/l/g;$word = s/[cqxz]/q/g;Figure 1: Transformations performed for normalizingSwedish consonants (Perl notation).?
word-1,word+1 as bigram vs compound?
word-1 as true prefix vs single word?
word+1 as true suffix vs single wordwhere -1 refers to the word before the merge point,and +1 to the word after.We aimed to include features representing theknowledge available to the list and POS heuristics,by including part-of-speech tags and frequencies forcompounds and bigrams, as well as a comparisonbetween them.
Features were also inspired by pre-vious work on compound splitting, based on the in-tuition that features that are useful for splitting com-pounds, could also be useful for merging.
Charac-ter n-grams has successfully been used for splittingSwedish compounds, as the only knowledge sourceby Brodda (1979), and as one of several knowl-edge sources by Sjo?bergh and Kann (2004).
Friberg(2007) tried to normalize letters, beside using theoriginal letters.
While she was not successful, westill believe in the potential of this feature.
Larson etal.
(2000), used frequencies of prefixes and suffixesfrom a corpus, as a basis of their method for splittingGerman compounds.4.2.2 Training data for the sequence labelerSince features are strongly lexicalized, a suitablylarge training dataset is required to prevent overfit-ting, ruling out the possibility of manual labelling.We created our training data automatically, usingthe two heuristics described earlier, plus a third oneenabled by the availability, when estimating parame-ters for the CRF, of a reference translation: merge iftwo tokens are observed combined in the referencetranslation (possibly as a sub-sequence of a longerword).
We compared multiple alternative combina-tions of heuristics on a validation dataset.
The val-idation and test data were created by applying allheuristics, and then manually check all positive an-notations.A first possibility to automatically generate atraining dataset consists in applying the compoundsplitting preprocessing of choice to the target side ofthe parallel training corpus for the SMT system: sep-aration points where merges should occur are thustrivially identified.
In practice, however, mergingdecisions will need be taken on the noisy output ofthe SMT system, and not on the clean training data.To acquire training data that is similar to the testdata, we could have held out from SMT training alarge fraction of the training data, used the trainedSMT to translate the source side of it, and then la-bel decision points according to the heuristics.
Thiswould, however, imply making a large fraction ofthe data unavailable to training of the SMT.
We thussettled for a compromise: we trained the SMT sys-tem on the whole training data, translated the wholesource, then labeled decision points according to theheuristics.
The translations we obtain are thus bi-ased, of higher quality than those we should expectto obtain on unseen data.
Nevertheless they are sub-stantially more similar to what will be observed inoperations than the reference translations.5 ExperimentsWe performed experiments on translation from En-glish into Swedish and Danish on two different cor-pora, an automotive corpus collected from a propri-etary translation memory, and on Europarl (Koehn,2005) for the merging experiments.
We used fac-tored translation (Koehn and Hoang, 2007), withboth surface words and part-of-speech tags on the254EU-Sv Auto-Sv Auto-DaCorpus Europarl Automotive AutomotiveLanguages English?Swedish English?Swedish English?DanishCompounds split N, V, Adj N, V, Adj NPOS tag-sets POS POS,RPOS RPOSDecoder Moses in-house in-houseTraining sentences SMT 1,520,549 329,090 168,047Training words SMT (target) 34,282,247 3,061,282 1,553,382Training sentences CRF 248,808 317,398 164,702Extra training sentences CRF 3,000 3,000 163,201Table 2: Overview of the experimental settingstarget side, with a sequence model on part-of-speech.
We used two decoders, Matrax (Simard etal., 2005) and Moses (Koehn et al, 2007), both stan-dard statistical phrase based decoders.
For parame-ter optimization we used minimum error rate train-ing (Och, 2003) with Moses and gradient ascent onsmoothed NIST for the in-house decoder.
In themerging experiments we used the CRF++ toolkit.2Compounds were split before training using acorpus-based method (Koehn and Knight, 2003;Stymne, 2008).
For each word we explored all pos-sible segmentations into parts that had at least 3characters, and choose the segmentation which hadthe highest arithmetic mean of frequencies for eachpart in the training corpus.
We constrained the split-ting based on part-of-speech by only allowing split-ting options where the compound head had the sametag as the full word.
The split compound parts kepttheir form, which can be special to compounds, andno symbols or other markup were added.The experiment setup is summarized in Table 2.The extra training sentences for CRF are sentencesthat were not also used to train the SMT system.
Fortuning, test and validation data we used 1,000 sen-tence sets, except for Swedish auto, where we used2,000 sentences for tuning.
In the Swedish experi-ments we split nouns, adjectives and verbs, and usedthe full POS-set, except in the coalescence exper-iments where we compared the full and restrictedPOS-sets.
For Danish we only split nouns, andused the restricted POS-set.
For frequency calcu-lations of compounds and compound parts that wereneeded for compound splitting and some of the com-2Available at http://crfpp.sourceforge.net/pound merging strategies, we used the respectivetraining data in all cases.
Significance testing wasperformed using approximate randomization (Rie-zler and Maxwell, 2005), with 10,000 iterations, and?
< 0.05.5.1 Experiments: Promoting compoundcoalescenceWe performed experiments with factored translationmodels with the restricted part-of-speech set on theDanish and Swedish automotive corpus.
In these ex-periments we compared the restricted part-of-speechset we suggest in this work to several baseline sys-tems without any compound processing and withfactored models using the extended part-of-speechset suggested by Stymne (2008).
Compound partswere merged using the POS-based heuristic.
Resultsare reported on two standard metrics, NIST (Dod-dington, 2002) and Bleu (Papineni et al, 2002), onlower-cased data.
For all sequence models we use3-grams.Results on the two Automotive corpora are sum-marized in Table 3.
The scores are very high, whichis due to the fact that it is an easy domain with manyrepetitive sentence types.
On the Danish dataset,we observe significant improvements in BLEU andNIST over the baseline for all methods where com-pounds were split before translation and merged af-terwards.
Some of the gain is already obtained us-ing a language model on the extended part-of-speechset.
Additional gains can however be obtained us-ing instead a language model on a reduced set ofPOS-tags (RPOS), and with a count feature explic-itly boosting desirable RPOS sequences.
The countfeature on undesirable sequences did not bring any255improvements over any of the systems with com-pound splitting.Results on the Swedish automotive corpus are lessclear-cut than for Danish, with mostly insignificantdifferences between systems.
The system with de-composition and a restricted part-of-speech modelis significantly better on Bleu than all other systems,except the system with decomposition and a stan-dard part-of-speech model.
Not splitting actuallygives the highest NIST score, even though the dif-ference to the other systems is not significant, ex-cept for the system with a combination of a trainedRPOS model and a boost model, which also has sig-nificantly lower Bleu score than the other systemswith compound splitting.5.2 Experiments: Compound mergingWe compared alternative combinations of heuristicson our three validation datasets, see Figure 2.
Inorder to estimate the amount of false negatives forall three heuristics, we inspected the first 100 sen-tences of each validation set, looking for words thatshould be merged, but were not marked by any ofthe heuristics.
In no case we could find any suchwords, so we thus assume that between them, theheuristics can find the overwhelming majority of allcompounds to be merged.We conducted a round of preliminary experimentsto identify the best combination of the heuristicsavailable at training time (modified list-based, POS-based, and reference-based) to use to create auto-matically the training data for the CRF.
Best resultson the validation data are obtained by different com-bination of heuristics for the three datasets, as couldbe expected by the different distribution of errorsin Figure 2.
In the experiments below we trainedthe CRF using for each dataset the combination ofheuristics corresponding to leaving out the grey por-tions of the Venn diagrams.
This sort of prelimi-nary optimization requires hand-labelling a certainamount of data.
Based on our experiments, skippingthis optimization and just using ref?
(list?POS) (theoptimal configuration for the Swedish-English Eu-roparl corpus) seems to be a reasonable alternative.The validation data was also used to set a fre-quency cut-off for feature occurrences (set at 3 inthe following experiments) and to tune the regu-larization parameter in the CRF objective function.448OK1212/00-154150/41411/30-154/11listPOS refAutomotive, Swedish48OK18282/0OO/0-4OK15880/88-4/83088/8OlistPKS refEuroparl, Swedish488OK8812/0/012-0-01253/154//12l3l213istPSOr ef?Automotive, DanishFigure 2: Evaluation of the different heuristics on valida-tion files from the three corpora.
The number in each re-gion of the Venn diagrams indicates the number of timesa certain combination of heuristics fired (i.e.
the num-ber of positives for that combination).
The two smallernumbers below indicate the number of true and false pos-itive, respectively.
Venn diagram regions correspondingto unreliable combinations of heuristics have correspond-ing figures on a grey background.
OK means that a largefraction of the Venn cell was inspected, and no error wasfound.256Danish auto Swedish autoBLEU NIST BLEU NISTNo compoundsplittingBase 70.91 8.8816Base+POSLM 72.08 8.9338 56.79 9.2674WithcompoundsplittingPOSLM 74.11* 9.2341* 57.28 9.1717RPOSLM 74.26* 9.2767* 58.12* 9.1694punish model 73.34* 9.1543*boost model 74.96** 9.3028** 57.31 9.1736RPOSLM + boost 74.76** 9.3368** 55.82 9.1088Table 3: Results of experiments with methods for promoting coalescence.
Compounds are merged based on the POSheuristic.
Scores that are significantly better than Base+POSLM, are marked ?
*?, and scores that are also better thanPOSLM with ?
**?.Results are largely insensitive to variations in thesehyper-parameters, especially to the CRF regulariza-tion parameter.For the Danish auto corpus we had access to train-ing data that were not also used to train the SMTsystem, that we used to compare the performancewith that on the possibly biased training data thatwas also used to train the SMT system.
There wereno significant differences between the two types oftraining data on validation data, which confirmedthat reusing the SMT training data for CRF trainingwas a reasonable strategy.The overall merging results of the heuristics, thebest sequence labeler, and the sequence labeler with-out POS are shown in Table 4.
Notice how the (mod-ified) list and POS heuristics have complementarysets of false negatives: when merging on the OR ofthe two heuristics, the number of false negatives de-creases drastically, in general compensating for theinevitable increase in false positives.Among the heuristics, the combination of the im-proved list heuristic and the POS-based heuristic hasa significantly higher recall and F-score than thePOS-based heuristic alone in all cases except on thevalidation data for Swedish Auto, and than the list-based strategy in several cases.
The list heuristicalone performs reasonably well on the two Swedishdata sets, but has a very low recall on the Danishdataset.
In all three cases the SMT training datahas been used for the list used by the heuristic, sothis is unexpected, especially considering the factthat the Danish dataset is in the same domain asone of the Swedish datasets.
The Danish trainingdata is smaller than the Swedish data though, whichmight be an influencing factor.
It is possible that thisheuristic could perform better also for Danish givenmore data for frequency calculations.The sequence labeler is competitive with theheuristics; on F-score it is only significantly worsethan any of the heuristics once, for Danish auto testdata, and in several cases it has a significantly higherF-score than some of the heuristics.
The sequencelabeler has a higher precision, significantly so inthree cases, than the best heuristic, the combina-tion heuristic, which is positive, since erroneouslymerged compounds are usually more disturbing fora reader or post-editor than non-merged compounds.The sequence-labelling approach can be used alsoin the absence of a POS tagger, which can be impor-tant if no such tool of suitable quality is availablefor the target language and the domain of interest.We thus also trained a CRF-based compound mergerwithout using POS features, and without using thePOS-based heuristic when constructing the trainingdata.
Compared to the CRF with access to POS-tags,on validation data F-score is significantly worse onthe Europarl Swedish condition and the AutomotiveDanish condition, and are unchanged on Automo-tive Swedish.
On test data there are no significantdifferences of the two sequence labelers on the twoAutomotive corpora.
On Swedish Europarl, the CRFwithout POS has a higher recall at the cost of a lowerprecision.
Compared to the list heuristic, which isthe only other alternative strategy that works in theabsence of a POS tagger, the CRF without POS per-forms significantly better on recall and F-score forDanish automotive, and mostly comparative on thetwo Swedish corpora.257Validation data Test dataPrecision Recall F-score Precision Recall F-scoreSwedish autolist .9889p,lp .9936p .9912p .9900 .9770 .9835POS .9757 .9632 .9694 .9916lp .9737 .9826list?POS .9720 1p .9858p .9822 .9984l,p,c,cp .9902l,p,cpCRF (ref?list) .9873p,lp .9984p .9928p,lp .9869 .9869 .9869CRF without POS .9873p,lp .9968p .9920p,lp .9836 .9852 .9844Swedish Europarllist .9923lp,c,cp .9819 .9871 .9882lp,cp .9849 .9865POS .9867lp .9785 .9825 .9893lp .9751 .9822list?POS .9795 .9958l,p,c,cp .9876p,cp .9782 .9993l,p,c,cp .9886p,cpCRF (ref?
(list?POS)) .9841cp .9916l,p .9879p,cp .9953l,p,lp,cp .9790 .9871pCRF without POS .9780 .9882p .9831 .9805 .9882p,c .9843Danish autolist .9250 .7603 .8346 .9905lp .7640 .8626POS .9814l,lp .9635l,cp .9724l,lp,cp .9779 .9294l .9538llist?POS .9251 .9863l,p,cp .9547l .9760 .9878l,p,c .9819l,p,cCRF (ref?list?POS) .9775l,lp .9932l,p,cp .9853l,p,lp,cp .9778 .9659l,p .9718l,pCRF without POS .9924l,lp,c .8973l .9424l .9826 .9635l,p .9729l,pTable 4: Precision, Recall, and F-score for compound merging methods based on heuristics or sequence labelling onvalidation data and on held-out test data.
The superscripts marks the systems that are significantly worse than thesystem in question (l-list, p-POS, lp-list?POS, c-best CRF configuration, cp-CRF without POS).The sequence labeler has the advantage overthe heuristics that it is able to merge completelynovel compounds, whereas the list strategy canonly merge compounds that it has seen, and thePOS-based strategy can create novel compounds,but only with known modifiers.
An inspection ofthe test data showed that there were a few novelcompounds merged by the sequence labeler thatwere not identified with either of the heuristics.
Inthe test data we found knap+start (button start)and vand+neds?nkning (water submersion) in Dan-ish Auto, and kvarts sekel (quarter century) andbostad(s)+ersa?ttning (housing grant) in SwedishEuroparl.
This confirms that the sequence labeler,from automatically labeled data based on heuristics,can learn to merge new compounds that the heuris-tics themselves cannot find.6 Discussion and conclusionsIn this article, we described several methods forpromoting coalescence and deciding if and how tomerge word compounds that are either competitivewith, or superior to, any currently known method.For promoting compound coalescence we exper-imented with introducing additional LMs based ona restricted set of POS-tags, and with dedicatedSMT model features counting the number of se-quences known a priori to be desirable and unde-sirable.
Experiments showed that this method canlead to large improvements over systems using nocompound processing, and over previously knowncompound processing methods.For merging, we improved an existing list-basedheuristic, consisting in checking whether the first oftwo consecutive words has been observed in a cor-pus as a compound modifier and their combinationhas been observed as a compound, introducing theadditional constraint that words are merged only iftheir corpus frequency as a compound is larger thantheir frequency as a bigram.We observed that the false negatives of this im-proved list-based heuristic and of another, known,heuristic based on part-of-speech tags were comple-mentary, and proposed a logical OR of them thatgenerally improves over both.We furthermore cast the compound merging prob-258lem as a sequence labelling problem, opening it tosolutions based on a broad array of models and al-gorithms.
We experimented with one model, Condi-tional Random Fields, designed a set of easily com-puted features reaching beyond the information ac-cessed by the heuristics, and showed that it givesvery competitive results.Depending on the choice of the features, the se-quence labelling approach has the potential to betruly productive, i.e.
to form new compounds inan unrestricted way.
This is for instance the casewith the feature set we experimented with.
The list-based heuristic is not productive: it can only forma compound if this was already observed as such.The POS-based heuristic presents some limited pro-ductivity.
Since it uses special POS-tags for com-pound modifiers, it can form a compound providedits head has been seen alone or as a head, and itsmodifier(s) have been seen elsewhere, possibly sep-arately, as modifier(s) of compounds.
The sequencelabelling approach can decide to merge two consec-utive words even if neither was ever seen before in acompound.In this paper we presented results on Swedish andDanish.
We believe that the methods would workwell also for other compounding languages such asGerman and Finnish.
If the linguistic resources re-quired to extract some of the features, e.g.
a POStagger, are unavailable (or are available only at train-ing time but not in operations) for some language,the sequence-labelling method can still be applied.
Itis competitive or better than the list heuristic, whichis the only heuristic available in that scenario.Experiments on three datasets show that the im-proved and combined heuristics perform generallybetter than any already known method, and that, be-sides being fully productive, the sequence-labellingversion is highly competitive, tends to generatefewer false positives than the combination heuristic,and can be used flexibly with limited or no linguisticresources.ReferencesBenny Brodda.
1979.
Na?got om de svenska ordens fono-tax och morfotax: Iakttagelse med utga?ngspunkt fra?nexperiment med automatisk morfologisk analys.
InPILUS nr 38.
Inst.
fo?r lingvistik, Stockholms univer-sitet, Sweden.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and experi-ments with perceptron algorithms.
In Proceedings ofthe 2002 Conference on Empirical Methods in NaturalLanguage Processing, Philadelphia, PA.George Doddington.
2002.
Automatic evaluation of ma-chine translation quality using n-gram co-occurencestatistics.
In Proceedings of the Second InternationalConference on Human Language Technology, pages228?231, San Diego, California, USA.Chris Dyer.
2010.
A Formal Model of Ambiguity andits Applications in Machine Translation.
Ph.D. thesis,University of Maryland, USA.I?lknur Durgar El-Kahlout and Kemal Oflazer.
2006.
Ini-tial explorations in English to Turkish statistical ma-chine translation.
In Proceedings of the Workshopon Statistical Machine Translation, pages 7?14, NewYork City, New York, USA.Alexander Fraser.
2009.
Experiments in morphosyntac-tic processing for translating to and from German.
InProceedings of the Fourth Workshop on Statistical Ma-chine Translation, pages 115?119, Athens, Greece.Karin Friberg.
2007.
Decomposing Swedish compoundsusing memory-based learning.
In Proceedings of the16th Nordic Conference on Computational Linguistics(Nodalida?07), pages 224?230, Tartu, Estonia.Philipp Koehn and Hieu Hoang.
2007.
Factored transla-tion models.
In Proceedings of the 2007 Joint Confer-ence on Empirical Methods in Natural Language Pro-cessing and Computational Natural Language Learn-ing, pages 868?876, Prague, Czech Republic.Philipp Koehn and Kevin Knight.
2003.
Empirical meth-ods for compound splitting.
In Proceedings of the 10thConference of the EACL, pages 187?193, Budapest,Hungary.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proceed-ings of the 45th Annual Meeting of the ACL, demon-stration session, pages 177?180, Prague, Czech Re-public.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In Proceedings of MTSummit X, pages 79?86, Phuket, Thailand.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic mod-els for segmenting and labeling sequence data.
In Pro-ceedings of the Eighteenth International Conferenceon Machine Learning, Williamstown, MA.259Martha Larson, Daniel Willett, Joachim Ko?hler, and Ger-hard Rigoll.
2000.
Compound splitting and lexi-cal unit recombination for improved performance ofa speech recognition system for German parliamen-tary speeches.
In Proceedings of the Sixth Interna-tional Conference on Spoken Language Processing,volume 3, pages 945?948, Beijing, China, October.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of the42nd Annual Meeting of the ACL, pages 160?167, Sap-poro, Japan.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automaticevaluation of machine translation.
In Proceedings ofthe 40th Annual Meeting of the ACL, pages 311?318,Philadelphia, Pennsylvania, USA.Maja Popovic?, Daniel Stein, and Hermann Ney.
2006.Statistical machine translation of German compoundwords.
In Proceedings of FinTAL ?
5th InternationalConference on Natural Language Processing, pages616?624, Turku, Finland.
Springer Verlag, LNCS.Lawrence R. Rabiner.
1989.
A tutorial on hidden markovmodels and selected applications in speech recogni-tion.
Proceedings of IEEE, 77(2):257?286.Stefan Riezler and John T. Maxwell.
2005.
On some pit-falls in automatic evaluation and significance testingfor MT.
In Proceedings of the Workshop on Intrin-sic and Extrinsic Evaluation Measures for MT and/orSummarization at ACL?05, pages 57?64, Ann Arbor,Michigan, USA.Michel Simard, Nicola Cancedda, Bruno Cavestro, MarcDymetman, Eric Gaussier, Cyril Goutte, Kenji Ya-mada, Philippe Langlais, and Arne Mauser.
2005.Translating with non-contiguous phrases.
In Proceed-ings of the Human Language Technology Conferenceand the conference on Empirical Methods in Natu-ral Language Processing, pages 755?762, Vancouver,British Columbia, Canada.Jonas Sjo?bergh and Viggo Kann.
2004.
Finding the cor-rect interpretation of Swedish compounds, a statisti-cal approach.
In Proceedings of the 4th InternationalConference on Language Resources and Evaluation(LREC?04), Lisbon, Portugal.Sara Stymne and Maria Holmqvist.
2008.
Processing ofSwedish compounds for phrase-based statistical ma-chine translation.
In Proceedings of the 12th AnnualConference of the European Association for MachineTranslation, pages 180?189, Hamburg, Germany.Sara Stymne.
2008.
German compounds in factored sta-tistical machine translation.
In Proceedings of Go-TAL ?
6th International Conference on Natural Lan-guage Processing, pages 464?475, Gothenburg, Swe-den.
Springer Verlag, LNCS/LNAI.Sara Stymne.
2009a.
A comparison of merging strategiesfor translation of German compounds.
In Proceedingsof the EACL 2009 Student Research Workshop, pages61?69, Athens, Greece.Sara Stymne.
2009b.
Compound processing for phrase-based statistical machine translation.
Licentiate the-sis, Linko?ping University, Sweden.Sami Virpioja, Jaako J. Va?yrynen, Mathias Creutz, andMarkus Sadeniemi.
2007.
Morphology-aware statis-tical machine translation based on morphs induced inan unsupervised manner.
In Proceedings of MT Sum-mit XI, pages 491?498, Copenhagen, Denmark.260
