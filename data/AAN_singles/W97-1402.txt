Referring in Multimodal Systems:The Importance of User Expertise and System FeaturesDanie la  Pet re l l iIRST - Istituto per la Ricerca Scientifica e Tecnologica1-38050 Povo - Trento, Italypetrelli@irst, itc.
itAntone l la  De  Ange l i  Wa l te r  Gerb ino  G iu l ia  CassanoCognitive Technology Lab.
- Psychology Dept.- University of TriesteVia deU'Universit?, 7 - 34123 Trieste - Italydeangeli,gerbino,cassano~univ.t rieste itAbstract  forth HCI) (Schmauks, 1987).
Currently, little em-This paper empirically investigates how hu-mans use reference in space when inter-acting with a multimodal system able tounderstand written natural anguage andpointing with the mouse.We verified that user expertise plays animportant role in the use of multimodalsystems: experienced users performed 84%multimodal inputs while inexpert only30%.
Moreover experienced are able to ef-ficiently use multimodality shortening thewritten input and transferring part of thereference meaning on the pointing.Results showed also the importance of thesystem layout: when very short labels(one character) are available users stronglyadopt a redundant reference strategy, i.e.they referred to the object in a linguisticway and use pointing too.Starting from these facts some guidelinesfor future multimodal systems are sug-gested.1 In t roduct ionMultimodal communication is used frequently andefficiently by humans to identify objects in physi-cal space.
By combining different modalities (e.g.speech and gestures), multimodal references act asefficient ools for coping with the complexity of thephysical space -as conveyed by visual perception-which can be communicate only partially by verbalexpressions (Glenberg and McDaniel, 1992).
There-fore, multimodal references can easily substitute toocomplex, too detailed, ambiguous, or undeterminedverbal expressions.
In particular, they simplify refer-ent identification when the speaker or the hearer donot know the name of the target, or how to describeit.For a long time, face-to-face communication hasbeen considered a reliable model for natural an-guage based human-computer interaction (hence-pirical work is available on what actually happens inmultimodal HCI and how communication featurescohabit with modern graphical interfaces (Oviatt,1996; De Angeli et al, 1996; Oviatt et al, 1997;Siroux et al, 1995).
Moreover, with only a fewexceptions (Buxton, 1991; Brennan, 1991; Stock,1995), direct manipulation i terfaces have been seenas an antagonist ofconversational interfaces, hinder-ing a desirable synergy between the two communi-cation styles.We believe that in multimodal HCI users findstrategies that overcome natural language communi-cation or direct manipulation paradigm alone, creat-ing a new mixed communication form that makes thebest use of both (Oviatt, 1996; Oviatt et al, 1997).The new communication, even if similar in principleto face-to-face communication, might be carried outin a far different fashion because one partner is acomputer.
As a matter of fact, some studies howedthat people do adopt conversational and social ruleswhen interacting with computers (Nass et al, 1994)but humans also design their utterances with thespecial partner in mind (Brennan, 1991).Empirical studies on natural language human-computer interaction confirm general HCI peculiar-ities.
Talkingto a computer humans maintain aconversational framework but tend to simplify thesyntactic structure, to reduce utterances length, lex-icon richness and use of pronouns (Jonsson andDahlback, 1988; Dahlback and Jonsson, 1989; Ovi-att, 1995).
In other words users select a simplifiedregister to interact with computers even if the (sim-ulated) system has human capabilities (De Angeli,1991).These results suggest hat face-to-face communi-cation is not an adequate model for HCI.
Thereforeempirical studies are needed to develop predictivemodels of multimodal communication in HCI.Empirical research becomes even more importantwhen a multimodal system reproduces an unnatu-ral modality combination, such as writing combinedwith pointing.
Pointing while writing is highly dif-ferent from pointing while speaking.
In the first case,Referring in Multimodal Systems 15in fact, multimodal communication is hampered bya single-modality-production constraint.
Indeed, therequirement of moving the dominant hand back andforth between the keyboard and a pointing deviceimplies a substitution of the natural parallel syn-chronization pattern (Levelt et al, 1985) with an un-natural sequential one.
Nevertheless, the obligationof using the same effector for writing and pointingdoes not seem to have any inhibitory effect on mul-timodal input production (De Angeli et al, 1996).In general, deixis 1 was found to be the most fre-quent referent identification strategy adopted byusers to indicate objects.
However, its occur-rence depends trongly on the effort needed to in-dicate the target by a pure verbal reference.
More-over, we found a relevant percentage of redundantreferences 2, a strategy mentioned in the relevant lit-erature only for speech and pointing (Siroux et al,1995) and pretty different from common face-to-facecommunication strategies (De Angeli et al, 1996).Following the iterative design principles (Nielsen,1993), we assume that experimental research, in theform of early simulations, should improve multi-modal design by allowing to formulate reliable guide-lines.
From this point of view, the purpose of thispaper is to investigate the effect of user expertise andsystem features on multimodal interaction in orderto infer useful guidelines for future systems.2 HCI  issues in mult imodal  referringIn a HCI context where the user is required to writeand point, we analyze the communication strategiesthat users spontaneously adopted at the very begin-ning of interaction.
The main purpose of analyz-ing users' spontaneous behavior is to develop designguidelines that might be taken into account whendeveloping multimodal systems that have to supportsuccessful interaction from the very beginning, like"walk-up-and-use" interfaces.Results of a simulation experiment have been an-alyzed to answer the following question:Is multimodal interaction reallyinstinctive, i.e.
do naive usersperform as experienced ones?In general, multimodal systems appear to improveHCI by allowing humans to communicate in a morespontaneous way (Oviatt and Olsen, 1994; Oviatt,1996).
Therefore, one could infer that multimodalcommunication is the best interaction style for naive1Deixis concerns the ways in which languages encodeor grammaticalize f atures of the context of utteranceor speech event (Levinson, 1983).
Among the varioustypes we consider here only space or place deixis used ina gestural way.2We defined redundant reference as multimodal ref-erences composed by a full linguistic reference and a notneeded additional pointing.users.
However, some authors suggest that lan-guage based interaction is mainly suitable for ex-perienced users (Hutchins et al, 1986; Gentner andNielsen, 1996).
Indeed the opacity of language al-lows very flexible interaction, but requires previousknowledge 3.
We believe that experience, defined ascomputer science literacy, may increase the efficientuse of multimodality.
The notion of efficiency is de-fined, following (Mac Aogain and Reilly, 1990), asthe capacity of the multimodal input to derive im-portant semantic parts from information channelsother than language, i.e.
from pointing.
In otherwords, efficiency is operationalized asthe proportionof written input replaced by the gestural one.3 MethodIn order to evaluate spontaneous multimodal inputproduction, data from the training session of a sim-ulation experiment were analyzed.P rocedure  The multimodal system called SIM,Sistema Interattivo per la Modulistica, was simu-lated to assist students with form filling tasks.
Con-versing with SIM, users had to gather informationon how to fill out form fields (user questions) and toprovide personal data for automatic insertion (useranswers).
Hard-copy instructions described systemcapability and required participants to complete thetask as quickly and accurately as possible.
No exam-ples of dialogue were directly given, to avoid biasingcommunication behavior.
Participants worked indi-vidually in a user room and were monitored by aclosed circuit camera.
Dialogues and pointing werelogged and interactions videotaped uring all exper-imental sessions.
At the end all students filled in auser satisfaction questionnaire (USQ) and were de-briefed.S imulat ion  The system was simulated by theWizard of Oz technique, in which a human (thewizard) plays the role of the computer behindthe human-computer interface (Fraser and Gilbert,1991).
A semi-automatic procedure supported thesimulation that was carried out on two connectedSUN SPARC workstations.
Interface constraintsand several pre-loaded utterances (including a cou-ple of prefixed answers for every task-relevant action,error messages, help and welcoming phrases) sup-ported two trained wizards.
These strategies havebeen found to increase simulation reliability by re-ducing response delays and lessening the attentionaldemand on the wizard (Oviatt et al, 1992).The user interface was composed by a dialoguewindow in the upper part and a form window in thelower part of the screen (figure 1).
In the dialogue3This is opposite to WYSIWYG (What You See IsWhat You Get) interfaces where what can be done isclearly visible.16 D. Petrelli, A. DeAngeli,  W. Gerbino and G. Cassano-~ ; t l~  ?~s D4~ctte ~ l ~  ta a ~tsA  ~s~ta  et r ~  ?
~ttss l  mi io  ~t~,~a ~ v ~  ~4 ~ mmcmtt~i ,b , .~ v t ,p t .
,  ~t~g, t  Ct. ?1.
Iumrtm * rs.
~L.
,~,.
4In  fo r le~la~ ~.
re,Lll nPo~mons ,  t~ l~ts l~ '  Iff ..........
o'.._~.'.'.
'...o n ~ t ~;'i;J~,.
'"'""nL~?o~luoe~ Lk,.~i,aU U"  ~1 ....... ' - -  m : .............. Ci=~ ......... U .... ; ' : ' :  \[OFigure 1: The user screen during an interaction.window users typed their input and read system out-put.
Pointing was supported by mouse and pointerwas constrained inside the form window.SIM was simulated with good dialogue capabili-ties, anyway still far away from human abilities.
Itaccepted every type of multimodal references, i.e.with or without linguistic anchorage for the gestureand either close or far pointing.
It could understandellipses and complete linguistic references.The form layout had nine fields grouped in threerows.
Rows and fields had meaningful labels (one ortwo words) suggesting the required content.
Userscould refer both to single fields and to rows as awhole.
At row selection SIM gave instruction on thewhole row content.
After users received row infor-mation, to further fields selection corresponded moresynthetic instructions.SIM required to click on the referred field and gavevisual feedback to this 4.
It supported multiple refer-ences too.
System answers were always multimodalwith demonstrative pronouns and synchronized (vi-sual) pointing.Par t i c ipants  and  Des ign Twenty five studentsfrom Trieste University participated in the simula-tions as paid volunteers.
Ages of participants rangedfrom 20 to 31 and all were Italian native speakers.Participants were grouped in two different setsaccording to their computer experience.
Selectionwas achieved by a self-administered questionnaire oncomputer attitude and experience.
Half sample wasrepresented by experienced users, skilled typists withpositive attitude towards computers and some pro-gramming experience.
The other half was composedby students who had never used a computer before.4In a previous study we demonstrated that visualfeedback allows more efficient multimodal input, in-creases integration of pointing within writing and is pre-ferred by users (De Angeli et al, 1996; De Angeli, 1997).4 Results and DiscussionData were available from 24 participants yielding acorpus of 210 user questions (due to technical prob-lems one inexpert was discarded).
User answers hadto provide personal data for automatic insertion, butthey did not require to identify fields.
So user an-swers were not included in the analysis.Each user question was tabulated in one of the fol-lowing five categories according to the referent iden-tification strategy adopted in it:?
d i rect  naming:  it is a unimodal reference andoccurs when the field label is explicitly used inthe utterance, e.g., il campo dati anagrafici (thepersonal data field);?
l anguage re ference:  it is a unimodal refer-ence and occurs whenever the field is referredby a pure verbal input, but without direct nam-ing, e.g., l'ultimo campo (the last field).
Thiscategory includes, among others, anaphoric ref-erence and metonymia;?
deixis: it is a multimodal reference that occurswhenever an explicit anchor (deictic linguisticexpression) for the pointing exists, e.g., questo,,2 campo ( th is /2  field);?
mixed:  it is a multimodal reference that occurswhen the reference contains both linguistic andgestural part, but no deictic mark can be foundin the utterance, e.g., in / z  (in ,7);?
redundant :  it is a multimodal reference; it oc-curs when one component (or part of it) is notneeded for the understanding, e.g., il campo A/2 (the field A /2 ) .Figure 2 shows percentages of each referent iden-tification strategies as a function of user exper-tise.
It clearly emerges that previous knowledgeaffects strategy selection.
Multimodal input werestrongly preferred by expert users, while inexpertExperienced Umtlrm Unt~mrt  Uzrmm D~ec~ nm~a im Lm~aqie t'z/zrmr-tm D~I n NFigure 2: Referent identification strategies percent-ages as a factor of users expertise.Referring in Multimodal Systems 17Ic 1 j :  ....................... :J0II IIlIIFigure 3: The MIS layout.preferred unimodal inguistic references, especiallydirect naming 5.
These results imply that communi-cation behavior may be predicted by knowing previ-ous expertise.Multimodal occurrence strongly increased effi-ciency of communication.
Utterance length wasfound to be inverse correlated to the number of mul-timodal input (r=.48, p<.05).
In average, expertusers wrote almost 3 words each utterance, whileinexpert nearly the double 6.It is interesting to notice that, on the total sam-ple, deixis and mixed input occur close to the samefrequency.
Mixed input implies a contraction of ver-bal input which is partially substituted by pointing,as in cosa /z (what /z).
This phenomenon is pe-culiar to HCI, and pretty infrequent in face to facecommunication where deixis (e.g.
cosa qui 7 - whathere/~-) represent the maximum efficient input.Redundant input was pretty rare, with no signif-icant difference due to expertise.
This finding is incontrast with what we observed in (De Angeli etal., 1996).
There, the simulated system, called MIS(Multimodal Intelligent System), had a quite differ-ent layout: each field had a very short label (a singleletter) not related to the required content of the field(figure 3).
With this layout redundant input was the25% of the total.We evince that the significant different rate wheninteracting with the two systems was due to formlayouts.
Indeed, redundant references in the casewhere labels were one character long was no-costcompared to the case where labels where one/twowords long.
This suggests that system layout mayinfluence communication behavior.
In the next5According to the results of a Mann-Whitney U testthe ratio of multirnodal input to total questions in thetwo experience groups is statistically significant, U =10,5, (N=23) p<.001 .6The difference is significant according to results ofan ANOVA F(1,22) = 12.21 p<.001 .chapter we discuss related guidelines.When designing whatever system, specialistsshould consider both system functionalities and in-teraction features depending on the typology of usersand on the tasks they will perform.
In current multi-modal systems, this balancing among functionalitiesand users has not been considered enough.For example, the obligation for the user to pointat a certain time while writing was found to be incontrast with his/her natural inclination.
This con-straint would be justified only if synchronization u -derstanding is a true problem, e.g.
if the users usemultiple selection or pars-pro-toto.
Our data showthat, at least, this is not the case at the beginningof the interaction: expert used multiple selection inthe 0.22% of the total multimodal references whilefor inexpert he percentage decrease furthermore to0.14%.5 Lessons  Learned  and  ProposedGu ide l inesIn this section we state some guidelines useful fordesigners of walk-up-and-use multimodal systems orfor systems that have to have a successful interactionfrom the very beginning.As widely discussed above, user expertise is a fac-tor that deeply influences multimodal interaction.In our experiments, experienced users took advan-tage of multimodality in the 84% of all the consid-ered interactions.
At the opposite, multimodalitywas not exploited by the inexpert hat used it onlyfor 30%.
These data indicate that multimodal sys-tems are definitely suited for expert users even fromthe very beginning of the interaction, while inexpertshave difficulties in exploiting multimodality interac-tion limiting themselves to inefficient linguistic ref-erences.To help a naive user to overcome this initial gap,it may be useful to plan some mechanisms, uch ascontextual help or specific tutoring answers, aimingat directing linguistic references toward multimodalones.
This strategy could be especially importantin systems, like tourist kiosks, where the user mighthave difficulties in stating the name correctly.Another interesting point is that experiencedusers perform nearly the same percentage of deixis(38%) and of mixed modality (42%).
This suggeststhat systems hould be flexible enough to acceptwhatever combination of pointing and writing, notrequiring a well formed deixis.An important guideline is therefore to not requirea prefixed behavior from the user, to say to not pre-tend well formed deixis or the pointing in a specificposition.
We claim that flexibility has to be pre-ferred to more sophisticated system facilities, suchas multiple pointing, since users do not make themost of them.18 D. Petrelli, A. DeAngeli, W. Gerbino and G. CassanoThe flexibility concept is strengthened bythe factthat users can find very efficient ways for referring,optimizing writing and pointing and exploiting thecontext as in ,,/z?,, where the meaning is conveyedby the gesture, by the minimal writing and by con-sidering the task the user is performing.Consequently, a further step toward a flexible sys-tem would be to use all the possible informationsources to interpret user multimodal input, to saynot only linguistic and gestural input but also dis-course history and task model.Lastly, the influence of the layout on the user be-havior has to be underlined.
In fact, the possibil-ity of referring to objects in a no-cost linguistic way(e.g.
a single character) encourages the user to useredundant references.
This suggests to design theinterface using very short labels whenever a doublereference is useful to discriminate objects, for exam-ple on dense maps.More in general, we verified that conclusive re-suits coming from related fields can not be sim-ply transferred to the multimodal domain.
Thisis the case of models from human-human commu-nication that do not exhaustively describe all phe-nomena occurring in multimodal human-computerinteraction.
For example, we showed that mixedinputs like cosa ,7 (what /z) are pretty frequentwhereas they are hardly used in face to face dia-logues.
Similarly, Gestalt guidelines for graphicalinterface design may not have the expected effect onusers behavior in complex multimodal referring.
Forexample, even thought rows of objects are clearlydisplayed (a frame around homogeneous objects letthe user perceive a single set), users seldom referto rows (e.g.
by their title or by clicking the rowbackground) preferring repeated references to eachobject.Building on our experience, we believe that, evenif some work has been already done, empirical inves-tigations are still needed to complete the picture ofhuman-computer multimodal interaction.AcknowledgmentsThis research was partially supported by the CNRGrant for the project "An ecological approach to theimplementation a d evolution of multimodal sys-tems".Re ferencesSusan Brennan.
1991.
Conversation as direct ma-nipulation: An iconoclastic view.
In Brenda Lau-rel, editor, The Art of Human-Computer InterfaceDesign, pages 393-404.
Addison-Wesley.Bill Buxton.
1991.
The "natural" language ofinteraction: A perspective on nonverbal dia-logues.
In Brenda Laurel, editor, The Art ofHuman-Computer Interface Design, pages 405-416.
Addison-Wesley.Nils Dahlback and Arne Jonsson.
1989.
Empiri-cal studies of discourse representations fornaturallanguage interfaces.
In ~th European Conferenceof ACL, pages 291-298.Antonella De Angeli, Daniela Petrelli, and WalterGerbino.
1996.
Interface features affecting deixisproduction: A simulation study.
In WIGLS -Workshop on the Integration of Gesture in Lan-guage and Speech (at ICSLP'96), pages 195-204.Antonella De Angeli.
1991.
Dillo a MAIA.
Master'sthesis, Psychology Dept.- University of Trieste.Antonella De Angeli.
1997.
Valutare i sistemiflessibili: un approccio globale alia HCI.
Ph.D.thesis, University of Trieste.N.
Fraser and G. Gilbert.
1991.
Simulating speechsystems.
Computer Speech and Language, pages81-99.Don Gentner and Jakob Nielsen.
1996.
Theanti-mac interface.
Communication of the ACM,39(8):70-82.A.
Glenberg and M. McDaniel.
1992.
Mentalmodels, pictures, and text: Integration of spatialand verbal information.
Memory and Cognition,20(5):458-460.Edwin L. Hutchins, James D. Hollan, and Donald A.Norman.
1986.
Direct manipulation i terfaces.
InDonald A. Norman and Stephen W. Draper, edi-tors, User Centered System Design: new Perspec-tives on Human-Computer Interaction, chapter 5,pages 87-124.
Lawrance Erlbaum Associates.Arne Jonsson and Nils Dahlback.
1988.
Talkingto a computer is not like talking to your bestfriend.
In Scandinavian Conference on ArtificialIntelligence- SCAI'88, pages 53-68.Willem Levelt, Graham Richardson, and WidoLa Heij.
1985.
Pointing and voicing in deicticexpressions.
Journal of Memory and Language,24:133-164.Stephan Levinson.
1983.
Pragmatics.
CambridgeTextbook in Linguistics.
Cambridge UniversityPress.Eoghan Mac Aogain and Ronan Reilly.
1990.
Dis-course theory and interface design: The case ofpointing with the mouse.
International Journalof Man-Machine Studies, 32:591-602.Clifford Nass, Jonathan Steuer, and Ellen Tauber.1994.
Computers are social actors.
In CH1'94Human Factors in Computing Systems, pages 72-78.
ACM Press.Jakob Nielsen.
1993.
Usability Engineering.
Ac-demic Press.Referring in Mult imodal Systems 19Sharon Oviatt and Eric Olsen.
1994.
Integrationthemes in multimodal human-computer interac-tion.
In International Conference on Spoken Lan-guage Processing, pages 551-554.Sharon Oviatt, Philip Cohen, M. Fong, andM.
Frank.
1992.
A rapid semiautomatic simula-tion technique for investigating interactive speechand hand writing.
In International Conference onSpoken Language Processing, pages 1351-1354.Sharon Oviatt, Antonella De Angeli, and KarenKuhn.
1997.
Integration and synchronization ofinput modes during multimodal human-computerinteraction.
In CHI'97- Human Factors on Com-puting Systems.
ACM Press.Sharon Oviatt.
1995.
Predicting spoken disfluencesduring human-computer interaction.
ComputerSpeech and Language, 9:19-35.Sharon Oviatt.
1996.
Multimodal interfaces for dy-namic interactive maps.
In CHI'96 - Human Fac-tors in Computing Systems.
ACM Press.Dagmar Schmauks.
1987.
Natural and simulatedpointing: An interdisciplinary survey.
Technicalreport, FB Informatik, Universit~it Saarbriicken.Jacques Siroux, M. Guyomard, F. Multon, andC.
Remondeau.
1995.
Oral and gestural activi-ties of the users in the georal system.
In H. Bunt,R.
Beun, and T. Borghuis, editors, InternationalConference on Cooperative Multimodal Communi-cation (CMC'95), volume 2, pages 287-298, May.Oliviero Stock.
1995.
A third modality?
ArtificialIntelligence Review, 9:129-146.
