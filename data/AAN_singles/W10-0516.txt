Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics in a World of Social Media, pages 31?32,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsDetecting controversies in Twitter: a first studyMarco PennacchiottiYahoo!
LabsSunnyvale, CA.pennac@yahoo-inc.comAna-Maria PopescuYahoo!
LabsSunnyvale, CA.amp@yahoo-inc.comSocial media gives researchers a great opportunityto understand how the public feels and thinks abouta variety of topics, from political issues to entertain-ment choices.
While previous research has exploredthe likes and dislikes of audiences, we focus on arelated but different task of detecting controversiesinvolving popular entities, and understanding theircauses.
Intuitively, if people hotly debate an entityin a given period of time, there is a good chance of acontroversy occurring.
Consequently, we use Twit-ter data, boosted with knowledge extracted from theWeb, as a starting approach: This paper introducesour task, an initial method and encouraging early re-sults.Controversy Detection.
We focus on detect-ing controversies involving known entities in Twit-ter data.
Let a snapshot denote a triple s =(e,?t, tweets), where e is an entity, ?t is a timeperiod and tweets is the set of tweets from the tar-get time period which refer to the target entity.1.
Letcont(s) denote the level of controversy associatedwith entity e in the context of the snapshot s. Ourtask is as follows:Task.
Given an entity set E and a snapshot setS = {(e,?t, tweets)|e ?
E}, compute the con-troversy level cont(s) for each snapshot s in S andrank S with respect to the resulting scores.Overall Solution.
Figure 1 gives an overview ofour solution.
We first select the set B ?
S, consist-ing of candidate snapshots that are likely to be con-troversial (buzzy snapshots).
Then, for each snap-shot in B, we compute the controversy score cont,by combining a timely controversy score (tcont) anda historical controversy score (hcont).Resources.
Our method uses a sentiment lexi-con SL (7590 terms) and a controversy lexicon CL1We use 1-day as the time period ?t.
E.g.
s=(?BradPitt?,12/11/2009,tweets)Algorithm 0.1: CONTROVERSYDETECTION(S, Twitter)select buzzy snapshots B ?
Sfor s ?
B{tcont(s) = ?
?MixSent(s) + (1?
?)
?
Controv(s))cont(s) = ?
?
tcont(s) + (1?
?)
?
hcont(s)rank B on scoresreturn (B)Figure 1: Controversy Detection: Overview(750 terms).
The sentiment lexicon is composed byaugmenting the set of positive and negative polarityterms in OpinionFinder 1.5 2 (e.g.
?love?,?wrong?
)with terms bootstrapped from a large set of userreviews.
The controversy lexicon is compiled bymining controversial terms (e.g.
?trial?, ?apology?
)from Wikipedia pages of people included in theWikipedia controversial topic list.Selecting buzzy snapshots.
We make the simpleassumption that if in a given time period, an entity isdiscussed more than in the recent past, then a contro-versy involving the entity is likely to occurr in thatperiod.
We model the intuition with the score:b(s) =|tweetss|(?i?prev(s,N)|tweetsi|)/Nwhere tweetss is the set of tweets in the snapshots; and prev(s,N) is the set of snapshots referring tothe same entity of s, in N time periods previous tos.
In our experiment, we use N = 2, i.e.
we focuson two days before s. We retain as buzzy snapshotsonly those with b(s) > 3.0.Historical controversy score.
The hcont scoreestimates the overall controversy level of an entityin Web data, independently of time.
We considerhcont our baseline system, to which we comparethe Twitter-based models.
The score is estimatedon Web document data using the CL lexicon as fol-2J.
Wiebe, T. Wilson, and C. Cardie.
2005.
Annotating ex-pressions of opinions and emotions in language.
In LanguageResources and Evaluation.31lows: hcont(e) = k/|CL|, where k is the number ofcontroversy terms t?
s.t.
PMI(e, t?)
> A3.Timely controversy score.
tcont estimates thecontroversy of an entity by analyzing the discussionamong Twitter?s users in a given time period, i.e.
ina given snapshot.
It is a linear combination (tunedwith ?
?
[0, 1]) of two scores:MixSent(s): reflects the relative disagreementabout the entity in the Twitter data from snapshots.
First, each of the N tweets in s is placed in one ofthe following sets: Positive (Pos), Negative (Neg),Neutral (Neu), based on the number of positive andnegative SL terms in the tweet.
MixSent is com-puted as:MixSent(s) =Min(|Pos|, |Neg|)Max(|Pos)|, |Neg|)?|Pos|+ |Neg|NControv(s): this score reflects the presence ofexplicit controversy terms in tweets.
It is computedas: Controv(s) = |ctv|/N , where ctv is the set oftweets in s which contain at least one controversyterm from CL.Overall controversy score.
The overall scoreis a linear combination of the timely and historicalscores: cont(s) = ??tcont(s)+(1??
)?hcont(s),where ?
?
[0, 1] is a parameter.Experimental ResultsWe evaluate our model on the task of ranking snap-shots according to their controversy level.
Our cor-pus is a large set of Twitter data from Jul-2009 toFeb-2010.
The set of entities E is composed of104,713 celebrity names scraped from Wikipediafor the Actor, Athlete, Politician and Musician cat-egories.
The overall size of S amounts to 661,226(we consider only snapshots with a minimum of 10tweets).
The number of buzzy snapshots in B is30,451.
For evaluation, we use a gold standard of120 snapshots randomly sampled from B, and man-ually annotated as controversial or not-controversialby two expert annotators (detailed guidelines will bepresented at the workshop).
Kappa-agreement be-tween the annotators, estimated on a subset of 20snapshots, is 0.89 (?almost perfect?
agreement).
Weexperiment with different ?
and ?
values, as re-ported in Table 1, in order to discern the value offinal score components.
We use Average Precision3PMI is computed based on the co-occurrences of entitiesand terms in Web documents; here we use A = 2.Model ?
?
AP AROChcont (baseline) 0.0 0.0 0.614 0.581tcont-MixSent 1.0 1.0 0.651 0.642tcont-Controv 0.0 1.0 0.614 0.611tcont-combined 0.5 1.0 0.637 0.642cont 0.5 0.5 0.628 0.646cont 0.8 0.8 0.643 0.642cont 1.0 0.5 0.660 0.662Table 1: Controversial Snapshot Detection: results overdifferent model parametrizations(AP), and the area under the ROC curve (AROC) asour evaluation measures.The results in Table 1 show that all Twitter-basedmodels perform better than the Web-based baseline.The most effective basic model is MixSent, sug-gesting that the presence of mixed polarity sentimentterms in a snapshot is a good indicator of contro-versy.
For example, ?Claudia Jordan?
appears in asnapshot with a mix of positive and negative terms-in a debate about a red carpet appearance- but thehcont and Controv scores are low as there is norecord of historical controversy or explicit contro-versy terms in the target tweets.
Best overall per-formance is achieved by a mixed model combiningthe hcont and theMixSent score (last row in Tablelabel 1).
There are indeed cases in which the evi-dence from MixSent is not enough - e.g., a snap-shot discussing ?Jesse Jackson?
?s appearance on atv show lacks common positive or negative terms,but reflects users?
confusion nevertheless; however,?Jesse Jackson?
has a high historical controversyscore, which leads our combined model to correctlyassign a high controversy score to the snapshot.
In-terestingly, most controversies in the gold standardrefer to micro-events (e.g., tv show, award show orathletic event appearances), rather than more tradi-tional controversial events found in news streams(e.g., speeches about climate change, controversialmovie releases, etc.
); this further strengthens thecase that Twitter is a complementary informationsource wrt news corpora.We plan to follow up on this very preliminaryinvestigation by improving our Twitter-based sen-timent detection, incorporating blog and news dataand generalizing our controversy model (e.g., dis-covering the ?what?
and the ?why?
of a controversy,and tracking common controversial behaviors of en-tities over time).32
