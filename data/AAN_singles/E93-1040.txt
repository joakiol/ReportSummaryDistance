Parsing the Wall Street Journal with theInside-Outside AlgorithmYves Schabes Michal Roth Randy OsborneMitsubishi Electric Research LaboratoriesCambridge MA 02139USA(schabes/roth/osborne@merl.com)AbstractWe report grammar inference xperiments onpartially parsed sentences taken from the WallStreet Journal corpus using the inside-outsidealgorithm for stochastic context-free grammars.The initial grammar for the inference processmakes no ,assumption of the kinds of structuresand their distributions.
The inferred grammar isevaluated by its predicting power and by com-paring the bracketing of held out sentencesimposed by the inferred grammar with the par-tial bracketings of these sentences given in thecorpus.
Using part-of-speech tags as the onlysource of lexical information, high bracketingaccuracy is achieved even with a small subsetof the available training material (1045 sen-tences): 94.4% for test sentences shorter than10 words and 90.2% for sentences shorter than15 words.1 IntroductionMost broad coverage natural language parsers havebeen designed by incorporating hand-crafted rules.These rules are also very often further efined by statisti-cal training.
Furthermore, it is widely believed that highperformance an only be achieved by disambiguatinglexically sensitive phenomena such as prepositionalattachment ambiguity, coordination or subcategoriza-don.So far, grammar inference has not been shown to beeffective for designing wide coverage parsers.Baker (1979) describes a training algorithm for sto-chastic ontext-free grammars (SCFG) which can beused for grammar reestimation (Fujisaki et al 1989,Sharrnan et al 1990, Black et al 1992, Briscoe and Wae-gner 1992) or grammar inference from scratch (Lari andYoung 1990).
However, the application of SCFGs andthe original inside-outside algorithm for grammar infer-ence has been inconclusive for two reasons.
First, eachiteration of the algorithm on a gr,-unmar with n nontermi-nals requires O(n31wl 3)time per t~ning sentence w. Sec-ond, the inferred grammar imposes bracketings which donot agree with linguistic judgments of sentence struc-ture.Pereira nd Schabes (1992) extended the inside-out-side algorithm for inferring the parameters ofa stochas-tic context-free grammar to take advantage ofconstituent bracketing information i the training text.Although they report encouraging experiments (90%bracketing accuracy) on h'mguage transcriptions in theTexas Instrument subset of the Air Travel InformationSystem (ATIS), the small size of the corpus (770 brack-eted sentences containing a total of 7812 words), its lin-guistic simplicity, and the computation time required tovain the grammar were reasons to believe that theseresults may not scale up to a larger and more diverse cor-pus.We report grammar inference xperiments with thisalgorithm from the parsed Wall Street Journal corpus.341The experiments prove the feasibility and effectivenessof the inside-outside algorithm on a htrge corpus.Such experiments are made possible by assumi'ng aright br~mching structure whenever the parsed corpusleaves portions of the parsed tree unspecified.
This pre-processing of the corpus makes it fully bracketed.
Bytaking adv~mtage of this fact in the implementation f theinside-outside ~dgorithm, its complexity becomes line~trwith respect to the input length (as noted by Pereira ndSchabes, 1992) ,and therefore tractable for large corpora.We report experiments u ing several kinds of initialgr~unmars ~md a variety of subsets of the corpus as train-ing data.
When the entire Wall Street Journal corpus wasused as training material, the time required for traininghas been further educed by using a par~dlel implementa-tion of the inside-outside ~dgorithm.The inferred grammar is evaluated by measuring thepercentage ofcompatible brackets of the bracketingimposed by the inferred grammar with the partial brack-eting of held out sentences.
Surprisingly high bracketingaccuracy is achieved with only 1042 sentences as train-?
ing materi,'d: 94.4% for test sentences shorter th,-m 10words ~md 90.2% for sentences shorter than 15 words.Furthermore, the bracketing accuracy does not dropdrastic~dly as longer sentences ,are considered.
Theseresults ,are surprising since the training uses part-of-speech tags as the only source of lexical information.This raises questions about he statistical distribution ofsentence structures observed in naturally occurring text.After having described the training material used, wereport experiments u ing several subsets of the availabletraining material ,and evaluate the effect of the trainingsize on the bracketing perform,'mce.
Then, we describe amethod for reducing the number of parameters in theinferred gr~unmars.
Finally, we suggest a stochasticmodel for inferring labels on the produced binarybr~mching trees.2 Training CorpusThe experiments u e texts from the Wall Street Journ~dCorpus ,and its partially bracketed version provided bythe Penn Treebank (Brill et al, 1990).
Out of 38 600bracketed sentences (914 000 words), we extracted34500 sentences (817 000 words) as possible source oftraining material ,and 4100 sentences (97 000 words) assource for testing.
We experimented with several subsets(350, 1095, 8000 ,and 34500 sentences) of the availabletraining materi~d.For practiced purposes, the part of the tree bank usedfor training is preprocessed before being used.
First, fiatportions of parse trees found in the tree b,'mk are turnedinto a right linear binary br~mching structure.
Thisenables us to take full adv~mtage of the fact that theextended inside-outside ~dgorithm (as described inPereira nd Schabes, 1992) behaves in linear time whenthe text is fully bracketed.
Then, the syntactic labels areignored.
This allows the reestimation algorithm to dis-tribute its own set of labels based on their actual distri-bution.
We later suggest a method for recovering theselabels.The following is ,an ex~unple of a partially parsed sen-tence found in the Penn Treeb~mk:SNP VBZ VPhas VBN VPI Ibeen VBNIselDT NN PPI INo price IN NPf?r D~T JIJ NI~ISt e new sharesThe above parse corresponds tothe fully bracketedunlabeled parseDTNo NNIprice INIfor DTt~e JJ NNSI If lew sharesVBZhas VBN ?I Ibeen VBNIselfound in the tr,'fining corpus.
The experiments reportedin this paper use only the p,'trt-of-speech sequences ofthis corpus ,and the resulting fully bracketed parses.
Forthe above example, the following bracketing is used inthe training material:(DT (NN (IN (DT (JJ NNS)))) (VBZ (VBN VBN)))3 Inferring BracketingsFor the set of experiments described in this section,the initial gr,'unmar consists of,all 4095 possible Chore-342sky Normal Form rules over 15 nonterminals(X i, 1 < i < 15) and 48 termin,'d symbols (t,,, 1 < m < 48)for part-of-speech tags (the same set as the one used inthe Penn Treebank):X i =:~ X\]X kX i =~ t mThe parameters of the initial stochastic context-freegrammar are set randomly while maintaining the properconditions for stochastic context-free grammars.
1Using the algorithm described in Pereira nd Schabes(1992), the current rule probabilities and the parsedtraining set C are used to estimate the expected frequen-cies of each rule.
Once these frequencies are computedover each bracketed sentence c in the training set, newrule probabilities ,are assigned in a way that increases theestimated probability of the bracketed training set.
Thisprocess is iterated until the increase in the estimatedprobability of the bracketed training text becomes negli-gible, or equivalently, until the decrease in cross entropy(negative log probability)Z logP (c)~t (c,G) = cEcZ IclceCbecomes negligible.
In the above formula, the probabil-ity P(c) of the partially bracketed sentence c is computedas the sum of the probabilities of all derivations compat-ible with the bracketing of the sentence.
This notion ofcompatible bracketing isdefined in details in Pereim andSchabes (1992).
Informally speaking, aderivation iscompatible with the bracketing of the input given in thetree bank, if no bracket imposed by the derivationcrosses abracket in the input.Compatible bracketInput bracketingIncompatible bracketInput bracketing( )A( )As refining material, we selected randomly out of theavailable training material 1042 sentences of lengthshorter than 15 words.
For evaluation purposes, we also1.
The sum of the probabilities ofthe rules with same left handside must be one.nmdomly selected 84 sentences of length shorter than 15words among the test sentences.Figure 1 shows the cross entropy of the training aftereach iteration.
It also shows for each iteration the crossentropies f /o f  84 sentences randomly selected ,amongthe test sentences of length shorter than 15 words.
Thecross entropy decreases ,as more iterations ,are performedand no over training is observed..008.587.576.565.554.543.5Train ing set.
H -Test.
set H - - -~ ' ~ .
~  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.I I I I20 40 60 80i te ra t ion00Figure 1.
Training and Test Set -log prob1009080706050403020100f3~tac  e. Ac.cu l:a cy.1:JNI I I I20 40 60 80i t .e ra t  ion100Figure 2.
Bracketing and sentence accuracy of 84test sentences horter than 15 words.To evaluate the quality of the analyses yielded by theinferred grammars obtained ,after each iteration, we useda Viterbi-style parser to find the most likely analyses ofsentences in several test samples, and compared themwith the Treebank partial bmcketings of the sentences ofthose samples.
For each sample, we counted the percent-343age of brackets of the most likely ~malysis that are not"crossing" the partiid bracketing of the same sentencesfound in the Treebank.
This percentage is called thebracketing accuracy (see Pereira and Schabes, 1992 torthe precise definition of this measure).
We also com-puted the percentage of sentences in each smnple inwhich no crossing bracket wits found.
This percentage iscalled the sentence accuracy.Figure 2 shows the bracketing and sentence accuracyfor the s,'une 84 test sentences.Table 1 shows the bracketing and sentence accuracyfor test sentences within various length ranges.
Highbracketing accuracy is obtained even on relatively longsentences.
However, as expected, the sentence accuracydecreases rapidly as the sentences get longer.LengthBracketingAccuracySentenceAccuracyTABLE 1.0-10 0-15 10-19 20-3094.4% 90.2% 82.5% 71.5%82% 57.1% 30% 6.8%Bracketing Accuracy on test sentences odifferent lengths (using 1042 sentences oflengths shorter than 15 words as trainingmaterial).Table 2 compares our results with the bracketing accu-racy of analyses obtained by a systematic right linearbranching structure for all words except for the finalpunctuation mark (which we att~tched high).
2 We alsoevaluated the stochastic context-free gr, unmar obtainedby collecting each level of the trees found in the trainingtree bimk (see Table 2).Length 0-10 0-15 10-19 20-30Inferred grammar 94.4% 90.2% 82.5% 71.5%Right linear trees 76% 70% 63% 50%Treebank Grmmnar 46% 31% 25%TABLE 2.
Bracketing accuracy of the inferredgrammar, of right linear structures and ofthe Treebank grammar.Right linear structures perform surprisingly well.
Ourresults improve by 20 percentage points upon this baseline performance.
These results uggest that the distribu-tion of sentence structure in naturally occurring text issimpler than one may have thought, especially sinceonly part-of-speech tags were used.
This may suggest2.
We thank Eric Brill and David Yarowsky for suggestingthese xperiments.the existence of clusters of trees in the training material.However, using the number of crossing brackets ils a dis-tance between trees, we have been unable to reveal theexistence of clusters.The grammar obtained by collecting rules from thetree bank performs very poorly.
One can conclude thatthe labels used in the tree bank do not have ,'my statisti-cal property.
The task of inferring a stochastic grammarfrom a tree bank is not trivial and therefore requires ta-tistical training.In the appendix we give examples of the most likelyanalyses output by the inferred grammar on severld testsentencesIn Table 3, different subsets of the available trltiningsentences of lengths up to 15 words long and the gram-mars were evaluated on the same set of test sentences oflengths horter than 15 words.
The size of the trainingset does not seem to ,affect he performimce of the parser.Training Size 350 1095 8000(sentences)Bracketing 89.37% 90.22% 89.86%AccuracySentence 52.38% 57.14% 55.95%AccuracyTABLE 3.
Effect of the size of the training set on thebracketing and sentence accuracy.However if one includes all available sentences(34700 sentences), for the stone test set, the bracketingaccuracy drops to 84% ,and the sentence accuracy to40%.We have also experimented with the following initialgrmnmar which defines a large number of rules(I 10640):X i ~ X jX  kX i ~ t iIn this grammar, each non-terminal symbol is uniquely,associated with a terminal symbol.
We observed over-Ix,fining with this grmnmar ,and better statistic~d conver-gence was obtained, however the performance of theparser did not improve.3444 Reducing the Grammar Size andSmoothing IssuesAs grammars are being inferred at each iteration, thetraining algorithm was designed to guarantee that noparameter was set below some small threshold.
Thisconstraint is important for smoothing.
It implies that norule ever disappears at a reestimation step.However, once the final grammar is found, for practi-cal purposes, one can reduce the number of parametersbeing used.
For example, the size of the grammar can bereduced by eliminating the rules whose probabilities arebelow some threshold or by keeping for each non-termi-nal only the top rules rewriting it.However, one runs into the risk of not being able toparse sentences given as input.
We used the followingsmoothing heuristics.Lexieal rule smoothing.
In the case no rule in thegnunmar introduces a terminal symbol found in the inputstring, we assigned a lexical rule (X i ~ tin) with very low?
probability for all non-terminal symbols.
This case willnot happen if the training is representative of the lexicalitems.Syntactic rule smoothing.
When the sentence is notrecognized from the starting symbol, we considered ,allpossible non-terminal symbols as starting symbols ,andconsidered as starting symbol the one that yields themost likely ,'malysis.
Although this procedure may notguarantee that ,all sentences will be recognized, we foundit is very useful in practice.When none of the above procedures enable parsing ofthe sentence, we used the entire set of parameters of theinferred gr,~mar (this was never the case on the testsentences we considered).For example, the grammar whose performance isdepicted in Table 2 defines 4095 parameters.
However,the same performance is achieved on these test sets byusing only 450 rules (the top 20 binary branching rulesX i ~ XjXk for each non-terminal symbol ,and the top 10lexical rules X i ~ I m for each non-terminal symbol),5.
ImplementationPereira nd Schabes (1992) note that the training ,algo-rithm behaves in linear time (with respect to the sentencelength) when the training material consists of fullybracketed sentences.
By taking advantage of this fact,the experiments u ing a small number of initial rules anda small subset of the available training materials do notrequire a lot of computation time and can be performedon a single workstation.
However, the experiments u inglarger initial grammars or using more material requiremore computation.The training algorithm can be parallelized by dividingthe training corpus into fixed size blocks of sentences,and by having multiple workstations processing eachone of them independently.
When ,all blocks have beencomputed, the counts are merged and the parameters arereestimated.
For this purpose, we used PVM (Beguelinet al, 1991) as a mechanism for message passing acrossworkstations.. Stochastic Model of Labeling forBinary Branching TreesThe stochastic grmnmars inferred by the training pro-cedures produce unlabeled parse trees.
We are currentlyevaluating the following stochastic model for labeling abinary branching tree.
In this approach, we make thesimplifying assumption that the label of a node onlydepends on the labels of its children.
Under this assump-tion, the probability of labeling a tree is the product ofthe probability of labeling each level in the tree.
Forexample, the probability of the following labeling:SNP VPA mDT NN VBZ NNSis P(S ~ NP VP) P(NP ~ DTNN)  P(VP ~ VBZNNS)These probabilities can be estimated in a simple man-her given a tree bank.
For example, the probability oflabeling a level as NP ~ DTNN is estimated as the num-ber of occurrences (in the tree bank) ofNP  ~ DTNNdivided by the number of occurrences ofX =~ DTNNwhere X ranges over every label.Then the probability of a labeling can be computedbottom-up from leaves to root.
Using dyn,'unic program-ruing on increasingly arge subtrees, the labeling withthe highest probability can be computed.345We are currently evzduating the effectiveness ofthisvnethod.7.
ConclusionThe experiments described in this paper prove theeffectiveness ofthe inside-outside ~dgorithm on a htrgecorpus, ,and also shed some light on the distribution ofsentence structures found in natural languages.We reported gr~unmar inference xperiments u ing theinside-outside algorithm on the parsed Wall Street Jour-md corpus.
The experiments were made possible byturning the partially parsed training corpus into a fullybracketed corpus.Considering the fact that part-of-speech tags were theonly source of lexical information actually used, surpris-ingly high bracketing accuracy is achieved (90.2% onsentences of length up to 15).
We believe that evenhigher esults can be achieved by using a richer set ofpart-of-speech tags.
These results how that he use ofsimple distributions of constituency structures c~m pro-vide high accuracy perfonnance for broad coverage nat-und hmguage parsers.AcknowledgmentsWe thank Eric Brill, Aravind Joshi, Mark Liberman,Mitchel Marcus, Fernando Pereira, Stuart Shieber ,andDavid Yarowsky for valuable discussions.ReferencesBaker, J.K. 1979.
Trainable grammars for speech recog-nition.
In Jared J. Wolf,and Dennis H. Klatt, editors,Speech communication papers presented at the 97 thMeeting of the Acoustical Society of America, MIT,Cambridge, MA, June.Adam Beguelin, Jack Dongarra, A1 Geist, RobertM,'mchek, Vaidy Sunderam.
July 1991.
"A Users'guide to PVM Parallel Virtual Machine", Oak RidgeNational Lab, TM-11826.E.
Black, S. Abney, D. Flickenger, R. Grishman, P. Har-rison, D. Hindle, R. Ingria, F. Jelinek, J. Khwans, M.Liberman, M. Marcus, S. Roukos, B. S~mtorini, ~md T.Strzalkowski.
1991.
A procedure for quantitativelycomparing the syntactic overage of English grmn-mars.
DARPA Speech and Natural Language Work-shop, pages 3(i)6-311, Pacific Grove, California.Morgan Kaufinann.Ezra Black, John L;dferty, and Salim Roukos.
1992.Development and Evaluation of a Broad-CoverageProbabilistic Grmnmar of English-Language Com-puter Manuals.
In 20 th Meeting ~+the Association fi)rComputational Linguistics (A CL' 92), Newark, Dela-ware.Eric Brill, David Magerm,'m, Mitchell Marcus, and Beat-rice Santorini.
1990.
Deducing linguistic structurefrom the statistics of htrge corpora.
In DARPA Speechand Natural Language Workshop.
Morgan Kaufinann,Hidden Valley, Pennsylv~mia, June.Ted Briscoe ,and Nick Waegner.
July 1992.
Robust Sto-chastic Parsing Using the Inside-Outside Algorithm.In AAAI workshop on Statistically-based Techniquesin Natural Language Processing.T.
Fujimtki, F. Jelinek, J. Cocke, E. Black, and T. Nish-ino.
1989.
A probabilistic parsing method for sentencedisarnbiguation.
Proceedings of the InternationalWorkshop on Parsing Technologies, Pittsburgh,August.K.
L,'ui ,and S.J.
Young.
1990.
The estimation of stochas-tic context-free gr,-unmars u ing the Inside-Outside,algorithm.
Computer Speech and Language, 4:35-56.Pereira, Fern,'mdo and Yves Schabes.
1992.
Inside-out-side reestimation from partially bracketed corpora.
In20 th Meeting of the Association for ComputationalLinguistics (ACL' 92), Newark, Delaware.346Appendix Examples of parsesThe following parsed sentences are the most likely analyses output by the grammar inferred from 1042 training sen-tences (at iteration 68) for some randomly selected sentences oflength not exceeding 10 words.
Each parse is pre-ceded by the bracketing given in the Treebank.
SeritenceS output by the parser are printed in bold face and crossingbrackets are marked with an asterisk (*).
(((The/DT Celtona/NP operations/NNS) would/MD (become/VB (part/NN (of/IN (those/DT ventures/NNS))))) .L)(((The/DT (Celtona/NP operations/NNS)) (would/MD (become/VB (part/NN (of/IN (those/DT ventures/NNS))))))) i.
)((But/CC then/RB they/PP (wake/VBP up/IN (tofI'O (a/I)T nightmare/NN)))) ./.
((But/CC (then/RB (they/PP (wake/VBP (up/IN (to/TO (a/DT nightmare/NN))))))) J.
)(((Mr./NP Strieber/NP) (knows/VBZ (a/DT lot/NN (about/IN aliens/NNS)))) ./.
)(((Mr./NP Strieber/NP) (knows/VBZ ((a/DT lot/NN) (about/IN aliens/NNS)))) ./.
)(((The/DT companies/NNS) (are/VBP (automotive-emissions-testing/JJ co cems/NNS))) ./.
)(((The/DT companies/NNS) (are/VBP (automotive-emissions-testing/JJ concerns/NNS))) ./.
)(((Chief/JJ executives/NNS and/CC presidents/NNS) had/VBD (come/VBN and/CC gone/VBN) ./.
))(((Chief/JJ (executives/NNS (and/CC presidents/NNS))) (had/VBD (come/VBN (and/CC gone/VBN)))) ./.
)(((HowAVRB quickly/RB) (things/NNS ch,'mge/VBP) ./.
))((How/WRB (* quickly/RB (things/NNS change/VBP) *)) ,/.
)((This/DT (means/VBZ ((the/DT returns/NNS) can/MD (vary/VB (a/DT great/JJ deal/NN))))) ./.
)((This/DT (means/VBZ ((the/DT returns/NNS) (can/MD (vary/VB (a/DT (great/JJ deal/NN))))))) ./.
)(((Flight/NN Attendants/NNS) (Lag/NN (Before/IN (Jets/NNS Even/RB Land/VBP)))))((* Flight/NN (* Attendants/NNS (* Lag/NN (* Before/IN Jets/NNS *) *) *) *) (Even/RB LantUVBP))((They/PP (talked/VBD (of/IN (the/DT home/NN run/NN)))) ./.
)((They/PP (talked/VBD (of/IN (the/DT (home/NN run/NN))))) J.
)(((The/DT entire/JJ division/NN) (employs/VBZ (about/IN 850/CD workers/NNS))) ./.
)(((The/DT (entire/JJ division/NN)) (employs/VBZ (about/IN (850/CD workers/NNS)))) ./.
)(((At/IN least/JJS) (before/IN (8/CD p.m/RB)) ./.
))(((At/IN leasl/JJS) (before/IN (8/CD p.m/RB))) ./.
)((Pretend/VB (Nothing/NN Happened/VBD)))((* Pretend/VB Nothing/NN *) Happened/VBD)(((The/DT highlight/N'N) :/:(a/DT "'/'" fragrance/NN control/NN system/NN ./.
"/")))((* (The/DT highlight/NN) (* :/: (a/DT (("/'" fragrance/NN) (control/NN system/NN))) * *) (./.
"/"))(((Stock/NP prices/NNS) (slipped/VBD lower/DR (in/IN (moderate/JJ trading/NN))) ./.
))(((Stock/NP prices/NNS) (slipped/VBD (lower/J JR (in/IN (moderate/JJ trading/NN))))) ./.
)(((Some/DT jewelers/NNS) (have/VBP (Geiger/NP counters/NNS) (to/TO (measure/VB (top~tz/NN radiation/NN))))./3)(((Some/DT jewelers/NNS) (have/VBP ((Geiger/NP counters/NNS) (to/TO (measure/VB (topaz/NN radiation/NN)))))) ./.
)((That/DT ('s/VBZ ( (the/DT only/JJ question/NN ) (we/PP (need/VBP (to/TO address/VB)))))) ./.
)((That/DT ('s/VBZ ((the/DT (only/JJ question/NN)) (we/PP (need/VBP (to/TO address/VB)))))) ./.
)((She/PP (was/VBD (as/RB (cool/JJ (as/IN (a/DT cucumber/NN)))))) ./.
((She/PP (was/VBD (as/RB (cool/JJ (as/IN (a/DT cucumber/NN)))))) ./.
)(((The/DT index/NN) (gained/VBD (99.14/CD points/NNS) Monday/NP)) ./.
)(((The/DT index/NN) (gained/VBD ((99.14/CD points/NNS) Monday/NP))) J.
)347
