Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conferenceand the Shared Task, pages 90?95, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational LinguisticsPolyUCOMP-CORE TYPED: Computing Semantic Textual Similarityusing Overlapped SensesJian Xu Qin LuThe Hong Kong Polytechnic UniversityDepartment of ComputingHung Hom, Kowloon, Hong Kong{csjxu, csluqin}@comp.polyu.edu.hkAbstractThe Semantic Textual Similarity (STS)task aims to exam the degree of semanticequivalence between sentences (Agirre etal., 2012).
This paper presents the workof the Hong Kong Polytechnic University(PolyUCOMP) team which has participatedin the STS core and typed tasks of SemEval-2013.
For the STS core task, the PolyUCOMPsystem disambiguates words senses usingcontexts and then determine sentencesimilarity by counting the number of sensesthey shared.
For the STS typed task, the stringkernel (Lodhi et al 2002) is used to computesimilarity between two entities to avoid stringvariations in entities.1 IntroductionSentence similarity computation plays an importantrole in text summarization and social networkapplications (Erkan et al 2004; Jin et al 2011).The SemEval 2012 competition initiated a tasktargeted at Semantic Textual Similarity (STS)between sentence pairs (Agirre et al 2012).
Givena set of sentence pairs, participants are required toassign to each sentence pair a similarity score.Because a sentence has only a limited amount ofcontent words, it is difficult to determine sentencesimilarities.
To solve this problem, Hatzivassiloglouet al(1999) proposed to use linguistic featuresas indicators of text similarity to address theproblem of sparse representation of sentences.Mihalcea et al(2006) measured sentence similarityusing component words in sentences.
Li et al(2006) proposed to incorporate the semantic vectorand word order to calculate sentence similarity.Biemann et al(2012) applied the log-linearregression model by combining the simple stringbased measures, for example, word ngrams andsemantic similarity measures, for example, textualentailment.
Similarly, Saric et al(2012) used asupport vector regression model which incorporatesfeatures computed from sentence pairs.
The featuresare knowledge- and corpus-based word similarity,ngram overlaps, WordNet augmented word overlap,syntactic features and so on.
Xu et al(2012)combined semantic vectors with skip bigrams todetermine sentence similarity, whereas the skipbigrams take into the sequential order betweenwords.In our approach to the STS task, words insentences are assigned with appropriate senses usingtheir contexts.
Sentence similarity is computed bycalculating the number of shared senses in bothsentences since it is reasonable to assume thatsimilar sentences should have more overlappingsenses.
For the STS-TYPED task, variationsmight occur in author names, people involved,time expression and location.
Thus, string kernelis applied to compute similarity between entitiesbecause it can capture variations between entities.Moreover, for the event similarity in STS-TYPEDtask, semantic relatedness between verbs is derivedthe WordNet.The rest of this paper is structured as follows.Section 2 describes sentence similarity using senseoverlapping and string kernel.
Section 3 gives theperformance evaluation.
Section 4 is the conclusion.902 Similarity between SentencesWords are used to convey meaning in a sentence.They are tagged with appropriate senses initially andthen sentence similarity is calculated based on thenumber of shared senses.2.1 Sense OverlappingWhen comparing word features, we did not comparetheir surface equality, but we first conceptualizethese words and then calculate their similaritiesbased on the hierarchial structure in WordNet.
For aword in a sentence, it will be assigned a WordNetsense.
In this paper, we focus on the WordSense Disambiguation (WSD) algorithm taken byBanerjee and Pederson (2003).
They measured thesemantic relatedness between concepts by countingthe shared words in their WordNet glosses.In WordNet, a word sense is represented by asynset which has a gloss that defines the conceptthat it represents.
For example, the words walking,afoot, ambulate constitute a single synset which hasgloss representations as follows,walking: the act of traveling by footafoot: traveling by footambulate: walk aboutTo lift the limitations of dictionary glosses whichare fairly short with insufficient vocabulary, weutilize the glosses of related senses since we assumethat words co-occur in one sentence share relatedsenses and the more glosses two senses share, themore similar they are.
Therefore, we extract notonly glosses of target synset, but also the glossesof the hypernym, hyponym, meronym, holonym andtroponym synsets of the target synset to form asynset context.
Finally, we compare the sentencecontexts with different synset contexts to determinewhich sense should be assigned to the words.To disambiguate word senses, a window ofcontexts surrounding the the target word is specifiedand a set of candidate word senses are extracted forthe content word (noun, verb, adjective) within thatwindow.
Let the current target word index i = 0 thatis,w0, the window size be 2n+1 and?n ?
i ?
+n.Let |wi| be the number of senses for word wi and thejth sense of wi is si,j , where 1 ?
j ?
|wi|.
Next isto assign an appropriate sense k to the target word.We achieve this by adding together the relatednessscores calculated by comparing the senses of thetarget word and senses of every non-target wordwithin the window of context.
The sense score forthe current target word w0 is defined as,Sensek =n?i=?n|wi|?j=1relatedness(s0,k, si,j) (1)The kth sense which has the biggest sense scorewill be chosen as the right sense for the target wordw0.
Now remains the question of how to define therelatedness between two synsets.
It is defined as,relatedness(s0,k, si,j) =score(gloss(s0,k), gloss(si,j))+score(hype(s0,k), hype(si,j))+score(hypo(s0,k), hypo(si,j))+score(hype(s0,k), gloss(si,j))+score(gloss(s0,k), hype(si,j))(2)In Equation 2, the score function counts thenumber of overlapping words between two glosses.However, if there is a phrasal n-word overlap, thena score of n2 will be assigned, thus encouraging thelonger n-word overlap.
Let V denote the set of n-word overlaps shared between two glosses, the scoreis defined as,score =?w?V?w?2 (3)where ?w?
refers to the number of words in w. Inso doing, we can have corresponding senses for thesentence Castro celebrates 86th birthday Mondayas follows,castro/10886929-n celebrate/02490877-vbirthday/15250178-n monday/15163979-nTo find the n-word overlap, we found thatcontiguous words in two glosses lie in the diagonalof a matrix, take the senses walk and afoot forexample, their glosses are,walking: the act of traveling by footafoot: traveling by foot91Place the walking glosses in rows and afootglosses in columns, we get the matrix representationin Figure 1,Figure 1: n-word overlap representationFigure 1 shows that travel by foot is a continuoussequence of words shared by two glosses.
Steps tofind n-word overlapping are:(1) Construct a matrix for two sentences;(2) Get continuous n-word overlapping, n isgreater than 1;(3) Set the cell values to 0 if they are contained incontinuous n-word.
(4) Get the words (unigrams) which are shared bytwo sentences.Take a b c d and b c a d for example, we will havethe matrix as follows,b c a da 0 0 1 0b 1 0 0 0c 0 1 0 0d 0 0 0 1Table 1: Matrix representation for two sentencesBy the step 2, we will get the b c and itscorresponding cells cell(1,0) and cell(2,1).
We thenset the two cells to zero, and obtain an updatedmatrix as follows,b c a da 0 0 1 0b 0 0 0 0c 0 0 0 0d 0 0 0 1Table 2: Updated matrix representation for two sentencesIn Table 2, we found that cell(0,2) and cell(3,3)have values greater than zero.
Therefore, a and bwill be extracted the common terms.This approach can also be applied to find commonn-word overlaps between sentences, for example,s1: Olli Heinonen, the Head of the InternationalAtomic Energy Agency delegation to Iran, declaredyesterday that the agency has reached an agreementwith Tehran on the method of conducting thenegotiations pertaining to its nuclear program.s2: leader of international atomic energy agencydelegation to iran , olli heinonen said yesterday ,that the agency concluded a mutual understandingwith tehran on the way to manage talks dependingupon its atomic program .We will have ngrams with n ranging from 1 to 7,such as,unigram: of, to, its, program, yesterdaybigram: olli heinonentrigram: that the agencyfour-gram: with tehran on theseven-gram: international atomic energy agencydelegation to iranSimilarity between two sentences is calculated bycounting the number of overlapped n-words.
Thesimilarity for s1 and s2 is, (1 + 1 + 1 + 1 + 1) +(2)2 + (3)2 + (4)2 + (7)2 = 83.2.2 String kernelFor the STS-TYPED task, when comparing whetherpeople or authors are similar or not, we found thatsome entity mentions may have tiny variations, forexample,E Vincent Harris and E.Vincent HarrisThe difference between the entities lies in fact thatthe second entity has one more dot.
In this case,string kernel would be a good choice in verifyingthey are similar or not.
If we consider n=2, we obtain79-dimensional feature space where the two entitiesare mapped in Table 3.In Table 3, ?
is the decay factor, in the rangeof [0,1], that penalizes the longer distance of asubsequence.
Formally, string kernel is defined as,Kn(s, t) =?u??n?
?u(s) ?
?u(t)?
(4)92ev ei en ?
?
?
e. ?
?
?
rs is?
(evincentharris) ?2 ?3 + ?13 ?2 + ?4 + ?7 ?
?
?
0 ?
?
?
?3 + ?4 ?2 + ?12?
(e.vincentharris) ?3 ?4 + ?14 ?2 + ?5 + ?8 ?
?
?
?2 ?
?
?
?3 + ?4 ?2 + ?12Table 3: Feature mapping for two entitiesTEAM headlines OnWN FNWN SMT mean rankRUN1 0.5176 0.1517 0.2496 0.2914 0.3284 77Table 4: Experimental results for STS-COREwhere?n is the set of all possible subsequencesof length n. u indicates an item in the set, forexample, the subsequence ev in Table 3.
?u(s) isthe feature mapping of the subsequences in s. Inso doing, we can have similarity between entities inTable 3 as follows:Kn(s, t) = ?2?
?3 + (?3 + ?13)?
(?4 + ?14) +?
?
?+(?3+?4)?(?3+?4)+(?2+?12)?
(?2+?12)To avoid enumeration of all subsequences forsimilarity measurement, dynamic programming,similar to the method by Lodhi et al(2002) is usedhere for similarity calculation.3 ExperimentsThe STS-CORE task is to quantify how similartwo sentences are.
We simply use the senseoverlapping approach to compute the similarity.Since this approach needs to find appropriate sensesfor each word based on its contexts.
The numberof contextual words is set to 5.
Experimentsare conducted on four datasets.
They are:headlines mined from news sources by EuropeanMedia, OnWN extracted from from WordNet andOntoNotes, FNWN from WordNet and FrameNetand SMT dataset from DARPA GALE HTER andHyTER.
The results of our system (PolyUCOMP-RUN1) are given in Table 4 ,Our system achieves rather lower performancein the OnWN and FNWN datasets.
This is becauseit is difficult to use contextual terms to find thecorrect senses for words in sentences of these twodatasets.
Take the two sentences in OnWN datasetfor example,s1: the act of choosing among alternativess2: the act of changing one thing for anotherthing.The valid concepts for the two sentences are:c1: 06532095-n 05790944-nc2: 00030358-n 00126264-v 00002452-n00002452-nc1 and c2 have no shared senses, resulting in azero similarity between s1 and s2.
However, s1 ands2 should have the same meaning.
Moreover, in theFNWN dataset, the sentence lengths are unbalanced,for example,s1: there exist a number of different possibleevents that may happen in the future.
in most cases,there is an agent involved who has to consider whichof the possible events will or should occur.
a saliententity which is deeply involved in the event may alsobe mentioned.s2: doing as one pleases or chooses;s1 has 48 tokens with punctuations beingexcluded and s2 has only 6 tokens.
This would affectour system performance as well.For the STS-TYPED task, data set is takenfrom Europeana, which provides millions of books,paintings, films, museum objects and archivalrecords that have been digitised throughout Europe.Each item has one line per type, where the typecan be the title of a record, list of subject terms,textual description of the record, creator of therecord and date of the record.
Participating systemsare supposed to compute similarities between semi-structured items.
In this task, we take the strategiesin Table 5,Jaccard denotes the Jaccard similarity measure.Stringkernel + Jaccard means that two typesare similar if they share many terms, for example,93TEAM general author people time location event subject description mean rankRUN1 0.4888 0.6940 0.3223 0.3820 0.3621 0.1625 0.3962 0.4816 0.4112 12RUN2 0.4893 0.6940 0.3253 0.3777 0.3628 0.1968 0.3962 0.4816 0.4155 11RUN3 0.4915 0.6940 0.3254 0.3737 0.3667 0.2207 0.3962 0.4816 0.4187 10Table 6: Experimental results for STS-TYPEDType Strategyauthor String kernelpeople String kernel + Jaccardtime String kernel + Jaccardlocation String kernel + Jaccardevent WordNet + Jaccardsubject Sense overlappingdescription Sense overlappingTable 5: Strategies for computing similaritylocation; and string kernel is used to determinewhether two locations are similar or not.
For thetype of event, we extract verbs from records andcount the number of shared verbs between tworecords.
The verb similarity is obtained throughWordNet.
The general similarity is equal to theaverage of the 7 scores.
Also, Stanford CoreNLPtool1 is used to extract author, date, time, locationand handle part-of-speech tagging.In this STS-TYPED task, we use string kernel andWordNet to determine whether two terms are similarand increase the number of counts if their similarityexceeds a certain threshold.
Therefore, we havechosen 0.4, 0.5 and 0.6 in a heuristic manner andobtained three different runs.
Experimental resultsare given in Table 6.Since the types of author, subject anddescription are not related to either string kernelor WordNet, their performances remain unchangedduring three runs.4 Conclusions and Future WorkIn the Semantic Textual Similarity task of SemEval-2013, to capture the meaning between sentences,we proposed to disambiguate word senses usingcontexts and then determine sentence similarityby counting the senses they shared.
First, wordsenses are disambiguated by means of the contextual1http://nlp.stanford.edu/software/corenlp.shtmlwords.
When determining similarity between twosenses (synsets), n-word overlapping approach isused for counting the number of shared wordsin two glosses.
Besides, string kernel is usedto capture similarity between entities to avoidvariations between entities.
Our approach is simpleand we will apply regression models to determinesentence similarity on the basis of these features infuture work.ReferencesDaniel B., Chris Biemann, Iryna Gurevych and TorstenZesch.
2012.
UKP: Computing Semantic TextualSimilarity by Combining Multiple Content SimilarityMeasures.
Proceedings of the 6th InternationalWorkshop on Semantic Evaluation (SemEval 2012), inconjunction with the First Joint Conference on Lexicaland Computational Semantics (*SEM 2012).Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonza-lez-Agirre.
2012.
SemEval-2012 Task 6: A Piloton Semantic Textual Similarity.
Proceedings of the6th International Workshop on Semantic Evaluation(SemEval 2012), in conjunction with the First JointConference on Lexical and Computational Semantics(*SEM 2012).Frane Saric, Goran Glavas, Mladen Karan, Jan Snajderand Bojana Dalbelo Basia.
2012.
TakeLab: Systemsfor Measuring Semantic Text Similarity.
Proceedingsof the 6th International Workshop on SemanticEvaluation (SemEval 2012), in conjunction with theFirst Joint Conference on Lexical and ComputationalSemantics (*SEM 2012).Gunes Erkan and Dragomir R. Radev.
2004.
Lexrank:Graph-based Lexical Centrality as Salience in TextSummarization.
Journal of Artificial IntelligenceResearch, 22(2004):457?479.Huma Lodhi, Craig Saunders, John Shawe-Taylor,Nello Cristianini, and Chris Watkins.
2002.
TextClassification using String Kernels.
The Journal ofMachine Learning Research, 2(2002):419?444.Jian Xu, Qin Lu and Zhengzhong Liu.
2012.PolyUCOMP: Combining Semantic Vectors withSkip-bigrams for Semantic Textual Similarity.94Proceedings of the 6th International Workshop onSemantic Evaluation (SemEval 2012), in conjunctionwith the First Joint Conference on Lexical andComputational Semantics (*SEM 2012).Ou Jin, Nathan Nan Liu, Yong Yu and Qiang Yang 2011.Transferring Topical Knowledge from Auxiliary LongText for Short Text Understanding.
Proceedings of the20th ACM Conference on Information and KnowledgeManagement (ACM CIKM 2011).Rada Mihalcea and Courtney Corley.
2006.
Corpusbasedand Knowledge-based Measures of Text SemanticSimilarity.
Proceeding of the Twenty-First NationalConference on Artificial Intelligence and theEighteenth Innovative Applications of ArtificialIntelligence Conference..Satanjeev Banerjee and Ted Pedersen.
2003.
ExtendedGloss Overlaps as a Measure of Semantic Relatedness.Proceedings of the 18th International JointConference on Artificial Intelligence.Vasileios Hatzivassiloglou, Judith L. Klavans , EleazarEskin.
1999.
Detecting Text Similarity over ShortPassages: Exploring Linguistic Feature Combinationsvia Machine Learning.
Proceeding of EmpiricalMethods in natural language processing and VeryLarge Corpora.Yuhua Li, David Mclean, Zuhair B, James D. O?sheaand Keeley Crockett.
2006.
Sentence SimilarityBased on Semantic Nets and Corpus Statistics.
IEEETransactions on Knowledge and Data Engineering,18(8):1138?1149.95
