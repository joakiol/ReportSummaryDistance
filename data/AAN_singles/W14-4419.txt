Proceedings of the 8th International Natural Language Generation Conference, pages 123?127,Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational LinguisticsGenerating Annotated Graphs using the NLG Pipeline ArchitectureSaad Mahamood, William Bradshaw and Ehud ReiterArria NLG plcAberdeen, Scotland, United Kingdom{saad.mahamood, william.bradshaw, ehud.reiter}@arria.comAbstractThe Arria NLG Engine has been extendedto generate annotated graphs: data graphsthat contain computer-generated textualannotations to explain phenomena in thosegraphs.
These graphs are generated along-side text-only data summaries.1 IntroductionArria NLG1 develops NLG solutions, primarily inthe data-to-text area.
These solutions are NLGsystems, which generate textual summaries oflarge numeric data sets.
Arria?s core product isthe Arria NLG Engine,2 which is configured andcustomised for the needs of different clients.Recently Arria has extended this core engineso that it can automatically produce annotatedgraphs, that is, data graphs that have textual an-notations explaining phenomena in those graphs(see example in Figure 1).
This was developed af-ter listening to one of our customers, whose staffmanually created annotated graphs and found thisprocess to be very time-consuming.
The anno-tated graph generation process is integrated intothe NLG pipeline, and is carried out in conjunc-tion with the generation of a textual summary of adata set.In this short paper we summarise the relevantresearch background, and briefly describe what wehave achieved in this area.2 Background: Multimodality and NLGRich media such as web pages and electronic doc-uments typically include several modalities in agiven document.
A web page, for example, cancontain images, graphs, and interactive elements.Because of this there has been an interest within1Arria NLG plc (https://www.arria.com)2For more information see: https://www.arria.com/technology-A300.phpthe NLG community in generating multimodaldocuments.
However, basic questions remain ashow best to combine and integrate multiple modal-ities within a given NLG application.2.1 Annotated GraphicsSripada and Gao (2007) conducted a small studywhere they showed scuba divers different possi-ble outputs from their ScubaText system, includingtext-only, graph-only and annotated graphs.
Theyfound that divers preferred the annotated graphpresentation.
The ScubaText software could not inpractice produce annotated graphs for arbitrary in-put data sets and automatically set the scale basedon detected events, so this was primarily a studyof user preferences.McCoy and colleagues have been developingtechniques to automatically generate textual sum-maries of data graphics for visually impaired users(Demier et al., 2008).
This differs from our workbecause their goal is to replace the graph, whereasour goal is to generate an integrated text/graphicspresentation.There were several early systems in the 1990s(Wahlster et al., 1993; Feiner and McKeown,1990, for example), which generated integratedpresentations of figures and texts, but these sys-tems focused on annotating static pictures and dia-grams, not data graphics.
The WIP system, whichcombined static diagram images and text, used adeep planning approach to produce tightly inte-grated multimodal documents; it is not clear howrobustly this approach handled new data sets andcontexts.2.2 Embodied Conversational AgentsIn recent years the challenge of combining mul-tiple modalities such as text, speech, and/or ani-mation has been addressed in the context of Em-bodied Conversational Agents or ECAs.
One ex-ample is the NECA system (Krenn et al., 2002).123It allowed two embodied agents to converse witheach other via spoken dialogue while being ableto make gestures as well.
From an architecturalperspective, NECA used a pipeline architecture insome ways similar to the standard NLG data-to-text pipeline (Reiter and Dale, 2000).
DocumentPlanning is handled by the Scene Generator, whichselects the dialogue content.
The ?Multi-modalNLG?
(M-NLG) component handles Microplan-ning and Surface Realisation, and also deals withspecifying gestures, mood, and information struc-ture.
Thus the textual output generated by the sur-face realiser in the NECA M-NLG component isannotated with metadata for other modalities.
Inparticular, information on gestures, emotions, in-formation structure, syntactic structure and dia-logue structure (Krenn et al., 2002) are also in-cluded to help inform the speech synthesis andgesture assignment modules.2.3 Background: PsychologyThe question of whether information is best pre-sented in text or graphics is in principle largely onefor cognitive psychology.
Which type of presenta-tion is most effective, and in which context?
Theanswer of course depends on the communicativegoal, the type of data being presented, the type ofuser, the communication medium and other con-textual factors.In particular, a number of researchers (Petre,1995, for example) have pointed out that graphicalpresentations require expertise to interpret themand hence may be more appropriate for experi-enced users than for novices.
Tufte (1983) pointsout that statistical graphs can be very misleadingfor people who are not used to interpreting them.Alberdi et al (2001) report on a number of psy-chological studies on effectiveness of data visual-isations which were performed with clinicians ina Neonatal Intensive Care Unit (NICU).
At a highlevel, these studies found that visualisations wereless effective and less used than had been hoped.Detailed findings include the following:?
Although consultants, junior doctors, andnurses all claimed in interviews to makeheavy use of the computer system whichdisplayed visualisations, when observed on-ward only senior consultants actually did so;junior doctors and nurses rarely looked at thecomputer screen.?
Senior consultants were much better than ju-nior staff at distinguishing real events fromnoise (sensor artefacts).?
Even senior consultants could only identify70% of key events purely from the visualisa-tions.Law et al (2005) followed up the above work byexplicitly comparing the effectiveness of visuali-sations and textual summaries.
The textual sum-maries in the experiment were manually written,but did not contain any diagnoses and instead fo-cused on describing the data.
Law et al found thatclinicians at all levels made better decisions whenshowed the textual summaries; however at all lev-els they preferred the visualisations.A similar study with computer generated sum-mary texts produced by the Babytalk BT45 sys-tem (Portet et al., 2009), conducted by van derMeulen et al (2008), found that decision qualitywas best when clinicians were shown manuallywritten summaries; computer generated texts wereof similar effectiveness to visualisations.
An er-ror analysis of this study (Reiter et al., 2008) con-cluded that computer generated texts were muchmore effective in some contexts than in others.An implication of the above studies is that inmany cases the ideal strategy is to produce bothtext and graphics.
This increases decision effec-tiveness (since the modalities work best in differ-ent situations), and also increases user satisfaction,since users see the modality they like as well as theone which is most effective for decision support.2.4 Annotated Graphs in NLG EngineWe have extended our NLG Engine to generate an-notated graphs as well as texts; an example output,generated by a demonstration system, is shown infigure 1.
This example shows a very simple textualoutput; examples of more complex textual outputare on the Arria website3.This example output shows a comparison ofperformance between the FTSE 100 and a givenstock portfolio.
The value of the FTSE 100 is usedas a performance benchmark to see if a given stockportfolio is performing better or worse than com-pared to the stock market in general.As can be seen in the graph in figure 1, the anno-tations are small text fragments, which are placed3A more detailed example is given here: https://www.arria.com/case-study-oilgas-A231.php124Figure 1: Combined text and annotated graph detailing the stock portfolio performanceFigure 2: Graph illustrating stacking capabilitieswhen two annotations intersect each otheron top of the graph, and are linked to the relevantevents in that occur in the graph.
Annotations canalso be placed at the bottom of graphs and at thesides and can rearrange themselves depending onthe space available.
In figure 1 the range annota-tion used indicates the reason for the increase inthe value of a given stock portfolio over a particu-lar time period.
If one or more annotations collideor intersect a stacking algorithm is used prior topresentation to rearrange the placement of collid-ing annotations as shown in figure 2.Figure 3 illustrates the architecture that is usedby our NLG engine.
The data analysis and datainterpretation modules analyse the input data andproduce a set of messages which can be communi-cated to the user in the generated report.
The doc-ument planner decides on which messages shouldbe communicated overall, and where messagesshould appear (for example, situational analysistext, diagnosis text, impact text, graph annotation,or a combination of these).
The document planneralso decides on the type of graph used, and whichdata channels it displays; these data channels mustinclude any channels which are annotated, but insome cases other channels are displayed as well.Once document planning is complete, the vi-sualisation planning module generates the graphdesign, including X and Y scale and the positionof the annotations on the graph.
The time rangeshown in the graph is largely determined by theannotation messages.
In other words, the decisionabout what data to show on the graph is partiallydriven by the annotations.The annotation microplanner and realiser gener-ate the actual annotation texts, using special ruleswhich are optimised for annotations (which need125Figure 3: Pipeline architecture of the Arria NLGEngineto be short and have different referring expres-sions).
After this has been completed, a rendererproduces the actual annotated graph.
The finaltask lies with the presenter module, which recom-bines the separately generated summary text (gen-erated by the NLG Microplanning and Realisationmodules) with the annotated graphs.3 ConclusionAnnotated graphs are a very appealing mechanismfor combining text and data graphics into a sin-gle multimodal information presentation; this isshown both by the findings of Sripada and Gao(2007) and by the experiences of our customers.Amongst other benefits, we believe that annotatedgraphs will address some of the deficiencies indata graphics which were pointed out by Alberdiet al (2001), by helping users (especially inexpe-rienced ones) to more easily identify key eventsin a graph and also to distinguish real events fromsensor artefacts and other noise.We have developed software to create annotatedgraphs, by modifying the standard NLG data-to-text pipeline as described above.
Our clients havereacted very positively so far, and we are now ex-ploring extensions, for example by making anno-tated graphs interactive.ReferencesE.
Alberdi, J. C. Becher, K. J. Gilhooly, J. R.W.
Hunter,R.
H. Logie, A. Lyon, N. McIntosh, and J. Reiss.2001.
Expertise and the interpretation of comput-erised physiological data: Implications for the de-sign of computerised physiological monitoring inneonatal intensive care.
International Journal ofHuman Computer Studies, 55(3):191?216.S.
Demier, S. Carberry, and K. F. McCoy.
2008.
Gen-erating textual summaries of bar charts.
In FifthInternational Natural Language Generation Con-ference (INLG 2008), pages 7?15.
Association forComputational Linguistics.S.
Feiner and K. R. McKeown.
1990.
Generating Co-ordinated Multimedia Explanations.
In Sixth Con-ference on Artificial Intelligence Applications, vol-ume 290-296.B.
Krenn, H. Pirker, M. Grice, S. Baumann, P. Pi-wek, K. van Deemter, M. Schroeder, M. Klesen, andE.
Gstrein.
2002.
Generation of multi-modal di-alogue for a net environment.
In Proceedings ofKONVENS-02, Saarbruecken, Germany.A.
S. Law, Y.
Freer, J.
Hunter, R. H. Logie, N. McIn-tosh, and J. Quinn.
2005.
A comparison of graph-ical and textual presentations of time series data tosupport medical decision making in the neonatal in-tensive care unit.
Journal of Clinical Monitoringand Computing, 19(3):183?194.M.
Petre.
1995.
Why Looking isn?t always See-ing: Readership Skills and Graphical Programming.Communications of the ACM, 38:33?44.F.
Portet, E. Reiter, A. Gatt, J.
Hunter, S. Sripada,Y.
Freer, and C. Sykes.
2009.
Automatic gener-ation of textual summaries from neonatal intensivecare data.
Artificial Intelligence, 173(7-8):789?816.E.
Reiter and R. Dale.
2000.
Building Natural Lan-guage Generation Systems.
Cambridge UniversityPress.E.
Reiter, A. Gatt, F. Portet, and M. van der Meulen.2008.
The Importance of Narrative and OtherLessons from an Evaluation of an NLG System thatSummarises Clinical Data.
Fifth International Natu-ral Language Generation Conference (INLG 2008),pages 147?155.S.
G. Sripada and F. Gao.
2007.
Summarizing divecomputer data: A case study in integrating textualand graphical presentations of numerical data.
InMOG 2007 Workshop on Multimodal Output Gen-eration, pages 149?157.126E.
Tufte.
1983.
The Visual Display of QuantitativeInformation.
Graphics Press.M.
van der Meulen, R. Logie, Y.
Freer, C. Sykes,N.
McIntosh, and J.
Hunter.
2008.
When a graphis poorer than 100 words: A comparison of com-puterised natural language generation, human gen-erated descriptions and graphical displays in neona-tal intensive care.
Applied Cognitive Psychology,24:77?89.W.
Wahlster, E.
Andre?, W. Finkle, HJ.
Profitlich, andT.
Rist.
1993.
Plan-based integration of naturallanguage and graphics generation.
Artificial Intel-ligence, 63:387?427.127
