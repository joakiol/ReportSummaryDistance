Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 25?28,Paris, October 2009. c?2009 Association for Computational LinguisticsAutomatic Adaptation of Annotation Standards for Dependency Parsing?
Using Projected Treebank as Source CorpusWenbin Jiang and Qun LiuKey Lab.
of Intelligent Information ProcessingInstitute of Computing TechnologyChinese Academy of SciencesP.O.
Box 2704, Beijing 100190, China{jiangwenbin, liuqun}@ict.ac.cnAbstractWe describe for dependency parsing an an-notation adaptation strategy, which can au-tomatically transfer the knowledge froma source corpus with a different annota-tion standard to the desired target parser,with the supervision by a target corpus an-notated in the desired standard.
Further-more, instead of a hand-annotated one, aprojected treebank derived from a bilin-gual corpus is used as the source cor-pus.
This benefits the resource-scarcelanguages which haven?t different hand-annotated treebanks.
Experiments showthat the target parser gains significant im-provement over the baseline parser trainedon the target corpus only, when the targetcorpus is smaller.1 IntroductionAutomatic annotation adaptation for sequence la-beling (Jiang et al, 2009) aims to enhance atagger with one annotation standard by transfer-ring knowledge from a source corpus annotated inanother standard.
It would be valuable to adaptthis strategy to parsing, since for some languagesthere are also several treebanks with different an-notation standards, such as Chomskian-style PennTreebank (Marcus et al, 1993) and HPSG LinGoRedwoods Treebank (Oepen et al, 2002) for En-glish.
However, we are not content with conduct-ing annotation adaptation between existing differ-ent treebanks, because it would be more valuableto boost the parsers also for the resource-scarcelanguages, rather than only for the resource-richones that already have several treebanks.Although hand-annotated treebanks are costlyand scarce, it is not difficult for many languages tocollect large numbers of bilingual sentence-pairsaligned to English.
According to the word align-ment, the English parses can be projected acrossto their translations, and the projected trees can beleveraged to boost parsing.
Many efforts are de-voted to the research on projected treebanks, suchas (Lu?
et al, 2002), (Hwa et al, 2005) and(Ganchev et al, 2009), etc.
Considering the factthat a projected treebank partially inherits the En-glish annotation standard, some hand-written rulesare designed to deal with the divergence betweenlanguages such as in (Hwa et al, 2002).
How-ever, it will be more valuable and interesting toadapt this divergence automatically and boost theexisting parsers with this projected treebank.In this paper, we investigate the automatic anno-tation adaptation strategy for Chinese dependencyparsing, where the source corpus for adaptation isa projected treebank derived from a bilingual cor-pus aligned to English with word alignment andEnglish trees.
We also propose a novel, error-tolerant tree-projecting algorithm, which dynam-ically searches the project Chinese tree that hasthe largest consistency with the corresponding En-glish tree, according to an alignment matrix ratherthan a single alignment.
Experiments show thatwhen the target corpus is smaller, the projectedChinese treebank, although with inevitable noisecaused by non-literal translation and word align-ment error, can be successfully utilized and re-sult in significant improvement over the baselinemodel trained on the target corpus only.In the rest of the paper, we first present the tree-projecting algorithm (section 2), and then the an-notation adaptation strategy (section 3).
After dis-cussing the related work (section 4) we show theexperiments (section 5).2 Error-Tolerant Tree-ProjectingAlgorithmPrevious works making use of projected cor-pus usually adopt the direct-mapping method forstructure projection (Yarowsky and Ngai, 2001;Hwa et al, 2005; Ganchev et al, 2009), where25some filtering is needed to eliminate the inaccurateor conflicting labels or dependency edges.
Herewe propose a more robust algorithm for depen-dency tree projection.
According to the align-ment matrix, this algorithm dynamically searchesthe projected Chinese dependency tree which hasthe largest consistency with the corresponding En-glish tree.We briefly introduce the alignment matrix be-fore describing our projecting algorithm.
Givena Chinese sentence C1:M and its English transla-tion E1:N , the alignment matrix A is an M ?
Nmatrix with each element Ai,j denoting the proba-bility of Chinese word Ci aligned to English wordEj .
Such structure potentially encodes many morepossible alignments.Using C(TC |TE , A) to denote the degree of Chi-nese tree TC being consistent with English tree TEaccording to alignment matrix A, the projecting al-gorithm aims to findT?C = argmaxTCC(TC |TE , A) (1)C(TC |TE , A) can be factorized into each depen-dency edge x ?
y in TC , that is to sayC(TC |TE , A) =?x?y?TCCe(x ?
y|TE , A) (2)We can obtain Ce by simple accumulation acrossall possible alignmentsCe(x ?
y|TE, A)= ?1?x?,y??|E|Ax,x?
?Ay,y?
?
?
(x?, y?|TE) (3)where ?
(x?, y?|TE) is a 0-1 function that equals 1only if x?
?
y?
exists in TE .The searching procedure, argmax operation inequation 1, can be effectively solved by a simple,bottom-up dynamic algorithm with cube-pruningspeed-up (Huang and Chiang, 2005).
We omit thedetailed algorithm here due to space restrictions.3 Annotation Adaptation forDependency ParsingThe automatic annotation adaptation strategy forsequence labeling (Jiang et al, 2009) aims tostrengthen a tagger trained on a corpus annotatedin one annotation standard with a larger assistantcorpus annotated in another standard.
We can de-fine the purpose of the automatic annotation adap-tation for dependency parsing in the same way.Similar to that in sequence labeling, the train-ing corpus with the desired annotation standard iscalled the target corpus while the assistant cor-pus annotated in a different standard is calledthe source corpus.
For training, an intermediateparser, called the source parser, is trained directlyon the source corpus and then used to parse the tar-get corpus.
After that a second parser, called thetarget parser, is trained on the target corpus withguide features extracted from the source parser?sparsing results.
For testing, a token sequence isfirst parsed by the source parser to obtain an inter-mediate parsing result with the source annotationstandard, and then parsed by the target parser withthe guide features extracted from the intermediateparsing result to obtain the final result.The design of the guide features is the most im-portant, and is specific to the parsing algorithm ofthe target parser.
In this work we adopt the max-imum spanning tree (MST) algorithm (McDon-ald et al, 2005; McDonald and Pereira, 2006) forboth the source and the target parser, so the guidefeatures should be defined on dependency edgesin accordance with the edge-factored property ofMST models.
In the decoding procedure of thetarget parser, the degree of a dependency edge be-ing supported can be adjusted by the relationshipbetween this edge?s head and modifier in the in-termediate parsing result of the source parser.
Themost intuitionistic relationship is whether the de-pendency between head and modifier exists in thisintermediate result.
Such a bi-valued relationshipis similar to that in the stacking method for com-bining dependency parsers (Martins et al, 2008;Nivre and McDonald, 2008).
The guide featuresare then defined as this relationship itself as well asits combinations with the lexical features of MSTmodels.Furthermore, in order to explore more de-tailed knowledge from the source parser, we re-define the relationship as a four-valued variablewhich covers the following situations: parent-child, child-parent, siblings and else.
With theguide features, the parameter tuning procedure ofthe target parser will automatically learn the regu-larity of using the source parser?s intermediate re-sult to guide its decision making.4 Related WorksMany works have been devoted to obtain pars-ing knowledge from word aligned bilingual cor-26pora.
(Lu?
et al, 2002) learns Chinese bracket-ing knowledge via ITG alignment; (Hwa et al,2005) and (Ganchev et al, 2009) induces depen-dency grammar via projection from aligned En-glish, where some filtering is used to reduce thenoise and some hand-designed rules to handle lan-guage heterogeneity.Just recently, Smith and Eisner (2009) gavean idea similar to ours.
They perform depen-dency projection and annotation adaptation withQuasi-Synchronous Grammar (QG) Features.
Al-though both related to projection and annotation,there are still important differences between thesetwo works.
First, we design an error-tolerantalignment-matrix-based tree-projecting algorithmto perform whole-tree projection, while they re-sort to QG features to score local configurationsof aligned source and target trees.
Second, theiradaptation emphasizes to transform a tree fromone annotation standard to another, while ouradaptation emphasizes to strengthen the parser us-ing a treebank annotated in a different standard.5 ExperimentsThe source corpus for annotation adaptation, thatis, the projected Chinese treebank, is derived from5.6 millions LDC Chinese-English sentence pairs.The Chinese side of the bilingual corpus is word-segmented and POS-tagged by an implementationof (Jiang et al, 2008), and the English sentencesare parsed by an implementation of (McDonaldand Pereira, 2006) which is instead trained on WSJsection of Penn English Treebank (Marcus et al,1993).
The alignment matrixes for sentence pairsare obtained according to (Liu et al, 2009).
TheEnglish trees are then projected across to Chineseusing the algorithm in section 2.
Out of these pro-jected trees, we only select 500 thousands withword count l s.t.
6 ?
l ?
100 and with project-ing confidence c = C(TC |TE , A)1/l s.t.
c ?
0.35.While for the target corpus, we take Penn ChineseTreebank (CTB) 1.0 and CTB 5.0 (Xue et al,2005) respectively, and follow the traditional cor-pus splitting: chapters 271-300 for testing, chap-ters 301-325 for development, and else for train-ing.We adopt the 2nd-order MST model (McDon-ald et al, 2005) as the target parser for betterperformance, and the 1st-order MST model asthe source parser for fast training.
Both the twoparsers are trained with averaged perceptron algo-Model P% on CTB 1 P% on CTB 5source parser 53.28 53.28target parser 83.56 87.34baseline parser 82.23 87.15Table 1: Performances of annotation adaptationwith CTB 1.0 and CTB 5.0 as the target corpus re-spectively, as well as of the baseline parsers (2nd-order MST parsers trained on the target corpora).0.70.750.80.85100  1000  10000dependencyaccuracysentence count of target corpusbaselinetarget parserFigure 1: Performance of the target parsers withtarget corpora of different scales.rithm (Collins, 2002).
The development set ofCTB is also used to determine the best model forthe source parser, conditioned on the hypothesisof larger isomorphisme between Chinese and En-glish.Table 1 shows that the experimental results ofannotation adaptation, with CTB 1.0 and CTB 5.0as the target corpus respectively.
We can see thatthe source parsers, directly trained on the sourcecorpora of projected trees, performs poorly onboth CTB test sets (which are in fact the same).This is partly due to the noise in the projected tree-bank, and partly due to the heterogeneous betweenthe CTB trees and the projected trees.
On thecontrary, automatic annotation adaptation effec-tively transfers the knowledge to the target parsers,achieving improvement on both target corpora.Especially on CTB 1.0, an accuracy increment of1.3 points is obtained over the baseline parser.We observe that for the much larger CTB 5.0,the performance of annotation adaptation is muchlower.
To further investigate the adaptation perfor-mances with target corpora of different scales, weconduct annotation adaptation on a series of tar-get corpora which consist of different amount ofdependency trees from CTB 5.0.
Curves in Fig-ure 1 shows the experimental results.
We see thatthe smaller the training corpus is, the more signif-icant improvement can be obtained.
For example,27with a target corpus composed of 2K trees, nearly2 points of accuracy increment is achieved.
Thisis a good news to the resource-scarce languages.6 Conclusion and Future WorksThis paper describes for dependency parsing anautomatic annotation adaptation strategy.
Whatis more important, we use a projected treebank,rather than a hand-annotated one, as the sourcecorpus for adaptation.
This is quite different fromprevious works on projected trees (Hwa et al,2005; Ganchev et al, 2009), and is also more valu-able than previous works of annotation adaptation(Jiang et al, 2009).
Experiments show that thisstrategy gains improvement over baseline parserswith target corpora of different scales, especiallythe smaller ones.
This provides a new strategy forresource-scarce languages to train high-precisiondependency parsers.
In the future, we will adaptthis strategy to constituent parsing, which is morechallenging and interesting due to the complexityof projection between constituent trees, and dueto the obscurity of annotation adaptation for con-stituent parsing.AcknowledgementThis project was supported by National NaturalScience Foundation of China, Contracts 60603095and 60736014, and 863 State Key Project No.2006AA010108.
We are grateful to the anony-mous reviewers for their valuable suggestions.
Wealso thank Yang Liu for sharing his codes of align-ment matrix generation, and Liang Huang andHaitao Mi for helpful discussions.ReferencesMichael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and exper-iments with perceptron algorithms.
In Proceedingsof the EMNLP, pages 1?8, Philadelphia, USA.Kuzman Ganchev, Jennifer Gillenwater, and BenTaskar.
2009.
Dependency grammar induction viabitext projection constraints.
In Proceedings of the47th ACL.Liang Huang and David Chiang.
2005.
Better k-bestparsing.
In Proceedings of the IWPT, pages 53?64.Rebecca Hwa, Philip Resnik, Amy Weinberg, andOkan Kolak.
2002.
Evaluating translational corre-spondence using annotation projection.
In Proceed-ings of the ACL.Rebecca Hwa, Philip Resnik, Amy Weinberg, ClaraCabezas, and Okan Kolak.
2005.
Bootstrappingparsers via syntactic projection across parallel texts.In Natural Language Engineering, volume 11, pages311?325.Wenbin Jiang, Liang Huang, Yajuan Lu?, and Qun Liu.2008.
A cascaded linear model for joint chineseword segmentation and part-of-speech tagging.
InProceedings of the ACL.Wenbin Jiang, Liang Huang, and Qun Liu.
2009.
Au-tomatic adaptation of annotation standards: Chineseword segmentation and pos tagging?a case study.
InProceedings of the 47th ACL.Yang Liu, Tian Xia, Xinyan Xiao, and Qun Liu.
2009.Weighted alignment matrices for statistical machinetranslation.
In Proceedings of the EMNLP.Yajuan Lu?, Sheng Li, Tiejun Zhao, and Muyun Yang.2002.
Learning chinese bracketing knowledgebased on a bilingual language model.
In Proceed-ings of the COLING.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of english: The penn treebank.
In Computa-tional Linguistics.Andre?
F. T. Martins, Dipanjan Das, Noah A. Smith, andEric P. Xing.
2008.
Stacking dependency parsers.In Proceedings of EMNLP.Ryan McDonald and Fernando Pereira.
2006.
Onlinelearning of approximate dependency parsing algo-rithms.
In Proceedings of EACL, pages 81?88.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005.
Online large-margin training of de-pendency parsers.
In Proceedings of ACL, pages 91?98.Joakim Nivre and Ryan McDonald.
2008.
Integrat-ing graph-based and transition-based dependencyparsers.
In Proceedings of ACL.Stephan Oepen, Kristina Toutanova, Stuart Shieber,Christopher Manning Dan Flickinger, and ThorstenBrants.
2002.
The lingo redwoods treebank: Moti-vation and preliminary applications.
In In Proceed-ings of COLING.David Smith and Jason Eisner.
2009.
Parser adap-tation and projection with quasi-synchronous gram-mar features.
In Proceedings of EMNLP.Nianwen Xue, Fei Xia, Fu-Dong Chiou, and MarthaPalmer.
2005.
The penn chinese treebank: Phrasestructure annotation of a large corpus.
In NaturalLanguage Engineering.David Yarowsky and Grace Ngai.
2001.
Inducing mul-tilingual pos taggers and np bracketers via robustprojection across aligned corpora.
In Proceedingsof the NAACL.28
