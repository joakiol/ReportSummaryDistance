Empirical Estimates of Adaptation:The chance of Two Noriegas is closer to p/2 than p 2Kenneth W. ChurchAT&T Labs-Research, 180 Park Ave., Florham Park, NJ., USAkwc@research.att.comAbstractRepetition is very common.
Adaptive language models, which allow probabilities to change or adaptafter seeing just a few words of a text, were introduced in speech recognition to account for text cohesion.Suppose a document mentions Noriega once.
What is the chance that he will be mentioned again?
if thefirst instance has probability p, then under standard (bag-of words) independence assumptions, two in-stances ought to have probability p2, but we find the probability is actually closer to p/2.
The first men-tion of a word obviously depends on frequency, but surprisingly, the second does not.
Adaptation de-pends more on lexical content han fl'equency; there is more adaptation for content words (proper nouns,technical terminology and good keywords for information retrieval), and less adaptation for functionwords, cliches and ordinary first names.1.
IntroductionAdaptive language models were introduced in theSpeech Recognition literature to model repetition.Jelinek (1997, p. 254) describes cache-basedmodels which combine two estimates of word(ngram) probabilities, Prl., a local estimate basedon a relatively small cache of recently seenwords, and PrG, a global estimate based on alarge training corpus.1.
Additive:Pr A (w)= XPrL (w) +(1 - X) PrG (w)2.
Case-based:JX I Pr/.
(w) if w~ cache" Pr G (w) otherwise PF c (W) = 1~ 2Intuitively, if a word has been mentioned re-cently, then (a) the probability of that word (andrelated words) should go way up, and (b) manyother words should go down a little.
We will re-fer to (a) as positive adaptation and (b) asnegative adaptation.
Our empirical experimentsconfirm the intuition that positive adaptation,Pr(+adapt), is typically much larger than neg-ative adaptation, Pr( - adapt).
That is,Pr( +adapt) >> Pr(prior) > Pr(-adapt).
Twomethods, Pr( + adapt ) and Pr( + adapt2), willbe introduced for estimating positive adaptation.1.
Pr( +adapt 1)=PrOve test\[w~ history)2.
Pr(+adapt2)=Pr(k'>_2lk>_l )=d.f2/dflThe two methods produce similar results, usuallywell within a factor of two of one another.
Thefirst lnethod splits each document into two equalpieces, a hislory portion and a test portion.
Theadapted probabilities are modeled as the chancethat a word will appeal" in the test portion, giventhat it appeared in the history.
The secondmethod, suggested by Church and Gale (1995),models adaptation as the chance of a second lnen-tion (probability that a word will appear two orinore times, given that it appeared one or moretimes).
Pr(+adapt2) is approximated bydJ2/dfl, where c./\['k is the number of documentsthat contain the word/ngram k or more times.(dfa.
is a generalization of document .frequeno,,d.f~ a standard term in information Retrieval.
)Both inethods are non-parametric (unlike cachelnodels).
Parametric assumptions, when appro-priate, can be very powerful (better estimatesfrom less training data), but errors resulting frominappropriate assumptions can outweigh tilebenefits.
In this elnpirical investigation of themagnitude and shape o1' adaptation we decided touse conservative non-parametric methods tohedge against he risk of inappropriate parametricassumptions.The two plots (below) illustrate some of thereasons for being concerned about standardparametric assumptions.
The first plot shows thenumber of times that tile word "said" appears illeach of the 500 documents ill the Brown Corpus(Francis & Kucera, 1982).
Note that there arequite a few documents with more than 15 in-stances of "said," especially in Press and Fic-tion.
There are also quite a few documents withhardly any instances of "said," especially in theLearned genre.
We have found a similar patternin other collections; "said" is more common innewswire (Associated Press and Wall StreetJournal) than technical writing (Department ofEnergy abstracts).180The second plot (below) conlpares these f:hownCorpus o\[-)sorvations to a Poisson.
Tile circlesindicate the nulnber of docun-ierits that have .r in-stances of "said."
As mentioned above, Pressand Fiction docunlents can lilentioil "said" 15times or lllore, while doculllOlltS in the Learnedgenre might not mention the word at all.
The lineshows what woukt be expected under a Poisson.Clearly the line does not fit the circles very well.The probability of "said" depends on manyfactors (e.g, genre, topic, style, author) that makethe distributions broader than chance (Poisson).We lind especially broad distributions for wordsthat adapt a lot.
"said" in Brown CorpusRePress,bbi ,,sionBc_oreI i;i qi \[,i!0!
!ili111'I,;,!~,1 ?
~ ,:,iilLearned i o (4 O \  '|Ile-Lettt.
's FictiohHU n r, i'1'i I,~ i0 100 200 300 400 500document numberPoisson Doesn't Fitgix)OO;Oo' O0oO OOoo oO ?
00 5 10oo  00 oo oo 0 0 o o oo15 20 25 30freqWe will show that adaptation is huge.Pr(+ adapt) is ot'ten several orders of magnitudelarger than Pr(prior).
In addition, we lind thatPr(+adapt) has a very different shape fiomPr(prior).
By construction, Pr(prior) wu'iesover many orders o1' magnitude depending on thefrequency of the word.
Interestingly, though, wefind that Pr(+adapt) has ahnost no dependenceon word frequency, although there is a stronglexical dependence.
Some words adapt more thanothers.
The result is quite robust.
Words that ad-apt more in one cortms also tend to adapt more inanother corpus of similar material.
Both themagnitude and especially the shape (lack of de-pendence on fiequency as well as dependence oncontent) are hard to capture ill an additive-basedcache model.Later in the paper, we will study neighbmw,words that do not appear in the history but doappear in documents near the history using an in-formation retrieval notion of near.
We find thatneighbors adapt more than non-neighbors, but notas much as the history.
The shape is in betweenas well.
Neighbors have a modest dependency onfiequency, more than the history, but not as muchas the prior.Neighbors are an extension of Florian & Yarow-sky (1999), who used topic clustering to build alanguage model for contexts uch as: "It is atleast on the Serb side a real setback to lhe x.
"Their work was motivated by speech recognitionapl)lications where it would be desirable for thehmguage model to l'avor x = "peace" over x ="piece."
Obviously, acoustic evidence is notvery hell~l'tfl in this case.
Trigrams are also notvery helpful because the strongest clues (e.g.,"Serb," "side" and "setback") are beyond thewindow of three words.
Florian & Yarowskycluster documents into about 102 topics, andcompute a separate trigram language model foreach topic.
Neighbors are similar in spirit, butsupport more topics.2.
Es t imates  o f  Adaptation: Method 1Method 1 splits each document into two equalpieces.
The first hall' of each document is re-ferred to as the histoo, portion of the documentand the second half of each document is referredto as the test portion of the documenl.
The task isto predict he test portion of the document giventhe histm3,.
We star!
by computing a contingencytable for each word, as ilhlstrated below:l)ocuments containing "hostages" in 1990 APtest testhistory a =638 b =505histoo, c =557 d =76787This table indicates that there are (a) 638 doc-uments with hostages in both the first half(history) and the second half (test), (b) 505 doc-uments with "hostages" in just the first half, (c)557 documents with "hostages" in just thesecond halt', and (d) 76,787 documents with"hostages" in neither half.
Positive and negativeadaptation are detined in terms a, b, c and d.181Pr( + adapt I ) = Pr(w E test Iw e histoo,)aa+bPr ( -adapt  l )=Pr (we test\]-~we histoo,)= cc+dAdapted probabilities will be compared to:Pr (prior) = Pr( w e test) = ( a + c ) /DwhereD =a+b +c+d.Positive adaptation tends to be much large, thanthe prior, which is just a little larger than negativeadaptation, as illustrated in the table below for theword "hostages" in four years of the AssociatedPress (AP) newswire.
We find remarkably con-sistent results when we compare one yea," of theAP news to another (though topics do come andgo over time).
Generally, the differences ofinterest are huge (orders of magnitude) comparedto the differences among various control condi-tions (at most factors of two or three).
Note thatvalues are more similar within colmnns thanacross columns.Pr(+adapt) >> Pr(prior) > Pr(-adapt)prior +adapt -adapt source w0.014 0.56 0.0069 AP870.015 0.56 0.0072 AP900.013 0.59 0.0057 AP910.0044 0.39 0.0030 AP93hostages3.
Adaptation is LexicalWe find that some words adapt more than others,and that words that adapt more in one year of theAP also tend to adapt more in another year of theAP.
In general, words that adapt a lot tend tohave more content (e.g., good keywords for infor-mation retrieval (IR)) and words that adapt lesshave less content (e.g., function words).It is often assumed that word fi'equency is a good(inverse) con'elate of content.
In the psycholin-guistic literature, the term "high frequency" isoften used syrlouymously with "functionwords," and "low frequency" with "contentwords."
In IR, inverse document fiequency(IDF) is commonly used for weighting keywords.The table below is interesting because it ques-tions this very basic assumption.
We comparetwo words, "Kennedy" and "except," that areabout equally frequent (similar priors).Intuitively, "Kennedy" is a content word and"except" is not.
This intuition is supported bythe adaptation statistics: the adaptation ratio,Pr(+adapt) /Pr(pr ior) ,  is nmch larger for"Kennedy" than for "except."
A similar patternholds for negative adaptation, but in the reversedirection.
That is, Pr ( -adapt ) /P r (pr io r )  islnuch slnaller for "Kennedy" than for "except.
"Kenneclv adapts more than exceptprior +adapt -adapt source w0.012 0.27 0.0091 AP900.015 0.40 0.0084 AP910.014 0.32 0.0094 AP93Kennedy0.016 0.049 0.016 AP900.014 0.047 0.014 AP910.012 0.048 0.012 AP93exceptIn general, we expect more adaptation for betterkeywords (e.g., "Kennedy") and less adaptatioufor less good keywords (e.g., fnnction words suchas "except").
This observation runs counter tothe standard practice of weighting keywordssolely on the basis of frequency, without con-sidering adaptation.
In a related paper, Umemuraand Church (submitted), we describe a termweighting method that makes use of adaptation(sometimes referred to as burstiness).Distinctive surnames adapt morethan ordinary first namesprior +adapt -adapt source w0.0079 0.71 0.0026 AP90 Noriega0.0038 0.80 0.0009 AP910.0006 0.90 0.0002 AP90 Aristide0.0035 0.77 0.0009 AP910.0011 0.47 0.0006 AP90 Escobar0.0014 0.74 0.0006 AP910.068 0.18 0.059 AP90 John0.066 0.16 0.057 AP910.025 0.11 0.022 AP90 George0.025 0.13 0.022 AP910.029 0.15 0.025 AP90 Paul0.028 0.13 0.025 AP91The table above compares surnames with firstnames.
These surnames are excellent keywordsunlike the first names, which are nearly as uselessfor IR as function words.
The adaptation ratio,Pr(+adapt) /Pr (pr ior ) ,  is much larger for thesurnames than for the first names.What is the probability of seeing two Noriegas ina document?
The chance of the first one isp=0.006.
According to the table above, thechance of two is about 0.75p, closer to p/2 than1 )2.
Finding a rare word like Noriega in a doc-ument is like lighming.
We might not expect182lightning to strike twice, but it hapt)ens all thetime, especially for good keywords.4.
Smoothing (for low frequency words)Thus fitr, we have seen that adaptation can belarge, but to delnonstrate ile shape property (lackof dependence on frequency), tile counts in thecontingency table need to be smoothed.
Theproblem is that the estimates of a, b, c, d, and es-pecially estimates of the ratios of these quantities,become unstable when the counts are small.
Thestandard methods of smoothing in tile speech re-cognition literature are Good-Turing (GT) andtteld-Out (He), described in sections 15.3 & 15.4of Jelinek (1997).
In both cases, we let r be anobserved count of an object (e.g., the fi'equencyof a word and/or ngram), and r* be our bestestimate of r in another COl'pUS of the same size(all other things being equal).4.1 Standard Held-Out (He)He splits the training corpus into two halves.The first half is used to count r for all objects ofintercst (e.g., the frequency of all words in vocal>ulary).
These counts are then used to groupobjects into bins.
The r m bin contains all (andonly) tile words with count r. For each bin, wecolnpute N r, tile number of words in the r m bin.The second half of the training corpus is thenused to compute Cr, tile a,,,,re,,'~m~ ~,.~ frequency ofall the words in the r ~h bin.
The final result issimply: r*=Cr./N,,  ll' the two halves o1' tiletrail)ing corpora or the lest corpora have dilTercntsizes, then r* should be scaled appropriately.We chose He  in this work because it makes fewassumptions.
There is no parametric model.
Allthat is assumed is that tile two halves of tiletraining corpus are similar, and that both aresimilar to the testing corpus.
Even this assulnp-tion is a matter of some concern, since majorstories come and go over time.4.2 Application of He  to Contingency TablesAs above, the training corpus is split into twohalves.
We used two different years of AP news.The first hall' is used to count documentfrequency rl/: (Document frequency will be usedinstead of standard (term) frequency.)
Words arebinned by df and by their cell in the coutingencytable.
The first half of tile corpus is used tocompute the number of words in each bin: Nd, .,N4fj,, N41.
(: and Ndl.,,t; the second half of thecorpus is used to compute the aggregate doc-ument flequency for the words in each bin: C,!f, a,C41.,l), Cdl:,,c and C4f,d.
The final result is stro-p!y:c~}.=C,/.~/N~l.r and d~i=C4f,,//N4f,~/.
We ?
./ .I,: ,/,' , Icompute tile probabilities as before, but replace a,b, c, d with a *, b *, c *, d*, respectively.ho5n2?1History (h) >> Neighborhood (n) >> Prior (p)n " nPP5 IO Jro 1oo 5oo lOOOOocunmnt Frequent}, (d\[)With these smoothed estimates, we arc able toshow that Pr(+adcq~t), labeled h in tile plotabove, is larger and less dependent on fi'equencythan l)r(prior), labeled p. The plot shows a thirdgroup, labeled n for neighbors, which will bedescribed later.
Note that Ihe ns fall between tileps and tile hs.Thus far, we have seen that adaptation can behuge: Pr(+a&q)l)>> Pr(prior), often by two orthree orders of magnitude.
Perhaps even moresurprisingly, although Ihe first mention dependsstrongly on frequency (d./), the second does not.Some words adapt more (e.g., Noriega, Aristide,Escobar) and some words adapt less (e.g., John,George, Paul).
Tile results are robust.
Wordsthat adapt more in one year of AP news tend toadapt more in another year, and vice versa.5.
Method 2: l'r( + adapt2 )So far, we have limited our attention to the rel-atively simple case where the history and the testarc tile same size.
In practice, this won't be thecase.
We were concerned that tile observationsabove might be artil'acts omehow caused by thislimitation.We exl~erimented with two approaches for under-standing the effect of this limitation and foundthat the size of the history doesn't changePr(+adal)t ) very much.
The first approach splitthe history and the test at wlrious points rangingfrom 5% to 95%.
Generally, Pr(+adaptl ) in-creases as the size of the test portion grows rel-ative to the size o f  the history, but the effect is183relatively small (more like a factor of two than anorder of magnitude).We were even more convinced by the secondapproach, which uses Pr(+adapt2 ), a completelydifferent argument for estimating adaptation anddoesn't depend on the relative size of the historyand the test.
The two methods produceremarkably silnilar results, usually well within afactor of two of one another (even when adaptedprobabilities are orders of magnitude larger thanthe prior).Pr(+adapt2)  makes use of d./)(w), a generaliz-ation of document frequency, d,/)(w) is thenumber of documents with .j or more instances ofw; (dfl is the standard notion of dJ).Pr( + adapt 2 ) = Pr(k>_2 \[k>_ 1 ) = df2/(!/" 1Method 2 has some advantages and some disad-vantages in comparison with method 1.
On thepositive side, method 2 can be generalized tocompute the chance of a third instance:Pr(k>_31k>_2 ).
But unfortunately, we do notknow how to use method 2 to estimate negativeadaptation; we leave that as an open question.2Adaptation is huge (and hardly dependent on frequency)pt Ik  >= 3 \] k >= 2) = d\[3* / dr2".
.
.
.
.
.2 2 22222Pr(k >= 2 I k >= 1) = dr2' / dr1",1111111 11I10Pr(k >= 1)=df l * /DI I :1 100 1000 10030 100000Document  Froquoncy (all}The plot (above) is similar to the plot in section4.2 which showed that adapted probabilities(labeled h) are larger and less dependent onfrequency than the prior (labeled p).
So too, theplot (above) shows that the second and third men-tions of a word (labeled 2 and 3, respectively) arelarger and less dependent on frequency than thefirst mention (labeled 1).
The plot in section 4.2used method 1 whereas the plot (above) usesmethod 2.
Both plots use the He  smoothing, sothere is only one point per bin (df value), ratherthan one per word.6.
Neighborhoods (Near)Florian and Yarowsky's example, "It is at leaston the Serb side a real setback to the x," providesa nice motivation for neighborhoods.
Supposethe context (history) mentions a number of wordsrelated to a peace process, but doesn't mentionthe word "peace."
Intuitively, there should stillbe some adaptation.
That is, the probability of"peace" should go up quite a bit (positive adap-tation), and the probability of many other wordssuch as "piece" should go down a little (negativeadaptation).We start by partitioniug the vocabulary into threeexhaustive and mutually exclusive sets: hist, nearand other (abbreviations for history, neighbor-hood and otherwise, respectively).
The first set,hist, contains the words that appear in the firsthalf of the document, as before.
Other is acatchall for the words that are in neither of thefirst two sets.The interesting set is near.
It is generated byquery expansion.
The history is treated as aquery in an information retrieval document-ranking engine.
(We implemented our ownranking engine using simple IDF weighting.
)The neighborhood is the set of words that appearin the k= 10 or k = 100 top documents returned bythe retrieval engine.
To ensure that the three setspartition the vocabulary, we exclude the historyfiom the neighborhood:near = words in query expansion of hist - histThe adaptation probabilities are estimated using acontingency table like before, but we now have athree-way partition (hist, near and other) of thevocabulary instead of the two-way partition, as il-lustrated below.Documents containingI testhisto O, a =2125c = 1963"peace" in 1991 APb =2t60d =74573histi lea l "othertesta =2125 b =2160e =1479 f=22516g =484 h =52057In estilnating adaptation probabilities, we con-tinue to use a, b, c and d as before, but four newvariables are introduced: e, f, g and h, wherec=e+g andd=f+h.184Pr(wc test) =(a +c) /Dp; (  w c tes; l wc  hist ) =al( a + b )l ' r (we test lwc near) =e/(e +f )l 'r( w E test \[ w c other) =g/ (g  + h)priorhistneatotherThe table below shows that "Kennedy" adaptsmore than "except" and that "peace" adaptsmore than "piece."
That is, "Kennedy" has alarger spread than "except" between tile historyand tile otherwise case..l)Jior hist near other src w0.026 0.40 0.022 0.0050 AP91 Kennedy0.020 0.32 0.025 0.0038 AP930.026 0.05 0.018 0.0122 AP91 except0.019 0.05 0.014 0.0081 AP930.077 0.50 0.062 0.0092 AP91 peace0.074 0.49 0.066 0.0069 AP930.015 0.10 0.014 0.0066 AP91 piccc0.013 0.08 0.015 0.0046 AP93When (.\]/' is small (d/'< 100), I\]O smoothing isused to group words into bins by {!/\] Adaptationprol)abilities are computed for each bill, ratherthan for each word.
Since these probabilities areimplicitly conditional on ,qJ; they have ah'eadybeen weighted by (!fin some sense, and therefore,it is unnecessary to introduce an additionalexplicit weighting scheme based on (!/'or a simpletransl'orm thereof such as II)1:.The experiments below split tile neighborhoodinto four chisses, ranging fronl belier nei.g, hborsto wmwe neighbotw, del)ending oil expansionfrequency, e/\] el'(1) is a number between 1 and k,indicating how many of the k top scoring doc-uments contain I.
(Better neighbors appear inmore of the top scoring documents, and worseneighbors appear in fewer.)
All the neighborhoodclasses fall between hist and other, with betterneighbors adapting tllore than ~,OlSe neighbors.7.
Experimental ResultsRecall that the task is to predict the test portion(the second half) of a document given the histoo,(the first half).
The following table shows aselection of words (sorted by the third cohunn)from the test portion of one of the test doculnents.The table is separated into thirds by horizontallines.
The words in the top third receive nmchhigher scores by the proposed method (S) than bya baseline (B).
These words are such goodkeywords that one can faMy contidently guesswhat the story is about.
Most of these words re-ceive a high score because they were mentionedin the history portion of the document, but"laid-off" receives a high score by the neighl)or-hood mechanism.
Although "hiid-off" is notmentioned explicitly in the history, it is obviouslyclosely related to a number of words that were,especially "layoffs," but also "notices" and"cuts."
It is reassuring to see tile neighborhoodmechanism doing what it was designed to do.The middle third shows words whose scores areabout the same as the baseline.
These words tendto be function words and other low content wordsthat give tts little sense of what the document isabout.
The bottoln third contains words whosescores are much lower than the baseline.
Thesewords tend to be high in content, but misleading.The word ' al us , "  for example, might suggestthat story is about a military conflict.S l?, Iog2(S/B) Set Ternl0.19 0.00 I 1.06 hist Binder0.22 0.00 7.45 hist layoff0.06 0.00 5.71 hist notices0.36 0.01 5.66 hist 13oeing0.02 0.00 5.11 near3 laid-off0.25 0.02 3.79 hist cuts0.01 0.01 0.18 near3 projects0.89 0.81 0.15 hist said0.06 0.05 0. l 1 near4 announced0.06 0.06 0.09 near4 As0.00 0.00 0.09 ncat+l employed0.00 0.00 -0.61 other 7140.00 0.01 -0.77 other managed0.01 0.02 -1.05 near2 additional0.00 0.01 - 1.56 other wave0.00 0.03 -3.41 other armsThe proposed score, S, shown in colunln 1, is:Pr(wlhist) if wc histPr (wlnear i )  if wc nearjPr(w\[near 2) if wc near aPrs (w)= Pr(winear 3) if w6near  3l 'r(wJnear4) if we near4Pr(wiother) otherwisewhere near I through near 4 are four neighbor-l\]oods (k=100).
Words in near4 are the bestneighbors (e\[>10) and words in near I are theworst neighbors (e/'= 1).
Tile baseline, B, shownin column 2, is: Prl~(w)=df/D.
Colun\]i\] 3 con\]-pares the first two cohnnns.We applied this procedure to a year of the APnews and found a sizable gain in information on185average: 0.75 bits per word type per doculnent.In addition, there were many more big winners(20% of the documents gained 1 bit/type) thanbig losers (0% lost 1 bit/type).
The largestwinners include lists of major cities and theirtemperatures, lists of major currencies and theirprices, and lists of commodities and their prices.Neighborhoods are quite successful in guessingthe second half of such lists.On the other hand, there were a few big losers,e.g., articles that summarize the major stories ofthe clay, week and year.
The second half of asummary article is almost never about the samesubject its the first half.
There were also a fewend-of-document delimiters that were garbled intranslnission causing two different documents tobe treated as if they were one.
These garbleddocuments tended to cause trouble for theproposed method; in such cases, the historycomes fi'om one document and the test comesfrom another.In general, the proposed adaptation method per-formed well when the history is helpful for pre-dicting the test portion of the document, and itperformed poorly when the history is misleading.This suggests that we ought to measure topicshifts using methods uggested by Hearst (1994)and Florian & Yarowsky (1999).
We should notuse the history when we believe that there hasbeen a major topic shift.8.
ConclusionsAdaptive language models were introduced toaccount for repetition.
It is well known that thesecond instance of a word (or ngram) is nmchmore likely than the first.
But what we findsurprising is just how large the effect is.
Thechance of two Noriegas is closer to p/2 than p 2.in addition to the magnitude of adaptation, wewere also surprised by the shape: while the firstinstance of a word depends very strongly onfrequency, the second does not.
Adaptation de-pends more on content than flequency; adaptationis stronger for content words such as propernouns, technical terminology and good keywordsfor information retrieval, and weaker for functiollwords, cliches and first nalnes.The shape and magnitude of adaptation has im-plications for psycholinguistics, informationretrieval and language modeling.
Psycholinguis-tics has tended to equate word frequency withcontent, but our results suggest hat two wordswith similar frequency (e.g., "Kennedy" and"except") can be distinguished on the basis oftheir adaptation.
Information retrieval has tendedto use frequency in a similar way, weightingterms by IDF (inverse document frequency), withlittle attention paid to adaptation.
We propose aterm weighting method that makes use of adapta-tion (burstiness) and expansion frequency in arelated paper (Umelnura nd Church, submitted).Two estimation methods were introduced todemonstrate ile magnitude and shape of adapta-tion.
Both methods produce similar results.?
Pr(+ adapt I ) = Pr(test\] hist)?
Pr(+adapt2)=Pr(k>2\]k>_l )Neighborhoods were then introduced for wordssuch as "laid-off" that were not in the history butwere close ("laid-off" is related to "layoff,"which was in the history).
Neighborhoods weredefined in terms of query expansion.
The historyis treated as a query in an information retriewddocument-ranking system.
Words in the k top-ranking documents (but not in the history) arecalled neighbors.
Neighbors adapt more dmnother terms, but not as much as words that actual-ly appeared in the history.
Better neighbors(larger et) adapt more than worse neighbors(slnaller el).ReferencesChurch, K. and Gale, W. (1995) "PoissonMixtures," Journal of Natural Language Engi-neering, 1:2, pp.
163-190.Floriau, P,.
and Yarowsky, D. (1999) "l)ynamicNonlocal Language Modeling via HierarchicalTopic-Based Adaptation," ACL, pp.
167-174.Francis, W., and Kucera, H. (1982) FrequencyAnalysis of English Usage, Houghton MifflinColnpany, Boston, MA.Hearst, M. (1994) Context and Structure inAutomated Full-Text Information Access, PhDThesis, Berkeley, available via www.sims.ber-keley.edu/~hearst.Jelinek, F. (1997) Statistical Methods.for SpeechRecognition, MIT Press, Cambridge, MA, USA.Umemura, K. and Church, K. (submitted)"Empirical Term Weighting: A Framework forStudying Limits, Stop Lists, Burstiness andQuery Expansion.186
