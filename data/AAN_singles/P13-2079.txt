Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 446?450,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsModeling Human Inference Process forTextual Entailment RecognitionHen-Hsen Huang Kai-Chun Chang Hsin-Hsi ChenDepartment of Computer Science and Information EngineeringNational Taiwan UniversityNo.
1, Sec.
4, Roosevelt Road, Taipei, 10617 Taiwan{hhhuang, kcchang}@nlg.csie.ntu.edu.tw; hhchen@ntu.edu.twAbstractThis paper aims at understanding what hu-man think in textual entailment (TE) recogni-tion process and modeling their thinking pro-cess to deal with this problem.
We first ana-lyze a labeled RTE-5 test set and find that thenegative entailment phenomena are very ef-fective features for TE recognition.
Then, amethod is proposed to extract this kind ofphenomena from text-hypothesis pairs auto-matically.
We evaluate the performance ofusing the negative entailment phenomena onboth the English RTE-5 dataset and ChineseNTCIR-9 RITE dataset, and conclude thesame findings.1 IntroductionTextual Entailment (TE) is a directional relation-ship between pairs of text expressions, text (T)and hypothesis (H).
If human would agree thatthe meaning of H can be inferred from the mean-ing of T, we say that T entails H (Dagan et al,2006).
The researches on textual entailment haveattracted much attention in recent years due to itspotential applications (Androutsopoulos and Ma-lakasiotis, 2010).
Recognizing Textual Entail-ment (RTE) (Bentivogli, et al, 2011), a series ofevaluations on the developments of English TErecognition technologies, have been held seventimes up to 2011.
In the meanwhile, TE recogni-tion technologies in other languages are also un-derway (Shima, et al, 2011).Sammons, et al, (2010) propose an evaluationmetric to examine the characteristics of a TErecognition system.
They annotate text-hypothesis pairs selected from the RTE-5 test setwith a series of linguistic phenomena required inthe human inference process.
The RTE systemsare evaluated by the new indicators, such as howmany T-H pairs annotated with a particular phe-nomenon can be correctly recognized.
The indi-cators can tell developers which systems are bet-ter to deal with T-H pairs with the appearance ofwhich phenomenon.
That would give developersa direction to enhance their RTE systems.Such linguistic phenomena are thought as im-portant in the human inference process by anno-tators.
In this paper, we use this valuable re-source from a different aspect.
We aim at know-ing the ultimate performance of TE recognitionsystems which embody human knowledge in theinference process.
The experiments show fivenegative entailment phenomena are strong fea-tures for TE recognition, and this finding con-firms the previous study of Vanderwende et al(2006).
We propose a method to acquire the lin-guistic phenomena automatically and use them inTE recognition.This paper is organized as follows.
In Section2, we introduce linguistic phenomena used byannotators in the inference process and point outfive significant negative entailment phenomena.Section 3 proposes a method to extract themfrom T-H pairs automatically, and discuss theireffects on TE recognition.
In Section 4, we ex-tend the methodology to the BC (binary classsubtask) dataset distributed by NTCIR-9 RITEtask (Shima, et al, 2011) and discuss their ef-fects on TE recognition in Chinese.
Section 5concludes the remarks.2 Human Inference Process in TEWe regard the human annotated phenomena asfeatures in recognizing the binary entailment re-lation between the given T-H pairs, i.e., EN-TAILMENT and NO ENTAILMENT.
Total 210T-H pairs are chosen from the RTE-5 test set bySammons et al (2010), and total 39 linguisticphenomena divided into the 5 aspects, includingknowledge domains, hypothesis structures, infer-ence phenomena, negative entailment phenome-446na, and knowledge resources, are annotated onthe selected dataset.2.1 Five aspects as featuresWe train SVM classifiers to evaluate the perfor-mances of the five aspects of phenomena as fea-tures for TE recognition.
LIBSVM RBF kernel(Chang and Lin, 2011) is adopted to developclassifiers with the parameters tuned by gridsearch.
The experiments are done with 10-foldcross validation.For the dataset of Sammons et al (2010), twoannotators are involved in labeling the above 39linguistic phenomena on the T-H pairs.
Theymay agree or disagree in the annotation.
In theexperiments, we consider the effects of theiragreement.
Table 1 shows the results.
Five as-pects are first regarded as individual features,and are then merged together.
Schemes ?Annota-tor A?
and ?Annotator B?
mean the phenomenalabelled by annotator A and annotator B are usedas features respectively.
The ?A AND B?scheme, a strict criterion, denotes a phenomenonexists in a T-H pair only if both annotators agreewith its appearance.
In contrast, the ?A OR B?scheme, a looser criterion, denotes a phenome-non exists in a T-H pair if at least one annotatormarks its appearance.We can see that the aspect of negative entail-ment phenomena is the most significant featureamong the five aspects.
With only 9 phenomenain this aspect, the SVM classifier achieves accu-racy above 90% no matter which labelingschemes are adopted.
Comparatively, the bestaccuracy in RTE-5 task is 73.5% (Iftene andMoruz, 2009).
In negative entailment phenomenaaspect, the ?A OR B?
scheme achieves the bestaccuracy.
In the following experiments, we adoptthis labeling scheme.2.2 Negative entailment phenomenaThere is a large gap between using negative en-tailment phenomena and using the second effec-tive features (i.e., inference phenomena).
Moreo-ver, using the negative entailment phenomena asfeatures only is even better than using all the 39linguistic phenomena.
We further analyze whichnegative entailment phenomena are more signifi-cant.There are nine linguistic phenomena in the as-pect of negative entailment.
We take each phe-nomenon as a single feature to do the two-waytextual entailment recognition.
The ?A OR B?scheme is applied.
Table 2 shows the experi-mental results.Annotator A Annotator B A AND B A OR BKnowledgeDomains50.95% 52.38% 52.38% 50.95%HypothesisStructures50.95% 51.90% 50.95% 51.90%InferencePhenomena74.29% 72.38% 72.86% 74.76%NegativeEntailmentPhenomena97.14% 95.71% 92.38% 97.62%KnowledgeResources69.05% 69.52% 67.62% 69.52%ALL  97.14% 92.20% 90.48% 97.14%Table 1: Accuracy of recognizing binary TE rela-tion with the five aspects as features.Phenomenon ID Negative entailmentPhenomenonAccuracy0 Named Entity mismatch 60.95%1 Numeric Quantity mismatch 54.76%2 Disconnected argument 55.24%3 Disconnected relation 57.62%4 Exclusive argument 61.90%5 Exclusive relation 56.67%6 Missing modifier 56.19%7 Missing argument 69.52%8 Missing relation 68.57%Table 2: Accuracy of recognizing TE relationwith individual negative entailment phenomena.The 1st column is phenomenon ID, the 2nd col-umn is the phenomenon, and the 3rd column isthe accuracy of using the phenomenon in the bi-nary classification.
Comparing with the best ac-curacy 97.62% shown in Table 1, the highestaccuracy in Table 2 is 69.52%, when missingargument is adopted.
It shows that each phenom-enon is suitable for some T-H pairs, and mergingall negative entailment phenomena togetherachieves the best performance.We consider all possible combinations ofthese 9 negative entailment phenomena, i.e.,+?+=511 feature settings, and use eachfeature setting to do 2-way entailment relationrecognition by LIBSVM.
The notationde-notes a set of(   )feature settings, each withn features.The model using all nine phenomena achievesthe best accuracy of 97.62%.
Examining thecombination sets, we find phenomena IDs 3, 4, 5,7 and 8 appear quite often in the top 4 featuresettings of each combination set.
In fact, this set-ting achieves an accuracy of 95.24%, which isthe best performance incombination set.
Onthe one hand, adding more phenomena into (3, 4,5, 7, 8) setting does not have much performancedifference.In the above experiments, we do all the anal-yses on the corpus annotated with linguistic phe-nomena by human.
We aim at knowing the ulti-447mate performance of TE recognition systemsembodying human knowledge in the inference.The human knowledge in the inference cannot becaptured by TE recognition systems fully correct-ly.
In the later experiments, we explore the fivecritical features, (3, 4, 5, 7, 8), and examine howthe performance is affected if they are extractedautomatically.3 Negative Entailment Phenomena Ex-tractionThe experimental results in Section 2.2 show thatdisconnected relation, exclusive argument, ex-clusive relation, missing argument, and missingrelation are significant.
We follow the definitionsof Sammons et al (2010) and show them as fol-lows.
(a) Disconnected Relation.
The arguments andthe relations in Hypothesis (H) are all matchedby counterparts in Text (T).
None of the argu-ments in T is connected to the matching relation.
(b) Exclusive Argument.
There is a relationcommon to both the hypothesis and the text, butone argument is matched in a way that makes Hcontradict T.(c) Exclusive Relation.
There are two or morearguments in the hypothesis that are also relatedin the text, but by a relation that means H contra-dicts T.(d) Missing Argument.
Entailment fails be-cause an argument in the Hypothesis is not pre-sent in the Text, either explicitly or implicitly.
(e) Missing Relation.
Entailment fails becausea relation in the Hypothesis is not present in theText, either explicitly or implicitly.To model the annotator?s inference process,we must first determine the arguments and therelations existing in T and H, and then align thearguments and relations in H to the related onesin T. It is easy for human to find the importantparts in a text description in the inference process,but it is challenging for a machine to determinewhat words are important and what are not, andto detect the boundary of arguments and relations.Moreover, two arguments (relations) of strongsemantic relatedness are not always literallyidentical.In the following, a method is proposed to ex-tract the phenomena from T-H pairs automatical-ly.
Before extraction, the English T-H pairs arepre-processed by numerical character transfor-mation, POS tagging, and dependency parsingwith Stanford Parser (Marneffe, et al, 2006;Levy and Manning, 2003), and stemming withNLTK (Bird, 2006).3.1 A feature extraction methodGiven a T-H pair, we first extract 4 sets of nounphrases based on their POS tags, including {nounin H}, {named entity (nnp) in H}, {compoundnoun (cnn) in T}, and {compound noun (cnn) inH}.
Then, we extract 2 sets of relations, includ-ing {relation in H} and {relation in T}, whereeach relation in the sets is in a form of Predi-cate(Argument1, Argument2).
Some typical ex-amples of relations are verb(subject, object) forverb phrases, neg(A, B) for negations, num(Noun,number) for numeric modifier, and tmod(C, tem-poral argument) for temporal modifier.
A predi-cate has only 2 arguments in this representation.Thus, a di-transitive verb is in terms of two rela-tions.Instead of measuring the relatedness of T-Hpairs by comparing T and H on the predicate-argument structure (Wang and Zhang, 2009), ourmethod tries to find the five negative entailmentphenomena based on the similar representation.Each of the five negative entailment phenomenais extracted as follows according to their defini-tions.
To reduce the error propagation which maybe arisen from the parsing errors, we directlymatch those nouns and named entities appearingin H to the text T. Furthermore, we introduceWordNet to align arguments in H to T.(a) Disconnected Relation.
If (1) for each a ?
{noun in H}?
{nnp in H}?
{cnn in H}, we canfind a ?
T too, and (2) for each r1=h(a1,a2) ?
{relation in H}, we can find a relation r2=h(a3,a4)?
{relation in T} with the same header h, butwith different arguments, i.e., a3?a1 and a4?a2,then we say the T-H pair has the ?DisconnectedRelation?
phenomenon.
(b) Exclusive Argument.
If there exist a rela-tion r1=h(a1,a2)?
{relation in H}, and a relationr2=h(a3,a4)?
{relation in T} where both relationshave the same header h, but either the pair (a1,a3)or the pair (a2,a4) is an antonym by looking upWordNet, then we say the T-H pair has the ?Ex-clusive Argument?
phenomenon.
(c) Exclusive Relation.
If there exist a relationr1=h1(a1,a2)?
{relation in T}, and a relationr2=h2(a1,a2)?
{relation in H} where both relationshave the same arguments, but h1 and h2 have theopposite meanings by consulting WordNet, thenwe say that the T-H pair has the ?Exclusive Rela-tion?
phenomenon.448(d) Missing Argument.
For each argument a1?
{noun in H}?
{nnp in H}?
{cnn in H}, if theredoes not exist an argument a2?T such that a1=a2,then we say that the T-H pair has ?Missing Ar-gument?
phenomenon.
(e) Missing Relation.
For each relationr1=h1(a1,a2)?
{relation in H}, if there does notexist a relation r2=h2(a3,a4)?
{relation in T} suchthat h1=h2, then we say that the T-H pair has?Missing Relation?
phenomenon.3.2 Experiments and discussionThe following two datasets are used in EnglishTE recognition experiments.
(a) 210 pairs from part of RTE-5 test set.
The210 T-H pairs are annotated with the linguisticphenomena by human annotators.
They are se-lected from the 600 pairs in RTE-5 test set, in-cluding 51% ENTAILMENT and 49% NO EN-TAILMENT.
(b) 600 pairs of RTE-5 test set.
The originalRTE-5 test set, including 50% ENTAILMENTand 50% NO ENTAILMENT.Table 3 shows the performances of TE recog-nition.
The ?Machine-annotated?
and the ?Hu-man-annotated?
columns denote that the phe-nomena annotated by machine and human areused in the evaluation respectively.
Using ?Hu-man-annotated?
phenomena can be seen as theupper-bound of the experiments.
The perfor-mance of using machine-annotated features in210-pair and 600-pair datasets is 52.38% and59.17% respectively.Though the performance of using the phenom-ena extracted automatically by machine is notcomparable to that of using the human annotatedones, the accuracy achieved by using only 5 fea-tures (59.17%) is just a little lower than the aver-age accuracy of all runs in RTE-5 formal runs(60.36%) (Bentivogli, et al, 2009).
It shows thatthe significant phenomena are really effective indealing with entailment recognition.
If we canimprove the performance of the automatic phe-nomena extraction, it may make a great progresson the textual entailment.Phenomena 210 pairs 600 pairsMachine-annotatedHuman-annotatedMachine-annotatedDisconnected Relation 50.95% 57.62% 54.17%Exclusive Argument 50.95% 61.90% 55.67%Exclusive Relation 50.95% 56.67% 51.33%Missing Argument 53.81% 69.52% 56.17%Missing Relation 50.95% 68.57% 52.83%All 52.38% 95.24% 59.17%Table 3: Accuracy of textual entailment recogni-tion using the extracted phenomena as features.4 Negative Entailment Phenomena inChinese RITE DatasetTo make sure if negative entailment phenomenaexist in other languages, we apply the methodol-ogies in Sections 2 and 3 to the RITE dataset inNTCIR-9.
We annotate all the 9 negative entail-ment phenomena on Chinese T-H pairs accordingto the definitions by Sammons et al (2010) andanalyze the effects of various combinations ofthe phenomena on the new annotated Chinesedata.
The accuracy of using all the 9 phenomenaas features (i.e.,setting) is 91.11%.
It showsthe same tendency as the analyses on Englishdata.
The significant negative entailment phe-nomena on Chinese data, i.e., (3, 4, 5, 7, 8), arealso identical to those on English data.
The mod-el using only 5 phenomena achieves an accuracyof 90.78%, which is very close to the perfor-mance using all phenomena.We also classify the entailment relation usingthe phenomena extracted automatically by thesimilar method shown in Section 3.1, and get asimilar result.
The accuracy achieved by usingthe five automatically extracted phenomena asfeatures is 57.11%, and the average accuracy ofall runs in NTCIR-9 RITE task is 59.36% (Shima,et al, 2011).
Compared to the other methods us-ing a lot of features, only a small number of bi-nary features are used in our method.
Those ob-servations establish what we can call a usefulbaseline for TE recognition.5 ConclusionIn this paper we conclude that the negative en-tailment phenomena have a great effect in deal-ing with TE recognition.
Systems with humanannotated knowledge achieve very good perfor-mance.
Experimental results show that not onlycan it be applied to the English TE problem, butalso has the similar effect on the Chinese TErecognition.
Though the automatic extraction ofthe negative entailment phenomena still needs alot of efforts, it gives us a new direction to dealwith the TE problem.The fundamental issues such as determiningthe boundary of the arguments and the relations,finding the implicit arguments and relations, ver-ifying the antonyms of arguments and relations,and determining their alignments need to be fur-ther examined to extract correct negative entail-ment phenomena.
Besides, learning-based ap-proaches to extract phenomena and multi-classTE recognition will be explored in the future.449AcknowledgmentsThis research was partially supported by Excel-lent Research Projects of National Taiwan Uni-versity under contract 102R890858 and 2012Google Research Award.ReferencesIon Androutsopoulos and Prodromos Malakasiotis.2010.
A Survey of Paraphrasing and Textual En-tailment Methods.
Journal of Artificial IntelligenceResearch, 38:135-187.Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa TrangDang, and Danilo Giampiccolo.
2011.
The seventhPASCAL recognizing textual entailment challenge.In Proceedings of the 2011 Text AnalysisConference (TAC 2011), Gaithersburg, Maryland,USA..Luisa Bentivogli, Ido Dagan, Hoa Trang Dang,Danilo Giampiccolo, and Bernardo Magnini.
2009.The fifth PASCAL recognizing textual entailmentchallenge.
In Proceedings of the 2009 TextAnalysis Conference (TAC 2009), Gaithersburg,Maryland, USA.Steven Bird.
2006.
NLTK: the natural languagetoolkit.
In Proceedings of the 21st InternationalConference on Computational Linguistics and 44thAnnual Meeting of the Association for Computa-tional Linguistics (COLING-ACL 2006), pages 69-72.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIBSVM:a Library for Support Vector Machines.
ACMTransactions on Intelligent Systems and Technolo-gy, 2:27:1-27:27.
Software available athttp://www.csie.ntu.edu.tw/~cjlin/libsvm.Ido Dagan, Oren Glickman, and Bernardo Magnini.2006.
The PASCAL Recognising Textual Entail-ment Challenge.
Lecture Notes in Computer Sci-ence, 3944:177-190.Adrian Iftene and Mihai Alex Moruz.
2009.
UAICParticipation at RTE5.
In Proceedings of the 2009Text Analysis Conference (TAC 2009),Gaithersburg, Maryland, USA.Roger Levy and Christopher D. Manning.
2003.
Is itharder to parse Chinese, or the Chinese Treebank?In Proceedings of the 41st Annual Meeting on As-sociation for Computational Linguistics (ACL2003), pages 439-446.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InThe Fifth International Conference on LanguageResources and Evaluation (LREC 2006), pages449-454.Mark Sammons, V.G.Vinod Vydiswaran, and DanRoth.
2010.
Ask not what textual entailment can dofor you...
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguis-tics (ACL 2010), pages 1199-1208, Uppsala, Swe-den.Hideki Shima, Hiroshi Kanayama, Cheng-Wei Lee,Chuan-Jie Lin, Teruko Mitamura, Yusuke Miyao,Shuming Shi, and Koichi Takeda.
2011.
Overviewof NTCIR-9 RITE: Recognizing inference in text.In Proceedings of the NTCIR-9 Workshop Meeting,Tokyo, Japan.Lucy Vanderwende, Arul Menezes, and Rion Snow.2006.
Microsoft Research at RTE-2: SyntacticContributions in the Entailment Task: an imple-mentation.
In Proceedings of the Second PASCALChallenges Workshop.Rui Wang and Yi Zhang.
2009.
Recognizing TextualRelatedness with Predicate-Argument Structures.In Proceedings of the 2009 Conference on Empiri-cal Methods in Natural Language Processing, pag-es 784?792, Singapore.450
