Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
102?111, Prague, June 2007. c?2007 Association for Computational LinguisticsIncremental generation of plural descriptions: Similarity and partitioningAlbert Gatt and Kees van DeemterDepartment of Computing ScienceUniversity of Aberdeen{agatt,kvdeemte}@csd.abdn.ac.ukAbstractApproaches to plural reference generationemphasise descriptive brevity, but often lackempirical backing.
This paper describesa corpus-based study of plural descrip-tions, and proposes a psycholinguistically-motivated algorithm for plural referencegeneration.
The descriptive strategy is basedon partitioning and incorporates corpus-derived heuristics.
An exhaustive evaluationshows that the output closely matches hu-man data.1 IntroductionGeneration of Referring Expressions (GRE) is awell-studied sub-task of microplanning in NaturalLanguage Generation.
Most algorithms in this areaview GRE as a content determination problem, thatis, their emphasis is on the construction of a se-mantic representation which is eventually mappedto a linguistic realisation (i.e.
a noun phrase).
Con-tent Determination for GRE starts from a KnowledgeBase (KB) consisting of a set of entities U and a setof properties P represented as attribute-value pairs,and searches for a description D ?
P which distin-guishes a referent r ?
U from its distractors.
Underthis view, reference is mainly about identification ofan entitiy in a given context (represented by the KB),a well-studied pragmatic function of definite nounphrases in both the psycholinguistic and the compu-tational literature (Olson, 1970).For example, the KB in Table 1 represents 8 en-tities in a 2D visual domain, each with 6 attributes,including their location, represented as a combina-tion of horizontal (X) and vertical (Y) numerical co-TYPE COLOUR ORIENTATION SIZE X Ye1 desk red back small 3 1e2 sofa blue back small 5 2e3 desk red back large 1 1e4 desk red front large 2 3e5 desk blue right large 2 4e6 sofa red back large 4 1e7 sofa red front large 3 3e8 sofa blue back large 3 2Table 1: A visual domainordinates.
To refer to an entity an algorithm searchesthrough values of the different attributes.GRE has been dominated by Dale and Reiter?s(1995) Incremental Algorithm (IA), one versionof which, generalised to deal with non-disjunctiveplural references, is shown in Algorithm 1 (vanDeemter, 2002).
A non-disjunctive reference to aset R is possible just in case all the elements of Rcan be distinguished using the same attribute-valuepairs.
Such a description is equivalent to the logicalconjunction of the properties in question.
This al-gorithm, IAplur, initialises a description D and a setof distractors C [1.1?1.2], and traverses an orderedlist of properties, called the preference order (PO)[1.3], which reflects general or domain-specific pref-Algorithm 1 IAplur(R,U,PO1: D ?
?2: C ?
U ?R3: for ?A : v?
?
PO do4: if R ?
[[ ?A : v? ]]
?
[[ ?A : v?
]]?
C 6= ?
then5: D ?
D ?
{?A : v?
}6: C ?
C ?
[[ ?A : v?
]]7: if [[ D ]] = R then8: return D9: end if10: end if11: end for12: return D102erences for attributes.
For instance, with the PO inthe top row of the Table, the algorithm first consid-ers values of TYPE, then COLOUR, and so on, addinga property to D if it is true of the intended referentsR, and has some contrastive value, that is, excludessome distractors [1.4].
The description and the dis-tractor set C are updated accordingly [1.5?1.6], andthe description returned if it is distinguishing [1.7].Given R = {e1, e2}, this algorithm would return thefollowing description:(1) ?ORIENTATION : back?
?
?SIZE : small?This description is overspecified, because ORI-ENTATION is not strictly necessary to distinguishthe referents (?SIZE : small?
suffices).
Moreover,the description does not include TYPE, though ithas been argued that this is always required, as itmaps to the head noun of an NP (Dale and Re-iter, 1995).
We will adopt this assumption here, forreasons explained below.
Due to its hillclimbingnature, the IA avoids combinatorial search, unlikesome predecessors which searched exhaustively forthe briefest possible description of a referent (Dale,1989), based on a strict interpretation of the GriceanMaxim of Quantity (Grice, 1975).
Given that, un-der the view proposed by Olson (1970) among oth-ers, the function of a referential NP is to identify, astrict Gricean interpretation holds that it should con-tain no more information than necessary to achievethis goal.The Incremental Algorithm constitutes a depar-ture from this view given that it can overspecifythrough its use of a PO.
This has been justifiedon psycholinguistic grounds.
Speakers overspecifytheir descriptions because they begin their formula-tion of a reference without exhaustively scanning adomain (Pechmann, 1989; Belke and Meyer, 2002).They prioritise the basic-level category (TYPE) of anobject, and salient, absolute properties like COLOUR(Pechmann, 1989; Eikmeyer and Ahlse`n, 1996), aswell as locative properties in the vertical dimen-sion (Arts, 2004).
Relative attributes like SIZEare avoided unless absolutely required for identi-fication (Belke and Meyer, 2002).
This evidencesuggests that speakers conceptualise referents asgestalts (Pechmann, 1989) whose core is their basic-level TYPE (Murphy, 2002) and some other salientattributes like COLOUR.
For instance, according toSchriefers and Pechmann (1988), an NP such as thelarge black triangle reflects a conceptualisation ofthe referent as a black triangle, of which the SIZEproperty is predicated.
Thus, the TYPE+COLOURcombination is not mentally represented as two sep-arable dimensions.In what follows, we will sometimes refer to this prin-ciple as theConceptual Gestalts Principle.
Note thatthe IA does not fully mirror these human tendencies,since it only includes preferred attributes in a de-scription if they remove some distractors given thecurrent state of the algorithm, whereas psycholin-guistic research suggests that people include themirrespective of contrastiveness (but cf.
van der Sluisand Krahmer (2005)).More recent research on plural GRE has de-emphasised these issues, especially in case of dis-junctive plural reference.
Disjunction is requiredwhenever elements of a set of referents R do nothave identical distinguishing properties.
For exam-ple, {e1, e3} can be distinguished by the followingConjunctive Normal Form (CNF) description1:(2) ?TYPE : desk?
?`?COLOUR : red??
?COLOUR : blue??
?`?ORIENTATION : right?
?
?ORIENTATION : back?
?Such a description would be returned by a gen-eralised version of Algorithm 1 proposed by vanDeemter (2002).
This generalisation, IAbool (socalled because it handles all Boolean operators, suchas negation and disjunction), first tries to find a non-disjunctive description using Algorithm 1.
Failingthis, it searches through disjunctions of propertiesof increasing length, conjoining them to the descrip-tion.
This procedure has three consequences:1.
Efficiency: Searching through disjunctivecombinations results in a combinatorial explo-sion (van Deemter, 2002).2.
Gestalts and content: The notion of a ?pre-ferred attribute?
is obscured, since it is dif-ficult to apply the same reasoning that moti-vated the PO in the IA to combinations like(COLOUR ?
SIZE).1Note that logical disjunction is usually rendered as linguis-tic coordination using and.
Thus, the table and the desk is theunion of things which are desks or tables.1033.
Form: Descriptions can become logically verycomplex (Gardent, 2002; Horacek, 2004).Proposals to deal with (3) include Gardent?s(2002) non-incremental, constraint-based algorithmto generate the briefest available description of aset, an approach extended in Gardent et al (2004).An alternative, by Horacek (2004), combines best-first search with optimisation to reduce logical com-plexity.
Neither approach benefits from empiri-cal grounding, and both leave open the question ofwhether previous psycholinguistic research on sin-gular reference is applicable to plurals.This paper reports a corpus-based analysis of plu-ral descriptions elicited in well-defined domains, ofwhich Table 1 is an example.
This study falls withina recent trend in which empirical issues in GRE havebegun to be tackled (Gupta and Stent, 2005; JordanandWalker, 2005; Viethen and Dale, 2006).
We thenpropose an efficient algorithm for the generation ofreferences to arbitrary sets, which combines corpus-derived heuristics and a partitioning-based proce-dure, comparing this to IAbool.
Unlike van Deemter(2002), we only focus on disjunction, leaving nega-tion aside.
Our starting point is the assumption thatplurals, like singulars, evince preferences for certainattributes as predicted by the Conceptual GestaltsPrinciple.
Based on previous work in Gestalt per-ception (Wertheimer, 1938; Rock, 1983), we pro-pose an extension of this to sets, whereby plural de-scriptions are preferred if (a) they maximise the sim-ilarity of their referents, using the same attributes todescribe them as far as possible; (b) prioritise salient(?preferred?)
attributes which are central to the con-ceptual representation of an object.
We address (3)above by investigating the logical form of plurals inthe corpus.
One determinant of logical form is thebasic-level category of objects.
For example, to re-fer to {e1, e2} in the Table, an author has at least thefollowing options:(3) (a) the small desk and sofa(b) the small red desk and the small blue sofa(c) the small desk and the small blue sofa(d) the small objectsThese descriptions exemplify three possible sourcesof variation:Disjunctive/Non-disjunctive: The last description,(3d), is non-disjunctive (i.e.
it is logically a conjunc-tion of properties).
This, however, is only achiev-able through the use of a non-basic level value forthe TYPE of the entities (objects).
Using the basic-level would require the disjunction (?TYPE : desk??
?TYPE : sofa?
), which is the case in (3a?c).
Giventhat basic-level categories are preferred on indepen-dent grounds (Rosch et al, 1976), we would expectexamples like (3d) to be relatively infrequent.Aggregation: If a description is disjunctive, it maybe aggregated, with properties common to all ob-jects realised as wide-scope modifiers.
For instance,in (3a), small modifies desk and sofa.
By contrast,(3b) is non-aggregated: small occurs twice (modi-fying each coordinate in the NP).
Non-aggregated,disjunctive descriptions are logically equivalent to apartition of a set.
For instance, (3c) partitions theset R = {e1, e2} into {{e1}, {e2}}, describing eachelement separately.
Descriptions like (3b) are moreoverspecified than their aggregated counterparts dueto the repetition of information.Paralellism/Similarity: Non-aggregated, disjunc-tive descriptions (partitions) may exhibit semanticparallelism: In (3b), elements of the partition aredescribed using exactly the same attributes (that is,TYPE, COLOUR, and SIZE).
This is not the case in(3c), which does represent a partition but is non-parallel.
Parallel structures maximise the similarityof elements of a partition, using the same attributesto describe both.
The likelihood of propagation of anattribute across disjuncts is probably dependent onits degree of salience or preference (e.g.
COLOUR isexpected to be more likely to be found in a parallelstructure than SIZE).2 The dataThe data for our study is a subset of the TUNA Cor-pus (Gatt et al, 2007), consisting of 900 referencesto furniture and household items, collected via acontrolled experiment involving 45 participants.
Inaddition to their TYPE, objects in the domains haveCOLOUR, ORIENTATION and SIZE (see Table 1).
Foreach subset of these three attributes, there was anequal number of domains in which the minimallydistinguishing description (MD) consisted of valuesof that subset.
For example, Table 1 represents a do-main in which the intended referents, {e1, e2}, can104<DESCRIPTION num=?pl?><DESCRIPTION num=?sg?><ATTRIBUTE name=?size?
value=?small?>small</ATTRIBUTE><ATTRIBUTE name=?colour?
value=?red?>red</ATTRIBUTE><ATTRIBUTE name=?type?
value=?desk?>desk</ATTRIBUTE></DESCRIPTION>and<DESCRIPTION num=?sg?><ATTRIBUTE name=?size?
value=?small?>small</ATTRIBUTE><ATTRIBUTE name=?colour?
value=?blue?>blue</ATTRIBUTE><ATTRIBUTE name=?type?
value=?sofa?>sofa</ATTRIBUTE></DESCRIPTION></DESCRIPTION>(?SIZE : small?
?
?COLOUR : red?
?
?TYPE : desk?)?
(?SIZE : small?
?
?COLOUR : blue?
?
?TYPE : sofa?
)Figure 1: Corpus annotation examplesbe minimally distinguished using only SIZE2.
Thus,overspecified usage of attributes can be identifiedin authors?
descriptions.
Domain objects were ran-domly placed in a 3 (row) ?
5 (column) grid, rep-resented by X and Y in Table 1.
These are relevantfor a subset of descriptions which contain locativeexpressions.Corpus descriptions are paired with an explicitXML domain representation, and annotated with se-mantic markup which makes clear which attributesa description contains.
This markup abstracts awayfrom differences in lexicalisation, making it an idealresource to evaluate content determination algo-rithms, because it is semantically transparent, inthe sense of this term used by van Deemter et al(2006).
This markup scheme also enables the com-positional derivation of a logical form from a naturallanguage description.
For example, the XML repre-sentation of (3b) is shown in Figure 1, which alsodisplays the LF derived from it.
Each constituent NPin (3b) is annotated as a set of attributes enclosed bya DESCRIPTION tag, which is marked up as singular(sg).
The two coordinates are further enclosed ina plural DESCRIPTION; correspondingly, the LF is adisjunction of (the LFs of) the two internal descrip-tions.Descriptions in the corpus were elicited in 7 do-mains with one referent, and 13 domains with 2referents.
Plural domains represented levels of aValue Similarity factor.
In 7 Value-Similar (VS)domains, referents were identifiable using identicalvalues of the minimally distinguishing attributes.
Inthe remaining 6 Value-Dissimilar (VDS) domains,the minimally distinguishing values were different.Table 1 represents a VS domain, where {e1, e2} can2TYPE was not included in the calculation of MD.VS VDS+Disj ?Disj +Disj ?Disj+aggr 20.2 15.5 2.4 3.7?aggr 64.3 ?
93.9 ?% overall 84.5 15.5 96.3 3.7Table 2: % disjunctive and non-disjunctive pluralsbe minimally distinguished using the same value ofSIZE (small).In terms of our introductory discussion, referentsin Value-Similar conditions could be minimally dis-tinguished using a conjunction of properties, whileValue-Dissimilar referents required a disjunctionsince, if two referents could be minimally distin-guished by different values v and v?
of an attributeA, then MD had the form ?A : v?
?
?A : v??.
How-ever, even in the VS condition, referents had differ-ent basic-level types.
Thus, an author faced with adomain like Table 1 had at least the descriptive op-tions in (3a?d).
If they chose to refer to entities usingbasic-level values of TYPE, their description wouldbe disjunctive (e.g.
3a).
A non-disjunctive descrip-tion would require the use of a superordinate value,as in (3d).Our analysis will focus on a stratified randomsample of 180 plural descriptions, referred to as PL1,generated by taking 4 descriptions from each author(2 each from VS and VDS conditions).
We also usethe singular data (SG; N = 315).
The remainingplural descriptions (PL2; N = 405) are used forevaluation.3 The logical form of pluralsDescriptions in PL1 were first classified according towhether they were non-disjunctive (cf.
3d) or dis-junctive (3a?c).
The latter were further classifiedinto aggregated (3a) and non-aggregated (3b).
Ta-ble 2 displays the percentage of descriptions in eachof the four categories, within each level of ValueSimilarity.
Disjunctive descriptions were a major-ity in either condition, and most of these were non-aggregated.
As noted in ?1, these descriptions cor-respond to partitions of the set of referents.Since referents in VS had identical properties ex-cept for TYPE values, the most likely reason for themajority of disjunctives in VS is that people?s de-scriptions represented a partition of a set of refer-ents induced by the basic-level category of the ob-105Non-Parallel Parallel ?2 (p ?
.001)overspec.
24.6 75.4 92.467underspec.
5.3 94.7 42.217well-spec.
11 89 26Table 3: Parallelism: % per description typejects.
This is strengthened by the finding that thelikelihood of a description being disjunctive or non-disjunctive did not differ as a function of Value Sim-ilarity (?2 = 2.56, p > .1).
A ?2 test on overall fre-quencies of aggregated versus non-aggregated dis-junctives showed that the non-aggregated descrip-tions (?true?
partitions) were a significant major-ity (?2 = 83.63, p < .001).
However, thegreater frequency of aggregation in VS comparedto VDS turned out to be significant (?2 = 15.498,p < .001).
Note that the predominance of non-aggregated descriptions in VS implies that proper-ties are repeated in two disjuncts (resp.
coordinateNPs), suggesting that authors are likely to redun-dantly propagate properties across disjuncts.
Thisevidence goes against some recent proposals for plu-ral reference generation which emphasise brevity(Gardent, 2002).3.1 Conceptual gestalts and similarityAllowing for the independent motivation for set par-titioning based on TYPE values, we suggested in ?1that parallel descriptions such as (3b) may be morelikely than non-parallel ones (3c), since the latterdoes not use the same properties to describe the tworeferents.
Similarity, however, should also interactwith attribute preferences.For this part of the analysis, we focus exclusivelyon the disjunctive descriptions in PL1 (N = 150) inboth VS and VDS.
The descriptions were categorisedaccording to whether they had parallel or non-parallel semantic structure.
Evidence for Similarityinteracting with attribute preferences is strongest ifit is found in those cases where an attribute is over-specified (i.e.
used when not required for a distin-guishing description).
In those cases where corpusdescriptions do not contain locative expressions (theX and/or Y attributes), such an overspecified usageis straightforwardly identified based on the MD ofa domain.
This is less straightforward in the case oflocatives, since the position of objects was randomlydetermined in each domain.
Therefore, we dividedActual Predictedp(A, SG) p(A, PPS) p(A, PPS)COLOUR .680 .835 .61SIZE .290 .359 .28ORIENTATION .280 .269 .26X-DIMENSION .440 .517 .52Y-DIMENSION .630 .647 .65Table 4: Actual and predicted usage probabilitiesdescriptions into three classes, whereby a descrip-tion is considered to be:1. underspecified if it does not include a locativeexpression and omits some MD attributes;2. overspecified if either (a) it does not omit anyMD attributes, but includes locatives and/ornon-required visual attributes; or (b) it omitssome MD attributes, but includes both a locativeexpression and other, non-required attributes;3. well-specified otherwise.Proportions of Parallel and Non-Parallel descrip-tions for each of the three classes are are shownin Table 3.
In all three description types, there isan overwhelming majority of Parallel descriptions,confirmed by a ?2 analysis.
The difference in pro-portions of description types did not differ betweenVS and VDS (?2 < 1, p > .8), suggesting that thetendency to redundantly repeat attributes, avoidingaggregation, is independent of whether elements ofa set can be minimally distinguished using identicalvalues.Our second prediction was that the likelihoodwith which an attribute is used in a parallel structureis a function of its overall ?preference?.
Thus, weexpect attributes such as COLOUR to feature morethan once (perhaps redundantly) in a parallel de-scription to a greater extent than SIZE.
To test this,we used the SG sample, estimating the overall prob-ability of occurrence of a given attribute in a singu-lar description (denoted p(A, SG)), and using this ina non-linear regression model to predict the likeli-hood of usage of an attribute in a plural partitioneddescription with parallel semantic structure (denotedp(A, PPS)).
The data was fitted to a regression equa-tion of the form p(A, PPS) = k?
p(A, SG)S .
The re-sulting equation, shown in (4), had a near-perfect fit106to the data (R2 = .910)3.
This is confirmed by com-paring actual probability of occurrence in the secondcolumn of Table 4, to the predicted probabilities inthe third column, which are estimated from singularprobabilities using (4).p(A, PPS) = .713 p(A, SG).912 (4)Note that the probabilities in the Table con-firm previous psycholinguistic findings.
To the ex-tent that probability of occurrence reflects salienceand/or conceptual importance, an order over thethree attributes COLOUR, SIZE and ORIENTATIONcan be deduced (C>>O>>S), which is compatiblewith the findings of Pechmann (1989), Belke andMeyer (2002) and others.
The locative attributesare also ordered (Y>>X), confirming the findingsof Arts (2004) that vertical location is preferred.
Or-derings deducible from the SG data in turn are ex-cellent predictors of the likelihood of ?propagating?an attribute across disjuncts in a plural description,something which is likely even if an attribute is re-dundant, modulo the centrality or salience of the at-tribute in the mental gestalt corresponding to the set.Together with the earlier findings on logical form,the data evinces a dual strategy whereby (a) setsare partitioned based on basic-level conceptual cat-egory; (b) elements of the partitions are describedusing the same attributes if they are easily perceivedand conceptualised.
Thus, of the descriptions in (3)above, it is (3b) that is the norm among authors.4 Content determination by partitioningIn this section we describe IApart, a partitioning-based content determination algorithm.
Though pre-sented as a version of the IA, the basic strategy isgeneralisable beyond it.
For our purposes, the as-sumption of a preference order will be maintained.IApart is distinguished from the original IA andIAbool (cf.
?1) in two respects.
First, it induces par-titions opportunistically based on KB information,and this is is reflected in the way descriptions arerepresented.
Second,, the criteria whereby a prop-erty is added to a description include a considerationof the overall salience or preference of an attribute,and its contribution to the conceptual cohesiveness3A similar analysis using linear regression gave essentiallythe same results.of the description.
Throughout the following discus-sion, we maintain a running example from Table 1,in which R = {e1, e2, e5}.4.1 Partitioned descriptionsIApart generates a partitioned description (Dpart) ofa set R, corresponding to a formula in DisjunctiveNormal Form.
Dpart is a set of Description Frag-ments (DFs).
A DF is a triple ?RDF, TDF,MDF?, whereRDF ?
R, TDF is a value of TYPE, and MDF is a pos-sibly empty set of other properties.
DFs refer to dis-joint subsets of R. As the representation suggests,TYPE is given a special status.
IApart starts by se-lecting the basic-level values of TYPE, partitioningR and creating a DF for each element of the partitionon this basis.
In our example, the selection of TYPEresults in two DFs, with MDF initialised to empty:(5) DF1?
{e1, e5}, ?TYPE : desk?, ??DF2?
{e2}, ?TYPE : sofa?, ?
?Although neither DF is distinguishing, RDF indicateswhich referents a fragment is intended to identify.In this way, the algorithm incorporates a ?divide-and-conquer?
strategy, splitting up the referential in-tention into ?sub-intentions?
to refer to elements ofa partition.
Following the initial step of selectingTYPE, the algorithm considers other properties inPO.
Suppose ?COLOUR : blue?
is considered first.This property is true of e2 and e5.
Since DF2 refers toe2, the new property can be added to MDF2 .
Since e5is not the sole referent of DF1, the property inducesa further partitioning of this fragment, resulting in anew DF.
This is identical to DF1 except that it refersonly to e5 and contains ?COLOUR : blue?.
DF1 it-self now refers only to e1.
Once ?COLOUR : red?
isconsidered, it is added to the latter, yielding (6).
(6) DF1?
{e1}, ?TYPE : desk?, {?COLOUR : red?}?DF2?
{e2}, ?TYPE : sofa?, {?COLOUR : blue?}?DF3?
{e5}, ?TYPE : desk?, {?COLOUR : blue?
}?The procedure updateDescription, which cre-ates and updates DFs, is formalised in Algorithm 2.When some property ?A : v?
is found to be ?use-ful?
in relation to R (in a sense to be made precise),this function is called with two arguments: ?A : v?itself, and R?
= [[ ?A : v? ]]
?
R, the referents ofwhich ?A : v?
is true.
The procedure iterates through107Algorithm 2 updateDescription(?A : v?, R?
)1: for ?RDF, TDF,MDF?
?
Dpart do2: if R?
= ?
then3: return4: else if RDF ?
R?
then5: MDF ?MDF ??
?A : v?
?6: R?
?
R?
?RDF7: else if RDF ?R?
6= ?
then8: Rnew ?
RDF ?R?9: DFnew ?
?Rnew, TDF,MDF ?
{?A : v?
}?10: Dpart ?
Dpart ?
?DFnew?11: RDF ?
RDF ?Rnew12: R?
?
R?
?Rnew13: end if14: end for15: if A = TYPE then16: Dpart ?
Dpart ??
?R?, ?A : v?, ??
?17: else18: Dpart ?
Dpart ??
?R?,?, {?A : v?}?
?19: end ifthe DFs in Dpart, adding the property to any DF suchthatRDF?R?
6= ?, untilR?
is empty and all referentsin it have been accounted for [2.2].
As indicated inthe informal discussion, there are two cases to con-sider for each DF:1.
RDF ?
R?
[2.4].
This corresponds to our exam-ple involving ?COLOUR : blue?
and DF2.
Theproperty is simply added to MDF [2.5] and R?is updated by removing the elements thus ac-counted for [2.6].2.
Suppose RDF 6?
R?.
If RDF ?
R?
is empty, then?A : v?
is not useful.
Suppose on the other handthat RDF ?
R?
6= ?
[2.7].
This occurred with?COLOUR : red?
in relation to DF1.
The proce-dure initialises Rnew, a set holding those refer-ents in RDF which are also in R?
[2.8].
A newDF (DFnew) is created, which is a copy of theold DF, except that (a) it contains the new prop-erty; and (b) its intended referents are Rnew[2.9].
The new DF is included in the description[2.10], while the old DF is altered by removingRnew from RDF [2.11].
This ensures that DFsdenote disjoint subsets of R.Two special cases arise when Dpart is empty, orthere are some elements of R?
for which no DF ex-ists.
Both cases result in the construction of a newDF.
An example of the former case is the initial stateof the algorithm, when TYPE is added.
As in exam-ple (5), the TYPE results in a new DF [2.16].
If aproperty is not a TYPE, the new DF has T set to null(?)
and the property is included in M [2.18]4.
Notethat this procedure easily generalises to the singularcase, where Dpart would only contain one DF.4.2 Property selection criteriaIApart?s content determination strategy maximisesthe similarity of a set by generating semanticallyparallel structures.
Though contrastiveness plays arole in property selection, the ?preference?
or con-ceptual salience of an attribute is also considered inthe decision to propagate it across DFs.Candidate properties for addition need only betrue of at least one element of R. Because of thepartitioning strategy, properties are not equally con-strastive for all referents.
For instance, in (5), e2needs to be distinguished from the other sofas in Ta-ble 1, while {e1, e5} need to be distinguished fromthe desks.
Therefore, distractors are held in an as-sociative array C, such that for all r ?
R, C[r] isthe set of distractors for that referent at a given stagein the procedure.
Contrastiveness is defined via thefollowing Boolean function:contrastive(?A : v?, R) ?
?r ?
R : C[r] ?
[[ ?A : v? ]]
6= ?
(7)We turn next to salience and similarity.
LetA(Dpart) be the set of attributes included in Dpart.A property is salient with respect to Dpart if it satis-fies the following:salient(?A : v?, Dpart) ?A ?
A(Dpart) ?
(.713 p(A, SG).912 > 0.5) (8)that is, the attribute is already included in the de-scription, and the predicted probability of its be-ing propagated in more than one fragment of a de-scription is greater than chance.
A potential prob-lem arises here.
Consider the description in (5)once more.
At this stage, IApart begins to considerCOLOUR.
The value red is true of e1, but non-contrastive (all the desks which are not inR are red).If this is the first value of COLOUR considered, (8)returns false because the attribute has not beenused in any part of the description.
On later con-sidering ?COLOUR : blue?, the algorithm adds it to4This only occurs if the KB is incomplete, that is, there someentities have no TYPE, so that R is not fully covered by theintended referents of the DFs when TYPE is initially added.108Dpart, since it is contrastive for {e2, e5}, but willhave failed to propagate COLOUR across fragments.As a result, IApart considers values of an attribute inorder of discriminatory power (Dale, 1989), definedin the present context as follows:|[[ ?A : v? ]]
?R| + |[[ ?A : v? ]]
?
(U ?R)||[[ ?A : v?
]]|(9)Discriminatory power depends on the number of ref-erents a property includes in its extension, and thenumber of distractors (U?R) it removes.
By priori-tising discriminatory values, the algorithm first con-siders and adds ?COLOUR : blue?, and subsequentlywill include red because (8) returns true.To continue with the example, at the stage repre-sented by (6), only e5 has been distinguished.
ORI-ENTATION, the next attribute considered, is not con-trastive for any referent.
On considering SIZE, smallis found to be contrastive for e1 and e2, and added toDF1 and DF2.
However, SIZE is not added to DF3, inspite of being present in two other fragments.
Thisis because the probability function p(SIZE, PPS) re-turns a value below 0.5 (see Table 4, reflecting therelatively low conceptual salience of this attribute.The final description is the blue desk, the small reddesk and the small blue sofa.
This example illus-trates the limits set on semantic parallelism and sim-ilarity: only attributes which are salient enough areredundantly propagated across DFs.4.3 ComplexityAn estimate of the complexity of IApart must ac-count for the way properties are selected (?4.2) andthe way descriptions are updated (Algorithm 2).Property selection involves checking propertiesfor contrastive value and salience, and updating theordering of values of each attribute based on dis-criminatory power (9).
Clearly, the number of timesthis is carried out is bounded by the number of prop-erties in the KB, which we denote np.
Every time aproperty is selected, the discriminatory power of val-ues changes (since the number of remaining distrac-tors changes).
Now, in the worst case, all np proper-ties are selected by the algorithm 5.
Each time, thealgorithm must compare the remaining properties5Only unique properties need to be considered, as each prop-erty is selected at most once, though it can be included in morethan one DF.Mean Mode PRPIAbool+ LOC 7.716 7 .7?
LOC 8.335 7 3.5IApart+ LOC 4.345 4 6.8?
LOC 1.93 0 44.7Table 5: Edit distance scorespairwise for discriminatory power, a quadratic op-eration with complexity O(n2p).
With respect to theprocedure updateDescription, we need to considerthe number of iterations in the for loop starting atline [2.1].
This is bounded by nr = |R| (there can beno more DFs than there are referents).
Once again,if at most np properties are selected, then the algo-rithm makes at most nr iterations np times, yield-ing complexity O(npnr).
Overall, then, IApart has aworst-case runtime complexity O(n3pnr).5 EvaluationIApart was compared to van Deemter?s IAbool (?1)against human output in the evaluation sub-corpusPL2 (N = 405).
This was considered an ade-quate comparison, since IAbool shares with the cur-rent framework a genetic relationship with the IA.Other approaches, such as Gardent?s (2002) brevity-oriented algorithm, would perform poorly on ourdata.
As shown in ?3, overspecification is extremelycommon in plural descriptions, suggesting that sucha strategy is on the wrong track (but see ?6).IApart and IAbool were each run over the domainrepresentation paired with each corpus description.The output logical form was compared to the LFcompiled from the XML representation of an au-thor?s description (cf.
Figure 1).
LFs were repre-sented as and-or trees, and compared using the treeedit distance algorithm of Shasha and Zhang (1990).On this measure, a value of 0 indicates identity.Because only a subset of descriptions con-tain locative expressions, PL2 was divided intoa +LOC dataset (N = 148) and a ?LOCdataset (N = 257).
The preference orders forboth algorithms were (C>>O>>S) for ?LOC and(Y>>C>>X>>S>>O) for +LOC.
These are sug-gested by the attribute probabilities in Table 4.
Ta-ble 5 displays the mean Edit score obtained byeach algorithm on the two datasets, the modal (mostfrequent) value, and the perfect recall percentage(PRP), the proportion of Edit scores of 0, indicating109perfect agreement with an author.As the means and modes indicate, IApart outper-formed IAbool on both datasets, with a consistentlyhigher PRP (this coincides with the modal score inthe case of ?LOC).
Pairwise t?tests showed thatthe trends were significant in both +LOC (t(147) =9.28, p < .001) and ?LOC (t(256) = 10.039,p < .001).IAbool has a higher (worse) mean on ?LOC, but abetter PRP than on+LOC.
This apparent discrepancyis partly due to variance in the edit distance scores.For instance, because the Y attribute was highest inthe preference order for +LOC, there were occasionswhen both referents could be identified using thesame value of Y, which was therefore included byIAbool at first pass, before considering disjunctions.Since Y was highly preferred by authors (see Table4), there was higher agreement on these cases, com-pared to those where the values of Y were differentfor the two referents.
In the latter case, Y was onlywhen disjunctions were considered, if at all.
Theworse performance of IApart on +LOC is due to alarger choice of attributes, also resulting in greatervariance, and occasionally incurring higher Edit costwhen the algorithm overspecified more than a hu-man author.
This is a potential shortcoming of thepartitioning strategy outlined here, when it is appliedto more complex domains.Some example outputs are given below, in a do-main where COLOUR sufficed to distinguish the ref-erents, which had different values of this attribute(i.e.
an instance of the VDS condition).
The formulareturned by IApart (10a) is identical to the (LF of)the human-authored description (with Edit score of0).
The output of IAbool is shown in (10b).
(10) (a)`fan ?
green?
?`sofa ?
blue?
?the green fan and the big sofa?
(b)`sofa ?
fan??
small ?
front ?`blue ?
green?
?the small, blue and green sofa and fan?As a result of IAbool?s requiring a property or dis-junction to be true of the the entire set of refer-ents, COLOUR is not included until disjunctions areconsidered, while values of SIZE and ORIENTATIONare included at first pass.
By contrast, IApart in-cludes COLOUR before any other attribute apart fromTYPE.
Though overspecification is common in ourdata, IAbool overspecifies with the ?wrong?
attributes(those which are relatively dispreferred).
The ratio-nale in IApart is to overspecify only if a propertywill enhance referent similarity, and is sufficientlysalient.
As for logical form, the Conjunctive Nor-mal Form output of IAbool increases the Edit score,given the larger number of logical operators in (10b)compared to (10a).6 Summary and conclusionsThis paper presented a study of plural reference,showing that people (a) partition sets based on thebasic level TYPE or category of their elements and(b) redundantly propagate attributes across disjunctsin a description, modulo their salience.
Our algo-rithm partitions a set opportunistically, and incor-porates a corpus-derived heuristic to estimate thesalience of a property.
Evaluation results showedthat these principles are on the right track, with sig-nificantly better performance over a previous model(van Deemter, 2002).
The partitioning strategy isrelated to a proposal by van Deemter and Krah-mer (2007), which performs exhaustive search fora partition of a set whose elements can be describednon-disjunctively.
Unlike the present approach, thisalgorithm is non-incremental and computationallycostly.IApart initially performs partitioning based on thebasic-level TYPE of objects, in line with the evi-dence.
However, later partitions can be induced byother properties, possible yielding partitions evenwith same-TYPE referents (e.g.
the blue chair andthe red chair).
Aggregation (the blue and red chairs)may be desirable in such cases, but limits on syntac-tic complexity of NPs are bound to play a role (Ho-racek, 2004).
Another possible limitation of IApartis that, despite strong evidence for overspecifica-tion, complex domains could yield very lengthy out-puts.
Strategies to avoid them include the utilisationof other boolean operators like negation (the deskswhich are not red) (Horacek, 2004).
These issuesare open to future empirical research.7 AcknowledgementsThanks to Ehud Reiter and Ielka van der Sluis foruseful comments.
This work forms part of the TUNAproject (www.csd.abdn.ac.uk/research/tuna/),supported by EPSRC grant GR/S13330/01.110ReferencesA.
Arts.
2004.
Overspecification in Instructive Texts.Ph.D.
thesis, Univiersity of Tilburg.E.
Belke and A. Meyer.
2002.
Tracking the time courseof multidimensional stimulus discrimination: Analy-sis of viewing patterns and processing times duringsame-different decisions.
European Journal of Cog-nitive Psychology, 14(2):237?266.R.
Dale and E. Reiter.
1995.
Computational interpreta-tion of the Gricean maxims in the generation of refer-ring expressions.
Cognitive Science, 19(8):233?263.Robert Dale.
1989.
Cooking up referring expressions.
InProceedings of the 27th Annual Meeting of the Associ-ation for Computational Linguistics, ACL-89.H.
J. Eikmeyer and E. Ahlse`n.
1996.
The cognitive pro-cess of referring to an object: A comparative study ofgerman and swedish.
In Proceedings of the 16th Scan-dinavian Conference on Linguistics.C.
Gardent, H. Manue?lian, K. Striegnitz, and M. Amoia.2004.
Generating definite descriptions: Non-incrementality, inference, and data.
In T. Pechmanand C. Habel, editors, Multidisciplinary Approachesto Language Production.
Mouton de Gruyter.C.
Gardent.
2002.
Generating minimal definite descrip-tions.
In Proceedings of the 40th Annual Meeting ofthe Association for Computational Linguistics, ACL-02.A.
Gatt, I. van der Sluis, and K. van Deemter.
2007.Evaluating algorithms for the generation of referringexpressions using a balanced corpus.
In Proceedingsof the 11th European Workshop on Natural LanguageGeneration, ENLG-07.
To appear.H.P.
Grice.
1975.
Logic and conversation.
In P. Cole andJ.L.
Morgan, editors, Syntax and Semantics: SpeechActs., volume III.
Academic Press.S.
Gupta and A. J. Stent.
2005.
Automatic evaluationof referring expression generation using corpora.
InProceedings of the 1st Workshop on Using Corpora inNLG, Birmingham, UK.H.
Horacek.
2004.
On referring to sets of objects natu-rally.
In Proceedings of the 3rd International Confer-ence on Natural Language Generation, INLG-04.P.
W. Jordan and M. Walker.
2005.
Learning content se-lection rules for generating object descriptions in di-alogue.
Journal of Artificial Intelligence Research,24:157?194.G.
L. Murphy.
2002.
The big book of concepts.
MITPress, Cambridge, Ma.D.
R. Olson.
1970.
Language and thought: Aspects of acognitive theory of semantics.
Psychological Review,77:257?273.Thomas Pechmann.
1989.
Incremental speech pro-duction and referential overspecification.
Linguistics,27:89?110.I.
Rock.
1983.
The Logic of Perception.
MIT Press,Cambridge, Ma.E.
Rosch, C. B. Mervis, W. Gray, D. Johnson, andP.
Boyes-Braem.
1976.
Basic objects in natural cat-egories.
Cognitive Psychology, 8:382?439.H.
Schriefers and T. Pechmann.
1988.
Incremental pro-duction of referential noun phrases by human speak-ers.
In M. Zock and G. Sabah, editors, Advances inNatural Language Generation, volume 1.
Pinter, Lon-don.D.
Shasha and K. Zhang.
1990.
Fast algorithms for unitcost editing distance between trees.
Journal of Algo-rithms, 11:581?621.K.
van Deemter and E. Krahmer.
2007.
Graphs andbooleans: On the generation of referring expressions.In H. Bunt and R. Muskens, editors, Computing Mean-ing, volume III.
Springer, Berlin.K.
van Deemter, I. van der Sluis, and A. Gatt.
2006.Building a semantically transparent corpus for thegeneration of referring expressions.
In Proceedingsof the 4th International Conference on Natural Lan-guage Generation (Special Session on Data Sharingand Evaluation), INLG-06.K.
van Deemter.
2002.
Generating referring expres-sions: Boolean extensions of the incremental algo-rithm.
Computational Linguistics, 28(1):37?52.I.
van der Sluis and E. Krahmer.
2005.
Towards the gen-eration of overspecified multimodal referring expres-sions.
In Proceedings of the Symposium on DialogueModelling and Generation, 15th Annual Meeting ofthe Society for Text and Discourse, STD-05.J.
Viethen and R. Dale.
2006.
Algorithms for generat-ing referring expressions: Do they do what people do?In Proceedings of the 4th International Conference onNatural Language Generation, INLG-06.M.
Wertheimer.
1938.
Laws of organization in per-ceptual forms.
In W. Ellis, editor, A Source Book ofGestalt Psychology.Routledge &Kegan Paul, London.111
