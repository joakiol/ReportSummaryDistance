Hierarchical Orderings of Textual UnitsAlexander MehlerUniversity of TrierUniversita?tsring 15D-54286 Trier, Germanymehler@uni-trier.deAbstractText representation is a central task for any ap-proach to automatic learning from texts.
It re-quires a format which allows to interrelate textseven if they do not share content words, butdeal with similar topics.
Furthermore, measur-ing text similarities raises the question of howto organize the resulting clusters.
This paperpresents cohesion trees (CT) as a data structurefor the perspective, hierarchical organization oftext corpora.
CTs operate on alternative textrepresentation models taking lexical organiza-tion, quantitative text characteristics, and textstructure into account.
It is shown that CTsrealize text linkages which are lexically morehomogeneous than those produced by minimalspanning trees.1 IntroductionText representation is a central task for ap-proaches to text classification or categorization.They require a format which allows to seman-tically relate words, texts, and thematic cate-gories.
The majority of approaches to automa-tic learning from texts use the vector space orbag of words model.
Although there is much re-search for alternative formats, whether phrase-or hyperonym-based, their effects seem to besmall (Scott and Matwin, 1999).
More serious-ly (Riloff, 1995) argues that the bag of wordsmodel ignores morphological and syntactical in-formation which she found to be essential forsolving some categorization tasks.
An alterna-tive to the vector space model are semanticspaces, which have been proposed as a high-dimensional format for representing relations ofsemantic proximity.
Relying on sparse know-ledge resources, they prove to be efficient in cog-nitive science (Kintsch, 1998; Landauer and Du-mais, 1997), computational linguistics (Rieger,1984; Schu?tze, 1998), and information retrieval.Although semantic spaces prove to be an al-ternative to the vector space model, they leavethe question unanswered of how to explore andvisualize similarities of signs mapped onto them.In case that texts are represented as points insemantic space, this question refers to the ex-ploration of their implicit, content based rela-tions.
Several methods for solving this task havebeen proposed which range from simple lists viaminimal spanning trees to cluster analysis aspart of scatter/gahter algorithms (Hearst andPedersen, 1996).
Representing a sign?s environ-ment in space by means of lists runs the risk ofsuccessively ordering semantically or themati-cally diverse units.
Obviously, lists neglect thepoly-hierarchical structure of semantic spaceswhich may induce divergent thematic progres-sions starting from the same polysemous unit.Although clustering proves to be an alternativeto lists, it seeks a global, possibly nested par-tition in which clusters represent sets of indis-tinguishable objects regarding the cluster crite-rion.
In contrast to this, we present cohesiontrees (CT) as a data structure, in which singleobjects are hierarchically ordered on the basis oflexical cohesion.
CTs, whose field of applicationis the management of search results in IR, shiftthe perspective from sets of clustered objects tocohesive paths of interlinked signs.The paper is organized as follows: the nextsection presents alternative text representationmodels as extensions of the semantic space ap-proach.
They are used in section (3) as a back-ground of the discussion of cohesion trees.
Bothtypes of models, i.e.
the text representationmodels and cohesion trees as a tool for hierarchi-cally traversing semantic spaces, are evaluatedin section (4).
Finally, section (5) gives someconclusions and prospects future work.2 Numerical Text RepresentationThis paper uses semantic spaces as a formatfor text representation.
Although it neglectssentence as well as rhetorical structure, it de-parts from the bag of words model by refer-ring to paradigmatic similarity as the funda-mental feature type: instead of measuring in-tersections of lexical distributions, texts are in-terrelated on the basis of the paradigmatic regu-larities of their constituents.
A coordinate valueof a feature vector of a sign mapped onto se-mantic space measures the extent to which thissign (or its constituents in case of texts) sharesparadigmatic usage regularities with the worddefining the corresponding dimension.
Becauseof this sensitivity to paradigmatics, semanticspaces can capture indirect meaning relations:words can be linked even if they never co-occur,but tend to occur in similar contexts.
Further-more, texts can be linked even if they do notshare content words, but deal with similar top-ics (Landauer and Dumais, 1997).
Using thismodel as a starting point, we go a step furtherin departing from the bag of words model bytaking quantitative characteristics of text struc-ture into account (see below).Semantic spaces focus on meaning as useas described by the weak contextual hypothe-sis (Miller and Charles, 1991), which says thatthe similarity of contextual representations ofwords contributes to their semantic similarity.Regarding the level of texts, reformulating thishypothesis is straightforward:Contextual hypothesis for texts: the contextualsimilarity of the lexical constituents of two textscontributes to their semantic similarity.In other words: the more two texts share se-mantically similar words, the higher the proba-bility that they deal with similar topics.
Clearly,this hypothesis does not imply that texts havingcontextually similar components to a high de-gree also share propositional content.
It is thestructural (connotative), not the propositional(denotative) meaning aspect to which this hy-pothesis applies.
Moreover, this version of thecontextual hypothesis neglects the structural di-mension of similarity relations: not only thata text is structured into thematic components,each of which may semantically relate to differ-ent units, but units similar to the text as a wholedo not form isolated, unstructured clumps.
Ne-glecting the former we focus on the latter phe-nomenon, which demands a supplementary hy-pothesis:Structure sensitive contextual hypothesis: units,which are similar to a text according to the con-textual hypothesis, contribute to the structur-ing of its meaning.Since we seek a model for automatic text rep-resentation for which nonlinguistic context isinaccessible, we limit contextual similarity toparadigmatic similarity.
On this basis the lattertwo hypotheses can be summarized as follows:Definition 1.
Let C be a corpus in which weobserve paradigmatic regularities of words.
Thetextual connotation of a text x with respect toC includes those texts of C, whose constituentsrealize similar paradigmatic regularities as thelexical constituents of x.
The connotation of x isstructured on the basis of the same relation of(indirect) paradigmatic similarity interrelatingthe connoted texts.In order to model this concept of structuredconnotation, we use the space model M0 of(Rieger, 1984) as a point of departure and de-rive three text representation models M1, M2,M3.
Since M0 only maps words onto semanticspace we extend it in order to derive meaningpoints of texts.
This is done as follows:M0 analyses word meanings as the result ofa two-stage process of unsupervised learning.
Itbuilds a lexical semantic space by modeling syn-tagmatic regularities with a correlation coeffi-cient ?
: W ?
C ?
Rn and their differences withan Euclidean metric ?
: C ?
S ?
Rn, where Wis the set of words, C is called corpus space repre-senting syntagmatic regularities, and S is calledsemantic space representing paradigmatic regu-larities.
|W | = n is the number of dimensions ofboth spaces.
Neighborhoods of meaning pointsassigned to words model their semantic similar-ity: the shorter the points?
distances in seman-tic space, the more paradigmatically similar thewords.The set of words W , spanning the semanticspace, is selected on the basis of the criterion ofdocument frequency, which proves to be of com-parable effectiveness as information gain and?2-statistics (Yang and Pedersen, 1997).
Fur-thermore, instead of using explicit stop wordlists, we restricted W to the set of lemmatizednouns, verbs, adjectives, and adverbs.M1: In a second step, we use S as a formatfor representing meaning points of texts, whichare mapped onto S with the help of a weightedmean of the meaning points assigned to theirlexical constituents:~xk =?ai?W (xk)wik~ai ?
S (1)~xk is the meaning point of text xk ?
C, ~ai themeaning point of word ai ?
W , and W (xk) isthe set of all types of all tokens in xk.
Finally,wik is a weight having the same role as the tfidf-scores in IR (Salton and Buckley, 1988).
Asa result of mapping texts onto S, they can becompared with respect to the paradigmatic sim-ilarity of their lexical organization.
This is donewith the help of a similarity measure ?
basedon an Euclidean metric ?
operating on meaningpoints and standardized to the unit interval:?
: {~x |x ?
C}2 ?
[0, 1] (2)?
is interpreted as follows: the higher ?
(~x, ~y)for two texts x and y, the shorter the distanceof their meaning points ~x and ~y in semanticspace, the more similar the paradigmatic usageregularities of their lexical constituents, and fi-nally the more semantically similar these textsaccording to the extended contextual hypothe-sis.
This is the point, where semantic spacesdepart from the vector space model, since theydo not demand that the texts in question shareany lexical constituents in order to be similar;the intersection of the sets of their lexical con-stituents may even be empty.M2: So far, only lexical features are consid-ered.
We depart a step further from the bagof words model by additionally comparing textswith respect to their organization.
This is donewith the help of a set of quantitative text char-acteristics used by (Tuldava, 1998) for auto-matic genre analysis: type-token ratio, hapaxlegomena, (variation of) mean word frequency,average sentence length, and action coefficient(i.e.
the standardized ratio of verbs and adjec-tives in a text).
In order to make these fea-tures comparable, they were standardized us-ing z-scores so that random variables were de-rived with means of 0 and variances of 1.
Be-yond these characteristics, a further feature wasconsidered: each text was mapped onto a socalled text structure string representing its divi-sion into sections, paragraphs, and sentences asa course approximation of its rhetorical struc-ture.
For example, a text structure string(T (D(S))(D(S ?
S ?
S))) (3)denotes a text T of two sections D, where thefirst includes 1 and the second 3 sentences S.Using the Levenshtein metric for string compa-rison, this allows to measure the rhetorical simi-larity of texts in a first approximation.
The ideais to distinguish units connoted by a text, whichin spite of having similar lexical organizationsdiffer texturally.
If for example a short com-mentary connotes two equally similar texts, an-other commentary and a long report, the com-mentary should be preferred.
Thus, in M2 thetextual connotation of a text is not only seen tobe structured on the basis of the criterion of sim-ilarity of lexical organization, but also by meansof genre specific features modeled as quantita-tive text characteristics.
This approach follows(Herdan, 1966), who programmatically asked,whether difference in style correlates with dif-ference in frequency of use of linguistic forms.See (Wolters and Kirsten, 1999) who, followingthis approach, already used POS frequency as asource for genre classification, a task which goesbeyond the scope of the given paper.On this background a compound text similar-ity measure can be derived as a linear model:?
(x, y) =3?i=1?i?i(x, y) ?
[0, 1] (4)a. where ?1(x, y) = ?
(~x, ~y) models lexical se-mantics of texts x, y according to M1;b.
?2 uses the Levenshtein metric for measur-ing the similarity of the text structure stingsassigned to x and y;c. and ?3 measures, based on an Euclidean me-tric, the similarity of texts with respect tothe quantitative features enumerated above.
?i biases the contribution of these different di-mensions of text representation.
We yield goodresults for ?1 = 0.9, ?2 = ?3 = 0.05.M3: Finally, we experimented with a textrepresentation model resulting from the aggre-gation (i.e.
weighted mean) of the vector repre-sentations of a text in both spaces, i.e.
vectorand semantic space.
This approach, which de-mands both spaces to have exactly the samedimensions and standardized coordinate values,follows the idea to reduce the noise inherent toboth models: whether syntagmatic as in caseof vector spaces, or paradigmatic as in case ofsemantic spaces.
We experimented with equalweights of both input vectors.In the next section we use the text represen-tation models M1, M2, M3 as different startingpoints for modeling the concept of structuredconnotation as defined in definition (1):3 Text LinkageDeparting from ordinary list as well as clusterstructures, we model the connotation of a textas a hierarchy, where each node represents a sin-gle connoted text (and not a set of texts as incase of agglomerative cluster analysis).
In or-der to narrow down a solution for this task weneed a linguistic criterion, which bridges be-tween the linguistic knowledge represented insemantic spaces and the task of connotative textlinkage.
For this purpose we refer to the con-cept of lexical cohesion introduced by (Hallidayand Hasan, 1976); see (Morris and Hirst, 1991;Hearst, 1997; Marcu, 2000) who already use thisconcept for text segmentation.
According tothis approach, lexical cohesion results from re-iterating words, which are semantically relatedon the basis of (un-)systematic relations (e.g.synonymy or hyponymy).
Unsystematic lexi-cal cohesion results from patterns of contextual,paradigmatic similarity: ?[.
.
. ]
lexical itemshaving similar patterns of collocation?that is,tending to appear in similar contexts?will gen-erate a cohesive force if they occur in adjacentsentences.?
(Halliday and Hasan, 1976, p. 286).Several factors influencing this cohesive forceare decisive for reconstructing the concept oftextual connotation:(i) the contextual similarityof the words in question, (ii) their syntagmaticorder, and (iii) the distances of their occurren-ces.
These factors cooperate as follows: theshorter the distance of similar words in a textthe higher their cohesive force.
Furthermore,preceding lexical choices restrict (the interpre-tation of) subsequent ones, an effect, which re-tards as their distance grows.
But longer dis-tances may be compensated by higher contex-tual similarities so that highly related words cancontribute to the cohesion of a text span evenif they distantly co-occur.
By means of restrict-ing contextual to paradigmatic similarity andtherefore measuring unsystematic lexical cohe-sion as a function of paradigmatic regularities,the transfer of this concept to the task of hierar-chically modeling textual connotations becomesstraightforward.
Given a text x, whose connota-tion is to be represented as a tree T , we demandfor any path P starting with root x:(i) Similarity: If text y is more similar to xthan z, then the path between x and y isshorter than between x and z, supposedthat y and z belong to the same path P .
(ii) Order: The shorter the distance between yand z in P , the higher their cohesive force,and vice versa: the longer the path, thehigher the probability that the subsequentz is paradigmatically dissimilar to y.
(iii) Distance: A cohesive impact is preservedeven in case of longer paths, supposed thatthe textual nodes lying in between areparadigmatically similar to a high degree.The reason underlying these criteria is theneed to control negative effects of intransitivesimilarity relations: in case that text x is highlysimilar to y, and y to z, it is not guaranteed that(x, y, z) is a cohesive path, since similarity is nottransitive.
In order to reduce this risk of incohe-sive paths, the latter criteria demand that thereis a cohesive force even between nodes which arenot immediately linked.
This demand decreasesas the path distance of nodes increases so thattopic changes latently controlled by precedingnodes can be realized.
In other words: addingtext z to the hierarchically structured connota-tion of x, we do not simply look for an alreadyinserted text y, to which z is most similar, butto a path P , which minimizes the loss of cohe-sion in the overall tree, when z is attached to P .These comments induce an optimality criterionwhich tries to optimize cohesion not only of di-rectly linked nodes, but of whole paths, therebyreflecting their syntagmatic order.
Looking fora mathematical model of this optimality crite-rion, minimal spanning trees (MST) drop out,since they only optimize direct node-to-nodesimilarities disregarding any path context.
Fur-thermore, whereas we expect to yield differ-ent trees modeling the connotations of differ-ent texts, MSTs ignore this aspect dependencysince they focus on a unique spanning tree ofthe underlying feature space.
Another candi-date is given by dependency trees (Rieger, 1984)which are equal to similarity trees (Lin, 1998):for a given root x, the nodes are inserted intoits similarity tree (ST) in descending order oftheir similarity to x, where the predecessor ofany node z is chosen to be the node y alreadyinserted, to which z is most similar.
AlthoughSTs already capture the aspect dependency in-duced by their varying roots, the path criterionis still not met.
Thus, we generalize the conceptof a ST to that of a cohesion tree as follows:First, we observe that the construction of STsuses two types of order relations: the first, letit call ?1x, determines the order of the nodesinserted dependent on root x; the second, letit call ?2y, varies with node y to be insertedand determines its predecessor.
Next, in orderto build cohesion trees out of this skeleton, weinstantiate all relations ?2y in a way, which findsthe path of minimal loss of cohesion when y isattached to it.
This is done with the help ofa distance measure which induces a descendingorder of cohesion of paths:Definition 2.
Let G = ?V,E?
be a graph andP = (v1, .
.
.
, vk) a simple path in G. The pathsensitive distance ??
(P, y) of y ?
V with respectto P is defined as??
(P, y) =1max(?
)?vi?V (P )?i?
(~y,~vi) ?
[0, 1],where?vi?V (P )?i ?
1, max(?)
is the maximalvalue assumed by distance measure ?, and V (P )is the set of all nodes of path P .It is clear that for any of the text representa-tion models M1, M2, M3 and their correspond-ing similarity measures we get different distancemeasures ??
which can be used to instantiate theorder relations ?2y in order to determine the endvertex of the path of minimal loss of cohesionwhen y is attached to it.
In case of increasingbiases ?i for increasing index i in definition (2)the syntagmatic order of path P is reflected inthe sense that the shorter the distance of x toany vertex in P , the higher the impact of their(dis-)similarity measured by ?, the higher theircohesive force.
Using the relations ?2y we cannow formalize the concept of a cohesion tree:Definition 3.
Let G = ?V,E, ??
be a completeweighted graph induced by a semantic space,and x ?
V a node.
The graph D(G, x) =?V, E , ??
with E = {{v, w} | v <1x w ?
?
?y ?
V :y <1x w?y <2w v} and ?
: E ?
R, the restrictionof ?
to E , is called cohesion tree induced by x.Using this definition of a cohesion tree (CT)we can compute hierarchical models of the con-notations of texts, in which not only aspect de-pendency induced by the corresponding root,but also path cohesion is taken into account.A note on the relation between CTs and clus-ter analysis: CTs do not only depart from clus-ter hierarchies, since their nodes represent sin-gle objects, and not sets, but also because theyrefer to a local, contextsensitive building crite-rion (with respect to their roots and paths).
Incontrast to this, cluster analysis tries to find aglobal partition of the data set.
Neverthelessthere is a connection between both methods ofunsupervised learning: Given a MST, there isa simple procedure to yield a divisive partition(Duda et al, 2001).
Moreover, single linkagegraphs are based on a comparable criterion asMSTs.
Analogously, a given CT can be dividedinto non-overlapping clusters by deleting thoseedges whose length is above a certain threshold.This induces, so to say, perspective clusters or-ganized dependent on the perspective of the rootand paths of the underlying CT.4 EvaluationFigure (1) exemplifies a CT based on M3 using atextual root dealing with the ?BSE Food Scan-dal?
from 1996.
The text sample belongs to acorpus of 502 texts of the German newspaperSu?ddeutsche Zeitung of about 320,000 run-ning words.
Each text belongs to an element ofa set T of 18 different subject categories (e.g.politics, sports).
Based on the lemmatized cor-pus a semantic space of 2715 lexical dimensionswas built and all texts were mapped onto thisspace according to the specifications of M3.
Infigure (1) each textual node of the CT is rep-resented by its headline and subject categoryas found in the newspaper.
All computationsFigure 1: A sample CT.were performed using a set of C++ programs es-pecially implemented for this study.In order to rate models M1, M2, M3 in com-parison to the vector space model (VS) usingMSTs, STs and CTs as alternative hierarchi-cal models we proceed as follows: as a simplemeasure of representational goodness we com-pute the average categorial cohesion of links ofall MSTs, STs and CTs for the different modelsand all texts in the corpus.
Let G = ?V,E?
bea tree of textual nodes x ?
V , each of which isassigned to a subject category ?
(x) ?
T , andP (G) the set of all paths in G starting withroot x and ending with a leaf, then the cate-gorial cohesion of G is the average number oflinks (vi, vj) ?
E per path P ?
P (G), where?
(vi) = ?(vj).
The more nodes of identical cat-egories are linked in paths in G, the more cat-egorially homogeneous these paths, the higherthe average categorial cohesion of G. Accordingto the conceptual basis of CTs we expect thesetrees to be of highest categorial link cohesion,but this is not true: MSTs produce the highestcohesion values in case of VS and M3.
Further-more, we observe that model M3 induces treesof highest cohesion and lowest variance, whereasVS shows the highest variance and lowest cohe-sion scores in case of STs and CTs.
In otherwords: based on semantic spaces, models M1,M2, and M3 produce more stable results thanthe vector space model.Using M3 as a starting point it can beasked more precisely, which tree class producesthe most cohesive model of text connotation.Clearly, the measure of categorial link cohesionis not sufficient to evaluate the classes, since twoimmediately linked texts belonging to the sameModel MSTs STs CTsVS 1325.88 462.04 598.87M1 1093.06 680.06 1185.92M2 1097.39 661.72 1168.63M3 1488.38 628.51 1032.55Table 1: Alternative representation models andscores of trees derived from them.subject category may nevertheless deal with dif-ferent topics.
Thus we need a finer-grainedmeasure which operates directly on the texts?meaning representations.
In case of unsuper-vised clustering, where fine-grained class labelsare missed, (Steinbach et al, 2000) propose ameasure which estimates the overall cohesion ofa cluster.
This measure can be directly appliedto trees: let Pv1,vn = (v1, .
.
.
, vn) be a path intree G = ?V,E?
starting with root v1 = x, wecompute the cohesion of P irrespective of theorder of its nodes as follows:?
(Pv1,vn) = 1?1n2n?i,j=11max(?)?
(vi, vj) (5)The more similar the nodes of path P accor-ding to metric ?, the more cohesive P .
?
isderived from the distance measure operatingon the semantic space to which texts vi aremapped.
As before, all scores ?
(P ) are summedup for all paths in P (G) and standardized bymeans of |P (G)|.
This guarantees that neithertrees of maximum height (MHT) nor of max-imum degree (MDT), i.e.
trees which triviallycorrespond to lists, are assigned highest cohe-sion values.
The results of summing up thesescores for all trees of a given class for all textsin the test corpus are shown in table (2).
Now,Type??
(G) Type??
(G)MDT 388.1 MST 416.3MHT 388.1 DT 430.9RST 386.6 CT 438.6Table 2: The sum of the cohesion scores for alltree classes and all texts in the test corpus.CTs and STs realize the most cohesive struc-tures.
This is more obvious if the scores ?
(G)are compared for each text in separation: in494 cases, CTs are of highest cohesion accord-ing to measure (5).
In only 7 cases, MST areof highest cohesion, and in only one case, thecorresponding ST is of highest cohesion.
More-over, even the stochastically organized so calledrandom successor trees (RST), in which succes-sor node?s and their predecessors are randomlychosen, produce more cohesive structures thanlists (i.e.
MDTs and MHTs), which form thepredominant format used to organize search re-sults in Internet.To sum up: Table (2) rates CTs in combi-nation with model M3 on highest level.
Thus,from the point of view of lexical semantics CTsrealize more cohesive branches than MSTs.
Butwhether these differences are significant, is hardto evaluate, since their theoretical distributionis unknown.
Thus, future work will be on find-ing these distributions.5 ConclusionThis paper proposed 3 numerical representationformats as means for modeling the hierarchicalconnotation of texts in combination with cohe-sion trees.
This was done by extending the weakcontextual hypothesis onto the level of texts incombination with a reinterpretation of the con-cept of lexical cohesion as a source for text link-age.
Although the formats used depart fromthe bag of words model there is still the need ofinvestigating numerical formats which rely onlinguistically more profound discourse models.ReferencesR.
O. Duda, P. E. Hart, and D. G. Stork.
2001.Pattern Classification.
Wiley, New York.Michael A. K. Halliday and R. Hasan.
1976.Cohesion in English.
Longman, London.M.
A. Hearst and J. O. Pedersen.
1996.
Reex-amining the cluster hypothesis: Scatter/gath-er on retrieval results.
In Proc.
ACM SIGIR.M.
A. Hearst.
1997.
Texttiling: Segmentingtext into multi-paragraph subtopic passages.Computational Linguistics, 23(1):33?64.G.
Herdan.
1966.
The Advanced Theory ofLanguage as Choice and Chance.
Springer,Berlin.W.
Kintsch.
1998.
Comprehension.
A Paradigmfor Cognition.
Cambridge University Press.T.
K. Landauer and S. T. Dumais.
1997.
A solu-tion to plato?s problem.
Psychological Review,104(2):211?240.D.
Lin.
1998.
Automatic retrieval and cluster-ing of similar words.
In Proc.
COLING-ACL.D.
Marcu.
2000.
The Theory and Practice ofDiscourse Parsing and Summarization.
MITPress, Cambridge, Massachusetts.G.
A. Miller and W. G. Charles.
1991.
Con-textual correlates of semantic similarity.
Lan-guage and Cognitive Processes, 6(1):1?28.J.
Morris and G. Hirst.
1991.
Lexical cohesioncomputed by thesaural relations as an indi-cator of the structure of text.
ComputationalLinguistics, 17(1):21?48.B.
Rieger.
1984.
Semantic relevance and as-pect dependency in a given subject domain.In Proc.
10th COLING.E.
Riloff.
1995.
Little words can make a bigdifference for text classification.
In Proc.SIGIR-95.G.
Salton and C. Buckley.
1988.
Termweighting approaches in automatic text re-trieval.
Information Processing Management,24(5):513?523.H.
Schu?tze.
1998.
Automatic word sensediscrimination.
Computational Linguistics,24(1):97?123.S.
Scott and S. Matwin.
1999.
Feature engi-neering for text classification.
In Proc.
16thICML, pages 379?388.M.
Steinbach, G. Karypis, and V. Kumar.
2000.A comparison of document clustering tech-niques.
In KDD Workshop on Text Mining.J.
Tuldava.
1998.
Probleme und Methoden derquantitativ-systemischen Lexikologie.
Wis-senschaftlicher Verlag, Trier.M.
Wolters and M. Kirsten.
1999.
Exploringthe use of linguistic features in domain andgenre classication.
In Proc.
EACL.Y.
Yang and J. O. Pedersen.
1997.
A compar-ative study on feature selection in text cate-gorization.
In Proc.
14th ICML.
