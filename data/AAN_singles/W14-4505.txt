Proceedings of the AHA!
Workshop on Information Discovery in Text, pages 25?30,Dublin, Ireland, August 23 2014.Word Clustering Based on Un-LP AlgorithmJiguang Liang1, Xiaofei Zhou1, Yue Hu1, Li Guo1?, Shuo Bai1,21National Engineering Laboratory for Information Security Technologies,Institute of Information Engineering, Chinese Academy of Sciences,Beijing 100190, China2Shanghai Stock Exchange, Shanghai 200120, China{liangjiguang, zhouxiaofei, huyue, guoli, baishuo}@iie.ac.cnAbstractWord clustering which generalizes specific features cluster words in the same syntactic or seman-tic categories into a group.
It is an effective approach to reduce feature dimensionality and featuresparseness which are clearly useful for many NLP applications.
This paper proposes an unsu-pervised label propagation algorithm (Un-LP) for word clustering which uses multi-exemplarsto represent a cluster.
Experiments on a synthetic 2D dataset show the strong ability of self-correcting of the proposed algorithm.
Besides, the experimental results on 20NG demonstratethat our algorithm outperforms the conventional cluster algorithms.1 IntroductionWord clustering is the task of the division of words into a certain number of clusters (groups or cat-egories).
Each cluster is required to consist of words that are similar to one another in syntactic orsemantic construct and dissimilar to words in distinctive groups.
Word clustering generalizes specificfeatures by considering the common characteristics and ignoring the specific characteristics among theindividual features.
It is an effective approach to reduce feature dimensionality and feature sparseness(Han et al., 2005).Recently, word clustering offers great potential for various useful NLP applications.
Several studieshave addressed dependency parsing (Koo et al., 2008; Sagae and Gordon, 2009).
Momtazi and Klakow(2009) propose a word clustering approach to improve the performance of sentence retrieval in QuestionAnswering (QA) systems.
Wu et al.
(2010) present an approach to integrate word clustering informationinto the process of unsupervised feature selection.
Sun et al.
(2011) use large-scale word clustering fora semi-supervised relation extraction system.
It also contributes to word sense disambiguation (Jin etal., 2007), named entity recognition (Turian et al., 2010), part-of-speech tagging (Candito and Seddah,2010) and machine translation (Uszkoreit and Brants, 2008; Jeff et al., 2011).This paper presents an unsupervised algorithm for word clustering based on a probabilistic transitionmatrix.
Given a text document dataset, a list of words is generated by removing stop words and veryunfrequent words.
Each word is required to be represented by the documents in the dataset, which resultsin a co-occurrence matrix.
By calculating the similarity of words, a word similarity graph with transition(propagation) probabilities as weight edges is created.
Then, a new kind word clustering algorithm, basedon label propagation, is applied.The remaining parts of this paper are organized as follows: Section 2 formulates word clusteringproblem in the context of unsupervised learning.
Then we describe the word clustering algorithm inSection 3 and present our experiments in Section 4.
Finally we conclude our work in Section 5.2 Problem setupAssume that we have a corpus with N documents denoted by D = {d1, d2, ?
?
?
, dN}; each document inthe corpus consists of a list of words denoted by di= {w1, w2, ?
?
?
, wNd} where each wiis an item froma vocabulary index with V distinct terms denoted by W = {v1, v2, ?
?
?
, vV} and Ndis the documentThis work is licensed under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/.25Algorithm 1 Semi-supervised LP Algorithm Algorithm 2 Unsupervised LP Word ClusteringInput: Input:Wl= {vi}li=1labeled data W = {vi}ui=1(u = V ) unlabeled wordsWu= {vi}Vi=uunlabeled data Tuu= {Tij} 1 ?
i, j ?
V transition matrixT = {Tij} 1 ?
i, j ?
V transition matrix Output:Output: YU?
= {(?1,?2, ?
?
?
,?L} word-clusters1: Begin 1: Begin2: Row-normalize T by Tij= Tij/?Vk=1Tik2: {V 0L, YL, T0ul} = initialization(W )3: While not converged do 3: While not converged do4: Propagate the labels by Y t+1 = TY t 4: Y t+1U= Semi ?
LP (V tL,YtL,T0ul,Tuu)5: Row-normalize Y t+1 5: ?t+1 = partition cluster(Y t+1U)6: Clamp the labeled data 6: {V t+1L, Tt+1ul} = update(?t+1)7: End while 7: End while8: End 8: End9: Return YU9: Return ?t+1length.
We define the vector of word viin the vocabulary to be vi=< vid1, vid2, ?
?
?
, vidN>.
Thisallows us to define a V ?
N word-document matrix WD for the vocabularies.
WDijis equal to 1 ifvi?
djand equal to 0 otherwise.
Then we take these words as the vertices of a connected graph.
Inthis paper, we define the edge weight ?ijas the co-occurrence frequency between viand vj.
Obviously,we expect that larger edge weights allow labels to travel through more easily.
So we define a V ?
Vprobabilistic transition matrix T where Tij= P (vj?
vi) = ?ij/?Vk=1?kj.The L value which is used to represent the number of word clusters is specified.
We define a V ?
Llabel matrix Y .
Clearly, yi?
Y represents the label probability distributions of word viand Y ?i=argmax Yik(0 < k ?
L) is its cluster label.
For example, suppose L = 3 and a word v has a labeldistribution y =< 0.1, 0.8, 0.1 >, it implies that v belongs to the second class.3 Unsupervised LP Word ClusteringLabel propagation (Zhu and Ghahramani, 2002) is a semi-supervised algorithm (Semi-LP) which needslabeled data.
Let {(v1, y1), ?
?
?
, (vl, yl)} be labeled data, {(vl+1, yl+1), ?
?
?
, (vl+u, yl+u)} be unlabeledones where l + u = V , YL= [y1, , ?
?
?
, yl]T and YU= [yl+1, ?
?
?
, yl+u]T. YUis un-known and l << u.The label propagation algorithm is summarized in Algorithm 1.In clustering problems, the goal is to select a set of exemplars from a dataset that are representativeof the dataset and each cluster is represented by one and only one exemplar (Krause and Gomes, 2010).However, these exemplars are just all Semi-LP needs for clustering.
LP lacks labeled data when is usedfor unsupervised learning.
In this paper, we are interested in partitioning words into several clusterswithout any label priori using unsupervised LP (Un-LP) algorithm.
Firstly we randomly select K (K ?L, usually K is a multiple of L) words to construct an exemplar set E = {Ei}Ki=1which is differentfrom the conventional exemplar-based cluster algorithms, assign class labels to them and construct thecorresponding probabilistic transition matrix T 0ul(initialization).
These exemplars are considered aslabeled words and the rest U = W ?
E are unlabeled words.
Tulis the probability of transition fromunlabeled words to labeled ones.
At this step, it needs the assurance that each class could be representedby at least one exemplar and each exemplar could only be assigned one class label.Now the connected weighted graph consists of two parts: G = (E ?
U, Tul?
Tuu) where Tuuisthe transition probability between unlabeled words.
Next, our algorithm iterates between the followingthree steps: given a set of LP parameters, we first propagate labels to unlabeled words with the initiallabel distributions and get the corresponding labels (Semi?LP ).
Then, these derived label distributionsare used to guide the partitioning of unlabeled data (partition cluster) to L clusters.
We use residualsum of squares (RSS) to choose the most centrally located words and replace the old exemplars thatrepresent the cluster.
Specifically, for a word cluster ci= {v1, ?
?
?
vn}, RSSi=?nj=1?ij.
Then we sortRSSi(0 < i < n) and update exemplars by the words with bigger RSS for this cluster (update).
Allof the above steps, summarized in Algorithm 2, are iterated until convergence.264 Experiment4.1 Experimental SetupTo demonstrate properties of our proposed algorithm we investigate both a synthetic dataset and a real-world dataset.
Figure 1(a) shows the synthetic dataset.
For a real world example we test Un-LP on asubset of 20 Newsgroups (20NG) dataset which is preprocessed by removing common stop-words andstemming.
We use the classes atheism, hardware, hockey and space for test and randomly select300 samples from each class as the test dataset in this section.
However, 20NG is not suited for wordclustering evaluation.
So, firstly, we reconstruct it by pair-wise testing which is a specification-basedtesting criterion.
Then we can obtain six (C24= 6) pairwise subsets represented by {D1, ?
?
?
,D6}.
Inorder to facilitate the evaluation, we use those words that only occur in one class for clustering.4.2 Exemplar Self-correctionThis multi-step iterative method is simple to implement and surprisingly effective even with wrong initiallabeled data.
To illustrate the point, we describe a simulated dataset with two-moon pattern.
Obviously,the points in one moon should be more similar to each other than the points across the moons as shownin Figure 1(b).
During the initialization phase, four points in the lower moon are selected and assignedwith different labels.
The exemplars of the upper moon are mis-labeled as shown in Figure 1(c).
In thenext five iteration steps, exemplars have been gradually moved to the center of the upper moon.
Finally,when t ?
5 Un-LP converges to a fixed assignment, which achieves an ideal cluster result.
?2 ?1 0 1 2 3?2?1012(1)  Synthetic Dataset?2 ?1 0 1 2 3?2?1012(2) Ideal Cluster?2 ?1 0 1 2 3?2?1012(3) t=1?2 ?1 0 1 2 3?2?1012(4) t=2?2 ?1 0 1 2 3?2?1012(5) t=3?2 ?1 0 1 2 3?2?1012(6) t=4?2 ?1 0 1 2 3?2?1012(7) t=5?2 ?1 0 1 2 3?2?1012(8) t=6Figure 1: Clustering result of unsupervised LP clustering algorithm on two-moon pattern dataset.
(a)Two-moon pattern dataset without any labeled points, (b) ideal clustering results.
The convergence pro-cess of unsupervised LP with t from 1 to 6 is shown from (c) to (h).
Solid points are labeled data that areselected to represent the clusters.4.3 Word Clustering PerformanceThis section provides empirical evidence that the proposed algorithm performs well in the problem ofword clustering.
Figure 2 shows the mean precisions and recalls over 10 runs of the baseline algorithmsas well as Un-LP.From Figure 2, it can be clearly observed that Un-LP (K/L = 5) yields the best performance, followedby Semi-LP with 20 labeled words.
In general, the recalls with k-means and k-medoids are higher,while the precisions are much lower.
Figure 2 also shows the results of other two semi-supervised word27Cluster1 Cluster2 Cluster3 Cluster4Atheism Hardware Hockey Spacegeode religiously bene-factor meng stackermcl mormon maddenmythology timmons cb-newsj agnostics fanatismengr chade tan falsifiableexisted ucsb sentencedriver soundblaster card-s isbn manufacturer portalprize mastering connectorsfloppies dock adapter mul-timedia installing bowmanconfigure physchem jumpersmotherboardsfdisk seagategoalies bug hfd johanssonbreton scorers carpenterstevens smythe janneyfleury vancouver stlcheveldae selanne win-nipeg canadiens bure nyrcapitalshub atom aug larson stsorbital skydive parityaccelerations desire an-niversary projects digitalprotection atari temper-atures voyagers zoologyupdated teflonTable 1: Top-20 words extracted by unsupervised LP word cluster algorithm.clustering algorithms, PCK-means (Basu et al., 2004) and MPCK-means (Bilenko et al., 2004) with 200must-link and cannot-link constraints.
Also when comparing these unsupervised and semi-supervisedapproaches previously mentioned, we can find that our unsupervised algorithm consistently achievessignificantly better results.
Therefore, unsupervised LP seems to be a more reasonable algorithm designin terms of word clustering.0.4 0.7 10.80.91PrecisionRecall(b) D20.4 0.7 10.80.91PrecisionRecall(c) D30.4 0.7 10.80.91PrecisionRecall(d) D40.4 0.7 10.80.91PrecisionRecall(e) D50.4 0.7 10.80.91PrecisionRecall(f) D6data3 K?medoids     PCK         MPCK0.4 0.7 10.60.81PrecisionRecall(a) D1Un?LP          Semi?LP   K?meansFigure 2: Precision vs. recall of clustering results on 20NG where D1= {atheism vs. hardware}, D2={atheism vs. hockey}, D3= {atheism vs. space}, D4= {hardware vs. hcokey}, D5= {hardwarevs.
space} and D6= {hockey vs. space}.4.4 Effect of exemplar number eWe now investigate how the number of exemplar (e) for each cluster affects the clustering.
In particular,we are interested in performance under conditions when the number of exemplar grows - which is themotivation for using more than one exemplars to represent a cluster.
From Figure 3, we can observe thatwhen more words are labeled, Semi-LP shows further improvement in F-value.
However, the changesfor PCK-means and MPCK-means are not obvious.
Interestingly, even with the number of labeled datagrowing, Semi-LP still performs worse than Un-LP.
As is shown in Figure 3, Un-LP benefits much frommulti-exemplars (e ?
2).
For D4, Un-LP is capable of achieving 99.58% in F-value when e = 7,obtaining 21.32% improvement over the baseline (e = 1).
This indicates that our algorithm leveragesthe additional exemplars effectively.4.5 Case StudyWe conduct an experiment to illustrate the characteristics of the proposed algorithm in this subsection.We cluster the words in all the four domain datasets and select the most representative words for eachcluster by sorting yi.
In the experiment, we set L = 4 in order to match the class number of thedataset.
Table 1 shows top-20 representative words for each cluster, where the bold words are the ones281 3 5 7 90.50.70.9(a) Results on D11 3 5 7 90.50.70.9(b) Results on D21 3 5 7 90.50.70.9(c) Results on D31 3 5 7 90.50.70.9(d) Results on D41 3 5 7 90.50.70.9(e) Results on D51 3 5 7 90.50.70.9(f) Results on D6Semi?LP Un?LP PCK?Means MPCK?MeansFigure 3: Results on 20NG where X-axis is e, Y-axis is F-value.domain meng configure johansson aug geode isbn bug parityAtheism 100.00% 0 0 0 0 91.67% 89.47% 0Hardware 0 90.91% 0 0 0 0 10.53% 66.67%Hockey 0 9.09% 100.00% 0 0 8.33% 0 0Space 0 0 0 100.00% 100.00% 0 0 33.33%Table 2: Distributions of the incorrect words partitioned by the literal meaning.with correct cluster label inferencing from the literal meaning.
We observe that the accuracy of wordclustering on 20NG is very low (28.75%), which is at variance with the preceding conclusion.
Onereason is that words in 20NG are stemmed, so, from Table 1 it can be clearly seen that there are somenon-English words (e.g., ?mcl?, ?hfd?, ?stl?, etc.)
that don?t have actual meanings.In order to gain further insights into the reasons, the distributions of these incorrect words have beenmade in statistics.
Partial results are shown in Table 2.
From the distributions, we can find that manywords marked in italics in Table 1 have been correctly clustered, although they have nothing to do withcorresponding class in the literal meaning.
Taking these words into account, the accuracy can reach81.25% which demonstrates once again the effectiveness of Un-LP word clustering algorithm.5 ConclusionIn this paper, we propose an unsupervised label propagation algorithm to tackle the problem of wordclustering.
The proposed algorithm uses a similarity graph based on co-occurrence information to en-courage similar words to have similar cluster labels.
One of the advantages of this algorithm is that ituses multi-exemplars to represent a cluster, which can significantly improve the clustering results.AcknowledgementsThis work was supported by Strategic Priority Research Program of Chinese Academy of Sciences(XDA06030602), National Nature Science Foundation of China (No.
61202226), National 863 Program(No.
2011AA010703), IIE Program (No.Y3Z0062201).29ReferencesBasu S, Bilenko M, Mooney R J.
2004.
A probabilistic framework for semi-supervised clustering.
In Proceedingsof SIGKDD, pages 59-68.Bilenko M, Basu S, Mooney R J.
2004.
Integrating constraints and metric learning in semi-supervised clustering.In Proceedings of ICML.Blei D M, Ng A Y, Jordan M I.
2003.
Latent dirichlet allocation.
The Journal of machine Learning research,pages 993-1022.Candito M, Seddah D. 2010.
Parsing word clusters.
In Proceedings of the NAACL HLT 2010 First Workshop onStatistical Parsing of Morphologically-Rich Languages, pages 76-84.Han H, Manavoglu E, Zha H, et al.
2005.
Rule-based word clustering for document metadata extraction.
InProceedings of the 2005 ACM symposium on Applied computing, pages 1049-1053.Jeff M A, Matsoukas S, Schwartz R. 2011.
Improving Low-Resource Statistical Machine Translation with a NovelSemantic Word Clustering Algorithm.
In Proceedings of the MT Summit XIII.Jin P, Sun X, Wu Y, et al.
2007.
Word clustering for collocation-based word sense disambiguation.
ComputationalLinguistics and Intelligent Text Processing, pages 267-274.Koo T, Carreras X, Collins M. 2008.
Simple semi-supervised dependency parsing.
In Proceedings of ACL-HLT,pages 595-603.Krause A, Gomes R G. 2010.
Budgeted nonparametric learning from data streams.
In Proceedings of ICML,pages 391-398.Momtazi S, Klakow D. 2009.
A word clustering approach for language model-based sentence retrieval in questionanswering systems.
In Proceedings of CIKM, pages 1911-1914.Sagae K, Gordon A S. 2009.
Clustering words by syntactic similarity improves dependency parsing of predicate-argument structures.
In Proceedings of the 11th International Conference on Parsing Technologies, pages 192-201.Sun A, Grishman R, Sekine S. 2011.
Semi-supervised relation extraction with large-scale word clustering.
InProceedings of ACL, pages 521-529.Turian J, Ratinov L, Bengio Y.
2010.
Word representations: a simple and general method for semi-supervisedlearning.
In Proceedings of ACL, pages 384-394.Uszkoreit J, Brants T. 2008.
Distributed Word Clustering for Large Scale Class-Based Language Modeling inMachine Translation.
In Proceedings of ACL, pages 755-762.Wu Q, Ye Y, Ng M, et al.
2010.
Exploiting word cluster information for unsupervised feature selection Trends inArtificial Intelligence, pages 292-303.Zhu X, Ghahramani Z.
2002.
Learning from labeled and unlabeled data with label propagation.
Technical ReportCMU-CALD-02-107, Carnegie Mellon University.Zhu X, Ghahramani Z, Lafferty J.
2003.
Semi-supervised learning using gaussian fields and harmonic functions.In Proceedings of ICML, pages 912-919.30
