Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 55?64,Sofia, Bulgaria, August 8 2013. c?2013 Association for Computational LinguisticsTowards a Tool for Interactive Concept Building for Large Scale Analysisin the HumanitiesAndre Blessing1 Jonathan Sonntag2 Fritz Kliche3Ulrich Heid3 Jonas Kuhn1 Manfred Stede21Institute for Natural Language ProcessingUniversitaet Stuttgart, Germany2Institute for Applied Computational LinguisticsUniversity of Potsdam, Germany3Institute for Information Science and Natural Language ProcessingUniversity of Hildesheim, GermanyAbstractWe develop a pipeline consisting of var-ious text processing tools which is de-signed to assist political scientists in find-ing specific, complex concepts withinlarge amounts of text.
Our main focus isthe interaction between the political scien-tists and the natural language processinggroups to ensure a beneficial assistance forthe political scientists and new applicationchallenges for NLP.
It is of particular im-portance to find a ?common language?
be-tween the different disciplines.
Therefore,we use an interactive web-interface whichis easily usable by non-experts.
It inter-faces an active learning algorithm whichis complemented by the NLP pipeline toprovide a rich feature selection.
Politicalscientists are thus enabled to use their ownintuitions to find custom concepts.1 IntroductionIn this paper, we give examples of how NLP meth-ods and tools can be used to provide support forcomplex tasks in political sciences.
Many con-cepts of political science are complex and faceted;they tend to come in different linguistic realiza-tions, often in complex ones; many concepts arenot directly identifiable by means of (a small setof) individual lexical items, but require some in-terpretation.Many researchers in political sciences eitherwork qualitatively on small amounts of data whichthey interpret instance-wise, or, if they are in-terested in quantitative trends, they use compara-tively simple tools, such as keyword-based searchin corpora or text classification on the basis ofterms only; this latter approach may lead to im-precise results due to a rather unspecific search aswell as semantically invalid or ambigious searchwords.
On the other hand, large amounts of e.g.news texts are available, also over longer periodsof time, such that e.g.
tendencies over time canbe derived.
The corpora we are currently workingon contain ca.
700,000 articles from British, Irish,German and Austrian newspapers, as well as (yetunexplored) material in French.Figure 1 depicts a simple example of a quantita-tive analysis.1 The example shows how often twoterms, Friedensmission(?peace operation?
), andAuslandseinsatz(?foreign intervention?)
are usedin the last two decades in newspaper texts aboutinterventions and wars.
The long-term goal of theproject is to provide similar analysis for complexconcepts.
An example of a complex concept isthe evocation of collective identities in politicalcontexts, as indirect in the news.
Examples forsuch collective identities are: the Europeans, theFrench, the Catholics.The objective of the work we are going to dis-cuss in this paper is to provide NLP methods andtools for assisting political scientists in the ex-ploration of large data sets, with a view to both,a detailed qualitative analysis of text instances,and a quantitative overview of trends over time,at the level of corpora.
The examples discussedhere have to do with (possibly multiple) collectiveidentities.
Typical context of such identities tendto report communication, as direct or as indirectspeech.
Examples of such contexts are given in 1.
(1) DieTheEuropa?erEuropeanswu?rdenwoulddietheLu?ckegapfu?llen,fill,1The figure shows a screenshot of our web-basedprototype.55Figure 1: The screenshot of our web-based system shows a simple quantitative analysis of the frequencyof two terms in news articles over time.
While in the 90s the term Friedensmission (peace operation) waspredominant a reverse tendency can be observed since 2001 with Auslandseinsatz (foreign intervention)being now frequently used.sagtesaidRu?he.Ru?he.,,The Europeans would fill the gap, Ru?he said.
?The tool support is meant to be semi-automatic,as the automatic tools propose candidates thatneed to be validated or refused by the political sci-entists.We combine a chain of corpus processing toolswith classifier-based tools, e.g.
for topic clas-sifiers, commentary/report classifiers, etc., makethe tools interoperable to ensure flexible data ex-change and multiple usage scenarios, and we em-bed the tool collection under a web (service) -based user interface.The remainder of this paper is structured as fol-lows.
In section 2, we present an outline of the ar-chitecture of our tool collection, and we motivatethe architecture.
Section 3 presents examples ofimplemented modules, both from corpus process-ing and search and retrieval of instances of com-plex concepts.
We also show how our tools are re-lated to the infrastructural standards in use in theCLARIN community.
In section 4, we exemplifythe intended use of the methods with case studiesabout steps necessary for identifying evocation:being able to separate reports from comments, andstrategies for identifying indirect speech.
Section6 is devoted to a conclusion and to the discussionof future work.2 Project GoalsA collaboration between political scientists andcomputational linguists necessarily involves find-ing a common language in order to agree onthe precise objectives of a project.
For exam-ple, social scientists use the term codebook formanual annotations of text, similar to annotationschemes or guidelines in NLP.
Both disciplinesshare methodologies of interactive text analysiswhich combine term based search, manual an-notation and learning-based annotation of largeamounts of data.
In this section, we give a brief56summary of the goals from the perspective of eachof the two disciplines, and then describe the textcorpus that is used in the project.
Section 3 willdescribe our approach to devising a system archi-tecture that serves to realize the goals.2.1 Social Science Research IssueGiven the complexity of the underlying researchissues (cf.
Section 1) and the methodological tra-dition of manual text coding by very well-trainedannotators in the social science and particular inpolitical science, our project does not aim at anyfully-automatic solution for empirical issues in po-litical science.
Instead, the goal is to provide asmuch assistance to the human text analyst as possi-ble, by means of a workbench that integrates manytasks that otherwise would have to be carried outwith different software tools (e.g., corpus prepro-cessing, KWIC searches, statistics).
In our project,the human analyst is concerned specifically withmanifestations of collective identities in newspa-per texts on issues of war and military interven-tions: who are the actors in political crisis man-agement or conflict?
How is this perspective ofresponsible actors characterized in different news-papers (with different political orientation; in dif-ferent countries)?
The analyst wants to find doc-uments that contain facets of such constellations,which requires search techniques involving con-cepts on different levels of abstraction, rangingfrom specific words or named entities (which mayappear with different names in different texts) toevent types (which may be realized with differentverb-argument configurations).
Thus the text cor-pus should be enriched with information relevantto such queries, and the workbench shall providea comfortable interface for building such queries.Moreover, various types and (possibly concurrent)layers of human annotations have to complementthe automatic analysis, and the manual annota-tion would benefit from automatic control of code-book2 compliance and the convergence of codingdecisions.2.2 Natural Language Processing ResearchIssueLarge collections of text provide an excellent op-portunity for computational linguists to scale theirmethods.
In the scenario of a project like ours, thisbecomes especially challenging, because standard2or, in NLP terms: annotation scheme.automatic analysis components have to be com-bined with manual annotation or interactive inter-vention of the human analyst.In addition to this principled challenge, theremay be more mundane issues resulting from pro-cessing corpora whose origin stretches over manyyears.
In our case, the data collection phase coin-cided with a spelling reform in German-speakingcountries.
Many aspects of spelling changed twice(in 1996 and in 2006), and thus it is the responsi-bility of the NLP branch of the project to providean abstraction over such changes and to enable to-day?s users to run a homogeneous search over thetexts using only the current spelling.
While thismight be less important for generic web search ap-plications, it is of great importance for our project,where the overall objective is a combination ofquantitative and qualitative text analysis.In our processing chain, we first need to harmo-nize the data formats so that the processing toolsoperate on a common format.
Rather than defin-ing these from scratch, we aim at compatibilitywith the standardization efforts of CLARIN3 andDARIAH4, two large language technology infras-tructure projects in Europe that in particular targeteHumanities applications.
One of the objectivesis to provide advanced tools to discover, explore,exploit, annotate, analyse or combine textual re-sources.
In the next section we give more detailsabout how we interact which the CLARIN-D in-frastructure (Boehlke et al 2013).3 ArchitectureThe main goal is to provide a web-based user-interface to the social scientist to avoid any soft-ware installation.
Figure 2 presents the workflowof the different processing steps in this project.The first part considers format issues that occurif documents from different sources are used.
Themain challenge is to recognize metadata correctly.Date and source name are two types of metadatawhich are required for analyses in the social sci-ences.
But also the separation of document con-tent (text) and metadata is important to ensure thatonly real content is processed with the NLP meth-ods.
The results are stored in a repository whichuses a relational database as a back-end.
All fur-ther modules are used to add more annotations tothe textual data.
First a complex linguistic pro-3http://www.clarin.eu/4http://www.dariah.eu/57cessing chain is used to provide state-of-the-artcorpus linguistic annotations (see Section 3.2 fordetails).
Then, to ensure that statistics over oc-currence counts of words, word combinations andconstructions are valid and not blurred by the mul-tiple presence of texts or text passages in the cor-pus, we filter duplicates.
Duplicates can occurif our document set contains the same documenttwice or if two documents are very similar, e.g.they differ in only one sentence.Raw documentsRepository:MetadataStructural dataTextual data Topic filterDuplicate filterLinguistic analysisSentence splitter TokenizerWeb-basedUserinterfaceTagger ParserCoref NERImportExploration WorkbenchConcept detectionComplex Concept BuilderFigure 2: Overview of the complete processingchain.We split the workflow for the user into twoparts: The first part is only used if the user im-ports new data into the repository.
For that hecan use the exploration workbench (Section 3.1).Secondly, all steps for analyzing the data are donewith the Complex Concept Builder (Section 3.2).3.1 Exploration WorkbenchFormal corpus inhomogeneity (e.g.
various dataformats and inconsistent data structures) are a ma-jor issue for researchers working on text corpora.The web-based ?Exploration Workbench?
allowsfor the creation of a consistent corpus from vari-ous types of data and prepares data for further pro-cessing with computational linguistic tools.
Theworkbench can interact with to existing computa-tional linguistic infrastructure (e.g.
CLARIN) andprovides input for the repository also used by theComplex Concept Builder.The workbench converts several input formats(TXT, RTF, HTML) to a consistent XML repre-sentation.
The conversion tools account for differ-ent file encodings and convert input files to Uni-code (UTF-8).
We currently work on newspa-per articles wrapped with metadata.
Text miningcomponents read out those metadata and identifytext content in the documents.
Metadata appearat varying positions and in diverse notations, e.g.for dates, indications of authors or newspaper sec-tions.
The components account for these varia-tions and convert them to a consistent machinereadable format.
The extracted metadata are ap-pended to the XML representation.
The result-ing XML is the starting point for further compu-tational linguistic processing of the source docu-ments.The workbench contains a tool to identify textduplicates and semi-duplicates via similarity mea-sures of pairs of articles (Kantner et al 2011).The method is based on a comparison of 5-grams,weighted by significance (tf-idf measure (Saltonand Buckley, 1988)).
For a pair of documents ityields a value on a ?similarity scale?
ranging from0 to 1.
Values at medium range (0.4 to 0.8) areconsidered semi-duplicates.Data cleaning is important for the data-drivenstudies.
Not only duplicate articles have a nega-tive impact, also articles which are not of interestfor the given topic have to be filtered out.
Thereare different approaches to classify articles into arange of predefined topics.
In the last years LDA(Blei et al 2003; Niekler and Ja?hnichen, 2012)is one of the most successful methods to find top-ics in articles.
But for social scientists the cate-gories typically used in LDA are not sufficient.
Wefollow the idea of Dualist (Settles, 2011; Settlesand Zhu, 2012) which is an interactive method forclassification.
The architecture of Dualist is basedon MALLET (McCallum, 2002) which is easilyintegrable into our architecture.
Our goal is todesign the correct feature to find relevant articlesfor a given topic.
Word features are not sufficientsince we have to model more complex features (cf.Section 2.1).The workbench is not exclusively geared to thedata of the current project.
We chose a modularset-up of the tools of the workbench and provideuser-modifiable templates for the extraction of var-ious kinds of metadata, in order to keep the work-bench adaptable to new data and to develop toolssuitable for data beyond the scope of the currentcorpus.583.2 Complex Concept BuilderA central problem for political scientists who in-tend to work on large corpora is the linguistic va-riety in the expression of technical terms and com-plex concepts.
An editorial or a politician citedin a news item can mobilize a collective identitywhich can be construed from e.g.
regional or so-cial affiliation, nationality or religion.
A reason-able goal in the context of the search for collec-tive identity evocation contexts is therefore to findall texts which (possibly) contain collective iden-tities.
Moreover, while we are training our inter-active tools on a corpus on wars and military in-terventions the same collective identities might beexpressed in different ways in a corpus i.e.
on theEurocrisis.From a computational point of view, many dif-ferent tools need to be joined to detect interest-ing texts.
An example application could be a casewhere a political scientist intends to extract news-paper articles that cite a politician who tries torally support for his political party.
In order todetect such text, we need a system to identify di-rect and indirect speech and a sentiment system todetermine the orientation of the statement.
Thesesystems in turn need various kinds of preprocess-ing starting from tokenization over syntactic pars-ing up to coreference resolution.
The ComplexConcept Builder is the collection of all these sys-tems with the goal to assist the political scientists.So far, the Complex Concept Builder imple-ments tokenization (Schmid, 2009), lemmatisation(Schmid, 1995), part-of-speech tagging (Schmidand Laws, 2008), named entity detection (Faruquiand Pado?, 2010), syntactical parsing (Bohnet,2010), coreference analysis for German (Lappinand Leass, 1994; Stuckardt, 2001), relation extrac-tion (Blessing et al 2012) and sentiment analysisfor English (Taboada et al 2011).It is important for a researcher of the humanitiesto be able to adapt existing classification systemsaccording to his own needs.
A common procedurein both, NLP and political sciences, is to annotatedata.
Therefore, one major goal of the project andthe Complex Concept Builder is to provide ma-chine learning systems with a wide range of pos-sible features ?
including high level informationlike sentiment, text type, relations to other texts,etc.
?
that can be used by non-experts for semi-automatic annotation and text selection.
Activelearning is used to provide immediate results thatcan then be improved continuously.
This aspectof the Complex Concept Builder is especially im-portant because new or adapted concepts that maybe looked for can be found without further help ofnatural language processing experts.3.3 ImplementationWe decided to use a web-based platform for oursystem since the social scientist needs no softwareinstallation and we are independent of the usedoperating system.
Only a state-of-the-art web-browser is needed.
On the server side, we use atomcat installation that interacts with our UIMApipeline (Ferrucci and Lally, 2004).
A HTML-rendering component designed in the project (andparametrizable) allows for a flexible presentationof the data.
A major issue of our work is interac-tion.
To solve this, we use JQuery and AJAX todynamically interact between client- and server-side.4 Case StudyIn this section we explore the interaction betweenvarious sub-systems and how they collaborate tofind complex political concepts.
The followingSection 4.1 describes the detection of direct andindirect speech and its evaluation follows in Sec-tion 4.2.
Section 4.3 is a general exploration of afew selected sub-systems which require, or benefitfrom direct and indirect speech.
Finally, Section4.4 discusses a specific usage scenario for indirectspeech.4.1 Identifying Indirect SpeechThe Complex Concept Builder provides analy-ses on different linguistic levels (currently mor-phosyntax, dependency syntax, named entities) ofannotation.
We exploit this knowledge to identifyindirect speech along with a mentioned speaker.Our indirect speech recognizer is based on threeconditions: i) Consider all sentences that containat least one word which is tagged as subjunctive(i.e.
?*.SUBJ?)
by the RFTagger.
ii) This verbhas to be a direct successor of another verb in thedependency tree.
iii) This verb needs to have asubject.Figure 3 depicts the dependency parse tree ofsentence 2.
(2) Der Einsatz werde wegen der Risiken fu?r dieunbewaffneten Beobachter ausgesetzt, teilte59EinsatzmissiontheDer,,ausgesetztstoppedwerdewegenbecause ofRisikorisksteilteinformedwill be Missionschefhead of missionMoodMoodRobertRobertmitamSaturdaySamstagon..SBOCVFIN.Aux.3.Sg.Pres.SubjVFIN.Full.3.Sg.Past.IndRFTagsFigure 3: Dependency parse of a sentence thatcontains indirect speech (see Sentence 2).Missionschef Robert Mood am Samstag mit.The mission will be stopped because of the risks to theunarmed observers, informed Head of Mission RobertMood on Saturday.The speaker of the indirect speech in Sentence2 is correctly identified as Missionschef (Head ofMission) and the corresponding verb is teilte mit(from mitteilen) (to inform).The parsing-based analysis helps to identify thespeaker of the citation which is a necessary in-formation for the later interpretation of the cita-tion.
As a further advantage, such an approachhelps to minimize the need of lexical knowledgefor the identification of indirect speech.
Our er-ror analysis below will show that in some casesa lexicon can help to avoid false positives.
A lexi-con of verbs of communication can easily be boot-strapped by using our approach to identify candi-dates for the list of verbs which then restrict theclassifier in order to achieve a higher precision.4.2 Indirect Speech EvaluationFor a first impression, we present a list of sen-tences which were automatically annotated as pos-itive instances by our indirect speech detector.The sentences were rated by political scientists.Additionally, for each sentence we extracted thespeaker and the used verb of speech.
We man-ually evaluated 200 extracted triples (sentence,speaker, verb of speech): The precision of oursystem is: 92.5%Examples 2, 3 and 4 present good candidateswhich are helpful for further investigations on col-lective identities.
In example 3 Cardinal Lehmannis a representative speaker of the Catholic commu-nity which is a collective identity.
Our extractedsentences accelerate the search for such candidateswhich amounts to looking manually for needles ina haystack.example speaker verb of speech(2) Robert Mood teilte (told)(3) Kardinal Karl Lehmann sagte (said)(4) Sergej Ordzhonikidse sagte (said)(5) Bild (picture) tru?ben (tarnish)(6) sein (be) sein (be)Examples 5 and 6 show problems of our firstapproach.
In this case, the speaker is not a personor an organisation, and the verb is not a verb ofspeech.
(3) Ein Angriffskrieg jeder Art sei ?
sit-tlich verwerflich ?, sagte der Vorsitzendeder Bischoffskonferenz, Kardinal KarlLehmann.Any kind of war of aggression is ?morally reprehen-sible,?
said the chairman of the Bishops?
Conference,Cardinal Karl Lehmann.
(4) Derartige Erkla?rungen eines Staatschefsseien im Rahmen der internationalenBeziehungen inakzeptabel, sagte der UN-Generaldirektor Sergej Ordzhonikidsegestern in Genf.Such statements of heads of states are unacceptable inthe context of international relations, said UN GeneralDirector Sergei Ordzhonikidse in Geneva yesterday.
(5) Wu?rden die Wahlen verschoben, tru?bte sichdas gescho?nte Bild.Would the elections be postponed, the embellished im-age would tarnish.
(6) Dies sei alles andere als einfach, ist aus Of-fizierskreisen zu ho?ren.This is anything but simple, is to hear from militarycircles.60EinsatzEimosheDrhsatzs,uDigaopsgasatzEdidsowshgbcdsatzhsu hdopgddsgDsatzEcihsoEbchsgwsatzfhgdsociwsatzciRsofshksatzfgDDowsd asatzspuciEglsoshlrcDsatzdsDDoMSMMy.SMMy BMSMMyB.SMMyOMSMMyO.SMMyCMSMMyC.SMMyVMSMMyp EdtFEsktEussbctRshwEFigure 4: 10 most used verbs (lemma) in indirectspeech.4.3 Using Indirect SpeechOther modules benefit from the identification ofindirect speech, as can be seen from Sentence 7.The sentiment system assigns a negative polarityof ?2.15 to the sentence.
The nested sentimentsources, as described by (Wiebe et al 2005), ofthis sentence require a) a direct speech with thespeaker ?Mazower?
and b) an indirect speech withthe speaker ?no one?
to be found.5(7) ?There were serious arguments about whatshould happen to the Slavs and Poles in east-ern Europe,?
says Mazower, ?and how manyof them should be sent to the camps and whatproportion could be Germanised .
.
.
No oneever came out and directly said Hitler had gotit wrong, but there was plenty of implied crit-icism through comparisons with the Romanempire.
[...]?6A collective identity evoked in Sentence 7 is?the Germans??
although the term is not explic-itly mentioned.
This collective identity is de-scribed as non-homogeneous in the citation andcan be further explored manually by the politicalscientists.The following are further applications of theidentified indirect speeches a) using the frequencyof speeches per text as a feature for classifica-tion; e.g.
a classification system for news re-ports/commentaries as described in Section 4.4 b)a project-goal is to find texts in which collective5The reported sentiment value for the whole sentence isapplicable only to the direct speech.
The indirect speech (i.e.
?Hitler had got it wrong?)
needs a more fine-grained polarityscore.
Since our Complex Concept Builder is very flexible, itis trivial to score each component separately.6http://www.guardian.co.uk/education/2008/jul/01/academicexperts.highereducationprofileidentities are mobilised by entities of political de-bate (i.e.
persons, organisations, etc.
); the detec-tion of indirect speech is mandatory for any suchanalysis.4.4 Commentary/Report ClassificationA useful distinction for political scientists dealingwith newspaper articles is the distinction betweenarticles that report objectively on events or back-grounds and editorials or press commentaries.We first extracted opinionated and objectivetexts from DeReKo corpus (Stede, 2004; Kupietzet al 2010).
Some texts were removed in order tobalance the corpus.
The balanced corpus contains2848 documents and has been split into a develop-ment and a training and test set.
570 documentswere used for the manual creation of features.
Theremaining 2278 documents were used to train andevaluate classifiers using 10-fold cross-validationwith the WEKA machine learning toolkit (Hall etal., 2009) and various classifiers (cf.
Table 1).The challenge is that the newspaper articlesfrom the training and evaluation corpus come fromdifferent newspapers and, of course, from differ-ent authors.
Commentaries in the yellow presstend to have a very different style and vocabularythan commentaries from broadsheet press.
There-fore, special attention needs to be paid to the in-dependence of the classifier from different authorsand different newspapers.
For this reason, we usehand-crafted features tailored to this problem.
Inreturn, this means omitting surface-form features(i.e.
words themselves).The support vector machine used the SMO al-gorithm (Platt and others, 1998) with a polynomialkernel K(x, y) =< x, y > e with e = 2.
All otheralgorithms were used with default settings.precision recall f-scoreSVM 0.819 0.814 0.813Naive Bayes 0.79 0.768 0.764Multilayer Percep-tron0.796 0.795 0.794Table 1: Results of a 10-fold cross-validation forvarious machine learning algorithms.A qualitative evaluation shows that direct andindirect speech is a problem for the classifier.Opinions voiced via indirect speech should notlead to a classification as ?Commentary?, butshould be ignored.
Additionally, the number of61uses of direct and indirect speech by the author canprovide insight into the intention of the author.
Acommon way to voice one?s own opinion, withouthaving to do so explicitly, is to use indirect speechthat the author agrees with.
Therefore, the numberof direct and indirect speech uses will be addedto the classifier.
First experiments indicate that theinclusion of direct and indirect speech increase theperformance of the classifier.5 Related WorkMany approaches exist to assist social scientists indealing with large scale data.
We discuss somewell-known ones and highlight differences to theapproach described above.The Europe Media Monitor (EMM) (Stein-berger et al 2009) analyses large amounts ofnewspaper articles and assists anyone interested innews.
It allows its users to search for specific top-ics and automatically clusters articles from differ-ent sources.
This is a key concept of the EMM,because it collects about 100, 000 articles in ap-proximately 50 languages per day and it is impos-sible to scan through these by hand.
EMM usersare EU institutions, national institutions of the EUmember states, international organisations and thepublic (Steinberger et al 2009).The topic clusters provide insight into ?hot?topics by simply counting the amount of articlesper cluster or by measuring the amount of news ona specific topic with regards to its normal amountof news.
Articles are also data-mined for geo-graphical information, e.g.
to update in whichgeographical region the article was written andwhere the topic is located.
Social network infor-mation is gathered and visualised as well.Major differences between the EMM and ourapproach are the user group and the domain ofthe corpus.
The complex concepts political sci-entists are interested in are much more nuancedthan the concepts relevant for topic detection andthe construction of social networks.
Additionally,the EMM does not allow its users to look for theirown concepts and issues, while this interactivityis a central contribution of our approach (cf.
Sec-tions 1, 2.1 and 3.2).The CLARIN-D project also provides a web-based platform to create NLP-chains.
It is calledWebLicht (Hinrichs et al 2010), but in its cur-rent form, the tool is not immediately usable forsocial scientists as the separation of metadata andtextual data and the encoding of the data is hardfor non-experts.
Furthermore, WebLicht does notyet support the combination of manual and au-tomatic annotation needed for text exploration inthe social science.
Our approach is based on thewebservices used by WebLicht.
But in contrast toWebLicht, we provide two additional componentsthat simplify the integration (exploration work-bench) and the interpretation (complex conceptbuilder) of the research data.
The former is in-tended, in the medium term, to be made availablein the CLARIN framework.6 Conclusion and OutlookWe developed and implemented a pipeline of var-ious text processing tools which is designed to as-sist political scientists in finding specific, complexconcepts within large amounts of text.
Our casestudies showed that our approach can provide ben-eficial assistance for the research of political sci-entists as well as researcher from other social sci-ences and the humanities.
A future aspect will beto find metrics to evaluate our pipeline.
In recentlystarted annotation experiments on topic classifica-tion Cohen?s kappa coefficient (Carletta, 1996) ismediocre.
It may very well be possible that thecomplex concepts, like multiple collective identi-ties, are intrinsically hard to detect, and the anno-tations cannot be improved substantially.The extension of the NLP pipeline will be an-other major working area in the future.
Examplesare sentiment analysis for German, adding worldknowledge about named entities (e.g.
persons andevents), identification of relations between enti-ties.Finally, all these systems need to be evaluatednot only in terms of f-score, precision and recall,but also in terms of usability for the political scien-tists.
This also includes a detailed investigation ofvarious political science concepts and if they canbe detected automatically or if natural languageprocessing can help the political scientists to de-tect their concepts semi-automatically.
The defini-tion of such evaluation is an open research topic initself.AcknowledgementsThe research leading to these results has beendone in the project eIdentity which is funded fromthe Federal Ministry of Education and Research(BMBF) under grant agreement 01UG1234.62ReferencesDavid M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alcation.
J. Mach.
Learn.Res., 3:993?1022, March.Andre Blessing, Jens Stegmann, and Jonas Kuhn.2012.
SOA meets relation extraction: Less may bemore in interaction.
In Proceedings of the Work-shop on Service-oriented Architectures (SOAs) forthe Humanities: Solutions and Impacts, Digital Hu-manities, pages 6?11.Volker Boehlke, Gerhard Heyer, and Peter Wittenburg.2013.
IT-based research infrastructures for the hu-manities and social sciences - developments, exam-ples, standards, and technology.
it - InformationTechnology, 55(1):26?33, February.Bernd Bohnet.
2010.
Top accuracy and fast depen-dency parsing is not a contradiction.
In Proceedingsof the 23rd International Conference on Computa-tional, pages 89?97.Jean Carletta.
1996.
Assessing agreement on classi-fication tasks: The kappa statistic.
ComputationalLinguistics, 22(2):249?254.Manaal Faruqui and Sebastian Pado?.
2010.
Train-ing and evaluating a german named entity recog-nizer with semantic generalization.
In Proceedingsof KONVENS 2010, Saarbru?cken, Germany.D.
Ferrucci and A. Lally.
2004.
UIMA: an architec-tural approach to unstructured information process-ing in the corporate research environment.
NaturalLanguage Engineering, 10(3-4):327?348.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H Witten.2009.
The weka data mining software: an update.ACM SIGKDD Explorations Newsletter, 11(1):10?18.Erhard W. Hinrichs, Marie Hinrichs, and Thomas Za-strow.
2010.
WebLicht: Web-Based LRT Servicesfor German.
In Proceedings of the ACL 2010 SystemDemonstrations, pages 25?29.Cathleen Kantner, Amelie Kutter, Andreas Hilde-brandt, and Mark Puettcher.
2011.
How to get ridof the noise in the corpus: Cleaning large samplesof digital newspaper texts.
International RelationsOnline Working Paper, 2, July.Marc Kupietz, Cyril Belica, Holger Keibel, and An-dreas Witt.
2010.
The german reference corpusdereko: a primordial sample for linguistic research.In Proceedings of the 7th conference on interna-tional language resources and evaluation (LREC2010), pages 1848?1854.Shalom Lappin and Herbert J Leass.
1994.
An algo-rithm for pronominal anaphora resolution.
Compu-tational linguistics, 20(4):535?561.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://www.cs.umass.edu/ mccallum/mallet.Andreas Niekler and Patrick Ja?hnichen.
2012.
Match-ing results of latent dirichlet alcation for text.In Proceedings of ICCM 2012, 11th InternationalConference on Cognitive Modeling, pages 317?322.Universita?tsverlag der TU Berlin.John Platt et al1998.
Sequential minimal optimiza-tion: A fast algorithm for training support vectormachines.
technical report msr-tr-98-14, MicrosoftResearch.Gerard Salton and Christopher Buckley.
1988.
Term-weighting approaches in automatic text retrieval.
In-formation processing & management, 24(5):513?523.Helmut Schmid and Florian Laws.
2008.
Estima-tion of conditional probabilities with decision treesand an application to fine-grained POS tagging.
InProceedings of the 22nd International Conferenceon Computational Linguistics (Coling 2008), pages777?784, Manchester, UK, August.Helmut Schmid.
1995.
Improvements in part-of-speech tagging with an application to german.
InProceedings of the ACL SIGDAT-Workshop, pages47?50.Helmut Schmid, 2009.
Corpus Linguistics: An In-ternational Handbook, chapter Tokenizing and Part-of-Speech Tagging.
Handbooks of Linguistics andCommunication Science.
Walter de Gruyter, Berlin.Burr Settles and Xiaojin Zhu.
2012.
Behavioral fac-tors in interactive training of text classifiers.
In Pro-ceedings of the 2012 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages563?567.
Association for Computational Linguis-tics.Burr Settles.
2011.
Closing the loop: Fast, inter-active semi-supervised annotation with queries onfeatures and instances.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1467?1478.
Association for Com-putational Linguistics.Manfred Stede.
2004.
The potsdam commentarycorpus.
In Proceedings of the 2004 ACL Work-shop on Discourse Annotation, DiscAnnotation ?04,pages 96?102, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Ralf Steinberger, Bruno Pouliquen, and Erik VanDer Goot.
2009.
An introduction to the europe me-dia monitor family of applications.
In Proceedingsof the Information Access in a Multilingual World-Proceedings of the SIGIR 2009 Workshop, pages 1?8.63Roland Stuckardt.
2001.
Design and enhanced evalua-tion of a robust anaphor resolution algorithm.
Com-putational Linguistics, 27(4):479?506.Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-berly Voll, and Manfred Stede.
2011.
Lexicon-based methods for sentiment analysis.
Computa-tional linguistics, 37(2):267?307.Janyce Wiebe, Theresa Wilson, and Claire Cardie.2005.
Annotating expressions of opinions and emo-tions in language.
Language Resources and Evalu-ation, 39(2-3):165?210.64
