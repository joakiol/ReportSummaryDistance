Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1144?1153, Dublin, Ireland, August 23-29 2014.A Lexicalized Reordering Modelfor Hierarchical Phrase-based Translation1Hailong Cao1, Dongdong Zhang2, Mu Li2, Ming Zhou2 and Tiejun Zhao11Harbin Institute of Technology, Harbin, P.R.
China2Microsoft Research Asia, Beijing, P.R.
China{hailong, tjzhao}@mtlab.hit.edu.cn{Dongdong.Zhang, muli, mingzhou}@microsoft.comAbstractLexicalized reordering model plays a central role in phrase-based statistical machine translation sys-tems.
The reordering model specifies the orientation for each phrase and calculates its probability con-ditioned on the phrase.
In this paper, we describe the necessity and the challenge of introducing such areordering model for hierarchical phrase-based translation.
To deal with the challenge, we propose anovel lexicalized reordering model which is built directly on synchronous rules.
For each target phrasecontained in a rule, we calculate its orientation probability conditioned on the rule.
We test our modelon both small and large scale data.
On NIST machine translation test sets, our reordering modelachieved a 0.6-1.2 BLEU point improvements for Chinese-English translation over a strong baselinehierarchical phrase-based system.1 IntroductionIn statistical machine translation, the problem of reordering source language into the word order of thetarget language remains a central research topic.
Statistical phrase-based translation models (Och andNey, 2004; Koehn et al., 2003) are good at local reordering, or the reordering of words within thephrase, since the order is specified by phrasal translations.
However, phrase-based models remainweak at long-distance reordering, or the reordering of the phrases.
To improve the reordering of thephrases, two types of models have been developed.The first one is lexicalized reordering models (Tillman, 2004; Huang et al., 2005; Al-Onaizan andPapineni, 2006; Nagata et al., 2006; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Gal-ley and Manning, 2008; Cherry et al., 2012) which predict reordering by taking advantage of lexicalinformation.
The model in (Koehn et al., 2007) distinguishes three orientations with respect to the pre-vious and the next phrase?monotone (M), swap (S) and discontinuous (D).
For example, we can ex-tract a phrase pair ?xiayou ||| the lower reach of?
whose orientations with respect to the previous andthe next phrase are D and S respectively, as shown in Figure 1.
Such a model is simple and effective,and has become a standard component of phrase-based systems such as MOSES.Figure 1.
Phrase orientations for Chinese-English translation.The other is a hierarchical phrase-based (HPB) translation model (Chiang, 2007) based on synchro-nous grammar.
In the HPB model, a synchronous grammar rule may contain both terminals (words)and nonterminals (sub-phrases).
The order of terminals and nonterminal are specified by the rule.
ForThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1144example, the translation rule <X xiayou, the lower reach of X > specifies that the translation of subphrase X before ?xiayou?
should be put after ?the lower reach of?.One problem with the HPB model is that the application of a rule is independent of the actual subphrase.
For example, the rule <X xiayou, the lower reach of X > will always swap the translation of Xand ?xiayou?, no matter what is covered by X.
This is an over-generalization problem.
Much work hasbeen done to solve this issue.
For example, Zollmann and Venugopal (2006) annotate non-terminalsby syntactic categories.
He et al.
(2008) proposes maximum entropy models which combine rich con-text information for selecting translation rules during decoding.
Huang et al.
(2010) automatically in-duce a set of latent syntactic categories to annotate nonterminals.
These works alleviate the over-generalization problem by considering the content of X.
In this paper, we try to solve it from an alter-native view by modeling whether the phrases covered by X prefer the order specified by the rule.
Thishas led us to borrow the lexicalized reordering model from the phrase-based model for the HPB model.We propose a novel lexicalized reordering model for hierarchical phrase-based translation andachieved a 0.6-1.2 BLEU point improvements for Chinese-English translation over a strong HPBbaseline system.2 Related workIn this section, we briefly review two types of related work which are a nonterminal-based lexicalizedreordering models and a path-based lexicalized reordering model.
Both of them calculate the orienta-tion for HPB translation.2.1 Nonterminal-based lexicalized reordering modelsXiao et al.
(2011) proposed an orientation model for HPB translation.
The orientation probability of aderivation is calculated as the product of orientation probabilities of all nonterminals except the root.In order to define the relative orders of nonterminals and their adjacent phrase, they expand the align-ment in a rule to include both terminals and nonterminals.
There may be multiple ways to segment arule into phrases; they use the maximum adjacent phrase similar to Galley and Manning (2008).
Theysignificantly outperformed the HPB system on both Chinese-English and German-English translation.Xiao et al.
(2011) use the boundary word feature of nonterminals without considering their internalstructure.
For example, in Figure 1, suppose nonterminal X1 is not the root node and the orientationprobability of X1 will condition on ?zhe, xiayou, this, river?.In this paper, we will consider how the words covered by the nonterminal X1 are reordered.
Ratherthan using ?xiayou?
as a feature to determine the orientation of X1 with respect to the next phrase, wethink the immediately translated source word ?huanghe?
could be more informative through it is noton the boundary of X1 , since ?huanghe?
is the exact starting point from where we search for the nextphrase to translate.Huck et al.
(2013) proposed a very effective phrase orientation model for HPB translation.
Themodel is also based on nonterminal.
They extracted phrase orientation probabilities from word-alignedtraining data for use with hierarchical phrase inventories, and scored orientations in hierarchical de-coding.2.2 Path-based lexicalized reordering modelThe most recent related work is Nguyen and Vogel (2013).
They map a HPB derivation into a discon-tinuous phrase-based translation path in the following two steps:1) Represent each rule as a sequence of phrase pairs and non-terminals.2) The rules?
sequences are used to find the corresponding phrase-based path of a HPB derivationand calculate the phrase-based reordering features.Figure 2.
The phrase-based path of the derivation in Figure 1.1145A phrase-based path is the sequence of phrase pairs, whose source sides covers the source sentencesand whose target sides generated the target sentences from left to right.
For example, the phrase-basedpath of the derivation in Figure 1 is shown in Figure 2.The phrase-based reordering features for the above phrase-based path are:>)is  thisshi, zhe|<(log DPnext ,                      >)of reachlower   thexiayou,|<(log DPprevious ,>)of reachlower   thexiayou,|<(log SPnext , >)river yellow  thehuanghe,|<(log SPprevious .Nguyen and Vogel (2013) achieved significant improvement over both phrase-based and HPB modelson three language pairs respectively.One problem with the above work is that they did not use rules with unaligned source or targetphrases.
Though this can get faster and better Arabic-English translation, it leads to a 0.49 BLEU pointloss for Chinese-English translation.Another problem with path-based model is: there are many forms of HPB rules which we cannotmap into a reasonable sequence of phrase pairs and non-terminals.
We will show this with an examplederivation shown in Figure 3.
The main difference between Figure 3 and Figure 1 is there is such arule <fangzhi X, prevent X from> that a source phrase ?fangzhi?
is aligned with a discontinuous targetphrase ?prevent?from?.
This makes it hard to find the corresponding phrase-based path because wedo not know what is the right order of ?fangzhi ||| prevent?from?
and ?daozei ||| the thieves?
in thediscontinuous phrase-based path.
We face the following dilemmas:?
If ?fangzhi ||| prevent?from?
goes first, then the discontinuous phrase-based path is as shown inFigure 4(a).
On such a path, we will consider the orientation of ?the thieves?
with respect to?breaking in?.
This is unreasonable because ?the thieves?
and ?breaking in?
are not adjacent in thetarget side.
It does not satisfy the definition of the phrase-based reordering model which predictsthe orientation with respect to previous or next adjacent target phrase.?
If ?daozei ||| the thieves?
goes first, then the discontinuous phrase-based path is as shown in Figure4(b).
This is unreasonable because ?The policeman?
and ?the thieves?
are not adjacent on the tar-get side.Figure 3.
Example of Chinese-English translation and its derivation.
(a)                                                                                         (b)Figure 4.
Two discontinuous phrase-based path candidates of the HPB derivation.From the above example, we can see that if a target phrase is aligned to a discontinuous targetphrase in a HPB rule, then it is hard to find a reasonable path whose target sides can generate the tar-get sentence from left to right.11463 Our lexicalized reordering modelRather than mapping a HPB derivation into a discontinuous phrase-based path and applying reorderingmodel built on phrases, we propose a lexicalized reordering model which is built directly on HPBrules.
For each target phrase contained in a HPB rule, we calculate its orientation probability condi-tioned on the rule.
For the example derivation in Figure 3, we represent it by the structure shown in thefollowing figure:Figure 5.
Our representation of the HPB derivation in Figure 3.Different from Figure 4(a) and Figure 4(b) which contain a discontinuous phrase ?prevent?from?, werepresent ?prevent?from?
as two individual target phrases: ?prevent?
and ?from?.
Instead of consid-ering the orientation of ?prevent?from?, we consider the orientation of ?prevent?
and ?from?
respec-tively.
For example, we will consider the orientation of ?prevent?
with respect the previous phrase?the policeman?prevent)(previousO, and the orientation of ?prevent?
with respect the next phrase ?thethieves?
prevent)(nextO .
The probabilities of both prevent)(previousOand prevent)(nextO are conditionedon the rule <fangzhi X, prevent X from>.In Figure 5, every two neighboring target phrases are adjacent in the original target side.
In this way,we can borrow the phrase-based reordering model which calculates the orientation with respect to pre-vious and next adjacent phrase.More formally, we represent a HPB rule in the general form of:???
?,X...XX,X...XX 2211022110 nnnn ttttssssrwhere n is the number of nonterminals.
...n,isi 1?
, is the source phrase which is a continuous sourceword sequences.
...n,iti 1?
, is the target phrase which is a continuous target word sequences.
We use?
to represent the alignment of words and nonterminals in the rule.
Note that is or it can be empty ifthere are adjacent nonterminals or there is nonterminal on the boundary.
The lexicalized reorderingprobability of rule r is defined as the product of each target phrase?s orientation probabilities condi-tioned on the rule r:)|)(()|)((0r,itOPr,itOP inextnextniipreviousprevious?
?In the above equation, each probability is conditioned on the whole rule.
In this way, we avoid theproblem of mapping a HPB derivation into a discontinuous phrase-based path.
There are two ad-vantages for our reordering model:?
It is compatible with HPB rules which contain unaligned phrases.?
It is compatible with HPB rules in which a source phrase is aligned to a discontinuous targetphrase.Actually, our model is compatible with any kind of HPB rules since it is defined on the generalform of rule.Now we describe how to define )( iprevious tO and )( inext tO in the model.
Suppose it  contains ik targetwords and we writeit as )()1-()2()1( ... ii kikiiii wwwwt ?.
Then we define:),()()( )1(1-)1()1( iiipreviousiprevious wwOwOtO ??
,          ),()(( 1)()()( ???
iii kikikinextinext wwOwOtO1147where ),( 1?jj wwO is the orientation of two adjacent target words and is determined as follows:If ( )(1)( 1???
jj wlmwrm)           MwwO jj ??
),( 1;Else if (  )(1)( 1 jj wlmwrm ???)
SwO jj ??
),( 1;Else                                                         DwwO jj ??
),( 1;)(wrm is the position of the right most source word aligned to target word w; )(wlm is the position ofthe left most source word aligned to target word w.Above is our lexicalized reordering model which is built upon HPB rules.
We complete its descrip-tion using an example.
For the rule <fangzhi X, prevent X from>, n=1,0 prevent?
?
?t  and1 from?
?
?t , the lexicalized reordering probability is:( (prevent)|<fangzhi X, prevent X from>,0 ) ( (prevent)|<fangzhi X, prevent X from> ,0)( (from)| f i , r t  fr ,1) ( (from)|<fangzhi X,??
?previous previous next nextprevious previous next nextP O P OO P O  prevent X from> ,1)Note that we calculate the orientation of plain phrase pairs in the same way as for HPB rules.
Wecan represent a phrase pair in the form of ???
?,, 00 tsr , which is a rule that does not contain anynonterminal.
Then we can apply our above model which is general enough to cover both HPB rulesand plain phrase pairs.4 Training and decodingThe training of our model is similar to the reordering model of Moses.
During the standard phrase pairextraction and rule extraction, besides the nonterminal alignment in rules, we also keep the lexicalalignments and orientations.
If a phrase pair or a rule is observed with more than one set of alignment,we only keep the most frequent one and only count the orientations corresponding to the most frequentalignment.Following Moses, we use relative frequency and add 0.5 smoothing technique to estimate the orien-tation probability based on all samples collected from the training corpus.
Generally, given a rule rwith n target phrases, we estimated the reordering probability for eachit as follows:0.5 # ( )( ( )| ) 1.5 #( )?
????
?previous iprevious previous iO t rP O t r, i r,         0 5 ( ( ), )( ( ) | ) 1 5 ( )??
?next inext next i.
# O t rP O t r, i .
# rFor each parallel sentences pair, we add a start and an end mark on both sides.
They are aligned re-spectively.Our phrase pairs and rules are extracted from word aligned parallel sentences.
There are manyphrase pairs and rules which contain unaligned target or source words.
How to deal with them is quiteimportant for our reordering model.
We will describe how to process them in the following two sub-sections.4.1 The processing of unaligned target wordsOur main principle for processing an unaligned word is to: skip it and use the nearest aligned word.For example in Figure 3, the orientation of ?prevent?
with respect to the next phrase is determined by:)  the(prevent,prevent)( OOnext ?If the target word ?the?
is unaligned and ?thieves?
is aligned with ?daozei?, we will define:(prevent) (prevent, the) (prevent, thieves)?
?
?nextO O O MSimilarly, in Figure 1, the orientation of ?the lower reach of?
with respect with ?the yellow river?
isdetermined by O(of, the).
Suppose both ?of?
and ?the?
are unaligned and there are alignments for?reach-xiayou?
and ?yellow-huanghe?, we will have:1148SOO = yellow)(reach, = the)(of,We believe this orientation is consistent with our intuitions.More formally, before we determine the orientation of two adjacent target words ),( qp wwO ?weapply the following processing procedure:While (target wordpw is unaligned) p--;While (target wordqw is unaligned) q++;If all words in a target phraseit  are unaligned, we do not need to consider its orientation since itdoes not trigger any movement along the source words at all.
Actually, it will be skipped when we de-termine the orientation of the previous and next aligned target phrases.
(See also the decoding algo-rithm in Section 4.3)4.2 The processing of unaligned source wordsThe processing of Section 4.1 can guarantee that the orientation is determined based on two alignedtarget words, namelypw and qw ,which must be continuous or separated by unaligned target words.Now we introduce the processing of unaligned source words.
Before we determine the orientationof two target words ),( qp wwO ?we apply the following procedure to modify the position index ofthe left most source word aligned topw and qw respectively:While (theth1)-)(( pwlm  source word is unaligned) --)( pwlm ;While (the thqwlm )1-)((  source word is unaligned) --)( qwlm ;For the example shown in the Figure 6, initially we have 1)( 1 ?wrm and 4)( 4 ?wlm .
Since thesource words3w  and 2w  are unaligned, our procedure will modify the value of )( 4wlm from 4 to 2.Finally, since )(1)( 41 wlmwrm ??
, the orientation of the two phrases marked by rectangular boxes inFigure 6 is:MwwOwwO ??
),(),( 4132Again, we believe this result is consistent with our intuition.Figure 6.
An example of phrases contain unaligned wordsNote that during decoding, both the unaligned source and target words are also processed in thesame way as in the training step.
This makes our lexicalized reordering model consistent.4.3 DecodingNow we introduce how to integrate our reordering model into the HPB system during the standardCYK bottom-up decoding.During decoding, if we just apply a plain phrase, we do not need to consider the orientation at once.It will be triggered when the phrase is used to compose a larger translation hypothesis together withother phrases or rules.We need to calculate the reordering features whenever we apply a HPB rule or a glue rule duringthe CYK decoding.
Generally, given a rule ???
?,X...XX,X...XX 2211022110 nnnn ttttssssr  defined insection 3, we calculate the reordering probability for the span covered by r with algorithm 1.
In thealgorithm, LL(X) represents the lowest rule which covers the left most word of X; LR(X) is the lowest1149rule which covers the right most word of X; Both LL(X) and LR(X) can be found by traversing thederivation tree top to down recursively.
LI(r) is the index of the last target phrase of rule r.As in the example shown in Figure 3, for the rule r2=<X2 jinru, X2 breaking in>, the orientation ofX2 and ?breaking in?
is:(breaking in) (from, breaking)?
?
?previousO O O DThe right most target word of X2 is ?from?, the lowest rule covering ?from?
is r3=<fangzhi X4, preventX4 from> and the index of the last target phrase of r3 is 1.
So the reordering probability is:)1,0, 31 (D|rP)(D|rPprob nextprevious ?
?Note that, for readability, we use the product of probabilities to demonstrate the decoding process.Actually in practice, we use a linear model which sums the weighted log probabilities.prob=1;for (int i=1; i<=n; i++){if (1?it  is not empty and contains aligned words){;0,*;1,*;1))(O|LL(XPprob)i(O|rPprob)(tOOipreviousnextinext????
?
}if (it  is not empty and contains aligned words){);|(*;)(,*);(r,iOPprob))LR(XLI)(O|LR(XPprobtOOpreviousiinextiprevious???
}else if (i<n){//iX  and 1?iX  are continuous//or all words between them is unaligned1??????
( );( );the firs  phra??
?se of ;( );* ,LI( ))????
* ;,0);?????????
?p iq iqpreviousnext p pprevious qrule r LR Xrule r LL Xt rO O tprob P (O | r rprob P (O | ri}}Algorithm 1.
Calculating the reordering probability for a span covered by a rule:???
?,X...XX,X...XX 2211022110 nnnn ttttssssr .As shown in Algorithm 1, the reordering probability depends on the lowest rules which cover theleft/right most word.
Therefore, we keep the lowest rules which cover the left/right most word for eachpartial translation.
If two partial translations are same in everything but differ in the lowest rule, weneed to keep both of them, rather than only keep the one with higher score.
This will increase thecomplexity of the searching.4.4 DiscussionOrientation can be determined based on word, phrase and hierarchical phrase (Galley and Manning,2008).
What we adopt in this paper is word based orientation.
It is based on the following considera-tions:?
Our baseline is a HPB system, which can capture hierarchical orientation.
We use word based ori-entation with the aim to complement the HPB system.?
Word based orientation is consistent during training and decoding; phrase based orientation isprone to inconsistent between training and decoding.Galley and Manning (2008) has pointed out an inconsistency in Moses between training and decod-ing.
Here we would like to note that phrase based orientation depends on phrase segmentation.
Forexample, in Figure 1, the orientation of phrase ?this is?
with respect to next phrase could be either:?
D, if we think the next phrase is ?the lower reach of ?
which is what Figure 1 shows.?
or S, if the next phrase is ?the lower reach of the yellow river?
which can compose a legal phrasepair with ?huanghe xiayou?
according to the standard phrase pair extraction algorithm.1150The decision to adopt word-based orientation makes our work similar with Hayashi et al.
(2010) whoproposed a word-based reordering model for HPB system.
The difference between our work andHayashi et al.
(2010) is: they adopt the reordering model proposed by Tromble and Eisner (2009) forthe preprocessing approach, while we borrow the idea of lexicalized  reordering models which areoriginally proposed for phrase-based machine translation.5 Experiments5.1 Experimental settingsOur baseline system is re-implementation of Hiero, a hierarchical phrase-based system (Chiang,2007).
Besides the standard features of a HPB model, there are six reordering features in our reorder-ing model which are M, S and D with respect to the previous and next phrase respectively.
They areintegrated into the log-linear model of the HPB system.
The Minimum Error Rate Training (MERT)(Och, 2003) algorithm is adopted to tune feature weights for translation systems.We test our reordering model on a Chinese-English translation task.
The NIST evaluation set MT06was used as our development set to tune the feature weights, and the test data are MT04, MT 05 andMT08.
We first conduct experiments by using the FBIS parallel training corpus, and then further testthe effect of our method on a large scale parallel training corpus.Word alignment is performed by GIZA++ (Och and Ney, 2000) in both directions with the defaultsetting.
The language model is a 4-gram model trained with the Xinhua portion of LDC English Gi-gaword Version 3.0 and the English part of the bilingual training data.
Translation performances aremeasured with case-insensitive BLEU4 score (Papineni et al., 2002).5.2 Experimental results on FBIS corpusWe first conduct experiments by using the FBIS parallel corpus to train the model of both the baselineand our lexicalized reordering model.
After pre-processing, the statistics of FBIS corpus is shown intable 1.#sentences #wordsChinese 128832 3016570English 128832 3922816Table 1.
The statistics of FBIS corpusTable 2 summarizes the translation performance.
The first row shows the results of baseline HPBsystem, and the second row shows the results when we integrated our lexicalized reordering model(LRM).
We get 1.2, 0.8 and 0.7 BLEU point improvements over the baseline HPB system on three testsets respectively.MT04 MT05 MT08HPB 33.53 32.97 25.08HPB+LRM 34.71 33.77 25.84Table 2.
Translation performance on the FBIS corpus.5.3 Experimental results on large scale  corpusTo further test the effect of our reordering model, we use a large scale corpus released by LDC.
Thecatalog number of them is LDC2003E07, LDC2003E14, LDC2005T06, LDC2005T10, LDC2005E83,LDC2006E26, LDC2006E34, LDC2006E85 and LDC2006E92.
There are 498K sentence pairs, 12.1MChinese words and 13.8M English words.
Table 3 summarizes the translation performance on thelarge scale of corpus.MT04 MT05 MT08HPB 38.72 37.59 29.03HPB+LRM 39.81 38.24 29.63Table 3.
Translation performance on a large scale parallel corpus.1151Our model is still effective when we train the translation system on large scale data.
We get 1.1, 0.7and 0.6 BLEU point improvements over the baseline HPB system on three test sets respectively.6 Conclusion and future workWe proposed a novel lexicalized reordering model for hierarchical phrase based machine translation.The model is compatible with any kind of HPB rules no matter how complex the alignments are.
Wetested our reordering model on both small and large scale data.
On NIST machine translation test sets,our reordering model achieved a 0.6-1.2 BLEU point improvements for Chinese-English translationover a strong baseline hierarchical phrase-based system.In future work, we will further test our model on other language pairs and compare it with other re-ordering models for HPB translation.AcknowledgmentsWe thank anonymous reviewers for insightful comments.
The work of Hailong Cao is sponsored byMicrosoft Research Asia Star Track Visiting Young Faculty Program.
The work of HIT is also fundedby the project of National Natural Science Foundation of China (No.
61173073) and International Sci-ence & Technology Cooperation Program of China (No.
2014DFA11350).ReferenceYaser Al-Onaizan and Kishore Papineni.
2006.
Distortion Models for Statistical Machine Translation.
In Pro-ceedings of ACL.Colin Cherry, Robert C. Moore and Chris Quirk.
2012.
On Hierarchical Re-ordering and Permutation Parsing forPhrase-based Decoding.
In Proceedings of  NAACL Workshop on SMT.David Chiang.
2007.
Hierarchical Phrase-based Translation.
Computational Linguistics, 33(2):201?228.Michel Galley and Christopher D. Manning.
2008.
A Simple and Effective Hierarchical Phrase Reordering Mod-el.
In Proceedings of EMNLP.Katsuhiko Hayashi, Hajime Tsukada, Katsuhito Sudoh, Kevin Duh and Seiichi Yamamoto.
2010.
HierarchicalPhrase-based Machine Translation with Word-based Reordering Model.
In Proceedings of COLING.Zhongjun He, Qun Liu, Shouxun Lin.
2008.
Improving Statistical Machine Translation using Lexicalized RuleSelection.
In Proceedings of COLING.Liang Huang, Hao Zhang and Daniel Gildea.
2005.
Machine Translation as Lexicalized Parsing with Hooks.
InProceedings of IWPT.Zhongqiang Huang, Martin ?mejrek, and Bowen Zhou.
2010.
Soft Syntactic Constraints for Hierarchical Phrase-based Translation Using Latent Syntactic Distributions.
In Proceedings of EMNLP.Matthias Huck, Joern Wuebker, Felix Rietig, and Hermann Ney.
2013.
A Phrase Orientation Model for Hierar-chical Machine Translation.
In Proceedings of ACL Workshop on SMT.Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne, Christopher Callison-Burch, Marcello Federico, NicolaBertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.. 2007.
Moses: Open Source Toolkit for Statistical Machine Translation.
In ACLdemonstration session.Masaaki Nagata, Kuniko Saito, Kazuhide Yamamoto and Kazuteru Ohashi.
2006.
A Clustered Global PhraseReordering Model for Statistical Machine Translation.
In Proceedings of ACL.Thuylinh Nguyen and Stephan Vogel.
2013.
Integrating Phrase-based Reordering Features into Chart-based De-coder for Machine Translation.
In Proceedings of ACL.Franz Josef Och and Hermann Ney.
2000.
Improved Statistical Alignment Models.
In Proceedings of ACL.Franz Josef Och and Hermann Ney.
2004.
The Alignment Template Approach to Statistical Machine Translation.Computational Linguistics, 30(4):417?449.Franz Josef Och.
2003.
Minimum Error Rate Training in Statistical Machine Translation.
In Proceedings of ACL.1152Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a Method for Automatic Evalua-tion of Machine Translation.
In Proceedings of ACL.Christoph Tillmann.
2004.
A Unigram Orientation Model for Statistical Machine Translation.
In Proceedings ofHLT-NAACL.Roy Tromble, Jason Eisner.
2009.
Learning Linear Ordering Problems for Better Translation.
In Proceedings ofEMNLP.Xinyan Xiao, Jinsong Su, Yang Liu, Qun Liu, and Shouxun Lin.
2011.
An Orientation Model for HierarchicalPhrase-based Translation.
In Proceedings of IALP.Deyi Xiong, Qun Liu, and Shouxun Lin.
2006.
Maximum Entropy Based Phrase Reordering Model for Statisti-cal Machine Translation.
In Proceedings of ACL.Richard Zens and Hermann Ney.
2006.
Discriminative Reordering Models for Statistical Machine Translation.In Proceedings of Workshop on SMT.Andreas Zollmann and Ashish Venugopal.
2006.
Syntax Augmented Machine Translation via Chart Parsing.
InProceedings of NAACL Workshop on SMT.1153
