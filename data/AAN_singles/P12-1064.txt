Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 611?619,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsIterative Viterbi A* Algorithm forK-Best Sequential DecodingZhiheng Huang?, Yi Chang, Bo Long, Jean-Francois Crespo?,Anlei Dong, Sathiya Keerthi and Su-Lin WuYahoo!
Labs701 First Avenue, SunnyvaleCA 94089, USA{zhiheng huang,jfcrespo}@yahoo.com?
{yichang,bolong,anlei,selvarak,sulin}@yahoo-inc.comAbstractSequential modeling has been widely used ina variety of important applications includingnamed entity recognition and shallow pars-ing.
However, as more and more real timelarge-scale tagging applications arise, decod-ing speed has become a bottleneck for exist-ing sequential tagging algorithms.
In this pa-per we propose 1-best A*, 1-best iterative A*,k-best A* and k-best iterative Viterbi A* al-gorithms for sequential decoding.
We showthe efficiency of these proposed algorithms forfive NLP tagging tasks.
In particular, we showthat iterative Viterbi A* decoding can be sev-eral times or orders of magnitude faster thanthe state-of-the-art algorithm for tagging taskswith a large number of labels.
This algorithmmakes real-time large-scale tagging applica-tions with thousands of labels feasible.1 IntroductionSequence tagging algorithms including HMMs (Ra-biner, 1989), CRFs (Lafferty et al, 2001), andCollins?s perceptron (Collins, 2002) have beenwidely employed in NLP applications.
Sequentialdecoding, which finds the best tag sequences forgiven inputs, is an important part of the sequentialtagging framework.
Traditionally, the Viterbi al-gorithm (Viterbi, 1967) is used.
This algorithm isquite efficient when the label size of problem mod-eled is low.
Unfortunately, due to its O(TL2) timecomplexity, where T is the input token size and Lis the label size, the Viterbi decoding can becomeprohibitively slow when the label size is large (say,larger than 200).It is not uncommon that the problem modeledconsists of more than 200 labels.
The Viterbi al-gorithm cannot find the best sequences in tolerableresponse time.
To resolve this, Esposito and Radi-cioni (2009) have proposed a Carpediem algorithmwhich opens only necessary nodes in searching thebest sequence.
More recently, Kaji et al (2010) pro-posed a staggered decoding algorithm, which provesto be very efficient on datasets with a large numberof labels.What the aforementioned literature does not coveris the k-best sequential decoding problem, which isindeed frequently required in practice.
For exampleto pursue a high recall ratio, a named entity recogni-tion system may have to adopt k-best sequences incase the true entities are not recognized at the bestone.
The k-best parses have been extensively stud-ied in syntactic parsing context (Huang, 2005; Paulsand Klein, 2009), but it is not well accommodatedin sequential decoding context.
To our best knowl-edge, the state-of-the-art k-best sequential decodingalgorithm is Viterbi A* 1.
In this paper, we general-ize the iterative process from the work of (Kaji et al,2010) and propose a k-best sequential decoding al-gorithm, namely iterative Viterbi A*.
We show thatthe proposed algorithm is several times or orders ofmagnitude faster than the state-of-the-art in all tag-ging tasks which consist of more than 200 labels.Our contributions can be summarized as follows.
(1) We apply the A* search framework to sequentialdecoding problem.
We show that A* with a properheuristic can outperform the classic Viterbi decod-ing.
(2) We propose 1-best A*, 1-best iterative A*decoding algorithms which are the second and thirdfastest decoding algorithms among the five decod-ing algorithms for comparison, although there is asignificant gap to the fastest 1-best decoding algo-rithm.
(3) We propose k-best A* and k-best iterativeViterbi A* algorithms.
The latter is several times ororders of magnitude faster than the state-of-the-art1Implemented in both CRFPP (http://crfpp.sourceforge.net/)and LingPipe (http://alias-i.com/lingpipe/) packages.611k-best decoding algorithm.
This algorithm makesreal-time large-scale tagging applications with thou-sands of labels feasible.2 Problem formulationIn this section, we formulate the sequential decod-ing problem in the context of perceptron algorithm(Collins, 2002) and CRFs (Lafferty et al, 2001).
Allthe discussions apply to HMMs as well.
Formally, aperceptron model isf(y,x) =T?t=1K?k=1?kfk(yt, yt?1,xt), (1)and a CRFs model isp(y|x) =1Z(x)exp{T?t=1K?k=1?kfk(yt, yt?1,xt)}, (2)where x and y is an observation sequence and a la-bel sequence respectively, t is the sequence position,T is the sequence size, fk are feature functions andK is the number of feature functions.
?k are the pa-rameters that need to be estimated.
They representthe importance of feature functions fk in prediction.For CRFs, Z(x) is an instance-specific normaliza-tion functionZ(x) =?yexp{T?t=1K?k=1?kfk(yt, yt?1,xt)}.
(3)If x is given, the decoding is to find the best y whichmaximizes the score of f(y,x) for perceptron or theprobability of p(y|x) for CRFs.
As Z(x) is a con-stant for any given input sequence x, the decodingfor perceptron or CRFs is identical, that is,argmaxyf(y,x).
(4)To simplify the discussion, we divide the featuresinto two groups: unigram label features and bi-gram label features.
Unigram features are of formfk(yt,xt) which are concerned with the current la-bel and arbitrary feature patterns from input se-quence.
Bigram features are of form fk(yt, yt?1,xt)which are concerned with both the previous and thecurrent labels.
We thus rewrite the decoding prob-lem asargmaxyT?t=1(K1?k=1?1kf1k (yt,xt)+K2?k=1?2kf2k (yt, yt?1,xt)).
(5)For a better understanding, one can inter-pret the term?K1k=1 ?1kf1k (yt,xt) as node yt?sscore at position t, and interpret the term?K2k=1 ?2kf2k (yt, yt?1,xt) as edge (yt?1, yt)?sscore.
So the sequential decoding problem is cast asa max score pathfinding problem2.
In the discussionhereafter, we assume scores of nodes and edges arepre-computed (denoted as n(yt) and e(yt?1, yt)),and we can thus focus on the analysis of differentdecoding algorithms.3 BackgroundWe present the existing algorithms for both 1-bestand k-best sequential decoding in this section.
Thesealgorithms serve as basis for the proposed algo-rithms in Section 4.3.1 1-Best ViterbiThe Viterbi algorithm is a classic dynamic program-ming based decoding algorithm.
It has the computa-tional complexity of O(TL2), where T is the inputsequence size and L is the label size3.
Formally, theViterbi computes ?
(yt), the best score from startingposition to label yt, as follows.maxyt?1(?yt?1 + e(yt?1, yt)) + n(yt), (6)where e(yt?1, yt) is the edge score between nodesyt?1 and yt, n(yt) is the node score for yt.
Notethat the terms ?yt?1 and e(yt?1, yt) take value 0 fort = 0 at initialization.
Using the recursion definedabove, we can compute the highest score at end po-sition T ?
1 and its corresponding sequence.
Therecursive computation of ?yt is denoted as forwardpass since the computing traverses the lattice fromleft to right.
Conversely, the backward pass com-putes ?yt as the follows.maxyt+1(?yt+1 + e(yt, yt+1) + n(yt+1)).
(7)Note that ?yT?1 = 0 at initialization.
The maxscore can be computed using maxy0(?0 + n(y0)).We can use either forward or backward pass tocompute the best sequence.
Table 1 summarizesthe computational complexity of all decoding algo-rithms including Viterbi, which has the complexityof TL2 for both best and worst cases.
Note thatN/A means the decoding algorithms are not applica-ble (for example, iterative Viterbi is not applicableto k-best decoding).
The proposed algorithms (seeSection 4) are highlighted in bold.3.2 1-Best iterative ViterbiKaji et al (Kaji et al, 2010) presented an efficientsequential decoding algorithm named staggered de-coding.
We use the name iterative Viterbi to describe2With the constraint that the path consists of one and onlyone node at each position.3We ignore the feature size terms for simplicity.612this algorithm for the reason that the iterative pro-cess plays a central role in this algorithm.
Indeed,this iterative process is generalized in this paper tohandle k-best sequential decoding (see Section 4.4).The main idea is to start with a coarse latticewhich consists of both active labels and degeneratelabels.
A label is referred to as an active label if itis not grouped (e.g., all labels in Fig.
1 (a) and la-bel A at each position in Fig.
1 (b)), and otherwiseas an inactive label (i.e., dotted nodes).
The new la-bel, which is made by grouping the inactive labels,is referred to as a degenerate label (i.e., large nodescovering the dotted ones).
Fig.
1 (a) shows a latticewhich consists of active labels only and (b) showsa lattice which consists of both active and degener-ate ones.
The score of a degenerate label is the maxscore of inactive labels which are included in the de-generate label.
Similarly, the edge score between adegenerate label z and an active label y?
is the maxedge score between any inactive label y ?
z and y?,and the score of two degenerate labels z and z?
is themax edge score between any inactive label y ?
zand y?
?
z?.
Using the above definitions, the bestsequence derived from a degenerate lattice would bethe upper bound of the sequence derived from theoriginal lattice.
If the best sequence does not includeany degenerate labels, it is indeed the best sequencefor the original lattice.FABCDEFABCDEFABCDEFABCDEFA ABCDEFABCDEFABCDEFBCDEFigure 1: (a) A lattice consisting of active labels only.
(b) A lattice consisting of both active labels and degener-ate ones.
Each position has one active label (A) and onedegenerate label (consisting of B, C. D, E, and F).The pseudo code for this algorithm is shown inAlgorithm 1.
The lattice is initialized to include oneactive label and one degenerate label at each position(see Figure 1 (b)).
Note that the labels are rankedby the probabilities estimated from the training data.The Viterbi algorithm is applied to the lattice to findthe best sequence.
If the sequence consists of ac-tive labels only, the algorithm terminates and returnssuch a sequence.
Otherwise, the lower bound lb4 ofthe active sequence in the lattice is updated and thelattice is expanded.
The lower bound can be initial-ized to the best sequence score using a beam search(with beam size being 1).
After either a forward ora backward pass, the lower bound is assigned with4The maximum score of the active sequences found so far.the best active sequence score best(lattice)5 if theformer is less than the latter.
The expansion of lat-tice ensures that the lattice has twice active labelsas before at a given position.
Figure 2 shows thecolumn-wise expansion step.
The number of activelabels in the column is doubled only if the best se-quence of the degenerate lattice passes through thedegenerate label of that column.Algorithm 1 Iterative Viterbi Algorithm1: lb = best score from beam search2: init lattice3: for i=0;;i++ do4: if i %2 == 0 then5: y = forward()6: else7: y = backward()8: end if9: if y consists of active labels only then10: return y11: end if12: if lb < best(lattice) then13: lb = best(lattice)14: end if15: expand lattice16: end forAlgorithm 2 Forward1: for i=0; i < T; i++ do2: Compute ?
(yi) and ?
(yi) according to Equations (6) and (7)3: if ?
(yi) + ?
(yi) < lb then4: prune yi from the current lattice5: end if6: end for7: Node b = argmaxyT?1 ?
(yT?1)8: return sequence back tracked by b(c)BCDEFBCDEFBCDEFBCDEFA A A ABCDEFBCDEFBCDEFBCDEFA A A ABCDEFBCDEFBCDEFBCDEFA A A A(a) (b)Figure 2: Column-wise lattice expansion: (a) The bestsequence of the initial degenerate lattice, which does notpass through the degenerate label in the first column.
(b)Column-wise expansion is performed and the best se-quence is searched again.
Notice that the active label inthe first column is not expanded.
(c) The final result.Algorithm 2 shows the forward pass in which thenode pruning is performed.
That is, for any node,if the best score of sequence which passes such anode is less than the lower bound lb, such a nodeis removed from the lattice.
This removal is safeas such a node does not have a chance to form anoptimal sequence.
It is worth noting that, if a nodeis removed, it can no longer be added into the lattice.5We do not update the lower bound lb if we cannot find anactive sequence.613This property ensures the efficiency of the iterativeViterbi algorithm.
The backward pass is similar tothe forward one and it is thus omitted.The alternative calls of forward and backwardpasses (in Algorithm 1) ensure the alternative updat-ing/lowering of node forward and backward scores,which makes the node pruning in either forward pass(see Algorithm 2) or backward pass more efficient.The lower bound lb is updated once in each iterationof the main loop in Algorithm 1.
While the forwardand backwards scores of nodes gradually decreaseand the lower bound lb increases, more and morenodes are pruned.The iterative Viterbi algorithm has computationalcomplexity of T and TL2 for best and worst casesrespectively.
This can be proved as follows (Kaji etal., 2010).
At the m-th iteration in Algorithm 1, it-erative Viterbi decoding requires order of T4m timebecause there are 2m active labels (plus one degen-erate label).
Therefore, it has?mi=0 T4i time com-plexity if it terminates at the m-th iteration.
In thebest case in which m = 0, the time complexity is T .In the worst case in which m = dlog2 Le ?
1 (d.e isthe ceiling function which maps a real number to thesmallest following integer), the time complexity isorder of TL2 because?dlog2 Le?1i=0 T4i < 4/3TL2.3.3 1-Best CarpediemEsposito and Radicioni (2009) have proposed anovel 1-best6 sequential decoding algorithm, Car-pediem, which attempts to open only necessarynodes in searching the best sequence in a given lat-tice.
Carpediem has the complexity of TL logL andTL2 for the best and worst cases respectively.
Weskip the description of this algorithm due to spacelimitations.
Carpediem is used as a baseline in ourexperiments for decoding speed comparison.3.4 K-Best ViterbiIn order to produce k-best sequences, it is notenough to store 1-best label per node, as the k-best sequences may include suboptimal labels.
Thek-best sequential decoding gives up this 1-bestlabel memorization in the dynamic programmingparadigm.
It stores up to k-best labels which are nec-essary to form k-best sequences.
The k-best Viterbialgorithm thus has the computational complexity ofKTL2 for both best and worst cases.Once we store the k-best labels per node in a lat-tice, the k-best Viterbi algorithm calls either the for-ward or the backward passes just in the same way asthe 1-best Viterbi decoding does.
We can compute6They did not provide k-best solutions.the k highest score at the end position T ?
1 and thecorresponding k-best sequences.3.5 K-Best Viterbi A*To our best knowledge the most efficient k-best se-quence algorithm is the Viterbi A* algorithm asshown in Algorithm 3.
The algorithm consists of oneforward pass and an A* backward pass.
The forwardpass computes and stores the Viterbi forward scores,which are the best scores from the start to the cur-rent nodes.
In addition, each node stores a backlinkwhich points to its predecessor.The major part of Algorithm 3 describes the back-ward A* pass.
Before describing the algorithm, wenote that each node in the agenda represents a se-quence.
So the operations on nodes (push or pop)correspond to the operations on sequences.
Initially,the L nodes at position T ?
1 are pushed to anagenda.
Each of the L nodes ni, i = 0, .
.
.
, L ?
1,represents a sequence.
That is, node ni representsthe best sequence from the start to itself.
The best ofthe L sequences is the globally best sequence.
How-ever, the i-th best, i = 2, .
.
.
, k, of the L sequencemay not be the globally i-th best sequence.
The pri-ority of each node is set as the score of the sequencewhich is derived by such a node.
The algorithm thengoes to a loop of k. In each loop, the best node ispopped off from the agenda and is stored in a set r.The algorithm adds alternative candidate nodes (orsequences) to the agenda via a double nested loop.The idea is that, when an optimal node (or sequence)is popped off, we have to push to the agenda allnodes (sequences) which are slightly worse than thejust popped one.
The interpretation of slightly worseis to replace one edge from the popped node (se-quence).
The slightly worse sequences can be foundby the exact heuristic derived from the first Viterbiforward pass.Figure 3 shows an example of the push operationsfor a lattice of T = 4, Y = 4.
Suppose an optimalnode 2:B (in red, standing for node B at position 2,representing the sequence of 0:A 1:D 2:B 3:C) ispopped off, new nodes of 1:A, 1:B, 1:C and 0:B,0:C and 0:D are pushed to the agenda according tothe double nested for loop in Algorithm 3.
Eachof the pushed nodes represents a sequence, for ex-ample, node 1:B represents a sequence which con-sists of three parts: Viterb sequence from start to1:B (0:C 1:B), 2:B and forward link of 2:B (3:Cin this case).
All of these pushed nodes (sequences)are served as candidates for the next agenda pop op-eration.The algorithm terminates the loop once it has op-timal k nodes.
The k-best sequences can be de-rived by the k optimal nodes.
This algorithm has614TBCDBCDBCDBCDA A A A31 20Figure 3: Alternative nodes push after popping an opti-mal node.computation complexity of TL2 + TL for both bestand worst cases, with the first term accounting forViterbi forward pass and the second term account-ing for A* backward process.
The bottleneck is thusat the Viterbi forward pass.Algorithm 3K-Best Viterbi A* algorithm1: forward()2: push L best nodes to agenda q3: c = 04: r = {}5: while c < K do6: Node n = q.pop()7: r = r ?
n8: for i = n.t?
1; i ?
0; i??
do9: for j = 0; j < L; j + + do10: if j!
= n.backlink.y then11: create new node s at position i and label j12: s.forwardlink = n13: q.push(s)14: end if15: end for16: n = n.backlink17: end for18: c+ +19: end while20: return K best sequences derived by r4 Proposed AlgorithmsIn this section, we propose A* based sequen-tial decoding algorithms that can efficiently handledatasets with a large number of labels.
In particular,we first propose the A* and the iterative A* decod-ing algorithm for 1-best sequential decoding.
Wethen extend the 1-best A* algorithm to a k-best A*decoding algorithm.
We finally apply the iterativeprocess to the Viterbi A* algorithm, resulting in theiterative Viterbi A* decoding algorithm.4.1 1-Best A*A*(Hart et al, 1968; Russell and Norvig, 1995), asa classic search algorithm, has been successfully ap-plied in syntactic parsing (Klein and Manning, 2003;Pauls and Klein, 2009).
The general idea of A* is toconsider labels yt which are likely to result in thebest sequence using a score f as follows.f(y) = g(y) + h(y), (8)where g(y) is the score from start to the current nodeand h(y) is a heuristic which estimates the scorefrom the current node to the target.
A* uses anagenda (based on the f score) to decide which nodesare to be processed next.
If the heuristic satisfies thecondition h(yt?1) ?
e(yt?1, yt) + h(yt), then h iscalled monotone or admissible.
In such a case, A* isguaranteed to find the best sequence.
We start withthe naive (but admissible) heuristic as followsh(yt) =T?1?i=t+1(maxn(yi) + max e(yi?1, yi)).
(9)That is, the heuristic of node yt to the end is the sumof max edge scores between any two positions andmax node scores per position.
Similar to (Pauls andKlein, 2009) we explore the heuristic in differentcoarse levels.
We apply the Viterbi backward passto different degenerate lattices and use the Viterbibackward scores as different heuristics.
Differentdegenerate lattices are generated from different it-erations of Algorithm 1: The m-th iteration corre-sponds to a lattice of (2m+1)?T nodes.
A largermindicates a more accurate heuristic, which results ina more efficient A* search (fewer nodes being pro-cessed).
However, this efficiency comes with theprice that such an accurate heuristic requires morecomputation time in the Viterbi backward pass.
Inour experiments, we try the naive heuristic and thefollowing values of m: 0, 3, 6 and 9.In the best case, A* expands one node per posi-tion, and each expansion results in the push of allnodes at next position to the agenda.
The search issimilar to the beam search with beam size being 1.The complexity is thus TL.
In the worst case, A*expands every node per position, and each expan-sion results in the push of all nodes at next positionto the agenda.
The complexity thus becomes TL2.4.2 1-Best Iterative A*The iterative process as described in the iterativeViterbi decoding can be used to boost A* algorithm,resulting in the iterative A* algorithm.
For simplic-ity, we only make use of the naive heuristic in Equa-tion (9) in the iterative A* algorithm.
We initializethe lattice with one active label and one degeneratelabel at each position (see Figure 1 (b)).
We then runA* algorithm on the degenerate lattice and get thebest sequence.
If the sequence is active we returnit.
Otherwise we expand the lattice in each iterationuntil we find the best active sequence.
Similar toiterative Viterbi algorithm, iterative A* has the com-plexity of T and TL2 for the best and worst casesrespectively.4.3 K-Best A*The extension from 1-best A* to k-best A* is againdue to the memorization of k-best labels per node.615Table 1: Best case and worst case computational complexity of various decoding algorithms.1-best decoding K-best decodingbest case worst case best case worst casebeam TL TL KTL KTLViterbi TL2 TL2 KTL2 KTL2iterative Viterbi T TL2 N/A N/ACarpediem TL logL TL2 N/A N/AA* TL TL2 KTL KTL2iterative A* T TL2 N/A N/AViterbi A* N/A N/A TL2 +KTL TL2 +KTLiterative Viterbi A* N/A N/A T +KT TL2 +KTLWe use either the naive heuristic (Equation (9)) ordifferent coarse level heuristics by setting m to be 0,3, 6 or 9 (see Section 4.1).
The first k nodes whichare popped off the agenda can be used to back trackthe k-best sequences.
The k-best A* algorithm hasthe computational complexity of KTL and KTL2for best and worst cases respectively.4.4 K-Best Iterative Viterbi A*We now present the k-best iterative Viterbi A* algo-rithm (see Algorithm 4) which applies the iterativeprocess to k-best Viterbi A* algorithm.
The majordifference between 1-best iterative Viterbi A* algo-rithm (Algorithm 1) and this algorithm is that thelatter calls the k-best Vitebi A* (Algorithm 3) afterthe best sequence is found.
If the k-best sequencesare all active, we terminate the algorithm and returnthe k-best sequences.
If we cannot find either thebest active sequence or the k-best active sequences,we expand the lattice to continue the search in thenext iteration.As in the iterative Viterbi algorithm (see Section3.2), nodes are pruned at each position in forwardor backward passes.
Efficient pruning contributessignificantly to speeding up decoding.
Therefore, tohave a tighter (higher) lower bound lb is important.We initialize the lower bound lb with the k-th bestscore from beam search (with beam size being k) atline 1.
Note that the beam search is performed on theoriginal lattice which consists of L active labels perposition.
The beam search time is negligible com-pared to the total decoding time.
At line 16, we up-date lb as follows.
We enumerate the best active se-quences backtracked by the nodes at position T ?
1.If the current lb is less than the k-th active sequencescore, we update the lbwith the k-th active sequencescore (we do not update lb if there are less than k ac-tive sequences).
At line 19, we use the sequencesreturned from Viterbi A* algorithm to update the lbin the same manner.
To enable this update, we re-quest the Viterbi A* algorithm to return k?, k?
> k,sequences (line 10).
A larger number of k?
resultsin a higher chance to find the k-th active sequence,which in turn offers a tighter (higher) lb, but it comeswith the expense of additional time (the backwardA* process takes O(TL) time to return one moresequence).
In experiments, we found the lb updateson line 1 and line 16 are essential for fast decoding.The updating of lb using Viterbi A* sequences (line19) can boost the decoding speed further.
We exper-imented with different k?
values (k?
= nk, where nis an integer) and selected k?
= 2k which results inthe largest decoding speed boost.Algorithm 4K-Best iterative Viterbi A* algorithm1: lb = k-th best (original lattice)2: init lattice3: for i = 0; ; i+ + do4: if i%2 == 0 then5: y = forward()6: else7: y = backward()8: end if9: if y consists of active labels only then10: ys= k-best Viterbi A* (Algorithm 3)11: if ys consists of active sequences only then12: return ys13: end if14: end if15: if lb < k-th best(lattice) then16: lb = k-th best(lattice)17: end if18: if lb < k-th best(ys) then19: lb = k-th best(ys)20: end if21: expand lattice22: end for5 ExperimentsWe compare aforementioned 1-best and k-best se-quential decoding algorithms using five datasets inthis section.5.1 Experimental settingWe apply 1-best and k-best sequential decoding al-gorithms to five NLP tagging tasks: Penn TreeBank(PTB) POS tagging, CoNLL2000 joint POS tag-ging and chunking, CoNLL 2003 joint POS tagging,chunking and named entity tagging, HPSG supertag-ging (Matsuzaki et al, 2007) and a search querynamed entity recognition (NER) dataset.
We used616sections 02-21 of PTB for training and section 23for testing in POS task.
As in (Kaji et al, 2010),we combine the POS tags and chunk tags to formjoint tags for CoNLL 2000 dataset, e.g., NN|B-NP.Similarly we combine the POS tags, chunk tags, andnamed entity tags to form joint tags for CoNLL 2003dataset, e.g., PRP$|I-NP|O.
Note that by such tagjoining, we are able to offer different tag decodings(for example, chunking and named entity tagging)simultaneously.
This indeed is one of the effectiveapproaches for joint tag decoding problems.
Thesearch query NER dataset is an in-house annotateddataset which assigns semantic labels, such as prod-uct, business tags to web search queries.Table 2 shows the training and test sets size (sen-tence #), the average token length of test dataset andthe label size for the five datasets.
POS and su-pertag datasets assign tags to tokens while CoNLL2000 , CoNLL 2003 and search query datasets as-sign tags to phrases.
We use the standard BIO en-coding for CoNLL 2000, CoNLL 2003 and searchquery datasets.Table 2: Training and test datasets size, average tokenlength of test set and label size for five datasets.training # test # token length label sizePOS 39831 2415 23 45CoNLL2000 8936 2012 23 319CoNLL2003 14987 3684 12 443Supertag 37806 2291 22 2602search query 79569 6867 3 323Due to the long CRF training time (days to weekseven for stochastic gradient descent training) forthese large label size datasets, we choose the percep-tron algorithm for training.
The models are averagedover 10 iterations (Collins, 2002).
The training timetakes minutes to hours for all datasets.
We note thatthe selection of training algorithm does not affectthe decoding process: the decoding is identical forboth CRF and perceptron training algorithms.
Weuse the common features which are adopted in previ-ous studies, for example (Sha and Periera, 2003).
Inparticular, we use the unigrams of the current and itsneighboring words, word bigrams, prefixes and suf-fixes of the current word, capitalization, all-number,punctuation, and tag bigrams for POS, CoNLL2000and CoNLL 2003 datasets.
For supertag dataset,we use the same features for the word inputs, andthe unigrams and bigrams for gold POS inputs.
Forsearch query dataset, we use the same features plusgazetteer based features.5.2 ResultsWe report the token accuracy for all datasets to facil-itate comparison to previous work.
They are 97.00,94.70, 95.80, 90.60 and 88.60 for POS, CoNLL2000, CoNLL 2003, supertag, and search query re-spectively.
We note that all decoding algorithms aslisted in Section 3 and Section 4 are exact.
That is,they produce exactly the same accuracy.
The accu-racy we get for the first four tasks is comparable tothe state-of-the-art.
We do not have a baseline tocompare with for the last dataset as it is not pub-licly available7.
Higher accuracy may be achieved ifmore task specific features are introduced on top ofthe standard features.
As this paper is more con-cerned with the decoding speed, the feature engi-neering is beyond the scope of this paper.Table 3 shows how many iterations in averageare required for iterative Viterbi and iterative ViterbiA* algorithms.
Although the max iteration size isbounded to dlog2 Le for each position (for exam-ple, 9 for CoNLL 2003 dataset), the total iterationnumber for the whole lattice may be greater thandlog2 Le as different positions may not expand atthe same time.
Despite the large number of itera-tions used in iterative based algorithms (especiallyiterative Viterbi A* algorithm), the algorithms arestill very efficient (see below).Table 3: Iteration numbers of iterative Viterbi and itera-tive Viterbi A* algorithms for five datasets.POS CoNLL2000 CoNLL2003 Supertag search queryiter Viter 6.32 8.76 9.18 10.63 6.71iter Viter A* 14.42 16.40 15.41 18.62 9.48Table 4 and 5 show the decoding speed (sen-tences per second) of 1-best and 5-best decoding al-gorithms respectively.
The proposed decoding algo-rithms and the largest decoding speeds across differ-ent decoding algorithms (other than beam) are high-lighted in bold.
We exclude the time for feature ex-traction in computing the speed.
The beam searchdecoding is also shown as a baseline.
We note thatbeam decoding is the only approximate decoding al-gorithm in this table.
All other decoding algorithmsproduce exactly the same accuracy, which is usuallymuch better than the accuracy of beam decoding.For 1-best decoding, iterative Viterbi always out-performs other ones.
A* with a proper heuristic de-noted as A* (best), that is, the best A* using naiveheuristic or the values of m being 0, 3, 6 or 9 (seeSection 4.1), can be the second best choice (ex-cept for the POS task), although the gap betweeniterative Viterbi and A* is significant.
For exam-ple, for CoNLL 2003 dataset, the former can de-code 2239 sentences per second while the latter onlydecodes 225 sentences per second.
The iterativeprocess successfully boosts the decoding speed ofiterative Viterbi compared to Viterbi, but it slowsdown the decoding speed of iterative A* compared7The lower accuracy is due to the dynamic nature of queries:many of test query tokens are unseen in the training set.617to A*(best).
This is because in the Viterbi case,the iterative process has a node pruning procedure,while it does not have such pruning in A*(best)algorithm.
Take CoNLL 2003 data as an exam-ple, the removal of the pruning slows down the 1-best iterative Viterbi decoding from 2239 to 604sentences/second.
Carpediem algorithm performspoorly in four out of five tasks.
This can be ex-plained as follows.
The Carpediem implicitly as-sumes that the node scores are the dominant factorsto determine the best sequence.
However, this as-sumption does not hold as the edge scores play animportant role.For 5-best decoding, k-best Viterbi decoding isvery slow.
A* with a proper heuristic is still slow.For example, it only reaches 11 sentences per secondfor CoNLL 2003 dataset.
The classic Viterbi A* canusually obtain a decent decoding speed, for example,40 sentences per second for CoNLL 2003 dataset.The only exception is supertag dataset, on which theViterbi A* decodes 0.1 sentence per second whilethe A* decodes 3.
This indicates the scalability is-sue of Viterbi A* algorithm for datasets with morethan one thousand labels.
The proposed iterativeViterbi A* is clearly the winner.
It speeds up theViterbi A* to factors of 4, 7, 360, and 3 for CoNLL2000, CoNLL 2003, supertag and query search datarespectively.
The decoding speed of iterative ViterbiA* can even be comparable to that of beam search.Figure 4 shows k-best decoding algorithms de-coding speed with respect to different k values forCoNLL 2003 data .
The Viterbi A* and iterativeViterbi A* algorithms are significantly faster thanthe Viterbi and A*(best) algorithms.
Although theiterative Viterbi A* significantly outperforms theViterbi A* for k < 30, the speed of the former con-verges to the latter when k becomes 90 or larger.This is expected as the k-best sequences span overthe whole lattice: the earlier iteration in iterativeViterbi A* algorithm cannot provide the k-best se-quences using the degenerate lattice.
The over-head of multiple iterations slows down the decodingspeed compared to the Viterbi A* algorithm.l l l l l l l l l l10 20 30 40 50 60 70 80 90 100020406080100120140160180200ksentences/second l ViterbiA*(best)Viterbi A*iterative Viterbi A*Figure 4: Decoding speed of k-best decoding algorithmsfor various k for CoNLL 2003 dataset.6 Related workThe Viterbi algorithm is the only exact algorithmwidely adopted in the NLP applications.
Espositoand Radicioni (2009) proposed an algorithm whichopens necessary nodes in a lattice in searching thebest sequence.
The staggered decoding (Kaji et al,2010) forms the basis for our work on iterative baseddecoding algorithms.
Apart from the exact decod-ing, approximate decoding algorithms such as beamsearch are also related to our work.
Tsuruoka andTsujii (2005) proposed easiest-first deterministic de-coding.
Siddiqi and Moore (2005) presented the pa-rameter tying approach for fast inference in HMMs.A similar idea was applied to CRFs as well (Cohn,2006; Jeong, 2009).
We note that the exact algo-rithm always guarantees the optimality which can-not be attained in approximate algorithms.In terms of k-best parsing, Huang and Chiang(2005) proposed an efficient algorithm which is sim-ilar to the k-best Viterbi A* algorithm presented inthis paper.
Pauls and Klein (2009) proposed an algo-rithm which replaces the Viterbi forward pass withan A* search.
Their algorithm optimizes the Viterbipass, while the proposed iterative Viterbi A* algo-rithm optimizes both Viterbi and A* passes.This paper is also related to the coarse to finePCFG parsing (Charniak et al, 2006) as the degen-erate labels can be treated as coarse levels.
How-ever, the difference is that the coarse-to-fine parsingis an approximate decoding while ours is exact one.In terms of different coarse levels of heuristic usedin A* decoding, this paper is related to the work ofhierarchical A* framework (Raphael, 2001; Felzen-szwalb et al, 2007).
In terms of iterative process,this paper is close to (Burkett et al, 2011) as bothexploit the search-and-expand approach.7 ConclusionsWe have presented and evaluated the A* and itera-tive A* algorithms for 1-best sequential decoding inthis paper.
In addition, we proposed A* and iterativeViterbi A* algorithm for k-best sequential decoding.K-best Iterative A* algorithm can be several timesor orders of magnitude faster than the state-of-the-art k-best decoding algorithm.
It makes real-timelarge-scale tagging applications with thousands oflabels feasible.AcknowledgmentsWe wish to thank Yusuke Miyao and Nobuhiro Kajifor providing us the HPSG Treebank data.
We aregrateful for the invaluable comments offered by theanonymous reviewers.618Table 4: Decoding speed (sentences per second) of 1-best decoding algorithms for five datasets.POS CoNLL2000 CoNLL2003 supertag query searchbeam 7252 1381 1650 395 7571Viterbi 2779 51 41 0.19 443iterative Viterbi 5833 972 2239 213 6805Carpediem 2638 14 20 0.15 243A* (best) 802 131 225 8 880iterative A* 1112 84 109 3 501Table 5: Decoding speed (sentences per second) of 5-best decoding algorithms for five datasets.POS CoNLL2000 CoNLL2003 supertag query searchbeam 2760 461 592 75 4354Viterbi 19 0.41 0.25 0.12 3.83A* (best) 205 4 11 3 92Viterbi A* 1266 47 40 0.1 357iterative Viterbi A* 788 200 295 36 1025ReferencesD.
Burkett, D. Hall, and D. Klein.
2011.
Optimal graphsearch with iterated graph cuts.
Proceedings of AAAI.E.
Charniak, M. Johnson, M. Elsner, J. Austerweil, D.Ellis, I. Haxton, C. Hill, R. Shrivaths, J. Moore, M.Pozar, and T. Vu.
2006.
Multi-level coarse-to-finePCFG parsing.
Proceedings of NAACL.T.
Cohn.
2006.
Efficient inference in large conditionalrandom fields.
Proceedings of ECML.M.
Collins.
2002.
Discriminative training methods forhidden Markov models: Theory and experiments withperceptron algorithms.
Proceedings of EMNLP.R.
Esposito and D. P. Radicioni.
2009.
Carpediem:Optimizing the Viterbi Algorithm and Applications toSupervised Sequential Learning.
Journal of MachineLearning Research.P.
Felzenszwalb and D. McAllester.
2007.
The general-ized A* architecture.
Journal of Artificial IntelligenceResearch.P.
E. Hart, N. J. Nilsson, and B. Raphael.
1968.
A For-mal Basis for the Heuristic Determination of MinimumCost Paths.
IEEE Transactions on Systems Scienceand Cybernetics.L.
Huang and D. Chiang.
2005.
Better k-best parsing.Proceedings of the International Workshops on ParsingTechnologies (IWPT).M.
Jeong, C. Y. Lin, and G. G. Lee.
2009.
Efficient infer-ence of CRFs for large-scale natural language data.Proceedings of ACL-IJCNLP Short Papers.N.
Kaji, Y. Fujiwara, N. Yoshinaga, and M. Kitsuregawa.2010.
Efficient Staggered Decoding for Sequence La-beling.
Proceedings of ACL.D.
Klein and C. Manning.
2003.
A* parsing: Fast exactViterbi parse selection.
Proceedings of ACL.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
Proceedings ofICML.T.
Matsuzaki, Y. Miyao, and J. Tsujii.
2007.
EfficientHPSG parsing with supertagging and CFG-filtering.Proceedings of IJCAI.A.
Pauls and D. Klein.
2009.
K-Best A* Parsing.
Pro-ceedings of ACL.L.
R. Rabiner.
1989.
A tutorial on hidden Markov modelsand selected applications in speech recognition.
Pro-ceedings of The IEEE.C.
Raphael.
2001.
Coarse-to-fine dynamic program-ming.
IEEE Transactions on Pattern Analysis and Ma-chine Intelligence.S.
Russell and P. Norvig.
1995.
Artificial Intelligence: AModern Approach.F.
Sha and F. Pereira.
2003.
Shallow parsing with condi-tional random fields.
Proceedings of HLT-NAACL.S.
M. Siddiqi and A. Moore.
2005.
Fast inference andlearning in large-state-space HMMs.
Proceedings ofICML.Y.
Tsuruoka and J. Tsujii.
2005.
Bidirectional in-ference with the easiest-first strategy for tagging se-quence data.
Proceedings of HLT/EMNLP.A.
J. Viterbi.
1967.
Error bounds for convolutionalcodes and an asymptotically optimum decoding algo-rithm.
IEEE Transactions on Information Theory.619
