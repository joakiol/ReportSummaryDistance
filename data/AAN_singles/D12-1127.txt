Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 1389?1398, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsWiki-ly Supervised Part-of-Speech TaggingShen LiComputer & Information ScienceUniversity of Pennsylvaniashenli@seas.upenn.eduJoa?o V. Grac?aL2F INESC-IDLisboa, Portugaljavg@l2f.inesc-id.ptBen TaskarComputer & Information ScienceUniversity of Pennsylvaniataskar@cis.upenn.eduAbstractDespite significant recent work, purely unsu-pervised techniques for part-of-speech (POS)tagging have not achieved useful accuraciesrequired by many language processing tasks.Use of parallel text between resource-rich andresource-poor languages is one source of weaksupervision that significantly improves accu-racy.
However, parallel text is not alwaysavailable and techniques for using it requiremultiple complex algorithmic steps.
In thispaper we show that we can build POS-taggersexceeding state-of-the-art bilingual methodsby using simple hidden Markov models anda freely available and naturally growing re-source, the Wiktionary.
Across eight lan-guages for which we have labeled data to eval-uate results, we achieve accuracy that signifi-cantly exceeds best unsupervised and paralleltext methods.
We achieve highest accuracy re-ported for several languages and show that ourapproach yields better out-of-domain taggersthan those trained using fully supervised PennTreebank.1 IntroductionPart-of-speech categories are elementary buildingblocks that play an important role in many natu-ral language processing tasks, from machine trans-lation to information extraction.
Supervised learn-ing of taggers from POS-annotated training text isa well-studied task, with several methods achievingnear-human tagging accuracy (Ratnaparkhi, 1996;Toutanova et al 2003; Shen et al 2007).
How-ever, while English and a handful of other languagesare fortunate enough to have comprehensive POS-annotated corpora such as the Penn Treebank (Mar-cus et al 1993), most of the world?s languages haveno labeled corpora.
The annotated corpora that doexist were costly to build (Abeille?, 2003), and areoften not freely available or restricted to research-only use.
Furthermore, much of the annotated text isof limited genre, normally focusing on newswire orliterary text.
Performance of treebank-trained sys-tems degrades significantly when applied to new do-mains (Blitzer et al 2006).Unsupervised induction of POS taggers offers thepossibility of avoiding costly annotation, but de-spite recent progress, the accuracy of unsupervisedPOS taggers still falls far behind supervised sys-tems, and is not suitable for most applications (Berg-Kirkpatrick et al 2010; Grac?a et al 2011; Lee etal., 2010).
Using additional information, in the formof tag dictionaries or parallel text, seems unavoid-able at present.
Early work on using tag dictionariesused a labeled corpus to extract all allowed word-tagpairs (Merialdo, 1994), which is quite an unrealis-tic scenario.
More recent work has used a subset ofthe observed word-tag pairs and focused on gener-alizing dictionary entries (Smith and Eisner, 2005;Haghighi and Klein, 2006; Toutanova and Johnson,2007; Goldwater and Griffiths, 2007).
Using corpus-based dictionaries greatly biases the test results, andgives little information about the capacity to gener-alize to different domains.Recent work by Das and Petrov (2011) buildsa dictionary for a particular language by transfer-ring annotated data from a resource-rich languagethrough the use of word alignments in parallel text.1389The main idea is to rely on existing dictionaries forsome languages (e.g.
English) and use parallel datato build a dictionary in the desired language and ex-tend the dictionary coverage using label propaga-tion.
However, parallel text does not exist for manypairs of languages and the proposed bilingual pro-jection algorithms are fairly complex.In this work we use the Wiktionary, a freely avail-able, high coverage and constantly growing dic-tionary for a large number of languages.
We ex-periment with a very simple second-order HiddenMarkov Model with feature-based emissions (Berg-Kirkpatrick et al 2010; Grac?a et al 2011).
We out-perform best current results using parallel text su-pervision across 8 different languages, even whenthe word type coverage is as low as 20%.
Further-more, using the Brown corpus as out-of-domain datawe show that using the Wiktionary produces bet-ter taggers than using the Penn Treebank dictionary(88.5% vs 85.9%).
Our empirical analysis and thenatural growth rate of the Wiktionary suggest thatfree, high-quality and multi-domain POS-taggers fora large number of languages can be obtained by stan-dard and efficient models.The source code, the dictionary mappings andthe trained models described in this work areavailable at http://code.google.com/p/wikily-supervised-pos-tagger/.2 Related WorkThe scarcity of labeled corpora for resource poorlanguages and the challenges of domain adaptationhave led to several efforts to build systems for unsu-pervised POStagging.Several lines of research have addressed the fullyunsupervised POS-tagging task: mutual informationclustering (Brown et al 1992; Clark, 2003) has beenused to group words according to their distributionalcontext.
Using dimensionality reduction on wordcontexts followed by clustering has led to accuracygains (Schu?tze, 1995; Lamar et al 2010).
Sequencemodels, HMMs in particular, have been used to rep-resent the probabilistic dependencies between con-secutive tags.
In these approaches, each observa-tion corresponds to a particular word and each hid-den state corresponds to a cluster.
However, us-ing maximum likelihood training for such modelsdoes not achieve good results (Clark, 2003): max-imum likelihood training tends to result in very am-biguous distributions for common words, in contra-diction with the rather sparse word-tag distribution.Several approaches have been proposed to mitigatethis problem, including Bayesian approaches usingan improper Dirichlet prior to favor sparse modelparameters (Johnson, 2007; Gao and Johnson, 2008;Goldwater and Griffiths, 2007), or using the Poste-rior Regularization to penalize ambiguous posteri-ors distributions of tags given tokens (Grac?a et al2009).
Berg-Kirkpatrick et al(2010) and Grac?a etal.
(2011) proposed replacing the multinomial emis-sion distributions of standard HMMs by maximumentropy (ME) feature-based distributions.
This al-lows the use of features to capture morphological in-formation, and achieves very promising results.
De-spite these improvements, fully unsupervised sys-tems require an oracle to map clusters to true tagsand the performance still fails to be of practical use.In this paper we follow a different line of workwhere we rely on a prior tag dictionary indicating foreach word type what POS tags it can take on (Meri-aldo, 1994).
The task is then, for each word tokenin the corpus, to disambiguate between the possiblePOS tags.
Even when using a tag dictionary, disam-biguating from all possible tags is still a hard prob-lem and the accuracy of these methods is still fall farbehind their supervised counterparts.
The scarcityof large, manually-constructed tag dictionaries ledto the development of methods that try to generalizefrom a small dictionary with only a handful of en-tries (Smith and Eisner, 2005; Haghighi and Klein,2006; Toutanova and Johnson, 2007; Goldwater andGriffiths, 2007), however most previous works buildthe dictionary from the labeled corpus they learn on,which does not represent a realistic dictionary.
Inthis paper, we argue that the Wiktionary can serve asan effective and much less biased tag dictionary.We note that most of the previous dictionarybased approaches can be applied using the Wik-tionary and would likely lead to similar accuracy in-creases that we show in this paper.
For example, thework if Ravi and Knight (2009) minimizes the num-ber of possible tag-tag transitions in the HMM viaa integer program, hence discarding unlikely tran-sitions that would confuse the model.
Models canalso be trained jointly using parallel corpora in sev-1390eral languages, exploiting the fact that different lan-guages present different ambiguities (Snyder et al2008).The Wiktionary has been used extensively forother tasks such as domain specific informationretrieval (Mu?ller and Gurevych, 2009), ontologymatching (Krizhanovsky and Lin, 2009), synonymydetection (Navarro et al 2009), sentiment classifi-cation (Chesley et al 2006).
Recently, Ding (2011)used the Wiktionary to initialize an HMM for Chi-nese POS tagging combined with label propagation.3 The Wiktionary and tagged corporaThe Wiktionary1 is a collaborative project that aimsto produce a free, large-scale multilingual dictio-nary.
Its goal is to describe all words from all lan-guages (currently more than 400) using definitionsand descriptions in English.
The coverage of theWiktionary varies greatly between languages: cur-rently there are around 75 languages for which thereexists more than 1000 word types, and 27 for whichthere exists more than 10,000 word types.
Neverthe-less, the Wiktionary has been growing at a consid-erable rate (see Figure 1), and the number of avail-able words has almost doubled in the last three years.As more people use the Wiktionary, it is likely togrow.
Unlike tagged corpora, the Wiktionary pro-vides natural incentives for users to contribute miss-ing entries and expand this communal resource akinto Wikipedia.
As with Wikipedia, the questions ofaccuracy, bias, consistency across languages, and se-lective coverage are paramount.
In this section, weexplore these concerns by comparing Wiktionary todictionaries derived from tagged corpora.3.1 Labeled corpora and Universal tagsWe collected part-of-speech tagged corpora for9 languages, from CoNLL-X and CoNLL-2007shared tasks on dependency parsing (Buchholz andMarsi, 2006; Nivre et al 2007).
In this work weuse the Universal POS tag set (Petrov et al 2011)that defines 12 universal categories with a relativelystable functional definition across languages.
Thesecategories include NOUN, VERB, ADJ = adjective,ADV = adverb, NUM = number, ADP = adposition,CONJ = conjunction, DET = determiner, PRON =1http://www.wiktionary.org/!"#""$%!&#""$%!'#""$%!(#""$%!)#""$%!*#""$%!+#""$%!,#""$%!!#""$%!-#""$%-"#""$%"%*"""""%&""""""%&*"""""%'""""""%'*"""""%(""""""%(*"""""%)""""""%)*"""""%*""""""%./01&"%2301&"%./41&"%5671&"%5681&"%2691&"%:;31&"%<=>1&"%?@A1&"%B;=1&"%5/71&&%C;D1&&%./01&&%2301&&%./41&&%5671&&%5681&&%2691&&%:;31&&%<=>1&&%?
@A1&&%B;=1&&%5/71&'%C;D1&'%288% -E/79F% 2==60/=4%Figure 1: Growth of the Wiktionary over the last threeyears, showing total number of entries for all languagesand for the 9 languages we consider (left axis).
Wealso show the corresponding increase in average accuracy(right axis) achieved by our model across the 9 languages(see details below).pronoun, PUNC = punctuation, PRT = particle, andX = residual (a category for language-specific cat-egories which defy cross-linguistic classification).We found several small problems with the mapping2which we corrected as follows.
In Spanish, the fine-level tag for date (?w?)
is mapped to universal tagNUM, while it should be mapped to NOUN.
In Dan-ish there were no PRT, NUM, PUNC, or DET tags inthe mapping.
After examining the corpus guidelinesand the mapping more closely, we found that the tagAC (Cardinal numeral) and AO (Ordinal numeral)are mapped to ADJ.
Although the corpus guidelinesindicate the category SsCatgram ?adjective?
that en-compasses both ?normal?
adjectives (AN) as well ascardinal numeral (AC) and ordinal numerals (AO),we decided to tag AC and AO as NUM, since thisassignment better fits the existing mapping.
We alsoreassigned all punctuation marks, which were erro-neously mapped to X, to PUNC and the tag U whichis used for words at, de and som, to PRT.3.2 Wiktionary to Universal tagsThere are a total of 330 distinct POS-type tagsin Wiktionary across all languages which we havemapped to the Universal tagset.
Most of the map-ping was straightforward since the tags used in theWiktionary are in fact close to the Universal tagset.
Some exceptions like ?Initialism?, ?Suffix?2http://code.google.com/p/universal-pos-tags/1391were discarded.
We also mapped relatively rare tagssuch as ?Interjection?, ?Symbol?
to the ?X?
tag.A example of POS tags for several words in theWiktionary is shown in Table 1.
All the mappingsare available at http://code.google.com/p/wikily-supervised-pos-tagger/.3.3 Wiktionary coverageThere are two kinds of coverage of interest: typecoverage and token coverage.
We define type cov-erage as the proportion of word types in the corpusthat simply appear in the Wiktionary (accuracy ofthe tag sets are considered in the next subsection).Token coverage is defined similarly as the portionof all word tokens in the corpus that appear in theWiktionary.
These statistics reflect two aspects ofthe usefulness of a dictionary that affect learning indifferent ways: token coverage increases the densityof supervised signal while type coverage increasesthe diversity of word shape supervision.
At one ex-treme, with 100% word and token coverage, we re-cover the POS tag disambiguation scenario and, onthe other extreme of 0% coverage, we recover theunsupervised POS induction scenario.The type and token coverage of Wiktionary foreach of the languages we are using for evaluationis shown in Figure 2.
We plot the coverage bar forthree different versions of Wiktionary (v20100326,v20110321, v20120320), arranged chronologically.We chose these three versions of the Wiktionarysimply by date, not any other factors like coverage,quality or tagging accuracy.As expected, the newer versions of the Wiktionarygenerally have larger coverage both on type leveland token level.
Nevertheless, even for languageswhose type coverage is relatively low, such as Greek(el), the token level coverage is still quite good(more than half of the tokens are covered).
The rea-son for this is likely the bias of the contributors to-wards more frequent words.
This trend is even moreevident when we break up the coverage by frequencyof the words.
Since the number of words varies fromcorpus to corpus, we normalize the word counts bythe count of the most frequent word(s) in its corpusand group the normalized frequency into three cat-egories labeled as ?low?, ?medium?
and ?high?
andfor each category, we calculate the word type cover-age, shown in Figure 3.Figure 2: Type-level (top) and token-level (bottom) cov-erage for the nine languages in three versions of the Wik-tionary.We also compared the coverage provided by theWiktionary versus the Penn Treebank (PTB) ex-tracted dictionary on the Brown corpus.
Figure 4shows that the Wiktionary provides a greater cover-age for all sections of the Brown corpus, hence beinga better dictionary for tagging English text in gen-eral.
This is also reflected in the gain in accuracy onBrown over the taggers learned from the PTB dic-tionary in our experiments.3.4 Wiktionary accuracyA more refined notion of quality is the accuracy ofthe tag sets for covered words, as measured againstdictionaries extracted from labeled tree bank cor-pora.
We consider word types that are in both theWiktionary (W) and the tree bank dictionaries (T).For each word type, we compare the two tag setsand distinguish five different possibilities:1.
Identical: W = T2.
Superset: W ?
T3.
Subset: W ?
T4.
Overlap: W ?
T 6= ?1392Wiktionary EntriesUniversal POS SetLanguage Word POS DefinitionEnglish today Adverb # In the current [[era]]; nowadays.
{ADV, NOUN}English today Adverb # On the current [[day]] or [[date]].English today Noun # A current day or date.German achtzig Numeral # [[eighty]] {NUM}Swedish SCB Acronym # [[statistiska]] ... {NOUN}Portuguese nessa Contraction # {{contraction ... discard entryTable 1: Examples of constructing Universal POS tag sets from the Wiktionary.!"!!#$%!"!!#$&!"!!#$'!"!!#$(!"!!#$)!!"!
!#$*+$ *,$ ,-$ ,.$ ,/$ 01$ .-$ 21$ /3$-45$ 6,*076$ 8098$Figure 3: Word type coverage by normalized frequency:words are grouped by word count / highest word countratio: low [0, 0.01), medium [0.01, 0.1), high [0.1, 1].5.
Disjoint: W ?
T = ?.In Figure 5, the word types are grouped into thecategories described above.
Most of the tag sets(around 90%) in the Wiktionary are identical to orsupersets of the tree bank tag sets for our nine lan-guages, which is surprisingly accurate.
About 10%of the Wiktionary tag sets are subsets of, partiallyoverlapping with, or disjoint from the tree bank tagsets.
Our learning methods, which assume the giventag sets are correct, may be somewhat hurt by theseword types, as we discuss in Section 5.6.4 ModelsOur basic models are first and second order HiddenMarkov Models (HMM and SHMM).
We also usedfeature-based max-ent emission models with both(HMM-ME and SHMM-ME).
Below, we denote thesequence of words in a sentence as boldface x andthe sequence of hidden states which correspond topart-of-speech tags as boldface y.
To simplify nota-tion, we assume that every tag sequence is prefixed!"!!#$%!"!!#$&!"!!#$'!"!!#$(!"!!#$)!!"!
!#$*$ +$ ,$ -$ .$ /$ 0$ 1$ 2$ 3$ 4$ 5$ 6$ 7$ 8$9+-$ :;<=>?
@AB$Figure 4: PTB vs. Wiktionary type coverage across sec-tions of the Brown corpus.with two conventional start tags y0 = start, y?1 =start, allowing us to write as p(y1|y0, y?1) the ini-tial state probability of the SHMM.The probability of a sentence x along with a par-ticular hidden state sequence y in the SHMM isgiven by:p(x,y) =length(x)?i=1pt(yi | yi?1, yi?2)po(xi | yi),(1)where po(xi | yi) is the probability of observ-ing word xi in state yi (emission probability), andpt(yi | yi?1, yi?2) is the probability of being in stateyi, given two previous states yi?1, yi?2 (transitionprobability).In this work, we compare multinomial and maxi-mum entropy (log-linear) emission models.
Specifi-cally, the max-ent emission model is:po(x|y) =exp(?
?
f(x, y))?x?
exp(?
?
f(x?, y))(2)where f(x, y) is a feature function, x ranges over all1393!"#$!"#%!"#&!"#'!"#(!"#)!"#*!"#+!"#,!"#$!!
"#-.# -/# /0# /1# /2# 34# 10# 54# 26#3-/178.0# 295/:2/4# 29;2/4# <6/:0.5# -32=<314#Figure 5: The Wiktionary vs. tree bank tag sets.
Around90% of the Wiktionary tag sets are identical or subsumetree bank tag sets.
See text for details.word types, and ?
are the model parameters.
We usethe following feature templates:?
Word identity - lowercased word form if theword appears more than 10 times in the corpus.?
Hyphen - word contains a hyphen?
Capital - word is uppercased?
Suffix - last 2 and 3 letters of a word if theyappear in more than 20 different word types.?
Number - word contains a digitThe idea of replacing the multinomial models of anHMM by maximum entropy models has been ap-plied before in different domains (Chen, 2003), aswell as in POS induction (Berg-Kirkpatrick et al2010; Grac?a et al 2011).We use the EM algorithm to learn the models,restricting the tags of each word to those specifiedby the dictionary.
For each tag y, the observa-tions probabilities po(x | y) were initialized ran-domly for every word type that allows tag y accord-ing to the Wiktionary and zero otherwise.
For theM-step in max-ent models, there is no closed formsolution so we need to solve an unconstrained op-timization problem.
We use L-BFGS with Wolfe?srule line search (Nocedal and Wright, 1999).
Wefound that EM achieved higher accuracy across lan-guages compared to direct gradient approach (Berg-Kirkpatrick et al 2010).5 ResultsWe evaluate the accuracy of taggers trained usingthe Wiktionary using the 4 different models: Afirst order Hidden Markov Model (HMM), a sec-ond order Hidden Markov Model (SHMM), a firstorder Hidden Markov Model with Maximum En-tropy emission models (HMM-ME) and a second or-der Hidden Markov Model with Maximum Entropyemission models (SHMM-ME).
For each model weran EM for 50 iterations, which was sufficient forconvergence of the likelihood.
Following previouswork (Grac?a et al 2011), we used a Gaussian priorwith variance of 10 for the max-ent model param-eters.
We obtain hard assignments using posteriordecoding, where for each position we pick the la-bel with highest posterior probability: this producessmall but consistent improvements over Viterbi de-coding.5.1 Upper and lower boundsWe situate our results against several upper boundsthat use more supervision.
We trained the SHMM-ME model with a dictionary built from the train-ing and test tree bank (ALL TBD) and also withtree bank dictionary intersected with the Wiktionary(Covered TBD).
The Covered TBD dictionary ismore supervised than the Wiktionary in the sensethat some of the tag set mismatches of the Wik-tionary are cleaned using the true corpus tags.
Wealso report results from training the SHMM-ME inthe standard supervised fashion, using 50 (50 Sent.
),100 (100 Sent.)
and all sentences (All Sent.
).As a lower bound we include the results for un-supervised systems: a regular HMM model trainedwith EM (Johnson, 2007) and an HMM model usinga ME emission model trained using direct gradient(Berg-Kirkpatrick et al 2010)3.5.2 Bilingual baselinesFinally, we also compare our system against a strongset of baselines that use bilingual data.
These ap-proaches build a dictionary by transferring labeleddata from a resource rich language (English) to a re-source poor language (Das and Petrov, 2011).
Wecompare against two such methods.
The first, pro-jection, builds a dictionary by transferring the pos3Values for these systems where taken from the D&P paper.1394tags from English to the new language using wordalignments.
The second method, D&P, is the cur-rent state-of-the-art system, and runs label propaga-tion on the dictionary resulting from the projectedmethod.
We note that both of these approaches areorthogonal to ours and could be used simultaneouslywith the Wiktionary.5.3 AnalysisTable 2 shows results for the different models acrosslanguages.
We note that the results are not di-rectly comparable since both the Unsupervised andthe Bilingual results use a different setup, using thenumber of fine grained tags for each language as hid-den states instead of 12 (as we do).
This greatly in-creases the degrees of freedom of the model allow-ing it to capture more fine grained distinctions.The first two observations are that using the MEentropy emission model always improves over thestandard multinomial model, and using a second or-der model always performs better.
Comparing withthe work of D&P, we see that our model achievesbetter accuracy on average and on 5 out of 8 lan-guages.The most common errors are due to tag set id-iosyncrasies.
For instance, for English the symbol %is tagged as NUM by our system while in the Penntreebank it is tagged as Noun.
Other common mis-takes for English include tagging to as an adposition(preposition) instead of particle and tagging whichas a pronoun instead of determiner.
In the next sub-sections we analyze the errors in more detail.Finally, for English we also trained the SHMM-ME model using the Celex2 dictionary availablefrom LDC4.
Celex2 coverage for the PTB cor-pus is much smaller than the coverage providedby the Wiktionary (43.8% type coverage versus80.0%).
Correspondingly, the accuracy of the modeltrained using Celex2 is 75.5% compared 87.1%when trained using the Wiktionary.5.4 Performance vs. Wiktionary ambiguityWhile many words overwhelmingly appear with onetag in a given genre, in the Wiktionary a large pro-portion of words are annotated with several tags,even when those are extremely rare events.
Around4http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=LDC96L1435% of word types in English have more than onetag according to the Wiktionary.
This increases thedifficulty of predicting the correct tag as comparedto having a corpus-based dictionary, where wordshave a smaller level of ambiguity.
For example, inEnglish, for words with one tag, the accuracy is 95%(the reason it is not 100% is due to a discrepancy be-tween the Wiktionary and the tree bank.)
For wordswith two possible tags, accuracy is 81% and for threetags, it drops to 63%.5.5 Generalization to unknown wordsComparing the performance of the proposed modelfor words in the Wiktionary against words not inthe Wiktionary, we see an average drop from 89%to 63% for out-of-vocabulary words across nine lan-guages.
Table 2 shows that the average loss of accu-racy between All TBD and Covered TBD of 4.5%(which is due purely to decrease in coverage) islarger than the loss between Covered TBD and thebest Wiktionary model, of 3.2% (which is due to tagset inconsistency).One advantage of the Wiktionary is that it is a gen-eral purpose dictionary and not tailored for a partic-ular domain.
To illustrate this we compared severalmodels on the Brown corpus: the SHMM-ME modelusing the Wiktionary (Wik), against using a modeltrained using a dictionary extracted from the PTBcorpus (PTBD), or trained fully supervised using thePTB corpus (PTB).
We tested all these models on the15 different sections of the Brown corpus.
We alsocompare against a state-of-the-art POS-tagger tagger(ST)5.Figure 6 shows the accuracy results for eachmodel on the different sections.
The fully super-vised SHMM-ME model did not perform as well asthe the Stanford tagger (about 3% behind on aver-age), most likely because of generative vs. discrim-inate training of the two models and feature differ-ences.
However, quite surprisingly, the Wiktionary-tag-set-trained model performs much better not onlythan the PTB-tag-set-trained model but also the su-pervised model on the Brown corpus.5Available at http://nlp.stanford.edu/software/tagger.shtml1395Danish Dutch German Greek English Italian Portuguese Spanish Swedish avg.UnsupervisedHMM 68.7 57.0 75.9 65.8 63.7 62.9 71.5 68.4 66.7HMM-ME 69.1 65.1 81.3 71.8 68.1 78.4 80.2 70.1 73.0BilingualProjection 73.6 77.0 83.2 79.3 79.7 82.6 80.1 74.7 78.8D&P 83.2 79.5 82.8 82.5 86.8 87.9 84.2 80.5 83.4WiktionaryHMM 71.8 80.8 77.1 73.1 85.4 84.6 79.1 83.9 76.7 78.4HMM-ME 82.8 86.1 81.2 80.1 86.1 85.4 83.7 84.6 85.9 83.7SHMM 74.5 81.6 81.2 73.1 85.0 85.2 79.9 84.5 78.7 79.8SHMM-ME 83.3 86.3 85.8 79.2 87.1 86.5 84.5 86.4 86.1 84.8SupervisedCovered TBD 90.1 91.4 89.4 79.7 92.7 86.3 91.5 85.1 91.0 88.6All TBD 93.6 91.2 95.6 87.9 90.6 92.9 91.2 92.1 83.8 91.050 Sent.
65.3 48.5 74.5 74.2 70.2 76.2 79.2 76.2 54.7 68.6100 Sent.
73.9 52.3 80.9 81.6 77.3 75.3 82.0 80.1 64.8 73.9All Sent.
93.9 90.9 97.4 95.1 95.8 93.8 95.5 93.8 95.5 94.5Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models.
Avg.
is the average of all lan-guages except English.
Unsupervised models are trained without dictionary and use an oracle to map tags to clusters.Bilingual systems are trained using a dictionary transferred from English into the target language using word align-ments.
The Projection model uses a dictionary build directly from the part-of-speech projection.
The D&P modelextends the Projection model dictionary by using Label Propagation.
Supervised models are trained using tree bankinformation with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionaryand All TBD uses tree bank tag sets for all words.
50, 100 and All Sent.
models are trained in a supervised mannerusing increasing numbers of training sentences.Figure 6: Model accuracy across the Brown cor-pus sections.
ST: Stanford tagger, Wik: Wiktionary-tag-set-trained SHMM-ME, PTBD: PTB-tag-set-trainedSHMM-ME, PTB: Supervised SHMM-ME.
Wik outper-forms PTB and PTBD overall.5.6 Error breakdownIn Section 3.4 we discussed the accuracy of theWiktionary tag sets and as Table 2 shows, a dictio-nary with better tag set quality generally (except forGreek) improves the POS tagging accuracy.
In Fig-ure 7, we group actual errors by the word type clas-sified into the five cases discussed above: identical,superset, subset, overlap, disjoint.
We also add oov ?out-of-vocabulary word types.
The largest source oferror across languages are out-of-vocabulary (oov)word types at around 45% of the errors, followedby tag set mismatch types: subset, overlap, dis-joint, which together comprise another 50% of theerrors.
As Wiktionary grows, these types of errorswill likely diminish.Figure 7: Tag errors broken down by the word type clas-sified into the six classes: oov, identical, superset, subset,overlap, disjoint (see text for detail).
The largest source oferror across languages are out-of-vocabulary (oov) wordtypes, followed by tag set mismatch types: subset, over-lap, disjoint.6 ConclusionWe have shown that the Wiktionary can be usedto train a very simple model to achieve state-of-art weakly-supervised and out-of-domain POS tag-gers.
The methods outlined in the paper are stan-dard and easy to replicate, yet highly accurate andshould serve as baselines for more complex propos-1396als.
These encouraging results show that using free,collaborative NLP resources can in fact produce re-sults of the same level or better than using expensiveannotations for many languages.
Furthermore, theWiktionary contains other possibly useful informa-tion, such as glosses and translations.
It would bevery interesting and perhaps necessary to incorpo-rate this additional data in order to tackle challengesthat arise across a larger number of language types,specifically non-European languages.AcknowledgementsWe would like to thank Slav Petrov, KuzmanGanchev and Andre?
Martins for their helpful feed-back in early versions of the manuscript.
We wouldalso like to thank to our anonymous reviewers fortheir comments and suggestions.
Ben Taskar waspartially supported by a Sloan Fellowship, ONR2010 Young Investigator Award and NSF Grant1116676.ReferencesA.
Abeille?.
2003.
Treebanks: Building and Using ParsedCorpora.
Springer.Taylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?,John DeNero, and Dan Klein.
2010.
Painless unsuper-vised learning with features.
In Proc.
NAACL, June.John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In Conference on Empirical Methodsin Natural Language Processing, Sydney, Australia.Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-cent J. Della Pietra, and Jenifer C. Lai.
1992.
Class-based n-gram models of natural language.
Computa-tional Linguistics, 18:467?479.S.
Buchholz and E. Marsi.
2006.
Conll-x shared taskon multilingual dependency parsing.
In Proceedingsof the Tenth Conference on Computational NaturalLanguage Learning, pages 149?164.
Association forComputational Linguistics.S.F.
Chen.
2003.
Conditional and joint models forgrapheme-to-phoneme conversion.
In Proc.
ECSCT.P.
Chesley, B. Vincent, L. Xu, and R.K. Srihari.
2006.Using verbs and adjectives to automatically classifyblog sentiment.
Training, 580(263):233.Alexander Clark.
2003.
Combining distributional andmorphological information for part of speech induc-tion.
In Proc.
EACL.Dipanjan Das and Slav Petrov.
2011.
Unsupervisedpart-of-speech tagging with bilingual graph-based pro-jections.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics:Human Language Technologies, pages 600?609, Port-land, Oregon, USA, June.
Association for Computa-tional Linguistics.Weiwei Ding.
2011.
Weakly supervised part-of-speechtagging for chinese using label propagation.
Master?sthesis, University of Texas at Austin.Jianfeng Gao and Mark Johnson.
2008.
A comparison ofBayesian estimators for unsupervised hidden Markovmodel POS taggers.
In In Proc.
EMNLP, pages 344?352, Honolulu, Hawaii, October.
ACL.S.
Goldwater and T. Griffiths.
2007.
A fully Bayesianapproach to unsupervised part-of-speech tagging.
InIn Proc.
ACL, volume 45, page 744.J.V.
Grac?a, K. Ganchev, L. Coheur, F. Pereira, andB.
Taskar.
2011.
Controlling complexity in part-of-speech induction.
Journal of Artificial Intelligence Re-search, 41(2):527?551.J.
Grac?a, K. Ganchev, F. Pereira, and B. Taskar.
2009.Parameter vs. posterior sparisty in latent variable mod-els.
In Proc.
NIPS.A.
Haghighi and D. Klein.
2006.
Prototype-driven learn-ing for sequence models.
In Proc.
HTL-NAACL.
ACL.M Johnson.
2007.
Why doesn?t EM find good HMMPOS-taggers.
In In Proc.
EMNLP-CoNLL.AA Krizhanovsky and F. Lin.
2009.
Relatedterms search based on wordnet/wiktionary and itsapplication in ontology matching.
Arxiv preprintarXiv:0907.2209.Michael Lamar, Yariv Maron, Mark Johnson, and ElieBienenstock.
2010.
SVD and clustering for unsuper-vised POS tagging.
In Proceedings of the ACL 2010Conference: Short Papers, pages 215?219, Uppsala,Sweden, July.
Association for Computational Linguis-tics.Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.2010.
Simple type-level unsupervised POS tagging.In Proceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, pages 853?861, Cambridge, MA, October.
Association for Com-putational Linguistics.M.P.
Marcus, M.A.
Marcinkiewicz, and B. Santorini.1993.
Building a large annotated corpus of En-glish: The Penn Treebank.
Computational linguistics,19(2):313?330.B.
Merialdo.
1994.
Tagging English text with a proba-bilistic model.
Computational linguistics, 20(2):155?171.C.
Mu?ller and I. Gurevych.
2009.
Using wikipedia andwiktionary in domain-specific information retrieval.1397Evaluating Systems for Multilingual and MultimodalInformation Access, pages 219?226.E.
Navarro, F. Sajous, B. Gaume, L. Pre?vot, H. ShuKai,K.
Tzu-Yi, P. Magistry, and H. Chu-Ren.
2009.
Wik-tionary and nlp: Improving synonymy networks.
InProceedings of the 2009 Workshop on The People?sWeb Meets NLP: Collaboratively Constructed Seman-tic Resources, pages 19?27.
Association for Computa-tional Linguistics.J.
Nivre, J.
Hall, S. Ku?bler, R. McDonald, J. Nilsson,S.
Riedel, and D. Yuret.
2007.
The conll 2007 sharedtask on dependency parsing.
In Proceedings of theCoNLL Shared Task Session of EMNLP-CoNLL 2007.Association for Computational Linguistics.J.
Nocedal and Stephen J. Wright.
1999.
Numerical op-timization.
Springer.S.
Petrov, D. Das, and R. McDonald.
2011.
Auniversal part-of-speech tagset.
Arxiv preprintArXiv:1104.2086.A.
Ratnaparkhi.
1996.
A maximum entropy model forpart-of-speech tagging.
In Proc.
EMNLP.
ACL.Sujith Ravi and Kevin Knight.
2009.
Minimized modelsfor unsupervised part-of-speech tagging.
In In Proc.ACL.H.
Schu?tze.
1995.
Distributional part-of-speech tagging.In Proc.
EACL, pages 141?148.Libin Shen, Giorgio Satta, and Aravind Joshi.
2007.Guided learning for bidirectional sequence classifica-tion.
In Proc.
ACL, Prague, Czech Republic, June.N.
Smith and J. Eisner.
2005.
Contrastive estimation:Training log-linear models on unlabeled data.
In Proc.ACL.
ACL.B.
Snyder, T. Naseem, J. Eisenstein, and R. Barzilay.2008.
Unsupervised multilingual learning for POStagging.
In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing, pages1041?1050.
Association for Computational Linguis-tics.K.
Toutanova and M. Johnson.
2007.
A Bayesian LDA-based model for semi-supervised part-of-speech tag-ging.
In Proc.
NIPS, 20.K.
Toutanova, D. Klein, C. Manning, and Y. Singer.2003.
Feature-rich part-of-speech tagging with acyclic dependency network.
In In Proc.
HLT-NAACL.1398
