Proceedings of NAACL HLT 2007, Companion Volume, pages 217?220,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsA Semi-Automatic Evaluation Scheme: Automated Nuggetization forManual AnnotationLiang Zhou, Namhee Kwon, and Eduard HovyInformation Sciences InstituteUniversity of Southern California4676 Admiralty WayMarina del Rey, CA 90292{liangz, nkwon, hovy}@isi.eduAbstractIn this paper we describe automatic in-formation nuggetization and its applica-tion to text comparison.
Morespecifically, we take a close look at howmachine-generated nuggets can be used tocreate evaluation material.
A semi-automatic annotation scheme is designedto produce gold-standard data with excep-tionally high inter-human agreement.1 IntroductionIn many natural language processing (NLP) tasks,we are faced with the problem of determining theappropriate granularity level for information units.Most commonly, we use sentences to model indi-vidual pieces of information.
However, more NLPapplications require us to define text units smallerthan sentences, essentially decomposing sentencesinto a collection of phrases.
Each phrase carries anindependent piece of information that can be usedas a standalone unit.
These finer-grained informa-tion units are usually referred to as nuggets.When performing within-sentence comparisonfor redundancy and/or relevancy judgments, with-out a precise and consistent breakdown of nuggetswe can only rely on rudimentary n-gram segmenta-tions of sentences to form nuggets and performsubsequent n-gram-wise text comparison.
This isnot satisfactory for a variety of reasons.
For exam-ple, one n-gram window may contain several sepa-rate pieces of information, while another of thesame length may not contain even one completepiece of information.Previous work shows that humans can createnuggets in a relatively straightforward fashion.
Inthe PYRAMID scheme for manual evaluation ofsummaries (Nenkova and Passonneau, 2004), ma-chine-generated summaries were compared withhuman-written ones at the nugget level.
However,automatic creation of the nuggets is not trivial.Hamly et al (2005) explore the enumeration andcombination of all words in a sentence to create theset of all possible nuggets.
Their automation proc-ess still requires nuggets to be manually created apriori for reference summaries before any sum-mary comparison takes place.
This human in-volvement allows a much smaller subset of phrasesegments, resulting from word enumeration, to bematched in summary comparisons.
Without thehuman-created nuggets, text comparison falls backto its dependency on n-grams.
Similarly, in ques-tion-answering (QA) evaluations, gold-standardanswers use manually created nuggets and com-pare them against system-produced answers bro-ken down into n-gram pieces, as shown inPOURPRE (Lin and Demner-Fushman, 2005) andNUGGETEER (Marton and Radul, 2006).A serious problem in manual nugget creation isthe inconsistency in human decisions (Lin andHovy, 2003).
The same nugget will not be markedconsistently with the same words when sentencescontaining multiple instances of it are presented tohuman annotators.
And if the annotation is per-formed over an extended period of time, the con-sistency is even lower.
In recent exercises of thePYRAMID evaluation, inconsistent nuggets areflagged by a tracking program and returned back tothe annotators, and resolved manually.Given these issues, we address two questions inthis paper: First, how do we define nuggets so thatthey are consistent in definition?
Secondly, how do217we utilize automatically extracted nuggets for vari-ous evaluation purposes?2  Nugget DefinitionBased on our manual analysis and computationalmodeling of nuggets, we define them as follows:Definition:?
A nugget is predicated on either an event  oran entity .?
Each nugget consists of two parts: the an-chor and the content.The anchor is either:?
the head noun of the entity, or?
the head verb of the event, plus the headnoun of its associated entity (if more thanone entity is attached to the verb, then itssubject).The content is a coherent single piece of infor-mation associated with the anchor.
Each anchormay have several separate contents.When a nugget contains nested sentences, thisdefinition is applied recursively.
Figure 1 shows anexample.
Anchors are marked with square brack-ets.
If the anchor is a verb, then its entity attach-ment is marked with curly brackets.
If the sentencein question is a compound and/or complex sen-tence, then this definition is applied recursively toallow decomposition.
For example, in Figure 1,without recursive decomposition, only two nuggetsare formed: 1) ?
[girl] working at the bookstore inHollywood?, and 2) ?
{girl} [talked] to the diplo-mat living in Britain?.
In this example, recursivedecomposition produces nuggets with labels 1-a, 1-b, 2-a, and 2-b.2.1  Nugget ExtractionWe use syntactic parse trees produced by theCollins parser (Collins, 1999) to obtain the struc-tural representation of sentences.
Nuggets are ex-tracted by identifying subtrees that are descriptionsfor entities and events.
For entity nuggets, we ex-amine subtrees headed by ?NP?
; for event nuggets,subtrees headed by ?VP?
are examined and theircorresponding subjects (siblings headed by ?NP?
)are treated as entity attachments for the verbphrases.3  Utilizing Nuggets in EvaluationsIn recent QA and summarization evaluation exer-cises, manually created nuggets play a determinaterole in judging system qualities.
Although the twotask evaluations are similar, the text comparisontask in summarization evaluation is more complexbecause systems are required to produce long re-sponses and thus it is hard to yield high agreementif manual annotations are performed.
The follow-ing experiments are conducted in the realm ofsummarization evaluation.3.1  Manually Created NuggetsDuring the recent two Document UnderstandingConfereces (DUC-05 and DUC-06) (NIST, 2002?2007), the PYRAMID framework (Nenkova andPassonneau, 2004) was used for manual summaryevaluations.
In this framework, human annotatorsselect and highlight portions of reference summa-ries to form a pyramid of summary content units(SCUs) for each docset.
A pyramid is constructedfrom SCUs and their corresponding popularityscores?the number of reference summaries theyappeared in individually.
SCUs carrying the sameinformation do not necessarily have the same sur-face-level words.
Annotators need to make the de-cisions based on semantic equivalence amongFigure 1.
Nugget definition examples.Sentence:The girl working at the bookstore in Hollywoodtalked to the diplomat living in Britain.Nuggets are:1)  [girl] working at the bookstore in Holly-wooda.
[girl] working at the bookstoreb.
[bookstore] in Hollywood2) {girl} [talked] to the diplomat living inBritaina.
{girl} [talked] to the diplomatb.
[diplomat] living in BritianAnchors:1)  [girl]a.
[girl]b.
[bookstore]2) {girl} [talked]: talked is the anchor verband girl is its entity attachment.a.
{girl} [talked]b.
[diplomat]218various SCUs.
To evaluate a peer summary from aparticular docset, annotators highlight portions oftext in the peer summary that convey the same in-formation as those SCUs in previously constructedpyramids.3.
2  Automatically Created NuggetsWe envisage the nuggetization process beingautomated and nugget comparison and aggregationbeing performed by humans.
It is crucial to involvehumans in the evaluation process because recog-nizing semantically equivalent units is not a trivialtask computationally.
In addition, since nuggets aresystem-produced and can be imperfect, annotatorsare allowed to reject and re-create them.
We per-form record-keeping in the background on whichnugget or nugget groups are edited so that furtherimprovements can be made for nuggetization.The evaluation scheme is designed as follows:For reference summaries  (per docset):?
Nuggets are created for all sentences;?
Annotators will group equivalent nuggets.?
Popularity scores are automatically assignedto nugget groups.For peer summaries :?
Nuggets are created for all sentences;?
Annotators will match/align peer?s nuggetswith reference nugget groups.?
Recall scores are to be computed.3.
3  Consistency in Human InvolvementThe process of creating nuggets has been auto-mated and we can assume a certain level of consis-tency based on the usage of the syntactic parser.However, a more important issue emerges.
Whengiven the same set of nuggets, would human anno-tators agree on nugget group selections and theircorresponding contributing nuggets?
What levelsof agreement and disagreement should be ex-pected?
Two annotators, one familiar with the no-tion of nuggetization (C1) and one not (C2),participated in the following experiments.Figure 2 shows the annotation procedure forreference summaries.
After two rounds of individ-ual annotations and consolidations and one finalround of conflict resolution, a set of gold-standardnugget groups is created for each docset and willbe subsequently used in peer summary annotations.The first round of annotation is needed since oneof the annotators, C2, is not familiar with nuggeti-zation.
After the initial introduction of the task,concerns and questions arisen can be addressed.Then the annotators proceed to the second round ofannotation.
Naturally, some differences and con-flicts remain.
Annotators must resolve these prob-lems during the final round of conflict resolutionand create the agreed-upon gold-standard data.Previous manual nugget annotation has used oneannotator as the primary nugget creator and an-other annotator as an inspector (Nenkova and Pas-sonneau, 2004).
In our annotation experiment, weencourage both annotators to play equally activeroles.
Conflicts between annotators resulting fromideology, comprehension, and interpretation differ-ences helped us to understand that completeagreement between annotators is not realistic andnot achievable, unless one annotator is dominantover the other.
We should expect a 5-10% annota-tion variation.In Figure 3, we show annotation comparisonsfrom first to second round.
The x -axis shows thenugget groups that C1 and C2 have agreed on.
They -axis shows the popularity score a particular nug-get group received.
Selecting from three referencesummaries, a score of three for a nugget group in-dicates it was created from nuggets in all threeFigure 2.
Reference annotation and gold-standarddata creation.219summaries.
The first round initially appears suc-cessful because the two annotators had 100%agreement on nugget groups and their correspond-ing scores.
However, C2, the novice nuggetizer,was much more conservative than C1, becauseonly 10 nugget groups were created.
The geometricmean of agreement on all nugget group assignmentis merely 0.4786.
During the second round, differ-ences in group-score allocations emerge, 0.9192,because C2 is creating more nugget groups.
Thegeometric mean of agreement on all nugget groupassignment has been improved to 0.7465.After the final round of conflict resolution,gold-standard data was created.
Since all conflictsmust be resolved, annotators have to either con-vince or be convinced by the other.
How muchchange is there between an annotator?s second-round annotation and the gold-standard?
Geomet-ric mean of agreement on all nugget group assign-ment for C1 is 0.7543 and for C2 is 0.8099.Agreement on nugget group score allocation forC1 is 0.9681 and for C2 is 0.9333.
From these fig-ures, we see that while C2 contributed more to thegold-standard?s nugget group creations, C1 hadmore accuracy in finding the correct number ofnugget occurrences in reference summaries.
Thisconfirms that both annotators played an active role.Using the gold-standard nugget groups, the annota-tors performed 4 peer summary annotations.
Theagreement among peer summary annotations isquite high, at approximately 0.95.
Among the four,annotations on one peer summary from the twoannotators are completely identical.4  ConclusionIn this paper we have given a concrete definitionfor information nuggets and provided a systematicimplementation of them.
Our main goal is to usethese machine-generated nuggets in a semi-automatic evaluation environment for various NLPapplications.
We took a close look at how this canbe accomplished for summary evaluation, usingnuggets created from reference summaries to gradepeer summaries.
Inter-annotator agreements aremeasured to insure the quality of the gold-standarddata created.
And the agreements are very high byfollowing a meticulous procedure.
We are cur-rently preparing to deploy our design into full-scale evaluation exercises.ReferencesCollins, M. 1999.
Head-driven statistical models fornatural language processing.
Ph D Dissertation , Uni-versity of Pennsylvania.Hamly, A., A. Nenkova, R. Passonneau, and O. Ram-bow.
2005.
Automation of summary evaluation bythe pyramid method.
In Proceedings of RANLP.Lin, C.Y.
and E. Hovy.
2003.
Automatic evaluation ofsummaries using n-gram co-occurrence statistics.
InProceedings of NAACL-HLT.Lin, J. and D. Demner-Fushman.
2005.
Automaticallyevaluating answers to definition questions.
In Pro-ceedings of HLT-EMNLP.Marton, G. and A. Radul.
2006.
Nuggeteer: automaticnugget-based evaluation using description and judg-ments.
In Proceedings NAACL-HLT.Nenkova, A. and R. Passonneau.
2004.
Evaluating con-tent selection in summarization: the pyramid method.In Proceedings NAACL-HLT.NIST.
2001?2007.
Document Understanding Confer-ence.
www-nlpir.nist.gov/projects/duc/index.html.Figure 3.
Annotation comparisons from 1 st to2nd round.220
