Predicting Part-of-Speech Information about Unknown Wordsusing Statistical MethodsScott  M.  ThedePurdue UniversityWest Lafayette, IN 47907AbstractThis paper examines the feasibility of using sta-tistical methods to train a part-of-speech pre-dictor for unknown words.
By using statisticalmethods, without incorporating hand-craftedlinguistic information, the predictor could beused with any language for which there is alarge tagged training corpus.
Encouraging re-sults have been obtained by testing the predic-tor on unknown words from the Brown corpus.The relative value of information sources suchas affixes and context is discussed.
This part-of-speech predictor will be used in a part-of-speechtagger to handle out-of-lexicon words.1 In t roduct ionPart-of-speech tagging involves selecting themost likely sequence of syntactic ategories forthe words in a sentence.
These syntactic at-egories, or tags, generally consist of parts ofspeech, often with feature information included.An example set of tags can be found in the PennTreebank project (Marcus et al, 1993).
Part-of-speech tagging is useful for speeding up parsingsystems, and allowing the use of partial parsing.Many current systems make use of a Hid-den Markov Model (HMM) for part-of-speechtagging.
Other methods include rule-basedsystems (Brill, 1995), maximum entropy mod-els (Ratnaparkhi, 1996), and memory-basedmodels (Daelemans et al, 1996).
In an HMMtagger the Markov assumption is made so thatthe current word depends only on the currenttag, and the current tag depends only on ad-jacent tags.
Charniak (Charniak et al, 1993)gives a thorough explanation of the equationsfor an HMM model, and Kupiec (Kupiec, 1992)describes an HMM tagging system in detail.One important area of research in part-of-speech tagging is how to handle unknown words.If a word is not in the lexicon, then the lexicalprobabilities must be provided from some othersource.
One common approach is to use affixa-tion rules to "learn" the probabilities for wordsbased on their suffixes or prefixes.
Weischedel'sgroup (Weischedel et al, 1993) examines un-known words in the context of part-of-speechtagging.
Their method creates a probability dis-tribution for an unknown word based on certainfeatures: word endings, hyphenation, and capi-talization.
The features to be used are chosen byhand for the system.
Mikheev (Mikheev, 1996;Mikheev, 1997) uses a general purpose lexiconto learn affix and word ending information to beused in tagging unknown words.
His work re-turns a set of possible tags for unknown words,with no probabilities attached, relying on thetagger to disambiguate hem.This work investigates the possibility of au-tomatically creating a probability distributionover all tags for an unknown word, instead of asimple set of tags.
This can be done by creat-ing a probabilistic lexicon from a large taggedcorpus (in this case, the Brown corpus), and us-ing that data to estimate distributions for wordswith a given "prefix" or "suffix".
Prefix andsuffix indicate substrings that come at the be-ginning and end of a word respectively, and arenot necessarily morphologically meaningful.This predictor will offer a probability distri-bution of possible tags for an unknown word,based solely on statistical data available in thetraining corpus.
Mikheev's and Weischedel'ssystems, along with many others, uses languagespecific information by using a hand-generatedset of English affixes.
This paper investigateswhat information sources can be automaticallyconstructed, and which are most useful in pre-dicting tags for unknown words.2 Creat ing  the  Pred ic torTo build the unknown word predictor, a lexiconwas created from the Brown corpus.
The entryfor a word consists of a list of all tags assignedto that word, and the number of times that tagwas assigned to that word in the entire trainingcorpus.
For example, the lexicon entry for the1505word advanced is the following:advanced ((VBN 31) (JJ 12) (VBD 8))This means that the word advanced appeareda total of 51 times in the corpus: 31 as a pastparticiple (VBN), 12 as an adjective (J J), and8 as a past tense verb (VBD).
We can then usethis lexicon to estimate P(wilti).This lexicon is used as a preliminary sourceto construct the unknown word predictor.
Thispredictor is constructed based on the assump-tion that new words in a language are createdusing a well-defined morphological process.
Wewish to use suffixes and prefixes to predict pos-sible tags for unknown words.
For example, aword ending in -ed is likely to be a past tenseverb or a past participle.
This rough stem-ming is a preliminary technique, but it avoidsthe need for hand-crafted morphological infor-mation.
To create a distribution for each givenaffix, the tags for all words with that affix aretotaled.
Affixes up to four characters long, orup to two characters less than the length ofthe word, whichever is smaller, are considered.Only open-class tags are considered when con-structing the distributions.
Processing all thewords in the lexicon creates a probability distri-bution for all affixes that appear in the corpus.One problem is that data is available for bothprefixes and suffixes--how should both sets ofdata be used?
First, the longest applicable suf-fix and prefix are chosen for the word.
Then, asa baseline system, a simple heuristic method ofselecting the distribution with the fewest pos-sible tags was used.
Thus, if the prefix has adistribution over three possible tags, and thesuffix has a distribution over five possible tags,the distribution from the prefix is used.3 Ref in ing  the  Pred ic t ionsThere are several techniques that can be usedto refine the distributions of possible tags forunknown words.
Some of these that are used inour system are listed here.3.1 Ent ropy  Calculat ionsA method was developed that uses the entropyof the prefix and suffix distributions to deter-mine which is more useful.
Entropy, used insome part-of-speech tagging systems (Ratna-parkhi, 1996), is a measure of how much in-formation is necessary to separate data.
Theentropy of a tag distribution is determined bythe following equation:ni j  1- -  t ni jEntropy of i-th affix = - / _ /~ i  *?g2t~i)3wherenlj = j-th tag occurrences in i-th affix wordsNi = total occurrences of the i-th affixThe distribution with the smallest entropy isused, as this is the distribution that offers themost information.3.2 Open-Class Smooth ingIn the baseline method, the distributions pro-duced by the predictor are smoothed with theoverall distribution of tags.
In other words, ifp(x) is the distribution for the affix, and q(x)is the overall distribution, we form a new dis-tribution p'(x) = Ap(x) + (1 - A)q(x).
We useA = 0.9 for these experiments.
We hypothesizethat smoothing using the open-class tag distri-bution, instead of the overall distribution, willoffer better esults.3.3 Contextua l  In fo rmat ionContextual probabilities offer another source ofinformation about the possible tags for an un-known word.
The probabilities P(tilti_l) aretrained from the 90% set of training data, andcombined with the unknown word's distribu-tion.
This use of context will normally be donein the tagger proper, but is included here forillustrative purposes.3.4 Using Suffixes OnlyPrefixes eem to offer less information than suf-fixes.
To determine if calculating distributionsbased on prefixes is helpful, a predictor thatonly uses suffix information is also tested.4 The  Exper imentThe experiments were performed using theBrown corpus.
A 10-fold cross-validation tech-nique was used to generate the data.
The sen-tences from the corpus were split into ten files,nine of which were used to train the predictor,and one which was the test set.
The lexicon forthe test run is created using the data from thetraining set.
All unknown words in the test set(those that did not occur in the training set)were assigned a tag distribution by the predic-tor.
Then the results are checked to see if thecorrect tag is in the n-best tags.
The resultsfrom all ten test files were combined to rate theoverall performance for the experiment.5 Resu l tsThe results from the initial experiments areshown in Table 1.
Some trends can be seenin this data.
For example, choosing whether1506Method Open?
Con?
l -bes tBase l ine  no no 57.6%Base l ine  no  yes  61.5%Base l ine  yes no  57.6%Base l ine  yes  yes  61.3%Ent ropy  no no 62.2%Ent ropy  no yes  65.7%Ent ropy  yes  no 62.2%Ent ropy  yes  yes  65.4%End ings  no no 67.1%Endings no yes  70.9%Endings yes  no 67.1%Endings yes  yes 70.9%Open?
- sys temCon?
- sys tem2-best73.2%75.0%73.6%78.2%77.6%78.9%78.1%81.8%83.5%86.5%83.6%87.6%3-best79.5%81.7%83.2%87.0%83.4%85.1%86.9%89.6%91.4%92.6%92.2%93.8%uses open-class smoothinguses context  in fo rmat ionTable 1: Results using Various Methodsto use the prefix distribution or suffix distribu-tion using entropy calculations clearly improvesthe performance over using the baseline method(about 4-5% overall), and using only suffix dis-tributions improves it another 4-5%.
The use ofcontext improves the likelihood that the correcttag is in the n-best predicted for small valuesof n (improves nearly 4% for 1-best), but it isless important for larger values of n. On theother hand, smoothing the distributions withopen-class tag distributions offers no improve-ment for the 1-best results, but improves then-best performance for larger values of n.Overall, the best performing system wasthe system using both context and open-classsmoothing, relying on only the suffix informa-tion.
To offer a more valid comparison betweenthis work and Mikheev's latest work (Mikheev,1997), the accuracies were tested again, ignor-ing mistags between NN and NNP (commonand proper nouns) as Mikheev did.
This im-proved results to 77.5% for 1-best, 89.9% for2-best, and 94.9% for 3-best.
Mikheev obtains87.5% accuracy when using a full HMM taggingsystem with his cascading tagger.
It should benoted that our system is not using a full tag-ger, and presumably a full tagger would cor-rectly disambiguate many of the words wherethe correct ag was not the 1-best choice.
Also,Mikheev's work suffers from reduced coverage,while our predictor offers a prediction for everyunknown word encountered.6 Conc lus ions  and  Fur ther  WorkThe experiments documented in this paper sug-gest that a tagger can be trained to handle un-known words effectively.
By using the prob-abilistic lexicon, we can predict tags for un-known words based on probabilities estimatedfrom training data, not hand-crafted rules.
Themodular approach to unknown word predictionallows us to determine what sorts of informationare most important.Further work will attempt o improve the ac-curacy of the predictor, using new knowledgesources.
We will explore the use of the con-cept of a confidence measure, as well as usingonly infrequently occurring words from the lex-icon to train the predictor, which would presum-ably offer a better approximation of the distri-bution of an unknown word.
We also plan tointegrate the predictor into a full HMM taggingsystem, where it can be tested in real-world ap-plications, using the hidden Markov model todisambiguate problem words.Re ferencesEric Brill.
1995.
Transformation-based rror-driven learning and natural anguage process-ing: A case study in part of speech tagging.Computational Linguistics, 21 (4):543-565.Eugene Charniak, Curtis Hendrickson, Neff Ja-cobson, and Mike Perkowitz.
1993.
Equa-tions for part-of-speech tagging.
Proceedingsof the Eleventh National Conference on Arti-ficial Intelligence, pages 784-789.Walter Da~lemans, Jakub Zavrel, Peter Berck,and Steven Gillis.
1996.
MBT: A memory-based part of speech tagger-generator.
Pro-ceedings of the Fourth Workshop on VeryLarge Corpora, pages 14-27.Julian Kupiec.
1992.
Robust part-of-speechtagging using a hidden markov model.
Com-puter Speech and Language, 6(3):225-242.Mitchell Marcus, Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Buildinga large annotated corpus of English: ThePenn Treebank.
Computational Linguistics,19(2):313-330.Andrei Mikheev.
1996.
Unsupervised learningof word-category guessing rules.
Proceedingsof the 34th Annual Meeting of the Associationfor Compuatational Linguistics, pages 327-334.Andrei Mikheev.
1997.
Automatic rule induc-tion for unknown-word guessing.
Computa-tional Linguistics, 23(3):405-423.Adwait Ratnaparkhi.
1996.
A maximum en-tropy model for part-of-speech tagging.
Pro-ceedings of the Conference on EmpiricalMethods in Natural Language Processing.Ralph Weischedel, Marie Meeter, RichardSchwartz, Lance Ramshaw, and Jeff Pal-mucci.
1993.
Coping with ambiguity andunknown words through probabilitic models.Computational Linguistics, 19:359-382.1507
