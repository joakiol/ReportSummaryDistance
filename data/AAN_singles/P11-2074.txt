Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 424?428,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsDiscriminative Feature-Tied Mixture Modeling for Statistical MachineTranslationBing Xiang and Abraham IttycheriahIBM T. J. Watson Research CenterYorktown Heights, NY 10598{bxiang,abei}@us.ibm.comAbstractIn this paper we present a novel discrimi-native mixture model for statistical machinetranslation (SMT).
We model the feature spacewith a log-linear combination of multiple mix-ture components.
Each component contains alarge set of features trained in a maximum-entropy framework.
All features within thesame mixture component are tied and sharethe same mixture weights, where the mixtureweights are trained discriminatively to max-imize the translation performance.
This ap-proach aims at bridging the gap between themaximum-likelihood training and the discrim-inative training for SMT.
It is shown that thefeature space can be partitioned in a vari-ety of ways, such as based on feature types,word alignments, or domains, for various ap-plications.
The proposed approach improvesthe translation performance significantly on alarge-scale Arabic-to-English MT task.1 IntroductionSignificant progress has been made in statisti-cal machine translation (SMT) in recent years.Among all the proposed approaches, the phrase-based method (Koehn et al, 2003) has become thewidely adopted one in SMT due to its capabilityof capturing local context information from adja-cent words.
There exists significant amount of workfocused on the improvement of translation perfor-mance with better features.
The feature set could beeither small (at the order of 10), or large (up to mil-lions).
For example, the system described in (Koehnet al, 2003) is a widely known one using small num-ber of features in a maximum-entropy (log-linear)model (Och and Ney, 2002).
The features includephrase translation probabilities, lexical probabilities,number of phrases, and language model scores, etc.The feature weights are usually optimized with min-imum error rate training (MERT) as in (Och, 2003).Besides the MERT-based feature weight opti-mization, there exist other alternative discriminativetraining methods for MT, such as in (Tillmann andZhang, 2006; Liang et al, 2006; Blunsom et al,2008).
However, scalability is a challenge for theseapproaches, where all possible translations of eachtraining example need to be searched, which is com-putationally expensive.In (Chiang et al, 2009), there are 11K syntac-tic features proposed for a hierarchical phrase-basedsystem.
The feature weights are trained with theMargin Infused Relaxed Algorithm (MIRA) effi-ciently on a forest of translations from a develop-ment set.
Even though significant improvement hasbeen obtained compared to the baseline that hassmall number of features, it is hard to apply thesame approach to millions of features due to the datasparseness issue, since the development set is usu-ally small.In (Ittycheriah and Roukos, 2007), a maximumentropy (ME) model is proposed, which utilizes mil-lions of features.
All the feature weights are trainedwith a maximum-likelihood (ML) approach on thefull training corpus.
It achieves significantly bet-ter performance than a normal phrase-based system.However, the estimation of feature weights has nodirect connection with the final translation perfor-424mance.In this paper, we propose a hybrid framework, adiscriminative mixture model, to bridge the gap be-tween the ML training and the discriminative train-ing for SMT.
In Section 2, we briefly review the MEbaseline of this work.
In Section 3, we introduce thediscriminative mixture model that combines varioustypes of features.
In Section 4, we present experi-mental results on a large-scale Arabic-English MTtask with focuses on feature combination, alignmentcombination, and domain adaptation, respectively.Section 5 concludes the paper.2 Maximum-Entropy Model for MTIn this section we give a brief review of a specialmaximum-entropy (ME) model as introduced in (It-tycheriah and Roukos, 2007).
The model has thefollowing form,p(t, j|s) = p0(t, j|s)Z(s)exp?i?i?i(t, j, s), (1)where s is a source phrase, and t is a target phrase.j is the jump distance from the previously translatedsource word to the current source word.
Duringtraining j can vary widely due to automatic wordalignment in the parallel corpus.
To limit the sparse-ness created by long jumps, j is capped to a win-dow of source words (-5 to 5 words) around the lasttranslated source word.
Jumps outside the windoware treated as being to the edge of the window.
InEq.
(1), p0 is a prior distribution, Z is a normal-izing term, and ?i(t, j, s) are the features of themodel, each being a binary question asked about thesource, distortion, and target information.
The fea-ture weights ?i can be estimated with the ImprovedIterative Scaling (IIS) algorithm (Della Pietra et al,1997), a maximum-likelihood-based approach.3 Discriminative Mixture Model3.1 Mixture ModelNow we introduce the discriminative mixture model.Suppose we partition the feature space into multipleclusters (details in Section 3.2).
Let the probabil-ity of target phrase and jump given certain sourcephrase for cluster k bepk(t, j|s) =1Zk(s)exp?i?ki?ki(t, j, s), (2)where Zk is a normalizing factor for cluster k.We propose a log-linear mixture model as shownin Eq.
(3).p(t, j|s) = p0(t, j|s)Z(s)?kpk(t, j|s)wk .
(3)It can be rewritten in the log domain aslog p(t, j|s) = log p0(t, j|s)Z(s)+?kwk log pk(t, j|s)= logp0(t, j|s)Z(s)?
?kwk log Zk(s)+?kwk?i?ki?ki(t, j, s).
(4)The individual feature weights ?ki for the i-thfeature in cluster k are estimated in the maximum-entropy framework as in the baseline model.
How-ever, the mixture weights wk can be optimized di-rectly towards the translation evaluation metric, suchas BLEU (Papineni et al, 2002), along with otherusual costs (e.g.
language model scores) on a devel-opment set.
Note that the number of mixture com-ponents is relatively small (less than 10) comparedto millions of features in baseline.
Hence the opti-mization can be conducted easily to generate reliablemixture weights for decoding with MERT (Och,2003) or other optimization algorithms, such asthe Simplex Armijo Downhill algorithm proposedin (Zhao and Chen, 2009).3.2 Partition of Feature SpaceGiven the proposed mixture model, how to split thefeature space into multiple regions becomes crucial.In order to surpass the baseline model, where allfeatures can be viewed as existing in a single mix-ture component, the separated mixture componentsshould be complementary to each other.
In thiswork, we explore three different ways of partitions,based on either feature types, word alignment types,or the domain of training data.In the feature-type-based partition, we split theME features into 8 categories:?
F1: Lexical features that examine source word,target word and jump;425?
F2: Lexical context features that examinesource word, target word, the previous sourceword, the next source word and jump;?
F3: Lexical context features that examinesource word, target word, the previous sourceword, the previous target word and jump;?
F4: Lexical context features that examinesource word, target word, the previous or nextsource word and jump;?
F5: Segmentation features based on mor-phological analysis that examine source mor-phemes, target word and jump;?
F6: Part-of-speech (POS) features that examinethe source and target POS tags and their neigh-bors, along with target word and jump;?
F7: Source parse tree features that collect theinformation from the parse labels of the sourcewords and their siblings in the parse trees,along with target word and jump;?
F8: Coverage features that examine the cover-age status of the source words to the left andto the right.
They fire only if the left sourceis open (untranslated) or the right source isclosed.All the features falling in the same feature cate-gory/cluster are tied to each other to share the samemixture weights at the upper level as in Eq.
(3).Besides the feature-type-based clustering, we canalso divide the feature space based on word align-ment types, such as supervised alignment versus un-supervised alignment (to be described in the exper-iment section).
For each type of word alignment,we build a mixture component with millions of MEfeatures.
On the task of domain adaptation, wecan also split the training data based on their do-main/resources, with each mixture component rep-resenting a specific domain.4 Experiments4.1 Data and BaselineWe conduct a set of experiments on an Arabic-to-English MT task.
The training data includes the UNparallel corpus and LDC-released parallel corpora,with about 10M sentence pairs and 300M words intotal (counted at the English side).
For each sentencein the training, three types of word alignments arecreated: maximum entropy alignment (Ittycheriahand Roukos, 2005), GIZA++ alignment (Och andNey, 2000), and HMM alignment (Vogel et al,1996).
Our tuning and test sets are extracted fromthe GALE DEV10 Newswire set, with no overlapbetween tuning and test.
There are 1063 sentences(168 documents) in the tuning set, and 1089 sen-tences (168 documents) in the test set.
Both setshave one reference translation for each sentence.
In-stead of using all the training data, we sample thetraining corpus based on the tuning/test set to trainthe systems more efficiently.
In the end, about 1.5Msentence pairs are selected for the sampled training.A 5-gram language model is trained from the En-glish Gigaword corpus and the English portion of theparallel corpus used in the translation model train-ing.
In this work, the decoding weights for boththe baseline and the mixture model are tuned withthe Simplex Armijo Downhill algorithm (Zhao andChen, 2009) towards the maximum BLEU.System Features BLEUF1 685K 37.11F2 5516K 38.43F3 4457K 37.75F4 3884K 37.56F5 103K 36.03F6 325K 37.89F7 1584K 38.56F8 1605K 37.49Baseline 18159K 39.36Mixture 18159K 39.97Table 1: MT results with individual mixture component(F1 to F8), baseline, or mixture model.4.2 Feature CombinationWe first experiment with the feature-type-basedclustering as described in Section 3.2.
The trans-lation results on the test set from the baseline andthe mixture model are listed in Table 1.
The MTperformance is measured with the widely adoptedBLEU metric.
We also evaluate the systems that uti-lize only one of the mixture components (F1 to F8).The number of features used in each system is also426listed in the table.
As we can see, when using all18M features in the baseline model, without mixtureweighting, the baseline achieved 3.3 points higherBLEU score than F5 (the worst component), and 0.8higher BLEU score than F7 (the best component).With the log-linear mixture model, we obtained 0.6gain compared to the baseline.
Since there are ex-actly the same number of features in the baselineand mixture model, the better performance is dueto two facts: separate training of the feature weights?
within each mixture component; the discrimina-tive training of mixture weights w. The first one al-lows better parameter estimation given the numberof features in each mixture component is much lessthan that in the baseline.
The second factor connectsthe mixture weighting to the final translation perfor-mance directly.
In the baseline, all feature weightsare trained together solely under the maximum like-lihood criterion, with no differentiation of the vari-ous types of features in terms of their contribution tothe translation performance.System Features BLEUME 5687K 39.04GIZA 5716K 38.75HMM 5589K 38.65Baseline 18159K 39.36Mixture 16992K 39.86Table 2: MT results with different alignments, baseline,or mixture model.4.3 Alignment CombinationIn the baseline mentioned above, three types of wordalignments are used (via corpus concatenation) forphrase extraction and feature training.
Given themixture model structure, we can apply it to an align-ment combination problem.
With the phrase tableextracted from all the alignments, we train threefeature mixture components, each on one type ofalignments.
Each mixture component contains mil-lions of features from all feature types described inSection 3.2.
Again, the mixture weights are op-timized towards the maximum BLEU.
The resultsare shown in Table 2.
The baseline system onlyachieved 0.3 minor gain compared to extracting fea-tures from ME alignment only (note that phrases arefrom all the alignments).
With the mixture model,we can achieve another 0.5 gain compared to thebaseline, especially with less number of features.This presents a new way of doing alignment com-bination in the feature space instead of in the usualphrase space.System Features BLEUNewswire 8898K 38.82Weblog 1990K 38.20UN 4700K 38.21Baseline 18159K 39.36Mixture 15588K 39.81Table 3: MT results with different training sub-corpora,baseline, or mixture model.4.4 Domain AdaptationAnother popular task in SMT is domain adapta-tion (Foster et al, 2010).
It tries to take advantage ofany out-of-domain training data by combining themwith the in-domain data in an appropriate way.
Inour sub-sampled training corpus, there exist threesubsets: newswire (1M sentences), weblog (200K),and UN data (300K).
We train three mixture com-ponents, each on one of the training subsets.
All re-sults are compared in Table 3.
The baseline that wastrained on all the data achieved 0.5 gain compared tousing the newswire training data alone (understand-ably it is the best component given the newswire testdata).
Note that since the baseline is trained on sub-sampled training data, there is already certain do-main adaptation effect involved.
On top of that, themixture model results in another 0.45 gain in BLEU.All the improvements in the mixture models aboveagainst the baseline are statistically significant withp-value < 0.0001 by using the confidence tool de-scribed in (Zhang and Vogel, 2004).5 ConclusionIn this paper we presented a novel discriminativemixture model for bridging the gap between themaximum-likelihood training and the discriminativetraining in SMT.
We partition the feature space intomultiple regions.
The features in each region are tiedtogether to share the same mixture weights that areoptimized towards the maximum BLEU scores.
Itwas shown that the same model structure can be ef-427fectively applied to feature combination, alignmentcombination and domain adaptation.
We also pointout that it is straightforward to combine any of thesethree.
For example, we can cluster the features basedon both feature types and alignments.
Further im-provement may be achieved with other feature spacepartition approaches in the future.AcknowledgmentsWe would like to acknowledge the support ofDARPA under Grant HR0011-08-C-0110 for fund-ing part of this work.
The views, opinions, and/orfindings contained in this article/presentation arethose of the author/presenter and should not be in-terpreted as representing the official views or poli-cies, either expressed or implied, of the Defense Ad-vanced Research Projects Agency or the Departmentof Defense.ReferencesPhil Blunsom, Trevor Cohn, and Miles Osborne.
2008.A discriminative latent variable model for statisticalmachine translation.
In Proceedings of ACL-08:HLT.David Chiang, Kevin Knight, and Wei Wang.
2009.11,001 new features for statistical machine translation.In Proceedings of NAACL-HLT.Stephen Della Pietra, Vincent Della Pietra, and John Laf-ferty.
1997.
Inducing features of random fields.
IEEETransactions on Pattern Analysis and Machine Intelli-gence.George Foster, Cyril Goutte, and Roland Kuhn.
2010.Discriminative instance weighting for domain adapta-tion in satistical machine translation.
In Proceedingsof EMNLP.Abraham Ittycheriah and Salim Roukos.
2005.
A maxi-mum entropy word aligner for arabic-english machinetranslation.
In Proceedings of HLT/EMNLP, pages89?96, October.Abraham Ittycheriah and Salim Roukos.
2007.
Di-rect translation model 2.
In Proceedings HLT/NAACL,pages 57?64, April.Philipp Koehn, Franz Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proceedings ofNAACL/HLT.Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, andBen Taskar.
2006.
An end-to-end discriminativeapproach to machine translation.
In Proceedings ofACL/COLING, pages 761?768, Sydney, Australia.Franz Josef Och and Hermann Ney.
2000.
Improvedstatistical alignment models.
In Proceedings of ACL,pages 440?447, Hong Kong, China, October.Franz Josef Och and Hermann Ney.
2002.
Discrimi-native training and maximum entropy models for sta-tistical machine translations.
In Proceedings of ACL,pages 295?302, Philadelphia, PA, July.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of ACL,pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-jing Zhu.
2002.
Bleu: a method for automatic evalu-ation of machine translation.
In Proceedings of ACL,pages 311?318.Christoph Tillmann and Tong Zhang.
2006.
A discrim-inative global training algorithm for statistical mt.
InProceedings of ACL/COLING, pages 721?728, Syd-ney, Australia.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
Hmm-based word alignment in statistical trans-lation.
In Proceedings of COLING, pages 836?841.Ying Zhang and Stephan Vogel.
2004.
Measuring con-fidence intervals for the machine translation evalua-tion metrics.
In Proceedings of The 10th InternationalConference on Theoretical and Methodological Issuesin Machine Translation.Bing Zhao and Shengyuan Chen.
2009.
A simplexarmijo downhill algorithm for optimizing statisticalmachine translation decoding parameters.
In Proceed-ings of NAACL-HLT.428
