HIGH-ACCURACY LARGE-VOCABULARY SPEECH RECOGNITIONUSING MIXTURE TYING AND CONSISTENCY MODELINGVassilios Digalakis and Hy MurveitSRI  InternationalSpeech Technology and Research Laboratory333 Ravenswood Ave., Menlo Park, CA  94025-3493ABSTRACTImproved acoustic modeling can significantly decrease the errorrate in large-vocabulary speech recognition.
Our approach to theproblem is twofold.
We first propose ascheme that optimizes thedegree of mixture tying for a given amount of training data andcomputational resources.
Experimental results on the Wall StreetJournal (WSJ) Corpus show that this new form of output distri-bution achieves a 25% reduction in error rate over typical tied-mixture systems.
We then show that an additional improvementcan be achieved by modeling local time correlation with lineardiscriminant features.1.
INTRODUCTIONTo improve the acoustic-modeling component of SRI's DECI-PHER TM speech recognition system, our research as focused ontwo main directions.
The first is to decrease the degree of mix-ture tying in the mixture observation densities, since conrinuous-density hidden Markov models (HMMs) have recently beenshown to outperform discrete-density and tied-mixture HMMs\[16\].
The second is the removal of the simplifying output inde-pendence assumption commonly used in HMMs.Tied mixtures (TM) achieve robust estimation and efficient com-putation of the density likelihoods.
However, the typical mixturesize used in TM systems is small and does not provide a goodrepresentation f the acoustic space.
Increasing the number ofthe mixture components (the codebook size) is not a feasiblesolution, since the mixture-weight distributions become toosparse.
In large-vocabulary problems, where a large number ofbasic HMMs is used and each has only a few observations in thetraining data, sparse mixture-weight distributions cannot be esti-mated robustly and are expensive to store.
To solve this problem,we follow the approach of simultaneously reducing the code-book size and increasing the number of different sets of mixturecomponents (or codebooks).
This procedure reduces the degreeof tying, and the two changes can be balanced so that the totalnumber of component densities in the system is effectivelyincreased.
The mapping from HMM states to codebooks can bedetermined using clustering techniques.
Since our algorithmtransforms a "less" continuous, or fled-mixture system, to a"more" continuous one, it has enabled us to investigate a numberof traditional differences between tied-mixture and fully continu-ous HMMs, including codebook size and modeling of the speechfeatures using multiple vs. single observation streams.Our second main research direction is focused on removing thesimplifying assumption used in HMMs that speech features fromdifferent frames are statistically independent given the underly-ing state sequence.
In this paper we will deal with the modelingof the local temporal dependencies, that is, ones that span theduration of a phonetic segment.
We will show through the use ofrecognition experiments and information theoretic criteria thatachieving decorrelation of the speech features is not a sufficientcondition for the improvement in recognition performance.
Toachieve the latter, it is necessary to improve the discriminationpower of the output distributions through the use of new infor-marion.
Local correlation modeling has recently been incorpo-rated in our system through the use of linear discriminantfeatures, and has reduced the word error rate by 7% on the WallStreet Journal (WSJ) corpus.The remainder of the paper is organized as follows: in Section 2we present the general form of mixture observation distributionsused in HMMs, we discuss variations of this form that haveappeared in the literature, and present an algorithm that enablesus to adjust the mixture tying for optimum recognition perfor-mance.
In Section 3 we deal with the problem of local time-cor-relation modeling: we comment on the potential improvement inrecognition performance by incorporating conditional distribu-tions, and describe the type of local consistency modeling cur-rently used in our system.
In Section 4 we present experimentalresults on the WSJ Corpus.
These results are mainly a by-productof the system development for the November 1993 ARPA evalu-ation \[16\].
Finally, we conclude in Section 5.2.
GENONIC  MIXTURESA typical mixture observation distribution in an HMM-basedspeech recognizer has the formP(XtlS) = E P(qJs)f(xtlq)q E--~(s)(i)where s represents the HMM state, x t the observed feature atflame t, and Q(s) the set of mixture-component densities used instate s. We will use the term codebook todenote the set Q(s).
Thestream of continuous vector observations can be modeleddirectly using Gaussians or other types of densities in the place313off(x t I q), and HMMs with this form of observation distributionsare known as continuous HMMs \[19\].Various forms of tying have appeared in the literature.
Whentying is not used, the sets of component densities are different fordifferent HMM states--that is, Q (s) ~ Q (s') if s # s'.
We willrefer to HMMs that use no sharing of mixture components asfully continuous HMMs.
The other extreme is when all HMMstates hare the same set of mixture components--that is, Q(s) =Q is independent of the state s. HMMs with this degree of shar-ing were proposed in \[8\], \[2\] under the names Semi-Continuousand Tied-Mixture (TM) HMMs.
Tied-mixture distributions havealso been used with segment-based models, and a good review isgiven !in \[11\].
Intermediate degrees of tying have also beenexamined.
In phone-based tying, described in \[17\], \[13\], onlyHMM states that belong to allophones of the same phone sharethe sanae mixture components--that is, Q(s) = Q(s') if s and s'are states of context-dependent HMMs with the same centerphone.
We will use the term phonetically tied to describe thiskind of tying.
Of course, for context-independent models, pho-netically tied and fully continuous HMMs are equivalent.
How-ever, phonetically tied mixtures (PTM) did not significantlyimprove recognition performance in previous work.The continuum between fully continuous and tied-mixtureHMMs can be sampled at any other point.
The choice of phonet-ically tied mixtures, although linguistically motivated, is some-what arbitrary and may not achieve the optimum trade-offbetween resolution and trainability.
We have recently introducedan algorithm \[4\] that allows as to select he degree of tying thatattains optimum recognition performance for the given computa-tional resources.
This algorithm follows a bootstrap approachfrom a system that has a higher degree of tying (i.e., a TM or aPTM system), and progressively unties the mixtures using threesteps: clustering, splitting and pruning, and reestirnafion.2.1.
C lus ter ingThe HMM states of all allophones of a phone are clustered fol-lowing an agglomerative procedure.
The clustering is based onthe weighted-by-counts entropy of the mixture-weight distribu-tions \[12\].
The clustering procedure partitions the set of HMMstates S into disjoint sets of statesS = S luS2u  .
.
.
uS n (2)The same codebooks will be used for all HMM states belongingto a particular cluster Si.2.2.
Splitting and PruningAfter determination f the sets of HMM states that will share thesame codebook, seed eodebooks for each set of states that will beused by the next re, estimation phase are constructed.
These seedcodebooks can be constructed by either one or a combination oftwo procedures:?
Identifying the most likely subset of mixture components ofthe boot system for each cluster of HMM states Si and usingthese subsets Q (Si) c Q (S) as seed codebooks for thenext phase?
Copying the original eodebook multiple times (one for eachcluster of states) and performing one iteration of the Baum-Welch algorithm over the training data with the new tyingscheme; the number of component densities in each code-book can then be reduced using clustering \[10\]2,3, Reest imat ionThe parameters are reestimated using the Baum-Welch algo-rithm.
This step allows the codebooks to deviate from the initialvalues and achieve abetter approximation f the distributions.We will refer to the Gaussian codebooks as genones and to theHMMs with arbitrary tying of Gaussian mixtures as genonicHMMs.
Clustering of either phone or subphone units in HMMshas also been used in \[18\], \[12\], \[1\], \[9\].
Mixture-weight cluster-ing of different HMM states can reduce the number of freeparameters in the system and, potentially, improve recognitionperformance because of the more robust estimation.
It cannot,however, improve the resolution with which the acoustic space isrepresented, since the total number of component densities in thesystem remains the same.
In our approach, we use clustering toidentify sets of subphonetic regions that will share mixture com-ponents.
The later steps of the algorithm, where the original setof mixture components is split into multiple overlapping genonesand each one is reestimated using data from the states belongingto the corresponding cluster, effectively increase the number ofdistinct densities in the system and provide the desired detail inthe resolution.Reestimation of the parameters can be achieved using the stan-dard Baum-Weleh reestimation formulae for HMMs with Gauss-Jan mixture observation densities, since tying does not alter theirform, as pointed out in \[21\].
During recognition, and to reducethe large amount of computation i volved in evaluating Gaussianlikelihoods, we can use the fast computational techniquesdescribed in \[15\].In place of the component densifiesf(x t I q) we use exponentiallyweighted Gaussian distributions:p(xtls) = qe~s)P(qLs )  \ [N(xt ;~q,~q)\]  a (3)where the exponent ?x ~ 1 is used to reduce the dynamic rangeof the Gaussian scores (that would, otherwise, dominate the mix-ture probabilities p(q / s)) and also to provide a smoothing effectat the tails of the Gaussians.3.
T IME CORRELAT ION MODEL INGFor a given HMM state sequence, the observed features atnearby frames are highly correlated.
Modeling time correlationcan significantly improve speech recognition performance fortwo reasons.
First, dynamic information is very important \[6\],and explicit time-correlation modeling can potentially outper-form more traditional and simplistic approaches like the incorpo-ration of cepstral derivatives as additional feature streams.314Second, sources of variability--such as microphone, vocal tractshape, speaker dialect, and speech rate--will not dominate thelikelihood computation during Viterbi decoding by being res-cored at every frame.
We will call techniques that model suchtemporal dependencies consistency modeling.The output-independence assumption is not necessary for thedevelopment of the HMM recognition (Viterbi) and training(Baum-Welch) algorithms.
Both of these algorithms can be mod-ified to cover the case when the features depend not only on thecurrent HMM state, but also on features at previous frames \[20\].However, with the exception of the work reported in \[3\] that wasbased on segment models, explicit ime-correlation modeling hasnot improved the performance of HMM-based speechrecognizers.To investigate these results, we conducted a pilot study to esti-mate the potential improvement in recognition performancewhen using explicit correlation modeling over more traditionalmethods like rime-derivative information.
We used information-theoretic riteria and measured the amount of mutual informa-tion between the current HMM state and the eepstral coefficientsat a previous "history" frame.
The mutual information wasalways conditioned on the identity of the left phone, and wasmeasured under three different conditions:I(h.s)--mutual information between the current HMM state sand a cepstral coefficient h at the history frame; a single, left-context-dependent Gaussian distribution for the cepstralcoefficient at the history frame was hypothesized,?
l(h,s/c)--conditional mutual information between the cur-rent HMM state s and a cepstral coefficient h at the historyframe when the corresponding cepstral coefficient c of thecurrent frame is given; a left-context-dependent, jointGauss-Jan distribution for the cepstral coefficients at the current andthe history frames was hypothesized,?
l(h,s/c,d)--same as above, but conditioned on both the ceps-tral coefficient c and its corresponding derivative d at thecurrent frame.The results are summarized in Table 1 for history frames withtags of 1, 2, 4 and a variable one.
In the latter case, we conditionthe mutual information on features extracted at the last frame toof the previous HMM state, as located by a forced Viterbi align-meat.
We can see from this table that in the unconditional case,the cepstral coefficients at frames closer to the current one pro-vide more information about he identity of the current phone.However, the amount of additional information that these coeffi-cients provide when the knowledge of the current cepstra ndtheir derivatives i taken into account is smaller.
The additionalinformation in this case is larger for lags greater than 1, and ismaximum for the variable lag.These measurements predict hat the previous frame's observa-tion is not the optimal frame to use when conditioning a state'soutput distribution.
To verify this, and to actually evaluate recog-nition performance, we incorporated time-correlation modelingin an HMM system with genonic mixtures.
Specifically, we gen-eralized the Gaussian mixtures to mixtures of conditional Gauss-ians, with the current cepstral coefficient x t conditioned on thecorresponding cepstral coefficient Xto of the history frame to:Lag t o 0 1 2 4 Variablel I(h, s) 0.28 0.27 0.25 0.19 0.25I(h, s I c) 0 0.13 0.15 0.15 0.21I(h, s I c, d) 0 0.11 0.14 0.13 0.20Table 1.
Mutual informati0n (in bits) between HMM state s attime t and eepstral coefficient h at time t-t o for various lags;included is the conditional mutual information when thecorresponding cepstral coefficient and its derivative at rime t aregivenP(xt ls ,  xt-to) = EsP(q \ ]s ) f (x t \ [q ,  xto)q?
'-~-( )(4)We either eplaced the original unconditional distributions of thecepstral coefficients and their derivatives with the conditionalGaussian distributions, or we used them in parallel as additionalobservation streams.
The results on the 5,000-word recognitiontask of the WSI0 corpus are summarized in Table 2 for fixed-laghistory frames.
We can see that the recognition results are in per-feet agreement with the behavior predicted by the mutual-infor-marion study.
The improvements in recognition performanceover the system that does not use conditional distributions areactually proportional to the measured amount of conditionalmutual information at the various history frames.
However, theseimprovements are small and statistically insignificant, and indi-cate that the derivative features effectively model the localdynamics.Delay0124Word Error-- Word Error--Conditional only (%) Both (%) I(h, s I c, d)10.32 010.98 10.19 0.1110.50 9.65 0.1410.32 9.83 0.13Table 2.
Recognition rates on 5,000-word WSJ corpus withconditional distributions either eplacing the unconditional onesor used in parallelInstead of using conditional Gaussian distributions, one canalternatively choose to use features obtained with linear discrim-inants.
Local time correlation can be modeled by estimating thetransformations over multiple consecutive flames \[5\],\[7\].
Thisapproach as the additional advantage that it is computationallyless expensive, since the discriminant transformations can becomputed in the recognizer front end and only once at eachflame.
However, as we will see in the following section, lineardiscriminants gave only moderate improvements in recognitionperformance, and this is consistent with the conditional Gaussianresults of this section.
From the conditional information mea-surements that we have presented, we can see that in order toprovide additional information to the recognizer we must condi-tion the output distributions not only on a previous history frame,but also on the start ime of the current subphonefic segment, andthis is an area that we are currently investigating.3154.
EXPERIMENTAL  RESULTSWe used the algorithms described in this paper on the 5,000- and64,000-word ecognition tasks of the WSJ corpus.
We used theprogressive-search framework \[14\] for fast experimentation.With this approach, an initial fast recognition pass creates wordlattices for all sentences in the development set.
These word lat-tices are used to constrain the search space in all subsequentexperiments.
In our development we used both the WSJ0 5,000word and the WSJ1 64,000 word portions of the database, andthe baseline bigram and trigram language models provided byLincoln Laboratory.4.1.
Degree of Mixture TyingTo determine the effect of mixture tying on the recognition per-formance, we evaluated a number of different systems on bothWSJ0 and WSJ1.
Table 3 compares the performance and thenumber of free parameters of fled mixtures, phonetically fledmixtures, and genonic mixtures on a development set that con-sists of 18 male speakers and 360 sentences of the 5,000-wordWS\]0 task.
The training data for this experiment included 3,500sentences from 42 speakers.
We can see that systems with asmaller degree of tying outperform the conventional fled mix-tures by 25%, and at the same time have a smaller number of freeparameters because of the reduction i  the codebook size.TotalNumber of Gaussians Parameters WordSystem Genones per genone (thousands) Error (%)TM 1 256 5,126 14.1PTM 40 100 2,096 11.6Genones 495 48 1,530 10.6Table 3.
Comparison of various degrees of tying on 5,000-wordWSJ development setThe difference in recognition performance between PTM andgenonie HMMs with smaller tying is, however, much more dra-marie in the WSJ1 portion of the database.
The training data con-sisted of 37,000 sentences from 280 speakers, and gender?dependent models were built.
The male subset of the 20,000-word November 1992 evaluation set was used, with a bigrarnlanguage model.
Table 4 compares various degrees of tying byvarying the number of genones used in the system.
We earl seethat, because of the larger amount of available training data, theimprovement i  performance ofgenonie systems over PTM sys-tems is much larger (20%) than in our 5,000-word experiments.Moreover, the best performance is achieved for a larger numberof genones--l,700 instead of the 495 used in the 5,000-wordexperiments.
These results are depicted in Figure 1.PTMNumber ofGenones 40Word errorrate (%) 14.776012.3Genonic HMMs1250 1700 240011.8 11.4 l 12.0Table 4.
Recognition performance on the male subset of 20,000-word WSJ November 1992 ARPA evaluation set for variousnumbers of codebooks using a bigram language model.15142,14,~13.EH.5H"- \\\\\\\\\\\xk.WSJ0WSJ1Number of GenonesFigure 1: Recognition performance for different degrees of tyingon the 5,000-word WSJ0 and 20,000-word WSJ1 tasks of theWSJ corpusIn Table 5 we explore the additional degree of freedom thatgenonie HMMs have over fully continuous HMMs, namely thatstates mapped to the same genone can have different mixtureweights.
We can see that ying the mixture weights in addition tothe Gaussians introduces a significant degradation i  recognitionperformance.
This degradation i creases when the features aremodeled using multiple observation streams (see following sec-tion) and as the amount of training data and the number ofgenones decrease.Number Number of Word Error (%)of G'enones Streams Tied Untied5K WSJ0 495 6 9.7 7.720KWSJ1 1,700 1 j 12.2 11.4Table 5.
Comparison of state-specific vs. genone-sp~ificmixture weights for different recognition tasks3164.2.
Multiple vs.
Single Observat ion  S t reamsAnother traditional difference between fully continuous and tiedmixture systems is the independence assumption of the latterwhen modeling multiple speech features.
Tied mixture systemstypically model static and dynamic spectral and energy featuresas conditionally independent observation streams given theHMM state, because tied mixture systems provide a very coarserepresentation f the acoustic space.
It is, therefore, necessary to"quantize" each feature separately and artificially increase theresolution by modeling the features as independent: the numberof "bins" of the augmented feature is equal to the product of thenumber of "bins" of all individual features.
The disadvantage is,of course, the independence assumption.
When, however, thedegree of tying is smaller, the finer representation f the acousticspace makes it unnecessary to artificially improve the resolutionaccuracy by modeling the features as independent.
Hence, forsystems that are loosely tied we can remove the feature-indepen-dence assumption.
This claim is verified experimentally in Table6.
The first row shows the recognition performance of a systemthat models the six static and dynamic spectral and energy fea-tures used in DECIPHER TM as independent observation streams.The second row shows the performance of a system that modelsthe six features in a single stream.
We can see that the perfor-mance of the two systems i  similar.WordSystem Sub (%) Del (%) Ins (%) Error (%)6 streams 9.0 0.8 2.5 12.31 stream 8.7 0.8 2.3 11.8Table 6.
Comparison of modeling using 6 versus 1 observationstreams for 6 underlying features on the male subset of 20,000-word WSI November 1992 evaluation set with a bigrarnlanguage model4.3.
Linear Discriminant FeaturesTo capture local time correlation we used a linear discriminantfeature xtracted using a transformation f the features within awindow around the current frame.
The discriminant transforma-tion was obtained using linear discriminant analysis with classesdefined as the HMM state of the context-independent phone.
Thestate index that was assigned to the frame was determined usingthe maximum a-posteriori criterion and the forward-backwardalgorithm.We found that the performance of the linear discriminant featurewas similar to that of the original features.
However, we foundthat an improvement in performance an be obtained if the dis-cnminant features are used in parallel with the original features.A genonic HMM system with 1,700 genones and linear discrimi-nants as an additional feature was evaluated on the 20,000-wordopen-vocabulary November 1993 ARPA evaluation set.
Itachieved word-error rates of 16.5% and 14.5% with the standardbigram and trigram language models, respectively.
These results,however, were contaminated bythe presence of a large DC offsetin most of the waveforms of the phase 1 WSI1 corpus.
We laterremoved the DC offset from the waveforms, and reestimated themodels using the exact procedure followed during the develop-ment of the system used in the November 1993 evaluation.
FromTable 6, we can see that the linear discriminant feature reducedSystem Bigram LM Trigram LM1,700 Genones 20.5 17.0+ Linear Discriminants 19.1 15.8Table 7.
Word error rates (%) on the 20,000-word open-vocabulary male development set of the WSJ1 corpus with andwithout linear discriminant transformationsthe error rate on the WSJ1 20,000-word open-vocabulary maledevelopment set by approximately 7% using either a bigram or atrigram language model.
Table 4 presents the results of the sys-tem with linear diseriminants on various test and developmentsets.Test setGrammar Nov92 WSJ1 Dev Nov93Bigram 11.2 16.6 16.2Trigrarn 9.3 13.6 13.6Table 8.
Word error rates on the November 1992 evaluation, theWSI1 development, and the November 1993 evaluation setsusing 20,000-word open-vocabulary bigrarn and trigramlanguage models5.
CONCLUSIONSNew acoustic modeling techniques ignificantly decrease theerror rate in large-vocabulary continuous peech recognition.The genonic HMMs balance the trade-off between resolution andtrainability, and achieve the degree of tying that is best suited tothe available training data and computational resources.
Forexample, one can decrease the computational load by decreasingthe number of genones (i.e., increasing the degree of tying) witha small penalty in recognition performance \[15\].
Our results onthe various test sets represent state-of-the-art recognition perfor-mance on the 20,000-word open-vocabulary WSI task.ACKNOWLEDGMENTSWe gratefuUy acknowledge support for this work from ARPAthrough Office of Naval Research Contract N00014-92-C-0154.The Government has certain fights in this material.
Any opin-ions, findings, and conclusions or recommendations expressed inthis material are those of the authors and do not necessarilyreflect he views of the Government funding agencies.REFERENCESL.
R. Bahl, P. V. de Souza, P. S. Gopalakrishnan, D. Naha-moo and M. A. Picheny, "Context Dependent Modeling ofPhones in Continuous Speech Using Decision Trees,"317DARPA Workshop on Speech and Natural Language, pp.264-269, February 1991.2.
J.R. Bellegarda nd D. Nahamoo, "Tied Mixture ContinuousParameter Modeling for Speech Recognition," IEEE Trans.ASSP, Vol.
38(12), pp.
2033-2045, Dec. 1990.3.
V. Digalalds, J. R. Rohlicek and M. Ostendorf, "ML Estima-tion of a Stochastic Linear System with the EM Algorithmand its Application to Speech Recognition," IEEE Trans.Speech and Audio Processing, October 1993.4.
V. Digalakis and H. Murveit, "Genones: Optimizing theDegree of Tying in a Large Vocabulary HMM-based SpeechRecognizer," to appear in Proc.
ICASSP, 1994.5.
G.R.
Doddington, "Phonetically Sensitive Discriminants forImproved Speech Recognition," Proceedings ICASSP-89,pp.
556-559.6.
S. Furui, "On the Role of Spectral Transition for Speech Per-ception," Journal of the Acoustical Society of America, vol.80(4), pp.
1016-1025, October 1986.7.
R. Haeb-Umbach and H. Ney, "Linear Discriminant Analy-sis for Improved Large Vocabulary Continuous Speech Rec-ognition," Proc.
ICASSP, pp.
1-13 - 1-16, March 1992.8.
X.D.
Huang and M. A. Jack, "Performance ComparisonBetween Semi-continuous and Discrete Hidden MarkovModels," lEE Electronics Letters, Vol.
24 no.
3, pp.
149-150.9.
M.-Y.
Hwang and X. D. Huang, "Subphonetic Modelingwith Markov States - Senone," Proc.
ICASSP, pp.
1-33-36,March 1992.10.
A. Kannan, M. Ostendorf and J. R. Rohlicek, "MaximumLikelihood Clustering of Gaussians for Speech Recogni-tion," in IEEE Trans.
Speech and Audio Processing, toappear July 1994.11.
O. Kimball and M. Ostendorf, "On the Use of Tied-MixtureDistributions," Proc.
ARPA HLT Workshop, March 1993.12.
K. F. Lee, "Context-Dependent Phonetic Hidden MarkovModels for Speaker-Independent Continuous Speech Recog-nition," IEEE Trans.
ASSP, pp.
599-609, April 1990.13.
C. Lee, L. Rabiner, R. Pieraccini and J. Wilpon, "AcousticModeling for Large Vocabulary Speech Recognition," Com-puter Speech and Language, April.
1990, pp.
127-165.14.
H. Murveit, J. Butzberger, V. Digalalds and M. Weintraub,"Large Vocabulary Dictation using SKI's DECIPHER TMSpeech Recognition System: Progressive Search Tech-niques," Proc.
ICASSP, pp.
II-319 - II-322, April 1993.15.
H. Murveit, P. Monaco, V. Digalakis and J. Butzberger,"Techniques to Achieve an Accurate Real-Time Large-Vocabulary Speech Recognition System," this proceedings.16.
D. Pallet, J. G. Fiscus, W. M. Fisher and J, S. Garofolo,"1993 Benchmark Tests for the ARPA Spoken LanguageProgram," this proceedings.17.
D. B. Paul, "The Lincoln Robust Continuous Speech Recog-nizer," Proc.
ICASSP, pp.
449-452, May 1989.18.
D. B. Paul and E. A. Martin, "Speaker Stress-resistant Con-tinuous Speech Recognition," Proc.
ICASSP, pp.
283-286,April 1988.19.
L. R. Rabiner, B. H. Juang, S. E. Levinson and M. M. Son-dhi, "Recognition of Isolated Digits Using Hidden MarkovModels with Continuous Mixture Densities," Bell SystemsTech.
Journal, Vol.
64(6), pp.
1211-34, 1985.20.
WeUekens, C., "Explicit Time Correlation in Hidden MarkovModels for Speech Recognition," Proc.
ICASSP-87.21.
S. J.
Young, "The General Use of Tying in Phoneme-BasedHMM Speech Recognizers," Proc.
ICASSP, pp.
1-569 - 1-572, March 1992.318
