Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 163?170,Baltimore, Maryland USA, June 26?27, 2014.c?2014 Association for Computational LinguisticsLarge-scale Exact Decoding: The IMS-TTT submission to WMT14?Daniel QuernheimIMSUniversity of Stuttgartdaniel@ims.uni-stuttgart.deFabienne CapCISLudwig-Maximilian University of Munichcap@cis.uni-muenchen.deAbstractWe present the IMS-TTT submission toWMT14, an experimental statistical tree-to-tree machine translation system basedon the multi-bottom up tree transducer in-cluding rule extraction, tuning and decod-ing.
Thanks to input parse forests anda ?no pruning?
strategy during decoding,the obtained translations are competitive.The drawbacks are a restricted coverageof 70% on test data, in part due to ex-act input parse tree matching, and a rela-tively high runtime.
Advantages includeeasy redecoding with a different weightvector, since the full translation forests canbe stored after the first decoding pass.1 IntroductionIn this contribution, we present an implementationof a translation model that is based on `MBOT(the multi bottom-up tree transducer of Arnold andDauchet (1982) and Lilin (1978)).
Intuitively, anMBOT is a synchronous tree sequence substitutiongrammar (STSSG, Zhang et al.
(2008a); Zhang etal.
(2008b); Sun et al.
(2009)) that has discon-tiguities only on the target side (Maletti, 2011).From an algorithmic point of view, this makes theMBOT more appealing than STSSG as demon-strated by Maletti (2010).
Formally, MBOT isexpressive enough to express all sensible trans-lations (Maletti, 2012)1.
Figure 2 displays sam-ple rules of the MBOT variant, called `MBOT,?This work was supported by Deutsche Forschungsge-meinschaft grants Models of Morphosyntax for StatisticalMachine Translation (Phase 2) and MA/4959/1?1.1A translation is sensible if it is of linear size increaseand can be computed by some (potentially copying) top-downtree transducer.that we use (in a graphical representation of thetrees and the alignment).
Recently, a shallow ver-sion of MBOT has been integrated into the popularMoses toolkit (Braune et al., 2013).
Our imple-mentation is exact in the sense that it does abso-lutely no pruning during decoding and thus pre-serves all translation candidates, while having nomechanism to handle unknown structures.
(Weadded dummy rules that leave unseen lexical ma-terial untranslated.)
The coverage is thus limited,but still considerably high.
Source-side and target-side syntax restrict the search space so that decod-ing stays tractable.
Only the language model scor-ing is implemented as a separate reranker2.
Thishas several advantages: (1) We can use input parseforests (Liu et al., 2009).
(2) Not only is the out-put optimal with regard to the theoretical model,also the space of translation candidates can be ef-ficiently stored as a weighted regular tree gram-mar.
The best translations can then be extractedusing the k-best algorithm by Huang and Chiang(2005).
Rule weights can be changed without theneed for explicit redecoding, the parameters of thelog-linear model can be changed, and even newfeatures can be added.
These properties are espe-cially helpful in tuning, where only the k-best al-gorithm has to be re-run in each iteration.
A modelin similar spirit has been described by Huang et al.
(2006); however, it used target syntax only (usinga top-down tree-to-string transducer backwards),and was restricted to sentences of length at most25.
We do not make such restrictions.The theoretical aspects of `MBOT and their usein our translation model are presented in Section 2.Based on this, we implemented a machine transla-tion system that we are going to make available to2Strictly speaking, this does introduce pruning into thepipeline.163the public.
Section 4 presents the most importantcomponents of our `MBOT implementation, andSection 5 presents our submission to the WMT14shared translation task.2 Theoretical ModelIn this section, we present the theoretical genera-tive model that is used in our approach to syntax-based machine translation: the multi bottom-uptree transducer (Maletti, 2011).
We omit the tech-nical details and give graphical examples only toillustrate how the device works, but refer to the lit-erature for the theoretical background.
Roughlyspeaking, a local multi bottom-up tree transducer(`MBOT) has rules that replace one nonterminalsymbol N on the source side by a tree, and a se-quence of nonterminal symbols on the target sidelinked to N by one tree each.
These trees againhave linked nonterminals, thus allowing furtherrule applications.Our `MBOT rules are obtained automaticallyfrom data like that in Figure 1.
Thus, we (word)align the bilingual text and parse it in both thesource and the target language.
In this manner weobtain sentence pairs like the one shown in Fig-ure 1.
To these sentence pairs we apply the ruleextraction method of Maletti (2011).
The rulesextracted from the sentence pair of Figure 1 areshown in Figure 2.
Note the discontiguous align-ment of went to ist and gegangen, resulting in dis-contiguous rules.The application of those rules is illustrated inFigure 3 (a pre-translation is a pair consisting of asource tree and a sequence of target trees).
Whileit shows a synchronous derivation, our main usecase of `MBOT rules is forward application or in-put restriction, that is the calculation of all targettrees that can be derived given a source tree.
Fora given synchronous derivation d, the source treegenerated by d is s(d), and the target tree is t(d).The yield of a tree is the string obtained by con-catenating its leaves.Apart from `MBOT application to input trees,we can even apply `MBOT to parse forests andeven weighted regular tree grammars (RTGs)(F?ul?op and Vogler, 2009).
RTGs offer an ef-ficient representation of weighted forests, whichare sets of trees such that each individual tree isequipped with a weight.
This representation iseven more efficient than packed forests (Mi et al.,2008) and moreover can represent an infinite num-ber of weighted trees.
The most important prop-erty that we utilize is that the output tree languageis regular, so we can represent it by an RTG (cf.preservation of regularity (Maletti, 2011)).
In-deed, every input tree can only be transformed intofinitely many output trees by our model, so for agiven finite input forest (which the output of theparser is) the computed output forest will also befinite and thus regular.3 Translation ModelGiven a source language sentence e and corre-sponding weighted parse forest F (e), our trans-lation model aims to find the best correspondingtarget language translation g?;3i.e.,g?
= argmaxgp(g|e) .We estimate the probability p(g|e) through a log-linear combination of component models with pa-rameters ?mscored on the derivations d such thatthe source tree of d is in the parse forest of e andthe yield of the target tree reads g. WithD(e, g) = {d | s(d) ?
F (e) and yield(t(d)) = g},we thus have:4p(g|e) ?
?d?D(e,g)11?m=1hm(d)?mOur model uses the following features hm(?)
for aderivation:(1) Translation weight normalized by source rootsymbol(2) Translation weight normalized by all rootsymbols(3) Translation weight normalized by leaves onthe source side(4) Lexical translation weight source?
target(5) Lexical translation weight target?
source(6) Target side language model: p(g)(7) Number of words in g(8) Number of rules used in the derivation(9) Number of gaps in the target side sequences(10) Penalty for rules that have more lexical ma-terial on the source side than on the target sideor vice versa (absolute value)3Our main translation direction is English to German.4While this is the clean theoretical formulation, we maketwo approximations to D(e, g): (1) The parser we use returnsa pruned parse forest.
(2) We only sum over derivations withthe same target sentence that actually appear in the k-best list.164SNPNNPMaxVPVBDwentNPNNhomeS-TOPPN-SB-Nom.Sg.MascNE-HD-Nom.Sg.MascMaxVAFIN-HD-SgistVP-OC/ppPP-MO/VAPPR-ACnachADJD-HD-Pos/NhauseVVPP-HDgegangenFigure 1: Aligned parsed sentences.NPNNPMax?(PN-SB-Nom.Sg.MascNE-HD-Nom.Sg.MascMax)VBDwent?(VAFIN-HD-Sgist,VVPP-HDgegangen)NPNNhome?
(PP-MO/VAPPR-ACnachADJD-HD-Pos/Nhause)VPVBD NP?
(VAFIN-HD-Sg,VP-OC/ppPP-MO/V VVPP-HD)SNP VP?
(S-TOPPN-SB-Nom.Sg.Masc VAFIN-HD-Sg VP-OC/pp)Figure 2: Extracted rules.
(11) Input parse tree probability assigned to s(t)by the parser of eThe rule weights required for (1) are relativefrequencies normalized over all extracted ruleswith the same root symbol on the left-hand side.
Inthe same fashion the rule weights required for (2)are relative frequencies normalized over all ruleswith the same root symbols on both sides.
Thelexical weights for (4) and (5) are obtained by mul-tiplying the word translations w(gi|ej) [respec-tively, w(ej|gi)] of lexically aligned words (gi, ej)across (possibly discontiguous) target side se-quences.5Whenever a source word ejis alignedto multiple target words, we average over the wordtranslations:6h4(d)=?lexical iteme occurs in s(d)average {w(g|e) | g aligned to e}4 ImplementationOur implementation is very close to the theoreticalmodel and consists of several independent compo-5The lexical alignments are different from the links usedto link nonterminals.6If the word ejhas no alignment to a target word, thenit is assumed to be aligned to a special NULL word and thisalignment is scored.nents, most of which are implemented in Python.The system does not have any dependencies otherthan the need for parsers for the source and tar-get language, a word alignment tool and option-ally an implementation of some tuning algorithm.A schematic depiction of the training and decod-ing pipeline can be seen in Figure 4.Rule extraction From a parallel corpus ofwhich both halves have been parsed and wordaligned, multi bottom-up tree transducer rules areextracted according to the procedure laid out in(Maletti, 2011).
In order to handle unknownwords, we add dummy identity translation rulesfor lexical material that was not present in thetraining data.Translation model building Given a set ofrules, translation weights (see above) are com-puted for each unique rule.
The translation modelis then converted into a source, a weight and a tar-get model.
The source model (an RTG representedin an efficient binary format) is used for decod-ing and maps input trees to trees over rule iden-tifiers representing derivations.
The weight modeland the target model can be used to reconstruct theweight and the target realization of a given deriva-tion.165Composing 3 rules:VPVBD NP?
(VAFIN-HD-Sg,VP-OC/ppPP-MO/V VVPP-HD)VBDwent?(VAFIN-HD-Sgist,VVPP-HDgegangen)NPNNhome?
(PP-MO/VAPPR-ACnachADJD-HD-Pos/Nhause)Obtained pre-translation:VPVBDwentNPNNhome?
(VAFIN-HD-Sgist,VP-OC/ppPP-MO/VAPPR-ACnachADJD-HD-Pos/NhauseVVPP-HDgegangen)Figure 3: Synchronous rule application.Figure 4: Our machine translation system.166Decoder The decoder transforms a forest of in-put sentence parse trees to a forest of transla-tion derivations by means of forward application.These derivations are trees over the set of rules(represented by rule identifiers).
One of the mostuseful aspects of our model is the fact that decod-ing is completely independent of the weights, asno pruning is performed and all translation candi-dates are preserved in the translation forest.
Thus,even after decoding, the weight model can bechanged, augmented by new features, etc.
; eventhe target model can be changed, e.g.
to supportparse tree output instead of string output.
In allof our experiments, we used string output, but it isconceivable to use other realizations.
For instance,a syntactic language model could be used for out-put tree scoring.
Also, recasing is extremely easywhen we have part-of-speech tags to base our de-cision on (proper names are typically uppercase,as are all nouns in German).Another benefit of having a packed representa-tion of all candidates is that we can easily checkwhether the reference translation is included in thecandidate set (?force decoding?).
The freedom toallow arbitrary target models that rewrite deriva-tions is related to current work on interpreted reg-ular tree grammars (Koller and Kuhlmann, 2011),where arbitrary algebras can be used to compute arealization of the output tree.k-best extractor From the translation derivationRTGs, a k-best list of derivations can be extracted(Huang and Chiang, 2005) very efficiently.
Thisis the only step that has to be repeated if the ruleweights or the parameters of the log-linear modelchange.
The derivations are then mapped to tar-get language sentences (if several derivations re-alize the same target sentence, their weights aresummed) and reranked according to a languagemodel (as was done in Huang et al.
(2006)).
Thisis the only part of the pipeline where we deviatefrom the theoretical log-linear model, and this iswhere we might make search errors.
In principle,one could integrate the language model by inter-section with the translation model (as the statefulMBOT model is closed under intersection with fi-nite automata), but this is (currently) not computa-tionally feasible due to the size of models.Tuning Minimum error rate training (Och,2003) is implemented using Z-MERT7(Zaidan,7http://cs.jhu.edu/?ozaidan/zmert/2009).
A set of source sentences has to be (forest-)parsed and decoded; the translation forests arestored on disk.
Then, in each iteration of Z-MERT,it suffices to extract k-best lists from the transla-tion forests according to the current weight vector.5 WMT14 Experimental setupWe used the training data that was made avail-able for the WMT14 shared translation task onEnglish?German8.
It consists of three parallel cor-pora (1.9M sentences of European parliament pro-ceedings, 201K sentences of newswire text, and2M sentences of web text) and additional mono-lingual news data for language model training.The English half of the parallel data was parsedusing Egret9which is a re-implementation of theBerkeley parser (Petrov et al., 2006).
For the Ger-man parse, we used the BitPar parser (Schmid,2004; Schmid, 2006).
The BitPar German gram-mar is highly detailed, which makes the syntac-tic information contained in the parses extremelyuseful.
Part-of-speech tags and category label areaugmented by case, number and gender informa-tion, as can be seen in the German parse tree inFigure 1.
We only kept the best parse for eachsentence during training.
After parsing, we pre-pared three versions of the German corpus: a)RAW, with no morphological post-processing; b)UNSPLIT, using SMOR, a rule-based morpho-logical analyser (Schmid et al., 2004), to reducewords to their base form; c) SPLIT, using SMORto reduce words to their base form and split com-pound nouns.
After translation, compounds weremerged again, and words were re-inflected.
Pre-vious experiments using SMOR to lemmatise andsplit compounds in phrase-based SMT showed im-proved translation performances, see (Cap et al.,2014a) for details.We then trained three 5-gram language modelson monolingual data using KenLM10(Heafield,2011; Heafield et al., 2013 to appear) for thethree setups.
For SPLIT and UNSPLIT, we wereonly able to use the German side of the paralleldata, since parsing is a prerequisite for our mor-phological post-processing and we did not havethe resources to parse more data.
For RAW, weadditionally used the monolingual German data8http://www.statmt.org/wmt14/translation-task.html9https://sites.google.com/site/zhangh1982/egret10http://kheafield.com/code/kenlm/167system BLEU BLEU-cased TERRAW 17.0 16.4 .770UNSPLIT 16.4 15.8 .773SPLIT 16.3 15.7 .773Table 1: BLEU and TER scores of the submittedsystems.that was distributed for the shared task.
Wordalignment for all three setups was achieved usingGIZA++11.
As usual, we discarded sentence pairswhere one sentence was significantly longer thanthe other, as well as those that were too long or tooshort.For tuning, we chose the WMT12 test set (3,003sentences of newswire text), available as partof the development data for the WMT13 sharedtranslation task.
Since our system had limited cov-erage on this tuning set, we limited ourselves tothe first a subset of sentences we could translate.When translating the test set, our models usedparse trees delivered by the Egret parser.
Aftertranslation, recasing was done by examining theoutput syntax tree, using a simple heuristics look-ing for nouns and sentence boundaries.
Since cov-erage on the test set was also limited, we used thesystems as described in (Cap et al., 2014b)12as afallback to translate sentences that our system wasnot able to translate.6 ResultsWe report the overall translation quality, as listedon http://matrix.statmt.org/, mea-sured using BLEU (Papineni et al., 2002) andTER (Snover et al., 2006), in Table 1.We assume that the poor performance of UN-SPLIT and SPLIT compared to RAW is due to thefact that we use a significantly smaller languagemodel (as explained above) for these two settings.A detailed analysis will follow after the end of themanual evaluation period.7 Conclusion and further workWe presented our submission to the WMT14shared translation task based on a novel, promising?full syntax, no pruning?
tree-to-tree approach tostatistical machine translation, inspired by Huang11https://code.google.com/p/giza-pp/12We use raw as described in (Cap et al., 2014b) as a fall-back for RAW, RI for UNSPLIT and CoRI for SPLIT.et al.
(2006).
There are, however, still major draw-backs and open problems associated with our ap-proach.
Firstly, the coverage can still be signifi-cantly improved.
In these experiments, our modelwas able to translate only 70% of the test sen-tences.
To some extent, this number can be im-proved by providing more training data.
Also,more rules can be extracted if we not only use thebest parse for rule extraction, but multiple parsetrees, or even switch to forest-based rule extrac-tion (Mi and Huang, 2008).
Finally, the size of theinput parse forest plays a role.
For instance, if weonly supply the best parse to our model, transla-tion will fail for approximately half of the input.However, there are inherent coverage limits.Since our model is extremely strict, it will neverbe able to translate sentences whose parse treescontain structures it has never seen before, sinceit has to match at least one input parse tree ex-actly.
While we implemented a simple solution tohandle unknown words, the issue with unknownstructures is not so easy to solve without breakingthe otherwise theoretically sound approach.
Pos-sibly, glue rules can help.The second drawback is runtime.
We wereable to translate about 15 sentences per hour onone processor.
Distributing the translation taskon different machines, we were able to translatethe WMT14 test set (10k sentences) in roughlyfour days.
Given that the trend goes towards par-allel programming, and considering the fact thatour decoder is written in the rather slow languagePython, we are confident that this is not a majorproblem.
We were able to run the whole pipelineof training, tuning and evaluation on the WMT14shared task data in less than one week.
We are cur-rently investigating whether A* k-best algorithms(Pauls and Klein, 2009; Pauls et al., 2010) can helpto guide the translation process while maintainingoptimality.Thirdly, currently the language model is not in-tegrated, but implemented as a separate rerank-ing component.
We are aware that this might in-troduce search errors, and that an integrated lan-guage model might improve translation quality(see e.g.
Chiang (2007) where 3?4 BLEU pointsare gained by LM integration).
Some research onthis topic already exists, e.g.
(Rush and Collins,2011) who use dual decomposition, and (Aziz etal., 2013) who replace intersection with an upperbound which is easier to compute.168ReferencesAndr?e Arnold and Max Dauchet.
1982.
Morphismeset bimorphismes d?arbres.
Theoret.
Comput.
Sci.,20(1):33?93.Wilker Aziz, Marc Dymetman, and Sriram Venkatap-athy.
2013.
Investigations in exact inference forhierarchical translation.
In Proc.
8th WMT, pages472?483.Fabienne Braune, Nina Seemann, Daniel Quernheim,and Andreas Maletti.
2013.
Shallow local multi-bottom-up tree transducers in statistical machinetranslation.
In Proc.
51th ACL, pages 811?821.Fabienne Cap, Alexander Fraser, Marion Weller, andAoife Cahill.
2014a.
How to Produce UnseenTeddy Bears: Improved Morphological Processingof Compounds in SMT.
In Proc.
14th EACL.Fabienne Cap, Marion Weller, Anita Ramm, andAlexander Fraser.
2014b.
CimS ?
The CIS and IMSjoint submission to WMT 2014 translating from En-glish into German.
In Proc.
9th WMT.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computat.
Linguist., 33(2):201?228.Zolt?an F?ul?op and Heiko Vogler.
2009.
Weighted treeautomata and tree transducers.
In Manfred Droste,Werner Kuich, and Heiko Vogler, editors, Hand-book of Weighted Automata, EATCS Monographson Theoret.
Comput.
Sci., chapter 9, pages 313?403.Springer.Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.Clark, and Philipp Koehn.
2013 (to appear).
Scal-able modified Kneser-Ney language model estima-tion.
In Proc.
51st ACL.Kenneth Heafield.
2011.
KenLM: faster and smallerlanguage model queries.
In Proc.
6th WMT, pages187?197.Liang Huang and David Chiang.
2005.
Better k-bestparsing.
In Proc.
IWPT, pages 53?64.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.Statistical syntax-directed translation with extendeddomain of locality.
In Proc.
7th Conf.
AMTA, pages66?73.Alexander Koller and Marco Kuhlmann.
2011.
A gen-eralized view on parsing and translation.
In Proc.IWPT, pages 2?13.Eric Lilin.
1978.
Une g?en?eralisation des transducteursd?
?etats finis d?arbres: les S-transducteurs.
Th`ese3`eme cycle, Universit?e de Lille.Yang Liu, Yajuan L?u, and Qun Liu.
2009.
Improvingtree-to-tree translation with packed forests.
In Proc.47th ACL, pages 558?566.Andreas Maletti.
2010.
Why synchronous tree sub-stitution grammars?
In Proc.
HLT-NAACL, pages876?884.Andreas Maletti.
2011.
How to train your multibottom-up tree transducer.
In Proc.
49th ACL, pages825?834.Andreas Maletti.
2012.
Every sensible extended top-down tree transducer is a multi bottom-up tree trans-ducer.
In Proc.
HLT-NAACL, pages 263?273.Haitao Mi and Liang Huang.
2008.
Forest-based trans-lation rule extraction.
In Proc.
EMNLP, pages 206?214.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-based translation.
In Proc.
46th ACL, pages 192?199.
ACL.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proc.
41st ACL,pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Weijing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proc.
40thACL, pages 311?318.Adam Pauls and Dan Klein.
2009.
K-best A* parsing.In Proc.
47th ACL, pages 958?966.Adam Pauls, Dan Klein, and Chris Quirk.
2010.
Top-down k-best A* parsing.
In Proc.
48th ACL, pages200?204.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and in-terpretable tree annotation.
In Proc.
COLING-ACL,pages 433?440.Alexander M. Rush and Michael Collins.
2011.
Ex-act decoding of syntactic translation models throughlagrangian relaxation.
In Proc.
49th ACL, pages 72?82.Helmut Schmid, Arne Fitschen, and Ulrich Heid.2004.
SMOR: A German Computational Morphol-ogy Covering Derivation, Composition and Inflec-tion.
In Proc.
4th LREC.Helmut Schmid.
2004.
Efficient parsing of highly am-biguous context-free grammars with bit vectors.
InProc.
20th COLING, pages 162?168.Helmut Schmid.
2006.
Trace prediction and recov-ery with unlexicalized PCFGs and slash features.
InProc.
44th ACL.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proc.
AMTA.Jun Sun, Min Zhang, and Chew Lim Tan.
2009.
A non-contiguous tree sequence alignment-based model forstatistical machine translation.
In Proc.
47th ACL,pages 914?922.169Omar F. Zaidan.
2009.
Z-MERT: A fully configurableopen source tool for minimum error rate training ofmachine translation systems.
The Prague Bulletin ofMathematical Linguistics, 91:79?88.Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,Chew Lim Tan, and Sheng Li.
2008a.
A treesequence alignment-based tree-to-tree translationmodel.
In Proc.
46th ACL, pages 559?567.Min Zhang, Hongfei Jiang, Haizhou Li, Aiti Aw, andSheng Li.
2008b.
Grammar comparison studyfor translational equivalence modeling and statisticalmachine translation.
In Proc.
22nd COLING, pages1097?1104.170
