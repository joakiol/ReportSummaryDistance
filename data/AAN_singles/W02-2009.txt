Cross-dataset Clustering: Revealing CorrespondingThemes Across Multiple CorporaIdo DAGANDepartment of Computer ScienceBar-Ilan UniversityRamat-Gan, Israel, 52900and LingoMotors Inc.dagan@lingomotors.comZvika MARXCenter for Neural ComputationThe Hebrew Universityand CS Dept., Bar-Ilan UniversityRamat-Gan, Israel, 52900zvim@cs.huji.ac.ilEli SHAMIRSchool of Computer Scienceand EngineeringThe Hebrew UniversityJerusalem, Israel, 91904shamir@cs.huji.ac.ilAbstractWe present a method for identifyingcorresponding themes across several corporathat are focused on related, but distinct,domains.
This task is approached throughsimultaneous clustering of keyword setsextracted from the analyzed corpora.
Ouralgorithm extends the information-bottleneck soft clustering method for asuitable setting consisting of severaldatasets.
Experimentation with topicalcorpora reveals similar aspects of threedistinct religions.
The evaluation is by wayof comparison to clusters constructedmanually by an expert.1 IntroductionThis paper addresses the problem of detectingcorresponding subtopics, or themes, withinrelated bodies of text.
Such task is typical tocomparative research, whether commercial orscientific: a conceivable application would aimat detecting corresponding characteristicsregarding, e.g., companies, markets, legalsystems or political organizations.Clustering has often been perceived as a meanfor extracting meaningful components from data(Tishby, Pereira and Bialek, 1999).
Regardingtextual data, clusters of words (Pereira, Tishbyand Lee, 1993) or documents (Lee and Seung,1999; Dhillon and Modha, 2001) are ofteninterpreted as capturing topics or themes thatplay prominent role in the analyzed texts.Our work extends the ?standard?
clusteringparadigm, which pertains to a single dataset.We address a setting in which several datasets,corresponding to related domains, are given.We focus on the comparative task of detectingthose themes that are expressed across severaldatasets, rather than discovering internal themeswithin each individual dataset.More specifically, we address the task ofclustering simultaneously multiple datasets suchthat each cluster includes elements from severaldatasets, capturing a common theme, which isshared across the sets.
We term this task cross-dataset (CD) clustering.In this article we demonstrate CD clusteringthrough detecting corresponding themes acrossthree different religions.
That is: we apply ourapproach to three sets of religion-relatedkeywords, extracted from three corpora, whichinclude encyclopedic entries and introductoryarticles regarding Buddhism, Christianity andIslam.
Each one of three representativekeyword-sets, which were extracted from theabove corpora, presumably encapsulates topicsand themes discussed within its source corpus.Our algorithm succeeds to reveal commonthemes such as scriptures, rituals and schoolsthrough respective keyword clusters consistingof terms such as Sutra, Bible and Quran;Full Moon, Easter and Id al Fitr; Theravada,Protestant and Shiite (see Table 1 below for adetailed depiction of our results).The CD clustering algorithm, presented inSection 2.2 below, extends the informationbottleneck (IB) soft clustering method.
Ourmodifications to the IB formulation enable theclustering algorithm to capture characteristicpatterns that run across different datasets, ratherthen being ?trapped?
by unique characteristics ofindividual datasets.Like other topic discovery tasks that areapproached by clustering, the goal of CDclustering is not defined in precise terms.
Yet, itis clear that its focus on detecting themes in acomparative manner, within multiple datasets,distinguishes CD clustering substantially fromthe standard single-dataset clustering paradigm.A related problem, of detecting analogiesbetween different information systems has beenaddressed in the past within cognitive research(e.g.
Gentner, 1983; Hofstadter et al, 1995).Recently, a related computational method fordetecting corresponding themes has beenintroduced (coupled clustering, Marx et al,2002).
The coupled clustering setting, however,being focused on detecting analogies, is limitedto two data sets.
Further, it requires similarityvalues between pairs of data elements as input:this setting does not seem straightforwardlyapplicable to the multiple dataset setting.
Ourmethod, in distinction, uses a more direct sourceof information, namely word co-occurrencestatistics within the analyzed corpora.
Anotherdifference is that we take the ?soft?
approach toclustering, producing probabilities ofassignments into clusters rather than adeterministic 0/1 assignment values.2 Algorithmic Framework2.1 Review of the IB Clustering AlgorithmThe information bottleneck (IB) iterativeclustering method is a recent approach to soft(probabilistic) clustering for a single set, denotedby X, consisting of elements to be clustered(Tishby, Pereira & Bialek, 1999).
Each elementx?X is identified by a probabilistic featurevector, with an entry, p(y|x), for every feature yfrom a pre-determined set of features Y.  Thep(y|x) values are estimated from given co-occurrence data:p(y|x)  = ?
?Yy yxcountyxcount' )',(),((hence ?y?Y  p(y|x) = 1 for every x in X).The IB algorithm is derived from informationtheoretic considerations that we do not addresshere.
It computes, through an iterative EM-likeprocess, probabilistic assignments p(c|x) foreach element x into each cluster c.  Starting withrandom (or heuristically chosen) p(c|x) values attime t = 0, the IB algorithm iterates thefollowing steps until convergence:IB1: Calculate for each cluster c its marginalprobability:?
?
?= Xx tt xcpxpcp )|()()( 1 .IB2: Calculate for each feature y and cluster ca conditional probability p(y|c):?
?
?= Xx tt cxpxypcyp )|()|()|( 1 .
(Bayes' rule is used to compute p(x|c)).IB3: Calculate for each element x and eachcluster c a value p(c|x), indicating the?probability of assignment?
of x into c:?= ' ,,)',()'(),()()|(cYttYttt cxsimcpcxsimcpxcp ?
?,with simtY,?
(x,c) = exp{?
?DKL[ p(y|x)|| pt(y|c) ]}(DKL is the Kullback-Leibler divergence, seeCover & Thomas, 1991).The parameter ?
controls the sensitivity of theclustering procedure to differences between thep(y|c) values.
The higher ?
is, the more?determined?
the algorithm becomes inassigning each element into the closest cluster.As ?
is increased, more clusters that areseparable from each other are obtained uponconvergence (the target number of clusters isfixed).
We want to ensure, however, thatassignments do not follow more than necessaryminute details of the given data, as a result oftoo high ?
(similarly to over generalization insupervised settings).
The IB algorithm istherefore applied repeatedly in a cooling-likeprocess: it starts with a low ?
value,corresponding to low temperature, which isincreased every repetition of the whole iterativeconverging cycle, until the desired number ofseparate clusters is obtained.2.2 The Cross-dataset (CD) Clustering MethodThe (soft) CD clustering algorithm receives asinput multiple datasets along with their featurevectors.
In the current application, we havethree sets extracted from the correspondingcorpora ?
XBuddhism, XChristianity, and XIslam ?
each of~150 keywords to be clustered.
A particularkeyword might appear in two or more of thedatasets, but the CD setting considers it as adistinct element within each dataset, thuskeeping the sets of clustered elements disjoint.Like the IB clustering algorithm, the CDalgorithm produces probabilistic assignments ofthe data elements.The feature set Y consists, in the current work, ofabout 7000 content words, each occurs in at leasttwo of the examined corpora.
The set offeatures is used commonly for all datasets, thusit underlies a common representation, whichenables the clustering process to compareelements of different sets.Naively approached, the original IB algorithmcould be utilized unaltered to the multiple-dataset setting, simply by applying it to theunified set X, consisting of the union of thedisjoint Xi's.
The problem of this simplisticapproach is that each dataset has its owncharacteristic features and feature combinations,which correspond to prominent topics discusseduniquely in that corpus.
A standard clusteringmethod, such as the IB algorithm, would have atendency to cluster together elements thatoriginate in the same dataset, producing clusterspopulated mostly by elements from a singledataset (cf.
Marx et al 2002).
The goal of CDclustering is to neutralize this tendency and tocreate clusters containing elements that sharecommon features across different datasets.To accomplish this goal, we change the criterionby which elements are assigned into clusters.Recall that the assignment of an element x to acluster c is determined by the similarity of theircharacterizing feature distributions, p(y|x) andp(y|c) (step IB3).
The problem lies in using thep(y|c) distribution, which is determined bysumming p(y|x) values over all cluster elements,to characterize a cluster without taking intoaccount dataset boundaries.
Thus, for a certainy, p(y|c) might be high despite of beingcharacteristic only for cluster elementsoriginating in a single dataset.
This results inthe tendency discussed above to favor clustersconsisting of elements of a single dataset.Therefore, we define a biased probabilitydistribution, p~c(y), to be used by the CDclustering algorithm for characterizing a clusterc.
It is designed to call attention to y's that aretypical for cluster members in all, or most,different datasets.
Consequently, an element xwould be assigned to a cluster c (as in step IB3)in accordance to the degree of similaritybetween its own characteristic features and thosecharacterizing other cluster members from alldatasets.
The resulting clusters would thuscontain representatives of all datasets.The definition of p~c(y) is based on the jointprobability p(y,c,Xi).
First, compute thegeometric mean of p(y,c,Xi) over all Xi, weightedby p(Xi):?
(y,c) = ?i (p(y,c,Xi)) p(Xi)(see Appendix below for the details of howp(Xi) and p(y,c,Xi) are calculated).?
is not a probability measure, but just afunction of y and c into [0,1].
However, since ageometric mean reflects ?uniformity?
of theaveraged values, ?
captures the degree to whichp(y,c,Xi) values are high across all datasets.We found empirically that at this stage, it isadvantageous to normalize ?
across all clustersand then to rescale the resulting probabilities(over the c's, for each y) by the original p(y):?
'(y,c) = (?
(y,c) / ?c' ?
(y,c') ) ?
p(y) .Finally, to obtain a probability distribution overy for each cluster c, normalize the obtained?
'(y,c) over all y's:p~c(y) = ?
'(y,c) / ?y' ?
'(y',c) .As explained, p~c(y) characterizes c (analogouslyto p(y|c) in IB), while ensuring that the feature-based similarity of c to any element x reflectsfeature distribution across all data sets.The CD clustering algorithm, starting at t = 0,iterates, in correspondence to the IB algorithm,the following steps:CD1: Calculate for each cluster c its marginalprobability (same as IB1):?
?
?= Ui iXx tt xcpxpcp )|()()( 1 .CD2:  Compute p~c(y) as described above.CD3: Compute p(c|x) (with p~c(y) playing therole played by p(y|c) in IB3):?= ' ,,)',()'(),()()|(cYttYttt cxSIMcpcxSIMcpxcp ?
?,with SIMtY,?
(x,c) = exp {?
?DKL[ p(y|x)|| ctp~ (y)]}.3 CD Clustering for Religion ComparisonThe three corpora that are focused on thecompared religions ?
Buddhism, Christianityand Islam ?
have been downloaded from theInternet.
Each corpus contains 20,000?40,000word tokens (5?10 Megabyte).
We have used atext mining tool to extract most religionkeywords that form the three sets to which weapplied the CD algorithm.
The software wehave used ?
TextAnalyst 2.0 ?
identifies withinthe corpora key-phrases, from which we haveexcluded items that appear in fewer than threeTable 1: Results of religion keyword CD clustering.
The authors have set the cluster titles.
For eachcluster c and each religion, the 15 keywords x with the highest probability of assignment within thecluster are displayed (assignment probabilities, i.e.
p(c|x) values are indicated in brackets).
Termsthat were used by the expert (see Table 2) are underlined.Buddhism Christianity Islam?1 (Cherished Qualities)god (.68), amida (.58), bodhisattva (.50),salvation (.45), enlightenment (.43), deva(.43), attain (.41), sacrifice (.39), awaken(.25), spirit (.25), nirvana (.24),buddha nature (.24), humanity (.22),speech (.18), teach (.18)god (.69), good works (.65), love of god(.62), salvation (.60), gift (.58), intercession(.56), repentance (.55), righteousness (.53),peace (.52), love (.51), obey god (.49),saviour (.48), atonement (.46), holy ghost(.45), jesus christ (.45)god (.86), one god (.84), allah (.76),bless (.76), worship (.75), submission(.73), peace (.73), command (.72),guide (.71), divinity (.70), messanger(.70), believe (.62), mankind (.61),commandment (.58), witness (.57)?2  (Customs and Festivals)full moon (.99), stupa (.98), mantra (.96),pilgrim (.96), monastery (.89), temple (.86),statue (.73), worship (.61), monk (.54),mandala (.32), trained (.23), bhikkhu (.15),disciple (.12), meditation (.11), nun (.11)easter (.99), sunday (.99), christmas (.99),service (.98), city (.98), eucharist (.96),pilgrim (.95), pentecost (.93), jerusalem(.91), pray (.89), worship (.82), minister (.73),ministry (.70), read bible (.50), mass (.24)id al fitr (.99), friday (.99), ramadan(.99), eid (.99), pilgrim (.99), mosque(.99), mecca (.99), kaaba (.99), salat(.99), fasting (.99), medina (.98), city(.98), pray (.98), hijra (.97), charity (.96)?3 (Spiritual States)phenomena (.94), problem (.93),mindfulness (.92), awareness (.92),consciousness (.91), law (.88), emptiness(.88), samadhi (.87), sense (.87),experience (.86), wisdom (.83), moral (.83),karma (.82), find (.81), exist (.80)moral (.96), problem (.94), argue (.91),question (.87), argument (.74), experience(.73), incarnation (.72), relationship (.71),idolatry (.58), find (.45), law (.41), learn (.38),confession (.34), foundation (.32), faith (.31)moral (.93), spirit (.79), question (.75),life (.71), freedom (.67), existence(.56), humanity (.53), find (.52), faith(.52), code (.51), law (.41), universe(.39), being (.36), teach (.35),commandment (.29)?4  (Sorrow, Sin, Punishment and Reward)lamentation (.99), grief (.99), animal (.89),pain (.87), death (.86), kill (.84),reincarnation (.81), realm (.76), samsara(.69), rebirth (.61), dukkha (.56), anger (.53),soul (.43), nirvana (.43), birth (.33)punish (.94), hell (.93), violence (.86), fish(.86), sin (.83), earth (.81), soul (.78), death(.77), sinner (.76), sinful (.74), heaven (.73),satan (.72), suffer (.71), flesh (.71),judgment (.67)hell (.97), earth (.88), heaven (.87),death (.85), sin (.85), alcohol (.69),satan (.60), face (.59),day of judgment (.52), deed (.48),angel (.25), being (.24), universe (.16),existence (.13), bearing (.12)?5 (Schools, Traditions and their Originating Places)korea (.99), china (.99), tibet (.99),theravada (.99), school (.99), asia (.99),founded (.99), west (.99), sri lanka (.99),mahayana (.99), india (.99), history (.99),hindu (.99), japan (.99), study (.99)cardinal (.99), orthodox (.99), protestant(.99), university (.99), vatican (.99), catholic(.99), bishop (.99), rome (.99), pope (.99),monk (.99), tradition (.99), theology (.99),baptist (.98), church (.98), divinity (.93)africa (.99), shiite (.99), sunni (.99),shia (.99), west (.99), christianity (.99),arab (.99), founded (.98), arabia (.97),sufi (.96), history (.96), fiqh (.95),scholar (.91), imam (.90), jew (.89)?6 (Names, Places, Characters, Narratives)gautama (.96), king (.95), friend (.68),disciple (.60), birth (.48), hear (.43), ascetic(.41), amida (.40), deva (.33), teach (.19),sacrifice (.15), statue (.14), buddha (.12),bodhisattva (.12), dharma (.09)bethlehem (.98), jordan (.97), mary (.95),lamb (.90), king (.90), second coming (.81),born (.76), israel (.74), child (.73), elijah (.72),baptize (.70), john the baptist (.68), priest(.68), adultery (.65), zion (.61)husband (.99), ismail (.98), father(.97), son (.95), mother (.94), born(.92), wife (.92), child (.89), ali (.88),musa (.71), isa (.70), ibrahim (.67),caliph (.43), tribe (.35), saint (.30)?7 (Scripture)tripitaka (.98), sanskrit (.94), translate (.93),sutra (.85), discourse (.79), pali canon(.74), story (.66), book (.64), word (.61), write(.45), buddha (.39), lama (.32), text (.23),dharma (.21), teacher (.17)hebrew (.99), translate (.99), gospels (.99),greek (.99), book (.98), new testament(.98), old testament (.96), passage (.96),matthew (.95), write (.94), luke (.93),apostle (.93),  bible (.91),  paul (.90),john (.90)translatee (.99), bible (.99), write (.98),book (.97), hadith (.96), sunna (.96),quran (.94), word (.93), story (.93),revelation (.88), companion (.80),muhammad (.80), prophet (.73),writing (.71), read quran (.46)corpus documents1.
Thus, composite and rareterms as well as phrases that the software hasinappropriately segmented were filtered out.
Wehave added to the automatically extracted termsadditional items contributed by a comparativereligion expert (about 15% of the sets were thusnot extracted automatically, but those termsoccur frequently enough to underlie informativeco-occurrence vectors).The common set of features consists of allcorpus words that occur in at least three differentdocuments within two or three of the corpora,excluding a list of common function words.
Co-occurrences were counted within a bi-directionalfive-word window, truncated by sentence ends.The number of clusters produced ?
seven ?
wasempirically determined as the maximal numberwith relatively large proportion (p(c) > .01) forall clusters.
Trying eight clusters or more, weobtain clusters of minute size, which apparentlydo not reveal additional themes or topics.
Table1 presents, for each cluster c and each religion,the 15 keywords x with the highest p(c|x) values.The number 15 has no special meaning otherthan providing rich, balanced and displayablenotion of all clusters.
The displayed 3?15keyword subsets are denoted ?1?
?7.We gave each cluster a title, reflecting our(naive) impression of its content.
As weinterpret the clusters, they indeed revealprominent aspects of religion: rituals (?2),schools (?5), narratives (?6) and scriptures (?7).More delicate issues, such as cherished qualities(?1), spiritual states (?3), suffering and sin (?4)are reflected as well, in spite of the verydifferent position taken by the distinct religionswith regard to these issues.3.1 Comparison to Expert DataWe have asked an expert of comparative religionstudies to simulate roughly the CD clusteringtask: assigning (freely-chosen) keywords intocorresponding subsets, reflecting prominentresembling aspects that cut across the threeexamined religions.
The expert was not asked toindicate a probability of assignment, but he wasallowed to use the same keyword in more than1 An evaluation copy of TextAnalyst, byMicroSystems Ltd., can be downloaded fromhttp://www.megaputer.com/php/eval.php3one cluster.
The expert clusters, with theexclusion of terms that we were not able tolocate in our corpora, are displayed in Table 2.In addition to our tags ?
e1?e8 ?
the expert gavea title to each cluster.Although the keyword-clustering task is highlysubjective, there are notable overlapped regionsshared by the expert clusters and ours.
Twoexpert topics ?
?Books?
(e1) and ?Ritual?
(e3) ?are clearly captured (by ?7 and ?2 respectively).
?Society and Politics?
(e4) and ?Establishments?
(e5) ?
are both in some correspondence with our?Schools and Traditions?
cluster (?5).
On theother hand, our output fails to capture the?Mysticism?
expert cluster (e6).
Further, ouroutput suggests the ?spiritual states?
theme (?3)and distinguishes cherished qualities (?1) fromsin and suffering (?4).
Such observations mightmake sense but are not covered by the expert.To quantify the overall level of overlap betweenour output and the expert's, we introducesuitable versions of recall and precisionmeasures.We want the recall measure to reflect theproportion of expert terms that are captured byour configuration, provided that an optimalcorrespondence between our clusters to theexpert is considered.
Hence, for each expertcluster, ej, we find a particular ?k with maximalnumber of overlapping terms (note that two ormore expert clusters are allowed to be coveredby a single ?k, to reflect cases where severalrelated sub-topics are merged within our results).Denote this maximal number by M(ej):M(ej) = maxk =1?7  |{x?ej: x?
?k) }| .Consequently, the recall measure R is defined tobe the sum of the above maximal overlap countsover all expert clusters, divided by all 131 expertterms (repetitions in distinct clusters counted):R =?j=1?8 M(ej) / 131.To estimate how precise our results are, we areinterested in the relative part of our clusters,reduced to the expert terms, which has beenassigned to the ?right?
expert cluster by thesame optimal correspondence.
Note that in thiscase we do not want to sum up several M valuesthat are associated with a single ?k: a singlecluster covering several expert clusters shouldbe considered as an indication of poor precision.Furthermore, if we do this, we might recountsome of ?k's terms (specifically, keywords thatthe expert has included in several clusters; thismight result in precision > 100%).
We needtherefore to consider at most one M value per ?k,namely the largest one.
DefineM*(?k) = maxj =1?7 {M(ej): |?k ?
ej| = M(ej)}(M*(?k) = 0 if the set on the right-hand side isempty, i.e.
there is no ej that share M(ej)elements with ?k).
The global precision measureP is the sum of all M* values, divided by thenumber of expert terms appearing among the ?k's(repetitions counted), which are, in the currentcase, the 94 underlined terms in Table 1:P =?k =1?7 M*(?k) / 94.Our algorithm has achieved the followingvalues: R=67/131=0.51, P=  58/94=0.62.This is a notable improvement relatively to theIB algorithm results: R=42/131=0.32 andP=32/82=0.39 (random assignment of thekeywords into seven clusters yield averagevalues R=0.36, P=0.33).
As we haveexpected, three of the clusters produced by theIB algorithms are populated, with very highprobability, by most keywords of a singlereligion.
Within these specific religion clustersas well as the other sparsely populated clusters,the ranking inducted by the p(c|x) values is notvery informative regarding particular sub-topics.Thus, the IB performs the CD clustering taskpoorly, even in comparison to random results.We note that, similarly to our algorithm, the IBalgorithm produces at most 7 clusters of non-negligible size.
This somewhat supports ourTable 2: The expert cross-dataset clusters.
Cluster titles were assigned by the expert.
For each expertcluster, the best fitting automated cross-dataset cluster is indicated on the right-hand side, as well asthe number of relevant expert words it includes.
The terms of this best-fit cluster are underlined.Superscripts indicate indices of the cross-dataset cluster(s), among ?1?
?7, to which each termbelongs.Buddhism Christianity Islame1: Scriptures                 ?7 !
14 (of 19)sutra7, mantra2, mandala2, koan,pali canon7new  testament7, old  testament7,bible7, apostle7, revelation, john7,paul7,  luke7,  matthew7quran7, hadith7, sunna7, sharia,muhammad7e2: Beliefs and Ideas                 ?4 !
10 (of 25)nirvana14, four  noble  truths,dharma6,7, dukkha4, buddhanature1, tantra, emptiness3,reincarnation4resurrection, heaven4, hell4, trinity,second  coming6,    jesus  christ1,love  of  god1, god1, satan4, cross,dove4,  fish4prophet7, allah1, one  god1,five  pillars,  heaven4,  hell4e3: Ritual, Prayer, Holydays                 ?2 !
16 (of 20)meditation2, statue2, sacrifice1,6,gift,  stupa2sunday2, pray2, confession3,eucharist2,   christmas2,  baptismpilgrim2, charity2, ramadan2,fasting2, id  al  fitr2, pray2,friday2,   kaaba2,   mecca2e4: Society and Politics                 ?5 !
9 (of 19)dalai  lama, monk2, bodhisattva1,6,lama7rome5, vatican5, church5, minister2,priest6,  cardinal5,  pope5,  bishop5sharia, caliph6, imam5, shia5,sunna7, ali6, sufi5e5: Establishments                 ?5 !
6 (of 10)monastery2, temple2, sangha,school5church5,   cardinal5,  pope5,  bishop5 mosque2, imam5e6: Mysticism                 ?2 !
2 (of 11)meditation2, nirvana14, samadhi3,tantraeucharist2, miracle, crucifixion,suffer4,   love1,   saintsufi5e7: Learning and Education                 ?5 !
4 (of 8)monastery2, monk2, sutra7,meditation2monk5, university5, theology5,divinity5e8: Names and Places                 ?6 !
7 (of 20)gautama6,  buddha6,7 jesus  christ1, john  the  baptist6,jordan6, jerusalem2, bethlehem6,mary6, rome5, john7, paul7, luke7,matthew7,  zion6muhammad7, ali6, mecca2,medina2impression that the limit on number of?interesting?
clusters reflects intrinsicexhaustion of the information embodied withinthe given data.
It is yet to be carefully examinedwhether this observation provides any hintregarding the general issue of the ?right?
numberof clusters.4 ConclusionThis paper addressed the relatively unexploredproblem of detecting corresponding themesacross multiple corpora.
We have developed anextended clustering algorithm that is based onthe appealing and highly general InformationBottleneck method.
Substantial effort has beendevoted to adopting this method for the Cross-Dataset clustering task.Our approach was demonstrated empirically onthe challenging task of finding correspondingthemes across different religions.
Subjectiveexamination of the system's output, as well as itscomparison to the output of a human expert,demonstrate the potential benefits of applyingthis approach in the framework of comparativeresearch, and possibly in additional text miningapplications.Given the early stage of this line of research,there is plenty of room for future work.
Inparticular, further research is needed to providetheoretic grounding for the CD clusteringformulations and to specify their properties.Empirical work is needed to explore thepotential of the proposed paradigm for othertextual domains as well as for relatedapplications.
Particularly, we have recentlypresented a similar framework for templateinduction in information extraction (cross-component clustering, Marx, Dagan, & Shamir,2002), which should be studied in relation to theCD algorithm presented here.AppendixThe value of p(Xi), which is required for thecalculations in Section 3.2, is given directlyfrom the input co-occurrence data as follows:p(Xi)  = ?????
?YyXxYyXxyxcountyxcounti'),'(),(The values pt(c|Xi), pt(y|c,Xi) are calculated fromvalues that are available at time step t?1:?
?
?= iXx tit xcpxpXcp )|()()|( 1 ,?
?
?= iXx itit XcxpxypXcyp ),|()|(),|( 1(pt?1(x|c,Xi) is due to Bayes' rule conditioned onXi: pt?1(x|c,Xi) = pt?1(c|x) ?
p(x) / pt?1(c|Xi); notethat pt?1(c|x) = pt?1(c|x,Xi) ).Finally we have:pt(y,c,Xi) = pt(y|c,Xi) ?
pt(c|Xi) ?
p(Xi) .AcknowledgmentsWe thank Eitan Reich for providing the expertdata, as well as for illuminating discussions.This work has been partially supported byISRAEL SCIENCE FOUNDATION founded byThe Academy of Sciences and Humanities(grants 574/98-1 and 489/00).ReferencesCover, T. M. and Thomas, J.
A.
(1991).
Elements ofInformation Theory.
New York: John Wiley &Sons, Inc.Dhillon I. S. and Modha D. S. (2001).
Conceptdecompositions for large sparse text data usingclustering.
Machine Learning, 42/1, pp.
143?175.Gentner, D. (1983).
Structure-mapping: a theoreticalframework for analogy.
Cognitive Science, 7, pp.155?170.Hofstadter, D. R. and the Fluid Analogies ResearchGroup (1995).
Fluid Concepts and CreativeAnalogies.
New-York: Basic Books, 518 p.Lee D. D. and Seung H. S. (1999).
Learning theparts of objects by non-negative matrixfactorization.
Nature, 401/6755, pp.
788?791.Marx, Z., Dagan, I. and Shamir, E. (2002).
Cross-component clustering for template induction.Workshop on Text Learning (TextML-2002),Sydney, Australia.Marx, Z., Dagan, I., Buhmann, J. M. and Shamir, E.(2002).
Coupled clustering: a method for detectingstructural correspondence.
Journal of MachineLearning Research, accepted for publication.Pereira, F. C. N., Tishby N. and Lee L. J.
(1993).Distributional clustering of English words.
In:Proceedings of the 31st Annual Meeting of theAssociation for Computational Linguistics ACL'93, Columbus, OH, pp.
183?190.Tishby, N., Pereira, F. C. and Bialek, W.  (1999).The information bottleneck method.
In: The 37thAnnual Allerton Conference on Communication,Control, and Computing, Urbana-Champaign, IL,pp.
368?379.
