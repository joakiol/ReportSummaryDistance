Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 53?61,Beijing, August 2010Fast and Accurate Arc Filtering for Dependency ParsingShane BergsmaDepartment of Computing ScienceUniversity of Albertasbergsma@ualberta.caColin CherryInstitute for Information TechnologyNational Research Council Canadacolin.cherry@nrc-cnrc.gc.caAbstractWe propose a series of learned arc fil-ters to speed up graph-based dependencyparsing.
A cascade of filters identify im-plausible head-modifier pairs, with timecomplexity that is first linear, and thenquadratic in the length of the sentence.The linear filters reliably predict, in con-text, words that are roots or leaves of de-pendency trees, and words that are likelyto have heads on their left or right.
Weuse this information to quickly prune arcsfrom the dependency graph.
More than78% of total arcs are pruned while retain-ing 99.5% of the true dependencies.
Thesefilters improve the speed of two state-of-the-art dependency parsers, with low over-head and negligible loss in accuracy.1 IntroductionDependency parsing finds direct syntactic rela-tionships between words by connecting head-modifier pairs into a tree structure.
Depen-dency information is useful for a wealth of nat-ural language processing tasks, including ques-tion answering (Wang et al, 2007), semantic pars-ing (Poon and Domingos, 2009), and machinetranslation (Galley and Manning, 2009).We propose and test a series of arc filters forgraph-based dependency parsers, which rule outpotential head-modifier pairs before parsing be-gins.
In doing so, we hope to eliminate im-plausible links early, saving the costs associatedwith them, and speeding up parsing.
In addi-tion to the scaling benefits that come with fasterprocessing, we hope to enable richer featuresfor parsing by constraining the set of arcs thatneed to be considered.
This could allow ex-tremely large feature sets (Koo et al, 2008), or thelook-up of expensive corpus-based features suchas word-pair mutual information (Wang et al,2006).
These filters could also facilitate expen-sive learning algorithms, such as semi-supervisedapproaches (Wang et al, 2008).We propose three levels of filtering, which areapplied in a sequence of increasing complexity:Rules: A simple set of machine-learned rulesbased only on parts-of-speech.
They prune over25% of potential arcs with almost no loss in cover-age.
Rules save on the wasted effort for assessingimplausible arcs such as DT?
DT.Linear: A series of classifiers that tag words ac-cording to their possible roles in the dependencytree.
By treating each word independently and en-suring constant-time feature extraction, they oper-ate in linear time.
We view these as a dependency-parsing analogue to the span-pruning proposed byRoark and Hollingshead (2008).
Our fast linearfilters prune 54.2% of potential arcs while recov-ering 99.7% of true pairs.Quadratic: A final stage that looks at pairs ofwords to prune unlikely arcs from the dependencytree.
By employing a light-weight feature set, thishigh-precision filter can enable more expensiveprocessing on the remaining plausible dependen-cies.Collectively, we show that more than 78% oftotal arcs can be pruned while retaining 99.5% ofthe true dependencies.
We test the impact of thesefilters at both train and test time, using two state-of-the-art discriminative parsers, demonstratingspeed-ups of between 1.9 and 5.6, with little im-pact on parsing accuracy.53Investors continue to pour cash into money fundsFigure 1: An example dependency parse.2 Dependency ParsingA dependency tree represents the syntactic struc-ture of a sentence as a directed graph (Figure 1),with a node for each word, and arcs indicat-ing head-modifier pairs (Me?lc?uk, 1987).
Thoughdependencies can be extracted from many for-malisms, there is a growing interest in predict-ing dependency trees directly.
To that end, thereare two dominant approaches: graph-based meth-ods, characterized by arc features in an exhaus-tive search, and transition-based methods, char-acterized by operational features in a greedysearch (McDonald and Nivre, 2007).
We focus ongraph-based parsing, as its exhaustive search hasthe most to gain from our filters.Graph-based dependency parsing finds thehighest-scoring tree according to a scoring func-tion that decomposes under an exhaustive search(McDonald et al, 2005).
The most natural de-composition scores individual arcs, represented ashead-modifier pairs [h,m].
This enables searchby either minimum spanning tree (West, 2001) orby Eisner?s (1996) projective parser.
This paperfocuses on the projective case, though our tech-niques transfer to spanning tree parsing.
With alinear scoring function, the parser solves:parse(s) = argmaxt?s?[h,m]?tw?
?
f?
(h,m, s)The weights w?
are typically learned using anonline method, such as an averaged percep-tron (Collins, 2002) or MIRA (Crammer andSinger, 2003).
2nd-order searches, which considertwo siblings at a time, are available with no in-crease in asymptotic complexity (McDonald andPereira, 2006; Carreras, 2007).The complexity of graph-based parsing isbounded by two processes: parsing (carrying outthe argmax) and arc scoring (calculating w?
?f?
(h,m, s)).
For a sentence with n words, pro-jective parsing takes O(n3) time, while the span-ning tree algorithm is O(n2).
Both parsers requirescores for arcs connecting each possible [h,m]pair in s; therefore, the cost of arc scoring is alsoO(n2), and may become O(n3) if the features in-clude words in s between h and m (Galley andManning, 2009).
Arc scoring also has a signif-icant constant term: the number of features ex-tracted for an [h,m] pair.
Our in-house graph-based parser collects on average 62 features foreach potential arc, a number larger than the lengthof most sentences.
With the cluster-based featuressuggested by Koo et al (2008), this could easilygrow by a factor of 3 or 4.The high cost of arc scoring, coupled withthe parsing stage?s low grammar constant, meansthat graph-based parsers spend much of their timescoring potential arcs.
Johnson (2007) reports thatwhen arc scores have been precomputed, the dy-namic programming component of his 1st-orderparser can process an amazing 3,580 sentences persecond.1 Beyond reducing the number of features,the easiest way to reduce the computational bur-den of arc scoring is to score only plausible arcs.3 Related Work3.1 Vine ParsingFiltering dependency arcs has been explored pri-marily in the form of vine parsing (Eisner andSmith, 2005; Dreyer et al, 2006).
Vine pars-ing establishes that, since most dependencies areshort, one can parse quickly by placing a hardconstraint on arc length.
As this coarse fil-ter quickly degrades the best achievable perfor-mance, Eisner and Smith (2005) also considerconditioning the constraint on the part-of-speech(PoS) tags being linked and the direction of thearc, resulting in a separate threshold for each[tag(h), tag(m),dir(h,m)] triple.
They sketchan algorithm where the thresholded length foreach triple starts at the highest value seen in thetraining data.
Thresholds are then decreased ina greedy fashion, with each step producing thesmallest possible reduction in reachable trainingarcs.
We employ this algorithm as a baseline inour experiments.
To our knowledge, vine parsing1To calibrate this speed, consider that the publicly avail-able 1st-order MST parser processes 16 sentences per secondon modern hardware.
This includes I/O costs in addition tothe costs of arc scoring and parsing.54has not previously been tested with a state-of-the-art, discriminative dependency parser.3.2 CFG Cell ClassificationRoark and Hollingshead (2008) speed up anotherexhaustive parsing algorithm, the CKY parser forCFGs, by classifying each word in the sentenceaccording to whether it can open (or close) amulti-word constituent.
With a high-precisiontagger that errs on the side of permitting con-stituents, they show a significant improvement inspeed with no reduction in accuracy.It is difficult to port their idea directly to depen-dency parsing without committing to a particularsearch algorithm,2 and thereby sacrificing someof the graph-based formalism?s modularity.
How-ever, some of our linear filters (see Section 4.3)were inspired by their constraints.3.3 Coarse-to-fine ParsingAnother common method employed to speed upexhaustive parsers is a coarse-to-fine approach,where a cheap, coarse model prunes the searchspace for later, more expensive models (Charniaket al, 2006; Petrov and Klein, 2007).
This ap-proach assumes a common forest or chart repre-sentation, shared by all granularities, where onecan efficiently track the pruning decisions of thecoarse models.
One could imagine applying sucha solution to dependency parsing, but the exactimplementation of the coarse pass would vary ac-cording to the choice in search algorithm.
Our fil-ters are much more modular: they apply to both1st-order spanning tree parsing and 2nd-order pro-jective parsing, with no modification.Carreras et al (2008) use coarse-to-fine pruningwith dependency parsing, but in that case, a graph-based dependency parser provides the coarse pass,with the fine pass being a far-more-expensive tree-adjoining grammar.
Our filters could become a0th pass, further increasing the efficiency of theirapproach.4 Arc FiltersWe propose arc filtering as a preprocessing stepfor dependency parsing.
An arc filter removes im-2Johnson?s (2007) split-head CFG could implement thisidea directly with little effort.plausible head-modifier arcs from the completedependency graph (which initially includes allhead-modifier arcs).
We use three stages of filtersthat operate in sequence on progressively sparsergraphs: 1) rule-based, 2) linear: a single passthrough the n nodes in a sentence (O(n) complex-ity), and 3) quadratic: a scoring of all remainingarcs (O(n2)).
The less intensive filters are usedfirst, saving time by leaving fewer arcs to be pro-cessed by the more intensive systems.Implementations of our rule-based, linear, andquadratic filters are publicly available at:http://code.google.com/p/arcfilter/4.1 Filter FrameworkOur filters assume the input sentences have beenPoS-tagged.
We also add an artificial root nodeto each sentence to be the head of the tree?s root.Initially, this node is a potential head for all wordsin the sentence.Each filter is a supervised classifier.
For exam-ple, the quadratic filter directly classifies whethera proposed head-modifier pair is not a link in thedependency tree.
Training data is created from an-notated trees.
All possible arcs are extracted foreach training sentence, and those that are presentin the annotated tree are labeled as class?1, whilethose not present are +1.
A similar process gener-ates training examples for the other filters.
Sinceour goal is to only filter very implausible arcs, webias the classifier to high precision, increasing thecost for misclassifying a true arc during learning.3Class-specific costs are command-line parame-ters for many learning packages.
One can inter-pret the learning objective as minimizing regular-ized, weighted loss:minw?12 ||w?||2 + C1?i:yi=1l(w?, yi, x?i)+C2?i:yi=?1l(w?, yi, x?i) (1)where l() is the learning method?s loss function,x?i and yi are the features and label for the ith3Learning with a cost model is generally preferable tofirst optimizing error rate and then thresholding the predic-tion values to select a high-confidence subset (Joachims,2005), but the latter approach was used successfully for cellclassification in Roark and Hollingshead (2008).55not a h ?
?
, .
; | CC PRP$ PRP EX-RRB- -LRB-no ?
?
m EX LS POS PRP$no m?
?
.
RPnot a root , DTno h?m DT?{DT,JJ,NN,NNP,NNS,.
}CD?CD NN?{DT,NNP}NNP?
{DT,NN,NNS}no m?h {DT,IN,JJ,NN,NNP}?DTNNP?IN IN?JJTable 1: Learned rules for filtering dependencyarcs using PoS tags.
The rules filter 25% of pos-sible arcs while recovering 99.9% of true links.training example, w?
is the learned weight vector,and C1 and C2 are the class-specific costs.
Highprecision is obtained when C2 >> C1.
For anSVM, l(w?, yi, x?i) is the standard hinge loss.We solve the SVM objective using LIBLIN-EAR (Fan et al, 2008).
In our experiments, eachfilter is a linear SVM with the typical L1 loss andL2 regularization.4 We search for the best com-bination of C1 and C2 using a grid search on de-velopment data.
At test time, an arc is filtered ifw?
?
x?
> 0.4.2 Rule-Based FilteringOur rule-based filters seek to instantly removethose arcs that are trivially implausible on the ba-sis of their head and modifier PoS tags.
We firstextract labeled examples from gold-standard treesfor whenever a) a word is not a head, b) a worddoes not have a head on the left (resp.
right), andc) a pair of words is not linked.
We then trainedhigh-precision SVM classifiers.
The only featuresin x?
are the PoS tag(s) of the head and/or modi-fier.
The learned feature weights identify the tagsand tag-pairs to be filtered.
For example, if a taghas a positive weight in the not-a-head classifier,all arcs having that node as head are filtered.The classier selects a small number of high-4We also tried L1-regularized filters.
L1 encourages mostfeatures to have zero weight, leading to more compact andhence faster models.
We found the L1 filters to prune fewerarcs at a given coverage level, providing less speed-up atparsing time.
Both L1 and L2 models are available in ourpublicly available implementation.precision rules, shown in Table 1.
Note that therules tend to use common tags with well-definedroles.
By focusing on weighted loss as opposedto arc frequency, the classifier discovers struc-tural zeros (Mohri and Roark, 2006), events whichcould have been observed, but were not.
Weconsider this an improvement over the frequency-based length thresholds employed previously intag-specific vine parsing.4.3 Linear-Time FilteringIn the linear filtering stage, we filter arcs on thebasis of single nodes and their contexts, passingthrough the sentences in linear time.
For eachnode, eight separate classifiers decide whether:1.
It is not a head (i.e., it is a leaf of the tree).2.
Its head is on the left/right.3.
Its head is within 5 nodes on the left/right.4.
Its head is immediately on the left/right.5.
It is the root.For each of these decisions, we again train high-precision SVMs with C2 >> C1, and filter di-rectly based on the classifier output.If a word is not a head, all arcs with the givenword as head can be pruned.
If a word is deemedto have a head within a certain range on the leftor right, then all arcs that do not obey this con-straint can be pruned.
If a root is found, no otherwords should link to the artificial root node.
Fur-thermore, in a projective dependency tree, no arcwill cross the root, i.e., there will be no arcs wherea head and a modifier lie on either side of the root.We can therefore also filter arcs that violate thisconstraint when parsing projectively.S?gaard and Kuhn (2009) previously proposeda tagger to further constrain a vine parser.
Theirtags are a subset of our decisions (items 4 and 5above), and have not yet been tested in a state-of-the-art system.Development experiments show that if wecould perfectly make decisions 1-5 for each word,we could remove 91.7% of the total arcs or 95%of negative arcs, close to the upper bound.FeaturesUnlike rule-based filtering, linear filtering usesa rich set of features (Table 2).
Each feature is a56PoS-tag features Other featurestagi worditagi, tagi?1 wordi+1tagi, tagi+1 wordi?1tagi?1, tagi+1 shapeitagi?2, tagi?1 prefixitagi+1, tagi+2 suffixitagj , Left, j=i?5...i?1 itagj , Right, j=i+1...i+5 i, ntagj , (i-j), j=i?5...i?1 n - itagj , (i-j), j=i+1...i+5Table 2: Linear filter features for a node at po-sition i in a sentence of length n. Each featureis also conjoined (unless redundant) with wordi,tagi, shapei, prefixi, and suffixi (both 4 letters).The shape is the word normalized using the regu-lar expressions [A-Z]+?
A and [a-z]+?
a.binary indicator feature.
To increase the speed ofapplying eight classifiers, we use the same featurevector for each of the decisions; learning giveseight different weight vectors, one correspondingto each decision function.
Feature extraction isconstrained to be O(1) for each node, so that over-all feature extraction and classification remain afast O(n) complexity.
Feature extraction wouldbe O(n2) if, for example, we had a feature for ev-ery tag on the left or right of a node.Combining linear decisionsWe originally optimized the C1 and C2 param-eter separately for each linear decision function.However, we found we could substantially im-prove the collective performance of the linear fil-ters by searching for the optimal combination ofthe component decisions, testing different levelsof precision for each component.
We selected afew of the best settings for each decision when op-timized separately, and then searched for the bestcombination of these candidates on developmentdata (testing 12960 combinations in all).4.4 Quadratic-Time FilteringIn the quadratic filtering stage, a single classifierdecides whether each head-modifier pair shouldbe filtered.
It is trained and applied as describedin Section 4.1.Binary featuressign(h-m) tagshmtagm?1, tagshm tagm+1, tagshmtagh?1, tagshm tagh+1, tagshmsign(h-m), tagh, wordmsign(h-m), wordh, tagmReal features?
valuessign(h-m)?
h-mtagh, tagm ?
h-mtagk , tagshm ?
Count(tagk ?
tagsh...m)wordk , tagshm ?
Count(wordk ?
wordsh...m)Table 3: Quadratic filter features for a head at po-sition h and a modifier at position m in a sentenceof length n. Here tagshm = (sign(h-m), tagh,tagm), while tagsh...m and wordsh...m are all thetags (resp.
words) between h and m, but within?5 positions of h or m.While theoretically of the same complexity asthe parser?s arc-scoring function (O(n2)), thisprocess can nevertheless save time by employinga compact feature set.
We view quadratic filter-ing as a light preprocessing step, using only a por-tion of the resources that might be used in the finalscoring function.FeaturesQuadratic filtering uses both binary and real-valued features (Table 3).
Real-valued featurespromote a smaller feature space.
For example,one value can encode distance rather than separatefeatures for different distances.
We also general-ize the ?between-tag features?
used in McDonaldet al (2005) to be the count of each tag betweenthe head and modifier.
The count may be more in-formative than tag presence alone, particularly forhigh-precision filters.
We follow Galley and Man-ning (2009) in using only between-tags within afixed range of the head or modifier, so that the ex-traction for each pair is O(1) and the overall fea-ture extraction is O(n2).Using only a subset of the between-tags as fea-tures has been shown to improve speed but im-pair parser performance (Galley and Manning,2009).
By filtering quickly first, then scoring allremaining arcs with a cubic scoring function in theparser, we hope to get the best of both worlds.575 Filter ExperimentsDataWe extract dependency structures from thePenn Treebank using the Penn2Malt extractiontool,5 which implements the head rules of Yamadaand Matsumoto (2003).
Following convention, wedivide the Treebank into train (sections 2?21), de-velopment (22) and test sets (23).
The develop-ment and test sets are re-tagged using the Stanfordtagger (Toutanova et al, 2003).Evaluation MetricsTo measure intrinsic filter quality, we defineReduction as the proportion of total arcs re-moved, and Coverage as the proportion of truehead-modifier arcs retained.
Our evaluation asks,for each filter, what Reduction can be obtained ata given Coverage level?
We also give Time: howlong it takes to apply the filters to the test set (ex-cluding initialization).We compute an Upper Bound for Reduction ondevelopment data.
There are 1.2 million poten-tial dependency links in those sentences, 96.5%of which are not present in a gold standard depen-dency tree.
Therefore, the maximum achievableReduction is 96.5%.SystemsWe evaluate the following systems:?
Rules: the rule-based filter (Section 4.2)?
Lin.
: the linear-time filters (Section 4.3)?
Quad.
: the quadratic filter (Section 4.4)The latter two approaches run on the output of theprevious stage.
We compare to the two vine pars-ing approaches described in Section 3.1:?
Len-Vine uses a hard limit on arc length.?
Tag-Vine (later, Vine) learns a maxi-mum length for dependency arcs for everyhead/modifier tag-combination and order.5.1 ResultsWe set each filter?s parameters by selectinga Coverage-Reduction tradeoff on development5http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html203040506070809010099.3 99.4 99.5 99.6 99.7 99.8 99.9Reduction(%)Coverage (%)Upper BdLin-Orac.QuadLinTag-VineLen-VineFigure 2: Filtering performance for different fil-ters and cost parameters on development data.Lin-Orac indicates the percentage filtered usingperfect decisions by the linear components.Filter Coverage Reduct.
Time (s)Vine 99.62 44.0 2.9sRules 99.86 25.8 1.3sLin.
99.73 54.2 7.3sQuad.
99.50 78.4 16.1sTable 4: Performance (%) of filters on test data.data (Figure 2).
The Lin curve is obtained by vary-ing both the C1/C2 cost parameters and the combi-nation of components (plotting the best Reductionat each Coverage level).
We chose the linear fil-ters with 99.8% Coverage at a 54.2% Reduction.We apply Quad on this output, varying the costparameters to produce its curve.
Aside from Len-Vine, all filters remove a large number of arcs withlittle drop in Coverage.After selecting a desired trade-off for each clas-sifier, we move to final filtering experiments onunseen test data (Table 4).
The linear filter re-moves well over half the links but retains an as-tounding 99.7% of correct arcs.
Quad removes78.4% of arcs at 99.5% Coverage.
It thus reducesthe number of links to be scored by a dependencyparser by a factor of five.The time for filtering the 2416 test sentencesvaries from almost instantaneous for Vine andRules to around 16 seconds for Quad.
Speed num-bers are highly machine, design, and implemen-58Decision Precision RecallNo-Head 99.9 44.8Right-?
99.9 28.7Left-?
99.9 39.0Right-5 99.8 31.5Left-5 99.9 19.7Right-1 99.7 6.2Left-1 99.7 27.3Root 98.6 25.5Table 5: Linear Filters: Test-set performance (%)on decisions for components of the combined 54.2Reduct./99.73 Coverage linear filter.Type Coverage Reduct.
OracleAll 99.73 54.2 91.8All\No-Head 99.76 46.4 87.2All\Left-?
99.74 53.2 91.4All\Right-?
99.75 53.6 90.7All\Left-5 99.74 53.2 89.7All\Right-5 99.74 51.6 90.4All\Left-1 99.75 53.5 90.8All\Right-1 99.73 53.9 90.6All\Root 99.76 50.2 90.0Table 6: Contribution of different linear filters totest set performance (%).
Oracle indicates the per-centage filtered by perfect decisions.tation dependent, and thus we have stressed theasymptotic complexity of the filters.
However, thetiming numbers show that arc filtering can be donequite quickly.
Section 6 confirms that these arevery reasonable costs in light of the speed-up inoverall parsing.5.2 Linear Filtering AnalysisIt is instructive to further analyze the componentsof the linear filter.
Table 5 gives the performanceof each classifier on its specific decision.
Preci-sion is the proportion of positive classificationsthat are correct.
Recall is the proportion of pos-itive instances that are classified positively (e.g.the proportion of actual roots that were classifiedas roots).
The decisions correspond to items 1-5 inSection 4.3.
For example, Right-?
is the decisionthat a word has no head on the right.Most notably, the optimum Root decision hasmuch lower Precision than the others, but this haslittle effect on its overall accuracy as a filter (Ta-ble 6).
This is perhaps because the few cases offalse positives are still likely to be main verbs orauxiliaries, and thus still still likely to have fewlinks crossing them.
Thus many of the filteredlinks are still correct.Table 6 provides the performance of the classi-fier combination when each linear decision is ex-cluded.
No-Head is the most important compo-nent in the oracle and the actual combination.6 Parsing Experiments6.1 Set-upIn this section, we investigate the impact of our fil-ters on graph-based dependency parsers.
We traineach parser unfiltered, and then measure its speedand accuracy once filters have been applied.
Weuse the same training, development and test setsdescribed in Section 5.
We evaluate unlabeled de-pendency parsing using head accuracy: the per-centage of words (ignoring punctuation) that areassigned the correct head.The filters bypass feature extraction for each fil-tered arc, and replace its score with an extremelylow negative value.
Note that 2nd-order featuresconsider O(n3) [h,m1,m2] triples.
These triplesare filtered if at least one component arc ([h,m1]or [h,m2]) is filtered.In an optimal implementation, we might alsohave the parser re-use features extracted duringfiltering when scoring the remaining arcs.
We didnot do this.
Instead, filtering was treated as a pre-processing step, which maximizes the portabilityof the filters across parsers.
We test on two state-of-the art parsers:MST We modified the publicly-available MSTparser (McDonald et al, 2005)6 to employ our fil-ters before carrying out feature extraction.
MSTis trained with 5-best MIRA.DepPercep We also test an in-house depen-dency parser, which conducts projective first and2nd-order searches using the split-head CFG de-scribed by Johnson (2007), with a weight vec-tor trained using an averaged perceptron (Collins,6http://sourceforge.net/projects/mstparser/59DepPercep-1 DepPercep-2 MST-1 MST-2Filter Cost Acc.
Time Acc.
Time Acc.
Time Acc.
TimeNone +0 91.8 348 92.5 832 91.2 153 91.9 200Vine +3 91.7 192 92.3 407 91.2 99 91.8 139Rules +1 91.7 264 92.4 609 91.2 125 91.9 167Linear +7 91.7 168 92.4 334 91.2 88 91.8 121Quad.
+16 91.7 79 92.3 125 91.2 58 91.8 80Table 7: The effect of filtering on the speed and accuracy on 1st and 2nd-order dependency parsing.2002).
Its features are a mixture of those de-scribed by McDonald et al (2005), and those usedin the Koo et al (2008) baseline system; we do notuse word-cluster features.DepPercep makes some small improvements toMST?s 1st-order feature set.
We carefully de-termined which feature types should have dis-tance appended in addition to direction.
Also, in-spired by the reported utility of mixing PoS tagsand word-clusters (Koo et al, 2008), we createdversions of all of the ?Between?
and ?Surround-ing Word?
features described by McDonald et al(2005) where we mix tags and words.7DepPercep was developed with quadratic filtersin place, which enabled a fast development cyclefor feature engineering.
As a result, it does notimplement many of the optimizations in place inMST, and is relatively slow unfiltered.6.2 ResultsThe parsing results are shown in Table 7, wheretimes are given in seconds, and Cost indicates theadditional cost of filtering.
Note that the impactof all filters on accuracy is negligible, with a de-crease of at most 0.2%.
In general, parsing speed-ups mirror the amount of arc reduction measuredin our filter analysis (Section 5.1).Accounting for filter costs, the benefits ofquadratic filtering depend on the parser.
The extrabenefit of quadratic over linear is substantial forDepPercep, but less so for 1st-order MST.MST shows more modest speed-ups than Dep-Percep, but MST is already among the fastestpublicly-available data-driven parsers.
Underquadratic filtering, MST-2 goes from processing7This was enabled by using word features only when theword is among the 800 most frequent in the training set.12 sentences per second to 23 sentences.8DepPercep-2 starts slow, but benefits greatlyfrom filtering.
This is because, unlike MST-2,it does not optimize feature extraction by fac-toring its ten 2nd-order features into two triple([h,m1,m2]) and eight sibling ([m1,m2]) fea-tures.
This suggests that filtering could have a dra-matic effect on a parser that uses more than a fewtriple features, such as Koo et al (2008).7 ConclusionWe have presented a series of arc filters that speedup graph-based dependency parsing.
By treat-ing filtering as weighted classification, we learn acascade of increasingly complex filters from tree-annotated data.
Linear-time filters prune 54%of total arcs, while quadratic-time filters prune78%.
Both retain at least 99.5% of true dependen-cies.
By testing two state-of-the-art dependencyparsers, we have shown that our filters producesubstantial speed improvements in even carefully-optimized parsers, with negligible losses in ac-curacy.
In the future we hope to leverage thisreduced search space to explore features derivedfrom large corpora.ReferencesCarreras, Xavier, Michael Collins, and Terry Koo.2008.
TAG, dynamic programming, and the percep-tron for efficient, feature-rich parsing.
In CoNLL.Carreras, Xavier.
2007.
Experiments with a higher-order projective dependency parser.
In EMNLP-CoNLL.8This speed accounts for 25 total seconds to apply therules, linear, and quadratic filters.60Charniak, Eugene, Mark Johnson, Micha Elsner,Joseph Austerweil, David Ellis, Isaac Haxton,Catherine Hill, R. Shrivaths, Jeremy Moore,Michael Pozar, and Theresa Vu.
2006.
Multilevelcoarse-to-fine PCFG parsing.
In HLT-NAACL.Collins, Michael.
2002.
Discriminative training meth-ods for hidden markov models: Theory and experi-ments with perceptron algorithms.
In EMNLP.Crammer, Koby and Yoram Singer.
2003.
Ultracon-servative online algorithms for multiclass problems.JMLR, 3:951?991.Dreyer, Markus, David A. Smith, and Noah A. Smith.2006.
Vine parsing and minimum risk reranking forspeed and precision.
In CoNLL.Eisner, Jason and Noah A. Smith.
2005.
Parsing withsoft and hard constraints on dependency length.
InIWPT.Eisner, Jason.
1996.
Three new probabilistic modelsfor dependency parsing: An exploration.
In COL-ING.Fan, Rong-En, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
LIBLINEAR:A library for large linear classification.
JMLR,9:1871?1874.Galley, Michel and Christopher D. Manning.
2009.Quadratic-time dependency parsing for machinetranslation.
In ACL-IJCNLP.Joachims, Thorsten.
2005.
A support vector methodfor multivariate performance measures.
In ICML.Johnson, Mark.
2007.
Transforming projective bilex-ical dependency grammars into efficiently-parsableCFGs with unfold-fold.
In ACL.Koo, Terry, Xavier Carreras, and Michael Collins.2008.
Simple semi-supervised dependency parsing.In ACL-08: HLT.McDonald, Ryan and Joakim Nivre.
2007.
Character-izing the errors of data-driven dependency parsingmodels.
In EMNLP-CoNLL.McDonald, Ryan and Fernando Pereira.
2006.
Onlinelearning of approximate dependency parsing algo-rithms.
In EACL.McDonald, Ryan, Koby Crammer, and FernandoPereira.
2005.
Online large-margin training of de-pendency parsers.
In ACL.Me?lc?uk, Igor A.
1987.
Dependency syntax: theoryand practice.
State University of New York Press.Mohri, Mehryar and Brian Roark.
2006.
Probabilisticcontext-free grammar induction based on structuralzeros.
In HLT-NAACL.Petrov, Slav and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In HLT-NAACL.Poon, Hoifung and Pedro Domingos.
2009.
Unsuper-vised semantic parsing.
In EMNLP.Roark, Brian and Kristy Hollingshead.
2008.
Classi-fying chart cells for quadratic complexity context-free inference.
In COLING.S?gaard, Anders and Jonas Kuhn.
2009.
Using a max-imum entropy-based tagger to improve a very fastvine parser.
In IWPT.Toutanova, Kristina, Dan Klein, Christopher D. Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In NAACL.Wang, Qin Iris, Colin Cherry, Dan Lizotte, and DaleSchuurmans.
2006.
Improved large margin depen-dency parsing via local constraints and Laplacianregularization.
In CoNLL.Wang, Mengqiu, Noah A. Smith, and Teruko Mita-mura.
2007.
What is the Jeopardy model?
A quasi-synchronous grammar for QA.
In EMNLP-CoNLL.Wang, Qin Iris, Dale Schuurmans, and Dekang Lin.2008.
Semi-supervised convex training for depen-dency parsing.
In ACL-08: HLT.West, D. 2001.
Introduction to Graph Theory.
Pren-tice Hall, 2nd edition.Yamada, Hiroyasu and Yuji Matsumoto.
2003.
Statis-tical dependency analysis with support vector ma-chines.
In IWPT.61
