Deducing Linguistic Structurefrom the Statistics of Large CorporaEric Brill~ David  Magerman~ Mitchel l  Marcus~ Beatr ice  SantoriniDepartment of Computer and Information ScienceUniversity of PennsylvaniaPhiladelphia, PA 191041 Int roduct ionWithin the last two years, approaches using bothstochastic and symbolic techniques have proved ade-quate to deduce lexical ambiguity resolution rules withless than 3-4% error rate, when trained on moderatesized (500K word) corpora of English text (e.g.
Church,1988; Hindle, 1989).
The success of these techniquessuggests that much of the grammatical structure of lan-guage may be derived automatically through distribu-tional analysis, an approach attempted and abandonedin the 1950s.We describe here two experiments to see how farpurely distributional techniques can be pushed to au-tomatically provide both a set of part of speech tagsfor English, and a grammatical analysis of free Englishtext.
We also discuss the state of a tagged NL corpus toaid such research (now amounting to 4 million words ofhand-corrected part-of-speech tagging).In the experiment described in Section 2, we have de-veloped a constituent boundary parsing algorithm whichderives an (unlabelled) bracketing iven text annotatedfor part of speech as input.
This method is based onthe hypothesis that constituent boundaries can be ex-tracted from a given part-of-speech n-gram by analyzingthe mutual information values within the n-gram, ex-tended to a new generalization of the information the-oretic measure of mutua l  informat ion.
This hypothesisis supported by the performance of an implementationof this parsing algorithm which determines recursivelynested sentence structure, with an error rate of roughly2 misplaced boundaries for test sentences of length 10-15 words, and five misplaced boundaries for sentencesof 15-30 tokens.
To combat a limited set of specific cir-cumstances in which the hypothesis fails, we use a small(4 rule, 8 symbol) dist i tuent grammar,  which indicateswhen two parts of speech cannol remain in the sameconstituent.In another experiment, described in Section 3, we in-vestigate whether a distributional analysis can discover1This work was partially supported by DARPA grantNo.N0014-85-K0018, byDARPA and AFOSR jointly under grantNo.
AFOSR-90-0066, and by ARO grant No.
DAAL 03-89-C0031PRI.
Thanks to Ken Church, Stuart Shleber, Max Mintz, AravindJoshi, Lila Gleitman and Tom Veatch for their valued suggestionsand discussion.a part of speech tag set which might prove adequate tosupport experiments like that discussed above.
We havedeveloped a similarity measure which accurately clus-ters closed-class lexical items of the same grammaticalcategory, excepting words which are ambiguous betweenmultiple parts of speech.2 A Mutua l  In format ion Parser2.1 IntroductionIn this section, we characterize a constituent boundaryparsing algorithm, using an information-theoretic mea-sure called generalized mutual information, which servesas an alternative to traditional grammar-based parsingmethods.
We view part-of-speech sequences as stochas-tic events and apply probabilistic models to these events.Our hypothesis is that constituent boundaries, or "dis-tituents," can be extracted from a sequence of n cate-gories, or an n-gram, by analyzing the mutual informa-tion values of the part-of-speech sequences within thatn-gram.
In particular, we demonstrate that the gener-alized mutual information statistic, an extension of thebigram (pairwise) mutual information of two events inton-space, acts as a viable measure of continuity in a sen-tence.This hypothesis assumes that, given any constituentn-gram, a la2 .
.
.a , ,  the probability of that constituentoccurring is usually significantly higher than the proba-bility of ala2 .. ?
a ,a ,+ l  occurring.
This is true, in gen-eral, because most constituents appear in a variety ofcontexts.
Once a constituent is detected, it is usuallyvery difficult to predict what part-of-speech will comenext.
As it turns out, however, there are cases in whichthis assumption is not valid, but only a handful of thesecases are responsible for a majority of the errors made bythe parser.
To deal with these cases, our algorithm in-cludes what we will call a distituent grammar - -  a list oftag pairs which cannot be adjacent within a constituent.One such pair is noun prep, since English does not allowa constituent consisting of a noun followed by a preposi-tion.
Notice that the nominal head of a noun phrase maybe followed by a prepositional phrase; in the context ofdistituent parsing, once a sequence of tags, such as (prepnoun),  is grouped as a constituent, it is considered as275a unit.
Our current distituent grammar consists of fourrules of two tokens each.Our current implementation of this parsing algorithmdetermines a recursive unlabeled bracketing of unre-stricted English text.
The generalized mutual informa-tion statistic and the distituent grammar combine toparse sentences with, on average, two errors per sen-tence for sentences of 15 words or less, and five errors persentence for sentences of 30 words or less (based on sen-tences from a reserved test subset of the Tagged BrownCorpus, see footnote 2).
Many of the errors on longersentences result from conjunctions, which are tradition-ally troublesome for grammar-based algorithms as well.Further, this parsing technique is reasonably efficient,parsing a 35,000 word corpus in under 10 minutes on aSun 4/280.While many stochastic approaches to natural anguageprocessing that utilize frequencies to estimate probabili-ties suffer from sparse data, sparse data is not a concernin the domain of our algorithm.
Sparse data usuallyresults from the infrequency of word sequences in a cor-pus.
The statistics extracted from our training corpusare based on tag n-grams for a set of 64 tags, not word n-grams.
2 The corpus size is sufficiently large that enoughtag n-grams occur with sufficient frequency to permitaccurate stimates of their probabilities.
Therefore, thekinds of estimation methods of (n + 1)-gram probabili-ties using n-gram probabilities discussed in Katz (1987)and Church & Gale (1989) are not needed.This line of research was motivated by a series ofsuccessful applications of mutual information statisticsto other problems in natural language processing.
Inthe last decade, research in speech recognition (Je-linek 1985), noun classification (Hindle 1988), predicateargument relations (Church & Hanks 1989), and otherareas have shown that mutual information statistics pro-vide a wealth of information for solving these problems.2 .2  Mutua l  In fo rmat ion  S ta t i s t i csThe mutual information statistic (Fano 1961) is a mea-sure of the interdependence of two signals in a message.It is a function of the probabilities of the two events:Mz( , u) = log u)  x(z)Pv(y)"In this paper, the events x and y will be part-of-speechn-grams (instead of single parts-of-speech, as in someearlier work).Experiments that we will not report here show thatsimple mutual information statistics computed on n-gram sequences are not sufficient for the task at hand.Instead, we have moved to a statistic which we will call"generalized mutual information," because it is a gen-eralization of the mutual information of part-of-speech2The corpus we use to train our  parser  is the Tagged BrownCorpus (Francis and Ku~era, 1982).
Ninety percent of the corpusis used for t ra in ing the parser, and the other ten percent is usedfor testing.
The  tag set used is a subset  of the Brown Corpus tagset.bigrams into n-space.
Generalized mutual informationuses the context on both sides of adjacent parts-of-speechto determine a measure of its distituency in a given sen-tence.While our distituent parsing technique relies on gen-eralized mutual information of n-grams, the foundationsof the technique will be illustrated with the base case ofsimple mutual information over the space of bigrams forexpository convenience.2.2.1 Genera l i zed  Mutua l  In fo rmat ionIn applying the concept of mutual information to theanalysis of sentences, the interdependence of part-of-speech n-grams (sequences of n parts-of-speech) mustbe considered.
Thus, we consider an n-gram as a bigramof an nx-gram and an n2-gram, where nl + n2 = n. Themutual information of this bigram is.
?427(n i-gram, n2-gram) P \[n-gram\]= log 79\[nl_gram\]:P\[nz_gram\].Notice that there are (n -1 )  ways of partitioning an n-gram.
Thus, for each n-gram, there is an (n -  1) vector ofmutual information values.
For a given n-gram za .. .
Zn,we can define the mutual information values of z by:== log 7~(X l .
.
.
z , )where l<k<n.Notice that, in the above equation, for each 2vt27~(z),the numerator, 7~(xl .
.
.
x , ) ,  remains the same while thedenominator, P (Z l .
.
.
Zk)~(Xk+l  .
.
.
Xn) ,  depends on k.Thus, the mutual information value achieves its mini-mum at the point where the denominator is maximized.The empirical claim to be tested in this paper is thatthe minimum is achieved when the two components ofthis n-gram are in two different constituents, i.e.
whenzkzk+l is a distituent.
Our experiments show that thisclaim is largely true with a few interesting exceptions.A straightforward approach would assign each poten-tial distituent a single real number corresponding to theextent to which its context suggests it is a distituent.But the simple extension of bigram mutual informationassigns each potential distituent a number for each n-gram of which it is a part.
The question remains howto combine these numbers in order to achieve a validmeasure of distituency.Our investigations revealed that a useful way to com-bine mutual information values is, for each possible dis-tituent zy, to take a weighted sum of the mutual infor-mation values of all possible pairings of n-grams endingwith z and n-grams beginning with y, within a fixedsize window.
So, for a window of size w = 4, given thecontext z l  z2zaz4, the generalized mutual information ofX2X3 :M274(xlz2, z3z4),= /e13,~27(z2, z3) + k23AZ(z2, z~z4) +276which is equivalent tolog (k \ /In general, the generalized mutual information of anygiven bigram xy in the context x l .
.
.X i - l xyy l .
.
.Y  j -1is equivalent to/ 1Yi  x 'Exl )Xcrosses  zyXXdoes  not  cross  zyThis formula behaves in a manner consistent withone's expectation of a generalized mutual informationstatistic.
It incorporates all of the mutual informationdata within the given window in a symmetric manner.Since it is the sum of bigram mutual information values,its behavior parallels that of bigram mutual information.The standard deviation of the values of the bigrammutual information vector of an n-gram is a valid mea-sure of the confidence of these values.
Since distituencyis indicated by mutual information minima, we use thereciprocal of the standard eviation as a weighting func-tion.2 .3  The  Pars ing  A lgor i thmThe generalized mutual information statistic is the mosttheoretically significant aspect of the mutual informationparser.
However, if it were used in a completely straight-forward way, it would perform rather poorly on sentenceswhich exceed the size of the maximum word window.Generalized mutual information is a local measure whichcan only be compared in a meaningful way with othervalues which are less than a word window away.
In fact,the further apart two potential distituents are, the lessmeaningful the comparison between their correspondingG.A4Z values.
Thus, it is necessary to compensate for thelocal nature of this measure algorithmically.He directed the cortege of autos to the dunesnear Santa Monica.Figure 1: Sample sentence from the Brown Corpusgiven by that n-gram.
These values are calculated oncefor each sentence and referenced frequently in the parseprocess.Distituent Pass 1 DG Pass 2 Pass 3pro verb 3.28 3.28 i 3.P8 3.28verb det 3.13 3.13 I 3.13 3.13det noun 11.18 11.18noun prep 11.14 -co  8.18prep noun 1.20 1.20noun prep 7.41 -co  3.91 2.45prep det 16.89 16.89 10.83det noun 16.43 16.43noun prep 12.73 -co  7.64 4.13prep noun 7.36 7.36Figure 2: Parse node table for sample sentenceNext, a parse node is allocated for each tag in the sen-tence.
A generalized mutual information value is com-puted for each possible distituent, i.e.
each pair of parsenodes, using the previously calculated bigram mutual in-formation values.
The resulting parse node table for thesample sentence is indicated by Pass 1 in the parse nodetable (Figure 2).At this point, the algorithm deviates from what onemight expect.
As a preprocessing step, the distituentgrammar is invoked to flag any known distituents byreplacing their G.A427 value with -co .
The results of thisphase are indicated in the DG column in the parse nodetable.The first w tags in the sentence are processed usingan n-ary-branching recursive function which branchesat the minimum G.A4I value of the given window, withmarginal differences between Q.A4Z values ignored.
Thelocal minima at which branching occurs in each pass ofthe parse are indicated by italics in the parse node table.Instead of using this tree in its entirety, only thenodes in the leftmost and rightmost constituent leavesare pruned.
The rest of the nodes in the window arethrown back into the pool of nodes.
The algorithm isapplied again to the leftmost and rightmost w remain-ing tags until no more tags remain.
The first pass of theparser is complete, and the sentence has been partitionedinto constituents (Figure 3).In Magerman and Marcus (1990) we describe the pars-ing algorithm in detail, and trace the parsing of a sam-ple sentence (Figure 1) selected from the section of theTagged Brown Corpus which was not used for trainingthe parser.
The sample sentence is viewed by the parseras a tag sequence, since the words in the sentence arenot accounted for in the parser's tatistical model.A bigram mutual information value vector and itsstandard eviation are calculated for each n-gram in thesentence, where 2 _< n _< 10.
If the frequency of ann-gram is below a certain threshold (< 10, determinedexperimentally), then the mutual information values areall assumed to be 1, indicating that no information is(He) (directed) (the cortege) (of autos)(to) (the dunes) (near Santa Monica)Figure 3: Constituent structure after Pass 1The algorithm terminates when no new structure hasbeen ascertained on a pass, or when the lengths of twoadjacent constituents sum to greater than w. After twomore passes of the algorithm, the sample sentence is par-titioned into two adjacent constituents, and thus the al-gorithm terminates, with the result in figure 4.
In thisexample, the prepositional phrase "near Santa Monica"is not attached to the noun phrase "the dunes" as it277should be; therefore, the parser output for the samplesentence has one error.
(He (d i rec ted  ( ( the  cor tege)  (of  autos ) ) )( ( to  ( the dunes))(near  Santa Monica)))Figure 4: Resulting constituent structure after Pass 3cover the feature set and word classes of a language.
3 Itis based upon the following idea, a variant of the dis-tributional analysis methods from Structural Linguistics(Harris 51,Harris 68): features license the distributionalbehavior of lexical items.
At the two extremes, a wordwith no features would not be licensed to appear in anycontext at all, whereas a word marked with all featuresof the language would be licensed to appear in everypossible context.2 .4  Resu l tsA careful evaluation of this parser, like any other, re-quires some "gold standard" against which to judge itsoutput.
Soon, we will be able to use the skeletal pars-ing of the Penn Treebank we are about to begin pro-ducing to evaluate this work (although evaluating thisparser against materials which we ourselves provide isadmittedly problematic).
For the moment, we have sim-ply graded the output of the parser by hand ourselves.While the error rate for short sentences (15 words orless) with simple constructs is accurate, the error ratefor longer sentences i more of an approximation than arigorous value.On unconstrained free text from a reserved test cor-pus, the parser averages about two errors per sentencefor sentences under 15 words in length.
On sentencesbetween 16 and 30 tokens in length, it averages between5 and 6 errors per sentence.
In nearly all of these longersentences and many of shorter ones, at least one of theerrors is caused by confusion about conjuncts.One interesting possibility is to use the generalizedmutual information statistic to extract a grammar froma corpus.
Since the statistic is consistent, and its win-dow can span more than two constituents, it could beused to find constituent units which occur with the samedistribution in similar contexts.
Given the results ofthe next section, it may well be possible to use auto-matic techniques to first determine a first approxima-tion to the set of word classes of a language, given onlya large corpus of text, and then extract a grammar forthat set of word classes.
Such a goal is very difficult,of course, but we believe that it is worth pursuing.
Inthe end, we believe that this, like many problems innatural anguage processing, cannot be solved eJficienilyby grammar-based algorithms nor accurately by purelystochastic algorithms.
We believe strongly that the so-lution to some of these problems may well be a combi-nation of both approaches.3 Discover ing the Word Classesof a Language3.1  In t roduct ionAs we ask immediately above, to what extent is it pos-sible to discover by some kind of distributional analysisthe kind of part-of-speech tags upon which our mutualinformation parser depends?
In this section, we exam-ine the possibility of using distributional analysis to dis-3 .2  The  A lgor i thmThe feature discovery system works as follows.
First,a large amount of text is examined to discover the fre-quency of occurrence of different bigrams.
4 Based uponthis data, the system groups words into classes.
Twowords are in the same class if they can occur in the samecontexts.
In order to determine whether x and y belongto the same class, the sytem first examines all bigramscontaining x.
If for a high percentage of these bigrams,the corresponding bigram with y substituted for x existsin the corpus, then it is likely that y has all of the fea-tures that x has (and maybe more).
I f  upon examiningthe bigrams containing y the system is able to concludethat x also has all of the features that y has, it thenconcludes that x and y are in the same class.For every pair of bigrams, the system must determinehow much to weigh the presence of those bigrams as ev-idence that two words have features in common.
Forinstance, assume: (a) the bigram ~he boy appears manytimes in the corpus being analyzed, while the sits neveroccurs.
Also assume: (b) the bigram boy the (as in theboy  the  girl kissed .
.
. )
occurs once and sits ~he neveroccurs.
Case (a) should be much stronger evidence thatboy  and sits are not in the same class than case (b).For each bigram o~x occurring in the corpus, evidenceoffered by the presence (or absence) of the bigram ayis scaled by the frequency of ax in the text divided bythe total number of bigrams containing x on their righthand side.
Since the end-of-phrase position is less re-strictive, we would expect each bigram involving thisposition and the word to the right of it to occur less fre-quently than bigrams of two phrase-internal words.
Byweighing the evidence, bigrams which cross boundarieswill be weighed less than those which do not.3.2.1 The  Specif icsThe function impl ies(x,y)  calculates the likelihood (ona scale of \[0..1\]) that word y contains all of the featuresof word x.
For example, we would expect the value ofimpl ies( 'a ' ,  'the') to be close to 1, since 'the' can occurin any context which 'a' can occur in.
Note that: im-plies(x,y) A implies(y,x) iff x and y are in the sameclass.aWe consider the set of features of a particular language to beall attr ibutes which that language makes reference to in its syntax.4For this experiment, we take a very local view of context, onlyconsidering bigrams.278The function leftimply(x,y) is the likelihood (on ascale of [0..1]) that y contains all of the features ofx, where this likelihood is derived from looking a t  bi-grams of the form: x a .
rightimply(x,y) derives thelikelihood by examining all bigrams of the form: ax.bothoccur(a,P)  is 1 if both bigrams a and /?
occurin the corpus, and p occurs with a frequency at least11THRESHOLD of that of a, for some THRESHOLD.5bothoccur  accounts for the fact that we cannot expectthe distribution of two equivalent words over bigrams tobe precisely the same, but we would not expect the twodistributions to be too dissimilar either.bothoccurleft (ab, cd) =1 if bigrams ab and cd appear in the corpus andpercentageleft(c,d) 2 (11THRESHOLD *t (a$))0 otherwiseWhen computing the relation between x and allother words, we use the following function, percent-age, to weigh the evidence (as described above), wherecount(ab) is the number of occurrences of the bigramab in the corpus, and numright(x) (numleft(x)) is thetotal number of bigrams with x on their right hand side(left hand side).count (x y)percentageleft (x, y) =numle ft(x)For all pairs of words, x and y ,  we calculate im-p l i e s ( ~ , ~ )  and implies(y,x).
We can then find wordclasses in the following way.
We first determine a thresh-old value, where a stronger value will result in more spe-cific classes.
Then, for each word x, we find all words51n the experiments we ran, we found THRESHOLD = 6 to givethe best results.
This value was found by examining the values ofimplication found between the ,  a and an.y such that both irnplies(x,y) and implies(y,x) aregreater than the threshold.
We next take the transi-tive closure of pairs of sets with nonempty intersectionover all of these sets, and the result is a set of sets, whereeach set is a word class.
Classes of different degrees ofspecificity are found by varying the degree of similaritybetween distributions needed to conclude that two wordsare in the same class.
If a high degree of similarity is re-quired, all words in a class will have the same features.If a lower degree of similarity is required, then words ina class must have most, but not all, of the same features.3.3 The ExperimentTo test the algorithm discussed above, we ran the fol-lowing experiment.
First, the number of occurrences ofeach bigram in the corpus was determined.
Statistics ondistribution were determined by examining the completeBrown Corpus (Francis 82), where infrequently occurringopen-ciass words were replaced with their part-of-speechtag.
We then ran the program on a group of words in-cluding all closed-class words which occurred more than250 times in the corpus, and the most frequently occur-ring open-class words.
Note that the system attemptedto determine the relations between these words; this doesnot mean that it only considered bigrams a@, where botha and ,f3 were from this list of words which were beingpartitioned.
All bigrams which occurred more than 5times were considered in the distributional analysis.3.4 Analysis of the ExperimentThe program successfully partitioned words into word~ l a s s e s .
~  In addition, it was able to find more fine-grained features.
Among the features found were:[possessive-pronoun] , [singular-determiner], [definite-determiner], [wh-adjunct] and [pronoun+be].
A descrip-tion of some of the word classes the program discoveredcan be found in Appendix A.3.5 The Psychological Plausibility ofDistributional AnalysisIf a child does not know a priori what features are usedin her language, there are two ways in which she canacquire this information: by using either syntactic or s emantic cues.
The child could use syntactic cues such asthe method of distributional analysis described in thispaper.
The child might also rely upon semantic cues.There is evidence that children use syntactic rather thansemantic cues in classifying words.
Peter Gordon (Gor-don 85) ran an experiment where the child was presentedwith an object which was given a made up name.
Forobjects with semantic properties of count nouns (massnouns), the word was used in lexical environments whichonly mass nouns (count nouns) are permitted to be in.Gordon showed that the children overwhelmingly used60ne exception was the class of pronouns.
Since [+nominative]and [-nominative] pronouns do not have similar distribution, theywere not found to be in the same class.Raw no.
Times Total no.of words tagged of wordsBrown Corpus 1,159,381 1 1,159,381Library of America 159,267 2 318,534DOE abstracts 199,928 2 399,856DoT Jones Corpus 2,644,618 1 2,644,618Grand total 4,163,194 4,522,389Tagger No.
of errors Error rateRF 105 1.9Ctt 151 2.8MAM 127 2.3MP 158 2.9MW 136 2.5Mean 135 2.5Table 2: Error rates Table 1: Number of words taggedthe distributional cues and not the semantic ues in clas-sifying the words.
Virginia Gathercole (Gathercole 85)found that "children do not approach the co-occurrenceconditions of much and many with various nouns froma semantic point of view, but rather from a morphosyn-tactic or surface-distributional one."
Yonata Levy (Levy83) examined the mistakes young children make in clas-sifying words.
The mistakes made were not those onewould expect he child to make if she were using seman-tic cues to classify words.4 Penn TreebankIn this section, we report some recent performance mea-sures of the Penn Treebank Project.To date, we have tagged over 4 million words by part ofspeech (cf.
Table 1).
We are tagging this material witha much simpler tagset than used by previous projects,as discussed at the Oct. 1989 DARPA Workshop.
Thematerial is first processed using Ken Church's tagger(Church 1988), which labels it as if it were Brown Corpusmaterial, and then is mapped to our tagset by a SED-script.
Because of fundamental differences in taggingstrategy between the Penn Treebank Project and theBrown project, the resulting mapping is about 9% in-accurate, given the tagging guidelines of the Penn Tree-bank project (as given in 40 pages of explicit taggingguidelines).
This material is then hand-corrected by ourannotators; the result is consistent within annotators toabout 3% (cf.
Table 3), and correct (again, given ourtagging guidelines) to about 2.5% (cf.
Table 2), as willbe discussed below.
We intend to use this material toretrain Church's tagger, which we then believe will beaccurate to less than 3% error rate.
We will then adju-dicate between the output of this new tagger, run on thesame corpus, and the previously tagged material.
Webelieve that this will yield well below 1% error, at anadditional cost of between 5 and 10 minutes per 1000words of material.
To provide exceptionally accuratebigram frequency evidence for retraining the automatictagger we are using, two subcorpora (Library of America,DOE abstracts) were tagged twice by different annota-tors, and the Library of America texts were adjudicatedby a third annotator, yielding ~160,000 words taggedwith an accuracy estimated to exceed 99.5%.Table 2 provides an estimate of error rate for part-of-speech annotation based on the tagging of the sampledescribed above.
Error rate is measured in terms of theCH MAM MP MWRF 2.6% 3.5% 3.2% 3.0%CH - 2.9% 3.9% 3.7%MAM - - 3.3% 2.7%MP - - - 2.8%Mean: 3.2%Table 3: Inter-annotator inconsistencynumber of disagreements with a benchmark version ofthe sample prepared by Beatrice Santorini.
We havealso estimated the rate of inter-annotator inconsistencybased on the tagging of the sample described above (cf.Table 3).
Inconsistency is measured in terms of the pro-portion of disagreements of each of the annotators witheach other over the total number of words in the testcorpus (5,425 words).Table 4 provides an estimate of speed of part-of-speech annotation for a set of ten randomly selected textsfrom the DoT Jones Corpus (containing a total of 5,425words), corrected by each of our annotators.
The an-notators were throughly familiar with the genre, havingspent over three months immediately prior to the ex-periment correcting texts from the same Corpus.
Giventhat the average productivity overall of our project hasbeen between 3,000-3,500 words per hour of time billedby our annotators, it appears that our strategy of hiringannotators for no more than 3 hours a day has provento be quite successful.Finally, the summary statistics in Table 5 provide anestimate of improvement of annotation speed as a func-tion of familiarity with genre.
We compared the anno-tators' speed on two samples of the Brown Corpus (10texts) and the DoT Jones Corpus (100 texts).
We ex-amined the first and last samples of each genre that theTagger Time Words Minutes per(in minutes) per hour 1,000 wordsRF 68 4,804 12.5CH 79 4,129 14.5MAM 57 5,751 10.4MP 74 4,423 13.3MW 100 3,268 18.3Mean 76 4,283 14.0280Table 4: Speed of part-of-speech annotationvWords Minutes perper hour 1,000 wordsEarly Brown 2,816 21.3Dow Jones 1,711 35.1Mean 2,621 22.9Late Brown 3,483 17.2Dow Jones 3,641 16.5Mean 3,511 17.1I I Improvement 34% 25% 1Table 5: Speed as function of familiarity with genreannotators tagged; in each case, more than two monthsof experience lay between the samples.ReferencesChurch, K. 1988.
A Stochastic Parts Program andNoun Phrase Parser for Unrestricted Text.
In Pro-ceedings of the Second Conference on Applied Nat-ural Language Processing.
Austin, Texas.Church, K. and Gale, W. 1990.
Enhanced Good-Turing and Cat-Cal: Two New Methods for Esti-mating Probabilities of English Bigrams.
Comput-ers, Speech and Language.Church, K. and Hanks, P. 1989.
Word AssociationNorms, Mutual Information, and Lexicography.
InProceedings of the 27th Annual Conference of theAssociation of Computational Linguistics.Fano, R. 1961.
Transmission of Information.
NewYork, New York: MIT Press.Francis, W. and KuEera, H. 1982.
Frequency Anal-ysis of English Usage: Lexicon and Grammar.Boston, Mass.
: Houghton Mifflin Company.Gathercole, V. 'He has too much hard questions':the acquisition of the linguistic mass-count distinc-tion in much and many.
Journal of Child Language,12: 395-415.Gordon, P. Evaluating the semantic categories hy-pothesis: the case of the count/mass distinction.Cognition, 20: 209-242.Harris, Z.S.
(1951) Structural Linguistics.
Chicago:University of Chicago Press.Harris, Z.S.
(1968) Mathematical Stmctures of Lan-guage.
New York: Wiley.Hindle, D. 1988.
Acquiring a Noun Classificationfrom Predicate-Argument Structures.
Bell Labora-tories.Jelinek, F. 1985.
Self-organizing Language Modeling[12] Katz, S. M. 1987.
Estimation of Probabilities fromSparse Data for the Language Model Component ofa Speech Recognizer.
IEEE Transactions on Acous-tics, Speech, and Signal Processing, Vol.
ASSP-$5,No.
3.
[13] Levy, Y.
It's frogs all the way down.
Cognition, 15:7593.
[14] Magerman, D. and Marcus, M. Parsing a Natu-ral Language Using Mutual Information Statistics,Proceedings of AAAI-90, Boston, Mass (forthcom-ing).
[15] Pinker, S. Learnability and Cognition.
Cambridge:MIT Press.for Speech Recognition.
IBM Report.Where rn
