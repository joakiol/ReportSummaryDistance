Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 130?141,Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational LinguisticsAn Empirical Evaluation of a Statistical Dialog System in Public UseJason D. WilliamsAT&T Labs - Research, Shannon Laboratory, 180 Park Ave., Florham Park, NJ 07932, USAjdw@research.att.comAbstractThis paper provides a first assessment of a sta-tistical dialog system in public use.
In our di-alog system there are four main recognitiontasks, or slots ?
bus route names, bus-stop lo-cations, dates, and times.
Whereas a conven-tional system tracks a single value for each slot?
i.e., the speech recognizer?s top hypothesis?
our statistical system tracks a distributionof many possible values over each slot.
Pastwork in lab studies has showed that this distri-bution improves robustness to speech recog-nition errors; but to our surprise, we foundthe distribution yielded an increase in accu-racy for only two of the four slots, and actu-ally decreased accuracy in the other two.
Inthis paper, we identify root causes for thesedifferences in performance, including intrin-sic properties of N-best lists, parameter set-tings, and the quality of statistical models.
Wesynthesize our findings into a set of guidelineswhich aim to assist researchers and practition-ers employing statistical techniques in futuredialog systems.1 IntroductionOver the past decade, researchers have worked to ap-ply statistical techniques to spoken dialog systems,and in controlled laboratory studies, statistical di-alog systems have been shown to improve robust-ness to errors compared to conventional approaches(Henderson and Lemon, 2008; Young et al, 2010;Thomson and Young, 2010).
However, statisticaltechniques have not yet been evaluated in a publiclydeployed system, and real users behave very differ-ently to usability subjects (Raux et al, 2005; Ai etal., 2008).
So there is an important open questionwhether statistical dialog systems improve perfor-mance with real users.This paper provides a first evaluation of a publi-cally deployed statistical dialog system, AT&T Let?sGo (Williams et al, 2010).
AT&T Let?s Go pro-vides bus times for Pittsburgh, and received approx-imately 750 calls from real bus riders during the2010 Spoken Dialog Challenge (Black et al, 2010).AT&T Let?s Go is based on a publicly availabletoolkit (Williams, 2010a) and achieved the highestrates of successful task completion on real callers inthe challenge, so it provides a relevant exercise fromwhich to draw inferences.AT&T Let?s Go collected four types of informa-tion, or slots: bus route names, bus-stop names,dates, and times.
For each slot, we measured turn-level accuracy of the deployed statistical system andcompared it to accuracy without application of thestatistical techniques (i.e., the top speech recogni-tion result).To our surprise, we found that statistical tech-niques appeared to improve accuracy for only two ofthe four slots, and decreased accuracy for the othertwo.
To investigate this, we considered four mech-anisms by which statistical methods can differ fromthe top speech recognition result.
Analyzing the ef-fects of each mechanism on each slot enables un-derlying causes to be identified: for example, onemechanism performed exceptionally well when itsstatistical models was well matched to usage data,but rather poorly when its model diverged from realusage.
We believe this analysis ?
the focus of thispaper ?
is relevant to researchers as well as practi-130tioners applying statistical techniques to productionsystems.In this paper, Section 2 reviews the operation ofstatistical spoken dialog systems.
Section 3 thendescribes the AT&T Let?s Go dialog system.
Sec-tion 4 reports on overall accuracy, then analyzes theunderlying reasons for accuracy gains and losses.Section 5 tackles how well error in the belief statecan be identified compared to speech recognition er-rors.
Section 6 concludes by summarizing lessonslearned.2 Statistical dialog systemsStatistical dialog systems maintain a distributionover a set of hidden dialog states.
A dialog stateincludes information not directly observable to thedialog system, such as the user?s overall goal in thedialog or the user?s true action (e.g., the user?s truedialog act).
For each dialog state s, a posterior prob-ability of correctness called a belief is maintainedb(s).
The set of hidden dialog states and their be-liefs is collectively called the belief state, and up-dating the belief state is called belief tracking.
Herewe will present belief tracking at a level sufficientfor our purposes; for a more general treatment, see(Williams and Young, 2007).At the start of the dialog, the belief state is initial-ized to a prior distribution b0(s).
The system thentakes an action a, and the user takes an action inresponse.
The automatic speech recognizer (ASR)then produces a ranked list of N hypotheses for theuser?s action, u = (u1, .
.
.
, uN ), called an N-bestlist.
For each N-best list the ASR also produces adistribution Pasr(u) which assigns a local, context-independent probability of correctness to each item,often called a confidence score.
The belief state isthen updated:b?
(s) = k ?
?uPasr(u)Pact(u|s, a)b(s) (1)where Pact(u|s, a) is the probability of the user tak-ing action u given the dialog is in hidden state s andthe system takes action a. k is a normalizing con-stant.In practice specialized techniques must be used tocompute Eq 1 in real-time.
The system in this paperuses incremental partition recombination (Williams,2010b); alternatives include the Hidden InformationState (Young et al, 2010), Bayesian Update of Dia-log States (Thomson and Young, 2010), and particlefilters (Williams, 2007).
The details are not impor-tant for this paper ?
the key idea is that Eq 1 synthe-sizes a prior distribution over dialog states togetherwith all of the ASR N-best lists and local confidencescores to form a cumulative, whole-dialog poste-rior probability distribution over all possible dialogstates, b(s).In the system studied in this paper, slots arequeried separately, and an independent belief state ismaintained for each.
Consequently, within each slotuser actions u and hidden states s are drawn fromthe same set of slot values.
Thus the top ASR resultu1 represents the ASR?s best hypothesis for the slotvalue in the current utterance, whereas the top dia-log state argmaxs b(s) = s?
represents the beliefstate?s best hypothesis for the slot value given all ofthe ASR results so far, a prior over the slot values,and models of user action likelihoods.
The promiseof statistical dialog systems is that s?
will (we hope!
)be correct more often than u1.
In the next section,we measure this in real dialogs.3 AT&T Let?s GoAT&T Let?s Go is a statistical dialog system thatprovides bus timetable information for Pittsburgh,USA.
This system was created to demonstrate aproduction-grade system built following practicescommon in industry, but which incorporates two sta-tistical techniques: belief tracking with the AT&TStatistical Dialog Toolkit (Williams, 2010a), andregression-based ASR confidence scores (Williamsand Balakrishnan, 2009).As with most commercial dialog systems, AT&TLet?s Go follows a highly directed flow, collectingone slot at a time.
There are four types of slots:ROUTE, LOCATION, DATE, and TIME.
The sys-tem can only recognize values for the slot beingqueried, plus a handful of global commands (?re-peat?, ?go back?, ?start over?, ?goodbye?, etc.)
?mixed initiative and over-completion were not sup-ported.
As mentioned above, an independent beliefstate is maintained for each slot: this was an inten-tional design decision made in order to use statisticaltechniques within current commercial practices.131The system opens by asking the user to say a busROUTE, or to say ?I?m not sure.?
The system nextasks for the origin and destination LOCATIONs.
Thesystem then asks if the caller wants times for the?next few buses?
; if not, the system asks for theDATE then TIME in two separate questions.
Finallybus times are read out.After requesting the value of a slot, the system re-ceives an N-best list, assigns each item a confidencescore Pasr(u), and updates the belief in (only) thatslot using Eq 1.
The top dialog hypothesis s?
andits belief b(s?)
are used to determine which actionto take next, following a hand-crafted policy.
This isin contrast to a conventional dialog system, in whichthe top ASR result and its confidence govern dialogflow.
Figure 6 shows the design of AT&T Let?s Go.In the period July 16 ?
August 16 2010, AT&TLet?s Go received 742 calls, of which 670 had oneor more user utterances.
These calls contained atotal of 8269 user utterances, of which 4085 werein response to requests for one of the four slots.
(The remainder were responses to yes/no questions,timetable navigation commands like ?next bus?,etc.
)Our goal in this paper is to determine whethertracking a distribution over multiple dialog statesimproved turn-level accuracy compared to the topASR result.
To measure this, we compare the accu-racy of the top belief state and the top ASR result.
Atranscriber listened to each utterance and marked thetop ASR hypothesis as correct if it was an exact lex-ical or semantic match, or incorrect otherwise.
Thesame was then done for the top dialog hypothesis ineach turn.Accuracy of the top ASR hypothesis and the topbelief state are shown in Table 1, which indicatesthat belief monitoring improved accuracy for ROUTEand DATE, but degraded accuracy for LOCATION andTIME.
We had hoped that belief tracking would im-prove accuracy for all slots; seeing that it hadn?tprompted us to investigate the underlying causes.4 Belief tracking analysisWhen an ASR result is provided to Eq 1 and a newbelief state is computed, the top dialog state hypoth-esis s?
may differ from top ASR result u1.
For-mally, these differences are simply the result of eval-Slot ROUTE LOCATION DATE TIMEUtts 1520 2235 173 157ASR 769 1326 124 80correct 50.6% 59.3% 71.7% 51.0 %Belief 799 1246 139 63correct 52.6% 55.7% 80.3% 40.1%Belief +30 -80 +15 -17?
ASR +2.0% -3.6% +8.7% -10.8%Table 1: Accuracy of the top ASR result and top be-lief state.
LOCATION includes both origin and des-tination utterances.
Most callers requested the nextbus so few were asked for DATE and TIME.uating this equation.
However, intuitively there arefour mechanisms which cause differences, and eachdifference can be explained by the action of one ormore mechanisms.
These mechanisms are summa-rized here; the appendix provides graphical illustra-tions.1?
ASR re-ranking: When computing a con-fidence score Pasr(u), it is possible that theentry with the highest confidence u?
=argmaxu Pasr(u) will not be the first ASR re-sult, u1 6= u?.
In other words, if the confidencescore re-ranks the N-best list, this may cause s?to differ from u1 (Figure 7).?
Prior re-ranking: Statistical techniques use aprior probability for each possible dialog state?
in our system, each slot value ?
b0(s).
If anitem recognized lower-down on the N-best listhas a high prior, it can obtain the most belief,causing s?
to differ from u1 (Figure 8).?
Confidence aggregation: If the top beliefstate s?
has high belief, then subsequent low-confidence recognitions which do not contains?
will not dislodge s?
from the top position,causing s?
to differ from u1 (Figure 9).?
N-best synthesis: If an item appears in two N-best lists, but is not in the top ASR N-best posi-tion in the latter recognition, it may still obtainthe highest belief, causing s?
to differ from u1(Figure 10).1This taxonomy was developed for belief tracking over asingle slot.
For systems which track joint beliefs over multipleslots, additional mechanisms could be identified.132   		fffiffi   		fffiffi! !" "		fffiffi# #  		fffiffiLOCATIONDATEROUTETIMEAccuracyAccuracyFigure 1: Differences in accuracy between ASR and belief monitoring.
?Baseline?
indicates accuracy amongutterances where belief monitoring had no effect ?
where ASR and belief monitoring are both correct, orboth incorrect.
Blue bars show cases where the top belief state s?
is correct and the top ASR result u1 isnot; red bars show cases where u1 is correct and s?
is not.
The plot is arranged to show a running totalwhere blue bars increase the total and red bars decrease the total.
Percentages under blue and red bars showthe change in accuracy due to each mechanism.
The black bar on the right shows the resulting accuracy indeployment.We selected utterances where the correctness of thetop ASR result and top dialog hypothesis differed ?where one was correct and the other was not ?
andlabeled these by hand to indicate which of the fourmechanisms was responsible for the difference.
Ina few cases multiple mechanisms were responsible;these were labeled with the first contributing mech-anism in the order listed above.Figures 1 shows results.
Of the four mechanisms,prior re-ranking occurred most often, and confidenceaggregation occurred least often.
Interestingly, somemechanisms provided a performance gain for certainslots and a degradation for others.
This led us to lookat each mechanism in detail.4.1 Evaluation of ASR Re-rankingThe recognizer used by AT&T Let?s Go produced anN-best list ordered by decoder cost.
After decoding,a confidence score was assigned to each item on theN-best list using a regression model that operated onfeatures of the recognition (Williams and Balakrish-nan, 2009).
The purpose of this regression was toassign a probability of correctness to each item onthe N-best list; while it was not designed to re-rankthe N-best list, the design of this model did allow itto assign a higher score to the n = 2 hypothesis thanthe n = 1 hypothesis.
When this happens, we saythe N-best list was re-ranked.
Table 2 shows howoften ASR re-ranking occurred, and how often the133fl$flfl$%fl$&fl$'fl$(fl$)fl$*fl$+fl$,fl$-%$flfl$fl fl$% fl$& fl$' fl$( fl$) fl$* fl$+ fl$, fl$- %$fl./0/123456748394:/34;<=>?
@A BC DBEE>D@ F@>G BH IJK>L@ MFL@N OL O CEOD@FBH BC MFL@ M>HP@AQRSTU VWXTYZTSFigure 2: Cumulative distribution of the positionof the correct item on N-Best lists for the ROUTEwhen the correct item is in position 2 .
.
.
N .
Depthis shown as a fraction of the N-Best list length.ASR re-ranking helped and hurt ASR accuracy.
Wefound that re-ranking degraded ASR accuracy for allslots, except DATE where it had a trivial positive im-pact.
This suggested a problem with our confidencescore; examining ROUTE, LOCATION, and TIME wefound that the distributions used by the confidencescore that apportions mass to items 2 .
.
.
N were farmore concentrated on the N=2 entry than observedin deployment (Figure 2).
Investigation revealed abug in the model estimation code for these slots.Where ASR re-ranking decreased ASR accuracy,we?d expect to see it also decrease belief state ac-curacy.
Indeed, for the TIME slot, ASR re-rankingcauses a substantial decrease in belief state accu-racy, highlighting the importance of an accurate con-fidence score to statistical techniques.
However, forthe ROUTE slot, we see an increase in belief state ac-curacy attributed to ASR re-ranking.
This can be ex-plained by interaction between ASR re-ranking andprior re-ranking, discussed next.4.2 Evaluation of prior re-rankingWhereas N-best re-ranking affects b?
(s) via Pasr,prior re-ranking affects b?
(s) via the prior proba-bility in a slot b0(s) ?
i.e., the initial belief, at thestart of the dialog, for each value the slot may take.If the slot?s prior is uniform (non-informative), weexpect to see no effect on accuracy due to the prior?
indeed, Figure 1 shows that priors had no effecton belief accuracy for DATE and TIME, which useduniform priors.ROUTE and LOCATION employed a non-uniformprior, and here we?d expect to see a gain in perfor-mance if the prior matches actual use.
Both priorswere computed using a simple heuristic in which theprior was proportional to the number of distinct bus-stops on the route or covered by the location expres-sion, smoothed with a smoothing factor.
For exam-ple, the phrase ?downtown?
covered 17 stops and itsprior was 0.018; the phrase ?airport?
covered 1 stopand its prior was 0.00079.
Even though historical us-age data was available to Spoken Dialog Challenge2010 participants (Parent and Eskenazi, 2010), weinstead chose to base priors on bus-stop counts as atest of whether effective priors could be constructedwithout access to usage data.Overall the prior for ROUTE fit actual usage datawell (Figure 3), and we see a corresponding net gainin belief accuracy of 3.7% = 4.0% ?
0.3% in Fig-ure 1.
However the prior for LOCATION was a poormatch with actual usage (Figure 4), and this causeda net degradation in belief accuracy of ?0.9% =0.5% ?
1.4%.
The key problem is that the heuris-tic wrongly assumed all stops are equally popular:for example, although the airport contained a sin-gle stop (and thus received a very low prior), it wasvery popular.
This suggests that it would be betterto estimate priors based on usage data rather thanthe bus-stop count heuristic.
More broadly, it alsounderscores the importance of accurate priors to sta-tistical dialog techniques.In the previous section, for ROUTE, it was ob-served that ASR re-ranking degraded ASR accuracy,yet caused an improvement in belief accuracy.
Theeffects of the prior explain this: the prior was oftenstronger, such that an error introduced by ASR re-ranking was cancelled by prior re-ranking.
Exam-ining cases where ASR re-ranking occurred but thebelief state was still correct confirmed this.
WhereASR re-ranking and prior re-ranking agreed, theASR re-ranking received credit.
Looking at LOCA-TION, the prior was essentially noise, so ASR re-ranking errors could not be systematically canceledby prior re-ranking in the same way ?
indeed, LO-CATION belief accuracy was degraded by both ASRre-ranking and prior re-ranking.
More broadly, thisprovides a nice illustration of how statistical tech-134Slot ROUTE LOCATION DATE TIMEAll utterances 1520 2235 173 157Utterances with 505 305 3 40ASR re-ranking 33.2% 13.6% 1.7% 25.5%ASR re-ranked; N=2 correct 36 11 1 3(ASR re-ranking helped) +2.4% +0.5 % +0.6 % +1.9 %ASR re-ranked; N=1 correct 63 33 0 9(ASR re-ranking hurt) -4.1% -1.5 % 0 % -5.7 %Net gain from -27 -22 +1 -6ASR re-ranking -1.8 % -1.0% +0.6% -3.8%Table 2: ASR re-ranking.
[\[[][\[[^[\[[_[\[[`[\[]a[\[b^[\[a_[\]^`[\^ca[\c]^defghgijiklfemenopnqrlsjfturhjnvwxy z{x|} ~}}z?
???
?|}?
y?{???????????
????
?Figure 3: Modeled prior for ROUTE vs. observedusage.
The modeled prior was a relatively good pre-dictor of actual usage.niques can combine conflicting evidence ?
in thiscase, from the prior and ASR.4.3 Evaluation of confidence score aggregationThe conditions for confidence score aggregation oc-cur somewhat rarely: for no slot did it have the great-est effect on belief accuracy.
It had the largest effecton DATE; investigation revealed that belief scores forDATE were relatively lower than for other slots (Ta-ble 3).
Since all slots used the same thresholds tomake accept/reject decisions, DATE had proportion-ally more retries in which the top belief hypothesiswas correct, yielding more opportunities for confi-dence aggregation to have an effect.But why were belief values for DATE lower thanfor other slots?
Investigation revealed that a bug??????????????????????????????????????????????????????????????????????????????????????????
???????????
??????
????
????
??????????????
????
?Figure 4: Modeled prior for LOCATION vs. ob-served usage.
The modeled prior was essentiallynoise compared to actual usage.Slot ROUTE LOCATION DATE TIMECorrect 0.90 0.89 0.60 0.73Incorrect 0.52 0.59 0.34 0.53Table 3: Average belief in the top dialog state hy-pothesis when that hypothesis was correct or incor-rect.was causing priors for DATE to be nearly an or-der of magnitude too small, so that each recognizeddate was artificially improbable.
As a result, DATEeffectively had a more stringent threshold for ac-cept/reject decisions.
Although caused by a bug, thiscase study provides a more general illustration: ob-taining sufficient belief to meet higher thresholds re-quires more ASR evidence in the form of more re-135Slot ROUTE LOCATION DATE TIMEAverage N-best list length 5.0 2.8 2.1 4.3N-best accuracy 27.9% 10.6% 46.0% 34.7%Average position of correct item (n > 1) 3.3 3.2 2.6 2.9Table 4: Descriptive statistics for N-best lists.
Average N-best list length indicates the average length of allN-best lists, regardless of accuracy.
N-best accuracy indicates how often the correct item appeared in anyposition n > 1 among cases where the top ASR result n = 1 was not correct.
Average position of correctitem refers to the average n among cases where the correct item appeared with n > 1.tries.4.4 Evaluation of N-best synthesisFor DATE, N-best synthesis had a large positive ef-fect, TIME and LOCATION a small positive effect(or no effect), and ROUTE a small negative effect.N-best synthesis occurs when commonality existsacross N-best lists, so we next examined the N-bestlists for each slot.Table 4 shows three key properties of the N-bestlists.
ROUTE and DATE had the most extreme values:ROUTE had the longest N-best lists, comparativelypoor N-best accuracy, and the correct item appearedfurthest down the N-best list.
By contrast, DATE hadthe shortest N-best lists, the best N-best accuracy,and the correct item appeared closest to the top.
LO-CATION and TIME were between the two.
This rela-tive ordering aligns with the observed effect that N-best synthesis had on belief accuracy, where DATEenjoyed a large improvement and ROUTE suffered asmall degradation.This correlation suggests that basic properties ofthe N-best list govern the effectiveness of N-bestsynthesis: when N-best lists are shorter, more of-ten contain the correct answer, and when the correctanswer is closer to the top position, N-best synthesiscan lead to large gains.
When N-best lists are longer,less often contain the correct answer, and when thecorrect answer is farther from the top position, N-best synthesis can lead to small gains or even degra-dations.5 Identifying belief state errorsThe analysis in the preceding section assessed theaccuracy of the belief state.
In practice, a systemmust decide whether to accept or reject a hypoth-esis, so it is also important to evaluate the abilityof the belief state to discriminate between correctand incorrect hypotheses.
We studied this by plot-ting receiver operating characteristic (ROC) curvesfor each slot, in Figure 5.Where the belief state has higher accuracy(ROUTE, DATE), the belief state shows somewhatbetter ROC results, especially at higher false-acceptrates.
However, gains in ROC performance appearto be due entirely to gains in accuracy: In LOCA-TION, belief tracking made nearly no difference toaccuracy, and the belief state shows virtually no dif-ference to ASR in ROC performance.
TIME suf-fered degradations in both accuracy and ROC perfor-mance.
The trend appears to be that if belief trackingdoes not improve over ASR 1-best, then it seems thatbelief tracking does not enable better accept/rejectdecision to made.
Perhaps addressing the model de-ficiencies mentioned above will improve discrimina-tion ?
this is left to future work.6 ConclusionsThis paper has provided a first assessment of sta-tistical techniques in a spoken dialog system underreal use.
We have found that belief tracking is notguaranteed to improve accuracy ?
its effects vary de-pending on the operating conditions:?
Overall the effects of prior re-ranking and N-best synthesis are largest; confidence aggrega-tion has the smallest effect.?
When N-best lists are useful, N-best synthesiscan have a large positive effect (DATE); whenN-best lists are more noisy, N-best synthesishas a small or even negative effect (ROUTE).?
In the presence of more rejection, confidenceaggregation can have a positive effect (DATE),136LocationDateRouteTimeFigure 5: ROC curves.
Red curves show the top-scored ASR hypothesis u?
with accept/reject decisionsmade using the confidence score Pasr(u); blue curves show the top belief state s?
with accept/reject decisionsmade using its belief b(s?
).but otherwise plays a small role.?
When there exists an informative prior and it isestimated correctly, prior re-ranking producesan accuracy gain (ROUTE); when estimatedpoorly, it degrades accuracy (LOCATION).?
The belief state, at least when using our currentmodels, improves accept/reject decisions onlywhen belief tracking produces a gain in accu-racy over ASR.
Absent an accuracy increase,the belief state is no more informative than agood confidence score for making accept/rejectdecisions.We believe these findings validate that statisticaltechniques ?
properly employed ?
have the capabil-ity to improve ASR robustness under real use.
Thispaper has focused on descriptive results; in futurework, we plan to test whether correcting the modeldeficiencies and re-running belief tracking does in-deed improve performance.
For now, we hope thatthis work serves as a guide to practitioners buildingstatistical dialog systems, providing some instruc-tion on the importance of accurate model building,and examples of the effects of different design deci-sions.AcknowledgmentsThanks to Barbara Hollister and the AT&T labelinglab for their excellent work on this project.137??
???
????
?????
???
???????????????????????
?????????????
??
?
???????
????????
?????????
??
????????????????????????????????????
??
???????????????
??????
????
??????
???????????
????
???
????
?????????????????
*????
???
?????????
????
???
???
???????
????????????
????????????
 ??????????
???
????? ?
??????
??? ?
????????????
??
????????
????.????????????.???????
????????????
??????
???
???????????
????????
??????????
?????
???
?0 ????
???
?????????
????
? ? *???
????
???
???????????????
? ? ?
? ??? ????????
??????????
??????
?????? ?
????
??????
?
??
????????????????
?0 ????
???
????????
??
?? *???
????
???
???????????????
? ? ?
? ??? ???????
????????
+???????????? ?
????
?????????
????
???
????????????
??+?????????
?
???
??????
??
???
 ????
????????????
???
????
??????
??? ?
????????????
???
????
?????
?3 ???????????????
?? ????
?  ??????
???
?????????????????????
??  ff?????
????
?fiflffiffi?????
?????ffffi!ffffi?????
?????!ff?????
?????"ffi#?????
????
?  $% ??????
???
???????????
?????
?????
?Figure 6: Flowchart of AT&T Let?s Go.
The system asks for the bus route, then the origin bus stop, thenthe destination bus stop.
If the user does not want the next few buses, the system also asks for the date andtime.
Prompts shown are paraphrases; actual system prompts include example responses and are tailored todialog context.
Different language models are used for each slot, and separate belief states are maintainedover each of these 5 slots.
In the analysis in this paper, results for the origin and destination slots have beencombined to form the LOCATION slot.138ReferencesH Ai, A Raux, D Bohus, M Eskenzai, and D Litman.2008.
Comparing spoken dialog corpora collectedwith recruited subjects versus real users.
In Proc SIG-dial, Columbus, Ohio, USA.AW Black, S Burger, B Langner, G Parent, and M Eske-nazi.
2010.
Spoken dialog challenge 2010.
In ProcSLT, Berkeley, CA.J Henderson and O Lemon.
2008.
Mixture modelPOMDPs for efficient handling of uncertainty in di-alogue management.
In Proc ACL-HLT, Columbus,Ohio.G Parent and M Eskenazi.
2010.
Toward better crowd-sourced transcription: Transcription of a year of thelet?s go bus information system data.
In Proc SLT,Berkeley, CA.A Raux, B Langner, D Bohus, A Black, and M Eskenazi.2005.
Let?s go public!
Taking a spoken dialog systemto the real world.
In Proc INTERSPEECH, Lisbon.B Thomson and SJ Young.
2010.
Bayesian updateof dialogue state: A POMDP framework for spokendialogue systems.
Computer Speech and Language,24:562?588.JD Williams and S Balakrishnan.
2009.
Estimating prob-ability of correctness for ASR N-best lists.
In ProcSIGdial, London, UK.JD Williams and SJ Young.
2007.
Partially observableMarkov decision processes for spoken dialog systems.Computer Speech and Language, 21(2):393?422.JD Williams, I Arizmendi, and A Conkie.
2010.
Demon-stration of AT&T ?Let?s Go?
: A production-grade sta-tistical spoken dialog system.
In Proc SLT, Berkeley,CA.JD Williams.
2007.
Using particle filters to track di-alogue state.
In Proc IEEE Workshop on AutomaticSpeech Recognition and Understanding (ASRU), Ky-oto, Japan.JD Williams, 2010a.
AT&T Statistical DialogToolkit.
http://www.research.att.com/people/Williams_Jason_D.JD Williams.
2010b.
Incremental partition recombina-tion for efficient tracking of multiple dialog states.
InProc Intl Conf on Acoustics, Speech, and Signal Pro-cessing (ICASSP), Dallas, USA.SJ Young, M Gas?ic?, S Keizer, F Mairesse, J Schatzmann,B Thomson, and K Yu.
2010.
The hidden informationstate model: a practical framework for POMDP-basedspoken dialogue management.
Computer Speech andLanguage, 24(2):150?174, April.139Appendix: Mechanism illustrationsThis appendix provides graphical illustrations ofeach of the four mechanisms that can cause the topASR hypothesis to be different from the top beliefstate hypothesis.
These examples were taken fromlogs of calls with real users, although some surfaceforms have been simplified for space.At the top of each panel is the system action taken.The user?s true response is shown in italics in theleft-most column.
The second column shows thetop 7 entries from the ASR N-best list, displayedin the order produced by the speech recognition en-gine.
The third column shows the confidence score ?the local probability of correctness assigned to eachASR N-best entry.
The last column shows the re-sulting belief state, sorted by the magnitude of thebelief.
Correct entries are shown in bold red.ASR re-ranking and prior re-ranking occur withinone turn, and confidence aggregation and N-bestsynthesis occur across two turns.
These examplesall show cases where the belief state is correct andthe ASR is incorrect; however, the opposite also oc-curs of course.ASRResultConfScoreBeliefStateseven PMseven AMten AM--------seven AMseven PMten AM--------1234567Useraction"seven AM"System : "What time are you leaving?
"Figure 7: Illustration of ASR re-ranking: The correct ASR hypothesis (?seven AM?)
is in the n = 2position, but it is assigned a higher confidence score than the misrecognized n = 1 entry ?seven PM?.TIME uses a flat prior, so the higher confidence score results in ?seven AM?
attaining the highest belief.ASRResultConfScoreBeliefState84C54C----------54C84C----------1234567Useraction"54C"System : "Say a bus route, or say I'm not sure.
"Figure 8: Illustration of Prior re-ranking: The correct ASR hypothesis (?54C?)
is in the n = 2 position,and it is assigned less confidence than the mis-recognized n = 1 entry, ?84C?.
However, the prior on 54Cis much higher than on 84C, so 54C obtains the highest belief.140ASRResultConfScoreBeliefStatetomorrow------------tomorrow------------1234567Useraction"tomorrow"ASRResultConfScoreBeliefStatejuly 8thjuly 3rdtuesdaysundayjuly 5thjuly 6th--tomorrow&july 8thjuly 3rdtuesdaysundayjuly 5thjuly 6th1234567Useraction"tomorrow"System : "Say the day you want, like today."
System : "Sorry, say the day you want, like Tuesday.
"Figure 9: Illustration of Confidence aggregation: In the first turn, ?tomorrow?
is recognized with mediumconfidence.
In the second turn, ?tomorrow?
does not appear on the N-best list; however the recognitionresult has very low confidence, so this misrecognition is unable to dislodge ?tomorrow?
from the top beliefposition.
At the end of the second update, the belief state?s top hypothesis of ?tomorrow?
is correct eventhough it didn?t appear on the second N-best list.ASRResultConfScoreBeliefStateridge avedallas avevernon avelinden avehighland avekelly ave--ridge avekelly avedallas avelinden avehighland avevernon ave--1234567Useraction"highlandave"heron aveherman avehighland ave--------highland ave'ridge avekelly aveheron avedallas aveherman avelinden aveASRResultConfScoreBeliefState1234567Useraction"highlandave"System : "Where are you leaving from?"
System : "Sorry, where are you leaving from?
"Figure 10: Illustration of N-best synthesis: In the first turn, the correct item ?highland ave?
is on theASR N-best list but not in the top position.
It appears in the belief state but not in the top position.
Inthe second turn, the correct item ?highland ave?
is again on the ASR N-best list but again not in the topposition.
However, because it appeared in the previous belief state, it obtains the highest belief after thesecond update.
Even though ?highland ave?
was mis-recognized twice in a row, the commonality across thetwo N-best lists causes it to have the highest belief after the second update.141
