Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 65?68,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsAssessing the Costs of Sampling Methods in Active Learning for AnnotationRobbie Haertel, Eric Ringger, Kevin Seppi, James Carroll, Peter McClanahanDepartment of Computer ScienceBrigham Young UniversityProvo, UT 84602, USArobbie haertel@byu.edu, ringger@cs.byu.edu, kseppi@cs.byu.edu,jlcarroll@gmail.com, petermcclanahan@gmail.comAbstractTraditional Active Learning (AL) techniquesassume that the annotation of each datum coststhe same.
This is not the case when anno-tating sequences; some sequences will takelonger than others.
We show that the AL tech-nique which performs best depends on howcost is measured.
Applying an hourly costmodel based on the results of an annotationuser study, we approximate the amount of timenecessary to annotate a given sentence.
Thismodel allows us to evaluate the effectivenessof AL sampling methods in terms of timespent in annotation.
We acheive a 77% re-duction in hours from a random baseline toachieve 96.5% tag accuracy on the Penn Tree-bank.
More significantly, we make the casefor measuring cost in assessing AL methods.1 IntroductionObtaining human annotations for linguistic data islabor intensive and typically the costliest part of theacquisition of an annotated corpus.
Hence, there isstrong motivation to reduce annotation costs, but notat the expense of quality.
Active learning (AL) canbe employed to reduce the costs of corpus annotation(Engelson and Dagan, 1996; Ringger et al, 2007;Tomanek et al, 2007).
With the assistance of AL,the role of the human oracle is either to label a da-tum or simply to correct the label from an automaticlabeler.
For the present work, we assume that cor-rection is less costly than annotation from scratch;testing this assumption is the subject of future work.In AL, the learner leverages newly provided anno-tations to select more informative sentences whichin turn can be used by the automatic labeler to pro-vide more accurate annotations in future iterations.Ideally, this process yields accurate labels with lesshuman effort.Annotation cost is project dependent.
For in-stance, annotators may be paid for the number of an-notations they produce or by the hour.
In the contextof parse tree annotation, Hwa (2004) estimates costusing the number of constituents needing labelingand Osborne & Baldridge (2004) use a measure re-lated to the number of possible parses.
With few ex-ceptions, previous work on AL has largely ignoredthe question of actual labeling time.
One excep-tion is (Ngai and Yarowsky, 2000) (discussed later)which compares the cost of manual rule writing withAL-based annotation for noun phrase chunking.
Incontrast, we focus on the performance of AL algo-rithms using different estimates of cost (includingtime) for part of speech (POS) tagging, although theresults are applicable to AL for sequential labelingin general.
We make the case for measuring cost inassessing AL methods by showing that the choice ofa cost function significantly affects the choice of ALalgorithm.2 Benefit and Cost in Active LearningEvery annotation task begins with a set of un-annotated items U .
The ordered set A ?
U con-sists of all annotated data after annotation is com-plete or after available financial resources (or time)have been exhausted.
We expand the goal of ALto produce the annotated set A?
such that the benefitgained is maximized and cost is minimized.In the case of POS tagging, tag accuracy is usu-65ally used as the measure of benefit.
Several heuristicAL methods have been investigated for determiningwhich data will provide the most information andhopefully the best accuracy.
Perhaps the best knownare Query by Committee (QBC) (Seung et al, 1992)and uncertainty sampling (or Query by Uncertainty,QBU) (Thrun and Moeller, 1992).
Unfortunately,AL algorithms such as these ignore the cost term ofthe maximization problem and thus assume a uni-form cost of annotating each item.
In this case, theordering of annotated dataAwill depend entirely onthe algorithm?s estimate of the expected benefit.However, for AL in POS tagging, the cost termmay not be uniform.
If annotators are required tochange only those automatically generated tags thatare incorrect, and depending on how annotators arepaid, the cost of tagging one sentence can dependgreatly on what is known from sentences already an-notated.
Thus, in POS tagging both the benefit (in-crease in accuracy) and cost of annotating a sentencedepend not only on properties of the sentence butalso on the order in which the items are annotated.Therefore, when evaluating the performance of anAL technique, cost should be taken into account.
Toillustrate this, consider some basic AL algorithmsevaluated using several simple cost metrics.
The re-sults are presented against a random baseline whichselects sentences at random; the learning curves rep-resent the average of five runs starting from a ran-dom initial sentence.
If annotators are paid by thesentence, Figure 1(a) presents a learning curve in-dicating that the AL policy that selects the longestsentence (LS) performs rather well.
Figure 1(a) alsoshows that given this cost model, QBU and QBC areessentially tied, with QBU enjoying a slight advan-tage.
This indicates that if annotators are paid bythe sentence, QBU is the best solution, and LS is areasonable alternative.
Next, Figure 1(b) illustratesthat the results differ substantially if annotators arepaid by the word.
In this case, using LS as an ALpolicy is worse than random selection.
Furthermore,QBC outperforms QBU.
Finally, Figure 1(c) showswhat happens if annotators are paid by the numberof word labels corrected.
Notice that in this case, therandom selector marginally outperforms the othertechniques.
This is because QBU, QBC, and LS tendto select data that require many corrections.
Con-sidered together, Figures 1(a)-Figure 1(c) show thesignificant impact of choosing a cost model on therelative performance of AL algorithms.
This leadsus to conclude that AL techniques should be eval-uated and compared with respect to a specific costfunction.While not all of these cost functions are neces-sarily used in real-life annotation, each can be re-garded as an important component of a cost modelof payment by the hour.
Since each of these func-tions depends on factors having a significant effecton the perceived performance of the various AL al-gorithms, it is important to combine them in a waythat will accurately reflect the true performance ofthe selection algorithms.In prior work, we describe such a cost model forPOS annotation on the basis of the time required forannotation (Ringger et al, 2008).
We refer to thismodel as the ?hourly cost model?.
This model iscomputed from data obtained from a user study in-volving a POS annotation task.
In the study, tim-ing information was gathered from many subjectswho annotated both sentences and individual words.This study included tests in which words were pre-labeled with a candidate labeling obtained from anautomatic tagger (with a known error rate) as wouldoccur in the context of AL.
Linear regression on thestudy data yielded a model of POS annotation cost:h = (3.795 ?
l + 5.387 ?
c + 12.57)/3600 (1)where h is the time in hours spent on the sentence, lis the number of tokens in the sentence, and c is thenumber of words in the sentence needing correction.For this model, the Relative Standard Error (RSE) is89.5, and the adjusted correlation (R2) is 0.181.
Thismodel reflects the abilities of the annotators in thestudy and may not be representative of annotators inother projects.
However, the purpose of this paper isto create a framework for accounting for cost in ALalgorithms.
In contrast to the model presented byNgai and Yarowsky (2000), which predicts mone-tary cost given time spent, this model estimates timespent from characteristics of a sentence.3 Evaluation Methodology and ResultsOur test data consists of English prose from thePOS-tagged Wall Street Journal text in the PennTreebank (PTB) version 3.
We use sections 2-21 as660.860.880.90.920.940.960  500  1000  1500  2000TagAccuracyAnnotated SentencesRandomLSQBUQBC(a)0.860.880.90.920.940.960  20000  40000  60000  80000  100000TagAccuracyAnnotated WordsRandomLSQBUQBC(b)0.860.880.90.920.940.960  2000  4000  6000  8000  10000TagAccuracyCumulative Tags CorrectedRandomLSQBUQBC(c)Figure 1: QBU, LS, QBC, and the random baseline plotted in terms of accuracy versus various cost functions: (a)number of sentences annotated; (b) number of words annotated; and (c) number of tags corrected.initially unannotated data.
We employ section 24 asthe development test set on which tag accuracy iscomputed at the end of every iteration of AL.For tagging, we employ an order two MaximumEntropyMarkovModel (MEMM).
For decoding, wefound that a beam of size five sped up the decoderwith almost no degradation in accuracy fromViterbi.The features used in this work are typical for modernMEMM POS tagging and are mostly based on workby Toutanova and Manning (2000).In our implementation, QBU employs a singleMEMM tagger.
We approximate the entropy of theper-sentence tag sequences by summing over per-word entropy and have found that this approxima-tion provides equivalent performance to the exact se-quence entropy.
We also consider another selectionalgorithm introduced in (Ringger et al, 2007) thateliminates the overhead of entropy computations al-together by estimating per-sentence uncertainty with1 ?
P (t?
), where t?
is the Viterbi (best) tag sequence.We label this scheme QBUOMM (OMM = ?OneMinus Max?
).Our implementation of QBC employs a commit-tee of three MEMM taggers to balance computa-tional cost and diversity, following Tomanek et al(2007).
Each committee member?s training set is arandom bootstrap sample of the available annotateddata, but is otherwise as described above for QBU.We follow Engelson & Dagan (1996) in the imple-mentation of vote entropy for sentence selection us-ing these models.When comparing the relative performance of ALalgorithms, learning curves can be challenging to in-terpret.
As curves proceed to the right, they can ap-proach one another so closely that it may be difficultto see the advantage of one curve over another.
Forthis reason, we introduce the ?cost reduction curve?.In such a curve, the accuracy is the independent vari-able.
We then compute the percent reduction in cost(e.g., number of words or hours) over the cost of therandom baseline for the same accuracy a:redux(a) = (costrnd(a) ?
cost(a))/costrnd(a)Consequently, the random baseline represents thetrajectory redux(a) = 0.0.
Algorithms less costlythan the baseline appear above the baseline.
For aspecific accuracy value on a learning curve, the cor-responding value of the cost on the random baselineis estimated by interpolation between neighboringpoints on the baseline.
Using hourly cost, Figure 2shows the cost reduction curves of several AL al-gorithms, including those already considered in thelearning curves of Figure 1 (except LS).
Restrictingthe discussion to the random baseline, QBC, andQBU: for low accuracies, random selection is thecheapest according to hourly cost; QBU begins tobe cost-effective at around 91%; and QBC begins tooutperform the baseline and QBU around 80%.4 Normalized MethodsOne approach to convert existing AL algorithms intocost-conscious algorithms is to normalize the resultsof the original algorithm by the estimated cost.
Itshould be somewhat obvious that many selection al-gorithms are inherently length-biased for sequencelabeling tasks.
For instance, since QBU is the sum67-0.100.10.20.30.40.50.60.70.80.86  0.88  0.9  0.92  0.94  0.96ReductioninHourlyCostTag AccuracyRandomQBUOMM/NQBC/NQBU/NQBUOMMQBCQBUFigure 2: Cost reduction curves for QBU, QBC,QBUOMM, their normalized variants, and the randombaseline on the basis of hourly costof entropy over all words, longer sentences will tendto have higher uncertainty.
The easiest solution isto normalize by sentence length, as has been donepreviously (Engelson and Dagan, 1996; Tomanek etal., 2007).
This of course assumes that annotatorsare paid by the word, which may or may not be true.Nevertheless, this approach can be justified by thehourly cost model.
Replacing the number of wordsneeding correction, c, with the product of l (the sen-tence length) and the accuracy p of the model, equa-tion 1 can be re-written as the estimate:h?
= ((3.795 + 5.387p) ?
l + 12.57)/3600Within a single iteration of AL, p is constant, so thecost is approximately proportional to the length ofthe sentence.
Figure 2 shows that normalized AL al-gorithms (suffixed with ?/N?)
generally outperformthe standard algorithms based on hourly cost (incontrast to the cost models used in Figures 1(a) -(c)).
All algorithms shown have significant costsavings over the random baseline for accuracy lev-els above 92%.
Furthermore, all algorithms exceptQBU depict trends of further increasing the advan-tage after 95%.
According to the hourly cost model,QBUOMM/N has an advantage over all other algo-rithms for accuracies over 91%, achieving a signifi-cant 77% reduction in cost at 96.5% accuracy.5 ConclusionsWe have shown that annotation cost affects the as-sessment of AL algorithms used in POS annotationand advocate the use of a cost estimate that best es-timates the true cost.
For this reason, we employedan hourly cost model to evaluate AL algorithms forPOS annotation.
We have also introduced the costreduction plot in order to assess the cost savings pro-vided by AL.
Furthermore, inspired by the notionof cost, we evaluated normalized variants of well-known AL algorithms and showed that these vari-ants out-perform the standard versions with respectto the proposed hourly cost measure.
In future workwe will build better cost-conscious AL algorithms.ReferencesS.
Engelson and I. Dagan.
1996.
Minimizing manualannotation cost in supervised training from corpora.
InProc.
of ACL, pages 319?326.R.
Hwa.
2004.
Sample selection for statistical parsing.Computational Linguistics, 30:253?276.G.
Ngai and D. Yarowsky.
2000.
Rule writing or an-notation: cost-efficient resource usage for base nounphrase chunking.
In Proc.
of ACL, pages 117?125.M.
Osborne and J. Baldridge.
2004.
Ensemble-basedactive learning for parse selection.
In Proc.
of HLT-NAACL, pages 89?96.E.
Ringger, P. McClanahan, R. Haertel, G. Busby,M.
Carmen, J. Carroll, K. Seppi, and D. Lonsdale.2007.
Active learning for part-of-speech tagging: Ac-celerating corpus annotation.
In Proc.
of LinguisticAnnotation Workshop, pages 101?108.E.
Ringger, M. Carmen, R. Haertel, K. Seppi, D. Lond-sale, P. McClanahan, J. Carroll, and N. Ellison.
2008.Assessing the costs of machine-assisted corpus anno-tation through a user study.
In Proc.
of LREC.H.
S. Seung, M. Opper, and H. Sompolinsky.
1992.Query by committee.
In Proc.
of CoLT, pages 287?294.S.
Thrun and K. Moeller.
1992.
Active exploration in dy-namic environments.
In NIPS, volume 4, pages 531?538.K.
Tomanek, J. Wermter, and U. Hahn.
2007.
An ap-proach to text corpus construction which cuts annota-tion costs and maintains reusability of annotated data.Proc.
of EMNLP-CoNLL, pages 486?495.K.
Toutanova and C. Manning.
2000.
Enriching theknowledge sources used in a maximum entropy part-of-speech tagger.
In Proc.
of EMNLP, pages 63?70.68
