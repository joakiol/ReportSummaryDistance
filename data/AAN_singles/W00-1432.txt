Sentence generation and neural networks2 .
.
.
.
.
.~ ' .
-Kathrine HammervoldUniversity of BergenSydnesplass 7N-5007 Bergen, Norway.
.
.
.
.
.
.
kat~ine.hamme~old@lhs.be .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.AbstractIn this paper we describe a neural networksapproach to generation.
The task is to generatesentences with hotel-information from astructured atabase.
The system is inspired byKaren Kukich's ANA, but expands on it byadding generality in the form of languageindependence in representations and lexicallook-up.IntroductionIn the growing field of intelligentcommunication (web-browsers, dialoguesystems, etc.)
the need for a flexible generatorhas become more important (e.g.
Hovy & Lin,1999).
NLG is usually seen as a two-stageprocess where the planning component akescare of the inter-sentential content planning,while the surface realisation componenttransforms the content representation i to astring of words.
Interactions between the twocomponents have called for the micro-planningstage to be postulated in the middle, but still therule-based pipeline architecture has problemswith sequential rules and their two-wayrelations.
Statistical approaches have beendeveloped, and seem to provide flexibility togeneration tasks.The approach taken in this thesis, however,explores generation as .a .classification taskwhereby the representation that describes theintended meaning of the utterance is ultimatelyto be classified into an appropriate surface form.Although the task as such is a complex one, theapproach allows its decomposition i to a seriesof smaller classification tasks tbrmulated asinput-output mappings rather than step-wiserules.
One of the goals of the thesis is to studythe ways generation could .be broken down intosuitable sub-classification tasks so as to enhanceflexibility in the generation process in general.Artificial neural networks are a classificationtechnique that is robust and resistant o noisyinput, and learns to classify inputs on the basisof training examples, without specific rules thatdescribe how the classification is to be done.There is not much research into using ANN's forgeneration, the main reason being long trainingtimes.
Two notable exceptions are Kukich(1987) and Ward (1997), both argue in favour ofNN's robustness, but at the same time point outproblems with scalability.
We believe that withimproved computer facilities that shorten thetraining time, this new way of looking atgeneration as a classification task constitutes aninteresting approach to generation.
We havechosen Kukich's approach, as our applicationdomain is to generate utterances from structureddatabases.This paper is structured as follows; we firstdiscuss the general model.
The second partbriefly describes neural networks.
We continuewith describing a possible implementation f themodel, and finally we draw some conclusionsand point to future challenges.1 The modelThe task chosen for the system is to generatesentences.
with information about hotels.
Theinformation is presented in a structured way in adatabase.
It is assumed that certain features andvalues are given as input to the system.. Thesystem's task is then to generate a syntactically(and semantically) well-formed sentence as aresponse to some user request, based on the239information it gets from the database.
These aresome example sentences the model will be ableto generate:The hotel Regina has twenty rooms.The hotel Regina is a small hotel.The hotel Regina has thirty single roomsThe hotel Regina is an expensive hotel.The hOtel Regina is.an expensi~e~hotel.in:lhe:city .
~.center and the single room price is 4000 BEF.A single room costs 2000 BEF.In Karen Kukich's stock-reporter system ANAthe task is divided into two parts, represented bytwo neural networks, one sememe-to-morphemenetwork and one morpheme-to-phrase network.Facts about the status of the stock market weregiven to the network as semantic attributes.
Thenetwork was trainde to map eight possiblesemantic attributes onto morphemes marked forsemantic attributes.
These morphemes were thenlinearized in the second network.The output of Kukich's first net is a set ofEnglish morphemes.
The actual morphemes arepresent on the output nodes of the net (72possible morphemes).
This makes the wholesystem both language dependent and difficult tomodify.
In order to add a morpheme to the list ofpossible morphemes the whole network must bemodified.In order to introduce more flexibili~, the taskcould be broken into a language hTdependentand a language dependent task.
The databasefacts are not dependent on which langmage theyare represented in.
Instead of letting the outputnodes stand for actual morphemes they couldrepresent he different roles the elements canplay in a sentence.
Part of the output will standfor the s,Lbject or theme of the final sentece.
Themental space theory of Fauconnier (1985)provides an attractive way qf presenting therelations between the facts without commitingoneself to a particular langTmge.
The output ofNeural Network I (NN I) will therefbre beinterpreted as an event space, or template, thatcan be used for language dependent generation.The actual values on the output nodes refer to aconcept stored in a lexicon.There is also a discourse model in the system,which receives information about which featuresin the database are relevant for the nextsentence.
It also determines the level ofgeneralisation required for the output.
Considerthe two following sentences:A double room costs 3000 BEF and a singleroom costs .5000 BEE in hotel Regina.Hotel Regina is expensive.Saying exactly how much something costs andsaying how expensive or cheap it is, is just twoways of communicating the same thing.
Bothsentences are based on the same facts.
Whichsentence is chosen may depend on the level ofspecificity required.
This problem of synonymyis present in various ways in all languagegeneration systems.
Kukich has an example oftwo phrases in her system corresponding to theexact same semantic values (or "sememes").
Toget around the problem she added two extrasememes to the input and assigned them randomvalues.
In this model we have also introduced anextra input category, but it serves as a featuretelling the network whether to output he generalor the specific sentence.In Kukich's network the task of the secondnetwork is the ordering of the morphemes into asyntactic string.
The second network in thismodel will also have to order the concepts andmap them onto forms that can represent realwords in a language.
We now also have theadvantage of having the event space to help thenetwork generate the real sentence.h7 English sentences there are phenomena suchas agreement that may span over several wordsin a sentence.
The target sentences above showagT"eement between subject and verb, in otherlanguages e.g.
Norwegian there is agreementbetween adjectives and the nouns they mod~.There have been experiments using neural netsto recognise relationships based on constituencvand syntactic structure in sentences.
Elman(1989 and 1990) has shown that neural nets canlearn to represent the concept of word,240constituency and structural relations byintroducing the concept of time into theprocessing of the representations.
Elman takesas a starting point a neural networksarchitechture first described by Jordan (1986).The novel approach is to represent time by theeffect it has on processing and not as anadditional dimension on the input.
The solutionconsists .in giving, the~sk~stem memory.
This isdone by introducing a fourth type of units, calledcontext units, into the network apart from thetypical input, hidden and output units.
At eachcycle the hidden unit activations are copied ontothe context units.
In the following cycle thecontext nodes combine with the new input toactivate the hidden units.Elman poses the problem of whether a networkcan learn underlying aspects of sentencestructure from word order.
For the simulation heused 15 different sentence templates capable ofbeing filled with nouns and verbs.
He used sixdifferent classes of nouns (human, animate, etc.
)and six different classes of verbs (transitive,intransitive tc.).
The network was trained onrandom two or three word sentences.
Thenetwork was supposed to learn to predict theorder of successive words.
For any givensequence of words there are a limited number ofpossible successors.
It is impossible to knowwith absolute certainty which word follows theprevious, but generalisations can be made basedon type of verb (e.g.
a noun should not beexpected to follow an intransitive verb).
Theperformance of the network was thereforemeasured according to the expected frequenciesof occurrence of possible successors, not whichword in reality occurred.On the basis of the training the networkdeveloped internal representations whichreflected the facts about the possible sequentialordering of the inputs.
"The network is not able to predict heprecise order of words, but recognizesthat (in this cot79us) there is a class ofinputs Ozamelv, verb.i) that .typicallyJollow other inputs (namely, nouns).This knowledge of class behavior" isquite detailed, form the fact that there isa class of items which always precedes"chase ", "bread", "smash ", it infersthat the large animals form a class.
"(Elman 1990, p. 199)He also succeeds in representing agreementbetween subject and verb, even in sentenceslike:Dog \[who chases cat\] sees girlThis method of teaching a network the relationbetween different words in a sentence could alsobe exploited for language generation.
Thenetwork can be trained on possible sentencestructures and agreement between the elementsin the sentence.
As a starting point the sentencetypes of the example sentences above could beused.
In a symbolic system they could berepresented by the following phrase structurerules, depending on the language in question:S ~NP VPNP "-~ DET (MOD) NVP ~ V NPPP --) P NPEach of the categories (N, P etc.)
will be outputnodes of  NN II according to the lhwar orderthey may occur in the language in question, andin addition there will be placeholders for"number on nouns, verbs and modifiers.
Theoutput of NN 11 is now a linear" structure wherewe Iozou, the phrase types.
This irformationcould e.g.
be used by a text-to-speech system toassig71 stress to certain word etc.Our model can now be represented like this:241Database (DB)Facts about hotels\] I Discourse Model .
.
.
I~ (Level of generalisation, \]I" ' - " " -~-" " " ' - - \ ]  ~_____.
._ .
.
._ .
- - .
-  sentence aggregation etc.)
.... JZ2NeuralNetworkl - - - ~  \ ] /  - -  /Features to concepts inevent space \] ~ , _ .
_~Language,qDependentLexiconNeural Network IlConcepts to tagged sentenceI Post ProcessorLook-up in lexiconOutput o user, text-to-speech system etc.Concepts andgramm.features towords.e.g.Hotel Regina has t3ven~' rooms2 Brief introduction to neural networks242An artificial neural network is a metaphor of theway human and other mammalian brains work.The biological brain consists of a great numberof interacting elements, neuron.
A neuroncollects electrical impulses inside the cellmembrane.
If the level of impulses reaches acertain threshold, the neuron will generate anaction potential, a pulse that travels along a thinfibre to other.neurons,.~oausing Ihem to-.store-theelectrical impulse.
Each neuron may havesynaptic connections to thousands of otherneurons.An artificial neural network consists of nodes orunits, modelled on neurons.
These nodes canreceive and transfer activation.
Each node isconnected to many other nodes, and the strengthor efficiency of each connection is determinedby a weight.
Together with the nodes theconnections are the most salient features of aneural network.
The weights on the connectionscan be modified by learning rules, enabling thenetwork to learn new tasks.
There are severaltypes of learning algorithms that may be used totrain neural networks, but back propagation isprobably the most common one.
During training,an input/output pair is presented to the network.After a whole set of input/output pairs have beenrun through the network the back propagationalgorithm calculates how far the actual output ofthe net is from the desired output, and theweights on the connections adjusted in the rightdirection.
If there is any overall pattern to thedata, or some consistent relationship between theinputs and results of each record, the networkshould be able to eventually create an internalmapping of weights that can accuratelyreproduce the expected output.
Since theknowledge the network acquires is a result of themappings, how the input and output isrepresented is of great importance.3 ImplementationThe following features are used to describe theinformation i the database:Feature Possible Value type # unitsvalues ininputService_d Hotel Binaryomain 1Name Ariadne, BinaryRabbit,5-50 #_single_rooms# doUble " 5-50roomsSingle_roDouble r"7  oom_prtceLocationI000-40002000-6000Citycenter /BusinessPark2 I2 I.___............_._.......qNumerical IvalueNumerical 1valuevalueNumerical 1valueBinary- - - - - - - - - - - - - - - t2 IThe feature selector fetches the necessary values(determined by the discourse model) and inputsthem to NN I.
The input vector is eleven unitslong.
Ten units are the local representations ofthe features in the database and the last unitrepresents the generalizer feature from thediscourse model.
The Stuttgart Neural NetworksSimulator (SNNS 2) which will be used for theimplementation ly allows values between -1and 1, so the numerical values will benormalized to fit into the vector.
This is alsonecessary so the relative importance of thedifferent features are not out of proportion.The event space in the output will consist of thetbllowing elements:(see table 1 at the end)The vocabulary needed for the generation task isrepresented by binary codes, e.g.
based on thealphabetical order of the forms.
If we let thesubject/theme part of the vector be 7 units longI At the moment we deal only with hotel info.http:l/www.informatik.unistuttgart.de/ipvr/bv/projekte/snns/snns/html243we can represent 27 (128) different word withnumerical values.
0000001 is concept number 1,00000 !0 is concept number 2 and so on.0000001List of conceptsADRIANE0000010 BEF0000011 BIG0000100 CHEAP0000101 CITY CENTER0000110 COST0000111 DOUBLE ROOM0001000 EXPENSIVE0001001 BUSINESS PARK.0001010 HAVE0001011 HOTEL000 !
100 PRICE0001101 RABBIT0001110 REASONABLE0001111 REGINA0010000 ROOM0010001 SINGLE ROOM0010010 SMALLIn table 2 are some example inputs and outputs,a 1 represents activation on an input or outputnode.Now that we have a language independentrepresentation f the sentence we would like togenerate, it needs to be cast into a sentence in anatural language.
The languages of this systemwill be English and Norwegian, but the intentionis that other languages may also be represented.These input-output combinations shown aboveshould ultimately correspond to the tbllowingtarget sentences (after NN II and postprocessing):l) The hotel Regina has twen O, single rooms.Hotell Regina har tjue enkeltrom.4) A double room costs seven thousand Belgianfrancs and a single room costs four thousandBelgian J~ancs.Et dobbeltrom koster syv tusen belgiske francog et enkeltrom koster fire tusen belgiskefranc.5) The hotel Regina is expensive.. .
.
.
.
.
.
'~:~4totetb Regina:er dyrt.
.
.
.
.
.
.6) The hotel Ariadne is a cheap hotel in the citycentre.Hotell Ariadne er et billig hotell i sentrum.The whole output vector is shown in table 3.NN II must be trained on agreement.
This isdone by teaching it to discover relationships,such as the fact that the feature SINGULAR onthe subject noun, is associated with the featureSINGULAR on the main verb.
The input nodeson Network II will be similar to the output of thefirst net, but the input will be fed sequentially tothe network (theme, number, main_event,complement e c.If we assume that the output ofNN I now servesas the input for NN lI, this will be our desiredoutput (only the activated nodes are shownhere):II : REGINA^SG^HAVE^20^SINGLE ROOMO1:REGINA^DEF^SG^HAVE^SG^20^PLUR^SINGLE ROOM^INDEF^PLURAfter post-processing: The hotel Regina hasnventy single rooms12: REG1N AASINGASMALLAHOTELASING2)3)The hotel Regina is a small hotel.Hotell Regina er et lite hotell.,4 single room costs four thousand Belgian.\]t?-ancs.Et enkeltrom koster fire tusen belgiske fi'anc.01:REG1N A^DE F^SINGABEAS1NG^SMA LL^SINGAINDEF^HOTEL^INDEFASINGAfter post-processing: The Hotel Regina is asmall hoteland so on...244After a look-up in an English dictionary we findthat the singular form of BE is is, and the pluralform of SINGLE_ROOM is single rooms.
Thereason we do not outPUt this directly is that wewould then require different output nodes for allthe different forms of a word.
Instead wecombine the word with the feature to find thecorrect morphological form.
Numbers could infact be ?
processed.by a~speci.almuml:mr~.,grammarto avoid having to list all numbers in a lexicon.These tasks could of course also be solved usingother neural networks.The nodes in the output vector representsdifferent syntactic ategories, so we also get asurface syntactic structure directly output fromthe net, which could be used for stressinformation etc.
to be input to a speechgenerator.4 ResultsThe two networks were trained usingbackpropagation with momentum (learning rate0.5), each training set consisting of 57 sentenceswas run for 600 cycles.
For the first network themean square error (MSE) was 0.08, for thesecond network 0.175.
The number of hiddennodes in each network was 20, a higher or lowernumber of hidden nodes resulted in a higherMSE.Using a threshold value of 0.8, the networkcould be said to have correctly learned themapping from output o input activations.ReferencesFauconnier, G. 1985.
Mental Spaces."
Aspects ofMeaning construction in Natural Language.Cambridge, Mass.
: MIT Press.,.-,- Eiman, ,d,,-':E~.-~'l'9.0Ov,,,Fimding.~strueture 4n.
ime:.Cognitive Science 14, 172-211.Elman, J. E. 1991.
Distributed Representations,Simple Recurrent Networks, and GrammaticalStructure.
Machine Learning 7, 195-225.
KluwerAcademic Publishers, Boston.Jordan, M L 1986.
Serial Order: A paralleldistributed processing approach (Tech.
Rep. NO.8604).
San Diego: University of California, Institutefor Cognitive Science).Hovy & Lin 1999.
Automated Text Summarization inSUMMARIST.
In Mani & Maybury (eds.)
Advancesin Automated Text Summarization.
MIT Press.Kukich, K. 1987.
Where do phrases come from: somepreliminary experiments in eonnectionist phrasegeneration.
In Natural Language Generation: Newresults in Artificial Intelligence, Psychology andLinguistics.
Gerard Kempen (editor).
MartinusNijhoff Publishers, Dordrecht.Ward, N. 1994.
A connectionist language generator.Norwood, N.J. Ablex Pub.
Corp.ConclusionExtensive experimentation is needed to definethe proper typology and parameters for theneural networks.
We especially need toexperiment more with different learningmethods.
A future research topic will be to seewhat kinds of further subdivision of the tasks areneeded.
Elman suggests that simple recurrentnetworks could be capable of learningpronominal reference, and this would be aninteresting extension of the system.245TABLE 1Event elements Possible valuesTheme RABBIT, ARIADNE, REGINA,DOUBLE ROOMSINGLE PRICE, DOUBLE PRICESINGLE_ROOM,Number of units in output vector7 + 2 units representing the featurenumber (possible values ing/plur)Main event HAVE, COST 7Complement SINGLE_ROOM, DOUBLE_ROOM, ROOM, 7 + 2 units representing the featureBEF number (possible values ing/plur)Subject__predicate EXPENSIVE, REASONABLE, CHEAP, SMALL: BI G .
.. 7+ 2 .units .rep(eseating : the, fgature.
.
.
.
.
.
.
.
.
number (possible values ing/plur)Modifier EXPENSIVE, REASONABLE, CHEAP, SMALL, BIG 7 unitsNumerical values (e.g.
20, 4000)TABLE 2INPUT TO NETWORK I#si ngle_ro Price_single_ro Price_double_ro Location Categorizer Nan le ServicedomainI1 I 1 I12 I 1 i I13 I 0 0 114 0 0 I14' I 0- 0 I15 __L___L iI6 0 1 IOH S20#double_rooms om40004000om70000 0 4000 70000 0 0 I00 00 00 00 00 00 0o iTABLE 3OUTPUT OF NETWORK IThemeOl \] 000111101!
(REGINA._ _  sine..__,.202 000111101(REGINA, def,03 \] 0010001(SINGLE ROO______~.
.~___04 I 00001100 I(DOUBLE_ROOM.
sinG.
)04" \[ 001000101(SINGLE_ROOM.
sin .
)05 \] 000111101(REGINA.sino.
)06 \[ 000000101(ARIADNE,sing.
)Main event0001010(HAVE)00001 I0 (COST)00001 I 0 (COST)0000110(COST.)Modifier200010010(SMALL)4000700040000000100(CHEAP)Complement001000110(SINGLE_ROOM.
plur.)000001010(BEF.
plur)000001010(BEF.
plur.)000001010(BEF.
plur)000101101(HOTEL.
sing.
)Subject_predicate000101101(HOTEL, sing.)000100000(EXPENSIVE,sing.
)Location0000101(CITYCENTER.sing)246
