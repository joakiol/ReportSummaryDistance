Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 171?182, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsAligning Predicates across Monolingual Comparable Textsusing Graph-based ClusteringMichael Roth and Anette FrankDepartment of Computational LinguisticsHeidelberg UniversityGermany{mroth,frank}@cl.uni-heidelberg.deAbstractGenerating coherent discourse is an importantaspect in natural language generation.
Ouraim is to learn factors that constitute coherentdiscourse from data, with a focus on how to re-alize predicate-argument structures in a modelthat exceeds the sentence level.
We presentan important subtask for this overall goal, inwhich we align predicates across compara-ble texts, admitting partial argument struc-ture correspondence.
The contribution of thiswork is two-fold: We first construct a largecorpus resource of comparable texts, includ-ing an evaluation set with manual predicatealignments.
Secondly, we present a novel ap-proach for aligning predicates across compa-rable texts using graph-based clustering withMincuts.
Our method significantly outper-forms other alignment techniques when ap-plied to this novel alignment task, by a marginof at least 6.5 percentage points in F1-score.1 IntroductionDiscourse coherence is an important aspect in natu-ral language generation (NLG) applications.
A num-ber of theories have investigated coherence inducingfactors.
A prominent example is Centering Theory(Grosz et al1995), which models local coherenceby relating the choice of referring expressions to theimportance of an entity at a certain stage of a dis-course.
A data-driven model based on this theoryis the entity-based approach by Barzilay and Lap-ata (2008), which models coherence phenomena byobserving sentence-to-sentence transitions of entityoccurrences.Barzilay and Lapata show that their approach candiscriminate between a coherent and a non-coherentset of ordered sentences.
However, their model isnot able to generate alternative entity realizations byitself.
Furthermore, the entity-based approach onlyinvestigates realization patterns for individual enti-ties in discourse in terms of core grammatical func-tions.
It does not investigate the interplay betweenentity transitions and realization patterns for full-fledged semantic structures.
This interplay, how-ever, is an important factor for a semantics-based,generative model of discourse coherence.The main hypothesis of our work is that we canautomatically learn context-specific realization pat-terns for predicate argument structures (PAS) from asemantically parsed corpus of comparable text pairs.Our assumption builds on the success of previousresearch, where comparable and parallel texts havebeen exploited for a range of related learning tasks,e.g., unsupervised discourse segmentation (Barzilayand Lee, 2004) and bootstrapping semantic analyz-ers (Titov and Kozhevnikov, 2010).For our purposes, we are interested in finding cor-responding PAS across comparable texts that areknown to talk about the same events, and hence in-volve the same set of underlying event participants.By aligning predicates in such texts, we can inves-tigate the factors that determine discourse coher-ence in the realization patterns for the involved argu-ments.
These include the specific forms of argumentrealization, as a pronoun or a specific type of refer-ential expression, as studied in prior work in NLG(Belz et al2009, inter alia).
The specific set-upwe examine, however, allows us to further investi-171gate the factors that govern the non-realization ofan argument position, as a special form of coher-ence inducing element in discourse.
Example (1),extracted from our corpus of aligned texts,illustratesthis point: Both texts report on the same event oflocating victims in an avalanche.
While (1.a) explic-itly talks about the location of this event, the role re-mains implicit in the second sentence of (1.b), giventhat it can be recovered from the preceding sentence.In fact, realization of this argument role would im-pede the fluency of discourse by being overly repet-itive.
(1) a. .
.
.
The official said that [no bodies]Arg1 hadbeen recovered [from the avalanches]Arg2 whichoccurred late Friday in the Central Asian coun-try near the Afghan border some 300 kilometers(185 miles) southeast of the capital Dushanbe.b.
Three other victims were trapped in anavalanche in the village of Khichikh.
[Noneof the victims bodies]Arg1 have been found[ ]Argm-loc.This phenomenon clearly relates to the problemof discourse-linking of implicit roles, a very chal-lenging task in discourse processing.1 In our work,we consider this problem from a content-based gen-eration perspective, concentrating on the discoursefactors that allow for the omission of a role.Thus, our aim is to identify comparable predica-tions across aligned texts, and to study the discoursecoherence factors that determine the realization pat-terns of arguments in the respective discourses.
Thiscan be achieved by considering the full set of argu-ments that can be recovered from the aligned pred-ications.
This paper focuses on the first of thesetasks, henceforth called predicate alignment.2In line with data-driven approaches in NLP, weautomatically align predicates in a suitable corpus ofpaired texts.
The induced alignments will (i) serve toidentify events described in both comparable texts,and (ii) provide information about the underlying ar-gument structures and how they are realized in eachcontext to establish a coherent discourse.
We in-vestigate a graph-based clustering method for induc-1See the recent SemEval 2010 task: Linking Events andtheir Participants in Discourse, (Ruppenhofer et al2010).2Note that we provide details regarding the construction ofa suitable data set and further examples involving non-realizedarguments in a complementary paper (Roth and Frank, 2012).ing such alignments as clustering provides a suitableframework to implicitly relate alignment decisionsto one another, by exploiting global information en-coded in a graph.The remainder of this paper is structured as fol-lows: In Section 2, we discuss previous work in re-lated tasks.
Section 3 describes our task and a suit-able data set.
Section 4 introduces a graph-basedclustering model using Mincuts for the alignment ofpredicates.
Section 5 outlines the experiments andpresents evaluation results.
Finally, we conclude inSection 6 and discuss future work.2 Related WorkThe task of aligning words in general has been stud-ied extensively in previous work, for example as partof research in statistical machine translation (SMT).Typically, alignment models in SMT are trained byobserving and (re-)estimating co-occurrence countsof word pairs in parallel sentences (Brown et al1993).
The same methods have also been appliedin monolingual settings, for example to align wordsin paraphrases (Cohn et al2008).
In contrast totraditional word alignment tasks, our focus is not onpairs of isolated sentences but on aligning predicateswithin the discourse contexts in which they are sit-uated.
Furthermore, text pairs for our task shouldnot be strictly parallel as we are specifically inter-ested in the impact of different discourse contexts.In Section 5, we will show that this particular set-ting indeed constitutes a more challenging task com-pared to traditional word alignment in parallel orparaphrasing sentences.Another set of related tasks is found in the area oftextual inference.
Since 2006, there have been reg-ular challenges on the task of Recognizing TextualEntailment (RTE).
In the original task description,Dagan et al2006) define textual entailment ?as adirectional relationship between pairs of text expres-sions, denoted by T - the entailing ?Text?
-, and H- the entailed ?Hypothesis?.
(.
.
. )
T entails H if themeaning of H can be inferred from the meaning ofT, as would typically be interpreted by people.?
Al-though this relation does not necessarily require thepresence of corresponding predicates, previous workby MacCartney et al2008) shows that word align-ments can serve as a good indicator of entailment.172As a matter of fact, the same holds true for the taskof detecting paraphrases.
In contrast to RTE, this lat-ter task requires bi-directional entailments, i.e., eachof the two phrases must entail the other.
Wan et al(2006) show that a simple approach solely based onword (and lemmatized n-gram) overlap can alreadyachieve an F1-score of up to 83% for detecting para-phrases in the Microsoft Research Paraphrase Cor-pus (Dolan and Brockett, 2005, MSRPC).
In fact,this is just 0.6% points below the state-of-the-art re-sults recently reported by Socher et al2011).The MSRPC and data sets from the first RTEchallenges only consisted of isolated pairs of sen-tences.
The Fifth PASCAL Recognizing Textual En-tailment Challenge (Bentivogli et al2009) intro-duced a ?Search Task?, where entailing sentencesfor a hypothesis have to be found in a set of fulldocuments.
This new task first opened the doors forassessing the role of discourse (Mirkin et al2010a;Mirkin et al2010b) in RTE.
However, this setting isstill limited as discourse contexts are only providedfor the entailing part (T ) of each text pair but not forthe hypothesis H .A further task related to ours is the detectionof event coreference.
The goal of this task is toidentify all mentions of the same event within adocument and, in some settings, also across docu-ments.
However, the task setting is typically morerestricted than ours in that its focus lies on iden-tical events/references (cf.
Walker et al2006),Weischedel et al2011), inter alia).
In particular,verbalizations of different aspects of an event (e.g.,?buy??
?sell?, ?kill??
?die?, ?recover???find?)
are gen-erally not linked in this paradigm.
In contrast to co-reference methods that identify chains of events, weare interested in pairs of corresponding predicates(and their argument structure), for which we can ob-serve alternative realizations in discourse.3 Aligning Predicates Across TextsThis section summarizes how we built a large cor-pus of comparable texts, as a basis for the predicatealignment task.
We motivate the choice of the cor-pus and present a strategy for extracting comparabletext pairs.
Subsequently, we report on the prepara-tion of an evaluation data set with manual predicatealignments across the paired texts.
We conclude thissection with an example that showcases the poten-tial of using aligned predicates for the study of co-herence phenomena.
More detailed information re-garding corpus creation, annotation guidelines andadditional examples illustrating the potential of thiscorpus can be found in Roth and Frank (2012).3.1 Corpus CreationThe goal of our work is to investigate coherence fac-tors for argument structure realization, using com-parable texts that describe the same events, but thatinclude variation in textual presentation.
This re-quirement fits well with the news domain, for whichwe can trace varying textual sources that describethe same underlying events.
The English GigawordFifth Edition (Parker et al2011) corpus (henceforthjust Gigaword) is one of the largest corpus collec-tions for English.
It comprises a total of 9.8 millionnewswire articles from seven distinct sources.In previous work (Roth and Frank, 2012), we in-troduced GigaPairs, a sub-corpus extracted from Gi-gaword that includes over 160,000 pairs of newswirearticles from distinct sources.
GigaPairs has beenderived from Gigaword using the pairwise similar-ity method on headlines presented by Wubben et al(2009).
In addition to calculating the similarity ofnews titles, we impose an additional date constraintto further increase the precision of extracted pairs oftexts.
Random inspection of about 100 documentsrevealed only two texts describing different events.Overall, we extracted 167,728 document pairs con-taining a total of 50 million word tokens.
Each doc-ument in this corpus consists of up to 7.564 wordswith a mean and median of 301 and 213 words, re-spectively.
All texts have been pre-processed us-ing MATE tools (Bjo?rkelund et al2010; Bohnet,2010), a pipeline of NLP modules including a state-of-the-art semantic role labeler that computes Prop-Bank/NomBank annotations (Palmer et al2005;Meyers et al2008).3.2 Gold Standard AnnotationWe selected 70 text pairs from the GigaPairs cor-pus for manual predicate alignment.
All documentpairs were randomly chosen with the constraint thateach text consists of 100 to 300 words.3 Predi-3This constraint is satisfied by 75.3% of all documents inGigaPairs.173cates identified by the semantic parser are providedas pre-labeled annotations for alignment.
We askedtwo students4 to tag corresponding predicates acrosseach text pair.
Following standard practice in wordalignment tasks (cf.
Cohn et al2008)) the annota-tors were instructed to distinguish between sure andpossible alignments, depending on how certainly, intheir opinion, two predicates describe verbalizationsof the same event.
The following examples showpredicate pairings marked as sure (2) and as possi-ble alignments (3).
(2) a.
The regulator ruled on September 27 that Nas-daq too was qualified to bid for OMX [.
.
.
]b.
The authority [.
.
. ]
had already approved a sim-ilar application by Nasdaq.
(3) a. Myanmar?s military government said earlier thisyear it has released some 220 political prisoners[.
.
.
]b.
The government has been regularly releasingmembers of Suu Kyi?s National League forDemocracy party [.
.
.
]In total, the annotators (A/B) aligned 487/451 sureand 221/180 possible alignments with a Kappa score(Cohen, 1960) of 0.86.5 For the construction of agold standard, we merged the alignments from bothannotators by taking the union of all possible align-ments and the intersection of all sure alignments.Cases which involved a sure alignment on which theannotators disagreed were resolved in a group dis-cussion with the first author.We split the final corpus into a development setof 10 document pairs and a test set of 60 documentpairs.
The test set contains a total of 3,453 predicates(1,531 nouns and 1,922 verbs).
Its gold standard an-notation consists of 446 sure and 361 possible align-ments, which corresponds to an average of 7.4 sure(6.0 possible) alignments per document pair.
Mostof the gold alignments (82.4%) are between predi-cates of the same part-of-speech (242 noun and 423verb pairs).
A total of 383 gold alignments (47.5%)have been annotated between predicates with iden-tical lemma form.
Diverging numbers of realizedarguments can be observed in 320 pairs (39.7%).4Both annotators are students in computational linguistics,one undergraduate (A) and one postgraduate (B) student.5Following Brockett (2007), we computed agreement on la-beled annotations, including unaligned predicate pairs as an ad-ditional null category.3.3 Potential for Discourse CoherenceThis section presents an example of an alignedpredicate pair from our development set that il-lustrates the potential of aggregating correspondingPAS across comparable texts.
The example repre-sents one of eleven cases involving unrealized argu-ments that can be found in our development set ofonly ten document pairs.
(4) a.
The Chadians said theyArg0 had fled in fear oftheir lives.b.
The United Nations says some 20,000refugeesArg0 have fled into CameroonArg1.In both sentences, the Arg0 role of the predicate fleeis filled, but Arg1 (here: the goal) has not been real-ized in (4.a).
However, sentence (4.a) is still part of acoherent discourse, as a role filler for the omitted ar-gument can be inferred from the preceding context.For the goal of our work, we are interested in factorsthat license such omissions of an argument.
Poten-tial factors on the discourse level include the infor-mation status of the entity filling an argument posi-tion, and its salience at the corresponding point indiscourse.
Roth and Frank (2012) discuss additionalexamples that demonstrate the importance of fac-tors on further linguistic levels, e.g., lexical choiceof predicates and their syntactic realization.In the example above, the aggregation of alignedPAS presents an effective means to identify appro-priate fillers for unrealized roles.
Hence, we can uti-lize each such pair as one positive and one negativetraining instance for a model of discourse coherencethat controls the omissibility of arguments.
In whatfollows, we introduce an alignment approach thatcan be used to automatically acquire more trainingdata using the entire GigaPairs corpus.4 ModelFor the automatic induction of predicate alignmentsacross texts, we opt for an unsupervised graph-basedclustering method.
In this section, we first define agraph representation for pairs of documents.
In par-ticular, predicates are represented as nodes in such agraph and similarities between predicates as edges.We then proceed to describe various similarity mea-sures that can be used to identify similar predicateinstances.
Finally, we introduce the clustering algo-rithm that we apply to graphs (representing pairs of174documents) in order to induce alignments betweencorresponding predicates.4.1 Graph representationWe build a bipartite graph representation for eachpair of texts, using as vertices the predicate argu-ment structures assigned in pre-processing (cf.
Sec-tion 3.1).
We represent each predicate as a node andintegrate information about arguments only implic-itly.
Given the sets of predicates P1 and P2 of twocomparable texts T1 and T2, respectively, we for-mally define an undirected graph GP1,P2 as follows:GP1,P2 = ?V,E?
whereV = P1 ?
P2E = P1 ?
P2(1)Edge weights.
We specify the edge weight be-tween two nodes representing predicates p1 ?
P1and p2 ?
P2 as a weighted linear combination offour similarity measures described in the next sec-tion: WordNet and VerbNet similarity, Distributionalsimilarity and Argument similarity.wp1p2 = ?1 ?
simWN(p1, p2)+ ?2 ?
simVN(p1, p2)+ ?3 ?
simDist(p1, p2)+ ?4 ?
simArg(p1, p2)(2)Initially we set aleighting parameters ?1 .
.
.
?4 tohave uniform weights by default.
In Section 5, wedefine an optimized weighting setting for the indi-vidual similarity measures.4.2 Similarity MeasuresWe employ a number of similarity measuresthat make use of complementary informationthat is type-based (simWN/VN/Dist) or token-based(simArg).6 Given two lemmatized predicates p1, p2and their set of arguments A1 = args(p1), A2 =args(p2), we define the following measures.WordNet similarity.
Given all pairs of synsets s1,s2 that contain the predicates p1, p2, respectively,we compute the maximal similarity using the infor-mation theoretic measure described in Lin (1998).Our implementation exploits the WordNet hierarchy6All token-based frequency counts (i.e., freq() and idf())are computed over all documents from the AFP and APW partsof the English Gigaword Fifth Edition.
(Fellbaum, 1998) to find the synset of the least com-mon subsumer (lcs) and uses the pre-computed In-formation Content (IC) files from Pedersen et al(2004) to compute Lin?s measure:simWN(p1, p2) =IC(lcs(s1, s2))IC(s1) ?
IC(s2)(3)In order to compute similarities between verbal andnominal predicates, we further use derivation infor-mation from NomBank (Meyers et al2008): if anoun represents a nominalization of a verbal pred-icate, we resort to the corresponding verb synset.If no relation can be found between two predicates,we set a default value of simWN = 0.
This appliesin particular to all cases that involve a predicate notpresent in WordNet.VerbNet similarity.
To overcome systematicproblems with the WordNet verb hierarchy (cf.Richens (2008)), we further compute similaritybetween verbal predicates using VerbNet (Kipperet al2008).
Verbs in VerbNet are categorized intosemantic classes according to their syntactic behav-ior.
A class C can recursively embed sub-classesCs ?
sub(C) that represent finer semantic andsyntactic distinctions.
We define a simple similarityfunction that defines fixed similarity scores between0 and 1 for pairs of predicates p1, p2 depending ontheir relatedness within the VerbNet class hierarchy:simVN(p1, p2) =????????
?1.0 if ?C : p1, p2 ?
C0.8 if ?C,Cs : Cs ?
sub(C)?
p1, p2 ?
C ?
Cs0.0 else(4)Distributional similarity.
As some predicatesmay not be covered by the WordNet and VerbNet hi-erarchies, we additionally calculate similarity basedon distributional meaning in a semantic space (Lan-dauer and Dumais, 1997).
Following the traditionalbag-of-words approach that has been applied in re-lated tasks (Guo and Diab, 2011; Mitchell and La-pata, 2010), we consider the 2,000 most frequentcontext words c1, .
.
.
, c2000 ?
C as dimensions ofa vector space and define predicates as vectors usingtheir Pointwise Mutual Information (PMI):~p = (PMI(p, c1), .
.
.
,PMI(p, c2000) (5)175with PMI(x, y) =freq(x, y)freq(x) ?
freq(y)Given the vector representations of two predicates,we calculate their similarity as the cosine of the an-gle between the two vectors:simDist(p1, p2) =~p1 ?
~p2|~p1| ?
|~p2|(6)Argument similarity.
While the previous similar-ity measures are purely type-based, argument simi-larity integrates token-based, i.e., discourse-specific,similarity information about predications by takinginto account the similarity of their arguments.
Thismeasure calculates the association between the ar-guments A1 of the first and the arguments A2 of thesecond predicate by determining the ratio of over-lapping words in both argument sets.simArg(p1, p2) =?w?A1?A2 idf(w)?w?A1 idf(w) +?w?A2 idf(w)(7)In order to give higher weight to (rare) contentwords, we weight each word by its Inverse Docu-ment Frequency (IDF), which we calculate over alldocuments d from the AFP and APW sections of theGigaword corpus:idf(w) = log|D||{d : w ?
D|}(8)Normalization.
In order to make the outputs of allsimilarity measures comparable, we normalize theirvalue ranges on the development set to have a meanand standard deviation of 1.0.4.3 Mincut-based ClusteringOur graph clustering method uses minimum cuts (orMincut) in order to partition the bipartite text graphinto clusters of aligned predicates.
A Mincut op-eration divides a given graph into two disjoint sub-graphs.
Each minimum cut is performed as a cutbetween some source node s and some target nodet, such that (i) each of the two nodes will be in adifferent sub-graph and (ii) the sum of weights of allremoved edges will be as small as possible.
Our sys-tem determines each Mincut using an implementa-tion of the method by Goldberg and Tarjan (1986).77Basic graph operations are performed using the freelyavailable Java library JGraph, cf.
http://jgrapht.org/.function CLUSTER(G)clusters?
?E ?
GETEDGES(G) .
Step 1e?
GETEDGEWITHLOWESTWEIGHT(E)s?
GETSOURCENODE(e)t?
GETTARGETNODE(e)G?
?
MINCUT(G, s, t) .
Step 2C ?
GETCONNECTEDCOMPONENTS(G?
)for all Gs ?
C do .
Step 3if SIZE(Gs) <= 2 thenclusters?
clusters ?Gselseclusters?
clusters ?
CLUSTER(Gs)end ifend forreturn clusters;end functionFigure 2: Pseudo code of our clustering algorithmAs our goal is to induce clusters that correspond topairs of similar predicates, we set a maximum num-ber of two nodes per cluster as stopping criterion.Given an input graph G, our algorithm recursivelyapplies Mincuts in three steps as described in Figure2.
Step 1 identifies the edge e with lowest weight inthe given graph G. Step 2 performs the actual Min-cut operation on G. Finally, the stopping criterionand recursion are applied in Step 3.
An example ofa clustered graph is illustrated in Figure 1.The advantage of our method compared to off-the-shelf clustering techniques is two-fold: On theone hand, the clustering algorithm is free of any pa-rameters, such as the number of clusters or a clus-tering threshold, that require fine-tuning.
On theother hand, the approach makes use of a termina-tion criterion that very well represents the nature ofthe goal of our task, namely to align pairs of predi-cates across comparable texts.
The next section pro-vides empirical evidence for the advantage of thisapproach.5 ExperimentsThis section evaluates our graph-clustering modelon the task of aligning predicates across compara-ble texts.
For comparison to related tasks and meth-ods, we describe different evaluation settings, vari-176Figure 1: The predicates of two sentences (white: ?The company has said it plans to restate its earnings for 2000through 2002.?
; grey: ?The company had announced in January that it would have to restate earnings (.
.
.
)?)
from theMicrosoft Research Paragraph Corpus are aligned by computing clusters with minimum cuts.ous baselines, as well as results for these baselinesand the model presented above.5.1 SettingsIn order to benchmark our model against tradi-tional methods for word alignment, we first applyour graph-based alignment model (Full) on threesentence-based paraphrase corpora.
This model usesthe similarity measures defined in Section 4.2 andthe clustering algorithm introduced in Section 4.3.In a second experiment, we evaluate Full on ournovel task of inducing predicate alignments acrosscomparable monolingual texts, using the GigaPairsdata set described in Section 3.
We evaluate againstthe manually annotated gold alignments in the testdata set described in Section 3.2.
To gain more in-sight into the performance of the various similar-ity measures included in the Full model, we eval-uate simplified versions that omit individual similar-ity measures (Full?
[measure name]).The relative differences in performance againstvarious baselines will help us quantify the differ-ences and difficulties between a traditional sentence-based word alignment setting and our novel align-ment task that operates on full texts.5.1.1 Sentence-level Alignment SettingFor sentence-based predicate alignment we makeuse of the following three corpora that are word-aligned subsets of the paraphrase collections de-scribed in (Cohn et al2008): MTC consists of 100sentence pairs from the Multiple-Translation Chi-nese Corpus (Huang et al2002), Leagues contains100 sentential paraphrases from two translations ofJules Verne?s ?Twenty Thousand Leagues Underthe Sea?, and MSR is a sub-set of the MicrosoftResearch Paraphrase Corpus (Dolan and Brockett,2005), consisting of 130 sentence pairs.
All threeparaphrase collections are in English.Results for these experiments are reported in Sec-tion 5.3.1.
Note that in order to determine alignmentcandidates, we apply the same pre-processing stepsas used for the annotation of our corpus.
The se-mantic parser identified an average number of 3.8,5.1 and 4.7 predicates per text (i.e., per paraphrasesentence) in MTC, Leagues and MSR, respectively.All models are evaluated against the subset of goldstandard alignments (cf.
Cohn et al2008)) betweenpairs of words marked as predicates.5.1.2 Text-level Alignment SettingResults for our own data set, GigaPairs, are reportedin Section 5.3.2.
In this setting, models are evaluatedagainst the annotated gold standard alignments be-tween predicates as described in Section 3.2.
Sinceall text pairs in GigaPairs comprise multiple sen-tences each, the average number of predicates pertext to consider (27.5) is much higher than in theparaphrase settings.
As the full graph representa-tion becomes rather inefficient to handle (by default,edges are inserted between all predicate pairs), weuse the development set of 10 text pairs to estimate177MTC Leagues MSRPrecision Recall F1 Precision Recall F1 Precision Recall F1LemmaId 25.1** 74.9 37.6** 31.5** 67.2 42.9** 42.3** 90.8 57.7**Greedy 74.8** 88.3** 81.0 75.0** 86.0** 80.1 80.7** 97.0** 88.1WordAlign 99.3 86.6 92.5 98.7 78.5 87.4 99.5 96.0* 97.7*Full 92.3 72.2 81.1 92.7 69.4 79.4 94.5 88.3 91.3Table 1: Results for sentence-based predicte alignment in the three benchmark settings MTC, Leagues and MSR (allnumbers in %); results that significantly differ from Full are marked with asterisks (* p<0.05; ** p<0.01).a threshold on predicate similarity for adding edges.We tested all thresholds from 1.5 to 4.0 with a step-size of 0.25 and found 2.5 to perform best.
Thisthreshold is applied in the evaluation of all graph-based models.5.2 BaselinesA simple baseline for both settings is to align allpredicates whose lemmas are identical.
This base-line, henceforth called LemmaId, is computed as alower bound for all settings.
In order to assess thebenefits of the clustering step, we propose a secondbaseline that uses the same similarity measures andthresholds as our Full model, but omits the cluster-ing step described in Section 4.3.
Instead, it greed-ily computes as many 1-to-1 alignments as possible,starting from the highest similarity to the learnedthreshold (Greedy).As a more sophisticated baseline, we makeuse of alignment tools commonly used in sta-tistical machine translation (SMT).
For the threesentence-based paraphrase settings MTC, Leaguesand MSR, Cohn et al2008) readily provideGIZA++ (Och and Ney, 2003) alignments as partof their word-aligned paraphrase corpus.
For theexperiments in the GigaPairs setting, we train ourown word alignment model using the state-of-the-art word alignment tool Berkeley Aligner (Liang etal., 2006).
As word alignment tools require pairs ofsentences as input, we first extract paraphrases in thelatter setting using a re-implementation of the para-phrase detection system by Wan et al2006).8 Inthe following section, we abbreviate both baselinesusing SMT alignment tools as WordAlign.8Note that the performance of this system lies slightly be-low the state-of-the-art results reported by Socher et al2011)However, we were not able to reproduce the results of Socher etal.
using the publicly available release of their software.5.3 ResultsWe measure precision as the number of predictedalignments that are annotated in the gold standarddivided by the total number of predictions.
Recallis measured as the number of correctly predictedsure alignments divided by the total number of surealignments in the gold standard.
This conforms toevaluation measures used for word alignment mod-els in SMT (Och and Ney, 2003).
Following Cohnet al2008), we subsequently compute the F1-scoreas the harmonic mean between precision and recall.We compute statistical significance of result dif-ferences with a paired t-test (Cohen, 1995) over theaffected test set documents and provide correspond-ing significance levels where appropriate.5.3.1 Sentence-level Predicate AlignmentThe results for MTC, Leagues and MSR are pre-sented in Table 1.
The numbers indicate thatWordAlign consistently outperforms all other mod-els on the three data sets in terms of F1-score.
Sta-tistical significance of result differences betweenWordAlign and Full can only be observed for recalland F1-score on the MSR data set (p<0.05).
Otherdifferences are not significant due to high varianceof results compared to data set sizes.The overall performance of WordAlign does notcome much as a surprise, seeing that all three datasets consist of highly parallel sentence pairs.
Infact, the results for LemmaId show that by align-ing all predicates with identical lemmas, most of thesure alignments in the three settings are already cov-ered.
The reason for the low precision lies in thefact that the same lemma can occur multiple timesin the same paraphrase, a phenomenon that is bet-ter handled by WordAlign, Greedy and Full.
In-terestingly, the Greedy model achieves the highestrecall in all settings but it performs below our Full178model in terms of precision and F1-score.
The per-formance differences between Greedy and Full arestatistically significant (p<0.01) regarding precisionand recall.5.3.2 Text-level Predicate AlignmentWe now turn to the experiments on our own dataset, GigaPairs, which comprises full documentsof unequal lengths instead of pairs of single sen-tences.
Table 2 presents the results for our full modeland the three baselines.
From all four approaches,WordAlign yields lowest performance.
We observetwo main reasons for this: On the one hand, sen-tence paraphrase detection does not perform per-fectly.
Hence, the extracted sentence pairs do notalways contain gold alignments.
On the other hand,even sentence pairs that contain gold alignments aregenerally less parallel than in the previous settings,which make them harder to align.
The increased dif-ficulty can also be seen in the results for the Greedybaseline, which only achieves an F1-score of 20.1%in this setting.
In contrast, we observe that the ma-jority of all sure alignments (60.3%) can be retrievedby applying the LemmaId model.The Full model achieves a recall of 46.6%, butit significantly outperforms LemmaId (p<0.01) interms of precision (58.7%, +18.4 percentage points).This is an important factor for us, as we plan to usethe alignments in subsequent tasks.
With 52.0%,Full achieves the best overall F1-score.Ablating similarity measures.
All aforemen-tioned results were conducted in experiments witha uniform weighting scheme of similarity measuresas introduced in Section 4.3.
Table 3 shows the per-formance impact of individual similarity measuresby removing them completely (i.e., setting theirweight to 0.0).
The numbers indicate that not allmeasures contribute positively to the overall perfor-mance when using equal weights.
However, a signif-icant difference can only be observed when remov-ing the argument similarity measure, which drasti-cally reduces the results.
This clearly highlights theimportance of incorporating the context of individ-ual predications in this task.Tuning weights.
Subsequently, we tested variouscombinations of weights on our development set inorder to estimate a good overall weighting scheme.Precision Recall F1LemmaId 40.3** 60.3** 48.3Greedy 19.6** 20.6** 20.1**WordAlign 19.7** 15.2** 17.2**Full 58.7 46.6 52.0Table 2: Results for GigaPairs (all numbers in %); re-sults that significantly differ from Full are marked withasterisks (* p<0.05; ** p<0.01).Precision Recall F1Full?WN 58.9 48.0 52.9Full?VN 57.3 48.7 52.6Full?Dist 54.3 42.8 47.9Full?Args 40.1** 24.0** 30.0**Full 58.7 46.6 52.0Full+tuned 59.7** 50.7** 54.8**Table 3: Impact of removing individual measures and us-ing a tuned weighting scheme (all numbers in %); resultsthat significantly differ from Full are marked with aster-isks (* p<0.05; ** p<0.01).This tuning procedure is implemented as a brute-force technique, in which we fix the weight of onesimilarity measure and allow all other measures toreceive a weight assignment between 0.25 to 5.0times the fixed weight.
Finally, the resulting weightsare normalized to sum to 1.0.
We found the best per-forming weighting scheme to be 0.09, 0.48, 0.24 and0.19 for ?1, .
.
.
, ?4, respectively (cf.
Eq.
(2), Section4).
The performance gains of the resulting model(Full+tuned) can be seen in Table 3.
Comput-ing statistical significance of the result differencesbetween Full+tuned and all baseline models con-firmed significant improvements (p<0.01) for bothprecision and F1-score.5.4 Error AnalysisWe perform an error analysis on the output ofFull+tuned on the development set of GigaPairsin order to determine re-occurring problems.
In to-tal, the model missed 13 out of 35 sure alignments(Type I errors) and predicted 23 alignments not an-notated in the gold standard (Type II errors).Six Type I errors (46%) occurred when the lemmaof an affected predicate occurred more than once in atext and the model missed a correct link.
Vice versa,identical predicates that refer to different events have179been the source of 8 Type II errors (35%).
We ob-serve that these errors are frequently related to pred-icates, such as ?say?
and ?appear?, that often occurin news texts.
Altogether, we find 15 Type II errors(65%) that are due to high predicate similarity de-spite low argument overlap (cf.
Example (5)).
(5) a.
The US alert (.
.
. )
followed intelligence reportsthat .
.
.b.
The Foreign Ministry announcement called onJapanese citizens to be cautious .
.
.We observe that argument overlap itself can be loweven for correct alignments.
This clearly indicatesthat a better integration of context is needed.
Ex-ample (6.a) illustrates a case in which the agent ofa warning event is not realized.
Here, contextual in-formation is required to correctly align it to the firstwarning event in (6.b).
This involves inference be-yond the local PAS.
(6) a.
The US alert (.
.
. )
is one step down from a full[travel]Arg1 warning [ ]Arg0.b.
Japan has issued a travel alert .
.
.
(which)follows similar warnings [from Ameri-can and British authorities]Arg0.
(.
.
. )
An offi-cial said it was highly unusual for [Tokyo]Arg0to issue such a warning .
.
.6 ConclusionWe presented a novel task for predicate alignmentacross comparable monolingual texts, which we ad-dress using graph-based clustering with Mincuts.The motivation for this task is to acquire empiricaldata for studying discourse coherence factors relatedto argument structure realization.As a first step, we constructed a data set of com-parable texts that provide full discourse contextsfor alternative verbalizations of the same underlyingevents.
The data set is derived from all newswirepairs found in the English Gigaword Fifth Editionand contains a total of more than 160,000 paireddocuments.A subset of these pairs forms an evaluation set,annotated with gold alignments that relate predica-tions, which exhibit a (possibly partial) correspond-ing argument structure.
We established that the an-notation task, while difficult, can be performed withgood inter-annotator agreement (?
at 0.86).Our main contribution is a novel clustering ap-proach using Mincuts for aligning predicationsacross comparable texts.
Our experiments estab-lished that recursive clustering improves on greedyselection methods by profiting from global infor-mation encoded in the graph representation.
Whilethe Mincut-based method is in itself unsupervised, asmall amount of development data is needed to tuneparameters for the construction of particularly suit-able input graphs.We tested our full model against two additionalbaselines: simple heuristic alignment based on iden-tical lemma forms and a combination of techniquesfrom SMT and paraphrase detection.
The evalua-tion for our novel task was complemented by a tra-ditional word alignment task using established para-phrase data sets.
We determined clear differences inperformance for all models for the two types of tasksettings.
While word alignment methods from SMToutperform the competing models in the sentence-based alignment tasks, they perform poorly in thediscourse setting.In future work, we will enhance our model byincorporating more refined similarity measures in-cluding discourse-based criteria.
We will further ex-plore tuning techniques, e.g., a more suitable pre-selection method for edges in graph construction, inorder to increase either precision or recall.
The deci-sion of optimizing towards one measure or anotheris clearly task-dependent.
In our case, high preci-sion is favorable as we plan to learn accurate dis-course model parameters from the computed align-ments.
Even though such an optimization will resultin an overall lower recall, application of the align-ment model on the entire GigaPairs corpus can stillprovide us with a large amount of precise predicatealignments.
Using this set of alignments, we willthen proceed to exploit contextual information in or-der to learn a semantic model for discourse coher-ence in argument structure realization.AcknowledgementsWe are grateful to the Landesgraduiertenfo?rderungBaden-Wu?rttemberg for funding within the researchinitiative ?Coherence in language processing?
atHeidelberg University.
We thank Danny Rehl andLukas Funk for annotation.180ReferencesEneko Agirre, Daniel Cer, Mona Diab, and AitorGonzalez-Agirre.
2012.
SemEval-2012 Task 6: A pi-lot on semantic textual similarity.
In Proceedings ofthe 6th International Workshop on Semantic Evalua-tions, Montreal, Canada, June.
to appear.Regina Barzilay and Mirella Lapata.
2008.
Modelinglocal coherence: An entity-based approach.
Computa-tional Linguistics, 34(1):1?34.Regina Barzilay and Lillian Lee.
2004.
Catching thedrift: Probabilistic content models, with applicationsto generation and summarization.
In Proceedings ofthe Human Language Technology Conference of theNorth American Chapter of the Association for Com-putational Linguistics, Boston, Mass., 2?7 May 2004,pages 113?120.Anja Belz, Eric Kow, Jette Viethen, and Albert Gatt.2009.
The grec main subject reference generationchallenge 2009: overview and evaluation results.
InProceedings of the 2009 Workshop on Language Gen-eration and Summarisation, pages 79?87.Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, DaniloGiampiccolo, and Bernardo Magnini.
2009.
The fifthpascal recognizing textual entailment challenge.
InProceedings of TAC.Anders Bjo?rkelund, Bernd Bohnet, Love Hafdell, andPierre Nugues.
2010.
A high-performance syntac-tic and semantic dependency parser.
In Coling 2010:Demonstration Volume, pages 33?36, Beijing, China,August.
Coling 2010 Organizing Committee.Bernd Bohnet.
2010.
Top accuracy and fast dependencyparsing is not a contradiction.
In Proceedings of the23rd International Conference on Computational Lin-guistics (Coling 2010), pages 89?97, Beijing, China,August.
Coling 2010 Organizing Committee.Chris Brockett.
2007.
Aligning the RTE 2006 Corpus.Microsoft Research.Peter F. Brown, Vincent J. Della Pietra, Stephan A. DellaPietra, and Robert L. Mercer.
1993.
The mathematicsof statistical machine translation: Parameter estima-tion.
Computational Linguistics, 19:263?311.Jacob Cohen.
1960.
A coefficient of agreement for nom-inal scales.
Educational and Psychological Measure-ment, 20:37?46.Paul R. Cohen.
1995.
Empirical methods for artificialintelligence.
MIT Press, Cambridge, MA, USA.Trevor Cohn, Chris Callison-Burch, and Mirella Lap-ata.
2008.
Constructing Corpora for Development andEvaluation of Paraphrase Systems.
34(4).Ido Dagan, Oren Glickman, and Bernardo Magnini.2006.
The PASCAL recognising textual entailmentchallenge.
In J. Quin?onero-Candela, I. Dagan, andB.
Magnini, editors, Machine Learning Challenges,pages 177?190.
Springer, Heidelberg, Germany.William B. Dolan and Chris Brockett.
2005.
Automat-ically constructing a corpus of sentential paraphrases.In Proceedings of the Third International Workshop onParaphrasing.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press, Cambridge,Mass.Adrew V. Goldberg and Robert E. Tarjan.
1986.
Anew approach to the maximum flow problem.
In Pro-ceedings of the eighteenth annual ACM symposium onTheory of computing, pages 136?146, New York, NY,USA.Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.1995.
Centering: A framework for modeling the lo-cal coherence of discourse.
Computational Linguis-tics, 21(2):203?225.Weiwei Guo and Mona Diab.
2011.
Semantic topic mod-els: Combining word distributional statistics and dic-tionary definitions.
In Proceedings of the 2011 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 552?561, July.Shudong Huang, David Graff, and George Doddington.2002.
Multiple-Translation Chinese Corpus.
Linguis-tic Data Consortium, Philadelphia.Karin Kipper, Anna Korhonen, Neville Ryant, andMartha Palmer.
2008.
A Large-scale Classificationof English Verbs.
42(1):21?40.Thomas K. Landauer and Susan T. Dumais.
1997.
A so-lution to Plato?s problem: The Latent Semantic Anal-ysis theory of the acquisition, induction, and represen-tation of knowledge.
Psychological Review, 104:211?240.Percy Liang, Benjamin Taskar, and Dan Klein.
2006.Alignment by agreement.
In North American Associ-ation for Computational Linguistics (NAACL), pages104?111.Dekang Lin.
1998.
An information-theoretic defini-tion of similarity.
In Proceedings of the 15th Inter-national Conference on Machine Learning, Madison,Wisc., 24?27 July 1998, pages 296?304.Bill MacCartney, Michael Galley, and Christopher D.Manning.
2008.
A phrase-based alignment modelfor natural language inference.
In Proceedings of the2008 Conference on Empirical Methods in NaturalLanguage Processing, Waikiki, Honolulu, Hawaii, 25-27 October 2008.Adam Meyers, Ruth Reeves, and Catherine Macleod.2008.
NomBank v1.0.
Linguistic Data Consortium,Philadelphia.Shachar Mirkin, Jonathan Berant, Ido Dagan, and EyalShnarch.
2010a.
Recognising entailment within dis-181course.
In Proceedings of the 23rd International Con-ference on Computational Linguistics (Coling 2010),Beijing, China, August.
Coling 2010 Organizing Com-mittee.Shachar Mirkin, Ido Dagan, and Sebastian Pado?.
2010b.Assessing the role of discourse references in entail-ment inference.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics, Uppsala, Sweden, 11?16 July 2010.Jeff Mitchell and Mirella Lapata.
2010.
Compositionin Distributional Models of Semantics.
34(8):1388?1429.Franz Josef Och and Hermann Ney.
2003.
A SystematicComparison of Various Statistical Alignment Models.29(1):19?51.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The proposition bank: An annotated corpus ofsemantic roles.
Computational Linguistics, 31(1):71?105.Robert Parker, David Graff, Jumbo Kong, Ke Chen, andKazuaki Maeda.
2011.
English Gigaword Fifth Edi-tion.
Linguistic Data Consortium, Philadelphia.Ted Pedersen, Siddharth Patwardhan, and Jason Miche-lizzi.
2004.
WordNet::Similarity ?
Measuring the re-latedness of concepts.
In Companion Volume to theProceedings of the Human Language Technology Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics, Boston, Mass.,2?7 May 2004, pages 267?270.Tom Richens.
2008.
Anomalies in the wordnet verb hier-archy.
In Proceedings of the 22nd International Con-ference on Computational Linguistics (Coling 2008),pages 729?736.
Association for Computational Lin-guistics.Michael Roth and Anette Frank.
2012.
Aligning pred-icate argument structures in monolingual comparabletexts: A new corpus for a new task.
In Proceedingsof the First Joint Conference on Lexical and Computa-tional Semantics, Montreal, Canada, June.Josef Ruppenhofer, Caroline Sporleder, Roser Morante,Collin Baker, and Martha Palmer.
2010.
SemEval-2010 Task 10: Linking Events and Their Participantsin Discourse.
In Proceedings of the 5th InternationalWorkshop on Semantic Evaluations, pages 45?50, Up-psala, Sweden, July.Richard Socher, Eric H. Huang, Jeffrey Pennington, An-drew Y. Ng, and Christopher D. Manning.
2011.
Dy-namic pooling and unfolding recursive autoencodersfor paraphrase detection.
In Advances in Neural Infor-mation Processing Systems (NIPS 2011).Ivan Titov and Mikhail Kozhevnikov.
2010.
Bootstrap-ping semantic analyzers from non-contradictory texts.In Proceedings of the 48th Annual Meeting of the Asso-ciation for Computational Linguistics, Uppsala, Swe-den, 11?16 July 2010, pages 958?967.Christopher Walker, Stephanie Strassel, Julie Medero,and Kazuaki Maeda.
2006.
ACE 2005 Multilin-gual Training Corpus.
Linguistic Data Consortium,Philadelphia.Stephen Wan, Mark Dras, Robert Dale, and Cecile Paris.2006.
Using dependency-based features to take the?Para-farce?
out of paraphrase.
In Proceedings of theAustralasian Language Technology Workshop, pages131?138.Ralph Weischedel, Martha Palmer, Mitchell Marcus, Ed-uard Hovy, Sameer Pradhan, Lance Ramshaw, Nian-wen Xue, Ann Taylor, Jeff Kaufman, Michelle Fran-chini, Mohammed El-Bachouti, Robert Belvin, andAnn Houston.
2011.
OntoNotes Release 4.0.
Lin-guistic Data Consortium, Philadelphia.Sander Wubben, Antal van den Bosch, Emiel Krahmer,and Erwin Marsi.
2009.
Clustering and matchingheadlines for automatic paraphrase acquisition.
InProceedings of the 12th European Workshop on Nat-ural Language Generation (ENLG 2009), pages 122?125, Athens, Greece, March.
Association for Compu-tational Linguistics.182
