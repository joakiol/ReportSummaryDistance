Proceedings of NAACL HLT 2007, pages 356?363,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsNear-Synonym Choice in an Intelligent ThesaurusDiana InkpenSchool of Information Technology and Engineering,University of Ottawa800 King Edward, Ottawa, ON, Canada, K1N 6N5diana@site.uottawa.caAbstractAn intelligent thesaurus assists a writerwith alternative choices of words and or-ders them by their suitability in the writ-ing context.
In this paper we focus onmethods for automatically choosing near-synonyms by their semantic coherencewith the context.
Our statistical methoduses the Web as a corpus to computemutual information scores.
Evaluationexperiments show that this method per-forms better than a previous method onthe same task.
We also propose and evalu-ate two more methods, one that uses anti-collocations, and one that uses supervisedlearning.
To asses the difficulty of thetask, we present results obtained by hu-man judges.1 IntroductionWhen composing a text, a writer can access a the-saurus to retrieve words that are similar to a giventarget word, when there is a need to avoid repeatingthe same word, or when the word does not seem tobe the best choice in the context.Our intelligent thesaurus is an interactive appli-cation that presents the user with a list of alterna-tive words (near-synonyms), and, unlike standardthesauri, it orders the choices by their suitability tothe writing context.
We investigate how the collo-cational properties of near-synonyms can help withchoosing the best words.
This problem is difficultbecause the near-synonyms have senses that are veryclose to each other, and therefore they occur in sim-ilar contexts; we need to capture the subtle differ-ences specific to each near-synonym.Our thesaurus brings up only alternatives thathave the same part-of-speech with the target word.The choices could come from various inventoriesof near-synonyms or similar words, for example theRoget thesaurus (Roget, 1852), dictionaries of syn-onyms (Hayakawa, 1994), or clusters acquired fromcorpora (Lin, 1998).In this paper we focus on the task of automat-ically selecting the best near-synonym that shouldbe used in a particular context.
The natural way tovalidate an algorithm for this task would be to askhuman readers to evaluate the quality of the algo-rithm?s output, but this kind of evaluation would bevery laborious.
Instead, we validate the algorithmsby deleting selected words from sample sentences,to see whether the algorithms can restore the miss-ing words.
That is, we create a lexical gap and eval-uate the ability of the algorithms to fill the gap.
Twoexamples are presented in Figure 1.
All the near-synonyms of the original word, including the worditself, become the choices in the solution set (see thefigure for two examples of solution sets).
The task isto automatically fill the gap with the best choice inthe particular context.
We present a method of scor-ing the choices.
The highest scoring near-synonymwill be chosen.
In order to evaluate how well ourmethod works we consider that the only correct so-lution is the original word.
This will cause our eval-uation scores to underestimate the performance, asmore than one choice will sometimes be a perfect356Sentence: This could be improved by more detailed considera-tion of the processes of ......... propagation inherent in digitizingprocedures.Original near-synonym: errorSolution set: mistake, blooper, blunder, boner, contretemps,error, faux pas, goof, slip, solecismSentence: The day after this raid was the official start of oper-ation strangle, an attempt to completely destroy the ......... linesof communication.Original near-synonym: enemySolution set: opponent, adversary, antagonist, competitor, en-emy, foe, rivalFigure 1: Examples of sentences with a lexical gap,and candidate near-synonyms to fill the gap.solution.
Moreover, what we consider to be the bestchoice is the typical usage in the corpus, but it mayvary from writer to writer.
Nonetheless, it is a con-venient way of producing test data.The statistical method that we propose here isbased on semantic coherence scores (based on mu-tual information) of each candidate with the wordsin the context.
We explore how far such a methodcan go when using the Web as a corpus.
We estimatethe counts by using the Waterloo MultiText1 system(Clarke and Terra, 2003b) with a corpus of aboutone terabyte of text collected by a Web crawler.
Wealso propose a method that uses collocations andanti-collocations, and a supervised method that useswords and mutual information scores as featured formachine learning.2 Related workThe idea of using the Web as a corpus of textshas been exploited by many researchers.
Grefen-stette (1999) used the Web for example-based ma-chine translation; Kilgarriff (2001) investigated thetype of noise in Web data; Mihalcea and Moldovan(1999) and Agirre and Martinez (2000) used it as anadditional resource for word sense disambiguation;Resnik (1999) mined the Web for bilingual texts;Turney (2001) used Web frequency counts to com-pute information retrieval-based mutual-informationscores.
In a Computational Linguistics special issueon the Web as a corpus (Kilgarriff and Grefenstette,1We thank Egidio Terra, Charlie Clarke, and Univ.
of Wa-terloo for allowing us to use MultiText, and to Peter Turney andIIT/NRC for giving us access to their local copy of the corpus.2003), Keller and Lapata (2003) show that Webcounts correlate well with counts collected from abalanced corpus: the size of the Web compensatesfor the noise in the data.
In this paper we are using avery large corpus of Web pages to address a problemthat has not been successfully solved before.In fact, the only work that addresses exactly thesame task is that of Edmonds (1997), as far as weare aware.
Edmonds gives a solution based on alexical co-occurrence network that included second-order co-occurrences.
We use a much larger corpusand a simpler method, and we obtain much betterresults.Our task has similarities to the word sense disam-biguation task.
Our near-synonyms have senses thatare very close to each other.
In Senseval, some ofthe fine-grained senses are also close to each other,so they might occur in similar contexts, while thecoarse-grained senses are expected to occur in dis-tinct contexts.
In our case, the near-synonyms aredifferent words to choose from, not the same wordwith different senses.3 A statistical method for near-synonymchoiceOur method computes a score for each candidatenear-synonym that could fill in the gap.
The near-synonym with the highest score is the proposed so-lution.
The score for each candidate reflects howwell a near-synonym fits in with the context.
It isbased on the mutual information scores between anear-synonym and the content words in the context(we filter out the stopwords).The pointwise mutual information (PMI) be-tween two words x and y compares the probabil-ity of observing the two words together (their jointprobability) to the probabilities of observing x and yindependently (the probability of occurring togetherby chance) (Church and Hanks, 1991): PMI(x, y) =log2P (x,y)P (x)P (y)The probabilities can be approximated by:P (x) = C(x)/N , P (y) = C(y)/N , P (x, y) =C(x, y)/N , where C denotes frequency counts andN is the total number of words in the corpus.
There-fore: PMI(x, y) = log2C(x,y)?NC(x)?C(y) , where N can beignored in comparisons.We model the context as a window of size 2k357around the gap (the missing word): k words to theleft and k words to the right of the gap.
If the sen-tence is s = ?
?
?w1 ?
?
?wk Gap wk+1 ?
?
?w2k ?
?
?,for each near-synonym NSi from the group of can-didates, the semantic coherence score is computedby the following formula:Score(NSi, s) = ?kj=1PMI(NSi, wj) +?2kj=k+1PMI(NSi, wj).We also experimented with the same formulawhen the sum is replaced with maximum to seewhether a particular word in the context has higherinfluence than the sum of all contributions (thoughthe sum worked better).Because we are using the Waterloo terabyte cor-pus and we issue queries to its search engine,we have several possibilities of computing the fre-quency counts.
C(x, y) can be the number of co-occurrences of x and y when y immediately followsx, or the distance between x and y can be up to q.We call q the query frame size.
The tool for access-ing the corpus allows us to use various values for q.The search engine also allows us to approxi-mate words counts with document counts.
If thecounts C(x), C(y), and C(x, y) are approximatedas the number of document in which they appear,we obtain the PMI-IR formula (Turney, 2001).
Thequeries we need to send to the search engine arethe same but they are restricted to document counts:C(x) is the number of document in which x occurs;C(x, y) is the number of documents in which x isfollowed by y in a frame of size q.Other statistical association measures, such aslog-likelihood, could be used.
We tried only PMIbecause it is easy to compute on a Web corpus andbecause PMI performed better than other measuresin (Clarke and Terra, 2003a).We present the results in Section 6.1, where wecompare our method to a baseline algorithm that al-ways chooses the most frequent near-synonyms andto Edmonds?s method for the same task, on the samedata set.
First, however, we present two other meth-ods to which we compare our results.4 The anti-collocations methodFor the task of near-synonym choice, anothermethod that we implemented is the anti-collocationsmethod.
By anti-collocation we mean a combina-ghastly mistake spelling mistake?ghastly error spelling errorghastly blunder ?spelling blunder?ghastly faux pas ?spelling faux pas?ghastly blooper ?spelling blooper?ghastly solecism ?spelling solecism?ghastly goof ?spelling goof?ghastly contretemps ?spelling contretemps?ghastly boner ?spelling boner?ghastly slip ?spelling slipTable 1: Examples of collocations and anti-collocations.
The ?
indicates the anti-collocations.tion of words that a native speaker would not useand therefore should not be used when automaticallygenerating text.
This method uses a knowledge-base of collocational behavior of near-synonyms ac-quired in previous work (Inkpen and Hirst, 2006).
Afragment of the knowledge-base is presented in Ta-ble 1, for the near-synonyms of the word error andtwo collocate words ghastly and spelling.
The linesmarked by ?
represent anti-collocations and the restrepresent strong collocations.The anti-collocations method simply ranks thestrong collocations higher than the anti-collocations.In case of ties it chooses the most frequent near-synonym.
In Section 6.2 we present the results ofcomparing this method to the method from the pre-vious section.5 A supervised learning methodWe can also apply supervised learning techniques toour task.
It is easy to obtain labeled training data,the same way we collected test data for the two un-supervised methods presented above.
We train clas-sifiers for each group of near-synonyms.
The classesare the near-synonyms in the solution set.
The wordthat produced the gap is the expected solution, theclass label; this is a convenient way of producingtraining data, no need for manual annotation.
Eachsentence is converted into a vector of features to beused for training the supervised classifiers.
We usedtwo types of features.
The features of the first typeare the PMI scores of the left and right context witheach class (each near-synonym from the group).
Thenumber of features of this type is twice the numberof classes, one score for the part of the sentence atthe left of the gap, and one for the part at the rightof the gap.
The features of the second type are the3581.
mistake, error, fault2.
job, task, chore3.
duty, responsibility, obligation4.
difficult, hard5.
material, stuff6.
put up, provide, offer7.
decide, settle, resolve, adjudicate.Table 2: The near-synonym groups used in Exp1.words in the context window.
For each group ofnear-synonyms, we used as features the 500 most-frequent words situated close to the gaps in a devel-opment set.
The value of a word feature for eachtraining example is 1 if the word is present in thesentence (at the left or at the right of the gap), and 0otherwise.
We trained classifiers using several ma-chine learning algorithms, to see which one is bestat discriminating among the near-synonyms.
In Sec-tion 6.3, we present the results of several classifiers.A disadvantage of the supervised method is that itrequires training for each group of near-synonyms.Additional training would be required whenever weadd more near-synonyms to our knowledge-base.6 Evaluation6.1 Comparison to Edmonds?s methodIn this section we present results of the statisticalmethod explained in Section 3.
We compare ourresults with those of Edmonds?s (1997), whose so-lution used the texts from the year 1989 of theWall Street Journal (WSJ) to build a lexical co-occurrence network for each of the seven groupsof near-synonyms from Table 2.
The network in-cluded second-order co-occurrences.
Edmonds usedthe WSJ 1987 texts for testing, and reported accura-cies only a little higher than the baseline.
The near-synonyms in the seven groups were chosen to havelow polysemy.
This means that some sentences withwrong senses of near-synonyms might be in theFor comparison purposes, in this section we usethe same test data (WSJ 1987) and the same groupsof near-synonyms (we call these sentences the Exp1data set).
Our method is based on mutual informa-tion, not on co-occurrence counts.
Our counts arecollected from a much larger corpus.Table 3 presents the comparative results for theseven groups of near-synonyms (we did not repeatAccuracySet No.
of Base- Edmonds Stat.
Stat.cases line method method method(Docs) (Words)1.
6,630 41.7% 47.9% 61.0% 59.1%2.
1,052 30.9% 48.9% 66.4% 61.5%3.
5,506 70.2% 68.9% 69.7% 73.3%4.
3,115 38.0% 45.3% 64.1% 66.0%5.
1,715 59.5% 64.6% 68.6% 72.2%6.
11,504 36.7% 48.6% 52.0% 52.7%7.
1,594 37.0% 65.9% 74.5% 76.9%AVG 31,116 44.8% 55.7% 65.1% 66.0%Table 3: Comparison between the statistical methodfrom Section 3, baseline algorithm, and Edmonds?smethod (Exp1 data set).them in the first column of the table, only the num-ber of the group.).
The last row averages the ac-curacies for all the test sentences.
The second col-umn shows how many test sentences we collectedfor each near-synonym group.
The third column isfor the baseline algorithm that always chooses themost frequent near-synonym.
The fourth columnpresents the results reported in (Edmonds, 1997).column show the results of the supervised learningclassifier described in Section 5.
The fifth columnpresents the result of our method when using doc-ument counts in PMI-IR, and the last column is forthe same method when using word counts in PMI.We show in bold the best accuracy for each data set.We notice that the automatic choice is more difficultfor some near-synonym groups than for the others.In this paper, by accuracy we mean the number ofcorrect choices made by each method (the number ofgaps that were correctly filled).
The correct choice isthe near-synonym that was initially replaced by thegap in the test sentence.To fine-tune our statistical method, we used thedata set for the near-synonyms of the word difficultcollected from the WSJ 1989 corpus as a develop-ment set.
We varied the context window size k andthe query frame q, and determined good values forthe parameter k and q.
The best results were ob-tained for small window sizes, k = 1 and k = 2(meaning k words to the left and k words to the rightof the gap).
For each k, we varied the query framesize q.
The results are best for a relatively smallquery frame, q = 3, 4, 5, when the query frame isthe same or slightly larger then the context window.359The results are worse for a very small query frame,q = 1, 2 and for larger query frames q = 6, 7, ..., 20or unlimited.
The results presented in the rest of thepaper are for k = 2 and q = 5.
For all the other datasets used in this paper (from WSJ 1987 and BNC)we use the parameter values as determined on thedevelopment set.Table 3 shows that the performance is generallybetter for word counts than for document counts.Therefore, we prefer the method that uses wordcounts (which is also faster in our particular set-ting).
The difference between them is not statis-tically significant.
Our statistical method performssignificantly better than both Edmond?s method andthe baseline algorithm.
For all the results presentedin this paper, statistical significance tests were doneusing the paired t-test, as described in (Manning andSchu?tze, 1999), page 209.On average, our method performs 22 percentagepoints better than the baseline algorithm, and 10percentage points better than Edmonds?s method.Its performance is similar to that of the supervisedmethod (see Section 6.3).
An important advan-tage of our method is that it works on any groupof near-synonyms without training, whereas Ed-monds?s method required a lexical co-occurrencenetwork to be built in advance for each group ofnear-synonyms and the supervised method requiredtraining for each near-synonym group.We note that the occasional presence of near-synonyms with other senses than the ones we needmight make the task somewhat easier.
Nonetheless,the task is still difficult, even for human judges, aswe will see in Section 6.4.
On the other hand, be-cause the solution allows only one correct answerthe accuracies are underestimated.6.2 Comparison to the anti-collocationsmethodIn a second experiment we compare the results ofour methods with the anti-collocation method de-scribed in Section 4.
We use the data set from ourprevious work, which contain sentences from thefirst half of the British National Corpus, with near-synonyms from the eleven groups from Table 4.The number of near-synonyms in each group ishigher compared with WordNet synonyms, becausethey are taken from (Hayakawa, 1994), a dictionary1.
benefit, advantage, favor, gain, profit2.
low, gush, pour, run, spout, spurt, squirt, stream3.
deficient, inadequate, poor, unsatisfactory4.
afraid, aghast, alarmed, anxious, apprehensive, fearful,frightened, scared, terror-stricken5.
disapproval, animadversion, aspersion, blame, criticism, rep-rehension6.
mistake, blooper, blunder, boner, contretemps, error, fauxpas, goof, slip, solecism7.
alcoholic, boozer, drunk, drunkard, lush, sot8.
leave, abandon, desert, forsake9.
opponent, adversary, antagonist, competitor, enemy, foe, ri-val10.
thin, lean, scrawny, skinny, slender, slim, spare, svelte, wil-lowy, wiry11.
lie, falsehood, fib, prevarication, rationalization, untruthTable 4: The near-synonym groups used in Exp2.that explains differences between near-synonyms.Moreover we retain only the sentences in which atleast one of the context words is in our previouslyacquired knowledge-base of near-synonym colloca-tions.
That is, the anti-collocations method worksonly if we know how a word in the context collo-cates with the near-synonyms from a group.
For thesentences that do not contain collocations or anti-collocations, it will perform no better than the base-line, because the information needed by the methodis not available in the knowledge-base.
Even if weincrease the coverage of the knowledge-base, theanti-collocation method might still fail too often dueto words that were not included.Table 5 presents the results of the comparison.
Weused two data sets: TestSample, which includes atmost two sentences per collocation (the first two sen-tences from the corpus); and TestAll, which includesall the sentences with collocations as they occurredin the corpus.
The reason we chose these two tests isnot to bias the results due to frequent collocations.The last two columns are the accuracies achievedby our method.
The second last column shows theresults of the method when the word counts are ap-proximated with document counts.
The improve-ment over the baseline is 16 to 27 percentage points.The improvement over the anti-collocations methodis 10 to 17 percentage points.6.3 Comparison to supervised learningWe present the results of the supervised methodfrom Section 5 on the data sets used in Section 6.1.360AccuracyTest set No.
Base- Anti- Stat.
Stat.of line collocs method methodcases method (Docs) (Words)Test 171 57.0% 63.3% 75.6% 73.3%SampleTestAll 332 48.5% 58.6% 70.0% 75.6%Table 5: Comparison between the statistical methodfrom Section 3 and the anti-collocations methodfrom Section 4.
(Exp2 data set from Section 6.2).ML method (Weka) Features AccuracyDecision Trees PMI scores 65.4%Decision Rules PMI scores 65.5%Na?
?ve Bayes PMI scores 52.5%K-Nearest Neighbor PMI scores 64.5%Kernel Density PMI scores 60.5%Boosting (Dec.
Stumps) PMI scores 67.7%Na?
?ve Bayes 500 words 68.0%Decision Trees 500 words 67.0%Na?
?ve Bayes PMI + 500 words 66.5%Boosting (Dec.
Stumps) PMI + 500 words 69.2%Table 6: Comparative results for the supervisedlearning method using various ML learning algo-rithms (Weka), averaged over the seven groups ofnear-synonyms from the Exp1 data set.As explained before, the data sets contain sentenceswith a lexical gap.
For each of the seven groupsof near-synonyms, the class to choose from, in or-der to fill in the gaps is one of the near-synonyms ineach cluster.
We implemented classifiers that use asfeatures either the PMI scores of the left and rightcontext with each class, or the words in the con-text windows, or both types of features combined.We used as features the 500 most-frequent words foreach group of near-synonyms.
We report accuraciesfor 10-fold cross-validation.Table 6 presents the results, averaged for the sevengroups of near-synonyms, of several classifiers fromthe Weka package (Witten and Frank, 2000).
Theclassifiers that use PMI features are Decision Trees,Decision Rules, Na?
?ve Bayes, K-Nearest Neighbor,Kernel Density, and Boosting a weak classifier (De-cision Stumps ?
which are shallow decision trees).Then, a Na?
?ve Bayes classifier that uses only theword features is presented, and the same type ofclassifiers with both types of features.
The otherclassifiers from the Weka package were also tried,but the results did not improve and these algorithmsAccuracyTest Base- Supervised Supervised Unsuper-set line Boosting Boosting vised(PMI) (PMI+words) method1.
41.7% 55.8% 57.3% 59.1%2.
30.9% 68.1% 70.8% 61.5%3.
70.2% 86.5% 86.7% 73.3%4.
38.0% 66.5% 66.7% 66.0%5.
59.5% 70.4% 71.0% 72.2%6.
36.7% 53.0% 56.1% 52.7%7.
37.0% 74.0% 75.8% 76.9%AVG 44.8% 67.7% 69.2% 66.0%Table 7: Comparison between the unsupervised sta-tistical method from Section 3 and the supervisedmethod described in Section 5, on the Exp1 data set.had difficulties in scaling up.
In particular, whenusing the 500 word features for each training exam-ple, only the Na?
?ve Bayes algorithm was able to runin reasonable time.
We noticed that the Na?
?ve Bayesclassifier performs very poorly on PMI features only(55% average accuracy), but performs very well onword features (68% average accuracy).
In contrast,the Decision Tree classifier performs well on PMIfeatures, especially when using boosting with Deci-sion Stumps.
When using both the PMI scores andthe word features, the results are slightly higher.
Itseems that both types of features are sufficient fortraining a good classifier, but combining them addsvalue.Table 7 presents the detailed results of two of thesupervised classifiers, and repeats, for easier com-parison, the results of the unsupervised statisticalmethod from Section 6.1.
The supervised classifierthat uses only PMI scores performs similar to the un-supervised method.
The best supervised classifier,that uses both types of features, performs slightlybetter than the unsupervised statistical method, butthe difference is not statistically significant.
We con-clude that the results of the supervised methods andthe unsupervised statistical method are similar.
Animportant advantage of the unsupervised method isthat it works on any group of near-synonyms withouttraining.6.4 Results obtained by human judgesWe asked two human judges, native speakers of En-glish, to guess the missing word in a random sampleof the Exp1 data set (50 sentences for each of the361Test set J1-J2 J1 J2 SystemAgreement Acc.
Acc.
Accuracy1.
72% 70% 76% 53%2.
82% 84% 84% 68%3.
86% 92% 92% 78%4.
76% 82% 76% 66%5.
76% 82% 74% 64%6.
78% 68% 70% 52%7.
80% 80% 90% 77%AVG 78.5% 79.7% 80.2% 65.4%Table 8: Results obtained by two human judges on arandom subset of the Exp1 data set.7 groups of near-synonyms, 350 sentences in total).The judges were instructed to choose words from thelist of near-synonyms.
The choice of a word not inthe list was allowed, but not used by the two judges.The results in Table 8 show that the agreement be-tween the two judges is high (78.5%), but not per-fect.
This means the task is difficult, even if somewrong senses in the test data might have made thetask easier in a few cases.The human judges were allowed to choose morethan one correct answer when they were convincedthat more than one near-synonym fits well in thecontext.
They used this option sparingly, only in 5%of the 350 sentences.
Taking the accuracy achievedof the human judges as an upper limit, the automaticmethod has room for improvement (10-15 percent-age points).
In future work, we plan to allow thesystem to make more than one choice when appro-priate (for example when the second choice has avery close score to the first choice).7 The intelligent thesaurusOur experiments show that the accuracy of the firstchoice being the best choice is 66 to 75%; thereforethere will be cases when the writer will not choosethe first alternative.
But the accuracy for the firsttwo choices is quite high, around 90%, as presentedin Table 9.If the writer is in the process of writing and selectsa word to be replaced with a near-synonym proposedby the thesaurus, then only the context on the left ofthe word can be used for ordering the alternatives.Our method can be easily adapted to consider onlythe context on the left of the gap.
The results ofthis case are presented in Table 10, for the data setsTest set Accuracy Accuracyfirst choice first 2 choicesExp1, AVG 66.0% 88.5%Exp2, TestSample 73.3% 94.1%Exp2, TestAll 75.6% 87.5%Table 9: Accuracies for the first two choices as or-dered by an interactive intelligent thesaurus.Test set Accuracy Accuracyfirst choice first 2 choicesExp1, AVG 58.0% 84.8%Exp2, TestSample 57.4% 75.1%Exp2, TestAll 56.1% 77.4%Table 10: Results of the statistical method whenonly the left context is considered.used in the previous sections.
The accuracy valuesare lower than in the case when both the left and theright context are considered (Table 9).
This is duein part to the fact that some sentences in the test setshave very little left context, or no left context at all.On the other hand, many times the writer composesa sentence or paragraph and then she/he goes backto change a word that does not sound right.
In thiscase, both the left and right context will be available.In the intelligent thesaurus, we could combinethe supervised and unsupervised method, by usinga supervised classifier when the confidence in theclassification is high, and by using the unsupervisedmethod otherwise.
Also the unsupervised statisti-cal method would be used for the groups of near-synonyms for which a supervised classifier was notpreviously trained.8 ConclusionWe presented a statistical method of choosing thebest near-synonym in a context.
We compared thismethod to a previous method (Edmonds?s method)and to the anti-collocation method and showed thatthe performance improved considerably.
We alsoshow that the unsupervised statistical method per-forms comparably to a supervised learning method.Our method based on PMI scores performs well,despite the well-known limitations of PMI in cor-pora.
PMI tends to have problems mostly on verysmall counts, but it works reasonably with largercounts.
Our web corpus is quite large, therefore theproblem of small counts does not appear.362In the intelligent thesaurus, we do not make thenear-synonym choice automatically, but we let theuser choose.
The first choice offered by the the-saurus is the best one quite often; the first twochoices are correct 90% of the time.Future work includes a word sense disambigua-tion module.
In case the target word selected by thewriter has multiple senses, they could trigger sev-eral groups of near-synonyms.
The system will de-cide which group represents the most likely sensesby computing the semantic coherence scores aver-aged over the near-synonyms from each group.We plan to explore the question of which inven-tory of near-synonyms or similar words is the mostsuitable for use in the intelligent thesaurus.Choosing the right near-synonym in context isalso useful in other applications, such as natural lan-guage generation (NLG) and machine translation.In fact we already used the near-synonym choicemodule in an NLG system, for complementing thechoices made by using the symbolic knowledge in-corporated into the system.ReferencesEneko Agirre and David Martinez.
2000.
Exploring au-tomatic word sense disambiguation with decision listsand the Web.
In Proceedings of the Workshop on Se-mantic Annotation And Intelligent Content, COLING2000, Saarbru?cken/Luxembourg/Nancy.Kenneth Church and Patrick Hanks.
1991.
Word asso-ciation norms, mutual information and lexicography.Computational Linguistics, 16 (1):22?29.Charles L. A. Clarke and Egidio Terra.
2003a.
Fre-quency estimates for statistical word similarity mea-sures.
In Proceedings of the Human Language Tech-nology Conference of the North American Chapter ofthe Association for Computational Linguistics (HLT-NAACL 2003), pages 165?172, Edmonton, Canada.Charles L. A. Clarke and Egidio Terra.
2003b.
Passageretrieval vs. document retrieval for factoid question an-swering.
In Proceedings of the 26th Annual Interna-tional ACM SIGIR Conference on Research and De-velopment in Information Retrieval, pages 427?428,Toronto, Canada.Philip Edmonds.
1997.
Choosing the word most typicalin context using a lexical co-occurrence network.
InProceedings of the 35th Annual Meeting of the Associ-ation for Computational Linguistics, pages 507?509,Madrid, Spain.Gregory Grefenstette.
1999.
The World Wide Web as aresource for example-based machine translation tasks.In Proceedings of the ASLIB Conference on Translat-ing and Computers, London, UK.S.
I. Hayakawa, editor.
1994.
Choose the Right Word.Second Edition, revised by Eugene Ehrlich.
Harper-Collins Publishers.Diana Inkpen and Graeme Hirst.
2006.
Building andusing a lexical knowledge-base of near-synonym dif-ferences.
Computational Linguistics, 32 (2):223?262.Frank Keller andMirella Lapata.
2003.
Using theWeb toobtain frequencies for unseen bigrams.
ComputationalLinguistics, 29 (3):459?484.Adam Kilgarriff and Gregory Grefenstette.
2003.
Intro-duction to the special issue on the Web as a corpus.Computational Linguistics, 29 (3):333?347.Adam Kilgarriff.
2001.
Web as corpus.
In Proceedingsof the 2001 Corpus Linguistics conference, pages 342?345, Lancaster, UK.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of the 36th AnnualMeeting of the Association for Computational Linguis-tics joint with 17th International Conference on Com-putational Linguistics (ACL-COLING?98), pages 768?774, Montreal, Quebec, Canada.Christopher Manning and Hinrich Schu?tze.
1999.
Foun-dations of Statistical Natural Language Processing.The MIT Press, Cambridge, MA.Rada Mihalcea and Dan Moldovan.
1999.
A method forword sense disambiguation from unrestricted text.
InProceedings of the 37th Annual Meeting of the Associ-ation for Computational Linguistics, pages 152?158,Maryland, MD.Philip Resnik.
1999.
Mining the Web for bilingual text.In Proceedings of the 37th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 527?534,Maryland, MD.Peter Mark Roget, editor.
1852.
Roget?s Thesaurus ofEnglish Words and Phrases.
Longman Group Ltd.,Harlow, Essex, UK.Peter Turney.
2001.
Mining the Web for synonyms:PMI-IR versus LSA on TOEFL.
In Proceedings ofthe Twelfth European Conference on Machine Learn-ing (ECML 2001), pages 491?502, Freiburg, Germany.Ian H. Witten and Eibe Frank.
2000.
Data Mining:Practical machine learning tools with Java implemen-tations.
Morgan Kaufmann, San Francisco, CA.363
