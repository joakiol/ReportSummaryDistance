Proceedings of the ACL 2010 Student Research Workshop, pages 31?36,Uppsala, Sweden, 13 July 2010.c?2010 Association for Computational LinguisticsUnsupervised Search for The Optimal Segmentation for StatisticalMachine TranslationCos?kun Mermer1,3and Ahmet Afs?
?n Ak?n2,31Bo?gazic?i University, Bebek, Istanbul, Turkey2Istanbul Technical University, Sar?yer, Istanbul, Turkey3T?UB?ITAK-UEKAE, Gebze, Kocaeli, Turkey{coskun,ahmetaa}@uekae.tubitak.gov.trAbstractWe tackle the previously unaddressedproblem of unsupervised determination ofthe optimal morphological segmentationfor statistical machine translation (SMT)and propose a segmentation metric thattakes into account both sides of the SMTtraining corpus.
We formulate the objec-tive function as the posterior probability ofthe training corpus according to a genera-tive segmentation-translation model.
Wedescribe how the IBM Model-1 transla-tion likelihood can be computed incremen-tally between adjacent segmentation statesfor efficient computation.
Submerging theproposed segmentation method in a SMTtask from morphologically-rich Turkish toEnglish does not exhibit the expected im-provement in translation BLEU scores andconfirms the robustness of phrase-basedSMT to translation unit combinatorics.A positive outcome of this work is thedescribed modification to the sequentialsearch algorithm of Morfessor (Creutz andLagus, 2007) that enables arbitrary-foldparallelization of the computation, whichunexpectedly improves the translation per-formance as measured by BLEU.1 IntroductionIn statistical machine translation (SMT), wordsare normally considered as the building blocks oftranslation models.
However, especially for mor-phologically complex languages such as Finnish,Turkish, Czech, Arabic etc., it has been shownthat using sub-lexical units obtained after morpho-logical preprocessing can improve the machinetranslation performance over a word-based sys-tem (Habash and Sadat, 2006; Oflazer and DurgarEl-Kahlout, 2007; Bisazza and Federico, 2009).However, the effect of segmentation on transla-tion performance is indirect and difficult to isolate(Lopez and Resnik, 2006).The challenge in designing a sub-lexical SMTsystem is the decision of what segmentation to use.Linguistic morphological analysis is intuitive, butit is language-dependent and could be highly am-biguous.
Furthermore, it is not necessarily opti-mal in that (i) manually engineered segmentationschemes can outperform a straightforward linguis-tic morphological segmentation, e.g., (Habash andSadat, 2006), and (ii) it may result in even worseperformance than a word-based system, e.g., (Dur-gar El-Kahlout and Oflazer, 2006).A SMT system designer has to decide whatsegmentation is optimal for the translation taskat hand.
Existing solutions to this problem arepredominantly heuristic, language-dependent, andas such are not easily portable to other lan-guages.
Another point to consider is that the op-timal degree of segmentation might decrease asthe amount of training data increases (Lee, 2004;Habash and Sadat, 2006).
This brings into ques-tion: For the particular language pair and trainingcorpus at hand, what is the optimal (level of) sub-word segmentation?
Therefore, it is desirable tolearn the optimal segmentation in an unsupervisedmanner.In this work, we extend the method of Creutzand Lagus (2007) so as to maximize the transla-tion posterior in unsupervised segmentation.
Thelearning process is tailored to the particular SMTtask via the same parallel corpus that is used intraining the statistical translation models.2 Related WorkMost works in SMT-oriented segmentation are su-pervised in that they consist of manual experimen-tation to choose the best among a set of segmen-tation schemes, and are language(pair)-dependent.For Arabic, Sadat and Habash (2006) present sev-eral morphological preprocessing schemes that en-tail varying degrees of decomposition and com-31pare the resulting translation performances in anArabic-to-English task.
Shen et al (2007) use asubset of the morphology and apply only a fewsimple rules in segmenting words.
Durgar El-Kahlout and Oflazer (2006) tackle this problemwhen translating from English to Turkish, an ag-glutinative language.
They use a morphologi-cal analyzer and disambiguation to arrive at mor-phemes as tokens.
However, training the trans-lation models with morphemes actually degradesthe translation performance.
They outperformthe word-based baseline only after some selec-tive morpheme grouping.
Bisazza and Federico(2009) adopt an approach similar to the Arabicsegmentation studies above, this time in a Turkish-to-English translation setting.Unsupervised segmentation by itself has gar-nered considerable attention in the computationallinguistics literature (Poon et al, 2009; Snyder andBarzilay, 2008; Dasgupta and Ng, 2007; Creutzand Lagus, 2007; Brent, 1999).
However, fewworks report their performance in a translationtask.
Virpioja et al (2007) used Morfessor (Creutzand Lagus, 2007) to segment both sides of the par-allel training corpora in translation between Dan-ish, Finnish, and Swedish, but without a consistentimprovement in results.Morfessor, which gives state of the art results inmany tests (Kurimo et al, 2009), uses only mono-lingual information in its objective function.
It isconceivable that we can achieve a better segmenta-tion for translation by considering not one but bothsides of the parallel corpus.
A posssible choice isthe post-segmentation alignment accuracy.
How-ever, Elming et al (2009) show that optimizingsegmentation with respect to alignment error rate(AER) does not improve and even degrades ma-chine translation performance.
Snyder and Barzi-lay (2008) use bilingual information but the seg-mentation is learned independently from transla-tion modeling.In Chang et al (2008), the granularity of theChinese word segmentation is optimized by train-ing SMT systems for several values of a granular-ity bias parameter and it is found that the value thatmaximizes translation performance (as measuredby BLEU) is different than the value that maxi-mizes segmentation accuracy (as measured by pre-cision and recall).One motivation in morphological preprocess-ing before translation modeling is ?morphologymatching?
as in Lee (2004) and in the scheme?EN?
of Habash and Sadat (2006).
In Lee (2004),the goal is to match the lexical granularities of thetwo languages by starting with a fine-grained seg-mentation of the Arabic side of the corpus andthen merging or deleting Arabic morphemes us-ing alignments with a part-of-speech tagged En-glish corpus.
But this method is not completelyunsupervised since it requires external linguisticresources in initializing the segmentation with theoutput of a morphological analyzer and disam-biguator.
Talbot and Osborne (2006) tackle a spe-cial case of morphology matching by identifyingredundant distinctions in the morphology of onelanguage compared to another.3 MethodMaximizing translation performance directlywould require SMT training and decoding foreach segmentation hypothesis considered, whichis computationally infeasible.
So we make someconditional independence assumptions using agenerative model and decompose the posteriorprobability P (Mf|e, f).
In this notation e and fdenote the two sides of a parallel corpus and Mfdenotes the segmentation model hypothesized forf .
Our approach is an extension of Morfessor(Creutz and Lagus, 2007) so as to include thetranslation model probability in its cost calcula-tion.
Specifically, the segmentation model takesinto account the likelihood of both sides of theparallel corpus while searching for the optimalsegmentation.
The joint likelihood is decomposedinto a prior, a monolingual likelihood, and atranslation likelihood, as shown in Eq.
1.P (e, f,Mf) = P (Mf)P (f |Mf)P (e|f,Mf)(1)Assuming conditional independence betweene and Mfgiven f , the maximum a posteriori(MAP) objective can be written as:?Mf= arg maxMfP (Mf)P (f |Mf)P (e|f) (2)The role of the bilingual component P (e|f)in Eq.
2 can be motivated with a simple exam-ple as follows.
Consider an occurrence of twophrase pairs in a Turkish-English parallel corpusand the two hypothesized sets of segmentationsfor the Turkish phrases as in Table 1.
Without ac-cess to the English side of the corpus, a monolin-gual segmenter can quite possibly score Seg.
#132Phrase #1 Phrase #2Turkish phrase: anahtar anahtar?mEnglish phrase: key my keySeg.
#1: anahtar anahtar?
+mSeg.
#2: anahtar anahtar +?mTable 1: Example segmentation hypotheseshigher than Seg.
#2 (e.g., due to the high fre-quency of the observed morph ?+m?).
On theother hand, a bilingual segmenter is expected toassign a higher alignment probability P (e|f) toSeg.
#2 than Seg.
#1, because of the aligned wordskey||anahtar, therefore ranking Seg.
#2 higher.The two monolingual components of Eq.
2 arecomputed as in Creutz and Lagus (2007).
To sum-marize briefly, the prior P (Mf) is assumed to onlydepend on the frequencies and lengths of the indi-vidual morphs, which are also assumed to be in-dependent.
The monolingual likelihood P (f |Mf)is computed as the product of morph probabilitiesestimated from their frequencies in the corpus.To compute the bilingual (translation) likeli-hood P (e|f), we use IBM Model 1 (Brown etal., 1993).
Let an aligned sentence pair be rep-resented by (se, sf), which consists of word se-quences se= e1, ..., eland sf= f1, ..., fm.
Us-ing a purely notational switch of the corpus labelsfrom here on to be consistent with the SMT lit-erature, where the derivations are in the form ofP (f |e), the desired translation probability is givenby the expression:P (f |e) =P (m|e)(l + 1)mm?j=1l?i=0t(fj|ei), (3)The sentence length probability distributionP (m|e) is assumed to be Poisson with the ex-pected sentence length equal to m.3.1 Incremental computation of Model-1likelihoodDuring search, the translation likelihood P (e|f)needs to be calculated according to Eq.
3 for everyhypothesized segmentation.To compute Eq.
3, we need to have at hand theindividual morph translation probabilities t(fj|ei).These can be estimated using the EM algorithmgiven by (Brown, 1993), which is guaranteed toconverge to a global maximum of the likelihoodfor Model 1.
However, running the EM algorithmto optimization for each considered segmentationmodel can be computationally expensive, and canresult in overtraining.
Therefore, in this work weused the likelihood computed after the first EMiteration, which also has the nice property thatP (f |e) can be computed incrementally from onesegmentation hypothesis to the next.The incremental updates are derived from theequations for the count collection and probabilityestimation steps of the EM algorithm as follows.In the count collection step, in the first iteration,we need to compute the fractional counts c(fj|ei)(Brown et al, 1993):c(fj|ei) =1l + 1(#fj)(#ei), (4)where (#fj) and (#ei) denote the number of occur-rences of fjin sfand eiin se, respectively.Let fkdenote the word hypothesized to be seg-mented.
Let the resulting two sub-words be fpandfq, any of which may or may not previously existin the vocabulary.
Then, according to Eq.
(4), as aresult of the segmentation no update is needed forc(fj|ei) for j = 1 .
.
.
N , j 6= p, q, i = 1 .
.
.M(note that fkno longer exists); and the necessaryupdates ?c(fj|ei) for c(fj|ei), where j = p, q;i = 1 .
.
.M are given by:?c(fj|ei) =1l + 1(#fk)(#ei).
(5)Note that Eq.
(5) is nothing but the previouscount value for the segmented word, c(fk|ei).
So,all needed in the count collection step is to copythe set of values c(fk|ei) to c(fp|ei) and c(fq|ei),adding if they already exist.Then in the probability estimation step, the nor-malization is performed including the newly addedfractional counts.3.2 Parallelization of searchIn an iteration of the algorithm, all words are pro-cessed in random order, computing for each wordthe posterior probability of the generative modelafter each possible binary segmentation (splitting)of the word.
If the highest-scoring split increasesthe posterior probability compared to not splitting,that split is accepted (for all occurrences of theword) and the resulting sub-words are explored re-cursively for further segmentations.
The process isrepeated until an iteration no more results in a sig-nificant increase in the posterior probability.The search algorithm of Morfessor is a greedyalgorithm where the costs of the next search points33Word-basedMorfessorMorfessor-pMorfessor-bi51.451.651.85252.252.452.652.85353.253.4Segmentation methodBLEU scoreFigure 1: BLEU scores obtained with differentsegmentation methods.
Multiple data points fora system correspond to different random orders inprocessing the data (Creutz and Lagus, 2007).are affected by the decision in the current step.This leads to a sequential search and does not lenditself to parallelization.We propose a slightly modified search proce-dure, where the segmentation decisions are storedbut not applied until the end of an iteration.
Inthis way, the cost calculations (which is the mosttime-consuming component) can all be performedindependently and in parallel.
Since the model isnot updated at every decision, the search path candiffer from that in the sequential greedy search andhence result in different segmentations.4 ResultsWe performed in vivo testing of the segmenta-tion algorithm on the Turkish side of a Turkish-to-English task.
We compared the segmenta-tions produced by Morfessor, Morfessor modi-fied for parallel search (Morfessor-p), and Mor-fessor with bilingual cost (Morfessor-bi) againstthe word-based performance.
We used the ATRBasic Travel Expression Corpus (BTEC) (Kikuiet al, 2006), which contains travel conversa-tion sentences similar to those in phrase-booksfor tourists traveling abroad.
The training cor-pus contained 19,972 sentences with average sen-tence length 5.6 and 7.7 words for Turkish andEnglish, respectively.
The test corpus consistedof 1,512 sentences with 16 reference translations.We used GIZA++ (Och and Ney, 2003) for post-segmentation token alignments and the Mosestoolkit (Koehn et al, 2007) with default param-eters for phrase-based translation model genera-tion and decoding.
Target language models were1.5581.561.5621.5641.5661.5681.57 x 10651.451.651.85252.252.452.652.85353.253.4Morfessor costBLEU score1.0721.0741.0761.0781.081.0821.084 x 10651.85252.252.452.652.85353.253.453.6Morfessor-bi costBLEU scoreFigure 2: Cost-BLEU plots of Morfessor andMorfessor-bi.
Correlation coefficients are -0.005and -0.279, respectively.trained on the English side of the training cor-pus using the SRILM toolkit (Stolcke, 2002).
TheBLEU metric (Papineni et al, 2002) was used fortranslation evaluation.Figure 1 compares the translation performanceobtained using the described segmentation meth-ods.
All segmentation methods generally im-prove the translation performance (Morfessor andMorfessor-p) compared to the word-based models.However, Morfessor-bi, which utilizes both sidesof the parallel corpus in segmenting, does not con-vincingly outperform the monolingual methods.In order to investigate whether the proposedbilingual segmentation cost correlates any betterthan the monolingual segmentation cost of Mor-fessor, we show several cost-BLEU pairs obtainedfrom the final and intermediate segmentations ofMorfessor and Morfessor-bi in Fig.
2.
The cor-relation coefficients show that the proposed bilin-gual metric is somewhat predictive of the trans-lation performance as measured by BLEU, whilethe monolingual Morfessor cost metric has almostno correlation.
Yet, the strong noise in the BLEUscores (vertical variation in Fig.
2) diminishes theeffect of this correlation, which explains the incon-sistency of the results in Fig.
1.
Indeed, in our ex-periments even though the total cost kept decreas-ing at each iteration of the search algorithm, theBLEU scores obtained by those intermediate seg-mentations fluctuated without any consistent im-provement.Table 2 displays sample segmentations pro-duced by both the monolingual and bilingual seg-mentation algorithms.
We can observe that uti-lizing the English side of the corpus enabled34Count Morfessor Morfessor-bi English Gloss7 anahtar anahtar (the) key6 anahtar + ?m?
anahtar + ?m?
my key (ACC.
)5 anahtarla anahtar + la with (the) key4 anahtar?
anahtar + ?1(the) key (ACC.
);2his/her key3 anahtar?
+ m anahtar + ?m my key3 anahtar?
+ n anahtar + ?n1your key;2of (the) key1 anahtar?
+ n?z anahtar + ?n?z your (pl.)
key1 anahtar?
+ n?
anahtar + ?n?1your key (ACC.
);2his/her key (ACC.
)1 anahtar + ?n?z?
anahtar + ?n?z?
your (pl.)
key (ACC.
)1 oyun + lar oyunlar (the) games2 oyun + lar?
oyunlar + ?1(the) games (ACC.
);2his/her games;3their game(s)1 oyun + lar?n oyunlar + ?
+ n1of (the) games;2your games1 oyun + lar?n?z?
oyunlar + ?
+ n + ?z?
your (pl.)
games (ACC.
)Table 2: Sample segmentations produced by Morfessor and Morfessor-biMorfessor-bi: (i) to consistently identify the rootword ?anahtar?
(top portion), and (ii) to match theEnglish plural word form ?games?
with the Turk-ish plural word form ?oyunlar?
(bottom portion).Monolingual Morfessor is unaware of the targetsegmentation, and hence it is up to the subsequenttranslation model training to learn that ?oyun?
issometimes translated as ?game?
and sometimes as?games?
in the segmented training corpus.5 ConclusionWe have presented a method for determining opti-mal sub-word translation units automatically froma parallel corpus.
We have also showed a methodof incrementally computing the first iteration pa-rameters of IBM Model-1 between segmentationhypotheses.
Being language-independent, the pro-posed algorithm can be added as a one-time pre-processing step prior to training in a SMT systemwithout requiring any additional data/linguistic re-sources.
The initial experiments presented hereshow that the translation units learned by theproposed algorithm improves on the word-basedbaseline in both translation directions.One avenue for future work is to relax some ofthe several independence assumptions made in thegenerative model.
For example, independence ofconsecutive morphs could be relaxed by an HMMmodel for transitions between morphs (Creutz andLagus, 2007).
Other future work includes optimiz-ing the segmentation of both sides of the corpusand experimenting with other language pairs.It is also possible that the probability distribu-tions are not discriminative enough to outweighthe model prior tendencies since the translationprobabilities are estimated only crudely (single it-eration of Model-1 EM algorithm).
A possiblecandidate solution would be to weigh the transla-tion likelihood more in calculating the overall cost.In fact, this idea could be generalized into a log-linear modeling (e.g., (Poon et al, 2009)) of thevarious components of the joint corpus likelihoodand possibly other features.Finally, integration of sub-word segmentationwith the phrasal lexicon learning process in SMTis desireable (e.g., translation-driven segmenta-tion in Wu (1997)).
Hierarchical models (Chiang,2007) could cover this gap and provide a means toseamlessly integrate sub-word segmentation withstatistical machine translation.AcknowledgementsThe authors would like to thank Murat Sarac?larfor valuable discussions and guidance in this work,and the anonymous reviewers for very useful com-ments and suggestions.
Murat Sarac?lar is sup-ported by the T?UBA-GEB?IP award.ReferencesArianna Bisazza and Marcello Federico.
2009.
Mor-phological Pre-Processing for Turkish to EnglishStatistical Machine Translation.
In Proc.
of the In-ternational Workshop on Spoken Language Transla-tion, pages 129?135, Tokyo, Japan.M.R.
Brent.
1999.
An efficient, probabilisticallysound algorithm for segmentation and word discov-ery.
Machine Learning, 34(1):71?105.35P.F.
Brown, V.J.
Della Pietra, S.A. Della Pietra, andR.L.
Mercer.
1993.
The mathematics of statisticalmachine translation: Parameter estimation.
Compu-tational Linguistics, 19(2):263?311.Pi-Chuan Chang, Michel Galley, and Christopher D.Manning.
2008.
Optimizing Chinese word segmen-tation for machine translation performance.
In Pro-ceedings of the Third Workshop on Statistical Ma-chine Translation, pages 224?232, Columbus, Ohio.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228.M.
Creutz and K. Lagus.
2007.
Unsupervised modelsfor morpheme segmentation and morphology learn-ing.
ACM Transactions on Speech and LanguageProcessing, 4(1):1?34.Sajib Dasgupta and Vincent Ng.
2007.
High-performance, language-independent morphologicalsegmentation.
In Proceedings of HLT-NAACL,pages 155?163, Rochester, New York.
?Ilknur Durgar El-Kahlout and Kemal Oflazer.
2006.Initial explorations in English to Turkish statisticalmachine translation.
In Proceedings of the Work-shop on Statistical Machine Translation, pages 7?14, New York City, New York, USA.Jakob Elming, Nizar Habash, and Josep M. Crego.2009.
Combination of statistical word alignmentsbased on multiple preprocessing schemes.
In CyrillGoutte, Nicola Cancedda, Marc Dymetman, andGeorge Foster, editors, Learning Machine Transla-tion, chapter 5, pages 93?110.
MIT Press.Nizar Habash and Fatiha Sadat.
2006.
Arabic prepro-cessing schemes for statistical machine translation.In Proc.
of the HLT-NAACL, Companion Volume:Short Papers, pages 49?52, New York City, USA.G.
Kikui, S. Yamamoto, T. Takezawa, and E. Sumita.2006.
Comparative study on corpora for speechtranslation.
IEEE Transactions on Audio, Speechand Language Processing, 14(5):1674?1682.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProceedings of the 45th Annual Meeting of the Asso-ciation for Computational Linguistics, CompanionVolume: Proceedings of the Demo and Poster Ses-sions, pages 177?180, Prague, Czech Republic.M.
Kurimo, S. Virpioja, V.T.
Turunen, G.W.
Black-wood, and W. Byrne.
2009.
Overview and Resultsof Morpho Challenge 2009.
In Working notes of theCLEF workshop.Young-Suk Lee.
2004.
Morphological analysis for sta-tistical machine translation.
In Proceedings of HLT-NAACL, Companion Volume: Short Papers, pages57?60, Boston, Massachusetts, USA.Adam Lopez and Philip Resnik.
2006.
Word-basedalignment, phrase-based translation: What?s thelink?
In Proceedings of the 7th Conference of theAssociation for Machine Translation in the Ameri-cas (AMTA-06), pages 90?99.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Kemal Oflazer and?Ilknur Durgar El-Kahlout.
2007.Exploring different representational units inEnglish-to-Turkish statistical machine translation.In Proceedings of the Second Workshop on Statis-tical Machine Translation, pages 25?32, Prague,Czech Republic.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedingsof 40th Annual Meeting of the Association for Com-putational Linguistics, pages 311?318, Philadelphia,Pennsylvania, USA.Hoifung Poon, Colin Cherry, and Kristina Toutanova.2009.
Unsupervised morphological segmentationwith log-linear models.
In Proceedings of HLT-NAACL, pages 209?217, Boulder, Colorado.Fatiha Sadat and Nizar Habash.
2006.
Combinationof Arabic preprocessing schemes for statistical ma-chine translation.
In Proc.
of the 21st InternationalConference on Computational Linguistics and 44thAnnual Meeting of the Association for Computa-tional Linguistics, pages 1?8, Sydney, Australia.Wade Shen, Brian Delaney, and Tim Anderson.
2007.The MIT-LL/AFRL IWSLT-2007 MT system.
InProc.
of the International Workshop on Spoken Lan-guage Translation, Trento, Italy.Benjamin Snyder and Regina Barzilay.
2008.
Un-supervised multilingual learning for morphologicalsegmentation.
In Proceedings of the 46th AnnualMeeting of the Association for Computational Lin-guistics: HLT, pages 737?745, Columbus, Ohio.A.
Stolcke.
2002.
SRILM-an extensible languagemodeling toolkit.
In Seventh International Confer-ence on Spoken Language Processing, volume 3.David Talbot and Miles Osborne.
2006.
Modellinglexical redundancy for machine translation.
In Pro-ceedings of the 21st International Conference onComputational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguis-tics, pages 969?976, Sydney, Australia.S.
Virpioja, J.J. V?ayrynen, M. Creutz, and M. Sade-niemi.
2007.
Morphology-aware statistical machinetranslation based on morphs induced in an unsuper-vised manner.
In Machine Translation Summit XI,pages 491?498, Copenhagen, Denmark.D.
Wu.
1997.
Stochastic inversion transduction gram-mars and bilingual parsing of parallel corpora.
Com-putational Linguistics, 23(3):377?403.36
