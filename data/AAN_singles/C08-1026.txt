Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 201?208Manchester, August 2008Representations for category disambiguationMarkus DickinsonIndiana UniversityBloomington, IN 47405md7@indiana.eduAbstractAs it serves as a basis for POS tagging, cat-egory induction, and human category ac-quisition, we investigate the informationneeded to disambiguate a word in a lo-cal context, when using corpus categories.Specifically, we increase the recall of anerror detection method by abstracting theword to be disambiguated to a represen-tation containing information about someof its inherent properties, namely the setof categories it can potentially have.
Thiswork thus provides insights into the rela-tion of corpus categories to categories de-rived from local contexts.1 Introduction and MotivationCategory induction techniques generally rely onlocal contexts, i.e., surrounding words, to clusterword types together (e.g., Clark, 2003; Sch?utze,1995), using information of a kind also foundin human category acquisition tasks (e.g., Mintz,2002, 2003).
Such information is also at thecore of standard part-of-speech (POS) tagging, ordisambiguation, methods (see, e.g., Manning andSch?utze, 1999, ch.
10), with the contexts generallyabstracted to POS tags.
The contextual informa-tion is similar in both tasks because induction isfounded in part upon the notion that local contextsare useful for disambiguation: one morphosyntac-tically clusters words which should have the samecategory in the same contexts.
But which contextscount as being the ?same??
And to what extent docategories based on context distributions resemblec?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.corpus annotation categories?
Since disambigua-tion is in some sense more primary, to begin toanswer these questions we investigate which rep-resentations are effective for category disambigua-tion.Disambiguating a word?s category in contexthas of course been explored in other situations,especially POS tagging.
Rarely, however, has itbeen shown as to which information is the mostaccurate at disambiguation and which informationis absolutely necessary, without mixing these is-sues with other tagging issues, such as smoothingand unknown word tagging.
We need techniqueswhich isolate disambiguation, placing less empha-sis on generalizing contexts to new data.
To de-termine the essential information needed for accu-rate disambiguation, we start with a precise modeland generalize it.
Changing the model in smallways and evaluating the resulting precision will in-dicate how particular aspects of the representationare contributing to successful disambiguation.The central question of this paper is: whichrepresentation (of a word and its context) indi-cates that two situations should be categorized thesame?
In this context, POS annotation error detec-tion provides an ideal setting to explore represen-tations for disambiguation.
Error detection relieson the assumption that words should be annotatedconsistently?in other words, contexts are groupedwhich accurately identify the category of a word asbeing consistent?and it does this with an empha-sis on high precision.
In essence, error detectionalready investigates where disambiguation can bedone, often using local contexts (e.g., Dickinson,2005).
With an emphasis on high precision, how-ever, many corpus instances are essentially uncat-egorized and are thus in need of generalization.To get at the central question of an appropriate201representation for disambiguation, then, our task isto generalize error detection and increase the recallof errors found in a corpus by exploiting more gen-eral properties of a corpus.
Given that annotationerrors can have a profound impact on the qualityof training and testing on such data (see Dickin-son, 2005, ch.
1), this task also serves an immensepractical need in its own right.In exploring error detection recall, we can con-nect the task to another with much of the same em-phasis.
Human category acquisition experimentshave also focused on precision: instead of ask-ing how every word is categorized, they examinehow some words are categorized, from which oth-ers can be bootstrapped.
As outlined in sections 2and 3, we can use such studies as a starting pointfor generalizing error detection.2 Background2.1 The variation n-gram methodThe error detection method we build from is thevariation n-gram method (Dickinson and Meur-ers, 2003; Dickinson, 2005).
The approach de-tects items which occur multiple times in the cor-pus with varying annotation, the so-called varia-tion nuclei.
A nucleus with its repeated surround-ing context is referred to as a variation n-gram.Every detected variation in the annotation of a nu-cleus is classified as an error or a genuine ambi-guity using a basic heuristic requiring at least oneword of context on each side of the nucleus.For example, in the WSJ corpus, part of thePenn Treebank 3 release (Marcus et al, 1993), thestring in (1) is a variation 12-gram since off is avariation nucleus that is tagged preposition (IN)in one corpus occurrence and particle (RP) in an-other.1Dickinson (2005) shows that examiningthose cases with identical local context?in thiscase, looking at ward off a?results in an estimatederror detection precision of 92.5%.
(1) to ward off a hostile takeover attempt by twoEuropean shipping concernsThis method can be applied to syntactic annota-tion, and for this annotation, one can increase therecall of errors found by abstracting the nuclei toPOS tags (Boyd et al, 2007).
Clearly, this is nota feasible abstraction here, given that we are at-tempting to detect errors in POS annotation.1To distinguish variation nuclei, we shade them in grayand underline the immediately surrounding context.2.2 Frames for language acquisitionResearch on language acquisition has addressedthe question of how humans discover and learn cat-egories of words, using virtually the same contextsas in the variation n-gram method.
Mintz (2002)shows that local context, in the form of a frameof two words surrounding a target word, leads tocategorization in adults of the target, and Mintz(2003) shows that frequent frames supply categoryinformation, consistent across child language cor-pora.
A frame is defined as ?two jointly occurringwords with one word intervening?
(Mintz, 2003),e.g., you it.
The frame is not decomposed into itsleft side and right side (cf., e.g., Redington et al,1998; Clark, 2003) , but instead is taken as the oc-currence of both sides.
The target word is the in-tervening word, but it is not included in the frame(unlike variation nuclei).For category acquisition, only frequent framesare used, those with a frequency above a cer-tain threshold.
Frequent frames predict cate-gory membership: the set of words appearingin a given frame should represent a single cate-gory.
The frequent frame you it, for example,largely identifies verbs, as shown in (2), taken fromthe CHILDES database of child-directed speech(MacWhinney, 2000).
Analyzing the frequentframes in six subcorpora of CHILDES, Mintz(2003) obtains both high type and high token ac-curacy in grouping words into the same categories.
(2) a. you put itb.
you see itTo take this work as a basis for investigating dis-ambiguation, some points are in order about the re-sults.
First, accuracies slightly degrade when mov-ing from the ?Standard Labeling?
category set2tothe more fine-grained ?Expanded Labeling?
cate-gory set,3i.e., a .98 to .91 drop in token accuracyand .93 to .91 drop in type accuracy.
It is not clearwhat happens with even more fine-grained corpustagsets.
Secondly, Mintz (2003) assumes that, atleast for his experiments, each word has only oneclass (see also Redington et al, 1998, p. 439-440).The tasks of category induction and category dis-ambiguation are thus conflated into a single step.We do not know for sure whether frames induce2Categories = noun, verb, adjective, preposition, adverb,determiner, wh-word, not, conjunction, and interjection.3Nouns split into nouns and pronouns; verbs split intoverbs, auxiliaries, and copula202coherent sets of words or whether they accuratelydisambiguate a word, or both.
In other words, canframes be used to group the target words (induc-tion) or to group the contexts (disambiguation)?While we investigate using frames for disam-biguation in English (and somewhat in German),the concept of a frame has been shown to be cross-linguistically viable (Chemla et al, in press), andin principle could extend to languages encoding re-lations through morphology instead of linear order(see the discussion in Mintz, 2003).3 Generalizing error detection via framesBoth strands of research employ local contexts foridentifying categories, but the variation n-grammethod relies on identical words to serve as vari-ation nuclei, or target words to be disambiguated.To increase the recall of the method in a way relat-ing to acquisition, the nucleus should be abstractedto something more general than a word.
As a (fre-quent) frame does not include the target, predictingthat the category within that context is always thesame, a first step in abstracting the nucleus is torequire no similarity between nuclei.We thus search for all identical nuclei withframe context?or what we will call framed vari-ation nuclei?such that there is variation in label-ing for the nucleus, but we require no identity ofthe nucleus.
We investigate the WSJ portion of thePenn Treebank, and, to provide more robust evalu-ation, also compare the TIGER corpus of German,version 2 (Brants et al, 2002) where appropriate.Given that punctuation is less informative for de-termining a category, we remove from considera-tion frames containing punctuation as one of thecontext words, and obtain 48,717 variations in theWSJ and 22,613 in TIGER.Although basic hand-examination reveals someerrors, a majority of cases contain acceptable vari-ations.
As one example, in the WSJ the frame theof occurs as the most frequent frame with vari-ation in labeling for the target (5737 instances).This is a nominal position, and thus we find varia-tion between a variety of correct nominal tags: car-dinal number (CD), adjective (JJ, JJR, JJS), com-mon noun (NN, NNS), and proper noun (NNP,NNPS), in addition to the erroneous verbal tagsVBD (past tense verb) and VBG (verb, -ing form).Restricting our attention to the frequent frames, asin Mintz (2003), is not helpful: the problem occursirrespective of frequency.
Indeed, there is an aver-age of 2.56 categories per variation, with one vari-ation (and in) having 21 categories.
This is con-sistent in TIGER, which has 2.57 categories pervariation and 22 categories for und in.While more context could help, the real issue isthe definition of a nucleus.
In the example above,which nominal tag is used depends upon inher-ent properties of the word involved.
Consider theframe that the.
Among the 18 possible tags,there is variation between NN (common noun) forwords like afternoon and VBZ (present tense verb,3rd person singular) for words like says.
Both arelegitimate, and the primary way to tell is by exam-ining information about the target word.
In gen-eralizing the nucleus, instead of abstracting it tonothing, we need to abstract it to something indi-cating broad characteristics of the word.4 An appropriate level of abstractionOn the one hand, the variation n-gram method hashigh precision; on the other, using frames results inhigh recall, but too low a precision to sort through.Both methods rely on the same identical contexts;the issue is in finding which words are comparable.Consider the frame n?t that.
Some words are in-herently similar and should have the same tags: thecorrect n?t help/VB that and the erroneous n?t mat-ter/NN that, for instance, are comparable.
Othercases are not: one/CD and shown/VBN can neverhave the same category.
We need to find classesof words that, within the same context, should notvary in their annotation, and it makes sense to com-pare words in context if they have the same cate-gory possibilities.4.1 Complete ambiguity classesAmbiguity classes capture the relevant propertywe are interested in: words with the same cate-gory possibilities are grouped together.4And am-biguity classes have been shown to be success-fully employed, in a variety of ways, to improvePOS tagging (e.g., Cutting et al, 1992; Daelemanset al, 1996; Dickinson, 2007; Goldberg et al,2008; Tseng et al, 2005).
Only certain words cantake one of two (or more) tags, and these should bedisambiguated in the same way in context.
As anexample of how using ambiguity classes as varia-tion nuclei can increase recall, consider the framebeing by in example (3).
There are at least 274One could group affixes by ambiguity class for languageslike Chinese (cf.
CTBMorph features in Tseng et al, 2005).203different VBN (past participle) verbs appearing be-tween being and by (3a), but none of these verbsever appear as VBD here, even though all of themcould be VBD.
Two other VBD/VBN verbs, re-jected (3b) and played (3c), erroneously appear asVBD here, but never as VBN.
With the nucleusVBD/VBN, we can find this erroneous variation.
(3) a. being { raised/VBN , infringed/VBN ,supported/VBN , ... } byb.
as probable as being rejected/VBD bythe Book-of-the-Month Clubc.
the ... role in takeover financing beingplayed/VBD by Japanese banksThus, to define complete ambiguity class varia-tion nuclei, we make a first pass through the cor-pus to calculate every word?s ambiguity class.
Ona second pass, the ambiguity class serves as the(framed) variation nucleus, e.g., being VBD/VBNby.
Ambiguity class nuclei with more than one tagin a frame context are flagged as a potential error.4.2 Pairwise ambiguity classesWhile abstracting to a word?s possible classes canincrease the number of errors found, potentiallyerroneous classes prevent further increased recall.For example, the class for plans is erroneouslyclassified as NNS/VBP/VBZ, even though its oneinstance of VBP (present tense verb, non-3rd per-son singular) in the corpus is erroneous.
Withoutthat case, we would have NNS/VBZ and more di-rectly comparable words.As a second experiment, then, we define pair-wise ambiguity class variation nuclei, using sub-sets of ambiguity classes to define a nucleus.
If thevariation is only between NNS and VBZ, we needto allow all words with NNS/VBZ variation tocount as comparable nuclei.
As above, we calcu-lated a word?s ambiguity class during a first pass.In the second pass through the corpus, we breakthe ambiguity class down into its pairs, and eachrelevant pair is stored as a variation nucleus.
Therelevant pairs of tags are those which contain thetag at that position since classes without that tagcan never have meaningful variation.
Taking theexample of company plans to, with the ambiguityclass NNS/VBP/VBZ for plans, if the current cor-pus position marks plans as NNS, then we storethe two trigrams in (4).
(4) a. company NNS/VBZ tob.
company NNS/VBP toLooking over the whole corpus, we find vari-ation between NNS and VBZ, but none betweenNNS and VBP.
In principle, this instance ofplans/NNS could be in both an NNS/VBZ and anNNS/VBP variation; this is necessary since we donot a priori know which variations will be prob-lematic.5 Results and Insights5.1 Complete ambiguity classesUsing complete ambiguity class variation nuclei,we find 4131 framed variation nuclei in the WSJ.Almost all variations involve only two or threetags, with 2.03 tags per variation.
TIGER has 626framed variation nuclei, with 2.01 tags per varia-tion.From the 4131 variations, we randomly sampled100 cases and hand-evaluated whether they containan error, and whether its detection is attributable tothe generalization to complete ambiguity classes.Of the 100, 79 of the cases contain at least oneerror, and 15 of these cases are new examples,i.e., cases without identical words.
With a pointestimate of .79, we estimate 3263 errors and ob-tain a 95% confidence interval of (0.7102, 0.8698),meaning that we predict between 2933 and 3593of the 4131 cases contain errors.
The 79 erroneouscases point to 134 token errors, of which 23 arenew.In addition to increasing the recall of themethod, the cases are arguably more thoroughlygrouped than before.
For instance, we see in (5)that both pretax and third-quarter vary between JJand NN in the variation said JJ/NN profit, withfirst-half additionally appearing only as JJ.
SinceJJ is the correct tag for all instances, the two NNerrors are detected with word nuclei, but here allthe relevant examples are together.
This providesevidence for the claim that an ambiguity class isa level of abstraction supporting identical disam-biguation in the same context.
(5) said { first-half/JJ , third-quarter/JJ ,pretax/JJ , third-quarter/NN , pretax/NN }profitThe recall has increased, but 79% is below the92.5% precision previously obtained for the varia-tion n-gram method with word nuclei (Dickinson,2005).
However, that result used distinct variation204nuclei, meaning that the longest contexts were ex-amined before working down to shorter contexts.Furthermore, it is not clear how well the originalword nuclei method scales up to larger corpora.Some of the new false positives we observe wouldlikely be false positives for word nuclei, givenmore data.
For example, the new method turnsup generally VBD/VBN the as a false positive, asin (6), because of the non-local tagset distinctionand short context.
With more data, we are morelikely to see an acceptable use of, e.g., generallyfavored/VBD the, a false positive for word nuclei.In some sense, then, this 79% precision might be amore general indication of the method?s precisionfor this tagset and genre.
(6) a. TV news coverage has generallyfavored/VBN the governmentb.
Members ... generally received/VBD theregional officialsFinally, of the 21 false positives (20 of which arenew), five of them stem from an error in the ambi-guity class, corresponding to five token errors.
Forexample, there is variation for JJ/NN words in theframe of pills, as in (7).
However, poison shouldnever be JJ: its ambiguity class should be NN, notthe incorrect JJ/NN.
For error detection, this means84 of the 100 samples lead to some kind of POSerror; for investigating disambiguation contexts,this means that 83% (79/95) of the cases supportcomplete disambiguation.
Thus, when abstractingto ambiguity class nuclei, local context generallyprovides sufficient information for disambiguation(see also section 6).
(7) of { birth-control/JJ , poison/NN } pillsOne limitation of the variation n-grammethod isthe fact that some distinctions often need non-localinformation (cf.
(6)).
A bigger problem for group-ing words by ambiguity classes is the fact that an-notation can be semantically-based.
For example,the variation of JJ/NN bank is a legitimate ambi-guity because the distinction between JJ and NNis semantic.
Compare a sort of merchant/NN bankwith an extension of senior/JJ bank debt: both nu-clei are clearly in a noun modifier position, butthe tags are different based on what they denote.This shows the limitations of local distributionalinformation without lexical information, for mak-ing these tagset distinctions.5.2 Pairwise ambiguity classesWith pairwise ambiguity classes serving as varia-tion nuclei, we find 6235 variation frames in theWSJ and 874 in TIGER, significant increases overusing complete ambiguity class nuclei.
To evalu-ate the method, we want to know: a) how manytotal errors we detect, b) how many of these weredetected by using either complete or pairwise am-biguity classes, and c) how many were detectedspecifically with pairwise ambiguity classes.A sample of 100 of the WSJ cases reveals (a)59 total errors, (b) 18 of which involve ambigu-ity class nuclei that would not have been foundwith word nuclei.
Of these 18, (c) 8 cases canonly be found by extending the method to pairwiseclasses.
For the point estimate of .59, we estimateapproximately 3679 variations to be errors (95%CI: 3078 to 4280 errors).
The 59 erroneous varia-tions point to a total of (a) 134 token errors, (b) 30of which were detected by ambiguity classes; (c)17 of these were detected by pairwise ambiguityclasses.
Clearly, using pairwise ambiguity classesincreases the number of errors found.As an example, consider (8), centering on theframe came for.
The original variation n-grammethod turns up no variation here, but neither doesthe complete ambiguity class extension: in has theambiguity class FW/IN/NN/RB/RBR/RP, and outthe class IN/JJ/NN/RB/RP.
Since the only relevantvariation is between IN and RP, the pairwise nucleimethod turns up such cases with the variation cameIN/RP for, pointing to an error in the two cases ofout.
(8) a. accounts came in/RP for some blocksb.
numbers came out/IN for Septemberc.
he again came out/IN for an amendmentBut what of the 41 false positives, 22 of whichare due to the pairwise classes?
We have increasedrecall, but there is also a 20% absolute drop in pre-cision.
Is this tradeoff worth it?
To answer this,it is important to note that 15 of the false posi-tives are due to faulty ambiguity classes, as dis-cussed above, and 10 of those 15 are from pair-wise classes.
For error detection, this means 74 ofthe 100 samples lead to some POS error; for inves-tigating disambiguation contexts, this means 69%(59/85) of the cases support disambiguation.Additionally, the 15 cases point to 53 tokenerrors, much more than in the previous experi-ment, due to 44 token errors from the new pair-205wise ambiguity classes.
For example, in the varia-tion frame as DT/JJ sales, the words which varyare a (tagged DT (determiner), with a completeambiguity class of DT/FW/IN/JJ/LS/NNP/SYM)and many (tagged JJ, with an ambiguity classof DT/JJ/NNS/PDT/RB/VB).
Unsurprisingly, ashould never have been tagged JJ in the corpus,i.e., its ambiguity class is wrong.In addition to the issue of erroneous tags in anambiguity class, atypical tags also pose a prob-lem.
Consider the frame that JJ/RB in, as illus-trated in (9), with acceptable variation.
It mightappear that sometime has a problem with its ambi-guity class, but the use of JJ is actually correct, asshown in (10), where sometime is atypically mod-ifying a noun.
To counter atypical uses, one coulduse only ?typical?
ambiguity classes (cf.
Dickin-son, 2007) or define ambiguity classes accordingto order of frequency (cf.
Daelemans et al, 1996),e.g., JJ/RB vs.
RB/JJ.
(9) a. a departure from the past that many/JJ inthe industry ...b. hope that sometime/RB in the near fu-ture(10) real estate magnate and sometime/JJ raiderDonald TrumpThis illustrates that the selection of an abstractedclass for a nucleus definition is non-trivial, and am-biguity classes are simply an approximation.POS contexts One problem for our method isthat word contexts are not always truly compara-ble; identical context words can be used differ-ently.
For instance, with the variation that NN/VBPalong in (11), the uses of that are clearly distinctand are marked as such by their tags.
(11) a. gifts that/WDT go/VBP along with pur-chasesb.
We are considering that/DT offer/NNalong with all other alternativesBut do tagset categories actually aid in local dis-ambiguation?
To quickly gauge this, we take theprevious sample of 100 variations and recover thePOS information for the context.
Isolating thosecases with non-identical POS tags for the sameword contexts, we find 10 examples and hypothe-size that these will more likely be acceptable varia-tions.
Interestingly, however, of those ten, six suc-cessfully identified errors; it turns out that the POSof the word is often irrelevant for disambiguation.For the variation paid JJR/RBR than in (12), forexample, the tag of the context word paid is differ-ent in these cases, but that does not matter for thetag of more, which should be consistent.
(12) a. they paid/VBD more/JJR than $ 1 mil-lionb.
he has paid/VBN more/RBR than $70,000More problematically, four of the erroneousvariation nuclei also contained POS errors in thecontext, as in example (13).
The variation allCC/RB disappeared points to an error in the wordbut, yet there is also a noticeable inconsistency inthe word all.
(13) a. have all/DT but/CC disappearedb.
have all/RB but/RB disappearedIn other words, it is often the case that weshould ignore the POS of the context words, dueto the fact that erroneous contexts exist and, moreimportantly, that not all categories aid in disam-biguation.
Exploring which contextual categoriesaid in target category disambiguation (cf., e.g.,Brants, 1997) could aid in developing better dis-ambiguation models, and perhaps also a bettersense of what categories are useful to induce (e.g.,a broader category Verb in (12) for paid).6 Representations for disambiguationWe have shown that local lexical context providesa generally unambiguous context for corpus tags,given sufficient information about the word to bedisambiguated.
The information need not be veryabstract, either: frames using ambiguity class nu-clei only require a word?s category possibilities.Even for many unsupervised situations, this isavailable from a lexicon (e.g., Banko and Moore,2004; Goldberg et al, 2008).We have only looked at cases with variation intagging; fully gauging the accuracy of such a datarepresentation for disambiguation requires moreof the framed nuclei from the corpus, includingthose without variation.
For this, we could takeall framed nuclei from a corpus and compare thelevel of ambiguity for differing abstractions.
How-ever, most framed nuclei occur only once, and itis not clear how meaningful it is to say that theseare unambiguous.
Thus, we examine framed nu-clei which occur at least twice and report in table 1206for the WSJ how unambiguous a particular level ofnucleus abstraction is.5Abstraction Unamb.
Total AccuracyWord 84,784 87,390 97.02%Complete AC 90,341 94,472 95.63%No info.
51,945 100,662 51.60%Table 1: Disambiguation accuracy for the WSJWhile abstracting to the case where the nucleuscontains no information (No info.)
creates morecases which are classifiable?over 100,000?theaccuracy of disambiguation drops from the up-per 90% range to 52%.
Note, however, that theabstraction to complete ambiguity class (AC) nu-clei has minimal degradation in accuracy, yet in-creases the number of accurately classified cases.When we recall that approximately 79% of of the4131 variation frames should have a single tag,i.e., 3263 cases, this means that the overall dis-ambiguation accuracy is estimated to be 99.08%(93,604/94,472).In addition to the disambiguation accuracy offrames, we can look at the accuracy of word tokensidentified by frames.
To gauge this, we identify themost likely tag of each framed variation nucleusand assign it to all instances of the nucleus.
In thecase of ties, one tag is randomly selected; since weare only calculating overall word token accuracy,the exact tag selected is unimportant.
The resultsof comparing to the benchmark tags are given intable 2.
Even though the abstraction to no infor-mation identifies more word tokens, the ambigu-ity class abstraction correctly categorizes nearly asmany words.Abstraction Correct Total AccuracyWord 340,860 345,139 98.76%Complete AC 441,603 448,402 98.74%No info.
444,635 582,601 76.32%Table 2: Word token accuracy for the WSJWith the smaller and likely more accuratelytagged TIGER corpus, we find exactly the sametrends, as shown in table 3.
This supports theclaim across corpora that local context is often suf-ficient to disambiguate a word, if some informationfrom the word?here, the category possibilities?is present in the nucleus.5As pairwise ambiguity classes involve more than one nu-cleus per corpus position, we use complete ambiguity classes.Abstraction Unamb.
Total AccuracyWord 37,038 37,324 99.23%Complete AC 47,832 48,458 98.71%No info.
33,881 56,494 59.97%Table 3: Disambiguation accuracy for TIGERThe poor accuracy for framed nuclei with noinformation indicates that methods which intendto match corpus annotation categories could facedifficulties in obtaining a single category with-out using more information.
There is still muchspace to explore, however, between using ambi-guity class nuclei and no information, in order tofurther increase the number of comparable caseswithout losing accuracy and in order to be moreknowledge-free.7 Summary and OutlookMotivated by work on category acquisition, wehave shown that local contexts?i.e., immediatelysurrounding words, or frames?can delineate cor-pus categories when the level of abstraction for theword to be disambiguated indicates some inherentproperties of the word, namely the categories it canhave.
By abstracting away from lexical items tobroader classes of words, we have been able to in-crease the recall of an error detection method with-out much drop in its precision.Having successfully defined a representation fordisambiguation, the next step is to make the rep-resentation more general, in order to include morecomparable instances.
As what we have done is es-sentially a form of nearest neighbor classification,one could in the future explore more sophisticatedtechniques to cluster contexts.At the same time, we wish to use as littleannotated knowledge as possible.
Thus, an or-thogonal line of research can involve inducingclasses for words which are more general than sin-gle categories, i.e., something akin to ambiguityclasses (see, e.g., the discussion of ambiguity classguessers in Goldberg et al, 2008).
This couldmake error detection completely independent ofthe annotation and, more importantly, lead to animproved understanding of the best knowledge-free representation for disambiguation.Since induction is founded to some extent upondisambiguating contexts, this work has some bear-ing on the evaluation of induced categories withcorpus annotation; not only is there more than207one tagset in existence (see discussion in Clark,2003), but annotation schemes make distinctionsthat morphosyntactic contexts cannot readily cap-ture.
For example, there is an implicit notion of in-herency in the distinction between JJ and NN in thePenn Treebank (Santorini, 1990, p. 12-13).
Fullyoutlining these inherent properties could provideinsights into induction and its evaluation.AcknowledgmentsThanks to the three anonymous reviewers for theiruseful comments and to Charles Jochim for helpfuldiscussion.
This material is based upon work sup-ported by the National Science Foundation underGrant No.
IIS-0623837.ReferencesBanko, Michele and Robert C. Moore (2004).
Part-of-Speech Tagging in Context.
In Proceed-ings of COLING 2004.
Geneva, Switzerland, pp.556?561.Boyd, Adriane, Markus Dickinson and DetmarMeurers (2007).
Increasing the Recall of Cor-pus Annotation Error Detection.
In Proceedingsof TLT 2007.
Bergen, Norway, pp.
19?30.Brants, Sabine, Stefanie Dipper, Silvia Hansen,Wolfgang Lezius and George Smith (2002).
TheTIGER Treebank.
In Proceedings of TLT-02.Sozopol, Bulgaria.Brants, Thorsten (1997).
Internal and ExternalTagsets in Part-of-Speech Tagging.
In Proceed-ings of Eurospeech.
Rhodes, Greece.Chemla, E., T. H. Mintz, S. Bernal andA.
Christophe (in press).
Categorizing wordsusing ?Frequent Frames?
: What cross-linguisicanalyses reveal about core principles.
Develop-mental Science .Clark, Alexander (2003).
Combining Distribu-tional and Morphological Information for Partof Speech Induction.
In Proceedings of EACL-03.
Budapest, pp.
59?66.Cutting, Doug, Julian Kupiec, Jan Pedersen andPenelope Sibun (1992).
A Practical part-of-speech tagger.
In Proceedings of the ANLP-92.Trento, Italy, pp.
133?140.Daelemans, Walter, Jakub Zavrel, Peter Berck andSteven Gillis (1996).
MBT: A Memory-BasedPart of Speech Tagger-Generator.
In Proceed-ings of the Fourth Workshop on Very Large Cor-pora (VLC).
Copenhagen, pp.
14?27.Dickinson, Markus (2005).
Error detection andcorrection in annotated corpora.
Ph.D. thesis,The Ohio State University.Dickinson, Markus (2007).
Determining Ambigu-ity Classes for Part-of-Speech Tagging.
In Pro-ceedings of RANLP-07.
Borovets, Bulgaria.Dickinson, Markus and W. Detmar Meurers(2003).
Detecting Errors in Part-of-Speech An-notation.
In Proceedings of EACL-03.
Budapest,pp.
107?114.Goldberg, Yoav, Meni Adler and Michael Elhadad(2008).
EM Can Find Pretty Good HMM POS-Taggers (When Given a Good Start).
In Pro-ceedings of ACL-08.
Columbus, OH, pp.
746?754.MacWhinney, Brian (2000).
The CHILDESproject: Tools for analyzing talk.
Mahwah, NJ:Lawrence Erlbaum Associates, third edn.Manning, Christopher D. and Hinrich Sch?utze(1999).
Foundations of Statistical Natural Lan-guage Processing.
Cambridge, MA: The MITPress.Marcus, M., Beatrice Santorini and M. A.Marcinkiewicz (1993).
Building a large anno-tated corpus of English: The Penn Treebank.Computational Linguistics 19(2), 313?330.Mintz, Toben H. (2002).
Category inductionfrom distributional cues in an artificial language.Memory & Cognition 30, 678?686.Mintz, Toben H. (2003).
Frequent frames as acue for grammatical categories in child directedspeech.
Cognition 90, 91?117.Redington, Martin, Nick Chater and Steven Finch(1998).
Distributional Information: A PowerfulCue for Acquiring Syntactic Categories.
Cogni-tive Science 22(4), 425?469.Santorini, Beatrice (1990).
Part-Of-Speech Tag-ging Guidelines for the Penn Treebank Project(3rd Revision, 2nd printing).
Tech.
Rep. MS-CIS-90-47, The University of Pennsylvania,Philadelphia, PA.Sch?utze, Hinrich (1995).
Distributional Part-of-Speech Tagging.
In Proceedings of EACL-95.Dublin, Ireland, pp.
141?148.Tseng, Huihsin, Daniel Jurafsky and ChristopherManning (2005).
Morphological features helpPOS tagging of unknown words across languagevarieties.
In Proceedings of the Fourth SIGHANWorkshop on Chinese Language Processing.208
