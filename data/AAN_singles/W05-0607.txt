Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),pages 48?55, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsA Bayesian mixture model for term re-occurrence and burstinessAvik Sarkar1, Paul H Garthwaite2, Anne De Roeck11 Department of Computing, 2 Department of StatisticsThe Open UniversityMilton Keynes, MK7 6AA, UK{a.sarkar, p.h.garthwaite, a.deroeck}@open.ac.ukAbstractThis paper proposes a model for term re-occurrence in a text collection based onthe gaps between successive occurrencesof a term.
These gaps are modeled usinga mixture of exponential distributions.
Pa-rameter estimation is based on a Bayesianframework that allows us to fit a flexi-ble model.
The model provides measuresof a term?s re-occurrence rate and within-document burstiness.
The model worksfor all kinds of terms, be it rare contentword, medium frequency term or frequentfunction word.
A measure is proposed toaccount for the term?s importance basedon its distribution pattern in the corpus.1 IntroductionTraditionally, Information Retrieval (IR) and Statis-tical Natural Language Processing (NLP) applica-tions have been based on the ?bag of words?
model.This model assumes term independence and homo-geneity of the text and document under considera-tion, i.e.
the terms in a document are all assumedto be distributed homogeneously.
This immediatelyleads to the Vector Space representation of text.
Theimmense popularity of this model is due to the easewith which mathematical and statistical techniquescan be applied to it.The model assumes that once a term occurs in adocument, its overall frequency in the entire doc-ument is the only useful measure that associates aterm with a document.
It does not take into consid-eration whether the term occurred in the beginning,middle or end of the document.
Neither does it con-sider whether the term occurs many times in closesuccession or whether it occurs uniformly through-out the document.
It also assumes that additionalpositional information does not provide any extraleverage to the performance of the NLP and IR ap-plications based on it.
This assumption has beenshown to be wrong in certain applications (Franz,1997).Existing models for term distribution are based onthe above assumption, so they can merely estimatethe term?s frequency in a document or a term?s top-ical behavior for a content term.
The occurrence ofa content word is classified as topical or non-topicalbased on whether it occurs once or many times inthe document (Katz, 1996).
We are not aware of anyexisting model that makes less stringent assumptionsand models the distribution of occurrences of a term.In this paper we describe a model for term re-occurrence in text based on the gaps between succes-sive occurrences of the term and the position of itsfirst occurrence in a document.
The gaps are mod-eled by a mixture of exponential distributions.
Non-occurrence of a term in a document is modeled bythe statistical concept of censoring, which states thatthe event of observing a certain term is censored atthe end of the document, i.e.
the document length.The modeling is done in a Bayesian framework.The organization of the paper is as follows.
Insection 2 we discuss existing term distribution mod-els, the issue of burstiness and some other work thatdemonstrates the failure of the ?bag of words?
as-48sumption.
In section 3 we describe our mixturemodel, the issue of censoring and the Bayesian for-mulation of the model.
Section 4 describes theBayesian estimation theory and methodology.
Insection 5 we talk about ways of drawing infer-ences from our model, present parameter estimateson some chosen terms and present case studies for afew selected terms.
We discuss our conclusions andsuggest directions for future work in section 6.2 Existing Work2.1 ModelsPrevious attempts to model a term?s distribution pat-tern have been based on the Poisson distribution.
Ifthe number of occurrences of a term in a documentis denoted by k, then the model assumes:p(k) = e??
?kk!for k = 0, 1, 2, .
.
.
Estimates based on this modelare good for non-content, non-informative terms, butnot for the more informative content terms (Manningand Schu?tze, 1999).The two-Poisson model is suggested as a variationof the Poisson distribution (Bookstein and Swanson,1974; Church and Gale, 1995b).
This model as-sumes that there are two classes of documents as-sociated with a term, one class with a low averagenumber of occurrences and the other with a high av-erage number of occurrences.p(k) = ?e?
?1?k1k!+ (1 ?
?)e?
?2?k2k!,where ?
and (1 ?
?)
denote the probabilities of adocument in each of these classes.
Often this modelunder-estimates the probability that a term will oc-cur exactly twice in a document.2.2 BurstinessBurstiness is a phenomenon of content words,whereby they are likely to occur again in a text af-ter they have occurred once.
Katz (1996) describeswithin-document burstiness as the close proximity ofall or some individual instances of a word within adocument exhibiting multiple occurrences.He proposes a model for within-document bursti-ness with three parameters as:?
the probability that a term occurs in a documentat all (document frequency)?
the probability that it will occur a second timein a document given that it has occurred once?
the probability that it will occur another time,given that it has already occurred k times(where k > 1).The drawbacks of this model are: (a) it cannot han-dle non-occurrence of a term in a document; (b) themodel can handle only content terms, and is not suit-able for high frequency function words or mediumfrequency terms; and (c) the rate of re-occurrence ofthe term or the length of gaps cannot be accountedfor.
We overcome these drawbacks in our model.A measure of burstiness was proposed as a binaryvalue that is based on the magnitude of average-termfrequency of the term in the corpus (Kwok, 1996).This measure takes the value 1 (bursty term) if theaverage-term frequency value is large and 0 other-wise.
The measure is too naive and incomplete toaccount for term burstiness.2.3 Homogeneity AssumptionThe popular ?bag of words?
assumption for textstates that a term?s occurrence is uniform and ho-mogeneous throughout.
A measure of homogeneityor self-similarity of a corpus can be calculated, bydividing the corpus into two frequency lists basedon the term frequency and then calculating the ?2statistic between them (Kilgarriff, 1997).
Variousschemes for dividing the corpus were used (DeRoeck et al, 2004a) to detect homogeneity of termsat document level, within-document level and bychoosing text chunks of various sizes.
Their workrevealed that homogeneity increases by nullifyingthe within document term distribution pattern andhomogeneity decreases when chunks of larger sizeare chosen as it incorporates more document struc-ture in it.
Other work based on the same method-ology (De Roeck et al, 2004b) reveals that evenvery frequent function words do not distribute ho-mogeneously over a corpus or document.
These (DeRoeck et al, 2004a; De Roeck et al, 2004b) provideevidence of the fact that the ?bag of words?
assump-tion is invalid.
Thus it sets the platform for a model49that defies the independence assumption and consid-ers the term distribution pattern in a document andcorpus.3 Modeling3.1 Terminology and NotationWe build a single model for a particular term in agiven corpus.
Let us suppose the term under consid-eration is x as shown in Figure 1.
We describe thenotation for a particular document, i in the corpus.Figure 1: The document structure and the gaps be-tween terms?
didenotes the number of words in document i(i.e.
the document length).?
nidenotes the number of occurrences of termx in document i.?
wi1denotes the position of the first occurrenceof term x in document i.?
wi2, .
.
.
, winidenotes the successive gaps be-tween occurrences of term x in document i.?
wini+1denotes the gap for the next occurrenceof x, somewhere after the document ends.?
ceniis the value at which observation wini+1is censored, as explained in section 3.2.2.3.2 The ModelWe suppose we are looking through a document,noting when the term of interest occurs.
Our modelassumes that the term occurs at some low underly-ing base rate 1/?1but, after the term has occurred,then the probability of it occurring soon afterwardsis increased to some higher rate 1/?2.
Specifically,the rate of re-occurrence is modeled by a mixture oftwo exponential distributions.
Each of the exponen-tial components is described as follows:?
The exponential component with larger mean(average), 1/?1, determines the rate with whichthe particular term will occur if it has not oc-curred before or it has not occurred recently.?
The second component with smaller mean(average), 1/?2, determines the rate of re-occurrence in a document or text chunk giventhat it has already occurred recently.
This com-ponent captures the bursty nature of the term inthe text (or document) i.e.
the within-documentburstiness.The mixture model is described as follows:?
(wij) = p?1e?
?1wij + (1 ?
p)?2e?
?2wijfor j ?
{2, .
.
.
, ni}.
p and (1 ?
p) denote respec-tively, the probabilities of membership for the firstand the second exponential distribution.There are a few boundary conditions that themodel is expected to handle.
We take each of thesecases and discuss them briefly:3.2.1 First occurrenceThe model treats the first occurrence of a term dif-ferently from the other gaps.
The second exponen-tial component measuring burstiness does not fea-ture in it.
Hence the distribution is:?1(wi1) = ?1e?
?1wi13.2.2 CensoringHere we discuss the modeling of two cases thatrequire special attention, corresponding to gaps thathave a minimum length but whose actual length isunknown.
These cases are:?
The last occurrence of a term in a document.?
The term does not occur in a document at all.We follow a standard technique from clinical tri-als, where a patient is observed for a certain amountof time and the observation of the study is expectedin that time period (the observation might be thetime until death, for example).
In some cases it hap-pens that the observation for a patient does not occurin that time period.
In such a case it is assumed thatthe observation would occur at sometime in the fu-ture.
This is called censoring at a certain point.50In our case, we assume the particular term wouldeventually occur, but the document has ended beforeit occurs so we do not observe it.
In our notation weobserve the term nitimes, so the (ni+ 1)th time theterm occurs is after the end of the document.
Hencethe distribution of wini+1is censored at length ceni.If ceniis small, so that the nthioccurrence of theterm is near the end of the document, then it is notsurprising that wini+1is censored.
In contrast if ceniis large, so the nthioccurrence is far from the endof the document, then either it is surprising that theterm did not re-occur, or it suggests the term is rare.The information about the model parameters that isgiven by the censored occurrence is,Pr(wini+1> ceni) =??ceni?
(x)dx= pe?
?1ceni + (1 ?
p)e?
?2ceni ; where,ceni= di?ni?j=1wijAlso when a particular term does not occur in adocument, our model assumes that the term wouldeventually occur had the document continued indef-initely.
In this case the first occurrence is censoredand censoring takes place at the document length.
Ifa term does not occur in a long document, it suggeststhe term is rare.3.3 Bayesian formulationOur modeling is based on a Bayesian approach (Gel-man et al, 1995).
The Bayesian approach differsfrom the traditional frequentist approach.
In the fre-quentist approach it is assumed that the parametersof a distribution are constant and the data varies.In the Bayesian approach one can assign distrib-utions to the parameters in a model.
We choosenon-informative priors, as is common practice inBayesian applications.
So we put,p ?
Uniform(0, 1), and?1?
Uniform(0, 1)To tell the model that ?2is the larger of the two ?s,we put ?2= ?1+ ?, where ?
> 0, and?
?
Uniform(0, 1)Also cenidepends on the document length diandthe number of occurrences of the term in that doc-ument, ni.
Fitting mixture techniques is tricky andFigure 2: Bayesian dependencies between the para-metersrequires special methods.
We use data augmenta-tion to make it feasible to fit the model using GibbsSampling (section 4.2).
For details about this, seeRobert (1996) who describes in detail the fitting ofmixture models in MCMC methods (section 4.2).4 Parameter Estimation4.1 Bayesian EstimationIn the Bayesian approach of parameter estimation,the parameters are uncertain, and it is assumed thatthey follow some distribution.
In our case the para-meters and the data are defined as:~?
= {p, ?1, ?2} denote the parameters of the model.~W = {wi1, .
.
.
, wini, wini+1} denotes the data.Hence based on this we may define the following:?
f(~?)
is the prior distribution of ~?
as assignedin section 3.3.
It summarizes everything weknow about ~?
apart from the data ~W .?
f( ~W |~?)
is the likelihood function.
It is ourmodel for the data ~W conditional on the para-meters ~?.
(As well as the observed data, thelikelihood also conveys the information givenby the censored values)?
f(~?| ~W ) is the posterior distribution of ~?,given ~W .
It describes our beliefs about the pa-rameters given the information we have.51Deriving the density function for a parameter set ~?after observing data ~W , can be achieved by usingBayes Theorem as:f(~?| ~W ) =f( ~W |~?)f(~?
)f( ~W )(1)where f( ~W ) is simply a normalizing constant, inde-pendent of ~?.
It can be computed in terms of thelikelihood and prior as:f( ~W ) =?f( ~W |~?)f(~?
)d~?Hence equation 1 is reduced to:f(~?| ~W ) ?
f( ~W |~?)f(~?
)So, once we have specified the posterior densityfunction f(~?| ~W ), we can obtain the estimates of theparameters ~?
by simply averaging the values gener-ated by f(~?| ~W ).4.2 Gibbs SamplingThe density function of ?i, f(?i|~W ) can be ob-tained by integrating f(~?| ~W ) over the remainingparameters of ~?.
But in many cases, as in ours, it isimpossible to find a closed form solution of f(?i).In such cases we may use a simulation processbased on random numbers, Markov Chain MonteCarlo (MCMC) (Gilks et al, 1996).
By generatinga large sample of observations from the joint distri-bution f(~?, ~W ), the integrals of the complex dis-tributions can be approximated from the generateddata.
The values are generated based on the Markovchain assumption, which states that the next gener-ated value only depends on the present value anddoes not depend on the values previous to it.
Basedon mild regularity conditions, the chain will gradu-ally forget its initial starting point and will eventu-ally converge to a unique stationary distribution.Gibbs Sampling (Gilks et al, 1996) is a popularmethod used for MCMC analysis.
It provides an ele-gant way for sampling from the joint distributions ofmultiple variables: sample repeatedly from the dis-tributions of one-dimensional conditionals given thecurrent observations.
Initial random values are as-signed to each of the parameters.
And then these val-ues are updated iteratively based on the joint distri-bution, until the values settle down and converge toa stationary distribution.
The values generated fromthe start to the point where the chain settles down arediscarded and are called the burn-in values.
The pa-rameter estimates are based on the values generatedthereafter.5 ResultsParameter estimation was carried out using Gibb?sSampling on the WinBUGS software (Spiegelhalteret al, 2003).
Values from the first 1000 iterationwere discarded as burn-in.
It had been observed thatin most cases the chain reached the stationary distri-bution well within 1000 iterations.
A further 5000 it-erations were run to obtain the parameter estimates.5.1 Interpretation of ParametersThe parameters of the model can be interpreted inthe following manner:??
?1= 1/?1is the mean of an exponential dis-tribution with parameter ?1.?
?1measures therate at which this term is expected in a runningtext corpus.
?
?1determines the rarity of a termin a corpus, as it is the average gap at whichthe term occurs if it has not occurred recently.Thus, a large value of ?
?1tells us that the termis very rare in the corpus and vice-versa.?
Similarly, ?
?2measures the within-documentburstiness, i.e.
the rate of occurrence of a termgiven that it has occurred recently.
It measuresthe term re-occurrence rate in a burst withina document.
Small values of ?
?2indicate thebursty nature of the term.?
p?
and 1 ?
p?
denote, respectively, the probabil-ities of the term occurring with rate?
?1and ?
?2in the entire corpus.Table 1 presents some heuristics for drawing in-ference based on the values of the parameter esti-mates.5.2 DataWe choose for evaluation, terms from the Associ-ated Press (AP) newswire articles, as this is a stan-dard corpus for language research.
We picked termswhich had been used previously in the literature(Church and Gale, 1995a; Church, 2000; Manning52?1small ?1large?2small frequently occur-ring and commonfunction wordtopical contentword occurring inbursts?2large comparativelyfrequent but well-spaced functionwordinfrequent and scat-tered function wordTable 1: Heuristics for inference, based on the para-meter estimates.and Schu?tze, 1999; Umemura and Church, 2000)with respect to modeling different distribution, so asto present a comparative picture.
For building themodel we randomly selected 1% of the documentsfrom the corpus, as the software (Spiegelhalter et al,2003) we used is Windows PC based and could nothandle enormous volume of data with our availablehardware resources.
As stated earlier, our model canhandle both frequent function terms and rare contentterms.
We chose terms suitable for demonstratingthis.
We also used some medium frequency terms todemonstrate their characteristics.5.3 Parameter estimatesTable 2 shows the parameter estimates for the cho-sen terms.
The table does not show the values of1 ?
p?
as they can be obtained from the value of p?.
Ithas been observed that the value??1/?
?2is a good in-dicator of the nature of terms, hence the rows in thetable containing terms are sorted on the basis of thatvalue.
The table is divided into three parts.
The toppart contains very frequent (function) words.
Thesecond part contains terms in the medium frequencyrange.
And the bottom part contains rarely occurringand content terms.5.4 DiscussionThe top part of the table consists of the very fre-quently occurring function words occurring fre-quently throughout the corpus.
These statements aresupported by the low values of ?
?1and ??2.
Thesevalues are quite close, indicating that the occurrenceof these terms shows low burstiness in a running textchunk.
This supports our heuristics about the valueof ??1/?
?2, which is small for such terms.
Moder-ate, not very high values of p?
also support this state-ment, as the term is then quite likely to be gener-Term p ?1?2?1/?2the 0.82 16.54 16.08 1.03and 0.46 46.86 45.19 1.04of 0.58 38.85 37.22 1.04except 0.67 21551.72 8496.18 2.54follows 0.56 80000.00 30330.60 2.64yet 0.51 10789.81 3846.15 2.81he 0.51 296.12 48.22 6.14said 0.03 895.26 69.06 12.96government 0.60 1975.50 134.34 14.71somewhat 0.84 75244.54 4349.72 17.30federal 0.84 2334.27 102.57 22.76here 0.94 3442.34 110.63 31.12she 0.73 1696.35 41.41 40.97george 0.88 17379.21 323.73 53.68bush 0.71 3844.68 53.48 71.90soviet 0.71 4496.40 59.74 75.27kennedy 0.78 14641.29 99.11 147.73church 0.92 11291.78 70.13 161.02book 0.92 17143.84 79.68 215.16vietnam 0.92 32701.11 97.66 334.86boycott 0.98 105630.08 110.56 955.42noriega 0.91 86281.28 56.88 1516.82Table 2: Parameter estimates of the model for someselected terms, sorted by the??1/?
?2valueated from either of the exponential distributions (thehas high value of p?, but since the values of ?
areso close, it doesn?t really matter which distributiongenerated the observation).
We observe compara-tively larger values of ?
?1for terms like yet, followsand except since they have some dependence on thedocument topic.
One may claim that these are someoutliers having large values of both?
?1and ??2.
Thelarge value of?
?1can be explained, as these terms arerarely occurring function words in the corpus.
Theydo not occur in bursts and their occurrences are scat-tered, so values of?
?2are also large (Table 1).
Inter-estingly, based on our heuristics these large valuesnullify each other to obtain a small value of??1/?
?2.But since these cases are exceptional, they find theirplace on the boundary region of the division.The second part of the table contains mostly non-topical content terms as defined in the literature(Katz, 1996).
They do not describe the main topicof the document, but some useful aspects of the doc-ument or a nearby topical term.
Special attentionmay be given to the term george, which describesthe topical term bush.
In a document about GeorgeBush, the complete name is mentioned possibly onlyonce in the beginning and further references to it aremade using the word bush, leading to bush being as-53signed as a topical term, but not george.
The termgovernment in the group refers to some newswirearticle about some government in any state or anycountry, future references to which are made us-ing this term.
Similarly the term federal is usedto make future references to the US Government.As the words federal and government are used fre-quently for referencing, they exhibit comparativelysmall values of??2.
We were surprised by the occur-rence of terms like said, here and she in the secondgroup, as they are commonly considered as func-tion words.
Closer examination revealed the details.Said has some dependence on the document genre,with respect to the content and reporting style.
Thedata were based on newswire articles about impor-tant people and events.
It is true, though unfor-tunate, that the majority of such people are male,hence there are more articles about men than women(he occurs 757, 301 times in 163, 884 documents asthe 13th most frequent term in the corpus, whereasshe occurs 164, 030 times in 48, 794 documents asthe 70th frequent term).
This explains why he hasa smaller value of ?
?1than she.
But the ?
?2valuesfor both of them are quite close, showing that theyhave similar usage pattern.
Again, newswire articlesare mostly about people and events, and rarely aboutsome location, referenced by the term here.
This ex-plains the large value of?
?1for here.
Again, becauseof its usage for referencing, it re-occurs frequentlywhile describing a particular location, leading to asmall value of??2.
Possibly, in a collection of ?traveldocuments?, here will have a smaller value of?
?1andthus occur higher up in the list, which would allowthe model to be used for characterizing genre.Terms in the third part, as expected, are topicalcontent terms.
An occurrence of such a term de-fines the topic or the main content word of the doc-ument or the text chunk under consideration.
Theseterms are rare in the entire corpus, and only appearin documents that are about this term, resulting invery high values of ??1.
Also low values of ?
?2forthese terms mean that repeat occurrences within thesame document are quite frequent; the characteris-tic expected from a topical content term.
Because ofthese characteristics, based on our heuristics theseterms have very high values of??1/?
?2, and hence areconsidered the most informative terms in the corpus.5.5 Case StudiesHere we study selected terms based on our model.These terms have been studied before by other re-searchers.
We study these terms to compare ourfindings with previous work and also demonstratethe range of inferences that may be derived from ourmodel.5.5.1 somewhat vrs boycottThese terms occur an approximately equal num-ber of times in the AP corpus, and inverse doc-ument frequency was used to distinguish betweenthem (Church and Gale, 1995a).
Our model alsogives approximately similar rates of occurrence (?
?1)for these two terms as shown in Table 2.
But the re-occurrence rate, ?
?2, is 110.56 for boycott, which isvery small in comparison with the value of 4349.72for somewhat.
Hence based on this, our model as-signs somewhat as a rare function word occurring ina scattered manner over the entire corpus.
Whereasboycott is assigned as a topical content word, as itshould be.5.5.2 follows vrs sovietThese terms were studied in connection with fit-ting Poisson distributions to their term distribution(Manning and Schu?tze, 1999), and hence determin-ing their characteristics1 .
In our model, follows haslarge values of both ?
?1and ?
?2(Table 2), so that ithas the characteristics of a rare function word.
Butsoviet has a large?
?1value and a very small?
?2value,so that it has the characteristics of a topical contentword.
So the findings from our model agree with theoriginal work.5.5.3 kennedy vrs exceptBoth these terms have nearly equal inverse doc-ument frequency for the AP corpus (Church, 2000;Umemura and Church, 2000) and will be assignedequal weight.
They used a method (Kwok, 1996)based on average-term frequency to determine thenature of the term.
According to our model, the?
?2value of kennedy is very small as compared to thatfor except.
Hence using the??1/?
?2measure, we cancorrectly identify kennedy as a topical content term1The original study was based on the New York Times, ourson the Associated Press corpus54and except as an infrequent function word.
This is inagreement with the findings of the original analysis.5.5.4 noriega and saidThese terms were studied in the context of anadaptive language model to demonstrate the fact thatthe probability of a repeat occurrence of a term in adocument defies the ?bag of words?
independenceassumption (Church, 2000).
The deviation from in-dependence is greater for content terms like noriegaas compared to general terms like said.
This can beexplained in the context of our model as said hassmall values of?
?1and ?
?2, and their values are quiteclose to each other (as compared to other terms, seeTable 2).
Hence said is distributed more evenly inthe corpus than noriega.
Therefore, noriega defiesthe independence assumption to a much greater ex-tent than said.
Hence their findings (Church, 2000)are well explained by our model.6 ConclusionIn this paper we present a model for term re-occurrence in text based on gaps between succes-sive occurrences of a term in a document.
Parameterestimates based on this model reveal various charac-teristics of term use in a collection.
The model candifferentiate a term?s dependence on genre and col-lection and we intend to investigate use of the modelfor purposes like genre detection, corpus profiling,authorship attribution, text classification, etc.
Theproposed measure of ??1/?
?2can be appropriatelyadopted as a means of feature selection that takesinto account the term?s occurrence pattern in a cor-pus.
We can capture both within-document bursti-ness and rate of occurrence of a term in a singlemodel.ReferencesA.
Bookstein and D.R Swanson.
1974.
Probabilisticmodels for automatic indexing.
Journal of the Ameri-can Society for Information Science, 25:312?318.K.
Church and W. Gale.
1995a.
Inverse document fre-quency (idf): A measure of deviation from poisson.In Proceedings of the Third Workshop on Very LargeCorpora, pages 121?130.K.
Church and W. Gale.
1995b.
Poisson mixtures.
Nat-ural Language Engineering, 1(2):163?190.K.
Church.
2000.
Empirical estimates of adaptation: Thechance of two noriega?s is closer to p/2 than p2.
InCOLING, pages 173?179.Anne De Roeck, Avik Sarkar, and Paul H Garthwaite.2004a.
Defeating the homogeneity assumption.
InProceedings of 7th International Conference on theStatistical Analysis of Textual Data (JADT), pages282?294.Anne De Roeck, Avik Sarkar, and Paul H Garthwaite.2004b.
Frequent term distribution measures fordataset profiling.
In Proceedings of the 4th Interna-tional conference of Language Resources and Evalua-tion (LREC), pages 1647?1650.Alexander Franz.
1997.
Independence assumptions con-sidered harmful.
In Proceedings of the eighth confer-ence on European chapter of the Association for Com-putational Linguistics, pages 182?189.A.
Gelman, J. Carlin, H.S.
Stern, and D.B.
Rubin.
1995.Bayesian Data Analysis.
Chapman and Hall, London,UK.W.R.
Gilks, S. Richardson, and D.J.
Spiegelhalter.
1996.Markov Chain Monte Carlo in Practice.
Interdisci-plinary Statistics Series.
Chapman and Hall, London,UK.Slava M. Katz.
1996.
Distribution of content words andphrases in text and language modelling.
Natural Lan-guage Engineering, 2(1):15?60.A Kilgarriff.
1997.
Using word frequency lists to mea-sure corpus homogeneity and similarity between cor-pora.
In Proceedings of ACL-SIGDAT Workshop onvery large corpora, Hong Kong.K.
L. Kwok.
1996.
A new method of weighting queryterms for ad-hoc retrieval.
In SIGIR, pages 187?195.Christopher D. Manning and Hinrich Schu?tze.
1999.Foundations of Statistical Natural Language Process-ing.
The MIT Press, Cambridge, Massachusetts.Christian.
P. Robert.
1996.
Mixtures of distributions: in-ference and estimation.
In W.R. Gilks, S. Richardson,and D.J.
Spiegelhalter, editors, Markov Chain MonteCarlo in Practice, pages 441?464.D.J.
Spiegelhalter, A. Thomas, N. G. Best, and D. Lunn.2003.
Winbugs: Windows version of bayesian infer-ence using gibbs sampling, version 1.4.K.
Umemura and K. Church.
2000.
Empirical termweighting and expansion frequency.
In EmpiricalMethods in Natural Language Processing and VeryLarge Corpora, pages 117?123.55
