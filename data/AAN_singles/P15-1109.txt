Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1127?1137,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsEnd-to-end Learning of Semantic Role Labeling Using Recurrent NeuralNetworksJie Zhou and Wei XuBaidu Research{zhoujie01,wei.xu}@baidu.comAbstractSemantic role labeling (SRL) is one of thebasic natural language processing (NLP)problems.
To this date, most of the suc-cessful SRL systems were built on top ofsome form of parsing results (Koomen etal., 2005; Palmer et al, 2010; Pradhan etal., 2013), where pre-defined feature tem-plates over the syntactic structure are used.The attempts of building an end-to-endSRL learning system without using pars-ing were less successful (Collobert et al,2011).
In this work, we propose to usedeep bi-directional recurrent network as anend-to-end system for SRL.
We take on-ly original text information as input fea-ture, without using any syntactic knowl-edge.
The proposed algorithm for seman-tic role labeling was mainly evaluated onCoNLL-2005 shared task and achieved F1score of 81.07.
This result outperformsthe previous state-of-the-art system fromthe combination of different parsing treesor models.
We also obtained the sameconclusion with F1= 81.27 on CoNLL-2012 shared task.
As a result of simplicity,our model is also computationally efficientthat the parsing speed is 6.7k tokens persecond.
Our analysis shows that our modelis better at handling longer sentences thantraditional models.
And the latent vari-ables of our model implicitly capture thesyntactic structure of a sentence.1 IntroductionSemantic role labeling (SRL) is a form of shal-low semantic parsing whose goal is to discoverthe predicate-argument structure of each predicatein a given input sentence.
Given a sentence, foreach target verb (predicate) all the constituents inthe sentence which fill a semantic role of the verbhave to be recognized.
Typical semantic argu-ments include Agent, Patient, Instrument, etc., andalso adjuncts such as Locative, Temporal, Man-ner, Cause, etc.. SRL is useful as an intermedi-ate step in a wide range of natural language pro-cessing (NLP) tasks, such as information extrac-tion (Bastianelli et al, 2013), automatic documentcategorization (Persson et al, 2009) and question-answering (Dan and Lapata, 2007; Surdeanu et al,2003; Moschitti et al, 2003).SRL is considered as a supervised machinelearning problem.
In traditional methods, linearclassifier such as SVM is often employed to per-form this task based on features extracted from thetraining corpus.
Actually, people often treat thisproblem as a multi-step classification task.
First,whether an argument is related to the predicate isdetermined; next the detail relation type was de-cided(Palmer et al, 2010).Syntactic information is considered to play anessential role in solving this problem (Punyakanoket al, 2008a).
The location of an argument on syn-tactic tree provides an intermediate tag for improv-ing the performance.
However, building this syn-tactic tree also introduces the prediction risk in-evitably.
The analysis in (Pradhan et al, 2005)found that the major source of the incorrect pre-dictions was the syntactic parser.
Combination ofdifferent syntactic parsers was proposed to addressthis problem, from both feature level and modellevel (Surdeanu et al, 2007; Koomen et al, 2005;Pradhan et al, 2005).Besides, feature templates in this classificationtask strongly rely on the expert experience.
Theyneed iterative modification after analyzing how thesystem performs on development data.
When thecorpus and data distribution are changed, or whenpeople move to another language, the feature tem-plates have to be re-designed.To address the above issues, (Collobert et al,11272011) proposed a unified neural network architec-ture using word embedding and convolution.
Theyapplied their architecture on four standard NLPtasks: Part-Of-Speech tagging (POS), chunking(CHUNK), Named Entity Recognition (NER) andSemantic Role Labeling (SRL).
They were able toreach the previous state-of-the-art performance onall these tasks except for SRL.
They had to resortto parsing features in order to make the systemcompetitive with state-of-the-art performance.In this work, we propose an end-to-end systemusing deep bi-directional long short-term memo-ry (DB-LSTM) model to address the above dif-ficulties.
We take only original text as the in-put features, without any intermediate tag suchas syntactic information.
The input features areprocessed by the following 8 layers of LSTM bi-directionally.
At the top locates the conditionalrandom field (CRF) model for tag sequence pre-diction.
We achieve the state-of-the-art perfor-mance of f-score F1= 81.07 on CoNLL-2005shared task and F1= 81.27 on CoNLL-2012shared task.
At last, we find the traditional syn-tactic information can also be inferred from thelearned representations.2 Related WorkPeople solve SRL problems in two major ways.The first one follows the traditional spirit widelyused in NLP basic problems.
A linear classifier isemployed with feature templates.
Most efforts fo-cus on how to extract the feature templates thatcan best describe the text properties from train-ing corpus.
One of the most important featuresis from syntactic parsing, although syntactic pars-ing is also considered as a difficult problem.
Thussystem combination appear to be the general solu-tion.In the work of (Pradhan et al, 2005), the syn-tactic tags are produced by Charniak parser (Char-niak, 2000; Charniak and Johnson, 2005) andCollins parser (Collins, 2003) respectively.
Basedon this, different systems are built to generateSRL tags.
These SRL tags are used to extend theoriginal feature templates, along with flat syntacticchunking results.
At last another classifier learnsthe final SRL tag from the above results.
In theiranalysis, the combination of three different syntac-tic view brings large improvement for the system.Similarly, Koomen et al (Koomen et al, 2005)combined the system in another way.
They builtmultiple classifiers and then all outputs are com-bined through an optimization problem.
Surdeanuet al fully discussed the combination strategy in(Surdeanu et al, 2007).Beyond the above traditional methods, the sec-ond way try to solve this problem without featureengineering.
Collobert et al (Collobert et al,2011) introduced a neural network model consistsof word embedding layer, convolution layers andCRF layer.
This pipeline addressed the data spar-sity by initializing the model with word embed-dings which is trained from large unlabeled textcorpus.
However, the convolution layer is not thebest way to model long distance dependency sinceit only includes words within limited context.
Sothey processed the whole sequence for each giv-en pair of argument and predicate.
This results inthe computational complexity ofO(npL2), with Ldenoting the sequence length and npthe numberof predicate, while the complexity of our model islinear (O(npL)).
Moreover, in order to catch upwith the performance of traditional methods, theyhad to incorporate the syntactic features by usingparse trees of Charniak parser (Charniak, 2000)which still provides the major contribution.At the inference stage, structural constraints of-ten lead to improved results (Punyakanok et al,2008b).
The constraints comes from annotationconventions of the task and other linguistic consid-erations.
With dynamic programming, (T?ackstr?omet al, 2015) enhance the inference efficiency fur-ther.
But designation of the constraints dependsmuch on the linguistic knowledge.Nevertheless, the attempts of building end-to-end systems for NLP become popular in recen-t years.
Inspired by the work in computer vi-sion, people hierarchically organized a window ofwords through convolution layers in deep formto account for the higher level of organization tosolve the document classification task (Kim, 2014;Zhang and LeCun, 2015).
Step further, peoplehave also achieved success in directly mappingthe sequence to sequence level target as the workin dependency parsing and machine translation(Vinyals et al, 2014; Sutskever et al, 2014).3 ApproachesIn this paper, we propose an end-to-end systembased on recurrent topology.
Recurrent neural net-work (RNN) has natural advantage in modelingsequence problems.
The past information is built1128up through the recurrent layer when model con-sumes the sequence word by word as shown in E-q.
1. x and y are the input and output of the recur-rent layer with (t) denoting the time step, wmfandwmiare the matrix from input or recurrent layer tohidden layer.
?
is the activation function.
With-out y(t?1)term, the rnn model returns to the feedforward form.y(t)m= ?
(?fwmfx(t)f+?iwmiy(t?1)i) (1)However, people often met with two difficulties.First, information of the current word strongly de-pends on distant words, rather than its neighbor-hood.
Second, gradient parameters may explodeor vanish especially in processing long sequences(Bengio et al, 1994).
Thus long short-term mem-ory (LSTM) (Hochreiter and Schmidhuber, 1997)was proposed to address the above difficulties.In the following part, we will first give a briefintroduction about the LSTM and then demon-strate how to build up a network based on LSTMto solve a typical sequence tagging problem: se-mantic role labeling.3.1 Long Short-Term Memory (LSTM)Long short-term memory (LSTM) (Hochreiter andSchmidhuber, 1997; Graves et al, 2009) is anRNN architecture specifically designed to addressthe vanishing gradient and exploding gradientproblems.
The hidden neural units are replacedby a number of memory blocks.
Each memoryblock contains several cells, whose activations arecontrolled by three multiplicative gates: the inputgate, forget gate and output gate.
With the abovechange, the original rnn model is improved to be:y(t)m= ?
(s(t)c,m) ?
pi(t)m(2)= ?(n(t)m?
(t)m+ ?
(t)ms(t?1)c,m) ?
pi(t)m(3)Now y is the memory block output.
n is equivalentto the original hidden value y in rnn model.
?, ?and pi are the input, forget and output gates value.sc,mis state value of cell c in blockm and c is fixedto be 1 and omitted in common work.
The compu-tation of three multiplicative gates comes from in-put value, recurrent value and cell state value withdifferent activations ?
respectively as shown in thefollowing and Fig.
1:n(t)m: ?n(?fwmf,nx(t)f+?iwmi,ny(t?1)i) (4)?
(t)m: ??
(?fwmf,?x(t)f+?iwmi,?y(t?1)i+ wm?s(t?1)m)?
(t)m: ??
(?fwmf,?x(t)f+?iwmi,?y(t?1)i+ wm?s(t?1)m)pi(t)m: ?pi(?fwmf,pix(t)f+?iwmi,piy(t?1)i+ wmpis(t)m)Figure 1: LSTM memory block with a single cell.
(Graves et al, 2009)The effect of the gates is to allow the cells tostore and access information over long periods oftime.
When the input gate is closed, the new com-ing input information will not affect the previouscell state.
Forget gate is used to remove the histor-ical information stored in the cells.
The rest of thenetwork can access the stored value of a cell onlywhen its output gate is open.In language related problems, the structuralknowledge can be extracted out by processing se-quences both forward and backward so that thecomplementary information from the past and thefuture can be integrated for inference.
Thus bi-directional LSTM (B-LSTM) containing two hid-den layers were proposed(Schuster and Paliwal,1997).
Both hidden layers connect to the same in-put layer and output layer, processing the same se-quence in two directions respectively (A. Graves,2013).In this work, we utilize the bi-directional infor-mation in another way.
First a standard LSTMprocesses the sequence in forward direction.
Theoutput of this LSTM layer is taken by the next1129LSTM layer as input, processed in reversed di-rection.
These two standard LSTM layers com-pose a pair of LSTM.
Then we stack LSTM layer-s pair after pair to obtain the deep LSTM model.We call this topology as deep bi-directional LSTM(DB-LSTM) network.
Our experiments show thatthis architecture is critical to achieve good perfor-mance.3.2 PipelineWe process the sequence word by word.
Two in-put features play an essential role in this pipeline:predicate (pred) and argument (argu), with argu-ment describing the word under processing.
Theoutput for this pair of words is their semantic role.If a sequence has nppredicates, we will processthis sequence nptimes.We also introduce two other features, predicatecontext (ctx-p) and region mark (mr).
Since a s-ingle predicate word can not exactly describe thepredicate information, especially when the samewords appear more than one times in a sentence.With the expanded context, the ambiguity can belargely eliminated.
Similarly, we use region markmr= 1 to denote the argument position if it lo-cates in the predicate context region, or mr= 0if not.
These four simple features are all we needfor our SRL system.
In Tab.
1 we give an examplesequence with the labels for each word.
We do notuse other types of features such as part of speech(POS), syntactic parsing, etc..time argu pred ctx-p mrlabel1 A set been set .
0 B-A12 record set been set .
0 I-A13 date set been set .
0 I-A14 has set been set .
0 O5 n?t set been set .
0 B-AM-NEG6 been set been set .
1 O7 set set been set .
1 B-V8 .
set been set .
1 OTable 1: An example sequence with 4 input fea-tures: argument, predicate, predicate context (con-text length is 3) , region mark.
?IOB?
taggingscheme is used (Collobert et al, 2011).Because the large number of parameters asso-ciated with the argument words, similar to (Col-lobert et al, 2011), the pre-trained word represen-tations are employed to address the data sparsityissue.
We used a large unlabeled text corpus totrain a neural language model (NLM) (Bengio etal., 2006; Bengio et al, 2003) and then initial-ized the argument and predicate word representa-tions with parameters from the NLM representa-tions.
There are various ways of obtaining goodword representations (Mikolov et al, 2013; Col-lobert and Weston, 2008; Mnih and Kavukcuoglu,2013; Yu et al, 2014).
A systematic comparisonof them on the task of SRL is beyond the scope ofthis work.The above four features are concatenated to bethe input representation at this time step for thefollowing LSTM layers.
As described in Sec.
3.1,we use DB-LSTM topology to learn the sequenceknowledge and we build up to 8 layers of DB-LSTM in our work.As in traditional methods, we employ CRF(Lafferty et al, 2001) on top of the network forthe final prediction.
It takes the representationsprovided by the last LSTM layer as input to modelthe strong dependance among adjacent tags.Figure 2: DB-LSTM network.Shadow part denotethe predicate context within length 1.The complete model with 4 LSTM layers is il-lustrated in Fig.
2.
At the bottom of the graph lo-cates the word sequence in Tab.
1.
For a given timestep (step 2 as an example), argument and predi-cate are specified with different color.
We use theshadowed region to denote the predicate contex-t.
The temporal expanded version of the model isshown in Fig.
3.
L-H denotes the LSTM hiddenlayer.We use the stochastic gradient descent (SGD)algorithm as the training technique for the wholepipeline (Lecun et al, 1998).
For a given se-quence, we look up the embedding of each wordand process this vector with the following LSTMlayers for the high level representation.
After hav-ing finished the whole sequence, we take the rep-resentations of all time steps as the input features1130Figure 3: Temporal expanded DB-LSTM network.Bars denote that the connections are blocked bythe closed gates.
Shadow part denotes the predi-cate context.for CRF to perform the sequence tagging task.
Thetraditional viterbi decoding is used for inference.The gradient of the log-likelihood of the tag se-quence with respect to the input of the CRF is cal-culated and back-propagated to all the DB-LSTMlayers to get the gradient of the parameters (Col-lobert et al, 2011).4 ExperimentsWe mainly evaluated and analyzed our system onthe commonly used CoNLL-2005 shared task da-ta set and the conclusions are also validated onCoNLL-2012 shared task.4.1 Data setCoNLL-2005 data set takes section 2-21 of WallStreet Journal (WSJ) data as training set, and sec-tion 24 as development set.
The test set consist-s of section 23 of WSJ concatenated with 3 sec-tions from Brown corpus (Carreras and M`arquez,2005).
CoNLL-2012 data set is extracted fromOntoNotes v5.0 corpus.
The description and sep-aration of train, development and test data set canbe found in (Pradhan et al, 2013).4.2 Word embeddingWe trained word embeddings with EnglishWikipedia (Ewk) corpus using NLM (Bengio etal., 2006).
The corpus contains 995 million to-kens.
We transformed all the words into theirlowercase and the vocabulary size is 4.9 million.About 5% words in CoNLL 2005 data set can notbe found in Ewk dictionary and are marked as<unk>.
In all experiments, we use the same wordembedding with dimension 32.4.3 Network topologyIn this part, we will analyze the performance oftwo different networks, the CNN and LSTM net-work.
Although at last we find CNN can not pro-vide the results as good as that from LSTM, theanalysis still help us to gain a deep insight of thisproblem.
In CNN, we add argument context asthe fifth feature and the other four features are thesame as that used in LSTM.
In order to have goodunderstanding of the contribution from each mod-eling decision, we started from a simple model andadd more units step by step.4.3.1 Convolutional neural networkUsing CNN to solve SRL problem has been intro-duced in (Collobert et al, 2011).
Since we onlyfocus on the analysis of features, a simplified ver-sion is used here.Our feature set consists of five parts as de-scribed above.
The representation of argumentand predicate can be obtained by looking up theEmb(Ewk) dictionary.
And the representation ofargument context and predicate context can be ob-tained by concatenating the embedding of eachword in the context.
For each of the above fourparts, we add a hidden layer.
Then all these fourhidden layers together with region mark are pro-jected onto the next hidden layer.
At last we use aCRF layer for prediction (See Fig.
4).
With aboveset up, the computational complexity is O(npL).Figure 4: CNN Pipeline.
Shadow parts denotethe argument context and predicate context respec-tivelyThe size of hidden layers connected to argu-ment or predicate is set to be h1w= 32.
The sizeof the other two hidden layers connected to con-text embedding is set to be h1c= 128 since the1131corresponding inputs are larger.
To simplify theparameter setting and results comparison, we usethe same learning rate l = 1?
10?3for each layerand keep this rate a constant during model train-ing.
The second hidden layer dimension h2is also128.
All hidden layer activation function is tanh.In Tab.
2, it is shown that longer argumentand predicate context result in better performance,since longer context brings more information.
Weobserve the same trends in other NLP experiments,such as NER, POS tagging.
The difference is thatwe do not need to use the context length up to 11.This is because most of the useful information forNER and POS tagging is local respect the labelposition, while in SRL there exists long distancerelationship.
So in traditional methods for SRL,syntactic trees are often introduced to account forsuch relation.
In order to see whether the improve-ment from CNN-2 to CNN-3 is due to longer con-text or larger model size, we tested a model CNN-6 with same context length but more model param-eters.
As we can see from the result of CoNLL-2005 data set (Tab.
2), larger model does not im-prove the result.name h1cctx-a ctx-p mrF1(dev) F1CNN-1 128 1 5 y 41.22 41.24CNN-2 128 5 5 y 51.83 52.09CNN-3 128 11 5 y 52.81 53.07CNN-4 128 11 1 y 49.69 50.70CNN-5 128 11 5 n 36.40 37.50CNN-6 256 5 5 y 51.60 51.91Table 2: F1of CNN method on development setand test set of CoNLL-2005 data set.Without using region mark (mr) feature, the F1drops from the 53.07 of CNN-3 to the 37.50 ofCNN-5.
Since it is generally believed that wordsnear the predicate are more likely to be related tothe predicate.SRL is a typical problem with long distance de-pendency, while the convolution operation can on-ly learn the knowledge from the limited neighbor-hood.
This is why we have to introduce long con-text.
However, the language information can notbe expressed just by linearly expanding the con-text as what we did in CNN pipeline.
In order tobetter summarize the sequence structure, we turnto LSTM network.4.3.2 LSTM networkHere the feature set consists of four parts.
Ar-gument and predicate are necessary parts in thisproblem.
In recurrent model, argument context(ctx-a) is no longer needed and we only expand thepredicate context.
We also need the region markdefined in the same way as in CNN.
The archi-tecture has been shown in Fig.
2 and described inSec.
3.2.Since it is difficult to propagate the error fromthe top to the bottom layers, we use two learningrates.
At the bottom, i.e.
from embeddings to thefirst LSTM layer, we use lb= 1 ?
10?2for mod-el depth d <= 4 and lb= 2 ?
10?2for d > 4.For the other LSTM layers and CRF layer, we setlearning rate l = lb?
10?3.
We kept all learn-ing rates constant during training.
The model sizecan be enlarged by increasing the number of LST-M layers (d) or the dimension of hidden layers (h).L2 weight decay in SGD is used for model regu-larization and we set its strength r2= 8?
10?4:w ?
w ?
l ?
(g + r2?
w) (5)where w denotes the parameter, g the gradient ofthe log likelihood of the label with respect to theparameter.We started on CoNLL-2005 dataset from asmall model with only one LSTM layer and h =32.
All word embeddings were randomly initial-ized.
Predicate context length was 1.
Regionmark is not used.
With this model, we obtainedF1= 49.44 (Tab.
3), better than that of CNN with-out using argument context (41.24) or region mark(37.50).
This result suggests that, the recurrentstructure can extract sequential information moreeffectively than CNN.By adding predicate context with length 5, F1is improved from 49.44 to 56.85 (Tab.
3).
This isbecause we only recurrently process the argumentword, so we still need predicate context for moredetail.
Further more, F1rises to 58.71 with re-gion mark feature.
The reason is the same as weexplained in CNN pipeline.Next we change the random initialization ofword representation to the pre-trained word rep-resentation from Emb(Ewk).
This representationis fixed in the training process.
F1rises to 65.11(See Tab.
3).So far, we have shown the effect from each partof features in LSTM network.
The conclusion isconsistent with what we found in CNN network.Besides, LSTM exhibits better abilities to learn thesequence structure.
Next, we gradually increasethe model size to further enhance the performance.1132Emb d ctx-p mrh F1(dev) F1CoNLL-2005 data setRan 1 1 n 32 47.88 49.44Ran 1 5 n 32 54.63 56.85Ran 1 5 y 32 57.13 58.71Ewk 1 5 y 32 64.48 65.11Ewk 2 5 y 32 72.72 72.56Ewk 4 5 y 32 75.08 75.74Ewk 6 5 y 32 76.94 78.02Ewk 8 5 y 32 77.50 78.28Ewk 8 5 y 64 77.69 79.46Ewk 8 5 y 128 79.10 80.28fine tuningEwk 8 5 y 128 79.55 81.07CoNLL-2012 data setEwk 8 5 y 128 80.51 80.70fine tuningEwk 8 5 y 128 81.07 81.27Table 3: F1with LSTM method on developmentset and test set of CoNLL-2005 data set andCoNLL-2012 data set.
Emb: the type of embed-ding.
d: the number of LSTM layers.
ctx-p: pred-icate context length.
mr: region mark feature.
h:hidden layer size.We find that the critical improvement comesfrom increasing the depth of LSTM network.
Af-ter adding a reversed LSTM layer, F1is improvedfrom 65.11 to 72.56.
And the F1of the systemwith d = 4, 6, 8 are 75.74, 78.02 and 78.28 re-spectively.
With 6-layer network, we have outper-formed the CoNLL-2005 shared task winner sys-tem with F1= 77.92 (Koomen et al, 2005).
Ourexperiment results also show that the further per-formance gain by increasing the depth from 6 to 8is relative small.Another way to increase the model size is to in-crease the hidden layer dimension h. We graduallyincrease the dimension from 32 to 64, 128, and thecorresponding results are listed in Tab.
3.
The bestF1we obtained is 80.28 with h = 128.
We al-so show the result F1= 80.70 on CoNLL-2012dataset in Tab.
3 with exactly the same setup.In the above experiments, learning rate andweight decay rate are fixed for the sake of sim-plicity in comparing different models.
To fur-ther improve the model, we perform a fine tuningstep to adjust the parameters based on previous-ly trained model.
This includes the relaxation ofweight decay and decrease of learning rate.
Weset r2= 4 ?
10?4and lb= 1 ?
10?2, and obtainF1= 81.07 as the final result of CoNLL-2005 da-ta set and F1= 81.27 of CoNLL-2012 data set.F1F1CoNLL-2005 dev test WSJ BrownKoomen 77.35 77.92 79.44 67.75Koomen (single parser) 74.76 - - -Pradhan 78.34 77.30 78.63 68.44Collobert (w/ parser) 75.42 76.06 - -Collobert (w/o parser) 72.29 74.15 - -Surdeanu - - 80.6 70.1Toutanova 78.6 - 80.3 68.8T?ackstr?om 78.6 - 79.9 71.3Ours 79.55 81.07 82.84 69.41F1F1CoNLL-2012 dev test - -Pradhan - 75.53T?ackstr?om 79.1 79.4Ours 81.07 81.27Table 4: Comparison with previous methods.In Tab.
4, we compare the performance of oth-er works.
On CoNLL-2005 shared task, merg-ing syntactic tree at feature level instead of modellevel exhibits the similar performance with F1=77.30 (Pradhan et al, 2005).
After further investi-gation on model combination, Surdeanu et al ob-tained a better system (Surdeanu et al, 2007).
Wealso list the results from (Toutanova et al, 2008)and (T?ackstr?om et al, 2015) of the joint modelwith additional considerations of standard linguis-tic assumptions.
For convolution based methods(Collobert et al, 2011), the best F1is 76.06, inwhich syntactic parser plays an essential role.
Theresult without using parser drops down to 74.15.On Brown set, we observe the better performancefrom the work of (Surdeanu et al, 2007) and(T?ackstr?om et al, 2015).
We hypothesize that DB-LSTM is a data-driven method that can not per-forms well on out-domain dataset.On CoNLL-2012 data set, the traditionalmethod gives F1= 75.53 (Pradhan et al, 2013)and a dynamic programming algorithm for effi-cient constrained inference in SRL gives F1=79.4 (T?ackstr?om et al, 2015) , both of them alsorely on syntax trees.Since the input feature size is much smaller thenthe traditional sparse feature templates, the infer-ence stage is very efficient that the model can pro-cess 6.7k tokens per second on average.4.4 AnalysisWe analyze our results on CoNLL-2005 data set.First we list the details including the performanceon each sub-classes in Tab.
5.
The results ofCoNLL-2005 shared task winner system (Koomenet al, 2005) are also shown for comparison.
Their1133Results (Koomen et.al.)
Results (Ours)Data set P R F1P R F1dev 80.05 74.83 77.35 79.69 79.41 79.55dev (s) 75.40 74.13 74.76 79.69 79.41 79.55test WSJ 82.28 76.78 79.44 82.92 82.75 82.84test Brown 73.38 62.93 67.75 70.70 68.17 69.41test 81.18 74.92 77.92 81.33 80.80 81.07A0 88.22 87.88 88.05 90.08 89.73 89.91A1 82.25 77.69 79.91 82.00 82.87 82.43A2 78.27 60.36 68.16 70.50 72.63 71.55A3 82.73 52.60 64.31 63.98 55.68 59.54AM-ADV 63.82 56.13 59.73 66.03 53.00 58.80AM-DIS 75.44 80.62 77.95 73.76 78.07 75.85AM-LOC 66.67 55.10 60.33 65.17 58.48 61.65AM-MNR 66.79 53.20 59.22 56.36 54.63 55.48AM-MOD 96.11 98.73 97.40 94.62 98.60 96.57AM-NEG 97.40 97.83 97.61 95.70 95.36 95.53AM-TMP 78.16 76.72 77.44 78.61 82.74 80.62R-A0 89.72 85.71 87.67 94.72 93.57 94.14R-A1 70.00 76.28 73.01 80.00 90.40 84.88V 98.92 97.10 98.00 98.63 98.63 98.63Table 5: F1on each sub sets and classes (CoNLL-2005).
(We remove the classes with low statistics.
)final system is the combination of the results of5 parsing trees from two different parsers.
Theyalso reported the scores of each single system ondevelopment set and we list the best one of them(dev(s)).We observe the improvement of F1on develop-ment set and test set are 2.20 and 3.15 respective-ly.
For single system, the improvement is 4.79 ondevelopment set.
We also notice that our modelshow improvement on both WSJ and Brown testset.
The advantage of our model is even more sig-nificant when comparing with the previous effortof end-to-end training of SRL model (Collobert etal., 2011).
Without using linguistic features fromparse tree, the F1of Collobert?s model is 74.15,which is 6.92 lower than our model.Figure 5: F1vs.
sentence length (CoNLL-2005).In order to analyze the performance of our mod-el on the sentences with different lengths, we splitthe data into 6 bins according to the sentencelength, with bin width being 10 words and the lastbin includes sequences with L > 50 because of in-sufficient data for longer sentences.
Fig.
5 showsF1scores at different sequence lengths on WSJtest data and Brown test data for our model andKoomen?s model (baseline) (Koomen et al, 2005).In all curves, performance degrades with increasedsentence length.
However, the performance gainof our model over the baseline model is larger forlonger sentences.Figure 7: Averaged Forget gates value vs. Syn-tactic distance (CoNLL-2005).
The last point in-cludes instances with syntactic distance ds?
6.Since we do not use any syntactic informationas input feature, we are curious about whether thisinformation can be extracted out from the systemparameters.
In LSTM, forget gates are used tocontrol the use of historical information.
We com-pute the average value vfgof forget gates of the7thLSTM layer at word position for a given sen-tence.
We also introduce a variable named syntac-tic distance dsto represent the number of edgesbetween argument word and predicate word in thedependency parsing tree.
Four example sentencesare shown in Fig.
6.
For each figure, the bottomaxis denotes an example sentence.
At the top ofeach graph is the corresponding dependency treebuilt from gold dependency parsing tag.
At thebottom, vfgand dsare shown in black and redline.
Noticed that the higher forget gates valuesmeans ?Remember?
and smaller values ?Forget?.Smaller dsmeans that it is easy to make predictionthat long history is unnecessary.
On the contrary,large dsresults in a difficult prediction that longhistorical information is needed.
We also com-puted the average vfgover instances and found itmonotonously increases with ds(Fig.
7).
The co-incidence of vfgand dssuggests that the modelimplicitly captures some syntactic structure.5 Conclusion and Future workWe investigate a traditional NLP problem SRLwith DB-LSTM network.
With this model, we are1134Figure 6: Forget gates value vs. Syntactic distance on four example sentences.
Top: dependency parsingtree from gold tag.
Green square word: predicate word.
Bottom black solid lines: forget gates value ateach time step.
Bottom red empty square lines: gold syntactic distance between the current argumentand predicate.able to bypass the traditional steps for extractingthe intermediate NLP features such as POS andsyntactic parsing and avoid human engineering thefeature templates.
The model is trained to predictthe SRL tag directly from the original word se-quence with four simple features without any ex-plicit linguistic knowledge.
Our model achievesF1score of 81.07 on CoNLL-2005 shared taskand 81.27 on CoNLL-2012 shared task, both out-performing the previous systems based on parsingresults and feature engineering, which heavily re-ly on the linguistic knowledge from expert.
Fur-thermore, the simplified feature templates resultsin high inference efficiency with 6.7k tokens persecond.In our experiments, increasing the model depthis the major contribution to the final improvement.With deep model, we achieve strong ability oflearning semantic rules without worrying aboutover-fitting even on such limited training set.
It al-so outperforms the convolution method with largecontext length.
Moreover, with more sophisti-catedly designed network and training techniquebased on LSTM, such as the attempt to integratethe parse tree concept into LSTM framework (Taiet al, 2015), we believe the better performancecan be achieved.We show in our analysis that for long sequencesour model has even larger advantage over the tra-ditional models.
On one hand, LSTM network iscapable of capturing the long distance dependen-cy especially in its deep form.
On the other hand,the traditional feature templates are only good atdescribing the properties in neighborhood and asmall mistake in syntactic tree will results in largedeviation in SRL tagging.
Moreover, from theanalysis of the internal states of the deep network,we see that the model implicitly learn to capturesome syntactic structure similar to the dependen-cy parsing tree.It is encouraging to see that deep learning mod-els with end-to-end training can outperform tra-ditional models on tasks which are previouslybelieved to heavily depend on syntactic parsing(Koomen et al, 2005; Pradhan et al, 2013).
How-ever, we recognize that semantic role labeling it-self is an intermediate step towards the languageproblems we really care about, such as questionanswering, information extraction etc.
We believethat end-to-end training with some suitable deepstructure yet to be invented might be proven tobe effective to solving these problems.
And weare seeing some recent active research exploringthis possibility (Weston et al, 2014; Weston et al,2015; Graves et al, 2014).1135ReferencesG.
Hinton A. Graves, A. Mohamed.
2013.
Speechrecognition with deep recurrent neural network-s.
In IEEE International Conference on Acoustics,Speech, and Signal Processing, ICASSP 2013.Emanuele Bastianelli, Giuseppe Castellucci, DaniloCroce, and Roberto Basili.
2013.
Textual inferenceand meaning representation in human robot interac-tion.
In Proceedings of the Joint Symposium on Se-mantic Processing.
Textual Inference and Structuresin Corpora, pages 65?69.Yoshua Bengio, Patrice Simard, and Paolo Frasconi.1994.
Learning long-term dependencies with gradi-ent descent is difficult.
IEEE Transactions on Neu-ral Networks, 5(2):157?166.Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Re-search, 3:1137?1155, March.Yoshua Bengio, Holger Schwenk, Jean-Sbastien Sen-cal, Frderic Morin, and Jean-Luc Gauvain.
2006.Neural probabilistic language models.
In Innova-tions in Machine Learning, volume 194 of Studiesin Fuzziness and Soft Computing, pages 137?186.Springer Berlin Heidelberg.Xavier Carreras and Llu?
?s M`arquez.
2005.
Intro-duction to the CoNLL-2005 shared task: Semanticrole labeling.
In Proceedings of the Ninth Confer-ence on Computational Natural Language Learning(CoNLL-2005), pages 152?164, Ann Arbor, Michi-gan, June.
Association for Computational Linguis-tics.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative r-eranking.
In Proceedings of the 43rd Annual Meet-ing on Association for Computational Linguistics,ACL ?05, pages 173?180, Stroudsburg, PA, USA.Association for Computational Linguistics.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proceedings of the 1st NorthAmerican Chapter of the Association for Computa-tional Linguistics Conference, NAACL 2000, pages132?139, Stroudsburg, PA, USA.
Association forComputational Linguistics.Michael Collins.
2003.
Head-driven statistical mod-els for natural language parsing.
Comput.
Linguist.,29(4):589?637, December.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: Deepneural networks with multitask learning.
In Pro-ceedings of the 25th International Conference onMachine Learning, ICML ?08, pages 160?167, NewYork, NY, USA.
ACM.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
Journal of Marchine Learning Research,12:2493?2537, November.Shen Dan and Mirella Lapata.
2007.
Using seman-tic roles to improve question answering.
In Pro-ceedings of the 2007 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL).Alex Graves, Marcus Liwicki, Santiago Fernan-dez, Roman Bertolami, Horst Bunke, and J?urgenSchmidhuber.
2009.
A novel connectionist systemfor unconstrained handwriting recognition.
IEEETransactions on Pattern Analysis and Machine In-telligence, 31(5):855?868.Alex Graves, Greg Wayne, and Ivo Danihelka.
2014.Neural turing machines.
arXiv:1410.5401.S.
Hochreiter and J. J?urgen Schmidhuber.
1997.Long short-term memory.
Neural Computation,9(8):1735?1780.Yoon Kim.
2014.
Convolutional neural networks forsentence classification.
In Proceedings of the 2014Conference on Empirical Methods in Natural Lan-guage Processing, pages 1746?1751.Peter Koomen, Vasin Punyakanok, Dan Roth, andWen-tau Yih.
2005.
Generalized inference withmultiple semantic role labeling systems.
In Pro-ceedings of the 9th Conference on Computation-al Natural Language Learning, CONLL ?05, pages181?184, Stroudsburg, PA, USA.
Association forComputational Linguistics.John D. Lafferty, Andrew McCallum, and FernandoC.
N. Pereira.
2001.
Conditional random fields:Probabilistic models for segmenting and labeling se-quence data.
In Proceedings of the 8th InternationalConference on Machine Learning, ICML ?01, pages282?289, San Francisco, CA, USA.
Morgan Kauf-mann Publishers Inc.Yann Lecun, Lon Bottou, Yoshua Bengio, and PatrickHaffner.
1998.
Gradient-based learning applied todocument recognition.
In Proceedings of the IEEE,pages 2278?2324.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-rado, and Jeffrey Dean.
2013.
Distributed represen-tations of phrases and their compositionality.
In Ad-vances on Neural Information Processing Systems.Andriy Mnih and Koray Kavukcuoglu.
2013.
Learningword embeddings efficiently with noise-contrastiveestimation.
In Advances in Neural Information Pro-cessing Systems, pages 2265?2273.Alessandro Moschitti, Paul Morarescu, and Sanda M.Harabagiu.
2003.
Open domain information ex-traction via automatic semantic labeling.
In FLAIRSConference?03, pages 397?401.1136Martha Palmer, Daniel Gildea, and Nianwen Xue.2010.
Semantic Role Labeling.
Synthesis Lec-tures on Human Language Technology Series.
Mor-gan and Claypool.Jacob Persson, Richard Johansson, and Pierre Nugues.2009.
Text categorization using predicatecargumen-t structures.
In Proceedings of NODALIDA, pages142?149.Sameer Pradhan, Kadri Hacioglu, Wayne Ward,James H. Martin, and Daniel Jurafsky.
2005.
Se-mantic role chunking combining complementarysyntactic views.
In Proceedings of the 9th Confer-ence on Computational Natural Language Learning,CONLL ?05, pages 217?220, Stroudsburg, PA, US-A.
Association for Computational Linguistics.Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,Hwee Tou Ng, Anders Bj?orkelund, Olga Uryupina,Yuchen Zhang, and Zhi Zhong.
2013.
Towards ro-bust linguistic analysis using ontonotes.
In Proceed-ings of the Seventeenth Conference on Computa-tional Natural Language Learning, pages 143?152,Sofia, Bulgaria, August.
Association for Computa-tional Linguistics.V.
Punyakanok, D. Roth, and W. Yih.
2008a.
Theimportance of syntactic parsing and inference in se-mantic role labeling.
Computational Linguistics,34(2).Vasin Punyakanok, Dan Roth, and Wen tau Yih.
2008b.The importance of syntactic parsing and inference insemantic role labeling.
Computational linguistics,6(9).M.
Schuster and K. K. Paliwal.
1997.
Bidirection-al recurrent neural networks.
IEEE Transactions onSignal Processing, 45:2673?2681.Mihai Surdeanu, Sanda Harabagiu, John Williams, andPaul Aarseth.
2003.
Using predicate-argumentstructures for information extraction.
In Proceed-ings of the 41st Annual Meeting on Association forComputational Linguistics - Volume 1, ACL ?03,pages 8?15, Stroudsburg, PA, USA.
Association forComputational Linguistics.Mihai Surdeanu, Llu?
?s M`arquez, Xavier Carreras, andPere R. Comas.
2007.
Combination strategies forsemantic role labeling.
Journal of Artificial Intelli-gence Research, 29:105?151.Ilya Sutskever, Oriol Vinyals, and Quoc V Le.
2014.Sequence to sequence learning with neural network-s.
In Advances on Neural Information ProcessingSystems.Oscar T?ackstr?om, Kuzman Ganchev, and DipanjanDas.
2015.
Efficient inference and structured learn-ing for semantic role labeling.
Transactions of theAssociation for Computational Linguistics, 3:29?41.Kai Sheng Tai, Richard Socher, and Christopher D.Manning.
2015.
Improved semantic representation-s from tree-structured long short-term memory net-works.
In Proceedings of the 53st Annual Meetingon Association for Computational Linguistics, ACL?15, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Kristina Toutanova, Aria Haghighi, and Christopher D.Manning.
2008.
A global joint model for semanticrole labeling.
Computational Linguistics, 34:161?191.Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov,Ilya Sutskever, and Geoffrey Hinton.
2014.
Gram-mar as a foreign language.
arXiv:1412.7449.Jason Weston, Sumit Chopra, and Antoine Bordes.2014.
Memory networks.
arXiv:1410.3916.Jason Weston, Antoine Bordes, Sumit Chopra, andTomas Mikolov.
2015.
Towards ai-complete ques-tion answering: A set of prerequisite toy tasks.
arX-iv:1502.05698.Mo Yu, Matthew Gormley, and Mark Dredze.
2014.Factor-based compositional embedding models.
InAdvances in Neural Information Processing SystemsWorkshop on Learning Semantics.Xiang Zhang and Yann LeCun.
2015.
Text understand-ing from scratch.
arXiv:1502.01710.1137
