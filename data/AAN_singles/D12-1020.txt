Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 214?222, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsA Phrase-Discovering Topic Model Using Hierarchical Pitman-Yor ProcessesRobert V. LindseyUniversity of Colorado, Boulderrobert.lindsey@colorado.eduWilliam P. Headden IIITwo Cassowaries Inc.headdenw@twocassowaries.comMichael J. StipicevicGoogle Inc.stip@google.comAbstractTopic models traditionally rely on the bag-of-words assumption.
In data mining appli-cations, this often results in end-users beingpresented with inscrutable lists of topical un-igrams, single words inferred as representa-tive of their topics.
In this article, we presenta hierarchical generative probabilistic modelof topical phrases.
The model simultane-ously infers the location, length, and topic ofphrases within a corpus and relaxes the bag-of-words assumption within phrases by usinga hierarchy of Pitman-Yor processes.
We useMarkov chain Monte Carlo techniques for ap-proximate inference in the model and performslice sampling to learn its hyperparameters.We show via an experiment on human subjectsthat our model finds substantially better, moreinterpretable topical phrases than do compet-ing models.1 IntroductionProbabilistic topic models have been the focus ofintense study in recent years.
The archetypal topicmodel, Latent Dirichlet Allocation (LDA), positsthat words within a document are conditionallyindependent given their topic (Blei et al2003).This ?bag-of-words?
assumption is a common sim-plification in which word order is ignored, butone which introduces undesirable properties intoa model meant to serve as an unsupervised ex-ploratory tool for data analysis.When an end-user runs a topic model, the outputhe or she is often interested in is a list of topicalunigrams, words probable in a topic (hence, repre-sentative of it).
In many situations, such as duringthe use of the topic model for the analysis of a newor ill-understood corpus, these lists can be insuffi-ciently informative.
For instance, if a layperson ranLDA on the NIPS corpus, he would likely get a topicwhose most prominent words include policy, value,and reward.
Seeing these words isolated from theircontext in a list would not be particularly insightfulto the layperson unfamiliar with computer scienceresearch.
An alternative to LDA which producedricher output like policy iteration algorithm, valuefunction, and model-based reinforcement learningalongside the unigrams would be much more en-lightening.
Most situations where a topic model isactually useful for data exploration require a modelwhose output is rich enough to dispel the need forthe user?s extensive prior knowledge of the data.Furthermore, lists of topical unigrams are oftenmade only marginally interpretable by virtue of theirnon-compositionality, the principle that a colloca-tion?s meaning typically is not derivable from itsconstituent words (Schone and Jurafsky, 2001).
Forexample, the meaning of compact disc as a mu-sic medium comes from neither the unigram com-pact nor the unigram disc, but emerges from the bi-gram as a whole.
Moreover, non-compositionalityis topic dependent; compact disc should be inter-preted as a music medium in a music topic, and asa small region bounded by a circle in a mathemati-cal topic.
LDA is prone to decompose collocationsinto different topics and violate the principle of non-compositionality, and its unigram lists are harder tointerpret as a result.214We present an extension of LDA called Phrase-Discovering LDA (PDLDA) that satisfies twodesiderata: providing rich, interpretable output andhonoring the non-compositionality of collocations.PDLDA is built in the tradition of the ?Topical N-Gram?
(TNG) model of Wang et al2007).
TNG isa topic model which satisfies the first desideratum byproducing lists of representative, topically cohesiven-grams of the form shown in Figure 1.
We divergefrom TNG by our addressing the second desidera-tum, and we do so through a more straightforwardand intuitive definition of what constitutes a phraseand its topic.
In the furtherance of our goals, weemploy a hierarchical method of modeling phrasesthat uses dependent Pitman-Yor processes to ame-liorate overfitting.
Pitman-Yor processes have beensuccessfully used in the past in n-gram (Teh, 2006)and LDA-based models (Wallach, 2006) for creat-ing Bayesian language models which exploit wordorder, and they prove equally useful in this scenarioof exploiting both word order and topics.This article is organized as follows: after describ-ing TNG, we discuss PDLDA and how PDLDA ad-dresses the limitations of TNG.
We then provide de-tails of our inference procedures and evaluate ourmodel against competing models on a subset of theTREC AP corpus (Harman, 1992) in an experi-ment on human subjects which assesses the inter-pretability of topical n-gram lists.
The experimentis premised on the notion that topic models shouldbe evaluated through a real-world task instead ofthrough information-theoretic measures which oftennegatively correlate with topic quality (Chang et al2009).2 Background: LDA and TNGLDA represents documents as probabilistic mixturesof latent topics.
Each wordw in a corpus w is drawnfrom a distribution ?
indexed by a topic z, where z isdrawn from a distribution ?
indexed by its documentd.
The formal definition of LDA is?d ?
Dirichlet (?)
zi | d, ?
?
Discrete (?d)?z ?
Dirichlet (?)
wi | zi, ?
?
Discrete (?zi)where ?d is document d?s topic distribution, ?z istopic z?s distribution over words, zi is the topic as-signment of the ith token, and wi is the ith word.?
and ?
are hyperparameters to the Dirichlet priors.Here and throughout the article, we use a bold fontfor vector notation: for example, z is the vector of alltopic assignments, and its ith entry, zi, correspondsto the topic assignment of the ith token in the corpus.TNG extends LDA to model n-grams of arbitrarylength in order to create the kind of rich output fortext mining discussed in the introduction.
It doesthis by representing a joint distribution P (z, c|w)where each ci is a Boolean variable that signals thestart of a new n-gram beginning at the ith token.
cpartitions a corpus into consecutive non-overlappingn-grams of various lengths.
Formally, TNG differsfrom LDA by the distributional assumptionswi | wi?1, zi, ci = 1, ?
?
Discrete(?zi)wi | wi?1, zi, ci = 0, ?
?
Discrete(?ziwi?1)ci | wi?1, zi?1, pi ?
Bernoulli(pizi?1wi?1)where the new distributions pizw and ?zw are en-dowed with conjugate prior distributions: pizw ?Beta(?)
and ?zw ?
Dirichlet(?).
When ci = 0,word wi is joined into a topic-specific bigram withwi?1.
When ci = 1, wi is drawn from a topic-specific unigram distribution and is the start of a newn-gram.An unusual feature of TNG is that words withina topical n-gram, a sequence of words delineatedby c, do not share the same topic.
To compen-sate for this after running a Gibbs sampler, Wanget al2007) analyze each topical n-gram post hocas if the topic of the final word in the n-gram wasthe topic assignment of the entire n-gram.
Thoughthis design simplifies inference, we perceive it as ashortcoming since the aforementioned principle ofnon-compositionality supports the intuitive idea thateach collocation ought to be drawn from a singletopic.
Another potential drawback of TNG is thatthe topic-specific bigram distributions ?zw share noprobability mass between each other or with the un-igram distributions ?z .
Hence, observing a bigramunder one topic does not make it more likely underanother topic or make its constituent unigrams moreprobable.
To be more concrete, in TNG, observingspace shuttle under a topic z (or under two topics,one for each word) regrettably does not make spaceshuttle more likely under a topic z?
6= z, nor does itmake observing shuttle more likely under any topic.Smoothing, the sharing of probability mass between215matteratomselementselectronsatommoleculesformoxygenhydrogenparticleselementsolutionsubstancereactionnucleuschemical reactionsatomic numberhydrogen atomshydrogen atomperiodic tablechemical changephysical propertieschemical reactionwater moleculessodium chloridesmall amountspositive chargecarbon atomsphysical changechemical propertieslike charges repelpositively charged nucleusunlike charges attractouter energy levelreaction takes placenegatively charged electronschemical change takes placeform new substancesphysical change takes placeform sodium chloridemodern atomic theoryelectrically charged particlesincreasing atomic numbersecond ionization energieshigher energy levels(a) Topic 1presidentcongressvotepartyconstitutionstatemembersofficegovernmentstateselectedrepresentativessenatehousewashingtonsupreme courtnew yorkdemocratic partyvice presidentpolitical partiesnational governmentexecutive branchcivil rightsnew governmentpolitical partyandrew jacksonchief justicefederal governmentstate legislaturespublic opinioncivil rights actcivil rights movementsupreme court ruledpresident theodore rooseveltsecond continental congressequal rights amendmentstrong central governmentsherman antitrust actcivil rights legislationpublic opinion pollsmajor political partiescongress shall makefederal district courtsupreme court decisionsamerican foreign policy(b) Topic 2wordswordsentencewritewritingparagraphsentencesmeaningusesubjectlanguagereadexampleverbtopicmain ideatopic sentenceenglish languagefollowing paragraphwords likequotation marksdirect objectword processingsentence tellsfigurative languagewriting processfollowing sentencessubject matterstandard englishuse wordsword processing centerword processing systemsword processing equipmentspeak different languagesuse quotation markssingle main ideause words liketopic sentence statespresent perfect tenseexpress complete thoughtsword processing softwareuse formal englishstandard american englishcollective noun refersformal standard english(c) Topic 3energyusedoilheatcoalusefuelproducepowersourcelightelectricityburngasgasolinenatural resourcesnatural gasheat energyiron orecarbon dioxidepotential energysolar energylight energyfossil fuelshot watersteam enginelarge amountssun's energyradiant energynuclear energynuclear power plantsnuclear power plantimportant natural resourceselectric power plantscalled fossil fuelsimportant natural resourceproduce large amountscalled solar energyelectric light bulbuse electrical energyuse solar energycarbon dioxide gascalled potential energygas called carbon dioxidecalled crude oil(d) Topic 4waterairtemperatureheatliquidgasgaseshotpressureatmospherewarmcoldsurfaceoxygencloudswater vaporair pollutionair pressurewarm aircold waterearth's surfaceroom temperatureboiling pointdrinking wateratmospheric pressurecold warhigh temperaturesliquid watercold airwarm waterwater vapor condenseswarm air risescold air masscalled water vaporwater vapor changesprocess takes placewarm air massclean air actgas called water vapordry spell holdsair pressure insidesewage treatment plantair pollution lawshigh melting pointshigh melting point(e) Topic 5chinaafricaindiaeuropepeoplechineseasiaegyptworldromelandeasttradecountriesempiremiddle eastwestern europenorth africamediterranean seayears agoroman empirefar eastsoutheast asiawest africasaudi arabiacapital letterasia minorunited statescapital citycenturies ago2000 years agoeast india companyeastern united states4000 years agosouthwestern united statesmiddle atlantic statesnortheastern united stateswestern united statessoutheastern united states200 years agomiddle atlantic regionindus river valleywestern roman empirebritish north america actcoast guard station(f) Topic 6Figure 1: Six out of one hundred topics found by our model, PDLDA, on the Touchstone Applied ScienceAssociates (TASA) corpus (Landauer and Dumais, 1997).
Each column within a box shows the top fifteenphrases for a topic and is restricted to phrases of a minimum length of one, two, or three words, respectively.The rows are ordered by likelihood.216...
zi-1zizi+1wi-1wi+1cici+1.........G???
?wiDTua b?
?......V|u|Figure 2: PDLDA drawn in plate notation.contexts, is desirable so that a model like this doesnot need to independently infer the probability ofevery bigram under every topic.
The advantages ofsmoothing are especially pronounced for small cor-pora or for a large number of topics.
In these sit-uations, the observed number of bigrams in a giventopic will necessarily be very small and thus not sup-port strong inferences.3 PDLDAA more natural definition of a topical phrase, onewhich meets our second desideratum, is to have eachphrase possess a single topic.
We adopt this in-tuitive idea in PDLDA.
It can also be understoodthrough the lens of Bayesian changepoint detection.Changepoint detection is used in time series mod-els in which the generative parameters periodicallychange abruptly (Adams and MacKay, 2007).
View-ing a sentence as a time series of words, we posit thatthe generative parameter, the topic, changes period-ically in accordance with the changepoint indicatorsc.
Because there is no restriction on the number ofwords between changepoints, topical phrases can bearbitrarily long but will always have a single topicdrawn from ?d.The full definition of PDLDA is given bywi | u ?
Discrete(Gu)Gu ?
PYP(a|u|, b|u|, Gpi(u))G?
?
PYP(a0, b0, H)zi | d, zi?1, ?d, ci ?
{?zi?1 if ci = 0Discrete (?d) if ci = 1ci | wi?1, zi?1, pi ?
Bernoulli(piwi?1zi?1)with the prior distriutions over the parameters as?d ?
Dirichlet (?)
pizw ?
Beta (?
)a|u| ?
Beta (?)
b|u| ?
Gamma ()Like TNG, PDLDA assumes that the probabilityof a changepoint ci+1 after the ith token depends onthe current topic zi and word wi.
This causes thelength of a phrase to depend on its topic and con-stituent words.
The changepoints explicitly modelwhich words tend to start and end phrases in eachdocument.
Depending on ci, zi is either set deter-ministically to the preceding topic (when ci = 0)or is drawn anew from ?d (when ci = 1).
In thisway, each topical phrase has a single topic drawnfrom its document?s topic distribution.
As in TNG,the parameters pizw and ?d are given conjugate priorsparameterized by ?
and ?.Let u be a context vector consisting of thephrase topic and the past m words: u , <zi, wi?1, wi?2, .
.
.
, wi?m >.
The operator pi(u) de-notes the prefix of u, the vector with the rightmostelement of u removed.
|u| denotes the length of u,and ?
represents an empty context.
For practical rea-sons, we pad u with a special start symbol when thecontext overlaps a phrase boundary.
For example,the first word wi of a phrase beginning at a positioni necessarily has ci = 1; consequently, all the pre-ceding words wi?j in the context vector are treatedas start symbols so that wi is effectively drawn froma topic-specific unigram distribution.In PDLDA, each token is drawn from a distribu-tion conditioned on its context u.
When m = 1,this conditioning is analogous to TNG?s word dis-tribution.
However, in contrast with TNG, the word217.i -1i -1+??
?i z-1??????
??
?i -1+???????
??
????
?i - ?i - ?
+??
?- ???????
??
?i - ?
+???????
??
????
?Figure 3: Illustration of the hierarchical Pitman-Yorprocess for a toy two-word vocabulary V = {honda,civic} and two-topic (T = 2) model with m = 1.Each node G in the tree is a Pitman-Yor processwhose base distribution is its parent node, andH is auniform distribution over V .
When, for example, thecontext is u = z1 : honda, the darkened path is fol-lowed and the probability of the next word is calcu-lated from the shaded node using Equation 1, whichcombines predictions from all the nodes along thedarkened path.distributions used are Pitman-Yor processes (PYPs)linked together into a tree structure.
This hierar-chical construction creates the desired smoothingamong different contexts.
The next section explainsthis hierarchical distribution in more detail.3.1 Hierarchical Pitman-Yor processWords in PDLDA are emitted from Gu, which hasa PYP prior (Pitman and Yor, 1997).
PYPs are ageneralization of the Dirichlet Process, with the ad-dition of a discount parameter 0 ?
a ?
1.
Whenconsidering the distribution of a sequence of wordsw drawn iid from a PYP-distributed G, one can an-alytically marginalize G and consider the resultingconditional distribution of w given its parameters a,b, and base distribution ?.
This marginal can bestbe understood by considering the distribution of anywi|w1, .
.
.
, wi?1, a, b, ?, which is characterized bya generative process known as the generalized Chi-nese Restaurant Process (CRP) (Pitman, 2002).
Inthe CRP metaphor, one imagines a restaurant withan unbounded number of tables, where each tablehas one shared dish (a draw from ?)
and can seat anunlimited number of customers.
The CRP specifies aprocess by which customers entering the restaurantchoose a table to sit at and, consequently, the dishthey eat.
The first customer to arrive always sits atthe first table.
Subsequent customers sit at an occu-pied table k with probability proportional to ck ?
aand choose a new unoccupied table with probabil-ity proportional to b + ta, where ck is the numberof customers seated at table k and t is the numberof occupied tables in G. For our language modelingpurposes, ?customers?
are word tokens and ?dishes?are word types.The hierarchical PYP (HPYP) is an intuitive re-cursive formulation of the PYP in which the basedistribution ?
is itself PYP-distributed.
Figure 3demonstrates this principle as applied to PDLDA.The hierarchy forms a tree structure, where leavesare restaurants corresponding to full contexts and in-ternal nodes correspond to partial contexts.
An edgebetween a parent and child node represents a depen-dency of the child on the parent, where the base dis-tribution of the child node is its parent.
This smoothseach context?s distribution like the Bayesian n-grammodel of Teh (2006), which is a Bayesian versionof interpolated Kneser-Ney smoothing (Chen andGoodman, 1998).
One ramification of this setupis that if a word occurs in a context u, the shar-ing makes it more likely in other contexts that havesomething in common with u, such as a shared topicor word.The HPYP gives the following probability for aword following the context u being w:Pu(w | ?,a,b) =cuw?
?
a|u|tuwb|u| + cu?
?+b|u| + a|u|tu?b|u| + cu?
?Ppi(u)(w | ?,a,b) (1)where Ppi(?
)(w|?,a,b) = G?
(w), cuw?
is the num-ber of customers eating dish w in restaurant u, andtuw is the number of tables serving w in restau-rant u, and ?
represents the current seating arrange-ment.
Here and throughout the rest of the paper, weuse a dot to indicate marginal counts: e.g., cuw?
=?k cuwk where cuwk is the number of customerseating w in u at table k. The base distribution ofG?
was chosen to be uniform: H(w) = 1/V with Vbeing the vocabulary size.
The above equation an in-terpolation between distributions of context lengths218|u|, |u| ?
1, .
.
.
0 and realizes the sharing of statisti-cal strength between different contexts.3.2 InferenceIn this section, we describe Markov chain MonteCarlo procedures to sample from P (z, c, ?
|w, U),the posterior distribution over topic assignments z,phrase boundaries c, and seating arrangements ?given an observed corpus w. Let U be short-hand for ?, ?,a,b.
In order to draw samplesfrom P (z, c, ?
|w, U), we employ a Metropolis-Hastings sampler for approximate inference.
Thesampler we use is a collapsed sampler (Griffiths andSteyvers, 2004), wherein ?, ?, and G are analyti-cally marginalized.
Because we marginalize eachG,we use the Chinese Restaurant Franchise representa-tion of the hierarchical PYPs (Teh, 2006).
However,rather than onerously storing the table assignmentof every token in w, we store only the counts of howmany tables there are in a restaurant and how manycustomers are sitting at each table in that restaurant.We refer the inquisitive reader to the appendix ofTeh (2006) for further details of this procedure.Our sampling strategy for a given token i in doc-ument d is to jointly propose changes to the change-point ci and topic assignment zi, and then to theseating arrangement ?
.
Recall that according to themodel, if ci = 0, zi = zi?1; otherwise zi is gen-erated from the topic distribution for document d.Since the topic assignment remains the same until anew changepoint at a position i?
is reached, each to-ken wj for j from position i until i?
?
1 will dependon zi because for these j, zj = zi.
We call this set oftokens the phrase suffix of the ith token and denoteit s(i).
More formally, let s(i) be the maximal setof continuous indices j ?
i including i such that, ifj 6= i, cj = 0.
That is, s(i) are the indices compris-ing the remainder of the phrase beginning at positioni.
In addition, let x(i) indicate the extended suffixversion of s(i) which includes one additional index:x(i) , {s(i) ?
{max (s(i)) + 1}}.
In addition tothe words in the suffix s(i), the changepoint indica-tor variables cj for j in x(i) are also conditioned onzi.
To make these dependencies more explicit, werefer to zs(i) , zj ?j ?
s(i), which are constrainedby the model to share a topic.The variables that depend directly on zi, ci arezs(i),ws(i), cx(i).
The proposal distribution firstdraws from a multinomial over T + 1 options: oneoption for ci = 0, zi = zi ?
1; and one for ci = 1paired with each possible zi = z ?
1 .
.
.
T .
This isgiven byP (zs(i), ci | z?s(i), c?i, ?
?s(i),w, U) ?
?j?x(i)n?x(j)zj?1wj?1cj + ?cjn?x(j)zj?1wj?1?
+ ?0 + ?1?j?s(i)P (zj | c, z?s(j), U) Puj (wj | ?
?s(i), U)withP (zj | c, z?s(j), U) =??????
?n?s(j)dzj + ?n?s(j)d?
+ T?if cj = 1?zj ,zj?1 if cj = 0where Puj (wj | ?
?s(i), U) is given by Equation 1,T is the number of topics, n?s(j)dz is the number ofphrases in document d that have topic z when s(j)?sassignment is excluded, and n?s(j)zwc is the number oftimes a changepoint c has followed a word w withtopic z when s(j)?s assignments are excluded.After drawing a proposal for ci, zs(i) for token i,the sampler adds a customer eating wi to a tableserving wi in restaurant ui.
An old table k is se-lected with probability ?
max(0, cuwk ?
a|u|) anda new table is selected with probability ?
(b|ui| +a|ui|tui?
)Ppi(u)(wi).Let z?s(i), c?i, ?
?s(i) denote the proposed change tozs(i), ci, ?s(i).
We accept the proposal with probabil-ity min(A, 1) whereA =P?
(z?s(i), c?i, ?
?s(i)) Q(zs(i), ci, ?s(i))P?
(zs(i), ci, ?s(i)) Q(z?s(i), c?i, ?
?s(i))where Q is the proposal distribution and P?
is thetrue unnormalized distribution.
P?
differs from Q inthat the probability of each word wj and the seatingarrangement depends only on ?s(j), as opposed tothe simplification of using ?s(i).
Almost all propos-als are accepted; hence, this theoretically motivatedMetropolis Hastings correction step makes little dif-ference in practice.Because the parameters a and b have no intuitiveinterpretation and we lack any strong belief aboutwhat they should be, we give them vague priorswhere ?1 = ?2 = 1 and 1 = 10, 2 = .1.
We then219interleave a slice sampling algorithm (Neal, 2000)between sweeps of the Metropolis-Hastings samplerto learn these parameters.
We chose not to do infer-ence on ?
in order to make the tests of our modelagainst TNG more equitable.4 Related WorkAn integral part of modeling topical phrases is therelaxation of the bag-of-words assumption in LDA.There are many models that make this relaxation.Among them, Griffiths and Steyvers (2005) presenta model in which words are generated either con-ditioned on a topic or conditioned on the previousword in a bigram, but not both.
They use this tomodel human performance on a word-associationtask.
Wallach (2006) experiments with incorpo-rating LDA into a bigram language model.
Hermodel uses a hierarchical Dirichlet to share param-eters across bigrams in a topic in a manner similarto our use of PYPs, but it lacks a notion of the topicbeing shared between the words in an n-gram.
TheHidden Topic Markov Model (HTMM) (Gruber etal., 2007) assumes that all words in a sentence havethe same topic, and consecutive sentences are likelyto have the same topic.
By dropping the indepen-dence assumption among topics, HTMM is able toachieve lower perplexity scores than LDA at mini-mal additional computational costs.
These modelsare unconcerned with topical n-grams and thus donot model phrases.Johnson (2010) presents an Adaptor Grammarmodel of topical phrases.
Adaptor Grammars area framework for specifying nonparametric Bayesianmodels over context-free grammars in which certainsubtrees are ?memoized?
or remembered for reuse.In Johnson?s model, subtrees corresponding to com-mon phrases for a topic are memoized, resulting in amodel in which each topic is associated with a distri-bution over whole phrases.
While it is a theoreticallyelegant method for finding topical phrases, for largecorpora we found inference to be impractically slow.5 Phrase Intrusion ExperimentPerplexity is the typical information theoretic mea-sure of language model quality used in lieu of ex-trinsic measures, which are more difficult and costlyto run.
However, it is well known that perplexityTrial 1 of 80countriesbritainfrancemuseumTrial 2 of 80air forcebeverly hillsdefense ministeru.s.
troopsTrial 3 of 80fdabooksmokingcigarettesTrial 4 of 80roman catholic churchair traffic controllersroman catholic priestroman catholic bishopFigure 4: Experimental setup of the phrase intrusionexperiment in which subjects must click on the n-gram that does not belong.scores may negatively correlate with actual qualityas assessed by humans (Chang et al2009).
Withthat fact in mind, we expanded the methodology ofChang et al2009) to create a ?phrase intrusion?task that quantitatively compares the quality of thetopical n-gram lists produced by our model againstthose of other models.Each of 48 subjects underwent 80 trials of a web-based experiment on Amazon Mechanical Turk, areliable (Paolacci et al2010) and increasingly com-mon venue for conducting online experiments.
Ineach trial, a subject is presented with a randomly or-dered list of four n-grams (cf.
Figure 4).
Each sub-ject?s task is to select the intruder phrase, a spuriousn-gram not belonging with the others in the list.
If,other than the intruder, the items in the list are allon the same topic, then subjects can easily identifythe intruder because the list is semantically cohesiveand makes sense.
If the list is incohesive and has nodiscernible topic, subjects must guess arbitrarily andperformance is at random.To construct each trial?s list, we chose two top-ics z and z?
(z 6= z?
), then selected the three mostprobable n-grams from z and the intruder phrase, ann-gram probable in z?
and improbable in z. Thisdesign ensures that the intruder is not identifiabledue solely to its being rare.
Interspersed among thephrase intrusion trials were several simple screen-ing trials intended to affirm that subjects possesseda minimal level of attentiveness and reading com-prehension.
For example, one such screening trialpresented subjects with the list banana, apple, tele-vision, orange.
Subjects who got any of these trials220Unigrams Bigrams Trigrams00.20.40.60.81ModelPrecisionPDLDATNGLDA(a) Word repetition allowed within a list.Bigrams Trigrams00.20.40.60.81ModelPrecisionPDLDATNG(b) Word repetition not allowed.Figure 5: An across-subject measure of the ability to detect intruders as a function of n-gram size and model.Excluding trials with repeated words does not qualitatively affect the results.wrong were excluded from our analyses.Each subject was presented with trials constructedfrom the output of PDLDA and TNG for unigrams,bigrams, and trigrams.
For unigrams, we also testedthe output of the original smoothed LDA (Blei etal., 2003).
The experiment was conducted twice fora 2,246-document subset of the TREC AP corpus(Blei et al2003; Harman, 1992): the first time pro-ceeded as described above, but the second time didnot allow word repetition within a topic?s list.
Thetopical phrases found by TNG and PDLDA oftenrevolve around a central n-gram, with other wordspre- or post- appended to it.
In this intrusion exper-iment, any n-gram not containing the central wordor phrase may be trivially identifiable, regardless ofits relevance to the topic.
For example, the intruderin Trial 4 of Figure 4 is easily identifiable even ifa subject does not understand English.
This secondexperiment was designed to test whether our conclu-sions hinge on word repetition.We used the MALLET toolbox (McCallum,2002) for the implementations of LDA and TNG.Each model was run with 100 topics for 5,000 it-erations.
We set m = 2, ?
= .01, ?
= .01, ?
= 1,pi1 = pi2 = 1, ?1 = 10, and ?2 = .1.
For all mod-els, we treated certain punctuation as the start of aphrase by setting cj = 1 for all tokens j immediatelyfollowing periods, commas, semicolons, and excla-mation and question marks.
To reduce runtime, weremoved stopwords occuring in the MALLET tool-box?s stopword list.
Because TNG and LDA hadtrouble with single character words not in the sto-plist, we manually removed them before the experi-ment.
Any token immediately following a removedword was treated as if it were the start of a phrase.As in Chang et al2009), performance is mea-sured via model precision, the fraction of subjectsagreeing with the model.
It is defined as MPm,nk =?s1(im,nk,s = ?m,nk,s )/S where ?m,nk,s is the index ofthe intruding n-gram for subject s among the wordsgenerated from the kth topic of model m, im,nk,s is theintruder selected by s, and S is the number of sub-jects.
The model precisions are shown in Figure 5.PDLDA achieves the highest precision in all condi-tions.
Model precision is low in all models, which isa reflection of how challenging the task is on a smallcorpus laden with proper nouns and low-frequencywords.
Figure 5b demonstrates that the outcome ofthe experiment does not depend strongly on whetherthe topical n-gram lists have repeated words.6 ConclusionWe presented a topic model which simultaneouslysegments a corpus into phrases of varying lengthsand assigns topics to them.
The topical phrasesfound by PDLDA are much richer sources of in-formation than the topical unigrams typically pro-duced in topic modeling.
As evidenced by thephrase-intrusion experiment, the topical n-gram liststhat PDLDA finds are much more interpretable than221those found by TNG.The formalism of Bayesian changepoint detectionarose naturally from the intuitive assumption that thetopic of a sequence of tokens changes periodically,and that the tokens in between changepoints com-prise a phrase.
This formalism provides a principledway to discover phrases within the LDA framework.We presented a model embodying these principlesand showed how to incorporate dependent Pitman-Yor processes into it.AcknowledgementsThe first author is supported by an NSF GraduateResearch Fellowship.
The first and second authorsbegan this project while working at J.D.
Power &Associates.
We are indebted to Michael Mozer, MattWilder, and Nicolas Nicolov for their advice.ReferencesRyan Prescott Adams and David J.C. MacKay.
2007.Bayesian online changepoint detection.
Technical re-port, University of Cambridge, Cambridge, UK.David M. Blei, Andrew Y. Ng, Michael I. Jordan, andJohn Lafferty.
2003.
Latent dirichlet alation.
Jour-nal of Machine Learning Research, 3:993?1022.Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish,Chong Wang, and David M. Blei.
2009.
Reading tealeaves: How humans interpret topic models.
In NeuralInformation Processing Systems (NIPS).Stanley F. Chen and Joshua Goodman.
1998.
An empiri-cal study of smoothing techniques for language model-ing.
Technical Report TR-10-98, Center for Researchin Computing Technology, Harvard University.T.
L. Griffiths and M. Steyvers.
2004.
Finding scien-tific topics.
Proceedings of the National Academy ofSciences, 101(Suppl.
1):5228?5235, April.Thomas L. Griffiths, Mark Steyvers, David M. Blei, andJoshua B. Tenenbaum.
2005.
Integrating topics andsyntax.
In Advances in Neural Information ProcessingSystems 17, pages 537?544.
MIT Press.Thomas L. Griffiths, Joshua B. Tenenbaum, and MarkSteyvers.
2007.
Topics in semantic representation.Psychological Review, 114:211?244.Amit Gruber, Yair Weiss, and Michal Rosen-Zvi.
2007.Hidden topic Markov models.
Journal of MachineLearning Research - Proceedings Track, 2:163?170.Donna Harman.
1992.
Overview of the first text re-trieval conference (trec?1).
In Proceedings of thefirst Text REtrieval Conference (TREC?1), Washing-ton DC, USA.Mark Johnson.
2010.
PCFGs, Topic Models, AdaptorGrammars and Learning Topical Collocations and theStructure of Proper Names.
In Proceedings of the 48thAnnual Meeting of the Association for ComputationalLinguistics, pages 1148?1157, Uppsala, Sweden, July.Association for Computational Linguistics.Thomas K. Landauer and Susan T. Dumais.
1997.
A so-lution to plato?s problem: The latent semantic analysistheory of acquisition, induction, and representation ofknowledge.
Psychological Review, 104(2):211 ?
240.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://mallet.cs.umass.edu.Radford Neal.
2000.
Slice sampling.
Annals of Statis-tics, 31:705?767.Gabriele Paolacci, Jesse Chandler, and Panagiotis G.Ipeirotis.
2010.
Running experiments on AmazonMechanical Turk.
Judgment and Decision Making,5(5):411?419.J.
Pitman and M. Yor.
1997.
The two-parameter Poisson-Dirichlet distribution derived from a stable subordina-tor.
Annals of Probability, 25:855?900.J.
Pitman.
2002.
Combinatorial stochastic processes.Technical Report 621, Department of Statistics, Uni-versity of California at Berkeley.Patrick Schone and Daniel Jurafsky.
2001.
Isknowledge-free induction of multiword unit dictionaryheadwords a solved problem?
In Lillian Lee andDonna Harman, editors, Proceedings of the 2001 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 100?108.Yee Whye Teh.
2006.
A hierarchical Bayesian languagemodel based on Pitman-Yor processes.
In Proceed-ings of the 21st International Conference on Compu-tational Linguistics and the 44th Annual Meeting ofthe Association for Computational Linguistics, ACL-44, pages 985?992, Morristown, NJ, USA.
Associa-tion for Computational Linguistics.Hanna M. Wallach.
2006.
Topic modeling: beyond bag-of-words.
In Proceedings of the 23rd InternationalConference on Machine Learning, pages 977?984.Xuerui Wang, Andrew McCallum, and Xing Wei.
2007.Topical n-grams: Phrase and topic discovery, with anapplication to information retrieval.
In Proceedings ofthe 7th IEEE International Conference on Data Min-ing.222
