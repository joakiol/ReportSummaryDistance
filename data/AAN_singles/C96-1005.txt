Word Sense Disambiguationusing Conceptual DensityEneko Agirre*Lengoaia eta Sistema Informatikoak saila.
Euskal Herriko Universitatea.p.k.
649, 200800 Donostia.
Spain.
jibagbee@si.heu.esGerman Rigau**Departament de Llenguatges i Sistemes Informhtics.
Universitat Polit~cnica de Catalunya.Pau Gargallo 5, 08028 Barcelona.
Spain.
g.rigau@lsi.upc.esAbst ract .This paper presents a method for the resolutionof lexical ambiguity of nouns and its automaticevaluation over the Brown Corpus.
The methodrelies on the use oil' the wide-coverage nountaxonomy of WordNet and the notion ofconceptual distance among concepts, captured bya Conceptual Density formula developed for thispurpose.
This fully automatic method requiresno hand coding of lexical entries, hand taggingof text nor any kind of training process.
Theresults of the experiments have beenautomatically evaluated against SemCor, thesense-tagged version of the Brown Corpus.1 Int roduct ionMuch of recent work in lexical ambiguityresolution offers the prospect hat a disambiguationsystem might be able to receive as input unrestrictedtext and tag each word with the most likely sensewith fairly reasonable accuracy and efficiency.
Themost extended approach use the context of the word tobe disambiguatcd together with inl'ormation abouteach of its word senses to solve this problem.Interesting experiments have been performed inrecent years using preexisting lexical knowledgeresources: \[Cowie el al.
92\], \[Wilks et al 93\] withLDOCE, \[Yarowsky 92\] with Roget's InternationalThesaurus, and \[Sussna 93\], \[Voorhees 9311,\[Richardson etal.
94\], \[Resnik 95\] with WordNet.Although each of these techniques looks promisingfor disambiguation, either they have been onlyapplied to a small number of words, a few sentencesor not in a public domain corpus.
For this reason wehave tried to disambiguate all the nouns from real*Eneko Agirre was supported by a grant from the BasqueGoverment.
Part of this work is included in projects141226-TA248/95 of the Basque Country University andPI95-054 of the Basque Government.
**German Rigau was supported by a grant from theMinisterio de Educaci6n y Ciencia.texts in the public domain sense tagged version of theBrown corpus \[Francis & Kucera 67\], \[Miller et al93\], also called Semantic Concordance or SemCor forshort 1, The words in SemCor are tagged with wordsenses from WordNet, a broad semantic taxonomy forEnglish \[Miller 90\] 2.
Thus, SemCor provides anappropriate nvironment for testing our proceduresand comparing among alternatives in a fullyautomatic way.The automatic decision procedure for lexicalambiguity resolution presented in this paper is basedon an elaboration of the conceptual distance amongconcepts: Conceptual Density \[Agirre & Rigau 95\].Thc system needs to know how words are clustered insemantic classes, and how semantic classes arehierarchically organised.
For this purpose, we haveused WordNet.
Our system tries to resolve the lexicalambiguity ot' nouns by finding the combination ofsenses from a set of contiguous nouns thatmaximises the Conceptual Density among senses.The perlbrmance ofthe procedure was tested on fourSemCor texts chosen at random.
For comparisonpurposes two other approaches, \[Sussna 93\] and\[Yarowsky 92\], were also tried.
The results show thatour algorithm performs better on the test set.Following this short introduction the ConceptualDcnsity formula is presented.
The main procedure toresolve lexical ambiguity of nouns using ConceptualDensity is sketched on section 3.
Section 4 descri'besextensively the experiments and its results.
Finally,sections 5 and 6 deal with further work andconclusions.1Semcor comprises approximately 250,000 words.
Tiletagging was done manually, and the error rate measuredby the authors is around 10% for polysemous words.2The senses of a word are represented by synonym sets(or synscts), one for each word sense.
The nominal partof WordNct can be viewed as a tangled hierarchy ofhypo/hypernymy relations among synsets.
Nominalrelations include also three kinds of meronymicrelations, which can be paraphrased asmember-of, made-of and component-part-of.
The version used in this workis WordNet 1.4, The coverage in WordNet of senses lotopen-class words in SemCor reaches 96% according tothe authors.3_(52 Conceptual Density and WordSense DisambiguationConceptual distance tries to provide a basis formeasuring closeness in meaning among words, takingas reference a structured hierarchical net.
Conceptualdistance between two concepts is defined in IRada etal.
89\] as the length of the shortest path that connectsthe concepts in a hierarchical semantic net.
In asimilar approach, \[Sussna 931 employs the notion ofconceptual distance between etwork nodes in order toimprove precision during document indexing.
\[Resnik95\] captures semantic similarfly (closely related toconceptual distance) by means of the informationcontent of the concepts in a hierarchical net.
Ingeneral these alw;oaches focus on nouns.The measure ()1' conceptual distance among conceptswe are looking for should be scnsflive Io:?
the length of the shortest palh that connects lheconcepts involved.?
the depth in the hierarchy: concepts in a deeperpart of the hierarchy should be ranked closer.?
the density of concepts in the hierarchy: conceptsin a dense part of the hierarchy are relatively closerthan those in a more sparse region.- tile measure should be independent of the lltllllbero1' concepts we are measuring.We have experimented willl several fornmlas thatfollow the four criteria presented above.
Theexperiments reported here were pcrformcd using theConceptual Density formuhl \[Agirre & Rigau 95\],which compares areas of subhierarchies.To illustrate how Conceptual 1)ensity can help todisambiguate a word, in figure I lhe word W has foursenses and several context words.
Each sense of thewords belongs to a subhierarchy of WordNct.
Tile dotsin the subhierarchies represent the senses of eilhcr theword to be disambiguated (W) or the words in thecontext.
Conceptual Density will yield the highestdensity for lhe subhierarchy containing more senses oflhose, rehttive to the total amount of senses in thesubhierarchy.
Tim sense o1' W contained in thesubhierarchy with highest Conceptual l)ensity will bechosen as the sense disambiguating W in the givencontext.
In figure 1, sense2 would be chosen.WW0~d to be disarlJ0iguated: WContext words: wl w2 w3 w4 ...Figure 1: senses of a word in WordNetGiven a concept c, at the top of a sulfifierarchy, andgiven nhyp (mean number of hyponyms per node),the Conceptual Density for c when its subhierarchycontains a number m (nmrks) of senses of the wordsto disambiguate is given by the \[ormula below:m- IZ .0 20nh37~CI)(c, m)- ,::0descendants,,(1)l;ornlula I shows a lmralneter that was COlnputedexperimentally.
The 0.20 tries to smooth theexponential i, as m ranges between I and tim totalnumber of senses in WordNet.
Several values wereIfied for the parameter, and it was found that the bestlmrl'ormanee was attained consistently when theparameter was near 0.20.3 The Disambiguation AlgorithmUsing Conceptual DensityGiven a window size, the program moves thewindow one noun at a time from the beginning of thedocument owards its end, disambiguating in eachstep the noun in the middle of the window andconsidering the other nouns in the window as contexl.Non-noun words are ,lot taken into account.The algorilhm Io disambiguate a given noun w intile middle of a window o1' nouns W (c.f.
figure 2)roughly proceeds its folk)ws:S tep  \].
)S tep  2)S tep  3)Step 4)Step 5)t:r:ee :-: compute  t ree(words  in  w indow)looptree ::: compute  conc(~ptua\] d i s tanco( t ree)concept  -= se\].occt concept  w i th  llighest-._weigth(tree)J.f concept  :: null.
t:hen exJ_tlooptree := inark d:\[sambigui.tted senses  ( t ree,concept)end\ ]  oopoutput  disambJguatJ .on ~esu\].t (tree)Figure 2: algori(hm for each window17First, the algorithm represents in a lattice the nounspresent in the window, their senses and hypernyms(step 1).
Then, the program computes the ConceptualDensity of each concept in WordNet according to thesenses it contains in its subhierarchy (step 2).
Itselects the concept c with highest Conceptual Density(step 3) and selects the senses below it as the correctsenses for the respective words (step 4).The algorithm proceeds then to compute the densityfor the remaining senses in the lattice, and continuesto disambiguate he nouns left in W (back to steps 2,3 and 4).
When no further disambiguation is possible,the senses left for w are processed and the result ispresented (step 5).Besides completely disambiguating a word orfailing to do so, in some cases the disambiguationalgorithm returns everal possible senses for a word.In the experiments we considered these partialoutcomes as failure to disambiguate.4 The  Exper iments4.1 The textsWe selected four texts from SemCor at random: br-a01 (where a stands for gender "Press: Reportage"),br-b20 (b for "Press: Editorial"), br-j09 (j means"Learned: Science") and br-r05 (r for "Humour").Table 1 shows some statistics for each text.text words nouns nouns monosemousin WNbr-a01 2079 564 464 149 (32%)br-ab20 2153 453 377 128 (34%)br-.i09 2495 620 586 205 (34%)br-r05 2407 457 431 120 (27%)total 9134 2094 1858 602 (32%)Table 1 : data for each textAn average of 11% of all nouns in these four textswere not found in WordNet.
According to this data,the amount of monosemous nouns in these texts isbigger (32% average) than the one calculated for theopen-class words fi'om the whole SemCor (27.2%according to \[Miller et al 94\]).For our experiments, these texts play both the rol'eof input files (without semantic tags) and (tagged) testfiles.
When they are treated as input files, we throwaway all non-noun words, only leaving the lemmas ofthe nouns present in WordNet.4.2 Resul ts  and eva luat ionOne of the goals of the experiments was to decideamong different variants of the Conceptual Densityformula.
Results are given averaging the results of thefour files.
Partial disambiguation is treated as failureto disambiguate.
Precision (that is, the percentage ofactual answers which were correct) and recall (that is,the percentage ofpossible answers which were correct)are given in terms of polysemous nouns only.
Graphsare drawn against the size of the context 3 .?
meronymy does  not  improveper fo rmance  as expected.
A priori, the morerelations are taken in account (e.i.
meronymicrelations, in addition to the hypo/hypernymy relation)the better density would capture semantic relatedness,and therefore better esults can be expected.44~I~A 43v 42O -4 414039 % meron- - -o - - -  hyper38 I I i IWindow SizeFigure 3: meronymy and hyperonymyThe experiments ( ee figure 3) showed that there isnot much difference; adding meronymic informationdoes not improve precision, and raises coverage only3% (approximately).
Nevertheless, in the rest of theresults reported below, meronymy and hypernymywere used.?
g lobal  nhyp  is as good as local  nhyp.The average number of hypouyms or nhyp (c.f.formula 1) can be approximated in two ways.
If anindependent hyp is computed for every concept inWordNet we call it local nhyp.
If instead, a uniquenhyp is computed using the whole hierarchy, we haveglobal nhyp.4443A42.o 41 -?~ 40-3938localI I i I o ~, o ,~ ,9,Window SizeFigure 4: local nhyp vs. global nhyp3context size is given in terms of nouns.18While local nhyp is the actual average for a givenconcept, global nhyp gives only an estimation.
Theresults (c.f.
figure 4) show that local nhyp performsonly slightly better.
Therefore global nhyp isfavoured and was used in subsequent experiments.?
context  s i ze :  d i f fe rent  behav ionr  foreach text.
One could assume that the more contextlhere is, the better the disambiguation results wouldbe.
Our experiments how that each file fromSemCor has a different behaviour (c.f.
figure 5) whilebr-b20 shows clear improvement for bigger windowsizes, br-r05 gets a local maximum at a 10 sizewindow, etc.5045v.o 4oIDI:1,3530- - t3 - - -  br-a01 + br-b20+ br-r05 ----o---- br-j09I I o ~, g- -  averageI IWindow SizeFigure 5: context size and different filcsAs each text is structured a list of sentences,lacking any indication of headings, sections,paragraph endings, text changes, etc.
the programgathers the context without knowing whether thenouns actually occur in coherent pieces of text.
Thiscould account for the fact that in br-r05, composedmainly by short pieces of dialogues, the best resultsare for window size 10, the average size of thisdialogue pieces.
Likewise, the results for br-a01,which contains short journalistic texts, are hest forwindow sizes from 15 to 25, decreasing significatlyfor size 30.Ill addition, the actual nature of each text is for surean impommt factor, difficult to measure, which couldaccount for the different behawfiur on its own.
Inorder to give an overall view of the performance, weconsider the average hehaviour.?
file vs. sense.
WordNct groups noun sensesin 24 lexicographer's files.
The algorithm assigns anoun both an specific sense and a file label.
Both filematches and sense matches are interesting to count.Whilc the sense level gives a fine graded measure ofthe algorithm, the file level gives an indication of theperl'ormance if we were interested in a less sharp levelof disambiguation.
The granularity of the sensedistinctions made in \[Hearst, 91\], \[Yarowsky 92\] and\[Gale t al.
93\] also called homographs in \[Guthrie tal.
931\], can be compared to that of the file level inWordNct.For instance, in \[Yarowsky 92\] two homographs oftile noun }liNg are  considered, one characterised asMUSIC and the other as ANIMAL, INSECT.
InWordNet, the 6 senses of I~t~s related to music appearin the following files: ARTIFACT, ATTRIBUTE,COMMUNICATION and PERSON.
The 3 sensesrelated to animals appear in the files ANIMAL andFOOD.
This mcans that while the homograph levelin \[Yarowsky 92\] distinguishes two sets of senses,the file level in WordNet distinguishes six sets ofsenses, still finer in granularity.Figure 6 shows that, as expected, file-level matchesattain better performance (71.2% overall and 53.9%for polysemic nouns) than sense-level matches.55-?.
o  45ID4035- - -0 - -  SenseI I "  - I ' IWindow SizeFigure 6: sense level vs. file level?
eva luat ion  o f  the  results Figure 7 showsthat, overall, coverage over polyscmous nonnsincreases ignificantly with the window size, withoutlosing precision.
Coverage tends to get stabilised near80%, getting little improvement for window sizesbigger than 20.The figure also shows the guessing baseline,given hy selecting senses at random.
This baselinewas first calculated analytically and later checkedexperimentally.
We also compare the performance ofour algorithm with that of the "most frequent"heuristic.
The frequency counts for each sense werecollected using the rest of SemCor, and then appliedto the \['our texts.
While the precision is similar tothat of our algorithm, the coverage is 8% worse.3_980-706O -50-40-Coverage:  ~ semantic density.
.
.
.
.
most  frequentPrecis ion: - - - -0- - -  semantic density.
.
.
.
.
most  frequentguessing3o - - T  \[ T 1Window SizeFigure 7: precision and coverageAll the data for the best window size can be seen intable 2.
The precision and coverage shown in all thepreceding graphs were relative to the polysemousnouns only.
Including monosemic nouns precisionraises, as shown in table 2, from 43% to 64.5%, andthe coverage increases from 79.6% to 86.2%.% w:30 II Cove,-.
I Prec \[Recalloverall File 86.2 71.2 61.4Sense 64.5 55.5polysemic File 79.6 53.9 42.8Sense 43 34.2Table 2: overall data for the best window size4.3  Compar i son  w i th  o ther  worksThe raw results presented here seem to be poorwhen compared to those shown in \[Hearst 91\], \[Galeet al 93\] and \[Yarowsky 9211.
We think that severalfactors make the comparison difficult.
Most of thoseworks focus in a selected set of a few words, generallywith a couple of senses of very different meaning(coarse-grained istinctions), and for which theiralgorithm could gather enough evidence.
On thecontrary, we tested our method with all the nouns ina subset of an unfestricted public domain corpus(more than 9.000 words), making fine-graineddistinctions among all the senses in WordNct.An approach that uses hierarchical knowledge isthat of \[Resnik 9511, which additionally uses theinformation content of each concept gathered fromcorpora.
Unfortunately he applies his method on adifferent ask, that of disambiguating sets of relatednouns.
The evaluation is done on a set of relatednouns from Roger's Thesaurus tagged by hand.
Thefact that some senses were discarded because thehuman judged them not reliable makes comparisoneven more difficult.In order to compare our approach we decided toimplement \[Yarowsky 92\] and \[Sussna 93\], and testthem on our texts.
For \[Yarowsky 92\] we had toadapt it to work with WordNet.
His method relies oncooccurrence data gathered on Roget's Thesaurussemantic ategories.
Instead, on our experiment weuse saliency values 4 based on the lexicographic filetags in SemCor.
The results for a window size of 50nouns are those shown in table 35.
Tile precisionattained by our algorithm is higher.
To comparefigures better consider the results in table 4, were thecoverage of our algorithm was easily extended usingthe version presented below, increasing recall to70.1%.\[+ ii+?v+ i c ,=11 I C.Density 86.2 71.2 J 6 .4Yarowsky 100.0 64.0 1 64.0Table 3: comparison with \[Yarowsky 9211From the methods based on Conceptual Distance,\[Sussna 9311 is the most similar to ours.
Sussnadisambiguates everal documents from a publiccorpus using WordNet.
The test set was tagged byhand, allowing more than one correct senses for asingle word.
The method he uses has to overcome acombinatorial explosion 6 controlling the size of thewindow and "freezing" the senses for all the nounspreceding the noun to be disambiguated.
In order tofi'eeze the winning sense Sussna's algorithm is forcedto make a unique choice.
When Conceptual Distanceis not able to choose a single sense, the algorithmchooses one at random.Conceptual Density overcomes the combinatorialexplosion extending the notion of conceptual distancefrom a pair of words to n words, and therefore canyield more than one correct sense for a word.
Forcomparison, we altered our algorithm to also makerandom choices when unable to choose a single sense.We applied the algorithm Sussna considers best,4We tried both mutual information and association ratio,and the later performed better.5The results of our algorithm are those for window size30, file matches and overall.6In our replication of his experiment he mutualconstraint for the first 10 nouns (tile optimal windowsize according to his experiments) of file br-r05 had todeal with more than 200,000 synset pairs.20discarding the factors that do not  affect performancesignificantly 7,and obtain the results in table 4.% Cover.
\[ Prec.C.l)ensity File I00.0 70.1Sense 6(1.1Sussna File 100.0 64.5Sense 52.3Table 4: comparison with \[St, ssna 931A more thorougla comparison with these methodscould he desirable, hut not possible in this paper l'orthe sake of conciseness.might be only one of a number of complementaryevidences of the plausibility ol'a certain word sense.Furthermore, WordNet 1.4 is not a complete lexicaldatabase (current version is 1.5).?
Tune  the sense  d is t inc t ions  to  the  leve lbest  su i ted  fo r  the  app l i ca t ion .
On the onehand the sense distinctions made by WordNet 1.4 arcnot always satisl'actory.
On tire other hand, ouralgorithm is not designed to work on the file level,e.g.
il' the sense level is unable to distinguish amongtwo senses, the file level also fails, even if bothsenses were fronl the same file.
If the senses werecollapsed at the file level, the coverage and precisionof tile algorithm at the file level might be even better.5 Further WorkWe would like to have included in this paper astudy on whether there is or not a correlation amongcorrect and erroneous sense assignations and thedegree of Conceptual Density, that is, the actualfigure held by fommla I.
If this was the case, theerror rate could be furtber decreased setting a ccrtainlhreshold for Conceptual Density wdues of wilmingsenses.
We would also like to evaluate the usel'ulnessof partia~l disambiguation: decrease of ambiguity,number of times correct sense is among the chosenones, etc.There are some factors that could raise theperformmace of our algorithm:?
Work  on  coherent  chunks  o f  text .Unfortunately any information about discoursestructure is absent in SemCor, apart from sentenceendings Thc performance would gain from the factlhat sentences from unrelated topics wouht not beconsidered in the disamhiguation window.?
Ex tend  and  improve  the semant ic  data .WordNet provides sinonymy, hypernymy andmeronyny relations for nouns, but other relations aremissing.
For instance, WordNet lacks eross-categorialsemantic relations, which could he very useful toextend the notion of Conceptual Density of nouns toConceptual Density of words.
Apart from extendinglhe disambiguation to verbs, adjectives and adverbs,cross-catcgorial relations would allow to capture betterlhe relations alnong senses and provide firmer groundsfor disambiguating.These other relations could be extracted from otherknowledge sources, both corpus-based or MRD-based.If those relations could be given on WordNet senses,Conceptual Density could profit from them.
It is ot, rbelief, following the ideas of \[McRoy 92\] that full-fledged lexical ambiguity resolution should combineseveral information sources.
Conceptual Density"/Initial mutual constraint size is 10 and window size'is41.
Meronymic links are also considered.
All the linkshave the same weigth.6 ConclusionThe automatic method for the disambiguation ofnouns presented in this papcr is ready-usable in anygeneral domain and on free-running text, given part ofspeech tags.
It does not need any training and usesword sense tags from WordNet, an extensively usedIcxieal data base.Conceptual Density has been used for other tasksapart from the disambiguation of free-running test.
Itsapplication for automatic spelling correction isoutlined in tAgirre ct al.
94\].
It was also used onComputational Lexicography, enriching dictionarysenses with semantic tags extracted from WordNet\[Rigau 9411, or linking bilingual dictionaries toWordNet \[Rigau and Agirre 96\].In the experiments, the algorithm disambiguated\['our texts (about 10,000 words long) of SemCor, asubset of the Brown corpus.
The results were obtainedautomatically comparing the tags in SemCor withthose computed by the algorithm, which would allowthe comparison with other disambiguation methods.Two other methods, \[Sussna 93\] and \[Yarowsky 92\],were also tried on the same texts, showing that ouralgorithm performs better.Results are promising, considering the difficnlty ofthe task (free running text, large number of senses perword in WordNet), and the htck o1' any discoursestructure of the texts.
Two types el' results can beobtaincd: the specific scnse or a coarser, file level,tag.AcknowledgementsThis work, partially described ill \[Agirre &Rigau 9611,was started in the Computing Research Laboratory inNew Mexico State University.
We wish to thank all thestaff of the CRL and specially Jim Cowie, Joe Guthtrie,Louise Guthrie and David l"arwell.
We woukl also like tothank Xabier Arregi, Jose mari Arriola, Xabier Artola,Arantza Dfaz de llarraza, Kepa Sarasola nd Aitor Soroafiom the Computer Science Faculty of EHU and FraneescRibas, ltoracio Rodrfguez and Alicia Ageno from theComputer Science Department of UPC.22ReferencesAgirre E., Arregi X., Diaz de Ilarraza A. and SarasolaK.
1994.
Conceptual Distance and AutomaticSpelling Correction.
in Workshop on Speechrecognition and handwriting.
Leeds, England.Agirre E., Rigau G. 1995.
A Proposal for WordSense Disambiguation using conceptual Distance,International Conference on Recent Advances inNatural Language Processing.
Tzigov Chark,Bulgaria.Agirre, E. and Rigau G. 1996.
An Experiment inWord SenseDisambiguation of the Brown CorpusUsing WordNet.
Memoranda in Computer andCognitive Science, MCCS-96-291, ComputingResearch Laboratory, New Mexico StateUniversity, Las Cruces, New Mexico.Cowie J., Guthrie J., Guthrie L. 1992.
LexicalDisambiguation using Simulated annealing, inproceedings of DARPA WorkShop on Speech andNatural Language, New York.
238-242.Francis S. and Kucera H. 1967.
Computing analysisof present-day American English, Providenc, RI:Brown University Press, 1967.Gale W., Church K. and Yarowsky D. 1993.
AMethod for Disambiguating Word Sense sin aLarge Corpus, in Computers and the Humanities,n.
26.Guthrie L., Guthrie J. and Cowie J.
1993.
ResolvingLexical Ambiguity, in Memoranda in Computerand Cognitive Science MCCS-93-260,Computing Research Laboratory, New MexicoState University.
Las Cruces, New Mexico.Hearst M. 1991.
Towards Noun HomonymDisambiguation Using Local Context in LargeText Corpora, in Proceedings of the SeventhAnnual Conference of the UW Centre for the NewOED and Text Research.
Waterloo, Ontario.McRoy S. 1992.
Using Multiple Knowledge Sourcesfor Word Sense Discrimination, ComputationalLinguistics, vol.
18, num.
1.Miller G. 1990.
Five papers on WordNet, SpecialIssue of International Journal of Lexicogrphy3(4).
1990.Miller G. Leacock C., Randee T. and Bunker R.1993.
A Semantic Concordance, in proceedings ofthe 3rd DARPA Workshop on Human LanguageTechnology, 303-308, Plainsboro, New Jersey.Miller G., Chodorow M., Landes S., Leacock C. andThomas R. 1994.
Using a Semantic Concordancefor sense Identification, in proceedings of ARPAWorkshop on Human Language Technology, 232-235.Rada R., Mill H., Bicknell E. and Blettner M. 1989.Development an Applicationof a Metric onSemantic Nets, in IEEE Transactions onSystems, Man and Cybernetics, vol.
19, no.
1,17-30.Resnik P. 1995.
Disambiguating Noun Groupingswith Respect o WordNet Senses, in Proceedingsof the Third Workshop on Very Large Corpora,MIT.Richardson R., Smeaton A.F.
and Murphy J.
1994.Using WordNet as a Konwledge Base forMeasuring Semantic Similarity between Words,in Working Paper CA-1294, School of ComputerApplications, Dublin City University.
Dublin,h'eland.Rigau G. 1994.
An experiment on AutomaticSemantic Tagging of Dictionary Senses,WorkShop "The Future of Dictionary", Aix-les-Bains, France.
published as Research Report LSI-95-31-R. Computer Science Department.
UPC.Bm'celona.Rigau G. and Agirre E. 1996.
Linking BilingualDictionaries to WordNet, in proceedings of the7th Euralex International Congress onLexcography (Euralex'96), Gothenburg, Sweden,1996.Sussna M. 1993.
Word Sense Disambiguation forFree-text Indexing Using a Massive SemanticNetwork, in Proceedings of the SecondInternational Conference on Information andknowledge Management.
Arlington, Virginia.Voorhees E. 1993.
Using WordNet o DisambiguateWord Senses for Text Retrival, in proceedings ofthe Sixteenth Annual International ACM SIGIRConference on Research and Developement inInformation Retrieval, pages 171-180, PA.Wilks Y., Fass D., Guo C., McDonal J., Plate T.and Slator B.
1993.
Providing Machine TractablleDictionary Tools, in Semantics and the Lexicon(Pustejovsky J.
ed.
), 341-401.Yarowsky, D. 1992.
Word-Sense DisambiguationUsing Statistical Models of Roget's CategoriesTrained on Ixtrge Corpora, in proceedings of the15th International Conference on ComputationalLinguistics (Coling'92).
Nantes, France.22
