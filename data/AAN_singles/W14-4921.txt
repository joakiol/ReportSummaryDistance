LAW VIII - The 8th Linguistic Annotation Workshop, pages 149?158,Dublin, Ireland, August 23-24 2014.Situation entity annotationAnnemarie Friedrich Alexis PalmerDepartment of Computational LinguisticsSaarland University, Saarbr?ucken, Germany{afried,apalmer}@coli.uni-saarland.deAbstractThis paper presents an annotation scheme for a new semantic annotation task with relevance foranalysis and computation at both the clause level and the discourse level.
More specifically, welabel the finite clauses of texts with the type of situation entity (e.g., eventualities, statementsabout kinds, or statements of belief) they introduce to the discourse, following and extendingwork by Smith (2003).
We take a feature-driven approach to annotation, with the result thateach clause is also annotated with fundamental aspectual class, whether the main NP referent isspecific or generic, and whether the situation evoked is episodic or habitual.
This annotation isperformed (so far) on three sections of the MASC corpus, with each clause labeled by at leasttwo annotators.
In this paper we present the annotation scheme, statistics of the corpus in itscurrent version, and analyses of both inter-annotator agreement and intra-annotator consistency.1 IntroductionLinguistic expressions form patterns in discourse.
Passages of text can be analyzed in terms of theindividuals, concepts, times and situations that they introduce to the discourse.
In this paper we intro-duce a new semantic annotation task which focuses on the latter and in particular their aspectual nature.Situations are expressed at the clause level; situation entity (SE) annotation is the task of associatingindividual clauses of text with the type of SE introduced to the discourse by the clause.
Following Smith(2003), we distinguish the following SE types (see Sec.
3.1): EVENTS, STATES, GENERALIZING SEN-TENCES, GENERIC SENTENCES, FACTS, PROPOSITIONS, QUESTIONS and IMPERATIVES.
Althoughthese categories are clearly distinct from one another on theoretical grounds, in practice it can be difficultto cleanly draw boundaries between them.
We improve annotation consistency by defining the SE typesin terms of features whose values are easier for annotators to identify, and which provide guidance fordistinguishing the more complex SE types.As with most complex annotation tasks, multiple interpretations are often possible, and we cannotexpect agreement on all instances.
The feature-driven approach (see Sec.
3.2) is a valuable source ofinformation for investigating annotator disagreements, as the features indicate precisely how annotatorsdiffer in their interpretation of the situation.
Analysis of intra-annotator consistency shows that personalpreferences of annotators play a role, and we conclude that disagreements often highlight cases wheremultiple interpretations are possible.
We further argue that such cases should be handled carefully insupervised learning approaches targeting methods to automatically classify situation entity types.As the first phase of the SE annotation project, we are in the process of annotating the written portionof MASC (Ide et al., 2010), the manually-annotated subcorpus of the Open American National Corpus.MASC provides texts from 20 different genres and has already been annotated with various linguisticand semantic phenomena.1MASC offers several benefits: it includes text from a wide variety of genres,it facilitates study of interactions between various levels of analysis, and the data is freely availablewith straightforward mechanisms for distribution.
In this paper we report results for three of the MASCThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1http://www.americannationalcorpus.org/MASC/Full_MASC.html149genres: news, letters, and jokes.
Once a larger portion of MASC has been labeled with SEs and theirassociated features, we will add our annotations to those currently available for MASC.
We mark the SEtypes of clauses with the aim of providing a large corpus of annotated text for the following purposes:(1) To assess the applicability of SE type classification as described by Smith (2003): to what extentcan situations be classified easily, which borderline cases occur, and how do humans perform on thistask?
(see Sec.
4)(2) Training, development and evaluation of automatic systems classifying situation entities, as wellas sub-tasks which have (partially) been studied by the NLP community, but for which no largeannotated corpora are available (for example, automatically predicting the fundamental aspectualclass of verbs in context (Friedrich and Palmer, 2014) or the genericity of clauses and noun phrases).
(3) To provide a foundation for analysis of the theory of Discourse Modes (Smith, 2003), which weexplain next (Sec.
2).2 Background and related workWithin a text, one recognizes stretches that are intuitively of different types and can be clustered by theircharacteristic linguistic features and interpretations.
Smith (2003) posits five discourse modes: Narrative,Report, Description, Informative and Argument/Commentary.
Texts of almost all genre categories havepassages of different modes.
The discourse modes are characterized by (a) the type of situations (alsocalled situation entities) introduced in a text passage, and (b) the principle of text progression in themode (temporal or atemporal, and different manners of both temporal and atemporal progression).
Thisannotation project directly addresses the first of these characteristics, the situation entity types (SE types).Some previous work has addressed the task of classifying SE types at the clause level.
Palmer et al.
(2004) enrich LFG parses with lexical information from both a database of lexical conceptual structures(Dorr, 2001) and hand-collected groups of predicates associated with particular SE types.
The enrichedparses are then fed to an ordered set of transfer rules which encode linguistic features indicative of SEtypes.
The system is evaluated on roughly 200 manually-labeled clauses.
Palmer et al.
(2007) investigatevarious types of linguistic features in a maximum entropy model for SE type classification.
The bestresults are still below 50% accuracy (with a most-frequent-class baseline of 38%), and incorporatingfeatures from neighboring clauses is shown to increase performance.
Palmer et al.
(2007) annotate datafrom one section of the Brown corpus and a small amount of newswire text, with two annotators andno clear set annotation guidelines.
In addition, work by Cocco (2012) classifies clauses of French textaccording to a six-way scheme that falls somewhere between the SE level and the level of discoursemodes.
The types are: narrative, argumentative, descriptive, explicative, dialogal, and injunctive.Other related works address tasks related to the features we annotate.
One strand of work is in auto-matic classification of aspectual class (Siegel and McKeown, 2000; Siegel, 1999; Siegel, 1998; Klavansand Chodorow, 1992; Friedrich and Palmer, 2014) and its determination as part of temporal classification(UzZaman et al., 2013; Bethard, 2013; Costa and Branco, 2012).
A second aims to distinguish genericvs.
specific clauses (Louis and Nenkova, 2011) or to identify generic noun phrases (Reiter and Frank,2010).
The latter work leverages data with noun phrases annotated as either generic and specific fromthe ACE-2 corpus (Mitchell et al., 2003); their definitions of these two types match ours (see Sec.
3.2.1).3 Annotation Scheme and ProcessIn this section, we first present the inventory of SE types (Sec.
3.1).
We then describe our feature-driven approach to annotation (Sec.
3.2) and define the SE types with respect to three situation-relatedfeatures: main referent type, fundamental aspectual class, and habituality.
Some situation entity typesare easier to recognize than others.
While some can be identified on the basis of surface structure andclear linguistic indicators, others depend on internal temporal (and other) properties of the verb and itsarguments.
Annotators take the following approach: first, easily-identifiable SE types (Speech Acts andAbstract Entities) are marked.
If the clause?s SE type is not one of these, values for the three features aredetermined, and the final determination of SE type is based on the features.1503.1 Situation entity typesFollowing Smith (2003), we distinguish the following SE types:Eventualities.
These types describe particular situations such as STATES (1a) or EVENTS (2).
The typeREPORT, a subtype of EVENT, is used for situations introduced by verbs of speech (1b).
(1) (a) ?Carl is a tenacious fellow?, (STATE)(b) said a source close to USAir.
(EVENT ?
REPORT)(2) The lobster won the quadrille.
(EVENT)General Statives.
This class includes GENERALIZING SENTENCES (3), which report regularities re-lated to specific main referents, and GENERIC SENTENCES (4), which make statements about kinds.
(3) Mary often feeds my cats.
(GENERALIZING)(4) The lion has a bushy tail.
(GENERIC)Abstract Entities are the third class of SE types, and comprise FACTS (5) and PROPOSITIONS (6).These situations differ from the other types in how they relate to the world: Eventualities and GeneralStatives are located spatially and temporally in the world, but Abstract Entities are not.
FACTS are objectsof knowledge and PROPOSITIONS are objects of belief from the respective speaker?s point of view.
(5) I know that Mary refused the offer.
(FACT)(6) I believe that Mary refused the offer .
(PROPOSITION)We limit the annotation of Abstract Entities to the clausal complements of certain licensing predicates,as well as clauses modified by a certain class of adverbs, as it is not always possible to identify sentencesdirectly expressing Facts or Propositions on linguistic grounds (Smith, 2003).
In (6), believe is thelicensing predicate, and Mary refused the offer is a situation that is introduced as not being in the world,but about the world (Smith, 2003).
Annotators are asked to additionally label the embedded SE typewhen possible.
For example, that Mary refused the offer in (5) and (6) would be labeled as EVENT.Speech Acts.
This class comprises QUESTIONS and IMPERATIVE clauses (Searle, 1969).Derived SE types.
In some cases, the SE type of a clause changes based on the addition of somelinguistic indication of uncertainty about the status of the situation described.
We refer to these as derivedSE types.
More specifically, clauses that would otherwise be marked as EVENT may be coerced to thetype STATE due to negation, modality, future tense, conditionality, and sometimes subjectivity: e.g.
Johndid not win the lottery, a negated event, introduces a STATE to the discourse.3.2 Features for distinguishing situation entity typesIn this section, we describe three features that allow for the clear expression of differences between SEtypes.
Fleshing out the descriptions of SE types with these underlying features is useful to convey theannotation scheme to new annotators, to get partial information when an annotator has trouble making adecision on SE type, and to analyze disagreements between annotators.3.2.1 Main referent type: specific or genericThis feature indicates the type of the most central entity mentioned in the clause as a noun phrase.
Werefer to this entity as the clause?s main referent.
This referent can be found by asking the question: Whatis this clause about?
Usually, but not always, the main referent of a clause is realized as its grammaticalsubject.
We appeal to the annotator?s intuitions in order to determine the main referent of a clause.
Incase the main referent does not coincide with the grammatical subject as in example (7), this is to beindicated during annotation.
(7) There are two books on the table.
(specific main referent, STATE)151Some SE types (STATES, GENERALIZING SENTENCES and GENERIC SENTENCES, for details seeTable 1) are distinguished by whether they make a statement about some specific main referent or abouta generic main referent.
Specific main referents are particular entities (8), particular groups of entities (9),organizations (10), particular situations (11) or particular instantiations of a concept (12).
(8) Mary likes popcorn.
(particular entity ?
specific, STATE)(9) The students met at the cafeteria.
(a particular group ?
specific, STATE)(10) IBM was a very popular company in the 80s.
(organization ?
specific, STATE)(11) That she didn?t answer her phone really upset me.
(particular situation ?
specific, EVENT)(12) Today?s weather was really nice.
(particular instantiation of a concept ?
specific, STATE)The majority of generic main referents are noun phrases referring to a kind rather than to a particularentity, and generic mentions of concepts or notions (14).
Definite NPs and bare plural NPs (13) are themain kind-referring NP types (Smith, 2003).
(13) The lion has a bushy tail.
/ Dinosaurs are extinct.
(generic, GENERIC SENTENCE)(14) Security is an important issue in US electoral campaigns.
(generic, GENERIC SENTENCE)While some NPs clearly make reference to a well-established kind, other cases are not so clear cut,as humans tend to make up a context in which an NP describes some kind (Krifka et al., 1995).
Sen-tence (15) gives an example for such a case: while lions in captivity are not a generally well-establishedkind, this term describes a class of entities rather than a specific group of lions in this context.
(15) Lions in captivity have trouble producing offspring.
(generic, GENERIC SENTENCE)Gerunds may occur as the subject in English sentences.
When they describe a specific process as in(16a), we mark them as specific.
If they instead describe a kind of process as in (16b), we mark them asgeneric.
(16) (a) Knitting this scarf took me 3 days.
(specific, EVENT)(b) Knitting a scarf is generally fun.
(generic, GENERIC SENTENCE)We also give annotators the option to explicitly mark the main referent as expletive, as in (17).
(17) It seemed like (expletive = no main referent, STATE)he would win.
(specific, STATE)3.2.2 Fundamental aspectual class: stative or dynamicFollowing Siegel and McKeown (2000), we determine the fundamental aspectual class of a clause.
Thisnotion is the extension of lexical aspect or aktionsart, which describe the ?real life shape?
of situationsdenoted by verbs, to the level of clauses.
More specifically, aspectual class is a feature of the main verband a select group of modifiers, which may differ per verb.
The stative/dynamic distinction is the mostfundamental distinction in taxonomies of aspectual class (Vendler, 1967; Bach, 1986; Mourelatos, 1978).We allow three labels for this feature: dynamic for cases where the verb and its arguments describesome event (something happens), stative for cases where they introduce some properties of the mainreferent to the discourse, or both for cases where annotators see both interpretations.It is important to note that the fundamental aspectual class of a verb can be different from the typeof situation entity introduced by the clause as a whole.
The basic situation type of building a house isdynamic, and in the examples below we see this fundamental aspectual class appearing in clauses withdifferent situation entity types.
Example (18) describes an EVENT.
Clause (19), on the other hand, is aGENERALIZING SENTENCE, as it describes a pattern of events; this is a situation with a derived type.The same is true for example (20), which is a STATE because of its future tense.
(18) John built a house.
(EVENT, dynamic fundamental aspectual class)(19) John builds houses.
(GENERALIZING SENTENCE, dynamic fundamental aspectual class)(20) John is going to build a house.
(STATE, dynamic fundamental aspectual class)1523.2.3 HabitualityAnother dimension along which situations can be distinguished is whether they describe a static state, aone-time (episodic) event (21) or some regularity of an event (22) or a state (23), which is labeled ha-bitual.
The term habitual as used in this annotation project covers more than what is usually considereda matter of habit, extending to any clauses describing regularities (24).
The discussion related to thislinguistic feature in this section follows Carlson (2005).
If one can add a frequency adverbial such astypically/usually to the clause and the meaning of the resulting sentence differs at most slightly from themeaning of the original sentence, or the sentence contains a frequency adverbial such as never, the sen-tence expresses a regularity, i.e., is habitual.
Another property of habituals is that they are generalizationsand hence have the property of tolerating exceptions.
If we learn that Mary eats oatmeal for breakfast, itdoes not necessarily need to be true that she eats oatmeal at every breakfast.
It is important to note thatunlike fundamental aspectual class, habituality is an attribute of the entire situation.
(21) Mary ate oatmeal for breakfast this morning.
(episodic, EVENT)(22) Mary eats oatmeal for breakfast.
(habitual, GENERALIZING SENTENCE)(23) I often feel as if I only get half the story.
(habitual, stative fundamental aspectual class, GENER-ALIZING SENTENCE)(24) Glass breaks easily.
(habitual, GENERIC SENTENCE)3.3 SE types and their featuresThe feature-driven approach to annotation taken here is defined such that, ideally, each unique combina-tion of values for the three features leads to one SE type.
Table 1 shows the assignment of SE types tovarious combinations of feature values.
This table covers all SE types except ABSTRACT ENTITIES andSPEECH ACTS, which are more easily identifiable based on lexical and/or syntactic grounds.
Annotatorsare also provided with information about linguistic tests for some SE types and feature values, both formaking feature value determinations and to support selection of clause-level SE type labels.SE type main referent aspectual class habitualityEVENTspecificeventive episodicgenericSTATE specific stative staticGENERIC SENTENCE genericeventive habitualstative static, habitualGENERALIZINGspecificeventivehabitualSENTENCE stativeGeneral Stativespecificeventive habitualgenericTable 1: Situation entity types and their features.4 Annotator agreement and consistencyThis section presents analyses of inter-annotator agreement and intra-annotator consistency, looking atagreement for individual feature values as well as clause-level SE type.4.1 Data and annotatorsThe current version of our corpus consists of three sections (news, letters and jokes) of MASC corpus(Ide et al., 2010).
We hired three annotators, all either native or highly-skilled speakers of English, andhad a training phase of 3 weeks using several Wikipedia documents.
Afterwards, annotation of the textsbegan and annotators had no further communication with each other.
Two annotators (A and B) eachmarked the complete data set, and one additional annotator (C) marked the news section only.153ANNOTATORS NUMBER OF MAIN ASPECTUAL HABITUALITY SE TYPE SE TYPESEGMENTS REFERENT CLASS (REP=EVT)A:B 2563 0.35 0.81 0.77 0.56 0.66A:C 2524 0.29 0.77 0.76 0.55 0.65B:C 2556 0.45 0.73 0.76 0.76 0.74average 2545 0.36 0.77 0.76 0.62 0.68Table 2: Cohen?s ?, for pairs of annotators on the MASC news section.GENRE NUMBER OF MAIN ASPECTUAL HABITUALITY SE TYPE SE TYPESEGMENTS REFERENT CLASS (REP=EVT)jokes 3455 0.57 0.85 0.81 0.74 0.73news 2563 0.35 0.81 0.77 0.56 0.66letters 1851 0.41 0.71 0.65 0.56 0.56all 7869 0.47 0.80 0.77 0.64 0.68Table 3: Cohen?s ?, for two annotators on three different sections of MASC.4.2 Segmentation into clausesWe segment the texts into finite clauses using the SPADE discourse parser (Soricut and Marcu, 2003),applying some heuristic post-processing and allowing annotators to mark segments that do not containa situation (for instance, headlines or by-lines) or that should be merged with another segment in orderto describe a complete situation.
We filter out all segments marked by any annotator as having a seg-mentation problem.
Of the 2823 segments automatically created for the news section, 4% were markedas containing no situation by at least one of the three annotators, and 7% were merged to a differentsegment by at least one annotator.
All three annotators agree on the remaining 2515 segments (89%).
Ofthe 9428 automatically-created segments in the full data set, 11.5% were marked as no-situation by atleast one of two annotators, and a further 5% were merged to other segments by at least one annotator.7869 segments remain for studying agreement between two annotators on the full data set.The three genres vary as to the average segment length.
Segments in the letters texts have the longestaverage length (11.1 tokens), segments in jokes are the shortest (6.9 tokens on average), and segments innews fall in the middle with an average length of 9.9 tokens.4.3 Inter-annotator agreementAs we allow annotators to mark a segment as Speech Acts or Abstract Entities and in addition mark theSE type of the embedded situation with a non-surface type, we compute agreement for Eventualities andGeneral Statives in the following, and present the results for Speech Acts and Abstract Entities separately.news section, 3 annotators.
We compute Cohen?s unweighted ?
between all three pairs of annotatorsfor the news section, as shown in Table 2.
We compute agreement for the segments where both respectiveannotators agree on the segmention, i.e., that the segment describes a situation.
For aspectual class, wecompute agreement over the three labels stative, dynamic and both; for main referents, we computeagreement over the three labels specific, dynamic and expletive; for habituality, we compute agreementover the three labels episodic, habitual and static.
In each case, we omit segments for which one of theannotators did not give a label, which in each case are fewer than 26 segments.We observe good agreement for the features aspectual class and habituality, and for SE type betweenannotators B and C. Pairs involving annotator A reach lower agreement; we identify two causes.
Anno-tator A marks many segments marked as REPORT by the others as the corresponding supertype EVENT.This shows up in Table 2 as higher values of ?
when considering REPORT to match its supertype EVENT.The second cause is A?s different preference for marking main referents, causing lower ?
scores for agree-ment on the main referent type and also influencing agreement for situation entity types.
In more than92% of the 183 clauses on which annotators B and C agree with each other, but disagree with A, Band C assigned the value specific while A marked the main referent as generic.
Early in the annotationproject, a revision was made to the scheme for labeling main referents ?
one hypothesis is that A mightnot have updated her way of labeling these.
We estimate that roughly 40% of these cases were due to154A?s misunderstanding of feature value definitions, but around 30% of these cases do allow for both inter-pretations.
In the following sentence, the main referent of the second segment could either refer to thespecific set of all kids in New York, or to the class of children in New York: As governor, I?ll make sure// that every kid in New York has the same opportunity.
Another frequent case is the main referent you,which can be interpreted in a generic way or as specifically addressing the reader (e.g.
of a letter).
Suchdisagreements at the level of feature annotations allow us to detect cases where several interpretationsare possible.
Having annotators with different preferences on difficult cases can actually be a valuablesource of information for identifying such cases.The distribution of labels for main referents is highly skewed towards specific main referents for thenews section; when comparing B and C, they agree on 2358 segments to have a specific main referent.However, only 122 segments are labeled as having a generic main referent by at least one annotator, andthey agree only on 43 of them.
A further 49 are labeled generic by B but specific by C and a further 30vice versa.
In order to collect more reliable data and agreement numbers for the task of labeling mainreferent types, we plan to conduct a focused study with a carefully-balanced data set.news, jokes, letters: 2 annotators.
We report agreement for three sections, corresponding to threegenres, for two annotators (A and B) in Table 3.
We observe higher agreement for jokes than for news,and higher agreement for news than for letters.
Figure 1 shows the distribution of situation entity typesper genre.
The numbers express averages of percentages of label types assigned to the clauses of onegenre by the two annotators.
The letters genre is different in that it has more STATES, far fewer EVENTS,which are usually easy to detect, and more General Statives.
Most cases of confusion between annotatorsoccur between General Statives and STATES, so the more EVENTS texts have, the higher the agreement.letters news jokes0%20%40%60% STATEEVENTGENERALIZING SENTENCEGENERIC SENTENCEFigure 1: Distribution of situation entity types in three different genres.Speech Acts and Abstract Entities.
Figure 2 shows the percentage of segments of each genre thatwere marked as a Speech Act or an Abstract Entity by at least one annotator.
QUESTIONS are mostfrequent in the jokes genre, but about half of them are just marked by one annotator, which has to do withhow consistently indirect questions are marked.
The two annotators agree on almost all segments labeledas imperatives; while there are only very few IMPERATIVES in the news section, there are more in thejokes and letters sections.
The letters are mainly fund-raising letters, which explains the high percentageof IMPERATIVES (Please help Goodwill.
// Use the enclosed card // and give a generous gift today.
).FACTS and PROPOSITIONS, on the other hand, are rather infrequent in any genre, and annotators tend tomark them inconsistently.
We take from this analysis that we need to offer some help to the annotators indetecting Abstract Entities.
We plan to compile a list of verbs that may introduce Abstract Entities andspecifically highlight potential licensing constructions in order to increase recall for these types.4.4 Intra-annotator consistencyAfter the first round of annotation, we identified 11 documents with low inter-annotator agreement onSE type (5 news, 5 letters, 1 jokes) and presented them to two annotators for re-annotation.
For eachannotator, the elapsed time between the first and second rounds was at least 3 weeks.
We observe that ingeneral, the agreement of each annotator with herself is greater than agreement with the other annotator.This shows that the disagreements are not pure random noise, but that annotators have different prefer-ences for certain difficult decisions.
It is interesting to note that annotator B apparently changed how155letrs nrws jroorks0%024%026%02S%02TA%02 EVNENGRLRNIZ ktrC???
?enrZ ktrC???
?
?eo?letrs nrws jroorks0%024%026%02S%02TA%02 ??
?Lletrs nrws jroorks0%024%026%02S%02TA%02 R?E?V?LR?
?letrs nrws jroorks0%024%026%02S%02TA%02 ??
?GLRNIFigure 2: Percentage of segments marked as Speech Act or Abstract Entity by at least one annotator.GENRE NUMBER OF MAIN ASPECTUAL HABITUALITY SE TYPE SE TYPESEGMENTS REFERENT CLASS (REP=EVT)A1:B1 636 0.15 0.79 0.64 0.40 0.45A2:B2 599 0.12 0.78 0.70 0.42 0.48A1:A2 596 0.79 0.88 0.78 0.75 0.75B1:B2 620 0.55 0.84 0.78 0.75 0.75Table 4: Consistency study: Cohen?s ?, for two annotators, comparing against each other and againstthemselves (re-annotated data).
A1 = annotator A in first pass, B2 = annotator B in second pass etc.she annotates main referents; possibly this is also due to the above mentioned revision to the annotationscheme.
On the other hand, B annotated very few segments as generic (only 61 segments were markedas having a generic main referent in either the first or second pass, 27 of them in both passes), whichmay also have led to the low ?
value.
The fact that annotators do disagree with themselves indicates thatthere are noisy cases in our data set, where multiple interpretations are possible.
However, we want topoint out that the level of noise estimated by this intra-annotator consistency study is an upper bound aswe chose the most difficult documents for re-annotation; the overall level of noise in the data set can beassumed to be much lower.5 ConclusionWe have presented an annotation scheme for labeling clauses with their situation entity type along withfeatures indicating the type of main referent, fundamental aspectual class and habituality.
The feature-driven approach allows for a detailed analysis of annotator disagreements, showing in which way theannotators?
understandings of a clause differ.
The analysis in the previous chapter showed that whilegood inter-annotator agreement can be reached for most decisions required by our annotation schema,there remain hard cases, on which annotators disagree with each other or with their own first roundof annotations.
We do not yet observe satisfying agreement for main referent types or for identifyingabstract entities.
In both cases, data sparseness is a problem; there are only very few generic mainreferents and abstract entities in our current corpus.
We plan to conduct case studies on data that isspecifically selected for these phenomena.However, in many of the hard cases, several readings are possible.
Rather than using an adjudicateddata set for training and evaluation of supervised classifiers for labeling clauses with situation entities,we plan to leverage such disagreements for training, following proposals by Beigman Klebanov andBeigman (2009) and Plank et al.
(2014).The annotation reported here is ongoing; our next goal is to extend annotation to additional genreswithin MASC, starting with essays, journal, fiction, and travel guides.
Following SE annotation, we willextend the project to annotation of discourse modes.
Finally, we are very interested in exploring andannotating SEs in other languages, as we expect a similar inventory but different linguistic realizations.Acknowledgments We thank the anonymous reviewers, Bonnie Webber and Andreas Peldszus forhelpful comments, and our annotators Ambika Kirkland, Ruth K?uhn and Fernando Ardente.
This re-search was supported in part by the MMCI Cluster of Excellence, and the first author is supported by anIBM PhD Fellowship.156ReferencesEmmon Bach.
1986.
The algebra of events.
Linguistics and philosophy, 9(1):5?16.Beata Beigman Klebanov and Eyal Beigman.
2009.
From annotator agreement to noise models.
ComputationalLinguistics, 35(4):495?503.Steven Bethard.
2013.
ClearTK-TimeML: A minimalist approach to TempEval 2013.
In Second Joint Conferenceon Lexical and Computational Semantics (* SEM), volume 2, pages 10?14.Greg Carlson.
2005.
Generics, habituals and iteratives.
The Encyclopedia of Language and Linguistics.Christelle Cocco.
2012.
Discourse type clustering using pos n-gram profiles and high-dimensional embeddings.In Proceedings of the Student Research Workshop at the 13th Conference of the European Chapter of the Asso-ciation for Computational Linguistics, EACL 2012.Francisco Costa and Ant?onio Branco.
2012.
Aspectual type and temporal relation classification.
In Proceedings ofthe 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages266?275.Bonnie J. Dorr.
2001.
LCS verb database.
Online software database of Lexical Conceptual Structures, Universityof Maryland, College Park, MD.Annemarie Friedrich and Alexis Palmer.
2014.
Automatic prediction of aspectual class of verbs in context.
InProceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL).
Baltimore,USA.Nancy Ide, Christiane Fellbaum, Collin Baker, and Rebecca Passonneau.
2010.
The manually annotated sub-corpus: A community resource for and by the people.
In Proceedings of the ACL 2010 conference short papers,pages 68?73.Judith L. Klavans and Martin S. Chodorow.
1992.
Degrees of stativity: The lexical representation of verb aspect.In Proceedings of the 14th COLING, Nantes, France.Manfred Krifka, Francis Jeffry Pelletier, Gregory Carlson, Alice ter Meulen, Gennaro Chierchia, and GodehardLink.
1995.
Genericity: an introduction.
The Generic Book, pages 1?124.Annie Louis and Ani Nenkova.
2011.
Automatic identification of general and specific sentences by leveragingdiscourse annotations.
In Proceedings of IJCNLP 2011.Alexis Mitchell, Stephanie Strassel, Mark Przybocki, JK Davis, George Doddington, Ralph Grishman, Adam Mey-ers, Ada Brunstein, Lisa Ferro, and Beth Sundheim.
2003.
ACE-2 Version 1.0.
Linguistic Data Consortium,Philadelphia.Alexander PD Mourelatos.
1978.
Events, processes, and states.
Linguistics and philosophy, 2(3):415?434.Alexis Palmer, Jonas Kuhn, and Carlota Smith.
2004.
Utilization of multiple language resources for robustgrammar-based tense and aspect classification.
In Proceedings of LREC 2004.Alexis Palmer, Elias Ponvert, Jason Baldridge, and Carlota Smith.
2007.
A sequencing model for situation entityclassification.
Proceedings of ACL 2007.Barbara Plank, Dirk Hovy, and Anders S?gaard.
2014.
Learning part-of-speech taggers with inter-annotatoragreement loss.
In Proceedings of EACL 2014.Nils Reiter and Anette Frank.
2010.
Identifying generic noun phrases.
In Proceedings of the 48th Annual Meetingof the Association for Computational Linguistics (ACL).John Searle.
1969.
Speech Acts.
Cambridge University Press.Eric V Siegel and Kathleen R McKeown.
2000.
Learning methods to combine linguistic indicators: Improvingaspectual classification and revealing linguistic insights.
Computational Linguistics, 26(4):595?628.Eric V. Siegel.
1998.
Disambiguating verbs with the WordNet category of the direct object.
In Proceedings ofWorkshop on Usage of WordNet in Natural Language Processing Systems, Universite de Montreal.Eric V. Siegel.
1999.
Corpus-based linguistic indicators for aspectual classification.
In Proceedings of ACL37,University of Maryland, College Park.157Carlota S Smith.
2003.
Modes of discourse: The local structure of texts.
Cambridge University Press.Radu Soricut and Daniel Marcu.
2003.
Sentence level discourse parsing using syntactic and lexical information.In Proceedings of the 2003 Conference of the North American Chapter of the Association for ComputationalLinguistics on Human Language Technology-Volume 1, pages 149?156.
Association for Computational Linguis-tics.Naushad UzZaman, Hector Llorens, Leon Derczynski, Marc Verhagen, James Allen, and James Pustejovsky.
2013.Semeval-2013 task 1: Tempeval-3: Evaluating time expressions, events, and temporal relations.
In Second jointconference on lexical and computational semantics (* SEM), volume 2, pages 1?9.Zeno Vendler, 1967.
Linguistics in Philosophy, chapter Verbs and Times, pages 97?121.
Cornell University Press,Ithaca, New York.158
