Improved Automatic Keyword ExtractionGiven More Linguistic KnowledgeAnette HulthDepartment of Computer and Systems SciencesStockholm UniversitySwedenhulth@dsv.su.seAbstractIn this paper, experiments on automaticextraction of keywords from abstracts us-ing a supervised machine learning algo-rithm are discussed.
The main point of thispaper is that by adding linguistic know-ledge to the representation (such as syn-tactic features), rather than relying only onstatistics (such as term frequency and n-grams), a better result is obtained as mea-sured by keywords previously assigned byprofessional indexers.
In more detail, ex-tracting NP-chunks gives a better preci-sion than n-grams, and by adding the POStag(s) assigned to the term as a feature, adramatic improvement of the results is ob-tained, independent of the term selectionapproach applied.1 IntroductionAutomatic keyword assignment is a research topicthat has received less attention than it deserves, con-sidering keywords?
potential usefulness.
Keywordsmay, for example, serve as a dense summary for adocument, lead to improved information retrieval, orbe the entrance to a document collection.
However,relatively few documents have keywords assigned,and therefore finding methods to automate the as-signment is desirable.A related research area is that of terminology ex-traction (see e.g., Bourigault et al (2001)), whereall terms describing a domain are to be extracted.The aim of keyword assignment is to find a smallset of terms that describes a specific document, in-dependently of the domain it belongs to.
However,the latter may very well benefit from the results ofthe former, as appropriate keywords often are of aterminological character.In this work, the automatic keyword extraction istreated as a supervised machine learning task, an ap-proach first proposed by Turney (2000).
Two im-portant issues are how to define the potential terms,and what features of these terms are considered dis-criminative, i.e., how to represent the data, and con-sequently what is given as input to the learning al-gorithm.
In this paper, experiments with three termselection approaches are presented: n-grams; nounphrase (NP) chunks; and terms matching any of aset of part-of-speech (POS) tag sequences.
Four dif-ferent features are used: term frequency, collectionfrequency, relative position of the first occurrence,and the POS tag(s) assigned to the term.2 Points of DepartureTreating the automatic keyword extraction as a su-pervised machine learning task means that a clas-sifier is trained by using documents with knownkeywords.
The trained model is subsequently ap-plied to documents for which no keywords are as-signed: each defined term from these documentsis classified either as a keyword or a non-keyword;or?if a probabilistic model is used?the probabil-ity of the defined term being a keyword is given.Turney (2000) presents results for a comparison be-tween an extraction model based on a genetic algo-rithm and an implementation of bagged C4.5 deci-sion trees for the task.
The terms are all stemmed uni-grams, bigrams, and trigrams from the documents,after stopword removal.
The features used are, forexample, the frequency of the most frequent phrasecomponent; the relative number of characters of thephrase; the first relative occurrence of a phrase com-ponent; and whether the last word is an adjective,as judged by the unstemmed suffix.
Turney reportsthat the genetic algorithm outputs better keywordsthan the decision trees.
Part of the same training andtest material is later used by Frank et al (1999) forevaluating their algorithm in relation to Turney?s al-gorithm.
This algorithm, which is based on naiveBayes, uses a smaller and simpler set of features?term frequency, collection frequency (idf), and rel-ative position?although it performs equally well.Frank et al also discuss the addition of a fourth fea-ture that significantly improves the algorithm, whentrained and tested on domain-specific documents.This feature is the number of times a term is assignedas a keyword to other documents in the collection.It should be noted that the performance of thestate-of-the-art keyword extraction is much lowerthan for many other NLP-tasks, such as tagging andparsing, and there is plenty of room for improve-ments.
To give an idea of this, the results obtainedby the genetic algorithm trained by Turney (2000),and the naive Bayes approach by Frank et al (1999)are presented.
The number of terms assigned mustbe explicitly limited by the user for these algorithms.Turney and Frank et al report the precision for fiveand fifteen keywords per document.
Recall is not re-ported in their studies.
In Table 1 their results whentraining and testing on journal articles are shown,and the highest values for the two algorithms are pre-sented.Prec.
Corr.
mean5 terms* 29.0 1.4515 terms** 18.3 2.75Table 1: Precision, and the average number ofcorrect terms for Turney (2000)* and Frank et al(1999)**, for five and fifteen extracted terms.There are two drawbacks in common withthe approaches proposed by Turney (2000) andFrank et al (1999).
First, the number of tokens in akeyword is limited to three.
In the data used to trainthe classifiers evaluated in this paper, 9.1% of themanually assigned keywords consist of four tokensor more, and the longest keywords have eight tokens.Secondly, the user must state how many keywordsto extract from each document, as both algorithms,for each potential keyword, output the probability ofthe term being a keyword.
This could be solved bymanually setting a threshold value for the probabil-ity, but this decision should preferably be made bythe extraction system.Finding potential terms?when no machine learn-ing is involved in the process?by means of POSpatterns is a common approach.
For exam-ple, Barker and Cornacchia (2000) discuss an al-gorithm where the number of words and the fre-quency of a noun phrase, as well as the fre-quency of the head noun is used to determinewhat terms are keywords.
An extraction sys-tem called LinkIT (see e.g., Evans et al (2000))compiles the phrases having a noun as the head,and then ranks these according to the heads?
fre-quency.
Boguraev and Kennedy (1999) extract tech-nical terms based on the noun phrase patterns sug-gested by Justeson and Katz (1995); these terms arethen the basis for a headline-like characterisation ofa document.
The final example given in this paperis Daille et al (1994) who apply statistical filters onthe extracted noun phrases.
In that study it is con-cluded that term frequency is the best filter candi-date of the scores investigated.
When POS patternsare used to extract potential terms, the problem liesin how to restrict the number of terms, and only keepthe ones that are relevant.In the case of professional indexing, the terms arenormally limited to a domain-specific thesaurus, butnot to those present only in the document to whichthey are assigned.
For example, Steinberger (2001)presents work where as a first step, all lemmas afterstop word removal in a document are ranked accord-ing to the log-likelihood ratio, thus a list of contentdescriptors is obtained.
These terms are then usedto assign thesaurus terms, that have been automati-cally assigned associating lemmas during a trainingphase.
In this paper, however, the concern is not tolimit the terms to a set of allowed terms.As opposed to Turney (2000) and Frank et al(1999), who experiment with keyword extractionfrom full-length texts, this work concerns keywordextraction from abstracts.
The reason for this is thatmany journal papers are not available as full-lengthtexts, but as abstracts only, as is the case for exampleon the Internet.The starting point for this work was to examinewhether the data representation suggested by Franket al was adequate for constructing a keyword ex-traction model from and for abstracts.
As the resultswere poor, two alternatives to extracting n-gramsas the potential terms were explored.
The first ap-proach was to extract all noun phrases in the docu-ments as judged by an NP-chunker.
The second se-lection approach was to define a set of POS tag se-quences, and extract all words or sequences of wordsthat matched any of these, relying on a PoS tag-ger.
These two different approaches mean that thelength of the potential terms is not limited to some-thing arbitrary, but reflects a linguistic property.
Thesolution to limiting the number of terms?as themajority of the extracted words or phrases are notkeywords?was to apply a machine learning algo-rithm to decide which terms are keywords and whichare not.
The output from the machine learning algo-rithm is binary (a term is either a keyword or not),consequently the system itself limits the amount ofextracted keywords per document.
As for the fea-tures, a fourth feature was added to the ones usedby Frank et al, namely the POS tag(s) assigned tothe term.
This feature turned out to dramatically im-prove the results.3 The CorpusThe collection used for the experiments describedin this paper consists of 2 000 abstracts in En-glish, with their corresponding title and keywordsfrom the Inspec database.
The abstracts are fromthe years 1998 to 2002, from journal papers, andfrom the disciplines Computers and Control, and In-formation Technology.
Each abstract has two setsof keywords?assigned by a professional indexer?associated to them: a set of controlled terms, i.e.,terms restricted to the Inspec thesaurus; and a setof uncontrolled terms that can be any suitable terms.Both the controlled terms and the uncontrolled termsmay or may not be present in the abstracts.
However,the indexers had access to the full-length documentswhen assigning the keywords.
For the experimentsdescribed here, only the uncontrolled terms wereconsidered, as these to a larger extent are present inthe abstracts (76.2% as opposed to 18.1%).The set of abstracts was arbitrarily divided intothree sets: a training set (to construct the model)consisting of 1 000 documents, a validation set (toevaluate the models, and select the best perform-ing one) consisting of 500 documents, and a testset (to get unbiased results) with the remaining 500abstracts.
The set of manually assigned keywordswere then removed from the documents.
For all ex-periments the same training, validation, and test setswere used.4 Building the ClassifiersThis section begins with a discussion on the differ-ent ways the data were represented: in Section 4.1the term selection approaches are described, and inSection 4.2 the features are discussed.
Thereafter, abrief description of the machine learning approachis given.
Finally in Section 4.4, the training and theevaluation of the classifiers are discussed.4.1 Three Term Selection ApproachesIn this section, the three different term selection ap-proaches, in other words, the three definitions ofwhat constitutes a term in a document, are described.n-gramsIn a first set of runs, the terms were de-fined in a manner similar to Turney (2000) andFrank et al (1999).
(Their studies were introducedin Section 2.)
All unigrams, bigrams, and trigramswere extracted.
Thereafter a stoplist was used (fromFox (1992)), where all terms beginning or endingwith a stopword were removed.
Finally all remain-ing tokens were stemmed using Porter?s stemmer(Porter, 1980).
In this paper, this manner of selectingterms is referred to as the n-gram approach.The implementation differs from Frank et al(1999) in the following aspects:  Only non-alphanumeric characters that werenot present in any keyword in the training setwere removed (keeping e.g., C++).  Numbers were removed only if they stood sep-arately (keeping e.g., 4YourSoul.com).  Proper nouns were kept.  The stemming and the stoplist applied were dif-ferent.  The stems were kept even if they appeared onlyonce (which is true for 80.0% of the keywordspresent in the training set).NP-chunksThat nouns are appropriate as content descrip-tors seems to be something that most agree upon.When inspecting manually assigned keywords, thevast majority turn out to be nouns or noun phraseswith adjectives, and as discussed in Section 2, the re-search on term extraction focuses on noun patterns.To not let the selection of potential terms be an ar-bitrary process?which is the case when extractingn-grams?and better capture the idea of keywordshaving a certain linguistic property, I decided to ex-periment with noun phrases.In the next set of experiments a partial parser1 wasused to select all NP-chunks from the documents.Experiments with both unstemmed and stemmedterms were performed.
This way of defining theterms is in this paper called the chunking approach.POS Tag PatternsAs about half of the manual keywords present inthe training data were lost using the chunking ap-proach, I decided to define another term selectionapproach.
This still captures the idea of keywordshaving a certain syntactic property, but is based onempirical evidence in the training data.A set of POS tag patterns?in total 56?were de-fined, and all (part-of-speech tagged) words or se-quences of words that matched any of these wereextracted.
The patterns were those tag sequencesof the manually assigned keywords, present in thetraining data, that occurred ten or more times.
Thisway of defining the terms is here called the patternapproach.
As with the chunking approach, exper-iments with both unstemmed and stemmed termswere performed.Out of the 56 patterns, 51 contain one or morenoun tags.
To give an idea of the patterns, the1LT CHUNK, available at http://www.ltg.ed.-ac.uk/software/pos/index.html (without the hy-phen).five most frequently occurring ones of the keywordspresent in the training data are  ADJECTIVE NOUN (singular or mass)  NOUN NOUN (both sing.
or mass)  ADJECTIVE NOUN (plural)  NOUN (sing.
or mass) NOUN (pl.
)  NOUN (sing.
or mass)4.2 Four FeaturesInitially, the same features that Frank et al (1999)used for their domain-independent experimentswere used.
These were  Within-document frequency  Collection frequency  Relative position of the first occurrence (theproportion of the document preceding the firstoccurrence).The representation differed in that the term fre-quency and the collection frequency were notweighted together, but kept as two distinct features.In addition, the real values were not discretised, onlyrounded off to two decimals, thus more decision-making was handed over to the algorithm.
The col-lection frequency was calculated for the three datasets separately.In addition, experiments with a fourth featurewere performed.
This is the POS tag or tags as-signed to the term by the same partial parser usedfor finding the chunks and the tag patterns.
When aterm consists of several tokens, the tags are treatedlike a sequence.
As an example, an extracted phraselike random JJ excitations NNS gets the atomic fea-ture value JJ NNS.
In case a term occurs more thanonce in the document, the tag or tag sequence as-signed is the most frequently occurring one for thatterm in the entire document.
In case of a draw, thefirst occurring one is assigned.4.3 Rule InductionAs usual in machine learning, the input to the learn-ing algorithm consists of examples, where an exam-ple refers to the feature value vector for each, inthis case, potential keyword.
An example that is amanual keyword is assigned the class positive, andthose that are not are given the class negative.
Themachine learning approach used for the experimentsis that of rule induction, i.e., the model that is con-structed from the given examples, consists of a set ofrules2.
The strategy used to construct the rules is re-cursive partitioning (or divide-and-conquer), whichhas as the goal to maximise the separation betweenthe classes for each rule.The system used allows for different ensembletechniques to be applied, meaning that a number ofclassifiers are generated and then combined to pre-dict the class.
The one used for these experimentsis bagging (Breiman, 1996).
In bagging, examplesfrom the training data are drawn randomly with re-placement until a set of the original size is obtained.This new set is then used to train a classifier.
Thisprocedure is repeated n times to generate n classi-fiers that then vote to classify an instance.It should be noted that my intention is not to ar-gue for this machine learning approach in favour ofany other.
However, one advantage with rules is thatthey may be inspected, and thus might give an in-sight into how the learning component makes its de-cisions, although this is less applicable when apply-ing ensemble techniques.4.4 The Training and the EvaluationThe feature values were calculated for each ex-tracted unit in the training and the validation sets,that is for the n-grams, NP-chunks, stemmed NP-chunks, patterns, and the stemmed patterns respec-tively.
In other words, the within-document fre-quency, the collection frequency, and the proportionof the document preceding the first appearance foreach potential term were calculated.
Also, the POStag(s) for each term were extracted.
In addition,as the machine learning approach is supervised, theclass was added, i.e., whether the term is a manuallyassigned keyword or not.
For the stemmed terms,a unit was considered a keyword if it was equal toa stemmed manual keyword.
For the unstemmedterms, the term had to match exactly.The measure used to evaluate the results on thevalidation set was the F-score, defined as2The system is Rule Discovery System from CompumineAB.
www.compumine.com. fffiffifl !"#%$'&(&fi)*ffifl +,"#)$-&(&combining the precision and the recall obtained.
Inthis study, the main concern is the precision and therecall for the examples that have been assigned theclass positive, that is how many of the suggestedkeywords are correct (precision), and how many ofthe manually assigned keywords that are found (re-call).
As the proportion of correctly suggested key-words is considered equally important as the amountof terms assigned by a professional indexer that wasdetected,  was assigned the value 1, thus givingprecision and recall equal weights.When calculating the recall, the value for the to-tal number of manually assigned keywords presentin the documents is used, independent of the num-ber actually present in the different representations.This figure varies slightly for the unstemmed andthe stemmed data, and for the two the correspond-ing value is used.Several runs were made for each representation,with the goal to maximise the performance as eval-uated on the validation set: first the weights of thepositive examples were adjusted, as the data set isunbalanced.
A better performance was obtainedwhen the positive examples in the training data out-numbered the negative ones.
Thereafter experimentswith bagging were performed, and also, runs withand without the POS tag feature were made.
Theresults are presented next.5 The ResultsIn this section, the results obtained by the best per-forming model for each approach?as judged on thevalidation set?when run on the previously unseentest set are presented.
It should, however, be notedthat the number of possible runs is very large, byvarying for example the number of classifiers gen-erated by the ensemble technique.
It might well bethat better results are possible for any of the repre-sentations.As stemming with few exceptions led to better re-sults on the validation set over all runs, only thesevalues are presented in this section.
In Table 2, thenumber of assigned terms and the number of cor-rect terms, in total and on average per document areshown.
Also, precision, recall, and the F-score arepresented.
For each approach, both the results withand without the POS tag feature are given.The length of the abstracts in the test set variesfrom 338 to 23 tokens (the median is 121 tokens).The number of uncontrolled terms per document is31 to 2 (the median is 9 keywords).
The total numberof stemmed keywords present in the stemmed testset is 3 816, and the average number of terms is 7.63.Their distribution over the 500 documents is 27 tothree documents with 0 terms, with the median being7.As for bagging, it was noted that although theaccuracy (i.e., the number of correctly classifiedpositive and negative examples divided by the totalnumber of examples) improved when increasing thenumber of classifiers, the F-score often decreased.For the pattern approach without the tag features thebest model consists of a 5-bagged classifier, for thepattern approach with the tag feature a 20-bagged,and finally for the n-gram approach with the tag fea-ture a 10-bagged classifier.
For the other three runsa single classifier had the best performance.5.1 Results of the n-gram ApproachWhen extracting the terms from the test set accord-ing to the n-gram approach, the data consisted of42 159 negative examples, and 3 330 positive exam-ples, thus in total 45 489 examples were classified bythe trained model.
Using this manner of extractingthe terms meant that 12.8% of the keywords origi-nally present in the test set were lost.To summarise the n-gram approach (see Table 2),without the tag feature it finds on average 4.37keywords per document, out of originally on aver-age 7.63 manual keywords present in the abstracts.However, the price paid for these correct termsis high: almost 38 incorrect terms per document.When adding the fourth feature, the number of cor-rect terms decreases slightly, while the number ofincorrect terms is decreased to a third.
If lookingat the actual distribution of assigned terms for thesetwo runs, this varies between 134(!)
and 5 withoutthe tag feature, and from 48 to 1 with the tag feature.The median is 40 and 14 respectively.The F-scores (F     ) for these two runs are 17.6and 33.9 respectively.
33.9 is the highest F-scorethat was achieved for the six runs presented here.5.2 Results of the Chunking ApproachWhen extracting the terms according to the stemmedchunking approach, the test set consisted of 13 579negative, and 1 920 positive examples; in total15 499 examples.An F-score (F     ) of 22.7 is obtained without thePOS tag feature, and 33.0 with this feature.
Thenumber of terms on average per document is 16.38without the tag feature, and 9.58 with it.
If lookingat each document, the number of keywords assignedvaries from 46 to 0 (for three documents) with themedian 16, and 29 to 0 (for four documents) withthe median value being 9 terms.Extracting the terms with the chunking approachmeant that slightly more than half of the keywordsactually present in the test set were lost, and com-pared to the n-gram approach the number of cor-rect terms assigned was almost halved.
The numberof incorrect keywords, however, decreased consider-ably.
But, the difference is shown when the POS tagfeature is included: the number of correctly assignedterms is more or less the same for this approach withor without the tag feature, while the number of in-correct terms is halved.5.3 Results of the Pattern ApproachWhen extracting the terms according to the stemmedpattern approach, the test data consisted of 33 507examples.
Of these were 3 340 positive, and 30 167negative.
In total, 12.5% of the present keywordswere lost.The F-scores (F     ) for the two runs, displayed inTable 2, are 25.6 (without the tag feature) and 28.1(with the tag feature).
The number of terms assignedon average per document is 5.04 and 3.05 withoutand with the tag feature respectively.
The actualnumber of terms assigned per document is 100 to0 (for three documents) without the tag feature, and46 to 0 (for four documents) with the tag feature.The median is 30 and 12 respectively.6 Concluding Remarks and Future WorkIn this paper I have shown how keyword extrac-tion from abstracts can be achieved by using simplestatistical measures as well as syntactic informationfrom the documents, as input to a machine learn-ing algorithm.
If first considering the term selec-Method Assign.
tot.
Assign.
mean Corr.
tot.
Corr.
mean Prec.
Recall F-scoren-gram 21 104 42.21 2 187 4.37 10.4 57.3 17.6n-gram w. tag 7 815 15.63 1 973 3.95 25.2 51.7 33.9Chunking 8 189 16.38 1 364 2.73 16.7 35.7 22.7Chunking w. tag 4 788 9.58 1 421 2.84 29.7 37.2 33.0Pattern 15 882 31.76 2 519 5.04 15.9 66.0 25.6Pattern w. tag 7 012 14.02 1 523 3.05 21.7 39.9 28.1Table 2: For each representation is shown: the number of assigned (Assign.)
terms in total and mean perdocument; the number of correct (Corr.)
terms in total and mean per document; precision; recall; and F-score.
The highest value is shown in bold.
The total number of manually assigned terms present in theabstracts is 3 816, and the mean is 7.63 terms per document.tion approaches, extracting NP-chunks gives a betterprecision, while extracting all words or sequencesof words matching any of a set of POS tag pat-terns gives a higher recall compared to extracting n-grams.
The highest F-score is obtained by one ofthe n-gram runs.
The largest amount of assignedterms present in the abstracts are assigned by thepattern approach without the tag feature.
The pat-tern approach is also the approach which keeps thelargest number of assigned terms after that the datahave been pre-processed.
Using phrases means thatthe length of the potential terms is not restricted tosomething arbitrary, rather the terms are treat as theunits they are.
However, of the patterns that were se-lected for the experiments discussed here none waslonger than four tokens.
If looking at all assignedkeywords in the training set, 3.0% are then ruled outas potential terms.
The longest chunks in the test setthat were correctly assigned are five tokens long.
Asfor when syntactic information is included as a fea-ture (in the form of the POS tag(s) assigned to theterm), it is evident from the results presented in thispaper that this information is crucial for assigning anacceptable number of terms per document, indepen-dent of what term selection strategy is chosen.One shortcoming of the work is that there is cur-rently no relation between the different POS tag fea-ture values.
For example, a singular noun has nocloser relationship to a plural noun than to an adjec-tive.
In the future, the patterns should somehow becategorised reflecting their semantics, perhaps in ahierarchical manner, or morphological informationcould be removed.In this paper I have not touched upon the more in-tricate aspects of evaluation, but simply treated themanually assigned keywords as the gold standard.This is the most severe way to evaluate a keywordextractor, as many terms might be just as good, al-though for one reason or another not chosen by thehuman indexer.
Future work will examine alterna-tive approaches to evaluation.
One possibility fora more liberal evaluation could be to use humanevaluators with real information needs, as done byTurney (2000).
Another possibility would be to letseveral persons index each document, thus getting alarger set of acceptable terms to choose from.
Thiswould hopefully lead to a better precision, while re-call probably would be affected negatively; the im-portance of recall would then need to be reconsid-ered.Future work should also go in the direction ofgenerating (as opposed to extracting) keywords, byfor example exploring potential knowledge providedby a thesaurus.AcknowledgementsFor valuable comments and suggestions: Bea?taMegyesi, Henrik Bostro?m, Jussi Karlgren, HarkoVerhagen, Fredrik Kilander, and the anonymousEMNLP reviewers.ReferencesKen Barker and Nadia Cornacchia.
2000.
Using nounphrase heads to extract document keyphrases.
InCanadian Conference on AI.Branimir Boguraev and Christopher Kennedy.
1999.
Ap-plications of term identification technology: Domaindescription and content characterisation.
Natural Lan-guage Engineering, 5(1):17?44.Didier Bourigault, Christian Jacquemin, and Marie-Claude L?Homme, editors.
2001.
Recent Advancesin Computational Terminology.
John Benjamins Pub-lishing Company, Amsterdam.Leo Breiman.
1996.
Bagging predictors.
MachineLearning, 24(2):123?140.Be?atrice Daille, ?Eric Gaussier, and Jean-Marc Lange?.1994.
Towards automatic extraction of monolin-gual and bilingual terminology.
In Proceedings ofCOLING-94, pages 515?521, Kyoto, Japan.David K. Evans, Judith L. Klavans, and Nina Wacholder.2000.
Document processing with LinkIT.
In Proceed-ings of the RIAO Conference, Paris, France.Christopher Fox.
1992.
Lexical analysis and stoplists.
InWilliam B. Frakes and Ricardo Baeza-Yates, editors,Information Retrieval: Data Structures & Algorithms,pages 102?130.
Prentice-Hall, New Jersey.Eibe Frank, Gordon W. Paynter, Ian H. Witten, CarlGutwin, and Craig G. Nevill-Manning.
1999.Domain-specific keyphrase extraction.
In Proceed-ings of the International Joint Conference on ArtificialIntelligence (IJCAI?99), pages 668?673, Stockholm,Sweden.John S. Justeson and Slava M. Katz.
1995.
Technical ter-minology: some linguistic properties and an algorithmfor identification in text.
Natural Language Engineer-ing, 1(1):9?27.Martin Porter.
1980.
An algorithm for suffix stripping.Program, 14(3):130?137.Ralf Steinberger.
2001.
Cross-lingual keyword assign-ment.
In Proceedings of the XVII Conference of theSpanish Society for Natural Language Processing (SE-PLN?2001), pages 273?280, Jae?n, Spain.Peter D. Turney.
2000.
Learning algorithmsfor keyphrase extraction.
Information Retrieval,2(4):303?336.
