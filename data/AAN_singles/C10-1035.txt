Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 304?312,Beijing, August 2010Translation Model Generalization using Probability Averagingfor Machine TranslationNan Duan1, Hong SunSchool of Computer Science and TechnologyTianjin Universityv-naduan@microsoft.comv-hongsun@microsoft.comMing ZhouMicrosoft Research Asiamingzhou@microsoft.comAbstractPrevious methods on improving transla-tion quality by employing multiple SMTmodels usually carry out as a second-pass decision procedure on hypothesesfrom multiple systems using extra fea-tures instead of using features in existingmodels in more depth.
In this paper, wepropose translation model generalization(TMG), an approach that updates proba-bility feature values for the translationmodel being used based on the model it-self and a set of auxiliary models, aimingto enhance translation quality in the first-pass decoding.
We validate our approachon translation models based on auxiliarymodels built by two different ways.
Wealso introduce novel probability variancefeatures into the log-linear models forfurther improvements.
We conclude thatour approach can be developed indepen-dently and integrated into current SMTpipeline directly.
We demonstrate BLEUimprovements on the NIST Chinese-to-English MT tasks for single-system de-codings, a system combination approachand a model combination approach.11 IntroductionCurrent research on Statistical Machine Transla-tion (SMT) has made rapid progress in recentdecades.
Although differed on paradigms, suchas phrase-based (Koehn, 2004; Och and Ney,2004), hierarchical phrase-based (Chiang, 2007)and syntax-based (Galley et al, 2006; Shen etal., 2008; Huang, 2008), most SMT systems fol-1 This work has been done while the author was visitingMicrosoft Research Asia.low the similar pipeline and share commontranslation probability features which constitutethe principal components of translation models.However, due to different model structures ordata distributions, these features are usually as-signed with different values in different transla-tion models and result in translation outputs withindividual advantages and shortcomings.In order to obtain further improvements, manyapproaches have been explored over multiplesystems: system combination based on confu-sion network (Matusov et al, 2006; Rosti et al,2007; Li et al, 2009a) develop on multiple N-best outputs and outperform primary SMT sys-tems; consensus-based methods (Li et al, 2009b;DeNero et al, 2010), on the other hand, avoidthe alignment problem between translations can-didates and utilize n-gram consensus, aiming tooptimize special decoding objectives for hypo-thesis selection.
All these approaches act as thesecond-pass decision procedure on hypothesesfrom multiple systems by using extra features.They begin to work only after the generation oftranslation hypotheses has been finished.In this paper, we propose translation modelgeneralization (TMG), an approach that takeseffect during the first-pass decoding procedureby updating translation probability features forthe translation model being used based on themodel itself and a set of auxiliary models.
Baye-sian Model Averaging is used to integrate valuesof identical features between models.
Our con-tributions mainly include the following 3 aspects:?
Alleviate the model bias problem based ontranslation models with different paradigms.Because of various model constraints, trans-lation models based on different paradigmscould have individual biases.
For instance,phrase-based models prefer translation pairswith high frequencies and assign them high304probability values; yet such pairs could bedisliked or even be absent in syntax-basedmodels because of their violation on syntac-tic restrictions.
We alleviate such model biasproblem by using the generalized probabilityfeatures in first-pass decoding, which com-puted based on feature values from all trans-lation models instead of any single one.?
Alleviate the over-estimation problem basedon translation models with an identical pa-radigm but different training corpora.In order to obtain further improvements byusing an existing training module built for aspecified model paradigm, we present a ran-dom data sampling method inspired by bag-ging (Breiman, 1996) to construct transla-tion model ensembles from a unique data setfor usage in TMG.
Compared to results ofTMG based on models with different para-digms, TMG based on models built in such away can achieve larger improvements.?
Novel translation probability variance fea-tures introduced.We present how to compute the variance foreach probability feature based on its valuesin different involved translation models withprior model probabilities.
We add them intothe log-linear model as new features to makecurrent SMT models to be more flexible.The remainder of this paper is organized asfollows: we review various translation models inSection 2.
In Section 3, we first introduce Baye-sian Model Averaging method for SMT tasksand present a generic TMG algorithm based on it.We then discuss two solutions for constructingTM ensembles for usage in TMG.
We next in-troduce probability variance features into currentSMT models as new features.
We evaluate ourmethod on four state-of-the-art SMT systems, asystem combination approach and a model com-bination approach.
Evaluation results are shownin Section 4.
In Section 5, we discuss some re-lated work.
We conclude the paper in Section 6.2 Summary of Translation ModelsTranslation Model (TM) is the most importantcomponent in current SMT framework.
Itprovides basic translation units for decoders witha series of probability features for modelscoring.
Many literatures have paid attentions toTMs from different aspects: DeNeefe et al(2007) compared strengths and weaknesses of aphrase-based TM and a syntax-based TM fromthe statistic aspect; Zollmann et al (2008) madea systematic comparison of three TMs, includingphrasal, hierarchical and syntax-based, from theperformance aspect; and Auli et al (2009) madea systematic analysis of a phrase-based TM anda hierarchical TM from the search space aspect.Given a word-aligned training corpus, weseparate a TM training procedure into two phas-es: extraction phase and parameterization phase.Extraction phase aims to pick out all validtranslation pairs that are consistent with pre-defined model constraints.
We summarize cur-rent TMs based on their corresponding modelconstraints into two categories below:?
String-based TM (string-to-string): reservesall translation pairs that are consistent withword alignment and satisfy length limitation.SMT systems using such TMs can benefitfrom a large convergence of translation pairs.?
Tree-based TM (string-to-tree, tree-to-stringor tree-to-tree): needs to obey syntactic re-strictions in one side or even both sides oftranslation candidates.
The advantage of us-ing such TMs is that translation outputstrend to be more syntactically well-formed.Parameterization phase aims to assign a seriesof probability features to each translation pair.These features play the most important roles inthe decision process and are shared by most cur-rent SMT decoders.
In this paper, we mainlyfocus on the following four commonly used do-minant probability features including:?
translation probability features in two direc-tions:         and?
lexical weight features in two directions:andBoth string-based and tree-based TMs arestate-of-the-art models, and each extraction ap-proach has its own strengths and weaknessescomparing to others.
Due to different predefinedmodel constraints, translation pairs extracted bydifferent models usually have different distribu-tions, which could directly affect the resultingprobability feature values computed in parame-305terization phase.
In order to utilize translationpairs more fairly in decoding, it is desirable touse more information to measure the quality oftranslation pairs based on different TMs ratherthan totally believing any single one.3 Translation Model GeneralizationWe first introduce Bayesian Model Averagingmethod for SMT task.
Based on it, we then for-mally present the generic TMG algorithm.
Wealso provide two solutions for constructing TMensembles as auxiliary models.
We last intro-duce probability variance features based on mul-tiple TMs for further improvements.3.1 Bayesian Model Averaging for SMTBayesian Model Averaging (BMA) (Hoeting etal., 1999) is a technique designed to solve uncer-tainty inherent in model selection.Specifically, for SMT tasks,   is a source sen-tence,  is the training data,   is theth SMTmodel trained on    ,            representsthe probability score predicted by   that   canbe translated into a target sentence  .
BMA pro-vides a way to combine decisions of allSMT models by computing the final translationprobability score              as follows:(1)where          is the prior probability thatis a true model.
For convenience, we willomit all symbols    in following descriptions.Ideally, if all involved modelsshare the same search space, then translationhypotheses could only be differentiated in prob-ability scores assigned by different SMT models.In such case, BMA can be straightly developedon the whole SMT models in either span level orsentence level to re-compute translation scoresof hypotheses for better rankings.
However, be-cause of various reasons, e.g.
different pruningmethods, different training data used, differentgenerative capabilities of SMT models, searchspaces between different models are always notidentical.
Thus, it is intractable to develop BMAon the whole SMT model level directly.As a tradeoff, we notice that translation pairsbetween different TMs share a relatively largeconvergence because of word length limitation.So we instead utilize BMA method to multipleTMs by re-computing values of probability fea-tures between them, and we name this process astranslation model generalization.3.2 A Generic BMA-based TMG AlgorithmFor a translation model  , TMG aims to re-compute its values of probability features basedon itself and   collaborative TMs          .We describe the re-computation process for anarbitrary feature              as follows:(2)where            is the feature value assignedby  .
We denote   as the main model, andother collaborative TMs as auxiliary models.Figure 1 describes an example of TMG on twoTMs, where the main model is a phrasal TM.Equation 2 is a general framework that can beapplied to all TMs.
The only limitation is thatthe segmentation (or tokenization) standards forsource (or target) training sentences should beidentical for all models.
We describe the genericTMG procedure in Algorithm 12.2 In this paper, since all data sets used have relative largesizes and all SMT models have similar performances, weheuristically set al       equally to        .Figure 1.
TMG applied to a phrasal TM (mainmodel) and a syntax-based TM (auxiliary mod-el).
The value of a translation probability feature??
in TM1 is de-valued (from 0.6to 0.3), in which ?join the?
is absent in TM2 be-cause of its bad syntactic structure.?
?=0.6Phrase-based TM1(Main model)Syntax-based TM2(Auxiliary model)=0.5       =0.5Generalized TM1??=0.6*0.5+0.0*0.5=0.3?
?=0.0306Algorithm 1: TMG for a main model1: for the  th auxiliary TM do2:          run training procedure on    with specifiedmodel constraints and generate3: end for4: for each translation pair         in   do5:  for each probability feature           do6:          for each translation model   do7:      if          is contained in   then8:9    end if10:   end for11:  end for12: end for13: return the generalized   for SMT decoding3.3 Auxiliary Model ConstructionIn order to utilize TMG, more than one TM asauxiliary models is needed.
Building TMs withdifferent paradigms is one solution.
For exam-ple, we can build a syntax-based TM as an aux-iliary model for a phrase-based TM.
However, ithas to re-implement more complicated TM train-ing modules besides the existing one.In this sub-section, we present an alternativesolution to construct auxiliary model ensemblesby using the existing training module with dif-ferent training data extracted from a unique dataset.
We describe the general procedure for con-structing   auxiliary models as follows:1) Given a unique training corpus  , we ran-domly sample    bilingual sentence pairswithout replacement and denote them as   .is a number determined empirically;2) Based on  , we re-do word alignment andtrain an auxiliary model   using the exist-ing training module;3) We execute Step 1 and Step 2 iteratively fortimes, and finally obtain   auxiliary mod-els.
The optimal setting of   for TMG is al-so determined empirically.With all above steps finished, we can performTMG as we described in Algorithm 1 based onthe   auxiliary models generated already.The random data sampling process describedabove is very similar to bagging except for it notallowing replacement during sampling.
By mak-ing use of this process, translation pairs with lowfrequencies have relatively high probabilities tobe totally discarded, and in resulting TMs, theirprobabilities could be zero; meanwhile, transla-tion pairs with high frequencies still have highprobabilities to be reserved, and hold similarprobability feature values in resulting TMs com-paring to the main model.
Thus, after TMG pro-cedure, feature values could be smoothed fortranslation pairs with low frequencies, and bestable for translation pairs with high frequencies.From this point of view, TMG can also be seenas a TM smoothing technique based on multipleTMs instead of single one such as Foster et al(2006).
We will see in Section 4 that TMG basedon TMs generated by both of these two solutionscan improve translation quality for all baselinedecoders on a series of evaluation sets.3.4 Probability Variance FeatureThe re-computed values of probability featuresin Equation 2 are actually the feature expecta-tions based on their values from all involvedTMs.
In order to give more statistical meaningsto translation pairs, we also compute their cor-responding feature variances based on featureexpectations and TM-specified feature valueswith prior probabilities.
We introduce such va-riances as new features into the log-linear modelfor further improvements.
Our motivation is toquantify the differences of model preferencesbetween TMs for arbitrary probability features.The variance for an arbitrary probability fea-ture         can be computed as follows:(3)where        is the feature expectation computedby Equation 2,       is the feature value pre-dicted by  , and        is the prior probabil-ity for  .
Each probability feature now corres-ponds to a variance score.
We extend the origi-nal feature set of   with variance features add-ed in and list the updated set below:?
translation probability expectation featuresin two directions:           and?
translation probability variance features intwo directions:          and?
lexical weight expectation features in twodirections:and?
lexical weight variance features in two di-rections:and3074 Experiments4.1 Data ConditionWe conduct experiments on the NIST Chinese-to-English MT tasks.
We tune model parameterson the NIST 2003 (MT03) evaluation set byMERT (Och, 2003), and report results on NISTevaluation sets including the NIST 2004 (MT04),the NIST 2005 (MT05), the newswire portion ofthe NIST 2006 (MT06) and 2008 (MT08).
Per-formances are measured in terms of the case-insensitive BLEU scores in percentage numbers.Table 1 gives statistics over these evaluation sets.MT03 MT04 MT05 MT06 MT08Sent 919 1,788 1,082 616 691Word 23,788 48,215 29,263 17,316 17,424Table 1.
Statistics on dev/test evaluation setsWe use the selected data that picked out fromthe whole data available for the NIST 2008 con-strained track of Chinese-to-English machinetranslation task as the training corpora, includingLDC2003E07, LDC2003E14, LDC2005T06,LDC2005T10, LDC2005E83, LDC2006E26,LDC2006E34, LDC2006E85 and LDC2006E92,which contain about 498,000 sentence pairs afterpre-processing.
Word alignments are performedby GIZA++ (Och and Ney, 2000) in both direc-tions with an intersect-diag-grow refinement.A traditional 5-gram language model (LM)for all involved systems is trained on the Englishside of all bilingual data plus the Xinhua portionof LDC English Gigaword Version 3.0.
A lexi-calized reordering model (Xiong et al, 2006) istrained on the selected data in maximum entropyprinciple for the phrase-based system.
A tri-gram target dependency LM (DLM) is trainedon the English side of the selected data for thedependency-based hierarchical system.4.2 MT System DescriptionWe include four baseline systems.
The first one(Phr) is a phrasal system (Xiong et al, 2006)based on Bracketing Transduction Grammar(Wu, 1997) with a lexicalized reordering com-ponent based on maximum entropy model.
Thesecond one (Hier) is a hierarchical phrase-basedsystem (Chiang, 2007) based on SynchronousContext Free Grammar (SCFG).
The third one(Dep) is a string-to-dependency hierarchicalphrase-based system (Shen et al, 2008) with adependency language model, which translatessource strings to target dependency trees.
Thefourth one (Synx) is a syntax-based system (Gal-ley et al, 2006) that translates source strings totarget syntactic trees.4.3 TMG based on Multiple ParadigmsWe develop TMG for each baseline system?sTM based on the other three TMs as auxiliarymodels.
All prior probabilities of TMs are setequally to 0.25 heuristically as their similar per-formances.
Evaluation results are shown in Ta-ble 2, where gains more than 0.2 BLEU pointsare highlighted as improved cases.
Compared tobaseline systems, systems based on generalizedTMs improve in most cases (18 times out of 20).We also notice that the improvements achievedon tree-based systems (Dep and Synx) are rela-tively smaller than those on string-based systems(Phr and Hier).
A potential explanation can bethat with considering more syntactic restrictions,tree-based systems suffer less than string-basedsystems on the over-estimation problem.
We donot present further results with variance featuresadded because of their consistent un-promisingnumbers.
We think this may be due to the consi-derable portion of non-overlapping translationpairs between main model and auxiliary models,which cause the variances not so accurate.MT03(dev) MT04 MT05 MT06 MT08 AveragePhrBaseline 40.45 39.21 38.03 34.24 30.21 36.43TMG 41.19(+0.74) 39.74(+0.53) 38.39(+0.36) 34.71(+0.47) 30.69(+0.48) 36.94(+0.51)HierBaseline 41.30 39.63 38.83 34.63 30.46 36.97TMG 41.67(+0.37) 40.25(+0.62) 39.11(+0.28) 35.78(+1.15) 31.17(+0.71) 37.60(+0.63)DepBaseline 41.10 39.81 39.47 35.72 30.50 37.32TMG 41.37(+0.27) 39.92(+0.11) 39.91(+0.44) 35.99(+0.27) 31.07(+0.57) 37.65(+0.33)SynxBaseline 41.02 39.88 39.47 36.41 32.15 37.79TMG 41.26(+0.24) 40.09(+0.21) 39.90(+0.43) 36.77(+0.36) 32.15(+0.00) 38.03(+0.24)Table 2.
Results of TMG based on TMs with different paradigms3084.4 TMG based on Single ParadigmWe then evaluate TMG based on auxiliary mod-els generated by the random sampling method.We first decide the percentage of training datato be sampled.
We empirically vary this numberby 20%, 40%, 60%, 80% and 90% and use eachsampled data to train an auxiliary model.
Wethen run TMG on the baseline TM with differentauxiliary model used each time.
For time saving,we only evaluate on MT03 for Phr in Figure 2.Figure 2.
Affects of different percentages of dataThe optimal result is achieved when the per-centage is 80%, and we fix it as the default valuein following experiments.We then decide the number of auxiliary mod-els used for TMG by varying it from 1 to 5.
Welist different results on MT03 for Phr in Figure 3.Figure 3.
Affects of different numbers of auxi-liary modelsThe optimal result is achieved when the num-ber of auxiliary models is 4, and we fix it as thedefault value in following experiments.We now develop TMG for each baseline sys-tem?s TM based on auxiliary models constructedunder default settings determined above.
Evalua-tion results are shown in Table 3.
We also inves-tigate the affect of variance features for perfor-mance, whose results are denoted as TMG+Var.From Table 3 we can see that, compared tothe results on baseline systems, systems usinggeneralized TMs obtain improvements on almostall evaluation sets (19 times out of 20).
Withprobability variance features added further, theimprovements become even more stable than theones using TMG only (20 times out of 20).
Simi-lar to the trend in Table 2, we also notice thatTMG method is more preferred by string-basedsystems (Phr and Hier) rather than tree-basedsystems (Dep and Synx).
This makes our con-clusion more solidly that syntactic restrictionscan help to alleviate the over-estimation problem.4.5 Analysis on Phrase CoverageWe next empirically investigate on the transla-tion pair coverage between TM ensembles builtby different ways, and use them to analyze re-sults got from previous experiments.
Here, weonly focus on full lexicalized translation entriesbetween models.
Those entries with variablesare out of consideration in comparisons becauseof their model dependent properties.Phrase pairs in the first three TMs have alength limitation in source side up to 3 words,and each source phrase can be translated to atmost 20 target phrases.40.040.541.041.50% 20% 40% 60% 80% 90%Phr40.040.541.041.542.00 1 2 3 4 5PhrMT03(dev) MT04 MT05 MT06 MT08 AveragePhrBaseline 40.45 39.21 38.03 34.24 30.21 36.43TMG 41.77(+1.32) 40.28(+1.07) 39.13(+1.10) 35.38(+1.14) 31.12(+0.91) 37.54(+1.11)TMG+Var 41.77(+1.32) 40.31(+1.10) 39.43(+1.30) 35.61(+1.37) 31.62(+1.41) 37.74(+1.31)HierBaseline 41.30 39.63 38.83 34.63 30.46 36.97TMG 42.28(+0.98) 40.45(+0.82) 39.61(+0.78) 35.67(+1.04) 31.54(+1.08) 37.91(+0.94)TMG+Var 42.42(+1.12) 40.55(+0.92) 39.69(+0.86) 35.55(+0.92) 31.41(+0.95) 37.92(+0.95)DepBaseline 41.10 39.81 39.47 35.72 30.50 37.32TMG 41.49(+0.39) 40.20(+0.39) 40.00(+0.53) 36.13(+0.41) 31.24(+0.74) 37.81(+0.49)TMG+Var 41.72(+0.62) 40.57(+0.76) 40.44(+0.97) 36.15(+0.43) 31.31(+0.81) 38.04(+0.72)SynxBaseline 41.02 39.88 39.47 36.41 32.15 37.79TMG 41.18(+0.16) 40.30(+0.42) 39.90(+0.43) 36.99(+0.58) 32.45(+0.30) 38.16(+0.37)TMG+Var 41.42(+0.40) 40.55(+0.67) 40.17(+0.70) 36.89(+0.48) 32.51(+0.36) 38.31(+0.52)Table 3.
Results of TMG based on TMs constructed by random data sampling309For the fourth TM, these two limitations arereleased to 4 words and 30 target phrases.
Wetreat phrase pairs identical on both sides but withdifferent syntactic labels in the fourth TM as aunique pair for conveniences in statistics.We first make statistics on TMs with differentparadigms in Table 4.
We can see from Table 4that only slightly over half of the phrase pairscontained by the four involved TMs are common,which is also similar to the conclusion drawn inDeNeefe et al (2006).Models #Translation Pair #PercentagePhr 1,222,909 50.6%Hier 1,222,909 50.6%Dep 1,087,198 56.9%Synx 1,188,408 52.0%Overlaps 618,371 -Table 4.
Rule statistics on TMs constructed bydifferent paradigmsWe then make statistics on TMs with identicalparadigm in Table 5.
For each baseline TM andits corresponding four auxiliary models con-structed by random data sampling, we count thenumber of phrase pairs that are common be-tween them and compute the percentage num-bers based on it for each TM individually.Models TM0 TM1 TM2 TM3 TM4Phr 61.8% 74.0% 74.1% 73.9% 74.1%Hier 61.8% 74.0% 74.1% 73.9% 74.1%Dep 60.8% 73.6% 73.6% 73.5% 73.7%Synx 57.2% 68.4% 68.5% 68.5% 68.6%Table 5.
Rule statistics on TMs constructed byrandom sampling (TM0 is the main model)Compared to the numbers in Table 4, we findthat the coverage between baseline TM andsampled auxiliary models with identical para-digm is larger than that between baseline TMand auxiliary models with different paradigms(about 10 percents).
It is a potential reason canexplain why results of TMG based on sampledauxiliary models are more effective than thosebased on auxiliary models built with differentparadigms, as we infer that they share morecommon phrase pairs each other and make thecomputation of feature expectations and va-riances to be more reliable and accurate.4.6 Improvements on System CombinationBesides working for single-system decoding, wealso perform a system combination method onN-best outputs from systems using generalizedTMs.
We re-implement a state-of-the-art word-level System Combination  (SC) approach basedon incremental HMM alignment proposed by Liet al (2009a).
The default number of N-bestcandidates used is set to 20.We evaluate SC on N-best outputs generatedfrom 4 baseline decoders by using different TMsettings and list results in Table 6, where Basestands for combination results on systems usingdefault TMs; Paras stands for combination re-sults on systems using TMs generalized basedon auxiliary models with different paradigms;and Samp stands for combination results on sys-tems using TMs generalized based on auxiliarymodels constructed by the random data samplingmethod.
For the Samp setting, we also includeprobability variance features computed based onEquation 3 in the log-linear model.SC MT03 MT04 MT05 MT06 MT08Base 44.20 42.30 41.22 37.77 33.07Paras 44.40 42.69 41.53 38.05 33.31Samp 44.80 42.95 42.10 38.39 33.67Table 6.
Results on system combinationFrom Table 6 we can see that system combi-nation can benefit from TMG method.4.7 Improvements on Model CombinationAs an alternative, model combination is anothereffective way to improve translation perfor-mance by utilizing multiple systems.
We re-implement the Model Combination (MC) ap-proach (DeNero et al, 2010) using N-best listsas its inputs and develop it on N-best outputsused in Table 6.
Evaluation results are presentedin Table 7.MC MT03 MT04 MT05 MT06 MT08Base 42.31 40.57 40.31 38.65 33.88Paras 42.87 40.96 40.77 38.81 34.47Samp 43.29 41.29 41.11 39.28 34.77Table 7.
Results on model combination310From Table 7 we can see that model combina-tion can also benefit from TMG method.5 Related WorkFoster and Kuhn (2007) presented an approachthat resembles more to our work, in which theydivided the training corpus into different com-ponents and integrated models trained on eachcomponent using the mixture modeling.
Howev-er, their motivation was to address the domainadaption problem, and additional genre informa-tion should be provided for the corpus partitionto create multiple models for mixture.
We in-stead present two ways for the model ensembleconstruction without extra information needed:building models by different paradigms or by arandom data sampling technique inspired by amachine learning technique.
Compared to theprior work, our approach is more general, whichcan also be used for model adaptation.
We canalso treat TMG as a smoothing way to addressthe over-estimation problem existing in almostall TMs.
Some literatures have paid attention tothis issue as well, such as Foster et al (2006)and Mylonakis and Sima ?an (2008).
However,they did not leverage information between mul-tiple models as we did, and developed on singlemodels only.
Furthermore, we also make currenttranslation probability features to contain morestatistical meanings by introducing the probabili-ty variance features into the log-linear model,which are completely novel to prior work andprovide further improvements.6 Conclusion and Future WorkIn this paper, we have investigated a simple buteffective translation model generalization me-thod that benefits by integrating values of prob-ability features between multiple TMs and usingthem in decoding phase directly.
We also intro-duce novel probability variance features into thecurrent feature sets of translation models andmake the SMT models to be more flexible.
Weevaluate our method on four state-of-the-artSMT systems, and get promising results not onlyon single-system decodings, but also on a systemcombination approach and a model combinationapproach.Making use of different distributions of trans-lation probability features is the essential of thiswork.
In the future, we will extend TMG methodto other statistical models in SMT framework,(e.g.
LM), which could be also suffered from theover-estimation problem.
And we will make fur-ther research on how to tune prior probabilitiesof models automatically as well, in order tomake our method to be more robust and tunable.ReferencesAuli Michael, Adam Lopez, Hieu Hoang, and PhilippKoehn.
2009.
A Systematic Analysis of TranslationModel Search Spaces.
In 4th Workshop on Statis-tical Machine Translation, pages 224-232.Breiman Leo.
1996.
Bagging Predictors.
MachineLearning.Chiang David.
2007.
Hierarchical Phrase BasedTranslation.
Computational Linguistics, 33(2):201-228.DeNero John, Shankar Kumar, Ciprian Chelba, andFranz Och.
2010.
Model Combination for MachineTranslation.
To appear in Proc.
of the North Amer-ican Chapter of the Association for ComputationalLinguistic.DeNeefe Steve, Kevin Knight, Wei Wang, and Da-niel Marcu.
2007.
What Can Syntax-based MTLearn from Phrase-based MT?
In Proc.
of Empiri-cal Methods on Natural Language Processing,pages 755-763.Foster George, Roland Kuhn, and Howard Johnson.2006.
Phrasetable Smoothing for Statistical Ma-chine Translation.
In Proc.
of Empirical Methodson Natural Language Processing, pages 53-61.Foster George and Roland Kuhn.
2007.
Mixture-Model Adaptation for SMT.
In 2th Workshop onStatistical Machine Translation, pages 128-135.Galley Michel, Jonathan Graehl, Kevin Knight, Da-niel Marcu, Steve DeNeefe, Wei Wang, and Igna-cio Thayer.
2006.
Scalable Inference and Trainingof Context-Rich Syntactic Translation Models.
InProc.
of 44th Meeting of the Association for Com-putational Linguistics, pages: 961-968.Huang Liang.
2008.
Forest Reranking: Discrimina-tive Parsing with Non-Local Features.
In Proc.
of46th Meeting of the Association for ComputationalLinguistics, pages 586-594.Hoeting Jennifer, David Madigan, Adrian Raftery,and Chris Volinsky.
1999.
Bayesian Model Aver-aging: A tutorial.
Statistical Science, Vol.
14, pag-es 382-417.311He Xiaodong, Mei Yang, Jianfeng Gao, PatrickNguyen, and Robert Moore.
2008.
Indirect-HMM-based Hypothesis Alignment for Combining Out-puts from Machine Translation Systems.
In Proc.of Empirical Methods on Natural LanguageProcessing, pages 98-107.Koehn Philipp.
2004.
Phrase-based Model for SMT.Computational Linguistics, 28(1): 114-133.Li Chi-Ho, Xiaodong He, Yupeng Liu, and Ning Xi.2009a.
Incremental HMM Alignment for MT sys-tem Combination.
In Proc.
of 47th Meeting of theAssociation for Computational Linguistics, pages949-957.Li Mu, Nan Duan, Dongdong Zhang, Chi-Ho Li, andMing Zhou.
2009b.
Collaborative Decoding: Par-tial Hypothesis Re-Ranking Using TranslationConsensus between Decoders.
In Proc.
of 47thMeeting of the Association for Computational Lin-guistics, pages 585-592.Liu Yang, Haitao Mi, Yang Feng, and Qun Liu.
2009.Joint Decoding with Multiple Translation Models.In Proc.
of 47th Meeting of the Association forComputational Linguistics, pages 576-584.Mylonakis Markos and Khalil Sima ?an.
2008.Phrase Translation Probabilities with ITG Priorsand Smoothing as Learning Objective.
In Proc.
ofEmpirical Methods on Natural LanguageProcessing, pages 630-639.Matusov Evgeny, Nicola Ueffi ng, and HermannNey.
2006.
Computing consensus translation frommultiple machine translation systems using en-hanced hypotheses alignment.
In Proc.
of Euro-pean Charter of the Association for ComputationalLinguistics, pages 33-40.Och Franz and Hermann Ney.
2000.
Improved Statis-tical Alignment Models.
In Proc.
of 38th Meeting ofthe Association for Computational Linguistics,pages 440-447.Och Franz.
2003.
Minimum Error Rate Training inStatistical Machine Translation.
In Proc.
of 41thMeeting of the Association for Computational Lin-guistics, pages 160-167.Och Franz and Hermann Ney.
2004.
The Alignmenttemplate approach to Statistical Machine Transla-tion.
Computational Linguistics, 30(4): 417-449.Shen Libin, Jinxi Xu, and Ralph Weischedel.
2008.
Anew string-to-dependency machine translation al-gorithm with a target dependency language model.In Proc.
of 46th Meeting of the Association forComputational Linguistics, pages 577-585.Wu Dekai.
1997.
Stochastic Inversion TransductionGrammars and Bilingual Parsing of Parallel Cor-pora.
Computational Linguistics, 23(3): 377-404.Xiong Deyi, Qun Liu, and Shouxun Lin.
2006.
Max-imum Entropy based Phrase Reordering Model forStatistical Machine Translation.
In Proc.
of 44thMeeting of the Association for Computational Lin-guistics, pages 521-528.Zollmann Andreas, Ashish Venugopal, Franz Och,and Jay Ponte.
2008.
A Systematic Comparison ofPhrase-Based, Hierarchical and Syntax-Augmented Statistical MT.
In 23rd InternationalConference on Computational Linguistics, pages1145-1152.312
