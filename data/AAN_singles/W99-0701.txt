Unsupervised Learning of Word Boundary withDescription Length GainChunyu K i t  t$Dept.
of Chinese, Translation and LinguisticsCity University of Hong Kong tc tck i t@c i tyu ,  edu.
hkYor ick  Wi lks  tDepartment of Computer ScienceUniversity of Sheffield tyor ick@dcs,  she f .
ac.
ukAbst rac tThis paper presents an unsupervised approach tolexical acquisition with the goodness measure de-scription length gain (DLG) formulated followingclassic information theory within the minimum de-scription length (MDL) paradigm.
The learningalgorithm seeks for an optimal segmentation f anutterance that maximises the description lengthgain from the individual segments.
The resultantsegments how a nice correspondence to lexicalitems (in particular, words) in a natural anguagelike English.
Learning experiments on large-scMecorpora (e.g., the Brown corpus) have shown theeffectiveness of both the learning algorithm and thegoodness measure that guides that learning.1.
In t roduct ionDetecting and handling unknown words properlyhas become a crucial issue in today's practical nat-ural language processing (NLP) technology.
Nomatter how large the dictionary that is used ina NLP system, there can be many new words inrunning/real texts, e.g., in scientific articles, news-papers and Web pages, that the dictionary doesnot include.
Many such words are proper namesand special terminology that provide critical infor-mation.
It is unreliable to rest on delimiters uchas white spaces to detect new lexical units, be-cause many basic lexical items contain one or morespaces, e.g., as in "New York", "Hong Kong" and"hot dog".
It appears that unsupervised learningtechniques are necessary in order to alleviate theproblem of unknown words in the NLP domain.There have been a number of studies on lex-ical acquisition from language data of differenttypes.
Wolff attempts to infer word bound-aries from artificially-generated natural languagesentences, heavily relying on the co-occurrencefrequency of adjacent characters \[Wolff1975,Wolff 1977\].
Nevill-Manning's text compressionprogram Sequi tur  can also identify word bound-aries and gives a binary tree structure for anidentified word \[Nevill-Mmming 1996\].
de Mar-cken explores unsupervised lexical acquisitionfrom Enghsh spoken and written corpora andfrom a Chinese written corpus \[de Marken 1995:de Marken 1996\].In this paper, we present all unsuper-vised approach to lexical acquisition within theminimum description length (MDL) paradigm\[Rissanen 1978, Rissanen 1982\] \[Rissanen 1989\],with a goodness measure, namely, the descrip-tion length gain (DLG), which is formulated in\[Kit 1998\] following classic information theory\[Shannon 1948, Cover and Thomas 1991\].
Thismeasure is used, following the MDL princi-ple, to evaluate the goodness of identifying a(sub)sequence of characters in a corpus as a lex-ical item.
In order to rigorously evaluate the ef-fectiveness of this unsupervised learning approach,we do not limit ourselves to the detection of un-known words with respect o ally given dictionary.Rather, we use it to perform unsupervised lexi-cal acquisition from large-scale English text cor-pora.
Since it is a learning-via-compression ap-proach, the algorithm can be further extended todeal with text compression and, very likely, otherdata sequencing problems.The rest of the paper is organised as follows: Sec-tion 2 presents the formulation of the DLG mea-!!!!p!!!!!!|!!!!!
!sure in terms of classic information theory; Sec-tion 3 formulates the learning algorithm within theMDL framework, which aims to achieve an opti-mal segmentation of the given corpus into lexicalitems with regard to the DLG measure; Section4 presents experiments and discusses experimentalresults with respect o previous tudies; and finally,the conclusions of the paper are given in Section 5.2.
Description Length GainKit defines the description length of a corpus X =xlx2""xn, a sequence of linguistic tokens (e.g.,characters, words, POS tags), as the Shannon-Fano code length for the corpus \[Kit 1998\].
FoPs from X.
As an extracted s is supposed to be ap-pended to the modified corpus by a string concate-nation, as shown in (2), the original corpus can beeasily recovered by a transformation that reversesthe extraction, i.e., replacing all r's in X\[r --r s\]with the string s.It is worth noting that we can achieve the pur-pose of calculating DL(X\[r --?
s\] (9 s) withoutcarrying out the string substitution operationsthroughout the original corpus.
The calculationcan be based on the token count change involvedin the substitution operatious to derive the newcorpus X\[r -+ s\] (9 s, as follows:lowing classic information theory \[Shannon 1948, DL(X\[r -+ s\] (9 s) = ~ a;'(x)log d(x) (4)i Cover and Thomas 1991\], it can be formulated in n' xEVu{r}terms of token counts in the corpus as below forempirical calculation: where d(x) is the new count ofx in the new corpusand n ' is the new corpus length.
The new countsI DL(X) = n\[-I(X) and the new length are, straightforwardly,=e(s) i fx  = r;zeV d(x) = c(x) - c(s)cs(x) + cs(x) otherwise.
I = -~-~c(x)log c(x~) (I)?
IXl n'  = n - c (s ) l s l  + c (s )  + Isl + 1(5)I where V is the set of distinct tokens (i.e., the vo- where c(x) and cs(x) are the counts of in the xcabulary) in X and c(x) is the count of x in X. original corpus X and in the string s, respectively.Accordingly, the description length gain (DLG) A key problem in this straightforward calcula-t from identifying a (sub)sequence s = sis2.." sk in tion is that we need to derive the count c(s) forthe corpus X as a segment or chunk, which is ex- all possible string s's in the original corpus X, be-_ pected to have a nice correspondence to a linguis- cause during the lexical learning process it is nec-i tically significant unit (e.g., a lexical item such as to consider all fragments (i.e., all n-grams) essarya word, or a syntactic phrase), is formulated as in the corpus in order to select a set of good cami DLG(seX) = DL(X) -  DL(X\[r--+ s\] (9 s) (2) didates for lexical items.
Kit and Wilks providean efficient method for deriving n-ga'ams of anywhere r is an index, X\[r --+ s\] represents he resul- length and their counts from large-scale corporatant corpus by the operation of replacing all occur- \[Kit and Wilks 1998\].
It has been adopted as thefences s r through out (in words, operational implementation un- of with X other basis for the f thewe extract a rule r --+ s from X) and (9 represents supervised lexical acquisition algorithm that is tothe concatenation of two strings (e.g., X\[r -+ s\] be reported in the next sections.and s) with a delimiter inserted in between.
It isstraightforward that the average DLG for extract- 3.
Learn ing  A lgor i thming an individual s from X is Given an utterance U = totl".tn as a stringI of some linguistic tokens (e.g., characters, words, DLG(s) aDLG(s) c(s) (3) POS tags), the unsupervised lexical acquisition al-gorithm seeks for an optimal segmentation OS(U)This average DLG is an estimation of the compres- over the string U such that the sum of the compres-sion effect of extracting an individual instance of sion effect over the segments i maximal.
Formally|BSs\[j\]to tt t2 .
.
.
.
.
.
ti " -  tj tj+l "" tk "" tn(A) An illustration for the Viterbi segmentation0pSeg(U = t i t2 - "  in)For k - -  0 ,1 ,2 , - - - ,n  doInitialise OS\[k\] = ?
;For j = k -  I,-.-,0 doI f  c( \ [ t j+,- - - tk\ ] )  <2,  break ;I f  DLG(0S\[j\] (0 {\[tj+l "'" tk\]}) > DLG(0S\[k\])then OS\[k\] = OS\[j\] I@ {\[tj+l"" tk\]}The final result: OS\[n\].
(B) The Viterbi segmentation algorithmFigure 1: The Viterbi algorithm for optimal seg-mentation, with an illustrationput, it looks foros(u)  =karg max ~_, aDLG(s,) (6)sl...sk s.t.
U=sl+..-+sl, ,  z=lwhere 0 < k _< n, + represents a string concate-nation and aDLG(s,) is the average DLG for eachinstance of the string s, in the original corpus, asdefined in (3) above.Based on this description length gain calcula-tion, a Viterbi algorithm is formulated to searchfor the optimal segmentation over an utterance Uthat fulfils (6).
It is presented in Figure 1 with anillustration.
The algorithm uses a list of intermedi-ate variables OS\[0\], OS\[1\], .
.
- ,  OS\[n\], each OSs\[i\]stores the optimal segmentation ver tot1 ... ti (fori = 0, 1 .2 , - .
.
,n ) .
A segmentation is an orderedset (or list) of adjacent segments.
The sign ~ rep-resents an ordered set union operation.
The DLGover a list of seg~mnts, e.g., DLG(OS\[j\]), is de-3fined as the sum of all segments' DLGs in the set:DLG(OS\[ j \ ] )= ~ DLG(s)  (7)seOS\[i\]Notice that the algorithm has a bias against heextraction of a single token as a rule, due to thefact that a single token rule bears a negative DLG.When j = k - 1, OS\[j\] ~ \ [ t j+ , .
.
- tk \ ]  becomesOS\[k - 1\] ~ {\[t~\]}, which is less preferable thanOS\[k - 1\] t~ {tk}.
The difference between the de-notations \[tk\] and tk is that the former indicatesthat the string tk is extracted from the corpus asthe right-hand side of a rule (a deterministic CFGrule), which results in a negative DLG; whereasthe latter treats tk as an individual token insteadof a segment, which has a zero DLG.It is worth noting that the breaking conditionc(\[ tj .
.
.
tk\]) < 2 in the inner loop in the algo-rithm is an empirical condition.
Its main purposeis to speed up the algorithm by avoiding fruitlessiterations on strings of count 1.
According to ourobservation in experiments, learning without thisbreaking condition leads to exactly the s.ame.re-sult on large-scale corpora but the speed is manytimes slower.
Strings with a count c = 1 can beskipped in the learning, because they are all longstrings with a negative DLG*and none of them canbecome a good segment that contributes a positivecompression effect to the entire segmentation f theISince extract ing a str ing \[t , .
.
.t~.\] of count  1 as arule does not  change any token's  count  in the new corpusC\[r -4 t, ?
.. tk\] (9 t, .
.
.
tk), except the new non- termina l  rand the del imiter  ~,  whose counts  become 1 (i.e., c(r)  =c( \ [ t , .
.
,  tk\]) = 1 and c (~)  = 1) after the  extraction.- Thus,DLG( \ [ t , .
.
.
t#\ ] )  = DL(C)  - DL(C \ [ r  -4 t , .
.
.
t#\ ]  % t , .
.
.
tk )= -- Z c(t) log 2 c(t) ZtEV tE %"U{r.~9 }c(t) ~ c(t',= - ~ c(t)(log= I -~  - log~ ) + ~ c(0 log=,.
ICl -~tEV tE{r,~}ICl + 2 .
^, 1= - ~ c(0 log= -TSF  + z,og2 ICl + 2fEV= - IC l  log= ICl + 2 2 logdlCl + 2)ICl= - ( IC l  + 2)log2(ICl + 2)) + ICl log= ICl<0!IIutterance.
Rather, they can be broken into shortersegments with a positive DLG.Time complexity analysis also shows that thisbreaking condition can speed up the algorithm sig-nificantly.
Without this condition, the time com-plexity of the algorithm is O(n2).
With it, thecomplexity is bounded by O(mn), where m is themaximal common prefix length of sub-strings (i.e.,n-grams) in the corpus.
Accordingly, the averagetime complexity of the algorithm is O(an): where ais the average common prefix length in the corpus,which is much smaller than m.4.
Exper imentsWe have conducted a series of lexical acquisitionexperiments with the above algorithm on large-scale English corpora, e.g., the Brown corpus\[Francis and Kucera 1982\] and the PTB WSJ cor-pus \[Marcus et al 1993\].
Below is the segmenta-tion result on the first few sentences in the Browncorpus:\[the\] \[_fulton_county\] \[_grand_jury\] \[_said_\] \[friday_\]\[an\] \[_investigation_of\] \[_atlanta\] \[_'s_\] \[recent\]\[_primary_\] \[election\] \[_produced\] \[_' '_no\] \[_evidence\]\[_' '_\] \[that_any\] \[_irregularities\] \[_took_place_\] \[@\]\[_the_jury\] \[_further\] \[_said_\] \[in_term\] I-el \[nd_\]\[present\] \[ments\] \[_that_\] \[the_city_\] \[executive\]\[_committee\] \[_ ,_which_had\] \[_over-all_\] \[charge_of\]\[_the_election\] \[_, _' ' _\] \[deserves\] \[_the_\] \[praise\]\[_and_\] \[thanks\] \[_of_the_c\] \[ity_of_\] \[atlanta\] \[_''_\]\[for\] \[_the_manner_in_which\] \[_the_election\] \[_was\]\[_conducted_\] \[@\] \[_the\] \[_september\] \[-\] \[october_\]\[term\] \[_jury\] \[_had_been_\] \[charge\] \[d_by_\] \[fulton_\]\[superior_court\] \[_judge\] \[_dur\] \[wood_\] \[py\] \[e_to\]\[_investigat\] \[e\] \[_reports_of_\] \[possible\] \[_" _\]\[irregularities\] \[_ ' ' _\] \[in_the\] \[_hard-\] \[fought_\]\[primary\] \[_which_was_\] \[w\] \[on_by_\] \[mayor\] \[-\] \[nominat\]\[e_\] \[iv\] Jan_allen_\] \[jr\] \[_..\] \[_' '_\] \[only_a\] \[_relative\]\[_handful_of\] \[_such_\] \[reports\] \[_,as\] \[_received\]\[_' '_, _\] \[the_jury\] \[_said_,_" _\] \[considering_the\]\[_widespread\] \[_interest_in_\] \[the_election\] \[_,_\]\[the_number_of_\] \[vo\] \[ters_and_\] \[the_size_of\]\[_this_c\] \[ity_' '_\] \[@\] \[_the_jury_said\] \[_it_did\]\[_find\] \[_that_many_of\] \[_georgia_' s\] \[_registration\]\[_and_\] \[election\] \[_laws\] \[_"_\] \[are_\] \[out\] \[mode\] \[d_\]\[or\] \[_inadequate\] \[_and_often_\] \[ambiguous\] \[.
"_\] \[@\]\[_it\] \[_recommended\] \[_that_\] \[fulton\] \[_legislators_\]\ [act \ ]  [_' '_\] \[to_have_the\] Is\] \[e_lavs\] \[_studied_\]\[and_\] \[revi\] \[sed_to\] \[_the_end_of_\] \[moderniz\] ling\]\[_and_improv\] ling_them\] \[_"_\] \[@\] \[_the\] \[_grand_jury\]\[_commented_\] \[on\] \[_a_number_of_\] \[other_\] \[top\] \[ics_,_\]\[among_them\] \[_the_atlant\] \[a\] \[_and_\] \[fulton_county\]\[_purchasing\] \[_department\] Is_which_\] \[it\] \[_said\] \[_' '_\]\[are_well\] \[_operated_\] land_follow\] \[_generally.\]\[accepted_\] \[practices\] \[_which_in\] lure_to\] \[_the_best\]\[_interest\] \[_of_both\] \[_government\] Is_' '_\] \[@\]4where uppercase l tters are converted to lowercaseones, the spaces are visualised by all underscoreand the full-stops are all replaced by (@'s.Although a space is not distinguished from anyother characters for the learner, we have to relyon the spaces to judge the correctness of a wordboundary prediction: a predicted word boundaryimmediately before or after a space is judged ascorrect.
But we also have observed that this cri-terion overlooks many meaningful predictions like"-.-charge\] [d_by-..", "---are_outmode\] \[d_.-.
"and ".--government\] \[s...:'.
If this is taken intoaccount, the learning pcrformance is evidently bet-ter than the precision and recall figures reportedin Table 1 below.Interestingly, it is observed that n-gram countsderived from a larger volume of data can signifi-cantly improve the precision but decrease the recallof the word boundary prediction.
The correlationbetwee, the volume of data used tbr deriving n-gram counts and the change of precision and recallis shown in Table 1.
The effectiveness of the un-supervised learning is evidenced by the fact thatits precision and recall are, respectively, ~tll tl~reetimes as high as the precision and recall by randomguessing.
The best learning performance, in termsof both precision and recall, in the experiments io the one with 79.33% precision and 63.01~ recall,obtained from the experiment on the e,ltire Browncorpus.Table 1: The correlation between corpus size (mil-lion char.)
and precision/recallIt is straightforwardly understandable that theincrease of data volume leads to a significant in-crease of precision in the learning, because pre-diction based on more data is more reliable.
Thereason for the drop of recall is that when the vol-ume of data increases, more multi-word stringshave a higher compression effect (than individualwords) and, consequently: they are learned by thelearner as lexical items, e.g., \ [ fulton_county\] ,\[grand_jury\] and \[_took_place\].
If the creditin such nmlti-word lexical items is counted, the re-call nmst be much better than the one in Table1.
Of course, this also reflects a limitation of thelearning algorithm: it only conducts an optimalsegmentation i stead of a hierarchical chunking onan utterance.The precision and recall reported above is nota big surprise.
To our knowledge, however, itis the first time that the performance of unsu-pervised learning of word boundaries is exam-ined with the criteria of both precision and re-call.
Unfortunately, this performance can't becompared with any previous studies, for sev-eral reasons.
One is that the learning re-sults of previous studies are not presented ina comparable manner, for example, \[Wolff 1975,Wolff 1977\] and \[Nevill-Manning 1996\], as notedby \[de Marken 1996\] as well.
Another is that thelearning outcomes are different.
For example, theoutput of lexical learning from an utterance (as acharacter sequence) in \[Nevill-Manning 1996\] and\[de Marken 1995, de Marken 1996\] is a hierarchi-cal chunking of the utterance.
The chance to hitthe correct words in such chunking is obviouslymany times higher than that in a flat segmenta-tion.
The hierarchical chunking leads to a recallabove 90% in de Marken's work.
Interestingly,however, de Marken does not report the preci-sion, which seems too low, therefore meaningless,to report, because the learner produces o manychunks.5.
Conc lus ions  and  Future  WorkWe have presented an unsupervised learning algo-rithm for lexical acquisition based on the goodnessmeasure description length gain formulated follow-ing information theory.
The learning algorithnl fol-lows the essence of the MDL principle to search forthe optimal segmentation f an utterance that hasthe maximal description length gain (and there-fore approaches the minimum description lengthof the utterance).
Experiments on word boundaryprediction with large-scale corpora have shown theeffectiveness of the learning algorithm.For the time being, however, we are unable tocompare the learning performance with other re-searchers' previous work, simply because they donot present he performance of their learning al-gorithms in terms of the criteria of both precisionand recall.
Also, our algorithm is significantly sim-pler, in that it rests on n-gram counts only, insteadof any more complicated statistical data or a moresophisticated training algorithm.Our future work will focus on the investi-gation into two aspects of the lexical learningwith the DLG measure.
First, we will incor-porate tile expectation-maximization (EM) algo-rithm \[Dempster tal.
1977\] into our lexical earn-ing to see how nmch performance can be improved.Usually, a more sophisticated learning algorithmleads to a better learning result.
Second, we willexplore the hierarchical chunking with the DLGmeasure.
We are particularly interested to knowhow nmch nmre compression effect can be furthersqueezed out by hierarchical chunking from a textcorpus (e.g., the Brown corpus) and how much im-provement in the recall can be achieved.AcknowledgementsThe first author gratefully acknowledges a Univer-sity of Sheffield Research Scholarship awarded tohim that enables him to undertake this work.
Wewish to thank two anonymous reviewers for theirinvaluable comments, and thank Hamish Cun-ningham, Ted Dunning, Rob Gaizauskas, RandyLaPolla, Steve Renals, Jonathon Webster andmany other colleagues for various kinds of help anduseful discussions.Re ferences\[Cover and Thomas 1991\] Cover~ T. M., and J. A.Thomas, 1991.
Elements of Information Theory.John Wiley and Sons, Inc.. New York.\[de Marken 1995\] de Marken, C. 1995.
Tile Unsu-pervised Acquisition of a Lexicon from Contin-uous Speech.
Technical Report A.I.
Memo No.1558, AI Lab., MIT.
Cambridge, Massachusetts.\[de Marken 1996\] de Marken, C. 1996.
Unsuper-vised Language Acquisition.
Ph.D. dissertation,MIT, Cambridge, Massachusetts.I,!
!i!\[Dempster tal.
1977\] Dempster, A. P., N. M.Laird, and D. B. Rubin.
1977.
Maximum like-lihood from incomplete data via the EM algo-rithm.
Journal of the Royal Statistical Society,39(B):1-38.\[Francis and Kucera 1982\] Francis, W. N., and H.Kucera.
1982.
Frequency Analysis o\] English Us-age: Lexical and Grammar.
Houghton-Mifflin,Boston.\[Kit 1998\] Kit, C. 1998.
A goodness measurefor phrase learning via compression with theMDL principle.
In The ESSLLI-98 StudentSession, Chapter 13, pp.175-187.
Aug. 17-28,Saarbriiken.\[Kit and Wilks 1998\] Kit, C., and Y. Wilks.
1998.The Virtual Corpus approach to deriving n-gramstatistics front large scale corpora.
In C. N.Huang (ed.
), Proceedings of 1998 InternationalConference on Chinese Information Processing,pp.223-229.
Nov. 18-20, Beijing.\[Li and Vit?nyi 1993\] Li, M., and P. M. B.Vit?nyi.
1993.
Introduction to Kolmogorov Com-plexity and its Applications.
Springer-Verlag,New York.
Second edition, 1997.\[Marcus et al 1993\] Marcus, M., B. Santorini andM.
Marcinkiewicz.
1993.
Building a large an-notated corpus of English: The Penn Treebank.Computational Linguistics, 19(2):313-330.\[Nevill-Manning 1996\] Nevill-Manning, C. G. In-ferring Sequential Structure.
Ph.D. dissertation,University of Waikato, New Zealand.\[Powers 1997\] Powers, D. M. W. Unsupervisedlearning of linguistic structure: an empiricalevaluation.
International Journal of Corpus Lin-guistics: 2(1):91-131.\[Rissanen 1978\] Rissanen, J.
1978.
Modelling byshortest data description.
Automatica, 14:465-471.\[Rissanen 1982\] Rissanen, J.
1982.
A universalprior for integers and estimation by mil~imumdescription length.
Ann.
Statist.. 11:416-431.\[Rissanen 1989\] Rissanen, J.
1989.Com-plexity in Statistical Inquiry.entific, N.J.StochasticWorld Sci-\[Shannon 1948\] Shannon, C. 1948.
A mathemati-cal theory of communication.
Bell System Tech-nical Journal, 27:379-423, 623-656.\[Solomonoff 1964\] Solomonoff, R. J.
1964.
A for-real theory of inductive inference, part 1 and 2.Information Control, 7:1-22, 224-256.\[Stolcke 1994\] Stolcke, A.
1994.
Bayesian Learn-ing of Probabilistic Language Models.
Ph.D. dis-sertation, UC Berkeley, CA.\[Vit~nyi and Li 1996\] VitgLnyi, P. M. B., and M.Li.
1996.
Minimum Description Length Induc-tion, Bayesianism, and Kohnogorov Complexity.Manuscript, CWI, Amsterdam.\[Wolff 1975\] Wolff, J. G. An algorithm fbr thesegmentation of an artificial anguage analogue.British Journal of Psychology, 66:79-90.\[Wolff 1977\] Wolff, J. G. The discoverj, of seg-ments in natural language.
British Journal ofPsychology, 68:97-106.\[Wolff 1982\] Wolff, J. G. Language acquisition,data compression and generalization.
Languageand Communication, 2:57-89.
