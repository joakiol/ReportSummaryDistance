MMR-based feature selection for text categorizationChangki LeeDept.
of Computer Science and EngineeringPohang University of Science & TechnologySan 31, Hyoja-Dong, Pohang,790-784, South Koreaphone: +82-54-279-5581leeck@postech.ac.krGary Geunbae LeeDept.
of Computer Science and EngineeringPohang University of Science & TechnologySan 31, Hyoja-Dong, Pohang,790-784, South Koreaphone: +82-54-279-5581gblee@postech.ac.krAbstractWe introduce a new method of feature selec-tion for text categorization.
Our MMR-basedfeature selection method strives to reduce re-dundancy between features while maintaininginformation gain in selecting appropriate fea-tures for text categorization.
Empirical resultsshow that MMR-based feature selection ismore effective than Koller & Sahami?smethod, which is one of greedy feature selec-tion methods, and conventional informationgain which is commonly used in feature selec-tion for text categorization.
Moreover, MMR-based feature selection sometimes producessome improvements of conventional machinelearning algorithms over SVM which isknown to give the best classification accuracy.1  IntroductionText categorization is the problem of automatically as-signing predefined categories to free text documents.
Agrowing number of statistical classification methods andmachine learning techniques have been applied to textcategorization in recent years [9].A major characteristic, or difficulty, of text catego-rization problems is the high dimensionality of the fea-ture space [10].
The native feature space consists of theunique terms that occur in documents, which can be tensor hundreds of thousands of terms for even a moderate-sized text collection.
This is prohibitively high for manymachine learning algorithms.
If we reduce the set offeatures considered by the algorithm, we can serve twopurposes.
We can considerably decrease the runningtime of the learning algorithm, and we can increase theaccuracy of the resulting model.
In this line, a numberof researches have recently addressed the issue of fea-ture subset selection [2][4][8].
Yang and Pedersonfound information gain (IG) and chi-square test (CHI)most effective in aggressive term removal without los-ing categorization accuracy in their experiments [8].Another major characteristic of text categorizationproblems is the high level of feature redundancy [11].While there are generally many different features rele-vant to classification task, often several such cues occurin one document.
These cues are partly redundant.
Na-?ve Bayes, which is a popular learning algorithm, iscommonly justified using assumptions of conditionalindependence or linked dependence [12].
However, the-ses assumptions are generally accepted to be false fortext.
To remove these violations, more complex de-pendence models have been developed [13].Most previous works of feature selection empha-sized only the reduction of high dimensionality of thefeature space [2][4][8].
The most popular feature selec-tion method is IG.
IG works well with texts and hasoften been used.
IG looks at each feature in isolationand measures how important it is for the prediction ofthe correct class label.
In cases where all features arenot redundant with each other, IG is very appropriate.But in cases where many features are highly redundantwith each other, we must utilize other means, for exam-ple, more complex dependence models.In this paper, for the high dimensionality of the fea-ture space and the high level of feature redundancy, wepropose a new feature selection method which selectseach feature according to a combined criterion of infor-mation gain and novelty of information.
The lattermeasures the degree of dissimilarity between the featurebeing considered and previously selected features.Maximal Marginal Relevance (MMR) provides pre-cisely such functionality [5].
So we propose MMR-based feature selection method which strives to reduceredundancy between features while maintaining infor-mation gain in selecting appropriate features for textcategorization.In machine learning field, some greedy methods thatadd or subtract a single feature at a time have been de-veloped for feature selection [3][14].
S. Della Pietra etal.
proposed a method for incrementally constructingrandom field [14].
Their method builds increasinglycomplex fields to approximate the empirical distributionof a set of training examples by allowing features.
Fea-tures are incrementally added to the field using a top-down greedy algorithm, with the intent of capturing thesalient properties of the empirical sample while allow-ing generalization to new configurations.
However themethod is not simple, and this is problematic both com-putationally and statistically in large-scale problems.Koller and Sahami proposed another greedy featureselection method which provides a mechanism foreliminating features whose predictive information withrespect to the class is subsumed by the other features [3].This method is also based on the Kullback-Leibler di-vergence to minimize the amount of predictive informa-tion lost during feature elimination.In order to compare the performances of our methodand greedy feature selection methods, we implementedKoller and Sahami?s method, and empirically tested it insection 4.We also compared the performance of conventionalmachine learning algorithms using our feature selectionmethod with Support Vector Machine (SVM) using allfeatures in section 4.
Previous works show that SVMconsistently achieves good performance on text catego-rization tasks, outperforming existing methods substan-tially and significantly [10][11].
With its ability togeneralize well in high dimensional feature spaces andhigh level of feature redundancy, SVM is known that itdoes not need any feature selection [11].The remainder of this paper is organized as follows.In section 2, we describe the Maximal Marginal Rele-vance, and in section 3, we describe the MMR-basedfeature selection.
Section 4 presents the in-depth ex-periments and the results.
Section 5 concludes the re-search.2 Maximal Marginal RelevanceMost modern IR search engines produce a ranked list ofretrieved documents ordered by declining relevance tothe user's query.
In contrast, the need for ?relevant nov-elty?
was motivated as a potentially superior criterion.
Afirst approximation to relevant novelty is to measure therelevance and the novelty independently and provide alinear combination as the metric.The linear combination is called ?marginal rele-vance?
- i.e.
a document has high marginal relevance ifit is both relevant to the query and contains minimalsimilarity to previously selected documents.
In docu-ment retrieval and summarization, marginal relevance isstrived to maximize, hence the method is labeled?Maximal Marginal Relevance?
(MMR) [5].?????????=??
),(max)1(),(max 21\ jiSDiSRD DDSimQDSimArgMMRji?
?where C={D1,?,Di,?}
is a document collection (ordocument stream); Q is a query or user profile; R =IR(C, Q, ?
), i.e., the ranked list of documents retrievedby an IR system, given C and Q and a relevance thresh-old ?
, below which it will not retrieve documents (?can be degree of match or number of documents); S isthe subset of documents in R which is already selected;R\S is the set difference, i.e.
the set of as yet unselecteddocuments in R; Sim1 is the similarity metric used indocument retrieval and relevance ranking betweendocuments (passages) and a query; and Sim2 can be thesame as Sim1 or a different metric.3  MMR-based Feature SelectionWe propose a MMR-based feature selection whichselects each feature according to a combined criterion ofinformation gain and novelty of information.
We defineMMR-based feature selection as follows:?????????=??
)|;(max)1();(max_\CwwIGpairCwIGArgFSMMRjiSwiSRw ji?
?where C is the set of class labels, R is the set of candi-date features, S is the subset of features in R which wasalready selected, R\S is the set difference, i.e.
the set ofas yet unselected features in R, IG is the informationgain scores, and IGpair is the information gain scores ofco-occurrence of the word (feature) pairs.
IG and IGpairare defined as follows:???++?=kikikikikikikkkiwCpwCpwpwCpwCpwpCpCpCwIG)|(log)|()()|(log)|()()(log)();(??
?++?=kjikjikjikjikjikjikkkjiwCpwCpwpwCpwCpwpCpCpCwwIGpair)|(log)|()()|(log)|()()(log)()|;(,,,,,,where p(wi) is the probability that word wi occurred, iwmeans that word wi doesn?t occur, p(Ck) is the probabil-ity of the k-th class value, p(Ck|wi) is the conditionalprobability of the k-th class value given that wi occurred,p(wi,j) is the probability that wi and wj co-occurred, andiw  means that wi and wj doesn?t co-occur but wi or wjcan occur (i.e.
)(1)( ,, jiji wpwp ?= ).Given the above definition, MMR_FS computes in-crementally the information gain scores when theparameter ?
=1, and computes a maximal diversityamong the features in R when ?
=0.
For intermediatevalues of ?
in the interval [0,1], a linear combination ofboth criteria is optimized.4 ExperimentsIn order to compare the performance of MMR-basedfeature selection method with conventional IG andgreedy feature selection method (Koller & Sahami?smethod, labeled ?Greedy?
), we evaluated the three fea-ture selection methods with four different learning algo-rithms: naive Bayes, TFIDF/Rocchio, ProbabilisticIndexing (PrTFIDF [7]) and Maximum Entropy usingRainbow [6].We also compared the performance of conventionalmachine learning algorithms using our feature selectionmethod and SVM using all features.MMR-based feature selection and greedy feature se-lection method (Koller & Sahami?s method) requiresquadratic time with respect to the number of features.To reduce this complexity, for each data set, we firstselected 1000 features using IG, and then we appliedMMR-based feature selection and greedy feature selec-tion method to the selected 1000 features.For all datasets, we did not remove stopwords.
Theresults reported on all dataset are averaged over 10times of different test/training splits.
A random subsetof 20% of the data considered in an experiment wasused for testing (i.e.
we used Rainbow?s ?--test-set=0.2?and ?--test=10?
options), because Rainbow does notsupport 10-fold cross validation.MMR-based feature selection method needs to tunefor ?
.
It appears that a tuning method based on held-outdata is needed here.
We tested our method using 11 ?values (i.e.
0, 0.1, 0.2, ?, 1) and selected the best ?value.4.1 Reuters-21578The Reuters-21578 corpus contains 21578 articles takenfrom the Reuters newswire.
Each article is typicallydesignated into one or more semantic categories such as?earn?, ?trade?, ?corn?
etc., where the total number ofcategories is 114.Following [3], we constructed a subset from Reutercorpus.
The subset is comprised of articles on the topic?coffee?, ?iron-steel?, and ?livestock?.4.2 WebKBThis data set contains WWW-pages collected fromcomputer science departments of various universities inJanuary 1997 by the World Wide Knowledge Base(WebKb) project of the CMU text learning group.
The8282 pages were manually classified into 7 categories:?course?, ?department?, ?faculty?, ?project?, ?staff?, ?stu-dent?
and ?other?.
Following [1], we discarded the cate-gories ?other?, ?department?
and ?staff?.
The remainingpart of the corpus contains 4199 documents in fourcategories.4.3 Experimental ResultsFigure 1 displays the performance curves for four dif-ferent machine learning algorithms on the subset ofReuters after term selection using MMR-based featureselection (number of features is 25).
When the parame-ter ?
=0.5, most machine learning algorithms have bestperformance and significant improvements compared toconventional information gain (i.e.
?
=1) and SVM us-ing all features.Table 1.
WebKB.Table 1 shows the performance of four machinelearning algorithms on WebKB using three feature se-lection methods and all features (41763 terms).
In thisdata set, again MMR-based feature selection has bestperformance and significant improvements compared togreedy method and IG.
Using MMR-based feature se-lection, for example, the vocabulary is reduced from41763 terms to 200 (a 99.5% reduction), and the accu-racy is improved from 85.26% to 90.49% in Na?veBayes.
Using greedy method and IG, however, the accu-racy is improved from 85.26% to about 87% in Na?veFigure 1.
MMR feature selection for four machinelearning algorithms on Reuters (#features=25).Bayes.
PrTFIDF is most sensitive to feature selectionmethod.
Using MMR-based feature selection the bestaccuracy is 82.47%.
Using greedy method and IG, how-ever, the best accuracy is only 72~74%.
In this dataset,however, MMR-based feature selection does not pro-duce improvements of conventional machine learningalgorithms over SVM.The observation in Reuters and WebKB are highlyconsistent.
MMR-based feature selection is consistentlymore effective than greedy method and IG on two datasets, and sometimes produces improvements even overthe best SVM.5 ConclusionIn this paper, we proposed a MMR-based feature selec-tion method which strives to reduce redundancy be-tween features while maintaining information gain inselecting appropriate features for text categorization.We carried out extensive experiments to verify theproposed method.
Based on the experiment results, wecan verify that MMR-based feature selection is moreeffective than Koller & Sahami?s method, which is onekind of greedy methods, and conventional informationgain which is commonly used in feature selection fortext categorization.
Besides, MMR-based feature selec-tion method sometimes produces improvements of con-ventional machine learning algorithms over SVM whichis known to give the best classification accuracy.A disadvantage in using MMR-based feature selectionis that the computational cost of computing the pairwiseinformation gain (i.e.
IGpair) is quadratic time withrespect to the number of features.
To reduce this compu-tational cost, we can use MMR-based feature selectionmethod on the reduced feature set resulting from IG asour experiments in section 4.
Another drawback of ourmethod is the need to tune for ?
.
It appears that a tun-ing method based on held-out data is needed hereReferences[1] Andrew Mccallum and Kamal Nigam.
1998.
AComparison of Event Models for Naive Bayes TextClassification.
In AAAI-98 Workshop on Learningfor Text Categorization.
[2] David D. Lewis and Marc Ringuette.
1994.
A Com-parison of Two Learning Algorithms for Text Cate-gorization.
In Proceedings of SDAIR-94, 3rd AnnualSymposium on Document Analysis and InformationRetrieval.
[3] Daphne Koller and Mehran Sahami.
1996.
TowardOptimal Feature Selection.
In Proceedings of ICML-96, 13th International Conference on Machine Learn-ing.
[4] Hinrich Sch?tze and David A.
Hull, and Jan O.Pedersen.
1995.
A Comparison of Classifiers andDocument Representations for the Routing Problem.In Proceedings of the 18th Annual InternationalACM SIGIR Conference on Research and Develop-ment in Information Retrieval.
[5] Jaime Carbonell and Jade Goldstein.
1998.
The Useof MMR, Diversity-Based Reranking for ReorderingDocuments and Producing Summaries.
In Proceed-ings of the 21st ACM-SIGIR International Confer-ence on Research and Development in InformationRetrieval.
[6] McCallum and Andrew Kachites.
1996.
Bow: Atoolkit for statistical language modelling, text re-trieval, classification and clustering.http://www.cs.cmu.edu/~mccallum/bow.
[7] Thorsten Joachims.
1997.
A probabilistic analysisof the Rocchio algorithm with TFIDF for text catego-rization.
In Proceedings of ICML-97, 14th Interna-tional Conference on Machine Learning.
[8] Yiming Yang and Jan O. Pedersen.
1997.
A Com-parative Study on Feature Selection in Text Catego-rization.
In Proceedings of ICML-97, 14thInternational Conference on Machine Learning.
[9] Yiming Yang and Xin Liu.
1999.
A re-examinationof text categorization methods.
In Proceedings of the22nd ACM-SIGIR International Conference on Re-search and Development in Information Retrieval.
[10] Thorsten Joachims.
1998.
Text Categorization withSupport Vector Machines: Learning with Many Rele-vant Features.
In Proceedings of ECML-98, 10thEuropean Conference on Machine Learning.
[11] Thorsten Joachims.
2001.
A Statistical LearningModel of Text Classification for Support Vector Ma-chines.
In Proceedings of the 24th ACM-SIGIR In-ternational Conference on Research andDevelopment in Information Retrieval.
[12] William S. Cooper.
1991.
Some Inconsistenciesand Misnomers in Probabilistic Information Re-trieval.
In Proceedings of the 14th ACM SIGIR In-ternational Conference on Research andDevelopment in Information Retrieval.
[13] Mehran Sahami.
1998.
Using Machine Learning toImprove Information Access.
PhD thesis, StanfordUniversity.
[14] Stephen Della Pietra, Vincent Della Pietra, andJohn Lafferty.
1997.
Inducing Features of RandomFields.
IEEE Transactions on Pattern Analysis andMachine Intelligence.
