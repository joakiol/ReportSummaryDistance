Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1501?1511,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsLearning Semantic Word Embeddings based onOrdinal Knowledge ConstraintsQuan Liu?and Hui Jiang?and Si Wei?and Zhen-Hua Ling?and Yu Hu?
?National Engineering Laboratory for Speech and Language Information ProcessingUniversity of Science and Technology of China, Hefei, China?Department of Electrical Engineering and Computer Science, York University, Canada?iFLYTEK Research, Hefei, Chinaemails: quanliu@mail.ustc.edu.cn, hj@cse.yorku.ca,siwei@iflytek.com, zhling@ustc.edu.cn, yuhu@iflytek.comAbstractIn this paper, we propose a general frame-work to incorporate semantic knowledgeinto the popular data-driven learning pro-cess of word embeddings to improve thequality of them.
Under this framework,we represent semantic knowledge as manyordinal ranking inequalities and formu-late the learning of semantic word embed-dings (SWE) as a constrained optimiza-tion problem, where the data-derived ob-jective function is optimized subject to allordinal knowledge inequality constraintsextracted from available knowledge re-sources such as Thesaurus and Word-Net.
We have demonstrated that this con-strained optimization problem can be ef-ficiently solved by the stochastic gradientdescent (SGD) algorithm, even for a largenumber of inequality constraints.
Experi-mental results on four standard NLP tasks,including word similarity measure, sen-tence completion, name entity recogni-tion, and the TOEFL synonym selection,have all demonstrated that the quality oflearned word vectors can be significantlyimproved after semantic knowledge is in-corporated as inequality constraints duringthe learning process of word embeddings.1 IntroductionDistributed word representation (i.e., word embed-ding) is a technique that represents words as con-tinuous vectors, which is an important researchtopic in natural language processing (NLP) (Hin-ton et al, 1986; Turney et al, 2010).
In re-cent years, it has been widely used in variousNLP tasks, including neural language model (Ben-gio et al, 2003; Schwenk, 2007), sequence la-belling tasks (Collobert and Weston, 2008; Col-lobert et al, 2011), machine translation (Devlin etal., 2014; Sutskever et al, 2014), and antonym se-lection (Chen et al, 2015).
Typically, word vectorsare learned based on the distributional hypothesis(Harris, 1954; Miller and Charles, 1991), whichassumes that words with a similar context tendto have a similar meaning.
Under this hypothe-sis, various models, such as the skip-gram model(Mikolov et al, 2013a; Mikolov et al, 2013b;Levy and Goldberg, 2014) and GloVe model (Pen-nington et al, 2014), have been proposed to lever-age the context of each word in large corpora tolearn word embeddings.
These methods can ef-ficiently estimate the co-occurrence statistics tomodel contextual distributions from very large textcorpora and they have been demonstrated to bequite effective in a number of NLP tasks.
How-ever, they still suffer from some major limitations.In particular, these corpus-based methods usu-ally fail to capture the precise meanings for manywords.
For example, some semantically relatedbut dissimilar words may have similar contexts,such as synonyms and antonyms.
As a result, thesecorpus-based methods may lead to some antony-mous word vectors being located much closer inthe learned embedding space than many synony-mous words.
Moreover, as word representationsare mainly learned based on the co-occurrence in-formation, the learned word embeddings do notcapture the accurate relationship between two se-mantically similar words if either one appears lessfrequently in the corpus.To address these issues, some recent work hasbeen proposed to incorporate prior lexical knowl-edge (WordNet, PPDB, etc.)
or knowledge graph(Freebase, etc.)
into word representations.
Suchknowledge enhanced word embedding methodshave achieved considerable improvements on var-ious natural language processing tasks, like (Yuand Dredze, 2014; Bian et al, 2014; Xu et al,2014).
These methods attempt to increase the se-mantic similarities between words belonging to1501one semantic category or to explicitly model thesemantic relationships between different words.For example, Yu and Dredze (2014) have proposeda new learning objective function to enhanceword embeddings by combining neural modelsand a prior knowledge measure from semantic re-sources.
Bian et.
al (2014) have recently proposedto leverage morphological, syntactic, and semanticknowledge to improve the learning of word em-beddings.
Besides, a novel framework has beenproposed in (Xu et al, 2014) to take advantage ofboth relational and categorical knowledge to learnhigh-quality word representations, where two reg-ularization functions are used to model the re-lational and categorical knowledge respectively.More recently, a retrofitting technique has been in-troduced in (Faruqui et al, 2014) to improve se-mantic vectors by leveraging lexicon-derived rela-tional information in a post-processing stage.In this paper, we propose a new and flexiblemethod to incorporate semantic knowledge intothe corpus-based learning of word embeddings.In our approach, we propose to represent seman-tic knowledge as many word ordinal ranking in-equalities.
Furthermore, these inequalities are castas semantic constraints in the optimization pro-cess to learn semantically sensible word embed-dings.
The proposed method has several advan-tages.
Firstly, many different types of seman-tic knowledge can all be represented as a num-ber of such ranking inequalities, such as synonym-antonym, hyponym-hypernym and etc.
Secondly,these inequalities can be easily extracted frommany existing knowledge resources, such as The-saurus, WordNet (Miller, 1995) and knowledgegraphs.
Moreover, the ranking inequalities canalso be manually generated by human annotationbecause ranking orders is much easier for humanannotators than assigning specific scores.
Next,we present a flexible learning framework to learndistributed word representation based on the ordi-nal semantic knowledge.
By solving a constrainedoptimization problem using the efficient stochas-tic gradient descent algorithm, we can obtain se-mantic word embedding enhanced by the ordinalknowledge constraints.
Experiments on four pop-ular natural language processing tasks, includingword similarity, sentence completion, name en-tity recognition and synonym selection, have alldemonstrated that the proposed method can learngood semantically sensible word embeddings.2 Representing Knowledge By RankingMany types of lexical semantic knowledge can bequantitatively represented by a large number ofranking inequalities such as:similarity(wi, wj) > similarity(wi, wk) (1)where wi, wjand wkdenote any three words invocabulary.
For example, eq.
(1) holds if wjis asynonym of wiand wkis an antonym of wi.
Ingeneral, the similarity between a word and its syn-onymous word should be larger than the similar-ity between the word and its antonymous word.Moreover, a particular word should be more sim-ilar to the words belonging to the same semanticcategory as this word than other words belongingto a different category.
Besides, eq.
(1) holds if wiand wjhave shorter distance in a semantic hierar-chy than wiand wkdo in the same hierarchy (Lea-cock and Chodorow, 1998; Jurafsky and Martin,2000).Equivalently, each of the above similarity in-equalities may be represented as the followingconstraint in the embedding space:sim(w(1)i,w(1)j) > sim(w(1)i,w(1)k) (2)where w(1)i, w(1)jand w(1)kdenote the embeddingvectors of the words, wi, wjand wk.In this paper, we use the following three rules togather the ordinal semantic knowledge from avail-able lexical knowledge resources, such as The-saurus and WordNet.?
Synonym Antonym Rule: Similarities be-tween a word and its synonymous wordsare always larger than similarities be-tween the word and its antonymous words.For example, the similarity between fool-ish and stupid is expected to be biggerthan the similarity between foolish andclever, i.e., similarity(foolish, stupid) >similarity(foolish, clever).?
Semantic Category Rule: Similarities ofwords that belong to the same semantic cat-egory would be larger than similarities ofwords that belong to different categories.This rule refers to the idea of Fisher lin-ear discriminant algorithm in (Fisher, 1936).A semantic category may be defined as asynset in WordNet, a hypernym in a se-mantic hierarchy, or an entity category in1502...wk-N wk-N+1 wk+N-1 wk+NwkSkip-gramwi wjd(k,i) > d(k,j)wp wqd(k,p) > d(k,q)......Semantic Ordinal Constraintswt-c wt-c+1 wt+c-1 wt+cwtMachine: Hi, please give a similarity value to those two words.Human: Uh, it is difficult to do it, just give me a little time.Paradigm shift?> ranking idea.Machine: Hi, here are two word pairs, would you please give asimilarity ranking decision for them?Human: Yes, here you are!W(1)W(2)wt-c wt-c+1 wt+c-1 wt+cwtW(1)W(2)sim(wt,wi) > sim(wj,wk)wi wj wkShare EmbeddingSemantic Knowledge ResourcesW(1)WordNetHuman LabelingKnowledgeHasKnowledge?MinimizeQsemSemanticKnowledgeInput WordYESMaximizeQbaseWord SequenceGet training samplesColorRed Blue GreenPupleToolHammer SawChiselMalletwt-c wt-c+1 wt+c-1 wt+cwtW(1)W(2)sim(wi,wj) > sim(wj,wk)wi wj wkShare EmbeddingSemantic Knowledge ResourcesW(1)WordNetHuman LabelingKnowledgeBasesObjective-1:QbaseObjective-2:QsemJointOptimizationPlessor JigsawHacksawHypernymHyponyms Co-HyponymsFigure 1: An example of hyponym and hypernym.knowledge graphs.
Figure 1 shows a sim-ple example of the relationship between hy-ponyms and hypernyms.
From there, it isreasonable to assume the following similar-ity inequality: similarity(Mallet,Plessor) >similarity(Mallet,Hacksaw).?
Semantic Hierarchy Rule: Similarities be-tween words that have shorter distances ina semantic hierarchy should be larger thansimilarities of words that have longer dis-tances.
In this work, the semantic hi-erarchy refers to the hypernym and hy-ponym structure in WordNet.
From Fig-ure 1, this rule may suggest several inequal-ities like: similarity(Mallet,Hammer) >similarity(Mallet,Tool).In addition, we may generate many such se-mantically ranking similarity inequalities by hu-man annotation through crowdsourcing.3 Semantic Word EmbeddingIn this section, we first briefly review the conven-tional skip-gram model (Mikolov et al, 2013b).Next, we study how to incorporate the ordinal sim-ilarity inequalities to learn semantic word embed-dings.3.1 The skip-gram modelThe skip-gram model is a recently proposed learn-ing framework (Mikolov et al, 2013b; Mikolov etal., 2013a) to learn continuous word vectors fromtext corpora based on the aforementioned distribu-tional hypothesis, where each word in vocabulary(size of V ) is mapped to a continuous embeddingspace by looking up an embedding matrix W(1).And W(1)is learned by maximizing the predic-tion probability, calculated by another predictionmatrix W(2), of its neighbouring words within acontext window.Given a sequence of training data, denoted asw1, w2, w3, ..., wTwith T words, the skip-grammodel aims to maximize he following objectivefunction:Q =1TT?t=1?
?c?j?c,j 6=0log p(wt+j|wt) (3)where c is the size of context windows, wtde-notes the input central word andwt+jfor its neigh-bouring word.
The skip-gram model computesthe above conditional probability p(wt+j|wt) us-ing the following softmax function:p(wt+j|wt) =exp(w(2)t+j?w(1)t)?Vk=1exp(w(2)k?w(1)t)(4)where w(1)tand w(2)kdenotes row vectors in ma-trices W(1)and W(2), corresponding to word wtand wkrespectively.The training process of the skip-gram modelcan be formula ed as an optimization problem tomaximize the above objective function Q.
As in(Mikolov et al, 2013b), this optimization problemis solved by the stochastic gradient descent (SGD)method and the learned embedding matrix W(1)is used as the word embeddings for all words invocabulary.3.2 Semantic Word Embedding (SWE) asConstrained OptimizationHere we consider how to combine the ordinalknowledge representation in section 2 and theskip-gram model in 3.1 to learn semantic wordembeddings (SWE).As shown in section 2, each ranking inequal-ity involves a triplet, (i, j, k), of three words,{wi, wj, wk}.
Assume the ordinal knowledge isrepresented by a large number of such inequalities,denoted as the inequality set S. For ?
(i, j, k) ?
S,we have:similarity(wi, wj) > similarity(wi, wk)?
sim(w(1)i,w(1)j) > sim(w(1)i,w(1)k).For notational simplicity, we denote sij=sim(w(1)i,w(1)j) hereafter.Next, we propose to use the following con-strained optimization problem to learn semanticword embeddings (SWE):{W(1),W(2)} = arg maxW(1),W(2)Q(W(1),W(2))(5)1503...wk-N wk-N+1 wk+N-1 wk+NwkSkip-gramwi wjd(k,i) > d(k,j)wp wqd(k,p) > d(k,q)......Semantic Ordinal Constraintswt-c wt-c+1 wt+c-1 wt+cwtMachine: Hi, please give a similarity value to those two words.Human: Uh, it is difficult to do it, just give me a little time.Paradigm shift?> ranking idea.Machine: Hi, here are two word pairs, would you please give asimilarity ranking decision for them?Human: Yes, here you are!W(1)W(2)wt-c wt-c+1 wt+c-1 wt+cwtW(1)W(2)sim(wt,wi) > sim(wj,wk)wi wj wkShare EmbeddingSemantic Knowledge ResourcesW(1)WordNetHuman LabelingKnowledgeHasKnowledge?MinimizeQsemSemanticKnowledgeInput WordYESMaximizeQbaseWord SequenceGet training samplesColorRed Blue GreenPupleToolHammer SawChiselMalletwt-c wt-c+1 wt+c-1 wt+cwtW(1)W(2)sim(wi,wj) > sim(wi,wk)wi wj wkShare EmbeddingSemantic Knowledge ResourcesW(1)WordNetHuman LabelingKnowledgeBasesD is tr ib u tio n a lH y p o th e s isK n o wl e d g e  C o n s tr a in tsJointOptimizationPlessor JigsawHacksawHypernymHyponyms Co-HyponymsFigure 2: The proposed semantic word embedding (SWE) learning framework (The left part denotes thestate-of-the-art skip-gram model; The right part represents the semantic constraints).subject tosij> sik?
(i, j, k) ?
S. (6)In this work, we formulate the above con-strained optimization problem into an uncon-strained one by casting all the constraints as apenalty term in the objective function.
The penaltyterm can be expressed as follows:D =?
(i,j,k)?Sf(i, j, k) (7)where the function f(?)
is a normalization func-tion.
It can be a sigmoid function like f(i, j, k) =?(sik?
sij) with ?
(x) = 1/(1 + exp(?x)).
Al-ternatively, it may be a hinge loss function likef(i, j, k) = h(sik?sij) where h(x) = max(?0, x)with ?0denoting a parameter to control the de-cision margin.
In this work, we adopt to usethe hinge function to compute the penalty term ineq.
(7) and ?0is set to be 0 for all experiments.Finally, the proposed semantic word embed-ding (SWE) model aims to maximize the follow-ing combined objective function:Q?= Q?
?
?
D (8)where ?
is a control parameter to balance the con-tribution of the penalty term in the optimizationprocess.
It balances between the semantic infor-mation estimated from the corpus based on thedistributional hypothesis and the semantic knowl-edge encoded in the ordinal ranking inequalities.In Rockt?aschel et al (2014), a similar approachwas proposed to capture knowledge constraint asextra terms in the objective function for optimiza-tion.In Figure 2, we show a diagram for the the over-all SWE learning framework to incorporate se-mantic knowledge into the basic skip-gram wordembeddings.
Comparing with the previous workin (Xu et al, 2014) and (Faruqui et al, 2014),the proposed SWE framework is more general interms of encoding the semantic knowledge forlearning word embeddings.
It is straightforwardto show that the work in (Xu et al, 2014; Zweig,2014; Faruqui et al, 2014) can be viewed as somespecial cases under our SWE learning framework.3.3 Optimization algorithm for SWEIn this work, the proposed semantic word em-beddings (SWE) are learned using the standardmini-batch stochastic gradient descent (SGD) al-gorithm.
Furthermore, we adopt to use the cosinedistance of the embedding vectors to compute thesimilarity between two words in the penalty term.In the following, we show how to compute thederivatives of the penalty term for the SWE learn-ing.?D?w(1)t=?
(i,j,k)?S?f (sik?
sij)?w(1)t=?(i,j,k)?Sf??(?ik(t)?sik?w(1)t?
?ij(t)?sij?w(1)t)(9)where ?ik(t) and ?ij(t) are computed as?ik(t) ={1 t = i or t = k0 otherwise(10)and for the hinge loss function f(x), we havef?={1 (sik?
sij) > ?00 (sik?
sij) ?
?0(11)and the derivatives of the cosine similarity mea-sure, sij=w(1)i?w(1)j|w(1)i||w(1)j|, with respect to a word vec-1504tor, i.e.,?sik?w(1)i, which can be derived as follows:?sij?w(1)i= ?sijw(1)i|w(1)i|2+w(1)j|w(1)i||w(1)j|.
(12)The learning rate used for the SWE learningis the same as that for the skip-gram model.
Ineach mini-batch of SGD, we sample terms in thesame way as the skip-gram model.
As for the con-straints, we do not sample them but use all inequal-ities relevant to any words in a minibatch to updatethe model for the minibatch.
Finally, by jointly op-timizing the two terms in the combined objectivefunction, we may learn a new set of word vectorsencoding with ordinal semantic knowledge.4 ExperimentsIn this section, we report all experiments con-ducted to evaluate the effectiveness of the pro-posed semantic word embeddings (SWE).
Herewe compare the performance of the proposedSWE model with the conventional skip-grambaseline model on four popular natural languageprocessing tasks, including word similarity mea-sure, sentence completion, name entity recogni-tion, and synonym selection.
In the following,we first describe the experimental setup, trainingcorpora, semantic knowledge databases.
Next,we report the experimental results on these fourNLP tasks.
Note that the SWE training codesand scripts are made publicly available at http://home.ustc.edu.cn/?quanliu/.4.1 Experimental setup4.1.1 Training corporaIn this work, we use the popular Wikipedia cor-pus as our training data to learn word embeddingsfor experiments on the word similarity task andthe TOEFL synonym selection task.
Particularly,we utilize two Wikipedia corpora with differentsizes.
The first corpus with a smaller size is adata set including the first one billion charactersfrom Wikipedia1, named as Wiki-Small in our ex-periments.
The second corpus with a relativelylarge size is the latest Wikipedia dump2, namedas Wiki-Large in our experiments.
Both Wikipediacorpora have been pre-processed by removing all1http://mattmahoney.net/dc/enwik9.zip2http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2the HTML meta-data and hyper-links and replac-ing the digit numbers with English words using theperl script from the Matt Mahoney?s page3.
Aftertext normalization, the Wiki-Small corpus containstotally 130 million words, for which we create alexicon of 225,909 distinct words appearing morethan 5 times in the corpus.
Similarly, the Wiki-Large corpus contains about 5 billion words, forwhich we create a lexicon of 228,069 words ap-pearing more than 200 times.For the other two tasks, sentence completionand name entity recognition, we use the sametraining corpora from the previous state-of-the-artwork for fair comparisons.
The training corpus forthe sentence completion is the Holmes text (Zweigand Burges, 2011; Mikolov et al, 2013a).
Thetraining corpus for the name entity recognitiontask is the Reuters English newswire from RCV1(Turian et al, 2010; Lewis et al, 2004).
Referto section 4.4 and section 4.5 for detailed descrip-tions respectively.4.1.2 Semantic constraint collectionsIn this work, we use WordNet as the resource tocollect ordinal semantic knowledge.
WordNet is alarge semantic lexicon database of English words(Miller, 1995), where nouns, verbs, adjectives andadverbs are grouped into sets of cognitive syn-onyms (usually called synsets).
Each synset usu-ally expresses a distinct semantic concept.
Allsynsets in WordNet are interlinked by means ofconceptual-semantic and/or lexical relations suchas synonyms and antonyms, hypernyms and hy-ponyms.In our experiments, we use the versionWordNet-3.1 for creating the corresponding se-mantic constraints.
In detail, we follow the fol-lowing process to extract semantic similarity in-equalities from WordNet and Thesaurus:1.
Based on the Synonym Antonym Rule de-scribed in section 2, for each word in vo-cabulary, find its synset and use the syn-onym and antonym relations to find all re-lated synonymous and antonymous synsets.Note that the antonymous synset is selectedas long as there exists an antonymous rela-tion between any word in this synset and anyword in an synonymous synset.
After find-ing the synonymous and antonymous synsets3http://mattmahoney.net/dc/textdata.html1505of the current word, the similarity inequali-ties could be generated according to the rank-ing rule.
After processing all words, wehave collected about 30,000 inequalities re-lated to the synonym and antonym relations.Furthermore, we extract additional 320,000inequalities from an old English dictionary(Fernald, 1896).
In total, we have about345,000 inequalities related to the synonymand antonym relations.
This set of inequali-ties is denoted as Synon-Anton constraints inour experiments.2.
Based on the Semantic Category Rule and Se-mantic Hierarchy Rule, we extract another in-equality set consisting of 75,000 inequalitiesfrom WordNet.
We defined this collection asHyper-Hypon constraints in our experiments.In the following experiments, we just use all ofthese collected inequality constraints as is withoutfurther manually checking or cleaning-up.
Theymay contain a very small percentage of errors orconflicts (due to multiple senses of a word).4.1.3 Training parameter settingHere we describe the control parameters used tolearn the baseline skip-gram model and the pro-posed SWE model.
In our experiments, we use theopen-source word2vec toolkit4to train the base-line skip-gram model, where the context windowsize is set to be 5.
The initial learning rate is setas 0.025 and the learning rate is decreased lin-early during the SGD model training process.
Weuse the popular negative sampling technique tospeed up model training and set the negative sam-ple number as 5.To train the proposed SWE model, we use thesame configuration as the skip-gram model tomaximize Q?.
For the penalty term in eq.
(7), weset ?0= 0 for the hinge loss function.
The seman-tic similarity between words is computed by thecosine distance.
The combination coefficient ?
ineq.
(8) is usually set to be a number between 0.001and 0.3 in our experiments.In the following four NLP tasks, the dimension-ality of embedding vectors is different since wetry to use the same settings from the state-of-the-art work for the comparison purpose.
In the WordSimilarity task and the TOEFL Synonym Selec-tion task, we followed the state of the art work4https://code.google.com/p/word2vec.505560657075808590951000.00 0.05 0.10 0.15 0.20 0.25 0.30Satisfied Rate(%)beta (?
)Semantic Inequality Satisfied Rate CurveHyper-HyponSynon-AntonFigure 3: A curve of inequality satisfied rates (Allmodels trained on the Wiki-Small corpus.
Hyper-Hypon and Synon-Anton stand for different seman-tic constraint sets employed for training semanticword embeddings).in (Xu et al, 2014), to set word embeddings to300-dimension.
Similarly, we refer to Bian et al(2014) to set the dimensionality of word vectors to600 for the Sentence Completion task.
And we setthe dimensionality of word vectors to 50 for theNER task according to (Turian et al, 2010; Pen-nington et al, 2014).4.2 Semantic inequality satisfied ratesHere we first examine the inequality satisfied ratesof various word embeddings.
The inequality sat-isfied rate is defined as how many percentage ofsemantic inequalities are satisfied based on the un-derlying word embedding vectors.
In Figure 3,we show a typical curve of the inequality satis-fied rates as a function of ?
used in model train-ing.
This figure is plotted based on the Wiki-Smallcorpus.
Two semantic constraint sets Synon-Antonand Hyper-Hypon created in section 4.1.2 are em-ployed to learn semantic word embeddings.In the framework of the proposed semanticword embedding method, we just need to tuneone more parameter ?, comparing with the skip-gram model.
It shows that the baseline skip-gram(?
= 0) can only satisfy about 50-60% of inequal-ities in the training set.
As we choose a propervalue for ?, we may significantly improve the in-equality satisfied rate, up to 85-95%.
Althoughwe can get higher inequality satisfying rate on thetraining set by increasing beta continuously, how-ever, we do not suggest to use a big beta valuebecause it would make the model overfitting.
Themajor reason for this is that the constraints only1506cover a subset of words in vocabulary.
Increasingthe rate too much may screw up the entire wordembeddings due to the sparsity of the constraints.Meanwhile, we have found that the proposedSGD method is very efficient to handle a largenumber of inequalities in model training.
Whenwe use the total 345,000 inequalities, the SWEtraining is comparable with the baseline skip-grammodel in terms of training speed.
In the following,we continue to examine the SWE model on fourpopular natural language processing tasks, includ-ing word similarity, sentence completion, nameentity recognition and the TOEFL synonym selec-tion.4.3 Task 1: Word Similarity Task4.3.1 Task descriptionMeasuring word similarity is a traditional NLPtask (Rubenstein and Goodenough, 1965).
Herewe compare several word embedding models ona popular word similarity task, namely WordSim-353 (Finkelstein et al, 2001), which contains 353English word pairs along with human-assignedsimilarity scores, which measure the relatednessof each word pair on a scale from 0 (totally unre-lated words) to 10 (very much related or identicalwords).
The final similarity score for each pair isthe average across 13 to 16 human judges.
Whenevaluating word embeddings on this task, we mea-sure the performance by calculating the Spearmanrank correlation between the human judgmentsand the similarity scores computed based on thelearned word embeddings.4.3.2 Experimental resultsHere we compare the proposed SWE model withthe baseline skip-gram model on the WordSim-353 task.
Both word embedding models aretrained using the Wikipedia corpora.
We set the di-mension of word embedding vectors to be 300.
InTable 1, we have shown all the Spearman rank cor-relation results.
The baseline results on this taskinclude PPMI (Levy and Goldberg, 2014), GloVe(Pennington et al, 2014), and ESA-Wikipedia(Gabrilovich and Markovitch, 2007).From the results in Table 1, we can see that theproposed SWE model can achieve consistent im-provements over the baseline skip-gram model, nomatter which training corpus is used.
These re-sults have demonstrated that, by incorporating se-mantic ordinal knowledge into the word vectors,Word Embeddings ResultOthersSPPMI 0.6870GloVe (6 billion) 0.6580GloVe (42 billion) 0.7590ESA-Wikipedia 0.7500Skip-gram 0.6326Wiki-Small SWE + Synon-Anton 0.6584(0.13 billion) SWE + Hyper-Hypon 0.6407SWE + Both 0.6442Skip-gram 0.7085Wiki-Large SWE + Synon-Anton 0.7274(5 billion) SWE + Hyper-Hypon 0.7213SWE + Both 0.7236Table 1: Spearman results on the WordSim-353Task.the proposed semantic word embedding frame-work can capture much better semantics for manywords.
The SWE model using the Wiki-Largecorpus has achieved the state-of-the-art perfor-mance on this task, significantly outperformingother popular word embedding methods, such asskip-gram and GloVe.
Moreover, we also find thatthe Synon-Anton constraint set is more relevantthan Hyper-Hypon for the word similarity task.4.4 Task 2: Sentence Completion Task4.4.1 Task descriptionThe Microsoft sentence completion challenge hasrecently been introduced as a standard benchmarktask for language modeling and other NLP tech-niques (Zweig and Burges, 2011).
This task con-sists of 1040 sentences, each of which misses oneword.
The goal is to select a word that is themost coherent with the rest of the sentence, froma list of five candidates.
Many NLP techniqueshave already been reported on this task, includ-ing N-gram model and LSA-based model pro-posed in (Zweig and Burges, 2011), log-bilinearmodel (Mnih and Teh, 2012), recurrent neuralnetworks (RNN) (Mikolov, 2012), the skip-grammodel (Mikolov et al, 2013a), a combination ofthe skip-gram and RNN model, and a knowledgeenhanced word embedding model proposed byBian et.
al.
(2014).
The performance of all thesetechniques is listed in Table 2 for comparison.In this work, we follow the the same proce-dure as in (Mikolov et al, 2013a) to examine theperformance of our proposed semantic word em-beddings (SWE) on this task.
We first train 600-1507dimension word embeddings based on a trainingcorpus of 50M words provided by (Zweig andBurges, 2011), with and without using the col-lected ordinal knowledge.
Then, for each sen-tence in the test set, we use the learned word em-beddings to compute a sentence score for predict-ing all surrounding words based on each candidateword in the list.
Finally, we use the computed sen-tence prediction scores to choose the most likelyword from the given list to answer the question.System AccOthersN-gram model 39.0LSA-based model 49.0Log-bilinear model 54.8RNN 55.4Skip-gram 48.0Skip-gram + RNN 58.9Bian et alSkip-gram 41.2+ Syntactic knowledge 41.9+ Semantic knowledge 45.2+ Both knowledge 44.21 IterationSkip-gram 44.1SWE + Synon-Anton 47.9SWE + Hyper-Hypon 47.5SWE + Both 48.35 IterationsSkip-gram 51.5SWE + Synon-Anton 55.7SWE + Hyper-Hypon 55.4SWE + Both 56.2Table 2: Results on Sentence Completion Task.4.4.2 Experimental resultsIn Table 2, we have shown the sentence comple-tion accuracy on this task for various word em-bedding models.
We can see that the proposedSWE model has achieved considerable improve-ments over the baseline skip-gram model.
Onceagain, this suggests that the semantic knowledgerepresented by the ordinal inequalities can signif-icantly improve the quality of the word embed-dings.
Besides, the SWE model significantly out-performs the recent work in (Bian et al, 2014),which considers syntactics and semantics of thesentence contexts.4.5 Task 3: Name Entity Recognition4.5.1 Task descriptionTo further investigate the performance of seman-tic word embeddings, we have further conductedsome experiments on the standard CoNLL03name entity recognition (NER) task.
TheCoNLL03 NER dataset is drawn from the Reutersnewswire.
The training set contains 204K words(14K sentences, 946 documents), the test setcontains 46K words (3.5K sentences, 231 doc-uments), and the development set contains 51Kwords (3.3K sentences, 216 documents).
We havelisted the state-of-the-art performance in Table 3for this task (Turian et al, 2010).To make a fair comparison, we have used theexactly same experimental configurations as in(Turian et al, 2010), including the used trainingalgorithm, the baseline discrete features and so on.Like the C&W model, we use the same trainingtext resource to learn word vectors, which containsone year of Reuters English newswire from RCV1,from August 1996 to August 1997, having about810,000 news stories (Lewis et al, 2004).
Mean-while, the dimension of word embeddings is set to50 for all experiments on this task.4.5.2 Experimental resultsIn our experiments, we compare the proposedSWE model with the baseline skip-gram model forname entity recognition, measured by the standardF1 scores.
We present the final NER F1 scoreson the CoNLL03 NER task in Table 3.
The nota-tion ?Gaz?
stands for gazetteers that are added intothe NER system as an auxiliary feature.
For theSWE model, we experiment two configurations byadding gazetteers or not (denoted by ?IsGaz?
and?NoGaz?
respectively).System Dev Test MUC7OthersC&W 92.3 87.9 75.7C&W + Gaz 93.0 88.9 81.4NoGazSkip-gram 92.6 88.3 76.7+ Synon-Anton 92.5 88.4 77.2+ Hyper-Hypon 92.6 88.6 77.7+ Both 92.6 88.4 77.5IsGazSkip-gram 93.3 89.5 80.0+ Synon-Anton 93.1 89.6 80.7+ Hyper-Hypon 93.1 89.7 80.7+ Both 93.0 89.5 80.8Table 3: F1 scores on the CoNLL03 NER task.From the results shown in Table 3, we could findthe proposed semantic word embedding (SWE)model can consistently achieve 0.8% (or more) ab-solute improvements on the MUC7 task no mat-1508ter whether the gazetteers features are used or not.The proposed SWE model can also obtain 0.3%improvement in the CoNLL03 test set when nogazetteers is added into the NER system.
How-ever, no significant improvement is observed inthis test set for the proposed SWE model after weadd the gazetteers feature.4.6 Task 4: TOEFL Synonym Selection4.6.1 Task descriptionThe goal of a synonym selection task is to se-lect, from a list of candidate words, the semanti-cally closest word for each given target word.
Thedataset we use for this task is the standard TOEFLdataset (Landauer and Dumais, 1997), which con-tains 80 questions.
Each question consists of a tar-get word along with 4 candidate lexical substitutesfor selection.The evaluation criterion on this task is thesynonym selection accuracy which indicates howmany synonyms are correctly selected for all 80questions.
Similar to the configurations on theword similarity task, all the experiments on thistask are conducted on the English Wikipedia cor-pora.
In our experiments, we set al the vector di-mensions to 300.4.6.2 Experimental ResultsCorpus Model Accuracy (%)Wiki-SmallSkip-gram 61.25+ Synon-Anton 70.00+ Hyper-Hypon 66.25+ Both 71.25Wiki-LargeSkip-gram 83.75+ Synon-Anton 87.50+ Hyper-Hypon 85.00+ Both 88.75Table 4: The TOEFL synonym selection task.In Table 4, we have shown the experimen-tal results for different word embedding models,learned from different Wikipedia corpora: Wiki-Small or Wiki-Large.
We compare the proposedSWE with the baseline skip-gram model.
Fromthe experimental results in Table 4, we can see thatthe proposed SWE model can achieve consistentimprovements over the baseline skip-gram modelon the TOEFL synonym selection task, about 5-8% improvements on the selection accuracy.
Wefind the similar performance differences betweenthe SWE model trained with the Synon-Anton andHyper-Hypon constraint set.
The main reasonwould be that the synonym selection task is mainlyrelated to lexical level similarity and less relevantto the hypernym-hyponym relations.5 Conclusions and Future WorkWord embedding models with good semantic rep-resentations are quite invaluable to many natu-ral language processing tasks.
However, the cur-rent data-driven methods that learn word vectorsfrom corpora based on the distributional hypoth-esis tend to suffer from some major limitations.In this paper, we propose a general and flexibleframework to incorporate various types of seman-tic knowledge into the popular data-driven learn-ing procedure for word embeddings.
Our maincontributions are to represent semantic knowledgeas a number of ordinal similarity inequalities aswell as to formulate the entire learning process asa constrained optimization problem.
Meanwhile,the optimization problem could be solved by effi-cient stochastic gradient descend algorithm.
Ex-perimental results on four popular NLP tasks haveall demonstrated that the propose semantic wordembedding framework can significantly improvethe quality of word representations.As for the future work, we would incorpo-rate more types of knowledge, such as knowledgegraphs and FrameNet, into the learning processfor more powerful word representations.
We alsoexpect that some common sense related semanticknowledge may be generated as ordinal inequalityconstraints by human annotators for learning se-mantic word embeddings.
At the end, we plan toapply the SWE word embedding models for morenatural language processing tasks.AcknowledgmentsThis work was supported in part by the Scienceand Technology Development of Anhui Province,China (Grants No.
2014z02006) and the Funda-mental Research Funds for the Central Universi-ties.
Special thanks to Zhigang Chen, Ruiji Fu andthe anonymous reviewers for their insightful com-ments as well as suggestions.
The authors alsowant to thank Profs.
Wu Guo and Li-Rong Daifor their wonderful help and supports during theexperiments.1509ReferencesYoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Jauvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Re-search, 3:1137?1155.Jiang Bian, Bin Gao, and Tie-Yan Liu.
2014.Knowledge-powered deep learning for word embed-ding.
In Machine Learning and Knowledge Discov-ery in Databases, pages 132?148.
Springer.Zhigang Chen, Wei Lin, Qian Chen, Xiaoping Chen,Si Wei, Xiaodan Zhu, and Hui Jiang.
2015.
Revis-iting word embedding for contrasting meaning.
InProceedings of ACL.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: Deepneural networks with multitask learning.
In Pro-ceedings of ICML, pages 160?167.
ACM.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
Journal of Machine Learning Research,12:2493?2537.Jacob Devlin, Rabih Zbib, Zhongqiang Huang, ThomasLamar, Richard Schwartz, and John Makhoul.
2014.Fast and robust neural network joint models for sta-tistical machine translation.
In Proceedings of ACL,pages 1370?1380.Manaal Faruqui, Jesse Dodge, Sujay K Jauhar, ChrisDyer, Eduard Hovy, and Noah A Smith.
2014.Retrofitting word vectors to semantic lexicons.
InProceedings of the NIPS Deep learning and repre-sentation learning workshop.James Champlin Fernald.
1896.
English synonyms andantonyms.
Funk & Wagnalls Company.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-tan Ruppin.
2001.
Placing search in context: Theconcept revisited.
In Proceedings of WWW, pages406?414.
ACM.Ronald A Fisher.
1936.
The use of multiple measure-ments in taxonomic problems.
Annals of eugenics,7(2):179?188.Evgeniy Gabrilovich and Shaul Markovitch.
2007.Computing semantic relatedness using wikipedia-based explicit semantic analysis.
In Proceedings ofIJCAI, volume 7, pages 1606?1611.Zellig S Harris.
1954.
Distributional structure.
Word,10(23):146?162.Geoffrey E Hinton, James L McClelland, and David ERumelhart.
1986.
Distributed representations.
InParallel distributed processing: Explorations in themicrostructure of cognition.
Volume 1: Foundations,pages 77?109.
MIT Press.Dan Jurafsky and James H Martin.
2000.
Speech &language processing.
Pearson Education India.Thomas K Landauer and Susan T Dumais.
1997.
Asolution to plato?s problem: The latent semanticanalysis theory of acquisition, induction, and rep-resentation of knowledge.
Psychological review,104(2):211.Claudia Leacock and Martin Chodorow.
1998.
Com-bining local context and wordnet similarity for wordsense identification.
WordNet: An electronic lexicaldatabase, 49(2):265?283.Omer Levy and Yoav Goldberg.
2014.
Neural wordembedding as implicit matrix factorization.
In Pro-ceedings of NIPS, pages 2177?2185.David D Lewis, Yiming Yang, Tony G Rose, and FanLi.
2004.
RCV1: A new benchmark collectionfor text categorization research.
Journal of MachineLearning Research, 5:361?397.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013a.
Efficient estimation of word represen-tations in vector space.
In Proceedings of Workshopat ICLR.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013b.
Distributed representa-tions of words and phrases and their compositional-ity.
In Proceedings of NIPS, pages 3111?3119.Tom?a?s Mikolov.
2012.
Statistical language modelsbased on neural networks.
Ph.D. thesis, Ph.
D. the-sis, Brno University of Technology.George A Miller and Walter G Charles.
1991.
Contex-tual correlates of semantic similarity.
Language andcognitive processes, 6(1):1?28.George A Miller.
1995.
Wordnet: a lexicaldatabase for english.
Communications of the ACM,38(11):39?41.Andriy Mnih and Yee Whye Teh.
2012.
A fast andsimple algorithm for training neural probabilisticlanguage models.
In Proceedings of ICML.Jeffrey Pennington, Richard Socher, and Christopher DManning.
2014.
Glove: Global vectors for wordrepresentation.
Proceedings of EMNLP, 12:1532?1543.Tim Rockt?aschel, Matko Bo?snjak, Sameer Singh, andSebastian Riedel.
2014.
Low-dimensional embed-dings of logic.
In Proceedings of the ACL 2014Workshop on Semantic Parsing, pages 45?49, Bal-timore, MD, June.
Association for ComputationalLinguistics.Herbert Rubenstein and John B Goodenough.
1965.Contextual correlates of synonymy.
Communica-tions of the ACM, 8(10):627?633.1510Holger Schwenk.
2007.
Continuous space languagemodels.
Computer Speech & Language, 21(3):492?518.Ilya Sutskever, Oriol Vinyals, and Quoc VV Le.
2014.Sequence to sequence learning with neural net-works.
In Proceedings of NIPS, pages 3104?3112.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general methodfor semi-supervised learning.
In Proceedings ofACL, pages 384?394.
Association for Computa-tional Linguistics.Peter D Turney, Patrick Pantel, et al 2010.
Fromfrequency to meaning: Vector space models of se-mantics.
Journal of artificial intelligence research,37(1):141?188.Chang Xu, Yalong Bai, Jiang Bian, Bin Gao, GangWang, Xiaoguang Liu, and Tie-Yan Liu.
2014.
Rc-net: A general framework for incorporating knowl-edge into word representations.
In Proceedings ofCIKM, pages 1219?1228.
ACM.Mo Yu and Mark Dredze.
2014.
Improving lexical em-beddings with semantic knowledge.
In Proceedingsof ACL, volume 2, pages 545?550.Geoffrey Zweig and Christopher JC Burges.
2011.
Themicrosoft research sentence completion challenge.Technical report, Technical Report MSR-TR-2011-129, Microsoft.Geoffrey Zweig.
2014.
Explicit representation ofantonymy in language modelling.
Technical report,Technical Report MSR-TR-2014-52, Microsoft.1511
