Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1339?1344,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsResponse-based Learning for Machine Translationof Open-domain Database QueriesCarolin HaasComputational LinguisticsHeidelberg University69120 Heidelberg, Germanyhaas1@cl.uni-heidelberg.deStefan RiezlerComputational LinguisticsHeidelberg University69120 Heidelberg, Germanyriezler@cl.uni-heidelberg.deAbstractResponse-based learning allows to adapt a sta-tistical machine translation (SMT) system toan extrinsic task by extracting supervision sig-nals from task-specific feedback.
In this pa-per, we elicit response signals for SMT adap-tation by executing semantic parses of trans-lated queries against the Freebase database.The challenge of our work lies in scaling se-mantic parsers to the lexical diversity of open-domain databases.
We find that parser perfor-mance on incorrect English sentences, whichis standardly ignored in parser evaluation, iskey in model selection.
In our experiments,the biggest improvements in F1-score for re-turning the correct answer from a semanticparse for a translated query are achieved byselecting a parser that is carefully enhanced byparaphrases and synonyms.1 IntroductionIn response-based learning for SMT, supervisionsignals are extracted from an extrinsic response toa machine translation, in contrast to using human-generated reference translations for supervision.
Weapply this framework to a scenario in which a se-mantic parse of a translated database query is exe-cuted against the Freebase database.
We view learn-ing from such task-specific feedback as adaptationof SMT parameters to the task of translating open-domain database queries, thereby grounding SMT inthe task of multilingual database access.
The successcriterion for this task is F1-score in returning the cor-rect answer from a semantic parse of the translatedquery, rather than BLEU.
Since the semantic parserprovides feedback to the response-based learner anddefines the final evaluation criterion, the challengeof the presented work lies in scaling the seman-tic parser to the lexical diversity of open-domaindatabases such as Freebase.
Riezler et al (2014)showed how to use response-based learning to adaptan SMT system to a semantic parser for the Geo-query domain.
The state-of-the-art in semantic pars-ing on Geoquery achieves a parsing accuracy of over82% (see Andreas et al (2013) for an overview),while the state-of-the-art in semantic parsing on theFree917 data (Cai and Yates, 2013) achieves 68.5%accuracy (Berant and Liang, 2014).
This is due tothe lexical variability of Free917 (2,036 word types)compared to Geoquery (279 word types).In this paper, we compare different ways of scal-ing up state-of-the-art semantic parsers for Freebaseby adding synonyms and paraphrases.
First, we con-sider Berant and Liang (2014)?s own extension ofthe semantic parser of Berant et al (2013) by us-ing paraphrases.
Second, we apply WordNet syn-onyms (Miller, 1995) for selected parts of speech tothe queries in the Free917 dataset.
The new pairs ofqueries and logical forms are added to the dataseton which the semantic parsers are retrained.
Wefind that both techniques of enhancing the lexicalcoverage of the semantic parsers result in improvedparsing performance, and that the improvements addup nicely.
However, improved parsing performancedoes not correspond to improved F1-score in an-swer retrieval when using the respective parser in aresponse-based learning framework.
We show thatin order to produce helpful feedback for response-based learning, parser performance on incorrect En-1339glish queries needs to be taken into account, which isstandardly ignored in parser evaluation.
That is, forthe purpose of parsing translated queries, a parsershould retrieve correct answers for correct Englishqueries (true positives), and must not retrieve cor-rect answers for incorrect translations (false posi-tives).
In order to measure false discovery rate, weprepare a test set of manually verified incorrect En-glish in addition to a standard test set of original En-glish queries.
We show that if false discovery rateon incorrect English queries is taken into account inmodel selection, the semantic parser that yields bestresults for response-based learning in SMT can befound reliably.2 Related WorkOur work is most closely related to Riezler et al(2014).
We extend their application of response-based learning for SMT to a larger and lexicallymore diverse dataset and show how to performmodel selection in the environment from which re-sponse signals are obtained.
In contrast to theirwork where a monolingual SMT-based approach(Andreas et al, 2013) is used as semantic parser, ourwork builds on existing parsers for Freebase, with afocus on exploiting paraphrasing and synonym ex-tension for scaling semantic parsers to open-domaindatabase queries.Response-based learning has been applied in pre-vious work to semantic parsing itself (Kwiatowskiet al (2013), Berant et al (2013), Goldwasser andRoth (2013), inter alia).
In these works, extrinsic re-sponses in form of correct answers from a databaseare used to alleviate the problem of manual data an-notation in semantic parsing.
Saluja et al (2012) in-tegrate human binary feedback on the quality of anSMT system output into a discriminative learner.Further work on learning from weak supervisionsignals has been presented in the machine learningcommunity, e.g., in form of coactive learning (Shiv-aswamy and Joachims, 2012), reinforcement learn-ing (Sutton and Barto, 1998), or online learning withlimited feedback (Cesa-Bianchi and Lugosi, 2006).3 Response-based Online SMT LearningWe denote by ?
(x, y) a joint feature representa-tion of input sentences x and output translationsAlgorithm 1 Response-based Online Learningrepeatfor i = 1, .
.
.
, n doReceive input string x(i)Predict translation y?Receive task feedback e(y?)
?
{1, 0}if e(y?)
= 1 theny+?
y?Store y?
as reference y(i)for x(i)Compute y?elsey??
y?Receive reference y(i)Compute y+end ifw ?
w + ?(?
(x(i), y+)?
?
(x(i), y?
))end foruntil Convergencey ?
Y (x), and by s(x, y;w) = ?w, ?
(x, y)?
a lin-ear scoring function for predicting a translation y?.A response signal is denoted by a binary functione(y) ?
{1, 0} that executes a semantic parse againstthe database and checks whether it receives the sameanswer as the gold standard parse.
Furthermore, acost function c(y(i), y) = (1?BLEU(y(i), y)) basedon sentence-wise BLEU (Nakov et al, 2012) is used.Algorithm 1, called ?Response-based Online Learn-ing?
in Riezler et al (2014), is based on contrast-ing a ?positive?
translation y+that receives positivefeedback, has a high model score, and a low cost ofpredicting y instead of y(i), with a ?negative?
trans-lation y?that leads to negative feedback, has a highmodel score, and a high cost:y+= argmaxy?Y (x(i)):e(y)=1(s(x(i), y;w)?
c(y(i), y)),y?= argmaxy?Y (x(i)):e(y)=0(s(x(i), y;w) + c(y(i), y)).The central algorithm operates as follows: The SMTsystem predicts translation y?, and in case of positivetask feedback, the prediction is accepted and storedas positive example by setting y+?
y?.
In thatcase, y?needs to be computed in order to performthe stochastic gradient descent update of the weight1340vector.
If the feedback is negative, the prediction istreated as y?and y+needs to be computed for theupdate.
If either y+or y?cannot be computed, theexample is skipped.4 Scaling Semantic Parsing toOpen-domain Database QueriesThe main challenge of grounding SMT in seman-tic parsing for Freebase lies in scaling the seman-tic parser to the lexical diversity of the open-domaindatabase.
Our baseline system is the parser of Berantet al (2013), called SEMPRE.
We first consider theapproach presented by Berant and Liang (2014) toscale the baseline to open-domain database queries:In their system, called PARASEMPRE, pairs of logi-cal forms and utterances are generated from a givenquery and the database, and the pair whose utterancebest paraphrases the input query is selected.
Thesenew pairs of queries and logical forms are added asambiguous labels in training a model from query-answer pairs.Following a similar idea of extending parser cov-erage by paraphrases, we extend the training set withsynonyms from WordNet.
This is done by iterat-ing over the queries in the FREE917 dataset.
Toensure that the replacement is sensible, each sen-tence is first POS tagged (Toutanova et al, 2003) andWordNet lookups are restricted to matching POS be-tween synonym and query words, for nouns, verbs,adjectives and adverbs.
Lastly, in order to limitthe number of retrieved words, a WordNet lookupis performed by carefully choosing from the firstthree synsets which are ordered from most commonto least frequently used sense.
Within a synset alwords are taken.
The new training queries are ap-pended to the training portion of FREE917.5 Model SelectionThe most straightforward strategy to perform modelselection for the task of response-based learning forSMT is to rely on parsing evaluation scores that arestandardly reported in the literature.
However, aswe will show experimentally, if precision is taken asthe percentage of correct answers out of instancesfor which a parse could be produced, recall as thepercentage of total examples for which a correct an-swer could be found, and F1 score as their harmonicmean, the metrics are not appropriate for model se-lection in our case.
This is because for our goalof learning the language of correct English databasequeries from positive and negative parsing feedback,the semantic parser needs to be able to parse and re-trieve correct answers for correct database queries,but it must not do so for incorrect queries.However, information about incorrect queries isignored in the definition of the metrics given above.In fact, retrieving correct answers for incorrectdatabase queries hurts response-based learning forSMT.
The problem lies in the incomplete nature ofsemantic parsing databases, where terms that arenot parsed into logical forms in one context makea crucial difference in another context.
For exam-ple in Geoquery, the gold standard queries ?Peo-ple in Boulder??
and ?Number of people in Boul-der??
parse into the same logical form, however,the queries ?Give me the cities in Virginia?
and?Give me the number of cities in Virginia?
have dif-ferent parses and different answers.
While in thefirst case, for example in German-to-English transla-tion of database queries, the German ?Anzahl?
maybe translated incorrectly without consequences, it iscrucial to translate the term into ?number?
in thesecond case.
On an example from Free917, theSMT system translates the German ?Steinformatio-nen?
into ?kind of stone?, which is incorrect in thegeological context, where it should be ?rock forma-tions?.
If during response-based learning, the errorslips through because of an incomplete parse lead-ing to the correct answer, it might hurt on the testdata.
Negative parser feedback for incorrect transla-tions is thus crucial for learning how to avoid thesecases in response-based SMT.In order to evaluate parsing performance on in-correct translations, we need to extend standardevaluation data of correct English database querieswith evaluation data of incorrect English databasequeries.
For this purpose, we took translations ofan out-of-domain SMT system that were judged ei-ther grammatically or semantically incorrect by theauthors to create a dataset of negative examples.
Onthis dataset, we can define true positives (TP) as cor-rect English queries that were given a correct an-swer by the semantic parser, and false positives (FP)as wrong English queries that obtained the correctanswer.
The crucial evaluation metric is the false1341Model #data F1 FDRS 620 56.8 28.00P 620 66.54 25.22P1 3,982 65.38 24.89P2 6,740 66.92 26.38P3 8,465 66.15 25.97Table 1: Parsing F1 scores and False Discovery Rate(FDR) for SEMPRE (S), PARASEMPRE (P), and exten-sions of the latter with synonyms from first one (P1),first two (P2) and first three (P3) synsets, evaluated onthe FREE917 test set of correct database queries for F1and including the test set of incorrect database queries forFDR, and trained on #data training queries.
Best resultsare indicated in bold face.discovery rate (FDR) (Murphy, 2012), defined asFP/FP+TP, i.e., as the ratio of false positives outof all positive answer retrieval events.6 ExperimentsWe use a data dump of Freebase1which was hasbeen indexed by the Virtuoso SPARQL engine2asour knowledge base.
The corpus used in the ex-periments is the FREE917 corpus as assembled byCai and Yates (2013) and consists of 614 trainingand 276 test queries in English and correspondinglogical forms.3The dataset of negative examples,i.e., incorrect English database queries that shouldreceive incorrect answers, consists of 166 examplesthat were judged either grammatically or semanti-cally incorrect by the authors.The translation of the English queries inFREE917 into German, in order to provide a setof source sentences for SMT, was done by the au-thors.
The SMT framework used is CDEC (Dyeret al, 2010) with standard dense features and ad-ditional sparse features as described in Simianer etal.
(2012)4.
Training of the baseline SMT systemwas performed on the COMMON CRAWL5(Smith1http://www.freebase.com/2http://virtuoso.openlinksw.com/3Note that we filtered out 33 questions (21 from the trainingset and 12 from the test set) because their logical forms onlyreturned an empty string as an answer.4https://github.com/pks/cdec-dtrain5http://www.statmt.org/wmt13/training-parallel-commoncrawl.tgzet al, 2013) dataset consisting of 7.5M parallelEnglish-German segments extracted from the web.Response-based learning for SMT uses the code de-scribed in Riezler et al (2014)6.For semantic parsing we use the SEMPRE andPARASEMPRE tools of Berant et al (2013) and Be-rant and Liang (2014) which were trained on thetraining portion of the FREE917 corpus7.
Furthermodels use the training data enhanced with syn-onyms from WordNet as described in Section 4.
Fol-lowing Jones et al (2012), we evaluate semanticparsers according to precision, defined as the per-centage of correctly answered examples out of thosefor which a parse could be produced, recall, definedas the percentage of total examples answered cor-rectly, and F1-score, defined as harmonic mean ofprecision and recall.
Furthermore, we report falsediscovery rate (FDR) on the combined set of 276correct and 166 incorrect database queries.Table 1 reports standard parsing evaluationmetrics for the different parsers SEMPRE (S),PARASEMPRE (P), and extensions of the latter withsynonyms from the first one (P1), first two (P2) andfirst three (P3) synsets which are ordered accordingto frequency of use of the sense.
As shown in thesecond column, the size of the training data is in-creased up to 10 times by using various synonym ex-tensions.
As shown in the third column, PARASEM-PRE improves F1 by nearly 10 points over SEMPRE.Another 0.5 points are added by extending the train-ing data using two synsets.
The third column showsthat the system P1 that scored second-worst in termsof F1 score, scores best under the FDR metric8.Table 2 shows an evaluation of the use of differ-ent parsing models to retrieve correct answers fromthe FREE917 test set of correct database queries.The systems are applied to translated queries, butevaluated in terms of standard parsing metrics.
Sta-tistical significance is measured using an Approxi-mate Randomization test (Noreen, 1989; Riezler andMaxwell, 2005).
The baseline system is CDEC as de-scribed above.
It never sees the FREE917 data dur-ing training.
As a second baseline method we usea stochastic (sub)gradient descent variant of RAM-PION (Gimpel and Smith, 2012).
This system is6https://github.com/pks/rebol7www-nlp.stanford.edu/software/sempre8Note that in case of FDR, smaller is better.13421CDEC2RAMPION3REBOLS 40.0 40.36 42.9212P 42.92 44.59 45.85P1 42.92 46.36148.81P2 43.81 45.92 47.06P3 43.36 45.92 47.49Table 2: Parsing F1 score on FREE917 test set of trans-lated database queries using different parser models toprovide response for translated queries.
Best results areindicated in bold face.
Statistical significance of resultdifferences at p < 0.05 are indicated by algorithm num-ber in superscript.CDEC RAMPION REBOLF1 0.85 0.29 0.1FDR -0.21 -0.58 -0.7Table 3: Spearman correlation between F1 / FDR fromTable 1 and CDEC / RAMPION / REBOL F1 from Table 2.trained by using the correct English queries in theFREE917 training data as references.
Neither CDECnor RAMPION use parser feedback in training.
RE-BOL (Response-based Online Learning) is an im-plementation of Algorithm 1 described in Section 3.This algorithm makes use of positive parser feed-back to convert predicted translation into references,in addition to using the original English queries asreferences.
Training for both RAMPION and REBOLis performed for 10 epochs over the FREE917 train-ing set, using a constant learning rate ?
that waschosen via cross-validation.
All methods then pro-ceed to translate the FREE917 test set.
Best resultsin Table 2 are obtained by using an extension ofPARASEMPRE with one synset as parser in response-based learning with REBOL.
This parsing systemscored best under the FDR metric in Table 1.Table 3 shows the Spearman rank correlation(Siegel and Castellan, 1988) between the F1 / FDRranking of semantic parsers from Table 1 and theircontribution to F1 scores in Table 2 for parsingquery translations of CDEC, RAMPION or REBOL.The system CDEC cannot learn from parser perfor-mance based on query translations, thus best resultson translated queries correlate positively with goodparsing F1 score per se.
RAMPION can implicitlytake advantage of parsers with good FDR score sincelearning to move away from translations dissimilarto the reference is helpful if they do not lead tocorrect answers.
REBOL can make the best use ofparsers with low FDR score since it can learn to pre-vent incorrect translations from hurting parsing per-formance at test time.7 ConclusionWe presented an adaptation of SMT to translatingopen-domain database queries by using feedback ofa semantic parser to guide learning.
Our work high-lights an important aspect that is often overlooked inparser evaluation, namely that parser model selec-tion in real-world applications needs to take the pos-sibility of parsing incorrect language into account.We found that for our application of response-basedlearning for SMT, the key is to learn to preventcases where the correct answer is retrieved despitethe translation being incorrect.
This can be avoidedby performing model selection on semantic parsersthat parse and retrieve correct answers for correctdatabase queries, but do not do retrieve correct an-swers for incorrect queries.In our experiments, we found that the parser thatcontributes most to response-based learning in SMTis one that is carefully extended by paraphrases andsynonyms.
In future work, we would like to investi-gate additional techniques for paraphrasing and syn-onym extension.
For example, a good fit for our taskof response-based learning for SMT might be Ban-nard and Callison-Burch (2005)?s approach to para-phrasing via pivoting on SMT phrase tables.AcknowledgmentsThis research was supported in part by DFG grantRI-2221/2-1 ?Grounding Statistical Machine Trans-lation in Perception and Action?.ReferencesJacob Andreas, Andreas Vlachos, and Stephen Clark.2013.
Semantic parsing as machine translation.
InProceedings of the 51st Annual Meeting of the Associ-ation for Computational Linguistics (ACL?13), Sofia,Bulgaria.Colin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In Proceed-1343ings of the 43rd Annual Meeting of the Association forComputational Linguistics (ACL?05), Ann Arbor, MI.Jonathan Berant and Percy Liang.
2014.
Semantic pars-ing via paraphrasing.
In Proceedings of the 52nd An-nual Meeting of the Association for ComputationalLinguistics (ACL 2014), Baltimore, MD.Jonathan Berant, Andrew Chou, Roy Frostig, and PercyLiang.
2013.
Semantic parsing on freebase fromquestion-answer pairs.
In Proceedings of the 2013Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP?13), Seattle, WA.Qingqing Cai and Alexander Yates.
2013.
Large-scalesemantic parsing via schema matching and lexicon ex-tenstion.
In Proceedings of the 51st Annual Meet-ing of the Association for Computational Linguistics(ACL?13), Sofia, Bulgaria.Nicol`o Cesa-Bianchi and G`abor Lugosi.
2006.
Predic-tion, Learning, and Games.
Cambridge UniversityPress.Chris Dyer, Adam Lopez, Juri Ganitkevitch, JohnathanWeese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,Vladimir Eidelman, and Philip Resnik.
2010. cdec: Adecoder, alignment, and learning framework for finite-state and context-free translation models.
In Proceed-ings of the 48th Annual Meeting of the Association forComputational Linguistics (ACL?10), Uppsala, Swe-den.Kevin Gimpel and Noah A. Smith.
2012.
Structuredramp loss minimization for machine translation.
InProceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies(HLT-NAACL?12), Stroudsburg, PA.Dan Goldwasser and Dan Roth.
2013.
Learning fromnatural instructions.
Machine Learning, 94(2):205?232.Bevan K. Jones, Mark Johnson, and Sharon Goldwater.2012.
Semantic parsing with bayesion tree transduc-ers.
In Proceedings of the 50th Annual Meeting of theAssociation for Computational Linguistics (ACL?12),Jeju Island, Korea.Tom Kwiatowski, Eunsol Choi, Yoav Artzi, and LukeZettlemoyer.
2013.
Scaling semantic parsers withon-the-fly ontology matching.
In Proceedings of the2013 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP?13), Seattle, WA.George A. Miller.
1995.
Wordnet: A lexical database forenglish.
Communications of the ACM, 38(11):39?41.Kevin P. Murphy.
2012.
Machine Learning.
A Proba-bilistic Perspective.
The MIT Press.Preslav Nakov, Francisco Guzm?an, and Stephan Vogel.2012.
Optimizing for sentence-level bleu+1 yieldsshort translations.
In Proceedings of the 24th In-ternational Conference on Computational Linguistics(COLING 2012), Bombay, India.Eric W. Noreen.
1989.
Computer Intensive Methodsfor Testing Hypotheses: An Introduction.
Wiley, NewYork.Stefan Riezler and John Maxwell.
2005.
On some pit-falls in automatic evaluation and significance testingfor MT.
In Proceedings of the ACL-05 Workshop onIntrinsic and Extrinsic Evaluation Measures for MTand/or Summarization, Ann Arbor, MI.Stefan Riezler, Patrick Simianer, and Carolin Haas.
2014.Response-based learning for grounded machine trans-lation.
In Proceedings of the 52nd Annual Meetingof the Association for Computational Linguistics (ACL2014), Baltimore, MD.Avneesh Saluja, Ian Lane, and Ying Zhang.
2012.Machine translation with binary feedback: A large-margin approach.
In Proceedings of the 10th BiennialConference of the Association for Machine Translationin the Americas (AMTA?12), San Diego, CA.Pannaga Shivaswamy and Thorsten Joachims.
2012.
On-line structured prediction via coactive learning.
InProceedings of the 29th International Conference onMachine Learning (ICML?12), Scotland, UK.Sidney Siegel and John Castellan.
1988.
NonparametricStatistics for the Behavioral Sciences.
Second Edition.MacGraw-Hill, Boston, MA.Patrick Simianer, Stefan Riezler, and Chris Dyer.
2012.Joint feature selection in distributed stochastic learn-ing for large-scale discriminative training in SMT.
InProceedings of the 50th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL?12), JejuIsland, South Korea.Jason Smith, Herve Saint-Amand, Magdalena Plamada,Philipp Koehn, Chris Callison-Burch, and AdamLopez.
2013.
Dirt cheap web-scale parallel text fromthe Common Crawl.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Linguis-tics (ACL?13), Sofia, Bulgaria.Richard S. Sutton and Andrew G. Barto.
1998.
Rein-forcement Learning.
An Introduction.
The MIT Press.Kristina Toutanova, Dan Klein, Christopher Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In Pro-ceedings of the 2003 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies (HLT-NAACL?03).
Edmonton, Canada.1344
