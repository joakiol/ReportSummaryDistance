Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 94?103,Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational LinguisticsIncremental Grammar Induction from Child-DirectedDialogue Utterances?Arash EshghiInteraction LabHeriot-Watt UniversityEdinburgh, United Kingdomeshghi.a@gmail.comJulian Hough and Matthew PurverCognitive Science Research GroupQueen Mary University of LondonLondon, United Kingdom{julian.hough, mpurver}@eecs.qmul.ac.ukAbstractWe describe a method for learning an in-cremental semantic grammar from data inwhich utterances are paired with logicalforms representing their meaning.
Work-ing in an inherently incremental frame-work, Dynamic Syntax, we show howwords can be associated with probabilisticprocedures for the incremental projectionof meaning, providing a grammar whichcan be used directly in incremental prob-abilistic parsing and generation.
We testthis on child-directed utterances from theCHILDES corpus, and show that it resultsin good coverage and semantic accuracy,without requiring annotation at the wordlevel or any independent notion of syntax.1 IntroductionHuman language processing has long beenthought to function incrementally, both in pars-ing and production (Crocker et al 2000; Fer-reira, 1996).
This incrementality gives rise tomany characteristic phenomena in conversationaldialogue, including unfinished utterances, inter-ruptions and compound contributions constructedby more than one participant, which pose prob-lems for standard grammar formalisms (Howes etal., 2012).
In particular, examples such as (1) sug-gest that a suitable formalism would be one whichdefines grammaticality not in terms of licensingstrings, but in terms of constraints on the semanticconstruction process, and which ensures this pro-cess is common between parsing and generation.
(1) A: I burnt the toast.?
We are grateful to Ruth Kempson for her support andhelpful discussions throughout this work.
We also thankthe CMCL?2013 anonymous reviewers for their constructivecriticism.
This work was supported by the EPSRC, RISERproject (Ref: EP/J010383/1), and in part by the EU, FP7project, SpaceBook (Grant agreement no: 270019).B: But did you burn .
.
.A: Myself?
Fortunately not.
[where ?did you burn myself??
if uttered bythe same speaker is ungrammatical]One such formalism is Dynamic Syntax (DS)(Kempson et al 2001; Cann et al 2005); itrecognises no intermediate layer of syntax, butinstead reflects grammatical constraints via con-straints on the word-by-word incremental con-struction of meaning, underpinned by attendantconcepts of underspecification and update.Eshghi et al(2013) describe a method for in-ducing a probabilistic DS lexicon from sentencespaired with DS semantic trees (see below) repre-senting not only their meaning, but their function-argument structure with fine-grained typing infor-mation.
They apply their method only to an ar-tificial corpus generated using a known lexicon.Here, we build on that work to induce a lexi-con from real child-directed utterances paired withless structured Logical Forms in the form of TTRRecord Types (Cooper, 2005), thus providing lesssupervision.
By assuming only the availability of asmall set of general compositional semantic opera-tions, reflecting the properties of the lambda calcu-lus and the logic of finite trees, we ensure that thelexical entries learnt include the grammatical con-straints and corresponding compositional seman-tic structure of the language.
Our method exhibitsincrementality in two senses: incremental learn-ing, with the grammar being extended and refinedas each new sentence becomes available; resultingin an inherently incremental, probabilistic gram-mar for parsing and production, suitable for usein state-of-the-art incremental dialogue systems(Purver et al 2011) and for modelling human-human dialogue.94?Ty(t)?Ty(e),?
?Ty(e ?
t)???john?
?Ty(t)Ty(e),john?Ty(e ?
t),????upset?
?Ty(t)Ty(e),john ?Ty(e ?
t)?Ty(e),?Ty(e ?
(e ?
t)),?y?x.upset?(x)(y)???mary?Ty(t),?,upset?(john?)(mary?
)Ty(e),johnTy(e ?
t),?x.upset?(x)(mary?
)Ty(e),mary?Ty(e ?
(e ?
t)),?y?x.upset?
(x)(y)Figure 1: Incremental parsing in DS producing semantic trees: ?John upset Mary?2 Background2.1 Grammar Induction and SemanticsWe can view existing grammar induction meth-ods along a spectrum from supervised to unsu-pervised.
Fully supervised methods take a parsedcorpus as input, pairing sentences with syntactictrees and words with their syntactic categories, andgeneralise over the phrase structure rules to learna grammar which can be applied to a new set ofdata.
Probabilities for production rules sharing aLHS category can be estimated, producing a gram-mar suitable for probabilistic parsing and disam-biguation e.g.
a PCFG (Charniak, 1996).
Whilesuch methods have shown great success, they pre-suppose detailed prior linguistic information andare thus inadequate as human grammar learningmodels.
Fully unsupervised methods, on the otherhand, proceed from unannotated raw data; theyare thus closer to the human language acquisitionsetting, but have seen less success.
In its pureform ?positive data only, without bias?
unsu-pervised learning is computationally too complex(?unlearnable?)
in the worst case (Gold, 1967).Successful approaches involve some prior learningor bias (see (Clark and Lappin, 2011)) e.g.
a setof known lexical categories, a probability distri-bution bias (Klein and Manning, 2005) or a semi-supervised method with shallower (e.g.
POS-tag)annotation (Pereira and Schabes, 1992).Another point on the spectrum is lightly su-pervised learning: providing information whichconstrains learning but with little or no lexico-syntactic detail.
One possibility is the use of se-mantic annotation, using sentence-level proposi-tional Logical Forms (LF).
It seems more cogni-tively plausible, as the learner can be said to beable to understand, at least in part, the meaningof what she hears from evidence gathered from(1) her perception of her local, immediate environ-ment given appropriate biases on different patternsof individuation of entities and relationships be-tween them, and (2) helpful interaction, and jointfocus of attention with an adult (see e.g.
(Saxton,1997)).
Given this, the problem she is faced withis one of separating out the contribution of eachindividual linguistic token to the overall meaningof an uttered linguistic expression (i.e.
decompo-sition), while maintaining and generalising overseveral such hypotheses acquired through time asshe is exposed to more utterances involving eachtoken.This has been successfully applied in Combi-natorial Categorial Grammar (CCG) (Steedman,2000), as it tightly couples compositional seman-tics with syntax (Zettlemoyer and Collins, 2007;Kwiatkowski et al 2010; Kwiatkowski et al2012); as CCG is a lexicalist framework, grammarlearning involves inducing a lexicon assigning toeach word its syntactic and semantic contribution.Moreover, the grammar is learnt incrementally, inthe sense that the learner collects data over timeand does the learning sentence by sentence.Following this approach, Eshghi et al(2013)outline a method for inducing a DS grammarfrom semantic LFs.
This brings an added di-mension of incrementality: not only is learningsentence-by-sentence incremental, but the gram-mar learned is inherently word-by-word incre-mental (see section 2.2 below).
However, theirmethod requires a higher degree of supervisionthan (Kwiatkowski et al 2012): the LFs assumedare not simply flat semantic formulae, but full DSsemantic trees (see e.g.
Fig.
1) containing infor-mation about the function-argument structure re-95quired for their composition, in addition to finegrained type and formula annotations.
Further,they test their method only on artificial data cre-ated using a known, manually-specified DS gram-mar.
In contrast, in this paper we provide anapproach which can learn from LFs without anycompositional structure information, and test it onreal language data; thus providing the first prac-tical learning system for an explicitly incrementalgrammar that we are aware of.2.2 Dynamic Syntax (DS)Dynamic Syntax (Kempson et al 2001; Cann etal., 2005) is a parsing-directed grammar formal-ism, which models the word-by-word incrementalprocessing of linguistic input.
Unlike many otherformalisms, DS models the incremental buildingup of interpretations without presupposing or in-deed recognising an independent level of syntacticprocessing.
Thus, the output for any given stringof words is a purely semantic tree representingits predicate-argument structure; tree nodes cor-respond to terms in the lambda calculus, deco-rated with labels expressing their semantic type(e.g.
Ty(e)) and formula, with beta-reduction de-termining the type and formula at a mother nodefrom those at its daughters (Figure 1).These trees can be partial, containing unsatis-fied requirements for node labels (e.g.
?Ty(e) is arequirement for future development to Ty(e)), andcontain a pointer ?
labelling the node currentlyunder development.
Grammaticality is defined asparsability: the successful incremental construc-tion of a tree with no outstanding requirements (acomplete tree) using all information given by thewords in a sentence.
The complete sentential LFis then the formula decorating the root node ?
seeFigure 1.
Note that in these trees, leaf nodes donot necessarily correspond to words, and may notbe in linear sentence order; syntactic structure isnot explicitly represented, only the structure of se-mantic predicate-argument combination.2.2.1 Actions in DSThe parsing process is defined in terms of condi-tional actions: procedural specifications for mono-tonic tree growth.
These include general structure-building principles (computational actions), puta-tively independent of any particular natural lan-guage, and language-specific actions associatedwith particular lexical items (lexical actions).
Thelatter are what we learn from data here.Computational actions These form a small,fixed set, which we assume as given here.
Somemerely encode the properties of the lambda cal-culus and the logical tree formalism itself, LoFT(Blackburn and Meyer-Viol, 1994) ?
these weterm inferential actions.
Examples include THIN-NING (removal of satisfied requirements) andELIMINATION (beta-reduction of daughter nodesat the mother).
These actions are language-independent, cause no ambiguity, and add no newinformation to the tree; as such, they apply non-optionally whenever their preconditions are met.Other computational actions reflect the fun-damental predictivity and dynamics of the DSframework.
For example, *-ADJUNCTION in-troduces a single unfixed node with underspec-ified tree position (replacing feature-passing ortype-raising concepts for e.g.
long-distance depen-dency); and LINK-ADJUNCTION builds a paired(?linked?)
tree corresponding to semantic con-junction (licensing relative clauses, apposition andmore).
These actions represent possible parsingstrategies and can apply optionally whenever theirpreconditions are met.
While largely language-independent, some are specific to language type(e.g.
INTRODUCTION-PREDICTION in the formused here applies only to SVO languages).Lexical actions The lexicon associates wordswith lexical actions; like computational actions,these are sequences of tree-update actions in anIF..THEN..ELSE format, and composed of ex-plicitly procedural atomic tree-building actionssuch as make (creates a new daughter node),go (moves the pointer), and put (decorates thepointed node with a label).
Figure 2 shows an ex-ample for a proper noun, John.
The action checkswhether the pointed node (marked as ?)
has a re-quirement for type e; if so, it decorates it with typee (thus satisfying the requirement), formula John?and the bottom restriction ????
(meaning that thenode cannot have any daughters).
Otherwise theaction aborts, i.e.
the word ?John?
cannot be parsedin the context of the current tree.Graph-based Parsing & Generation These ac-tions define the parsing process.
Given a sequenceof words (w1, w2, ..., wn), the parser starts fromthe axiom tree T0 (a requirement to construct acomplete propositional tree, ?Ty(t)), and appliesthe corresponding lexical actions (a1, a2, .
.
.
, an),optionally interspersing computational actions.96Action Input tree Output treeJohnIF ?Ty(e)THEN put(Ty(e))put(Fo(John?)put(????
)ELSE ABORT?Ty(t)?Ty(e),?
?Ty(e ?
t)?John???
?Ty(t)Ty(e), ?Ty(e)John?, ????,?
?Ty(e ?
t)Figure 2: Lexical action for the word ?John?T0T1introT2predT3link-adjT4*-adjT5johnabortT6john?john?T7thinT8compT9predT10link-adjT11thinT12compT13likesabortabort?likes?Figure 3: DS parsing as a graph: actions (edges) are transitions between partial trees (nodes).This parsing process can be modelled as a di-rected acyclic graph (DAG) rooted at T0, with par-tial trees as nodes, and computational and lexi-cal actions as edges (i.e.
transitions between trees)(Sato, 2011).
Figure 3 shows an example: here,intro, pred and *adj correspond to the computa-tional actions INTRODUCTION, PREDICTION and*-ADJUNCTION respectively; and ?john?
is a lex-ical action.
Different DAG paths represent dif-ferent parsing strategies, which may succeed orfail depending on how the utterance is continued.Here, the path T0?T3 will succeed if ?John?
is thesubject of an upcoming verb (?John upset Mary?
);T0 ?
T4 will succeed if ?John?
turns out to be aleft-dislocated object (?John, Mary upset?
).This incrementally constructed DAG makes upthe entire parse state at any point.
The right-most nodes (i.e.
partial trees) make up the currentmaximal semantic information; these nodes withtheir paths back to the root (tree-transition actions)make up the linguistic context for ellipsis andpronominal construal (Purver et al 2011).
Givena conditional probability distribution P (a|w, T )over possible actions a given a word w and (someset of features of) the current partial tree T , we canparse probabilistically, constructing the DAG in abest-first, breadth-first or beam parsing manner.Generation uses exactly the same actions andstructures, and can be modelled on the same DAGwith the addition only of a goal tree; partialtrees are checked for subsumption of the goalat each stage.
The framework therefore inher-ently provides both parsing and generation thatare word-by-word incremental and interchange-able, commensurate with psycholinguistic results(Lombardo and Sturt, 1997; Ferreira and Swets,2002) and suitable for modelling dialogue (Howeset al 2012).
While standard grammar formalismscan of course also be used with incremental pars-ing or generation algorithms (Hale, 2001; Collinsand Roark, 2004; Clark and Curran, 2007), theirstring-based grammaticality and lack of inherentparsing-generation interoperability means exam-ples such as (1) remain problematic.3 MethodOur task here is to learn an incremental DS gram-mar; following Kwiatkowski et al(2012), weassume as input a set of sentences paired withtheir semantic LFs.
Eshghi et al(2013) outline amethod for inducing DS grammars from semanticDS trees (e.g.
Fig.
1), in which possible lexical en-tries are incrementally hypothesized, constrainedby subsumption of the target tree for the sentence.Here, however, this structured tree information isnot available to us; our method must therefore con-strain hypotheses via compatibility with the sen-tential LF, represented as Record Types of TypeTheory with Records (TTR).3.1 Type Theory with Records (TTR)Type Theory with Records (TTR) is an exten-sion of standard type theory shown useful in se-mantics and dialogue modelling (Cooper, 2005;Ginzburg, 2012).
It is also used for representing97non-linguistic context such as the visual percep-tion of objects (Dobnik et al 2012), suggestingpotential for embodied learning in future work.Some DS variants have incorporated TTR as thesemantic LF representation (Purver et al 2011;Hough and Purver, 2012; Eshghi et al 2012).Here, it can provide us with the mechanism weneed to constrain hypotheses in induction by re-stricting them to those which lead to subtypes ofthe known sentential LF.In TTR, logical forms are specified as recordtypes (RTs), sequences of fields of the form [ l : T ]containing a label l and a type T .
RTs can be wit-nessed (i.e.
judged true) by records of that type,where a record is a sequence of label-value pairs[ l = v ], and [ l = v ] is of type [ l : T ] just in casev is of type T .R1 :?
?l1 : T1l2=a : T2l3=p(l2) : T3??
R2 :[l1 : T1l2 : T2?
]R3 : []Figure 4: Example TTR record typesFields can be manifest, i.e.
given a singletontype e.g.
[ l : Ta ] where Ta is the type of whichonly a is a member; here, we write this using thesyntactic sugar [ l=a : T ].
Fields can also be de-pendent on fields preceding them (i.e.
higher) inthe record type ?
see R1 in Figure 4.
Importantlyfor us here, the standard subtyping relation ?
canbe defined for record types: R1 ?
R2 if for allfields [ l : T2 ] in R2, R1 contains [ l : T1 ] whereT1 ?
T2.
In Figure 4, R1 ?
R2 if T2 ?
T2?
, andboth R1 and R2 are subtypes of R3.Following Purver et al(2011), we assumethat DS tree nodes are decorated not with simpleatomic formulae but with RTs, and correspond-ing lambda abstracts representing functions fromRT to RT (e.g.
?r : [ l1 : T1 ].
[ l2=r.l1 : T1 ] wherer.l1 is a path expression referring to the label l1in r) ?
see Figure 5.
The equivalent of conjunc-tion for linked trees is now RT extension (concate-nation modulo relabelling ?
see (Cooper, 2005;Ferna?ndez, 2006)).
TTR?s subtyping relation nowallows a record type at the root node to be in-ferred for any partial tree, and incrementally fur-ther specified via subtyping as parsing proceeds(Hough and Purver, 2012).We assume a field head in all record types, withthis corresponding to the DS tree node type.
Wealso assume a neo-Davidsonian representation of?, T y(t),??
?x=john : ee=arrive : esp=subj(e,x) : thead=p : t??
?Ty(e),[x=john : ehead=x : e]Ty(e ?
t),?r :[head : e].??
?x=r.head : ee=arrive : esp=subj(e,x) : thead=p : t??
?Figure 5: DS-TTR treepredicates, with fields corresponding to the eventand to each semantic role; this allows all availablesemantic information to be specified incrementallyvia strict subtyping (e.g.
providing the subj() fieldwhen subject but not object has been parsed) ?
seeFigure 5 for an example.3.2 Problem StatementOur induction procedure now assumes as input:?
a known set of DS computational actions.?
a set of training examples of the form?Si, RTi?, where Si = ?w1 .
.
.
wn?
is a sen-tence of the language and RTi ?
henceforthreferred to as the target RT ?
is the recordtype representing the meaning of Si.The output is a grammar specifying the possi-ble lexical actions for each word in the corpus.Given our data-driven approach, we take a prob-abilistic view: we take this grammar as associat-ing each word w with a probability distribution ?wover lexical actions.
In principle, for use in pars-ing, this distribution should specify the posteriorprobability p(a|w, T ) of using a particular actiona to parse a word w in the context of a particularpartial tree T .
However, here we make the sim-plifying assumption that actions are conditionedsolely on one feature of a tree, the semantic typeTy of the currently pointed node; and that actionsapply exclusively to one such type (i.e.
ambiguityof type implies multiple actions).
This simplifiesour problem to specifying the probability p(a|w).In traditional DS terms, this is equivalent to as-suming that all lexical actions have a simple IFclause of the form IF ?Ty(X); this is true ofmost lexical actions in existing DS grammars (seeFig.
2), but not all.
Our assumption may there-fore lead to over-generation ?
inducing actionswhich can parse some ungrammatical strings ?
wemust rely on the probabilities learned to make such98parses unlikely, and evaluate this in Section 4.Given this, our focus here is on learning the THENclauses of lexical actions: sequences of DS atomicactions such as go, make, and put (Fig.
2), but nowwith attendant posterior probabilities.
We willhenceforth refer to these sequences as lexical hy-potheses.
We first describe how we construct lexi-cal hypotheses from individual training examples;we then show how to generalise over these, whileincrementally estimating corresponding probabil-ity distributions.3.3 Hypothesis constructionDS is strictly monotonic: actions can only extendthe current (partial) tree Tcur, deleting nothing ex-cept satisfied requirements.
Thus, we can hypoth-esise lexical actions by incrementally exploringthe space of all monotonic, well-formed exten-sions T of Tcur, whose maximal semantics R isa supertype of (extendible to) the target RT (i.e.R ?
RT ).
This gives a bounded space describedby a DAG equivalent to that of section 2.2.1: nodesare trees; edges are possible extensions; paths startfrom Tcur and end at any tree with LF RT .
Edgesmay be either known computational actions ornew lexical hypotheses.
The space is further con-strained by the properties of the lambda-calculusand the modal tree logic LoFT (not all possibletrees and extensions are well-formed).1Hypothesising increments In purely semanticterms, the hypothesis space at any point is the pos-sible set of TTR increments from the current LFR to the target RT .
We can efficiently computeand represent these possible increments using atype lattice (see Figure 6),2 which can be con-structed for the whole sentence before processingeach training example.
Each edge is a RTR repre-senting an increment from one RT, Rj , to another,Rj+1, such that Rj ?
RI = Rj+1 (where ?
rep-resents record type intersection (Cooper, 2005));possible parse DAG paths must correspond tosome path through this lattice.Hypothesising tree structure These DAG pathscan now be hypothesised with the lattice as a con-straint: hypothesising possible sequences of ac-1We also prevent arbitrary type-raising by restricting thetypes allowed, taking the standard DS assumption that nounphrases have semantic type e (rather than a higher type as inGeneralized Quantifier theory) and common nouns their owntype cn, see Cann et al(2005), chapter 3 for details.2Clark (2011) similarly use a concept lattice relatingstrings to their contexts in syntactic grammar induction.Ri : []R11 :[a : b]R12 :[c : d]R12 :[e : f]R21 :[a : bc : d]R22 :[a : be : f]R22 :[c : de : f]RT :?
?a : bc : de : f?
?Figure 6: RT extension hypothesis latticetions which extend the tree to produce the requiredsemantic increment, while the increments them-selves constitute a search space of their own whichwe explore by traversing the lattice.The lexical hypotheses comprising these DAGpaths are divide into two general classes: (1) tree-building hypotheses, which hypothesise appropri-ately typed daughters to compose a given node;and (2) content hypotheses, which decorate leafnodes with appropriate formulae from Ri (non-leaf nodes then receive their content via beta-reduction/extension of daughters).Tree-building can be divided into two generaloptions: functional decomposition (correspondingto the addition of daughter nodes with appropri-ate types and formulae which will form a suitablemother node by beta-reduction); and type exten-sion (corresponding to the adjunction of a linkedtree whose LF will extend that of the current tree,see Sec.
3.1 above).
The availability of the formeris constrained by the presence of suitable depen-dent types in the LF (e.g.
in Fig.
5, p = subj(e, x)depends on the fields with labels x and e, andcould therefore be hypothesised as the body of afunction with x and/or e as argument).
The latter ismore generally available, but constrained by shar-ing of a label between the resulting linked trees.Figure 7 shows an example: a template forfunctional decomposition hypotheses, extending anode with some type requirement ?Ty(X) withdaughter nodes which can combine to satisfy thatrequirement ?
here, of types Y and Y ?
X.Specific instantiations are limited to a finite set oftypes: e.g.
X = e ?
t and Y = e is allowed,but higher types for Y are not.
We implementthese constraints by packaging together permittedsequences of tree updates as macros, and usingthese macros to hypothesise DAG paths commen-surate with the lattice.Finally, semantic content decorations (as se-99IF ?Ty(X)THEN make(??0?
); go(??0?
)put(?Ty(Y )); go(???)make(??1?
); go(??1?
)put(?Ty(Y ?
X)); go(?
)ELSE ABORTFigure 7: Tree-building hypothesisquences of put operations) are hypothesised forthe leaf nodes of the tree thus constructed; theseare now determined entirely by the tree structureso far hypothesised and the target LF RT .3.4 Probabilistic Grammar EstimationThis procedure produces, for each training sen-tence ?w1 .
.
.
wn?, all possible sequences of ac-tions that lead from the axiom tree T0 to a treewith the target RT as its semantics.
These mustnow be split into n sub-sequences, hypothesisinga set of word boundaries to form discrete word hy-potheses; and a probability distribution estimatedover this (large) word hypothesis space to providea grammar that can be useful in parsing.
For this,we apply the procedure of Eshghi et al(2013).For each training sentence S = ?w1 .
.
.
wn?,we have a set HT of possible Hypothesis Tuples(sequences of word hypotheses), each of the formHTj = ?hj1 .
.
.
hjn?, where hji is the word hypoth-esis for wi in HTj .
We must estimate a prob-ability distribution ?w over hypotheses for eachword w, where ?w(h) is the posterior probabilityp(h|w) of a given word hypothesis h being used toparse w. Eshghi et al(2013) define an incremen-tal version of Expectation-Maximisation (Demp-ster et al 1977) for use in this setting.Re-estimation At any point, the Expectationstep assigns each hypothesis tuple HTj a proba-bility based on the current estimate ?
?w:p(HTj|S) =n?i=1p(hji |wi) =n?i=1?
?wi(hji ) (2)The Maximisation step then re-estimatesp(h|w) as the normalised sum of the probabilitiesof all observed tuples HTj which contain h,w:??
?w(h) =1Z?{j|h,w?HTj}n?i=1?
?wi(hji ) (3)where Z is the appropriate normalising constantsummed over all the HTj?s.Incremental update The estimate of ?w is nowupdated incrementally at each training example:the new estimate ?Nw is a weighted average of theprevious estimate ?N?1w and the new value fromthe current example ??
?w from equation (3):?Nw (h) =N ?
1N ?N?1w (h) +1N ??
?w(h) (4)?e.not(aux|do(v|have(pro|he, det|a(x,n|hat(x)), e), e), e)???????????????????
?e=have : esp3=not(e) : tp2=do-aux(e) : tr :?
?x : ep=hat(x) : thead=x : e??x2=?
(r.head,r) : ex1=he : ep1=object(e,x2) : tp=subject(e,x1) : thead=e : es??????????????????
?Figure 8: Conversion of LFs from FOL to TTR.For the first training example, a uniform distribu-tion is assumed; when subsequent examples pro-duce new previously unseen hypotheses these areassigned probabilities uniformly distributed over aheld-out probability mass.4 Experimental SetupCorpus We tested our approach on a sectionof the Eve corpus within CHILDES (MacWhin-ney, 2000), a series of English child-directed ut-terances, annotated with LFs by Kwiatkowski etal.
(2012) following Sagae et al(2004)?s syntacticannotation.
We convert these LFs into semanti-cally equivalent RTs; e.g.
Fig 8 shows the conver-sion to a record type for ?He doesn?t have a hat?.Importantly, our representations remove allpart-of-speech or syntactic information; e.g.
thesubject, object and indirect object predicates func-tion as purely semantic role information express-ing an event?s participants.
This includes e.g.do-aux(e) in (8), which is taken merely to rep-resent temporal/aspectual information about theevent, and could be part of any word hypothesis.From this corpus we selected 500 shortutterance-record type pairs.
The minimum utter-ance length in this set is 1 word, maximum 7,mean 3.7; it contains 1481 word tokens of 246types, giving a type:token ratio of 6.0).
We use thefirst 400 for training and 100 for testing; the testset al has a mean utterance length of 3.7 words,and contains only words seen in training.Evaluation We evaluate our learner by compar-ing the record type semantic LFs produced usingthe induced lexicon against the gold standard LFs,calculating precision, recall and f-score using amethod similar to Allen et al(2008).100Coverage % Precision Recall F-ScoreTop-1 59 0.548 0.549 0.548Top-2 85 0.786 0.782 0.782Top-3 92 0.854 0.851 0.851Table 1: Results: parse coverage & accuracy usingthe top N hypotheses induced in training.Each field has a potential score in the range[0,1].
A method maxMapping(R1, R2) con-structs a mapping from fields in R1 to those in R2to maximise alignment, with fields that map com-pletely scoring a full 1, and partially mapped fieldsreceiving less, depending on the proportion of theR1 field?s representation that subsumes its mappedR2 field;e.g.
a unary predicate field in RT2 suchas[p=there(e) : t]could score a maximum of3 - 1 for correct type t, 1 for correct predicatethere and 1 for the subsumption of its argumente; we use the total to normalise the final score.The potential maximum for any pair is thereforethe number of fields in R1 (including those in em-bedded record types).
So, for hypothesis H andgoal record type G, with NH and NG fields re-spectively:(5) precision = maxMapping(H,G)/NHrecall = maxMapping(H,G)/NG5 ResultsTable 1 shows that the grammar learned achievesboth good parsing coverage and semantic accu-racy.
Using the top 3 lexical hypotheses inducedfrom training, 92% of test set utterances receive aparse, and average LF f-score reaches 0.851.We manually inspected the learned lexicon forinstances of ambiguous words to assess the sys-tem?s ability to disambiguate (e.g.
the word ??s?
(is) has three different senses in our corpus: (1)auxiliary, e.g.
?the coffee?s coming?
; (2) verbpredicating NP identity, e.g.
?that?s a girl?
; and(3) verb predicating location, e.g.
?where?s thepencil?).
From these the first two were in the top3 hypotheses (probabilities p=0.227 and p=0.068).For example, the lexical entry learned for (2) isshown in Fig.
9.However, less common words fared worse: e.g.the double object verb ?put?, with only 3 tokens,had no correct hypothesis in the top 5.
Given suffi-cient frequency and variation in the token distribu-tions, our method appears successful in inducingthe correct incremental grammar.
However, thecomplexity of the search space also limits the pos-sibility of learning from larger record types, as thespace of possible subtypes used for hypothesisingIF ?Ty(e ?
t)THEN make(??0?
); go(??0?)put(?Ty(e))go(??0?)make(??1?
); go(??1?
)put(Ty(e ?
(e ?
t)))put(Fo(?r1 :[head : e]?r2 :[head : e].???????
?x1=r1.head : ex2=r2.head : ee=eq : esp1=subj(e,x2) : tp2=obj(e,x1) : thead=e : t????????))put(????
)ELSE ABORTFigure 9: Action learned for second sense of ?is?tree structure grows exponentially with the num-ber of fields in the type.
Therefore, when learningfrom longer, more complicated sentences, we mayneed to bring in further sources of bias to constrainour hypothesis process further (e.g.
learning fromshorter sentences first).6 ConclusionsWe have outlined a novel method for the induc-tion of a probabilistic grammar in an inherently in-cremental and semantic formalism, Dynamic Syn-tax, compatible with dialogue phenomena suchas compound contributions and with no indepen-dent level of syntactic phrase structure.
Assum-ing only general compositional mechanisms, ourmethod learns from utterances paired with theirlogical forms represented as TTR record types.Evaluation on a portion of the CHILDES corpusof child-directed dialogue utterances shows goodcoverage and semantic accuracy, which lends sup-port to viewing it as a plausible, yet idealised, lan-guage acquisition model.Future work planned includes refining themethod outlined above for learning from longerutterances, and then from larger corpora e.g.
theGroningen Meaning Bank (Basile et al 2012),which includes more complex structures.
This willin turn enable progress towards large-scale incre-mental semantic parsers and allow further investi-gation into semantically driven language learning.101ReferencesJames F. Allen, Mary Swift, and Will de Beaumont.2008.
Deep Semantic Analysis of Text.
In JohanBos and Rodolfo Delmonte, editors, Semantics inText Processing.
STEP 2008 Conference Proceed-ings, volume 1 of Research in Computational Se-mantics, pages 343?354.
College Publications.Valerio Basile, Johan Bos, Kilian Evang, and NoortjeVenhuizen.
2012.
Developing a large semanticallyannotated corpus.
In Proceedings of the Eight In-ternational Conference on Language Resources andEvaluation (LREC 2012), pages 3196?3200, Istan-bul, Turkey.Patrick Blackburn and Wilfried Meyer-Viol.
1994.Linguistics, logic and finite trees.
Logic Journalof the Interest Group of Pure and Applied Logics,2(1):3?29.Ronnie Cann, Ruth Kempson, and Lutz Marten.
2005.The Dynamics of Language.
Elsevier, Oxford.Eugene Charniak.
1996.
Statistical Language Learn-ing.
MIT Press.Stephen Clark and James Curran.
2007.
Wide-coverage efficient statistical parsing with CCGand log-linear models.
Computational Linguistics,33(4):493?552.Alexander Clark and Shalom Lappin.
2011.
LinguisticNativism and the Poverty of the Stimulus.
Wiley-Blackwell.Alexander Clark.
2011.
A learnable representation forsyntax using residuated lattices.
In Philippe Groote,Markus Egg, and Laura Kallmeyer, editors, FormalGrammar, volume 5591 of Lecture Notes in Com-puter Science, pages 183?198.
Springer Berlin Hei-delberg.Michael Collins and Brian Roark.
2004.
Incrementalparsing with the perceptron algorithm.
In Proceed-ings of the 42ndMeeting of the ACL, pages 111?118,Barcelona.Robin Cooper.
2005.
Records and record types in se-mantic theory.
Journal of Logic and Computation,15(2):99?112.Matthew Crocker, Martin Pickering, and CharlesClifton, editors.
2000.
Architectures and Mecha-nisms in Sentence Comprehension.
Cambridge Uni-versity Press.A.P.
Dempster, N.M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via theEM algorithm.
Journal of the Royal Statistical So-ciety.
Series B (Methodological), 39(1):1?38.Simon Dobnik, Robin Cooper, and Staffan Larsson.2012.
Modelling language, action, and perception intype theory with records.
In Proceedings of the 7thInternational Workshop on Constraint Solving andLanguage Processing (CSLP12), pages 51?63.Arash Eshghi, Julian Hough, Matthew Purver, RuthKempson, and Eleni Gregoromichelaki.
2012.
Con-versational interactions: Capturing dialogue dynam-ics.
In S. Larsson and L. Borin, editors, From Quan-tification to Conversation: Festschrift for RobinCooper on the occasion of his 65th birthday, vol-ume 19 of Tributes, pages 325?349.
College Publi-cations, London.Arash Eshghi, Matthew Purver, and Julian Hough.2013.
Probabilistic induction for an incremental se-mantic grammar.
In Proceedings of the 10th In-ternational Conference on Computational Seman-tics (IWCS 2013) ?
Long Papers, pages 107?118,Potsdam, Germany, March.
Association for Compu-tational Linguistics.Raquel Ferna?ndez.
2006.
Non-Sentential Utterancesin Dialogue: Classification, Resolution and Use.Ph.D.
thesis, King?s College London, University ofLondon.Fernanda Ferreira and Benjamin Swets.
2002.
Howincremental is language production?
evidence fromthe production of utterances requiring the compu-tation of arithmetic sums.
Journal of Memory andLanguage, 46:57?84.Victor Ferreira.
1996.
Is it better to give than to do-nate?
Syntactic flexibility in language production.Journal of Memory and Language, 35:724?755.Jonathan Ginzburg.
2012.
The Interactive Stance:Meaning for Conversation.
Oxford UniversityPress.E.
Mark Gold.
1967.
Language identification in thelimit.
Information and Control, 10(5):447?474.John Hale.
2001.
A probabilistic Earley parser asa psycholinguistic model.
In Proceedings of the2nd Conference of the North American Chapter ofthe Association for Computational Linguistics, Pitts-burgh, PA.Julian Hough and Matthew Purver.
2012.
Process-ing self-repairs in an incremental type-theoretic di-alogue system.
In Proceedings of the 16th SemDialWorkshop on the Semantics and Pragmatics of Di-alogue (SeineDial), pages 136?144, Paris, France,September.Christine Howes, Matthew Purver, Rose McCabe,Patrick G. T. Healey, and Mary Lavelle.
2012.Predicting adherence to treatment for schizophreniafrom dialogue transcripts.
In Proceedings of the13th Annual Meeting of the Special Interest Groupon Discourse and Dialogue (SIGDIAL 2012 Confer-ence), pages 79?83, Seoul, South Korea, July.
Asso-ciation for Computational Linguistics.Ruth Kempson,WilfriedMeyer-Viol, and Dov Gabbay.2001.
Dynamic Syntax: The Flow of Language Un-derstanding.
Blackwell.102Dan Klein and Christopher D. Manning.
2005.
Nat-ural language grammar induction with a genera-tive constituent-context mode.
Pattern Recognition,38(9):1407?1419.Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-ter, andMark Steedman.
2010.
Inducing probabilis-tic CCG grammars from logical form with higher-order unification.
In Proceedings of the 2010 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1223?1233, Cambridge, MA, Oc-tober.
Association for Computational Linguistics.Tom Kwiatkowski, Sharon Goldwater, Luke Zettle-moyer, and Mark Steedman.
2012.
A proba-bilistic model of syntactic and semantic acquisitionfrom child-directed utterances and their meanings.In Proceedings of the Conference of the EuropeanChapter of the Association for Computational Lin-guistics (EACL).Vincenzo Lombardo and Patrick Sturt.
1997.
Incre-mental processing and infinite local ambiguity.
InProceedings of the 1997 Cognitive Science Confer-ence.Brian MacWhinney.
2000.
The CHILDES Project:Tools for Analyzing Talk.
Lawrence Erlbaum As-sociates, Mahwah, New Jersey, third edition.Fernando Pereira and Yves Schabes.
1992.
Inside-outside reestimation from partially bracketed cor-pora.
In Proceedings of the 30th Annual Meetingof the Association for Computational Linguistics,pages 128?135, Newark, Delaware, USA, June.
As-sociation for Computational Linguistics.Matthew Purver, Arash Eshghi, and Julian Hough.2011.
Incremental semantic construction in a di-alogue system.
In J. Bos and S. Pulman, editors,Proceedings of the 9th International Conference onComputational Semantics, pages 365?369, Oxford,UK, January.Kenji Sagae, Brian MacWhinney, and Alon Lavie.2004.
Adding syntactic annotations to transcripts ofparent-child dialogs.
In Proceedings of the 4th In-ternational Conference on Language Resources andEvaluation (LREC), pages 1815?1818, Lisbon.Yo Sato.
2011.
Local ambiguity, search strate-gies and parsing in Dynamic Syntax.
In E. Gre-goromichelaki, R. Kempson, and C. Howes, editors,The Dynamics of Lexical Interfaces.
CSLI Publica-tions.Matthew Saxton.
1997.
The contrast theory of nega-tive input.
Journal of Child Language, 24(1):139?161.Mark Steedman.
2000.
The Syntactic Process.
MITPress, Cambridge, MA.Luke Zettlemoyer and Michael Collins.
2007.
On-line learning of relaxed CCG grammars for parsingto logical form.
In Proceedings of the Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning (EMNLP-CoNLL).103
