Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 17?24Manchester, August 2008How Is Meaning Grounded in Dictionary Definitions?A.
Blondin Masse?Laboratoire de combinatoire et d?informatique mathe?matiqueUniversite?
du Que?bec a` Montre?alMontre?al (QC), CANADA H3C 3P8alexandre.blondin.masse@gmail.comG.
Chicoisne, Y. Gargouri, S. Harnad, O. PicardInstitut des sciences cognitivesUniversite?
du Que?bec a` Montre?alMontre?al (QC), CANADA H3C 3P8chicoisne.guillaume@uqam.ca, yassinegargouri@hotmail.comharnad@ecs.soton.ac.uk, olivierpicard18@hotmail.comO.
MarcotteGroupe d?e?tudes et de recherche en analyse des de?cisions (GERAD) and UQ `AMHEC Montre?alMontre?al (Que?bec) Canada H3T 2A7Odile.Marcotte@gerad.caAbstractMeaning cannot be based on dictionary defini-tions all the way down: at some point the cir-cularity of definitions must be broken in someway, by grounding the meanings of certainwords in sensorimotor categories learned fromexperience or shaped by evolution.
This is the?symbol grounding problem?.
We introducethe concept of a reachable set ?
a larger vo-cabulary whose meanings can be learned froma smaller vocabulary through definition alone,as long as the meanings of the smaller vocabu-lary are themselves already grounded.
We pro-vide simple algorithms to compute reachablesets for any given dictionary.1 IntroductionWe know from the 19th century philosopher-mathematician Frege that the referent and the meaning(or ?sense?)
of a word (or phrase) are not the samething: two different words or phrases can refer to thevery same object without having the same meaning(Frege, 1948): ?George W. Bush?
and ?the currentpresident of the United States of America?
have thesame referent but a different meaning.
So do ?humanfemales?
and ?daughters?.
And ?things that are biggerthan a breadbox?
and ?things that are not the size of abreadbox or smaller?.A word?s ?extension?
is the set of things to which itrefers, and its ?intension?
is the rule for defining whatc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.things fall within its extension.. A word?s meaning ishence something closer to a rule for picking out its ref-erent.
Is the dictionary definition of a word, then, itsmeaning?Clearly, if we do not know the meaning of a word,we look up its definition in a dictionary.
But what ifwe do not know the meaning of any of the words in itsdictionary definition?
And what if we don?t know themeanings of the words in the definitions of the wordsdefining those words, and so on?
This is a problem ofinfinite regress, called the ?symbol grounding problem?
(Harnad, 1990; Harnad, 2003): the meanings of wordsin dictionary definitions are, in and of themselves, un-grounded.
The meanings of some of the words, at least,have to be grounded by some means other than dictio-nary definition look-up.How are word meanings grounded?
Almost certainlyin the sensorimotor capacity to pick out their referents(Harnad, 2005).
Knowing what to do with what is nota matter of definition but of adaptive sensorimotor in-teraction between autonomous, behaving systems andcategories of ?objects?
(including individuals, kinds,events, actions, traits and states).
Our embodied sen-sorimotor systems can also be described as applying in-formation processing rules to inputs in order to generatethe right outputs, just as a thermostat defending a tem-perature of 20 degrees can be.
But this dynamic processis in no useful way analogous to looking up a definitionin a dictionary.We will not be discussing sensorimotor grounding(Barsalou, 2008; Glenberg & Robertson, 2002; Steels,2007) in this paper.
We will assume some sort ofgrounding as given: when we consult a dictionary, wealready know the meanings of at least some words,17somehow.
A natural first hypothesis is that the ground-ing words ought to be more concrete, referring to thingsthat are closer to our overt sensorimotor experience,and learned earlier, but that remains to be tested (Clark,2003).
Apart from the question of the boundary condi-tions of grounding, however, there are basic questionsto be asked about the structure of word meanings in dic-tionary definition space.In the path from a word, to the definition of that word,to the definition of the words in the definition of thatword, and so on, through what sort of a structure arewe navigating (Ravasz & Barabasi, 2003; Steyvers &Tenenbaum, 2005)?
Meaning is compositional: A def-inition is composed of words, combined according tosyntactic rules to form a proposition (with a truth value:true or false).
For example, the word to be defined w(the ?definiendum?)
might mean w1&w2& .
.
.
&wn,where the wiare other words (the ?definientes?)
in itsdefinition.
Rarely does that proposition provide the fullnecessary and sufficient conditions for identifying thereferent of the word, w, but the approximation must atleast be close enough to allow most people, armed withthe definition, to understand and use the defined wordmost of the time, possibly after looking up a few of itsdefinientes dw, but without having to cycle through theentire dictionary, and without falling into circularity orinfinite regress.If enough of the definientes are grounded, then thereis no problem of infinite regress.
But we can still askthe question: What is the size of the grounding vocab-ulary?
and what words does it contain?
What is thelength and shape of the path that would be taken in arecursive definitional search, from a word, to its defi-nition, to the definition of the words in its definition,and so on?
Would it eventually cycle through the entiredictionary?
Or would there be disjoint subsets?This paper raises more questions than it answers, butit develops the formal groundwork for a new means offinding the answers to questions about how word mean-ing is explicitly represented in real dictionaries ?
andperhaps also about how it is implicitly represented in the?mental lexicon?
that each of us has in our brain (Hauket al, 2008).The remainder of this paper is organized as follows:In Section 2, we introduce the graph-theoretical defi-nitions and notations used for formulating the symbolgrounding problem in Section 3.
Sections 4 and 5 dealwith the implication of this approach in cognitive sci-ences and show in what ways grounding kernels maybe useful.2 Definitions and NotationsIn this section, we give mathematical definitions forthe dictionary-related terminology, relate them to natu-ral language dictionaries and supply the pertinent graphtheoretical definitions.
Additional details are given toensure mutual comprehensibility to specialists in thethree disciplines involved (mathematics, linguistics andpsychology).
Complete introductions to graph theoryand discrete mathematics are provided in (Bondy &Murty, 1978; Rosen, 2007).2.1 Relations and FunctionsLet A be any set.
A binary relation on A is any subsetR of A?
A.
We write xRy if (x, y) ?
R. The relationR is said to be (1) reflexive if for all x ?
A, we havexRx, (2) symmetric if for all x, y ?
A such that xRy,we have yRx and (3) transitive if for all x, y, z ?
Asuch that xRy and yRz, we have xRz.
The relation Ris an equivalence relation if it is reflexive, symmetricand transitive.
For any x ?
A, the equivalence class ofx, designated by [x], is given by [x] = {y ?
A | xRy}.It is easy to show that [x] = [y] if and only if xRy andthat the set of all equivalence classes forms a partitionof A.Let A be any set, f : A ?
A a function and k apositive integer.
We designate by fk the function f ?f ?
.
.
.
?
f (k times), where ?
denotes the compositionof functions.2.2 DictionariesAt its most basic level, a dictionary is a set of associ-ated pairs: a word and its definition, along with somedisambiguating parameters.
The word1 to be defined,w, is called the definiendum (plural: definienda) whilethe finite nonempty set of words that defines w, dw, iscalled the set of definientes of w (singular: definiens).Each dictionary entry accordingly consists of adefiniendum w followed by its set of definientesdw.
A dictionary D then consists of a finite setof pairs (w, dw) where w is a word and dw={w1, w2, .
.
.
, wn}, where n ?
1, is its definition, satis-fying the property that for all (w, dw) ?
D and for alld ?
dw, there exists (w?, dw?)
?
D such that d = w?.
Apair (w, dw) is called an entry of D. In other words, adictionary is a finite set of words, each of which is de-fined, and each of its defining words is likewise definedsomewhere in the dictionary.2.3 GraphsA directed graph is a pair G = (V,E) such that V isa finite set of vertices and E ?
V ?
V is a finite setof arcs.
Given V ?
?
V , the subgraph induced by V ?,designated by G[V ?
], is the graph G[V ?]
= (V ?, E?
)where E?
= E ?
(V ?
?
V ?).
For any v ?
V , N?
(v)and N+(v) designate, respectively, the set of incomingand outgoing neighbors of v, i.e.N?
(v) = {u ?
V | (u, v) ?
E}N+(v) = {u ?
V | (v, u) ?
E}.We write deg?
(v) = |N?
(v)| and deg+(v) =|N+(v)|, respectively.
A path of G is a sequence1In the context of this mathematical analysis, we will use?word?
to mean a finite string of uninterrupted letters havingsome associated meaning.18(v1, v2, .
.
.
, vn), where n is a positive integer, vi?
Vfor i = 1, 2, .
.
.
, n and (vi, vi+1) ?
E, for i =1, 2, .
.
.
, n ?
1.
A uv-path is a path starting with uand ending with v. Finally, we say that a uv-path is acycle if u = v.Given a directed graph G = (V,E) and u, v ?
V , wewrite u?
v if there exists a uv-path in G. We define arelation ?
asu ?
v ?
u?
v and v ?
u.It is an easy exercise to show that ?
is an equivalencerelation.
The equivalence classes of V with respect to?are called the strongly connected components of G. Inother words, in a directed graph, it might be possible togo directly from point A to point B, without being ableto get back from point B to point A (as in a city withonly one-way streets).
Strongly connected components,however, are subgraphs in which whenever it is possibleto go from point A to point B, it is also possible to comeback from point B to point A (the way back may bedifferent).There is a very natural way of representing defini-tional relations using graph theory, thus providing a for-mal tool for analyzing grounding properties of dictio-naries: words can be represented as vertices, with arcsrepresenting definitional relations, i.e.
there is an arc(u, v) between two words u and v if the word u appearsin the definition of the word v. More formally, for everydictionary D, its associated graph G = (V,E) is givenbyV = {w | ?dwsuch that (w, dw) ?
D},E = {(v, w) | ?dwsuch that (w, dw) ?
D andv ?
dw}.Note that every vertex v of G satisfies deg?G(v) > 0,but it is possible to have deg+G(v) = 0.
In other words,whereas every word has a definition, some words arenot used in any definition.Example 1.
Let D be the dictionary whose definitionsare given in Table 1.
Note that every word appearingin some definition is likewise defined in D (this is oneof the criteria for D to be a dictionary).
The associatedgraph G of D is represented in Figure 1.
Note that(not, good, eatable, fruit) is a path of G while (good,bad, good) is a cycle (as well as a path) of G.3 A Graph-Theoretical Formulation ofthe ProblemWe are now ready to formulate the symbol groundingproblem from a mathematical point of view.3.1 Reachable and Grounding SetsGiven a dictionary D of n words and a person x whoknows m out of these n words, assume that the onlyway x can learn new words is by consulting the dic-tionary definitions.
Can all n words be learned by xWord Definition Word Definitionapple red fruit bad not goodbanana yellow fruit color dark or lightdark not light eatable goodfruit eatable thing good not badlight not dark not notor or red dark colorthing thing tomato red fruityellow light colorTable 1: Definitions of the dictionary DapplebadbananacolordarkeatablefruitgoodlightnotorredthingtomatoyellowFigure 1: Graph representation of the dictionary D.through dictionary look-up alone?
If not, then exactlywhat subset of words can be learned by x through dic-tionary look-up alone?For this purpose, let G = (V,E) be a directed graphand consider the following application, where 2V de-notes the collection of all subsets of V :RG: 2V7??
2VU 7??
U ?
{v ?
V | N?
(v) ?
U}.When the context is clear, we omit the subscript G.Also we let Rk denote the kth power of R. We saythat v ?
V is k-reachable from U if v ?
Rk(U) andk is a nonnegative integer.
It is easy to show that thereexists an integer k such that R`(U) = Rk(U), for everyinteger ` > k. More precisely, we have the followingdefinitions:Definition 2.
Let G = (V,E) be a directed graph, Ua subset of V , and k an integer such that R`(U) =Rk(U) for all ` > k. The set Rk(U) is called the reach-able set from U and is denoted by R?(U).
Moreover, ifR?
(U) = V , then we say that U is a grounding set ofG.We say that G is p-groundable if there exists U ?
Vsuch that |U | = p and U is a grounding set of G. Thegrounding number of a graph G is the smallest integerp such that G is p-groundable.Reachable sets can be computed very simply using abreadth-first-search type algorithm, as shown by Algo-19rithm 1.Algorithm 1 Computing reachable sets1: function REACHABLESET(G,U )2: R?
U3: repeat4: S ?
{v ?
V | N?G(v) ?
R} ?R5: R?
R ?
S6: until S = ?7: return R8: end functionWe now present some examples of reachable sets andgrounding sets.Example 3.
Consider the dictionary D and the graphG of Example 1.
Let U = {bad, light, not, thing}.
NotethatR0(U) = UR1(U) = U ?
{dark, good},R2(U) = R1(U) ?
{eatable}R3(U) = R2(U) ?
{fruit}R4(U) = R3(U)so that R?
(U) = {bad, dark, eatable, fruit, good, light,not, thing} (see Figure 2).
In particular, this meansthat the word ?eatable?
is 2-reachable (but not 1-reachable) from U and all words in U are 0-reachablefrom U .
Moreover, we observe that U is not a ground-ing set of G (?color?, for example, is unreachable).
Onthe other hand, the set U ?
= U ?
{or} is a groundingset of G, so that G is 5-groundable.applebad0bananacolordark1eatable2 fruit3good1light0not0orredthing0tomatoyellowFigure 2: The set R?
(U) (the words in squares) ob-tained from U3.2 The Minimum Grounding Set ProblemGiven a dictionary and its associated graph G, we areinterested in finding minimum grounding sets of G.(Note that in general, there is more than one groundingset of minimum cardinality.)
This is related to a naturaldecision problem: we designate by k-GS the problemof deciding whether G is k-groundable.
We show thatk-GS is closely related to the problem of finding mini-mum feedback vertex sets.
First, we recall the definitionof a feedback vertex set.Definition 4.
Let G = (V,E) be a directed graph andU a subset of V .
We say that U is a feedback vertex setof G if for every cycle C of G, we have U ?
C 6= ?.
Inother words, U covers every cycle of G.The minimum feedback vertex set problem is theproblem of finding a feedback vertex set of G of mini-mum cardinality.
To show that feedback vertex sets andgrounding sets are the same, we begin by stating twosimple lemmas.Lemma 5.
Let G = (V,E) be a directed graph, C acycle of G and U ?
V a grounding set of G. ThenU ?
C 6= ?.Proof.
By contradiction, assume that U ?
C = ?
and,for all v ?
C, there exists an integer k such that v be-longs to Rk(U).
Let ` be the smallest index in the set{k | ?u ?
C such that u ?
Rk(U)}.
Let u be a vertexin C ?
R`(U) and w the predecessor of u in C. SinceU ?
C = ?, k must be greater than 0 and w a memberof R`?1(U), contradicting the minimality of `.Lemma 6.
Every directed acyclic graph G is 0-groundable.Proof.
We prove the statement by induction on |V |.BASIS.
If |V | = 1, then |E| = 0, so that the only vertexv of G satisfies N?G(v) = ?.
Hence R(?)
= V .INDUCTION.
Let v be a vertex such that deg+(v) = 0.Such a vertex exists since G is acyclic.
Moreover,let G?
be the (acyclic) graph obtained from G by re-moving vertex v and all its incident arcs.
By the in-duction hypothesis, there exists an integer k such thatRkG?(?)
= V ?
{v}.
Therefore, V ?
{v} ?
RkG(?)
sothat Rk+1G(?)
= V .The next theorem follows easily from Lemmas 5 and6.Theorem 7.
Let G = (V,E) be a directed graph andU ?
V .
Then U is a grounding set of G if and only ifU is a feedback vertex set of G.Proof.
(?)
Let C be a cycle of G. By Lemma 5, U ?C 6= ?, so that U is a minimum feedback vertex setof G.
(?)
Let G?
be the graph obtained from G byremoving U .
Then G?
is acyclic and ?
is a groundingset of G?.
Therefore, U ?
?
= U is a grounding set ofG.Corollary 8. k-GS is NP-complete.20Proof.
Denote by k-FVS the problem of decidingwhether a directed graph G admits a feedback vertexset of cardinality at most k. This problem is knownto be NP-complete and has been widely studied (Karp,1972; Garey & Johnson, 1979).
It follows directly fromTheorem 7 that k-GS is NP-complete as well since theproblems are equivalent.The fact that problems k-GS and k-FVS are equiv-alent is not very surprising.
Indeed, roughly speaking,the minimum grounding problem consists of finding aminimum set large enough to enable the reader to learn(reach) all the words of the dictionary.
On the otherhand, the minimum feedback vertex set problem con-sists of finding a minimum set large enough to break thecircularity of the definitions in the dictionary.
Hence,the problems are the same, even if they are stated dif-ferently.Although the problem is NP-complete in general, weshow that there is a simple way of reducing the com-plexity of the problem by considering the strongly con-nected components.3.3 Decomposing the ProblemLet G = (V,E) be a directed graph and G1, G2, .
.
.,Gmthe subgraphs induced by its strongly connectedcomponents, where m ?
1.
In particular, there areno cycles of G containing vertices in different stronglyconnected components.
Since the minimum ground-ing set problem is equivalent to the minimum feed-back vertex set problem, this means that when seekinga minimum grounding set of G, we can restrict our-selves to seeking minimum grounding sets of Gi, fori = 1, 2, .
.
.
,m. More precisely, we have the followingproposition.Proposition 9.
Let G = (V,E) be a directed graphwith m strongly connected components, with m ?
1,and let Gi= (Vi, Ei) be the subgraph induced by itsi-th strongly connected component, where 1 ?
i ?
m.Moreover, let Uibe a minimum grounding set of Gi,for i = 1, 2, .
.
.
,m. Then U = ?mi=1Uiis a minimumgrounding set of G.Proof.
First, we show that U is a grounding set of G.Let C be a cycle of G. Then C is completely containedin some strongly connected component of G, say Gj,where 1 ?
j ?
m. But Uj?
U is a grounding set ofGj, therefore Uj?C 6= ?
so that U ?C 6= ?.
It remainsto show that U is a minimum grounding set of G. Bycontradiction, assume that there exists a grounding setU?
of G, with |U ?| < |U | and let U ?i= U??
Vi.
Thenthere exists an index j, with 1 ?
j ?
m, such that|U?j| < |Uj|, contradicting the minimality of |Uj|.Note that this proposition may be very useful forgraphs having many small strongly connected compo-nents.
Indeed, by using Tarjan?s Algorithm (Tarjan,1972), the strongly connected components can be com-puted in linear time.
We illustrate this reduction by anexample.Example 10.
Consider again the dictionary D and thegraph G of Example 1.
The strongly connected com-ponents of G are encircled in Figure 3 and minimumgrounding sets (represented by words in squares) foreach of them are easily found.
Thus the grounding num-ber of G is 5.applebadbananacolordarkeatablefruitgoodlightnotorredthingtomatoyellowFigure 3: The strongly connected components and aminimum grounding set of G3.4 The Grounding KernelIn Example 10, we have seen that there exist somestrongly connected components consisting of only onevertex without any loop.
In particular, there existvertices with no successor, i.e.
vertices v such thatN+G(v) = 0.
For instance, this is the case of the words?apple?, ?banana?
and ?tomato?, which are not used inany definition in the dictionary.
Removing these threewords, we notice that ?fruit?, ?red?
and ?yellow?
arein the same situation and they can be removed as well.Pursuing the same idea, we can now remove the words?color?
and ?eatable?.
At this point, we cannot removeany further words.
The set of remaining words is calledthe grounding kernel of the graph G. More formally,we have the following definition..Definition 11.
Let D be a dictionary, G = (V,E) itsassociated graph and G1= (V1, E1), G2= (V2, E2),.
.
., Gm= (Vm, Em) the subgraphs induced by thestrongly connected components of G, where m ?
1.
LetV?
be the set of vertices u such that {u} is a stronglyconnected component without any loop (i.e., (u, u) isnot an arc of G).
For any u, let N?
(u) denote the setof vertices v such that G contains a uv-path.
Then thegrounding kernel of G, denoted by KG, is the set V ?
{u | u ?
V?
and N?
(u) ?
V ?
}.Clearly, every dictionary D admits a grounding ker-nel, as shown by Algorithm 2.
Moreover, the ground-ing kernel is a grounding set of its associated graph Gand every minimum grounding set of G is a subset ofthe grounding kernel.
Therefore, in studying the sym-bol grounding problem in dictionaries, we can restrict21Algorithm 2 Computing the grounding kernel1: function GROUNDINGKERNEL(G)2: G?
?
G3: repeat4: Let W be the set of vertices of G?5: U ?
{v ?W | N+G?
(v) = ?
}6: G?
?
G?
[W ?
U ]7: until U = ?8: return G?9: end functionourselves to the grounding kernel of the graph G corre-sponding to D. This phenomenon is interesting becauseevery dictionary contains many words that can be recur-sively removed without compromising the understand-ing of the other definitions.
Formally, this property re-lates to the level of a word: we will say of a word wthat it is of level k if it is k-reachable from KGbut not`-reachable from KG, for any ` < k. In particular, level0 indicates that the word is part of the grounding kernel.A similar concept has been studied in (Changizi, 2008).Example 12.
Continuing Example 10 and from whatwe have seen so far, it follows that the grounding kernelof G is given byKG= {bad, dark, good, light, not, or, thing}.Level 1 words are ?color?
and ?eatable?, level 2 wordsare ?fruit?, ?red?
and ?yellow?, and level 3 words are?apple?, ?banana?
and ?tomato?.4 Grounding Sets and the MentalLexiconIn Section 3, we introduced all the necessary terminol-ogy to study the symbol grounding problem using graphtheory and digital dictionaries.
In this section, we ex-plain how this model can be useful and on what assump-tions it is based.A dictionary is a formal symbol system.
The pre-ceding section showed how formal methods can beapplied to this system in order to extract formal fea-tures.
In cognitive science, this is the basis of com-putationalism (or cognitivism or ?disembodied cogni-tion?
(Pylyshyn, 1984)), according to which cognition,too, is a formal symbol system ?
one that can be stud-ied and explained independently of the hardware (or,insofar as it concerns humans, the wetware) on whichit is implemented.
However, pure computationalismis vulnerable to the problem of the grounding of sym-bols too (Harnad, 1990).
Some of this can be reme-died by the competing paradigm of embodied cogni-tion (Barsalou, 2008; Glenberg & Robertson, 2002;Steels, 2007), which draws on dynamical (noncompu-tational) systems theory to ground cognition in senso-rimotor experience.
Although computationalism andsymbol grounding provide the background context forour investigations and findings, the present paper doesnot favor any particular theory of mental representationof meaning.A dictionary is a symbol system that relates words towords in such a way that the meanings of the definiendaare conveyed via the definientes.
The user is intended toarrive at an understanding of an unknown word throughan understanding of its definition.
What was formallydemonstrated in Section 3 agrees with common sense:although one can learn new word meanings from a dic-tionary, the entire dictionary cannot be learned in thisway because of circular references in the definitions(cycles, in graph theoretic terminology).
Information?
nonverbal information ?
must come from outside thesystem to ground at least some of its symbols by somemeans other than just formal definition (Cangelosi &Harnad, 2001).
For humans, the two options are learnedsensorimotor grounding and innate grounding.
(Al-though the latter is no doubt important, our current fo-cus is more on the former.
)The need for information from outside the dictio-nary is formalized in Section 3.
Apart from confirmingthe need for such external grounding, we take a sym-metric stance: In natural language, some word mean-ings ?
especially highly abstract ones, such as thoseof mathematical or philosophical terms ?
are not orcannot be acquired through direct sensorimotor ground-ing.
They are acquired through the composition of pre-viously known words.
The meaning of some of thosewords, or of the words in their respective definitions,must in turn have been grounded through direct senso-rimotor experience.To state this in another way: Meaning is not just for-mal definitions all the way down; nor is it just sensori-motor experience all the way up.
The two extreme polesof that continuum are sensorimotor induction at onepole (trial and error experience with corrective feed-back; observation, pointing, gestures, imitation, etc.
),and symbolic instruction (definitions, descriptions, ex-planation, verbal examples etc.)
at the other pole.
Be-ing able to identify from their lexicological structurewhich words were acquired one way or the other wouldprovide us with important clues about the cognitive pro-cesses underlying language and the mental representa-tion of meaning.To compare the word meanings acquired via sensori-motor induction with word meanings acquired via sym-bolic instruction (definitions), we first need access tothe encoding of that knowledge.
In this componentof our research, our hypothesis is that the representa-tional structure of word meanings in dictionaries sharessome commonalities with the representational structureof word meanings in the human brain (Hauk et al,2008).
We are thus trying to extract from dictionar-ies the grounding kernel (and eventually a minimumgrounding set, which in general is a proper subset ofthis kernel), from which the rest of the dictionary can bereached through definitions alone.
We hypothesize thatthis kernel, identified through formal structural analy-22sis, will exhibit properties that are also reflected in themental lexicon.
In parallel ongoing studies, we are find-ing that the words in the grounding kernel are indeed(1) more frequent in oral and written usage, (2) moreconcrete, (3) more readily imageable, and (4) learnedearlier or at a younger age.
We also expect they will be(5) more universal (across dictionaries, languages andcultures) (Chicoisne et al, 2008).5 Grounding Kernels in NaturalLanguage DictionariesIn earlier research (Clark, 2003), we have been ana-lyzing two special dictionaries: the Longman?s Dic-tionary of Contemporary English (LDOCE) (Procter,1978) and the Cambridge International Dictionary ofEnglish (CIDE) (Procter, 1995).
Both are officiallydescribed as being based upon a defining vocabulary:a set of 2000 words which are purportedly the onlywords used in all the definitions of the dictionary, in-cluding the definitions of the defining vocabulary itself.A closer analysis of this defining vocabulary, however,has revealed that it is not always faithful to these con-straints: A significant number of words used in the def-initions turn out not to be in the defining vocabulary.Hence it became evident that we would ourselves haveto generate a grounding kernel (roughly equivalent tothe defining vocabulary) from these dictionaries.The method presented in this paper makes it possi-ble, given the graph structure of a dictionary, to extracta grounding kernel therefrom.
Extracting this struc-ture in turn confronts us with two further problems:morphology and polysemy.
Neither of these problemshas a definite algorithmic solution.
Morphology canbe treated through stemming and associated look-uplists for the simplest cases (i.e., was?
to be, and chil-dren?
child), but more elaborate or complicated caseswould require syntactic analysis or, ultimately, humanevaluation.
Polysemy is usually treated through statisti-cal analysis of the word context (as in Latent SemanticAnalysis) (Kintsch, 2007) or human evaluation.
Indeed,a good deal of background knowledge is necessary toanalyse an entry such as: ?dominant: the fifth note of amusical scale of eight notes?
(the LDOCE notes 16 dif-ferent meanings of scale and 4 for dominant, and in ourexample, none of these words are used with their mostfrequent meaning).Correct disambiguation of a dictionary is time-consuming work, as the most effective way to do itfor now is through consensus among human evaluators.Fortunately, a fully disambiguated version of the Word-Net database (Fellbaum, 1998; Fellbaum, 2005) has justbecome available.
We expect the grounding kernel ofWordNet to be of greater interest than the defining vo-cabulary of either CIDE or LDOCE (or what we extractfrom them and disambiguate automatically, and imper-fectly) for our analysis.6 Future WorkThe main purpose of this paper was to introduce a for-mal approach to the symbol grounding problem basedon the computational analysis of digital dictionaries.Ongoing and future work includes the following:The minimum grounding set problem.
We have seenthat the problem of finding a minimum grounding set isNP-complete for general graphs.
However, graphs as-sociated with dictionaries have a very specific structure.We intend to describe a class of graphs including thosespecific graphs and to try to design a polynomial-timealgorithm to solve the problem.
Another approach isto design approximation algorithms, yielding a solutionclose to the optimal solution, with some known guaran-tee.Grounding sets satisfying particular constraints.
LetD be a dictionary, G = (V,E) its associated graph,and U ?
V any subset of vertices satisfying a givenproperty P .
We can use Algorithm 1 to test whetheror not U is a grounding set.
In particular, it would beinteresting to test different sets U satisfying differentcognitive constraints.Relaxing the grounding conditions.
In this paperwe imposed strong conditions on the learning of newwords: One must know all the words of the definitionfully in order to learn a new word from them.
This isnot realistic, because we all know one can often under-stand a definition without knowing every single wordin it.
Hence one way to relax these conditions wouldbe to modify the learning rule so that one need only un-derstand at least r% of the definition, where r is somenumber between 0 and 100.
Another variation wouldbe to assign weights to words to take into account theirmorphosyntactic and semantic properties (rather thanjust treating them as an unordered list, as in the presentanalysis).
Finally, we could consider ?quasi-groundingsets?, whose associated reachable set consists of r% ofthe whole dictionary.Disambiguation of definitional relations.
Analyzingreal dictionaries raises, in its full generality, the prob-lem of word and text disambiguation in free text; thisis a very difficult problem.
For example, if the word?make?
appears in a definition, we do not know whichof its many senses is intended ?
nor even what itsgrammatical category is.
To our knowledge, the onlyavailable dictionary that endeavors to provide fully dis-ambiguated definitions is the just-released version ofWordNet.
On the other hand, dictionary definitionshave a very specific grammatical structure, presumablysimpler and more limited than the general case of freetext.
It might hence be feasible to develop automaticdisambiguation algorithms specifically dedicated to thespecial case of dictionary definitions.Concluding Remark: Definition can reach the sense(sometimes), but only the senses can reach the referent.Research funded by Canada Research Chair in Cog-nitive Sciences, SSHRC (S. Harnad)and NSERC (S.Harnad & O. Marcotte)23ReferencesBarsalou, L. (2008) Grounded Cognition.
Annual Re-view of Psychology (in press).Bondy, J.A.
& U.S.R.
Murty.
(1978) Graph theorywith applications.
Macmillan, New York.Cangelosi, A.
& Harnad, S. (2001) The Adap-tive Advantage of Symbolic Theft Over SensorimotorToil: Grounding Language in Perceptual Categories.Evol.
of Communication 4(1) 117-142.Changizi, M.A.
(2008) Economically organized hier-archies in WordNet and the Oxford English Dictio-nary.
Cognitive Systems Research (in press).Chicoisne G., A.
Blondin-Masse?, O. Picard, S. Har-nad (2008) Grounding Abstract Word DefinitionsIn Prior Concrete Experience.
6th Int.
Conf.
on theMental Lexicon, Banff, Alberta.Clark G. (2003) Recursion Through Dictionary Defi-nition Space: Concrete Versus Abstract Words.
(U.Southampton Tech Report).Fellbaum, C. (1998) WordNet: An electronic lexicaldatabase.
Cambridge: MIT Press.Fellbaum, C. (2005) Theories of human semantic rep-resentation of the mental lexicon.
In: Cruse, D.
A.(Ed.
), Handbook of Linguistics and CommunicationScience, Berlin, Germany: Walter de Gruyter, 1749-1758.Frege G. (1948) Sense and Reference.
The Philosoph-ical Review 57 (3) 209-230.Garey, M.R.
& D.S.
Johnson (1979) Computersand Intractability: A Guide to the Theory of NP-completeness.
W.H.
Freeman, New York.Glenberg A.M. & D.A.
Robertson (2002) SymbolGrounding and Meaning: A Comparison of High-Dimensional and Embodied Theories of Meaning.Journal of Memory and Language 43 (3) 379-401.Harnad, S. (1990) The Symbol Grounding Problem.Physica D 42:335-346.Harnad, S. (2003) Symbol-Grounding Problem.
En-cylopedia of Cognitive Science.
Nature PublishingGroup.
Macmillan.Harnad, S. (2005) To Cognize is to Categorize: Cog-nition is Categorization.
In Lefebvre, C. and Cohen,H.
(Eds.
), Handbook of Categorization.
Elsevier.Hauk, O., M.H.
Davis, F. Kherif, F.
Pulvermu?ller.
(2008) Imagery or meaning?
Evidence for a se-mantic origin of category-specific brain activity inmetabolic imaging.
European Journal of Neuro-science 27 (7) 1856-1866.Karp, R.M.
(1972) Reducibility among combinato-rial problems.
In: R.E.
Miller, J.W.
Thatcher (Eds.
),Complexity of Computer Computations, PlenumPress, New York, 1972, pp.
85-103.Kintsch, W. (2007) Meaning in Context.
In T.K.Landauer, D.S.
McNamara, S. Dennis & W.
Kintsch(Eds.
), Handbook of Latent Semantic Analysis.
Erl-baum.Procter, P. (1978) Longman Dictionary of Contempo-rary English.
Longman Group Ltd., Essex, UK.Procter, P. (1995) Cambridge International Dictionaryof English (CIDE).
Cambridge University Press.Pylyshyn, Z. W. (1984) Computation and Cognition:Towards a Foundation for Cognitive Science.
Cam-bridge: MIT Press.Ravasz, E. & Barabasi, A. L. (2003) Hierarchical or-ganization in complex networks.
Physical Review E67, 026112.Rosen, K.H.
(2007) Discrete mathematics and its ap-plications, 6th ed.
McGraw-Hill.Steels, L. (2007) The symbol grounding problem issolved, so what?s next?
In De Vega, M. and G. Glen-berg and A. Graesser (Eds.
), Symbols, embodimentand meaning.
Academic Press, North Haven.Steyvers, M. & Tenenbaum J.B. (2005) The large-scale structure of semantic networks: statisticalanalyses and a model of semantic growth.
CognitiveScience, 29(1) 41-78.Tarjan, R. (1972) Depth-first search and linear graphalgorithms.
SIAM Journal on Computing.
1 (2) 146-160.24
