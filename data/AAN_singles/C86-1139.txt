DIVIDED AND VALENCY-ORIENTED PAHSING IN SPEECH ONDEP~A~DJJ~GGerh.Th.NiedermairZT ZTI INFAbstractA parsing scheme for spoken utterances isproposed that deviates from traditional 'one go'left to right sentence parsing in that it devidesthe parsing process first into two seperate parallelprocesses.
Verbal constituents and nominal phrases(including prepositonal phrases) are treatedseperately and only brought together in an utteranceparser.
This allows especially the utterance parserto draw on valency information right from beginningwhen amalgamating the nominal constituents to theverbal core by means of binary sentence rules.
Thepaper also discusses problems of representing thevalency information in case-frames arising in aspoken language environment.O.Setu~In the framework of a speech understanding systemSPICOS (Siemens IPO Philips Continuous SpeechRecognition,Understanding and Dialog Project) whichis supported by the German Federal Ministry ofTechnology and Research, new parsing strategies havebeen investigated.
The whole system is designed asan interface for spoken language in German and Dutchto a relational database, that contains officeinformation on the project itself like letters,publications, dates, and persons involved, etc.
Itshould be able to answer all kind of questions andimperatives concerning the subject matter.
Thevocabulary cc~nprises about 1000 word-formes.I.
Goals and ProblemsWhether or not one argues in favour of an interfacebetween the acoustic and linguistic modules thatallows for passing information in both directions isto be kept seperate of the discussion on what kindof knowledge is available to the linguisticanalysis.
Only if it is able to reduce the number ofpossible sentences to a considerable extent it makessense to organize this knowledge most effectively.In order to reduce the flood of hypotheses one hasto make sure that the linguistic module uses as muchknowledge as can be made available at the most earlytime of processing.
Whether this knowledge is thenused to make predictions for the acoustic modules orif the whole process works sequentially is rather aquestion of efficiency than of principle.
(SeeBriscoe (1984))1.1.
Interface and System ArchitectureTo study the effects of different techniques inde-pendent of each other we have for the first versiondecided on a simple sequential interface.
Theacoustic module delivers a list of word hypotheseswith begin, end, and score.
But not every word, thatstarts physically at the same point of time wherethe previou~ hypothesis ends, is a possiblesuccessor.
One can limit the number of successorsto those which are phonetically justified.
Theconsequences of this are that the interface is anetwork of nodes and edges, where the nodesrepresent the words, rather than a list withbeginnings and ends, which a chart-parser normallyexpects.The interface also contains scores of the differentSiemens AGOtto-Hahn-Ring 68000 MOnchen 83word-hypotheses.
Nevertheless we do not use themyet, first because the analysis up to now worksexhaustively and non-deterministic.
This allows usto see how and where linguistic knowledge can bebrought to bear most effectively.
(See alsoThompson (1984), who also argues for keeping thesources seperate during the 'try-out-phase').Weighing different syntactic structures implies thatthey have a weight multiplying factor which isinherent to them.
(See Woods (1982)).
Yet there isno agreement as to how to add up the scores insyntactic analysis.
We believe that there is nogeneral procedure.
It is highly dependent on thedomain and the influence of the domain on thesyntactic struetures.1.2.
Flow of Analysis ComponentsKnowledge of relations between objects or objectsand processes can be expressed in terms ofcaseframes.
Our parsing strategy is mainly guidedby the hypothesis that one of the major sources ofrestrictive power on the sentence level is to befound through caseframe restrictions.
In order totake care of the restrictions that are carried outby the verb simple left to right parsers seem ratherinappropriate.
The caseframe restrictions can onlybe applied when the respective verb is encountered,which in German, badly enough, is mostly at the endof a sentence.
The nominal and prepositionalphrases are then grouped around the verb (see alsoM.Johnson, arguing that ~y  in a DCG frame-work).To cope with the huge number of hypotheses anattempt is made here to further cut them downthrough generative power in caseframes andnecessarily early verb-recognition.2.
Divided Parsin~This has lead us to a parsing strategy, that firstsplits up the parsing of the word-hypotheses intotwo different channels.
One is the Nominal-Parserthat takes care of all terminal elements that belongto a nominal group.
The other part is the verb-groupparser that is initialised with all verbalcategories.
They could work in parallel.
They arebrought together again in the utterance parser, thatdeals with one verb-hypothesis at a time.
Thisenables us to bring to bear the caseframerestrictions of that particular verb at a this earlypoint.
One verb-hypothesis is done after theother.
Since the type of rules is also different inboth cases the parsers can be tuned to therespective requirements.~~~utterance~mel5933.
The Nominal and the Verb PamserThe nominal-parser is in essence a chart parser (seeWinograd 1983), working with augmented context-freegrammar rules.
It also triggers actions to popfeatures up to the dominating node.
Prepositionaland nominal phrases specifying NP's get aloattached here according to the caseframe informationof the head of the NP-constituent.
This is only truefor immediately neighbouring PP's.Focused and there-fore moved PP's have to be treated differently.The parsing of the verbal groups, whose partsmay be scattered all over the sentence like:' Wer hat am 17.Mai einen Brief gesehrieben'(who has written a letter on the 17th of May)is carried out by a modified chart-parser, which isalso able to take care of discontinuous elements inthe grammar, like:VG ~ VRB (finite part) +:+ VNF (non-finite part)where +:+ indicates that the next constituent issomewhere to the right.
The output of this parseris a complete list of possible verb groups.
Whichcaseframe they point to is a feature accumulatedduring the parse.
In the case of verbal adjuncts thefeature is a result of both components.4.
The Utterance ParserThis again is a chart parser, which for one go isinitialized with the NP's, PP's, and AP's asterminal categories and the parts constituting thefirst verb-group hypothesis.
It selects the rightconstituents according to the information given inthe caseframe of the current verb-hypothesis.
Sinceour semantic representation is a kind of predicatecalculus formalism (see Bunt 1985 and v.Deemter1985) at this level almost every constituent (exeptfor focused PP's) can become arguments of thepredicate.
For this reason there is no pointwhatsoever in generating nodes that combineconstituents into any other than S(sentence)-nedes,as it is quite common in traditional grammars.Itdoes not contribute any additional meaning to asyntax-tree that has to be transformed into apredicate-argument structure.
The only purpose isfor us to restrict the linear precedence.
(Theyserve very much the same purpose as the LP rules inthe GPSG formalism (Gazdar 1985).
For linearprecedence in German see also Russell(1985).Rules like these would yield very flat trees andlead to quite a number of rules.
In order to avoidthat we have set up a set of binary rules that is alot smaller.
The deepest level nodes always take averb and one of the surrounding constituents.
Theycreate an artificial node that in turn can takeanother constituent and build a new artificialnode.The binary dependency trees generated by these ruleslook like: ~ b ( o u n d )~ NP(ob)VRB NP(sub)594That this grammar also demands rules of the kind:S ~--- S + VNFbmay be surprising at first sight.
But taking intoconsideration that no additional information isconveyed by the nodes higher up in the hierarchythis does not seem so bad any longer.
These rulescan be indexed acoording to how many NP-argumentsthey contain.
This is a lexical feature of a verb.Only those rules will be invoked whose index doesnot exceed the maximum number of NP-valencies of theverb.5.
The Valency LexiconWhen adding new constituents to a node, theirfeatures and, if necessary, also strings aretested.
Although the algorithm is not based onpattern-matching like some other frame basedapproaches (Hayes 1981 and Hayes 1985), the entriesin the case-frame lexicon sometimes do have to comevery close to it, in order to be of a power that notonly describes case indicators and fillers that mayoccur, but at the same time excludes the wrong ones;a feature that is generally referred to as stronggenerative power.As one can see from the rules, each constituentcarries an index, that is passed on to the testprocedure as one of its parameters.
It tells whichfunction this constituent has in the surfacestructure.
This function is also an entry in thecaseframes since some case-roles behave differentlydepending on the function they have to fullfill inthe surface structure.
It is at the same time ameans to restrict the ordering of the constituentson the surface, which even in German is not asliberal as to make this kind of informationredundant.
(See Russell 1985) Case-roles look ofthe following kind:~ TIME-POINTro-~Ie ~i~attributeIPNM Ivom IDAT~PNM Imit IDAT datumfiller- filler-descript, value+intervallmonth-valueyear-valuejahrPOB= prepositional objectPNM = prepositional phrase as noun modificationThe test procedure checks whether a certain slot canbe realised according to the feature parameters ofthe constituent.
If the test is sucessfull, itreturns a number which indicates the number of thecaserole, that the slot belonged to.
In order toprevent doubling of caseroles in a sentence, whichwith this kind of input can easily happen, it ischecked whether this case-role is not yet a memberof a set of case-roles, already accumulated.
Thisset is kept as a feature of the nodes in the binarytree.
If not, it is made a new member of this setand passed on to the next level node.Each caseframe comprises a selection of case-roles.There are frames for verbs as well as for nouns.The noun frames become crucial when attatching theproper PP's to NP's.
The caseframes are~unlike inother systems ( Brietzmann 1984, Hayes 1985), staticdata structures that are not instantiated, nor dothey trigger any actions.
Since it is very unclearas to the criterion of whether roles, especiallyprepositional objects and the like are obligatory,no distinction is made in the caseframes.
The factthat certain NP's are obligatory for a verb is takencare of by the argmnent-number of the verb.
Thereis also no distinction made between immediately verbdependent and free prepositional complements.
Firstone does not really know where to draw theborderline between the two, (see Vater 1978, Jacobs1985) and second, from the point of view of semanticinterpretation, they all have to be treated in thesame way, namely as arguments of the verb.Because of the requested power of the caseframes theinformation given in the slots has to be as generalas possible but also as explicit as necessary toprevent the attachment of those hypotheses which areproduced but we rather would not want to fit.
Infact each slot represents a combination ofcasemarkers and categories or particular strings ofcasefillers.
The latter are usually to be found ashead of the noun phrase.Taking a clo~er look at the heads of the phrases onewill notice that they can play very different roleswith respect to their function in realising acertain case-role.
They are either explicitdesignators of the role in cases like:'Brief mit Datum 17.1.86'("letter with date .... )where 'mit' works as an empty rolemarker, i.e it isnot role-specific, and the head 'datum' takes therole of the preposition.
Those strings we callrole-attributes.
They can also be descriptions ofthe value, that is ment to fill the slot.
On thesyntactic surface however they appear very much thesame, namely as :Prep + Nom + Propernamelike in: 'von Monat Mai' (fr~n (the month of) May)In these cases one can use the semantic categoriesof the heads in order to identify them as properfillers.There are also cases where there are specific valuedescriptions, whose status is usually somewhatinbetween the two mentioned above.
They demand aparticular preposition, like 'jahr'~in the slotsabove.
For ex~nple using 'Jahr' as a valuedescription demands 'aus' as a preposition, which inturn cannot be used with a possible value of 'Jahr',which would for instance be '1985'.
If the head ofthe constituent is only a value like in:'der Brief yon 1985'The semantic category of this value has to appear inthe slot restrictions.
That this is heavilyapplication dependent is clear enough.
This kind ofinformation can be kept in a seperate network.Leaving it to the database to decide whether a valueis appropriate means that the database can neverdecide whether this value has not been stored orwhether it looked for a non-existent relation.
Thisagain clearly influences the possible response ofthe system.
With this information stored in thecase-fr~nes the system can answer maybe that it hasno letters from this year, whereas in thealternative case it could just answer 'no resultretrieved',They also behave differently in terms of case-rolerestrictions, as in the above example you could say:'aus dem jahr 1985' (from the year 1985)but not: ~ 'aus 1985' (from 1985)Therefore we have decided to demand semanticcategories in the casefr~nes for values too.7.ConclusionWe have introduced a parsing strategy that heavilyrelies on ease-frmne and therefore also on semanticlabelling informat?on, In order to detect theverbs, that set up the appropriate case-frames hascaused us to split the parsing process first intotwo parallel processes.
One parses the nominals andthe prepositional phrases, the other one the verbgroups, The two processes are brought together anda sentence-parse is tried on the basis of thehypothesised verb-frame, The parsers work withaugmented context-free gr~ars ,  that alsoperculate features to the higher nodes.
The nodesdo not have to convey any additonal information.The also trigger tests to check ease~rolerestrictions.Literatm~eI~'r A., Feigenbaum E?A~: Understanding SpokenLanguage.
in:The Handbook of ArtificialIntelligence.
Vol I , London 1981Brietzmann A.: Semantische und Pra~natischeAnalyse im Erlanger Spracherkennungsprojekt.
in:Arbeitsberichte des Inst~tuts f. Mathem.
Maschinenund Datenverarbeitung Bd.17 Nr.5, Erlangen 1984Bri~eoe, Bogura.ev: Control Structures andTheories of Interaction in Speech UnderstandingSystems.
Proceedings of the Coling 1984Bunt H.: Mass Nouns and Model Theoretic Semantics.1985v.Deemter K~: The Locical Languages of TENDUM andSPICES.
1985Gazdar G,, Klein E., PullumG., Sag l.:GeneralizedPhrase Structure Grammar.
Cambridge, Mass.
1985Haye~ P.: Semantic Caseframe Parsing andSyntactic Generality, in: Proceedings of the ACL 1985Hayes, Carbonell:Multi Strategy Parsing.
1981Jaeob~ J,: Thesen zur Valenz.
Unpubl.
MS , 1985Lea W,(ed):Trends in Speech Recognition.
N.York 1980PereiraF.
: A New Characterisation of AttatchmentPreferences.
in: Dowry, Kartunnen, Zwicky (ed): Nat.Language Processing: Psyeholingistic, Computationaland Theoretical Perspectives, Cambridge 1984ProudianD,, Pollard C?
: Parsing Head DrivenPhrase Structure Grammar.
in: Proc, of the ACL 1985Russell G~: A GPS-Grammar for German Word Order,in: Klemk U.
(ed):, Kontextfreie Syntaxen, TObingen1985, p.19-32Th~npsonH.
:Speeeh Transcription: An IncrementalInteractive Approach.
in: Prec.
of the ECAI 1984Vater H.: Probleme der Verbvalenz.
in: KLAGE I,1978Winograd T.: Language as a Cognitive ProcessVol I , 1983WoGds W?Ao: Optimal Search Strategies for SpeechUnderstanding Control.
in: Artificial Intellegence18, 1982 , p.295-326595
