Proceedings of the NAACL HLT 2010 Workshop on Semantic Search, pages 44?52,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsObject Search: Supporting Structured Queries in Web Search EnginesKim Cuong Pham?, Nicholas Rizzolo?, Kevin Small?, Kevin Chen-Chuan Chang?, Dan Roth?University of Illinois at Urbana-Champaign?Department of Computer Science{kimpham2, rizzolo, kcchang, danr}@illinois.eduTufts University?Department of Computer Sciencekevin.small@tufts.eduAbstractAs the web evolves, increasing quantities ofstructured information is embedded in webpages in disparate formats.
For example, adigital camera?s description may include itsprice and megapixels whereas a professor?sdescription may include her name, univer-sity, and research interests.
Both types ofpages may include additional ambiguous in-formation.
General search engines (GSEs)do not support queries over these types ofdata because they ignore the web documentsemantics.
Conversely, describing requi-site semantics through structured queries intodatabases populated by information extraction(IE) techniques are expensive and not easilyadaptable to new domains.
This paper de-scribes a methodology for rapidly develop-ing search engines capable of answering struc-tured queries over unstructured corpora by uti-lizing machine learning to avoid explicit IE.We empirically show that with minimum ad-ditional human effort, our system outperformsa GSE with respect to structured queries withclear object semantics.1 IntroductionGeneral search engines (GSEs) are sufficient forfulfilling the information needs of most queries.However, they are often inadequate for retrievingweb pages that concisely describe real world ob-jects as these queries require analysis of both un-structured text and structured data contained in webpages.
For example, digital cameras with specificbrand, megapixel, zoom, and price attributes mightbe found on an online shopping website, or a pro-fessor with her name, university, department, andresearch interest attributes might be found on herhomepage.
Correspondingly, as the web continuesto evolve from a general text corpus into a hetero-geneous collection of documents, targeted retrievalstrategies must be developed for satisfying thesemore precise information needs.
We accomplish thisby using structured queries to capture the intendedsemantics of a user query and learning domain spe-cific ranking functions to represent the hidden se-mantics of object classes contained in web pages.It is not uncommon for a user to want to pose anobject query on the web.
For example, an onlineshopper might be looking for shopping pages thatsell canon digital cameras with 5 megapixels cost-ing no more than $300.
A graduate student mightbe looking for homepages of computer science pro-fessors who work in the information retrieval area.Such users expect to get a list web pages containingobjects they are looking for, or object pages, whichwe will define more precisely in later sections.GSEs rarely return satisfactory results when theuser has a structured query in mind for two primaryreasons.
Firstly, GSEs only handle keyword querieswhereas structured queries frequently involve datafield semantics (e.g.
numerical constraints) and ex-hibit field interdependencies.
Secondly, since GSEsare domain-agnostic, they will generally rank cam-era pages utilizing the same functions as a profes-sor?s homepage, ignoring much of the structured in-formation specific to particular domains.Conversely, vertical search engines (e.g.
DBLife,cazoodle.com, Rexa.info, etc.)
approach this prob-44lem from the information extraction (IE) perspec-tive.
Instead of searching an inverted index directly,they first extract data records from text (Kushmer-ick et al, 1997; McCallum et al, 2000).
IE solu-tions, even with large scale techniques (Agichtein,2005), do not scale to the entire web and cost signif-icantly more than GSEs.
Secondly, creating domain-specific models or wrappers require labeling trainingexamples and human expertise for each individualsite.
Thirdly, pre-extracting information lacks flexi-bility; decisions made during IE are irrevocable, andat query time, users may find additional value in par-tial or noisy records that were discarded by the IEsystem.These issues motivate our novel approach for de-signing a GSE capable of answering complex struc-tured queries, which we refer to as Object Search.At a high level, we search web pages containingstructured information directly over their feature in-dex, similarly to GSEs, adding expressivity by re-formulating the structured query such that it can beexecuted on a traditional inverted index.
Thus, weavoid the expense incurred by IE approaches whensupporting new object domains.
From a techni-cal perspective, this work describes a principled ap-proach to customizing GSEs to answer structuredqueries from any domain by proposing a composi-tional ranking model for ranking web pages withregards to structured queries and presenting an in-teractive learning approach that eases the process oftraining for a new domain.2 The Object Search ProblemThe Object Search problem is to find the objectpages that answer a user?s object query.
An objectquery belongs to an object domain.
An object do-main defines a set of object attributes.
An objectquery is simply a set of constraints over these at-tributes.
Thus we define an object query as a tupleof n constraints q ?
c1 ?
c2 ?
.. ?
cn, where ci is aconstraint on attribute ai.
More specifically, a con-straint ci is defined as a set of acceptable values ?ifor attribute ai; i.e.
ci = (ai ?
?i).
For example, anequality constraint such as ?the brand is Canon?
canbe specified as (abrand ?
{Canon}) and a numericrange constraint such as ?the price is at most $200?can be specified as (aprice ?
[0, 200]).
When theuser does not care about an attribute, the constraintis the constant true.Given an object query, we want a set of satis-fying object pages.
Specifically, object pages arepages that represent exactly one inherent object onthe web.
Pages that list several objects such as adepartment directory page or camera listing pagesare not considered object pages because even thoughthey mentioned the object, they do not represent anyparticular object.
There is often a single object pagebut there are many web pages that mention the ob-ject.The goal of Object Search is similar to learning torank problems (Liu, 2009), in that its goal is to learna ranking function ?
: D ?
Q ?
R that ranks any(document, query) pairs.
This is accomplished bylearning an function over a set of relevant features.Each feature can be modeled as a function that takesthe pair and outputs a real value ?
: D ?
Q ?
R.For example, a term frequency feature outputs thenumber of times the query appears in the document.We define a function ?
= (?1, ?2, ...?n) that takes a(document, query) pair and outputs a vector of fea-tures.
The original ranking function can be writtenas ?
(d, q) = ??(?
(d, q)) where ??
: Rn ?
R is thefunction; i.e.:?
= ??
?
?
(1)Despite the similarities, Object Search differsfrom traditional information retrieval (IR) problemsin many respects.
First, IR can answer only keywordqueries whereas an object query is structured bykeyword constraints as well as numeric constraints.Second, Object Search results are ?focused?, in thesense that they must contain an object, as opposedto the broad notion of relevance in IR.
Finally, sinceobject pages of different domains might have littlein common, we cannot apply the same ranking func-tion for different object domains.As a consequence, in a learning to rank problem,the set of features ?
are fixed for all query.
Themajor concern is learning the function ??.
In ObjectSearch settings, we expect different ?
for each ob-ject domain.
Thus, we have to derive both ?
and?
?.There are a number of challenges in solving theseproblems.
First, we need a deeper understanding of45structured information embedded in web pages.
Inmany cases, an object attribute such as professor?suniversity might appear only once in his homepage.Thus, using a traditional bag-of-words model is of-ten insufficient, because one cannot distinguish theprofessor own university from other university men-tioned in his homepage.
Second, we will need train-ing data to train a new ranking function for eachnew object domain.
Thus, we require an efficientbootstrapping method to tackle this problem.
Fi-nally, any acceptable solution must scale to the sizeof the web.
This requirement poses challenges forefficient query processing and efficient ranking viathe learned ranking function.3 Object Search FrameworkIn this section, we illustrate the primary intuitionsbehind our aproach for an Object Search solu-tion.
We describe its architecture, which servesas a search engine framework to support structuredqueries of any domain.
The technical details of ma-jor components are left for subsequent sections.3.1 IntuitionThe main idea behind our proposed approach is thatwe develop different vertical search engines to sup-port object queries in different domains.
However,we want to keep the cost of supporting each newdomain as small as possible.
The key principles tokeep the cost small are to 1) share as much as pos-sible between search engines of different domainsand 2) automate the process as much as possibleusing machine learning techniques.
To illustrateour proposed approach, we suppose that an user issearching the web for cameras.
Her object query isq = abrand ?
{canon} ?
aprice ?
[0, 200].First, we have to automatically learn a function ?that ranks web pages given an object query as de-scribed in Section 2.
We observe web pages rele-vant to the query and notice several salient featuressuch as ?the word canon appears in the title?, ?theword canon appears near manufacturer?, ?interest-ing words that appear include powershot, eos, ixus?,and ?a price value appears after ?$?
near the wordprice or sale?.
Intuitively, pages containing thesefeatures have a much higher chance of containingthe Canon camera being searched.
Given labeledtraining data, we can learn a ranking function thatcombines these features to produce the probabilityof a page containing the desired camera object.Furthermore, we need to answer user query atquery time.
We need to be able to look up thesefeatures efficiently from our index of the web.
Ana?
?ve method to index the web is to store a list ofweb pages that have the above features, and at querytime, union all pages that have one or more features,aggregate the score for each web page, and returnthe ranked result.
There are three problems with thismethod.
First, these features are dependent on eachobject domain; thus, the size of the index will in-crease as the number of domains grows.
Second,each time a new domain is added, a new set of fea-tures needs to be indexed, and we have to extractfeatures for every single web page again.
Third, wehave to know beforehand the list of camera brands,megapixel ranges, price ranges, etc, which is infea-sible for most object domain.However, we observe that the above query de-pendent features can be computed efficiently froma query independent index.
For example, whether?the word canon appears near manufacturer?
can becomputed if we index all occurrences of the wordscanon and manufacturer.
Similarly, the feature ?theword canon appears in the title?
can be computed ifwe index all the words from web pages?
title, whichonly depends on the web pages themselves.
Sincethe words and numbers from different parts of a webpage can be indexed independently of the object do-main, we can share them across different domains.Thus, we follow the first principle mentioned above.Of course, computing query dependent featuresfrom the domain independent index is more expen-sive than computing it from the na?
?ve index above.However, this cost is scalable to the web.
As a mat-ter of fact, these features are equivalent to ?phrasesearch?
features in modern search engines.Thus, at a high level, we solve the Object Searchproblem by learning a domain dependent rankingfunction for each object domain.
We store basic do-main independent features of the web in our index.At query time, we compute domain dependent fea-tures from this index and apply the ranking functionto return a ranked list of web pages.
In this paper, wefocus on the learning problems, leaving the problemof efficient query processing for future work.46Figure 1: Object Search Architecture3.2 System ArchitectureThe main goal of our Object Search system is to en-able searching the web with object queries.
In orderto do this, the system must address the challengesdescribed in Section 2.
From the end-user?s pointof view, the system must promptly and accuratelyreturn web pages for their object query.
From thedeveloper?s point of view, the system must facilitatebuilding a new search engine to support his objectdomain of interest.
The goal of the architecture is toorchestrate all of these requirements.Figure 1 depicts Object Search architecture.
Itshows how different components of Object Searchinteract with an end-user and a developer.
The end-user can issue any object query of known domains.Each time the system receives an object query fromthe end-user, it translates the query into a domain in-dependent feature query.
Then the Query Processorexecutes the feature query on the inverted index, ag-gregates the features using learned function ?
?, andreturns a ranked list of web pages to the user.The developer?s job is to define his object domainand train a ranking function for it.
He does it byincrementally training the function.
He starts by an-notating a few web pages and running a learning al-gorithm to produce a ranking function, which is thenused to retrieve more data for the developer to anno-tate.
The process iterates until the developer is satis-fied with his trained ranking function for the objectdomain.More specifically, the Ranking Function Learnermodule learns the function ??
and ?
as mentioned inSection 2.
The Query Translator instantiates ?
withuser object query q, resulting in ?(q).
Recall that ?is a set of feature functions ?i.
Each ?i is a functionof a (d, q) pair such as ?term frequency of ak in title?
(ak is an attribute of the object).
Thus we can instan-tiate ?
(q) by replacing ak with ?k, which is part ofthe query q.
For example, if ?k = {canon} in theprevious example, then ?
(q) is ?term frequency ofcanon in title?.
Thus ?
(q) becomes a query indepen-dent feature and ?
(q) becomes a feature query thatcan be executed in our inverted index by the QueryProcessor.4 Learning for Structured RankingWe now describe how we learn the domain depen-dent ranking function ?, which is the core learn-ing aspect of Object Search.
As mentioned in theprevious section, ?
differs from existing learningto rank work due to the structure in object queries.We exploit this structure to decompose the rankingfunction into several components (Section 4.1) andcombine them using a probabilistic model.
Exist-ing learning to rank methods can then be leveragedto rank the individual components.
Section 4.2 de-scribes how we fit individual ranking scores into ourprobabilistic model by calibrating their probability.4.1 Ranking modelAs stated, ?
models the joint probability distribu-tion over the space of documents and queries ?
=P (d, q).
Once estimated, this distribution can rankdocuments inD according to their probability of sat-isfying q.
Since we are only interested in findingsatisfying object pages, we introduce a variable ?which indicates if the document d is an object page.Furthermore, we introduce n variables ?i which in-dicate whether constraint ci in the query q is satis-fied.
The probability computed by ?
is then:P (d, q) = P (?1, .
.
.
, ?n, d)= P (?1, .
.
.
, ?n, d, ?
)+P (?1, .
.
.
, ?n, d, ?
)= P (d)P (?|d)P (?1, .
.
.
, ?n|d, ?
)+P (d)P (?|d)P (?1, .
.
.
, ?n|d, ?
)= P (d)P (?|d)P (?1, .
.
.
, ?n|d, ?)
(2)47' P (?|d)n?i=1P (?i|d, ?)
(3)Equation 2 holds because non-object pages donot satisfy the query, thus, P (?1, .
.
.
, ?n|d, ?)
= 0.Equation 3 holds because we assume a uniform dis-tribution over d and conditional independence over?i given d and ?.Thus, the rest of the problem is estimating P (?|d)and P (?i|d, ?).
The difference between these prob-ability estimates lies in the features we use.
Since ?depends only in d but not q, we use query indepen-dent features.
Similarly, ?i only depends on d andci, thus we use features depending on ci and d.4.2 Calibrating ranking probabilityIn theory, we can use any learning algorithm men-tioned in (Liu, 2009)?s survey to obtain the terms inEquation 3.
In practice, however, such learning al-gorithms often output a ranking score that does notestimate the probability.
Thus, in order to use themin our ranking model, we must transform that rank-ing score into a probability.For empirical purposes, we use the averaged Per-ceptron (Freund and Schapire, 1999) to discrimina-tively train each component of the factored distri-bution independently.
This algorithm requires a setof input vectors, which we obtain by applying therelational feature functions to the paired documentsand queries.
For each constraint ci, we have a fea-ture vector xi = ?i(d, q).
The algorithm produces aweight vector of parameterswi as output.
The prob-ability of ci being satisfied by d given that d containsan object can then be estimated with a sigmoid func-tion as:P (ci|d, ?)
?
P (true|?i(d, q)) ?11 + exp(?wTi xi)(4)Similarly, to estimate P (?|d), we use a fea-ture vector that is dependent only on d. De-noting the function as ?0, we have P (?|d) =P (true|?0(d, q)), which can be obtained from (4).While the sigmoid function has performed wellempirically, probabilities it produces are not cali-brated.
For better calibrated probabilities, one canapply Platt scaling (Platt, 1999).
This method intro-duces two parameters A and B, which can be com-puted using maximum likelihood estimation:P (true|?i(d, q)) ?11 + exp(AwTi ?i(d, q) + B)(5)In contrast to the sigmoid function, Platt scaling canalso be applied to methods that give un-normalizedscores such as RankSVM (Cao et al, 2006).Substituting (4) and (5) into (3), we see that ourfinal learned ranking function has the form?
(d, q) =n?i=01(1 + exp(AiwTi ?i(d, q) + Bi))(6)5 Learning Based ProgrammingLearning plays a crucial role in developing a new ob-ject domain.
In addition to using supervised meth-ods to learn ?, we also exploit active learning to ac-quire training data from unlabeled web pages.
Thecombination of these efforts would benefit from aunified framework and interface to machine learn-ing.
Learning Based Programming (LBP) (Roth,2005) is such a principled framework.
In this sec-tion, we describe how we applied and extended LBPto provide a user friendly interface for the developerto specify features and guide the learning process.Section 5.1 describes how we structured our frame-work around Learning Based Java (LBJ), an instanceof LBP.
Section 5.2 extends the framework to sup-port interactive learning.5.1 Learning Based JavaLBP is a programming paradigm for systems whosebehaviors depend on naturally occurring data andthat require reasoning about data and concepts inways that are hard, if not impossible, to write explic-itly.
This is exactly our situation.
Not only do wenot know how to specify a ranking function for anobject query, we might not even know exactly whatfeatures to use.
Using LBP, we can specify abstractinformation sources that might contribute to deci-sions and apply a learning operator to them, therebyletting a learning algorithm figure out their impor-tances in a data-driven way.Learning Based Java (LBJ) (Rizzolo and Roth,2007) is an implementation of LBP which we usedand extended for our purposes.
The most usefulabstraction in LBJ is that of the feature generation48function (FGF).
This allows the programmer to rea-son in terms of feature types, rather than specifyingindividual features separately, and to treat them asnative building blocks in a language for constructinglearned functions.
For example, instead of specify-ing individual features such as the phrases ?profes-sor of?,?product description?, etc., we can specify ahigher level feature type called ?bigram?, and let analgorithm select individual features for ranking pur-poses.From the programming point of view, LBJ pro-vides a clean interface and abstracts away the te-dium of feature extraction and learning implemen-tations.
This enabled us to build our system quicklyand shorten our development cycle.5.2 Interactive Machine LearningWe advocate an interactive training process (Failsand Olsen, 2003), in which the developer iterativelyimproves the learner via two types of interaction(Algorithm 1).The first type of interaction is similar to activelearning where the learner presents unlabeled in-stances to the developer for annotation which it be-lieves will most positively impact learning.
In rank-ing problems, top ranked documents are presentedas they strongly influence the loss function.
Thesmall difference from traditional active learning inour setting is that the developer assists this processby also providing more queries other than those en-countered in the current training set.The second type of interaction is feature selec-tion.
We observed that feature selection contributedsignificantly in the performance of the learner espe-cially when training data is scarce.
This is becausewith little training data and a huge feature space, thelearner tends to over-fit.
Fortunately in web search,the features used in ranking are in natural languageand thereby intuitive to the developer.
For example,one type of feature used in ranking the universityconstraint of a professor object query is the wordssurrounding the query field as in ?university of ...?or ?...
university?.
If the learner only sees examplesfrom the University of Anystate at Anytown, thenit?s likely that Anytown will have a high weight inaddition to University and of.
However, the Any-town feature will not generalize for documents fromother universities.
Having background knowledgelike this, the developer can unselect such features.Furthermore, the fact that Anytown has a high weightis also an indication that the developer needs to pro-vide more examples of other universities so that thelearner can generalize (the first type of interaction).Algorithm 1 Interactive Learning Algorithm1: The developer uses keyword search to find andannotate an initial training set.2: The system presents a ranked list of featurescomputed from labeled data.3: The developer adds/removes features.4: The system learns the ranking function using se-lected features.5: The developer issues queries and annotates topranked unlabeled documents returned by thesystem.6: If performance is not satisfactory, go to step 2.The iterative algorithm starts with zero trainingdata and continues until the learner?s performancereaches a satisfactory point.
At step 2, the developeris presented with a ranked list of features.
To deter-mine which features played the biggest role in theclassifier?s decision making, we use a simple rank-ing metric called expected entropy loss (Glover etal., 2001).
Let f represent the event that a givenfeature is active.
Let C be the event that the givenexample is classified as true.
The conditional en-tropy of the classification distribution given thatf occurs is H(C|f) ?
?P (C|f) log(P (C|f)) ?P (C|f) log(P (C|f) and similarly, when f does notoccur, we replace f by f .
The expected entropy lossisL(C|f) ?
H(C)?
E[H(C|f)]= H(C)?
(P (f)H(C|f) +P (f)H(C|f) (7)The intuition here is that if the classification losesa lot of entropy when conditioned on a particularfeature, that feature must be very discriminative andcorrelated with the classification itself.It is noted that feature selection plays two impor-tant roles in our framework.
First, it avoids over-fitting when training data is scarce, thus increas-ing the effectiveness of our active learning protocol.Second, since search time depends on how many49domain # pages train testhomepage 22.1 11.1 11laptop 21 10.6 10.4camera 18 9 9random 97.8 48.9 48.8total 158.9 79.6 79.2Table 1: Number of web pages (in thousands) collectedfor experimentfeatures we use to query the web pages, keeping thenumber of features small will ensure that searchingis fast enough to be useful.6 Experimental ResultsIn this section we present an experiment that com-pares Object Search with keyword search engines.6.1 Experimental SettingSince we are the first to tackle this problem of an-swering structured query on the web, there is noknown dataset available for our experiment.
We col-lected the data ourselves using various sources fromthe web.
Then we labeled search results from differ-ent object queries using the same annotation proce-dure described in Section 5.We collected URLs from two main sources: theopen directory (DMOZ) and existing search en-gines (SE).
For DMOZ, we included URLs fromrelevant categories.
For SE, we manually enteredqueries with keywords related to professors?
home-pages, laptops, and digital cameras, and includedall returned URLs.
Having collected the URLs, wecrawled their content and indexed them.
Table 1summarizes web page data we have collected.We split the data randomly into two parts, one fortraining and one for testing, and created a single in-verted index for both of them.
The developer canonly see the training documents to select featuresand train ranking functions.
At testing time, we ran-domly generate object queries, and evaluate on thetesting set.
Since Google?s results come not fromour corpus but the whole web, it might not be fair tocompare against our small corpus.
To accommodatethis, we also added Google?s results into our testingcorpus.
We believe that most ?difficult?
web pagesthat hurt Google?s performance would have been in-Field Keywords ExampleLaptop domainbrand laptop,notebook lenovo laptopprocessor ghz, processor 2.2 ghzprice $, price $1000..1100Professor domainname professor, re-search professor,facultyresearch profes-sor scottuniversity university, uni-versity ofstanforduniversityTable 2: Sample keyword reformulation for Googlecluded in the top Google result.
Thus, they are alsoavailable to test ours.
In the future, we plan to im-plement a local IR engine to compare against oursand conduct a larger scale experiment to compare toGoogle.We evaluated the experiment with two differentdomains: professor and laptop.
We consider home-pages and online shopping pages as object pages forthe professor and laptop domains respectively.For each domain, we generated 5 random objectqueries with different field configurations.
SinceGoogle does not understand structured queries, wereformulated each structured query into a simplekeyword query.
We do so by pairing the query fieldwith several keywords.
For example, a query fieldabrand ?
{lenovo} can be reformulated as ?lenovolaptop?.
We tried different combinations of key-words as shown in table 2.
To deal with numbers,we use Google?s advanced search feature that sup-ports numeric range queries1.
For example, a priceconstraint aprice ?
[100, 200] might be reformulatedas ?price $100..200?.
Since it is too expensive tofind the best keyword formulations for every query,we picked the combination that gives the best resultfor the first Google result page (Top 10 URLs).6.2 ResultWe measure the ranking performance with averageprecision.
Table 3 shows the results for our searchengine (OSE) and Google.
Our ranking functionoutperforms Google for most queries, especially in1A numeric range written as ?100..200?
is treated as a key-word that appears everywhere a number in the range appears50Qry Professor LaptopOSE Google OSE Google1 0.92 (71) 0.90(65) 0.7 (15) 0.44 (12)2 0.83(88) 0.91(73) 0.62 (12) 0.26 (11)3 0.51(73) 0.66(48) 0.44 (40) 0.31 (24)4 0.42(49) 0.3(30) 0.36 (3) 0.09 (1)5 0.91(18) 0.2(16) 0.77 (17) 0.42 (3)Table 3: Average precision for 5 random queries.
Thenumber of positive documents are in bracketsthe laptop domain.
In the professor domain, Googlewins in two queries (?UC Berkeley professor?
and?economics professors?).
This suggests that in cer-tain cases, reformulating to keyword query is a sen-sible approach, especially if all the fields in the ob-ject query are keywords.
Even though Google canbe used to reformulate some queries, it is not clearhow and when this will succeed.
Therefore, we needa principled solution as proposed in this paper.7 Related WorkMany recent works propose methods for supportingstructured queries on unstructured text (Jain et al,2007), (Cafarella et al, 2007), (Gruhl et al, 2004).These works follow a typical extract-then-query ap-proach, which has several problems as we discussedin section 1.
(Agichtein, 2005) proposed using sev-eral large scale techniques.
Their idea of using spe-cialized index and search engine is similar to ourwork.
However those methods assumes that struc-tured data follows some textual patterns whereas oursystem can flexibly handle structured object usingtextual patterns as well as web page features.Interestingly, the approach of translating struc-tured queries to unstructured queries has been stud-ied in (Liu et al, 2006).
The main difference isthat SEMEX relies on carefully hand-tuned heuris-tics on open-domain SQL queries while we use ma-chine learning to do the translation on domain spe-cific queries.Machine Learning approaches to rank documentshave been studied extensively in IR (Liu, 2009).Even though much of existing works can be used torank individual constraints in the structured query.We proposed an effective way to aggregate theseranking scores.
Further more, existing learning torank works assumed a fixed set of features, whereas,the feature set in object search depends on objectdomain.
As we have shown, the effectiveness ofthe ranking function depends much on the set offeatures.
Thus, an semi-automatic method to learnthese was proposed in section 5.Our interactive learning protocol inherits featuresfrom existing works in Active Learning (see (Set-tles, 2009) for a survey).
(Fails and Olsen, 2003)coined the term ?interactive machine learning?
andshowed that a learner can take advantage of user in-teraction to quickly acquire necessary training data.
(Roth and Small, 2009) proposed another interactivelearning protocol that improves upon a relation ex-traction task by incremetally modifying the featurerepresentation.Finally, this work is related to document re-trieval mechanisms used for question answeringtasks (Voorhees, 2001) where precise retrieval meth-ods are necessary to find documents which con-tain specific information for answering factoids(Agichtein et al, 2001).8 ConclusionWe introduces the Object Search framework thatsearches the web for documents containing real-world objects.
We formalized the problem as alearning to rank for IR problem and showed an ef-fective method to solve it.
Our approach goes be-yond the traditional bag-of-words representation andviews each web page as a set of domain independentfeatures.
This representation enabled us to rank webpages with respect to object query.
Our experimentsshowed that, with small human effort, it is possi-ble to create specialized search engines that out-performs GSEs on domain specific queries.
More-over, it is possible to search the web for documentswith deeper meaning, such as those found in objectpages.
Our work is a small step toward semanticsearch engines by handling deeper semantic queries.AcknowledgementThis work is supported by DARPA funding underthe Bootstrap Learning Program, MIAS, a DHS-IDS Center for Multimodal Information Access andSynthesis at UIUC, NSF grant NSF SoD-HCER-0613885 and a grant from Yahoo!
Inc.51ReferencesEugene Agichtein, Steve Lawrence, and Luis Gravano.2001.
Learning search engine specific query trans-formations for question answering.
In WWW ?01:Proceedings of the 10th international conference onWorld Wide Web, pages 169?178, New York, NY,USA.
ACM.Eugene Agichtein.
2005.
Scaling Information Extractionto Large Document Collections.
IEEE Data Eng.
Bull,28:3.Michael Cafarella, Christopher Re, Dan Suciu, and OrenEtzioni.
2007.
Structured Querying of Web Text Data:A Technical Challenge.
In CIDR.Yunbo Cao, Jun Xu, Tie-Yan Liu, Hang Li, Yalou Huang,and Hsiao-Wuen Hon.
2006.
Adapting Ranking SVMto Document Retrieval.
In SIGIR ?06: Proceedings ofthe 29th annual international ACM SIGIR conferenceon Research and development in information retrieval,pages 186?193, New York, NY, USA.
ACM.Jerry Alan Fails and Dan R. Olsen, Jr. 2003.
Interactivemachine learning.
In IUI ?03: Proceedings of the 8thinternational conference on Intelligent user interfaces,pages 39?45, New York, NY, USA.
ACM.Yoav Freund and Robert E. Schapire.
1999.
Large Mar-gin Classification Using the Perceptron Algorithm.Machine Learning, 37(3):277?296.Eric J. Glover, Gary W. Flake, Steve Lawrence, AndriesKruger, David M. Pennock, William P. Birmingham,and C. Lee Giles.
2001.
Improving Category SpecificWeb Search by Learning Query Modifications.
Ap-plications and the Internet, IEEE/IPSJ InternationalSymposium on, 0:23.D.
Gruhl, L. Chavet, D. Gibson, J. Meyer, P. Pattanayak,A.
Tomkins, and J. Zien.
2004.
How to Build a Web-Fountain: An Architecture for Very Large Scale TextAnalytics.
IBM Systems Journal.A.
Jain, A. Doan, and L. Gravano.
2007.
SQL QueriesOver Unstructured Text Databases.
In Data Engineer-ing, 2007.
ICDE 2007.
IEEE 23rd International Con-ference on, pages 1255?1257.N.
Kushmerick, D. Weld, and R. Doorenbos.
1997.Wrapper Induction for Information Extraction.
In IJ-CAI, pages 729?737.Jing Liu, Xin Dong, and Alon Halevy.
2006.
AnsweringStructured Queries on Unstructured Data.
In WebDB.Tie-Yan Liu.
2009.
Learning to Rank for InformationRetrieval.
Found.
Trends Inf.
Retr., 3(3):225?331.Andrew Kachites McCallum, Kamal Nigam, Jason Ren-nie, and Kristie Seymore.
2000.
Automating the Con-struction of Internet Portals with Machine Learning.Information Retrieval, 3(2):127?163.J.
Platt.
1999.
Probabilistic outputs for support vec-tor machines and comparison to regularized likelihoodmethods.
In In Advances in Large Margin Classifiers.MIT Press.N.
Rizzolo and D. Roth.
2007.
Modeling DiscriminativeGlobal Inference.
In Proceedings of the First Inter-national Conference on Semantic Computing (ICSC),pages 597?604, Irvine, California, September.
IEEE.Dan Roth and Kevin Small.
2009.
Interactive featurespace construction using semantic information.
InCoNLL ?09: Proceedings of the Thirteenth Conferenceon Computational Natural Language Learning, pages66?74, Morristown, NJ, USA.
Association for Com-putational Linguistics.Dan Roth.
2005.
Learning Based Programming.
Innova-tions in Machine Learning: Theory and Applications.Burr Settles.
2009.
Active learning literature survey.Computer Sciences Technical Report 1648, Universityof Wisconsin-Madison.Ellen M. Voorhees.
2001.
The trec question answeringtrack.
Nat.
Lang.
Eng., 7(4):361?378.52
