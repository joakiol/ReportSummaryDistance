Errors in wikis: new challenges and new opportunities ?
a discussiondocumentAnn CopestakeComputer LaboratoryUniversity of Cambridgeaac@cl.cam.ac.ukAbstractThis discussion document concerns thechallenges to assessments of reliabilityposed by wikis and the potential for lan-guage processing techniques for aidingreaders to decide whether to trust partic-ular text.1 Wikis and the trust problemWikis, especially open wikis, pose new challengesfor readers in deciding whether information istrustworthy.
An article in a wikipedia may begenerally well-written and appear authoritative, sothat the reader is inclined to trust it, but have someadditions by other authors which are incorrect.Corrections may eventually get made, but therewill be a time lag.
In particular, many people arenow using Wikipedia (www.wikipedia.org)as a major reference source, so the potential formisinformation to be spread is increasing.
Ithas already become apparent that articles aboutpoliticians are being edited by their staff to makethem more favourable and no doubt various inter-est groups are manipulating information in moresubtle ways.
In fact, as wikis develop, problemswith reliability may get worse: authors who wrotean article several years ago won?t care so muchabout its content and may not bother to check ed-its.
When obscure topics are covered by a wiki,the community which is capable of checking factsmay be small.Of course errors arise in old text too, but agenerally authoritative conventional article is un-likely to contain a really major error about a cen-tral topic.
Different old text publications havedifferent perspectives, political or otherwise, butthe overall slant is usually generally known andhence not problematic.
Non-wiki web pages mayhave unknown authors, but the domain offers someguide to reliability and to likely skew and thepages can be assessed as a whole.
The issue hereis not the overall number of errors in wikis ver-sus published text or web pages, but how a readercan decide to trust a particular piece of informa-tion when they cannot use the article as a whole asa guide.There is a need for automatic tools which couldprovide an aid for the reader who needs to assesstrustworthiness and also for authors and modera-tors scanning changes.
Similarly, moderators needtools for identification of vandalism, libel, adver-tising and so on.Questions:1.
Is wiki reliability really a problem for read-ers, as I hypothesise?
Perhaps readers whoare not expert in a topic can detect problem-atic material in a wiki article, despite the mul-tiple authorship.2.
Can we use language processing tools to helpreaders identify errors and misinformation inwiki pages?2 Learning trustworthinessThe availability of change histories on wikis isa resource which could be exploited for train-ing purposes by language processing systemsdesigned to evaluate trustworthiness.
If it ispossible to categorise users as trustworthy ornon-trustworthy/unknown by independent criteria(such as overall contribution level), then we canuse changes made by trustworthy users that deleteadditions made by the unknown users as a meansof categorising some text as bad.
(Possibly the9comments made by the editors could lead to sub-categorization of the badness as error vs vandalismetc.)
A tool for highlighting possible problem ed-its in wikis might thus be developed on the basisof a large amount of training data.
Techniques de-rived from areas such as language-based spam de-tection, subjectivity measurement and so on couldbe relevant.
However, one of the relatively novelaspects of the wiki problem is that we are look-ing at categorisation of small text snippets ratherthan larger quantities of text.
Thus techniques thatrely on stylistic cues probably won?t work.
Ide-ally, we need to be able to identify the actual in-formation provided by individual contributors andclassify this as reliable or unreliable.
One way oflooking at this is by dividing text into factoids (inthe summarisation sense).
Factoid identification isa really hard problem, but maybe the wiki editsthemselves could help here.Questions:1.
Can we automatically classify wiki contribu-tors as reliable/unreliable?2.
Do trustworthy users?
edits provide goodtraining data?3.
Are there any features of text snippets that al-low classification of reliability?
(My guess:identification of vandalism will be possiblebut more subtle effects won?t be detectable.)4.
What tools could be adapted from other ar-eas of language processing to address theseissues?3 An ontology of errors?As an extension of the ideas in the previous sec-tion, perhaps wiki histories could be mined as arepository of commonly believed false informa-tion.
For instance, the EN wikipedia entry forUniversity of Cambridge currently (Jan 5th, 2006)states:Undergraduate admission to Cambridgecolleges used to depend on knowledgeof Latin and Ancient Greek, subjectstaught principally in the United King-dom at fee-paying schools, called publicschools.
(?public schools?
was linked)One way in which this is wrong is that British?public schools?
(in this sense) are only a smallproportion of the fee-paying schools, but equat-ing public schools with all fee-paying schools is acommon error.
Suppose a trustworthy editor cor-rects this particular error in this article (and per-haps similar errors in the same or other articles).
Ifwe can automatically analyse and store the correc-tion, we could use it to check for the same error inother text.
As wikis get larger, this might becomea useful resource for error detection/evaluation ofmany text types.
Thus errors in wikis are an op-portunity as well as a challenge.10
