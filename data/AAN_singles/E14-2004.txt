Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 13?16,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsSemantic Annotation, Analysis and Comparison:A Multilingual and Cross-lingual Text Analytics ToolkitLei ZhangInstitute AIFBKarlsruhe Institute of Technology76128 Karlsruhe, Germanyl.zhang@kit.eduAchim RettingerInstitute AIFBKarlsruhe Institute of Technology76128 Karlsruhe, Germanyrettinger@kit.eduAbstractWithin the context of globalization,multilinguality and cross-linguality forinformation access have emerged as issuesof major interest.
In order to achievethe goal that users from all countrieshave access to the same information,there is an impending need for systemsthat can help in overcoming languagebarriers by facilitating multilingual andcross-lingual access to data.
In thispaper, we demonstrate such a toolkit,which supports both service-oriented anduser-oriented interfaces for semanticallyannotating, analyzing and comparingmultilingual texts across the boundariesof languages.
We conducted an extensiveuser study that shows that our toolkitallows users to solve cross-lingual entitytracking and article matching tasks moreefficiently and with higher accuracycompared to the baseline approach.1 IntroductionAutomatic text understanding has been anunsolved research problem for many years.
Thispartially results from the dynamic and divergingnature of human languages, which results in manydifferent varieties of natural language.
Thesevariations range from the individual level, toregional and social dialects, and up to seeminglyseparate languages and language families.In recent years there have been considerableachievements in approaches to computationallinguistics exploiting the information acrosslanguages.
This progress in multilingual andcross-lingual text analytics is largely dueto the increased availability of multilingualknowledge bases such as Wikipedia, which helpsat scaling the traditionally monolingual tasksto multilingual and cross-lingual applications.From the application side, there is a clear needfor multilingual and cross-lingual text analyticstechnologies and services.Text analytics in this work is defined asthree tasks: (i) semantic annotation by linkingentity mentions in the documents to theircorresponding representations in the knowledgebase; (ii) semantic analysis by linking thedocuments by topics to the relevant resources inthe knowledge base; (iii) semantic comparisonby measuring semantic relatedness betweendocuments.
While multilingual text analyticsaddresses these tasks for multiple languages,cross-lingual text analytics goes one step beyond,as it faces these tasks across the boundaries oflanguages, i.e., the text to be processed and theresources in the knowledge base, or the documentsto be compared, are in different languages.Due to the ever growing richness of itscontent, Wikipedia has been increasingly gainingattention as a precious knowledge base thatcontains an enormous number of entities andtopics in diverse domains.
In addition, Wikipediapages that provide information about the sameconcept in different languages are connectedthrough cross-language links.
Therefore, we useWikipedia as the central knowledge base.With the goal of overcoming language barriers,we would like to demonstrate our multilingualand cross-lingual text analytics toolkit, whichsupports both service-oriented and user-orientedinterfaces for semantically annotating, analyzingand comparing multilingual texts across theboundaries of languages.2 TechniquesIn this section, we first present the techniquesbehind our toolkit w.r.t.
its three components:semantic annotation (Sec.
2.1), semantic analysisand semantic comparison (Sec.
2.2).132.1 Wikipedia-based AnnotationThe process of augmenting phrases in text withlinks to their corresponding Wikipedia articles(in the sense of Wikipedia-based annotation) isknown as wikification.
There is a large bodyof work that links phrases in unstructured textto relevant Wikipedia articles.
While Mihalceaand Csomai (Mihalcea and Csomai, 2007) metthe challenge of wikification by using linkprobabilities obtained from Wikipedia?s articlesand by a comparison of features extracted from thecontext of the phrases, Milne and Witten (Milneand Witten, 2008) could improve the wikificationservice significantly by viewing wikification evenmore as a supervised machine learning task:Wikipedia is used here not only as a source ofinformation to point to, but also as training datato find always the appropriate link.For multilingual semantic annotation, weadopted the wikification system in (Milne andWitten, 2008) and trained it for each languageusing the corresponding Wikipedia version.
Toperform cross-lingual semantic annotation, weextended the wikification system by making use ofthe cross-language links in Wikipedia to find thecorresponding Wikipedia articles in the differenttarget languages.
More details can be found in ourprevious work (Zhang et al., 2013).2.2 Explicit Semantic AnalysisExplicit Semantic Analysis (ESA) has beenproposed as an approach for semantic modelingof natural language text (Gabrilovich andMarkovitch, 2006).
Based on a given set ofconcepts with textual descriptions, ESA definesthe concept-based representation of documents.Various sources for concept definitions havebeen used, such as Wikipedia and Reuters Corpus.Using the concept-based document representation,ESA has been successfully applied to computesemantic relatedness between texts (Gabrilovichand Markovitch, 2007).
In the context of thecross-language information retrieval (CLIR) task,ESA has been extended to a cross-lingual setting(CL-ESA) by mapping the semantic documentrepresentation from a concept space of onelanguage to an interlingual concept space (Sorgand Cimiano, 2008).The semantic analysis and semantic comparisoncomponents of our toolkit are based on CL-ESAin (Sorg and Cimiano, 2008).
The semanticFigure 2: Architecture of our Toolkit.analysis component takes as input a document in asource language and maps it to a high-dimensionalvector in the interlingual concept space, suchthat each dimension corresponds to an Wikipediaarticle in any target language acting as aconcept.
For semantic comparison, the documentsin different languages are first translated intovectors in the interlingual concept space and thenthe cross-lingual semantic relatedness betweenthe documents in different languages can becalculated using the standard similarity measurebetween the resulting vectors.3 ImplementationOur multilingual and cross-lingual toolkit isimplemented using a client-server architecturewith communication over HTTP using a XMLschema defined in XLike project1.
The serveris a RESTful web service and the client userinterface is implemented using Adobe Flex asboth Desktop and Web Applications.
Thetoolkit can easily be extended or adapted toswitch out the server or client.
In this way, itsupports both service-oriented and user-orientedinterfaces for semantically annotating, analyzingand comparing multilingual texts across theboundaries of languages.
The architecture of ourtoolkit is shown in Figure 2.For all three components, namely semanticannotation, analysis and comparison, we useWikipedia as the central knowledge base.
Table 1shows the statistics of the Wikipedia articles inEnglish, German, Spanish and French as well asthe cross-language links between the them in theselanguages extracted from Wikipedia snapshots ofMay 20122, which are used to build our toolkit.We now describe the user interfaces of these1http://www.xlike.org/2http://dumps.wikimedia.org/14Figure 1: Screenshot of the Semantic Annotation Component of our Toolkit.English (EN) German (DE) Spanish (ES) French (FR)#Articles 4,014,643 1,438,325 896,691 1,234,567(a) Number of articles.EN-DE EN-ES EN-FR DE-ES DE-FR ES-FR#Links (?)
721,878 568,210 779,363 295,415 455,829 378,052#Links (?)
718,401 581,978 777,798 302,502 457,306 370,552#Links (merged) 722,069 593,571 795,340 307,130 464,628 383,851(b) Number of cross-language links.Table 1: Statistics about Wikipedia.components.
Due to the lack of space, we onlyshow the screenshot of the semantic annotationcomponent in Figure 1.
The semantic annotationcomponent allows the users to find the entitiesin Wikipedia mentioned in the input document.Given the input document in one language, theusers can select the output language, namelythe language of Wikipedia articles describingthe mentioned entities.
In the left pie chart,the users can see the percentage of Wikipediaarticles in different languages as annotations of theinput document.
According to their weights, theWikipedia articles in each language are organizedin 3 relevance categories: high, medium and low.In the middle bar chart, the number of Wikipediaarticles in each language and in each categoryis illustrated.
The right data grid provides theWikipedia article titles with their weights in theoutput language and the mentions in the inputdocument.
Clicking an individual title opensthe corresponding Wikipedia article in the outputlanguage.
The semantic analysis componenthas the similar user interface as the semanticannotation component.
The difference is that theWikipedia articles listed in the right data grid aretopically relevant to the input documents insteadof being mentioned as entities.
Regarding the userinterface of semantic comparison component, themain inputs are two documents that might be indifferent languages and the output is the semanticrelatedness between them.4 User StudyWe conducted a task-based user study and thegoal is to assess the effectiveness and usability ofour multilingual and cross-lingual text analyticstoolkit.
We design two tasks reflecting the real-lifeinformation needs, namely entity tracking andarticle matching, to assess the functionality ofour toolkit from different perspectives.
The entitytracking task is to detect mentions of the givenentities in the articles, where the descriptionsof the entities and the articles are in differentlanguages.
Given articles in one language ascontext, the article matching task is to find themost similar articles in another language.The participants of our user study are 16volunteers and each of them got both tasks, whichthey had to solve in two ways: (1) using a majoronline machine translation service as baselineand (2) using our multilingual and cross-lingualtext analytics toolkit with all the functionality.For each task, we randomly selected 10 parallelarticles in English, French and Spanish from theJRC-Acquis parallel corpus3.
After a survey,3http://langtech.jrc.it/JRC-Acquis.html15(a) Avg.
successrate per task / method(b) Avg.
time spent per task / methodFigure 3: Evaluation Results of the User Study.we decided to provide the entity descriptions forentity tracking task and the context documentsfor article matching task in English, which allparticipants can speak.
Regarding the articles tobe processed, we set up the tasks using Spanisharticles for the participants who do not knowSpanish, and tasks with French articles for theparticipants who cannot speak French.To measure the overall effectiveness of ourtoolkit, we have analysed the ratio of tasks thatwere completed successfully and correctly and thetime the participants required for the tasks.
Theaverage success rate and time spent per task andper method are illustrated in Figure 3.
For entitytracking task, we observe that a success rate of80% was achieved using our toolkit in comparisonwith the success rate of 70% yielded by using thebaseline.
In addition, there is a significant gapbetween the time spent using different methods.While it took 21.5 minutes on average to solvethe task using the baseline, only 6.75 minuteswere needed when using our toolkit.
Regardingthe article matching task, both methods performedvery well.
Using our toolkit obtained a slightlyhigher success rate of 99% than 94% using thebaseline.
The time spent using both methods is notso different.
The participants spent 15.75 minuteson average using the baseline while 2 minutes lesswere needed using our toolkit.In terms of the user study, our toolkit ismore effective than the baseline for both entitytracking and article matching tasks.
Therefore,we conclude that our toolkit provides usefulfunctionality to make searching entities, analyzingand comparing articles more easily and accuratelyin the multilingual and cross-lingual scenarios.AcknowledgmentsThis work was supported by the EuropeanCommunity?s Seventh Framework ProgrammeFP7-ICT-2011-7 (XLike, Grant 288342).References[Gabrilovich and Markovitch2006] EvgeniyGabrilovich and Shaul Markovitch.
2006.Overcoming the Brittleness Bottleneck usingWikipedia: Enhancing Text Categorization withEncyclopedic Knowledge.
In AAAI, pages1301?1306.
[Gabrilovich and Markovitch2007] EvgeniyGabrilovich and Shaul Markovitch.
2007.Computing semantic relatedness usingwikipedia-based explicit semantic analysis.
InProceedings of the 20th international jointconference on artificial intelligence, volume 6,page 12.
[Mihalcea and Csomai2007] Rada Mihalcea andAndras Csomai.
2007.
Wikify!
: linking documentsto encyclopedic knowledge.
In In CIKM ?07:Proceedings of the sixteenth ACM conferenceon Conference on information and knowledgemanagement, pages 233?242.
ACM.
[Milne and Witten2008] David Milne and Ian H.Witten.
2008.
Learning to link with wikipedia.In Proceedings of the 17th ACM conference onInformation and knowledge management, CIKM?08, pages 509?518, New York, NY, USA.
ACM.
[Sorg and Cimiano2008] P. Sorg and P. Cimiano.
2008.Cross-lingual Information Retrieval with ExplicitSemantic Analysis.
In Working Notes of the AnnualCLEF Meeting.
[Zhang et al.2013] Lei Zhang, Achim Rettinger,Michael Frber, and Marko Tadic.
2013.
Acomparative evaluation of cross-lingual textannotation techniques.
In CLEF, pages 124?135.16
