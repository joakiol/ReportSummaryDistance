A Resource-light Approach to Russian Morphology: Tagging Russian usingCzech resourcesJiri Hana and Anna Feldman and Chris BrewDepartment of LinguisticsOhio State UniversityColumbus, OH 43210AbstractIn this paper, we describe a resource-light systemfor the automatic morphological analysis and tag-ging of Russian.
We eschew the use of extensiveresources (particularly, large annotated corpora andlexicons), exploiting instead (i) pre-existing anno-tated corpora of Czech; (ii) an unannotated corpusof Russian.
We show that our approach has benefits,and present what we believe to be one of the first fullevaluations of a Russian tagger in the openly avail-able literature.1 IntroductionMorphological processing and part-of-speech tag-ging are essential for many NLP tasks, includingmachine translation, information retrieval and pars-ing.
In this paper, we describe a resource-light ap-proach to the tagging of Russian.
Because Russianis a highly inflected language with a high degreeof morpheme homonymy (cf.
Table 11) the tags in-volved are more numerous and elaborate than thosetypically used for English.
This complicates the tag-ging task, although as has been previously noted(Elworthy, 1995), the increased complexity of thetags does not necessarily translate into a more de-manding tagging task.
Because no large annotatedcorpora of Russian are available to us, we insteadchose to use an annotated corpus of Czech.
Czechis sufficiently similar to Russian that it is reasonableto suppose that information about Czech will be rel-evant in some way to the tagging of Russian.The languages share many linguistic properties (freeword order and a rich morphology which playsa considerable role in determining agreement andargument relationships).
We created a morpho-logical analyzer for Russian, combined the resultswith information derived from Czech and used theTnT (Brants, 2000) tagger in a number of differ-1All Russian examples in this paper are transcribed in theRoman alphabet.
Our system is able to analyze Russian textsin both Cyrillic and various transcriptions.krasiv-a beautiful (short adjective, feminine)muz?-a husband (noun, masc., sing., genitive)husband (noun, masc., sing., accusative)okn-a window (noun, neuter, sing., genitive)window (noun, neuter, pl., nominative)window (noun, neuter, pl., accusative)knig-a book (noun, fem., sing., nominative)dom-a house (noun, masc., sing., genitive)house (noun, masc., pl., nominative)house (noun, masc., pl., accusative)skazal-a say (verb, fem., sing., past tense)dv-a two (numeral, masc., nominative)Table 1: Homonymy of the a endingent ways, including a a committee-based approach,which turned out to give the best results.
To eval-uate the results, we morphologically annotated (byhand) a small corpus of Russian: part of the transla-tion of Orwell?s ?1984?
from the MULTEXT-EASTproject (Ve?ronis, 1996).2 Why TnT?Readers may wonder why we chose to use TnT,which was not designed for Slavic languages.
Theshort answer is that it is convenient and successful,but the following two sections address the issue inrather more detail.2.1 The encoding of lexical information in TnTTnT records some lexical information in the emis-sion probabilities of its second order MarkovModel.
Since Russian and Czech do not use thesame words we cannot use this information (at leastnot directly) to tag Russian.
Given this, the movefrom Czech to Russian involves a loss of detailedlexical information.
Therefore we implemented amorphological analyzer for Russian, the output ofwhich we use to provide surrogate emission proba-bilities for the TnT tagger (Brants, 2000).
The de-tails are described below in section 4.2.2.2 The modelling of word order in TnTBoth Russian and Czech have relatively free wordorder, so it may seem an odd choice to use a Markovmodel (MM) tagger.
Why should second orderMM be able to capture useful facts about such lan-guages?
Firstly, even if a language has the poten-tial for free word order, it may still turn out thatthere are recurring patterns in the progressions ofparts-of-speech attested in a training corpus.
Sec-ondly, n-gram models including MM have indeedbeen shown to be successful for various Slavic lan-guages, e.g., Czech (Hajic?
et al, 2001) or Slovene(Dz?eroski et al, 2000); although not as much asfor English.
This shows that the transitional in-formation captured by the second-order MM froma Czech or Slovene corpus is useful for Czech orSlovene.2 The present paper shows that transitionalinformation acquired from Czech is also useful forRussian.3 Russian versus CzechA deep comparative analysis of Czech and Russianis far beyond the scope of this paper.
However, wewould like to mention just a number of the most im-portant facts.
Both languages are Slavic (Czech isWest Slavonic, Russian is East Slavonic).
Both haveextensive morphology whose role is important indetermining the grammatical functions of phrases.In both languages, the main verb agrees in personand number with subject; adjectives agree in gen-der, number and case with nouns.
Both languagesare free constituent order languages.
The word or-der in a sentence is determined mainly by discourse.It turns out that the word order in Czech and Russianis very similar.
For instance, old information mostlyprecedes new information.
The ?neutral?
order inthe two languages is Subject-Verb-Object.
Here is aparallel Czech-Russian example from our develop-ment corpus:(1) a.
[Czech]BylwasMasc.Pastjasny?,brightMasc.Sg.Nomstudeny?coldMasc.Sg.Nomdubnovy?AprilMasc.Sg.NomdendayMasc.Sg.NomiandhodinyclocksFem.P l.Nomodb?
?jelystrokeFem.P l.Pasttr?ina?ctou.thirteenthFem.Sg.Accb.
[Russian]2Respectively, and if the techniques in the present papergeneralize, probably also irrespectively.BylwasMasc.Pastjasnyj,brightMasc.Sg.NomxolodnyjcoldMasc.Sg.Nomaprel?skijAprilMasc.Sg.Nomden?dayMasc.Sg.Nomiandc?asyclocksPl.NomprobilistrokePl.Pasttrinadtsat?.thirteenAcc?It was a bright cold day in April, and theclocks were striking thirteen.?
[from Orwell?s?1984?
]Of course, not all utterances are so similar.
Sec-tion 5.4 briefly mentions how to improve the utilityof the corpus by eradicating some of the systematicdifferences.4 Realization4.1 The tag systemWe adopted the Czech tag system (Hajic?, 2000) forRussian.
Every tag is represented as a string of 15symbols each corresponding to one morphologicalcategory.
For example, the word vidjela is assignedthe tag VpFS- - -XR-AA- - -, because it is a verb (V),past participle (p), feminine (F), singular (S), doesnot distinguish case (-), possessive gender (-), pos-sessive number (-), can be any person (X), is pasttense (R), is not gradable (-), affirmative (A), activevoice (A), and does not have any stylistic variants(the final hyphen).No.
Description Abbr.
No.
of valuesCz Ru1 POS P 12 122 SubPOS ?
detailed POS S 75 323 Gender g 11 54 Number n 6 45 Case c 9 86 Possessor?s Gender G 5 47 Possessor?s Number N 3 38 Person p 5 59 Tense t 5 510 Degree of comparison d 4 411 Negation a 3 312 Voice v 3 313 Unused 1 114 Unused 1 115 Variant, Style V 10 2Table 2: Overview and comparison of the tagsetsThe tagset used for Czech (4290+ tags) is largerthan the tagset we use for Russian (about 900 tags).There is a good theoretical reason for this choice?
Russian morphological categories usually havefewer values (e.g., 6 cases in Russian vs. 7 in Czech;Czech often has formal and colloquial variants ofthe same morpheme); but there is also an immedi-ate practical reason ?
the Czech tag system is veryelaborate and specifically devised to serve multipleneeds, while our tagset is designed solely to capturethe core of Russian morphology, as we need it forour primary purpose of demonstrating the portabil-ity and feasibility of our technique.
Still, our tagsetis much larger than the Penn Treebank tagset, whichuses only 36 non-punctuation tags (Marcus et al,1993).4.2 Morphological analysisIn this section we describe our approach to aresource-light encoding of salient facts about theRussian lexicon.
Our techniques are not as rad-ical as previously explored unsupervised methods(Goldsmith, 2001; Yarowsky and Wicentowski,2000), but are designed to be feasible for languagesfor which serious morphological expertise is un-available to us.
We use a paradigm-based morphol-ogy that avoids the need to explicitly create a largelexicon.
The price that we pay for this is overgener-ation.
Most of these analyses look very implausibleto a Russian speaker, but significantly increasing theprecision would be at the cost of greater develop-ment time than our resource-light approach is ableto commit.
We wish our work to be portable at leastto other Slavic languages, for which we assume thatelaborate morphological analyzers will not be avail-able.
We do use two simple pre-processing methodsto decrease the ambiguity of the results handed tothe tagger ?
longest ending filtering and an automat-ically acquired lexicon of stems.
These were easy toimplement and surprisingly effective.Our analyzer captures just a few textbook factsabout the Russian morphology (Wade, 1992), ex-cluding the majority of exceptions and including in-formation about 4 declension classes of nouns, 3conjugation classes of verbs.
In total our databasecontains 80 paradigms.
A paradigm is a set of end-ings and POS tags that can go with a particular setof stems.
Thus, for example, the paradigm in Table3 is a set of inflections that go with the masculinestems ending on the ?hard?
consonants, e.g., slon?elephant?, stol ?table?.Unlike the traditional notions of stem and ending,for us a stem is the part of the word that does notchange within its paradigm, and the ending is thepart of the word that follows such a stem.
For ex-ample, the forms of the verb moc??
?can.INF?
: mogu?1sg?, moz?es??
?2sg?, moz?et ?3sg?, etc.
are analyzed as0 NNMS1 - - - - - - - - - - y NNMP1 - - - - - - - - - -a NNMS2 - - - - - - - - - - ov NNMP2 - - - - - - - - - -u NNMS3 - - - - - - - - - - am NNMP3 - - - - - - - - - -a NNMS4 - - - - - - - - - - ov NNMP4 - - - - - - - - - -u NNMS4 - - - - - - - - - 1e NNMS6 - - - - - - - - - - ax NNMP6 - - - - - - - - - -u NNMS6 - - - - - - - - - 1om NNMS7 - - - - - - - - - - ami NNMP7 - - - - - - - - - -Table 3: A paradigm for ?hard?
consonant mascu-line nounsthe stem mo followed by the endings gu, z?es?
?, z?et.
Amore linguistically oriented analysis would involvethe endings u, es?
?, et and phonological alternationsin the stem.
All stem internal variations are treatedas suppletion.3Unlike the morphological analyzers that exist forRussian (Segalovich and Titov, 2000; Segalovich,2003; Segalovich and Maslov, 1989; Kovalev, 2002;Mikheev and Liubushkina, 1995; Yablonsky, 1999;Segalovich, 2003; Kovalev, 2002, among others)(Segalovich, 2003; Kovalev, 2002; Mikheev and Li-ubushkina, 1995; Yablonsky, 1999, among others),our analyzer does not rely on a substantial manu-ally created lexicon.
This is in keeping with our aimof being resource-light.
When analyzing a word,the system first checks a list of monomorphemicclosed-class words and then segments the word intoall possible prefix-stem-ending triples.4 The resulthas quite good coverage (95.4%), but the averageambiguity is very high (10.9 tags/token), and evenhigher for open class words.
We therefore have twostrategies for reducing ambiguity.4.2.1 Longest ending filtering (LEF)The first approach to ambiguity reduction is basedon a simple heuristic ?
the correct ending is usuallyone of the longest candidate endings.
In English, itwould mean that if a word is analyzed either as hav-ing a zero ending or an -ing ending, we would con-sider only the latter; obviously, in the vast majorityof cases that would be the correct analysis.
In addi-tion, we specify that a few long but very rare end-ings should not be included in the maximum lengthcalculation (e.g., 2nd person pl.
imperative).3We do in fact have a very similar analysis, the analyzer?srun-time representation of the paradigms is automatically pro-duced from a more compact and linguistically attractive spec-ification of the paradigms.
It is possible to specify the ba-sic paradigms and then specify the subparadigms, exceptionsand paradigms involving phonological changes by referring tothem.4Currently, we consider only two inflectional prefixes ?
neg-ative ne and superlative nai.4.2.2 Deriving a lexiconThe second approach uses a large raw corpus5 togenerate an open class lexicon of possible stemswith their paradigms.
In this paper, we can onlysketch the method, for more details see (Hana andFeldman, to appear).
It is based on the idea thatopen-class lemmata are likely to occur in more thanone form.
First, we run the morphological analyzeron the text (without any filtering), then we add tothe lexicon those entries that occurred with at least acertain number of distinct forms and cover the high-est number of forms.
If we encounter the word talk-ing, using the information about paradigms, we canassume that it is either the -ing form of the lemmatalk or that it is a monomorphemic word (such assibling).
Based on this single form we cannot reallysay more.
However, if we also encounter the formstalk, talks and talked, the former analysis seemsmore probable; and therefore, it seems reasonableto include the lemma talk as a verb into the lexi-con.
If we encountered also talkings, talkinged andtalkinging, we would include both lemmata talk andtalking as verbs.Obviously, morphological analysis based on sucha lexicon overgenerates, but it overgenerates muchless than if based on the endings alone.
For ex-ample, for the word form partii of the lemma par-tija ?party?, our analysis gives 8 possibilities ?
the5 correct ones (noun fem sg gen/dat/loc sg and plnom/acc) and 3 incorrect ones (noun masc sg loc,pl nom, and noun neut pl acc; note that only gen-der is incorrect).
Analysis based on endings alonewould allow 20 possibilities ?
15 of them incorrect(including adjectives and an imperative).4.3 TaggingWe use the TnT tagger (Brants, 2000), an imple-mentation of the Viterbi algorithm for second orderMarkov models.
We train the transition probabili-ties on Czech (1.5M tokens of the Prague Depen-dency Treebank (Be?mova?
et al, 1999)).
We ob-tain surrogate emission probabilities by running ourmorphological analyzer, then assuming a uniformdistribution over the resulting emissions.5 Experiments5.1 CorporaFor evaluation purposes, we selected and morpho-logically annotated (by hand) a small portion from5We used The Uppsala Russian Corpus (1M tokens), whichis freely available from Uppsala University at http://www.slaviska.uu.se/ryska/corpus.html.the Russian translation of Orwell?s ?1984?.
This cor-pus contains 4011 tokens and 1858 types.
For devel-opment, we used another part of ?1984?.
Since wewant to work with minimal language resources, thedevelopment corpus is intentionally small ?
1788 to-kens.
We used it to test our hypotheses and tune theparameters of our tools.In the following sections, we discuss our experi-ments and report the results.
Note that we do notreport the results for tag position 13 and 14, sincethese positions are unused; and therefore, alwaystrivially correct.5.2 Morphological analysisAs can be seen from Table 4, morphological anal-ysis without any filters gives good recall (althoughon a non-fiction text it would probably be lower),but also very high average ambiguity.
Both fil-ters (the longest-ending filter and automatically ac-quired lexicon) reduce the ambiguity significantly;the former producing a considerable drop of recall,the latter retaining high recall.
However, we do bestif we first attempt lexical lookup, then apply LEFto the words not found.
This keeps recall reason-ably high at the same time as decreasing ambiguity.As expected, performance increases with the size ofthe unannotated Russian corpus used to generate thelexicon.
All subsequent experimental results wereobtained using this best filter combination, i.e., thecombination of the lexicon based on the 1Mwordcorpus and LEF.LEF no no no yes yes yesLexicon based on 0 100K 1M 0 100K 1Mrecall 95.4 94 93.1 84.4 88.3 90.4avg ambig (tag/word) 10.9 7.0 4.7 4.1 3.5 3.1Tagging ?
accuracy 50.7 62.1 67.5 62.1 66.8 69.4Table 4: Morph.
analysis with various parameters5.3 TaggingTable 7 summarizes the results of our taggers on testdata.
Our baseline is produced by the morphologi-cal analyzer without any filters followed by a taggerrandomly selecting a tag among the tags offered bythe morphological analyzer.
The direct-full tag col-umn shows the result of the TNT tagger with transi-tion probabilities obtained directly from the Czechcorpus and the emission symbols based on the mor-phological analyzer with the best filters.To further improve the results, we used two tech-niques: (i) we modified the training corpus to re-move some systematic differences between Czechand Russian (5.4); (ii) we trained batteries of tag-gers on subtags to address the data sparsity problem(5.5 and 5.6).5.4 RussificationWe experimented with ?russified?
models.
Wetrained the TnT tagger on the Czech corpus withmodifications that made the structure of trainingdata look more like Russian.
For example, pluraladjectives and participles in Russian, unlike Czech,do not distinguish gender.
(2) a.
Nadan?
?Giftedmasc.plmuz?imensoutez?ili.competedmasc.pl?Gifted sportsmen were competing.?
[Cz]b. Nadane?Giftedfem.plz?enywomensoutez?ily.competedfem.pl?Gifted women were competing.?
[Cz]c. Nadana?Giftedneut.plde?vc?atagirlsneutsoute?z?ila.competingneut.pl?Gifted girls were competing.?
[Cz]d. TalantlivyeGiftedplmuz?c?iny/z?ens?c?inymen/womensorevnovalis?.competedpl?Gifted men/women were competing.?
[Ru]Negation in Czech is in the majority of cases is ex-pressed by the prefix ne-, whereas in Russian it isvery common to see a separate particle (ne) instead:(3) a. Nicnothingner?ekl.not-said?He didn?t say anything.?
[Cz]b. Onhenic?egonothingnenotskazal.said?He didn?t say anything.?
[Ru]In addition, reflexive verbs in Czech are formed by averb followed by a reflexive clitic, whereas in Rus-sian, the reflexivization is the affixation process:(4) a.
FilipFilipseREFL-CLjes?te?stillnehol?
?.not-shaves?Filip doesn?t shave yet.?
[Cz]b. FilipFilipesc?estillnenotbreet+sja.shaves+REFL.SUFFIX?Filip doesn?t shave yet.?
[Ru]Even though auxiliaries and the copula are the formsof the same verb byt?
?to be?, both in Russian and inCzech, the use of this verb is different in the twolanguages.
For example, Russian does not use anauxiliary to form past tense:(5) a. Ja?Ijsemaux1sgpsal.wrote?I was writing/I wrote.?
[Cz]b. JaIpisal.wrote?I was writing/I wrote.?
[Ru]It also does not use the present tense copula, exceptfor emphasis; but it uses forms of the verb byt?
insome other constructions like past passive.We implemented a number of simple ?russifica-tions?.
The combination of random omission of theverb byt?, omission of the reflexive clitics, and nega-tion transformation gave us the best results on thedevelopment corpus.
Their combination improvesthe overall result from 68.0% to 69.4%.
We admitwe expected a larger improvement.5.5 Sub-taggersOne of the problems when tagging with a largetagset is data sparsity; with 1000 tags there are10003 potential trigrams.
It is very unlikely that anaturally occurring corpus will contain all the ac-ceptable tag combinations with sufficient frequencyto reliably distinguish them from the unacceptablecombinations.
However, not all morphological at-tributes are useful for predicting the attributes of thesucceeding word (e.g., tense is not really useful forcase).
We therefore tried to train the tagger on indi-vidual components of the full tag, in the hope thateach sub-tagger would be able to learn what it needsfor prediction.
This move has the additional bene-fit of making the tag set of each such tagger smallerand reducing data sparsity.
We focused on the first 5positions ?
POS (P), SubPOS (S), gender (g), num-ber (n), case (c) and person (p).
The selection ofthe slots is based on our linguistic intuition ?
forexample it is reasonable to assume that the infor-mation about part-of-speech and the agreement fea-tures (gnc) of previous words should help in pre-diction of the same slots of the current word; orinformation about part-of-speech, case and personshould assist in determining person.
On the otherhand, the combination of tense and case is prima fa-cie unlikely to be much use for prediction.
Indeed,most of our expectations have been met.
The perfor-mance of some of the models on the developmentcorpus is summarized in Table 5.
The bold num-bers indicate that the tagger outperforms the full-tagtagger.
As can be seen, the taggers trained on indi-vidual positions are worse than the full-tag taggeron these positions.
This proves that a smaller tagsetdoes not necessarily imply that tagging is easier ?see (Elworthy, 1995) for more discussion of this in-teresting relation.
Similarly, there is no improve-ment from the combination of unrelated slots ?
caseand tense (ct) or gender and negation (ga).
How-ever, the combinations of (detailed) part-of-speechwith various agreement features (e.g., Snc) outper-form the full-tag tagger on at least some of the slots.full-tag P S g n c1 (P) 89.0 87.2 ?
?
?
?2 (S) 86.6 ?
84.5 ?
?
?3 (g) 81.4 ?
?
78.8 ?
?4 (n) 92.4 ?
?
?
91.2 ?5 (c) 80.9 ?
?
?
?
78.4full-tag Pc gc ga nc cp ct1 (P) 89.0 87.5 ?
?
?
?
?2 (S) 86.6 ?
?
?
?
?
?3 (g) 81.4 ?
80.4 78.7 ?
?
?4 (n) 92.4 ?
?
?
91.8 ?
?5 (c) 80.9 80.6 81.1 ?
81.5 79.3 79.58 (p) 98.3 ?
?
?
?
96.9 ?9 (t) 97.0 ?
?
?
?
?
96.111 (a) 97.0 ?
?
95.4 ?
?
?full-tag Pgc Pnc Sgc Snc Sgnc1 (P) 89.0 87.9 87.5 ?
?
?2 (S) 86.6 ?
?
86.1 86.4 87.13 (g) 81.4 80.3 ?
81.4 ?
82.74 (n) 92.4 ?
92.4 ?
93.0 92.85 (c) 80.9 81.8 81.4 80.9 82.9 82.3Table 5: Performance of the TnT tagger trained onvarious subtags (development data)5.6 Combining Sub-taggersWe now need to put the sub-tags back together toproduce estimates of the correct full tags.
We can-not simply combine the values offered by the besttaggers for each slot, because that could yield ille-gal tags (e.g., nouns in past tense).
Instead we selectthe best tag from those offered by our morphologi-cal analyzer using the following formula:(6) bestTag = argmaxt?TMAval(t)TMA ?
the set of tags offered by MAval(t) =?14k=0 Nk(t)/NkNk(t) ?
# of taggers voting for k-th slot of tNk ?
the total # of taggers on slot kThat means, that the best tag is the tag that receivedthe highest average percentage of votes for each offull-tag all best 1 best 3overall 69.5 70.3 70.7 71.11 (P) 89.0 88.9 89.1 89.22 (S) 86.6 86.5 86.9 86.93 (g) 81.4 81.8 83.0 83.24 (n) 92.4 92.6 93.1 93.25 (c) 80.9 82.1 83.0 83.26 (G) 98.5 98.5 98.7 98.77 (N) 99.6 99.7 99.8 99.88 (p) 98.3 98.2 98.4 98.39 (t) 97.0 97.0 97.0 97.010 (G) 96.0 96.0 96.0 96.011 (a) 97.0 97.0 96.9 97.012 (v) 97.4 97.3 97.5 97.415 (V) 99.1 99.1 99.0 99.0Table 6: Combining sub-taggers (development data)Baseline Direct Russified RussifiedTagger random full-tag full-tag votingAccuracyTags 33.6 69.4 72.6 73.51 (POS) 63.2 88.5 90.1 90.42 (SubPOS) 57.0 86.8 88.1 88.63 (Gender) 59.2 82.5 84.5 85.04 (Number) 75.9 91.2 92.6 93.45 (Case) 47.3 80.4 84.1 85.36 (PossGen) 83.4 98.4 98.8 99.07 (PossNr) 99.6 99.6 99.6 99.88 (Person) 97.1 99.3 98.9 98.99 (Tense) 86.6 96.5 97.6 97.610 (Grade) 90.1 95.9 96.6 96.611 (Neg) 81.4 95.3 95.5 95.512 (Voice) 86.4 97.2 97.9 97.915 (Variant) 97.0 99.1 99.5 99.5Table 7: Tagging with various parameters (test data)its slots.
If we cared about certain slots more thanabout others we could weight the slots in the valfunction.We ran several experiments, the results of three ofthem are summarized in Table 6.
All of them workbetter than the full-tag tagger.
One (?all?)
uses allavailable subtaggers, other (?best 1?)
uses the besttagger for each slot (therefore voting in Formula 6reduces to finding a closest legal tag).
The best re-sult is obtained by the third tagger (?best 3?)
whichuses the three best taggers for each of the Pgcp slotsand the best tagger for the rest.
We selected this tag-ger to tag the test corpus, for which the results aresummarized in Table 7.Russian Gloss Correct Xerox Ours?Clen member noun nom genpartii party noun gen oblpo prep prep obl accvozmoz?nosti possibility noun obl accstaralsja tried vfinnje not ptclgovorit?
to-speak vinfni nor ptclo about prep oblBratstvje Brotherhood noun obl, cmni nor ptclo about prep oblknigje book noun oblErrors 3 1?Neither the Brotherhood nor the book was a subjectthat any ordinary Party member would mention ifthere was a way of avoiding it.?
[Orwell: ?1984?
]Table 8: Tagging with Xerox & our tagger5.7 Comparison with Xerox taggerA tagger for Russian is part of the Xerox languagetools.
We could not perform a detailed evaluationsince the tool is not freely available.
We used theonline demo version of Xerox?s Disambiguator6 totag a few sentences and compared the results withthe results of our tagger.
The Xerox tagset is muchsmaller than ours, it uses 63 tags, collapsing somecases, not distinguishing gender, number, person,tense etc.
(However, it uses different tags for dif-ferent punctuation, while we have one tag for allpunctuation).
For the comparison, we translated ourtagset to theirs.
On 201 tokens of the testing cor-pus, the Xerox tagger achieved an accuracy of 82%,while our tagger obtained 88%; i.e., a 33% reduc-tion in error rate.
A sample analysis is in Table 8.5.8 Comparison with Czech taggersThe numbers we obtain are significantly worse thanthe numbers reported for Czech (Hajic?
et al, 2001)(95.16% accuracy); however, they use an extensivemanually created morphological lexicon (200K+entries) which gives 100.0% recall on their testingdata.
Moreover, they train and test their taggers onthe same language.6 Ongoing ResearchWe are currently working on improving both themorphological analysis and tagging.
We would like6http://www.xrce.xerox.com/competencies/content-analysis/demos/russianto improve the recall of filters following morpholog-ical analysis, e.g., using n maximal values insteadof 1, using some basic knowledge of derivationalmorphology, etc.
We are incorporating phonologicalconditions on stems into the guesser module as wellas trying to deal with different morphological phe-nomena specific to Russian, e.g., verb reflexiviza-tion.
However, we try to stay language independent(at least within Slavic languages) as much as possi-ble and limit the language dependent components toa minimum.Currently, we are working on more sophisticatedrussifications that would be still easily portable toother languages.
For example, instead of omittingauxiliaries randomly, we want to use the syntac-tic information present in Prague Dependency Tree-bank to omit only the ?right?
ones.If possible, we would like to avoid entirely throw-ing away the Czech emission probabilities, becauseour intuition tells us that there are useful lexicalsimilarities between Russian and Czech, and thatsome suitable process of cognate detection will al-low us to transfer information from the Czech tothe Russian emission probabilities.
Just as a knowl-edge of English words is sometimes helpful (mod-ulo sound changes) when reading German, a knowl-edge of the Czech lexicon should be helpful (mod-ulo character set issues) when reading Russian.
Weare seeking the right way to operationalize this in-tuition in our system, bearing in mind that we wanta sufficiently general algorithm to make the methodportable to other languages, for which we assumewe have neither the time nor the expertise to under-take knowledge-intensive work.
A potentially suit-able cognate algorithm is described by (Kondrak,2001).Finally, we would like to extend our work to Slaviclanguages for which there are even fewer availableresources than Russian, such as Belarusian, sincethis was the original motivation for undertaking thework in the first place.AcknowledgementsWe thank Erhard Hinrichs and Eric Fosler-Lussierfor giving us feedback on previous versions of thepaper and providing useful suggestions for subtag-gers and voting; Jan Hajic?
for the help with theCzech tag system and the morphological analyzer;to the Clippers discussion group for allowing us tointerview ourselves in front of them, and for ensuingdiscussion, and to two anonymous EMNLP review-ers for extremely constructive feedback.ReferencesAlena Be?mova?, Jan Hajic?, Barbora Hladka?, andJarmila Panevova?.
1999.
Morphological and syn-tactic tagging of the prague dependency treebank.In Proceedings of ATALA Workshop, pages 21?29.Paris, France.Thorsten Brants.
2000.
TnT - A Statistical Part-of-Speech Tagger.
In Proceedings of ANLP-NAACL,pages 224?231.Sas?o Dz?eroski, Tomaz?
Erjavec, and JakubZavrel.
2000.
Morphosyntactic Tagging ofSlovene:Evaluating Taggers and Tagsets.
In Pro-ceedings of the Second International Conferenceon Language Resources and Evaluation, pages1099?1104.David Elworthy.
1995.
Tagset design and inflectedlanguages.
In EACL SIGDAT workshop ?FromTexts to Tags: Issues in Multilingual LanguageAnalysis?, pages 1?10, Dublin, April.John Goldsmith.
2001.
Unsupervised Learning ofthe Morphology of a Natural Language.
Computa-tional Linguistics, 27(2):153?198.Jan Hajic?, Pavel Krbec, Pavel Kve?ton?, Karel Oliva,and Vladim?
?r Petkevic?.
2001.
Serial Combinationof Rules and Statistics: A Case Study in Czech Tag-ging.
In Proceedings of ACL Conference, Toulouse,France.Jan Hajic?.
2000.
Morphological Tagging: Datavs.
Dictionaries.
In Proceedings of ANLP-NAACLConference, pages 94?101, Seattle, Washington,USA.Jiri Hana and Anna Feldman.
to appear.
PortableLanguage Technology: The case of Czech and Rus-sian.
In Proceedings from the Midwest Computa-tional Linguistics Colloquium, June 25-26, 2004,Bloomington, Indiana.Greg Kondrak.
2001.
Identifying cognates by pho-netic and semantic similarity.
In Proceedings ofthe Second Meeting of the North American Chap-ter of the Association for Computational Linguistics(NAACL-2001), pages 103?110, June.Andrey Kovalev.
2002.
A Probabilistic Mor-phological Analyzer for Russian and Ukranian.http://linguist.nm.ru/stemka/stemka.html.Mitchell Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: The Penn Treebank.
Computa-tional Linguistics, 19(2):313?330.Andrei Mikheev and Liubov Liubushkina.
1995.Russian Morphology: An Engineering Approach.Natural Language Engineering, 3(1):235?260.Ilya Segalovich and Michail Maslov.
1989.Dictionary-based Russian morphological analy-sis and synthesis with generation of morpho-logical models of unknown words (in Russian).http://company.yandex.ru/articles/article1.html.Ilya Segalovich and Vitaly Titov.
2000.
Au-tomatic morphological annotation MYSTEM.http://corpora.narod.ru/article.html.Ilya Segalovich.
2003.
A fast morpholog-ical algorithm with unknown word guessinginduced by a dictionary for a web searchengine.
http://company.yandex.ru/articles/iseg-las-vegas.html.Jean Ve?ronis.
1996.
MULTEXT-EAST (Copernicus 106).http://www.lpl.univaix.fr/projects/multext-east.Terence Wade.
1992.
A Comprehensive RussianGrammar.
Blackwell.
582 pp.Serge A. Yablonsky.
1999.
Russian MorphologicalAnalysis.
In Proceedings VEXTAL.David Yarowsky and Richard Wicentowski.
2000.Minimally supervised morphological analysis bymultimodal alignment.
In Proceedings of the 38thMeeting of the Association for Computational Lin-guistics, pages 207?216.
