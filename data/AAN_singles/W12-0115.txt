Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 113?118,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsCan Machine Learning Algorithms ImprovePhrase Selection in Hybrid Machine Translation?Christian FedermannLanguage Technology LabGerman Research Center for Artificial IntelligenceStuhlsatzenhausweg 3, D-66123 Saarbru?cken, GERMANYcfedermann@dfki.deAbstractWe describe a substitution-based, hybridmachine translation (MT) system that hasbeen extended with a machine learningcomponent controlling its phrase selection.Our approach is based on a rule-based MT(RBMT) system which creates templatetranslations.
Based on the generation parsetree of the RBMT system and standardword alignment computation, we identifypotential ?translation snippets?
from one ormore translation engines which could besubstituted into our translation templates.The substitution process is controlled by abinary classifier trained on feature vectorsfrom the different MT engines.
Using a setof manually annotated training data, we areable to observe improvements in terms ofBLEU scores over a baseline version of thehybrid system.1 IntroductionIn recent years, the overall quality of machinetranslation output has improved greatly.
Still,each technological paradigm seems to suffer fromits own particular kinds of errors: statistical MT(SMT) engines often show poor syntax, whilerule-based MT systems suffer from missing datain their vocabularies.
Hybrid approaches try toovercome these typical errors by combining tech-niques from both (or even more) paradigms in anoptimal manner.In this paper we report on experiments with anextended version of the hybrid system we developin our group (Federmann and Hunsicker, 2011;Federmann et al, 2010).
We take the output froman RBMT engine as ?translation template?
for ourhybrid translations and substitute noun phrases1by translations from one or several MT engines2.Even though a general increase in quality could beobserved in previous work, our system introducederrors of its own during the substitution process.In an internal error analysis, these degradationscould be classified in the following way:- external translations were incorrect;- the structure degraded through substitution;- phrase substitution failed.Errors of the first class cannot be corrected, as wedo not have an easy way of knowing when thetranslation obtained from an external MT engineis incorrect.
The other classes could, however, beeliminated by introducing additional steps for pre-and post-processing as well as by improving thehybrid substitution algorithm itself.
So far, ouralgorithm relied on many, hand-crafted decisionfactors; in order to improve translation quality andprocessing speed, we decided to apply machinelearning methods to our training data to train alinear classifier which could be used instead.This paper is structured in the following way.After having introduced the topics of our work inSection 1, we give a description of our hybrid MTsystem architecture in Section 2.
Afterwards wedescribe in detail the various decision factors we1We are focusing on noun phrases for the moment asthese worked best in previous experiments with substitution-based MT; likely because they usually form consecutivespans in the translation output.2While this could be SMT systems only, our approachsupports engines from all MT paradigms.
If not all featuresinside our feature vectors can be filled using the output ofsome system X , we use defaults as fallback values.113have defined and how these could be used in fea-ture vectors for machine learning methods in Sec-tion 3.
Our experiments with the classifier-based,hybrid MT system are reported in Section 4.
Weconclude by giving a summary of our work andthen provide an outlook to related future work inSection 5.2 ArchitectureOur hybrid machine translation system combinestranslation output from:a) the Lucy RBMT system, described in moredetail in (Alonso and Thurmair, 2003), andb) one or several other MT systems, e.g.Moses (Koehn et al, 2007), or Joshua (Li etal., 2009).The rule-based component of our hybrid systemis described in more detail in section 2.2 while weprovide more detailed information on the ?other?systems in section 2.3.2.1 Basic ApproachWe first identify noun phrases inside the rule-based translation and compute the most proba-ble correspondences in the translation output fromthe other systems.
For the resulting phrases, weapply a factored substitution method that decideswhether the original RBMT phrase should be keptor rather be replaced by one of the candidatephrases.
As this shallow substitution process mayintroduce errors at phrase boundaries, we performseveral post-processing steps that clean up andfinalise the hybrid translation result.
A schematicoverview of our hybrid system and its main com-ponents is given in figure 1.2.2 Rule-Based Translation TemplatesWe obtain the ?translation template?
as well asany linguistic structures from the RBMT system.Previous work with these structures had shownthat they are usually of a high quality, supportingour initial decision to consider the RBMT outputas template for our hybrid translation approach.The Lucy translation output can include markupthat allows to identify unknown words or otherphenomena.The Lucy system is a transfer-based RBMTsystem that performs translation in three phases,namely analysis, transfer, and generation.
TreeFigure 1: Schematic overview of the architecture ofour substitution-based, hybrid MT system.structures for each of the translation phases canbe extracted from the Lucy system to guide thehybrid system.
Only the 1-best path through thethree phases is given, so no alternative translationpossibilities can be extracted from the given data;a fact that clearly limits the potential for moredeeply integrated hybrid translation approaches.Nonetheless, the availability of these 1-best treesalready allowed us to improve the translationquality of the RBMT system as we had shown inprevious work.2.3 Substitution Candidate TranslationsWe use state-of-the-art SMT systems to createstatistical, phrase-based translations of our inputtext, together with the bidirectional word align-ments between the source texts and the transla-tions.
Again, we make use of markup which helpsto identify unknown words as this will later beuseful in the factored substitution method.Translation models for our SMT systems weretrained with lower-cased and tokenised Europarl(Koehn, 2005) training data.
We used the LDCGigaword corpus to train large scale languagemodels and tokenised the source texts using thetokenisers available from the WMT shared taskwebsite3.
All translations are re-cased before theyare sent to the hybrid system together with theword alignment information.3Available at http://www.statmt.org/wmt12/114The hybrid MT system can easily be adaptedto support other translation engines.
If there is noalignment information available directly, a wordalignment tool is needed as the alignment is akey requirement for the hybrid system.
For part-of-speech tagging and lemmatisation we used theTreeTagger (Schmid, 1994).2.4 Aligning RBMT and SMT OutputWe compute alignment in several components ofthe hybrid system, namely:source-text-to-tree: we first find an alignmentbetween the source text and the correspond-ing analysis tree.
As Lucy tends to subdividelarge sentences into several smaller units, itsometimes becomes necessary to align morethan one tree structure to a source sentence.analysis-transfer-generation: for each of theanalysis trees, we re-construct the path fromits tree nodes, via the transfer tree, to thecorresponding generation tree nodes.tree-to-target-text: similarly to the first align-ment process, we find a connection betweengeneration tree nodes and the correspondingtranslation output of the RBMT system.source-text-to-tokenised: as the Lucy RBMTsystem works on non-tokenised input textand our SMT systems take tokenised input,we need to align the original source text withits tokenised form.Given the aforementioned alignments, we canthen correlate phrases from the rule-based trans-lation with their counterparts from the statisticaltranslations, both on source or target side.
Asour hybrid approach relies on the identification ofsuch phrase pairs, the computation of the differ-ent alignments is critical to achieve a good systemcombination quality.All tree-based alignments can be computedwith a very high accuracy.
However, due to thenature of statistical word alignment, the samedoes not hold for the alignment obtained from theSMT systems.
If the alignment process produceserroneous phrase tables, it is very likely that Lucyphrases and their ?aligned?
SMT matches simplydo not fit the ?open slot?
inside the translationtemplate.
Or put the other way round: the betterthe underlying SMT word alignment, the greaterthe potential of the hybrid substitution approach.2.5 Factored SubstitutionGiven the results of the alignment process, wecan then identify ?interesting?
phrases for substi-tution.
Following our experimental setup from theWMT10 shared task, we again decided to focuson noun phrases as these seem to be best-suitedfor in-place swapping of phrases.To avoid errors or problems with non-matchinginsertions, we want to keep some control on thesubstitution process.
As the substitution processproved to be a very difficult task during previousexperiments with the hybrid system, we decidedto use machine learning methods instead.
For this,we refined our previously defined set of decisionfactors into values v ?
R which allows to com-bine them in feature vectors xi = v1 .
.
.
vp.
Wedescribe the integration of the linear classifier inmore detail in Section 3.2.6 Decision FactorsWe used the following factors:1. frequency: frequency of a given candidatephrase compared to total number of candi-dates for the current phrase;2.
LM(phrase): language model (LM) score ofthe phrase;3.
LM(phrase)+1: phrase with right-context;4.
LM(phrase)-1: phrase with left-context;5.
Part-of-speech match?
: checks if the part-of-speech tags of the left/right context matchthe current candidate phrase?s context;6.
LM(pos) LM score for part-of-speech (PoS);7.
LM(pos)+1 PoS with right-context;8.
LM(pos)-1 PoS with left-context;9.
Lemma checks if the lemma of the candidatephrase fits the reference;10.
LM(lemma) LM score for the lemma;11.
LM(lemma)+1 lemma with right-context;12.
LM(lemma)-1 lemma with left-context.1152.7 Post-processing StepsAfter the hybrid translation has been computed,we perform several post-processing steps to cleanup and finalise the result:cleanup first, we perform some basic cleanupsuch as whitespace normalisation;multi-words then, we take care of multi-wordexpressions.
Using the tree structures fromthe RBMT system we remove superfluouswhitespace and join multi-words, even ifthey were separated in the substituted phrase;prepositions finally, prepositions are checked asexperience from previous work had shownthat these contributed to a large extent to theamount of avoidable errors.3 Machine Learning-based SelectionInstead of using hand-crafted decision rules in thesubstitution process, we aim to train a classifier ona set of annotated training examples which may bebetter able to extract useful information from thevarious decision factors.3.1 Formal RepresentationOur training set D can be represented formally asD = {(xi, yi)|xi ?
Rp, yi ?
{?1, 1}}ni=1 (1)where each xi represents the feature vector forsentence i while the yi value contains the anno-tated class information.
We use a binary classifi-cation scheme, simply defining 1 as ?good?
and?1 as ?bad?
translations.
In order to make use ofmachine learning methods such as decision trees(Breiman et al, 1984), SVMs (Vapnik, 1995), orthe Perceptron (Rosenblatt, 1958) algorithm, wehave to prepare our training set with a sufficientlylarge number of annotated training instances.
Wegive further details on the creation of an annotatedtraining set in section 4.1.3.2 Creating Hybrid TranslationsUsing suitable training data, we can train a binaryclassifier (using either a decision tree, an SVM, orthe Perceptron algorithm) that can be used in ourhybrid combination algorithm.The pseudo-code in Algorithm 1 illustrates howsuch a classifier can be used in our hybrid MTdecoder.Algorithm 1 Decoding using linear classifier1: good candidates?
[]2: for all substitution candidates Ci do3: if CLASSIFY(Ci) == ?good?
then4: good candidates?
Ci5: end if6: end for7: Cbest ?
SELECT-BEST(good candidates)8: SUBSTITUTE-IN(Cbest)We first collect all ?good?
translations using theCLASSIFY() operation, then choose the ?best?candidate for substitution with SELECT-BEST(),and finally integrate the resulting candidatephrase into the generated translation usingSUBSTITUTE-IN().
SELECT-BEST() coulduse system-specific confidences obtained duringthe tuning phase of our hybrid system.
We arestill experimenting on its exact definition.4 ExperimentsIn order to obtain initial experimental results, wecreated a decision-tree-based variant of our hy-brid MT system.
We implemented a decision treelearning module following the CART algorithm(Breiman et al, 1984).
We opted for this solutionas decision trees represent a straightforward firststep when it comes to integrating machine learn-ing into our hybrid system.4.1 Generating Training DataFor this, we first created an annotated data set.
Ina nutshell, we computed feature vectors and po-tential substitution candidates for all noun phrasesin our training data4 and then collected data fromhuman annotators which of the substitution candi-dates were ?good?
translations and which shouldrather be considered ?bad?
examples.
We usedAppraise (Federmann, 2010) for the annotation,and collected 24,996 labeled training instanceswith the help of six human annotators.
Table 1gives an overview of the data sets characteristics.Translation CandidatesTotal ?good?
?bad?Count 24,996 10,666 14,330Table 1: Training data set characteristics4We used the WMT12 ?newstest2011?
development setas training data for the annotation task.116Hybrid Systems Baseline SystemsBaseline +Decision Tree Lucy Linguatec Moses JoshuaBLEU 13.9 14.2 14.0 14.7 14.6 15.9BLEU-cased 13.5 13.8 13.7 14.2 13.5 14.9TER 0.776 0.773 0.774 0.775 0.772 0.774Table 2: Experimental results comparing baseline hybrid system using hand-crafted decision rules to a decision-tree-based variant; both applied to the WMT12 ?newstest2012?
test set data for language pair English?German.4.2 Experimental ResultsUsing the annotated data set, we then trained adecision tree and integrated it into our hybrid sys-tem.
To evaluate translation quality, we createdtranslations of the WMT12 ?newstest2012?
testset, for the language pair English?German, witha) a baseline hybrid system using hand-crafted de-cision rules and b) an extended version of our hy-brid system using the decision tree.Both hybrid systems relied on a Lucy trans-lation template and were given additional trans-lation candidates from another rule-based sys-tem (Aleksic and Thurmair, 2011), a statisticalsystem based on the Moses decoder, and a sta-tistical system based on Joshua.
If more than one?good?
translation was found, we used the hand-crafted rules to determine the single, winningtranslation candidate (implementing SELECT-BEST in the simplest, possible way).Table 2 shows results for our two hybrid sys-tem variants as well as for the individual base-line systems.
We report results from automaticBLEU (Papineni et al, 2001) scoring and alsofrom its case-sensitive variant, BLEU-cased.4.3 Discussion of ResultsWe can observe improvements in both BLEUand BLEU-cased scores when comparing thedecision-tree-based hybrid system to the baselineversion relying on hand-crafted decision rules.This shows that the extension of the hybrid sys-tem with a learnt classifier can result in improvedtranslation quality.On the other hand, it is also obvious, that theimproved hybrid system was not able to outper-form the scores of some of the individual base-line systems; there is additional research requiredto investigate in more detail how the hybrid ap-proach can be improved further.5 Conclusion and OutlookIn this paper, we reported on experiments aimingto improve the phrase selection component of ahybrid MT system using machine learning.
Wedescribed the architecture of our hybrid machinetranslation system and its main components.We explained how to train a decision tree basedon feature vectors that emulate previously used,hand-crafted decision factors.
To obtain trainingdata for the classifier, we manually annotated aset of 24,996 feature vectors and compared thedecision-tree-based, hybrid system to a baselineversion.
We observed improved BLEU scoresfor the language pair English?German on theWMT12 ?newstest2012?
test set.Future work will include experiments withother machine learning classifiers such as SVMs.It will also be interesting to investigate what otherfeatures can be useful for training.
Also, weintend to experiment with heterogeneous featuresets for the different source systems (resulting inlarge but sparse feature vectors), adding system-specific annotations from the various systems andwill investigate their performance in the contextof hybrid MT systems.AcknowledgmentsThis work has been funded under the SeventhFramework Programme for Research and Tech-nological Development of the European Commis-sion through the T4ME contract (grant agreementno.
: 249119).
The author would like to thankSabine Hunsicker and Yu Chen for their support increating the WMT12 translations, and is indebtedto Herve?
Saint-Amand for providing help with theautomated metrics scores.
Also, we are grateful tothe anonymous reviewers for their valuable feed-back and comments.117ReferencesVera Aleksic and Gregor Thurmair.
2011.
Personaltranslator at wmt2011.
In Proceedings of the SixthWorkshop on Statistical Machine Translation, pages303?308, Edinburgh, Scotland, July.
Associationfor Computational Linguistics.Juan A. Alonso and Gregor Thurmair.
2003.
TheComprendium Translator system.
In Proceedingsof the Ninth Machine Translation Summit, New Or-leans, USA.L.
Breiman, J. Friedman, R. Olshen, and C. Stone.1984.
Classification and Regression Trees.Wadsworth and Brooks, Monterey, CA.Christian Federmann and Sabine Hunsicker.
2011.Stochastic parse tree selection for an existing rbmtsystem.
In Proceedings of the Sixth Workshopon Statistical Machine Translation, pages 351?357,Edinburgh, Scotland, July.
Association for Compu-tational Linguistics.Christian Federmann, Andreas Eisele, Yu Chen,Sabine Hunsicker, Jia Xu, and Hans Uszkoreit.2010.
Further experiments with shallow hybrid mtsystems.
In Proceedings of the Joint Fifth Work-shop on Statistical Machine Translation and Met-ricsMATR, pages 77?81, Uppsala, Sweden, July.Association for Computational Linguistics.Christian Federmann.
2010.
Appraise: An open-source toolkit for manual phrase-based evaluationof translations.
In Nicoletta Calzolari (ConferenceChair), Khalid Choukri, Bente Maegaard, JosephMariani, Jan Odijk, Stelios Piperidis, Mike Ros-ner, and Daniel Tapias, editors, Proceedings of theSeventh International Conference on Language Re-sources and Evaluation (LREC?10), Valletta, Malta,may.
European Language Resources Association(ELRA).Philipp Koehn, Hieu Hoang, Alexandra Birch,Chris Callison-Burch, Marcello Federico, NicolaBertoldi, Brooke Cowan, Wade Shen, ChristineMoran, Richard Zens, Chris Dyer, Ondrej Bojar,Alexandra Constantin, and Evan Herbst.
2007.Moses: Open source toolkit for statistical machinetranslation.
In Proc.
of ACL Demo and Poster Ses-sions, pages 177?180, Jun.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In Proceedings ofthe MT Summit 2005.Zhifei Li, Chris Callison-Burch, Chris Dyer, San-jeev Khudanpur, Lane Schwartz, Wren Thornton,Jonathan Weese, and Omar Zaidan.
2009.
Joshua:An open source toolkit for parsing-based machinetranslation.
In Proceedings of the Fourth Workshopon Statistical Machine Translation, pages 135?139,Athens, Greece, March.
Association for Computa-tional Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2001.
Bleu: a method for automaticevaluation of machine translation.
IBM ResearchReport RC22176(W0109-022), IBM.F.
Rosenblatt.
1958.
The Perceptron: A probabilisticmodel for information storage and organization inthe brain.
Psychological Review, 65:386?408.Helmut Schmid.
1994.
Probabilistic part-of-speechtagging using decision trees.
In Proceedings of theInternational Conference on New Methods in Lan-guage Processing, Manchester, UK.Toby Segaran.
2007.
Programming Collective In-telligence: Building Smart Web 2.0 Applications.O?Reilly, Beijing.V.
N. Vapnik.
1995.
The nature of statistical learningtheory.
Springer, New York.118
