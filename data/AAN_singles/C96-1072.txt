Learning to Recognize Names Across LanguagesAnthony F. GallippiUnivers i ty of  Southern Cal i forniaUnivers i ty  Park, EEB 234Los Angeles,  CA  90089USAgallippi @ aludra.usc.eduAbstractThe development of natural anguage pro-ccssing (NLP) systems that perform ma-chine translation (MT) and information re-trieval (IR) has highlighted the need for theautomatic recognition of proper names.While various name recognizers have beendeveloped, they suffer from being too lim-ited; some only recognize one name class,and all are language specific.
This workdevelops an approach to multilingual namerecognition that allows a system optimizedfor one language to be ported to anotherwith little additional effort and resources.An initial core set of linguistic features,useful for name recognition in most lan-guages, is identified.
When porting to anew language, these features need to beconverted (partly by hand, partly by on-linelists), after which point machine learning(ML) techniques build decision trees thatmap features to name classes.
A systeminitially optimized for English has beensuccessfully ported to Spanish and Japa-nese.
Only a few days of human effort foreach new language results in performancelevels comparable to that of the best cur-rent English systems.1 IntroductionProper names represent a unique challenge for MTand IR systems.
They are not found in dictionaries,are very large in number, come and go every day, andappear in many alias forms.
For these reasons, listbased matching schemes do not achieve desiredperformance l vels.
Hand coded heuristics can bedeveloped to achieve high accuracy, however thisapproach lacks portability.
Much human effort isneeded to port the system to a new domain.A desirable approach is one that maximizes reuseand minimizes human effort.
This paper presents anapproach to proper name recognition that uses ma-chine learning and a language independent fi'ame-work.
Knowledge incorporated into the framework isbased on a set of measurable linguistic characteris-tics, or ,features.
Some of this knowledge is constantacross languages.
The rest can be generated auto-matically through machine learning techniques.The problem being considered is that of segment-ing natural anguage text into lexical units, and oftagging those units with various syntactic and se-mantic features.
A lexical unit may be a word (e.g.,"started") or a phrase (e.g., "The Washington Post").The particular lexical units of interest here are propernames.
Segmenting and tagging proper names isvery important for natural anguage processing, par-ticularly IR and MT.Whether a phrase is a proper name, and what typeof proper name it is (company name, location name,person name, date, other) depends on (1) the internalstructure of the phrase, and (2) the surrounding con-text.Internal: "Mr. Brandon"Context: "The new company, Safetek, will makeair bags.
"The person title "Mr." reliably shows "Mr. Brandon"to be a person name.
"Safetek" can be recognized asa company name by utilizing the preceding contex-tual phrase and appositive "The new company,".The recognition task can be broken down into de-limitation and classification.
Delimitation is the de-termination of the boundaries of the proper name,while classification serves to provide a more specificcategory.Original: John Smith , chairman of Safetek , an-nounced his resignation yesterday.Delimit: <PN> John Smith </PN> , chairman of<PN> Safetek </PN>, announced his resig-nation yesterday.Classify: <person> John Smith </person> , chairmanof <company> Safetek </company> , an-nounced his resignation yesterday.424During the delimit step, the boundarics of all propernames are identified.
Next, the delimited propernames  are classified into more specific categodcs.How can a system developed in one language beported to another language with minimal additionaleffort and comparable performance results?
Howmuch additional elTort will be required, and whatdegradation i performance, if any, is to be expected?These questions are addressed in the following sec-tions.2 MethodThe approach taken here is to utilize a data-drivcnknowledge acquisition strategy based on decisiontrees which uses contextual information.
This differsfrom other approaches which attempt o achieve thistask  by: (1) hand-coded heuristics, (2) l ist-basedmatching schemes, (3) human-generated knowledgebases, and (4) combinations thereof.
Delimitationoccurs through the application of phrasal templates.These temphttes, built by hand, use logical ol~eralors(AND, OR, etc.)
to combine features trongly asst)ci-ated with proper names, including: proper ,mun,ampersand, hyphen, and comma.
In addition, ambi-guities with delimitation are handled by includingother predictive features within the templates.To acquire the knowledge required for classilica-tion, each word is tagged with all of its associatedfeatures.
These  features  are obtained through auto-mated and manual techniques.
A decision trec isbuilt (lk)r each name class) from the initial feature setusing a rccursive partitioning algorithm (Quinhtn,1986; l)treiman et al, 1984) that uses the followingfunction as its selection (splitting) criterion:-p*log2Q)) - (1-p)*log2(I-p) (i)where p represents the proportion of names behmg-ing to the class for which the tree is built.
The fea--ture which minimizes the weighted sum o1' tiffsfunction across both child nodes resulting from thesplit is chosen.
A nmltitrce approach was chosenover learning a single tree for all name classes be-cause it allows for the straightforward association offeatures within the tree with specific natllC classes,and facilitates troubleshooting.The result is a hierarchical collection of' co-occur-ring fcatures which predict inclusion to or exclusionfrom a particuhtr proper name class.
Since a tree isbuilt for each nmne class el' interest, the trees arc allapplied individually, and then the results are mergcd.2.1 FeaturesVarious types o1' features indicate the type el' name:parts of speech, designators, morphology, syntax, se-mantics, and more.
1)esignators are features whichaltmc provide strong evidence lbr or against a partic~ular nantc type.
l';xamplcs include "Co." (company),"l)r." (person), anti "County" (location).
For exam-pie, of all the company nmnes in the English trainingtext, 28% are associated with a corporate designator.Other features are predetermined, obtained via on-line lists, or are selected automatical ly based onstatistical measures.
Parts of speech features arepredetermined based on the part of speech taggeremployed.
On-line lists provide lists of cities, personnames, nationalities, regitms, etc.
The initial set oflexical features is selected by choosing those thatappear most frequently (above somc threshold)throughout he training data, and those that appearmost \['requcntly near the positive instances in thetraining data.Some features, such as morphological, keyword,and key phrase features ,  are determined by hantlanalysis tff the text.
Capitalization is one obviousTable 1.
Features ummary.TypePart of Speech1/csignato'~" CompanyPersOllLocalkmI)ateMorpholo~yg ' = .
.
.
.
.
.
.lastTemphtte_ =Special Purposel"eature .
.
.
.
.
.
.
I'ixamplcProper Nmm "Aristotle"Common Noun .
"philoso\[!h~"'"(2orp.
", 1 ,td.""Mr.
", "PresidelW'Cotlnh'y, Stale, CityMonth, l)a~?
of week .
"'A-", "IL""-Gorp', "-tee"WI.>8, WL<3"IBM", "AT&T""Smith".
"Michael""(hdf of Mexico""JtlptlllCSC""based in", "sa!d lie"CapitalizalionCompany SuffixW~wd LengfllCompalfiCsPcrsonsLocationsNationalitiesK.eyword(s)CompanyPCI'SO\[ILocationl)ateProper NameLngst (?Ill SbslrDuplicated PNs< NNP CN .dcsig >< P_.Desig NNP >< NNI' L dcsig >< MM Num, Num>< NNP NNP > .
.
.
"VW" <- VolkswagenDUP 2+, I)UP 5+llow inane.
,NANA100E, IIOS, 60J70 E, 70 S, 43 J520 E, 900 S, 570 J56E, 19S, !9J|E, 1S, 0J5 E, 0 S, 30 J4E, 4S, 2J0E, 100S,7KJ21KE, 2 IKS,  185KJ20 E, 20 S, 2K J220 E, 0 S, 0 J44 E, 49 S, 54 J210E, 210 S, 210 J90 E, 95 S, 90 J190E, 190S, 190J17E, 18S, 70J140E, 140S, 140JIE , |S ,  IJ5E, 5S, 2J425morphological feature of importance.
Determiningkeyword and key phrase features amounts to select-ing prudent subject categories.
These categories areassociated with lists of lexical items or already exist-ing features.
For example, many of the statisticallyderived lexical features may fall under common sub-ject categories.
The words "build", "make","manufacture", and "produce" can be associated withthe subject category "make-type verbs".
Analysis ofthe immediate context surrounding company namesmay lead to the discovery of key phrases like "saidit", "entered a venture", and "is located in".
Table 1shows a summary of various types of features used insystem development.
The longest common substring(LCS) feature (Jacobs et al, 1993) is useful forfinding proper name aliases.2.2 Feature TreesThe ID3 algorithm (Quinlan, 1986) selects and orga-nizes features into a discrimination tree, one tree foreach type of name (person, company, etc.).
The tree,once built, typically contains 100+ nodes, each oneinquiring about one feature in the text, within thelocality of the current proper name of interest.An example of a tree which was generated forcompanies is shown in Figure 1.
The context levelfor this example is 3, meaning that the feature inquestion must occur within the region starting 3words to the left of and ending 3 words to the right ofthe proper name's left boundary.
A "(L)" or "(R)"following the feature name indicates that the featuremust occur to the left of or to the right of the propername's left boundary respectively.
The numbers di-rectly beneath a node of the tree represent the num-ber of negative and positive examples present fromthe training set.
These numbers are useful for associ-ating a confidence level with each classification.Definitions for the features in Figure 1 (and otherabbreviations) can be found in the appendix.The training set used for this example contains1084 negative and 669 positive examples.
To obtainthe best initial split of the training set, the feature"CN_alias" is chosen.
Recursively visiting and op-timally splitting each concurrent subset results in thegeneration of 97 nodes (not including leaf nodes).N N .
.
.
.
.
.
N ' -  N - '  PFigure 1.
Company tree example (context is +/- 3).2.3 ArchitectureFigure 2 shows the working development system.The starting point is training text which has been pre-tagged with the locations of all proper names.
Thetokenizer separates punctuation from words.
Fornon-token languages (no spaces between words), italso separates contiguous characters into constituentwords.
The part of speech (POS) tagger (Brill, 1992;Farwell et.
al., 1994; Matsumoto et al, 1992) at-taches parts of speech.
Thc set of derived features isattached.
During the delimitation phase, propernames are delimited using a set of POS-based hand-coded tcmplates.
Using ID3, a dccision tree is gen-erated based on the existing feature set and thc speci-fied level of context o be considered.
The generatedtree is applied to test data and scored.
Manual analy-sis of the tree and scored result leads to the discoveryof new features.
The new features are added to thetokenized training text, and the process repeats.2.4 Cross Language PortingIn order to work with another language, the follow-ing resources are needed: (1) pre-tagged training textin the new language using same tags as belore, (2) atokenizer for non-token languages, (3) a POS tagger(plus translation of the tags to a standard POS con-vention), and (4) translation of designators andlexical (list-based) features.These language-specific modules are highlightedin Figure 2 with bold bordcrs.
Feature translationoccurs through the utilization of: on-line resources,dictionaries, atlases, bilingual speakers, etc.
Theremainder is constant across languages: a languageindependent core development system, and an opti-mally derived feature set for English.Also worth noting are the parts of developmentsystem that are executed by hand.
These are shownshaded.
Everything else is automatic.3 ExperimentThe system was first built for English and thenported to Spanish and Japanese.
For English, thetraining text consisted of 50 messages obtained fromthe English Joint Ventures (EJV) domain MUC-5corpus of the US Advanced Research ProjectsAgency (ARPA).
This data was hand-tagged withthe locations of company names, person names,locations names, and dates.
The test set consisted of10 new messages.Experimental results were obtained by applyingthe generated trees to test texts.
The initial raw textis tokenized and tagged with parts of speech.
Allfeatures necessary to apply rules and trees are at-tached.
Phrasal template rules are applied in order todelimit proper names.
Then trees for each propername type are applied individually to the propernames in the featurized text.
Proper names which are426Figure 2.
Multilingual development system.voted into more than one class are handled bychoosing the highest priority class.
Priorities aredetermined based on the independent perlormance ofeach tree.
For example, if person trces performbetter independently than location trees, then a per-son classification will be chosen over a locationclassification.
Also, designators have a large impacton resolving conflicts.3.1 EnglishVarious parameterizations were used for systemdevelopment, including: (1) context depth, (2)feature set size, (3) training set size, and (4) incorpo-ration of hand-coded phrasal templates.Figure 3 shows ttle performance r sults for Eng-lish.
The metrics used were recall (R), precision (P),and an averaging measure, P&R, defined as:P&R = 2*P*R/(P+R) (2)Obtained results for English compare to the Englishresults of Rau (1992) and McDonald (1993).
Theweighted average of the P&R for companies, per-sons, locations, and dates is 94.0%.Ii RecaH?
Precisioncompanlos persons locations datesFigure 3.
English performance r sults.The date grammar is rather small in comparisonto other name classes, hence the performance fordates was perfect.
Locations, by contrast, exhibitedthe lowest performance.
This can be attributedmainly to: (1) locations are commonly associatedwith commas, which can create ambiguities withdelimitation, and (2) locations made up a smallpercentage of all names in the training set, whichcould have resulted in overfitting of the built tree tothe training data.Features trengths were measured for companies,persons, and locations.
This experiment involvedremoving one feature at a time from the text used fortesting and then reapplying the stone tree.
Figure 4and Table 2 show performance r sults (P&R) whenthe three most powerful features are removed, one ata time, for companies, persons, and locations respec-tively.
This experiment demonstrates tim power ofdesignator features across all proper name types, andthe importance of the alias feature for companies.10.90.80 .70 .60.50.40 .30.20.1F1 F2 F3 F4 NoneFeature removedNCompaniesI I PersonsLocat ions jFigure 4.
Feature strengths for English.
"Fable 2.
Strongest features for English.Feature CompaniesFI CAPF2 CN desigF3 CN aliasF4 HyphenPersons LocationsP_desig CAPCAP L desi[jATH_reg InF I L Re~ion3.2 SpanishThree experiments have been conducted for Spanish.In the first experiment, he English trees, generated427from the feature set optimized for English, are ap-plied to the Spanish text (E-E-S).
In the second ex-periment, new Spanish-specific trees are generatedfrom the feature set optimized for English and ap-plied to the Spanish text (S-E-S).
The third experi-ment proceeds like the second, except hat minor ad-justments and additions are made to the t'eature setwith the goal of improving performance (S-S-S).The additional resources required for the firstSpanish experiment (E-E-S) are a Spanish POS-tag-ger (Farwell et al, 1994) and also the translated fea-ture set (including POS) optimally derived for Eng-lish.
The second and third Spanish experiments(S-E-S, S-S-S) require in addition pre-tagged Spanishtraining text using the same tags as for English.The obtained Spanish scores as compared to thescores from the initial English experiment (E-E-E)are shown in figure 5.10.90.8O,7O,60,50.4compar l las  persons  Iocatloos dato5E-E-Eill E-E-S!
SES~:'%Figure 5.
P&R scores lbr Spanish versus English.The additional Spanish specific features derived forS-S-S are shown in Table 3.
Only a few new featuresadded to the core feature set alows for significantpcrfommnce improvement.Table 3.
Spanish specific features for S-S-S.Type FeatureList  CompaniesKe~cword(s)Template PersonPersonDateDateInstances Howman~'"IBM", "AT&T', ... 100"del" (OF THE) l< FN DE LN > 1< FN DE NNP > 1< Num OF MM > 1<Num OF MM OF Num> l3.3 JapaneseThe same three experiments conducted lor Spanishare being conducted for Japanese.
The first two,E-E-J and J-E-J, have been completed; J-J-J is inprogress.The additional resources required for the firstJapanese xperiment (E-E-J) are a Japanese tokenizerand POS-tagger (Matsumoto et al, 1992) and alsothe translated feature set optimally derived for Eng-lish.
The second and third Japanese experiments(J-E-J, J-J-J) require in addition pre-taggcd Japanesetraining text using the same tags as for English.The obtained Japanese scores as compared to thescores from the initial English experiment (E-E-E)are shown in Figure 6.
The weighted averages of theP&R measures across all languages, for companies,persons, locations, and dates, are shown in Figure 7.Table 4 shows comparisons to other work.companies persons locations dalesI~ E-E'E JFigure 6.
P&R scores for Japanese versus English.1o6?ii 107ii ?
iiiit!i;i 0.5 0,4 0 .3  0 .20.10 iE'E~E E-E~S S-E-S S-S~S E4E~I J-E.IFigure 7.
Weighted P&R scores comparison.Table 4.
Performance comparison to other work.System Lang.
Class R P P&RRau English.
Corn NA 95 NAPNF English Corn NA NA "Near(McDonald) Pers 100%"LocDateP~mglyzer Spanish NA NA 80 NAMAJESTY Japanese Corn 84.3 81.4 82.8Pers 93.1 98.6 95.8Loc 92.6 96.8 94.7MNR English Corn 97.6 91.6 94.5(Gallippi) Pers 98.2 100 99.1Loc 85.7 91.7 88.6Date 100 100 100(Avg) 94.0MNR Spanish Corn 74.1 90.9 81.6Pers 97.4 79.2 87.4Loc 93.1 87.5 89.4Date 100 100 100(Avg) 89.2MNR Japanese Corn 60.0 6010 60.0Pers 86.5 84.9 85.7Loc 80.4 82.1 81.3Date 90.0 94.7 92.3(Avg) 83.14284 Related WorkProper name recognition has been addressed byothers (Farwell et al, 1994; Kitani & Mitamura,1994; Rau, 1992), with the goal of incorporating thiscapability into IR and MT systems.
Related prob-lems have been studied which utilize contextualinformation and learning.
Examples include posteoditing of documents (article seleclion) (Knight &Chander, 1994), word sense disambiguation (Black,1988; Siegel & McKeown, 1994), and discourseanalysis (Soderland & Letmert, 1994).5 Future WorkAn investigation of the causes of i)erl'ormance degra-dation across languages will be conducted, with timgoal of lfinpointing and concurrently taking steps tominimize their effects.
Other plans ir~clude usingMI, techniques to \[:urther educe the altlOUllt o|human effort: (1) automate the building of templatesfor delimitation, (2) automale the discovery of newfeatures froni test results, and (3) expand the searchspace traversed by lhe tree building algorithm loinchide splils on feature combinalions.AcknowledgmentsThe author would like to offer special thanks andgratitude to \]~duard llovy for all of his support,direction, and ericouragcmcnt front the onset of thiswork.
Thanks als() to Kevin Knight for his earlysuggestions, and to the lnfornmtion Sciences Institutefor use of their facilities and resources.ReferencesBlack, 1';.
1988.
An I';xpcrinmnt in Computalionall)iscriminalion of English Word Senses.
in IBMJournal of Research and l)evelopntent, 32(2).Breiman, l,., Friedman, J.H., Olshcn, R.A., andStone, C..J.
1984.
Classification and Regression77"ees.
Wadsworth International Group.Brill, i';.
1992.
A Simple Rule-Based Part ofSpeech Tagger.
hi i'roceedings elthe Third Cot!i?n'oelite on Applied Natural Language Processing, ACL.Farwell, D., Hehnreich, S., Jin, W., Casper, M.,Hargravc, J., Molina-Salgado, ll., and Weng, 1;.1994.
Panglyzer: Spanish l,anguage AnalysisSystem.
in Proceedings of the Conference of theAssociation of Machine Translation in the Americas(ATMA).
Columbia, MI).Jacobs, P.S., Krupta, C,., Rau, I,., Mauldin, M./..,Mitamura, T., Kitaui, T., Sider, 1. and Childs, L.1993.
Gt!-CMU: l)escription of the SH()GUNSystem Used for MU('.-5.
In l'roceeding.v o\[' theFifth Message Undetwtanding UonJ'erenee (MUC-5).Morgan Katltrnann, pp.
109-120.Kitani, T. and Mitamura, T. 1994.
An AccurateMorphological Almlysis and Proper Name ldcntifi-cation for Japanese Text l'rocessing.
In 77"ansactionsof hzJbrmation Processing Society of Japan, Vol.
35,No.
3, pp.
404-413.Knight, K. and Chandcr, I.
1994.
AutomatedPoslediting (If Documents.
In Proceedings of theTwe!/Th National Cotlference on Artificial Intelli-gence (AAAI), pp.
779-784.LeMert, W., McCarthy, J., Soderland, S., Riloff,E., Card|e, C., Peterson, J., Feng, F., Dolan, C., andGoldman, S. 1993.
UMass/Hughes: Description ofthe CIRCUS System Used for MUC-5.
In Proceed-.ings of the Fifth Message (h~derstanding Conference(MUC-5).
Morgan Kaufinann, pp.
277-292.Matsumolo, Y., Kurohashi, S., Taegi, I{.
andNagao, M. 1992.
JUMAN Users' Manual Vetwion0.8, Nagao Laboratory, Kyoto University.McDonald, I).
1993. internal and \[ixternall{vidcnce in tim Identification and Semantic Catego-rizalion of Proper Names.
In Proceedings of theSINGLFX workshop on.
"Acquisition el LexicalKnowledge fiom Text", pp.
32.-43.Quin\[an, J.R. 1986.
Induction of Decision Trees.\[n Machine LeatvUng, pp.
81-106.Rau, L.F. 1992.
Extracting Company Namesfrom '\['ext.
In Proceedings of the Seventh Coq/~er-elite on Art(fieial Intelligence Applications, pp.
189-194..Siegel, E.V.
and McKeown, K.R.
1994. limer-gent Linguistic Rules l'rom Inducing l)ecision 'Frees:I)isaml)iguating Discourse Clue Words.
In Pro-ceedings of the 7'we!flh National Conference onArtificial Intelligence (AAAI), pp.
820-826.Soderland, S. and l.ehnert, W. 1994.
Corpus-Driven Knowledge Acquisition for l)iscourse Analy-sis.
In Proceedings of the 7'we!/}h National Confer-ence on Artificial Intelligence (AAAI), pp.
827-832.Appendix A. AbbreviationsTable 5. l)efinitions for abbreviations.Abbreviation DefinitionACI,~AT!
1 regCAP('N aliasCN dsgC(mnlryFN1" l l~HyphenIN regionInI ,CSI,NL .desigNNPNotlnPN_cmlPN .2X I-PIingI '  dcsigRegionSO regionSnt end&Acl'onylllOccurs ill <Aulhor> ... </Author>CapitalizedLCS of full company name(\]Oll/pillly llalllc designatorCounh'y nalneFirst (given) nmneFirst name + initial + lasl name!
lyphen (punctuatiol0Occurs ill <IN> ... </IN> region1 ,cxical "in"Longcsl COllllll(HI std)slringl,ast (family) nameLocation designatorProper 11o1111General nounProper Ilanlc clld delimiterl'ropcr lla\[llC occtirs 2-b timesPunctuationl'cl'SOil designatorGeogral)hicaI region nameOccurs in <SO> ... </SO> regionSentence nd boulRlaryA n~!ersand dmracter429
