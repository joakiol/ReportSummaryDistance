Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 523?530, Vancouver, October 2005. c?2005 Association for Computational LinguisticsNon-projective Dependency Parsing using Spanning Tree AlgorithmsRyan McDonald Fernando PereiraDepartment of Computer and Information ScienceUniversity of Pennsylvania{ryantm,pereira}@cis.upenn.eduKiril Ribarov Jan Hajic?Institute of Formal and Applied LinguisticsCharles University{ribarov,hajic}@ufal.ms.mff.cuni.czAbstractWe formalize weighted dependency pars-ing as searching for maximum spanningtrees (MSTs) in directed graphs.
Usingthis representation, the parsing algorithmof Eisner (1996) is sufficient for search-ing over all projective trees in O(n3) time.More surprisingly, the representation isextended naturally to non-projective pars-ing using Chu-Liu-Edmonds (Chu andLiu, 1965; Edmonds, 1967) MST al-gorithm, yielding an O(n2) parsing al-gorithm.
We evaluate these methodson the Prague Dependency Treebank us-ing online large-margin learning tech-niques (Crammer et al, 2003; McDonaldet al, 2005) and show that MST parsingincreases efficiency and accuracy for lan-guages with non-projective dependencies.1 IntroductionDependency parsing has seen a surge of inter-est lately for applications such as relation extrac-tion (Culotta and Sorensen, 2004), machine trans-lation (Ding and Palmer, 2005), synonym genera-tion (Shinyama et al, 2002), and lexical resourceaugmentation (Snow et al, 2004).
The primaryreasons for using dependency structures instead ofmore informative lexicalized phrase structures isthat they are more efficient to learn and parse whilestill encoding much of the predicate-argument infor-mation needed in applications.root John hit the ball with the batFigure 1: An example dependency tree.Dependency representations, which link words totheir arguments, have a long history (Hudson, 1984).Figure 1 shows a dependency tree for the sentenceJohn hit the ball with the bat.
We restrict ourselvesto dependency tree analyses, in which each word de-pends on exactly one parent, either another word or adummy root symbol as shown in the figure.
The treein Figure 1 is projective, meaning that if we put thewords in their linear order, preceded by the root, theedges can be drawn above the words without cross-ings, or, equivalently, a word and its descendantsform a contiguous substring of the sentence.In English, projective trees are sufficient to ana-lyze most sentence types.
In fact, the largest sourceof English dependency trees is automatically gener-ated from the Penn Treebank (Marcus et al, 1993)and is by convention exclusively projective.
How-ever, there are certain examples in which a non-projective tree is preferable.
Consider the sentenceJohn saw a dog yesterday which was a Yorkshire Ter-rier.
Here the relative clause which was a YorkshireTerrier and the object it modifies (the dog) are sep-arated by an adverb.
There is no way to draw thedependency tree for this sentence in the plane withno crossing edges, as illustrated in Figure 2.
In lan-guages with more flexible word order than English,such as German, Dutch and Czech, non-projectivedependencies are more frequent.
Rich inflectionsystems reduce reliance on word order to express523root John saw a dog yesterday which was a Yorkshire Terrierroot O to nove?
ve?ts?inou nema?
ani za?jem a taky na to ve?ts?inou nema?
pen?
?zeHe is mostly not even interested in the new things and in most cases, he has no money for it either.Figure 2: Non-projective dependency trees in English and Czech.grammatical relations, allowing non-projective de-pendencies that we need to represent and parse ef-ficiently.
A non-projective example from the CzechPrague Dependency Treebank (Hajic?
et al, 2001) isalso shown in Figure 2.Most previous dependency parsing models havefocused on projective trees, including the workof Eisner (1996), Collins et al (1999), Yamada andMatsumoto (2003), Nivre and Scholz (2004), andMcDonald et al (2005).
These systems have shownthat accurate projective dependency parsers can beautomatically learned from parsed data.
However,non-projective analyses have recently attracted someinterest, not only for languages with freer word orderbut also for English.
In particular, Wang and Harper(2004) describe a broad coverage non-projectiveparser for English based on a hand-constructed con-straint dependency grammar rich in lexical and syn-tactic information.
Nivre and Nilsson (2005) pre-sented a parsing model that allows for the introduc-tion of non-projective edges into dependency treesthrough learned edge transformations within theirmemory-based parser.
They test this system onCzech and show improved accuracy relative to a pro-jective parser.
Our approach differs from those ear-lier efforts in searching optimally and efficiently thefull space of non-projective trees.The main idea of our method is that dependencyparsing can be formalized as the search for a maxi-mum spanning tree in a directed graph.
This formal-ization generalizes standard projective parsing mod-els based on the Eisner algorithm (Eisner, 1996) toyield efficient O(n2) exact parsing methods for non-projective languages like Czech.
Using this span-ning tree representation, we extend the work of Mc-Donald et al (2005) on online large-margin discrim-inative training methods to non-projective depen-dencies.The present work is related to that of Hirakawa(2001) who, like us, reduces the problem of depen-dency parsing to spanning tree search.
However, hisparsing method uses a branch and bound algorithmthat is exponential in the worst case, even thoughit appears to perform reasonably in limited experi-ments.
Furthermore, his work does not adequatelyaddress learning or measure parsing accuracy onheld-out data.Section 2 describes an edge-based factorizationof dependency trees and uses it to equate depen-dency parsing to the problem of finding maximumspanning trees in directed graphs.
Section 3 out-lines the online large-margin learning frameworkused to train our dependency parsers.
Finally, inSection 4 we present parsing results for Czech.
Thetrees in Figure 1 and Figure 2 are untyped, thatis, edges are not partitioned into types representingadditional syntactic information such as grammati-cal function.
We study untyped dependency treesmainly, but edge types can be added with simple ex-tensions to the methods discussed here.2 Dependency Parsing and Spanning Trees2.1 Edge Based FactorizationIn what follows, x = x1 ?
?
?
xn represents a genericinput sentence, and y represents a generic depen-dency tree for sentence x.
Seeing y as the set of treeedges, we write (i, j) ?
y if there is a dependencyin y from word xi to word xj .In this paper we follow a common method of fac-toring the score of a dependency tree as the sum ofthe scores of all edges in the tree.
In particular, wedefine the score of an edge to be the dot product be-524tween a high dimensional feature representation ofthe edge and a weight vector,s(i, j) = w ?
f(i, j)Thus the score of a dependency tree y for sentencex is,s(x,y) =?
(i,j)?ys(i, j) =?
(i,j)?yw ?
f(i, j)Assuming an appropriate feature representation aswell as a weight vector w, dependency parsing is thetask of finding the dependency tree y with highestscore for a given sentence x.For the rest of this section we assume that theweight vector w is known and thus we know thescore s(i, j) of each possible edge.
In Section 3 wepresent a method for learning the weight vector.2.2 Maximum Spanning TreesWe represent the generic directed graph G = (V,E)by its vertex set V = {v1, .
.
.
, vn} and set E ?
[1 :n]?
[1 : n] of pairs (i, j) of directed edges vi ?
vj .Each such edge has a score s(i, j).
Since G is di-rected, s(i, j) does not necessarily equal s(j, i).
Amaximum spanning tree (MST) of G is a tree y ?
Ethat maximizes the value?
(i,j)?y s(i, j) such thatevery vertex in V appears in y.
The maximum pro-jective spanning tree of G is constructed similarlyexcept that it can only contain projective edges rel-ative to some total order on the vertices of G. TheMST problem for directed graphs is also known asthe maximum arborescence problem.For each sentence x we define the directed graphGx = (Vx, Ex) given byVx = {x0 = root, x1, .
.
.
, xn}Ex = {(i, j) : i 6= j, (i, j) ?
[0 : n] ?
[1 : n]}That is, Gx is a graph with the sentence words andthe dummy root symbol as vertices and a directededge between every pair of distinct words and fromthe root symbol to every word.
It is clear that de-pendency trees for x and spanning trees for Gx co-incide, since both kinds of trees are required to berooted at the dummy root and reach all the wordsin the sentence.
Hence, finding a (projective) depen-dency tree with highest score is equivalent to findinga maximum (projective) spanning tree in Gx.Chu-Liu-Edmonds(G, s)Graph G = (V, E)Edge weight function s : E ?
R1.
Let M = {(x?, x) : x ?
V, x?
= arg maxx?
s(x?, x)}2.
Let GM = (V, M)3.
If GM has no cycles, then it is an MST: return GM4.
Otherwise, find a cycle C in GM5.
Let GC = contract(G, C, s)6.
Let y = Chu-Liu-Edmonds(GC , s)7.
Find a vertex x ?
C s. t.
(x?, x) ?
y, (x?
?, x) ?
C8.
return y ?
C ?
{(x?
?, x)}contract(G = (V, E), C, s)1.
Let GC be the subgraph of G excluding nodes in C2.
Add a node c to GC representing cycle C3.
For x ?
V ?
C : ?x?
?C(x?, x) ?
EAdd edge (c, x) to GC withs(c, x) = maxx?
?C s(x?, x)4.
For x ?
V ?
C : ?x?
?C(x, x?)
?
EAdd edge (x, c) to GC withs(x, c) = maxx?
?C [s(x, x?)
?
s(a(x?
), x?)
+ s(C)]where a(v) is the predecessor of v in Cand s(C) = Pv?C s(a(v), v)5. return GCFigure 3: Chu-Liu-Edmonds algorithm for findingmaximum spanning trees in directed graphs.2.2.1 Non-projective TreesTo find the highest scoring non-projective tree wesimply search the entire space of spanning trees withno restrictions.
Well-known algorithms exist for theless general case of finding spanning trees in undi-rected graphs (Cormen et al, 1990).Efficient algorithms for the directed case are lesswell known, but they exist.
We will use here theChu-Liu-Edmonds algorithm (Chu and Liu, 1965;Edmonds, 1967), sketched in Figure 3 follow-ing Leonidas (2003).
Informally, the algorithm haseach vertex in the graph greedily select the incomingedge with highest weight.
If a tree results, it must bethe maximum spanning tree.
If not, there must be acycle.
The procedure identifies a cycle and contractsit into a single vertex and recalculates edge weightsgoing into and out of the cycle.
It can be shown thata maximum spanning tree on the contracted graph isequivalent to a maximum spanning tree in the orig-inal graph (Leonidas, 2003).
Hence the algorithmcan recursively call itself on the new graph.
Naively,this algorithm runs in O(n3) time since each recur-sive call takes O(n2) to find the highest incomingedge for each word and to contract the graph.
Thereare at most O(n) recursive calls since we cannotcontract the graph more then n times.
However,525Tarjan (1977) gives an efficient implementation ofthe algorithm with O(n2) time complexity for densegraphs, which is what we need here.To find the highest scoring non-projective tree fora sentence, x, we simply construct the graph Gxand run it through the Chu-Liu-Edmonds algorithm.The resulting spanning tree is the best non-projectivedependency tree.
We illustrate here the applicationof the Chu-Liu-Edmonds algorithm to dependencyparsing on the simple example x = John saw Mary,with directed graph representation Gx,rootsawJohn Mary10993030203011The first step of the algorithm is to find, for eachword, the highest scoring incoming edgerootsawJohn Mary303020If the result were a tree, it would have to be themaximum spanning tree.
However, in this case wehave a cycle, so we will contract it into a single nodeand recalculate edge weights according to Figure 3.rootsawJohn Mary4093031wjsThe new vertex wjs represents the contraction ofvertices John and saw.
The edge from wjs to Maryis 30 since that is the highest scoring edge from anyvertex in wjs.
The edge from root into wjs is set to40 since this represents the score of the best span-ning tree originating from root and including onlythe vertices in wjs.
The same leads to the edgefrom Mary to wjs.
The fundamental property of theChu-Liu-Edmonds algorithm is that an MST in thisgraph can be transformed into an MST in the orig-inal graph (Leonidas, 2003).
Thus, we recursivelycall the algorithm on this graph.
Note that we needto keep track of the real endpoints of the edges intoand out of wjs for reconstruction later.
Running thealgorithm, we must find the best incoming edge toall wordsrootsawJohn Mary4030wjsThis is a tree and thus the MST of this graph.
Wenow need to go up a level and reconstruct the graph.The edge from wjs to Mary originally was from theword saw, so we include that edge.
Furthermore, theedge from root to wjs represented a tree from root tosaw to John, so we include all those edges to get thefinal (and correct) MST,rootsawJohn Mary103030A possible concern with searching the entire spaceof spanning trees is that we have not used any syn-tactic constraints to guide the search.
Many lan-guages that allow non-projectivity are still primarilyprojective.
By searching all possible non-projectivetrees, we run the risk of finding extremely bad trees.We address this concern in Section 4.2.2.2 Projective TreesIt is well known that projective dependency pars-ing using edge based factorization can be handledwith the Eisner algorithm (Eisner, 1996).
This al-gorithm has a runtime of O(n3) and has been em-ployed successfully in both generative and discrimi-native parsing models (Eisner, 1996; McDonald etal., 2005).
Furthermore, it is trivial to show thatthe Eisner algorithm solves the maximum projectivespanning tree problem.The Eisner algorithm differs significantly fromthe Chu-Liu-Edmonds algorithm.
First of all, it is abottom-up dynamic programming algorithm as op-posed to a greedy recursive one.
A bottom-up al-gorithm is necessary for the projective case since itmust maintain the nested structural constraint, whichis unnecessary for the non-projective case.2.3 Dependency Trees as MSTs: SummaryIn the preceding discussion, we have shown that nat-ural language dependency parsing can be reduced tofinding maximum spanning trees in directed graphs.This reduction results from edge-based factoriza-tion and can be applied to projective languages with526the Eisner parsing algorithm and non-projective lan-guages with the Chu-Liu-Edmonds maximum span-ning tree algorithm.
The only remaining problem ishow to learn the weight vector w.A major advantage of our approach over otherdependency parsing models is its uniformity andsimplicity.
By viewing dependency structures asspanning trees, we have provided a general frame-work for parsing trees for both projective and non-projective languages.
Furthermore, the resultingparsing algorithms are more efficient than lexi-calized phrase structure approaches to dependencyparsing, allowing us to search the entire space with-out any pruning.
In particular the non-projectiveparsing algorithm based on the Chu-Liu-EdmondsMST algorithm provides true non-projective pars-ing.
This is in contrast to other non-projective meth-ods, such as that of Nivre and Nilsson (2005), whoimplement non-projectivity in a pseudo-projectiveparser with edge transformations.
This formulationalso dispels the notion that non-projective parsing is?harder?
than projective parsing.
In fact, it is eas-ier since non-projective parsing does not need to en-force the non-crossing constraint of projective trees.As a result, non-projective parsing complexity is justO(n2), against the O(n3) complexity of the Eis-ner dynamic programming algorithm, which by con-struction enforces the non-crossing constraint.3 Online Large Margin LearningIn this section, we review the work of McDonald etal.
(2005) for online large-margin dependency pars-ing.
As usual for supervised learning, we assume atraining set T = {(xt,yt)}Tt=1, consisting of pairsof a sentence xt and its correct dependency tree yt.In what follows, dt(x) denotes the set of possibledependency trees for sentence x.The basic idea is to extend the Margin InfusedRelaxed Algorithm (MIRA) (Crammer and Singer,2003; Crammer et al, 2003) to learning with struc-tured outputs, in the present case dependency trees.Figure 4 gives pseudo-code for the MIRA algorithmas presented by McDonald et al (2005).
An on-line learning algorithm considers a single traininginstance at each update to w. The auxiliary vectorv accumulates the successive values of w, so that thefinal weight vector is the average of the weight vec-Training data: T = {(xt, yt)}Tt=11.
w0 = 0; v = 0; i = 02. for n : 1..N3.
for t : 1..T4.
min??
?w(i+1) ?
w(i)???s.t.
s(xt, yt) ?
s(xt, y?)
?
L(yt, y?
), ?y?
?
dt(xt)5. v = v + w(i+1)6. i = i + 17. w = v/(N ?
T )Figure 4: MIRA learning algorithm.tors after each iteration.
This averaging effect hasbeen shown to help overfitting (Collins, 2002).On each update, MIRA attempts to keep the newweight vector as close as possible to the old weightvector, subject to correctly classifying the instanceunder consideration with a margin given by the lossof the incorrect classifications.
For dependencytrees, the loss of a tree is defined to be the numberof words with incorrect parents relative to the correcttree.
This is closely related to the Hamming loss thatis often used for sequences (Taskar et al, 2003).For arbitrary inputs, there are typically exponen-tially many possible parses and thus exponentiallymany margin constraints in line 4 of Figure 4.3.1 Single-best MIRAOne solution for the exponential blow-up in numberof trees is to relax the optimization by using only thesingle margin constraint for the tree with the highestscore, s(x,y).
The resulting online update (to beinserted in Figure 4, line 4) would then be:min?
?w(i+1) ?
w(i)??s.t.
s(xt,yt) ?
s(xt,y?)
?
L(yt,y?
)where y?
= arg maxy?
s(xt,y?
)McDonald et al (2005) used a similar update withk constraints for the k highest-scoring trees, andshowed that small values of k are sufficient toachieve the best accuracy for these methods.
How-ever, here we stay with a single best tree because k-best extensions to the Chu-Liu-Edmonds algorithmare too inefficient (Hou, 1996).This model is related to the averaged perceptronalgorithm of Collins (2002).
In that algorithm, thesingle highest scoring tree (or structure) is used toupdate the weight vector.
However, MIRA aggres-sively updates w to maximize the margin between527the correct tree and the highest scoring tree, whichhas been shown to lead to increased accuracy.3.2 Factored MIRAIt is also possible to exploit the structure of the out-put space and factor the exponential number of mar-gin constraints into a polynomial number of localconstraints (Taskar et al, 2003; Taskar et al, 2004).For the directed maximum spanning tree problem,we can factor the output by edges to obtain the fol-lowing constraints:min?
?w(i+1) ?
w(i)??s.t.
s(l, j) ?
s(k, j) ?
1?
(l, j) ?
yt, (k, j) /?
ytThis states that the weight of the correct incomingedge to the word xj and the weight of all other in-coming edges must be separated by a margin of 1.It is easy to show that when all these constraintsare satisfied, the correct spanning tree and all incor-rect spanning trees are separated by a score at leastas large as the number of incorrect incoming edges.This is because the scores for all the correct arcs can-cel out, leaving only the scores for the errors causingthe difference in overall score.
Since each single er-ror results in a score increase of at least 1, the entirescore difference must be at least the number of er-rors.
For sequences, this form of factorization hasbeen called local lattice preference (Crammer et al,2004).
Let n be the number of nodes in graph Gx.Then the number of constraints is O(n2), since foreach node we must maintain n ?
1 constraints.The factored constraints are in general more re-strictive than the original constraints, so they mayrule out the optimal solution to the original prob-lem.
McDonald et al (2005) examines briefly fac-tored MIRA for projective English dependency pars-ing, but for that application, k-best MIRA performsas well or better, and is much faster to train.4 ExperimentsWe performed experiments on the Czech Prague De-pendency Treebank (PDT) (Hajic?, 1998; Hajic?
et al,2001).
We used the predefined training, develop-ment and testing split of this data set.
Furthermore,we used the automatically generated POS tags thatare provided with the data.
Czech POS tags are verycomplex, consisting of a series of slots that may ormay not be filled with some value.
These slots rep-resent lexical and grammatical properties such asstandard POS, case, gender, and tense.
The resultis that Czech POS tags are rich in information, butquite sparse when viewed as a whole.
To reducesparseness, our features rely only on the reducedPOS tag set from Collins et al (1999).
The num-ber of features extracted from the PDT training setwas 13, 450, 672, using the feature set outlined byMcDonald et al (2005).Czech has more flexible word order than Englishand as a result the PDT contains non-projective de-pendencies.
On average, 23% of the sentences inthe training, development and test sets have at leastone non-projective dependency.
However, less than2% of total edges are actually non-projective.
There-fore, handling non-projective edges correctly has arelatively small effect on overall accuracy.
To showthe effect more clearly, we created two Czech datasets.
The first, Czech-A, consists of the entire PDT.The second, Czech-B, includes only the 23% of sen-tences with at least one non-projective dependency.This second set will allow us to analyze the effec-tiveness of the algorithms on non-projective mate-rial.
We compared the following systems:1.
COLL1999: The projective lexicalized phrase-structureparser of Collins et al (1999).2.
N&N2005: The pseudo-projective parser of Nivre andNilsson (2005).3.
McD2005: The projective parser of McDonald et al(2005) that uses the Eisner algorithm for both training andtesting.
This system uses k-best MIRA with k=5.4.
Single-best MIRA: In this system we use the Chu-Liu-Edmonds algorithm to find the best dependency tree forSingle-best MIRA training and testing.5.
Factored MIRA: Uses the quadratic set of constraintsbased on edge factorization as described in Section 3.2.We use the Chu-Liu-Edmonds algorithm to find the besttree for the test data.4.1 ResultsResults are shown in Table 1.
There are two mainmetrics.
The first and most widely recognized is Ac-curacy, which measures the number of words thatcorrectly identified their parent in the tree.
Completemeasures the number of sentences in which the re-sulting tree was completely correct.Clearly, there is an advantage in using the Chu-Liu-Edmonds algorithm for Czech dependency pars-528Czech-A Czech-BAccuracy Complete Accuracy CompleteCOLL1999 82.8 - - -N&N2005 80.0 31.8 - -McD2005 83.3 31.3 74.8 0.0Single-best MIRA 84.1 32.2 81.0 14.9Factored MIRA 84.4 32.3 81.5 14.3Table 1: Dependency parsing results for Czech.
Czech-B is the subset of Czech-A containing only sentenceswith at least one non-projective dependency.ing.
Even though less than 2% of all dependenciesare non-projective, we still see an absolute improve-ment of up to 1.1% in overall accuracy over theprojective model.
Furthermore, when we focus onthe subset of data that only contains sentences withat least one non-projective dependency, the effectis amplified.
Another major improvement here isthat the Chu-Liu-Edmonds non-projective MST al-gorithm has a parsing complexity of O(n2), versusthe O(n3) complexity of the projective Eisner algo-rithm, which in practice leads to improvements inparsing time.
The results also show that in termsof Accuracy, factored MIRA performs better thansingle-best MIRA.
However, for the factored model,we do have O(n2) margin constraints, which re-sults in a significant increase in training time oversingle-best MIRA.
Furthermore, we can also see thatthe MST parsers perform favorably compared to themore powerful lexicalized phrase-structure parsers,such as those presented by Collins et al (1999) andZeman (2004) that use expensive O(n5) parsing al-gorithms.
We should note that the results in Collinset al (1999) are different then reported here due todifferent training and testing data sets.One concern raised in Section 2.2.1 is that search-ing the entire space of non-projective trees couldcause problems for languages that are primarily pro-jective.
However, as we can see, this is not a prob-lem.
This is because the model sets its weights withrespect to the parsing algorithm and will disfavorfeatures over unlikely non-projective edges.Since the space of projective trees is a subset ofthe space of non-projective trees, it is natural to won-der how the Chu-Liu-Edmonds parsing algorithmperforms on projective data since it is asymptoticallybetter than the Eisner algorithm.
Table 2 shows theresults for English projective dependency trees ex-tracted from the Penn Treebank (Marcus et al, 1993)using the rules of Yamada and Matsumoto (2003).EnglishAccuracy CompleteMcD2005 90.9 37.5Single-best MIRA 90.2 33.2Factored MIRA 90.2 32.3Table 2: Dependency parsing results for English us-ing spanning tree algorithms.This shows that for projective data sets, trainingand testing with the Chu-Liu-Edmonds algorithm isworse than using the Eisner algorithm.
This is notsurprising since the Eisner algorithm uses the a pri-ori knowledge that all trees are projective.5 DiscussionWe presented a general framework for parsing de-pendency trees based on an equivalence to maxi-mum spanning trees in directed graphs.
This frame-work provides natural and efficient mechanismsfor parsing both projective and non-projective lan-guages through the use of the Eisner and Chu-Liu-Edmonds algorithms.
To learn these structures weused online large-margin learning (McDonald et al,2005) that empirically provides state-of-the-art per-formance for Czech.A major advantage of our models is the abil-ity to naturally model non-projective parses.
Non-projective parsing is commonly considered moredifficult than projective parsing.
However, underour framework, we show that the opposite is actuallytrue that non-projective parsing has a lower asymp-totic complexity.
Using this framework, we pre-sented results showing that the non-projective modeloutperforms the projective model on the Prague De-pendency Treebank, which contains a small numberof non-projective edges.Our method requires a tree score that decomposesaccording to the edges of the dependency tree.
Onemight hope that the method would generalize to529include features of larger substructures.
Unfortu-nately, that would make the search for the best treeintractable (Ho?ffgen, 1993).AcknowledgmentsWe thank Lillian Lee for bringing an importantmissed connection to our attention, and Koby Cram-mer for his help with learning algorithms.
This workhas been supported by NSF ITR grants 0205448 and0428193.ReferencesY.J.
Chu and T.H.
Liu.
1965.
On the shortest arbores-cence of a directed graph.
Science Sinica, 14:1396?1400.M.
Collins, J.
Hajic?, L. Ramshaw, and C. Tillmann.
1999.A statistical parser for Czech.
In Proc.
ACL.M.
Collins.
2002.
Discriminative training methods forhidden Markov models: Theory and experiments withperceptron algorithms.
In Proc.
EMNLP.T.H.
Cormen, C.E.
Leiserson, and R.L.
Rivest.
1990.
In-troduction to Algorithms.
MIT Press/McGraw-Hill.K.
Crammer and Y.
Singer.
2003.
Ultraconservative on-line algorithms for multiclass problems.
JMLR.K.
Crammer, O. Dekel, S. Shalev-Shwartz, and Y. Singer.2003.
Online passive aggressive algorithms.
In Proc.NIPS.K.
Crammer, R. McDonald, and F. Pereira.
2004.
Newlarge margin algorithms for structured prediction.
InLearning with Structured Outputs Workshop (NIPS).A.
Culotta and J. Sorensen.
2004.
Dependency tree ker-nels for relation extraction.
In Proc.
ACL.Y.
Ding and M. Palmer.
2005.
Machine translation usingprobabilistic synchronous dependency insertion gram-mars.
In Proc.
ACL.J.
Edmonds.
1967.
Optimum branchings.
Journal of Re-search of the National Bureau of Standards, 71B:233?240.J.
Eisner.
1996.
Three new probabilistic models for de-pendency parsing: An exploration.
In Proc.
COLING.J.
Hajic?, E. Hajicova, P. Pajas, J. Panevova, P. Sgall, andB.
Vidova Hladka.
2001.
The Prague DependencyTreebank 1.0 CDROM.
Linguistics Data ConsortiumCat.
No.
LDC2001T10.J.
Hajic?.
1998.
Building a syntactically annotated cor-pus: The Prague dependency treebank.
Issues of Va-lency and Meaning, pages 106?132.H.
Hirakawa.
2001.
Semantic dependency analysismethod for Japanese based on optimum tree search al-gorithm.
In Proc.
of PACLING.Klaus-U.
Ho?ffgen.
1993.
Learning and robust learningof product distributions.
In Proceedings of COLT?93,pages 77?83.W.
Hou.
1996.
Algorithm for finding the first k shortestarborescences of a digraph.
Mathematica Applicata,9(1):1?4.R.
Hudson.
1984.
Word Grammar.
Blackwell.G.
Leonidas.
2003.
Arborescence optimization problemssolvable by Edmonds?
algorithm.
Theoretical Com-puter Science, 301:427 ?
437.M.
Marcus, B. Santorini, and M. Marcinkiewicz.
1993.Building a large annotated corpus of English: the PennTreebank.
Computational Linguistics, 19(2):313?330.R.
McDonald, K. Crammer, and F. Pereira.
2005.
Onlinelarge-margin training of dependency parsers.
In Proc.ACL.J.
Nivre and J. Nilsson.
2005.
Pseudo-projective depen-dency parsing.
In Proc.
ACL.J.
Nivre and M. Scholz.
2004.
Deterministic dependencyparsing of english text.
In Proc.
COLING.Y.
Shinyama, S. Sekine, K. Sudo, and R. Grishman.2002.
Automatic paraphrase acquisition from news ar-ticles.
In Proc.
HLT.R.
Snow, D. Jurafsky, and A. Y. Ng.
2004.
Learningsyntactic patterns for automatic hypernym discovery.In NIPS 2004.R.E.
Tarjan.
1977.
Finding optimum branchings.
Net-works, 7:25?35.B.
Taskar, C. Guestrin, and D. Koller.
2003.
Max-marginMarkov networks.
In Proc.
NIPS.B.
Taskar, D. Klein, M. Collins, D. Koller, and C. Man-ning.
2004.
Max-margin parsing.
In Proc.
EMNLP.W.
Wang and M. P. Harper.
2004.
A statistical constraintdependency grammar (CDG) parser.
In Workshop onIncremental Parsing: Bringing Engineering and Cog-nition Together (ACL).H.
Yamada and Y. Matsumoto.
2003.
Statistical depen-dency analysis with support vector machines.
In Proc.IWPT.D.
Zeman.
2004.
Parsing with a Statistical DependencyModel.
Ph.D. thesis, Univerzita Karlova, Praha.530
