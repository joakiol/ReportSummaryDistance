Proceedings of NAACL HLT 2007, Companion Volume, pages 13?16,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsJOINT VERSUS INDEPENDENT PHONOLOGICAL FEATUREMODELS WITHIN CRF PHONE RECOGNITIONIlana Bromberg?, Jeremy Morris?, and Eric Fosler-Lussier??
?Department of Linguistics?Department of Computer Science and EngineeringThe Ohio State University, Columbus, OHbromberg@ling.ohio-state.edu, {morrijer, fosler}@cse.ohio-state.eduAbstractWe compare the effect of joint modelingof phonological features to independentfeature detectors in a Conditional RandomFields framework.
Joint modeling of fea-tures is achieved by deriving phonologicalfeature posteriors from the posterior prob-abilities of the phonemes.
We find thatjoint modeling provides superior perfor-mance to the independent models on theTIMIT phone recognition task.
We ex-plore the effects of varying relationshipsbetween phonological features, and sug-gest that in an ASR system, phonologicalfeatures should be handled as correlated,rather than independent.1 IntroductionPhonological features have received attention as alinguistically-based representation for sub-word in-formation in automatic speech recognition.
Thesesub-phonetic features allow for a more refined repre-sentation of speech by allowing for temporal desyn-chronization between articulators, and help accountfor some phonological changes common in sponta-neous speech, such as devoicing (Kirchhoff, 1999;Livescu, 2005).
A number of methods have been de-veloped for detecting acoustic phonological featuresand related acoustic landmarks directly from datausing Multi-Layer Perceptrons (Kirchhoff, 1999),Support Vector Machines (Hasegawa-Johnson et al,2005; Sharenborg et al, 2006), or Hidden MarkovModels (Li and Lee, 2005).
These techniquestypically assume that acoustic phonological featureevents are independent for ease of modeling.In one study that broke the independence assump-tion (Chang et al, 2001), the investigators devel-oped conditional detectors: MLP detectors of acous-tic phonological features that are hierarchically de-pendent on a different phonological class.
In (Ra-jamanohar and Fosler-Lussier, 2005) it was shownthat such a conditional training of detectors tendedto have correlated frame errors, and that improve-ments in detection could be obtained by trainingjoint detectors.
For many features, the best detectorcan be obtained by collapsing MLP phone posteriorsinto feature classes by marginalizing across phoneswithin a class.
This was shown only for frame-levelclassification rather than phone recognition.Posterior estimates of phonological featureclasses, as in Table 1, particularly those derivedfrom MLPs, have been used as input to HMMs(Launay et al, 2002), Dynamic Bayesian Networks(DBNs) (Frankel et al, 2004; Livescu, 2005),and Conditional Random Fields (CRFs) (Morrisand Fosler-Lussier, 2006).
Here we evaluatephonological feature detectors created from MLPphone posterior estimators (joint feature models)rather than the independently trained MLP featuredetectors used in previous work.2 Conditional Random FieldsCRFs (Lafferty et al, 2001) are a joint model ofa label sequence conditioned on a set of inputs.No independence is assumed among the input; theCRF model discriminates between hypothesized la-bel sequences according to an exponential functionof weighted feature functions:P (y|x) ?
exp?i(S(x,y, i) + T(x,y, i)) (1)13Class Feature ValuesSONORITY Vowel, Obstruent, Sonorant, Syllabic, SilenceVOICE Voiced, Unvoiced, N/AMANNER Fricative, Stop, Stop-Closure, Flap, Nasal, Approximant, Nasalflap, N/APLACE Labial, Dental, Alveolar, Palatal, Velar, Glottal, Lateral, Rhotic, N/AHEIGHT High, Mid, Low, Lowhigh, Midhigh, N/AFRONT Front, Back, Central, Backfront, N/AROUND Round, Nonround, Roundnonround, Nonroundround, N/ATENSE Tense, Lax N/ATable 1: Phonetic feature classes and associated valueswhere P (y|x) is the probability of label sequencey given an input frame sequence x, i is the frameindex, and S and T are a set of state feature functionsand a set of transition feature functions, defined as:S(x, y, i) =?j?jsj(y, x, i), and (2)T (x, y, i) =?k?ktk(yi?1, yi, x, i) (3)where ?
and ?
are weights determined by the learn-ing algorithm.
In NLP applications, the componentfeature functions sj and tk are typically realized asbinary indicator functions indicating the presence orabsence of a feature, but in ASR applications it ismore typical to utilize real-valued functions, such asthose derived from the sufficient statistics of Gaus-sians (e.g., (Gunawardana et al, 2005)).We can use posterior estimates of phone classes orphonological feature classes from MLPs as featurefunctions (inputs) within the CRF model.
A moredetailed description of this CRF paradigm can befound in (Morris and Fosler-Lussier, 2006), whichshows that the results of phone recognition usingCRFs is comparable to that of HMMs or Tandemsystems, with fewer constraints being imposed onthe model.
State feature functions in our system aredefined such thats?,f (yi,x, i) ={NNf (xi), ifyi = ?0, otherwise(4)where the MLP output for feature f at time i isNNf (xi).
This allows for an association betweena phone ?
and a feature f (even if f is traditionallynot associated with ?
).In this study, we experiment with different meth-ods of generating these feature functions.
In variousexperiments, they are generated by training MLPphone detectors, by evaluating the feature informa-tion inherent in the MLP phone posteriors, and bytraining independent MLPs to detect the various fea-tures within the classes described.
The use of CRFsallows us to explore the dependencies among featureclasses, as well as the usefulness of phone posteriorsversus feature classes as inputs.3 Experimental SetupWe use the TIMIT speech corpus for all training andtesting (Garofolo et al, 1993).
The acoustic datais manually labeled at the phonetic level, and wepropagate this phonetic label information to everyframe of data.
For the feature analyses, we employa lookup table that defines each phone in terms of8 feature classes, as shown in Table 1.
We extractacoustic features in the form of 12th order PLP fea-tures plus delta coefficients.
We then use these asinputs to several sets of neural networks using theICSI QuickNet MLP neural network software (John-son, 2004), with the 39 acoustic features as input, avarying number of phone or feature class posteriorsas output, and 1000 hidden nodes.4 Joint Phone Posteriors vs. IndependentFeature PosteriorsThe first experiment contrasts joint versus indepen-dent feature modeling within the CRF system.
Wecompare a set of phonological feature probabilitiesderived from the phone posteriors (a joint model)with MLP phone posteriors and with independentlytrained MLP phonological feature posteriors.The inputs to the first CRF are sets of 61 state fea-ture functions from the phonemic MLP posteriors,each function is an estimate of the posterior proba-14Input Type.
Phn.
Accuracy Phn.
CorrectPhones 67.27 68.77Features 65.25 66.65Phn.
?
Feat.
66.45 67.94Table 2: Results for Exp.
1: Phone and feature pos-teriors as input to the CRF phone recognitionbility of one phone.
The inputs to the second CRFmodel are sets of 44 functions corresponding to thephonological features listed in Table 1.
The CRFmodels are trained to associate these feature func-tions with phoneme labels, incorporating the pat-terns of variation seen in the MLPs.The results show that phone-based posteri-ors produce better phone recognition results thanindependently-trained phonological features.
Thiscould be due in part to the larger number of param-eters in the system, but it could also be due to thejoint modeling that occurs in the phone classifier.In order to equalize the feature spaces, we use theoutput of the phoneme classifier to derive phonolog-ical feature posteriors.
In each frame we sum theMLP phone posteriors of all phones that contain agiven feature.
For instance, in the first frame, forthe feature LOW, we sum the posterior estimates at-tributed to the phones aa, ae and ao.
This is repeatedfor each feature in each frame.
The CRF model istrained on these data and tested accordingly.
The re-sults are significantly better (p?.001) than the previ-ous features model, but are significantly worse thanthe phone posteriors (p?.005).The results of Experiment 1 confirm the hypoth-esis of (Rajamanohar and Fosler-Lussier, 2005) thatjoint modeling using several types of feature infor-mation is superior to individual modeling in phonerecognition, where only phoneme information isused.
The difference between the phone posteriorsand individual feature posteriors seems to be relatedboth to the larger CRF parameter space with largerinput, and the joint modeling provided by phoneposteriors.5 Phonological Feature Class AnalysisIn the second experiment, we examine the influenceof each feature class on the accuracy of the recog-nizer.
We iteratively remove the set of state fea-ture functions corresponding to each feature classClass Removed Feats.
Phn.
Acc.
Phn.
Corr.None 44 65.25 66.65Sonority 39 65.15 66.58Voice 41 63.60* 65.03*Manner 36 58.92* 60.60*Place 35 53.22* 55.13*Height 38 62.58* 64.07*Front 39 64.51* 65.95*Round 39 65.19 66.64Tense 41 64.20* 65.65** p?.05, different from no features removedTable 3: Results of Exp.
2: Removing featureclasses from the inputfrom the input to the CRF.
The original functionsare the output of the independently-trained featureclass MLPs.
The phone recognition accuracy for theCRF having removed each class is shown in Table 3.In Table 4 we show how removing each feature classaffects the labeling of vowels and consonants.Manner provides an example of the influence of asingle feature class.
Both the Accuracy and Correct-ness scores decrease significantly when features as-sociated with Manner are removed.
Manner featuresdistinguish consonants but not vowels, so the effectis concentrated on the recognition of consonants.The results of Experiment 2 show that certain fea-ture classes are redundant from the point of view ofphone recognition.
In English, Round is correlatedwith Front.
When we remove Round, the phonemesremain uniquely identified by the other classes.
Thesame is true for the Sonority class.
The results showthat the inclusion of these redundant features is notdetrimental to the recognition accuracy.
Accuracyand Correctness improve non-significantly when theredundant features are included.Clearly, the ?independent?
phonological featurestreams are not truly independent.
Otherwise, per-formance would decrease overall as we removedeach feature class, assuming predictiveness.Removal of Place causes a slight worsening ofrecognition of vowels.
This is surprising, becausePlace does not characterize vowels.
An analysis ofthe MLP activations showed that the detector forPlace=N/A is a stronger indicator for vowels thanis the Sonority=Vowel detector.
This is especiallytrue for the vowel ax, which is frequent in the data,15Class Removed Percent Correct:Vowels ConsonantsNone 62.68 68.91Sonority 62.18 69.08Voice 62.39 66.53*Manner 61.84 59.89*Place 60.77* 51.94*Height 55.92* 68.69Frontness 60.80* 68.87Roundness 62.25 69.13Tenseness 60.15* 68.76* p?.05, different from no features removedTable 4: Effect of removing each feature class onrecognition accuracy of vowels and consonantsthus greatly influences the vowel recognition statis-tic.
Removing the Place detectors leads to a loss invowel vs. consonant information.
This results in anincreased number of consonant for vowel substitu-tions (from 560 to 976), thus a decrease in vowelrecognition accuracy.Besides extending the findings in (Rajamanoharand Fosler-Lussier, 2005), this provides a cautionarytale for incorporating redundant phonological fea-ture estimators into ASR: these systems need to beable to handle correlated input, either by design (asin a CRF), through full or semi-tied covariance ma-trices in HMMs, or by including the appropriate sta-tistical dependencies in DBNs.6 SummaryWe have shown the effect of using joint model-ing of phonetic feature information in conjunctionwith the use of CRFs as a discriminative classifier.Phonetic posteriors, as joint models of phonologicalfeatures, provide superior phone recognition perfor-mance over independently-trained phonological fea-ture models.
We also find that redundant features areoften modeled well within the CRF framework.7 AcknowledgmentsThe authors thank the International Computer Sci-ence Institute for providing the neural network soft-ware.
The authors also thank four anonymous re-viewers.
This work was supported by NSF ITRgrant IIS-0427413; the opinions and conclusions ex-pressed in this work are those of the authors and notof any funding agency.ReferencesS.
Chang, S. Greenberg, and M. Wester.
2001.
An eli-tist approach to articulatory-acoustic feature classifica-tion.
In Interspeech.J.
Frankel, M. Wester, and S. King.
2004.
Articulatoryfeature recognition using dynamic bayesian networks.In ICSLP.J.
Garofolo, L. Lamel, W. Fisher, J. Fiscus, D. Pallett, andN.
Dahlgren.
1993.
DARPA TIMIT acoustic-phoneticcontinuous speech corpus.
Technical Report NISTIR4930, National Institute of Standards and Technology,Gaithersburg, MD, February.
Speech Data publishedon CD-ROM: NIST Speech Disc 1-1.1, October 1990.A.
Gunawardana, M. Mahajan, A. Acero, and J. Platt.2005.
Hidden conditional random fields for phoneclassification.
In Interspeech.M.
Hasegawa-Johnson et al 2005.
Landmark-basedspeech recognition: Report of the 2004 Johns HopkinsSummer Workshop.
In ICASSP.D.
Johnson.
2004.
ICSI Quicknet software package.http://www.icsi.berkeley.edu/Speech/qn.html.K.
Kirchhoff.
1999.
Robust Speech Recognition UsingArticulatory Information.
Ph.D. thesis, University ofBielefeld.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proceedings ofthe 18th International Conference on Machine Learn-ing.B.
Launay, O. Siohan, A. C. Surendran, and C.-H. Lee.2002.
Towards knowledge based features for large vo-cabulary automatic speech recognition.
In ICASSP.J.
Li and C.-H. Lee.
2005.
On designing and evaluatingspeech event detectors.
In Interspeech.K.
Livescu.
2005.
Feature-Based Pronunciation Model-ing for Automatic Speech Recognition.
Ph.D. thesis,MIT.J.
Morris and E. Fosler-Lussier.
2006.
Combining pho-netic attributes using conditional random fields.
In In-terspeech.M.
Rajamanohar and E. Fosler-Lussier.
2005.
An evalu-ation of hierarchical articulatory feature detectors.
InIEEE Automatic Speech Recogntion and Understand-ing Workshop.O.
Sharenborg, V. Wan, and R.K. Moore.
2006.
Cap-turing fine-phonetic variation in speech through auto-matic classification of articulatory features.
In ITRWon Speech Recognition and Intrinsic Variation.16
