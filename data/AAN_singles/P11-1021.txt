Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 201?210,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsA Large Scale Distributed Syntactic, Semantic and LexicalLanguage Model for Machine TranslationMing Tan Wenli Zhou Lei Zheng Shaojun WangKno.e.sis CenterDepartment of Computer Science and EngineeringWright State UniversityDayton, OH 45435, USA{tan.6,zhou.23,lei.zheng,shaojun.wang}@wright.eduAbstractThis paper presents an attempt at buildinga large scale distributed composite languagemodel that simultaneously accounts for localword lexical information, mid-range sentencesyntactic structure, and long-span documentsemantic content under a directed Markov ran-dom field paradigm.
The composite languagemodel has been trained by performing a con-vergent N-best list approximate EM algorithmthat has linear time complexity and a follow-up EM algorithm to improve word predictionpower on corpora with up to a billion tokensand stored on a supercomputer.
The largescale distributed composite language modelgives drastic perplexity reduction over n-grams and achieves significantly better trans-lation quality measured by the BLEU scoreand ?readability?
when applied to the task ofre-ranking the N-best list from a state-of-the-art parsing-based machine translation system.1 IntroductionThe Markov chain (n-gram) source models, whichpredict each word on the basis of previous n-1words, have been the workhorses of state-of-the-artspeech recognizers and machine translators that helpto resolve acoustic or foreign language ambiguitiesby placing higher probability on more likely originalunderlying word strings.
Research groups (Brants etal., 2007; Zhang, 2008) have shown that using animmense distributed computing paradigm, up to 6-grams can be trained on up to billions and trillionsof words, yielding consistent system improvements,but Zhang (2008) did not observe much improve-ment beyond 6-grams.
Although the Markov chainsare efficient at encoding local word interactions, then-gram model clearly ignores the rich syntactic andsemantic structures that constrain natural languages.As the machine translation (MT) working groupsstated on page 3 of their final report (Lavie et al,2006), ?These approaches have resulted in small im-provements in MT quality, but have not fundamen-tally solved the problem.
There is a dire need for de-veloping novel approaches to language modeling.
?Wang et al (2006) integrated n-gram, structuredlanguage model (SLM) (Chelba and Jelinek, 2000)and probabilistic latent semantic analysis (PLSA)(Hofmann, 2001) under the directed MRF frame-work (Wang et al, 2005) and studied the stochas-tic properties for the composite language model.They derived a generalized inside-outside algorithmto train the composite language model from a gen-eral EM (Dempster et al, 1977) by following Je-linek?s ingenious definition of the inside and outsideprobabilities for SLM (Jelinek, 2004) with 6th orderof sentence length time complexity.
Unfortunately,there are no experimental results reported.In this paper, we study the same composite lan-guage model.
Instead of using the 6th order general-ized inside-outside algorithm proposed in (Wang etal., 2006), we train this composite model by a con-vergent N-best list approximate EM algorithm thathas linear time complexity and a follow-up EM al-gorithm to improve word prediction power.
We con-duct comprehensive experiments on corpora with 44million tokens, 230 million tokens, and 1.3 billiontokens and compare perplexity results with n-grams(n=3,4,5 respectively) on these three corpora, weobtain drastic perplexity reductions.
Finally, we ap-201ply our language models to the task of re-rankingthe N-best list from Hiero (Chiang, 2005; Chiang,2007), a state-of-the-art parsing-based MT system,we achieve significantly better translation qualitymeasured by the BLEU score and ?readability?.2 Composite language modelThe n-gram language model is essentially a wordpredictor that given its entire document history itpredicts next word wk+1 based on the last n-1 wordswith probability p(wk+1|wkk?n+2) where wkk?n+2 =wk?n+2, ?
?
?
, wk .The SLM (Chelba and Jelinek, 1998; Chelba andJelinek, 2000) uses syntactic information beyondthe regular n-gram models to capture sentence levellong range dependencies.
The SLM is based on sta-tistical parsing techniques that allow syntactic anal-ysis of sentences; it assigns a probability p(W,T ) toevery sentence W and every possible binary parseT .
The terminals of T are the words of W with POStags, and the nodes of T are annotated with phraseheadwords and non-terminal labels.
Let W be a sen-tence of length n words to which we have prependedthe sentence beginning marker <s> and appendedthe sentence end marker </s> so that w0 =<s>and wn+1 =</s>.
Let Wk = w0, ?
?
?
, wk be theword k-prefix of the sentence ?
the words from thebeginning of the sentence up to the current positionk and WkTk the word-parse k-prefix.
A word-parsek-prefix has a set of exposed heads h?m, ?
?
?
, h?1,with each head being a pair (headword, non-terminallabel), or in the case of a root-only tree (word,POS tag).
An m-th order SLM (m-SLM) hasthree operators to generate a sentence: WORD-PREDICTOR predicts the next word wk+1 basedon the m left-most exposed headwords h?1?m =h?m, ?
?
?
, h?1 in the word-parse k-prefix with prob-ability p(wk+1|h?1?m), and then passes control to theTAGGER; the TAGGER predicts the POS tag tk+1to the next word wk+1 based on the next word wk+1and the POS tags of the m left-most exposed head-words h?1?m in the word-parse k-prefix with prob-ability p(tk+1|wk+1, h?m.tag, ?
?
?
, h?1.tag); theCONSTRUCTOR builds the partial parse Tk fromTk?1, wk, and tk in a series of moves ending withNULL, where a parse move a is made with proba-bility p(a|h?1?m); a ?
A={(unary, NTlabel), (adjoin-left, NTlabel), (adjoin-right, NTlabel), null}.
Oncethe CONSTRUCTOR hits NULL, it passes controlto the WORD-PREDICTOR.
See detailed descrip-tion in (Chelba and Jelinek, 2000).A PLSA model (Hofmann, 2001) is a gener-ative probabilistic model of word-document co-occurrences using the bag-of-words assumption de-scribed as follows: (i) choose a document d withprobability p(d); (ii) SEMANTIZER: select a se-mantic class g with probability p(g|d); and (iii)WORD-PREDICTOR: pick a word w with proba-bility p(w|g).
Since only one pair of (d,w) is beingobserved, as a result, the joint probability model isa mixture of log-linear model with the expressionp(d,w) = p(d)?g p(w|g)p(g|d).
Typically, thenumber of documents and vocabulary size are muchlarger than the size of latent semantic class variables.Thus, latent semantic class variables function as bot-tleneck variables to constrain word occurrences indocuments.When combining n-gram, m order SLM andPLSA models together to build a composite gen-erative language model under the directed MRFparadigm (Wang et al, 2005; Wang et al, 2006),the TAGGER and CONSTRUCTOR in SLM andSEMANTIZER in PLSA remain unchanged; how-ever the WORD-PREDICTORs in n-gram, m-SLMand PLSA are combined to form a stronger WORD-PREDICTOR that generates the next word, wk+1,not only depending on the m left-most exposedheadwords h?1?m in the word-parse k-prefix but alsoits n-gram history wkk?n+2 and its semantic con-tent gk+1.
The parameter for WORD-PREDICTORin the composite n-gram/m-SLM/PLSA languagemodel becomes p(wk+1|wkk?n+2h?1?mgk+1).
The re-sulting composite language model has an even morecomplex dependency structure but with more ex-pressive power than the original SLM.
Figure 1 il-lustrates the structure of a composite n-gram/m-SLM/PLSA language model.The composite n-gram/m-SLM/PLSA lan-guage model can be formulated as a directedMRF model (Wang et al, 2006) with lo-cal normalization constraints for the param-eters of each model component, WORD-PREDICTOR, TAGGER, CONSTRUCTOR,SEMANTIZER, i.e.,?w?V p(w|w?1?n+1h?1?mg) =1,?t?O p(t|wh?1?m.tag) = 1,?a?A p(a|h?1?m) =1,?g?G p(g|d) = 1.202..................gwg g g...... ...... ...... ......</s>dkk?n+2j+1......<s> w1 ii............g1wk wk+1gk+1h?1h?2h?mj+1wwjg j......k?n+2w......Figure 1: A composite n-gram/m-SLM/PLSA languagemodel where the hidden information is the parse treeT and semantic content g. The WORD-PREDICTORgenerates the next word wk+1 with probabilityp(wk+1|wkk?n+2h?1?mgk+1) instead of p(wk+1|wkk?n+2),p(wk+1|h?1?m) and p(wk+1|gk+1) respectively.3 Training algorithmUnder the composite n-gram/m-SLM/PLSA lan-guage model, the likelihood of a training corpus D,a collection of documents, can be written asL(D, p) =Yd?DYlXGlXT lPp(W l, T l, Gl|d)!!!
(1)where (W l, T l, Gl, d) denote the joint sequence ofthe lth sentence W l with its parse tree structure T land semantic annotation string Gl in document d.This sequence is produced by a unique sequenceof model actions: WORD-PREDICTOR, TAGGER,CONSTRUCTOR, SEMANTIZER moves, its prob-ability is obtained by chaining the probabilities ofthese movesPp(W l, T l, Gl|d)=Yg?G0@p(g|d)#(g,Wl,Gl,d) Yh?1,???
,h?m?HYw,w?1 ,???
,w?n+1?Vp(w|w?1?n+1h?1?mg)#(w?1?n+1wh?1?mg,Wl,T l,Gl,d)Yt?Op(t|wh?1?m.tag)#(t,wh?1?m.tag,Wl,T l,d)Ya?Ap(a|h?1?m)#(a,h?1?m,Wl,T l,d)!where #(g,W l, Gl, d) is the count of seman-tic content g in semantic annotation stringGl of the lth sentence W l in document d,#(w?1?n+1wh?1?mg,W l, T l, Gl, d) is the countof n-grams, its m most recent exposed headwordsand semantic content g in parse T l and semanticannotation string Gl of the lth sentence W l indocument d, #(twh?1?m.tag,W l, T l, d) is the countof tag t predicted by word w and the tags of mmost recent exposed headwords in parse tree T lof the lth sentence W l in document d, and finally#(ah?1?m,W l, T l, d) is the count of constructormove a conditioning on m exposed headwords h?1?min parse tree T l of the lth sentence W l in documentd.The objective of maximum likelihood estimationis to maximize the likelihood L(D, p) respect tomodel parameters.
For a given sentence, its parsetree and semantic content are hidden and the num-ber of parse trees grows faster than exponential withsentence length, Wang et al (2006) have derived ageneralized inside-outside algorithm by applying thestandard EM algorithm.
However, the complexity ofthis algorithm is 6th order of sentence length, thus itis computationally too expensive to be practical fora large corpus even with the use of pruning on charts(Jelinek and Chelba, 1999; Jelinek, 2004).3.1 N-best list approximate EMSimilar to SLM (Chelba and Jelinek, 2000), weadopt an N -best list approximate EM re-estimationwith modular modifications to seamlessly incorpo-rate the effect of n-gram and PLSA components.Instead of maximizing the likelihood L(D, p), wemaximize the N -best list likelihood,maxT ?NL(D, p, T ?N ) =Yd?DYlmaxT ?lN?T ?NXGl0@XT l?T ?lN ,||T ?lN ||=NPp(W l, T l, Gl|d)1A1A1Awhere T ?lN is a set of N parse trees for sentence W lin document d and || ?
|| denotes the cardinality andT ?N is a collection of T ?lN for sentences over entirecorpus D.The N-best list approximate EM involves twosteps:1.
N-best list search: For each sentence W in doc-ument d, find N -best parse trees,T lN = arg maxT ?lNnXGlXT l?T ?lNPp(W l, T l, Gl|d), ||T ?lN || = Noand denote TN as the collection of N -best listparse trees for sentences over entire corpus Dunder model parameter p.2.
EM update: Perform one iteration (or severaliterations) of EM algorithm to estimate model203parameters that maximizes N -best-list likeli-hood of the training corpus D,L?
(D, p,TN ) =Yd?D(Yl(XGl(XT l?T lN?TNPp(W l, T l, Gl|d))))That is,(a) E-step: Compute the auxiliary function ofthe N -best-list likelihoodQ?
(p?, p, TN ) =Xd?DXlXGlXT l?T lN?TNPp(T l, Gl|W l, d)logPp?
(W l, T l, Gl|d)(b) M-step: Maximize Q?
(p?, p,TN ) with re-spect to p?
to get new update for p.Iterate steps (1) and (2) until the convergence of theN -best-list likelihood.
Due to space constraints, weomit the proof of the convergence of the N-best listapproximate EM algorithm which uses Zangwill?sglobal convergence theorem (Zangwill, 1969).N -best list search strategy: To extract the N -best parse trees, we adopt a synchronous, multi-stack search strategy that is similar to the one in(Chelba and Jelinek, 2000), which involves a setof stacks storing partial parses of the most likelyones for a given prefix Wk and the less probableparses are purged.
Each stack contains hypotheses(partial parses) that have been constructed by thesame number of WORD-PREDICTOR and the samenumber of CONSTRUCTOR operations.
The hy-potheses in each stack are ranked according to thelog(?Gk Pp(Wk, Tk, Gk|d)) score with the higheston top, where Pp(Wk, Tk, Gk|d) is the joint prob-ability of prefix Wk = w0, ?
?
?
, wk with its parsestructure Tk and semantic annotation string Gk =g1, ?
?
?
, gk in a document d. A stack vector consistsof the ordered set of stacks containing partial parseswith the same number of WORD-PREDICTOR op-erations but different number of CONSTRUCTORoperations.
In WORD-PREDICTOR and TAGGERoperations, some hypotheses are discarded due tothe maximum number of hypotheses the stack cancontain at any given time.
In CONSTRUCTORoperation, the resulting hypotheses are discardeddue to either finite stack size or the log-probabilitythreshold: the maximum tolerable difference be-tween the log-probability score of the top-most hy-pothesis and the bottom-most hypothesis at anygiven state of the stack.EM update: Once we have the N -best parse treesfor each sentence in document d and N -best topicsfor document d, we derive the EM algorithm to esti-mate model parameters.In E-step, we compute the expected count ofeach model parameter over sentence W l in docu-ment d in the training corpus D. For the WORD-PREDICTOR and the SEMANTIZER, the numberof possible semantic annotation sequences is expo-nential, we use forward-backward recursive formu-las that are similar to those in hidden Markov mod-els to compute the expected counts.
We define theforward vector ?l(g|d) to be?lk+1(g|d) =XGlkPp(W lk, T lk, wkk?n+2wk+1h?1?mg,Glk|d)that can be recursively computed in a forward man-ner, where W lk is the word k-prefix for sentence W l,T lk is the parse for k-prefix.
We define backwardvector ?l(g|d) to be?lk+1(g|d)=XGlk+1,?Pp(W lk+1,?, T lk+1,?, Glk+1,?|wkk?n+2wk+1h?1?mg, d)that can be computed in a backward manner, hereW lk+1,?
is the subsequence after k+1th word in sen-tence W l, T lk+1,?
is the incremental parse struc-ture after the parse structure T lk+1 of word k+1-prefix W lk+1 that generates parse tree T l, Glk+1,?
isthe semantic subsequence in Gl relevant to W lk+1,?.Then, the expected count of w?1?n+1wh?1?mg for theWORD-PREDICTOR on sentence W l in documentd isXGlPp(T l, Gl|W l, d)#(w?1?n+1wh?1?mg,W l, T l, Gl, d)=XlXk?lk+1(g|d)?lk+1(g|d)p(g|d)?
(wkk?n+2wk+1h?1?mgk+1 = w?1?n+1wh?1?mg)/Pp(W l|d)where ?(?)
is an indicator function and the expectedcount of g for the SEMANTIZER on sentence W lin document d isXGlPp(T l, Gl|W l, d)#(g,W l, Gl, d)=j?1Xk=0?lk+1(g|d)?lk+1(g|d)p(g|d)/Pp(W l|d)For the TAGGER and the CONSTRUCTOR,the expected count of each event of twh?1?m.tagand ah?1?m over parse T l of sentence W l in204document d is the real count appeared in parsetree T l of sentence W l in document d timesthe conditional distribution Pp(T l|W l, d) =Pp(T l,W l|d)/?T l?T l Pp(T l,W l|d) respectively.In M-step, the recursive linear interpolationscheme (Jelinek and Mercer, 1981) is usedto obtain a smooth probability estimate foreach model component, WORD-PREDICTOR,TAGGER, and CONSTRUCTOR.
The TAGGERand CONSTRUCTOR are conditional probabilis-tic models of the type p(u|z1, ?
?
?
, zn) whereu, z1, ?
?
?
, zn belong to a mixed set of words, POStags, NTtags, CONSTRUCTOR actions (u only),and z1, ?
?
?
, zn form a linear Markov chain.
The re-cursive mixing scheme is the standard one amongrelative frequency estimates of different orders k =0, ?
?
?
, n as explained in (Chelba and Jelinek, 2000).The WORD-PREDICTOR is, however, a condi-tional probabilistic model p(w|w?1?n+1h?1?mg) wherethere are three kinds of context w?1?n+1, h?1?m and g,each forms a linear Markov chain.
The model hasa combinatorial number of relative frequency esti-mates of different orders among three linear Markovchains.
We generalize Jelinek and Mercer?s originalrecursive mixing scheme (Jelinek and Mercer, 1981)and form a lattice to handle the situation where thecontext is a mixture of Markov chains.3.2 Follow-up EMAs explained in (Chelba and Jelinek, 2000), for theSLM component, a large fraction of the partial parsetrees that can be used for assigning probability to thenext word do not survive in the synchronous, multi-stack search strategy, thus they are not used in theN-best approximate EM algorithm for the estima-tion of WORD-PREDICTOR to improve its predic-tive power.
To remedy this weakness, we estimateWORD-PREDICTOR using the algorithm below.The language model probability assignment forthe word at position k+1 in the input sentence ofdocument d can be computed asPp(wk+1|Wk, d) =Xh?1?m?Tk;Tk?Zk,gk+1?Gdp(wk+1|wkk?n+2h?1?mgk+1)Pp(Tk|Wk, d)p(gk+1|d) (2)where Pp(Tk|Wk, d) =PGkPp(Wk,Tk,Gk|d)PTk?ZkPGkPp(Wk,Tk,Gk|d)and Zk is the set of all parses present in the stacksat the current stage k during the synchronous multi-stack pruning strategy and it is a function of the wordk-prefix Wk.The likelihood of a training corpus D under thislanguage model probability assignment that usespartial parse trees generated during the process ofthe synchronous, multi-stack search strategy can bewritten asL?
(D, p) =Yd?DYl?XkPp(w(l)k+1|Wlk, d)?
(3)We employ a second stage of parameter re-estimation for p(wk+1|wkk?n+2h?1?mgk+1) andp(gk+1|d) by using EM again to maximizeEquation (3) to improve the predictive power ofWORD-PREDICTOR.3.3 Distributed architectureWhen using very large corpora to train our compos-ite language model, both the data and the parameterscan?t be stored in a single machine, so we have toresort to distributed computing.
The topic of largescale distributed language models is relatively new,and existing works are restricted to n-grams only(Brants et al, 2007; Emami et al, 2007; Zhang etal., 2006).
Even though all use distributed archi-tectures that follow the client-server paradigm, thereal implementations are in fact different.
Zhanget al (2006) and Emami et al (2007) store train-ing corpora in suffix arrays such that one sub-corpusper server serves raw counts and test sentences areloaded in a client.
This implies that when comput-ing the language model probability of a sentence ina client, all servers need to be contacted for each n-gram request.
The approach by Brants et al (2007)follows a standard MapReduce paradigm (Dean andGhemawat, 2004): the corpus is first divided andloaded into a number of clients, and n-gram countsare collected at each client, then the n-gram countsmapped and stored in a number of servers, result-ing in exactly one server being contacted per n-gramwhen computing the language model probability ofa sentence.
We adopt a similar approach to Brantset al and make it suitable to perform iterationsof N -best list approximate EM algorithm, see Fig-ure 2.
The corpus is divided and loaded into a num-ber of clients.
We use a public available parser toparse the sentences in each client to get the initialcounts for w?1?n+1wh?1?mg etc., finish the Map part,and then the counts for a particular w?1?n+1wh?1?mgat different clients are summed up and stored in one205Server 2Server 1 Server LClient 1 Client 2 Client MFigure 2: Distributed architecture is essentially a MapRe-duce paradigm: clients store partitioned data and per-form E-step: compute expected counts, this is Map;servers store parameters (counts) for M-step wherecounts of w?1?n+1wh?1?mg are hashed by word w?1 (orh?1) and its topic g to evenly distribute these model pa-rameters into servers as much as possible, this is Reduce.of the servers by hashing through the word w?1 (orh?1) and its topic g, finish the Reduce part.
Thisis the initialization of the N -best list approximateEM step.
Each client then calls the servers for pa-rameters to perform synchronous multi-stack searchfor each sentence to get the N -best list parse trees.Again, the expected count for a particular parameterof w?1?n+1wh?1?mg at the clients are computed, thuswe finish a Map part, then summed up and stored inone of the servers by hashing through the word w?1(or h?1) and its topic g, thus we finish the Reducepart.
We repeat this procedure until convergence.Similarly, we use a distributed architecture as inFigure 2 to perform the follow-up EM algorithm tore-estimate WORD-PREDICTOR.4 Experimental resultsWe have trained our language models using threedifferent training sets: one has 44 million tokens,another has 230 million tokens, and the other has1.3 billion tokens.
An independent test set whichhas 354 k tokens is chosen.
The independent checkdata set used to determine the linear interpolationcoefficients has 1.7 million tokens for the 44 mil-lion tokens training corpus, 13.7 million tokens forboth 230 million and 1.3 billion tokens training cor-pora.
All these data sets are taken from the LDCEnglish Gigaword corpus with non-verbalized punc-tuation and we remove all punctuation.
Table 1 givesthe detailed information on how these data sets arechosen from the LDC English Gigaword corpus.The vocabulary sizes in all three cases are:?
word (also WORD-PREDICTOR operation)1.3 BILLION TOKENS TRAINING CORPUSAFP 19940512.0003 ?
19961015.0568AFW 19941111.0001 ?
19960414.0652NYT 19940701.0001 ?
19950131.0483NYT 19950401.0001 ?
20040909.0063XIN 19970901.0001 ?
20041125.0119230 MILLION TOKENS TRAINING CORPUSAFP 19940622.0336 ?
19961031.0797APW 19941111.0001 ?
19960419.0765NYT 19940701.0001 ?
19941130.040544 MILLION TOKENS TRAINING CORPUSAFP 19940601.0001 ?
19950721.013713.7 MILLION TOKENS CHECK CORPUSNYT 19950201.0001 ?
19950331.04941.7 MILLION TOKENS CHECK CORPUSAFP 19940512.0003 ?
19940531.0197354 K TOKENS TEST CORPUSCNA 20041101.0006 ?
20041217.0009Table 1: The corpora used in our experiments are selectedfrom the LDC English Gigaword corpus and specified inthis table, AFP, AFW, NYT, XIN and CNA denote thesections of the LDC English Gigaword corpus.vocabulary: 60 k, open - all words outside thevocabulary are mapped to the <unk> token,these 60 k words are chosen from the most fre-quently occurred words in 44 millions tokenscorpus;?
POS tag (also TAGGER operation) vocabulary:69, closed;?
non-terminal tag vocabulary: 54, closed;?
CONSTRUCTOR operation vocabulary: 157,closed.Similar to SLM (Chelba and Jelinek, 2000), af-ter the parses undergo headword percolation andbinarization, each model component of WORD-PREDICTOR, TAGGER, and CONSTRUCTOR isinitialized from a set of parsed sentences.
We usethe ?openNLP?
software (Northedge, 2005) to parsea large amount of sentences in the LDC English Gi-gaword corpus to generate an automatic treebank,which has a slightly different word-tokenizationthan that of the manual treebank such as the UpennTreebank used in (Chelba and Jelinek, 2000).
Forthe 44 and 230 million tokens corpora, all sentencesare automatically parsed and used to initialize modelparameters, while for 1.3 billion tokens corpus, weparse the sentences from a portion of the corpus that206contain 230 million tokens, then use them to initial-ize model parameters.
The parser at ?openNLP?
istrained by Upenn treebank with 1 million tokens andthere is a mismatch between Upenn treebank andLDC English Gigaword corpus.
Nevertheless, ex-perimental results show that this approach is effec-tive to provide initial values of model parameters.As we have explained, the proposed EM algo-rithms can be naturally cast into a MapReduceframework, see more discussion in (Lin and Dyer,2010).
If we have access to a large cluster ofmachines with Hadoop installed that are powerfulenough to process a billion tokens level corpus,we just need to specify a map function and a re-duce function etc., Hadoop will automatically par-allelize and execute programs written in this func-tional style.
Unfortunately, we don?t have this kindof resources available.
Instead, we have access to asupercomputer at a supercomputer center with MPIinstalled that has more than 1000 core processors us-able.
Thus we implement our algorithms using C++under MPI on the supercomputer, where we have towrite C++ codes for Map part and Reduce part, andthe MPI is used to take care of massage passing,scheduling, synchronization, etc.
between clientsand servers.
This involves a fair amount of pro-gramming work, even though our implementationunder MPI is not as reliable as under Hadoop butit is more efficient.
We use up to 1000 core proces-sors to train the composite language models for 1.3billion tokens corpus where 900 core processors areused to store the parameters alone.
We decide to uselinearly smoothed trigram as the baseline model for44 million token corpus, linearly smoothed 4-gramas the baseline model for 230 million token corpus,and linearly smoothed 5-gram as the baseline modelfor 1.3 billion token corpus.
Model size is a big is-sue, we have to keep only a small set of topics due tothe consideration in both computational time and re-source demand.
Table 2 shows the perplexity resultsand computation time of composite n-gram/PLSAlanguage models that are trained on three corporawhen the pre-defined number of total topics is 200but different numbers of most likely topics are keptfor each document in PLSA, the rest are pruned.
Forcomposite 5-gram/PLSA model trained on 1.3 bil-lion tokens corpus, 400 cores have to be used tokeep top 5 most likely topics.
For composite tri-gram/PLSA model trained on 44M tokens corpus,the computation time increases drastically with lessthan 5% percent perplexity improvement.
So in thefollowing experiments, we keep top 5 topics for eachdocument from total 200 topics and all other 195topics are pruned.All composite language models are first trainedby performing N-best list approximate EM algo-rithm until convergence, then EM algorithm for asecond stage of parameter re-estimation for WORD-PREDICTOR and SEMANTIZER until conver-gence.
We fix the size of topics in PLSA to be 200and then prune to 5 in the experiments, where theunpruned 5 topics in general account for 70% prob-ability in p(g|d).
Table 3 shows comprehensive per-plexity results for a variety of different models suchas composite n-gram/m-SLM, n-gram/PLSA, m-SLM/PLSA, their linear combinations, etc., wherewe use online EM with fixed learning rate to re-estimate the parameters of the SEMANTIZER oftest document.
The m-SLM performs competitivelywith its counterpart n-gram (n=m+1) on large scalecorpus.
In Table 3, for composite n-gram/m-SLMmodel (n = 3,m = 2 and n = 4,m = 3) trainedon 44 million tokens and 230 million tokens, we cutoff its fractional expected counts that are less than athreshold 0.005, this significantly reduces the num-ber of predictor?s types by 85%.
When we trainthe composite language on 1.3 billion tokens cor-pus, we have to both aggressively prune the param-eters of WORD-PREDICTOR and shrink the orderof n-gram and m-SLM in order to store them in asupercomputer having 1000 cores.
In particular, forcomposite 5-gram/4-SLM model, its size is too bigto store, thus we use its approximation, a linear com-bination of 5-gram/2-SLM and 2-gram/4-SLM, andfor 5-gram/2-SLM or 2-gram/4-SLM, again we cutoff its fractional expected counts that are less than athreshold 0.005, this significantly reduces the num-ber of predictor?s types by 85%.
For composite 4-SLM/PLSA model, we cut off its fractional expectedcounts that are less than a threshold 0.002, again thissignificantly reduces the number of predictor?s typesby 85%.
For composite 4-SLM/PLSA model or itslinear combination with models, we ignore all thetags and use only the words in the 4 head words.In this table, we have three items missing (markedby ?
), since the size of corresponding model is207CORPUS n # OF PPL TIME # OF # OF # OF TYPESTOPICS (HOURS) SERVERS CLIENTS OF ww?1?n+1g44M 3 5 196 0.5 40 100 120.1M3 10 194 1.0 40 100 218.6M3 20 190 2.7 80 100 537.8M3 50 189 6.3 80 100 1.123B3 100 189 11.2 80 100 1.616B3 200 188 19.3 80 100 2.280B230M 4 5 146 25.6 280 100 0.681B1.3B 5 2 111 26.5 400 100 1.790B5 5 102 75.0 400 100 4.391BTable 2: Perplexity (ppl) results and time consumed of composite n-gram/PLSA language model trained on threecorpora when different numbers of most likely topics are kept for each document in PLSA.LANGUAGE MODEL 44M REDUC- 230M REDUC- 1.3B REDUC-n=3,m=2 TION n=4,m=3 TION n=5,m=4 TIONBASELINE n-GRAM (LINEAR) 262 200 138n-GRAM (KNESER-NEY) 244 6.9% 183 8.5% ?
?m-SLM 279 -6.5% 190 5.0% 137 0.0%PLSA 825 -214.9% 812 -306.0% 773 -460.0%n-GRAM+m-SLM 247 5.7% 184 8.0% 129 6.5%n-GRAM+PLSA 235 10.3% 179 10.5% 128 7.2%n-GRAM+m-SLM+PLSA 222 15.3% 175 12.5% 123 10.9%n-GRAM/m-SLM 243 7.3% 171 14.5% (125) 9.4%n-GRAM/PLSA 196 25.2% 146 27.0% 102 26.1%m-SLM/PLSA 198 24.4% 140 30.0% (103) 25.4%n-GRAM/PLSA+m-SLM/PLSA 183 30.2% 140 30.0% (93) 32.6%n-GRAM/m-SLM+m-SLM/PLSA 183 30.2% 139 30.5% (94) 31.9%n-GRAM/m-SLM+n-GRAM/PLSA 184 29.8% 137 31.5% (91) 34.1%n-GRAM/m-SLM+n-GRAM/PLSA 180 31.3% 130 35.0% ?
?+m-SLM/PLSAn-GRAM/m-SLM/PLSA 176 32.8% ?
?
?
?Table 3: Perplexity results for various language models on test corpus, where + denotes linear combination, / denotescomposite model; n denotes the order of n-gram and m denotes the order of SLM; the topic nodes are pruned from200 to 5.too big to store in the supercomputer.
The com-posite n-gram/m-SLM/PLSA model gives signifi-cant perplexity reductions over baseline n-grams,n = 3, 4, 5 and m-SLMs, m = 2, 3, 4.
The major-ity of gains comes from PLSA component, but whenadding SLM component into n-gram/PLSA, there isa further 10% relative perplexity reduction.We have applied our composite 5-gram/2-SLM+2-gram/4-SLM+5-gram/PLSA languagemodel that is trained by 1.3 billion word corpus forthe task of re-ranking the N -best list in statisticalmachine translation.
We used the same 1000-bestlist that is used by Zhang et al (2006).
Thislist was generated on 919 sentences from theMT03 Chinese-English evaluation set by Hiero(Chiang, 2005; Chiang, 2007), a state-of-the-artparsing-based translation model.
Its decoder usesa trigram language model trained with modifiedKneser-Ney smoothing (Kneser and Ney, 1995) ona 200 million tokens corpus.
Each translation has11 features and language model is one of them.We substitute our language model and use MERT(Och, 2003) to optimize the BLEU score (Papineniet al, 2002).
We partition the data into ten pieces,9 pieces are used as training data to optimize theBLEU score (Papineni et al, 2002) by MERT (Och,2082003), a remaining single piece is used to re-rankthe 1000-best list and obtain the BLEU score.
Thecross-validation process is then repeated 10 times(the folds), with each of the 10 pieces used exactlyonce as the validation data.
The 10 results from thefolds then can be averaged (or otherwise combined)to produce a single estimation for BLEU score.Table 4 shows the BLEU scores through 10-foldcross-validation.
The composite 5-gram/2-SLM+2-gram/4-SLM+5-gram/PLSA language model gives1.57% BLEU score improvement over the baselineand 0.79% BLEU score improvement over the5-gram.
This is because there is not much diversityon the 1000-best list, and essentially only 20 ?
30distinct sentences are there in the 1000-best list.Chiang (2007) studied the performance of machinetranslation on Hiero, the BLEU score is 33.31%when n-gram is used to re-rank the N -best list, how-ever, the BLEU score becomes significantly higher37.09% when the n-gram is embedded directly intoHiero?s one pass decoder, this is because there is notmuch diversity in the N -best list.
It is expected thatputting the our composite language into a one passdecoder of both phrase-based (Koehn et al, 2003)and parsing-based (Chiang, 2005; Chiang, 2007)MT systems should result in much improved BLEUscores.SYSTEM MODEL MEAN (%)BASELINE 31.755-GRAM 32.535-GRAM/2-SLM+2-GRAM/4-SLM 32.875-GRAM/PLSA 33.015-GRAM/2-SLM+2-GRAM/4-SLM 33.32+5-GRAM/PLSATable 4: 10-fold cross-validation BLEU score results forthe task of re-ranking the N -best list.Besides reporting the BLEU scores, we look at the?readability?
of translations similar to the study con-ducted by Charniak et al (2003).
The translationsare sorted into four groups: good/bad syntax crossedwith good/bad meaning by human judges, see Ta-ble 5.
We find that many more sentences are perfect,many more are grammatically correct, and manymore are semantically correct.
The syntactic lan-guage model (Charniak, 2001; Charniak, 2003) onlyimproves translations to have good grammar, butdoes not improve translations to preserve meaning.The composite 5-gram/2-SLM+2-gram/4-SLM+5-gram/PLSA language model improves both signif-icantly.
Bear in mind that Charniak et al (2003) in-tegrated Charniak?s language model with the syntax-based translation model Yamada and Knight pro-posed (2001) to rescore a tree-to-string translationforest, whereas we use only our language modelfor N -best list re-ranking.
Also, in the same studyin (Charniak, 2003), they found that the outputsproduced using the n-grams received higher scoresfrom BLEU; ours did not.
The difference betweenhuman judgments and BLEU scores indicate thatcloser agreement may be possible by incorporatingsyntactic structure and semantic information into theBLEU score evaluation.
For example, semanticallysimilar words like ?insure?
and ?ensure?
in the ex-ample of BLEU paper (Papineni et al, 2002) shouldbe substituted in the formula, and there is a weightto measure the goodness of syntactic structure.
Thismodification will lead to a better metric and suchinformation can be provided by our composite lan-guage models.SYSTEM MODEL P S G WBASELINE 95 398 20 4065-GRAM 122 406 24 3675-GRAM/2-SLM 151 425 33 310+2-GRAM/4-SLM+5-GRAM/PLSATable 5: Results of ?readability?
evaluation on 919 trans-lated sentences, P: perfect, S: only semantically correct,G: only grammatically correct, W: wrong.5 ConclusionAs far as we know, this is the first work of building acomplex large scale distributed language model witha principled approach that is more powerful than n-grams when both trained on a very large corpus withup to a billion tokens.
We believe our results stillhold on web scale corpora that have trillion tokens,since the composite language model effectively en-codes long range dependencies of natural languagethat n-gram is not viable to consider.
Of course,this implies that we have to take a huge amount ofresources to perform the computation, neverthelessthis becomes feasible, affordable, and cheap in theera of cloud computing.209ReferencesL.
Bahl and J. Baker,F.
Jelinek and R. Mercer.
1977.
Per-plexity?a measure of difficulty of speech recognitiontasks.
94th Meeting of the Acoustical Society of Amer-ica, 62:S63, Supplement 1.T.
Brants et al.
2007.
Large language models in ma-chine translation.
The 2007 Conference on EmpiricalMethods in Natural Language Processing (EMNLP),858-867.E.
Charniak.
2001.
Immediate-head parsing for languagemodels.
The 39th Annual Conference on Associationof Computational Linguistics (ACL), 124-131.E.
Charniak, K. Knight and K. Yamada.
2003.
Syntax-based language models for statistical machine transla-tion.
MT Summit IX., Intl.
Assoc.
for Machine Trans-lation.C.
Chelba and F. Jelinek.
1998.
Exploiting syntacticstructure for language modeling.
The 36th AnnualConference on Association of Computational Linguis-tics (ACL), 225-231.C.
Chelba and F. Jelinek.
2000.
Structured lan-guage modeling.
Computer Speech and Language,14(4):283-332.D.
Chiang.
2005.
A hierarchical phrase-based model forstatistical machine translation.
The 43th Annual Con-ference on Association of Computational Linguistics(ACL), 263-270.D.
Chiang.
2007.
Hierarchical phrase-based translation.Computational Linguistics, 33(2):201-228.J.
Dean and S. Ghemawat.
2004.
MapReduce: Simpli-fied data processing on large clusters.
Operating Sys-tems Design and Implementation (OSDI), 137-150.A.
Dempster, N. Laird and D. Rubin.
1977.
Maximumlikelihood estimation from incomplete data via the EMalgorithm.
Journal of Royal Statistical Society, 39:1-38.A.
Emami, K. Papineni and J. Sorensen.
2007.
Large-scale distributed language modeling.
The 32nd IEEEInternational Conference on Acoustics, Speech, andSignal Processing (ICASSP), IV:37-40.T.
Hofmann.
2001.
Unsupervised learning by proba-bilistic latent semantic analysis.
Machine Learning,42(1):177-196.F.
Jelinek and R. Mercer.
1981.
Interpolated estimationof Markov source parameters from sparse data.
Pat-tern Recognition in Practice, 381-397.F.
Jelinek and C. Chelba.
1999.
Putting languageinto language modeling.
Sixth European Confer-ence on Speech Communication and Technology (EU-ROSPEECH), Keynote Paper 1.F.
Jelinek.
2004.
Stochastic analysis of structured lan-guage modeling.
Mathematical Foundations of Speechand Language Processing, 37-72, Springer-Verlag.D.
Jurafsky and J. Martin.
2008.
Speech and LanguageProcessing, 2nd Edition, Prentice Hall.R.
Kneser and H. Ney.
1995.
Improved backing-off form-gram language modeling.
The 20th IEEE Interna-tional Conference on Acoustics, Speech, and SignalProcessing (ICASSP), 181-184.P.
Koehn, F. Och and D. Marcu.
2003.
Statistical phrase-based translation.
The Human Language TechnologyConference (HLT), 48-54.S.
Khudanpur and J. Wu.
2000.
Maximum entropy tech-niques for exploiting syntactic, semantic and colloca-tional dependencies in language modeling.
ComputerSpeech and Language, 14(4):355-372.A.
Lavie et al 2006.
MINDS Workshops MachineTranslation Working Group Final Report.
http://www-nlpir.nist.gov/MINDS/FINAL/MT.web.pdfJ.
Lin and C. Dyer.
2010.
Data-Intensive Text Processingwith MapReduce.
Morgan and Claypool Publishers.R.
Northedge.
2005.
OpenNLP softwarehttp://www.codeproject.com/KB/recipes/englishparsing.aspxF.
Och.
2003.
Minimum error rate training in statisti-cal machine translation.
The 41th Annual meeting ofthe Association for Computational Linguistics (ACL),311-318.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.Bleu: a method for automatic evaluation of machinetranslation.
The 40th Annual meeting of the Associa-tion for Computational Linguistics (ACL), 311-318.B.
Roark.
2001.
Probabilistic top-down parsingand language modeling.
Computational Linguistics,27(2):249-276.S.
Wang et al 2005.
Exploiting syntactic, semantic andlexical regularities in language modeling via directedMarkov random fields.
The 22nd International Con-ference on Machine Learning (ICML), 953-960.S.
Wang et al 2006.
Stochastic analysis of lexical andsemantic enhanced structural language model.
The 8thInternational Colloquium on Grammatical Inference(ICGI), 97-111.K.
Yamada and K. Knight.
2001.
A syntax-based statis-tical translation model.
The 39th Annual Conferenceon Association of Computational Linguistics (ACL),1067-1074.W.
Zangwill.
1969.
Nonlinear Programming: A UnifiedApproach.
Prentice-Hall.Y.
Zhang, A. Hildebrand and S. Vogel.
2006.
Dis-tributed language modeling for N-best list re-ranking.The 2006 Conference on Empirical Methods in Natu-ral Language Processing (EMNLP), 216-223.Y.
Zhang, 2008.
Structured language models for statisti-cal machine translation.
Ph.D. dissertation, CMU.210
