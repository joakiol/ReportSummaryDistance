Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 674?682,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPGenre distinctions for Discourse in the Penn TreeBankBonnie WebberSchool of InformaticsUniversity of EdinburghEdinburgh EH8 9LW, UKbonnie.webber@ed.ac.ukAbstractArticles in the Penn TreeBank were iden-tified as being reviews, summaries, let-ters to the editor, news reportage, correc-tions, wit and short verse, or quarterlyprofit reports.
All but the latter threewere then characterised in terms of fea-tures manually annotated in the Penn Dis-course TreeBank ?
discourse connectivesand their senses.
Summaries turned outto display very different discourse featuresthan the other three genres.
Letters alsoappeared to have some different features.The two main findings involve (1) differ-ences between genres in the senses asso-ciated with intra-sentential discourse con-nectives, inter-sentential discourse con-nectives and inter-sentential discourse re-lations that are not lexically marked; and(2) differences within all four genres be-tween the senses of discourse relationsnot lexically marked and those that aremarked.
The first finding means that genreshould be made a factor in automatedsense labelling of non-lexically markeddiscourse relations.
The second meansthat lexically marked relations provide apoor model for automated sense labellingof relations that are not lexically marked.1 IntroductionIt is well-known that texts differ from each other ina variety of ways, including their topic, the read-ing level of their intended audience, and their in-tended purpose (eg, to instruct, to inform, to ex-press an opinion, to summarize, to take issue withor disagree, to correct, to entertain, etc.).
Thispaper considers differences in texts in the well-known Penn TreeBank (hereafter, PTB) and inparticular, how these differences show up in thePenn Discourse TreeBank (Prasad et al, 2008).It first describes ways in which texts can vary(Section 2).
It then illustrates the variety of textsto be found in the the PTB and suggests theirgrouping into four broad genres (Section 3).
Aftera brief introduction to the Penn Discourse Tree-Bank (hereafter, PDTB) in Section 4, Sections 5and 6 show that these four genres display differ-ences in connective frequency and in terms of thesenses associated with intra-sentential connectives(eg, subordinating conjunctions), inter-sententialconnectives (eg, inter-sentential coordinating con-junctions) and those inter-sentential relations thatare not lexically marked.
Section 7 considers re-cent efforts to induce effective procedures for au-tomated sense labelling of discourse relations thatare not lexically marked (Elwell and Baldridge,2008; Marcu and Echihabi, 2002; Pitler et al,2009; Wellner and Pustejovsky, 2007; Wellner,2008).
It makes two points.
First, because gen-res differ from each other in the senses associatedwith such relations, genre should be made a factorin their automated sense labelling.
Secondly, be-cause different senses are being conveyed when arelation is lexically marked than when it isn?t, lex-ically marked relations provide a poor model forautomated sense labelling of relations that are notlexically marked.2 Two Perspectives on GenreThe dimension of text variation of interest here isgenre, which can be viewed externally, in termsof the communicative purpose of a text (Swales,1990), or internally, in terms of features com-mon to texts sharing a communicative purpose.
(Kessler et al, 1997) combine these views by say-ing that a genre should not be so broad that thetexts belonging to it don?t share any distinguish-ing properties ?.
.
.we would probably not use the term?genre?
to describe merely the class of674texts that have the objective of persuad-ing someone to do something, since thatclass ?
which would include editorials,sermons, prayers, advertisements, andso forth ?
has no distinguishing formalproperties (Kessler et al, 1997, p. 33).A balanced corpus like the Brown Corpus ofAmerican English or the British National Corpus,will sample texts from different genres, to give arepresentative view of how the language is used.For example, the fifteen categories of publishedmaterial sampled for the Brown Corpus includePRESS REPORTAGE, PRESS EDITORIALS, PRESSREVIEWS and five different types of FICTION.In contrast, experiments on what genres wouldbe helpful in web search for particular types of in-formation on a topic led (Rosso, 2008), to 18 classlabels that his subjects could reliably apply to webpages (here, ones from an .edu domain) with over50% agreement.
These class labels included ARTI-CLE, COURSE DESCRIPTION, COURSE LIST, DI-ARY, WEBLOG OR BLOG, FAQ/HELP and FORM.In both Brown?s published material and Rosso?sweb pages, the selected class labels (genres) re-flect external purpose rather than distinctive inter-nal features.Such features are, however, of great interest inboth text analysis and text processing.
Text an-alysts have shown that there are indeed interest-ing features that correlate more strongly with cer-tain genres than with others.
For example, (Biber,1986) considered 41 linguistic features previouslymentioned in the literature, including type/tokenratio, average word length, and such frequenciesas that of particular words (eg, I/you, it, the pro-verb do), particular word types (eg, place adverbs,hedges), particular parts-of-speech (eg, past tenseverbs, adjectives), and particular syntactic con-structions (eg, that-clauses, if -clauses, reducedrelative clauses).
He found certain clusters ofthese features (i.e.
their presense or absense) cor-related well with certain text types.
For example,press reportage scored the highest with respect tohigh frequency of that-clauses and contractions,and low type-token ratio (i.e.
a varied vocabu-lary for a given length of text), while general andromantic fiction scored much lower on these fea-tures.
(Biber, 2003) showed significant differencesin the internal structure of noun phrases used infiction, news, academic writing and face-to-faceconversations.Such features are of similar interest in text pro-cessing ?
in particular, automated genre classifi-cation (Dewdney et al, 2001; Finn and Kushmer-ick, 2006; Kessler et al, 1997; Stamatatos et al,2000; Wolters and Kirsten, 1999) ?
which relieson there being reliably detectable features that canbe used to distinguish one class from another.
Thisis where the caveat from (Kessler et al, 1997) be-comes relevant: A particular genre shouldn?t betaken so broadly as to have no distinguishing fea-tures, nor so narrowly as to have no general appli-cability.
But this still allows variability in what istaken to be a genre.
There is no one ?right set?.3 Genre in the Penn TreeBankAlthough the files in the Penn TreeBank (PTB)lack any classificatory meta-data, leading the PTBto be treated as a single homogeneous collectionof ?news articles?, researchers who have manuallyexamined it in detail have noted that it includes avariety of ?financial reports, general interest sto-ries, business-related news, cultural reviews, ed-itorials and letters to the editor?
(Carlson et al,2002, p. 7).To date, ignoring this variety hasn?t really mat-tered since the PTB has primarily been used indeveloping word-level and sentence-level toolsfor automated language analysis such as wide-coverage part-of-speech taggers, robust parsersand statistical sentence generators.
Any genre-related differences in word usage and/or syntaxhave just meant a wider variety of words and sen-tences shaping the covereage of these tools.
How-ever, ignoring this variety may actually hinder thedevelopment of robust language technology foranalysing and/or generating multi-sentence text.As such, it is worth considering genre in the PTB,since doing so can allow texts from different gen-res to be weighted differently when tools are beingdeveloped.This is a start on such an undertaking.
In lieuof any informative meta-data in the PTB files1, Ilooked at line-level patterns in the 2159 files thatmake up the Penn Discourse TreeBank subset ofthe PTB, and then manually confirmed the texttypes I found.2 The resulting set includes all the1Subsequent to this paper, I discovered that the TIPSTERCollection (LDC Catalog entry LDC93T3B) contains a smallamount of meta-data that can be projected onto the PTB files,to refine the semi-automatic, manually-verified analysis donehere.
This work is now in progress.2Similar patterns can also be found among the 153 files in675genres noted by Carlson et al (2002) and others aswell:1.
Op-Ed pieces and reviews ending with a by-line (73 files): wsj 0071, wsj 0087, wsj 0108,wsj 0186, wsj 0207, wsj 0239, wsj 0257, etc.2.
Sourced articles from another newspaper ormagazine (8 files): wsj 1453, wsj 1569, wsj 1623,wsj 1635, wsj 1809, wsj 1970, wsj 2017, wsj 21533.
Editorials and other reviews, similar to theabove, but lacking a by-line or source (11files): wsj 0039, wsj 0456, wsj 0765, wsj 0794,wsj 0819, wsj 0972, wsj 1259 wsj 1315, etc.4.
Essays on topics commemorating the WSJ?scentennial (12 files): wsj 0022, wsj 0339,wsj 0406, wsj 0676, wsj 0933, 2sj 1164, etc.5.
Daily summaries of offerings and pricings inU.S.
and non-U.S. capital markets (13 files):wsj 0125, wsj 0271, wsj 0476, wsj 0612, wsj 0704,wsj 1001, wsj 1161, wsj 1312, wsj 1441, etc.6.
Daily summaries of financially significantevents, ending with a summary of the day?smarket figures (14 files): wsj 0178, wsj 0350,wsj 0493, wsj 0675, wsj 1043, wsj 1217, etc.7.
Daily summaries of interest rates (12 files):wsj 0219, wsj 0457, wsj 0602, wsj 0986, etc.8.
Summaries of recent SEC filings (4 files):wsj 0599, wsj 0770, wsj 1156, wsj 12479.
Weekly market summaries (12 files):wsj 0137, wsj 0231, wsj 0374, wsj 0586, wsj 1015,wsj 1187, wsj 1337, wsj 1505, wsj 1723, etc.10.
Letters to the editor (49 files3): wsj 0091,wsj 0094, wsj 0095, wsj 0266, wsj 0268, wsj 0360,wsj 0411, wsj 0433, wsj 0508, wsj 0687, etc.11.
Corrections (24 files): wsj 0104, wsj 0200,wsj 0211, wsj 0410, wsj 0603, wsj 0605, etc.12.
Wit and short verse (14 files): wsj 0139,wsj 0312, wsj 0594, wsj 0403, wsj 0757, etc.13.
Quarterly profit reports ?
introductory para-graphs alone (11 files): wsj 0190, wsj 0364,wsj 0511, wsj 0696, wsj 1056, wsj 1228, etc.the Penn TreeBank that aren?t included in the PDTB.
How-ever, such files were excluded so that all further analysescould be carried out on the same set of files.3The relation between letters and files is not one-to-one:13 (26.5%) of these files contain between two and six letters.This is relevant at the end of this section when consideringlength as a potentially distinguishing feature of a text.14.
News reports (1902 files)A complete listing of these classes can be found inan electronic appendix to this article at the PDTBhome page (http://www.seas.upenn.edu/?pdtb).In order to consider discourse-level features dis-tinctive to genres within the PTB, I have ignored,for the time being, both CORRECTIONS and WITAND SHORT VERSE since they are so obviouslydifferent from the other texts, and also QUAR-TERLY PROFIT REPORTS, since they turn out tobe multiple simply copies of the same text be-cause the distinguishing company listings havebeen omitted.The remaining eleven classes have been ag-gregated into four broad genres: ESSAYS (104files, classes 1-4), SUMMARIES (55 files, classes5-9), LETTERS (49 files, class 10) and NEWS(1902 files, class 14).
The latter corre-sponds to the Brown Corpus class PRESS RE-PORTAGE and the class NEWS in the NewYork Times annotated corpus (Evan Sandhaus,2008), excluding CORRECTIONS and OBITUAR-IES.
The LETTERS class here corresponds tothe NYT class OPINION/LETTERS, while ES-SAYS here spans both Brown Corpus classesPRESS REVIEWS and PRESS EDITORIALS, andthe NYT corpus classes OPINION/EDITORIALS,OPINION/OPED, FEATURES/XXX/COLUMNS andFEATURES/XXX/REVIEWS, where XXX rangesover Arts, Books, Dining and Wine, Movies,Style, etc.
The class called SUMMARIES has nocorresponding class in Brown.
In the NYT Cor-pus, it corresponds to those articles whose tax-onomic classifiers field is NEWS/BUSINESS andwhose types of material field is SCHEDULE.There are two things to note here.
First, noclaim is being made that these are the only classesto be found in the PTB.
For example, the classlabelled NEWS contains a subset of 80 short (1-3sentence) articles announcing personnel changes?
eg, promotions, appointments to supervisoryboards, etc.
(eg, wsj 0001, wsj 0014, wsj 0066,wsj 0069, wsj 0218, etc.)
I have not lookedfor more specific classes because even classes atthis level of specificity show that ignoring genre-specific discourse features can hinder the devel-opment of robust language technology for eitheranalysing or generating multi-sentence text.
Sec-ondly, no claim is being made that the four se-lected classes comprise the ?right?
set of genresfor future use of the PTB for discourse-related676language technology, just that some sensitivity togenre will lead to better performance.Some simple differences between the four broadgenre can be seen in Figure 1, in terms of the av-erage length of a file in words, sentences or para-graphs4, and the average number of sentences perparagraph.
Figure 1 shows that essays are, on aver-age, longer than texts from the other three classes,and have longer paragraphs.
The relevance of thelatter will become clear in the next section, whenI describe PDTB annotation as background forgenre differences related to this annotation.4 The Penn Discourse TreeBankGenre differences at the level of discourse in thePTB can be seen in the manual annotations of thePenn Discourse TreeBank (Prasad et al, 2008).There are several elements to PDTB annotation.First, the PDTB annotates the arguments of ex-plicit discourse connectives:(1) Even so, according to Mr. Salmore, the adwas ?devastating?
because it raised ques-tions about Mr. Courter?s credibility.
But it?sbuilding on a long tradition.
(0041)Here, the explicit connective (?but?)
is underlined.Its first argument, ARG1, is shown in italics andits second, ARG2, in boldface.
The number 0041indicates that the example comes from subsectionwsj 0041 of the PTB.Secondly, the PDTB annotates implicit dis-course relations between adjacent sentenceswithin the same paragraph, where the second doesnot contain an explicit inter-sentential connective:(2) The projects already under construction willincrease Las Vegas?s supply of hotel rooms by11,795, or nearly 20%, to 75,500.
[Implicit?so?]
By a rule of thumb of 1.5 new jobs foreach new hotel room, Clark County willhave nearly 18,000 new jobs.
(0994)With implicit discourse relations, annotators wereasked to identify one or more explicit connectivesthat could be inserted to lexicalize the relation be-tween the arguments.
Here, they have been identi-fied as the connective ?so?.Where annotators could not identify such an im-plicit connective, they were asked if they couldidentify a non-connective phrase in ARG2 (e.g.4A file usually contains a single article, except (as notedearlier) files in the class LETTERS, which may contain morethan one letter.
?this means?)
that realised the implicit discourserelation instead (ALTLEX), or a relation holdingbetween the second sentence and an entity men-tioned in the first (ENTREL), rather than the inter-pretation of the previous sentence itself:(3) Rated triple-A by Moody?s and S&P, the issuewill be sold through First Boston Corp. Theissue is backed by a 12% letter of creditfrom Credit Suisse.If the annotators couldn?t identify either, theywould assert that no discourse relation held be-tween the adjacent sentences (NOREL).
Note thatbecause resource limitations meant that implicitdiscourse relations (comprising implicit connec-tives, ALTLEX, ENTREL and NOREL) were onlyannotated within paragraphs, longer paragraphs(as there were in ESSAYS) could potentially meanmore implicit discourse relations were annotated.The third element of PDTB annotation is thatof the senses of connectives, both explicit and im-plicit.
These have been manually annotated usingthe three-level sense hierarchy described in detailin (Miltsakaki et al, 2008).
Briefly, there are fourtop-level classes:?
TEMPORAL, where the situations describedin the arguments are related temporally;?
CONTINGENCY, where the situation de-scribed in one argument causally influencesthat described in the other;?
COMPARISON, used to highlight someprominent difference that holds between thesituations described in the two arguments;?
EXPANSION, where one argument expandsthe situation described in the other and movesthe narrative or exposition forward.TEMPORAL relations can be further specified toASYNCHRONOUS and SYNCHRONOUS, depend-ing on whether or not the situations described bythe arguments are temporally ordered.
CONTIN-GENCY can be further specified to CAUSE andCONDITION, depending on whether or not the ex-istential status of the arguments depends on theconnective (i.e.
no for CAUSE, and yes for CON-DITION).COMPARISON can be further specified to CON-TRAST, where the two arguments share a predicateor property whose difference is being highlighted,and CONCESSION, where ?the highlighted differ-ences are related to expectations raised by one677Total Total Total Total Avg.
words Avg.
sentences Avg.
?s Avg.
sentencesGenre files paragraphs sentences words per file per file per file per ?ESSAYS 104 1580 4774 98376 945.92 45.9 15.2 3.02SUMMARIES 55 1047 2118 37604 683.71 38.5 19.1 2.02LETTERS 49 339 739 15613 318.63 15.1 7.1 2.14NEWS 1902 18437 40095 837367 440.26 21.1 9.7 2.17Figure 1: Distribution of Words, Sentences and Paragraphs by Genre (?
stands for ?paragraph?.
)argument which are then denied by the other?
(Miltsakaki et al, 2008, p.282).
Finally, EX-PANSION has six subtypese, including CONJUNC-TION, where the situation described in ARG2, pro-vides new information related to the situation de-scribed in ARG1; RESTATEMENT, where ARG2restates or redescribes the situation described inARG1; and ALTERNATIVE, where the two argu-ments evoke situations taken to be alternatives.These two levels are sufficient to show signifi-cant differences between genres.
The only otherthing to note is that annotators could be as specificas they chose in annotating the sense of a connec-tive: If they could not decide on the specific typeof COMPARISON holding between the two argu-ments of a connective, or they felt that both sub-types of COMPARISON were being expressed, theycould simply sense annotate the connective withthe label COMPARISON.
I will comment on this inSection 6.The fourth element of PDTB annotation is at-tribution (Prasad et al, 2007; Prasad et al, 2008).This was not considered in the current analysis,although here too, genre-related differences arelikely.5 Connective Frequency by GenreThe analysis that follows distinguishes betweentwo kinds of relations associated with explicit con-nectives in the PDTB: (1) intra-sentential dis-course relations, which hold between clauseswithin the same sentence and are associated withsubordinating conjunctions, intra-sentential coor-dinating conjunctions, and discourse adverbialswhose arguments occur within the same sen-tence5); and (2) explicit inter-sentential discourserelations, which hold across sentences and areassociated with explicit inter-sentential connec-tives (inter-sentential coordinating conjunctionsand discourse adverbials whose arguments are not5Limited resources meant that intra-sentential discourserelations associated with subordinators like ?in order to?
and?so that?
or with free adjuncts were not annotated in thePDTB.in the same sentence).It is the latter that are effectively in complemen-tary distribution with implicit discourse relationsin the PDTB6, and Figures 2 and 3 show their dis-tribution across the four genres.7 Figure 2 showsthat among explicit inter-sentential connectives,S-initial coordinating conjunctions (?And?, ?Or?and ?But?)
are a feature of ESSAYS, SUMMARIESand NEWS but not of LETTERS.
LETTERS are writ-ten by members of the public, not by the journal-ists or editors working for the Wall Street Journal.This suggests that the use of S-initial coordinatingconjunctions is an element of Wall Street Journal?house style?, as opposed to a common feature ofmodern writing.Figure 3 shows several things about the dif-ferent patterning across genres of implicit dis-course relations (Columns 4?7 for implicit con-nectives, ALTLEX, ENTREL and NOREL) andexplicit inter-sentential connectives (Column 3).First, SUMMARIES are distinctive in two ways:While the ratio of implicit connectives to explicitinter-sentential connectives is around 3:1 in theother three genres, for SUMMARIES it is around4:1 ?
there are just many fewer explicit inter-sentential connectives.
Secondly, while the ra-tio of ENTREL relations to implicit connectivesranges from 0.19 to 0.32 in the other three gen-res, in SUMMARIES, ENTREL predominates (as inExample 3 from one of the daily summaries of of-ferings and pricings).
In fact, there are nearly as6This is not quite true for two reasons ?
first, because thefirst argument of a discourse adverbial is not restricted to theimmediately adjacent sentence and secondly, because a sen-tence can have both an initial coordinating conjunction and adiscourse adverbial, as in ?So, for example, he?ll eat tofu withfried pork rinds.?
But it?s a reasonable first approximation.7Although annotated in the PDTB, throughout this paperI have ignored the S-medial discourse adverbial also, as in?John also eats fish?, since such instances are better regardedas presuppositional.
That is, as well as a textual antecedent,they can be licensed through inference (e.g.
?John claimsto be a vegetarian, but he also eats fish.?)
or accommodatedby listeners with respect to the spatio-temporal context (e.g.Watching John dig into a bowl of tofu, one might remark?Don?t worry.
He also eats fish.?)
The other discourse ad-verbials annotated in the PDTB do not have this property.678Total Explicit Density of Explicit S-initial S-initial S-medialTotal Inter-Sentential Inter-Sentential Coordinating Discourse Inter-SententialGenre Sentences Connectives Connectives/Sentence Conjunctions Adverbials Disc AdvsESSAYS 4774 691 0.145 334 (48.3%) 244 (35.3%) 113 (16.4%)SUMMARIES 2118 95 0.045 46 (48.4%) 39 (41.1%) 10 (10.5%)LETTERS 739 85 0.115 26 (30.6%) 37 (43.5%) 18 (21.2%)NEWS 40095 4709 0.117 2389 (50.7%) 1610 (34.2%) 718 (15.3%)Figure 2: Distribution of Explicit Inter-Sentential Connectives.Total Total ExplicitInter-Sentential Inter-Sentential ImplicitGenre Discourse Rels Connectives Connectives ENTREL ALTLEX NORELESSAYS 3302 691 (20.9%) 2112 (64.0%) 397 (12.0%) 86 (2.6%) 16 (0.5%)SUMMARIES 916 95 (10.4%) 363 (39.6%) 434 (47.4%) 12 (1.3%) 12 (1.3%)LETTERS 433 85 (19.6%) 267 (61.7%) 58 (13.4%) 22 (5.1%) 1 (0.2%)NEWS 23017 4709 (20.5%) 13287 (57.7%) 4293 (18.7%) 504 (2.2%) 224 (1%)Figure 3: Distribution of Inter-Sentential Discourse Relations, including Explicits from Figure 2.many ENTREL relations in summaries as the totalof explicit and implicit connectives combined.Finally, it is possible that the higher frequencyof alternative lexicalizations of discourse connec-tives (ALTLEX) in LETTERS than in the other threegenres means that they are not part of Wall StreetJournal ?house style?.
(Other elements of WSJ?house style?
?
or possibly, news style in general?
are observable in the significantly higher fre-quency of direct and indirect quotations in newsthan in the other three genres.
This property is notdiscussed further here, but is worth investigatingin the future.
)With respect to explicit intra-sentential con-nectives, the main point of interest in Figure 4is that SUMMARIES display a significantly lowerdensity of intra-sentential connectives overall thanthe other three genres, as well as a significantlylower relative frequency of intra-sentential dis-course adverbials.
As the next section will show,these intra-sentential connectives, while few, areselected most often to express CONTRAST and sit-uations changing over time, reflecting the natureof SUMMARIES as regular periodic summaries ofa changing world.6 Connective Sense by Genre(Pitler et al, 2008) show a difference across Level1 senses (COMPARISON, CONTINGENCY, TEM-PORAL and EXPANSION) in the PDTB in terms oftheir tendency to be realised by explicit connec-tives (a tendency of COMPARISON and TEMPO-RAL relations) or by Implicit Connectives (a ten-dency of CONTINGENCY and EXPANSION).
HereI show differences (focussing on Level 2 senses,which are more informative) in their frequencyof occurance in the four genres, by type of con-nective: explicit intra-sentential connectives (Fig-ure 5), explicit inter-sentential connectives (Fig-ure 6), and implicit inter-sentential connectives(Figure 7).
SUMMARIES and LETTERS are eachdistinctly different from ESSAYS and NEWS withrespect to each type of connective.One difference in sense annotation across thefour genres harkens back to a comment made inSection 4 ?
that annotators could be as specificas they chose in annotating the sense of a con-nective.
If they could not decide between spe-cific level n+1 labels for the sense of a connective,they could simply assign it a level n label.
It isperhaps suggestive then of the relative complexityof ESSAYS and LETTERS, as compared to NEWS,that the top-level label COMPARISON was usedapproximately twice as often in labelling explicitinter-sentential connectives in ESSAYS (7.2%) andLETTERS (9.4%) than in news (4.3%).
(The top-level labels EXPANSION, TEMPORAL and CON-TINGENCY were used far less often, as to be sim-ply noise.)
In any case, this aspect of readabil-ity may be worth further investigation (Pitler andNenkova, 2008).7 Automated Sense Labelling ofDiscourse ConnectivesThe focus here is on automated sense labellingof discourse connectives (Elwell and Baldridge,2008; Marcu and Echihabi, 2002; Pitler et al,2009; Wellner and Pustejovsky, 2007; Wellner,679Total Density of Intra-Sentential Intra-SententialTotal Intra-Sentential Intra-Sentential Subordinating Coordinating DiscourseGenre Sentences Connectives Connectives/Sentence Conjunctions Conjunctions AdverbialsESSAYS 4774 1397 0.293 808 (57.8%) 438 (31.4%) 151 (10.8%)SUMMARIES 2118 275 0.130 166 (60.4%) 99 (36.0%) 10 (3.6%)LETTERS 739 200 0.271 126 (63.0%) 56 (28.0%) 18 (9.0%)NEWS 40095 9336 0.233 5514 (59.1%) 3015 (32.3%) 807 (8.6%)Figure 4: Distribution of Explicit Intra-Sentential Connectives.Relation Essays Summaries Letters NewsExpansion.Conjunction 253 (18.1%) 50 (18.2%) 31 (15.5%) 1907 (20.4%)Contingency.Cause 208 (14.9%) 37 (13.5%) 32 (16%) 1354 (14.5%)Contingency.Condition 205 (14.7%) 15 (5.5%) 22 (11%) 1082 (11.6%)Temporal.Asynchronous 187 (13.4%) 54 (19.6%) 19 (9.5%) 1444 (15.5%)Comparison.Contrast 187 (13.4%) 56 (20.4%) 29 (14.5%) 1416 (15.2%)Temporal.Synchrony 165 (11.8%) 32 (11.6%) 27 (13.5%) 1061 (11.4%)Total 1397 275 200 9336Figure 5: Explicit Intra-Sentential Connectives: Most common Level 2 SensesRelation Essays Summaries Letters NewsComparison.Contrast 231 (33.4%) 47 (49.5%) 20 (23.5%) 1853 (39.4%)Expansion.Conjunction 156 (22.6%) 24 (25.3%) 20 (23.5%) 1144 (24.3%)Comparison.Concession 75 (10.9%) 11 (11.6%) 5 (5.9%) 462 (9.8%)Comparison 50 (7.2%) ?
8 (9.4%) 204 (4.3%)Temporal.Asynchronous 40 (5.8%) 1 (1.1%) 5 (5.8%) 265 (5.6%)Expansion.Instantiation 37 (5.4%) 3 (3.2%) 3 (3.5%) 236 (5.0%)Contingency.Cause 32 (4.6%) 1 (1.1%) 12 (14.1%) 136 (2.9%)Expansion.Restatement 27 (3.9%) ?
6 (7.1%) 93 (2.0%)Total 691 95 85 4709Figure 6: Explicit Inter-Sentential Connectives: Most common Level 2 SensesRelation Essays Summaries Letters NewsContingency.Cause 577 (27.3%) 70 (19.28%) 75 (28.1%) 3389 (25.5%)Expansion.Restatement 395 (18.7%) 62 (17.07%) 55 (20.6%) 2591 (19.5%)Expansion.Conjunction 362 (17.1%) 126 (34.7%) 40 (15.0%) 2908 (21.9%)Comparison.Contrast 254 (12.0%) 53 (14.60%) 42 (15.7%) 1704 (12.8%)Expansion.Instantiation 211 (10.0%) 18 (4.96%) 14 (5.2%) 1152 (8.7%)Temporal.Asynchronous 110 (5.2%) 7 (1.93%) 6 (2.3%) 524 (3.9%)Total 2112 363 267 13287Figure 7: Implicit Connectives: Most common Level 2 SensesEssays SummariesRelation: Implicit Inter-Sent Intra-Sent Implicit Inter-Sent Intra-SentContingency.Cause 577 (27.3%) 32 (4.6%) 208 (14.9%) 70 (19.28%) 1 (1.1%) 37 (13.5%)Expansion.Restatement 395 (18.7%) 27 (3.9%) 4 (0.3%) 62 (17.07%) ?
?Expansion.Conjunction 362 (17.1%) 156 (22.6%) 253 (18.1%) 126 (34.7%) 24 (25.3%) 50 (18.2%)Comparison.Contrast 254 (12.0%) 231 (33.4%) 187 (13.4%) 53 (14.60%) 47 (49.5%) 56 (20.4%)Expansion.Instantiation 211 (10.0%) 37 (5.4%) 5 (0.3%) 18 (5.0%) 3 (3.2%) ?Total: 2112 691 1397 363 95 275Figure 8: Essays and Summaries: Connective sense frequency680Letters NewsRelation: Implicit Inter-Sent Intra-Sent Implicit Inter-Sent Intra-SentContingency.Cause 75 (28.1%) 12 (14.1%) 32 (16%) 3389 (25.5%) 136 (2.9%) 1354 (14.5%)Expansion.Restatement 55 (20.6%) 6 (7.1%) 4 (2%) 2591 (19.5%) 93 (2.0%) 20 (0.2%)Expansion.Conjunction 40 (15.0%) 20 (23.5%) 31 (15.5%) 2908 (21.9%) 1144 (24.3%) 1907 (20.4%)Comparison.Contrast 42 (15.7%) 20 (23.5%) 29 (14.5%) 1704 (12.8%) 1853 (39.4%) 1416 (15.2%)Expansion.Instantiation 14 (5.2%) 3 (3.5%) ?
1152 (8.7%) 236 (5.0%) 18 (0.2%)Total 267 85 200 13287 4709 9336Figure 9: Letters and News: Connective sense frequency2008).
There are two points to make.
First, Fig-ure 7 provides evidence (in terms of differencesbetween genres in the senses associated with inter-sentential discourse relations that are not lexicallymarked) for taking genre as a factor in automatedsense labelling of those relations.Secondly, Figures 8 and 9 summarize Figures 5,6 and 7 with respect to the five senses that oc-cur most frequently in the four genre with dis-course relations that are not lexically marked,covering between 84% and 91% of those rela-tions.
These Figures show that, no matter whatgenre one considers, different senses tend to beexpressed with (explicit) intra-sentential connec-tives, with explicit inter-sentential connectives andwith implicit connectives.
This means that lexi-cally marked relations provide a poor model forautomated sense labelling of relations that are notlexically marked.
This is new evidence for thesuggestion (Sporleder and Lascarides, 2008) thatintrinsic differences between explicit and implicitdiscourse relations mean that the latter have to belearned independently of the former.8 ConclusionThis paper has, for the first time, provided genreinformation about the articles in the Penn Tree-Bank.
It has characterised each genre in terms offeatures manually annotated in the Penn DiscourseTreeBank, and used this to show that genre shouldbe made a factor in automated sense labelling ofdiscourse relations that are not explicitly marked.There are clearly other potential differences thatone might usefully investigate: For example, fol-lowing (Pitler et al, 2008), one might look atwhether connectives with multiple senses occurwith only one of those senses (or mainly so) ina particular genre.
Or one might investigate howpatterns of attribution vary in different genres,since this is relevant to subjectivity in text.
Otheraspects of genre may be even more significant forlanguage technology.
For example, whereas thefirst sentence of a news article might be an effec-tive summary of its contents ?
e.g.
(4) Singer Bette Midler won a $400,000 federalcourt jury verdict against Young & Rubicamin a case that threatens a popular advertisingindustry practice of using ?sound-alike?
per-formers to tout products.
(wsj 0485)it might be less so in the case of an essay, even oneof about the same length ?
e.g.
(5) On June 30, a major part of our trade deficitwent poof!
(wsj 0447)Of course, to exploit these differences, it is im-portant to be able to automatically identify whatgenre or genres a text belongs to.
Fortunately,there is a growing body of work on genre-basedtext classification, including (Dewdney et al,2001; Finn and Kushmerick, 2006; Kessler et al,1997; Stamatatos et al, 2000; Wolters and Kirsten,1999).
Of particular interest in this regard iswhether other news corpora, such as the New YorkTimes Annotated Corpus (Linguistics Data Con-sortium Catalog Number: LDC2008T19) manifestsimilar properties to theWSJ in their different gen-res.
If so, then genre-specific extrapolation fromthe WSJ Corpus may enable better performanceon a wider range of corpora.AcknowledgmentsI thank my three anonymous reviewers for theiruseful comments.
Additional thoughtful com-ments came from Mark Steedman, Alan Lee,Rashmi Prasad and Ani Nenkova.ReferencesDouglas Biber.
1986.
Spoken and written textual di-mensions in english.
Language, 62(2):384?414.Douglas Biber.
2003.
Compressed noun-phrase struc-tures in newspaper discourse.
In Jean Aitchison andDiana Lewis, editors, New Media Language, pages169?181.
Routledge.681Lynn Carlson, Daniel Marcu, and Mary EllenOkurowski.
2002.
Building a discourse-tagged cor-pus in the framework of rhetorical structure theory.In Proceedings of the 2nd SIGdial Workshop on Dis-course and Dialogue, Aalborg, Denmark.Nigel Dewdney, Carol VanEss-Dykema, and RichardMacMillan.
2001.
The form is the substance:classification of genres in text.
In Proceedings ofthe Workshop on Human Language Technology andKnowledge Management, pages 1?8.Robert Elwell and Jason Baldridge.
2008.
Discourseconnective argument identication with connectivespecic rankers.
In Proceedings of the IEEE Con-ference on Semantic Computing.Evan Sandhaus.
2008.
New york times corpus: Corpusoverview.
Provided with the corpus, LDC catalogueentry LDC2008T19.Aidan Finn and Nicholas Kushmerick.
2006.
Learningto classify documents according to genre.
Journalof the American Society for Information Science andTechnology, 57.Brett Kessler, Geoffrey Numberg, and Hinrich Schu?tze.1997.
Automatic detection of text genre.
In Pro-ceedings of the 35th Annual Meeting of the ACL,pages 32?38.Daniel Marcu and Abdessamad Echihabi.
2002.
Anunsupervised approach to recognizing discourse re-lations.
In Proceedings of the Association for Com-putational Linguistics.Eleni Miltsakaki, Livio Robaldo, Alan Lee, and Ar-avind Joshi.
2008.
Sense annotation in the penndiscourse treebank.
In Computational Linguisticsand Intelligent Text Processing, pages 275?286.Springer.Emily Pitler and Ani Nenkova.
2008.
Revisitingreadability: A unified framework for predicting textquality.
In Proceedings of EMNLP.Emily Pitler, Mridhula Raghupathy, Hena Mehta, AniNenkova, Alan Lee, and Aravind Joshi.
2008.
Eas-ily identifiable discourse relations.
In Proceedingsof COLING, Manchester.Emily Pitler, Annie Louis, and Ani Nenkova.
2009.Automatic sense prediction for implicit discourse re-lations in text.
In Proceedings of ACL-IJCNLP, Sin-gapore.Rashmi Prasad, Nikhil Dinesh, Alan Lee, AravindJoshi, and Bonnie Webber.
2007.
Attribution andits annotation in the Penn Discourse TreeBank.
TAL(Traitement Automatique des Langues), 42(2).Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-sakaki, Livio Robaldo, Aravind Joshi, and BonnieWebber.
2008.
The Penn Discourse TreeBank2.0.
In Proceedings, 6th International Conferenceon Language Resources and Evaluation, Marrakech,Morocco.Mark Rosso.
2008.
User-based identification of webgenres.
J American Society for Information Scienceand Technology, 59(7):1053?1072.Caroline Sporleder and Alex Lascarides.
2008.
Usingautomatically labelled examples to classify rhetori-cal relations: an assessment.
Natural Language En-gineering, 14(3):369?416.Efstathios Stamatatos, Nikos Fakotakis, and GeorgeKokkinakis.
2000.
Text genre detection using com-mon word frequencies.
In Proceedings of the 18thAnnual Conference of the ACL, pages 808?814.John Swales.
1990.
Genre Analysis.
Cambridge Uni-versity Press, Cambridge.Ben Wellner and James Pustejovsky.
2007.
Automati-cally identifying the arguments to discourse connec-tives.
In Proceedings of the 2007 Conference onEmpirical Methods in Natural Language Processing(EMNLP), Prague CZ.Ben Wellner.
2008.
Sequence Models and RankingMethods for Discourse Parsing.
Ph.D. thesis, Bran-deis University.Maria Wolters and Mathias Kirsten.
1999.
Exploringthe use of linguistic features in domain and genreclassification.
In Proceedings of the 9th Meeting ofthe European Chapter of the Assoc.
for Computa-tional Linguistics, pages 142?149, Bergen, Norway.682
