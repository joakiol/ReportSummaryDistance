Proceedings of the SIGDIAL 2013 Conference, pages 404?413,Metz, France, 22-24 August 2013. c?2013 Association for Computational LinguisticsThe Dialog State Tracking ChallengeJason Williams1, Antoine Raux2?, Deepak Ramachandran3?, and Alan Black41Microsoft Research, Redmond, WA, USA 2Lenovo Corporation, Santa Clara, CA, USA3Nuance Communications, Mountain View, CA, USA 4Carnegie Mellon University, Pittsburgh, PA, USAjason.williams@microsoft.com araux@lenovo.com deepak.ramachandran@nuance.com awb@cmu.eduAbstractIn a spoken dialog system, dialog statetracking deduces information about theuser?s goal as the dialog progresses, syn-thesizing evidence such as dialog acts overmultiple turns with external data sources.Recent approaches have been shown toovercome ASR and SLU errors in someapplications.
However, there are currentlyno common testbeds or evaluation mea-sures for this task, hampering progress.The dialog state tracking challenge seeksto address this by providing a heteroge-neous corpus of 15K human-computer di-alogs in a standard format, along with asuite of 11 evaluation metrics.
The chal-lenge received a total of 27 entries from 9research groups.
The results show that thesuite of performance metrics cluster into 4natural groups.
Moreover, the dialog sys-tems that benefit most from dialog statetracking are those with less discriminativespeech recognition confidence scores.
Fi-nally, generalization is a key problem: in2 of the 4 test sets, fewer than half of theentries out-performed simple baselines.1 Overview and motivationSpoken dialog systems interact with users via nat-ural language to help them achieve a goal.
As theinteraction progresses, the dialog manager main-tains a representation of the state of the dialogin a process called dialog state tracking (DST).For example, in a bus schedule information sys-tem, the dialog state might indicate the user?s de-sired bus route, origin, and destination.
Dialogstate tracking is difficult because automatic speech?Most of the work for the challenge was performed whenthe second and third authors were with Honda Research In-stitute, Mountain View, CA, USArecognition (ASR) and spoken language under-standing (SLU) errors are common, and can causethe system to misunderstand the user?s needs.
Atthe same time, state tracking is crucial becausethe system relies on the estimated dialog state tochoose actions ?
for example, which bus scheduleinformation to present to the user.Most commercial systems use hand-craftedheuristics for state tracking, selecting the SLU re-sult with the highest confidence score, and dis-carding alternatives.
In contrast, statistical ap-proaches compute scores for many hypotheses forthe dialog state (Figure 1).
By exploiting correla-tions between turns and information from externaldata sources ?
such as maps, bus timetables, ormodels of past dialogs ?
statistical approaches canovercome some SLU errors.Numerous techniques for dialog state trackinghave been proposed, including heuristic scores(Higashinaka et al 2003), Bayesian networks(Paek and Horvitz, 2000; Williams and Young,2007), kernel density estimators (Ma et al 2012),and discriminative models (Bohus and Rudnicky,2006).
Techniques have been fielded which scaleto realistically sized dialog problems and operatein real time (Young et al 2010; Thomson andYoung, 2010; Williams, 2010; Mehta et al 2010).In end-to-end dialog systems, dialog state trackinghas been shown to improve overall system perfor-mance (Young et al 2010; Thomson and Young,2010).Despite this progress, direct comparisons be-tween methods have not been possible becausepast studies use different domains and systemcomponents, for speech recognition, spoken lan-guage understanding, dialog control, etc.
More-over, there is little agreement on how to evaluatedialog state tracking.
Together these issues limitprogress in this research area.The Dialog State Tracking Challenge (DSTC)provides a first common testbed and evaluation404Figure 1: Overview of dialog state tracking.
In this example, the dialog state contains the user?s desiredbus route.
At each turn t, the system produces a spoken output.
The user?s spoken response is processedto extract a set of spoken language understanding (SLU) results, each with a local confidence score.
Aset of Nt dialog state hypotheses is formed by considering all SLU results observed so far, including thecurrent turn and all previous turns.
Here, N1 = 3 and N2 = 5.
The dialog state tracker uses features ofthe dialog context to produce a distribution over all Nt hypotheses and the meta-hypothesis that none ofthem are correct.suite for dialog state tracking.
The DSTC orga-nizers made available a public, heterogeneous cor-pus of over 15K transcribed and labeled human-computer dialogs.
Nine teams entered the chal-lenge, anonymously submitting a total of 27 dialogstate trackers.This paper serves two roles.
First, sections 2and 3 provide an overview of the challenge, data,and evaluation metrics, all of which will remainpublicly available to the community (DST, 2013).Second, this paper summarizes the results of thechallenge, with an emphasis on gaining new in-sights into the dialog state tracking problem, inSection 4.
Section 5 briefly concludes.2 Challenge overview2.1 Problem statementFirst, we define the dialog state tracking problem.A dialog state tracker takes as input all of the ob-servable elements up to time t in a dialog, includ-ing all of the results from the automatic speechrecognition (ASR) and spoken language under-standing (SLU) components, and external knowl-edge sources such as bus timetable databases andmodels of past dialogs.
It also takes as input aset of Nt possible dialog state hypotheses, wherea hypothesis is an assignment of values to slots inthe system.
The tracker outputs a probability dis-tribution over the set of Nt hypotheses, and themeta-hypothesis REST which indicates that noneof them are correct.
The goal is to assign probabil-ity 1.0 to the correct state, and 0.0 to other states.Note that the set of dialog states is given.
Alsonote that Nt varies with t ?
typically as the dia-log progresses and more concepts are discussed,the number of candidate hypotheses increases.
Anexample is given in Figure 1.In this challenge, dialog states are generated inthe usual way, by enumerating all slots values thathave appeared in the SLU N-best lists or systemoutput up until the current turn.
While this ap-proach precludes a tracker assigning a score to an405SLU value that has not been observed, the cardi-nality of the slots is generally large, so the likeli-hood of a tracker correctly guessing a slot valuewhich hasn?t been observed anywhere in the inputor output is vanishingly small.2.2 Challenge designThe dialog state tracking challenge studies thisproblem as a corpus-based task ?
i.e., dialog statetrackers are trained and tested on a static corpusof dialogs, recorded from systems using a varietyof state tracking models and dialog managers.
Thechallenge task is to re-run state tracking on thesedialogs ?
i.e., to take as input the runtime systemlogs including the SLU results and system output,and to output scores for dialog states formed fromthe runtime SLU results.
This corpus-based de-sign was chosen because it allows different track-ers to be evaluated on the same data, and because acorpus-based task has a much lower barrier to en-try for research groups than building an end-to-enddialog system.In practice of course, a state tracker will be usedin an end-to-end dialog system, and will drive ac-tion selection, thereby affecting the distribution ofthe dialog data the tracker experiences.
In otherwords, it is known in advance that the distribu-tion in the training data and live data will be mis-matched, although the nature and extent of themis-match are not known.
Hence, unlike muchof supervised learning research, drawing train andtest data from the same distribution in offline ex-periments may overstate performance.
So in theDSTC, train/test mis-match was explicitly createdby choosing test data to be from different dialogsystems.2.3 Source data and challenge corporaThe DSTC uses data from the public deploymentof several systems in the Spoken Dialog Challenge(SDC) (Black et al 2010), provided by the DialogResearch Center at Carnegie Mellon University.
Inthe SDC, telephone calls from real passengers ofthe Port Authority of Allegheny County, who runscity buses in Pittsburgh, were forwarded to dialogsystems built by different research groups.
Thegoal was to provide bus riders with bus timetableinformation.
For example, a caller might wantto find out the time of the next bus leaving fromDowntown to the airport.The SDC received dialog systems from threedifferent research groups, here called Groups A,B, and C. Each group used its own ASR, SLU,and dialog manager.
The dialog strategies acrossgroups varied considerably: for example, GroupsA and C used a mixed-initiative design, where thesystem could recognize any concept at any turn,but Group B used a directed design, where thesystem asked for concepts sequentially and couldonly recognize the concept being queried.
Groupstrialled different system variants over a period ofalmost 3 years.
These variants differed in acousticand language models, confidence scoring model,state tracking method and parameters, number ofsupported bus routes, user population, and pres-ence of minor bugs.
Example dialogs from eachgroup are shown in the Appendix.The dialog data was partitioned into 5 train-ing corpora and 4 testing corpora (Table 1).The partioning was intended to explore differenttypes of mis-match between the training and testdata.
Specifically, the dialog system in TRAIN1A,TRAIN1B, TRAIN1C, TRAIN2, and TEST1 are allvery similar, so TEST1 tests the case where thereis a large amount of similar data.
TEST2 uses thesame ASR and SLU but a different dialog con-troller, so tests the case where there is a largeamount of somewhat similar data.
TEST3 is verysimilar to TRAIN3 and tests the case where thereis a small amount of similar data.
TEST4 uses acompletely different dialog system to any of thetraining data.2.4 Data preparationThe dialog system log data from all three groupswas converted to a common format, whichdescribed SLU results and system output usinga uniform set of dialog acts.
For example, thesystem speech East Pittsburgh Bus Schedules.Say a bus route, like 28X, or say I?m not sure.was represented as hello(), request(route), exam-ple(route=28x), example(route=dontknow).
Theuser ASR hypothesis the next 61c from oakland tomckeesport transportation center was representedas inform(time.rel=next), inform(route=61c),inform(from.neighborhood=oakland), in-form(to.desc=?mckeesport transportationcenter?).
In this domain there were a totalof 9 slots: the bus route, date, time, and threecomponents each for the origin and destination,corresponding to streets, neighborhoods, andpoints-of-interest like universities.
For completedetails see (Williams et al 2012).406TRAIN TEST1A 1B 1C 2 3 1 2 3 4Group A A A A B A A B CYear(s) 2009 2009 2009 2010 2010 2011 2012 2011-2 2010Dialogs 1013 1117 9502 643 688 715 750 1020 438Turns/Dialog 14.7 13.3 14.5 14.5 12.6 14.1 14.5 13.0 10.9Sys acts/turn 4.0 3.8 3.8 4.0 8.4 2.8 3.2 8.2 4.6Av N-best len 21.7 22.3 21.9 22.4 2.9 21.2 20.5 5.0 3.2Acts/N-best hyp 2.2 2.2 2.2 2.3 1.0 2.1 2.0 1.0 1.6Slots/turn 44.0 46.5 45.6 49.0 2.1 41.4 36.9 4.3 3.5Transcribed?
yes yes yes yes yes yes yes yes yesLabelled?
yes no no yes yes yes yes yes yes1-best WER 42.9% 41.1% 42.1% 58.2% 40.5% 57.9% 62.1% 48.1% 55.6%1-best SLU Prec.
0.356 - - 0.303 0.560 0.252 0.275 0.470 0.3341-best SLU Recall 0.522 - - 0.388 0.650 0.362 0.393 0.515 0.376N-best SLU Recall 0.577 - - 0.485 0.738 0.456 0.492 0.634 0.413Table 1: Summary of the datasets.
One turn includes a system output and a user response.
Slots arenamed entity types such as bus route, origin neighborhood, date, time, etc.
N-best SLU Recall indicatesthe fraction of concepts which appear anywhere on the SLU N-best list.Group B and C systems produced N-best listsof ASR and SLU output, which were included inthe log files.
Group A systems produced only 1-best lists, so for Group A systems, recognition wasre-run with the Pocketsphinx speech recognizer(Huggins-Daines et al 2006) with N-best outputenabled, and the results were included in the logfiles.Some information in the raw system logs wasspecific to a group.
For example, Group B?s logsincluded information about word confusion net-works, but other groups did not.
All of this infor-mation was included in a ?system specific?
sec-tion of the log files.
Group A logs contained about40 system-specific name/value pairs per turn, andGroup B about 600 system-specific name/valuepairs per turn.
Group C logs contained no systemspecific data.3 Labeling and evaluation designThe output of a dialog state tracker is a proba-bility distribution over a set of given dialog statehypotheses, plus the REST meta-hypothesis.
Toevaluate this output, a label is needed for each di-alog state hypothesis indicating its correctness.In this task-oriented domain, we note that theuser enters the call with a specific goal in mind.Further, when goal changes do occur, they areusually explicitly marked: since all of the sys-tems first collect slot values, and then provide bustimetables, if the user wishes to change their goal,they need to start over from the beginning.
These?start over?
transitions are obvious in the logs.This structure allows the correctness of each di-alog state to be equated to the correctness of theSLU items it contains.
As a result, in the DSTCwe labeled the correctness of SLU hypotheses ineach turn, and then assumed these labels remainvalid until either the call ends, or until a ?startover?
event.
Thus to produce the labels, the la-beling task followed was to assign a correctnessvalue to every SLU hypothesis on the N-best list,given a transcript of the words actually spoken inthe dialog up to the current turn.To accomplish this, first all user speech wastranscribed.
The TRAIN1 datasets had been tran-scribed using crowd-sourcing in a prior project(Parent and Eskenazi, 2010); the remainder weretranscribed by professionals.
Then each SLU hy-pothesis was labled as correct or incorrect.
When atranscription exactly and unambiguously matcheda recognized slot value, such as the bus route?sixty one c?, labels were assigned automati-cally.
The remainder were assigned using crowd-sourcing, where three workers were shown the truewords spoken and the recognized concept, andasked to indicate if the recognized concept wascorrect ?
even if it did not match the recognizedwords exactly.
Workers were also shown dialog407history, which helps decipher the user?s meaningwhen their speech was ambiguous.
If the 3 work-ers were not unanimous in their labels (about 4%of all turns), the item was labeled manually by theorganizers.
The REST meta-hypothesis was notexplicitly labeled; rather, it was deemed to be cor-rect if none of the prior SLU results were labeledas correct.In this challenge, state tracking performancewas measured on each of the 9 slots separately,and also on a joint dialog state consisting of all theslots.
So at each turn in the dialog, a tracker output10 scored lists: one for each slot, plus a 10th listwhere each dialog state contains values from allslots.
Scores were constrained to be in the range[0, 1] and to sum to 1.To evaluate tracker output, at each turn, each hy-pothesis (including REST) on each of the 10 listswas labeled as correct or incorrect by looking upits corresponding SLU label(s).
The scores and la-bels over all of the dialogs were then compiled tocompute 11 metrics.
Accuracy measures the per-cent of turns where the top-ranked hypothesis iscorrect.
This indicates the correctness of the itemwith the maximum score.
L2 measures the L2 dis-tance between the vector of scores, and a vector ofzeros with 1 in the position of the correct hypoth-esis.
This indicates the quality of all scores, whenthe scores as viewed as probabilities.AvgP measures the mean score of the first cor-rect hypothesis.
This indicates the quality of thescore assigned to the correct hypothesis, ignoringthe distribution of scores to incorrect hypotheses.MRR measures the mean reciprocal rank of thefirst correct hypothesis.
This indicates the qualityof the ordering the scores produces (without nec-essarily treating the scores as probabilities).The remaining measures relate to receiver-operating characteristic (ROC) curves, whichmeasure the discrimination of the score for thehighest-ranked state hypothesis.
Two versionsof ROC are computed ?
V1 and V2.
V1 com-putes correct-accepts (CA), false-accepts (FA),and false-rejects (FR) as fractions of all utter-ances, so for exampleCA.V 1(s) = #CA(s)N (1)where #CA(s) indicates the number of correctlyaccepted states when only those states with score?
s are accepted, and N is the total numberof states in the sample.
The V1 metrics are a20%30%40%50%60%70%80%90%100%schedule2 accuracy for allslotsTrackers Oracle Baseline0 Baseline1train293% test175% test289% test348%train382% test438%Figure 2: Schedule2 accuracy averaged over slotsfor every tracker on every dataset.
Percentages un-der the datasets indicate the percent of the track-ers which exceeded the performance of both base-lines.useful indication of overall performance becausethey combine discrimination and overall accuracy?
i.e., the maximum CA.V 1(s) value is equal toaccuracy computed above.V2 considers fractions of correctly classified ut-terances, so for exampleCA.V 2(s) = #CA(s)#CA(0) .
(2)The V2 metrics are useful because they measurethe discrimination of the scoring independently ofaccuracy ?
i.e., the maximum value of CA.V 2(s)is always 1, regardless of accuracy.From these ROC statistics, several met-rics are computed.
ROC.V1.EER computesFA.V 1(s) where FA.V 1(s) = FR.V 1(s).The metrics ROC.V1.CA05, ROC.V1.CA10,and ROC.V1.CA20 compute CA.V 1(s) whenFA.V 1(s) = 0.05, 0.10, and 0.20 respec-tively.
ROC.V2.CA05, ROC.V2.CA10, andROC.V2.CA20 do the same using the V2 ver-sions.Apart from what to measure, there is currentlyno standard that specifies when to measure ?
i.e.,which turns to include when computing each met-ric.
So for this challenge, a set of 3 schedules wereused.
schedule1 includes every turn.
schedule2include turns where the target slot is either presenton the SLU N-best list, or where the target slotis included in a system confirmation action ?
i.e.,where there is some observable new information4080%10%20%30%40%50%60%70%80%90%100%0% 20% 40% 60% 80% 100%Truepositive rateFalse positive ratetest4test3test2test1Figure 3: Receiver operating characteristc (ROC)curve for SLU confidence scores of the 1-best hy-pothesis in the test datasets.
The SLU confidencescore in TEST3 is most discriminative; TEST1 andTEST2 are the least discriminative.about the target slot.
schedule3 includes only thelast turn of a dialog.In sum, for each tracker, one measurement is re-ported for each test set (4), schedule (3), and met-ric (11) for each of the 9 slots, the ?joint?
slot, anda weighted average of the individual slots (11), fora total of 4 ?
3 ?
11 ?
11 = 1452 measurements pertracker.
In addition, each tracker reported averagelatency per turn ?
this ranged from 10ms to 1s.3.1 Baseline trackersFor comparisons, two simple baselines were im-plemented.
The first (Baseline0) is a majorityclass baseline that always guesses REST withscore 1.
The second (Baseline1) follows simplerules which are commonly used in spoken dialogsystems.
It maintains a single hypothesis for eachslot.
Its value is the SLU 1-best with the highestconfidence score observed so far, with score equalto that SLU item?s confidence score.4 Results and discussionLogistically, the training data and labels, bustimetable database, scoring scripts, and baselinesystem were publicly released in late December2012.
The test data (without labels) was releasedon 22 March 2013, and teams were given a week torun their trackers and send results back to the orga-nizers for evaluation.
After the evaluation, the testlabels were published.
Each team could enter upto 5 trackers.
For the evaluation, teams were askedto process the test dialogs online ?
i.e., to make a135791113151719accuracy l2 roc.v1_eer roc.v2_ca05Averagerankin test datasetsMetric - schedule2 - weighted average over all slotsT3.E2T5.E1T5.E2T5.E5T6.E2T6.E3T6.E4T6.E5T9.E1Figure 4: Average rank of top-performing trackersfor the four metrics identified in Figure 6.
Rank-ing was done using the given metric, schedule2,and the weighted average of all slots.
Tn.Em in-dicates team n, entry m.single pass over the data, as if the tracker were be-ing run in deployment.
Participation was open toresearchers at any institution, including the orga-nizers and advisory board.
To encourage partici-pation, the organizers agreed not to identify par-ticipants in publications, and there was no require-ment to disclose how trackers were implemented.9 teams entered the DSTC, submitting a total of27 trackers.
The raw output and all 1452 measure-ments for each tracker (and the 2 baselines) areavailable from the DSTC homepage (DST, 2013).4.1 Analysis of trackers and datasetsWe begin by looking at one illustrative metric,schedule2 accuracy averaged over slots, whichmeasures the accuracy of the top dialog hypothe-sis for every slot when it either appears on the SLUN-best list or is confirmed by the system.1 Resultsin Figure 2 show two key trends.
First, relativeto the baselines, performance on the test data ismarkedly lower than the training data.
ComparingTRAIN2 to TEST1/TEST2 and TRAIN3 to TEST3,the relative gain over the baselines is much loweron test data.
Moreover, only 38% of trackers per-formed better than a simple majority-class base-line on TEST4, for which there was no matchedtraining data.
These findings suggests that gen-eralization is an important open issues for dialogstate trackers.Second, Figure 2 indicates that the gains made1Results using the joint dialog state are broadly similar,and are omitted for space.40920%30%40%50%60%70%80%90%0% 5% 10% 15% 20% 25% 30% 35% 40%schedule2accuracyfor all slots% of turns where top dialog hypothesis was not top SLU resultTrackers Baseline0 Baseline1(a) TEST120%30%40%50%60%70%80%90%0% 5% 10% 15% 20% 25% 30% 35% 40%schedule2accuracyfor all slots% of turns where top dialog hypothesis was not top SLU result(b) TEST220%30%40%50%60%70%80%90%0% 5% 10% 15% 20% 25% 30% 35% 40%schedule2accuracyfor all slots% of turns where top dialog hypothesis was not top SLU result(c) TEST320%30%40%50%60%70%80%90%0% 5% 10% 15% 20% 25% 30% 35% 40%schedule2accuracyfor all slots% of turns where top dialog hypothesis was not top SLU result(d) TEST4Figure 5: Percent of highest-scored dialog state hypotheses which did not appear in the top-ranked SLUposition vs. schedule2 accuracy over all slots.
Trackers ?
including those with the highest accuracy ?for TEST1 and TEST2 rarely assigned the highest score to an SLU hypothesis other than the top.
Alltrackers for TEST3 and TEST4 assigned the highest score to an SLU hypothesis other than the top in anon-trivial percent of turns.by the trackers over the baselines are largerfor Group A systems (TEST1 and TEST2) thanfor Group B (TEST3) and C (TEST4) systems.Whereas the baselines consider only the top SLUhypothesis, statistical trackers can make use ofthe entire N-best list, increasing recall ?
comparethe 1-best and N-best SLU recall rates in Table 1.However, Group A trackers almost never assignedthe highest score to an item below the top positionin the SLU N-best list.
Rather, the larger gains forGroup A systems seem due to the relatively poordiscrimination of Group A?s SLU confidence score(Figure 3): whereas the trackers use a multitudeof features to assign scores, the baselines rely en-tirely on the SLU confidence for their scores, soundiscriminative SLU confidence measures ham-per baseline performance.4.2 Analysis of metricsThis challenge makes it possible to study the em-pirical differences among the evaluation metrics.Intuitively, if the purpose of a metric is to ordera set of trackers from best to worst, then 2 met-rics are similar if they yield a similar ordering overtrackers.
Specifically, for every metricm, we havea value x(m, d, s, t) where d is the dataset, ands is the evaluation schedule, and t is the tracker.We define r(m, d, s, t) as the rank of tracker twhen ordered using metric m, dataset d and eval-uation schedule s. Using these ranks, we computeKendall?s Tau for every d, s, and pair of metricsm1 and m2 (Kendall, 1938).
We then compute theaverage Kendall?s Tau for m1 and m2 by averag-ing over all d and s.2Results are in Figure 6.
Here we see 4 natu-ral clusters emerge: a cluster for correctness withAccuracy, MRR, and the ROC.V1.CA measures; acluster for probability quality with L2 and Aver-age score; and two clusters for score discrimina-tion ?
one with ROC.V1.EER and the other withthe three ROC.V2 metrics.
This finding suggest2A similar analysis over schedules showed that the differ-ences in ranking for different schedules were smaller than formetrics.410accuracymrrroc_v1.ca05roc_v1.ca10roc_v1.ca20roc.v1_eeravgpl2roc.v2_ca05roc.v2_ca05roc.v2_ca05Figure 6: Average divergence between rank orderings produced by different metrics.
The size of a circleat (x, y) is given by 1??
, where ?
is the average Kendall?s Tau computed on the rank orderings producedby methods x and y.
Larger circles indicate dissimilar rankings; smaller circles indicate similar rankings;missing circles indicate identical rankings.
The red boxes indicate groups of metrics that yield similarrankings.that measuring one metric from each cluster willcontain nearly the same information as all 9 met-rics.
For example, one might report only Accu-racy, L2, ROC.V1.EER, and ROC.V2.CA5.Using these 4 metrics, we rank-ordered eachtracker, using schedule2 and a weighted averageof all slots.
We then computed the average rankacross the 4 test sets.
Finally we selected the setof trackers with the top three average ranks foreach metric.
Results in Figure 4 emphasize thatdifferent trackers are tuned for different perfor-mance measures, and the optimal tracking algo-rithm depends crucially on the target performancemeasure.5 ConclusionThe dialog state tracking challenge has providedthe first common testbed for this task.
The data,evaluation tools, and baselines will continue to befreely available to the research community (DST,2013).
The details of the trackers themselves willbe published at SIGDIAL 2013.The results of the challenge show that thesuite of performance metrics cluster into 4 naturalgroups.
We also find that larger gains over conven-tional rule-based baselines are present in dialogsystems where the speech recognition confidencescore has poor discrimination.
Finally, we observesubstantial limitations on generalization: in mis-matched conditions, around half of the trackers en-tered did not exceed the performance of two sim-ple baselines.In future work, it should be verified that im-provements in dialog state tracking lead to im-provements in end-to-end dialog performance(e.g., task completion, user satisfaction, etc.).
Inaddition, it would be interesting to study dialogswhere goal changes are more common.AcknowledgementsThe organizers thank the advisory board for theirvaluable input on the design of the challenge:Daniel Boies, Paul Crook, Maxine Eskenazi, Mil-ica Gasic, Dilek Hakkani-Tur, Helen Hastie, Kee-Eung Kim, Ian Lane, Sungjin Lee, Teruhisa Misu,Olivier Pietquin, Joelle Pineau, Blaise Thomson,David Traum, and Luke Zettlemoyer.
The orga-nizers also thank Ian Lane for his support for tran-scription, and Microsoft and Honda Research In-stitute USA for funding the challenge.
Finally,we thank the participants for making the challengesuccessful.411ReferencesAW Black, S Burger, B Langner, G Parent, and M Es-kenazi.
2010.
Spoken dialog challenge 2010.
InProc SLT, Berkeley.D Bohus and AI Rudnicky.
2006.
A ?K hypotheses +other?
belief updating model.
In Proc AAAI Work-shop on Statistical and Empirical Approaches forSpoken Dialogue Systems, Boston.2013.
Dialog State Tracking Challenge Home-page.
http://research.microsoft.com/events/dstc/.H Higashinaka, M Nakano, and K Aikawa.
2003.Corpus-based discourse understanding in spoken di-alogue systems.
In Proc ACL, Sapporo.D Huggins-Daines, M Kumar, A Chan, A W Black,M Ravishankar, and A I Rudnicky.
2006.
Pock-etSphinx: A Free, Real-Time Continuous SpeechRecognition System for Hand-Held Devices.
InProc ICASSP, Toulouse.M Kendall.
1938.
A new measure of rank correlation.Biometrika, 30(1-2):81?89.Y Ma, A Raux, D Ramachandran, and R Gupta.
2012.Landmark-based location belief tracking in a spokendialog system.
In Proc SigDial, Seoul.N Mehta, R Gupta, A Raux, D Ramachandran, andS Krawczyk.
2010.
Probabilistic ontology trees forbelief tracking in dialog systems.
In Proc SigDial,Tokyo.T Paek and E Horvitz.
2000.
Conversation as ac-tion under uncertainty.
In Proc UAI, Stanford, pages455?464.G Parent and M Eskenazi.
2010.
Toward Bet-ter Crowdsourced Transcription: Transcription of aYear of the Let?s Go Bus Information System Data.In Proc SLT, Berkeley.B Thomson and SJ Young.
2010.
Bayesian updateof dialogue state: A POMDP framework for spokendialogue systems.
Computer Speech and Language,24(4):562?588.JD Williams and SJ Young.
2007.
Partially observableMarkov decision processes for spoken dialog sys-tems.
Computer Speech and Language, 21(2):393?422.JD Williams, A Raux, D Ramachandran, andAW Black.
2012.
Dialog state tracking challengehandbook.
Technical report, Microsoft Research.JD Williams.
2010.
Incremental partition recombina-tion for efficient tracking of multiple dialogue states.In Proc.
of ICASSP.SJ Young, M Gas?ic?, S Keizer, F Mairesse, J Schatz-mann, B Thomson, and K Yu.
2010.
The hid-den information state model: a practical frameworkfor POMDP-based spoken dialogue management.Computer Speech and Language, 24(2):150?174.Appendix 1: Sample dialog excerptsBelow are transcripts of calls in the DSTC corpus.The user?s side shows utterances as transcribed byhuman annotators.Group ASYS: Welcome to the CMU Let?s Go bus informa-tion system.
To get help at any time, just say Helpor press zero.
What can I do for you?USR: 61c evening hoursSYS: For example, you can say, when is the next28X from DOWNTOWN to THE AIRPORT?
orI?d like to go from MCKEESPORT to HOME-STEAD tomorrow at 10 a.m.USR: i?d like to go [noise]SYS: Where are you leaving from?USR: from Kennywood tomorrow at 6:30 PMSYS: Leaving from MCKEE.
Is this correct?USR: yesSYS: Okay What is your destination?USR: downtownSYS: Going to DOWNTOWN.
Is this correct?USR: yesSYS: Right When do you want to travel?USR: 6:30 PMSYS: Leaving at 6 30 p.m.. Is this correct?USR: [noise] yesSYS: Alright Hold on.
I?ll look that up.
There isa 71D leaving FIFTH AVENUE AT DARRAGHMCKEE at 6 40 p.m..
It will arrive at FIFTH AV-ENUE AT MARKET at 6 58 p.m.. To get moreinformation about buses related to this trip, youcan say, when is the next bus, or, when is the pre-vious bus.
To ask about a different trip, you cansay, start a new query.
If you are finished, you cansay goodbye.USR: start a new querySYS: Okay, let?s start from the beginning.USR: ...Group BSYS: East Pittsburgh Bus Schedules.
Say a busroute, like 28X, or say I?m not sure.USR: 61ASYS: Okay, 61A.
To change, say go back.
Whereare you leaving from?USR: DowntownSYS: Okay, downtown.
You can always say goback.
And where are you going to?USR: East Pittsburgh East PittsburghSYS: Pittsburgh.
Is that right?412USR: NoSYS: Oh, sorry, where are you going to?USR: East PittsburghSYS: Okay, East Pittsburgh.
Do you want timesfor the next few buses?
Say yes or no.USR: YesSYS: Ok, the next few buses.
To change, say goback.
At 7:45 PM today, there is a 61 A, fromFifth Avenue And Market Street, Downtown, ar-riving Dynamo Alley And Beech Avenue, In EastPittsburgh, at 8:42 PM.
Say repeat, next, previous,or start over.Group CSYS: Welcome to the Lets Go!
bus informationsystem for Pittsburgh.
Where would you like toleave from?USR: [noise]SYS: Where do you wanna leave from?USR: [noise] um I?m leaving from Forbes avenueand Murray avenueSYS: I?m sorry but I do not know any stops in themoon neighborhood.
The routes I currently coverare the following: 28X, 54C, 56U, 61A, 61B, 61C,61D and 64.USR: um I need the scheduling information for the61DSYS: Where would you like to leave from?USR: Forbes and MurraySYS: Where do you wanna go?USR: GreenfieldSYS: When are you going to take that bus?USR: 61DSYS: When are you going to take that bus?USR: 61DSYS: When would you like to travel?
Say next busfor the the next bus .USR: next busSYS: The next bus from forbes avenue at murrayto greenfield is the 61D, leaving at 8 43 p.m. .
Itarrives at murray avenue at loretta at 8 52 p.m. .You may now say.
next bus, previous bus, restart,or goodbye.USR: [noise] good-byeSYS: Thank you goodbyeAppendix 2: Example log fragmentFollowing is a log from a call in TRAIN3.
Someelements have been omitted for space.
Logs areformatted in JavaScript Object Notation (JSON).
{"session-id": "dt-201007221854-4808D","turns": [ {"input": {"asr-hyps": [{ "asr-hyp": "sixty one b","score": 0.834314,},...],"slu-hyps": [{ "slu-hyp": [{"act": "inform","slots": [["route", "61b"]]},],"score": 0.834314,},...],"audio-file": "002.raw","start-time": 7.504},"output": {"start-time": 0.078,"transcript": "East Pittsburgh BusSchedules.
Say a bus route, like28X, or say I?m not sure.
","dialog-acts": [{ "act": "hello","slots": [] },{ "act": "request","slots": [["route",null]] },{ "act": "example","slots": [["route","28x"]] },{ "act": "example","slots": [["route","dontknow"]] }],}"system-specific": {"wcn": {"best_path_score": 0.9965,"best_path": {"word": ["sixty","one","b"],"prob": [ 1.0,1.0,0.9965 ],}"network": [ ... ]},"nbest": [{"normSpeechLhood": -152.654,"lastSpeechFrame": 266,"numFrames": 354,"udelta": -3.0280,"speechLikelihood": -15876.0,},...],...},},...]}413
