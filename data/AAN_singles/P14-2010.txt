Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 55?60,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsSprinkling Topics for Weakly Supervised Text ClassificationSwapnil Hingmire1,2swapnil.hingmire@tcs.comSutanu Chakraborti2sutanuc@cse.iitm.ac.in1Systems Research Lab, Tata Research Development and Design Center, Pune, India2Department of Computer Science and Engineering,Indian Institute of Technology Madras, Chennai, IndiaAbstractSupervised text classification algorithmsrequire a large number of documents la-beled by humans, that involve a labor-intensive and time consuming process.In this paper, we propose a weakly su-pervised algorithm in which supervisioncomes in the form of labeling of LatentDirichlet Allocation (LDA) topics.
Wethen use this weak supervision to ?sprin-kle?
artificial words to the training docu-ments to identify topics in accordance withthe underlying class structure of the cor-pus based on the higher order word asso-ciations.
We evaluate this approach to im-prove performance of text classification onthree real world datasets.1 IntroductionIn supervised text classification learning algo-rithms, the learner (a program) takes human la-beled documents as input and learns a decisionfunction that can classify a previously unseen doc-ument to one of the predefined classes.
Usually alarge number of documents labeled by humans areused by the learner to classify unseen documentswith adequate accuracy.
Unfortunately, labelinga large number of documents is a labor-intensiveand time consuming process.In this paper, we propose a text classificationalgorithm based on Latent Dirichlet Allocation(LDA) (Blei et al, 2003) which does not need la-beled documents.
LDA is an unsupervised prob-abilistic topic model and it is widely used to dis-cover latent semantic structure of a document col-lection by modeling words in the documents.
Bleiet al (Blei et al, 2003) used LDA topics as fea-tures in text classification, but they use labeleddocuments while learning a classifier.
sLDA (Bleiand McAuliffe, 2007), DiscLDA (Lacoste-Julienet al, 2008) and MedLDA (Zhu et al, 2009) arefew extensions of LDA which model both classlabels and words in the documents.
These modelscan be used for text classification, but they needexpensive labeled documents.An approach that is less demanding in termsof knowledge engineering is ClassifyLDA (Hing-mire et al, 2013).
In this approach, a topic modelon a given set of unlabeled training documents isconstructed using LDA, then an annotator assignsa class label to some topics based on their mostprobable words.
These labeled topics are usedto create a new topic model such that in the newmodel topics are better aligned to class labels.
Aclass label is assigned to a test document on the ba-sis of its most prominent topics.
We extend Clas-sifyLDA algorithm by ?sprinkling?
topics to unla-beled documents.Sprinkling (Chakraborti et al, 2007) integratesclass labels of documents into Latent Semantic In-dexing (LSI)(Deerwester et al, 1990).
The ba-sic idea involves encoding of class labels as ar-tificial words which are ?sprinkled?
(appended)to training documents.
As LSI uses higher or-der word associations (Kontostathis and Pottenger,2006), sprinkling of artificial words gives betterand class-enriched latent semantic structure.
How-ever, Sprinkled LSI is a supervised technique andhence it requires expensive labeled documents.The paper revolves around the idea of labeling top-ics (which are far fewer in number compared todocuments) as in ClassifyLDA, and using these la-beled topic for sprinkling.As in ClassifyLDA, we ask an annotator to as-sign class labels to a set of topics inferred on theunlabeled training documents.
We use the labeledtopics to find probability distribution of each train-ing document over the class labels.
We create aset of artificial words corresponding to a class la-bel and add (or sprinkle) them to the document.The number of such artificial terms is propor-55tional to the probability of generating the docu-ment by the class label.
We then infer a set oftopics on the sprinkled training documents.
AsLDA uses higher order word associations (Lee etal., 2010) while discovering topics, we hypothe-size that sprinkling will improve text classificationperformance of ClassifyLDA.
We experimentallyverify this hypothesis on three real world datasets.2 Related WorkSeveral researchers have proposed semi-supervised text classification algorithms withthe aim of reducing the time, effort and costinvolved in labeling documents.
These algorithmscan be broadly categorized into three categoriesdepending on how supervision is provided.
In thefirst category, a small set of labeled documentsand a large set of unlabeled documents is usedwhile learning a classifier.
Semi-supervised textclassification algorithms proposed in (Nigam etal., 2000), (Joachims, 1999), (Zhu and Ghahra-mani, 2002) and (Blum and Mitchell, 1998) are afew examples of this type.
However, these algo-rithms are sensitive to initial labeled documentsand hyper-parameters of the algorithm.In the second category, supervision comes in theform of labeled words (features).
(Liu et al, 2004)and (Druck et al, 2008) are a few examples of thistype.
An important limitation of these algorithmsis coming up with a small set of words that shouldbe presented to the annotators for labeling.
Alsoa human annotator may discard or mislabel a pol-ysemous word, which may affect the performanceof a text classifier.The third type of semi-supervised text classifi-cation algorithms is based on active learning.
Inactive learning, particular unlabeled documents orfeatures are selected and queried to an oracle (e.g.human annotator).
(Godbole et al, 2004), (Ragha-van et al, 2006), (Druck et al, 2009) are a few ex-amples of active learning based text classificationalgorithms.
However, these algorithms are sensi-tive to the sampling strategy used to query docu-ments or features.In our approach, an annotator does not labeldocuments or words, rather she labels a small setof interpretable topics which are inferred in an un-supervised manner.
These topics are very few,when compared to the number of documents.
Asthe most probable words of topics are representa-tive of the dataset, there is no need for the annota-tor to search for the right set of features for eachclass.
As LDA topics are semantically more mean-ingful than individual words and can be acquiredeasily, our approach overcomes limitations of thesemi-supervised methods discussed above.3 Background3.1 LDALDA is an unsupervised probabilistic generativemodel for collections of discrete data such as textdocuments.
The generative process of LDA can bedescribed as follows:1. for each topic t, draw a distribution overwords: ?t?
Dirichlet(?w)2. for each document d ?
Da.
Draw a vector of topic proportions:?d?
Dirichlet(?t)b. for each word w at position n in di.
Draw a topic assignment:zd,n?
Multinomial(?d)ii.
Draw a word:wd,n?
Multinomial(zd,n)Where, T is the number of topics, ?tis the wordprobabilities for topic t, ?dis the topic probabil-ity distribution, zd,nis topic assignment and wd,nis word assignment for nth word position in docu-ment d respectively.
?tand ?ware topic and wordDirichlet priors.The key problem in LDA is posterior inference.The posterior inference involves the inference ofthe hidden topic structure given the observed doc-uments.
However, computing the exact posteriorinference is intractable.
In this paper we estimateapproximate posterior inference using collapsedGibbs sampling (Griffiths and Steyvers, 2004).The Gibbs sampling equation used to update theassignment of a topic t to the word w ?
W at theposition n in document d, conditioned on ?t, ?wis:P (zd,n= t|zd,?n, wd,n= w,?t, ?w) ?
?w,t+ ?w?
1?v?W?v,t+ ?v?
1?
(?t,d+ ?t?
1) (1)where ?w,cis the count of the word w assignedto the topic c, ?c,dis the count of the topic cassigned to words in the document d and W isthe vocabulary of the corpus.
We use a subscriptd,?n to denote the current token, zd,nis ignoredin the Gibbs sampling update.
After performingcollapsed Gibbs sampling using equation 1, weuse word topic assignments to compute a point56estimate of the distribution over words ?w,canda point estimate of the posterior distribution overtopics for each document d (?d) is:?w,t=?w,t+ ?w[?v?W?v,t+ ?v](2)?t,d=?t,d+ ?t[T?i=1?i,d+ ?i](3)Let MD=< Z,?,?
> be the hidden topicstructure, where Z is per word per document topicassignment, ?
= {?t} and ?
= {?d}.3.2 Sprinkling(Chakraborti et al, 2007) propose a simple ap-proach called ?sprinkling?
to incorporate class la-bels of documents into LSI.
In sprinkling, a set ofartificial words are appended to a training docu-ment which are specific to the class label of thedocument.
Consider a case of binary classificationwith classes c1and c2.
If a document d belongsto the class c1then a set of artificial words whichrepresent the class c1are appended into the doc-ument d, otherwise a set of artificial words whichrepresent the class c2are appended.Singular Value Decomposition (SVD) is thenperformed on the sprinkled training documentsand a lower rank approximation is constructedby ignoring dimensions corresponding to lowersingular values.
Then, the sprinkled terms areremoved from the lower rank approximation.
(Chakraborti et al, 2007) empirically show thatsprinkled words boost higher order word associ-ations and projects documents with same class la-bels close to each other in latent semantic space.4 Topic Sprinkling in LDAIn our text classification algorithm, we first infer aset of topics on the given unlabeled document cor-pus.
We then ask a human annotator to assign oneor more class labels to the topics based on theirmost probable words.
We use these labeled topicsto create a new LDA model as follows.
If the topicassigned to the word w at the position n in docu-ment d is t, then we replace it by the class labelassigned to the topic t. If more than one class la-bels are assigned to the topic t, then we randomlyselect one of the class labels assigned to the topict.
If the annotator is unable to label a topic thenwe randomly select a class label from the set of allclass labels.
We then update the new LDA modelusing collapsed Gibbs sampling.We use this new model to infer the probabilitydistribution of each unlabeled training documentover the class labels.
Let, ?c,dbe the probability ofgenerating document d by class c. We then sprin-kle s artificial words of class label c to documentd, such that s = K ?
?c,dfor some constant K.We then infer a set of |C| number of topics onthe sprinkled dataset using collapsed Gibbs sam-pling, where C is the set of class labels of thetraining documents.
We modify collapsed Gibbssampling update in Equation 1 to carry class labelinformation while inferring topics.
If a word in adocument is a sprinkled word then while samplinga class label for it, we sample the class label asso-ciated with the sprinkled word, otherwise we sam-ple a class label for the word using Gibbs updatein Equation 1.We name this model as Topic Sprinkled LDA(TS-LDA).
While classifying a test document, itsprobability distribution over class labels is inferredusing TS-LDA model and it is classified to its mostprobable class label.
Algorithm for TS-LDA issummarized in Table 1.5 Experimental EvaluationWe determine the effectiveness of our algorithmin relation to ClassifyLDA algorithm proposed in(Hingmire et al, 2013).
We evaluate and com-pare our text classification algorithm by comput-ing Macro averaged F1.
As the inference of LDAis approximate, we repeat all the experiments foreach dataset ten times and report average Macro-F1.
Similar to (Blei et al, 2003) we also learnsupervised SVM classifier (LDA-SVM) for eachdataset using topics as features and report averageMacro-F1.5.1 DatasetsWe use the following datasets in our experiments.1.
20 Newsgroups: This dataset containsmessages across twenty newsgroups.
In ourexperiments, we use bydate version of the20Newsgroup dataset1.
This version of the datasetis divided into training (60%) and test (40%)datasets.
We construct classifiers on trainingdatasets and evaluate them on test datasets.2.
SRAA: Simulated/Real/Aviation/AutoUseNet data2: This dataset contains 73,2181http://qwone.com/?jason/20Newsgroups/2http://people.cs.umass.edu/?mccallum/data.html57?
Input: unlabeled document corpus-D, number oftopics-T and number of sprinkled terms-K1.
Infer T number of topics on D for LDA using col-lapsed Gibbs sampling.
Let MDbe the hiddentopic structure of this model.2.
Ask an annotator to assign one or more class labelsci?
C to a topic based on its 30 most probablewords.3.
Initialization: For nth word in document d ?
Dif zd,n= t and the annotator has labeled topic twith cithen, zd,n= ci4.
Update MDusing collapsed Gibbs sampling up-date in Equation 1.5.
Sprinkling: For each document d ?
D:(a) Infer a probability distribution ?dover classlabels using MDusing Equation 3.
(b) Let, ?c,dbe probability of generating docu-ment d by class c.(c) InsertK ?
?c,ddistinct words associated withthe class c to the document d.6.
Infer |C| number of topics on the sprinkled docu-ment corpus D using collapsed Gibbs sampling up-date.7.
Let M?Dbe the new hidden topic structure.
Let uscall this hidden structure as TS-LDA.8.
Classification of an unlabled document d(a) Infer ?
?dfor document d using M?D.
(b) k = argmaxi?
?i,d(c) yd= ckTable 1: Algorithm for sprinkling LDA topics fortext classificationUseNet articles from four discussion groups,for simulated auto racing (sim auto), simulatedaviation (sim aviation), real autos (real auto), realaviation (real aviation).
Following are the threeclassification tasks associated with this dataset.1.
sim auto vs sim aviation vs real auto vsreal aviation2.
auto (sim auto + real auto) vs aviation(sim aviation + real aviation)3. simulated (sim auto + sim aviation) vs real(real auto + real aviation)We randomly split SRAA dataset such that 80%is used as training data and remaining is used astest data.3.
WebKB: The WebKB dataset3contains 8145web pages gathered from university computer3http://www.cs.cmu.edu/?webkb/science departments.
The task is to classify thewebpages as student, course, faculty or project.We randomly split this dataset such that 80% isused as training and 20% is used as test data.We preprocess these datasets by removingHTML tags and stop-words.For various subsets of the 20Newsgroups andWebKB datasets discussed above, we choosenumber of topics as twice the number of classes.For SRAA dataset we infer 8 topics on the train-ing dataset and label these 8 topics for all the threeclassification tasks.
While labeling a topic, weshow its 30 most probable words to the human an-notator.Similar to (Griffiths and Steyvers, 2004), we setsymmetric Dirichlet word prior (?w) for each topicto 0.01 and symmetric Dirichlet topic prior (?t)for each document to 50/T, where T is number oftopics.
We set K i.e.
maximum number of wordssprinkled per class to 10.5.2 ResultsTable 2 shows experimental results.
We can ob-serve that, TS-LDA performs better than Classi-fyLDA in 5 of the total 9 subsets.
For the comp-religion-sci dataset TS-LDA and ClassifyLDAhave the same performance.
However, Classi-fyLDA performs better than TS-LDA for the threeclassification tasks of SRAA dataset.
We can alsoobserve that, performance of TS-LDA is close tosupervised LDA-SVM.
We should note here thatin TS-LDA, the annotator only labels a few topicsand not a single document.
Hence, our approachexerts a low cognitive load on the annotator, atthe same time achieves text classification perfor-mance close to LDA-SVM which needs labeleddocuments.5.3 ExampleTable 3 shows most prominent words of fourtopics inferred on the med-space subset of the20Newsgroup dataset.
We can observe here thatmost prominent words of the first topic do not rep-resent a single class, while other topics representeither med (medical) or space class.
We can sayhere that, these topics are not ?coherent?.We use these labeled topics and create a TS-LDA model using the algorithm described in Table1.
Table 4 shows words corresponding to the toptwo topics of the TS-LDA model.
We can observehere that these two topics are more coherent thanthe topics in Table 3.58Text Classification (Macro-F1)Dataset # Topics ClassifyLDA TS-LDA LDA-SVM20Newsgroupsmed-space 4 0.892 0.938 0.933politics-religion 4 0.836 0.897 0.901politics-sci 4 0.887 0.901 0.910comp-religion-sci 6 0.853 0.853 0.872politics-rec-religion-sci 8 0.842 0.858 0.862SRAAreal auto-real aviation-sim auto-sim aviation8 0.766 0.741 0.820auto-aviation 8 0.926 0.910 0.934real-sim 8 0.918 0.902 0.923WebKBWebKB 8 0.627 0.672 0.730Table 2: Experimental results of text classification on various datasets.ID Most prominent words in thetopicClass (med/ space)0 science scientific idea large theorybit pat thought problem isnmed +space1 information health research medi-cal water cancer hiv aids childreninstitute newslettermed2 msg food doctor disease painday treatment blood steve dyermedicine symptomsmed3 space nasa launch earth orbitmoon shuttle data lunar satellitespaceTable 3: Topic labeling on the med-space subset of the20Newsgroup datasetID Most prominent words in thetopicClass (med/ space)0 msg medical health food diseaseyears problem information doctorpain cancermed1 space launch earth data orbitmoon program shuttle lunar satel-litespaceTable 4: Topics inferred on the med-space subset of the20Newsgroup dataset after sprinkling labeled topics from Ta-ble 3.Hence, we can say here that, in addition to textclassification, sprinkling improves coherence oftopics.We should note here that, in ClassifyLDA, theannotator is able to assign a single class label toa topic.
If the annotator assigns a wrong class la-bel to a topic representing multiple classes (e.g.first topic in Table 3), then it may affect the perfor-mance of the resulting classifier.
However, in ourapproach the annotator can assign multiple classlabels to a topic, hence our approach is more flexi-ble for the annotator to encode her domain knowl-edge efficiently.6 Conclusions and Future WorkIn this paper we propose a novel algorithm thatclassifies documents based on class labels overfew topics.
This reduces the need to label a largecollection of documents.
We have used the ideaof sprinkling originally proposed in the contextof supervised Latent Semantic Analysis, but thesetting here is quite different.
Unlike the workin (Chakraborti et al, 2007), we do not assumethat we have class labels over the set of trainingdocuments.
Instead, to realize our goal of reduc-ing knowledge acquisition overhead, we propose away of propagating knowledge of few topic labelsto the words and inducing a new topic distribu-tion that has its topics more closely aligned to theclass labels.
The results show that the approachcan yield performance comparable to entirely su-pervised settings.
In future work, we also envi-sion the possibility of sprinkling knowledge frombackground knowledge sources like Wikipedia(Gabrilovich and Markovitch, 2007) to realize analignment of topics to Wikipedia concepts.
Wewould like to study effect of change in number oftopics on the text classification performance.
Wewill also explore techniques which will help an-notators to encode their domain knowledge effi-ciently when the topics are not well aligned to theclass labels.ReferencesDavid M. Blei and Jon D. McAuliffe.
2007.
Super-vised Topic Models.
In NIPS.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet Allocation.
The Journal ofMachine Learning Research, 3:993?1022, March.59Avrim Blum and Tom Mitchell.
1998.
Combining La-beled and Unlabeled Data with Co-Training.
In Pro-ceedings of the eleventh annual conference on Com-putational learning theory, pages 92?100.Sutanu Chakraborti, Rahman Mukras, Robert Lothian,Nirmalie Wiratunga, Stuart N. K. Watt, David J.Harper.
2007.
Supervised Latent Semantic IndexingUsing Adaptive Sprinkling.
In IJCAI, pages 1582-1587.Scott Deerwester, Susan T. Dumais, George W. Fur-nas, Thomas K. Landauer, and Richard Harshman.1990.
Indexing by Latent Semantic Analysis.
JA-SIS, 41(6):391?407.Gregory Druck, Gideon Mann, and Andrew McCal-lum.
2008.
Learning from Labeled Features usingGeneralized Expectation criteria.
In SIGIR, pages595?602.Gregory Druck, Burr Settles, and Andrew McCallum.2009.
Active Learning by Labeling Features.
InEMNLP, pages 81?90.Shantanu Godbole, Abhay Harpale, Sunita Sarawagi,and Soumen Chakrabarti.
2004.
Document Classifi-cation through Interactive Supervision of Documentand Term Labels.
In PKDD, pages 185?196.Thomas L. Griffiths and Mark Steyvers.
2004.
FindingScientific Topics.
PNAS, 101(suppl.
1):5228?5235,April.Swapnil Hingmire, Sandeep Chougule, Girish K. Pal-shikar, and Sutanu Chakraborti.
2013.
DocumentClassification by Topic Labeling.
In SIGIR, pages877?880.Thorsten Joachims.
1999.
Transductive Inference forText Classification using Support Vector Machines.In ICML, pages 200?209.April Kontostathis and William M. Pottenger.
2006.
AFramework for Understanding Latent Semantic In-dexing (LSI) Performance.
Inf.
Process.
Manage.,42(1):56?73, January.Simon Lacoste-Julien, Fei Sha, and Michael I. Jordan.2008.
DiscLDA: Discriminative Learning for Di-mensionality Reduction and Classification.
In NIPS.Sangno Lee, Jeff Baker, Jaeki Song, and James C.Wetherbe.
2010.
An Empirical Comparison ofFour Text Mining Methods.
In Proceedings of the2010 43rd Hawaii International Conference on Sys-tem Sciences, pages 1?10.Bing Liu, Xiaoli Li, Wee Sun Lee, and Philip S. Yu.2004.
Text Classification by Labeling Words.
InProceedings of the 19th national conference on Ar-tifical intelligence, pages 425?430.Kamal Nigam, Andrew Kachites McCallum, SebastianThrun, and Tom Mitchell.
2000.
Text Classificationfrom Labeled and Unlabeled Documents using EM.Machine Learning - Special issue on information re-trieval, 39(2-3), May-June.Hema Raghavan, Omid Madani, and Rosie Jones.2006.
Active Learning with Feedback on Featuresand Instances.
JMLR, 7:1655?1686, December.Xiaojin Zhu and Zoubin Ghahramani.
2002.
Learningfrom Labeled and Unlabeled Data with Label Prop-agation.
Technical report, Carnegie Mellon Univer-sity.Jun Zhu, Amr Ahmed, and Eric P. Xing.
2009.MedLDA: Maximum Margin Supervised TopicModels for Regression and Classification.
In ICML,pages 1257?1264.Evgeniy Gabrilovich and Shaul Markovitch.
2007.Computing Semantic Relatedness Using Wikipedia-based Explicit Semantic Analysis.
In IJCAI, pages1606?1611.60
