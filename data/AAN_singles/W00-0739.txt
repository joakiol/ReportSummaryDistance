In: Proceedings of CoNLL-2000 and LLL-2000, pages 176-183, Lisbon, Portugal, 2000.Learning from a Substructural PerspectiveP ie ter  Adr iaans  and Er ik  de  HaasSyllogic,P.O.
Box 2729, 3800GG Amersfoort,  The Netherlands,andUniversity of Amsterdam, Fac.
of Mathematics,  Computer  Science, Physics and Astronomy,Plantage Muidergracht 24, 1018TV Amsterdam, The Netherlandspieter, adriaans@ps.net, erik@propersolution.nlAbst rac tIn this paper we study learning from a logicalperspective.
We show that there is a strong re-lationship between a learning strategy, its for-mal learning framework and its logical represen-tational theory.
This relationship enables oneto translate learnability results from one theoryto another.
Moreover if we go from a classi-cal logic theory to a substructural logic theory,we can transform learnability results of logicalconcepts to results for string languages.
In thispaper we will demonstrate such a translation bytransforming the Valiant learnability result forboolean concepts to a learnability :result for aclass of string pattern languages.1 In t roduct ionThere is a strong relation between a learn-ing strategy, its formal learning framework andits representational theory.
Such a representa-tional theory typically is (equivalent to) a logic.As an example for this strong relationship as-sume that the implication A ~ B is a givenfact, and you observe A; then you can deduceB, which means that you can learn B from Abased on the underlying representational the-ory.
The learning strategy is very tightly con-nected to its underlying logic.
Continuing theabove example, suppose you observe -~B.
In arepresentational theory based on classical ogicyou may deduce ~A given the fact A ~ B.In intuitionistic logic however, this deductionis not valid.
This example shows that the char-acter of the representational theory is essentialfor your learning strategy, in terms of what canbe learned from the facts and examples.In the science of the representational theo-ries, i.e.
logic, it is a common approach toconnect different representational theories, andtransform results of one representational theoryto results in an other representational theory.Interesting is now whether we can transformlearnability results of learning strategies withinone representational theory to others.
Observethat to get from a first order calculus to a stringcalculus one needs to eliminate structural rulesfrom the calculus.
Imagine now that we do thesame transformation to the learning strategies,we would come up with a learning strategy forthe substructural string calculus tarting from alearning strategy for the full first order calculus.The observation that learning categorialgrammars translates to the task of learningderivations in a substructural logic theory moti-vates a research program that investigates learn-ing strategies from a logical point of view (Adri-aans and de Haas, 1999).
Many domains forlearning tasks can be embedded in a formallearning framework based on a logical repre-sentational theory.
In Adriaans and de Haas(1999) we presented two examples of substruc-tural logics, that were suitable representationaltheories for different learning tasks; The firstexample was the Lambek calculus for learningcategorial grammars, the second example dealtwith a substructural logic that was designed tostudy modern Object Oriented modeling lan-guages like UML (OMG, 1997), (Fowler, 1997).In the first case the representation theory is firstorder logic without structural rules, the formallearning theory from a logical point of view isinductive substructural logic programming andan example of a learning strategy in this frame-work is EMILE, a learning algorithm that learnscategorial grammars (Adriaans, 1992).In this paper we concentrate on the trans-formation of classical logic to substructurallogic and show that Valiant's proof of PAC-176learnability of boolean concepts can be trans-formed to a PAC learnability proof for learninga class of finite languages.
We discuss the ex-tension of this learnability approach to the fullrange of substructural logics.
Our strategy inexploring the concept of learning is to look atthe logical structure of a learning algorithm, andby this reveal the inner working of the learningstrategy.In Valiant (1984) the principle of ProbablyApproximately Correct learning (PAC learning)was introduced.
There it has been shown thatk-CNF (k-length Conjunctive Normal Form)boolean concepts can be learned efficiently inthe model of PAC learning.
For the proofthat shows that these boolean concepts can belearned efficiently Valiant presents a learning al-gorithm and shows by probabilistic argumentsthat boolean concept can be PAC learned inpolynomial time.
In this paper we investigatethe logical mechanism behind the learning al-gorithm.
By revealing the logical mechanismbehind this learning algorithm we are able tostudy PAC learnability of various other logics inthe substructural landscape of first order propo-sitional ogic.In this paper we will first briefly introducesubstructural logic in section 2.
Consequentlywe will reconstruct in section 3 Valiant's resulton learnability of boolean concepts in terms oflogic.
Then in section 4 we will show that thelearnability result of Valiant for k-CNF booleanconcepts can be transformed to a learnability re-sult for a grammar of string patterns denoted bya substructural variant of the k-CNF formulas.We will conclude this paper with a discussionan indicate how this result could be extendedto learnability results for categorial grammars.2 Subst ructura l  log icIn Gentzen style sequential formalisms a sub-structural logic shows itself by the absence of(some of) the so-called structural rules.
Exam-ples of such logics are relevance logic (Dunn,1986), linear logic (Girard, 1987) and BCK logic(Grishin, 1974).
Notable is the substructuralbehavior of categorial logic, which in its proto-type form is the Lambek calculus.
Categoriallogics are motivated by its use as grammar fornatural languages.
The absence of the struc-tural rules degrades the abstraction of sets inthe semantic domain to strings, where elementsin a string have position and arity, while theydo not have that in a set.
As we will see furtheron in this paper the elimination of the struc-tural rules in the learning context of the booleanconcepts will transform the learning frameworkfrom sets of valuated variables to strings of val-uated variables.Example  2.1 In a domain of sets the following'expressions' are equivalent, while they are notin the domain of strings:a, a, b, a ~ a, b, bIn a calculus with all the structural rules the fea-tures 'position' and 'arity' are irrelevant in thesemantic domain, because aggregates that differin these features can be proved equivalent withthe structural rules.
To see this observe thatthe left side of the above equation can be trans-formed to the right side by performing the fol-lowing operation:a, a, b, aa, b, aa, a, ba, ba, b, bcontract a, a in .first two positionsto aexchange b, a in last to positions toa,bcontract again a, a in first twopositions to aweaken expression b in last positionto b, bIn figure 2 we list the axiomatics of the firstorder propositional sequent calculus 1, with theaxioms , the cut rule, rules for the connectivesand the structural rules for exchange, weakeningand contraction.3 PAC Boolean concept learningrev is i tedIn this section we describe the principle of Prob-ably Approximately Correct Learning (PAClearning) of Boolean concepts.
We will reveal1Note that  in the variant we use here we have a specialcase of the RA rule.177representat iona ltheoryFirst orderpropositional ) ~logic j Iformal  learningf rameworklearn ing st rategyBoolean \ ~ PAC learning ,-4 concepts ~ k-CNF )1Substructuralproposition= ) 4111 1String .
PAC learning ,,~  languagesFigure 1: Relation between learning strategy, learning framework and representational theory(Ax) A ~ A (Cut)(LA) F ,A ,B~A (RA)F, AAB~A(LV) F ,A~A F ,B~AF,A V B ~ A (RV)F =~ A,A F~,A,~ AF', F ~ A', AF~A,A  F t~B,AF,F t =~ AAB,  AF ~ A,A F ~ B ,AF~AVB,  A F~AVB,  A(Ex) F'AAB'F~=-~ AF,B A A,F ~ ~ AF~A(Weak) F, A ~ A(Contr) F, A, A ~ AF ,A~AFigure 2: First order propositional sequent calculusthe logical deduction process behind the learn-ing algorithm.Consider the sample space for boolean con-cepts.
An example is a vector denoting thetruth (presence,l) or falsehood (absence,0) ofpropositional variables.
Such an example vec-tor can be described by a formula consisting ofthe conjunction of all propositional variables ornegations of propositional variables, dependingon the fact whether there is a 1 or a 0 in theposition of the propositional variable name inthe vector.
A collection of vectors, i.e.
a con-cept, in its turn can be denoted by a formulatoo, being the disjunction of all the formula's ofthe vectors.Example  3.1 Let universe U = {a,b} and letconcept f = {(0, 1)}, then the following formulaexactly describes f :~Ab178A little more extensive: Let uni-verse \[.j, = {a,b,c} and let conceptf '  = {(0, 0, 0), (0, 0, 1), (0, 1, 1), (1, 1, 1)}Then the following formula exactly describes f l(with a clear translation):(~AbAa) V (~ AbA c) V (~A bA c) V (aAbAc)Note that these formulas are in Disjunctive nor-mal form (DNF).An interesting observation ow is that thelearning algorithm of Valiant that learns k-CNFformulas actually is trying to prove the equiv-alence between a DNF formula and a k-CNFformula.Example  3.2 Let universe U = {a,b} and letconcept f = {(0, 1)}, then the following sequentshould be 'learned' by a 2-CNF learning algo-rithm 2:~ A b ,?:,.
(aVb) A (~Vb) A (~Vb)A little more extensive: Let U' ={a, b, c} and let concept f '  ={(0, 0, 0), (0, 0, 1), (0, 1, 1), (1, 1, 1)} Thenthe following sequent should be 'learned' by a2-CNF learning algorithm:(~ Ab A ~) V (HAhA c) V (~A bA c) V (aAbAc)(~V b) A (~V b) A (a V b)The above observation says in logical termsthat the learning algorithm needs to implementan inductive procedure to find this desired proofand the concluding concept description (2-CNFformula) from examples.
In the search space forthis proof the learning algorithm can use the ax-ioms and rules from the representational theory.In the framework of boolean concept learningthis means that the learning algorithm may useall the rules and axioms from the representa-tional theory of classical propositional logic.Example  3.3 Let IJ = {a, b} and let conceptf = {(0, 1)} and assume f can be representedby a 2-CNF formula, to learn the 2-CNF de-scription of concept f the learning algorithmneeds to find the proof for a sequent starting2i.e.
an algorithm that can learn 2-CNF boolean con-cepts.from the DNF formula ~ A b to a 2-CNF for-mula and vice versa (?~.)
and to do so it mayuse all the rules and axioms from the first or-der propositional calculus including the struc-tural rules.
The proof for one side of such asequent is spelled out in figure 3.In general an inductive logic programming al-gorithm for the underlying representational the-ory can do the job of learning the concept; i.e.from the examples (DNF formulas) one can in-duce possible sequents, targeting on a 2-CNFsequent on the righthand side.
The learning al-gorithm we present here is more specific andsimply shows that an efficient algorithm for theproof search exists.The steps:1.
Form the collection G of all 2-CNFclauses (p V q)2. do l times(a)(b)pick an example al A.-.
Aamform the collection of all2-CNF clauses deducible fromal A ... A am and intersect thiscollection with G resulting ina new CCorrectness proof  (outl ine): By (Ax),(RV), (Weak), (LA) and (Ex) we can proofthat for any conjunction (i.e.
example vector)a l  A .
.
.
A am we have for all 1 _< i < m andany b a clause of a 2-CNF in which ai occurswith b, hence having all clauses deducible fromthe vector proven individually enabling one toform the collection of all clauses deducible froma vector; i.e.al A ...  Aam ~ ai Vbal A ...  A am :::*" b V aiBy (RA) and (Contr) we can proof the conjunc-tion of an arbitrary subset of all the clauses de-ducible from the vector, in particular all thoseclauses that happen to be common to all thevectors for each individual vector we have seenso far, hence proving the 2-CNF for every indi-vidual vector; i.e.al A .. ?
A am ~ clause1 A .. ?
A clausep179b ~ b (Ax) b =*- b (Ax) (av) (av)b ~ E V b (Weak) b =~ a V b (Weak)gg=t-E(Ax) (Rv) b ,g~ggVb (L^) b ,g~aVb (LA)EE=~,,.EVb:_ (Weak) bAE=*,ggVb bAg=~aVb (Sx) E ,b~EVb_  (L^) ggAb~EVb (Ex) EAb=~aVb (RA)EAb~EVb (E A b), (E A b) =-~ (E V b) A (a V b) (a^)(E A b), (E A b), (E A b) ~ (E V b) A (E V b) A (a V b)(Contr)(EAb) , (EAb)~(EVb)A(EVb)A(aVb)(~A b) =* (~Vb) A (EV b) A (a V b)(Contr)Figure 3: Proof to be found for boolean concept learningNow by (LV)  we can prove the complete DNFto 2-CNF sequent; i.e.vector1 V ?
?
?
V vector/ ~ clause1 A ?
?
?
A clausepIt is easy to see that for the above algorithmthe same complexity analysis holds as for theValiant algorithm, because we have the sameprogression in l steps, an the individual stepshave constant overhead.4 PAC learn ing  subst ructura l  log icWhen we transform the representational theoryof the boolean concept learning framework to asubstructural logic, we do the following:?
eliminate the structural rules from the cal-culus of first order propositional logicWhen we want to translate the learnability re-sult of k-CNF expressible boolean concepts weneed to do the same with the formal learningframework and the strategy (algorithm).
Inother words:?
the learning framework will contain con-cepts that are sensitive to the featureswhich were before abstracted by the struc-tural rules ('position' and 'arity' )?
the learning algorithm from above is nolonger allowed to use the structural rulesin its inductive steps.Below we present a learning algorithm forthe substructural logic representational theory.Suppose again the universe U = {al , .
.
.
,an},and the concept f is a CNF expressible conceptfor vectors of length m.1.
start with m empty clauses (i.e.
disjunctionof zero literals) clause1,.
.
.
,  clausem2.
do l times(a) pick an example al A .
.
.
A am(b) for all 1 < i < m add ai to clause/ ifai does not occur in clause/.Cor rectness  proo f  (out l ine) :  By (Ax) and(RV) we can proof for any ai that the sequentai =-~ clause/for any clause/containing ai as oneof its disjuncts, especially for a clause/contain-ing next to ai all the a~ from the former exam-ples.
Then by (RA) and (LA) we can positionall the vectors and clauses in the right-hand po-sition; i.e.al A .
.
.
A am ~ clause1 A -.. A clausemHence justifying the adding of the literal ai ofa vector in clausei.
Now (LV)  completes thesequent for all the example vectors; i.e.
(al A .
.
.
A am) V (a i A .
.
.
A aim ) V .
.
.clause1 A .-.
A clausemFor the algorithmic complexity in terms ofPAC learning, suppose we want present exam-ples of concept f and that the algorithm learnedconcept ff in l steps.
Concept ff then de-scribes a subset of concept f because on everyposition in the CNF formula contains a sub-set of the allowed variables; i.e.
those vari-ables that have encountered in the examples 3.anote that the CNF formula's can only describe par-ticular sets of n-strings; namely those sets that are com-plete for varying symbols locally on the different posi-tions in the string.180~ ~ (Ax)~vbb ~ b (Ax) b ~ b (Ax) (RV) (RV)b~Vb b~aVb (at)b, b =* (~V) A (a V b) (RV) (LA)bAb~ (gVD) A(aVb)  (at)~,b Ab ~ (~V b) A (~V b) A (a V b)gAbAb ~ (~Vb) A (~Vb) A (aVb)(LA)(EAEA a) V (gAEA b) V (gA bAa) V (EA bA b) V (bAEA a)V(bAEA b) V (bA bA a) V (bA bA b) ~ (gVb) A (gV b) A (a V b)(LV)Figure 4: Proof to be found for string pattern learningNow let e = P( fA f  ~) be the error then again5 = (1 - e) TM is the confidence parameter as wehave m positions in the string.
By the sameargument as for the Valiant algorithm we mayconclude that e and 5 decrease xponentially inthe number of examples l, meaning that we havean efficient polynomial t ime learning algorithmfor arbitrary e and 5.5 D iscuss ionWe showed that the learnability result ofValiant for learning boolean concepts can betransformed to a learnability result for pat-tern languages by looking at the transforma-tion of the underlying representational theories;i.e.
looking at the transformation from clas-sical first order propositional logic (underlyingthe boolean concepts) to substructural first or-der propositional logic (underlying the patternlanguages).
An interesting extension would beto look at the substructural concept languagethat includes implication (instead of the CNFformula's only).
A language that allows impli-cation coincides with the full Lambek calculus,and a learning algorithm and learnability resultfor this framework amounts to results for all lan-guages that can be described by context freegrammars.
This is subject to future research.References  "P. Adriaans and E. de Haas.
1999.
Grammar in-duction as substructural inductive logic program-ming.
In Proceedings ofthe workshop on LearningLanguage in Logic (LLL99), pages 117-126, Bled,Slovenia, jun.P.
Adriaans.
1992.
Language Learning from a Cate-gorial Perspective.
Ph.D. thesis, Universiteit vanAmsterdam.
Academisch proefschrift.J.
Dunn.
1986.
Relevance logic and entailment.
InF.
Guenthner D. Gabbay, editor, Handbook ofPhilosophical Logic III, pages 117-224.
D. ReidelPublishing Company.M.
Fowler.
1997.
UML Distilled: Applying the Stan-dard Object Modeling Language.
Addison WesleyLongman.J.-Y.
Girard.
1987.
Linear logic.
Theoretical Com-purer Science, 50:1-102.V.N.
Grishin.
1974.
A non-standard logic, and itsapplications to set theory.
In Studies in formal-ized languages and nonclassical logics, pages 135-171.
Nanka.Object Management Group OMG.
1997.
Uml 1.1specification.
OMG documents ad970802-ad0809.L.G.
Valiant.
1984.
Theory of the learnable.
Comm.o/the ACM, 27:1134-1142.181Addendum:  PAC l earn ingThe model of PAC learning arises from the workof Valiant (Valiant, 1984).
In this model oflearning it is assumed that we have a samplespace U* of vectors over an alphabet U, whereeach position in a vector denotes the presence(1) or absence (0) of a symbol a ~_-- U in thesample vector.
A concept f is a subset of vec-tors from the sample space U*.Example 5.1 Let  U = {a ,b}  be an alphabet,then the following table describes the samplespace U* over U:a b0 00 11 01 1an example of a concept is f := {(0, 1)} and another example is g := {(0, 0), (0, 1), (1, 1)}.A concept can be learned by an algorithm bygiving this algorithm positive and/or  negativeexamples of the target concept to be learned.An algorithm efficiently learns a concept if thisalgorithm produces a description of this con-cept in polynomial time.
Informally eL concept isPAC (Probably Approximately Correct) learnedif the algorithm produces a description of a con-cept that is by approximation the same as thetarget concept from which examples are feededinto the algorithm.
A collection of concepts con-stitutes to a concept class.
A concept class canbe (PAC) learned if all the concepts in the con-cept class can be (PAC) learned.Def in i t ion  5.2 (PAC Learnable) Let F be aconcept class, 5 (0 < 5 < 1) a confidence param-eter, c (0 < e < 1) an error parameter.
A con-cept class F is PAC learnable if for all targetsf E F and all probability distributions P on thesample space U* the learning algorithm A out-puts a concept g E F such that with probability(1-5)  it holds that we have a chance on an errorwith P ( f  Ag) _< e (where fag  = (f -g )U(g - f ) )We are especially interested in concept classesthat are defined by some formalism (language).In other words a language can describe comecollection of concepts.
An example of sucha language is the language of boolean formu-las.
A boolean formula describes a conceptthat consists of all the vectors over the alpha-bet of propositional variable names that satisfythe formula.
These concepts are called booleanconcepts.Example  5.3 Let U := {a, b} be an alphabet ofpropositional variable names.
Then the formulaA b describes the concept f := {(0, 1)} of thesample space U*; and the formula ~V b describesthe concept g := {(0, 0), (0, 1), (1, 1)}.In Valiant (1984) Valiant proves that the lan-guage of k-CNF boolean formula's can be ef-ficiently PAC learned.
This means that for anarbitrary k the concept class defined by the lan-guage of k-CNF formula's can be PAC learnedby an algorithm in a polynomial number ofsteps.
Below we briefly recapitulate this result.Def in i t ion  5.4 (Boolean concept languages)Let U be a set of propositional variable names,then the language L of boolean formulas is de-fined by:L := UIL V LIL A LILA literal is a propositional variable or a negationof a propositional variable; i.e.LIT := UIUA conjunction of a collection of formulas C isa finite sequence of formulas from C connectedby the binary connective A; i.e.CON(C) := CICON(C) A CA disjunction of a collection of formulas C is afinite sequence of formulas from C connected bythe binary connective V; i.e.DIS(C) := CIDIS(C) V CA formula is a CNF.formula (Conjunctive Nor-mal Form) if the formula is a conjunction ofdisjunctions of literals.
A formula is a k-CNFformula if all the disjuctions in the formula areof length k. A formula is a DNF formula (Dis-junctive Normal Form) if the formula is a dis-junction of conjunctions of literals.Theorem 5.5 (Valiant (198~)) The classes ofk-CNF boolean concept languages are PAClearnable in polynomial time.182v.v2 ~al ansam pie space(set of all vectors)fa rFigure 5: Valiant's proofP roo f  (out l ine) :  Let U := {a l , .
.
.
,an}(n  ?Af) be a alphabet and let concept f be a setof vectors V := {vl , .
.
.
,Vm}(m _< n) over U*,which is equivalent to the k-CNF formula A.Let P be an arbitrary probability distributionover concept f such that Ev~e/P(vi) = 1; i.e.P( f )  -- 1.
Examples picked using the distribu-tion based on P will be feeded into the followinglearning algorithm:?
Form the col lect ion G := {ci,... ,Cnk }of all the clauses (disjunctions ofl iterals) of length k.?
do l t imes- v := pick-an-example- for each ci in G?
delete ci from G if v 7-z ciNow suppose that the algorithm learned con-cept f '  from l examples (l taken from the algo-rithm).
The concept f '  now is a concept hatis a subset of f ,  because it may not have seenenough examples to eliminate all the clausesthat are in conflict with f ;  i.e.
there are stillclauses in ff' restricting this concept in the con-junction of clauses, while it is disqualified by avector in f .
What is the size of the number ofexamples I we need to let f '  approximate f withfor boolean concept learninga confidence 5 and error e. We have thatP( f )  = 1= P ( fA f ' )(the error is the chance of rejecting anexample in f because it is not in f ' )= (1 - , )m(confidence is the chance of not making anerror after learning from I examples)thusln5 < lln(1 - c)resulting in the following expression for hln5l<- ln (1  - e)This means that the confidence parameter 5 andthe error parameter e are exponentially smallw.r.t, the number of examples l feeded into thelearning algorithm.
This means that for an arbi-trary 5 and e we can keep l polynomial becausethe 5 and e decrease xponentially with respectto I.183
