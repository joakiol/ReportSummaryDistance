Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 51?56,Uppsala, Sweden, 15-16 July 2010.c?2010 Association for Computational LinguisticsSemEval-2010 Task 12: Parser Evaluation using Textual EntailmentsDeniz YuretKoc?
University?Istanbul, Turkeydyuret@ku.edu.trAyd?n HanKoc?
University?Istanbul, Turkeyahan@ku.edu.trZehra TurgutKoc?
University?Istanbul, Turkeyzturgut@ku.edu.trAbstractParser Evaluation using Textual Entail-ments (PETE) is a shared task in theSemEval-2010 Evaluation Exercises onSemantic Evaluation.
The task involvesrecognizing textual entailments based onsyntactic information alone.
PETE intro-duces a new parser evaluation scheme thatis formalism independent, less prone toannotation error, and focused on semanti-cally relevant distinctions.1 IntroductionParser Evaluation using Textual Entailments(PETE) is a shared task that involves recognizingtextual entailments based on syntactic informationalone.
Given two text fragments called ?text?
and?hypothesis?, textual entailment recognition is thetask of determining whether the meaning of thehypothesis is entailed (can be inferred) from thetext.
In contrast with general RTE tasks (Daganet al, 2009) the PETE task focuses on syntacticentailments:Text: The man with the hat was tired.Hypothesis-1: The man was tired.
(yes)Hypothesis-2: The hat was tired.
(no)PETE is an evaluation scheme based on a natu-ral human linguistic competence (i.e.
the ability tocomprehend sentences and answer simple yes/noquestions about them).
We believe systems shouldtry to model natural human linguistic competencerather than their dubious competence in artificialtagging tasks.The PARSEVAL measures introduced nearly twodecades ago (Black et al, 1991) still dominate thefield of parser evaluation.
These methods com-pare phrase-structure bracketings produced by theparser with bracketings in the annotated corpus, or?treebank?.
Parser evaluation using short textualentailments has the following advantages com-pared to treebank based evaluation.Consistency: Recognizing syntactic entail-ments is a more natural task for people thantreebank annotation.
Focusing on a naturalhuman competence makes it practical to collecthigh quality evaluation data from untrainedannotators.
The PETE dataset was annotated byuntrained Amazon Mechanical Turk workers atan insignificant cost and each annotation is basedon the unanimous agreement of at least threeworkers.
In contrast, of the 36306 constituentstrings that appear multiple times in the PennTreebank (Marcus et al, 1994), 5646 (15%) havemultiple conflicting annotations.
If indicative ofthe general level of inconsistency, 15% is a veryhigh number given that the state of the art parsersclaim f-scores above 90% (Charniak and Johnson,2005).Relevance: PETE automatically focuses atten-tion on semantically relevant phenomena ratherthan differences in annotation style or linguisticconvention.
Whether a phrase is tagged ADJP vsADVP rarely affects semantic interpretation.
At-taching the wrong subject to a verb or the wrongprepositional phrase to a noun changes the mean-ing of the sentence.
Standard treebank based eval-uation metrics do not distinguish between seman-tically relevant and irrelevant errors (Bonnema etal., 1997).
In PETE semantically relevant differ-ences lead to different entailments, semanticallyirrelevant differences do not.Framework independence: Entailment recog-nition is a formalism independent task.
A com-mon evaluation method for parsers that do not usethe Penn Treebank formalism is to automaticallyconvert the Penn Treebank to the appropriate for-malism and to perform treebank based evaluation(Nivre et al, 2007a; Hockenmaier and Steedman,512007).
The inevitable conversion errors compoundthe already mentioned problems of treebank basedevaluation.
In addition, manually designed tree-banks do not naturally lend themselves to unsu-pervised parser evaluation.
Unlike treebank basedevaluation, PETE can compare phrase structureparsers, dependency parsers, unsupervised parsersand other approaches on an equal footing.PETE was inspired by earlier work on represen-tations of grammatical dependency, proposed forease of use by end users and suitable for parserevaluation.
These include the grammatical rela-tions (GR) by (Carroll et al, 1999), the PARC rep-resentation (King et al, 2003), and Stanford typeddependencies (SD) (De Marneffe et al, 2006) (See(Bos and others, 2008) for other proposals).
Eachuse a set of binary relations between words ina sentence as the primary unit of representation.They share some common motivations: usabilityby people who are not (computational) linguistsand suitability for relation extraction applications.Here is an example sentence and its SD represen-tation (De Marneffe and Manning, 2008):Bell, based in Los Angeles, makes and dis-tributes electronic, computer and building prod-ucts.nsubj(makes-8, Bell-1)nsubj(distributes-10, Bell-1)partmod(Bell-1, based-3)nn(Angeles-6, Los-5)prep-in(based-3, Angeles-6)conj-and(makes-8, distributes-10)amod(products-16, electronic-11)conj-and(electronic-11, computer-13)amod(products-16, computer-13)conj-and(electronic-11, building-15)amod(products-16, building-15)dobj(makes-8, products-16)PETE goes one step further by translating mostof these dependencies into natural language entail-ments.Bell makes something.Bell distributes something.Someone is based in Los Angeles.Someone makes products.PETE has some advantages over representationsbased on grammatical relations.
For example SDdefines 55 relations organized in a hierarchy, andit may be non-trivial for a non-linguist to under-stand the difference between ccomp (clausal com-plement with internal subject) and xcomp (clausalcomplement with external subject) or betweennsubj (nominal subject) and xsubj (controllingsubject).
In fact it could be argued that proposalslike SD replace one artificial annotation formal-ism with another and no two such proposals agreeon the ideal set of binary relations to use.
In con-trast, untrained annotators have no difficulty unan-imously agreeing on the validity of most PETEtype entailments.However there are also significant challengesassociated with an evaluation scheme like PETE.It is not always clear how to convert certain rela-tions into grammatical hypothesis sentences with-out including most of the original sentence in thehypothesis.
Including too much of the sentence inthe hypothesis would increase the chances of get-ting the right answer with the wrong parse.
Gram-matical hypothesis sentences are especially diffi-cult to construct when a (negative) entailment isbased on a bad parse of the sentence.
Introduc-ing dummy words like ?someone?
or ?something?alleviates part of the problem but does not helpin the case of clausal complements.
In summary,PETE makes the annotation phase more practicaland consistent but shifts the difficulty to the entail-ment creation phase.PETE gets closer to an extrinsic evaluation byfocusing on semantically relevant, application ori-ented differences that can be expressed in natu-ral language sentences.
This makes the evaluationprocedure indirect: a parser developer has to writean extension that can handle entailment questions.However, given the simplicity of the entailments,the complexity of such an extension is comparableto one that extracts grammatical relations.The balance of what is being evaluated is alsoimportant.
A treebank based evaluation schememay mix semantically relevant and irrelevant mis-takes, but at least it covers every sentence at a uni-form level of detail.
In this evaluation, we focusedon sentences and relations where state of the artparsers disagree.
We hope this methodology willuncover weaknesses that the next generation sys-tems can focus on.The remaining sections will go into more de-tail about these challenges and the solutions wehave chosen to implement.
Section 2 explains themethod followed to create the PETE dataset.
Sec-52tion 3 evaluates the baseline systems the task or-ganizers created by implementing simple entail-ment extensions for several state of the art parsers.Section 4 presents the participating systems, theirmethods and results.
Section 5 summarizes ourcontribution.2 DatasetTo generate the entailments for the PETE task wefollowed the following three steps:1.
Identify syntactic dependencies that are chal-lenging to state of the art parsers.2.
Construct short entailment sentences thatparaphrase those dependencies.3.
Identify the subset of the entailments withhigh inter-annotator agreement.2.1 Identifying Challenging DependenciesTo identify syntactic dependencies that are chal-lenging for current state of the art parsers, we usedexample sentences from the following sources:?
The ?Unbounded Dependency Corpus?
(Rimell et al, 2009).
An unbounded de-pendency construction contains a word orphrase which appears to have been moved,while being interpreted in the position ofthe resulting ?gap?.
An unlimited numberof clause boundaries may intervene betweenthe moved element and the gap (hence?unbounded?).?
A list of sentences from the Penn Treebankon which the Charniak parser (Charniak andJohnson, 2005) performs poorly1.?
The Brown section of the Penn Treebank.We tested a number of parsers (both phrasestructure and dependency) on these sentences andidentified the differences in their output.
We tooksentences where at least one of the parsers gave adifferent answer than the others or the gold parse.Some of these differences reflected linguistic con-vention rather than semantic disagreement (e.g.representation of coordination) and some did notrepresent meaningful differences that can be ex-pressed with entailments (e.g.
labeling a phraseADJP vs ADVP).
The remaining differences typ-ically reflected genuine semantic disagreements1http://www.cs.brown.edu/?ec/papers/badPars.txt.gzthat would effect downstream applications.
Thesewere chosen to turn into entailments in the nextstep.2.2 Constructing EntailmentsWe tried to make the entailments as targeted aspossible by building them around two contentwords that are syntactically related.
When the twocontent words were not sufficient to construct agrammatical sentence we used one of the follow-ing techniques:?
Complete the mandatory elements using thewords ?somebody?
or ?something?.
(e.g.To test the subject-verb dependency in ?Johnkissed Mary.?
we construct the entailment?John kissed somebody.?)?
Make a passive sentence to avoid using a spu-rious subject.
(e.g.
To test the verb-objectdependency in ?John kissed Mary.?
we con-struct the entailment ?Mary was kissed.?)?
Make a copular sentence or use existen-tial ?there?
to express noun modification.(e.g.
To test the noun-modifier dependencyin ?The big red boat sank.?
we construct theentailment ?The boat was big.?
or ?There wasa big boat.?
)2.3 Filtering EntailmentsTo identify the entailments that are clear to humanjudgement we used the following procedure:1.
Each entailment was tagged by 5 untrainedannotators from the Amazon MechanicalTurk crowdsourcing service.2.
The results from the annotators whose agree-ment with the gold parse fell below 70% wereeliminated.3.
The entailments for which there was unani-mous agreement of at least 3 annotators werekept.The instructions for the annotators were briefand targeted people with no linguistic background:Computers try to understand long sentences bydividing them into a set of short facts.
You willhelp judge whether the computer extracted theright facts from a given set of 25 English sen-tences.
Each of the following examples consistsof a sentence (T), and a short statement (H) de-rived from this sentence by a computer.
Please53read both of them carefully and choose ?Yes?if the meaning of (H) can be inferred from themeaning of (T).
Here is an example:(T) Any lingering suspicion that this was a trickAl Budd had thought up was dispelled.
(H) The suspicion was dispelled.
Answer: YES(H) The suspicion was a trick.
Answer: NOYou can choose the third option ?Not sure?
whenthe (H) statement is unrelated, unclear, ungram-matical or confusing in any other manner.The ?Not sure?
answers were grouped with the?No?
answers during evaluation.
Approximately50% of the original entailments were retained afterthe inter-annotator agreement filtering.2.4 Dataset statisticsThe final dataset contained 367 entailments whichwere randomly divided into a 66 sentence devel-opment test and a 301 sentence test set.
52% ofthe entailments in the test set were positive.Approximately half of the final entailmentswere from the Unbounded Dependency Corpus,a third were from the Brown section of the PennTreebank, and the remaining were from the Char-niak sentences.
Table 1 lists the most frequentgrammatical relations encountered in the entail-ments.GR EntailmentsDirect object 42%Nominal subject 33%Reduced relative clause 21%Relative clause 13%Passive nominal subject 6%Object of preposition 5%Prepositional modifier 4%Conjunct 2%Adverbial modifier 2%Free relative 2%Table 1: Most frequent grammatical relations en-countered in the entailments.3 BaselinesIn order to establish baseline results for this task,we built an entailment decision system for CoNLLformat dependency files and tested several pub-licly available parsers.
The parsers used were theBerkeley Parser (Petrov and Klein, 2007), Char-niak Parser (Charniak and Johnson, 2005), CollinsParser (Collins, 2003), Malt Parser (Nivre et al,2007b), MSTParser (McDonald et al, 2005) andStanford Parser (Klein and Manning, 2003).
Eachparser was trained on sections 02-21 of the WSJsection of Penn Treebank.
Outputs of phrasestructure parsers were automatically annotatedwith function tags using Blaheta?s function tag-ger (Blaheta and Charniak, 2000) and converted tothe dependency structure with LTH Constituent-to-Dependency Conversion Tool (Johansson andNugues, 2007).To decide the entailments both the test andhypothesis sentences were parsed.
All the con-tent words in the hypothesis sentence were de-termined by using part-of-speech tags and depen-dency relations.
After applying some heuristicssuch as active-passive conversion, the extracteddependency path between the content words wassearched in the dependency graph of the test sen-tence.
In this search process, same relation typesfor the direct relations between the content wordpairs and isomorphic subgraphs in the test and hy-pothesis sentences were required for the ?YES?answer.Table 2 lists the baseline results achieved.
Thereare significant differences in the entailment accu-racies of systems that have comparable unlabeledattachment scores.
One potential reason for thisdifference is the composition of the PETE datasetwhich emphasizes challenging syntactic construc-tions that some parsers may be better at.
Anotherreason is the complete indifference of treebankbased measures like UAS to the semantic signif-icance of various dependencies and their impacton potential applications.System PETE UASBerkeley Parser 68.1% 91.2Stanford Parser 66.1% 90.2Malt Parser 65.5% 89.8Charniak Parser 64.5% 93.2Collins Parser 63.5% 91.6MST Parser 59.8% 92.0Table 2: Baseline systems: The second columngives the performance on the PETE test set, thethird column gives the unlabeled attachment scoreon section 23 of the Penn Treebank.4 SystemsThere were 20 systems from 7 teams participat-ing in the PETE task.
Table 3 gives the percent-age of correct answers for each system.
12 sys-54System Accuracy Precision Recall F1360-418-Cambridge 0.7243 0.7967 0.6282 0.7025459-505-SCHWA 0.7043 0.6831 0.8013 0.7375473-568-MARS-3 0.6678 0.6591 0.7436 0.6988372-404-MDParser 0.6545 0.7407 0.5128 0.6061372-509-MaltParser 0.6512 0.7429 0.5000 0.5977473-582-MARS-5 0.6346 0.6278 0.7244 0.6726166-415-JU-CSE-TASK12-2 0.5781 0.5714 0.7436 0.6462166-370-JU-CSE-TASK12 0.5482 0.5820 0.4551 0.5108390-433-Berkeley Parser Based 0.5415 0.5425 0.7372 0.6250473-566-MARS-1 0.5282 0.5547 0.4551 0.5108473-569-MARS-4 0.5249 0.5419 0.5385 0.5402390-431-Brown Parser Based 0.5216 0.5349 0.5897 0.5610473-567-MARS-2 0.5116 0.5328 0.4679 0.4983363-450-VENSES 0.5083 0.5220 0.6090 0.5621473-583-MARS-6 0.5050 0.5207 0.5641 0.5415390-432-Brown Reranker Parser Based 0.5017 0.5217 0.4615 0.4898390-435-Berkeley Parser with substates 0.5017 0.5395 0.2628 0.3534390-434-Berkeley Parser with Self Training 0.4983 0.5248 0.3397 0.4125390-437-Combined 0.4850 0.5050 0.3269 0.3969390-436-Berkeley Parser with Viterbi Decoding 0.4784 0.4964 0.4359 0.4642Table 3: Participating systems and their scores.
The system identifier consists of the participant ID,system ID, and the system name given by the participant.
Accuracy gives the percentage of correctentailments.
Precision, Recall and F1 are calculated for positive entailments.tems performed above the ?always yes?
baselineof 51.83%.Most systems started the entailment decisionprocess by extracting syntactic dependencies,grammatical relations, or predicates by parsing thetext and hypothesis sentences.
Several submis-sions, including the top two scoring systems usedthe C&C Parser (Clark and Curran, 2007) whichis based on Combinatory Categorical Grammar(CCG) formalism.
Others used dependency struc-tures produced by Malt Parser (Nivre et al,2007b), MSTParser (McDonald et al, 2005) andStanford Parser (Klein and Manning, 2003).After the parsing step, the decision for the en-tailment was based on the comparison of relations,predicates, or dependency paths between the textand the hypothesis.
Most systems relied on heuris-tic methods of comparison.
A notable exception isthe MARS-3 system which used an SVM-basedclassifier to decide on the entailment using depen-dency path features.Table 4 lists the frequency of various grammati-cal relations in the instances where the top systemmade mistakes.
A comparison with Table 1 showsthe direct objects and reduced relative clauses tobe the frequent causes of error.5 ContributionsWe introduced PETE, a new method for parserevaluation using textual entailments.
By basingthe entailments on dependencies that current stateGR EntailmentsDirect object 51%Reduced relative clause 36%Nominal subject 20%Object of preposition 7%Passive nominal subject 7%Table 4: Frequency of grammatical relations in en-tailment instances that got wrong answers from theCambridge system.of the art parsers disagree on, we hoped to cre-ate a dataset that would focus attention on thelong tail of parsing problems that do not get suffi-cient attention using common evaluation metrics.By further restricting ourselves to differences thatcan be expressed by natural language entailments,we hoped to focus on semantically relevant deci-sions rather than accidents of convention whichget mixed up in common evaluation metrics.
Wechose to rely on untrained annotators on a natu-ral inference task rather than trained annotatorson an artificial tagging task because we believe(i) many subfields of computational linguistics arestruggling to make progress because of the noisein artificially tagged data, and (ii) systems shouldtry to model natural human linguistic competencerather than their dubious competence in artificialtagging tasks.
Our hope is datasets like PETE willbe used not only for evaluation but also for trainingand fine-tuning of systems in the future.
Further55work is needed to automate the entailment gener-ation process and to balance the composition ofsyntactic phenomena covered in a PETE dataset.AcknowledgmentsWe would like to thank Laura Rimell, StephanOepen and Anna Mac for their careful analysis andvaluable suggestions.
?Onder Eker contributed tothe early development of the PETE task.ReferencesE.
Black, S. Abney, D. Flickenger, C. Gdaniec, R. Gr-ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,J.
Klavans, et al 1991.
A Procedure for Quanti-tatively Comparing the Syntactic Coverage of En-glish Grammars.
In Speech and natural language:proceedings of a workshop, held at Pacific Grove,California, February 19-22, 1991, page 306.
Mor-gan Kaufmann Pub.D.
Blaheta and E. Charniak.
2000.
Assigning func-tion tags to parsed text.
In Proceedings of the 1stNorth American chapter of the Association for Com-putational Linguistics conference, page 240.
Mor-gan Kaufmann Publishers Inc.R.
Bonnema, R. Bod, and R. Scha.
1997.
A DOPmodel for semantic interpretation.
In Proceedingsof the eighth conference on European chapter of theAssociation for Computational Linguistics, pages159?167.
Association for Computational Linguis-tics.Johan Bos et al, editors.
2008.
Proceedings of theWorkshop on Cross-Framework and Cross-DomainParser Evaluation.
In connection with the 22nd In-ternational Conference on Computational Linguis-tics.J.
Carroll, G. Minnen, and T. Briscoe.
1999.
Cor-pus annotation for parser evaluation.
In Proceedingsof the EACL workshop on Linguistically InterpretedCorpora (LINC).E.
Charniak and M. Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminative reranking.In Proceedings of the 43rd Annual Meeting on As-sociation for Computational Linguistics, page 180.Association for Computational Linguistics.S.
Clark and J.R. Curran.
2007.
Wide-coverage ef-ficient statistical parsing with CCG and log-linearmodels.
Computational Linguistics, 33(4):493?552.M.
Collins.
2003.
Head-driven statistical models fornatural language parsing.
Computational linguis-tics, 29(4):589?637.I.
Dagan, B. Dolan, B. Magnini, and D. Roth.
2009.Recognizing textual entailment: Rational, evalua-tion and approaches.
Natural Language Engineer-ing, 15(04).M.C.
De Marneffe and C.D.
Manning, 2008.
Stanfordtyped dependencies manual.M.C.
De Marneffe, B. MacCartney, and C.D.
Manning.2006.
Generating typed dependency parses fromphrase structure parses.
In LREC 2006.J.
Hockenmaier and M. Steedman.
2007.
CCGbank:a corpus of CCG derivations and dependency struc-tures extracted from the Penn Treebank.
Computa-tional Linguistics, 33(3):355?396.R.
Johansson and P. Nugues.
2007.
Extendedconstituent-to-dependency conversion for English.In Proc.
of the 16th Nordic Conference on Compu-tational Linguistics (NODALIDA).T.H.
King, R. Crouch, S. Riezler, M. Dalrymple, andR.
Kaplan.
2003.
The PARC 700 dependencybank.
In Proceedings of the EACL03: 4th Interna-tional Workshop on Linguistically Interpreted Cor-pora (LINC-03), pages 1?8.D.
Klein and C.D.
Manning.
2003.
Accurate un-lexicalized parsing.
In Proceedings of the 41stAnnual Meeting on Association for ComputationalLinguistics-Volume 1, pages 423?430.
Associationfor Computational Linguistics.M.P.
Marcus, B. Santorini, and M.A.
Marcinkiewicz.1994.
Building a large annotated corpus of En-glish: The Penn Treebank.
Computational linguis-tics, 19(2):313?330.R.
McDonald, F. Pereira, K. Ribarov, and J. Ha-jic.
2005.
Non-projective dependency parsing us-ing spanning tree algorithms.
In Proceedings ofHLT/EMNLP, pages 523?530.J.
Nivre, J.
Hall, S. K?ubler, R. McDonald, J. Nilsson,S.
Riedel, and D. Yuret.
2007a.
The CoNLL 2007shared task on dependency parsing.
In Proceed-ings of the CoNLL Shared Task Session of EMNLP-CoNLL, volume 7, pages 915?932.J.
Nivre, J.
Hall, J. Nilsson, A. Chanev, G. Eryigit,S.
K?ubler, S. Marinov, and E. Marsi.
2007b.
Malt-Parser: A language-independent system for data-driven dependency parsing.
Natural Language En-gineering, 13(02):95?135.S.
Petrov and D. Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Proceedings of NAACLHLT 2007, pages 404?411.L.
Rimell, S. Clark, and M. Steedman.
2009.
Un-bounded dependency recovery for parser evaluation.In Proceedings of the 2009 Conference on Empiri-cal Methods in Natural Language Processing, pages813?821.
Association for Computational Linguis-tics.56
