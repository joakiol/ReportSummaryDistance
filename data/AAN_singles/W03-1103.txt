An Approach for Combining Content-based and Collaborative FiltersQing LiDept.
of Computer SciencesKumoh National Institute of TechnologyKumi, kyungpook, 730-701,South Korealiqing@se.Kumoh.ac.krByeong Man KimDept.
of Computer SciencesKumoh National Institute of TechnologyKumi, kyungpook, 730-701,South Koreabmkim@se.Kumoh.ac.krAbstractIn this work, we apply a clustering tech-nique to integrate the contents of itemsinto the item-based collaborative filteringframework.
The group rating informationthat is obtained from the clustering resultprovides a way to introduce content in-formation into collaborative recommenda-tion and solves the cold start problem.Extensive experiments have been con-ducted on MovieLens data to analyze thecharacteristics of our technique.
The re-sults show that our approach contributesto the improvement of prediction qualityof the item-based collaborative filtering,especially for the cold start problem.1 IntroductionThere are two dominant research paradigms of in-formation filtering: content-based and collabora-tive filtering.
Content-based filtering selects theright information for users by comparing represen-tations of searching information to representationsof contents of user profiles which express interestsof users.
Content-based information filtering hasproven to be effective in locating textual itemsrelevant to a topic using techniques, such as Boo-lean queries (Anick et al, 1990; Lee et al, 1993;Verhoeff et al, 1961), vector-space queries (Saltonand Buckley, 1998), probabilistic model (Robert-son and Sparck, 1976), neural network (Kim andRaghavan, 2000) and fuzzy set model (Ogawa etal., 1991).
However, content-based filtering hassome limitations:?
It is hard for content-based filtering to pro-vide serendipitous recommendations, be-cause all the information is selected andrecommended based on the content.?
It is hard for novices to use content-basedsystems effectively.Collaborative filtering is the technique of usingpeer opinions to predict the interests of others.
Atarget user is matched against the database to dis-cover neighbors, who have historically had similarinterests to target user.
Items that neighbors likeare then recommended to the target user.
The Tap-estry text filtering system, developed by Nicholsand others at the Xerox Palo Alto Research Center(PARC), applied collaborative filtering (Douglas,1993; Harman, 1994).
The GroupLens project atthe University of Minnesota is a popular collabora-tive system.
Collaborative systems have beenwidely used in so many areas, such as Ringo sys-tem recommends music albums (Upendar and Patti,1995), MovieLens system recommends movies,Jeter system recommends jokes (Gupta et al, 1999)and Flycasting recommends online radio (Hauver,2001).Collaborative filtering system overcomes somelimitations of content-based filtering.
The systemcan suggest items (the things to be recommended,such as books, music etc.)
to users and recommen-dations are based on the ratings of items, instead ofthe contents of the items, which can improve thequality of recommendations.
Although collabora-tive filtering has been successfully used in bothresearch and practice, there still remain some chal-lenges for it as an efficient information filtering.This work was supported by Korea Research FoundationGrant (KRF-2002-041-D00459).?
Cold start problem, where recommendationsare required for items that no user has yetrated.?
Although collaborative filtering can improvethe quality of recommendations based on theuser ratings, it completely denies any infor-mation that can be extracted from contents.It is obvious that the content-based filteringdoes not suffer the above problems.
So it is a natu-ral way to combine them in order to achieve a bet-ter performance of filtering, and take theadvantages of each.The rest of the paper is organized as follows.The next section provides a brief describing of re-lated work.
In section 3, we present the detail algo-rithmic components of our approach, and look intothe methods of grouping items, calculating thesimilarities between items and solving the coldstart problem.
Section 4 describes our experimentalwork.
It provides details of our data sets, evalua-tion metrics, results of our experiment and discus-sion of the results.
The final section provides someconcluding remarks.2 Related workProposed approaches to hybrid system, whichcombines content-based and collaborative filterstogether, can be categorized into two groups.One group is the linear combination of resultsof collaborative and content-based filtering, suchas systems that are described by Claypool (1999)and Wasfi (1999).
ProfBuilder (Wasfi, 1999) rec-ommends web pages using both content-based andcollaborative filters, and each creates a recommen-dation list without combining them to make acombined prediction.
Claypool (1999) describes ahybrid approach for an online newspaper domain,combining the two predictions using an adaptiveweighted average: as the number of users access-ing an item increases, the weight of the collabora-tive component tends to increase.
But how todecide the weights of collaborative and content-based components is unclearly given by the author.The other group is the sequential combinationof content-based filtering and collaborative filter-ing.
In this system, firstly, content-based filteringalgorithm is applied to find users, who share simi-lar interests.
Secondly, collaborative algorithm isapplied to make predictions, such as RAAP(Delgado et al, 1998) and Fab filtering systems(Balabanovic and Shoham, 1990).
RAAP is a con-tent-based collaborative information filtering forhelping the user to classify domain specific infor-mation found in the WWW, and also recommendsthese URLs to other users with similar interests.
Todecide the similar interests of users is using scal-able Pearson correlation algorithm based on webpage category.
Fab system, which uses content-based techniques instead of user ratings to createprofiles of users.
So the quality of predictions isfully depended on the content-based techniques,inaccurate profiles result in inaccurate correlationswith other users and thus make poor predictions.As for collaborative recommendation, there aretwo ways to calculate the similarity for clique rec-ommendation ?
item-based and user-based.
Sarwar(Sarwar et al 2001) has proved that item-basedcollaborative filtering is better than user-based col-laborative filtering at precision and computationcomplexity.Figure1.
Overview of the our approach3 Overview of our approachIn this paper, we suggest a technique that intro-duces the contents of items into the item-basedcollaborative filtering to improve its predictionquality and solve the cold start problem.
Shortly,we call the technique ICHM (Item-based Cluster-ing Hybrid Method).In ICHM, we integrate the item information anduser ratings to calculate the item-item similarity.Figure 1 shows this procedure.
The detail proce-dure of our approach is described as follows:?
Apply clustering algorithm to group theitems, then use the result, which is repre-sented by the fuzzy set, to create a group-rating matrix.?
Compute the similarity: firstly, calculate thesimilarity of group-rating matrix using ad-justed-cosine algorithm, then calculate thesimilarity of item-rating matrix using Pear-son correlation-based algorithm.
At last, theRating Data+ItemratingCollaborativefilter GroupratingGroupraterClustering ItemcontentItemgroupvectortotal similarity is the linear combination ofthe above two.?
Make a prediction for an item by perform-ing a weighted average of deviations fromthe neighbour?s mean.3.1  Group ratingThe goal of grouping ratings is to group the itemsinto several cliques and provides content-basedinformation for collaborative similarity calculation.Each item has it?s own attribute features, such asmovie item, which may have actor, actress, direc-tor, genre, and synopsis as its attribute features.Thus, we can group the items based on them.The algorithm that is applied for grouping rat-ings is derived from K-means Clustering Algo-rithm (Han and Kamber, 2000).
The difference isthat we apply the fuzzy set theory to represent theaffiliation between object and cluster.
As shown inFigure 2, firstly, items are grouped into a givennumber of clusters.
After completion of grouping,the probability of one object j (here one objectmeans one item) to be assigned to a certain clusteris calculated as follows.
( , )Pr ( , ) 1-                                (1)( , )CS j ko j kMaxCS i k=where Pr ( , )o j k means the probability of object j  tobe assigned to cluster k ; The ( , )CS j k  means thefunction to calculate the counter-similarity be-tween object j  and cluster k ;  ( , )Max CS i k meansthe maximum counter-similarity between an objectand cluster k .Input : the number of clusters k  and item attributesOutput: a set of k clusters that minimizes the squared-error criterion, and the probability of each item to beassigned to each cluster center, which are represented asa fuzzy set.
(1) Arbitrarily choose k  objects as the initial clustercenters(2) Repeat (a) and (b) until no change(a) (Re) assign each object to the cluster to which theobject is the most similar, based on the mean value ofthe objects in the cluster(b) Update the cluster means, i.e., calculate the meanvalue of the objects of each cluster;(3) Compute the probability between objects and eachcluster center.Figure 2.
Algorithm for grouping ratingsThe counter-similarity ( , )CS j k  can be calcu-lated by Euclidean distance or Cosine method.3.2 Similarity computationAs we can see, after grouping the items, we get anew rating matrix.
We can use the item-based col-laborative algorithm to calculate the similarity andmake the predictions for users.There are many ways to compute the similarity.In our approach, we use two of them, and make alinear combination of their results.3.2.1 Pearson correlation-based similarityThe most common measure for calculating thesimilarity is the Pearson correlation algorithm.Pearson correlation measures the degree to which alinear relationship exists between two variables.The Pearson correlation coefficient is derived froma linear regression model, which relies on a set ofassumptions regarding the data, namely that therelationship must be linear, and the errors must beindependent and have a probability distributionwith mean 0 and constant variance for every set-ting of the independent variable (McClave andDietrich, 1998)., ,12 2, ,1 1( )( )cov( , )( , )      (2)( ) ( )mu k k u l lum mk l u k k u l lu iR R R Rk lsim k lR R R R?
?== =?
?= =?
???
?where ( , )sim k l  means the similarity between itemk  and l ; m  means the total number of users, whorated on both item k  and l ; kR , lR  are the averageratings of item k  and l , respectively;,u kR , ,u lR mean the rating of user u on item k  and lrespectively.3.2.2 Adjust cosine similarityCosine similarity once has been used to calculatethe similarity of users but it has one shortcoming.The difference in rating scale between differentusers will result in a quite different similarity.
Forinstance, if Bob only rates score 4 on the bestmovie, he never rates 5 on any movie; and he rates1 on the bad movie, instead of the standard levelscore 2.
But Oliver always rates according to thestandard level.
He rates score 5 on the best movie,and 2 on the bad movie.
If we use traditional co-sine similarity, both of them are quite different.The adjusted cosine similarity (Sarwar et al, 2001)was provided to offset this drawback., ,12 2, ,1 1( )( )( , )               (3)( ) ( )mu k u u l uum mu k u u l uu uR R R Rsim k lR R R R== =?
?=?
???
?where ( , )sim k l  means the similarity between itemk  and l ; m  means the total number of users, whorates on both item k  and l ; uR  are the average rat-ings of user u ; ,u kR , ,u lR mean the rating of user u onitem k  and l  respectively.3.2.3 Linear combination of similarityDue to difference in value range between item-rating matrix and group-rating matrix, we use dif-ferent methods to calculate the similarity.
As foritem-ratings matrix, the rating value is integer; Asfor group-rating matrix, it is the real value rangingfrom 0 to 1.
The natural way is to enlarge the con-tinuous data range from [0 1] to [1 5] or reduce thediscrete data range from [1 5] to [0 1] and then ap-ply Pearson correlation-based algorithm or ad-justed cosine algorithm to calculate similarity.
Wecall this enlarged ICHM.
We also propose anothermethod: firstly, use Pearson correlation-based al-gorithm to calculate the similarity from item-ratingmatrix, and then calculate the similarity fromgroup-rating matrix by adjusted cosine algorithm,at last, the total user similarity is linear combina-tion of the above two, we call this combinationICHM.
( , ) ( , ) (1- ) ( , )            (4)item groupsim k l sim k l c sim k l c= ?
+ ?where ( , )sim k l  means the similarity between itemk and l ; c  means the combination coefficient;( , )itemsim k l means that the similarity between itemk and l , which is calculated from item-rating ma-trix; ( , )groupsim k l means that the similarity betweenitem k and l , which is calculated from group-rating matrix.3.3 Collaborative predictionPrediction for an item is then computed by per-forming a weighted average of deviations from theneighbour?s mean.
Here we use top N  rule to se-lect the nearest N  neighbours based on the simi-larities of items.
The general formula for aprediction on item k of user u (Resnick et al, 1994)is:,1,1( ) ( , )(5)( , )nu i iiu k k niR R sim k iP Rsim k i==?
?= + ?
?where ,u kP  represents the predication for the useru on item k ; n  means the total neighbours of itemk ; ,u iR means the user u  rating on item i ; kR  is theaverage ratings on item k ; ( , )sim k i  means the simi-larity between item k  and its?
neighbour i ; iRmeans the average ratings on item i .3.4 Cold start problemIn traditional collaborative filtering approach, it ishard for pure collaborative filtering to recommenda new item to user since no user made any ratingon this new item.
However, in our approach, basedon the information from group-rating matrix, wecan make predictions for the new item.
In our ex-periment, it shows a good recommendation per-formance for the new items.
In Equation 5, kR  isthe average rating of all ratings on item k .
As forthe new item, no user makes any rating on it, kRshould be the zero.
Since kR  is the standard base-line of user ratings and it is zero, it is unreasonablefor us to apply Equation 5 to new item.
Therefore,for a new item, we use the neighborsR , the average rat-ing of all ratings on the new item?s nearestneighbour instead of kR , which is inferred by thegroup-rating matrix.3.5 A scenario of our approachz Users:Number of users: threeUser name: Tom, Jack, and Oliverz Items:Item category: movieNumber of items: fiveTitle of items: Gone with the Wind, PearlHarbour, Swordfish, Hero, The Sound of Musicz Ratings: 1~5 integer scoreToo  bad:1  Bad:2  Common:3  Good:4  too good:5Table 1: Item-ratingTom  Jack  OliveGone with the Wind 5 3Swordfish 5 2 4Pearl Harbour 2 5Hero 4 2The Sound of MusicTable 2.
Group-ratingCluster1  Cluster2Gone with the Wind 98% 0.13%Swordfish 100% 0.02%Pearl Harbour 1.0% 95%Hero 95% 1.2%The Sound of Music 0.12% 98%The following is a procedure of our approach.?
Based on the item contents, such as moviegenre, director, actor, actress, even synopsis,we apply clustering algorithm to group theitems.
Here, we use fuzzy set to representthe clustering result.
Assume the result is asfollows: Cluster 1: {Gone with the Wind(98%), Swordfish (100%), Pearl Harbour(1.0%), Hero (95%), The Sound of Music(0.12%)}, Cluster 2: {Gone with the Wind(0.13%), Swordfish (0.02%), Pearl Harbour(95%), Hero (1.2%), The Sound of Music(98%)}, the number in the parenthesis fol-lowing the movie name means the probabil-ity of the movie belonging to the cluster.?
We use group-rating engine to make agroup-rating matrix.
As Table 2 shows.Then combine the group-rating matrix anditem-rating matrix to form a new rating ma-trix.?
Now, we can calculate the similarity be-tween items based on this new unified ratingdata matrix.
The similarity between itemsconsists of two parts.
The first part calcu-lates the similarity based on user ratings, us-ing the Pearson correlation-based algorithm.The second part calculates the similaritybased on the clustering result by using ad-justed cosine algorithm.
The total similaritybetween items is the linear combination ofthem.
For example, when we calculate thesimilarity between Gone with the Wind andSwordfish, firstly, itemsim(G,S) and groupsim(G,S)are calculated based on Equation 2 and 3separately.item 2 2 2 2(5-4) (5-3.5)+(3-4) (2-3.5)sim(G,S) = 1(5-4) +(3-4) (5-3.5) +(3.5-2)?
?
=?group2 2 2 2sim(G,S) =(0.98-0.59) (1-0.59)+(0.013-0.39) (0.002-0.39)(0.98-0.59) +(0.013-0.39) (1-0.59) +(0.002-0.39)0.9999?
?=Secondly, sim(G,S) is calculated based onFormula 4, here the combination coefficientis 0.4.sim(G,S)=1 (1-0.4)+0.9999 0.4=0.9999 ?
??
Then, predictions for items are calculated byperforming a weighted average of deviationsfrom the neighbour?s mean.In the example, we can observe, the item - TheSound of Music, which no one make any rating on,can be treated as a new item.
In traditional item-based collaborative method, which makes predic-tion only based on item-based matrix (Table 1), itis impossible to make predictions on this item.However, in our approach, we can make predictionfor users, based on group rating (Table 2).From the description of our approach, we canobserve that this approach can fully realize thestrengths of content-based filtering, mitigating theeffects of the new user problem.
In addition, whencalculating the similarity, our approach considersthe information not only from personal tastes butalso from the contents, which provides a latentability for better prediction and makes serendipi-tous recommendation.3.6 UCHMUCHMMovie 1 Movie 2 Movie 3 Cluster 1 Cluster 2User 1 ?
?
?
?
?User 2 ?
?
?
?
?User 3 ?
?
?
?
?ICHMICHM User 1 User 2 User 3 Cluster 1 Cluster 2Movie 1 ?
?
?
?
?Movie 2 ?
?
?
?
?Movie 3 ?
?
?
?
?Figure 3.
UCHM & ICHMClustering technique not only can be applied toitem-based collaborative recommenders but alsocan be applied to user-based collaborative recom-menders.
Shortly we call the late one UCHM(User-based Clustering Hybrid Method)In UCHM, clustering is based on the attributesof user profiles and clustering result is treated asitems.
However, in ICHM, clustering is based onthe attributes of items and clustering result istreated as users, as Figure 3 shows.In Combination UCHM, we apply Equation 2 tocalculate the similarity in user-rating matrix, andUser-rating   Matrix Group-rating MatrixGroup-rating   Matrix   Item-rating MatrixEquation 3 to calculate the similarity in group-rating matrix.
Then make a linear combination ofthem.
When we apply Equation 2 and 3 to UCHM,k  and l  mean the user and u means the item, in-stead the original meaning.As for UCHM, clustering is based on the userprofiles.
User profiles indicate the informationneeds or preferences on items that users are inter-ested in.
A user profile can consist of several pro-file vectors and each profile vector represents anaspect of his preferences, such as movie genre,director, actor, actress and synopsis.
The profilevectors are automatically constructed from ratingdata by the following simple equation.
( )   /                                           8A m n=where, n is the number of items whose rankingvalue is lager than a given threshold, m is the num-ber of items containing attribute A among n itemsand its ranking is larger than threshold.
In our ex-periment, we set the value of the threshold as 3.For example, in Section 3.5, Tom makes ratings onfour movies, and three of them lager than thethreshold 3.
From the genre information, we knowGone with the Wind belongs to love genre, sword-fish and Hero belong to action genre.
So Tom?sprofile is as follows.
Tom {love (1/3), action (2/3)}.4 Experimental evaluations4.1   Data setCurrently, we perform experiment on a subset ofmovie rating data collected from the MovieLensweb-based recommender.
The data set contained100,000 ratings from 943 users and 1,682 movies,with each user rating at least 20 items.
We dividedata set into a training set and a test data set.4.2 Evaluation metricsMAE (Mean Absolute Error) has widely been usedin evaluating the accuracy of a recommender sys-tem by comparing the numerical recommendationscores against the actual user ratings in the testdata.
The MAE is calculated by summing theseabsolute errors of the corresponding rating-prediction pairs and then computing the average., ,1                               (7)nu i u iuP RMAEn= ?= ?where ,u iP  means the user u prediction on item i ;,u iR  means the user u  rating on item i  in the testdata; n is the number of rating-prediction pairs be-tween the test data and the prediction result.
Thelower the MAE, the more accurate.4.3 Behaviours of our method0.7350.7450.7550.7650.7750 5 10 20 30 40 50 60 70No.
of ClustersMAEICHM UCHMFigure 4.
Sensitivity of the cluster sizeWe implement group-rating method described insection 3.1 and test them on MovieLens data withthe different number of clusters.
Figure 4 showsthe experimental results.
It can be observed that thenumber of clusters does affect the quality of pre-diction, no matter in UCHM or ICHM.Figure 5.
CoefficientIn order to find the optimal combination coeffi-cient c in the Equation 4, we conducted a series ofexperiments by changing combination coefficientfrom 0 to 1 with a constant step 0.1.
Figure 5shows that when the coefficient arrives at 0.4, anoptimal recommendation performance is achieved.0.730.7350.740.7450.750.7550.7610 20 30 40 50 60NO.of NeigborsMAECosine Angle Euclidean DistanceFigure 6.
Grouping itemsAs described in Section 3.2, our grouping rat-ings method needs to calculate similarity between0.730.740.750.760.770.780.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1CoefficientMAEICHM UCHMobjects and clusters.
So, we try two methods ?
oneis Euclidean distance and the other cosine angle.
Itcan be observed in Figure 6 that the approach ofcosine angle method has a trend to show better per-formance than the Euclidean Distance method, butthe difference is negligible.Figure 7.
ComparisonFrom the Figure 7, it can be observed that theperformance of combination ICHM is the best, andthe second is the enlarged ICHM, which is fol-lowed by the item-based collaborative method, thelast is UCHM (User-based Clustering HybridMethod) which applies the clustering techniquedescribed in Section 3 to user-based collaborativefiltering, where user profiles are clustered insteadof item contents.We also can observe that the size of neighbour-hood does affect the quality of prediction (Her-locker et al, 1999).
The performance improves aswe increase the neighbourhood size from 10 to 30,then tends to be flat.123451 2 3 4 5 6 7 8 9 10 11Movie itemsRatingsReal Value Predict ValueFigure 8.
Cold start problemTable 3.
MAE of new item10 20 30 40 50 100MAE 0.743 0.755 0.812 0.732 0.762 0.757As for cold start problem, we choose the itemsfrom the training data set and delete all the ratingsof those items, thus we can treat them as new items.First, we randomly selected item No.946.
In thetest data, user No.946 has 11 ratings, which is de-scribed by bar real value in Figure 8.
We can ob-serve that the prediction for a new item canpartially reflect the user preference.
To generalizethe observation, we randomly select the number ofitems from 10 to 50 with the step of 10 and 100from the test data, and delete all the ratings ofthose items and treat them as new items.
Table 3shows that ICHM can solve the cold start problem.0.720.730.740.75MAESynopsis GenreFigure 9.
Item attributeWhen we apply clustering method to movieitems, we use the item attribute ?
movie genre.However, our approach can consider more dimen-sion of item attribute, such as actor, actress, anddirector, even the synopsis.
In order to observe theeffect of the high dimension item attributes, wecollect the 100 movie synopsis from InternetMovie Database (http://www.imdb.com) to provideattribute information for clustering movies.
In ourexperiment, it shows that the correct attributes ofmovies can further improve the performance ofrecommender system, as Figure 9 shows.4.4    Our method versus the classic oneAlthough some hybrid recommender systems havealready exited, it is hard to make an evaluationamong them.
Some systems (Delgado et al, 1998)use Boolean value (relevant or irrelevant) to repre-sent user preferences, while others use numericvalue.
The same evaluation metrics cannot make afair comparison.
Further more, the quality of somesystems depends on the time, in which system pa-rameters are changed with user feedback (Claypoolet al, 1999), and Claypool does not clearly de-scribe how to change the weight with time passed.However, we can make a simple concept compari-son.
In Fab system, the similarity for prediction isonly based on the user profiles.
As for UCHM,which groups the content information of user pro-files and uses user-based collaborative algorithminstead of ICHM, the impact of combination coef-ficient can be observed in Figure 5.
In UCHM,when the value of coefficient equals to 1, it de-scribes condition that Fab applied, which meansthe similarity between users is only calculatedfrom the group-rating matrix.
In that condition, theMAE shows the worst quality of recommendation.0.7350.7450.7550.7650.77510 20 30 40 50 60 70 80 90 100No.
of neighborsMAECombination ICHMItem-based CollaborativeEnlarged ICHMCombination UCHM5 ConclusionsWe apply clustering technique to the item contentinformation to complement the user rating infor-mation, which improves the correctness of collabo-rative similarity, and solves the cold start problem.Our work indicates that the correct application ofthe item information can improve therecommendation performance.ReferencesAnick, P. G., Brennan, J. D., Flynn, R. A., Hanssen, D.R., Alvey, B. and Robbins, J.M.. 1990.
A Direct Ma-nipulation Interface for Boolean Information Re-trieval via Natural Language Query, In Proc.
ACM-SIGIR Conf., pp.135-150.Balabanovic, M. and Shoham, Y.. 1997.
Fab: Content-Based, Collaborative Recommendation, Communica-tions of the ACM, 40(3), pp.66-72.Claypool, M., Gokhale, A., Miranda, T., Murnikov, P.,Netes, D. and Sartin, M.. 1999.
Combining content-based and collaborative filters in an online newspa-per , In Proc.
ACM-SIGIR Workshop on Recom-mender Systems: Algorithms and Evaluation.Delgado, J., Ishii, N. and Ura, T.. 1998.
Content-basedCollaborative Information Filtering: Actively Learn-ing to Classify and Recommend Documents, In Proc.Second Int.
Workshop, CIA'98, pp.206-215.Douglas B. Terry.
1993.
A tour through tapestry, InProc.
ACM Conf.
on Organizational Computing Sys-tems (COOCS).
pp.21?30.Gupta, D., Digiovanni, M., Narita, H. and Goldberg, K..1999.
Jester 2.0: A New Linear-Time CollaborativeFiltering Algorithm Applied to Jokes, In Proc.
ACM-SIGIR Workshop on Recommender Systems: Algo-rithms and Evaluation.Han, J., and Kamber, M.. 2000.
Data mining: Conceptsand Techniques.
New York: Morgan-Kaufman.Harman D.. 1994.
Overview of TREC-3, In Proc.TREC-3, pp.1-19.Hauver, D. B.. 2001.
Flycasting: Using CollaborativeFiltering to Generate a Play list for Online Radio, InInt.
Conf.
on Web Delivery of Music.Herlocker, J., Konstan, J., Borchers A., and Riedl, J..1999.
An algorithmic framework for performing col-laborative Filtering, In Proc.
ACM-SIGIR Conf.,1999, pp.
230-237.Kim, M. and Raghavan, V.V.. 2000.
Adaptive concept-based retrieval using a neural network, In Proc.
OfACM-SIGIR Workshop on Mathematical/FormalMethods in IR.McClave, J. T. and Dietrich, F. H.. 1998.
Statistics.
SanFrancisco: Ellen Publishing Company.Lee, J.H., Kim, M.H.
and Lee, Y.H.. 1993.
Rankingdocuments in thesaurus-based Boolean retrieval sys-tems, Information Processing and Management, 30(1),pp.79-91.Oard, D.W. and Marchionini, G.. 1996.
A conceptualframework for text filtering, Technical Report EE-TR-96-25, CAR-TR-830, CS-TR3643.Ogawa, Y., Morita, T. and Kobayashi, K.. 1991.
A fuzzydocument retrieval system using the keyword connec-tion matrix and a learning method, Fuzzy sets andSystems, 1991, pp.39, pp.163-179.O'Conner, M. and Herlocker, J.. 1999.
Clustering itemsfor collaborative filtering, In Proc.
ACM-SIGIRWorkshop on Recommender Systems.Resnick, P., Iacovou, N., Suchak, M., Bergstorm, P. andRiedl, J.. 1994.
GroupLens: An open architecture forcollaborative filtering of Netnews, In Proc.
ACMConf.
on Computer-Supported Cooperative Work.pp.175-186.Ricardo Baeza-Yates, Berthier Riberio-Neto.
1999.Modern Information Retrieval.
New York:Addison-Wesley Publishers.Robertson S. E. and Sparck Jones K.. 1976.
Relevanceweighting of search terms, J. of the American Societyfor Information Science, 1976, pp.27, pp.129-146.Salton, G. and Buckley, C.. 1988.
Term-weight ap-proaches in automatic retrieval, Information Proc-essing and Management, 24(5), 1988, pp.513-523.Sarwar, B. M., Karypis, G., Konstan, J.
A. and Riedl, J..2001.
Item-based Collaborative Filtering Recom-mendation Algorithms, In Proc.
Tenth Int.
WWWConf.
2001, pp.
285-295.Upendra, S. and Patti, M.. 1995.
Social InformationFiltering: Algorithms for Automating "Word ofMouth", In Proc.
ACM CHI'95 Conf.
on Human Fac-tors in Computing Systems.
pp.210?217.Verhoeff, J., Goffman, W. and Belzer, J.. 1961.
Ineffi-ciency of the use of the boolean functions for infor-mation retrieval systems, Communications of theACM, 4, pp.557--558, pp.594.Wasfi, A. M. A.. 1999.
Collecting User Access Patternsfor Building user Profiles and Collaborative Filter-ing, In Int.
Conf.
on Intelligent User Interfaces.pp.57- 64.
