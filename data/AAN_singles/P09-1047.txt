Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 414?422,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPProfile Based Cross-Document CoreferenceUsing Kernelized Fuzzy Relational ClusteringJian Huang?
Sarah M. Taylor?
Jonathan L. Smith?
Konstantinos A. Fotiadis?
C. Lee Giles?
?College of Information Sciences and TechnologyPennsylvania State University, University Park, PA 16802, USA{jhuang, giles}@ist.psu.edu?Advanced Technology Office, Lockheed Martin IS&GS, Arlington, VA 22203, USA{sarah.m.taylor, jonathan.l.smith, konstantinos.a.fotiadis}@lmco.comAbstractCoreferencing entities across documentsin a large corpus enables advanceddocument understanding tasks such asquestion answering.
This paper presentsa novel cross document coreferenceapproach that leverages the profilesof entities which are constructed byusing information extraction tools andreconciled by using a within-documentcoreference module.
We propose tomatch the profiles by using a learnedensemble distance function comprisedof a suite of similarity specialists.
Wedevelop a kernelized soft relationalclustering algorithm that makes use ofthe learned distance function to partitionthe entities into fuzzy sets of identities.We compare the kernelized clusteringmethod with a popular fuzzy relationclustering algorithm (FRC) and show 5%improvement in coreference performance.Evaluation of our proposed methodson a large benchmark disambiguationcollection shows that they comparefavorably with the top runs in theSemEval evaluation.1 IntroductionA named entity that represents a person, an or-ganization or a geo-location may appear withinand across documents in different forms.
Crossdocument coreference (CDC) is the task of con-solidating named entities that appear in multipledocuments according to their real referents.
CDCis a stepping stone for achieving intelligent in-formation access to vast and heterogeneous textcorpora, which includes advanced NLP techniquessuch as document summarization and question an-swering.
A related and well studied task is withindocument coreference (WDC), which limits thescope of disambiguation to within the boundary ofa document.
When namesakes appear in an article,the author can explicitly help to disambiguate, us-ing titles and suffixes (as in the example, ?GeorgeBush Sr. ... the younger Bush?)
besides othermeans.
Cross document coreference, on the otherhand, is a more challenging task because theselinguistics cues and sentence structures no longerapply, given the wide variety of context and stylesin different documents.Cross document coreference research has re-cently become more popular due to the increasinginterests in the web person search task (Artileset al, 2007).
Here, a search query for a personname is entered into a search engine and thedesired outputs are documents clustered accordingto the identities of the entities in question.
Inour work, we propose to drill down to the sub-document mention level and construct an entityprofile with the support of information extractiontools and reconciled with WDC methods.
Henceour IE based approach has access to accurateinformation such as a person?s mentions and geo-locations for disambiguation.
Simple IR basedCDC approaches (e.g.
(Gooi and Allan, 2004)), onthe other hand, may simply use all the terms andthis can be detrimental to accuracy.
For example, abiography of John F. Kennedy is likely to mentionmembers of his family with related positions,besides references to other political figures.
Evenwith careful word selection, these textual featurescan still confuse the disambiguation system aboutthe true identity of the person.We propose to handle the CDC task using anovel kernelized fuzzy relational clustering algo-rithm, which allows probabilistic cluster mem-bership assignment.
This not only addresses theintrinsic uncertainty nature of the CDC problem,but also yields additional performance improve-ment.
We propose to use a specialist ensemble414learning approach to aggregate the diverse set ofsimilarities in comparing attributes and relation-ships in entity profiles.
Our approach is first fullydescribed in Section 2.
The effectiveness of theproposed method is demonstrated using real worldbenchmark test sets in Section 3.
We reviewrelated work in cross document coreference andconclude in Section 5.2 Methods2.1 Document Level and Profile Based CDCWe make distinctions between document level andprofile based cross document coreference.
Docu-ment level CDC makes a simplifying assumptionthat a named entity (and its variants) in a documenthas one underlying real identity.
The assump-tion is generally acceptable but may be violatedwhen a document refers to namesakes at the sametime (e.g.
George W. Bush and George H. W.Bush referred to as George or President Bush).Furthermore, the context surrounding the personNE President Clinton can be counterproductivefor disambiguating the NE Senator Clinton, withboth entities likely to appear in a document at thesame time.
The simplified document level CDChas nevertheless been used in the WePS evaluation(Artiles et al, 2007), called the web people task.In this work, we advocate profile based disam-biguation that aims to leverage the advances inNLP techniques.
Rather than treating a documentas simply a bag of words, an information extrac-tion tool first extracts NE?s and their relationships.For the NE?s of interest (i.e.
persons in this work),a within-document coreference (WDC) modulethen links the entities deemed as referring tothe same underlying identity into a WDC chain.This process includes both anaphora resolution(resolving ?He?
and its antecedent ?President Clin-ton?)
and entity tracking (resolving ?Bill?
and?President Clinton?).
Let E = {e1, ..., eN} denotethe set of N chained entities (each correspondingto a WDC chain), provided as input to the CDCsystem.
We intentionally do not distinguish whichdocument each ej belongs to, as profile basedCDC can potentially rectify WDC errors by lever-aging information across document boundaries.Each ei is represented as a profile which containsthe NE, its attributes and associated relationships,i.e.
ej =< ej,1, ..., ej,L > (ej,l can be a textualattribute or a pointer to another entity).
The profilebased CDC method generates a partition of E ,represented by a partition matrix U (where uijdenotes the membership of an entity ej to the i-th identity cluster).
Therefore, the chained entitiesplaced in a name cluster are deemed as coreferent.Profile based CDC addresses a finer grainedcoreference problem in the mention level, enabledby the recent advances in IE and WDC techniques.In addition, profile based CDC facilitates userinformation consumption with structured informa-tion and short summary passages.
Next, we focuson the relational clustering algorithm that lies atthe core of the profile based CDC system.
We thenturn our attention to the specialist learning algo-rithm for the distance function used in clustering,capable of leveraging the available training data.2.2 CDC Using Fuzzy Relational Clustering2.2.1 PreliminariesTraditionally, hard clustering algorithms (whereuij ?
{0, 1}) such as complete linkage hierarchi-cal agglomerative clustering (Mann and Yarowsky,2003) have been applied to the disambiguationproblem.
In this work, we propose to use fuzzyclustering methods (relaxing the membership con-dition to uij ?
[0, 1]) as a better way of handlinguncertainty in cross document coreference.
First,consider the following motivating example,Example.
The named entity President Bush isextracted from the sentence ?President Bush ad-dressed the nation from the Oval Office Monday.??
Without additional cues, a hard clusteringalgorithm has to arbitrarily assign themention ?President Bush?
to either the NE?George W. Bush?
or ?George H. W.
Bush?.?
A soft clustering algorithm, on the otherhand, can assign equal probability to the twoidentities, indicating low entropy or highuncertainty in the solution.
Additionally, thesoft clustering algorithm can assign lowerprobability to the identity ?Governor JebBush?, reflecting a less likely (though notimpossible) coreference decision.We first formalize the cross document corefer-ence problem as a soft clustering problem, whichminimizes the following objective function:JC(E) =C?i=1N?j=1umijd2(ej ,vi) (1)s.t.C?i=1uij = 1 andN?j=1uij > 0, uij ?
[0, 1]415where vi is a virtual (implicit) prototype of the i-thcluster (ej ,vi ?
D) and m controls the fuzzinessof the solution (m > 1; the solution approacheshard clustering as m approaches 1).
We willfurther explain the generic distance function d :D ?
D ?
R in the next subsection.
The goalof the optimization is to minimize the sum ofdeviations of patterns to the cluster prototypes.The clustering solution is a fuzzy partition P?
={Ci}, where ej ?
Ci if and only if uij > ?.We note from the outset that the optimizationfunctional has the same form as the classicalFuzzy C-Means (FCM) algorithm (Bezdek, 1981),but major differences exist.
FCM, as most ob-ject clustering algorithms, deals with object datarepresented in a vectorial form.
In our case, thedata is purely relational and only the mutual rela-tionships between entities can be determined.
Tobe exact, we can define the similarity/dissimilaritybetween a pair of attributes or relationships ofthe same type l between entities ej and ek ass(l)(ej , ek).
For instance, the similarity betweenthe occupations ?President?
and ?Commander inChief?
can be computed using the JC semanticdistance (Jiang and Conrath, 1997) with WordNet;the similarity of co-occurrence with other peoplecan be measured by the Jaccard coefficient.
In thenext section, we propose to compute the relationstrength r(?, ?)
from the component similaritiesusing aggregation weights learned from trainingdata.
Hence the N chained entities to be clusteredcan be represented as relational data using an n?nmatrix R, where rj,k = r(ej , ek).
The Any Rela-tion Clustering Algorithm (ARCA) (Corsini et al,2005; Cimino et al, 2006) represents relationaldata as object data using their mutual relationstrength and uses FCM for clustering.
We adoptthis approach to transform (objectify) a relationalpattern ej into an N dimensional vector rj (i.e.the j-th row in the matrix R) using a mapping?
: D ?
RN .
In other words, each chained entityis represented as a vector of its relation strengthswith all the entities.
Fuzzy clusters can thenbe obtained by grouping closely related patternsusing object clustering algorithm.Furthermore, it is well known that FCMis a spherical clustering algorithm and thusis not generally applicable to relational datawhich may yield relational clusters of arbitraryand complicated shapes.
Also, the distance inthe transformed space may be non-Euclidean,rendering many clustering algorithms ineffective(many FCM extensions theoretically requirethe underlying distance to satisfy certain metricproperties).
In this work, we propose kernelizedARCA (called KARC) which uses a kernel-induced metric to handle the objectified relationaldata, as we introduce next.2.2.2 Kernelized Fuzzy ClusteringKernelization (Scho?lkopf and Smola, 2002) is amachine learning technique to transform patternsin the data space to a high-dimensional featurespace so that the structure of the data can be moreeasily and adequately discovered.
Specifically, anonlinear transformation ?
maps data in RN toH of possibly infinite dimensions (Hilbert space).The key idea is the kernel trick ?
without explicitlyspecifying ?
and H, the inner product in H canbe computed by evaluating a kernel function K inthe data space, i.e.
< ?(ri),?
(rj) >= K(ri, rj)(one of the most frequently used kernel func-tions is the Gaussian RBF kernel: K(rj , rk) =exp(??
?rj ?
rk?2)).
This technique has beensuccessfully applied to SVMs to classify non-linearly separable data (Vapnik, 1995).
Kerneliza-tion preserves the simplicity in the formalism ofthe underlying clustering algorithm, meanwhile ityields highly nonlinear boundaries so that spheri-cal clustering algorithms can apply (e.g.
(Zhangand Chen, 2003) developed a kernelized objectclustering algorithm based on FCM).Let wi denote the objectified virtual cluster vi,i.e.
wi = ?(vi).
Using the kernel trick, thesquared distance between ?
(rj) and ?
(wi) in thefeature space H can be computed as:??(rj)?
?
(wi)?2H (2)= < ?(rj)?
?(wi),?(rj)?
?
(wi) >= < ?(rj),?
(rj) > ?2 < ?(rj),?
(wi) >+ < ?(wi),?
(wi) >= 2?
2K(rj ,wi) (3)assuming K(r, r) = 1.
The KARC algorithmdefines the generic distance d as d2(ej ,vi) def=??(rj)??
(wi)?2H = ??(?(ej))??(?
(vi))?2H(we also use d2ji as a notational shorthand).Using Lagrange Multiplier as in FCM, the opti-mal solution for Equation (1) is:uij =?????
[C?h=1(d2jid2jh)1/(m?1)]?1, (d2ji 6= 0)1 , (d2ji = 0)(4)416?
(wi) =N?k=1umik?
(rk)N?k=1umik(5)Since ?
is an implicit mapping, Eq.
(5) cannot be explicitly evaluated.
On the other hand,plugging Eq.
(5) into Eq.
(3), d2ji can be explicitlyrepresented by using the kernel matrix,d2ji = 2?
2 ?N?k=1umikK(rj , rk)N?k=1umik(6)With the derivation, the kernelized fuzzy clus-tering algorithm KARC works as follows.
Thechained entities E are first objectified into therelation strength matrix R using SEG, the detailsof which are described in the following section.The Gram matrix K is then computed based onthe relation strength vectors using the kernel func-tion.
For a given number of clusters C, theinitialization step is done by randomly picking Cpatterns as cluster centers, equivalently, C indices{n1, .., nC} are randomly picked from {1, .., N}.D0 is initialized by setting d2ji = 2?
2K(rj , rni).KARC alternately updates the membership matrixU and the kernel distance matrix D until conver-gence or running more than maxIter iterations(Algorithm 1).
Finally, the soft partition is gen-erated based on the membership matrix U , whichis the desired cross document coreference result.Algorithm 1 KARC Alternating OptimizationInput: Gram matrix K; #Clusters C; threshold ?initialize D0t ?
0repeatt ?
t+ 1// 1?
Update membership matrix U t:uij = (d2ji)?
1m?1?Ch=1 (d2jh)?
1m?1// 2?
Update kernel distance matrix Dt:d2ji = 2?
2 ?N?k=1umikKjkN?k=1umikuntil (t > maxIter) or(t > 1 and |U t ?
U t?1| < ?)P?
?
Generate soft partition(U t, ?
)Output: Fuzzy partition P?2.2.3 Cluster ValidationIn the CDC setting, the number of true underlyingidentities may vary depending on the entities?
levelof ambiguity (e.g.
name frequency).
Selecting theoptimal number of clusters is in general a hardresearch question in clustering1.
We adopt theXie-Beni Index (XBI) (Xie and Beni, 1991) as inARCA, which is one of the most popular clustervalidities for fuzzy clustering algorithms.
Xie-Beni Index (XBI) measures the goodness of clus-tering using the ratio of the intra-cluster variationand the inter-cluster separation.
We measure thekernelized XBI (KXBI) in the feature space as,KXBI =C?i=1N?j=1umij ??(rj)?
?
(wi)?2HN ?
min1?i<j?C??(wi)?
?
(wj)?2Hwhere the nominator is readily computed using Dand the inter-cluster separation in the denominatorcan be evaluated using the similar kernel trickabove (details omitted).
Note that KXBI is onlydefined for C > 1.
Thus we pick the C thatcorresponds to the first minimum of KXBI, andthen compare its objective function value JC withthe cluster variance (J1 for C = 1).
The optimalC is chosen from the minimum of the two2.2.3 Specialist Ensemble Learning of RelationStrengths between EntitiesOne remaining element in the overall CDC ap-proach is how the relation strength rj,k betweentwo entities is computed.
In (Cohen et al, 2003),a binary SVM model is trained and its confidencein predicting the non-coreferent class is used asthe distance metric.
In our case of using in-formation extraction results for disambiguation,however, only some of the similarity features arepresent based on the available relationships in twoprofiles.
In this work, we propose to treat eachsimilarity function as a specialist that specializesin computing the similarity of a particular typeof relationship.
Indeed, the similarity functionbetween a pair of attributes or relationships may initself be a sophisticated component algorithm.
Weutilize the specialist ensemble learning framework(Freund et al, 1997) to combine these component1In particular, clustering algorithms that regularize theoptimization with cluster size are not applicable in our case.2In practice, the entities to be disambiguated tend to bedominated by several major identities.
Hence performancegenerally does not vary much in the range of large C values.417similarities into the relation strength for clustering.Here, a specialist is awakened for prediction onlywhen the same type of relationships are present inboth chained entities.
A specialist can choose notto make a prediction if it is not confident enoughfor an instance.
These aspects contrast with thetraditional insomniac ensemble learning methods,where each component learner is always availablefor prediction (Freund et al, 1997).
Also, spe-cialists have different weights (in addition to theirprediction) on the final relation strength, e.g.
amatch in a family relationship is considered moreimportant than in a co-occurrence relationship.Algorithm 2 SEG (Freund et al, 1997)Input: Initial weight distribution p1;learning rate ?
> 0; training set {< st, yt >}1: for t=1 to T do2: Predict using:y?t =?i?Et ptisti?i?Et pti(7)3: Observe the true label yt and incur squareloss L(y?t, yt) = (y?t ?
yt)24: Update weight distribution: for i ?
Etpt+1i =ptie?2?xti(y?t?yt)?j?Etptje?2?xti(y?t?yt)?
?j?Etptj (8)Otherwise: pt+1i = pti5: end forOutput: Model pThe ensemble relation strength model is learnedas follows.
Given training data, the set of chainedentities Etrain is extracted as described earlier.
Fora pair of entities ej and ek, a similarity vectors is computed using the component similarityfunctions for the respective attributes and rela-tionships, and the true label is defined as y =I{ej and ek are coreferent}.
The instances aresubsampled to yield a balanced pairwise train-ing set {< st, yt >}.
We adopt the Special-ist Exponentiated Gradient (SEG) (Freund et al,1997) algorithm to learn the mixing weights of thespecialists?
prediction (Algorithm 2) in an onlinemanner.
In each training iteration, an instance< st, yt > is presented to the learner (with Etdenoting the set of indices of awake specialists inst).
The SEG algorithm first predicts the value y?tbased on the awake specialists?
decisions.
The truevalue yt is then revealed and the learner incurs asquare loss between the predicted and the true val-ues.
The current weight distribution p is updatedto minimize square loss: awake specialists arepromoted or demoted in their weights according tothe difference between the predicted and the truevalue.
The learning iterations can run a few passestill convergence, and the model is learned in lineartime with respect to T and is thus very efficient.
Inprediction time, let E(jk) denote the set of activespecialists for the pair of entities ej and ek, ands(jk) denote the computed similarity vector.
Thepredicted relation strength rj,k is,rj,k =?i?E(jk) pis(jk)i?i?E(jk) pi(9)2.4 RemarksBefore we conclude this section, we make severalcomments on using fuzzy clustering for crossdocument coreference.
First, instead of conduct-ing CDC for all entities concurrently (which canbe computationally intensive with a large cor-pus), chained entities are first distributed into non-overlapping blocks.
Clustering is performed foreach block which is a drastically smaller problemspace, while entities from different blocks areunlikely to be coreferent.
Our CDC system usesphonetic blocking on the full name, so that namevariations arising from translation, transliterationand abbreviation can be accommodated.
Ad-ditional link constraints checking is also imple-mented to improve scalability though these are notthe main focus of the paper.There are several additional benefits in usinga fuzzy clustering method besides the capabil-ity of probabilistic membership assignments inthe CDC solution.
In the clustered web searchcontext, splitting a true identity into two clustersis perceived as a more severe error than puttingirrelevant records in a cluster, as it is more difficultfor the user to collect records in different clusters(to reconstruct the real underlying identity) thanto prune away noisy records.
While there is nouniversal way to handle this with hard clustering,soft clustering algorithms can more easily avoidthe false negatives by allowing records to prob-abilistically appear in different clusters (subjectto the sum of 1) using a more lenient threshold.Also, while there is no real prototypical elementsin relational clustering, soft relational clustering418methods can naturally rank the profiles withina cluster according to their membership levels,which is an additional advantage for enhancinguser consumption of the disambiguation results.3 ExperimentsIn this section, we first formally define the evalu-ation metrics, followed by the introduction to thebenchmark test sets and the system?s performance.3.1 Evaluation MetricsWe benchmarked our method using the standardpurity and inverse purity clustering metrics as inthe WePS evaluation.
Let a set of clusters P ={Ci} denote the system?s partition as aforemen-tioned and a set of categories Q = {Dj} be thegold standard.
The precision of a cluster Ci withrespect to a category Dj is defined as,Precision(Ci,Dj) = |Ci ?
Dj ||Ci|Purity is in turn defined as the weighted averageof the maximum precision achieved by the clusterson one of the categories,Purity(P,Q) =C?i=1|Ci|n maxj Precision(Ci,Dj)where n = ?
|Ci|.
Hence purity penalizes puttingnoise chained entities in a cluster.
Trivially, themaximum purity (i.e.
1) can be achieved bymaking one cluster per chained entity (referred toas the one-in-one baseline).
Reversing the role ofclusters and categories, Inverse purity(P,Q) def=Purity(Q,P).
Inverse Purity penalizes splittingchained entities belonging to the same categoryinto different clusters.
The maximum inversepurity can be similarly achieved by putting allentities into one cluster (all-in-one baseline).Purity and inverse purity are similar to theprecision and recall measures commonly used inIR.
The F score, F = 1/(?
1Purity + (1 ??)
1InversePurity ), is used in performance evalua-tion.
?
= 0.2 is used to give more weight toinverse purity, with the justification for the webperson search mentioned earlier.3.2 DatasetWe evaluate our methods using the benchmarktest collection from the ACL SemEval-2007 webperson search task (WePS) (Artiles et al, 2007).The test collection consists of three sets of 10different names, sampled from ambiguous namesfrom English Wikipedia (famous people), partici-pants of the ACL 2006 conference (computer sci-entists) and common names from the US Censusdata, respectively.
For each name, the top 100documents retrieved from the Yahoo!
Search APIwere annotated, yielding on average 45 real worldidentities per set and about 3k documents in total.As we note in the beginning of Section 2, thehuman markup for the entities corresponding tothe search queries is on the document level.
Theprofile-based CDC approach, however, is to mergethe mention-level entities.
In our evaluation, weadopt the document label (and the person searchquery) to annotate the entity profiles that corre-sponds to the person name search query.
Despitethe difference, the results of the one-in-one andall-in-one baselines are almost identical to thosereported in the WePS evaluation (F = 0.52, 0.58respectively).
Hence the performance reportedhere is comparable to the official evaluation results(Artiles et al, 2007).3.3 Information Extraction and SimilaritiesWe use an information extraction tool AeroText(Taylor, 2004) to construct the entity profiles.AeroText extracts two types of information foran entity.
First, the attribute information aboutthe person named entity includes first/middle/lastnames, gender, mention, etc.
In addition,AeroText extracts relationship informationbetween named entities, such as Family, List,Employment, Ownership, Citizen-Resident-Religion-Ethnicity and so on, as specified in theACE evaluation.
AeroText resolves the referencesof entities within a document and produces theentity profiles, used as input to the CDC system.Note that alternative IE or WDC tools, as wellas additional attributes or relationships, can bereadily used in the CDC methods we proposed.A suite of similarity functions is designed todetermine if the attributes relationships in a pairof entity profiles match or not:Text similarity.
To decide whether two namesin the co-occurrence or family relationship match,we use the SoftTFIDF measure (Cohen et al,2003), which is a hybrid matching scheme thatcombines the token-based TFIDF with the Jaro-Winkler string distance metric.
This permits in-exact matching of named entities due to name419variations, typos, etc.Semantic similarity.
Text or syntactic similarityis not always sufficient for matching relationships.WordNet and the information theoretic semanticdistance (Jiang and Conrath, 1997) are used tomeasure the semantic similarity between conceptsin relationships such as mention, employment,ownership, etc.Other rule-based similarity.
Several othercases require special treatment.
For example,the employment relationships of Senator andD-N.Y. should match based on domain knowledge.Also, we design dictionary-based similarityfunctions to handle nicknames (Bill and William),acronyms (COLING for International Conferenceon Computational Linguistics), and geo-locations.3.4 Evaluation ResultsFrom the WePS training data, we generated atraining set of around 32k pairwise instances aspreviously stated in Section 2.3.
We then usedthe SEG algorithm to learn the weight distributionmodel.
We tuned the parameters in the KARCalgorithm using the training set with discrete gridsearch and chose m = 1.6 and ?
= 0.3.
The RBFkernel (Gaussian) is used with ?
= 0.015.Table 1: Cross document coreference performance(I.
Purity denotes inverse purity).Method Purity I.
Purity FKARC-S 0.657 0.795 0.740KARC-H 0.662 0.762 0.710FRC 0.484 0.840 0.697One-in-one 1.000 0.482 0.524All-in-one 0.279 1.000 0.571The macro-averaged cross document corefer-ence on the WePS test sets are reported in Table1.
The F score of our CDC system (KARC-S) is 0.740, comparable to the test results of thefirst tier systems in the official evaluation.
Thetwo baselines are also included.
Since differentfeature sets, NLP tools, etc are used in differentbenchmarked systems, we are also interested incomparing the proposed algorithm with differ-ent soft relational clustering variants.
First, we?harden?
the fuzzy partition produced by KARCby allowing an entity to appear in the clusterwith highest membership value (KARC-H).
Purityimproves because of the removal of noise entities,though at the sacrifice of inverse purity and theTable 2: Cross document coreference performanceon subsets (I.
Purity denotes inverse purity).Test set Identity Purity I.
Purity FWikipedia 56.5 0.666 0.752 0.717ACL-06 31.0 0.783 0.771 0.773US Census 50.3 0.554 0.889 0.754F score deteriorates.
We also implement a pop-ular fuzzy relational clustering algorithm calledFRC (Dave and Sen, 2002), whose optimizationfunctional directly minimizes with respect to therelation matrix.
With the same feature sets anddistance function, KARC-S outperforms FRC in Fscore by about 5%.
Because the test set is very am-biguous (on average only two documents per realworld entity), the baselines have relatively high Fscore as observed in the WePS evaluation (Artileset al, 2007).
Table 2 further analyzes KARC-S?s result on the three subsets Wikipedia, ACL06and US Census.
The F score is higher in theless ambiguous (the average number of identities)dataset and lower in the more ambiguous one, witha spread of 6%.We study how the cross document coreferenceperformance changes as we vary the fuzziness inthe solution (controlled by m).
In Figure 1, asm increases from 1.4 to 1.9, purity improves by10% to 0.67, which indicates that more correctcoreference decisions (true positives) can be madein a softer configuration.
The complimentary istrue for inverse purity, though to a lesser extent.In this case, more false negatives, correspondingto the entities of different coreferents incorrectly0.50.550.60.650.70.750.80.851.4  1.5  1.6  1.7  1.8  1.9mKARC performance with different mpurityinverse purityFFigure 1: Purity, inverse purity and F score withdifferent fuzzifiers m.4200.60.650.70.750.80.850.1  0.2  0.3  0.4  0.5  0.6?KARC performance with different ?purityinverse purityFFigure 2: CDC performance with different ?.linked, are made in a softer partition.
The Fscore peaks at 0.74 (m = 1.6) and then slightlydecreases, as the gain in purity is outweighed bythe loss in inverse purity.Figure 2 evaluates the impact of the differentsettings of ?
(the threshold of including a chainedentity in the fuzzy cluster) on the coreferenceperformance.
We observe that as we increase?, purity improves indicating less ?noise?
entitiesare included in the solution.
On the other hand,inverse purity decreases meaning more coreferententities are not linked due to the stricter threshold.Overall, the changes in the two metrics offset eachother and the F score is relatively stable across abroad range of ?
settings.4 Related WorkThe original work in (Bagga and Baldwin, 1998)proposed a CDC system by first performing WDCand then disambiguating based on the summarysentences of the chains.
This is similar to ours inthat mentions rather than documents are clustered,leveraging the advances in state-of-the-art WDCmethods developed in NLP, e.g.
(Ng and Cardie,2001; Yang et al, 2008).
On the other hand, ourwork goes beyond the simple bag-of-word featuresand vector space model in (Bagga and Baldwin,1998; Gooi and Allan, 2004) with IE results.
(Wanet al, 2005) describes a person resolution systemWebHawk that clusters web pages using someextracted personal information including personname, title, organization, email and phone number,besides lexical features.
(Mann and Yarowsky,2003) extracts biographical information, which isrelatively scarce in web data, for disambiguation.With the support of state-of-the-art informationextraction tools, the profiles of entities in this workcovers a broader range of relational information.
(Niu et al, 2004) also leveraged IE support, buttheir approach was evaluated on a small artificialcorpus.
Also, the pairwise distance model isinsomniac (i.e.
all similarity specialists are awakefor prediction) and our work extends this with aspecialist learning framework.Prior work has largely relied on using hier-archical clustering methods for CDC, with thethreshold for stopping the merging set using thetraining data, e.g.
(Mann and Yarowsky, 2003;Chen and Martin, 2007; Baron and Freedman,2008).
The fuzzy relational clustering methodproposed in this paper we believe better addressesthe uncertainty aspect of the CDC problem.There are also orthogonal research directionsfor the CDC problem.
(Li et al, 2004) solved theCDC problem by adopting a probabilistic view onhow documents are generated and how names aresprinkled into them.
(Bunescu and Pasca, 2006)showed that external information from Wikipediacan improve the disambiguation performance.5 ConclusionsWe have presented a profile-based Cross Docu-ment Coreference (CDC) approach based on anovel fuzzy relational clustering algorithm KARC.In contrast to traditional hard clustering methods,KARC produces fuzzy sets of identities whichbetter reflect the intrinsic uncertainty of the CDCproblem.
Kernelization, as used in KARC, enablesthe optimization of clustering that is sphericalin nature to apply to relational data that tend tohave complicated shapes.
KARC partitions namedentities based on their profiles constructed by aninformation extraction tool.
To match the pro-files, a specialist ensemble algorithm predicts thepairwise distance by aggregating the similarities ofthe attributes and relationships in the profiles.
Weevaluated the proposed methods with experimentson a large benchmark collection and demonstratethat the proposed methods compare favorably withthe top runs in the SemEval evaluation.The focus of this work is on the novel learningand clustering methods for coreference.
Futureresearch directions include developing rich featuresets and using corpus level or external informa-tion.
We believe that such efforts can further im-prove cross document coreference performance.421ReferencesJavier Artiles, Julio Gonzalo, and Satoshi Sekine.2007.
The SemEval-2007 WePS evaluation:Establishing a benchmark for the web people searchtask.
In Proceedings of the 4th InternationalWorkshop on Semantic Evaluations (SemEval-2007), pages 64?69.Amit Bagga and Breck Baldwin.
1998.
Entity-basedcross-document coreferencing using the vectorspace model.
In Proceedings of 36th InternationalConference On Computational Linguistics (ACL)and 17th international conference on Computationallinguistics (COLING), pages 79?85.Alex Baron and Marjorie Freedman.
2008.
Whois who and what is what: Experiments in cross-document co-reference.
In Proceedings of the2008 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 274?283.J.
C. Bezdek.
1981.
Pattern Recognition with FuzzyObjective Function Algoritms.
Plenum Press, NY.Razvan Bunescu and Marius Pasca.
2006.
Usingencyclopedic knowledge for named entity disam-biguation.
In Proceedings of the 11th Conferenceof the European Chapter of the Association forComputational Linguistics (EACL), pages 9?16.Ying Chen and James Martin.
2007.
Towardsrobust unsupervised personal name disambiguation.In Proc.
of 2007 Joint Conference on EmpiricalMethods in Natural Language Processing andComputational Natural Language Learning.Mario G. C. A. Cimino, Beatrice Lazzerini, andFrancesco Marcelloni.
2006.
A novel approachto fuzzy clustering based on a dissimilarity relationextracted from data using a TS system.
PatternRecognition, 39(11):2077?2091.William W. Cohen, Pradeep Ravikumar, andStephen E. Fienberg.
2003.
A comparison ofstring distance metrics for name-matching tasks.In Proceedings of IJCAI Workshop on InformationIntegration on the Web.Paolo Corsini, Beatrice Lazzerini, and FrancescoMarcelloni.
2005.
A new fuzzy relational clusteringalgorithm based on the fuzzy c-means algorithm.Soft Computing, 9(6):439 ?
447.Rajesh N. Dave and Sumit Sen. 2002.
Robust fuzzyclustering of relational data.
IEEE Transactions onFuzzy Systems, 10(6):713?727.Yoav Freund, Robert E. Schapire, Yoram Singer, andManfred K. Warmuth.
1997.
Using and combiningpredictors that specialize.
In Proceedings of thetwenty-ninth annual ACM symposium on Theory ofcomputing (STOC), pages 334?343.Chung H. Gooi and James Allan.
2004.
Cross-document coreference on a large scale corpus.
InProceedings of the Human Language TechnologyConference of the North American Chapter ofthe Association for Computational Linguistics(NAACL), pages 9?16.Jay J. Jiang and David W. Conrath.
1997.Semantic similarity based on corpus statistics andlexical taxonomy.
In Proceedings of InternationalConference Research on Computational Linguistics.Xin Li, Paul Morie, and Dan Roth.
2004.
Robustreading: Identification and tracing of ambiguousnames.
In Proceedings of the Human LanguageTechnology Conference and the North AmericanChapter of the Association for ComputationalLinguistics (HLT-NAACL), pages 17?24.Gideon S. Mann and David Yarowsky.
2003.Unsupervised personal name disambiguation.
InConference on Computational Natural LanguageLearning (CoNLL), pages 33?40.Vincent Ng and Claire Cardie.
2001.
Improving ma-chine learning approaches to coreference resolution.In Proceedings of the 40th Annual Meeting of theAssociation for Computational Linguistics (ACL),pages 104?111.Cheng Niu, Wei Li, and Rohini K. Srihari.
2004.Weakly supervised learning for cross-documentperson name disambiguation supported by infor-mation extraction.
In Proceedings of the 42ndAnnual Meeting on Association for ComputationalLinguistics (ACL), pages 597?604.Bernhard Scho?lkopf and Alex Smola.
2002.
Learningwith Kernels.
MIT Press, Cambridge, MA.Sarah M. Taylor.
2004.
Information extraction tools:Deciphering human language.
IT Professional,6(6):28 ?
34.Vladimir Vapnik.
1995.
The Nature of StatisticalLearning Theory.
Springer-Verlag New York.Xiaojun Wan, Jianfeng Gao, Mu Li, and BinggongDing.
2005.
Person resolution in person searchresults: WebHawk.
In Proceedings of the 14thACM international conference on Information andknowledge management (CIKM), pages 163?170.Xuanli Lisa Xie and Gerardo Beni.
1991.
A validitymeasure for fuzzy clustering.
IEEE Transactionson Pattern Analysis and Machine Intelligence,13(8):841 ?
847.Xiaofeng Yang, Jian Su, Jun Lang, Chew L. Tan,Ting Liu, and Sheng Li.
2008.
An entity-mention model for coreference resolution withinductive logic programming.
In Proceedings ofthe 46th Annual Meeting of the Association forComputational Linguistics (ACL), pages 843?851.Dao-Qiang Zhang and Song-Can Chen.
2003.Clustering incomplete data using kernel-based fuzzyc-means algorithm.
Neural Processing Letters,18(3):155 ?
162.422
