Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 665?670,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsFilling Knowledge Base Gaps for Distant Supervisionof Relation ExtractionWei Xu+ Raphael Hoffmann?
Le Zhao#,* Ralph Grishman++New York University, New York, NY, USA{xuwei, grishman}@cs.nyu.edu?University of Washington, Seattle, WA, USAraphaelh@cs.washington.edu#Google Inc., Mountain View, CA, USAlezhao@google.comAbstractDistant supervision has attracted recent in-terest for training information extractionsystems because it does not require anyhuman annotation but rather employs ex-isting knowledge bases to heuristically la-bel a training corpus.
However, previouswork has failed to address the problemof false negative training examples misla-beled due to the incompleteness of knowl-edge bases.
To tackle this problem, wepropose a simple yet novel framework thatcombines a passage retrieval model usingcoarse features into a state-of-the-art rela-tion extractor using multi-instance learn-ing with fine features.
We adapt the in-formation retrieval technique of pseudo-relevance feedback to expand knowledgebases, assuming entity pairs in top-rankedpassages are more likely to express a rela-tion.
Our proposed technique significantlyimproves the quality of distantly super-vised relation extraction, boosting recallfrom 47.7% to 61.2% with a consistentlyhigh level of precision of around 93% inthe experiments.1 IntroductionA recent approach for training information ex-traction systems is distant supervision, which ex-ploits existing knowledge bases instead of anno-tated texts as the source of supervision (Cravenand Kumlien, 1999; Mintz et al, 2009; Nguyenand Moschitti, 2011).
To combat the noisy train-ing data produced by heuristic labeling in distantsupervision, researchers (Bunescu and Mooney,2007; Riedel et al, 2010; Hoffmann et al, 2011;Surdeanu et al, 2012) exploited multi-instance*This work was done while Le Zhao was at CarnegieMellon University.learning models.
Only a few studies have directlyexamined the influence of the quality of the train-ing data and attempted to enhance it (Sun et al,2011; Wang et al, 2011; Takamatsu et al, 2012).However, their methods are handicapped by thebuilt-in assumption that a sentence does not ex-press a relation unless it mentions two entitieswhich participate in the relation in the knowledgebase, leading to false negatives.alignedmentionstruementions 5.5% 2.7% 1.7% false  negatives falsepositivesFigure 1: Noisy training data in distant supervi-sionIn reality, knowledge bases are often incom-plete, giving rise to numerous false negatives inthe training data.
We sampled 1834 sentences thatcontain two entities in the New York Times 2006corpus and manually evaluated whether they ex-press any of a set of 50 common Freebase1 rela-tions.
As shown in Figure 1, of the 133 (7.3%)sentences that truly express one of these relations,only 32 (1.7%) are covered by Freebase, leaving101 (5.5%) false negatives.
Even for one of themost complete relations in Freebase, Employee-of(with more than 100,000 entity pairs), 6 out of 27sentences with the pattern ?PERSON executive ofORGANIZATION?
contain a fact that is not in-cluded in Freebase and are thus mislabeled as neg-ative.
These mislabelings dilute the discriminativecapability of useful features and confuse the mod-els.
In this paper, we will show how reducing thissource of noise can significantly improve the per-formance of distant supervision.
In fact, our sys-tem corrects the relation labels of the above 6 sen-tences before training the relation extractor.1http://www.freebase.com665D ocuments  KnowledgeBaseRelationExtractorPassageRetriever??
?Pseudo - relevantRelation Instances??
?Figure 2: Overall system architecture: The system(1) matches relation instances to sentences and (2)learns a passage retrieval model to (3) provide rel-evance feedback on sentences; Relevant sentences(4) yield new relation instances which are addedto the knowledge base; Finally, instances are again(5) matched to sentences to (6) create training datafor relation extraction.Encouraged by the recent success of simplemethods for coreference resolution (Raghunathanet al, 2010) and inspired by pseudo-relevancefeedback (Xu and Croft, 1996; Lavrenko andCroft, 2001; Matveeva et al, 2006; Cao et al,2008) in the field of information retrieval, whichexpands or reformulates query terms based onthe highest ranked documents of an initial query,we propose to increase the quality and quantityof training data generated by distant supervisionfor information extraction task using pseudo feed-back.
As shown in Figure 2, we expand an orig-inal knowledge base with possibly missing rela-tion instances with information from the highestranked sentences returned by a passage retrievalmodel (Xu et al, 2011) trained on the same data.We use coarse features for our passage retrievalmodel to aggressively expand the knowledge basefor maximum recall; at the same time, we exploita multi-instance learning model with fine featuresfor relation extraction to handle the newly intro-duced false positives and maintain high precision.Similar to iterative bootstrapping tech-niques (Yangarber, 2001), this mechanism usesthe outputs of the first trained model to expandtraining data for the second model, but unlikebootstrapping it does not require iteration andavoids the problem of semantic drift.
We furthernote that iterative bootstrapping over a singledistant supervision system is difficult, becausestate-of-the-art systems (Surdeanu et al, 2012;Hoffmann et al, 2011; Riedel et al, 2010; Mintzet al, 2009), detect only few false negatives in thetraining data due to their high-precision low-recallfeatures, which were originally proposed by Mintzet al (2009).
We present a reliable and novel wayto address these issues and achieve significantimprovement over the MULTIR system (Hoff-mann et al, 2011), increasing recall from 47.7%to 61.2% at comparable precision.
The key to thissuccess is the combination of two different viewsas in co-training (Blum and Mitchell, 1998):an information extraction technique with finefeatures for high precision and an informationretrieval technique with coarse features for highrecall.
Our work is developed in parallel withMin et al (2013), who take a very differentapproach by adding additional latent variables toa multi-instance multi-label model (Surdeanu etal., 2012) to solve this same problem.2 System DetailsIn this section, we first introduce some formal no-tations then describe in detail each component ofthe proposed system in Figure 2.2.1 DefinitionsA relation instance is an expression r(e1, e2)where r is a binary relation, and e1 and e2 aretwo entities having such a relation, for exampleCEO-of(Tim Cook, Apple).
The knowledge-baseddistant supervised learning problem takes as input(1) ?, a training corpus, (2) E, a set of entitiesmentioned in that corpus, (3) R, a set of relationnames, and (4) ?, a set of ground facts of relationsin R. To generate our training data, we further as-sume (5) T , a set of entity types, as well as typesignature r(E1, E2) for relations.We define the positive data set POS(r) to bethe set of sentences in which any related pairof entities of relation r (according to the knowl-edge base) is mentioned.
The negative data setRAW (r) is the rest of the training data, whichcontain two entities of the required types in theknowledge base, e.g.
one person and one or-ganization for the CEO-of relation in Freebase.Another negative data set with more conservativesense NEG(r) is defined as the set of sentenceswhich contain the primary entity e1 (e.g.
personin any CEO-of relation in the knowledge base) andany secondary entity e2 of required type (e.g.
or-ganization for the CEO-of relation) but the relationdoes not hold for this pair of entities in the knowl-edge base.6662.2 Distantly Supervised Passage RetrievalWe extend the learning-to-rank techniques (Liu,2011) to distant supervision setting (Xu et al,2011) to create a robust passage retrieval system.While relation extraction systems exploit rich andcomplex features that are necessary to extract theexact relation (Mintz et al, 2009; Riedel et al,2010; Hoffmann et al, 2011), passage retrievalcomponents use coarse features in order to providedifferent and complementary feedback to informa-tion extraction models.We exploit two types of lexical features: Bag-Of-Words and Word-Position.
The two types ofsimple binary features are shown in the followingexample:Sentence: Apple founder Steve Jobs died.Target (Primary) entity: Steve JobsBag-Of-Word features: ?apple?
?founder?
?died?
?.
?Word-Position features: ?apple:-2?
?founder:-1??died:+1?
?.
:+2?For each relation r, we assume each sentencehas a binary relevance label to form distantly su-pervised training data: sentences in POS(r) arerelevant and sentences in NEG(r) are irrelevant.As a pointwise learning-to-rank approach (Nallap-ati, 2004), the probabilities of relevance estimatedby SVMs (Platt and others, 1999) are used forranking all the sentences in the original trainingcorpus for each relation respectively.
We use Lib-SVM 2 (Chang and Lin, 2011) in our implementa-tion.2.3 Psuedo-relevance Relation FeedbackIn the field of information retrieval, pseudo-relevance feedback assumes that the top-rankeddocuments from an initial retrieval are likely rel-evant, and extracts relevant terms to expand theoriginal query (Xu and Croft, 1996; Lavrenko andCroft, 2001; Cao et al, 2008).
Analogously, ourassumption is that entity pairs that appear in morerelevant and more sentences are more likely toexpress the relation, and can be used to expandknowledge base and reduce false negative noise inthe training data for information extraction.
Weidentify the most likely relevant entity pairs as fol-lows:2http://www.csie.ntu.edu.tw/?cjlin/libsvminitialize ??
??
?for each relation type r ?
R dolearn a passage (sentence) retrieval model L(r)using coarse features and POS(r)?NEG(r)as training datascore the sentences in the RAW (r) by L(r)score the entity pairs according to the scoresof sentences they are involved inselect the top ranked pairs of entities, then addthe relation r to their label in ?
?end forWe select the entity pairs whose average scoreof the sentences they are involved in is greaterthan p, where p is a parameter tuned on develop-ment data.3 The relation extraction model is thentrained using (?, E,R,??)
with a more completedatabase than the original knowledge base ?.2.4 Distantly Supervised Relation ExtractionWe use a state-of-the-art open-source system,MULTIR (Hoffmann et al, 2011), as the rela-tion extraction component.
MULTIR is basedon multi-instance learning, which assumes thatat least one sentence of those matching a givenentity-pair contains the relation of interest (Riedelet al, 2010) in the given knowledge base to tol-erate false positive noise in the training data andsuperior than previous models (Riedel et al, 2010;Mintz et al, 2009) by allowing overlapping rela-tions.
MULTIR uses features which are based onMintz et al (2009) and consist of conjunctions ofnamed entity tags, syntactic dependency paths be-tween arguments, and lexical information.3 ExperimentsFor evaluating extraction accuracy, we follow theexperimental setup of Hoffmann et al (2011), anduse their implementation of MULTIR4 with 50training iterations as our baseline.
Our completesystem, which we call IRMIE, combines our pas-sage retrieval component with MULTIR.
We usethe same datasets as in Hoffmann et al (2011) andRiedel et al (2010), which include 3-years of NewYork Times articles aligned with Freebase.
Thesentential extraction evaluation is performed ona small amount of manually annotated sentences,sampled from the union of matched sentences and3We found p = 0.5 to work well in practice.4http://homes.cs.washington.edu/?raphaelh/mr/667Test Data Set Original Test Set Corrected Test SetP?
R?
F?
?F?
P?
R?
F?
?F?MULTIR 80.0 44.6 62.3 92.7 47.7 70.2IRMIE 84.6 56.1 70.3 +8.0 92.6 61.2 76.9 +6.7MULTIRLEX 91.8 43.0 67.4 79.6 57.0 68.3IRMIELEX 89.2 52.5 70.9 +3.5 78.0 69.2 73.6 +5.3Table 1: Overall sentential extraction performance evaluated on the original test set of Hoffmann etal.
(2011) and our corrected test set: Our proposed relevance feedback technique yields a substantialincrease in recall.system predictions.
We define Se as the sentenceswhere some system extracted a relation and SFas the sentences that match the arguments of afact in ?.
The sentential precision and recall iscomputed on a randomly sampled set of sentencesfrom Se?SF , in which each sentence is manuallylabeled whether it expresses any relation in R.Figure 3 shows the precision/recall curves forMULTIR with and without pseudo-relevance feed-back computed on the test dataset of 1000 sen-tence used by Hoffmann et al (2011).
With thepseudo-relevance feedback from passage retrieval,IRMIE achieves significantly higher recall at aconsistently high level of precision.
At the highestrecall point, IRMIE reaches 78.5% precision and59.2% recall, for an F1 score of 68.9%.Because the two types of lexical features used inour passage retrieval models are not used in MUL-TIR, we created another baseline MULTIRLEXby adding these features into MULTIR in orderto rule out the improvement from additional infor-mation.
Note that the sentences are sampled fromthe union of Freebase matches and sentences fromwhich some systems in Hoffmann et al (2011) ex-tracted a relation.
It underestimates the improve-ments of the newly developed systems in this pa-per.
We therefore also created a new test set of1000 sentences by sampling from the union ofFreebase matches and sentences where MULTIR-LEX or IRMIELEX extracted a relation.
Table 1shows the overall precision and recall computedagainst these two test datasets, with and withoutadding lexical features into multi-instance learn-ing models.
The performance improvement by us-ing pseudo-feedback is significant (p < 0.05) inMcNemar?s test for both datasets.4 Conclusion and PerspectivesThis paper proposes a novel approach to addressan overlooked problem in distant supervision: theknowledge base is often incomplete causing nu-RecallPrecision0.0 0.1 0.2 0.3 0.4 0.5 0.60.50.60.70.80.91.0IRMIEMULTIRFigure 3: Sentential extraction: precision/recallcurves using exact same training and test data,features and system settings as in Hoffmann etal.
(2011).merous false negatives in the training data.
Itgreatly improves a state-of-the-art multi-instancelearning model by correcting the most likely falsenegatives in the training data based on the rankingof a passage retrieval model.In the future, we would like to more tightly inte-grate a coarser featured estimator of sentential rel-evance and a finer featured relation extractor, suchthat a single joint-model can be learned.AcknowledgmentsSupported in part by NSF grant IIS-1018317,the Air Force Research Laboratory (AFRL)under prime contract number FA8750-09-C-0181 and the Intelligence Advanced ResearchProjects Activity (IARPA) via Department of In-terior National Business Center contract numberD11PC20154.
The U.S. Government is authorizedto reproduce and distribute reprints for Govern-mental purposes notwithstanding any copyrightannotation thereon.
Disclaimer: The views andconclusions contained herein are those of the au-thors and should not be interpreted as necessarilyrepresenting the official policies or endorsements,either expressed or implied, of AFRL, IARPA,DoI/NBC, or the U.S. Government.668ReferencesAvrim Blum and Tom M. Mitchell.
1998.
Combin-ing labeled and unlabeled sata with co-training.
InProceedings of the 11th Annual Conference on Com-putational Learning Theory (COLT), pages 92?100.Razvan C. Bunescu and Raymond J. Mooney.
2007.Learning to extract relations from the web usingminimal supervision.
In Proceedings of the 45th An-nual Meeting of the Association for ComputationalLinguistics (ACL).Guihong Cao, Jian-Yun Nie, Jianfeng Gao, andStephen Robertson.
2008.
Selecting good expan-sion terms for pseudo-relevance feedback.
In Pro-ceedigns of the 31st Annual International ACM SI-GIR Conference on Research and Development inInformation Retrieval (SIGIR), pages 243?250.Chih-Chung Chang and Chih-Jen Lin.
2011.
Lib-svm: A library for support vector machines.
ACMTransactions on Intelligent Systems and Technology,2(3):27.Mark Craven and Johan Kumlien.
1999.
Constructingbiological knowledge bases by extracting informa-tion from text sources.
In Proceedings of the Sev-enth International Conference on Intelligent Systemsfor Molecular Biology (ISMB), pages 77?86.Raphael Hoffmann, Congle Zhang, Xiao Ling,Luke S. Zettlemoyer, and Daniel S. Weld.
2011.Knowledge-based weak supervision for informationextraction of overlapping relations.
In Proceedingsof the 49th Annual Meeting of the Association forComputational Linguistics (ACL), pages 541?550.Victor Lavrenko and W. Bruce Croft.
2001.Relevance-based language models.
In Proceedingsof the 24th Annual International ACM SIGIR Con-ference on Research and Development in Informa-tion Retrieval (SIGIR), pages 120?127.Tie-Yan Liu.
2011.
Learning to Rank for InformationRetrieval.
Springer-Verlag Berlin Heidelberg.Irina Matveeva, Chris Burges, Timo Burkard, AndyLaucius, and Leon Wong.
2006.
High accuracy re-trieval with multiple nested ranker.
In Proceedingsof the 29th Annual International ACM SIGIR Con-ference on Research and Development in Informa-tion Retrieval (SIGIR), pages 437?444.Bonan Min, Ralph Grishman, Li Wan, Chang Wang,and David Gondek.
2013.
Distant supervision forrelation extraction with an incomplete knowledgebase.
In Proceedings of the Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics (NAACL 2013).Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-rafsky.
2009.
Distant supervision for relation ex-traction without labeled data.
In Proceedigns of the47th Annual Meeting of the Association for Compu-tational Linguistics and the 4th International JointConference on Natural Language Processing (ACL),pages 1003?1011.Ramesh Nallapati.
2004.
Discriminative models forinformation retrieval.
In Proceedigns of the 27th An-nual International ACM SIGIR Conference on Re-search and Development in Information Retrieval(SIGIR), pages 64?71.Truc Vien T Nguyen and Alessandro Moschitti.
2011.End-to-end relation extraction using distant super-vision from external semantic repositories.
In Pro-ceedings of the 49th Annual Meeting of the Associ-ation for Computational Linguistics: Human Lan-guage Technologies, pages 277?282.John Platt et al 1999.
Probabilistic outputs for sup-port vector machines and comparisons to regular-ized likelihood methods.
Advances in Large MarginClassifiers, 10(3):61?74.Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-garajan, Nathanael Chambers, Mihai Surdeanu, DanJurafsky, and Christopher Manning.
2010.
A multi-pass sieve for coreference resolution.
In Proceed-ings of the 2010 Conference on Empirical Methodsin Natural Language Processing, pages 492?501.Association for Computational Linguistics.Sebastian Riedel, Limin Yao, and Andrew McCallum.2010.
Modeling relations and their mentions with-out labeled text.
In Proceedigns of the EuropeanConference on Machine Learning and Principlesand Practice of Knowledge Discovery in Databases(ECML/PKDD), pages 148?163.Ang Sun, Ralph Grishman, Wei Xu, and Bonan Min.2011.
New york university 2011 system for kbp slotfilling.
In Text Analysis Conference 2011 Workshop.Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,and Christopher D. Manning.
2012.
Multi-instancemulti-label learning for relation extraction.
In Pro-ceedings of the 50th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL), pages455?465.Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.2012.
Reducing wrong labels in distant supervi-sion for relation extraction.
In Proceedings of the50th Annual Meeting of the Association for Compu-tational Linguistics (ACL), pages 721?729.Chang Wang, James Fan, Aditya Kalyanpur, and DavidGondek.
2011.
Relation extraction with relationtopics.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing(EMNLP), pages 1426?1436.Jinxi Xu and W. Bruce Croft.
1996.
Query expansionusing local and global document analysis.
In Hans-Peter Frei, Donna Harman, Peter Scha?uble, and RossWilkinson, editors, Proceedings of the 19th AnnualInternational ACM SIGIR Conference on Researchand Development in Information Retrieval (SIGIR),pages 4?11.
ACM.669Wei Xu, Ralph Grishman, and Le Zhao.
2011.
Passageretrieval for information extraction using distant su-pervision.
In Proceedings of the International JointConference on Natural Language Processing (IJC-NLP), pages 1046?1054.Roman Yangarber.
2001.
Scenario customization forinformation extraction.
Ph.D. thesis, Department ofComputer Science, Graduate School of Arts and Sci-ence, New York University.670
