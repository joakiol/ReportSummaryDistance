Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 736?741,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsDiathesis alternation approximation for verb clusteringLin SunGreedy Intelligence LtdHangzhou, Chinalin.sun@greedyint.comDiana McCarthy and Anna KorhonenDTAL and Computer LaboratoryUniversity of CambridgeCambridge, UKdiana@dianamccarthy.co.ukalk23@cam.ac.ukAbstractAlthough diathesis alternations have beenused as features for manual verb clas-sification, and there is recent work onincorporating such features in computa-tional models of human language acquisi-tion, work on large scale verb classifica-tion has yet to examine the potential forusing diathesis alternations as input fea-tures to the clustering process.
This pa-per proposes a method for approximatingdiathesis alternation behaviour in corpusdata and shows, using a state-of-the-artverb clustering system, that features basedon alternation approximation outperformthose based on independent subcategoriza-tion frames.
Our alternation-based ap-proach is particularly adept at leveraginginformation from less frequent data.1 IntroductionDiathesis alternations (DAs) are regular alterna-tions of the syntactic expression of verbal argu-ments, sometimes accompanied by a change inmeaning.
For example, The man broke the win-dow ?
The window broke.
The syntactic phe-nomena are triggered by the underlying semanticsof the participating verbs.
Levin (1993)?s seminalbook provides a manual inventory both of DAs andverb classes where membership is determined ac-cording to participation in these alternations.
Forexample, most of the COOK verbs (e.g.
bake,cook, fry .
.
. )
can all take various DAs, such asthe causative alternation, middle alternation andinstrument subject alternation.In computational linguistics, work inspired byLevin?s classification has exploited the link be-tween syntax and semantics for producing clas-sifications of verbs.
Such classifications are use-ful for a wide variety of purposes such as se-mantic role labelling (Gildea and Jurafsky, 2002),predicting unseen syntax (Parisien and Steven-son, 2010), argument zoning (Guo et al 2011)and metaphor identification (Shutova et al 2010).While Levin?s classification can be extended man-ually (Kipper-Schuler, 2005), a large body of re-search has developed methods for automatic verbclassification since such methods can be appliedeasily to other domains and languages.Existing work on automatic classification relieslargely on syntactic features such as subcatego-rization frames (SCF)s (Schulte im Walde, 2006;Sun and Korhonen, 2011; Vlachos et al 2009;Brew and Schulte im Walde, 2002).
There has alsobeen some success incorporating selectional pref-erences (Sun and Korhonen, 2009).Few have attempted to use, or approximate,diathesis features directly for verb classificationalthough manual classifications have relied onthem heavily, and there has been related work onidentifying the DAs themselves automatically us-ing SCF and semantic information (Resnik, 1993;McCarthy and Korhonen, 1998; Lapata, 1999;McCarthy, 2000; Tsang and Stevenson, 2004).Exceptions to this include Merlo and Stevenson(2001), Joanis et al(2008) and Parisien andStevenson (2010, 2011).
Merlo and Stevenson(2001) used cues such as passive voice, animacyand syntactic frames coupled with the overlapof lexical fillers between the alternating slots topredict a 3-way classification (unergative, unac-cusative and object-drop).
Joanis et al(2008)used similar features to classify verbs on a muchlarger scale.
They classify up to 496 verbs us-ing 11 different classifications each having be-tween 2 and 14 classes.
Parisien and Steven-son (2010, 2011) used hierarchical Bayesian mod-els on slot frequency data obtained from child-directed speech parsed with a dependency parserto model acquisition of SCF, alternations and ul-timately verb classes which provided predictionsfor unseen syntactic behaviour of class members.736Frame Example sentence FreqNP+PPon Jessica sprayed paint on the wall 40NP+PPwith Jessica sprayed the wall with paint 30PPwith *The wall sprayed with paint 0PPon Jessica sprayed paint on the wall 30Table 1: Example frames for verb sprayIn this paper, like Sun and Korhonen (2009);Joanis et al(2008) we seek to automatically clas-sify verbs into a broad range of classes.
Like Joa-nis et al we include evidence of DA, but we donot manually select features attributed to specificalternations but rather experiment with syntacticevidence for alternation approximation.
We usethe verb clustering system presented in Sun andKorhonen (2009) because it achieves state-of-the-art results on several datasets, including those ofJoanis et al even without the additional boost inperformance from the selectional preference data.We are interested in the improvement that can beachieved to verb clustering using approximationsfor DAs, rather than the DA per se.
As such wemake the simple assumption that if a pair of SCFstends to occur with the same verbs, we have a po-tential occurrence of DA.
Although this approx-imation can give rise to false positives (pairs offrames that co-occur frequently but are not DA)we are nevertheless interested in investigating itspotential usefulness for verb classification.
Oneattractive aspect of this method is that it does notrequire a pre-defined list of possible alternations.2 Diathesis Alternation ApproximationA DA can be approximated by a pair of SCFs.We parameterize frames involving prepositionalphrases with the preposition.
Example SCFs forthe verb ?spray?
are shown in Table 1.
The featurevalue of a single frame feature is the frequencyof the SCF.
Given two frames fv(i), fv(j) of averb v, they can be transformed into a feature pair(fv(i), fv(j)) as an approximation to a DA.
Thefeature value of the DA feature (fv(i), fv(j)) is ap-proximated by the joint probability of the pair offrames p(fv(i), fv(j)|v), obtained by integratingall the possible DAs.
The key assumption is thatthe joint probability of two SCFs has a strong cor-relation with a DA on the grounds that the DA givesrise to both SCFs in the pair.
We use the DA feature(fv(i), fv(j)) with its value p(fv(i), fv(j)|v) as anew feature for verb clustering.
As a comparisonpoint, we can ignore the DA and make a frame in-dependence assumption.
The joint probability isdecomposed as:p(fv(i), fv(j)|v)?
, p(fv(i)|v) ?
p(fv(j)|v) (1)We assume that SCFs are dependent as they aregenerated by the underlying meaning components(Levin and Hovav, 2006).
The frame dependencyis represented by a simple graphical model in fig-ure 1.Figure 1: Graphical model for the joint probability of pairs offrames.
v represents a verb, a represents a DA and f repre-sents a specific frame in total of M possible framesIn the data, the verb (v) and frames (f ) are ob-served, and any underlying alternation (a) is hid-den.
The aim is to approximate but not to detect aDA, so a is summed out:p(fv(i), fv(j)|v) =?ap(fv(i), fv(j)|a) ?
p(a|v)(2)In order to evaluate this sum, we use a relaxation1: the sum in equation 1 is replaced with the max-imum (max).
This is a reasonable relaxation, as apair of frames rarely participates in more than onetype of a DA.p(fv(i), fv(j)|v) ?
max(p(fv(i), fv(j)|a)?p(a|v))(3)The second relaxation further relaxes the first oneby replacing the max with the least upper bound(sup): If fv(i) occurs a times, fv(j) occurs b timesand b < a, the number of times that a DA occursbetween fv(i) and fv(j) must be smaller or equalto b.p(fv(i), fv(j)|v) ?
sup{p(fv(i), fv(j)|a)} ?
sup{p(a|v)}(4)sup{p(fv(i), fv(j)|a)} = Z?1 ?min(fv(i), fv(j))sup{p(a|v)} = 1Z =?m?nmin(fv(m), fv(n))1A relaxation is used in mathematical optimization for re-laxing the strict requirement, by either substituting it with aneasier requirement or dropping it completely.737Frame pair Possible DA FrequencyNP+PPon NP+PPwith Locative 30NP+PPon PPwith Causative(with) 0NP+PPon PPon Causative(on) 30NP+PPwith PPwith ?
0NP+PPwith PPon ?
30PPwith PPon ?
0NP+PPon NP+PPon - 40NP+PPwith NP+PPwith - 30PPwith PPwith - 0PPon PPon - 30Table 2: Example frame pair features for spraySo we end up with a simple form:p(fv(i), fv(j)|v) ?
Z?1 ?min(fv(i), fv(j)) (5)The equation is intuitive: If fv(i) occurs 40 timesand fv(j) 30 times, the DA between fv(i) andfv(j) ?
30 times.
This upper bound value is usedas the feature value of the DA feature.
The originalfeature vector f of dimension M is transformedinto M2 dimensions feature vector f?
.
Table 2shows the transformed feature space for spray.The feature space matches our expectation well:valid DAs have a value greater than 0 and invalidDAs have a value of 0.3 ExperimentsWe evaluated this model by performing verb clus-tering experiments using three feature sets:F1: SCF parameterized with preposition.
Exam-ples are shown in Table 1.F2: The frame pair features built from F1 withthe frame independence assumption (equa-tion 1).
This feature is not a DA feature asit ignores the inter-dependency of the frames.F3: The frame pair features (DAs) built fromF1 with the frame dependency assumption(equation 4).
This is the DA feature whichconsiders the correlation of the two frameswhich are generated from the alternation.F3 implicitly includes F1, as a frame can pairwith itself.
2 In the example in Table 2, the framepair ?PP(on) PP(on)?
will always have the samevalue as the ?PP(on)?
frame in F1.We extracted the SCFs using the system ofPreiss et al(2007) which classifies each corpus2We did this so that F3 included the SCF features as wellas the DA approximation features.
It would be possible infuture work to exclude the pairs involving identical frames,thereby relying solely on the DA approximations, and com-pare performance with the results obtained here.occurrence of a verb as a member of one of the 168SCFs on the basis of grammatical relations iden-tified by the RASP (Briscoe et al 2006) parser.We experimented with two datasets that have beenused in prior work on verb clustering: the test sets7-11 (3-14 classes) in Joanis et al(2008), and the17 classes set in Sun et al(2008).We used the spectral clustering (SPEC) methodand settings as in Sun and Korhonen (2009) butadopted the Bhattacharyya kernel (Jebara andKondor, 2003) to improve the computational effi-ciency of the approach given the high dimension-ality of the quadratic feature space.wb(v, v?)
=D?d=1(vdv?d)1/2 (6)The mean-filed bound of the Bhattacharyya kernelis very similar to the KL divergence kernel (Jebaraet al 2004) which is frequently used in verb clus-tering experiments (Korhonen et al 2003; Sunand Korhonen, 2009).To further reduce computational complexity, werestricted our scope to the more frequent features.In the experiment described in this section we usedthe 50 most frequent features for the 3-6 way clas-sifications (Joanis et als test set 7-9) and 100 fea-tures for the 7-17 way classifications.
In the nextsection, we will demonstrate that F3 outperformsF1 regardless of the feature number setting.
Thefeatures are normalized to sum 1.The clustering results are evaluated using F-Measure as in Sun and Korhonen (2009) whichprovides the harmonic mean of precision (P ) andrecall (R)P is calculated using modified purity ?
a globalmeasure which evaluates the mean precision ofclusters.
Each cluster (ki ?
K) is associatedwith the gold-standard class to which the major-ity of its members belong.
The number of verbsin a cluster (ki) that take this class is denoted bynprevalent(ki).P =?ki?K:nprevalent(ki)>2nprevalent(ki)|verbs|R is calculated using weighted class accuracy:the proportion of members of the dominant clusterDOM-CLUSTi within each of the gold-standardclasses ci ?
C.738DatasetsJoanis et alSun et al 8 9 10 11F1 54.54 49.97 35.77 46.61 38.81 60.03F2 50.00 49.50 32.79 54.13 40.61 64.00F3 56.36 53.79 52.90 66.32 50.97 69.62Table 3: Results when using F3 (DA), F2 (pair of independentframes) and F1 (single frame) features with Bhattacharyyakernel on Joanis et aland Sun et aldatasetsR =?|C|i=1 |verbs in DOM-CLUSTi||verbs|The results are shown in Table 3.
The result ofF2 is lower than that of F3, and even lower thanthat of F1 for 3-6 way classification.
This indi-cates that the frame independence assumption isa poor assumption.
F3 yields substantially betterresult than F2 and F1.
The result of F3 is 6.4%higher than the result (F=63.28) reported in Sunand Korhonen (2009) using the F1 feature.This experiment shows, on two datasets, that DAfeatures are clearly more effective than the framefeatures for verb clustering, even when relaxationsare used.4 Analysis of Feature FrequencyA further experiment was carried out using F1 andF3 on Joanis et al(2008)?s test sets 10 and 11.The frequency ranked features were added to theclustering one at a time, starting from the mostfrequent one.
The results are shown in figure 2.F3 outperforms F1 clearly on all the feature num-ber settings.
After adding some highly frequentframes (22 for test set 10 and 67 for test set 11),the performance for F1 is not further improved.The performance of F3, in contrast, is improvedfor almost all (including the mid-range frequency)frames, although to a lesser degree for low fre-quency frames.5 Related workParisien and Stevenson (2010) introduced a hier-archical Bayesian model capable of learning verbalternations and constructions from syntactic in-put.
The focus was on modelling and explainingthe child alternation acquisition rather than on au-tomatic verb classification.
Therefore, no quanti-tative evaluation of the clustering is reported, andthe number of verbs under the novel verb gen-eralization test is relatively small.
Parisien and1 22 1000.20.250.30.350.40.450.50.550.60.650.71 20 1.53416347347634?34?634.34.63463466???
?
??
?
?????
?1?1?
?Figure 2: Comparison between frame features (F1) and DAfeatures (F3) with different feature number settings.
DA fea-tures clearly outperform frame features.
The top figure is theresult on test set 10 (8 ways).
The bottom figure is the resulton test set 11 (14 ways).
The x axis is the number of features.The y axis is the F-Measure result.Stevenson (2011) extended this work by addingsemantic features.Parisien and Stevenson?s (2010) model 2 has asimilar structure to the graphic model in figure 1.A fundamental difference is that we explicitly usea probability distribution over alternations (pair offrames) to represent a verb, whereas they representa verb by a distribution over the observed framessimilar to Vlachos et al(2009) ?s approach.
Alsothe parameters in their model were inferred byGibbs sampling whereas we avoided this inferencestep by using relaxation.6 Conclusion and Future workWe have demonstrated the merits of using DAs forverb clustering compared to the SCF data fromwhich they are derived on standard verb classi-fication datasets and when integrated in a state-of-the-art verb clustering system.
We have alsodemonstrated that the performance of frame fea-tures is dominated by the high frequency frames.In contrast, the DA features enable the mid-rangefrequency frames to further improve the perfor-mance.739In the future, we plan to evaluate the perfor-mance of DA features in a larger scale experiment.Due to the high dimensionality of the transformedfeature space (quadratic of the original featurespace), we will need to improve the computationalefficiency further, e.g.
via use of an unsuperviseddimensionality reduction technique Zhao and Liu(2007).
Moreover, we plan to use Bayesian in-ference as in Vlachos et al(2009); Parisien andStevenson (2010, 2011) to infer the actual param-eter values and avoid the relaxation.Finally, we plan to supplement the DA featurewith evidence from the slot fillers of the alternat-ing slots, in the spirit of earlier work (McCarthy,2000; Merlo and Stevenson, 2001; Joanis et al2008).
Unlike these previous works, we will useselectional preferences to generalize the argumentheads but will do so using preferences from dis-tributional data (Sun and Korhonen, 2009) ratherthan WordNet, and use all argument head data inall frames.
We envisage using maximum averagedistributional similarity of the argument heads inany potentially alternating slots in a pair of co-occurring frames as a feature, just as we currentlyuse the frequency of the less frequent co-occurringframe.AcknowledgementOur work was funded by the Royal SocietyUniversity Research Fellowship (AK) and theDorothy Hodgkin Postgraduate Award (LS).ReferencesC.
Brew and S. Schulte im Walde.
Spectral clus-tering for German verbs.
In Proceedings ofEMNLP, 2002.E.
Briscoe, J. Carroll, and R. Watson.
The secondrelease of the RASP system.
In Proceedingsof the COLING/ACL on Interactive presentationsessions, pages 77?80, 2006.D.
Gildea and D. Jurafsky.
Automatic labelingof semantic roles.
Computational Linguistics,28(3):245?288, 2002.Y.
Guo, A. Korhonen, and T. Poibeau.
Aweakly-supervised approach to argumentativezoning of scientific documents.
In Proceedingsof EMNLP, pages 273?283, Stroudsburg, PA,USA, 2011.
ACL.T.
Jebara and R. Kondor.
Bhattacharyya and ex-pected likelihood kernels.
In Learning Theoryand Kernel Machines: 16th Annual Conferenceon Learning Theory and 7th Kernel Workshop,page 57.
Springer, 2003.T.
Jebara, R. Kondor, and A. Howard.
Probabilityproduct kernels.
The Journal of Machine Learn-ing Research, 5:819?844, 2004.E.
Joanis, S. Stevenson, and D. James.
A generalfeature space for automatic verb classification.Natural Language Engineering, 2008.K.
Kipper-Schuler.
VerbNet: A broad-coverage,comprehensive verb lexicon.
PhD thesis, Com-puter and Information Science Dept., Universityof Pennsylvania, Philadelphia, PA, June 2005.A.
Korhonen, Y. Krymolowski, and Z. Marx.Clustering polysemic subcategorization framedistributions semantically.
In Proceedings ofACL, pages 64?71, Morristown, NJ, USA,2003.
ACL.M.
Lapata.
Acquiring lexical generalizations fromcorpora: A case study for diathesis alternations.In Proceedings of ACL, pages 397?404.
ACLMorristown, NJ, USA, 1999.B.
Levin and M. Hovav.
Argument realiza-tion.
Computational Linguistics, 32(3):447?450, 2006.B.
Levin.
English Verb Classes and Alterna-tions: a preliminary investigation.
Universityof Chicago Press, Chicago and London, 1993.D.
McCarthy and A. Korhonen.
Detecting verbalparticipation in diathesis alternations.
In Pro-ceedings of ACL, volume 36, pages 1493?1495.ACL, 1998.D.
McCarthy.
Using semantic preferences to iden-tify verbal participation in role switching alter-nations.
In Proceedings of NAACL, pages 256?263.
Morgan Kaufmann Publishers Inc. SanFrancisco, CA, USA, 2000.P.
Merlo and S. Stevenson.
Automatic verb clas-sification based on statistical distributions of ar-gument structure.
Computational Linguistics,27(3):373?408, 2001.C.
Parisien and S. Stevenson.
Learning verb al-ternations in a usage-based Bayesian model.
InProceedings of the 32nd annual meeting of theCognitive Science Society, 2010.C.
Parisien and S. Stevenson.
Generalizing be-tween form and meaning using learned verbclasses.
In Proceedings of the 33rd AnnualMeeting of the Cognitive Science Society, 2011.740J.
Preiss, T. Briscoe, and A. Korhonen.
A systemfor large-scale acquisition of verbal, nominaland adjectival subcategorization frames fromcorpora.
In Proceedings of ACL, volume 45,page 912, 2007.P.
Resnik.
Selection and Information: A Class-Based Approach to Lexical Relationships.
PhDthesis, University of Pennsylvania, 1993.S.
Schulte im Walde.
Experiments on the au-tomatic induction of German semantic verbclasses.
Computational Linguistics, 32(2):159?194, 2006.E.
Shutova, L. Sun, and A. Korhonen.
Metaphoridentification using verb and noun clustering.In Proceedings of COLING, pages 1002?1010.ACL, 2010.L.
Sun and A. Korhonen.
Improving verb clus-tering with automatically acquired selectionalpreferences.
In Proceedings of EMNLP, pages638?647, 2009.L.
Sun and A. Korhonen.
Hierarchical verb clus-tering using graph factorization.
In Proceedingsof EMNLP, pages 1023?1033, Edinburgh, Scot-land, UK., July 2011.
ACL.L.
Sun, A. Korhonen, and Y. Krymolowski.
Verbclass discovery from rich syntactic data.
LectureNotes in Computer Science, 4919:16, 2008.V.
Tsang and S. Stevenson.
Using selectionalprofile distance to detect verb alternations.
InHLT/NAACL 2004 Workshop on ComputationalLexical Semantics, 2004.A.
Vlachos, A. Korhonen, and Z. Ghahramani.Unsupervised and constrained dirichlet processmixture models for verb clustering.
In Proceed-ings of the Workshop on Geometrical Modelsof Natural Language Semantics, pages 74?82,2009.Z.
Zhao and H. Liu.
Spectral feature selectionfor supervised and unsupervised learning.
InProceedings of ICML, pages 1151?1157, NewYork, NY, USA, 2007.
ACM.741
