Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 29?35Manchester, August 2008Parser Evaluation across Frameworks without Format ConversionWai Lok TamInterfaculty Initiative inInformation StudiesUniversity of Tokyo7-3-1 Hongo Bunkyo-kuTokyo 113-0033 JapanYo SatoDept of Computer ScienceQueen MaryUniversity of LondonMile End RoadLondon E1 4NS, U.K.Yusuke MiyaoDept of Computer ScienceUniversity of Tokyo7-3-1 Hongo Bunkyo-kuTokyo 113-0033 JapanJun-ichi TsujiiAbstractIn the area of parser evaluation, formatslike GR and SD which are based ondependencies, the simplest representationof syntactic information, are proposed asframework-independent metrics for parserevaluation.
The assumption behind theseproposals is that the simplicity of depen-dencies would make conversion from syn-tactic structures and semantic representa-tions used in other formalisms to GR/SD aeasy job.
But (Miyao et al, 2007) reportsthat even conversion between these twoformats is not easy at all.
Not to mentionthat the 80% success rate of conversionis not meaningful for parsers that boast90% accuracy.
In this paper, we makean attempt at evaluation across frame-works without format conversion.
Thisis achieved by generating a list of namesof phenomena with each parse.
Thesenames of phenomena are matched againstthe phenomena given in the gold stan-dard.
The number of matches found is usedfor evaluating the parser that produces theparses.
The evaluation method is more ef-fective than evaluation methods which in-volve format conversion because the gen-eration of names of phenomena from theoutput of a parser loaded is done by a rec-ognizer that has a 100% success rate ofrecognizing a phenomenon illustrated by asentence.
The success rate is made pos-sible by the reuse of native codes: codesc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.used for writing the parser and rules of thegrammar loaded into the parser.1 IntroductionThe traditional evaluation method for a deep parseris to test it against a list of sentences, each of whichis paired with a yes or no.
The parser is evaluatedon the number of grammatical sentences it acceptsand that of ungrammatical sentences it rules out.A problem with this approach to evaluation is thatit neither penalizes a parser for getting an analy-sis wrong for a sentence nor rewards it for gettingit right.
What prevents the NLP community fromworking out a universally applicable reward andpenalty scheme is the absence of a gold standardthat can be used across frameworks.
The correct-ness of an analysis produced by a parser can onlybe judged by matching it to the analysis producedby linguists in syntactic structures and semanticrepresentations created specifically for the frame-work on which the grammar is based.
A match ora mismatch between analyses produced by differ-ent parsers based on different frameworks does notlend itself for a meaningful comparison that leadsto a fair evaluation of the parsers.
To evaluate twoparsers across frameworks, two kinds of methodssuggest themselves:1.
Converting an analysis given in a certain for-mat native to one framework to another na-tive to a differernt framework (e.g.
convertingfrom a CCG (Steedman, 2000) derivation treeto an HPSG (Pollard and Sag, 1994) phrasestructure tree with AVM)2.
Converting analyses given in differentframework-specific formats to some simplerformat proposed as a framework-independentevaluation schema (e.g.
converting from29HPSG phrase structure tree with AVM to GR(Briscoe et al, 2006))However, the feasibility of either solution isquestionable.
Even conversion between two eval-uation schemata which make use of the simplestrepresentation of syntactic information in the formof dependencies is reported to be problematic by(Miyao et al, 2007).In this paper, therefore, we propose a differentmethod of parser evaluation that makes no attemptat any conversion of syntactic structures and se-mantic representations.
We remove the need forsuch conversion by abstracting away from com-parison of syntactic structures and semantic rep-resentations.
The basic idea is to generate a listof names of phenomena with each parse.
Thesenames of phenomena are matched against the phe-nomena given in the gold standard for the samesentence.
The number of matches found is usedfor evaluating the parser that produces the parse.2 Research ProblemGrammar formalisms differ in many aspects.
Insyntax, they differ in POS label assignment, phrasestructure (if any), syntactic head assignment (ifany) and so on, while in semantics, they differfrom each other in semantic head assignment, roleassignment, number of arguments taken by pred-icates, etc.
Finding a common denominator be-tween grammar formalisms in full and complexrepresentation of syntactic information and seman-tic information has been generally considered bythe NLP community to be an unrealistic task, al-though some serious attempts have been made re-cently to offer simpler representation of syntacticinformation (Briscoe et al, 2006; de Marneffe etal., 2006).Briscoe et al(2006)?s Grammatical Rela-tion (GR) scheme is proposed as a framework-independent metric for parsing accuracy.
Thepromise of GR lies actually in its dependence ona framework that makes use of simple representa-tion of syntactic information.
The assumption be-hind the usefulness of GR for evaluating the out-put of parsers is that most conflicts between gram-mar formalisms would be removed by discardingless useful information carried by complex syn-tactic or semantic representations used in gram-mar formalisms during conversion to GRs.
Butis this assumption true?
The answer is not clear.A GR represents syntactic information in the formof a binary relation between a token assigned asthe head of the relation and other tokens assignedas its dependents.
Notice however that grammarframeworks considerably disagree in the way theyassign heads and non-heads.
This would raise thedoubt that, no matter how much information is re-moved, there could still remain disagreements be-tween grammar formalisms in what is left.The simplicity of GR, or other dependency-based metrics, may give the impression that con-version from a more complex representation intoit is easier than conversion between two complexrepresentations.
In other words, GRs or a sim-ilar dependency relation looks like a promisingcandidate for lingua franca of grammar frame-works.
However the experiment results given byMiyao et al(2007) show that even conversion intoGRs of predicate-argument structures, which is notmuch more complex than GRs, is not a trivial task.Miyao et al(2007) manage to convert 80% of thepredicate-argument structures outputted by theirdeep parser, ENJU, to GRs correctly.
However theparser, with an over 90% accuracy, is too good forthe 80% conversion rate.
The lesson here is thatsimplicity of a representation is a different thingfrom simplicity in converting into that representa-tion.3 Outline of our SolutionThe problem of finding a common denominator forgrammar formalisms and the problem of conver-sion to a common denominator may be best ad-dressed by evaluating parsers without making anyattempt to find a common denominator or conductany conversion.
Let us describe briefly in this sec-tion how such evaluation can be realised.3.1 Creating the Gold StandardThe first step of our evaluation method is to con-struct or find a number of sentences and get an an-notator to mark each sentence for the phenomenaillustrated by each sentence.
After annotating allthe sentences in a test suite, we get a list of pairs,whose first element is a sentence ID and second isagain a list, one of the corresponding phenomena.This list of pairs is our gold standard.
To illustrate,suppose we only get sentence 1 and sentence 2 inour test suite.
(1) John gives a flower to Mary(2) John gives Mary a flower30Sentence 1 is assigned the phenomena: propernoun, unshifted ditransitive, preposition.
Sentence2 is assigned the phenomena: proper noun, dative-shifted ditransitive.
Our gold standard is thus thefollowing list of pairs:?1, ?proper noun, unshifted ditransitive, preposition?
?,?2, ?proper noun,dative-shifted ditransitive?
?3.2 Phenomena RecognitionThe second step of our evaluation method requiresa small program that recognises what phenomenaare illustrated by an input sentence taken from thetest suite based on the output resulted from pars-ing the sentence.
The recogniser provides a setof conditions that assign names of phenomena toan output, based on which the output is matchedwith some framework-specific regular expressions.It looks for hints like the rule being applied at anode, the POS label being assigned to a node, thephrase structure and the role assigned to a refer-ence marker.
The names of phenomena assignedto a sentence are stored in a list.
The list of phe-nomena forms a pair with the ID of the sentence,and running the recogniser on multiple outputs ob-tained by batch parsing (with the parser to be eval-uated) will produce a list of such pairs, in exactlythe same format as our gold standard.
Let us illus-trate this with a parser that:1. assigns a monotransitive verb analysis to?give?
and an adjunct analysis to ?to Mary?
in12.
assigns a ditransitive verb analysis to ?give?
in2The list of pairs we obtain from running therecogniser on the results produced by batch pars-ing the test suite with the parser to be evaluated isthe following:?1,?proper noun,monotransitive,preposition,adjunct?
?,?2, ?proper noun,dative-shifted ditransitive?
?3.3 Performance Measure CalculationComparing the two list of pairs generated from theprevious steps, we can calculate the precision andrecall of a parser using the following formulae:Precision = (n?i=1| Ri?Ai|| Ri|)?
n (1)Recall = (n?i=1| Ri?Ai|| Ai|)?
n (2)where list Riis the list generated by the recogniserfor sentence i, list Aiis the list produced by anno-tators for sentence i, and n the number of sentencesin the test suite.In our example, the parser that does a good jobwith dative-shifted ditransitives but does a poor jobwith unshifted ditranstives would have a precisionof:(24+22)?
2 = 0.75and a recall of:(23+22)?
2 = 0.834 Refining our SolutionIn order for the precision and recall given above tobe a fair measure, it is necessary for both the recog-niser and the annotators to produce an exhaustivelist of the phenomena illustrated by a sentence.But we foresee that annotation errors are likelyto be a problem of exhaustive annotation, as is re-ported in Miyao et al(2007) for the gold standarddescribed in Briscoe et al(2006).
Exhaustive an-notation procedures require annotators to repeat-edly parse a sentence in search for a number ofphenomena, which is not the way language is nor-mally processed by humans.
Forcing annotators todo this, particularly for a long and complex sen-tence, is a probable reason for the annotation er-rors in the gold standard described in (Briscoe etal., 2006).To avoid the same problem in our creation of agold standard, we propose to allow non-exhaustiveannotation.
In fact, our proposal is to limit thenumber of phenomena assigned to a sentence toone.
This decision on which phenomenon to be as-signed is made, when the test suite is constructed,for each of the sentences contained in it.
Follow-ing the traditional approach, we include every sen-tence in the test suite, along with the core phe-nomenon we intend to test it on (Lehmann andOepen, 1996).
Thus, Sentence 1 would be as-signed the phenomenon of unshifted ditransitive.Sentence 2 would be assigned the phenomenon of31dative-shifted ditransitive.
This revision of anno-tation policy removes the need for exhaustive an-notation.
Instead, annotators are given a new task.They are asked to assign to each sentence the mostcommon error that a parser is likely to make.
ThusSentence 1 would be assigned adjunct for such anerror.
Sentence 2 would be assigned the error ofnoun-noun compound.
Note that these errors arealso names of phenomena.This change in annotation policy calls for achange in the calculation of precision and recall.We leave the recogniser as it is, i.e.
to produce anexhaustive list of phenomena, since it is far beyondour remit to render it intelligent enough to select asingle, intended, phenomenon.
Therefore, an in-correctly low precision would result from a mis-match between the exhaustive list generated by therecogniser and the singleton list produced by an-notators for a sentence.
For example, suppose weonly have sentence 2 in our test suite and the parsercorrectly analyses the sentence.
Our recogniser as-signs two phenomena (proper noun, dative-shiftedditransitive) to this sentence as before.
This wouldresult in a precision of 0.5.Thus we need to revise our definition of preci-sion, but before we give our new definition, let usdefine a truth function t:t(A ?
B) ={1 A ?
B0 A ?B = ?t(A ?B = ?)
={0 A ?B 6= ?1 A ?B = ?Now, our new definition of precision and recallis as follows:Precision (3)=(?ni=1t(Ri?APi)+t(Ri?ANi=?
)2)nRecall (4)=(?ni=1|Ri?APi||APi|)nwhere list APiis the list of phenomena producedby annotators for sentence i, and list ANiis the listof errors produced by annotators for sentence i.While the change in the definition of recall istrivial, the new definition of precision requiressome explanation.
The exhaustive list of phenom-ena generated by our recogniser for each sentenceis taken as a combination of two answers to twoquestions on the two lists produced by annotatorsfor each sentence.
The correct answer to the ques-tion on the one-item-list of phenomenon producedby annotators for a sentence is a superset-subset re-lation between the list generated by our recogniserand the one-item-list of phenomenon produced byannotators.
The correct answer to the question onthe one-item-list of error produced by annotatorsfor a sentence is the non-existence of any commonmember between the list generated by our recog-niser and the one-item-list of error produced by an-notators.To illustrate, let us try a parser that does a goodjob with dative-shifted ditransitives but does a poorjob with unshifted ditranstives on both 2 and 1.The precision of such a parser would be:(02+22)?
2 = 0.5and its recall would be:(01+11)?
2 = 0.55 ExperimentFor this abstract, we evaluate ENJU (Miyao,2006), a released deep parser based on the HPSGformalism and a parser based on the Dynamic Syn-tax formalism (Kempson et al, 2001) under devel-opment against the gold standard given in table 1.The precision and recall of the two parsers(ENJU and DSPD, which stands for ?DynamicSyntax Parser under Development?)
are given intable 3:The experiment that we report here is intendedto be an experiment with the evaluation method de-scribed in the last section, rather than a very seri-ous attempt to evaluate the two parsers in question.The sentences in table 1 are carefully selected toinclude both sentences that illustrate core phenom-ena and sentences that illustrate rarer but more in-teresting (to linguists) phenomena.
But there aretoo few of them.
In fact, the most important num-ber that we have obtained from our experiment isthe 100% success rate in recognizing the phenom-ena given in table 1.32ID Phenomenon Error1 unshifted ditransi-tiveadjunct2 dative-shifted di-transitivenoun-noun com-pound3 passive adjunct4 nominal gerund verb that takesverbal comple-ment5 verbal gerund imperative6 preposition particle7 particle preposition8 adjective with ex-trapolated senten-tial complementrelative clause9 inversion question10 raising controlFigure 1: Gold Standard for Parser EvaluationID Sentence1 John gives a flower to Mary2 John give Mary a flower3 John is dumped by Mary4 Your walking me pleases me5 Abandoning children increased6 He talks to Mary7 John makes up the story8 It is obvious that John is a fool9 Hardly does anyone know Mary10 John continues to please MaryFigure 2: Sentences Used in the Gold StandardMeasure ENJU DSPDPrecision 0.8 0.7Recall 0.7 0.5Figure 3: Performance of Two Parsers6 Discussion6.1 Recognition RateThe 100% success rate is not as surprising as itmay look.
We made use of two recognisers, onefor each parser.
Each of them is written by theone of us who is somehow involved in the devel-opment of the parser whose output is being recog-nised and familiar with the formalism on which theoutput is based.
This is a clear advantage to for-mat conversion used in other evaluation methods,which is usually done by someone familiar with ei-ther the source or the target of conversion, but notboth, as such a recogniser only requires knowledgeof one formalism and one parser.
For someonewho is involved in the development of the gram-mar and of the parser that runs it, it is straight-forward to write a recogniser that can make useof the code built into the parser or rules includedin the grammar.
We can imagine that the 100%recognition rate would drop a little if we neededto recognise a large number of sentences but werenot allowed sufficient time to write detailed regularexpressions.
Even in such a situation, we are con-fident that the success rate of recognition would behigher than the conversion method.Note that the effectiveness of our evaluationmethod depends on the success rate of recognitionto the same extent that the conversion method em-ployed in Briscoe et al (2006) and de Marneff etal.
(2006) depends on the conversion rate.
Giventhe high success rate of recognition, we argue thatour evaluation method is more effective than anyevaluation method which makes use of a formatclaimed to be framework independent and involvesconversion of output based on a different formal-ism to the proposed format.6.2 Strictness of Recognition and PrecisionThere are some precautions regarding the use ofour evaluation method.
The redefined precision 4is affected by the strictness of the recogniser.
Toillustrate, let us take Sentence 8 in Table 1 as anexample.
ENJU provides the correct phrase struc-ture analysis using the desired rules for this sen-tence but makes some mistakes in assigning rolesto the adjective and the copular verb.
The recog-niser we write for ENJU is very strict and refusesto assign the phenomenon ?adjective with extrap-olated sentential complement?
based on the outputgiven by ENJU.
So ENJU gets 0 point for its an-swer to the question on the singleton list of phe-33nomenon in the gold standard.
But it gets 1 pointfor its answer to the question on the singleton listof error in the gold standard because it does notgo to the other extreme: a relative clause analysis,yielding a 0.5 precision.
In this case, this value isfair for ENJU, which produces a partially correctanalysis.
However, a parser that does not acceptthe sentence at all, a parser that fails to produceany output or one that erroneously produces an un-expected phenomenon would get the same result:for Sentence 8, such a parser would still get a pre-cision of 0.5, simply because its output does notshow that it assigns a relative clause analysis.We can however rectify this situation.
For thelack of parse output, we can add an exceptionclause to make the parser automatically get a 0 pre-cision (for that sentence).
Parsers that make unex-pected mistakes are more problematic.
An obvi-ous solution to deal with these parsers is to comeup with an exhaustive list of mistakes but this is anunrealistic task.
For the moment, a temporary butrealistic solution would be to expand the list of er-rors assigned to each sentence in the gold standardand ask annotators to make more intelligent guessof the mistakes that can be made by parsers by con-sidering factors such as similarities in phrase struc-tures or the sharing of sub-trees.6.3 Combining Evaluation MethodsFor all measures, some distortion is unavoidablewhen applied to exceptional cases.
This is true forthe classical precision and recall, and our redefinedprecision and recall is no exception.
In the case ofthe classical precision and recall, the distortion iscountered by the inverse relation between them sothat even if one is distorted, we can tell from theother that how well (poorly) the object of evalua-tion performs.
Our redefined precision and recallworks pretty much the same way.What motivates us to derive measures so closelyrelated to the classical precision and recall is theease to combine the redefined precision and recallobtained from our evaluation method with the clas-sical precision and recall obtained from other eval-uation methods, so as to obtain a full picture ofthe performance of the object of evaluation.
Forexample, our redefined precision and recall figuresgiven in Table 3 (or figures obtained from runningthe same experiment on a larger test set) for ENJUcan be combined with the precision and recall fig-ures given in Miyao et al (2006) for ENJU, whichis based on a evaluation method that compares itspredicate-argument structures those given in PennTreebank.
Here the precision and recall figures arecalculated by assigning an equal weight to everysentence in Section 23 of Penn Treebank.
Thismeans that different weights are assigned to dif-ferent phenomena depending on their frequency inthe Penn Treebank.
Such assignment of weightsmay not be desirable for linguists or developersof NLP systems who are targeting a corpus with avery different distribution of phenomena from thisparticular section of the Penn Treebank.
For exam-ple, a linguist may wish to assign an equal weightacross phenomena or more weights to ?interesting?phenomena.
A developer of a question-answeringsystem may wish to give more weights to question-related phenomena than other phenomena of lessinterest which are nevertheless attested more fre-quently in the Penn Treebank.In sum, the classical precision and recall fig-ures calculated by assigning equal weight to ev-ery sentence could be considered skewed from theperspective of phenomena, whereas our redefinedprecision and recall figures may be seen as skewedfrom the frequency perspective.
Frequency is rela-tive to domains: less common phenomena in somedomains could occur more often in others.
Our re-defined precision and recall are not only useful forthose who want a performance measure skewed theway they want, but also useful for those who wanta performance measure as ?unskewed?
as possible.This may be obtained by combining our redefinedprecision and recall with the classical precisionand recall yielded from other evaluation methods.7 ConclusionWe have presented a parser evaluation methodthat addresses the problem of conversion betweenframeworks by totally removing the need for thatkind of conversion.
We do some conversion butit is a different sort.
We convert the output of aparser to a list of names of phenomena by drawingonly on the framework that the parser is based on.It may be inevitable for some loss or inaccuracyto occur during this kind of intra-framework con-version if we try our method on a much larger testset with a much larger variety of longer sentences.But we are confident that the loss would still befar less than any inter-framework conversion workdone in other proposals of cross-framework evalu-ation methods.
What we believe to be a more prob-34lematic area is the annotation methods we havesuggested.
At the time we write this paper basedon a small-scale experiment, we get slightly bet-ter result by asking our annotator to give one phe-nomenon and one common mistake for each sen-tence.
This may be attributed to the fact that heis a member of the NLP community and hence hegets the knowledge to identify the core phenom-ena we want to test and the common error thatparsers tend to make.
If we expand our test setand includes longer sentences, annotators wouldmake more mistakes whether they attempt exhaus-tive annotation or non-exhaustive annotation.
Itis difficult to tell whether exhaustive annotationor non-exhaustive annotation would be better forlarge scale experiments.
As future work, we intendto try our evaluation method on more test data todetermine which one is better and find ways to im-prove the one we believe to be better for large scaleevaluation.ReferencesBriscoe, Ted, John Carroll, and Rebecca Watson.
2006.The second release of the RASP system.
In Proceed-ings of COLING/ACL 2006.de Marneffe, Marie-Catherine, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InProceedings of LREC 2006.Kempson, Ruth, Wilfried Meyer-Viol, and Dov Gab-bay.
2001.
Dynamic Syntax: The Flow of LanguageUnderstanding.
Blackwell.Lehmann, Sabine and Stephan Oepen.
1996.
TSNLPtest suites for natural language processing.
In Pro-ceedings of COLING 1996.Miyao, Yusuke, Kenji Sagae, and Junichi Tsujii.
2007.Towards framework-independent evaluation of deeplinguistic parsers.
In Proceedings of GEAF 2007.Miyao, Yusuke.
2006.
From Linguistic Theory to Syn-tactic Analysis: Corpus-Oriented Grammar Devel-opment and Feature Forest Model.
Ph.D. thesis, Uni-versity of Tokyo.Pollard, Carl and Ivan A.
Sag.
1994.
Head-DrivenPhrase Structure Grammar.
University of ChicagoPress and CSLI Publications.Steedman, Mark.
2000.
Syntactic Process.
MIT Press.35
