NAACL-HLT 2012 Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data, pages 9?10,Montre?al, Canada, June 7, 2012. c?2012 Association for Computational LinguisticsStatistical User Simulation for Spoken Dialogue Systems:What for, Which Data, Which Future?
?Olivier PietquinSUPELEC - UMI 2958 (GeorgiaTech - CNRS)2 rue Edouard Belin57070 Metz - Franceolivier.pietquin@supelec.frAbstractThere has been a lot of interest for user sim-ulation in the field of spoken dialogue sys-tems during the last decades.
User simulationwas first proposed to assess the performanceof SDS before a public release.
Since the late90?s, user simulation is also used for dialoguemanagement optimisation.
In this position pa-per, we focus on statistical methods for usersimulation, their main advantages and draw-backs.
We initiate a reflection about the util-ity of such methods and give some insights ofwhat their future should be.1 IntroductionUser simulation for Spoken Dialogue Systems(SDS) aims at generating artificial interactions sup-posed to be representative of what would be an ac-tual dialogue between a human user and a givendialogue system.
User simulation is thus differentfrom user modeling which is often included into thesystems to infer user goals from observable clues(user?s utterances, intonations etc.)
(Zukerman andAlbrecht, 2001).
In this paper we focus on statisticalmethods for user simulation, that is methods purelybased on data and statistical models and not cogni-tive models.
Also, we only address user simulationsworking at the intention level, that is generating dia-log acts and not speech or natural language (Schatz-mann et al, 2006).
User modeling, used to infer userintentions in dialogue systems is not addressed.
?This work as been partially funded by the INTERREG IVaproject ALLEGRO and the Re?gion LorraineThe aim of user simulation was initially to as-sess the performance of a SDS before a public re-lease (Eckert et al, 1997).
Given a performancemetric and a simulation method, the natural idea ofautomatically optimizing SDS (using reinforcementlearning RL) appeared in the literature in the late90?s (Levin et al, 2000).2 Is user simulation useful?Initially, SDS optimisation required a lot of data be-cause of inefficiency of RL algorithms, justifyingthe use of simulation.
In recent years, sample effi-cient RL methods were applied to SDS optimization.This allows learning optimal dialogue strategies di-rectly from batches of data collected between sub-optimal systems and actual users (Li et al, 2009;Pietquin et al, 2011b) but also from online interac-tions (Pietquin et al, 2011a; Gasic et al, 2011).
Dowe have to conclude that user simulation is useless?3 Do we need to train models?It is commonly admitted that learning parameters ofuser simulation models is hard because most of vari-ables are hidden (user goal, mental states etc.)
andtricky to annotate.
This is why current user simula-tors are trainable but rarely trained (Pietquin, 2006;Schatzmann et al, 2007).
Do we really need to trainuser simulation models?
If so, which data and anno-tation schemes do we need?4 Does simulation reach the target?User simulation aims at reproducing plausible inter-actions but in contexts that were not seen in the data9collected to train the model.
It is generally hard toassess the quality of such models.
Especially, it ishard to find a single metric to assess user simulationperformances (Pietquin and Hastie, 2011).
Also, ithas been shown that user simulation affects a lot theresult of SDS strategy optimisation (Schatzmann etal., 2005).
What should be assessed?
Statisticalconsistency, ability to generalize, ability to generatesequences of interactions similar to real dialogues,ability to produce optimal strategies by RL?
If onewants to learn an optimal simulation model, there isa need for a single optimality criterion.5 What?s the future of user simulation forSDS?Whatever the use one wants to make of user simula-tion (learning or assessment for SDS), the future ofthis research field relies probably on a redefinition ofthe role of user simulation.
So far, user simulationis seen as a generative systems, generating dialogacts according to the context.
Current user simula-tion models are therefore based on a large amount ofconditional probabilities which are hard to learn, andthe training (if there is one) requires a lot of priorknowledge, the introduction of smoothing parame-ters etc.We believe that user simulation should be rede-fined as a sequential decision making problem inwhich a user tries to reach a goal in a natural and ef-ficient way, helped by an artificial agent (the SDS).One major difference between this vision and thecommon probabilistic one is that it takes into ac-count the fact that human users adapt their behav-ior to the performances and the strategy of the SDS.This can be called ?co-adaptation?
between humanusers and artificial systems and justifies that usersimulation should still be studied.Recently, user simulation models based on inversereinforcement learning have been proposed (Chan-dramohan et al, 2011).
In this framework, a useris modeled as optimizing it?s behavior accordingto some unknown reward which is inferred fromrecorded data.
This might be an answer to the co-adaptation problem.
Yet, is user simulation still use-ful in this framework?
Knowing the reward of theuser, do we still need simulation or is it possible tocompute directly an optimal dialogue strategy?ReferencesS.
Chandramohan, M. Geist, F. Lefe`vre, and O. Pietquin.2011.
User Simulation in Dialogue Systems using In-verse Reinforcement Learning.
In Proc.
of Interspeech2011, Florence (Italy).W.
Eckert, E. Levin, and R. Pieraccini.
1997.
UserModeling for Spoken Dialogue System Evaluation.
InProc.
of ASRU?97, Santa Barbara (USA).M.
Gasic, F. Jurcicek, B. Thomson, K. Yu, and S. Young.2011.
On-line policy optimisation of spoken dialoguesystems via live interaction with human subjects?.
InProc.
of ASRU 2011, Hawaii (USA).E.
Levin, R. Pieraccini, and W. Eckert.
2000.
A Stochas-tic Model of Human-Machine Interaction for learningdialog Strategies.
IEEE Transactions on Speech andAudio Processing, 8:11?23.L.
Li, S. Balakrishnan, and J. Williams.
2009.
Reinforce-ment Learning for Dialog Management using Least-Squares Policy Iteration and Fast Feature Selection.
InProc.
of InterSpeech?09, Brighton (UK).O.
Pietquin and H. Hastie.
2011.
A survey on metrics forthe evaluation of user simulations.
Knowledge Engi-neering Review.O.
Pietquin, M. Geist, and S. Chandramohan.
2011a.Sample Efficient On-line Learning of Optimal Dia-logue Policies with Kalman Temporal Differences.
InProc.
of IJCAI 2011, Barcelona, Spain.O.
Pietquin, M. Geist, S. Chandramohan, and H. Frezza-Buet.
2011b.
Sample-Efficient Batch Reinforce-ment Learning for Dialogue Management Optimiza-tion.
ACM Transactions on Speech and Language Pro-cessing, 7(3):7:1?7:21, May.O.
Pietquin.
2006.
Consistent goal-directed user modelfor realistic man-machine task-oriented spoken dia-logue simulation.
In ICME?06, Toronto (Canada).J.
Schatzmann, M.Stuttle, K. Weilhammer, and S. Young.2005.
Effects of the user model on simulation-basedlearning of dialogue strategies.
In Proc.
of ASRU?05.J.
Schatzmann, K. Weilhammer, M. Stuttle, andS.
Young.
2006.
A survey of statistical user simula-tion techniques for reinforcement-learning of dialoguemanagement strategies.
Knowledge Engineering Re-view, vol.
21(2), pp.
97?126.J.
Schatzmann, B. Thomson, K. Weilhammer, H. Ye, andS.
Young.
2007.
Agenda-based User Simulation forBootstrapping a POMDP Dialogue System.
In Proc.of HLT NAACL.I.
Zukerman and D. Albrecht.
2001.
Predictive statisticalmodels for user modeling.
User Modeling and User-Adapted Interaction, 11(1-2):5?18.
invited paper.10
