BUILDING A LEX ICAL  DOMAIN MAP FROM TEXT CORPORATomek StrzalkowskiCourant  Institute of Mathematical  Sciences, New York Univers i ty715 Broadway,  rm, 704, New York, NY 10003, tomek@cs.nyu.eduSUMMARYIn information retrieval the task is to extract from thedatabase ~dl ,and only the documents which are relevant toa user query, even when the query and the documents uselittle common vocabul~u'y.
In this paper we discuss theproblem of automatic generation of lexical relationsbetween words ,and phrltses from large text corpora :rodtheir application to automatic query expansion ill informa-tion retrieval.
Reported here ,are some preliminary resuhsand observations from the experiments with a 85 millionword Wall Street Journal dalabase and a 45 million wordSan Jose Mercury News database (piu'ts of 0.5 billionword TIPSTER/TREC datab`ase).INTRODUCTIONTile task of information retrieval is to extractrelevant documents from large collection of documents illresponse to a user's query.
When the documents cont:dnprimm'ily unrestricted text (e.g., newspaper `articles, leglddocuments, etc.)
the relev,'mce of a document is esta-blished through 'full-text' retriewd.
This has been usuallyaccomplished by identifying key terms in the documents(the process known as 'indexing') which could then bematched against erms in queries (Salton, 1989).
Theeffectiveness of ,any such term-b`ased approach is directlyrelated to the accuracy with which a set of termsrepresents he content of a document, ,as well as how wellit contrasts a given document with respect o other docu-ments.
In other words, we ,are looking for a represeutat ionR such that for any text items D1 and D2, R(DI) = R(D2)iff meaning(D1) = meaning(D2), at an appropriate l velof abstraction (which may depend on types and characterof anticipated queries).For all kinds of terms that can be assigned 1o therepresentation of a docmnent, e.g., words, operator-m'gument pairs, fixed phrases, ~md proper n,'unes, vltriouslevels of "reguh'u'ization" ,are needed to ,assure that syn-tactic or lexie,'d v,'u'iations of input do not obscure under-lying semantic uniformity.
Without actually doingsemantic analysis, tiffs kind of normalization can beachieved through the following processes: ~(1) morpbological stemming: e.g., retrieving isreduced to retriev;An altematlve, but less efficient method is to generate all vari-ants (lexical, syntactic, etc.)
of words/phrases in the queries (Sparck-Jones & "Fail, 1984).
(2) lexicon-based word nonnldizntion: e.g., retrievalis reduced to retrieve;(3) operator-argument representation f phr'tses: e.g.,information retrieval, retrievhlg of information,and retrieve relewmt information ,are ,all assignedthe slune representation, retrieve+btformation;(4) conlext-blmed term clustering into synonymyclasses and subsumption hierarchies: e.g., take-over is a kind of acquisition (in business), luldFortran is a programming language.We have established the general architecture ofa NLP-IRsystem that accommodates these considerations.
In a gen-eral view of this design, depicted schematic~dly below, anadvanced NLP module is inserted between the textuldinput (new documeuts, user queries) and the datab~Jsesearch engine (in our c`ase, NIST's PRISE system).NLP: ' FA \ [~ PARSER temlsThis design has already shown some promise in produc-ing signific~mtly better performance than the base statisti-cld system (Strz~dkowski, 1993).
Its practical significancestems in no slnall part from the use of a tkst and robustparser, TI'P, 2 which can process unrestricted text atspeeds below 0.2 sec per sentence.
TI'P's output is It reg-ularized representation 1' each sentence which reflectslogical prcdicalc-argumclll su'uclure, e.g., Iogic:d subjectand logical objects are identilicd depending upon themain verb subcategorization frame.
For example, Ihe verbabide has, among others, a subcategorization frame inwhich the object is a prepositional pbrase with by, i.e.,ABIDE: subject NP object PREP by NPSubcategorization inlbrmution is rend from the on-lineOxford Advanced Le`arner's Diction,try (OALD) whichTTP uses.TFP stands for Tagged Text Parser, and it has I:een described indetail in (Strzalkowski, 1992) and ev~duated in (Strzalkowski &Scheyen, 1993).604ltEAD-MODIFIER STRUCTURESTTP p,'u'se structures are p~tssed to the phraseextraction module where head+modifier (includingpredicate+,'u'gument) pairs are extracted and collected intooccurrence patterns.
The following types ofhead+modifier pairs m'e extracted:(1) a head noun and its left adjective or noun adjunct,(2) a head noun ,and the head of its right adjunct,(3) the m,'fin verb of a clanse and the head of itsobject pbrase.These types of p,'firs account for most of the syntacticvm'i~mts for relating two words (or simple phrases) intopairs c,'urying compatible semantic ontent.
For example,the pair retrieve+information will be extracted from mtyof the following fragments: information retrieval system;retrieval of it~rmation /)'om databases; and informationthat can be retrieved by a user-controlled interactivesearch process.
3Figure 1 shows TTP parse and head+modifier pairsextracted.
Whenever multiple-noun strings (two nounsplus another noun or adjective) are present, they need Iobe structurally disambiguated before any pairs emt beextracted.
This is accomplished using statistically-basedpreferences, e.g., world+third is pt'etizn'ed to eithercountry+world or cot#ltry+third when extracted fromthird world country.
If such preferences cannot be cont-puted, all alternatives ,'u'e discarded to avokl noisy inputto clustering progrmns.\[S;m Jose Mercury News {}8/30/91 Busilmss SectlonlFor McCaw, it wouhl have hurt the company's tralegyof building a seamless national cellular ilelWolk.\[assell,\[\[will auxl,llpeff,\[havell,Ilvetb,lhmtll,\[sul',jeet,lnl',,lu,it111,\[ol~jeel,\[t'q},\[n,slnltegy\],\[t_l?~s,the I,In~ms,lposs.
tn ,corn pany Ill.\[of,l lverb J buihlll,\[subject,~myouel,I{d}ject,l.p,ln.l~etwoz k l,\[t f, os,a I,ladj,lse;unless II,\[adj,luational\]l,\[adj.lcellularllll\]ll\]ll,I for,lup,Iname,lmccawllllll.EX'I'I~.A UIT21 } I'A1RS:hall+ s| Fate~, ,y  slFalegy+colnl}allybui ld+nelwork net work+cclhJlaruet wet k+llali'ollal he|work+seamlessF'tgnre 1.
Extracting Ilead+Modilier pairs from parsed sentences.TERM CORRELATIONS FROM TEXTHead-modifier pairs serve as occurrence contextsfor terms included in them: both single words (as shownin Fignre 1) and other pairs (in case of nested pairs, e.g.,cottntry+\[world+third\]).
If two terms tend to be modilicdwith a number of common modifiers but otherwise appearin few distinct contexts, we assign them a simih'uitycoefficient, a real number between 0 and 1.
The similarityis determined by comparing distribution characlerislicsfor both terms within the corpus: in general we will credithigh-content terms appem'ing in multiple identical elm-texts, provided that these contexts are not too common-place.
4 Figure 2 shows exmnples of terms sharing anumber of common contexts along with frequencies ofoccurrence in a 250 MByte subset of Wall Street Journaldatabase.
A head context is when two distinct modifiers,are attached to the same head element; a rood context iswhen the s,'une term modilles two distinct heads.To compute term similarities we used a variant ofweighted Jacc\[u'd's measure described in e.g., (Grefen-TFRMI TERM2 COMM CNTXT FRQI FRQ2IIEAD MODvice delmtyI1|;11) heypresident 9295 29chaiml,'m l(X)7 146director 6 158minisler 37 17premier 7 8sloly 9 3chib fi 4age 18 3mother 4 5bad 4 4yotmg 258 12ohler 18 ,Ili'tgure 2.
L:xample pairs of related re.ms.3 snbject+ved} pairs are also extracted but these are not used in thelexical clustering procedure described here.4 It would not be appropriale to predict similarity I~.
'tweenlanguage and logarithm on the basis of their co-occurrence with mztural.stette, 1992): 5In another series of exf, orinmt~ts (Swzalkowskl & Vauthey,1992} we used a Mtllnal lnfo0maliou I}ased classillcalion formula (e.g.,Church and ttanks, 1990; lliudle, 1990), but we l~,}und it less effeclivefor diverse dalabases, uch as WSJ.605~__,MIN (W ( \ [x,  a t t \ ] ) ,  W (\[y,att \])SIM (x t , x2)  = att~.MAX (W (\[x,att \]), W (\[y,att \])~lttwithW (\[x,y 1) = aEW (x)*tog (f.,a)GEW(x)=I+ ny v ~ny j |tog (N) 1In rite above, f~,y stands for absolute fi'equency of pair\[x,y\] in tile corpus, ny is the frequency of term y, and N isttte number of single-word terms.hi order to generate better sitnilarities, we requirethat words xt and x2 appear in at least M distinct conl-ilion contexts, where It common context is a couple ofpairs \[xt,Y\] and \[x2,y\], or \[y,x 1\] and \[y,r 2\] such that theyeach occun'ed at legist K times.
Thus, banana and Balticwill not be considered for similm-ity relation on the basisof tlteir occurrences in the common context of republic,no matter how frequent, unless there are M-1 other suchcommon contexts comparably frequent (there wasn't anyin TREC's WSJ database).
For smaller or narrow domaindatabases M=2 is usually sufficient, e.g., CACM d:ltab:t,~eof computer science abstracts.
For large databases cover-ing a diverse subject matter, like WSJ or SJMN (S,'m JoseMercury News), we used M>_5.
6 This, however, turnedout not to be sufficient.
We would still genemle faMystrong simih'u'ity links between terms such as aerospacemid pharmaceutical where 6 and more comlnon contextswere found, even after a number of comlnon contexts,such ,'is company or market, have already been rejectedbecause they were paired with too msmy different words,and thus had a dispersion ratio too high.
The remainingcommon contexts m'e listed in Figure 3, ~dong with theirGEW scores, all occurring at the head (left) position of apair.CONTEXT (;EW frequency wilhaerospace idutrlnacettticalfilm 0.58 9 22induslry 0.51 84 56sector 0.61 5 9coneem 0.50 130 115analyst 0.62 23 8division 0.53 36 28giant 0.62 15 12Figure 3.
Common (head) contexts for aerospace and idlarmaeeutieal.6 For example &tnana mM Dominican were found to have twocommon contexts: republic and plant, although tiffs second occurred inapparently different senses in Dominican plant and banatla ptattt.When analyzing Figure 3, we should note thatwhile some of the GEW weights are quite low (GEWtakes values between 0 and 1), thus indicating a lowilnportance context, the frequencies with which these con-texts occurred wilh both ter,ns were high and balanced onboth sides (e.g., concern), thus adding to tile slrength ofassociation.
To liher out such casts we established thres-holds for adlnissible values of GEW factor, and disre-Du'ded contexts with entropy weights falling below thethreshold.
In the most recent experiments with WSJ texts,we found that 0.6 is a good threshold.
We also observedthat clustering bead terms using their moditiers as con-texts converges faster and gives generally ntore reliablelinks thai\] when rood terms are clustered using heads ascontext (e.g., in the above example).
In onr experimentwith tile WSJ database, we fotmd that an occurrence of  acommon head context needs to be considered Its eoulri-bttting less to the total context cotint than an occurrenceof a common rood context: we used 0.6 and l, respec-tively.
Using this formtda, terms man and boy in Figure 2share 5.4 contexts (4 head contexts and 3 rood contexts).hlilially, term similmities are organized into clus-ters around a centmid term.
Figure 4 shows top 10 ele-ments (sorted by similarity wflue) of tile chister forpresident.
Note that in this case lhe SIM value drops sud-denly after the second element of the cluster.
Changes inSIM vahle are nsed to deternline cut-off points for clus-ters.
Tile role of GTS factor will be explgfined later.
Sam-ple clusters obtained fi'om approx.
250 MByte (42 millionwords) snbset of WSJ (years 1990-1992) are given inTable 1.It may be worth pointing out that the similarities arccalculated ilsing term co-occurrences in syntaclic ratherthan in document-size contexts, the latter Ix:ing the usualpractice it1 non-linguistic hlstering (e.g., Sparck Jonesand Batlx:r, 1971; Crouch, 1988; Lewis and Croft, 1990).Although the two methods of te,'m clustering inay be COll-sidered mntttally complementary in certaitt situations, webelieve that more and slrouger associations can beobtained tllrough syntactic-context chlstering, givensuflicient alnonnt of data and a reasonably accnralc syu-CI{NTI(OII)presidentTI!RM SIM (Yl'g0.001 Idirector 0.2481 0./1017chaim~;m 0.2,149 0.0028office 0.1689 0.0010m,'ulage O.
1656 0.0007executive 0.1626 0.0012official 0.1612 0.0008head 0.1564 0.0018meml)er 0.1506 0.0014lead 0.1311 (I.0009Figure 4.
A cluster for president.606word dustertakeoverbenefitcapitalstaff"atlraclsensitivespeetllatepresident_ _ +VICeoutlook Il aw Iearnings prffit, revemfe, incomeportfolio asset, invest, loaninflate growth, deniand, earningsituhtstry business, eompatly, marketgrowth increase, rise, gainfirm bank, concern, group, tlnitenviron climate, condition, siluationdebt loan, sectire, botldlawyer attorneyCOltnsel attorney, administrator, secretaryconlpule mac\]llne, software, eqtlO~mentcompetitor riwll, competition, bayeralliance i~artnersI iOl,  veotnre, eoosortiunlbig ktrge, major, bu.ee, significaotfight battle, attack, war, cl allet gebase facile, source, reserve, stqqu~rtshareholder creditor, customer, clientinvestor, stockhohlermerge, bay-out, acquire, bMcompensate, aid, espensecash, fitnd, moneypersonnel, emfloyee,foreehire, draw, woocrucial, difficult, criticalrtimor, tlncertainty, tensiondirector, chairmandeputyfi)recast, t~rospect, trendrule, policy, leg&late, billTahle 1.
Selected chlsters &taiued fronl syntat:lic contexts, derivedfrom approx.
40 millio~l words of WSJ tcxl, wiih weighted Jaceaid for-mula.tactic parser\]?
Nell-syntactic contexts cross sentetlce lmundaries with no fuss,which is hell)ful with shorl, succinct documents (such as CACMabstracts), but less so wilh longer texls; see also (Grlshmali el al., 1986).QUERY I (XPANSIONSitnilltl'ity rdaiions are t,sed to expand user querieswith new lernts, lit an "tttelnpt o make tile tinal Seluchtiuery more colnprehensive (adding synonytlis) and/ormore pointed (adding specializalions).
11 follows that notall similiu'ily relatiolls will be equally useful ill queryexpansion, liar instance, eomplemelltary anti aitlonymousrelaliolts like Ihe one between Australian and Catladitl#l,ftCCel;t aild rejecl, or even gelieralizaliOilS like Iroill(1?
'1"0X13(IC( ~ tO industry may actually hllrin systeln's perlor-nialice, Siliee we Iliay end till retrieviiig many h'relevailldocumenls.
On the olher hand, dalal)ase search is likely tomiss relewtill doctlnlenls if we overlook the fact that vh:edirector Call a lso be depety dit+et?lor, of  that ltlkt'ov('r cglnalso be merge, buy-ottl, or acqtdsition.
We noled that anaverage SOl of similarities generated from it lexl corpusconlahis abotit as many "good" relations (synottylny, spe-cializalion) as "lind" rclaliolts {anlonyiny, conipleinorlla-lion, generalizalion), as seen froin the query exp;lliSiOliviewpoinl.
Therefore aiiy alleinpt Io sepai~ile these twoclasses alid 1o hlerease Ihe proporlion o1 "good" relalionsshotlld result in improved relrieval.
Tills has hldeed heelltJonlirined in our experinlenls where a relalively crlidefiller has visibly hlcreased reiriewil precision.hi order It) creale an appropriate liller, we devised aglobal lerm speciliciiy ineasiiro ((ITS) whidl is calculatedfor each lerili across all conic?is iii which ii occiirs.
Thegeneral philosophy here is thal ti niore specilicword/phrase WOllld h/lYe 11 iilore Iillliled use, i.e., a illOlespecilic term wotild appear iit fewer distinct contexts, hithis respecl, GTS is similar it) tile standard ire'erred tlOCli-met# fi'eqttetu 7 (idJ) measure excepl lhal lerni frequencyix iltt3aStlie(l over  syntactic tlililS Iather Ihall doct l l l lenl  sizeunils.
TenliS with higher GTS vahies are generally coil-sidered more specilic, but the specificily compa,'isotl isonly meanillgful for terms which are already kllown to besimilar.
We bdieve that measuring lerm specilicily overdoeumelli-size contexts (e.g., Sparck Jones, 1972) ,nayiiot fie appropriale iii this case.
In particular, synllax-basedcontexts allow for processint~ lexls without any inlernaldoct in lenl  slr i icl l lre,The new function is calculaled according to the fol-htwing forltiill'i:IICt+(w) * lC#,,(w) if bolll exist(;'I'S (w)=~ lCte(w) if otlly ICte(vv) e.vislsiL I Q ( w ) otherwisewhere (wilh nw, el,,.
> 0):#1wICt+(w) = IC (Iw,_}) =dw(nw+dw- 1)II wIC1?
(w) = IC (I ,w  I) -dw(n,,,+dw- 1)In the ahove, dw is di.~7)ersion f lerm w mlderslood as Ihemmd~er of distinct COlltexls in which w is found.
For anytwo  ternls  W 1 alld w2,  all(l it constant  ~1 > 1, ir(77"S (w2) _> 8t * (;TF (w 1) then w 2 is considered morespeciiic lhall w 1 .
hi addition, ifSlM,,o,.,n(Wl,W2)=fI> 01, where 01 is an elrli)irically607established threshold, then w2 c,'m be added to the querycontaining term w t with weight ~*to, 8 where co is theweight w2 would have if it were present in the query.Simil,'u'ly, if GTS(w2) <~2 * GTS(wL) ,'rodSIM,,orm(wl,w2) =~ > 0:~ (with 82 < 8t ,and 0t < 02) thenwe may consider w~ as synonymous to w~.
All other ela-tions ,are discarded.
For example, the following wereobtained from the WSJ training database:GTS(takeover) =0.00145576GTS(merge) = 0.00094518GTS (buy-out) = 0.00272580GTS(acquire) = 0.00057906withSIM (takeover,merge) = 0.190444SIM (takeover,buy-out) =0.157410SIM (takeover,acquire) = 0.139497SIM (merge,buy-out) = 0.133800SIM (merge,acquire) = 0.263772SIM(buy-out,acquire) = 0.109106Therefore both takeover and buy-out can be used to spe-cialize merge or acquire.
With this filter, the relationshipsbetween takeover ~md buy-out and between merge ~mdacquire ,are either both discarded or accepted assynonymous.
At this time we are unable to tellsynonymous or ne,'u" synonymous relationships from thosewhich ,are prim,wily complement~u-y, e.g., matt ,andwomatt.Filtered simih'u'ity relations create a domain map ofterms.
At present it may cont~fin only two types of links:equiv,'dence (synonymy and near-synonymy) ,and sub-sumption (specification).
Figure 5 shows a small frag-ment of such map derived from lexic,-d relation computedfrom WSJ datab`ase.
The domain map is used to expanduser queries with related terms, either automatically or ina feedback mode by showing the user appropriate p~u'ts ofthe map.cost number easeexp n in~ 'tigat __a l l 'ge /w.
'u i t/ \subsumptionequivalenceFigure 5.
A fragment of the domain map network.
Note the emergingsenses  of 'charge' as 'expense' and 'allege'.s For TREC-2 we used 0=0.2; ,5 varied between l 0 and 100.We should add that the query exp~msion (in thesense considered here, Ibough not quite in the stone way)has been used in information retrieval research befo*'e(e.g., Sp,'trck Jones and Tail, 1984; Harm\[m, 1988), usu-aUy with mixed results.
The main difference between thecurrent approach ,'u~d those previous attempts is that weuse lexico-sernantic evidence for selecting extra terms,while they relied on term co-occurrence within the samedocuments.
In fact we consider these to methods colnple-mentary with the latter being more appropriate forautomatic relevance feedback.
An alternative queryexpansion to is to use term clusters to create new terms,"metaterms", and use them to index the database instead(e.g., Crouch, 1988; Lewis ,and Croft, 1990).
We foundthat the query exp~sion approach gives the system moreflexibility, for inst,'mce, by making room lbr hypertext-style topic exploration via user feedback.CONCLUSIONSWe discussed selected aspecLq our inlormationretrieved system consisting of an advanced NLP moduleand a 'st~mdard' statistical core engine, ht this paper weconcentrated on the problem of automatic generation oflexical correlations among terms which (aloug withappropriate weighting scheme) represent the content ofboth the dat:d)ase documents :rod the user queries.
Since itsuccessful retrieval relies on actual term matches betweenthe queries ,'u~d the documents, it is essential t mt any lexi-cal alternatives of describing a given topic ,are taken intoaccount.
In our system this is achieved through the expan-sion of user's queries with related terms: we addequiwdent ,and more specific terms.
Lexical relationsbetween terms are c;dculated irectly from the databaseand stored in tbe form of a dom~dn map, which thus actsas a domaln-specilic thesaurus.
Query expansion can bedone in the user-feedback mode (with user's assistance)or automatically.
In this latter c~se, local context isexplored to ,assure meaningful exp~msious, i.e., to prevente.g., exp,'mding 'charge' with 'expense' when 'allege' or'blame' is meant, as in the following ex~unple query:Documents will report on corruption, incompetence,on' inefficiency in the m.magement of the UnitedN.
'~litm's st'dT.
Alleg~dions t~l' lnIil|agelnelll railings,as well as Felofls Io StlCh charges ~u'e relevanl.Many problems remain, however, we attempted 1odemonstrate that the architecture described here isnonetbeless viable and h`as practiced significance.
Moreadvanced NLP techniques (including semantic ,'m~dysis)may prove to be still more effective, in the future, how-ever their enormous cost limits ~my experimental evi-dence to small scale tests (e.g., Mauldin, 1991).ACKNOWLEDGEMENTSWe would like to thank Donna Har,n~m of NIST formaking her PRISE system av,'filable to us.
We would ,alsolike to thank R~dph Weischedel and Heidi Fox of BBN forproviding and ,'tssisting in the use of the p~u't of speechtagger.
This paper is based upon work supported by theAdv,'mced Research Project Agency under Contract608N00014-90-J-1851 from the Office of Nawd Research,under Contract N00600-gS-D-3717 from PRC Inc., andthe Nalional Science Foundalion under Gu~mt 1RI-93-02615.
We ~dso acknowledge support from the Canadianlnsti|ule for Robolics and Intelligent Sysletns (IRIS).RI?~I?EI~J,;NCI(SChurch, Kenneth Ward and flanks, Patrick.
1991/.
"Wordassociation orms, mutual informalitm, and lexicogra-phy."
Computational Linguistics, 1611), MIT Press,pp.
22-29.Crotlch, Carolyn J.
1988.
"A cluster-based approach tothesaurus construction."
Proceedings of ACMSIGIR-88, pp.
309-320.Grefcnsleue, Gregory.
1992.
"Use of Syntactic CoulcxtTo Produce Term Association Lists Ik~, TextReh'iewd."
Proceedings of SIGIR-92, Copenhagen,Denmark.
pp.
89-97.Grishm,'m, Ralph, Lynette Hirschman, and Nee T. Nhan.1986.
"Discovery procedures R)r snblangnagc selec-tional patterns: inilial experiments".
Computatiotmll,inguistics, 12(3), pp.
205-215,Ilarman, Donna.
1988.
"Towards inleraclive queryexpansion."
Proceedings of ACM SIGIR-S8, pp.321-331.ltindle, Donald.
1990.
"Noun classiticalion fi'ompredicate-m'gument slructurcs."
l)roc.
28 Meeliug of1he ACI,, Pittsburgh, PA, pp.
268-275.Lewis, David D. and W. Bruce Croft.
1990.
"Term Clus-tering of Syntactic Phrases".
Proceedings of ACMSIGIR-90, pp.
385-405.Mauldin, Michael.
1991.
"Relrieval PerlBrmtmce in Fer-ret: A Conceptual Information Relrieval System.
"Proceedings of ACM SIGIR.-91, pp.
347-355.Sallon, Gerard.
1989.
Automatic Text Processing."
thetransformation, attalysis, (tIM retrieval of infi)rmalio.by computer.
Addison-Wesley, Reading, MA.Sl)arck Jones, Karen.
1972.
"Slalistical interpretation oflcrm specilicity lind ils application in retrieval.
"Journal of Documentation, 28(1 ), pp.
I 1-20.Sparek Jones, K. and E. O. P~arber.
1971.
"What makesatltomatie keyword elassilicalion effective?"
Journalof the Americatz Society for InJbrmatiotz Science,May-June, pp.
166-175.Sparck Jones, K. :ulc.l J. I. Tail.
1984.
"Aulomalic searchterln vm'iant generatioa."
Journal qf I)ocz#nenlaliotL40(1), pp.
50-66.Strzalkowski, Tomek and Barbara Vaulhey.
1992.
"Iulo,'-malion Retricwd Using Robust Natt,ml Langnage Pro-cessing."
Prec.
of Ihe 301h ACL Meeting, Newark,DE, June-July.
pp.
1/)4-111.Slrzalkowski, Tomek.
1992.
"TrP :  A Fasl aM RobustParser lbr Natural L,-mguage."
Proceedings of the14111 lnternalional Couference on C()mputationalLinguistics (COLING), Nantes, Frauce, Jnly 1992. pp.198-204.Strzalkowski, Tomek.
1993.
"Robust Text Processing inAutomated hfformation Relrieval."
Prec.
of ACI,-sponsored workshop on Very Lart, e Coq)ora.
OhioSlate Univ.
Coh\]mbus, Julle 22.Slrzalkowski, Tomek.
1994.
"Document Representationin Natural Language Text Relrieval."
To appear inproceedings of ARPA l luman Language TechnologyWorkshop, Princelon, NJ.
March 8-11.Slrzalkowski, Tomek and Jose Perez-Cm'ballo.
1994.
"Recenl Developments in Natural IAiJIgn,3ge TextRetriewd."
To appem" in proceedings of Sectmd TexlRetrieval Conference (TREC-2), Gailhersbvrg, Md,August 30 - Seplember l, 1993.Slrzalkowski, Tomek, and Peler Scheyen.
1993.
"Ewthla-lion of TI'P F'arscr: a preliminary report."
F'roceed-ings of lnterualional Workshop on Parsing Technolo-gies (lWPT-93), Tilburg, Netherlands and Durbny,Belgium, Angus( 10-13.APPI~,NI)IX: An examlfle queryThe li)llowiug is an example infommtion requesl(based on TREC's lOl)ic 113) and file resulliug query.Except for its inverled document frequency score, eachlerm has a "conlidence level" weight which is set Io 1.0 ifIhe term is fouad in the nser's query, and is less lhau 1.0if the term is added Ihrough an expansion fl'om 1hedomain map.
Only non-negaled terms wilh idf of 6.0 orgreater arc incltlde(I.<title> New ,Space Satellile Applicatim~s<desc> l)ocument will repf)rt on non-traditional p-plicati(ms of space satellite technol{~gy.<hart> A relevant dOCtllncrll will discuss more recerl((~l" emerging applicalions of space satellite technolo-gy.
NOT relewml are such "traditional" ar early sa-tellite age usages as INTELSAT transmission ofvoice ilIId dillll cOtllllltllliGatlolls tel" telephone co in -panics or program feeds fro" established televisionnetwm'ks.
AIs() N()T relewm( are such establishedUSeS c,f sat?lliles as military ,..:omnnlllic.alilms, eaulhlllitlel'a\] i'e~;{}tlrt:e. Illappillg, \[tl ld s\[lppor( OF weathel"fi~rccasling.
A few examples f~f newer applicati~msare the Imikling of private satellite nelworks fi)n'transfer {ff business dala, facsimile Iransmission t~l"newspapers to be printed in mulliple Iocalimls, anddirect Immdcasling of TV signals.
Tile underlyingpurp(~sc of lifts topic is (o collecl inlbrmalion on re-cenl or emerging trends in lhe applicali{m of spacesatellilc lechnology.77?RM IDI."
WEIGIITal)ply+cqui p 18.402237 0.458666satdlite+latest 18.402237 0.25,1058television+slgnal 18.402237 11.359777television+dlrect 18/102237 0.359777apply+equip 18.402237 0.458666broadcast.b-direct 16.402237 1+(100000locatkm+mtultiple 16.402237 1.000000btoadcasl+signal 16.080309 I.
(XI0(X)0supimwFforecast 15.817275 1.000000(hda-t Imsiness I5.817275 l.(K'lO000forecast+internal 15.402238 0.283029trans fea+infom'~ 15.232312 0.5119411Iransfer+dala 14.817275 1.00(11111(1ligme+buslness Id.594883 0.453631609technology+satellite 14.495347 1.00000()transmit4facsimile 14,402238 1.000000exluip+satellite 14.232312 0,458(x66signal+broadcast 13.701797 0,441993signal+iv 13,701797 I.IX)O000signal+television 13.594883 0.813987news+business 13.495347 0.352291netwolk+satellite 13.154310 1.000000develop+network 12.942806 0.409144non+traditional 12.758382 1.000000inform+business 12.729813 0.511940apply+technology 12.471500 1.
(X)O000build+network I 1.212413 1.000C(}Ofacsimile 10.217362 1 .
(~30~.X~Ousage 9,902391 I.O00(X)Onewer 9.306841 1.000000elderly 8.202565 0.361246feed 7.802325 1.000000satellite 7.567767 1.0,(30000underly 7,370192 1.000000transmit 7,299606 1,000000multiple 7.241736 1.
IkqO 0()0broadcast 7.019614 1.
(X)O000location 6.992316 1.000000print 6,351709 1.000000space 6,226376 1.000000transfer 6.155497 1,000000collect 6.126113 1.000000signal 6.080873 1 .
(300000phone 6,072441 0.663414tv 6.003761 1.000000610
