REVERSIB IL ITY  AND MODULARITY  INNATURAL LANGUAGE GENERATIONGfinter NeumannLehrstuhl ffir ComputerlinguistikSFB 314, Projekt N3 BiLDUniversit?t des SaarlandesIm Stadtwald 15D-6600 Saarbrficken 11, FRGneumann@coli.uni-sb.deABSTRACTA consequent use of reversible grammars withinnatural language generation systems has strongimplications for the separation into strategic andtactical components.
A central goal of this paperis to make plausible that a uniform architecturefor grammatical Processing will serve as a basisto achieve more flexible and efficient generationsystems.1 In t roduct ionIn general, the goal of parsing is the derivation ofall possible grammatical structures defined by agrammar of a given string a (i.e.
especially thedetermination of all possible logical forms of o~)and the goal of the corresponding generation taskis the computation, of all possible strings definedby a grammar of a!
given logical form & that arelogically equivalent to ~ (see also (Shieber, 1988),(Calder et al, 1989)).
Recently, there is a strongtendency to use the same grammar for perform-ing both tasks.
Besides more practically moti-vated reasons - obtaining more compact systemsor avoidance of inconsistencies between the inputand output of a system - there are also theoreti-cal (a single mode!
of language behaviour) andpsychological evidences (empirical evidence forshared processors or facilities, cf.
(Garrett, 1982),(Frazier, 1982), (Ja'ckendoff, 1987)) to adopt thisview.From a formal point of view the main interest inobtaining non-directional grammars is the spec-ification of the relationship between strings andlogical forms.
1 According to van Noord (1990),a grammar is reversible if the parsing and gen-eration problem is computable and the relationbetween strings and logical forms is symmetric.In this case parsing and generation are viewed asmutually inverse processes.Furthermore there are also approaches that as-sume that it is possible to use the same algo-rithm for processing the grammar in both direc-tions (e.g.
(Hasida and Isizaki, 1987), (Shieber,1988), (Dymetman et aL, 1990), (Emele and Za-jac, 1990)).
A great advantage of a uniform pro-cess is that a discourse and task independentmodule for grammatical processing is available.This means that during performing both tasksthe same grammatical power is potentially dis-posable (regardless of the actual language use).Nevertheless, in most of the 'real' generationsystems where all aspects of the generation pro-cess of natural language utterances are consid-ered, grammars are used that are especially de-signed for generation purposes (cf.
(Hovy, 1987),(Dale, 1990), (Horacek, 1990), (McKeown el al.,1990), (Reithinger, 1991)).
~The purpose of this paper is to show that theuse of a uniform architecture for grammatical pro-cessing has important influences for the wholegeneration task.
A consequent use of a uniformprocess within a natural anguage generation sys-tem affects the separation into strategic and tacti-11 assume a notion of grammars  that integrate phono-logical, syntactical and semantical levels of description,e.g., (Pollard and Sag, 1987).2But it is important to note here, that most of theproposed grammars are unification-based which is an im-portant common property with respect to current parsinggranmaars.31cal components.
On the one hand, existing prob-lems with this separation emerge, on the otherhand uniform architectures will serve as an im-portant (linguistic) basis to achieve first solutionsfor the problems.In the next section I will discuss importantproblems and restrictions with the modular de-sign of current generation systems and will thenshow why a uniform architecture as the gram-matical basis can contribute to solutions of theproblems.2 Modularity in GenerationSystemsThe Prob lem It is widely accepted to cutdown the problem of natural language generation(NLG) into two subtasks:?
determination f the content of an utterance?
determination f its linguistic realizationThis 'divide and conquer' view of generation isthe base of current architectures of systems.
Withfew exceptions (e.g., (Appelt, 1985)) the followingtwo components are assumed:?
'what to say' part (strategic component)?
'how to say it' part (tactical component)But, as it has been demonstrated bysome au-thors ((hppelt, 1985), (Hovy, 1987), (P~ubinoff,1988), (Neumann, 1991), (Reithinger, 1991))itis not possible to separate the two phases of thegeneration process completely, e.g., in the caseof lexieal gaps, choice between ear synonyms orparaphrases.Currently, in systems where the separation isadvocated the problems are sometimes ' olved'in such a way that the strategic omponent hasto provide all information eeded by the tacticalcomponent to make decisions about lexical andsyntactic hoices (McDonald, 1983), (McKeown,1985), (Busemann, 1990), (Horacek, 1990).
As aconsequence, this implies that the input for tac-tical components i  tailored to determine a goodsentence, making the use of powerful grammaticalprocesses redundant.
In such approaches, tacticalcomponents are only front-ends and the strategiccomponent needs detailed information about thelanguage to use.Hence, they are not separate modules becausethis implies that both components hare thegrammar.
As pointed out in Fodor (1983) one ofthe characteristic properties of a module is thatit is computationally autonomous.
But a rele-vant consideration of computationally autonomyis that modules do not share sources (in our casethe grammar).Looking for More Symmetr ic  Architec-tures To maintain the modular design a moresymmetric division into strategic and tactical sep-aration is needed:?
Strategic omponent ~ primarly concernedwith conceptual decisions?
Tactical component ?
, primarly concernedwith linguistic decisionsA consequence of this view is that the strate-gic component has no detailed information aboutthe specific grammar and lexicon.
This meansthat in general a message which is constructedprecisely enough to satisfy the strategic ompo-nent's goal can be underspecified from the tacticalviewpoint.
For example, if the strategic ompo-nent specifies as input to the tactical componentthat 'Peter loves Maria', and 'Maria' is the cur-rent focus, then in German it is possible to utter:1 Maria wird von Peter geliebt'Maria is loved by Peter'Or2 Maria liebt Peter'Maria, Peter loves'Of course, a 'real' generation system needs tochoose between the possible paraphrases.
Anadequate generation system should avoid to ut-ter 2 because for this utterance there exists alsothe unmarked reading that 'Maria loves Peter'.As long as the strategic omponent has no de-tailed knowledge of a specific grammar it couldnot express 'choose the passive form to avoid am-biguity'.
But then the process can only chooserandomly between paraphrases during generationand this means that the underlying message willpossibly not be conveyed.There is also psychologically grounded evidenceto assume that the input to a tactical componentmight not be necessary and sufficient to make lin-guistic decisions.
This is best observed in exam-32ples of self-correction (Levelt, 1989).
For exam-ple, in the following utterance: a"but aaa, bands like aaa- aaa- aaa- errr-like groups, pot bands, - groups, youknow what I mean like aaa.
"the speaker discovers two words (the near-synonymous 'groulp' and 'band') each of whichcomes close to the underlying concept and hasproblems to decide which one is the most suit-able.
In this case, the problem is because of amis-match between what the strategic compo-nent want to express and what the language iscapable to express (Rubinoff, 1988).Cur rent  Approaches  In order to be able tohandle these proble~ ins, more flexible tactical com-ponents are necessary that are able to handle e.g.underspecified inpht.
In (Hovy, 1987), (Finklerand Neumann, 1989) and (Reithinger, 1991) ap-proaches are described how such more flexiblecomponents can be achieved.
A major point ofthese systems is to assume a bidirectional flowof control betweenl the strategic and the tacticalcomponents.The problem with systems where a high degreeof feedback between the strategic and the tacticalcomponents i necessary in order to perform thegeneration task is that one component could notperform its specific task without the help of theother.
But when the mode of operation of e.g.the tactical component is continuously influencedby feedback from the strategic component thenthe tactical component will lose its autonomy andconsequently this means that it is not a module(see also (LeveR, 1989)).3 Integration of Parsing andGenerationA promising approach for achieving more au-tonomous tactical components i  to integrate gen-eration and parsing in a more strict way.
By thisI mean:?
the use of resulting structures of one direc-tion directly in the other direction,aThis example is taken from Rubinoff (1988) and isoriginally from a corpus of speech collected at the Univer-sity of Pennsylvania.
'?
the use of one mode of operation (e.g., pars-ing) for monitoring the other mode (e.g., gen-eration).A main thesis of this paper is that the bestway to achieve such integrated approach is to usea uniform grammatical process as the linguisticbasis of a tactical component.Use o f  Same St ructures  in Both  Direct ionsIf parsing and generation share the same data (i.e.grammar and lexicon) then it is possible that re-sulting structures of one direction could be useddirectly in the other direction.
For example, dur-ing the generation of paraphrases of the ambigu-ous utterance 'Remove the folder with the sys-tem tools.'
the generator can use directly theanalysed structures of the two NPs 'the folder'and 'the system tools' in the corresponding para-phrases.
In a similiar way parsing and generationof elliptic utterances can also be performed moreefficiently.
For example, consider the followingdialog between person A and B:A: 'Peter comes to the party tonight.
'B: 'Mary, too.
'In order to be able to parse B's utterance A candirectly use parts of the grammatical structure ofhis own utterance in order to complete the ellipticstructure.
4Adaptab i l i ty  to Language Use o f  OthersAnother very important argument for the use ofuniform knowledge sources is the possibility tomodel the assumption that during communica-tion the use of language of one interlocutor isinfluenced by means of the language use of theothers.For example, in a uniform lexicon it does notmatter wether the lexeme was accessed uringparsing or generation.
This means that the use oflinguistic elements of the interlocutor influencesthe choice of lexical material during generationif the frequency of lexemes will serve as a deci-sion criterion.
This will help to choose betweenlexemes which are synonymous in the actual situ-ation or when the semantic input cannot be suffi-ciently specified.
E.g.
some drinking-devices canbe denoted either 'cup' or 'mug' because their4In this particular case, A can use the whole VP 'willcome to the party'.
In general the process is more compli-cated e.g., if B's answer would be 'Mary and John, too'.33shape cannot be interpreted unequivocally.
Anappropriate choice would be to use the same lex-eme that was previously used by the hearer (if noother information is available), in order to ensurethat the same object will be denoted.
In prin-ciple this is also possible for the choice betweenalternative syntactic structures.This adaptability othe use of language of part-ners in communication is one of the sources forthe fact that the global generation process of hu-mans is flexible and efficient.
Of course, adapt-ability is also a kind of co-operative behaviour.This is necessary if new ideas have to be expressedfor which no mutually known linguistic terms ex-ist (e.g., during communication between expertsand novices).
In this case adaptability to the useof language of the hearer is necessary in orderto make possible that the hearer will be able tounderstand the new information.In principle this kind of adaptability meansthat the structures of the input computed uringthe understanding process carry some informa-tion that can be used to parametrize the genera-tion process.
This leads to more flexibility: notall necessary parameters need to be specified inthe input of a generator because decision pointscan also be set during run-time.This dynamic behaviour of a generation systemwill increase fficiency, too.
As McDonald et al(1987) define, one generator design is more effi-cient than another, if it is able to solve the sameproblem with fewer steps.
They argue that"thekey element governing the difficulty of utteranceproduction is the degree of familiarity with thesituation".
The efficiency of the generation pro-cess depends on the competence and experienceone has acquired for a particular situation.
Butto have experience in the use of linguistic objectsthat are adequate in a particular situation meansto be adaptable.Monitor ing As Levelt (1989) pointed out"speakers monitor what they are saying and howthey are saying it", i.e.
they monitor not only formeaning but also for linguistic well-formedness.To be able to monitor what one is saying is veryimportant for processing of underspecified inputand hence for achieving a more balanced ivisonof the generation task (see sec.
2).
For exam-ple to choose between the two paraphrases of theexample in sec.
2, the tactical component couldparse the resulting strings in order to decide tochoose the less ambiguous string 'Mary is lovedby Peter.'
It only needs to know from the strate-gic component that unambiguous tterances arepreferred (as a pragmactic constraint).In Levelt's model parsing and generation areperformed in an isolated way by means of twodifferent grammars.
In such flow of control thecomplete structure has to be generated again ifambiguities are detected that have to be avoided.If, for example an intelligent help-system thatsupports a user by using an operation researchsystem (e.g.
Unix, (Wilensky et al, 1984)), re-ceives as input the utterance "Remove the folderwith the system tools" then the system is not ableto perform the corresponding action directly be-cause it is ambiguous.
But the system could askthe user "Do you mean 'Remove the folder bymeans of the system tools' or 'Remove the folderthat contains the system tools' ".
This situationis summarized in the following figure (LF' andLF" symbolize two readings of S):LF' LF"S: Remove the folder withthe system toolsS~: Remove the folder by means ofthe system toolsS": Remove the folder that containsthe system toolsFigure 1: Relationship between ambiguities andparaphrasesIf parsing and generation are performed in anisolated way then generation of paraphrases canbe very inefficient, because the source of the am-biguous utterance S is not used directly to guidethe generation process.Generat ion of Paraphrases In order to clar-ify, why an integrated approach can help to solvethe problem I will consider the problem of gener-ation of paraphrases in more detail.If a reversible grammar is used in both direc-tions then the links between the strings and logi-34phon : (remove the folder with the system tools)synsem : S\[imp\]head: synsem : VP\[fin\]\[ phon : (the foider) \]dtrs : eomp : ( synsem : N P\[ace\] )adjunct : PP \ [< with the system tools >\]Figure 2: 'with the system tools' in modifier position of the VP" phon : (removesynsem S\[imp\]!head : \[dtrs : .comp : (the folder with the system tools)phon : (remove)synsem : VP\[fin\] \]phon : (the folder with the system tools)synsem : N P\[aec\]\[ head:NP\[< the folder >\] \]dtrs : adjunct : PP\[< with the system tools >\]Figure 3: The same PP as a modifier of the NP 'the folder'cal forms in fig.
1 are bidirectional.
5A first naive algorithm that performs genera-tion of paraphrasds using a reversible grammarcan be described as follows: Suppose S is the in-put for the parser :then the set{(S,!LF'), (S, LF")}is computed.
Now LF' respectively LF" is givenas input to the generator to compute possibleparaphrases.
The sets{(LF', S'), (LF', S))respectively{(LF", S), (LF", S")}result.
By means of comparison of the elementsof the sets obtained during generation with the5It is not of central role here wether the 'competence'grammar is actually compiled in two efficient parsing andgeneration grammars ~ long as the symmetry property isnot affected.
This inh~erent property of a reversible gram-mar is very important in the case of generation of para-phrases because it ensures that the ambiguous structureand the corresponding paxaphrases are related together.If this would not be ~he case then this would mean thatone is only able to generate the paraphrases but not theambiguous structure.set obtained during parsing one can easily deter-mine the two paraphrases S' and S" because ofthe relationship between strings and logical formsdefined by the grammar.This algorithm is naive because of the assump-tion that it is possible to generate all possi-ble paraphrases at once.
Although 'all-parses'algorithms are widley used during parsing innatural language systems a corresponding 'all-paraphrases' trategy is not practicle because ingeneral the search space during generation ismuch larger (which is a consequence of the mod-ular design discussed in sec.
2).Of course, from a formal point of view one isinterested in algorithms that compute all gram-matically well-formed structures - at least poten-tially.
So most of the currently developed gener-ators and uniform algorithms assume - more orless explictly - an all-paraphrases strategy (e.g.,(Shieber, 1988), (Calder et al, 1989), (Shieber etal., 1989), (Dymetman et al, 1990), (Emele andZajac, 1990)).
But from a practical point of viewthey are not directly usable in such specific situ-ations.35More  Su i tab le  S t ra teg ies  A more suitablestrategy would be to generate only one para-phrase for each ambiguous logical form.
As longas parsing and generation axe performed in an iso-lated way the problem with this strategy is thatthere is no control over the choice of paraphrases.In order to make clear this point I will look closerto the underlying structure of the example's ut-terances.The problem why there are two readings is thatthe PP 'with the system folder' can be attachedinto modifier position of the NP 'the folder' (ex-pressing the semantic relation that 'folder' con-tains 'system tools') or of the verb 'remove' (ex-pressing semantically that 'folder' is the instru-ment of the described situation).
Fig.
2 and 3(see above) show the internal grammatical struc-ture in a HPSG-style notation (omitting details,that are not relevant in this context).As long as the source of the ambiguity is notknown it is possible to generate in both cases theutterance 'Remove the folder with the system-tools' as a paraphrase of itself.
Of course, it ispossible to compare the resulting strings with theinput string S. But because the source of the am-biguity is not known the loop between the iso-lated processes must be performed several timesin general.A better strategy would be to recognize rele-vant sources of ambiguities during parsing andto use this information to guide the generationprocess.
Meteer and Shaked (1988) propose anapproach where during the repeated parse of anambiguous utterance potential sources of ambigu-ity can be detected.
For example when in the caseof lexical ambiguity a noun can be associated totwo semantic lasses a so called 'lexical ambigu-ity specialist' records the noun as the ambiguitysource and the two different classes.
These twoclasses are then explicitly used in the generatorinput and are realized as e.g.
modifiers for theambiguous noun.The only common knowledge source for theparaphraser is a high order intensional logic lan-guage called World Model Language.
It serves asthe interface between parser and generator.
Theproblem with this approach is that parsing andgeneration are performed in an isolated way usingtwo different grammars.
If an ambiguous utter-ance S need to be paraphrased S has to be parsedagain.
During this repeated parse all potentialambiguities have to be recognized and recorded(i.e.
have to be monitored) by means of different'ambiguity specialists'.
The problem here is thatalso local ambiguities have to be considered thatare not relevant for the whole structure.An  A l te rnat ive  Approach  I will now de-scribe the basic idea of an approach that is basedon an integrated approach where both tasks sharethe same grammar.
The advantage of this ap-proach is that no repeated parse is necessary tocompute potential ambiguity sources because thedifferent grammatical structures determined ur-ing parsing are used directly to guide the gen-eration process.
By means of this strategy it isalso ensured that an ambiguous utterance is notgenerated as a paraphrase of itself.In principle the algorithm works as follows:During the generation of paraphrases the gen-eration process is monitored in such a way thatthe monitor compares in each step the resultingstructures of the generation process with the cor-responding structures from parsing maintained inthe alternative parse trees (I will now assume thattwo parse trees P1 and P2 corresponding to thestructures given in fig.
2 and 3 are obtained ur-ing parsing).
Suppose that LF' (cf.
fig.
1) isspecified as the input to the generator.
In the casewhere the generator encounters alternative gram-matical structures to be expanded, the monitorguides the generator by means of inspection ofthe corresponding parse trees.
In the case whereactual considered parts pl and p2 of P1 and P2(e.g., same NPs) axe equal the generator has tochoose the same grammatical structure that wasused to build Pl and p~ (or more efficiently thegenerator can use the partial structure directly asa kind of compiled knowledge).
In the case wherea partial structure of e.g.
parse tree P1 has nocorrespondence in P2 (cf.
fig.
2 and 3) an ambi-guity source is detected.
In this case an alterna-tive grammatical structure has to be chosen.
6At this point it should be clear that the easiestway in order to be able to generate 'along parsedstructures' is to use the same grammar in bothdirections.
In this case grammatical structuresobtained during parsing can be used directly torestrict he potential search space during genera-tion.
?Of course, the described algorithm is too restrictive,in order to handle non-structural (e.g.
contextual) para-phrases.
But, I assume that this approach is also appli-cable in the case of lexiccal amibiguities prerequisite wordmeanings are structurally described by means of lexical se-mantics (e.g., Jackendoff's Lexiccal Conceptual Structures( Jackendoff, 1990))36This approach :is not only restricted in caseswhere the input is ambiguous and the para-phrases must contrast he different meanings.
Itcan also be used for self-monitoring when it hasto be checked wel~her a produced utterance S ofan input form LF is ambiguous.
In this case S willbe parsed.
If during parsing e.g., two readings LFt I .
.and LF are deduced LF IS generated again alongthe parse tree obtained for S. Now an utteranceS' can be generated that has the same meaningbut differs with respect o the ambiguity sourceof S.4 Cur rent  WorkWe have now started a project called BiLD (shortfor Bidirectional iLinguistic Deduction) at theUniversity of Saarland (Saarbriicken) where itwill be investigated how an integrated approachk of parsing and generation can be realized effi-ciently by means of a uniform architecture andhow such a model can be used for increasing flex-ibility and efficiency during natural language pro-cessing.The main topic lof the BiLD project is the de-velopment of a uniform parametrized deductionprocess for grammahcal processing.
This processconstitutes the core of a flexible and symetric tac-tical module.
In order to realize the integratedapproach and to obtain a high degree of efficiencyin both directions'we will develop methods for adeclarative encoding of information of control inthe lexicon and grammar.We follow a sign-based approach for the de-scription of linguistic entities based on Head-driven Phrase Structure Grammar (Pollard andSag, 1987) and the variant described in Reape(1989).
Besides theoretical reasons there are alsoreasons with respect o system's design criterionsto adopt this view because all levels of descrip-tions (i.e.
phonological, syntactic and semanticstructure) of lingffistics entities (i.e.
words andphrases) are described simultanueous in a uni-form way by means of partial information struc-tures.
None of the levels of description has aprivileged status but expresses possible mutuallyco-ocurrence r strictions of structures of differentlevels.Furthermore a high degree of lexicalism is as-sumed so that the lexicon as a complex hierachi-cal ordered data Structure plays a central rolein BiLD.
As it has been shown this lexicalizedview supports revei'sibility (el.
(Newman, 1990),(Dymetman et al, 1990)) and the performingof specific processing strategies (e.g., incrementaland parallel generation, (Neumann and Finkler,1990)).The task of the deduction process during gener-ation is to construct the graphemic form of a spec-ified feature description of a semantic form.
Forexample, to yield the utterance "A man sings.
"the deduction process gets as input the semanticfeature structuretel : sing'agens:quant : exists'oar  :restr : \[ pred : man' \]var :and deduces the graphematic structure\[ graph : (A_man_sings.)
\]by means of successive application of lexical andgrammatical information.
In the same way thededuction process computes from the graphe-matic structure an appropriate semantic struc-ture in parsing direction.
A first prototype basedon head-driven bottom-up strategy is now underdevelopment (cf.
(van Noord, 1991)).A major aspect of the BiLD project is thata specific parametrization f the deduction pro-cess is represented in the lexicon as well as in thegrammar to obtain efficient structures of control(Uszkoreit, 1991).
The main idea is that pref-erence values are assigned to the elements (dis-juncts or conjuncts) of feature descriptions.
Forexample, in HPSG all lexical entries are put to-gether into one large disjunctive form.
From apurely declarative point of view these elementsare unordered.
But a preference structure is usedduring processing in order to guide the processof lexical choice efficiently which itself influencesthe grammatical process.5 Conc lus ionA main thesis of this paper was to show that ex-isting problems with the modular design of cur-rent generation systems emerge when a reversiblegrammar is used.
In order to maintain the mod-ular design I have proposed an approach thatis based on a strong integration of parsing andgeneration of grammatical structures using a re-versible grammar and monitoring mechanisms.37By means of such an integrated approach per-forming of e.g.
generation of paraphrases can bedone more easier and efficently.AcknowledgementsThis research was supported by SFB 314, ProjectN3 BiLD.BibliographyDouglas E. Appelt.
Planning English Sentences.Cambridge University Press, Cambridge, 1985.Stefan Busemann.
Generierung natlirlicherSprache mit Generalisierten Phrasenstruktur-Grammatiken.
PhD thesis, University of Saar-land (Saarbriicken), 1990.Jonathan Calder, Mike Reape, and Henk Zeevat.An algorithm for generation in unification cat-egorial grammar.
In Fourth Conference of theEuropean Chapter of the Association for Com-putational Linguistics, pages 233-240, Manch-ester, 1989.Robert Dale.
Generating receipes: An overviewof epicure.
In Robert Dale, Chris Mellish,and Michael Zock, editors, Current Research inNatural Language Generation, pages 229-255.Academic Press, London, 1990.Mare Dymetman, Pierre Isabelle, and FrancoisPerrault.
A symmetrical approach to parsingand generation.
In Proceedings of the 13th In-ternational Conference on Computational Lin-guistics (COLING), pages 90-96, Helsinki,1990.Martin Emele and Po!mi Zajac.
Typed unificationgrammars.
In Proceedings of the 13th Interna-tional Conference on Computational Linguis-tics (COLING), pages 293-298, Helsinki, 1990.Wolfgang Finkler and Giinter Neumann.
Popel-how: A distributed parallel model for incre-mental natural anguage production with feed-back.
In Proceedings of the Eleventh Inter-national Joint Conference on Artificial Intel-ligence, pages 1518-1523, Detroit, 1989.Jerry A. Fodor.
The Modularity of Mind: An Es-say on Faculty Psychology.
A Bradford Book,MIT Press, Cambridge, Massachusetts, 1983.Lyn Frazier.
Shared components of productionand perception.
In M. A. Arbib et al, editor,Neural Models of Language Processes, pages225-236.
Academic Press, New York, 1982.Merrill F. Garrett.
Remarks on the relation be-tween language production and language com-prehension systems.
In M. A. Arbib et al,editor, Neural Models of Language Processes,pages 209-224.
Academic Press, New York,1982.K.
Hssida and S. Isizaki.
Dependency propa-gation: A unified theory of sentence compre-hension and generation.
In Proceedings of theTenth International Joint Conference on Artifi-cial Intelligence, pages 664-670, Mailand, 1987.Helmut Horacek.
The architecture of a genera-tion component in a complete natural anguagedialogue system.
In Robert Dale, Chris Mel-lish, and Michael Zock, editors, Current Re-search in Natural Language Generation, pages193 - 227.
Academic Press, London, 1990.Eduard.
H. Hovy.
Generating Natural Languageunder Pragmatic Constraints.
PhD thesis, YaleUniversity, 1987.Ray Jackendoff.
Consciousness and the Compu-tational Mind.
MIT Press, Cambridge, Mas-sachusetts, 1987.Ray Jackendoff.
Semantic Structures.
MITPress, Cambridge, Massachusetts, 1990.Willem J. M. Levelt.
Speaking: From Intentionto Articulation.
MIT Press, Cambridge, Mas-sachusetts, 1989.David D. McDonald.
Natural language genera-tion as a computational problem: An intro-duction.
In M. Brady and C. Berwiek, edi-tors, Computational Models of Discourse.
MITPress, Cambridge, Massachusetts, 1983.David D. McDonald, Marie W. Meteer, andJames D. Pustejovsky.
Factors contributing toefficiency in natural language generation.
InK.
Kempen, editor, Natural Language Gener-ation: New Results in Artificial Intelligence,Psychology and Linguistics, pages 159-182.Martinus Nijhoff, Dordrecht, 1987.Kathleen R. McKeown.
Text Generation: UsingDiscourse Strategies and Focus Constraints toGenerate Natural Language Text.
CambridgeUniversity Press, Cambridge, 1985.38Kathleen R. McKeown, Michael Elhadad, Yu-miko Fukomoto, Jong Lim, Christine Lom-bardi, Jacques Robin, and Frank Smadja.
Nat-ural language generation in comet.
In RobertDale, Chris Mellish, and Michael Zock, editors,Current Research in Natural Language Genera-tion, pages 103 2 139.
Academic Press, London,1990.Marie Meteer and Varda Shake& Strategies foreffective paraphrasing.
In Proceedings of the12th International Conference on Computa-tional Linguistics (COLING), Budapest, 1988.Gfinter Neumann.
A bidirectional model for nat-ural language processing.
In Fifth Conferenceof the Europea'n Chapter of the Associationfor Computational Linguistics, pages 245-250,Berlin, 1991.Gfinter Neumann and Wolfgang Finkler.
A head-driven approach to incremental and parallelgeneration of syntactic structures.
In Proceed-ings of the 13th International Conference onComputational Linguistics (COLING), pages288-293, Helsinki, 1990.Paula Newman.
Towards convenient bi-directional grammar formalisms.
In Proceed-ings of the 13th International Conference onComputational iLinguistics (COLING), pages294-298, Helsinki , 1990.Carl Pollard and ivan Sag.
Information BasedSyntax and Semantics, Volume 1.
Center forthe Study of Language and Information Stan-ford, 1987.Mike Reape.
A logical treatment of semi-freeword order and bounded discontinuous con-stituency.
In FOurth Conference of the Euro-pean Chapter of the Association for Computa-tional Linguistics , pages 103-110, Manchester,1989.Norbert Reithinger.
Popel: A parallel and in-cremental natural anguage generation system.In C. L. Paris et al, editor, Natural LanguageGeneration in Artificial Intelligence and Com-putational Linguistics, pages 179-199.
Kluwer,1991.Robert Rubinoff.
A cooperative model of strat-egy and tactics in generation.
In Paperpresented at the' Fourth International Work-shop on Natural: Language Generation, SantaCatalina Island, 1988.Stuart M. Shieber.
A uniform architecture forparsing and generation.
In Proceedings of the12th International Conference on Computa-tional Linguistics (COLING), Budapest, 1988.Stuart M. Shieber, Gertjan van Noord, Robert C.Moore, and Fernando C.N.
Pereira.
Asemantic-head-driven generation algorithm forunification based formalisms.
In 27th AnnualMeeting of the Association for ComputationalLinguistics, Vancouver, 1989.Hans Uszkoreit.
Strategies for adding control in-formation to declarative grammars.
In 29thAnnual Meeting of the Association for Com-putational Linguistics, Berkeley, 1991.Gertjan van Noord.
Reversible unification-basedmachine translation.
In Proceedings of the13th International Conference on Computa-tional Linguistics (COLING), Helsinki, 1990.Gertjan van Noord.
Towards uniform process-ing for constraint-based categorial grammars.In Proceedings of the ACL Workshop on Re-versible Grammar in Natural Language Pro-cessing, Berkeley, 1991.R.
Wilensky, Y. Arens, and D. Chin.
Talking tounix in english: An overview of uc.
Communi-cations of the ACM, pages 574 -593, 1984.39
