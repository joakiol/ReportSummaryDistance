Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 663?672,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsNeutralizing Linguistically Problematic Annotationsin Unsupervised Dependency Parsing EvaluationRoy Schwartz1 Omri Abend1?
Roi Reichart2 Ari Rappoport11Institute of Computer ScienceHebrew University of Jerusalem{roys02|omria01|arir}@cs.huji.ac.il2Computer Science and Artificial Intelligence LaboratoryMassachusetts Institute of Technologyroiri@csail.mit.eduAbstractDependency parsing is a central NLP task.
Inthis paper we show that the common eval-uation for unsupervised dependency parsingis highly sensitive to problematic annotations.We show that for three leading unsupervisedparsers (Klein and Manning, 2004; Cohen andSmith, 2009; Spitkovsky et al, 2010a), a smallset of parameters can be found whose mod-ification yields a significant improvement instandard evaluation measures.
These param-eters correspond to local cases where no lin-guistic consensus exists as to the proper goldannotation.
Therefore, the standard evaluationdoes not provide a true indication of algorithmquality.
We present a new measure, NeutralEdge Direction (NED), and show that it greatlyreduces this undesired phenomenon.1 IntroductionUnsupervised induction of dependency parsers is amajor NLP task that attracts a substantial amountof research (Klein and Manning, 2004; Cohen etal., 2008; Headden et al, 2009; Spitkovsky et al,2010a; Gillenwater et al, 2010; Berg-Kirkpatricket al, 2010; Blunsom and Cohn, 2010, inter alia).Parser quality is usually evaluated by comparing itsoutput to a gold standard whose annotations are lin-guistically motivated.
However, there are cases inwhich there is no linguistic consensus as to what thecorrect annotation is (Ku?bler et al, 2009).
Examplesinclude which verb is the head in a verb group struc-ture (e.g., ?can?
or ?eat?
in ?can eat?
), and which?
Omri Abend is grateful to the Azrieli Foundation for theaward of an Azrieli Fellowship.noun is the head in a sequence of proper nouns (e.g.,?John?
or ?Doe?
in ?John Doe?).
We refer to suchannotations as (linguistically) problematic.
For suchcases, evaluation measures should not punish the al-gorithm for deviating from the gold standard.In this paper we show that the evaluation mea-sures reported in current works are highly sensitiveto the annotation in problematic cases, and proposea simple new measure that greatly neutralizes theproblem.We start from the following observation: for threeleading algorithms (Klein and Manning, 2004; Co-hen and Smith, 2009; Spitkovsky et al, 2010a), asmall set (at most 18 out of a few thousands) of pa-rameters can be found whose modification dramati-cally improves the standard evaluation measures (theattachment score measure by 9.3-15.1%, and theundirected measure by a smaller but still significant1.3-7.7%).
The phenomenon is implementation in-dependent, occurring with several algorithms basedon a fundamental probabilistic dependency model1.We show that these parameter changes can bemapped to edge direction changes in local structuresin the dependency graph, and that these correspondto problematic annotations.
Thus, the standard eval-uation measures do not reflect the true quality of theevaluated algorithm.We explain why the standard undirected evalua-tion measure is in fact sensitive to such edge direc-1It is also language-independent; we have produced it in fivedifferent languages: English, Czech, Japanese, Portuguese, andTurkish.
Due to space considerations, in this paper we focuson English, because it is the most studied language for this taskand the most practically useful one at present.663tion changes, and present a new evaluation measure,Neutral Edge Direction (NED), which greatly allevi-ates the problem by ignoring the edge direction in lo-cal structures.
Using NED, manual modifications ofmodel parameters always yields small performancedifferences.
Moreover, NED sometimes punishessuch manual parameter tweaking by yielding worseresults.
We explain this behavior using an exper-iment revealing that NED always prefers the struc-tures that are more consistent with the modeling as-sumptions lying in the basis of the algorithm.
Whenmanual parameter modification is done against thispreference, the NED results decrease.The contributions of this paper are as follows.First, we show the impact of a small number of an-notation decisions on the performance of unsuper-vised dependency parsers.
Second, we observe thatoften these decisions are linguistically controversialand therefore this impact is misleading.
This revealsa problem in the common evaluation of unsuper-vised dependency parsing.
This is further demon-strated by noting that recent papers evaluate the taskusing three gold standards which differ in such deci-sions and which yield substantially different results.Third, we present the NED measure, which is agnos-tic to errors arising from choosing the non-gold di-rection in such cases.Section 2 reviews related work.
Section 3 de-scribes the performed parameter modifications.
Sec-tion 4 discusses the linguistic controversies in anno-tating problematic dependency structures.
Section 5presents NED.
Section 6 describes experiments withit.
A discussion is given in Section 7.2 Related WorkGrammar induction received considerable attentionover the years (see (Clark, 2001; Klein, 2005) forreviews).
For unsupervised dependency parsing, theDependency Model with Valence (DMV) (Klein andManning, 2004) was the first to beat the simpleright-branching baseline.
A technical description ofDMV is given at the end of this section.The great majority of recent works, includingthose experimented with in this paper, are elabora-tions of DMV.
Smith and Eisner (2005) improvedthe DMV results by generalizing the function maxi-mized by DMV?s EM training algorithm.
Smith andEisner (2006) used a structural locality bias, experi-menting on five languages.
Cohen et al (2008) ex-tended DMV by using a variational EM training al-gorithm and adding logistic normal priors.
Cohenand Smith (2009, 2010) further extended it by us-ing a shared logistic normal prior which provided anew way to encode the knowledge that some POStags are more similar than others.
A bilingual jointlearning further improved their performance.Headden et al (2009) obtained the best reportedresults on WSJ10 by using a lexical extension ofDMV.
Gillenwater et al (2010) used posterior reg-ularization to bias the training towards a small num-ber of parent-child combinations.
Berg-Kirkpatricket al (2010) added new features to the M step of theDMV EM procedure.
Berg-Kirkpatrick and Klein(2010) used a phylogenetic tree to model parame-ter drift between different languages.
Spitkovskyet al (2010a) explored several training protocolsfor DMV.
Spitkovsky et al (2010c) showed thebenefits of Viterbi (?hard?)
EM to DMV training.Spitkovsky et al (2010b) presented a novel lightly-supervised approach that used hyper-text mark-upannotation of web-pages to train DMV.A few non-DMV-based works were recently pre-sented.
Daume?
III (2009) used shift-reduce tech-niques.
Blunsom and Cohn (2010) used tree sub-stitution grammar to achieve best results on WSJ?.Druck et al (2009) took a semi-supervised ap-proach, using a set of rules such as ?A noun is usu-ally the parent of a determiner which is to its left?,experimenting on several languages.
Naseem et al(2010) further extended this idea by using a singleset of rules which globally applies to six differentlanguages.
The latter used a model similar to DMV.The controversial nature of some dependencystructures was discussed in (Nivre, 2006; Ku?bleret al, 2009).
Klein (2005) discussed controversialconstituency structures and the evaluation problemsstemming from them, stressing the importance of aconsistent standard of evaluation.A few works explored the effects of annotationconventions on parsing performance.
Nilsson etal.
(2006) transformed the dependency annotationsof coordinations and verb groups in the PragueTreeBank.
They trained the supervised MaltParser(Nivre et al, 2006) on the transformed data, parsedthe test data and re-transformed the resulting parse,664w3 w2 w1(a)w3 w2 w1(b)Figure 1: A dependency structure on the wordsw1, w2, w3 before (Figure 1(a)) and after (Figure 1(b))an edge-flip of w2?w1.thus improving performance.
Klein and Manning(2004) observed that a large portion of their errors iscaused by predicting the wrong direction of the edgebetween a noun and its determiner.
Ku?bler (2005)compared two different conversion schemes in Ger-man supervised constituency parsing and found oneto have positive influence on parsing quality.Dependency Model with Valence (DMV).
DMV(Klein and Manning, 2004) defines a probabilisticgrammar for unlabeled dependency structures.
It isdefined as follows: the root of the sentence is firstgenerated, and then each head recursively generatesits right and left dependents.
The parameters of themodel are of two types: PSTOP and PATTACH .PSTOP (dir, h, adj) determines the probability tostop generating arguments, and is conditioned on 3arguments: the head h, the direction dir ((L)eftor (R)ight) and adjacency adj (whether the headalready has dependents ((Y )es) in direction dir ornot ((N)o)).
PATTACH(arg|h, dir) determines theprobability to generate arg as head h?s dependent indirection dir.3 Significant Effects of Edge FlippingIn this section we present recurring error patternsin some of the leading unsupervised dependencyparsers.
These patterns are all local, confined to asequence of up to three words (but mainly of justtwo consecutive words).
They can often be mendedby changing the directions of a few types of edges.The modified parameters described in this sectionwere handpicked to improve performance: we ex-amined the local parser errors occurring the largestnumber of times, and found the corresponding pa-rameters.
Note that this is a valid methodology,since our goal is not to design a new algorithm butto demonstrate that modifying a small set of param-eters can yield a major performance boost and even-tually discover problems with evaluation methods oralgorithms.IPRPwantVBPtoTOeatVB.ROOTFigure 2: A parse of the sentence ?I want to eat?, before(straight line) and after (dashed line) an edge-flip of theedge ?to??
?eat?.We start with a few definitions.
Consider Fig-ure 1(a) that shows a dependency structure on thewords w1, w2, w3.
Edge flipping (henceforth, edge-flip) the edge w2?w1 is the following modificationof a parse tree: (1) setting w2?s parent as w1 (insteadof the other way around), and (2) setting w1?s par-ent as w3 (instead of the edge w3?w2).
Figure 1(b)shows the dependency structure after the edge-flip.Note that (1) imposes setting a new parent to w2,as otherwise it would have had no parent.
Settingthis parent to be w3 is the minimal modification ofthe original parse, since it does not change the at-tachment of the structure [w2, w1] to the rest of thesentence, but only the direction of the internal edge.Figure 2 presents a parse of the sentence ?I wantto eat?, before and after an edge-flip of the edge?to??
?eat?.Since unsupervised dependency parsers are gen-erally structure prediction models, the predictionsof the parse edges are not independent.
Therefore,there is no single parameter which completely con-trols the edge direction, and hence there is no directway to perform an edge-flip by parameter modifica-tion.
However, setting extreme values for the param-eters controlling the direction of a certain edge typecreates a strong preference towards one of the direc-tions, and effectively determines the edge direction.This procedure is henceforth termed parameter-flip.We show that by performing a few parameter-flips, a substantial improvement in the attachmentscore can be obtained.
Results are reported for threealgorithms.Parameter Changes.
All the works experimentedwith in this paper are not lexical and use sequencesof POS tags as their input.
In addition, they all usethe DMV parameter set (PSTOP and PATTACH) forparsing.
We will henceforth refer to this set, condi-tioned on POS tags, as the model parameter set.We show how an edge in the dependency graphis encoded using the DMV parameters.
Say the665model prefers setting ?to?
(POS tag: TO) as a de-pendent of the infinitive verb (POS tag: V B) to itsright (e.g., ?to eat?).
This is reflected by a highvalue of PATTACH(TO|V B,L), a low value ofPATTACH(V B|TO,R), since ?to?
tends to be a leftdependent of the verb and not the other way around,and a low value of PSTOP (V B,L,N), as the verbusually has at least one left argument (i.e., ?to?
).A parameter-flip of w1?w2 is hence performedby setting PATTACH(w2|w1, R) to a very lowvalue and PATTACH(w1|w2, L) to a very highvalue.
When the modifications to PATTACHare insufficient to modify the edge direction,PSTOP (w2, L,N) is set to a very low value andPSTOP (w1, R,N) to a very high value2.Table 1 describes the changes made for the threealgorithms.
The ?+?
signs in the table correspond toedges in which the algorithm disagreed with the goldstandard, and were thus modified.
Similarly, the ??
?signs in the table correspond to edges in which thealgorithm agreed with the gold standard, and werethus not modified.
The number of modified param-eters does not exceed 18 (out of a few thousands).The Freq.
column in the table shows the percent-age of the tokens in sections 2-21 of PTB WSJ thatparticipate in each structure.
Equivalently, the per-centage of edges in the corpus which are of eitherof the types appearing in the Orig.
Edge column.As the table shows, the modified structures cover asignificant portion of the tokens.
Indeed, 42.9% ofthe tokens in the corpus participate in at least one ofthem3.Experimenting with Edge Flipping.
We experi-mented with three DMV-based algorithms: a repli-cation of (Klein and Manning, 2004), as appears in(Cohen et al, 2008) (henceforth, km04), Cohen andSmith (2009) (henceforth, cs09), and Spitkovsky etal.
(2010a) (henceforth, saj10a).
Decoding is doneusing the Viterbi algorithm4.
For each of these algo-rithms we present the performance gain when com-pared to the original parameters.The training set is sections 2-21 of the Wall Street2Note that this yields unnormalized models.
Again, this isjustified since the resulting model is only used as a basis fordiscussion and is not a fully fledged algorithm.3Some tokens participate in more than one structure.4http://www.cs.cmu.edu/?scohen/parser.html.Structure Freq.
Orig.
Edge km04 cs09 saj10aCoordination(?John & Mary?)
2.9% CC?NNP ?
+ ?PrepositionalPhrase (?inthe house?
)32.7%DT?NN + + +DT?NNP ?
+ +DT?NNS ?
?
+IN?DT + + ?IN?NN + + ?IN?NNP + ?
?IN?NNS ?
+ ?PRP$?NN ?
?
+Modal Verb(?can eat?)
2.4% MD?V B ?
+ ?Infinitive Verb(?to eat?)
4.5% TO?V B ?
+ +Proper NameSequence(?John Doe?
)18.5% NNP?NNP + ?
?Table 1: Parameter changes for the three algorithms.
TheFreq.
column shows what percentage of the tokens in sec-tions 2-21 of PTB WSJ participate in each structure.
TheOrig.
column indicates the original edge.
The modifiededge is of the opposite direction.
The other columns showthe different algorithms: km04: basic DMV model (repli-cation of (Klein and Manning, 2004)); cs09; (Cohen andSmith, 2009); saj10a: (Spitkovsky et al, 2010a).Journal Penn TreeBank (Marcus et al, 1993).
Test-ing is done on section 23.
The constituency annota-tion was converted to dependencies using the rulesof (Yamada and Matsumoto, 2003)5.Following standard practice, we present the at-tachment score (i.e., percentage of words that have acorrect head) of each algorithm, with both the origi-nal parameters and the modified ones.
We presentresults both on all sentences and on sentences oflength ?
10, excluding punctuation.Table 2 shows results for all algorithms6.
Theperformance difference between the original and themodified parameter set is considerable for all datasets, where differences exceed 9.3%, and go up to15.1%.
These are enormous differences from theperspective of current algorithm evaluation results.4 Linguistically Problematic AnnotationsIn this section, we discuss the controversial natureof the annotation in the modified structures (Ku?bler5http://www.jaist.ac.jp/?h-yamada/6Results are slightly worse than the ones published in theoriginal papers due to the different decoding algorithms (cs09use MBR while we used Viterbi) and a different conversion pro-cedure (saj10a used (Collins, 1999) and not (Yamada and Mat-sumoto, 2003)) ; see Section 5.666Algo.
?
10 ?
?Orig.
Mod.
?
Orig.
Mod.
?km04 45.8 59.8 14 34.6 43.9 9.3cs09 60.9 72.9 12 39.9 54.6 14.7saj10a 54.7 69.8 15.1 41.6 54.3 12.7Table 2: Results of the original (Orig.
columns), themodified (Mod.
columns) parameter sets and their dif-ference (?
columns) for the three algorithms.et al, 2009).
We remind the reader that structuresfor which no linguistic consensus exists as to theircorrect annotation are referred to as (linguistically)problematic.We begin by showing that all the structures mod-ified are indeed linguistically problematic.
We thennote that these controversies are reflected in the eval-uation of this task, resulting in three, significantlydifferent, gold standards currently in use.Coordination Structures are composed of twoproper nouns, separated by a conjunctor (e.g., ?Johnand Mary?).
It is not clear which token should be thehead of this structure, if any (Nilsson et al, 2006).Prepositional Phrases (e.g., ?in the house?
or ?inRome?
), where every word is a reasonable candidateto head this structure.
For example, in the annotationscheme used by (Collins, 1999) the preposition is thehead, in the scheme used by (Johansson and Nugues,2007) the noun is the head, while TUT annotation,presented in (Bosco and Lombardo, 2004), takes thedeterminer to be the noun?s head.Verb Groups are composed of a verb and an aux-iliary or a modal verb (e.g., ?can eat?).
Someschemes choose the modal as the head (Collins,1999), others choose the verb (Rambow et al, 2002).Infinitive Verbs (e.g., ?to eat?)
are also in contro-versy, as in (Yamada and Matsumoto, 2003) the verbis the head while in (Collins, 1999; Bosco and Lom-bardo, 2004) the ?to?
token is the head.Sequences of Proper Nouns (e.g., ?John Doe?
)are also subject to debate, as PTB?s scheme takes thelast proper noun as the head, and BIO?s scheme de-fines a more complex scheme (Dredze et al, 2007).Evaluation Inconsistency Across Papers.
A factthat may not be recognized by some readers is thatcomparing the results of unsupervised dependencyparsers across different papers is not directly pos-sible, since different papers use different gold stan-dard annotations even when they are all derived fromthe Penn Treebank constituency annotation.
Thishappens because they use different rules for con-verting constituency annotation to dependency an-notation.
A probable explanation for this fact is thatpeople have tried to correct linguistically problem-atic annotations in different ways, which is why wenote this issue here7.There are three different annotation schemesin current use: (1) Collins head rules (Collins,1999), used in e.g., (Berg-Kirkpatrick et al, 2010;Spitkovsky et al, 2010a); (2) Conversion rules of(Yamada and Matsumoto, 2003), used in e.g., (Co-hen and Smith, 2009; Gillenwater et al, 2010); (3)Conversion rules of (Johansson and Nugues, 2007)used, e.g., in the CoNLL shared task 2007 (Nivre etal., 2007) and in (Blunsom and Cohn, 2010).The differences between the schemes are substan-tial.
For instance, 14.4% of section 23 is tagged dif-ferently by (1) and (2)8.5 The Neutral Edge Direction (NED)MeasureAs shown in the previous sections, the annotationof problematic edges can substantially affect perfor-mance.
This was briefly discussed in (Klein andManning, 2004), which used undirected evaluationas a measure which is less sensitive to alternativeannotations.
Undirected accuracy was commonlyused since to assess the performance of unsuper-vised parsers (e.g., (Smith and Eisner, 2006; Head-den et al, 2008; Spitkovsky et al, 2010a)) but alsoof supervised ones (Wang et al, 2005; Wang et al,2006).
In this section we discuss why this measureis in fact not indifferent to edge-flips and propose anew measure, Neutral Edge Direction (NED).7Indeed, half a dozen flags in the LTH Constituent-to-Dependency Conversion Tool (Johansson and Nugues, 2007)are used to control the conversion in problematic cases.8In our experiments we used the scheme of (Yamada andMatsumoto, 2003), see Section 3.
The significant effects ofedge flipping were observed with the other two schemes as well.667w1w2w3(a)w1w3w2(b)w4w3w2(c)Figure 3: A dependency structure on the wordsw1, w2, w3 before (Figure 3(a)) and after (Figure 3(b)) anedge-flip of w2?w3, and when the direction of the edgebetween w2 and w3 is switched and the new parent of w3is set to be some other word, w4 (Figure 3(c)).Undirected Evaluation.
The measure is definedas follows: traverse over the tokens and mark a cor-rect attachment if the token?s induced parent is either(1) its gold parent or (2) its gold child.
The score isthe ratio of correct attachments and the number oftokens.We show that this measure does not ignore edge-flips.
Consider Figure 3 that shows a depen-dency structure on the words w1, w2, w3 before (Fig-ure 3(a)) and after (Figure 3(b)) an edge-flip ofw2?w3.
Assume that 3(a) is the gold standard andthat 3(b) is the induced parse.
Consider w2.
Itsinduced parent (w3) is its gold child, and thus undi-rected evaluation does not consider it an error.
Onthe other hand, w3 is assigned w2?s gold parent, w1.This is considered an error, since w1 is neither w3?sgold parent (as it is w2), nor its gold child9.
There-fore, one of the two tokens involved in the edge-flipis penalized by the measure.Recall the example ?I want to eat?
and the edge-flip of the edge ?to???eat?
(Figure 2).
As ?to?
?sparent in the induced graph (?want?)
is neither itsgold parent nor its gold child, the undirected evalu-ation measure marks it as an error.
This is an exam-ple where an edge-flip in a problematic edge, whichshould not be considered an error, was in fact con-sidered an error by undirected evaluation.Neutral Edge Direction (NED).
The NED measureis a simple extension of the undirected evaluationmeasure10.
Unlike undirected evaluation, NED ig-nores all errors directly resulting from an edge-flip.9Otherwise, the gold parse would have contained aw1?w2?w3?w1 cycle.10An implementation of NED is available athttp://www.cs.huji.ac.il/?roys02/software/ned.htmlNED is defined as follows: traverse over the to-kens and mark a correct attachment if the token?s in-duced parent is either (1) its gold parent (2) its goldchild or (3) its gold grandparent.
The score is the ra-tio of correct attachments and the number of tokens.NED, by its definition, ignores edge-flips.
Con-sider again Figure 3, where we assume that 3(a) isthe gold standard and that 3(b) is the induced parse.Much like undirected evaluation, NED will mark theattachment of w2 as correct, since its induced parentis its gold child.
However, unlike undirected evalua-tion, w3?s induced attachment will also be marked ascorrect, as its induced parent is its gold grandparent.Now consider another induced parse in which thedirection of the edge between w2 and w3 is switchedand the w3?s parent is set to be some other word,w4 (Figure 3(c)).
This should be marked as an er-ror, even if the direction of the edge between w2 andw3 is controversial, since the structure [w2, w3] is nolonger a dependent of w1.
It is indeed a NED error.Note that undirected evaluation gives the parses inFigure 3(b) and Figure 3(c) the same score, while ifthe structure [w2, w3] is problematic, there is a majordifference in their correctness.Discussion.
Problematic structures are ubiquitous,with more than 40% of the tokens in PTB WSJappearing in at least one of them (see Section 3).Therefore, even a substantial difference in the at-tachment between two parsers is not necessarily in-dicative of a true quality difference.
However, an at-tachment score difference that persists under NED isan indication of a true quality difference, since gen-erally problematic structures are local (i.e., obtainedby an edge-flip) and NED ignores such errors.Reporting NED alone is insufficient, as obviouslythe edge direction does matter in some cases.
Forexample, in adjective?noun structures (e.g., ?bighouse?
), the correct edge direction is widely agreedupon (?big???house?)
(Ku?bler et al, 2009), andthus choosing the wrong direction should be con-sidered an error.
Therefore, we suggest evaluatingusing both NED and attachment score in order to geta full picture of the parser?s performance.A possible criticism on NED is that it is only in-different to alternative annotations in structures ofsize 2 (e.g., ?to eat?)
and does not necessarily handlelarger problematic structures, such as coordinations668ROOTJohnand Mary(a)ROOTJohnandMary(b)ROOTinhousethe(c)ROOTinthehouse(d)ROOThouseinthe(e)Figure 4: Alternative parses of ?John and Mary?
and ?inthe house?.
Figure 4(a) follows (Collins, 1999), Fig-ure 4(b) follows (Johansson and Nugues, 2007).
Fig-ure 4(c) follows (Collins, 1999; Yamada and Matsumoto,2003).
Figure 4(d) and Figure 4(e) show induced parsesmade by (km04,saj10a) and cs09, respectively.
(see Section 4).
For example, Figure 4(a) and Fig-ure 4(b) present two alternative annotations of thesentence ?John and Mary?.
Assume the parse in Fig-ure 4(a) is the gold parse and that in Figure 4(b) isthe induced parse.
The word ?Mary?
is a NED error,since its induced parent (?and?)
is neither its goldchild nor its gold grandparent.
Thus, NED does notaccept all possible annotations of structures of size3.
On the other hand, using a method which acceptsall possible annotations of structures of size 3 seemstoo permissive.
A better solution may be to modifythe gold standard annotation, so to explicitly anno-tate problematic structures as such.
We defer thisline of research to future work.NED is therefore an evaluation measure which isindifferent to edge-flips, and is consequently lesssensitive to alternative annotations.
We now showthat NED is indifferent to the differences between thestructures originally learned by the algorithms men-tioned in Section 3 and the gold standard annotationin all the problematic cases we consider.Most of the modifications made are edge-flips,and are therefore ignored by NED.
The exceptionsare coordinations and prepositional phrases whichare structures of size 3.
In the former, the alter-native annotations differ only in a single edge-flip(i.e., CC?NNP ), and are thus not NED errors.
Re-garding prepositional phrases, Figure 4(c) presentsthe gold standard of ?in the house?, Figure 4(d) theparse induced by km04 and saj10a and Figure 4(e)the parse induced by cs09.
As the reader can verify,both induced parses receive a perfect NED score.In order to further demonstrate NED?s insensitiv-ity to alternative annotations, we took two of thethree common gold standard annotations (see Sec-tion 4) and evaluated them one against the other.
Weconsidered section 23 of WSJ following the schemeof (Yamada and Matsumoto, 2003) as the gold stan-dard and of (Collins, 1999) as the evaluated set.
Re-sults show that the attachment score is only 85.6%,the undirected accuracy is improved to 90.3%, whilethe NED score is 95.3%.
This shows that NED is sig-nificantly less sensitive to the differences betweenthe different annotation schemes, compared to theother evaluation measures.6 Experimenting with NEDIn this section we show that NED indeed reducesthe performance difference between the original andthe modified parameter sets, thus providing empiri-cal evidence for its validity.
For brevity, we presentresults only for the entire WSJ corpus.
Results onWSJ10 are similar.
The datasets and decoding algo-rithms are the same as those used in Section 3.Table 3 shows the score differences between theparameter sets using attachment score, undirectedevaluation and NED.
A substantial difference per-sists under undirected evaluation: a gap of 7.7% incs09, of 3.5% in saj10a and of 1.3% in km04.The differences are further reduced using NED.This is consistent with our discussion in Section 5,and shows that undirected evaluation only ignoressome of the errors inflicted by edge-flips.For cs09, the difference is substantially reduced,but a 4.2% performance gap remains.
For km04 andsaj10a, the original parameters outperform the newones by 3.6% and 1% respectively.We can see that even when ignoring edge-flips,some difference remains, albeit not necessarily inthe favor of the modified models.
This is becausewe did not directly perform edge-flips, but ratherparameter-flips.
The difference is thus a result ofsecond-order effects stemming from the parameter-flips.
In the next section, we explain why the remain-ing difference is positive for some algorithms (cs09)and negative for others (km04, saj10a).For completeness, Table 4 shows a comparison ofsome of the current state-of-the-art algorithms, usingattachment score, undirected evaluation and NED.The training and test sets are those used in Section 3.The table shows that the relative orderings of the al-gorithms under NED is different than under the other669Algo.
Mod.
?
Orig.Attach.
Undir.
NEDkm04 9.3 (43.9?34.6) 1.3 (54.2?52.9) ?3.6 (63?66.6)cs09 14.7 (54.6?39.9) 7.7 (56.9?49.2) 4.2 (66.8?62.6)saj10a 12.7 (54.3?41.6) 3.5 (59.4?55.9) ?1 (66.8?67.8)Table 3: Differences between the modified and originalparameter sets when evaluated using attachment score(Attach.
), undirected evaluation (Undir.
), and NED.measures.
This is an indication that NED provides adifferent perspective on algorithm quality11 .Algo.
Att10 Att?
Un10 Un?
NED10 NED?bbdk10 66.1 49.6 70.1 56.0 75.5 61.8bc10 67.2 53.6 73 61.7 81.6 70.2cs09 61.5 42 66.9 50.4 81.5 62.9gggtp10 57.1 45 62.5 53.2 80.4 65.1km04 45.8 34.6 60.3 52.9 78.4 66.6saj10a 54.7 41.6 66.5 55.9 78.9 67.8saj10c 63.8 46.1 72.6 58.8 84.2 70.8saj10b?
67.9 48.2 74.0 57.7 86.0 70.7Table 4: A comparison of recent works, using Att (at-tachment score) Un (undirected evaluation) and NED, onsentences of length ?
10 (excluding punctuation) andon all sentences.
The gold standard is obtained usingthe rules of (Yamada and Matsumoto, 2003).
bbdk10:(Berg-Kirkpatrick et al, 2010), bc10: (Blunsom andCohn, 2010), cs09: (Cohen and Smith, 2009), gggtp10:(Gillenwater et al, 2010), km04: A replication of (Kleinand Manning, 2004), saj10a: (Spitkovsky et al, 2010a),saj10c: (Spitkovsky et al, 2010c), saj10b?
: A lightly-supervised algorithm (Spitkovsky et al, 2010b).7 DiscussionIn this paper we explored two ways of dealing withcases in which there is no clear theoretical justifi-cation to prefer one dependency structure over an-other.
Our experiments suggest that it is crucial todeal with such structures if we would like to havea proper evaluation of unsupervised parsing algo-rithms against a gold standard.The first way was to modify the parameters of theparsing algorithms so that in cases where such prob-lematic decisions are to be made they follow the goldstandard annotation.
Indeed, this modification leadsto a substantial improvement in the attachment scoreof the algorithms.11Results may be different than the ones published in theoriginal papers due to the different conversion procedures usedin each work.
See Section 4 for discussion.The second way was to change the evaluation.The NED measure we proposed does not punish fordifferences between gold and induced structures inthe problematic cases.
Indeed, in Section 6 (Table 3)we show that the differences between the originaland modified models are much smaller when eval-uating with NED compared to when evaluating withthe traditional attachment score.As Table 3 reveals, however, even when evaluat-ing with NED, there is still some difference betweenthe original and the modified model, for each of thealgorithms we consider.
Moreover, for two of the al-gorithms (km04 and saj10a) NED prefers the originalmodel while for one (cs09) it prefers the modifiedversion.
In this section we explain these patterns andshow that they are both consistent and predictable.Our hypothesis, for which we provide empiricaljustification, is that in cases where there is no theo-retically preferred annotation, NED prefers the struc-tures that are more learnable by DMV.
That is, NEDgives higher scores to the annotations that better fitthe assumptions and modeling decisions of DMV,the model that lies in the basis of the parsing algo-rithms.To support our hypothesis we perform an experi-ment requiring two preparatory steps for each algo-rithm.
First, we construct a supervised version ofthe algorithm.
This supervised version consists ofthe same statistical model as the original unsuper-vised algorithm, but the parameters are estimated tomaximize the likelihood of a syntactically annotatedtraining corpus, rather than of a plain text corpus.Second, we construct two corpora for the algo-rithm, both consist of the same text and differ onlyin their syntactic annotation.
The first is annotatedwith the gold standard annotation.
The second issimilarly annotated except in the linguistically prob-lematic structures.
We replace these structures withthe ones that would have been created with the un-supervised version of the algorithm (see Table 1 forthe relevant structures for each algorithm)12.
Each12In cases the structures are comprised of a single edge, thesecond corpus is obtained from the gold standard by an edge-flip.
The only exceptions are the cases of the prepositionalphrases.
Their gold standard and the learned structures for eachof the algorithms are shown in Figure 4.
In this case, the sec-ond corpus is obtained from the gold standard by replacing eachprepositional phrase in the gold standard with the corresponding670corpus is divided into a training and a test set.We then train the supervised version of the algo-rithms on each of the training sets.
We parse the testdata twice, once with each of the resulting models.We evaluate both parsed corpora against the corpusannotation from which they originated.The training set of each corpus consists of sec-tions 2?21 of WSJ20 (i.e., WSJ sentences of length?20, excluding punctuation)13 and the test set is sec-tion 23 of WSJ?.
Evaluation is performed usingboth NED and attachment score.
The patterns weobserved are very similar for both.
For brevity, wereport only attachment score results.km04 cs09 saj10aOrig.
Gold Orig.
Gold Orig.
GoldNED,Unsup.
66.6 63 62.6 66.8 67.8 66.8Sup.
71.3 69.9 63.3 69.9 71.8 69.9Table 5: The first line shows the NED results fromSection 6, when using the original parameters (Orig.columns) and the modified parameters (Gold columns).The second line shows the results of the supervised ver-sions of the algorithms using the corpus which agreeswith the unsupervised model in the problematic cases(Orig.)
and the gold standard (Gold).The results of our experiment are presented in Ta-ble 5 along with a comparison to the NED scoresfrom Section 6.
The table clearly demonstrates that aset of parameters (original or modified) is preferredby NED in the unsupervised experiments reported inSection 6 (top line) if and only if the structures pro-duced by this set are better learned by the supervisedversion of the algorithm (bottom line).This observation supports our hypothesis that incases where there is no theoretical preference forone structure over the other, NED (unlike the othermeasures) prefers the structures that are more con-sistent with the modeling assumptions lying in thebasis of the algorithm.
We consider this to be a de-sired property of a measure since a more consistentmodel should be preferred where no theoretical pref-erence exists.learned structure.13In using WSJ20, we follow (Spitkovsky et al, 2010a),which showed that training the DMV on sentences of boundedlength yields a higher score than using the entire corpus.
Weuse it as we aim to use an optimal setting.8 ConclusionIn this paper we showed that the standard evalua-tion of unsupervised dependency parsers is highlysensitive to problematic annotations.
We modified asmall set of parameters that controls the annotationin such problematic cases in three leading parsers.This resulted in a major performance boost, whichis unindicative of a true difference in quality.We presented Neutral Edge Direction (NED), ameasure that is less sensitive to the annotation oflocal structures.
As the problematic structures aregenerally local, NED is less sensitive to their alterna-tive annotations.
In the future, we suggest reportingNED along with the current measures.Acknowledgements.
We would like to thank ShayCohen for his assistance with his implementation ofthe DMV parser and Taylor Berg-Kirkpatrick, PhilBlunsom and Jennifer Gillenwater for providing uswith their data sets.
We would also like to thankValentin I. Spitkovsky for his comments and for pro-viding us with his data sets.ReferencesTaylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?,John DeNero and Dan Klein, 2010.
Painless unsu-pervised learning with features.
In Proc.
of NAACL.Taylor Berg-Kirkpatrick and Dan Klein, 2010.
Phyloge-netic Grammar Induction.
In Proc.
of ACL.Cristina Bosco and Vincenzo Lombardo, 2004.
Depen-dency and relational structure in treebank annotation.In Proc.
of the Workshop on Recent Advances in De-pendency Grammar at COLING?04.Phil Blunsom and Trevor Cohn, 2010.
UnsupervisedInduction of Tree Substitution Grammars for Depen-dency Parsing.
In Proc.
of EMNLP.Shay B. Cohen, Kevin Gimpel and Noah A. Smith, 2008.Logistic Normal Priors for Unsupervised ProbabilisticGrammar Induction.
In Proc.
of NIPS.Shay B. Cohen and Noah A. Smith, 2009.
Shared Logis-tic Normal Distributions for Soft Parameter Tying.
InProc.
of HLT-NAACL.Michael J. Collins, 1999.
Head-driven statistical modelsfor natural language parsing.
Ph.D. thesis, Universityof Pennsylvania, Philadelphia.Alexander Clark, 2001.
Unsupervised language acquisi-tion: theory and practice.
Ph.D. thesis, University ofSussex.Hal Daume?
III, 2009.
Unsupervised search-based struc-tured prediction.
In Proc.
of ICML.671Mark Dredze, John Blitzer, Partha Pratim Talukdar, Kuz-man Ganchev, Joa?o V. Grac?a and Fernando Pereira,2007.
Frustratingly Hard Domain Adaptation for De-pendency Parsing.
In Proc.
of the CoNLL 2007 SharedTask.
EMNLP-CoNLL.Gregory Druck, Gideon Mann and Andrew McCal-lum, 2009.
Semi-supervised learning of dependencyparsers using generalized expectation criteria.
InProc.
of ACL.Jennifer Gillenwater, Kuzman Ganchev, Joa?o V. Grac?a,Ben Taskar and Fernando Preira, 2010.
Sparsity independency grammar induction.
In Proc.
of ACL.William P. Headden III, David McClosky, and EugeneCharniak, 2008.
Evaluating Unsupervised Part-of-Speech Tagging for Grammar Induction.
In Proc.
ofCOLING.William P. Headden III, Mark Johnson and David Mc-Closky, 2009.
Improving unsupervised dependencyparsing with richer contexts and smoothing.
In Proc.of HLT-NAACL.Richard Johansson and Pierre Nugues, 2007.
Ex-tended Constituent-to-Dependency Conversion for En-glish.
In Proc.
of NODALIDA.Dan Klein, 2005.
The unsupervised learning of naturallanguage structure.
Ph.D. thesis, Stanford University.Dan Klein and Christopher Manning, 2004.
Corpus-based induction of syntactic structure: Models of de-pendency and constituency.
In Proc.
of ACL.Sandra Ku?bler, 2005.
How Do Treebank AnnotationSchemes Influence Parsing Results?
Or How Not toCompare Apples And Oranges.
In Proc.
of RANLP.Sandra Ku?bler, R. McDonald and Joakim Nivre, 2009.Dependency Parsing.
Morgan And Claypool Publish-ers.Mitchell Marcus, Beatrice Santorini and Mary AnnMarcinkiewicz, 1993.
Building a large annotated cor-pus of English: The Penn treebank.
ComputationalLinguistics 19:313-330.Tahira Naseem, Harr Chen, Regina Barzilay and MarkJohnson, 2010.
Using universal linguistic knowledgeto guide grammar induction.
In Proc.
of EMNLP.Joakim Nivre, 2006.
Inductive Dependency Parsing.Springer.Joakim Nivre, Johan Hall and Jens Nilsson, 2006.
Malt-Parser: A data-driven parser-generator for depen-dency parsing.
In Proc.
of LREC-2006.Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-ald, Jens Nilsson, Sebastian Riedel and Deniz Yuret,2007.
The CoNLL 2007 shared task on dependencyparsing.
In Proc.
of the CoNLL Shared Task, EMNLP-CoNLL, 2007.Jens Nilsson, Joakim Nivre and Johan Hall, 2006.
Graphtransformations in data-driven dependency parsing.In Proc.
of ACL.Owen Rambow, Cassandre Creswell, Rachel Szekely,Harriet Tauber and Marilyn Walker, 2002.
A depen-dency treebank for English.
In Proc.
of LREC.Noah A. Smith and Jason Eisner, 2005.
Guiding unsu-pervised grammar induction using contrastive estima-tion.
In Proc.
of IJCAI.Noah A. Smith and Jason Eisner, 2006.
Annealing struc-tural bias in multilingual weighted grammar induc-tion.
In Proc.
of ACL.Valentin I. Spitkovsky, Hiyan Alshawi and Daniel Juraf-sky, 2010a.
From Baby Steps to Leapfrog: How ?Lessis More?
in Unsupervised Dependency Parsing.
InProc.
of NAACL-HLT.Valentin I. Spitkovsky, Hiyan Alshawi and Daniel Juraf-sky, 2010b.
Profiting from Mark-Up: Hyper-Text An-notations for Guided Parsing.
In Proc.
of ACL.Valentin I. Spitkovsky, Hiyan Alshawi, Daniel Jurafskyand Christopher D. Manning, 2010c.
Viterbi trainingimproves unsupervised dependency parsing.
In Proc.of CoNLL.Qin Iris Wang, Dale Schuurmans and Dekang Lin, 2005.Strictly Lexical Dependency Parsing.
In IWPT.Qin Iris Wang, Colin Cherry, Dan Lizotte and Dale Schu-urmans, 2006.
Improved Large Margin DependencyParsing via Local Constraints and Laplacian Regular-ization.
In Proc.
of CoNLL.Hiroyasu Yamada and Yuji Matsumoto, 2003.
Statisticaldependency analysis with support vector machines.
InProc.
of the International Workshop on Parsing Tech-nologies.672
