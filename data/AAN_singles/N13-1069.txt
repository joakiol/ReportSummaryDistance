Proceedings of NAACL-HLT 2013, pages 612?616,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsSystematic Comparison of Professional and Crowdsourced ReferenceTranslations for Machine TranslationRabih Zbib, Gretchen Markiewicz, Spyros Matsoukas,Richard Schwartz, John MakhoulRaytheon BBN TechnologiesCambridge, MA 02138, USA{rzbib,gmarkiew,smatsouk,schwartz,makhoul}@bbn.comAbstractWe present a systematic study of the effect ofcrowdsourced translations on Machine Trans-lation performance.
We compare MachineTranslation systems trained on the same databut with translations obtained using Amazon?sMechanical Turk vs. professional translations,and show that the same performance is ob-tained from Mechanical Turk translations at1/5th the cost.
We also show that adding a Me-chanical Turk reference translation of the de-velopment set improves parameter tuning andoutput evaluation.1 IntroductionOnline crowdsourcing services have been shown tobe a cheap and effective data annotation resourcefor various Natural Language Processing (NLP)tasks (Callison-Burch and Dredze, 2010; Zaidan andCallison-Burch, 2011a; Zaidan and Callison-Burch,2011b).
The resulting quality of annotations is highenough to be used for training statistical NLP mod-els, with a saving in cost and time of up to an or-der of magnitude.
Statistical Machine Translation(SMT) is one of the NLP tasks that can benefit fromcrowdsourced annotations.
With appropriate qualitycontrol mechanisms, reference translations collectedby crowdsourcing have been successfully used fortraining and evaluating SMT systems (Zbib et al2012; Zaidan and Callison-Burch, 2011b).In this work, we used Amazon?s Mechanical Turk(MTurk) to obtain alternative reference translationsof four Arabic-English parallel corpora previouslyreleased by the Linguistic Data Consortium (LDC)for the DARPA BOLT program.
This data, totalingover 500K Arabic tokens, was originally collectedfrom web discussion forums and translated profes-sionally to English.
We used alternative MTurktranslations of the same data to train and evalua-tion MT systems; and conducted the first systematicstudy that quantifies the effect of the reference trans-lation process on MT output.
We found that:?
Mechanical Turk can be used to translateenough data for training an MT system at1/10th the price of professional translation, andat a much faster rate.?
Training MT systems on MTurk referencetranslations gives the same performance astraining with professional translations at 20%of the cost.?
A second translation of the development set ob-tained via MTurk improves parameter tuningand output evaluation.2 Previous WorkThere have been several publications on crowd-sourcing data annotation for NLP.
Callison-Burchand Dredze (2010) give an overview of the NAACL-2010 Workshop on using Mechanical Turk for dataannotation.
They describe tasks for which MTurkcan be used, and summarize a set of best practices.They also include references to the workshop con-tributions.Zaidan and Callison-Burch (2011a) created amonolingual Arabic data set rich in dialectal con-tent from user commentaries on newspaper web-sites.
They hired native Arabic speakers on MTurk612to identify the dialect level and used the collected la-bels to train automatic dialect identification systems.They did not translate the collected data, however.Zaidan and Callison-Burch (2011b) obtained mul-tiple translations of the NIST 2009 Urdu-Englishevaluation set using MTurk.
They trained a statis-tical model on a set of features to select among themultiple translations.
They showed that the MTurktranslations selected by their model approached therange of quality of professional translations, and thatthe selected MTurk translations can be used reliablyto score the outputs of different MT systems submit-ted to the NIST evaluation.
Unlike our work, theydid not investigate the use of crowdsourced trans-lations for training or parameter tuning.
Zbib et al(2012) trained a Dialectal Arabic to English MT sys-tem using Mechanical Turk translations.
But thedata they translated on MTurk does not have profes-sional translations to conduct the systematic com-parison we do in this paper.It is well known that scoring MT output againstmultiple references improves MT scores such asBLEU significantly, since it increases the chance ofmatching n-grams between the MT output and thereferences.
Tuning system parameter with multi-ple references also improves machine translation forthe same reason Madnani et al(2007) and Madnaniet al(2008) showed that tuning on additional ref-erences obtained by automatic paraphrasing helpswhen only few tuning references are available.3 Data TranslationThe data we used are Arabic-English parallel cor-pora released by the LDC for the DARPA BOLTPhase 1 program1.
The data was collected fromEgyptian online discussion forums, and consists ofseparate discussion threads, each composed of aninitial user posting and multiple reply postings.
Thedata tends to be bimodal: the first posting in thethread is often formal and expressed in ModernStandard Arabic, while the subsequent threads usea less formal style, and contain colloquial Egyptiandialect.
The data was manually segmented into sen-tence units, and translated professionally.We used non-professional translators hired onMTurk to get second translations.
We used several1Corpora: LDC2012E15, LDC2012E19, LDC2012E55measures to control the quality of translations anddetect cheaters.
Those include the rendering of Ara-bic sentences as images, comparing the output toGoogle Translate and Bing Translator, and other au-tomatic checks.
The quality of individual worker?stranslations was quantified by asking a native Ara-bic speaker judge to score a sample of the Turker?stranslations.
The translation task unit (aka HumanIntelligence Task or HIT) consisted of a sequenceof contiguous sentences from a discussion threadamounting to between 40 and 60 words.
The in-structions were simply to translate the Arabic sourcefully and accurately, and to take surrounding sen-tence segments into account to help resolve ambigu-ities.
The HIT rewards were set to 2.5?
per word.At the end of the effort, we had 26 different work-ers translate 567K Arabic tokens in 4 weeks.
Theresulting translations were less fluent than their pro-fessional counterparts, and 10% shorter on average.The following section presents results of MT exper-iments using the MTurk translations.4 MT ExperimentsThe MT system used is based on a string-to-dependency-tree hierarchical model of Shen etal.
(2008).
Sentence alignment was done usingGIZA++ (Och and Ney, 2003).
Decoder fea-tures include translation probabilities, smoothed lex-ical probabilities, and a dependency tree languagemodel.
Additionally, we used 50,000 sparse, binary-valued source and target features based on Chianget al(2009).
The English language model wastrained on 7 billion words from the LDC Gigawordcorpus and from a web crawl.
We used expectedBLEU maximization (Devlin, 2009) to tune featureweights.We defined a tuning set (3581 segments, 43.3Ktokens) and a test set (4166 segments, 47.7K to-kens) using LDC2012E30, the corpus designatedas a development set by the LDC, augmented witharound 50K Words held out from LDC2012E15and LDC2012E19, to make a development set largeenough to tune the large number of feature weights2.The remaining data was used for training.
We de-fined three nested training sets containing 100K,200K and 400K Arabic tokens respectively, with2Only full forum threads were held out613Training Web-forum Only Newswire(10MW)+Web-forum100KW 200KW 400KW 0KW 100KW 200KW 400KWProf.
refs 17.71 20.23 22.61 22.82 24.05 24.85 25.19MTurk refs 16.41 18.43 20.08 22.82 23.79 24.20 24.51Two Training refs 19.03 21.19 23.06 22.82 24.26 25.19 25.38Add?l Training data - 19.80 21.53 22.82 - 24.31 25.16Table 1: Comparison of the effect of web forum training data when using professional and MTurk reference transla-tions.
All results use professional references for the tuning and test sets.two versions of each set: one with the professionalreference translations for the target, and the otherwith the same source data, but the MTurk transla-tions.
We defined two versions of the test and tuningsets similarly.
We report translation results in termsof lower-case BLEU scores (Papineni et al 2002).4.1 Training Data ReferencesWe first study the effect of training data refer-ences, varying the amount of training data and typeof translations, while using the same professionaltranslation references for tuning and scoring.
Thefirst set of baseline experiments were trained onweb forum data only, using professional transla-tions.
The first line of Table 1 shows that doubling ofthe training data adds 2.5 then 2.3 BLEU points.
Werepeated the experiments, but with MTurk trainingreferences, and saw that the scores are lower by 1.3-2.5 BLEU points, depending on the size of trainingdata, and that the gain obtained from doubling thetraining data decreases to 2.0 and 1.6 BLEU points.The lower MT scores and slower learning curve ofthe MTurk systems are both due to the lower qualityof the translations, and to the mismatch with the pro-fessional development set translations (we discussthis issue further in ?4.3).
However, by interpolationof the MT scores, we find that the same MT perfor-mance can be obtained by using twice the amount ofMTurk translated data as professional data.
Consid-ering that the MTurk translations is 10 times cheaperthan professional translations (2.5?
versus 25-30?
),this constitutes a cost ratio of 5x.We repeated the above experiments, but this timeadded 10 million words of parallel data from theNIST MT 2012 corpora (mostly news) for training.We weighted the web forum part of the training databy a factor of 5.
Note from the results in the righthalf of Table 1 that the newswire data improves theBLEU score by 2.5 to 6.3 BLEU points, depend-ing on the size of the web forum data.
This signif-icant improvement is because some of the web fo-rum user postings are formal and written in MSA(?3).
More relevant to our aims is the comparisonwhen we vary the web forum training references inthe presence of the newswire training.
The differ-ence between the MTurk translation systems and theprofessional translation drops to 0.26-0.68 points.We conclude that in a domain adaptation scenario,where out-of-domain training data (i.e.
newswire)already exists, crowdsourced translations for the in-domain (i.e.
web forum) training data can be usedwith little to no loss in MT performance.4.2 More Data vs.
Multiple TranslationsTo our knowledge no previous work has comparedusing multiple reference translations for trainingdata versus using additional training data of the samesize.
We studied this question by using both transla-tions on the target side of the training data.
Using theMTurk translations in addition to the professionaltranslations in training gave a gain of 0.4 to 1.3BLEU points (bottom half of Table 1).
The gain wassmaller in the presence of the GALE newswire data.When we compared with using the same amount ofdifferent training data instead of multiple references,we saw that training on new data with crowdsourcedtranslations is better: training on two translations of100KW gives 19.03, compared to 19.80 when train-ing on a single translation of 200KW.
The advantageof different-source data drops to 0.34 points whenwe start with 200KW.
With a larger initial corpus,the additional source coverage of new data is not ascritical, and the advantage of more variety on thetarget-side of the extracted translation rules becomesmore competitive.
This coverage is even less criti-cal in the presence of the news data, where the ad-614Training Tuning Test Training Data Size100KW 200KW 400KW 400KW(no lex)Prof. Prof. Prof. 17.71 20.23 22.61 20.01Prof.
Prof. Prof.+MTurk 22.53 25.75 28.38 25.42Prof.
Prof. (len=0.95) Prof.+MTurk 23.63 26.84 29.54 26.17Prof.
Prof.+MTurk Prof.+MTurk 25.26 28.44 30.94 27.22MTurk MTurk MTurk 16.66 18.47 20.35 17.75MTurk MTurk Prof.+MTurk 23.83 26.45 28.66 25.44MTurk MTurk (len=1.05) Prof.+MTurk 23.73 26.19 28.74 25.87MTurk Prof.+MTurk Prof.+MTurk 24.91 27.66 29.78 26.45Table 2: Effect of Tuning and Scoring References on MT.vantage of new web forum source data disappears(lower-right quadrant of Table 1).4.3 Development Data ReferencesSo far, we have focused on varying training dataconditions, and kept the tuning and evaluation con-ditions fixed.
But since we have re-translated thetuning and test sets on MTurk as well, we can studythe effect of their reference translations on MT.
AsTable 2 shows, scoring the MT output using bothreference translations, the BLEU scores increase byover 5 points (and more for the MTurk-trained sys-tem).
This increase by itself is not remarkable.
Whatis important to note is that the gain obtained by dou-bling the amount of training data is larger when mea-sured using the multiple reference test set.
We alsoran experiments with 400KW training data, but withthe lexical smoothing features (Koehn et al 2003;Devlin, 2009) turned off.
The bigger gains show thatimprovements in the MT output (from additionaltraining or new features) can be better measured us-ing a second MTurk reference of the test set.Finally, we study the effect of tuning the systemparameters using both translation references.
Look-ing at the system trained on the professional trans-lations, we see a gain of 2.5 to 2.7 BLEU pointsfrom adding the MTurk references to the tuning set.But as we mentioned earlier, the MTurk transla-tions are shorter than the professional translationsby around 10% on average.
Tuning on both ref-erences, therefore, shortens the system output byaround 5%.
To neutralize the effect of length mis-match, we compared to a fairer baseline tuned onthe professional references only, but we tuned theoutput-to-reference length ratio to be 0.95 (thus pro-ducing a shorter output).
In this case, we see a gainof 1.4 points from adding the MTurk references tothe tuning set.We also used the multiple-reference tuning setto retune the systems trained on MTurk transla-tions.
Comparing that to a baseline that is tuned andscored using MTurk references only, we see a gainof around 1%.
Note, however, that in this case thelength mismach is reversed, and the output of themultiple-reference system is around 5% longer thanthat of the baseline.
If we compare with a baselinethat is tuned with a length ratio of 1.05 (to produce alonger output), we see the gain shrink only slightly.To sum up this section, a second set of refer-ence translations obtained via MTurk makes mea-surements of improvement on the test set more re-liable.
Also, a second set of references for tuningimproves the output of the MT systems trained oneither professional or MTurk references.5 ConclusionWe compared professional and crowdsourced trans-lations of the same data for training, tuning and scor-ing Arabic-English SMT systems.
We showed thatthe crowdsourced translations yield the same MTperformance as professional translations for as lit-tle as 20% of the cost.
We also showed that a sec-ond crowsourced reference translation of the devel-opment set alws for a more accurate evaluation ofMT output.AcknowledgmentsThis work was supported in part by DARPA/IPTOContract No.
HR0011-12-C-0014 under the BOLT615Program.
The views expressed are those of the au-thors and do not reflect the official policy or positionof the Department of Defense or the U.S. Govern-ment.
Distribution Statement A (Approved for Pub-lic Release, Distribution Unlimited).ReferencesChris Callison-Burch and Mark Dredze.
2010.
Creatingspeech and language data with Amazon?s MechanicalTurk.
In Proceedings of the NAACL HLT 2010 Work-shop on Creating Speech and Language Data withAmazon?s Mechanical Turk, pages 1?12, Los Angeles,June.David Chiang, Kevin Knight, and Wei Wang.
2009.11,001 new features for statistical machine translation.In NAACL ?09: Proceedings of the 2009 Human Lan-guage Technology Conference of the North AmericanChapter of the Association for Computational Linguis-tics, Boulder, Colorado.Jacob Devlin.
2009.
Lexical features for statistical ma-chine translation.
Master?s thesis, University of Mary-land, December.P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statisticalphrase-based translation.
In Proceedings of the 2003Human Language Technology Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 48?54, Edmonton, Canada.Nitin Madnani, Necip Fazil, Ayan, Philip Resnik, andBonnie Dorr.
2007.
Using paraphrases for parametertuning in statistical machine translation.
In Proceed-ings of the Second Workshop on Statistical MachineTranslation, pages 120?127, Prague, Czech Republic.Association for Computational Linguistics.Nitin Madnani, Philip Resnik, Bonnie Dorr, and RichardSchwartz.
2008.
Are multiple reference transla-tions necessary?
investigating the value of paraphrasedreference translations in parameter optimization.
InProceedings of the 8th Conf.
of the Association forMachine Translation in the Americas (AMTA 2008),Waikiki, Hawaii, USA.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic evalua-tion of machine translation.
In Proceedings of the 40thAnnual Meeting of the Association for ComputationalLinguistics (ACL), Philadelphia, PA.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.
Anew string-to-dependency machine translation algo-rithm with a target dependency language model.
InProceedings of the 46th Annual Meeting of the As-sociation for Computational Linguistics (ACL), pages577?585, Columbus, Ohio.Omar F. Zaidan and Chris Callison-Burch.
2011a.The Arabic online commentary dataset: an annotateddataset of informal Arabic with high dialectal content.In Proceedings of the 49th Annual Meeting of the As-sociation for Computational Linguistics: Human Lan-guage Technologies, pages 37?41, Portland, Oregon,June.Omar F. Zaidan and Chris Callison-Burch.
2011b.Crowdsourcing translation: Professional quality fromnon-professionals.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Lin-guistics: Human Language Technologies, pages 1220?1229, Portland, Oregon, June.Rabih Zbib, Erika Malchiodi, Jacob Devlin, DavidStallard, Spyros Matsoukas, Richard Schwartz, JohnMakhoul, Omar F. Zaidan, and Chris Callison-Burch.2012.
Machine translation of arabic dialects.
In The2012 Conference of the North American Chapter of theAssociation for Computational Linguistics, Montreal,June.
Association for Computational Linguistics.616
