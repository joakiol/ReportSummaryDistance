Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 225?235,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsLifelong-RL: Lifelong Relaxation Labeling for Separating Entities andAspects in Opinion TargetsLei Shu1, Bing Liu1, Hu Xu1, Annice Kim21Department of Computer Science, University of Illinois at Chicago, USA2Center for Health Policy Science and Tobacco Research, RTI International, USA1{lshu3, liub, hxu48}@uic.edu, 2akim@rti.orgAbstractIt is well-known that opinions have targets.Extracting such targets is an important prob-lem of opinion mining because without know-ing the target of an opinion, the opinion is oflimited use.
So far many algorithms have beenproposed to extract opinion targets.
However,an opinion target can be an entity or an as-pect (part or attribute) of an entity.
An opinionabout an entity is an opinion about the entityas a whole, while an opinion about an aspectis just an opinion about that specific attributeor aspect of an entity.
Thus, opinion targetsshould be separated into entities and aspectsbefore use because they represent very dif-ferent things about opinions.
This paper pro-poses a novel algorithm, called Lifelong-RL,to solve the problem based on lifelong ma-chine learning and relaxation labeling.
Ex-tensive experiments show that the proposedalgorithm Lifelong-RL outperforms baselinemethods markedly.1 IntroductionA core problem of opinion mining or sentiment anal-ysis is to identify each opinion/sentiment target andto classify the opinion/sentiment polarity on the tar-get (Liu, 2012).
For example, in a review sen-tence for a car, one wrote ?Although the engine isslightly weak, this car is great.?
The person is posi-tive (opinion polarity) about the car (opinion target)as a whole, but slightly negative (opinion polarity)about the car?s engine (opinion target).Past research has proposed many techniques toextract opinion targets (we will just call them targetshereafter for simplicity) and also to classify senti-ment polarities on the targets.
However, a target canbe an entity or an aspect (part or attribute) of an en-tity.
?Engine?
in the above sentence is just one as-pect of the car, while ?this car?
refers to the wholecar.
Note that in (Liu, 2012), an entity is called ageneral aspect.
For effective opinion mining, weneed to classify whether a target is an entity or an as-pect because they refer to very different things.
Onecan be positive about the whole entity (car) but neg-ative about some aspects of it (e.g., engine) and viceversa.
This paper aims to perform the target classi-fication task, which, to our knowledge, has not beenattempted before.
Although in supervised extractionone can annotate entities and aspects with separatelabels in the training data to build a model to extractthem separately, in this paper our goal is to help un-supervised target extraction methods to classify tar-gets.
Unsupervised target extraction methods are of-ten preferred because they save the time-consumingdata labeling or annotation step for each domain.Problem Statement: Given a set of opinion tar-gets T = {t1, .
.
.
, tn} extracted from an opinioncorpus d, we want to classify each target ti ?
T intoone of the three classes, entity, aspect, or NIL, whichare called class labels.
NIL means that the target isneither an entity nor an aspect and is used becausetarget extraction algorithms can make mistakes.This paper does not propose a new target extrac-tion algorithm.
We use an existing unsupervisedmethod, called Double Propagation (DP) (Qiu et al,2011), for extraction.
We only focus on target clas-sification after the targets have been extracted.
Notethat an entity here can be a named entity, a prod-225uct category, or an abstract product (e.g., ?this ma-chine?
and ?this product?).
An named entity can bethe name of a brand, a model, or a manufacturer.
Anaspect is a part or attribute of an entity, e.g., ?bat-tery?
and ?price?
of the entity ?camera?.Since our entities not just include the traditionalnamed entities (e.g., ?Microsoft?
and ?Google?)
butalso other expressions that refer to such entities, tra-ditional named entity recognition algorithms are notsufficient.
Pronouns such as ?it,?
?they,?
etc., are notconsidered in this paper as co-reference resolution isout of the scope of this work.We solve this problem in an unsupervised mannerso that there is no need for labor-intensive manuallabeling of the training data.
One key observation ofthe problem is that although entities and aspects aredifferent, they are closely related because aspects areparts or attributes of entities and they often have syn-tactic relationships in a sentence, e.g., ?This phone?sscreen is super.?
Thus it is natural to solve the prob-lem using a relational learning method.
We employthe graph labeling algorithm, Relaxation Labeling(RL) (Hummel and Zucker, 1983), which performsunsupervised belief propagation on a graph.
In ourcase, each target extracted from the given corpus dforms a graph node and each relation identified ind between two targets forms an edge.
With someinitial probability assignments, RL can assign eachtarget node the most probable class label.
Althoughsome other graph labeling methods can be appliedas well, the key issue here is that just using a propa-gation method in isolation is far from sufficient dueto lack of information from the given corpus, whichwe detail in Section 5.
We then employ Lifelong Ma-chine Learning (LML) (Thrun, 1998; Chen and Liu,2014b) to make a major improvement.LML works as follows: The learner has per-formed a number learning tasks in the past andhas retained the knowledge gained so far.
In thenew/current task, it makes use of the past knowledgeto help current learning and problem solving.
SinceRL is unsupervised, we can assume that the systemhas performed the same task on reviews of a largenumber of products/domains (or corpora).
It hasalso saved all the graphs and classification resultsfrom those past domains in a Knowledge Base (KB).It then exploits this past knowledge to help clas-sification in the current task/domain.
We call thiscombined approach of relaxation labeling and LMLLifelong-RL.
The approach is effective because thereis a significant amount of sharing of targets and tar-get relations across domains.LML is different from the classic learningparadigm (supervised or unsupervised) becauseclassic learning has no memory.
It basically runs alearning algorithm on a given data in isolation with-out considering any past learned knowledge (Silveret al, 2013).
LML aims to mimic human learning,which always retains the learned knowledge fromthe past and uses it to help future learning.Our experimental results show that the pro-posed Lifelong-RL system is highly promising.
Theparadigm of LML helps improve the classificationresults greatly.2 Related WorkAlthough many target extraction methods exist (Huand Liu, 2004; Zhuang et al, 2006; Ku et al, 2006;Wang and Wang, 2008; Wu et al, 2009; Lin andHe, 2009; Zhang et al, 2010; Mei et al, 2007; Liet al, 2010; Brody and Elhadad, 2010; Wang et al,2010; Mukherjee and Liu, 2012; Fang and Huang,2012; Zhou et al, 2013; Liu et al, 2013; Poriaet al, 2014), we are not aware of any attempt tosolve the proposed problem.
As mentioned in the in-troduction, although in supervised target extraction,one can annotate entities and aspects with differentlabels, supervised methods need manually labeledtraining data, which is time-consuming and labor-intensive to produce (Jakob and Gurevych, 2010;Choi and Cardie, 2010; Mitchell et al, 2013).
Notethat relaxation labeling was used for sentiment clas-sification in (Popescu and Etzioni, 2007), but not fortarget classification.
More details of opinion miningcan be found in (Liu, 2012; Pang and Lee, 2008).Our work is related to transfer learning (Pan andYang, 2010), which uses the source domain labeleddata to help target domain learning, which has lit-tle or no labeled data.
Our work is not just usinga source domain to help a target domain.
It is acontinuous and cumulative learning process.
Eachnew task can make use of the knowledge learnedfrom all past tasks.
Knowledge learned from thenew task can also help improve learning of any pasttask.
Transfer learning is not continuous, does not226accumulate knowledge over time and cannot im-prove learning in the source domain.
Our work isalso related to multi-task learning (Caruana, 1997),which jointly optimizes a set of related learningtasks.
Clearly, multi-task learning is different as welearn and save information which is more realisticwhen a large number of tasks are involved.Our work is most related to Lifelong MachineLearning (LML).
Traditional LML focuses on su-pervised learning (Thrun, 1998; Ruvolo and Eaton,2013; Chen et al, 2015).
Recent work used LMLin topic modeling (Chen and Liu, 2014a), which isunsupervised.
Basically, they used topics generatedfrom past domains to help current domain model in-ference.
However, they are just for aspect extrac-tion.
So is the method in (Liu et al, 2016).
Theydo not solve our problem.
Their LML methods arealso different from ours as we use a graph and resultsobtained in the past domains to augment the currenttask/domain graph to solve the problem.3 Lifelong-RL: The General FrameworkIn this section, we present the proposed generalframework of lifelong relaxation labeling (Lifelong-RL).
We first give an overview of the relaxation la-beling algorithm, which forms the base.
We thenincorporate it with the LML capability.
The nexttwo sections detail how this general framework isapplied to our proposed task of separating entitiesand aspects in opinion targets.3.1 Relaxation LabelingRelaxation Labeling (RL) is an unsupervised graph-based label propagation algorithm that works iter-atively.
The graph consists of nodes and edges.Each edge represents a binary relationship betweentwo nodes.
Each node ti in the graph is associatedwith a multinomial distribution P (L(ti)) (L(ti) be-ing the label of ti) on a label set Y .
Each edge isassociated with two conditional probability distri-butions P (L(ti)|L(tj)) and P (L(tj)|L(ti)), whereP (L(ti)|L(tj)) represents how the label L(tj) influ-ences the label L(ti) and vice versa.
The neighborsNe(ti) of a node ti are associated with a weight dis-tribution w(tj |ti) with?tj?Ne(ti)w(tj |ti) = 1.Given the initial values of these quantities as in-puts, RL iteratively updates the label distributionof each node until convergence.
Initially, we haveP 0(L(ti)).
Let ?P r+1(L(ti)) be the change ofP (L(ti)) at iteration r+ 1.
Given P r(L(ti)) at iter-ation r, ?P r+1(L(ti)) is computed by:?P r+1(L(ti)) =?tj?Ne(ti)(w(tj |ti)?
?y?Y (P (L(ti)|L(tj) = y)P r(L(tj) = y)))(1)Then, the updated label distribution for iterationr + 1, P r+1(L(ti)), is computed as follows:P r+1(L(ti)) =P r(L(ti))(1+?P r+1(L(ti)))?y?Y P r(L(ti)=y)(1+?P r+1(L(ti)=y))(2)Once RL ends, the final label of node ti is its highestprobable label: L(ti) = argmaxy?Y(P (L(ti) = y)).Note that P (L(ti)|L(tj)) and w(tj |ti) are not up-dated in each RL iteration but only P (L(ti)) is.P (L(ti)|L(tj)), w(tj |ti) and P 0(L(ti)) are pro-vided by the user or computed based on the appli-cation context.
RL uses these values as input anditeratively updates P (L(ti)) based on Equations (1)and (2) until convergence.
Next we discuss how toincorporate LML in RL.3.2 Lifelong Relaxation LabelingFor LML, it is assumed that at any time step, thesystem has worked on u past domain corpora D ={d1, .
.
.
, du}.
For each past domain corpus d ?D, the same Lifelong-RL algorithm was appliedand its results were saved in the Knowledge Base(KB).
Then the algorithm can borrow some usefulprior/past knowledge in the KB to help RL in thenew/current domain du+1.
Once the results of thecurrent domain are produced, they are also added tothe KB for future use.We now detail the specific types of informationor knowledge that can be obtained from the past do-mains to help RL in the future, which should thus bestored in the KB.1.
Prior edges: In many applications, the graphis not given.
Instead, it has to be constructedbased on the data from the new task/domaindata du+1.
However, due to the limited data indu+1, some edges between nodes that shouldbe present are not extracted from the data.
Butsuch edges between the nodes may exist in227some past domains.
Then, those edges and theirassociated probabilities can be borrowed.2.
Prior labels: Some nodes in the current newdomain may also exist in some past domains.Their labels in the past domains are very likelyto be the same as those in the current domain.Then, those prior labels can give us a better ideaabout the initial label probability distributionsof the nodes in the current domain du+1.To leverage those edges and labels from the pastdomains, the system needs to ensure that they arelikely to be correct and applicable to the current taskdomain.
This is a challenge problem.
In the nexttwo sections, we detail how to ensure these to a largeextent in our application context along with how tocompute those initial probabilities.4 Initialization of Relaxation LabelingWe now discuss how the proposed Lifelong-RL gen-eral framework is applied to solve our problem.
Inour case, each node in the graph is an extracted tar-get ti ?
T , and each edge represents a binary re-lationship between two targets.
T is the given setof all opinion targets extracted by an extraction al-gorithm from a review dataset/corpus d. The labelset for each target is Y = {entity, aspect,NIL}.
Inthis section, we describe how to use text clues in thecorpus d to compute P (L(ti)|L(tj)), w(tj |ti) andP 0(L(ti)).
In the next section, we present how thesequantities are improved using prior knowledge fromthe past domains in the LML fashion.4.1 Text Clues for InitializationWe use two kinds of text clues, called type modifiersM(t) and relation modifiers MR to compute the ini-tial label distribution P (L(ti)) and conditional labeldistribution P (L(ti)|L(tj)) respectively.Type Modifier: This has two kinds MT ={mE ,mA}, where mE and mA represent entitymodifier and aspect modifier respectively.
For ex-ample, the word ?this?
as in ?this camera is great?indicates that ?camera?
is probably an entity.
Thus,?this?
is a type modifier indicating M(camera) =mE .
?These?
is also a type modifier.
Aspect mod-ifier is implicitly assumed when the number of ap-pearances of entity modifiers is less than or equal toa threshold (see Section 4.2).Relation Modifier: Given two targets, ti and tj ,we use Mtj (ti) to denote the relation modifier thatthe label of target ti is influenced by the label of tar-get tj .
Relation modifiers are further divided into 3kinds: MR = {mc,mA|E ,mE|A}.Conjunction modifier mc: Conjoined items areusually of the same type.
For example, in ?price andservice?, ?and service?
indicates a conjunction mod-ifier for ?price?
and vice versa.Entity-aspect modifier mA|E : A possessive ex-pression indicates an entity and an aspect relation.For example, in ?the camera?s battery?, ?camera?
in-dicates an entity-aspect modifier for ?battery?.Aspect-entity modifier mE|A: Same as above ex-cept that ?battery?
indicates an aspect-entity modi-fier for ?camera?.Modifier Extraction: These modifiers are iden-tified from the corpus d using three syntactic rules.?This?
and ?these?
are used to extract type modifierM(t) = mE .
CmE (t) is the occurrence count ofthat modifier on target t, which is used in determin-ing the initial label distribution in Section 4.2.Relation modifiers are identified by dependencyrelations conj(ti, tj) and poss(ti, tj) using the Stan-ford Parser (Klein and Manning, 2003).
Each oc-currence of a relation rule contributes one count ofMtj (ti) for ti and one count of Mti(tj) for tj .
Weuse Cmc,tj (ti), CmA|E ,tj (ti) and CmE|A,tj (ti) to de-note the count of tj modifying ti with conjunction,entity-aspect and aspect-entity modifiers respec-tively.
For example, ?price and service?
will con-tribute one count to Cmc,price(service) and one countto Cmc,service(price).
Similarly, ?camera?s battery?will contribute one count to CmA|E ,camera(battery)and one count to CmE|A,battery(camera).4.2 Computing Initial ProbabilitiesThe initial label probability distribution of target t iscomputed based on CmE (t), i.e.,P 0(L(t)) ={PmE (L(t)) if CmE (t) > ?PmA(L(t)) if CmE (t) ?
?
(3)Here, we have two pre-defined distributions: PmEand PmA , which have a higher probability on entityand aspect respectively.
The parameter ?
is a thresh-old indicating that if the entity modifier rarely oc-curs, the target is more likely to be an aspect.
These228values are set empirically (see Section 6).Let term q(Mtj (ti) = m) be the normalizedweight on the count for each kind of relation modi-fier m ?MR:q(Mtj (ti) = m) =Cm,tj (ti)Ctj (ti)(4)where Ctj (ti) =?m?MR Cm,tj (ti).The conditional label distribution P (L(ti)|L(tj))of ti given the label of tj is the weighted sum overthe three kinds of relation modifiers:P (L(ti)|L(tj)) =q(Mtj (ti) = mc) ?
Pmc(L(ti)|L(tj))+q(Mtj (ti) = mA|E) ?
PmA|E (L(ti)|L(tj))+q(Mtj (ti) = mE|A) ?
PmE|A(L(ti)|L(tj))(5)where Pmc , PmA|E , and PmE|A are pre-defined con-ditional distributions.
They are filled with values tomodel the label influence from neighbors and can befound in Section 6.Finally, target ti?s neighbor weight for target tj ,i.e., w(tj |ti), is the ratio of the count of relationmodifiers Ctj (ti) over the total of all ti?s neighbors:w(tj |ti) =Ctj (ti)?tj??Ne(ti)Ctj?
(ti)(6)If Ctj (ti) = 0, ti and tj has no edge between them.5 Using Past Knowledge in Lifelong-RLDue to the fact that the review corpus du+1 in thecurrent task domain may not be very large and thatwe use high quality syntactic rules to extract rela-tions to build the graph to ensure precision, the num-ber of relations extracted can be small and insuffi-cient to produce a graph that is information rich withaccurate initial probabilities.
We thus apply LML tohelp using knowledge learned in the past.
The pro-posed LML process in Lifelong-RL for our task isshown in Figure 1.Our prior knowledge includes type modi-fiers, relation modifiers and labels of targetsobtained from past domains in D. Eachrecord in the KB is stored as a 9-tuple:(d, ti, tj ,Md(ti),Md(tj), Cdm,tj (ti), Cdm,ti(tj), Ld(ti), Ld(tj))where d ?
D is a past domain; ti and tj aretwo targets; Md(ti), Md(tj) are their typeFigure 1: The proposed LML process.modifiers, Cdm,tj (ti) and Cdm,ti(tj) are countsfor relation modifiers; Ld(ti) and Ld(tj) arelabels decided by RL.
For example, the sen-tence ?This camera?s battery is good?
forms:(d, camera, battery,mE ,mA, CmE|A,battery(camera) = 1,CmA|E ,camera(battery) = 1, entity, aspect) .
It means thatin the past domain d, ?camera?
and ?battery?
areextracted targets.
Since ?camera?
is followed by?this?, its type modifier is mE .
Since ?battery?
isnot identified by an entity modifier, it is mA.
Thepattern ?camera?s battery?
contributes one count forboth relation modifiers CmE|A,battery(camera) andCmA|E ,camera(battery).
RL has labeled ?camera?as entity and ?battery?
as aspect in d.The next two subsections present how to use theknowledge in the KB to improve the initial assign-ments for the label distributions, conditional labeldistributions and neighborhood weight distributionsin order to achieve better final labeling/classificationresults for the current/new domain du+1.5.1 Exploiting Relation Modifiers in the KBIf two targets in the current domain corpus have noedge, we can check whether relation modifiers of thesame two targets exist in some past domains.
If so,we may be able to borrow them.
But to ensure suit-ability, two consistency checks are performed.Label Consistency Check: Since RL makes mis-takes, we need to ensure that relation modifiers in arecord in the KB are consistent with target labels inthat past domain.
For example, ?camera?s battery?
isconfirmed by ?camera?
being labeled as entity and?battery?
being labeled as aspect in a past domaind ?
D. Without this consistency, the record may notbe reliable and should be discarded from the KB.We define an indicator variable Idm,tj (ti) to en-sure that the record r?s relation modifier is consistent229with the labels of its two targets:IdmA|E ,tj (ti) =??????
?1if CdmA|E ,tj (ti) > 0and Ld(ti) = aspectand Ld(tj) = entity0 otherwise(7)For example, if ?camera?
is labeled as entityand ?battery?
is labeled as aspect in the past do-main d, we have IdmA|E ,camera(battery) = 1 andIdmE|A,battery(camera) = 1.Type Consistency Check: Here we ensure thetype modifiers for two targets in the current domaindu+1 are consistent with these type modifiers in thepast domain d ?
D. This is because an item can bean aspect in one domain but an entity in another.
Forexample, if the current domain is ?Cellphone?, bor-rowing the relation ?camera?s battery?
from domain?Camera?
can introduce an error because ?camera?is an aspect in domain ?Cellphone?.Syntactic pattern ?this?
is a good indicator for thischecking.
In the ?Cellphone?
domain, ?its camera?or ?the camera?
are often mentioned but not ?thiscamera?.
In the ?Camera?
domain, ?this camera?
isoften mentioned.
The type modifier of ?camera?
in?Cellphone?
is mA, but in ?Camera?
it is mE .Updating Probabilities in Current Domaindu+1: Edges for RL are in the forms of conditionallabel distribution P (L(ti)|L(tj)) and neighborhoodweight distribution w(tj |ti).
We now discuss how touse the KB to estimate them more accurately.Updating Conditional Label Distribution: Equa-tion (5) tells that conditional label distributionP (L(ti)|L(tj)) is the weighted sum of relation mod-ifiers?
label distributions Pmc , PmA|E , and PmE|A .These 3 label distributions are pre-defined and givenin Table 2.
They are not changed.
Thus, we up-date conditional label distribution through updatingthe three relation modifiers?
weights q(Mtj (ti)) withthe knowledge in the KB.
Recall the three relationmodifiers are MR = {mc,mA|E ,mE|A}.After consistency check, there can be multiple re-lation modifiers between two targets in similar pastdomains Ds ?
D. The number of domains sup-porting a relation modifier m ?
MR can tell whichkind of relation modifiers is common and likely tobe correct.
For example, given many past domainslike ?Laptop?, ?Tablet?, ?Cellphone?, etc., ?cameraand battery?
appears more than ?camera?s battery?,?camera?
should be modified by ?battery?
more withmE|A rather than mc (likely to be an aspect).Let Cdu+1m,tj (ti) be the count that target ti modi-fied by target tj on relation m in the current domaindu+1 (not in KB).
The count C(CL) is for updatingthe Conditional Label (CL) distributions consider-ing the information in both the current domain du+1and the KB.
It is calculated as:C(CL)m,tj (ti) ={Cdu+1m,tj (ti) if Cdu+1m,tj (ti) > 0?d?Ds Idm,tj (ti) if ?m?MR Cdu+1m,tj (ti)) = 0This equation says that if there is any relation mod-ifier existing between the two targets in the newdomain du+1, we do not borrow edges from theKB; Otherwise, the number of similar past domainssupporting the relation modifier m is used.
Recallthat Idm,tj (ti) is the result calculated by Equation (7)after label consistency check.We use count C(CL)m,tj (ti) to update qdu+1(Mtj (ti))using Equation (4) in Section 4.2.
Then the con-ditional label distribution accommodating relationmodifiers in the KB, P (LL1)(L(ti)|L(tj)), is calcu-lated by Equation, (5) using qdu+1(Mtj (ti)).
LL1denotes Lifelong Learning 1.Updating Neighbor Weight Distribution: Equa-tion (6) says that w(tj |ti) is the importance of targetti?s neighbor tj to ti among all ti?s neighbors.
Whenupdating conditional label distribution using the KB,the number of domains can decide which kind of re-lation modifiersm is more common between the twotargets ti and tj .
But we cannot tell that neighbor tjis more important than another neighbor tj?
to ti.For example, given the past domains such as?Laptop?, ?Tablet?, ?Cellphone?, etc., no matterhow many domains believe ?camera?
is an aspectgiven ?battery?
is also an aspect, if the current do-main is ?All-in-one desktop computer?, we shouldnot consider the strong influences from ?battery?in the past domains.
We should rely more on theweights of ?camera?
?s neighbors provided by ?All-in-one desktop computer?.
That means ?mouse?,?keyboard?, ?screen?
etc., should have strong influ-ences on ?camera?
than ?battery?
because most All-in-one desktops (e.g.
iMac) do not have battery.We introduce another indicator variableIDm,tj (ti) =?d?Ds Idm,tj (ti), to indicate whethertarget tj modified ti on relation m in past similardomains Ds.
It only considers the existence of a230relation modifier m among domains Ds.The count C(w)tj (ti) for updating the neighborweight (w) distribution considers both the KBand the current domain du+1.
It is as follows:C(w)tj (ti) ={ ?m?MR Cdu+1m,tj (ti) if ?m?MR Cdu+1m,tj (ti) > 0?m?MR IDum,tj (ti) if ?m?MR Cdu+1m,tj (ti) = 0This equation tells that if there are relation modifiersexisting between the two targets in the new domaindu+1, we count the total times that tj modifies tiin the new domain; Otherwise, we count the totalkinds of relation modifiers in MR if a relationmodifier m ?
MR existed in past domains.
Letw(LL1)(tj |ti) be the neighbor weight distributionconsidering knowledge from the KB and du+1.
It iscalculated by Equation (6) using C(w)tj (ti).The initial label distribution P du+1,0 is calculatedby Equation (3) only using type modifiers found inthe new domain du+1.
We use Lifelong-RL-1 to de-note the method that employs P (LL1)(L(ti)|L(tj)),w(LL1)(tj |ti) and P du+1,0 as inputs for RL.5.2 Exploiting Target Labels in the KBSince we have target labels from past domains, wemay have a better idea about the initial label prob-abilities of targets in the current domain du+1.
Forexample, after labeling domains like ?Cellphone?,?Laptop?, ?Tablet,?
and ?E-reader?, we may have agood sense that ?camera?
is likely to be an aspect.To use such knowledge, we need to check if the typemodifier of target t in the current domain matchesthose in past domains and only keep those domainsthat have such a matching type modifier.Let Ds ?
D be the past domains consistent withtarget t?s type modifier in the current domain du+1.Let CDs(L(t)) be the number of domains in Dsthat target t is labeled as L(t).
Let ?
be the ratiothat controls how much we trust knowledge fromthe KB.
Then the initial label probability distribu-tion P du+1,0 calculated by Equation (3) only usingtype modifier found in du+1 is replaced by :P (LL2),0(L(t)) = |D|?Pdu+1,0(L(t))+?CDs (L(t))|D|+?|D|(8)Similarly, let Ds ?
D be the past domains con-sistent with both targets ti?s and tj?s type modifiersin du+1.
Let CDs(L(ti), L(tj)) be the number ofdomains inDs that ti and tj are labeled as L(ti) andL(tj) respectively.
The conditional label probabil-ity distribution accommodating relation modifiers inthe KB, P (LL1)(L(ti)|L(tj)), is further updated toP (LL2)(L(ti)|L(tj)) by exploiting the target labelsin KB (LL2 denotes Lifelong Learning 2):P (LL2)(L(ti)|L(tj)) =|D|?P (LL1)(L(ti)|L(tj))+?CDs (L(ti),L(tj))|D|+?|D|(9)For example, given ?this camera?, ?battery?
inthe current domain, we are more likely to considerdomains (e.g.
?Film Camera?, ?DSLR?, but not?Cellphone?)
that have entity modifiers on ?camera?and aspect modifiers on ?battery?.
Then we countthe number of those domains that label ?camera?
asentity and ?battery?
as aspect: CDs(L(camera) =entity, L(battery) = aspect).
Similarly, we countdomains having other types of target labels on ?cam-era?
and ?battery?.
These counts form an updatedconditional label distribution that estimates ?cam-era?
as an entity and ?battery?
as an aspect.Note that |D ?
Ds|, the number of past do-mains not consistent with targets?
type modifiers,is added to CDs(L(ti) = NIL) and CDs(L(ti) =NIL, L(tj)) for Equations (8) and (9) respec-tively to make the sum over L(ti) equal to1.
We use Lifelong-RL to denote this methodwhich uses P (LL2),0(L(t)), P (LL2)(L(ti)|L(tj)) andw(LL1)(tj |ti) as input for RL.6 ExperimentsWe now evaluate the proposed method and comparewith baselines.
We use the DP method for target ex-traction (Qiu et al, 2011).
This method uses depen-dency relations between opinion words and targetsto extract targets using seed opinion words.
Sinceour paper does not focus on extraction, interestedreaders can refer to (Qiu et al, 2011) for details.6.1 Experiment SettingsEvaluation Datasets: We use two sets of datasets.The first set consists of eight (8) annotated reviewdatasets.
We use each of them as the new domaindata in LML to compute precision, recall, F1 scores.Five of them are from (Hu and Liu, 2004), and theremaining three are from (Liu et al, 2016).
Theyhave been used for target extraction, and thus haveannotated targets, but no annotation on whether a231Dataset Product Type # of Sentence # of entity # of aspectD1 Computer 531 50 151D2 Wireless Router 879 97 186D3 Speaker 689 64 218D4 DVD Player 740 50 159D5 Digital Camera 597 70 239D6 MP3 Player 1716 60 370D7 Digital Camera 346 28 151D8 Cell Phone 546 36 188Table 1: Annotation details of the benchmark datasets.Distribution L(t) = entity L(t) = aspect L(t) = NILPmE 0.45 0.25 0.3PmA 0.3 0.4 0.3Pmc L(tj) = entity L(tj) = aspect L(tj) = NILL(ti) = entity 0.8 0.0 0.33L(ti) = aspect 0.0 0.8 0.33L(ti) = NIL 0.2 0.2 0.33PmE|A L(tj) = entity L(tj) = aspect L(tj) = NILL(ti) = entity 0.33 0.8 0.33L(ti) = aspect 0.33 0.0 0.33L(ti) = NIL 0.33 0.2 0.33PmA|E L(tj) = entity L(tj) = aspect L(tj) = NILL(ti) = entity 0.0 0.33 0.33L(ti) = aspect 0.8 0.33 0.33L(ti) = NIL 0.2 0.33 0.33Table 2: Label Distribution for PE and PA and Condi-tional Label Distribution for Pmc , PmA|E and PmE|Atarget is an entity or aspect.
We made this annota-tion, which is straightforward.
We used two annota-tors to annotate the datasets.
The Cohen?s kappa is0.84.
Through discussion, the annotators got com-plete agreement.
Details of the datasets are listed inTable 1.
Each cell is the number of distinct terms.These datasets are not very large but they are realis-tic because many products do not have a large num-ber of reviews.The second set consists of unlabeled reviewdatasets from 100 diverse products or domains(Chen and Liu 2014).
Each domain has 1000 re-views.
They are treated as past domain data in LMLsince they are not annotated and thus cannot be usedfor computing evaluation measures.Evaluating Measures: We mainly use precisionP , recall R, and F1-score F1 as evaluation mea-sures.
We take multiple occurrences of the sametarget as one count, and only evaluate entities andaspects.
We will also give the accuracy results.Compared Methods: We compare the followingmethods, including our proposed method, Lifelong-RL.NER+TM: NER is Named Entity Recognition.We can regard the extracted terms from a NER sys-tem as entities and the rest of the targets as as-pects.
However, a NER system cannot identify enti-ties such as ?this car?
from ?this car is great.?
Its re-sult is rather poor.
But our type modifier (TM) doesthat, i.e., if an opinion target appears after ?this?
or?these?
in at least two sentences, TM labels the tar-get as an entity; otherwise an aspect.
However, TMcannot extract named entities.
Its result is also ratherpoor.
We thus combine the two methods to giveNER+TM as they complement each other very well.To make NER more powerful, we use two NER sys-tems: Stanford-NER 1(Manning et al, 2014) andUIUC-NER2 (Ratinov and Roth, 2009).
NER+TMtreats the extracted entities by the three systems asentities and the rest of the targets as aspects.NER+TM+DICT: We run NER+TM on the 100datasets for LML to get a list of entities, which wecall the dictionary (DICT).
For a new task, if anytarget word is in the list, it is treated as an entity;otherwise an aspect.RL: This is the base method described in Section3.
It performs relaxation labeling (RL) without thehelp of LML.Lifelong-RL-1: This performs LML with RL butthe current task only uses the relations in the KBfrom previous tasks (Section 5.1).Lifelong-RL: This is our proposed final method.It improves Lifelong-RL-1 by further incorporatingtarget labels in the KB from previous tasks (Section5.2).Parameter Settings: RL has 2 initial label distri-butions PmE and PmA and 3 conditional label dis-tributions Pmc , PmE|A and PmA|E .
Like other beliefpropagation algorithms, these probabilities need tobe set empirically, as shown in Table 2.
The parame-ter ?
is set to 1.
Our LML method has one parameter?
for Lifelong-RL.
We set it to 0.1.6.2 Results AnalysisTable 3 shows the test results of all systems in pre-cision, recall and F1-score except NER+TM+DICT.NER+TM+DICT is not included due to space lim-itations and because it performed very poorly.
Thereason is that a target can be an entity in one domain1http://nlp.stanford.edu/software/CRF-NER.shtml2https://cogcomp.cs.illinois.edu/page/software view/NETagger232Dataset Entity AspectNER+TM RL Lifelong-RL-1 Lifelong-RL NER+TM RL Lifelong-RL-1 Lifelong-RLP R F1 P R F1 P R F1 P R F1 P R F1 P R F1 P R F1 P R F1D1 56.3 88 68.7 80 56 65.9 76.1 70 72.9 83.1 66 73.6 71.0 74.8 72.9 72.6 74.2 73.4 75.3 71.5 73.4 74.2 72.8 73.5D2 64.8 75.3 69.7 71.6 42.3 53.2 81.1 62.9 70.9 85.5 78.4 81.8 61.9 90.3 73.5 61.4 85 71.3 67.2 92.5 77.9 70.4 90.9 79.3D3 56.8 68.6 62.2 63.4 37.5 47.1 79.8 62.5 70.1 76.3 64.1 69.6 76.3 81.7 78.9 76.6 77.5 77.1 73.7 84.4 78.7 73.5 82.6 77.8D4 76.7 42 54.3 69.3 42 52.3 77.9 70 73.7 78.6 70 74 68.8 71.7 70.2 68.3 70.4 69.3 70.4 65.4 67.8 70.6 66 68.2D5 62.7 54.3 58.2 62.1 61.4 61.8 78.5 94.3 85.7 86.4 91.4 88.9 85.6 81.6 83.5 85.5 77.8 81.5 87 81.2 84 87.7 82 84.8D6 69.9 38.3 49.5 67 56.7 61.4 74.7 75 74.8 77.4 73.3 75.3 75.4 83 79 76.2 81.1 78.6 78.8 85.9 82.2 78.9 86.2 82.4D7 95 64.28 76.7 95.2 67.9 79.2 93.8 92.8 93.3 94.7 92.9 93.8 87.5 86.1 86.8 87.9 86.8 87.3 89.1 88.1 88.6 90.7 88.7 89.7D8 65.9 41.7 51.1 65.5 72.2 68.7 72.3 83.3 77.4 79.4 86.1 82.6 76.1 81.9 78.9 77.8 80.9 79.3 81.4 89.4 85.2 81.9 89.9 85.7Average 68.5 59.1 61.3 71.8 54.5 61.2 79.3 76.4 77.4 82.7 77.8 79.9 75.3 81.4 78 75.8 79.2 77.2 77.9 82.3 79.7 78.5 82.4 80.2Table 3: Comparative results on Entity and Aspect in precision, recall and F1 score: NER+TM+DICT?sresults are very poor and not included (see Section 6.2) for the average results.but an aspect in another.
Its average F1-score for en-tity is only 49.2, and for aspect is only 50.2.Entity Results Comparison: We observe fromthe table that although NER+TM combines NERand TM, its result for entities is still rather poor.
Wenotice that phrases like ?this price?
causes low pre-cision.
Since it does not use many other relationsand NER does not recognize many named entitiesthat are written in lower case letters (e.g., ?apple isgood?
), its recall is also low.RL has a higher precision as it considers rela-tion modifiers.
However, its recall is low becauseit lacks information in its graph, which causes RL tomake many wrong decisions.
Lifelong-RL-1 intro-duces relation modifiers in KB from past domainsinto the current task.
Both precision and recall in-crease markedly.Lifelong-RL improves Lifelong-RL-1 further byconsidering target labels of past domains.
Theircounts improve the initial label probability distribu-tions and conditional label probability distributions.For example, ?this price?
may appear in some do-mains but ?price?
?s target label is mostly aspect.
Weconsider their counts in initial label distributions andthus rectify the initial distribution of ?price?.
Thismakes ?price?
easier to be classified as aspect andthus improves the precision for entity.Aspect Results Comparison: For aspects, thetrend is the same but the improvements are not asdramatic as for entity.
This is because the distribu-tion of entity and aspect in the data is highly skewed.There are many more aspects than entities as wecan see from the Table 1.
When an entity term iswrongly classified as an aspect, it has much less im-pact on the aspect result than on the entity result.Accuracy Results Comparison: Table 4 givesthe classification accuracy results considering allDataset NER+TM RL Lifelong-RL-1 Lifelong-RLD1 64.93 74.29 75.51 76.34D2 62.94 63.53 69.8 73.82D3 70.04 73.74 74.83 74.1D4 70.81 68.57 73.33 73.63D5 82.07 81.46 85.22 87.5D6 74.83 75.06 78 78.63D7 88.18 88.63 89.68 91.3D8 74.54 75.43 79.57 81.4Average 73.55 75.07 78.24 79.59Table 4: Results in accuracy: NER+TM+DICT?s re-sults are again very poor and thus not included.three classes.
We can see the similar trend.NER+TM+DICT?s average accuracy is only 45.89and is not included in the table.7 ConclusionThis paper studied the problem of classifying opin-ion targets into entities and aspects.
To the best ofour knowledge, this problem has not been attemptedin the unsupervised opinion target extraction setting.But this is an important problem because withoutseparating or classifying them one will not knowwhether an opinion is about an entity as a wholeor about a specific aspect of an entity.
This paperproposed a novel method based on relaxation label-ing and the paradigm of lifelong machine learning tosolve the problem.
Experimental results showed theeffectiveness of the proposed method.AcknowledgmentsThis work was partially supported by National Sci-ence Foundation (NSF) grants IIS-1407927 and IIS-1650900, and NCI grant R01CA192240.
The con-tent of the paper is solely the responsibility of theauthors and does not necessarily represent the offi-cial views of the NSF or NCI.233ReferencesSamuel Brody and Noemie Elhadad.
2010.
An unsuper-vised aspect-sentiment model for online reviews.
InNAACL ?10, pages 804?812.Rich Caruana.
1997.
Multitask learning.
Machine learn-ing, 28(1):41?75.Zhiyuan Chen and Bing Liu.
2014a.
Mining topics indocuments: Standing on the shoulders of big data.
InKDD ?14, pages 1116?1125.Zhiyuan Chen and Bing Liu.
2014b.
Topic modeling us-ing topics from many domains, lifelong learning andbig data.
In Proceedings of the 31st International Con-ference on Machine Learning (ICML-14), pages 703?711.Zhiyuan Chen, Nianzu Ma, and Bing Liu.
2015.
Life-long learning for sentiment classification.
Volume 2:Short Papers, pages 750?756.Yejin Choi and Claire Cardie.
2010.
Hierarchical se-quential learning for extracting opinions and their at-tributes.
In ACL ?10, pages 269?274.Lei Fang and Minlie Huang.
2012.
Fine granular aspectanalysis using latent structural models.
In Proceed-ings of the 50th Annual Meeting of the Associationfor Computational Linguistics: Short Papers-Volume2, pages 333?337.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the tenthACM SIGKDD international conference on Knowl-edge discovery and data mining, pages 168?177.Robert A Hummel and Steven W Zucker.
1983.
On thefoundations of relaxation labeling processes.
PatternAnalysis and Machine Intelligence, IEEE Transactionson, (3):267?287.Niklas Jakob and Iryna Gurevych.
2010.
Extractingopinion targets in a single- and cross-domain settingwith conditional random fields.
In EMNLP ?10, pages1035?1045.Dan Klein and Christopher D Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics-Volume 1, pages 423?430.Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen.
2006.Opinion extraction, summarization and tracking innews and blog corpora.
In AAAI spring symposium:Computational approaches to analyzing weblogs, vol-ume 100107.Fangtao Li, Minlie Huang, and Xiaoyan Zhu.
2010.
Sen-timent analysis with global topics and local depen-dency.
In AAAI ?10, pages 1371?1376.Chenghua Lin and Yulan He.
2009.
Joint sentiment/topicmodel for sentiment analysis.
In Proceedings of the18th ACM conference on Information and knowledgemanagement, pages 375?384.Kang Liu, Liheng Xu, and Jun Zhao.
2013.
Syntac-tic patterns versus word alignment: Extracting opiniontargets from online reviews.
In ACL (1), pages 1754?1763.Qian Liu, Bing Liu, Yuanlin Zhang, DooSoon Kim, andZhiqiang Gao.
2016.
Improving opinion aspect ex-traction using semantic similarity and aspect associa-tions.
In AAAI.Bing Liu.
2012.
Sentiment analysis and opinion min-ing.
Synthesis lectures on human language technolo-gies, 5(1):1?167.Christopher D. Manning, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven J. Bethard, and David McClosky.2014.
The Stanford CoreNLP natural language pro-cessing toolkit.
In Association for Computational Lin-guistics (ACL) System Demonstrations, pages 55?60.Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, andChengXiang Zhai.
2007.
Topic sentiment mixture:Modeling facets and opinions in weblogs.
In WWW?07, pages 171?180.Margaret Mitchell, Jacqui Aguilar, Theresa Wilson, andBenjamin Van Durme.
2013.
Open domain targetedsentiment.
In ACL ?13, pages 1643?1654.Arjun Mukherjee and Bing Liu.
2012.
Aspect extrac-tion through semi-supervised modeling.
In ACL ?12,volume 1, pages 339?348.Sinno Jialin Pan and Qiang Yang.
2010.
A survey ontransfer learning.
Knowledge and Data Engineering,IEEE Transactions on, 22(10):1345?1359.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and trends in infor-mation retrieval, 2(1-2):1?135.Ana-Maria Popescu and Orena Etzioni.
2007.
Extractingproduct features and opinions from reviews.
In Natu-ral language processing and text mining, pages 9?28.Springer.Soujanya Poria, Erik Cambria, Lun-Wei Ku, Chen Gui,and Alexander Gelbukh.
2014.
A rule-based approachto aspect extraction from product reviews.
SocialNLP2014, page 28.Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.2011.
Opinion word expansion and target extractionthrough double propagation.
Computational linguis-tics, 37(1):9?27.L.
Ratinov and D. Roth.
2009.
Design challengesand misconceptions in named entity recognition.
InCoNLL, 6.Paul Ruvolo and Eric Eaton.
2013.
Active task selectionfor lifelong machine learning.
In AAAI.Daniel L Silver, Qiang Yang, and Lianghao Li.
2013.Lifelong machine learning systems: Beyond learningalgorithms.
In AAAI Spring Symposium: Lifelong Ma-chine Learning, pages 49?55.234Sebastian Thrun.
1998.
Lifelong learning algorithms.
InLearning to learn, pages 181?209.
Springer.Bo Wang and Houfeng Wang.
2008.
Bootstrapping bothproduct features and opinion words from chinese cus-tomer reviews with cross-inducing.
In IJCNLP ?08,pages 289?295.Hongning Wang, Yue Lu, and Chengxiang Zhai.
2010.Latent aspect rating analysis on review text data: Arating regression approach.
In KDD ?10, pages 783?792.Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.2009.
Phrase dependency parsing for opinion mining.In EMNLP ?09, pages 1533?1541.Lei Zhang, Bing Liu, Suk Hwan Lim, and EamonnO?Brien-Strain.
2010.
Extracting and ranking prod-uct features in opinion documents.
In COLING ?10:Posters, pages 1462?1470.Xinjie Zhou, Xiaojun Wan, and Jianguo Xiao.
2013.Collective opinion target extraction in chinese mi-croblogs.
In EMNLP, volume 13, pages 1840?1850.Li Zhuang, Feng Jing, and Xiao-Yan Zhu.
2006.
Moviereview mining and summarization.
In CIKM ?06,pages 43?50.235
