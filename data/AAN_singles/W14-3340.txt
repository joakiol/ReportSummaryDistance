Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 322?328,Baltimore, Maryland USA, June 26?27, 2014.c?2014 Association for Computational LinguisticsFBK-UPV-UEdin participation in the WMT14 Quality Estimationshared-taskJos?e G. C. de Souza?University of TrentoFondazione Bruno KesslerTrento, Italydesouza@fbk.euJes?us Gonz?alez-Rubio?PRHLT GroupU.
Polit`ecnica de Val`enciaValencia, Spainjegonzalez@prhlt.upv.esChristian Buck?University of EdinburghSchool of InformaticsEdinburgh, Scotland, UKcbuck@lantis.deMarco Turchi, Matteo NegriFondazione Bruno Kesslerturchi,negri@fbk.euAbstractThis paper describes the joint submissionof Fondazione Bruno Kessler, UniversitatPolit`ecnica de Val`encia and University ofEdinburgh to the Quality Estimation tasksof the Workshop on Statistical MachineTranslation 2014.
We present our submis-sions for Task 1.2, 1.3 and 2.
Our systemsranked first for Task 1.2 and for the Binaryand Level1 settings in Task 2.1 IntroductionQuality Estimation (QE) for Machine Translation(MT) is the task of evaluating the quality of theoutput of an MT system without reference transla-tions.
Within the WMT 2014 QE Shared Task fourevaluation tasks were proposed, covering bothword and sentence level QE.
In this work we de-scribe the Fondazione Bruno Kessler (FBK), Uni-versitat Polit`ecnica de Val`encia (UPV) and Uni-versity of Edinburgh (UEdin) approach and sys-tem setup for the shared task.We developed models for two sentence-leveltasks: Task 1.2, scoring for post-editing effort,and Task 1.3, predicting post-editing time, andfor all word-level variants of Task 2, binary andmulticlass classification.
As opposed to previouseditions of the shared task, this year the partici-pants were not supplied with the MT system thatwas used to produce the translation.
Furthermoreno system-internal features were provided.
Thus,while the trained models are tuned to detect theerrors of a specific system the features have to begenerated independently (black-box).2 Sentence Level QEWe submitted runs to two sentence-level tasks:Task 1.2 and Task 1.3.
The first task aims at?Contributed equally to this work.predicting the Human mediated Translation EditRate (HTER) (Snover et al., 2006) between a sug-gestion generated by a machine translation sys-tem and its manually post-edited version.
Thedata set contains 1,104 English-Spanish sentencepairs post-edited by one translator (896 for train-ing and 208 for test).
The second task requiresto predict the time, in milliseconds, that was re-quired to post edit a translation given by a ma-chine translation system.
Participants are providedwith 858 English-Spanish sentence pairs, sourceand suggestion, along with their respective post-edited sentence and post-editing time in seconds(650 data points for training and 208 for test).
Weparticipated in the scoring mode of both tasks.2.1 FeaturesFor our sentence-level submissions we computefeatures using different resources that do not usethe MT system internals.
We use the same set offeatures for both Task 1.2 and 1.3.QuEst Black-box features (quest79).
We ex-tract 79 black-box features that capture the com-plexity, fluency and adequacy aspects of the QEproblem.
These features are extracted using theimplementation provided by the QuEst framework(Specia et al., 2013).
Among them are the 17 base-line features provided by the task organizers.The complexity features are computed on thesource sentence and indicate the complexity oftranslating the segment.
Examples of these fea-tures are the language model (LM) probabilitiesof the source sentence computed in a corpus of thesource language, different surface counts like thenumber of punctuation marks and the number oftokens in the source sentence, among others.The fluency features are computed over thetranslation generated by the MT system and in-dicate how fluent the translation is in the target322language.
One example would again be the LMprobability of the translation given by a LM modeltrained on a corpus of the target language.
Anotherexample is the average number of occurrences ofthe target word within the target segment.The third aspect covered by the QuEst featuresis the adequacy of the translation with respect tothe source sentence, i.e., how the meaning of thesource is preserved in the translation.
Examples offeatures are the ratio of nouns, verbs and adjectivesin the source and in the translation.
For a moredetailed description of the features in this groupplease refer to (Specia et al., 2013).Word alignment (wla).
Following our lastyear?s submission (de Souza et al., 2013a) we ex-plore information about word alignments to ex-tract quantitative (amount and distribution of thealignments) and qualitative features (importanceof the aligned terms).
Our assumption is thatfeatures that explore what is aligned can bringimprovements to tasks where sentence-level se-mantic relations need to be identified.
We trainthe word alignment models with the MGIZA++toolkit (Gao and Vogel, 2008) implementation ofthe IBM models (Brown et al., 1993).
The modelsare built on the concatenation of Europarl, NewsCommentary, and MultiUN parallel corpora madeavailable in the QE shared task of 2013, compris-ing about 12.8 million sentence pairs.
A more de-tailed description of the 89 features extracted canbe found in (de Souza et al., 2013a; de Souza etal., 2013b).Word Posterior Probabilities (wpp).
Using anexternal SMT system we produce 100k-best listsfrom which we derive Word Posterior Probabili-ties as detailed in Subsection 3.1.We use the geometric mean of these probabili-ties to derive a sentence-level score.Because the system that we use to produce theN-best list is not the same that generated the sug-gestions some suggested words never appear in theN-best list and thus receive zero probability.
Toovercome this issue we first clip the WPPs to aminimum probability.
Using a small sample of thedata to estimate this number we arrive at:log(p)min= ?2.N-best diversity (div).
Using the same 100k-best list as above we extract a number of measuresthat grasp the spatial distribution of hypotheses inthe search space as described in (de Souza et al.,2013a).Word Prediction (wpred).
We introduce theuse of the predictions provided by the word-levelQE system described in Section 3 to leverage in-formation for the sentence-level tasks.
We com-bine the binary word-level predictions in differentways, with the objective of measuring the fluencyof the translation in a more fine-grained way.
Wetarget a quantitative aspect of the words by com-puting ratios of OK or BAD predictions.
Further-more, we also explore a qualitative aspect by cal-culating ratios of different classes of words givenby their part-of-speech tags, indicating the qual-ity of distinct meaningful regions that compose thetranslation sentence.
In total, we compute 18 fea-tures:?
number of OK predictions divided by the no.of words in the translation sentence (1 fea-ture);?
number of OK function/content words predic-tions divided by the no.
of function/contentwords in the translation (2 features);?
number of OK nouns, verbs, proper-nouns,adjective, pronouns predictions divided bythe total nouns, verbs, proper-nouns, adjec-tive, pronouns (5 features);?
size of the longest sequence of OK/BAD wordpredictions divided by the total number ofOK/BAD predictions in the translation (2 fea-tures);?
number of OK predicted n-grams divided bythe total number of n-grams in the transla-tion.
We vary n from 2 to 5 (4 features);?
number of words predicted as OK in thefirst/second half of the translation divided bythe total number of words in the first/secondhalf of the translation (2 features).?
number of words predicted as OK in thefirst/second quarter of the translation di-vided by the total number of words in thefirst/second quarter of the translation (2 fea-tures).For some instances of the sentence-level taskswe were not able to produce word-level predic-tions due to an incomplete overlap between theword-level and sentence-level tasks datasets.
Forsuch data points we use the median of the featurecolumn for Task 1.2 and the mean for Task 1.3.323Method Features Train T1.2 Train T1.3 Test T1.2 Test T1.3SVR baseline 16.90 16864 15.23 21490ET baseline 16.25 17888 17.73 19400ET quest79 + wla + wpp 15.62 17474 14.44 18658ET quest79 + wla + wpp + div215.57 17471 14.38 18693ET quest79 + wla + wpp + div + wpred115.05 16392 12.89 17477Table 1: Training and test results for Task 1.2 and 1.3.
Scores are the MAE on a development setrandomly sampled from the training data (20%).
Baseline features were provided by the shared taskorganizers.
We used Support Vector Machines (SVM) regression to train the baseline models (first row).Submissions are marked with1and2for primary and secondary, respectively.2.2 Experimental SetupWe build the sentence-level models for both tasks(T1.2 and T1.3) with the features described in Sec-tion 2.1 using one learning algorithm: extremelyrandomized trees (ET) (Geurts et al., 2006).
ET isan ensemble of randomized trees in which eachdecision tree can be parameterized differently.When a tree is built, the node splitting step is doneat random by picking the best split among a ran-dom subset of the input features.
All the treesare grown on the whole training set and the re-sults of the individual trees are combined by aver-aging their predictions.
The models produced bythis method demonstrated to be robust to a largenumber of input features.
For our experiments andsubmissions we used the ET implementation in-cluded in the Scikit-learn library (Pedregosa et al.,2011).During training we evaluate the models on adevelopment set.
The development set was ob-tained by randomly sampling 20% of the trainingdata.
The remaining 80% were used for training.The training process was carried out by optimiz-ing the ET hyper-parameters with 100 iterationsof random search optimization (Bergstra and Ben-gio, 2012) set to minimize the mean absolute er-ror (MAE)1on 10-fold cross-validation over thetraining data.
The ET hyper-parameters optimizedare: the number of decision trees in the ensemble,the maximum number of features to consider whenlooking for the best split, the maximum depth ofthe trees used in the ensembles, the minimal num-ber of samples required to split a node of the tree,and the minimum number of samples in newly cre-ated leaves.
For the final submissions we run therandom search with 1000 iterations over the wholetraining dataset.1Given by MAE =?Ni=1|H(si)?V (si)|N, where H(si) isthe hypothesis score for the entry siand V (si) is the goldstandard value for siin a dataset with N entries.2.3 ResultsWe train models on different combinations of fea-ture groups (described in Section 2.1).
Experi-ments results are summarized in Table 1.
We haveresults with baseline features for both SVR and theET models.
For Task 1.2, adding features from dif-ferent groups leads to increasing improvements.The combination of the quest79, wla and wppgroups outperforms the SVR baseline for Task 1.2but not for Task 1.3.
However, when comparedto the ET model trained with the baseline fea-tures, it is possible to observe improvements withthis group of features.
In addition, adding thediv group on top of the previous three leads tomarginal improvements for both tasks.
The bestfeature combination is given when adding the fea-tures based on the word-level predictions, config-uring the combination of all the feature groups to-gether (a total of 221 features).
For both tasksthis is our primary submission.
The contrastiverun for both tasks is the best feature group com-bination without the word-prediction-based fea-tures, quest79, wla, wpp and div for Task 1.2 andquest79, wla, wpp for Task 1.3.Results on the test set can be found in the twolast columns of Table 1 and are in line with whatwe found in the training phase.
The rows that donot correspond to the official submissions and thatare reported on the test set are experiments doneafter the evaluation phase.
For both tasks the im-provements increase as we add features on top ofthe baseline feature set and the best performanceis reached when using the word prediction fea-tures with all the other features.
The SVR base-lines performance are the official numbers pro-vided by the organizers.
For Task 1.2 our primarysubmission achieves a MAE score lower than thescore achieved during the training phase, show-ing that the model is robust.
For Task 1.3, how-ever, we do not observe such trend.
Even though324the primary submission for this task consistentlyimproves over the other feature combinations, itdoes not outperform the score obtained during thetraining phase.
This might be explained due tothe difference in the distribution between train-ing and test labels.
In Task 1.2 the two distri-butions are more similar than in Task 1.3, whichpresents slightly different distributions betweentraining and test data.3 Word-Level QETask 2 is the word-level quality estimation of auto-matically translated news sentences without givenreference translations.
Participants are required toproduce a label for each word in one or more ofthe following settings:Binary classification: a OK/BAD label, whereBAD indicates the need for editing the word.Level1 classification: OK, Accuracy, orFluency label specifying a coarser level oferrors for each word, or OK for words withno error.Multi-Class classification: one of the 20 error la-bels described in the shared-task descriptionor OK for words with no error.We submit word-level quality estimations forthe English-Spanish translation direction.
The cor-pus contains 1957 training sentences for a total of47411 Spanish words, and 382 test sentences for atotal of 9613 words.3.1 FeaturesWord Posterior Probabilities (WPP) In orderto generate an approximation of the decoder?ssearch space as well as an N-best list of possi-ble translations we re-translate the source usingthe system that is available for the 2013 WMT QEShared Task (Bojar et al., 2013).Certainly, there is a mismatch between the orig-inal system and the one that we used but, since oursystem was trained using the same news domainas the QE data, we assume that both face similarambiguous words or possible reorderings.
Usingthis system we generate a 100k-best list which isthe foundation of several features.We extract a set of word-level features based onposterior probabilities computed over N-best listsas proposed by previous works (Blatz et al., 2004;Ueffing and Ney, 2007; Sanchis et al., 2007).Consider a target word eibelonging to a transla-tion e = e1.
.
.
ei.
.
.
e|e|generated from a sourcesentence f .
Let N (f) be the list of N-best trans-lations for f .
We compute features as the nor-malized sum of probabilities of those translationsS(ei) ?
N (f) that ?contain?
word ei:1?e??
?N (f)P(e?
?| f)?e?
?S(ei)P(e?| f) (1)where P(e | f) is the probability translation e givensource sentence f according to the SMT model.We follow (Zens and Ney, 2006) and extractthree different WPP features depending on thecriteria chosen to compute S(ei):S(ei) = {e??
N (f) | a=Le(e?, e)?e?ai= ei}S(ei) contain those translations e?for which theword Levenshtein-aligned (Levenshtein, 1966) toposition i in e is equal to ei.S(ei) = {e??
N (f) | e?i= ei}A second option is to select those translationse?that contain the word eiat position i.S(ei) = {e??
N (f) | ?i?
: e?i?= ei}As third option, we select those translations e?that contain the word ei, disregarding its position.Confusion Networks (CN) We use the same N-best list used to compute the WPP features in theprevious section to compute features based on thegraph topology of confusion networks (Luong etal., 2014).
First, we Levenshtein-align all trans-lations in the N-best list using e as skeleton, andmerge all of them into a confusion network.
In thisnetwork, each word-edge is labelled with the pos-terior probability of the word.
The output edges ofeach node define different confusion sets of words,each word belonging to one single confusion set.Each complete path passing through all nodes inthe network represents one sentence in the N-bestlist, and must contain exactly one link from eachconfusion set.
Looking to the confusion set whichthe hypothesis word belongs to, we extract fourdifferent features: maximum and minimum proba-bility in the set (2 features), number of alternativesin the set (1 feature) and entropy of the alternativesin the set (1 feature).Language Models (LM) As language modelfeatures we produced n-gram length/backoff be-haviour and conditional probabilities for everyword in the sentence.
We employed both an inter-polated LM taken from the MT system discussed325in Section 3 as well as a very large LM which webuilt on 62 billion tokens of monolingual data ex-tracted from Common Crawl, a public web crawl.While generally following the procedure of Bucket al.
(2014) we apply an additional lowercasingstep before training the model.Word Lexicons (WL) We compute two dif-ferent features based on statistical word lexi-cons (Blatz et al., 2004):Avg.
probability:1|f |+1?|f |j=0P(ei| fj)Max.
probability: max0?j?|f |P(ei| fj)where P(e | f) is a probabilistic lexicon, and f0isthe source ?NULL?
word (Brown et al., 1993).POS tags (POS) We extract the part-of-speech(POS) tags for both source and translation sen-tences using TreeTagger (Schmid, 1994).
We usethe actual POS tag of the target word as a feature.Specifically, we represent it as a one-hot indicatorvector where all values are equal to zero exceptthe one representing the current tag of the word,which is set to one.
Regarding the source POStags, we first compute the lexical probability ofeach target word given each source word.
Then,we compute two different feature vectors for eachtarget word.
On the one hand, we use an indica-tor vector to represent the POS tag of the maxi-mum probability source word.
On the other hand,we sum up the indicator vectors for all the sourcewords each one weighted by the lexical probabilityof the corresponding word.
As a result, we obtaina vector that represents the probability distributionof source POS tags for each target word.
Addi-tionally, we extract a binary feature that indicateswhether the word is a stop word or not.2Stacking (S) Finally, we also exploit the diversegranularity of the word labels.
The word classesfor the Level1 and Multi-class conditions are finegrained versions of the Binary annotation, i.e.
theOK examples are the same for all cases.We re-use our binary predictions as an addi-tional feature for the finer-grained classes.
How-ever, due to time constrains, we were not able torun the proper nested cross-validation but used amodel trained on all available data, which there-fore over-fits on the training data.
Cross-validationresults using the stacking approach are thus veryoptimistic.2https://code.google.com/p/stop-words/3.2 ClassifiersWe use bidirectional long short-term memoryrecurrent neural networks (BLSTM-RNNs) asimplemented in the RNNLib package (Graves,2008).
Recurrent neural networks are a connec-tionist model containing a self-connected hiddenlayer.
The recurrent connection provides informa-tion of previous inputs, hence, the network canbenefit from past contextual information.
Longshort-term memory is an advanced RNN archi-tecture that allows context information over longperiods of time.
Finally, BLSTM-RNNs com-bine bidirectional recurrent neural networks andthe long short-term memory architecture allowingforward and backward context information.
Us-ing such context modelling classifier we can avoidthe use of context-based features that have beenshown to lead to only slight improvements in QEaccuracy (Gonz?alez-Rubio et al., 2013).As a secondary binary model we train a CRF.Our choice of implementation is Pocket CRF3which, while currently unmaintained, implementscontinuous valued features.
We use a history ofsize 2 for all features and perform 10-fold cross-validation, training on 9 folds each time.3.3 Experimental SetupThe free parameters of the BLSTM-RNNs are op-timized by 10-fold cross-validation on the train-ing set.
Each cross-validation experiment con-sider eight folds for training, one held-out foldfor development, and a final held-out fold for test-ing.
We estimate the neural network with the eighttraining folds using the prediction performance inthe validation fold as stopping criterion.
The re-sult of each complete cross-validation experimentis the average of the results for the predictions ofthe ten held-out test folds.
Additionally, to avoidnoise due to the random initialization of the net-work, we repeat each cross-validation experimentten times and average the results.
Once the opti-mal values of the free parameters are established,we estimate a new BLSTM-RNN using the fulltraining corpus and we use it as the final modelto predict the class labels of the test words.Since our objective is to detect words that needto be edited, we use the weighted averaged F1score over the different class labels that denote anerror as our main performance metric (wF1err).We also report the weighted averaged F1scores3http://pocket-crf-1.sourceforge.net/326Binary Level1 MultiClassMethod Features wF1errwF1allwF1errwF1allwF1errwF1allBLSTM-RNNs LM+WPP+CN+WL 35.9 63.0 23.7 59.4 10.7 55.5+POS 38.5162.7 26.7159.5 12.7155.5+Stacking ?
?
82.9293.9 64.7288.0CRF LM+WPP+CN+WL+POS 39.5262.4 0 ?
?
?
?Table 2: Cross-validation results for the different setups tested for Task 2.
Our two submissions aremarked as (1) and (2) respectively.over all the classes (wF1all).3.4 ResultsTable 2 presents the wF1errand wF1allscoresfor different sets of features.
Our initial experi-ment includes language model (LM), word poste-rior probability (WPP), confusion network (CN),and word lexicon (WL) features for a total of 11features.
We extend this basic feature set with theindicator features based on POS tags for a total of163 features.
We further extend the feature vectorsby adding the stacking feature in a total of 164 fea-tures.Analyzing the results we observe that predictionaccuracy is quite low.
Our hypothesis is that this isdue to the skewed class distribution.
Even for thebinary classification scenario (the most balancedof the three conditions), OK labels account for twothirds of the samples.
This effect worsens with in-creasing number of error classes and the resultingsparsity of observations.
As a result, the systemtends to classify all samples as OK which leads tothe low F1scores presented in Table 2.We can observe that the use of POS tags indica-tor features clearly improved the prediction accu-racy of the systems in the three conditions.
Thissetup is our primary submission for the three con-ditions of task 2.In addition, we observe that the use of the stack-ing feature provides a considerable improvementin prediction accuracy for Level1 and MultiClass.As discussed above the cross-validation results forthe stacking features are very optimistic.
Test pre-dictions using this setup are our contrastive sub-mission for Level1 and MultiClass conditions.Results achieved on the official test set can befound in Table 3.
Much in line with our cross-validation results the stacking-features prove help-ful, albeit by a much lower margin.
For the bi-nary task the RNN model strongly outperforms theCRF.Setup Binary Level1 MultiClassBLSTM-RNN 48.7 37.2 17.1+ Stacking ?
38.5 23.1CRF 42.6 ?
?Table 3: Test results for Task 2.
Numbers areweighted averaged F1scores (%) for all but theOK class.4 ConclusionThis paper describes the approaches and systemsetups of FBK, UPV and UEdin in the WMT14Quality Estimation shared-task.
In the sentence-level QE tasks 1.2 (predicting post-edition effort)and 1.3 (predicting post-editing time, in ms) weexplored different features and predicted with asupervised tree-based ensemble learning method.We were able to improve our results by explor-ing features based on the word-level predictionsmade by the system developed for Task 2.
Our bestsystem for Task 1.2 ranked first among all partici-pants.In the word-level QE task (Task 2), we exploreddifferent sets of features using a BLSTM-RNN asour classification model.
Cross-validation resultsshow that POS indicator features, despite sparse,were able to improve the results of the baselinefeatures.
Also, the use of the stacking feature pro-vided a big leap in prediction accuracy.
With thismodel, we ranked first in the Binary and Level1settings of Task 2 in the evaluation campaign.AcknowledgmentsThis work was supported by the MateCat and Cas-macat projects, which are funded by the EC un-der the 7thFramework Programme.
The authorswould like to thank Francisco?Alvaro Mu?noz forproviding the RNN classification software.327ReferencesJames Bergstra and Yoshua Bengio.
2012.
RandomSearch for Hyper-Parameter Optimization.
Journalof Machine Learning Research, 13:281?305.John Blatz, Erin Fitzgerald, George Foster, SimonaGandrabur, Cyril Goutte, Alex Kulesza, AlbertoSanchis, and Nicola Ueffing.
2004.
Confidence es-timation for machine translation.
In Proceedings ofthe international conference on Computational Lin-guistics, pages 315?321.Ond?rej Bojar, Christian Buck, Chris Callison-Burch,Christian Federmann, Barry Haddow, PhilippKoehn, Christof Monz, Matt Post, Radu Soricut, andLucia Specia.
2013.
Findings of the 2013 Work-shop on Statistical Machine Translation.
In Pro-ceedings of the Eighth Workshop on Statistical Ma-chine Translation, pages 1?44.Peter F. Brown, Vincent J. Della Pietra, StephenA.
Della Pietra, and Robert L. Mercer.
1993.The mathematics of statistical machine translation:parameter estimation.
Computational Linguistics,19:263?311.Christian Buck, Kenneth Heafield, and Bas van Ooyen.2014.
N-gram Counts and Language Models fromthe Common Crawl.
In Proceedings of the Lan-guage Resources and Evaluation Conference.Jos?e G. C. de Souza, Christian Buck, Marco Turchi,and Matteo Negri.
2013a.
FBK-UEdin participationto the WMT13 Quality Estimation shared-task.
InProceedings of the Eighth Workshop on StatisticalMachine Translation, pages 352?358.Jos?e G. C. de Souza, Miquel Espl?a-Gomis, MarcoTurchi, and Matteo Negri.
2013b.
Exploiting qual-itative information from automatic word alignmentfor cross-lingual nlp tasks.
In Proceedings of the51st Annual Meeting of the Association for Compu-tational Linguistics, pages 771?776.Qin Gao and Stephan Vogel.
2008.
Parallel implemen-tations of word alignment tool.
In Software Engi-neering, Testing, and Quality Assurance for NaturalLanguage Processing, pages 49?57.Pierre Geurts, Damien Ernst, and Louis Wehenkel.2006.
Extremely randomized trees.
Machine Learn-ing, 63(1):3?42.Jes?us Gonz?alez-Rubio, Jos?e R. Navarro-Cerdan, andFrancisco Casacuberta.
2013.
Partial least squaresfor word confidence estimation in machine transla-tion.
In 6th Iberian Conference on Pattern Recog-nition and Image Analysis, (IbPRIA) LNCS 7887,pages 500?508.
Springer.Alex Graves.
2008.
Rnnlib: A recurrent neuralnetwork library for sequence learning problems.http://sourceforge.net/projects/rnnl/.Vladimir Levenshtein.
1966.
Binary codes capable ofcorrecting deletions, insertions and reversals.
SovietPhysics Doklady, 10(8):707?710.Ngoc-Quang Luong, Laurent Besacier, and BenjaminLecouteux.
2014.
Word confidence estimation andits integration in sentence quality estimation for ma-chine translation.
In Knowledge and Systems Engi-neering, volume 244, pages 85?98.
Springer.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Pretten-hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-sos, D. Cournapeau, M. Brucher, M. Perrot, andE.
Duchesnay.
2011.
Scikit-learn : Machine Learn-ing in Python.
Journal of Machine Learning Re-search, 12:2825?2830.Alberto Sanchis, Alfons Juan, and Enrique Vidal.2007.
Estimation of confidence measures for ma-chine translation.
In Proceedings of the MachineTranslation Summit XI, pages 407?412.Helmut Schmid.
1994.
Probabilistic part-of-speechtagging using decision trees.
In Proceedings ofInternational Conference on New Methods in Lan-guage Processing, volume 12, pages 44?49.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A Studyof Translation Edit Rate with Targeted Human An-notation.
In Association for Machine Translation inthe Americas.Lucia Specia, Kashif Shah, Jos?e G. C. de Souza, andTrevor Cohn.
2013.
QuEst?a translation quality es-timation framework.
In Proceedings of the 51st An-nual Meeting of the Association for ComputationalLinguistics, pages 79?84.Nicola Ueffing and Hermann Ney.
2007.
Word-level confidence estimation for machine translation.Computational Linguistics, 33:9?40.Richard Zens and Hermann Ney.
2006.
N-gram poste-rior probabilities for statistical machine translation.In Proceedings of the Workshop on Statistical Ma-chine Translation, pages 72?77.328
