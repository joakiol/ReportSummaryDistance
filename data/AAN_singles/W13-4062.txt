Proceedings of the SIGDIAL 2013 Conference, pages 375?383,Metz, France, 22-24 August 2013. c?2013 Association for Computational LinguisticsA Data-driven Model for Timing Feedbackin a Map Task Dialogue SystemRaveesh Meena Gabriel Skantze Joakim GustafsonKTH Speech, Music and HearingStockholm, Swedenraveesh@csc.kth.se, gabriel@speech.kth.se, jocke@speech.kth.seAbstractWe present a data-driven model for de-tecting suitable response locations in theuser?s speech.
The model has beentrained on human?machine dialogue dataand implemented and tested in a spokendialogue system that can perform theMap Task with users.
To our knowledge,this is the first example of a dialogue sys-tem that uses automatically extractedsyntactic, prosodic and contextual fea-tures for online detection of response lo-cations.
A subjective evaluation of thedialogue system suggests that interac-tions with a system using our trainedmodel were perceived significantly betterthan those with a system using a modelthat made decisions at random.1 IntroductionTraditionally, dialogue systems have rested on avery simple model for turn-taking, where the sys-tem uses a fixed silence threshold to detect theend of the user?s utterance, after which the sys-tem responds.
However, this model does not cap-ture human-human dialogue very accurately;sometimes a speaker just hesitates and no turn-change is intended, sometimes the turn changesafter barely any silence (Sacks et al 1974).Therefore, such models can result in systems thatinterrupt the user or are perceived as unrespon-sive.
Related to the problem of turn-taking is thatof backchannels (Yngve, 1970).
Backchannelfeedback ?
short acknowledgements such as uh-huh or mm-hm ?
are used by human interlocutorsto signal continued attention to the speaker,without claiming the floor.
If a dialogue systemshould be able to manage smooth turn-taking andback-channelling, it must be able to first identifysuitable locations in the user?s speech to do so.Duncan (1972) found that human interlocutorscontinuously monitor several cues, such as con-tent, syntax, intonation, paralanguage, and bodymotion, in parallel to manage turn-taking.
Simi-lar observations have been made in various otherstudies investigating the turn-taking and back-channelling phenomena in human conversations.Ward (1996) has suggested that a low pitch re-gion is a good cue that backchannel feedback isappropriate.
On the other hand, Koiso et al(1998) have argued that both syntactic and pro-sodic features make significant contributions inidentifying turn-taking and back-channelling rel-evant places.
Cathcart et al(2003) have shownthat syntax in combination with pause duration isa strong predictor for backchannel continuers.Gravano & Hirschberg (2009) observed that thelikelihood of occurrence of a backchannel in-creases with the number of syntactic and prosod-ic cues conjointly displayed by the speaker.However, there is a general lack of studies onhow such models could be used online in dia-logue systems and to what extent that would im-prove the interaction.
There are two main prob-lems in doing so.
First, the data used in the stud-ies mentioned above are from human?humandialogue and it is not obvious to what extent themodels derived from such data transfers to hu-man?machine dialogue.
Second, many of thefeatures used were manually extracted.
This isespecially true for the transcription of utterances,but several studies also rely on manually anno-tated prosodic features.In this paper, we present a data-driven modelof what we call Response Location Detection(RLD), which is fully online.
Thus, it only relies375on automatically extractable features?coveringsyntax, prosody and context.
The model has beentrained on human?machine dialogue data and hasbeen implemented in a dialogue system that is inturn evaluated with users.
The setting is that of aMap Task, where the user describes the route andthe system may respond with for exampleacknowledgements and clarification requests.2 BackgroundTwo influential theories that have examined theturn-taking mechanism in human conversationsare the signal-based mechanism of Duncan(1972) and the rule-based mechanism proposedby Sacks (1974).
According to Duncan, ?theturn-taking mechanism is mediated through sig-nals composed of clear-cut behavioural cues,considered to be perceived as discrete?.
Duncanidentified six discrete behavioural cues that aspeaker may use to signal the intent to yield theturn.
These behavioural cues are: (i) any devia-tion from the sustained intermediate pitch level;(ii) drawl on the final syllable of a terminalclause; (iii) termination of any hand gesticulationor the relaxation of tensed hand position?duringa turn; (iv) a stereotyped expression with trailingoff effect; (v) a drop in pitch and/or loudness;and (vi) completion of a grammatical clause.
Ac-cording to the rule-based mechanism of Sacks(1974) turn-taking is regulated by applying rules(e.g.
?one party at a time?)
at Transition-Relevance Places (TRPs)?possible completionpoints of basic units of turns, in order to mini-mize gaps and overlaps.
The basic units of turns(or turn-constructional units) include sentential,clausal, phrasal, and lexical constructions.Duncan (1972) also suggested that speakersmay display behavioural cues either singly ortogether, and when displayed together they mayoccur either simultaneously or in tight sequence.In his analysis, he found that the likelihood that alistener attempts to take the turn is higher whenthe cues are conjointly displayed across the vari-ous modalities.While these theories have offered a function-based account of turn-taking, another line of re-search has delved into corpora-based techniquesto build models for detecting turn-transition andfeedback relevant places in speaker utterances.Ward (1996) suggested that a 110 millisecond(ms) region of low pitch is a fairly good predic-tor for back-channel feedback in casual conver-sational interactions.
He also argued that moreobvious factors, such as utterance end, rising in-tonation, and specific lexical items, account forless than they seem to.
He contended that proso-dy alone is sometimes enough to tell you what tosay and when to say.In their analysis of turn-taking and backchan-nels based on prosodic and syntactic features, inJapanese Map Task dialogs, Koiso et al(1998)observed that some part-of-speech (POS) fea-tures are strong syntactic cues for turn-change,and some others are strongly associated with noturn-change.
Using manually extracted prosodicfeatures for their analysis, they observed thatfalling and rising F0 patterns are related tochanges of turn, and flat, flat-fall and rise-fallpatterns are indications of the speaker continuingto speak.
Extending their analysis to backchan-nels, they asserted that syntactic features, such asfilled pauses, alone might be sufficient to dis-criminate when back-channelling is inappropri-ate, whereas presence of backchannels is alwayspreceded by certain prosodic patterns.Cathcart et al(2003) presented a shallowmodel for predicting the location of backchannelcontinuers in the HCRC Map Task Corpus(Anderson et al 1991).
They explored featuressuch as POS, word count in the preceding speak-er turn, and silence pause duration in their mod-els.
A model based on silence pause only insert-ed a backchannel in every speaker pause longerthan 900 ms and performed better than a wordmodel that predicted a backchannel every sev-enth word.
A tri-gram POS model predicted thatnouns and pronouns before a pause are the twomost important cues for predicting backchannelcontinuers.
The combination of the tri-gram POSmodel and pause duration model offered a five-fold improvement over the others.Gravano & Hirschberg (2009) investigatedwhether backchannel-inviting cues differ fromturn-yielding cues.
They examined a number ofacoustic features and lexical cues in the speakerutterances preceding smooth turn-changes, back-channels, and holds.
They have identified sixmeasureable events that are strong predictors of abackchannel at the end of an inter-pausal unit: (i)a final rising intonation; (ii) a higher intensitylevel; (iii) a higher pitch level; (iv) a final POSbi-gram equal to ?DT NN?, ?JJ NN?, or ?NNNN?
; (v) lower values of noise-to-harmonic rati-os; and (vi) a longer IPU duration.
They also ob-served that the likelihood of a backchannel in-creases in quadratic fashion with the number ofcues conjointly displayed by the speaker.When it comes to using these features formaking turn-taking decisions in dialogue sys-376tems, there is however, very little related work.One notable exception is Raux & Eskenazi(2008) who presented an algorithm for dynami-cally setting endpointing silence thresholds basedon features from discourse, semantics, prosody,timing, and speaker characteristics.
The modelwas also applied and evaluated in the Let?s Godialogue system for bus timetable information.However, that model only predicted the end-pointing threshold based on the previous interac-tion up to the last system utterance, it did notbase the decision on the current user utterance towhich the system response is to be made.In this paper, we train a model for online Re-sponse Location Detection that makes a decisionwhether to respond at every point where a veryshort silence (200 ms) is detected.
The model istrained on human?machine dialogue data takenfrom a first set of interactions with a system thatused a very na?ve policy for Response LocationDetection.
The trained model is then applied tothe same system, which has allowed us to evalu-ate the model online in interaction with users.3 A Map Task dialogue systemIn a previous study, we presented a fully auto-mated spoken dialogue system that can performthe Map Task with a user (Skantze, 2012).
MapTask is a common experimental paradigm forstudying human-human dialogue, where one sub-ject (the information giver) is given the task ofdescribing a route on a map to another subject(the information follower).
In our case, the useracts as the giver and the system as the follower.The choice of Map Task is motivated partly be-cause the system may allow the user to keep theinitiative during the whole dialogue, and thusonly produce responses that are not intended totake the initiative, most often some kind of feed-back.
Thus, the system might be described as anattentive listener.Implementing a Map Task dialogue systemwith full speech understanding would indeed bea challenging task, given the state-of-the-art inautomatic recognition of conversational speech.In order to make the task feasible, we have im-plemented a trick: the user is presented with amap on a screen (see Figure 1) and instructed tomove the mouse cursor along the route as it isbeing described.
The user is told that this is forlogging purposes, but the real reason for this isthat the system tracks the mouse position andthus knows what the user is currently talkingabout.
It is thereby possible to produce a coher-ent system behaviour without any speech recog-nition at all, only basic speech detection.
Thisoften results in a very realistic interaction, ascompared to what users are typically used towhen interacting with dialogue systems?in ourexperiments, several users first thought that therewas a hidden operator behind it1.Figure 1: The user interface, showing the map.The basic components of the system can beseen in Figure 2.
Dashed lines indicate compo-nents that were not part of the first iteration ofthe system (used for data collection), but whichhave been used in the model presented and eval-uated here.
The system uses a simple energy-based speech detector to chunk the user?s speechinto inter-pausal units (IPUs), that is, periods ofspeech that contain no sequence of silence longerthan 200 ms.
Such a short threshold allows thesystem to give backchannels (seemingly) whilethe user is speaking or take the turn with barelyany gap.
Similar to Gravano & Hirschberg(2009) and Koiso et al(1998), we define the endof an IPU as a candidate for the Response Loca-tion Detection model to identify as a ResponseLocation (RL).
We use the term turn to refer to asequence of IPUs which do not have any re-sponses between them.Figure 2: The basic components of the system.1 An example video can be seen athttp://www.youtube.com/watch?v=MzL-B9pVbOE.ProsodicanalysisDialoguemanagerMapWindoSpeechdet ctorResponseLocationDetectorContextualfeaturesProsodicfeaturesIPUs ResponseLocationMouse movementsSpeechsynthesizerResponseASR Syntactic features377Each time the RLD model detected a RL, thedialogue manager produced a Response, depend-ing on the current state of the dialogue and theposition of the mouse cursor.
Table 1 shows thedifferent types of responses the system couldproduce.
The dialogue manager always startedwith an Introduction and ended with an Ending,once the mouse cursor had reached the destina-tion.
Between these, it selected from the otherresponses, partly randomly, but also dependingon the length of the last user turn and the currentmouse location.
Longer turns often led to Restartor Repetition Requests, thus discouraging longersequences of speech that did not invite the sys-tem to respond.
If the system detected that themouse had been at the same place over a longertime, it pushed the task forward by making aGuess response.
We also wanted to explore otherkinds of feedback than just backchannels, andtherefore added short Reprise Fragments andClarification Requests (see for example Skantze(2007) for a discussion on these).Table 1: Different responses from the systemIntroduction ?Could you help me to find my way tothe train station?
?Backchannel ?Yeah?, ?Mhm?, ?Okay?, ?Uhu?RepriseFragment?A station, yeah?ClarificationRequest?A station?
?Restart ?Eh, I think I lost you at the hotel, howshould I continue from there?
?RepetitionRequest?Sorry, could you take that again?
?Guess ?Should I continue above the church?
?Ending ?Okay, thanks a lot.
?A na?ve version of the system was used to col-lect data.
Since we initially did not have any so-phisticated model of RLD, it was simply set towait for a random period between 0 and 800 msafter an IPU ended.
If no new IPUs were initiatedduring this period, a RL was detected, resultingin random response delays between 200 and1000 ms.
Ten subjects participated in the datacollection.
Each subject did 5 consecutive taskson 5 different maps, resulting in a total of 50 dia-logues.Each IPU in the corpus was manually annotat-ed into three categories: Hold (a response wouldbe inappropriate), Respond (a response is ex-pected) and Optional (a response would not beinappropriate, but it is perfectly fine not to re-spond).
Two human-annotators labelled the cor-pus separately.
For all the three categories thekappa score was 0.68, which is substantialagreement (Landis & Koch, 1977).
Since only2.1% of all the IPUs in the corpus were identifiedfor category Optional, we excluded them fromthe corpus and focused on the Respond and Holdcategories only.
The data-set contains 2272 IPUsin total; the majority of which belong to the classRespond (50.79%), which we take as our majori-ty class baseline.
Since the two annotators agreedon 87.20% of the cases, this can be regarded asan approximate upper limit for the performanceexpected from a model trained on this data.In (Skantze, 2012), we used this collected datato build an offline model of RLD that wastrained on prosodic and contextual features.
Inthis paper, we extend this work in three ways.First, we bring in Automatic Speech Recognition(ASR) for adding syntactic features to the model.Second, the model is implemented as a modulein the dialogue system so that it can extract theprosodic features online.
Third, we evaluate theperformance of our RLD model against a base-line system that makes a random choice, in a dia-logue system interacting with users.In contrast to some related work (e.g.
Koiso etal., 1998), we do not discriminate between loca-tions for backchannels and turn-changes.
Instead,we propose a general model for response loca-tion detection.
The reason for this is that the sys-tem mostly plays the role of an attentive listenerthat produces utterances that are not intended totake the initiative or claim the floor, but only toprovide different types of feedback (cf.
Table 1).Thus, suitable response locations will be wherethe user invites the system to give feedback, re-gardless of whether the feedback is simply anacknowledgement that encourages the system tocontinue, or a clarification request.
Moreover, itis not clear whether the acknowledgements thesystem produces in this domain should really beclassified as backchannels, since they do not onlysignal continued attention, but also that someaction has been performed (cf.
Clark, 1996).
In-deed, none of the annotators felt the need to markrelevant response locations within IPUs.4 A data-driven model for response lo-cation detectionThe human?machine Map Task corpus describedin the previous section was used for training anew model of RLD.
We describe below how weextracted prosodic, syntactic and contextual fea-tures from the IPUs.
We test the contribution ofthese feature categories?individually as well as378in combination, in classifying a given IPU aseither Respond or Hold type.
For this we explorethe Na?ve Bayes (NB) and Support Vector Ma-chine (SVM) algorithms in the WEKA toolkit(Hall et al 2009).
All results presented here arebased on 10-fold cross-validation.4.1 Prosodic featuresPitch and intensity (sampled at 10 ms) for eachIPU were extracted using ESPS inWavesurfer/Snack (Sj?lander & Beskow, 2000).The values were transformed to log scale and z-normalized for each user.
The final 200 msvoiced region was then identified for each IPU.For this region, the mean pitch, slope of thepitch (using linear regression)?in combinationwith the correlation coefficient r for the regres-sion line, were used as features.
In addition tothese, we also used the duration of the voicedregion as a feature.
The last 500 ms of each IPUwere used to obtain the mean intensity (also z-normalised).
Table 2 illustrates the power of pro-sodic features, individually as well as collective-ly (last row), in classifying an IPU as either Re-spond or Hold type.
Except for mean intensity allother features individually provide an improve-ment over the baseline.
The best accuracy,64.5%, was obtained by the SVM algorithm us-ing all the prosodic features.
This should becompared against the baseline of 50.79%.Table 2: Percentage accuracy of prosodic featuresin detecting response locationsAlgorithmFeature(s) NB  SVMMean pitch 60.3 62.7Pitch slope 59.0 57.8Duration 58.1 55.6Mean intensity 50.3 52.2Prosody (all combined) 63.3 64.54.2 Syntactic featuresAs lexico-syntactic features, we use the wordform and part-of-speech tag of the last twowords in an IPU.
All the IPUs in the Map Taskcorpus were manually transcribed.
To obtain thepart-of-speech tag we used the LBJ toolkit(Rizzolo & Roth, 2010).
Column three in Table 3illustrates the discriminatory power of syntacticfeatures?extracted from the manual transcrip-tion of the IPUs.
Using the last two words andtheir POS tags, the Na?ve Bayes learner achievesthe best accuracy of 83.6% (cf.
row 7).
WhilePOS tag is a generic feature that would enablethe model to generalize, using word form as afeature has the advantage that some words, suchas yeah, are strong cues for predicting the Re-spond class, whereas pause fillers, such as ehm,are strong predictors of the Hold class.Table 3: Percentage accuracy of syntactic featuresin detecting response locationsManualtranscriptionsASRresults# Feature(s) NB SVM NB SVM1 Last word (Lw) 82.5 83.9 80.8 80.92Last word part-of-speech (Lw-POS)79.4 79.5 74.5 74.63Second last word(2ndLw)68.1 67.7 67.1 67.04Second last wordPart-of-speech(2ndLw-POS)66.9 66.5 65.8 66.15 Lw + 2ndLw 82.3 81.5 80.8 80.66Lw-POS+ 2ndLw-POS80.3 80.5 75.4 74.877Lw + 2ndLw+ Lw-POS+ 2ndLw-POS83.6 81.7 79.7 79.78Last word diction-ary (Lw-Dict)83.4 83.4 78.0 78.09Lw-Dict+ 2ndLw-Dict81.2 82.6 76.1 77.710Lw + 2ndLw+ Lw-Conf+ 2ndLw-Conf82.3 81.5 81.1 80.5An RLD model for online predictions requiresthat the syntactic features are extracted from theoutput of a speech recogniser.
Since speechrecognition is prone to errors, an RLD modeltrained on manual transcriptions alone would notbe robust when making predictions in noisy data.Therefore we train our RLD model on actualspeech recognised results.
To achieve this, wedid an 80-20 split of the Map Task corpus intotraining and test sets respectively.
The transcrip-tions of IPUs in the training set were used totrain the language model of the Nuance 9 ASRsystem.
The audio recordings of the IPUs in thetest set were then recognised by the trained ASRsystem.
After performing five iterations of split-ting, training and testing, we had obtained thespeech recognised results for all the IPUs in theMap Task corpus.
The mean word error rate forthe five iterations was 17.22% (SD = 3.8%).Column four in Table 3 illustrates the corre-sponding performances of the RLD modeltrained on syntactic features extracted from thebest speech recognized hypotheses for the IPUs.With the introduction of a word error rate of17.22%, the performances of all the models us-379ing only POS tag feature decline.
The perfor-mances are bound to decline further with in-crease in ASR errors.
This is because the POStagger itself uses the left context to make POStag predictions.
With the introduction of errors inthe left context, the tagger?s accuracy is affected,which in turn affects the accuracy of the RLDmodels.
However, this decline is not significantfor models that use word form as a feature.
Thissuggests that using context independent lexico-syntactic features would still offer better perfor-mance for an online model of RLD.
We thereforealso created a word class dictionary, which gen-eralises the words into domain-specific classes ina simple way (much like a class-based n-grammodel).
Row 9 in Table 3 illustrates that using adictionary instead of POS tag (cf.
row 6) im-proves the performance of the online model.
Wehave also explored the use of word-level confi-dence scores (Conf) from the ASR as anotherfeature that could be used to reinforce a learningalgorithm?s confidence in trusting the recognisedwords (cf.
row 10 in Table 3).The best accuracy, 81.1%, for the online mod-el of RLD is achieved by the Na?ve Bayes algo-rithm using the features word form and confi-dence score, for last two words in an IPU.4.3 Contextual featuresWe have explored three discourse context fea-tures: turn and IPU length (in words and se-conds) and last system dialogue act.
Dialogueact history information have been shown to bevital for predicting a listener response when thespeaker has just responded to the listener?s clari-fication request (Koiso et al(1998); Cathcart etal.
2003; Gravano & Hirschberg (2009); Skantze,2012).
To verify if this rule holds in our corpus,we extracted turn length and dialogue act labelsfor the IPUs, and trained a J48 decision treelearner.
The decision tree achieved an accuracyof 65.7%.
One of the rules learned by the deci-sion tree is: if the last system dialogue act isClarification or Guess (cf.
Table 1), and the turnword count is less than equal to 1, then Respond.In other words, if the system had previouslysought a clarification, and the user has respondedwith a yes/no utterance, then a system responseis expected.
A more general rule in the decisiontree suggests that: if the last system dialogue actwas a Restart or Repetition Request, and if theturn word count is more than 4 then Respondotherwise Hold.
In other words, the systemshould wait until it gets some amount of infor-mation from the user.Table 4 illustrates the power of these contex-tual features in discriminating IPUs, using theNB and the SVM algorithms.
All the featuresindividually provide improvement over the base-line of 50.79%.
The best accuracy, 64.8%, isachieved by the SVM learner using the featureslast system dialogue act and turn word count.Table 4: Percentage accuracy of contextual featuresin detecting response locationsManualtranscriptionsASRresultsFeatures NB  SVM  NB  SVMLast system dialogue act 54.1 54.1 54.1 54.1Turn word count 61.8 61.9 61.5 62.9Turn length in seconds 58.4 58.8 58.4 58.8IPU word count 58.4 58.2 58.1 59.3IPU length in seconds 57.3 61.2 57.3 61.2Last system dialogue act+ Turn word count59.9 64.5 60.4 64.84.4 Combined modelTable 5 illustrates the performances of the RLDmodel using various feature category combina-tions.
It could be argued that the discriminatorypower of prosodic and contextual feature catego-ries is comparable.
A model combining prosodicand contextual features offers an improvementover their individual performances.
Using thethree feature categories in combination, the Na-?ve Bayes learner provided the best accuracy:84.6% (on transcriptions) and 82.0% (on ASRoutput).
These figures are significantly betterthan the majority class baseline of 50.79% andapproach the expected upper limit of 87.20% onthe performance.Table 5: Percentage accuracy of combined modelsManualtranscriptionsASRresultsFeature categories NB SVM NB SVMProsody  63.3 64.5 63.3 64.5Context  59.9 64.5 60.4 64.8Syntax  82.3 81.5 81.1 80.5Prosody + Context 67.7 70.2 67.5 69.1Prosody + Context+ Syntax84.6 77.2 82.0 77.1Table 6 illustrates that the Na?ve Bayes modelfor Response Location Detection trained oncombined syntactic, prosodic and contextual fea-tures, offers better precision (fraction of correctdecisions in all model decisions) and recall (frac-tion of all relevant decisions correctly made) incomparison to the SVM model.380Table 6: Precision and Recall scores of the NB andthe SVM learners trained on combined prosodic, con-textual and syntactic features.Prediction classPrecision (in %) Recall (in %)NB  SVM  NB  SVMRespond 81.0  73.0 87.0 84.0Hold 85.0 81.0 78.0 68.05 User evaluationIn order to evaluate the usefulness of the com-bined model, we have performed a user evalua-tion where we test the trained model in the MapTask dialogue system that was used to collect thecorpus (cf.
section 3).
A version of the dialoguesystem was created that uses a Random model,which makes a random choice between Respondand Hold.
The Random model thus approximatesour majority class baseline (50.79% for Re-spond).
Another version of the system used theTrained model ?
our data-driven model ?
tomake the decision.
For both models, if the deci-sion was a Hold, the system waited 1.5 secondsand then responded anyway if no more speechwas detected from the user.We hypothesize that since the Random modelmakes random choices, it is likely to producefalse-positive responses (resulting in overlap ininteraction) as well as false-negative responses(resulting in gap/delayed response) in equal pro-portion.
The Trained model on the other handwould produce fewer overlaps and gaps.In order to evaluate the models, 8 subjects (2female, 6 male) were asked to perform the MapTask with the two systems.
Each subject per-formed five dialogues (which included 1 trial and2 tests) with each version of the system.
Thisresulted in 16 test dialogues each for the two sys-tems.
The trial session was used to allow the us-ers to familiarize themselves with the dialoguesystem.
Also, the audio recording of the users?speech from this session was used to normalizethe user pitch and intensity for the online prosod-ic extraction.
The order in which the systems andmaps were presented to the subjects was variedover the subjects to avoid any ordering effect inthe analysis.The 32 dialogues from the user evaluationwere, on average, 1.7 min long (SD = 0.5 min).The duration of the interactions with the Randomand the Trained model were not significantlydifferent.
A total of 557 IPUs were classified bythe Random model whereas the Trained modelclassified 544 IPUs.
While the Trained modelclassified 57.7% of the IPUs as Respond type theRandom model classified only 48.29% of thetotal IPUs as Respond type, suggesting that theRandom model was somewhat quieter.It turned out that it was very hard for the sub-jects to perform the Map Task and at the sametime make a valid subjective comparison be-tween the two versions of the system, as we hadinitially intended.
Therefore, we instead con-ducted another subjective evaluation to comparethe two systems.
We asked subjects to listen tothe interactions and press a key whenever a sys-tem response was either lacking or inappropriate.The subjects were asked not to consider how thesystem actually responded, only evaluate the tim-ing of the response.Eight users participated in this subjectivejudgment task.
Although five of these were fromthe same set of users who had performed theMap Task, none of them got to judge their owninteractions.
The judges listened to the Map Taskinteractions in the same order as the users hadinteracted, including the trial session.
Whereas ithad been hard for the subjects who participatedin the dialogues to characterize the two versionsof the system, almost all of the judges couldclearly tell the two versions apart.
They statedthat the Trained system provided for a smoothflow of dialogue.
The timing of the IPUs wasaligned with the timing of the judges?
key-presses in order to measure the numbers of IPUsthat had been given inappropriate response deci-sions.
The results show that for the Randommodel, 26.75% of the RLD decisions were per-ceived as inappropriate, whereas only 11.39% ofthe RLD decisions for the Trained model wereperceived inappropriate.
A two-tailed two-sample t-test for difference in mean of the frac-tion of inappropriate instances (key-press countdivided by IPU count) for Random and Trainedmodel show a clear significant difference (t =4.66, dF = 30, p < 0.001).We have not yet analysed whether judges pe-nalized false-positives or false-negatives to alarger extent, this is left to future work.
Howev-er, some judges informed us that they did notpenalize delayed response (false-negative), as thesystem eventually responded after a delay.
In thecontext of a system trying to follow a route de-scription, such delays could sometimes be ex-pected and wouldn?t be unnatural.
For othertypes of interactions (such as story-telling), suchdelays may on the other hand be perceived asunresponsive.
Thus, the balance between false-positives and false-negatives might need to betuned depending on the topic of the conversation.3816 ConclusionWe have presented a data-driven model for de-tecting response locations in the user?s speech.The model has been trained on human?machinedialogue data and has been integrated and testedin a spoken dialogue system that can perform theMap Task with users.
To our knowledge, this isthe first example of a dialogue system that usesautomatically extracted syntactic, prosodic andcontextual features for making online detectionof response locations.
The models presented inearlier works have used only prosody (Ward,1996), or combinations of syntax and prosody(Koiso et al 1998), syntax and context (Cathcartet al 2003), prosody and context (Skantze,2012), or prosody, context and semantics (Raux& Eskenazi (2008).
Furthermore, we have evalu-ated the usefulness of our model by performing auser evaluation of a dialogue system interactingwith users.
None of the earlier models have beentested in user evaluations.The significant improvement of the modelgained by adding lexico-syntactic features suchas word form and part-of-speech tag corroborateswith earlier observations about the contributionof syntax in predicting response location (Koisoet al 1998; Cathcart et al 2003; Gravano &Hirschberg, 2009).
While POS tag alone is astrong generic feature for making predictions inoffline models its contribution to decision mak-ing in online models is reduced due to speechrecognition errors.
This is because the POS tag-ger itself uses the left context to make predic-tions, and is not typically trained to handle noisyinput.
We have shown that using only the wordform or a dictionary offers a better performancedespite speech recognition errors.
However, thisof course results in a more domain-dependentmodel.Koiso et al (1998), have shown that prosodicfeatures contribute almost as strongly to responselocation prediction as the syntactic features.
Wedo not find such results with our model.
Thisdifference could be partly attributed to inter-speaker variation in the human?machine MapTask corpus used for training the models.
All theusers who participated in the corpus collectionwere non-native speakers of English.
Also, ouralgorithm for extracting prosodic features is notas powerful as the manual extraction schemeused in (Koiso et al 1998).
Although prosodicand contextual features do not seem to improvethe performance very much when syntactic fea-tures are available, they are clearly useful whenno ASR is available (70.2% as compared to thebaseline of 50.79%).The subjective evaluation indicates that the in-teractions with a system using our trained modelwere perceived as smoother (more accurate re-sponses) as compared to a system using a modelthat makes a random choice between Respondand Hold.7 Future workCoordination problems in turn-transition and re-sponsiveness have been identified as importantshort-comings of turn-taking models in currentdialogue systems (Ward et al 2005).
In continu-ation of the current evaluation exercise, wewould next evaluate our Trained model?on anobjective scale, in terms of its responsivenessand smoothness in turn-taking and back-channels.
An objective measure is the proportionof judge key-presses coinciding with false-positive and false-negative model decisions.
Weargue that in comparison to the Random modelour Trained model produces (i) fewer instancesof false-negatives (gap/delayed response) andtherefore has a faster response time, and (ii) few-er instances of false-positives (overlap) and thusprovides for smooth turn-transitions.We have so far explored syntactic, prosodicand contextual features for predicting responselocation.
An immediate extension to our modelwould be to bring semantic features in the model.In Meena et al(2012) we have presented a data-driven method for semantic interpretation of ver-bal route descriptions into conceptual routegraphs?a semantic representation that capturesthe semantics of the way human structure infor-mation in route descriptions.
Another possibleextension is to situate the interaction in a face-to-face Map Task between a human and a robot andadd features from other modalities such as gaze.In a future version of the system, we do notonly want to determine when to give responsesbut also what to respond.
In order to do this, thesystem will need to extract the semantic conceptsof the route directions (as described above) andutilize the confidence scores from the spokenlanguage understanding component in order toselect between different forms of clarificationrequests and acknowledgements.AcknowledgmentsThis work is supported by the Swedish researchcouncil (VR) project Incremental processing inmultimodal conversational systems (2011-6237).382ReferencesAnderson, A., Bader, M., Bard, E., Boyle, E.,Doherty, G., Garrod, S., Isard, S., Kowtko, J.,McAllister, J., Miller, J., Sotillo, C., Thompson,H., & Weinert, R. (1991).
The HCRC Map Taskcorpus.
Language and Speech, 34(4), 351-366.Cathcart, N., Carletta, J., & Klein, E. (2003).
A shal-low model of backchannel continuers in spoken di-alogue.
In 10th Conference of the European Chap-ter of the Association for Computational Linguis-tics.
Budapest.Clark, H. H. (1996).
Using language.
Cambridge,UK: Cambridge University Press.Duncan, S. (1972).
Some Signals and Rules for Tak-ing Speaking Turns in Conversations.
Journal ofPersonality and Social Psychology, 23(2), 283-292.Gravano, A., & Hirschberg, J.
(2009).
Backchannel-inviting cues in task-oriented dialogue.
In Proceed-ings of Interspeech 2009 (pp.
1019-1022).
Bright-on, U.K.Hall, M., Frank, E., Holmes, G., Pfahringer, B.,Reutemann, P., & Witten, I. H. (2009).
The WEKAData Mining Software: An Update.
SIGKDD Ex-plorations, 11(1).Koiso, H., Horiuchi, Y., Tutiya, S., Ichikawa, A., &Den, Y.
(1998).
An analysis of turn-taking andbackchannels based on prosodic and syntactic fea-tures in Japanese Map Task dialogs.
Language andSpeech, 41, 295-321.Landis, J., & Koch, G. (1977).
The measurement ofobserver agreement for categorical data.
Biomet-rics, 33(1), 159-174.Meena, R., Skantze, G., & Gustafson, J.
(2012).
AData-driven Approach to Understanding SpokenRoute Directions in Human-Robot Dialogue.
InProceedings of Interspeech.
Portland, OR, US.Raux, A., & Eskenazi, M. (2008).
Optimizing end-pointing thresholds using dialogue features in aspoken dialogue system.
In Proceedings of SIGdial2008.
Columbus, OH, USA.Rizzolo, N., & Roth, D. (2010).
Learning Based Javafor Rapid Development of NLP Systems.
Lan-guage Resources and Evaluation.Sacks, H., Schegloff, E., & Jefferson, G. (1974).
Asimplest systematics for the organization of turn-taking for conversation.
Language, 50, 696-735.Sj?lander, K., & Beskow, J.
(2000).
WaveSurfer - anopen source speech tool.
In Yuan, B., Huang, T., &Tang, X.
(Eds.
), Proceedings of ICSLP 2000, 6thIntl Conf on Spoken Language Processing (pp.464-467).
Beijing.Skantze, G. (2007).
Error Handling in Spoken Dia-logue Systems - Managing Uncertainty, Groundingand Miscommunication.
Doctoral dissertation,KTH, Department of Speech, Music and Hearing.Skantze, G. (2012).
A Testbed for Examining theTiming of Feedback using a Map Task.
In Pro-ceedings of the Interdisciplinary Workshop onFeedback Behaviors in Dialog.
Portland, OR.Ward, N., Rivera, A., Ward, K., & Novick, D. (2005).Root causes of lost time and user stress in a simpledialog system.
In Proceedings of Interspeech 2005.Lisbon, Portugal.Ward, N. (1996).
Using prosodic clues to decide whento produce backchannel utterances.
In Proceedingsof the fourth International Conference on SpokenLanguage Processing (pp.
1728-1731).
Philadelph-ia, USA.Yngve, V. H. (1970).
On getting a word in edgewise.In Papers from the sixth regional meeting of theChicago Linguistic Society (pp.
567-578).
Chicago.383
