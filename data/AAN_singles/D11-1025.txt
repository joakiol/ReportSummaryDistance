Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 273?283,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsA Weakly-supervised Approach to Argumentative Zoning of ScientificDocumentsYufan GuoComputer LaboratoryUniversity of Cambridge, UKyg244@cam.ac.ukAnna KorhonenComputer LaboratoryUniversity of Cambridge, UKalk23@cam.ac.ukThierry PoibeauLaTTiCe, UMR8094CNRS & ENS, Francethierry.poibeau@ens.frAbstractArgumentative Zoning (AZ) ?
analysis of theargumentative structure of a scientific paper ?has proved useful for a number of informa-tion access tasks.
Current approaches to AZrely on supervised machine learning (ML).Requiring large amounts of annotated data,these approaches are expensive to develop andport to different domains and tasks.
A poten-tial solution to this problem is to use weakly-supervised ML instead.
We investigate theperformance of four weakly-supervised clas-sifiers on scientific abstract data annotated formultiple AZ classes.
Our best classifier basedon the combination of active learning and self-training outperforms our best supervised clas-sifier, yielding a high accuracy of 81% whenusing just 10% of the labeled data.
This re-sult suggests that weakly-supervised learningcould be employed to improve the practicalapplicability and portability of AZ across dif-ferent information access tasks.1 IntroductionMany practical tasks require accessing specific typesof information in scientific literature.
For example,a reader of scientific literature may be looking forinformation about the objective of the study in ques-tion, the methods used in the study, the results ob-tained, or the conclusions drawn by authors.
Sim-ilarly, many Natural Language Processing (NLP)tasks focus on the extraction of specific types of in-formation in documents only.To date, a number of approaches have been pro-posed for sentence-based classification of scien-tific literature according to categories of informationstructure (or discourse, rhetorical, argumentative orconceptual structure, depending on the frameworkin question).
Some of these classify sentences ac-cording to typical section names seen in scientificdocuments (Lin et al, 2006; Hirohata et al, 2008),while others are based e.g.
on argumentative zones(Teufel and Moens, 2002; Mizuta et al, 2006; Teufelet al, 2009), qualitative dimensions (Shatkay et al,2008) or conceptual structure (Liakata et al, 2010)of documents.The best of current approaches have yieldedpromising results and proved useful for informationretrieval, information extraction and summarizationtasks (Teufel and Moens, 2002; Mizuta et al, 2006;Tbahriti et al, 2006; Ruch et al, 2007).
How-ever, relying on fully supervised machine learning(ML) and a large body of annotated data, existingapproaches are expensive to develop and port to dif-ferent scientific domains and tasks.A potential solution to this bottleneck is to de-velop techniques based on weakly-supervised ML.Relying on a small amount of labeled data anda large pool of unlabeled data, weakly-supervisedtechniques (e.g.
semi-supervision, active learning,co/tri-training, self-training) aim to keep the advan-tages of fully supervised approaches.
They havebeen applied to a wide range of NLP tasks, includ-ing named-entity recognition, question answering,information extraction, text classification and manyothers (Abney, 2008), yielding performance levelssimilar or equivalent to those of fully supervisedtechniques.To the best of our knowledge, such techniques273have not yet been applied to the analysis of infor-mation structure of scientific documents by afore-mentioned approaches.
Recent experiments havedemonstrated the usefulness of weakly-supervisedlearning for classifying discourse relations in scien-tific texts, e.g.
(Hernault et al, 2011).
However, fo-cusing on local (rather than global) structure of doc-uments and being much more fine-grained in nature,this related task differs from ours considerably.In this paper, we investigate the potential ofweakly-supervised learning for Argumentative Zon-ing (AZ) of scientific abstracts.
AZ is an approach toinformation structure which provides an analysis ofthe rhetorical progression of the scientific argumentin a document (Teufel and Moens, 2002).
It hasbeen used to analyze scientific texts in various disci-plines ?
including computational linguistics (Teufeland Moens, 2002), law, (Hachey and Grover, 2006),biology (Mizuta et al, 2006) and chemistry (Teufelet al, 2009) ?
and has proved useful for NLP taskssuch as summarization (Teufel and Moens, 2002).Although the basic scheme is said to be discipline-independent (Teufel et al, 2009), its application todifferent domains has resulted in various modifica-tions and laborious annotation exercises.
This sug-gests that a weakly-supervised approach would bemore practical than a fully supervised one for thereal-world application of AZ.Taking two supervised classifiers as a comparisonpoint ?
Support Vector Machines (SVM) and Con-ditional Random Fields (CRF) ?
we investigate theperformance of four weakly-supervised classifierson the AZ task: two based on semi-supervised learn-ing (transductive SVM and semi-supervised CRF)and two on active learning (Active SVM alone andin combination with self-training).The results are promising.
Our best weakly-supervised classifier (Active SVM with self-training) outperforms the best supervised classifier(SVM), yielding high accuracy of 81% when usingjust 10% of the labeled data.
When using just onethird of the labeled data, it performs equally well asa fully supervised SVM which uses 100% of the la-beled data.
Our investigation suggests that weakly-supervised learning could be employed to improvethe practical applicability and portability of AZ todifferent information access tasks.2 DataWe used in our experiments the recent dataset of(Guo et al, 2010).
Guo et al (2010) provide a cor-pus of 1000 biomedical abstracts (consisting of 7985sentences and 225785 words) annotated accordingto three schemes of information structure ?
thosebased on section names (Hirohata et al, 2008), AZ(Mizuta et al, 2006) and Core Scientific Concepts(CoreSC) (Liakata et al, 2010).
We focus here onAZ only, because it subsumes all the categories ofthe simple section name -based scheme, and accord-ing to the inter-annotator agreement and ML experi-ments reported by Guo et al (2010) it performs bet-ter on this data than the fairly fine-grained CoreSCscheme.AZ is a scheme which provides an analysis ofthe rhetorical progression of the scientific argument,following the knowledge claims made by authors.
(Teufel and Moens, 2002) introduced AZ and ap-plied it first to computational linguistics papers.
(Hachey and Grover, 2006) applied the scheme laterto legal texts and (Mizuta et al, 2006) modified it forbiology papers.
More recently, (Teufel et al, 2009)introduced a refined version of AZ and applied it tochemistry papers.The biomedical dataset of (Guo et al, 2010) hasbeen annotated according to the version of AZ de-veloped for biology papers (Mizuta et al, 2006)(with only minor modifications concerning zonenames).
Seven categories of this scheme (out of the10 possible) actually appear in abstracts and in theresulting corpus.
These are shown and explainedin Table 1.
For example, the Method zone (METH)is for sentences which describe a way of doing re-search, esp.
according to a defined and regularplan; a special form of procedure or characteristicset of procedures employed in a field of study as amode of investigation and inquiry.An example of a biomedical abstract annotatedaccording to AZ is shown in Figure 1, with differentzones highlighted in different colors.
For example,the RES zone is highlighted in lemon green.Table 2 shows the distribution of sentences perscheme category in the corpus: Results (RES) isby far the most frequent zone (accounting for 40%of the corpus), while Background (BKG), Objective(OBJ), Method (METH) and Conclusion (CON) cover274Table 1: Categories of AZ appearing in the corpus of (Guo et al, 2010)Category Abbr.
DefinitionBackground BKG The circumstances pertaining to the current work, situation, or its causes, history, etc.Objective OBJ A thing aimed at or sought, a target or goalMethod METH A way of doing research, esp.
according to a defined and regular plan; a special formof procedure or characteristic set of procedures employed in a field of study as a modeof investigation and inquiryResult RES The effect, consequence, issue or outcome of an experiment; the quantity, formula,etc.
obtained by calculationConclusion CON A judgment or statement arrived at by any reasoning process; an inference, deduction,induction; a proposition deduced by reasoning from other propositions; the result ofa discussion, or examination of a question, final determination, decision, resolution,final arrangement or agreementRelated work REL A comparison between the current work and the related workFuture work FUT The work that needs to be done in the futureFigure 1: An example of an annotated abstractButadiene(BD)metabolismshowsgender,speciesandconcentrationdependency,makingtheextrapolationofanimalresultstohumanscomplex.BDismetabolizedmainlybycytochromeP4502E1tothreeepoxides,1,2-epoxy-3-butene(EB),1,2;3,4-diepoxybutane(DEB)and1,2-epoxy-butanediol(EB-diol).Foraccurateriskassessmentitisimportanttoelucidatespeciesdifferencesintheinternalformationoftheindividualepoxidesinordertoassigntherelativerisksassociatedwiththeirdifferentmutagenicpotencies.AnalysisofN-terminalglobinadductsisacommonapproachformonitoringtheinternalformationofBDderivedepoxides.OurlongtermstrategyistodevelopanLC-MS/MSmethodforsimultaneousdetectionofallthreeBDhemoglobinadducts.ThisapproachismodeledaftertherecentlyreportedimmunoaffinityLC-MS/MSmethodforthecyclicN,N-(2,3-dihydroxy-1,4-butadyil)-valine(pyr-Val,derivedfromDEB).WereporthereintheanalysisoftheEB-derived2-hydroxyl-3-butenyl-valinepeptide(HB-Val).Theprocedureutilizestrypsinhydrolysisofglobinandimmunoaffinity(IA)purificationofalkylatedheptapeptides.QuantitationisbasedonLC-MS/MSmonitoringofthetransitionfromthesinglychargedmolecularionofHB-Val(1-7)tothea(1)fragment.HumanHB-Val(1-11)wassynthesizedandusedforantibodyproduction.Asinternalstandard,thelabeledrat-[(13)C(5)(15)N]-Val(1-11)waspreparedthroughdirectalkylationofthecorrespondingpeptidewithEB.StandardswerecharacterizedandquantifiedbyLC-MS/MSandLC-UV.ThemethodwasvalidatedwithdifferentamountsofhumanHB-Valstandard.Therecoverywas>75%andcoefficientofvariation<25%.TheLOQwassetto100fmol/injection.Foraproofofprincipalexperiment,globinsamplesfrommaleandfemaleratsexposedto1000ppmBDfor90dayswereanalyzed.TheamountsofHB-Valpresentwere268.2+/-56and350+/-70pmol/g(mean+/-S.D.
)formalesandfemales,respectively.NoHB-Valwasdetectedincontrols.ThesedataaremuchlowercomparedtopreviouslyreportedvaluesmeasuredbyGC-MS/MS.ThedifferencemaybeduehigherspecificityoftheLC-MS/MSmethodtotheN-terminalpeptidefromthealpha-chainversusderivatizationofbothalpha-andbeta-chainbyEdmandegradation,andpossibleinstabilityofHB-Valadductsduringlongtermstorage(about10years)betweentheanalyses.Thesedifferenceswillberesolvedbyexaminingrecentlycollectedsamples,usingthesameinternalstandardforparallelanalysisbyGC-MS/MSandLC-MS/MS.Basedonourexperiencewithpyr-ValadductassayweanticipatethatthisassaywillbesuitableforevaluationofHB-Valinmultiplespecies.BackgroundObjectiveMethodResultConclusionRelatedworkFutureworkTable 2: Distribution of sentences in the AZ-annotatedcorpusBKG OBJ METH RES CON REL FUTWord 36828 23493 41544 89538 30752 2456 1174Sentence 1429 674 1473 3185 1082 95 47Sentence 18% 8% 18% 40% 14% 1% 1%8-18% of the corpus each.
Two categories are verylow in frequency, only covering 1% of the corpuseach: Related work (REL) and Future work (FUT).Guo et al (2010) report the inter-annotator agree-ment between their three annotators: one linguist,one computational linguist and one domain expert.According to Cohen?s kappa (Cohen, 1960) theagreement is relatively high: ?
= 0.85.3 Automatic identification of AZ3.1 Features and feature extractionGuo et al (2010) used a variety of features intheir fully supervised ML experiments on differentschemes of information structure.
Since their fea-ture types cover the best performing feature types inearlier works e.g.
(Teufel and Moens, 2002; Lin etal., 2006; Mullen et al, 2005; Hirohata et al, 2008;Merity et al, 2009) we re-implemented and usedthem in our experiment1.
However, being awareof the fact that some of these features may not beoptimal for weakly-supervised learning (i.e.
whenlearning from smaller data), we evaluate their per-formance and suitability for the task later in sec-tion 4.3.?
Location.
Zones tend to appear in typical po-sitions in abstracts.
Each abstract was there-1The only exception is the history feature which was left outbecause it cannot be applied to all of our methods275fore divided into ten parts (1-10, measured bythe number of words), and the location was de-fined by the parts where the sentence beginsand ends.?
Word.
All the words in the corpus.?
Bi-gram.
Any combination of two adjacentwords in the corpus.?
Verb.
All the verbs in the corpus.?
Verb Class.
60 verb classes appearing inbiomedical journal articles.?
Part-of-Speech ?
POS.
The POS tag of eachverb in the corpus.?
Grammatical Relation ?
GR.
Subject (nc-subj), direct object (dobj), indirect object (iobj)and second object (obj2) relations in the cor-pus.
e.g.
(ncsubj observed 14 difference 5obj).
The value of this feature equals 1 if itoccurs in a particular sentence (and 0 if not).?
Subj and Obj.
The subjects and objects ap-pearing with any verbs in the corpus (extractedfrom above GRs).?
Voice.
The voice of verbs (active or passive) inthe corpus.These features were extracted from the corpus us-ing a number of tools.
A tokenizer was used to detectthe boundaries of sentences and to separate punctu-ation from adjacent words e.g.
in complex biomed-ical terms such as 2-amino-3,8-diethylimidazo[4,5-f]quinoxaline.
The C&C tools (Curran et al, 2007)trained on biomedical literature were employed forPOS tagging, lemmatization and parsing.
Thelemma output was used for creating Word, Bi-gramand Verb features.
The GR output was used for cre-ating the GR, Subj, Obj and Voice features.
The?obj?
marker in a subject relation indicates passivevoice (e.g.
(ncsubj observed 14 difference 5 obj)).The verb classes were acquired automatically fromthe corpus using the unsupervised spectral cluster-ing method of (Sun and Korhonen, 2009).
To con-trol the number of features we lemmatized the lexi-cal items for all the features, and removed the wordsand GRs with fewer than 2 occurrences and bi-gramswith fewer than 5 occurrences.3.2 Machine learning methodsSupport Vector Machines (SVM) and ConditionalRandom Fields (CRF) have proved the best perform-ing fully supervised methods in most recent workson information structure, e.g.
(Teufel and Moens,2002; Mullen et al, 2005; Hirohata et al, 2008; Guoet al, 2010).
We therefore implemented these meth-ods as well as weakly supervised variations of them:active SVM with and without self-training, transduc-tive SVM and semi-supervised CRF.3.2.1 Supervised methodsSVM constructs hyperplanes in a multidimen-sional space to separate data points of differentclasses.
Good separation is achieved by the hyper-plane that has the largest distance from the nearestdata points of any class.
The hyperplane has theform w ?
x ?
b = 0, where w is its normal vec-tor.
We want to maximize the distance from the hy-perplane to the data points, or the distance betweentwo parallel hyperplanes each of which separates thedata.
The parallel hyperplanes can be written as:w ?
x ?
b = 1 and w ?
x ?
b = ?1, and the dis-tance between them is 2|w| .
The problem reduces to:Minimize |w| (in w, b)Subject tow ?
x?
b ?
1 for x of one class,w ?
x?
b ?
?1 for x of the other,which can be solved by using the SMO algorithm(Platt, 1999b).
We used Weka software (Hall et al,2009) (employing its linear kernel) for SVM experi-ments.CRF is an undirected graphical model which de-fines a probability distribution over the hidden states(e.g.
label sequences) given the observations.
Theprobability of a label sequence y given an observa-tion sequence x can be written as:p(y|x, ?)
= 1Z(x)exp(?j ?jFj(y, x)),where Fj(y, x) is a real-valued feature function ofthe states and the observations; ?j is the weight ofFj , and Z(x) is a normalization factor.
The ?
pa-rameters can be learned using the L-BFGS algorithm(Nocedal, 1980).
We used Mallet software (McCal-lum, 2002) for CRF experiments.3.2.2 Weakly-supervised methodsActive SVM (ASVM) starts with a small amount oflabeled data, and iteratively chooses a proportion of276unlabeled data for which SVM has less confidenceto be labeled (the labels can be restored from theoriginal corpus) and used in the next round of learn-ing, i.e.
active learning.
Query strategies based onthe structure of SVM are frequently employed (Tongand Koller, 2001; Novak et al, 2006).
For exam-ple, it is often assumed that the data points close tothe separating hyperplane are those that the SVM isuncertain about.
Unlike these methods, our learn-ing algorithm compares the posterior probabilitiesof the best estimate given each unlabeled instance,and queries those with the lowest probabilities forthe next round of learning.
The probabilities can beobtained by fitting a Sigmoid after the standard SVM(Platt, 1999a), and combined using a pairwise cou-pling algorithm (Hastie and Tibshirani, 1998) in themulti-class case.
We used the SVM linear kernel inWeka for classification, and the -M flag in Weka forcalculating the posterior probabilities.Active SVM with self-training (ASSVM) is an ex-tension of ASVM where each round of training hastwo steps: (i) training on the labeled, and testingon the unlabeled data, and querying; (ii) training onboth labeled and unlabeled/machine-labeled data byusing the estimates from step (i).
The idea of ASSVMis to make the best use of the labeled data, and tomake the most use of the unlabeled data.Transductive SVM (TSVM) is an extension ofSVM which takes advantage of both labeled and un-labeled data (Vapnik, 1998).
Similar to SVM, theproblem is defined as:Minimize |w| (in w, b, y(u))Subject toy(l)(w ?
x(l) ?
b) ?
1,y(u)(w ?
x(u) ?
b) ?
1 ,y(u) ?
{?1, 1},where x(u) is unlabeled data and y(u) the estimateof its label.
The problem can be solved by usingthe CCCP algorithm (Collobert et al, 2006).
Weused UniverSVM software (Sinz, 2011) for TSVMexperiments.Semi-supervised CRF (SSCRF) can be imple-mented with entropy regularization (ER).
It ex-tends the objective function on Labeled data?L log p(y(l)|x(l), ?)
with an additional term?U?Y p(y|x(u), ?)
log p(y|x(u), ?)
to minimizethe conditional entropy of the model?s predictions onUnlabeled data (Jiao et al, 2006; Mann and Mccal-lum, 2007).
We used Mallet software (McCallum,2002) for SSCRF experiments.4 Experimental evaluation4.1 Evaluation methodsWe evaluated the ML results in terms of accuracy,precision, recall, and F-measure against manual AZannotations in the corpus:acc = no.
of correctly classified sentencestotal no.
ofsentences in the corpusp = no.
of sentences correctly identified as Classitotal no.
of sentences identified as Classir = no.
of sentences correctly identified as Classitotal no.
of sentences in Classif = 2?p?rp+rWe used 10-fold cross validation for all the meth-ods to avoid the possible bias introduced by rely-ing on any particular split of the data.
More specif-ically, the data was randomly assigned to ten foldsof roughly the same size.
Each fold was used onceas test data and the remaining nine folds as trainingdata.
The results were then averaged.Following (Dietterich, 1998), we used McNe-mar?s test (McNemar, 1947) to measure the statisti-cal significance between the results of different MLmethods.
The chosen significance level was .05.4.2 ResultsTable 3 shows the results for the four weakly-supervised and two supervised methods when 10%of the training data (i.e.
?700 sentences) has beenlabeled.
We can see that ASSVM is the best perform-ing method with an accuracy of 81% and the macroTable 3: Results when using 10% of the labeled dataAcc.
F-scoreMF BKG OBJ METH RES CON REL FUTSVM .77 .74 .84 .68 .71 .82 .64 - -CRF .70 .65 .75 .46 .48 .78 .76 - -ASVM .80 .75 .88 .56 .68 .87 .78 .33ASSVM .81 .76 .86 .56 .76 .88 .76 - -TSVM .76 .73 .84 .61 .71 .79 .71 - -SSCRF .73 .67 .76 .48 .52 .81 .78 - -MF: Macro F-score of the five high frequency categories:BKG, OBJ, METH, RES, CON.277Figure 2: Learning curve for different methods when using 0-100% of the labeled data0.30.40.50.60.70.80.910% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%LabeledAccuracySVM CRF ASVM ASSVM TSVM SSCRFFigure 3: Area under learning curves at different intervals00.10.20.30.40.50.60.70.80.91SVM CRF ASVM ASSVM TSVM SSCRFArea(0,10%] [10%,20%] [20%,40%] [40%,100%]F-score of .76 (the macro F-score is calculated forthe 5 scheme categories which are found by all themethods).
ASVM performs nearly as well, with anaccuracy of 80% and F-score of .75.
Both methodsoutperform supervised SVM with a statistically sig-nificant difference (p < .001).TSVM is the lowest performing SVM-basedmethod.
Yielding an accuracy of 76% and F-scoreof .73 its performance is lower than that of the super-vised SVM.
However, it does outperform both CRF-based methods.
SSCRF performs better than CRFwith 3% higher accuracy and .02 higher F-score.The difference in accuracy is statistically significant(p < .001).Only one method (ASVM) identifies six out of theseven possible categories.
Other methods identifyfive categories.
The 1-2 missing categories are verylow in frequency (accounting for 1% of the corpusdata each, see table 2).
Looking at the results forother categories, they seem to reflect the amount ofcorpus data available for each category (Table 2),with RES (Results) being the highest and OBJ (Ob-jective) the lowest performing category with mostmethods.
Interestingly, the only method that per-forms relatively well on OBJ is the supervised SVM.The best method ASSVM outperforms other meth-ods most clearly on METH (Method) category.
Al-though METH is a high frequency category (account-ing for 18% of the corpus data) other methods tendto confuse it with OBJ, presumably because a singlesentence may contain elements of both (e.g.
scien-tists may describe some of their method when de-scribing the objective of the study).Figure 2 shows the learning curve of differentmethods (in terms of accuracy) when the percentageof the labeled data (in the training set) ranges from 0to 100%.
ASSVM outperforms other methods, reach-ing its best performance of 88% accuracy when us-ing ?40% of the labeled data.
Indeed when using33% of the labeled data, it performs already equallywell as fully-supervised SVM using 100% of the la-beled data.
The advantage of ASSVM over ASVM(the second best method) is clear especially when20-40% of the labeled data is used.
SVM and TSVMtend to perform quite similarly with each other whenmore than 25% of the labeled data is used, but whenless data is available, SVM performs better.
Look-ing at the CRF-based methods, SSCRF outperformsCRF in particular when 10-25% of the labeled datais used.
However, neither of them reaches the per-formance level of SVM-based methods.Figure 3 shows the area under the learning curves(by the trapezoidal rule) at different intervals, whichgives a reasonable approximation to the overall per-formance of different methods.
The area underASSVM is the largest at each of the four intervals,with a value of .08 at (0,10%], .07 at [10%,20%],278.20 at [20%, 40%] and .50 at [40%,100%].
The dif-ference between supervised and weakly-supervisedmethods is more significant at (0, 20%] than at[20%,100%].4.3 Further analysis of the featuresAs explained in section 3.1, we employed in ourexperiments a collection of features which had per-formed well in previous supervised AZ experiments.We conducted further analysis to investigate whichof these features are the most (and the least) usefulfor weakly-supervised learning.
We took our bestperforming method ASSVM and conducted leave-one-out analysis of the features with 10% of the la-beled data.
The results are shown in Table 4.Table 4: Leaving one feature out results for ASSVM whenusing 10% of the labeled dataAcc.
F-scoreMF BKG OBJ METH RES CON REL FUTLocation .73 .67 .67 .55 .62 .85 .65 - -Word .80 .78 .87 .70 .74 .85 .72 - -Bigram .81 .75 .83 .57 .71 .87 .78 .33 -Verb .81 .79 .84 .77 .73 .87 .75 - -VC .79 .75 .86 .62 .72 .84 .70 - -POS .74 .70 .66 .65 .66 .82 .73 - -GR .79 .75 .83 .67 .69 .84 .72 - -Subj .80 .76 .87 .65 .73 .85 .72 - -Obj .80 .78 .84 .75 .70 .85 .75 - -Voice .78 .75 .88 .70 .71 .83 .62 - -?
.81 .76 .86 .56 .76 .88 .76 - -MF: Macro F-score of the five high frequency categories:BKG, OBJ, METH, RES, CON.?
: Employing all the features.We can see that the Location feature is by far themost useful feature for ASSVM.
The performancedrops 8% in accuracy and .09 in F-score in the ab-sence of this feature.
Location is particularly im-portant for BKG (which nearly always appears in thesame location: in the beginning of an abstract) and ishighly useful for METH and CON as well.
RemovingPOS has almost equally strong effect, in particularon BKG and METH, suggesting that verb tense is par-ticularly useful for distinguishing these categories.Also Voice, Verb class and GR contribute to gen-eral performance, especially to accuracy.
Voice isparticularly important for CON, which differs fromother categories in the sense that it is marked by fre-quent usage of active voice.
Verb class is helpful forMETH, RES and CON while GR is helpful for all highfrequency categories.Among the least helpful features are those whichsuffer from sparse data problems, including e.g.Word, Bi-gram, and Verb.
They perform particularlybadly when applied to low frequency zones.
How-ever, this is not the case when using fully-supervisedmethods (i.e.
100% of the labeled data), suggest-ing that a good performance in fully supervised ex-periments does not necessarily translate into a goodperformance in weakly-supervised experiments, andthat careful feature analysis and selection is impor-tant when aiming to optimize the performance whenlearning from sparse data.5 DiscussionIn our experiments, the majority of weakly-supervised methods outperformed their correspond-ing supervised methods when using just 10% ofthe labeled data.
The SVM-based methods per-formed better than the CRF-based ones (regardless ofwhether they were weakly or fully supervised).
Guoet al (2010) made a similar discovery when com-paring fully supervised versions of SVM and CRF.Our best performing weakly-supervised methodswere those based on active learning.
Making a gooduse of both labeled and unlabeled data, active learn-ing combined with self-training (ASSVM) proved tobe the most useful method.
Given 10% of the la-beled data, ASSVM obtained an accuracy of 81% andF-score of .76, outperforming the best supervisedmethod SVM with a statistically significant differ-ence.
It reached its top performance (88% accuracy)when using 40% of the labeled data, and performedequally well as fully supervised SVM (i.e.
100% ofthe labeled data) when using just one third of the la-beled data.This result is in line with the results of manyother text classification works where active learn-ing (alone or in combination with other techniquessuch as self-training) has proved similarly useful,e.g.
(Lewis and Gale, 1994; Tong and Koller, 2002;Brinker, 2006; Novak et al, 2006; Esuli and Sebas-tiani, 2009; Yang et al, 2009).While active learning iteratively explores theunknown aspects of the unlabeled data, semi-supervised learning attempts to make the best use279of what it already knows about the data.
In our ex-periments, semi-supervised methods (TSVM and SS-CRF) did not perform equally well as active learning?
TSVM even produced a lower accuracy than SVMwith the same amount of labeled data ?
althoughthese methods have gained success in related works.We therefore looked into related works usingTSVM, e.g.
(Chapelle and Zien, 2005), and discov-ered that our dataset is much higher in dimensional-ity than those employed in many other works.
Highdimensional data is more sensitive, and thereforefine-tuning with unlabeled data may cause a big de-viation.
We also looked into related works usingSSCRF, in particular the work of (Jiao et al, 2006)who used the same SSCRF as the one we used in ourexperiments.
Jiao et al (2006) employed a muchlarger data set than we did ?
one including 5448 la-beled instances (in 3 classes) and 5210-25145 unla-beled instances.
Given more labeled and unlabeleddata per class we might be able to obtain better per-formance using SSCRF also on our task.
However,given the high cost of obtaining labeled data meth-ods not needing it are preferable.6 Conclusions and future workOur experiments show that weakly-supervisedlearning can be used to identify AZ in scientificdocuments with good accuracy when only a limitedamount of labeled data is available.
This is helpfulthinking of the real-world application and porting ofthe approach to different tasks and domains.
To thebest of our knowledge, no previous work has beendone on weakly-supervised learning of informationstructure according to schemes of the type we havefocused on (Teufel and Moens, 2002; Mizuta et al,2006; Lin et al, 2006; Hirohata et al, 2008; Shatkayet al, 2008; Liakata et al, 2010).Recently, some work has been done on the relatedtask of classification of discourse relations in sci-entific texts: (Hernault et al, 2011) used structurallearning (Ando and Zhang, 2005) for this task.
Theyobtained 30-60% accuracy on the RST DiscourseTreebank (including 41 relation types) when using100-10000 labeled and 100000 unlabeled instances.The accuracy was 20-60% when using the labeleddata only.
However, although related, the task ofdiscourse relation classification differs substantiallyfrom our task in that it focuses on local discourse re-lations while our task focuses on the global structureof the scientific document.In the future, we plan to improve and extend thiswork in several directions.
First, the approach toactive learning could be improved in various ways.The query strategy we employed (uncertainty sam-pling) is a relatively straightforward method whichonly considers the best estimate for each unlabeledinstance, disregarding other estimates that may con-tain useful information.
In the future, we plan toexperiment with more sophisticated strategies, e.g.the margin sampling algorithm by (Scheffer et al,2001) and the query-by-committee (QBC) algorithmby (Seung et al, 1992).
In addition, there are al-gorithms designed for reducing the redundancy inqueries which may be worth investigating (Hoi et al,2006).Also, (Hoi et al, 2006) shows that Logistic Re-gression (LR) outperforms SVM when used with ac-tive learning, yielding higher F-score on the Reuters-21578 data set (binary classification, 10,788 docu-ments in total, 100 of them labeled).
It would beinteresting to explore whether supervised methodsother than SVM are optimal for active learning whenapplied to our task.Secondly, we plan to investigate other semi-supervised methods, for example, the Expectation-Maximization (EM) algorithm.
(Lanquillon, 2000)has shown that EM SVM performs better than super-vised and transductive SVM on a text classificationtask when applied to the dataset of 20 Newsgroups(20 classes, 4000 documents for testing, 10000 un-labeled ones), yielding up to ?10% higher accu-racy when 200-5000 labeled documents are used fortraining.In addition, other combinations of weakly-supervised methods might be worth looking into,such as EM+active learning (McCallum and Nigam,1998) and co-training+EM+active learning (Musleaet al, 2002), which have proved promising in relatedtext classification works.Besides looking for optimal ML strategies, weplan to look for optimal features for the task.
Ourfeature analysis showed that not all the featureswhich had proved promising in fully supervised ex-periments were equally promising when applied toweakly-supervised learning from smaller data.
We280plan to look into ways of reducing the sparse dataproblem in features, e.g.
by classifying not onlyverbs but also other word classes into semantically-motivated categories.One the key motivations for developing a weakly-supervised approach is to facilitate easy porting ofschemes such as AZ to new tasks and domains.
Re-cent research shows that active learning in a targetdomain can leverage information from a differentbut related (source) domain (Rai et al, 2010).
Mak-ing use of existing annotated datasets in biology,chemistry, computational linguistics and law (Teufeland Moens, 2002; Mizuta et al, 2006; Hacheyand Grover, 2006; Teufel et al, 2009) we will ex-plore optimal ways of combining weakly-supervisedlearning with domain-adaptation.The work presented in this paper has focused onthe abstracts annotated according to the AZ scheme.In the future, we plan to investigate the usefulnessof weakly-supervised learning for identifying otherschemes of information structure, e.g.
(Lin et al,2006; Hirohata et al, 2008; Shatkay et al, 2008;Liakata et al, 2010), and not only in scientific ab-stracts but also in full journal papers which typicallyexemplify a larger set of scheme categories.Finally, an important avenue of future researchis to evaluate the usefulness of weakly-supervisedidentification of information structure for NLP taskssuch as summarization and information extraction(Tbahriti et al, 2006; Ruch et al, 2007), and forpractical tasks such as manual review of scientificpapers for research purposes (Guo et al, 2010).AcknowledgmentsThe work reported in this paper was funded by theRoyal Society (UK).
YG was funded by the Cam-bridge International Scholarship.ReferencesSteven Abney.
2008.
Semi-supervised learning for com-putational linguistics.
Chapman & Hall / CRC.Rie Kubota Ando and Tong Zhang.
2005.
A frameworkfor learning predictive structures from multiple tasksand unlabeled data.
J. Mach.
Learn.
Res., 6:1817?1853.Klaus Brinker.
2006.
On active learning in multi-labelclassification.
In From Data and Information Analysisto Knowledge Engineering, pages 206?213.Olivier Chapelle and Alexander Zien.
2005.
Semi-supervised classification by low density separation.J.
Cohen.
1960.
A coefficient of agreement for nominalscales.
Educational and Psychological Measurement,20(1):37?46.Ronan Collobert, Fabian Sinz, Jason Weston, and Le?onBottou.
2006.
Trading convexity for scalability.
InProceedings of the 23rd international conference onMachine learning.J.
R. Curran, S. Clark, and J. Bos.
2007.
Linguisticallymotivated large-scale nlp with c&c and boxer.
In Pro-ceedings of the ACL 2007 Demonstrations Session.Thomas G. Dietterich.
1998.
Approximate statisticaltests for comparing supervised classification learningalgorithms.
Neural Comput., 10:1895?1923.Andrea Esuli and Fabrizio Sebastiani.
2009.
Activelearning strategies for multi-label text classification.In Proceedings of the 31th European Conference onIR Research on Advances in Information Retrieval.Yufan Guo, Anna Korhonen, Maria Liakata, Ilona SilinsKarolinska, Lin Sun, and Ulla Stenius.
2010.
Identi-fying the information structure of scientific abstracts:an investigation of three different schemes.
In Pro-ceedings of the 2010 Workshop on Biomedical NaturalLanguage Processing.Ben Hachey and Claire Grover.
2006.
Extractive sum-marisation of legal texts.
Artif.
Intell.
Law, 14:305?345.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The weka data mining software: an update.SIGKDD Explor.
Newsl., 11:10?18.T.
Hastie and R. Tibshirani.
1998.
Classification by pair-wise coupling.
Advances in Neural Information Pro-cessing Systems, 10.Hugo Hernault, Danushka Bollegala, and MitsuruIshizuka.
2011.
Semi-supervised discourse relationclassification with structural learning.
In CICLing (1).K.
Hirohata, N. Okazaki, S. Ananiadou, and M. Ishizuka.2008.
Identifying sections in scientific abstracts us-ing conditional random fields.
In Proceedings of 3rdInternational Joint Conference on Natural LanguageProcessing.Steven C. H. Hoi, Rong Jin, and Michael R. Lyu.
2006.Large-scale text categorization by batch mode activelearning.
In Proceedings of the 15th international con-ference on World Wide Web.F.
Jiao, S. Wang, C. Lee, R. Greiner, and D. Schuur-mans.
2006.
Semi-supervised conditional randomfields for improved sequence segmentation and label-ing.
In COLING/ACL.Carsten Lanquillon.
2000.
Learning from labeled andunlabeled documents: A comparative study on semi-supervised text classification.
In Proceedings of the2814th European Conference on Principles of Data Min-ing and Knowledge Discovery.David D. Lewis and William A. Gale.
1994.
A sequentialalgorithm for training text classifiers.
In Proceedingsof the 17th annual international ACM SIGIR confer-ence on Research and development in information re-trieval.M.
Liakata, S. Teufel, A. Siddharthan, and C. Batche-lor.
2010.
Corpora for the conceptualisation and zon-ing of scientific papers.
In Proceedings of the Seventhconference on International Language Resources andEvaluation (LREC?10).J.
Lin, D. Karakos, D. Demner-Fushman, and S. Khu-danpur.
2006.
Generative content models for struc-tural analysis of medical abstracts.
In Proceedings ofBioNLP-06.G.
S. Mann and A. Mccallum.
2007.
Efficient compu-tation of entropy gradient for semi-supervised condi-tional random fields.
In HLT-NAACL.Andrew McCallum and Kamal Nigam.
1998.
Employ-ing em and pool-based active learning for text classi-fication.
In Proceedings of the Fifteenth InternationalConference on Machine Learning.A.
K. McCallum.
2002.
Mallet: A machine learning forlanguage toolkit.
http://mallet.cs.umass.edu.Quinn McNemar.
1947.
Note on the Sampling Errorof the Difference Between Correlated Proportions orPercentages.
Psychometrika, 12(2):153?157.S.
Merity, T. Murphy, and J. R. Curran.
2009.
Accurateargumentative zoning with maximum entropy models.In Proceedings of the 2009 Workshop on Text and Ci-tation Analysis for Scholarly Digital Libraries.Y.
Mizuta, A. Korhonen, T. Mullen, and N. Collier.
2006.Zone analysis in biology articles as a basis for in-formation extraction.
International Journal of Med-ical Informatics on Natural Language Processing inBiomedicine and Its Applications, 75(6):468?487.T.
Mullen, Y. Mizuta, and N. Collier.
2005.
A base-line feature set for learning rhetorical zones using fullarticles in the biomedical domain.
Natural languageprocessing and text mining, 7(1):52?58.Ion Muslea, Steven Minton, and Craig A. Knoblock.2002.
Active + semi-supervised learning = robustmulti-view learning.
In Proceedings of the NineteenthInternational Conference on Machine Learning.Jorge Nocedal.
1980.
Updating Quasi-Newton Matriceswith Limited Storage.
Mathematics of Computation,35(151):773?782.Bla Novak, Dunja Mladeni, and Marko Grobelnik.
2006.Text classification with active learning.
In From Dataand Information Analysis to Knowledge Engineering,pages 398?405.J.
C. Platt.
1999a.
Probabilistic outputs for support vec-tor machines and comparisons to regularized likeli-hood methods.
Advances in Large Margin Classiers,pages 61?74.John C. Platt.
1999b.
Using analytic qp and sparsenessto speed training of support vector machines.
In Pro-ceedings of the 1998 conference on Advances in neuralinformation processing systems II.Piyush Rai, Avishek Saha, Hal Daume?, III, and SureshVenkatasubramanian.
2010.
Domain adaptationmeets active learning.
In Proceedings of the NAACLHLT 2010 Workshop on Active Learning for NaturalLanguage Processing.P.
Ruch, C. Boyer, C. Chichester, I. Tbahriti, A. Geiss-buhler, P. Fabry, J. Gobeill, V. Pillet, D. Rebholz-Schuhmann, C. Lovis, and A. L. Veuthey.
2007.
Usingargumentation to extract key sentences from biomedi-cal abstracts.
Int J Med Inform, 76(2-3):195?200.Tobias Scheffer, Christian Decomain, and Stefan Wro-bel.
2001.
Active hidden markov models for informa-tion extraction.
In Proceedings of the 4th InternationalConference on Advances in Intelligent Data Analysis.H.
S. Seung, M. Opper, and H. Sompolinsky.
1992.Query by committee.
In Proceedings of the fifth an-nual workshop on Computational learning theory.H.
Shatkay, F. Pan, A. Rzhetsky, and W. J. Wilbur.
2008.Multi-dimensional classification of biomedical text:Toward automated, practical provision of high-utilitytext to diverse users.
Bioinformatics, 24(18):2086?2093.F.
Sinz, 2011.
UniverSVM Support Vector Ma-chine with Large Scale CCCP Functionality.http://www.kyb.mpg.de/bs/people/fabee/universvm.html.L.
Sun and A. Korhonen.
2009.
Improving verb cluster-ing with automatically acquired selectional preference.In Proceedings of the Conference on Empirical Meth-ods in Natural Language Processing.I.
Tbahriti, C. Chichester, Frederique Lisacek, andP.
Ruch.
2006.
Using argumentation to retrievearticles with similar citations.
Int J Med Inform,75(6):488?495.S.
Teufel and M. Moens.
2002.
Summarizing scien-tific articles: Experiments with relevance and rhetor-ical status.
Computational Linguistics, 28:409?445.S.
Teufel, A. Siddharthan, and C. Batchelor.
2009.
To-wards domain-independent argumentative zoning: Ev-idence from chemistry and computational linguistics.In Proceedings of EMNLP.S.
Tong and D. Koller.
2001.
Support vector machineactive learning with applications to text classification.Journal of Machine Learning Research, 2:45?66.Simon Tong and Daphne Koller.
2002.
Support vectormachine active learning with applications to text clas-sification.
J. Mach.
Learn.
Res., 2:45?66.282V.
N. Vapnik.
1998.
Statistical learning theory.
Wiley,New York.Bishan Yang, Jian-Tao Sun, Tengjiao Wang, and ZhengChen.
2009.
Effective multi-label active learning fortext classification.
In Proceedings of the 15th ACMSIGKDD international conference on Knowledge dis-covery and data mining.283
