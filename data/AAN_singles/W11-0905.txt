Proceedings of the ACL 2011 Workshop on Relational Models of Semantics (RELMS 2011), pages 28?36,Portland, Oregon, USA, June 23, 2011. c?2011 Association for Computational LinguisticsVigNet: Grounding Language in Graphics using Frame SemanticsBob Coyne and Daniel Bauer and Owen RambowColumbia UniversityNew York, NY 10027, USA{coyne, bauer, rambow}@cs.columbia.eduAbstractThis paper introduces Vignette Semantics, alexical semantic theory based on Frame Se-mantics that represents conceptual and graph-ical relations.
We also describe a lexical re-source that implements this theory, VigNet,and its application in text-to-scene generation.1 IntroductionOur goal is to build a comprehensive text-to-graphics system.
When considering sentences suchas John is washing an apple and John is washingthe floor, we discover that rather different graphicalknowledge is needed to generate static scenes rep-resenting the meaning of these two sentences (seeFigure 1): the human actor is assuming differentposes, he is interacting differently with the thing be-ing washed, and the water, present in both scenes,is supplied differently.
If we consider the types ofknowledge needed for scene generation, we find thatwe cannot simply associate a single set of knowl-edge with the English verb wash.
The questionarises: how can we organize this knowledge andassociate it with lexical items, so that the resultinglexical knowledge base both is usable in a wide-coverage text-to-graphics system, and can be pop-ulated with the required knowledge using limited re-sources?In this paper, we present a new knowledge basethat we use for text-to-graphics generation.
We dis-tinguish three types of knowledge needed for ourtask.
The first is conceptual knowledge, which isknowledge about concepts, often evoked by words.For example, if I am told John bought an apple, thenI know that that event necessarily also involved theseller and money.
Second, we need world knowl-Figure 1: Mocked-up scenes using the WASH-SMALL-FRUIT vignette (?John washes the apple?)
and WASH-FLOOR-W-SPONGE vignette (?John washes the floor?).edge.
For example, apples grow on trees in cer-tain geographic locations at certain times of the year.Third, we need grounding knowledge, which tellsus how concepts are related to sensory experiences.In our application, we model grounding knowledgewith a database of 3-dimensional graphical models.We will refer to this type of grounding knowledgeas graphical knowledge.
An example of groundingknowledge is knowing that several specific graphicalmodels represent apple trees.Conceptual knowledge is already the object of ex-tensive work in frame semantics; FrameNet (Rup-penhofer et al, 2010) is an extensive (but not com-plete) relational semantic encoding of lexical mean-ing in a frame-semantic conceptual framework.
Weuse this prior work, both the theory and the resource,in our work.
The encoding of world knowledge hasbeen the topic of much work in Artificial Intelli-gence.
Our specific contribution in this paper is theintegration of the representation for world knowl-edge and graphical knowledge into a frame-semanticapproach.
In order to integrate these knowledgetypes, we extend FrameNet in three manners.1.
Frames describe complex relations betweentheir frame elements, but these relations, i.e.28the internal structure of a frame, is not explic-itly formulated in frame semantics.
FrameNetframes do not have any intensional meaningbesides the informal English definition of theframes (and what is expressed by so-called?frame-to-frame relations?).
From the pointof view of graphics generation, internal struc-ture is necessary.
While for many applicationsa semantic representation can remain vague, ascene must contain concrete objects and spatialrelations between them.2.
Some frames are not semantically specificenough.
For example, there is a frameSELF MOTION, which includes both walk andswim; these verbs clearly need different graph-ical realizations, but they are also differentfrom a general semantic point of view.
Whilethis situation could be remedied by extend-ing the inventory of frames by adding WALKand SWIM frames, which would inherit fromSELF MOTION, the situation is more complex.Consider wash an apple and wash the floor,discussed above.
While the core meaning ofwash is the same in both phrases, the graphi-cal realization is again very different.
However,we cannot simply create two new frames, sinceat some level (though not the graphical level)the meaning is indeed compositional.
We thusneed a new mechanism.3.
FrameNet is a lexical resource that illustrateshow language can be used to refer to frames,which are abstract definitions of concepts, andtheir frame elements.
It is not intended to bea formalism for deep semantic interpretation.The FrameNet annotations show the frame ele-ments of frames (e.g.
the goal frame element ofthe SELF MOTION frame) being filled with textpassages (e.g.
into the garden) rather than withconcrete semantic objects (e.g.
an ?instance?of a LOCALE BY USE frame evoked by gar-den).
Because such objects are needed in or-der to fully represent the meaning of a sentenceand to assert world knowledge, we introducesemantic nodes which are discourse referentsof lexical items (whereas frames describe theirmeanings).In this paper, we present VigNet, a resource whichextends FrameNet to incorporate world and graph-ical knowledge.
We achieve this goal by address-ing the three issues above.
We first extend framesby adding more information to them (specifically,about decomposition relevant to graphical ground-ing and more precise selectional restrictions).
Wecall a frame with graphical information a vignette.We then extend the structure defined by FrameNetby adding new frames and vignettes, for examplefor wash an apple.
The result we call VigNet.
Fi-nally, we extend VigNet with a system of nodeswhich instantiate frames; these nodes we call se-mantic nodes.
They get their meaning only from theframes they instantiate.
All three extensions are con-servative extensions of frames and FrameNet.
Thesemantic theory that VigNet instantiates we call Vi-gnette Semantics and we believe it to be a conser-vative extension (and thus in the spirit of) frame se-mantics.This paper is structured as follows.
In Section 2,we review frame semantics and FrameNet.
Section 3presents a more detailed description of VigNet, andwe provide examples in Section 4.
Since VigNet isintended to be used in a large-coverage system, thepopulation of VigNet with knowledge is a crucial is-sue which we address in Section 5.
We discuss re-lated work in Section 6 and conclude in Section 7.2 Frame Semantics and FrameNetFrame Semantics (FS; Fillmore (1982)) is based onthe idea that the meaning of a word can only be fullyunderstood in context of the entire conceptual struc-ture surrounding it, called the word?s frame.
Whenthe meaning of a word is evoked in a hearer?s mindall related concepts are activated simultaneously andwe can rely on this structure to transfer informationin a conversation.
Frames can describe states-of-affairs, events or complex objects.
Each frame con-tains a set of specific frame elements (FEs), whichare labeled semantic argument slots describing par-ticipants in the frame.
For instance, the word buyevokes the frame for a commercial transaction sce-nario, which includes a buyer and a seller that ex-change money for goods.
A speaker is aware of whattypical buyers, sellers, and goods are.
He may alsohave a mental prototype of the visual scenario itself29(e.g.
standing at a counter in a store).
In FS therole of syntactic theory and the lexicon is to explainhow the syntactic dependents of a word that realizesa frame (i.e.
arguments and adjuncts) are mapped toframe elements via valence patterns.FrameNet (FN; Baker et al (1998), Ruppenhoferet al (2010)) is a lexical resource based on FS.Frames in FN (around 1000) 1 are defined in termsof their frame elements, relations to other framesand semantic types of FEs.
Beyond this, the mean-ing of the frame (how the FEs are related to eachother) is only described in natural language.
FNcontains about 11,800 lexical units, which are pair-ings of words and frames.
These come with anno-tated example sentences (about 150,000) to illustratetheir valence patterns.
FN contains a network ofdirected frame-to-frame relations.
In the INHERI-TANCE relation a child-frame inherits all semanticproperties from the superframe.
The frame rela-tions SUBFRAME and PRECEDES refer to sub-eventsand events following in temporal order respec-tively.
The parent frame?s FEs are mapped to thechild?s FEs.
For instance CAUSE TO WAKE inher-its from TRANSITIVE ACTION and its sleeper FEmaps to agent.
Other relations include PERSPEC-TIVE ON, CAUSATIVE OF, and INCHOATIVE OF.Frame relations captures important semantic factsabout frames.
For instance the hierarchical organi-zation of INHERITANCE allows to view an event onvarying levels of specificity.
Finally, FN containsa small ontology of semantic types for frame ele-ments, which can be interpreted as selectional re-strictions (e.g.
an agent frame element must befilled by a sentient being).3 Vignette SemanticsIn Section 1, we motivated VigNet by the needfor a resource that allows us to relate language toa grounded semantics, where for us the graphicalrepresentation is a stand-in for grounding.
We de-scribed three reasons for extending FrameNet to Vi-gNet: we need more meaning in a frame, we needmore frames and more types of frames, and we needto instantiate frames in a clean manner.
We discussthese refinements in more detail in this section.1Numbers refer to FrameNet 1.5?
Vignettes are frames that are decomposed intographical primitives and can be visualized.Like other fames they are motivated by framesemantics; they correspond to a conceptualstructure evoked by the lexical units which areassociated with it.?
VigNet includes individual frames for each(content) lexical item.
This provides finer-grained semantics than given with FrameNetframes themselves.
These lexically-coupledframes leverage the existing structure of theirparent frames.
For example, the SELF MOTIONframe contains lexical items for run and swimwhich have very different meaning even thoughthey share the same frame and FEs (such asSOURCE, GOAL, and PATH).
We thereforedefine frames for RUN and SWIM which in-herit from SELF MOTION.
We assume also thatframes and lexical items that are missing fromFrameNet are defined and linked to the rest ofFrameNet as needed.?
Even more specific frames are created to rep-resent composed vignettes.
These are vi-gnettes that ground meaning in different waysthan the primitive vignette that they special-ize.
The only motivation for their existenceis the graphical grounding.
For example, wecannot determine how to represent washing anapple from the knowledge of how to repre-sent generic washing and an apple.
So we de-fine a new vignette specifically for washing asmall fruit.
From the point of view of lexi-cal semantics, it uses two lexical items (washand apple) and their interpretation, but for us,since we are interested in grounding, it is asingle vignette.
Note that it is not necessaryto create specific vignettes for every concreteverb/argument combination.
Because vignettesare visually inspired relatively few general vi-gnettes (e.g.
manipulate an object on a fixture)suffices to visualize many possible scenarios.?
A new type of frame-to-frame relation, whichwe call SUBFRAME-PARALLEL is used to de-compose vignettes into a set of more primitivesemantic relations between their arguments.Unlike FrameNet?s SUBFRAME relation which30represents temporally sequential subframes, inSUBFRAME-PARALLEL, the subframes are allactive at the same time, provide a conceptualand spatial decomposition of the frame, and canserve as spatial constraints on the frame ele-ments.
A frame is called a vignette if it canbe decomposed into graphical primitives usingSUBFRAME-PARALLEL relations.
For instancein the vignette WASH-SMALL-OBJ for washinga small object in a sink, the washer has to bein front of the sink.
We assert a SUBFRAME-PARALLEL relation between WASH-SMALL-OBJ and FRONTOF, mapping the washer FEto the figure FE and sink to ground.?
FrameNet has a very limited number of seman-tic types that are used to restrict the valuesof FEs.
Vignette semantics uses selectionalrestrictions to differentiate between vignettesthat have the same parent.
For example, thevignette invoked for washing a small object ina sink would restrict the semantic type of thetheme (the entity being washed) to anythingsmall, or, more generally, to any object that iswashed in this way (apples, hard-boiled eggs,etc).
The vignette used for washing a vehicle ina driveway with a hose would restrict its themeto some set of large objects or vehicle types.Selectional restrictions are asserted using thesame mechanism as decompositions.?
As mentioned in Section 1, in FrameNet an-notations frame elements (FEs) are filled withtext spans.
Therefore, while frame seman-tics in general is a deep semantic theory,FrameNet annotations only represent shallowsemantics and it is not immediately obvioushow FrameNet can be used to build a full se-mantic representations of a sentence.
In Vi-gnette semantics, when a frame is evoked bya lexical item, it is instantiated as a semanticnode.
Its FEs are then bound not to subphrases,but to semantic nodes which are the instantia-tions of the frames evoked by those subphrases.Section 3.1 investigates semantic nodes in more de-tail.
Section 3.2 illustrates different types of vi-gnettes (objects, actions, locations) and how they aredefined using the SUBFRAME PARALLEL relation.In Section 3.3 we discuss selectional restrictions.3.1 Semantic Nodes and Relational KnowledgeThe intuition behind semantic nodes is that they rep-resent objects, events or situations.
They can alsorepresent plurals or generics.
For instance we couldhave semantic node city, denoting the class of citiesand a semantic node paris, that denotes the cityParis.
Note that there is also a frame CITY and aframe PARIS that contain the conceptual structureassociated with the words city and Paris.
Framesrepresent the linguistic and the conceptual aspectof knowledge; the intensional meaning of a word.They provide knowledge to answer questions suchas ?What is an apple??
or ?How do you wash an ap-ple??.
In contrast, semantic nodes are extensional,i.e.
denotations.
They represent the knowledge toanswer questions such as ?In what season are applesharvested??
or ?How did Percy wash that apple justnow?
?.As mentioned above semantic nodes allow us tobuild full meaning representations of entire sen-tences in discourse.
Therefore, while frame defi-nitions are fixed, semantic nodes can be added dy-namically during discourse understanding or gener-ation to model the instances of frames that languageis evoking.
We call such nodes temporary seman-tic nodes.
They they are closely related to the dis-course referents of Discourse Representation Theory(Kamp, 1981) and related concepts in other theories.In contrast, persistent semantic nodes are used tostore world knowledge which is distinct from theconceptual knowledge encoded within frames andtheir relations; for example, the frame for moon willnot encode the fact that the moon?s circumference is6,790 miles, but we may record that using a knowl-edge based of external assertions semantic nodes aregiven their meaning by corresponding frames (CIR-CUMFERENCE, MILE, etc.).
A temporary semanticnode can become persistent by being retained in theknowledge base.3.2 Vignette Types and their DecompositionA vignette is a frame in the FrameNet sense that isdecomposed to a set of more primitive frames us-ing the SUBFRAME-PARALLEL frame-to-frame re-lation.
The frame elements (FEs) of a vignette are31defined as in FrameNet, except that our groundingin the graphical representation gives us a new, strongcriterion to choose what the FEs are: they are the ob-jects necessarily involved in the visual scene associ-ated with that vignette.
The subframes represent thespatial and other relations between the FEs.
The re-sulting semantic relations specify how the scene el-ements are spatially arranged.
This mechanism cov-ers several different cases.For actions, we conceptually freeze the action intime, much as in a comic book panel, and repre-sent it in a vignette with a set of objects, spatialrelations between those objects, and poses charac-teristic for the humans (and other pliable beings) in-volved in that action.
Action vignettes will typicallybe specialized to composed vignettes, so that the ap-plicability of different vignettes with the same par-ent frame will depend on the values of the FEs ofthe parent.
In the process of creating composed vi-gnettes, FEs are often added because additional ob-jects are required to play auxiliary roles.
As a re-sult, the FEs of an action vignette are the union ofthe semantic roles of the important participants andprops involved in that enactment of the action withthe FEs of the parent frame.
For instance the follow-ing vignette describes one concrete way of washinga small fruit.
Note that we have included a new FEsink which is not motivated in the frame WASH.2Note also that this vignette also contains a selec-tional restriction on its theme, which we will dis-cuss in the next subsection and which is not shownhere.WASH-SMALL-FRUIT(washer, theme, sink)FRONTOF(figure:washer, figure:sink)FACING(figure:washer, figure:sink)GRASP(grasper:washer, theme:theme)REACH(reacher:washer, target:sink)In this notation the head row contains the vignettename and its FEs in parentheses.
For readability wewill often omit FEs that are part of the vignette butnot restricted or used in any mentioned relation.
Thelower box contains the vignette decomposition andimplicitly specifies SUBFRAME-PARALLEL frame-to-frame relations.
In the decomposition of a vi-gnette V we use the notation F(a:b, ?
?
? )
to indicatethat the FE a of frame F is mapped to the FE b of V.2FrameNet does not currently contain a WASH frame, but ifit did, it would not contain an FE sink.When V is instantiated the semantic node binding toa must also be able to bind to b in F.Locations are represented by vignettes which ex-press constraints between a set of objects character-istic for the given location.
The FEs of location vi-gnettes include these constituent objects.
For exam-ple, one type of living room (of many possible ones)might contain a couch, a coffee table, and a fireplacein a certain arrangement.LIVING-ROOM 42(left wall, far wall, couch,coffee table, fireplace)TOUCHING(figure:couch, ground:left wall)FACING(figure:couch, ground:right wall)FRONTOF(figure:coffee table, ground: sofa)EMBEDDED(figure:fire-place, ground:far wall)Even ordinary physical objects will have certaincharacteristic parts with size, shape, and spatial re-lations that can be expressed by vignettes.
For ex-ample, an object type such as a kind of stop sign canbe defined as a two-foot-wide, red, hexagonal metalsheet displaying the word ?STOP?
positioned on thetop of a 6 foot high post.STOP-SIGN(sign-part, post-part, texture)MATERIAL(theme:sign-part, material:METAL)MATERIAL(theme:post-part, material:METAL)DIAMETER(theme:sign-part, diameter:2 feet)HEIGHT(theme:post-part, height:6 feet)ONTOP(figure:sign-part, ground:post-part)TEXTURE(theme:sign-part, texture:?STOP?
)In addition, many real-world objects do not corre-spond to lexical items but are elaborations on themor combinations.
These sublexical entities can berepresented by vignettes as well.
For example, onesuch 3D object in our text-to-scene system is a goathead mounted on a piece of wood.
This object isrepresented by a vignette with two FEs (ghead,gwood) representing the goat?s head and the wood.The vignette decomposes into ON(ghead, gwood).While there can be many vignettes for a singlelexical item, representing the many ways a location,action, or object can be constituted, vignettes neednot be specialized for every particular situation andcan be more or less general.
In one exteme creat-ing vignettes for every verb/argument combinationwould clearly lead to a combinatorial explosion andis not feasible.
In the other extreme we can definerather general vignettes.
For example, a vignette32USE-TOOL for using a tool on a theme can be repre-sented by the user GRASPING the tool and REACH-ING towards the theme.
These vignettes can beused in decompositions of more concrete vignettes(e.g.
HAMMER-NAIL-INTO-WALL).
They can alsobe used directly if no other more concrete vignettecan be applied (because it does not exist or its selec-tional restrictions cannot be satisfied).
In this wayby defining a small set of such vignettes we can vi-sualize approximate scenes for a large number of de-scriptions.3.3 Selectional Restrictions on Frame ElementsTo define a frame we need to specify selectional re-strictions on the semantic type of its FEs.
Insteadof relying on a fixed inventory of semantic types,we assert conceptual knowledge and external asser-tions over persistent semantic types.
This allows usto use VigNet?s large set of frames to represent suchknowledge.
For example, an apple can be defined asa small round fruit.APPLE(self)SHAPEOF(figure:self, shape:spherical)SIZEOF(figure:self, size:small)APPLE is simply a frame that contains a self FE,which allows us to make assertions about the con-cept (i.e.
about any semantic node bound to theself FE).
Frame elements of this type are not un-usual in FrameNet, where they are mainly used forframes containing common nouns (for instance theSubstance FE contains a substance FE).
In Vi-gNet we implicitly use self in all frames, includingframes describing situations and events.We use the same mechanism to define specializedcompound vignettes such as WASH SMALL FRUIT.We extend WASH in the following way to restrictit to small fruits (we abreviate F(self:a) as a=F forreadability).WASH-SMALL-FRUIT(washer, theme, sink)% selectional restrictionssink=SINK, washer=PERSON,theme=x, x=FRUIT,SIZEOF(figure:x,size:small)% decompositionFRONTOF(figure:washer, figure:sink)FACING(figure:washer, figure:sink)GRASP(grasper:washer, theme:theme)REACH(reacher:washer, target:sink)4 ExamplesIn this section we give further examples of visualaction vignettes for the verb wash.
The selectionalrestrictions and graphical decomposition of these vi-gnettes vary depending on the type of object be-ing washed.
The first example shows a vignette forwashing a vehicle.WASH-VEHICLE(washer, theme, instr, location)washer=PERSON, theme=VEHICLE,instr=HOSE, location=DRIVEWAYONSURFACE(figure:theme, ground:location)FRONTOF(figure:washer, ground:theme)FACING(figure:washer, ground:theme)GRASP(grasper:washer, theme:instrument)AIM(aimer:washer, theme:instr, target:theme)The following two vignettes represent a case wherethe object being washed alone does not determinewhich vignette to apply.
If the instrument is unspec-ified one or the other could be used.
We illustrateone option in figure 1 (right).WASH-FLOOR-W-SPONGE(washer,theme,instr)washer=PERSON, theme=FLOOR,instr=SPONGEKNEELING(agent:washer),GRASP(grasper:washer, theme:instr),REACH(reacher:washer, target:theme)WASH-FLOOR-W-MOP(washer, theme, instr)washer=PERSON, theme=FLOOR, instr=MOPGRASP(grasper:washer, theme:instr),REACHWITH(reacher:washer, target:theme,instr:instr)It is easy to come up with other concrete vi-gnettes for wash (washing windows, babies, hands,dishes...).
As mentioned in section 3.2 more gen-eral vignettes can be defined for very broad objectclasses.
In choosing vignettes, the most specific willbe used (looking at type matching hierarchies), sogeneral vignettes will only be chosen when morespecific ones are unavailable.
The following genericvignette describes washing any large object.WASH-LARGE-OBJECT(washer, theme instrument)washer=PERSON, theme=OBJECT,instrument=SPONGE,SIZEOF(figure:theme, size:large)FACING(figure:washer, ground:theme)GRASP(grasper:washer, theme:instrument)REACH(reacher:washer, target:theme)33In our final example, a vignette for picking fruit usesthe following assertion of world knowledge aboutparticular types of fruit and the trees they comefrom:SOURCE-OF(theme:x, source:y), APPLE(self:x),APPLETREE(self:y)In matching the vignette to the verb frame and its ar-guments, the source frame element is bound to thetype of tree for the given theme (fruit).PICK-FRUIT(picker, theme, source)picker=PERSON, theme=FRUIT, source=TREE,SOURCEOF(theme:theme, source:source)UNDERCANOPY(figure:picker, canopy:source)GRASP(grasper:picker, theme:theme)REACH(reacher:picker, target:source.branch)5 VigNetWe are developing VigNet as a general purpose re-source, but with the specific goal of using it in text-to-scene generation.
In this section we first describevarious methods to populate VigNet.
We then sketchhow we create graphical representations from Vi-gNet meaning representations.5.1 Populating VigNetVigNet is being populated using several approaches:?
Amazon Mechanical Turk is being used to ac-quire scene elements for location and action vi-gnettes as well as the spatial relations amongthose elements.
For locations, Turkers areshown representative pictures of different lo-cations as well as variants of similar locations,thereby providing distinct vignettes for each lo-cation.
We also use Mechanical Turk to acquiregeneral purpose relational information for ob-jects and actions such as default locations, ma-terials, contents, and parts.?
We extract relations such as typical locationsfor actions from corpora based on co-occurancepatterns of location and action terms.
This isbased on ideas described in (Sproat, 2001).
Wealso rely on corpora to induce new lexical unitsand selectional preferences.?
A large set of semantic nodes and frames fornouns has been imported from the noun lexiconof the WordsEye text-to-scene system (Coyneand Sproat, 2001).
This lexicon currently con-tains 15,000 lexical items and is tied to a li-brary of 2,200 3D objects and 10,000 imagesSemantic relations between these nodes includeparthood, containment, size, style (e.g.
antiqueor modern), overall shape, material, as well asspatial tags denoting important spatial regionson the object.
We also import graphically-oriented vignettes from WordsEye.
These areused to capture the meaning of sub-lexical 3Dobjects such as the mounted goat head de-scribed earlier.?
Finally, we intend to use WordsEye itself to al-low users to visualize vignettes as they definethem, as a way to improve vignette accuracyand relevancy to the actual use of the system.While the population of VigNet is not the fo-cus of this paper, it is our goal to create a usableresource that can be populated with a reasonableamount of effort.
We note that opposed to resourceslike FrameNet that require skilled lexicographers,we only need simple visual annotation that can eas-ily be done by untrained Mechanical Turkers.
Inaddition, as described in section 3.2, vignettes de-fined at more abstract levels of the frame hierar-chy can be used and composed to cover large num-bers of frames in a plausible manner.
This allowsmore specific vignettes to be defined where the dif-ferences are most significant.
VigNet is is focusedon visually-oriented language involving tangible ob-jects.
However, abstract, process-oriented languageand relations such as negation can be depicted icon-ically with general vignettes.
Examples of these canbe seen in the figurative and metaphorical depictionsshown in (Coyne and Sproat, 2001).5.2 Using VigNet in Text-to-Scene GenerationTo compose a scene from text input such as theman is washing the apple it is necessary to parsethe sentence into a semantic representation (evokingframes for each content word) and to then resolvethe language-level semantics to a set of graphicalentities and relations.
To create a low-level graph-ical representation all frame elements need to befilled with appropriate semantic nodes.
Frames sup-port the selection of these nodes by specifying con-straints on them using selectional restrictions.
The34SUBFRAME-PARALLEL decomposition of vignettesthen ultimately relates these nodes using elementaryspatial vignettes (FRONTOF, ON, ...).Note that it is possible to describe scenes directlyusing these vignettes (such as The man is in front ofthe sink.
He is holding an apple.
), as was used tocreate the mock-ups in figure 1.Vignettes can be directly applied or composed to-gether.
Composing vignettes involves unifying theirframe elements.
For example, in washing an ap-ple, the WASH-SMALL-FRUIT vignette uses a sink.From world knowledge we know (via instances ofthe TYPICAL-LOCATION frame) that washing foodtypically takes place in the KITCHEN.
To create ascene we compose the two vignettes together by uni-fying the sink in the location vignette with the sinkin the action vignette.6 Related WorkThe grounding of natural language to graphical re-lations has been investigated in very early text-to-scene systems (Boberg, 1972), (Simmons, 1975),(Kahn, 1979), (Adorni et al, 1984), and then laterin Put (Clay and Wilhelms, 1996), and WordsEye(Coyne and Sproat, 2001).
Other systems, such asCarSim (Dupuy et al, 2001), Jack (Badler et al,1998), and CONFUCIUS (Ma and McKevitt, 2006)target animation and virtual environments ratherthan scene construction.
A graphically groundedlexical-semantic resource such as VigNet would beof use to these and related domains.
The concept ofvignettes as graphical realizations of more generalframes was introduced in (Coyne et al, 2010).In addition to FrameNet, much work has beendone in developing theories and resources for lexi-cal semantics and common-sense knowledge.
Verb-Net (Kipper et al, 2000) focuses on verb subcat pat-terns grouped by Levin verb classes (Levin, 1993),but also grounds verb semantics into a small num-ber of causal primitives representing temporal con-straints tied to causality and state changes.
VerbNetlacks the ability to compose semantic constraintsor use arbitrary semantic relations in those con-straints.
Conceptual Dependency theory (Schankand Abelson, 1977) specifies a small number ofstate-change primitives into which all verbs are re-duced.
Event Logic (Siskind, 1995) decomposes ac-tions into intervals describing state changes and al-lows visual grounding by specifying truth conditionsfor a small set of spatial primitives (a similar for-malism is used by Ma and McKevitt (2006)).
(Bai-ley et al, 1998) and related work proposes a rep-resentation in many ways similar to ours, in whichlexical items are paired with a detailed specifica-tion of actions in terms of elementary body posesand movements.
In contrast to these temporally-oriented approaches, VigNet grounds semantics inspatial constraints active at a single moment in time.This allows for and emphasizes contextual reason-ing rather than causal reasoning.
In addition, VigNetemphasizes a holistic frame semantic perspective,rather than emphasizing decomposition alone.
Sev-eral resources for common-sense knowledge exist orhave been proposed.
In OpenMind and ConceptNet(Havasi et al, 2007) online crowd-sourcing is usedto collect a large set of common-sense assertions.These assertions are normalized into a set of a cou-ple dozen relations.
The Cyc project is using the webto augment its large ontology and knowledge base ofcommon sense knowledge (Matuszek et al, 2005).PRAXICON (Pastra, 2008) is a grounded concep-tual resources that integrates motor-sensoric, visual,pragmatic and lexical knowledge (via WordNet).
Ittargets the embodied robotics community and doesnot directly focus on scene generation.
It also fo-cuses on individual lexical items, while VigNet, likeFrameNet, takes syntactic context into account.7 ConclusionWe have described a new semantic paradigm that wecall vignette semantics.
Vignettes are extensions ofFrameNet frames and represent the specific ways inwhich semantic frames can be realized in the world.Mapping frames to vignettes involves translating be-tween high-level frame semantics and the lower-level relations used to compose a scene.
Knowledgeabout objects, both in terms of their semantic typesand the affordances they provide is used to make thattranslation.
FrameNet frames, coupled with seman-tic nodes representing entity classes, provide a pow-erful relational framework to express such knowl-edge.
We are developing a new resource VigNetwhich will implement this framework and be usedin our text-to-scene generation system.35ReferencesG.
Adorni, M. Di Manzo, and F. Giunchiglia.
1984.
Nat-ural Language Driven Image Generation.
In Proceed-ings of COLING 1984, pages 495?500, Stanford, CA.N.
Badler, R. Bindiganavale, J. Bourne, M. Palmer, J. Shi,and W. Schule.
1998.
A parameterized action rep-resentation for virtual human agents.
In Workshopon Embodied Conversational Characters, Tahoe City,CA.D.
Bailey, N. Chang, J. Feldman, and S. Narayanan.1998.
Extending Embodied Lexical Development.
InProceedings of the Annual Meeting of the CognitiveScience Society, Madison, WI.C.
Baker, C. Fillmore, and J. Lowe.
1998.
The BerkeleyFramenet Project.
In Proceedings of COLING 1998,pages 86?90.R.
Boberg.
1972.
Generating line drawings from ab-stract scene descriptions.
Master?s thesis, Dept.
ofElec.
Eng, MIT, Cambridge, MA.S.
R. Clay and J. Wilhelms.
1996.
Put: Language-basedinteractive manipulation of objects.
IEEE ComputerGraphics and Applications, 16(2):31?39.B.
Coyne and R. Sproat.
2001.
WordsEye: An automatictext-to-scene conversion system.
In Proceedings ofthe Annual Conference on Computer Graphics, pages487?496, Los Angeles, CA.B.
Coyne, O. Rambow, J. Hirschberg, and R. Sproat.2010.
Frame Semantics in Text-to-Scene Generation.In Proceedings of the KES?10 workshop on 3D Visual-isation of Natural Language, Cardiff, Wales.S.
Dupuy, A. Egges, V. Legendre, and P. Nugues.
2001.Generating a 3D Simulation Of a Car Accident from aWritten Description in Natural Language: The CarSimSystem.
In Proceedings of ACL Workshop on Tem-poral and Spatial Information Processing, pages 1?8,Toulouse, France.C.
J. Fillmore.
1982.
Frame semantics.
In LinguisticSociety of Korea, editor, Linguistics in the MorningCalm, pages 111?137.
Hanshin Publishing Company,Seoul.C.
Havasi, R. Speer, and J. Alonso.
2007.
ConceptNet 3:a Flexible, Multilingual Semantic Network for Com-mon Sense Knowledge.
In Proceedings of RANLP2007, Borovets, Bulgaria.K.
Kahn.
1979.
Creation of Computer Animation fromStory Descriptions.
Ph.D. thesis, MIT, AI Lab, Cam-bridge, MA.H.
Kamp.
1981.
A Theory of Truth and Semantic Rep-resentation.
In Groenendijk, J. and Janssen, T. andStokhof, M., editor, Formal Methods in the Study ofLanguage, pages 277?322.
de Gruyter, Amsterdam.K.
Kipper, H. T. Dang, and M. Palmer.
2000.
Class-Based Construction of a Verb Lexicon.
In Proceedingsof AAAI 2000, Austin, TX.B.
Levin.
1993.
English verb classes and alternations:a preliminary investigation.
University Of ChicagoPress.M.
Ma and P. McKevitt.
2006.
Virtual human anima-tion in natural language visualisation.
Artificial Intel-ligence Review, 25:37?53, April.C.
Matuszek, M. Witbrock, R. C. Kahlert, J. Cabral,D.
Schneider, P. Shah, and D. Lenat.
2005.
Search-ing for Common Sense: Populating Cyc from the Web.In Proceedings of AAAI 2005, pages 1430?1435, Pitts-burgh, PA.K.
Pastra.
2008.
PRAXICON: The Development of aGrounding Resource.
In Proceedings of the Interna-tional Workshop on Human-Computer Conversation,Bellagio, Italy.J.
Ruppenhofer, M. Ellsworth, M. Petruck, C. R. John-son, and J. Scheffczyk.
2010.
Framenet II: ExtendedTheory and Practice.
ICSI Berkeley.R.
C. Schank and R. Abelson.
1977.
Scripts, Plans,Goals, and Understanding.
Earlbaum, Hillsdale, NJ.R.
Simmons.
1975.
The CLOWNS Microworld.
In Pro-ceedings of the Workshop on Theoretical Issues in Nat-ural Language Processing, pages 17?19, Cambridge,MA.J.
M. Siskind.
1995.
Grounding language in perception.Artificial Intelligence Review, 8:371?391.R.
Sproat.
2001.
Inferring the environment in a text-to-scene conversion system.
In International Conferenceon Knowledge Capture, Victoria, BC.36
