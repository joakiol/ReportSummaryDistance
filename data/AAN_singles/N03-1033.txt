Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency NetworkKristina Toutanova Dan KleinComputer Science Dept.
Computer Science Dept.Stanford University Stanford UniversityStanford, CA 94305-9040 Stanford, CA 94305-9040kristina@cs.stanford.edu klein@cs.stanford.eduChristopher D. Manning Yoram SingerComputer Science Dept.
School of Computer ScienceStanford University The Hebrew UniversityStanford, CA 94305-9040 Jerusalem 91904, Israelmanning@stanford.edu singer@cs.huji.ac.ilAbstractWe present a new part-of-speech tagger thatdemonstrates the following ideas: (i) explicituse of both preceding and following tag con-texts via a dependency network representa-tion, (ii) broad use of lexical features, includ-ing jointly conditioning on multiple consecu-tive words, (iii) effective use of priors in con-ditional loglinear models, and (iv) fine-grainedmodeling of unknown word features.
Usingthese ideas together, the resulting tagger givesa 97.24% accuracy on the Penn Treebank WSJ,an error reduction of 4.4% on the best previoussingle automatically learned tagging result.1 IntroductionAlmost all approaches to sequence problems such as part-of-speech tagging take a unidirectional approach to con-ditioning inference along the sequence.
Regardless ofwhether one is using HMMs, maximum entropy condi-tional sequence models, or other techniques like decisiontrees, most systems work in one direction through thesequence (normally left to right, but occasionally rightto left, e.g., Church (1988)).
There are a few excep-tions, such as Brill?s transformation-based learning (Brill,1995), but most of the best known and most successfulapproaches of recent years have been unidirectional.Most sequence models can be seen as chaining to-gether the scores or decisions from successive local mod-els to form a global model for an entire sequence.
Clearlythe identity of a tag is correlated with both past and futuretags?
identities.
However, in the unidirectional (causal)case, only one direction of influence is explicitly consid-ered at each local point.
For example, in a left-to-rightfirst-order HMM, the current tag t0 is predicted based onthe previous tag t?1 (and the current word).1 The back-ward interaction between t0 and the next tag t+1 showsup implicitly later, when t+1 is generated in turn.
Whileunidirectional models are therefore able to capture bothdirections of influence, there are good reasons for sus-pecting that it would be advantageous to make informa-tion from both directions explicitly available for condi-tioning at each local point in the model: (i) because ofsmoothing and interactions with other modeled features,terms like P(t0|t+1, .
.
.)
might give a sharp estimate of t0even when terms like P(t+1|t0, .
.
.)
do not, and (ii) jointlyconsidering the left and right context together might beespecially revealing.
In this paper we exploit this idea,using dependency networks, with a series of local con-ditional loglinear (aka maximum entropy or multiclasslogistic regression) models as one way of providing ef-ficient bidirectional inference.Secondly, while all taggers use lexical information,and, indeed, it is well-known that lexical probabilitiesare much more revealing than tag sequence probabilities(Charniak et al, 1993), most taggers make quite limiteduse of lexical probabilities (compared with, for example,the bilexical probabilities commonly used in current sta-tistical parsers).
While modern taggers may be more prin-cipled than the classic CLAWS tagger (Marshall, 1987),they are in some respects inferior in their use of lexicalinformation: CLAWS, through its IDIOMTAG module,categorically captured many important, correct taggingsof frequent idiomatic word sequences.
In this work, weincorporate appropriate multiword feature templates sothat such facts can be learned and used automatically by1Rather than subscripting all variables with a position index,we use a hopefully clearer relative notation, where t0 denotesthe current position and t?n and t+n are left and right contexttags, and similarly for words.Edmonton, May-June 2003Main Papers , pp.
173-180Proceedings of HLT-NAACL 2003w1 w2 w3 .
.
.
.
wnt1 t2 t3 tn(a) Left-to-Right CMMw1 w2 w3 .
.
.
.
wnt1 t2 t3 tn(b) Right-to-Left CMMw1 w2 w3 .
.
.
.
wnt1 t2 t3 tn(c) Bidirectional Dependency NetworkFigure 1: Dependency networks: (a) the (standard) left-to-rightfirst-order CMM, (b) the (reversed) right-to-left CMM, and (c)the bidirectional dependency network.the model.Having expressive templates leads to a large numberof features, but we show that by suitable use of a prior(i.e., regularization) in the conditional loglinear model ?something not used by previous maximum entropy tag-gers ?
many such features can be added with an overallpositive effect on the model.
Indeed, as for the voted per-ceptron of Collins (2002), we can get performance gainsby reducing the support threshold for features to be in-cluded in the model.
Combining all these ideas, togetherwith a few additional handcrafted unknown word fea-tures, gives us a part-of-speech tagger with a per-positiontag accuracy of 97.24%, and a whole-sentence correctrate of 56.34% on Penn Treebank WSJ data.
This is thebest automatically learned part-of-speech tagging resultknown to us, representing an error reduction of 4.4% onthe model presented in Collins (2002), using the samedata splits, and a larger error reduction of 12.1% from themore similar best previous loglinear model in Toutanovaand Manning (2000).2 Bidirectional Dependency NetworksWhen building probabilistic models for tag sequences,we often decompose the global probability of sequencesusing a directed graphical model (e.g., an HMM (Brants,2000) or a conditional Markov model (CMM) (Ratna-parkhi, 1996)).
In such models, the probability assignedto a tagged sequence of words x = ?t, w?
is the productof a sequence of local portions of the graphical model,one from each time slice.
For example, in the left-to-rightCMM shown in figure 1(a),P(t, w) =?iP(ti |ti?1, wi )That is, the replicated structure is a local modelP(t0|t?1, w0).2 Of course, if there are too many con-ditioned quantities, these local models may have to beestimated in some sophisticated way; it is typical in tag-ging to populate these models with little maximum en-tropy models.
For example, we might populate a modelfor P(t0|t?1, w0) with a maxent model of the form:P?
(t0|t?1, w0) =exp(??t0,t?1?
+ ??t0,w0?
)?t ?0exp(?
?t ?0,t?1?
+ ?
?t ?0,w0?
)In this case, the w0 and t?1 can have joint effects on t0, butthere are not joint features involving all three variables(though there could have been such features).
We say thatthis model uses the feature templates ?t0, t?1?
(previoustag features) and ?t0, w0?
(current word features).Clearly, both the preceding tag t?1 and following tagt+1 carry useful information about a current tag t0.
Unidi-rectional models do not ignore this influence; in the caseof a left-to-right CMM, the influence of t?1 on t0 is ex-plicit in the P(t0|t?1, w0) local model, while the influ-ence of t+1 on t0 is implicit in the local model at the nextposition (via P(t+1|t0, w+1)).
The situation is reversedfor the right-to-left CMM in figure 1(b).From a seat-of-the-pants machine learning perspective,when building a classifier to label the tag at a certain posi-tion, the obvious thing to do is to explicitly include in thelocal model all predictive features, no matter on whichside of the target position they lie.
There are two goodformal reasons to expect that a model explicitly condi-tioning on both sides at each position, like figure 1(c)could be advantageous.
First, because of smoothingeffects and interaction with other conditioning features(like the words), left-to-right factors like P(t0|t?1, w0)do not always suffice when t0 is implicitly needed to de-termine t?1.
For example, consider a case of observationbias (Klein and Manning, 2002) for a first-order left-to-right CMM.
The word to has only one tag (TO) in the PTBtag set.
The TO tag is often preceded by nouns, but rarelyby modals (MD).
In a sequence will to fight, that trendindicates that will should be a noun rather than a modalverb.
However, that effect is completely lost in a CMMlike (a): P(twill |will, ?star t?)
prefers the modal tagging,and P(TO|to, twill ) is roughly 1 regardless of twill .
Whilethe model has an arrow between the two tag positions,that path of influence is severed.3 The same problem ex-ists in the other direction.
If we use the symmetric right-2Throughout this paper we assume that enough boundarysymbols always exist that we can ignore the differences whichwould otherwise exist at the initial and final few positions.3Despite use of names like ?label bias?
(Lafferty et al, 2001)or ?observation bias?, these effects are really just unwantedexplaining-away effects (Cowell et al, 1999, 19), where twonodes which are not actually in causal competition have beenmodeled as if they were.A B A B A B(a) (b) (c)Figure 2: Simple dependency nets: (a) the Bayes?
net forP(A)P(B|A), (b) the Bayes?
net for P(A|B)P(B), (c) a bidi-rectional net with models of P(A|B) and P(B|A), which is nota Bayes?
net.to-left model, fight will receive its more common nountagging by symmetric reasoning.
However, the bidirec-tional model (c) discussed in the next section makes bothdirections available for conditioning at all locations, us-ing replicated models of P(t0|t?1, t+1, w0), and will beable to get this example correct.42.1 Semantics of Dependency NetworksWhile the structures in figure 1(a) and (b) are well-understood graphical models with well-known semantics,figure 1(c) is not a standard Bayes?
net, precisely becausethe graph has cycles.
Rather, it is a more general de-pendency network (Heckerman et al, 2000).
Each noderepresents a random variable along with a local condi-tional probability model of that variable, conditioned onthe source variables of all incoming arcs.
In this sense,the semantics are the same as for standard Bayes?
nets.However, because the graph is cyclic, the net does notcorrespond to a proper factorization of a large joint prob-ability estimate into local conditional factors.
Considerthe two-node cases shown in figure 2.
Formally, for thenet in (a), we can write P(a, b) = P(a)P(b|a).
For (b)we write P(a, b) = P(b)P(a|b).
However, in (c), thenodes A and B carry the information P(a|b) and P(b|a)respectively.
The chain rule doesn?t allow us to recon-struct P(a, b) by multiplying these two quantities.
Un-der appropriate conditions, we could reconstruct P(a, b)from these quantities using Gibbs sampling, and, in gen-eral, that is the best we can do.
However, while recon-structing the joint probabilities from these local condi-tional probabilities may be difficult, estimating the localprobabilities themselves is no harder than it is for acyclicmodels: we take observations of the local environmentsand use any maximum likelihood estimation method wedesire.
In our experiments, we used local maxent models,but if the event space allowed, (smoothed) relative countswould do.4The effect of indirect influence being weaker than direct in-fluence is more pronounced for conditionally structured models,but is potentially an issue even with a simple HMM.
The prob-abilistic models for basic left-to-right and right-to-left HMMswith emissions on their states can be shown to be equivalent us-ing Bayes?
rule on the transitions, provided start and end sym-bols are modeled.
However, this equivalence is violated in prac-tice by the addition of smoothing.function bestScore()return bestScoreSub(n + 2, ?end, end, end?
);function bestScoreSub(i + 1, ?ti?1, ti , ti+1?
)% memoizationif (cached(i + 1, ?ti?1, ti , ti+1?
))return cache(i + 1, ?ti?1, ti , ti+1?
);% left boundary caseif (i = ?1)if (?ti?1, ti , ti+1?
== ?star t, star t, star t?
)return 1;elsereturn 0;% recursive casereturn maxti?2 bestScoreSub(i, ?ti?2, ti?1, ti ?
)?P(ti |ti?1, ti+1, wi );Figure 3: Pseudocode for polynomial inference for the first-order bidirectional CMM (memoized version).2.2 Inference for Linear Dependency NetworksCyclic or not, we can view the product of local probabil-ities from a dependency network as a score:score(x) =?iP(xi |Pa(xi ))where Pa(xi ) are the nodes with arcs to the node xi .
In thecase of an acyclic model, this score will be the joint prob-ability of the event x , P(x).
In the general case, it will notbe.
However, we can still ask for the event, in this case thetag sequence, with the highest score.
For dependency net-works like those in figure 1, an adaptation of the Viterbialgorithm can be used to find the maximizing sequencein polynomial time.
Figure 3 gives pseudocode for theconcrete case of the network in figure 1(d); the generalcase is similar, and is in fact just a max-plus version ofstandard inference algorithms for Bayes?
nets (Cowell etal., 1999, 97).
In essence, there is no difference betweeninference on this network and a second-order left-to-rightCMM or HMM.
The only difference is that, when theMarkov window is at a position i , rather than receivingthe score for P(ti |ti?1, ti?2, wi ), one receives the scorefor P(ti?1|ti , ti?2, wi?1).There are some foundational issues worth mention-ing.
As discussed previously, the maximum scoring se-quence need not be the sequence with maximum likeli-hood according to the model.
There is therefore a worrywith these models about a kind of ?collusion?
where themodel locks onto conditionally consistent but jointly un-likely sequences.
Consider the two-node network in fig-ure 2(c).
If we have the following distribution of ob-servations (in the form ab) ?11, 11, 11, 12, 21, 33?, thenclearly the most likely state of the network is 11.
How-ever, the score of 11 is P(a = 1|b = 1)P(b = 1|a = 1)= 3/4 ?
3/4 = 9/16, while the score of 33 is 1.
An ad-ditional related problem is that the training set loss (sumof negative logarithms of the sequence scores) does notbound the training set error (0/1 loss on sequences) fromData Set Sect?ns Sent.
Tokens UnknTraining 0?18 38,219 912,344 0Develop 19?21 5,527 131,768 4,467Test 22?24 5,462 129,654 3,649Table 1: Data set splits used.above.
Consider the following training set, for the samenetwork, with each entire data point considered as a label:?11, 22?.
The relative-frequency model assigns loss 0 toboth training examples, but cannot do better than 50%error in regenerating the training data labels.
These is-sues are further discussed in Heckerman et al (2000).Preliminary work of ours suggests that practical use ofdependency networks is not in general immune to thesetheoretical concerns: a dependency network can choose asequence model that is bidirectionally very consistent butdoes not match the data very well.
However, this problemdoes not appear to have prevented the networks from per-forming well on the tagging problem, probably becausefeatures linking tags and observations are generally muchsharper discriminators than tag sequence features.It is useful to contrast this framework with the con-ditional random fields of Lafferty et al (2001).
TheCRF approach uses similar local features, but rather thanchaining together local models, they construct a sin-gle, globally normalized model.
The principal advan-tage of the dependency network approach is that advan-tageous bidirectional effects can be obtained without theextremely expensive global training required for CRFs.To summarize, we draw a dependency network inwhich each node has as neighbors all the other nodesthat we would like to have influence it directly.
Eachnode?s neighborhood is then considered in isolation anda local model is trained to maximize the conditional like-lihood over the training data of that node.
At test time,the sequence with the highest product of local conditionalscores is calculated and returned.
We can always find theexact maximizing sequence, but only in the case of anacyclic net is it guaranteed to be the maximum likelihoodsequence.3 ExperimentsThe part of speech tagged data used in our experiments isthe Wall Street Journal data from Penn Treebank III (Mar-cus et al, 1994).
We extracted tagged sentences from theparse trees.5 We split the data into training, development,and test sets as in (Collins, 2002).
Table 1 lists character-5Note that these tags (and sentences) are not identical tothose obtained from the tagged/pos directories of the same disk:hundreds of tags in the RB/RP/IN set were changed to be moreconsistent in the parsed/mrg version.
Maybe we were the last todiscover this, but we?ve never seen it in print.istics of the three splits.6 Except where indicated for themodel BEST, all results are on the development set.One innovation in our reporting of results is that wepresent whole-sentence accuracy numbers as well as thetraditional per-tag accuracy measure (over all tokens,even unambiguous ones).
This is the quantity that mostsequence models attempt to maximize (and has been mo-tivated over doing per-state optimization as being moreuseful for subsequent linguistic processing: one wants tofind a coherent sentence interpretation).
Further, whilesome tag errors matter much more than others, to a firstcut getting a single tag wrong in many of the more com-mon ways (e.g., proper noun vs. common noun; noun vs.verb) would lead to errors in a subsequent processor suchas an information extraction system or a parser that wouldgreatly degrade results for the entire sentence.
Finally,the fact that the measure has much more dynamic rangehas some appeal when reporting tagging results.The per-state models in this paper are log-linear mod-els, building upon the models in (Ratnaparkhi, 1996) and(Toutanova and Manning, 2000), though some models arein fact strictly simpler.
The features in the models aredefined using templates; there are different templates forrare words aimed at learning the correct tags for unknownwords.7 We present the results of three classes of experi-ments: experiments with directionality, experiments withlexicalization, and experiments with smoothing.3.1 Experiments with DirectionalityIn this section, we report experiments using log-linearCMMs to populate nets with various structures, exploringthe relative value of neighboring words?
tags.
Table 2 liststhe discussed networks.
All networks have the same ver-tical feature templates: ?t0, w0?
features for known wordsand various ?t0, ?
(w1n)?
word signature features for allwords, known or not, including spelling and capitaliza-tion features (see section 3.3).Just this vertical conditioning gives an accuracy of93.69% (denoted as ?Baseline?
in table 2).8 Condition-6Tagger results are only comparable when tested not only onthe same data and tag set, but with the same amount of trainingdata.
Brants (2000) illustrates very clearly how tagging perfor-mance increases as training set size grows, largely because thepercentage of unknown words decreases while system perfor-mance on them increases (they become increasingly restrictedas to word class).7Except where otherwise stated, a count cutoff of 2 was usedfor common word features and 35 for rare word features (tem-plates need a support set strictly greater in size than the cutoffbefore they are included in the model).8Charniak et al (1993) noted that such a simple model got90.25%, but this was with no unknown word model beyonda prior distribution over tags.
Abney et al (1999) raise thisbaseline to 92.34%, and with our sophisticated unknown wordmodel, it gets even higher.
The large number of unambiguoustokens and ones with very skewed distributions make the base-Model Feature Templates?
Features Sentence Token Unkn.
WordAccuracy Accuracy AccuracyBaseline ?
56,805 26.74% 93.69% 82.61%L ?t0, t?1?
27,474 41.89% 95.79% 85.49%R ?t0, t+1?
27,648 36.31% 95.14% 85.65%L+L2 ?t0, t?1?, ?t0, t?2?
32,935 44.04% 96.05% 85.92%R+R2 ?t0, t+1?, ?t0, t+2?
33,423 37.20% 95.25% 84.49%L+R ?t0, t?1?, ?t0, t+1?
32,610 49.50% 96.57% 87.15%LL ?t0, t?1, t?2?
45,532 44.60% 96.10% 86.48%RR ?t0, t+1, t+2?
45,446 38.41% 95.40% 85.58%LR ?t0, t?1, t+1?
45,478 49.30% 96.55% 87.26%L+LL+LLL ?t0, t?1?, ?t0, t?1, t?2?, ?t0, t?1, t?2, t?3?
118,752 45.14% 96.20% 86.52%R+LR+LLR ?t0, t+1?, ?t0, t?1, t+1?, ?t0, t?1, t?2, t+1?
115,790 51.69% 96.77% 87.91%L+LL+LR+RR+R ?t0, t?1?, ?t0, t?1, t?2?, ?t0, t?1, t+1?, ?t0, t+1?, ?t0, t+1, t+2?
81,049 53.23% 96.92% 87.91%Table 2: Tagging accuracy on the development set with different sequence feature templates.
?All models include the same verticalword-tag features (?t0, w0?
and various ?t0, ?
(w1n)?
), though the baseline uses a lower cutoff for these features.Model Feature Templates Support Features Sentence Token UnknownCutoff Accuracy Accuracy AccuracyBASELINE ?t0, w0?
2 6,501 1.63% 60.16% 82.98%?t0, w0?
0 56,805 26.74% 93.69% 82.61%3W ?t0, w0?, ?t0, w?1?, ?t0, w+1?
2 239,767 48.27% 96.57% 86.78%3W+TAGS tag sequences, ?t0, w0?, ?t0, w?1?, ?t0, w+1?
2 263,160 53.83% 97.02% 88.05%BEST see text 2 460,552 55.31% 97.15% 88.61%Table 3: Tagging accuracy with different lexical feature templates on the development set.Model Feature Templates Support Features Sentence Token UnknownCutoff Accuracy Accuracy AccuracyBEST see text 2 460,552 56.34% 97.24% 89.04%Table 4: Final tagging accuracy for the best model on the test set.ing on the previous tag as well (model L, ?t0, t?1?
fea-tures) gives 95.79%.
The reverse, model R, using thenext tag instead, is slightly inferior at 95.14%.
ModelL+R, using both tags simultaneously (but with only theindividual-direction features) gives a much better accu-racy of 96.57%.
Since this model has roughly twice asmany tag-tag features, the fact that it outperforms the uni-directional models is not by itself compelling evidencefor using bidirectional networks.
However, it also out-performs model L+L2 which adds the ?t0, t?2?
second-previous word features instead of next word features,which gives only 96.05% (and R+R2 gives 95.25%).
Weconclude that, if one wishes to condition on two neigh-boring nodes (using two sets of 2-tag features), the sym-metric bidirectional model is superior.High-performance taggers typically also include jointthree-tag counts in some way, either as tag trigrams(Brants, 2000) or tag-triple features (Ratnaparkhi, 1996,Toutanova and Manning, 2000).
Models LL, RR, and CRuse only the vertical features and a single set of tag-triplefeatures: the left tags (t?2, t?1 and t0), right tags (t0, t+1,t+2), or centered tags (t?1, t0, t+1) respectively.
Again,with roughly equivalent feature sets, the left context isbetter than the right, and the centered context is betterthan either unidirectional context.line for this task high, while substantial annotator noise createsan unknown upper bound on the task.3.2 LexicalizationLexicalization has been a key factor in the advance ofstatistical parsing models, but has been less exploitedfor tagging.
Words surrounding the current word havebeen occasionally used in taggers, such as (Ratnaparkhi,1996), Brill?s transformation based tagger (Brill, 1995),and the HMM model of Lee et al (2000), but neverthe-less, the only lexicalization consistently included in tag-ging models is the dependence of the part of speech tagof a word on the word itself.In maximum entropy models, joint features which lookat surrounding words and their tags, as well as joint fea-tures of the current word and surrounding words are inprinciple straightforward additions, but have not been in-corporated into previous models.
We have found thesefeatures to be very useful.
We explore here lexicaliza-tion both alone and in combination with preceding andfollowing tag histories.Table 3 shows the development set accuracy of severalmodels with various lexical features.
All models use thesame rare word features as the models in Table 2.
Thefirst two rows show a baseline model using the currentword only.
The count cutoff for this feature was 0 in thefirst model and 2 for the model in the second row.
Asthere are no tag sequence features in these models, the ac-curacy drops significantly if a higher cutoff is used (froma per tag accuracy of about 93.7% to only 60.2%).The third row shows a model where a tag is de-cided solely by the three words centered at the tag po-sition (3W).
As far as we are aware, models of thissort have not been explored previously, but its accu-racy is surprisingly high: despite having no sequencemodel at all, it is more accurate than a model which usesstandard tag fourgram HMM features (?t0, w0?, ?t0, t?1?,?t0, t?1, t?2?, ?t0, t?1, t?2, t?3?, shown in Table 2, modelL+LL+LLL).The fourth and fifth rows show models with bi-directional tagging features.
The fourth model(3W+TAGS) uses the same tag sequence features asthe last model in Table 2 (?t0, t?1?, ?t0, t?1, t?2?,?t0, t?1, t+1?, ?t0, t+1?, ?t0, t+1, t+2?)
and current, previ-ous, and next word.
The last model has in ad-dition the feature templates ?t0, w0, t?1?, ?t0, w0, t+1?,?t0, w?1, w0?, and ?t0, w0, w+1?, and includes the im-provements in unknown word modeling discussed in sec-tion 3.3.9 We call this model BEST.
BEST has a to-ken accuracy on the final test set of 97.24% and a sen-tence accuracy of 56.34% (see Table 4).
A 95% confi-dence interval for the accuracy (using a binomial model)is (97.15%, 97.33%).In order to understand the gains from using right con-text tags and more lexicalization, let us look at an exam-ple of an error that the enriched models learn not to make.An interesting example of a common tagging error of thesimpler models which could be corrected by a determinis-tic fixup rule of the kind used in the IDIOMTAG moduleof (Marshall, 1987) is the expression as X as (often, asfar as).
This should be tagged as/RB X/{RB,JJ} as/IN inthe Penn Treebank.
A model using only current word andtwo left tags (model L+L2 in Table 2), made 87 errors onthis expression, tagging it as/IN X as/IN ?
since the tagsequence probabilities do not give strong reasons to dis-prefer the most common tagging of as (it is tagged as INover 80% of the time).
However, the model 3W+TAGS,which uses two right tags and the two surrounding wordsin addition, made only 8 errors of this kind, and modelBEST made only 6 errors.3.3 Unknown word featuresMost of the models presented here use a set of un-known word features basically inherited from (Ratna-parkhi, 1996), which include using character n-gram pre-fixes and suffixes (for n up to 4), and detectors for a fewother prominent features of words, such as capitaliza-tion, hyphens, and numbers.
Doing error analysis on un-known words on a simple tagging model (with ?t0, t?1?,?t0, t?1, t?2?, and ?w0, t0?
features) suggested several ad-ditional specialized features that can usefully improve9Thede and Harper (1999) use ?t?1, t0, w0?
templates intheir ?full-second order?
HMM, achieving an accuracy of96.86%.
Here we can add the opposite tiling and other features.Smoothed Features Sentence Token Unk.
W.Accuracy Acc.
Acc.yes 45,532 44.60% 96.10% 86.48%no 45,532 42.81% 95.88% 83.08%yes 292,649 54.88% 97.10% 88.20%no 292,649 48.85% 96.54% 85.20%Table 5: Accuracy with and without quadratic regularization.performance.
By far the most significant is a crude com-pany name detector which marks capitalized words fol-lowed within 3 words by a company name suffix like Co.or Inc.
This suggests that further gains could be made byincorporating a good named entity recognizer as a prepro-cessor to the tagger (reversing the most common order ofprocessing in pipelined systems!
), and is a good exampleof something that can only be done when using a condi-tional model.
Minor gains come from a few additionalfeatures: an allcaps feature, and a conjunction feature ofwords that are capitalized and have a digit and a dash inthem (such words are normally common nouns, such asCFC-12 or F/A-18).
We also found it advantageous touse prefixes and suffixes of length up to 10.
Togetherwith the larger templates, these features contribute to ourunknown word accuracies being higher than those of pre-viously reported taggers.3.4 SmoothingWith so many features in the model, overtraining is a dis-tinct possibility when using pure maximum likelihood es-timation.
We avoid this by using a Gaussian prior (akaquadratic regularization or quadratic penalization) whichresists high feature weights unless they produce greatscore gain.
The regularized objective F is:F(?)
=?ilog(P?
(ti |w, t)) +?nj=1?2j2?
2Since we use a conjugate-gradient procedure to maximizethe data likelihood, the addition of a penalty term is eas-ily incorporated.
Both the total size of the penalty andthe partial derivatives with repsect to each ?
j are triv-ial to compute; these are added to the log-likelihood andlog-likelihood derivatives, and the penalized optimizationprocedes without further modification.We have not extensively experimented with the valueof ?
2 ?
which can even be set differently for different pa-rameters or parameter classes.
All the results in this paperuse a constant ?
2 = 0.5, so that the denominator disap-pears in the above expression.
Experiments on a simplemodel with ?
made an order of magnitude higher or lowerboth resulted in worse performance than with ?
2 = 0.5.Our experiments show that quadratic regularizationis very effective in improving the generalization perfor-mance of tagging models, mostly by increasing the num-ber of features which could usefully be incorporated.
TheTagger Support cutoff AccuracyCollins (2002) 0 96.60%5 96.72%Model 3W+TAGS variant 1 96.97%5 96.93%Table 6: Effect of changing common word feature cutoffs (fea-tures with support ?
cutoff are excluded from the model).number of features used in our complex models ?
in theseveral hundreds of thousands, is extremely high in com-parison with the data set size and the number of featuresused in other machine learning domains.
We describe twosets of experiments aimed at comparing models with andwithout regularization.
One is for a simple model with arelatively small number of features, and the other is for amodel with a large number of features.The usefulness of priors in maximum entropy modelsis not new to this work: Gaussian prior smoothing is ad-vocated in Chen and Rosenfeld (2000), and used in allthe stochastic LFG work (Johnson et al, 1999).
How-ever, until recently, its role and importance have not beenwidely understood.
For example, Zhang and Oles (2001)attribute the perceived limited success of logistic regres-sion for text categorization to a lack of use of regular-ization.
At any rate, regularized conditional loglinearmodels have not previously been applied to the prob-lem of producing a high quality part-of-speech tagger:Ratnaparkhi (1996), Toutanova and Manning (2000), andCollins (2002) all present unregularized models.
Indeed,the result of Collins (2002) that including low supportfeatures helps a voted perceptron model but harms a max-imum entropy model is undone once the weights of themaximum entropy model are regularized.Table 5 shows results on the development set from twopairs of experiments.
The first pair of models use com-mon word templates ?t0, w0?, ?t0, t?1, t?2?
and the samerare word templates as used in the models in table 2.
Thesecond pair of models use the same features as modelBEST with a higher frequency cutoff of 5 for commonword features.For the first pair of models, the error reduction fromsmoothing is 5.3% overall and 20.1% on unknown words.For the second pair of models, the error reduction iseven bigger: 16.2% overall after convergence and 5.8% iflooking at the best accuracy achieved by the unsmoothedmodel (by stopping training after 75 iterations; see be-low).
The especially large reduction in unknown word er-ror reflects the fact that, because penalties are effectivelystronger for rare features than frequent ones, the presenceof penalties increases the degree to which more generalcross-word signature features (which apply to unknownwords) are used, relative to word-specific sparse features(which do not apply to unknown words).Secondly, use of regularization allows us to incorporatefeatures with low support into the model while improving96,396,496,596,696,796,896,99797,197,20100200300400Training IterationsAccuracyNo SmoothingSmoothingFigure 4: Accuracy by training iterations, with and withoutquadratic regularization.performance.
Whereas Ratnaparkhi (1996) used featuresupport cutoffs and early stopping to stop overfitting ofthe model, and Collins (2002) contends that includinglow support features harms a maximum entropy model,our results show that low support features are useful in aregularized maximum entropy model.
Table 6 contrastsour results with those from Collins (2002).
Since themodels are not the same, the exact numbers are incompa-rable, but the difference in direction is important: in theregularized model, performance improves with the inclu-sion of low support features.Finally, in addition to being significantly more accu-rate, smoothed models train much faster than unsmoothedones, and do not benefit from early stopping.
For ex-ample, the first smoothed model in Table 5 required 80conjugate gradient iterations to converge (somewhat ar-bitrarily defined as a maximum difference of 10?4 in fea-ture weights between iterations), while its correspondingunsmoothed model required 335 iterations, thus trainingwas roughly 4 times slower.10 The second pair of modelsrequired 134 and 370 iterations respectively.
As mightbe expected, unsmoothed models reach their highest gen-eralization capacity long before convergence and accu-racy on an unseen test set drops considerably with fur-ther iterations.
This is not the case for smoothed mod-els, as their test set accuracy increases almost monoton-ically with training iterations.11 Figure 4 shows a graphof training iterations versus accuracy for the second pairof models on the development set.4 ConclusionWe have shown how broad feature use, when combinedwith appropriate model regularization, produces a supe-rior level of tagger performance.
While experience sug-10On a 2GHz PC, this is still an important difference: ourlargest models require about 25 minutes per iteration to train.11In practice one notices some wiggling in the curve, butthe trend remains upward even beyond our chosen convergencepoint.gests that the final accuracy number presented here couldbe slightly improved upon by classifier combination, it isworth noting that not only is this tagger better than anyprevious single tagger, but it also appears to outperformBrill and Wu (1998), the best-known combination tagger(they report an accuracy of 97.16% over the same WSJdata, but using a larger training set, which should favorthem).While part-of-speech tagging is now a fairly well-wornroad, and our ability to win performance increases inthis domain is starting to be limited by the rate of er-rors and inconsistencies in the Penn Treebank trainingdata, this work also has broader implications.
Acrossthe many NLP problems which involve sequence mod-els over sparse multinomial distributions, it suggests thatfeature-rich models with extensive lexicalization, bidirec-tional inference, and effective regularization will be keyelements in producing state-of-the-art results.AcknowledgementsThis work was supported in part by the Advanced Re-search and Development Activity (ARDA)?s AdvancedQuestion Answering for Intelligence (AQUAINT) Pro-gram, by the National Science Foundation under GrantNo.
IIS-0085896, and by an IBM Faculty PartnershipAward.ReferencesSteven Abney, Robert E. Schapire, and Yoram Singer.
1999.Boosting applied to tagging and PP attachment.
InEMNLP/VLC 1999, pages 38?45.Thorsten Brants.
2000.
TnT ?
a statistical part-of-speech tagger.In ANLP 6, pages 224?231.Eric Brill and Jun Wu.
1998.
Classifier combination forimproved lexical disambiguation.
In ACL 36/COLING 17,pages 191?195.Eric Brill.
1995.
Transformation-based error-driven learningand natural language processing: A case study in part-of-speech tagging.
Computational Linguistics, 21(4):543?565.Eugene Charniak, Curtis Hendrickson, Neil Jacobson, and MikePerkowitz.
1993.
Equations for part-of-speech tagging.
InAAAI 11, pages 784?789.Stanley F. Chen and Ronald Rosenfeld.
2000.
A survey ofsmoothing techniques for maximum entropy models.
IEEETransactions on Speech and Audio Processing, 8(1):37?50.Kenneth W. Church.
1988.
A stochastic parts program andnoun phrase parser for unrestricted text.
In ANLP 2, pages136?143.Michael Collins.
2002.
Discriminative training methods forHidden Markov Models: Theory and experiments with per-ceptron algorithms.
In EMNLP 2002.Robert G. Cowell, A. Philip Dawid, Steffen L. Lauritzen, andDavid J. Spiegelhalter.
1999.
Probabilistic Networks andExpert Systems.
Springer-Verlag, New York.David Heckerman, David Maxwell Chickering, ChristopherMeek, Robert Rounthwaite, and Carl Myers Kadie.
2000.Dependency networks for inference, collaborative filteringand data visualization.
Journal of Machine Learning Re-search, 1(1):49?75.Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi, andStefan Riezler.
1999.
Estimators for stochastic ?unification-based?
grammars.
In ACL 37, pages 535?541.Dan Klein and Christopher D. Manning.
2002.
Conditionalstructure versus conditional estimation in NLP models.
InEMNLP 2002, pages 9?16.John Lafferty, Andrew McCallum, and Fernando Pereira.
2001.Conditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In ICML-2001, pages282?289.Sang-Zoo Lee, Jun ichi Tsujii, and Hae-Chang Rim.
2000.
Part-of-speech tagging based on Hidden Markov Model assumingjoint independence.
In ACL 38, pages 263?169.Mitchell P. Marcus, Beatrice Santorini, and Mary A. Marcinkie-wicz.
1994.
Building a large annotated corpus of English:The Penn Treebank.
Computational Linguistics, 19:313?330.Ian Marshall.
1987.
Tag selection using probabilistic methods.In Roger Garside, Geoffrey Sampson, and Geoffrey Leech,editors, The Computational analysis of English: a corpus-based approach, pages 42?65.
Longman, London.Adwait Ratnaparkhi.
1996.
A maximum entropy model forpart-of-speech tagging.
In EMNLP 1, pages 133?142.Scott M. Thede and Mary P. Harper.
1999.
Second-order hiddenMarkov model for part-of-speech tagging.
In ACL 37, pages175?182.Kristina Toutanova and Christopher Manning.
2000.
Enrichingthe knowledge sources used in a maximum entropy part-of-speech tagger.
In EMNLP/VLC 1999, pages 63?71.Tong Zhang and Frank J. Oles.
2001.
Text categorization basedon regularized linear classification methods.
Information Re-trieval, 4:5?31.
