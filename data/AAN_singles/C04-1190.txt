Semi-Supervised Training of a Kernel PCA-Based Modelfor Word Sense DisambiguationWeifeng SU Marine CARPUAT Dekai WU1weifeng@cs.ust.hk marine@cs.ust.hk dekai@cs.ust.hkHuman Language Technology CenterHKUSTDepartment of Computer ScienceUniversity of Science and Technology, Clear Water Bay, Hong KongAbstractIn this paper, we introduce a new semi-supervised learningmodel for word sense disambiguation based on Kernel Prin-cipal Component Analysis (KPCA), with experiments showingthat it can further improve accuracy over supervised KPCAmodels that have achieved WSD accuracy superior to the bestpublished individual models.
Although empirical results withsupervised KPCA models demonstrate significantly better ac-curacy compared to the state-of-the-art achieved by either na?
?veBayes or maximum entropy models on Senseval-2 data, weidentify specific sparse data conditions under which supervisedKPCA models deteriorate to essentially a most-frequent-sensepredictor.
We discuss the potential of KPCA for leveragingunannotated data for partially-unsupervised training to addressthese issues, leading to a composite model that combines boththe supervised and semi-supervised models.1 IntroductionWu et al (2004) propose an efficient and accurate newsupervised learning model for word sense disambigua-tion (WSD), that exploits a nonlinear Kernel PrincipalComponent Analysis (KPCA) technique to make pre-dictions implicitly based on generalizations over featurecombinations.
Experiments performed on the Senseval-2 English lexical sample data show that KPCA-basedword sense disambiguation method is capable of outper-forming other widely used WSD models including na?
?veBayes, maximum entropy, and SVM models.Despite the excellent performance of the supervisedKPCA-based WSD model on average, though, our fur-ther error analysis investigations have suggested certainlimitations.
In particular, the supervised KPCA-basedmodel often appears to perform poorly when it encoun-ters target words whose contexts are highly dissimilarto those of any previously seen instances in the train-ing set.
Empirically, the supervised KPCA-based modelnearly always disambiguates target words of this kindto the most frequent sense.
As a result, for this partic-ular subset of test instances, the precision achieved bythe KPCA-based model is essentially no higher than theprecision achieved by the most-frequent-sense baselinemodel (which simply always selects the most frequentsense for the target word).
The work reported in this pa-per stems from a hypothesis that the most-frequent-sense1The author would like to thank the Hong Kong Research GrantsCouncil (RGC) for supporting this research in part through grantsRGC6083/99E, RGC6256/00E, and DAG03/04.EG09.strategy can be bettered for this category of errors.This is a case of data sparseness, so the observationshould not be very surprising.
Such behavior is to be ex-pected from classifiers in general, and not just the KPCA-based model.
Put another way, even though KPCA isable to generalize over combinations of dependent fea-tures, there must be a sufficient number of training in-stances from which to generalize.The nature of KPCA, however, suggests a strategy thatis not applicable to many of the other conventional WSDmodels.
We propose a model in this paper that takes ad-vantage of unsupervised training using large quantities ofunannotated corpora, to help compensate for sparse data.Note that although we are using the WSD task to ex-plain the model, in fact the proposed model is not lim-ited to WSD applications.
We have hypothesized thatthe KPCA-based method is likely to be widely applica-ble to other NLP tasks; since data sparseness is a com-mon problem in many NLP tasks, a weakly-supervisedapproach allowing the KPCA-based method to compen-sate for data sparseness is highly desirable.
The generaltechnique we describe here is applicable to any similarclassification task where insufficient labeled training datais available.The paper is organized as follows.
After a brief lookat related work, we review the baseline supervised WSDmodel, which is based on Kernel PCA.
We then discusshow data sparseness affects the model, and propose anew semi-supervised model that takes advantage of un-labeled data, along with a composite model that com-bines both the supervised and semi-supervised models.Finally, details of the experimental setup and compara-tive results are given.2 Related workThe long history of WSD research includes numerousstatistically trained methods; space only permits us tosummarize a few key points here.
Na?
?ve Bayes models(e.g., Mooney (1996), Chodorow et al (1999), Pedersen(2001), Yarowsky and Florian (2002)) as well as max-imum entropy models (e.g., Dang and Palmer (2002),Klein and Manning (2002)) in particular have shown alarge degree of success for WSD, and have establishedchallenging state-of-the-art benchmarks.
The Sensevalseries of evaluations facilitates comparing the strengthsand weaknesses of various WSD models on commondata sets, with Senseval-1 (Kilgarriff and Rosenzweig,1999), Senseval-2 (Kilgarriff, 2001), and Senseval-3 heldin 1998, 2001, and 2004 respectively.3 Supervised KPCA baseline modelOur baseline WSD model is a supervised learning modelthat also makes use of Kernel Principal ComponentAnalysis (KPCA), proposed by (Scho?lkopf et al, 1998)as a generalization of PCA.
KPCA has been successfullyapplied in many areas such as de-noising of images ofhand-written digits (Mika et al, 1999) and modeling thedistribution of non-linear data sets in the context of shapemodelling for real objects (Active Shape Models) (Twin-ing and Taylor, 2001).
In this section, we first review thetheory of KPCA and explanation of why it is suited forWSD applications.3.1 Kernel Principal Component AnalysisThe Kernel Principal Component Analysis technique, orKPCA, is a method of nonlinear principal component ex-traction.
A nonlinear function maps the n-dimensionalinput vectors from their original space Rn to a high-dimensional feature space F where linear PCA is per-formed.
In real applications, the nonlinear function isusually not explicitly provided.
Instead we use a kernelfunction to implicitly define the nonlinear mapping; inthis respect KPCA is similar to Support Vector Machines(Scho?lkopf et al, 1998).Compared with other common analysis techniques,KPCA has several advantages:?
As with other kernel methods it inherently takescombinations of predictive features into accountwhen optimizing dimensionality reduction.
For nat-ural language problems in general, of course, it iswidely recognized that significant accuracy gainscan often be achieved by generalizing over relevantfeature combinations (e.g., Kudo and Matsumoto(2003)).?
We can select suitable kernel function according tothe task we are dealing with and the knowledge wehave about the task.?
Another advantage of KPCA is that it is good atdealing with input data with very high dimension-ality, a condition where kernel methods excel.Nonlinear principal components (Diamantaras andKung, 1996) may be defined as follows.
Suppose weare given a training set of M pairs (xt, ct) where theobserved vectors xt ?
Rn in an n-dimensional inputspace X represent the context of the target word beingdisambiguated, and the correct class ct represents thesense of the word, for t = 1, ..,M .
Suppose ?
is anonlinear mapping from the input space Rn to the fea-ture space F .
Without loss of generality we assume theM vectors are centered vectors in the feature space, i.e.,?Mt=1 ?
(xt) = 0; uncentered vectors can easily be con-verted to centered vectors (Scho?lkopf et al, 1998).
Wewish to diagonalize the covariance matrix in F :C =1MM?j=1?
(xj) ?T (xj) (1)To do this requires solving the equation ?v = Cv foreigenvalues ?
?
0 and eigenvectors v ?
F .
BecauseCv =1MM?j=1(?
(xj) ?
v)?
(xj) (2)we can derive the following two useful results.
First,?
(?
(xt) ?
v) = ?
(xt) ?
Cv (3)for t = 1, ..,M .
Second, there exist ?i for i = 1, ...,Msuch thatv =M?i=1?i?
(xi) (4)Combining (1), (3), and (4), we obtainM?M?i=1?i (?
(xt) ?
?
(xi ))=M?i=1?i(?
(xt) ?M?j=1?
(xj)) (?
(xj) ?
?
(xi ))for t = 1, ..,M .
Let K?
be the M ?M matrix such thatK?ij = ?
(xi) ?
?
(xj) (5)and let ?
?1 ?
?
?2 ?
.
.
.
?
?
?M denote the eigenvaluesof K?
and ?
?1 ,..., ?
?M denote the corresponding completeset of normalized eigenvectors, such that ??t(?
?t ?
?
?t) = 1when ?
?t > 0.
Then the lth nonlinear principal compo-nent of any test vector xt is defined asylt =M?i=1?
?li (?
(xi) ?
?
(xt )) (6)where ?
?li is the lth element of ?
?l .3.2 Why is KPCA suited to WSD?The potential of nonlinear principal components forWSD can be illustrated by a simplified disambiguationexample for the ambiguous target word ?art?, with thetwo senses shown in Table 1.
Assume a training cor-pus of the eight sentences as shown in Table 2, adaptedfrom Senseval-2 English lexical sample corpus.
For eachsentence, we show the feature set associated with thatoccurrence of ?art?
and the correct sense class.
Theseeight occurrences of ?art?
can be transformed to a binaryvector representation containing one dimension for eachfeature, as shown in Table 3.Extracting nonlinear principal components for the vec-tors in this simple corpus results in nonlinear generaliza-tion, reflecting an implicit consideration of combinationsof features.
Table 2 shows the first three dimensions ofthe principal component vectors obtained by transform-ing each of the eight training vectors xt into (a) principalcomponent vectors zt using the linear transform obtainedvia PCA, and (b) nonlinear principal component vectorsyt using the nonlinear transform obtained via KPCA asdescribed below.Table 1: A tiny corpus for the target word ?art?, adapted from the Senseval-2 English lexical sample corpus (Kilgarriff2001), together with a tiny example set of features.
The training and testing examples can be represented as a set ofbinary vectors: each row shows the correct class c for an observed vector x of five dimensions.TRAINING design/N media/N the/DT entertainment/N world/N Classx1 He studies art in London.
1x2 Punch?s weekly guide to theworld of the arts, entertain-ment, media and more.1 1 1 1x3 All such studies have influ-enced every form of art, de-sign, and entertainment insome way.1 1 1x4 Among the technical arts cul-tivated in some continentalschools that began to affectEngland soon after the Nor-man Conquest were thoseof measurement and calcula-tion.1 2x5 The Art of Love.
1 2x6 Indeed, the art of doctor-ing does contribute to bet-ter health results and discour-ages unwarranted malprac-tice litigation.1 2x7 Countless books and classesteach the art of assertingoneself.1 2x8 Pop art is an example.
1TESTINGx9 In the world of de-sign arts particularly, this ledto appointments made forpolitical rather than academicreasons.1 1 1 1Table 2: The original observed training vectors (showing only the first three dimensions) and their first three principalcomponents as transformed via PCA and KPCA.Observed vectors PCA-transformed vectors KPCA-transformed vectors Classt (x1t , x2t , x3t ) (z1t , z2t , z3t ) (y1t , y2t , y3t ) ct1 (0, 0, 0) (-1.961, 0.2829, 0.2014) (0.2801, -1.005, -0.06861) 12 (0, 1, 1) (1.675, -1.132, 0.1049) (1.149, 0.02934, 0.322) 13 (1, 0, 0) (-0.367, 1.697, -0.2391) (0.8209, 0.7722, -0.2015) 14 (0, 0, 1) (-1.675, -1.132, -0.1049) (-1.774, -0.1216, 0.03258) 25 (0, 0, 1) (-1.675, -1.132, -0.1049) (-1.774, -0.1216, 0.03258) 26 (0, 0, 1) (-1.675, -1.132, -0.1049) (-1.774, -0.1216, 0.03258) 27 (0, 0, 1) (-1.675, -1.132, -0.1049) (-1.774, -0.1216, 0.03258) 28 (0, 0, 0) (-1.961, 0.2829, 0.2014) (0.2801, -1.005, -0.06861) 1Similarly, for the test vector x9, Table 3 shows thefirst three dimensions of the principal component vec-tors obtained by transforming it into (a) a principal com-ponent vector z9 using the linear PCA transform ob-tained from training, and (b) a nonlinear principal com-ponent vector y9 using the nonlinear KPCA transformobtained obtained from training.
The vector similaritiesin the KPCA-transformed space can be quite differentfrom those in the PCA-transformed space.
This causesthe KPCA-based model to be able to make the correctTable 3: Testing vector (showing only the first three dimensions) and its first three principal components as transformedvia the trained PCA and KPCA parameters.
The PCA-based and KPCA-based sense class predictions disagree.ObservedvectorsPCA-transformed vectors KPCA-transformed vectors PredictedClassCorrectClasst (x1t , x2t , x3t ) (z1t , z2t , z3t ) (y1t , y2t , y3t ) c?t ct9 (1, 0, 1) (-0.3671, -0.5658, -0.2392) 2 19 (1, 0, 1) (4e-06, 8e-07, 1.111e-18) 1 1class prediction, whereas the PCA-based model makesthe wrong class prediction.What permits KPCA to apply stronger generalizationbiases is its implicit consideration of combinations offeature information in the data distribution from the high-dimensional training vectors.
In this simplified illustra-tive example, there are just five input dimensions; theeffect is stronger in more realistic high dimensional vec-tor spaces.
Since the KPCA transform is computed fromunsupervised training vector data, and extracts general-izations that are subsequently utilized during supervisedclassification, it is possible to combine large amounts ofunsupervised data with reasonable smaller amounts ofsupervised data.Interpreting this example graphically can be illuminat-ing even though the interpretation in three dimensions isseverely limiting.
Figure 1(a) depicts the eight originalobserved training vectors xt in the first three of the fivedimensions; note that among these eight vectors, therehappen to be only four unique points when restrictingour view to these three dimensions.
Ordinary linear PCAcan be straightforwardly seen as projecting the originalpoints onto the principal axis, as can be seen for the caseof the first principal axis in Figure 1(b).
Note that in thisspace, the sense 2 instances are surrounded by sense 1instances.
We can traverse each of the projections ontothe principal axis in linear order, simply by visiting eachof the first principal components z1t along the principleaxis in order of their values, i.e., such thatz11 ?
z18 ?
z14 ?
z15 ?
z16 ?
z17 ?
z12 ?
z13 ?
z19It is significantly more difficult to visualize the non-linear principal components case, however.
Note thatin general, there may not exist any principal axis in X ,since an inverse mapping from F may not exist.
If weattempt to follow the same procedure to traverse each ofthe projections onto the first principal axis as in the caseof linear PCA, by considering each of the first principalcomponents y1t in order of their value, i.e., such thaty14 ?
y15 ?
y16 ?
y17 ?
y19 ?
y11 ?
y18 ?
y13 ?
y12then we must arbitrarily select a ?quasi-projection?
di-rection for each y1t since there is no actual principal axistoward which to project.
This results in a ?quasi-axis?roughly as shown in Figure 1(c) which, though not pre-cisely accurate, provides some idea as to how the non-linear generalization capability allows the data points tobe grouped by principal components reflecting nonlin-ear patterns in the data distribution, in ways that linearFigure 1: Original vectors, PCA projections, and KPCA?quasi-projections?
(see text).PCA cannot do.
Note that in this space, the sense 1 in-stances are already better separated from sense 2 datapoints.
Moreover, unlike linear PCA, there may be upto M of the ?quasi-axes?, which may number far morethan five.
Such effects can become pronounced in thehigh dimensional spaces are actually used for real wordsense disambiguation tasks.3.3 AlgorithmTo extract nonlinear principal components efficiently,note that in both Equations (5) and (6) the explicit formof ?
(xi) is required only in the form of (?
(xi) ??
(xj)),i.e., the dot product of vectors in F .
This means that wecan calculate the nonlinear principal components by sub-stituting a kernel function k(xi, xj) for (?
(xi) ??
(xj ))in Equations (5) and (6) without knowing the mapping ?explicitly; instead, the mapping ?
is implicitly definedby the kernel function.
It is always possible to constructa mapping into a space where k acts as a dot productso long as k is a continuous kernel of a positive integraloperator (Scho?lkopf et al, 1998).Thus we train the KPCA model using the followingalgorithm:1.
Compute an M ?M matrix K?
such thatK?ij = k(xi, xj) (7)2.
Compute the eigenvalues and eigenvectors of matrixK?
and normalize the eigenvectors.
Let ?
?1 ?
?
?2 ?.
.
.
?
?
?M denote the eigenvalues and ?
?1,..., ?
?M de-note the corresponding complete set of normalizedeigenvectors.To obtain the sense predictions for test instances, weneed only transform the corresponding vectors using thetrained KPCA model and classify the resultant vectorsusing nearest neighbors.
For a given test instance vectorx, its lth nonlinear principal component isylt =M?i=1?
?lik(xi, xt) (8)where ?
?li is the ith element of ?
?l.For our disambiguation experiments we employ apolynomial kernel function of the form k(xi, xj) =(xi ?
xj)d, although other kernel functions such as gaus-sians could be used as well.
Note that the degeneratecase of d = 1 yields the dot product kernel k(xi, xj) =(xi?xj) which covers linear PCA as a special case, whichmay explain why KPCA always outperforms PCA.4 Semi-supervised KPCA model4.1 Utilizing unlabeled dataIn WSD, as with many NLP tasks, features are often in-terdependent.
For example, the features that representwords that frequently co-occur are typically highly in-terdependent.
Similarly, the features that represent syn-onyms tend to be highly interdependent.It is a strength of the KPCA-based model that it gen-eralizes over combinations of interdependent features.This enables the model to predict the correct sense evenwhen the context surrounding a target word has not beenpreviously seen, by exploiting the similarity to featurecombinations that have been seen.However, in practice the labeled training corpus forWSD is typically relatively small, and does not yieldenough training instances to reliably extract dependen-cies between features.
For example, in the Senseval-2 English lexical sample data, for each target wordthere are only about 120 training instances on average,whereas on the other hand we typically have thousandsof features for each target word.The KPCA model can fail when it encounters a targetword whose context contains a combination of featuresthat may in fact be interdependent, but are not similar toany combinations that occurred in the limited amountsof labeled training data.
Because of the sparse data, theKPCA model wrongly considers the context of the tar-get word to be dissimilar to those previously seen?eventhough the contexts may in truth be similar.
In the ab-sence of any contexts it believes to be similar, the modeltherefore tends simply to predict the most frequent sense.The potential solution we propose to this problem isto add much larger quantities of unannotated data, withwhich the KPCA model can first be trained in unsu-pervised fashion.
This provides a significantly broaderdataset from which to generalize over combinations ofdependent features.
One of the advantages of our WSDmodel is that during KPCA training, the sense class is nottaken into consideration.
Thus we can take advantage ofthe vast amounts of cheap unannotated corpora, in addi-tion to the relatively small amounts of labeled trainingdata.
Adding a large quantity of unlabeled data makesit much likelier that dependent features can be identifiedduring KPCA training.4.2 AlgorithmThe primary difference of the semi-supervised KPCAmodel from the supervised KPCA baseline model de-scribed above lies in the eigenvector calculation step.
Aswe mentioned earlier, in KPCA-based model, we needto calculate the eigenvectors of matrix K, where Kij =(?
(xi) ?
?
(xj )).
In the supervised KPCA model, train-ing vectors such as xi and xj are only drawn from thelabeled training corpus.
In the semi-supervised KPCAmodel, training vectors are drawn from both the labeledtraining corpus and a much larger unlabeled training cor-pus.
As a consequence, the maximum number of eigen-vectors in the supervised KPCA model is the minimumof the number of features and the number of vectors fromthe labeled training corpus, while the maximum numberof eigenvectors for the semi-supervised KPCA model isthe minimum of the number of features and total num-ber of vectors from the combined labeled and unlabeledtraining corpora.However, one would not want to apply the semi-supervised KPCA model indiscriminately.
While it canbe expected to be valuable in cases where the data wastoo sparse for reliable training of the supervised KPCAmodel, at the same time it is important to note that the un-labeled data is typically drawn from quite different dis-tributions than the labeled data, and may therefore be ex-pected to introduce a new source of noise.We therefore define a composite semi-supervisedKPCA model based on the following assumption.
If weare sufficiently confident about the prediction made bythe supervised KPCA model as to the predicted sensefor the target word, we need not resort to the semi-supervised KPCA method.
On the other hand, if weare not confident about the supervised KPCA model?sprediction, we then turn to the semi-supervised KPCAmodel and take its classification as the predicted sense.Specifically, the composite model uses the followingalgorithm to combine the sense predictions of the super-vised and semi-supervised KPCA models in order to dis-ambiguate the target word in a given test instance x:1. let s1 be the predicted sense of x using the super-vised KPCA baseline model2.
let c be the similarity between x and its most similartraining instance3.
if c ?
t or s1 6= smf (where t is a preset thresh-old, and smf is the most frequent sense of the targetword):?
then predict the sense of the target word of xto be s1?
else predict the sense of the target word ofx to be s2, the sense predicted by the semi-supervised KPCA modelThe two conditions checked in step 3 serve to fil-ter those instances where the supervised KPCA baselinemodel is confident enough to skip the semi-supervisedKPCA model.
In particular:?
The threshold t specifies a minimum level of thesupervised KPCA baseline model?s confidence, interms of similarity.
If c ?
t, then there were traininginstances that were of sufficient similarity to the testinstance so that the model can be confident that acorrect disambiguation can be predicted based onlyon those similar training instances.
In this case thesemi-supervised KPCA model is not needed.?
If s1 is not the most frequent sense smf of thetarget word, then there is strong evidence that thetest instance should be disambiguated as s1 becausethis is overriding an otherwise strong tendency todisambiguate the target word to the most frequentsense.
Again, in this case the semi-supervisedKPCA model should be avoided.The threshold t is defined to rise as the relative fre-quency of the most frequent sense falls.
Specifically,t = 1?
P (smf) + c where P (smf) is the probability ofmost frequent sense in the training corpus and c is a smallconstant.
This reflects the assumption that the higher theprobability of the most frequent sense, the less likely thata test instance disambiguated as the most frequent senseis wrong.5 Experimental setupWe evaluated the composite semi-supervised KPCAmodel using data from the Senseval-2 English lexicalsample task (Kilgarriff, 2001)(Palmer et al, 2001).
Wechose to focus on verbs, which have proven particularlydifficult to disambiguate.
Our task consists in disam-biguating several instances of 16 different target verbs.Table 4: The semi-supervised KPCA model outperformssupervised na?
?ve Bayes and maximum entropy models,as well as the most-frequent-sense and supervised KPCAbaseline models.Fine-grainedaccuracyCoarse-grainedaccuracyMost frequentsense41.4% 51.7%Na?
?ve Bayes 55.4% 64.2%Maximum entropy 54.9% 64.1%Supervised KPCA 57.0% 66.6%Composite semi-supervised KPCA57.4% 67.2%For each target word, training and test instances manu-ally tagged with WordNet senses are available.
There arean average of about 10.5 senses per target word, rang-ing from 4 to 19.
All our models are evaluated on theSenseval-2 test data, but trained on different training sets.We report accuracy, the number of correct predictionsover the total number of test instances, at two differentlevels of sense granularity.The supervised models are trained on the Senseval-2 training data.
On average, 137 annotated training in-stances per target word are available.In addition to the small annotated Senseval-2 dataset, the semi-supervised KPCA model can make use oflarge amounts of unannotated data.
Since most of theSenseval-2 verb data comes from the Wall Street Journal,we choose to augment the Senseval-2 data by collectingadditional training instances from the Wall Street Jour-nal Tipster corpus.
In order to minimize the noise duringKPCA learning, we only extract the sentences in whichthe target word occurs.
For each target word, up to 1500additional training instances were extracted.
The result-ing training corpus for the semi-supervised KPCA modelis more than 10 times larger than the Senseval-2 trainingset, with an average of 1637 training instances per targetword.The set of features used is as described by Yarowskyand Florian (2002) in their ?feature-enhanced na?
?veBayes model?, with position-sensitive, syntactic, and lo-cal collocational features.6 ResultsTable 4 shows that the composite semi-supervised KPCAmodel improves on the high-performance supervisedKPCA model, for both coarse-grained and fined-grainedsense distinctions.
The supervised KPCA model signif-icantly outperforms a na?
?ve Bayes model, and a max-imum entropy model, which are among the top per-forming models for WSD.
Note that these results areconsistent with the larger study of supervised modelsconducted by Wu et al (2004).
The composite semi-supervised KPCA model outperforms all of the three su-pervised models, and in particular, it further improves theTable 5: Semi-supervised KPCA is not necessary whensupervised KPCA is very confident.Fine-grainedaccuracyCoarse-grainedaccuracySupervised KPCA 62.1% 71.3%Semi-supervisedKPCA57.1% 67.1%Table 6: Semi-supervised KPCA outperforms supervisedKPCA when supervised KPCA is not confident: addingtraining data helps when there are no similar instances inthe training set.Fine-grainedaccuracyCoarse-grainedaccuracySupervised KPCA 30.8% 44.11%Semi-supervisedKPCA38.3% 51.47%accuracy of the supervised KPCA model.Overall, with the addition of the semi-supervisedmodel, the accuracy for disambiguating the verbs in-creases from 57% to 57.4% on the fine-grained task, andfrom 66.6% to 67.2% on the coarse-grained task.In our composite model, the supervised KPCA modelpredicts senses with high confidence for more than 94%of the test instances.
The predictions of the semi-supervised model are used for the remaining 6% of thetest instances.
Table 5 shows that it is not necessary touse the semi-supervised training model for all the train-ing instances.
In fact, when the supervised model is con-fident, its predictions are significantly more accurate thanthose of the semi-supervised model alone.When the predictions of the supervised KPCA modelare not accurate, the semi-supervised KPCA model out-performs the supervised model.
This happens when (1)there is no training instance that is very similar to the testinstance considered and when (2) in the absence of rele-vant features to learn from in the small annotated train-ing set, the supervised KPCA model can only predict themost frequent sense for the current target.
In these condi-tions, our experiment results in Table 6 confirm that thesemi-supervised KPCA model benefits from the large ad-ditional training data, suggesting it is able to learn usefulfeature conjunctions, which help to give better predic-tions.The composite semi-supervised KPCA model there-fore chooses the best model depending on the degreeof confidence of the supervised model.
All the KPCAweights, for both the supervised and the semi-supervisedmodel, have been pre-computed during training, and itis therefore inexpensive to switch from one model to theother at testing time.7 ConclusionWe have proposed a new composite semi-supervisedWSD model based on the Kernel PCA technique, thatemploys both supervised and semi-supervised compo-nents.
This strategy allows us to combine large amountsof cheap unlabeled data with smaller amounts of labeleddata.
Experiments on the hard-to-disambiguate verbsfrom the Senseval-2 English lexical sample task confirmthat when the supervised KPCA model is insufficientlyconfident in its sense predictions, taking advantage of thesemi-supervised KPCA model trained with the unlabeleddata can help to give a better prediction.
The compositesemi-supervised KPCA model exploits this to improveupon the accuracy of the supervised KPCA model intro-duced by Wu et al (2004).ReferencesMartin Chodorow, Claudia Leacock, and George A. Miller.
A topical/local clas-sifier for word sense identification.
Computers and the Humanities, 34(1-2):115?120, 1999.
Special issue on SENSEVAL.Hoa Trang Dang and Martha Palmer.
Combining contextual features for wordsense disambiguation.
In Proceedings of the SIGLEX/SENSEVAL Workshopon Word Sense Disambiguation: Recent Successes and Future Directions,pages 88?94, Philadelphia, July 2002.
SIGLEX, Association for Computa-tional Linguistics.Konstantinos I. Diamantaras and Sun Yuan Kung.
Principal Component NeuralNetworks.
Wiley, New York, 1996.Adam Kilgarriff and Joseph Rosenzweig.
Framework and results for EnglishSenseval.
Computers and the Humanities, 34(1):15?48, 1999.
Special issueon SENSEVAL.Adam Kilgarriff.
English lexical sample task description.
In Proceedings ofSenseval-2, Second International Workshop on Evaluating Word Sense Dis-ambiguation Systems, pages 17?20, Toulouse, France, July 2001.
SIGLEX,Association for Computational Linguistics.Dan Klein and Christopher D. Manning.
Conditional structure versus conditionalestimation in NLP models.
In Proceedings of EMNLP-2002, Conference onEmpirical Methods in Natural Language Processing, pages 9?16, Philadel-phia, July 2002.
SIGDAT, Association for Computational Linguistics.Taku Kudo and Yuji Matsumoto.
Fast methods for kernel-based text analysis.In Proceedings of the 41set Annual Meeting of the Asoociation for Computa-tional Linguistics, pages 24?31, 2003.S.
Mika, B. Scho?lkopf, A. Smola, K.-R. Mu?ller, M. Scholz, and G. Ra?tsch.
Ker-nel PCA and de-noising in feature spaces.
Advances in Neural InformationProcessing Systems, 1999.Raymond J. Mooney.
Comparative experiments on disambiguating word senses:An illustration of the role of bias in machine learning.
In Proceedings of theConference on Empirical Methods in Natural Language Processing, Philadel-phia, May 1996.
SIGDAT, Association for Computational Linguistics.Martha Palmer, Christiane Fellbaum, Scott Cotton, Lauren Delfs, and Hoa TrangDang.
English tasks: All-words and verb lexical sample.
In Proceedings ofSenseval-2, Second International Workshop on Evaluating Word Sense Dis-ambiguation Systems, pages 21?24, Toulouse, France, July 2001.
SIGLEX,Association for Computational Linguistics.Ted Pedersen.
Machine learning with lexical features: The Duluth approach toSENSEVAL-2.
In Proceedings of Senseval-2, Second International Work-shop on Evaluating Word Sense Disambiguation Systems, pages 139?142,Toulouse, France, July 2001.
SIGLEX, Association for Computational Lin-guistics.Bernhard Scho?lkopf, Alexander Smola, and Klaus-Rober Mu?ller.
Nonlinear com-ponent analysis as a kernel eigenvalue problem.
Neural Computation, 10(5),1998.C.
J. Twining and C. J. Taylor.
Kernel principal component analysis and the con-struction of non-linear active shape models.
In Proceedings of BMVC20001,2001.Dekai Wu, Weifeng Su, and Marine Carpuat.
A Kernel PCA method for superiorword sense disambiguation.
In Proceedings of the 42nd Annual Meeting ofthe Association for Computational Linguistics, Barcelona, July 2004.David Yarowsky and Radu Florian.
Evaluating sense disambiguation across di-verse parameter spaces.
Natural Language Engineering, 8(4):293?310, 2002.
