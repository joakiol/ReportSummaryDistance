Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 743?747,Dublin, Ireland, August 23-24, 2014.UNAL-NLP: Cross-Lingual Phrase Sense Disambiguation withSyntactic Dependency TreesEmilio Silva-SchlenkerDepartamento de Ling?
?sticaUniversidad Nacional de ColombiaDepartamento de Ingenier?a de SistemasUniversidad de los Andes,Bogot?
D.C., Colombiaesilvas@unal.edu.coSergio Jimenez and Julia BaqueroUniversidad Nacional de Colombia,Bogot?
D.C., Colombiasgjimenezv@unal.edu.cojmbauqerov@unal.edu.coAbstractIn this paper we describe our participa-tion in the SemEval 2014, Task 5, con-sisting of the construction of a translationassistance system that translates L1 frag-ments, written in L2 context, to their cor-rect L2 translation.
Our approach con-sists of a bilingual parallel corpus, a sys-tem of syntactic features extraction and astatistical memory-based classification al-gorithm.
Our system ranked 4th and 6thamong the 10 participating systems thatused the English-Spanish data set.1 IntroductionAn L2 writing assistant is a tool intended for lan-guage learners who need to improve their writingskills.
This tool lets them write a text in L2, but fallback to their native L1 whenever they are not sureabout a certain word or expression.
In these cases,the assistant automatically translates this text forthem (van Gompel et al., 2014).Although at first glance this may be seen asa classification problem, it might be better ful-filled by a cross-lingual word sense disambigua-tion (WSD) approach, which takes context intoaccount by means of contextual features used ina machine learning setting.
The main differencesbetween this and previous approaches to cross-lingual WSD are the bilingual nature of the inputsentences (see section 2.3) and the annotation oftarget phrases, rather than single words.The remainder of this article is organized as fol-lows.
Section 2 describes the proposed method.A description of the system we submitted, the ob-tained results and an error analysis are discussedThis work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/in section 3.
In section 4 we present a brief dis-cussion about the results.
Finally, in section 5 wemake some concluding remarks.2 Method DescriptionThe core of the proposed system uses techniquesfrom memory-based classification to find the mostappropriate translation of a target phrase in agiven context.
It receives an input as in (1) andyields an output as in (2).
(1) No creo que ella is coming.
(2) No creo que ella venga.It does so on the basis of a syntactic selec-tion of context features, a large bilingual parallelcorpus and a classifier built using the TilburgMemory-Based Learner, TiMBL (Daelemans etal., 2010).The proposed system consists of several stages.First, a large bilingual corpus is aligned at wordand phrase level.
Next, an index is built by eachphrase in the L1 side of the corpus to retrieve ef-ficiently the occurrences of a particular L1 phrasein the aligned corpus along with their translationsand contexts in L2 (subsection 2.1).
Second, therelevant contexts for each L1 phrase in the test set(example sentences) are retrieved from the corpusand a set of syntactic features are extracted fromeach sentence (subsection 2.2).
Third, a specialtwo-stage process is used to extract the same fea-tures from the sentences in the test set to deal withthe fact that these sentences were written in twolanguages (subsection 2.3).
Finally, each targetphrase is translated using the IBL algorithm andthe translations were incorporated in the originaltest sentences (subsection 2.4).743Input sentence Parallel example sentencesNo creo que las necesidadesafectivas de las personas est?nnecesariamente linked almatrimonio.He said Boyd already linkedhim to Brendan.Dijo que Boyd ya le hab?a rela-cionado con Brendan.The three things are inextrica-bly linked, and I have the for-mula right here.Las tres cosas est?n es-trechamente vinculadas, ytengo la f?rmula aqu?.Table 1: An input sentence and 2 example sentences from Linguee.com.2.1 Parallel Corpus Selection andPreparationAs no training corpus was given prior to develop-ing this system, finding and processing the mostsuitable corpus for this task was paramount.
Asthe purpose of this system is to help language stu-dents, the corpus needs to account for simple yetcorrect everyday speech.In an initial stage of development we optedto use the 70-million sentences OpenSubtitles.orgcorpus compiled by the Opus Project (Tiedemann,2012), which includes many informal everyday ut-terances, at the expense of a less accurate transla-tion quality1.
Although the use of this training cor-pus yielded over 95% of recall on the trial corpusgiven by the task organizers, only 80% of the trialsentences had enough (>100) training examples inorder to produce a quality translation.
To solve thisissue, an ad-hoc corpus compilation mechanismwas created by using the Linguee.com.
Thus, aset of parallel example sentences is retrieved fromLinguee.com by querying all the L1 target phrasesfrom the evaluation data (see an example in Table1).The corpus preparation procedure consisted ofseveral steps.
The first step was to clean the cor-pus with the Moses cleaning script (Koehn et al.,2007).
Next, the corpus was tokenized and PoS-tagged using FreeLing (Padr?
and Stanilovsky,2012) (HMM tagger was used).
After that, thecorpus was word-aligned using Giza++ (Och andNey, 2003) over Moses (Koehn et al., 2007).
Theresulting alignment was then combined with thetagged version of the corpus.
Finally, a phrase in-dex was built using a SMT phrase extraction algo-rithm (Ling et al., 2010) including for each phrasepointers to all its occurrences in the corpus for fur-ther retrieval.1The EPPS corpus (Lambert et al., 2005) was very usefulas a training corpus in the developing stages of this system.It was however not used in the final system training.2.2 Syntactic Feature ExtractionThe syntactic tags feature is a novel feature we areintroducing for the CLWSD problem (Lefever andHoste, 2013).
They are linearizations of syntacticdependency trees.
These trees were built by Freel-ing?s Txala Parser (Lloberes et al., 2010) and wereintroduced as individual tags in a sentence analy-sis by parsing the tree and mapping its leaves withtheir corresponding order in the source sentence.Then, each leaf?s label and parent number was ex-tracted.
For the root, the special parent tag ?S?
wasused.The WSD literature commonly distinguishesbetween local and global context features (Mar-tinez and Agirre, 2001).
The former are extractedfrom the neighboring words and the latter are ex-tracted from words of the whole context providedusing some heuristic to select relevant.
Unlikeglobal features, the relevance of the surroundingwords is not put into question or are weightedby the degree of relevance according to their po-sition in the sentence and lexicographic distancefrom the target phrase (van Gompel, 2010).
Thereis a linguistic explanation as to why surroundingwords play a significant role in determining thetarget?s translation.
Often, these words have a di-rect dependency relation with the target.
Indeed,physical closeness is an approximation of syntac-tic relatedness.
What we propose in this paper isthat the relevance of the context words for deter-mining a correct translation is proportional to theirsyntactic relatedness to the target, rather than theirphysical closeness in the sentence.
Unlike Mar-tinez et al.
(2002), what we propose here is to usesyntax as a feature selector, rather than as a featureitself.Instead of defining a local and a global set ofrelevant words, we selected a single set of relevantwords according to their syntactic relation to thetarget phrase.
This set consisted of all the childrenof the target words, and the parents of the maintarget words.
The main target words are the subset7440 1 2 3 4 5 6Forms Las tres cosas est?n estrechamente vinculadas .Lemmas el 3 cosa estar estrechamente vincular .PoS Tags DA0FP0 Z NCFP000 VAIP3P0 RG VMP00PF FpSyn Tags espec:1 espec:2 subj:3 co-v:7 espec:5 att:3 ?
:7Table 2: Tagging of the sentence ?Las tres cosas est?n estrechamente vinculadas.
?of words with the highest number of (nested) chil-dren within the target phrase.
Table 3 features therules used for selecting the relevant words.This Feature Extraction method uses the depen-dency labels as a means of selecting only rele-vant examples.
Take for instance the example sen-tences in Table 1.
Given that the target word is anattribute, the subject is included as a relevant fea-ture, as per the last rule in Table 3.
Any examplesentence in which there is no subject as the sib-ling of the target word (as is the case for the firstexample sentence in Table 1) will have an emptyfeature, which increases its likelihood of not beingincluded in the training set of this sentence.2.3 Test Data Pre-processingThe test data for this task is composed of bilin-gual input sentences, making it impossible to ob-tain a correct tagging or parsing.
To overcome thisissue, a two-stage process wherein the first stageobtains translations for the L1 portions was per-formed.
These plausible translations are obtainedby TiMBL using as features the neighboring wordsof the target phrases.
Once the sentences are in asingle language (L2) they are tagged and parsedsyntactically.
Finally, the second stage consists inapplying the same feature selection algorithm pro-posed in subsection 2.2.2.4 Translation SelectionThe processing of each sentence consists of sev-eral steps.
In the first step, the L1 target phraseis searched for in the phrase index Given an L1phrase, a binary search algorithm iterates throughthe phrase index and returns an array of point-ers2to the corpus.
Then, a multi-threaded subrou-tine reads the word-aligned bilingual corpus andextracts all the referenced sentences.
Thus, foreach input sentence, a set of example bilingualword-aligned sentences is extracted from the cor-pus.
Relevant features are extracted according to2Given that line breaks are just regular characters, what isactually referenced in the phrase index are byte offsets.a syntactic analysis as explained in subsection 2.2,and written to text files in the C4.5 format.
Thefeatures extracted from the example sentences, aswell as the L2 translations of the target phrasesin each sentence, are used as the training set forTiMBL, while the features extracted from the in-put sentence are used as its (singleton) test set.The L2 translations of each target phrase in theexample sentences are used as the classes for thetraining set, in order to turn a bilingual disam-biguation problem into a machine learning clas-sification problem.
TiMBL learns how to classifythe training feature vectors into their correspond-ing classes and then predicts the class for the testset feature vector, i.e.
its most likely translationusing an IBL algorithm (Aha et al., 1991), whichis a variation of the k-nearest neighbor classifier.3 System SubmissionsWe submitted three result sets for the English-Spanish language pair.
Two of them were submit-ted for the ?Best?
evaluation type, and the otherone was submitted for the ?out-of-five?
evaluationtype.
The difference between these two evaluationtypes is that out-of-five evaluation expects up tofive different translations for every target phrase,while ?best?
only accepts one.
The evaluation met-rics include accuracy and recall, and also a word-based special type of accuracy, which takes intoaccount partially correct translations.Of the two runs submitted in the ?Best?
evalu-ation type, Run1-best (see table 4) used our pro-posed syntactic feature extraction method, whileRun2-best used a regular 2-word window aroundthe target phrase.
For the Run1-oof we combinedthe two methods mentioned above with differentvalues of k.3.1 ResultsThe test data consisted in 500 sentences written inSpanish, with target English phrases.
The officialresults obtained by our runs are shown in Table 4.Our control run, Run2-best, yielded slightly745Case Rule ExampleOne of the target words is asubject.Include any sibling which is anauxiliary or modal verb.Our cat quiere comerse la en-salada.The parent of one of the maintarget words is a coordinativeconjunction.Include its closest sibling.
No quer?a ni eat, ni dormir.The parent of one of the maintarget words is a relative pro-noun.Include its grandparent.
No creo que ella is coming.One of the target words is anattribute.Include any sibling which issubject.Mis t?as est?n very tired.Table 3: Relevant word selection rules.better results than our experimental run, Run1-best.
This means that our method of syntactic fea-tures extraction did not improve translation qual-ity.3.2 Error AnalysisBy analyzing our results, we detected three groupsof recurrent errors.
The first group of errors is re-lated to verb morphology, in which a single En-glish verbal form corresponds to many Spanishverbal forms.
In these cases, our system often out-puts an infinitive form or a past participle insteadof a finite verb.The second group of errors we detected com-prises incomplete translations.
In these cases, asingle word in English has a multiword Spanishtranslation, but our system often outputs a single-word translation.The third group of errors are related to Englishwords with multiple possible parts of speech, as?flood?, which can be a noun but also a verb.
Oursystem tends to output nouns instead of verbs andvice versa.4 DiscussionThere are two main reasons as to why the syntac-tic feature extraction method did not work.
Thefirst reason is related to the nature of the task; thesecond is related to the scope of the method.The fact that this task involved analyzing sen-tences partly written in two languages made syn-tactic analysis extremely difficult as dependenciesspan all over the bilingual sentence.
The best solu-tion we found for this was to divide the operationof the system in two stages, where the first one didnot involve syntactic dependencies and provided aworking translation, and the second one used thisfirst translation to perform a syntactic analysis andthen rerun the classification step.
This, however,favored error propagation.
Although translationquality did improve between the two stages, therewere many cases in which a bad initial translationinvolved a bad syntactic analysis, which in turn re-sulted in a bad final translation.A more sophisticated version of his method wasinitially developed for the English-Spanish lan-guage pair and involved several language-specificrules.
However, we decided to make this methodlanguage-independent, so we simplified it to its ac-tual version.
This simplified version uses syntacticdependencies as feature selectors, but the featuresthemselves are regular lemma/PoS combinations,which is not always the best feature choice.5 ConclusionSyntactic dependency relations are an importantmeans of analyzing the internal structure of a sen-tence and can successfully be used to improve thefeature selection process in WSD.
However, syn-tactic parsing is far away from optimal in Spanish,a fortiori if it involves sentences written in two lan-guages.
For this kind of task, perhaps a statisticallanguage model of L2 would have yielded betterresults.AcknowledgmentsWe would like to specially thank Professor Sil-via Takahashi of Universidad de los Andes for hercontinued advice and support.
We would also liketo thank Pedro Rodr?guez for his development ofthe Linguee crawler, Mar?a De-Arteaga, AlejandroRiveros and David Hoyos for their useful sugges-tions in the conception and development of thisproject, and the rest of the UNAL-NLP team for746Run Recall Accuracy Word Accuracy Rank (runs) Rank (systems)Run1-best 0.993 0.721 0.794 5 2Run2-best 0.993 0.733 0.809 4 2Run1-oof 0.993 0.823 0.880 6 3Table 4: Official results.their interest and encouragement.
Many thanks toJay C. Soper for proof-reading this article.ReferencesDavid W. Aha, Dennis Kibler, and Marc K. Albert.1991.
Instance-based learning algorithms.
MachineLearning, 6:37?66.Walter Daelemans, Jakub Zavrel, Ko Van der Sloot,and Antal Van den Bosch.
2010.
Timbl: Tilburgmemory-based learner.
reference guide.
ILK Re-search Group, Tilburg University.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, andRichard Zens.
2007.
Moses: Open source toolkitfor statistical machine translation.
In Proceedings ofthe 45th Annual Meeting of the ACL on InteractivePoster and Demonstration Sessions, page 177?180.Patrik Lambert, Adri?
Gispert, Rafael Banchs, andJos?
B. Mari?o.
2005.
Guidelines for word align-ment evaluation and manual alignment.
LanguageResources and Evaluation, 39(4):267?285, Decem-ber.Els Lefever and V?ronique Hoste.
2013.
Semeval-2013 task 10: Cross-lingual word sense disambigua-tion.
In Second joint conference on lexical and com-putational semantics, volume 2, page 158?166.Wang Ling, Tiago Lu?s, Jo?o Gra?a, Lu?sa Coheur, andIsabel Trancoso.
2010.
Towards a general and ex-tensible phrase-extraction algorithm.
In IWSLT?10:International Workshop on Spoken Language Trans-lation, page 313?320.Marina Lloberes, Irene Castell?n, and Llu?s Padr?.2010.
Spanish FreeLing dependency grammar.
InLREC, volume 10, page 693?699.David Martinez and Eneko Agirre.
2001.
Deci-sion lists for english and basque.
In The Proceed-ings of the Second International Workshop on Eval-uating Word Sense Disambiguation Systems, page115?118.David Mart?nez, Eneko Agirre, and Llu?s M?rquez.2002.
Syntactic features for high precision wordsense disambiguation.
In Proceedings of the19th international conference on Computationallinguistics-Volume 1, page 1?7.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational linguistics, 29(1):19?51.Llu?s Padr?
and Evgeny Stanilovsky.
2012.
Freeling3.0: Towards wider multilinguality.
In Proceedingsof the Language Resources and Evaluation Confer-ence, pages 2473?2479, Istambul, Turkey, May.J?rg Tiedemann.
2012.
Parallel data, tools and in-terfaces in OPUS.
In Proceedings of the Lan-guage Resources and Evaluation Conference, page2214?2218, Istambul, Turkey, May.Maarten van Gompel, Iris Hendrickx, Antal van denBosh, Els Lefever, and V?ronique Hoste.
2014.Semeval-2014 task 5: L2 writing assistant.
In Pro-ceedings of the 8th International Workshop on Se-mantic Evaluation (SemEval-2014), Dublin, Ireland,August.Maarten van Gompel.
2010.
UvT-WSD1: a cross-lingual word sense disambiguation system.
In Pro-ceedings of the 5th international workshop on se-mantic evaluation, page 238?241, Uppsala, Sweden.747
