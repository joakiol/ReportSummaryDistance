Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 77?82,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsReference Bias in Monolingual Machine Translation EvaluationMarina FomichevaInstitute for Applied LinguisticsPompeu Fabra University, Spainmarina.fomicheva@upf.eduLucia SpeciaDepartment of Computer ScienceUniversity of Sheffield, UKl.specia@sheffield.ac.ukAbstractIn the translation industry, human transla-tions are assessed by comparison with thesource texts.
In the Machine Translation(MT) research community, however, it isa common practice to perform quality as-sessment using a reference translation in-stead of the source text.
In this paper weshow that this practice has a serious issue?
annotators are strongly biased by the ref-erence translation provided, and this canhave a negative impact on the assessmentof MT quality.1 IntroductionEquivalence to the source text is the defining char-acteristic of translation.
One of the fundamentalaspects of translation quality is, therefore, its se-mantic adequacy, which reflects to what extent themeaning of the original text is preserved in thetranslation.
In the field of Machine Translation(MT), on the other hand, it has recently becomecommon practice to perform quality assessmentusing a human reference translation instead of thesource text.
Reference-based evaluation is an at-tractive practical solution since it does not requirebilingual speakers.However, we believe this approach has a strongconceptual flaw: the assumption that the task oftranslation has a single correct solution.
In real-ity, except for very short sentences or very specifictechnical domains, the same source sentence maybe correctly translated in many different ways.Depending on a broad textual and real-world con-text, the translation can differ from the source textat any linguistic level ?
lexical, syntactic, seman-tic or even discourse ?
and still be considered per-fectly correct.
Therefore, using a single translationas a proxy for the original text may be unreliable.In the monolingual, reference-based evaluationscenario, human judges are expected to recognizeacceptable variations between translation optionsand assign a high score to a good MT, even ifit happens to be different from a particular hu-man reference provided.
In this paper we arguethat, contrary to this expectation, annotators arestrongly biased by the reference.
They inadver-tently favor machine translations (MTs) that makesimilar choices to the ones present in the referencetranslation.
To test this hypothesis, we perform anexperiment where the same set of MT outputs ismanually assessed using different reference trans-lations and analyze the discrepancies between theresulting quality scores.The results confirm that annotators are indeedheavily influenced by the particular human trans-lation that was used for evaluation.
We discussthe implications of this finding on the reliabilityof current practices in manual quality assessment.Our general recommendation is that, in order toavoid reference bias, the assessment should be per-formed by comparing the MT output to the origi-nal text, rather than to a reference.The rest of this paper is organized as follows.In Section 2 we present related work.
In Section 3we describe our experimental settings.
In Section4 we focus on the effect of reference bias on MTevaluation.
In Section 5 we examine the impact ofthe fatigue factor on the results of our experiments.2 Related WorkIt has become widely acceptable in the MT com-munity to use human translation instead of (oralong with) the source segment for MT evalua-tion.
In most major evaluation campaigns (ARPA(White et al, 1994), 2008 NIST Metrics forMachine Translation Challenge (Przybocki et al,2008), and annual Workshops on Statistical Ma-77chine Translation (Callison-Burch et al, 2007;Bojar et al, 2015)), manual assessment is ex-pected to consider both MT fluency and adequacy,with a human (reference) translation commonlyused as a proxy for the source text to allow foradequacy judgement by monolingual judges.The reference bias problem has been exten-sively discussed in the context of automatic MTevaluation.
Evaluation systems based on string-level comparison, such as the well known BLEUmetric (Papineni et al, 2002) heavily penalize po-tentially acceptable variations between MT andhuman reference.
A variety of methods have beenproposed to address this issue, from using multiplereferences (Dreyer and Marcu, 2012) to reference-free evaluation (Specia et al, 2010).Research in manual evaluation has focused onovercoming annotator bias, i.e.
the preferencesand expectations of individual annotators with re-spect to translation quality that lead to low levelsof inter-annotator agreement (Cohn and Specia,2013; Denkowski and Lavie, 2010; Graham et al,2013; Guzm?an et al, 2015).
The problem of ref-erence bias, however, has not been examined inprevious work.
By contrast to automatic MT eval-uation, monolingual quality assessment is consid-ered unproblematic, since human annotators aresupposed to recognize meaning-preserving varia-tions between the MT output and a given humanreference.
However, as will be shown in what fol-lows, manual evaluation is also strongly affectedby biases due to specific reference translations.3 SettingsTo show that monolingual quality assessment de-pends on the human translation used as gold-standard, we devised an evaluation task whereannotators were asked to assess the same set ofMT outputs using different references.
As controlgroups, we have annotators assessing MT usingthe same reference, and using the source segments.3.1 DatasetMT data with multiple references is rare.
We usedMTC-P4 Chinese-English dataset, produced byLinguistic Data Consortium (LDC2006T04).
Thedataset contains 919 source sentences from newsdomain, 4 reference translations and MT outputsgenerated by 10 translation systems.
Human trans-lations were produced by four teams of profes-sional translators and included editor?s proofread-ing.
All teams used the same translation guide-lines, which emphasize faithfulness to the sourcesentence as one of the main requirements.We note that even in such a scenario, humantranslations differ from each other.
We measuredthe average similarity between the four referencesin the dataset using the Meteor evaluation met-ric (Denkowski and Lavie, 2014).
Meteor scoresrange between 0 and 1 and reflect the proportionof similar words occurring in similar order.
Thismetric is normally used to compare the MT out-put with a human reference, but it can also be ap-plied to measure similarity between any two trans-lations.
We computed Meteor for all possible com-binations between the four available referencesand took the average score.
Even though Me-teor covers certain amount of acceptable linguis-tic variation by allowing for synonym and para-phrase matching, the resulting score is only 0.33,which shows that, not surprisingly, human transla-tions vary substantially.To make the annotation process feasible giventhe resources available, we selected a subset of100 source sentences for the experiment.
To en-sure variable levels of similarity between the MTand each of the references, we computed sentence-level Meteor scores for the MT outputs using eachof the references and selected the sentences withthe highest standard deviation between the scores.3.2 MethodWe developed a simple online interface to collecthuman judgments.
Our evaluation task was basedon the adequacy criterion.
Specifically, judgeswere asked to estimate how much of the meaningof the human translation was expressed in the MToutput (see Figure 1).
The responses were inter-preted on a five-point scale, with the labels in Fig-ure 1 corresponding to numbers from 1 (?None?
)to 5 (?All?
).For the main task, judgments were collected us-ing English native speakers who volunteered toparticipate.
They were either professional trans-lators or researchers with a degree in Computa-tional Linguistics, English or Translation Stud-ies.
20 annotators participated in this monolin-gual task.
Each of them evaluated the same setof 100 MT outputs.
Our estimates showed thatthe task could be completed in approximatelyone hour.
The annotators were divided into fourgroups, corresponding to the four available refer-78Figure 1: Evaluation Interfaceences.
Each group contained five annotators in-dependently evaluating the same set of sentences.Having multiple annotators in each group allowedus to minimize the effect of individual annotators?biases, preferences and expectations.As a control group, five annotators (nativespeakers of English, fluent in Chinese or bilingualspeakers) performed a bilingual evaluation task forthe same MT outputs.
In the bilingual task, an-notators were presented with an MT output andits corresponding source sentence and asked howmuch of the meaning of the source sentence wasexpressed in the MT.In total, we collected 2,500 judgments.
Boththe data and the tool for collecting humanjudgments are available at https://github.com/mfomicheva/tradopad.git.4 Reference BiasThe goal of the experiment is to show that depend-ing on the reference translation used for evalua-tion, the quality of the same MT output will be per-ceived differently.
However, we are aware that MTevaluation is a subjective task.
Certain discrepan-cies between evaluation scores produced by dif-ferent raters are expected simply because of theirbackgrounds, individual perceptions and expecta-tions regarding translation quality.To show that some differences are related toreference bias and not to the bias introduced byindividual annotators, we compare the agreementbetween annotators evaluating with the same andwith different references.
First, we randomly se-lect from the data 20 pairs of annotators who usedthe same reference translations and 20 pairs ofannotators who used different reference transla-tions.
The agreement is then computed for eachpair.
Next, we calculate the average agreement forthe same-reference and different-reference groups.We repeat the experiment 100 times and report thecorresponding averages and confidence intervals.Table 1 shows the results in terms of stan-dard (Cohen, 1960) and linearly weighted (Cohen,1968) Kappa coefficient (k).1We also report one-off version of weighted k, which discards the dis-agreements unless they are larger than one cate-gory.Kappa Diff.
ref.
Same ref.
SourceStandard .163?.01 .197?.01 0.190?.02Weighted .330?.01 .373?.01 0.336?.02One-off .597?.01 .662?.01 0.643?.02Table 1: Inter-annotator agreement for different-references (Diff.
ref.
), same-reference (Same ref.
)and source-based evaluation (Source)As shown in Table 1, the agreement is consis-tently lower for annotators using different refer-ences.
In other words, the same MT outputs sys-tematically receive different scores when differ-1In MT evaluation, agreement is usually computed usingstandard k both for ranking different translations and for scor-ing translations on an interval-level scale.
We note, however,that weighted k is more appropriate for scoring, since it al-lows the use of weights to describe the closeness of the agree-ment between categories (Artstein and Poesio, 2008).79ent human translations are used for their evalua-tion.
Here and in what follows, the differencesbetween the results for the same-reference annota-tor group and different-reference annotator groupwere found to be statistically significant with p-value < 0.01.The agreement between annotators using thesource sentences is slightly lower than in themonolingual, same-reference scenario, but it ishigher than in the case of the different-referencegroup.
This may be an indication that reference-based evaluation is an easier task for annotators,perhaps because in this case they are not requiredto shift between languages.
Nevertheless, the factthat given a different reference, the same MT out-puts receive different scores, undermines the reli-ability of this type of evaluation.Human score BLEU scoreReference 1 1.980 0.1649Reference 2 2.342 0.1369Reference 3 2.562 0.1680Reference 4 2.740 0.1058Table 2: Average human scores for the groups ofannotators using different references and BLEUscores calculated with the corresponding refer-ences.
Human scores range from 1 to 5, whileBLEU scores range from 0 to 1.In Table 2 we computed average evaluationscores for each group of annotators.
Averagescores vary considerably across groups of anno-tators.
This shows that MT quality is perceiveddifferently depending on the human translationused as gold-standard.
For the sake of compari-son, we also present the scores from the widelyused automatic evaluation metric BLEU.
Not sur-prisingly, BLEU scores are also strongly affectedby the reference bias.
Below we give an exampleof linguistic variation in professional humantranslations and its effect on reference-based MTevaluation.Src: ?????????
?2MT: But all this is beyond the control of you.R1: But all this is beyond your control.R2: However, you cannot choose yourself.R3: However, not everything is up to you todecide.2Literally: ?However these all totally beyond the controlof you.
?R4: But you can?t choose that.Although all the references carry the same mes-sage, the linguistic means used by the translatorsare very different.
Most of these references arehigh-level paraphrases of what we would considera close version of the source sentence.
Annota-tors are expected to recognize meaning-preservingvariation between the MT and any of the refer-ences.
However, the average score for this sen-tence was 3.4 in case of Reference 1, and 2.0, 2.0and 2.8 in case of the other three references, re-spectively, which illustrates the bias introduced bythe reference translation.5 Time EffectIt is well known that the reliability and consistencyof human annotation tasks is affected by fatigue(Llor`a et al, 2005).
In this section we examinehow this factor may gave influenced the evalua-tion on the impact of reference bias and thus thereliability of our experiment.We measured inter-annotator agreement for thesame-reference and different-reference annotatorsat different stages of the evaluation process.
Wedivided the dataset in five sets of sentences basedon the chronological order in which they were an-notated (0-20, 20-40, ..., 80-100).
For each sliceof the data we repeated the procedure reported inSection 4.
Figure 2 shows the results.First, we note that the agreement is alwayshigher in the case of same-reference annotators.Second, in the intermediate stages of the taskwe observe the highest inter-annotator agreement(sentences 20-40) and the smallest difference be-tween the same-reference and different-referenceannotators (sentences 40-60).
This seems to in-dicate that the effect of reference bias is minimalhalf-way through the evaluation process.
In otherwords, when the annotators are already acquaintedwith the task but not yet tired, they are able tobetter recognize meaning-preserving variation be-tween different translation options.To further investigate how fatigue affects theevaluation process, we tested the variability of hu-man scores in different (chronological) slices ofthe data.
We again divided the data in five setsof sentences and calculated standard deviation be-tween the scores in each set.
We repeated this pro-cedure for each annotator and averaged the results.As can be seen in Figure 3, the variation between800?20 20?40 40?60 60?80 80?1000.250.30.350.4Evaluated sentencesAverageWeightedkSame referenceDifferent referencesFigure 2: Inter-annotator agreement at differentstages of evaluation processthe scores is lower in the last stages of the evalua-tion process.
This could mean that towards the endof the task the annotators tend to indiscriminatelygive similar scores to any translation, making theevaluation less informative.0?20 20?40 40?60 60?80 80?1000.850.90.9511.051.1Evaluated sentences?Figure 3: Average standard deviations betweenhuman scores for all annotators at different stagesof evaluation process6 ConclusionsIn this work we examined the effect of referencebias on monolingual MT evaluation.
We com-pared the agreement between the annotators whoused the same human reference translation andthose who used different reference translations.We were able to show that in addition to the in-evitable bias introduced by different annotators,monolingual evaluation is systematically affectedby the reference provided.
Annotators consistentlyassign different scores to the same MT outputswhen a different human translation is used as gold-standard.
The MTs that are correct but happento be different from a particular human translationare inadvertently penalized during evaluation.We also analyzed the relation between referencebias and annotation at different times throughoutthe process.
The results suggest that annotatorsare less influenced by specific translation choicespresent in the reference in the intermediate stagesof the evaluation process, when they have alreadyfamiliarized themselves with the task but are notyet fatigued by it.
To reduce the fatigue effect, thetask may be done in smaller batches over time.
Re-garding the lack of experience, annotators shouldreceive previous training.Quality assessment is instrumental in the devel-opment and deployment of MT systems.
If evalua-tion is to be objective and informative, its purposemust be clearly defined.
The same sentence canbe translated in many different ways.
Using a hu-man reference as a proxy for the source sentence,we evaluate the similarity of the MT to a partic-ular reference, which does not necessarily reflecthow well the contents of the original is expressedin the MT or how suitable it is for a given pur-pose.
Therefore, monolingual evaluation under-mines the reliability of quality assessment.
Werecommend that unless the evaluation is aimed fora very specific translation task, where the numberof possible translations is indeed limited, the as-sessment should be performed by comparing MTto the original text.AcknowledgmentsMarina Fomicheva was supported by funding fromIULA (UPF) and the FI-DGR grant program of theGeneralitat de Catalunya.
Lucia Specia was sup-ported by the QT21 project (H2020 No.
645452).The authors would also like to thank the threeanonymous reviewers for their helpful commentsand suggestions.ReferencesRon Artstein and Massimo Poesio.
2008.
Inter-coderAgreement for Computational Linguistics.
Compu-tational Linguistics, 34(4):555?596.Ond?rej Bojar, Rajen Chatterjee, Christian Federmann,Barry Haddow, Matthias Huck, Chris Hokamp,Philipp Koehn, Varvara Logacheva, Christof Monz,81Matteo Negri, Matt Post, Carolina Scarton, LuciaSpecia, and Marco Turchi.
2015.
Findings of the2015 Workshop on Statistical Machine Translation.In Proceedings of the Tenth Workshop on StatisticalMachine Translation, pages 1?46, Lisboa, Portugal.Chris Callison-Burch, Cameron Fordyce, PhilippKoehn, Christof Monz, and Josh Schroeder.
2007.
(meta-) evaluation of machine translation.
In Pro-ceedings of the Second Workshop on Statistical Ma-chine Translation, pages 136?158.Jacob Cohen.
1960.
A Coefficient of Agreement forNominal Scales.
Educational and PsychologicalMeasurement, 20:37?46.Jacob Cohen.
1968.
Weighted Kappa: Nominal ScaleAgreement Provision for Scaled Disagreement orPartial Credit.
Psychological bulletin, 70(4):213?220.Trevor Cohn and Lucia Specia.
2013.
ModellingAnnotator Bias with Multi-task Gaussian Processes:An Application to Machine Translation Quality Es-timation.
In Proceedings of 51st Annual Meetingof the Association for Computational Linguistics,pages 32?42.Michael Denkowski and Alon Lavie.
2010.
Choos-ing the Right Evaluation for Machine Translation:an Examination of Annotator and Automatic MetricPerformance on Human Judgment Tasks.
In Pro-ceedings of the Ninth Biennal Conference of the As-sociation for Machine Translation in the Americas.Michael Denkowski and Alon Lavie.
2014.
MeteorUniversal: Language Specific Translation Evalua-tion for Any Target Language.
In Proceedings of theEACL 2014 Workshop on Statistical Machine Trans-lation, pages 376?380.Markus Dreyer and Daniel Marcu.
2012.
HyTER:Meaning-equivalent Semantics for Translation Eval-uation.
In Proceedings of 2012 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 162?171.Yvette Graham, Timothy Baldwin, Alistair Moffat,and Justin Zobel.
2013.
Continuous MeasurementScales in Human Evaluation of Machine Trans-lation.
In Proceedings 7th Linguistic Annota-tion Workshop and Interoperability with Discourse,pages 33?41.Francisco Guzm?an, Ahmed Abdelali, Irina Temnikova,Hassan Sajjad, and Stephan Vogel.
2015.
How doHumans Evaluate Machine Translation.
In Proceed-ings of the Tenth Workshop on Statistical MachineTranslation, pages 457?466.Xavier Llor`a, Kumara Sastry, David E Goldberg, Ab-himanyu Gupta, and Lalitha Lakshmi.
2005.
Com-bating User Fatigue in iGAs: Partial Ordering, Sup-port Vector Machines, and Synthetic Fitness.
In Pro-ceedings of the 7th Annual Conference on Geneticand Evolutionary Computation, pages 1363?1370.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a Method for AutomaticEvaluation of Machine Translation.
In Proceedingsof the 40th Annual Meeting of the ACL, pages 311?318.Mark Przybocki, Kay Peterson, and Sebastian Bron-sart.
2008.
Official Results of the NIST 2008 ?Met-rics for MAchine TRanslation?
Challenge (Metrics-MATR08).
In Proceedings of the AMTA-2008Work-shop on Metrics for Machine Translation, Honolulu,Hawaii, USA.Lucia Specia, Dhwaj Raj, and Marco Turchi.
2010.Machine Translation Evaluation versus Quality Es-timation.
Machine Translation, 24(1):39?50.John White, Theresa O?Connell, and Francis O?Mara.1994.
The ARPA MT Evaluation Methodologies:Evolution, Lessons, and Future Approaches.
In Pro-ceedings of the Association for Machine Transla-tion in the Americas Conference, pages 193?205,Columbia, Maryland, USA.82
